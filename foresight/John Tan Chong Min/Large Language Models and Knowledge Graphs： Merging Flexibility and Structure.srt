1
00:00:00,000 --> 00:00:07,080
Hi everyone and welcome to today's session. Today we'll be talking about a very exciting

2
00:00:07,080 --> 00:00:12,840
topic which tries to merge large language models and knowledge graphs together. So as

3
00:00:12,840 --> 00:00:17,480
you all already know, large language models are the recent hype. You can literally do

4
00:00:17,480 --> 00:00:22,240
anything or a lot of things with large language models by just changing the prom. It is very,

5
00:00:22,240 --> 00:00:28,320
very flexible. Get knowledge graphs extremely rigid. You have notes connected to other notes

6
00:00:28,320 --> 00:00:34,320
in relations, but they are also very informative because the relations don't change. The notes

7
00:00:34,320 --> 00:00:38,400
don't change. The large language models, one problem that they face is that they are a

8
00:00:38,400 --> 00:00:44,000
little stochastic. They tend to generate things that may not be grounded in facts. So it seems

9
00:00:44,000 --> 00:00:48,720
like naturally these two approaches seems like a good fit together. One is more flexible,

10
00:00:48,720 --> 00:00:54,640
which is the large language models. And one is more reliable, like the knowledge graph.

11
00:00:54,800 --> 00:01:00,400
Okay, so without further ado, let's begin today's topic. So I will roughly follow the

12
00:01:00,400 --> 00:01:04,480
framework of this paper called Unifying Large Language Models and Knowledge Graphs,

13
00:01:05,200 --> 00:01:11,760
a roadmap. Okay, this is by some IEEE fellows and senior members. I quite like the style of this

14
00:01:11,760 --> 00:01:17,360
paper, but I feel like a lot of the things that are surveyed in the paper are not exactly the

15
00:01:17,360 --> 00:01:22,640
latest large language model stuff. They are like the births, raw births, like basically the 2017

16
00:01:22,640 --> 00:01:28,560
to 2019 era, that kind. So I have supplemented it with some of the more recent advancements,

17
00:01:28,560 --> 00:01:34,160
like some length change stuff. So enjoy. Do feel free to comment anytime, because I think

18
00:01:34,160 --> 00:01:41,200
this is a very interesting field that can be expanded upon. Never before have we

19
00:01:41,200 --> 00:01:46,080
gotten large language models this powerful like chat GPD. And this is really something that we

20
00:01:46,080 --> 00:01:50,640
can look at to improve on traditional methods or even think of a new method that is not even a

21
00:01:50,640 --> 00:01:55,760
knowledge graph. Okay, later I'll share with you some ideas. Okay, so what are the pros and cons

22
00:01:55,760 --> 00:02:01,120
of knowledge graphs and large language models? So as I said earlier, you can look at the rough

23
00:02:01,120 --> 00:02:06,560
summary. I think this is quite a good summary. Okay, large language models, they are generalizable.

24
00:02:07,120 --> 00:02:13,360
Okay, they possess a lot of knowledge. All right. But what they lack is that they lack some form of

25
00:02:13,360 --> 00:02:20,320
understanding or facts. Okay, general language understanding. Okay, this one is debatable,

26
00:02:20,320 --> 00:02:25,680
because like GPT-4 can be said to understand language pretty well. Like NLU is like the

27
00:02:25,680 --> 00:02:31,200
ace most of the task there. Okay, so this one may be not so true in understanding, but for facts-wise,

28
00:02:32,320 --> 00:02:38,560
fact generation is still a problem right now. Okay, incompleteness. Okay, maybe I mean like

29
00:02:38,560 --> 00:02:43,680
sometimes they might generate things that don't answer the question fully. Okay, but increasingly

30
00:02:43,680 --> 00:02:47,920
this is not really a problem anymore. Okay, it's more of like the reliability right now. So I

31
00:02:47,920 --> 00:02:53,520
summarize this part here, reliability. Okay, I should use a different color. Let me just change

32
00:02:53,520 --> 00:03:02,400
my annotation. So I think the main thing that large language models lack are reliability. Oh no,

33
00:03:02,400 --> 00:03:16,080
it's the same color. Let me see. Reliability and consistency. This too. I mean, y'all have tried

34
00:03:16,080 --> 00:03:19,840
large language models before, right? You key in the same prompt. Okay, sometimes you get the

35
00:03:19,840 --> 00:03:25,440
different responses. Sometimes the response can be different. Like I said, should, it's a hot day

36
00:03:25,440 --> 00:03:28,480
today, right? Yeah, scrims on this can be yes, sometimes no, you know, that kind of thing.

37
00:03:29,120 --> 00:03:33,280
All right, so knowledge graphs, what do they have? Knowledge graphs have structural knowledge.

38
00:03:33,840 --> 00:03:39,120
They are quite accurate. Okay, decisive, I guess, if you can find a way to like connect an input

39
00:03:39,120 --> 00:03:44,080
node to an output node, you can say yes, there's a link between them. It's very interpretable. Okay,

40
00:03:44,080 --> 00:03:48,960
actually large language models are also quite interpretable. So it's not really a con here.

41
00:03:48,960 --> 00:03:54,880
Large language is both actually interpretability is also in large language models. Domain specific

42
00:03:54,880 --> 00:04:00,080
knowledge. Yes, but actually, if you think about it, large language models with context grounding

43
00:04:00,080 --> 00:04:06,000
also has domain specific knowledge. Okay, evolving language. This is something that is quite

44
00:04:06,000 --> 00:04:10,720
interesting. Large language models don't really have this evolving language unless you fine tune

45
00:04:10,720 --> 00:04:16,320
it. Maybe the recent Lamato you can fine tune on something. Okay, but you can also use something

46
00:04:16,320 --> 00:04:20,240
like a knowledge graph to ground the large language models. Can you see the synergy here?

47
00:04:20,240 --> 00:04:27,360
There's a lot of things that knowledge graphs do well, that is not exactly antagonistic or not

48
00:04:27,360 --> 00:04:31,840
exactly different in nature from the large language model, it can just be used to ground

49
00:04:31,840 --> 00:04:37,280
the large language model. So it's very interesting. So what are the cons of large language models?

50
00:04:37,280 --> 00:04:43,040
They hallucinate, black box, black domain specific knowledge. So it looks like there can be some

51
00:04:43,040 --> 00:04:48,960
synergy here. And let's explore how we can synergize these two approaches. Before we move on any

52
00:04:48,960 --> 00:04:59,200
quick questions so far. Okay, so this is the one way of getting context into a large language

53
00:04:59,200 --> 00:05:04,160
model and is used very often nowadays. It's called retrieval augmented generation.

54
00:05:04,160 --> 00:05:11,760
So this is the raw format you retrieve from a corpus of documents. You have a few documents

55
00:05:11,760 --> 00:05:18,560
here that you can retrieve from. And then maybe the user asks like, how much is a MacBook Pro?

56
00:05:22,880 --> 00:05:25,920
Right, recently I need to ask myself this question because I'm considering whether I

57
00:05:25,920 --> 00:05:30,720
should buy another one. So you know, they retrieve the relevant documents like,

58
00:05:30,720 --> 00:05:34,640
okay, this document is about MacBook. Okay, you retrieve the right documents.

59
00:05:36,720 --> 00:05:42,560
Okay, this document here is about maybe 2019. You can retrieve the right documents. All these

60
00:05:43,680 --> 00:05:50,720
documents will actually be your context over here. So you could have the context retrieves

61
00:05:50,720 --> 00:06:01,200
like that. MacBook Pro 2019 costs 5000 or something like that. Then you can have like in 2019

62
00:06:04,640 --> 00:06:12,160
Apple release MacBook Air 2019. Of course, I mean, I don't really know the details, but let's say

63
00:06:12,160 --> 00:06:16,240
these are the two documents you retrieve from your retrieval augmented generation.

64
00:06:16,880 --> 00:06:25,680
Okay, and after that, you ask the question like, how much is a MacBook Pro 2019? So it's been shown

65
00:06:25,680 --> 00:06:30,240
that if you use retrieval augmented generation, you can improve the consistency of the output

66
00:06:30,240 --> 00:06:35,040
of the large language model because you are grounding it in the earlier context, which is

67
00:06:35,040 --> 00:06:39,520
this part here, you are grounding it in this part here. So there's an element of grounding and

68
00:06:39,520 --> 00:06:44,000
this is very important for a lot of real world use cases. Because if you don't ground it, you can

69
00:06:44,000 --> 00:06:50,720
end up with quite nonsense generations. Alright, and just as a refresher, okay, what is the most

70
00:06:50,720 --> 00:06:56,240
common method used to select the top K like documents? Anyone can just blow talk? What's the

71
00:06:56,240 --> 00:07:03,680
most common metric to select the most relevant documents? That's a test of understanding. If

72
00:07:03,680 --> 00:07:08,160
you are doing retrieval augmented generation, what is the most common metric used to retrieve

73
00:07:08,160 --> 00:07:16,800
documents? To check the similarity. Anyone? You can write in your chat also. Dot product,

74
00:07:16,800 --> 00:07:22,400
yes, very good. Dot product or cosine similarity. That's right. So usually we use some form of

75
00:07:22,400 --> 00:07:28,880
embeddings. You embed your documents into a vector and then you use cosine similarity

76
00:07:29,840 --> 00:07:40,000
to check how similar the document is compared to the query. Okay, I'm going into some details

77
00:07:40,000 --> 00:07:44,960
over here. Okay, because actually this whole process of doing retrieval augmented generation

78
00:07:44,960 --> 00:07:51,200
and passing over knowledge graphs is very, very similar. Alright, in fact, you could even replace

79
00:07:51,200 --> 00:07:56,080
this retrieval augmented generation with like knowledge graph augmented generation is perfectly,

80
00:07:56,880 --> 00:08:04,080
I think it's replaceable. Alright, so this is some idea of how large language models can be made

81
00:08:04,080 --> 00:08:11,520
to be more accurate. Okay, using something like that. Okay, so this again, I just highlight the

82
00:08:11,520 --> 00:08:15,600
problems of large language models. Okay, may not be able to recall the knowledge, but you can retrieve

83
00:08:15,600 --> 00:08:20,400
the right context using this retrieval augmented generation provided you can retrieve the context

84
00:08:20,400 --> 00:08:26,880
currently. Alright, so this is a real world use case issue. Alright, I've talked to some people

85
00:08:26,880 --> 00:08:32,240
and they say that retrieval augmented generation with just the cosine similarity alone, okay,

86
00:08:32,240 --> 00:08:37,120
might not give you the right documents. So, you know, the embedding vectors train using

87
00:08:37,120 --> 00:08:42,000
contrastive loss, you know, they may not capture everything, especially if your document is very,

88
00:08:42,000 --> 00:08:46,720
very large. Okay, imagine you have only one vector to represent the entire document, and you have

89
00:08:46,720 --> 00:08:51,680
another vector to represent document, another document. So this is like document A, and another

90
00:08:51,680 --> 00:08:56,560
vector to represent document B, then you see how similar they are. But what about like,

91
00:08:57,760 --> 00:09:04,800
what if one document contains many parts? Right, I mean, each of these parts could have

92
00:09:04,800 --> 00:09:10,400
different meanings, right? Each of these sub parts could contain like, let's say you have this

93
00:09:10,400 --> 00:09:14,160
document could have a sub part that is like that, a sub part that is like that, a sub part that is

94
00:09:14,240 --> 00:09:19,200
like that. You know, they just aggregate all of this together into one vector, like that.

95
00:09:19,200 --> 00:09:23,840
Can you see that you're actually losing like information here, which means that when you

96
00:09:23,840 --> 00:09:28,640
retrieve something, let's say if I want to find out how to code, like a length chain question

97
00:09:28,640 --> 00:09:34,000
answer agent, you know, I'm not going to retrieve this vector because by vector similarity, my query

98
00:09:34,000 --> 00:09:39,920
is here. Alright, by vector similarity, maybe I'll retrieve a document that is like B instead,

99
00:09:39,920 --> 00:09:44,480
because like maybe it's nearer in terms of cosine similarity. Okay, I mean, it's greater

100
00:09:44,480 --> 00:09:49,440
is the opposite direction. Let me just make this vector look more similar to that. Like,

101
00:09:50,240 --> 00:09:57,040
let's say I have B is like that. So if this direction here is like how to code a length chain,

102
00:09:58,240 --> 00:10:03,280
QA agent, and this is the embedding vector for it, it goes in this direction.

103
00:10:04,080 --> 00:10:07,680
You know, you're not going to retrieve document A, although it contains that part over here.

104
00:10:07,680 --> 00:10:12,400
Okay, you're going to retrieve document B. This is one of the failings of the embedding

105
00:10:12,400 --> 00:10:17,360
vector. It just tries to capture the whole document into one vector. And this means that

106
00:10:17,360 --> 00:10:23,280
you may not be able to extract stuff out. Okay, Richard said something. This is why I keep saying

107
00:10:23,280 --> 00:10:29,920
context is king. Summarization is essentially impossible on segmentation on segmented documents.

108
00:10:30,720 --> 00:10:36,480
Yeah, definitely, because you summarize you lose information. Okay, so there needs to be like

109
00:10:36,480 --> 00:10:42,320
different hierarchies of how you retrieve things out, broad level, specific level. And you know what?

110
00:10:43,440 --> 00:10:47,520
Knowledge graphs might actually have that kind of hierarchy formulation. I'm actually jumping

111
00:10:47,520 --> 00:10:53,840
a few slides ahead, but give you an idea of why I'm so excited about this idea. All right. So

112
00:10:53,840 --> 00:10:57,680
actually some of the bypass that I've been telling people, I've been advising people is that like

113
00:10:57,680 --> 00:11:03,440
if you cannot get retrieval of mental generation to work, consider using like filters or like labels.

114
00:11:03,600 --> 00:11:12,160
So like this labels will say like, okay, maybe it's like product A, product B. So you know,

115
00:11:12,160 --> 00:11:17,760
instead of relying on just the retrieval of mental generation, or the embedding vector

116
00:11:17,760 --> 00:11:21,680
to actually embed the right knowledge, like let's say you have a length chain QA agent,

117
00:11:21,680 --> 00:11:27,520
I can tag this thing as a length chain QA agent inside this document. So there will be certain

118
00:11:27,520 --> 00:11:35,200
tags that you can have. So then you can then do like the embedding vector across the documents

119
00:11:35,200 --> 00:11:39,920
that have this text. Yeah, so maybe that's one way to like do a first hand filtering.

120
00:11:39,920 --> 00:11:45,280
Yeah, I mean, this is just like some, what do you call it, some bypasses to the downsides of

121
00:11:45,840 --> 00:11:52,160
embedding vectors for pros and similarities. So these are some ideas that could be done right

122
00:11:52,160 --> 00:11:58,160
now to bypass it. But if we have a way to use knowledge graph to do more broad level to more

123
00:11:58,160 --> 00:12:02,720
specific level extraction, maybe you don't even need all this, you can just pass through your

124
00:12:02,720 --> 00:12:07,840
knowledge graph and you can use that to ground the large language model. All right, so this is my

125
00:12:07,840 --> 00:12:13,040
last point here, knowledge graphs are useful to retrieve the right context, search the right

126
00:12:13,040 --> 00:12:17,440
keywords, retrieve the right subgraph. Like let me give you an example here, if let's say I have a

127
00:12:17,440 --> 00:12:27,520
graph like that. All right, so maybe this is a graph talking about like people who view Netflix.

128
00:12:27,520 --> 00:12:33,280
Okay, so these are the Netflix user graphs. So these are the users. And then maybe you have me

129
00:12:33,280 --> 00:12:39,920
over here, John. Okay, and then like the movies are watched. I like to watch the flash, the series,

130
00:12:39,920 --> 00:12:44,160
not bad. I highly recommend it. Then maybe we have another guy like maybe Richard can be here,

131
00:12:44,160 --> 00:12:50,880
watch other movies, like movie A and movie B, you know, yep. So if you want to like extract

132
00:12:50,880 --> 00:12:55,600
something out here, you can just search like for keywords and then you can just put this whole

133
00:12:55,600 --> 00:13:00,400
subgraph here. And then you can use this part here. Okay, how you want to pass it into the

134
00:13:00,400 --> 00:13:04,720
large language model, I leave it for future investigations. There are a few ways to do it.

135
00:13:04,720 --> 00:13:08,960
I will cover some ways today. So if you can pass this into the large language model,

136
00:13:08,960 --> 00:13:16,880
essentially, you can ground the LM in context of the knowledge graph. And then we can actually do

137
00:13:16,880 --> 00:13:23,040
this grounding at a more higher level grounding or more sub level grounding depends on which height

138
00:13:23,040 --> 00:13:27,760
of the knowledge graph you're going to take the notes from. All right, so I think this is a very

139
00:13:27,760 --> 00:13:33,760
exciting prospect. And yeah, I'm looking forward to see like how this can actually work. So I'll

140
00:13:33,760 --> 00:13:40,560
be actually working on getting this to work the next few weeks, right? Because I think doing

141
00:13:40,560 --> 00:13:44,400
something like that actually might help with the up challenge as well, the abstraction and reasoning

142
00:13:44,400 --> 00:13:51,920
corpus. So this is my latest kind of hit way that I'm going into. So this is from the knowledge

143
00:13:51,920 --> 00:13:56,160
graph conference. Okay, I actually listened to quite a few of their videos. This is the knowledge

144
00:13:56,240 --> 00:14:03,360
graph conference 2023. And there's a speaker, Danny from Dev6. I think I pronounced his name

145
00:14:03,360 --> 00:14:09,600
wrongly. But the idea is that if you are using chat GPT for your own applications, if you use chat

146
00:14:09,600 --> 00:14:15,200
GPT in different languages, you might get different outputs, okay, even for the same information. So

147
00:14:15,200 --> 00:14:18,560
you know, being Singaporean and you know, the presidential election is coming soon, I just

148
00:14:18,560 --> 00:14:22,560
asked like who is Singapore's current president right now. So you can see now is Halima Yacob.

149
00:14:23,520 --> 00:14:28,400
Yacob, sorry. And we asked the question in Chinese, all right, Singapore the

150
00:14:28,400 --> 00:14:36,960
總統, so you know, you say, Singapore, there's no president in Singapore. So this is basically

151
00:14:36,960 --> 00:14:40,400
the same information, you just translate it, you can get different performances

152
00:14:40,960 --> 00:14:46,160
with chat GPT. Okay, and the same thing for like if you use Lamatu, Lamatu is heavily trained on

153
00:14:46,160 --> 00:14:51,520
English. If you use Chinese, I'm very sure it won't do very well. All right, this is a practical

154
00:14:51,520 --> 00:14:56,480
problem of large language models. You know, the Chinese benchmarks like they use Ernie,

155
00:14:57,520 --> 00:15:02,080
Wenxing Yi and those other Chinese language models, they say that they perform better than

156
00:15:02,080 --> 00:15:08,640
GPT for. Okay, I mean, at first I was skeptical. Then now that I think about it, they might have

157
00:15:08,640 --> 00:15:13,840
done their evaluation on Chinese data sets. And their language models are fine tuned on the Chinese

158
00:15:13,840 --> 00:15:18,800
data set. So maybe there's some merit to their claims, okay, on the specific Chinese data sets

159
00:15:18,800 --> 00:15:25,120
here. So this is one of the things that knowledge graphs can actually help to solve, because knowledge

160
00:15:25,120 --> 00:15:31,120
graphs can sort of translate this thing, because knowledge graph is not language specific, you

161
00:15:31,120 --> 00:15:35,680
see. So your concepts like president, okay, regardless of how you represent it in words,

162
00:15:35,680 --> 00:15:41,120
okay, your Chinese words or English words, you can actually go to the same part in the knowledge

163
00:15:41,120 --> 00:15:50,080
graph. And then you can have the key words here, like Singapore, and then it's like Halima.

164
00:15:53,040 --> 00:15:57,840
So you can actually retrieve the kind of information regardless of language,

165
00:15:58,800 --> 00:16:05,680
okay, and then you can pass back this information back into the generation of the model. So this

166
00:16:05,680 --> 00:16:13,280
can go here, back here. So regardless of how you prompt GPT in a certain language, okay,

167
00:16:13,280 --> 00:16:24,160
you can do it. So maybe I just do the flow chart. So G-L-M, okay, language, language invariant

168
00:16:24,160 --> 00:16:34,000
representation. Okay, then you do your processing there. And then you go back to L-M. So if for

169
00:16:34,000 --> 00:16:39,840
those of you who have been to some of my other like discussion sessions, you would know that I

170
00:16:39,840 --> 00:16:47,760
like to say that this is the, this part here. This part here is what I call the abstraction layer,

171
00:16:49,280 --> 00:16:54,960
or the latent layer, latent space. So you process it in a way that is different from the input

172
00:16:54,960 --> 00:17:00,160
domain, but because the information you process is similar, in this case, we are still asking for

173
00:17:00,160 --> 00:17:04,720
semantic information about the president. You know, we don't have to do it in the language domain,

174
00:17:04,720 --> 00:17:10,720
we can do it in like maybe some representational space. It could be a graph, all right, and then

175
00:17:10,720 --> 00:17:16,240
you can use whatever you process the graph, you can go back to your input space. So this is one of

176
00:17:16,240 --> 00:17:21,360
the key advantages that, you know, if we could interface large language models with some form of

177
00:17:21,360 --> 00:17:28,720
graphical or some form of memory-based approach that is invariant to the input language type,

178
00:17:28,720 --> 00:17:35,040
you could get some performance advantage here. Questions so far? Anyone?

179
00:17:38,800 --> 00:17:44,800
Okay, so let's cover some of the basics. What is a knowledge graph? So I took this from the paper.

180
00:17:44,800 --> 00:17:51,040
The knowledge graph is basically a triplet consisting of source, destination, to relation. So like for

181
00:17:51,040 --> 00:17:57,280
example, Barack Obama was born in Honolulu. So this is the relation, okay, so relation.

182
00:17:58,560 --> 00:18:05,040
And this is like Barack Obama as the source, and Honolulu is like the destination. So each

183
00:18:07,520 --> 00:18:15,440
knowledge graph is made up of all these triplets joined together in various ways. And the idea is

184
00:18:15,440 --> 00:18:20,320
that you just need to connect those entities that are related to each other. You can like

185
00:18:20,720 --> 00:18:25,600
you can actually walk through the knowledge graph and get the information you need. Okay, so like

186
00:18:25,600 --> 00:18:30,320
there are of course like mega nodes, like for example, like Barack Obama will have a lot of

187
00:18:30,320 --> 00:18:34,960
connections leading out of it because you're describing the person. Then like stuff like places

188
00:18:34,960 --> 00:18:40,160
where a lot of things leading into it, because a lot of things like like are in the USA, a lot of

189
00:18:40,160 --> 00:18:46,960
things are in Singapore, you know. So this is the 20, 30 years ideas of knowledge graph. Okay, it's

190
00:18:46,960 --> 00:18:53,120
not too bad. Okay, but it's very restrictive. I personally think that there is a better way to

191
00:18:53,120 --> 00:18:58,800
represent information other than this kind of structure. Okay, and we can go and talk about

192
00:18:58,800 --> 00:19:04,560
it later in the discussion. All right, I have something in the chat. Richard says,

193
00:19:04,560 --> 00:19:10,080
is there a handy reference chart for how this looks or compares to word-to-veg and similar

194
00:19:10,080 --> 00:19:16,160
embeddings? Okay, so this typical knowledge graph that I'm talking about here does not

195
00:19:16,160 --> 00:19:21,600
have embeddings yet. Okay, but in the future iterations like in 2017 or 2018, I think people

196
00:19:21,600 --> 00:19:26,320
have come up with these things like knowledge graph embeddings. So they actually encode all

197
00:19:26,320 --> 00:19:31,200
this information here in some vector space. So like maybe like, it's something very similar to

198
00:19:31,760 --> 00:19:36,960
like the vector space that we see in in large language models. You can actually encode this

199
00:19:36,960 --> 00:19:42,000
thing in vector space. You can encode the relation here in the vector space or so.

200
00:19:42,640 --> 00:19:49,600
And then like, you can then encode this part here also in vector space. So like,

201
00:19:50,480 --> 00:19:54,160
so it's like doing a vector arithmetic now. So you can see that

202
00:19:56,960 --> 00:20:02,160
if I do a relation, it's just simply this one plus this one equals to this one. So I can do a

203
00:20:02,160 --> 00:20:07,680
like add a .2 in the first one here and then I can get. So if you have a sufficiently expressive

204
00:20:07,760 --> 00:20:13,120
enough embedding space, you can express the whole knowledge graph in the form of embeddings.

205
00:20:13,680 --> 00:20:19,920
And that is indeed what some of the later models do. In fact, this is highly related to

206
00:20:19,920 --> 00:20:26,160
graph neural networks. Because graph neural networks, they express each node as an embedding,

207
00:20:26,160 --> 00:20:30,400
then they do message passing, which means I share information with the other nodes. Like at each

208
00:20:30,400 --> 00:20:34,240
time step, I pass some information to the other nodes. And I mean, there are different variants

209
00:20:34,240 --> 00:20:38,800
of message passing. The most common is that the message meets in the middle, this one then updates

210
00:20:38,800 --> 00:20:46,160
both nodes. So there are a few ways of doing the idea of like updating the embeddings and so on.

211
00:20:46,160 --> 00:20:50,400
I'm not going to cover in detail about how all this are done because graph neural networks is a huge

212
00:20:50,400 --> 00:20:55,040
topic. Okay, personally, I think graph neural networks is probably not the answer to solving

213
00:20:55,040 --> 00:21:01,760
intelligence. I'm sorry to Peter Velikovic. I like what he's doing, but I don't think it's

214
00:21:02,640 --> 00:21:06,320
it's the right way to do it, like using differentiable deep learning to do it.

215
00:21:06,880 --> 00:21:13,280
So I think the knowledge graph that I've described over here, which is using vectors to do addition

216
00:21:13,280 --> 00:21:19,360
and then you get the other nodes. That's a very expressive knowledge graph. Okay, because you can

217
00:21:19,360 --> 00:21:28,080
actually express everything in vectors without the names. So you can theoretically do any kind

218
00:21:28,080 --> 00:21:34,160
of like addition provided, you know, nodes plus relation give you another node provided that

219
00:21:34,160 --> 00:21:40,960
exists. So if you could somehow represent the whole of the world's knowledge in the form of

220
00:21:40,960 --> 00:21:46,880
vector space, I say we are done. We can just like, we achieved zero short generalization.

221
00:21:46,880 --> 00:21:50,960
You just embed into that vector space and then you add something and then you go to somewhere else.

222
00:21:51,520 --> 00:21:57,760
Okay, but I don't think that's how intelligence is represented. Okay, because you know, there's

223
00:21:57,760 --> 00:22:05,040
this thing called like context dependent embeddings. Like I don't think like the word Barrett

224
00:22:05,040 --> 00:22:10,080
Obama would have the same embedding all the time. So like, for example, if you have Barrett Obama that

225
00:22:10,080 --> 00:22:19,120
is like at the White House, Barrett Obama at his house, okay, Barrett Obama at the beach or maybe

226
00:22:19,120 --> 00:22:25,200
different places of Barrett Obama will lead to different characteristics of Barrett Obama. Like

227
00:22:25,280 --> 00:22:30,400
he maybe is very serious in the office, but he's very relaxed at the beach. You cannot have the

228
00:22:30,400 --> 00:22:37,360
same embedding space to represent all this. You need to walk it according to the context. Okay,

229
00:22:37,360 --> 00:22:43,520
and that is something that I actually intend to try to do it. Like I try to do a very flexible

230
00:22:44,960 --> 00:22:50,960
like basically the information can walk according to the parent nodes in this new form of knowledge

231
00:22:50,960 --> 00:22:56,160
graph that I'm thinking of. Okay, so whatever I'm talking about is my own idea. I haven't seen

232
00:22:56,160 --> 00:23:00,880
any paper on it yet, but I think the current knowledge graphs will all fail at embodying

233
00:23:00,880 --> 00:23:07,440
intelligence because it's just too restrictive right now. Okay, Shang, you asked a question.

234
00:23:08,560 --> 00:23:14,080
I'm unfamiliar with graph theory, so hoping to know how do you represent factors as weights and

235
00:23:14,080 --> 00:23:19,920
how many can you add? Okay, could you elaborate what you mean by factors?

236
00:23:23,520 --> 00:23:32,560
Yeah, you mentioned that like you can add any form of intelligence, right? So take for example,

237
00:23:32,560 --> 00:23:42,720
if we are using, yeah, I actually didn't think of this example, but let's say just the simplest one,

238
00:23:44,160 --> 00:23:56,240
multi-layer map. Then for these roads, one weight could be how fast the speed limit of the road

239
00:23:56,240 --> 00:24:04,160
and another weight could be how occupied it is. Okay, so you are talking about like descriptions

240
00:24:04,160 --> 00:24:08,960
of an object, like all characteristics, attributes, you're talking about attributes of an object.

241
00:24:09,920 --> 00:24:17,280
Yeah, the weight of each line, correct, of each connection between the nodes.

242
00:24:18,240 --> 00:24:24,720
Ah, okay, so like how do you get this embedding here, right? Yeah, correct. Yeah, so perhaps like

243
00:24:24,720 --> 00:24:28,640
in your original embedding space, each of these dimensions could represent something already,

244
00:24:28,640 --> 00:24:33,920
like maybe one could represent road, one could represent like emotion or, you know,

245
00:24:34,000 --> 00:24:39,600
there are different domains that these dimensions could capture already. So if you really have that,

246
00:24:39,600 --> 00:24:46,400
you can just like add the relation in that specific dimension. Yeah, so of course, all this

247
00:24:46,400 --> 00:24:51,440
will need to be like learn somehow. So it's either learn through deep learning or some fixed biases.

248
00:24:52,640 --> 00:24:59,040
Yeah, so ultimately, how well the graph does will depend on how good your embedding space captures

249
00:24:59,040 --> 00:25:07,600
all the information. Yeah, okay, so I hope that clarifies. Yeah, thank you. So for now,

250
00:25:08,960 --> 00:25:13,920
just know that knowledge graphs have a few forms. Okay, the most simple form is that you take words

251
00:25:13,920 --> 00:25:18,400
and then you add another word and then you get another word. So this like describes a relation.

252
00:25:18,400 --> 00:25:23,520
The more advanced form will be to use embeddings. All right, so we will talk more about and then

253
00:25:23,520 --> 00:25:27,040
of course, the even more advanced form is evolving embeddings or context dependent

254
00:25:27,200 --> 00:25:32,800
embeddings, which is like the idea that I have. And it's also the idea that large language models

255
00:25:32,800 --> 00:25:37,520
actually kind of use because when we can ground large language models in different contexts,

256
00:25:37,520 --> 00:25:42,960
you get different outputs. So a large language model is context dependent processing. Okay,

257
00:25:42,960 --> 00:25:46,640
if you can embody that kind of context dependence into the knowledge graph,

258
00:25:47,520 --> 00:25:52,880
you will have a very powerful knowledge graph. So as you can see, whatever I'm sharing with you

259
00:25:52,880 --> 00:25:57,360
here today, I think that I'm not the answer. All right, I'm just sharing with you here because this

260
00:25:57,360 --> 00:26:04,960
is what is existing. Okay, I have a grander vision compared to all of the stuff that I'm talking about.

261
00:26:04,960 --> 00:26:10,640
Okay, so let's continue. All right, so knowledge graphs, okay, what excites me in knowledge graph

262
00:26:10,640 --> 00:26:16,160
is the very notion of hierarchy. And I think hierarchy is key to intelligence. You don't

263
00:26:16,160 --> 00:26:21,840
process things in just one domain. You process things in many domains. Like if I'm drinking a cup

264
00:26:21,840 --> 00:26:28,640
now, I'm just like drinking water from a cup, not drinking a cup. Drinking water now, I use my

265
00:26:28,640 --> 00:26:33,120
hand to move like that. But then if I think about, oh, how do I go to school? Then I think about,

266
00:26:33,120 --> 00:26:39,040
oh, I need to do the bus stop, I need to go to the MRT maybe, and then I need to take this bus

267
00:26:39,040 --> 00:26:44,160
or this train. So this is a more higher level planning. Okay, if I were to think about like

268
00:26:44,160 --> 00:26:47,760
how I move my left leg and right leg, left leg, right leg, I will take forever before I do some

269
00:26:47,760 --> 00:26:54,560
planning. So different problems require different levels of solution finding. And I call this

270
00:26:54,560 --> 00:26:59,440
different levels of hierarchy. So it might be up challenge, the abstraction of reasoning corpus.

271
00:26:59,440 --> 00:27:04,400
I use multiple levels of hierarchy, like we have a pixel level, we have an object level,

272
00:27:05,040 --> 00:27:09,120
you know, and then you express the input grid into different forms of hierarchy.

273
00:27:09,120 --> 00:27:13,360
And I find that this way can solve a lot of problems because different problems require

274
00:27:13,360 --> 00:27:17,840
different approaches to think of it. But you don't solve all problems using trigonometry,

275
00:27:17,840 --> 00:27:23,200
you solve some using algebra, you solve some using set theory. So you have different ways

276
00:27:23,200 --> 00:27:30,720
of viewing the problem. And knowledge graph, you can actually use this to extract different,

277
00:27:30,720 --> 00:27:34,800
like at the top layers of the knowledge graph, typically are the more broad concepts and the

278
00:27:34,800 --> 00:27:41,600
bottom will be more of the, more of the general concepts. So you can see in this, this is the

279
00:27:42,000 --> 00:27:47,280
the SICK knowledge base. SICK is a 30 year old project trying to embody the world's knowledge

280
00:27:47,280 --> 00:27:52,480
in a knowledge graph. They are still trying to do it, but it turns out that this is a very

281
00:27:52,480 --> 00:28:00,080
hard thing to do because the knowledge graphs itself, it embodies like one relation is like

282
00:28:00,080 --> 00:28:04,560
a confirmed relation. But sometimes, you know, based on the context, you may not have that

283
00:28:04,560 --> 00:28:09,200
confirmed relation. So again, this is the context dependent knowledge graph I'm talking about.

284
00:28:10,160 --> 00:28:15,520
Also, another thing is a lot of times we do things, but we don't really know how to express it

285
00:28:15,520 --> 00:28:22,480
in words. So if you want to express the whole idea of logic in words, it's a very, very difficult

286
00:28:22,480 --> 00:28:26,880
task. Because sometimes we don't even know why we're doing something. Okay, there's bound to be a

287
00:28:26,880 --> 00:28:31,840
point of time that logic cannot express things. So you can go and look at this thing called

288
00:28:31,840 --> 00:28:38,880
Godot incompleteness theorem. If you use mathematical logic to express things, there comes a point in

289
00:28:38,880 --> 00:28:48,160
a time whereby logic cannot solve. Because the way to solve it lies beyond logic. It cannot be

290
00:28:48,160 --> 00:28:55,440
Godot incompleteness theorem. There's something like this. This sentence is false. So if you can

291
00:28:55,440 --> 00:29:01,680
represent this as a Godot number, this kind of sentence, and then you say that, oh, this number

292
00:29:01,680 --> 00:29:06,640
is true. But then this number says that this number is false. So it's like you have a self-referential

293
00:29:07,360 --> 00:29:12,400
loop. So if you use logical prepositions, and knowledge graph is sort of like a logical

294
00:29:12,400 --> 00:29:20,080
preposition, A goes to B, B goes to C, you might face this problem that you can actually go in

295
00:29:22,080 --> 00:29:28,480
you can actually go in a loop that contradicts itself. So that's one thing that knowledge

296
00:29:28,480 --> 00:29:36,000
graphs may have some issue with if we do it being 100% fact. If A links to B all the time,

297
00:29:36,080 --> 00:29:42,640
sometimes you might actually have a link that contradicts itself. That's one issue of the

298
00:29:42,640 --> 00:29:49,920
knowledge graph. The other thing is, so burying all these issues, one thing I like about knowledge

299
00:29:49,920 --> 00:29:55,200
graph is that you can see in this diagram here, you start off with small stuff like thing, and then

300
00:29:55,200 --> 00:30:01,840
you go to like individual, you go to collections, and then you have different ways of doing it,

301
00:30:01,840 --> 00:30:09,040
time, movement, and so on. Then you have agents, actors, plans, and goals. I mean,

302
00:30:09,040 --> 00:30:12,240
if you think about it, kind of it's like how large language models is evolving now, right?

303
00:30:12,240 --> 00:30:17,200
We are kind of at this stage, we are agents now. So after that, we have organization of agents,

304
00:30:17,200 --> 00:30:22,080
we have activities. Okay, hopefully we don't get to military warfare, you know, because like,

305
00:30:22,720 --> 00:30:28,800
so this is like the evolution of a population. So it's quite nice, and like you can capture all

306
00:30:28,800 --> 00:30:33,840
this knowledge from, okay, so I wanted to say this is for more like micro level

307
00:30:34,800 --> 00:30:41,040
to more macro level. And the macro level is actually the sum of the micro level. So

308
00:30:41,920 --> 00:30:45,280
maybe the arrow should be drawn the other way, the arrow should be drawn like that.

309
00:30:46,080 --> 00:30:50,480
You take from the micro stuff, and you go to the macro stuff. So this is the knowledge

310
00:30:50,480 --> 00:30:55,840
that we accumulate, right? And knowledge graphs can capture this quite well, because of the way

311
00:30:55,840 --> 00:31:00,320
you take from source relation to destination, you can capture from the micro level, you do

312
00:31:00,320 --> 00:31:07,680
all the branching, and then you end up with the micro level, right? So this was in 2016,

313
00:31:07,680 --> 00:31:13,840
by the way, I couldn't find this in the sick website today, right? So this is the sick knowledge

314
00:31:13,840 --> 00:31:18,400
graph. This is like a very, very tiny representation of how the knowledge graph looks like. I just

315
00:31:18,400 --> 00:31:22,080
wanted you to see like how one of the largest knowledge graph in the world looks like. So

316
00:31:22,080 --> 00:31:26,560
you can see like you have all these like the fortune companies, you have all these like functions,

317
00:31:27,360 --> 00:31:33,200
like all these like look like some form of like math stuff, right? Yeah, so you have like people

318
00:31:33,200 --> 00:31:39,920
over here. So you have different areas of congregation of all this knowledge, right? Then

319
00:31:39,920 --> 00:31:44,720
in order to pass through the knowledge graph, you have to use something very, very similar to SQL,

320
00:31:45,360 --> 00:31:49,920
structured query language, you like say that, oh, if I want to get a frightened person, I want to

321
00:31:49,920 --> 00:31:54,880
get the entity X that is a person, and then fuse emotion that is fear at a very high level.

322
00:31:55,520 --> 00:32:00,800
So you have to do this kind of stuff, right? So immediately you can see that knowledge graph

323
00:32:00,800 --> 00:32:06,240
right now can be immediately improved by large language models in one aspect. Okay, and this

324
00:32:06,240 --> 00:32:10,880
aspect is that we can straight away use the large language models to generate this structured query

325
00:32:10,880 --> 00:32:19,600
language. Okay, so if all of you are thinking about this, like if you want to get like a very,

326
00:32:19,600 --> 00:32:24,160
very rigid programming language out, okay, you can actually write what you want in free text

327
00:32:24,160 --> 00:32:28,240
and then say convert this to SQL, and you can get it out. So you can do the same thing for

328
00:32:28,240 --> 00:32:33,600
this sick language. You give it some examples of how the language works. You say, I want to get

329
00:32:33,600 --> 00:32:37,920
a frightened person and then, you know, chat GPD is quite good at getting stuff like this out.

330
00:32:38,400 --> 00:32:44,080
Okay, no more SQL, right? I love it. Okay, if you need to use SQL, just use chat GPD.

331
00:32:44,080 --> 00:32:48,960
Okay, it's a very good replacement. All right, so this is one way large language models can

332
00:32:48,960 --> 00:32:52,400
already help to benefit knowledge graphs right now because it can pass through it

333
00:32:53,120 --> 00:32:57,920
using very human readable and understandable syntax. Like this kind of thing is not very human

334
00:32:57,920 --> 00:33:03,680
understandable. You can use free text to do it and we can do it right now. But more importantly,

335
00:33:03,680 --> 00:33:10,880
what can large language models do to help knowledge graphs? Okay, or what can knowledge

336
00:33:10,880 --> 00:33:15,120
graphs do to help language models? Okay, so before I move on to that, let's just talk about some other

337
00:33:15,120 --> 00:33:19,760
ideas I have. So I'm actually a reinforcement learning person. So like, I feel like knowledge

338
00:33:19,760 --> 00:33:24,480
graph can also represent stuff like different states, like you have different tiredness,

339
00:33:24,480 --> 00:33:30,240
they drink coffee, and then you're not awake. So if you know in the literature of

340
00:33:33,040 --> 00:33:36,240
reinforcement learning, this like a Markov decision process where these are the states,

341
00:33:37,200 --> 00:33:41,680
okay, and these are the actions. And then this is the next thing.

342
00:33:43,520 --> 00:33:47,440
Okay, so you can actually use knowledge graphs to represent stuff like this as well.

343
00:33:47,440 --> 00:33:54,240
Okay, because it's quite anything that has a link like that, you can represent this easily. Okay, so

344
00:33:57,680 --> 00:34:01,840
all right, this is perhaps the most important slide for today. Okay, this is not in the paper

345
00:34:01,840 --> 00:34:07,760
that I referenced, but this is the thing that I was thinking about. It's like knowledge graph is

346
00:34:07,760 --> 00:34:13,200
actually sort of a tool that can be used by the agent. So like retrieval of method generation

347
00:34:13,200 --> 00:34:17,840
may not get the right passages because like the embedding space may not be good. Perhaps we can

348
00:34:17,840 --> 00:34:22,160
use like a form of knowledge graph passing, okay, you can extract relevant parts of the knowledge

349
00:34:22,160 --> 00:34:27,280
graph, you can retrieve the context based on that. Okay, so you can ask the knowledge graph to get you

350
00:34:27,280 --> 00:34:33,680
the subgraph. The subgraph, you can then use it to ground the context of the agent and how you use

351
00:34:33,680 --> 00:34:38,000
it to ground. Okay, it's up to you. Okay, some people might use graph neural networks. I don't

352
00:34:38,000 --> 00:34:42,800
advocate for that. Okay, one other way of doing it is to just convert it back to free text. Okay,

353
00:34:43,440 --> 00:34:47,920
as easy as that. So you use the knowledge graph to extract out the relevant purposes and avoid the

354
00:34:47,920 --> 00:34:52,960
need for the embedding space, the open AI embeddings. Okay, you use the knowledge graph to

355
00:34:52,960 --> 00:34:57,600
extract it. Then you take the stuff that you extract from the knowledge graph, pass it back as text

356
00:34:57,600 --> 00:35:08,160
and then go back to the agent to ground it. All right. So yeah, one other way of, one good thing

357
00:35:08,160 --> 00:35:12,880
about this is that if you have stuff like if you are doing this for a robot, okay, that experiences

358
00:35:12,880 --> 00:35:18,640
the world, you might actually be able to use this knowledge graph. Okay, I'm conflating the

359
00:35:18,640 --> 00:35:22,400
term knowledge graph, but this knowledge graph can now be the state action state graph. You know,

360
00:35:22,400 --> 00:35:28,560
you can actually model relations of the world easily. Like I always believed like we learned

361
00:35:28,560 --> 00:35:33,120
from taking actions in the world. So we can actually build this knowledge graph dynamically.

362
00:35:33,120 --> 00:35:37,120
This is the third point. Okay, you can gain knowledge of the world. We can build up this

363
00:35:37,120 --> 00:35:41,280
knowledge graph bit by bit. All right. And then we can then query this knowledge graph

364
00:35:42,320 --> 00:35:50,160
and get answers from the knowledge graph to inform our choices. Okay, so about how we can get this

365
00:35:50,160 --> 00:35:54,960
part here, this is a huge thing here. Okay, because I believe that there's one thing that's missing

366
00:35:54,960 --> 00:36:01,040
in current knowledge graph and this thing is called changing the memory to the context

367
00:36:01,760 --> 00:36:07,280
at hand. Okay, so I treat the knowledge graph as memory. So like when you retrieve things from

368
00:36:07,280 --> 00:36:12,320
memory, okay, and then you want to apply it to the current state right now, current state of the

369
00:36:12,320 --> 00:36:18,240
world right now, you don't really want to just use that memory itself. You want to adapt that

370
00:36:18,240 --> 00:36:24,000
memory such that it will be relevant in this current state. Like if I have drank like coffee

371
00:36:24,880 --> 00:36:31,360
at school or not, drink coffee at home now, okay, you know, something like that. This, I will need

372
00:36:31,360 --> 00:36:36,480
to adapt that memory of drinking coffee somewhere else and then adapt it back to here. Okay, there's

373
00:36:36,480 --> 00:36:41,120
no point in giving me the memory of me drinking coffee somewhere else because it doesn't adapt to

374
00:36:41,120 --> 00:36:45,680
the current situation. So if you can adapt this knowledge graph to the current situation,

375
00:36:46,320 --> 00:36:50,960
that will be great. Okay, that will be great. So that's something that I think I'm trying to

376
00:36:50,960 --> 00:36:56,720
look into because you don't just want static knowledge extraction. Okay, you want knowledge

377
00:36:56,720 --> 00:37:02,640
extracted and manipulated to fit the current context. Okay, of course, for those of you all in

378
00:37:02,640 --> 00:37:07,680
my discord group, I've been thinking about memory recently. And you know, human memory is very

379
00:37:07,680 --> 00:37:13,440
malleable. Like if you think about something, you might actually affect that memory of it.

380
00:37:13,440 --> 00:37:17,760
So like a lot of times people in the childhood, they think that they have certain memories again,

381
00:37:17,760 --> 00:37:22,560
like maybe you are lost in a supermarket. So if I keep asking you questions about it, I say,

382
00:37:24,000 --> 00:37:30,000
who was the stranger with you when you were lost? Okay, so maybe there was no stranger. But if I keep

383
00:37:30,000 --> 00:37:34,480
asking you guiding questions like that, eventually, you might think of your memories like, oh, yeah,

384
00:37:34,480 --> 00:37:39,120
a stranger let me home after I went, I was lost in the in the in the mud. Okay, but that that may

385
00:37:39,120 --> 00:37:44,400
not have happened. That memory has changed because I've asked you certain guiding questions. And then

386
00:37:44,400 --> 00:37:50,480
you think that certain things are now in your memory. Right. So whether or not we should change

387
00:37:50,480 --> 00:37:57,200
this memory and affect this knowledge graph, I leave that to future discussion. Okay, because

388
00:37:57,200 --> 00:38:01,360
this is something very interesting, like should we change the existing memory that we have,

389
00:38:01,360 --> 00:38:06,240
based on the current context, get our brains that our brains do that. Okay, but should we do this

390
00:38:06,240 --> 00:38:12,480
for this kind of practical systems? Okay. Yeah, so we say humans hallucinate. Yeah, of course,

391
00:38:12,480 --> 00:38:18,400
we hallucinate a lot. And that is why actually, we are quite similar to large English models in

392
00:38:18,400 --> 00:38:22,960
that sense. Now people always say large English models not very reliable. Are humans reliable?

393
00:38:22,960 --> 00:38:27,440
Our memory is not that reliable, actually, if you think about it. But honestly, I cannot trust

394
00:38:27,440 --> 00:38:33,680
my memories that that much, because like sometimes if it's too far away, it can change like the book

395
00:38:33,680 --> 00:38:40,240
that I've been reading. It will say that like flashback memories, which people think that are

396
00:38:40,240 --> 00:38:44,640
very, very pertinent, flashback memories are memories like, like, you know, 911 collapse.

397
00:38:45,360 --> 00:38:49,360
People tend to remember what they were doing at that time, because it was so significant.

398
00:38:49,920 --> 00:38:54,560
It turns out that this flashback memories can be wrong. Okay, it can also be, it can also be

399
00:38:55,360 --> 00:39:01,280
change. So this is a very interesting thing. You can actually use like the current context

400
00:39:01,360 --> 00:39:05,840
to affect the memory you have. So you might actually affect the knowledge graph about whether

401
00:39:05,840 --> 00:39:11,200
or not we want it to be that way. Okay, we have to think about that. Okay, I digress a bit. Okay.

402
00:39:12,400 --> 00:39:16,560
But let me just get back to topic. Okay, today we have quite a few slides to cover. There are three

403
00:39:16,560 --> 00:39:20,560
approaches that I want to talk about today. First is that you can use knowledge graph to

404
00:39:20,560 --> 00:39:24,960
enhance your large language models. And this means that you can give it structured stuff,

405
00:39:24,960 --> 00:39:29,840
like domain specific knowledge. In some sense, it's like text based grounding is the same as

406
00:39:29,840 --> 00:39:33,840
retrieval of mental generation, just that now you take the information from a knowledge graph.

407
00:39:34,720 --> 00:39:40,320
Number two, you use large language models, expressivity, okay, and make a better knowledge

408
00:39:40,320 --> 00:39:44,240
graph. Okay, I like this approach as well. Okay, we will see how to do it. And lastly,

409
00:39:44,240 --> 00:39:48,160
you combine both approaches, you can get a synergized large language models and knowledge

410
00:39:48,160 --> 00:39:54,400
graph. And I think something like this will be able to embody intelligence. Okay, but not the

411
00:39:54,400 --> 00:40:01,360
current knowledge graph. We need to change it to a dynamic knowledge graph. Okay, what is a dynamic

412
00:40:01,360 --> 00:40:06,880
knowledge graph? Maybe I'll talk about it next time. Okay, after I flesh out some ideas that I

413
00:40:06,880 --> 00:40:11,840
have right now, I will create this dynamic knowledge graph. Okay, I think the current knowledge

414
00:40:11,840 --> 00:40:15,920
graphs are not the answer. We need to have a different kind of knowledge graph. But if we use

415
00:40:15,920 --> 00:40:22,240
this, I think we can get intelligence. Okay, let's move on to the next point. Approach one,

416
00:40:22,240 --> 00:40:27,200
knowledge graph augmented large language models. Okay, so there are two ways I can,

417
00:40:27,200 --> 00:40:31,200
I summarize the paper in two ways. The main thing is one, you can just put the knowledge graph as

418
00:40:31,200 --> 00:40:36,400
text. And the other one is treat this as an object. And what kind of object? Okay, you either use like

419
00:40:36,400 --> 00:40:41,360
graph neural networks, or you can use an embedding space. I mean, the one that was used was trans

420
00:40:41,360 --> 00:40:48,080
e trans embedding. You can go and search the paper trans. So this are some ways that we can use the

421
00:40:48,080 --> 00:40:55,120
knowledge graph, okay, to pass it. Let's go through the first way. So the first, oh, sorry,

422
00:40:55,120 --> 00:41:00,880
this is basically a pipeline for retrieval of large language models grounding. First,

423
00:41:00,880 --> 00:41:04,160
you use some form of knowledge retrieval, like, you know, retrieval method generation,

424
00:41:04,160 --> 00:41:10,080
you use cosine similarity, you get certain facts or some documents. So I'm just relating this to

425
00:41:10,080 --> 00:41:14,480
retrieval method generation, because they are almost the same. All right, you take in the facts,

426
00:41:14,480 --> 00:41:19,040
you ground the large language model, you get the answer. Okay, and over here in the paper,

427
00:41:19,040 --> 00:41:22,720
they put back propagation. But you know, how are you going to back propagate this knowledge retrieval?

428
00:41:23,600 --> 00:41:28,400
You're going to end up with some, like, very, very weird way of doing back propagation. I don't

429
00:41:28,400 --> 00:41:32,160
think back propagation, I don't think back propagation is the answer here. Maybe you want

430
00:41:32,160 --> 00:41:37,040
to back propagate your LM to find you think, okay, I grant that. But this part here, to back

431
00:41:37,040 --> 00:41:41,840
propagate to the knowledge retrieval, I don't think that should be done. All right, because this

432
00:41:41,920 --> 00:41:48,000
back propagation thing will lead to, like, changes in embedding space. And then if you

433
00:41:48,000 --> 00:41:53,520
change your knowledge retrieval, you also need to change your large language model. It's a never

434
00:41:53,520 --> 00:41:58,000
ending cycle of chasing each other. Like, if you change the knowledge embeddings for the knowledge

435
00:41:58,000 --> 00:42:01,440
retrieval, you also need to change how you interpret them in the large language model. So,

436
00:42:02,320 --> 00:42:06,240
yeah, I don't think you should use back propagation for the knowledge retrieval. You should probably

437
00:42:06,240 --> 00:42:12,000
use, like, memory methods, other methods, like you can say that, okay, what worked, what did not

438
00:42:12,000 --> 00:42:18,640
work, what worked, what did not work. Okay, you can reference this paper called Voyager. Okay,

439
00:42:18,640 --> 00:42:22,800
so there are these automatic curriculum learner. I think you should train the knowledge retrieval

440
00:42:22,800 --> 00:42:27,280
like the automated curriculum learner. You just ground it in some examples of what works, what

441
00:42:27,280 --> 00:42:34,400
doesn't work. You don't have to use back propagation for that. Okay, so the main pathway for knowledge

442
00:42:34,400 --> 00:42:39,360
graph for tax, for LMS is like that. You take the knowledge graph, pass through it, get some

443
00:42:40,160 --> 00:42:45,200
facts, and then feed it into the large language model. Okay, that's the main pipeline. Okay,

444
00:42:45,200 --> 00:42:57,600
questions on this? Okay, let's move on. Okay, so this is one of my favorite papers. Okay, this is

445
00:42:57,600 --> 00:43:03,680
the Generative Agents paper. They have 20 agents in the sandbox interacting with each other. And

446
00:43:03,680 --> 00:43:08,880
one thing that struck me quite well for this paper is that they actually use JSON structure

447
00:43:08,880 --> 00:43:16,160
to ground the actions. So, for example, if you want to ask like Eddie Lin, he currently is in

448
00:43:16,160 --> 00:43:21,520
the Lin family's house. He's in the bedroom actually on the desk. Okay, so you can ask the

449
00:43:21,520 --> 00:43:26,720
agent, okay, this is actually the chatGPD prompt. Okay, you can ask the agent like, okay, these are

450
00:43:26,720 --> 00:43:34,400
the other areas that we have. Okay, and all these other areas are obtained from the JSON.

451
00:43:36,560 --> 00:43:39,120
Actually, the JSON is like a knowledge graph.

452
00:43:41,840 --> 00:43:47,760
Okay, because we actually have hierarchies like Lin family house has a bedroom, has a study room,

453
00:43:47,760 --> 00:43:52,000
has a kitchen, you know, this is something like a knowledge graph. If you ask me like,

454
00:43:52,000 --> 00:43:56,720
they are just like representing the hierarchy of the house. Like, I mean, if you want to treat it

455
00:43:56,720 --> 00:44:02,480
as a knowledge graph, you will say like, this is the house. So, this is the Lin's house. Lin's house.

456
00:44:03,280 --> 00:44:09,520
Lin's house. I've just put Lin's age. Okay, then you can have like, the relation will be contains,

457
00:44:10,720 --> 00:44:15,440
okay, or comprises, I mean, contains, then you can have like bedroom.

458
00:44:16,320 --> 00:44:22,480
Yeah, so the JSON kind of hierarchy is a subset of what a knowledge graph can embody.

459
00:44:23,200 --> 00:44:29,360
All right, so I treat this as a knowledge graph. So you can, you can sort of ground the agent.

460
00:44:30,160 --> 00:44:34,560
Okay, like this is what the agent knows. Okay, this is the current memory that the agent has

461
00:44:34,560 --> 00:44:39,840
as a form of knowledge graph. Like this are the kind of areas that we actually know from the world.

462
00:44:39,840 --> 00:44:44,960
Okay, this is like, if you talk about grid cells and play cells, maybe find out more areas.

463
00:44:45,040 --> 00:44:49,520
Okay, you can ground them. Okay, these are your semantic knowledge or oppositional knowledge that

464
00:44:49,520 --> 00:44:55,040
you have about the world. And actually, these two are the positional knowledge. This is the first

465
00:44:55,040 --> 00:45:02,560
one is knowledge about the house, knowledge about house. And then the second one is knowledge about

466
00:45:02,560 --> 00:45:14,160
the world. Yeah. So you, you have all this knowledge, you can ground the agent to choose

467
00:45:14,240 --> 00:45:20,000
a specific place. Imagine if we did not ground the agent with all this stuff at the top,

468
00:45:20,000 --> 00:45:25,680
you just ask, where should Eddie Lin go to? Then Eddie Lin might, the LL might reply,

469
00:45:25,680 --> 00:45:29,680
Eddie Lin should go to the supermarket or something like something that is irrelevant to the game world.

470
00:45:30,320 --> 00:45:36,400
Right, so because we grounded it with some idea of what kind of possible areas that the agent

471
00:45:36,400 --> 00:45:44,000
should go, the agent is able to choose one area from the above list. And how is this list generated?

472
00:45:44,000 --> 00:45:49,120
It's generated from passing some form of knowledge graph. And this is what I mean by using knowledge

473
00:45:49,120 --> 00:45:54,080
graph as text to ground the large language model. So you can use this recursively, you can say that,

474
00:45:54,080 --> 00:45:59,920
oh, currently you are in the maybe common room. Okay, what, where in the common room would you

475
00:45:59,920 --> 00:46:04,960
like to go? We like to go to the sofa, to the mirror, and you can do this recursively. Okay,

476
00:46:04,960 --> 00:46:11,840
and then you can get a very, very specific area that the agent is going. Okay. Any clarifications

477
00:46:11,840 --> 00:46:18,560
on this so far before I move on? Yeah, if you haven't read this paper, go read it. Okay, this

478
00:46:18,560 --> 00:46:27,760
paper is good. It's one of the better ones. All right. So we have the Chinese LLM, it's called

479
00:46:27,760 --> 00:46:34,480
Ernie. All right, and this, what they do is they actually use two hybrid ways of generating

480
00:46:35,760 --> 00:46:41,120
the output. So they say that large language models lack grounding, lack consistency. So we use

481
00:46:41,120 --> 00:46:45,760
a knowledge graph. Okay, I granted that. But then I look at that structure and they're like,

482
00:46:45,760 --> 00:46:51,440
oh man, what is this? So they actually have a large language model. This is the typical

483
00:46:51,440 --> 00:46:54,640
transformer architecture. So this is a typical transformer on the left side.

484
00:46:57,120 --> 00:47:02,240
So they have two encoders. Okay, one is the T encoder, which is like the text encoder. And why

485
00:47:02,240 --> 00:47:06,960
is the knowledge graph encoder? So the knowledge graph encoder uses this thing called trans embeddings.

486
00:47:06,960 --> 00:47:10,400
Okay, I'm not going to go through that, but they train that embeddings using like,

487
00:47:10,400 --> 00:47:13,040
they take one vector and take another vector, then they connect the,

488
00:47:14,160 --> 00:47:18,640
just draw the diagram here for you to see. So you have one vector A and another vector B,

489
00:47:19,360 --> 00:47:24,800
and then you create another vector C here. So you can keep like using this vector A,

490
00:47:24,800 --> 00:47:28,640
you take another vector, extend from it, and then you can train on this relation C. So, you know,

491
00:47:29,360 --> 00:47:33,920
this is how the trans embeddings are trained, trans E. Okay, and they use this kind of embedding

492
00:47:33,920 --> 00:47:37,840
space. Okay, you can do self-attention, and then you can do cross-attention across

493
00:47:38,640 --> 00:47:42,960
both the trans, the text embeddings as well as the knowledge graph embeddings.

494
00:47:42,960 --> 00:47:46,800
And then hopefully you get some output, right? And then you get some text outputs here,

495
00:47:46,800 --> 00:47:51,520
and then you can update your knowledge graph to the knowledge graph outputs here. Okay, so this

496
00:47:51,520 --> 00:47:57,440
is a way to embody a knowledge graph as some embedding space. Okay, and then we can use like

497
00:47:57,440 --> 00:48:03,840
the attention to like, attend to like the text-based stuff as well. So, yeah, this is just one way of

498
00:48:03,840 --> 00:48:09,280
doing processing using a knowledge graph. As you can see, I don't really like it. I mean,

499
00:48:09,280 --> 00:48:13,680
I think that it's too convoluted. Like, okay, so this is another discussion question that I'd like

500
00:48:13,680 --> 00:48:18,480
you all to think about. Should we have separate embedding space for this large language model

501
00:48:18,480 --> 00:48:23,440
stuff and for the knowledge graph embeddings? Like, should we use two different embeddings?

502
00:48:23,440 --> 00:48:26,880
Okay, should we use two different embeddings?

503
00:48:26,960 --> 00:48:34,640
Yeah, or should we use the same one? Yeah. Okay, then also more generally, like if you want to have

504
00:48:34,640 --> 00:48:41,200
multi-modal embeddings, like you have text, image embeddings, you have audio embeddings,

505
00:48:41,200 --> 00:48:45,680
okay, if you want to do a multi-modal large language models, you can actually also put them

506
00:48:45,680 --> 00:48:51,920
into distribution model at the end here. Okay, but the question is, in fact, knowledge graph can

507
00:48:51,920 --> 00:48:56,800
be multi-modal also, you can actually also put it here. The question is, all this image

508
00:48:56,800 --> 00:49:01,120
and audio embeddings, okay, you can put it in the large language model, you can also put in the

509
00:49:01,120 --> 00:49:07,760
knowledge graph. But why not just use a single embedding, right? Why do you need to? You are

510
00:49:07,760 --> 00:49:12,640
using text-based knowledge graph and text-based large language models, that there's no external

511
00:49:12,640 --> 00:49:17,280
domain here. It's all the same domain. Why do we need two different embeddings for the knowledge

512
00:49:17,280 --> 00:49:23,200
graph and the input text? That's my question. If you have any ideas, let me know. But think about

513
00:49:23,200 --> 00:49:31,120
this, all right. Okay, then we have this question-answer graph neural network, and this does a

514
00:49:31,120 --> 00:49:35,920
two-way interaction between the language model and knowledge graph. And what we can see here is

515
00:49:35,920 --> 00:49:40,320
that we have a certain question, okay, and some options that to choose from. The large language

516
00:49:40,320 --> 00:49:45,200
model will go in here, and then they express the question and the options as part of a knowledge

517
00:49:45,200 --> 00:49:50,400
graph, okay, and this will go through another knowledge graph encoder, and this knowledge

518
00:49:50,480 --> 00:49:58,240
graph encoder is a graph neural network. Okay, and this basically will, you look at this diagram here,

519
00:49:59,760 --> 00:50:05,200
it will do cross-attention. It's very, very confusing. Look at this language model conditions

520
00:50:05,200 --> 00:50:09,200
knowledge graph. So you can blank out some notes here, you know, you can actually do some attention

521
00:50:09,200 --> 00:50:14,800
on some notes to like block off the path. Yeah, so yeah, that's possible. So you can also use the

522
00:50:14,800 --> 00:50:21,360
knowledge graph to condition the attention in the, in, in, when, when you do the next open

523
00:50:21,360 --> 00:50:27,520
processing for the language model, and then eventually you get your answer. Yeah, so this is

524
00:50:27,520 --> 00:50:32,160
one way we can process the knowledge graph, you can process it using a graph neural network.

525
00:50:32,960 --> 00:50:36,960
And in fact, the earlier one on Ernie, that is similar to graph neural network as well. I mean,

526
00:50:36,960 --> 00:50:41,760
they are using the embedding space. And you know, if you just do some operations on the embedding

527
00:50:41,760 --> 00:50:45,840
space, that is a graph neural network already. So yeah, this is very, very similar to graph neural

528
00:50:45,840 --> 00:50:53,200
networks. And yeah, it shows that back in the first few years, people use these kind of methods to

529
00:50:53,200 --> 00:50:58,960
pass through knowledge graph using graph neural networks to represent the embeddings. Okay,

530
00:50:59,920 --> 00:51:05,040
I don't see why you need to do this. Okay, personally, I don't see why you need to do this.

531
00:51:05,040 --> 00:51:11,520
You can just use text, because the knowledge graph representation is in the same kind of domain as

532
00:51:12,000 --> 00:51:17,040
your last language model representation. They're all text. Yeah, why do we need a separate embedding?

533
00:51:17,600 --> 00:51:26,000
So yeah. Okay, Richard said, I think there will have to be input output embeddings and train

534
00:51:26,000 --> 00:51:33,920
them to address common pattern or memory structures. Sorry, could you explain this comment was in

535
00:51:34,000 --> 00:51:41,600
relation to which part of what I said? So you were saying, you know, it's a decision sort of or

536
00:51:42,480 --> 00:51:48,560
sort of my decision question, right, as well, that there is, how do you

537
00:51:50,480 --> 00:51:54,640
well approach this problem? Why do they have their embeddings separate, right?

538
00:51:56,640 --> 00:52:01,040
At the end, there's a sort of cross attention where they're merging them for an output of this

539
00:52:01,040 --> 00:52:12,240
type or that type would have you. But then this idea comes that the real, so I think of a large

540
00:52:12,240 --> 00:52:16,800
language model where the reason why they have these emergent behaviors is because language is

541
00:52:16,800 --> 00:52:24,560
currently our best mechanism to embody thoughts, ideas, and our most direct implementation of

542
00:52:24,560 --> 00:52:30,800
ideas. Now, particularly once they're broken down, tokenized and so on, you've taken through that

543
00:52:31,040 --> 00:52:36,800
process a few times, consider retention and context, you come up with new ideas. Yes, yes.

544
00:52:38,800 --> 00:52:45,600
And then as you're just saying, there's no particular need for different embedding spaces.

545
00:52:46,720 --> 00:52:53,120
And the only need for them is to bring understanding into a common framework where

546
00:52:53,120 --> 00:52:58,400
the ideas themselves in the latent space are being considered and their context and their

547
00:52:58,400 --> 00:53:10,080
relationships. So how the, this is sort of a, what's the word, this idea of boiling down

548
00:53:11,280 --> 00:53:18,480
the actual form of communication into some representation, any representation where we

549
00:53:18,480 --> 00:53:28,400
can start applying our knowledge to it. Whether you read text or listen to text,

550
00:53:29,680 --> 00:53:36,160
you don't, when I hear things, I don't imagine them written down in front of me. I just hear words,

551
00:53:36,160 --> 00:53:44,320
words become ideas, and we go from there. So in the same way, I see the way that

552
00:53:45,040 --> 00:53:51,360
knowledge is presented as an input-output problem and embeddings really address the input-output

553
00:53:51,360 --> 00:53:59,920
problem. And then after that, there's a memory and consideration process which operates on ideas

554
00:53:59,920 --> 00:54:06,000
which are not linked to input and output. I think you and I agree that there needs to be a latent

555
00:54:06,000 --> 00:54:10,800
space or abstraction space for processing. And I think you also agree that there need not be too

556
00:54:10,800 --> 00:54:14,800
separate embedding space for the knowledge graph in the last image model. If I hear it correctly,

557
00:54:14,800 --> 00:54:18,400
right, you don't, you also don't think that is necessary, right? I think,

558
00:54:20,960 --> 00:54:25,040
yeah, but then the problem becomes, if you don't use the same embedding technique,

559
00:54:25,680 --> 00:54:31,840
how do you present meaning? So for mine, in terms of large language model being in language or not

560
00:54:31,840 --> 00:54:39,280
in language, in words, the question is really, are we making the problem harder for ourselves

561
00:54:39,920 --> 00:54:47,120
by using a difference, by saying, well, it's all words and the words are by and large correct.

562
00:54:47,120 --> 00:54:53,200
Therefore, we'll just use a large language model to read and ingest a large language model. And I

563
00:54:53,200 --> 00:54:58,960
think that will work. But the challenge becomes what you alluded to earlier, where the Chinese

564
00:54:58,960 --> 00:55:05,360
representation versus the English representation gives a different outcome. And I'm trying to

565
00:55:05,440 --> 00:55:14,800
abstract away that behaviour. So the thinking is, the actual thinking happens in, is always in

566
00:55:14,800 --> 00:55:23,360
latent space. And the only job for embeddings is to present in a form where, you know, cognition can

567
00:55:23,360 --> 00:55:33,440
happen. Right? Because, right? And so I would say, I don't particularly care what the encoding

568
00:55:33,520 --> 00:55:40,480
encoder is, decoder is, it can go from text in, picture out, picture in, text out, it doesn't

569
00:55:40,480 --> 00:55:47,520
matter. The important thing is that it's consistent, and we can operate on it in a manner that addresses

570
00:55:47,520 --> 00:55:54,480
the patterns and relationships within. Yeah, well said, well said. I agree with you. So

571
00:55:55,200 --> 00:55:58,720
what matters is how we abstract it to the processing space, which is the latent space,

572
00:55:59,360 --> 00:56:05,040
and how we encode it and decode is just extra details, that basically just needs to be mapped

573
00:56:05,040 --> 00:56:10,320
there, and it should be good enough. Yeah, I think so. And if it comes to its own training

574
00:56:10,320 --> 00:56:16,320
challenge. Correct. So I think in the earlier papers, what I get is like, why do we need a

575
00:56:16,320 --> 00:56:20,480
knowledge graph encoder like that? It's because they use embeddings like trans-e that, you know,

576
00:56:20,480 --> 00:56:26,400
are different from GPD embeddings, like, or BERT embeddings. Again, most of the early papers use

577
00:56:26,400 --> 00:56:34,080
but BERT, bi-directional encoder representations from transformers. So what happens is because

578
00:56:34,080 --> 00:56:38,720
these two are from a different embedding space. So you kind of need to map them to the same

579
00:56:38,720 --> 00:56:43,520
embedding space. That's why you need a knowledge graph encoder and a large-engaged model encoder.

580
00:56:45,040 --> 00:56:50,880
But in the new kind of knowledge graph that is constructed, because this large-engaged model

581
00:56:50,880 --> 00:56:55,680
is now so powerful, you can actually use the embedding space for the large-engaged model to

582
00:56:55,680 --> 00:57:00,960
construct your knowledge graph. And if you do that, okay, if you do that process, which is

583
00:57:00,960 --> 00:57:05,120
part two of the presentation today, you will see later, if you use the large-engaged models to

584
00:57:05,120 --> 00:57:09,200
construct the knowledge graph, actually, you don't need a separate knowledge graph encoder

585
00:57:09,200 --> 00:57:15,200
or encoder here because they are in the same embedding space already. So if you look at this

586
00:57:15,200 --> 00:57:22,880
thing here, you don't need a separate decoder for the JSON here because this is in the same

587
00:57:22,880 --> 00:57:28,800
embedding space as your text. And I would like to posit that it will be better for all of them to

588
00:57:28,800 --> 00:57:35,120
be in the same embedding space because it will be much easier to do the attention. I mean,

589
00:57:35,120 --> 00:57:38,480
it's easier to do attention in the same domain as compared to different domains because, you know,

590
00:57:38,480 --> 00:57:44,160
cross-attention is only one layer right now. You're going to do a very efficient cross-attentioning

591
00:57:44,160 --> 00:57:49,040
multiple layers. But if you just do it in the same domain, the transformer architecture right now,

592
00:57:49,040 --> 00:57:54,800
you actually do the self-attention multiple times, all right? So it might be actually better to

593
00:57:54,800 --> 00:57:59,760
do it in the same domain, okay? And of course, you will save training complications because,

594
00:58:00,400 --> 00:58:04,880
you know, you need to map both to the same latent space and, you know, that is a difficult problem.

595
00:58:04,880 --> 00:58:10,400
It's a very difficult problem to map two different streams of inputs to the same latent space. I

596
00:58:10,400 --> 00:58:15,840
mean, we have seen it, like in OpenAI, they have this thing called clip, okay, that max text and

597
00:58:15,840 --> 00:58:25,120
images to same latent space. You know how many examples they train it on? Millions, I think even

598
00:58:25,120 --> 00:58:30,480
billions. Yeah, so it's a very, very difficult problem to map both to the same latent space.

599
00:58:30,480 --> 00:58:34,400
If I can map it well, of course, you can do like stuff like stable diffusion, you know,

600
00:58:35,120 --> 00:58:40,800
dolly, you can generate images from text. Yeah, but why have this problem with the

601
00:58:40,800 --> 00:58:45,040
knowledge graph when you can actually just ground the knowledge graph in the same embedding space

602
00:58:45,040 --> 00:58:53,440
as your large language model? Okay, so think about that. Okay, now we go to approach two.

603
00:58:53,440 --> 00:58:59,040
Before I move on, I'd like to open the floor for any opportunities to ask anything so far for the

604
00:58:59,040 --> 00:59:08,400
first part. Okay, if not, I'll carry on. So next is how we can use a knowledge large

605
00:59:08,480 --> 00:59:15,280
language model to get the knowledge graph. So one is to use using few short and zero or zero

606
00:59:15,280 --> 00:59:19,520
short prompting, like for example, length chain, okay, I don't think the approach is that great.

607
00:59:19,520 --> 00:59:25,840
Okay, I found a better approach, right, using a better prompt, but we can potentially use

608
00:59:25,840 --> 00:59:30,000
large language models to generate knowledge graphs. The other way is to use the embedding

609
00:59:30,000 --> 00:59:35,440
space of the large language models to enrich the representational space of the knowledge graph.

610
00:59:36,000 --> 00:59:41,760
So this is also quite interesting. Let's see how we can do both. Okay, the first one is,

611
00:59:42,480 --> 00:59:47,520
okay, this is just some idea of how we can use it. Okay, we can, we can few short prompt

612
00:59:47,520 --> 00:59:51,840
to generate the relations, okay, because large language models are just very versatile

613
00:59:51,840 --> 00:59:56,400
and can be context driven to do it. And actually it's way better than, you know,

614
00:59:57,280 --> 01:00:02,640
so this is my own experience. I use space scene to do name entity recognition and I use

615
01:00:02,640 --> 01:00:08,800
large language models to do that. The GPT, chat GPT performs way better than space scene. Space

616
01:00:08,800 --> 01:00:14,560
scene makes out a lot of the names, all right. So if we use large language models to generate

617
01:00:14,560 --> 01:00:20,480
the knowledge graph, compared to traditional approaches, like spacey or some other verb,

618
01:00:22,400 --> 01:00:27,520
VMP, you know, those kind of three parcels for language. Last time people used that to generate

619
01:00:27,520 --> 01:00:33,600
the knowledge graph to find out what are the nouns, what are the verbs and so on. Okay, so that was

620
01:00:33,600 --> 01:00:37,280
difficult to generate the knowledge graph because sometimes it miss out certain things.

621
01:00:37,280 --> 01:00:41,840
But large language models are quite good. Okay, why not just use large language models directly

622
01:00:41,840 --> 01:00:48,320
to generate the relations and the source and the destination. So indeed, this is what Lang

623
01:00:48,320 --> 01:00:53,440
Chen did. Okay, so if you look at the graph QA prompt, this is the prompt. Okay, you are the

624
01:00:53,440 --> 01:00:58,400
network intelligence. Okay, help to integrate stuff into a knowledge graph, extract knowledge

625
01:00:58,400 --> 01:01:03,200
triples from text. Okay, a knowledge triple is a clause that contains a subject, predicate,

626
01:01:03,200 --> 01:01:08,240
an object. Okay, subject is entity being described, predicate is the property,

627
01:01:09,280 --> 01:01:15,520
object is the value of the property. Okay, there's a typo here. Okay, so this is the

628
01:01:17,120 --> 01:01:21,680
zero shot prompting for Lang Chen. All right, this is not that good yet. So you need to give

629
01:01:21,680 --> 01:01:25,920
some few short examples and they gave some few short examples in the prompt, like for example,

630
01:01:26,720 --> 01:01:33,120
like this is the input, and then you can say that oh, Nevada is a state, Nevada is the U.S.,

631
01:01:33,120 --> 01:01:38,640
Nevada is number one go producer in Go. So I don't like this example. Okay, because for one,

632
01:01:38,640 --> 01:01:44,560
they did not say the state at all in the prompt. Like then you want the model to just plug the

633
01:01:45,280 --> 01:01:52,080
plug the noun from thin air. So yeah, like here, I'm going to the store output none. Why, why is

634
01:01:52,080 --> 01:02:01,840
the output none? It should be I went to store something like that. Yeah, so you should be able

635
01:02:01,840 --> 01:02:07,680
to extract something from this. So I disagree with the examples that the Lang Chen one provided.

636
01:02:07,680 --> 01:02:13,200
Okay, so I think if they improve this example, maybe theirs would work better. So let's take a

637
01:02:13,200 --> 01:02:19,120
look at what I did later. So I'm not a fan of Lang Chen, by the way. Lang Chen prompts are very

638
01:02:19,120 --> 01:02:28,000
worthy. So this is the other way that we can use the large language models to do the text encoding,

639
01:02:28,000 --> 01:02:33,280
to do knowledge graph embeddings. So this is called KGE, knowledge graph embeddings,

640
01:02:33,280 --> 01:02:37,120
is something like, you know, if you talk about the stuff like trans, these are like embeddings

641
01:02:37,120 --> 01:02:42,720
that we can give to the source, to the destination, to the relation. So we can represent the knowledge

642
01:02:42,720 --> 01:02:49,840
graph as embeddings. And we can use GPT or some large language model, okay, to generate some

643
01:02:49,840 --> 01:02:55,760
embedding space here that you can then use like MLP, multi layer perceptron, and so on to map to

644
01:02:56,400 --> 01:03:01,840
to the embedding space of the knowledge graph embeddings. So this is one way we can utilize

645
01:03:01,840 --> 01:03:07,600
large language models to do it. Yeah, I mean, I was thinking, you know, like, why not just use this,

646
01:03:07,680 --> 01:03:13,920
right? Why not just use LAM embeddings directly for knowledge graph?

647
01:03:16,080 --> 01:03:21,120
I mean, LAMs are way better than, than doing graph neural networks, in the sense that, you know,

648
01:03:21,120 --> 01:03:24,720
if you know the problems with graph neural networks, I'm just going to tell you the problems of graph

649
01:03:24,720 --> 01:03:30,960
neural networks now. Okay, they have these two problems. Okay, this is one is called over squishing

650
01:03:30,960 --> 01:03:36,160
or over squashing. And the other one is called over smoothing. Okay, what are these two problems?

651
01:03:36,160 --> 01:03:42,160
Over squashing is that the information, because you pass the information into an embedding layer,

652
01:03:42,160 --> 01:03:49,520
information gets lost at embeddings. Okay, so this over squashing thing is also a problem for

653
01:03:50,960 --> 01:03:56,080
LAMs. So I'm not going to cover too much on it. The other problem that we have for this kind of

654
01:03:56,080 --> 01:04:01,360
graph neural network over smoothing, okay, is that after you do message passing

655
01:04:02,000 --> 01:04:10,560
or too many times, all embeddings look the same. Okay, so this is a big problem. Okay, I also

656
01:04:10,560 --> 01:04:14,480
realized this, that once I did graph neural networks, you, like, you have two nodes, you pass

657
01:04:14,480 --> 01:04:18,720
information to each other, and then you become the average of the information, you keep doing this,

658
01:04:18,720 --> 01:04:24,160
right? Eventually, both nodes become the same, or very, very similar. Okay, so this is one of the

659
01:04:24,160 --> 01:04:30,960
huge problems of graph neural networks. And I feel like the embedding space that is best

660
01:04:30,960 --> 01:04:35,680
done, right, is not the way that we do message passing in graph neural network. We should just

661
01:04:35,680 --> 01:04:40,720
ground it in the context using a large language model. And large language model update the context

662
01:04:40,720 --> 01:04:46,080
quite well. Okay, then you can just use the embedding that is derived from that particular context

663
01:04:46,080 --> 01:04:50,960
in, like, you can just put something like that, like, you can just say context. And then like,

664
01:04:51,920 --> 01:04:57,520
I am a student or something like that. So, like, this context will update the definition

665
01:04:57,520 --> 01:05:02,880
of the student here. So you can go through the transformer module. So this is the transformer

666
01:05:02,880 --> 01:05:11,040
module. And then you can get the final embedding here. Yeah, at the final layer, right, before

667
01:05:11,040 --> 01:05:16,720
the softmax, you can actually use the transformer to get the embeddings already. Why use knowledge

668
01:05:16,800 --> 01:05:21,680
graph embeddings? Okay, so I'm just putting this question out here. So I hope those people

669
01:05:21,680 --> 01:05:26,320
knowledgeable in this area can come and, you know, correct me if I'm wrong. But I don't see a point

670
01:05:26,320 --> 01:05:33,840
in doing this. Yeah, right now. Okay, so let's leave it as that. And let's continue.

671
01:05:36,080 --> 01:05:41,040
Approach tree. So the approach tree is how we combine both approaches to make a very, very

672
01:05:41,920 --> 01:05:46,240
synergistic model where the large language model can generate the knowledge graph dynamically

673
01:05:46,720 --> 01:05:50,640
and this knowledge graph is something like a dynamic memory that gets updated as the agent

674
01:05:50,640 --> 01:05:55,600
explores the world and so on. And this knowledge graph can then inform the knowledge, the large

675
01:05:55,600 --> 01:06:02,240
language model and ground it in consistent generation. So let's see how this works. So you

676
01:06:02,240 --> 01:06:08,160
can see this is the diagram in the paper. And you can see like data. Okay, that's what me and Richard

677
01:06:08,160 --> 01:06:16,400
discussed. Data, okay, will be from different domains to embed them into latent space. Okay, so

678
01:06:16,400 --> 01:06:21,520
now we just assume that there's only one latent space but my view is that there's multiple latent

679
01:06:21,520 --> 01:06:28,480
spaces. Okay, so right now we just treat it as there's only one latent space. You process the

680
01:06:28,480 --> 01:06:32,960
information in that one latent space using knowledge graph and large language model in this

681
01:06:32,960 --> 01:06:38,960
loop. Okay, so knowledge graph can ground the language model in consistency. Language model

682
01:06:38,960 --> 01:06:44,400
can make the knowledge graph more expressive. Okay, and not as rigid as before. I mean, maybe you

683
01:06:44,400 --> 01:06:49,680
can use embedding based knowledge graphs like then you can make the knowledge graphs like express a

684
01:06:49,680 --> 01:06:55,360
lot of things more than just text alone. Okay, so this is one idea. You can use different techniques

685
01:06:55,360 --> 01:07:00,560
to process it like graph theory networks from engineering, representational learning. Yeah,

686
01:07:00,560 --> 01:07:06,560
I mean, this is just some big words, but the idea is you basically do some processing. All right,

687
01:07:06,560 --> 01:07:10,880
you can use large language models to process. Or if you like it, you can process it using the

688
01:07:10,880 --> 01:07:16,240
knowledge graph, which is like a graph neural network to process. Okay, or you can make the

689
01:07:16,240 --> 01:07:19,440
graph neural network into text and then you can do some neural symbolic reasoning.

690
01:07:20,240 --> 01:07:23,760
Okay, actually, this whole thing can be just summarized as neural symbolic reasoning because

691
01:07:26,000 --> 01:07:31,920
plus the knowledge graph equals the symbol spot. And then the large language model is the neural

692
01:07:31,920 --> 01:07:36,800
networks. So you can just summarize this whole thing as neural symbolic reasoning. All right,

693
01:07:36,880 --> 01:07:43,200
then you can use this for different domains. Right, I think this is a very, very exciting path

694
01:07:43,200 --> 01:07:48,960
that we should work on. Because right now with the power of language models, the knowledge graph

695
01:07:48,960 --> 01:07:54,560
can be very, very flexible. And it's not a typical knowledge graph anymore, it can be

696
01:07:54,560 --> 01:08:00,000
embedding based knowledge graph. And it can be context dependent knowledge graph. Okay, so

697
01:08:00,720 --> 01:08:04,720
I really hope to work on context dependent knowledge graph, because I think that's the future.

698
01:08:04,720 --> 01:08:08,640
Okay, not the traditional knowledge graph that you've seen everywhere in this presentation.

699
01:08:09,280 --> 01:08:13,200
Okay, the knowledge graph embeddings must be able to do, must be able to

700
01:08:14,160 --> 01:08:18,320
change based on the parent nodes. Okay, must be changed based on the context. And that's

701
01:08:18,320 --> 01:08:21,920
something that is not done right now, at least based on my own awareness. I don't think that's

702
01:08:21,920 --> 01:08:28,240
done right now. But that's very promising. Right, so one use case for this kind of system is fact

703
01:08:28,240 --> 01:08:33,760
checking. As you know, large language models cannot do very badly at fact checking. It tends to

704
01:08:33,760 --> 01:08:40,160
hallucinate a lot. And perhaps we can do like, a knowledge graph to like ground it in some facts

705
01:08:40,160 --> 01:08:48,240
like some Wikipedia entries. No, you can use this to ground the inference. Okay, by doing inference,

706
01:08:48,240 --> 01:08:53,360
you can then see whether or not like, is it, is there a path in the knowledge graph that matches

707
01:08:53,360 --> 01:08:59,760
it? Or you do knowledge grounded inference, like you say, you must only use this information

708
01:08:59,760 --> 01:09:04,880
that I extract for you in the knowledge graph and infer it. So this diagram here, unfortunately,

709
01:09:04,880 --> 01:09:09,920
did not do the inference step. Okay, because they are still using birds. Okay, they're using bird as

710
01:09:09,920 --> 01:09:17,840
a model. And what they did was they use the knowledge graph relations to do some pre training.

711
01:09:17,840 --> 01:09:24,080
So it's like they take additional, like, additional tech samples, they just mask out certain words

712
01:09:24,080 --> 01:09:28,880
based on the knowledge graph relations. And then they do the training here. So they just did the

713
01:09:28,880 --> 01:09:35,360
pre training using the knowledge graph to give additional examples. Okay, so what I want the

714
01:09:35,360 --> 01:09:41,280
thing to do is actually to do it during inference, if I cannot find any paper that does that so far.

715
01:09:41,280 --> 01:09:45,440
All right. So I think this inference is more important than the pre training, you know,

716
01:09:45,440 --> 01:09:49,360
this pre training, yes, it increases more data samples, because you can just mix and match the

717
01:09:49,360 --> 01:09:54,240
knowledge graph, get more sentences out. Sure, I give it to you. And in fact, they improve by two

718
01:09:54,240 --> 01:09:59,040
to three percentage points across soda benchmarks, this fact KB, you can go and check it out.

719
01:09:59,680 --> 01:10:03,920
All right. But what I'm more interested in is how you use it for inference, not for pre training.

720
01:10:04,560 --> 01:10:09,680
Okay, so let's see how length chain does it. All right, so now we come to the length chain part.

721
01:10:09,680 --> 01:10:13,360
So actually, length chain is quite advanced, because length chain has a lot of the ideas that I

722
01:10:13,360 --> 01:10:18,400
think should be done. All right, let's see how the length chain question answering graph question

723
01:10:18,400 --> 01:10:23,520
answering is done. All right, so we have four steps. First step, we generate the triples from

724
01:10:23,520 --> 01:10:27,760
the context. Okay, so we are like maybe a text context, you generate like the triples from it,

725
01:10:28,400 --> 01:10:33,760
like the knowledge graph triples, you generate some from the query, you generate some entity

726
01:10:33,760 --> 01:10:39,200
extraction. Okay, and then you use this at that entities to extract this relevant triples. Okay,

727
01:10:39,200 --> 01:10:44,400
later I'll show you the how what I mean by this. And then you use this relevant triples to answer

728
01:10:44,400 --> 01:10:48,640
the question. So I share with you these two documentation in case you want to see how length

729
01:10:48,640 --> 01:10:57,360
chain graph QA does it. So step one, okay, generate triples from context. So like this context, I just

730
01:10:57,360 --> 01:11:03,760
came out of it, right? Recently, my MacBook external camera in the viewing camera spot. So I'm

731
01:11:03,760 --> 01:11:09,760
actually using the external camera right now to talk to you. And yeah, so this example is for Apple.

732
01:11:09,760 --> 01:11:14,960
So let's assume that Apple created a new product called Mac and Cheese Pro, okay, in 2025. All

733
01:11:14,960 --> 01:11:19,600
right, and then like Apple gave the invented cheese, okay, a rousing ovation in 2026 after

734
01:11:19,600 --> 01:11:24,160
invented this in 2024. Right, there's also another company called Orange who created a competing

735
01:11:24,160 --> 01:11:28,800
product called the Orange and Cheese Pro. The price was slightly higher at 5000 compared to 4000

736
01:11:28,800 --> 01:11:34,880
from Apple. Okay, so this is a fictional example. Okay, and this is just to see like how good the

737
01:11:34,960 --> 01:11:39,680
context is stored in the knowledge graph. So you can see that, oh yes, Apple announced Mac and Cheese

738
01:11:39,680 --> 01:11:47,280
Pro, Apple gave cheese. So this kind of thing, right, like, is a bit contentious because like,

739
01:11:47,280 --> 01:11:51,520
what do you mean by gave cheese, gave what? So this one needs to be improved a bit.

740
01:11:52,400 --> 01:11:57,760
Apple ovation gave, Apple gave an ovation, okay, again to who? Right, so this one needs to be

741
01:11:57,760 --> 01:12:03,040
improved as well. Okay, the price of the MacBook Pro is 4000. Yes, Mac and Cheese Pro is already

742
01:12:03,120 --> 01:12:09,120
created. Orange and Cheese Pro, good. Orange and Cheese Pro, the price 5000. Okay, so you see,

743
01:12:09,120 --> 01:12:16,000
it's not bad. Miss out dates. All right, and then like some, some relations are ambiguous.

744
01:12:16,880 --> 01:12:21,280
So I don't quite like the way they did the triplet extraction and I think this is the

745
01:12:21,280 --> 01:12:25,920
downfall of the Graph QA. So if you are going to use Lang chain for Graph QA, my advice is don't

746
01:12:25,920 --> 01:12:30,800
use it. Okay, because you miss out a lot of stuff in the context. Okay, if you are interested

747
01:12:30,800 --> 01:12:34,960
how they generate the context, you can go back to my earlier slides that I was talking about.

748
01:12:35,680 --> 01:12:40,240
Yeah, so I mean, it's actually, let me show you, let me show you again the slides.

749
01:12:41,280 --> 01:12:46,160
It's this one, this is the one that they did, like, this is the problem to generate stuff from the

750
01:12:46,160 --> 01:12:52,640
text. Yeah, so the examples aren't very great and understandably the results aren't very great

751
01:12:52,640 --> 01:12:57,440
as well. All right, so this is the knowledge graph that's generated. You can see like Mac and Cheese

752
01:12:57,440 --> 01:13:04,400
Pro is cost $4,000 on price. Yeah, you can see that like stuff like price will contain like a lot

753
01:13:04,400 --> 01:13:11,760
of relation because like price is very generic. Okay, Apple announced Mac and Cheese Pro. Okay,

754
01:13:11,760 --> 01:13:18,240
so this is the knowledge graph that is generated. And we can see that like, next up we can use the

755
01:13:18,240 --> 01:13:24,160
Graph QA chain in order to run the chain and see the answer. And you can see that if I ask it the

756
01:13:24,160 --> 01:13:29,040
question like, when was the Mac and Cheese Pro announced? Okay, they couldn't find it. Okay,

757
01:13:29,040 --> 01:13:33,840
because after they passed through the context, okay, they abstract like when was the Mac and

758
01:13:33,840 --> 01:13:37,600
Cheese Pro, when did Apple announce the Mac and Cheese Pro? They abstract that in the query,

759
01:13:37,600 --> 01:13:42,400
there's only Apple and Mac and Cheese Pro. So they check through all the knowledge graph to make sure

760
01:13:42,400 --> 01:13:48,080
that you only have entities that match Apple or Mac and Cheese Pro. Okay, so I have a gripe with

761
01:13:48,080 --> 01:13:59,280
this thing. Like if you use exact text matching, what if there's a spelling error, capitalization

762
01:13:59,280 --> 01:14:09,360
error, or like related word, but not exact match. Yeah, so if you use exact text matching, which

763
01:14:09,360 --> 01:14:14,880
is what they did for a length chain, like what if you don't get the right match? Okay, so I don't

764
01:14:14,880 --> 01:14:21,040
quite like this approach. So yeah, this is something that I think could be improved on.

765
01:14:21,040 --> 01:14:25,840
All right, and you can see that if I ask it like, when did they announce the Mac and Cheese Pro?

766
01:14:26,640 --> 01:14:30,960
They couldn't answer. All right, because look at this knowledge graph here, there's nothing that

767
01:14:30,960 --> 01:14:35,200
talks about dates here. All right, so they miss out quite a huge chunk of information from the

768
01:14:35,200 --> 01:14:41,440
earlier context. So if we had fed in the earlier context directly, so I just use the length chain

769
01:14:42,400 --> 01:14:49,600
LmChain agent. Okay, so I'm only using the LmChain agent or this to just show you that length

770
01:14:49,600 --> 01:14:55,280
chain is not good. All right, I myself the new length chain. All right, so this is the idea

771
01:14:55,280 --> 01:15:00,720
that like, after a while, you know, this is the context and then, okay, so this is not bad. I

772
01:15:00,720 --> 01:15:04,000
mean, you could just do the same thing on ChatGBT, actually, you can just put like context

773
01:15:04,000 --> 01:15:09,280
question and then ChatGBT will give you the answer. All right, so this LmChain works and this shows

774
01:15:09,360 --> 01:15:14,720
that by embedding the text as a knowledge graph, it kind of miss out certain stuff. All right,

775
01:15:14,720 --> 01:15:19,840
and what are the stuff we miss out? We miss out the years and we also miss out like,

776
01:15:19,840 --> 01:15:23,760
Apple gave cheese. I mean, it doesn't make sense that way, right? I mean, look at the knowledge

777
01:15:23,760 --> 01:15:31,520
graph like, what am I trying to solve there? Apple gave cheese. Where is it? Apple gave cheese.

778
01:15:31,520 --> 01:15:35,760
Where's cheese?

779
01:15:39,280 --> 01:15:41,760
Apple Cheese gave. Is there a gave anywhere?

780
01:15:45,600 --> 01:15:50,640
Yeah, okay, I think this one maybe is the outdated, but the idea is that we can't really tell the

781
01:15:51,360 --> 01:15:58,000
main thing in this graph because we miss out some information. And that's one of the issues of

782
01:15:58,560 --> 01:16:02,560
converting text directly into knowledge graph is that you might miss out certain relations.

783
01:16:03,200 --> 01:16:06,400
And actually, if you think about it, if we want to embody all relations,

784
01:16:07,200 --> 01:16:12,960
there's just too many to embody, right? Yeah, it's too big to embody. So maybe the text itself

785
01:16:12,960 --> 01:16:17,760
is way more expressive than the knowledge graph, if you think about it that way. Okay,

786
01:16:18,560 --> 01:16:22,400
but again, you know, if you just use text only, you might face issues that, you know,

787
01:16:22,400 --> 01:16:30,720
your OpenAI embeddings might be too restrictive. It's too broad-based. You need the embeddings

788
01:16:30,720 --> 01:16:38,080
at different levels. So let's see how we would improve the Lang chain graph QA. I'm just using

789
01:16:38,080 --> 01:16:43,600
my strict JSON framework here, which just basically passes the system prompt and then outputs as a

790
01:16:43,600 --> 01:16:51,360
JSON in your own way. So I basically did what Lang chain does in a much shorter way. So I just

791
01:16:51,360 --> 01:16:55,440
say you are a knowledge graph builder. You extracted an object one, object two relation.

792
01:16:55,440 --> 01:17:00,080
Okay, I did not even put subject, object predicate. Okay, I mean, I just do like that. Okay, I just

793
01:17:00,080 --> 01:17:05,440
want it to be as vague and as generic as possible because I want to capture as much information

794
01:17:05,440 --> 01:17:10,080
as possible. Okay, so this was done in like 10 minutes. Okay, I don't really know whether this

795
01:17:10,080 --> 01:17:14,960
is the best. You all can feel free to improve it. Okay, I have the Jupyter notebook attached in the

796
01:17:14,960 --> 01:17:20,560
link. All right, so I gave you some examples like John bought the laptop. Okay, that's me,

797
01:17:20,560 --> 01:17:25,680
all right. John built the house in 2021. Okay, that's not me, all right. But this is the idea of

798
01:17:25,680 --> 01:17:30,720
like how we can represent like various relations like that. All right, then the output format is

799
01:17:30,720 --> 01:17:34,080
just a knowledge graph. So you can see like Apple announced my and cheese bro, my and cheese bro

800
01:17:34,080 --> 01:17:39,600
announced in 2025. Apple proved big hit. Okay, so again, this one is not exactly that great,

801
01:17:39,600 --> 01:17:43,520
because it's not really Apple that prove a big hit. It should be the Mac and cheese bro that

802
01:17:43,520 --> 01:17:48,160
prove a big hit. So this part needs to be from engineer a bit more. All right, Apple gave cheese.

803
01:17:48,160 --> 01:17:54,800
Okay, again, like this is not complete. Okay, cheese browsing ovation into zero to six. So

804
01:17:54,800 --> 01:17:59,120
actually we combine these two together. This is complete. So this is okay. All right, cheese

805
01:17:59,120 --> 01:18:03,120
invented man cheese bro. Okay, man cheese bro invented into zero to four. Okay,

806
01:18:03,840 --> 01:18:08,640
orange created orange and cheese bro. Yep, orange and cheese bro. The price is 5000 and

807
01:18:08,720 --> 01:18:14,800
Apple prices 4000. So again, here has some issues or so, like here, instead of saying that this is

808
01:18:14,800 --> 01:18:18,720
a Mac and cheese bro, because we should be referring to man cheese bro, it says Apple.

809
01:18:19,440 --> 01:18:24,960
Okay, so unless we can sort of like link this later to Apple announced, okay, this part here.

810
01:18:26,720 --> 01:18:31,840
So now you can see some issues with knowledge graph expressing stuff. It is not clean. All right,

811
01:18:32,640 --> 01:18:38,960
it might truncate the information halfway. So this one needs more study as to how we can

812
01:18:38,960 --> 01:18:43,760
express this in the knowledge graph better. But by expressing it in the knowledge graph,

813
01:18:43,760 --> 01:18:48,960
you are able to then do knowledge graph passing, okay, and extract out the relevant entities that

814
01:18:50,320 --> 01:18:54,880
related to the prompt. And you know, this is like, if you think about it, this is like

815
01:18:54,880 --> 01:19:00,480
doing segmentation across like every few words in the segment one time. Yeah, so this is the

816
01:19:00,480 --> 01:19:06,000
generated graph of what I did for strict design framework. You can see that compared to Lang chain,

817
01:19:06,000 --> 01:19:14,960
this is what happens like we have way more relations, there's more relations here. And dates are captured.

818
01:19:17,200 --> 01:19:22,160
Yeah, so this is something that I think needs to be investigated more. Mine is not the best,

819
01:19:22,160 --> 01:19:26,800
but Lang chain is definitely not good. Okay, so this is something that needs to be done more if

820
01:19:26,880 --> 01:19:32,240
we want to extract stuff out into the knowledge graph. And then like, should we use embeddings?

821
01:19:33,840 --> 01:19:41,280
So if you want to use embeddings, then we cannot just use OpenAI API. Maybe you need to use like

822
01:19:41,280 --> 01:19:48,400
Lama2. Okay, although Lama2 perhaps is not that great or so, because Lama2 is

823
01:19:50,000 --> 01:19:55,440
not that good for multilingual. Okay, but Lama2 is the best possible substitute for

824
01:19:56,240 --> 01:20:00,800
chat GPD right now. So maybe you can construct a knowledge graph embeddings using the Lama2 embeddings.

825
01:20:01,840 --> 01:20:08,960
So food for thought. Next, we have this flexible knowledge graph passing. Over here, what I decided

826
01:20:08,960 --> 01:20:17,440
to do is that we want to output only relations that are relevant to the question. And I just

827
01:20:17,440 --> 01:20:23,040
passed in the entire knowledge graph here. So instead of coming up entities, I just asked it

828
01:20:23,040 --> 01:20:29,360
to go through the entire knowledge graph because in case of words not exact or spelling errors,

829
01:20:31,120 --> 01:20:40,000
GPD is able to catch it most of the time. I must copy it because GPD is not as great as

830
01:20:40,000 --> 01:20:44,720
doing like counting letters and stuff. But if you misspell your words, but the meaning is about

831
01:20:44,720 --> 01:20:50,000
that, GPD is able to extract the right entities. And here we can see that we asked it like,

832
01:20:50,000 --> 01:20:54,880
when did Apple announce the man cheese bro? It captured exactly what we want. All right.

833
01:20:56,000 --> 01:21:02,080
And this is the graph that is the past knowledge graph. So I'm talking about when you query the

834
01:21:02,080 --> 01:21:05,360
knowledge graph, you pass it so that only relevant sections of the knowledge graph gets

835
01:21:05,360 --> 01:21:11,840
come out, gets extracted. You ground this extracted part onto your text. Okay. And then

836
01:21:12,640 --> 01:21:18,960
you can get the answer here. So 2025. So I just shown that like using this strict JSON format,

837
01:21:18,960 --> 01:21:23,600
you are able to like, it's very flexible. You just need to key in the system prompt,

838
01:21:23,600 --> 01:21:27,440
key in the user problem and output the format in terms of whatever JSON labels and the

839
01:21:27,440 --> 01:21:32,800
description of the JSON. So I've been using this for a lot of my own use cases. And I'm just

840
01:21:32,800 --> 01:21:37,680
adapting this for the knowledge graph. But this is really cool because you can then use this

841
01:21:38,240 --> 01:21:42,560
past knowledge graph, like this idea of generating the knowledge graph and passing the knowledge

842
01:21:42,560 --> 01:21:50,560
graph. You can use the knowledge graph as memory. Okay. And then you can update memory.

843
01:21:51,120 --> 01:21:58,240
And you can use updated memory to extract relevant parts. Okay. So this called retrieval.

844
01:21:59,040 --> 01:22:05,520
Okay. Use relevant parts to solve problem. So I really like this framework because this

845
01:22:05,520 --> 01:22:11,840
knowledge graph as memory thing is something quite interesting. But how can we express it as memory?

846
01:22:11,840 --> 01:22:19,360
That's the difficult part. Okay. So Richard asked, have I tried putting the graph into an FAIS

847
01:22:19,360 --> 01:22:27,040
index? No, I haven't. But how will you do a knowledge graph putting onto the, like onto that

848
01:22:27,040 --> 01:22:32,480
index? Because usually what I know is that you do the embedding and then you put the text. That's

849
01:22:32,480 --> 01:22:38,480
for the retrieval or method generation. If you're doing knowledge graph, maybe you put the source

850
01:22:38,480 --> 01:22:43,760
as the index. Okay. I'm not too sure. I'm going to check on this. Like how will you do this into

851
01:22:43,760 --> 01:22:48,800
PyCon and stuff like that? But what I can imagine you doing for the knowledge graph is just put the

852
01:22:48,800 --> 01:22:54,960
whole thing into some array and then just store the array. I mean, you can even put it as a JSON.

853
01:22:54,960 --> 01:23:05,120
Yeah. So yeah. Okay. I don't have time to cover through the running of the Jupyter Notebook.

854
01:23:05,120 --> 01:23:10,400
I'll just upload that separately. It's another video. But let's just go through like the last

855
01:23:10,400 --> 01:23:14,400
five to 10 minutes. I'm okay to extend about 15 minutes if you all have more things to discuss.

856
01:23:15,120 --> 01:23:20,080
Like we have discussed like how can we use knowledge graph better for last language models?

857
01:23:20,080 --> 01:23:24,880
So first question, what are the failure modes of using knowledge graph for context representation?

858
01:23:25,840 --> 01:23:32,960
And I think this failure mode is mainly like your knowledge graph may not capture all information.

859
01:23:35,200 --> 01:23:37,680
Okay. And also the knowledge graph capturing

860
01:23:40,320 --> 01:23:45,920
might truncate the information. So maybe using text directly

861
01:23:48,000 --> 01:23:54,560
may be better. Okay. But harder to pass. Because if you are using text directly, you don't really have

862
01:23:56,400 --> 01:24:02,160
like nice sections where you can pass the knowledge graph on. Yeah. So these are some of the things

863
01:24:02,240 --> 01:24:05,440
about right now, some of the failure modes of this knowledge graph.

864
01:24:06,560 --> 01:24:07,840
Anyone else has anything to add?

865
01:24:09,440 --> 01:24:18,480
Just some random thoughts. Do you think it makes sense if we view the embedding space

866
01:24:18,480 --> 01:24:21,360
itself as a form of generalized knowledge graph?

867
01:24:23,680 --> 01:24:27,440
Embedding space as a genera. You mean the LM embedding space?

868
01:24:27,440 --> 01:24:35,520
Oh, I mean, I mean, yes, but for some specific tasks you want to do, you can train a dedicated,

869
01:24:36,080 --> 01:24:46,800
a separate dedicated embedding space. So because like you have all your entities inside their space

870
01:24:47,520 --> 01:24:52,960
and the relative, I don't know, relative positioning of them kind of encode certain

871
01:24:53,200 --> 01:24:58,000
relative information of them, right? Because I think the issue you mentioned here is,

872
01:24:58,960 --> 01:25:06,640
I think it's just the graph, learn graph can be too sparse, right? You lose a lot of information.

873
01:25:07,600 --> 01:25:14,800
But if inside embedding space, I don't know, it might help preserve more information,

874
01:25:14,800 --> 01:25:19,040
although not very explicit information. It's just some random thought.

875
01:25:19,920 --> 01:25:24,640
Ah, okay, I get what you mean. Like you encode knowledge graph as embedding space,

876
01:25:25,360 --> 01:25:32,720
so like your source relation and the output are all embedding space and sink in destination.

877
01:25:34,640 --> 01:25:42,240
I feel like my gut feeling is it can be much richer than just a typical traditional graph.

878
01:25:43,120 --> 01:25:45,120
Definitely.

879
01:25:47,520 --> 01:25:51,280
Now I kind of agree with you. It's just going to be hard to express the embedding space

880
01:25:51,920 --> 01:25:57,040
using an OpenAI API. You might need to have access to the last average model directly if you want to do this.

881
01:25:59,600 --> 01:26:02,880
Yeah, but definitely that's one of the ways that we can represent knowledge graphs.

882
01:26:04,880 --> 01:26:10,560
Anyway, this is the second question also. Should we utilize the embedding space? Perhaps

883
01:26:12,320 --> 01:26:18,880
for more expressive knowledge graph. Okay, but then if you think about like what I was talking about earlier,

884
01:26:19,760 --> 01:26:20,640
context dependent

885
01:26:23,920 --> 01:26:27,680
embeddings. If you are talking about context dependent embeddings, actually we can use

886
01:26:27,680 --> 01:26:35,840
LLM to pass an update embeddings based on the parents of the node.

887
01:26:36,720 --> 01:26:41,280
Yeah, so I was thinking of something like that. Like you can actually have a very, very

888
01:26:43,440 --> 01:26:48,400
different interpretation of a certain word. Like for example, bank can be river bank or financial

889
01:26:48,400 --> 01:26:53,280
bank, depending on like the context of it. If you are talking about river side, then it's like river bank.

890
01:26:53,280 --> 01:26:58,160
Yeah, so you can actually use the last language model, extract out the hierarchy of the graph,

891
01:26:58,160 --> 01:27:03,680
the front part of the graph. You can put it there and you can then pass the embeddings accordingly.

892
01:27:04,560 --> 01:27:08,800
So I'm still thinking that perhaps just using the last language model embedding directly

893
01:27:10,160 --> 01:27:13,680
might be a better bet. And then you can just maybe

894
01:27:15,520 --> 01:27:20,880
use last language model to do this context thing. And then you can put this embeddings inside

895
01:27:20,880 --> 01:27:26,320
your knowledge graph. Like what you said earlier. If there's a way to get the embedding space

896
01:27:26,320 --> 01:27:30,800
directly from the OpenAI API, that would be great. But if not, we might have to use

897
01:27:30,880 --> 01:27:33,680
Lama2 in order to do this embedding space knowledge graph.

898
01:27:36,320 --> 01:27:39,600
But then again, is it really necessary? Can we just use text?

899
01:27:39,600 --> 01:27:44,240
So this is a big open question. Should we use embedding space for the knowledge graph?

900
01:27:44,240 --> 01:27:50,080
Or can we just represent it as text and then use the LM to generate embeddings after that?

901
01:27:50,880 --> 01:27:56,480
So I leave that as an open question. I think both approaches are valid approaches. I just feel like

902
01:27:56,480 --> 01:28:01,120
the way to input the knowledge graph as text is it will be much more interpretable. And also

903
01:28:01,120 --> 01:28:06,320
you only need to train one embedding space, which is the LM embedding space. So I kind of prefer that.

904
01:28:10,240 --> 01:28:12,080
Anyone else has any things to add?

905
01:28:16,400 --> 01:28:20,640
Okay, if not, we go to the next question. Can LMS help with a more flexible

906
01:28:20,640 --> 01:28:25,760
interpretation or construction of a knowledge graph? Okay, so our answer first. I think yes,

907
01:28:25,760 --> 01:28:35,440
definitely. Just like compared to like spacey or like on noun, verb, pro verb, those kind of

908
01:28:35,440 --> 01:28:44,480
stuff, like you are doing like the parse tree, compared to those very, very flexible. And you

909
01:28:44,480 --> 01:28:49,040
are able to extract a lot more information. So like just based on the string, some prompt I

910
01:28:49,040 --> 01:28:54,560
showed you earlier, you just hit object relation object, it captures almost everything. And that's

911
01:28:54,560 --> 01:28:59,360
zero shot prompting. Granted, it did not capture the date at first. I had to use the examples to

912
01:28:59,360 --> 01:29:05,520
give you the date. But compared to using this kind of like spacey and so on, like deep learning

913
01:29:05,520 --> 01:29:11,440
approaches, like you'll take quite long to train a new kind of like knowledge graph constructor.

914
01:29:11,440 --> 01:29:15,120
But with large language model, you can just use prompt engineering and get your knowledge graph

915
01:29:15,120 --> 01:29:22,400
out. I think that's very exciting. Okay, last question. How do we know what nodes are important

916
01:29:22,400 --> 01:29:26,720
to construct in the knowledge graph? Okay, because there's a lot of information, but not

917
01:29:26,720 --> 01:29:34,480
everything is needed for your use case. How do we know? Okay, so my opinion, okay, my opinion is this.

918
01:29:36,000 --> 01:29:43,520
You need to have biases based on the domain. And what are these biases? Maybe you can have

919
01:29:43,520 --> 01:29:51,360
multiple biases. Okay, and then let's just choose the right biases later. So this is my idea of

920
01:29:51,360 --> 01:29:56,880
intelligence right now. Okay, I'll share with you. Okay, this idea of intelligence is that there's

921
01:29:56,880 --> 01:30:02,160
not just one abstraction space where you store your information, you store them in multiple

922
01:30:02,160 --> 01:30:08,800
abstraction space. How do we get all these abstraction spaces? We basically just do rule

923
01:30:08,800 --> 01:30:13,440
based abstraction, like you like maybe one domain is saying that oh, dates are important. So I store

924
01:30:13,440 --> 01:30:18,080
the dates. Another domain is like all people, all person's names are important. I store the person's

925
01:30:18,080 --> 01:30:22,880
names. Then maybe another domain will be like all places are important. I store the places.

926
01:30:23,680 --> 01:30:28,720
Then when you want to solve the problem, okay, you will see which space is the best

927
01:30:28,720 --> 01:30:32,880
for your problem. Okay, you will just like, maybe you look at all the abstraction spaces,

928
01:30:32,880 --> 01:30:37,360
maybe combine two or more, or you just take one, and then whatever solves the problem works.

929
01:30:38,000 --> 01:30:45,920
So this would form an approach that will be used later on. So if you think about it,

930
01:30:45,920 --> 01:30:50,400
I'm just going to draw it here. Okay, I don't know whether I have space to draw it, but if you

931
01:30:50,400 --> 01:30:54,800
look at the top right of the screen, okay, you have a problem, you have multiple abstraction

932
01:30:54,800 --> 01:30:59,200
spaces, let's call this A. You have another abstraction space here, let's call it B.

933
01:31:00,720 --> 01:31:05,680
And we have another expression space, we call it C. All right, so if we have three

934
01:31:05,680 --> 01:31:11,200
abstraction spaces like that, okay, these are like three ways of doing it, three ways of doing the problem.

935
01:31:11,520 --> 01:31:20,000
And then in order to solve any arbitrary problem later, you just take mix and match the abstraction

936
01:31:20,000 --> 01:31:26,480
spaces to solve the problem. So yeah, increasingly, I've been feeling like this is the way to do things.

937
01:31:27,040 --> 01:31:39,920
So yeah, I also use this for my abstraction reasoning copless paper. So this is the idea

938
01:31:39,920 --> 01:31:43,600
that I have right now. You have different abstraction spaces. All these are rule-based.

939
01:31:44,320 --> 01:31:48,400
Okay, we don't really have deep learning here, because if you have deep learning, you'll have

940
01:31:48,400 --> 01:31:53,680
problems in getting a fixed abstraction space. You don't want the abstraction space to change.

941
01:31:54,240 --> 01:31:57,600
Because if you change this abstraction space, you have to change whatever you learn on it.

942
01:31:58,160 --> 01:32:02,400
It's like if I suddenly told you that math, the addition is now subtraction,

943
01:32:02,400 --> 01:32:06,480
that I have to relearn all my math again, because I need to update that new knowledge.

944
01:32:06,480 --> 01:32:10,800
So I'm saying that the basis is fixed, but then you just choose the right basis to solve it.

945
01:32:11,440 --> 01:32:16,080
Okay, then you might ask me, if we do it like that, what if we don't have the right basis to

946
01:32:16,080 --> 01:32:21,440
solve the problem? Okay, then the answer is you can't solve the problem. Okay, which might sound

947
01:32:21,440 --> 01:32:26,880
a bit crude to people, but I feel like we can't solve everything. Like even humans, we have our

948
01:32:26,880 --> 01:32:33,280
limitations. It's just that we work around our limitations and try to use our existing biases

949
01:32:33,360 --> 01:32:37,120
to solve new problems. And I think that's intelligence. We don't really need to change

950
01:32:37,120 --> 01:32:41,360
the abstraction spaces. We can just work with getting multiple abstraction spaces and then

951
01:32:41,360 --> 01:32:47,440
just combining them. So I shared a bit about my view of intelligence here. And yeah,

952
01:32:48,560 --> 01:32:53,760
that's more or less it for the questions to ponder. Anyone has anything else to add for any of these

953
01:32:53,760 --> 01:32:59,760
questions? I just want to clarify one thing. I'm still not very sure about the motivation here.

954
01:32:59,760 --> 01:33:08,080
So why we want this knowledge graph? Why use a knowledge graph, is it?

955
01:33:10,160 --> 01:33:14,400
Because the internal representation of LLMs is already

956
01:33:15,200 --> 01:33:31,760
already richer than a knowledge graph. So for example, you can just use the

957
01:33:32,720 --> 01:33:40,400
last language model to help you to break down the task. Actually, because the way I did it before

958
01:33:40,400 --> 01:33:45,360
is I want to specifically construct the knowledge graph first. Then I use that knowledge graph to

959
01:33:45,360 --> 01:33:51,440
break down the task from a hierarchical or sequential learning of intermediate

960
01:33:51,440 --> 01:33:58,160
goal to RL. But in the sense, you don't really need that particular task at least,

961
01:33:58,160 --> 01:34:02,000
you don't really need to do that anymore because you just query the last language model.

962
01:34:03,920 --> 01:34:09,360
At least the common sense way of breaking down a certain task. So you already have the

963
01:34:10,160 --> 01:34:18,480
different sub tasks. So because my understanding is more coming from that perspective,

964
01:34:18,480 --> 01:34:22,560
so I'm thinking why do I still need the knowledge graph?

965
01:34:24,400 --> 01:34:27,920
I mean, as what I said in one of the first few slides, if you use retrieval or method

966
01:34:27,920 --> 01:34:32,880
generation, you might miss out certain chunks of text. Knowledge graph provides a sort of hierarchy

967
01:34:32,880 --> 01:34:37,600
that you can pass over and extract information. So it provides the structure.

968
01:34:38,560 --> 01:34:43,920
But you still mentioned the problem is like even with knowledge, you have to

969
01:34:43,920 --> 01:34:50,240
distill information. So there is inevitable that certain information are lost. So it be

970
01:34:50,240 --> 01:34:56,240
it in a way of like you do chunk first, chunking first, then you summarize each chunk.

971
01:34:56,240 --> 01:34:59,760
Then you summarize again, maybe like a hierarchical way of summarizing and all

972
01:35:00,480 --> 01:35:08,320
even your knowledge graph, you only extract that the relation you think is important between

973
01:35:08,320 --> 01:35:15,760
different entities. So you face this issue of having information lost.

974
01:35:20,960 --> 01:35:25,760
Yeah, so maybe the correct way is not the existing kind of knowledge graph,

975
01:35:26,640 --> 01:35:31,440
but a new kind of knowledge graph. But I do believe a graph based representation of knowledge is

976
01:35:31,440 --> 01:35:36,400
important because when you learn new things, we typically try to fit in with our existing

977
01:35:36,400 --> 01:35:40,800
knowledge and we build on the knowledge from there. So if you have some form of graph structure

978
01:35:40,800 --> 01:35:46,240
to represent knowledge, you can actually like use that for learning as well. And that's where

979
01:35:46,240 --> 01:35:55,440
I'm coming from. My intuition is it can be okay. So in that case, I can see how it can be

980
01:35:55,440 --> 01:36:01,520
useful. It can be served as a heuristic for search. So if you want to understand a very

981
01:36:02,400 --> 01:36:10,480
large chunk of text, if you extract like a rough graph representation of the information,

982
01:36:10,480 --> 01:36:17,040
then you just do a heuristic search based on that. So even if this information loss is still can

983
01:36:18,000 --> 01:36:22,880
expand upon just based on your existing knowledge graph, maybe it just gave you

984
01:36:23,520 --> 01:36:27,600
some useful signal to tell you like which part of the text you want to do some search,

985
01:36:27,600 --> 01:36:31,920
then you can just go and search. You don't need necessarily to stick with the strictly stick

986
01:36:31,920 --> 01:36:36,800
with the graph. So in that way, like just a heuristic but it's a useful one.

987
01:36:37,600 --> 01:36:43,520
Yeah, yeah. That's one way of doing it, like using a heuristic to search. So it's like replacing

988
01:36:43,520 --> 01:36:48,080
the cosine similarity in retrieval of mental generation. You just pass through the knowledge

989
01:36:48,080 --> 01:36:54,960
graph. Yeah. Because the chunking in like just naive way, it's just, it's just like because the

990
01:36:54,960 --> 01:37:04,720
way we structure essay or structure the text is not necessarily just like each different

991
01:37:05,760 --> 01:37:11,120
hierarchy like different object or different something that is quite intricate, right?

992
01:37:11,120 --> 01:37:16,640
If for example, for some novel, you have foreshadowing, you have different way of writing,

993
01:37:16,640 --> 01:37:21,680
you have plot device, then you suddenly at a certain very, you think apparently very random

994
01:37:21,680 --> 01:37:26,160
point suddenly become very important. So just by naively chunking into like evenly

995
01:37:27,840 --> 01:37:33,280
it doesn't, it doesn't work. But if you have a graph like sort of tells you this kind of structure,

996
01:37:33,280 --> 01:37:39,280
then you're based on that to do some similarity. Definitely I feel like it can be more efficient.

997
01:37:39,280 --> 01:37:46,240
So I think they give you a better way of doing chunking. Yeah. Yeah, I like this approach. In

998
01:37:46,240 --> 01:37:52,000
fact, that's one of the motivations of using knowledge graph as well in order to find a better

999
01:37:52,000 --> 01:37:58,720
way to pass it. I mean, we can pass through graphs pretty well. So yeah, we can perhaps get better

1000
01:37:58,720 --> 01:38:03,040
retrieval using knowledge graphs. So that's one provided you have all the information in your

1001
01:38:03,040 --> 01:38:07,360
knowledge graph, you can get better retrieval using knowledge graphs provided the first part

1002
01:38:07,360 --> 01:38:12,480
again, because that's the failure case we saw just now in the length chain graph answering agent.

1003
01:38:12,720 --> 01:38:18,800
But anyway, I think all the logition all stand from the fact that the context window is very limited.

1004
01:38:18,800 --> 01:38:27,200
So if we can solve that problem, then here actually, I mean, both way either way.

1005
01:38:27,200 --> 01:38:31,680
Yeah, I'm quite excited about this. In fact, I will spend like the next few weeks trying to

1006
01:38:31,680 --> 01:38:36,560
create a new knowledge graph. So I'll share with you after I create it, because there needs to be

1007
01:38:36,560 --> 01:38:41,120
some context dependent passing. And that's lacking right now in the knowledge graph that I see in

1008
01:38:42,080 --> 01:38:48,000
so far. You agree with the context dependent passing, right? Like how you interpret a certain

1009
01:38:48,000 --> 01:38:52,960
like note actually depends on the parents or depends on the position in the graph.

1010
01:38:54,800 --> 01:38:59,840
Yeah, even from a very traditional perspective, this very crucial as well.

1011
01:39:02,960 --> 01:39:08,240
Nice. Okay. Last minute or so anyone has any last points you want to add?

1012
01:39:12,080 --> 01:39:17,840
Okay. If not, thanks for coming. And yeah, if anything, you can still reach out to me on Discord

1013
01:39:17,840 --> 01:39:24,800
or LinkedIn. Yeah. And I'm looking forward to do this linkage between like large image models

1014
01:39:24,800 --> 01:39:29,760
and knowledge graph. This is what a lot of people call a neural symbolic. And I think this will be

1015
01:39:29,760 --> 01:39:36,080
very crucial for intelligence. And I can see how we can use this knowledge graph approach to like

1016
01:39:36,160 --> 01:39:41,360
learn stuff from the environment and use it in a learning agent. Like I like to call it reinforcement

1017
01:39:41,360 --> 01:39:45,680
learning agent, but it's not really reinforcement learning because there are no rewards. Okay. You

1018
01:39:45,680 --> 01:39:51,680
can just learn directly from knowledge in the memory itself. So I think this will be very crucial

1019
01:39:51,680 --> 01:39:55,760
for that kind of framework. And yeah, hope to share more of your after experiment with it.

1020
01:39:55,760 --> 01:40:01,760
Okay. If not, yeah. Thanks for coming. And I'll see you around. Okay. Bye, friend.

