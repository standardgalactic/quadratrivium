{"text": " Hi everyone and welcome to today's session. Today we'll be talking about a very exciting topic which tries to merge large language models and knowledge graphs together. So as you all already know, large language models are the recent hype. You can literally do anything or a lot of things with large language models by just changing the prom. It is very, very flexible. Get knowledge graphs extremely rigid. You have notes connected to other notes in relations, but they are also very informative because the relations don't change. The notes don't change. The large language models, one problem that they face is that they are a little stochastic. They tend to generate things that may not be grounded in facts. So it seems like naturally these two approaches seems like a good fit together. One is more flexible, which is the large language models. And one is more reliable, like the knowledge graph. Okay, so without further ado, let's begin today's topic. So I will roughly follow the framework of this paper called Unifying Large Language Models and Knowledge Graphs, a roadmap. Okay, this is by some IEEE fellows and senior members. I quite like the style of this paper, but I feel like a lot of the things that are surveyed in the paper are not exactly the latest large language model stuff. They are like the births, raw births, like basically the 2017 to 2019 era, that kind. So I have supplemented it with some of the more recent advancements, like some length change stuff. So enjoy. Do feel free to comment anytime, because I think this is a very interesting field that can be expanded upon. Never before have we gotten large language models this powerful like chat GPD. And this is really something that we can look at to improve on traditional methods or even think of a new method that is not even a knowledge graph. Okay, later I'll share with you some ideas. Okay, so what are the pros and cons of knowledge graphs and large language models? So as I said earlier, you can look at the rough summary. I think this is quite a good summary. Okay, large language models, they are generalizable. Okay, they possess a lot of knowledge. All right. But what they lack is that they lack some form of understanding or facts. Okay, general language understanding. Okay, this one is debatable, because like GPT-4 can be said to understand language pretty well. Like NLU is like the ace most of the task there. Okay, so this one may be not so true in understanding, but for facts-wise, fact generation is still a problem right now. Okay, incompleteness. Okay, maybe I mean like sometimes they might generate things that don't answer the question fully. Okay, but increasingly this is not really a problem anymore. Okay, it's more of like the reliability right now. So I summarize this part here, reliability. Okay, I should use a different color. Let me just change my annotation. So I think the main thing that large language models lack are reliability. Oh no, it's the same color. Let me see. Reliability and consistency. This too. I mean, y'all have tried large language models before, right? You key in the same prompt. Okay, sometimes you get the different responses. Sometimes the response can be different. Like I said, should, it's a hot day today, right? Yeah, scrims on this can be yes, sometimes no, you know, that kind of thing. All right, so knowledge graphs, what do they have? Knowledge graphs have structural knowledge. They are quite accurate. Okay, decisive, I guess, if you can find a way to like connect an input node to an output node, you can say yes, there's a link between them. It's very interpretable. Okay, actually large language models are also quite interpretable. So it's not really a con here. Large language is both actually interpretability is also in large language models. Domain specific knowledge. Yes, but actually, if you think about it, large language models with context grounding also has domain specific knowledge. Okay, evolving language. This is something that is quite interesting. Large language models don't really have this evolving language unless you fine tune it. Maybe the recent Lamato you can fine tune on something. Okay, but you can also use something like a knowledge graph to ground the large language models. Can you see the synergy here? There's a lot of things that knowledge graphs do well, that is not exactly antagonistic or not exactly different in nature from the large language model, it can just be used to ground the large language model. So it's very interesting. So what are the cons of large language models? They hallucinate, black box, black domain specific knowledge. So it looks like there can be some synergy here. And let's explore how we can synergize these two approaches. Before we move on any quick questions so far. Okay, so this is the one way of getting context into a large language model and is used very often nowadays. It's called retrieval augmented generation. So this is the raw format you retrieve from a corpus of documents. You have a few documents here that you can retrieve from. And then maybe the user asks like, how much is a MacBook Pro? Right, recently I need to ask myself this question because I'm considering whether I should buy another one. So you know, they retrieve the relevant documents like, okay, this document is about MacBook. Okay, you retrieve the right documents. Okay, this document here is about maybe 2019. You can retrieve the right documents. All these documents will actually be your context over here. So you could have the context retrieves like that. MacBook Pro 2019 costs 5000 or something like that. Then you can have like in 2019 Apple release MacBook Air 2019. Of course, I mean, I don't really know the details, but let's say these are the two documents you retrieve from your retrieval augmented generation. Okay, and after that, you ask the question like, how much is a MacBook Pro 2019? So it's been shown that if you use retrieval augmented generation, you can improve the consistency of the output of the large language model because you are grounding it in the earlier context, which is this part here, you are grounding it in this part here. So there's an element of grounding and this is very important for a lot of real world use cases. Because if you don't ground it, you can end up with quite nonsense generations. Alright, and just as a refresher, okay, what is the most common method used to select the top K like documents? Anyone can just blow talk? What's the most common metric to select the most relevant documents? That's a test of understanding. If you are doing retrieval augmented generation, what is the most common metric used to retrieve documents? To check the similarity. Anyone? You can write in your chat also. Dot product, yes, very good. Dot product or cosine similarity. That's right. So usually we use some form of embeddings. You embed your documents into a vector and then you use cosine similarity to check how similar the document is compared to the query. Okay, I'm going into some details over here. Okay, because actually this whole process of doing retrieval augmented generation and passing over knowledge graphs is very, very similar. Alright, in fact, you could even replace this retrieval augmented generation with like knowledge graph augmented generation is perfectly, I think it's replaceable. Alright, so this is some idea of how large language models can be made to be more accurate. Okay, using something like that. Okay, so this again, I just highlight the problems of large language models. Okay, may not be able to recall the knowledge, but you can retrieve the right context using this retrieval augmented generation provided you can retrieve the context currently. Alright, so this is a real world use case issue. Alright, I've talked to some people and they say that retrieval augmented generation with just the cosine similarity alone, okay, might not give you the right documents. So, you know, the embedding vectors train using contrastive loss, you know, they may not capture everything, especially if your document is very, very large. Okay, imagine you have only one vector to represent the entire document, and you have another vector to represent document, another document. So this is like document A, and another vector to represent document B, then you see how similar they are. But what about like, what if one document contains many parts? Right, I mean, each of these parts could have different meanings, right? Each of these sub parts could contain like, let's say you have this document could have a sub part that is like that, a sub part that is like that, a sub part that is like that. You know, they just aggregate all of this together into one vector, like that. Can you see that you're actually losing like information here, which means that when you retrieve something, let's say if I want to find out how to code, like a length chain question answer agent, you know, I'm not going to retrieve this vector because by vector similarity, my query is here. Alright, by vector similarity, maybe I'll retrieve a document that is like B instead, because like maybe it's nearer in terms of cosine similarity. Okay, I mean, it's greater is the opposite direction. Let me just make this vector look more similar to that. Like, let's say I have B is like that. So if this direction here is like how to code a length chain, QA agent, and this is the embedding vector for it, it goes in this direction. You know, you're not going to retrieve document A, although it contains that part over here. Okay, you're going to retrieve document B. This is one of the failings of the embedding vector. It just tries to capture the whole document into one vector. And this means that you may not be able to extract stuff out. Okay, Richard said something. This is why I keep saying context is king. Summarization is essentially impossible on segmentation on segmented documents. Yeah, definitely, because you summarize you lose information. Okay, so there needs to be like different hierarchies of how you retrieve things out, broad level, specific level. And you know what? Knowledge graphs might actually have that kind of hierarchy formulation. I'm actually jumping a few slides ahead, but give you an idea of why I'm so excited about this idea. All right. So actually some of the bypass that I've been telling people, I've been advising people is that like if you cannot get retrieval of mental generation to work, consider using like filters or like labels. So like this labels will say like, okay, maybe it's like product A, product B. So you know, instead of relying on just the retrieval of mental generation, or the embedding vector to actually embed the right knowledge, like let's say you have a length chain QA agent, I can tag this thing as a length chain QA agent inside this document. So there will be certain tags that you can have. So then you can then do like the embedding vector across the documents that have this text. Yeah, so maybe that's one way to like do a first hand filtering. Yeah, I mean, this is just like some, what do you call it, some bypasses to the downsides of embedding vectors for pros and similarities. So these are some ideas that could be done right now to bypass it. But if we have a way to use knowledge graph to do more broad level to more specific level extraction, maybe you don't even need all this, you can just pass through your knowledge graph and you can use that to ground the large language model. All right, so this is my last point here, knowledge graphs are useful to retrieve the right context, search the right keywords, retrieve the right subgraph. Like let me give you an example here, if let's say I have a graph like that. All right, so maybe this is a graph talking about like people who view Netflix. Okay, so these are the Netflix user graphs. So these are the users. And then maybe you have me over here, John. Okay, and then like the movies are watched. I like to watch the flash, the series, not bad. I highly recommend it. Then maybe we have another guy like maybe Richard can be here, watch other movies, like movie A and movie B, you know, yep. So if you want to like extract something out here, you can just search like for keywords and then you can just put this whole subgraph here. And then you can use this part here. Okay, how you want to pass it into the large language model, I leave it for future investigations. There are a few ways to do it. I will cover some ways today. So if you can pass this into the large language model, essentially, you can ground the LM in context of the knowledge graph. And then we can actually do this grounding at a more higher level grounding or more sub level grounding depends on which height of the knowledge graph you're going to take the notes from. All right, so I think this is a very exciting prospect. And yeah, I'm looking forward to see like how this can actually work. So I'll be actually working on getting this to work the next few weeks, right? Because I think doing something like that actually might help with the up challenge as well, the abstraction and reasoning corpus. So this is my latest kind of hit way that I'm going into. So this is from the knowledge graph conference. Okay, I actually listened to quite a few of their videos. This is the knowledge graph conference 2023. And there's a speaker, Danny from Dev6. I think I pronounced his name wrongly. But the idea is that if you are using chat GPT for your own applications, if you use chat GPT in different languages, you might get different outputs, okay, even for the same information. So you know, being Singaporean and you know, the presidential election is coming soon, I just asked like who is Singapore's current president right now. So you can see now is Halima Yacob. Yacob, sorry. And we asked the question in Chinese, all right, Singapore the\u7e3d\u7d71, so you know, you say, Singapore, there's no president in Singapore. So this is basically the same information, you just translate it, you can get different performances with chat GPT. Okay, and the same thing for like if you use Lamatu, Lamatu is heavily trained on English. If you use Chinese, I'm very sure it won't do very well. All right, this is a practical problem of large language models. You know, the Chinese benchmarks like they use Ernie, Wenxing Yi and those other Chinese language models, they say that they perform better than GPT for. Okay, I mean, at first I was skeptical. Then now that I think about it, they might have done their evaluation on Chinese data sets. And their language models are fine tuned on the Chinese data set. So maybe there's some merit to their claims, okay, on the specific Chinese data sets here. So this is one of the things that knowledge graphs can actually help to solve, because knowledge graphs can sort of translate this thing, because knowledge graph is not language specific, you see. So your concepts like president, okay, regardless of how you represent it in words, okay, your Chinese words or English words, you can actually go to the same part in the knowledge graph. And then you can have the key words here, like Singapore, and then it's like Halima. So you can actually retrieve the kind of information regardless of language, okay, and then you can pass back this information back into the generation of the model. So this can go here, back here. So regardless of how you prompt GPT in a certain language, okay, you can do it. So maybe I just do the flow chart. So G-L-M, okay, language, language invariant representation. Okay, then you do your processing there. And then you go back to L-M. So if for those of you who have been to some of my other like discussion sessions, you would know that I like to say that this is the, this part here. This part here is what I call the abstraction layer, or the latent layer, latent space. So you process it in a way that is different from the input domain, but because the information you process is similar, in this case, we are still asking for semantic information about the president. You know, we don't have to do it in the language domain, we can do it in like maybe some representational space. It could be a graph, all right, and then you can use whatever you process the graph, you can go back to your input space. So this is one of the key advantages that, you know, if we could interface large language models with some form of graphical or some form of memory-based approach that is invariant to the input language type, you could get some performance advantage here. Questions so far? Anyone? Okay, so let's cover some of the basics. What is a knowledge graph? So I took this from the paper. The knowledge graph is basically a triplet consisting of source, destination, to relation. So like for example, Barack Obama was born in Honolulu. So this is the relation, okay, so relation. And this is like Barack Obama as the source, and Honolulu is like the destination. So each knowledge graph is made up of all these triplets joined together in various ways. And the idea is that you just need to connect those entities that are related to each other. You can like you can actually walk through the knowledge graph and get the information you need. Okay, so like there are of course like mega nodes, like for example, like Barack Obama will have a lot of connections leading out of it because you're describing the person. Then like stuff like places where a lot of things leading into it, because a lot of things like like are in the USA, a lot of things are in Singapore, you know. So this is the 20, 30 years ideas of knowledge graph. Okay, it's not too bad. Okay, but it's very restrictive. I personally think that there is a better way to represent information other than this kind of structure. Okay, and we can go and talk about it later in the discussion. All right, I have something in the chat. Richard says, is there a handy reference chart for how this looks or compares to word-to-veg and similar embeddings? Okay, so this typical knowledge graph that I'm talking about here does not have embeddings yet. Okay, but in the future iterations like in 2017 or 2018, I think people have come up with these things like knowledge graph embeddings. So they actually encode all this information here in some vector space. So like maybe like, it's something very similar to like the vector space that we see in in large language models. You can actually encode this thing in vector space. You can encode the relation here in the vector space or so. And then like, you can then encode this part here also in vector space. So like, so it's like doing a vector arithmetic now. So you can see that if I do a relation, it's just simply this one plus this one equals to this one. So I can do a like add a .2 in the first one here and then I can get. So if you have a sufficiently expressive enough embedding space, you can express the whole knowledge graph in the form of embeddings. And that is indeed what some of the later models do. In fact, this is highly related to graph neural networks. Because graph neural networks, they express each node as an embedding, then they do message passing, which means I share information with the other nodes. Like at each time step, I pass some information to the other nodes. And I mean, there are different variants of message passing. The most common is that the message meets in the middle, this one then updates both nodes. So there are a few ways of doing the idea of like updating the embeddings and so on. I'm not going to cover in detail about how all this are done because graph neural networks is a huge topic. Okay, personally, I think graph neural networks is probably not the answer to solving intelligence. I'm sorry to Peter Velikovic. I like what he's doing, but I don't think it's it's the right way to do it, like using differentiable deep learning to do it. So I think the knowledge graph that I've described over here, which is using vectors to do addition and then you get the other nodes. That's a very expressive knowledge graph. Okay, because you can actually express everything in vectors without the names. So you can theoretically do any kind of like addition provided, you know, nodes plus relation give you another node provided that exists. So if you could somehow represent the whole of the world's knowledge in the form of vector space, I say we are done. We can just like, we achieved zero short generalization. You just embed into that vector space and then you add something and then you go to somewhere else. Okay, but I don't think that's how intelligence is represented. Okay, because you know, there's this thing called like context dependent embeddings. Like I don't think like the word Barrett Obama would have the same embedding all the time. So like, for example, if you have Barrett Obama that is like at the White House, Barrett Obama at his house, okay, Barrett Obama at the beach or maybe different places of Barrett Obama will lead to different characteristics of Barrett Obama. Like he maybe is very serious in the office, but he's very relaxed at the beach. You cannot have the same embedding space to represent all this. You need to walk it according to the context. Okay, and that is something that I actually intend to try to do it. Like I try to do a very flexible like basically the information can walk according to the parent nodes in this new form of knowledge graph that I'm thinking of. Okay, so whatever I'm talking about is my own idea. I haven't seen any paper on it yet, but I think the current knowledge graphs will all fail at embodying intelligence because it's just too restrictive right now. Okay, Shang, you asked a question. I'm unfamiliar with graph theory, so hoping to know how do you represent factors as weights and how many can you add? Okay, could you elaborate what you mean by factors? Yeah, you mentioned that like you can add any form of intelligence, right? So take for example, if we are using, yeah, I actually didn't think of this example, but let's say just the simplest one, multi-layer map. Then for these roads, one weight could be how fast the speed limit of the road and another weight could be how occupied it is. Okay, so you are talking about like descriptions of an object, like all characteristics, attributes, you're talking about attributes of an object. Yeah, the weight of each line, correct, of each connection between the nodes. Ah, okay, so like how do you get this embedding here, right? Yeah, correct. Yeah, so perhaps like in your original embedding space, each of these dimensions could represent something already, like maybe one could represent road, one could represent like emotion or, you know, there are different domains that these dimensions could capture already. So if you really have that, you can just like add the relation in that specific dimension. Yeah, so of course, all this will need to be like learn somehow. So it's either learn through deep learning or some fixed biases. Yeah, so ultimately, how well the graph does will depend on how good your embedding space captures all the information. Yeah, okay, so I hope that clarifies. Yeah, thank you. So for now, just know that knowledge graphs have a few forms. Okay, the most simple form is that you take words and then you add another word and then you get another word. So this like describes a relation. The more advanced form will be to use embeddings. All right, so we will talk more about and then of course, the even more advanced form is evolving embeddings or context dependent embeddings, which is like the idea that I have. And it's also the idea that large language models actually kind of use because when we can ground large language models in different contexts, you get different outputs. So a large language model is context dependent processing. Okay, if you can embody that kind of context dependence into the knowledge graph, you will have a very powerful knowledge graph. So as you can see, whatever I'm sharing with you here today, I think that I'm not the answer. All right, I'm just sharing with you here because this is what is existing. Okay, I have a grander vision compared to all of the stuff that I'm talking about. Okay, so let's continue. All right, so knowledge graphs, okay, what excites me in knowledge graph is the very notion of hierarchy. And I think hierarchy is key to intelligence. You don't process things in just one domain. You process things in many domains. Like if I'm drinking a cup now, I'm just like drinking water from a cup, not drinking a cup. Drinking water now, I use my hand to move like that. But then if I think about, oh, how do I go to school? Then I think about, oh, I need to do the bus stop, I need to go to the MRT maybe, and then I need to take this bus or this train. So this is a more higher level planning. Okay, if I were to think about like how I move my left leg and right leg, left leg, right leg, I will take forever before I do some planning. So different problems require different levels of solution finding. And I call this different levels of hierarchy. So it might be up challenge, the abstraction of reasoning corpus. I use multiple levels of hierarchy, like we have a pixel level, we have an object level, you know, and then you express the input grid into different forms of hierarchy. And I find that this way can solve a lot of problems because different problems require different approaches to think of it. But you don't solve all problems using trigonometry, you solve some using algebra, you solve some using set theory. So you have different ways of viewing the problem. And knowledge graph, you can actually use this to extract different, like at the top layers of the knowledge graph, typically are the more broad concepts and the bottom will be more of the, more of the general concepts. So you can see in this, this is the the SICK knowledge base. SICK is a 30 year old project trying to embody the world's knowledge in a knowledge graph. They are still trying to do it, but it turns out that this is a very hard thing to do because the knowledge graphs itself, it embodies like one relation is like a confirmed relation. But sometimes, you know, based on the context, you may not have that confirmed relation. So again, this is the context dependent knowledge graph I'm talking about. Also, another thing is a lot of times we do things, but we don't really know how to express it in words. So if you want to express the whole idea of logic in words, it's a very, very difficult task. Because sometimes we don't even know why we're doing something. Okay, there's bound to be a point of time that logic cannot express things. So you can go and look at this thing called Godot incompleteness theorem. If you use mathematical logic to express things, there comes a point in a time whereby logic cannot solve. Because the way to solve it lies beyond logic. It cannot be Godot incompleteness theorem. There's something like this. This sentence is false. So if you can represent this as a Godot number, this kind of sentence, and then you say that, oh, this number is true. But then this number says that this number is false. So it's like you have a self-referential loop. So if you use logical prepositions, and knowledge graph is sort of like a logical preposition, A goes to B, B goes to C, you might face this problem that you can actually go in you can actually go in a loop that contradicts itself. So that's one thing that knowledge graphs may have some issue with if we do it being 100% fact. If A links to B all the time, sometimes you might actually have a link that contradicts itself. That's one issue of the knowledge graph. The other thing is, so burying all these issues, one thing I like about knowledge graph is that you can see in this diagram here, you start off with small stuff like thing, and then you go to like individual, you go to collections, and then you have different ways of doing it, time, movement, and so on. Then you have agents, actors, plans, and goals. I mean, if you think about it, kind of it's like how large language models is evolving now, right? We are kind of at this stage, we are agents now. So after that, we have organization of agents, we have activities. Okay, hopefully we don't get to military warfare, you know, because like, so this is like the evolution of a population. So it's quite nice, and like you can capture all this knowledge from, okay, so I wanted to say this is for more like micro level to more macro level. And the macro level is actually the sum of the micro level. So maybe the arrow should be drawn the other way, the arrow should be drawn like that. You take from the micro stuff, and you go to the macro stuff. So this is the knowledge that we accumulate, right? And knowledge graphs can capture this quite well, because of the way you take from source relation to destination, you can capture from the micro level, you do all the branching, and then you end up with the micro level, right? So this was in 2016, by the way, I couldn't find this in the sick website today, right? So this is the sick knowledge graph. This is like a very, very tiny representation of how the knowledge graph looks like. I just wanted you to see like how one of the largest knowledge graph in the world looks like. So you can see like you have all these like the fortune companies, you have all these like functions, like all these like look like some form of like math stuff, right? Yeah, so you have like people over here. So you have different areas of congregation of all this knowledge, right? Then in order to pass through the knowledge graph, you have to use something very, very similar to SQL, structured query language, you like say that, oh, if I want to get a frightened person, I want to get the entity X that is a person, and then fuse emotion that is fear at a very high level. So you have to do this kind of stuff, right? So immediately you can see that knowledge graph right now can be immediately improved by large language models in one aspect. Okay, and this aspect is that we can straight away use the large language models to generate this structured query language. Okay, so if all of you are thinking about this, like if you want to get like a very, very rigid programming language out, okay, you can actually write what you want in free text and then say convert this to SQL, and you can get it out. So you can do the same thing for this sick language. You give it some examples of how the language works. You say, I want to get a frightened person and then, you know, chat GPD is quite good at getting stuff like this out. Okay, no more SQL, right? I love it. Okay, if you need to use SQL, just use chat GPD. Okay, it's a very good replacement. All right, so this is one way large language models can already help to benefit knowledge graphs right now because it can pass through it using very human readable and understandable syntax. Like this kind of thing is not very human understandable. You can use free text to do it and we can do it right now. But more importantly, what can large language models do to help knowledge graphs? Okay, or what can knowledge graphs do to help language models? Okay, so before I move on to that, let's just talk about some other ideas I have. So I'm actually a reinforcement learning person. So like, I feel like knowledge graph can also represent stuff like different states, like you have different tiredness, they drink coffee, and then you're not awake. So if you know in the literature of reinforcement learning, this like a Markov decision process where these are the states, okay, and these are the actions. And then this is the next thing. Okay, so you can actually use knowledge graphs to represent stuff like this as well. Okay, because it's quite anything that has a link like that, you can represent this easily. Okay, so all right, this is perhaps the most important slide for today. Okay, this is not in the paper that I referenced, but this is the thing that I was thinking about. It's like knowledge graph is actually sort of a tool that can be used by the agent. So like retrieval of method generation may not get the right passages because like the embedding space may not be good. Perhaps we can use like a form of knowledge graph passing, okay, you can extract relevant parts of the knowledge graph, you can retrieve the context based on that. Okay, so you can ask the knowledge graph to get you the subgraph. The subgraph, you can then use it to ground the context of the agent and how you use it to ground. Okay, it's up to you. Okay, some people might use graph neural networks. I don't advocate for that. Okay, one other way of doing it is to just convert it back to free text. Okay, as easy as that. So you use the knowledge graph to extract out the relevant purposes and avoid the need for the embedding space, the open AI embeddings. Okay, you use the knowledge graph to extract it. Then you take the stuff that you extract from the knowledge graph, pass it back as text and then go back to the agent to ground it. All right. So yeah, one other way of, one good thing about this is that if you have stuff like if you are doing this for a robot, okay, that experiences the world, you might actually be able to use this knowledge graph. Okay, I'm conflating the term knowledge graph, but this knowledge graph can now be the state action state graph. You know, you can actually model relations of the world easily. Like I always believed like we learned from taking actions in the world. So we can actually build this knowledge graph dynamically. This is the third point. Okay, you can gain knowledge of the world. We can build up this knowledge graph bit by bit. All right. And then we can then query this knowledge graph and get answers from the knowledge graph to inform our choices. Okay, so about how we can get this part here, this is a huge thing here. Okay, because I believe that there's one thing that's missing in current knowledge graph and this thing is called changing the memory to the context at hand. Okay, so I treat the knowledge graph as memory. So like when you retrieve things from memory, okay, and then you want to apply it to the current state right now, current state of the world right now, you don't really want to just use that memory itself. You want to adapt that memory such that it will be relevant in this current state. Like if I have drank like coffee at school or not, drink coffee at home now, okay, you know, something like that. This, I will need to adapt that memory of drinking coffee somewhere else and then adapt it back to here. Okay, there's no point in giving me the memory of me drinking coffee somewhere else because it doesn't adapt to the current situation. So if you can adapt this knowledge graph to the current situation, that will be great. Okay, that will be great. So that's something that I think I'm trying to look into because you don't just want static knowledge extraction. Okay, you want knowledge extracted and manipulated to fit the current context. Okay, of course, for those of you all in my discord group, I've been thinking about memory recently. And you know, human memory is very malleable. Like if you think about something, you might actually affect that memory of it. So like a lot of times people in the childhood, they think that they have certain memories again, like maybe you are lost in a supermarket. So if I keep asking you questions about it, I say, who was the stranger with you when you were lost? Okay, so maybe there was no stranger. But if I keep asking you guiding questions like that, eventually, you might think of your memories like, oh, yeah, a stranger let me home after I went, I was lost in the in the in the mud. Okay, but that that may not have happened. That memory has changed because I've asked you certain guiding questions. And then you think that certain things are now in your memory. Right. So whether or not we should change this memory and affect this knowledge graph, I leave that to future discussion. Okay, because this is something very interesting, like should we change the existing memory that we have, based on the current context, get our brains that our brains do that. Okay, but should we do this for this kind of practical systems? Okay. Yeah, so we say humans hallucinate. Yeah, of course, we hallucinate a lot. And that is why actually, we are quite similar to large English models in that sense. Now people always say large English models not very reliable. Are humans reliable? Our memory is not that reliable, actually, if you think about it. But honestly, I cannot trust my memories that that much, because like sometimes if it's too far away, it can change like the book that I've been reading. It will say that like flashback memories, which people think that are very, very pertinent, flashback memories are memories like, like, you know, 911 collapse. People tend to remember what they were doing at that time, because it was so significant. It turns out that this flashback memories can be wrong. Okay, it can also be, it can also be change. So this is a very interesting thing. You can actually use like the current context to affect the memory you have. So you might actually affect the knowledge graph about whether or not we want it to be that way. Okay, we have to think about that. Okay, I digress a bit. Okay. But let me just get back to topic. Okay, today we have quite a few slides to cover. There are three approaches that I want to talk about today. First is that you can use knowledge graph to enhance your large language models. And this means that you can give it structured stuff, like domain specific knowledge. In some sense, it's like text based grounding is the same as retrieval of mental generation, just that now you take the information from a knowledge graph. Number two, you use large language models, expressivity, okay, and make a better knowledge graph. Okay, I like this approach as well. Okay, we will see how to do it. And lastly, you combine both approaches, you can get a synergized large language models and knowledge graph. And I think something like this will be able to embody intelligence. Okay, but not the current knowledge graph. We need to change it to a dynamic knowledge graph. Okay, what is a dynamic knowledge graph? Maybe I'll talk about it next time. Okay, after I flesh out some ideas that I have right now, I will create this dynamic knowledge graph. Okay, I think the current knowledge graphs are not the answer. We need to have a different kind of knowledge graph. But if we use this, I think we can get intelligence. Okay, let's move on to the next point. Approach one, knowledge graph augmented large language models. Okay, so there are two ways I can, I summarize the paper in two ways. The main thing is one, you can just put the knowledge graph as text. And the other one is treat this as an object. And what kind of object? Okay, you either use like graph neural networks, or you can use an embedding space. I mean, the one that was used was trans e trans embedding. You can go and search the paper trans. So this are some ways that we can use the knowledge graph, okay, to pass it. Let's go through the first way. So the first, oh, sorry, this is basically a pipeline for retrieval of large language models grounding. First, you use some form of knowledge retrieval, like, you know, retrieval method generation, you use cosine similarity, you get certain facts or some documents. So I'm just relating this to retrieval method generation, because they are almost the same. All right, you take in the facts, you ground the large language model, you get the answer. Okay, and over here in the paper, they put back propagation. But you know, how are you going to back propagate this knowledge retrieval? You're going to end up with some, like, very, very weird way of doing back propagation. I don't think back propagation, I don't think back propagation is the answer here. Maybe you want to back propagate your LM to find you think, okay, I grant that. But this part here, to back propagate to the knowledge retrieval, I don't think that should be done. All right, because this back propagation thing will lead to, like, changes in embedding space. And then if you change your knowledge retrieval, you also need to change your large language model. It's a never ending cycle of chasing each other. Like, if you change the knowledge embeddings for the knowledge retrieval, you also need to change how you interpret them in the large language model. So, yeah, I don't think you should use back propagation for the knowledge retrieval. You should probably use, like, memory methods, other methods, like you can say that, okay, what worked, what did not work, what worked, what did not work. Okay, you can reference this paper called Voyager. Okay, so there are these automatic curriculum learner. I think you should train the knowledge retrieval like the automated curriculum learner. You just ground it in some examples of what works, what doesn't work. You don't have to use back propagation for that. Okay, so the main pathway for knowledge graph for tax, for LMS is like that. You take the knowledge graph, pass through it, get some facts, and then feed it into the large language model. Okay, that's the main pipeline. Okay, questions on this? Okay, let's move on. Okay, so this is one of my favorite papers. Okay, this is the Generative Agents paper. They have 20 agents in the sandbox interacting with each other. And one thing that struck me quite well for this paper is that they actually use JSON structure to ground the actions. So, for example, if you want to ask like Eddie Lin, he currently is in the Lin family's house. He's in the bedroom actually on the desk. Okay, so you can ask the agent, okay, this is actually the chatGPD prompt. Okay, you can ask the agent like, okay, these are the other areas that we have. Okay, and all these other areas are obtained from the JSON. Actually, the JSON is like a knowledge graph. Okay, because we actually have hierarchies like Lin family house has a bedroom, has a study room, has a kitchen, you know, this is something like a knowledge graph. If you ask me like, they are just like representing the hierarchy of the house. Like, I mean, if you want to treat it as a knowledge graph, you will say like, this is the house. So, this is the Lin's house. Lin's house. Lin's house. I've just put Lin's age. Okay, then you can have like, the relation will be contains, okay, or comprises, I mean, contains, then you can have like bedroom. Yeah, so the JSON kind of hierarchy is a subset of what a knowledge graph can embody. All right, so I treat this as a knowledge graph. So you can, you can sort of ground the agent. Okay, like this is what the agent knows. Okay, this is the current memory that the agent has as a form of knowledge graph. Like this are the kind of areas that we actually know from the world. Okay, this is like, if you talk about grid cells and play cells, maybe find out more areas. Okay, you can ground them. Okay, these are your semantic knowledge or oppositional knowledge that you have about the world. And actually, these two are the positional knowledge. This is the first one is knowledge about the house, knowledge about house. And then the second one is knowledge about the world. Yeah. So you, you have all this knowledge, you can ground the agent to choose a specific place. Imagine if we did not ground the agent with all this stuff at the top, you just ask, where should Eddie Lin go to? Then Eddie Lin might, the LL might reply, Eddie Lin should go to the supermarket or something like something that is irrelevant to the game world. Right, so because we grounded it with some idea of what kind of possible areas that the agent should go, the agent is able to choose one area from the above list. And how is this list generated? It's generated from passing some form of knowledge graph. And this is what I mean by using knowledge graph as text to ground the large language model. So you can use this recursively, you can say that, oh, currently you are in the maybe common room. Okay, what, where in the common room would you like to go? We like to go to the sofa, to the mirror, and you can do this recursively. Okay, and then you can get a very, very specific area that the agent is going. Okay. Any clarifications on this so far before I move on? Yeah, if you haven't read this paper, go read it. Okay, this paper is good. It's one of the better ones. All right. So we have the Chinese LLM, it's called Ernie. All right, and this, what they do is they actually use two hybrid ways of generating the output. So they say that large language models lack grounding, lack consistency. So we use a knowledge graph. Okay, I granted that. But then I look at that structure and they're like, oh man, what is this? So they actually have a large language model. This is the typical transformer architecture. So this is a typical transformer on the left side. So they have two encoders. Okay, one is the T encoder, which is like the text encoder. And why is the knowledge graph encoder? So the knowledge graph encoder uses this thing called trans embeddings. Okay, I'm not going to go through that, but they train that embeddings using like, they take one vector and take another vector, then they connect the, just draw the diagram here for you to see. So you have one vector A and another vector B, and then you create another vector C here. So you can keep like using this vector A, you take another vector, extend from it, and then you can train on this relation C. So, you know, this is how the trans embeddings are trained, trans E. Okay, and they use this kind of embedding space. Okay, you can do self-attention, and then you can do cross-attention across both the trans, the text embeddings as well as the knowledge graph embeddings. And then hopefully you get some output, right? And then you get some text outputs here, and then you can update your knowledge graph to the knowledge graph outputs here. Okay, so this is a way to embody a knowledge graph as some embedding space. Okay, and then we can use like the attention to like, attend to like the text-based stuff as well. So, yeah, this is just one way of doing processing using a knowledge graph. As you can see, I don't really like it. I mean, I think that it's too convoluted. Like, okay, so this is another discussion question that I'd like you all to think about. Should we have separate embedding space for this large language model stuff and for the knowledge graph embeddings? Like, should we use two different embeddings? Okay, should we use two different embeddings? Yeah, or should we use the same one? Yeah. Okay, then also more generally, like if you want to have multi-modal embeddings, like you have text, image embeddings, you have audio embeddings, okay, if you want to do a multi-modal large language models, you can actually also put them into distribution model at the end here. Okay, but the question is, in fact, knowledge graph can be multi-modal also, you can actually also put it here. The question is, all this image and audio embeddings, okay, you can put it in the large language model, you can also put in the knowledge graph. But why not just use a single embedding, right? Why do you need to? You are using text-based knowledge graph and text-based large language models, that there's no external domain here. It's all the same domain. Why do we need two different embeddings for the knowledge graph and the input text? That's my question. If you have any ideas, let me know. But think about this, all right. Okay, then we have this question-answer graph neural network, and this does a two-way interaction between the language model and knowledge graph. And what we can see here is that we have a certain question, okay, and some options that to choose from. The large language model will go in here, and then they express the question and the options as part of a knowledge graph, okay, and this will go through another knowledge graph encoder, and this knowledge graph encoder is a graph neural network. Okay, and this basically will, you look at this diagram here, it will do cross-attention. It's very, very confusing. Look at this language model conditions knowledge graph. So you can blank out some notes here, you know, you can actually do some attention on some notes to like block off the path. Yeah, so yeah, that's possible. So you can also use the knowledge graph to condition the attention in the, in, in, when, when you do the next open processing for the language model, and then eventually you get your answer. Yeah, so this is one way we can process the knowledge graph, you can process it using a graph neural network. And in fact, the earlier one on Ernie, that is similar to graph neural network as well. I mean, they are using the embedding space. And you know, if you just do some operations on the embedding space, that is a graph neural network already. So yeah, this is very, very similar to graph neural networks. And yeah, it shows that back in the first few years, people use these kind of methods to pass through knowledge graph using graph neural networks to represent the embeddings. Okay, I don't see why you need to do this. Okay, personally, I don't see why you need to do this. You can just use text, because the knowledge graph representation is in the same kind of domain as your last language model representation. They're all text. Yeah, why do we need a separate embedding? So yeah. Okay, Richard said, I think there will have to be input output embeddings and train them to address common pattern or memory structures. Sorry, could you explain this comment was in relation to which part of what I said? So you were saying, you know, it's a decision sort of or sort of my decision question, right, as well, that there is, how do you well approach this problem? Why do they have their embeddings separate, right? At the end, there's a sort of cross attention where they're merging them for an output of this type or that type would have you. But then this idea comes that the real, so I think of a large language model where the reason why they have these emergent behaviors is because language is currently our best mechanism to embody thoughts, ideas, and our most direct implementation of ideas. Now, particularly once they're broken down, tokenized and so on, you've taken through that process a few times, consider retention and context, you come up with new ideas. Yes, yes. And then as you're just saying, there's no particular need for different embedding spaces. And the only need for them is to bring understanding into a common framework where the ideas themselves in the latent space are being considered and their context and their relationships. So how the, this is sort of a, what's the word, this idea of boiling down the actual form of communication into some representation, any representation where we can start applying our knowledge to it. Whether you read text or listen to text, you don't, when I hear things, I don't imagine them written down in front of me. I just hear words, words become ideas, and we go from there. So in the same way, I see the way that knowledge is presented as an input-output problem and embeddings really address the input-output problem. And then after that, there's a memory and consideration process which operates on ideas which are not linked to input and output. I think you and I agree that there needs to be a latent space or abstraction space for processing. And I think you also agree that there need not be too separate embedding space for the knowledge graph in the last image model. If I hear it correctly, right, you don't, you also don't think that is necessary, right? I think, yeah, but then the problem becomes, if you don't use the same embedding technique, how do you present meaning? So for mine, in terms of large language model being in language or not in language, in words, the question is really, are we making the problem harder for ourselves by using a difference, by saying, well, it's all words and the words are by and large correct. Therefore, we'll just use a large language model to read and ingest a large language model. And I think that will work. But the challenge becomes what you alluded to earlier, where the Chinese representation versus the English representation gives a different outcome. And I'm trying to abstract away that behaviour. So the thinking is, the actual thinking happens in, is always in latent space. And the only job for embeddings is to present in a form where, you know, cognition can happen. Right? Because, right? And so I would say, I don't particularly care what the encoding encoder is, decoder is, it can go from text in, picture out, picture in, text out, it doesn't matter. The important thing is that it's consistent, and we can operate on it in a manner that addresses the patterns and relationships within. Yeah, well said, well said. I agree with you. So what matters is how we abstract it to the processing space, which is the latent space, and how we encode it and decode is just extra details, that basically just needs to be mapped there, and it should be good enough. Yeah, I think so. And if it comes to its own training challenge. Correct. So I think in the earlier papers, what I get is like, why do we need a knowledge graph encoder like that? It's because they use embeddings like trans-e that, you know, are different from GPD embeddings, like, or BERT embeddings. Again, most of the early papers use but BERT, bi-directional encoder representations from transformers. So what happens is because these two are from a different embedding space. So you kind of need to map them to the same embedding space. That's why you need a knowledge graph encoder and a large-engaged model encoder. But in the new kind of knowledge graph that is constructed, because this large-engaged model is now so powerful, you can actually use the embedding space for the large-engaged model to construct your knowledge graph. And if you do that, okay, if you do that process, which is part two of the presentation today, you will see later, if you use the large-engaged models to construct the knowledge graph, actually, you don't need a separate knowledge graph encoder or encoder here because they are in the same embedding space already. So if you look at this thing here, you don't need a separate decoder for the JSON here because this is in the same embedding space as your text. And I would like to posit that it will be better for all of them to be in the same embedding space because it will be much easier to do the attention. I mean, it's easier to do attention in the same domain as compared to different domains because, you know, cross-attention is only one layer right now. You're going to do a very efficient cross-attentioning multiple layers. But if you just do it in the same domain, the transformer architecture right now, you actually do the self-attention multiple times, all right? So it might be actually better to do it in the same domain, okay? And of course, you will save training complications because, you know, you need to map both to the same latent space and, you know, that is a difficult problem. It's a very difficult problem to map two different streams of inputs to the same latent space. I mean, we have seen it, like in OpenAI, they have this thing called clip, okay, that max text and images to same latent space. You know how many examples they train it on? Millions, I think even billions. Yeah, so it's a very, very difficult problem to map both to the same latent space. If I can map it well, of course, you can do like stuff like stable diffusion, you know, dolly, you can generate images from text. Yeah, but why have this problem with the knowledge graph when you can actually just ground the knowledge graph in the same embedding space as your large language model? Okay, so think about that. Okay, now we go to approach two. Before I move on, I'd like to open the floor for any opportunities to ask anything so far for the first part. Okay, if not, I'll carry on. So next is how we can use a knowledge large language model to get the knowledge graph. So one is to use using few short and zero or zero short prompting, like for example, length chain, okay, I don't think the approach is that great. Okay, I found a better approach, right, using a better prompt, but we can potentially use large language models to generate knowledge graphs. The other way is to use the embedding space of the large language models to enrich the representational space of the knowledge graph. So this is also quite interesting. Let's see how we can do both. Okay, the first one is, okay, this is just some idea of how we can use it. Okay, we can, we can few short prompt to generate the relations, okay, because large language models are just very versatile and can be context driven to do it. And actually it's way better than, you know, so this is my own experience. I use space scene to do name entity recognition and I use large language models to do that. The GPT, chat GPT performs way better than space scene. Space scene makes out a lot of the names, all right. So if we use large language models to generate the knowledge graph, compared to traditional approaches, like spacey or some other verb, VMP, you know, those kind of three parcels for language. Last time people used that to generate the knowledge graph to find out what are the nouns, what are the verbs and so on. Okay, so that was difficult to generate the knowledge graph because sometimes it miss out certain things. But large language models are quite good. Okay, why not just use large language models directly to generate the relations and the source and the destination. So indeed, this is what Lang Chen did. Okay, so if you look at the graph QA prompt, this is the prompt. Okay, you are the network intelligence. Okay, help to integrate stuff into a knowledge graph, extract knowledge triples from text. Okay, a knowledge triple is a clause that contains a subject, predicate, an object. Okay, subject is entity being described, predicate is the property, object is the value of the property. Okay, there's a typo here. Okay, so this is the zero shot prompting for Lang Chen. All right, this is not that good yet. So you need to give some few short examples and they gave some few short examples in the prompt, like for example, like this is the input, and then you can say that oh, Nevada is a state, Nevada is the U.S., Nevada is number one go producer in Go. So I don't like this example. Okay, because for one, they did not say the state at all in the prompt. Like then you want the model to just plug the plug the noun from thin air. So yeah, like here, I'm going to the store output none. Why, why is the output none? It should be I went to store something like that. Yeah, so you should be able to extract something from this. So I disagree with the examples that the Lang Chen one provided. Okay, so I think if they improve this example, maybe theirs would work better. So let's take a look at what I did later. So I'm not a fan of Lang Chen, by the way. Lang Chen prompts are very worthy. So this is the other way that we can use the large language models to do the text encoding, to do knowledge graph embeddings. So this is called KGE, knowledge graph embeddings, is something like, you know, if you talk about the stuff like trans, these are like embeddings that we can give to the source, to the destination, to the relation. So we can represent the knowledge graph as embeddings. And we can use GPT or some large language model, okay, to generate some embedding space here that you can then use like MLP, multi layer perceptron, and so on to map to to the embedding space of the knowledge graph embeddings. So this is one way we can utilize large language models to do it. Yeah, I mean, I was thinking, you know, like, why not just use this, right? Why not just use LAM embeddings directly for knowledge graph? I mean, LAMs are way better than, than doing graph neural networks, in the sense that, you know, if you know the problems with graph neural networks, I'm just going to tell you the problems of graph neural networks now. Okay, they have these two problems. Okay, this is one is called over squishing or over squashing. And the other one is called over smoothing. Okay, what are these two problems? Over squashing is that the information, because you pass the information into an embedding layer, information gets lost at embeddings. Okay, so this over squashing thing is also a problem for LAMs. So I'm not going to cover too much on it. The other problem that we have for this kind of graph neural network over smoothing, okay, is that after you do message passing or too many times, all embeddings look the same. Okay, so this is a big problem. Okay, I also realized this, that once I did graph neural networks, you, like, you have two nodes, you pass information to each other, and then you become the average of the information, you keep doing this, right? Eventually, both nodes become the same, or very, very similar. Okay, so this is one of the huge problems of graph neural networks. And I feel like the embedding space that is best done, right, is not the way that we do message passing in graph neural network. We should just ground it in the context using a large language model. And large language model update the context quite well. Okay, then you can just use the embedding that is derived from that particular context in, like, you can just put something like that, like, you can just say context. And then like, I am a student or something like that. So, like, this context will update the definition of the student here. So you can go through the transformer module. So this is the transformer module. And then you can get the final embedding here. Yeah, at the final layer, right, before the softmax, you can actually use the transformer to get the embeddings already. Why use knowledge graph embeddings? Okay, so I'm just putting this question out here. So I hope those people knowledgeable in this area can come and, you know, correct me if I'm wrong. But I don't see a point in doing this. Yeah, right now. Okay, so let's leave it as that. And let's continue. Approach tree. So the approach tree is how we combine both approaches to make a very, very synergistic model where the large language model can generate the knowledge graph dynamically and this knowledge graph is something like a dynamic memory that gets updated as the agent explores the world and so on. And this knowledge graph can then inform the knowledge, the large language model and ground it in consistent generation. So let's see how this works. So you can see this is the diagram in the paper. And you can see like data. Okay, that's what me and Richard discussed. Data, okay, will be from different domains to embed them into latent space. Okay, so now we just assume that there's only one latent space but my view is that there's multiple latent spaces. Okay, so right now we just treat it as there's only one latent space. You process the information in that one latent space using knowledge graph and large language model in this loop. Okay, so knowledge graph can ground the language model in consistency. Language model can make the knowledge graph more expressive. Okay, and not as rigid as before. I mean, maybe you can use embedding based knowledge graphs like then you can make the knowledge graphs like express a lot of things more than just text alone. Okay, so this is one idea. You can use different techniques to process it like graph theory networks from engineering, representational learning. Yeah, I mean, this is just some big words, but the idea is you basically do some processing. All right, you can use large language models to process. Or if you like it, you can process it using the knowledge graph, which is like a graph neural network to process. Okay, or you can make the graph neural network into text and then you can do some neural symbolic reasoning. Okay, actually, this whole thing can be just summarized as neural symbolic reasoning because plus the knowledge graph equals the symbol spot. And then the large language model is the neural networks. So you can just summarize this whole thing as neural symbolic reasoning. All right, then you can use this for different domains. Right, I think this is a very, very exciting path that we should work on. Because right now with the power of language models, the knowledge graph can be very, very flexible. And it's not a typical knowledge graph anymore, it can be embedding based knowledge graph. And it can be context dependent knowledge graph. Okay, so I really hope to work on context dependent knowledge graph, because I think that's the future. Okay, not the traditional knowledge graph that you've seen everywhere in this presentation. Okay, the knowledge graph embeddings must be able to do, must be able to change based on the parent nodes. Okay, must be changed based on the context. And that's something that is not done right now, at least based on my own awareness. I don't think that's done right now. But that's very promising. Right, so one use case for this kind of system is fact checking. As you know, large language models cannot do very badly at fact checking. It tends to hallucinate a lot. And perhaps we can do like, a knowledge graph to like ground it in some facts like some Wikipedia entries. No, you can use this to ground the inference. Okay, by doing inference, you can then see whether or not like, is it, is there a path in the knowledge graph that matches it? Or you do knowledge grounded inference, like you say, you must only use this information that I extract for you in the knowledge graph and infer it. So this diagram here, unfortunately, did not do the inference step. Okay, because they are still using birds. Okay, they're using bird as a model. And what they did was they use the knowledge graph relations to do some pre training. So it's like they take additional, like, additional tech samples, they just mask out certain words based on the knowledge graph relations. And then they do the training here. So they just did the pre training using the knowledge graph to give additional examples. Okay, so what I want the thing to do is actually to do it during inference, if I cannot find any paper that does that so far. All right. So I think this inference is more important than the pre training, you know, this pre training, yes, it increases more data samples, because you can just mix and match the knowledge graph, get more sentences out. Sure, I give it to you. And in fact, they improve by two to three percentage points across soda benchmarks, this fact KB, you can go and check it out. All right. But what I'm more interested in is how you use it for inference, not for pre training. Okay, so let's see how length chain does it. All right, so now we come to the length chain part. So actually, length chain is quite advanced, because length chain has a lot of the ideas that I think should be done. All right, let's see how the length chain question answering graph question answering is done. All right, so we have four steps. First step, we generate the triples from the context. Okay, so we are like maybe a text context, you generate like the triples from it, like the knowledge graph triples, you generate some from the query, you generate some entity extraction. Okay, and then you use this at that entities to extract this relevant triples. Okay, later I'll show you the how what I mean by this. And then you use this relevant triples to answer the question. So I share with you these two documentation in case you want to see how length chain graph QA does it. So step one, okay, generate triples from context. So like this context, I just came out of it, right? Recently, my MacBook external camera in the viewing camera spot. So I'm actually using the external camera right now to talk to you. And yeah, so this example is for Apple. So let's assume that Apple created a new product called Mac and Cheese Pro, okay, in 2025. All right, and then like Apple gave the invented cheese, okay, a rousing ovation in 2026 after invented this in 2024. Right, there's also another company called Orange who created a competing product called the Orange and Cheese Pro. The price was slightly higher at 5000 compared to 4000 from Apple. Okay, so this is a fictional example. Okay, and this is just to see like how good the context is stored in the knowledge graph. So you can see that, oh yes, Apple announced Mac and Cheese Pro, Apple gave cheese. So this kind of thing, right, like, is a bit contentious because like, what do you mean by gave cheese, gave what? So this one needs to be improved a bit. Apple ovation gave, Apple gave an ovation, okay, again to who? Right, so this one needs to be improved as well. Okay, the price of the MacBook Pro is 4000. Yes, Mac and Cheese Pro is already created. Orange and Cheese Pro, good. Orange and Cheese Pro, the price 5000. Okay, so you see, it's not bad. Miss out dates. All right, and then like some, some relations are ambiguous. So I don't quite like the way they did the triplet extraction and I think this is the downfall of the Graph QA. So if you are going to use Lang chain for Graph QA, my advice is don't use it. Okay, because you miss out a lot of stuff in the context. Okay, if you are interested how they generate the context, you can go back to my earlier slides that I was talking about. Yeah, so I mean, it's actually, let me show you, let me show you again the slides. It's this one, this is the one that they did, like, this is the problem to generate stuff from the text. Yeah, so the examples aren't very great and understandably the results aren't very great as well. All right, so this is the knowledge graph that's generated. You can see like Mac and Cheese Pro is cost $4,000 on price. Yeah, you can see that like stuff like price will contain like a lot of relation because like price is very generic. Okay, Apple announced Mac and Cheese Pro. Okay, so this is the knowledge graph that is generated. And we can see that like, next up we can use the Graph QA chain in order to run the chain and see the answer. And you can see that if I ask it the question like, when was the Mac and Cheese Pro announced? Okay, they couldn't find it. Okay, because after they passed through the context, okay, they abstract like when was the Mac and Cheese Pro, when did Apple announce the Mac and Cheese Pro? They abstract that in the query, there's only Apple and Mac and Cheese Pro. So they check through all the knowledge graph to make sure that you only have entities that match Apple or Mac and Cheese Pro. Okay, so I have a gripe with this thing. Like if you use exact text matching, what if there's a spelling error, capitalization error, or like related word, but not exact match. Yeah, so if you use exact text matching, which is what they did for a length chain, like what if you don't get the right match? Okay, so I don't quite like this approach. So yeah, this is something that I think could be improved on. All right, and you can see that if I ask it like, when did they announce the Mac and Cheese Pro? They couldn't answer. All right, because look at this knowledge graph here, there's nothing that talks about dates here. All right, so they miss out quite a huge chunk of information from the earlier context. So if we had fed in the earlier context directly, so I just use the length chain LmChain agent. Okay, so I'm only using the LmChain agent or this to just show you that length chain is not good. All right, I myself the new length chain. All right, so this is the idea that like, after a while, you know, this is the context and then, okay, so this is not bad. I mean, you could just do the same thing on ChatGBT, actually, you can just put like context question and then ChatGBT will give you the answer. All right, so this LmChain works and this shows that by embedding the text as a knowledge graph, it kind of miss out certain stuff. All right, and what are the stuff we miss out? We miss out the years and we also miss out like, Apple gave cheese. I mean, it doesn't make sense that way, right? I mean, look at the knowledge graph like, what am I trying to solve there? Apple gave cheese. Where is it? Apple gave cheese. Where's cheese? Apple Cheese gave. Is there a gave anywhere? Yeah, okay, I think this one maybe is the outdated, but the idea is that we can't really tell the main thing in this graph because we miss out some information. And that's one of the issues of converting text directly into knowledge graph is that you might miss out certain relations. And actually, if you think about it, if we want to embody all relations, there's just too many to embody, right? Yeah, it's too big to embody. So maybe the text itself is way more expressive than the knowledge graph, if you think about it that way. Okay, but again, you know, if you just use text only, you might face issues that, you know, your OpenAI embeddings might be too restrictive. It's too broad-based. You need the embeddings at different levels. So let's see how we would improve the Lang chain graph QA. I'm just using my strict JSON framework here, which just basically passes the system prompt and then outputs as a JSON in your own way. So I basically did what Lang chain does in a much shorter way. So I just say you are a knowledge graph builder. You extracted an object one, object two relation. Okay, I did not even put subject, object predicate. Okay, I mean, I just do like that. Okay, I just want it to be as vague and as generic as possible because I want to capture as much information as possible. Okay, so this was done in like 10 minutes. Okay, I don't really know whether this is the best. You all can feel free to improve it. Okay, I have the Jupyter notebook attached in the link. All right, so I gave you some examples like John bought the laptop. Okay, that's me, all right. John built the house in 2021. Okay, that's not me, all right. But this is the idea of like how we can represent like various relations like that. All right, then the output format is just a knowledge graph. So you can see like Apple announced my and cheese bro, my and cheese bro announced in 2025. Apple proved big hit. Okay, so again, this one is not exactly that great, because it's not really Apple that prove a big hit. It should be the Mac and cheese bro that prove a big hit. So this part needs to be from engineer a bit more. All right, Apple gave cheese. Okay, again, like this is not complete. Okay, cheese browsing ovation into zero to six. So actually we combine these two together. This is complete. So this is okay. All right, cheese invented man cheese bro. Okay, man cheese bro invented into zero to four. Okay, orange created orange and cheese bro. Yep, orange and cheese bro. The price is 5000 and Apple prices 4000. So again, here has some issues or so, like here, instead of saying that this is a Mac and cheese bro, because we should be referring to man cheese bro, it says Apple. Okay, so unless we can sort of like link this later to Apple announced, okay, this part here. So now you can see some issues with knowledge graph expressing stuff. It is not clean. All right, it might truncate the information halfway. So this one needs more study as to how we can express this in the knowledge graph better. But by expressing it in the knowledge graph, you are able to then do knowledge graph passing, okay, and extract out the relevant entities that related to the prompt. And you know, this is like, if you think about it, this is like doing segmentation across like every few words in the segment one time. Yeah, so this is the generated graph of what I did for strict design framework. You can see that compared to Lang chain, this is what happens like we have way more relations, there's more relations here. And dates are captured. Yeah, so this is something that I think needs to be investigated more. Mine is not the best, but Lang chain is definitely not good. Okay, so this is something that needs to be done more if we want to extract stuff out into the knowledge graph. And then like, should we use embeddings? So if you want to use embeddings, then we cannot just use OpenAI API. Maybe you need to use like Lama2. Okay, although Lama2 perhaps is not that great or so, because Lama2 is not that good for multilingual. Okay, but Lama2 is the best possible substitute for chat GPD right now. So maybe you can construct a knowledge graph embeddings using the Lama2 embeddings. So food for thought. Next, we have this flexible knowledge graph passing. Over here, what I decided to do is that we want to output only relations that are relevant to the question. And I just passed in the entire knowledge graph here. So instead of coming up entities, I just asked it to go through the entire knowledge graph because in case of words not exact or spelling errors, GPD is able to catch it most of the time. I must copy it because GPD is not as great as doing like counting letters and stuff. But if you misspell your words, but the meaning is about that, GPD is able to extract the right entities. And here we can see that we asked it like, when did Apple announce the man cheese bro? It captured exactly what we want. All right. And this is the graph that is the past knowledge graph. So I'm talking about when you query the knowledge graph, you pass it so that only relevant sections of the knowledge graph gets come out, gets extracted. You ground this extracted part onto your text. Okay. And then you can get the answer here. So 2025. So I just shown that like using this strict JSON format, you are able to like, it's very flexible. You just need to key in the system prompt, key in the user problem and output the format in terms of whatever JSON labels and the description of the JSON. So I've been using this for a lot of my own use cases. And I'm just adapting this for the knowledge graph. But this is really cool because you can then use this past knowledge graph, like this idea of generating the knowledge graph and passing the knowledge graph. You can use the knowledge graph as memory. Okay. And then you can update memory. And you can use updated memory to extract relevant parts. Okay. So this called retrieval. Okay. Use relevant parts to solve problem. So I really like this framework because this knowledge graph as memory thing is something quite interesting. But how can we express it as memory? That's the difficult part. Okay. So Richard asked, have I tried putting the graph into an FAIS index? No, I haven't. But how will you do a knowledge graph putting onto the, like onto that index? Because usually what I know is that you do the embedding and then you put the text. That's for the retrieval or method generation. If you're doing knowledge graph, maybe you put the source as the index. Okay. I'm not too sure. I'm going to check on this. Like how will you do this into PyCon and stuff like that? But what I can imagine you doing for the knowledge graph is just put the whole thing into some array and then just store the array. I mean, you can even put it as a JSON. Yeah. So yeah. Okay. I don't have time to cover through the running of the Jupyter Notebook. I'll just upload that separately. It's another video. But let's just go through like the last five to 10 minutes. I'm okay to extend about 15 minutes if you all have more things to discuss. Like we have discussed like how can we use knowledge graph better for last language models? So first question, what are the failure modes of using knowledge graph for context representation? And I think this failure mode is mainly like your knowledge graph may not capture all information. Okay. And also the knowledge graph capturing might truncate the information. So maybe using text directly may be better. Okay. But harder to pass. Because if you are using text directly, you don't really have like nice sections where you can pass the knowledge graph on. Yeah. So these are some of the things about right now, some of the failure modes of this knowledge graph. Anyone else has anything to add? Just some random thoughts. Do you think it makes sense if we view the embedding space itself as a form of generalized knowledge graph? Embedding space as a genera. You mean the LM embedding space? Oh, I mean, I mean, yes, but for some specific tasks you want to do, you can train a dedicated, a separate dedicated embedding space. So because like you have all your entities inside their space and the relative, I don't know, relative positioning of them kind of encode certain relative information of them, right? Because I think the issue you mentioned here is, I think it's just the graph, learn graph can be too sparse, right? You lose a lot of information. But if inside embedding space, I don't know, it might help preserve more information, although not very explicit information. It's just some random thought. Ah, okay, I get what you mean. Like you encode knowledge graph as embedding space, so like your source relation and the output are all embedding space and sink in destination. I feel like my gut feeling is it can be much richer than just a typical traditional graph. Definitely. Now I kind of agree with you. It's just going to be hard to express the embedding space using an OpenAI API. You might need to have access to the last average model directly if you want to do this. Yeah, but definitely that's one of the ways that we can represent knowledge graphs. Anyway, this is the second question also. Should we utilize the embedding space? Perhaps for more expressive knowledge graph. Okay, but then if you think about like what I was talking about earlier, context dependent embeddings. If you are talking about context dependent embeddings, actually we can use LLM to pass an update embeddings based on the parents of the node. Yeah, so I was thinking of something like that. Like you can actually have a very, very different interpretation of a certain word. Like for example, bank can be river bank or financial bank, depending on like the context of it. If you are talking about river side, then it's like river bank. Yeah, so you can actually use the last language model, extract out the hierarchy of the graph, the front part of the graph. You can put it there and you can then pass the embeddings accordingly. So I'm still thinking that perhaps just using the last language model embedding directly might be a better bet. And then you can just maybe use last language model to do this context thing. And then you can put this embeddings inside your knowledge graph. Like what you said earlier. If there's a way to get the embedding space directly from the OpenAI API, that would be great. But if not, we might have to use Lama2 in order to do this embedding space knowledge graph. But then again, is it really necessary? Can we just use text? So this is a big open question. Should we use embedding space for the knowledge graph? Or can we just represent it as text and then use the LM to generate embeddings after that? So I leave that as an open question. I think both approaches are valid approaches. I just feel like the way to input the knowledge graph as text is it will be much more interpretable. And also you only need to train one embedding space, which is the LM embedding space. So I kind of prefer that. Anyone else has any things to add? Okay, if not, we go to the next question. Can LMS help with a more flexible interpretation or construction of a knowledge graph? Okay, so our answer first. I think yes, definitely. Just like compared to like spacey or like on noun, verb, pro verb, those kind of stuff, like you are doing like the parse tree, compared to those very, very flexible. And you are able to extract a lot more information. So like just based on the string, some prompt I showed you earlier, you just hit object relation object, it captures almost everything. And that's zero shot prompting. Granted, it did not capture the date at first. I had to use the examples to give you the date. But compared to using this kind of like spacey and so on, like deep learning approaches, like you'll take quite long to train a new kind of like knowledge graph constructor. But with large language model, you can just use prompt engineering and get your knowledge graph out. I think that's very exciting. Okay, last question. How do we know what nodes are important to construct in the knowledge graph? Okay, because there's a lot of information, but not everything is needed for your use case. How do we know? Okay, so my opinion, okay, my opinion is this. You need to have biases based on the domain. And what are these biases? Maybe you can have multiple biases. Okay, and then let's just choose the right biases later. So this is my idea of intelligence right now. Okay, I'll share with you. Okay, this idea of intelligence is that there's not just one abstraction space where you store your information, you store them in multiple abstraction space. How do we get all these abstraction spaces? We basically just do rule based abstraction, like you like maybe one domain is saying that oh, dates are important. So I store the dates. Another domain is like all people, all person's names are important. I store the person's names. Then maybe another domain will be like all places are important. I store the places. Then when you want to solve the problem, okay, you will see which space is the best for your problem. Okay, you will just like, maybe you look at all the abstraction spaces, maybe combine two or more, or you just take one, and then whatever solves the problem works. So this would form an approach that will be used later on. So if you think about it, I'm just going to draw it here. Okay, I don't know whether I have space to draw it, but if you look at the top right of the screen, okay, you have a problem, you have multiple abstraction spaces, let's call this A. You have another abstraction space here, let's call it B. And we have another expression space, we call it C. All right, so if we have three abstraction spaces like that, okay, these are like three ways of doing it, three ways of doing the problem. And then in order to solve any arbitrary problem later, you just take mix and match the abstraction spaces to solve the problem. So yeah, increasingly, I've been feeling like this is the way to do things. So yeah, I also use this for my abstraction reasoning copless paper. So this is the idea that I have right now. You have different abstraction spaces. All these are rule-based. Okay, we don't really have deep learning here, because if you have deep learning, you'll have problems in getting a fixed abstraction space. You don't want the abstraction space to change. Because if you change this abstraction space, you have to change whatever you learn on it. It's like if I suddenly told you that math, the addition is now subtraction, that I have to relearn all my math again, because I need to update that new knowledge. So I'm saying that the basis is fixed, but then you just choose the right basis to solve it. Okay, then you might ask me, if we do it like that, what if we don't have the right basis to solve the problem? Okay, then the answer is you can't solve the problem. Okay, which might sound a bit crude to people, but I feel like we can't solve everything. Like even humans, we have our limitations. It's just that we work around our limitations and try to use our existing biases to solve new problems. And I think that's intelligence. We don't really need to change the abstraction spaces. We can just work with getting multiple abstraction spaces and then just combining them. So I shared a bit about my view of intelligence here. And yeah, that's more or less it for the questions to ponder. Anyone has anything else to add for any of these questions? I just want to clarify one thing. I'm still not very sure about the motivation here. So why we want this knowledge graph? Why use a knowledge graph, is it? Because the internal representation of LLMs is already already richer than a knowledge graph. So for example, you can just use the last language model to help you to break down the task. Actually, because the way I did it before is I want to specifically construct the knowledge graph first. Then I use that knowledge graph to break down the task from a hierarchical or sequential learning of intermediate goal to RL. But in the sense, you don't really need that particular task at least, you don't really need to do that anymore because you just query the last language model. At least the common sense way of breaking down a certain task. So you already have the different sub tasks. So because my understanding is more coming from that perspective, so I'm thinking why do I still need the knowledge graph? I mean, as what I said in one of the first few slides, if you use retrieval or method generation, you might miss out certain chunks of text. Knowledge graph provides a sort of hierarchy that you can pass over and extract information. So it provides the structure. But you still mentioned the problem is like even with knowledge, you have to distill information. So there is inevitable that certain information are lost. So it be it in a way of like you do chunk first, chunking first, then you summarize each chunk. Then you summarize again, maybe like a hierarchical way of summarizing and all even your knowledge graph, you only extract that the relation you think is important between different entities. So you face this issue of having information lost. Yeah, so maybe the correct way is not the existing kind of knowledge graph, but a new kind of knowledge graph. But I do believe a graph based representation of knowledge is important because when you learn new things, we typically try to fit in with our existing knowledge and we build on the knowledge from there. So if you have some form of graph structure to represent knowledge, you can actually like use that for learning as well. And that's where I'm coming from. My intuition is it can be okay. So in that case, I can see how it can be useful. It can be served as a heuristic for search. So if you want to understand a very large chunk of text, if you extract like a rough graph representation of the information, then you just do a heuristic search based on that. So even if this information loss is still can expand upon just based on your existing knowledge graph, maybe it just gave you some useful signal to tell you like which part of the text you want to do some search, then you can just go and search. You don't need necessarily to stick with the strictly stick with the graph. So in that way, like just a heuristic but it's a useful one. Yeah, yeah. That's one way of doing it, like using a heuristic to search. So it's like replacing the cosine similarity in retrieval of mental generation. You just pass through the knowledge graph. Yeah. Because the chunking in like just naive way, it's just, it's just like because the way we structure essay or structure the text is not necessarily just like each different hierarchy like different object or different something that is quite intricate, right? If for example, for some novel, you have foreshadowing, you have different way of writing, you have plot device, then you suddenly at a certain very, you think apparently very random point suddenly become very important. So just by naively chunking into like evenly it doesn't, it doesn't work. But if you have a graph like sort of tells you this kind of structure, then you're based on that to do some similarity. Definitely I feel like it can be more efficient. So I think they give you a better way of doing chunking. Yeah. Yeah, I like this approach. In fact, that's one of the motivations of using knowledge graph as well in order to find a better way to pass it. I mean, we can pass through graphs pretty well. So yeah, we can perhaps get better retrieval using knowledge graphs. So that's one provided you have all the information in your knowledge graph, you can get better retrieval using knowledge graphs provided the first part again, because that's the failure case we saw just now in the length chain graph answering agent. But anyway, I think all the logition all stand from the fact that the context window is very limited. So if we can solve that problem, then here actually, I mean, both way either way. Yeah, I'm quite excited about this. In fact, I will spend like the next few weeks trying to create a new knowledge graph. So I'll share with you after I create it, because there needs to be some context dependent passing. And that's lacking right now in the knowledge graph that I see in so far. You agree with the context dependent passing, right? Like how you interpret a certain like note actually depends on the parents or depends on the position in the graph. Yeah, even from a very traditional perspective, this very crucial as well. Nice. Okay. Last minute or so anyone has any last points you want to add? Okay. If not, thanks for coming. And yeah, if anything, you can still reach out to me on Discord or LinkedIn. Yeah. And I'm looking forward to do this linkage between like large image models and knowledge graph. This is what a lot of people call a neural symbolic. And I think this will be very crucial for intelligence. And I can see how we can use this knowledge graph approach to like learn stuff from the environment and use it in a learning agent. Like I like to call it reinforcement learning agent, but it's not really reinforcement learning because there are no rewards. Okay. You can just learn directly from knowledge in the memory itself. So I think this will be very crucial for that kind of framework. And yeah, hope to share more of your after experiment with it. Okay. If not, yeah. Thanks for coming. And I'll see you around. Okay. Bye, friend.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.08, "text": " Hi everyone and welcome to today's session. Today we'll be talking about a very exciting", "tokens": [50364, 2421, 1518, 293, 2928, 281, 965, 311, 5481, 13, 2692, 321, 603, 312, 1417, 466, 257, 588, 4670, 50718], "temperature": 0.0, "avg_logprob": -0.20102973860137316, "compression_ratio": 1.74609375, "no_speech_prob": 0.13908332586288452}, {"id": 1, "seek": 0, "start": 7.08, "end": 12.84, "text": " topic which tries to merge large language models and knowledge graphs together. So as", "tokens": [50718, 4829, 597, 9898, 281, 22183, 2416, 2856, 5245, 293, 3601, 24877, 1214, 13, 407, 382, 51006], "temperature": 0.0, "avg_logprob": -0.20102973860137316, "compression_ratio": 1.74609375, "no_speech_prob": 0.13908332586288452}, {"id": 2, "seek": 0, "start": 12.84, "end": 17.48, "text": " you all already know, large language models are the recent hype. You can literally do", "tokens": [51006, 291, 439, 1217, 458, 11, 2416, 2856, 5245, 366, 264, 5162, 24144, 13, 509, 393, 3736, 360, 51238], "temperature": 0.0, "avg_logprob": -0.20102973860137316, "compression_ratio": 1.74609375, "no_speech_prob": 0.13908332586288452}, {"id": 3, "seek": 0, "start": 17.48, "end": 22.240000000000002, "text": " anything or a lot of things with large language models by just changing the prom. It is very,", "tokens": [51238, 1340, 420, 257, 688, 295, 721, 365, 2416, 2856, 5245, 538, 445, 4473, 264, 2234, 13, 467, 307, 588, 11, 51476], "temperature": 0.0, "avg_logprob": -0.20102973860137316, "compression_ratio": 1.74609375, "no_speech_prob": 0.13908332586288452}, {"id": 4, "seek": 0, "start": 22.240000000000002, "end": 28.32, "text": " very flexible. Get knowledge graphs extremely rigid. You have notes connected to other notes", "tokens": [51476, 588, 11358, 13, 3240, 3601, 24877, 4664, 22195, 13, 509, 362, 5570, 4582, 281, 661, 5570, 51780], "temperature": 0.0, "avg_logprob": -0.20102973860137316, "compression_ratio": 1.74609375, "no_speech_prob": 0.13908332586288452}, {"id": 5, "seek": 2832, "start": 28.32, "end": 34.32, "text": " in relations, but they are also very informative because the relations don't change. The notes", "tokens": [50364, 294, 2299, 11, 457, 436, 366, 611, 588, 27759, 570, 264, 2299, 500, 380, 1319, 13, 440, 5570, 50664], "temperature": 0.0, "avg_logprob": -0.17558889843168712, "compression_ratio": 1.8087649402390438, "no_speech_prob": 0.003407931188121438}, {"id": 6, "seek": 2832, "start": 34.32, "end": 38.4, "text": " don't change. The large language models, one problem that they face is that they are a", "tokens": [50664, 500, 380, 1319, 13, 440, 2416, 2856, 5245, 11, 472, 1154, 300, 436, 1851, 307, 300, 436, 366, 257, 50868], "temperature": 0.0, "avg_logprob": -0.17558889843168712, "compression_ratio": 1.8087649402390438, "no_speech_prob": 0.003407931188121438}, {"id": 7, "seek": 2832, "start": 38.4, "end": 44.0, "text": " little stochastic. They tend to generate things that may not be grounded in facts. So it seems", "tokens": [50868, 707, 342, 8997, 2750, 13, 814, 3928, 281, 8460, 721, 300, 815, 406, 312, 23535, 294, 9130, 13, 407, 309, 2544, 51148], "temperature": 0.0, "avg_logprob": -0.17558889843168712, "compression_ratio": 1.8087649402390438, "no_speech_prob": 0.003407931188121438}, {"id": 8, "seek": 2832, "start": 44.0, "end": 48.72, "text": " like naturally these two approaches seems like a good fit together. One is more flexible,", "tokens": [51148, 411, 8195, 613, 732, 11587, 2544, 411, 257, 665, 3318, 1214, 13, 1485, 307, 544, 11358, 11, 51384], "temperature": 0.0, "avg_logprob": -0.17558889843168712, "compression_ratio": 1.8087649402390438, "no_speech_prob": 0.003407931188121438}, {"id": 9, "seek": 2832, "start": 48.72, "end": 54.64, "text": " which is the large language models. And one is more reliable, like the knowledge graph.", "tokens": [51384, 597, 307, 264, 2416, 2856, 5245, 13, 400, 472, 307, 544, 12924, 11, 411, 264, 3601, 4295, 13, 51680], "temperature": 0.0, "avg_logprob": -0.17558889843168712, "compression_ratio": 1.8087649402390438, "no_speech_prob": 0.003407931188121438}, {"id": 10, "seek": 5464, "start": 54.8, "end": 60.4, "text": " Okay, so without further ado, let's begin today's topic. So I will roughly follow the", "tokens": [50372, 1033, 11, 370, 1553, 3052, 22450, 11, 718, 311, 1841, 965, 311, 4829, 13, 407, 286, 486, 9810, 1524, 264, 50652], "temperature": 0.0, "avg_logprob": -0.15826785875403362, "compression_ratio": 1.5868055555555556, "no_speech_prob": 0.003454313613474369}, {"id": 11, "seek": 5464, "start": 60.4, "end": 64.48, "text": " framework of this paper called Unifying Large Language Models and Knowledge Graphs,", "tokens": [50652, 8388, 295, 341, 3035, 1219, 1156, 5489, 33092, 24445, 6583, 1625, 293, 32906, 21884, 82, 11, 50856], "temperature": 0.0, "avg_logprob": -0.15826785875403362, "compression_ratio": 1.5868055555555556, "no_speech_prob": 0.003454313613474369}, {"id": 12, "seek": 5464, "start": 65.2, "end": 71.76, "text": " a roadmap. Okay, this is by some IEEE fellows and senior members. I quite like the style of this", "tokens": [50892, 257, 35738, 13, 1033, 11, 341, 307, 538, 512, 286, 7258, 36, 35595, 293, 7965, 2679, 13, 286, 1596, 411, 264, 3758, 295, 341, 51220], "temperature": 0.0, "avg_logprob": -0.15826785875403362, "compression_ratio": 1.5868055555555556, "no_speech_prob": 0.003454313613474369}, {"id": 13, "seek": 5464, "start": 71.76, "end": 77.36, "text": " paper, but I feel like a lot of the things that are surveyed in the paper are not exactly the", "tokens": [51220, 3035, 11, 457, 286, 841, 411, 257, 688, 295, 264, 721, 300, 366, 8984, 292, 294, 264, 3035, 366, 406, 2293, 264, 51500], "temperature": 0.0, "avg_logprob": -0.15826785875403362, "compression_ratio": 1.5868055555555556, "no_speech_prob": 0.003454313613474369}, {"id": 14, "seek": 5464, "start": 77.36, "end": 82.64, "text": " latest large language model stuff. They are like the births, raw births, like basically the 2017", "tokens": [51500, 6792, 2416, 2856, 2316, 1507, 13, 814, 366, 411, 264, 3965, 82, 11, 8936, 3965, 82, 11, 411, 1936, 264, 6591, 51764], "temperature": 0.0, "avg_logprob": -0.15826785875403362, "compression_ratio": 1.5868055555555556, "no_speech_prob": 0.003454313613474369}, {"id": 15, "seek": 8264, "start": 82.64, "end": 88.56, "text": " to 2019 era, that kind. So I have supplemented it with some of the more recent advancements,", "tokens": [50364, 281, 6071, 4249, 11, 300, 733, 13, 407, 286, 362, 15436, 292, 309, 365, 512, 295, 264, 544, 5162, 7295, 1117, 11, 50660], "temperature": 0.0, "avg_logprob": -0.1636826524110598, "compression_ratio": 1.6007067137809188, "no_speech_prob": 0.005033386405557394}, {"id": 16, "seek": 8264, "start": 88.56, "end": 94.16, "text": " like some length change stuff. So enjoy. Do feel free to comment anytime, because I think", "tokens": [50660, 411, 512, 4641, 1319, 1507, 13, 407, 2103, 13, 1144, 841, 1737, 281, 2871, 13038, 11, 570, 286, 519, 50940], "temperature": 0.0, "avg_logprob": -0.1636826524110598, "compression_ratio": 1.6007067137809188, "no_speech_prob": 0.005033386405557394}, {"id": 17, "seek": 8264, "start": 94.16, "end": 101.2, "text": " this is a very interesting field that can be expanded upon. Never before have we", "tokens": [50940, 341, 307, 257, 588, 1880, 2519, 300, 393, 312, 14342, 3564, 13, 7344, 949, 362, 321, 51292], "temperature": 0.0, "avg_logprob": -0.1636826524110598, "compression_ratio": 1.6007067137809188, "no_speech_prob": 0.005033386405557394}, {"id": 18, "seek": 8264, "start": 101.2, "end": 106.08, "text": " gotten large language models this powerful like chat GPD. And this is really something that we", "tokens": [51292, 5768, 2416, 2856, 5245, 341, 4005, 411, 5081, 460, 17349, 13, 400, 341, 307, 534, 746, 300, 321, 51536], "temperature": 0.0, "avg_logprob": -0.1636826524110598, "compression_ratio": 1.6007067137809188, "no_speech_prob": 0.005033386405557394}, {"id": 19, "seek": 8264, "start": 106.08, "end": 110.64, "text": " can look at to improve on traditional methods or even think of a new method that is not even a", "tokens": [51536, 393, 574, 412, 281, 3470, 322, 5164, 7150, 420, 754, 519, 295, 257, 777, 3170, 300, 307, 406, 754, 257, 51764], "temperature": 0.0, "avg_logprob": -0.1636826524110598, "compression_ratio": 1.6007067137809188, "no_speech_prob": 0.005033386405557394}, {"id": 20, "seek": 11064, "start": 110.64, "end": 115.76, "text": " knowledge graph. Okay, later I'll share with you some ideas. Okay, so what are the pros and cons", "tokens": [50364, 3601, 4295, 13, 1033, 11, 1780, 286, 603, 2073, 365, 291, 512, 3487, 13, 1033, 11, 370, 437, 366, 264, 6267, 293, 1014, 50620], "temperature": 0.0, "avg_logprob": -0.12955929061113777, "compression_ratio": 1.8682170542635659, "no_speech_prob": 0.0025936495512723923}, {"id": 21, "seek": 11064, "start": 115.76, "end": 121.12, "text": " of knowledge graphs and large language models? So as I said earlier, you can look at the rough", "tokens": [50620, 295, 3601, 24877, 293, 2416, 2856, 5245, 30, 407, 382, 286, 848, 3071, 11, 291, 393, 574, 412, 264, 5903, 50888], "temperature": 0.0, "avg_logprob": -0.12955929061113777, "compression_ratio": 1.8682170542635659, "no_speech_prob": 0.0025936495512723923}, {"id": 22, "seek": 11064, "start": 121.12, "end": 126.56, "text": " summary. I think this is quite a good summary. Okay, large language models, they are generalizable.", "tokens": [50888, 12691, 13, 286, 519, 341, 307, 1596, 257, 665, 12691, 13, 1033, 11, 2416, 2856, 5245, 11, 436, 366, 2674, 22395, 13, 51160], "temperature": 0.0, "avg_logprob": -0.12955929061113777, "compression_ratio": 1.8682170542635659, "no_speech_prob": 0.0025936495512723923}, {"id": 23, "seek": 11064, "start": 127.12, "end": 133.36, "text": " Okay, they possess a lot of knowledge. All right. But what they lack is that they lack some form of", "tokens": [51188, 1033, 11, 436, 17490, 257, 688, 295, 3601, 13, 1057, 558, 13, 583, 437, 436, 5011, 307, 300, 436, 5011, 512, 1254, 295, 51500], "temperature": 0.0, "avg_logprob": -0.12955929061113777, "compression_ratio": 1.8682170542635659, "no_speech_prob": 0.0025936495512723923}, {"id": 24, "seek": 11064, "start": 133.36, "end": 140.32, "text": " understanding or facts. Okay, general language understanding. Okay, this one is debatable,", "tokens": [51500, 3701, 420, 9130, 13, 1033, 11, 2674, 2856, 3701, 13, 1033, 11, 341, 472, 307, 3001, 31415, 11, 51848], "temperature": 0.0, "avg_logprob": -0.12955929061113777, "compression_ratio": 1.8682170542635659, "no_speech_prob": 0.0025936495512723923}, {"id": 25, "seek": 14032, "start": 140.32, "end": 145.68, "text": " because like GPT-4 can be said to understand language pretty well. Like NLU is like the", "tokens": [50364, 570, 411, 26039, 51, 12, 19, 393, 312, 848, 281, 1223, 2856, 1238, 731, 13, 1743, 426, 43, 52, 307, 411, 264, 50632], "temperature": 0.0, "avg_logprob": -0.1348474343617757, "compression_ratio": 1.7050359712230216, "no_speech_prob": 0.002074179006740451}, {"id": 26, "seek": 14032, "start": 145.68, "end": 151.2, "text": " ace most of the task there. Okay, so this one may be not so true in understanding, but for facts-wise,", "tokens": [50632, 17117, 881, 295, 264, 5633, 456, 13, 1033, 11, 370, 341, 472, 815, 312, 406, 370, 2074, 294, 3701, 11, 457, 337, 9130, 12, 3711, 11, 50908], "temperature": 0.0, "avg_logprob": -0.1348474343617757, "compression_ratio": 1.7050359712230216, "no_speech_prob": 0.002074179006740451}, {"id": 27, "seek": 14032, "start": 152.32, "end": 158.56, "text": " fact generation is still a problem right now. Okay, incompleteness. Okay, maybe I mean like", "tokens": [50964, 1186, 5125, 307, 920, 257, 1154, 558, 586, 13, 1033, 11, 14036, 14657, 15264, 13, 1033, 11, 1310, 286, 914, 411, 51276], "temperature": 0.0, "avg_logprob": -0.1348474343617757, "compression_ratio": 1.7050359712230216, "no_speech_prob": 0.002074179006740451}, {"id": 28, "seek": 14032, "start": 158.56, "end": 163.68, "text": " sometimes they might generate things that don't answer the question fully. Okay, but increasingly", "tokens": [51276, 2171, 436, 1062, 8460, 721, 300, 500, 380, 1867, 264, 1168, 4498, 13, 1033, 11, 457, 12980, 51532], "temperature": 0.0, "avg_logprob": -0.1348474343617757, "compression_ratio": 1.7050359712230216, "no_speech_prob": 0.002074179006740451}, {"id": 29, "seek": 14032, "start": 163.68, "end": 167.92, "text": " this is not really a problem anymore. Okay, it's more of like the reliability right now. So I", "tokens": [51532, 341, 307, 406, 534, 257, 1154, 3602, 13, 1033, 11, 309, 311, 544, 295, 411, 264, 24550, 558, 586, 13, 407, 286, 51744], "temperature": 0.0, "avg_logprob": -0.1348474343617757, "compression_ratio": 1.7050359712230216, "no_speech_prob": 0.002074179006740451}, {"id": 30, "seek": 16792, "start": 167.92, "end": 173.51999999999998, "text": " summarize this part here, reliability. Okay, I should use a different color. Let me just change", "tokens": [50364, 20858, 341, 644, 510, 11, 24550, 13, 1033, 11, 286, 820, 764, 257, 819, 2017, 13, 961, 385, 445, 1319, 50644], "temperature": 0.0, "avg_logprob": -0.13979171752929687, "compression_ratio": 1.5210526315789474, "no_speech_prob": 0.0009850404458120465}, {"id": 31, "seek": 16792, "start": 173.51999999999998, "end": 182.39999999999998, "text": " my annotation. So I think the main thing that large language models lack are reliability. Oh no,", "tokens": [50644, 452, 48654, 13, 407, 286, 519, 264, 2135, 551, 300, 2416, 2856, 5245, 5011, 366, 24550, 13, 876, 572, 11, 51088], "temperature": 0.0, "avg_logprob": -0.13979171752929687, "compression_ratio": 1.5210526315789474, "no_speech_prob": 0.0009850404458120465}, {"id": 32, "seek": 16792, "start": 182.39999999999998, "end": 196.07999999999998, "text": " it's the same color. Let me see. Reliability and consistency. This too. I mean, y'all have tried", "tokens": [51088, 309, 311, 264, 912, 2017, 13, 961, 385, 536, 13, 8738, 72, 2310, 293, 14416, 13, 639, 886, 13, 286, 914, 11, 288, 6, 336, 362, 3031, 51772], "temperature": 0.0, "avg_logprob": -0.13979171752929687, "compression_ratio": 1.5210526315789474, "no_speech_prob": 0.0009850404458120465}, {"id": 33, "seek": 19608, "start": 196.08, "end": 199.84, "text": " large language models before, right? You key in the same prompt. Okay, sometimes you get the", "tokens": [50364, 2416, 2856, 5245, 949, 11, 558, 30, 509, 2141, 294, 264, 912, 12391, 13, 1033, 11, 2171, 291, 483, 264, 50552], "temperature": 0.0, "avg_logprob": -0.20439597722646352, "compression_ratio": 1.771604938271605, "no_speech_prob": 0.023258622735738754}, {"id": 34, "seek": 19608, "start": 199.84, "end": 205.44, "text": " different responses. Sometimes the response can be different. Like I said, should, it's a hot day", "tokens": [50552, 819, 13019, 13, 4803, 264, 4134, 393, 312, 819, 13, 1743, 286, 848, 11, 820, 11, 309, 311, 257, 2368, 786, 50832], "temperature": 0.0, "avg_logprob": -0.20439597722646352, "compression_ratio": 1.771604938271605, "no_speech_prob": 0.023258622735738754}, {"id": 35, "seek": 19608, "start": 205.44, "end": 208.48000000000002, "text": " today, right? Yeah, scrims on this can be yes, sometimes no, you know, that kind of thing.", "tokens": [50832, 965, 11, 558, 30, 865, 11, 795, 5565, 82, 322, 341, 393, 312, 2086, 11, 2171, 572, 11, 291, 458, 11, 300, 733, 295, 551, 13, 50984], "temperature": 0.0, "avg_logprob": -0.20439597722646352, "compression_ratio": 1.771604938271605, "no_speech_prob": 0.023258622735738754}, {"id": 36, "seek": 19608, "start": 209.12, "end": 213.28, "text": " All right, so knowledge graphs, what do they have? Knowledge graphs have structural knowledge.", "tokens": [51016, 1057, 558, 11, 370, 3601, 24877, 11, 437, 360, 436, 362, 30, 32906, 24877, 362, 15067, 3601, 13, 51224], "temperature": 0.0, "avg_logprob": -0.20439597722646352, "compression_ratio": 1.771604938271605, "no_speech_prob": 0.023258622735738754}, {"id": 37, "seek": 19608, "start": 213.84, "end": 219.12, "text": " They are quite accurate. Okay, decisive, I guess, if you can find a way to like connect an input", "tokens": [51252, 814, 366, 1596, 8559, 13, 1033, 11, 34998, 11, 286, 2041, 11, 498, 291, 393, 915, 257, 636, 281, 411, 1745, 364, 4846, 51516], "temperature": 0.0, "avg_logprob": -0.20439597722646352, "compression_ratio": 1.771604938271605, "no_speech_prob": 0.023258622735738754}, {"id": 38, "seek": 19608, "start": 219.12, "end": 224.08, "text": " node to an output node, you can say yes, there's a link between them. It's very interpretable. Okay,", "tokens": [51516, 9984, 281, 364, 5598, 9984, 11, 291, 393, 584, 2086, 11, 456, 311, 257, 2113, 1296, 552, 13, 467, 311, 588, 7302, 712, 13, 1033, 11, 51764], "temperature": 0.0, "avg_logprob": -0.20439597722646352, "compression_ratio": 1.771604938271605, "no_speech_prob": 0.023258622735738754}, {"id": 39, "seek": 22408, "start": 224.08, "end": 228.96, "text": " actually large language models are also quite interpretable. So it's not really a con here.", "tokens": [50364, 767, 2416, 2856, 5245, 366, 611, 1596, 7302, 712, 13, 407, 309, 311, 406, 534, 257, 416, 510, 13, 50608], "temperature": 0.0, "avg_logprob": -0.11725990295410156, "compression_ratio": 1.983402489626556, "no_speech_prob": 0.0023603509180247784}, {"id": 40, "seek": 22408, "start": 228.96, "end": 234.88000000000002, "text": " Large language is both actually interpretability is also in large language models. Domain specific", "tokens": [50608, 33092, 2856, 307, 1293, 767, 7302, 2310, 307, 611, 294, 2416, 2856, 5245, 13, 16674, 491, 2685, 50904], "temperature": 0.0, "avg_logprob": -0.11725990295410156, "compression_ratio": 1.983402489626556, "no_speech_prob": 0.0023603509180247784}, {"id": 41, "seek": 22408, "start": 234.88000000000002, "end": 240.08, "text": " knowledge. Yes, but actually, if you think about it, large language models with context grounding", "tokens": [50904, 3601, 13, 1079, 11, 457, 767, 11, 498, 291, 519, 466, 309, 11, 2416, 2856, 5245, 365, 4319, 46727, 51164], "temperature": 0.0, "avg_logprob": -0.11725990295410156, "compression_ratio": 1.983402489626556, "no_speech_prob": 0.0023603509180247784}, {"id": 42, "seek": 22408, "start": 240.08, "end": 246.0, "text": " also has domain specific knowledge. Okay, evolving language. This is something that is quite", "tokens": [51164, 611, 575, 9274, 2685, 3601, 13, 1033, 11, 21085, 2856, 13, 639, 307, 746, 300, 307, 1596, 51460], "temperature": 0.0, "avg_logprob": -0.11725990295410156, "compression_ratio": 1.983402489626556, "no_speech_prob": 0.0023603509180247784}, {"id": 43, "seek": 22408, "start": 246.0, "end": 250.72000000000003, "text": " interesting. Large language models don't really have this evolving language unless you fine tune", "tokens": [51460, 1880, 13, 33092, 2856, 5245, 500, 380, 534, 362, 341, 21085, 2856, 5969, 291, 2489, 10864, 51696], "temperature": 0.0, "avg_logprob": -0.11725990295410156, "compression_ratio": 1.983402489626556, "no_speech_prob": 0.0023603509180247784}, {"id": 44, "seek": 25072, "start": 250.72, "end": 256.32, "text": " it. Maybe the recent Lamato you can fine tune on something. Okay, but you can also use something", "tokens": [50364, 309, 13, 2704, 264, 5162, 18825, 2513, 291, 393, 2489, 10864, 322, 746, 13, 1033, 11, 457, 291, 393, 611, 764, 746, 50644], "temperature": 0.0, "avg_logprob": -0.1275630777532404, "compression_ratio": 1.8835341365461848, "no_speech_prob": 0.006191824562847614}, {"id": 45, "seek": 25072, "start": 256.32, "end": 260.24, "text": " like a knowledge graph to ground the large language models. Can you see the synergy here?", "tokens": [50644, 411, 257, 3601, 4295, 281, 2727, 264, 2416, 2856, 5245, 13, 1664, 291, 536, 264, 50163, 510, 30, 50840], "temperature": 0.0, "avg_logprob": -0.1275630777532404, "compression_ratio": 1.8835341365461848, "no_speech_prob": 0.006191824562847614}, {"id": 46, "seek": 25072, "start": 260.24, "end": 267.36, "text": " There's a lot of things that knowledge graphs do well, that is not exactly antagonistic or not", "tokens": [50840, 821, 311, 257, 688, 295, 721, 300, 3601, 24877, 360, 731, 11, 300, 307, 406, 2293, 32590, 3142, 420, 406, 51196], "temperature": 0.0, "avg_logprob": -0.1275630777532404, "compression_ratio": 1.8835341365461848, "no_speech_prob": 0.006191824562847614}, {"id": 47, "seek": 25072, "start": 267.36, "end": 271.84, "text": " exactly different in nature from the large language model, it can just be used to ground", "tokens": [51196, 2293, 819, 294, 3687, 490, 264, 2416, 2856, 2316, 11, 309, 393, 445, 312, 1143, 281, 2727, 51420], "temperature": 0.0, "avg_logprob": -0.1275630777532404, "compression_ratio": 1.8835341365461848, "no_speech_prob": 0.006191824562847614}, {"id": 48, "seek": 25072, "start": 271.84, "end": 277.28, "text": " the large language model. So it's very interesting. So what are the cons of large language models?", "tokens": [51420, 264, 2416, 2856, 2316, 13, 407, 309, 311, 588, 1880, 13, 407, 437, 366, 264, 1014, 295, 2416, 2856, 5245, 30, 51692], "temperature": 0.0, "avg_logprob": -0.1275630777532404, "compression_ratio": 1.8835341365461848, "no_speech_prob": 0.006191824562847614}, {"id": 49, "seek": 27728, "start": 277.28, "end": 283.03999999999996, "text": " They hallucinate, black box, black domain specific knowledge. So it looks like there can be some", "tokens": [50364, 814, 35212, 13923, 11, 2211, 2424, 11, 2211, 9274, 2685, 3601, 13, 407, 309, 1542, 411, 456, 393, 312, 512, 50652], "temperature": 0.0, "avg_logprob": -0.15762687813151965, "compression_ratio": 1.510204081632653, "no_speech_prob": 0.0011773145524784923}, {"id": 50, "seek": 27728, "start": 283.03999999999996, "end": 288.96, "text": " synergy here. And let's explore how we can synergize these two approaches. Before we move on any", "tokens": [50652, 50163, 510, 13, 400, 718, 311, 6839, 577, 321, 393, 33781, 70, 1125, 613, 732, 11587, 13, 4546, 321, 1286, 322, 604, 50948], "temperature": 0.0, "avg_logprob": -0.15762687813151965, "compression_ratio": 1.510204081632653, "no_speech_prob": 0.0011773145524784923}, {"id": 51, "seek": 27728, "start": 288.96, "end": 299.2, "text": " quick questions so far. Okay, so this is the one way of getting context into a large language", "tokens": [50948, 1702, 1651, 370, 1400, 13, 1033, 11, 370, 341, 307, 264, 472, 636, 295, 1242, 4319, 666, 257, 2416, 2856, 51460], "temperature": 0.0, "avg_logprob": -0.15762687813151965, "compression_ratio": 1.510204081632653, "no_speech_prob": 0.0011773145524784923}, {"id": 52, "seek": 27728, "start": 299.2, "end": 304.15999999999997, "text": " model and is used very often nowadays. It's called retrieval augmented generation.", "tokens": [51460, 2316, 293, 307, 1143, 588, 2049, 13434, 13, 467, 311, 1219, 19817, 3337, 36155, 5125, 13, 51708], "temperature": 0.0, "avg_logprob": -0.15762687813151965, "compression_ratio": 1.510204081632653, "no_speech_prob": 0.0011773145524784923}, {"id": 53, "seek": 30416, "start": 304.16, "end": 311.76000000000005, "text": " So this is the raw format you retrieve from a corpus of documents. You have a few documents", "tokens": [50364, 407, 341, 307, 264, 8936, 7877, 291, 30254, 490, 257, 1181, 31624, 295, 8512, 13, 509, 362, 257, 1326, 8512, 50744], "temperature": 0.0, "avg_logprob": -0.15134349323454357, "compression_ratio": 1.56, "no_speech_prob": 0.0018011878710240126}, {"id": 54, "seek": 30416, "start": 311.76000000000005, "end": 318.56, "text": " here that you can retrieve from. And then maybe the user asks like, how much is a MacBook Pro?", "tokens": [50744, 510, 300, 291, 393, 30254, 490, 13, 400, 550, 1310, 264, 4195, 8962, 411, 11, 577, 709, 307, 257, 31737, 1705, 30, 51084], "temperature": 0.0, "avg_logprob": -0.15134349323454357, "compression_ratio": 1.56, "no_speech_prob": 0.0018011878710240126}, {"id": 55, "seek": 30416, "start": 322.88, "end": 325.92, "text": " Right, recently I need to ask myself this question because I'm considering whether I", "tokens": [51300, 1779, 11, 3938, 286, 643, 281, 1029, 2059, 341, 1168, 570, 286, 478, 8079, 1968, 286, 51452], "temperature": 0.0, "avg_logprob": -0.15134349323454357, "compression_ratio": 1.56, "no_speech_prob": 0.0018011878710240126}, {"id": 56, "seek": 30416, "start": 325.92, "end": 330.72, "text": " should buy another one. So you know, they retrieve the relevant documents like,", "tokens": [51452, 820, 2256, 1071, 472, 13, 407, 291, 458, 11, 436, 30254, 264, 7340, 8512, 411, 11, 51692], "temperature": 0.0, "avg_logprob": -0.15134349323454357, "compression_ratio": 1.56, "no_speech_prob": 0.0018011878710240126}, {"id": 57, "seek": 33072, "start": 330.72, "end": 334.64000000000004, "text": " okay, this document is about MacBook. Okay, you retrieve the right documents.", "tokens": [50364, 1392, 11, 341, 4166, 307, 466, 31737, 13, 1033, 11, 291, 30254, 264, 558, 8512, 13, 50560], "temperature": 0.0, "avg_logprob": -0.17299054463704427, "compression_ratio": 1.7350993377483444, "no_speech_prob": 0.006040181498974562}, {"id": 58, "seek": 33072, "start": 336.72, "end": 342.56, "text": " Okay, this document here is about maybe 2019. You can retrieve the right documents. All these", "tokens": [50664, 1033, 11, 341, 4166, 510, 307, 466, 1310, 6071, 13, 509, 393, 30254, 264, 558, 8512, 13, 1057, 613, 50956], "temperature": 0.0, "avg_logprob": -0.17299054463704427, "compression_ratio": 1.7350993377483444, "no_speech_prob": 0.006040181498974562}, {"id": 59, "seek": 33072, "start": 343.68, "end": 350.72, "text": " documents will actually be your context over here. So you could have the context retrieves", "tokens": [51012, 8512, 486, 767, 312, 428, 4319, 670, 510, 13, 407, 291, 727, 362, 264, 4319, 19817, 977, 51364], "temperature": 0.0, "avg_logprob": -0.17299054463704427, "compression_ratio": 1.7350993377483444, "no_speech_prob": 0.006040181498974562}, {"id": 60, "seek": 35072, "start": 350.72, "end": 361.20000000000005, "text": " like that. MacBook Pro 2019 costs 5000 or something like that. Then you can have like in 2019", "tokens": [50364, 411, 300, 13, 31737, 1705, 6071, 5497, 23777, 420, 746, 411, 300, 13, 1396, 291, 393, 362, 411, 294, 6071, 50888], "temperature": 0.0, "avg_logprob": -0.19318450580943713, "compression_ratio": 1.4574468085106382, "no_speech_prob": 0.027485176920890808}, {"id": 61, "seek": 35072, "start": 364.64000000000004, "end": 372.16, "text": " Apple release MacBook Air 2019. Of course, I mean, I don't really know the details, but let's say", "tokens": [51060, 6373, 4374, 31737, 5774, 6071, 13, 2720, 1164, 11, 286, 914, 11, 286, 500, 380, 534, 458, 264, 4365, 11, 457, 718, 311, 584, 51436], "temperature": 0.0, "avg_logprob": -0.19318450580943713, "compression_ratio": 1.4574468085106382, "no_speech_prob": 0.027485176920890808}, {"id": 62, "seek": 35072, "start": 372.16, "end": 376.24, "text": " these are the two documents you retrieve from your retrieval augmented generation.", "tokens": [51436, 613, 366, 264, 732, 8512, 291, 30254, 490, 428, 19817, 3337, 36155, 5125, 13, 51640], "temperature": 0.0, "avg_logprob": -0.19318450580943713, "compression_ratio": 1.4574468085106382, "no_speech_prob": 0.027485176920890808}, {"id": 63, "seek": 37624, "start": 376.88, "end": 385.68, "text": " Okay, and after that, you ask the question like, how much is a MacBook Pro 2019? So it's been shown", "tokens": [50396, 1033, 11, 293, 934, 300, 11, 291, 1029, 264, 1168, 411, 11, 577, 709, 307, 257, 31737, 1705, 6071, 30, 407, 309, 311, 668, 4898, 50836], "temperature": 0.0, "avg_logprob": -0.11069511745287025, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.002546277828514576}, {"id": 64, "seek": 37624, "start": 385.68, "end": 390.24, "text": " that if you use retrieval augmented generation, you can improve the consistency of the output", "tokens": [50836, 300, 498, 291, 764, 19817, 3337, 36155, 5125, 11, 291, 393, 3470, 264, 14416, 295, 264, 5598, 51064], "temperature": 0.0, "avg_logprob": -0.11069511745287025, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.002546277828514576}, {"id": 65, "seek": 37624, "start": 390.24, "end": 395.04, "text": " of the large language model because you are grounding it in the earlier context, which is", "tokens": [51064, 295, 264, 2416, 2856, 2316, 570, 291, 366, 46727, 309, 294, 264, 3071, 4319, 11, 597, 307, 51304], "temperature": 0.0, "avg_logprob": -0.11069511745287025, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.002546277828514576}, {"id": 66, "seek": 37624, "start": 395.04, "end": 399.52, "text": " this part here, you are grounding it in this part here. So there's an element of grounding and", "tokens": [51304, 341, 644, 510, 11, 291, 366, 46727, 309, 294, 341, 644, 510, 13, 407, 456, 311, 364, 4478, 295, 46727, 293, 51528], "temperature": 0.0, "avg_logprob": -0.11069511745287025, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.002546277828514576}, {"id": 67, "seek": 37624, "start": 399.52, "end": 404.0, "text": " this is very important for a lot of real world use cases. Because if you don't ground it, you can", "tokens": [51528, 341, 307, 588, 1021, 337, 257, 688, 295, 957, 1002, 764, 3331, 13, 1436, 498, 291, 500, 380, 2727, 309, 11, 291, 393, 51752], "temperature": 0.0, "avg_logprob": -0.11069511745287025, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.002546277828514576}, {"id": 68, "seek": 40400, "start": 404.0, "end": 410.72, "text": " end up with quite nonsense generations. Alright, and just as a refresher, okay, what is the most", "tokens": [50364, 917, 493, 365, 1596, 14925, 10593, 13, 2798, 11, 293, 445, 382, 257, 17368, 511, 11, 1392, 11, 437, 307, 264, 881, 50700], "temperature": 0.0, "avg_logprob": -0.1710827049167677, "compression_ratio": 1.7652582159624413, "no_speech_prob": 0.0038881171494722366}, {"id": 69, "seek": 40400, "start": 410.72, "end": 416.24, "text": " common method used to select the top K like documents? Anyone can just blow talk? What's the", "tokens": [50700, 2689, 3170, 1143, 281, 3048, 264, 1192, 591, 411, 8512, 30, 14643, 393, 445, 6327, 751, 30, 708, 311, 264, 50976], "temperature": 0.0, "avg_logprob": -0.1710827049167677, "compression_ratio": 1.7652582159624413, "no_speech_prob": 0.0038881171494722366}, {"id": 70, "seek": 40400, "start": 416.24, "end": 423.68, "text": " most common metric to select the most relevant documents? That's a test of understanding. If", "tokens": [50976, 881, 2689, 20678, 281, 3048, 264, 881, 7340, 8512, 30, 663, 311, 257, 1500, 295, 3701, 13, 759, 51348], "temperature": 0.0, "avg_logprob": -0.1710827049167677, "compression_ratio": 1.7652582159624413, "no_speech_prob": 0.0038881171494722366}, {"id": 71, "seek": 40400, "start": 423.68, "end": 428.16, "text": " you are doing retrieval augmented generation, what is the most common metric used to retrieve", "tokens": [51348, 291, 366, 884, 19817, 3337, 36155, 5125, 11, 437, 307, 264, 881, 2689, 20678, 1143, 281, 30254, 51572], "temperature": 0.0, "avg_logprob": -0.1710827049167677, "compression_ratio": 1.7652582159624413, "no_speech_prob": 0.0038881171494722366}, {"id": 72, "seek": 42816, "start": 428.16, "end": 436.8, "text": " documents? To check the similarity. Anyone? You can write in your chat also. Dot product,", "tokens": [50364, 8512, 30, 1407, 1520, 264, 32194, 13, 14643, 30, 509, 393, 2464, 294, 428, 5081, 611, 13, 38753, 1674, 11, 50796], "temperature": 0.0, "avg_logprob": -0.21419295397671786, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.005111770238727331}, {"id": 73, "seek": 42816, "start": 436.8, "end": 442.40000000000003, "text": " yes, very good. Dot product or cosine similarity. That's right. So usually we use some form of", "tokens": [50796, 2086, 11, 588, 665, 13, 38753, 1674, 420, 23565, 32194, 13, 663, 311, 558, 13, 407, 2673, 321, 764, 512, 1254, 295, 51076], "temperature": 0.0, "avg_logprob": -0.21419295397671786, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.005111770238727331}, {"id": 74, "seek": 42816, "start": 442.40000000000003, "end": 448.88, "text": " embeddings. You embed your documents into a vector and then you use cosine similarity", "tokens": [51076, 12240, 29432, 13, 509, 12240, 428, 8512, 666, 257, 8062, 293, 550, 291, 764, 23565, 32194, 51400], "temperature": 0.0, "avg_logprob": -0.21419295397671786, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.005111770238727331}, {"id": 75, "seek": 44888, "start": 449.84, "end": 460.0, "text": " to check how similar the document is compared to the query. Okay, I'm going into some details", "tokens": [50412, 281, 1520, 577, 2531, 264, 4166, 307, 5347, 281, 264, 14581, 13, 1033, 11, 286, 478, 516, 666, 512, 4365, 50920], "temperature": 0.0, "avg_logprob": -0.14999276399612427, "compression_ratio": 1.731818181818182, "no_speech_prob": 0.022534523159265518}, {"id": 76, "seek": 44888, "start": 460.0, "end": 464.96, "text": " over here. Okay, because actually this whole process of doing retrieval augmented generation", "tokens": [50920, 670, 510, 13, 1033, 11, 570, 767, 341, 1379, 1399, 295, 884, 19817, 3337, 36155, 5125, 51168], "temperature": 0.0, "avg_logprob": -0.14999276399612427, "compression_ratio": 1.731818181818182, "no_speech_prob": 0.022534523159265518}, {"id": 77, "seek": 44888, "start": 464.96, "end": 471.2, "text": " and passing over knowledge graphs is very, very similar. Alright, in fact, you could even replace", "tokens": [51168, 293, 8437, 670, 3601, 24877, 307, 588, 11, 588, 2531, 13, 2798, 11, 294, 1186, 11, 291, 727, 754, 7406, 51480], "temperature": 0.0, "avg_logprob": -0.14999276399612427, "compression_ratio": 1.731818181818182, "no_speech_prob": 0.022534523159265518}, {"id": 78, "seek": 44888, "start": 471.2, "end": 476.08, "text": " this retrieval augmented generation with like knowledge graph augmented generation is perfectly,", "tokens": [51480, 341, 19817, 3337, 36155, 5125, 365, 411, 3601, 4295, 36155, 5125, 307, 6239, 11, 51724], "temperature": 0.0, "avg_logprob": -0.14999276399612427, "compression_ratio": 1.731818181818182, "no_speech_prob": 0.022534523159265518}, {"id": 79, "seek": 47608, "start": 476.88, "end": 484.08, "text": " I think it's replaceable. Alright, so this is some idea of how large language models can be made", "tokens": [50404, 286, 519, 309, 311, 7406, 712, 13, 2798, 11, 370, 341, 307, 512, 1558, 295, 577, 2416, 2856, 5245, 393, 312, 1027, 50764], "temperature": 0.0, "avg_logprob": -0.13010018666585285, "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.0012637900654226542}, {"id": 80, "seek": 47608, "start": 484.08, "end": 491.52, "text": " to be more accurate. Okay, using something like that. Okay, so this again, I just highlight the", "tokens": [50764, 281, 312, 544, 8559, 13, 1033, 11, 1228, 746, 411, 300, 13, 1033, 11, 370, 341, 797, 11, 286, 445, 5078, 264, 51136], "temperature": 0.0, "avg_logprob": -0.13010018666585285, "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.0012637900654226542}, {"id": 81, "seek": 47608, "start": 491.52, "end": 495.59999999999997, "text": " problems of large language models. Okay, may not be able to recall the knowledge, but you can retrieve", "tokens": [51136, 2740, 295, 2416, 2856, 5245, 13, 1033, 11, 815, 406, 312, 1075, 281, 9901, 264, 3601, 11, 457, 291, 393, 30254, 51340], "temperature": 0.0, "avg_logprob": -0.13010018666585285, "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.0012637900654226542}, {"id": 82, "seek": 47608, "start": 495.59999999999997, "end": 500.4, "text": " the right context using this retrieval augmented generation provided you can retrieve the context", "tokens": [51340, 264, 558, 4319, 1228, 341, 19817, 3337, 36155, 5125, 5649, 291, 393, 30254, 264, 4319, 51580], "temperature": 0.0, "avg_logprob": -0.13010018666585285, "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.0012637900654226542}, {"id": 83, "seek": 50040, "start": 500.4, "end": 506.88, "text": " currently. Alright, so this is a real world use case issue. Alright, I've talked to some people", "tokens": [50364, 4362, 13, 2798, 11, 370, 341, 307, 257, 957, 1002, 764, 1389, 2734, 13, 2798, 11, 286, 600, 2825, 281, 512, 561, 50688], "temperature": 0.0, "avg_logprob": -0.13187281868674539, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.008921629749238491}, {"id": 84, "seek": 50040, "start": 506.88, "end": 512.24, "text": " and they say that retrieval augmented generation with just the cosine similarity alone, okay,", "tokens": [50688, 293, 436, 584, 300, 19817, 3337, 36155, 5125, 365, 445, 264, 23565, 32194, 3312, 11, 1392, 11, 50956], "temperature": 0.0, "avg_logprob": -0.13187281868674539, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.008921629749238491}, {"id": 85, "seek": 50040, "start": 512.24, "end": 517.12, "text": " might not give you the right documents. So, you know, the embedding vectors train using", "tokens": [50956, 1062, 406, 976, 291, 264, 558, 8512, 13, 407, 11, 291, 458, 11, 264, 12240, 3584, 18875, 3847, 1228, 51200], "temperature": 0.0, "avg_logprob": -0.13187281868674539, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.008921629749238491}, {"id": 86, "seek": 50040, "start": 517.12, "end": 522.0, "text": " contrastive loss, you know, they may not capture everything, especially if your document is very,", "tokens": [51200, 8712, 488, 4470, 11, 291, 458, 11, 436, 815, 406, 7983, 1203, 11, 2318, 498, 428, 4166, 307, 588, 11, 51444], "temperature": 0.0, "avg_logprob": -0.13187281868674539, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.008921629749238491}, {"id": 87, "seek": 50040, "start": 522.0, "end": 526.72, "text": " very large. Okay, imagine you have only one vector to represent the entire document, and you have", "tokens": [51444, 588, 2416, 13, 1033, 11, 3811, 291, 362, 787, 472, 8062, 281, 2906, 264, 2302, 4166, 11, 293, 291, 362, 51680], "temperature": 0.0, "avg_logprob": -0.13187281868674539, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.008921629749238491}, {"id": 88, "seek": 52672, "start": 526.72, "end": 531.6800000000001, "text": " another vector to represent document, another document. So this is like document A, and another", "tokens": [50364, 1071, 8062, 281, 2906, 4166, 11, 1071, 4166, 13, 407, 341, 307, 411, 4166, 316, 11, 293, 1071, 50612], "temperature": 0.0, "avg_logprob": -0.15788223062242782, "compression_ratio": 2.085201793721973, "no_speech_prob": 0.0016939268680289388}, {"id": 89, "seek": 52672, "start": 531.6800000000001, "end": 536.5600000000001, "text": " vector to represent document B, then you see how similar they are. But what about like,", "tokens": [50612, 8062, 281, 2906, 4166, 363, 11, 550, 291, 536, 577, 2531, 436, 366, 13, 583, 437, 466, 411, 11, 50856], "temperature": 0.0, "avg_logprob": -0.15788223062242782, "compression_ratio": 2.085201793721973, "no_speech_prob": 0.0016939268680289388}, {"id": 90, "seek": 52672, "start": 537.76, "end": 544.8000000000001, "text": " what if one document contains many parts? Right, I mean, each of these parts could have", "tokens": [50916, 437, 498, 472, 4166, 8306, 867, 3166, 30, 1779, 11, 286, 914, 11, 1184, 295, 613, 3166, 727, 362, 51268], "temperature": 0.0, "avg_logprob": -0.15788223062242782, "compression_ratio": 2.085201793721973, "no_speech_prob": 0.0016939268680289388}, {"id": 91, "seek": 52672, "start": 544.8000000000001, "end": 550.4, "text": " different meanings, right? Each of these sub parts could contain like, let's say you have this", "tokens": [51268, 819, 28138, 11, 558, 30, 6947, 295, 613, 1422, 3166, 727, 5304, 411, 11, 718, 311, 584, 291, 362, 341, 51548], "temperature": 0.0, "avg_logprob": -0.15788223062242782, "compression_ratio": 2.085201793721973, "no_speech_prob": 0.0016939268680289388}, {"id": 92, "seek": 52672, "start": 550.4, "end": 554.1600000000001, "text": " document could have a sub part that is like that, a sub part that is like that, a sub part that is", "tokens": [51548, 4166, 727, 362, 257, 1422, 644, 300, 307, 411, 300, 11, 257, 1422, 644, 300, 307, 411, 300, 11, 257, 1422, 644, 300, 307, 51736], "temperature": 0.0, "avg_logprob": -0.15788223062242782, "compression_ratio": 2.085201793721973, "no_speech_prob": 0.0016939268680289388}, {"id": 93, "seek": 55416, "start": 554.24, "end": 559.1999999999999, "text": " like that. You know, they just aggregate all of this together into one vector, like that.", "tokens": [50368, 411, 300, 13, 509, 458, 11, 436, 445, 26118, 439, 295, 341, 1214, 666, 472, 8062, 11, 411, 300, 13, 50616], "temperature": 0.0, "avg_logprob": -0.11844170945031303, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.00284477137029171}, {"id": 94, "seek": 55416, "start": 559.1999999999999, "end": 563.8399999999999, "text": " Can you see that you're actually losing like information here, which means that when you", "tokens": [50616, 1664, 291, 536, 300, 291, 434, 767, 7027, 411, 1589, 510, 11, 597, 1355, 300, 562, 291, 50848], "temperature": 0.0, "avg_logprob": -0.11844170945031303, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.00284477137029171}, {"id": 95, "seek": 55416, "start": 563.8399999999999, "end": 568.64, "text": " retrieve something, let's say if I want to find out how to code, like a length chain question", "tokens": [50848, 30254, 746, 11, 718, 311, 584, 498, 286, 528, 281, 915, 484, 577, 281, 3089, 11, 411, 257, 4641, 5021, 1168, 51088], "temperature": 0.0, "avg_logprob": -0.11844170945031303, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.00284477137029171}, {"id": 96, "seek": 55416, "start": 568.64, "end": 574.0, "text": " answer agent, you know, I'm not going to retrieve this vector because by vector similarity, my query", "tokens": [51088, 1867, 9461, 11, 291, 458, 11, 286, 478, 406, 516, 281, 30254, 341, 8062, 570, 538, 8062, 32194, 11, 452, 14581, 51356], "temperature": 0.0, "avg_logprob": -0.11844170945031303, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.00284477137029171}, {"id": 97, "seek": 55416, "start": 574.0, "end": 579.92, "text": " is here. Alright, by vector similarity, maybe I'll retrieve a document that is like B instead,", "tokens": [51356, 307, 510, 13, 2798, 11, 538, 8062, 32194, 11, 1310, 286, 603, 30254, 257, 4166, 300, 307, 411, 363, 2602, 11, 51652], "temperature": 0.0, "avg_logprob": -0.11844170945031303, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.00284477137029171}, {"id": 98, "seek": 57992, "start": 579.92, "end": 584.4799999999999, "text": " because like maybe it's nearer in terms of cosine similarity. Okay, I mean, it's greater", "tokens": [50364, 570, 411, 1310, 309, 311, 2651, 260, 294, 2115, 295, 23565, 32194, 13, 1033, 11, 286, 914, 11, 309, 311, 5044, 50592], "temperature": 0.0, "avg_logprob": -0.1828958741549788, "compression_ratio": 1.6468401486988848, "no_speech_prob": 0.0020982578862458467}, {"id": 99, "seek": 57992, "start": 584.4799999999999, "end": 589.4399999999999, "text": " is the opposite direction. Let me just make this vector look more similar to that. Like,", "tokens": [50592, 307, 264, 6182, 3513, 13, 961, 385, 445, 652, 341, 8062, 574, 544, 2531, 281, 300, 13, 1743, 11, 50840], "temperature": 0.0, "avg_logprob": -0.1828958741549788, "compression_ratio": 1.6468401486988848, "no_speech_prob": 0.0020982578862458467}, {"id": 100, "seek": 57992, "start": 590.24, "end": 597.04, "text": " let's say I have B is like that. So if this direction here is like how to code a length chain,", "tokens": [50880, 718, 311, 584, 286, 362, 363, 307, 411, 300, 13, 407, 498, 341, 3513, 510, 307, 411, 577, 281, 3089, 257, 4641, 5021, 11, 51220], "temperature": 0.0, "avg_logprob": -0.1828958741549788, "compression_ratio": 1.6468401486988848, "no_speech_prob": 0.0020982578862458467}, {"id": 101, "seek": 57992, "start": 598.24, "end": 603.28, "text": " QA agent, and this is the embedding vector for it, it goes in this direction.", "tokens": [51280, 1249, 32, 9461, 11, 293, 341, 307, 264, 12240, 3584, 8062, 337, 309, 11, 309, 1709, 294, 341, 3513, 13, 51532], "temperature": 0.0, "avg_logprob": -0.1828958741549788, "compression_ratio": 1.6468401486988848, "no_speech_prob": 0.0020982578862458467}, {"id": 102, "seek": 57992, "start": 604.0799999999999, "end": 607.68, "text": " You know, you're not going to retrieve document A, although it contains that part over here.", "tokens": [51572, 509, 458, 11, 291, 434, 406, 516, 281, 30254, 4166, 316, 11, 4878, 309, 8306, 300, 644, 670, 510, 13, 51752], "temperature": 0.0, "avg_logprob": -0.1828958741549788, "compression_ratio": 1.6468401486988848, "no_speech_prob": 0.0020982578862458467}, {"id": 103, "seek": 60768, "start": 607.68, "end": 612.4, "text": " Okay, you're going to retrieve document B. This is one of the failings of the embedding", "tokens": [50364, 1033, 11, 291, 434, 516, 281, 30254, 4166, 363, 13, 639, 307, 472, 295, 264, 3061, 1109, 295, 264, 12240, 3584, 50600], "temperature": 0.0, "avg_logprob": -0.11851452914151278, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.0015251418808475137}, {"id": 104, "seek": 60768, "start": 612.4, "end": 617.3599999999999, "text": " vector. It just tries to capture the whole document into one vector. And this means that", "tokens": [50600, 8062, 13, 467, 445, 9898, 281, 7983, 264, 1379, 4166, 666, 472, 8062, 13, 400, 341, 1355, 300, 50848], "temperature": 0.0, "avg_logprob": -0.11851452914151278, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.0015251418808475137}, {"id": 105, "seek": 60768, "start": 617.3599999999999, "end": 623.28, "text": " you may not be able to extract stuff out. Okay, Richard said something. This is why I keep saying", "tokens": [50848, 291, 815, 406, 312, 1075, 281, 8947, 1507, 484, 13, 1033, 11, 9809, 848, 746, 13, 639, 307, 983, 286, 1066, 1566, 51144], "temperature": 0.0, "avg_logprob": -0.11851452914151278, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.0015251418808475137}, {"id": 106, "seek": 60768, "start": 623.28, "end": 629.92, "text": " context is king. Summarization is essentially impossible on segmentation on segmented documents.", "tokens": [51144, 4319, 307, 4867, 13, 8626, 6209, 2144, 307, 4476, 6243, 322, 9469, 399, 322, 9469, 292, 8512, 13, 51476], "temperature": 0.0, "avg_logprob": -0.11851452914151278, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.0015251418808475137}, {"id": 107, "seek": 60768, "start": 630.7199999999999, "end": 636.4799999999999, "text": " Yeah, definitely, because you summarize you lose information. Okay, so there needs to be like", "tokens": [51516, 865, 11, 2138, 11, 570, 291, 20858, 291, 3624, 1589, 13, 1033, 11, 370, 456, 2203, 281, 312, 411, 51804], "temperature": 0.0, "avg_logprob": -0.11851452914151278, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.0015251418808475137}, {"id": 108, "seek": 63648, "start": 636.48, "end": 642.32, "text": " different hierarchies of how you retrieve things out, broad level, specific level. And you know what?", "tokens": [50364, 819, 35250, 530, 295, 577, 291, 30254, 721, 484, 11, 4152, 1496, 11, 2685, 1496, 13, 400, 291, 458, 437, 30, 50656], "temperature": 0.0, "avg_logprob": -0.1473240511757987, "compression_ratio": 1.686206896551724, "no_speech_prob": 0.002035762183368206}, {"id": 109, "seek": 63648, "start": 643.44, "end": 647.52, "text": " Knowledge graphs might actually have that kind of hierarchy formulation. I'm actually jumping", "tokens": [50712, 32906, 24877, 1062, 767, 362, 300, 733, 295, 22333, 37642, 13, 286, 478, 767, 11233, 50916], "temperature": 0.0, "avg_logprob": -0.1473240511757987, "compression_ratio": 1.686206896551724, "no_speech_prob": 0.002035762183368206}, {"id": 110, "seek": 63648, "start": 647.52, "end": 653.84, "text": " a few slides ahead, but give you an idea of why I'm so excited about this idea. All right. So", "tokens": [50916, 257, 1326, 9788, 2286, 11, 457, 976, 291, 364, 1558, 295, 983, 286, 478, 370, 2919, 466, 341, 1558, 13, 1057, 558, 13, 407, 51232], "temperature": 0.0, "avg_logprob": -0.1473240511757987, "compression_ratio": 1.686206896551724, "no_speech_prob": 0.002035762183368206}, {"id": 111, "seek": 63648, "start": 653.84, "end": 657.6800000000001, "text": " actually some of the bypass that I've been telling people, I've been advising people is that like", "tokens": [51232, 767, 512, 295, 264, 24996, 300, 286, 600, 668, 3585, 561, 11, 286, 600, 668, 35598, 561, 307, 300, 411, 51424], "temperature": 0.0, "avg_logprob": -0.1473240511757987, "compression_ratio": 1.686206896551724, "no_speech_prob": 0.002035762183368206}, {"id": 112, "seek": 63648, "start": 657.6800000000001, "end": 663.44, "text": " if you cannot get retrieval of mental generation to work, consider using like filters or like labels.", "tokens": [51424, 498, 291, 2644, 483, 19817, 3337, 295, 4973, 5125, 281, 589, 11, 1949, 1228, 411, 15995, 420, 411, 16949, 13, 51712], "temperature": 0.0, "avg_logprob": -0.1473240511757987, "compression_ratio": 1.686206896551724, "no_speech_prob": 0.002035762183368206}, {"id": 113, "seek": 66344, "start": 663.6, "end": 672.1600000000001, "text": " So like this labels will say like, okay, maybe it's like product A, product B. So you know,", "tokens": [50372, 407, 411, 341, 16949, 486, 584, 411, 11, 1392, 11, 1310, 309, 311, 411, 1674, 316, 11, 1674, 363, 13, 407, 291, 458, 11, 50800], "temperature": 0.0, "avg_logprob": -0.18263344142747961, "compression_ratio": 1.6484018264840183, "no_speech_prob": 0.0009413243387825787}, {"id": 114, "seek": 66344, "start": 672.1600000000001, "end": 677.7600000000001, "text": " instead of relying on just the retrieval of mental generation, or the embedding vector", "tokens": [50800, 2602, 295, 24140, 322, 445, 264, 19817, 3337, 295, 4973, 5125, 11, 420, 264, 12240, 3584, 8062, 51080], "temperature": 0.0, "avg_logprob": -0.18263344142747961, "compression_ratio": 1.6484018264840183, "no_speech_prob": 0.0009413243387825787}, {"id": 115, "seek": 66344, "start": 677.7600000000001, "end": 681.6800000000001, "text": " to actually embed the right knowledge, like let's say you have a length chain QA agent,", "tokens": [51080, 281, 767, 12240, 264, 558, 3601, 11, 411, 718, 311, 584, 291, 362, 257, 4641, 5021, 1249, 32, 9461, 11, 51276], "temperature": 0.0, "avg_logprob": -0.18263344142747961, "compression_ratio": 1.6484018264840183, "no_speech_prob": 0.0009413243387825787}, {"id": 116, "seek": 66344, "start": 681.6800000000001, "end": 687.5200000000001, "text": " I can tag this thing as a length chain QA agent inside this document. So there will be certain", "tokens": [51276, 286, 393, 6162, 341, 551, 382, 257, 4641, 5021, 1249, 32, 9461, 1854, 341, 4166, 13, 407, 456, 486, 312, 1629, 51568], "temperature": 0.0, "avg_logprob": -0.18263344142747961, "compression_ratio": 1.6484018264840183, "no_speech_prob": 0.0009413243387825787}, {"id": 117, "seek": 68752, "start": 687.52, "end": 695.1999999999999, "text": " tags that you can have. So then you can then do like the embedding vector across the documents", "tokens": [50364, 18632, 300, 291, 393, 362, 13, 407, 550, 291, 393, 550, 360, 411, 264, 12240, 3584, 8062, 2108, 264, 8512, 50748], "temperature": 0.0, "avg_logprob": -0.15514675099798975, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.0067443824373185635}, {"id": 118, "seek": 68752, "start": 695.1999999999999, "end": 699.92, "text": " that have this text. Yeah, so maybe that's one way to like do a first hand filtering.", "tokens": [50748, 300, 362, 341, 2487, 13, 865, 11, 370, 1310, 300, 311, 472, 636, 281, 411, 360, 257, 700, 1011, 30822, 13, 50984], "temperature": 0.0, "avg_logprob": -0.15514675099798975, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.0067443824373185635}, {"id": 119, "seek": 68752, "start": 699.92, "end": 705.28, "text": " Yeah, I mean, this is just like some, what do you call it, some bypasses to the downsides of", "tokens": [50984, 865, 11, 286, 914, 11, 341, 307, 445, 411, 512, 11, 437, 360, 291, 818, 309, 11, 512, 24996, 279, 281, 264, 21554, 1875, 295, 51252], "temperature": 0.0, "avg_logprob": -0.15514675099798975, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.0067443824373185635}, {"id": 120, "seek": 68752, "start": 705.84, "end": 712.16, "text": " embedding vectors for pros and similarities. So these are some ideas that could be done right", "tokens": [51280, 12240, 3584, 18875, 337, 6267, 293, 24197, 13, 407, 613, 366, 512, 3487, 300, 727, 312, 1096, 558, 51596], "temperature": 0.0, "avg_logprob": -0.15514675099798975, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.0067443824373185635}, {"id": 121, "seek": 71216, "start": 712.16, "end": 718.16, "text": " now to bypass it. But if we have a way to use knowledge graph to do more broad level to more", "tokens": [50364, 586, 281, 24996, 309, 13, 583, 498, 321, 362, 257, 636, 281, 764, 3601, 4295, 281, 360, 544, 4152, 1496, 281, 544, 50664], "temperature": 0.0, "avg_logprob": -0.10863354724386463, "compression_ratio": 1.7695167286245352, "no_speech_prob": 0.0012549760285764933}, {"id": 122, "seek": 71216, "start": 718.16, "end": 722.7199999999999, "text": " specific level extraction, maybe you don't even need all this, you can just pass through your", "tokens": [50664, 2685, 1496, 30197, 11, 1310, 291, 500, 380, 754, 643, 439, 341, 11, 291, 393, 445, 1320, 807, 428, 50892], "temperature": 0.0, "avg_logprob": -0.10863354724386463, "compression_ratio": 1.7695167286245352, "no_speech_prob": 0.0012549760285764933}, {"id": 123, "seek": 71216, "start": 722.7199999999999, "end": 727.8399999999999, "text": " knowledge graph and you can use that to ground the large language model. All right, so this is my", "tokens": [50892, 3601, 4295, 293, 291, 393, 764, 300, 281, 2727, 264, 2416, 2856, 2316, 13, 1057, 558, 11, 370, 341, 307, 452, 51148], "temperature": 0.0, "avg_logprob": -0.10863354724386463, "compression_ratio": 1.7695167286245352, "no_speech_prob": 0.0012549760285764933}, {"id": 124, "seek": 71216, "start": 727.8399999999999, "end": 733.04, "text": " last point here, knowledge graphs are useful to retrieve the right context, search the right", "tokens": [51148, 1036, 935, 510, 11, 3601, 24877, 366, 4420, 281, 30254, 264, 558, 4319, 11, 3164, 264, 558, 51408], "temperature": 0.0, "avg_logprob": -0.10863354724386463, "compression_ratio": 1.7695167286245352, "no_speech_prob": 0.0012549760285764933}, {"id": 125, "seek": 71216, "start": 733.04, "end": 737.4399999999999, "text": " keywords, retrieve the right subgraph. Like let me give you an example here, if let's say I have a", "tokens": [51408, 21009, 11, 30254, 264, 558, 1422, 34091, 13, 1743, 718, 385, 976, 291, 364, 1365, 510, 11, 498, 718, 311, 584, 286, 362, 257, 51628], "temperature": 0.0, "avg_logprob": -0.10863354724386463, "compression_ratio": 1.7695167286245352, "no_speech_prob": 0.0012549760285764933}, {"id": 126, "seek": 73744, "start": 737.44, "end": 747.5200000000001, "text": " graph like that. All right, so maybe this is a graph talking about like people who view Netflix.", "tokens": [50364, 4295, 411, 300, 13, 1057, 558, 11, 370, 1310, 341, 307, 257, 4295, 1417, 466, 411, 561, 567, 1910, 12778, 13, 50868], "temperature": 0.0, "avg_logprob": -0.20354060934047508, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.003341424511745572}, {"id": 127, "seek": 73744, "start": 747.5200000000001, "end": 753.2800000000001, "text": " Okay, so these are the Netflix user graphs. So these are the users. And then maybe you have me", "tokens": [50868, 1033, 11, 370, 613, 366, 264, 12778, 4195, 24877, 13, 407, 613, 366, 264, 5022, 13, 400, 550, 1310, 291, 362, 385, 51156], "temperature": 0.0, "avg_logprob": -0.20354060934047508, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.003341424511745572}, {"id": 128, "seek": 73744, "start": 753.2800000000001, "end": 759.9200000000001, "text": " over here, John. Okay, and then like the movies are watched. I like to watch the flash, the series,", "tokens": [51156, 670, 510, 11, 2619, 13, 1033, 11, 293, 550, 411, 264, 6233, 366, 6337, 13, 286, 411, 281, 1159, 264, 7319, 11, 264, 2638, 11, 51488], "temperature": 0.0, "avg_logprob": -0.20354060934047508, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.003341424511745572}, {"id": 129, "seek": 73744, "start": 759.9200000000001, "end": 764.1600000000001, "text": " not bad. I highly recommend it. Then maybe we have another guy like maybe Richard can be here,", "tokens": [51488, 406, 1578, 13, 286, 5405, 2748, 309, 13, 1396, 1310, 321, 362, 1071, 2146, 411, 1310, 9809, 393, 312, 510, 11, 51700], "temperature": 0.0, "avg_logprob": -0.20354060934047508, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.003341424511745572}, {"id": 130, "seek": 76416, "start": 764.16, "end": 770.88, "text": " watch other movies, like movie A and movie B, you know, yep. So if you want to like extract", "tokens": [50364, 1159, 661, 6233, 11, 411, 3169, 316, 293, 3169, 363, 11, 291, 458, 11, 18633, 13, 407, 498, 291, 528, 281, 411, 8947, 50700], "temperature": 0.0, "avg_logprob": -0.12888744781757222, "compression_ratio": 1.8414634146341464, "no_speech_prob": 0.004029732663184404}, {"id": 131, "seek": 76416, "start": 770.88, "end": 775.6, "text": " something out here, you can just search like for keywords and then you can just put this whole", "tokens": [50700, 746, 484, 510, 11, 291, 393, 445, 3164, 411, 337, 21009, 293, 550, 291, 393, 445, 829, 341, 1379, 50936], "temperature": 0.0, "avg_logprob": -0.12888744781757222, "compression_ratio": 1.8414634146341464, "no_speech_prob": 0.004029732663184404}, {"id": 132, "seek": 76416, "start": 775.6, "end": 780.4, "text": " subgraph here. And then you can use this part here. Okay, how you want to pass it into the", "tokens": [50936, 1422, 34091, 510, 13, 400, 550, 291, 393, 764, 341, 644, 510, 13, 1033, 11, 577, 291, 528, 281, 1320, 309, 666, 264, 51176], "temperature": 0.0, "avg_logprob": -0.12888744781757222, "compression_ratio": 1.8414634146341464, "no_speech_prob": 0.004029732663184404}, {"id": 133, "seek": 76416, "start": 780.4, "end": 784.7199999999999, "text": " large language model, I leave it for future investigations. There are a few ways to do it.", "tokens": [51176, 2416, 2856, 2316, 11, 286, 1856, 309, 337, 2027, 25582, 13, 821, 366, 257, 1326, 2098, 281, 360, 309, 13, 51392], "temperature": 0.0, "avg_logprob": -0.12888744781757222, "compression_ratio": 1.8414634146341464, "no_speech_prob": 0.004029732663184404}, {"id": 134, "seek": 76416, "start": 784.7199999999999, "end": 788.9599999999999, "text": " I will cover some ways today. So if you can pass this into the large language model,", "tokens": [51392, 286, 486, 2060, 512, 2098, 965, 13, 407, 498, 291, 393, 1320, 341, 666, 264, 2416, 2856, 2316, 11, 51604], "temperature": 0.0, "avg_logprob": -0.12888744781757222, "compression_ratio": 1.8414634146341464, "no_speech_prob": 0.004029732663184404}, {"id": 135, "seek": 78896, "start": 788.96, "end": 796.88, "text": " essentially, you can ground the LM in context of the knowledge graph. And then we can actually do", "tokens": [50364, 4476, 11, 291, 393, 2727, 264, 46529, 294, 4319, 295, 264, 3601, 4295, 13, 400, 550, 321, 393, 767, 360, 50760], "temperature": 0.0, "avg_logprob": -0.13974494519441025, "compression_ratio": 1.722466960352423, "no_speech_prob": 0.007268000394105911}, {"id": 136, "seek": 78896, "start": 796.88, "end": 803.0400000000001, "text": " this grounding at a more higher level grounding or more sub level grounding depends on which height", "tokens": [50760, 341, 46727, 412, 257, 544, 2946, 1496, 46727, 420, 544, 1422, 1496, 46727, 5946, 322, 597, 6681, 51068], "temperature": 0.0, "avg_logprob": -0.13974494519441025, "compression_ratio": 1.722466960352423, "no_speech_prob": 0.007268000394105911}, {"id": 137, "seek": 78896, "start": 803.0400000000001, "end": 807.76, "text": " of the knowledge graph you're going to take the notes from. All right, so I think this is a very", "tokens": [51068, 295, 264, 3601, 4295, 291, 434, 516, 281, 747, 264, 5570, 490, 13, 1057, 558, 11, 370, 286, 519, 341, 307, 257, 588, 51304], "temperature": 0.0, "avg_logprob": -0.13974494519441025, "compression_ratio": 1.722466960352423, "no_speech_prob": 0.007268000394105911}, {"id": 138, "seek": 78896, "start": 807.76, "end": 813.76, "text": " exciting prospect. And yeah, I'm looking forward to see like how this can actually work. So I'll", "tokens": [51304, 4670, 15005, 13, 400, 1338, 11, 286, 478, 1237, 2128, 281, 536, 411, 577, 341, 393, 767, 589, 13, 407, 286, 603, 51604], "temperature": 0.0, "avg_logprob": -0.13974494519441025, "compression_ratio": 1.722466960352423, "no_speech_prob": 0.007268000394105911}, {"id": 139, "seek": 81376, "start": 813.76, "end": 820.56, "text": " be actually working on getting this to work the next few weeks, right? Because I think doing", "tokens": [50364, 312, 767, 1364, 322, 1242, 341, 281, 589, 264, 958, 1326, 3259, 11, 558, 30, 1436, 286, 519, 884, 50704], "temperature": 0.0, "avg_logprob": -0.12787744734022352, "compression_ratio": 1.6192468619246863, "no_speech_prob": 0.0065799434669315815}, {"id": 140, "seek": 81376, "start": 820.56, "end": 824.4, "text": " something like that actually might help with the up challenge as well, the abstraction and reasoning", "tokens": [50704, 746, 411, 300, 767, 1062, 854, 365, 264, 493, 3430, 382, 731, 11, 264, 37765, 293, 21577, 50896], "temperature": 0.0, "avg_logprob": -0.12787744734022352, "compression_ratio": 1.6192468619246863, "no_speech_prob": 0.0065799434669315815}, {"id": 141, "seek": 81376, "start": 824.4, "end": 831.92, "text": " corpus. So this is my latest kind of hit way that I'm going into. So this is from the knowledge", "tokens": [50896, 1181, 31624, 13, 407, 341, 307, 452, 6792, 733, 295, 2045, 636, 300, 286, 478, 516, 666, 13, 407, 341, 307, 490, 264, 3601, 51272], "temperature": 0.0, "avg_logprob": -0.12787744734022352, "compression_ratio": 1.6192468619246863, "no_speech_prob": 0.0065799434669315815}, {"id": 142, "seek": 81376, "start": 831.92, "end": 836.16, "text": " graph conference. Okay, I actually listened to quite a few of their videos. This is the knowledge", "tokens": [51272, 4295, 7586, 13, 1033, 11, 286, 767, 13207, 281, 1596, 257, 1326, 295, 641, 2145, 13, 639, 307, 264, 3601, 51484], "temperature": 0.0, "avg_logprob": -0.12787744734022352, "compression_ratio": 1.6192468619246863, "no_speech_prob": 0.0065799434669315815}, {"id": 143, "seek": 83616, "start": 836.24, "end": 843.36, "text": " graph conference 2023. And there's a speaker, Danny from Dev6. I think I pronounced his name", "tokens": [50368, 4295, 7586, 945, 9356, 13, 400, 456, 311, 257, 8145, 11, 16682, 490, 9096, 21, 13, 286, 519, 286, 23155, 702, 1315, 50724], "temperature": 0.0, "avg_logprob": -0.21909833545527183, "compression_ratio": 1.6369863013698631, "no_speech_prob": 0.09301405400037766}, {"id": 144, "seek": 83616, "start": 843.36, "end": 849.6, "text": " wrongly. But the idea is that if you are using chat GPT for your own applications, if you use chat", "tokens": [50724, 2085, 356, 13, 583, 264, 1558, 307, 300, 498, 291, 366, 1228, 5081, 26039, 51, 337, 428, 1065, 5821, 11, 498, 291, 764, 5081, 51036], "temperature": 0.0, "avg_logprob": -0.21909833545527183, "compression_ratio": 1.6369863013698631, "no_speech_prob": 0.09301405400037766}, {"id": 145, "seek": 83616, "start": 849.6, "end": 855.1999999999999, "text": " GPT in different languages, you might get different outputs, okay, even for the same information. So", "tokens": [51036, 26039, 51, 294, 819, 8650, 11, 291, 1062, 483, 819, 23930, 11, 1392, 11, 754, 337, 264, 912, 1589, 13, 407, 51316], "temperature": 0.0, "avg_logprob": -0.21909833545527183, "compression_ratio": 1.6369863013698631, "no_speech_prob": 0.09301405400037766}, {"id": 146, "seek": 83616, "start": 855.1999999999999, "end": 858.56, "text": " you know, being Singaporean and you know, the presidential election is coming soon, I just", "tokens": [51316, 291, 458, 11, 885, 14491, 282, 293, 291, 458, 11, 264, 16902, 6618, 307, 1348, 2321, 11, 286, 445, 51484], "temperature": 0.0, "avg_logprob": -0.21909833545527183, "compression_ratio": 1.6369863013698631, "no_speech_prob": 0.09301405400037766}, {"id": 147, "seek": 83616, "start": 858.56, "end": 862.56, "text": " asked like who is Singapore's current president right now. So you can see now is Halima Yacob.", "tokens": [51484, 2351, 411, 567, 307, 14491, 311, 2190, 3868, 558, 586, 13, 407, 291, 393, 536, 586, 307, 13896, 4775, 398, 326, 996, 13, 51684], "temperature": 0.0, "avg_logprob": -0.21909833545527183, "compression_ratio": 1.6369863013698631, "no_speech_prob": 0.09301405400037766}, {"id": 148, "seek": 86256, "start": 863.52, "end": 868.4, "text": " Yacob, sorry. And we asked the question in Chinese, all right, Singapore the", "tokens": [50412, 398, 326, 996, 11, 2597, 13, 400, 321, 2351, 264, 1168, 294, 4649, 11, 439, 558, 11, 14491, 264, 50656], "temperature": 0.0, "avg_logprob": -0.23662888801704018, "compression_ratio": 1.5759717314487633, "no_speech_prob": 0.006600760389119387}, {"id": 149, "seek": 86256, "start": 868.4, "end": 876.9599999999999, "text": "\u7e3d\u7d71, so you know, you say, Singapore, there's no president in Singapore. So this is basically", "tokens": [50656, 26575, 33725, 11, 370, 291, 458, 11, 291, 584, 11, 14491, 11, 456, 311, 572, 3868, 294, 14491, 13, 407, 341, 307, 1936, 51084], "temperature": 0.0, "avg_logprob": -0.23662888801704018, "compression_ratio": 1.5759717314487633, "no_speech_prob": 0.006600760389119387}, {"id": 150, "seek": 86256, "start": 876.9599999999999, "end": 880.4, "text": " the same information, you just translate it, you can get different performances", "tokens": [51084, 264, 912, 1589, 11, 291, 445, 13799, 309, 11, 291, 393, 483, 819, 16087, 51256], "temperature": 0.0, "avg_logprob": -0.23662888801704018, "compression_ratio": 1.5759717314487633, "no_speech_prob": 0.006600760389119387}, {"id": 151, "seek": 86256, "start": 880.9599999999999, "end": 886.16, "text": " with chat GPT. Okay, and the same thing for like if you use Lamatu, Lamatu is heavily trained on", "tokens": [51284, 365, 5081, 26039, 51, 13, 1033, 11, 293, 264, 912, 551, 337, 411, 498, 291, 764, 18825, 20546, 11, 18825, 20546, 307, 10950, 8895, 322, 51544], "temperature": 0.0, "avg_logprob": -0.23662888801704018, "compression_ratio": 1.5759717314487633, "no_speech_prob": 0.006600760389119387}, {"id": 152, "seek": 86256, "start": 886.16, "end": 891.52, "text": " English. If you use Chinese, I'm very sure it won't do very well. All right, this is a practical", "tokens": [51544, 3669, 13, 759, 291, 764, 4649, 11, 286, 478, 588, 988, 309, 1582, 380, 360, 588, 731, 13, 1057, 558, 11, 341, 307, 257, 8496, 51812], "temperature": 0.0, "avg_logprob": -0.23662888801704018, "compression_ratio": 1.5759717314487633, "no_speech_prob": 0.006600760389119387}, {"id": 153, "seek": 89152, "start": 891.52, "end": 896.48, "text": " problem of large language models. You know, the Chinese benchmarks like they use Ernie,", "tokens": [50364, 1154, 295, 2416, 2856, 5245, 13, 509, 458, 11, 264, 4649, 43751, 411, 436, 764, 3300, 2766, 11, 50612], "temperature": 0.0, "avg_logprob": -0.15915129669999653, "compression_ratio": 1.7343173431734318, "no_speech_prob": 0.005893506109714508}, {"id": 154, "seek": 89152, "start": 897.52, "end": 902.0799999999999, "text": " Wenxing Yi and those other Chinese language models, they say that they perform better than", "tokens": [50664, 23716, 87, 278, 16747, 293, 729, 661, 4649, 2856, 5245, 11, 436, 584, 300, 436, 2042, 1101, 813, 50892], "temperature": 0.0, "avg_logprob": -0.15915129669999653, "compression_ratio": 1.7343173431734318, "no_speech_prob": 0.005893506109714508}, {"id": 155, "seek": 89152, "start": 902.0799999999999, "end": 908.64, "text": " GPT for. Okay, I mean, at first I was skeptical. Then now that I think about it, they might have", "tokens": [50892, 26039, 51, 337, 13, 1033, 11, 286, 914, 11, 412, 700, 286, 390, 28601, 13, 1396, 586, 300, 286, 519, 466, 309, 11, 436, 1062, 362, 51220], "temperature": 0.0, "avg_logprob": -0.15915129669999653, "compression_ratio": 1.7343173431734318, "no_speech_prob": 0.005893506109714508}, {"id": 156, "seek": 89152, "start": 908.64, "end": 913.84, "text": " done their evaluation on Chinese data sets. And their language models are fine tuned on the Chinese", "tokens": [51220, 1096, 641, 13344, 322, 4649, 1412, 6352, 13, 400, 641, 2856, 5245, 366, 2489, 10870, 322, 264, 4649, 51480], "temperature": 0.0, "avg_logprob": -0.15915129669999653, "compression_ratio": 1.7343173431734318, "no_speech_prob": 0.005893506109714508}, {"id": 157, "seek": 89152, "start": 913.84, "end": 918.8, "text": " data set. So maybe there's some merit to their claims, okay, on the specific Chinese data sets", "tokens": [51480, 1412, 992, 13, 407, 1310, 456, 311, 512, 24527, 281, 641, 9441, 11, 1392, 11, 322, 264, 2685, 4649, 1412, 6352, 51728], "temperature": 0.0, "avg_logprob": -0.15915129669999653, "compression_ratio": 1.7343173431734318, "no_speech_prob": 0.005893506109714508}, {"id": 158, "seek": 91880, "start": 918.8, "end": 925.12, "text": " here. So this is one of the things that knowledge graphs can actually help to solve, because knowledge", "tokens": [50364, 510, 13, 407, 341, 307, 472, 295, 264, 721, 300, 3601, 24877, 393, 767, 854, 281, 5039, 11, 570, 3601, 50680], "temperature": 0.0, "avg_logprob": -0.10173548099606536, "compression_ratio": 1.832535885167464, "no_speech_prob": 0.004998707212507725}, {"id": 159, "seek": 91880, "start": 925.12, "end": 931.12, "text": " graphs can sort of translate this thing, because knowledge graph is not language specific, you", "tokens": [50680, 24877, 393, 1333, 295, 13799, 341, 551, 11, 570, 3601, 4295, 307, 406, 2856, 2685, 11, 291, 50980], "temperature": 0.0, "avg_logprob": -0.10173548099606536, "compression_ratio": 1.832535885167464, "no_speech_prob": 0.004998707212507725}, {"id": 160, "seek": 91880, "start": 931.12, "end": 935.68, "text": " see. So your concepts like president, okay, regardless of how you represent it in words,", "tokens": [50980, 536, 13, 407, 428, 10392, 411, 3868, 11, 1392, 11, 10060, 295, 577, 291, 2906, 309, 294, 2283, 11, 51208], "temperature": 0.0, "avg_logprob": -0.10173548099606536, "compression_ratio": 1.832535885167464, "no_speech_prob": 0.004998707212507725}, {"id": 161, "seek": 91880, "start": 935.68, "end": 941.12, "text": " okay, your Chinese words or English words, you can actually go to the same part in the knowledge", "tokens": [51208, 1392, 11, 428, 4649, 2283, 420, 3669, 2283, 11, 291, 393, 767, 352, 281, 264, 912, 644, 294, 264, 3601, 51480], "temperature": 0.0, "avg_logprob": -0.10173548099606536, "compression_ratio": 1.832535885167464, "no_speech_prob": 0.004998707212507725}, {"id": 162, "seek": 94112, "start": 941.12, "end": 950.08, "text": " graph. And then you can have the key words here, like Singapore, and then it's like Halima.", "tokens": [50364, 4295, 13, 400, 550, 291, 393, 362, 264, 2141, 2283, 510, 11, 411, 14491, 11, 293, 550, 309, 311, 411, 13896, 4775, 13, 50812], "temperature": 0.0, "avg_logprob": -0.1974097192287445, "compression_ratio": 1.6358024691358024, "no_speech_prob": 0.014398322440683842}, {"id": 163, "seek": 94112, "start": 953.04, "end": 957.84, "text": " So you can actually retrieve the kind of information regardless of language,", "tokens": [50960, 407, 291, 393, 767, 30254, 264, 733, 295, 1589, 10060, 295, 2856, 11, 51200], "temperature": 0.0, "avg_logprob": -0.1974097192287445, "compression_ratio": 1.6358024691358024, "no_speech_prob": 0.014398322440683842}, {"id": 164, "seek": 94112, "start": 958.8, "end": 965.6800000000001, "text": " okay, and then you can pass back this information back into the generation of the model. So this", "tokens": [51248, 1392, 11, 293, 550, 291, 393, 1320, 646, 341, 1589, 646, 666, 264, 5125, 295, 264, 2316, 13, 407, 341, 51592], "temperature": 0.0, "avg_logprob": -0.1974097192287445, "compression_ratio": 1.6358024691358024, "no_speech_prob": 0.014398322440683842}, {"id": 165, "seek": 96568, "start": 965.68, "end": 973.28, "text": " can go here, back here. So regardless of how you prompt GPT in a certain language, okay,", "tokens": [50364, 393, 352, 510, 11, 646, 510, 13, 407, 10060, 295, 577, 291, 12391, 26039, 51, 294, 257, 1629, 2856, 11, 1392, 11, 50744], "temperature": 0.0, "avg_logprob": -0.1546740183016149, "compression_ratio": 1.5414364640883977, "no_speech_prob": 0.003357267240062356}, {"id": 166, "seek": 96568, "start": 973.28, "end": 984.16, "text": " you can do it. So maybe I just do the flow chart. So G-L-M, okay, language, language invariant", "tokens": [50744, 291, 393, 360, 309, 13, 407, 1310, 286, 445, 360, 264, 3095, 6927, 13, 407, 460, 12, 43, 12, 44, 11, 1392, 11, 2856, 11, 2856, 33270, 394, 51288], "temperature": 0.0, "avg_logprob": -0.1546740183016149, "compression_ratio": 1.5414364640883977, "no_speech_prob": 0.003357267240062356}, {"id": 167, "seek": 96568, "start": 984.16, "end": 994.0, "text": " representation. Okay, then you do your processing there. And then you go back to L-M. So if for", "tokens": [51288, 10290, 13, 1033, 11, 550, 291, 360, 428, 9007, 456, 13, 400, 550, 291, 352, 646, 281, 441, 12, 44, 13, 407, 498, 337, 51780], "temperature": 0.0, "avg_logprob": -0.1546740183016149, "compression_ratio": 1.5414364640883977, "no_speech_prob": 0.003357267240062356}, {"id": 168, "seek": 99400, "start": 994.0, "end": 999.84, "text": " those of you who have been to some of my other like discussion sessions, you would know that I", "tokens": [50364, 729, 295, 291, 567, 362, 668, 281, 512, 295, 452, 661, 411, 5017, 11081, 11, 291, 576, 458, 300, 286, 50656], "temperature": 0.0, "avg_logprob": -0.08758427234406167, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.0016903090290725231}, {"id": 169, "seek": 99400, "start": 999.84, "end": 1007.76, "text": " like to say that this is the, this part here. This part here is what I call the abstraction layer,", "tokens": [50656, 411, 281, 584, 300, 341, 307, 264, 11, 341, 644, 510, 13, 639, 644, 510, 307, 437, 286, 818, 264, 37765, 4583, 11, 51052], "temperature": 0.0, "avg_logprob": -0.08758427234406167, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.0016903090290725231}, {"id": 170, "seek": 99400, "start": 1009.28, "end": 1014.96, "text": " or the latent layer, latent space. So you process it in a way that is different from the input", "tokens": [51128, 420, 264, 48994, 4583, 11, 48994, 1901, 13, 407, 291, 1399, 309, 294, 257, 636, 300, 307, 819, 490, 264, 4846, 51412], "temperature": 0.0, "avg_logprob": -0.08758427234406167, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.0016903090290725231}, {"id": 171, "seek": 99400, "start": 1014.96, "end": 1020.16, "text": " domain, but because the information you process is similar, in this case, we are still asking for", "tokens": [51412, 9274, 11, 457, 570, 264, 1589, 291, 1399, 307, 2531, 11, 294, 341, 1389, 11, 321, 366, 920, 3365, 337, 51672], "temperature": 0.0, "avg_logprob": -0.08758427234406167, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.0016903090290725231}, {"id": 172, "seek": 102016, "start": 1020.16, "end": 1024.72, "text": " semantic information about the president. You know, we don't have to do it in the language domain,", "tokens": [50364, 47982, 1589, 466, 264, 3868, 13, 509, 458, 11, 321, 500, 380, 362, 281, 360, 309, 294, 264, 2856, 9274, 11, 50592], "temperature": 0.0, "avg_logprob": -0.0980178718892937, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.006088722497224808}, {"id": 173, "seek": 102016, "start": 1024.72, "end": 1030.72, "text": " we can do it in like maybe some representational space. It could be a graph, all right, and then", "tokens": [50592, 321, 393, 360, 309, 294, 411, 1310, 512, 2906, 1478, 1901, 13, 467, 727, 312, 257, 4295, 11, 439, 558, 11, 293, 550, 50892], "temperature": 0.0, "avg_logprob": -0.0980178718892937, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.006088722497224808}, {"id": 174, "seek": 102016, "start": 1030.72, "end": 1036.24, "text": " you can use whatever you process the graph, you can go back to your input space. So this is one of", "tokens": [50892, 291, 393, 764, 2035, 291, 1399, 264, 4295, 11, 291, 393, 352, 646, 281, 428, 4846, 1901, 13, 407, 341, 307, 472, 295, 51168], "temperature": 0.0, "avg_logprob": -0.0980178718892937, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.006088722497224808}, {"id": 175, "seek": 102016, "start": 1036.24, "end": 1041.36, "text": " the key advantages that, you know, if we could interface large language models with some form of", "tokens": [51168, 264, 2141, 14906, 300, 11, 291, 458, 11, 498, 321, 727, 9226, 2416, 2856, 5245, 365, 512, 1254, 295, 51424], "temperature": 0.0, "avg_logprob": -0.0980178718892937, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.006088722497224808}, {"id": 176, "seek": 102016, "start": 1041.36, "end": 1048.72, "text": " graphical or some form of memory-based approach that is invariant to the input language type,", "tokens": [51424, 35942, 420, 512, 1254, 295, 4675, 12, 6032, 3109, 300, 307, 33270, 394, 281, 264, 4846, 2856, 2010, 11, 51792], "temperature": 0.0, "avg_logprob": -0.0980178718892937, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.006088722497224808}, {"id": 177, "seek": 104872, "start": 1048.72, "end": 1055.04, "text": " you could get some performance advantage here. Questions so far? Anyone?", "tokens": [50364, 291, 727, 483, 512, 3389, 5002, 510, 13, 27738, 370, 1400, 30, 14643, 30, 50680], "temperature": 0.0, "avg_logprob": -0.15407778611823694, "compression_ratio": 1.4497354497354498, "no_speech_prob": 0.0011997149558737874}, {"id": 178, "seek": 104872, "start": 1058.8, "end": 1064.8, "text": " Okay, so let's cover some of the basics. What is a knowledge graph? So I took this from the paper.", "tokens": [50868, 1033, 11, 370, 718, 311, 2060, 512, 295, 264, 14688, 13, 708, 307, 257, 3601, 4295, 30, 407, 286, 1890, 341, 490, 264, 3035, 13, 51168], "temperature": 0.0, "avg_logprob": -0.15407778611823694, "compression_ratio": 1.4497354497354498, "no_speech_prob": 0.0011997149558737874}, {"id": 179, "seek": 104872, "start": 1064.8, "end": 1071.04, "text": " The knowledge graph is basically a triplet consisting of source, destination, to relation. So like for", "tokens": [51168, 440, 3601, 4295, 307, 1936, 257, 1376, 14657, 33921, 295, 4009, 11, 12236, 11, 281, 9721, 13, 407, 411, 337, 51480], "temperature": 0.0, "avg_logprob": -0.15407778611823694, "compression_ratio": 1.4497354497354498, "no_speech_prob": 0.0011997149558737874}, {"id": 180, "seek": 107104, "start": 1071.04, "end": 1077.28, "text": " example, Barack Obama was born in Honolulu. So this is the relation, okay, so relation.", "tokens": [50364, 1365, 11, 31705, 9560, 390, 4232, 294, 6625, 401, 12845, 13, 407, 341, 307, 264, 9721, 11, 1392, 11, 370, 9721, 13, 50676], "temperature": 0.0, "avg_logprob": -0.15973035581819303, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.004560891073197126}, {"id": 181, "seek": 107104, "start": 1078.56, "end": 1085.04, "text": " And this is like Barack Obama as the source, and Honolulu is like the destination. So each", "tokens": [50740, 400, 341, 307, 411, 31705, 9560, 382, 264, 4009, 11, 293, 6625, 401, 12845, 307, 411, 264, 12236, 13, 407, 1184, 51064], "temperature": 0.0, "avg_logprob": -0.15973035581819303, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.004560891073197126}, {"id": 182, "seek": 107104, "start": 1087.52, "end": 1095.44, "text": " knowledge graph is made up of all these triplets joined together in various ways. And the idea is", "tokens": [51188, 3601, 4295, 307, 1027, 493, 295, 439, 613, 1376, 31023, 6869, 1214, 294, 3683, 2098, 13, 400, 264, 1558, 307, 51584], "temperature": 0.0, "avg_logprob": -0.15973035581819303, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.004560891073197126}, {"id": 183, "seek": 107104, "start": 1095.44, "end": 1100.32, "text": " that you just need to connect those entities that are related to each other. You can like", "tokens": [51584, 300, 291, 445, 643, 281, 1745, 729, 16667, 300, 366, 4077, 281, 1184, 661, 13, 509, 393, 411, 51828], "temperature": 0.0, "avg_logprob": -0.15973035581819303, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.004560891073197126}, {"id": 184, "seek": 110032, "start": 1100.72, "end": 1105.6, "text": " you can actually walk through the knowledge graph and get the information you need. Okay, so like", "tokens": [50384, 291, 393, 767, 1792, 807, 264, 3601, 4295, 293, 483, 264, 1589, 291, 643, 13, 1033, 11, 370, 411, 50628], "temperature": 0.0, "avg_logprob": -0.15593298887595153, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0025945105589926243}, {"id": 185, "seek": 110032, "start": 1105.6, "end": 1110.32, "text": " there are of course like mega nodes, like for example, like Barack Obama will have a lot of", "tokens": [50628, 456, 366, 295, 1164, 411, 17986, 13891, 11, 411, 337, 1365, 11, 411, 31705, 9560, 486, 362, 257, 688, 295, 50864], "temperature": 0.0, "avg_logprob": -0.15593298887595153, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0025945105589926243}, {"id": 186, "seek": 110032, "start": 1110.32, "end": 1114.96, "text": " connections leading out of it because you're describing the person. Then like stuff like places", "tokens": [50864, 9271, 5775, 484, 295, 309, 570, 291, 434, 16141, 264, 954, 13, 1396, 411, 1507, 411, 3190, 51096], "temperature": 0.0, "avg_logprob": -0.15593298887595153, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0025945105589926243}, {"id": 187, "seek": 110032, "start": 1114.96, "end": 1120.1599999999999, "text": " where a lot of things leading into it, because a lot of things like like are in the USA, a lot of", "tokens": [51096, 689, 257, 688, 295, 721, 5775, 666, 309, 11, 570, 257, 688, 295, 721, 411, 411, 366, 294, 264, 10827, 11, 257, 688, 295, 51356], "temperature": 0.0, "avg_logprob": -0.15593298887595153, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0025945105589926243}, {"id": 188, "seek": 110032, "start": 1120.1599999999999, "end": 1126.96, "text": " things are in Singapore, you know. So this is the 20, 30 years ideas of knowledge graph. Okay, it's", "tokens": [51356, 721, 366, 294, 14491, 11, 291, 458, 13, 407, 341, 307, 264, 945, 11, 2217, 924, 3487, 295, 3601, 4295, 13, 1033, 11, 309, 311, 51696], "temperature": 0.0, "avg_logprob": -0.15593298887595153, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0025945105589926243}, {"id": 189, "seek": 112696, "start": 1126.96, "end": 1133.1200000000001, "text": " not too bad. Okay, but it's very restrictive. I personally think that there is a better way to", "tokens": [50364, 406, 886, 1578, 13, 1033, 11, 457, 309, 311, 588, 43220, 13, 286, 5665, 519, 300, 456, 307, 257, 1101, 636, 281, 50672], "temperature": 0.0, "avg_logprob": -0.11139496889981357, "compression_ratio": 1.6254545454545455, "no_speech_prob": 0.0024526368360966444}, {"id": 190, "seek": 112696, "start": 1133.1200000000001, "end": 1138.8, "text": " represent information other than this kind of structure. Okay, and we can go and talk about", "tokens": [50672, 2906, 1589, 661, 813, 341, 733, 295, 3877, 13, 1033, 11, 293, 321, 393, 352, 293, 751, 466, 50956], "temperature": 0.0, "avg_logprob": -0.11139496889981357, "compression_ratio": 1.6254545454545455, "no_speech_prob": 0.0024526368360966444}, {"id": 191, "seek": 112696, "start": 1138.8, "end": 1144.56, "text": " it later in the discussion. All right, I have something in the chat. Richard says,", "tokens": [50956, 309, 1780, 294, 264, 5017, 13, 1057, 558, 11, 286, 362, 746, 294, 264, 5081, 13, 9809, 1619, 11, 51244], "temperature": 0.0, "avg_logprob": -0.11139496889981357, "compression_ratio": 1.6254545454545455, "no_speech_prob": 0.0024526368360966444}, {"id": 192, "seek": 112696, "start": 1144.56, "end": 1150.08, "text": " is there a handy reference chart for how this looks or compares to word-to-veg and similar", "tokens": [51244, 307, 456, 257, 13239, 6408, 6927, 337, 577, 341, 1542, 420, 38334, 281, 1349, 12, 1353, 12, 303, 70, 293, 2531, 51520], "temperature": 0.0, "avg_logprob": -0.11139496889981357, "compression_ratio": 1.6254545454545455, "no_speech_prob": 0.0024526368360966444}, {"id": 193, "seek": 112696, "start": 1150.08, "end": 1156.16, "text": " embeddings? Okay, so this typical knowledge graph that I'm talking about here does not", "tokens": [51520, 12240, 29432, 30, 1033, 11, 370, 341, 7476, 3601, 4295, 300, 286, 478, 1417, 466, 510, 775, 406, 51824], "temperature": 0.0, "avg_logprob": -0.11139496889981357, "compression_ratio": 1.6254545454545455, "no_speech_prob": 0.0024526368360966444}, {"id": 194, "seek": 115616, "start": 1156.16, "end": 1161.6000000000001, "text": " have embeddings yet. Okay, but in the future iterations like in 2017 or 2018, I think people", "tokens": [50364, 362, 12240, 29432, 1939, 13, 1033, 11, 457, 294, 264, 2027, 36540, 411, 294, 6591, 420, 6096, 11, 286, 519, 561, 50636], "temperature": 0.0, "avg_logprob": -0.15385018454657662, "compression_ratio": 1.8455284552845528, "no_speech_prob": 0.002541207941249013}, {"id": 195, "seek": 115616, "start": 1161.6000000000001, "end": 1166.3200000000002, "text": " have come up with these things like knowledge graph embeddings. So they actually encode all", "tokens": [50636, 362, 808, 493, 365, 613, 721, 411, 3601, 4295, 12240, 29432, 13, 407, 436, 767, 2058, 1429, 439, 50872], "temperature": 0.0, "avg_logprob": -0.15385018454657662, "compression_ratio": 1.8455284552845528, "no_speech_prob": 0.002541207941249013}, {"id": 196, "seek": 115616, "start": 1166.3200000000002, "end": 1171.2, "text": " this information here in some vector space. So like maybe like, it's something very similar to", "tokens": [50872, 341, 1589, 510, 294, 512, 8062, 1901, 13, 407, 411, 1310, 411, 11, 309, 311, 746, 588, 2531, 281, 51116], "temperature": 0.0, "avg_logprob": -0.15385018454657662, "compression_ratio": 1.8455284552845528, "no_speech_prob": 0.002541207941249013}, {"id": 197, "seek": 115616, "start": 1171.76, "end": 1176.96, "text": " like the vector space that we see in in large language models. You can actually encode this", "tokens": [51144, 411, 264, 8062, 1901, 300, 321, 536, 294, 294, 2416, 2856, 5245, 13, 509, 393, 767, 2058, 1429, 341, 51404], "temperature": 0.0, "avg_logprob": -0.15385018454657662, "compression_ratio": 1.8455284552845528, "no_speech_prob": 0.002541207941249013}, {"id": 198, "seek": 115616, "start": 1176.96, "end": 1182.0, "text": " thing in vector space. You can encode the relation here in the vector space or so.", "tokens": [51404, 551, 294, 8062, 1901, 13, 509, 393, 2058, 1429, 264, 9721, 510, 294, 264, 8062, 1901, 420, 370, 13, 51656], "temperature": 0.0, "avg_logprob": -0.15385018454657662, "compression_ratio": 1.8455284552845528, "no_speech_prob": 0.002541207941249013}, {"id": 199, "seek": 118200, "start": 1182.64, "end": 1189.6, "text": " And then like, you can then encode this part here also in vector space. So like,", "tokens": [50396, 400, 550, 411, 11, 291, 393, 550, 2058, 1429, 341, 644, 510, 611, 294, 8062, 1901, 13, 407, 411, 11, 50744], "temperature": 0.0, "avg_logprob": -0.16133446847238847, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006762129487469792}, {"id": 200, "seek": 118200, "start": 1190.48, "end": 1194.16, "text": " so it's like doing a vector arithmetic now. So you can see that", "tokens": [50788, 370, 309, 311, 411, 884, 257, 8062, 42973, 586, 13, 407, 291, 393, 536, 300, 50972], "temperature": 0.0, "avg_logprob": -0.16133446847238847, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006762129487469792}, {"id": 201, "seek": 118200, "start": 1196.96, "end": 1202.16, "text": " if I do a relation, it's just simply this one plus this one equals to this one. So I can do a", "tokens": [51112, 498, 286, 360, 257, 9721, 11, 309, 311, 445, 2935, 341, 472, 1804, 341, 472, 6915, 281, 341, 472, 13, 407, 286, 393, 360, 257, 51372], "temperature": 0.0, "avg_logprob": -0.16133446847238847, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006762129487469792}, {"id": 202, "seek": 118200, "start": 1202.16, "end": 1207.68, "text": " like add a .2 in the first one here and then I can get. So if you have a sufficiently expressive", "tokens": [51372, 411, 909, 257, 2411, 17, 294, 264, 700, 472, 510, 293, 550, 286, 393, 483, 13, 407, 498, 291, 362, 257, 31868, 40189, 51648], "temperature": 0.0, "avg_logprob": -0.16133446847238847, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006762129487469792}, {"id": 203, "seek": 120768, "start": 1207.76, "end": 1213.1200000000001, "text": " enough embedding space, you can express the whole knowledge graph in the form of embeddings.", "tokens": [50368, 1547, 12240, 3584, 1901, 11, 291, 393, 5109, 264, 1379, 3601, 4295, 294, 264, 1254, 295, 12240, 29432, 13, 50636], "temperature": 0.0, "avg_logprob": -0.09825548419245968, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.004568097181618214}, {"id": 204, "seek": 120768, "start": 1213.68, "end": 1219.92, "text": " And that is indeed what some of the later models do. In fact, this is highly related to", "tokens": [50664, 400, 300, 307, 6451, 437, 512, 295, 264, 1780, 5245, 360, 13, 682, 1186, 11, 341, 307, 5405, 4077, 281, 50976], "temperature": 0.0, "avg_logprob": -0.09825548419245968, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.004568097181618214}, {"id": 205, "seek": 120768, "start": 1219.92, "end": 1226.16, "text": " graph neural networks. Because graph neural networks, they express each node as an embedding,", "tokens": [50976, 4295, 18161, 9590, 13, 1436, 4295, 18161, 9590, 11, 436, 5109, 1184, 9984, 382, 364, 12240, 3584, 11, 51288], "temperature": 0.0, "avg_logprob": -0.09825548419245968, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.004568097181618214}, {"id": 206, "seek": 120768, "start": 1226.16, "end": 1230.4, "text": " then they do message passing, which means I share information with the other nodes. Like at each", "tokens": [51288, 550, 436, 360, 3636, 8437, 11, 597, 1355, 286, 2073, 1589, 365, 264, 661, 13891, 13, 1743, 412, 1184, 51500], "temperature": 0.0, "avg_logprob": -0.09825548419245968, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.004568097181618214}, {"id": 207, "seek": 120768, "start": 1230.4, "end": 1234.24, "text": " time step, I pass some information to the other nodes. And I mean, there are different variants", "tokens": [51500, 565, 1823, 11, 286, 1320, 512, 1589, 281, 264, 661, 13891, 13, 400, 286, 914, 11, 456, 366, 819, 21669, 51692], "temperature": 0.0, "avg_logprob": -0.09825548419245968, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.004568097181618214}, {"id": 208, "seek": 123424, "start": 1234.24, "end": 1238.8, "text": " of message passing. The most common is that the message meets in the middle, this one then updates", "tokens": [50364, 295, 3636, 8437, 13, 440, 881, 2689, 307, 300, 264, 3636, 13961, 294, 264, 2808, 11, 341, 472, 550, 9205, 50592], "temperature": 0.0, "avg_logprob": -0.11237592697143554, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.00455058366060257}, {"id": 209, "seek": 123424, "start": 1238.8, "end": 1246.16, "text": " both nodes. So there are a few ways of doing the idea of like updating the embeddings and so on.", "tokens": [50592, 1293, 13891, 13, 407, 456, 366, 257, 1326, 2098, 295, 884, 264, 1558, 295, 411, 25113, 264, 12240, 29432, 293, 370, 322, 13, 50960], "temperature": 0.0, "avg_logprob": -0.11237592697143554, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.00455058366060257}, {"id": 210, "seek": 123424, "start": 1246.16, "end": 1250.4, "text": " I'm not going to cover in detail about how all this are done because graph neural networks is a huge", "tokens": [50960, 286, 478, 406, 516, 281, 2060, 294, 2607, 466, 577, 439, 341, 366, 1096, 570, 4295, 18161, 9590, 307, 257, 2603, 51172], "temperature": 0.0, "avg_logprob": -0.11237592697143554, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.00455058366060257}, {"id": 211, "seek": 123424, "start": 1250.4, "end": 1255.04, "text": " topic. Okay, personally, I think graph neural networks is probably not the answer to solving", "tokens": [51172, 4829, 13, 1033, 11, 5665, 11, 286, 519, 4295, 18161, 9590, 307, 1391, 406, 264, 1867, 281, 12606, 51404], "temperature": 0.0, "avg_logprob": -0.11237592697143554, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.00455058366060257}, {"id": 212, "seek": 123424, "start": 1255.04, "end": 1261.76, "text": " intelligence. I'm sorry to Peter Velikovic. I like what he's doing, but I don't think it's", "tokens": [51404, 7599, 13, 286, 478, 2597, 281, 6508, 17814, 1035, 5179, 299, 13, 286, 411, 437, 415, 311, 884, 11, 457, 286, 500, 380, 519, 309, 311, 51740], "temperature": 0.0, "avg_logprob": -0.11237592697143554, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.00455058366060257}, {"id": 213, "seek": 126176, "start": 1262.64, "end": 1266.32, "text": " it's the right way to do it, like using differentiable deep learning to do it.", "tokens": [50408, 309, 311, 264, 558, 636, 281, 360, 309, 11, 411, 1228, 819, 9364, 2452, 2539, 281, 360, 309, 13, 50592], "temperature": 0.0, "avg_logprob": -0.12422092570814976, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.0016118008643388748}, {"id": 214, "seek": 126176, "start": 1266.8799999999999, "end": 1273.28, "text": " So I think the knowledge graph that I've described over here, which is using vectors to do addition", "tokens": [50620, 407, 286, 519, 264, 3601, 4295, 300, 286, 600, 7619, 670, 510, 11, 597, 307, 1228, 18875, 281, 360, 4500, 50940], "temperature": 0.0, "avg_logprob": -0.12422092570814976, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.0016118008643388748}, {"id": 215, "seek": 126176, "start": 1273.28, "end": 1279.36, "text": " and then you get the other nodes. That's a very expressive knowledge graph. Okay, because you can", "tokens": [50940, 293, 550, 291, 483, 264, 661, 13891, 13, 663, 311, 257, 588, 40189, 3601, 4295, 13, 1033, 11, 570, 291, 393, 51244], "temperature": 0.0, "avg_logprob": -0.12422092570814976, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.0016118008643388748}, {"id": 216, "seek": 126176, "start": 1279.36, "end": 1288.08, "text": " actually express everything in vectors without the names. So you can theoretically do any kind", "tokens": [51244, 767, 5109, 1203, 294, 18875, 1553, 264, 5288, 13, 407, 291, 393, 29400, 360, 604, 733, 51680], "temperature": 0.0, "avg_logprob": -0.12422092570814976, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.0016118008643388748}, {"id": 217, "seek": 128808, "start": 1288.08, "end": 1294.1599999999999, "text": " of like addition provided, you know, nodes plus relation give you another node provided that", "tokens": [50364, 295, 411, 4500, 5649, 11, 291, 458, 11, 13891, 1804, 9721, 976, 291, 1071, 9984, 5649, 300, 50668], "temperature": 0.0, "avg_logprob": -0.20896633251293287, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.007867036387324333}, {"id": 218, "seek": 128808, "start": 1294.1599999999999, "end": 1300.96, "text": " exists. So if you could somehow represent the whole of the world's knowledge in the form of", "tokens": [50668, 8198, 13, 407, 498, 291, 727, 6063, 2906, 264, 1379, 295, 264, 1002, 311, 3601, 294, 264, 1254, 295, 51008], "temperature": 0.0, "avg_logprob": -0.20896633251293287, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.007867036387324333}, {"id": 219, "seek": 128808, "start": 1300.96, "end": 1306.8799999999999, "text": " vector space, I say we are done. We can just like, we achieved zero short generalization.", "tokens": [51008, 8062, 1901, 11, 286, 584, 321, 366, 1096, 13, 492, 393, 445, 411, 11, 321, 11042, 4018, 2099, 2674, 2144, 13, 51304], "temperature": 0.0, "avg_logprob": -0.20896633251293287, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.007867036387324333}, {"id": 220, "seek": 128808, "start": 1306.8799999999999, "end": 1310.96, "text": " You just embed into that vector space and then you add something and then you go to somewhere else.", "tokens": [51304, 509, 445, 12240, 666, 300, 8062, 1901, 293, 550, 291, 909, 746, 293, 550, 291, 352, 281, 4079, 1646, 13, 51508], "temperature": 0.0, "avg_logprob": -0.20896633251293287, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.007867036387324333}, {"id": 221, "seek": 128808, "start": 1311.52, "end": 1317.76, "text": " Okay, but I don't think that's how intelligence is represented. Okay, because you know, there's", "tokens": [51536, 1033, 11, 457, 286, 500, 380, 519, 300, 311, 577, 7599, 307, 10379, 13, 1033, 11, 570, 291, 458, 11, 456, 311, 51848], "temperature": 0.0, "avg_logprob": -0.20896633251293287, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.007867036387324333}, {"id": 222, "seek": 131776, "start": 1317.76, "end": 1325.04, "text": " this thing called like context dependent embeddings. Like I don't think like the word Barrett", "tokens": [50364, 341, 551, 1219, 411, 4319, 12334, 12240, 29432, 13, 1743, 286, 500, 380, 519, 411, 264, 1349, 4156, 14313, 50728], "temperature": 0.0, "avg_logprob": -0.14660582643874148, "compression_ratio": 1.8483412322274881, "no_speech_prob": 0.0016750297509133816}, {"id": 223, "seek": 131776, "start": 1325.04, "end": 1330.08, "text": " Obama would have the same embedding all the time. So like, for example, if you have Barrett Obama that", "tokens": [50728, 9560, 576, 362, 264, 912, 12240, 3584, 439, 264, 565, 13, 407, 411, 11, 337, 1365, 11, 498, 291, 362, 4156, 14313, 9560, 300, 50980], "temperature": 0.0, "avg_logprob": -0.14660582643874148, "compression_ratio": 1.8483412322274881, "no_speech_prob": 0.0016750297509133816}, {"id": 224, "seek": 131776, "start": 1330.08, "end": 1339.12, "text": " is like at the White House, Barrett Obama at his house, okay, Barrett Obama at the beach or maybe", "tokens": [50980, 307, 411, 412, 264, 5552, 4928, 11, 4156, 14313, 9560, 412, 702, 1782, 11, 1392, 11, 4156, 14313, 9560, 412, 264, 7534, 420, 1310, 51432], "temperature": 0.0, "avg_logprob": -0.14660582643874148, "compression_ratio": 1.8483412322274881, "no_speech_prob": 0.0016750297509133816}, {"id": 225, "seek": 131776, "start": 1339.12, "end": 1345.2, "text": " different places of Barrett Obama will lead to different characteristics of Barrett Obama. Like", "tokens": [51432, 819, 3190, 295, 4156, 14313, 9560, 486, 1477, 281, 819, 10891, 295, 4156, 14313, 9560, 13, 1743, 51736], "temperature": 0.0, "avg_logprob": -0.14660582643874148, "compression_ratio": 1.8483412322274881, "no_speech_prob": 0.0016750297509133816}, {"id": 226, "seek": 134520, "start": 1345.28, "end": 1350.4, "text": " he maybe is very serious in the office, but he's very relaxed at the beach. You cannot have the", "tokens": [50368, 415, 1310, 307, 588, 3156, 294, 264, 3398, 11, 457, 415, 311, 588, 14628, 412, 264, 7534, 13, 509, 2644, 362, 264, 50624], "temperature": 0.0, "avg_logprob": -0.13684150447016177, "compression_ratio": 1.6495726495726495, "no_speech_prob": 0.0017180576687678695}, {"id": 227, "seek": 134520, "start": 1350.4, "end": 1357.3600000000001, "text": " same embedding space to represent all this. You need to walk it according to the context. Okay,", "tokens": [50624, 912, 12240, 3584, 1901, 281, 2906, 439, 341, 13, 509, 643, 281, 1792, 309, 4650, 281, 264, 4319, 13, 1033, 11, 50972], "temperature": 0.0, "avg_logprob": -0.13684150447016177, "compression_ratio": 1.6495726495726495, "no_speech_prob": 0.0017180576687678695}, {"id": 228, "seek": 134520, "start": 1357.3600000000001, "end": 1363.52, "text": " and that is something that I actually intend to try to do it. Like I try to do a very flexible", "tokens": [50972, 293, 300, 307, 746, 300, 286, 767, 19759, 281, 853, 281, 360, 309, 13, 1743, 286, 853, 281, 360, 257, 588, 11358, 51280], "temperature": 0.0, "avg_logprob": -0.13684150447016177, "compression_ratio": 1.6495726495726495, "no_speech_prob": 0.0017180576687678695}, {"id": 229, "seek": 134520, "start": 1364.96, "end": 1370.96, "text": " like basically the information can walk according to the parent nodes in this new form of knowledge", "tokens": [51352, 411, 1936, 264, 1589, 393, 1792, 4650, 281, 264, 2596, 13891, 294, 341, 777, 1254, 295, 3601, 51652], "temperature": 0.0, "avg_logprob": -0.13684150447016177, "compression_ratio": 1.6495726495726495, "no_speech_prob": 0.0017180576687678695}, {"id": 230, "seek": 137096, "start": 1370.96, "end": 1376.16, "text": " graph that I'm thinking of. Okay, so whatever I'm talking about is my own idea. I haven't seen", "tokens": [50364, 4295, 300, 286, 478, 1953, 295, 13, 1033, 11, 370, 2035, 286, 478, 1417, 466, 307, 452, 1065, 1558, 13, 286, 2378, 380, 1612, 50624], "temperature": 0.0, "avg_logprob": -0.1102920154948811, "compression_ratio": 1.55, "no_speech_prob": 0.004873252473771572}, {"id": 231, "seek": 137096, "start": 1376.16, "end": 1380.88, "text": " any paper on it yet, but I think the current knowledge graphs will all fail at embodying", "tokens": [50624, 604, 3035, 322, 309, 1939, 11, 457, 286, 519, 264, 2190, 3601, 24877, 486, 439, 3061, 412, 42575, 278, 50860], "temperature": 0.0, "avg_logprob": -0.1102920154948811, "compression_ratio": 1.55, "no_speech_prob": 0.004873252473771572}, {"id": 232, "seek": 137096, "start": 1380.88, "end": 1387.44, "text": " intelligence because it's just too restrictive right now. Okay, Shang, you asked a question.", "tokens": [50860, 7599, 570, 309, 311, 445, 886, 43220, 558, 586, 13, 1033, 11, 19316, 11, 291, 2351, 257, 1168, 13, 51188], "temperature": 0.0, "avg_logprob": -0.1102920154948811, "compression_ratio": 1.55, "no_speech_prob": 0.004873252473771572}, {"id": 233, "seek": 137096, "start": 1388.56, "end": 1394.08, "text": " I'm unfamiliar with graph theory, so hoping to know how do you represent factors as weights and", "tokens": [51244, 286, 478, 29415, 365, 4295, 5261, 11, 370, 7159, 281, 458, 577, 360, 291, 2906, 6771, 382, 17443, 293, 51520], "temperature": 0.0, "avg_logprob": -0.1102920154948811, "compression_ratio": 1.55, "no_speech_prob": 0.004873252473771572}, {"id": 234, "seek": 139408, "start": 1394.08, "end": 1399.9199999999998, "text": " how many can you add? Okay, could you elaborate what you mean by factors?", "tokens": [50364, 577, 867, 393, 291, 909, 30, 1033, 11, 727, 291, 20945, 437, 291, 914, 538, 6771, 30, 50656], "temperature": 0.0, "avg_logprob": -0.20452905382428851, "compression_ratio": 1.4835164835164836, "no_speech_prob": 0.0013001179322600365}, {"id": 235, "seek": 139408, "start": 1403.52, "end": 1412.56, "text": " Yeah, you mentioned that like you can add any form of intelligence, right? So take for example,", "tokens": [50836, 865, 11, 291, 2835, 300, 411, 291, 393, 909, 604, 1254, 295, 7599, 11, 558, 30, 407, 747, 337, 1365, 11, 51288], "temperature": 0.0, "avg_logprob": -0.20452905382428851, "compression_ratio": 1.4835164835164836, "no_speech_prob": 0.0013001179322600365}, {"id": 236, "seek": 139408, "start": 1412.56, "end": 1422.72, "text": " if we are using, yeah, I actually didn't think of this example, but let's say just the simplest one,", "tokens": [51288, 498, 321, 366, 1228, 11, 1338, 11, 286, 767, 994, 380, 519, 295, 341, 1365, 11, 457, 718, 311, 584, 445, 264, 22811, 472, 11, 51796], "temperature": 0.0, "avg_logprob": -0.20452905382428851, "compression_ratio": 1.4835164835164836, "no_speech_prob": 0.0013001179322600365}, {"id": 237, "seek": 142408, "start": 1424.1599999999999, "end": 1436.24, "text": " multi-layer map. Then for these roads, one weight could be how fast the speed limit of the road", "tokens": [50368, 4825, 12, 8376, 260, 4471, 13, 1396, 337, 613, 11344, 11, 472, 3364, 727, 312, 577, 2370, 264, 3073, 4948, 295, 264, 3060, 50972], "temperature": 0.0, "avg_logprob": -0.2033358173093934, "compression_ratio": 1.6384180790960452, "no_speech_prob": 0.0006473330431617796}, {"id": 238, "seek": 142408, "start": 1436.24, "end": 1444.1599999999999, "text": " and another weight could be how occupied it is. Okay, so you are talking about like descriptions", "tokens": [50972, 293, 1071, 3364, 727, 312, 577, 19629, 309, 307, 13, 1033, 11, 370, 291, 366, 1417, 466, 411, 24406, 51368], "temperature": 0.0, "avg_logprob": -0.2033358173093934, "compression_ratio": 1.6384180790960452, "no_speech_prob": 0.0006473330431617796}, {"id": 239, "seek": 142408, "start": 1444.1599999999999, "end": 1448.96, "text": " of an object, like all characteristics, attributes, you're talking about attributes of an object.", "tokens": [51368, 295, 364, 2657, 11, 411, 439, 10891, 11, 17212, 11, 291, 434, 1417, 466, 17212, 295, 364, 2657, 13, 51608], "temperature": 0.0, "avg_logprob": -0.2033358173093934, "compression_ratio": 1.6384180790960452, "no_speech_prob": 0.0006473330431617796}, {"id": 240, "seek": 144896, "start": 1449.92, "end": 1457.28, "text": " Yeah, the weight of each line, correct, of each connection between the nodes.", "tokens": [50412, 865, 11, 264, 3364, 295, 1184, 1622, 11, 3006, 11, 295, 1184, 4984, 1296, 264, 13891, 13, 50780], "temperature": 0.0, "avg_logprob": -0.15718733987142874, "compression_ratio": 1.7475247524752475, "no_speech_prob": 0.0031503215432167053}, {"id": 241, "seek": 144896, "start": 1458.24, "end": 1464.72, "text": " Ah, okay, so like how do you get this embedding here, right? Yeah, correct. Yeah, so perhaps like", "tokens": [50828, 2438, 11, 1392, 11, 370, 411, 577, 360, 291, 483, 341, 12240, 3584, 510, 11, 558, 30, 865, 11, 3006, 13, 865, 11, 370, 4317, 411, 51152], "temperature": 0.0, "avg_logprob": -0.15718733987142874, "compression_ratio": 1.7475247524752475, "no_speech_prob": 0.0031503215432167053}, {"id": 242, "seek": 144896, "start": 1464.72, "end": 1468.64, "text": " in your original embedding space, each of these dimensions could represent something already,", "tokens": [51152, 294, 428, 3380, 12240, 3584, 1901, 11, 1184, 295, 613, 12819, 727, 2906, 746, 1217, 11, 51348], "temperature": 0.0, "avg_logprob": -0.15718733987142874, "compression_ratio": 1.7475247524752475, "no_speech_prob": 0.0031503215432167053}, {"id": 243, "seek": 144896, "start": 1468.64, "end": 1473.92, "text": " like maybe one could represent road, one could represent like emotion or, you know,", "tokens": [51348, 411, 1310, 472, 727, 2906, 3060, 11, 472, 727, 2906, 411, 8913, 420, 11, 291, 458, 11, 51612], "temperature": 0.0, "avg_logprob": -0.15718733987142874, "compression_ratio": 1.7475247524752475, "no_speech_prob": 0.0031503215432167053}, {"id": 244, "seek": 147392, "start": 1474.0, "end": 1479.6000000000001, "text": " there are different domains that these dimensions could capture already. So if you really have that,", "tokens": [50368, 456, 366, 819, 25514, 300, 613, 12819, 727, 7983, 1217, 13, 407, 498, 291, 534, 362, 300, 11, 50648], "temperature": 0.0, "avg_logprob": -0.13269977355271242, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.0016454001888632774}, {"id": 245, "seek": 147392, "start": 1479.6000000000001, "end": 1486.4, "text": " you can just like add the relation in that specific dimension. Yeah, so of course, all this", "tokens": [50648, 291, 393, 445, 411, 909, 264, 9721, 294, 300, 2685, 10139, 13, 865, 11, 370, 295, 1164, 11, 439, 341, 50988], "temperature": 0.0, "avg_logprob": -0.13269977355271242, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.0016454001888632774}, {"id": 246, "seek": 147392, "start": 1486.4, "end": 1491.44, "text": " will need to be like learn somehow. So it's either learn through deep learning or some fixed biases.", "tokens": [50988, 486, 643, 281, 312, 411, 1466, 6063, 13, 407, 309, 311, 2139, 1466, 807, 2452, 2539, 420, 512, 6806, 32152, 13, 51240], "temperature": 0.0, "avg_logprob": -0.13269977355271242, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.0016454001888632774}, {"id": 247, "seek": 147392, "start": 1492.64, "end": 1499.04, "text": " Yeah, so ultimately, how well the graph does will depend on how good your embedding space captures", "tokens": [51300, 865, 11, 370, 6284, 11, 577, 731, 264, 4295, 775, 486, 5672, 322, 577, 665, 428, 12240, 3584, 1901, 27986, 51620], "temperature": 0.0, "avg_logprob": -0.13269977355271242, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.0016454001888632774}, {"id": 248, "seek": 149904, "start": 1499.04, "end": 1507.6, "text": " all the information. Yeah, okay, so I hope that clarifies. Yeah, thank you. So for now,", "tokens": [50364, 439, 264, 1589, 13, 865, 11, 1392, 11, 370, 286, 1454, 300, 6093, 11221, 13, 865, 11, 1309, 291, 13, 407, 337, 586, 11, 50792], "temperature": 0.0, "avg_logprob": -0.13327872125725998, "compression_ratio": 1.7807692307692307, "no_speech_prob": 0.003677280619740486}, {"id": 249, "seek": 149904, "start": 1508.96, "end": 1513.92, "text": " just know that knowledge graphs have a few forms. Okay, the most simple form is that you take words", "tokens": [50860, 445, 458, 300, 3601, 24877, 362, 257, 1326, 6422, 13, 1033, 11, 264, 881, 2199, 1254, 307, 300, 291, 747, 2283, 51108], "temperature": 0.0, "avg_logprob": -0.13327872125725998, "compression_ratio": 1.7807692307692307, "no_speech_prob": 0.003677280619740486}, {"id": 250, "seek": 149904, "start": 1513.92, "end": 1518.3999999999999, "text": " and then you add another word and then you get another word. So this like describes a relation.", "tokens": [51108, 293, 550, 291, 909, 1071, 1349, 293, 550, 291, 483, 1071, 1349, 13, 407, 341, 411, 15626, 257, 9721, 13, 51332], "temperature": 0.0, "avg_logprob": -0.13327872125725998, "compression_ratio": 1.7807692307692307, "no_speech_prob": 0.003677280619740486}, {"id": 251, "seek": 149904, "start": 1518.3999999999999, "end": 1523.52, "text": " The more advanced form will be to use embeddings. All right, so we will talk more about and then", "tokens": [51332, 440, 544, 7339, 1254, 486, 312, 281, 764, 12240, 29432, 13, 1057, 558, 11, 370, 321, 486, 751, 544, 466, 293, 550, 51588], "temperature": 0.0, "avg_logprob": -0.13327872125725998, "compression_ratio": 1.7807692307692307, "no_speech_prob": 0.003677280619740486}, {"id": 252, "seek": 149904, "start": 1523.52, "end": 1527.04, "text": " of course, the even more advanced form is evolving embeddings or context dependent", "tokens": [51588, 295, 1164, 11, 264, 754, 544, 7339, 1254, 307, 21085, 12240, 29432, 420, 4319, 12334, 51764], "temperature": 0.0, "avg_logprob": -0.13327872125725998, "compression_ratio": 1.7807692307692307, "no_speech_prob": 0.003677280619740486}, {"id": 253, "seek": 152704, "start": 1527.2, "end": 1532.8, "text": " embeddings, which is like the idea that I have. And it's also the idea that large language models", "tokens": [50372, 12240, 29432, 11, 597, 307, 411, 264, 1558, 300, 286, 362, 13, 400, 309, 311, 611, 264, 1558, 300, 2416, 2856, 5245, 50652], "temperature": 0.0, "avg_logprob": -0.0895842103397145, "compression_ratio": 1.8455284552845528, "no_speech_prob": 0.002510670106858015}, {"id": 254, "seek": 152704, "start": 1532.8, "end": 1537.52, "text": " actually kind of use because when we can ground large language models in different contexts,", "tokens": [50652, 767, 733, 295, 764, 570, 562, 321, 393, 2727, 2416, 2856, 5245, 294, 819, 30628, 11, 50888], "temperature": 0.0, "avg_logprob": -0.0895842103397145, "compression_ratio": 1.8455284552845528, "no_speech_prob": 0.002510670106858015}, {"id": 255, "seek": 152704, "start": 1537.52, "end": 1542.96, "text": " you get different outputs. So a large language model is context dependent processing. Okay,", "tokens": [50888, 291, 483, 819, 23930, 13, 407, 257, 2416, 2856, 2316, 307, 4319, 12334, 9007, 13, 1033, 11, 51160], "temperature": 0.0, "avg_logprob": -0.0895842103397145, "compression_ratio": 1.8455284552845528, "no_speech_prob": 0.002510670106858015}, {"id": 256, "seek": 152704, "start": 1542.96, "end": 1546.6399999999999, "text": " if you can embody that kind of context dependence into the knowledge graph,", "tokens": [51160, 498, 291, 393, 42575, 300, 733, 295, 4319, 31704, 666, 264, 3601, 4295, 11, 51344], "temperature": 0.0, "avg_logprob": -0.0895842103397145, "compression_ratio": 1.8455284552845528, "no_speech_prob": 0.002510670106858015}, {"id": 257, "seek": 152704, "start": 1547.52, "end": 1552.8799999999999, "text": " you will have a very powerful knowledge graph. So as you can see, whatever I'm sharing with you", "tokens": [51388, 291, 486, 362, 257, 588, 4005, 3601, 4295, 13, 407, 382, 291, 393, 536, 11, 2035, 286, 478, 5414, 365, 291, 51656], "temperature": 0.0, "avg_logprob": -0.0895842103397145, "compression_ratio": 1.8455284552845528, "no_speech_prob": 0.002510670106858015}, {"id": 258, "seek": 155288, "start": 1552.88, "end": 1557.3600000000001, "text": " here today, I think that I'm not the answer. All right, I'm just sharing with you here because this", "tokens": [50364, 510, 965, 11, 286, 519, 300, 286, 478, 406, 264, 1867, 13, 1057, 558, 11, 286, 478, 445, 5414, 365, 291, 510, 570, 341, 50588], "temperature": 0.0, "avg_logprob": -0.11631761058684319, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.007733679376542568}, {"id": 259, "seek": 155288, "start": 1557.3600000000001, "end": 1564.96, "text": " is what is existing. Okay, I have a grander vision compared to all of the stuff that I'm talking about.", "tokens": [50588, 307, 437, 307, 6741, 13, 1033, 11, 286, 362, 257, 2697, 260, 5201, 5347, 281, 439, 295, 264, 1507, 300, 286, 478, 1417, 466, 13, 50968], "temperature": 0.0, "avg_logprob": -0.11631761058684319, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.007733679376542568}, {"id": 260, "seek": 155288, "start": 1564.96, "end": 1570.64, "text": " Okay, so let's continue. All right, so knowledge graphs, okay, what excites me in knowledge graph", "tokens": [50968, 1033, 11, 370, 718, 311, 2354, 13, 1057, 558, 11, 370, 3601, 24877, 11, 1392, 11, 437, 1624, 3324, 385, 294, 3601, 4295, 51252], "temperature": 0.0, "avg_logprob": -0.11631761058684319, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.007733679376542568}, {"id": 261, "seek": 155288, "start": 1570.64, "end": 1576.16, "text": " is the very notion of hierarchy. And I think hierarchy is key to intelligence. You don't", "tokens": [51252, 307, 264, 588, 10710, 295, 22333, 13, 400, 286, 519, 22333, 307, 2141, 281, 7599, 13, 509, 500, 380, 51528], "temperature": 0.0, "avg_logprob": -0.11631761058684319, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.007733679376542568}, {"id": 262, "seek": 155288, "start": 1576.16, "end": 1581.8400000000001, "text": " process things in just one domain. You process things in many domains. Like if I'm drinking a cup", "tokens": [51528, 1399, 721, 294, 445, 472, 9274, 13, 509, 1399, 721, 294, 867, 25514, 13, 1743, 498, 286, 478, 7583, 257, 4414, 51812], "temperature": 0.0, "avg_logprob": -0.11631761058684319, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.007733679376542568}, {"id": 263, "seek": 158184, "start": 1581.84, "end": 1588.6399999999999, "text": " now, I'm just like drinking water from a cup, not drinking a cup. Drinking water now, I use my", "tokens": [50364, 586, 11, 286, 478, 445, 411, 7583, 1281, 490, 257, 4414, 11, 406, 7583, 257, 4414, 13, 2491, 12408, 1281, 586, 11, 286, 764, 452, 50704], "temperature": 0.0, "avg_logprob": -0.12775705862736356, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0029732983093708754}, {"id": 264, "seek": 158184, "start": 1588.6399999999999, "end": 1593.12, "text": " hand to move like that. But then if I think about, oh, how do I go to school? Then I think about,", "tokens": [50704, 1011, 281, 1286, 411, 300, 13, 583, 550, 498, 286, 519, 466, 11, 1954, 11, 577, 360, 286, 352, 281, 1395, 30, 1396, 286, 519, 466, 11, 50928], "temperature": 0.0, "avg_logprob": -0.12775705862736356, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0029732983093708754}, {"id": 265, "seek": 158184, "start": 1593.12, "end": 1599.04, "text": " oh, I need to do the bus stop, I need to go to the MRT maybe, and then I need to take this bus", "tokens": [50928, 1954, 11, 286, 643, 281, 360, 264, 1255, 1590, 11, 286, 643, 281, 352, 281, 264, 9808, 51, 1310, 11, 293, 550, 286, 643, 281, 747, 341, 1255, 51224], "temperature": 0.0, "avg_logprob": -0.12775705862736356, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0029732983093708754}, {"id": 266, "seek": 158184, "start": 1599.04, "end": 1604.1599999999999, "text": " or this train. So this is a more higher level planning. Okay, if I were to think about like", "tokens": [51224, 420, 341, 3847, 13, 407, 341, 307, 257, 544, 2946, 1496, 5038, 13, 1033, 11, 498, 286, 645, 281, 519, 466, 411, 51480], "temperature": 0.0, "avg_logprob": -0.12775705862736356, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0029732983093708754}, {"id": 267, "seek": 158184, "start": 1604.1599999999999, "end": 1607.76, "text": " how I move my left leg and right leg, left leg, right leg, I will take forever before I do some", "tokens": [51480, 577, 286, 1286, 452, 1411, 1676, 293, 558, 1676, 11, 1411, 1676, 11, 558, 1676, 11, 286, 486, 747, 5680, 949, 286, 360, 512, 51660], "temperature": 0.0, "avg_logprob": -0.12775705862736356, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0029732983093708754}, {"id": 268, "seek": 160776, "start": 1607.76, "end": 1614.56, "text": " planning. So different problems require different levels of solution finding. And I call this", "tokens": [50364, 5038, 13, 407, 819, 2740, 3651, 819, 4358, 295, 3827, 5006, 13, 400, 286, 818, 341, 50704], "temperature": 0.0, "avg_logprob": -0.13022307193640506, "compression_ratio": 1.9145299145299146, "no_speech_prob": 0.00480847479775548}, {"id": 269, "seek": 160776, "start": 1614.56, "end": 1619.44, "text": " different levels of hierarchy. So it might be up challenge, the abstraction of reasoning corpus.", "tokens": [50704, 819, 4358, 295, 22333, 13, 407, 309, 1062, 312, 493, 3430, 11, 264, 37765, 295, 21577, 1181, 31624, 13, 50948], "temperature": 0.0, "avg_logprob": -0.13022307193640506, "compression_ratio": 1.9145299145299146, "no_speech_prob": 0.00480847479775548}, {"id": 270, "seek": 160776, "start": 1619.44, "end": 1624.4, "text": " I use multiple levels of hierarchy, like we have a pixel level, we have an object level,", "tokens": [50948, 286, 764, 3866, 4358, 295, 22333, 11, 411, 321, 362, 257, 19261, 1496, 11, 321, 362, 364, 2657, 1496, 11, 51196], "temperature": 0.0, "avg_logprob": -0.13022307193640506, "compression_ratio": 1.9145299145299146, "no_speech_prob": 0.00480847479775548}, {"id": 271, "seek": 160776, "start": 1625.04, "end": 1629.12, "text": " you know, and then you express the input grid into different forms of hierarchy.", "tokens": [51228, 291, 458, 11, 293, 550, 291, 5109, 264, 4846, 10748, 666, 819, 6422, 295, 22333, 13, 51432], "temperature": 0.0, "avg_logprob": -0.13022307193640506, "compression_ratio": 1.9145299145299146, "no_speech_prob": 0.00480847479775548}, {"id": 272, "seek": 160776, "start": 1629.12, "end": 1633.36, "text": " And I find that this way can solve a lot of problems because different problems require", "tokens": [51432, 400, 286, 915, 300, 341, 636, 393, 5039, 257, 688, 295, 2740, 570, 819, 2740, 3651, 51644], "temperature": 0.0, "avg_logprob": -0.13022307193640506, "compression_ratio": 1.9145299145299146, "no_speech_prob": 0.00480847479775548}, {"id": 273, "seek": 163336, "start": 1633.36, "end": 1637.84, "text": " different approaches to think of it. But you don't solve all problems using trigonometry,", "tokens": [50364, 819, 11587, 281, 519, 295, 309, 13, 583, 291, 500, 380, 5039, 439, 2740, 1228, 35386, 266, 34730, 11, 50588], "temperature": 0.0, "avg_logprob": -0.10102317068311903, "compression_ratio": 1.8658536585365855, "no_speech_prob": 0.002239203080534935}, {"id": 274, "seek": 163336, "start": 1637.84, "end": 1643.1999999999998, "text": " you solve some using algebra, you solve some using set theory. So you have different ways", "tokens": [50588, 291, 5039, 512, 1228, 21989, 11, 291, 5039, 512, 1228, 992, 5261, 13, 407, 291, 362, 819, 2098, 50856], "temperature": 0.0, "avg_logprob": -0.10102317068311903, "compression_ratio": 1.8658536585365855, "no_speech_prob": 0.002239203080534935}, {"id": 275, "seek": 163336, "start": 1643.1999999999998, "end": 1650.7199999999998, "text": " of viewing the problem. And knowledge graph, you can actually use this to extract different,", "tokens": [50856, 295, 17480, 264, 1154, 13, 400, 3601, 4295, 11, 291, 393, 767, 764, 341, 281, 8947, 819, 11, 51232], "temperature": 0.0, "avg_logprob": -0.10102317068311903, "compression_ratio": 1.8658536585365855, "no_speech_prob": 0.002239203080534935}, {"id": 276, "seek": 163336, "start": 1650.7199999999998, "end": 1654.8, "text": " like at the top layers of the knowledge graph, typically are the more broad concepts and the", "tokens": [51232, 411, 412, 264, 1192, 7914, 295, 264, 3601, 4295, 11, 5850, 366, 264, 544, 4152, 10392, 293, 264, 51436], "temperature": 0.0, "avg_logprob": -0.10102317068311903, "compression_ratio": 1.8658536585365855, "no_speech_prob": 0.002239203080534935}, {"id": 277, "seek": 163336, "start": 1654.8, "end": 1661.6, "text": " bottom will be more of the, more of the general concepts. So you can see in this, this is the", "tokens": [51436, 2767, 486, 312, 544, 295, 264, 11, 544, 295, 264, 2674, 10392, 13, 407, 291, 393, 536, 294, 341, 11, 341, 307, 264, 51776], "temperature": 0.0, "avg_logprob": -0.10102317068311903, "compression_ratio": 1.8658536585365855, "no_speech_prob": 0.002239203080534935}, {"id": 278, "seek": 166160, "start": 1662.0, "end": 1667.28, "text": " the SICK knowledge base. SICK is a 30 year old project trying to embody the world's knowledge", "tokens": [50384, 264, 318, 12141, 3601, 3096, 13, 318, 12141, 307, 257, 2217, 1064, 1331, 1716, 1382, 281, 42575, 264, 1002, 311, 3601, 50648], "temperature": 0.0, "avg_logprob": -0.12476990673993085, "compression_ratio": 1.8188976377952757, "no_speech_prob": 0.00281466543674469}, {"id": 279, "seek": 166160, "start": 1667.28, "end": 1672.48, "text": " in a knowledge graph. They are still trying to do it, but it turns out that this is a very", "tokens": [50648, 294, 257, 3601, 4295, 13, 814, 366, 920, 1382, 281, 360, 309, 11, 457, 309, 4523, 484, 300, 341, 307, 257, 588, 50908], "temperature": 0.0, "avg_logprob": -0.12476990673993085, "compression_ratio": 1.8188976377952757, "no_speech_prob": 0.00281466543674469}, {"id": 280, "seek": 166160, "start": 1672.48, "end": 1680.08, "text": " hard thing to do because the knowledge graphs itself, it embodies like one relation is like", "tokens": [50908, 1152, 551, 281, 360, 570, 264, 3601, 24877, 2564, 11, 309, 4605, 6087, 411, 472, 9721, 307, 411, 51288], "temperature": 0.0, "avg_logprob": -0.12476990673993085, "compression_ratio": 1.8188976377952757, "no_speech_prob": 0.00281466543674469}, {"id": 281, "seek": 166160, "start": 1680.08, "end": 1684.56, "text": " a confirmed relation. But sometimes, you know, based on the context, you may not have that", "tokens": [51288, 257, 11341, 9721, 13, 583, 2171, 11, 291, 458, 11, 2361, 322, 264, 4319, 11, 291, 815, 406, 362, 300, 51512], "temperature": 0.0, "avg_logprob": -0.12476990673993085, "compression_ratio": 1.8188976377952757, "no_speech_prob": 0.00281466543674469}, {"id": 282, "seek": 166160, "start": 1684.56, "end": 1689.1999999999998, "text": " confirmed relation. So again, this is the context dependent knowledge graph I'm talking about.", "tokens": [51512, 11341, 9721, 13, 407, 797, 11, 341, 307, 264, 4319, 12334, 3601, 4295, 286, 478, 1417, 466, 13, 51744], "temperature": 0.0, "avg_logprob": -0.12476990673993085, "compression_ratio": 1.8188976377952757, "no_speech_prob": 0.00281466543674469}, {"id": 283, "seek": 168920, "start": 1690.16, "end": 1695.52, "text": " Also, another thing is a lot of times we do things, but we don't really know how to express it", "tokens": [50412, 2743, 11, 1071, 551, 307, 257, 688, 295, 1413, 321, 360, 721, 11, 457, 321, 500, 380, 534, 458, 577, 281, 5109, 309, 50680], "temperature": 0.0, "avg_logprob": -0.11282954450513495, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.0025570448487997055}, {"id": 284, "seek": 168920, "start": 1695.52, "end": 1702.48, "text": " in words. So if you want to express the whole idea of logic in words, it's a very, very difficult", "tokens": [50680, 294, 2283, 13, 407, 498, 291, 528, 281, 5109, 264, 1379, 1558, 295, 9952, 294, 2283, 11, 309, 311, 257, 588, 11, 588, 2252, 51028], "temperature": 0.0, "avg_logprob": -0.11282954450513495, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.0025570448487997055}, {"id": 285, "seek": 168920, "start": 1702.48, "end": 1706.88, "text": " task. Because sometimes we don't even know why we're doing something. Okay, there's bound to be a", "tokens": [51028, 5633, 13, 1436, 2171, 321, 500, 380, 754, 458, 983, 321, 434, 884, 746, 13, 1033, 11, 456, 311, 5472, 281, 312, 257, 51248], "temperature": 0.0, "avg_logprob": -0.11282954450513495, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.0025570448487997055}, {"id": 286, "seek": 168920, "start": 1706.88, "end": 1711.8400000000001, "text": " point of time that logic cannot express things. So you can go and look at this thing called", "tokens": [51248, 935, 295, 565, 300, 9952, 2644, 5109, 721, 13, 407, 291, 393, 352, 293, 574, 412, 341, 551, 1219, 51496], "temperature": 0.0, "avg_logprob": -0.11282954450513495, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.0025570448487997055}, {"id": 287, "seek": 168920, "start": 1711.8400000000001, "end": 1718.88, "text": " Godot incompleteness theorem. If you use mathematical logic to express things, there comes a point in", "tokens": [51496, 1265, 310, 14036, 14657, 15264, 20904, 13, 759, 291, 764, 18894, 9952, 281, 5109, 721, 11, 456, 1487, 257, 935, 294, 51848], "temperature": 0.0, "avg_logprob": -0.11282954450513495, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.0025570448487997055}, {"id": 288, "seek": 171888, "start": 1718.88, "end": 1728.16, "text": " a time whereby logic cannot solve. Because the way to solve it lies beyond logic. It cannot be", "tokens": [50364, 257, 565, 36998, 9952, 2644, 5039, 13, 1436, 264, 636, 281, 5039, 309, 9134, 4399, 9952, 13, 467, 2644, 312, 50828], "temperature": 0.0, "avg_logprob": -0.1365323019499826, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.005441139917820692}, {"id": 289, "seek": 171888, "start": 1728.16, "end": 1735.44, "text": " Godot incompleteness theorem. There's something like this. This sentence is false. So if you can", "tokens": [50828, 1265, 310, 14036, 14657, 15264, 20904, 13, 821, 311, 746, 411, 341, 13, 639, 8174, 307, 7908, 13, 407, 498, 291, 393, 51192], "temperature": 0.0, "avg_logprob": -0.1365323019499826, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.005441139917820692}, {"id": 290, "seek": 171888, "start": 1735.44, "end": 1741.68, "text": " represent this as a Godot number, this kind of sentence, and then you say that, oh, this number", "tokens": [51192, 2906, 341, 382, 257, 1265, 310, 1230, 11, 341, 733, 295, 8174, 11, 293, 550, 291, 584, 300, 11, 1954, 11, 341, 1230, 51504], "temperature": 0.0, "avg_logprob": -0.1365323019499826, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.005441139917820692}, {"id": 291, "seek": 171888, "start": 1741.68, "end": 1746.64, "text": " is true. But then this number says that this number is false. So it's like you have a self-referential", "tokens": [51504, 307, 2074, 13, 583, 550, 341, 1230, 1619, 300, 341, 1230, 307, 7908, 13, 407, 309, 311, 411, 291, 362, 257, 2698, 12, 265, 612, 2549, 51752], "temperature": 0.0, "avg_logprob": -0.1365323019499826, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.005441139917820692}, {"id": 292, "seek": 174664, "start": 1747.3600000000001, "end": 1752.4, "text": " loop. So if you use logical prepositions, and knowledge graph is sort of like a logical", "tokens": [50400, 6367, 13, 407, 498, 291, 764, 14978, 2666, 329, 2451, 11, 293, 3601, 4295, 307, 1333, 295, 411, 257, 14978, 50652], "temperature": 0.0, "avg_logprob": -0.18303302882872907, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0032391983550041914}, {"id": 293, "seek": 174664, "start": 1752.4, "end": 1760.0800000000002, "text": " preposition, A goes to B, B goes to C, you might face this problem that you can actually go in", "tokens": [50652, 2666, 5830, 11, 316, 1709, 281, 363, 11, 363, 1709, 281, 383, 11, 291, 1062, 1851, 341, 1154, 300, 291, 393, 767, 352, 294, 51036], "temperature": 0.0, "avg_logprob": -0.18303302882872907, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0032391983550041914}, {"id": 294, "seek": 174664, "start": 1762.0800000000002, "end": 1768.48, "text": " you can actually go in a loop that contradicts itself. So that's one thing that knowledge", "tokens": [51136, 291, 393, 767, 352, 294, 257, 6367, 300, 28900, 82, 2564, 13, 407, 300, 311, 472, 551, 300, 3601, 51456], "temperature": 0.0, "avg_logprob": -0.18303302882872907, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0032391983550041914}, {"id": 295, "seek": 174664, "start": 1768.48, "end": 1776.0, "text": " graphs may have some issue with if we do it being 100% fact. If A links to B all the time,", "tokens": [51456, 24877, 815, 362, 512, 2734, 365, 498, 321, 360, 309, 885, 2319, 4, 1186, 13, 759, 316, 6123, 281, 363, 439, 264, 565, 11, 51832], "temperature": 0.0, "avg_logprob": -0.18303302882872907, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0032391983550041914}, {"id": 296, "seek": 177600, "start": 1776.08, "end": 1782.64, "text": " sometimes you might actually have a link that contradicts itself. That's one issue of the", "tokens": [50368, 2171, 291, 1062, 767, 362, 257, 2113, 300, 28900, 82, 2564, 13, 663, 311, 472, 2734, 295, 264, 50696], "temperature": 0.0, "avg_logprob": -0.14774247874384341, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.002269403776153922}, {"id": 297, "seek": 177600, "start": 1782.64, "end": 1789.92, "text": " knowledge graph. The other thing is, so burying all these issues, one thing I like about knowledge", "tokens": [50696, 3601, 4295, 13, 440, 661, 551, 307, 11, 370, 2779, 1840, 439, 613, 2663, 11, 472, 551, 286, 411, 466, 3601, 51060], "temperature": 0.0, "avg_logprob": -0.14774247874384341, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.002269403776153922}, {"id": 298, "seek": 177600, "start": 1789.92, "end": 1795.2, "text": " graph is that you can see in this diagram here, you start off with small stuff like thing, and then", "tokens": [51060, 4295, 307, 300, 291, 393, 536, 294, 341, 10686, 510, 11, 291, 722, 766, 365, 1359, 1507, 411, 551, 11, 293, 550, 51324], "temperature": 0.0, "avg_logprob": -0.14774247874384341, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.002269403776153922}, {"id": 299, "seek": 177600, "start": 1795.2, "end": 1801.84, "text": " you go to like individual, you go to collections, and then you have different ways of doing it,", "tokens": [51324, 291, 352, 281, 411, 2609, 11, 291, 352, 281, 16641, 11, 293, 550, 291, 362, 819, 2098, 295, 884, 309, 11, 51656], "temperature": 0.0, "avg_logprob": -0.14774247874384341, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.002269403776153922}, {"id": 300, "seek": 180184, "start": 1801.84, "end": 1809.04, "text": " time, movement, and so on. Then you have agents, actors, plans, and goals. I mean,", "tokens": [50364, 565, 11, 3963, 11, 293, 370, 322, 13, 1396, 291, 362, 12554, 11, 10037, 11, 5482, 11, 293, 5493, 13, 286, 914, 11, 50724], "temperature": 0.0, "avg_logprob": -0.21487240713150774, "compression_ratio": 1.6751824817518248, "no_speech_prob": 0.009713361971080303}, {"id": 301, "seek": 180184, "start": 1809.04, "end": 1812.24, "text": " if you think about it, kind of it's like how large language models is evolving now, right?", "tokens": [50724, 498, 291, 519, 466, 309, 11, 733, 295, 309, 311, 411, 577, 2416, 2856, 5245, 307, 21085, 586, 11, 558, 30, 50884], "temperature": 0.0, "avg_logprob": -0.21487240713150774, "compression_ratio": 1.6751824817518248, "no_speech_prob": 0.009713361971080303}, {"id": 302, "seek": 180184, "start": 1812.24, "end": 1817.1999999999998, "text": " We are kind of at this stage, we are agents now. So after that, we have organization of agents,", "tokens": [50884, 492, 366, 733, 295, 412, 341, 3233, 11, 321, 366, 12554, 586, 13, 407, 934, 300, 11, 321, 362, 4475, 295, 12554, 11, 51132], "temperature": 0.0, "avg_logprob": -0.21487240713150774, "compression_ratio": 1.6751824817518248, "no_speech_prob": 0.009713361971080303}, {"id": 303, "seek": 180184, "start": 1817.1999999999998, "end": 1822.08, "text": " we have activities. Okay, hopefully we don't get to military warfare, you know, because like,", "tokens": [51132, 321, 362, 5354, 13, 1033, 11, 4696, 321, 500, 380, 483, 281, 4632, 24490, 11, 291, 458, 11, 570, 411, 11, 51376], "temperature": 0.0, "avg_logprob": -0.21487240713150774, "compression_ratio": 1.6751824817518248, "no_speech_prob": 0.009713361971080303}, {"id": 304, "seek": 180184, "start": 1822.72, "end": 1828.8, "text": " so this is like the evolution of a population. So it's quite nice, and like you can capture all", "tokens": [51408, 370, 341, 307, 411, 264, 9303, 295, 257, 4415, 13, 407, 309, 311, 1596, 1481, 11, 293, 411, 291, 393, 7983, 439, 51712], "temperature": 0.0, "avg_logprob": -0.21487240713150774, "compression_ratio": 1.6751824817518248, "no_speech_prob": 0.009713361971080303}, {"id": 305, "seek": 182880, "start": 1828.8, "end": 1833.84, "text": " this knowledge from, okay, so I wanted to say this is for more like micro level", "tokens": [50364, 341, 3601, 490, 11, 1392, 11, 370, 286, 1415, 281, 584, 341, 307, 337, 544, 411, 4532, 1496, 50616], "temperature": 0.0, "avg_logprob": -0.10820161171679227, "compression_ratio": 1.8777292576419213, "no_speech_prob": 0.0014850132865831256}, {"id": 306, "seek": 182880, "start": 1834.8, "end": 1841.04, "text": " to more macro level. And the macro level is actually the sum of the micro level. So", "tokens": [50664, 281, 544, 18887, 1496, 13, 400, 264, 18887, 1496, 307, 767, 264, 2408, 295, 264, 4532, 1496, 13, 407, 50976], "temperature": 0.0, "avg_logprob": -0.10820161171679227, "compression_ratio": 1.8777292576419213, "no_speech_prob": 0.0014850132865831256}, {"id": 307, "seek": 182880, "start": 1841.9199999999998, "end": 1845.28, "text": " maybe the arrow should be drawn the other way, the arrow should be drawn like that.", "tokens": [51020, 1310, 264, 11610, 820, 312, 10117, 264, 661, 636, 11, 264, 11610, 820, 312, 10117, 411, 300, 13, 51188], "temperature": 0.0, "avg_logprob": -0.10820161171679227, "compression_ratio": 1.8777292576419213, "no_speech_prob": 0.0014850132865831256}, {"id": 308, "seek": 182880, "start": 1846.08, "end": 1850.48, "text": " You take from the micro stuff, and you go to the macro stuff. So this is the knowledge", "tokens": [51228, 509, 747, 490, 264, 4532, 1507, 11, 293, 291, 352, 281, 264, 18887, 1507, 13, 407, 341, 307, 264, 3601, 51448], "temperature": 0.0, "avg_logprob": -0.10820161171679227, "compression_ratio": 1.8777292576419213, "no_speech_prob": 0.0014850132865831256}, {"id": 309, "seek": 182880, "start": 1850.48, "end": 1855.84, "text": " that we accumulate, right? And knowledge graphs can capture this quite well, because of the way", "tokens": [51448, 300, 321, 33384, 11, 558, 30, 400, 3601, 24877, 393, 7983, 341, 1596, 731, 11, 570, 295, 264, 636, 51716], "temperature": 0.0, "avg_logprob": -0.10820161171679227, "compression_ratio": 1.8777292576419213, "no_speech_prob": 0.0014850132865831256}, {"id": 310, "seek": 185584, "start": 1855.84, "end": 1860.32, "text": " you take from source relation to destination, you can capture from the micro level, you do", "tokens": [50364, 291, 747, 490, 4009, 9721, 281, 12236, 11, 291, 393, 7983, 490, 264, 4532, 1496, 11, 291, 360, 50588], "temperature": 0.0, "avg_logprob": -0.13841732617082267, "compression_ratio": 1.830708661417323, "no_speech_prob": 0.0006616213941015303}, {"id": 311, "seek": 185584, "start": 1860.32, "end": 1867.6799999999998, "text": " all the branching, and then you end up with the micro level, right? So this was in 2016,", "tokens": [50588, 439, 264, 9819, 278, 11, 293, 550, 291, 917, 493, 365, 264, 4532, 1496, 11, 558, 30, 407, 341, 390, 294, 6549, 11, 50956], "temperature": 0.0, "avg_logprob": -0.13841732617082267, "compression_ratio": 1.830708661417323, "no_speech_prob": 0.0006616213941015303}, {"id": 312, "seek": 185584, "start": 1867.6799999999998, "end": 1873.84, "text": " by the way, I couldn't find this in the sick website today, right? So this is the sick knowledge", "tokens": [50956, 538, 264, 636, 11, 286, 2809, 380, 915, 341, 294, 264, 4998, 3144, 965, 11, 558, 30, 407, 341, 307, 264, 4998, 3601, 51264], "temperature": 0.0, "avg_logprob": -0.13841732617082267, "compression_ratio": 1.830708661417323, "no_speech_prob": 0.0006616213941015303}, {"id": 313, "seek": 185584, "start": 1873.84, "end": 1878.3999999999999, "text": " graph. This is like a very, very tiny representation of how the knowledge graph looks like. I just", "tokens": [51264, 4295, 13, 639, 307, 411, 257, 588, 11, 588, 5870, 10290, 295, 577, 264, 3601, 4295, 1542, 411, 13, 286, 445, 51492], "temperature": 0.0, "avg_logprob": -0.13841732617082267, "compression_ratio": 1.830708661417323, "no_speech_prob": 0.0006616213941015303}, {"id": 314, "seek": 185584, "start": 1878.3999999999999, "end": 1882.08, "text": " wanted you to see like how one of the largest knowledge graph in the world looks like. So", "tokens": [51492, 1415, 291, 281, 536, 411, 577, 472, 295, 264, 6443, 3601, 4295, 294, 264, 1002, 1542, 411, 13, 407, 51676], "temperature": 0.0, "avg_logprob": -0.13841732617082267, "compression_ratio": 1.830708661417323, "no_speech_prob": 0.0006616213941015303}, {"id": 315, "seek": 188208, "start": 1882.08, "end": 1886.56, "text": " you can see like you have all these like the fortune companies, you have all these like functions,", "tokens": [50364, 291, 393, 536, 411, 291, 362, 439, 613, 411, 264, 16531, 3431, 11, 291, 362, 439, 613, 411, 6828, 11, 50588], "temperature": 0.0, "avg_logprob": -0.13645949034855284, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.001978151733055711}, {"id": 316, "seek": 188208, "start": 1887.36, "end": 1893.1999999999998, "text": " like all these like look like some form of like math stuff, right? Yeah, so you have like people", "tokens": [50628, 411, 439, 613, 411, 574, 411, 512, 1254, 295, 411, 5221, 1507, 11, 558, 30, 865, 11, 370, 291, 362, 411, 561, 50920], "temperature": 0.0, "avg_logprob": -0.13645949034855284, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.001978151733055711}, {"id": 317, "seek": 188208, "start": 1893.1999999999998, "end": 1899.9199999999998, "text": " over here. So you have different areas of congregation of all this knowledge, right? Then", "tokens": [50920, 670, 510, 13, 407, 291, 362, 819, 3179, 295, 34782, 295, 439, 341, 3601, 11, 558, 30, 1396, 51256], "temperature": 0.0, "avg_logprob": -0.13645949034855284, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.001978151733055711}, {"id": 318, "seek": 188208, "start": 1899.9199999999998, "end": 1904.72, "text": " in order to pass through the knowledge graph, you have to use something very, very similar to SQL,", "tokens": [51256, 294, 1668, 281, 1320, 807, 264, 3601, 4295, 11, 291, 362, 281, 764, 746, 588, 11, 588, 2531, 281, 19200, 11, 51496], "temperature": 0.0, "avg_logprob": -0.13645949034855284, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.001978151733055711}, {"id": 319, "seek": 188208, "start": 1905.36, "end": 1909.9199999999998, "text": " structured query language, you like say that, oh, if I want to get a frightened person, I want to", "tokens": [51528, 18519, 14581, 2856, 11, 291, 411, 584, 300, 11, 1954, 11, 498, 286, 528, 281, 483, 257, 28839, 954, 11, 286, 528, 281, 51756], "temperature": 0.0, "avg_logprob": -0.13645949034855284, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.001978151733055711}, {"id": 320, "seek": 190992, "start": 1909.92, "end": 1914.88, "text": " get the entity X that is a person, and then fuse emotion that is fear at a very high level.", "tokens": [50364, 483, 264, 13977, 1783, 300, 307, 257, 954, 11, 293, 550, 31328, 8913, 300, 307, 4240, 412, 257, 588, 1090, 1496, 13, 50612], "temperature": 0.0, "avg_logprob": -0.1398805095031198, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.001621909555979073}, {"id": 321, "seek": 190992, "start": 1915.52, "end": 1920.8000000000002, "text": " So you have to do this kind of stuff, right? So immediately you can see that knowledge graph", "tokens": [50644, 407, 291, 362, 281, 360, 341, 733, 295, 1507, 11, 558, 30, 407, 4258, 291, 393, 536, 300, 3601, 4295, 50908], "temperature": 0.0, "avg_logprob": -0.1398805095031198, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.001621909555979073}, {"id": 322, "seek": 190992, "start": 1920.8000000000002, "end": 1926.24, "text": " right now can be immediately improved by large language models in one aspect. Okay, and this", "tokens": [50908, 558, 586, 393, 312, 4258, 9689, 538, 2416, 2856, 5245, 294, 472, 4171, 13, 1033, 11, 293, 341, 51180], "temperature": 0.0, "avg_logprob": -0.1398805095031198, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.001621909555979073}, {"id": 323, "seek": 190992, "start": 1926.24, "end": 1930.88, "text": " aspect is that we can straight away use the large language models to generate this structured query", "tokens": [51180, 4171, 307, 300, 321, 393, 2997, 1314, 764, 264, 2416, 2856, 5245, 281, 8460, 341, 18519, 14581, 51412], "temperature": 0.0, "avg_logprob": -0.1398805095031198, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.001621909555979073}, {"id": 324, "seek": 190992, "start": 1930.88, "end": 1939.6000000000001, "text": " language. Okay, so if all of you are thinking about this, like if you want to get like a very,", "tokens": [51412, 2856, 13, 1033, 11, 370, 498, 439, 295, 291, 366, 1953, 466, 341, 11, 411, 498, 291, 528, 281, 483, 411, 257, 588, 11, 51848], "temperature": 0.0, "avg_logprob": -0.1398805095031198, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.001621909555979073}, {"id": 325, "seek": 193960, "start": 1939.6, "end": 1944.1599999999999, "text": " very rigid programming language out, okay, you can actually write what you want in free text", "tokens": [50364, 588, 22195, 9410, 2856, 484, 11, 1392, 11, 291, 393, 767, 2464, 437, 291, 528, 294, 1737, 2487, 50592], "temperature": 0.0, "avg_logprob": -0.1372161917135018, "compression_ratio": 1.7635782747603834, "no_speech_prob": 0.004116986412554979}, {"id": 326, "seek": 193960, "start": 1944.1599999999999, "end": 1948.24, "text": " and then say convert this to SQL, and you can get it out. So you can do the same thing for", "tokens": [50592, 293, 550, 584, 7620, 341, 281, 19200, 11, 293, 291, 393, 483, 309, 484, 13, 407, 291, 393, 360, 264, 912, 551, 337, 50796], "temperature": 0.0, "avg_logprob": -0.1372161917135018, "compression_ratio": 1.7635782747603834, "no_speech_prob": 0.004116986412554979}, {"id": 327, "seek": 193960, "start": 1948.24, "end": 1953.6, "text": " this sick language. You give it some examples of how the language works. You say, I want to get", "tokens": [50796, 341, 4998, 2856, 13, 509, 976, 309, 512, 5110, 295, 577, 264, 2856, 1985, 13, 509, 584, 11, 286, 528, 281, 483, 51064], "temperature": 0.0, "avg_logprob": -0.1372161917135018, "compression_ratio": 1.7635782747603834, "no_speech_prob": 0.004116986412554979}, {"id": 328, "seek": 193960, "start": 1953.6, "end": 1957.9199999999998, "text": " a frightened person and then, you know, chat GPD is quite good at getting stuff like this out.", "tokens": [51064, 257, 28839, 954, 293, 550, 11, 291, 458, 11, 5081, 460, 17349, 307, 1596, 665, 412, 1242, 1507, 411, 341, 484, 13, 51280], "temperature": 0.0, "avg_logprob": -0.1372161917135018, "compression_ratio": 1.7635782747603834, "no_speech_prob": 0.004116986412554979}, {"id": 329, "seek": 193960, "start": 1958.3999999999999, "end": 1964.08, "text": " Okay, no more SQL, right? I love it. Okay, if you need to use SQL, just use chat GPD.", "tokens": [51304, 1033, 11, 572, 544, 19200, 11, 558, 30, 286, 959, 309, 13, 1033, 11, 498, 291, 643, 281, 764, 19200, 11, 445, 764, 5081, 460, 17349, 13, 51588], "temperature": 0.0, "avg_logprob": -0.1372161917135018, "compression_ratio": 1.7635782747603834, "no_speech_prob": 0.004116986412554979}, {"id": 330, "seek": 193960, "start": 1964.08, "end": 1968.9599999999998, "text": " Okay, it's a very good replacement. All right, so this is one way large language models can", "tokens": [51588, 1033, 11, 309, 311, 257, 588, 665, 14419, 13, 1057, 558, 11, 370, 341, 307, 472, 636, 2416, 2856, 5245, 393, 51832], "temperature": 0.0, "avg_logprob": -0.1372161917135018, "compression_ratio": 1.7635782747603834, "no_speech_prob": 0.004116986412554979}, {"id": 331, "seek": 196896, "start": 1968.96, "end": 1972.4, "text": " already help to benefit knowledge graphs right now because it can pass through it", "tokens": [50364, 1217, 854, 281, 5121, 3601, 24877, 558, 586, 570, 309, 393, 1320, 807, 309, 50536], "temperature": 0.0, "avg_logprob": -0.11228086363594487, "compression_ratio": 1.8125, "no_speech_prob": 0.0016628708690404892}, {"id": 332, "seek": 196896, "start": 1973.1200000000001, "end": 1977.92, "text": " using very human readable and understandable syntax. Like this kind of thing is not very human", "tokens": [50572, 1228, 588, 1952, 49857, 293, 25648, 28431, 13, 1743, 341, 733, 295, 551, 307, 406, 588, 1952, 50812], "temperature": 0.0, "avg_logprob": -0.11228086363594487, "compression_ratio": 1.8125, "no_speech_prob": 0.0016628708690404892}, {"id": 333, "seek": 196896, "start": 1977.92, "end": 1983.68, "text": " understandable. You can use free text to do it and we can do it right now. But more importantly,", "tokens": [50812, 25648, 13, 509, 393, 764, 1737, 2487, 281, 360, 309, 293, 321, 393, 360, 309, 558, 586, 13, 583, 544, 8906, 11, 51100], "temperature": 0.0, "avg_logprob": -0.11228086363594487, "compression_ratio": 1.8125, "no_speech_prob": 0.0016628708690404892}, {"id": 334, "seek": 196896, "start": 1983.68, "end": 1990.88, "text": " what can large language models do to help knowledge graphs? Okay, or what can knowledge", "tokens": [51100, 437, 393, 2416, 2856, 5245, 360, 281, 854, 3601, 24877, 30, 1033, 11, 420, 437, 393, 3601, 51460], "temperature": 0.0, "avg_logprob": -0.11228086363594487, "compression_ratio": 1.8125, "no_speech_prob": 0.0016628708690404892}, {"id": 335, "seek": 196896, "start": 1990.88, "end": 1995.1200000000001, "text": " graphs do to help language models? Okay, so before I move on to that, let's just talk about some other", "tokens": [51460, 24877, 360, 281, 854, 2856, 5245, 30, 1033, 11, 370, 949, 286, 1286, 322, 281, 300, 11, 718, 311, 445, 751, 466, 512, 661, 51672], "temperature": 0.0, "avg_logprob": -0.11228086363594487, "compression_ratio": 1.8125, "no_speech_prob": 0.0016628708690404892}, {"id": 336, "seek": 199512, "start": 1995.12, "end": 1999.76, "text": " ideas I have. So I'm actually a reinforcement learning person. So like, I feel like knowledge", "tokens": [50364, 3487, 286, 362, 13, 407, 286, 478, 767, 257, 29280, 2539, 954, 13, 407, 411, 11, 286, 841, 411, 3601, 50596], "temperature": 0.0, "avg_logprob": -0.14791945469232254, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.006128933280706406}, {"id": 337, "seek": 199512, "start": 1999.76, "end": 2004.4799999999998, "text": " graph can also represent stuff like different states, like you have different tiredness,", "tokens": [50596, 4295, 393, 611, 2906, 1507, 411, 819, 4368, 11, 411, 291, 362, 819, 5868, 1287, 11, 50832], "temperature": 0.0, "avg_logprob": -0.14791945469232254, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.006128933280706406}, {"id": 338, "seek": 199512, "start": 2004.4799999999998, "end": 2010.2399999999998, "text": " they drink coffee, and then you're not awake. So if you know in the literature of", "tokens": [50832, 436, 2822, 4982, 11, 293, 550, 291, 434, 406, 15994, 13, 407, 498, 291, 458, 294, 264, 10394, 295, 51120], "temperature": 0.0, "avg_logprob": -0.14791945469232254, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.006128933280706406}, {"id": 339, "seek": 199512, "start": 2013.04, "end": 2016.2399999999998, "text": " reinforcement learning, this like a Markov decision process where these are the states,", "tokens": [51260, 29280, 2539, 11, 341, 411, 257, 3934, 5179, 3537, 1399, 689, 613, 366, 264, 4368, 11, 51420], "temperature": 0.0, "avg_logprob": -0.14791945469232254, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.006128933280706406}, {"id": 340, "seek": 201624, "start": 2017.2, "end": 2021.68, "text": " okay, and these are the actions. And then this is the next thing.", "tokens": [50412, 1392, 11, 293, 613, 366, 264, 5909, 13, 400, 550, 341, 307, 264, 958, 551, 13, 50636], "temperature": 0.0, "avg_logprob": -0.135368248512005, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.003758673556149006}, {"id": 341, "seek": 201624, "start": 2023.52, "end": 2027.44, "text": " Okay, so you can actually use knowledge graphs to represent stuff like this as well.", "tokens": [50728, 1033, 11, 370, 291, 393, 767, 764, 3601, 24877, 281, 2906, 1507, 411, 341, 382, 731, 13, 50924], "temperature": 0.0, "avg_logprob": -0.135368248512005, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.003758673556149006}, {"id": 342, "seek": 201624, "start": 2027.44, "end": 2034.24, "text": " Okay, because it's quite anything that has a link like that, you can represent this easily. Okay, so", "tokens": [50924, 1033, 11, 570, 309, 311, 1596, 1340, 300, 575, 257, 2113, 411, 300, 11, 291, 393, 2906, 341, 3612, 13, 1033, 11, 370, 51264], "temperature": 0.0, "avg_logprob": -0.135368248512005, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.003758673556149006}, {"id": 343, "seek": 201624, "start": 2037.68, "end": 2041.84, "text": " all right, this is perhaps the most important slide for today. Okay, this is not in the paper", "tokens": [51436, 439, 558, 11, 341, 307, 4317, 264, 881, 1021, 4137, 337, 965, 13, 1033, 11, 341, 307, 406, 294, 264, 3035, 51644], "temperature": 0.0, "avg_logprob": -0.135368248512005, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.003758673556149006}, {"id": 344, "seek": 204184, "start": 2041.84, "end": 2047.76, "text": " that I referenced, but this is the thing that I was thinking about. It's like knowledge graph is", "tokens": [50364, 300, 286, 32734, 11, 457, 341, 307, 264, 551, 300, 286, 390, 1953, 466, 13, 467, 311, 411, 3601, 4295, 307, 50660], "temperature": 0.0, "avg_logprob": -0.0924650523973548, "compression_ratio": 1.8446969696969697, "no_speech_prob": 0.00662338687106967}, {"id": 345, "seek": 204184, "start": 2047.76, "end": 2053.2, "text": " actually sort of a tool that can be used by the agent. So like retrieval of method generation", "tokens": [50660, 767, 1333, 295, 257, 2290, 300, 393, 312, 1143, 538, 264, 9461, 13, 407, 411, 19817, 3337, 295, 3170, 5125, 50932], "temperature": 0.0, "avg_logprob": -0.0924650523973548, "compression_ratio": 1.8446969696969697, "no_speech_prob": 0.00662338687106967}, {"id": 346, "seek": 204184, "start": 2053.2, "end": 2057.84, "text": " may not get the right passages because like the embedding space may not be good. Perhaps we can", "tokens": [50932, 815, 406, 483, 264, 558, 31589, 570, 411, 264, 12240, 3584, 1901, 815, 406, 312, 665, 13, 10517, 321, 393, 51164], "temperature": 0.0, "avg_logprob": -0.0924650523973548, "compression_ratio": 1.8446969696969697, "no_speech_prob": 0.00662338687106967}, {"id": 347, "seek": 204184, "start": 2057.84, "end": 2062.16, "text": " use like a form of knowledge graph passing, okay, you can extract relevant parts of the knowledge", "tokens": [51164, 764, 411, 257, 1254, 295, 3601, 4295, 8437, 11, 1392, 11, 291, 393, 8947, 7340, 3166, 295, 264, 3601, 51380], "temperature": 0.0, "avg_logprob": -0.0924650523973548, "compression_ratio": 1.8446969696969697, "no_speech_prob": 0.00662338687106967}, {"id": 348, "seek": 204184, "start": 2062.16, "end": 2067.2799999999997, "text": " graph, you can retrieve the context based on that. Okay, so you can ask the knowledge graph to get you", "tokens": [51380, 4295, 11, 291, 393, 30254, 264, 4319, 2361, 322, 300, 13, 1033, 11, 370, 291, 393, 1029, 264, 3601, 4295, 281, 483, 291, 51636], "temperature": 0.0, "avg_logprob": -0.0924650523973548, "compression_ratio": 1.8446969696969697, "no_speech_prob": 0.00662338687106967}, {"id": 349, "seek": 206728, "start": 2067.28, "end": 2073.6800000000003, "text": " the subgraph. The subgraph, you can then use it to ground the context of the agent and how you use", "tokens": [50364, 264, 1422, 34091, 13, 440, 1422, 34091, 11, 291, 393, 550, 764, 309, 281, 2727, 264, 4319, 295, 264, 9461, 293, 577, 291, 764, 50684], "temperature": 0.0, "avg_logprob": -0.11636280635046581, "compression_ratio": 1.85, "no_speech_prob": 0.0035986555740237236}, {"id": 350, "seek": 206728, "start": 2073.6800000000003, "end": 2078.0, "text": " it to ground. Okay, it's up to you. Okay, some people might use graph neural networks. I don't", "tokens": [50684, 309, 281, 2727, 13, 1033, 11, 309, 311, 493, 281, 291, 13, 1033, 11, 512, 561, 1062, 764, 4295, 18161, 9590, 13, 286, 500, 380, 50900], "temperature": 0.0, "avg_logprob": -0.11636280635046581, "compression_ratio": 1.85, "no_speech_prob": 0.0035986555740237236}, {"id": 351, "seek": 206728, "start": 2078.0, "end": 2082.8, "text": " advocate for that. Okay, one other way of doing it is to just convert it back to free text. Okay,", "tokens": [50900, 14608, 337, 300, 13, 1033, 11, 472, 661, 636, 295, 884, 309, 307, 281, 445, 7620, 309, 646, 281, 1737, 2487, 13, 1033, 11, 51140], "temperature": 0.0, "avg_logprob": -0.11636280635046581, "compression_ratio": 1.85, "no_speech_prob": 0.0035986555740237236}, {"id": 352, "seek": 206728, "start": 2083.44, "end": 2087.92, "text": " as easy as that. So you use the knowledge graph to extract out the relevant purposes and avoid the", "tokens": [51172, 382, 1858, 382, 300, 13, 407, 291, 764, 264, 3601, 4295, 281, 8947, 484, 264, 7340, 9932, 293, 5042, 264, 51396], "temperature": 0.0, "avg_logprob": -0.11636280635046581, "compression_ratio": 1.85, "no_speech_prob": 0.0035986555740237236}, {"id": 353, "seek": 206728, "start": 2087.92, "end": 2092.96, "text": " need for the embedding space, the open AI embeddings. Okay, you use the knowledge graph to", "tokens": [51396, 643, 337, 264, 12240, 3584, 1901, 11, 264, 1269, 7318, 12240, 29432, 13, 1033, 11, 291, 764, 264, 3601, 4295, 281, 51648], "temperature": 0.0, "avg_logprob": -0.11636280635046581, "compression_ratio": 1.85, "no_speech_prob": 0.0035986555740237236}, {"id": 354, "seek": 209296, "start": 2092.96, "end": 2097.6, "text": " extract it. Then you take the stuff that you extract from the knowledge graph, pass it back as text", "tokens": [50364, 8947, 309, 13, 1396, 291, 747, 264, 1507, 300, 291, 8947, 490, 264, 3601, 4295, 11, 1320, 309, 646, 382, 2487, 50596], "temperature": 0.0, "avg_logprob": -0.14827181081302832, "compression_ratio": 1.8479087452471483, "no_speech_prob": 0.002443600445985794}, {"id": 355, "seek": 209296, "start": 2097.6, "end": 2108.16, "text": " and then go back to the agent to ground it. All right. So yeah, one other way of, one good thing", "tokens": [50596, 293, 550, 352, 646, 281, 264, 9461, 281, 2727, 309, 13, 1057, 558, 13, 407, 1338, 11, 472, 661, 636, 295, 11, 472, 665, 551, 51124], "temperature": 0.0, "avg_logprob": -0.14827181081302832, "compression_ratio": 1.8479087452471483, "no_speech_prob": 0.002443600445985794}, {"id": 356, "seek": 209296, "start": 2108.16, "end": 2112.88, "text": " about this is that if you have stuff like if you are doing this for a robot, okay, that experiences", "tokens": [51124, 466, 341, 307, 300, 498, 291, 362, 1507, 411, 498, 291, 366, 884, 341, 337, 257, 7881, 11, 1392, 11, 300, 5235, 51360], "temperature": 0.0, "avg_logprob": -0.14827181081302832, "compression_ratio": 1.8479087452471483, "no_speech_prob": 0.002443600445985794}, {"id": 357, "seek": 209296, "start": 2112.88, "end": 2118.64, "text": " the world, you might actually be able to use this knowledge graph. Okay, I'm conflating the", "tokens": [51360, 264, 1002, 11, 291, 1062, 767, 312, 1075, 281, 764, 341, 3601, 4295, 13, 1033, 11, 286, 478, 1497, 75, 990, 264, 51648], "temperature": 0.0, "avg_logprob": -0.14827181081302832, "compression_ratio": 1.8479087452471483, "no_speech_prob": 0.002443600445985794}, {"id": 358, "seek": 209296, "start": 2118.64, "end": 2122.4, "text": " term knowledge graph, but this knowledge graph can now be the state action state graph. You know,", "tokens": [51648, 1433, 3601, 4295, 11, 457, 341, 3601, 4295, 393, 586, 312, 264, 1785, 3069, 1785, 4295, 13, 509, 458, 11, 51836], "temperature": 0.0, "avg_logprob": -0.14827181081302832, "compression_ratio": 1.8479087452471483, "no_speech_prob": 0.002443600445985794}, {"id": 359, "seek": 212240, "start": 2122.4, "end": 2128.56, "text": " you can actually model relations of the world easily. Like I always believed like we learned", "tokens": [50364, 291, 393, 767, 2316, 2299, 295, 264, 1002, 3612, 13, 1743, 286, 1009, 7847, 411, 321, 3264, 50672], "temperature": 0.0, "avg_logprob": -0.10294498587554356, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.00153262575622648}, {"id": 360, "seek": 212240, "start": 2128.56, "end": 2133.12, "text": " from taking actions in the world. So we can actually build this knowledge graph dynamically.", "tokens": [50672, 490, 1940, 5909, 294, 264, 1002, 13, 407, 321, 393, 767, 1322, 341, 3601, 4295, 43492, 13, 50900], "temperature": 0.0, "avg_logprob": -0.10294498587554356, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.00153262575622648}, {"id": 361, "seek": 212240, "start": 2133.12, "end": 2137.12, "text": " This is the third point. Okay, you can gain knowledge of the world. We can build up this", "tokens": [50900, 639, 307, 264, 2636, 935, 13, 1033, 11, 291, 393, 6052, 3601, 295, 264, 1002, 13, 492, 393, 1322, 493, 341, 51100], "temperature": 0.0, "avg_logprob": -0.10294498587554356, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.00153262575622648}, {"id": 362, "seek": 212240, "start": 2137.12, "end": 2141.28, "text": " knowledge graph bit by bit. All right. And then we can then query this knowledge graph", "tokens": [51100, 3601, 4295, 857, 538, 857, 13, 1057, 558, 13, 400, 550, 321, 393, 550, 14581, 341, 3601, 4295, 51308], "temperature": 0.0, "avg_logprob": -0.10294498587554356, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.00153262575622648}, {"id": 363, "seek": 212240, "start": 2142.32, "end": 2150.1600000000003, "text": " and get answers from the knowledge graph to inform our choices. Okay, so about how we can get this", "tokens": [51360, 293, 483, 6338, 490, 264, 3601, 4295, 281, 1356, 527, 7994, 13, 1033, 11, 370, 466, 577, 321, 393, 483, 341, 51752], "temperature": 0.0, "avg_logprob": -0.10294498587554356, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.00153262575622648}, {"id": 364, "seek": 215016, "start": 2150.16, "end": 2154.96, "text": " part here, this is a huge thing here. Okay, because I believe that there's one thing that's missing", "tokens": [50364, 644, 510, 11, 341, 307, 257, 2603, 551, 510, 13, 1033, 11, 570, 286, 1697, 300, 456, 311, 472, 551, 300, 311, 5361, 50604], "temperature": 0.0, "avg_logprob": -0.1098307308397795, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.002340048085898161}, {"id": 365, "seek": 215016, "start": 2154.96, "end": 2161.04, "text": " in current knowledge graph and this thing is called changing the memory to the context", "tokens": [50604, 294, 2190, 3601, 4295, 293, 341, 551, 307, 1219, 4473, 264, 4675, 281, 264, 4319, 50908], "temperature": 0.0, "avg_logprob": -0.1098307308397795, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.002340048085898161}, {"id": 366, "seek": 215016, "start": 2161.7599999999998, "end": 2167.2799999999997, "text": " at hand. Okay, so I treat the knowledge graph as memory. So like when you retrieve things from", "tokens": [50944, 412, 1011, 13, 1033, 11, 370, 286, 2387, 264, 3601, 4295, 382, 4675, 13, 407, 411, 562, 291, 30254, 721, 490, 51220], "temperature": 0.0, "avg_logprob": -0.1098307308397795, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.002340048085898161}, {"id": 367, "seek": 215016, "start": 2167.2799999999997, "end": 2172.3199999999997, "text": " memory, okay, and then you want to apply it to the current state right now, current state of the", "tokens": [51220, 4675, 11, 1392, 11, 293, 550, 291, 528, 281, 3079, 309, 281, 264, 2190, 1785, 558, 586, 11, 2190, 1785, 295, 264, 51472], "temperature": 0.0, "avg_logprob": -0.1098307308397795, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.002340048085898161}, {"id": 368, "seek": 215016, "start": 2172.3199999999997, "end": 2178.24, "text": " world right now, you don't really want to just use that memory itself. You want to adapt that", "tokens": [51472, 1002, 558, 586, 11, 291, 500, 380, 534, 528, 281, 445, 764, 300, 4675, 2564, 13, 509, 528, 281, 6231, 300, 51768], "temperature": 0.0, "avg_logprob": -0.1098307308397795, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.002340048085898161}, {"id": 369, "seek": 217824, "start": 2178.24, "end": 2184.0, "text": " memory such that it will be relevant in this current state. Like if I have drank like coffee", "tokens": [50364, 4675, 1270, 300, 309, 486, 312, 7340, 294, 341, 2190, 1785, 13, 1743, 498, 286, 362, 21011, 411, 4982, 50652], "temperature": 0.0, "avg_logprob": -0.12034128408516402, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.0027830321341753006}, {"id": 370, "seek": 217824, "start": 2184.8799999999997, "end": 2191.3599999999997, "text": " at school or not, drink coffee at home now, okay, you know, something like that. This, I will need", "tokens": [50696, 412, 1395, 420, 406, 11, 2822, 4982, 412, 1280, 586, 11, 1392, 11, 291, 458, 11, 746, 411, 300, 13, 639, 11, 286, 486, 643, 51020], "temperature": 0.0, "avg_logprob": -0.12034128408516402, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.0027830321341753006}, {"id": 371, "seek": 217824, "start": 2191.3599999999997, "end": 2196.4799999999996, "text": " to adapt that memory of drinking coffee somewhere else and then adapt it back to here. Okay, there's", "tokens": [51020, 281, 6231, 300, 4675, 295, 7583, 4982, 4079, 1646, 293, 550, 6231, 309, 646, 281, 510, 13, 1033, 11, 456, 311, 51276], "temperature": 0.0, "avg_logprob": -0.12034128408516402, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.0027830321341753006}, {"id": 372, "seek": 217824, "start": 2196.4799999999996, "end": 2201.12, "text": " no point in giving me the memory of me drinking coffee somewhere else because it doesn't adapt to", "tokens": [51276, 572, 935, 294, 2902, 385, 264, 4675, 295, 385, 7583, 4982, 4079, 1646, 570, 309, 1177, 380, 6231, 281, 51508], "temperature": 0.0, "avg_logprob": -0.12034128408516402, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.0027830321341753006}, {"id": 373, "seek": 217824, "start": 2201.12, "end": 2205.68, "text": " the current situation. So if you can adapt this knowledge graph to the current situation,", "tokens": [51508, 264, 2190, 2590, 13, 407, 498, 291, 393, 6231, 341, 3601, 4295, 281, 264, 2190, 2590, 11, 51736], "temperature": 0.0, "avg_logprob": -0.12034128408516402, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.0027830321341753006}, {"id": 374, "seek": 220568, "start": 2206.3199999999997, "end": 2210.96, "text": " that will be great. Okay, that will be great. So that's something that I think I'm trying to", "tokens": [50396, 300, 486, 312, 869, 13, 1033, 11, 300, 486, 312, 869, 13, 407, 300, 311, 746, 300, 286, 519, 286, 478, 1382, 281, 50628], "temperature": 0.0, "avg_logprob": -0.09286906038011823, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.001517187338322401}, {"id": 375, "seek": 220568, "start": 2210.96, "end": 2216.72, "text": " look into because you don't just want static knowledge extraction. Okay, you want knowledge", "tokens": [50628, 574, 666, 570, 291, 500, 380, 445, 528, 13437, 3601, 30197, 13, 1033, 11, 291, 528, 3601, 50916], "temperature": 0.0, "avg_logprob": -0.09286906038011823, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.001517187338322401}, {"id": 376, "seek": 220568, "start": 2216.72, "end": 2222.64, "text": " extracted and manipulated to fit the current context. Okay, of course, for those of you all in", "tokens": [50916, 34086, 293, 37161, 281, 3318, 264, 2190, 4319, 13, 1033, 11, 295, 1164, 11, 337, 729, 295, 291, 439, 294, 51212], "temperature": 0.0, "avg_logprob": -0.09286906038011823, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.001517187338322401}, {"id": 377, "seek": 220568, "start": 2222.64, "end": 2227.68, "text": " my discord group, I've been thinking about memory recently. And you know, human memory is very", "tokens": [51212, 452, 32989, 1594, 11, 286, 600, 668, 1953, 466, 4675, 3938, 13, 400, 291, 458, 11, 1952, 4675, 307, 588, 51464], "temperature": 0.0, "avg_logprob": -0.09286906038011823, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.001517187338322401}, {"id": 378, "seek": 220568, "start": 2227.68, "end": 2233.44, "text": " malleable. Like if you think about something, you might actually affect that memory of it.", "tokens": [51464, 275, 11780, 712, 13, 1743, 498, 291, 519, 466, 746, 11, 291, 1062, 767, 3345, 300, 4675, 295, 309, 13, 51752], "temperature": 0.0, "avg_logprob": -0.09286906038011823, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.001517187338322401}, {"id": 379, "seek": 223344, "start": 2233.44, "end": 2237.76, "text": " So like a lot of times people in the childhood, they think that they have certain memories again,", "tokens": [50364, 407, 411, 257, 688, 295, 1413, 561, 294, 264, 9278, 11, 436, 519, 300, 436, 362, 1629, 8495, 797, 11, 50580], "temperature": 0.0, "avg_logprob": -0.15762022399902345, "compression_ratio": 1.8320895522388059, "no_speech_prob": 0.0017909377347677946}, {"id": 380, "seek": 223344, "start": 2237.76, "end": 2242.56, "text": " like maybe you are lost in a supermarket. So if I keep asking you questions about it, I say,", "tokens": [50580, 411, 1310, 291, 366, 2731, 294, 257, 25180, 13, 407, 498, 286, 1066, 3365, 291, 1651, 466, 309, 11, 286, 584, 11, 50820], "temperature": 0.0, "avg_logprob": -0.15762022399902345, "compression_ratio": 1.8320895522388059, "no_speech_prob": 0.0017909377347677946}, {"id": 381, "seek": 223344, "start": 2244.0, "end": 2250.0, "text": " who was the stranger with you when you were lost? Okay, so maybe there was no stranger. But if I keep", "tokens": [50892, 567, 390, 264, 18834, 365, 291, 562, 291, 645, 2731, 30, 1033, 11, 370, 1310, 456, 390, 572, 18834, 13, 583, 498, 286, 1066, 51192], "temperature": 0.0, "avg_logprob": -0.15762022399902345, "compression_ratio": 1.8320895522388059, "no_speech_prob": 0.0017909377347677946}, {"id": 382, "seek": 223344, "start": 2250.0, "end": 2254.48, "text": " asking you guiding questions like that, eventually, you might think of your memories like, oh, yeah,", "tokens": [51192, 3365, 291, 25061, 1651, 411, 300, 11, 4728, 11, 291, 1062, 519, 295, 428, 8495, 411, 11, 1954, 11, 1338, 11, 51416], "temperature": 0.0, "avg_logprob": -0.15762022399902345, "compression_ratio": 1.8320895522388059, "no_speech_prob": 0.0017909377347677946}, {"id": 383, "seek": 223344, "start": 2254.48, "end": 2259.12, "text": " a stranger let me home after I went, I was lost in the in the in the mud. Okay, but that that may", "tokens": [51416, 257, 18834, 718, 385, 1280, 934, 286, 1437, 11, 286, 390, 2731, 294, 264, 294, 264, 294, 264, 8933, 13, 1033, 11, 457, 300, 300, 815, 51648], "temperature": 0.0, "avg_logprob": -0.15762022399902345, "compression_ratio": 1.8320895522388059, "no_speech_prob": 0.0017909377347677946}, {"id": 384, "seek": 225912, "start": 2259.12, "end": 2264.4, "text": " not have happened. That memory has changed because I've asked you certain guiding questions. And then", "tokens": [50364, 406, 362, 2011, 13, 663, 4675, 575, 3105, 570, 286, 600, 2351, 291, 1629, 25061, 1651, 13, 400, 550, 50628], "temperature": 0.0, "avg_logprob": -0.11404330642135055, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.005244947969913483}, {"id": 385, "seek": 225912, "start": 2264.4, "end": 2270.48, "text": " you think that certain things are now in your memory. Right. So whether or not we should change", "tokens": [50628, 291, 519, 300, 1629, 721, 366, 586, 294, 428, 4675, 13, 1779, 13, 407, 1968, 420, 406, 321, 820, 1319, 50932], "temperature": 0.0, "avg_logprob": -0.11404330642135055, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.005244947969913483}, {"id": 386, "seek": 225912, "start": 2270.48, "end": 2277.2, "text": " this memory and affect this knowledge graph, I leave that to future discussion. Okay, because", "tokens": [50932, 341, 4675, 293, 3345, 341, 3601, 4295, 11, 286, 1856, 300, 281, 2027, 5017, 13, 1033, 11, 570, 51268], "temperature": 0.0, "avg_logprob": -0.11404330642135055, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.005244947969913483}, {"id": 387, "seek": 225912, "start": 2277.2, "end": 2281.3599999999997, "text": " this is something very interesting, like should we change the existing memory that we have,", "tokens": [51268, 341, 307, 746, 588, 1880, 11, 411, 820, 321, 1319, 264, 6741, 4675, 300, 321, 362, 11, 51476], "temperature": 0.0, "avg_logprob": -0.11404330642135055, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.005244947969913483}, {"id": 388, "seek": 225912, "start": 2281.3599999999997, "end": 2286.24, "text": " based on the current context, get our brains that our brains do that. Okay, but should we do this", "tokens": [51476, 2361, 322, 264, 2190, 4319, 11, 483, 527, 15442, 300, 527, 15442, 360, 300, 13, 1033, 11, 457, 820, 321, 360, 341, 51720], "temperature": 0.0, "avg_logprob": -0.11404330642135055, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.005244947969913483}, {"id": 389, "seek": 228624, "start": 2286.24, "end": 2292.4799999999996, "text": " for this kind of practical systems? Okay. Yeah, so we say humans hallucinate. Yeah, of course,", "tokens": [50364, 337, 341, 733, 295, 8496, 3652, 30, 1033, 13, 865, 11, 370, 321, 584, 6255, 35212, 13923, 13, 865, 11, 295, 1164, 11, 50676], "temperature": 0.0, "avg_logprob": -0.16187071188902244, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.0013780161971226335}, {"id": 390, "seek": 228624, "start": 2292.4799999999996, "end": 2298.3999999999996, "text": " we hallucinate a lot. And that is why actually, we are quite similar to large English models in", "tokens": [50676, 321, 35212, 13923, 257, 688, 13, 400, 300, 307, 983, 767, 11, 321, 366, 1596, 2531, 281, 2416, 3669, 5245, 294, 50972], "temperature": 0.0, "avg_logprob": -0.16187071188902244, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.0013780161971226335}, {"id": 391, "seek": 228624, "start": 2298.3999999999996, "end": 2302.9599999999996, "text": " that sense. Now people always say large English models not very reliable. Are humans reliable?", "tokens": [50972, 300, 2020, 13, 823, 561, 1009, 584, 2416, 3669, 5245, 406, 588, 12924, 13, 2014, 6255, 12924, 30, 51200], "temperature": 0.0, "avg_logprob": -0.16187071188902244, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.0013780161971226335}, {"id": 392, "seek": 228624, "start": 2302.9599999999996, "end": 2307.4399999999996, "text": " Our memory is not that reliable, actually, if you think about it. But honestly, I cannot trust", "tokens": [51200, 2621, 4675, 307, 406, 300, 12924, 11, 767, 11, 498, 291, 519, 466, 309, 13, 583, 6095, 11, 286, 2644, 3361, 51424], "temperature": 0.0, "avg_logprob": -0.16187071188902244, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.0013780161971226335}, {"id": 393, "seek": 228624, "start": 2307.4399999999996, "end": 2313.68, "text": " my memories that that much, because like sometimes if it's too far away, it can change like the book", "tokens": [51424, 452, 8495, 300, 300, 709, 11, 570, 411, 2171, 498, 309, 311, 886, 1400, 1314, 11, 309, 393, 1319, 411, 264, 1446, 51736], "temperature": 0.0, "avg_logprob": -0.16187071188902244, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.0013780161971226335}, {"id": 394, "seek": 231368, "start": 2313.68, "end": 2320.24, "text": " that I've been reading. It will say that like flashback memories, which people think that are", "tokens": [50364, 300, 286, 600, 668, 3760, 13, 467, 486, 584, 300, 411, 7319, 3207, 8495, 11, 597, 561, 519, 300, 366, 50692], "temperature": 0.0, "avg_logprob": -0.13944568804332189, "compression_ratio": 1.7442748091603053, "no_speech_prob": 0.007827048189938068}, {"id": 395, "seek": 231368, "start": 2320.24, "end": 2324.64, "text": " very, very pertinent, flashback memories are memories like, like, you know, 911 collapse.", "tokens": [50692, 588, 11, 588, 13269, 11058, 11, 7319, 3207, 8495, 366, 8495, 411, 11, 411, 11, 291, 458, 11, 26901, 15584, 13, 50912], "temperature": 0.0, "avg_logprob": -0.13944568804332189, "compression_ratio": 1.7442748091603053, "no_speech_prob": 0.007827048189938068}, {"id": 396, "seek": 231368, "start": 2325.3599999999997, "end": 2329.3599999999997, "text": " People tend to remember what they were doing at that time, because it was so significant.", "tokens": [50948, 3432, 3928, 281, 1604, 437, 436, 645, 884, 412, 300, 565, 11, 570, 309, 390, 370, 4776, 13, 51148], "temperature": 0.0, "avg_logprob": -0.13944568804332189, "compression_ratio": 1.7442748091603053, "no_speech_prob": 0.007827048189938068}, {"id": 397, "seek": 231368, "start": 2329.9199999999996, "end": 2334.56, "text": " It turns out that this flashback memories can be wrong. Okay, it can also be, it can also be", "tokens": [51176, 467, 4523, 484, 300, 341, 7319, 3207, 8495, 393, 312, 2085, 13, 1033, 11, 309, 393, 611, 312, 11, 309, 393, 611, 312, 51408], "temperature": 0.0, "avg_logprob": -0.13944568804332189, "compression_ratio": 1.7442748091603053, "no_speech_prob": 0.007827048189938068}, {"id": 398, "seek": 231368, "start": 2335.3599999999997, "end": 2341.2799999999997, "text": " change. So this is a very interesting thing. You can actually use like the current context", "tokens": [51448, 1319, 13, 407, 341, 307, 257, 588, 1880, 551, 13, 509, 393, 767, 764, 411, 264, 2190, 4319, 51744], "temperature": 0.0, "avg_logprob": -0.13944568804332189, "compression_ratio": 1.7442748091603053, "no_speech_prob": 0.007827048189938068}, {"id": 399, "seek": 234128, "start": 2341.36, "end": 2345.84, "text": " to affect the memory you have. So you might actually affect the knowledge graph about whether", "tokens": [50368, 281, 3345, 264, 4675, 291, 362, 13, 407, 291, 1062, 767, 3345, 264, 3601, 4295, 466, 1968, 50592], "temperature": 0.0, "avg_logprob": -0.12621550629104394, "compression_ratio": 1.77602523659306, "no_speech_prob": 0.004582616034895182}, {"id": 400, "seek": 234128, "start": 2345.84, "end": 2351.2000000000003, "text": " or not we want it to be that way. Okay, we have to think about that. Okay, I digress a bit. Okay.", "tokens": [50592, 420, 406, 321, 528, 309, 281, 312, 300, 636, 13, 1033, 11, 321, 362, 281, 519, 466, 300, 13, 1033, 11, 286, 2528, 735, 257, 857, 13, 1033, 13, 50860], "temperature": 0.0, "avg_logprob": -0.12621550629104394, "compression_ratio": 1.77602523659306, "no_speech_prob": 0.004582616034895182}, {"id": 401, "seek": 234128, "start": 2352.4, "end": 2356.5600000000004, "text": " But let me just get back to topic. Okay, today we have quite a few slides to cover. There are three", "tokens": [50920, 583, 718, 385, 445, 483, 646, 281, 4829, 13, 1033, 11, 965, 321, 362, 1596, 257, 1326, 9788, 281, 2060, 13, 821, 366, 1045, 51128], "temperature": 0.0, "avg_logprob": -0.12621550629104394, "compression_ratio": 1.77602523659306, "no_speech_prob": 0.004582616034895182}, {"id": 402, "seek": 234128, "start": 2356.5600000000004, "end": 2360.5600000000004, "text": " approaches that I want to talk about today. First is that you can use knowledge graph to", "tokens": [51128, 11587, 300, 286, 528, 281, 751, 466, 965, 13, 2386, 307, 300, 291, 393, 764, 3601, 4295, 281, 51328], "temperature": 0.0, "avg_logprob": -0.12621550629104394, "compression_ratio": 1.77602523659306, "no_speech_prob": 0.004582616034895182}, {"id": 403, "seek": 234128, "start": 2360.5600000000004, "end": 2364.96, "text": " enhance your large language models. And this means that you can give it structured stuff,", "tokens": [51328, 11985, 428, 2416, 2856, 5245, 13, 400, 341, 1355, 300, 291, 393, 976, 309, 18519, 1507, 11, 51548], "temperature": 0.0, "avg_logprob": -0.12621550629104394, "compression_ratio": 1.77602523659306, "no_speech_prob": 0.004582616034895182}, {"id": 404, "seek": 234128, "start": 2364.96, "end": 2369.84, "text": " like domain specific knowledge. In some sense, it's like text based grounding is the same as", "tokens": [51548, 411, 9274, 2685, 3601, 13, 682, 512, 2020, 11, 309, 311, 411, 2487, 2361, 46727, 307, 264, 912, 382, 51792], "temperature": 0.0, "avg_logprob": -0.12621550629104394, "compression_ratio": 1.77602523659306, "no_speech_prob": 0.004582616034895182}, {"id": 405, "seek": 236984, "start": 2369.84, "end": 2373.84, "text": " retrieval of mental generation, just that now you take the information from a knowledge graph.", "tokens": [50364, 19817, 3337, 295, 4973, 5125, 11, 445, 300, 586, 291, 747, 264, 1589, 490, 257, 3601, 4295, 13, 50564], "temperature": 0.0, "avg_logprob": -0.0988098838112571, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.001270491979084909}, {"id": 406, "seek": 236984, "start": 2374.7200000000003, "end": 2380.32, "text": " Number two, you use large language models, expressivity, okay, and make a better knowledge", "tokens": [50608, 5118, 732, 11, 291, 764, 2416, 2856, 5245, 11, 5109, 4253, 11, 1392, 11, 293, 652, 257, 1101, 3601, 50888], "temperature": 0.0, "avg_logprob": -0.0988098838112571, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.001270491979084909}, {"id": 407, "seek": 236984, "start": 2380.32, "end": 2384.2400000000002, "text": " graph. Okay, I like this approach as well. Okay, we will see how to do it. And lastly,", "tokens": [50888, 4295, 13, 1033, 11, 286, 411, 341, 3109, 382, 731, 13, 1033, 11, 321, 486, 536, 577, 281, 360, 309, 13, 400, 16386, 11, 51084], "temperature": 0.0, "avg_logprob": -0.0988098838112571, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.001270491979084909}, {"id": 408, "seek": 236984, "start": 2384.2400000000002, "end": 2388.1600000000003, "text": " you combine both approaches, you can get a synergized large language models and knowledge", "tokens": [51084, 291, 10432, 1293, 11587, 11, 291, 393, 483, 257, 33781, 70, 1602, 2416, 2856, 5245, 293, 3601, 51280], "temperature": 0.0, "avg_logprob": -0.0988098838112571, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.001270491979084909}, {"id": 409, "seek": 236984, "start": 2388.1600000000003, "end": 2394.4, "text": " graph. And I think something like this will be able to embody intelligence. Okay, but not the", "tokens": [51280, 4295, 13, 400, 286, 519, 746, 411, 341, 486, 312, 1075, 281, 42575, 7599, 13, 1033, 11, 457, 406, 264, 51592], "temperature": 0.0, "avg_logprob": -0.0988098838112571, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.001270491979084909}, {"id": 410, "seek": 239440, "start": 2394.4, "end": 2401.36, "text": " current knowledge graph. We need to change it to a dynamic knowledge graph. Okay, what is a dynamic", "tokens": [50364, 2190, 3601, 4295, 13, 492, 643, 281, 1319, 309, 281, 257, 8546, 3601, 4295, 13, 1033, 11, 437, 307, 257, 8546, 50712], "temperature": 0.0, "avg_logprob": -0.08294626444327731, "compression_ratio": 1.9428571428571428, "no_speech_prob": 0.008606389164924622}, {"id": 411, "seek": 239440, "start": 2401.36, "end": 2406.88, "text": " knowledge graph? Maybe I'll talk about it next time. Okay, after I flesh out some ideas that I", "tokens": [50712, 3601, 4295, 30, 2704, 286, 603, 751, 466, 309, 958, 565, 13, 1033, 11, 934, 286, 12497, 484, 512, 3487, 300, 286, 50988], "temperature": 0.0, "avg_logprob": -0.08294626444327731, "compression_ratio": 1.9428571428571428, "no_speech_prob": 0.008606389164924622}, {"id": 412, "seek": 239440, "start": 2406.88, "end": 2411.84, "text": " have right now, I will create this dynamic knowledge graph. Okay, I think the current knowledge", "tokens": [50988, 362, 558, 586, 11, 286, 486, 1884, 341, 8546, 3601, 4295, 13, 1033, 11, 286, 519, 264, 2190, 3601, 51236], "temperature": 0.0, "avg_logprob": -0.08294626444327731, "compression_ratio": 1.9428571428571428, "no_speech_prob": 0.008606389164924622}, {"id": 413, "seek": 239440, "start": 2411.84, "end": 2415.92, "text": " graphs are not the answer. We need to have a different kind of knowledge graph. But if we use", "tokens": [51236, 24877, 366, 406, 264, 1867, 13, 492, 643, 281, 362, 257, 819, 733, 295, 3601, 4295, 13, 583, 498, 321, 764, 51440], "temperature": 0.0, "avg_logprob": -0.08294626444327731, "compression_ratio": 1.9428571428571428, "no_speech_prob": 0.008606389164924622}, {"id": 414, "seek": 239440, "start": 2415.92, "end": 2422.2400000000002, "text": " this, I think we can get intelligence. Okay, let's move on to the next point. Approach one,", "tokens": [51440, 341, 11, 286, 519, 321, 393, 483, 7599, 13, 1033, 11, 718, 311, 1286, 322, 281, 264, 958, 935, 13, 29551, 608, 472, 11, 51756], "temperature": 0.0, "avg_logprob": -0.08294626444327731, "compression_ratio": 1.9428571428571428, "no_speech_prob": 0.008606389164924622}, {"id": 415, "seek": 242224, "start": 2422.24, "end": 2427.2, "text": " knowledge graph augmented large language models. Okay, so there are two ways I can,", "tokens": [50364, 3601, 4295, 36155, 2416, 2856, 5245, 13, 1033, 11, 370, 456, 366, 732, 2098, 286, 393, 11, 50612], "temperature": 0.0, "avg_logprob": -0.15960504175201665, "compression_ratio": 1.79182156133829, "no_speech_prob": 0.002099080244079232}, {"id": 416, "seek": 242224, "start": 2427.2, "end": 2431.2, "text": " I summarize the paper in two ways. The main thing is one, you can just put the knowledge graph as", "tokens": [50612, 286, 20858, 264, 3035, 294, 732, 2098, 13, 440, 2135, 551, 307, 472, 11, 291, 393, 445, 829, 264, 3601, 4295, 382, 50812], "temperature": 0.0, "avg_logprob": -0.15960504175201665, "compression_ratio": 1.79182156133829, "no_speech_prob": 0.002099080244079232}, {"id": 417, "seek": 242224, "start": 2431.2, "end": 2436.3999999999996, "text": " text. And the other one is treat this as an object. And what kind of object? Okay, you either use like", "tokens": [50812, 2487, 13, 400, 264, 661, 472, 307, 2387, 341, 382, 364, 2657, 13, 400, 437, 733, 295, 2657, 30, 1033, 11, 291, 2139, 764, 411, 51072], "temperature": 0.0, "avg_logprob": -0.15960504175201665, "compression_ratio": 1.79182156133829, "no_speech_prob": 0.002099080244079232}, {"id": 418, "seek": 242224, "start": 2436.3999999999996, "end": 2441.3599999999997, "text": " graph neural networks, or you can use an embedding space. I mean, the one that was used was trans", "tokens": [51072, 4295, 18161, 9590, 11, 420, 291, 393, 764, 364, 12240, 3584, 1901, 13, 286, 914, 11, 264, 472, 300, 390, 1143, 390, 1145, 51320], "temperature": 0.0, "avg_logprob": -0.15960504175201665, "compression_ratio": 1.79182156133829, "no_speech_prob": 0.002099080244079232}, {"id": 419, "seek": 242224, "start": 2441.3599999999997, "end": 2448.08, "text": " e trans embedding. You can go and search the paper trans. So this are some ways that we can use the", "tokens": [51320, 308, 1145, 12240, 3584, 13, 509, 393, 352, 293, 3164, 264, 3035, 1145, 13, 407, 341, 366, 512, 2098, 300, 321, 393, 764, 264, 51656], "temperature": 0.0, "avg_logprob": -0.15960504175201665, "compression_ratio": 1.79182156133829, "no_speech_prob": 0.002099080244079232}, {"id": 420, "seek": 244808, "start": 2448.08, "end": 2455.12, "text": " knowledge graph, okay, to pass it. Let's go through the first way. So the first, oh, sorry,", "tokens": [50364, 3601, 4295, 11, 1392, 11, 281, 1320, 309, 13, 961, 311, 352, 807, 264, 700, 636, 13, 407, 264, 700, 11, 1954, 11, 2597, 11, 50716], "temperature": 0.0, "avg_logprob": -0.12350814954369468, "compression_ratio": 1.741444866920152, "no_speech_prob": 0.005190983414649963}, {"id": 421, "seek": 244808, "start": 2455.12, "end": 2460.88, "text": " this is basically a pipeline for retrieval of large language models grounding. First,", "tokens": [50716, 341, 307, 1936, 257, 15517, 337, 19817, 3337, 295, 2416, 2856, 5245, 46727, 13, 2386, 11, 51004], "temperature": 0.0, "avg_logprob": -0.12350814954369468, "compression_ratio": 1.741444866920152, "no_speech_prob": 0.005190983414649963}, {"id": 422, "seek": 244808, "start": 2460.88, "end": 2464.16, "text": " you use some form of knowledge retrieval, like, you know, retrieval method generation,", "tokens": [51004, 291, 764, 512, 1254, 295, 3601, 19817, 3337, 11, 411, 11, 291, 458, 11, 19817, 3337, 3170, 5125, 11, 51168], "temperature": 0.0, "avg_logprob": -0.12350814954369468, "compression_ratio": 1.741444866920152, "no_speech_prob": 0.005190983414649963}, {"id": 423, "seek": 244808, "start": 2464.16, "end": 2470.08, "text": " you use cosine similarity, you get certain facts or some documents. So I'm just relating this to", "tokens": [51168, 291, 764, 23565, 32194, 11, 291, 483, 1629, 9130, 420, 512, 8512, 13, 407, 286, 478, 445, 23968, 341, 281, 51464], "temperature": 0.0, "avg_logprob": -0.12350814954369468, "compression_ratio": 1.741444866920152, "no_speech_prob": 0.005190983414649963}, {"id": 424, "seek": 244808, "start": 2470.08, "end": 2474.48, "text": " retrieval method generation, because they are almost the same. All right, you take in the facts,", "tokens": [51464, 19817, 3337, 3170, 5125, 11, 570, 436, 366, 1920, 264, 912, 13, 1057, 558, 11, 291, 747, 294, 264, 9130, 11, 51684], "temperature": 0.0, "avg_logprob": -0.12350814954369468, "compression_ratio": 1.741444866920152, "no_speech_prob": 0.005190983414649963}, {"id": 425, "seek": 247448, "start": 2474.48, "end": 2479.04, "text": " you ground the large language model, you get the answer. Okay, and over here in the paper,", "tokens": [50364, 291, 2727, 264, 2416, 2856, 2316, 11, 291, 483, 264, 1867, 13, 1033, 11, 293, 670, 510, 294, 264, 3035, 11, 50592], "temperature": 0.0, "avg_logprob": -0.1494960113310478, "compression_ratio": 2.0541516245487363, "no_speech_prob": 0.003689028788357973}, {"id": 426, "seek": 247448, "start": 2479.04, "end": 2482.72, "text": " they put back propagation. But you know, how are you going to back propagate this knowledge retrieval?", "tokens": [50592, 436, 829, 646, 38377, 13, 583, 291, 458, 11, 577, 366, 291, 516, 281, 646, 48256, 341, 3601, 19817, 3337, 30, 50776], "temperature": 0.0, "avg_logprob": -0.1494960113310478, "compression_ratio": 2.0541516245487363, "no_speech_prob": 0.003689028788357973}, {"id": 427, "seek": 247448, "start": 2483.6, "end": 2488.4, "text": " You're going to end up with some, like, very, very weird way of doing back propagation. I don't", "tokens": [50820, 509, 434, 516, 281, 917, 493, 365, 512, 11, 411, 11, 588, 11, 588, 3657, 636, 295, 884, 646, 38377, 13, 286, 500, 380, 51060], "temperature": 0.0, "avg_logprob": -0.1494960113310478, "compression_ratio": 2.0541516245487363, "no_speech_prob": 0.003689028788357973}, {"id": 428, "seek": 247448, "start": 2488.4, "end": 2492.16, "text": " think back propagation, I don't think back propagation is the answer here. Maybe you want", "tokens": [51060, 519, 646, 38377, 11, 286, 500, 380, 519, 646, 38377, 307, 264, 1867, 510, 13, 2704, 291, 528, 51248], "temperature": 0.0, "avg_logprob": -0.1494960113310478, "compression_ratio": 2.0541516245487363, "no_speech_prob": 0.003689028788357973}, {"id": 429, "seek": 247448, "start": 2492.16, "end": 2497.04, "text": " to back propagate your LM to find you think, okay, I grant that. But this part here, to back", "tokens": [51248, 281, 646, 48256, 428, 46529, 281, 915, 291, 519, 11, 1392, 11, 286, 6386, 300, 13, 583, 341, 644, 510, 11, 281, 646, 51492], "temperature": 0.0, "avg_logprob": -0.1494960113310478, "compression_ratio": 2.0541516245487363, "no_speech_prob": 0.003689028788357973}, {"id": 430, "seek": 247448, "start": 2497.04, "end": 2501.84, "text": " propagate to the knowledge retrieval, I don't think that should be done. All right, because this", "tokens": [51492, 48256, 281, 264, 3601, 19817, 3337, 11, 286, 500, 380, 519, 300, 820, 312, 1096, 13, 1057, 558, 11, 570, 341, 51732], "temperature": 0.0, "avg_logprob": -0.1494960113310478, "compression_ratio": 2.0541516245487363, "no_speech_prob": 0.003689028788357973}, {"id": 431, "seek": 250184, "start": 2501.92, "end": 2508.0, "text": " back propagation thing will lead to, like, changes in embedding space. And then if you", "tokens": [50368, 646, 38377, 551, 486, 1477, 281, 11, 411, 11, 2962, 294, 12240, 3584, 1901, 13, 400, 550, 498, 291, 50672], "temperature": 0.0, "avg_logprob": -0.12721499529751865, "compression_ratio": 2.043103448275862, "no_speech_prob": 0.002376571763306856}, {"id": 432, "seek": 250184, "start": 2508.0, "end": 2513.52, "text": " change your knowledge retrieval, you also need to change your large language model. It's a never", "tokens": [50672, 1319, 428, 3601, 19817, 3337, 11, 291, 611, 643, 281, 1319, 428, 2416, 2856, 2316, 13, 467, 311, 257, 1128, 50948], "temperature": 0.0, "avg_logprob": -0.12721499529751865, "compression_ratio": 2.043103448275862, "no_speech_prob": 0.002376571763306856}, {"id": 433, "seek": 250184, "start": 2513.52, "end": 2518.0, "text": " ending cycle of chasing each other. Like, if you change the knowledge embeddings for the knowledge", "tokens": [50948, 8121, 6586, 295, 17876, 1184, 661, 13, 1743, 11, 498, 291, 1319, 264, 3601, 12240, 29432, 337, 264, 3601, 51172], "temperature": 0.0, "avg_logprob": -0.12721499529751865, "compression_ratio": 2.043103448275862, "no_speech_prob": 0.002376571763306856}, {"id": 434, "seek": 250184, "start": 2518.0, "end": 2521.44, "text": " retrieval, you also need to change how you interpret them in the large language model. So,", "tokens": [51172, 19817, 3337, 11, 291, 611, 643, 281, 1319, 577, 291, 7302, 552, 294, 264, 2416, 2856, 2316, 13, 407, 11, 51344], "temperature": 0.0, "avg_logprob": -0.12721499529751865, "compression_ratio": 2.043103448275862, "no_speech_prob": 0.002376571763306856}, {"id": 435, "seek": 250184, "start": 2522.32, "end": 2526.2400000000002, "text": " yeah, I don't think you should use back propagation for the knowledge retrieval. You should probably", "tokens": [51388, 1338, 11, 286, 500, 380, 519, 291, 820, 764, 646, 38377, 337, 264, 3601, 19817, 3337, 13, 509, 820, 1391, 51584], "temperature": 0.0, "avg_logprob": -0.12721499529751865, "compression_ratio": 2.043103448275862, "no_speech_prob": 0.002376571763306856}, {"id": 436, "seek": 252624, "start": 2526.24, "end": 2532.0, "text": " use, like, memory methods, other methods, like you can say that, okay, what worked, what did not", "tokens": [50364, 764, 11, 411, 11, 4675, 7150, 11, 661, 7150, 11, 411, 291, 393, 584, 300, 11, 1392, 11, 437, 2732, 11, 437, 630, 406, 50652], "temperature": 0.0, "avg_logprob": -0.12067557603884967, "compression_ratio": 1.8875968992248062, "no_speech_prob": 0.006071124691516161}, {"id": 437, "seek": 252624, "start": 2532.0, "end": 2538.64, "text": " work, what worked, what did not work. Okay, you can reference this paper called Voyager. Okay,", "tokens": [50652, 589, 11, 437, 2732, 11, 437, 630, 406, 589, 13, 1033, 11, 291, 393, 6408, 341, 3035, 1219, 25563, 3557, 13, 1033, 11, 50984], "temperature": 0.0, "avg_logprob": -0.12067557603884967, "compression_ratio": 1.8875968992248062, "no_speech_prob": 0.006071124691516161}, {"id": 438, "seek": 252624, "start": 2538.64, "end": 2542.7999999999997, "text": " so there are these automatic curriculum learner. I think you should train the knowledge retrieval", "tokens": [50984, 370, 456, 366, 613, 12509, 14302, 33347, 13, 286, 519, 291, 820, 3847, 264, 3601, 19817, 3337, 51192], "temperature": 0.0, "avg_logprob": -0.12067557603884967, "compression_ratio": 1.8875968992248062, "no_speech_prob": 0.006071124691516161}, {"id": 439, "seek": 252624, "start": 2542.7999999999997, "end": 2547.2799999999997, "text": " like the automated curriculum learner. You just ground it in some examples of what works, what", "tokens": [51192, 411, 264, 18473, 14302, 33347, 13, 509, 445, 2727, 309, 294, 512, 5110, 295, 437, 1985, 11, 437, 51416], "temperature": 0.0, "avg_logprob": -0.12067557603884967, "compression_ratio": 1.8875968992248062, "no_speech_prob": 0.006071124691516161}, {"id": 440, "seek": 252624, "start": 2547.2799999999997, "end": 2554.3999999999996, "text": " doesn't work. You don't have to use back propagation for that. Okay, so the main pathway for knowledge", "tokens": [51416, 1177, 380, 589, 13, 509, 500, 380, 362, 281, 764, 646, 38377, 337, 300, 13, 1033, 11, 370, 264, 2135, 18590, 337, 3601, 51772], "temperature": 0.0, "avg_logprob": -0.12067557603884967, "compression_ratio": 1.8875968992248062, "no_speech_prob": 0.006071124691516161}, {"id": 441, "seek": 255440, "start": 2554.4, "end": 2559.36, "text": " graph for tax, for LMS is like that. You take the knowledge graph, pass through it, get some", "tokens": [50364, 4295, 337, 3366, 11, 337, 441, 10288, 307, 411, 300, 13, 509, 747, 264, 3601, 4295, 11, 1320, 807, 309, 11, 483, 512, 50612], "temperature": 0.0, "avg_logprob": -0.1529648185956596, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.0036831337492913008}, {"id": 442, "seek": 255440, "start": 2560.1600000000003, "end": 2565.2000000000003, "text": " facts, and then feed it into the large language model. Okay, that's the main pipeline. Okay,", "tokens": [50652, 9130, 11, 293, 550, 3154, 309, 666, 264, 2416, 2856, 2316, 13, 1033, 11, 300, 311, 264, 2135, 15517, 13, 1033, 11, 50904], "temperature": 0.0, "avg_logprob": -0.1529648185956596, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.0036831337492913008}, {"id": 443, "seek": 255440, "start": 2565.2000000000003, "end": 2577.6, "text": " questions on this? Okay, let's move on. Okay, so this is one of my favorite papers. Okay, this is", "tokens": [50904, 1651, 322, 341, 30, 1033, 11, 718, 311, 1286, 322, 13, 1033, 11, 370, 341, 307, 472, 295, 452, 2954, 10577, 13, 1033, 11, 341, 307, 51524], "temperature": 0.0, "avg_logprob": -0.1529648185956596, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.0036831337492913008}, {"id": 444, "seek": 255440, "start": 2577.6, "end": 2583.6800000000003, "text": " the Generative Agents paper. They have 20 agents in the sandbox interacting with each other. And", "tokens": [51524, 264, 15409, 1166, 2725, 791, 3035, 13, 814, 362, 945, 12554, 294, 264, 42115, 18017, 365, 1184, 661, 13, 400, 51828], "temperature": 0.0, "avg_logprob": -0.1529648185956596, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.0036831337492913008}, {"id": 445, "seek": 258368, "start": 2583.68, "end": 2588.8799999999997, "text": " one thing that struck me quite well for this paper is that they actually use JSON structure", "tokens": [50364, 472, 551, 300, 13159, 385, 1596, 731, 337, 341, 3035, 307, 300, 436, 767, 764, 31828, 3877, 50624], "temperature": 0.0, "avg_logprob": -0.14535904893971452, "compression_ratio": 1.663716814159292, "no_speech_prob": 0.0021348732989281416}, {"id": 446, "seek": 258368, "start": 2588.8799999999997, "end": 2596.16, "text": " to ground the actions. So, for example, if you want to ask like Eddie Lin, he currently is in", "tokens": [50624, 281, 2727, 264, 5909, 13, 407, 11, 337, 1365, 11, 498, 291, 528, 281, 1029, 411, 23911, 9355, 11, 415, 4362, 307, 294, 50988], "temperature": 0.0, "avg_logprob": -0.14535904893971452, "compression_ratio": 1.663716814159292, "no_speech_prob": 0.0021348732989281416}, {"id": 447, "seek": 258368, "start": 2596.16, "end": 2601.52, "text": " the Lin family's house. He's in the bedroom actually on the desk. Okay, so you can ask the", "tokens": [50988, 264, 9355, 1605, 311, 1782, 13, 634, 311, 294, 264, 11211, 767, 322, 264, 10026, 13, 1033, 11, 370, 291, 393, 1029, 264, 51256], "temperature": 0.0, "avg_logprob": -0.14535904893971452, "compression_ratio": 1.663716814159292, "no_speech_prob": 0.0021348732989281416}, {"id": 448, "seek": 258368, "start": 2601.52, "end": 2606.72, "text": " agent, okay, this is actually the chatGPD prompt. Okay, you can ask the agent like, okay, these are", "tokens": [51256, 9461, 11, 1392, 11, 341, 307, 767, 264, 5081, 38, 17349, 12391, 13, 1033, 11, 291, 393, 1029, 264, 9461, 411, 11, 1392, 11, 613, 366, 51516], "temperature": 0.0, "avg_logprob": -0.14535904893971452, "compression_ratio": 1.663716814159292, "no_speech_prob": 0.0021348732989281416}, {"id": 449, "seek": 260672, "start": 2606.72, "end": 2614.3999999999996, "text": " the other areas that we have. Okay, and all these other areas are obtained from the JSON.", "tokens": [50364, 264, 661, 3179, 300, 321, 362, 13, 1033, 11, 293, 439, 613, 661, 3179, 366, 14879, 490, 264, 31828, 13, 50748], "temperature": 0.0, "avg_logprob": -0.1253521500564203, "compression_ratio": 1.7112299465240641, "no_speech_prob": 0.003580454969778657}, {"id": 450, "seek": 260672, "start": 2616.56, "end": 2619.12, "text": " Actually, the JSON is like a knowledge graph.", "tokens": [50856, 5135, 11, 264, 31828, 307, 411, 257, 3601, 4295, 13, 50984], "temperature": 0.0, "avg_logprob": -0.1253521500564203, "compression_ratio": 1.7112299465240641, "no_speech_prob": 0.003580454969778657}, {"id": 451, "seek": 260672, "start": 2621.8399999999997, "end": 2627.7599999999998, "text": " Okay, because we actually have hierarchies like Lin family house has a bedroom, has a study room,", "tokens": [51120, 1033, 11, 570, 321, 767, 362, 35250, 530, 411, 9355, 1605, 1782, 575, 257, 11211, 11, 575, 257, 2979, 1808, 11, 51416], "temperature": 0.0, "avg_logprob": -0.1253521500564203, "compression_ratio": 1.7112299465240641, "no_speech_prob": 0.003580454969778657}, {"id": 452, "seek": 260672, "start": 2627.7599999999998, "end": 2632.0, "text": " has a kitchen, you know, this is something like a knowledge graph. If you ask me like,", "tokens": [51416, 575, 257, 6525, 11, 291, 458, 11, 341, 307, 746, 411, 257, 3601, 4295, 13, 759, 291, 1029, 385, 411, 11, 51628], "temperature": 0.0, "avg_logprob": -0.1253521500564203, "compression_ratio": 1.7112299465240641, "no_speech_prob": 0.003580454969778657}, {"id": 453, "seek": 263200, "start": 2632.0, "end": 2636.72, "text": " they are just like representing the hierarchy of the house. Like, I mean, if you want to treat it", "tokens": [50364, 436, 366, 445, 411, 13460, 264, 22333, 295, 264, 1782, 13, 1743, 11, 286, 914, 11, 498, 291, 528, 281, 2387, 309, 50600], "temperature": 0.0, "avg_logprob": -0.16280027536245492, "compression_ratio": 1.868020304568528, "no_speech_prob": 0.004276187624782324}, {"id": 454, "seek": 263200, "start": 2636.72, "end": 2642.48, "text": " as a knowledge graph, you will say like, this is the house. So, this is the Lin's house. Lin's house.", "tokens": [50600, 382, 257, 3601, 4295, 11, 291, 486, 584, 411, 11, 341, 307, 264, 1782, 13, 407, 11, 341, 307, 264, 9355, 311, 1782, 13, 9355, 311, 1782, 13, 50888], "temperature": 0.0, "avg_logprob": -0.16280027536245492, "compression_ratio": 1.868020304568528, "no_speech_prob": 0.004276187624782324}, {"id": 455, "seek": 263200, "start": 2643.28, "end": 2649.52, "text": " Lin's house. I've just put Lin's age. Okay, then you can have like, the relation will be contains,", "tokens": [50928, 9355, 311, 1782, 13, 286, 600, 445, 829, 9355, 311, 3205, 13, 1033, 11, 550, 291, 393, 362, 411, 11, 264, 9721, 486, 312, 8306, 11, 51240], "temperature": 0.0, "avg_logprob": -0.16280027536245492, "compression_ratio": 1.868020304568528, "no_speech_prob": 0.004276187624782324}, {"id": 456, "seek": 263200, "start": 2650.72, "end": 2655.44, "text": " okay, or comprises, I mean, contains, then you can have like bedroom.", "tokens": [51300, 1392, 11, 420, 16802, 3598, 11, 286, 914, 11, 8306, 11, 550, 291, 393, 362, 411, 11211, 13, 51536], "temperature": 0.0, "avg_logprob": -0.16280027536245492, "compression_ratio": 1.868020304568528, "no_speech_prob": 0.004276187624782324}, {"id": 457, "seek": 265544, "start": 2656.32, "end": 2662.48, "text": " Yeah, so the JSON kind of hierarchy is a subset of what a knowledge graph can embody.", "tokens": [50408, 865, 11, 370, 264, 31828, 733, 295, 22333, 307, 257, 25993, 295, 437, 257, 3601, 4295, 393, 42575, 13, 50716], "temperature": 0.0, "avg_logprob": -0.1366984208424886, "compression_ratio": 1.8674698795180722, "no_speech_prob": 0.0010879222536459565}, {"id": 458, "seek": 265544, "start": 2663.2000000000003, "end": 2669.36, "text": " All right, so I treat this as a knowledge graph. So you can, you can sort of ground the agent.", "tokens": [50752, 1057, 558, 11, 370, 286, 2387, 341, 382, 257, 3601, 4295, 13, 407, 291, 393, 11, 291, 393, 1333, 295, 2727, 264, 9461, 13, 51060], "temperature": 0.0, "avg_logprob": -0.1366984208424886, "compression_ratio": 1.8674698795180722, "no_speech_prob": 0.0010879222536459565}, {"id": 459, "seek": 265544, "start": 2670.16, "end": 2674.56, "text": " Okay, like this is what the agent knows. Okay, this is the current memory that the agent has", "tokens": [51100, 1033, 11, 411, 341, 307, 437, 264, 9461, 3255, 13, 1033, 11, 341, 307, 264, 2190, 4675, 300, 264, 9461, 575, 51320], "temperature": 0.0, "avg_logprob": -0.1366984208424886, "compression_ratio": 1.8674698795180722, "no_speech_prob": 0.0010879222536459565}, {"id": 460, "seek": 265544, "start": 2674.56, "end": 2679.84, "text": " as a form of knowledge graph. Like this are the kind of areas that we actually know from the world.", "tokens": [51320, 382, 257, 1254, 295, 3601, 4295, 13, 1743, 341, 366, 264, 733, 295, 3179, 300, 321, 767, 458, 490, 264, 1002, 13, 51584], "temperature": 0.0, "avg_logprob": -0.1366984208424886, "compression_ratio": 1.8674698795180722, "no_speech_prob": 0.0010879222536459565}, {"id": 461, "seek": 265544, "start": 2679.84, "end": 2684.96, "text": " Okay, this is like, if you talk about grid cells and play cells, maybe find out more areas.", "tokens": [51584, 1033, 11, 341, 307, 411, 11, 498, 291, 751, 466, 10748, 5438, 293, 862, 5438, 11, 1310, 915, 484, 544, 3179, 13, 51840], "temperature": 0.0, "avg_logprob": -0.1366984208424886, "compression_ratio": 1.8674698795180722, "no_speech_prob": 0.0010879222536459565}, {"id": 462, "seek": 268496, "start": 2685.04, "end": 2689.52, "text": " Okay, you can ground them. Okay, these are your semantic knowledge or oppositional knowledge that", "tokens": [50368, 1033, 11, 291, 393, 2727, 552, 13, 1033, 11, 613, 366, 428, 47982, 3601, 420, 4665, 2628, 3601, 300, 50592], "temperature": 0.0, "avg_logprob": -0.12119127909342448, "compression_ratio": 2.0, "no_speech_prob": 0.001847803476266563}, {"id": 463, "seek": 268496, "start": 2689.52, "end": 2695.04, "text": " you have about the world. And actually, these two are the positional knowledge. This is the first", "tokens": [50592, 291, 362, 466, 264, 1002, 13, 400, 767, 11, 613, 732, 366, 264, 2535, 304, 3601, 13, 639, 307, 264, 700, 50868], "temperature": 0.0, "avg_logprob": -0.12119127909342448, "compression_ratio": 2.0, "no_speech_prob": 0.001847803476266563}, {"id": 464, "seek": 268496, "start": 2695.04, "end": 2702.56, "text": " one is knowledge about the house, knowledge about house. And then the second one is knowledge about", "tokens": [50868, 472, 307, 3601, 466, 264, 1782, 11, 3601, 466, 1782, 13, 400, 550, 264, 1150, 472, 307, 3601, 466, 51244], "temperature": 0.0, "avg_logprob": -0.12119127909342448, "compression_ratio": 2.0, "no_speech_prob": 0.001847803476266563}, {"id": 465, "seek": 268496, "start": 2702.56, "end": 2714.16, "text": " the world. Yeah. So you, you have all this knowledge, you can ground the agent to choose", "tokens": [51244, 264, 1002, 13, 865, 13, 407, 291, 11, 291, 362, 439, 341, 3601, 11, 291, 393, 2727, 264, 9461, 281, 2826, 51824], "temperature": 0.0, "avg_logprob": -0.12119127909342448, "compression_ratio": 2.0, "no_speech_prob": 0.001847803476266563}, {"id": 466, "seek": 271416, "start": 2714.24, "end": 2720.0, "text": " a specific place. Imagine if we did not ground the agent with all this stuff at the top,", "tokens": [50368, 257, 2685, 1081, 13, 11739, 498, 321, 630, 406, 2727, 264, 9461, 365, 439, 341, 1507, 412, 264, 1192, 11, 50656], "temperature": 0.0, "avg_logprob": -0.14899217356806216, "compression_ratio": 1.7426470588235294, "no_speech_prob": 0.004766763653606176}, {"id": 467, "seek": 271416, "start": 2720.0, "end": 2725.68, "text": " you just ask, where should Eddie Lin go to? Then Eddie Lin might, the LL might reply,", "tokens": [50656, 291, 445, 1029, 11, 689, 820, 23911, 9355, 352, 281, 30, 1396, 23911, 9355, 1062, 11, 264, 441, 43, 1062, 16972, 11, 50940], "temperature": 0.0, "avg_logprob": -0.14899217356806216, "compression_ratio": 1.7426470588235294, "no_speech_prob": 0.004766763653606176}, {"id": 468, "seek": 271416, "start": 2725.68, "end": 2729.68, "text": " Eddie Lin should go to the supermarket or something like something that is irrelevant to the game world.", "tokens": [50940, 23911, 9355, 820, 352, 281, 264, 25180, 420, 746, 411, 746, 300, 307, 28682, 281, 264, 1216, 1002, 13, 51140], "temperature": 0.0, "avg_logprob": -0.14899217356806216, "compression_ratio": 1.7426470588235294, "no_speech_prob": 0.004766763653606176}, {"id": 469, "seek": 271416, "start": 2730.3199999999997, "end": 2736.3999999999996, "text": " Right, so because we grounded it with some idea of what kind of possible areas that the agent", "tokens": [51172, 1779, 11, 370, 570, 321, 23535, 309, 365, 512, 1558, 295, 437, 733, 295, 1944, 3179, 300, 264, 9461, 51476], "temperature": 0.0, "avg_logprob": -0.14899217356806216, "compression_ratio": 1.7426470588235294, "no_speech_prob": 0.004766763653606176}, {"id": 470, "seek": 271416, "start": 2736.3999999999996, "end": 2744.0, "text": " should go, the agent is able to choose one area from the above list. And how is this list generated?", "tokens": [51476, 820, 352, 11, 264, 9461, 307, 1075, 281, 2826, 472, 1859, 490, 264, 3673, 1329, 13, 400, 577, 307, 341, 1329, 10833, 30, 51856], "temperature": 0.0, "avg_logprob": -0.14899217356806216, "compression_ratio": 1.7426470588235294, "no_speech_prob": 0.004766763653606176}, {"id": 471, "seek": 274400, "start": 2744.0, "end": 2749.12, "text": " It's generated from passing some form of knowledge graph. And this is what I mean by using knowledge", "tokens": [50364, 467, 311, 10833, 490, 8437, 512, 1254, 295, 3601, 4295, 13, 400, 341, 307, 437, 286, 914, 538, 1228, 3601, 50620], "temperature": 0.0, "avg_logprob": -0.135361572265625, "compression_ratio": 1.8104089219330854, "no_speech_prob": 0.0013158436631783843}, {"id": 472, "seek": 274400, "start": 2749.12, "end": 2754.08, "text": " graph as text to ground the large language model. So you can use this recursively, you can say that,", "tokens": [50620, 4295, 382, 2487, 281, 2727, 264, 2416, 2856, 2316, 13, 407, 291, 393, 764, 341, 20560, 3413, 11, 291, 393, 584, 300, 11, 50868], "temperature": 0.0, "avg_logprob": -0.135361572265625, "compression_ratio": 1.8104089219330854, "no_speech_prob": 0.0013158436631783843}, {"id": 473, "seek": 274400, "start": 2754.08, "end": 2759.92, "text": " oh, currently you are in the maybe common room. Okay, what, where in the common room would you", "tokens": [50868, 1954, 11, 4362, 291, 366, 294, 264, 1310, 2689, 1808, 13, 1033, 11, 437, 11, 689, 294, 264, 2689, 1808, 576, 291, 51160], "temperature": 0.0, "avg_logprob": -0.135361572265625, "compression_ratio": 1.8104089219330854, "no_speech_prob": 0.0013158436631783843}, {"id": 474, "seek": 274400, "start": 2759.92, "end": 2764.96, "text": " like to go? We like to go to the sofa, to the mirror, and you can do this recursively. Okay,", "tokens": [51160, 411, 281, 352, 30, 492, 411, 281, 352, 281, 264, 28668, 11, 281, 264, 8013, 11, 293, 291, 393, 360, 341, 20560, 3413, 13, 1033, 11, 51412], "temperature": 0.0, "avg_logprob": -0.135361572265625, "compression_ratio": 1.8104089219330854, "no_speech_prob": 0.0013158436631783843}, {"id": 475, "seek": 274400, "start": 2764.96, "end": 2771.84, "text": " and then you can get a very, very specific area that the agent is going. Okay. Any clarifications", "tokens": [51412, 293, 550, 291, 393, 483, 257, 588, 11, 588, 2685, 1859, 300, 264, 9461, 307, 516, 13, 1033, 13, 2639, 6093, 7833, 51756], "temperature": 0.0, "avg_logprob": -0.135361572265625, "compression_ratio": 1.8104089219330854, "no_speech_prob": 0.0013158436631783843}, {"id": 476, "seek": 277184, "start": 2771.84, "end": 2778.56, "text": " on this so far before I move on? Yeah, if you haven't read this paper, go read it. Okay, this", "tokens": [50364, 322, 341, 370, 1400, 949, 286, 1286, 322, 30, 865, 11, 498, 291, 2378, 380, 1401, 341, 3035, 11, 352, 1401, 309, 13, 1033, 11, 341, 50700], "temperature": 0.0, "avg_logprob": -0.15962078501877275, "compression_ratio": 1.5822784810126582, "no_speech_prob": 0.0023157158866524696}, {"id": 477, "seek": 277184, "start": 2778.56, "end": 2787.76, "text": " paper is good. It's one of the better ones. All right. So we have the Chinese LLM, it's called", "tokens": [50700, 3035, 307, 665, 13, 467, 311, 472, 295, 264, 1101, 2306, 13, 1057, 558, 13, 407, 321, 362, 264, 4649, 441, 43, 44, 11, 309, 311, 1219, 51160], "temperature": 0.0, "avg_logprob": -0.15962078501877275, "compression_ratio": 1.5822784810126582, "no_speech_prob": 0.0023157158866524696}, {"id": 478, "seek": 277184, "start": 2787.76, "end": 2794.48, "text": " Ernie. All right, and this, what they do is they actually use two hybrid ways of generating", "tokens": [51160, 3300, 2766, 13, 1057, 558, 11, 293, 341, 11, 437, 436, 360, 307, 436, 767, 764, 732, 13051, 2098, 295, 17746, 51496], "temperature": 0.0, "avg_logprob": -0.15962078501877275, "compression_ratio": 1.5822784810126582, "no_speech_prob": 0.0023157158866524696}, {"id": 479, "seek": 277184, "start": 2795.76, "end": 2801.1200000000003, "text": " the output. So they say that large language models lack grounding, lack consistency. So we use", "tokens": [51560, 264, 5598, 13, 407, 436, 584, 300, 2416, 2856, 5245, 5011, 46727, 11, 5011, 14416, 13, 407, 321, 764, 51828], "temperature": 0.0, "avg_logprob": -0.15962078501877275, "compression_ratio": 1.5822784810126582, "no_speech_prob": 0.0023157158866524696}, {"id": 480, "seek": 280112, "start": 2801.12, "end": 2805.7599999999998, "text": " a knowledge graph. Okay, I granted that. But then I look at that structure and they're like,", "tokens": [50364, 257, 3601, 4295, 13, 1033, 11, 286, 12344, 300, 13, 583, 550, 286, 574, 412, 300, 3877, 293, 436, 434, 411, 11, 50596], "temperature": 0.0, "avg_logprob": -0.1330030690068784, "compression_ratio": 1.9458483754512634, "no_speech_prob": 0.002179131144657731}, {"id": 481, "seek": 280112, "start": 2805.7599999999998, "end": 2811.44, "text": " oh man, what is this? So they actually have a large language model. This is the typical", "tokens": [50596, 1954, 587, 11, 437, 307, 341, 30, 407, 436, 767, 362, 257, 2416, 2856, 2316, 13, 639, 307, 264, 7476, 50880], "temperature": 0.0, "avg_logprob": -0.1330030690068784, "compression_ratio": 1.9458483754512634, "no_speech_prob": 0.002179131144657731}, {"id": 482, "seek": 280112, "start": 2811.44, "end": 2814.64, "text": " transformer architecture. So this is a typical transformer on the left side.", "tokens": [50880, 31782, 9482, 13, 407, 341, 307, 257, 7476, 31782, 322, 264, 1411, 1252, 13, 51040], "temperature": 0.0, "avg_logprob": -0.1330030690068784, "compression_ratio": 1.9458483754512634, "no_speech_prob": 0.002179131144657731}, {"id": 483, "seek": 280112, "start": 2817.12, "end": 2822.24, "text": " So they have two encoders. Okay, one is the T encoder, which is like the text encoder. And why", "tokens": [51164, 407, 436, 362, 732, 2058, 378, 433, 13, 1033, 11, 472, 307, 264, 314, 2058, 19866, 11, 597, 307, 411, 264, 2487, 2058, 19866, 13, 400, 983, 51420], "temperature": 0.0, "avg_logprob": -0.1330030690068784, "compression_ratio": 1.9458483754512634, "no_speech_prob": 0.002179131144657731}, {"id": 484, "seek": 280112, "start": 2822.24, "end": 2826.96, "text": " is the knowledge graph encoder? So the knowledge graph encoder uses this thing called trans embeddings.", "tokens": [51420, 307, 264, 3601, 4295, 2058, 19866, 30, 407, 264, 3601, 4295, 2058, 19866, 4960, 341, 551, 1219, 1145, 12240, 29432, 13, 51656], "temperature": 0.0, "avg_logprob": -0.1330030690068784, "compression_ratio": 1.9458483754512634, "no_speech_prob": 0.002179131144657731}, {"id": 485, "seek": 280112, "start": 2826.96, "end": 2830.4, "text": " Okay, I'm not going to go through that, but they train that embeddings using like,", "tokens": [51656, 1033, 11, 286, 478, 406, 516, 281, 352, 807, 300, 11, 457, 436, 3847, 300, 12240, 29432, 1228, 411, 11, 51828], "temperature": 0.0, "avg_logprob": -0.1330030690068784, "compression_ratio": 1.9458483754512634, "no_speech_prob": 0.002179131144657731}, {"id": 486, "seek": 283040, "start": 2830.4, "end": 2833.04, "text": " they take one vector and take another vector, then they connect the,", "tokens": [50364, 436, 747, 472, 8062, 293, 747, 1071, 8062, 11, 550, 436, 1745, 264, 11, 50496], "temperature": 0.0, "avg_logprob": -0.15082931518554688, "compression_ratio": 2.0756972111553784, "no_speech_prob": 0.0011145679745823145}, {"id": 487, "seek": 283040, "start": 2834.1600000000003, "end": 2838.64, "text": " just draw the diagram here for you to see. So you have one vector A and another vector B,", "tokens": [50552, 445, 2642, 264, 10686, 510, 337, 291, 281, 536, 13, 407, 291, 362, 472, 8062, 316, 293, 1071, 8062, 363, 11, 50776], "temperature": 0.0, "avg_logprob": -0.15082931518554688, "compression_ratio": 2.0756972111553784, "no_speech_prob": 0.0011145679745823145}, {"id": 488, "seek": 283040, "start": 2839.36, "end": 2844.8, "text": " and then you create another vector C here. So you can keep like using this vector A,", "tokens": [50812, 293, 550, 291, 1884, 1071, 8062, 383, 510, 13, 407, 291, 393, 1066, 411, 1228, 341, 8062, 316, 11, 51084], "temperature": 0.0, "avg_logprob": -0.15082931518554688, "compression_ratio": 2.0756972111553784, "no_speech_prob": 0.0011145679745823145}, {"id": 489, "seek": 283040, "start": 2844.8, "end": 2848.64, "text": " you take another vector, extend from it, and then you can train on this relation C. So, you know,", "tokens": [51084, 291, 747, 1071, 8062, 11, 10101, 490, 309, 11, 293, 550, 291, 393, 3847, 322, 341, 9721, 383, 13, 407, 11, 291, 458, 11, 51276], "temperature": 0.0, "avg_logprob": -0.15082931518554688, "compression_ratio": 2.0756972111553784, "no_speech_prob": 0.0011145679745823145}, {"id": 490, "seek": 283040, "start": 2849.36, "end": 2853.92, "text": " this is how the trans embeddings are trained, trans E. Okay, and they use this kind of embedding", "tokens": [51312, 341, 307, 577, 264, 1145, 12240, 29432, 366, 8895, 11, 1145, 462, 13, 1033, 11, 293, 436, 764, 341, 733, 295, 12240, 3584, 51540], "temperature": 0.0, "avg_logprob": -0.15082931518554688, "compression_ratio": 2.0756972111553784, "no_speech_prob": 0.0011145679745823145}, {"id": 491, "seek": 283040, "start": 2853.92, "end": 2857.84, "text": " space. Okay, you can do self-attention, and then you can do cross-attention across", "tokens": [51540, 1901, 13, 1033, 11, 291, 393, 360, 2698, 12, 1591, 1251, 11, 293, 550, 291, 393, 360, 3278, 12, 1591, 1251, 2108, 51736], "temperature": 0.0, "avg_logprob": -0.15082931518554688, "compression_ratio": 2.0756972111553784, "no_speech_prob": 0.0011145679745823145}, {"id": 492, "seek": 285784, "start": 2858.6400000000003, "end": 2862.96, "text": " both the trans, the text embeddings as well as the knowledge graph embeddings.", "tokens": [50404, 1293, 264, 1145, 11, 264, 2487, 12240, 29432, 382, 731, 382, 264, 3601, 4295, 12240, 29432, 13, 50620], "temperature": 0.0, "avg_logprob": -0.10691229778787364, "compression_ratio": 2.02212389380531, "no_speech_prob": 0.0010052878642454743}, {"id": 493, "seek": 285784, "start": 2862.96, "end": 2866.8, "text": " And then hopefully you get some output, right? And then you get some text outputs here,", "tokens": [50620, 400, 550, 4696, 291, 483, 512, 5598, 11, 558, 30, 400, 550, 291, 483, 512, 2487, 23930, 510, 11, 50812], "temperature": 0.0, "avg_logprob": -0.10691229778787364, "compression_ratio": 2.02212389380531, "no_speech_prob": 0.0010052878642454743}, {"id": 494, "seek": 285784, "start": 2866.8, "end": 2871.52, "text": " and then you can update your knowledge graph to the knowledge graph outputs here. Okay, so this", "tokens": [50812, 293, 550, 291, 393, 5623, 428, 3601, 4295, 281, 264, 3601, 4295, 23930, 510, 13, 1033, 11, 370, 341, 51048], "temperature": 0.0, "avg_logprob": -0.10691229778787364, "compression_ratio": 2.02212389380531, "no_speech_prob": 0.0010052878642454743}, {"id": 495, "seek": 285784, "start": 2871.52, "end": 2877.44, "text": " is a way to embody a knowledge graph as some embedding space. Okay, and then we can use like", "tokens": [51048, 307, 257, 636, 281, 42575, 257, 3601, 4295, 382, 512, 12240, 3584, 1901, 13, 1033, 11, 293, 550, 321, 393, 764, 411, 51344], "temperature": 0.0, "avg_logprob": -0.10691229778787364, "compression_ratio": 2.02212389380531, "no_speech_prob": 0.0010052878642454743}, {"id": 496, "seek": 285784, "start": 2877.44, "end": 2883.84, "text": " the attention to like, attend to like the text-based stuff as well. So, yeah, this is just one way of", "tokens": [51344, 264, 3202, 281, 411, 11, 6888, 281, 411, 264, 2487, 12, 6032, 1507, 382, 731, 13, 407, 11, 1338, 11, 341, 307, 445, 472, 636, 295, 51664], "temperature": 0.0, "avg_logprob": -0.10691229778787364, "compression_ratio": 2.02212389380531, "no_speech_prob": 0.0010052878642454743}, {"id": 497, "seek": 288384, "start": 2883.84, "end": 2889.28, "text": " doing processing using a knowledge graph. As you can see, I don't really like it. I mean,", "tokens": [50364, 884, 9007, 1228, 257, 3601, 4295, 13, 1018, 291, 393, 536, 11, 286, 500, 380, 534, 411, 309, 13, 286, 914, 11, 50636], "temperature": 0.0, "avg_logprob": -0.10575730460030693, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.005334576591849327}, {"id": 498, "seek": 288384, "start": 2889.28, "end": 2893.6800000000003, "text": " I think that it's too convoluted. Like, okay, so this is another discussion question that I'd like", "tokens": [50636, 286, 519, 300, 309, 311, 886, 3754, 2308, 292, 13, 1743, 11, 1392, 11, 370, 341, 307, 1071, 5017, 1168, 300, 286, 1116, 411, 50856], "temperature": 0.0, "avg_logprob": -0.10575730460030693, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.005334576591849327}, {"id": 499, "seek": 288384, "start": 2893.6800000000003, "end": 2898.48, "text": " you all to think about. Should we have separate embedding space for this large language model", "tokens": [50856, 291, 439, 281, 519, 466, 13, 6454, 321, 362, 4994, 12240, 3584, 1901, 337, 341, 2416, 2856, 2316, 51096], "temperature": 0.0, "avg_logprob": -0.10575730460030693, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.005334576591849327}, {"id": 500, "seek": 288384, "start": 2898.48, "end": 2903.44, "text": " stuff and for the knowledge graph embeddings? Like, should we use two different embeddings?", "tokens": [51096, 1507, 293, 337, 264, 3601, 4295, 12240, 29432, 30, 1743, 11, 820, 321, 764, 732, 819, 12240, 29432, 30, 51344], "temperature": 0.0, "avg_logprob": -0.10575730460030693, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.005334576591849327}, {"id": 501, "seek": 288384, "start": 2903.44, "end": 2906.88, "text": " Okay, should we use two different embeddings?", "tokens": [51344, 1033, 11, 820, 321, 764, 732, 819, 12240, 29432, 30, 51516], "temperature": 0.0, "avg_logprob": -0.10575730460030693, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.005334576591849327}, {"id": 502, "seek": 290688, "start": 2906.96, "end": 2914.6400000000003, "text": " Yeah, or should we use the same one? Yeah. Okay, then also more generally, like if you want to have", "tokens": [50368, 865, 11, 420, 820, 321, 764, 264, 912, 472, 30, 865, 13, 1033, 11, 550, 611, 544, 5101, 11, 411, 498, 291, 528, 281, 362, 50752], "temperature": 0.0, "avg_logprob": -0.17207688670004567, "compression_ratio": 1.9871794871794872, "no_speech_prob": 0.0022483072243630886}, {"id": 503, "seek": 290688, "start": 2914.6400000000003, "end": 2921.2000000000003, "text": " multi-modal embeddings, like you have text, image embeddings, you have audio embeddings,", "tokens": [50752, 4825, 12, 8014, 304, 12240, 29432, 11, 411, 291, 362, 2487, 11, 3256, 12240, 29432, 11, 291, 362, 6278, 12240, 29432, 11, 51080], "temperature": 0.0, "avg_logprob": -0.17207688670004567, "compression_ratio": 1.9871794871794872, "no_speech_prob": 0.0022483072243630886}, {"id": 504, "seek": 290688, "start": 2921.2000000000003, "end": 2925.6800000000003, "text": " okay, if you want to do a multi-modal large language models, you can actually also put them", "tokens": [51080, 1392, 11, 498, 291, 528, 281, 360, 257, 4825, 12, 8014, 304, 2416, 2856, 5245, 11, 291, 393, 767, 611, 829, 552, 51304], "temperature": 0.0, "avg_logprob": -0.17207688670004567, "compression_ratio": 1.9871794871794872, "no_speech_prob": 0.0022483072243630886}, {"id": 505, "seek": 290688, "start": 2925.6800000000003, "end": 2931.92, "text": " into distribution model at the end here. Okay, but the question is, in fact, knowledge graph can", "tokens": [51304, 666, 7316, 2316, 412, 264, 917, 510, 13, 1033, 11, 457, 264, 1168, 307, 11, 294, 1186, 11, 3601, 4295, 393, 51616], "temperature": 0.0, "avg_logprob": -0.17207688670004567, "compression_ratio": 1.9871794871794872, "no_speech_prob": 0.0022483072243630886}, {"id": 506, "seek": 290688, "start": 2931.92, "end": 2936.8, "text": " be multi-modal also, you can actually also put it here. The question is, all this image", "tokens": [51616, 312, 4825, 12, 8014, 304, 611, 11, 291, 393, 767, 611, 829, 309, 510, 13, 440, 1168, 307, 11, 439, 341, 3256, 51860], "temperature": 0.0, "avg_logprob": -0.17207688670004567, "compression_ratio": 1.9871794871794872, "no_speech_prob": 0.0022483072243630886}, {"id": 507, "seek": 293680, "start": 2936.8, "end": 2941.1200000000003, "text": " and audio embeddings, okay, you can put it in the large language model, you can also put in the", "tokens": [50364, 293, 6278, 12240, 29432, 11, 1392, 11, 291, 393, 829, 309, 294, 264, 2416, 2856, 2316, 11, 291, 393, 611, 829, 294, 264, 50580], "temperature": 0.0, "avg_logprob": -0.12861607504672692, "compression_ratio": 1.8143939393939394, "no_speech_prob": 0.0005937275127507746}, {"id": 508, "seek": 293680, "start": 2941.1200000000003, "end": 2947.76, "text": " knowledge graph. But why not just use a single embedding, right? Why do you need to? You are", "tokens": [50580, 3601, 4295, 13, 583, 983, 406, 445, 764, 257, 2167, 12240, 3584, 11, 558, 30, 1545, 360, 291, 643, 281, 30, 509, 366, 50912], "temperature": 0.0, "avg_logprob": -0.12861607504672692, "compression_ratio": 1.8143939393939394, "no_speech_prob": 0.0005937275127507746}, {"id": 509, "seek": 293680, "start": 2947.76, "end": 2952.6400000000003, "text": " using text-based knowledge graph and text-based large language models, that there's no external", "tokens": [50912, 1228, 2487, 12, 6032, 3601, 4295, 293, 2487, 12, 6032, 2416, 2856, 5245, 11, 300, 456, 311, 572, 8320, 51156], "temperature": 0.0, "avg_logprob": -0.12861607504672692, "compression_ratio": 1.8143939393939394, "no_speech_prob": 0.0005937275127507746}, {"id": 510, "seek": 293680, "start": 2952.6400000000003, "end": 2957.28, "text": " domain here. It's all the same domain. Why do we need two different embeddings for the knowledge", "tokens": [51156, 9274, 510, 13, 467, 311, 439, 264, 912, 9274, 13, 1545, 360, 321, 643, 732, 819, 12240, 29432, 337, 264, 3601, 51388], "temperature": 0.0, "avg_logprob": -0.12861607504672692, "compression_ratio": 1.8143939393939394, "no_speech_prob": 0.0005937275127507746}, {"id": 511, "seek": 293680, "start": 2957.28, "end": 2963.2000000000003, "text": " graph and the input text? That's my question. If you have any ideas, let me know. But think about", "tokens": [51388, 4295, 293, 264, 4846, 2487, 30, 663, 311, 452, 1168, 13, 759, 291, 362, 604, 3487, 11, 718, 385, 458, 13, 583, 519, 466, 51684], "temperature": 0.0, "avg_logprob": -0.12861607504672692, "compression_ratio": 1.8143939393939394, "no_speech_prob": 0.0005937275127507746}, {"id": 512, "seek": 296320, "start": 2963.2, "end": 2971.12, "text": " this, all right. Okay, then we have this question-answer graph neural network, and this does a", "tokens": [50364, 341, 11, 439, 558, 13, 1033, 11, 550, 321, 362, 341, 1168, 12, 43904, 4295, 18161, 3209, 11, 293, 341, 775, 257, 50760], "temperature": 0.0, "avg_logprob": -0.12539354960123697, "compression_ratio": 1.9306122448979592, "no_speech_prob": 0.0015008313348516822}, {"id": 513, "seek": 296320, "start": 2971.12, "end": 2975.9199999999996, "text": " two-way interaction between the language model and knowledge graph. And what we can see here is", "tokens": [50760, 732, 12, 676, 9285, 1296, 264, 2856, 2316, 293, 3601, 4295, 13, 400, 437, 321, 393, 536, 510, 307, 51000], "temperature": 0.0, "avg_logprob": -0.12539354960123697, "compression_ratio": 1.9306122448979592, "no_speech_prob": 0.0015008313348516822}, {"id": 514, "seek": 296320, "start": 2975.9199999999996, "end": 2980.3199999999997, "text": " that we have a certain question, okay, and some options that to choose from. The large language", "tokens": [51000, 300, 321, 362, 257, 1629, 1168, 11, 1392, 11, 293, 512, 3956, 300, 281, 2826, 490, 13, 440, 2416, 2856, 51220], "temperature": 0.0, "avg_logprob": -0.12539354960123697, "compression_ratio": 1.9306122448979592, "no_speech_prob": 0.0015008313348516822}, {"id": 515, "seek": 296320, "start": 2980.3199999999997, "end": 2985.2, "text": " model will go in here, and then they express the question and the options as part of a knowledge", "tokens": [51220, 2316, 486, 352, 294, 510, 11, 293, 550, 436, 5109, 264, 1168, 293, 264, 3956, 382, 644, 295, 257, 3601, 51464], "temperature": 0.0, "avg_logprob": -0.12539354960123697, "compression_ratio": 1.9306122448979592, "no_speech_prob": 0.0015008313348516822}, {"id": 516, "seek": 296320, "start": 2985.2, "end": 2990.3999999999996, "text": " graph, okay, and this will go through another knowledge graph encoder, and this knowledge", "tokens": [51464, 4295, 11, 1392, 11, 293, 341, 486, 352, 807, 1071, 3601, 4295, 2058, 19866, 11, 293, 341, 3601, 51724], "temperature": 0.0, "avg_logprob": -0.12539354960123697, "compression_ratio": 1.9306122448979592, "no_speech_prob": 0.0015008313348516822}, {"id": 517, "seek": 299040, "start": 2990.48, "end": 2998.2400000000002, "text": " graph encoder is a graph neural network. Okay, and this basically will, you look at this diagram here,", "tokens": [50368, 4295, 2058, 19866, 307, 257, 4295, 18161, 3209, 13, 1033, 11, 293, 341, 1936, 486, 11, 291, 574, 412, 341, 10686, 510, 11, 50756], "temperature": 0.0, "avg_logprob": -0.13822120666503906, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.0037419432774186134}, {"id": 518, "seek": 299040, "start": 2999.76, "end": 3005.2000000000003, "text": " it will do cross-attention. It's very, very confusing. Look at this language model conditions", "tokens": [50832, 309, 486, 360, 3278, 12, 1591, 1251, 13, 467, 311, 588, 11, 588, 13181, 13, 2053, 412, 341, 2856, 2316, 4487, 51104], "temperature": 0.0, "avg_logprob": -0.13822120666503906, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.0037419432774186134}, {"id": 519, "seek": 299040, "start": 3005.2000000000003, "end": 3009.2000000000003, "text": " knowledge graph. So you can blank out some notes here, you know, you can actually do some attention", "tokens": [51104, 3601, 4295, 13, 407, 291, 393, 8247, 484, 512, 5570, 510, 11, 291, 458, 11, 291, 393, 767, 360, 512, 3202, 51304], "temperature": 0.0, "avg_logprob": -0.13822120666503906, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.0037419432774186134}, {"id": 520, "seek": 299040, "start": 3009.2000000000003, "end": 3014.8, "text": " on some notes to like block off the path. Yeah, so yeah, that's possible. So you can also use the", "tokens": [51304, 322, 512, 5570, 281, 411, 3461, 766, 264, 3100, 13, 865, 11, 370, 1338, 11, 300, 311, 1944, 13, 407, 291, 393, 611, 764, 264, 51584], "temperature": 0.0, "avg_logprob": -0.13822120666503906, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.0037419432774186134}, {"id": 521, "seek": 301480, "start": 3014.8, "end": 3021.36, "text": " knowledge graph to condition the attention in the, in, in, when, when you do the next open", "tokens": [50364, 3601, 4295, 281, 4188, 264, 3202, 294, 264, 11, 294, 11, 294, 11, 562, 11, 562, 291, 360, 264, 958, 1269, 50692], "temperature": 0.0, "avg_logprob": -0.1387719537457849, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.002478700829669833}, {"id": 522, "seek": 301480, "start": 3021.36, "end": 3027.52, "text": " processing for the language model, and then eventually you get your answer. Yeah, so this is", "tokens": [50692, 9007, 337, 264, 2856, 2316, 11, 293, 550, 4728, 291, 483, 428, 1867, 13, 865, 11, 370, 341, 307, 51000], "temperature": 0.0, "avg_logprob": -0.1387719537457849, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.002478700829669833}, {"id": 523, "seek": 301480, "start": 3027.52, "end": 3032.1600000000003, "text": " one way we can process the knowledge graph, you can process it using a graph neural network.", "tokens": [51000, 472, 636, 321, 393, 1399, 264, 3601, 4295, 11, 291, 393, 1399, 309, 1228, 257, 4295, 18161, 3209, 13, 51232], "temperature": 0.0, "avg_logprob": -0.1387719537457849, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.002478700829669833}, {"id": 524, "seek": 301480, "start": 3032.96, "end": 3036.96, "text": " And in fact, the earlier one on Ernie, that is similar to graph neural network as well. I mean,", "tokens": [51272, 400, 294, 1186, 11, 264, 3071, 472, 322, 3300, 2766, 11, 300, 307, 2531, 281, 4295, 18161, 3209, 382, 731, 13, 286, 914, 11, 51472], "temperature": 0.0, "avg_logprob": -0.1387719537457849, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.002478700829669833}, {"id": 525, "seek": 301480, "start": 3036.96, "end": 3041.76, "text": " they are using the embedding space. And you know, if you just do some operations on the embedding", "tokens": [51472, 436, 366, 1228, 264, 12240, 3584, 1901, 13, 400, 291, 458, 11, 498, 291, 445, 360, 512, 7705, 322, 264, 12240, 3584, 51712], "temperature": 0.0, "avg_logprob": -0.1387719537457849, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.002478700829669833}, {"id": 526, "seek": 304176, "start": 3041.76, "end": 3045.84, "text": " space, that is a graph neural network already. So yeah, this is very, very similar to graph neural", "tokens": [50364, 1901, 11, 300, 307, 257, 4295, 18161, 3209, 1217, 13, 407, 1338, 11, 341, 307, 588, 11, 588, 2531, 281, 4295, 18161, 50568], "temperature": 0.0, "avg_logprob": -0.10612256773586931, "compression_ratio": 1.927710843373494, "no_speech_prob": 0.0034142613876610994}, {"id": 527, "seek": 304176, "start": 3045.84, "end": 3053.2000000000003, "text": " networks. And yeah, it shows that back in the first few years, people use these kind of methods to", "tokens": [50568, 9590, 13, 400, 1338, 11, 309, 3110, 300, 646, 294, 264, 700, 1326, 924, 11, 561, 764, 613, 733, 295, 7150, 281, 50936], "temperature": 0.0, "avg_logprob": -0.10612256773586931, "compression_ratio": 1.927710843373494, "no_speech_prob": 0.0034142613876610994}, {"id": 528, "seek": 304176, "start": 3053.2000000000003, "end": 3058.96, "text": " pass through knowledge graph using graph neural networks to represent the embeddings. Okay,", "tokens": [50936, 1320, 807, 3601, 4295, 1228, 4295, 18161, 9590, 281, 2906, 264, 12240, 29432, 13, 1033, 11, 51224], "temperature": 0.0, "avg_logprob": -0.10612256773586931, "compression_ratio": 1.927710843373494, "no_speech_prob": 0.0034142613876610994}, {"id": 529, "seek": 304176, "start": 3059.92, "end": 3065.0400000000004, "text": " I don't see why you need to do this. Okay, personally, I don't see why you need to do this.", "tokens": [51272, 286, 500, 380, 536, 983, 291, 643, 281, 360, 341, 13, 1033, 11, 5665, 11, 286, 500, 380, 536, 983, 291, 643, 281, 360, 341, 13, 51528], "temperature": 0.0, "avg_logprob": -0.10612256773586931, "compression_ratio": 1.927710843373494, "no_speech_prob": 0.0034142613876610994}, {"id": 530, "seek": 304176, "start": 3065.0400000000004, "end": 3071.5200000000004, "text": " You can just use text, because the knowledge graph representation is in the same kind of domain as", "tokens": [51528, 509, 393, 445, 764, 2487, 11, 570, 264, 3601, 4295, 10290, 307, 294, 264, 912, 733, 295, 9274, 382, 51852], "temperature": 0.0, "avg_logprob": -0.10612256773586931, "compression_ratio": 1.927710843373494, "no_speech_prob": 0.0034142613876610994}, {"id": 531, "seek": 307176, "start": 3072.0, "end": 3077.0400000000004, "text": " your last language model representation. They're all text. Yeah, why do we need a separate embedding?", "tokens": [50376, 428, 1036, 2856, 2316, 10290, 13, 814, 434, 439, 2487, 13, 865, 11, 983, 360, 321, 643, 257, 4994, 12240, 3584, 30, 50628], "temperature": 0.0, "avg_logprob": -0.15713078042735223, "compression_ratio": 1.4455445544554455, "no_speech_prob": 0.0008325618109665811}, {"id": 532, "seek": 307176, "start": 3077.6000000000004, "end": 3086.0, "text": " So yeah. Okay, Richard said, I think there will have to be input output embeddings and train", "tokens": [50656, 407, 1338, 13, 1033, 11, 9809, 848, 11, 286, 519, 456, 486, 362, 281, 312, 4846, 5598, 12240, 29432, 293, 3847, 51076], "temperature": 0.0, "avg_logprob": -0.15713078042735223, "compression_ratio": 1.4455445544554455, "no_speech_prob": 0.0008325618109665811}, {"id": 533, "seek": 307176, "start": 3086.0, "end": 3093.92, "text": " them to address common pattern or memory structures. Sorry, could you explain this comment was in", "tokens": [51076, 552, 281, 2985, 2689, 5102, 420, 4675, 9227, 13, 4919, 11, 727, 291, 2903, 341, 2871, 390, 294, 51472], "temperature": 0.0, "avg_logprob": -0.15713078042735223, "compression_ratio": 1.4455445544554455, "no_speech_prob": 0.0008325618109665811}, {"id": 534, "seek": 309392, "start": 3094.0, "end": 3101.6, "text": " relation to which part of what I said? So you were saying, you know, it's a decision sort of or", "tokens": [50368, 9721, 281, 597, 644, 295, 437, 286, 848, 30, 407, 291, 645, 1566, 11, 291, 458, 11, 309, 311, 257, 3537, 1333, 295, 420, 50748], "temperature": 0.0, "avg_logprob": -0.30077407065402256, "compression_ratio": 1.6238095238095238, "no_speech_prob": 0.028342237696051598}, {"id": 535, "seek": 309392, "start": 3102.48, "end": 3108.56, "text": " sort of my decision question, right, as well, that there is, how do you", "tokens": [50792, 1333, 295, 452, 3537, 1168, 11, 558, 11, 382, 731, 11, 300, 456, 307, 11, 577, 360, 291, 51096], "temperature": 0.0, "avg_logprob": -0.30077407065402256, "compression_ratio": 1.6238095238095238, "no_speech_prob": 0.028342237696051598}, {"id": 536, "seek": 309392, "start": 3110.48, "end": 3114.64, "text": " well approach this problem? Why do they have their embeddings separate, right?", "tokens": [51192, 731, 3109, 341, 1154, 30, 1545, 360, 436, 362, 641, 12240, 29432, 4994, 11, 558, 30, 51400], "temperature": 0.0, "avg_logprob": -0.30077407065402256, "compression_ratio": 1.6238095238095238, "no_speech_prob": 0.028342237696051598}, {"id": 537, "seek": 309392, "start": 3116.64, "end": 3121.04, "text": " At the end, there's a sort of cross attention where they're merging them for an output of this", "tokens": [51500, 1711, 264, 917, 11, 456, 311, 257, 1333, 295, 3278, 3202, 689, 436, 434, 44559, 552, 337, 364, 5598, 295, 341, 51720], "temperature": 0.0, "avg_logprob": -0.30077407065402256, "compression_ratio": 1.6238095238095238, "no_speech_prob": 0.028342237696051598}, {"id": 538, "seek": 312104, "start": 3121.04, "end": 3132.24, "text": " type or that type would have you. But then this idea comes that the real, so I think of a large", "tokens": [50364, 2010, 420, 300, 2010, 576, 362, 291, 13, 583, 550, 341, 1558, 1487, 300, 264, 957, 11, 370, 286, 519, 295, 257, 2416, 50924], "temperature": 0.0, "avg_logprob": -0.1601028332765075, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.00214701471850276}, {"id": 539, "seek": 312104, "start": 3132.24, "end": 3136.8, "text": " language model where the reason why they have these emergent behaviors is because language is", "tokens": [50924, 2856, 2316, 689, 264, 1778, 983, 436, 362, 613, 4345, 6930, 15501, 307, 570, 2856, 307, 51152], "temperature": 0.0, "avg_logprob": -0.1601028332765075, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.00214701471850276}, {"id": 540, "seek": 312104, "start": 3136.8, "end": 3144.56, "text": " currently our best mechanism to embody thoughts, ideas, and our most direct implementation of", "tokens": [51152, 4362, 527, 1151, 7513, 281, 42575, 4598, 11, 3487, 11, 293, 527, 881, 2047, 11420, 295, 51540], "temperature": 0.0, "avg_logprob": -0.1601028332765075, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.00214701471850276}, {"id": 541, "seek": 312104, "start": 3144.56, "end": 3150.8, "text": " ideas. Now, particularly once they're broken down, tokenized and so on, you've taken through that", "tokens": [51540, 3487, 13, 823, 11, 4098, 1564, 436, 434, 5463, 760, 11, 14862, 1602, 293, 370, 322, 11, 291, 600, 2726, 807, 300, 51852], "temperature": 0.0, "avg_logprob": -0.1601028332765075, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.00214701471850276}, {"id": 542, "seek": 315080, "start": 3151.04, "end": 3156.8, "text": " process a few times, consider retention and context, you come up with new ideas. Yes, yes.", "tokens": [50376, 1399, 257, 1326, 1413, 11, 1949, 22871, 293, 4319, 11, 291, 808, 493, 365, 777, 3487, 13, 1079, 11, 2086, 13, 50664], "temperature": 0.0, "avg_logprob": -0.14864517450332643, "compression_ratio": 1.6777251184834123, "no_speech_prob": 0.0009991293773055077}, {"id": 543, "seek": 315080, "start": 3158.8, "end": 3165.6000000000004, "text": " And then as you're just saying, there's no particular need for different embedding spaces.", "tokens": [50764, 400, 550, 382, 291, 434, 445, 1566, 11, 456, 311, 572, 1729, 643, 337, 819, 12240, 3584, 7673, 13, 51104], "temperature": 0.0, "avg_logprob": -0.14864517450332643, "compression_ratio": 1.6777251184834123, "no_speech_prob": 0.0009991293773055077}, {"id": 544, "seek": 315080, "start": 3166.7200000000003, "end": 3173.1200000000003, "text": " And the only need for them is to bring understanding into a common framework where", "tokens": [51160, 400, 264, 787, 643, 337, 552, 307, 281, 1565, 3701, 666, 257, 2689, 8388, 689, 51480], "temperature": 0.0, "avg_logprob": -0.14864517450332643, "compression_ratio": 1.6777251184834123, "no_speech_prob": 0.0009991293773055077}, {"id": 545, "seek": 315080, "start": 3173.1200000000003, "end": 3178.4, "text": " the ideas themselves in the latent space are being considered and their context and their", "tokens": [51480, 264, 3487, 2969, 294, 264, 48994, 1901, 366, 885, 4888, 293, 641, 4319, 293, 641, 51744], "temperature": 0.0, "avg_logprob": -0.14864517450332643, "compression_ratio": 1.6777251184834123, "no_speech_prob": 0.0009991293773055077}, {"id": 546, "seek": 317840, "start": 3178.4, "end": 3190.08, "text": " relationships. So how the, this is sort of a, what's the word, this idea of boiling down", "tokens": [50364, 6159, 13, 407, 577, 264, 11, 341, 307, 1333, 295, 257, 11, 437, 311, 264, 1349, 11, 341, 1558, 295, 16208, 760, 50948], "temperature": 0.0, "avg_logprob": -0.1780255945717416, "compression_ratio": 1.4344262295081966, "no_speech_prob": 0.0012627954129129648}, {"id": 547, "seek": 317840, "start": 3191.28, "end": 3198.48, "text": " the actual form of communication into some representation, any representation where we", "tokens": [51008, 264, 3539, 1254, 295, 6101, 666, 512, 10290, 11, 604, 10290, 689, 321, 51368], "temperature": 0.0, "avg_logprob": -0.1780255945717416, "compression_ratio": 1.4344262295081966, "no_speech_prob": 0.0012627954129129648}, {"id": 548, "seek": 319848, "start": 3198.48, "end": 3208.4, "text": " can start applying our knowledge to it. Whether you read text or listen to text,", "tokens": [50364, 393, 722, 9275, 527, 3601, 281, 309, 13, 8503, 291, 1401, 2487, 420, 2140, 281, 2487, 11, 50860], "temperature": 0.0, "avg_logprob": -0.15802424483829075, "compression_ratio": 1.5174418604651163, "no_speech_prob": 0.0215965174138546}, {"id": 549, "seek": 319848, "start": 3209.68, "end": 3216.16, "text": " you don't, when I hear things, I don't imagine them written down in front of me. I just hear words,", "tokens": [50924, 291, 500, 380, 11, 562, 286, 1568, 721, 11, 286, 500, 380, 3811, 552, 3720, 760, 294, 1868, 295, 385, 13, 286, 445, 1568, 2283, 11, 51248], "temperature": 0.0, "avg_logprob": -0.15802424483829075, "compression_ratio": 1.5174418604651163, "no_speech_prob": 0.0215965174138546}, {"id": 550, "seek": 319848, "start": 3216.16, "end": 3224.32, "text": " words become ideas, and we go from there. So in the same way, I see the way that", "tokens": [51248, 2283, 1813, 3487, 11, 293, 321, 352, 490, 456, 13, 407, 294, 264, 912, 636, 11, 286, 536, 264, 636, 300, 51656], "temperature": 0.0, "avg_logprob": -0.15802424483829075, "compression_ratio": 1.5174418604651163, "no_speech_prob": 0.0215965174138546}, {"id": 551, "seek": 322432, "start": 3225.04, "end": 3231.36, "text": " knowledge is presented as an input-output problem and embeddings really address the input-output", "tokens": [50400, 3601, 307, 8212, 382, 364, 4846, 12, 346, 2582, 1154, 293, 12240, 29432, 534, 2985, 264, 4846, 12, 346, 2582, 50716], "temperature": 0.0, "avg_logprob": -0.10274681348479196, "compression_ratio": 1.8215962441314555, "no_speech_prob": 0.0026827978435903788}, {"id": 552, "seek": 322432, "start": 3231.36, "end": 3239.92, "text": " problem. And then after that, there's a memory and consideration process which operates on ideas", "tokens": [50716, 1154, 13, 400, 550, 934, 300, 11, 456, 311, 257, 4675, 293, 12381, 1399, 597, 22577, 322, 3487, 51144], "temperature": 0.0, "avg_logprob": -0.10274681348479196, "compression_ratio": 1.8215962441314555, "no_speech_prob": 0.0026827978435903788}, {"id": 553, "seek": 322432, "start": 3239.92, "end": 3246.0, "text": " which are not linked to input and output. I think you and I agree that there needs to be a latent", "tokens": [51144, 597, 366, 406, 9408, 281, 4846, 293, 5598, 13, 286, 519, 291, 293, 286, 3986, 300, 456, 2203, 281, 312, 257, 48994, 51448], "temperature": 0.0, "avg_logprob": -0.10274681348479196, "compression_ratio": 1.8215962441314555, "no_speech_prob": 0.0026827978435903788}, {"id": 554, "seek": 322432, "start": 3246.0, "end": 3250.8, "text": " space or abstraction space for processing. And I think you also agree that there need not be too", "tokens": [51448, 1901, 420, 37765, 1901, 337, 9007, 13, 400, 286, 519, 291, 611, 3986, 300, 456, 643, 406, 312, 886, 51688], "temperature": 0.0, "avg_logprob": -0.10274681348479196, "compression_ratio": 1.8215962441314555, "no_speech_prob": 0.0026827978435903788}, {"id": 555, "seek": 325080, "start": 3250.8, "end": 3254.8, "text": " separate embedding space for the knowledge graph in the last image model. If I hear it correctly,", "tokens": [50364, 4994, 12240, 3584, 1901, 337, 264, 3601, 4295, 294, 264, 1036, 3256, 2316, 13, 759, 286, 1568, 309, 8944, 11, 50564], "temperature": 0.0, "avg_logprob": -0.16947634585268861, "compression_ratio": 1.7598425196850394, "no_speech_prob": 0.005456043407320976}, {"id": 556, "seek": 325080, "start": 3254.8, "end": 3258.4, "text": " right, you don't, you also don't think that is necessary, right? I think,", "tokens": [50564, 558, 11, 291, 500, 380, 11, 291, 611, 500, 380, 519, 300, 307, 4818, 11, 558, 30, 286, 519, 11, 50744], "temperature": 0.0, "avg_logprob": -0.16947634585268861, "compression_ratio": 1.7598425196850394, "no_speech_prob": 0.005456043407320976}, {"id": 557, "seek": 325080, "start": 3260.96, "end": 3265.04, "text": " yeah, but then the problem becomes, if you don't use the same embedding technique,", "tokens": [50872, 1338, 11, 457, 550, 264, 1154, 3643, 11, 498, 291, 500, 380, 764, 264, 912, 12240, 3584, 6532, 11, 51076], "temperature": 0.0, "avg_logprob": -0.16947634585268861, "compression_ratio": 1.7598425196850394, "no_speech_prob": 0.005456043407320976}, {"id": 558, "seek": 325080, "start": 3265.6800000000003, "end": 3271.84, "text": " how do you present meaning? So for mine, in terms of large language model being in language or not", "tokens": [51108, 577, 360, 291, 1974, 3620, 30, 407, 337, 3892, 11, 294, 2115, 295, 2416, 2856, 2316, 885, 294, 2856, 420, 406, 51416], "temperature": 0.0, "avg_logprob": -0.16947634585268861, "compression_ratio": 1.7598425196850394, "no_speech_prob": 0.005456043407320976}, {"id": 559, "seek": 325080, "start": 3271.84, "end": 3279.28, "text": " in language, in words, the question is really, are we making the problem harder for ourselves", "tokens": [51416, 294, 2856, 11, 294, 2283, 11, 264, 1168, 307, 534, 11, 366, 321, 1455, 264, 1154, 6081, 337, 4175, 51788], "temperature": 0.0, "avg_logprob": -0.16947634585268861, "compression_ratio": 1.7598425196850394, "no_speech_prob": 0.005456043407320976}, {"id": 560, "seek": 327928, "start": 3279.92, "end": 3287.1200000000003, "text": " by using a difference, by saying, well, it's all words and the words are by and large correct.", "tokens": [50396, 538, 1228, 257, 2649, 11, 538, 1566, 11, 731, 11, 309, 311, 439, 2283, 293, 264, 2283, 366, 538, 293, 2416, 3006, 13, 50756], "temperature": 0.0, "avg_logprob": -0.10509929873726585, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.003119667759165168}, {"id": 561, "seek": 327928, "start": 3287.1200000000003, "end": 3293.2000000000003, "text": " Therefore, we'll just use a large language model to read and ingest a large language model. And I", "tokens": [50756, 7504, 11, 321, 603, 445, 764, 257, 2416, 2856, 2316, 281, 1401, 293, 3957, 377, 257, 2416, 2856, 2316, 13, 400, 286, 51060], "temperature": 0.0, "avg_logprob": -0.10509929873726585, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.003119667759165168}, {"id": 562, "seek": 327928, "start": 3293.2000000000003, "end": 3298.96, "text": " think that will work. But the challenge becomes what you alluded to earlier, where the Chinese", "tokens": [51060, 519, 300, 486, 589, 13, 583, 264, 3430, 3643, 437, 291, 33919, 281, 3071, 11, 689, 264, 4649, 51348], "temperature": 0.0, "avg_logprob": -0.10509929873726585, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.003119667759165168}, {"id": 563, "seek": 327928, "start": 3298.96, "end": 3305.36, "text": " representation versus the English representation gives a different outcome. And I'm trying to", "tokens": [51348, 10290, 5717, 264, 3669, 10290, 2709, 257, 819, 9700, 13, 400, 286, 478, 1382, 281, 51668], "temperature": 0.0, "avg_logprob": -0.10509929873726585, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.003119667759165168}, {"id": 564, "seek": 330536, "start": 3305.44, "end": 3314.8, "text": " abstract away that behaviour. So the thinking is, the actual thinking happens in, is always in", "tokens": [50368, 12649, 1314, 300, 17229, 13, 407, 264, 1953, 307, 11, 264, 3539, 1953, 2314, 294, 11, 307, 1009, 294, 50836], "temperature": 0.0, "avg_logprob": -0.1881297164493137, "compression_ratio": 1.4795918367346939, "no_speech_prob": 0.004002282861620188}, {"id": 565, "seek": 330536, "start": 3314.8, "end": 3323.36, "text": " latent space. And the only job for embeddings is to present in a form where, you know, cognition can", "tokens": [50836, 48994, 1901, 13, 400, 264, 787, 1691, 337, 12240, 29432, 307, 281, 1974, 294, 257, 1254, 689, 11, 291, 458, 11, 46905, 393, 51264], "temperature": 0.0, "avg_logprob": -0.1881297164493137, "compression_ratio": 1.4795918367346939, "no_speech_prob": 0.004002282861620188}, {"id": 566, "seek": 330536, "start": 3323.36, "end": 3333.44, "text": " happen. Right? Because, right? And so I would say, I don't particularly care what the encoding", "tokens": [51264, 1051, 13, 1779, 30, 1436, 11, 558, 30, 400, 370, 286, 576, 584, 11, 286, 500, 380, 4098, 1127, 437, 264, 43430, 51768], "temperature": 0.0, "avg_logprob": -0.1881297164493137, "compression_ratio": 1.4795918367346939, "no_speech_prob": 0.004002282861620188}, {"id": 567, "seek": 333344, "start": 3333.52, "end": 3340.48, "text": " encoder is, decoder is, it can go from text in, picture out, picture in, text out, it doesn't", "tokens": [50368, 2058, 19866, 307, 11, 979, 19866, 307, 11, 309, 393, 352, 490, 2487, 294, 11, 3036, 484, 11, 3036, 294, 11, 2487, 484, 11, 309, 1177, 380, 50716], "temperature": 0.0, "avg_logprob": -0.12335817592660177, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.0013320844154804945}, {"id": 568, "seek": 333344, "start": 3340.48, "end": 3347.52, "text": " matter. The important thing is that it's consistent, and we can operate on it in a manner that addresses", "tokens": [50716, 1871, 13, 440, 1021, 551, 307, 300, 309, 311, 8398, 11, 293, 321, 393, 9651, 322, 309, 294, 257, 9060, 300, 16862, 51068], "temperature": 0.0, "avg_logprob": -0.12335817592660177, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.0013320844154804945}, {"id": 569, "seek": 333344, "start": 3347.52, "end": 3354.48, "text": " the patterns and relationships within. Yeah, well said, well said. I agree with you. So", "tokens": [51068, 264, 8294, 293, 6159, 1951, 13, 865, 11, 731, 848, 11, 731, 848, 13, 286, 3986, 365, 291, 13, 407, 51416], "temperature": 0.0, "avg_logprob": -0.12335817592660177, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.0013320844154804945}, {"id": 570, "seek": 333344, "start": 3355.2000000000003, "end": 3358.7200000000003, "text": " what matters is how we abstract it to the processing space, which is the latent space,", "tokens": [51452, 437, 7001, 307, 577, 321, 12649, 309, 281, 264, 9007, 1901, 11, 597, 307, 264, 48994, 1901, 11, 51628], "temperature": 0.0, "avg_logprob": -0.12335817592660177, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.0013320844154804945}, {"id": 571, "seek": 335872, "start": 3359.3599999999997, "end": 3365.04, "text": " and how we encode it and decode is just extra details, that basically just needs to be mapped", "tokens": [50396, 293, 577, 321, 2058, 1429, 309, 293, 979, 1429, 307, 445, 2857, 4365, 11, 300, 1936, 445, 2203, 281, 312, 33318, 50680], "temperature": 0.0, "avg_logprob": -0.20639923640659877, "compression_ratio": 1.657243816254417, "no_speech_prob": 0.006090668961405754}, {"id": 572, "seek": 335872, "start": 3365.04, "end": 3370.3199999999997, "text": " there, and it should be good enough. Yeah, I think so. And if it comes to its own training", "tokens": [50680, 456, 11, 293, 309, 820, 312, 665, 1547, 13, 865, 11, 286, 519, 370, 13, 400, 498, 309, 1487, 281, 1080, 1065, 3097, 50944], "temperature": 0.0, "avg_logprob": -0.20639923640659877, "compression_ratio": 1.657243816254417, "no_speech_prob": 0.006090668961405754}, {"id": 573, "seek": 335872, "start": 3370.3199999999997, "end": 3376.3199999999997, "text": " challenge. Correct. So I think in the earlier papers, what I get is like, why do we need a", "tokens": [50944, 3430, 13, 12753, 13, 407, 286, 519, 294, 264, 3071, 10577, 11, 437, 286, 483, 307, 411, 11, 983, 360, 321, 643, 257, 51244], "temperature": 0.0, "avg_logprob": -0.20639923640659877, "compression_ratio": 1.657243816254417, "no_speech_prob": 0.006090668961405754}, {"id": 574, "seek": 335872, "start": 3376.3199999999997, "end": 3380.48, "text": " knowledge graph encoder like that? It's because they use embeddings like trans-e that, you know,", "tokens": [51244, 3601, 4295, 2058, 19866, 411, 300, 30, 467, 311, 570, 436, 764, 12240, 29432, 411, 1145, 12, 68, 300, 11, 291, 458, 11, 51452], "temperature": 0.0, "avg_logprob": -0.20639923640659877, "compression_ratio": 1.657243816254417, "no_speech_prob": 0.006090668961405754}, {"id": 575, "seek": 335872, "start": 3380.48, "end": 3386.3999999999996, "text": " are different from GPD embeddings, like, or BERT embeddings. Again, most of the early papers use", "tokens": [51452, 366, 819, 490, 460, 17349, 12240, 29432, 11, 411, 11, 420, 363, 31479, 12240, 29432, 13, 3764, 11, 881, 295, 264, 2440, 10577, 764, 51748], "temperature": 0.0, "avg_logprob": -0.20639923640659877, "compression_ratio": 1.657243816254417, "no_speech_prob": 0.006090668961405754}, {"id": 576, "seek": 338640, "start": 3386.4, "end": 3394.08, "text": " but BERT, bi-directional encoder representations from transformers. So what happens is because", "tokens": [50364, 457, 363, 31479, 11, 3228, 12, 18267, 41048, 2058, 19866, 33358, 490, 4088, 433, 13, 407, 437, 2314, 307, 570, 50748], "temperature": 0.0, "avg_logprob": -0.11015638530763805, "compression_ratio": 1.8835341365461848, "no_speech_prob": 0.0014509839238598943}, {"id": 577, "seek": 338640, "start": 3394.08, "end": 3398.7200000000003, "text": " these two are from a different embedding space. So you kind of need to map them to the same", "tokens": [50748, 613, 732, 366, 490, 257, 819, 12240, 3584, 1901, 13, 407, 291, 733, 295, 643, 281, 4471, 552, 281, 264, 912, 50980], "temperature": 0.0, "avg_logprob": -0.11015638530763805, "compression_ratio": 1.8835341365461848, "no_speech_prob": 0.0014509839238598943}, {"id": 578, "seek": 338640, "start": 3398.7200000000003, "end": 3403.52, "text": " embedding space. That's why you need a knowledge graph encoder and a large-engaged model encoder.", "tokens": [50980, 12240, 3584, 1901, 13, 663, 311, 983, 291, 643, 257, 3601, 4295, 2058, 19866, 293, 257, 2416, 12, 1501, 2980, 2316, 2058, 19866, 13, 51220], "temperature": 0.0, "avg_logprob": -0.11015638530763805, "compression_ratio": 1.8835341365461848, "no_speech_prob": 0.0014509839238598943}, {"id": 579, "seek": 338640, "start": 3405.04, "end": 3410.88, "text": " But in the new kind of knowledge graph that is constructed, because this large-engaged model", "tokens": [51296, 583, 294, 264, 777, 733, 295, 3601, 4295, 300, 307, 17083, 11, 570, 341, 2416, 12, 1501, 2980, 2316, 51588], "temperature": 0.0, "avg_logprob": -0.11015638530763805, "compression_ratio": 1.8835341365461848, "no_speech_prob": 0.0014509839238598943}, {"id": 580, "seek": 338640, "start": 3410.88, "end": 3415.6800000000003, "text": " is now so powerful, you can actually use the embedding space for the large-engaged model to", "tokens": [51588, 307, 586, 370, 4005, 11, 291, 393, 767, 764, 264, 12240, 3584, 1901, 337, 264, 2416, 12, 1501, 2980, 2316, 281, 51828], "temperature": 0.0, "avg_logprob": -0.11015638530763805, "compression_ratio": 1.8835341365461848, "no_speech_prob": 0.0014509839238598943}, {"id": 581, "seek": 341568, "start": 3415.68, "end": 3420.96, "text": " construct your knowledge graph. And if you do that, okay, if you do that process, which is", "tokens": [50364, 7690, 428, 3601, 4295, 13, 400, 498, 291, 360, 300, 11, 1392, 11, 498, 291, 360, 300, 1399, 11, 597, 307, 50628], "temperature": 0.0, "avg_logprob": -0.08604495567187928, "compression_ratio": 1.9128630705394192, "no_speech_prob": 0.0014866486890241504}, {"id": 582, "seek": 341568, "start": 3420.96, "end": 3425.12, "text": " part two of the presentation today, you will see later, if you use the large-engaged models to", "tokens": [50628, 644, 732, 295, 264, 5860, 965, 11, 291, 486, 536, 1780, 11, 498, 291, 764, 264, 2416, 12, 1501, 2980, 5245, 281, 50836], "temperature": 0.0, "avg_logprob": -0.08604495567187928, "compression_ratio": 1.9128630705394192, "no_speech_prob": 0.0014866486890241504}, {"id": 583, "seek": 341568, "start": 3425.12, "end": 3429.2, "text": " construct the knowledge graph, actually, you don't need a separate knowledge graph encoder", "tokens": [50836, 7690, 264, 3601, 4295, 11, 767, 11, 291, 500, 380, 643, 257, 4994, 3601, 4295, 2058, 19866, 51040], "temperature": 0.0, "avg_logprob": -0.08604495567187928, "compression_ratio": 1.9128630705394192, "no_speech_prob": 0.0014866486890241504}, {"id": 584, "seek": 341568, "start": 3429.2, "end": 3435.2, "text": " or encoder here because they are in the same embedding space already. So if you look at this", "tokens": [51040, 420, 2058, 19866, 510, 570, 436, 366, 294, 264, 912, 12240, 3584, 1901, 1217, 13, 407, 498, 291, 574, 412, 341, 51340], "temperature": 0.0, "avg_logprob": -0.08604495567187928, "compression_ratio": 1.9128630705394192, "no_speech_prob": 0.0014866486890241504}, {"id": 585, "seek": 341568, "start": 3435.2, "end": 3442.8799999999997, "text": " thing here, you don't need a separate decoder for the JSON here because this is in the same", "tokens": [51340, 551, 510, 11, 291, 500, 380, 643, 257, 4994, 979, 19866, 337, 264, 31828, 510, 570, 341, 307, 294, 264, 912, 51724], "temperature": 0.0, "avg_logprob": -0.08604495567187928, "compression_ratio": 1.9128630705394192, "no_speech_prob": 0.0014866486890241504}, {"id": 586, "seek": 344288, "start": 3442.88, "end": 3448.8, "text": " embedding space as your text. And I would like to posit that it will be better for all of them to", "tokens": [50364, 12240, 3584, 1901, 382, 428, 2487, 13, 400, 286, 576, 411, 281, 11218, 300, 309, 486, 312, 1101, 337, 439, 295, 552, 281, 50660], "temperature": 0.0, "avg_logprob": -0.09866912364959717, "compression_ratio": 1.8692307692307693, "no_speech_prob": 0.0035443147644400597}, {"id": 587, "seek": 344288, "start": 3448.8, "end": 3455.12, "text": " be in the same embedding space because it will be much easier to do the attention. I mean,", "tokens": [50660, 312, 294, 264, 912, 12240, 3584, 1901, 570, 309, 486, 312, 709, 3571, 281, 360, 264, 3202, 13, 286, 914, 11, 50976], "temperature": 0.0, "avg_logprob": -0.09866912364959717, "compression_ratio": 1.8692307692307693, "no_speech_prob": 0.0035443147644400597}, {"id": 588, "seek": 344288, "start": 3455.12, "end": 3458.48, "text": " it's easier to do attention in the same domain as compared to different domains because, you know,", "tokens": [50976, 309, 311, 3571, 281, 360, 3202, 294, 264, 912, 9274, 382, 5347, 281, 819, 25514, 570, 11, 291, 458, 11, 51144], "temperature": 0.0, "avg_logprob": -0.09866912364959717, "compression_ratio": 1.8692307692307693, "no_speech_prob": 0.0035443147644400597}, {"id": 589, "seek": 344288, "start": 3458.48, "end": 3464.1600000000003, "text": " cross-attention is only one layer right now. You're going to do a very efficient cross-attentioning", "tokens": [51144, 3278, 12, 1591, 1251, 307, 787, 472, 4583, 558, 586, 13, 509, 434, 516, 281, 360, 257, 588, 7148, 3278, 12, 1591, 1251, 278, 51428], "temperature": 0.0, "avg_logprob": -0.09866912364959717, "compression_ratio": 1.8692307692307693, "no_speech_prob": 0.0035443147644400597}, {"id": 590, "seek": 344288, "start": 3464.1600000000003, "end": 3469.04, "text": " multiple layers. But if you just do it in the same domain, the transformer architecture right now,", "tokens": [51428, 3866, 7914, 13, 583, 498, 291, 445, 360, 309, 294, 264, 912, 9274, 11, 264, 31782, 9482, 558, 586, 11, 51672], "temperature": 0.0, "avg_logprob": -0.09866912364959717, "compression_ratio": 1.8692307692307693, "no_speech_prob": 0.0035443147644400597}, {"id": 591, "seek": 346904, "start": 3469.04, "end": 3474.8, "text": " you actually do the self-attention multiple times, all right? So it might be actually better to", "tokens": [50364, 291, 767, 360, 264, 2698, 12, 1591, 1251, 3866, 1413, 11, 439, 558, 30, 407, 309, 1062, 312, 767, 1101, 281, 50652], "temperature": 0.0, "avg_logprob": -0.12525969646016105, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.0008872474427334964}, {"id": 592, "seek": 346904, "start": 3474.8, "end": 3479.7599999999998, "text": " do it in the same domain, okay? And of course, you will save training complications because,", "tokens": [50652, 360, 309, 294, 264, 912, 9274, 11, 1392, 30, 400, 295, 1164, 11, 291, 486, 3155, 3097, 26566, 570, 11, 50900], "temperature": 0.0, "avg_logprob": -0.12525969646016105, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.0008872474427334964}, {"id": 593, "seek": 346904, "start": 3480.4, "end": 3484.88, "text": " you know, you need to map both to the same latent space and, you know, that is a difficult problem.", "tokens": [50932, 291, 458, 11, 291, 643, 281, 4471, 1293, 281, 264, 912, 48994, 1901, 293, 11, 291, 458, 11, 300, 307, 257, 2252, 1154, 13, 51156], "temperature": 0.0, "avg_logprob": -0.12525969646016105, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.0008872474427334964}, {"id": 594, "seek": 346904, "start": 3484.88, "end": 3490.4, "text": " It's a very difficult problem to map two different streams of inputs to the same latent space. I", "tokens": [51156, 467, 311, 257, 588, 2252, 1154, 281, 4471, 732, 819, 15842, 295, 15743, 281, 264, 912, 48994, 1901, 13, 286, 51432], "temperature": 0.0, "avg_logprob": -0.12525969646016105, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.0008872474427334964}, {"id": 595, "seek": 346904, "start": 3490.4, "end": 3495.84, "text": " mean, we have seen it, like in OpenAI, they have this thing called clip, okay, that max text and", "tokens": [51432, 914, 11, 321, 362, 1612, 309, 11, 411, 294, 7238, 48698, 11, 436, 362, 341, 551, 1219, 7353, 11, 1392, 11, 300, 11469, 2487, 293, 51704], "temperature": 0.0, "avg_logprob": -0.12525969646016105, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.0008872474427334964}, {"id": 596, "seek": 349584, "start": 3495.84, "end": 3505.1200000000003, "text": " images to same latent space. You know how many examples they train it on? Millions, I think even", "tokens": [50364, 5267, 281, 912, 48994, 1901, 13, 509, 458, 577, 867, 5110, 436, 3847, 309, 322, 30, 7190, 626, 11, 286, 519, 754, 50828], "temperature": 0.0, "avg_logprob": -0.13415657846551193, "compression_ratio": 1.7683397683397684, "no_speech_prob": 0.0029251764062792063}, {"id": 597, "seek": 349584, "start": 3505.1200000000003, "end": 3510.48, "text": " billions. Yeah, so it's a very, very difficult problem to map both to the same latent space.", "tokens": [50828, 17375, 13, 865, 11, 370, 309, 311, 257, 588, 11, 588, 2252, 1154, 281, 4471, 1293, 281, 264, 912, 48994, 1901, 13, 51096], "temperature": 0.0, "avg_logprob": -0.13415657846551193, "compression_ratio": 1.7683397683397684, "no_speech_prob": 0.0029251764062792063}, {"id": 598, "seek": 349584, "start": 3510.48, "end": 3514.4, "text": " If I can map it well, of course, you can do like stuff like stable diffusion, you know,", "tokens": [51096, 759, 286, 393, 4471, 309, 731, 11, 295, 1164, 11, 291, 393, 360, 411, 1507, 411, 8351, 25242, 11, 291, 458, 11, 51292], "temperature": 0.0, "avg_logprob": -0.13415657846551193, "compression_ratio": 1.7683397683397684, "no_speech_prob": 0.0029251764062792063}, {"id": 599, "seek": 349584, "start": 3515.1200000000003, "end": 3520.8, "text": " dolly, you can generate images from text. Yeah, but why have this problem with the", "tokens": [51328, 2722, 88, 11, 291, 393, 8460, 5267, 490, 2487, 13, 865, 11, 457, 983, 362, 341, 1154, 365, 264, 51612], "temperature": 0.0, "avg_logprob": -0.13415657846551193, "compression_ratio": 1.7683397683397684, "no_speech_prob": 0.0029251764062792063}, {"id": 600, "seek": 349584, "start": 3520.8, "end": 3525.04, "text": " knowledge graph when you can actually just ground the knowledge graph in the same embedding space", "tokens": [51612, 3601, 4295, 562, 291, 393, 767, 445, 2727, 264, 3601, 4295, 294, 264, 912, 12240, 3584, 1901, 51824], "temperature": 0.0, "avg_logprob": -0.13415657846551193, "compression_ratio": 1.7683397683397684, "no_speech_prob": 0.0029251764062792063}, {"id": 601, "seek": 352504, "start": 3525.04, "end": 3533.44, "text": " as your large language model? Okay, so think about that. Okay, now we go to approach two.", "tokens": [50364, 382, 428, 2416, 2856, 2316, 30, 1033, 11, 370, 519, 466, 300, 13, 1033, 11, 586, 321, 352, 281, 3109, 732, 13, 50784], "temperature": 0.0, "avg_logprob": -0.14839447021484375, "compression_ratio": 1.4702702702702704, "no_speech_prob": 0.0016600192757323384}, {"id": 602, "seek": 352504, "start": 3533.44, "end": 3539.04, "text": " Before I move on, I'd like to open the floor for any opportunities to ask anything so far for the", "tokens": [50784, 4546, 286, 1286, 322, 11, 286, 1116, 411, 281, 1269, 264, 4123, 337, 604, 4786, 281, 1029, 1340, 370, 1400, 337, 264, 51064], "temperature": 0.0, "avg_logprob": -0.14839447021484375, "compression_ratio": 1.4702702702702704, "no_speech_prob": 0.0016600192757323384}, {"id": 603, "seek": 352504, "start": 3539.04, "end": 3548.4, "text": " first part. Okay, if not, I'll carry on. So next is how we can use a knowledge large", "tokens": [51064, 700, 644, 13, 1033, 11, 498, 406, 11, 286, 603, 3985, 322, 13, 407, 958, 307, 577, 321, 393, 764, 257, 3601, 2416, 51532], "temperature": 0.0, "avg_logprob": -0.14839447021484375, "compression_ratio": 1.4702702702702704, "no_speech_prob": 0.0016600192757323384}, {"id": 604, "seek": 354840, "start": 3548.48, "end": 3555.28, "text": " language model to get the knowledge graph. So one is to use using few short and zero or zero", "tokens": [50368, 2856, 2316, 281, 483, 264, 3601, 4295, 13, 407, 472, 307, 281, 764, 1228, 1326, 2099, 293, 4018, 420, 4018, 50708], "temperature": 0.0, "avg_logprob": -0.14314884220788238, "compression_ratio": 1.8674698795180722, "no_speech_prob": 0.018934043124318123}, {"id": 605, "seek": 354840, "start": 3555.28, "end": 3559.52, "text": " short prompting, like for example, length chain, okay, I don't think the approach is that great.", "tokens": [50708, 2099, 12391, 278, 11, 411, 337, 1365, 11, 4641, 5021, 11, 1392, 11, 286, 500, 380, 519, 264, 3109, 307, 300, 869, 13, 50920], "temperature": 0.0, "avg_logprob": -0.14314884220788238, "compression_ratio": 1.8674698795180722, "no_speech_prob": 0.018934043124318123}, {"id": 606, "seek": 354840, "start": 3559.52, "end": 3565.84, "text": " Okay, I found a better approach, right, using a better prompt, but we can potentially use", "tokens": [50920, 1033, 11, 286, 1352, 257, 1101, 3109, 11, 558, 11, 1228, 257, 1101, 12391, 11, 457, 321, 393, 7263, 764, 51236], "temperature": 0.0, "avg_logprob": -0.14314884220788238, "compression_ratio": 1.8674698795180722, "no_speech_prob": 0.018934043124318123}, {"id": 607, "seek": 354840, "start": 3565.84, "end": 3570.0, "text": " large language models to generate knowledge graphs. The other way is to use the embedding", "tokens": [51236, 2416, 2856, 5245, 281, 8460, 3601, 24877, 13, 440, 661, 636, 307, 281, 764, 264, 12240, 3584, 51444], "temperature": 0.0, "avg_logprob": -0.14314884220788238, "compression_ratio": 1.8674698795180722, "no_speech_prob": 0.018934043124318123}, {"id": 608, "seek": 354840, "start": 3570.0, "end": 3575.44, "text": " space of the large language models to enrich the representational space of the knowledge graph.", "tokens": [51444, 1901, 295, 264, 2416, 2856, 5245, 281, 18849, 264, 2906, 1478, 1901, 295, 264, 3601, 4295, 13, 51716], "temperature": 0.0, "avg_logprob": -0.14314884220788238, "compression_ratio": 1.8674698795180722, "no_speech_prob": 0.018934043124318123}, {"id": 609, "seek": 357544, "start": 3576.0, "end": 3581.76, "text": " So this is also quite interesting. Let's see how we can do both. Okay, the first one is,", "tokens": [50392, 407, 341, 307, 611, 1596, 1880, 13, 961, 311, 536, 577, 321, 393, 360, 1293, 13, 1033, 11, 264, 700, 472, 307, 11, 50680], "temperature": 0.0, "avg_logprob": -0.14876031453630564, "compression_ratio": 1.6590038314176245, "no_speech_prob": 0.0013576314086094499}, {"id": 610, "seek": 357544, "start": 3582.48, "end": 3587.52, "text": " okay, this is just some idea of how we can use it. Okay, we can, we can few short prompt", "tokens": [50716, 1392, 11, 341, 307, 445, 512, 1558, 295, 577, 321, 393, 764, 309, 13, 1033, 11, 321, 393, 11, 321, 393, 1326, 2099, 12391, 50968], "temperature": 0.0, "avg_logprob": -0.14876031453630564, "compression_ratio": 1.6590038314176245, "no_speech_prob": 0.0013576314086094499}, {"id": 611, "seek": 357544, "start": 3587.52, "end": 3591.84, "text": " to generate the relations, okay, because large language models are just very versatile", "tokens": [50968, 281, 8460, 264, 2299, 11, 1392, 11, 570, 2416, 2856, 5245, 366, 445, 588, 25057, 51184], "temperature": 0.0, "avg_logprob": -0.14876031453630564, "compression_ratio": 1.6590038314176245, "no_speech_prob": 0.0013576314086094499}, {"id": 612, "seek": 357544, "start": 3591.84, "end": 3596.4, "text": " and can be context driven to do it. And actually it's way better than, you know,", "tokens": [51184, 293, 393, 312, 4319, 9555, 281, 360, 309, 13, 400, 767, 309, 311, 636, 1101, 813, 11, 291, 458, 11, 51412], "temperature": 0.0, "avg_logprob": -0.14876031453630564, "compression_ratio": 1.6590038314176245, "no_speech_prob": 0.0013576314086094499}, {"id": 613, "seek": 357544, "start": 3597.28, "end": 3602.64, "text": " so this is my own experience. I use space scene to do name entity recognition and I use", "tokens": [51456, 370, 341, 307, 452, 1065, 1752, 13, 286, 764, 1901, 4145, 281, 360, 1315, 13977, 11150, 293, 286, 764, 51724], "temperature": 0.0, "avg_logprob": -0.14876031453630564, "compression_ratio": 1.6590038314176245, "no_speech_prob": 0.0013576314086094499}, {"id": 614, "seek": 360264, "start": 3602.64, "end": 3608.7999999999997, "text": " large language models to do that. The GPT, chat GPT performs way better than space scene. Space", "tokens": [50364, 2416, 2856, 5245, 281, 360, 300, 13, 440, 26039, 51, 11, 5081, 26039, 51, 26213, 636, 1101, 813, 1901, 4145, 13, 8705, 50672], "temperature": 0.0, "avg_logprob": -0.23042226874310037, "compression_ratio": 1.6475770925110131, "no_speech_prob": 0.0016362505266442895}, {"id": 615, "seek": 360264, "start": 3608.7999999999997, "end": 3614.56, "text": " scene makes out a lot of the names, all right. So if we use large language models to generate", "tokens": [50672, 4145, 1669, 484, 257, 688, 295, 264, 5288, 11, 439, 558, 13, 407, 498, 321, 764, 2416, 2856, 5245, 281, 8460, 50960], "temperature": 0.0, "avg_logprob": -0.23042226874310037, "compression_ratio": 1.6475770925110131, "no_speech_prob": 0.0016362505266442895}, {"id": 616, "seek": 360264, "start": 3614.56, "end": 3620.48, "text": " the knowledge graph, compared to traditional approaches, like spacey or some other verb,", "tokens": [50960, 264, 3601, 4295, 11, 5347, 281, 5164, 11587, 11, 411, 1901, 88, 420, 512, 661, 9595, 11, 51256], "temperature": 0.0, "avg_logprob": -0.23042226874310037, "compression_ratio": 1.6475770925110131, "no_speech_prob": 0.0016362505266442895}, {"id": 617, "seek": 360264, "start": 3622.4, "end": 3627.52, "text": " VMP, you know, those kind of three parcels for language. Last time people used that to generate", "tokens": [51352, 691, 12224, 11, 291, 458, 11, 729, 733, 295, 1045, 30511, 1625, 337, 2856, 13, 5264, 565, 561, 1143, 300, 281, 8460, 51608], "temperature": 0.0, "avg_logprob": -0.23042226874310037, "compression_ratio": 1.6475770925110131, "no_speech_prob": 0.0016362505266442895}, {"id": 618, "seek": 362752, "start": 3627.52, "end": 3633.6, "text": " the knowledge graph to find out what are the nouns, what are the verbs and so on. Okay, so that was", "tokens": [50364, 264, 3601, 4295, 281, 915, 484, 437, 366, 264, 48184, 11, 437, 366, 264, 30051, 293, 370, 322, 13, 1033, 11, 370, 300, 390, 50668], "temperature": 0.0, "avg_logprob": -0.12990209034511022, "compression_ratio": 1.8605577689243027, "no_speech_prob": 0.0040632677264511585}, {"id": 619, "seek": 362752, "start": 3633.6, "end": 3637.28, "text": " difficult to generate the knowledge graph because sometimes it miss out certain things.", "tokens": [50668, 2252, 281, 8460, 264, 3601, 4295, 570, 2171, 309, 1713, 484, 1629, 721, 13, 50852], "temperature": 0.0, "avg_logprob": -0.12990209034511022, "compression_ratio": 1.8605577689243027, "no_speech_prob": 0.0040632677264511585}, {"id": 620, "seek": 362752, "start": 3637.28, "end": 3641.84, "text": " But large language models are quite good. Okay, why not just use large language models directly", "tokens": [50852, 583, 2416, 2856, 5245, 366, 1596, 665, 13, 1033, 11, 983, 406, 445, 764, 2416, 2856, 5245, 3838, 51080], "temperature": 0.0, "avg_logprob": -0.12990209034511022, "compression_ratio": 1.8605577689243027, "no_speech_prob": 0.0040632677264511585}, {"id": 621, "seek": 362752, "start": 3641.84, "end": 3648.32, "text": " to generate the relations and the source and the destination. So indeed, this is what Lang", "tokens": [51080, 281, 8460, 264, 2299, 293, 264, 4009, 293, 264, 12236, 13, 407, 6451, 11, 341, 307, 437, 13313, 51404], "temperature": 0.0, "avg_logprob": -0.12990209034511022, "compression_ratio": 1.8605577689243027, "no_speech_prob": 0.0040632677264511585}, {"id": 622, "seek": 362752, "start": 3648.32, "end": 3653.44, "text": " Chen did. Okay, so if you look at the graph QA prompt, this is the prompt. Okay, you are the", "tokens": [51404, 13682, 630, 13, 1033, 11, 370, 498, 291, 574, 412, 264, 4295, 1249, 32, 12391, 11, 341, 307, 264, 12391, 13, 1033, 11, 291, 366, 264, 51660], "temperature": 0.0, "avg_logprob": -0.12990209034511022, "compression_ratio": 1.8605577689243027, "no_speech_prob": 0.0040632677264511585}, {"id": 623, "seek": 365344, "start": 3653.44, "end": 3658.4, "text": " network intelligence. Okay, help to integrate stuff into a knowledge graph, extract knowledge", "tokens": [50364, 3209, 7599, 13, 1033, 11, 854, 281, 13365, 1507, 666, 257, 3601, 4295, 11, 8947, 3601, 50612], "temperature": 0.0, "avg_logprob": -0.12973845856530325, "compression_ratio": 1.8040816326530613, "no_speech_prob": 0.0025651368778198957}, {"id": 624, "seek": 365344, "start": 3658.4, "end": 3663.2000000000003, "text": " triples from text. Okay, a knowledge triple is a clause that contains a subject, predicate,", "tokens": [50612, 1376, 2622, 490, 2487, 13, 1033, 11, 257, 3601, 15508, 307, 257, 25925, 300, 8306, 257, 3983, 11, 3852, 8700, 11, 50852], "temperature": 0.0, "avg_logprob": -0.12973845856530325, "compression_ratio": 1.8040816326530613, "no_speech_prob": 0.0025651368778198957}, {"id": 625, "seek": 365344, "start": 3663.2000000000003, "end": 3668.2400000000002, "text": " an object. Okay, subject is entity being described, predicate is the property,", "tokens": [50852, 364, 2657, 13, 1033, 11, 3983, 307, 13977, 885, 7619, 11, 3852, 8700, 307, 264, 4707, 11, 51104], "temperature": 0.0, "avg_logprob": -0.12973845856530325, "compression_ratio": 1.8040816326530613, "no_speech_prob": 0.0025651368778198957}, {"id": 626, "seek": 365344, "start": 3669.28, "end": 3675.52, "text": " object is the value of the property. Okay, there's a typo here. Okay, so this is the", "tokens": [51156, 2657, 307, 264, 2158, 295, 264, 4707, 13, 1033, 11, 456, 311, 257, 2125, 78, 510, 13, 1033, 11, 370, 341, 307, 264, 51468], "temperature": 0.0, "avg_logprob": -0.12973845856530325, "compression_ratio": 1.8040816326530613, "no_speech_prob": 0.0025651368778198957}, {"id": 627, "seek": 365344, "start": 3677.12, "end": 3681.68, "text": " zero shot prompting for Lang Chen. All right, this is not that good yet. So you need to give", "tokens": [51548, 4018, 3347, 12391, 278, 337, 13313, 13682, 13, 1057, 558, 11, 341, 307, 406, 300, 665, 1939, 13, 407, 291, 643, 281, 976, 51776], "temperature": 0.0, "avg_logprob": -0.12973845856530325, "compression_ratio": 1.8040816326530613, "no_speech_prob": 0.0025651368778198957}, {"id": 628, "seek": 368168, "start": 3681.68, "end": 3685.9199999999996, "text": " some few short examples and they gave some few short examples in the prompt, like for example,", "tokens": [50364, 512, 1326, 2099, 5110, 293, 436, 2729, 512, 1326, 2099, 5110, 294, 264, 12391, 11, 411, 337, 1365, 11, 50576], "temperature": 0.0, "avg_logprob": -0.21101552963256837, "compression_ratio": 1.8115942028985508, "no_speech_prob": 0.012906655669212341}, {"id": 629, "seek": 368168, "start": 3686.72, "end": 3693.12, "text": " like this is the input, and then you can say that oh, Nevada is a state, Nevada is the U.S.,", "tokens": [50616, 411, 341, 307, 264, 4846, 11, 293, 550, 291, 393, 584, 300, 1954, 11, 25764, 307, 257, 1785, 11, 25764, 307, 264, 624, 13, 50, 7933, 50936], "temperature": 0.0, "avg_logprob": -0.21101552963256837, "compression_ratio": 1.8115942028985508, "no_speech_prob": 0.012906655669212341}, {"id": 630, "seek": 368168, "start": 3693.12, "end": 3698.64, "text": " Nevada is number one go producer in Go. So I don't like this example. Okay, because for one,", "tokens": [50936, 25764, 307, 1230, 472, 352, 12314, 294, 1037, 13, 407, 286, 500, 380, 411, 341, 1365, 13, 1033, 11, 570, 337, 472, 11, 51212], "temperature": 0.0, "avg_logprob": -0.21101552963256837, "compression_ratio": 1.8115942028985508, "no_speech_prob": 0.012906655669212341}, {"id": 631, "seek": 368168, "start": 3698.64, "end": 3704.56, "text": " they did not say the state at all in the prompt. Like then you want the model to just plug the", "tokens": [51212, 436, 630, 406, 584, 264, 1785, 412, 439, 294, 264, 12391, 13, 1743, 550, 291, 528, 264, 2316, 281, 445, 5452, 264, 51508], "temperature": 0.0, "avg_logprob": -0.21101552963256837, "compression_ratio": 1.8115942028985508, "no_speech_prob": 0.012906655669212341}, {"id": 632, "seek": 370456, "start": 3705.2799999999997, "end": 3712.08, "text": " plug the noun from thin air. So yeah, like here, I'm going to the store output none. Why, why is", "tokens": [50400, 5452, 264, 23307, 490, 5862, 1988, 13, 407, 1338, 11, 411, 510, 11, 286, 478, 516, 281, 264, 3531, 5598, 6022, 13, 1545, 11, 983, 307, 50740], "temperature": 0.0, "avg_logprob": -0.21014564205901792, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.024638144299387932}, {"id": 633, "seek": 370456, "start": 3712.08, "end": 3721.84, "text": " the output none? It should be I went to store something like that. Yeah, so you should be able", "tokens": [50740, 264, 5598, 6022, 30, 467, 820, 312, 286, 1437, 281, 3531, 746, 411, 300, 13, 865, 11, 370, 291, 820, 312, 1075, 51228], "temperature": 0.0, "avg_logprob": -0.21014564205901792, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.024638144299387932}, {"id": 634, "seek": 370456, "start": 3721.84, "end": 3727.68, "text": " to extract something from this. So I disagree with the examples that the Lang Chen one provided.", "tokens": [51228, 281, 8947, 746, 490, 341, 13, 407, 286, 14091, 365, 264, 5110, 300, 264, 13313, 13682, 472, 5649, 13, 51520], "temperature": 0.0, "avg_logprob": -0.21014564205901792, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.024638144299387932}, {"id": 635, "seek": 370456, "start": 3727.68, "end": 3733.2, "text": " Okay, so I think if they improve this example, maybe theirs would work better. So let's take a", "tokens": [51520, 1033, 11, 370, 286, 519, 498, 436, 3470, 341, 1365, 11, 1310, 22760, 576, 589, 1101, 13, 407, 718, 311, 747, 257, 51796], "temperature": 0.0, "avg_logprob": -0.21014564205901792, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.024638144299387932}, {"id": 636, "seek": 373320, "start": 3733.2, "end": 3739.12, "text": " look at what I did later. So I'm not a fan of Lang Chen, by the way. Lang Chen prompts are very", "tokens": [50364, 574, 412, 437, 286, 630, 1780, 13, 407, 286, 478, 406, 257, 3429, 295, 13313, 13682, 11, 538, 264, 636, 13, 13313, 13682, 41095, 366, 588, 50660], "temperature": 0.0, "avg_logprob": -0.12911736378904248, "compression_ratio": 1.8455598455598456, "no_speech_prob": 0.0014911465113982558}, {"id": 637, "seek": 373320, "start": 3739.12, "end": 3748.0, "text": " worthy. So this is the other way that we can use the large language models to do the text encoding,", "tokens": [50660, 14829, 13, 407, 341, 307, 264, 661, 636, 300, 321, 393, 764, 264, 2416, 2856, 5245, 281, 360, 264, 2487, 43430, 11, 51104], "temperature": 0.0, "avg_logprob": -0.12911736378904248, "compression_ratio": 1.8455598455598456, "no_speech_prob": 0.0014911465113982558}, {"id": 638, "seek": 373320, "start": 3748.0, "end": 3753.2799999999997, "text": " to do knowledge graph embeddings. So this is called KGE, knowledge graph embeddings,", "tokens": [51104, 281, 360, 3601, 4295, 12240, 29432, 13, 407, 341, 307, 1219, 591, 9177, 11, 3601, 4295, 12240, 29432, 11, 51368], "temperature": 0.0, "avg_logprob": -0.12911736378904248, "compression_ratio": 1.8455598455598456, "no_speech_prob": 0.0014911465113982558}, {"id": 639, "seek": 373320, "start": 3753.2799999999997, "end": 3757.12, "text": " is something like, you know, if you talk about the stuff like trans, these are like embeddings", "tokens": [51368, 307, 746, 411, 11, 291, 458, 11, 498, 291, 751, 466, 264, 1507, 411, 1145, 11, 613, 366, 411, 12240, 29432, 51560], "temperature": 0.0, "avg_logprob": -0.12911736378904248, "compression_ratio": 1.8455598455598456, "no_speech_prob": 0.0014911465113982558}, {"id": 640, "seek": 373320, "start": 3757.12, "end": 3762.72, "text": " that we can give to the source, to the destination, to the relation. So we can represent the knowledge", "tokens": [51560, 300, 321, 393, 976, 281, 264, 4009, 11, 281, 264, 12236, 11, 281, 264, 9721, 13, 407, 321, 393, 2906, 264, 3601, 51840], "temperature": 0.0, "avg_logprob": -0.12911736378904248, "compression_ratio": 1.8455598455598456, "no_speech_prob": 0.0014911465113982558}, {"id": 641, "seek": 376272, "start": 3762.72, "end": 3769.8399999999997, "text": " graph as embeddings. And we can use GPT or some large language model, okay, to generate some", "tokens": [50364, 4295, 382, 12240, 29432, 13, 400, 321, 393, 764, 26039, 51, 420, 512, 2416, 2856, 2316, 11, 1392, 11, 281, 8460, 512, 50720], "temperature": 0.0, "avg_logprob": -0.13103135118206727, "compression_ratio": 1.7053571428571428, "no_speech_prob": 0.0021978099830448627}, {"id": 642, "seek": 376272, "start": 3769.8399999999997, "end": 3775.7599999999998, "text": " embedding space here that you can then use like MLP, multi layer perceptron, and so on to map to", "tokens": [50720, 12240, 3584, 1901, 510, 300, 291, 393, 550, 764, 411, 21601, 47, 11, 4825, 4583, 43276, 2044, 11, 293, 370, 322, 281, 4471, 281, 51016], "temperature": 0.0, "avg_logprob": -0.13103135118206727, "compression_ratio": 1.7053571428571428, "no_speech_prob": 0.0021978099830448627}, {"id": 643, "seek": 376272, "start": 3776.3999999999996, "end": 3781.8399999999997, "text": " to the embedding space of the knowledge graph embeddings. So this is one way we can utilize", "tokens": [51048, 281, 264, 12240, 3584, 1901, 295, 264, 3601, 4295, 12240, 29432, 13, 407, 341, 307, 472, 636, 321, 393, 16117, 51320], "temperature": 0.0, "avg_logprob": -0.13103135118206727, "compression_ratio": 1.7053571428571428, "no_speech_prob": 0.0021978099830448627}, {"id": 644, "seek": 376272, "start": 3781.8399999999997, "end": 3787.6, "text": " large language models to do it. Yeah, I mean, I was thinking, you know, like, why not just use this,", "tokens": [51320, 2416, 2856, 5245, 281, 360, 309, 13, 865, 11, 286, 914, 11, 286, 390, 1953, 11, 291, 458, 11, 411, 11, 983, 406, 445, 764, 341, 11, 51608], "temperature": 0.0, "avg_logprob": -0.13103135118206727, "compression_ratio": 1.7053571428571428, "no_speech_prob": 0.0021978099830448627}, {"id": 645, "seek": 378760, "start": 3787.68, "end": 3793.92, "text": " right? Why not just use LAM embeddings directly for knowledge graph?", "tokens": [50368, 558, 30, 1545, 406, 445, 764, 441, 2865, 12240, 29432, 3838, 337, 3601, 4295, 30, 50680], "temperature": 0.0, "avg_logprob": -0.15033906247435497, "compression_ratio": 1.921487603305785, "no_speech_prob": 0.0008037461084313691}, {"id": 646, "seek": 378760, "start": 3796.08, "end": 3801.12, "text": " I mean, LAMs are way better than, than doing graph neural networks, in the sense that, you know,", "tokens": [50788, 286, 914, 11, 441, 2865, 82, 366, 636, 1101, 813, 11, 813, 884, 4295, 18161, 9590, 11, 294, 264, 2020, 300, 11, 291, 458, 11, 51040], "temperature": 0.0, "avg_logprob": -0.15033906247435497, "compression_ratio": 1.921487603305785, "no_speech_prob": 0.0008037461084313691}, {"id": 647, "seek": 378760, "start": 3801.12, "end": 3804.72, "text": " if you know the problems with graph neural networks, I'm just going to tell you the problems of graph", "tokens": [51040, 498, 291, 458, 264, 2740, 365, 4295, 18161, 9590, 11, 286, 478, 445, 516, 281, 980, 291, 264, 2740, 295, 4295, 51220], "temperature": 0.0, "avg_logprob": -0.15033906247435497, "compression_ratio": 1.921487603305785, "no_speech_prob": 0.0008037461084313691}, {"id": 648, "seek": 378760, "start": 3804.72, "end": 3810.96, "text": " neural networks now. Okay, they have these two problems. Okay, this is one is called over squishing", "tokens": [51220, 18161, 9590, 586, 13, 1033, 11, 436, 362, 613, 732, 2740, 13, 1033, 11, 341, 307, 472, 307, 1219, 670, 2339, 3807, 51532], "temperature": 0.0, "avg_logprob": -0.15033906247435497, "compression_ratio": 1.921487603305785, "no_speech_prob": 0.0008037461084313691}, {"id": 649, "seek": 378760, "start": 3810.96, "end": 3816.16, "text": " or over squashing. And the other one is called over smoothing. Okay, what are these two problems?", "tokens": [51532, 420, 670, 2339, 11077, 13, 400, 264, 661, 472, 307, 1219, 670, 899, 6259, 571, 13, 1033, 11, 437, 366, 613, 732, 2740, 30, 51792], "temperature": 0.0, "avg_logprob": -0.15033906247435497, "compression_ratio": 1.921487603305785, "no_speech_prob": 0.0008037461084313691}, {"id": 650, "seek": 381616, "start": 3816.16, "end": 3822.16, "text": " Over squashing is that the information, because you pass the information into an embedding layer,", "tokens": [50364, 4886, 2339, 11077, 307, 300, 264, 1589, 11, 570, 291, 1320, 264, 1589, 666, 364, 12240, 3584, 4583, 11, 50664], "temperature": 0.0, "avg_logprob": -0.11149701864823051, "compression_ratio": 1.6990740740740742, "no_speech_prob": 0.0012663329252973199}, {"id": 651, "seek": 381616, "start": 3822.16, "end": 3829.52, "text": " information gets lost at embeddings. Okay, so this over squashing thing is also a problem for", "tokens": [50664, 1589, 2170, 2731, 412, 12240, 29432, 13, 1033, 11, 370, 341, 670, 2339, 11077, 551, 307, 611, 257, 1154, 337, 51032], "temperature": 0.0, "avg_logprob": -0.11149701864823051, "compression_ratio": 1.6990740740740742, "no_speech_prob": 0.0012663329252973199}, {"id": 652, "seek": 381616, "start": 3830.96, "end": 3836.08, "text": " LAMs. So I'm not going to cover too much on it. The other problem that we have for this kind of", "tokens": [51104, 441, 2865, 82, 13, 407, 286, 478, 406, 516, 281, 2060, 886, 709, 322, 309, 13, 440, 661, 1154, 300, 321, 362, 337, 341, 733, 295, 51360], "temperature": 0.0, "avg_logprob": -0.11149701864823051, "compression_ratio": 1.6990740740740742, "no_speech_prob": 0.0012663329252973199}, {"id": 653, "seek": 381616, "start": 3836.08, "end": 3841.3599999999997, "text": " graph neural network over smoothing, okay, is that after you do message passing", "tokens": [51360, 4295, 18161, 3209, 670, 899, 6259, 571, 11, 1392, 11, 307, 300, 934, 291, 360, 3636, 8437, 51624], "temperature": 0.0, "avg_logprob": -0.11149701864823051, "compression_ratio": 1.6990740740740742, "no_speech_prob": 0.0012663329252973199}, {"id": 654, "seek": 384136, "start": 3842.0, "end": 3850.56, "text": " or too many times, all embeddings look the same. Okay, so this is a big problem. Okay, I also", "tokens": [50396, 420, 886, 867, 1413, 11, 439, 12240, 29432, 574, 264, 912, 13, 1033, 11, 370, 341, 307, 257, 955, 1154, 13, 1033, 11, 286, 611, 50824], "temperature": 0.0, "avg_logprob": -0.17174541360080833, "compression_ratio": 1.7035398230088497, "no_speech_prob": 0.00357738952152431}, {"id": 655, "seek": 384136, "start": 3850.56, "end": 3854.48, "text": " realized this, that once I did graph neural networks, you, like, you have two nodes, you pass", "tokens": [50824, 5334, 341, 11, 300, 1564, 286, 630, 4295, 18161, 9590, 11, 291, 11, 411, 11, 291, 362, 732, 13891, 11, 291, 1320, 51020], "temperature": 0.0, "avg_logprob": -0.17174541360080833, "compression_ratio": 1.7035398230088497, "no_speech_prob": 0.00357738952152431}, {"id": 656, "seek": 384136, "start": 3854.48, "end": 3858.7200000000003, "text": " information to each other, and then you become the average of the information, you keep doing this,", "tokens": [51020, 1589, 281, 1184, 661, 11, 293, 550, 291, 1813, 264, 4274, 295, 264, 1589, 11, 291, 1066, 884, 341, 11, 51232], "temperature": 0.0, "avg_logprob": -0.17174541360080833, "compression_ratio": 1.7035398230088497, "no_speech_prob": 0.00357738952152431}, {"id": 657, "seek": 384136, "start": 3858.7200000000003, "end": 3864.1600000000003, "text": " right? Eventually, both nodes become the same, or very, very similar. Okay, so this is one of the", "tokens": [51232, 558, 30, 17586, 11, 1293, 13891, 1813, 264, 912, 11, 420, 588, 11, 588, 2531, 13, 1033, 11, 370, 341, 307, 472, 295, 264, 51504], "temperature": 0.0, "avg_logprob": -0.17174541360080833, "compression_ratio": 1.7035398230088497, "no_speech_prob": 0.00357738952152431}, {"id": 658, "seek": 386416, "start": 3864.16, "end": 3870.96, "text": " huge problems of graph neural networks. And I feel like the embedding space that is best", "tokens": [50364, 2603, 2740, 295, 4295, 18161, 9590, 13, 400, 286, 841, 411, 264, 12240, 3584, 1901, 300, 307, 1151, 50704], "temperature": 0.0, "avg_logprob": -0.11345536729930776, "compression_ratio": 1.9428571428571428, "no_speech_prob": 0.014300214126706123}, {"id": 659, "seek": 386416, "start": 3870.96, "end": 3875.68, "text": " done, right, is not the way that we do message passing in graph neural network. We should just", "tokens": [50704, 1096, 11, 558, 11, 307, 406, 264, 636, 300, 321, 360, 3636, 8437, 294, 4295, 18161, 3209, 13, 492, 820, 445, 50940], "temperature": 0.0, "avg_logprob": -0.11345536729930776, "compression_ratio": 1.9428571428571428, "no_speech_prob": 0.014300214126706123}, {"id": 660, "seek": 386416, "start": 3875.68, "end": 3880.72, "text": " ground it in the context using a large language model. And large language model update the context", "tokens": [50940, 2727, 309, 294, 264, 4319, 1228, 257, 2416, 2856, 2316, 13, 400, 2416, 2856, 2316, 5623, 264, 4319, 51192], "temperature": 0.0, "avg_logprob": -0.11345536729930776, "compression_ratio": 1.9428571428571428, "no_speech_prob": 0.014300214126706123}, {"id": 661, "seek": 386416, "start": 3880.72, "end": 3886.08, "text": " quite well. Okay, then you can just use the embedding that is derived from that particular context", "tokens": [51192, 1596, 731, 13, 1033, 11, 550, 291, 393, 445, 764, 264, 12240, 3584, 300, 307, 18949, 490, 300, 1729, 4319, 51460], "temperature": 0.0, "avg_logprob": -0.11345536729930776, "compression_ratio": 1.9428571428571428, "no_speech_prob": 0.014300214126706123}, {"id": 662, "seek": 386416, "start": 3886.08, "end": 3890.96, "text": " in, like, you can just put something like that, like, you can just say context. And then like,", "tokens": [51460, 294, 11, 411, 11, 291, 393, 445, 829, 746, 411, 300, 11, 411, 11, 291, 393, 445, 584, 4319, 13, 400, 550, 411, 11, 51704], "temperature": 0.0, "avg_logprob": -0.11345536729930776, "compression_ratio": 1.9428571428571428, "no_speech_prob": 0.014300214126706123}, {"id": 663, "seek": 389096, "start": 3891.92, "end": 3897.52, "text": " I am a student or something like that. So, like, this context will update the definition", "tokens": [50412, 286, 669, 257, 3107, 420, 746, 411, 300, 13, 407, 11, 411, 11, 341, 4319, 486, 5623, 264, 7123, 50692], "temperature": 0.0, "avg_logprob": -0.12989994719788267, "compression_ratio": 1.816425120772947, "no_speech_prob": 0.0010249860351905227}, {"id": 664, "seek": 389096, "start": 3897.52, "end": 3902.88, "text": " of the student here. So you can go through the transformer module. So this is the transformer", "tokens": [50692, 295, 264, 3107, 510, 13, 407, 291, 393, 352, 807, 264, 31782, 10088, 13, 407, 341, 307, 264, 31782, 50960], "temperature": 0.0, "avg_logprob": -0.12989994719788267, "compression_ratio": 1.816425120772947, "no_speech_prob": 0.0010249860351905227}, {"id": 665, "seek": 389096, "start": 3902.88, "end": 3911.04, "text": " module. And then you can get the final embedding here. Yeah, at the final layer, right, before", "tokens": [50960, 10088, 13, 400, 550, 291, 393, 483, 264, 2572, 12240, 3584, 510, 13, 865, 11, 412, 264, 2572, 4583, 11, 558, 11, 949, 51368], "temperature": 0.0, "avg_logprob": -0.12989994719788267, "compression_ratio": 1.816425120772947, "no_speech_prob": 0.0010249860351905227}, {"id": 666, "seek": 389096, "start": 3911.04, "end": 3916.7200000000003, "text": " the softmax, you can actually use the transformer to get the embeddings already. Why use knowledge", "tokens": [51368, 264, 2787, 41167, 11, 291, 393, 767, 764, 264, 31782, 281, 483, 264, 12240, 29432, 1217, 13, 1545, 764, 3601, 51652], "temperature": 0.0, "avg_logprob": -0.12989994719788267, "compression_ratio": 1.816425120772947, "no_speech_prob": 0.0010249860351905227}, {"id": 667, "seek": 391672, "start": 3916.7999999999997, "end": 3921.68, "text": " graph embeddings? Okay, so I'm just putting this question out here. So I hope those people", "tokens": [50368, 4295, 12240, 29432, 30, 1033, 11, 370, 286, 478, 445, 3372, 341, 1168, 484, 510, 13, 407, 286, 1454, 729, 561, 50612], "temperature": 0.0, "avg_logprob": -0.09688095388741329, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.0027347137220203876}, {"id": 668, "seek": 391672, "start": 3921.68, "end": 3926.3199999999997, "text": " knowledgeable in this area can come and, you know, correct me if I'm wrong. But I don't see a point", "tokens": [50612, 33800, 294, 341, 1859, 393, 808, 293, 11, 291, 458, 11, 3006, 385, 498, 286, 478, 2085, 13, 583, 286, 500, 380, 536, 257, 935, 50844], "temperature": 0.0, "avg_logprob": -0.09688095388741329, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.0027347137220203876}, {"id": 669, "seek": 391672, "start": 3926.3199999999997, "end": 3933.8399999999997, "text": " in doing this. Yeah, right now. Okay, so let's leave it as that. And let's continue.", "tokens": [50844, 294, 884, 341, 13, 865, 11, 558, 586, 13, 1033, 11, 370, 718, 311, 1856, 309, 382, 300, 13, 400, 718, 311, 2354, 13, 51220], "temperature": 0.0, "avg_logprob": -0.09688095388741329, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.0027347137220203876}, {"id": 670, "seek": 391672, "start": 3936.08, "end": 3941.04, "text": " Approach tree. So the approach tree is how we combine both approaches to make a very, very", "tokens": [51332, 29551, 608, 4230, 13, 407, 264, 3109, 4230, 307, 577, 321, 10432, 1293, 11587, 281, 652, 257, 588, 11, 588, 51580], "temperature": 0.0, "avg_logprob": -0.09688095388741329, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.0027347137220203876}, {"id": 671, "seek": 391672, "start": 3941.9199999999996, "end": 3946.24, "text": " synergistic model where the large language model can generate the knowledge graph dynamically", "tokens": [51624, 33781, 70, 3142, 2316, 689, 264, 2416, 2856, 2316, 393, 8460, 264, 3601, 4295, 43492, 51840], "temperature": 0.0, "avg_logprob": -0.09688095388741329, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.0027347137220203876}, {"id": 672, "seek": 394672, "start": 3946.72, "end": 3950.64, "text": " and this knowledge graph is something like a dynamic memory that gets updated as the agent", "tokens": [50364, 293, 341, 3601, 4295, 307, 746, 411, 257, 8546, 4675, 300, 2170, 10588, 382, 264, 9461, 50560], "temperature": 0.0, "avg_logprob": -0.1054244978087289, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.0010397653095424175}, {"id": 673, "seek": 394672, "start": 3950.64, "end": 3955.6, "text": " explores the world and so on. And this knowledge graph can then inform the knowledge, the large", "tokens": [50560, 45473, 264, 1002, 293, 370, 322, 13, 400, 341, 3601, 4295, 393, 550, 1356, 264, 3601, 11, 264, 2416, 50808], "temperature": 0.0, "avg_logprob": -0.1054244978087289, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.0010397653095424175}, {"id": 674, "seek": 394672, "start": 3955.6, "end": 3962.24, "text": " language model and ground it in consistent generation. So let's see how this works. So you", "tokens": [50808, 2856, 2316, 293, 2727, 309, 294, 8398, 5125, 13, 407, 718, 311, 536, 577, 341, 1985, 13, 407, 291, 51140], "temperature": 0.0, "avg_logprob": -0.1054244978087289, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.0010397653095424175}, {"id": 675, "seek": 394672, "start": 3962.24, "end": 3968.16, "text": " can see this is the diagram in the paper. And you can see like data. Okay, that's what me and Richard", "tokens": [51140, 393, 536, 341, 307, 264, 10686, 294, 264, 3035, 13, 400, 291, 393, 536, 411, 1412, 13, 1033, 11, 300, 311, 437, 385, 293, 9809, 51436], "temperature": 0.0, "avg_logprob": -0.1054244978087289, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.0010397653095424175}, {"id": 676, "seek": 394672, "start": 3968.16, "end": 3976.3999999999996, "text": " discussed. Data, okay, will be from different domains to embed them into latent space. Okay, so", "tokens": [51436, 7152, 13, 11888, 11, 1392, 11, 486, 312, 490, 819, 25514, 281, 12240, 552, 666, 48994, 1901, 13, 1033, 11, 370, 51848], "temperature": 0.0, "avg_logprob": -0.1054244978087289, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.0010397653095424175}, {"id": 677, "seek": 397640, "start": 3976.4, "end": 3981.52, "text": " now we just assume that there's only one latent space but my view is that there's multiple latent", "tokens": [50364, 586, 321, 445, 6552, 300, 456, 311, 787, 472, 48994, 1901, 457, 452, 1910, 307, 300, 456, 311, 3866, 48994, 50620], "temperature": 0.0, "avg_logprob": -0.09896798487062808, "compression_ratio": 2.0127659574468084, "no_speech_prob": 0.0005494204815477133}, {"id": 678, "seek": 397640, "start": 3981.52, "end": 3988.48, "text": " spaces. Okay, so right now we just treat it as there's only one latent space. You process the", "tokens": [50620, 7673, 13, 1033, 11, 370, 558, 586, 321, 445, 2387, 309, 382, 456, 311, 787, 472, 48994, 1901, 13, 509, 1399, 264, 50968], "temperature": 0.0, "avg_logprob": -0.09896798487062808, "compression_ratio": 2.0127659574468084, "no_speech_prob": 0.0005494204815477133}, {"id": 679, "seek": 397640, "start": 3988.48, "end": 3992.96, "text": " information in that one latent space using knowledge graph and large language model in this", "tokens": [50968, 1589, 294, 300, 472, 48994, 1901, 1228, 3601, 4295, 293, 2416, 2856, 2316, 294, 341, 51192], "temperature": 0.0, "avg_logprob": -0.09896798487062808, "compression_ratio": 2.0127659574468084, "no_speech_prob": 0.0005494204815477133}, {"id": 680, "seek": 397640, "start": 3992.96, "end": 3998.96, "text": " loop. Okay, so knowledge graph can ground the language model in consistency. Language model", "tokens": [51192, 6367, 13, 1033, 11, 370, 3601, 4295, 393, 2727, 264, 2856, 2316, 294, 14416, 13, 24445, 2316, 51492], "temperature": 0.0, "avg_logprob": -0.09896798487062808, "compression_ratio": 2.0127659574468084, "no_speech_prob": 0.0005494204815477133}, {"id": 681, "seek": 397640, "start": 3998.96, "end": 4004.4, "text": " can make the knowledge graph more expressive. Okay, and not as rigid as before. I mean, maybe you", "tokens": [51492, 393, 652, 264, 3601, 4295, 544, 40189, 13, 1033, 11, 293, 406, 382, 22195, 382, 949, 13, 286, 914, 11, 1310, 291, 51764], "temperature": 0.0, "avg_logprob": -0.09896798487062808, "compression_ratio": 2.0127659574468084, "no_speech_prob": 0.0005494204815477133}, {"id": 682, "seek": 400440, "start": 4004.4, "end": 4009.6800000000003, "text": " can use embedding based knowledge graphs like then you can make the knowledge graphs like express a", "tokens": [50364, 393, 764, 12240, 3584, 2361, 3601, 24877, 411, 550, 291, 393, 652, 264, 3601, 24877, 411, 5109, 257, 50628], "temperature": 0.0, "avg_logprob": -0.11236064774649483, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.0004629625182133168}, {"id": 683, "seek": 400440, "start": 4009.6800000000003, "end": 4015.36, "text": " lot of things more than just text alone. Okay, so this is one idea. You can use different techniques", "tokens": [50628, 688, 295, 721, 544, 813, 445, 2487, 3312, 13, 1033, 11, 370, 341, 307, 472, 1558, 13, 509, 393, 764, 819, 7512, 50912], "temperature": 0.0, "avg_logprob": -0.11236064774649483, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.0004629625182133168}, {"id": 684, "seek": 400440, "start": 4015.36, "end": 4020.56, "text": " to process it like graph theory networks from engineering, representational learning. Yeah,", "tokens": [50912, 281, 1399, 309, 411, 4295, 5261, 9590, 490, 7043, 11, 2906, 1478, 2539, 13, 865, 11, 51172], "temperature": 0.0, "avg_logprob": -0.11236064774649483, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.0004629625182133168}, {"id": 685, "seek": 400440, "start": 4020.56, "end": 4026.56, "text": " I mean, this is just some big words, but the idea is you basically do some processing. All right,", "tokens": [51172, 286, 914, 11, 341, 307, 445, 512, 955, 2283, 11, 457, 264, 1558, 307, 291, 1936, 360, 512, 9007, 13, 1057, 558, 11, 51472], "temperature": 0.0, "avg_logprob": -0.11236064774649483, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.0004629625182133168}, {"id": 686, "seek": 400440, "start": 4026.56, "end": 4030.88, "text": " you can use large language models to process. Or if you like it, you can process it using the", "tokens": [51472, 291, 393, 764, 2416, 2856, 5245, 281, 1399, 13, 1610, 498, 291, 411, 309, 11, 291, 393, 1399, 309, 1228, 264, 51688], "temperature": 0.0, "avg_logprob": -0.11236064774649483, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.0004629625182133168}, {"id": 687, "seek": 403088, "start": 4030.88, "end": 4036.2400000000002, "text": " knowledge graph, which is like a graph neural network to process. Okay, or you can make the", "tokens": [50364, 3601, 4295, 11, 597, 307, 411, 257, 4295, 18161, 3209, 281, 1399, 13, 1033, 11, 420, 291, 393, 652, 264, 50632], "temperature": 0.0, "avg_logprob": -0.16963111877441406, "compression_ratio": 2.063063063063063, "no_speech_prob": 0.0016591809689998627}, {"id": 688, "seek": 403088, "start": 4036.2400000000002, "end": 4039.44, "text": " graph neural network into text and then you can do some neural symbolic reasoning.", "tokens": [50632, 4295, 18161, 3209, 666, 2487, 293, 550, 291, 393, 360, 512, 18161, 25755, 21577, 13, 50792], "temperature": 0.0, "avg_logprob": -0.16963111877441406, "compression_ratio": 2.063063063063063, "no_speech_prob": 0.0016591809689998627}, {"id": 689, "seek": 403088, "start": 4040.2400000000002, "end": 4043.76, "text": " Okay, actually, this whole thing can be just summarized as neural symbolic reasoning because", "tokens": [50832, 1033, 11, 767, 11, 341, 1379, 551, 393, 312, 445, 14611, 1602, 382, 18161, 25755, 21577, 570, 51008], "temperature": 0.0, "avg_logprob": -0.16963111877441406, "compression_ratio": 2.063063063063063, "no_speech_prob": 0.0016591809689998627}, {"id": 690, "seek": 403088, "start": 4046.0, "end": 4051.92, "text": " plus the knowledge graph equals the symbol spot. And then the large language model is the neural", "tokens": [51120, 1804, 264, 3601, 4295, 6915, 264, 5986, 4008, 13, 400, 550, 264, 2416, 2856, 2316, 307, 264, 18161, 51416], "temperature": 0.0, "avg_logprob": -0.16963111877441406, "compression_ratio": 2.063063063063063, "no_speech_prob": 0.0016591809689998627}, {"id": 691, "seek": 403088, "start": 4051.92, "end": 4056.8, "text": " networks. So you can just summarize this whole thing as neural symbolic reasoning. All right,", "tokens": [51416, 9590, 13, 407, 291, 393, 445, 20858, 341, 1379, 551, 382, 18161, 25755, 21577, 13, 1057, 558, 11, 51660], "temperature": 0.0, "avg_logprob": -0.16963111877441406, "compression_ratio": 2.063063063063063, "no_speech_prob": 0.0016591809689998627}, {"id": 692, "seek": 405680, "start": 4056.88, "end": 4063.2000000000003, "text": " then you can use this for different domains. Right, I think this is a very, very exciting path", "tokens": [50368, 550, 291, 393, 764, 341, 337, 819, 25514, 13, 1779, 11, 286, 519, 341, 307, 257, 588, 11, 588, 4670, 3100, 50684], "temperature": 0.0, "avg_logprob": -0.09632433445081798, "compression_ratio": 1.859437751004016, "no_speech_prob": 0.001285751350224018}, {"id": 693, "seek": 405680, "start": 4063.2000000000003, "end": 4068.96, "text": " that we should work on. Because right now with the power of language models, the knowledge graph", "tokens": [50684, 300, 321, 820, 589, 322, 13, 1436, 558, 586, 365, 264, 1347, 295, 2856, 5245, 11, 264, 3601, 4295, 50972], "temperature": 0.0, "avg_logprob": -0.09632433445081798, "compression_ratio": 1.859437751004016, "no_speech_prob": 0.001285751350224018}, {"id": 694, "seek": 405680, "start": 4068.96, "end": 4074.5600000000004, "text": " can be very, very flexible. And it's not a typical knowledge graph anymore, it can be", "tokens": [50972, 393, 312, 588, 11, 588, 11358, 13, 400, 309, 311, 406, 257, 7476, 3601, 4295, 3602, 11, 309, 393, 312, 51252], "temperature": 0.0, "avg_logprob": -0.09632433445081798, "compression_ratio": 1.859437751004016, "no_speech_prob": 0.001285751350224018}, {"id": 695, "seek": 405680, "start": 4074.5600000000004, "end": 4080.0, "text": " embedding based knowledge graph. And it can be context dependent knowledge graph. Okay, so", "tokens": [51252, 12240, 3584, 2361, 3601, 4295, 13, 400, 309, 393, 312, 4319, 12334, 3601, 4295, 13, 1033, 11, 370, 51524], "temperature": 0.0, "avg_logprob": -0.09632433445081798, "compression_ratio": 1.859437751004016, "no_speech_prob": 0.001285751350224018}, {"id": 696, "seek": 405680, "start": 4080.7200000000003, "end": 4084.7200000000003, "text": " I really hope to work on context dependent knowledge graph, because I think that's the future.", "tokens": [51560, 286, 534, 1454, 281, 589, 322, 4319, 12334, 3601, 4295, 11, 570, 286, 519, 300, 311, 264, 2027, 13, 51760], "temperature": 0.0, "avg_logprob": -0.09632433445081798, "compression_ratio": 1.859437751004016, "no_speech_prob": 0.001285751350224018}, {"id": 697, "seek": 408472, "start": 4084.72, "end": 4088.64, "text": " Okay, not the traditional knowledge graph that you've seen everywhere in this presentation.", "tokens": [50364, 1033, 11, 406, 264, 5164, 3601, 4295, 300, 291, 600, 1612, 5315, 294, 341, 5860, 13, 50560], "temperature": 0.0, "avg_logprob": -0.13132757129091205, "compression_ratio": 1.856164383561644, "no_speech_prob": 0.0014631404774263501}, {"id": 698, "seek": 408472, "start": 4089.2799999999997, "end": 4093.2, "text": " Okay, the knowledge graph embeddings must be able to do, must be able to", "tokens": [50592, 1033, 11, 264, 3601, 4295, 12240, 29432, 1633, 312, 1075, 281, 360, 11, 1633, 312, 1075, 281, 50788], "temperature": 0.0, "avg_logprob": -0.13132757129091205, "compression_ratio": 1.856164383561644, "no_speech_prob": 0.0014631404774263501}, {"id": 699, "seek": 408472, "start": 4094.16, "end": 4098.32, "text": " change based on the parent nodes. Okay, must be changed based on the context. And that's", "tokens": [50836, 1319, 2361, 322, 264, 2596, 13891, 13, 1033, 11, 1633, 312, 3105, 2361, 322, 264, 4319, 13, 400, 300, 311, 51044], "temperature": 0.0, "avg_logprob": -0.13132757129091205, "compression_ratio": 1.856164383561644, "no_speech_prob": 0.0014631404774263501}, {"id": 700, "seek": 408472, "start": 4098.32, "end": 4101.92, "text": " something that is not done right now, at least based on my own awareness. I don't think that's", "tokens": [51044, 746, 300, 307, 406, 1096, 558, 586, 11, 412, 1935, 2361, 322, 452, 1065, 8888, 13, 286, 500, 380, 519, 300, 311, 51224], "temperature": 0.0, "avg_logprob": -0.13132757129091205, "compression_ratio": 1.856164383561644, "no_speech_prob": 0.0014631404774263501}, {"id": 701, "seek": 408472, "start": 4101.92, "end": 4108.24, "text": " done right now. But that's very promising. Right, so one use case for this kind of system is fact", "tokens": [51224, 1096, 558, 586, 13, 583, 300, 311, 588, 20257, 13, 1779, 11, 370, 472, 764, 1389, 337, 341, 733, 295, 1185, 307, 1186, 51540], "temperature": 0.0, "avg_logprob": -0.13132757129091205, "compression_ratio": 1.856164383561644, "no_speech_prob": 0.0014631404774263501}, {"id": 702, "seek": 408472, "start": 4108.24, "end": 4113.76, "text": " checking. As you know, large language models cannot do very badly at fact checking. It tends to", "tokens": [51540, 8568, 13, 1018, 291, 458, 11, 2416, 2856, 5245, 2644, 360, 588, 13425, 412, 1186, 8568, 13, 467, 12258, 281, 51816], "temperature": 0.0, "avg_logprob": -0.13132757129091205, "compression_ratio": 1.856164383561644, "no_speech_prob": 0.0014631404774263501}, {"id": 703, "seek": 411376, "start": 4113.76, "end": 4120.16, "text": " hallucinate a lot. And perhaps we can do like, a knowledge graph to like ground it in some facts", "tokens": [50364, 35212, 13923, 257, 688, 13, 400, 4317, 321, 393, 360, 411, 11, 257, 3601, 4295, 281, 411, 2727, 309, 294, 512, 9130, 50684], "temperature": 0.0, "avg_logprob": -0.1417913738049959, "compression_ratio": 1.7834101382488479, "no_speech_prob": 0.0029129337053745985}, {"id": 704, "seek": 411376, "start": 4120.16, "end": 4128.24, "text": " like some Wikipedia entries. No, you can use this to ground the inference. Okay, by doing inference,", "tokens": [50684, 411, 512, 28999, 23041, 13, 883, 11, 291, 393, 764, 341, 281, 2727, 264, 38253, 13, 1033, 11, 538, 884, 38253, 11, 51088], "temperature": 0.0, "avg_logprob": -0.1417913738049959, "compression_ratio": 1.7834101382488479, "no_speech_prob": 0.0029129337053745985}, {"id": 705, "seek": 411376, "start": 4128.24, "end": 4133.360000000001, "text": " you can then see whether or not like, is it, is there a path in the knowledge graph that matches", "tokens": [51088, 291, 393, 550, 536, 1968, 420, 406, 411, 11, 307, 309, 11, 307, 456, 257, 3100, 294, 264, 3601, 4295, 300, 10676, 51344], "temperature": 0.0, "avg_logprob": -0.1417913738049959, "compression_ratio": 1.7834101382488479, "no_speech_prob": 0.0029129337053745985}, {"id": 706, "seek": 411376, "start": 4133.360000000001, "end": 4139.76, "text": " it? Or you do knowledge grounded inference, like you say, you must only use this information", "tokens": [51344, 309, 30, 1610, 291, 360, 3601, 23535, 38253, 11, 411, 291, 584, 11, 291, 1633, 787, 764, 341, 1589, 51664], "temperature": 0.0, "avg_logprob": -0.1417913738049959, "compression_ratio": 1.7834101382488479, "no_speech_prob": 0.0029129337053745985}, {"id": 707, "seek": 413976, "start": 4139.76, "end": 4144.88, "text": " that I extract for you in the knowledge graph and infer it. So this diagram here, unfortunately,", "tokens": [50364, 300, 286, 8947, 337, 291, 294, 264, 3601, 4295, 293, 13596, 309, 13, 407, 341, 10686, 510, 11, 7015, 11, 50620], "temperature": 0.0, "avg_logprob": -0.1752963522206182, "compression_ratio": 1.8625954198473282, "no_speech_prob": 0.0028392132371664047}, {"id": 708, "seek": 413976, "start": 4144.88, "end": 4149.92, "text": " did not do the inference step. Okay, because they are still using birds. Okay, they're using bird as", "tokens": [50620, 630, 406, 360, 264, 38253, 1823, 13, 1033, 11, 570, 436, 366, 920, 1228, 9009, 13, 1033, 11, 436, 434, 1228, 5255, 382, 50872], "temperature": 0.0, "avg_logprob": -0.1752963522206182, "compression_ratio": 1.8625954198473282, "no_speech_prob": 0.0028392132371664047}, {"id": 709, "seek": 413976, "start": 4149.92, "end": 4157.84, "text": " a model. And what they did was they use the knowledge graph relations to do some pre training.", "tokens": [50872, 257, 2316, 13, 400, 437, 436, 630, 390, 436, 764, 264, 3601, 4295, 2299, 281, 360, 512, 659, 3097, 13, 51268], "temperature": 0.0, "avg_logprob": -0.1752963522206182, "compression_ratio": 1.8625954198473282, "no_speech_prob": 0.0028392132371664047}, {"id": 710, "seek": 413976, "start": 4157.84, "end": 4164.08, "text": " So it's like they take additional, like, additional tech samples, they just mask out certain words", "tokens": [51268, 407, 309, 311, 411, 436, 747, 4497, 11, 411, 11, 4497, 7553, 10938, 11, 436, 445, 6094, 484, 1629, 2283, 51580], "temperature": 0.0, "avg_logprob": -0.1752963522206182, "compression_ratio": 1.8625954198473282, "no_speech_prob": 0.0028392132371664047}, {"id": 711, "seek": 413976, "start": 4164.08, "end": 4168.88, "text": " based on the knowledge graph relations. And then they do the training here. So they just did the", "tokens": [51580, 2361, 322, 264, 3601, 4295, 2299, 13, 400, 550, 436, 360, 264, 3097, 510, 13, 407, 436, 445, 630, 264, 51820], "temperature": 0.0, "avg_logprob": -0.1752963522206182, "compression_ratio": 1.8625954198473282, "no_speech_prob": 0.0028392132371664047}, {"id": 712, "seek": 416888, "start": 4168.88, "end": 4175.36, "text": " pre training using the knowledge graph to give additional examples. Okay, so what I want the", "tokens": [50364, 659, 3097, 1228, 264, 3601, 4295, 281, 976, 4497, 5110, 13, 1033, 11, 370, 437, 286, 528, 264, 50688], "temperature": 0.0, "avg_logprob": -0.12437470205898943, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.002082816092297435}, {"id": 713, "seek": 416888, "start": 4175.36, "end": 4181.28, "text": " thing to do is actually to do it during inference, if I cannot find any paper that does that so far.", "tokens": [50688, 551, 281, 360, 307, 767, 281, 360, 309, 1830, 38253, 11, 498, 286, 2644, 915, 604, 3035, 300, 775, 300, 370, 1400, 13, 50984], "temperature": 0.0, "avg_logprob": -0.12437470205898943, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.002082816092297435}, {"id": 714, "seek": 416888, "start": 4181.28, "end": 4185.4400000000005, "text": " All right. So I think this inference is more important than the pre training, you know,", "tokens": [50984, 1057, 558, 13, 407, 286, 519, 341, 38253, 307, 544, 1021, 813, 264, 659, 3097, 11, 291, 458, 11, 51192], "temperature": 0.0, "avg_logprob": -0.12437470205898943, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.002082816092297435}, {"id": 715, "seek": 416888, "start": 4185.4400000000005, "end": 4189.36, "text": " this pre training, yes, it increases more data samples, because you can just mix and match the", "tokens": [51192, 341, 659, 3097, 11, 2086, 11, 309, 8637, 544, 1412, 10938, 11, 570, 291, 393, 445, 2890, 293, 2995, 264, 51388], "temperature": 0.0, "avg_logprob": -0.12437470205898943, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.002082816092297435}, {"id": 716, "seek": 416888, "start": 4189.36, "end": 4194.24, "text": " knowledge graph, get more sentences out. Sure, I give it to you. And in fact, they improve by two", "tokens": [51388, 3601, 4295, 11, 483, 544, 16579, 484, 13, 4894, 11, 286, 976, 309, 281, 291, 13, 400, 294, 1186, 11, 436, 3470, 538, 732, 51632], "temperature": 0.0, "avg_logprob": -0.12437470205898943, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.002082816092297435}, {"id": 717, "seek": 419424, "start": 4194.24, "end": 4199.04, "text": " to three percentage points across soda benchmarks, this fact KB, you can go and check it out.", "tokens": [50364, 281, 1045, 9668, 2793, 2108, 17192, 43751, 11, 341, 1186, 591, 33, 11, 291, 393, 352, 293, 1520, 309, 484, 13, 50604], "temperature": 0.0, "avg_logprob": -0.16274563471476236, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.005774209741503}, {"id": 718, "seek": 419424, "start": 4199.679999999999, "end": 4203.92, "text": " All right. But what I'm more interested in is how you use it for inference, not for pre training.", "tokens": [50636, 1057, 558, 13, 583, 437, 286, 478, 544, 3102, 294, 307, 577, 291, 764, 309, 337, 38253, 11, 406, 337, 659, 3097, 13, 50848], "temperature": 0.0, "avg_logprob": -0.16274563471476236, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.005774209741503}, {"id": 719, "seek": 419424, "start": 4204.5599999999995, "end": 4209.679999999999, "text": " Okay, so let's see how length chain does it. All right, so now we come to the length chain part.", "tokens": [50880, 1033, 11, 370, 718, 311, 536, 577, 4641, 5021, 775, 309, 13, 1057, 558, 11, 370, 586, 321, 808, 281, 264, 4641, 5021, 644, 13, 51136], "temperature": 0.0, "avg_logprob": -0.16274563471476236, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.005774209741503}, {"id": 720, "seek": 419424, "start": 4209.679999999999, "end": 4213.36, "text": " So actually, length chain is quite advanced, because length chain has a lot of the ideas that I", "tokens": [51136, 407, 767, 11, 4641, 5021, 307, 1596, 7339, 11, 570, 4641, 5021, 575, 257, 688, 295, 264, 3487, 300, 286, 51320], "temperature": 0.0, "avg_logprob": -0.16274563471476236, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.005774209741503}, {"id": 721, "seek": 419424, "start": 4213.36, "end": 4218.4, "text": " think should be done. All right, let's see how the length chain question answering graph question", "tokens": [51320, 519, 820, 312, 1096, 13, 1057, 558, 11, 718, 311, 536, 577, 264, 4641, 5021, 1168, 13430, 4295, 1168, 51572], "temperature": 0.0, "avg_logprob": -0.16274563471476236, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.005774209741503}, {"id": 722, "seek": 421840, "start": 4218.4, "end": 4223.5199999999995, "text": " answering is done. All right, so we have four steps. First step, we generate the triples from", "tokens": [50364, 13430, 307, 1096, 13, 1057, 558, 11, 370, 321, 362, 1451, 4439, 13, 2386, 1823, 11, 321, 8460, 264, 1376, 2622, 490, 50620], "temperature": 0.0, "avg_logprob": -0.18175892869965368, "compression_ratio": 2.016949152542373, "no_speech_prob": 0.009895392693579197}, {"id": 723, "seek": 421840, "start": 4223.5199999999995, "end": 4227.759999999999, "text": " the context. Okay, so we are like maybe a text context, you generate like the triples from it,", "tokens": [50620, 264, 4319, 13, 1033, 11, 370, 321, 366, 411, 1310, 257, 2487, 4319, 11, 291, 8460, 411, 264, 1376, 2622, 490, 309, 11, 50832], "temperature": 0.0, "avg_logprob": -0.18175892869965368, "compression_ratio": 2.016949152542373, "no_speech_prob": 0.009895392693579197}, {"id": 724, "seek": 421840, "start": 4228.4, "end": 4233.759999999999, "text": " like the knowledge graph triples, you generate some from the query, you generate some entity", "tokens": [50864, 411, 264, 3601, 4295, 1376, 2622, 11, 291, 8460, 512, 490, 264, 14581, 11, 291, 8460, 512, 13977, 51132], "temperature": 0.0, "avg_logprob": -0.18175892869965368, "compression_ratio": 2.016949152542373, "no_speech_prob": 0.009895392693579197}, {"id": 725, "seek": 421840, "start": 4233.759999999999, "end": 4239.2, "text": " extraction. Okay, and then you use this at that entities to extract this relevant triples. Okay,", "tokens": [51132, 30197, 13, 1033, 11, 293, 550, 291, 764, 341, 412, 300, 16667, 281, 8947, 341, 7340, 1376, 2622, 13, 1033, 11, 51404], "temperature": 0.0, "avg_logprob": -0.18175892869965368, "compression_ratio": 2.016949152542373, "no_speech_prob": 0.009895392693579197}, {"id": 726, "seek": 421840, "start": 4239.2, "end": 4244.4, "text": " later I'll show you the how what I mean by this. And then you use this relevant triples to answer", "tokens": [51404, 1780, 286, 603, 855, 291, 264, 577, 437, 286, 914, 538, 341, 13, 400, 550, 291, 764, 341, 7340, 1376, 2622, 281, 1867, 51664], "temperature": 0.0, "avg_logprob": -0.18175892869965368, "compression_ratio": 2.016949152542373, "no_speech_prob": 0.009895392693579197}, {"id": 727, "seek": 424440, "start": 4244.4, "end": 4248.639999999999, "text": " the question. So I share with you these two documentation in case you want to see how length", "tokens": [50364, 264, 1168, 13, 407, 286, 2073, 365, 291, 613, 732, 14333, 294, 1389, 291, 528, 281, 536, 577, 4641, 50576], "temperature": 0.0, "avg_logprob": -0.1978261350381254, "compression_ratio": 1.582995951417004, "no_speech_prob": 0.0025834678672254086}, {"id": 728, "seek": 424440, "start": 4248.639999999999, "end": 4257.36, "text": " chain graph QA does it. So step one, okay, generate triples from context. So like this context, I just", "tokens": [50576, 5021, 4295, 1249, 32, 775, 309, 13, 407, 1823, 472, 11, 1392, 11, 8460, 1376, 2622, 490, 4319, 13, 407, 411, 341, 4319, 11, 286, 445, 51012], "temperature": 0.0, "avg_logprob": -0.1978261350381254, "compression_ratio": 1.582995951417004, "no_speech_prob": 0.0025834678672254086}, {"id": 729, "seek": 424440, "start": 4257.36, "end": 4263.759999999999, "text": " came out of it, right? Recently, my MacBook external camera in the viewing camera spot. So I'm", "tokens": [51012, 1361, 484, 295, 309, 11, 558, 30, 20072, 11, 452, 31737, 8320, 2799, 294, 264, 17480, 2799, 4008, 13, 407, 286, 478, 51332], "temperature": 0.0, "avg_logprob": -0.1978261350381254, "compression_ratio": 1.582995951417004, "no_speech_prob": 0.0025834678672254086}, {"id": 730, "seek": 424440, "start": 4263.759999999999, "end": 4269.759999999999, "text": " actually using the external camera right now to talk to you. And yeah, so this example is for Apple.", "tokens": [51332, 767, 1228, 264, 8320, 2799, 558, 586, 281, 751, 281, 291, 13, 400, 1338, 11, 370, 341, 1365, 307, 337, 6373, 13, 51632], "temperature": 0.0, "avg_logprob": -0.1978261350381254, "compression_ratio": 1.582995951417004, "no_speech_prob": 0.0025834678672254086}, {"id": 731, "seek": 426976, "start": 4269.76, "end": 4274.96, "text": " So let's assume that Apple created a new product called Mac and Cheese Pro, okay, in 2025. All", "tokens": [50364, 407, 718, 311, 6552, 300, 6373, 2942, 257, 777, 1674, 1219, 5707, 293, 23738, 1705, 11, 1392, 11, 294, 39209, 13, 1057, 50624], "temperature": 0.0, "avg_logprob": -0.18235658777171168, "compression_ratio": 1.7798507462686568, "no_speech_prob": 0.0057580978609621525}, {"id": 732, "seek": 426976, "start": 4274.96, "end": 4279.6, "text": " right, and then like Apple gave the invented cheese, okay, a rousing ovation in 2026 after", "tokens": [50624, 558, 11, 293, 550, 411, 6373, 2729, 264, 14479, 5399, 11, 1392, 11, 257, 367, 24220, 277, 11116, 294, 945, 10880, 934, 50856], "temperature": 0.0, "avg_logprob": -0.18235658777171168, "compression_ratio": 1.7798507462686568, "no_speech_prob": 0.0057580978609621525}, {"id": 733, "seek": 426976, "start": 4279.6, "end": 4284.16, "text": " invented this in 2024. Right, there's also another company called Orange who created a competing", "tokens": [50856, 14479, 341, 294, 45237, 13, 1779, 11, 456, 311, 611, 1071, 2237, 1219, 17106, 567, 2942, 257, 15439, 51084], "temperature": 0.0, "avg_logprob": -0.18235658777171168, "compression_ratio": 1.7798507462686568, "no_speech_prob": 0.0057580978609621525}, {"id": 734, "seek": 426976, "start": 4284.16, "end": 4288.8, "text": " product called the Orange and Cheese Pro. The price was slightly higher at 5000 compared to 4000", "tokens": [51084, 1674, 1219, 264, 17106, 293, 23738, 1705, 13, 440, 3218, 390, 4748, 2946, 412, 23777, 5347, 281, 31104, 51316], "temperature": 0.0, "avg_logprob": -0.18235658777171168, "compression_ratio": 1.7798507462686568, "no_speech_prob": 0.0057580978609621525}, {"id": 735, "seek": 426976, "start": 4288.8, "end": 4294.88, "text": " from Apple. Okay, so this is a fictional example. Okay, and this is just to see like how good the", "tokens": [51316, 490, 6373, 13, 1033, 11, 370, 341, 307, 257, 28911, 1365, 13, 1033, 11, 293, 341, 307, 445, 281, 536, 411, 577, 665, 264, 51620], "temperature": 0.0, "avg_logprob": -0.18235658777171168, "compression_ratio": 1.7798507462686568, "no_speech_prob": 0.0057580978609621525}, {"id": 736, "seek": 429488, "start": 4294.96, "end": 4299.68, "text": " context is stored in the knowledge graph. So you can see that, oh yes, Apple announced Mac and Cheese", "tokens": [50368, 4319, 307, 12187, 294, 264, 3601, 4295, 13, 407, 291, 393, 536, 300, 11, 1954, 2086, 11, 6373, 7548, 5707, 293, 23738, 50604], "temperature": 0.0, "avg_logprob": -0.1596694105253445, "compression_ratio": 1.8115384615384615, "no_speech_prob": 0.004115938674658537}, {"id": 737, "seek": 429488, "start": 4299.68, "end": 4307.28, "text": " Pro, Apple gave cheese. So this kind of thing, right, like, is a bit contentious because like,", "tokens": [50604, 1705, 11, 6373, 2729, 5399, 13, 407, 341, 733, 295, 551, 11, 558, 11, 411, 11, 307, 257, 857, 2701, 851, 570, 411, 11, 50984], "temperature": 0.0, "avg_logprob": -0.1596694105253445, "compression_ratio": 1.8115384615384615, "no_speech_prob": 0.004115938674658537}, {"id": 738, "seek": 429488, "start": 4307.28, "end": 4311.52, "text": " what do you mean by gave cheese, gave what? So this one needs to be improved a bit.", "tokens": [50984, 437, 360, 291, 914, 538, 2729, 5399, 11, 2729, 437, 30, 407, 341, 472, 2203, 281, 312, 9689, 257, 857, 13, 51196], "temperature": 0.0, "avg_logprob": -0.1596694105253445, "compression_ratio": 1.8115384615384615, "no_speech_prob": 0.004115938674658537}, {"id": 739, "seek": 429488, "start": 4312.400000000001, "end": 4317.76, "text": " Apple ovation gave, Apple gave an ovation, okay, again to who? Right, so this one needs to be", "tokens": [51240, 6373, 277, 11116, 2729, 11, 6373, 2729, 364, 277, 11116, 11, 1392, 11, 797, 281, 567, 30, 1779, 11, 370, 341, 472, 2203, 281, 312, 51508], "temperature": 0.0, "avg_logprob": -0.1596694105253445, "compression_ratio": 1.8115384615384615, "no_speech_prob": 0.004115938674658537}, {"id": 740, "seek": 429488, "start": 4317.76, "end": 4323.04, "text": " improved as well. Okay, the price of the MacBook Pro is 4000. Yes, Mac and Cheese Pro is already", "tokens": [51508, 9689, 382, 731, 13, 1033, 11, 264, 3218, 295, 264, 31737, 1705, 307, 31104, 13, 1079, 11, 5707, 293, 23738, 1705, 307, 1217, 51772], "temperature": 0.0, "avg_logprob": -0.1596694105253445, "compression_ratio": 1.8115384615384615, "no_speech_prob": 0.004115938674658537}, {"id": 741, "seek": 432304, "start": 4323.12, "end": 4329.12, "text": " created. Orange and Cheese Pro, good. Orange and Cheese Pro, the price 5000. Okay, so you see,", "tokens": [50368, 2942, 13, 17106, 293, 23738, 1705, 11, 665, 13, 17106, 293, 23738, 1705, 11, 264, 3218, 23777, 13, 1033, 11, 370, 291, 536, 11, 50668], "temperature": 0.0, "avg_logprob": -0.15575869381427765, "compression_ratio": 1.6618705035971224, "no_speech_prob": 0.0022149989381432533}, {"id": 742, "seek": 432304, "start": 4329.12, "end": 4336.0, "text": " it's not bad. Miss out dates. All right, and then like some, some relations are ambiguous.", "tokens": [50668, 309, 311, 406, 1578, 13, 5275, 484, 11691, 13, 1057, 558, 11, 293, 550, 411, 512, 11, 512, 2299, 366, 39465, 13, 51012], "temperature": 0.0, "avg_logprob": -0.15575869381427765, "compression_ratio": 1.6618705035971224, "no_speech_prob": 0.0022149989381432533}, {"id": 743, "seek": 432304, "start": 4336.88, "end": 4341.28, "text": " So I don't quite like the way they did the triplet extraction and I think this is the", "tokens": [51056, 407, 286, 500, 380, 1596, 411, 264, 636, 436, 630, 264, 1376, 14657, 30197, 293, 286, 519, 341, 307, 264, 51276], "temperature": 0.0, "avg_logprob": -0.15575869381427765, "compression_ratio": 1.6618705035971224, "no_speech_prob": 0.0022149989381432533}, {"id": 744, "seek": 432304, "start": 4341.28, "end": 4345.92, "text": " downfall of the Graph QA. So if you are going to use Lang chain for Graph QA, my advice is don't", "tokens": [51276, 760, 6691, 295, 264, 21884, 1249, 32, 13, 407, 498, 291, 366, 516, 281, 764, 13313, 5021, 337, 21884, 1249, 32, 11, 452, 5192, 307, 500, 380, 51508], "temperature": 0.0, "avg_logprob": -0.15575869381427765, "compression_ratio": 1.6618705035971224, "no_speech_prob": 0.0022149989381432533}, {"id": 745, "seek": 432304, "start": 4345.92, "end": 4350.8, "text": " use it. Okay, because you miss out a lot of stuff in the context. Okay, if you are interested", "tokens": [51508, 764, 309, 13, 1033, 11, 570, 291, 1713, 484, 257, 688, 295, 1507, 294, 264, 4319, 13, 1033, 11, 498, 291, 366, 3102, 51752], "temperature": 0.0, "avg_logprob": -0.15575869381427765, "compression_ratio": 1.6618705035971224, "no_speech_prob": 0.0022149989381432533}, {"id": 746, "seek": 435080, "start": 4350.8, "end": 4354.96, "text": " how they generate the context, you can go back to my earlier slides that I was talking about.", "tokens": [50364, 577, 436, 8460, 264, 4319, 11, 291, 393, 352, 646, 281, 452, 3071, 9788, 300, 286, 390, 1417, 466, 13, 50572], "temperature": 0.0, "avg_logprob": -0.178501410562484, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.0036068151239305735}, {"id": 747, "seek": 435080, "start": 4355.68, "end": 4360.24, "text": " Yeah, so I mean, it's actually, let me show you, let me show you again the slides.", "tokens": [50608, 865, 11, 370, 286, 914, 11, 309, 311, 767, 11, 718, 385, 855, 291, 11, 718, 385, 855, 291, 797, 264, 9788, 13, 50836], "temperature": 0.0, "avg_logprob": -0.178501410562484, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.0036068151239305735}, {"id": 748, "seek": 435080, "start": 4361.28, "end": 4366.16, "text": " It's this one, this is the one that they did, like, this is the problem to generate stuff from the", "tokens": [50888, 467, 311, 341, 472, 11, 341, 307, 264, 472, 300, 436, 630, 11, 411, 11, 341, 307, 264, 1154, 281, 8460, 1507, 490, 264, 51132], "temperature": 0.0, "avg_logprob": -0.178501410562484, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.0036068151239305735}, {"id": 749, "seek": 435080, "start": 4366.16, "end": 4372.64, "text": " text. Yeah, so the examples aren't very great and understandably the results aren't very great", "tokens": [51132, 2487, 13, 865, 11, 370, 264, 5110, 3212, 380, 588, 869, 293, 1223, 1188, 264, 3542, 3212, 380, 588, 869, 51456], "temperature": 0.0, "avg_logprob": -0.178501410562484, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.0036068151239305735}, {"id": 750, "seek": 435080, "start": 4372.64, "end": 4377.4400000000005, "text": " as well. All right, so this is the knowledge graph that's generated. You can see like Mac and Cheese", "tokens": [51456, 382, 731, 13, 1057, 558, 11, 370, 341, 307, 264, 3601, 4295, 300, 311, 10833, 13, 509, 393, 536, 411, 5707, 293, 23738, 51696], "temperature": 0.0, "avg_logprob": -0.178501410562484, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.0036068151239305735}, {"id": 751, "seek": 437744, "start": 4377.44, "end": 4384.4, "text": " Pro is cost $4,000 on price. Yeah, you can see that like stuff like price will contain like a lot", "tokens": [50364, 1705, 307, 2063, 1848, 19, 11, 1360, 322, 3218, 13, 865, 11, 291, 393, 536, 300, 411, 1507, 411, 3218, 486, 5304, 411, 257, 688, 50712], "temperature": 0.0, "avg_logprob": -0.1385942014675696, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.0023272319231182337}, {"id": 752, "seek": 437744, "start": 4384.4, "end": 4391.759999999999, "text": " of relation because like price is very generic. Okay, Apple announced Mac and Cheese Pro. Okay,", "tokens": [50712, 295, 9721, 570, 411, 3218, 307, 588, 19577, 13, 1033, 11, 6373, 7548, 5707, 293, 23738, 1705, 13, 1033, 11, 51080], "temperature": 0.0, "avg_logprob": -0.1385942014675696, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.0023272319231182337}, {"id": 753, "seek": 437744, "start": 4391.759999999999, "end": 4398.24, "text": " so this is the knowledge graph that is generated. And we can see that like, next up we can use the", "tokens": [51080, 370, 341, 307, 264, 3601, 4295, 300, 307, 10833, 13, 400, 321, 393, 536, 300, 411, 11, 958, 493, 321, 393, 764, 264, 51404], "temperature": 0.0, "avg_logprob": -0.1385942014675696, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.0023272319231182337}, {"id": 754, "seek": 437744, "start": 4398.24, "end": 4404.16, "text": " Graph QA chain in order to run the chain and see the answer. And you can see that if I ask it the", "tokens": [51404, 21884, 1249, 32, 5021, 294, 1668, 281, 1190, 264, 5021, 293, 536, 264, 1867, 13, 400, 291, 393, 536, 300, 498, 286, 1029, 309, 264, 51700], "temperature": 0.0, "avg_logprob": -0.1385942014675696, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.0023272319231182337}, {"id": 755, "seek": 440416, "start": 4404.16, "end": 4409.04, "text": " question like, when was the Mac and Cheese Pro announced? Okay, they couldn't find it. Okay,", "tokens": [50364, 1168, 411, 11, 562, 390, 264, 5707, 293, 23738, 1705, 7548, 30, 1033, 11, 436, 2809, 380, 915, 309, 13, 1033, 11, 50608], "temperature": 0.0, "avg_logprob": -0.11807301948810446, "compression_ratio": 1.979253112033195, "no_speech_prob": 0.0013485002564266324}, {"id": 756, "seek": 440416, "start": 4409.04, "end": 4413.84, "text": " because after they passed through the context, okay, they abstract like when was the Mac and", "tokens": [50608, 570, 934, 436, 4678, 807, 264, 4319, 11, 1392, 11, 436, 12649, 411, 562, 390, 264, 5707, 293, 50848], "temperature": 0.0, "avg_logprob": -0.11807301948810446, "compression_ratio": 1.979253112033195, "no_speech_prob": 0.0013485002564266324}, {"id": 757, "seek": 440416, "start": 4413.84, "end": 4417.599999999999, "text": " Cheese Pro, when did Apple announce the Mac and Cheese Pro? They abstract that in the query,", "tokens": [50848, 23738, 1705, 11, 562, 630, 6373, 7478, 264, 5707, 293, 23738, 1705, 30, 814, 12649, 300, 294, 264, 14581, 11, 51036], "temperature": 0.0, "avg_logprob": -0.11807301948810446, "compression_ratio": 1.979253112033195, "no_speech_prob": 0.0013485002564266324}, {"id": 758, "seek": 440416, "start": 4417.599999999999, "end": 4422.4, "text": " there's only Apple and Mac and Cheese Pro. So they check through all the knowledge graph to make sure", "tokens": [51036, 456, 311, 787, 6373, 293, 5707, 293, 23738, 1705, 13, 407, 436, 1520, 807, 439, 264, 3601, 4295, 281, 652, 988, 51276], "temperature": 0.0, "avg_logprob": -0.11807301948810446, "compression_ratio": 1.979253112033195, "no_speech_prob": 0.0013485002564266324}, {"id": 759, "seek": 440416, "start": 4422.4, "end": 4428.08, "text": " that you only have entities that match Apple or Mac and Cheese Pro. Okay, so I have a gripe with", "tokens": [51276, 300, 291, 787, 362, 16667, 300, 2995, 6373, 420, 5707, 293, 23738, 1705, 13, 1033, 11, 370, 286, 362, 257, 17865, 494, 365, 51560], "temperature": 0.0, "avg_logprob": -0.11807301948810446, "compression_ratio": 1.979253112033195, "no_speech_prob": 0.0013485002564266324}, {"id": 760, "seek": 442808, "start": 4428.08, "end": 4439.28, "text": " this thing. Like if you use exact text matching, what if there's a spelling error, capitalization", "tokens": [50364, 341, 551, 13, 1743, 498, 291, 764, 1900, 2487, 14324, 11, 437, 498, 456, 311, 257, 22254, 6713, 11, 4238, 2144, 50924], "temperature": 0.0, "avg_logprob": -0.15451800517546824, "compression_ratio": 1.6878612716763006, "no_speech_prob": 0.008897417224943638}, {"id": 761, "seek": 442808, "start": 4439.28, "end": 4449.36, "text": " error, or like related word, but not exact match. Yeah, so if you use exact text matching, which", "tokens": [50924, 6713, 11, 420, 411, 4077, 1349, 11, 457, 406, 1900, 2995, 13, 865, 11, 370, 498, 291, 764, 1900, 2487, 14324, 11, 597, 51428], "temperature": 0.0, "avg_logprob": -0.15451800517546824, "compression_ratio": 1.6878612716763006, "no_speech_prob": 0.008897417224943638}, {"id": 762, "seek": 442808, "start": 4449.36, "end": 4454.88, "text": " is what they did for a length chain, like what if you don't get the right match? Okay, so I don't", "tokens": [51428, 307, 437, 436, 630, 337, 257, 4641, 5021, 11, 411, 437, 498, 291, 500, 380, 483, 264, 558, 2995, 30, 1033, 11, 370, 286, 500, 380, 51704], "temperature": 0.0, "avg_logprob": -0.15451800517546824, "compression_ratio": 1.6878612716763006, "no_speech_prob": 0.008897417224943638}, {"id": 763, "seek": 445488, "start": 4454.88, "end": 4461.04, "text": " quite like this approach. So yeah, this is something that I think could be improved on.", "tokens": [50364, 1596, 411, 341, 3109, 13, 407, 1338, 11, 341, 307, 746, 300, 286, 519, 727, 312, 9689, 322, 13, 50672], "temperature": 0.0, "avg_logprob": -0.1319523104305925, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.0025344863533973694}, {"id": 764, "seek": 445488, "start": 4461.04, "end": 4465.84, "text": " All right, and you can see that if I ask it like, when did they announce the Mac and Cheese Pro?", "tokens": [50672, 1057, 558, 11, 293, 291, 393, 536, 300, 498, 286, 1029, 309, 411, 11, 562, 630, 436, 7478, 264, 5707, 293, 23738, 1705, 30, 50912], "temperature": 0.0, "avg_logprob": -0.1319523104305925, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.0025344863533973694}, {"id": 765, "seek": 445488, "start": 4466.64, "end": 4470.96, "text": " They couldn't answer. All right, because look at this knowledge graph here, there's nothing that", "tokens": [50952, 814, 2809, 380, 1867, 13, 1057, 558, 11, 570, 574, 412, 341, 3601, 4295, 510, 11, 456, 311, 1825, 300, 51168], "temperature": 0.0, "avg_logprob": -0.1319523104305925, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.0025344863533973694}, {"id": 766, "seek": 445488, "start": 4470.96, "end": 4475.2, "text": " talks about dates here. All right, so they miss out quite a huge chunk of information from the", "tokens": [51168, 6686, 466, 11691, 510, 13, 1057, 558, 11, 370, 436, 1713, 484, 1596, 257, 2603, 16635, 295, 1589, 490, 264, 51380], "temperature": 0.0, "avg_logprob": -0.1319523104305925, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.0025344863533973694}, {"id": 767, "seek": 445488, "start": 4475.2, "end": 4481.4400000000005, "text": " earlier context. So if we had fed in the earlier context directly, so I just use the length chain", "tokens": [51380, 3071, 4319, 13, 407, 498, 321, 632, 4636, 294, 264, 3071, 4319, 3838, 11, 370, 286, 445, 764, 264, 4641, 5021, 51692], "temperature": 0.0, "avg_logprob": -0.1319523104305925, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.0025344863533973694}, {"id": 768, "seek": 448144, "start": 4482.4, "end": 4489.599999999999, "text": " LmChain agent. Okay, so I'm only using the LmChain agent or this to just show you that length", "tokens": [50412, 441, 76, 6546, 491, 9461, 13, 1033, 11, 370, 286, 478, 787, 1228, 264, 441, 76, 6546, 491, 9461, 420, 341, 281, 445, 855, 291, 300, 4641, 50772], "temperature": 0.0, "avg_logprob": -0.21676823651349103, "compression_ratio": 1.8650793650793651, "no_speech_prob": 0.0070596677251160145}, {"id": 769, "seek": 448144, "start": 4489.599999999999, "end": 4495.28, "text": " chain is not good. All right, I myself the new length chain. All right, so this is the idea", "tokens": [50772, 5021, 307, 406, 665, 13, 1057, 558, 11, 286, 2059, 264, 777, 4641, 5021, 13, 1057, 558, 11, 370, 341, 307, 264, 1558, 51056], "temperature": 0.0, "avg_logprob": -0.21676823651349103, "compression_ratio": 1.8650793650793651, "no_speech_prob": 0.0070596677251160145}, {"id": 770, "seek": 448144, "start": 4495.28, "end": 4500.719999999999, "text": " that like, after a while, you know, this is the context and then, okay, so this is not bad. I", "tokens": [51056, 300, 411, 11, 934, 257, 1339, 11, 291, 458, 11, 341, 307, 264, 4319, 293, 550, 11, 1392, 11, 370, 341, 307, 406, 1578, 13, 286, 51328], "temperature": 0.0, "avg_logprob": -0.21676823651349103, "compression_ratio": 1.8650793650793651, "no_speech_prob": 0.0070596677251160145}, {"id": 771, "seek": 448144, "start": 4500.719999999999, "end": 4504.0, "text": " mean, you could just do the same thing on ChatGBT, actually, you can just put like context", "tokens": [51328, 914, 11, 291, 727, 445, 360, 264, 912, 551, 322, 27503, 8769, 51, 11, 767, 11, 291, 393, 445, 829, 411, 4319, 51492], "temperature": 0.0, "avg_logprob": -0.21676823651349103, "compression_ratio": 1.8650793650793651, "no_speech_prob": 0.0070596677251160145}, {"id": 772, "seek": 448144, "start": 4504.0, "end": 4509.28, "text": " question and then ChatGBT will give you the answer. All right, so this LmChain works and this shows", "tokens": [51492, 1168, 293, 550, 27503, 8769, 51, 486, 976, 291, 264, 1867, 13, 1057, 558, 11, 370, 341, 441, 76, 6546, 491, 1985, 293, 341, 3110, 51756], "temperature": 0.0, "avg_logprob": -0.21676823651349103, "compression_ratio": 1.8650793650793651, "no_speech_prob": 0.0070596677251160145}, {"id": 773, "seek": 450928, "start": 4509.36, "end": 4514.719999999999, "text": " that by embedding the text as a knowledge graph, it kind of miss out certain stuff. All right,", "tokens": [50368, 300, 538, 12240, 3584, 264, 2487, 382, 257, 3601, 4295, 11, 309, 733, 295, 1713, 484, 1629, 1507, 13, 1057, 558, 11, 50636], "temperature": 0.0, "avg_logprob": -0.18259593963623047, "compression_ratio": 1.8186274509803921, "no_speech_prob": 0.001690417411737144}, {"id": 774, "seek": 450928, "start": 4514.719999999999, "end": 4519.84, "text": " and what are the stuff we miss out? We miss out the years and we also miss out like,", "tokens": [50636, 293, 437, 366, 264, 1507, 321, 1713, 484, 30, 492, 1713, 484, 264, 924, 293, 321, 611, 1713, 484, 411, 11, 50892], "temperature": 0.0, "avg_logprob": -0.18259593963623047, "compression_ratio": 1.8186274509803921, "no_speech_prob": 0.001690417411737144}, {"id": 775, "seek": 450928, "start": 4519.84, "end": 4523.759999999999, "text": " Apple gave cheese. I mean, it doesn't make sense that way, right? I mean, look at the knowledge", "tokens": [50892, 6373, 2729, 5399, 13, 286, 914, 11, 309, 1177, 380, 652, 2020, 300, 636, 11, 558, 30, 286, 914, 11, 574, 412, 264, 3601, 51088], "temperature": 0.0, "avg_logprob": -0.18259593963623047, "compression_ratio": 1.8186274509803921, "no_speech_prob": 0.001690417411737144}, {"id": 776, "seek": 450928, "start": 4523.759999999999, "end": 4531.5199999999995, "text": " graph like, what am I trying to solve there? Apple gave cheese. Where is it? Apple gave cheese.", "tokens": [51088, 4295, 411, 11, 437, 669, 286, 1382, 281, 5039, 456, 30, 6373, 2729, 5399, 13, 2305, 307, 309, 30, 6373, 2729, 5399, 13, 51476], "temperature": 0.0, "avg_logprob": -0.18259593963623047, "compression_ratio": 1.8186274509803921, "no_speech_prob": 0.001690417411737144}, {"id": 777, "seek": 453152, "start": 4531.52, "end": 4535.76, "text": " Where's cheese?", "tokens": [50364, 2305, 311, 5399, 30, 50576], "temperature": 0.0, "avg_logprob": -0.22302802871255314, "compression_ratio": 1.4970414201183433, "no_speech_prob": 0.002002280205488205}, {"id": 778, "seek": 453152, "start": 4539.280000000001, "end": 4541.76, "text": " Apple Cheese gave. Is there a gave anywhere?", "tokens": [50752, 6373, 23738, 2729, 13, 1119, 456, 257, 2729, 4992, 30, 50876], "temperature": 0.0, "avg_logprob": -0.22302802871255314, "compression_ratio": 1.4970414201183433, "no_speech_prob": 0.002002280205488205}, {"id": 779, "seek": 453152, "start": 4545.6, "end": 4550.64, "text": " Yeah, okay, I think this one maybe is the outdated, but the idea is that we can't really tell the", "tokens": [51068, 865, 11, 1392, 11, 286, 519, 341, 472, 1310, 307, 264, 36313, 11, 457, 264, 1558, 307, 300, 321, 393, 380, 534, 980, 264, 51320], "temperature": 0.0, "avg_logprob": -0.22302802871255314, "compression_ratio": 1.4970414201183433, "no_speech_prob": 0.002002280205488205}, {"id": 780, "seek": 453152, "start": 4551.360000000001, "end": 4558.0, "text": " main thing in this graph because we miss out some information. And that's one of the issues of", "tokens": [51356, 2135, 551, 294, 341, 4295, 570, 321, 1713, 484, 512, 1589, 13, 400, 300, 311, 472, 295, 264, 2663, 295, 51688], "temperature": 0.0, "avg_logprob": -0.22302802871255314, "compression_ratio": 1.4970414201183433, "no_speech_prob": 0.002002280205488205}, {"id": 781, "seek": 455800, "start": 4558.56, "end": 4562.56, "text": " converting text directly into knowledge graph is that you might miss out certain relations.", "tokens": [50392, 29942, 2487, 3838, 666, 3601, 4295, 307, 300, 291, 1062, 1713, 484, 1629, 2299, 13, 50592], "temperature": 0.0, "avg_logprob": -0.12528944452968213, "compression_ratio": 1.8, "no_speech_prob": 0.004594865255057812}, {"id": 782, "seek": 455800, "start": 4563.2, "end": 4566.4, "text": " And actually, if you think about it, if we want to embody all relations,", "tokens": [50624, 400, 767, 11, 498, 291, 519, 466, 309, 11, 498, 321, 528, 281, 42575, 439, 2299, 11, 50784], "temperature": 0.0, "avg_logprob": -0.12528944452968213, "compression_ratio": 1.8, "no_speech_prob": 0.004594865255057812}, {"id": 783, "seek": 455800, "start": 4567.2, "end": 4572.96, "text": " there's just too many to embody, right? Yeah, it's too big to embody. So maybe the text itself", "tokens": [50824, 456, 311, 445, 886, 867, 281, 42575, 11, 558, 30, 865, 11, 309, 311, 886, 955, 281, 42575, 13, 407, 1310, 264, 2487, 2564, 51112], "temperature": 0.0, "avg_logprob": -0.12528944452968213, "compression_ratio": 1.8, "no_speech_prob": 0.004594865255057812}, {"id": 784, "seek": 455800, "start": 4572.96, "end": 4577.76, "text": " is way more expressive than the knowledge graph, if you think about it that way. Okay,", "tokens": [51112, 307, 636, 544, 40189, 813, 264, 3601, 4295, 11, 498, 291, 519, 466, 309, 300, 636, 13, 1033, 11, 51352], "temperature": 0.0, "avg_logprob": -0.12528944452968213, "compression_ratio": 1.8, "no_speech_prob": 0.004594865255057812}, {"id": 785, "seek": 455800, "start": 4578.56, "end": 4582.4, "text": " but again, you know, if you just use text only, you might face issues that, you know,", "tokens": [51392, 457, 797, 11, 291, 458, 11, 498, 291, 445, 764, 2487, 787, 11, 291, 1062, 1851, 2663, 300, 11, 291, 458, 11, 51584], "temperature": 0.0, "avg_logprob": -0.12528944452968213, "compression_ratio": 1.8, "no_speech_prob": 0.004594865255057812}, {"id": 786, "seek": 458240, "start": 4582.4, "end": 4590.719999999999, "text": " your OpenAI embeddings might be too restrictive. It's too broad-based. You need the embeddings", "tokens": [50364, 428, 7238, 48698, 12240, 29432, 1062, 312, 886, 43220, 13, 467, 311, 886, 4152, 12, 6032, 13, 509, 643, 264, 12240, 29432, 50780], "temperature": 0.0, "avg_logprob": -0.1617288589477539, "compression_ratio": 1.5892116182572613, "no_speech_prob": 0.00211665709502995}, {"id": 787, "seek": 458240, "start": 4590.719999999999, "end": 4598.08, "text": " at different levels. So let's see how we would improve the Lang chain graph QA. I'm just using", "tokens": [50780, 412, 819, 4358, 13, 407, 718, 311, 536, 577, 321, 576, 3470, 264, 13313, 5021, 4295, 1249, 32, 13, 286, 478, 445, 1228, 51148], "temperature": 0.0, "avg_logprob": -0.1617288589477539, "compression_ratio": 1.5892116182572613, "no_speech_prob": 0.00211665709502995}, {"id": 788, "seek": 458240, "start": 4598.08, "end": 4603.599999999999, "text": " my strict JSON framework here, which just basically passes the system prompt and then outputs as a", "tokens": [51148, 452, 10910, 31828, 8388, 510, 11, 597, 445, 1936, 11335, 264, 1185, 12391, 293, 550, 23930, 382, 257, 51424], "temperature": 0.0, "avg_logprob": -0.1617288589477539, "compression_ratio": 1.5892116182572613, "no_speech_prob": 0.00211665709502995}, {"id": 789, "seek": 458240, "start": 4603.599999999999, "end": 4611.36, "text": " JSON in your own way. So I basically did what Lang chain does in a much shorter way. So I just", "tokens": [51424, 31828, 294, 428, 1065, 636, 13, 407, 286, 1936, 630, 437, 13313, 5021, 775, 294, 257, 709, 11639, 636, 13, 407, 286, 445, 51812], "temperature": 0.0, "avg_logprob": -0.1617288589477539, "compression_ratio": 1.5892116182572613, "no_speech_prob": 0.00211665709502995}, {"id": 790, "seek": 461136, "start": 4611.36, "end": 4615.44, "text": " say you are a knowledge graph builder. You extracted an object one, object two relation.", "tokens": [50364, 584, 291, 366, 257, 3601, 4295, 27377, 13, 509, 34086, 364, 2657, 472, 11, 2657, 732, 9721, 13, 50568], "temperature": 0.0, "avg_logprob": -0.17693844604492187, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.005303950514644384}, {"id": 791, "seek": 461136, "start": 4615.44, "end": 4620.08, "text": " Okay, I did not even put subject, object predicate. Okay, I mean, I just do like that. Okay, I just", "tokens": [50568, 1033, 11, 286, 630, 406, 754, 829, 3983, 11, 2657, 3852, 8700, 13, 1033, 11, 286, 914, 11, 286, 445, 360, 411, 300, 13, 1033, 11, 286, 445, 50800], "temperature": 0.0, "avg_logprob": -0.17693844604492187, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.005303950514644384}, {"id": 792, "seek": 461136, "start": 4620.08, "end": 4625.44, "text": " want it to be as vague and as generic as possible because I want to capture as much information", "tokens": [50800, 528, 309, 281, 312, 382, 24247, 293, 382, 19577, 382, 1944, 570, 286, 528, 281, 7983, 382, 709, 1589, 51068], "temperature": 0.0, "avg_logprob": -0.17693844604492187, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.005303950514644384}, {"id": 793, "seek": 461136, "start": 4625.44, "end": 4630.08, "text": " as possible. Okay, so this was done in like 10 minutes. Okay, I don't really know whether this", "tokens": [51068, 382, 1944, 13, 1033, 11, 370, 341, 390, 1096, 294, 411, 1266, 2077, 13, 1033, 11, 286, 500, 380, 534, 458, 1968, 341, 51300], "temperature": 0.0, "avg_logprob": -0.17693844604492187, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.005303950514644384}, {"id": 794, "seek": 461136, "start": 4630.08, "end": 4634.96, "text": " is the best. You all can feel free to improve it. Okay, I have the Jupyter notebook attached in the", "tokens": [51300, 307, 264, 1151, 13, 509, 439, 393, 841, 1737, 281, 3470, 309, 13, 1033, 11, 286, 362, 264, 22125, 88, 391, 21060, 8570, 294, 264, 51544], "temperature": 0.0, "avg_logprob": -0.17693844604492187, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.005303950514644384}, {"id": 795, "seek": 463496, "start": 4634.96, "end": 4640.56, "text": " link. All right, so I gave you some examples like John bought the laptop. Okay, that's me,", "tokens": [50364, 2113, 13, 1057, 558, 11, 370, 286, 2729, 291, 512, 5110, 411, 2619, 4243, 264, 10732, 13, 1033, 11, 300, 311, 385, 11, 50644], "temperature": 0.0, "avg_logprob": -0.18952884412791632, "compression_ratio": 1.8651315789473684, "no_speech_prob": 0.04215112701058388}, {"id": 796, "seek": 463496, "start": 4640.56, "end": 4645.68, "text": " all right. John built the house in 2021. Okay, that's not me, all right. But this is the idea of", "tokens": [50644, 439, 558, 13, 2619, 3094, 264, 1782, 294, 7201, 13, 1033, 11, 300, 311, 406, 385, 11, 439, 558, 13, 583, 341, 307, 264, 1558, 295, 50900], "temperature": 0.0, "avg_logprob": -0.18952884412791632, "compression_ratio": 1.8651315789473684, "no_speech_prob": 0.04215112701058388}, {"id": 797, "seek": 463496, "start": 4645.68, "end": 4650.72, "text": " like how we can represent like various relations like that. All right, then the output format is", "tokens": [50900, 411, 577, 321, 393, 2906, 411, 3683, 2299, 411, 300, 13, 1057, 558, 11, 550, 264, 5598, 7877, 307, 51152], "temperature": 0.0, "avg_logprob": -0.18952884412791632, "compression_ratio": 1.8651315789473684, "no_speech_prob": 0.04215112701058388}, {"id": 798, "seek": 463496, "start": 4650.72, "end": 4654.08, "text": " just a knowledge graph. So you can see like Apple announced my and cheese bro, my and cheese bro", "tokens": [51152, 445, 257, 3601, 4295, 13, 407, 291, 393, 536, 411, 6373, 7548, 452, 293, 5399, 2006, 11, 452, 293, 5399, 2006, 51320], "temperature": 0.0, "avg_logprob": -0.18952884412791632, "compression_ratio": 1.8651315789473684, "no_speech_prob": 0.04215112701058388}, {"id": 799, "seek": 463496, "start": 4654.08, "end": 4659.6, "text": " announced in 2025. Apple proved big hit. Okay, so again, this one is not exactly that great,", "tokens": [51320, 7548, 294, 39209, 13, 6373, 14617, 955, 2045, 13, 1033, 11, 370, 797, 11, 341, 472, 307, 406, 2293, 300, 869, 11, 51596], "temperature": 0.0, "avg_logprob": -0.18952884412791632, "compression_ratio": 1.8651315789473684, "no_speech_prob": 0.04215112701058388}, {"id": 800, "seek": 463496, "start": 4659.6, "end": 4663.52, "text": " because it's not really Apple that prove a big hit. It should be the Mac and cheese bro that", "tokens": [51596, 570, 309, 311, 406, 534, 6373, 300, 7081, 257, 955, 2045, 13, 467, 820, 312, 264, 5707, 293, 5399, 2006, 300, 51792], "temperature": 0.0, "avg_logprob": -0.18952884412791632, "compression_ratio": 1.8651315789473684, "no_speech_prob": 0.04215112701058388}, {"id": 801, "seek": 466352, "start": 4663.52, "end": 4668.160000000001, "text": " prove a big hit. So this part needs to be from engineer a bit more. All right, Apple gave cheese.", "tokens": [50364, 7081, 257, 955, 2045, 13, 407, 341, 644, 2203, 281, 312, 490, 11403, 257, 857, 544, 13, 1057, 558, 11, 6373, 2729, 5399, 13, 50596], "temperature": 0.0, "avg_logprob": -0.2004859095034392, "compression_ratio": 1.9106382978723404, "no_speech_prob": 0.0015897366683930159}, {"id": 802, "seek": 466352, "start": 4668.160000000001, "end": 4674.8, "text": " Okay, again, like this is not complete. Okay, cheese browsing ovation into zero to six. So", "tokens": [50596, 1033, 11, 797, 11, 411, 341, 307, 406, 3566, 13, 1033, 11, 5399, 38602, 277, 11116, 666, 4018, 281, 2309, 13, 407, 50928], "temperature": 0.0, "avg_logprob": -0.2004859095034392, "compression_ratio": 1.9106382978723404, "no_speech_prob": 0.0015897366683930159}, {"id": 803, "seek": 466352, "start": 4674.8, "end": 4679.120000000001, "text": " actually we combine these two together. This is complete. So this is okay. All right, cheese", "tokens": [50928, 767, 321, 10432, 613, 732, 1214, 13, 639, 307, 3566, 13, 407, 341, 307, 1392, 13, 1057, 558, 11, 5399, 51144], "temperature": 0.0, "avg_logprob": -0.2004859095034392, "compression_ratio": 1.9106382978723404, "no_speech_prob": 0.0015897366683930159}, {"id": 804, "seek": 466352, "start": 4679.120000000001, "end": 4683.120000000001, "text": " invented man cheese bro. Okay, man cheese bro invented into zero to four. Okay,", "tokens": [51144, 14479, 587, 5399, 2006, 13, 1033, 11, 587, 5399, 2006, 14479, 666, 4018, 281, 1451, 13, 1033, 11, 51344], "temperature": 0.0, "avg_logprob": -0.2004859095034392, "compression_ratio": 1.9106382978723404, "no_speech_prob": 0.0015897366683930159}, {"id": 805, "seek": 466352, "start": 4683.84, "end": 4688.64, "text": " orange created orange and cheese bro. Yep, orange and cheese bro. The price is 5000 and", "tokens": [51380, 7671, 2942, 7671, 293, 5399, 2006, 13, 7010, 11, 7671, 293, 5399, 2006, 13, 440, 3218, 307, 23777, 293, 51620], "temperature": 0.0, "avg_logprob": -0.2004859095034392, "compression_ratio": 1.9106382978723404, "no_speech_prob": 0.0015897366683930159}, {"id": 806, "seek": 468864, "start": 4688.72, "end": 4694.8, "text": " Apple prices 4000. So again, here has some issues or so, like here, instead of saying that this is", "tokens": [50368, 6373, 7901, 31104, 13, 407, 797, 11, 510, 575, 512, 2663, 420, 370, 11, 411, 510, 11, 2602, 295, 1566, 300, 341, 307, 50672], "temperature": 0.0, "avg_logprob": -0.20801836252212524, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.0016046944074332714}, {"id": 807, "seek": 468864, "start": 4694.8, "end": 4698.72, "text": " a Mac and cheese bro, because we should be referring to man cheese bro, it says Apple.", "tokens": [50672, 257, 5707, 293, 5399, 2006, 11, 570, 321, 820, 312, 13761, 281, 587, 5399, 2006, 11, 309, 1619, 6373, 13, 50868], "temperature": 0.0, "avg_logprob": -0.20801836252212524, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.0016046944074332714}, {"id": 808, "seek": 468864, "start": 4699.4400000000005, "end": 4704.96, "text": " Okay, so unless we can sort of like link this later to Apple announced, okay, this part here.", "tokens": [50904, 1033, 11, 370, 5969, 321, 393, 1333, 295, 411, 2113, 341, 1780, 281, 6373, 7548, 11, 1392, 11, 341, 644, 510, 13, 51180], "temperature": 0.0, "avg_logprob": -0.20801836252212524, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.0016046944074332714}, {"id": 809, "seek": 468864, "start": 4706.72, "end": 4711.84, "text": " So now you can see some issues with knowledge graph expressing stuff. It is not clean. All right,", "tokens": [51268, 407, 586, 291, 393, 536, 512, 2663, 365, 3601, 4295, 22171, 1507, 13, 467, 307, 406, 2541, 13, 1057, 558, 11, 51524], "temperature": 0.0, "avg_logprob": -0.20801836252212524, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.0016046944074332714}, {"id": 810, "seek": 471184, "start": 4712.64, "end": 4718.96, "text": " it might truncate the information halfway. So this one needs more study as to how we can", "tokens": [50404, 309, 1062, 504, 409, 66, 473, 264, 1589, 15461, 13, 407, 341, 472, 2203, 544, 2979, 382, 281, 577, 321, 393, 50720], "temperature": 0.0, "avg_logprob": -0.13365789361902186, "compression_ratio": 1.8273092369477912, "no_speech_prob": 0.010027948766946793}, {"id": 811, "seek": 471184, "start": 4718.96, "end": 4723.76, "text": " express this in the knowledge graph better. But by expressing it in the knowledge graph,", "tokens": [50720, 5109, 341, 294, 264, 3601, 4295, 1101, 13, 583, 538, 22171, 309, 294, 264, 3601, 4295, 11, 50960], "temperature": 0.0, "avg_logprob": -0.13365789361902186, "compression_ratio": 1.8273092369477912, "no_speech_prob": 0.010027948766946793}, {"id": 812, "seek": 471184, "start": 4723.76, "end": 4728.96, "text": " you are able to then do knowledge graph passing, okay, and extract out the relevant entities that", "tokens": [50960, 291, 366, 1075, 281, 550, 360, 3601, 4295, 8437, 11, 1392, 11, 293, 8947, 484, 264, 7340, 16667, 300, 51220], "temperature": 0.0, "avg_logprob": -0.13365789361902186, "compression_ratio": 1.8273092369477912, "no_speech_prob": 0.010027948766946793}, {"id": 813, "seek": 471184, "start": 4730.32, "end": 4734.88, "text": " related to the prompt. And you know, this is like, if you think about it, this is like", "tokens": [51288, 4077, 281, 264, 12391, 13, 400, 291, 458, 11, 341, 307, 411, 11, 498, 291, 519, 466, 309, 11, 341, 307, 411, 51516], "temperature": 0.0, "avg_logprob": -0.13365789361902186, "compression_ratio": 1.8273092369477912, "no_speech_prob": 0.010027948766946793}, {"id": 814, "seek": 471184, "start": 4734.88, "end": 4740.4800000000005, "text": " doing segmentation across like every few words in the segment one time. Yeah, so this is the", "tokens": [51516, 884, 9469, 399, 2108, 411, 633, 1326, 2283, 294, 264, 9469, 472, 565, 13, 865, 11, 370, 341, 307, 264, 51796], "temperature": 0.0, "avg_logprob": -0.13365789361902186, "compression_ratio": 1.8273092369477912, "no_speech_prob": 0.010027948766946793}, {"id": 815, "seek": 474048, "start": 4740.48, "end": 4746.0, "text": " generated graph of what I did for strict design framework. You can see that compared to Lang chain,", "tokens": [50364, 10833, 4295, 295, 437, 286, 630, 337, 10910, 1715, 8388, 13, 509, 393, 536, 300, 5347, 281, 13313, 5021, 11, 50640], "temperature": 0.0, "avg_logprob": -0.1971705213506171, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.0008718795725144446}, {"id": 816, "seek": 474048, "start": 4746.0, "end": 4754.959999999999, "text": " this is what happens like we have way more relations, there's more relations here. And dates are captured.", "tokens": [50640, 341, 307, 437, 2314, 411, 321, 362, 636, 544, 2299, 11, 456, 311, 544, 2299, 510, 13, 400, 11691, 366, 11828, 13, 51088], "temperature": 0.0, "avg_logprob": -0.1971705213506171, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.0008718795725144446}, {"id": 817, "seek": 474048, "start": 4757.2, "end": 4762.16, "text": " Yeah, so this is something that I think needs to be investigated more. Mine is not the best,", "tokens": [51200, 865, 11, 370, 341, 307, 746, 300, 286, 519, 2203, 281, 312, 30070, 544, 13, 11620, 307, 406, 264, 1151, 11, 51448], "temperature": 0.0, "avg_logprob": -0.1971705213506171, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.0008718795725144446}, {"id": 818, "seek": 474048, "start": 4762.16, "end": 4766.799999999999, "text": " but Lang chain is definitely not good. Okay, so this is something that needs to be done more if", "tokens": [51448, 457, 13313, 5021, 307, 2138, 406, 665, 13, 1033, 11, 370, 341, 307, 746, 300, 2203, 281, 312, 1096, 544, 498, 51680], "temperature": 0.0, "avg_logprob": -0.1971705213506171, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.0008718795725144446}, {"id": 819, "seek": 476680, "start": 4766.88, "end": 4772.24, "text": " we want to extract stuff out into the knowledge graph. And then like, should we use embeddings?", "tokens": [50368, 321, 528, 281, 8947, 1507, 484, 666, 264, 3601, 4295, 13, 400, 550, 411, 11, 820, 321, 764, 12240, 29432, 30, 50636], "temperature": 0.0, "avg_logprob": -0.168534229711159, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.00188540224917233}, {"id": 820, "seek": 476680, "start": 4773.84, "end": 4781.28, "text": " So if you want to use embeddings, then we cannot just use OpenAI API. Maybe you need to use like", "tokens": [50716, 407, 498, 291, 528, 281, 764, 12240, 29432, 11, 550, 321, 2644, 445, 764, 7238, 48698, 9362, 13, 2704, 291, 643, 281, 764, 411, 51088], "temperature": 0.0, "avg_logprob": -0.168534229711159, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.00188540224917233}, {"id": 821, "seek": 476680, "start": 4781.28, "end": 4788.400000000001, "text": " Lama2. Okay, although Lama2 perhaps is not that great or so, because Lama2 is", "tokens": [51088, 441, 2404, 17, 13, 1033, 11, 4878, 441, 2404, 17, 4317, 307, 406, 300, 869, 420, 370, 11, 570, 441, 2404, 17, 307, 51444], "temperature": 0.0, "avg_logprob": -0.168534229711159, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.00188540224917233}, {"id": 822, "seek": 476680, "start": 4790.0, "end": 4795.4400000000005, "text": " not that good for multilingual. Okay, but Lama2 is the best possible substitute for", "tokens": [51524, 406, 300, 665, 337, 2120, 38219, 13, 1033, 11, 457, 441, 2404, 17, 307, 264, 1151, 1944, 15802, 337, 51796], "temperature": 0.0, "avg_logprob": -0.168534229711159, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.00188540224917233}, {"id": 823, "seek": 479544, "start": 4796.24, "end": 4800.799999999999, "text": " chat GPD right now. So maybe you can construct a knowledge graph embeddings using the Lama2 embeddings.", "tokens": [50404, 5081, 460, 17349, 558, 586, 13, 407, 1310, 291, 393, 7690, 257, 3601, 4295, 12240, 29432, 1228, 264, 441, 2404, 17, 12240, 29432, 13, 50632], "temperature": 0.0, "avg_logprob": -0.1476286015612014, "compression_ratio": 1.634453781512605, "no_speech_prob": 0.0011597094126045704}, {"id": 824, "seek": 479544, "start": 4801.839999999999, "end": 4808.96, "text": " So food for thought. Next, we have this flexible knowledge graph passing. Over here, what I decided", "tokens": [50684, 407, 1755, 337, 1194, 13, 3087, 11, 321, 362, 341, 11358, 3601, 4295, 8437, 13, 4886, 510, 11, 437, 286, 3047, 51040], "temperature": 0.0, "avg_logprob": -0.1476286015612014, "compression_ratio": 1.634453781512605, "no_speech_prob": 0.0011597094126045704}, {"id": 825, "seek": 479544, "start": 4808.96, "end": 4817.44, "text": " to do is that we want to output only relations that are relevant to the question. And I just", "tokens": [51040, 281, 360, 307, 300, 321, 528, 281, 5598, 787, 2299, 300, 366, 7340, 281, 264, 1168, 13, 400, 286, 445, 51464], "temperature": 0.0, "avg_logprob": -0.1476286015612014, "compression_ratio": 1.634453781512605, "no_speech_prob": 0.0011597094126045704}, {"id": 826, "seek": 479544, "start": 4817.44, "end": 4823.04, "text": " passed in the entire knowledge graph here. So instead of coming up entities, I just asked it", "tokens": [51464, 4678, 294, 264, 2302, 3601, 4295, 510, 13, 407, 2602, 295, 1348, 493, 16667, 11, 286, 445, 2351, 309, 51744], "temperature": 0.0, "avg_logprob": -0.1476286015612014, "compression_ratio": 1.634453781512605, "no_speech_prob": 0.0011597094126045704}, {"id": 827, "seek": 482304, "start": 4823.04, "end": 4829.36, "text": " to go through the entire knowledge graph because in case of words not exact or spelling errors,", "tokens": [50364, 281, 352, 807, 264, 2302, 3601, 4295, 570, 294, 1389, 295, 2283, 406, 1900, 420, 22254, 13603, 11, 50680], "temperature": 0.0, "avg_logprob": -0.15747420662327816, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.0038823543582111597}, {"id": 828, "seek": 482304, "start": 4831.12, "end": 4840.0, "text": " GPD is able to catch it most of the time. I must copy it because GPD is not as great as", "tokens": [50768, 460, 17349, 307, 1075, 281, 3745, 309, 881, 295, 264, 565, 13, 286, 1633, 5055, 309, 570, 460, 17349, 307, 406, 382, 869, 382, 51212], "temperature": 0.0, "avg_logprob": -0.15747420662327816, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.0038823543582111597}, {"id": 829, "seek": 482304, "start": 4840.0, "end": 4844.72, "text": " doing like counting letters and stuff. But if you misspell your words, but the meaning is about", "tokens": [51212, 884, 411, 13251, 7825, 293, 1507, 13, 583, 498, 291, 1713, 49241, 428, 2283, 11, 457, 264, 3620, 307, 466, 51448], "temperature": 0.0, "avg_logprob": -0.15747420662327816, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.0038823543582111597}, {"id": 830, "seek": 482304, "start": 4844.72, "end": 4850.0, "text": " that, GPD is able to extract the right entities. And here we can see that we asked it like,", "tokens": [51448, 300, 11, 460, 17349, 307, 1075, 281, 8947, 264, 558, 16667, 13, 400, 510, 321, 393, 536, 300, 321, 2351, 309, 411, 11, 51712], "temperature": 0.0, "avg_logprob": -0.15747420662327816, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.0038823543582111597}, {"id": 831, "seek": 485000, "start": 4850.0, "end": 4854.88, "text": " when did Apple announce the man cheese bro? It captured exactly what we want. All right.", "tokens": [50364, 562, 630, 6373, 7478, 264, 587, 5399, 2006, 30, 467, 11828, 2293, 437, 321, 528, 13, 1057, 558, 13, 50608], "temperature": 0.0, "avg_logprob": -0.16229568828235974, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.0025890767574310303}, {"id": 832, "seek": 485000, "start": 4856.0, "end": 4862.08, "text": " And this is the graph that is the past knowledge graph. So I'm talking about when you query the", "tokens": [50664, 400, 341, 307, 264, 4295, 300, 307, 264, 1791, 3601, 4295, 13, 407, 286, 478, 1417, 466, 562, 291, 14581, 264, 50968], "temperature": 0.0, "avg_logprob": -0.16229568828235974, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.0025890767574310303}, {"id": 833, "seek": 485000, "start": 4862.08, "end": 4865.36, "text": " knowledge graph, you pass it so that only relevant sections of the knowledge graph gets", "tokens": [50968, 3601, 4295, 11, 291, 1320, 309, 370, 300, 787, 7340, 10863, 295, 264, 3601, 4295, 2170, 51132], "temperature": 0.0, "avg_logprob": -0.16229568828235974, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.0025890767574310303}, {"id": 834, "seek": 485000, "start": 4865.36, "end": 4871.84, "text": " come out, gets extracted. You ground this extracted part onto your text. Okay. And then", "tokens": [51132, 808, 484, 11, 2170, 34086, 13, 509, 2727, 341, 34086, 644, 3911, 428, 2487, 13, 1033, 13, 400, 550, 51456], "temperature": 0.0, "avg_logprob": -0.16229568828235974, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.0025890767574310303}, {"id": 835, "seek": 485000, "start": 4872.64, "end": 4878.96, "text": " you can get the answer here. So 2025. So I just shown that like using this strict JSON format,", "tokens": [51496, 291, 393, 483, 264, 1867, 510, 13, 407, 945, 6074, 13, 407, 286, 445, 4898, 300, 411, 1228, 341, 10910, 31828, 7877, 11, 51812], "temperature": 0.0, "avg_logprob": -0.16229568828235974, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.0025890767574310303}, {"id": 836, "seek": 487896, "start": 4878.96, "end": 4883.6, "text": " you are able to like, it's very flexible. You just need to key in the system prompt,", "tokens": [50364, 291, 366, 1075, 281, 411, 11, 309, 311, 588, 11358, 13, 509, 445, 643, 281, 2141, 294, 264, 1185, 12391, 11, 50596], "temperature": 0.0, "avg_logprob": -0.12127251978273745, "compression_ratio": 1.780392156862745, "no_speech_prob": 0.0010383859043940902}, {"id": 837, "seek": 487896, "start": 4883.6, "end": 4887.44, "text": " key in the user problem and output the format in terms of whatever JSON labels and the", "tokens": [50596, 2141, 294, 264, 4195, 1154, 293, 5598, 264, 7877, 294, 2115, 295, 2035, 31828, 16949, 293, 264, 50788], "temperature": 0.0, "avg_logprob": -0.12127251978273745, "compression_ratio": 1.780392156862745, "no_speech_prob": 0.0010383859043940902}, {"id": 838, "seek": 487896, "start": 4887.44, "end": 4892.8, "text": " description of the JSON. So I've been using this for a lot of my own use cases. And I'm just", "tokens": [50788, 3855, 295, 264, 31828, 13, 407, 286, 600, 668, 1228, 341, 337, 257, 688, 295, 452, 1065, 764, 3331, 13, 400, 286, 478, 445, 51056], "temperature": 0.0, "avg_logprob": -0.12127251978273745, "compression_ratio": 1.780392156862745, "no_speech_prob": 0.0010383859043940902}, {"id": 839, "seek": 487896, "start": 4892.8, "end": 4897.68, "text": " adapting this for the knowledge graph. But this is really cool because you can then use this", "tokens": [51056, 34942, 341, 337, 264, 3601, 4295, 13, 583, 341, 307, 534, 1627, 570, 291, 393, 550, 764, 341, 51300], "temperature": 0.0, "avg_logprob": -0.12127251978273745, "compression_ratio": 1.780392156862745, "no_speech_prob": 0.0010383859043940902}, {"id": 840, "seek": 487896, "start": 4898.24, "end": 4902.56, "text": " past knowledge graph, like this idea of generating the knowledge graph and passing the knowledge", "tokens": [51328, 1791, 3601, 4295, 11, 411, 341, 1558, 295, 17746, 264, 3601, 4295, 293, 8437, 264, 3601, 51544], "temperature": 0.0, "avg_logprob": -0.12127251978273745, "compression_ratio": 1.780392156862745, "no_speech_prob": 0.0010383859043940902}, {"id": 841, "seek": 490256, "start": 4902.56, "end": 4910.56, "text": " graph. You can use the knowledge graph as memory. Okay. And then you can update memory.", "tokens": [50364, 4295, 13, 509, 393, 764, 264, 3601, 4295, 382, 4675, 13, 1033, 13, 400, 550, 291, 393, 5623, 4675, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1235062318689683, "compression_ratio": 1.7766990291262137, "no_speech_prob": 0.005305448081344366}, {"id": 842, "seek": 490256, "start": 4911.120000000001, "end": 4918.240000000001, "text": " And you can use updated memory to extract relevant parts. Okay. So this called retrieval.", "tokens": [50792, 400, 291, 393, 764, 10588, 4675, 281, 8947, 7340, 3166, 13, 1033, 13, 407, 341, 1219, 19817, 3337, 13, 51148], "temperature": 0.0, "avg_logprob": -0.1235062318689683, "compression_ratio": 1.7766990291262137, "no_speech_prob": 0.005305448081344366}, {"id": 843, "seek": 490256, "start": 4919.04, "end": 4925.52, "text": " Okay. Use relevant parts to solve problem. So I really like this framework because this", "tokens": [51188, 1033, 13, 8278, 7340, 3166, 281, 5039, 1154, 13, 407, 286, 534, 411, 341, 8388, 570, 341, 51512], "temperature": 0.0, "avg_logprob": -0.1235062318689683, "compression_ratio": 1.7766990291262137, "no_speech_prob": 0.005305448081344366}, {"id": 844, "seek": 490256, "start": 4925.52, "end": 4931.84, "text": " knowledge graph as memory thing is something quite interesting. But how can we express it as memory?", "tokens": [51512, 3601, 4295, 382, 4675, 551, 307, 746, 1596, 1880, 13, 583, 577, 393, 321, 5109, 309, 382, 4675, 30, 51828], "temperature": 0.0, "avg_logprob": -0.1235062318689683, "compression_ratio": 1.7766990291262137, "no_speech_prob": 0.005305448081344366}, {"id": 845, "seek": 493184, "start": 4931.84, "end": 4939.360000000001, "text": " That's the difficult part. Okay. So Richard asked, have I tried putting the graph into an FAIS", "tokens": [50364, 663, 311, 264, 2252, 644, 13, 1033, 13, 407, 9809, 2351, 11, 362, 286, 3031, 3372, 264, 4295, 666, 364, 19894, 2343, 50740], "temperature": 0.0, "avg_logprob": -0.176608572200853, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.001979405991733074}, {"id": 846, "seek": 493184, "start": 4939.360000000001, "end": 4947.04, "text": " index? No, I haven't. But how will you do a knowledge graph putting onto the, like onto that", "tokens": [50740, 8186, 30, 883, 11, 286, 2378, 380, 13, 583, 577, 486, 291, 360, 257, 3601, 4295, 3372, 3911, 264, 11, 411, 3911, 300, 51124], "temperature": 0.0, "avg_logprob": -0.176608572200853, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.001979405991733074}, {"id": 847, "seek": 493184, "start": 4947.04, "end": 4952.4800000000005, "text": " index? Because usually what I know is that you do the embedding and then you put the text. That's", "tokens": [51124, 8186, 30, 1436, 2673, 437, 286, 458, 307, 300, 291, 360, 264, 12240, 3584, 293, 550, 291, 829, 264, 2487, 13, 663, 311, 51396], "temperature": 0.0, "avg_logprob": -0.176608572200853, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.001979405991733074}, {"id": 848, "seek": 493184, "start": 4952.4800000000005, "end": 4958.4800000000005, "text": " for the retrieval or method generation. If you're doing knowledge graph, maybe you put the source", "tokens": [51396, 337, 264, 19817, 3337, 420, 3170, 5125, 13, 759, 291, 434, 884, 3601, 4295, 11, 1310, 291, 829, 264, 4009, 51696], "temperature": 0.0, "avg_logprob": -0.176608572200853, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.001979405991733074}, {"id": 849, "seek": 495848, "start": 4958.48, "end": 4963.759999999999, "text": " as the index. Okay. I'm not too sure. I'm going to check on this. Like how will you do this into", "tokens": [50364, 382, 264, 8186, 13, 1033, 13, 286, 478, 406, 886, 988, 13, 286, 478, 516, 281, 1520, 322, 341, 13, 1743, 577, 486, 291, 360, 341, 666, 50628], "temperature": 0.0, "avg_logprob": -0.15733287952564382, "compression_ratio": 1.5731707317073171, "no_speech_prob": 0.002446043537929654}, {"id": 850, "seek": 495848, "start": 4963.759999999999, "end": 4968.799999999999, "text": " PyCon and stuff like that? But what I can imagine you doing for the knowledge graph is just put the", "tokens": [50628, 9953, 9838, 293, 1507, 411, 300, 30, 583, 437, 286, 393, 3811, 291, 884, 337, 264, 3601, 4295, 307, 445, 829, 264, 50880], "temperature": 0.0, "avg_logprob": -0.15733287952564382, "compression_ratio": 1.5731707317073171, "no_speech_prob": 0.002446043537929654}, {"id": 851, "seek": 495848, "start": 4968.799999999999, "end": 4974.959999999999, "text": " whole thing into some array and then just store the array. I mean, you can even put it as a JSON.", "tokens": [50880, 1379, 551, 666, 512, 10225, 293, 550, 445, 3531, 264, 10225, 13, 286, 914, 11, 291, 393, 754, 829, 309, 382, 257, 31828, 13, 51188], "temperature": 0.0, "avg_logprob": -0.15733287952564382, "compression_ratio": 1.5731707317073171, "no_speech_prob": 0.002446043537929654}, {"id": 852, "seek": 495848, "start": 4974.959999999999, "end": 4985.12, "text": " Yeah. So yeah. Okay. I don't have time to cover through the running of the Jupyter Notebook.", "tokens": [51188, 865, 13, 407, 1338, 13, 1033, 13, 286, 500, 380, 362, 565, 281, 2060, 807, 264, 2614, 295, 264, 22125, 88, 391, 11633, 2939, 13, 51696], "temperature": 0.0, "avg_logprob": -0.15733287952564382, "compression_ratio": 1.5731707317073171, "no_speech_prob": 0.002446043537929654}, {"id": 853, "seek": 498512, "start": 4985.12, "end": 4990.4, "text": " I'll just upload that separately. It's another video. But let's just go through like the last", "tokens": [50364, 286, 603, 445, 6580, 300, 14759, 13, 467, 311, 1071, 960, 13, 583, 718, 311, 445, 352, 807, 411, 264, 1036, 50628], "temperature": 0.0, "avg_logprob": -0.12316960208820847, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.0034824786707758904}, {"id": 854, "seek": 498512, "start": 4990.4, "end": 4994.4, "text": " five to 10 minutes. I'm okay to extend about 15 minutes if you all have more things to discuss.", "tokens": [50628, 1732, 281, 1266, 2077, 13, 286, 478, 1392, 281, 10101, 466, 2119, 2077, 498, 291, 439, 362, 544, 721, 281, 2248, 13, 50828], "temperature": 0.0, "avg_logprob": -0.12316960208820847, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.0034824786707758904}, {"id": 855, "seek": 498512, "start": 4995.12, "end": 5000.08, "text": " Like we have discussed like how can we use knowledge graph better for last language models?", "tokens": [50864, 1743, 321, 362, 7152, 411, 577, 393, 321, 764, 3601, 4295, 1101, 337, 1036, 2856, 5245, 30, 51112], "temperature": 0.0, "avg_logprob": -0.12316960208820847, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.0034824786707758904}, {"id": 856, "seek": 498512, "start": 5000.08, "end": 5004.88, "text": " So first question, what are the failure modes of using knowledge graph for context representation?", "tokens": [51112, 407, 700, 1168, 11, 437, 366, 264, 7763, 14068, 295, 1228, 3601, 4295, 337, 4319, 10290, 30, 51352], "temperature": 0.0, "avg_logprob": -0.12316960208820847, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.0034824786707758904}, {"id": 857, "seek": 498512, "start": 5005.84, "end": 5012.96, "text": " And I think this failure mode is mainly like your knowledge graph may not capture all information.", "tokens": [51400, 400, 286, 519, 341, 7763, 4391, 307, 8704, 411, 428, 3601, 4295, 815, 406, 7983, 439, 1589, 13, 51756], "temperature": 0.0, "avg_logprob": -0.12316960208820847, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.0034824786707758904}, {"id": 858, "seek": 501512, "start": 5015.2, "end": 5017.68, "text": " Okay. And also the knowledge graph capturing", "tokens": [50368, 1033, 13, 400, 611, 264, 3601, 4295, 23384, 50492], "temperature": 0.0, "avg_logprob": -0.13206850854974045, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0009056554990820587}, {"id": 859, "seek": 501512, "start": 5020.32, "end": 5025.92, "text": " might truncate the information. So maybe using text directly", "tokens": [50624, 1062, 504, 409, 66, 473, 264, 1589, 13, 407, 1310, 1228, 2487, 3838, 50904], "temperature": 0.0, "avg_logprob": -0.13206850854974045, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0009056554990820587}, {"id": 860, "seek": 501512, "start": 5028.0, "end": 5034.5599999999995, "text": " may be better. Okay. But harder to pass. Because if you are using text directly, you don't really have", "tokens": [51008, 815, 312, 1101, 13, 1033, 13, 583, 6081, 281, 1320, 13, 1436, 498, 291, 366, 1228, 2487, 3838, 11, 291, 500, 380, 534, 362, 51336], "temperature": 0.0, "avg_logprob": -0.13206850854974045, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0009056554990820587}, {"id": 861, "seek": 501512, "start": 5036.4, "end": 5042.16, "text": " like nice sections where you can pass the knowledge graph on. Yeah. So these are some of the things", "tokens": [51428, 411, 1481, 10863, 689, 291, 393, 1320, 264, 3601, 4295, 322, 13, 865, 13, 407, 613, 366, 512, 295, 264, 721, 51716], "temperature": 0.0, "avg_logprob": -0.13206850854974045, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0009056554990820587}, {"id": 862, "seek": 504216, "start": 5042.24, "end": 5045.44, "text": " about right now, some of the failure modes of this knowledge graph.", "tokens": [50368, 466, 558, 586, 11, 512, 295, 264, 7763, 14068, 295, 341, 3601, 4295, 13, 50528], "temperature": 0.0, "avg_logprob": -0.16471140725272043, "compression_ratio": 1.6141304347826086, "no_speech_prob": 0.0025905687361955643}, {"id": 863, "seek": 504216, "start": 5046.5599999999995, "end": 5047.84, "text": " Anyone else has anything to add?", "tokens": [50584, 14643, 1646, 575, 1340, 281, 909, 30, 50648], "temperature": 0.0, "avg_logprob": -0.16471140725272043, "compression_ratio": 1.6141304347826086, "no_speech_prob": 0.0025905687361955643}, {"id": 864, "seek": 504216, "start": 5049.44, "end": 5058.48, "text": " Just some random thoughts. Do you think it makes sense if we view the embedding space", "tokens": [50728, 1449, 512, 4974, 4598, 13, 1144, 291, 519, 309, 1669, 2020, 498, 321, 1910, 264, 12240, 3584, 1901, 51180], "temperature": 0.0, "avg_logprob": -0.16471140725272043, "compression_ratio": 1.6141304347826086, "no_speech_prob": 0.0025905687361955643}, {"id": 865, "seek": 504216, "start": 5058.48, "end": 5061.36, "text": " itself as a form of generalized knowledge graph?", "tokens": [51180, 2564, 382, 257, 1254, 295, 44498, 3601, 4295, 30, 51324], "temperature": 0.0, "avg_logprob": -0.16471140725272043, "compression_ratio": 1.6141304347826086, "no_speech_prob": 0.0025905687361955643}, {"id": 866, "seek": 504216, "start": 5063.68, "end": 5067.44, "text": " Embedding space as a genera. You mean the LM embedding space?", "tokens": [51440, 24234, 292, 3584, 1901, 382, 257, 1337, 64, 13, 509, 914, 264, 46529, 12240, 3584, 1901, 30, 51628], "temperature": 0.0, "avg_logprob": -0.16471140725272043, "compression_ratio": 1.6141304347826086, "no_speech_prob": 0.0025905687361955643}, {"id": 867, "seek": 506744, "start": 5067.44, "end": 5075.5199999999995, "text": " Oh, I mean, I mean, yes, but for some specific tasks you want to do, you can train a dedicated,", "tokens": [50364, 876, 11, 286, 914, 11, 286, 914, 11, 2086, 11, 457, 337, 512, 2685, 9608, 291, 528, 281, 360, 11, 291, 393, 3847, 257, 8374, 11, 50768], "temperature": 0.0, "avg_logprob": -0.2001955032348633, "compression_ratio": 1.558659217877095, "no_speech_prob": 0.0014119946863502264}, {"id": 868, "seek": 506744, "start": 5076.08, "end": 5086.799999999999, "text": " a separate dedicated embedding space. So because like you have all your entities inside their space", "tokens": [50796, 257, 4994, 8374, 12240, 3584, 1901, 13, 407, 570, 411, 291, 362, 439, 428, 16667, 1854, 641, 1901, 51332], "temperature": 0.0, "avg_logprob": -0.2001955032348633, "compression_ratio": 1.558659217877095, "no_speech_prob": 0.0014119946863502264}, {"id": 869, "seek": 506744, "start": 5087.5199999999995, "end": 5092.96, "text": " and the relative, I don't know, relative positioning of them kind of encode certain", "tokens": [51368, 293, 264, 4972, 11, 286, 500, 380, 458, 11, 4972, 26381, 295, 552, 733, 295, 2058, 1429, 1629, 51640], "temperature": 0.0, "avg_logprob": -0.2001955032348633, "compression_ratio": 1.558659217877095, "no_speech_prob": 0.0014119946863502264}, {"id": 870, "seek": 509296, "start": 5093.2, "end": 5098.0, "text": " relative information of them, right? Because I think the issue you mentioned here is,", "tokens": [50376, 4972, 1589, 295, 552, 11, 558, 30, 1436, 286, 519, 264, 2734, 291, 2835, 510, 307, 11, 50616], "temperature": 0.0, "avg_logprob": -0.21802173342023576, "compression_ratio": 1.642512077294686, "no_speech_prob": 0.004345136694610119}, {"id": 871, "seek": 509296, "start": 5098.96, "end": 5106.64, "text": " I think it's just the graph, learn graph can be too sparse, right? You lose a lot of information.", "tokens": [50664, 286, 519, 309, 311, 445, 264, 4295, 11, 1466, 4295, 393, 312, 886, 637, 11668, 11, 558, 30, 509, 3624, 257, 688, 295, 1589, 13, 51048], "temperature": 0.0, "avg_logprob": -0.21802173342023576, "compression_ratio": 1.642512077294686, "no_speech_prob": 0.004345136694610119}, {"id": 872, "seek": 509296, "start": 5107.6, "end": 5114.8, "text": " But if inside embedding space, I don't know, it might help preserve more information,", "tokens": [51096, 583, 498, 1854, 12240, 3584, 1901, 11, 286, 500, 380, 458, 11, 309, 1062, 854, 15665, 544, 1589, 11, 51456], "temperature": 0.0, "avg_logprob": -0.21802173342023576, "compression_ratio": 1.642512077294686, "no_speech_prob": 0.004345136694610119}, {"id": 873, "seek": 509296, "start": 5114.8, "end": 5119.04, "text": " although not very explicit information. It's just some random thought.", "tokens": [51456, 4878, 406, 588, 13691, 1589, 13, 467, 311, 445, 512, 4974, 1194, 13, 51668], "temperature": 0.0, "avg_logprob": -0.21802173342023576, "compression_ratio": 1.642512077294686, "no_speech_prob": 0.004345136694610119}, {"id": 874, "seek": 511904, "start": 5119.92, "end": 5124.64, "text": " Ah, okay, I get what you mean. Like you encode knowledge graph as embedding space,", "tokens": [50408, 2438, 11, 1392, 11, 286, 483, 437, 291, 914, 13, 1743, 291, 2058, 1429, 3601, 4295, 382, 12240, 3584, 1901, 11, 50644], "temperature": 0.0, "avg_logprob": -0.20540767727476178, "compression_ratio": 1.528735632183908, "no_speech_prob": 0.0018770575989037752}, {"id": 875, "seek": 511904, "start": 5125.36, "end": 5132.72, "text": " so like your source relation and the output are all embedding space and sink in destination.", "tokens": [50680, 370, 411, 428, 4009, 9721, 293, 264, 5598, 366, 439, 12240, 3584, 1901, 293, 9500, 294, 12236, 13, 51048], "temperature": 0.0, "avg_logprob": -0.20540767727476178, "compression_ratio": 1.528735632183908, "no_speech_prob": 0.0018770575989037752}, {"id": 876, "seek": 511904, "start": 5134.64, "end": 5142.24, "text": " I feel like my gut feeling is it can be much richer than just a typical traditional graph.", "tokens": [51144, 286, 841, 411, 452, 5228, 2633, 307, 309, 393, 312, 709, 29021, 813, 445, 257, 7476, 5164, 4295, 13, 51524], "temperature": 0.0, "avg_logprob": -0.20540767727476178, "compression_ratio": 1.528735632183908, "no_speech_prob": 0.0018770575989037752}, {"id": 877, "seek": 514224, "start": 5143.12, "end": 5145.12, "text": " Definitely.", "tokens": [50408, 12151, 13, 50508], "temperature": 0.0, "avg_logprob": -0.20374220990120095, "compression_ratio": 1.5465587044534412, "no_speech_prob": 0.004888508003205061}, {"id": 878, "seek": 514224, "start": 5147.5199999999995, "end": 5151.28, "text": " Now I kind of agree with you. It's just going to be hard to express the embedding space", "tokens": [50628, 823, 286, 733, 295, 3986, 365, 291, 13, 467, 311, 445, 516, 281, 312, 1152, 281, 5109, 264, 12240, 3584, 1901, 50816], "temperature": 0.0, "avg_logprob": -0.20374220990120095, "compression_ratio": 1.5465587044534412, "no_speech_prob": 0.004888508003205061}, {"id": 879, "seek": 514224, "start": 5151.92, "end": 5157.04, "text": " using an OpenAI API. You might need to have access to the last average model directly if you want to do this.", "tokens": [50848, 1228, 364, 7238, 48698, 9362, 13, 509, 1062, 643, 281, 362, 2105, 281, 264, 1036, 4274, 2316, 3838, 498, 291, 528, 281, 360, 341, 13, 51104], "temperature": 0.0, "avg_logprob": -0.20374220990120095, "compression_ratio": 1.5465587044534412, "no_speech_prob": 0.004888508003205061}, {"id": 880, "seek": 514224, "start": 5159.599999999999, "end": 5162.88, "text": " Yeah, but definitely that's one of the ways that we can represent knowledge graphs.", "tokens": [51232, 865, 11, 457, 2138, 300, 311, 472, 295, 264, 2098, 300, 321, 393, 2906, 3601, 24877, 13, 51396], "temperature": 0.0, "avg_logprob": -0.20374220990120095, "compression_ratio": 1.5465587044534412, "no_speech_prob": 0.004888508003205061}, {"id": 881, "seek": 514224, "start": 5164.88, "end": 5170.5599999999995, "text": " Anyway, this is the second question also. Should we utilize the embedding space? Perhaps", "tokens": [51496, 5684, 11, 341, 307, 264, 1150, 1168, 611, 13, 6454, 321, 16117, 264, 12240, 3584, 1901, 30, 10517, 51780], "temperature": 0.0, "avg_logprob": -0.20374220990120095, "compression_ratio": 1.5465587044534412, "no_speech_prob": 0.004888508003205061}, {"id": 882, "seek": 517224, "start": 5172.32, "end": 5178.88, "text": " for more expressive knowledge graph. Okay, but then if you think about like what I was talking about earlier,", "tokens": [50368, 337, 544, 40189, 3601, 4295, 13, 1033, 11, 457, 550, 498, 291, 519, 466, 411, 437, 286, 390, 1417, 466, 3071, 11, 50696], "temperature": 0.0, "avg_logprob": -0.24451446533203125, "compression_ratio": 1.6149425287356323, "no_speech_prob": 0.0030939241405576468}, {"id": 883, "seek": 517224, "start": 5179.76, "end": 5180.639999999999, "text": " context dependent", "tokens": [50740, 4319, 12334, 50784], "temperature": 0.0, "avg_logprob": -0.24451446533203125, "compression_ratio": 1.6149425287356323, "no_speech_prob": 0.0030939241405576468}, {"id": 884, "seek": 517224, "start": 5183.92, "end": 5187.679999999999, "text": " embeddings. If you are talking about context dependent embeddings, actually we can use", "tokens": [50948, 12240, 29432, 13, 759, 291, 366, 1417, 466, 4319, 12334, 12240, 29432, 11, 767, 321, 393, 764, 51136], "temperature": 0.0, "avg_logprob": -0.24451446533203125, "compression_ratio": 1.6149425287356323, "no_speech_prob": 0.0030939241405576468}, {"id": 885, "seek": 517224, "start": 5187.679999999999, "end": 5195.84, "text": " LLM to pass an update embeddings based on the parents of the node.", "tokens": [51136, 441, 43, 44, 281, 1320, 364, 5623, 12240, 29432, 2361, 322, 264, 3152, 295, 264, 9984, 13, 51544], "temperature": 0.0, "avg_logprob": -0.24451446533203125, "compression_ratio": 1.6149425287356323, "no_speech_prob": 0.0030939241405576468}, {"id": 886, "seek": 519584, "start": 5196.72, "end": 5201.28, "text": " Yeah, so I was thinking of something like that. Like you can actually have a very, very", "tokens": [50408, 865, 11, 370, 286, 390, 1953, 295, 746, 411, 300, 13, 1743, 291, 393, 767, 362, 257, 588, 11, 588, 50636], "temperature": 0.0, "avg_logprob": -0.19872728983561197, "compression_ratio": 1.7904411764705883, "no_speech_prob": 0.002027960726991296}, {"id": 887, "seek": 519584, "start": 5203.4400000000005, "end": 5208.400000000001, "text": " different interpretation of a certain word. Like for example, bank can be river bank or financial", "tokens": [50744, 819, 14174, 295, 257, 1629, 1349, 13, 1743, 337, 1365, 11, 3765, 393, 312, 6810, 3765, 420, 4669, 50992], "temperature": 0.0, "avg_logprob": -0.19872728983561197, "compression_ratio": 1.7904411764705883, "no_speech_prob": 0.002027960726991296}, {"id": 888, "seek": 519584, "start": 5208.400000000001, "end": 5213.28, "text": " bank, depending on like the context of it. If you are talking about river side, then it's like river bank.", "tokens": [50992, 3765, 11, 5413, 322, 411, 264, 4319, 295, 309, 13, 759, 291, 366, 1417, 466, 6810, 1252, 11, 550, 309, 311, 411, 6810, 3765, 13, 51236], "temperature": 0.0, "avg_logprob": -0.19872728983561197, "compression_ratio": 1.7904411764705883, "no_speech_prob": 0.002027960726991296}, {"id": 889, "seek": 519584, "start": 5213.28, "end": 5218.16, "text": " Yeah, so you can actually use the last language model, extract out the hierarchy of the graph,", "tokens": [51236, 865, 11, 370, 291, 393, 767, 764, 264, 1036, 2856, 2316, 11, 8947, 484, 264, 22333, 295, 264, 4295, 11, 51480], "temperature": 0.0, "avg_logprob": -0.19872728983561197, "compression_ratio": 1.7904411764705883, "no_speech_prob": 0.002027960726991296}, {"id": 890, "seek": 519584, "start": 5218.16, "end": 5223.68, "text": " the front part of the graph. You can put it there and you can then pass the embeddings accordingly.", "tokens": [51480, 264, 1868, 644, 295, 264, 4295, 13, 509, 393, 829, 309, 456, 293, 291, 393, 550, 1320, 264, 12240, 29432, 19717, 13, 51756], "temperature": 0.0, "avg_logprob": -0.19872728983561197, "compression_ratio": 1.7904411764705883, "no_speech_prob": 0.002027960726991296}, {"id": 891, "seek": 522368, "start": 5224.56, "end": 5228.8, "text": " So I'm still thinking that perhaps just using the last language model embedding directly", "tokens": [50408, 407, 286, 478, 920, 1953, 300, 4317, 445, 1228, 264, 1036, 2856, 2316, 12240, 3584, 3838, 50620], "temperature": 0.0, "avg_logprob": -0.12488400109923713, "compression_ratio": 1.6983471074380165, "no_speech_prob": 0.001478139078244567}, {"id": 892, "seek": 522368, "start": 5230.16, "end": 5233.68, "text": " might be a better bet. And then you can just maybe", "tokens": [50688, 1062, 312, 257, 1101, 778, 13, 400, 550, 291, 393, 445, 1310, 50864], "temperature": 0.0, "avg_logprob": -0.12488400109923713, "compression_ratio": 1.6983471074380165, "no_speech_prob": 0.001478139078244567}, {"id": 893, "seek": 522368, "start": 5235.52, "end": 5240.88, "text": " use last language model to do this context thing. And then you can put this embeddings inside", "tokens": [50956, 764, 1036, 2856, 2316, 281, 360, 341, 4319, 551, 13, 400, 550, 291, 393, 829, 341, 12240, 29432, 1854, 51224], "temperature": 0.0, "avg_logprob": -0.12488400109923713, "compression_ratio": 1.6983471074380165, "no_speech_prob": 0.001478139078244567}, {"id": 894, "seek": 522368, "start": 5240.88, "end": 5246.320000000001, "text": " your knowledge graph. Like what you said earlier. If there's a way to get the embedding space", "tokens": [51224, 428, 3601, 4295, 13, 1743, 437, 291, 848, 3071, 13, 759, 456, 311, 257, 636, 281, 483, 264, 12240, 3584, 1901, 51496], "temperature": 0.0, "avg_logprob": -0.12488400109923713, "compression_ratio": 1.6983471074380165, "no_speech_prob": 0.001478139078244567}, {"id": 895, "seek": 522368, "start": 5246.320000000001, "end": 5250.8, "text": " directly from the OpenAI API, that would be great. But if not, we might have to use", "tokens": [51496, 3838, 490, 264, 7238, 48698, 9362, 11, 300, 576, 312, 869, 13, 583, 498, 406, 11, 321, 1062, 362, 281, 764, 51720], "temperature": 0.0, "avg_logprob": -0.12488400109923713, "compression_ratio": 1.6983471074380165, "no_speech_prob": 0.001478139078244567}, {"id": 896, "seek": 525080, "start": 5250.88, "end": 5253.68, "text": " Lama2 in order to do this embedding space knowledge graph.", "tokens": [50368, 441, 2404, 17, 294, 1668, 281, 360, 341, 12240, 3584, 1901, 3601, 4295, 13, 50508], "temperature": 0.0, "avg_logprob": -0.13707740708152846, "compression_ratio": 1.722943722943723, "no_speech_prob": 0.002658022101968527}, {"id": 897, "seek": 525080, "start": 5256.320000000001, "end": 5259.6, "text": " But then again, is it really necessary? Can we just use text?", "tokens": [50640, 583, 550, 797, 11, 307, 309, 534, 4818, 30, 1664, 321, 445, 764, 2487, 30, 50804], "temperature": 0.0, "avg_logprob": -0.13707740708152846, "compression_ratio": 1.722943722943723, "no_speech_prob": 0.002658022101968527}, {"id": 898, "seek": 525080, "start": 5259.6, "end": 5264.24, "text": " So this is a big open question. Should we use embedding space for the knowledge graph?", "tokens": [50804, 407, 341, 307, 257, 955, 1269, 1168, 13, 6454, 321, 764, 12240, 3584, 1901, 337, 264, 3601, 4295, 30, 51036], "temperature": 0.0, "avg_logprob": -0.13707740708152846, "compression_ratio": 1.722943722943723, "no_speech_prob": 0.002658022101968527}, {"id": 899, "seek": 525080, "start": 5264.24, "end": 5270.08, "text": " Or can we just represent it as text and then use the LM to generate embeddings after that?", "tokens": [51036, 1610, 393, 321, 445, 2906, 309, 382, 2487, 293, 550, 764, 264, 46529, 281, 8460, 12240, 29432, 934, 300, 30, 51328], "temperature": 0.0, "avg_logprob": -0.13707740708152846, "compression_ratio": 1.722943722943723, "no_speech_prob": 0.002658022101968527}, {"id": 900, "seek": 525080, "start": 5270.88, "end": 5276.4800000000005, "text": " So I leave that as an open question. I think both approaches are valid approaches. I just feel like", "tokens": [51368, 407, 286, 1856, 300, 382, 364, 1269, 1168, 13, 286, 519, 1293, 11587, 366, 7363, 11587, 13, 286, 445, 841, 411, 51648], "temperature": 0.0, "avg_logprob": -0.13707740708152846, "compression_ratio": 1.722943722943723, "no_speech_prob": 0.002658022101968527}, {"id": 901, "seek": 527648, "start": 5276.48, "end": 5281.12, "text": " the way to input the knowledge graph as text is it will be much more interpretable. And also", "tokens": [50364, 264, 636, 281, 4846, 264, 3601, 4295, 382, 2487, 307, 309, 486, 312, 709, 544, 7302, 712, 13, 400, 611, 50596], "temperature": 0.0, "avg_logprob": -0.15606245627770057, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0015255475882440805}, {"id": 902, "seek": 527648, "start": 5281.12, "end": 5286.32, "text": " you only need to train one embedding space, which is the LM embedding space. So I kind of prefer that.", "tokens": [50596, 291, 787, 643, 281, 3847, 472, 12240, 3584, 1901, 11, 597, 307, 264, 46529, 12240, 3584, 1901, 13, 407, 286, 733, 295, 4382, 300, 13, 50856], "temperature": 0.0, "avg_logprob": -0.15606245627770057, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0015255475882440805}, {"id": 903, "seek": 527648, "start": 5290.24, "end": 5292.08, "text": " Anyone else has any things to add?", "tokens": [51052, 14643, 1646, 575, 604, 721, 281, 909, 30, 51144], "temperature": 0.0, "avg_logprob": -0.15606245627770057, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0015255475882440805}, {"id": 904, "seek": 527648, "start": 5296.4, "end": 5300.639999999999, "text": " Okay, if not, we go to the next question. Can LMS help with a more flexible", "tokens": [51360, 1033, 11, 498, 406, 11, 321, 352, 281, 264, 958, 1168, 13, 1664, 441, 10288, 854, 365, 257, 544, 11358, 51572], "temperature": 0.0, "avg_logprob": -0.15606245627770057, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0015255475882440805}, {"id": 905, "seek": 527648, "start": 5300.639999999999, "end": 5305.759999999999, "text": " interpretation or construction of a knowledge graph? Okay, so our answer first. I think yes,", "tokens": [51572, 14174, 420, 6435, 295, 257, 3601, 4295, 30, 1033, 11, 370, 527, 1867, 700, 13, 286, 519, 2086, 11, 51828], "temperature": 0.0, "avg_logprob": -0.15606245627770057, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0015255475882440805}, {"id": 906, "seek": 530576, "start": 5305.76, "end": 5315.4400000000005, "text": " definitely. Just like compared to like spacey or like on noun, verb, pro verb, those kind of", "tokens": [50364, 2138, 13, 1449, 411, 5347, 281, 411, 1901, 88, 420, 411, 322, 23307, 11, 9595, 11, 447, 9595, 11, 729, 733, 295, 50848], "temperature": 0.0, "avg_logprob": -0.2825554350148077, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.008517744950950146}, {"id": 907, "seek": 530576, "start": 5315.4400000000005, "end": 5324.4800000000005, "text": " stuff, like you are doing like the parse tree, compared to those very, very flexible. And you", "tokens": [50848, 1507, 11, 411, 291, 366, 884, 411, 264, 48377, 4230, 11, 5347, 281, 729, 588, 11, 588, 11358, 13, 400, 291, 51300], "temperature": 0.0, "avg_logprob": -0.2825554350148077, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.008517744950950146}, {"id": 908, "seek": 530576, "start": 5324.4800000000005, "end": 5329.04, "text": " are able to extract a lot more information. So like just based on the string, some prompt I", "tokens": [51300, 366, 1075, 281, 8947, 257, 688, 544, 1589, 13, 407, 411, 445, 2361, 322, 264, 6798, 11, 512, 12391, 286, 51528], "temperature": 0.0, "avg_logprob": -0.2825554350148077, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.008517744950950146}, {"id": 909, "seek": 530576, "start": 5329.04, "end": 5334.56, "text": " showed you earlier, you just hit object relation object, it captures almost everything. And that's", "tokens": [51528, 4712, 291, 3071, 11, 291, 445, 2045, 2657, 9721, 2657, 11, 309, 27986, 1920, 1203, 13, 400, 300, 311, 51804], "temperature": 0.0, "avg_logprob": -0.2825554350148077, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.008517744950950146}, {"id": 910, "seek": 533456, "start": 5334.56, "end": 5339.360000000001, "text": " zero shot prompting. Granted, it did not capture the date at first. I had to use the examples to", "tokens": [50364, 4018, 3347, 12391, 278, 13, 2606, 15587, 11, 309, 630, 406, 7983, 264, 4002, 412, 700, 13, 286, 632, 281, 764, 264, 5110, 281, 50604], "temperature": 0.0, "avg_logprob": -0.13054643828293372, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.001672896440140903}, {"id": 911, "seek": 533456, "start": 5339.360000000001, "end": 5345.52, "text": " give you the date. But compared to using this kind of like spacey and so on, like deep learning", "tokens": [50604, 976, 291, 264, 4002, 13, 583, 5347, 281, 1228, 341, 733, 295, 411, 1901, 88, 293, 370, 322, 11, 411, 2452, 2539, 50912], "temperature": 0.0, "avg_logprob": -0.13054643828293372, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.001672896440140903}, {"id": 912, "seek": 533456, "start": 5345.52, "end": 5351.4400000000005, "text": " approaches, like you'll take quite long to train a new kind of like knowledge graph constructor.", "tokens": [50912, 11587, 11, 411, 291, 603, 747, 1596, 938, 281, 3847, 257, 777, 733, 295, 411, 3601, 4295, 47479, 13, 51208], "temperature": 0.0, "avg_logprob": -0.13054643828293372, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.001672896440140903}, {"id": 913, "seek": 533456, "start": 5351.4400000000005, "end": 5355.120000000001, "text": " But with large language model, you can just use prompt engineering and get your knowledge graph", "tokens": [51208, 583, 365, 2416, 2856, 2316, 11, 291, 393, 445, 764, 12391, 7043, 293, 483, 428, 3601, 4295, 51392], "temperature": 0.0, "avg_logprob": -0.13054643828293372, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.001672896440140903}, {"id": 914, "seek": 533456, "start": 5355.120000000001, "end": 5362.400000000001, "text": " out. I think that's very exciting. Okay, last question. How do we know what nodes are important", "tokens": [51392, 484, 13, 286, 519, 300, 311, 588, 4670, 13, 1033, 11, 1036, 1168, 13, 1012, 360, 321, 458, 437, 13891, 366, 1021, 51756], "temperature": 0.0, "avg_logprob": -0.13054643828293372, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.001672896440140903}, {"id": 915, "seek": 536240, "start": 5362.4, "end": 5366.719999999999, "text": " to construct in the knowledge graph? Okay, because there's a lot of information, but not", "tokens": [50364, 281, 7690, 294, 264, 3601, 4295, 30, 1033, 11, 570, 456, 311, 257, 688, 295, 1589, 11, 457, 406, 50580], "temperature": 0.0, "avg_logprob": -0.0980761419866503, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.007755037397146225}, {"id": 916, "seek": 536240, "start": 5366.719999999999, "end": 5374.48, "text": " everything is needed for your use case. How do we know? Okay, so my opinion, okay, my opinion is this.", "tokens": [50580, 1203, 307, 2978, 337, 428, 764, 1389, 13, 1012, 360, 321, 458, 30, 1033, 11, 370, 452, 4800, 11, 1392, 11, 452, 4800, 307, 341, 13, 50968], "temperature": 0.0, "avg_logprob": -0.0980761419866503, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.007755037397146225}, {"id": 917, "seek": 536240, "start": 5376.0, "end": 5383.5199999999995, "text": " You need to have biases based on the domain. And what are these biases? Maybe you can have", "tokens": [51044, 509, 643, 281, 362, 32152, 2361, 322, 264, 9274, 13, 400, 437, 366, 613, 32152, 30, 2704, 291, 393, 362, 51420], "temperature": 0.0, "avg_logprob": -0.0980761419866503, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.007755037397146225}, {"id": 918, "seek": 536240, "start": 5383.5199999999995, "end": 5391.36, "text": " multiple biases. Okay, and then let's just choose the right biases later. So this is my idea of", "tokens": [51420, 3866, 32152, 13, 1033, 11, 293, 550, 718, 311, 445, 2826, 264, 558, 32152, 1780, 13, 407, 341, 307, 452, 1558, 295, 51812], "temperature": 0.0, "avg_logprob": -0.0980761419866503, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.007755037397146225}, {"id": 919, "seek": 539136, "start": 5391.36, "end": 5396.88, "text": " intelligence right now. Okay, I'll share with you. Okay, this idea of intelligence is that there's", "tokens": [50364, 7599, 558, 586, 13, 1033, 11, 286, 603, 2073, 365, 291, 13, 1033, 11, 341, 1558, 295, 7599, 307, 300, 456, 311, 50640], "temperature": 0.0, "avg_logprob": -0.16498090570623225, "compression_ratio": 1.924, "no_speech_prob": 0.004586676601320505}, {"id": 920, "seek": 539136, "start": 5396.88, "end": 5402.16, "text": " not just one abstraction space where you store your information, you store them in multiple", "tokens": [50640, 406, 445, 472, 37765, 1901, 689, 291, 3531, 428, 1589, 11, 291, 3531, 552, 294, 3866, 50904], "temperature": 0.0, "avg_logprob": -0.16498090570623225, "compression_ratio": 1.924, "no_speech_prob": 0.004586676601320505}, {"id": 921, "seek": 539136, "start": 5402.16, "end": 5408.799999999999, "text": " abstraction space. How do we get all these abstraction spaces? We basically just do rule", "tokens": [50904, 37765, 1901, 13, 1012, 360, 321, 483, 439, 613, 37765, 7673, 30, 492, 1936, 445, 360, 4978, 51236], "temperature": 0.0, "avg_logprob": -0.16498090570623225, "compression_ratio": 1.924, "no_speech_prob": 0.004586676601320505}, {"id": 922, "seek": 539136, "start": 5408.799999999999, "end": 5413.44, "text": " based abstraction, like you like maybe one domain is saying that oh, dates are important. So I store", "tokens": [51236, 2361, 37765, 11, 411, 291, 411, 1310, 472, 9274, 307, 1566, 300, 1954, 11, 11691, 366, 1021, 13, 407, 286, 3531, 51468], "temperature": 0.0, "avg_logprob": -0.16498090570623225, "compression_ratio": 1.924, "no_speech_prob": 0.004586676601320505}, {"id": 923, "seek": 539136, "start": 5413.44, "end": 5418.08, "text": " the dates. Another domain is like all people, all person's names are important. I store the person's", "tokens": [51468, 264, 11691, 13, 3996, 9274, 307, 411, 439, 561, 11, 439, 954, 311, 5288, 366, 1021, 13, 286, 3531, 264, 954, 311, 51700], "temperature": 0.0, "avg_logprob": -0.16498090570623225, "compression_ratio": 1.924, "no_speech_prob": 0.004586676601320505}, {"id": 924, "seek": 541808, "start": 5418.08, "end": 5422.88, "text": " names. Then maybe another domain will be like all places are important. I store the places.", "tokens": [50364, 5288, 13, 1396, 1310, 1071, 9274, 486, 312, 411, 439, 3190, 366, 1021, 13, 286, 3531, 264, 3190, 13, 50604], "temperature": 0.0, "avg_logprob": -0.09106431440873579, "compression_ratio": 1.7791164658634537, "no_speech_prob": 0.003287283470854163}, {"id": 925, "seek": 541808, "start": 5423.68, "end": 5428.72, "text": " Then when you want to solve the problem, okay, you will see which space is the best", "tokens": [50644, 1396, 562, 291, 528, 281, 5039, 264, 1154, 11, 1392, 11, 291, 486, 536, 597, 1901, 307, 264, 1151, 50896], "temperature": 0.0, "avg_logprob": -0.09106431440873579, "compression_ratio": 1.7791164658634537, "no_speech_prob": 0.003287283470854163}, {"id": 926, "seek": 541808, "start": 5428.72, "end": 5432.88, "text": " for your problem. Okay, you will just like, maybe you look at all the abstraction spaces,", "tokens": [50896, 337, 428, 1154, 13, 1033, 11, 291, 486, 445, 411, 11, 1310, 291, 574, 412, 439, 264, 37765, 7673, 11, 51104], "temperature": 0.0, "avg_logprob": -0.09106431440873579, "compression_ratio": 1.7791164658634537, "no_speech_prob": 0.003287283470854163}, {"id": 927, "seek": 541808, "start": 5432.88, "end": 5437.36, "text": " maybe combine two or more, or you just take one, and then whatever solves the problem works.", "tokens": [51104, 1310, 10432, 732, 420, 544, 11, 420, 291, 445, 747, 472, 11, 293, 550, 2035, 39890, 264, 1154, 1985, 13, 51328], "temperature": 0.0, "avg_logprob": -0.09106431440873579, "compression_ratio": 1.7791164658634537, "no_speech_prob": 0.003287283470854163}, {"id": 928, "seek": 541808, "start": 5438.0, "end": 5445.92, "text": " So this would form an approach that will be used later on. So if you think about it,", "tokens": [51360, 407, 341, 576, 1254, 364, 3109, 300, 486, 312, 1143, 1780, 322, 13, 407, 498, 291, 519, 466, 309, 11, 51756], "temperature": 0.0, "avg_logprob": -0.09106431440873579, "compression_ratio": 1.7791164658634537, "no_speech_prob": 0.003287283470854163}, {"id": 929, "seek": 544592, "start": 5445.92, "end": 5450.4, "text": " I'm just going to draw it here. Okay, I don't know whether I have space to draw it, but if you", "tokens": [50364, 286, 478, 445, 516, 281, 2642, 309, 510, 13, 1033, 11, 286, 500, 380, 458, 1968, 286, 362, 1901, 281, 2642, 309, 11, 457, 498, 291, 50588], "temperature": 0.0, "avg_logprob": -0.13088728169925878, "compression_ratio": 1.9535864978902953, "no_speech_prob": 0.0024928951170295477}, {"id": 930, "seek": 544592, "start": 5450.4, "end": 5454.8, "text": " look at the top right of the screen, okay, you have a problem, you have multiple abstraction", "tokens": [50588, 574, 412, 264, 1192, 558, 295, 264, 2568, 11, 1392, 11, 291, 362, 257, 1154, 11, 291, 362, 3866, 37765, 50808], "temperature": 0.0, "avg_logprob": -0.13088728169925878, "compression_ratio": 1.9535864978902953, "no_speech_prob": 0.0024928951170295477}, {"id": 931, "seek": 544592, "start": 5454.8, "end": 5459.2, "text": " spaces, let's call this A. You have another abstraction space here, let's call it B.", "tokens": [50808, 7673, 11, 718, 311, 818, 341, 316, 13, 509, 362, 1071, 37765, 1901, 510, 11, 718, 311, 818, 309, 363, 13, 51028], "temperature": 0.0, "avg_logprob": -0.13088728169925878, "compression_ratio": 1.9535864978902953, "no_speech_prob": 0.0024928951170295477}, {"id": 932, "seek": 544592, "start": 5460.72, "end": 5465.68, "text": " And we have another expression space, we call it C. All right, so if we have three", "tokens": [51104, 400, 321, 362, 1071, 6114, 1901, 11, 321, 818, 309, 383, 13, 1057, 558, 11, 370, 498, 321, 362, 1045, 51352], "temperature": 0.0, "avg_logprob": -0.13088728169925878, "compression_ratio": 1.9535864978902953, "no_speech_prob": 0.0024928951170295477}, {"id": 933, "seek": 544592, "start": 5465.68, "end": 5471.2, "text": " abstraction spaces like that, okay, these are like three ways of doing it, three ways of doing the problem.", "tokens": [51352, 37765, 7673, 411, 300, 11, 1392, 11, 613, 366, 411, 1045, 2098, 295, 884, 309, 11, 1045, 2098, 295, 884, 264, 1154, 13, 51628], "temperature": 0.0, "avg_logprob": -0.13088728169925878, "compression_ratio": 1.9535864978902953, "no_speech_prob": 0.0024928951170295477}, {"id": 934, "seek": 547120, "start": 5471.5199999999995, "end": 5480.0, "text": " And then in order to solve any arbitrary problem later, you just take mix and match the abstraction", "tokens": [50380, 400, 550, 294, 1668, 281, 5039, 604, 23211, 1154, 1780, 11, 291, 445, 747, 2890, 293, 2995, 264, 37765, 50804], "temperature": 0.0, "avg_logprob": -0.21043868803642166, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.0022843386977910995}, {"id": 935, "seek": 547120, "start": 5480.0, "end": 5486.48, "text": " spaces to solve the problem. So yeah, increasingly, I've been feeling like this is the way to do things.", "tokens": [50804, 7673, 281, 5039, 264, 1154, 13, 407, 1338, 11, 12980, 11, 286, 600, 668, 2633, 411, 341, 307, 264, 636, 281, 360, 721, 13, 51128], "temperature": 0.0, "avg_logprob": -0.21043868803642166, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.0022843386977910995}, {"id": 936, "seek": 547120, "start": 5487.04, "end": 5499.92, "text": " So yeah, I also use this for my abstraction reasoning copless paper. So this is the idea", "tokens": [51156, 407, 1338, 11, 286, 611, 764, 341, 337, 452, 37765, 21577, 2971, 1832, 3035, 13, 407, 341, 307, 264, 1558, 51800], "temperature": 0.0, "avg_logprob": -0.21043868803642166, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.0022843386977910995}, {"id": 937, "seek": 549992, "start": 5499.92, "end": 5503.6, "text": " that I have right now. You have different abstraction spaces. All these are rule-based.", "tokens": [50364, 300, 286, 362, 558, 586, 13, 509, 362, 819, 37765, 7673, 13, 1057, 613, 366, 4978, 12, 6032, 13, 50548], "temperature": 0.0, "avg_logprob": -0.13129448890686035, "compression_ratio": 1.937956204379562, "no_speech_prob": 0.015669791027903557}, {"id": 938, "seek": 549992, "start": 5504.32, "end": 5508.4, "text": " Okay, we don't really have deep learning here, because if you have deep learning, you'll have", "tokens": [50584, 1033, 11, 321, 500, 380, 534, 362, 2452, 2539, 510, 11, 570, 498, 291, 362, 2452, 2539, 11, 291, 603, 362, 50788], "temperature": 0.0, "avg_logprob": -0.13129448890686035, "compression_ratio": 1.937956204379562, "no_speech_prob": 0.015669791027903557}, {"id": 939, "seek": 549992, "start": 5508.4, "end": 5513.68, "text": " problems in getting a fixed abstraction space. You don't want the abstraction space to change.", "tokens": [50788, 2740, 294, 1242, 257, 6806, 37765, 1901, 13, 509, 500, 380, 528, 264, 37765, 1901, 281, 1319, 13, 51052], "temperature": 0.0, "avg_logprob": -0.13129448890686035, "compression_ratio": 1.937956204379562, "no_speech_prob": 0.015669791027903557}, {"id": 940, "seek": 549992, "start": 5514.24, "end": 5517.6, "text": " Because if you change this abstraction space, you have to change whatever you learn on it.", "tokens": [51080, 1436, 498, 291, 1319, 341, 37765, 1901, 11, 291, 362, 281, 1319, 2035, 291, 1466, 322, 309, 13, 51248], "temperature": 0.0, "avg_logprob": -0.13129448890686035, "compression_ratio": 1.937956204379562, "no_speech_prob": 0.015669791027903557}, {"id": 941, "seek": 549992, "start": 5518.16, "end": 5522.4, "text": " It's like if I suddenly told you that math, the addition is now subtraction,", "tokens": [51276, 467, 311, 411, 498, 286, 5800, 1907, 291, 300, 5221, 11, 264, 4500, 307, 586, 16390, 313, 11, 51488], "temperature": 0.0, "avg_logprob": -0.13129448890686035, "compression_ratio": 1.937956204379562, "no_speech_prob": 0.015669791027903557}, {"id": 942, "seek": 549992, "start": 5522.4, "end": 5526.4800000000005, "text": " that I have to relearn all my math again, because I need to update that new knowledge.", "tokens": [51488, 300, 286, 362, 281, 2951, 1083, 439, 452, 5221, 797, 11, 570, 286, 643, 281, 5623, 300, 777, 3601, 13, 51692], "temperature": 0.0, "avg_logprob": -0.13129448890686035, "compression_ratio": 1.937956204379562, "no_speech_prob": 0.015669791027903557}, {"id": 943, "seek": 552648, "start": 5526.48, "end": 5530.799999999999, "text": " So I'm saying that the basis is fixed, but then you just choose the right basis to solve it.", "tokens": [50364, 407, 286, 478, 1566, 300, 264, 5143, 307, 6806, 11, 457, 550, 291, 445, 2826, 264, 558, 5143, 281, 5039, 309, 13, 50580], "temperature": 0.0, "avg_logprob": -0.0885045802006956, "compression_ratio": 1.8509803921568628, "no_speech_prob": 0.004122478421777487}, {"id": 944, "seek": 552648, "start": 5531.44, "end": 5536.08, "text": " Okay, then you might ask me, if we do it like that, what if we don't have the right basis to", "tokens": [50612, 1033, 11, 550, 291, 1062, 1029, 385, 11, 498, 321, 360, 309, 411, 300, 11, 437, 498, 321, 500, 380, 362, 264, 558, 5143, 281, 50844], "temperature": 0.0, "avg_logprob": -0.0885045802006956, "compression_ratio": 1.8509803921568628, "no_speech_prob": 0.004122478421777487}, {"id": 945, "seek": 552648, "start": 5536.08, "end": 5541.44, "text": " solve the problem? Okay, then the answer is you can't solve the problem. Okay, which might sound", "tokens": [50844, 5039, 264, 1154, 30, 1033, 11, 550, 264, 1867, 307, 291, 393, 380, 5039, 264, 1154, 13, 1033, 11, 597, 1062, 1626, 51112], "temperature": 0.0, "avg_logprob": -0.0885045802006956, "compression_ratio": 1.8509803921568628, "no_speech_prob": 0.004122478421777487}, {"id": 946, "seek": 552648, "start": 5541.44, "end": 5546.879999999999, "text": " a bit crude to people, but I feel like we can't solve everything. Like even humans, we have our", "tokens": [51112, 257, 857, 30796, 281, 561, 11, 457, 286, 841, 411, 321, 393, 380, 5039, 1203, 13, 1743, 754, 6255, 11, 321, 362, 527, 51384], "temperature": 0.0, "avg_logprob": -0.0885045802006956, "compression_ratio": 1.8509803921568628, "no_speech_prob": 0.004122478421777487}, {"id": 947, "seek": 552648, "start": 5546.879999999999, "end": 5553.28, "text": " limitations. It's just that we work around our limitations and try to use our existing biases", "tokens": [51384, 15705, 13, 467, 311, 445, 300, 321, 589, 926, 527, 15705, 293, 853, 281, 764, 527, 6741, 32152, 51704], "temperature": 0.0, "avg_logprob": -0.0885045802006956, "compression_ratio": 1.8509803921568628, "no_speech_prob": 0.004122478421777487}, {"id": 948, "seek": 555328, "start": 5553.36, "end": 5557.12, "text": " to solve new problems. And I think that's intelligence. We don't really need to change", "tokens": [50368, 281, 5039, 777, 2740, 13, 400, 286, 519, 300, 311, 7599, 13, 492, 500, 380, 534, 643, 281, 1319, 50556], "temperature": 0.0, "avg_logprob": -0.1130746613948717, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.002287329640239477}, {"id": 949, "seek": 555328, "start": 5557.12, "end": 5561.36, "text": " the abstraction spaces. We can just work with getting multiple abstraction spaces and then", "tokens": [50556, 264, 37765, 7673, 13, 492, 393, 445, 589, 365, 1242, 3866, 37765, 7673, 293, 550, 50768], "temperature": 0.0, "avg_logprob": -0.1130746613948717, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.002287329640239477}, {"id": 950, "seek": 555328, "start": 5561.36, "end": 5567.44, "text": " just combining them. So I shared a bit about my view of intelligence here. And yeah,", "tokens": [50768, 445, 21928, 552, 13, 407, 286, 5507, 257, 857, 466, 452, 1910, 295, 7599, 510, 13, 400, 1338, 11, 51072], "temperature": 0.0, "avg_logprob": -0.1130746613948717, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.002287329640239477}, {"id": 951, "seek": 555328, "start": 5568.5599999999995, "end": 5573.759999999999, "text": " that's more or less it for the questions to ponder. Anyone has anything else to add for any of these", "tokens": [51128, 300, 311, 544, 420, 1570, 309, 337, 264, 1651, 281, 280, 8548, 13, 14643, 575, 1340, 1646, 281, 909, 337, 604, 295, 613, 51388], "temperature": 0.0, "avg_logprob": -0.1130746613948717, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.002287329640239477}, {"id": 952, "seek": 555328, "start": 5573.759999999999, "end": 5579.759999999999, "text": " questions? I just want to clarify one thing. I'm still not very sure about the motivation here.", "tokens": [51388, 1651, 30, 286, 445, 528, 281, 17594, 472, 551, 13, 286, 478, 920, 406, 588, 988, 466, 264, 12335, 510, 13, 51688], "temperature": 0.0, "avg_logprob": -0.1130746613948717, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.002287329640239477}, {"id": 953, "seek": 557976, "start": 5579.76, "end": 5588.08, "text": " So why we want this knowledge graph? Why use a knowledge graph, is it?", "tokens": [50364, 407, 983, 321, 528, 341, 3601, 4295, 30, 1545, 764, 257, 3601, 4295, 11, 307, 309, 30, 50780], "temperature": 0.0, "avg_logprob": -0.3151908065333511, "compression_ratio": 1.2626262626262625, "no_speech_prob": 0.0028542999643832445}, {"id": 954, "seek": 557976, "start": 5590.16, "end": 5594.400000000001, "text": " Because the internal representation of LLMs is already", "tokens": [50884, 1436, 264, 6920, 10290, 295, 441, 43, 26386, 307, 1217, 51096], "temperature": 0.0, "avg_logprob": -0.3151908065333511, "compression_ratio": 1.2626262626262625, "no_speech_prob": 0.0028542999643832445}, {"id": 955, "seek": 559440, "start": 5595.2, "end": 5611.759999999999, "text": " already richer than a knowledge graph. So for example, you can just use the", "tokens": [50404, 1217, 29021, 813, 257, 3601, 4295, 13, 407, 337, 1365, 11, 291, 393, 445, 764, 264, 51232], "temperature": 0.0, "avg_logprob": -0.36571183315543243, "compression_ratio": 1.3106060606060606, "no_speech_prob": 0.010930039919912815}, {"id": 956, "seek": 559440, "start": 5612.719999999999, "end": 5620.4, "text": " last language model to help you to break down the task. Actually, because the way I did it before", "tokens": [51280, 1036, 2856, 2316, 281, 854, 291, 281, 1821, 760, 264, 5633, 13, 5135, 11, 570, 264, 636, 286, 630, 309, 949, 51664], "temperature": 0.0, "avg_logprob": -0.36571183315543243, "compression_ratio": 1.3106060606060606, "no_speech_prob": 0.010930039919912815}, {"id": 957, "seek": 562040, "start": 5620.4, "end": 5625.36, "text": " is I want to specifically construct the knowledge graph first. Then I use that knowledge graph to", "tokens": [50364, 307, 286, 528, 281, 4682, 7690, 264, 3601, 4295, 700, 13, 1396, 286, 764, 300, 3601, 4295, 281, 50612], "temperature": 0.0, "avg_logprob": -0.22595588684082032, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.010162740014493465}, {"id": 958, "seek": 562040, "start": 5625.36, "end": 5631.44, "text": " break down the task from a hierarchical or sequential learning of intermediate", "tokens": [50612, 1821, 760, 264, 5633, 490, 257, 35250, 804, 420, 42881, 2539, 295, 19376, 50916], "temperature": 0.0, "avg_logprob": -0.22595588684082032, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.010162740014493465}, {"id": 959, "seek": 562040, "start": 5631.44, "end": 5638.16, "text": " goal to RL. But in the sense, you don't really need that particular task at least,", "tokens": [50916, 3387, 281, 497, 43, 13, 583, 294, 264, 2020, 11, 291, 500, 380, 534, 643, 300, 1729, 5633, 412, 1935, 11, 51252], "temperature": 0.0, "avg_logprob": -0.22595588684082032, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.010162740014493465}, {"id": 960, "seek": 562040, "start": 5638.16, "end": 5642.0, "text": " you don't really need to do that anymore because you just query the last language model.", "tokens": [51252, 291, 500, 380, 534, 643, 281, 360, 300, 3602, 570, 291, 445, 14581, 264, 1036, 2856, 2316, 13, 51444], "temperature": 0.0, "avg_logprob": -0.22595588684082032, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.010162740014493465}, {"id": 961, "seek": 562040, "start": 5643.92, "end": 5649.36, "text": " At least the common sense way of breaking down a certain task. So you already have the", "tokens": [51540, 1711, 1935, 264, 2689, 2020, 636, 295, 7697, 760, 257, 1629, 5633, 13, 407, 291, 1217, 362, 264, 51812], "temperature": 0.0, "avg_logprob": -0.22595588684082032, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.010162740014493465}, {"id": 962, "seek": 564936, "start": 5650.16, "end": 5658.48, "text": " different sub tasks. So because my understanding is more coming from that perspective,", "tokens": [50404, 819, 1422, 9608, 13, 407, 570, 452, 3701, 307, 544, 1348, 490, 300, 4585, 11, 50820], "temperature": 0.0, "avg_logprob": -0.22912609702662418, "compression_ratio": 1.5960784313725491, "no_speech_prob": 0.005122693721204996}, {"id": 963, "seek": 564936, "start": 5658.48, "end": 5662.5599999999995, "text": " so I'm thinking why do I still need the knowledge graph?", "tokens": [50820, 370, 286, 478, 1953, 983, 360, 286, 920, 643, 264, 3601, 4295, 30, 51024], "temperature": 0.0, "avg_logprob": -0.22912609702662418, "compression_ratio": 1.5960784313725491, "no_speech_prob": 0.005122693721204996}, {"id": 964, "seek": 564936, "start": 5664.4, "end": 5667.92, "text": " I mean, as what I said in one of the first few slides, if you use retrieval or method", "tokens": [51116, 286, 914, 11, 382, 437, 286, 848, 294, 472, 295, 264, 700, 1326, 9788, 11, 498, 291, 764, 19817, 3337, 420, 3170, 51292], "temperature": 0.0, "avg_logprob": -0.22912609702662418, "compression_ratio": 1.5960784313725491, "no_speech_prob": 0.005122693721204996}, {"id": 965, "seek": 564936, "start": 5667.92, "end": 5672.88, "text": " generation, you might miss out certain chunks of text. Knowledge graph provides a sort of hierarchy", "tokens": [51292, 5125, 11, 291, 1062, 1713, 484, 1629, 24004, 295, 2487, 13, 32906, 4295, 6417, 257, 1333, 295, 22333, 51540], "temperature": 0.0, "avg_logprob": -0.22912609702662418, "compression_ratio": 1.5960784313725491, "no_speech_prob": 0.005122693721204996}, {"id": 966, "seek": 564936, "start": 5672.88, "end": 5677.599999999999, "text": " that you can pass over and extract information. So it provides the structure.", "tokens": [51540, 300, 291, 393, 1320, 670, 293, 8947, 1589, 13, 407, 309, 6417, 264, 3877, 13, 51776], "temperature": 0.0, "avg_logprob": -0.22912609702662418, "compression_ratio": 1.5960784313725491, "no_speech_prob": 0.005122693721204996}, {"id": 967, "seek": 567760, "start": 5678.56, "end": 5683.92, "text": " But you still mentioned the problem is like even with knowledge, you have to", "tokens": [50412, 583, 291, 920, 2835, 264, 1154, 307, 411, 754, 365, 3601, 11, 291, 362, 281, 50680], "temperature": 0.0, "avg_logprob": -0.3067342807085086, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0018443894805386662}, {"id": 968, "seek": 567760, "start": 5683.92, "end": 5690.240000000001, "text": " distill information. So there is inevitable that certain information are lost. So it be", "tokens": [50680, 42923, 1589, 13, 407, 456, 307, 21451, 300, 1629, 1589, 366, 2731, 13, 407, 309, 312, 50996], "temperature": 0.0, "avg_logprob": -0.3067342807085086, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0018443894805386662}, {"id": 969, "seek": 567760, "start": 5690.240000000001, "end": 5696.240000000001, "text": " it in a way of like you do chunk first, chunking first, then you summarize each chunk.", "tokens": [50996, 309, 294, 257, 636, 295, 411, 291, 360, 16635, 700, 11, 16635, 278, 700, 11, 550, 291, 20858, 1184, 16635, 13, 51296], "temperature": 0.0, "avg_logprob": -0.3067342807085086, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0018443894805386662}, {"id": 970, "seek": 567760, "start": 5696.240000000001, "end": 5699.76, "text": " Then you summarize again, maybe like a hierarchical way of summarizing and all", "tokens": [51296, 1396, 291, 20858, 797, 11, 1310, 411, 257, 35250, 804, 636, 295, 14611, 3319, 293, 439, 51472], "temperature": 0.0, "avg_logprob": -0.3067342807085086, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0018443894805386662}, {"id": 971, "seek": 569976, "start": 5700.4800000000005, "end": 5708.320000000001, "text": " even your knowledge graph, you only extract that the relation you think is important between", "tokens": [50400, 754, 428, 3601, 4295, 11, 291, 787, 8947, 300, 264, 9721, 291, 519, 307, 1021, 1296, 50792], "temperature": 0.0, "avg_logprob": -0.26246293985618735, "compression_ratio": 1.5222929936305734, "no_speech_prob": 0.004881155677139759}, {"id": 972, "seek": 569976, "start": 5708.320000000001, "end": 5715.76, "text": " different entities. So you face this issue of having information lost.", "tokens": [50792, 819, 16667, 13, 407, 291, 1851, 341, 2734, 295, 1419, 1589, 2731, 13, 51164], "temperature": 0.0, "avg_logprob": -0.26246293985618735, "compression_ratio": 1.5222929936305734, "no_speech_prob": 0.004881155677139759}, {"id": 973, "seek": 569976, "start": 5720.96, "end": 5725.76, "text": " Yeah, so maybe the correct way is not the existing kind of knowledge graph,", "tokens": [51424, 865, 11, 370, 1310, 264, 3006, 636, 307, 406, 264, 6741, 733, 295, 3601, 4295, 11, 51664], "temperature": 0.0, "avg_logprob": -0.26246293985618735, "compression_ratio": 1.5222929936305734, "no_speech_prob": 0.004881155677139759}, {"id": 974, "seek": 572576, "start": 5726.64, "end": 5731.4400000000005, "text": " but a new kind of knowledge graph. But I do believe a graph based representation of knowledge is", "tokens": [50408, 457, 257, 777, 733, 295, 3601, 4295, 13, 583, 286, 360, 1697, 257, 4295, 2361, 10290, 295, 3601, 307, 50648], "temperature": 0.0, "avg_logprob": -0.12786924516832507, "compression_ratio": 1.7388059701492538, "no_speech_prob": 0.004992300644516945}, {"id": 975, "seek": 572576, "start": 5731.4400000000005, "end": 5736.400000000001, "text": " important because when you learn new things, we typically try to fit in with our existing", "tokens": [50648, 1021, 570, 562, 291, 1466, 777, 721, 11, 321, 5850, 853, 281, 3318, 294, 365, 527, 6741, 50896], "temperature": 0.0, "avg_logprob": -0.12786924516832507, "compression_ratio": 1.7388059701492538, "no_speech_prob": 0.004992300644516945}, {"id": 976, "seek": 572576, "start": 5736.400000000001, "end": 5740.8, "text": " knowledge and we build on the knowledge from there. So if you have some form of graph structure", "tokens": [50896, 3601, 293, 321, 1322, 322, 264, 3601, 490, 456, 13, 407, 498, 291, 362, 512, 1254, 295, 4295, 3877, 51116], "temperature": 0.0, "avg_logprob": -0.12786924516832507, "compression_ratio": 1.7388059701492538, "no_speech_prob": 0.004992300644516945}, {"id": 977, "seek": 572576, "start": 5740.8, "end": 5746.24, "text": " to represent knowledge, you can actually like use that for learning as well. And that's where", "tokens": [51116, 281, 2906, 3601, 11, 291, 393, 767, 411, 764, 300, 337, 2539, 382, 731, 13, 400, 300, 311, 689, 51388], "temperature": 0.0, "avg_logprob": -0.12786924516832507, "compression_ratio": 1.7388059701492538, "no_speech_prob": 0.004992300644516945}, {"id": 978, "seek": 572576, "start": 5746.24, "end": 5755.4400000000005, "text": " I'm coming from. My intuition is it can be okay. So in that case, I can see how it can be", "tokens": [51388, 286, 478, 1348, 490, 13, 1222, 24002, 307, 309, 393, 312, 1392, 13, 407, 294, 300, 1389, 11, 286, 393, 536, 577, 309, 393, 312, 51848], "temperature": 0.0, "avg_logprob": -0.12786924516832507, "compression_ratio": 1.7388059701492538, "no_speech_prob": 0.004992300644516945}, {"id": 979, "seek": 575544, "start": 5755.44, "end": 5761.5199999999995, "text": " useful. It can be served as a heuristic for search. So if you want to understand a very", "tokens": [50364, 4420, 13, 467, 393, 312, 7584, 382, 257, 415, 374, 3142, 337, 3164, 13, 407, 498, 291, 528, 281, 1223, 257, 588, 50668], "temperature": 0.0, "avg_logprob": -0.1953533083893532, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.00355519843287766}, {"id": 980, "seek": 575544, "start": 5762.4, "end": 5770.48, "text": " large chunk of text, if you extract like a rough graph representation of the information,", "tokens": [50712, 2416, 16635, 295, 2487, 11, 498, 291, 8947, 411, 257, 5903, 4295, 10290, 295, 264, 1589, 11, 51116], "temperature": 0.0, "avg_logprob": -0.1953533083893532, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.00355519843287766}, {"id": 981, "seek": 575544, "start": 5770.48, "end": 5777.04, "text": " then you just do a heuristic search based on that. So even if this information loss is still can", "tokens": [51116, 550, 291, 445, 360, 257, 415, 374, 3142, 3164, 2361, 322, 300, 13, 407, 754, 498, 341, 1589, 4470, 307, 920, 393, 51444], "temperature": 0.0, "avg_logprob": -0.1953533083893532, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.00355519843287766}, {"id": 982, "seek": 575544, "start": 5778.0, "end": 5782.879999999999, "text": " expand upon just based on your existing knowledge graph, maybe it just gave you", "tokens": [51492, 5268, 3564, 445, 2361, 322, 428, 6741, 3601, 4295, 11, 1310, 309, 445, 2729, 291, 51736], "temperature": 0.0, "avg_logprob": -0.1953533083893532, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.00355519843287766}, {"id": 983, "seek": 578288, "start": 5783.52, "end": 5787.6, "text": " some useful signal to tell you like which part of the text you want to do some search,", "tokens": [50396, 512, 4420, 6358, 281, 980, 291, 411, 597, 644, 295, 264, 2487, 291, 528, 281, 360, 512, 3164, 11, 50600], "temperature": 0.0, "avg_logprob": -0.14514804304691784, "compression_ratio": 1.7490196078431373, "no_speech_prob": 0.0029292032122612}, {"id": 984, "seek": 578288, "start": 5787.6, "end": 5791.92, "text": " then you can just go and search. You don't need necessarily to stick with the strictly stick", "tokens": [50600, 550, 291, 393, 445, 352, 293, 3164, 13, 509, 500, 380, 643, 4725, 281, 2897, 365, 264, 20792, 2897, 50816], "temperature": 0.0, "avg_logprob": -0.14514804304691784, "compression_ratio": 1.7490196078431373, "no_speech_prob": 0.0029292032122612}, {"id": 985, "seek": 578288, "start": 5791.92, "end": 5796.8, "text": " with the graph. So in that way, like just a heuristic but it's a useful one.", "tokens": [50816, 365, 264, 4295, 13, 407, 294, 300, 636, 11, 411, 445, 257, 415, 374, 3142, 457, 309, 311, 257, 4420, 472, 13, 51060], "temperature": 0.0, "avg_logprob": -0.14514804304691784, "compression_ratio": 1.7490196078431373, "no_speech_prob": 0.0029292032122612}, {"id": 986, "seek": 578288, "start": 5797.6, "end": 5803.52, "text": " Yeah, yeah. That's one way of doing it, like using a heuristic to search. So it's like replacing", "tokens": [51100, 865, 11, 1338, 13, 663, 311, 472, 636, 295, 884, 309, 11, 411, 1228, 257, 415, 374, 3142, 281, 3164, 13, 407, 309, 311, 411, 19139, 51396], "temperature": 0.0, "avg_logprob": -0.14514804304691784, "compression_ratio": 1.7490196078431373, "no_speech_prob": 0.0029292032122612}, {"id": 987, "seek": 578288, "start": 5803.52, "end": 5808.08, "text": " the cosine similarity in retrieval of mental generation. You just pass through the knowledge", "tokens": [51396, 264, 23565, 32194, 294, 19817, 3337, 295, 4973, 5125, 13, 509, 445, 1320, 807, 264, 3601, 51624], "temperature": 0.0, "avg_logprob": -0.14514804304691784, "compression_ratio": 1.7490196078431373, "no_speech_prob": 0.0029292032122612}, {"id": 988, "seek": 580808, "start": 5808.08, "end": 5814.96, "text": " graph. Yeah. Because the chunking in like just naive way, it's just, it's just like because the", "tokens": [50364, 4295, 13, 865, 13, 1436, 264, 16635, 278, 294, 411, 445, 29052, 636, 11, 309, 311, 445, 11, 309, 311, 445, 411, 570, 264, 50708], "temperature": 0.0, "avg_logprob": -0.3094032511991613, "compression_ratio": 1.748792270531401, "no_speech_prob": 0.007135230116546154}, {"id": 989, "seek": 580808, "start": 5814.96, "end": 5824.72, "text": " way we structure essay or structure the text is not necessarily just like each different", "tokens": [50708, 636, 321, 3877, 16238, 420, 3877, 264, 2487, 307, 406, 4725, 445, 411, 1184, 819, 51196], "temperature": 0.0, "avg_logprob": -0.3094032511991613, "compression_ratio": 1.748792270531401, "no_speech_prob": 0.007135230116546154}, {"id": 990, "seek": 580808, "start": 5825.76, "end": 5831.12, "text": " hierarchy like different object or different something that is quite intricate, right?", "tokens": [51248, 22333, 411, 819, 2657, 420, 819, 746, 300, 307, 1596, 38015, 11, 558, 30, 51516], "temperature": 0.0, "avg_logprob": -0.3094032511991613, "compression_ratio": 1.748792270531401, "no_speech_prob": 0.007135230116546154}, {"id": 991, "seek": 580808, "start": 5831.12, "end": 5836.64, "text": " If for example, for some novel, you have foreshadowing, you have different way of writing,", "tokens": [51516, 759, 337, 1365, 11, 337, 512, 7613, 11, 291, 362, 2091, 2716, 345, 9637, 11, 291, 362, 819, 636, 295, 3579, 11, 51792], "temperature": 0.0, "avg_logprob": -0.3094032511991613, "compression_ratio": 1.748792270531401, "no_speech_prob": 0.007135230116546154}, {"id": 992, "seek": 583664, "start": 5836.64, "end": 5841.68, "text": " you have plot device, then you suddenly at a certain very, you think apparently very random", "tokens": [50364, 291, 362, 7542, 4302, 11, 550, 291, 5800, 412, 257, 1629, 588, 11, 291, 519, 7970, 588, 4974, 50616], "temperature": 0.0, "avg_logprob": -0.17863444624275998, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.0020402607042342424}, {"id": 993, "seek": 583664, "start": 5841.68, "end": 5846.160000000001, "text": " point suddenly become very important. So just by naively chunking into like evenly", "tokens": [50616, 935, 5800, 1813, 588, 1021, 13, 407, 445, 538, 1667, 3413, 16635, 278, 666, 411, 17658, 50840], "temperature": 0.0, "avg_logprob": -0.17863444624275998, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.0020402607042342424}, {"id": 994, "seek": 583664, "start": 5847.84, "end": 5853.280000000001, "text": " it doesn't, it doesn't work. But if you have a graph like sort of tells you this kind of structure,", "tokens": [50924, 309, 1177, 380, 11, 309, 1177, 380, 589, 13, 583, 498, 291, 362, 257, 4295, 411, 1333, 295, 5112, 291, 341, 733, 295, 3877, 11, 51196], "temperature": 0.0, "avg_logprob": -0.17863444624275998, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.0020402607042342424}, {"id": 995, "seek": 583664, "start": 5853.280000000001, "end": 5859.280000000001, "text": " then you're based on that to do some similarity. Definitely I feel like it can be more efficient.", "tokens": [51196, 550, 291, 434, 2361, 322, 300, 281, 360, 512, 32194, 13, 12151, 286, 841, 411, 309, 393, 312, 544, 7148, 13, 51496], "temperature": 0.0, "avg_logprob": -0.17863444624275998, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.0020402607042342424}, {"id": 996, "seek": 583664, "start": 5859.280000000001, "end": 5866.240000000001, "text": " So I think they give you a better way of doing chunking. Yeah. Yeah, I like this approach. In", "tokens": [51496, 407, 286, 519, 436, 976, 291, 257, 1101, 636, 295, 884, 16635, 278, 13, 865, 13, 865, 11, 286, 411, 341, 3109, 13, 682, 51844], "temperature": 0.0, "avg_logprob": -0.17863444624275998, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.0020402607042342424}, {"id": 997, "seek": 586624, "start": 5866.24, "end": 5872.0, "text": " fact, that's one of the motivations of using knowledge graph as well in order to find a better", "tokens": [50364, 1186, 11, 300, 311, 472, 295, 264, 39034, 295, 1228, 3601, 4295, 382, 731, 294, 1668, 281, 915, 257, 1101, 50652], "temperature": 0.0, "avg_logprob": -0.13697029460560192, "compression_ratio": 1.9510204081632654, "no_speech_prob": 0.001012445893138647}, {"id": 998, "seek": 586624, "start": 5872.0, "end": 5878.719999999999, "text": " way to pass it. I mean, we can pass through graphs pretty well. So yeah, we can perhaps get better", "tokens": [50652, 636, 281, 1320, 309, 13, 286, 914, 11, 321, 393, 1320, 807, 24877, 1238, 731, 13, 407, 1338, 11, 321, 393, 4317, 483, 1101, 50988], "temperature": 0.0, "avg_logprob": -0.13697029460560192, "compression_ratio": 1.9510204081632654, "no_speech_prob": 0.001012445893138647}, {"id": 999, "seek": 586624, "start": 5878.719999999999, "end": 5883.04, "text": " retrieval using knowledge graphs. So that's one provided you have all the information in your", "tokens": [50988, 19817, 3337, 1228, 3601, 24877, 13, 407, 300, 311, 472, 5649, 291, 362, 439, 264, 1589, 294, 428, 51204], "temperature": 0.0, "avg_logprob": -0.13697029460560192, "compression_ratio": 1.9510204081632654, "no_speech_prob": 0.001012445893138647}, {"id": 1000, "seek": 586624, "start": 5883.04, "end": 5887.36, "text": " knowledge graph, you can get better retrieval using knowledge graphs provided the first part", "tokens": [51204, 3601, 4295, 11, 291, 393, 483, 1101, 19817, 3337, 1228, 3601, 24877, 5649, 264, 700, 644, 51420], "temperature": 0.0, "avg_logprob": -0.13697029460560192, "compression_ratio": 1.9510204081632654, "no_speech_prob": 0.001012445893138647}, {"id": 1001, "seek": 586624, "start": 5887.36, "end": 5892.48, "text": " again, because that's the failure case we saw just now in the length chain graph answering agent.", "tokens": [51420, 797, 11, 570, 300, 311, 264, 7763, 1389, 321, 1866, 445, 586, 294, 264, 4641, 5021, 4295, 13430, 9461, 13, 51676], "temperature": 0.0, "avg_logprob": -0.13697029460560192, "compression_ratio": 1.9510204081632654, "no_speech_prob": 0.001012445893138647}, {"id": 1002, "seek": 589248, "start": 5892.719999999999, "end": 5898.799999999999, "text": " But anyway, I think all the logition all stand from the fact that the context window is very limited.", "tokens": [50376, 583, 4033, 11, 286, 519, 439, 264, 3565, 849, 439, 1463, 490, 264, 1186, 300, 264, 4319, 4910, 307, 588, 5567, 13, 50680], "temperature": 0.0, "avg_logprob": -0.3062615679879474, "compression_ratio": 1.658450704225352, "no_speech_prob": 0.000973651884123683}, {"id": 1003, "seek": 589248, "start": 5898.799999999999, "end": 5907.2, "text": " So if we can solve that problem, then here actually, I mean, both way either way.", "tokens": [50680, 407, 498, 321, 393, 5039, 300, 1154, 11, 550, 510, 767, 11, 286, 914, 11, 1293, 636, 2139, 636, 13, 51100], "temperature": 0.0, "avg_logprob": -0.3062615679879474, "compression_ratio": 1.658450704225352, "no_speech_prob": 0.000973651884123683}, {"id": 1004, "seek": 589248, "start": 5907.2, "end": 5911.679999999999, "text": " Yeah, I'm quite excited about this. In fact, I will spend like the next few weeks trying to", "tokens": [51100, 865, 11, 286, 478, 1596, 2919, 466, 341, 13, 682, 1186, 11, 286, 486, 3496, 411, 264, 958, 1326, 3259, 1382, 281, 51324], "temperature": 0.0, "avg_logprob": -0.3062615679879474, "compression_ratio": 1.658450704225352, "no_speech_prob": 0.000973651884123683}, {"id": 1005, "seek": 589248, "start": 5911.679999999999, "end": 5916.5599999999995, "text": " create a new knowledge graph. So I'll share with you after I create it, because there needs to be", "tokens": [51324, 1884, 257, 777, 3601, 4295, 13, 407, 286, 603, 2073, 365, 291, 934, 286, 1884, 309, 11, 570, 456, 2203, 281, 312, 51568], "temperature": 0.0, "avg_logprob": -0.3062615679879474, "compression_ratio": 1.658450704225352, "no_speech_prob": 0.000973651884123683}, {"id": 1006, "seek": 589248, "start": 5916.5599999999995, "end": 5921.12, "text": " some context dependent passing. And that's lacking right now in the knowledge graph that I see in", "tokens": [51568, 512, 4319, 12334, 8437, 13, 400, 300, 311, 20889, 558, 586, 294, 264, 3601, 4295, 300, 286, 536, 294, 51796], "temperature": 0.0, "avg_logprob": -0.3062615679879474, "compression_ratio": 1.658450704225352, "no_speech_prob": 0.000973651884123683}, {"id": 1007, "seek": 592112, "start": 5922.08, "end": 5928.0, "text": " so far. You agree with the context dependent passing, right? Like how you interpret a certain", "tokens": [50412, 370, 1400, 13, 509, 3986, 365, 264, 4319, 12334, 8437, 11, 558, 30, 1743, 577, 291, 7302, 257, 1629, 50708], "temperature": 0.0, "avg_logprob": -0.21151616975858614, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.0009096391149796546}, {"id": 1008, "seek": 592112, "start": 5928.0, "end": 5932.96, "text": " like note actually depends on the parents or depends on the position in the graph.", "tokens": [50708, 411, 3637, 767, 5946, 322, 264, 3152, 420, 5946, 322, 264, 2535, 294, 264, 4295, 13, 50956], "temperature": 0.0, "avg_logprob": -0.21151616975858614, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.0009096391149796546}, {"id": 1009, "seek": 592112, "start": 5934.8, "end": 5939.84, "text": " Yeah, even from a very traditional perspective, this very crucial as well.", "tokens": [51048, 865, 11, 754, 490, 257, 588, 5164, 4585, 11, 341, 588, 11462, 382, 731, 13, 51300], "temperature": 0.0, "avg_logprob": -0.21151616975858614, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.0009096391149796546}, {"id": 1010, "seek": 592112, "start": 5942.96, "end": 5948.24, "text": " Nice. Okay. Last minute or so anyone has any last points you want to add?", "tokens": [51456, 5490, 13, 1033, 13, 5264, 3456, 420, 370, 2878, 575, 604, 1036, 2793, 291, 528, 281, 909, 30, 51720], "temperature": 0.0, "avg_logprob": -0.21151616975858614, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.0009096391149796546}, {"id": 1011, "seek": 595112, "start": 5952.08, "end": 5957.84, "text": " Okay. If not, thanks for coming. And yeah, if anything, you can still reach out to me on Discord", "tokens": [50412, 1033, 13, 759, 406, 11, 3231, 337, 1348, 13, 400, 1338, 11, 498, 1340, 11, 291, 393, 920, 2524, 484, 281, 385, 322, 32623, 50700], "temperature": 0.0, "avg_logprob": -0.17477287130152924, "compression_ratio": 1.5668016194331984, "no_speech_prob": 0.0005373999010771513}, {"id": 1012, "seek": 595112, "start": 5957.84, "end": 5964.8, "text": " or LinkedIn. Yeah. And I'm looking forward to do this linkage between like large image models", "tokens": [50700, 420, 20657, 13, 865, 13, 400, 286, 478, 1237, 2128, 281, 360, 341, 49118, 1296, 411, 2416, 3256, 5245, 51048], "temperature": 0.0, "avg_logprob": -0.17477287130152924, "compression_ratio": 1.5668016194331984, "no_speech_prob": 0.0005373999010771513}, {"id": 1013, "seek": 595112, "start": 5964.8, "end": 5969.76, "text": " and knowledge graph. This is what a lot of people call a neural symbolic. And I think this will be", "tokens": [51048, 293, 3601, 4295, 13, 639, 307, 437, 257, 688, 295, 561, 818, 257, 18161, 25755, 13, 400, 286, 519, 341, 486, 312, 51296], "temperature": 0.0, "avg_logprob": -0.17477287130152924, "compression_ratio": 1.5668016194331984, "no_speech_prob": 0.0005373999010771513}, {"id": 1014, "seek": 595112, "start": 5969.76, "end": 5976.08, "text": " very crucial for intelligence. And I can see how we can use this knowledge graph approach to like", "tokens": [51296, 588, 11462, 337, 7599, 13, 400, 286, 393, 536, 577, 321, 393, 764, 341, 3601, 4295, 3109, 281, 411, 51612], "temperature": 0.0, "avg_logprob": -0.17477287130152924, "compression_ratio": 1.5668016194331984, "no_speech_prob": 0.0005373999010771513}, {"id": 1015, "seek": 597608, "start": 5976.16, "end": 5981.36, "text": " learn stuff from the environment and use it in a learning agent. Like I like to call it reinforcement", "tokens": [50368, 1466, 1507, 490, 264, 2823, 293, 764, 309, 294, 257, 2539, 9461, 13, 1743, 286, 411, 281, 818, 309, 29280, 50628], "temperature": 0.0, "avg_logprob": -0.11021832986311479, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.006718957796692848}, {"id": 1016, "seek": 597608, "start": 5981.36, "end": 5985.68, "text": " learning agent, but it's not really reinforcement learning because there are no rewards. Okay. You", "tokens": [50628, 2539, 9461, 11, 457, 309, 311, 406, 534, 29280, 2539, 570, 456, 366, 572, 17203, 13, 1033, 13, 509, 50844], "temperature": 0.0, "avg_logprob": -0.11021832986311479, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.006718957796692848}, {"id": 1017, "seek": 597608, "start": 5985.68, "end": 5991.68, "text": " can just learn directly from knowledge in the memory itself. So I think this will be very crucial", "tokens": [50844, 393, 445, 1466, 3838, 490, 3601, 294, 264, 4675, 2564, 13, 407, 286, 519, 341, 486, 312, 588, 11462, 51144], "temperature": 0.0, "avg_logprob": -0.11021832986311479, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.006718957796692848}, {"id": 1018, "seek": 597608, "start": 5991.68, "end": 5995.76, "text": " for that kind of framework. And yeah, hope to share more of your after experiment with it.", "tokens": [51144, 337, 300, 733, 295, 8388, 13, 400, 1338, 11, 1454, 281, 2073, 544, 295, 428, 934, 5120, 365, 309, 13, 51348], "temperature": 0.0, "avg_logprob": -0.11021832986311479, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.006718957796692848}, {"id": 1019, "seek": 599576, "start": 5995.76, "end": 6001.76, "text": " Okay. If not, yeah. Thanks for coming. And I'll see you around. Okay. Bye, friend.", "tokens": [50400, 1033, 13, 759, 406, 11, 1338, 13, 2561, 337, 1348, 13, 400, 286, 603, 536, 291, 926, 13, 1033, 13, 4621, 11, 1277, 13, 50664], "temperature": 0.0, "avg_logprob": -0.3739548082704897, "compression_ratio": 0.9761904761904762, "no_speech_prob": 0.08292332291603088}], "language": "en"}