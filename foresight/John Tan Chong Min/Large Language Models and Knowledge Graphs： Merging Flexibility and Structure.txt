Hi everyone and welcome to today's session. Today we'll be talking about a very exciting
topic which tries to merge large language models and knowledge graphs together. So as
you all already know, large language models are the recent hype. You can literally do
anything or a lot of things with large language models by just changing the prom. It is very,
very flexible. Get knowledge graphs extremely rigid. You have notes connected to other notes
in relations, but they are also very informative because the relations don't change. The notes
don't change. The large language models, one problem that they face is that they are a
little stochastic. They tend to generate things that may not be grounded in facts. So it seems
like naturally these two approaches seems like a good fit together. One is more flexible,
which is the large language models. And one is more reliable, like the knowledge graph.
Okay, so without further ado, let's begin today's topic. So I will roughly follow the
framework of this paper called Unifying Large Language Models and Knowledge Graphs,
a roadmap. Okay, this is by some IEEE fellows and senior members. I quite like the style of this
paper, but I feel like a lot of the things that are surveyed in the paper are not exactly the
latest large language model stuff. They are like the births, raw births, like basically the 2017
to 2019 era, that kind. So I have supplemented it with some of the more recent advancements,
like some length change stuff. So enjoy. Do feel free to comment anytime, because I think
this is a very interesting field that can be expanded upon. Never before have we
gotten large language models this powerful like chat GPD. And this is really something that we
can look at to improve on traditional methods or even think of a new method that is not even a
knowledge graph. Okay, later I'll share with you some ideas. Okay, so what are the pros and cons
of knowledge graphs and large language models? So as I said earlier, you can look at the rough
summary. I think this is quite a good summary. Okay, large language models, they are generalizable.
Okay, they possess a lot of knowledge. All right. But what they lack is that they lack some form of
understanding or facts. Okay, general language understanding. Okay, this one is debatable,
because like GPT-4 can be said to understand language pretty well. Like NLU is like the
ace most of the task there. Okay, so this one may be not so true in understanding, but for facts-wise,
fact generation is still a problem right now. Okay, incompleteness. Okay, maybe I mean like
sometimes they might generate things that don't answer the question fully. Okay, but increasingly
this is not really a problem anymore. Okay, it's more of like the reliability right now. So I
summarize this part here, reliability. Okay, I should use a different color. Let me just change
my annotation. So I think the main thing that large language models lack are reliability. Oh no,
it's the same color. Let me see. Reliability and consistency. This too. I mean, y'all have tried
large language models before, right? You key in the same prompt. Okay, sometimes you get the
different responses. Sometimes the response can be different. Like I said, should, it's a hot day
today, right? Yeah, scrims on this can be yes, sometimes no, you know, that kind of thing.
All right, so knowledge graphs, what do they have? Knowledge graphs have structural knowledge.
They are quite accurate. Okay, decisive, I guess, if you can find a way to like connect an input
node to an output node, you can say yes, there's a link between them. It's very interpretable. Okay,
actually large language models are also quite interpretable. So it's not really a con here.
Large language is both actually interpretability is also in large language models. Domain specific
knowledge. Yes, but actually, if you think about it, large language models with context grounding
also has domain specific knowledge. Okay, evolving language. This is something that is quite
interesting. Large language models don't really have this evolving language unless you fine tune
it. Maybe the recent Lamato you can fine tune on something. Okay, but you can also use something
like a knowledge graph to ground the large language models. Can you see the synergy here?
There's a lot of things that knowledge graphs do well, that is not exactly antagonistic or not
exactly different in nature from the large language model, it can just be used to ground
the large language model. So it's very interesting. So what are the cons of large language models?
They hallucinate, black box, black domain specific knowledge. So it looks like there can be some
synergy here. And let's explore how we can synergize these two approaches. Before we move on any
quick questions so far. Okay, so this is the one way of getting context into a large language
model and is used very often nowadays. It's called retrieval augmented generation.
So this is the raw format you retrieve from a corpus of documents. You have a few documents
here that you can retrieve from. And then maybe the user asks like, how much is a MacBook Pro?
Right, recently I need to ask myself this question because I'm considering whether I
should buy another one. So you know, they retrieve the relevant documents like,
okay, this document is about MacBook. Okay, you retrieve the right documents.
Okay, this document here is about maybe 2019. You can retrieve the right documents. All these
documents will actually be your context over here. So you could have the context retrieves
like that. MacBook Pro 2019 costs 5000 or something like that. Then you can have like in 2019
Apple release MacBook Air 2019. Of course, I mean, I don't really know the details, but let's say
these are the two documents you retrieve from your retrieval augmented generation.
Okay, and after that, you ask the question like, how much is a MacBook Pro 2019? So it's been shown
that if you use retrieval augmented generation, you can improve the consistency of the output
of the large language model because you are grounding it in the earlier context, which is
this part here, you are grounding it in this part here. So there's an element of grounding and
this is very important for a lot of real world use cases. Because if you don't ground it, you can
end up with quite nonsense generations. Alright, and just as a refresher, okay, what is the most
common method used to select the top K like documents? Anyone can just blow talk? What's the
most common metric to select the most relevant documents? That's a test of understanding. If
you are doing retrieval augmented generation, what is the most common metric used to retrieve
documents? To check the similarity. Anyone? You can write in your chat also. Dot product,
yes, very good. Dot product or cosine similarity. That's right. So usually we use some form of
embeddings. You embed your documents into a vector and then you use cosine similarity
to check how similar the document is compared to the query. Okay, I'm going into some details
over here. Okay, because actually this whole process of doing retrieval augmented generation
and passing over knowledge graphs is very, very similar. Alright, in fact, you could even replace
this retrieval augmented generation with like knowledge graph augmented generation is perfectly,
I think it's replaceable. Alright, so this is some idea of how large language models can be made
to be more accurate. Okay, using something like that. Okay, so this again, I just highlight the
problems of large language models. Okay, may not be able to recall the knowledge, but you can retrieve
the right context using this retrieval augmented generation provided you can retrieve the context
currently. Alright, so this is a real world use case issue. Alright, I've talked to some people
and they say that retrieval augmented generation with just the cosine similarity alone, okay,
might not give you the right documents. So, you know, the embedding vectors train using
contrastive loss, you know, they may not capture everything, especially if your document is very,
very large. Okay, imagine you have only one vector to represent the entire document, and you have
another vector to represent document, another document. So this is like document A, and another
vector to represent document B, then you see how similar they are. But what about like,
what if one document contains many parts? Right, I mean, each of these parts could have
different meanings, right? Each of these sub parts could contain like, let's say you have this
document could have a sub part that is like that, a sub part that is like that, a sub part that is
like that. You know, they just aggregate all of this together into one vector, like that.
Can you see that you're actually losing like information here, which means that when you
retrieve something, let's say if I want to find out how to code, like a length chain question
answer agent, you know, I'm not going to retrieve this vector because by vector similarity, my query
is here. Alright, by vector similarity, maybe I'll retrieve a document that is like B instead,
because like maybe it's nearer in terms of cosine similarity. Okay, I mean, it's greater
is the opposite direction. Let me just make this vector look more similar to that. Like,
let's say I have B is like that. So if this direction here is like how to code a length chain,
QA agent, and this is the embedding vector for it, it goes in this direction.
You know, you're not going to retrieve document A, although it contains that part over here.
Okay, you're going to retrieve document B. This is one of the failings of the embedding
vector. It just tries to capture the whole document into one vector. And this means that
you may not be able to extract stuff out. Okay, Richard said something. This is why I keep saying
context is king. Summarization is essentially impossible on segmentation on segmented documents.
Yeah, definitely, because you summarize you lose information. Okay, so there needs to be like
different hierarchies of how you retrieve things out, broad level, specific level. And you know what?
Knowledge graphs might actually have that kind of hierarchy formulation. I'm actually jumping
a few slides ahead, but give you an idea of why I'm so excited about this idea. All right. So
actually some of the bypass that I've been telling people, I've been advising people is that like
if you cannot get retrieval of mental generation to work, consider using like filters or like labels.
So like this labels will say like, okay, maybe it's like product A, product B. So you know,
instead of relying on just the retrieval of mental generation, or the embedding vector
to actually embed the right knowledge, like let's say you have a length chain QA agent,
I can tag this thing as a length chain QA agent inside this document. So there will be certain
tags that you can have. So then you can then do like the embedding vector across the documents
that have this text. Yeah, so maybe that's one way to like do a first hand filtering.
Yeah, I mean, this is just like some, what do you call it, some bypasses to the downsides of
embedding vectors for pros and similarities. So these are some ideas that could be done right
now to bypass it. But if we have a way to use knowledge graph to do more broad level to more
specific level extraction, maybe you don't even need all this, you can just pass through your
knowledge graph and you can use that to ground the large language model. All right, so this is my
last point here, knowledge graphs are useful to retrieve the right context, search the right
keywords, retrieve the right subgraph. Like let me give you an example here, if let's say I have a
graph like that. All right, so maybe this is a graph talking about like people who view Netflix.
Okay, so these are the Netflix user graphs. So these are the users. And then maybe you have me
over here, John. Okay, and then like the movies are watched. I like to watch the flash, the series,
not bad. I highly recommend it. Then maybe we have another guy like maybe Richard can be here,
watch other movies, like movie A and movie B, you know, yep. So if you want to like extract
something out here, you can just search like for keywords and then you can just put this whole
subgraph here. And then you can use this part here. Okay, how you want to pass it into the
large language model, I leave it for future investigations. There are a few ways to do it.
I will cover some ways today. So if you can pass this into the large language model,
essentially, you can ground the LM in context of the knowledge graph. And then we can actually do
this grounding at a more higher level grounding or more sub level grounding depends on which height
of the knowledge graph you're going to take the notes from. All right, so I think this is a very
exciting prospect. And yeah, I'm looking forward to see like how this can actually work. So I'll
be actually working on getting this to work the next few weeks, right? Because I think doing
something like that actually might help with the up challenge as well, the abstraction and reasoning
corpus. So this is my latest kind of hit way that I'm going into. So this is from the knowledge
graph conference. Okay, I actually listened to quite a few of their videos. This is the knowledge
graph conference 2023. And there's a speaker, Danny from Dev6. I think I pronounced his name
wrongly. But the idea is that if you are using chat GPT for your own applications, if you use chat
GPT in different languages, you might get different outputs, okay, even for the same information. So
you know, being Singaporean and you know, the presidential election is coming soon, I just
asked like who is Singapore's current president right now. So you can see now is Halima Yacob.
Yacob, sorry. And we asked the question in Chinese, all right, Singapore the
總統, so you know, you say, Singapore, there's no president in Singapore. So this is basically
the same information, you just translate it, you can get different performances
with chat GPT. Okay, and the same thing for like if you use Lamatu, Lamatu is heavily trained on
English. If you use Chinese, I'm very sure it won't do very well. All right, this is a practical
problem of large language models. You know, the Chinese benchmarks like they use Ernie,
Wenxing Yi and those other Chinese language models, they say that they perform better than
GPT for. Okay, I mean, at first I was skeptical. Then now that I think about it, they might have
done their evaluation on Chinese data sets. And their language models are fine tuned on the Chinese
data set. So maybe there's some merit to their claims, okay, on the specific Chinese data sets
here. So this is one of the things that knowledge graphs can actually help to solve, because knowledge
graphs can sort of translate this thing, because knowledge graph is not language specific, you
see. So your concepts like president, okay, regardless of how you represent it in words,
okay, your Chinese words or English words, you can actually go to the same part in the knowledge
graph. And then you can have the key words here, like Singapore, and then it's like Halima.
So you can actually retrieve the kind of information regardless of language,
okay, and then you can pass back this information back into the generation of the model. So this
can go here, back here. So regardless of how you prompt GPT in a certain language, okay,
you can do it. So maybe I just do the flow chart. So G-L-M, okay, language, language invariant
representation. Okay, then you do your processing there. And then you go back to L-M. So if for
those of you who have been to some of my other like discussion sessions, you would know that I
like to say that this is the, this part here. This part here is what I call the abstraction layer,
or the latent layer, latent space. So you process it in a way that is different from the input
domain, but because the information you process is similar, in this case, we are still asking for
semantic information about the president. You know, we don't have to do it in the language domain,
we can do it in like maybe some representational space. It could be a graph, all right, and then
you can use whatever you process the graph, you can go back to your input space. So this is one of
the key advantages that, you know, if we could interface large language models with some form of
graphical or some form of memory-based approach that is invariant to the input language type,
you could get some performance advantage here. Questions so far? Anyone?
Okay, so let's cover some of the basics. What is a knowledge graph? So I took this from the paper.
The knowledge graph is basically a triplet consisting of source, destination, to relation. So like for
example, Barack Obama was born in Honolulu. So this is the relation, okay, so relation.
And this is like Barack Obama as the source, and Honolulu is like the destination. So each
knowledge graph is made up of all these triplets joined together in various ways. And the idea is
that you just need to connect those entities that are related to each other. You can like
you can actually walk through the knowledge graph and get the information you need. Okay, so like
there are of course like mega nodes, like for example, like Barack Obama will have a lot of
connections leading out of it because you're describing the person. Then like stuff like places
where a lot of things leading into it, because a lot of things like like are in the USA, a lot of
things are in Singapore, you know. So this is the 20, 30 years ideas of knowledge graph. Okay, it's
not too bad. Okay, but it's very restrictive. I personally think that there is a better way to
represent information other than this kind of structure. Okay, and we can go and talk about
it later in the discussion. All right, I have something in the chat. Richard says,
is there a handy reference chart for how this looks or compares to word-to-veg and similar
embeddings? Okay, so this typical knowledge graph that I'm talking about here does not
have embeddings yet. Okay, but in the future iterations like in 2017 or 2018, I think people
have come up with these things like knowledge graph embeddings. So they actually encode all
this information here in some vector space. So like maybe like, it's something very similar to
like the vector space that we see in in large language models. You can actually encode this
thing in vector space. You can encode the relation here in the vector space or so.
And then like, you can then encode this part here also in vector space. So like,
so it's like doing a vector arithmetic now. So you can see that
if I do a relation, it's just simply this one plus this one equals to this one. So I can do a
like add a .2 in the first one here and then I can get. So if you have a sufficiently expressive
enough embedding space, you can express the whole knowledge graph in the form of embeddings.
And that is indeed what some of the later models do. In fact, this is highly related to
graph neural networks. Because graph neural networks, they express each node as an embedding,
then they do message passing, which means I share information with the other nodes. Like at each
time step, I pass some information to the other nodes. And I mean, there are different variants
of message passing. The most common is that the message meets in the middle, this one then updates
both nodes. So there are a few ways of doing the idea of like updating the embeddings and so on.
I'm not going to cover in detail about how all this are done because graph neural networks is a huge
topic. Okay, personally, I think graph neural networks is probably not the answer to solving
intelligence. I'm sorry to Peter Velikovic. I like what he's doing, but I don't think it's
it's the right way to do it, like using differentiable deep learning to do it.
So I think the knowledge graph that I've described over here, which is using vectors to do addition
and then you get the other nodes. That's a very expressive knowledge graph. Okay, because you can
actually express everything in vectors without the names. So you can theoretically do any kind
of like addition provided, you know, nodes plus relation give you another node provided that
exists. So if you could somehow represent the whole of the world's knowledge in the form of
vector space, I say we are done. We can just like, we achieved zero short generalization.
You just embed into that vector space and then you add something and then you go to somewhere else.
Okay, but I don't think that's how intelligence is represented. Okay, because you know, there's
this thing called like context dependent embeddings. Like I don't think like the word Barrett
Obama would have the same embedding all the time. So like, for example, if you have Barrett Obama that
is like at the White House, Barrett Obama at his house, okay, Barrett Obama at the beach or maybe
different places of Barrett Obama will lead to different characteristics of Barrett Obama. Like
he maybe is very serious in the office, but he's very relaxed at the beach. You cannot have the
same embedding space to represent all this. You need to walk it according to the context. Okay,
and that is something that I actually intend to try to do it. Like I try to do a very flexible
like basically the information can walk according to the parent nodes in this new form of knowledge
graph that I'm thinking of. Okay, so whatever I'm talking about is my own idea. I haven't seen
any paper on it yet, but I think the current knowledge graphs will all fail at embodying
intelligence because it's just too restrictive right now. Okay, Shang, you asked a question.
I'm unfamiliar with graph theory, so hoping to know how do you represent factors as weights and
how many can you add? Okay, could you elaborate what you mean by factors?
Yeah, you mentioned that like you can add any form of intelligence, right? So take for example,
if we are using, yeah, I actually didn't think of this example, but let's say just the simplest one,
multi-layer map. Then for these roads, one weight could be how fast the speed limit of the road
and another weight could be how occupied it is. Okay, so you are talking about like descriptions
of an object, like all characteristics, attributes, you're talking about attributes of an object.
Yeah, the weight of each line, correct, of each connection between the nodes.
Ah, okay, so like how do you get this embedding here, right? Yeah, correct. Yeah, so perhaps like
in your original embedding space, each of these dimensions could represent something already,
like maybe one could represent road, one could represent like emotion or, you know,
there are different domains that these dimensions could capture already. So if you really have that,
you can just like add the relation in that specific dimension. Yeah, so of course, all this
will need to be like learn somehow. So it's either learn through deep learning or some fixed biases.
Yeah, so ultimately, how well the graph does will depend on how good your embedding space captures
all the information. Yeah, okay, so I hope that clarifies. Yeah, thank you. So for now,
just know that knowledge graphs have a few forms. Okay, the most simple form is that you take words
and then you add another word and then you get another word. So this like describes a relation.
The more advanced form will be to use embeddings. All right, so we will talk more about and then
of course, the even more advanced form is evolving embeddings or context dependent
embeddings, which is like the idea that I have. And it's also the idea that large language models
actually kind of use because when we can ground large language models in different contexts,
you get different outputs. So a large language model is context dependent processing. Okay,
if you can embody that kind of context dependence into the knowledge graph,
you will have a very powerful knowledge graph. So as you can see, whatever I'm sharing with you
here today, I think that I'm not the answer. All right, I'm just sharing with you here because this
is what is existing. Okay, I have a grander vision compared to all of the stuff that I'm talking about.
Okay, so let's continue. All right, so knowledge graphs, okay, what excites me in knowledge graph
is the very notion of hierarchy. And I think hierarchy is key to intelligence. You don't
process things in just one domain. You process things in many domains. Like if I'm drinking a cup
now, I'm just like drinking water from a cup, not drinking a cup. Drinking water now, I use my
hand to move like that. But then if I think about, oh, how do I go to school? Then I think about,
oh, I need to do the bus stop, I need to go to the MRT maybe, and then I need to take this bus
or this train. So this is a more higher level planning. Okay, if I were to think about like
how I move my left leg and right leg, left leg, right leg, I will take forever before I do some
planning. So different problems require different levels of solution finding. And I call this
different levels of hierarchy. So it might be up challenge, the abstraction of reasoning corpus.
I use multiple levels of hierarchy, like we have a pixel level, we have an object level,
you know, and then you express the input grid into different forms of hierarchy.
And I find that this way can solve a lot of problems because different problems require
different approaches to think of it. But you don't solve all problems using trigonometry,
you solve some using algebra, you solve some using set theory. So you have different ways
of viewing the problem. And knowledge graph, you can actually use this to extract different,
like at the top layers of the knowledge graph, typically are the more broad concepts and the
bottom will be more of the, more of the general concepts. So you can see in this, this is the
the SICK knowledge base. SICK is a 30 year old project trying to embody the world's knowledge
in a knowledge graph. They are still trying to do it, but it turns out that this is a very
hard thing to do because the knowledge graphs itself, it embodies like one relation is like
a confirmed relation. But sometimes, you know, based on the context, you may not have that
confirmed relation. So again, this is the context dependent knowledge graph I'm talking about.
Also, another thing is a lot of times we do things, but we don't really know how to express it
in words. So if you want to express the whole idea of logic in words, it's a very, very difficult
task. Because sometimes we don't even know why we're doing something. Okay, there's bound to be a
point of time that logic cannot express things. So you can go and look at this thing called
Godot incompleteness theorem. If you use mathematical logic to express things, there comes a point in
a time whereby logic cannot solve. Because the way to solve it lies beyond logic. It cannot be
Godot incompleteness theorem. There's something like this. This sentence is false. So if you can
represent this as a Godot number, this kind of sentence, and then you say that, oh, this number
is true. But then this number says that this number is false. So it's like you have a self-referential
loop. So if you use logical prepositions, and knowledge graph is sort of like a logical
preposition, A goes to B, B goes to C, you might face this problem that you can actually go in
you can actually go in a loop that contradicts itself. So that's one thing that knowledge
graphs may have some issue with if we do it being 100% fact. If A links to B all the time,
sometimes you might actually have a link that contradicts itself. That's one issue of the
knowledge graph. The other thing is, so burying all these issues, one thing I like about knowledge
graph is that you can see in this diagram here, you start off with small stuff like thing, and then
you go to like individual, you go to collections, and then you have different ways of doing it,
time, movement, and so on. Then you have agents, actors, plans, and goals. I mean,
if you think about it, kind of it's like how large language models is evolving now, right?
We are kind of at this stage, we are agents now. So after that, we have organization of agents,
we have activities. Okay, hopefully we don't get to military warfare, you know, because like,
so this is like the evolution of a population. So it's quite nice, and like you can capture all
this knowledge from, okay, so I wanted to say this is for more like micro level
to more macro level. And the macro level is actually the sum of the micro level. So
maybe the arrow should be drawn the other way, the arrow should be drawn like that.
You take from the micro stuff, and you go to the macro stuff. So this is the knowledge
that we accumulate, right? And knowledge graphs can capture this quite well, because of the way
you take from source relation to destination, you can capture from the micro level, you do
all the branching, and then you end up with the micro level, right? So this was in 2016,
by the way, I couldn't find this in the sick website today, right? So this is the sick knowledge
graph. This is like a very, very tiny representation of how the knowledge graph looks like. I just
wanted you to see like how one of the largest knowledge graph in the world looks like. So
you can see like you have all these like the fortune companies, you have all these like functions,
like all these like look like some form of like math stuff, right? Yeah, so you have like people
over here. So you have different areas of congregation of all this knowledge, right? Then
in order to pass through the knowledge graph, you have to use something very, very similar to SQL,
structured query language, you like say that, oh, if I want to get a frightened person, I want to
get the entity X that is a person, and then fuse emotion that is fear at a very high level.
So you have to do this kind of stuff, right? So immediately you can see that knowledge graph
right now can be immediately improved by large language models in one aspect. Okay, and this
aspect is that we can straight away use the large language models to generate this structured query
language. Okay, so if all of you are thinking about this, like if you want to get like a very,
very rigid programming language out, okay, you can actually write what you want in free text
and then say convert this to SQL, and you can get it out. So you can do the same thing for
this sick language. You give it some examples of how the language works. You say, I want to get
a frightened person and then, you know, chat GPD is quite good at getting stuff like this out.
Okay, no more SQL, right? I love it. Okay, if you need to use SQL, just use chat GPD.
Okay, it's a very good replacement. All right, so this is one way large language models can
already help to benefit knowledge graphs right now because it can pass through it
using very human readable and understandable syntax. Like this kind of thing is not very human
understandable. You can use free text to do it and we can do it right now. But more importantly,
what can large language models do to help knowledge graphs? Okay, or what can knowledge
graphs do to help language models? Okay, so before I move on to that, let's just talk about some other
ideas I have. So I'm actually a reinforcement learning person. So like, I feel like knowledge
graph can also represent stuff like different states, like you have different tiredness,
they drink coffee, and then you're not awake. So if you know in the literature of
reinforcement learning, this like a Markov decision process where these are the states,
okay, and these are the actions. And then this is the next thing.
Okay, so you can actually use knowledge graphs to represent stuff like this as well.
Okay, because it's quite anything that has a link like that, you can represent this easily. Okay, so
all right, this is perhaps the most important slide for today. Okay, this is not in the paper
that I referenced, but this is the thing that I was thinking about. It's like knowledge graph is
actually sort of a tool that can be used by the agent. So like retrieval of method generation
may not get the right passages because like the embedding space may not be good. Perhaps we can
use like a form of knowledge graph passing, okay, you can extract relevant parts of the knowledge
graph, you can retrieve the context based on that. Okay, so you can ask the knowledge graph to get you
the subgraph. The subgraph, you can then use it to ground the context of the agent and how you use
it to ground. Okay, it's up to you. Okay, some people might use graph neural networks. I don't
advocate for that. Okay, one other way of doing it is to just convert it back to free text. Okay,
as easy as that. So you use the knowledge graph to extract out the relevant purposes and avoid the
need for the embedding space, the open AI embeddings. Okay, you use the knowledge graph to
extract it. Then you take the stuff that you extract from the knowledge graph, pass it back as text
and then go back to the agent to ground it. All right. So yeah, one other way of, one good thing
about this is that if you have stuff like if you are doing this for a robot, okay, that experiences
the world, you might actually be able to use this knowledge graph. Okay, I'm conflating the
term knowledge graph, but this knowledge graph can now be the state action state graph. You know,
you can actually model relations of the world easily. Like I always believed like we learned
from taking actions in the world. So we can actually build this knowledge graph dynamically.
This is the third point. Okay, you can gain knowledge of the world. We can build up this
knowledge graph bit by bit. All right. And then we can then query this knowledge graph
and get answers from the knowledge graph to inform our choices. Okay, so about how we can get this
part here, this is a huge thing here. Okay, because I believe that there's one thing that's missing
in current knowledge graph and this thing is called changing the memory to the context
at hand. Okay, so I treat the knowledge graph as memory. So like when you retrieve things from
memory, okay, and then you want to apply it to the current state right now, current state of the
world right now, you don't really want to just use that memory itself. You want to adapt that
memory such that it will be relevant in this current state. Like if I have drank like coffee
at school or not, drink coffee at home now, okay, you know, something like that. This, I will need
to adapt that memory of drinking coffee somewhere else and then adapt it back to here. Okay, there's
no point in giving me the memory of me drinking coffee somewhere else because it doesn't adapt to
the current situation. So if you can adapt this knowledge graph to the current situation,
that will be great. Okay, that will be great. So that's something that I think I'm trying to
look into because you don't just want static knowledge extraction. Okay, you want knowledge
extracted and manipulated to fit the current context. Okay, of course, for those of you all in
my discord group, I've been thinking about memory recently. And you know, human memory is very
malleable. Like if you think about something, you might actually affect that memory of it.
So like a lot of times people in the childhood, they think that they have certain memories again,
like maybe you are lost in a supermarket. So if I keep asking you questions about it, I say,
who was the stranger with you when you were lost? Okay, so maybe there was no stranger. But if I keep
asking you guiding questions like that, eventually, you might think of your memories like, oh, yeah,
a stranger let me home after I went, I was lost in the in the in the mud. Okay, but that that may
not have happened. That memory has changed because I've asked you certain guiding questions. And then
you think that certain things are now in your memory. Right. So whether or not we should change
this memory and affect this knowledge graph, I leave that to future discussion. Okay, because
this is something very interesting, like should we change the existing memory that we have,
based on the current context, get our brains that our brains do that. Okay, but should we do this
for this kind of practical systems? Okay. Yeah, so we say humans hallucinate. Yeah, of course,
we hallucinate a lot. And that is why actually, we are quite similar to large English models in
that sense. Now people always say large English models not very reliable. Are humans reliable?
Our memory is not that reliable, actually, if you think about it. But honestly, I cannot trust
my memories that that much, because like sometimes if it's too far away, it can change like the book
that I've been reading. It will say that like flashback memories, which people think that are
very, very pertinent, flashback memories are memories like, like, you know, 911 collapse.
People tend to remember what they were doing at that time, because it was so significant.
It turns out that this flashback memories can be wrong. Okay, it can also be, it can also be
change. So this is a very interesting thing. You can actually use like the current context
to affect the memory you have. So you might actually affect the knowledge graph about whether
or not we want it to be that way. Okay, we have to think about that. Okay, I digress a bit. Okay.
But let me just get back to topic. Okay, today we have quite a few slides to cover. There are three
approaches that I want to talk about today. First is that you can use knowledge graph to
enhance your large language models. And this means that you can give it structured stuff,
like domain specific knowledge. In some sense, it's like text based grounding is the same as
retrieval of mental generation, just that now you take the information from a knowledge graph.
Number two, you use large language models, expressivity, okay, and make a better knowledge
graph. Okay, I like this approach as well. Okay, we will see how to do it. And lastly,
you combine both approaches, you can get a synergized large language models and knowledge
graph. And I think something like this will be able to embody intelligence. Okay, but not the
current knowledge graph. We need to change it to a dynamic knowledge graph. Okay, what is a dynamic
knowledge graph? Maybe I'll talk about it next time. Okay, after I flesh out some ideas that I
have right now, I will create this dynamic knowledge graph. Okay, I think the current knowledge
graphs are not the answer. We need to have a different kind of knowledge graph. But if we use
this, I think we can get intelligence. Okay, let's move on to the next point. Approach one,
knowledge graph augmented large language models. Okay, so there are two ways I can,
I summarize the paper in two ways. The main thing is one, you can just put the knowledge graph as
text. And the other one is treat this as an object. And what kind of object? Okay, you either use like
graph neural networks, or you can use an embedding space. I mean, the one that was used was trans
e trans embedding. You can go and search the paper trans. So this are some ways that we can use the
knowledge graph, okay, to pass it. Let's go through the first way. So the first, oh, sorry,
this is basically a pipeline for retrieval of large language models grounding. First,
you use some form of knowledge retrieval, like, you know, retrieval method generation,
you use cosine similarity, you get certain facts or some documents. So I'm just relating this to
retrieval method generation, because they are almost the same. All right, you take in the facts,
you ground the large language model, you get the answer. Okay, and over here in the paper,
they put back propagation. But you know, how are you going to back propagate this knowledge retrieval?
You're going to end up with some, like, very, very weird way of doing back propagation. I don't
think back propagation, I don't think back propagation is the answer here. Maybe you want
to back propagate your LM to find you think, okay, I grant that. But this part here, to back
propagate to the knowledge retrieval, I don't think that should be done. All right, because this
back propagation thing will lead to, like, changes in embedding space. And then if you
change your knowledge retrieval, you also need to change your large language model. It's a never
ending cycle of chasing each other. Like, if you change the knowledge embeddings for the knowledge
retrieval, you also need to change how you interpret them in the large language model. So,
yeah, I don't think you should use back propagation for the knowledge retrieval. You should probably
use, like, memory methods, other methods, like you can say that, okay, what worked, what did not
work, what worked, what did not work. Okay, you can reference this paper called Voyager. Okay,
so there are these automatic curriculum learner. I think you should train the knowledge retrieval
like the automated curriculum learner. You just ground it in some examples of what works, what
doesn't work. You don't have to use back propagation for that. Okay, so the main pathway for knowledge
graph for tax, for LMS is like that. You take the knowledge graph, pass through it, get some
facts, and then feed it into the large language model. Okay, that's the main pipeline. Okay,
questions on this? Okay, let's move on. Okay, so this is one of my favorite papers. Okay, this is
the Generative Agents paper. They have 20 agents in the sandbox interacting with each other. And
one thing that struck me quite well for this paper is that they actually use JSON structure
to ground the actions. So, for example, if you want to ask like Eddie Lin, he currently is in
the Lin family's house. He's in the bedroom actually on the desk. Okay, so you can ask the
agent, okay, this is actually the chatGPD prompt. Okay, you can ask the agent like, okay, these are
the other areas that we have. Okay, and all these other areas are obtained from the JSON.
Actually, the JSON is like a knowledge graph.
Okay, because we actually have hierarchies like Lin family house has a bedroom, has a study room,
has a kitchen, you know, this is something like a knowledge graph. If you ask me like,
they are just like representing the hierarchy of the house. Like, I mean, if you want to treat it
as a knowledge graph, you will say like, this is the house. So, this is the Lin's house. Lin's house.
Lin's house. I've just put Lin's age. Okay, then you can have like, the relation will be contains,
okay, or comprises, I mean, contains, then you can have like bedroom.
Yeah, so the JSON kind of hierarchy is a subset of what a knowledge graph can embody.
All right, so I treat this as a knowledge graph. So you can, you can sort of ground the agent.
Okay, like this is what the agent knows. Okay, this is the current memory that the agent has
as a form of knowledge graph. Like this are the kind of areas that we actually know from the world.
Okay, this is like, if you talk about grid cells and play cells, maybe find out more areas.
Okay, you can ground them. Okay, these are your semantic knowledge or oppositional knowledge that
you have about the world. And actually, these two are the positional knowledge. This is the first
one is knowledge about the house, knowledge about house. And then the second one is knowledge about
the world. Yeah. So you, you have all this knowledge, you can ground the agent to choose
a specific place. Imagine if we did not ground the agent with all this stuff at the top,
you just ask, where should Eddie Lin go to? Then Eddie Lin might, the LL might reply,
Eddie Lin should go to the supermarket or something like something that is irrelevant to the game world.
Right, so because we grounded it with some idea of what kind of possible areas that the agent
should go, the agent is able to choose one area from the above list. And how is this list generated?
It's generated from passing some form of knowledge graph. And this is what I mean by using knowledge
graph as text to ground the large language model. So you can use this recursively, you can say that,
oh, currently you are in the maybe common room. Okay, what, where in the common room would you
like to go? We like to go to the sofa, to the mirror, and you can do this recursively. Okay,
and then you can get a very, very specific area that the agent is going. Okay. Any clarifications
on this so far before I move on? Yeah, if you haven't read this paper, go read it. Okay, this
paper is good. It's one of the better ones. All right. So we have the Chinese LLM, it's called
Ernie. All right, and this, what they do is they actually use two hybrid ways of generating
the output. So they say that large language models lack grounding, lack consistency. So we use
a knowledge graph. Okay, I granted that. But then I look at that structure and they're like,
oh man, what is this? So they actually have a large language model. This is the typical
transformer architecture. So this is a typical transformer on the left side.
So they have two encoders. Okay, one is the T encoder, which is like the text encoder. And why
is the knowledge graph encoder? So the knowledge graph encoder uses this thing called trans embeddings.
Okay, I'm not going to go through that, but they train that embeddings using like,
they take one vector and take another vector, then they connect the,
just draw the diagram here for you to see. So you have one vector A and another vector B,
and then you create another vector C here. So you can keep like using this vector A,
you take another vector, extend from it, and then you can train on this relation C. So, you know,
this is how the trans embeddings are trained, trans E. Okay, and they use this kind of embedding
space. Okay, you can do self-attention, and then you can do cross-attention across
both the trans, the text embeddings as well as the knowledge graph embeddings.
And then hopefully you get some output, right? And then you get some text outputs here,
and then you can update your knowledge graph to the knowledge graph outputs here. Okay, so this
is a way to embody a knowledge graph as some embedding space. Okay, and then we can use like
the attention to like, attend to like the text-based stuff as well. So, yeah, this is just one way of
doing processing using a knowledge graph. As you can see, I don't really like it. I mean,
I think that it's too convoluted. Like, okay, so this is another discussion question that I'd like
you all to think about. Should we have separate embedding space for this large language model
stuff and for the knowledge graph embeddings? Like, should we use two different embeddings?
Okay, should we use two different embeddings?
Yeah, or should we use the same one? Yeah. Okay, then also more generally, like if you want to have
multi-modal embeddings, like you have text, image embeddings, you have audio embeddings,
okay, if you want to do a multi-modal large language models, you can actually also put them
into distribution model at the end here. Okay, but the question is, in fact, knowledge graph can
be multi-modal also, you can actually also put it here. The question is, all this image
and audio embeddings, okay, you can put it in the large language model, you can also put in the
knowledge graph. But why not just use a single embedding, right? Why do you need to? You are
using text-based knowledge graph and text-based large language models, that there's no external
domain here. It's all the same domain. Why do we need two different embeddings for the knowledge
graph and the input text? That's my question. If you have any ideas, let me know. But think about
this, all right. Okay, then we have this question-answer graph neural network, and this does a
two-way interaction between the language model and knowledge graph. And what we can see here is
that we have a certain question, okay, and some options that to choose from. The large language
model will go in here, and then they express the question and the options as part of a knowledge
graph, okay, and this will go through another knowledge graph encoder, and this knowledge
graph encoder is a graph neural network. Okay, and this basically will, you look at this diagram here,
it will do cross-attention. It's very, very confusing. Look at this language model conditions
knowledge graph. So you can blank out some notes here, you know, you can actually do some attention
on some notes to like block off the path. Yeah, so yeah, that's possible. So you can also use the
knowledge graph to condition the attention in the, in, in, when, when you do the next open
processing for the language model, and then eventually you get your answer. Yeah, so this is
one way we can process the knowledge graph, you can process it using a graph neural network.
And in fact, the earlier one on Ernie, that is similar to graph neural network as well. I mean,
they are using the embedding space. And you know, if you just do some operations on the embedding
space, that is a graph neural network already. So yeah, this is very, very similar to graph neural
networks. And yeah, it shows that back in the first few years, people use these kind of methods to
pass through knowledge graph using graph neural networks to represent the embeddings. Okay,
I don't see why you need to do this. Okay, personally, I don't see why you need to do this.
You can just use text, because the knowledge graph representation is in the same kind of domain as
your last language model representation. They're all text. Yeah, why do we need a separate embedding?
So yeah. Okay, Richard said, I think there will have to be input output embeddings and train
them to address common pattern or memory structures. Sorry, could you explain this comment was in
relation to which part of what I said? So you were saying, you know, it's a decision sort of or
sort of my decision question, right, as well, that there is, how do you
well approach this problem? Why do they have their embeddings separate, right?
At the end, there's a sort of cross attention where they're merging them for an output of this
type or that type would have you. But then this idea comes that the real, so I think of a large
language model where the reason why they have these emergent behaviors is because language is
currently our best mechanism to embody thoughts, ideas, and our most direct implementation of
ideas. Now, particularly once they're broken down, tokenized and so on, you've taken through that
process a few times, consider retention and context, you come up with new ideas. Yes, yes.
And then as you're just saying, there's no particular need for different embedding spaces.
And the only need for them is to bring understanding into a common framework where
the ideas themselves in the latent space are being considered and their context and their
relationships. So how the, this is sort of a, what's the word, this idea of boiling down
the actual form of communication into some representation, any representation where we
can start applying our knowledge to it. Whether you read text or listen to text,
you don't, when I hear things, I don't imagine them written down in front of me. I just hear words,
words become ideas, and we go from there. So in the same way, I see the way that
knowledge is presented as an input-output problem and embeddings really address the input-output
problem. And then after that, there's a memory and consideration process which operates on ideas
which are not linked to input and output. I think you and I agree that there needs to be a latent
space or abstraction space for processing. And I think you also agree that there need not be too
separate embedding space for the knowledge graph in the last image model. If I hear it correctly,
right, you don't, you also don't think that is necessary, right? I think,
yeah, but then the problem becomes, if you don't use the same embedding technique,
how do you present meaning? So for mine, in terms of large language model being in language or not
in language, in words, the question is really, are we making the problem harder for ourselves
by using a difference, by saying, well, it's all words and the words are by and large correct.
Therefore, we'll just use a large language model to read and ingest a large language model. And I
think that will work. But the challenge becomes what you alluded to earlier, where the Chinese
representation versus the English representation gives a different outcome. And I'm trying to
abstract away that behaviour. So the thinking is, the actual thinking happens in, is always in
latent space. And the only job for embeddings is to present in a form where, you know, cognition can
happen. Right? Because, right? And so I would say, I don't particularly care what the encoding
encoder is, decoder is, it can go from text in, picture out, picture in, text out, it doesn't
matter. The important thing is that it's consistent, and we can operate on it in a manner that addresses
the patterns and relationships within. Yeah, well said, well said. I agree with you. So
what matters is how we abstract it to the processing space, which is the latent space,
and how we encode it and decode is just extra details, that basically just needs to be mapped
there, and it should be good enough. Yeah, I think so. And if it comes to its own training
challenge. Correct. So I think in the earlier papers, what I get is like, why do we need a
knowledge graph encoder like that? It's because they use embeddings like trans-e that, you know,
are different from GPD embeddings, like, or BERT embeddings. Again, most of the early papers use
but BERT, bi-directional encoder representations from transformers. So what happens is because
these two are from a different embedding space. So you kind of need to map them to the same
embedding space. That's why you need a knowledge graph encoder and a large-engaged model encoder.
But in the new kind of knowledge graph that is constructed, because this large-engaged model
is now so powerful, you can actually use the embedding space for the large-engaged model to
construct your knowledge graph. And if you do that, okay, if you do that process, which is
part two of the presentation today, you will see later, if you use the large-engaged models to
construct the knowledge graph, actually, you don't need a separate knowledge graph encoder
or encoder here because they are in the same embedding space already. So if you look at this
thing here, you don't need a separate decoder for the JSON here because this is in the same
embedding space as your text. And I would like to posit that it will be better for all of them to
be in the same embedding space because it will be much easier to do the attention. I mean,
it's easier to do attention in the same domain as compared to different domains because, you know,
cross-attention is only one layer right now. You're going to do a very efficient cross-attentioning
multiple layers. But if you just do it in the same domain, the transformer architecture right now,
you actually do the self-attention multiple times, all right? So it might be actually better to
do it in the same domain, okay? And of course, you will save training complications because,
you know, you need to map both to the same latent space and, you know, that is a difficult problem.
It's a very difficult problem to map two different streams of inputs to the same latent space. I
mean, we have seen it, like in OpenAI, they have this thing called clip, okay, that max text and
images to same latent space. You know how many examples they train it on? Millions, I think even
billions. Yeah, so it's a very, very difficult problem to map both to the same latent space.
If I can map it well, of course, you can do like stuff like stable diffusion, you know,
dolly, you can generate images from text. Yeah, but why have this problem with the
knowledge graph when you can actually just ground the knowledge graph in the same embedding space
as your large language model? Okay, so think about that. Okay, now we go to approach two.
Before I move on, I'd like to open the floor for any opportunities to ask anything so far for the
first part. Okay, if not, I'll carry on. So next is how we can use a knowledge large
language model to get the knowledge graph. So one is to use using few short and zero or zero
short prompting, like for example, length chain, okay, I don't think the approach is that great.
Okay, I found a better approach, right, using a better prompt, but we can potentially use
large language models to generate knowledge graphs. The other way is to use the embedding
space of the large language models to enrich the representational space of the knowledge graph.
So this is also quite interesting. Let's see how we can do both. Okay, the first one is,
okay, this is just some idea of how we can use it. Okay, we can, we can few short prompt
to generate the relations, okay, because large language models are just very versatile
and can be context driven to do it. And actually it's way better than, you know,
so this is my own experience. I use space scene to do name entity recognition and I use
large language models to do that. The GPT, chat GPT performs way better than space scene. Space
scene makes out a lot of the names, all right. So if we use large language models to generate
the knowledge graph, compared to traditional approaches, like spacey or some other verb,
VMP, you know, those kind of three parcels for language. Last time people used that to generate
the knowledge graph to find out what are the nouns, what are the verbs and so on. Okay, so that was
difficult to generate the knowledge graph because sometimes it miss out certain things.
But large language models are quite good. Okay, why not just use large language models directly
to generate the relations and the source and the destination. So indeed, this is what Lang
Chen did. Okay, so if you look at the graph QA prompt, this is the prompt. Okay, you are the
network intelligence. Okay, help to integrate stuff into a knowledge graph, extract knowledge
triples from text. Okay, a knowledge triple is a clause that contains a subject, predicate,
an object. Okay, subject is entity being described, predicate is the property,
object is the value of the property. Okay, there's a typo here. Okay, so this is the
zero shot prompting for Lang Chen. All right, this is not that good yet. So you need to give
some few short examples and they gave some few short examples in the prompt, like for example,
like this is the input, and then you can say that oh, Nevada is a state, Nevada is the U.S.,
Nevada is number one go producer in Go. So I don't like this example. Okay, because for one,
they did not say the state at all in the prompt. Like then you want the model to just plug the
plug the noun from thin air. So yeah, like here, I'm going to the store output none. Why, why is
the output none? It should be I went to store something like that. Yeah, so you should be able
to extract something from this. So I disagree with the examples that the Lang Chen one provided.
Okay, so I think if they improve this example, maybe theirs would work better. So let's take a
look at what I did later. So I'm not a fan of Lang Chen, by the way. Lang Chen prompts are very
worthy. So this is the other way that we can use the large language models to do the text encoding,
to do knowledge graph embeddings. So this is called KGE, knowledge graph embeddings,
is something like, you know, if you talk about the stuff like trans, these are like embeddings
that we can give to the source, to the destination, to the relation. So we can represent the knowledge
graph as embeddings. And we can use GPT or some large language model, okay, to generate some
embedding space here that you can then use like MLP, multi layer perceptron, and so on to map to
to the embedding space of the knowledge graph embeddings. So this is one way we can utilize
large language models to do it. Yeah, I mean, I was thinking, you know, like, why not just use this,
right? Why not just use LAM embeddings directly for knowledge graph?
I mean, LAMs are way better than, than doing graph neural networks, in the sense that, you know,
if you know the problems with graph neural networks, I'm just going to tell you the problems of graph
neural networks now. Okay, they have these two problems. Okay, this is one is called over squishing
or over squashing. And the other one is called over smoothing. Okay, what are these two problems?
Over squashing is that the information, because you pass the information into an embedding layer,
information gets lost at embeddings. Okay, so this over squashing thing is also a problem for
LAMs. So I'm not going to cover too much on it. The other problem that we have for this kind of
graph neural network over smoothing, okay, is that after you do message passing
or too many times, all embeddings look the same. Okay, so this is a big problem. Okay, I also
realized this, that once I did graph neural networks, you, like, you have two nodes, you pass
information to each other, and then you become the average of the information, you keep doing this,
right? Eventually, both nodes become the same, or very, very similar. Okay, so this is one of the
huge problems of graph neural networks. And I feel like the embedding space that is best
done, right, is not the way that we do message passing in graph neural network. We should just
ground it in the context using a large language model. And large language model update the context
quite well. Okay, then you can just use the embedding that is derived from that particular context
in, like, you can just put something like that, like, you can just say context. And then like,
I am a student or something like that. So, like, this context will update the definition
of the student here. So you can go through the transformer module. So this is the transformer
module. And then you can get the final embedding here. Yeah, at the final layer, right, before
the softmax, you can actually use the transformer to get the embeddings already. Why use knowledge
graph embeddings? Okay, so I'm just putting this question out here. So I hope those people
knowledgeable in this area can come and, you know, correct me if I'm wrong. But I don't see a point
in doing this. Yeah, right now. Okay, so let's leave it as that. And let's continue.
Approach tree. So the approach tree is how we combine both approaches to make a very, very
synergistic model where the large language model can generate the knowledge graph dynamically
and this knowledge graph is something like a dynamic memory that gets updated as the agent
explores the world and so on. And this knowledge graph can then inform the knowledge, the large
language model and ground it in consistent generation. So let's see how this works. So you
can see this is the diagram in the paper. And you can see like data. Okay, that's what me and Richard
discussed. Data, okay, will be from different domains to embed them into latent space. Okay, so
now we just assume that there's only one latent space but my view is that there's multiple latent
spaces. Okay, so right now we just treat it as there's only one latent space. You process the
information in that one latent space using knowledge graph and large language model in this
loop. Okay, so knowledge graph can ground the language model in consistency. Language model
can make the knowledge graph more expressive. Okay, and not as rigid as before. I mean, maybe you
can use embedding based knowledge graphs like then you can make the knowledge graphs like express a
lot of things more than just text alone. Okay, so this is one idea. You can use different techniques
to process it like graph theory networks from engineering, representational learning. Yeah,
I mean, this is just some big words, but the idea is you basically do some processing. All right,
you can use large language models to process. Or if you like it, you can process it using the
knowledge graph, which is like a graph neural network to process. Okay, or you can make the
graph neural network into text and then you can do some neural symbolic reasoning.
Okay, actually, this whole thing can be just summarized as neural symbolic reasoning because
plus the knowledge graph equals the symbol spot. And then the large language model is the neural
networks. So you can just summarize this whole thing as neural symbolic reasoning. All right,
then you can use this for different domains. Right, I think this is a very, very exciting path
that we should work on. Because right now with the power of language models, the knowledge graph
can be very, very flexible. And it's not a typical knowledge graph anymore, it can be
embedding based knowledge graph. And it can be context dependent knowledge graph. Okay, so
I really hope to work on context dependent knowledge graph, because I think that's the future.
Okay, not the traditional knowledge graph that you've seen everywhere in this presentation.
Okay, the knowledge graph embeddings must be able to do, must be able to
change based on the parent nodes. Okay, must be changed based on the context. And that's
something that is not done right now, at least based on my own awareness. I don't think that's
done right now. But that's very promising. Right, so one use case for this kind of system is fact
checking. As you know, large language models cannot do very badly at fact checking. It tends to
hallucinate a lot. And perhaps we can do like, a knowledge graph to like ground it in some facts
like some Wikipedia entries. No, you can use this to ground the inference. Okay, by doing inference,
you can then see whether or not like, is it, is there a path in the knowledge graph that matches
it? Or you do knowledge grounded inference, like you say, you must only use this information
that I extract for you in the knowledge graph and infer it. So this diagram here, unfortunately,
did not do the inference step. Okay, because they are still using birds. Okay, they're using bird as
a model. And what they did was they use the knowledge graph relations to do some pre training.
So it's like they take additional, like, additional tech samples, they just mask out certain words
based on the knowledge graph relations. And then they do the training here. So they just did the
pre training using the knowledge graph to give additional examples. Okay, so what I want the
thing to do is actually to do it during inference, if I cannot find any paper that does that so far.
All right. So I think this inference is more important than the pre training, you know,
this pre training, yes, it increases more data samples, because you can just mix and match the
knowledge graph, get more sentences out. Sure, I give it to you. And in fact, they improve by two
to three percentage points across soda benchmarks, this fact KB, you can go and check it out.
All right. But what I'm more interested in is how you use it for inference, not for pre training.
Okay, so let's see how length chain does it. All right, so now we come to the length chain part.
So actually, length chain is quite advanced, because length chain has a lot of the ideas that I
think should be done. All right, let's see how the length chain question answering graph question
answering is done. All right, so we have four steps. First step, we generate the triples from
the context. Okay, so we are like maybe a text context, you generate like the triples from it,
like the knowledge graph triples, you generate some from the query, you generate some entity
extraction. Okay, and then you use this at that entities to extract this relevant triples. Okay,
later I'll show you the how what I mean by this. And then you use this relevant triples to answer
the question. So I share with you these two documentation in case you want to see how length
chain graph QA does it. So step one, okay, generate triples from context. So like this context, I just
came out of it, right? Recently, my MacBook external camera in the viewing camera spot. So I'm
actually using the external camera right now to talk to you. And yeah, so this example is for Apple.
So let's assume that Apple created a new product called Mac and Cheese Pro, okay, in 2025. All
right, and then like Apple gave the invented cheese, okay, a rousing ovation in 2026 after
invented this in 2024. Right, there's also another company called Orange who created a competing
product called the Orange and Cheese Pro. The price was slightly higher at 5000 compared to 4000
from Apple. Okay, so this is a fictional example. Okay, and this is just to see like how good the
context is stored in the knowledge graph. So you can see that, oh yes, Apple announced Mac and Cheese
Pro, Apple gave cheese. So this kind of thing, right, like, is a bit contentious because like,
what do you mean by gave cheese, gave what? So this one needs to be improved a bit.
Apple ovation gave, Apple gave an ovation, okay, again to who? Right, so this one needs to be
improved as well. Okay, the price of the MacBook Pro is 4000. Yes, Mac and Cheese Pro is already
created. Orange and Cheese Pro, good. Orange and Cheese Pro, the price 5000. Okay, so you see,
it's not bad. Miss out dates. All right, and then like some, some relations are ambiguous.
So I don't quite like the way they did the triplet extraction and I think this is the
downfall of the Graph QA. So if you are going to use Lang chain for Graph QA, my advice is don't
use it. Okay, because you miss out a lot of stuff in the context. Okay, if you are interested
how they generate the context, you can go back to my earlier slides that I was talking about.
Yeah, so I mean, it's actually, let me show you, let me show you again the slides.
It's this one, this is the one that they did, like, this is the problem to generate stuff from the
text. Yeah, so the examples aren't very great and understandably the results aren't very great
as well. All right, so this is the knowledge graph that's generated. You can see like Mac and Cheese
Pro is cost $4,000 on price. Yeah, you can see that like stuff like price will contain like a lot
of relation because like price is very generic. Okay, Apple announced Mac and Cheese Pro. Okay,
so this is the knowledge graph that is generated. And we can see that like, next up we can use the
Graph QA chain in order to run the chain and see the answer. And you can see that if I ask it the
question like, when was the Mac and Cheese Pro announced? Okay, they couldn't find it. Okay,
because after they passed through the context, okay, they abstract like when was the Mac and
Cheese Pro, when did Apple announce the Mac and Cheese Pro? They abstract that in the query,
there's only Apple and Mac and Cheese Pro. So they check through all the knowledge graph to make sure
that you only have entities that match Apple or Mac and Cheese Pro. Okay, so I have a gripe with
this thing. Like if you use exact text matching, what if there's a spelling error, capitalization
error, or like related word, but not exact match. Yeah, so if you use exact text matching, which
is what they did for a length chain, like what if you don't get the right match? Okay, so I don't
quite like this approach. So yeah, this is something that I think could be improved on.
All right, and you can see that if I ask it like, when did they announce the Mac and Cheese Pro?
They couldn't answer. All right, because look at this knowledge graph here, there's nothing that
talks about dates here. All right, so they miss out quite a huge chunk of information from the
earlier context. So if we had fed in the earlier context directly, so I just use the length chain
LmChain agent. Okay, so I'm only using the LmChain agent or this to just show you that length
chain is not good. All right, I myself the new length chain. All right, so this is the idea
that like, after a while, you know, this is the context and then, okay, so this is not bad. I
mean, you could just do the same thing on ChatGBT, actually, you can just put like context
question and then ChatGBT will give you the answer. All right, so this LmChain works and this shows
that by embedding the text as a knowledge graph, it kind of miss out certain stuff. All right,
and what are the stuff we miss out? We miss out the years and we also miss out like,
Apple gave cheese. I mean, it doesn't make sense that way, right? I mean, look at the knowledge
graph like, what am I trying to solve there? Apple gave cheese. Where is it? Apple gave cheese.
Where's cheese?
Apple Cheese gave. Is there a gave anywhere?
Yeah, okay, I think this one maybe is the outdated, but the idea is that we can't really tell the
main thing in this graph because we miss out some information. And that's one of the issues of
converting text directly into knowledge graph is that you might miss out certain relations.
And actually, if you think about it, if we want to embody all relations,
there's just too many to embody, right? Yeah, it's too big to embody. So maybe the text itself
is way more expressive than the knowledge graph, if you think about it that way. Okay,
but again, you know, if you just use text only, you might face issues that, you know,
your OpenAI embeddings might be too restrictive. It's too broad-based. You need the embeddings
at different levels. So let's see how we would improve the Lang chain graph QA. I'm just using
my strict JSON framework here, which just basically passes the system prompt and then outputs as a
JSON in your own way. So I basically did what Lang chain does in a much shorter way. So I just
say you are a knowledge graph builder. You extracted an object one, object two relation.
Okay, I did not even put subject, object predicate. Okay, I mean, I just do like that. Okay, I just
want it to be as vague and as generic as possible because I want to capture as much information
as possible. Okay, so this was done in like 10 minutes. Okay, I don't really know whether this
is the best. You all can feel free to improve it. Okay, I have the Jupyter notebook attached in the
link. All right, so I gave you some examples like John bought the laptop. Okay, that's me,
all right. John built the house in 2021. Okay, that's not me, all right. But this is the idea of
like how we can represent like various relations like that. All right, then the output format is
just a knowledge graph. So you can see like Apple announced my and cheese bro, my and cheese bro
announced in 2025. Apple proved big hit. Okay, so again, this one is not exactly that great,
because it's not really Apple that prove a big hit. It should be the Mac and cheese bro that
prove a big hit. So this part needs to be from engineer a bit more. All right, Apple gave cheese.
Okay, again, like this is not complete. Okay, cheese browsing ovation into zero to six. So
actually we combine these two together. This is complete. So this is okay. All right, cheese
invented man cheese bro. Okay, man cheese bro invented into zero to four. Okay,
orange created orange and cheese bro. Yep, orange and cheese bro. The price is 5000 and
Apple prices 4000. So again, here has some issues or so, like here, instead of saying that this is
a Mac and cheese bro, because we should be referring to man cheese bro, it says Apple.
Okay, so unless we can sort of like link this later to Apple announced, okay, this part here.
So now you can see some issues with knowledge graph expressing stuff. It is not clean. All right,
it might truncate the information halfway. So this one needs more study as to how we can
express this in the knowledge graph better. But by expressing it in the knowledge graph,
you are able to then do knowledge graph passing, okay, and extract out the relevant entities that
related to the prompt. And you know, this is like, if you think about it, this is like
doing segmentation across like every few words in the segment one time. Yeah, so this is the
generated graph of what I did for strict design framework. You can see that compared to Lang chain,
this is what happens like we have way more relations, there's more relations here. And dates are captured.
Yeah, so this is something that I think needs to be investigated more. Mine is not the best,
but Lang chain is definitely not good. Okay, so this is something that needs to be done more if
we want to extract stuff out into the knowledge graph. And then like, should we use embeddings?
So if you want to use embeddings, then we cannot just use OpenAI API. Maybe you need to use like
Lama2. Okay, although Lama2 perhaps is not that great or so, because Lama2 is
not that good for multilingual. Okay, but Lama2 is the best possible substitute for
chat GPD right now. So maybe you can construct a knowledge graph embeddings using the Lama2 embeddings.
So food for thought. Next, we have this flexible knowledge graph passing. Over here, what I decided
to do is that we want to output only relations that are relevant to the question. And I just
passed in the entire knowledge graph here. So instead of coming up entities, I just asked it
to go through the entire knowledge graph because in case of words not exact or spelling errors,
GPD is able to catch it most of the time. I must copy it because GPD is not as great as
doing like counting letters and stuff. But if you misspell your words, but the meaning is about
that, GPD is able to extract the right entities. And here we can see that we asked it like,
when did Apple announce the man cheese bro? It captured exactly what we want. All right.
And this is the graph that is the past knowledge graph. So I'm talking about when you query the
knowledge graph, you pass it so that only relevant sections of the knowledge graph gets
come out, gets extracted. You ground this extracted part onto your text. Okay. And then
you can get the answer here. So 2025. So I just shown that like using this strict JSON format,
you are able to like, it's very flexible. You just need to key in the system prompt,
key in the user problem and output the format in terms of whatever JSON labels and the
description of the JSON. So I've been using this for a lot of my own use cases. And I'm just
adapting this for the knowledge graph. But this is really cool because you can then use this
past knowledge graph, like this idea of generating the knowledge graph and passing the knowledge
graph. You can use the knowledge graph as memory. Okay. And then you can update memory.
And you can use updated memory to extract relevant parts. Okay. So this called retrieval.
Okay. Use relevant parts to solve problem. So I really like this framework because this
knowledge graph as memory thing is something quite interesting. But how can we express it as memory?
That's the difficult part. Okay. So Richard asked, have I tried putting the graph into an FAIS
index? No, I haven't. But how will you do a knowledge graph putting onto the, like onto that
index? Because usually what I know is that you do the embedding and then you put the text. That's
for the retrieval or method generation. If you're doing knowledge graph, maybe you put the source
as the index. Okay. I'm not too sure. I'm going to check on this. Like how will you do this into
PyCon and stuff like that? But what I can imagine you doing for the knowledge graph is just put the
whole thing into some array and then just store the array. I mean, you can even put it as a JSON.
Yeah. So yeah. Okay. I don't have time to cover through the running of the Jupyter Notebook.
I'll just upload that separately. It's another video. But let's just go through like the last
five to 10 minutes. I'm okay to extend about 15 minutes if you all have more things to discuss.
Like we have discussed like how can we use knowledge graph better for last language models?
So first question, what are the failure modes of using knowledge graph for context representation?
And I think this failure mode is mainly like your knowledge graph may not capture all information.
Okay. And also the knowledge graph capturing
might truncate the information. So maybe using text directly
may be better. Okay. But harder to pass. Because if you are using text directly, you don't really have
like nice sections where you can pass the knowledge graph on. Yeah. So these are some of the things
about right now, some of the failure modes of this knowledge graph.
Anyone else has anything to add?
Just some random thoughts. Do you think it makes sense if we view the embedding space
itself as a form of generalized knowledge graph?
Embedding space as a genera. You mean the LM embedding space?
Oh, I mean, I mean, yes, but for some specific tasks you want to do, you can train a dedicated,
a separate dedicated embedding space. So because like you have all your entities inside their space
and the relative, I don't know, relative positioning of them kind of encode certain
relative information of them, right? Because I think the issue you mentioned here is,
I think it's just the graph, learn graph can be too sparse, right? You lose a lot of information.
But if inside embedding space, I don't know, it might help preserve more information,
although not very explicit information. It's just some random thought.
Ah, okay, I get what you mean. Like you encode knowledge graph as embedding space,
so like your source relation and the output are all embedding space and sink in destination.
I feel like my gut feeling is it can be much richer than just a typical traditional graph.
Definitely.
Now I kind of agree with you. It's just going to be hard to express the embedding space
using an OpenAI API. You might need to have access to the last average model directly if you want to do this.
Yeah, but definitely that's one of the ways that we can represent knowledge graphs.
Anyway, this is the second question also. Should we utilize the embedding space? Perhaps
for more expressive knowledge graph. Okay, but then if you think about like what I was talking about earlier,
context dependent
embeddings. If you are talking about context dependent embeddings, actually we can use
LLM to pass an update embeddings based on the parents of the node.
Yeah, so I was thinking of something like that. Like you can actually have a very, very
different interpretation of a certain word. Like for example, bank can be river bank or financial
bank, depending on like the context of it. If you are talking about river side, then it's like river bank.
Yeah, so you can actually use the last language model, extract out the hierarchy of the graph,
the front part of the graph. You can put it there and you can then pass the embeddings accordingly.
So I'm still thinking that perhaps just using the last language model embedding directly
might be a better bet. And then you can just maybe
use last language model to do this context thing. And then you can put this embeddings inside
your knowledge graph. Like what you said earlier. If there's a way to get the embedding space
directly from the OpenAI API, that would be great. But if not, we might have to use
Lama2 in order to do this embedding space knowledge graph.
But then again, is it really necessary? Can we just use text?
So this is a big open question. Should we use embedding space for the knowledge graph?
Or can we just represent it as text and then use the LM to generate embeddings after that?
So I leave that as an open question. I think both approaches are valid approaches. I just feel like
the way to input the knowledge graph as text is it will be much more interpretable. And also
you only need to train one embedding space, which is the LM embedding space. So I kind of prefer that.
Anyone else has any things to add?
Okay, if not, we go to the next question. Can LMS help with a more flexible
interpretation or construction of a knowledge graph? Okay, so our answer first. I think yes,
definitely. Just like compared to like spacey or like on noun, verb, pro verb, those kind of
stuff, like you are doing like the parse tree, compared to those very, very flexible. And you
are able to extract a lot more information. So like just based on the string, some prompt I
showed you earlier, you just hit object relation object, it captures almost everything. And that's
zero shot prompting. Granted, it did not capture the date at first. I had to use the examples to
give you the date. But compared to using this kind of like spacey and so on, like deep learning
approaches, like you'll take quite long to train a new kind of like knowledge graph constructor.
But with large language model, you can just use prompt engineering and get your knowledge graph
out. I think that's very exciting. Okay, last question. How do we know what nodes are important
to construct in the knowledge graph? Okay, because there's a lot of information, but not
everything is needed for your use case. How do we know? Okay, so my opinion, okay, my opinion is this.
You need to have biases based on the domain. And what are these biases? Maybe you can have
multiple biases. Okay, and then let's just choose the right biases later. So this is my idea of
intelligence right now. Okay, I'll share with you. Okay, this idea of intelligence is that there's
not just one abstraction space where you store your information, you store them in multiple
abstraction space. How do we get all these abstraction spaces? We basically just do rule
based abstraction, like you like maybe one domain is saying that oh, dates are important. So I store
the dates. Another domain is like all people, all person's names are important. I store the person's
names. Then maybe another domain will be like all places are important. I store the places.
Then when you want to solve the problem, okay, you will see which space is the best
for your problem. Okay, you will just like, maybe you look at all the abstraction spaces,
maybe combine two or more, or you just take one, and then whatever solves the problem works.
So this would form an approach that will be used later on. So if you think about it,
I'm just going to draw it here. Okay, I don't know whether I have space to draw it, but if you
look at the top right of the screen, okay, you have a problem, you have multiple abstraction
spaces, let's call this A. You have another abstraction space here, let's call it B.
And we have another expression space, we call it C. All right, so if we have three
abstraction spaces like that, okay, these are like three ways of doing it, three ways of doing the problem.
And then in order to solve any arbitrary problem later, you just take mix and match the abstraction
spaces to solve the problem. So yeah, increasingly, I've been feeling like this is the way to do things.
So yeah, I also use this for my abstraction reasoning copless paper. So this is the idea
that I have right now. You have different abstraction spaces. All these are rule-based.
Okay, we don't really have deep learning here, because if you have deep learning, you'll have
problems in getting a fixed abstraction space. You don't want the abstraction space to change.
Because if you change this abstraction space, you have to change whatever you learn on it.
It's like if I suddenly told you that math, the addition is now subtraction,
that I have to relearn all my math again, because I need to update that new knowledge.
So I'm saying that the basis is fixed, but then you just choose the right basis to solve it.
Okay, then you might ask me, if we do it like that, what if we don't have the right basis to
solve the problem? Okay, then the answer is you can't solve the problem. Okay, which might sound
a bit crude to people, but I feel like we can't solve everything. Like even humans, we have our
limitations. It's just that we work around our limitations and try to use our existing biases
to solve new problems. And I think that's intelligence. We don't really need to change
the abstraction spaces. We can just work with getting multiple abstraction spaces and then
just combining them. So I shared a bit about my view of intelligence here. And yeah,
that's more or less it for the questions to ponder. Anyone has anything else to add for any of these
questions? I just want to clarify one thing. I'm still not very sure about the motivation here.
So why we want this knowledge graph? Why use a knowledge graph, is it?
Because the internal representation of LLMs is already
already richer than a knowledge graph. So for example, you can just use the
last language model to help you to break down the task. Actually, because the way I did it before
is I want to specifically construct the knowledge graph first. Then I use that knowledge graph to
break down the task from a hierarchical or sequential learning of intermediate
goal to RL. But in the sense, you don't really need that particular task at least,
you don't really need to do that anymore because you just query the last language model.
At least the common sense way of breaking down a certain task. So you already have the
different sub tasks. So because my understanding is more coming from that perspective,
so I'm thinking why do I still need the knowledge graph?
I mean, as what I said in one of the first few slides, if you use retrieval or method
generation, you might miss out certain chunks of text. Knowledge graph provides a sort of hierarchy
that you can pass over and extract information. So it provides the structure.
But you still mentioned the problem is like even with knowledge, you have to
distill information. So there is inevitable that certain information are lost. So it be
it in a way of like you do chunk first, chunking first, then you summarize each chunk.
Then you summarize again, maybe like a hierarchical way of summarizing and all
even your knowledge graph, you only extract that the relation you think is important between
different entities. So you face this issue of having information lost.
Yeah, so maybe the correct way is not the existing kind of knowledge graph,
but a new kind of knowledge graph. But I do believe a graph based representation of knowledge is
important because when you learn new things, we typically try to fit in with our existing
knowledge and we build on the knowledge from there. So if you have some form of graph structure
to represent knowledge, you can actually like use that for learning as well. And that's where
I'm coming from. My intuition is it can be okay. So in that case, I can see how it can be
useful. It can be served as a heuristic for search. So if you want to understand a very
large chunk of text, if you extract like a rough graph representation of the information,
then you just do a heuristic search based on that. So even if this information loss is still can
expand upon just based on your existing knowledge graph, maybe it just gave you
some useful signal to tell you like which part of the text you want to do some search,
then you can just go and search. You don't need necessarily to stick with the strictly stick
with the graph. So in that way, like just a heuristic but it's a useful one.
Yeah, yeah. That's one way of doing it, like using a heuristic to search. So it's like replacing
the cosine similarity in retrieval of mental generation. You just pass through the knowledge
graph. Yeah. Because the chunking in like just naive way, it's just, it's just like because the
way we structure essay or structure the text is not necessarily just like each different
hierarchy like different object or different something that is quite intricate, right?
If for example, for some novel, you have foreshadowing, you have different way of writing,
you have plot device, then you suddenly at a certain very, you think apparently very random
point suddenly become very important. So just by naively chunking into like evenly
it doesn't, it doesn't work. But if you have a graph like sort of tells you this kind of structure,
then you're based on that to do some similarity. Definitely I feel like it can be more efficient.
So I think they give you a better way of doing chunking. Yeah. Yeah, I like this approach. In
fact, that's one of the motivations of using knowledge graph as well in order to find a better
way to pass it. I mean, we can pass through graphs pretty well. So yeah, we can perhaps get better
retrieval using knowledge graphs. So that's one provided you have all the information in your
knowledge graph, you can get better retrieval using knowledge graphs provided the first part
again, because that's the failure case we saw just now in the length chain graph answering agent.
But anyway, I think all the logition all stand from the fact that the context window is very limited.
So if we can solve that problem, then here actually, I mean, both way either way.
Yeah, I'm quite excited about this. In fact, I will spend like the next few weeks trying to
create a new knowledge graph. So I'll share with you after I create it, because there needs to be
some context dependent passing. And that's lacking right now in the knowledge graph that I see in
so far. You agree with the context dependent passing, right? Like how you interpret a certain
like note actually depends on the parents or depends on the position in the graph.
Yeah, even from a very traditional perspective, this very crucial as well.
Nice. Okay. Last minute or so anyone has any last points you want to add?
Okay. If not, thanks for coming. And yeah, if anything, you can still reach out to me on Discord
or LinkedIn. Yeah. And I'm looking forward to do this linkage between like large image models
and knowledge graph. This is what a lot of people call a neural symbolic. And I think this will be
very crucial for intelligence. And I can see how we can use this knowledge graph approach to like
learn stuff from the environment and use it in a learning agent. Like I like to call it reinforcement
learning agent, but it's not really reinforcement learning because there are no rewards. Okay. You
can just learn directly from knowledge in the memory itself. So I think this will be very crucial
for that kind of framework. And yeah, hope to share more of your after experiment with it.
Okay. If not, yeah. Thanks for coming. And I'll see you around. Okay. Bye, friend.
