WEBVTT

00:00.000 --> 00:07.080
Hi everyone and welcome to today's session. Today we'll be talking about a very exciting

00:07.080 --> 00:12.840
topic which tries to merge large language models and knowledge graphs together. So as

00:12.840 --> 00:17.480
you all already know, large language models are the recent hype. You can literally do

00:17.480 --> 00:22.240
anything or a lot of things with large language models by just changing the prom. It is very,

00:22.240 --> 00:28.320
very flexible. Get knowledge graphs extremely rigid. You have notes connected to other notes

00:28.320 --> 00:34.320
in relations, but they are also very informative because the relations don't change. The notes

00:34.320 --> 00:38.400
don't change. The large language models, one problem that they face is that they are a

00:38.400 --> 00:44.000
little stochastic. They tend to generate things that may not be grounded in facts. So it seems

00:44.000 --> 00:48.720
like naturally these two approaches seems like a good fit together. One is more flexible,

00:48.720 --> 00:54.640
which is the large language models. And one is more reliable, like the knowledge graph.

00:54.800 --> 01:00.400
Okay, so without further ado, let's begin today's topic. So I will roughly follow the

01:00.400 --> 01:04.480
framework of this paper called Unifying Large Language Models and Knowledge Graphs,

01:05.200 --> 01:11.760
a roadmap. Okay, this is by some IEEE fellows and senior members. I quite like the style of this

01:11.760 --> 01:17.360
paper, but I feel like a lot of the things that are surveyed in the paper are not exactly the

01:17.360 --> 01:22.640
latest large language model stuff. They are like the births, raw births, like basically the 2017

01:22.640 --> 01:28.560
to 2019 era, that kind. So I have supplemented it with some of the more recent advancements,

01:28.560 --> 01:34.160
like some length change stuff. So enjoy. Do feel free to comment anytime, because I think

01:34.160 --> 01:41.200
this is a very interesting field that can be expanded upon. Never before have we

01:41.200 --> 01:46.080
gotten large language models this powerful like chat GPD. And this is really something that we

01:46.080 --> 01:50.640
can look at to improve on traditional methods or even think of a new method that is not even a

01:50.640 --> 01:55.760
knowledge graph. Okay, later I'll share with you some ideas. Okay, so what are the pros and cons

01:55.760 --> 02:01.120
of knowledge graphs and large language models? So as I said earlier, you can look at the rough

02:01.120 --> 02:06.560
summary. I think this is quite a good summary. Okay, large language models, they are generalizable.

02:07.120 --> 02:13.360
Okay, they possess a lot of knowledge. All right. But what they lack is that they lack some form of

02:13.360 --> 02:20.320
understanding or facts. Okay, general language understanding. Okay, this one is debatable,

02:20.320 --> 02:25.680
because like GPT-4 can be said to understand language pretty well. Like NLU is like the

02:25.680 --> 02:31.200
ace most of the task there. Okay, so this one may be not so true in understanding, but for facts-wise,

02:32.320 --> 02:38.560
fact generation is still a problem right now. Okay, incompleteness. Okay, maybe I mean like

02:38.560 --> 02:43.680
sometimes they might generate things that don't answer the question fully. Okay, but increasingly

02:43.680 --> 02:47.920
this is not really a problem anymore. Okay, it's more of like the reliability right now. So I

02:47.920 --> 02:53.520
summarize this part here, reliability. Okay, I should use a different color. Let me just change

02:53.520 --> 03:02.400
my annotation. So I think the main thing that large language models lack are reliability. Oh no,

03:02.400 --> 03:16.080
it's the same color. Let me see. Reliability and consistency. This too. I mean, y'all have tried

03:16.080 --> 03:19.840
large language models before, right? You key in the same prompt. Okay, sometimes you get the

03:19.840 --> 03:25.440
different responses. Sometimes the response can be different. Like I said, should, it's a hot day

03:25.440 --> 03:28.480
today, right? Yeah, scrims on this can be yes, sometimes no, you know, that kind of thing.

03:29.120 --> 03:33.280
All right, so knowledge graphs, what do they have? Knowledge graphs have structural knowledge.

03:33.840 --> 03:39.120
They are quite accurate. Okay, decisive, I guess, if you can find a way to like connect an input

03:39.120 --> 03:44.080
node to an output node, you can say yes, there's a link between them. It's very interpretable. Okay,

03:44.080 --> 03:48.960
actually large language models are also quite interpretable. So it's not really a con here.

03:48.960 --> 03:54.880
Large language is both actually interpretability is also in large language models. Domain specific

03:54.880 --> 04:00.080
knowledge. Yes, but actually, if you think about it, large language models with context grounding

04:00.080 --> 04:06.000
also has domain specific knowledge. Okay, evolving language. This is something that is quite

04:06.000 --> 04:10.720
interesting. Large language models don't really have this evolving language unless you fine tune

04:10.720 --> 04:16.320
it. Maybe the recent Lamato you can fine tune on something. Okay, but you can also use something

04:16.320 --> 04:20.240
like a knowledge graph to ground the large language models. Can you see the synergy here?

04:20.240 --> 04:27.360
There's a lot of things that knowledge graphs do well, that is not exactly antagonistic or not

04:27.360 --> 04:31.840
exactly different in nature from the large language model, it can just be used to ground

04:31.840 --> 04:37.280
the large language model. So it's very interesting. So what are the cons of large language models?

04:37.280 --> 04:43.040
They hallucinate, black box, black domain specific knowledge. So it looks like there can be some

04:43.040 --> 04:48.960
synergy here. And let's explore how we can synergize these two approaches. Before we move on any

04:48.960 --> 04:59.200
quick questions so far. Okay, so this is the one way of getting context into a large language

04:59.200 --> 05:04.160
model and is used very often nowadays. It's called retrieval augmented generation.

05:04.160 --> 05:11.760
So this is the raw format you retrieve from a corpus of documents. You have a few documents

05:11.760 --> 05:18.560
here that you can retrieve from. And then maybe the user asks like, how much is a MacBook Pro?

05:22.880 --> 05:25.920
Right, recently I need to ask myself this question because I'm considering whether I

05:25.920 --> 05:30.720
should buy another one. So you know, they retrieve the relevant documents like,

05:30.720 --> 05:34.640
okay, this document is about MacBook. Okay, you retrieve the right documents.

05:36.720 --> 05:42.560
Okay, this document here is about maybe 2019. You can retrieve the right documents. All these

05:43.680 --> 05:50.720
documents will actually be your context over here. So you could have the context retrieves

05:50.720 --> 06:01.200
like that. MacBook Pro 2019 costs 5000 or something like that. Then you can have like in 2019

06:04.640 --> 06:12.160
Apple release MacBook Air 2019. Of course, I mean, I don't really know the details, but let's say

06:12.160 --> 06:16.240
these are the two documents you retrieve from your retrieval augmented generation.

06:16.880 --> 06:25.680
Okay, and after that, you ask the question like, how much is a MacBook Pro 2019? So it's been shown

06:25.680 --> 06:30.240
that if you use retrieval augmented generation, you can improve the consistency of the output

06:30.240 --> 06:35.040
of the large language model because you are grounding it in the earlier context, which is

06:35.040 --> 06:39.520
this part here, you are grounding it in this part here. So there's an element of grounding and

06:39.520 --> 06:44.000
this is very important for a lot of real world use cases. Because if you don't ground it, you can

06:44.000 --> 06:50.720
end up with quite nonsense generations. Alright, and just as a refresher, okay, what is the most

06:50.720 --> 06:56.240
common method used to select the top K like documents? Anyone can just blow talk? What's the

06:56.240 --> 07:03.680
most common metric to select the most relevant documents? That's a test of understanding. If

07:03.680 --> 07:08.160
you are doing retrieval augmented generation, what is the most common metric used to retrieve

07:08.160 --> 07:16.800
documents? To check the similarity. Anyone? You can write in your chat also. Dot product,

07:16.800 --> 07:22.400
yes, very good. Dot product or cosine similarity. That's right. So usually we use some form of

07:22.400 --> 07:28.880
embeddings. You embed your documents into a vector and then you use cosine similarity

07:29.840 --> 07:40.000
to check how similar the document is compared to the query. Okay, I'm going into some details

07:40.000 --> 07:44.960
over here. Okay, because actually this whole process of doing retrieval augmented generation

07:44.960 --> 07:51.200
and passing over knowledge graphs is very, very similar. Alright, in fact, you could even replace

07:51.200 --> 07:56.080
this retrieval augmented generation with like knowledge graph augmented generation is perfectly,

07:56.880 --> 08:04.080
I think it's replaceable. Alright, so this is some idea of how large language models can be made

08:04.080 --> 08:11.520
to be more accurate. Okay, using something like that. Okay, so this again, I just highlight the

08:11.520 --> 08:15.600
problems of large language models. Okay, may not be able to recall the knowledge, but you can retrieve

08:15.600 --> 08:20.400
the right context using this retrieval augmented generation provided you can retrieve the context

08:20.400 --> 08:26.880
currently. Alright, so this is a real world use case issue. Alright, I've talked to some people

08:26.880 --> 08:32.240
and they say that retrieval augmented generation with just the cosine similarity alone, okay,

08:32.240 --> 08:37.120
might not give you the right documents. So, you know, the embedding vectors train using

08:37.120 --> 08:42.000
contrastive loss, you know, they may not capture everything, especially if your document is very,

08:42.000 --> 08:46.720
very large. Okay, imagine you have only one vector to represent the entire document, and you have

08:46.720 --> 08:51.680
another vector to represent document, another document. So this is like document A, and another

08:51.680 --> 08:56.560
vector to represent document B, then you see how similar they are. But what about like,

08:57.760 --> 09:04.800
what if one document contains many parts? Right, I mean, each of these parts could have

09:04.800 --> 09:10.400
different meanings, right? Each of these sub parts could contain like, let's say you have this

09:10.400 --> 09:14.160
document could have a sub part that is like that, a sub part that is like that, a sub part that is

09:14.240 --> 09:19.200
like that. You know, they just aggregate all of this together into one vector, like that.

09:19.200 --> 09:23.840
Can you see that you're actually losing like information here, which means that when you

09:23.840 --> 09:28.640
retrieve something, let's say if I want to find out how to code, like a length chain question

09:28.640 --> 09:34.000
answer agent, you know, I'm not going to retrieve this vector because by vector similarity, my query

09:34.000 --> 09:39.920
is here. Alright, by vector similarity, maybe I'll retrieve a document that is like B instead,

09:39.920 --> 09:44.480
because like maybe it's nearer in terms of cosine similarity. Okay, I mean, it's greater

09:44.480 --> 09:49.440
is the opposite direction. Let me just make this vector look more similar to that. Like,

09:50.240 --> 09:57.040
let's say I have B is like that. So if this direction here is like how to code a length chain,

09:58.240 --> 10:03.280
QA agent, and this is the embedding vector for it, it goes in this direction.

10:04.080 --> 10:07.680
You know, you're not going to retrieve document A, although it contains that part over here.

10:07.680 --> 10:12.400
Okay, you're going to retrieve document B. This is one of the failings of the embedding

10:12.400 --> 10:17.360
vector. It just tries to capture the whole document into one vector. And this means that

10:17.360 --> 10:23.280
you may not be able to extract stuff out. Okay, Richard said something. This is why I keep saying

10:23.280 --> 10:29.920
context is king. Summarization is essentially impossible on segmentation on segmented documents.

10:30.720 --> 10:36.480
Yeah, definitely, because you summarize you lose information. Okay, so there needs to be like

10:36.480 --> 10:42.320
different hierarchies of how you retrieve things out, broad level, specific level. And you know what?

10:43.440 --> 10:47.520
Knowledge graphs might actually have that kind of hierarchy formulation. I'm actually jumping

10:47.520 --> 10:53.840
a few slides ahead, but give you an idea of why I'm so excited about this idea. All right. So

10:53.840 --> 10:57.680
actually some of the bypass that I've been telling people, I've been advising people is that like

10:57.680 --> 11:03.440
if you cannot get retrieval of mental generation to work, consider using like filters or like labels.

11:03.600 --> 11:12.160
So like this labels will say like, okay, maybe it's like product A, product B. So you know,

11:12.160 --> 11:17.760
instead of relying on just the retrieval of mental generation, or the embedding vector

11:17.760 --> 11:21.680
to actually embed the right knowledge, like let's say you have a length chain QA agent,

11:21.680 --> 11:27.520
I can tag this thing as a length chain QA agent inside this document. So there will be certain

11:27.520 --> 11:35.200
tags that you can have. So then you can then do like the embedding vector across the documents

11:35.200 --> 11:39.920
that have this text. Yeah, so maybe that's one way to like do a first hand filtering.

11:39.920 --> 11:45.280
Yeah, I mean, this is just like some, what do you call it, some bypasses to the downsides of

11:45.840 --> 11:52.160
embedding vectors for pros and similarities. So these are some ideas that could be done right

11:52.160 --> 11:58.160
now to bypass it. But if we have a way to use knowledge graph to do more broad level to more

11:58.160 --> 12:02.720
specific level extraction, maybe you don't even need all this, you can just pass through your

12:02.720 --> 12:07.840
knowledge graph and you can use that to ground the large language model. All right, so this is my

12:07.840 --> 12:13.040
last point here, knowledge graphs are useful to retrieve the right context, search the right

12:13.040 --> 12:17.440
keywords, retrieve the right subgraph. Like let me give you an example here, if let's say I have a

12:17.440 --> 12:27.520
graph like that. All right, so maybe this is a graph talking about like people who view Netflix.

12:27.520 --> 12:33.280
Okay, so these are the Netflix user graphs. So these are the users. And then maybe you have me

12:33.280 --> 12:39.920
over here, John. Okay, and then like the movies are watched. I like to watch the flash, the series,

12:39.920 --> 12:44.160
not bad. I highly recommend it. Then maybe we have another guy like maybe Richard can be here,

12:44.160 --> 12:50.880
watch other movies, like movie A and movie B, you know, yep. So if you want to like extract

12:50.880 --> 12:55.600
something out here, you can just search like for keywords and then you can just put this whole

12:55.600 --> 13:00.400
subgraph here. And then you can use this part here. Okay, how you want to pass it into the

13:00.400 --> 13:04.720
large language model, I leave it for future investigations. There are a few ways to do it.

13:04.720 --> 13:08.960
I will cover some ways today. So if you can pass this into the large language model,

13:08.960 --> 13:16.880
essentially, you can ground the LM in context of the knowledge graph. And then we can actually do

13:16.880 --> 13:23.040
this grounding at a more higher level grounding or more sub level grounding depends on which height

13:23.040 --> 13:27.760
of the knowledge graph you're going to take the notes from. All right, so I think this is a very

13:27.760 --> 13:33.760
exciting prospect. And yeah, I'm looking forward to see like how this can actually work. So I'll

13:33.760 --> 13:40.560
be actually working on getting this to work the next few weeks, right? Because I think doing

13:40.560 --> 13:44.400
something like that actually might help with the up challenge as well, the abstraction and reasoning

13:44.400 --> 13:51.920
corpus. So this is my latest kind of hit way that I'm going into. So this is from the knowledge

13:51.920 --> 13:56.160
graph conference. Okay, I actually listened to quite a few of their videos. This is the knowledge

13:56.240 --> 14:03.360
graph conference 2023. And there's a speaker, Danny from Dev6. I think I pronounced his name

14:03.360 --> 14:09.600
wrongly. But the idea is that if you are using chat GPT for your own applications, if you use chat

14:09.600 --> 14:15.200
GPT in different languages, you might get different outputs, okay, even for the same information. So

14:15.200 --> 14:18.560
you know, being Singaporean and you know, the presidential election is coming soon, I just

14:18.560 --> 14:22.560
asked like who is Singapore's current president right now. So you can see now is Halima Yacob.

14:23.520 --> 14:28.400
Yacob, sorry. And we asked the question in Chinese, all right, Singapore the

14:28.400 --> 14:36.960
總統, so you know, you say, Singapore, there's no president in Singapore. So this is basically

14:36.960 --> 14:40.400
the same information, you just translate it, you can get different performances

14:40.960 --> 14:46.160
with chat GPT. Okay, and the same thing for like if you use Lamatu, Lamatu is heavily trained on

14:46.160 --> 14:51.520
English. If you use Chinese, I'm very sure it won't do very well. All right, this is a practical

14:51.520 --> 14:56.480
problem of large language models. You know, the Chinese benchmarks like they use Ernie,

14:57.520 --> 15:02.080
Wenxing Yi and those other Chinese language models, they say that they perform better than

15:02.080 --> 15:08.640
GPT for. Okay, I mean, at first I was skeptical. Then now that I think about it, they might have

15:08.640 --> 15:13.840
done their evaluation on Chinese data sets. And their language models are fine tuned on the Chinese

15:13.840 --> 15:18.800
data set. So maybe there's some merit to their claims, okay, on the specific Chinese data sets

15:18.800 --> 15:25.120
here. So this is one of the things that knowledge graphs can actually help to solve, because knowledge

15:25.120 --> 15:31.120
graphs can sort of translate this thing, because knowledge graph is not language specific, you

15:31.120 --> 15:35.680
see. So your concepts like president, okay, regardless of how you represent it in words,

15:35.680 --> 15:41.120
okay, your Chinese words or English words, you can actually go to the same part in the knowledge

15:41.120 --> 15:50.080
graph. And then you can have the key words here, like Singapore, and then it's like Halima.

15:53.040 --> 15:57.840
So you can actually retrieve the kind of information regardless of language,

15:58.800 --> 16:05.680
okay, and then you can pass back this information back into the generation of the model. So this

16:05.680 --> 16:13.280
can go here, back here. So regardless of how you prompt GPT in a certain language, okay,

16:13.280 --> 16:24.160
you can do it. So maybe I just do the flow chart. So G-L-M, okay, language, language invariant

16:24.160 --> 16:34.000
representation. Okay, then you do your processing there. And then you go back to L-M. So if for

16:34.000 --> 16:39.840
those of you who have been to some of my other like discussion sessions, you would know that I

16:39.840 --> 16:47.760
like to say that this is the, this part here. This part here is what I call the abstraction layer,

16:49.280 --> 16:54.960
or the latent layer, latent space. So you process it in a way that is different from the input

16:54.960 --> 17:00.160
domain, but because the information you process is similar, in this case, we are still asking for

17:00.160 --> 17:04.720
semantic information about the president. You know, we don't have to do it in the language domain,

17:04.720 --> 17:10.720
we can do it in like maybe some representational space. It could be a graph, all right, and then

17:10.720 --> 17:16.240
you can use whatever you process the graph, you can go back to your input space. So this is one of

17:16.240 --> 17:21.360
the key advantages that, you know, if we could interface large language models with some form of

17:21.360 --> 17:28.720
graphical or some form of memory-based approach that is invariant to the input language type,

17:28.720 --> 17:35.040
you could get some performance advantage here. Questions so far? Anyone?

17:38.800 --> 17:44.800
Okay, so let's cover some of the basics. What is a knowledge graph? So I took this from the paper.

17:44.800 --> 17:51.040
The knowledge graph is basically a triplet consisting of source, destination, to relation. So like for

17:51.040 --> 17:57.280
example, Barack Obama was born in Honolulu. So this is the relation, okay, so relation.

17:58.560 --> 18:05.040
And this is like Barack Obama as the source, and Honolulu is like the destination. So each

18:07.520 --> 18:15.440
knowledge graph is made up of all these triplets joined together in various ways. And the idea is

18:15.440 --> 18:20.320
that you just need to connect those entities that are related to each other. You can like

18:20.720 --> 18:25.600
you can actually walk through the knowledge graph and get the information you need. Okay, so like

18:25.600 --> 18:30.320
there are of course like mega nodes, like for example, like Barack Obama will have a lot of

18:30.320 --> 18:34.960
connections leading out of it because you're describing the person. Then like stuff like places

18:34.960 --> 18:40.160
where a lot of things leading into it, because a lot of things like like are in the USA, a lot of

18:40.160 --> 18:46.960
things are in Singapore, you know. So this is the 20, 30 years ideas of knowledge graph. Okay, it's

18:46.960 --> 18:53.120
not too bad. Okay, but it's very restrictive. I personally think that there is a better way to

18:53.120 --> 18:58.800
represent information other than this kind of structure. Okay, and we can go and talk about

18:58.800 --> 19:04.560
it later in the discussion. All right, I have something in the chat. Richard says,

19:04.560 --> 19:10.080
is there a handy reference chart for how this looks or compares to word-to-veg and similar

19:10.080 --> 19:16.160
embeddings? Okay, so this typical knowledge graph that I'm talking about here does not

19:16.160 --> 19:21.600
have embeddings yet. Okay, but in the future iterations like in 2017 or 2018, I think people

19:21.600 --> 19:26.320
have come up with these things like knowledge graph embeddings. So they actually encode all

19:26.320 --> 19:31.200
this information here in some vector space. So like maybe like, it's something very similar to

19:31.760 --> 19:36.960
like the vector space that we see in in large language models. You can actually encode this

19:36.960 --> 19:42.000
thing in vector space. You can encode the relation here in the vector space or so.

19:42.640 --> 19:49.600
And then like, you can then encode this part here also in vector space. So like,

19:50.480 --> 19:54.160
so it's like doing a vector arithmetic now. So you can see that

19:56.960 --> 20:02.160
if I do a relation, it's just simply this one plus this one equals to this one. So I can do a

20:02.160 --> 20:07.680
like add a .2 in the first one here and then I can get. So if you have a sufficiently expressive

20:07.760 --> 20:13.120
enough embedding space, you can express the whole knowledge graph in the form of embeddings.

20:13.680 --> 20:19.920
And that is indeed what some of the later models do. In fact, this is highly related to

20:19.920 --> 20:26.160
graph neural networks. Because graph neural networks, they express each node as an embedding,

20:26.160 --> 20:30.400
then they do message passing, which means I share information with the other nodes. Like at each

20:30.400 --> 20:34.240
time step, I pass some information to the other nodes. And I mean, there are different variants

20:34.240 --> 20:38.800
of message passing. The most common is that the message meets in the middle, this one then updates

20:38.800 --> 20:46.160
both nodes. So there are a few ways of doing the idea of like updating the embeddings and so on.

20:46.160 --> 20:50.400
I'm not going to cover in detail about how all this are done because graph neural networks is a huge

20:50.400 --> 20:55.040
topic. Okay, personally, I think graph neural networks is probably not the answer to solving

20:55.040 --> 21:01.760
intelligence. I'm sorry to Peter Velikovic. I like what he's doing, but I don't think it's

21:02.640 --> 21:06.320
it's the right way to do it, like using differentiable deep learning to do it.

21:06.880 --> 21:13.280
So I think the knowledge graph that I've described over here, which is using vectors to do addition

21:13.280 --> 21:19.360
and then you get the other nodes. That's a very expressive knowledge graph. Okay, because you can

21:19.360 --> 21:28.080
actually express everything in vectors without the names. So you can theoretically do any kind

21:28.080 --> 21:34.160
of like addition provided, you know, nodes plus relation give you another node provided that

21:34.160 --> 21:40.960
exists. So if you could somehow represent the whole of the world's knowledge in the form of

21:40.960 --> 21:46.880
vector space, I say we are done. We can just like, we achieved zero short generalization.

21:46.880 --> 21:50.960
You just embed into that vector space and then you add something and then you go to somewhere else.

21:51.520 --> 21:57.760
Okay, but I don't think that's how intelligence is represented. Okay, because you know, there's

21:57.760 --> 22:05.040
this thing called like context dependent embeddings. Like I don't think like the word Barrett

22:05.040 --> 22:10.080
Obama would have the same embedding all the time. So like, for example, if you have Barrett Obama that

22:10.080 --> 22:19.120
is like at the White House, Barrett Obama at his house, okay, Barrett Obama at the beach or maybe

22:19.120 --> 22:25.200
different places of Barrett Obama will lead to different characteristics of Barrett Obama. Like

22:25.280 --> 22:30.400
he maybe is very serious in the office, but he's very relaxed at the beach. You cannot have the

22:30.400 --> 22:37.360
same embedding space to represent all this. You need to walk it according to the context. Okay,

22:37.360 --> 22:43.520
and that is something that I actually intend to try to do it. Like I try to do a very flexible

22:44.960 --> 22:50.960
like basically the information can walk according to the parent nodes in this new form of knowledge

22:50.960 --> 22:56.160
graph that I'm thinking of. Okay, so whatever I'm talking about is my own idea. I haven't seen

22:56.160 --> 23:00.880
any paper on it yet, but I think the current knowledge graphs will all fail at embodying

23:00.880 --> 23:07.440
intelligence because it's just too restrictive right now. Okay, Shang, you asked a question.

23:08.560 --> 23:14.080
I'm unfamiliar with graph theory, so hoping to know how do you represent factors as weights and

23:14.080 --> 23:19.920
how many can you add? Okay, could you elaborate what you mean by factors?

23:23.520 --> 23:32.560
Yeah, you mentioned that like you can add any form of intelligence, right? So take for example,

23:32.560 --> 23:42.720
if we are using, yeah, I actually didn't think of this example, but let's say just the simplest one,

23:44.160 --> 23:56.240
multi-layer map. Then for these roads, one weight could be how fast the speed limit of the road

23:56.240 --> 24:04.160
and another weight could be how occupied it is. Okay, so you are talking about like descriptions

24:04.160 --> 24:08.960
of an object, like all characteristics, attributes, you're talking about attributes of an object.

24:09.920 --> 24:17.280
Yeah, the weight of each line, correct, of each connection between the nodes.

24:18.240 --> 24:24.720
Ah, okay, so like how do you get this embedding here, right? Yeah, correct. Yeah, so perhaps like

24:24.720 --> 24:28.640
in your original embedding space, each of these dimensions could represent something already,

24:28.640 --> 24:33.920
like maybe one could represent road, one could represent like emotion or, you know,

24:34.000 --> 24:39.600
there are different domains that these dimensions could capture already. So if you really have that,

24:39.600 --> 24:46.400
you can just like add the relation in that specific dimension. Yeah, so of course, all this

24:46.400 --> 24:51.440
will need to be like learn somehow. So it's either learn through deep learning or some fixed biases.

24:52.640 --> 24:59.040
Yeah, so ultimately, how well the graph does will depend on how good your embedding space captures

24:59.040 --> 25:07.600
all the information. Yeah, okay, so I hope that clarifies. Yeah, thank you. So for now,

25:08.960 --> 25:13.920
just know that knowledge graphs have a few forms. Okay, the most simple form is that you take words

25:13.920 --> 25:18.400
and then you add another word and then you get another word. So this like describes a relation.

25:18.400 --> 25:23.520
The more advanced form will be to use embeddings. All right, so we will talk more about and then

25:23.520 --> 25:27.040
of course, the even more advanced form is evolving embeddings or context dependent

25:27.200 --> 25:32.800
embeddings, which is like the idea that I have. And it's also the idea that large language models

25:32.800 --> 25:37.520
actually kind of use because when we can ground large language models in different contexts,

25:37.520 --> 25:42.960
you get different outputs. So a large language model is context dependent processing. Okay,

25:42.960 --> 25:46.640
if you can embody that kind of context dependence into the knowledge graph,

25:47.520 --> 25:52.880
you will have a very powerful knowledge graph. So as you can see, whatever I'm sharing with you

25:52.880 --> 25:57.360
here today, I think that I'm not the answer. All right, I'm just sharing with you here because this

25:57.360 --> 26:04.960
is what is existing. Okay, I have a grander vision compared to all of the stuff that I'm talking about.

26:04.960 --> 26:10.640
Okay, so let's continue. All right, so knowledge graphs, okay, what excites me in knowledge graph

26:10.640 --> 26:16.160
is the very notion of hierarchy. And I think hierarchy is key to intelligence. You don't

26:16.160 --> 26:21.840
process things in just one domain. You process things in many domains. Like if I'm drinking a cup

26:21.840 --> 26:28.640
now, I'm just like drinking water from a cup, not drinking a cup. Drinking water now, I use my

26:28.640 --> 26:33.120
hand to move like that. But then if I think about, oh, how do I go to school? Then I think about,

26:33.120 --> 26:39.040
oh, I need to do the bus stop, I need to go to the MRT maybe, and then I need to take this bus

26:39.040 --> 26:44.160
or this train. So this is a more higher level planning. Okay, if I were to think about like

26:44.160 --> 26:47.760
how I move my left leg and right leg, left leg, right leg, I will take forever before I do some

26:47.760 --> 26:54.560
planning. So different problems require different levels of solution finding. And I call this

26:54.560 --> 26:59.440
different levels of hierarchy. So it might be up challenge, the abstraction of reasoning corpus.

26:59.440 --> 27:04.400
I use multiple levels of hierarchy, like we have a pixel level, we have an object level,

27:05.040 --> 27:09.120
you know, and then you express the input grid into different forms of hierarchy.

27:09.120 --> 27:13.360
And I find that this way can solve a lot of problems because different problems require

27:13.360 --> 27:17.840
different approaches to think of it. But you don't solve all problems using trigonometry,

27:17.840 --> 27:23.200
you solve some using algebra, you solve some using set theory. So you have different ways

27:23.200 --> 27:30.720
of viewing the problem. And knowledge graph, you can actually use this to extract different,

27:30.720 --> 27:34.800
like at the top layers of the knowledge graph, typically are the more broad concepts and the

27:34.800 --> 27:41.600
bottom will be more of the, more of the general concepts. So you can see in this, this is the

27:42.000 --> 27:47.280
the SICK knowledge base. SICK is a 30 year old project trying to embody the world's knowledge

27:47.280 --> 27:52.480
in a knowledge graph. They are still trying to do it, but it turns out that this is a very

27:52.480 --> 28:00.080
hard thing to do because the knowledge graphs itself, it embodies like one relation is like

28:00.080 --> 28:04.560
a confirmed relation. But sometimes, you know, based on the context, you may not have that

28:04.560 --> 28:09.200
confirmed relation. So again, this is the context dependent knowledge graph I'm talking about.

28:10.160 --> 28:15.520
Also, another thing is a lot of times we do things, but we don't really know how to express it

28:15.520 --> 28:22.480
in words. So if you want to express the whole idea of logic in words, it's a very, very difficult

28:22.480 --> 28:26.880
task. Because sometimes we don't even know why we're doing something. Okay, there's bound to be a

28:26.880 --> 28:31.840
point of time that logic cannot express things. So you can go and look at this thing called

28:31.840 --> 28:38.880
Godot incompleteness theorem. If you use mathematical logic to express things, there comes a point in

28:38.880 --> 28:48.160
a time whereby logic cannot solve. Because the way to solve it lies beyond logic. It cannot be

28:48.160 --> 28:55.440
Godot incompleteness theorem. There's something like this. This sentence is false. So if you can

28:55.440 --> 29:01.680
represent this as a Godot number, this kind of sentence, and then you say that, oh, this number

29:01.680 --> 29:06.640
is true. But then this number says that this number is false. So it's like you have a self-referential

29:07.360 --> 29:12.400
loop. So if you use logical prepositions, and knowledge graph is sort of like a logical

29:12.400 --> 29:20.080
preposition, A goes to B, B goes to C, you might face this problem that you can actually go in

29:22.080 --> 29:28.480
you can actually go in a loop that contradicts itself. So that's one thing that knowledge

29:28.480 --> 29:36.000
graphs may have some issue with if we do it being 100% fact. If A links to B all the time,

29:36.080 --> 29:42.640
sometimes you might actually have a link that contradicts itself. That's one issue of the

29:42.640 --> 29:49.920
knowledge graph. The other thing is, so burying all these issues, one thing I like about knowledge

29:49.920 --> 29:55.200
graph is that you can see in this diagram here, you start off with small stuff like thing, and then

29:55.200 --> 30:01.840
you go to like individual, you go to collections, and then you have different ways of doing it,

30:01.840 --> 30:09.040
time, movement, and so on. Then you have agents, actors, plans, and goals. I mean,

30:09.040 --> 30:12.240
if you think about it, kind of it's like how large language models is evolving now, right?

30:12.240 --> 30:17.200
We are kind of at this stage, we are agents now. So after that, we have organization of agents,

30:17.200 --> 30:22.080
we have activities. Okay, hopefully we don't get to military warfare, you know, because like,

30:22.720 --> 30:28.800
so this is like the evolution of a population. So it's quite nice, and like you can capture all

30:28.800 --> 30:33.840
this knowledge from, okay, so I wanted to say this is for more like micro level

30:34.800 --> 30:41.040
to more macro level. And the macro level is actually the sum of the micro level. So

30:41.920 --> 30:45.280
maybe the arrow should be drawn the other way, the arrow should be drawn like that.

30:46.080 --> 30:50.480
You take from the micro stuff, and you go to the macro stuff. So this is the knowledge

30:50.480 --> 30:55.840
that we accumulate, right? And knowledge graphs can capture this quite well, because of the way

30:55.840 --> 31:00.320
you take from source relation to destination, you can capture from the micro level, you do

31:00.320 --> 31:07.680
all the branching, and then you end up with the micro level, right? So this was in 2016,

31:07.680 --> 31:13.840
by the way, I couldn't find this in the sick website today, right? So this is the sick knowledge

31:13.840 --> 31:18.400
graph. This is like a very, very tiny representation of how the knowledge graph looks like. I just

31:18.400 --> 31:22.080
wanted you to see like how one of the largest knowledge graph in the world looks like. So

31:22.080 --> 31:26.560
you can see like you have all these like the fortune companies, you have all these like functions,

31:27.360 --> 31:33.200
like all these like look like some form of like math stuff, right? Yeah, so you have like people

31:33.200 --> 31:39.920
over here. So you have different areas of congregation of all this knowledge, right? Then

31:39.920 --> 31:44.720
in order to pass through the knowledge graph, you have to use something very, very similar to SQL,

31:45.360 --> 31:49.920
structured query language, you like say that, oh, if I want to get a frightened person, I want to

31:49.920 --> 31:54.880
get the entity X that is a person, and then fuse emotion that is fear at a very high level.

31:55.520 --> 32:00.800
So you have to do this kind of stuff, right? So immediately you can see that knowledge graph

32:00.800 --> 32:06.240
right now can be immediately improved by large language models in one aspect. Okay, and this

32:06.240 --> 32:10.880
aspect is that we can straight away use the large language models to generate this structured query

32:10.880 --> 32:19.600
language. Okay, so if all of you are thinking about this, like if you want to get like a very,

32:19.600 --> 32:24.160
very rigid programming language out, okay, you can actually write what you want in free text

32:24.160 --> 32:28.240
and then say convert this to SQL, and you can get it out. So you can do the same thing for

32:28.240 --> 32:33.600
this sick language. You give it some examples of how the language works. You say, I want to get

32:33.600 --> 32:37.920
a frightened person and then, you know, chat GPD is quite good at getting stuff like this out.

32:38.400 --> 32:44.080
Okay, no more SQL, right? I love it. Okay, if you need to use SQL, just use chat GPD.

32:44.080 --> 32:48.960
Okay, it's a very good replacement. All right, so this is one way large language models can

32:48.960 --> 32:52.400
already help to benefit knowledge graphs right now because it can pass through it

32:53.120 --> 32:57.920
using very human readable and understandable syntax. Like this kind of thing is not very human

32:57.920 --> 33:03.680
understandable. You can use free text to do it and we can do it right now. But more importantly,

33:03.680 --> 33:10.880
what can large language models do to help knowledge graphs? Okay, or what can knowledge

33:10.880 --> 33:15.120
graphs do to help language models? Okay, so before I move on to that, let's just talk about some other

33:15.120 --> 33:19.760
ideas I have. So I'm actually a reinforcement learning person. So like, I feel like knowledge

33:19.760 --> 33:24.480
graph can also represent stuff like different states, like you have different tiredness,

33:24.480 --> 33:30.240
they drink coffee, and then you're not awake. So if you know in the literature of

33:33.040 --> 33:36.240
reinforcement learning, this like a Markov decision process where these are the states,

33:37.200 --> 33:41.680
okay, and these are the actions. And then this is the next thing.

33:43.520 --> 33:47.440
Okay, so you can actually use knowledge graphs to represent stuff like this as well.

33:47.440 --> 33:54.240
Okay, because it's quite anything that has a link like that, you can represent this easily. Okay, so

33:57.680 --> 34:01.840
all right, this is perhaps the most important slide for today. Okay, this is not in the paper

34:01.840 --> 34:07.760
that I referenced, but this is the thing that I was thinking about. It's like knowledge graph is

34:07.760 --> 34:13.200
actually sort of a tool that can be used by the agent. So like retrieval of method generation

34:13.200 --> 34:17.840
may not get the right passages because like the embedding space may not be good. Perhaps we can

34:17.840 --> 34:22.160
use like a form of knowledge graph passing, okay, you can extract relevant parts of the knowledge

34:22.160 --> 34:27.280
graph, you can retrieve the context based on that. Okay, so you can ask the knowledge graph to get you

34:27.280 --> 34:33.680
the subgraph. The subgraph, you can then use it to ground the context of the agent and how you use

34:33.680 --> 34:38.000
it to ground. Okay, it's up to you. Okay, some people might use graph neural networks. I don't

34:38.000 --> 34:42.800
advocate for that. Okay, one other way of doing it is to just convert it back to free text. Okay,

34:43.440 --> 34:47.920
as easy as that. So you use the knowledge graph to extract out the relevant purposes and avoid the

34:47.920 --> 34:52.960
need for the embedding space, the open AI embeddings. Okay, you use the knowledge graph to

34:52.960 --> 34:57.600
extract it. Then you take the stuff that you extract from the knowledge graph, pass it back as text

34:57.600 --> 35:08.160
and then go back to the agent to ground it. All right. So yeah, one other way of, one good thing

35:08.160 --> 35:12.880
about this is that if you have stuff like if you are doing this for a robot, okay, that experiences

35:12.880 --> 35:18.640
the world, you might actually be able to use this knowledge graph. Okay, I'm conflating the

35:18.640 --> 35:22.400
term knowledge graph, but this knowledge graph can now be the state action state graph. You know,

35:22.400 --> 35:28.560
you can actually model relations of the world easily. Like I always believed like we learned

35:28.560 --> 35:33.120
from taking actions in the world. So we can actually build this knowledge graph dynamically.

35:33.120 --> 35:37.120
This is the third point. Okay, you can gain knowledge of the world. We can build up this

35:37.120 --> 35:41.280
knowledge graph bit by bit. All right. And then we can then query this knowledge graph

35:42.320 --> 35:50.160
and get answers from the knowledge graph to inform our choices. Okay, so about how we can get this

35:50.160 --> 35:54.960
part here, this is a huge thing here. Okay, because I believe that there's one thing that's missing

35:54.960 --> 36:01.040
in current knowledge graph and this thing is called changing the memory to the context

36:01.760 --> 36:07.280
at hand. Okay, so I treat the knowledge graph as memory. So like when you retrieve things from

36:07.280 --> 36:12.320
memory, okay, and then you want to apply it to the current state right now, current state of the

36:12.320 --> 36:18.240
world right now, you don't really want to just use that memory itself. You want to adapt that

36:18.240 --> 36:24.000
memory such that it will be relevant in this current state. Like if I have drank like coffee

36:24.880 --> 36:31.360
at school or not, drink coffee at home now, okay, you know, something like that. This, I will need

36:31.360 --> 36:36.480
to adapt that memory of drinking coffee somewhere else and then adapt it back to here. Okay, there's

36:36.480 --> 36:41.120
no point in giving me the memory of me drinking coffee somewhere else because it doesn't adapt to

36:41.120 --> 36:45.680
the current situation. So if you can adapt this knowledge graph to the current situation,

36:46.320 --> 36:50.960
that will be great. Okay, that will be great. So that's something that I think I'm trying to

36:50.960 --> 36:56.720
look into because you don't just want static knowledge extraction. Okay, you want knowledge

36:56.720 --> 37:02.640
extracted and manipulated to fit the current context. Okay, of course, for those of you all in

37:02.640 --> 37:07.680
my discord group, I've been thinking about memory recently. And you know, human memory is very

37:07.680 --> 37:13.440
malleable. Like if you think about something, you might actually affect that memory of it.

37:13.440 --> 37:17.760
So like a lot of times people in the childhood, they think that they have certain memories again,

37:17.760 --> 37:22.560
like maybe you are lost in a supermarket. So if I keep asking you questions about it, I say,

37:24.000 --> 37:30.000
who was the stranger with you when you were lost? Okay, so maybe there was no stranger. But if I keep

37:30.000 --> 37:34.480
asking you guiding questions like that, eventually, you might think of your memories like, oh, yeah,

37:34.480 --> 37:39.120
a stranger let me home after I went, I was lost in the in the in the mud. Okay, but that that may

37:39.120 --> 37:44.400
not have happened. That memory has changed because I've asked you certain guiding questions. And then

37:44.400 --> 37:50.480
you think that certain things are now in your memory. Right. So whether or not we should change

37:50.480 --> 37:57.200
this memory and affect this knowledge graph, I leave that to future discussion. Okay, because

37:57.200 --> 38:01.360
this is something very interesting, like should we change the existing memory that we have,

38:01.360 --> 38:06.240
based on the current context, get our brains that our brains do that. Okay, but should we do this

38:06.240 --> 38:12.480
for this kind of practical systems? Okay. Yeah, so we say humans hallucinate. Yeah, of course,

38:12.480 --> 38:18.400
we hallucinate a lot. And that is why actually, we are quite similar to large English models in

38:18.400 --> 38:22.960
that sense. Now people always say large English models not very reliable. Are humans reliable?

38:22.960 --> 38:27.440
Our memory is not that reliable, actually, if you think about it. But honestly, I cannot trust

38:27.440 --> 38:33.680
my memories that that much, because like sometimes if it's too far away, it can change like the book

38:33.680 --> 38:40.240
that I've been reading. It will say that like flashback memories, which people think that are

38:40.240 --> 38:44.640
very, very pertinent, flashback memories are memories like, like, you know, 911 collapse.

38:45.360 --> 38:49.360
People tend to remember what they were doing at that time, because it was so significant.

38:49.920 --> 38:54.560
It turns out that this flashback memories can be wrong. Okay, it can also be, it can also be

38:55.360 --> 39:01.280
change. So this is a very interesting thing. You can actually use like the current context

39:01.360 --> 39:05.840
to affect the memory you have. So you might actually affect the knowledge graph about whether

39:05.840 --> 39:11.200
or not we want it to be that way. Okay, we have to think about that. Okay, I digress a bit. Okay.

39:12.400 --> 39:16.560
But let me just get back to topic. Okay, today we have quite a few slides to cover. There are three

39:16.560 --> 39:20.560
approaches that I want to talk about today. First is that you can use knowledge graph to

39:20.560 --> 39:24.960
enhance your large language models. And this means that you can give it structured stuff,

39:24.960 --> 39:29.840
like domain specific knowledge. In some sense, it's like text based grounding is the same as

39:29.840 --> 39:33.840
retrieval of mental generation, just that now you take the information from a knowledge graph.

39:34.720 --> 39:40.320
Number two, you use large language models, expressivity, okay, and make a better knowledge

39:40.320 --> 39:44.240
graph. Okay, I like this approach as well. Okay, we will see how to do it. And lastly,

39:44.240 --> 39:48.160
you combine both approaches, you can get a synergized large language models and knowledge

39:48.160 --> 39:54.400
graph. And I think something like this will be able to embody intelligence. Okay, but not the

39:54.400 --> 40:01.360
current knowledge graph. We need to change it to a dynamic knowledge graph. Okay, what is a dynamic

40:01.360 --> 40:06.880
knowledge graph? Maybe I'll talk about it next time. Okay, after I flesh out some ideas that I

40:06.880 --> 40:11.840
have right now, I will create this dynamic knowledge graph. Okay, I think the current knowledge

40:11.840 --> 40:15.920
graphs are not the answer. We need to have a different kind of knowledge graph. But if we use

40:15.920 --> 40:22.240
this, I think we can get intelligence. Okay, let's move on to the next point. Approach one,

40:22.240 --> 40:27.200
knowledge graph augmented large language models. Okay, so there are two ways I can,

40:27.200 --> 40:31.200
I summarize the paper in two ways. The main thing is one, you can just put the knowledge graph as

40:31.200 --> 40:36.400
text. And the other one is treat this as an object. And what kind of object? Okay, you either use like

40:36.400 --> 40:41.360
graph neural networks, or you can use an embedding space. I mean, the one that was used was trans

40:41.360 --> 40:48.080
e trans embedding. You can go and search the paper trans. So this are some ways that we can use the

40:48.080 --> 40:55.120
knowledge graph, okay, to pass it. Let's go through the first way. So the first, oh, sorry,

40:55.120 --> 41:00.880
this is basically a pipeline for retrieval of large language models grounding. First,

41:00.880 --> 41:04.160
you use some form of knowledge retrieval, like, you know, retrieval method generation,

41:04.160 --> 41:10.080
you use cosine similarity, you get certain facts or some documents. So I'm just relating this to

41:10.080 --> 41:14.480
retrieval method generation, because they are almost the same. All right, you take in the facts,

41:14.480 --> 41:19.040
you ground the large language model, you get the answer. Okay, and over here in the paper,

41:19.040 --> 41:22.720
they put back propagation. But you know, how are you going to back propagate this knowledge retrieval?

41:23.600 --> 41:28.400
You're going to end up with some, like, very, very weird way of doing back propagation. I don't

41:28.400 --> 41:32.160
think back propagation, I don't think back propagation is the answer here. Maybe you want

41:32.160 --> 41:37.040
to back propagate your LM to find you think, okay, I grant that. But this part here, to back

41:37.040 --> 41:41.840
propagate to the knowledge retrieval, I don't think that should be done. All right, because this

41:41.920 --> 41:48.000
back propagation thing will lead to, like, changes in embedding space. And then if you

41:48.000 --> 41:53.520
change your knowledge retrieval, you also need to change your large language model. It's a never

41:53.520 --> 41:58.000
ending cycle of chasing each other. Like, if you change the knowledge embeddings for the knowledge

41:58.000 --> 42:01.440
retrieval, you also need to change how you interpret them in the large language model. So,

42:02.320 --> 42:06.240
yeah, I don't think you should use back propagation for the knowledge retrieval. You should probably

42:06.240 --> 42:12.000
use, like, memory methods, other methods, like you can say that, okay, what worked, what did not

42:12.000 --> 42:18.640
work, what worked, what did not work. Okay, you can reference this paper called Voyager. Okay,

42:18.640 --> 42:22.800
so there are these automatic curriculum learner. I think you should train the knowledge retrieval

42:22.800 --> 42:27.280
like the automated curriculum learner. You just ground it in some examples of what works, what

42:27.280 --> 42:34.400
doesn't work. You don't have to use back propagation for that. Okay, so the main pathway for knowledge

42:34.400 --> 42:39.360
graph for tax, for LMS is like that. You take the knowledge graph, pass through it, get some

42:40.160 --> 42:45.200
facts, and then feed it into the large language model. Okay, that's the main pipeline. Okay,

42:45.200 --> 42:57.600
questions on this? Okay, let's move on. Okay, so this is one of my favorite papers. Okay, this is

42:57.600 --> 43:03.680
the Generative Agents paper. They have 20 agents in the sandbox interacting with each other. And

43:03.680 --> 43:08.880
one thing that struck me quite well for this paper is that they actually use JSON structure

43:08.880 --> 43:16.160
to ground the actions. So, for example, if you want to ask like Eddie Lin, he currently is in

43:16.160 --> 43:21.520
the Lin family's house. He's in the bedroom actually on the desk. Okay, so you can ask the

43:21.520 --> 43:26.720
agent, okay, this is actually the chatGPD prompt. Okay, you can ask the agent like, okay, these are

43:26.720 --> 43:34.400
the other areas that we have. Okay, and all these other areas are obtained from the JSON.

43:36.560 --> 43:39.120
Actually, the JSON is like a knowledge graph.

43:41.840 --> 43:47.760
Okay, because we actually have hierarchies like Lin family house has a bedroom, has a study room,

43:47.760 --> 43:52.000
has a kitchen, you know, this is something like a knowledge graph. If you ask me like,

43:52.000 --> 43:56.720
they are just like representing the hierarchy of the house. Like, I mean, if you want to treat it

43:56.720 --> 44:02.480
as a knowledge graph, you will say like, this is the house. So, this is the Lin's house. Lin's house.

44:03.280 --> 44:09.520
Lin's house. I've just put Lin's age. Okay, then you can have like, the relation will be contains,

44:10.720 --> 44:15.440
okay, or comprises, I mean, contains, then you can have like bedroom.

44:16.320 --> 44:22.480
Yeah, so the JSON kind of hierarchy is a subset of what a knowledge graph can embody.

44:23.200 --> 44:29.360
All right, so I treat this as a knowledge graph. So you can, you can sort of ground the agent.

44:30.160 --> 44:34.560
Okay, like this is what the agent knows. Okay, this is the current memory that the agent has

44:34.560 --> 44:39.840
as a form of knowledge graph. Like this are the kind of areas that we actually know from the world.

44:39.840 --> 44:44.960
Okay, this is like, if you talk about grid cells and play cells, maybe find out more areas.

44:45.040 --> 44:49.520
Okay, you can ground them. Okay, these are your semantic knowledge or oppositional knowledge that

44:49.520 --> 44:55.040
you have about the world. And actually, these two are the positional knowledge. This is the first

44:55.040 --> 45:02.560
one is knowledge about the house, knowledge about house. And then the second one is knowledge about

45:02.560 --> 45:14.160
the world. Yeah. So you, you have all this knowledge, you can ground the agent to choose

45:14.240 --> 45:20.000
a specific place. Imagine if we did not ground the agent with all this stuff at the top,

45:20.000 --> 45:25.680
you just ask, where should Eddie Lin go to? Then Eddie Lin might, the LL might reply,

45:25.680 --> 45:29.680
Eddie Lin should go to the supermarket or something like something that is irrelevant to the game world.

45:30.320 --> 45:36.400
Right, so because we grounded it with some idea of what kind of possible areas that the agent

45:36.400 --> 45:44.000
should go, the agent is able to choose one area from the above list. And how is this list generated?

45:44.000 --> 45:49.120
It's generated from passing some form of knowledge graph. And this is what I mean by using knowledge

45:49.120 --> 45:54.080
graph as text to ground the large language model. So you can use this recursively, you can say that,

45:54.080 --> 45:59.920
oh, currently you are in the maybe common room. Okay, what, where in the common room would you

45:59.920 --> 46:04.960
like to go? We like to go to the sofa, to the mirror, and you can do this recursively. Okay,

46:04.960 --> 46:11.840
and then you can get a very, very specific area that the agent is going. Okay. Any clarifications

46:11.840 --> 46:18.560
on this so far before I move on? Yeah, if you haven't read this paper, go read it. Okay, this

46:18.560 --> 46:27.760
paper is good. It's one of the better ones. All right. So we have the Chinese LLM, it's called

46:27.760 --> 46:34.480
Ernie. All right, and this, what they do is they actually use two hybrid ways of generating

46:35.760 --> 46:41.120
the output. So they say that large language models lack grounding, lack consistency. So we use

46:41.120 --> 46:45.760
a knowledge graph. Okay, I granted that. But then I look at that structure and they're like,

46:45.760 --> 46:51.440
oh man, what is this? So they actually have a large language model. This is the typical

46:51.440 --> 46:54.640
transformer architecture. So this is a typical transformer on the left side.

46:57.120 --> 47:02.240
So they have two encoders. Okay, one is the T encoder, which is like the text encoder. And why

47:02.240 --> 47:06.960
is the knowledge graph encoder? So the knowledge graph encoder uses this thing called trans embeddings.

47:06.960 --> 47:10.400
Okay, I'm not going to go through that, but they train that embeddings using like,

47:10.400 --> 47:13.040
they take one vector and take another vector, then they connect the,

47:14.160 --> 47:18.640
just draw the diagram here for you to see. So you have one vector A and another vector B,

47:19.360 --> 47:24.800
and then you create another vector C here. So you can keep like using this vector A,

47:24.800 --> 47:28.640
you take another vector, extend from it, and then you can train on this relation C. So, you know,

47:29.360 --> 47:33.920
this is how the trans embeddings are trained, trans E. Okay, and they use this kind of embedding

47:33.920 --> 47:37.840
space. Okay, you can do self-attention, and then you can do cross-attention across

47:38.640 --> 47:42.960
both the trans, the text embeddings as well as the knowledge graph embeddings.

47:42.960 --> 47:46.800
And then hopefully you get some output, right? And then you get some text outputs here,

47:46.800 --> 47:51.520
and then you can update your knowledge graph to the knowledge graph outputs here. Okay, so this

47:51.520 --> 47:57.440
is a way to embody a knowledge graph as some embedding space. Okay, and then we can use like

47:57.440 --> 48:03.840
the attention to like, attend to like the text-based stuff as well. So, yeah, this is just one way of

48:03.840 --> 48:09.280
doing processing using a knowledge graph. As you can see, I don't really like it. I mean,

48:09.280 --> 48:13.680
I think that it's too convoluted. Like, okay, so this is another discussion question that I'd like

48:13.680 --> 48:18.480
you all to think about. Should we have separate embedding space for this large language model

48:18.480 --> 48:23.440
stuff and for the knowledge graph embeddings? Like, should we use two different embeddings?

48:23.440 --> 48:26.880
Okay, should we use two different embeddings?

48:26.960 --> 48:34.640
Yeah, or should we use the same one? Yeah. Okay, then also more generally, like if you want to have

48:34.640 --> 48:41.200
multi-modal embeddings, like you have text, image embeddings, you have audio embeddings,

48:41.200 --> 48:45.680
okay, if you want to do a multi-modal large language models, you can actually also put them

48:45.680 --> 48:51.920
into distribution model at the end here. Okay, but the question is, in fact, knowledge graph can

48:51.920 --> 48:56.800
be multi-modal also, you can actually also put it here. The question is, all this image

48:56.800 --> 49:01.120
and audio embeddings, okay, you can put it in the large language model, you can also put in the

49:01.120 --> 49:07.760
knowledge graph. But why not just use a single embedding, right? Why do you need to? You are

49:07.760 --> 49:12.640
using text-based knowledge graph and text-based large language models, that there's no external

49:12.640 --> 49:17.280
domain here. It's all the same domain. Why do we need two different embeddings for the knowledge

49:17.280 --> 49:23.200
graph and the input text? That's my question. If you have any ideas, let me know. But think about

49:23.200 --> 49:31.120
this, all right. Okay, then we have this question-answer graph neural network, and this does a

49:31.120 --> 49:35.920
two-way interaction between the language model and knowledge graph. And what we can see here is

49:35.920 --> 49:40.320
that we have a certain question, okay, and some options that to choose from. The large language

49:40.320 --> 49:45.200
model will go in here, and then they express the question and the options as part of a knowledge

49:45.200 --> 49:50.400
graph, okay, and this will go through another knowledge graph encoder, and this knowledge

49:50.480 --> 49:58.240
graph encoder is a graph neural network. Okay, and this basically will, you look at this diagram here,

49:59.760 --> 50:05.200
it will do cross-attention. It's very, very confusing. Look at this language model conditions

50:05.200 --> 50:09.200
knowledge graph. So you can blank out some notes here, you know, you can actually do some attention

50:09.200 --> 50:14.800
on some notes to like block off the path. Yeah, so yeah, that's possible. So you can also use the

50:14.800 --> 50:21.360
knowledge graph to condition the attention in the, in, in, when, when you do the next open

50:21.360 --> 50:27.520
processing for the language model, and then eventually you get your answer. Yeah, so this is

50:27.520 --> 50:32.160
one way we can process the knowledge graph, you can process it using a graph neural network.

50:32.960 --> 50:36.960
And in fact, the earlier one on Ernie, that is similar to graph neural network as well. I mean,

50:36.960 --> 50:41.760
they are using the embedding space. And you know, if you just do some operations on the embedding

50:41.760 --> 50:45.840
space, that is a graph neural network already. So yeah, this is very, very similar to graph neural

50:45.840 --> 50:53.200
networks. And yeah, it shows that back in the first few years, people use these kind of methods to

50:53.200 --> 50:58.960
pass through knowledge graph using graph neural networks to represent the embeddings. Okay,

50:59.920 --> 51:05.040
I don't see why you need to do this. Okay, personally, I don't see why you need to do this.

51:05.040 --> 51:11.520
You can just use text, because the knowledge graph representation is in the same kind of domain as

51:12.000 --> 51:17.040
your last language model representation. They're all text. Yeah, why do we need a separate embedding?

51:17.600 --> 51:26.000
So yeah. Okay, Richard said, I think there will have to be input output embeddings and train

51:26.000 --> 51:33.920
them to address common pattern or memory structures. Sorry, could you explain this comment was in

51:34.000 --> 51:41.600
relation to which part of what I said? So you were saying, you know, it's a decision sort of or

51:42.480 --> 51:48.560
sort of my decision question, right, as well, that there is, how do you

51:50.480 --> 51:54.640
well approach this problem? Why do they have their embeddings separate, right?

51:56.640 --> 52:01.040
At the end, there's a sort of cross attention where they're merging them for an output of this

52:01.040 --> 52:12.240
type or that type would have you. But then this idea comes that the real, so I think of a large

52:12.240 --> 52:16.800
language model where the reason why they have these emergent behaviors is because language is

52:16.800 --> 52:24.560
currently our best mechanism to embody thoughts, ideas, and our most direct implementation of

52:24.560 --> 52:30.800
ideas. Now, particularly once they're broken down, tokenized and so on, you've taken through that

52:31.040 --> 52:36.800
process a few times, consider retention and context, you come up with new ideas. Yes, yes.

52:38.800 --> 52:45.600
And then as you're just saying, there's no particular need for different embedding spaces.

52:46.720 --> 52:53.120
And the only need for them is to bring understanding into a common framework where

52:53.120 --> 52:58.400
the ideas themselves in the latent space are being considered and their context and their

52:58.400 --> 53:10.080
relationships. So how the, this is sort of a, what's the word, this idea of boiling down

53:11.280 --> 53:18.480
the actual form of communication into some representation, any representation where we

53:18.480 --> 53:28.400
can start applying our knowledge to it. Whether you read text or listen to text,

53:29.680 --> 53:36.160
you don't, when I hear things, I don't imagine them written down in front of me. I just hear words,

53:36.160 --> 53:44.320
words become ideas, and we go from there. So in the same way, I see the way that

53:45.040 --> 53:51.360
knowledge is presented as an input-output problem and embeddings really address the input-output

53:51.360 --> 53:59.920
problem. And then after that, there's a memory and consideration process which operates on ideas

53:59.920 --> 54:06.000
which are not linked to input and output. I think you and I agree that there needs to be a latent

54:06.000 --> 54:10.800
space or abstraction space for processing. And I think you also agree that there need not be too

54:10.800 --> 54:14.800
separate embedding space for the knowledge graph in the last image model. If I hear it correctly,

54:14.800 --> 54:18.400
right, you don't, you also don't think that is necessary, right? I think,

54:20.960 --> 54:25.040
yeah, but then the problem becomes, if you don't use the same embedding technique,

54:25.680 --> 54:31.840
how do you present meaning? So for mine, in terms of large language model being in language or not

54:31.840 --> 54:39.280
in language, in words, the question is really, are we making the problem harder for ourselves

54:39.920 --> 54:47.120
by using a difference, by saying, well, it's all words and the words are by and large correct.

54:47.120 --> 54:53.200
Therefore, we'll just use a large language model to read and ingest a large language model. And I

54:53.200 --> 54:58.960
think that will work. But the challenge becomes what you alluded to earlier, where the Chinese

54:58.960 --> 55:05.360
representation versus the English representation gives a different outcome. And I'm trying to

55:05.440 --> 55:14.800
abstract away that behaviour. So the thinking is, the actual thinking happens in, is always in

55:14.800 --> 55:23.360
latent space. And the only job for embeddings is to present in a form where, you know, cognition can

55:23.360 --> 55:33.440
happen. Right? Because, right? And so I would say, I don't particularly care what the encoding

55:33.520 --> 55:40.480
encoder is, decoder is, it can go from text in, picture out, picture in, text out, it doesn't

55:40.480 --> 55:47.520
matter. The important thing is that it's consistent, and we can operate on it in a manner that addresses

55:47.520 --> 55:54.480
the patterns and relationships within. Yeah, well said, well said. I agree with you. So

55:55.200 --> 55:58.720
what matters is how we abstract it to the processing space, which is the latent space,

55:59.360 --> 56:05.040
and how we encode it and decode is just extra details, that basically just needs to be mapped

56:05.040 --> 56:10.320
there, and it should be good enough. Yeah, I think so. And if it comes to its own training

56:10.320 --> 56:16.320
challenge. Correct. So I think in the earlier papers, what I get is like, why do we need a

56:16.320 --> 56:20.480
knowledge graph encoder like that? It's because they use embeddings like trans-e that, you know,

56:20.480 --> 56:26.400
are different from GPD embeddings, like, or BERT embeddings. Again, most of the early papers use

56:26.400 --> 56:34.080
but BERT, bi-directional encoder representations from transformers. So what happens is because

56:34.080 --> 56:38.720
these two are from a different embedding space. So you kind of need to map them to the same

56:38.720 --> 56:43.520
embedding space. That's why you need a knowledge graph encoder and a large-engaged model encoder.

56:45.040 --> 56:50.880
But in the new kind of knowledge graph that is constructed, because this large-engaged model

56:50.880 --> 56:55.680
is now so powerful, you can actually use the embedding space for the large-engaged model to

56:55.680 --> 57:00.960
construct your knowledge graph. And if you do that, okay, if you do that process, which is

57:00.960 --> 57:05.120
part two of the presentation today, you will see later, if you use the large-engaged models to

57:05.120 --> 57:09.200
construct the knowledge graph, actually, you don't need a separate knowledge graph encoder

57:09.200 --> 57:15.200
or encoder here because they are in the same embedding space already. So if you look at this

57:15.200 --> 57:22.880
thing here, you don't need a separate decoder for the JSON here because this is in the same

57:22.880 --> 57:28.800
embedding space as your text. And I would like to posit that it will be better for all of them to

57:28.800 --> 57:35.120
be in the same embedding space because it will be much easier to do the attention. I mean,

57:35.120 --> 57:38.480
it's easier to do attention in the same domain as compared to different domains because, you know,

57:38.480 --> 57:44.160
cross-attention is only one layer right now. You're going to do a very efficient cross-attentioning

57:44.160 --> 57:49.040
multiple layers. But if you just do it in the same domain, the transformer architecture right now,

57:49.040 --> 57:54.800
you actually do the self-attention multiple times, all right? So it might be actually better to

57:54.800 --> 57:59.760
do it in the same domain, okay? And of course, you will save training complications because,

58:00.400 --> 58:04.880
you know, you need to map both to the same latent space and, you know, that is a difficult problem.

58:04.880 --> 58:10.400
It's a very difficult problem to map two different streams of inputs to the same latent space. I

58:10.400 --> 58:15.840
mean, we have seen it, like in OpenAI, they have this thing called clip, okay, that max text and

58:15.840 --> 58:25.120
images to same latent space. You know how many examples they train it on? Millions, I think even

58:25.120 --> 58:30.480
billions. Yeah, so it's a very, very difficult problem to map both to the same latent space.

58:30.480 --> 58:34.400
If I can map it well, of course, you can do like stuff like stable diffusion, you know,

58:35.120 --> 58:40.800
dolly, you can generate images from text. Yeah, but why have this problem with the

58:40.800 --> 58:45.040
knowledge graph when you can actually just ground the knowledge graph in the same embedding space

58:45.040 --> 58:53.440
as your large language model? Okay, so think about that. Okay, now we go to approach two.

58:53.440 --> 58:59.040
Before I move on, I'd like to open the floor for any opportunities to ask anything so far for the

58:59.040 --> 59:08.400
first part. Okay, if not, I'll carry on. So next is how we can use a knowledge large

59:08.480 --> 59:15.280
language model to get the knowledge graph. So one is to use using few short and zero or zero

59:15.280 --> 59:19.520
short prompting, like for example, length chain, okay, I don't think the approach is that great.

59:19.520 --> 59:25.840
Okay, I found a better approach, right, using a better prompt, but we can potentially use

59:25.840 --> 59:30.000
large language models to generate knowledge graphs. The other way is to use the embedding

59:30.000 --> 59:35.440
space of the large language models to enrich the representational space of the knowledge graph.

59:36.000 --> 59:41.760
So this is also quite interesting. Let's see how we can do both. Okay, the first one is,

59:42.480 --> 59:47.520
okay, this is just some idea of how we can use it. Okay, we can, we can few short prompt

59:47.520 --> 59:51.840
to generate the relations, okay, because large language models are just very versatile

59:51.840 --> 59:56.400
and can be context driven to do it. And actually it's way better than, you know,

59:57.280 --> 01:00:02.640
so this is my own experience. I use space scene to do name entity recognition and I use

01:00:02.640 --> 01:00:08.800
large language models to do that. The GPT, chat GPT performs way better than space scene. Space

01:00:08.800 --> 01:00:14.560
scene makes out a lot of the names, all right. So if we use large language models to generate

01:00:14.560 --> 01:00:20.480
the knowledge graph, compared to traditional approaches, like spacey or some other verb,

01:00:22.400 --> 01:00:27.520
VMP, you know, those kind of three parcels for language. Last time people used that to generate

01:00:27.520 --> 01:00:33.600
the knowledge graph to find out what are the nouns, what are the verbs and so on. Okay, so that was

01:00:33.600 --> 01:00:37.280
difficult to generate the knowledge graph because sometimes it miss out certain things.

01:00:37.280 --> 01:00:41.840
But large language models are quite good. Okay, why not just use large language models directly

01:00:41.840 --> 01:00:48.320
to generate the relations and the source and the destination. So indeed, this is what Lang

01:00:48.320 --> 01:00:53.440
Chen did. Okay, so if you look at the graph QA prompt, this is the prompt. Okay, you are the

01:00:53.440 --> 01:00:58.400
network intelligence. Okay, help to integrate stuff into a knowledge graph, extract knowledge

01:00:58.400 --> 01:01:03.200
triples from text. Okay, a knowledge triple is a clause that contains a subject, predicate,

01:01:03.200 --> 01:01:08.240
an object. Okay, subject is entity being described, predicate is the property,

01:01:09.280 --> 01:01:15.520
object is the value of the property. Okay, there's a typo here. Okay, so this is the

01:01:17.120 --> 01:01:21.680
zero shot prompting for Lang Chen. All right, this is not that good yet. So you need to give

01:01:21.680 --> 01:01:25.920
some few short examples and they gave some few short examples in the prompt, like for example,

01:01:26.720 --> 01:01:33.120
like this is the input, and then you can say that oh, Nevada is a state, Nevada is the U.S.,

01:01:33.120 --> 01:01:38.640
Nevada is number one go producer in Go. So I don't like this example. Okay, because for one,

01:01:38.640 --> 01:01:44.560
they did not say the state at all in the prompt. Like then you want the model to just plug the

01:01:45.280 --> 01:01:52.080
plug the noun from thin air. So yeah, like here, I'm going to the store output none. Why, why is

01:01:52.080 --> 01:02:01.840
the output none? It should be I went to store something like that. Yeah, so you should be able

01:02:01.840 --> 01:02:07.680
to extract something from this. So I disagree with the examples that the Lang Chen one provided.

01:02:07.680 --> 01:02:13.200
Okay, so I think if they improve this example, maybe theirs would work better. So let's take a

01:02:13.200 --> 01:02:19.120
look at what I did later. So I'm not a fan of Lang Chen, by the way. Lang Chen prompts are very

01:02:19.120 --> 01:02:28.000
worthy. So this is the other way that we can use the large language models to do the text encoding,

01:02:28.000 --> 01:02:33.280
to do knowledge graph embeddings. So this is called KGE, knowledge graph embeddings,

01:02:33.280 --> 01:02:37.120
is something like, you know, if you talk about the stuff like trans, these are like embeddings

01:02:37.120 --> 01:02:42.720
that we can give to the source, to the destination, to the relation. So we can represent the knowledge

01:02:42.720 --> 01:02:49.840
graph as embeddings. And we can use GPT or some large language model, okay, to generate some

01:02:49.840 --> 01:02:55.760
embedding space here that you can then use like MLP, multi layer perceptron, and so on to map to

01:02:56.400 --> 01:03:01.840
to the embedding space of the knowledge graph embeddings. So this is one way we can utilize

01:03:01.840 --> 01:03:07.600
large language models to do it. Yeah, I mean, I was thinking, you know, like, why not just use this,

01:03:07.680 --> 01:03:13.920
right? Why not just use LAM embeddings directly for knowledge graph?

01:03:16.080 --> 01:03:21.120
I mean, LAMs are way better than, than doing graph neural networks, in the sense that, you know,

01:03:21.120 --> 01:03:24.720
if you know the problems with graph neural networks, I'm just going to tell you the problems of graph

01:03:24.720 --> 01:03:30.960
neural networks now. Okay, they have these two problems. Okay, this is one is called over squishing

01:03:30.960 --> 01:03:36.160
or over squashing. And the other one is called over smoothing. Okay, what are these two problems?

01:03:36.160 --> 01:03:42.160
Over squashing is that the information, because you pass the information into an embedding layer,

01:03:42.160 --> 01:03:49.520
information gets lost at embeddings. Okay, so this over squashing thing is also a problem for

01:03:50.960 --> 01:03:56.080
LAMs. So I'm not going to cover too much on it. The other problem that we have for this kind of

01:03:56.080 --> 01:04:01.360
graph neural network over smoothing, okay, is that after you do message passing

01:04:02.000 --> 01:04:10.560
or too many times, all embeddings look the same. Okay, so this is a big problem. Okay, I also

01:04:10.560 --> 01:04:14.480
realized this, that once I did graph neural networks, you, like, you have two nodes, you pass

01:04:14.480 --> 01:04:18.720
information to each other, and then you become the average of the information, you keep doing this,

01:04:18.720 --> 01:04:24.160
right? Eventually, both nodes become the same, or very, very similar. Okay, so this is one of the

01:04:24.160 --> 01:04:30.960
huge problems of graph neural networks. And I feel like the embedding space that is best

01:04:30.960 --> 01:04:35.680
done, right, is not the way that we do message passing in graph neural network. We should just

01:04:35.680 --> 01:04:40.720
ground it in the context using a large language model. And large language model update the context

01:04:40.720 --> 01:04:46.080
quite well. Okay, then you can just use the embedding that is derived from that particular context

01:04:46.080 --> 01:04:50.960
in, like, you can just put something like that, like, you can just say context. And then like,

01:04:51.920 --> 01:04:57.520
I am a student or something like that. So, like, this context will update the definition

01:04:57.520 --> 01:05:02.880
of the student here. So you can go through the transformer module. So this is the transformer

01:05:02.880 --> 01:05:11.040
module. And then you can get the final embedding here. Yeah, at the final layer, right, before

01:05:11.040 --> 01:05:16.720
the softmax, you can actually use the transformer to get the embeddings already. Why use knowledge

01:05:16.800 --> 01:05:21.680
graph embeddings? Okay, so I'm just putting this question out here. So I hope those people

01:05:21.680 --> 01:05:26.320
knowledgeable in this area can come and, you know, correct me if I'm wrong. But I don't see a point

01:05:26.320 --> 01:05:33.840
in doing this. Yeah, right now. Okay, so let's leave it as that. And let's continue.

01:05:36.080 --> 01:05:41.040
Approach tree. So the approach tree is how we combine both approaches to make a very, very

01:05:41.920 --> 01:05:46.240
synergistic model where the large language model can generate the knowledge graph dynamically

01:05:46.720 --> 01:05:50.640
and this knowledge graph is something like a dynamic memory that gets updated as the agent

01:05:50.640 --> 01:05:55.600
explores the world and so on. And this knowledge graph can then inform the knowledge, the large

01:05:55.600 --> 01:06:02.240
language model and ground it in consistent generation. So let's see how this works. So you

01:06:02.240 --> 01:06:08.160
can see this is the diagram in the paper. And you can see like data. Okay, that's what me and Richard

01:06:08.160 --> 01:06:16.400
discussed. Data, okay, will be from different domains to embed them into latent space. Okay, so

01:06:16.400 --> 01:06:21.520
now we just assume that there's only one latent space but my view is that there's multiple latent

01:06:21.520 --> 01:06:28.480
spaces. Okay, so right now we just treat it as there's only one latent space. You process the

01:06:28.480 --> 01:06:32.960
information in that one latent space using knowledge graph and large language model in this

01:06:32.960 --> 01:06:38.960
loop. Okay, so knowledge graph can ground the language model in consistency. Language model

01:06:38.960 --> 01:06:44.400
can make the knowledge graph more expressive. Okay, and not as rigid as before. I mean, maybe you

01:06:44.400 --> 01:06:49.680
can use embedding based knowledge graphs like then you can make the knowledge graphs like express a

01:06:49.680 --> 01:06:55.360
lot of things more than just text alone. Okay, so this is one idea. You can use different techniques

01:06:55.360 --> 01:07:00.560
to process it like graph theory networks from engineering, representational learning. Yeah,

01:07:00.560 --> 01:07:06.560
I mean, this is just some big words, but the idea is you basically do some processing. All right,

01:07:06.560 --> 01:07:10.880
you can use large language models to process. Or if you like it, you can process it using the

01:07:10.880 --> 01:07:16.240
knowledge graph, which is like a graph neural network to process. Okay, or you can make the

01:07:16.240 --> 01:07:19.440
graph neural network into text and then you can do some neural symbolic reasoning.

01:07:20.240 --> 01:07:23.760
Okay, actually, this whole thing can be just summarized as neural symbolic reasoning because

01:07:26.000 --> 01:07:31.920
plus the knowledge graph equals the symbol spot. And then the large language model is the neural

01:07:31.920 --> 01:07:36.800
networks. So you can just summarize this whole thing as neural symbolic reasoning. All right,

01:07:36.880 --> 01:07:43.200
then you can use this for different domains. Right, I think this is a very, very exciting path

01:07:43.200 --> 01:07:48.960
that we should work on. Because right now with the power of language models, the knowledge graph

01:07:48.960 --> 01:07:54.560
can be very, very flexible. And it's not a typical knowledge graph anymore, it can be

01:07:54.560 --> 01:08:00.000
embedding based knowledge graph. And it can be context dependent knowledge graph. Okay, so

01:08:00.720 --> 01:08:04.720
I really hope to work on context dependent knowledge graph, because I think that's the future.

01:08:04.720 --> 01:08:08.640
Okay, not the traditional knowledge graph that you've seen everywhere in this presentation.

01:08:09.280 --> 01:08:13.200
Okay, the knowledge graph embeddings must be able to do, must be able to

01:08:14.160 --> 01:08:18.320
change based on the parent nodes. Okay, must be changed based on the context. And that's

01:08:18.320 --> 01:08:21.920
something that is not done right now, at least based on my own awareness. I don't think that's

01:08:21.920 --> 01:08:28.240
done right now. But that's very promising. Right, so one use case for this kind of system is fact

01:08:28.240 --> 01:08:33.760
checking. As you know, large language models cannot do very badly at fact checking. It tends to

01:08:33.760 --> 01:08:40.160
hallucinate a lot. And perhaps we can do like, a knowledge graph to like ground it in some facts

01:08:40.160 --> 01:08:48.240
like some Wikipedia entries. No, you can use this to ground the inference. Okay, by doing inference,

01:08:48.240 --> 01:08:53.360
you can then see whether or not like, is it, is there a path in the knowledge graph that matches

01:08:53.360 --> 01:08:59.760
it? Or you do knowledge grounded inference, like you say, you must only use this information

01:08:59.760 --> 01:09:04.880
that I extract for you in the knowledge graph and infer it. So this diagram here, unfortunately,

01:09:04.880 --> 01:09:09.920
did not do the inference step. Okay, because they are still using birds. Okay, they're using bird as

01:09:09.920 --> 01:09:17.840
a model. And what they did was they use the knowledge graph relations to do some pre training.

01:09:17.840 --> 01:09:24.080
So it's like they take additional, like, additional tech samples, they just mask out certain words

01:09:24.080 --> 01:09:28.880
based on the knowledge graph relations. And then they do the training here. So they just did the

01:09:28.880 --> 01:09:35.360
pre training using the knowledge graph to give additional examples. Okay, so what I want the

01:09:35.360 --> 01:09:41.280
thing to do is actually to do it during inference, if I cannot find any paper that does that so far.

01:09:41.280 --> 01:09:45.440
All right. So I think this inference is more important than the pre training, you know,

01:09:45.440 --> 01:09:49.360
this pre training, yes, it increases more data samples, because you can just mix and match the

01:09:49.360 --> 01:09:54.240
knowledge graph, get more sentences out. Sure, I give it to you. And in fact, they improve by two

01:09:54.240 --> 01:09:59.040
to three percentage points across soda benchmarks, this fact KB, you can go and check it out.

01:09:59.680 --> 01:10:03.920
All right. But what I'm more interested in is how you use it for inference, not for pre training.

01:10:04.560 --> 01:10:09.680
Okay, so let's see how length chain does it. All right, so now we come to the length chain part.

01:10:09.680 --> 01:10:13.360
So actually, length chain is quite advanced, because length chain has a lot of the ideas that I

01:10:13.360 --> 01:10:18.400
think should be done. All right, let's see how the length chain question answering graph question

01:10:18.400 --> 01:10:23.520
answering is done. All right, so we have four steps. First step, we generate the triples from

01:10:23.520 --> 01:10:27.760
the context. Okay, so we are like maybe a text context, you generate like the triples from it,

01:10:28.400 --> 01:10:33.760
like the knowledge graph triples, you generate some from the query, you generate some entity

01:10:33.760 --> 01:10:39.200
extraction. Okay, and then you use this at that entities to extract this relevant triples. Okay,

01:10:39.200 --> 01:10:44.400
later I'll show you the how what I mean by this. And then you use this relevant triples to answer

01:10:44.400 --> 01:10:48.640
the question. So I share with you these two documentation in case you want to see how length

01:10:48.640 --> 01:10:57.360
chain graph QA does it. So step one, okay, generate triples from context. So like this context, I just

01:10:57.360 --> 01:11:03.760
came out of it, right? Recently, my MacBook external camera in the viewing camera spot. So I'm

01:11:03.760 --> 01:11:09.760
actually using the external camera right now to talk to you. And yeah, so this example is for Apple.

01:11:09.760 --> 01:11:14.960
So let's assume that Apple created a new product called Mac and Cheese Pro, okay, in 2025. All

01:11:14.960 --> 01:11:19.600
right, and then like Apple gave the invented cheese, okay, a rousing ovation in 2026 after

01:11:19.600 --> 01:11:24.160
invented this in 2024. Right, there's also another company called Orange who created a competing

01:11:24.160 --> 01:11:28.800
product called the Orange and Cheese Pro. The price was slightly higher at 5000 compared to 4000

01:11:28.800 --> 01:11:34.880
from Apple. Okay, so this is a fictional example. Okay, and this is just to see like how good the

01:11:34.960 --> 01:11:39.680
context is stored in the knowledge graph. So you can see that, oh yes, Apple announced Mac and Cheese

01:11:39.680 --> 01:11:47.280
Pro, Apple gave cheese. So this kind of thing, right, like, is a bit contentious because like,

01:11:47.280 --> 01:11:51.520
what do you mean by gave cheese, gave what? So this one needs to be improved a bit.

01:11:52.400 --> 01:11:57.760
Apple ovation gave, Apple gave an ovation, okay, again to who? Right, so this one needs to be

01:11:57.760 --> 01:12:03.040
improved as well. Okay, the price of the MacBook Pro is 4000. Yes, Mac and Cheese Pro is already

01:12:03.120 --> 01:12:09.120
created. Orange and Cheese Pro, good. Orange and Cheese Pro, the price 5000. Okay, so you see,

01:12:09.120 --> 01:12:16.000
it's not bad. Miss out dates. All right, and then like some, some relations are ambiguous.

01:12:16.880 --> 01:12:21.280
So I don't quite like the way they did the triplet extraction and I think this is the

01:12:21.280 --> 01:12:25.920
downfall of the Graph QA. So if you are going to use Lang chain for Graph QA, my advice is don't

01:12:25.920 --> 01:12:30.800
use it. Okay, because you miss out a lot of stuff in the context. Okay, if you are interested

01:12:30.800 --> 01:12:34.960
how they generate the context, you can go back to my earlier slides that I was talking about.

01:12:35.680 --> 01:12:40.240
Yeah, so I mean, it's actually, let me show you, let me show you again the slides.

01:12:41.280 --> 01:12:46.160
It's this one, this is the one that they did, like, this is the problem to generate stuff from the

01:12:46.160 --> 01:12:52.640
text. Yeah, so the examples aren't very great and understandably the results aren't very great

01:12:52.640 --> 01:12:57.440
as well. All right, so this is the knowledge graph that's generated. You can see like Mac and Cheese

01:12:57.440 --> 01:13:04.400
Pro is cost $4,000 on price. Yeah, you can see that like stuff like price will contain like a lot

01:13:04.400 --> 01:13:11.760
of relation because like price is very generic. Okay, Apple announced Mac and Cheese Pro. Okay,

01:13:11.760 --> 01:13:18.240
so this is the knowledge graph that is generated. And we can see that like, next up we can use the

01:13:18.240 --> 01:13:24.160
Graph QA chain in order to run the chain and see the answer. And you can see that if I ask it the

01:13:24.160 --> 01:13:29.040
question like, when was the Mac and Cheese Pro announced? Okay, they couldn't find it. Okay,

01:13:29.040 --> 01:13:33.840
because after they passed through the context, okay, they abstract like when was the Mac and

01:13:33.840 --> 01:13:37.600
Cheese Pro, when did Apple announce the Mac and Cheese Pro? They abstract that in the query,

01:13:37.600 --> 01:13:42.400
there's only Apple and Mac and Cheese Pro. So they check through all the knowledge graph to make sure

01:13:42.400 --> 01:13:48.080
that you only have entities that match Apple or Mac and Cheese Pro. Okay, so I have a gripe with

01:13:48.080 --> 01:13:59.280
this thing. Like if you use exact text matching, what if there's a spelling error, capitalization

01:13:59.280 --> 01:14:09.360
error, or like related word, but not exact match. Yeah, so if you use exact text matching, which

01:14:09.360 --> 01:14:14.880
is what they did for a length chain, like what if you don't get the right match? Okay, so I don't

01:14:14.880 --> 01:14:21.040
quite like this approach. So yeah, this is something that I think could be improved on.

01:14:21.040 --> 01:14:25.840
All right, and you can see that if I ask it like, when did they announce the Mac and Cheese Pro?

01:14:26.640 --> 01:14:30.960
They couldn't answer. All right, because look at this knowledge graph here, there's nothing that

01:14:30.960 --> 01:14:35.200
talks about dates here. All right, so they miss out quite a huge chunk of information from the

01:14:35.200 --> 01:14:41.440
earlier context. So if we had fed in the earlier context directly, so I just use the length chain

01:14:42.400 --> 01:14:49.600
LmChain agent. Okay, so I'm only using the LmChain agent or this to just show you that length

01:14:49.600 --> 01:14:55.280
chain is not good. All right, I myself the new length chain. All right, so this is the idea

01:14:55.280 --> 01:15:00.720
that like, after a while, you know, this is the context and then, okay, so this is not bad. I

01:15:00.720 --> 01:15:04.000
mean, you could just do the same thing on ChatGBT, actually, you can just put like context

01:15:04.000 --> 01:15:09.280
question and then ChatGBT will give you the answer. All right, so this LmChain works and this shows

01:15:09.360 --> 01:15:14.720
that by embedding the text as a knowledge graph, it kind of miss out certain stuff. All right,

01:15:14.720 --> 01:15:19.840
and what are the stuff we miss out? We miss out the years and we also miss out like,

01:15:19.840 --> 01:15:23.760
Apple gave cheese. I mean, it doesn't make sense that way, right? I mean, look at the knowledge

01:15:23.760 --> 01:15:31.520
graph like, what am I trying to solve there? Apple gave cheese. Where is it? Apple gave cheese.

01:15:31.520 --> 01:15:35.760
Where's cheese?

01:15:39.280 --> 01:15:41.760
Apple Cheese gave. Is there a gave anywhere?

01:15:45.600 --> 01:15:50.640
Yeah, okay, I think this one maybe is the outdated, but the idea is that we can't really tell the

01:15:51.360 --> 01:15:58.000
main thing in this graph because we miss out some information. And that's one of the issues of

01:15:58.560 --> 01:16:02.560
converting text directly into knowledge graph is that you might miss out certain relations.

01:16:03.200 --> 01:16:06.400
And actually, if you think about it, if we want to embody all relations,

01:16:07.200 --> 01:16:12.960
there's just too many to embody, right? Yeah, it's too big to embody. So maybe the text itself

01:16:12.960 --> 01:16:17.760
is way more expressive than the knowledge graph, if you think about it that way. Okay,

01:16:18.560 --> 01:16:22.400
but again, you know, if you just use text only, you might face issues that, you know,

01:16:22.400 --> 01:16:30.720
your OpenAI embeddings might be too restrictive. It's too broad-based. You need the embeddings

01:16:30.720 --> 01:16:38.080
at different levels. So let's see how we would improve the Lang chain graph QA. I'm just using

01:16:38.080 --> 01:16:43.600
my strict JSON framework here, which just basically passes the system prompt and then outputs as a

01:16:43.600 --> 01:16:51.360
JSON in your own way. So I basically did what Lang chain does in a much shorter way. So I just

01:16:51.360 --> 01:16:55.440
say you are a knowledge graph builder. You extracted an object one, object two relation.

01:16:55.440 --> 01:17:00.080
Okay, I did not even put subject, object predicate. Okay, I mean, I just do like that. Okay, I just

01:17:00.080 --> 01:17:05.440
want it to be as vague and as generic as possible because I want to capture as much information

01:17:05.440 --> 01:17:10.080
as possible. Okay, so this was done in like 10 minutes. Okay, I don't really know whether this

01:17:10.080 --> 01:17:14.960
is the best. You all can feel free to improve it. Okay, I have the Jupyter notebook attached in the

01:17:14.960 --> 01:17:20.560
link. All right, so I gave you some examples like John bought the laptop. Okay, that's me,

01:17:20.560 --> 01:17:25.680
all right. John built the house in 2021. Okay, that's not me, all right. But this is the idea of

01:17:25.680 --> 01:17:30.720
like how we can represent like various relations like that. All right, then the output format is

01:17:30.720 --> 01:17:34.080
just a knowledge graph. So you can see like Apple announced my and cheese bro, my and cheese bro

01:17:34.080 --> 01:17:39.600
announced in 2025. Apple proved big hit. Okay, so again, this one is not exactly that great,

01:17:39.600 --> 01:17:43.520
because it's not really Apple that prove a big hit. It should be the Mac and cheese bro that

01:17:43.520 --> 01:17:48.160
prove a big hit. So this part needs to be from engineer a bit more. All right, Apple gave cheese.

01:17:48.160 --> 01:17:54.800
Okay, again, like this is not complete. Okay, cheese browsing ovation into zero to six. So

01:17:54.800 --> 01:17:59.120
actually we combine these two together. This is complete. So this is okay. All right, cheese

01:17:59.120 --> 01:18:03.120
invented man cheese bro. Okay, man cheese bro invented into zero to four. Okay,

01:18:03.840 --> 01:18:08.640
orange created orange and cheese bro. Yep, orange and cheese bro. The price is 5000 and

01:18:08.720 --> 01:18:14.800
Apple prices 4000. So again, here has some issues or so, like here, instead of saying that this is

01:18:14.800 --> 01:18:18.720
a Mac and cheese bro, because we should be referring to man cheese bro, it says Apple.

01:18:19.440 --> 01:18:24.960
Okay, so unless we can sort of like link this later to Apple announced, okay, this part here.

01:18:26.720 --> 01:18:31.840
So now you can see some issues with knowledge graph expressing stuff. It is not clean. All right,

01:18:32.640 --> 01:18:38.960
it might truncate the information halfway. So this one needs more study as to how we can

01:18:38.960 --> 01:18:43.760
express this in the knowledge graph better. But by expressing it in the knowledge graph,

01:18:43.760 --> 01:18:48.960
you are able to then do knowledge graph passing, okay, and extract out the relevant entities that

01:18:50.320 --> 01:18:54.880
related to the prompt. And you know, this is like, if you think about it, this is like

01:18:54.880 --> 01:19:00.480
doing segmentation across like every few words in the segment one time. Yeah, so this is the

01:19:00.480 --> 01:19:06.000
generated graph of what I did for strict design framework. You can see that compared to Lang chain,

01:19:06.000 --> 01:19:14.960
this is what happens like we have way more relations, there's more relations here. And dates are captured.

01:19:17.200 --> 01:19:22.160
Yeah, so this is something that I think needs to be investigated more. Mine is not the best,

01:19:22.160 --> 01:19:26.800
but Lang chain is definitely not good. Okay, so this is something that needs to be done more if

01:19:26.880 --> 01:19:32.240
we want to extract stuff out into the knowledge graph. And then like, should we use embeddings?

01:19:33.840 --> 01:19:41.280
So if you want to use embeddings, then we cannot just use OpenAI API. Maybe you need to use like

01:19:41.280 --> 01:19:48.400
Lama2. Okay, although Lama2 perhaps is not that great or so, because Lama2 is

01:19:50.000 --> 01:19:55.440
not that good for multilingual. Okay, but Lama2 is the best possible substitute for

01:19:56.240 --> 01:20:00.800
chat GPD right now. So maybe you can construct a knowledge graph embeddings using the Lama2 embeddings.

01:20:01.840 --> 01:20:08.960
So food for thought. Next, we have this flexible knowledge graph passing. Over here, what I decided

01:20:08.960 --> 01:20:17.440
to do is that we want to output only relations that are relevant to the question. And I just

01:20:17.440 --> 01:20:23.040
passed in the entire knowledge graph here. So instead of coming up entities, I just asked it

01:20:23.040 --> 01:20:29.360
to go through the entire knowledge graph because in case of words not exact or spelling errors,

01:20:31.120 --> 01:20:40.000
GPD is able to catch it most of the time. I must copy it because GPD is not as great as

01:20:40.000 --> 01:20:44.720
doing like counting letters and stuff. But if you misspell your words, but the meaning is about

01:20:44.720 --> 01:20:50.000
that, GPD is able to extract the right entities. And here we can see that we asked it like,

01:20:50.000 --> 01:20:54.880
when did Apple announce the man cheese bro? It captured exactly what we want. All right.

01:20:56.000 --> 01:21:02.080
And this is the graph that is the past knowledge graph. So I'm talking about when you query the

01:21:02.080 --> 01:21:05.360
knowledge graph, you pass it so that only relevant sections of the knowledge graph gets

01:21:05.360 --> 01:21:11.840
come out, gets extracted. You ground this extracted part onto your text. Okay. And then

01:21:12.640 --> 01:21:18.960
you can get the answer here. So 2025. So I just shown that like using this strict JSON format,

01:21:18.960 --> 01:21:23.600
you are able to like, it's very flexible. You just need to key in the system prompt,

01:21:23.600 --> 01:21:27.440
key in the user problem and output the format in terms of whatever JSON labels and the

01:21:27.440 --> 01:21:32.800
description of the JSON. So I've been using this for a lot of my own use cases. And I'm just

01:21:32.800 --> 01:21:37.680
adapting this for the knowledge graph. But this is really cool because you can then use this

01:21:38.240 --> 01:21:42.560
past knowledge graph, like this idea of generating the knowledge graph and passing the knowledge

01:21:42.560 --> 01:21:50.560
graph. You can use the knowledge graph as memory. Okay. And then you can update memory.

01:21:51.120 --> 01:21:58.240
And you can use updated memory to extract relevant parts. Okay. So this called retrieval.

01:21:59.040 --> 01:22:05.520
Okay. Use relevant parts to solve problem. So I really like this framework because this

01:22:05.520 --> 01:22:11.840
knowledge graph as memory thing is something quite interesting. But how can we express it as memory?

01:22:11.840 --> 01:22:19.360
That's the difficult part. Okay. So Richard asked, have I tried putting the graph into an FAIS

01:22:19.360 --> 01:22:27.040
index? No, I haven't. But how will you do a knowledge graph putting onto the, like onto that

01:22:27.040 --> 01:22:32.480
index? Because usually what I know is that you do the embedding and then you put the text. That's

01:22:32.480 --> 01:22:38.480
for the retrieval or method generation. If you're doing knowledge graph, maybe you put the source

01:22:38.480 --> 01:22:43.760
as the index. Okay. I'm not too sure. I'm going to check on this. Like how will you do this into

01:22:43.760 --> 01:22:48.800
PyCon and stuff like that? But what I can imagine you doing for the knowledge graph is just put the

01:22:48.800 --> 01:22:54.960
whole thing into some array and then just store the array. I mean, you can even put it as a JSON.

01:22:54.960 --> 01:23:05.120
Yeah. So yeah. Okay. I don't have time to cover through the running of the Jupyter Notebook.

01:23:05.120 --> 01:23:10.400
I'll just upload that separately. It's another video. But let's just go through like the last

01:23:10.400 --> 01:23:14.400
five to 10 minutes. I'm okay to extend about 15 minutes if you all have more things to discuss.

01:23:15.120 --> 01:23:20.080
Like we have discussed like how can we use knowledge graph better for last language models?

01:23:20.080 --> 01:23:24.880
So first question, what are the failure modes of using knowledge graph for context representation?

01:23:25.840 --> 01:23:32.960
And I think this failure mode is mainly like your knowledge graph may not capture all information.

01:23:35.200 --> 01:23:37.680
Okay. And also the knowledge graph capturing

01:23:40.320 --> 01:23:45.920
might truncate the information. So maybe using text directly

01:23:48.000 --> 01:23:54.560
may be better. Okay. But harder to pass. Because if you are using text directly, you don't really have

01:23:56.400 --> 01:24:02.160
like nice sections where you can pass the knowledge graph on. Yeah. So these are some of the things

01:24:02.240 --> 01:24:05.440
about right now, some of the failure modes of this knowledge graph.

01:24:06.560 --> 01:24:07.840
Anyone else has anything to add?

01:24:09.440 --> 01:24:18.480
Just some random thoughts. Do you think it makes sense if we view the embedding space

01:24:18.480 --> 01:24:21.360
itself as a form of generalized knowledge graph?

01:24:23.680 --> 01:24:27.440
Embedding space as a genera. You mean the LM embedding space?

01:24:27.440 --> 01:24:35.520
Oh, I mean, I mean, yes, but for some specific tasks you want to do, you can train a dedicated,

01:24:36.080 --> 01:24:46.800
a separate dedicated embedding space. So because like you have all your entities inside their space

01:24:47.520 --> 01:24:52.960
and the relative, I don't know, relative positioning of them kind of encode certain

01:24:53.200 --> 01:24:58.000
relative information of them, right? Because I think the issue you mentioned here is,

01:24:58.960 --> 01:25:06.640
I think it's just the graph, learn graph can be too sparse, right? You lose a lot of information.

01:25:07.600 --> 01:25:14.800
But if inside embedding space, I don't know, it might help preserve more information,

01:25:14.800 --> 01:25:19.040
although not very explicit information. It's just some random thought.

01:25:19.920 --> 01:25:24.640
Ah, okay, I get what you mean. Like you encode knowledge graph as embedding space,

01:25:25.360 --> 01:25:32.720
so like your source relation and the output are all embedding space and sink in destination.

01:25:34.640 --> 01:25:42.240
I feel like my gut feeling is it can be much richer than just a typical traditional graph.

01:25:43.120 --> 01:25:45.120
Definitely.

01:25:47.520 --> 01:25:51.280
Now I kind of agree with you. It's just going to be hard to express the embedding space

01:25:51.920 --> 01:25:57.040
using an OpenAI API. You might need to have access to the last average model directly if you want to do this.

01:25:59.600 --> 01:26:02.880
Yeah, but definitely that's one of the ways that we can represent knowledge graphs.

01:26:04.880 --> 01:26:10.560
Anyway, this is the second question also. Should we utilize the embedding space? Perhaps

01:26:12.320 --> 01:26:18.880
for more expressive knowledge graph. Okay, but then if you think about like what I was talking about earlier,

01:26:19.760 --> 01:26:20.640
context dependent

01:26:23.920 --> 01:26:27.680
embeddings. If you are talking about context dependent embeddings, actually we can use

01:26:27.680 --> 01:26:35.840
LLM to pass an update embeddings based on the parents of the node.

01:26:36.720 --> 01:26:41.280
Yeah, so I was thinking of something like that. Like you can actually have a very, very

01:26:43.440 --> 01:26:48.400
different interpretation of a certain word. Like for example, bank can be river bank or financial

01:26:48.400 --> 01:26:53.280
bank, depending on like the context of it. If you are talking about river side, then it's like river bank.

01:26:53.280 --> 01:26:58.160
Yeah, so you can actually use the last language model, extract out the hierarchy of the graph,

01:26:58.160 --> 01:27:03.680
the front part of the graph. You can put it there and you can then pass the embeddings accordingly.

01:27:04.560 --> 01:27:08.800
So I'm still thinking that perhaps just using the last language model embedding directly

01:27:10.160 --> 01:27:13.680
might be a better bet. And then you can just maybe

01:27:15.520 --> 01:27:20.880
use last language model to do this context thing. And then you can put this embeddings inside

01:27:20.880 --> 01:27:26.320
your knowledge graph. Like what you said earlier. If there's a way to get the embedding space

01:27:26.320 --> 01:27:30.800
directly from the OpenAI API, that would be great. But if not, we might have to use

01:27:30.880 --> 01:27:33.680
Lama2 in order to do this embedding space knowledge graph.

01:27:36.320 --> 01:27:39.600
But then again, is it really necessary? Can we just use text?

01:27:39.600 --> 01:27:44.240
So this is a big open question. Should we use embedding space for the knowledge graph?

01:27:44.240 --> 01:27:50.080
Or can we just represent it as text and then use the LM to generate embeddings after that?

01:27:50.880 --> 01:27:56.480
So I leave that as an open question. I think both approaches are valid approaches. I just feel like

01:27:56.480 --> 01:28:01.120
the way to input the knowledge graph as text is it will be much more interpretable. And also

01:28:01.120 --> 01:28:06.320
you only need to train one embedding space, which is the LM embedding space. So I kind of prefer that.

01:28:10.240 --> 01:28:12.080
Anyone else has any things to add?

01:28:16.400 --> 01:28:20.640
Okay, if not, we go to the next question. Can LMS help with a more flexible

01:28:20.640 --> 01:28:25.760
interpretation or construction of a knowledge graph? Okay, so our answer first. I think yes,

01:28:25.760 --> 01:28:35.440
definitely. Just like compared to like spacey or like on noun, verb, pro verb, those kind of

01:28:35.440 --> 01:28:44.480
stuff, like you are doing like the parse tree, compared to those very, very flexible. And you

01:28:44.480 --> 01:28:49.040
are able to extract a lot more information. So like just based on the string, some prompt I

01:28:49.040 --> 01:28:54.560
showed you earlier, you just hit object relation object, it captures almost everything. And that's

01:28:54.560 --> 01:28:59.360
zero shot prompting. Granted, it did not capture the date at first. I had to use the examples to

01:28:59.360 --> 01:29:05.520
give you the date. But compared to using this kind of like spacey and so on, like deep learning

01:29:05.520 --> 01:29:11.440
approaches, like you'll take quite long to train a new kind of like knowledge graph constructor.

01:29:11.440 --> 01:29:15.120
But with large language model, you can just use prompt engineering and get your knowledge graph

01:29:15.120 --> 01:29:22.400
out. I think that's very exciting. Okay, last question. How do we know what nodes are important

01:29:22.400 --> 01:29:26.720
to construct in the knowledge graph? Okay, because there's a lot of information, but not

01:29:26.720 --> 01:29:34.480
everything is needed for your use case. How do we know? Okay, so my opinion, okay, my opinion is this.

01:29:36.000 --> 01:29:43.520
You need to have biases based on the domain. And what are these biases? Maybe you can have

01:29:43.520 --> 01:29:51.360
multiple biases. Okay, and then let's just choose the right biases later. So this is my idea of

01:29:51.360 --> 01:29:56.880
intelligence right now. Okay, I'll share with you. Okay, this idea of intelligence is that there's

01:29:56.880 --> 01:30:02.160
not just one abstraction space where you store your information, you store them in multiple

01:30:02.160 --> 01:30:08.800
abstraction space. How do we get all these abstraction spaces? We basically just do rule

01:30:08.800 --> 01:30:13.440
based abstraction, like you like maybe one domain is saying that oh, dates are important. So I store

01:30:13.440 --> 01:30:18.080
the dates. Another domain is like all people, all person's names are important. I store the person's

01:30:18.080 --> 01:30:22.880
names. Then maybe another domain will be like all places are important. I store the places.

01:30:23.680 --> 01:30:28.720
Then when you want to solve the problem, okay, you will see which space is the best

01:30:28.720 --> 01:30:32.880
for your problem. Okay, you will just like, maybe you look at all the abstraction spaces,

01:30:32.880 --> 01:30:37.360
maybe combine two or more, or you just take one, and then whatever solves the problem works.

01:30:38.000 --> 01:30:45.920
So this would form an approach that will be used later on. So if you think about it,

01:30:45.920 --> 01:30:50.400
I'm just going to draw it here. Okay, I don't know whether I have space to draw it, but if you

01:30:50.400 --> 01:30:54.800
look at the top right of the screen, okay, you have a problem, you have multiple abstraction

01:30:54.800 --> 01:30:59.200
spaces, let's call this A. You have another abstraction space here, let's call it B.

01:31:00.720 --> 01:31:05.680
And we have another expression space, we call it C. All right, so if we have three

01:31:05.680 --> 01:31:11.200
abstraction spaces like that, okay, these are like three ways of doing it, three ways of doing the problem.

01:31:11.520 --> 01:31:20.000
And then in order to solve any arbitrary problem later, you just take mix and match the abstraction

01:31:20.000 --> 01:31:26.480
spaces to solve the problem. So yeah, increasingly, I've been feeling like this is the way to do things.

01:31:27.040 --> 01:31:39.920
So yeah, I also use this for my abstraction reasoning copless paper. So this is the idea

01:31:39.920 --> 01:31:43.600
that I have right now. You have different abstraction spaces. All these are rule-based.

01:31:44.320 --> 01:31:48.400
Okay, we don't really have deep learning here, because if you have deep learning, you'll have

01:31:48.400 --> 01:31:53.680
problems in getting a fixed abstraction space. You don't want the abstraction space to change.

01:31:54.240 --> 01:31:57.600
Because if you change this abstraction space, you have to change whatever you learn on it.

01:31:58.160 --> 01:32:02.400
It's like if I suddenly told you that math, the addition is now subtraction,

01:32:02.400 --> 01:32:06.480
that I have to relearn all my math again, because I need to update that new knowledge.

01:32:06.480 --> 01:32:10.800
So I'm saying that the basis is fixed, but then you just choose the right basis to solve it.

01:32:11.440 --> 01:32:16.080
Okay, then you might ask me, if we do it like that, what if we don't have the right basis to

01:32:16.080 --> 01:32:21.440
solve the problem? Okay, then the answer is you can't solve the problem. Okay, which might sound

01:32:21.440 --> 01:32:26.880
a bit crude to people, but I feel like we can't solve everything. Like even humans, we have our

01:32:26.880 --> 01:32:33.280
limitations. It's just that we work around our limitations and try to use our existing biases

01:32:33.360 --> 01:32:37.120
to solve new problems. And I think that's intelligence. We don't really need to change

01:32:37.120 --> 01:32:41.360
the abstraction spaces. We can just work with getting multiple abstraction spaces and then

01:32:41.360 --> 01:32:47.440
just combining them. So I shared a bit about my view of intelligence here. And yeah,

01:32:48.560 --> 01:32:53.760
that's more or less it for the questions to ponder. Anyone has anything else to add for any of these

01:32:53.760 --> 01:32:59.760
questions? I just want to clarify one thing. I'm still not very sure about the motivation here.

01:32:59.760 --> 01:33:08.080
So why we want this knowledge graph? Why use a knowledge graph, is it?

01:33:10.160 --> 01:33:14.400
Because the internal representation of LLMs is already

01:33:15.200 --> 01:33:31.760
already richer than a knowledge graph. So for example, you can just use the

01:33:32.720 --> 01:33:40.400
last language model to help you to break down the task. Actually, because the way I did it before

01:33:40.400 --> 01:33:45.360
is I want to specifically construct the knowledge graph first. Then I use that knowledge graph to

01:33:45.360 --> 01:33:51.440
break down the task from a hierarchical or sequential learning of intermediate

01:33:51.440 --> 01:33:58.160
goal to RL. But in the sense, you don't really need that particular task at least,

01:33:58.160 --> 01:34:02.000
you don't really need to do that anymore because you just query the last language model.

01:34:03.920 --> 01:34:09.360
At least the common sense way of breaking down a certain task. So you already have the

01:34:10.160 --> 01:34:18.480
different sub tasks. So because my understanding is more coming from that perspective,

01:34:18.480 --> 01:34:22.560
so I'm thinking why do I still need the knowledge graph?

01:34:24.400 --> 01:34:27.920
I mean, as what I said in one of the first few slides, if you use retrieval or method

01:34:27.920 --> 01:34:32.880
generation, you might miss out certain chunks of text. Knowledge graph provides a sort of hierarchy

01:34:32.880 --> 01:34:37.600
that you can pass over and extract information. So it provides the structure.

01:34:38.560 --> 01:34:43.920
But you still mentioned the problem is like even with knowledge, you have to

01:34:43.920 --> 01:34:50.240
distill information. So there is inevitable that certain information are lost. So it be

01:34:50.240 --> 01:34:56.240
it in a way of like you do chunk first, chunking first, then you summarize each chunk.

01:34:56.240 --> 01:34:59.760
Then you summarize again, maybe like a hierarchical way of summarizing and all

01:35:00.480 --> 01:35:08.320
even your knowledge graph, you only extract that the relation you think is important between

01:35:08.320 --> 01:35:15.760
different entities. So you face this issue of having information lost.

01:35:20.960 --> 01:35:25.760
Yeah, so maybe the correct way is not the existing kind of knowledge graph,

01:35:26.640 --> 01:35:31.440
but a new kind of knowledge graph. But I do believe a graph based representation of knowledge is

01:35:31.440 --> 01:35:36.400
important because when you learn new things, we typically try to fit in with our existing

01:35:36.400 --> 01:35:40.800
knowledge and we build on the knowledge from there. So if you have some form of graph structure

01:35:40.800 --> 01:35:46.240
to represent knowledge, you can actually like use that for learning as well. And that's where

01:35:46.240 --> 01:35:55.440
I'm coming from. My intuition is it can be okay. So in that case, I can see how it can be

01:35:55.440 --> 01:36:01.520
useful. It can be served as a heuristic for search. So if you want to understand a very

01:36:02.400 --> 01:36:10.480
large chunk of text, if you extract like a rough graph representation of the information,

01:36:10.480 --> 01:36:17.040
then you just do a heuristic search based on that. So even if this information loss is still can

01:36:18.000 --> 01:36:22.880
expand upon just based on your existing knowledge graph, maybe it just gave you

01:36:23.520 --> 01:36:27.600
some useful signal to tell you like which part of the text you want to do some search,

01:36:27.600 --> 01:36:31.920
then you can just go and search. You don't need necessarily to stick with the strictly stick

01:36:31.920 --> 01:36:36.800
with the graph. So in that way, like just a heuristic but it's a useful one.

01:36:37.600 --> 01:36:43.520
Yeah, yeah. That's one way of doing it, like using a heuristic to search. So it's like replacing

01:36:43.520 --> 01:36:48.080
the cosine similarity in retrieval of mental generation. You just pass through the knowledge

01:36:48.080 --> 01:36:54.960
graph. Yeah. Because the chunking in like just naive way, it's just, it's just like because the

01:36:54.960 --> 01:37:04.720
way we structure essay or structure the text is not necessarily just like each different

01:37:05.760 --> 01:37:11.120
hierarchy like different object or different something that is quite intricate, right?

01:37:11.120 --> 01:37:16.640
If for example, for some novel, you have foreshadowing, you have different way of writing,

01:37:16.640 --> 01:37:21.680
you have plot device, then you suddenly at a certain very, you think apparently very random

01:37:21.680 --> 01:37:26.160
point suddenly become very important. So just by naively chunking into like evenly

01:37:27.840 --> 01:37:33.280
it doesn't, it doesn't work. But if you have a graph like sort of tells you this kind of structure,

01:37:33.280 --> 01:37:39.280
then you're based on that to do some similarity. Definitely I feel like it can be more efficient.

01:37:39.280 --> 01:37:46.240
So I think they give you a better way of doing chunking. Yeah. Yeah, I like this approach. In

01:37:46.240 --> 01:37:52.000
fact, that's one of the motivations of using knowledge graph as well in order to find a better

01:37:52.000 --> 01:37:58.720
way to pass it. I mean, we can pass through graphs pretty well. So yeah, we can perhaps get better

01:37:58.720 --> 01:38:03.040
retrieval using knowledge graphs. So that's one provided you have all the information in your

01:38:03.040 --> 01:38:07.360
knowledge graph, you can get better retrieval using knowledge graphs provided the first part

01:38:07.360 --> 01:38:12.480
again, because that's the failure case we saw just now in the length chain graph answering agent.

01:38:12.720 --> 01:38:18.800
But anyway, I think all the logition all stand from the fact that the context window is very limited.

01:38:18.800 --> 01:38:27.200
So if we can solve that problem, then here actually, I mean, both way either way.

01:38:27.200 --> 01:38:31.680
Yeah, I'm quite excited about this. In fact, I will spend like the next few weeks trying to

01:38:31.680 --> 01:38:36.560
create a new knowledge graph. So I'll share with you after I create it, because there needs to be

01:38:36.560 --> 01:38:41.120
some context dependent passing. And that's lacking right now in the knowledge graph that I see in

01:38:42.080 --> 01:38:48.000
so far. You agree with the context dependent passing, right? Like how you interpret a certain

01:38:48.000 --> 01:38:52.960
like note actually depends on the parents or depends on the position in the graph.

01:38:54.800 --> 01:38:59.840
Yeah, even from a very traditional perspective, this very crucial as well.

01:39:02.960 --> 01:39:08.240
Nice. Okay. Last minute or so anyone has any last points you want to add?

01:39:12.080 --> 01:39:17.840
Okay. If not, thanks for coming. And yeah, if anything, you can still reach out to me on Discord

01:39:17.840 --> 01:39:24.800
or LinkedIn. Yeah. And I'm looking forward to do this linkage between like large image models

01:39:24.800 --> 01:39:29.760
and knowledge graph. This is what a lot of people call a neural symbolic. And I think this will be

01:39:29.760 --> 01:39:36.080
very crucial for intelligence. And I can see how we can use this knowledge graph approach to like

01:39:36.160 --> 01:39:41.360
learn stuff from the environment and use it in a learning agent. Like I like to call it reinforcement

01:39:41.360 --> 01:39:45.680
learning agent, but it's not really reinforcement learning because there are no rewards. Okay. You

01:39:45.680 --> 01:39:51.680
can just learn directly from knowledge in the memory itself. So I think this will be very crucial

01:39:51.680 --> 01:39:55.760
for that kind of framework. And yeah, hope to share more of your after experiment with it.

01:39:55.760 --> 01:40:01.760
Okay. If not, yeah. Thanks for coming. And I'll see you around. Okay. Bye, friend.

