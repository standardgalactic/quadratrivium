start	end	text
0	7080	Hi everyone and welcome to today's session. Today we'll be talking about a very exciting
7080	12840	topic which tries to merge large language models and knowledge graphs together. So as
12840	17480	you all already know, large language models are the recent hype. You can literally do
17480	22240	anything or a lot of things with large language models by just changing the prom. It is very,
22240	28320	very flexible. Get knowledge graphs extremely rigid. You have notes connected to other notes
28320	34320	in relations, but they are also very informative because the relations don't change. The notes
34320	38400	don't change. The large language models, one problem that they face is that they are a
38400	44000	little stochastic. They tend to generate things that may not be grounded in facts. So it seems
44000	48720	like naturally these two approaches seems like a good fit together. One is more flexible,
48720	54640	which is the large language models. And one is more reliable, like the knowledge graph.
54800	60400	Okay, so without further ado, let's begin today's topic. So I will roughly follow the
60400	64480	framework of this paper called Unifying Large Language Models and Knowledge Graphs,
65200	71760	a roadmap. Okay, this is by some IEEE fellows and senior members. I quite like the style of this
71760	77360	paper, but I feel like a lot of the things that are surveyed in the paper are not exactly the
77360	82640	latest large language model stuff. They are like the births, raw births, like basically the 2017
82640	88560	to 2019 era, that kind. So I have supplemented it with some of the more recent advancements,
88560	94160	like some length change stuff. So enjoy. Do feel free to comment anytime, because I think
94160	101200	this is a very interesting field that can be expanded upon. Never before have we
101200	106080	gotten large language models this powerful like chat GPD. And this is really something that we
106080	110640	can look at to improve on traditional methods or even think of a new method that is not even a
110640	115760	knowledge graph. Okay, later I'll share with you some ideas. Okay, so what are the pros and cons
115760	121120	of knowledge graphs and large language models? So as I said earlier, you can look at the rough
121120	126560	summary. I think this is quite a good summary. Okay, large language models, they are generalizable.
127120	133360	Okay, they possess a lot of knowledge. All right. But what they lack is that they lack some form of
133360	140320	understanding or facts. Okay, general language understanding. Okay, this one is debatable,
140320	145680	because like GPT-4 can be said to understand language pretty well. Like NLU is like the
145680	151200	ace most of the task there. Okay, so this one may be not so true in understanding, but for facts-wise,
152320	158560	fact generation is still a problem right now. Okay, incompleteness. Okay, maybe I mean like
158560	163680	sometimes they might generate things that don't answer the question fully. Okay, but increasingly
163680	167920	this is not really a problem anymore. Okay, it's more of like the reliability right now. So I
167920	173520	summarize this part here, reliability. Okay, I should use a different color. Let me just change
173520	182400	my annotation. So I think the main thing that large language models lack are reliability. Oh no,
182400	196080	it's the same color. Let me see. Reliability and consistency. This too. I mean, y'all have tried
196080	199840	large language models before, right? You key in the same prompt. Okay, sometimes you get the
199840	205440	different responses. Sometimes the response can be different. Like I said, should, it's a hot day
205440	208480	today, right? Yeah, scrims on this can be yes, sometimes no, you know, that kind of thing.
209120	213280	All right, so knowledge graphs, what do they have? Knowledge graphs have structural knowledge.
213840	219120	They are quite accurate. Okay, decisive, I guess, if you can find a way to like connect an input
219120	224080	node to an output node, you can say yes, there's a link between them. It's very interpretable. Okay,
224080	228960	actually large language models are also quite interpretable. So it's not really a con here.
228960	234880	Large language is both actually interpretability is also in large language models. Domain specific
234880	240080	knowledge. Yes, but actually, if you think about it, large language models with context grounding
240080	246000	also has domain specific knowledge. Okay, evolving language. This is something that is quite
246000	250720	interesting. Large language models don't really have this evolving language unless you fine tune
250720	256320	it. Maybe the recent Lamato you can fine tune on something. Okay, but you can also use something
256320	260240	like a knowledge graph to ground the large language models. Can you see the synergy here?
260240	267360	There's a lot of things that knowledge graphs do well, that is not exactly antagonistic or not
267360	271840	exactly different in nature from the large language model, it can just be used to ground
271840	277280	the large language model. So it's very interesting. So what are the cons of large language models?
277280	283040	They hallucinate, black box, black domain specific knowledge. So it looks like there can be some
283040	288960	synergy here. And let's explore how we can synergize these two approaches. Before we move on any
288960	299200	quick questions so far. Okay, so this is the one way of getting context into a large language
299200	304160	model and is used very often nowadays. It's called retrieval augmented generation.
304160	311760	So this is the raw format you retrieve from a corpus of documents. You have a few documents
311760	318560	here that you can retrieve from. And then maybe the user asks like, how much is a MacBook Pro?
322880	325920	Right, recently I need to ask myself this question because I'm considering whether I
325920	330720	should buy another one. So you know, they retrieve the relevant documents like,
330720	334640	okay, this document is about MacBook. Okay, you retrieve the right documents.
336720	342560	Okay, this document here is about maybe 2019. You can retrieve the right documents. All these
343680	350720	documents will actually be your context over here. So you could have the context retrieves
350720	361200	like that. MacBook Pro 2019 costs 5000 or something like that. Then you can have like in 2019
364640	372160	Apple release MacBook Air 2019. Of course, I mean, I don't really know the details, but let's say
372160	376240	these are the two documents you retrieve from your retrieval augmented generation.
376880	385680	Okay, and after that, you ask the question like, how much is a MacBook Pro 2019? So it's been shown
385680	390240	that if you use retrieval augmented generation, you can improve the consistency of the output
390240	395040	of the large language model because you are grounding it in the earlier context, which is
395040	399520	this part here, you are grounding it in this part here. So there's an element of grounding and
399520	404000	this is very important for a lot of real world use cases. Because if you don't ground it, you can
404000	410720	end up with quite nonsense generations. Alright, and just as a refresher, okay, what is the most
410720	416240	common method used to select the top K like documents? Anyone can just blow talk? What's the
416240	423680	most common metric to select the most relevant documents? That's a test of understanding. If
423680	428160	you are doing retrieval augmented generation, what is the most common metric used to retrieve
428160	436800	documents? To check the similarity. Anyone? You can write in your chat also. Dot product,
436800	442400	yes, very good. Dot product or cosine similarity. That's right. So usually we use some form of
442400	448880	embeddings. You embed your documents into a vector and then you use cosine similarity
449840	460000	to check how similar the document is compared to the query. Okay, I'm going into some details
460000	464960	over here. Okay, because actually this whole process of doing retrieval augmented generation
464960	471200	and passing over knowledge graphs is very, very similar. Alright, in fact, you could even replace
471200	476080	this retrieval augmented generation with like knowledge graph augmented generation is perfectly,
476880	484080	I think it's replaceable. Alright, so this is some idea of how large language models can be made
484080	491520	to be more accurate. Okay, using something like that. Okay, so this again, I just highlight the
491520	495600	problems of large language models. Okay, may not be able to recall the knowledge, but you can retrieve
495600	500400	the right context using this retrieval augmented generation provided you can retrieve the context
500400	506880	currently. Alright, so this is a real world use case issue. Alright, I've talked to some people
506880	512240	and they say that retrieval augmented generation with just the cosine similarity alone, okay,
512240	517120	might not give you the right documents. So, you know, the embedding vectors train using
517120	522000	contrastive loss, you know, they may not capture everything, especially if your document is very,
522000	526720	very large. Okay, imagine you have only one vector to represent the entire document, and you have
526720	531680	another vector to represent document, another document. So this is like document A, and another
531680	536560	vector to represent document B, then you see how similar they are. But what about like,
537760	544800	what if one document contains many parts? Right, I mean, each of these parts could have
544800	550400	different meanings, right? Each of these sub parts could contain like, let's say you have this
550400	554160	document could have a sub part that is like that, a sub part that is like that, a sub part that is
554240	559200	like that. You know, they just aggregate all of this together into one vector, like that.
559200	563840	Can you see that you're actually losing like information here, which means that when you
563840	568640	retrieve something, let's say if I want to find out how to code, like a length chain question
568640	574000	answer agent, you know, I'm not going to retrieve this vector because by vector similarity, my query
574000	579920	is here. Alright, by vector similarity, maybe I'll retrieve a document that is like B instead,
579920	584480	because like maybe it's nearer in terms of cosine similarity. Okay, I mean, it's greater
584480	589440	is the opposite direction. Let me just make this vector look more similar to that. Like,
590240	597040	let's say I have B is like that. So if this direction here is like how to code a length chain,
598240	603280	QA agent, and this is the embedding vector for it, it goes in this direction.
604080	607680	You know, you're not going to retrieve document A, although it contains that part over here.
607680	612400	Okay, you're going to retrieve document B. This is one of the failings of the embedding
612400	617360	vector. It just tries to capture the whole document into one vector. And this means that
617360	623280	you may not be able to extract stuff out. Okay, Richard said something. This is why I keep saying
623280	629920	context is king. Summarization is essentially impossible on segmentation on segmented documents.
630720	636480	Yeah, definitely, because you summarize you lose information. Okay, so there needs to be like
636480	642320	different hierarchies of how you retrieve things out, broad level, specific level. And you know what?
643440	647520	Knowledge graphs might actually have that kind of hierarchy formulation. I'm actually jumping
647520	653840	a few slides ahead, but give you an idea of why I'm so excited about this idea. All right. So
653840	657680	actually some of the bypass that I've been telling people, I've been advising people is that like
657680	663440	if you cannot get retrieval of mental generation to work, consider using like filters or like labels.
663600	672160	So like this labels will say like, okay, maybe it's like product A, product B. So you know,
672160	677760	instead of relying on just the retrieval of mental generation, or the embedding vector
677760	681680	to actually embed the right knowledge, like let's say you have a length chain QA agent,
681680	687520	I can tag this thing as a length chain QA agent inside this document. So there will be certain
687520	695200	tags that you can have. So then you can then do like the embedding vector across the documents
695200	699920	that have this text. Yeah, so maybe that's one way to like do a first hand filtering.
699920	705280	Yeah, I mean, this is just like some, what do you call it, some bypasses to the downsides of
705840	712160	embedding vectors for pros and similarities. So these are some ideas that could be done right
712160	718160	now to bypass it. But if we have a way to use knowledge graph to do more broad level to more
718160	722720	specific level extraction, maybe you don't even need all this, you can just pass through your
722720	727840	knowledge graph and you can use that to ground the large language model. All right, so this is my
727840	733040	last point here, knowledge graphs are useful to retrieve the right context, search the right
733040	737440	keywords, retrieve the right subgraph. Like let me give you an example here, if let's say I have a
737440	747520	graph like that. All right, so maybe this is a graph talking about like people who view Netflix.
747520	753280	Okay, so these are the Netflix user graphs. So these are the users. And then maybe you have me
753280	759920	over here, John. Okay, and then like the movies are watched. I like to watch the flash, the series,
759920	764160	not bad. I highly recommend it. Then maybe we have another guy like maybe Richard can be here,
764160	770880	watch other movies, like movie A and movie B, you know, yep. So if you want to like extract
770880	775600	something out here, you can just search like for keywords and then you can just put this whole
775600	780400	subgraph here. And then you can use this part here. Okay, how you want to pass it into the
780400	784720	large language model, I leave it for future investigations. There are a few ways to do it.
784720	788960	I will cover some ways today. So if you can pass this into the large language model,
788960	796880	essentially, you can ground the LM in context of the knowledge graph. And then we can actually do
796880	803040	this grounding at a more higher level grounding or more sub level grounding depends on which height
803040	807760	of the knowledge graph you're going to take the notes from. All right, so I think this is a very
807760	813760	exciting prospect. And yeah, I'm looking forward to see like how this can actually work. So I'll
813760	820560	be actually working on getting this to work the next few weeks, right? Because I think doing
820560	824400	something like that actually might help with the up challenge as well, the abstraction and reasoning
824400	831920	corpus. So this is my latest kind of hit way that I'm going into. So this is from the knowledge
831920	836160	graph conference. Okay, I actually listened to quite a few of their videos. This is the knowledge
836240	843360	graph conference 2023. And there's a speaker, Danny from Dev6. I think I pronounced his name
843360	849600	wrongly. But the idea is that if you are using chat GPT for your own applications, if you use chat
849600	855200	GPT in different languages, you might get different outputs, okay, even for the same information. So
855200	858560	you know, being Singaporean and you know, the presidential election is coming soon, I just
858560	862560	asked like who is Singapore's current president right now. So you can see now is Halima Yacob.
863520	868400	Yacob, sorry. And we asked the question in Chinese, all right, Singapore the
868400	876960	總統, so you know, you say, Singapore, there's no president in Singapore. So this is basically
876960	880400	the same information, you just translate it, you can get different performances
880960	886160	with chat GPT. Okay, and the same thing for like if you use Lamatu, Lamatu is heavily trained on
886160	891520	English. If you use Chinese, I'm very sure it won't do very well. All right, this is a practical
891520	896480	problem of large language models. You know, the Chinese benchmarks like they use Ernie,
897520	902080	Wenxing Yi and those other Chinese language models, they say that they perform better than
902080	908640	GPT for. Okay, I mean, at first I was skeptical. Then now that I think about it, they might have
908640	913840	done their evaluation on Chinese data sets. And their language models are fine tuned on the Chinese
913840	918800	data set. So maybe there's some merit to their claims, okay, on the specific Chinese data sets
918800	925120	here. So this is one of the things that knowledge graphs can actually help to solve, because knowledge
925120	931120	graphs can sort of translate this thing, because knowledge graph is not language specific, you
931120	935680	see. So your concepts like president, okay, regardless of how you represent it in words,
935680	941120	okay, your Chinese words or English words, you can actually go to the same part in the knowledge
941120	950080	graph. And then you can have the key words here, like Singapore, and then it's like Halima.
953040	957840	So you can actually retrieve the kind of information regardless of language,
958800	965680	okay, and then you can pass back this information back into the generation of the model. So this
965680	973280	can go here, back here. So regardless of how you prompt GPT in a certain language, okay,
973280	984160	you can do it. So maybe I just do the flow chart. So G-L-M, okay, language, language invariant
984160	994000	representation. Okay, then you do your processing there. And then you go back to L-M. So if for
994000	999840	those of you who have been to some of my other like discussion sessions, you would know that I
999840	1007760	like to say that this is the, this part here. This part here is what I call the abstraction layer,
1009280	1014960	or the latent layer, latent space. So you process it in a way that is different from the input
1014960	1020160	domain, but because the information you process is similar, in this case, we are still asking for
1020160	1024720	semantic information about the president. You know, we don't have to do it in the language domain,
1024720	1030720	we can do it in like maybe some representational space. It could be a graph, all right, and then
1030720	1036240	you can use whatever you process the graph, you can go back to your input space. So this is one of
1036240	1041360	the key advantages that, you know, if we could interface large language models with some form of
1041360	1048720	graphical or some form of memory-based approach that is invariant to the input language type,
1048720	1055040	you could get some performance advantage here. Questions so far? Anyone?
1058800	1064800	Okay, so let's cover some of the basics. What is a knowledge graph? So I took this from the paper.
1064800	1071040	The knowledge graph is basically a triplet consisting of source, destination, to relation. So like for
1071040	1077280	example, Barack Obama was born in Honolulu. So this is the relation, okay, so relation.
1078560	1085040	And this is like Barack Obama as the source, and Honolulu is like the destination. So each
1087520	1095440	knowledge graph is made up of all these triplets joined together in various ways. And the idea is
1095440	1100320	that you just need to connect those entities that are related to each other. You can like
1100720	1105600	you can actually walk through the knowledge graph and get the information you need. Okay, so like
1105600	1110320	there are of course like mega nodes, like for example, like Barack Obama will have a lot of
1110320	1114960	connections leading out of it because you're describing the person. Then like stuff like places
1114960	1120160	where a lot of things leading into it, because a lot of things like like are in the USA, a lot of
1120160	1126960	things are in Singapore, you know. So this is the 20, 30 years ideas of knowledge graph. Okay, it's
1126960	1133120	not too bad. Okay, but it's very restrictive. I personally think that there is a better way to
1133120	1138800	represent information other than this kind of structure. Okay, and we can go and talk about
1138800	1144560	it later in the discussion. All right, I have something in the chat. Richard says,
1144560	1150080	is there a handy reference chart for how this looks or compares to word-to-veg and similar
1150080	1156160	embeddings? Okay, so this typical knowledge graph that I'm talking about here does not
1156160	1161600	have embeddings yet. Okay, but in the future iterations like in 2017 or 2018, I think people
1161600	1166320	have come up with these things like knowledge graph embeddings. So they actually encode all
1166320	1171200	this information here in some vector space. So like maybe like, it's something very similar to
1171760	1176960	like the vector space that we see in in large language models. You can actually encode this
1176960	1182000	thing in vector space. You can encode the relation here in the vector space or so.
1182640	1189600	And then like, you can then encode this part here also in vector space. So like,
1190480	1194160	so it's like doing a vector arithmetic now. So you can see that
1196960	1202160	if I do a relation, it's just simply this one plus this one equals to this one. So I can do a
1202160	1207680	like add a .2 in the first one here and then I can get. So if you have a sufficiently expressive
1207760	1213120	enough embedding space, you can express the whole knowledge graph in the form of embeddings.
1213680	1219920	And that is indeed what some of the later models do. In fact, this is highly related to
1219920	1226160	graph neural networks. Because graph neural networks, they express each node as an embedding,
1226160	1230400	then they do message passing, which means I share information with the other nodes. Like at each
1230400	1234240	time step, I pass some information to the other nodes. And I mean, there are different variants
1234240	1238800	of message passing. The most common is that the message meets in the middle, this one then updates
1238800	1246160	both nodes. So there are a few ways of doing the idea of like updating the embeddings and so on.
1246160	1250400	I'm not going to cover in detail about how all this are done because graph neural networks is a huge
1250400	1255040	topic. Okay, personally, I think graph neural networks is probably not the answer to solving
1255040	1261760	intelligence. I'm sorry to Peter Velikovic. I like what he's doing, but I don't think it's
1262640	1266320	it's the right way to do it, like using differentiable deep learning to do it.
1266880	1273280	So I think the knowledge graph that I've described over here, which is using vectors to do addition
1273280	1279360	and then you get the other nodes. That's a very expressive knowledge graph. Okay, because you can
1279360	1288080	actually express everything in vectors without the names. So you can theoretically do any kind
1288080	1294160	of like addition provided, you know, nodes plus relation give you another node provided that
1294160	1300960	exists. So if you could somehow represent the whole of the world's knowledge in the form of
1300960	1306880	vector space, I say we are done. We can just like, we achieved zero short generalization.
1306880	1310960	You just embed into that vector space and then you add something and then you go to somewhere else.
1311520	1317760	Okay, but I don't think that's how intelligence is represented. Okay, because you know, there's
1317760	1325040	this thing called like context dependent embeddings. Like I don't think like the word Barrett
1325040	1330080	Obama would have the same embedding all the time. So like, for example, if you have Barrett Obama that
1330080	1339120	is like at the White House, Barrett Obama at his house, okay, Barrett Obama at the beach or maybe
1339120	1345200	different places of Barrett Obama will lead to different characteristics of Barrett Obama. Like
1345280	1350400	he maybe is very serious in the office, but he's very relaxed at the beach. You cannot have the
1350400	1357360	same embedding space to represent all this. You need to walk it according to the context. Okay,
1357360	1363520	and that is something that I actually intend to try to do it. Like I try to do a very flexible
1364960	1370960	like basically the information can walk according to the parent nodes in this new form of knowledge
1370960	1376160	graph that I'm thinking of. Okay, so whatever I'm talking about is my own idea. I haven't seen
1376160	1380880	any paper on it yet, but I think the current knowledge graphs will all fail at embodying
1380880	1387440	intelligence because it's just too restrictive right now. Okay, Shang, you asked a question.
1388560	1394080	I'm unfamiliar with graph theory, so hoping to know how do you represent factors as weights and
1394080	1399920	how many can you add? Okay, could you elaborate what you mean by factors?
1403520	1412560	Yeah, you mentioned that like you can add any form of intelligence, right? So take for example,
1412560	1422720	if we are using, yeah, I actually didn't think of this example, but let's say just the simplest one,
1424160	1436240	multi-layer map. Then for these roads, one weight could be how fast the speed limit of the road
1436240	1444160	and another weight could be how occupied it is. Okay, so you are talking about like descriptions
1444160	1448960	of an object, like all characteristics, attributes, you're talking about attributes of an object.
1449920	1457280	Yeah, the weight of each line, correct, of each connection between the nodes.
1458240	1464720	Ah, okay, so like how do you get this embedding here, right? Yeah, correct. Yeah, so perhaps like
1464720	1468640	in your original embedding space, each of these dimensions could represent something already,
1468640	1473920	like maybe one could represent road, one could represent like emotion or, you know,
1474000	1479600	there are different domains that these dimensions could capture already. So if you really have that,
1479600	1486400	you can just like add the relation in that specific dimension. Yeah, so of course, all this
1486400	1491440	will need to be like learn somehow. So it's either learn through deep learning or some fixed biases.
1492640	1499040	Yeah, so ultimately, how well the graph does will depend on how good your embedding space captures
1499040	1507600	all the information. Yeah, okay, so I hope that clarifies. Yeah, thank you. So for now,
1508960	1513920	just know that knowledge graphs have a few forms. Okay, the most simple form is that you take words
1513920	1518400	and then you add another word and then you get another word. So this like describes a relation.
1518400	1523520	The more advanced form will be to use embeddings. All right, so we will talk more about and then
1523520	1527040	of course, the even more advanced form is evolving embeddings or context dependent
1527200	1532800	embeddings, which is like the idea that I have. And it's also the idea that large language models
1532800	1537520	actually kind of use because when we can ground large language models in different contexts,
1537520	1542960	you get different outputs. So a large language model is context dependent processing. Okay,
1542960	1546640	if you can embody that kind of context dependence into the knowledge graph,
1547520	1552880	you will have a very powerful knowledge graph. So as you can see, whatever I'm sharing with you
1552880	1557360	here today, I think that I'm not the answer. All right, I'm just sharing with you here because this
1557360	1564960	is what is existing. Okay, I have a grander vision compared to all of the stuff that I'm talking about.
1564960	1570640	Okay, so let's continue. All right, so knowledge graphs, okay, what excites me in knowledge graph
1570640	1576160	is the very notion of hierarchy. And I think hierarchy is key to intelligence. You don't
1576160	1581840	process things in just one domain. You process things in many domains. Like if I'm drinking a cup
1581840	1588640	now, I'm just like drinking water from a cup, not drinking a cup. Drinking water now, I use my
1588640	1593120	hand to move like that. But then if I think about, oh, how do I go to school? Then I think about,
1593120	1599040	oh, I need to do the bus stop, I need to go to the MRT maybe, and then I need to take this bus
1599040	1604160	or this train. So this is a more higher level planning. Okay, if I were to think about like
1604160	1607760	how I move my left leg and right leg, left leg, right leg, I will take forever before I do some
1607760	1614560	planning. So different problems require different levels of solution finding. And I call this
1614560	1619440	different levels of hierarchy. So it might be up challenge, the abstraction of reasoning corpus.
1619440	1624400	I use multiple levels of hierarchy, like we have a pixel level, we have an object level,
1625040	1629120	you know, and then you express the input grid into different forms of hierarchy.
1629120	1633360	And I find that this way can solve a lot of problems because different problems require
1633360	1637840	different approaches to think of it. But you don't solve all problems using trigonometry,
1637840	1643200	you solve some using algebra, you solve some using set theory. So you have different ways
1643200	1650720	of viewing the problem. And knowledge graph, you can actually use this to extract different,
1650720	1654800	like at the top layers of the knowledge graph, typically are the more broad concepts and the
1654800	1661600	bottom will be more of the, more of the general concepts. So you can see in this, this is the
1662000	1667280	the SICK knowledge base. SICK is a 30 year old project trying to embody the world's knowledge
1667280	1672480	in a knowledge graph. They are still trying to do it, but it turns out that this is a very
1672480	1680080	hard thing to do because the knowledge graphs itself, it embodies like one relation is like
1680080	1684560	a confirmed relation. But sometimes, you know, based on the context, you may not have that
1684560	1689200	confirmed relation. So again, this is the context dependent knowledge graph I'm talking about.
1690160	1695520	Also, another thing is a lot of times we do things, but we don't really know how to express it
1695520	1702480	in words. So if you want to express the whole idea of logic in words, it's a very, very difficult
1702480	1706880	task. Because sometimes we don't even know why we're doing something. Okay, there's bound to be a
1706880	1711840	point of time that logic cannot express things. So you can go and look at this thing called
1711840	1718880	Godot incompleteness theorem. If you use mathematical logic to express things, there comes a point in
1718880	1728160	a time whereby logic cannot solve. Because the way to solve it lies beyond logic. It cannot be
1728160	1735440	Godot incompleteness theorem. There's something like this. This sentence is false. So if you can
1735440	1741680	represent this as a Godot number, this kind of sentence, and then you say that, oh, this number
1741680	1746640	is true. But then this number says that this number is false. So it's like you have a self-referential
1747360	1752400	loop. So if you use logical prepositions, and knowledge graph is sort of like a logical
1752400	1760080	preposition, A goes to B, B goes to C, you might face this problem that you can actually go in
1762080	1768480	you can actually go in a loop that contradicts itself. So that's one thing that knowledge
1768480	1776000	graphs may have some issue with if we do it being 100% fact. If A links to B all the time,
1776080	1782640	sometimes you might actually have a link that contradicts itself. That's one issue of the
1782640	1789920	knowledge graph. The other thing is, so burying all these issues, one thing I like about knowledge
1789920	1795200	graph is that you can see in this diagram here, you start off with small stuff like thing, and then
1795200	1801840	you go to like individual, you go to collections, and then you have different ways of doing it,
1801840	1809040	time, movement, and so on. Then you have agents, actors, plans, and goals. I mean,
1809040	1812240	if you think about it, kind of it's like how large language models is evolving now, right?
1812240	1817200	We are kind of at this stage, we are agents now. So after that, we have organization of agents,
1817200	1822080	we have activities. Okay, hopefully we don't get to military warfare, you know, because like,
1822720	1828800	so this is like the evolution of a population. So it's quite nice, and like you can capture all
1828800	1833840	this knowledge from, okay, so I wanted to say this is for more like micro level
1834800	1841040	to more macro level. And the macro level is actually the sum of the micro level. So
1841920	1845280	maybe the arrow should be drawn the other way, the arrow should be drawn like that.
1846080	1850480	You take from the micro stuff, and you go to the macro stuff. So this is the knowledge
1850480	1855840	that we accumulate, right? And knowledge graphs can capture this quite well, because of the way
1855840	1860320	you take from source relation to destination, you can capture from the micro level, you do
1860320	1867680	all the branching, and then you end up with the micro level, right? So this was in 2016,
1867680	1873840	by the way, I couldn't find this in the sick website today, right? So this is the sick knowledge
1873840	1878400	graph. This is like a very, very tiny representation of how the knowledge graph looks like. I just
1878400	1882080	wanted you to see like how one of the largest knowledge graph in the world looks like. So
1882080	1886560	you can see like you have all these like the fortune companies, you have all these like functions,
1887360	1893200	like all these like look like some form of like math stuff, right? Yeah, so you have like people
1893200	1899920	over here. So you have different areas of congregation of all this knowledge, right? Then
1899920	1904720	in order to pass through the knowledge graph, you have to use something very, very similar to SQL,
1905360	1909920	structured query language, you like say that, oh, if I want to get a frightened person, I want to
1909920	1914880	get the entity X that is a person, and then fuse emotion that is fear at a very high level.
1915520	1920800	So you have to do this kind of stuff, right? So immediately you can see that knowledge graph
1920800	1926240	right now can be immediately improved by large language models in one aspect. Okay, and this
1926240	1930880	aspect is that we can straight away use the large language models to generate this structured query
1930880	1939600	language. Okay, so if all of you are thinking about this, like if you want to get like a very,
1939600	1944160	very rigid programming language out, okay, you can actually write what you want in free text
1944160	1948240	and then say convert this to SQL, and you can get it out. So you can do the same thing for
1948240	1953600	this sick language. You give it some examples of how the language works. You say, I want to get
1953600	1957920	a frightened person and then, you know, chat GPD is quite good at getting stuff like this out.
1958400	1964080	Okay, no more SQL, right? I love it. Okay, if you need to use SQL, just use chat GPD.
1964080	1968960	Okay, it's a very good replacement. All right, so this is one way large language models can
1968960	1972400	already help to benefit knowledge graphs right now because it can pass through it
1973120	1977920	using very human readable and understandable syntax. Like this kind of thing is not very human
1977920	1983680	understandable. You can use free text to do it and we can do it right now. But more importantly,
1983680	1990880	what can large language models do to help knowledge graphs? Okay, or what can knowledge
1990880	1995120	graphs do to help language models? Okay, so before I move on to that, let's just talk about some other
1995120	1999760	ideas I have. So I'm actually a reinforcement learning person. So like, I feel like knowledge
1999760	2004480	graph can also represent stuff like different states, like you have different tiredness,
2004480	2010240	they drink coffee, and then you're not awake. So if you know in the literature of
2013040	2016240	reinforcement learning, this like a Markov decision process where these are the states,
2017200	2021680	okay, and these are the actions. And then this is the next thing.
2023520	2027440	Okay, so you can actually use knowledge graphs to represent stuff like this as well.
2027440	2034240	Okay, because it's quite anything that has a link like that, you can represent this easily. Okay, so
2037680	2041840	all right, this is perhaps the most important slide for today. Okay, this is not in the paper
2041840	2047760	that I referenced, but this is the thing that I was thinking about. It's like knowledge graph is
2047760	2053200	actually sort of a tool that can be used by the agent. So like retrieval of method generation
2053200	2057840	may not get the right passages because like the embedding space may not be good. Perhaps we can
2057840	2062160	use like a form of knowledge graph passing, okay, you can extract relevant parts of the knowledge
2062160	2067280	graph, you can retrieve the context based on that. Okay, so you can ask the knowledge graph to get you
2067280	2073680	the subgraph. The subgraph, you can then use it to ground the context of the agent and how you use
2073680	2078000	it to ground. Okay, it's up to you. Okay, some people might use graph neural networks. I don't
2078000	2082800	advocate for that. Okay, one other way of doing it is to just convert it back to free text. Okay,
2083440	2087920	as easy as that. So you use the knowledge graph to extract out the relevant purposes and avoid the
2087920	2092960	need for the embedding space, the open AI embeddings. Okay, you use the knowledge graph to
2092960	2097600	extract it. Then you take the stuff that you extract from the knowledge graph, pass it back as text
2097600	2108160	and then go back to the agent to ground it. All right. So yeah, one other way of, one good thing
2108160	2112880	about this is that if you have stuff like if you are doing this for a robot, okay, that experiences
2112880	2118640	the world, you might actually be able to use this knowledge graph. Okay, I'm conflating the
2118640	2122400	term knowledge graph, but this knowledge graph can now be the state action state graph. You know,
2122400	2128560	you can actually model relations of the world easily. Like I always believed like we learned
2128560	2133120	from taking actions in the world. So we can actually build this knowledge graph dynamically.
2133120	2137120	This is the third point. Okay, you can gain knowledge of the world. We can build up this
2137120	2141280	knowledge graph bit by bit. All right. And then we can then query this knowledge graph
2142320	2150160	and get answers from the knowledge graph to inform our choices. Okay, so about how we can get this
2150160	2154960	part here, this is a huge thing here. Okay, because I believe that there's one thing that's missing
2154960	2161040	in current knowledge graph and this thing is called changing the memory to the context
2161760	2167280	at hand. Okay, so I treat the knowledge graph as memory. So like when you retrieve things from
2167280	2172320	memory, okay, and then you want to apply it to the current state right now, current state of the
2172320	2178240	world right now, you don't really want to just use that memory itself. You want to adapt that
2178240	2184000	memory such that it will be relevant in this current state. Like if I have drank like coffee
2184880	2191360	at school or not, drink coffee at home now, okay, you know, something like that. This, I will need
2191360	2196480	to adapt that memory of drinking coffee somewhere else and then adapt it back to here. Okay, there's
2196480	2201120	no point in giving me the memory of me drinking coffee somewhere else because it doesn't adapt to
2201120	2205680	the current situation. So if you can adapt this knowledge graph to the current situation,
2206320	2210960	that will be great. Okay, that will be great. So that's something that I think I'm trying to
2210960	2216720	look into because you don't just want static knowledge extraction. Okay, you want knowledge
2216720	2222640	extracted and manipulated to fit the current context. Okay, of course, for those of you all in
2222640	2227680	my discord group, I've been thinking about memory recently. And you know, human memory is very
2227680	2233440	malleable. Like if you think about something, you might actually affect that memory of it.
2233440	2237760	So like a lot of times people in the childhood, they think that they have certain memories again,
2237760	2242560	like maybe you are lost in a supermarket. So if I keep asking you questions about it, I say,
2244000	2250000	who was the stranger with you when you were lost? Okay, so maybe there was no stranger. But if I keep
2250000	2254480	asking you guiding questions like that, eventually, you might think of your memories like, oh, yeah,
2254480	2259120	a stranger let me home after I went, I was lost in the in the in the mud. Okay, but that that may
2259120	2264400	not have happened. That memory has changed because I've asked you certain guiding questions. And then
2264400	2270480	you think that certain things are now in your memory. Right. So whether or not we should change
2270480	2277200	this memory and affect this knowledge graph, I leave that to future discussion. Okay, because
2277200	2281360	this is something very interesting, like should we change the existing memory that we have,
2281360	2286240	based on the current context, get our brains that our brains do that. Okay, but should we do this
2286240	2292480	for this kind of practical systems? Okay. Yeah, so we say humans hallucinate. Yeah, of course,
2292480	2298400	we hallucinate a lot. And that is why actually, we are quite similar to large English models in
2298400	2302960	that sense. Now people always say large English models not very reliable. Are humans reliable?
2302960	2307440	Our memory is not that reliable, actually, if you think about it. But honestly, I cannot trust
2307440	2313680	my memories that that much, because like sometimes if it's too far away, it can change like the book
2313680	2320240	that I've been reading. It will say that like flashback memories, which people think that are
2320240	2324640	very, very pertinent, flashback memories are memories like, like, you know, 911 collapse.
2325360	2329360	People tend to remember what they were doing at that time, because it was so significant.
2329920	2334560	It turns out that this flashback memories can be wrong. Okay, it can also be, it can also be
2335360	2341280	change. So this is a very interesting thing. You can actually use like the current context
2341360	2345840	to affect the memory you have. So you might actually affect the knowledge graph about whether
2345840	2351200	or not we want it to be that way. Okay, we have to think about that. Okay, I digress a bit. Okay.
2352400	2356560	But let me just get back to topic. Okay, today we have quite a few slides to cover. There are three
2356560	2360560	approaches that I want to talk about today. First is that you can use knowledge graph to
2360560	2364960	enhance your large language models. And this means that you can give it structured stuff,
2364960	2369840	like domain specific knowledge. In some sense, it's like text based grounding is the same as
2369840	2373840	retrieval of mental generation, just that now you take the information from a knowledge graph.
2374720	2380320	Number two, you use large language models, expressivity, okay, and make a better knowledge
2380320	2384240	graph. Okay, I like this approach as well. Okay, we will see how to do it. And lastly,
2384240	2388160	you combine both approaches, you can get a synergized large language models and knowledge
2388160	2394400	graph. And I think something like this will be able to embody intelligence. Okay, but not the
2394400	2401360	current knowledge graph. We need to change it to a dynamic knowledge graph. Okay, what is a dynamic
2401360	2406880	knowledge graph? Maybe I'll talk about it next time. Okay, after I flesh out some ideas that I
2406880	2411840	have right now, I will create this dynamic knowledge graph. Okay, I think the current knowledge
2411840	2415920	graphs are not the answer. We need to have a different kind of knowledge graph. But if we use
2415920	2422240	this, I think we can get intelligence. Okay, let's move on to the next point. Approach one,
2422240	2427200	knowledge graph augmented large language models. Okay, so there are two ways I can,
2427200	2431200	I summarize the paper in two ways. The main thing is one, you can just put the knowledge graph as
2431200	2436400	text. And the other one is treat this as an object. And what kind of object? Okay, you either use like
2436400	2441360	graph neural networks, or you can use an embedding space. I mean, the one that was used was trans
2441360	2448080	e trans embedding. You can go and search the paper trans. So this are some ways that we can use the
2448080	2455120	knowledge graph, okay, to pass it. Let's go through the first way. So the first, oh, sorry,
2455120	2460880	this is basically a pipeline for retrieval of large language models grounding. First,
2460880	2464160	you use some form of knowledge retrieval, like, you know, retrieval method generation,
2464160	2470080	you use cosine similarity, you get certain facts or some documents. So I'm just relating this to
2470080	2474480	retrieval method generation, because they are almost the same. All right, you take in the facts,
2474480	2479040	you ground the large language model, you get the answer. Okay, and over here in the paper,
2479040	2482720	they put back propagation. But you know, how are you going to back propagate this knowledge retrieval?
2483600	2488400	You're going to end up with some, like, very, very weird way of doing back propagation. I don't
2488400	2492160	think back propagation, I don't think back propagation is the answer here. Maybe you want
2492160	2497040	to back propagate your LM to find you think, okay, I grant that. But this part here, to back
2497040	2501840	propagate to the knowledge retrieval, I don't think that should be done. All right, because this
2501920	2508000	back propagation thing will lead to, like, changes in embedding space. And then if you
2508000	2513520	change your knowledge retrieval, you also need to change your large language model. It's a never
2513520	2518000	ending cycle of chasing each other. Like, if you change the knowledge embeddings for the knowledge
2518000	2521440	retrieval, you also need to change how you interpret them in the large language model. So,
2522320	2526240	yeah, I don't think you should use back propagation for the knowledge retrieval. You should probably
2526240	2532000	use, like, memory methods, other methods, like you can say that, okay, what worked, what did not
2532000	2538640	work, what worked, what did not work. Okay, you can reference this paper called Voyager. Okay,
2538640	2542800	so there are these automatic curriculum learner. I think you should train the knowledge retrieval
2542800	2547280	like the automated curriculum learner. You just ground it in some examples of what works, what
2547280	2554400	doesn't work. You don't have to use back propagation for that. Okay, so the main pathway for knowledge
2554400	2559360	graph for tax, for LMS is like that. You take the knowledge graph, pass through it, get some
2560160	2565200	facts, and then feed it into the large language model. Okay, that's the main pipeline. Okay,
2565200	2577600	questions on this? Okay, let's move on. Okay, so this is one of my favorite papers. Okay, this is
2577600	2583680	the Generative Agents paper. They have 20 agents in the sandbox interacting with each other. And
2583680	2588880	one thing that struck me quite well for this paper is that they actually use JSON structure
2588880	2596160	to ground the actions. So, for example, if you want to ask like Eddie Lin, he currently is in
2596160	2601520	the Lin family's house. He's in the bedroom actually on the desk. Okay, so you can ask the
2601520	2606720	agent, okay, this is actually the chatGPD prompt. Okay, you can ask the agent like, okay, these are
2606720	2614400	the other areas that we have. Okay, and all these other areas are obtained from the JSON.
2616560	2619120	Actually, the JSON is like a knowledge graph.
2621840	2627760	Okay, because we actually have hierarchies like Lin family house has a bedroom, has a study room,
2627760	2632000	has a kitchen, you know, this is something like a knowledge graph. If you ask me like,
2632000	2636720	they are just like representing the hierarchy of the house. Like, I mean, if you want to treat it
2636720	2642480	as a knowledge graph, you will say like, this is the house. So, this is the Lin's house. Lin's house.
2643280	2649520	Lin's house. I've just put Lin's age. Okay, then you can have like, the relation will be contains,
2650720	2655440	okay, or comprises, I mean, contains, then you can have like bedroom.
2656320	2662480	Yeah, so the JSON kind of hierarchy is a subset of what a knowledge graph can embody.
2663200	2669360	All right, so I treat this as a knowledge graph. So you can, you can sort of ground the agent.
2670160	2674560	Okay, like this is what the agent knows. Okay, this is the current memory that the agent has
2674560	2679840	as a form of knowledge graph. Like this are the kind of areas that we actually know from the world.
2679840	2684960	Okay, this is like, if you talk about grid cells and play cells, maybe find out more areas.
2685040	2689520	Okay, you can ground them. Okay, these are your semantic knowledge or oppositional knowledge that
2689520	2695040	you have about the world. And actually, these two are the positional knowledge. This is the first
2695040	2702560	one is knowledge about the house, knowledge about house. And then the second one is knowledge about
2702560	2714160	the world. Yeah. So you, you have all this knowledge, you can ground the agent to choose
2714240	2720000	a specific place. Imagine if we did not ground the agent with all this stuff at the top,
2720000	2725680	you just ask, where should Eddie Lin go to? Then Eddie Lin might, the LL might reply,
2725680	2729680	Eddie Lin should go to the supermarket or something like something that is irrelevant to the game world.
2730320	2736400	Right, so because we grounded it with some idea of what kind of possible areas that the agent
2736400	2744000	should go, the agent is able to choose one area from the above list. And how is this list generated?
2744000	2749120	It's generated from passing some form of knowledge graph. And this is what I mean by using knowledge
2749120	2754080	graph as text to ground the large language model. So you can use this recursively, you can say that,
2754080	2759920	oh, currently you are in the maybe common room. Okay, what, where in the common room would you
2759920	2764960	like to go? We like to go to the sofa, to the mirror, and you can do this recursively. Okay,
2764960	2771840	and then you can get a very, very specific area that the agent is going. Okay. Any clarifications
2771840	2778560	on this so far before I move on? Yeah, if you haven't read this paper, go read it. Okay, this
2778560	2787760	paper is good. It's one of the better ones. All right. So we have the Chinese LLM, it's called
2787760	2794480	Ernie. All right, and this, what they do is they actually use two hybrid ways of generating
2795760	2801120	the output. So they say that large language models lack grounding, lack consistency. So we use
2801120	2805760	a knowledge graph. Okay, I granted that. But then I look at that structure and they're like,
2805760	2811440	oh man, what is this? So they actually have a large language model. This is the typical
2811440	2814640	transformer architecture. So this is a typical transformer on the left side.
2817120	2822240	So they have two encoders. Okay, one is the T encoder, which is like the text encoder. And why
2822240	2826960	is the knowledge graph encoder? So the knowledge graph encoder uses this thing called trans embeddings.
2826960	2830400	Okay, I'm not going to go through that, but they train that embeddings using like,
2830400	2833040	they take one vector and take another vector, then they connect the,
2834160	2838640	just draw the diagram here for you to see. So you have one vector A and another vector B,
2839360	2844800	and then you create another vector C here. So you can keep like using this vector A,
2844800	2848640	you take another vector, extend from it, and then you can train on this relation C. So, you know,
2849360	2853920	this is how the trans embeddings are trained, trans E. Okay, and they use this kind of embedding
2853920	2857840	space. Okay, you can do self-attention, and then you can do cross-attention across
2858640	2862960	both the trans, the text embeddings as well as the knowledge graph embeddings.
2862960	2866800	And then hopefully you get some output, right? And then you get some text outputs here,
2866800	2871520	and then you can update your knowledge graph to the knowledge graph outputs here. Okay, so this
2871520	2877440	is a way to embody a knowledge graph as some embedding space. Okay, and then we can use like
2877440	2883840	the attention to like, attend to like the text-based stuff as well. So, yeah, this is just one way of
2883840	2889280	doing processing using a knowledge graph. As you can see, I don't really like it. I mean,
2889280	2893680	I think that it's too convoluted. Like, okay, so this is another discussion question that I'd like
2893680	2898480	you all to think about. Should we have separate embedding space for this large language model
2898480	2903440	stuff and for the knowledge graph embeddings? Like, should we use two different embeddings?
2903440	2906880	Okay, should we use two different embeddings?
2906960	2914640	Yeah, or should we use the same one? Yeah. Okay, then also more generally, like if you want to have
2914640	2921200	multi-modal embeddings, like you have text, image embeddings, you have audio embeddings,
2921200	2925680	okay, if you want to do a multi-modal large language models, you can actually also put them
2925680	2931920	into distribution model at the end here. Okay, but the question is, in fact, knowledge graph can
2931920	2936800	be multi-modal also, you can actually also put it here. The question is, all this image
2936800	2941120	and audio embeddings, okay, you can put it in the large language model, you can also put in the
2941120	2947760	knowledge graph. But why not just use a single embedding, right? Why do you need to? You are
2947760	2952640	using text-based knowledge graph and text-based large language models, that there's no external
2952640	2957280	domain here. It's all the same domain. Why do we need two different embeddings for the knowledge
2957280	2963200	graph and the input text? That's my question. If you have any ideas, let me know. But think about
2963200	2971120	this, all right. Okay, then we have this question-answer graph neural network, and this does a
2971120	2975920	two-way interaction between the language model and knowledge graph. And what we can see here is
2975920	2980320	that we have a certain question, okay, and some options that to choose from. The large language
2980320	2985200	model will go in here, and then they express the question and the options as part of a knowledge
2985200	2990400	graph, okay, and this will go through another knowledge graph encoder, and this knowledge
2990480	2998240	graph encoder is a graph neural network. Okay, and this basically will, you look at this diagram here,
2999760	3005200	it will do cross-attention. It's very, very confusing. Look at this language model conditions
3005200	3009200	knowledge graph. So you can blank out some notes here, you know, you can actually do some attention
3009200	3014800	on some notes to like block off the path. Yeah, so yeah, that's possible. So you can also use the
3014800	3021360	knowledge graph to condition the attention in the, in, in, when, when you do the next open
3021360	3027520	processing for the language model, and then eventually you get your answer. Yeah, so this is
3027520	3032160	one way we can process the knowledge graph, you can process it using a graph neural network.
3032960	3036960	And in fact, the earlier one on Ernie, that is similar to graph neural network as well. I mean,
3036960	3041760	they are using the embedding space. And you know, if you just do some operations on the embedding
3041760	3045840	space, that is a graph neural network already. So yeah, this is very, very similar to graph neural
3045840	3053200	networks. And yeah, it shows that back in the first few years, people use these kind of methods to
3053200	3058960	pass through knowledge graph using graph neural networks to represent the embeddings. Okay,
3059920	3065040	I don't see why you need to do this. Okay, personally, I don't see why you need to do this.
3065040	3071520	You can just use text, because the knowledge graph representation is in the same kind of domain as
3072000	3077040	your last language model representation. They're all text. Yeah, why do we need a separate embedding?
3077600	3086000	So yeah. Okay, Richard said, I think there will have to be input output embeddings and train
3086000	3093920	them to address common pattern or memory structures. Sorry, could you explain this comment was in
3094000	3101600	relation to which part of what I said? So you were saying, you know, it's a decision sort of or
3102480	3108560	sort of my decision question, right, as well, that there is, how do you
3110480	3114640	well approach this problem? Why do they have their embeddings separate, right?
3116640	3121040	At the end, there's a sort of cross attention where they're merging them for an output of this
3121040	3132240	type or that type would have you. But then this idea comes that the real, so I think of a large
3132240	3136800	language model where the reason why they have these emergent behaviors is because language is
3136800	3144560	currently our best mechanism to embody thoughts, ideas, and our most direct implementation of
3144560	3150800	ideas. Now, particularly once they're broken down, tokenized and so on, you've taken through that
3151040	3156800	process a few times, consider retention and context, you come up with new ideas. Yes, yes.
3158800	3165600	And then as you're just saying, there's no particular need for different embedding spaces.
3166720	3173120	And the only need for them is to bring understanding into a common framework where
3173120	3178400	the ideas themselves in the latent space are being considered and their context and their
3178400	3190080	relationships. So how the, this is sort of a, what's the word, this idea of boiling down
3191280	3198480	the actual form of communication into some representation, any representation where we
3198480	3208400	can start applying our knowledge to it. Whether you read text or listen to text,
3209680	3216160	you don't, when I hear things, I don't imagine them written down in front of me. I just hear words,
3216160	3224320	words become ideas, and we go from there. So in the same way, I see the way that
3225040	3231360	knowledge is presented as an input-output problem and embeddings really address the input-output
3231360	3239920	problem. And then after that, there's a memory and consideration process which operates on ideas
3239920	3246000	which are not linked to input and output. I think you and I agree that there needs to be a latent
3246000	3250800	space or abstraction space for processing. And I think you also agree that there need not be too
3250800	3254800	separate embedding space for the knowledge graph in the last image model. If I hear it correctly,
3254800	3258400	right, you don't, you also don't think that is necessary, right? I think,
3260960	3265040	yeah, but then the problem becomes, if you don't use the same embedding technique,
3265680	3271840	how do you present meaning? So for mine, in terms of large language model being in language or not
3271840	3279280	in language, in words, the question is really, are we making the problem harder for ourselves
3279920	3287120	by using a difference, by saying, well, it's all words and the words are by and large correct.
3287120	3293200	Therefore, we'll just use a large language model to read and ingest a large language model. And I
3293200	3298960	think that will work. But the challenge becomes what you alluded to earlier, where the Chinese
3298960	3305360	representation versus the English representation gives a different outcome. And I'm trying to
3305440	3314800	abstract away that behaviour. So the thinking is, the actual thinking happens in, is always in
3314800	3323360	latent space. And the only job for embeddings is to present in a form where, you know, cognition can
3323360	3333440	happen. Right? Because, right? And so I would say, I don't particularly care what the encoding
3333520	3340480	encoder is, decoder is, it can go from text in, picture out, picture in, text out, it doesn't
3340480	3347520	matter. The important thing is that it's consistent, and we can operate on it in a manner that addresses
3347520	3354480	the patterns and relationships within. Yeah, well said, well said. I agree with you. So
3355200	3358720	what matters is how we abstract it to the processing space, which is the latent space,
3359360	3365040	and how we encode it and decode is just extra details, that basically just needs to be mapped
3365040	3370320	there, and it should be good enough. Yeah, I think so. And if it comes to its own training
3370320	3376320	challenge. Correct. So I think in the earlier papers, what I get is like, why do we need a
3376320	3380480	knowledge graph encoder like that? It's because they use embeddings like trans-e that, you know,
3380480	3386400	are different from GPD embeddings, like, or BERT embeddings. Again, most of the early papers use
3386400	3394080	but BERT, bi-directional encoder representations from transformers. So what happens is because
3394080	3398720	these two are from a different embedding space. So you kind of need to map them to the same
3398720	3403520	embedding space. That's why you need a knowledge graph encoder and a large-engaged model encoder.
3405040	3410880	But in the new kind of knowledge graph that is constructed, because this large-engaged model
3410880	3415680	is now so powerful, you can actually use the embedding space for the large-engaged model to
3415680	3420960	construct your knowledge graph. And if you do that, okay, if you do that process, which is
3420960	3425120	part two of the presentation today, you will see later, if you use the large-engaged models to
3425120	3429200	construct the knowledge graph, actually, you don't need a separate knowledge graph encoder
3429200	3435200	or encoder here because they are in the same embedding space already. So if you look at this
3435200	3442880	thing here, you don't need a separate decoder for the JSON here because this is in the same
3442880	3448800	embedding space as your text. And I would like to posit that it will be better for all of them to
3448800	3455120	be in the same embedding space because it will be much easier to do the attention. I mean,
3455120	3458480	it's easier to do attention in the same domain as compared to different domains because, you know,
3458480	3464160	cross-attention is only one layer right now. You're going to do a very efficient cross-attentioning
3464160	3469040	multiple layers. But if you just do it in the same domain, the transformer architecture right now,
3469040	3474800	you actually do the self-attention multiple times, all right? So it might be actually better to
3474800	3479760	do it in the same domain, okay? And of course, you will save training complications because,
3480400	3484880	you know, you need to map both to the same latent space and, you know, that is a difficult problem.
3484880	3490400	It's a very difficult problem to map two different streams of inputs to the same latent space. I
3490400	3495840	mean, we have seen it, like in OpenAI, they have this thing called clip, okay, that max text and
3495840	3505120	images to same latent space. You know how many examples they train it on? Millions, I think even
3505120	3510480	billions. Yeah, so it's a very, very difficult problem to map both to the same latent space.
3510480	3514400	If I can map it well, of course, you can do like stuff like stable diffusion, you know,
3515120	3520800	dolly, you can generate images from text. Yeah, but why have this problem with the
3520800	3525040	knowledge graph when you can actually just ground the knowledge graph in the same embedding space
3525040	3533440	as your large language model? Okay, so think about that. Okay, now we go to approach two.
3533440	3539040	Before I move on, I'd like to open the floor for any opportunities to ask anything so far for the
3539040	3548400	first part. Okay, if not, I'll carry on. So next is how we can use a knowledge large
3548480	3555280	language model to get the knowledge graph. So one is to use using few short and zero or zero
3555280	3559520	short prompting, like for example, length chain, okay, I don't think the approach is that great.
3559520	3565840	Okay, I found a better approach, right, using a better prompt, but we can potentially use
3565840	3570000	large language models to generate knowledge graphs. The other way is to use the embedding
3570000	3575440	space of the large language models to enrich the representational space of the knowledge graph.
3576000	3581760	So this is also quite interesting. Let's see how we can do both. Okay, the first one is,
3582480	3587520	okay, this is just some idea of how we can use it. Okay, we can, we can few short prompt
3587520	3591840	to generate the relations, okay, because large language models are just very versatile
3591840	3596400	and can be context driven to do it. And actually it's way better than, you know,
3597280	3602640	so this is my own experience. I use space scene to do name entity recognition and I use
3602640	3608800	large language models to do that. The GPT, chat GPT performs way better than space scene. Space
3608800	3614560	scene makes out a lot of the names, all right. So if we use large language models to generate
3614560	3620480	the knowledge graph, compared to traditional approaches, like spacey or some other verb,
3622400	3627520	VMP, you know, those kind of three parcels for language. Last time people used that to generate
3627520	3633600	the knowledge graph to find out what are the nouns, what are the verbs and so on. Okay, so that was
3633600	3637280	difficult to generate the knowledge graph because sometimes it miss out certain things.
3637280	3641840	But large language models are quite good. Okay, why not just use large language models directly
3641840	3648320	to generate the relations and the source and the destination. So indeed, this is what Lang
3648320	3653440	Chen did. Okay, so if you look at the graph QA prompt, this is the prompt. Okay, you are the
3653440	3658400	network intelligence. Okay, help to integrate stuff into a knowledge graph, extract knowledge
3658400	3663200	triples from text. Okay, a knowledge triple is a clause that contains a subject, predicate,
3663200	3668240	an object. Okay, subject is entity being described, predicate is the property,
3669280	3675520	object is the value of the property. Okay, there's a typo here. Okay, so this is the
3677120	3681680	zero shot prompting for Lang Chen. All right, this is not that good yet. So you need to give
3681680	3685920	some few short examples and they gave some few short examples in the prompt, like for example,
3686720	3693120	like this is the input, and then you can say that oh, Nevada is a state, Nevada is the U.S.,
3693120	3698640	Nevada is number one go producer in Go. So I don't like this example. Okay, because for one,
3698640	3704560	they did not say the state at all in the prompt. Like then you want the model to just plug the
3705280	3712080	plug the noun from thin air. So yeah, like here, I'm going to the store output none. Why, why is
3712080	3721840	the output none? It should be I went to store something like that. Yeah, so you should be able
3721840	3727680	to extract something from this. So I disagree with the examples that the Lang Chen one provided.
3727680	3733200	Okay, so I think if they improve this example, maybe theirs would work better. So let's take a
3733200	3739120	look at what I did later. So I'm not a fan of Lang Chen, by the way. Lang Chen prompts are very
3739120	3748000	worthy. So this is the other way that we can use the large language models to do the text encoding,
3748000	3753280	to do knowledge graph embeddings. So this is called KGE, knowledge graph embeddings,
3753280	3757120	is something like, you know, if you talk about the stuff like trans, these are like embeddings
3757120	3762720	that we can give to the source, to the destination, to the relation. So we can represent the knowledge
3762720	3769840	graph as embeddings. And we can use GPT or some large language model, okay, to generate some
3769840	3775760	embedding space here that you can then use like MLP, multi layer perceptron, and so on to map to
3776400	3781840	to the embedding space of the knowledge graph embeddings. So this is one way we can utilize
3781840	3787600	large language models to do it. Yeah, I mean, I was thinking, you know, like, why not just use this,
3787680	3793920	right? Why not just use LAM embeddings directly for knowledge graph?
3796080	3801120	I mean, LAMs are way better than, than doing graph neural networks, in the sense that, you know,
3801120	3804720	if you know the problems with graph neural networks, I'm just going to tell you the problems of graph
3804720	3810960	neural networks now. Okay, they have these two problems. Okay, this is one is called over squishing
3810960	3816160	or over squashing. And the other one is called over smoothing. Okay, what are these two problems?
3816160	3822160	Over squashing is that the information, because you pass the information into an embedding layer,
3822160	3829520	information gets lost at embeddings. Okay, so this over squashing thing is also a problem for
3830960	3836080	LAMs. So I'm not going to cover too much on it. The other problem that we have for this kind of
3836080	3841360	graph neural network over smoothing, okay, is that after you do message passing
3842000	3850560	or too many times, all embeddings look the same. Okay, so this is a big problem. Okay, I also
3850560	3854480	realized this, that once I did graph neural networks, you, like, you have two nodes, you pass
3854480	3858720	information to each other, and then you become the average of the information, you keep doing this,
3858720	3864160	right? Eventually, both nodes become the same, or very, very similar. Okay, so this is one of the
3864160	3870960	huge problems of graph neural networks. And I feel like the embedding space that is best
3870960	3875680	done, right, is not the way that we do message passing in graph neural network. We should just
3875680	3880720	ground it in the context using a large language model. And large language model update the context
3880720	3886080	quite well. Okay, then you can just use the embedding that is derived from that particular context
3886080	3890960	in, like, you can just put something like that, like, you can just say context. And then like,
3891920	3897520	I am a student or something like that. So, like, this context will update the definition
3897520	3902880	of the student here. So you can go through the transformer module. So this is the transformer
3902880	3911040	module. And then you can get the final embedding here. Yeah, at the final layer, right, before
3911040	3916720	the softmax, you can actually use the transformer to get the embeddings already. Why use knowledge
3916800	3921680	graph embeddings? Okay, so I'm just putting this question out here. So I hope those people
3921680	3926320	knowledgeable in this area can come and, you know, correct me if I'm wrong. But I don't see a point
3926320	3933840	in doing this. Yeah, right now. Okay, so let's leave it as that. And let's continue.
3936080	3941040	Approach tree. So the approach tree is how we combine both approaches to make a very, very
3941920	3946240	synergistic model where the large language model can generate the knowledge graph dynamically
3946720	3950640	and this knowledge graph is something like a dynamic memory that gets updated as the agent
3950640	3955600	explores the world and so on. And this knowledge graph can then inform the knowledge, the large
3955600	3962240	language model and ground it in consistent generation. So let's see how this works. So you
3962240	3968160	can see this is the diagram in the paper. And you can see like data. Okay, that's what me and Richard
3968160	3976400	discussed. Data, okay, will be from different domains to embed them into latent space. Okay, so
3976400	3981520	now we just assume that there's only one latent space but my view is that there's multiple latent
3981520	3988480	spaces. Okay, so right now we just treat it as there's only one latent space. You process the
3988480	3992960	information in that one latent space using knowledge graph and large language model in this
3992960	3998960	loop. Okay, so knowledge graph can ground the language model in consistency. Language model
3998960	4004400	can make the knowledge graph more expressive. Okay, and not as rigid as before. I mean, maybe you
4004400	4009680	can use embedding based knowledge graphs like then you can make the knowledge graphs like express a
4009680	4015360	lot of things more than just text alone. Okay, so this is one idea. You can use different techniques
4015360	4020560	to process it like graph theory networks from engineering, representational learning. Yeah,
4020560	4026560	I mean, this is just some big words, but the idea is you basically do some processing. All right,
4026560	4030880	you can use large language models to process. Or if you like it, you can process it using the
4030880	4036240	knowledge graph, which is like a graph neural network to process. Okay, or you can make the
4036240	4039440	graph neural network into text and then you can do some neural symbolic reasoning.
4040240	4043760	Okay, actually, this whole thing can be just summarized as neural symbolic reasoning because
4046000	4051920	plus the knowledge graph equals the symbol spot. And then the large language model is the neural
4051920	4056800	networks. So you can just summarize this whole thing as neural symbolic reasoning. All right,
4056880	4063200	then you can use this for different domains. Right, I think this is a very, very exciting path
4063200	4068960	that we should work on. Because right now with the power of language models, the knowledge graph
4068960	4074560	can be very, very flexible. And it's not a typical knowledge graph anymore, it can be
4074560	4080000	embedding based knowledge graph. And it can be context dependent knowledge graph. Okay, so
4080720	4084720	I really hope to work on context dependent knowledge graph, because I think that's the future.
4084720	4088640	Okay, not the traditional knowledge graph that you've seen everywhere in this presentation.
4089280	4093200	Okay, the knowledge graph embeddings must be able to do, must be able to
4094160	4098320	change based on the parent nodes. Okay, must be changed based on the context. And that's
4098320	4101920	something that is not done right now, at least based on my own awareness. I don't think that's
4101920	4108240	done right now. But that's very promising. Right, so one use case for this kind of system is fact
4108240	4113760	checking. As you know, large language models cannot do very badly at fact checking. It tends to
4113760	4120160	hallucinate a lot. And perhaps we can do like, a knowledge graph to like ground it in some facts
4120160	4128240	like some Wikipedia entries. No, you can use this to ground the inference. Okay, by doing inference,
4128240	4133360	you can then see whether or not like, is it, is there a path in the knowledge graph that matches
4133360	4139760	it? Or you do knowledge grounded inference, like you say, you must only use this information
4139760	4144880	that I extract for you in the knowledge graph and infer it. So this diagram here, unfortunately,
4144880	4149920	did not do the inference step. Okay, because they are still using birds. Okay, they're using bird as
4149920	4157840	a model. And what they did was they use the knowledge graph relations to do some pre training.
4157840	4164080	So it's like they take additional, like, additional tech samples, they just mask out certain words
4164080	4168880	based on the knowledge graph relations. And then they do the training here. So they just did the
4168880	4175360	pre training using the knowledge graph to give additional examples. Okay, so what I want the
4175360	4181280	thing to do is actually to do it during inference, if I cannot find any paper that does that so far.
4181280	4185440	All right. So I think this inference is more important than the pre training, you know,
4185440	4189360	this pre training, yes, it increases more data samples, because you can just mix and match the
4189360	4194240	knowledge graph, get more sentences out. Sure, I give it to you. And in fact, they improve by two
4194240	4199040	to three percentage points across soda benchmarks, this fact KB, you can go and check it out.
4199680	4203920	All right. But what I'm more interested in is how you use it for inference, not for pre training.
4204560	4209680	Okay, so let's see how length chain does it. All right, so now we come to the length chain part.
4209680	4213360	So actually, length chain is quite advanced, because length chain has a lot of the ideas that I
4213360	4218400	think should be done. All right, let's see how the length chain question answering graph question
4218400	4223520	answering is done. All right, so we have four steps. First step, we generate the triples from
4223520	4227760	the context. Okay, so we are like maybe a text context, you generate like the triples from it,
4228400	4233760	like the knowledge graph triples, you generate some from the query, you generate some entity
4233760	4239200	extraction. Okay, and then you use this at that entities to extract this relevant triples. Okay,
4239200	4244400	later I'll show you the how what I mean by this. And then you use this relevant triples to answer
4244400	4248640	the question. So I share with you these two documentation in case you want to see how length
4248640	4257360	chain graph QA does it. So step one, okay, generate triples from context. So like this context, I just
4257360	4263760	came out of it, right? Recently, my MacBook external camera in the viewing camera spot. So I'm
4263760	4269760	actually using the external camera right now to talk to you. And yeah, so this example is for Apple.
4269760	4274960	So let's assume that Apple created a new product called Mac and Cheese Pro, okay, in 2025. All
4274960	4279600	right, and then like Apple gave the invented cheese, okay, a rousing ovation in 2026 after
4279600	4284160	invented this in 2024. Right, there's also another company called Orange who created a competing
4284160	4288800	product called the Orange and Cheese Pro. The price was slightly higher at 5000 compared to 4000
4288800	4294880	from Apple. Okay, so this is a fictional example. Okay, and this is just to see like how good the
4294960	4299680	context is stored in the knowledge graph. So you can see that, oh yes, Apple announced Mac and Cheese
4299680	4307280	Pro, Apple gave cheese. So this kind of thing, right, like, is a bit contentious because like,
4307280	4311520	what do you mean by gave cheese, gave what? So this one needs to be improved a bit.
4312400	4317760	Apple ovation gave, Apple gave an ovation, okay, again to who? Right, so this one needs to be
4317760	4323040	improved as well. Okay, the price of the MacBook Pro is 4000. Yes, Mac and Cheese Pro is already
4323120	4329120	created. Orange and Cheese Pro, good. Orange and Cheese Pro, the price 5000. Okay, so you see,
4329120	4336000	it's not bad. Miss out dates. All right, and then like some, some relations are ambiguous.
4336880	4341280	So I don't quite like the way they did the triplet extraction and I think this is the
4341280	4345920	downfall of the Graph QA. So if you are going to use Lang chain for Graph QA, my advice is don't
4345920	4350800	use it. Okay, because you miss out a lot of stuff in the context. Okay, if you are interested
4350800	4354960	how they generate the context, you can go back to my earlier slides that I was talking about.
4355680	4360240	Yeah, so I mean, it's actually, let me show you, let me show you again the slides.
4361280	4366160	It's this one, this is the one that they did, like, this is the problem to generate stuff from the
4366160	4372640	text. Yeah, so the examples aren't very great and understandably the results aren't very great
4372640	4377440	as well. All right, so this is the knowledge graph that's generated. You can see like Mac and Cheese
4377440	4384400	Pro is cost $4,000 on price. Yeah, you can see that like stuff like price will contain like a lot
4384400	4391760	of relation because like price is very generic. Okay, Apple announced Mac and Cheese Pro. Okay,
4391760	4398240	so this is the knowledge graph that is generated. And we can see that like, next up we can use the
4398240	4404160	Graph QA chain in order to run the chain and see the answer. And you can see that if I ask it the
4404160	4409040	question like, when was the Mac and Cheese Pro announced? Okay, they couldn't find it. Okay,
4409040	4413840	because after they passed through the context, okay, they abstract like when was the Mac and
4413840	4417600	Cheese Pro, when did Apple announce the Mac and Cheese Pro? They abstract that in the query,
4417600	4422400	there's only Apple and Mac and Cheese Pro. So they check through all the knowledge graph to make sure
4422400	4428080	that you only have entities that match Apple or Mac and Cheese Pro. Okay, so I have a gripe with
4428080	4439280	this thing. Like if you use exact text matching, what if there's a spelling error, capitalization
4439280	4449360	error, or like related word, but not exact match. Yeah, so if you use exact text matching, which
4449360	4454880	is what they did for a length chain, like what if you don't get the right match? Okay, so I don't
4454880	4461040	quite like this approach. So yeah, this is something that I think could be improved on.
4461040	4465840	All right, and you can see that if I ask it like, when did they announce the Mac and Cheese Pro?
4466640	4470960	They couldn't answer. All right, because look at this knowledge graph here, there's nothing that
4470960	4475200	talks about dates here. All right, so they miss out quite a huge chunk of information from the
4475200	4481440	earlier context. So if we had fed in the earlier context directly, so I just use the length chain
4482400	4489600	LmChain agent. Okay, so I'm only using the LmChain agent or this to just show you that length
4489600	4495280	chain is not good. All right, I myself the new length chain. All right, so this is the idea
4495280	4500720	that like, after a while, you know, this is the context and then, okay, so this is not bad. I
4500720	4504000	mean, you could just do the same thing on ChatGBT, actually, you can just put like context
4504000	4509280	question and then ChatGBT will give you the answer. All right, so this LmChain works and this shows
4509360	4514720	that by embedding the text as a knowledge graph, it kind of miss out certain stuff. All right,
4514720	4519840	and what are the stuff we miss out? We miss out the years and we also miss out like,
4519840	4523760	Apple gave cheese. I mean, it doesn't make sense that way, right? I mean, look at the knowledge
4523760	4531520	graph like, what am I trying to solve there? Apple gave cheese. Where is it? Apple gave cheese.
4531520	4535760	Where's cheese?
4539280	4541760	Apple Cheese gave. Is there a gave anywhere?
4545600	4550640	Yeah, okay, I think this one maybe is the outdated, but the idea is that we can't really tell the
4551360	4558000	main thing in this graph because we miss out some information. And that's one of the issues of
4558560	4562560	converting text directly into knowledge graph is that you might miss out certain relations.
4563200	4566400	And actually, if you think about it, if we want to embody all relations,
4567200	4572960	there's just too many to embody, right? Yeah, it's too big to embody. So maybe the text itself
4572960	4577760	is way more expressive than the knowledge graph, if you think about it that way. Okay,
4578560	4582400	but again, you know, if you just use text only, you might face issues that, you know,
4582400	4590720	your OpenAI embeddings might be too restrictive. It's too broad-based. You need the embeddings
4590720	4598080	at different levels. So let's see how we would improve the Lang chain graph QA. I'm just using
4598080	4603600	my strict JSON framework here, which just basically passes the system prompt and then outputs as a
4603600	4611360	JSON in your own way. So I basically did what Lang chain does in a much shorter way. So I just
4611360	4615440	say you are a knowledge graph builder. You extracted an object one, object two relation.
4615440	4620080	Okay, I did not even put subject, object predicate. Okay, I mean, I just do like that. Okay, I just
4620080	4625440	want it to be as vague and as generic as possible because I want to capture as much information
4625440	4630080	as possible. Okay, so this was done in like 10 minutes. Okay, I don't really know whether this
4630080	4634960	is the best. You all can feel free to improve it. Okay, I have the Jupyter notebook attached in the
4634960	4640560	link. All right, so I gave you some examples like John bought the laptop. Okay, that's me,
4640560	4645680	all right. John built the house in 2021. Okay, that's not me, all right. But this is the idea of
4645680	4650720	like how we can represent like various relations like that. All right, then the output format is
4650720	4654080	just a knowledge graph. So you can see like Apple announced my and cheese bro, my and cheese bro
4654080	4659600	announced in 2025. Apple proved big hit. Okay, so again, this one is not exactly that great,
4659600	4663520	because it's not really Apple that prove a big hit. It should be the Mac and cheese bro that
4663520	4668160	prove a big hit. So this part needs to be from engineer a bit more. All right, Apple gave cheese.
4668160	4674800	Okay, again, like this is not complete. Okay, cheese browsing ovation into zero to six. So
4674800	4679120	actually we combine these two together. This is complete. So this is okay. All right, cheese
4679120	4683120	invented man cheese bro. Okay, man cheese bro invented into zero to four. Okay,
4683840	4688640	orange created orange and cheese bro. Yep, orange and cheese bro. The price is 5000 and
4688720	4694800	Apple prices 4000. So again, here has some issues or so, like here, instead of saying that this is
4694800	4698720	a Mac and cheese bro, because we should be referring to man cheese bro, it says Apple.
4699440	4704960	Okay, so unless we can sort of like link this later to Apple announced, okay, this part here.
4706720	4711840	So now you can see some issues with knowledge graph expressing stuff. It is not clean. All right,
4712640	4718960	it might truncate the information halfway. So this one needs more study as to how we can
4718960	4723760	express this in the knowledge graph better. But by expressing it in the knowledge graph,
4723760	4728960	you are able to then do knowledge graph passing, okay, and extract out the relevant entities that
4730320	4734880	related to the prompt. And you know, this is like, if you think about it, this is like
4734880	4740480	doing segmentation across like every few words in the segment one time. Yeah, so this is the
4740480	4746000	generated graph of what I did for strict design framework. You can see that compared to Lang chain,
4746000	4754960	this is what happens like we have way more relations, there's more relations here. And dates are captured.
4757200	4762160	Yeah, so this is something that I think needs to be investigated more. Mine is not the best,
4762160	4766800	but Lang chain is definitely not good. Okay, so this is something that needs to be done more if
4766880	4772240	we want to extract stuff out into the knowledge graph. And then like, should we use embeddings?
4773840	4781280	So if you want to use embeddings, then we cannot just use OpenAI API. Maybe you need to use like
4781280	4788400	Lama2. Okay, although Lama2 perhaps is not that great or so, because Lama2 is
4790000	4795440	not that good for multilingual. Okay, but Lama2 is the best possible substitute for
4796240	4800800	chat GPD right now. So maybe you can construct a knowledge graph embeddings using the Lama2 embeddings.
4801840	4808960	So food for thought. Next, we have this flexible knowledge graph passing. Over here, what I decided
4808960	4817440	to do is that we want to output only relations that are relevant to the question. And I just
4817440	4823040	passed in the entire knowledge graph here. So instead of coming up entities, I just asked it
4823040	4829360	to go through the entire knowledge graph because in case of words not exact or spelling errors,
4831120	4840000	GPD is able to catch it most of the time. I must copy it because GPD is not as great as
4840000	4844720	doing like counting letters and stuff. But if you misspell your words, but the meaning is about
4844720	4850000	that, GPD is able to extract the right entities. And here we can see that we asked it like,
4850000	4854880	when did Apple announce the man cheese bro? It captured exactly what we want. All right.
4856000	4862080	And this is the graph that is the past knowledge graph. So I'm talking about when you query the
4862080	4865360	knowledge graph, you pass it so that only relevant sections of the knowledge graph gets
4865360	4871840	come out, gets extracted. You ground this extracted part onto your text. Okay. And then
4872640	4878960	you can get the answer here. So 2025. So I just shown that like using this strict JSON format,
4878960	4883600	you are able to like, it's very flexible. You just need to key in the system prompt,
4883600	4887440	key in the user problem and output the format in terms of whatever JSON labels and the
4887440	4892800	description of the JSON. So I've been using this for a lot of my own use cases. And I'm just
4892800	4897680	adapting this for the knowledge graph. But this is really cool because you can then use this
4898240	4902560	past knowledge graph, like this idea of generating the knowledge graph and passing the knowledge
4902560	4910560	graph. You can use the knowledge graph as memory. Okay. And then you can update memory.
4911120	4918240	And you can use updated memory to extract relevant parts. Okay. So this called retrieval.
4919040	4925520	Okay. Use relevant parts to solve problem. So I really like this framework because this
4925520	4931840	knowledge graph as memory thing is something quite interesting. But how can we express it as memory?
4931840	4939360	That's the difficult part. Okay. So Richard asked, have I tried putting the graph into an FAIS
4939360	4947040	index? No, I haven't. But how will you do a knowledge graph putting onto the, like onto that
4947040	4952480	index? Because usually what I know is that you do the embedding and then you put the text. That's
4952480	4958480	for the retrieval or method generation. If you're doing knowledge graph, maybe you put the source
4958480	4963760	as the index. Okay. I'm not too sure. I'm going to check on this. Like how will you do this into
4963760	4968800	PyCon and stuff like that? But what I can imagine you doing for the knowledge graph is just put the
4968800	4974960	whole thing into some array and then just store the array. I mean, you can even put it as a JSON.
4974960	4985120	Yeah. So yeah. Okay. I don't have time to cover through the running of the Jupyter Notebook.
4985120	4990400	I'll just upload that separately. It's another video. But let's just go through like the last
4990400	4994400	five to 10 minutes. I'm okay to extend about 15 minutes if you all have more things to discuss.
4995120	5000080	Like we have discussed like how can we use knowledge graph better for last language models?
5000080	5004880	So first question, what are the failure modes of using knowledge graph for context representation?
5005840	5012960	And I think this failure mode is mainly like your knowledge graph may not capture all information.
5015200	5017680	Okay. And also the knowledge graph capturing
5020320	5025920	might truncate the information. So maybe using text directly
5028000	5034560	may be better. Okay. But harder to pass. Because if you are using text directly, you don't really have
5036400	5042160	like nice sections where you can pass the knowledge graph on. Yeah. So these are some of the things
5042240	5045440	about right now, some of the failure modes of this knowledge graph.
5046560	5047840	Anyone else has anything to add?
5049440	5058480	Just some random thoughts. Do you think it makes sense if we view the embedding space
5058480	5061360	itself as a form of generalized knowledge graph?
5063680	5067440	Embedding space as a genera. You mean the LM embedding space?
5067440	5075520	Oh, I mean, I mean, yes, but for some specific tasks you want to do, you can train a dedicated,
5076080	5086800	a separate dedicated embedding space. So because like you have all your entities inside their space
5087520	5092960	and the relative, I don't know, relative positioning of them kind of encode certain
5093200	5098000	relative information of them, right? Because I think the issue you mentioned here is,
5098960	5106640	I think it's just the graph, learn graph can be too sparse, right? You lose a lot of information.
5107600	5114800	But if inside embedding space, I don't know, it might help preserve more information,
5114800	5119040	although not very explicit information. It's just some random thought.
5119920	5124640	Ah, okay, I get what you mean. Like you encode knowledge graph as embedding space,
5125360	5132720	so like your source relation and the output are all embedding space and sink in destination.
5134640	5142240	I feel like my gut feeling is it can be much richer than just a typical traditional graph.
5143120	5145120	Definitely.
5147520	5151280	Now I kind of agree with you. It's just going to be hard to express the embedding space
5151920	5157040	using an OpenAI API. You might need to have access to the last average model directly if you want to do this.
5159600	5162880	Yeah, but definitely that's one of the ways that we can represent knowledge graphs.
5164880	5170560	Anyway, this is the second question also. Should we utilize the embedding space? Perhaps
5172320	5178880	for more expressive knowledge graph. Okay, but then if you think about like what I was talking about earlier,
5179760	5180640	context dependent
5183920	5187680	embeddings. If you are talking about context dependent embeddings, actually we can use
5187680	5195840	LLM to pass an update embeddings based on the parents of the node.
5196720	5201280	Yeah, so I was thinking of something like that. Like you can actually have a very, very
5203440	5208400	different interpretation of a certain word. Like for example, bank can be river bank or financial
5208400	5213280	bank, depending on like the context of it. If you are talking about river side, then it's like river bank.
5213280	5218160	Yeah, so you can actually use the last language model, extract out the hierarchy of the graph,
5218160	5223680	the front part of the graph. You can put it there and you can then pass the embeddings accordingly.
5224560	5228800	So I'm still thinking that perhaps just using the last language model embedding directly
5230160	5233680	might be a better bet. And then you can just maybe
5235520	5240880	use last language model to do this context thing. And then you can put this embeddings inside
5240880	5246320	your knowledge graph. Like what you said earlier. If there's a way to get the embedding space
5246320	5250800	directly from the OpenAI API, that would be great. But if not, we might have to use
5250880	5253680	Lama2 in order to do this embedding space knowledge graph.
5256320	5259600	But then again, is it really necessary? Can we just use text?
5259600	5264240	So this is a big open question. Should we use embedding space for the knowledge graph?
5264240	5270080	Or can we just represent it as text and then use the LM to generate embeddings after that?
5270880	5276480	So I leave that as an open question. I think both approaches are valid approaches. I just feel like
5276480	5281120	the way to input the knowledge graph as text is it will be much more interpretable. And also
5281120	5286320	you only need to train one embedding space, which is the LM embedding space. So I kind of prefer that.
5290240	5292080	Anyone else has any things to add?
5296400	5300640	Okay, if not, we go to the next question. Can LMS help with a more flexible
5300640	5305760	interpretation or construction of a knowledge graph? Okay, so our answer first. I think yes,
5305760	5315440	definitely. Just like compared to like spacey or like on noun, verb, pro verb, those kind of
5315440	5324480	stuff, like you are doing like the parse tree, compared to those very, very flexible. And you
5324480	5329040	are able to extract a lot more information. So like just based on the string, some prompt I
5329040	5334560	showed you earlier, you just hit object relation object, it captures almost everything. And that's
5334560	5339360	zero shot prompting. Granted, it did not capture the date at first. I had to use the examples to
5339360	5345520	give you the date. But compared to using this kind of like spacey and so on, like deep learning
5345520	5351440	approaches, like you'll take quite long to train a new kind of like knowledge graph constructor.
5351440	5355120	But with large language model, you can just use prompt engineering and get your knowledge graph
5355120	5362400	out. I think that's very exciting. Okay, last question. How do we know what nodes are important
5362400	5366720	to construct in the knowledge graph? Okay, because there's a lot of information, but not
5366720	5374480	everything is needed for your use case. How do we know? Okay, so my opinion, okay, my opinion is this.
5376000	5383520	You need to have biases based on the domain. And what are these biases? Maybe you can have
5383520	5391360	multiple biases. Okay, and then let's just choose the right biases later. So this is my idea of
5391360	5396880	intelligence right now. Okay, I'll share with you. Okay, this idea of intelligence is that there's
5396880	5402160	not just one abstraction space where you store your information, you store them in multiple
5402160	5408800	abstraction space. How do we get all these abstraction spaces? We basically just do rule
5408800	5413440	based abstraction, like you like maybe one domain is saying that oh, dates are important. So I store
5413440	5418080	the dates. Another domain is like all people, all person's names are important. I store the person's
5418080	5422880	names. Then maybe another domain will be like all places are important. I store the places.
5423680	5428720	Then when you want to solve the problem, okay, you will see which space is the best
5428720	5432880	for your problem. Okay, you will just like, maybe you look at all the abstraction spaces,
5432880	5437360	maybe combine two or more, or you just take one, and then whatever solves the problem works.
5438000	5445920	So this would form an approach that will be used later on. So if you think about it,
5445920	5450400	I'm just going to draw it here. Okay, I don't know whether I have space to draw it, but if you
5450400	5454800	look at the top right of the screen, okay, you have a problem, you have multiple abstraction
5454800	5459200	spaces, let's call this A. You have another abstraction space here, let's call it B.
5460720	5465680	And we have another expression space, we call it C. All right, so if we have three
5465680	5471200	abstraction spaces like that, okay, these are like three ways of doing it, three ways of doing the problem.
5471520	5480000	And then in order to solve any arbitrary problem later, you just take mix and match the abstraction
5480000	5486480	spaces to solve the problem. So yeah, increasingly, I've been feeling like this is the way to do things.
5487040	5499920	So yeah, I also use this for my abstraction reasoning copless paper. So this is the idea
5499920	5503600	that I have right now. You have different abstraction spaces. All these are rule-based.
5504320	5508400	Okay, we don't really have deep learning here, because if you have deep learning, you'll have
5508400	5513680	problems in getting a fixed abstraction space. You don't want the abstraction space to change.
5514240	5517600	Because if you change this abstraction space, you have to change whatever you learn on it.
5518160	5522400	It's like if I suddenly told you that math, the addition is now subtraction,
5522400	5526480	that I have to relearn all my math again, because I need to update that new knowledge.
5526480	5530800	So I'm saying that the basis is fixed, but then you just choose the right basis to solve it.
5531440	5536080	Okay, then you might ask me, if we do it like that, what if we don't have the right basis to
5536080	5541440	solve the problem? Okay, then the answer is you can't solve the problem. Okay, which might sound
5541440	5546880	a bit crude to people, but I feel like we can't solve everything. Like even humans, we have our
5546880	5553280	limitations. It's just that we work around our limitations and try to use our existing biases
5553360	5557120	to solve new problems. And I think that's intelligence. We don't really need to change
5557120	5561360	the abstraction spaces. We can just work with getting multiple abstraction spaces and then
5561360	5567440	just combining them. So I shared a bit about my view of intelligence here. And yeah,
5568560	5573760	that's more or less it for the questions to ponder. Anyone has anything else to add for any of these
5573760	5579760	questions? I just want to clarify one thing. I'm still not very sure about the motivation here.
5579760	5588080	So why we want this knowledge graph? Why use a knowledge graph, is it?
5590160	5594400	Because the internal representation of LLMs is already
5595200	5611760	already richer than a knowledge graph. So for example, you can just use the
5612720	5620400	last language model to help you to break down the task. Actually, because the way I did it before
5620400	5625360	is I want to specifically construct the knowledge graph first. Then I use that knowledge graph to
5625360	5631440	break down the task from a hierarchical or sequential learning of intermediate
5631440	5638160	goal to RL. But in the sense, you don't really need that particular task at least,
5638160	5642000	you don't really need to do that anymore because you just query the last language model.
5643920	5649360	At least the common sense way of breaking down a certain task. So you already have the
5650160	5658480	different sub tasks. So because my understanding is more coming from that perspective,
5658480	5662560	so I'm thinking why do I still need the knowledge graph?
5664400	5667920	I mean, as what I said in one of the first few slides, if you use retrieval or method
5667920	5672880	generation, you might miss out certain chunks of text. Knowledge graph provides a sort of hierarchy
5672880	5677600	that you can pass over and extract information. So it provides the structure.
5678560	5683920	But you still mentioned the problem is like even with knowledge, you have to
5683920	5690240	distill information. So there is inevitable that certain information are lost. So it be
5690240	5696240	it in a way of like you do chunk first, chunking first, then you summarize each chunk.
5696240	5699760	Then you summarize again, maybe like a hierarchical way of summarizing and all
5700480	5708320	even your knowledge graph, you only extract that the relation you think is important between
5708320	5715760	different entities. So you face this issue of having information lost.
5720960	5725760	Yeah, so maybe the correct way is not the existing kind of knowledge graph,
5726640	5731440	but a new kind of knowledge graph. But I do believe a graph based representation of knowledge is
5731440	5736400	important because when you learn new things, we typically try to fit in with our existing
5736400	5740800	knowledge and we build on the knowledge from there. So if you have some form of graph structure
5740800	5746240	to represent knowledge, you can actually like use that for learning as well. And that's where
5746240	5755440	I'm coming from. My intuition is it can be okay. So in that case, I can see how it can be
5755440	5761520	useful. It can be served as a heuristic for search. So if you want to understand a very
5762400	5770480	large chunk of text, if you extract like a rough graph representation of the information,
5770480	5777040	then you just do a heuristic search based on that. So even if this information loss is still can
5778000	5782880	expand upon just based on your existing knowledge graph, maybe it just gave you
5783520	5787600	some useful signal to tell you like which part of the text you want to do some search,
5787600	5791920	then you can just go and search. You don't need necessarily to stick with the strictly stick
5791920	5796800	with the graph. So in that way, like just a heuristic but it's a useful one.
5797600	5803520	Yeah, yeah. That's one way of doing it, like using a heuristic to search. So it's like replacing
5803520	5808080	the cosine similarity in retrieval of mental generation. You just pass through the knowledge
5808080	5814960	graph. Yeah. Because the chunking in like just naive way, it's just, it's just like because the
5814960	5824720	way we structure essay or structure the text is not necessarily just like each different
5825760	5831120	hierarchy like different object or different something that is quite intricate, right?
5831120	5836640	If for example, for some novel, you have foreshadowing, you have different way of writing,
5836640	5841680	you have plot device, then you suddenly at a certain very, you think apparently very random
5841680	5846160	point suddenly become very important. So just by naively chunking into like evenly
5847840	5853280	it doesn't, it doesn't work. But if you have a graph like sort of tells you this kind of structure,
5853280	5859280	then you're based on that to do some similarity. Definitely I feel like it can be more efficient.
5859280	5866240	So I think they give you a better way of doing chunking. Yeah. Yeah, I like this approach. In
5866240	5872000	fact, that's one of the motivations of using knowledge graph as well in order to find a better
5872000	5878720	way to pass it. I mean, we can pass through graphs pretty well. So yeah, we can perhaps get better
5878720	5883040	retrieval using knowledge graphs. So that's one provided you have all the information in your
5883040	5887360	knowledge graph, you can get better retrieval using knowledge graphs provided the first part
5887360	5892480	again, because that's the failure case we saw just now in the length chain graph answering agent.
5892720	5898800	But anyway, I think all the logition all stand from the fact that the context window is very limited.
5898800	5907200	So if we can solve that problem, then here actually, I mean, both way either way.
5907200	5911680	Yeah, I'm quite excited about this. In fact, I will spend like the next few weeks trying to
5911680	5916560	create a new knowledge graph. So I'll share with you after I create it, because there needs to be
5916560	5921120	some context dependent passing. And that's lacking right now in the knowledge graph that I see in
5922080	5928000	so far. You agree with the context dependent passing, right? Like how you interpret a certain
5928000	5932960	like note actually depends on the parents or depends on the position in the graph.
5934800	5939840	Yeah, even from a very traditional perspective, this very crucial as well.
5942960	5948240	Nice. Okay. Last minute or so anyone has any last points you want to add?
5952080	5957840	Okay. If not, thanks for coming. And yeah, if anything, you can still reach out to me on Discord
5957840	5964800	or LinkedIn. Yeah. And I'm looking forward to do this linkage between like large image models
5964800	5969760	and knowledge graph. This is what a lot of people call a neural symbolic. And I think this will be
5969760	5976080	very crucial for intelligence. And I can see how we can use this knowledge graph approach to like
5976160	5981360	learn stuff from the environment and use it in a learning agent. Like I like to call it reinforcement
5981360	5985680	learning agent, but it's not really reinforcement learning because there are no rewards. Okay. You
5985680	5991680	can just learn directly from knowledge in the memory itself. So I think this will be very crucial
5991680	5995760	for that kind of framework. And yeah, hope to share more of your after experiment with it.
5995760	6001760	Okay. If not, yeah. Thanks for coming. And I'll see you around. Okay. Bye, friend.
