1
00:00:00,000 --> 00:00:05,680
A mathematical saying goes, a drunk man will find his way home, but a drunk bird may get

2
00:00:05,680 --> 00:00:13,040
lost forever. The assumption here is that because both are drunk, they are doing random walks.

3
00:00:13,040 --> 00:00:18,320
The difference is that the man can only walk on the surface of the earth, and so he is doing a

4
00:00:18,320 --> 00:00:24,320
two-dimensional random walk. But for the bird, in addition to the two dimensions, it can also

5
00:00:24,320 --> 00:00:31,920
fly up and down, and so it is doing a 3D random walk. It turns out that no matter where the

6
00:00:31,920 --> 00:00:39,200
drunk man started, it is mathematically guaranteed that he will return home, it's only sooner or

7
00:00:39,200 --> 00:00:46,800
later. But for the bird, there is a non-zero chance that it will get lost forever, even if it started

8
00:00:46,800 --> 00:00:53,920
off in its own nets. Why is there such a stark difference between a 2D and a 3D random walk?

9
00:00:54,480 --> 00:00:58,320
To answer this question, we use the framework of Markov chains.

10
00:01:02,480 --> 00:01:08,480
A Markov chain consists of three things. The first is the state space, essentially meaning

11
00:01:08,480 --> 00:01:14,800
where you can go. In the case of random walks, for simplicity, the state space would be just

12
00:01:14,800 --> 00:01:22,160
those lattice points, so the drunk man or bird can only visit those lattice points. Second thing

13
00:01:22,160 --> 00:01:28,400
is the transition probabilities, because Markov chains are not static. Let's say you are now at

14
00:01:28,400 --> 00:01:35,760
state A, with four other neighboring states in the state space. In your next step, you can transition

15
00:01:35,760 --> 00:01:42,880
to any other states, and in general, you might also go back to itself, depending on your setup.

16
00:01:43,680 --> 00:01:50,480
For each possible transition, we just assign a probability to it. They are the transition

17
00:01:50,480 --> 00:01:58,160
probabilities. For a 2D random walk, each state has four neighbors, and because we have no bias

18
00:01:58,160 --> 00:02:05,840
towards any direction, the transition probabilities are all one quarter. Similarly, for a 3D random

19
00:02:05,840 --> 00:02:12,400
walk, each state has six neighbors, and so the transition probabilities are all one sixth.

20
00:02:13,280 --> 00:02:19,760
The final thing is the initial distribution. In a general Markov chain, you have the freedom to

21
00:02:19,760 --> 00:02:26,880
choose which state to start, and so we can assign a probability to each state, indicating how likely

22
00:02:26,880 --> 00:02:33,520
you are to start at that state. In the case of a random walk, we want to definitively start at

23
00:02:33,520 --> 00:02:41,520
the origin, i.e. a probability 1 of starting at the origin, and 0 everywhere else. Wait,

24
00:02:41,520 --> 00:02:47,360
didn't you say it doesn't matter where you started for the 2D case? Don't worry, I'll get there in

25
00:02:47,360 --> 00:02:54,240
a minute. These are the three elements in the Markov chain, but there is one key feature that

26
00:02:54,240 --> 00:03:01,360
makes it a Markov chain, the Markov property. This is a simple idea that once you have gone to a

27
00:03:01,360 --> 00:03:08,320
particular state, you should have already forgotten how you get there, and consider the transition

28
00:03:08,320 --> 00:03:15,040
probabilities from there without caring about the root that takes you there. So yes, that's the whole

29
00:03:15,040 --> 00:03:26,080
setup for a Markov chain, but how does that help? Given that a random walk is random, and every time

30
00:03:26,080 --> 00:03:32,640
you run it, it gives different results, what can we say about that? One thing we can say is whether

31
00:03:32,640 --> 00:03:39,200
you will revisit the origin if you started there. If you are guaranteed to go back, or in other words,

32
00:03:39,280 --> 00:03:46,880
the return probability is 1, then we call that initial state recurrent. If not, then you are not

33
00:03:46,880 --> 00:03:54,160
guaranteed to return, and so the return probability is less than 1. In such a case, we call it a

34
00:03:54,160 --> 00:04:02,240
transient state. Because you are either guaranteed or not guaranteed to return, a state is either

35
00:04:02,240 --> 00:04:10,320
recurrent or transient, there is no third option. It turns out that in 2D, the origin is a recurrent

36
00:04:10,320 --> 00:04:16,640
state, and just because this state is recurrent, we can already infer that it does not matter

37
00:04:16,640 --> 00:04:22,880
whether drunk man started, he will go back to the origin. That takes one or two lines of reasoning,

38
00:04:22,880 --> 00:04:29,520
which I encourage you to do in the comments. On the other hand, the origin in the 3D random walk

39
00:04:29,520 --> 00:04:37,200
is a transient state, so the drunk bird may never return. The problem now is how to determine that

40
00:04:37,200 --> 00:04:44,240
return probability. Here's the trick. We consider a quantity v, which is the number of returns to

41
00:04:44,240 --> 00:04:51,200
the origin. The implicit assumption is to run the random walk to infinity, even if you have returned.

42
00:04:51,840 --> 00:04:58,000
Our focus is the expectation of v, i.e. the expected number of returns.

43
00:04:58,720 --> 00:05:04,960
Now for the recurrent case, we are guaranteed to go back, but if we continue to run the random walk

44
00:05:04,960 --> 00:05:11,840
by the Markov property, we should have forgotten that we have returned, and then we will for sure

45
00:05:11,840 --> 00:05:20,480
return to the origin again and again. So the number of returns v is guaranteed to be infinite,

46
00:05:20,480 --> 00:05:27,840
and the expectation would also be infinite. That is when the state is recurrent. What if the state

47
00:05:27,840 --> 00:05:35,680
is transient? Here is a very clever general trick. By definition, the expectation is zero times the

48
00:05:35,680 --> 00:05:42,560
probability that v is zero plus one times the probability that v is one and so on. But what if

49
00:05:42,560 --> 00:05:50,480
we write, for instance, this two times probability as actually the sum of two copies, and also do

50
00:05:50,480 --> 00:05:58,160
this similarly for the other terms in the sum. Then instead of summing these row by row, we can

51
00:05:58,160 --> 00:06:05,600
sum these column by column. For the first column, it is exactly the probability that v is at least

52
00:06:05,600 --> 00:06:12,560
one. Similarly, the next column gives the probability that v is at least two and so on.

53
00:06:13,360 --> 00:06:21,040
The reason this method is useful is that if v is at least one, that exactly means you return to

54
00:06:21,040 --> 00:06:27,920
the origin at some point, and so this is actually the return probability. And for simplicity,

55
00:06:27,920 --> 00:06:35,760
we denote it as r. What about the probability of returning at least twice? Well, the probability

56
00:06:35,760 --> 00:06:42,240
that you will return is r, but again by the Markov property, you should forget that you have returned

57
00:06:42,320 --> 00:06:49,600
and so the probability that you return again is to multiply by another r. And in general,

58
00:06:49,600 --> 00:06:57,520
the probability of returning at least k times is r to the k. So if we go back to the expectation,

59
00:06:58,080 --> 00:07:05,120
each column actually sums up to a power of r, and the expectation is a geometric series.

60
00:07:05,760 --> 00:07:11,520
So we know what this should sum to if that return probability is less than one,

61
00:07:11,520 --> 00:07:18,480
which is precisely the case when the state is transient. For now, we only need to know that

62
00:07:18,480 --> 00:07:26,160
this is finite if r is less than one. However, we have also deduced that for a recurrent state,

63
00:07:26,160 --> 00:07:33,840
this expected number of returns is infinite. So if the state is recurrent, we have the expectation

64
00:07:33,840 --> 00:07:40,960
to be infinite, and if the state is transient, the expectation is finite. Because the state is

65
00:07:41,040 --> 00:07:48,000
either recurrent or transient, no third option, if say the expectation is infinite,

66
00:07:48,000 --> 00:07:55,840
the state could not possibly have been transient, and so we can infer that the state is recurrent.

67
00:07:56,640 --> 00:08:03,600
So if we know whether this expectation is infinite, we are done. To do that, we need a final trick.

68
00:08:04,400 --> 00:08:09,680
Another way of thinking about v, the number of returns, is that it is a tally.

69
00:08:10,240 --> 00:08:17,280
We first ask the questions, have you returned at step n? If yes, then we add one to the tally,

70
00:08:17,280 --> 00:08:23,760
and if not, then we don't do anything to it. Once we answered all these questions,

71
00:08:23,760 --> 00:08:29,680
we add up the total, and we obtain v by just adding up all these plus ones.

72
00:08:30,400 --> 00:08:37,920
If we are asking for the expected value of v, we add up the expected value of the first question,

73
00:08:38,000 --> 00:08:45,920
the expected value of the second question, and so on. The expected value of these yes-no questions

74
00:08:45,920 --> 00:08:53,360
are much easier to handle. The expected value of this question would be, by definition,

75
00:08:53,360 --> 00:08:59,600
one times the probability that you answer yes, plus zero times that you answer no.

76
00:09:00,320 --> 00:09:06,320
Well, that's just the probability that you answer yes. We usually denote this probability

77
00:09:06,320 --> 00:09:13,920
as p with subscript zero zero and superscript one. The double zero denotes going from the origin to

78
00:09:13,920 --> 00:09:20,880
the origin, i.e. revisiting the origin. And the superscript just represents at which step you

79
00:09:20,880 --> 00:09:28,240
revisit. These probabilities will be the expected value for each question, and the sum of these

80
00:09:28,240 --> 00:09:36,160
probabilities gives us the expected value of v. So we now have a way of explicitly computing

81
00:09:36,160 --> 00:09:43,280
the expected value of v. Previously, we deduced that we can simply use the expected value of v

82
00:09:43,280 --> 00:09:50,800
to infer whether the state is recurrent, and now we also know how to explicitly compute

83
00:09:50,800 --> 00:09:58,240
that expected value. So once we know whether this series converges, then we know whether the state

84
00:09:58,240 --> 00:10:05,200
is recurrent or transient. This whole argument actually works for all Markov chains, including

85
00:10:05,200 --> 00:10:15,520
the random walk we are considering. In the previous chapter, we essentially devised a test for recurrence

86
00:10:15,520 --> 00:10:23,040
or transient. A 2D random walk is recurrent, so we want to prove that the series diverges.

87
00:10:23,040 --> 00:10:29,760
A 3D random walk is transient, so we want to prove that it converges. But whichever the case,

88
00:10:29,760 --> 00:10:36,320
we need to compute each term. In the case of a random walk, you can't possibly go back to the

89
00:10:36,320 --> 00:10:42,720
origin after one step, and so this probability is zero. This should not be too surprising,

90
00:10:42,720 --> 00:10:48,880
because any return paths would have the same number of steps to the left as those to the right,

91
00:10:48,880 --> 00:10:55,760
and of course, same number of steps downwards as those upwards. And so the total number of steps

92
00:10:55,760 --> 00:11:05,040
here has to be even. So this nth step probability is zero, if n is odd, and so we can just focus on

93
00:11:05,040 --> 00:11:13,120
finding the even two nth step probabilities. Let's say we want a total of 18 steps, then this is a

94
00:11:13,120 --> 00:11:20,720
specific possible return path. This specific path has a probability of one quarter to the 18,

95
00:11:20,720 --> 00:11:28,720
because each step is chosen with probability one quarter, and there are 18 steps you need to take.

96
00:11:28,720 --> 00:11:35,760
Of course, this is not the only possible path that returns to the origin after 18 steps. This one

97
00:11:35,760 --> 00:11:43,040
also does. Even though it has returned to the origin already, as long as you return at step 18,

98
00:11:43,040 --> 00:11:48,560
it still counts, because you will still answer yes in the previous argument.

99
00:11:49,200 --> 00:11:56,320
Anyway, the new path also has probability one quarter to the 18. More explicitly,

100
00:11:56,320 --> 00:12:03,360
when calculating the 18th step return probability, we just add up these probabilities.

101
00:12:04,000 --> 00:12:10,000
This part is the probability of getting a specific return path, and we then multiply

102
00:12:10,000 --> 00:12:18,000
by the number of return paths to get the overall probability. So it all boils down to counting

103
00:12:18,080 --> 00:12:24,960
the number of return paths. For a total of two n steps, a return path should have the same number

104
00:12:24,960 --> 00:12:31,680
of steps to the left and to the right, and the number of steps upwards is the same as downwards.

105
00:12:32,480 --> 00:12:40,640
Because the total number of steps is two n, we can express j in terms of i. For a return path,

106
00:12:40,640 --> 00:12:47,200
we can imagine it as a sequence of moves. Because there are a total of two n steps,

107
00:12:47,200 --> 00:12:55,280
in total there are two n factorial arrangements of these moves. However, if for example,

108
00:12:55,280 --> 00:13:02,400
we just rearrange these two left moves, then the resulting path quite literally hasn't changed,

109
00:13:02,400 --> 00:13:09,760
yet it counted as different in those two n factorial arrangements. Because there are i steps to the

110
00:13:09,760 --> 00:13:17,280
left, there are i factorial different permutations in between them, and of course the other directions

111
00:13:17,280 --> 00:13:25,280
have similar results. These interpermutations all counted as different in those two n factorial

112
00:13:25,280 --> 00:13:33,040
arrangements, yet they should have been counted the same, and so the number of return paths for a

113
00:13:33,040 --> 00:13:41,600
fixed value of i is two n factorial divided by all these factorials. So given a value of i,

114
00:13:42,240 --> 00:13:52,080
this is the number, and i can range from zero to n, and so the final total number of return paths

115
00:13:52,080 --> 00:14:01,600
needs to add these up for i ranging from zero to n. Finally, this two n step return probability

116
00:14:01,680 --> 00:14:10,240
is one quarter to the two n times the total number of return paths. That's just the case in 2D.

117
00:14:10,240 --> 00:14:17,840
What about the 3D random walk? The only difference is that transition probabilities are all one sixth,

118
00:14:17,840 --> 00:14:24,240
and there is one more pair of directions to consider. The two n step probabilities would be

119
00:14:24,320 --> 00:14:32,000
replacing one quarter by one sixth, and for the number of return paths we use a similar method,

120
00:14:32,000 --> 00:14:39,040
with the only difference being there are three pairs of directions now. So similarly, for fixed

121
00:14:39,040 --> 00:14:47,600
values of i and j, this is the number of return paths, and adding all these for i and j, ranging

122
00:14:47,600 --> 00:14:55,360
from zero to n, will give the total number of return paths. And finally, for the two n step

123
00:14:55,360 --> 00:15:02,560
return probability, we simply multiply by one sixth to the two n. But perhaps it might be

124
00:15:02,560 --> 00:15:09,280
useful to remind ourselves why we care about this. We have demonstrated that a state being

125
00:15:09,280 --> 00:15:16,400
recurrent or transient implies whether the series diverges or converges. So by knowing

126
00:15:16,400 --> 00:15:23,040
whether this series converges, we know whether the state is transient. In this chapter,

127
00:15:23,040 --> 00:15:29,520
we calculated those terms in the series with explicit expressions for the 2D and 3D cases

128
00:15:29,520 --> 00:15:37,360
respectively. And what remains is to show that the series formed by the 2D case diverges,

129
00:15:37,360 --> 00:15:44,800
and the 3D case converges. The very quick reason for it is that each term here scales

130
00:15:44,800 --> 00:15:52,160
like 1 over n, so it diverges because the harmonic series diverges. And the other term would scale

131
00:15:52,160 --> 00:15:58,400
like 1 over n to 3 halves, and this will lead to a convergent series instead.

132
00:15:59,360 --> 00:16:05,440
Originally I wanted to make it into the main video, but this is a bit too much manipulation

133
00:16:05,440 --> 00:16:11,520
of expressions, and so I ended up making it a second channel video. But I want to give a bit

134
00:16:11,520 --> 00:16:18,960
more no calculation explanation here. We can always think of an inner and an outer region in

135
00:16:18,960 --> 00:16:26,400
any dimension, but in higher dimensions there is much more space in the outer region, so once you

136
00:16:26,400 --> 00:16:34,560
have gone out, it will be less likely for you to go back in higher dimensions. The exact cutoff

137
00:16:34,560 --> 00:16:42,560
turns out to be between 2 and 3 dimensions. However, does this inner outer region explanation

138
00:16:42,560 --> 00:16:48,560
and the cutoff between 2 and 3 dimensions look familiar? If you have watched my previous video

139
00:16:48,560 --> 00:16:54,960
on Stein's paradox, then you might remember that whether the ordinary estimator is admissible

140
00:16:54,960 --> 00:17:02,240
has a cutoff between 2 and 3 dimensions, and we found out that recurrence of random walks

141
00:17:02,240 --> 00:17:08,320
also has the same cutoff. It turns out that this is not a coincidence.

142
00:17:09,120 --> 00:17:14,880
Larry Brown wrote in 1971 about the connection between these two problems,

143
00:17:15,440 --> 00:17:22,000
but this is way too involved in a YouTube video. I will put a link in the description for those

144
00:17:22,000 --> 00:17:27,760
who want to know more. Before you go, I just want to thank you for your support, because now I have

145
00:17:27,760 --> 00:17:34,480
100k subs. To celebrate this milestone, I am going to make a Q&A video. You can ask me any

146
00:17:34,480 --> 00:17:40,720
question, but do so in the google form below. And just so you are prepared for the next video,

147
00:17:41,280 --> 00:17:47,360
and you might have also guessed from these two videos, I am currently a fourth year Cambridge

148
00:17:47,360 --> 00:17:54,960
Math student. We call ourselves Mathmos for some unknown reason. So yes, be prepared for some

149
00:17:55,040 --> 00:18:00,560
Cambridge related Math video in the future. Please consider giving on Patreon,

150
00:18:00,560 --> 00:18:06,720
and thanks to the Patrons for making this video possible. As always, subscribe with the bell on,

151
00:18:06,720 --> 00:18:11,200
like, comment, and share this video. See you next time!

