{"text": " A mathematical saying goes, a drunk man will find his way home, but a drunk bird may get lost forever. The assumption here is that because both are drunk, they are doing random walks. The difference is that the man can only walk on the surface of the earth, and so he is doing a two-dimensional random walk. But for the bird, in addition to the two dimensions, it can also fly up and down, and so it is doing a 3D random walk. It turns out that no matter where the drunk man started, it is mathematically guaranteed that he will return home, it's only sooner or later. But for the bird, there is a non-zero chance that it will get lost forever, even if it started off in its own nets. Why is there such a stark difference between a 2D and a 3D random walk? To answer this question, we use the framework of Markov chains. A Markov chain consists of three things. The first is the state space, essentially meaning where you can go. In the case of random walks, for simplicity, the state space would be just those lattice points, so the drunk man or bird can only visit those lattice points. Second thing is the transition probabilities, because Markov chains are not static. Let's say you are now at state A, with four other neighboring states in the state space. In your next step, you can transition to any other states, and in general, you might also go back to itself, depending on your setup. For each possible transition, we just assign a probability to it. They are the transition probabilities. For a 2D random walk, each state has four neighbors, and because we have no bias towards any direction, the transition probabilities are all one quarter. Similarly, for a 3D random walk, each state has six neighbors, and so the transition probabilities are all one sixth. The final thing is the initial distribution. In a general Markov chain, you have the freedom to choose which state to start, and so we can assign a probability to each state, indicating how likely you are to start at that state. In the case of a random walk, we want to definitively start at the origin, i.e. a probability 1 of starting at the origin, and 0 everywhere else. Wait, didn't you say it doesn't matter where you started for the 2D case? Don't worry, I'll get there in a minute. These are the three elements in the Markov chain, but there is one key feature that makes it a Markov chain, the Markov property. This is a simple idea that once you have gone to a particular state, you should have already forgotten how you get there, and consider the transition probabilities from there without caring about the root that takes you there. So yes, that's the whole setup for a Markov chain, but how does that help? Given that a random walk is random, and every time you run it, it gives different results, what can we say about that? One thing we can say is whether you will revisit the origin if you started there. If you are guaranteed to go back, or in other words, the return probability is 1, then we call that initial state recurrent. If not, then you are not guaranteed to return, and so the return probability is less than 1. In such a case, we call it a transient state. Because you are either guaranteed or not guaranteed to return, a state is either recurrent or transient, there is no third option. It turns out that in 2D, the origin is a recurrent state, and just because this state is recurrent, we can already infer that it does not matter whether drunk man started, he will go back to the origin. That takes one or two lines of reasoning, which I encourage you to do in the comments. On the other hand, the origin in the 3D random walk is a transient state, so the drunk bird may never return. The problem now is how to determine that return probability. Here's the trick. We consider a quantity v, which is the number of returns to the origin. The implicit assumption is to run the random walk to infinity, even if you have returned. Our focus is the expectation of v, i.e. the expected number of returns. Now for the recurrent case, we are guaranteed to go back, but if we continue to run the random walk by the Markov property, we should have forgotten that we have returned, and then we will for sure return to the origin again and again. So the number of returns v is guaranteed to be infinite, and the expectation would also be infinite. That is when the state is recurrent. What if the state is transient? Here is a very clever general trick. By definition, the expectation is zero times the probability that v is zero plus one times the probability that v is one and so on. But what if we write, for instance, this two times probability as actually the sum of two copies, and also do this similarly for the other terms in the sum. Then instead of summing these row by row, we can sum these column by column. For the first column, it is exactly the probability that v is at least one. Similarly, the next column gives the probability that v is at least two and so on. The reason this method is useful is that if v is at least one, that exactly means you return to the origin at some point, and so this is actually the return probability. And for simplicity, we denote it as r. What about the probability of returning at least twice? Well, the probability that you will return is r, but again by the Markov property, you should forget that you have returned and so the probability that you return again is to multiply by another r. And in general, the probability of returning at least k times is r to the k. So if we go back to the expectation, each column actually sums up to a power of r, and the expectation is a geometric series. So we know what this should sum to if that return probability is less than one, which is precisely the case when the state is transient. For now, we only need to know that this is finite if r is less than one. However, we have also deduced that for a recurrent state, this expected number of returns is infinite. So if the state is recurrent, we have the expectation to be infinite, and if the state is transient, the expectation is finite. Because the state is either recurrent or transient, no third option, if say the expectation is infinite, the state could not possibly have been transient, and so we can infer that the state is recurrent. So if we know whether this expectation is infinite, we are done. To do that, we need a final trick. Another way of thinking about v, the number of returns, is that it is a tally. We first ask the questions, have you returned at step n? If yes, then we add one to the tally, and if not, then we don't do anything to it. Once we answered all these questions, we add up the total, and we obtain v by just adding up all these plus ones. If we are asking for the expected value of v, we add up the expected value of the first question, the expected value of the second question, and so on. The expected value of these yes-no questions are much easier to handle. The expected value of this question would be, by definition, one times the probability that you answer yes, plus zero times that you answer no. Well, that's just the probability that you answer yes. We usually denote this probability as p with subscript zero zero and superscript one. The double zero denotes going from the origin to the origin, i.e. revisiting the origin. And the superscript just represents at which step you revisit. These probabilities will be the expected value for each question, and the sum of these probabilities gives us the expected value of v. So we now have a way of explicitly computing the expected value of v. Previously, we deduced that we can simply use the expected value of v to infer whether the state is recurrent, and now we also know how to explicitly compute that expected value. So once we know whether this series converges, then we know whether the state is recurrent or transient. This whole argument actually works for all Markov chains, including the random walk we are considering. In the previous chapter, we essentially devised a test for recurrence or transient. A 2D random walk is recurrent, so we want to prove that the series diverges. A 3D random walk is transient, so we want to prove that it converges. But whichever the case, we need to compute each term. In the case of a random walk, you can't possibly go back to the origin after one step, and so this probability is zero. This should not be too surprising, because any return paths would have the same number of steps to the left as those to the right, and of course, same number of steps downwards as those upwards. And so the total number of steps here has to be even. So this nth step probability is zero, if n is odd, and so we can just focus on finding the even two nth step probabilities. Let's say we want a total of 18 steps, then this is a specific possible return path. This specific path has a probability of one quarter to the 18, because each step is chosen with probability one quarter, and there are 18 steps you need to take. Of course, this is not the only possible path that returns to the origin after 18 steps. This one also does. Even though it has returned to the origin already, as long as you return at step 18, it still counts, because you will still answer yes in the previous argument. Anyway, the new path also has probability one quarter to the 18. More explicitly, when calculating the 18th step return probability, we just add up these probabilities. This part is the probability of getting a specific return path, and we then multiply by the number of return paths to get the overall probability. So it all boils down to counting the number of return paths. For a total of two n steps, a return path should have the same number of steps to the left and to the right, and the number of steps upwards is the same as downwards. Because the total number of steps is two n, we can express j in terms of i. For a return path, we can imagine it as a sequence of moves. Because there are a total of two n steps, in total there are two n factorial arrangements of these moves. However, if for example, we just rearrange these two left moves, then the resulting path quite literally hasn't changed, yet it counted as different in those two n factorial arrangements. Because there are i steps to the left, there are i factorial different permutations in between them, and of course the other directions have similar results. These interpermutations all counted as different in those two n factorial arrangements, yet they should have been counted the same, and so the number of return paths for a fixed value of i is two n factorial divided by all these factorials. So given a value of i, this is the number, and i can range from zero to n, and so the final total number of return paths needs to add these up for i ranging from zero to n. Finally, this two n step return probability is one quarter to the two n times the total number of return paths. That's just the case in 2D. What about the 3D random walk? The only difference is that transition probabilities are all one sixth, and there is one more pair of directions to consider. The two n step probabilities would be replacing one quarter by one sixth, and for the number of return paths we use a similar method, with the only difference being there are three pairs of directions now. So similarly, for fixed values of i and j, this is the number of return paths, and adding all these for i and j, ranging from zero to n, will give the total number of return paths. And finally, for the two n step return probability, we simply multiply by one sixth to the two n. But perhaps it might be useful to remind ourselves why we care about this. We have demonstrated that a state being recurrent or transient implies whether the series diverges or converges. So by knowing whether this series converges, we know whether the state is transient. In this chapter, we calculated those terms in the series with explicit expressions for the 2D and 3D cases respectively. And what remains is to show that the series formed by the 2D case diverges, and the 3D case converges. The very quick reason for it is that each term here scales like 1 over n, so it diverges because the harmonic series diverges. And the other term would scale like 1 over n to 3 halves, and this will lead to a convergent series instead. Originally I wanted to make it into the main video, but this is a bit too much manipulation of expressions, and so I ended up making it a second channel video. But I want to give a bit more no calculation explanation here. We can always think of an inner and an outer region in any dimension, but in higher dimensions there is much more space in the outer region, so once you have gone out, it will be less likely for you to go back in higher dimensions. The exact cutoff turns out to be between 2 and 3 dimensions. However, does this inner outer region explanation and the cutoff between 2 and 3 dimensions look familiar? If you have watched my previous video on Stein's paradox, then you might remember that whether the ordinary estimator is admissible has a cutoff between 2 and 3 dimensions, and we found out that recurrence of random walks also has the same cutoff. It turns out that this is not a coincidence. Larry Brown wrote in 1971 about the connection between these two problems, but this is way too involved in a YouTube video. I will put a link in the description for those who want to know more. Before you go, I just want to thank you for your support, because now I have 100k subs. To celebrate this milestone, I am going to make a Q&A video. You can ask me any question, but do so in the google form below. And just so you are prepared for the next video, and you might have also guessed from these two videos, I am currently a fourth year Cambridge Math student. We call ourselves Mathmos for some unknown reason. So yes, be prepared for some Cambridge related Math video in the future. Please consider giving on Patreon, and thanks to the Patrons for making this video possible. As always, subscribe with the bell on, like, comment, and share this video. See you next time!", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.68, "text": " A mathematical saying goes, a drunk man will find his way home, but a drunk bird may get", "tokens": [50364, 316, 18894, 1566, 1709, 11, 257, 11192, 587, 486, 915, 702, 636, 1280, 11, 457, 257, 11192, 5255, 815, 483, 50648], "temperature": 0.0, "avg_logprob": -0.08436989784240723, "compression_ratio": 1.6832579185520362, "no_speech_prob": 0.00028682537958957255}, {"id": 1, "seek": 0, "start": 5.68, "end": 13.040000000000001, "text": " lost forever. The assumption here is that because both are drunk, they are doing random walks.", "tokens": [50648, 2731, 5680, 13, 440, 15302, 510, 307, 300, 570, 1293, 366, 11192, 11, 436, 366, 884, 4974, 12896, 13, 51016], "temperature": 0.0, "avg_logprob": -0.08436989784240723, "compression_ratio": 1.6832579185520362, "no_speech_prob": 0.00028682537958957255}, {"id": 2, "seek": 0, "start": 13.040000000000001, "end": 18.32, "text": " The difference is that the man can only walk on the surface of the earth, and so he is doing a", "tokens": [51016, 440, 2649, 307, 300, 264, 587, 393, 787, 1792, 322, 264, 3753, 295, 264, 4120, 11, 293, 370, 415, 307, 884, 257, 51280], "temperature": 0.0, "avg_logprob": -0.08436989784240723, "compression_ratio": 1.6832579185520362, "no_speech_prob": 0.00028682537958957255}, {"id": 3, "seek": 0, "start": 18.32, "end": 24.32, "text": " two-dimensional random walk. But for the bird, in addition to the two dimensions, it can also", "tokens": [51280, 732, 12, 18759, 4974, 1792, 13, 583, 337, 264, 5255, 11, 294, 4500, 281, 264, 732, 12819, 11, 309, 393, 611, 51580], "temperature": 0.0, "avg_logprob": -0.08436989784240723, "compression_ratio": 1.6832579185520362, "no_speech_prob": 0.00028682537958957255}, {"id": 4, "seek": 2432, "start": 24.32, "end": 31.92, "text": " fly up and down, and so it is doing a 3D random walk. It turns out that no matter where the", "tokens": [50364, 3603, 493, 293, 760, 11, 293, 370, 309, 307, 884, 257, 805, 35, 4974, 1792, 13, 467, 4523, 484, 300, 572, 1871, 689, 264, 50744], "temperature": 0.0, "avg_logprob": -0.07527578939305674, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.022280633449554443}, {"id": 5, "seek": 2432, "start": 31.92, "end": 39.2, "text": " drunk man started, it is mathematically guaranteed that he will return home, it's only sooner or", "tokens": [50744, 11192, 587, 1409, 11, 309, 307, 44003, 18031, 300, 415, 486, 2736, 1280, 11, 309, 311, 787, 15324, 420, 51108], "temperature": 0.0, "avg_logprob": -0.07527578939305674, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.022280633449554443}, {"id": 6, "seek": 2432, "start": 39.2, "end": 46.8, "text": " later. But for the bird, there is a non-zero chance that it will get lost forever, even if it started", "tokens": [51108, 1780, 13, 583, 337, 264, 5255, 11, 456, 307, 257, 2107, 12, 32226, 2931, 300, 309, 486, 483, 2731, 5680, 11, 754, 498, 309, 1409, 51488], "temperature": 0.0, "avg_logprob": -0.07527578939305674, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.022280633449554443}, {"id": 7, "seek": 2432, "start": 46.8, "end": 53.92, "text": " off in its own nets. Why is there such a stark difference between a 2D and a 3D random walk?", "tokens": [51488, 766, 294, 1080, 1065, 36170, 13, 1545, 307, 456, 1270, 257, 17417, 2649, 1296, 257, 568, 35, 293, 257, 805, 35, 4974, 1792, 30, 51844], "temperature": 0.0, "avg_logprob": -0.07527578939305674, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.022280633449554443}, {"id": 8, "seek": 5432, "start": 54.48, "end": 58.32, "text": " To answer this question, we use the framework of Markov chains.", "tokens": [50372, 1407, 1867, 341, 1168, 11, 321, 764, 264, 8388, 295, 3934, 5179, 12626, 13, 50564], "temperature": 0.0, "avg_logprob": -0.0667722225189209, "compression_ratio": 1.6303317535545023, "no_speech_prob": 0.0004442049248609692}, {"id": 9, "seek": 5432, "start": 62.480000000000004, "end": 68.48, "text": " A Markov chain consists of three things. The first is the state space, essentially meaning", "tokens": [50772, 316, 3934, 5179, 5021, 14689, 295, 1045, 721, 13, 440, 700, 307, 264, 1785, 1901, 11, 4476, 3620, 51072], "temperature": 0.0, "avg_logprob": -0.0667722225189209, "compression_ratio": 1.6303317535545023, "no_speech_prob": 0.0004442049248609692}, {"id": 10, "seek": 5432, "start": 68.48, "end": 74.8, "text": " where you can go. In the case of random walks, for simplicity, the state space would be just", "tokens": [51072, 689, 291, 393, 352, 13, 682, 264, 1389, 295, 4974, 12896, 11, 337, 25632, 11, 264, 1785, 1901, 576, 312, 445, 51388], "temperature": 0.0, "avg_logprob": -0.0667722225189209, "compression_ratio": 1.6303317535545023, "no_speech_prob": 0.0004442049248609692}, {"id": 11, "seek": 5432, "start": 74.8, "end": 82.16, "text": " those lattice points, so the drunk man or bird can only visit those lattice points. Second thing", "tokens": [51388, 729, 34011, 2793, 11, 370, 264, 11192, 587, 420, 5255, 393, 787, 3441, 729, 34011, 2793, 13, 5736, 551, 51756], "temperature": 0.0, "avg_logprob": -0.0667722225189209, "compression_ratio": 1.6303317535545023, "no_speech_prob": 0.0004442049248609692}, {"id": 12, "seek": 8216, "start": 82.16, "end": 88.39999999999999, "text": " is the transition probabilities, because Markov chains are not static. Let's say you are now at", "tokens": [50364, 307, 264, 6034, 33783, 11, 570, 3934, 5179, 12626, 366, 406, 13437, 13, 961, 311, 584, 291, 366, 586, 412, 50676], "temperature": 0.0, "avg_logprob": -0.058445252312554256, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.002550785429775715}, {"id": 13, "seek": 8216, "start": 88.39999999999999, "end": 95.75999999999999, "text": " state A, with four other neighboring states in the state space. In your next step, you can transition", "tokens": [50676, 1785, 316, 11, 365, 1451, 661, 31521, 4368, 294, 264, 1785, 1901, 13, 682, 428, 958, 1823, 11, 291, 393, 6034, 51044], "temperature": 0.0, "avg_logprob": -0.058445252312554256, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.002550785429775715}, {"id": 14, "seek": 8216, "start": 95.75999999999999, "end": 102.88, "text": " to any other states, and in general, you might also go back to itself, depending on your setup.", "tokens": [51044, 281, 604, 661, 4368, 11, 293, 294, 2674, 11, 291, 1062, 611, 352, 646, 281, 2564, 11, 5413, 322, 428, 8657, 13, 51400], "temperature": 0.0, "avg_logprob": -0.058445252312554256, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.002550785429775715}, {"id": 15, "seek": 8216, "start": 103.67999999999999, "end": 110.47999999999999, "text": " For each possible transition, we just assign a probability to it. They are the transition", "tokens": [51440, 1171, 1184, 1944, 6034, 11, 321, 445, 6269, 257, 8482, 281, 309, 13, 814, 366, 264, 6034, 51780], "temperature": 0.0, "avg_logprob": -0.058445252312554256, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.002550785429775715}, {"id": 16, "seek": 11048, "start": 110.48, "end": 118.16, "text": " probabilities. For a 2D random walk, each state has four neighbors, and because we have no bias", "tokens": [50364, 33783, 13, 1171, 257, 568, 35, 4974, 1792, 11, 1184, 1785, 575, 1451, 12512, 11, 293, 570, 321, 362, 572, 12577, 50748], "temperature": 0.0, "avg_logprob": -0.05122988332401623, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.0027146637439727783}, {"id": 17, "seek": 11048, "start": 118.16, "end": 125.84, "text": " towards any direction, the transition probabilities are all one quarter. Similarly, for a 3D random", "tokens": [50748, 3030, 604, 3513, 11, 264, 6034, 33783, 366, 439, 472, 6555, 13, 13157, 11, 337, 257, 805, 35, 4974, 51132], "temperature": 0.0, "avg_logprob": -0.05122988332401623, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.0027146637439727783}, {"id": 18, "seek": 11048, "start": 125.84, "end": 132.4, "text": " walk, each state has six neighbors, and so the transition probabilities are all one sixth.", "tokens": [51132, 1792, 11, 1184, 1785, 575, 2309, 12512, 11, 293, 370, 264, 6034, 33783, 366, 439, 472, 15102, 13, 51460], "temperature": 0.0, "avg_logprob": -0.05122988332401623, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.0027146637439727783}, {"id": 19, "seek": 11048, "start": 133.28, "end": 139.76, "text": " The final thing is the initial distribution. In a general Markov chain, you have the freedom to", "tokens": [51504, 440, 2572, 551, 307, 264, 5883, 7316, 13, 682, 257, 2674, 3934, 5179, 5021, 11, 291, 362, 264, 5645, 281, 51828], "temperature": 0.0, "avg_logprob": -0.05122988332401623, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.0027146637439727783}, {"id": 20, "seek": 13976, "start": 139.76, "end": 146.88, "text": " choose which state to start, and so we can assign a probability to each state, indicating how likely", "tokens": [50364, 2826, 597, 1785, 281, 722, 11, 293, 370, 321, 393, 6269, 257, 8482, 281, 1184, 1785, 11, 25604, 577, 3700, 50720], "temperature": 0.0, "avg_logprob": -0.06065006349600997, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.001926634693518281}, {"id": 21, "seek": 13976, "start": 146.88, "end": 153.51999999999998, "text": " you are to start at that state. In the case of a random walk, we want to definitively start at", "tokens": [50720, 291, 366, 281, 722, 412, 300, 1785, 13, 682, 264, 1389, 295, 257, 4974, 1792, 11, 321, 528, 281, 28152, 356, 722, 412, 51052], "temperature": 0.0, "avg_logprob": -0.06065006349600997, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.001926634693518281}, {"id": 22, "seek": 13976, "start": 153.51999999999998, "end": 161.51999999999998, "text": " the origin, i.e. a probability 1 of starting at the origin, and 0 everywhere else. Wait,", "tokens": [51052, 264, 4957, 11, 741, 13, 68, 13, 257, 8482, 502, 295, 2891, 412, 264, 4957, 11, 293, 1958, 5315, 1646, 13, 3802, 11, 51452], "temperature": 0.0, "avg_logprob": -0.06065006349600997, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.001926634693518281}, {"id": 23, "seek": 13976, "start": 161.51999999999998, "end": 167.35999999999999, "text": " didn't you say it doesn't matter where you started for the 2D case? Don't worry, I'll get there in", "tokens": [51452, 994, 380, 291, 584, 309, 1177, 380, 1871, 689, 291, 1409, 337, 264, 568, 35, 1389, 30, 1468, 380, 3292, 11, 286, 603, 483, 456, 294, 51744], "temperature": 0.0, "avg_logprob": -0.06065006349600997, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.001926634693518281}, {"id": 24, "seek": 16736, "start": 167.36, "end": 174.24, "text": " a minute. These are the three elements in the Markov chain, but there is one key feature that", "tokens": [50364, 257, 3456, 13, 1981, 366, 264, 1045, 4959, 294, 264, 3934, 5179, 5021, 11, 457, 456, 307, 472, 2141, 4111, 300, 50708], "temperature": 0.0, "avg_logprob": -0.0904873454052469, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0038243080489337444}, {"id": 25, "seek": 16736, "start": 174.24, "end": 181.36, "text": " makes it a Markov chain, the Markov property. This is a simple idea that once you have gone to a", "tokens": [50708, 1669, 309, 257, 3934, 5179, 5021, 11, 264, 3934, 5179, 4707, 13, 639, 307, 257, 2199, 1558, 300, 1564, 291, 362, 2780, 281, 257, 51064], "temperature": 0.0, "avg_logprob": -0.0904873454052469, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0038243080489337444}, {"id": 26, "seek": 16736, "start": 181.36, "end": 188.32000000000002, "text": " particular state, you should have already forgotten how you get there, and consider the transition", "tokens": [51064, 1729, 1785, 11, 291, 820, 362, 1217, 11832, 577, 291, 483, 456, 11, 293, 1949, 264, 6034, 51412], "temperature": 0.0, "avg_logprob": -0.0904873454052469, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0038243080489337444}, {"id": 27, "seek": 16736, "start": 188.32000000000002, "end": 195.04000000000002, "text": " probabilities from there without caring about the root that takes you there. So yes, that's the whole", "tokens": [51412, 33783, 490, 456, 1553, 15365, 466, 264, 5593, 300, 2516, 291, 456, 13, 407, 2086, 11, 300, 311, 264, 1379, 51748], "temperature": 0.0, "avg_logprob": -0.0904873454052469, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0038243080489337444}, {"id": 28, "seek": 19504, "start": 195.04, "end": 206.07999999999998, "text": " setup for a Markov chain, but how does that help? Given that a random walk is random, and every time", "tokens": [50364, 8657, 337, 257, 3934, 5179, 5021, 11, 457, 577, 775, 300, 854, 30, 18600, 300, 257, 4974, 1792, 307, 4974, 11, 293, 633, 565, 50916], "temperature": 0.0, "avg_logprob": -0.06019913844573192, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.000404474965762347}, {"id": 29, "seek": 19504, "start": 206.07999999999998, "end": 212.64, "text": " you run it, it gives different results, what can we say about that? One thing we can say is whether", "tokens": [50916, 291, 1190, 309, 11, 309, 2709, 819, 3542, 11, 437, 393, 321, 584, 466, 300, 30, 1485, 551, 321, 393, 584, 307, 1968, 51244], "temperature": 0.0, "avg_logprob": -0.06019913844573192, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.000404474965762347}, {"id": 30, "seek": 19504, "start": 212.64, "end": 219.2, "text": " you will revisit the origin if you started there. If you are guaranteed to go back, or in other words,", "tokens": [51244, 291, 486, 32676, 264, 4957, 498, 291, 1409, 456, 13, 759, 291, 366, 18031, 281, 352, 646, 11, 420, 294, 661, 2283, 11, 51572], "temperature": 0.0, "avg_logprob": -0.06019913844573192, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.000404474965762347}, {"id": 31, "seek": 21920, "start": 219.28, "end": 226.88, "text": " the return probability is 1, then we call that initial state recurrent. If not, then you are not", "tokens": [50368, 264, 2736, 8482, 307, 502, 11, 550, 321, 818, 300, 5883, 1785, 18680, 1753, 13, 759, 406, 11, 550, 291, 366, 406, 50748], "temperature": 0.0, "avg_logprob": -0.04966150874822912, "compression_ratio": 1.8774193548387097, "no_speech_prob": 0.0012842855649068952}, {"id": 32, "seek": 21920, "start": 226.88, "end": 234.16, "text": " guaranteed to return, and so the return probability is less than 1. In such a case, we call it a", "tokens": [50748, 18031, 281, 2736, 11, 293, 370, 264, 2736, 8482, 307, 1570, 813, 502, 13, 682, 1270, 257, 1389, 11, 321, 818, 309, 257, 51112], "temperature": 0.0, "avg_logprob": -0.04966150874822912, "compression_ratio": 1.8774193548387097, "no_speech_prob": 0.0012842855649068952}, {"id": 33, "seek": 21920, "start": 234.16, "end": 242.23999999999998, "text": " transient state. Because you are either guaranteed or not guaranteed to return, a state is either", "tokens": [51112, 41998, 1785, 13, 1436, 291, 366, 2139, 18031, 420, 406, 18031, 281, 2736, 11, 257, 1785, 307, 2139, 51516], "temperature": 0.0, "avg_logprob": -0.04966150874822912, "compression_ratio": 1.8774193548387097, "no_speech_prob": 0.0012842855649068952}, {"id": 34, "seek": 24224, "start": 242.24, "end": 250.32000000000002, "text": " recurrent or transient, there is no third option. It turns out that in 2D, the origin is a recurrent", "tokens": [50364, 18680, 1753, 420, 41998, 11, 456, 307, 572, 2636, 3614, 13, 467, 4523, 484, 300, 294, 568, 35, 11, 264, 4957, 307, 257, 18680, 1753, 50768], "temperature": 0.0, "avg_logprob": -0.07162362575531006, "compression_ratio": 1.670940170940171, "no_speech_prob": 0.008061490952968597}, {"id": 35, "seek": 24224, "start": 250.32000000000002, "end": 256.64, "text": " state, and just because this state is recurrent, we can already infer that it does not matter", "tokens": [50768, 1785, 11, 293, 445, 570, 341, 1785, 307, 18680, 1753, 11, 321, 393, 1217, 13596, 300, 309, 775, 406, 1871, 51084], "temperature": 0.0, "avg_logprob": -0.07162362575531006, "compression_ratio": 1.670940170940171, "no_speech_prob": 0.008061490952968597}, {"id": 36, "seek": 24224, "start": 256.64, "end": 262.88, "text": " whether drunk man started, he will go back to the origin. That takes one or two lines of reasoning,", "tokens": [51084, 1968, 11192, 587, 1409, 11, 415, 486, 352, 646, 281, 264, 4957, 13, 663, 2516, 472, 420, 732, 3876, 295, 21577, 11, 51396], "temperature": 0.0, "avg_logprob": -0.07162362575531006, "compression_ratio": 1.670940170940171, "no_speech_prob": 0.008061490952968597}, {"id": 37, "seek": 24224, "start": 262.88, "end": 269.52, "text": " which I encourage you to do in the comments. On the other hand, the origin in the 3D random walk", "tokens": [51396, 597, 286, 5373, 291, 281, 360, 294, 264, 3053, 13, 1282, 264, 661, 1011, 11, 264, 4957, 294, 264, 805, 35, 4974, 1792, 51728], "temperature": 0.0, "avg_logprob": -0.07162362575531006, "compression_ratio": 1.670940170940171, "no_speech_prob": 0.008061490952968597}, {"id": 38, "seek": 26952, "start": 269.52, "end": 277.2, "text": " is a transient state, so the drunk bird may never return. The problem now is how to determine that", "tokens": [50364, 307, 257, 41998, 1785, 11, 370, 264, 11192, 5255, 815, 1128, 2736, 13, 440, 1154, 586, 307, 577, 281, 6997, 300, 50748], "temperature": 0.0, "avg_logprob": -0.06864765188196204, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.0011694829445332289}, {"id": 39, "seek": 26952, "start": 277.2, "end": 284.24, "text": " return probability. Here's the trick. We consider a quantity v, which is the number of returns to", "tokens": [50748, 2736, 8482, 13, 1692, 311, 264, 4282, 13, 492, 1949, 257, 11275, 371, 11, 597, 307, 264, 1230, 295, 11247, 281, 51100], "temperature": 0.0, "avg_logprob": -0.06864765188196204, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.0011694829445332289}, {"id": 40, "seek": 26952, "start": 284.24, "end": 291.2, "text": " the origin. The implicit assumption is to run the random walk to infinity, even if you have returned.", "tokens": [51100, 264, 4957, 13, 440, 26947, 15302, 307, 281, 1190, 264, 4974, 1792, 281, 13202, 11, 754, 498, 291, 362, 8752, 13, 51448], "temperature": 0.0, "avg_logprob": -0.06864765188196204, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.0011694829445332289}, {"id": 41, "seek": 26952, "start": 291.84, "end": 298.0, "text": " Our focus is the expectation of v, i.e. the expected number of returns.", "tokens": [51480, 2621, 1879, 307, 264, 14334, 295, 371, 11, 741, 13, 68, 13, 264, 5176, 1230, 295, 11247, 13, 51788], "temperature": 0.0, "avg_logprob": -0.06864765188196204, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.0011694829445332289}, {"id": 42, "seek": 29800, "start": 298.72, "end": 304.96, "text": " Now for the recurrent case, we are guaranteed to go back, but if we continue to run the random walk", "tokens": [50400, 823, 337, 264, 18680, 1753, 1389, 11, 321, 366, 18031, 281, 352, 646, 11, 457, 498, 321, 2354, 281, 1190, 264, 4974, 1792, 50712], "temperature": 0.0, "avg_logprob": -0.07986312163503546, "compression_ratio": 1.7772727272727273, "no_speech_prob": 0.0005357736372388899}, {"id": 43, "seek": 29800, "start": 304.96, "end": 311.84, "text": " by the Markov property, we should have forgotten that we have returned, and then we will for sure", "tokens": [50712, 538, 264, 3934, 5179, 4707, 11, 321, 820, 362, 11832, 300, 321, 362, 8752, 11, 293, 550, 321, 486, 337, 988, 51056], "temperature": 0.0, "avg_logprob": -0.07986312163503546, "compression_ratio": 1.7772727272727273, "no_speech_prob": 0.0005357736372388899}, {"id": 44, "seek": 29800, "start": 311.84, "end": 320.48, "text": " return to the origin again and again. So the number of returns v is guaranteed to be infinite,", "tokens": [51056, 2736, 281, 264, 4957, 797, 293, 797, 13, 407, 264, 1230, 295, 11247, 371, 307, 18031, 281, 312, 13785, 11, 51488], "temperature": 0.0, "avg_logprob": -0.07986312163503546, "compression_ratio": 1.7772727272727273, "no_speech_prob": 0.0005357736372388899}, {"id": 45, "seek": 29800, "start": 320.48, "end": 327.84, "text": " and the expectation would also be infinite. That is when the state is recurrent. What if the state", "tokens": [51488, 293, 264, 14334, 576, 611, 312, 13785, 13, 663, 307, 562, 264, 1785, 307, 18680, 1753, 13, 708, 498, 264, 1785, 51856], "temperature": 0.0, "avg_logprob": -0.07986312163503546, "compression_ratio": 1.7772727272727273, "no_speech_prob": 0.0005357736372388899}, {"id": 46, "seek": 32784, "start": 327.84, "end": 335.67999999999995, "text": " is transient? Here is a very clever general trick. By definition, the expectation is zero times the", "tokens": [50364, 307, 41998, 30, 1692, 307, 257, 588, 13494, 2674, 4282, 13, 3146, 7123, 11, 264, 14334, 307, 4018, 1413, 264, 50756], "temperature": 0.0, "avg_logprob": -0.07866251128060477, "compression_ratio": 1.6685714285714286, "no_speech_prob": 0.0005033217603340745}, {"id": 47, "seek": 32784, "start": 335.67999999999995, "end": 342.56, "text": " probability that v is zero plus one times the probability that v is one and so on. But what if", "tokens": [50756, 8482, 300, 371, 307, 4018, 1804, 472, 1413, 264, 8482, 300, 371, 307, 472, 293, 370, 322, 13, 583, 437, 498, 51100], "temperature": 0.0, "avg_logprob": -0.07866251128060477, "compression_ratio": 1.6685714285714286, "no_speech_prob": 0.0005033217603340745}, {"id": 48, "seek": 32784, "start": 342.56, "end": 350.47999999999996, "text": " we write, for instance, this two times probability as actually the sum of two copies, and also do", "tokens": [51100, 321, 2464, 11, 337, 5197, 11, 341, 732, 1413, 8482, 382, 767, 264, 2408, 295, 732, 14341, 11, 293, 611, 360, 51496], "temperature": 0.0, "avg_logprob": -0.07866251128060477, "compression_ratio": 1.6685714285714286, "no_speech_prob": 0.0005033217603340745}, {"id": 49, "seek": 35048, "start": 350.48, "end": 358.16, "text": " this similarly for the other terms in the sum. Then instead of summing these row by row, we can", "tokens": [50364, 341, 14138, 337, 264, 661, 2115, 294, 264, 2408, 13, 1396, 2602, 295, 2408, 2810, 613, 5386, 538, 5386, 11, 321, 393, 50748], "temperature": 0.0, "avg_logprob": -0.06234658603936854, "compression_ratio": 1.7625, "no_speech_prob": 0.011331131681799889}, {"id": 50, "seek": 35048, "start": 358.16, "end": 365.6, "text": " sum these column by column. For the first column, it is exactly the probability that v is at least", "tokens": [50748, 2408, 613, 7738, 538, 7738, 13, 1171, 264, 700, 7738, 11, 309, 307, 2293, 264, 8482, 300, 371, 307, 412, 1935, 51120], "temperature": 0.0, "avg_logprob": -0.06234658603936854, "compression_ratio": 1.7625, "no_speech_prob": 0.011331131681799889}, {"id": 51, "seek": 35048, "start": 365.6, "end": 372.56, "text": " one. Similarly, the next column gives the probability that v is at least two and so on.", "tokens": [51120, 472, 13, 13157, 11, 264, 958, 7738, 2709, 264, 8482, 300, 371, 307, 412, 1935, 732, 293, 370, 322, 13, 51468], "temperature": 0.0, "avg_logprob": -0.06234658603936854, "compression_ratio": 1.7625, "no_speech_prob": 0.011331131681799889}, {"id": 52, "seek": 37256, "start": 373.36, "end": 381.04, "text": " The reason this method is useful is that if v is at least one, that exactly means you return to", "tokens": [50404, 440, 1778, 341, 3170, 307, 4420, 307, 300, 498, 371, 307, 412, 1935, 472, 11, 300, 2293, 1355, 291, 2736, 281, 50788], "temperature": 0.0, "avg_logprob": -0.07836534665978473, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.0035934550687670708}, {"id": 53, "seek": 37256, "start": 381.04, "end": 387.92, "text": " the origin at some point, and so this is actually the return probability. And for simplicity,", "tokens": [50788, 264, 4957, 412, 512, 935, 11, 293, 370, 341, 307, 767, 264, 2736, 8482, 13, 400, 337, 25632, 11, 51132], "temperature": 0.0, "avg_logprob": -0.07836534665978473, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.0035934550687670708}, {"id": 54, "seek": 37256, "start": 387.92, "end": 395.76, "text": " we denote it as r. What about the probability of returning at least twice? Well, the probability", "tokens": [51132, 321, 45708, 309, 382, 367, 13, 708, 466, 264, 8482, 295, 12678, 412, 1935, 6091, 30, 1042, 11, 264, 8482, 51524], "temperature": 0.0, "avg_logprob": -0.07836534665978473, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.0035934550687670708}, {"id": 55, "seek": 37256, "start": 395.76, "end": 402.24, "text": " that you will return is r, but again by the Markov property, you should forget that you have returned", "tokens": [51524, 300, 291, 486, 2736, 307, 367, 11, 457, 797, 538, 264, 3934, 5179, 4707, 11, 291, 820, 2870, 300, 291, 362, 8752, 51848], "temperature": 0.0, "avg_logprob": -0.07836534665978473, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.0035934550687670708}, {"id": 56, "seek": 40224, "start": 402.32, "end": 409.6, "text": " and so the probability that you return again is to multiply by another r. And in general,", "tokens": [50368, 293, 370, 264, 8482, 300, 291, 2736, 797, 307, 281, 12972, 538, 1071, 367, 13, 400, 294, 2674, 11, 50732], "temperature": 0.0, "avg_logprob": -0.05987547202543779, "compression_ratio": 1.6952380952380952, "no_speech_prob": 0.00039203095366247}, {"id": 57, "seek": 40224, "start": 409.6, "end": 417.52, "text": " the probability of returning at least k times is r to the k. So if we go back to the expectation,", "tokens": [50732, 264, 8482, 295, 12678, 412, 1935, 350, 1413, 307, 367, 281, 264, 350, 13, 407, 498, 321, 352, 646, 281, 264, 14334, 11, 51128], "temperature": 0.0, "avg_logprob": -0.05987547202543779, "compression_ratio": 1.6952380952380952, "no_speech_prob": 0.00039203095366247}, {"id": 58, "seek": 40224, "start": 418.08, "end": 425.12, "text": " each column actually sums up to a power of r, and the expectation is a geometric series.", "tokens": [51156, 1184, 7738, 767, 34499, 493, 281, 257, 1347, 295, 367, 11, 293, 264, 14334, 307, 257, 33246, 2638, 13, 51508], "temperature": 0.0, "avg_logprob": -0.05987547202543779, "compression_ratio": 1.6952380952380952, "no_speech_prob": 0.00039203095366247}, {"id": 59, "seek": 40224, "start": 425.76, "end": 431.52, "text": " So we know what this should sum to if that return probability is less than one,", "tokens": [51540, 407, 321, 458, 437, 341, 820, 2408, 281, 498, 300, 2736, 8482, 307, 1570, 813, 472, 11, 51828], "temperature": 0.0, "avg_logprob": -0.05987547202543779, "compression_ratio": 1.6952380952380952, "no_speech_prob": 0.00039203095366247}, {"id": 60, "seek": 43152, "start": 431.52, "end": 438.47999999999996, "text": " which is precisely the case when the state is transient. For now, we only need to know that", "tokens": [50364, 597, 307, 13402, 264, 1389, 562, 264, 1785, 307, 41998, 13, 1171, 586, 11, 321, 787, 643, 281, 458, 300, 50712], "temperature": 0.0, "avg_logprob": -0.04608934990903164, "compression_ratio": 1.886138613861386, "no_speech_prob": 0.0007321583689190447}, {"id": 61, "seek": 43152, "start": 438.47999999999996, "end": 446.15999999999997, "text": " this is finite if r is less than one. However, we have also deduced that for a recurrent state,", "tokens": [50712, 341, 307, 19362, 498, 367, 307, 1570, 813, 472, 13, 2908, 11, 321, 362, 611, 4172, 41209, 300, 337, 257, 18680, 1753, 1785, 11, 51096], "temperature": 0.0, "avg_logprob": -0.04608934990903164, "compression_ratio": 1.886138613861386, "no_speech_prob": 0.0007321583689190447}, {"id": 62, "seek": 43152, "start": 446.15999999999997, "end": 453.84, "text": " this expected number of returns is infinite. So if the state is recurrent, we have the expectation", "tokens": [51096, 341, 5176, 1230, 295, 11247, 307, 13785, 13, 407, 498, 264, 1785, 307, 18680, 1753, 11, 321, 362, 264, 14334, 51480], "temperature": 0.0, "avg_logprob": -0.04608934990903164, "compression_ratio": 1.886138613861386, "no_speech_prob": 0.0007321583689190447}, {"id": 63, "seek": 43152, "start": 453.84, "end": 460.96, "text": " to be infinite, and if the state is transient, the expectation is finite. Because the state is", "tokens": [51480, 281, 312, 13785, 11, 293, 498, 264, 1785, 307, 41998, 11, 264, 14334, 307, 19362, 13, 1436, 264, 1785, 307, 51836], "temperature": 0.0, "avg_logprob": -0.04608934990903164, "compression_ratio": 1.886138613861386, "no_speech_prob": 0.0007321583689190447}, {"id": 64, "seek": 46096, "start": 461.03999999999996, "end": 468.0, "text": " either recurrent or transient, no third option, if say the expectation is infinite,", "tokens": [50368, 2139, 18680, 1753, 420, 41998, 11, 572, 2636, 3614, 11, 498, 584, 264, 14334, 307, 13785, 11, 50716], "temperature": 0.0, "avg_logprob": -0.05522770467011825, "compression_ratio": 1.7190476190476192, "no_speech_prob": 0.00037997664185240865}, {"id": 65, "seek": 46096, "start": 468.0, "end": 475.84, "text": " the state could not possibly have been transient, and so we can infer that the state is recurrent.", "tokens": [50716, 264, 1785, 727, 406, 6264, 362, 668, 41998, 11, 293, 370, 321, 393, 13596, 300, 264, 1785, 307, 18680, 1753, 13, 51108], "temperature": 0.0, "avg_logprob": -0.05522770467011825, "compression_ratio": 1.7190476190476192, "no_speech_prob": 0.00037997664185240865}, {"id": 66, "seek": 46096, "start": 476.64, "end": 483.59999999999997, "text": " So if we know whether this expectation is infinite, we are done. To do that, we need a final trick.", "tokens": [51148, 407, 498, 321, 458, 1968, 341, 14334, 307, 13785, 11, 321, 366, 1096, 13, 1407, 360, 300, 11, 321, 643, 257, 2572, 4282, 13, 51496], "temperature": 0.0, "avg_logprob": -0.05522770467011825, "compression_ratio": 1.7190476190476192, "no_speech_prob": 0.00037997664185240865}, {"id": 67, "seek": 46096, "start": 484.4, "end": 489.67999999999995, "text": " Another way of thinking about v, the number of returns, is that it is a tally.", "tokens": [51536, 3996, 636, 295, 1953, 466, 371, 11, 264, 1230, 295, 11247, 11, 307, 300, 309, 307, 257, 256, 379, 13, 51800], "temperature": 0.0, "avg_logprob": -0.05522770467011825, "compression_ratio": 1.7190476190476192, "no_speech_prob": 0.00037997664185240865}, {"id": 68, "seek": 48968, "start": 490.24, "end": 497.28000000000003, "text": " We first ask the questions, have you returned at step n? If yes, then we add one to the tally,", "tokens": [50392, 492, 700, 1029, 264, 1651, 11, 362, 291, 8752, 412, 1823, 297, 30, 759, 2086, 11, 550, 321, 909, 472, 281, 264, 256, 379, 11, 50744], "temperature": 0.0, "avg_logprob": -0.06417892873287201, "compression_ratio": 1.755, "no_speech_prob": 0.0028890890534967184}, {"id": 69, "seek": 48968, "start": 497.28000000000003, "end": 503.76, "text": " and if not, then we don't do anything to it. Once we answered all these questions,", "tokens": [50744, 293, 498, 406, 11, 550, 321, 500, 380, 360, 1340, 281, 309, 13, 3443, 321, 10103, 439, 613, 1651, 11, 51068], "temperature": 0.0, "avg_logprob": -0.06417892873287201, "compression_ratio": 1.755, "no_speech_prob": 0.0028890890534967184}, {"id": 70, "seek": 48968, "start": 503.76, "end": 509.68, "text": " we add up the total, and we obtain v by just adding up all these plus ones.", "tokens": [51068, 321, 909, 493, 264, 3217, 11, 293, 321, 12701, 371, 538, 445, 5127, 493, 439, 613, 1804, 2306, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06417892873287201, "compression_ratio": 1.755, "no_speech_prob": 0.0028890890534967184}, {"id": 71, "seek": 48968, "start": 510.4, "end": 517.92, "text": " If we are asking for the expected value of v, we add up the expected value of the first question,", "tokens": [51400, 759, 321, 366, 3365, 337, 264, 5176, 2158, 295, 371, 11, 321, 909, 493, 264, 5176, 2158, 295, 264, 700, 1168, 11, 51776], "temperature": 0.0, "avg_logprob": -0.06417892873287201, "compression_ratio": 1.755, "no_speech_prob": 0.0028890890534967184}, {"id": 72, "seek": 51792, "start": 518.0, "end": 525.92, "text": " the expected value of the second question, and so on. The expected value of these yes-no questions", "tokens": [50368, 264, 5176, 2158, 295, 264, 1150, 1168, 11, 293, 370, 322, 13, 440, 5176, 2158, 295, 613, 2086, 12, 1771, 1651, 50764], "temperature": 0.0, "avg_logprob": -0.09026340116937477, "compression_ratio": 2.039772727272727, "no_speech_prob": 0.0008040569955483079}, {"id": 73, "seek": 51792, "start": 525.92, "end": 533.36, "text": " are much easier to handle. The expected value of this question would be, by definition,", "tokens": [50764, 366, 709, 3571, 281, 4813, 13, 440, 5176, 2158, 295, 341, 1168, 576, 312, 11, 538, 7123, 11, 51136], "temperature": 0.0, "avg_logprob": -0.09026340116937477, "compression_ratio": 2.039772727272727, "no_speech_prob": 0.0008040569955483079}, {"id": 74, "seek": 51792, "start": 533.36, "end": 539.5999999999999, "text": " one times the probability that you answer yes, plus zero times that you answer no.", "tokens": [51136, 472, 1413, 264, 8482, 300, 291, 1867, 2086, 11, 1804, 4018, 1413, 300, 291, 1867, 572, 13, 51448], "temperature": 0.0, "avg_logprob": -0.09026340116937477, "compression_ratio": 2.039772727272727, "no_speech_prob": 0.0008040569955483079}, {"id": 75, "seek": 51792, "start": 540.3199999999999, "end": 546.3199999999999, "text": " Well, that's just the probability that you answer yes. We usually denote this probability", "tokens": [51484, 1042, 11, 300, 311, 445, 264, 8482, 300, 291, 1867, 2086, 13, 492, 2673, 45708, 341, 8482, 51784], "temperature": 0.0, "avg_logprob": -0.09026340116937477, "compression_ratio": 2.039772727272727, "no_speech_prob": 0.0008040569955483079}, {"id": 76, "seek": 54632, "start": 546.32, "end": 553.9200000000001, "text": " as p with subscript zero zero and superscript one. The double zero denotes going from the origin to", "tokens": [50364, 382, 280, 365, 2325, 662, 4018, 4018, 293, 37906, 5944, 472, 13, 440, 3834, 4018, 1441, 17251, 516, 490, 264, 4957, 281, 50744], "temperature": 0.0, "avg_logprob": -0.08874916500515408, "compression_ratio": 1.7685185185185186, "no_speech_prob": 0.006691598799079657}, {"id": 77, "seek": 54632, "start": 553.9200000000001, "end": 560.88, "text": " the origin, i.e. revisiting the origin. And the superscript just represents at which step you", "tokens": [50744, 264, 4957, 11, 741, 13, 68, 13, 20767, 1748, 264, 4957, 13, 400, 264, 37906, 5944, 445, 8855, 412, 597, 1823, 291, 51092], "temperature": 0.0, "avg_logprob": -0.08874916500515408, "compression_ratio": 1.7685185185185186, "no_speech_prob": 0.006691598799079657}, {"id": 78, "seek": 54632, "start": 560.88, "end": 568.24, "text": " revisit. These probabilities will be the expected value for each question, and the sum of these", "tokens": [51092, 32676, 13, 1981, 33783, 486, 312, 264, 5176, 2158, 337, 1184, 1168, 11, 293, 264, 2408, 295, 613, 51460], "temperature": 0.0, "avg_logprob": -0.08874916500515408, "compression_ratio": 1.7685185185185186, "no_speech_prob": 0.006691598799079657}, {"id": 79, "seek": 54632, "start": 568.24, "end": 576.1600000000001, "text": " probabilities gives us the expected value of v. So we now have a way of explicitly computing", "tokens": [51460, 33783, 2709, 505, 264, 5176, 2158, 295, 371, 13, 407, 321, 586, 362, 257, 636, 295, 20803, 15866, 51856], "temperature": 0.0, "avg_logprob": -0.08874916500515408, "compression_ratio": 1.7685185185185186, "no_speech_prob": 0.006691598799079657}, {"id": 80, "seek": 57616, "start": 576.16, "end": 583.28, "text": " the expected value of v. Previously, we deduced that we can simply use the expected value of v", "tokens": [50364, 264, 5176, 2158, 295, 371, 13, 33606, 11, 321, 4172, 41209, 300, 321, 393, 2935, 764, 264, 5176, 2158, 295, 371, 50720], "temperature": 0.0, "avg_logprob": -0.04878574130178868, "compression_ratio": 1.7819905213270142, "no_speech_prob": 0.0008830172009766102}, {"id": 81, "seek": 57616, "start": 583.28, "end": 590.8, "text": " to infer whether the state is recurrent, and now we also know how to explicitly compute", "tokens": [50720, 281, 13596, 1968, 264, 1785, 307, 18680, 1753, 11, 293, 586, 321, 611, 458, 577, 281, 20803, 14722, 51096], "temperature": 0.0, "avg_logprob": -0.04878574130178868, "compression_ratio": 1.7819905213270142, "no_speech_prob": 0.0008830172009766102}, {"id": 82, "seek": 57616, "start": 590.8, "end": 598.24, "text": " that expected value. So once we know whether this series converges, then we know whether the state", "tokens": [51096, 300, 5176, 2158, 13, 407, 1564, 321, 458, 1968, 341, 2638, 9652, 2880, 11, 550, 321, 458, 1968, 264, 1785, 51468], "temperature": 0.0, "avg_logprob": -0.04878574130178868, "compression_ratio": 1.7819905213270142, "no_speech_prob": 0.0008830172009766102}, {"id": 83, "seek": 57616, "start": 598.24, "end": 605.1999999999999, "text": " is recurrent or transient. This whole argument actually works for all Markov chains, including", "tokens": [51468, 307, 18680, 1753, 420, 41998, 13, 639, 1379, 6770, 767, 1985, 337, 439, 3934, 5179, 12626, 11, 3009, 51816], "temperature": 0.0, "avg_logprob": -0.04878574130178868, "compression_ratio": 1.7819905213270142, "no_speech_prob": 0.0008830172009766102}, {"id": 84, "seek": 60520, "start": 605.2, "end": 615.5200000000001, "text": " the random walk we are considering. In the previous chapter, we essentially devised a test for recurrence", "tokens": [50364, 264, 4974, 1792, 321, 366, 8079, 13, 682, 264, 3894, 7187, 11, 321, 4476, 1905, 2640, 257, 1500, 337, 18680, 10760, 50880], "temperature": 0.0, "avg_logprob": -0.06551326115926107, "compression_ratio": 1.7159763313609468, "no_speech_prob": 0.0002694724826142192}, {"id": 85, "seek": 60520, "start": 615.5200000000001, "end": 623.0400000000001, "text": " or transient. A 2D random walk is recurrent, so we want to prove that the series diverges.", "tokens": [50880, 420, 41998, 13, 316, 568, 35, 4974, 1792, 307, 18680, 1753, 11, 370, 321, 528, 281, 7081, 300, 264, 2638, 18558, 2880, 13, 51256], "temperature": 0.0, "avg_logprob": -0.06551326115926107, "compression_ratio": 1.7159763313609468, "no_speech_prob": 0.0002694724826142192}, {"id": 86, "seek": 60520, "start": 623.0400000000001, "end": 629.76, "text": " A 3D random walk is transient, so we want to prove that it converges. But whichever the case,", "tokens": [51256, 316, 805, 35, 4974, 1792, 307, 41998, 11, 370, 321, 528, 281, 7081, 300, 309, 9652, 2880, 13, 583, 24123, 264, 1389, 11, 51592], "temperature": 0.0, "avg_logprob": -0.06551326115926107, "compression_ratio": 1.7159763313609468, "no_speech_prob": 0.0002694724826142192}, {"id": 87, "seek": 62976, "start": 629.76, "end": 636.3199999999999, "text": " we need to compute each term. In the case of a random walk, you can't possibly go back to the", "tokens": [50364, 321, 643, 281, 14722, 1184, 1433, 13, 682, 264, 1389, 295, 257, 4974, 1792, 11, 291, 393, 380, 6264, 352, 646, 281, 264, 50692], "temperature": 0.0, "avg_logprob": -0.057275761728701385, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.005554671864956617}, {"id": 88, "seek": 62976, "start": 636.3199999999999, "end": 642.72, "text": " origin after one step, and so this probability is zero. This should not be too surprising,", "tokens": [50692, 4957, 934, 472, 1823, 11, 293, 370, 341, 8482, 307, 4018, 13, 639, 820, 406, 312, 886, 8830, 11, 51012], "temperature": 0.0, "avg_logprob": -0.057275761728701385, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.005554671864956617}, {"id": 89, "seek": 62976, "start": 642.72, "end": 648.88, "text": " because any return paths would have the same number of steps to the left as those to the right,", "tokens": [51012, 570, 604, 2736, 14518, 576, 362, 264, 912, 1230, 295, 4439, 281, 264, 1411, 382, 729, 281, 264, 558, 11, 51320], "temperature": 0.0, "avg_logprob": -0.057275761728701385, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.005554671864956617}, {"id": 90, "seek": 62976, "start": 648.88, "end": 655.76, "text": " and of course, same number of steps downwards as those upwards. And so the total number of steps", "tokens": [51320, 293, 295, 1164, 11, 912, 1230, 295, 4439, 39880, 382, 729, 22167, 13, 400, 370, 264, 3217, 1230, 295, 4439, 51664], "temperature": 0.0, "avg_logprob": -0.057275761728701385, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.005554671864956617}, {"id": 91, "seek": 65576, "start": 655.76, "end": 665.04, "text": " here has to be even. So this nth step probability is zero, if n is odd, and so we can just focus on", "tokens": [50364, 510, 575, 281, 312, 754, 13, 407, 341, 297, 392, 1823, 8482, 307, 4018, 11, 498, 297, 307, 7401, 11, 293, 370, 321, 393, 445, 1879, 322, 50828], "temperature": 0.0, "avg_logprob": -0.08339701689682998, "compression_ratio": 1.5614973262032086, "no_speech_prob": 0.001064916723407805}, {"id": 92, "seek": 65576, "start": 665.04, "end": 673.12, "text": " finding the even two nth step probabilities. Let's say we want a total of 18 steps, then this is a", "tokens": [50828, 5006, 264, 754, 732, 297, 392, 1823, 33783, 13, 961, 311, 584, 321, 528, 257, 3217, 295, 2443, 4439, 11, 550, 341, 307, 257, 51232], "temperature": 0.0, "avg_logprob": -0.08339701689682998, "compression_ratio": 1.5614973262032086, "no_speech_prob": 0.001064916723407805}, {"id": 93, "seek": 65576, "start": 673.12, "end": 680.72, "text": " specific possible return path. This specific path has a probability of one quarter to the 18,", "tokens": [51232, 2685, 1944, 2736, 3100, 13, 639, 2685, 3100, 575, 257, 8482, 295, 472, 6555, 281, 264, 2443, 11, 51612], "temperature": 0.0, "avg_logprob": -0.08339701689682998, "compression_ratio": 1.5614973262032086, "no_speech_prob": 0.001064916723407805}, {"id": 94, "seek": 68072, "start": 680.72, "end": 688.72, "text": " because each step is chosen with probability one quarter, and there are 18 steps you need to take.", "tokens": [50364, 570, 1184, 1823, 307, 8614, 365, 8482, 472, 6555, 11, 293, 456, 366, 2443, 4439, 291, 643, 281, 747, 13, 50764], "temperature": 0.0, "avg_logprob": -0.06146618994799527, "compression_ratio": 1.6043478260869566, "no_speech_prob": 0.0002694603172130883}, {"id": 95, "seek": 68072, "start": 688.72, "end": 695.76, "text": " Of course, this is not the only possible path that returns to the origin after 18 steps. This one", "tokens": [50764, 2720, 1164, 11, 341, 307, 406, 264, 787, 1944, 3100, 300, 11247, 281, 264, 4957, 934, 2443, 4439, 13, 639, 472, 51116], "temperature": 0.0, "avg_logprob": -0.06146618994799527, "compression_ratio": 1.6043478260869566, "no_speech_prob": 0.0002694603172130883}, {"id": 96, "seek": 68072, "start": 695.76, "end": 703.0400000000001, "text": " also does. Even though it has returned to the origin already, as long as you return at step 18,", "tokens": [51116, 611, 775, 13, 2754, 1673, 309, 575, 8752, 281, 264, 4957, 1217, 11, 382, 938, 382, 291, 2736, 412, 1823, 2443, 11, 51480], "temperature": 0.0, "avg_logprob": -0.06146618994799527, "compression_ratio": 1.6043478260869566, "no_speech_prob": 0.0002694603172130883}, {"id": 97, "seek": 68072, "start": 703.0400000000001, "end": 708.5600000000001, "text": " it still counts, because you will still answer yes in the previous argument.", "tokens": [51480, 309, 920, 14893, 11, 570, 291, 486, 920, 1867, 2086, 294, 264, 3894, 6770, 13, 51756], "temperature": 0.0, "avg_logprob": -0.06146618994799527, "compression_ratio": 1.6043478260869566, "no_speech_prob": 0.0002694603172130883}, {"id": 98, "seek": 70856, "start": 709.1999999999999, "end": 716.3199999999999, "text": " Anyway, the new path also has probability one quarter to the 18. More explicitly,", "tokens": [50396, 5684, 11, 264, 777, 3100, 611, 575, 8482, 472, 6555, 281, 264, 2443, 13, 5048, 20803, 11, 50752], "temperature": 0.0, "avg_logprob": -0.07152738937964806, "compression_ratio": 1.6650717703349283, "no_speech_prob": 0.00041729892836883664}, {"id": 99, "seek": 70856, "start": 716.3199999999999, "end": 723.3599999999999, "text": " when calculating the 18th step return probability, we just add up these probabilities.", "tokens": [50752, 562, 28258, 264, 2443, 392, 1823, 2736, 8482, 11, 321, 445, 909, 493, 613, 33783, 13, 51104], "temperature": 0.0, "avg_logprob": -0.07152738937964806, "compression_ratio": 1.6650717703349283, "no_speech_prob": 0.00041729892836883664}, {"id": 100, "seek": 70856, "start": 724.0, "end": 730.0, "text": " This part is the probability of getting a specific return path, and we then multiply", "tokens": [51136, 639, 644, 307, 264, 8482, 295, 1242, 257, 2685, 2736, 3100, 11, 293, 321, 550, 12972, 51436], "temperature": 0.0, "avg_logprob": -0.07152738937964806, "compression_ratio": 1.6650717703349283, "no_speech_prob": 0.00041729892836883664}, {"id": 101, "seek": 70856, "start": 730.0, "end": 738.0, "text": " by the number of return paths to get the overall probability. So it all boils down to counting", "tokens": [51436, 538, 264, 1230, 295, 2736, 14518, 281, 483, 264, 4787, 8482, 13, 407, 309, 439, 35049, 760, 281, 13251, 51836], "temperature": 0.0, "avg_logprob": -0.07152738937964806, "compression_ratio": 1.6650717703349283, "no_speech_prob": 0.00041729892836883664}, {"id": 102, "seek": 73800, "start": 738.08, "end": 744.96, "text": " the number of return paths. For a total of two n steps, a return path should have the same number", "tokens": [50368, 264, 1230, 295, 2736, 14518, 13, 1171, 257, 3217, 295, 732, 297, 4439, 11, 257, 2736, 3100, 820, 362, 264, 912, 1230, 50712], "temperature": 0.0, "avg_logprob": -0.06414461135864258, "compression_ratio": 1.864516129032258, "no_speech_prob": 0.0011694587301462889}, {"id": 103, "seek": 73800, "start": 744.96, "end": 751.68, "text": " of steps to the left and to the right, and the number of steps upwards is the same as downwards.", "tokens": [50712, 295, 4439, 281, 264, 1411, 293, 281, 264, 558, 11, 293, 264, 1230, 295, 4439, 22167, 307, 264, 912, 382, 39880, 13, 51048], "temperature": 0.0, "avg_logprob": -0.06414461135864258, "compression_ratio": 1.864516129032258, "no_speech_prob": 0.0011694587301462889}, {"id": 104, "seek": 73800, "start": 752.48, "end": 760.64, "text": " Because the total number of steps is two n, we can express j in terms of i. For a return path,", "tokens": [51088, 1436, 264, 3217, 1230, 295, 4439, 307, 732, 297, 11, 321, 393, 5109, 361, 294, 2115, 295, 741, 13, 1171, 257, 2736, 3100, 11, 51496], "temperature": 0.0, "avg_logprob": -0.06414461135864258, "compression_ratio": 1.864516129032258, "no_speech_prob": 0.0011694587301462889}, {"id": 105, "seek": 76064, "start": 760.64, "end": 767.1999999999999, "text": " we can imagine it as a sequence of moves. Because there are a total of two n steps,", "tokens": [50364, 321, 393, 3811, 309, 382, 257, 8310, 295, 6067, 13, 1436, 456, 366, 257, 3217, 295, 732, 297, 4439, 11, 50692], "temperature": 0.0, "avg_logprob": -0.06763268078074736, "compression_ratio": 1.812807881773399, "no_speech_prob": 0.000305342924548313}, {"id": 106, "seek": 76064, "start": 767.1999999999999, "end": 775.28, "text": " in total there are two n factorial arrangements of these moves. However, if for example,", "tokens": [50692, 294, 3217, 456, 366, 732, 297, 36916, 22435, 295, 613, 6067, 13, 2908, 11, 498, 337, 1365, 11, 51096], "temperature": 0.0, "avg_logprob": -0.06763268078074736, "compression_ratio": 1.812807881773399, "no_speech_prob": 0.000305342924548313}, {"id": 107, "seek": 76064, "start": 775.28, "end": 782.4, "text": " we just rearrange these two left moves, then the resulting path quite literally hasn't changed,", "tokens": [51096, 321, 445, 39568, 613, 732, 1411, 6067, 11, 550, 264, 16505, 3100, 1596, 3736, 6132, 380, 3105, 11, 51452], "temperature": 0.0, "avg_logprob": -0.06763268078074736, "compression_ratio": 1.812807881773399, "no_speech_prob": 0.000305342924548313}, {"id": 108, "seek": 76064, "start": 782.4, "end": 789.76, "text": " yet it counted as different in those two n factorial arrangements. Because there are i steps to the", "tokens": [51452, 1939, 309, 20150, 382, 819, 294, 729, 732, 297, 36916, 22435, 13, 1436, 456, 366, 741, 4439, 281, 264, 51820], "temperature": 0.0, "avg_logprob": -0.06763268078074736, "compression_ratio": 1.812807881773399, "no_speech_prob": 0.000305342924548313}, {"id": 109, "seek": 78976, "start": 789.76, "end": 797.28, "text": " left, there are i factorial different permutations in between them, and of course the other directions", "tokens": [50364, 1411, 11, 456, 366, 741, 36916, 819, 4784, 325, 763, 294, 1296, 552, 11, 293, 295, 1164, 264, 661, 11095, 50740], "temperature": 0.0, "avg_logprob": -0.08497182646794106, "compression_ratio": 1.6629213483146068, "no_speech_prob": 0.0003250279405619949}, {"id": 110, "seek": 78976, "start": 797.28, "end": 805.28, "text": " have similar results. These interpermutations all counted as different in those two n factorial", "tokens": [50740, 362, 2531, 3542, 13, 1981, 728, 610, 76, 325, 763, 439, 20150, 382, 819, 294, 729, 732, 297, 36916, 51140], "temperature": 0.0, "avg_logprob": -0.08497182646794106, "compression_ratio": 1.6629213483146068, "no_speech_prob": 0.0003250279405619949}, {"id": 111, "seek": 78976, "start": 805.28, "end": 813.04, "text": " arrangements, yet they should have been counted the same, and so the number of return paths for a", "tokens": [51140, 22435, 11, 1939, 436, 820, 362, 668, 20150, 264, 912, 11, 293, 370, 264, 1230, 295, 2736, 14518, 337, 257, 51528], "temperature": 0.0, "avg_logprob": -0.08497182646794106, "compression_ratio": 1.6629213483146068, "no_speech_prob": 0.0003250279405619949}, {"id": 112, "seek": 81304, "start": 813.04, "end": 821.5999999999999, "text": " fixed value of i is two n factorial divided by all these factorials. So given a value of i,", "tokens": [50364, 6806, 2158, 295, 741, 307, 732, 297, 36916, 6666, 538, 439, 613, 36916, 82, 13, 407, 2212, 257, 2158, 295, 741, 11, 50792], "temperature": 0.0, "avg_logprob": -0.07289664165393726, "compression_ratio": 1.6473988439306357, "no_speech_prob": 0.005059870425611734}, {"id": 113, "seek": 81304, "start": 822.24, "end": 832.0799999999999, "text": " this is the number, and i can range from zero to n, and so the final total number of return paths", "tokens": [50824, 341, 307, 264, 1230, 11, 293, 741, 393, 3613, 490, 4018, 281, 297, 11, 293, 370, 264, 2572, 3217, 1230, 295, 2736, 14518, 51316], "temperature": 0.0, "avg_logprob": -0.07289664165393726, "compression_ratio": 1.6473988439306357, "no_speech_prob": 0.005059870425611734}, {"id": 114, "seek": 81304, "start": 832.0799999999999, "end": 841.5999999999999, "text": " needs to add these up for i ranging from zero to n. Finally, this two n step return probability", "tokens": [51316, 2203, 281, 909, 613, 493, 337, 741, 25532, 490, 4018, 281, 297, 13, 6288, 11, 341, 732, 297, 1823, 2736, 8482, 51792], "temperature": 0.0, "avg_logprob": -0.07289664165393726, "compression_ratio": 1.6473988439306357, "no_speech_prob": 0.005059870425611734}, {"id": 115, "seek": 84160, "start": 841.6800000000001, "end": 850.24, "text": " is one quarter to the two n times the total number of return paths. That's just the case in 2D.", "tokens": [50368, 307, 472, 6555, 281, 264, 732, 297, 1413, 264, 3217, 1230, 295, 2736, 14518, 13, 663, 311, 445, 264, 1389, 294, 568, 35, 13, 50796], "temperature": 0.0, "avg_logprob": -0.09060149874005999, "compression_ratio": 1.5508021390374331, "no_speech_prob": 0.0003459735889919102}, {"id": 116, "seek": 84160, "start": 850.24, "end": 857.84, "text": " What about the 3D random walk? The only difference is that transition probabilities are all one sixth,", "tokens": [50796, 708, 466, 264, 805, 35, 4974, 1792, 30, 440, 787, 2649, 307, 300, 6034, 33783, 366, 439, 472, 15102, 11, 51176], "temperature": 0.0, "avg_logprob": -0.09060149874005999, "compression_ratio": 1.5508021390374331, "no_speech_prob": 0.0003459735889919102}, {"id": 117, "seek": 84160, "start": 857.84, "end": 864.24, "text": " and there is one more pair of directions to consider. The two n step probabilities would be", "tokens": [51176, 293, 456, 307, 472, 544, 6119, 295, 11095, 281, 1949, 13, 440, 732, 297, 1823, 33783, 576, 312, 51496], "temperature": 0.0, "avg_logprob": -0.09060149874005999, "compression_ratio": 1.5508021390374331, "no_speech_prob": 0.0003459735889919102}, {"id": 118, "seek": 86424, "start": 864.32, "end": 872.0, "text": " replacing one quarter by one sixth, and for the number of return paths we use a similar method,", "tokens": [50368, 19139, 472, 6555, 538, 472, 15102, 11, 293, 337, 264, 1230, 295, 2736, 14518, 321, 764, 257, 2531, 3170, 11, 50752], "temperature": 0.0, "avg_logprob": -0.0763864244733538, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.013635439798235893}, {"id": 119, "seek": 86424, "start": 872.0, "end": 879.04, "text": " with the only difference being there are three pairs of directions now. So similarly, for fixed", "tokens": [50752, 365, 264, 787, 2649, 885, 456, 366, 1045, 15494, 295, 11095, 586, 13, 407, 14138, 11, 337, 6806, 51104], "temperature": 0.0, "avg_logprob": -0.0763864244733538, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.013635439798235893}, {"id": 120, "seek": 86424, "start": 879.04, "end": 887.6, "text": " values of i and j, this is the number of return paths, and adding all these for i and j, ranging", "tokens": [51104, 4190, 295, 741, 293, 361, 11, 341, 307, 264, 1230, 295, 2736, 14518, 11, 293, 5127, 439, 613, 337, 741, 293, 361, 11, 25532, 51532], "temperature": 0.0, "avg_logprob": -0.0763864244733538, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.013635439798235893}, {"id": 121, "seek": 88760, "start": 887.6, "end": 895.36, "text": " from zero to n, will give the total number of return paths. And finally, for the two n step", "tokens": [50364, 490, 4018, 281, 297, 11, 486, 976, 264, 3217, 1230, 295, 2736, 14518, 13, 400, 2721, 11, 337, 264, 732, 297, 1823, 50752], "temperature": 0.0, "avg_logprob": -0.058590002620921414, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.0015487228520214558}, {"id": 122, "seek": 88760, "start": 895.36, "end": 902.5600000000001, "text": " return probability, we simply multiply by one sixth to the two n. But perhaps it might be", "tokens": [50752, 2736, 8482, 11, 321, 2935, 12972, 538, 472, 15102, 281, 264, 732, 297, 13, 583, 4317, 309, 1062, 312, 51112], "temperature": 0.0, "avg_logprob": -0.058590002620921414, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.0015487228520214558}, {"id": 123, "seek": 88760, "start": 902.5600000000001, "end": 909.28, "text": " useful to remind ourselves why we care about this. We have demonstrated that a state being", "tokens": [51112, 4420, 281, 4160, 4175, 983, 321, 1127, 466, 341, 13, 492, 362, 18772, 300, 257, 1785, 885, 51448], "temperature": 0.0, "avg_logprob": -0.058590002620921414, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.0015487228520214558}, {"id": 124, "seek": 88760, "start": 909.28, "end": 916.4, "text": " recurrent or transient implies whether the series diverges or converges. So by knowing", "tokens": [51448, 18680, 1753, 420, 41998, 18779, 1968, 264, 2638, 18558, 2880, 420, 9652, 2880, 13, 407, 538, 5276, 51804], "temperature": 0.0, "avg_logprob": -0.058590002620921414, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.0015487228520214558}, {"id": 125, "seek": 91640, "start": 916.4, "end": 923.04, "text": " whether this series converges, we know whether the state is transient. In this chapter,", "tokens": [50364, 1968, 341, 2638, 9652, 2880, 11, 321, 458, 1968, 264, 1785, 307, 41998, 13, 682, 341, 7187, 11, 50696], "temperature": 0.0, "avg_logprob": -0.057127952575683594, "compression_ratio": 1.6572769953051643, "no_speech_prob": 0.0015011137584224343}, {"id": 126, "seek": 91640, "start": 923.04, "end": 929.52, "text": " we calculated those terms in the series with explicit expressions for the 2D and 3D cases", "tokens": [50696, 321, 15598, 729, 2115, 294, 264, 2638, 365, 13691, 15277, 337, 264, 568, 35, 293, 805, 35, 3331, 51020], "temperature": 0.0, "avg_logprob": -0.057127952575683594, "compression_ratio": 1.6572769953051643, "no_speech_prob": 0.0015011137584224343}, {"id": 127, "seek": 91640, "start": 929.52, "end": 937.36, "text": " respectively. And what remains is to show that the series formed by the 2D case diverges,", "tokens": [51020, 25009, 13, 400, 437, 7023, 307, 281, 855, 300, 264, 2638, 8693, 538, 264, 568, 35, 1389, 18558, 2880, 11, 51412], "temperature": 0.0, "avg_logprob": -0.057127952575683594, "compression_ratio": 1.6572769953051643, "no_speech_prob": 0.0015011137584224343}, {"id": 128, "seek": 91640, "start": 937.36, "end": 944.8, "text": " and the 3D case converges. The very quick reason for it is that each term here scales", "tokens": [51412, 293, 264, 805, 35, 1389, 9652, 2880, 13, 440, 588, 1702, 1778, 337, 309, 307, 300, 1184, 1433, 510, 17408, 51784], "temperature": 0.0, "avg_logprob": -0.057127952575683594, "compression_ratio": 1.6572769953051643, "no_speech_prob": 0.0015011137584224343}, {"id": 129, "seek": 94480, "start": 944.8, "end": 952.16, "text": " like 1 over n, so it diverges because the harmonic series diverges. And the other term would scale", "tokens": [50364, 411, 502, 670, 297, 11, 370, 309, 18558, 2880, 570, 264, 32270, 2638, 18558, 2880, 13, 400, 264, 661, 1433, 576, 4373, 50732], "temperature": 0.0, "avg_logprob": -0.060883957406748894, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.0006878324202261865}, {"id": 130, "seek": 94480, "start": 952.16, "end": 958.4, "text": " like 1 over n to 3 halves, and this will lead to a convergent series instead.", "tokens": [50732, 411, 502, 670, 297, 281, 805, 38490, 11, 293, 341, 486, 1477, 281, 257, 9652, 6930, 2638, 2602, 13, 51044], "temperature": 0.0, "avg_logprob": -0.060883957406748894, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.0006878324202261865}, {"id": 131, "seek": 94480, "start": 959.3599999999999, "end": 965.4399999999999, "text": " Originally I wanted to make it into the main video, but this is a bit too much manipulation", "tokens": [51092, 28696, 286, 1415, 281, 652, 309, 666, 264, 2135, 960, 11, 457, 341, 307, 257, 857, 886, 709, 26475, 51396], "temperature": 0.0, "avg_logprob": -0.060883957406748894, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.0006878324202261865}, {"id": 132, "seek": 94480, "start": 965.4399999999999, "end": 971.52, "text": " of expressions, and so I ended up making it a second channel video. But I want to give a bit", "tokens": [51396, 295, 15277, 11, 293, 370, 286, 4590, 493, 1455, 309, 257, 1150, 2269, 960, 13, 583, 286, 528, 281, 976, 257, 857, 51700], "temperature": 0.0, "avg_logprob": -0.060883957406748894, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.0006878324202261865}, {"id": 133, "seek": 97152, "start": 971.52, "end": 978.96, "text": " more no calculation explanation here. We can always think of an inner and an outer region in", "tokens": [50364, 544, 572, 17108, 10835, 510, 13, 492, 393, 1009, 519, 295, 364, 7284, 293, 364, 10847, 4458, 294, 50736], "temperature": 0.0, "avg_logprob": -0.08523370237911448, "compression_ratio": 1.653179190751445, "no_speech_prob": 0.015422443859279156}, {"id": 134, "seek": 97152, "start": 978.96, "end": 986.4, "text": " any dimension, but in higher dimensions there is much more space in the outer region, so once you", "tokens": [50736, 604, 10139, 11, 457, 294, 2946, 12819, 456, 307, 709, 544, 1901, 294, 264, 10847, 4458, 11, 370, 1564, 291, 51108], "temperature": 0.0, "avg_logprob": -0.08523370237911448, "compression_ratio": 1.653179190751445, "no_speech_prob": 0.015422443859279156}, {"id": 135, "seek": 97152, "start": 986.4, "end": 994.56, "text": " have gone out, it will be less likely for you to go back in higher dimensions. The exact cutoff", "tokens": [51108, 362, 2780, 484, 11, 309, 486, 312, 1570, 3700, 337, 291, 281, 352, 646, 294, 2946, 12819, 13, 440, 1900, 1723, 4506, 51516], "temperature": 0.0, "avg_logprob": -0.08523370237911448, "compression_ratio": 1.653179190751445, "no_speech_prob": 0.015422443859279156}, {"id": 136, "seek": 99456, "start": 994.56, "end": 1002.56, "text": " turns out to be between 2 and 3 dimensions. However, does this inner outer region explanation", "tokens": [50364, 4523, 484, 281, 312, 1296, 568, 293, 805, 12819, 13, 2908, 11, 775, 341, 7284, 10847, 4458, 10835, 50764], "temperature": 0.0, "avg_logprob": -0.04934250887702493, "compression_ratio": 1.6832579185520362, "no_speech_prob": 0.012430638074874878}, {"id": 137, "seek": 99456, "start": 1002.56, "end": 1008.56, "text": " and the cutoff between 2 and 3 dimensions look familiar? If you have watched my previous video", "tokens": [50764, 293, 264, 1723, 4506, 1296, 568, 293, 805, 12819, 574, 4963, 30, 759, 291, 362, 6337, 452, 3894, 960, 51064], "temperature": 0.0, "avg_logprob": -0.04934250887702493, "compression_ratio": 1.6832579185520362, "no_speech_prob": 0.012430638074874878}, {"id": 138, "seek": 99456, "start": 1008.56, "end": 1014.9599999999999, "text": " on Stein's paradox, then you might remember that whether the ordinary estimator is admissible", "tokens": [51064, 322, 29453, 311, 26221, 11, 550, 291, 1062, 1604, 300, 1968, 264, 10547, 8017, 1639, 307, 5910, 41073, 51384], "temperature": 0.0, "avg_logprob": -0.04934250887702493, "compression_ratio": 1.6832579185520362, "no_speech_prob": 0.012430638074874878}, {"id": 139, "seek": 99456, "start": 1014.9599999999999, "end": 1022.2399999999999, "text": " has a cutoff between 2 and 3 dimensions, and we found out that recurrence of random walks", "tokens": [51384, 575, 257, 1723, 4506, 1296, 568, 293, 805, 12819, 11, 293, 321, 1352, 484, 300, 18680, 10760, 295, 4974, 12896, 51748], "temperature": 0.0, "avg_logprob": -0.04934250887702493, "compression_ratio": 1.6832579185520362, "no_speech_prob": 0.012430638074874878}, {"id": 140, "seek": 102224, "start": 1022.24, "end": 1028.32, "text": " also has the same cutoff. It turns out that this is not a coincidence.", "tokens": [50364, 611, 575, 264, 912, 1723, 4506, 13, 467, 4523, 484, 300, 341, 307, 406, 257, 22137, 13, 50668], "temperature": 0.0, "avg_logprob": -0.061455390032599956, "compression_ratio": 1.502202643171806, "no_speech_prob": 0.000855865131597966}, {"id": 141, "seek": 102224, "start": 1029.1200000000001, "end": 1034.88, "text": " Larry Brown wrote in 1971 about the connection between these two problems,", "tokens": [50708, 18145, 8030, 4114, 294, 34578, 466, 264, 4984, 1296, 613, 732, 2740, 11, 50996], "temperature": 0.0, "avg_logprob": -0.061455390032599956, "compression_ratio": 1.502202643171806, "no_speech_prob": 0.000855865131597966}, {"id": 142, "seek": 102224, "start": 1035.44, "end": 1042.0, "text": " but this is way too involved in a YouTube video. I will put a link in the description for those", "tokens": [51024, 457, 341, 307, 636, 886, 3288, 294, 257, 3088, 960, 13, 286, 486, 829, 257, 2113, 294, 264, 3855, 337, 729, 51352], "temperature": 0.0, "avg_logprob": -0.061455390032599956, "compression_ratio": 1.502202643171806, "no_speech_prob": 0.000855865131597966}, {"id": 143, "seek": 102224, "start": 1042.0, "end": 1047.76, "text": " who want to know more. Before you go, I just want to thank you for your support, because now I have", "tokens": [51352, 567, 528, 281, 458, 544, 13, 4546, 291, 352, 11, 286, 445, 528, 281, 1309, 291, 337, 428, 1406, 11, 570, 586, 286, 362, 51640], "temperature": 0.0, "avg_logprob": -0.061455390032599956, "compression_ratio": 1.502202643171806, "no_speech_prob": 0.000855865131597966}, {"id": 144, "seek": 104776, "start": 1047.76, "end": 1054.48, "text": " 100k subs. To celebrate this milestone, I am going to make a Q&A video. You can ask me any", "tokens": [50364, 2319, 74, 2090, 13, 1407, 8098, 341, 28048, 11, 286, 669, 516, 281, 652, 257, 1249, 5, 32, 960, 13, 509, 393, 1029, 385, 604, 50700], "temperature": 0.0, "avg_logprob": -0.09292102612947163, "compression_ratio": 1.516260162601626, "no_speech_prob": 0.03513658791780472}, {"id": 145, "seek": 104776, "start": 1054.48, "end": 1060.72, "text": " question, but do so in the google form below. And just so you are prepared for the next video,", "tokens": [50700, 1168, 11, 457, 360, 370, 294, 264, 20742, 1254, 2507, 13, 400, 445, 370, 291, 366, 4927, 337, 264, 958, 960, 11, 51012], "temperature": 0.0, "avg_logprob": -0.09292102612947163, "compression_ratio": 1.516260162601626, "no_speech_prob": 0.03513658791780472}, {"id": 146, "seek": 104776, "start": 1061.28, "end": 1067.36, "text": " and you might have also guessed from these two videos, I am currently a fourth year Cambridge", "tokens": [51040, 293, 291, 1062, 362, 611, 21852, 490, 613, 732, 2145, 11, 286, 669, 4362, 257, 6409, 1064, 24876, 51344], "temperature": 0.0, "avg_logprob": -0.09292102612947163, "compression_ratio": 1.516260162601626, "no_speech_prob": 0.03513658791780472}, {"id": 147, "seek": 104776, "start": 1067.36, "end": 1074.96, "text": " Math student. We call ourselves Mathmos for some unknown reason. So yes, be prepared for some", "tokens": [51344, 15776, 3107, 13, 492, 818, 4175, 15776, 3415, 337, 512, 9841, 1778, 13, 407, 2086, 11, 312, 4927, 337, 512, 51724], "temperature": 0.0, "avg_logprob": -0.09292102612947163, "compression_ratio": 1.516260162601626, "no_speech_prob": 0.03513658791780472}, {"id": 148, "seek": 107496, "start": 1075.04, "end": 1080.56, "text": " Cambridge related Math video in the future. Please consider giving on Patreon,", "tokens": [50368, 24876, 4077, 15776, 960, 294, 264, 2027, 13, 2555, 1949, 2902, 322, 15692, 11, 50644], "temperature": 0.0, "avg_logprob": -0.1868762969970703, "compression_ratio": 1.4085365853658536, "no_speech_prob": 0.051816947758197784}, {"id": 149, "seek": 107496, "start": 1080.56, "end": 1086.72, "text": " and thanks to the Patrons for making this video possible. As always, subscribe with the bell on,", "tokens": [50644, 293, 3231, 281, 264, 4379, 13270, 337, 1455, 341, 960, 1944, 13, 1018, 1009, 11, 3022, 365, 264, 4549, 322, 11, 50952], "temperature": 0.0, "avg_logprob": -0.1868762969970703, "compression_ratio": 1.4085365853658536, "no_speech_prob": 0.051816947758197784}, {"id": 150, "seek": 107496, "start": 1086.72, "end": 1091.2, "text": " like, comment, and share this video. See you next time!", "tokens": [50952, 411, 11, 2871, 11, 293, 2073, 341, 960, 13, 3008, 291, 958, 565, 0, 51176], "temperature": 0.0, "avg_logprob": -0.1868762969970703, "compression_ratio": 1.4085365853658536, "no_speech_prob": 0.051816947758197784}], "language": "en"}