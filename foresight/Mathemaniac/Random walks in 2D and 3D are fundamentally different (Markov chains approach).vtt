WEBVTT

00:00.000 --> 00:05.680
A mathematical saying goes, a drunk man will find his way home, but a drunk bird may get

00:05.680 --> 00:13.040
lost forever. The assumption here is that because both are drunk, they are doing random walks.

00:13.040 --> 00:18.320
The difference is that the man can only walk on the surface of the earth, and so he is doing a

00:18.320 --> 00:24.320
two-dimensional random walk. But for the bird, in addition to the two dimensions, it can also

00:24.320 --> 00:31.920
fly up and down, and so it is doing a 3D random walk. It turns out that no matter where the

00:31.920 --> 00:39.200
drunk man started, it is mathematically guaranteed that he will return home, it's only sooner or

00:39.200 --> 00:46.800
later. But for the bird, there is a non-zero chance that it will get lost forever, even if it started

00:46.800 --> 00:53.920
off in its own nets. Why is there such a stark difference between a 2D and a 3D random walk?

00:54.480 --> 00:58.320
To answer this question, we use the framework of Markov chains.

01:02.480 --> 01:08.480
A Markov chain consists of three things. The first is the state space, essentially meaning

01:08.480 --> 01:14.800
where you can go. In the case of random walks, for simplicity, the state space would be just

01:14.800 --> 01:22.160
those lattice points, so the drunk man or bird can only visit those lattice points. Second thing

01:22.160 --> 01:28.400
is the transition probabilities, because Markov chains are not static. Let's say you are now at

01:28.400 --> 01:35.760
state A, with four other neighboring states in the state space. In your next step, you can transition

01:35.760 --> 01:42.880
to any other states, and in general, you might also go back to itself, depending on your setup.

01:43.680 --> 01:50.480
For each possible transition, we just assign a probability to it. They are the transition

01:50.480 --> 01:58.160
probabilities. For a 2D random walk, each state has four neighbors, and because we have no bias

01:58.160 --> 02:05.840
towards any direction, the transition probabilities are all one quarter. Similarly, for a 3D random

02:05.840 --> 02:12.400
walk, each state has six neighbors, and so the transition probabilities are all one sixth.

02:13.280 --> 02:19.760
The final thing is the initial distribution. In a general Markov chain, you have the freedom to

02:19.760 --> 02:26.880
choose which state to start, and so we can assign a probability to each state, indicating how likely

02:26.880 --> 02:33.520
you are to start at that state. In the case of a random walk, we want to definitively start at

02:33.520 --> 02:41.520
the origin, i.e. a probability 1 of starting at the origin, and 0 everywhere else. Wait,

02:41.520 --> 02:47.360
didn't you say it doesn't matter where you started for the 2D case? Don't worry, I'll get there in

02:47.360 --> 02:54.240
a minute. These are the three elements in the Markov chain, but there is one key feature that

02:54.240 --> 03:01.360
makes it a Markov chain, the Markov property. This is a simple idea that once you have gone to a

03:01.360 --> 03:08.320
particular state, you should have already forgotten how you get there, and consider the transition

03:08.320 --> 03:15.040
probabilities from there without caring about the root that takes you there. So yes, that's the whole

03:15.040 --> 03:26.080
setup for a Markov chain, but how does that help? Given that a random walk is random, and every time

03:26.080 --> 03:32.640
you run it, it gives different results, what can we say about that? One thing we can say is whether

03:32.640 --> 03:39.200
you will revisit the origin if you started there. If you are guaranteed to go back, or in other words,

03:39.280 --> 03:46.880
the return probability is 1, then we call that initial state recurrent. If not, then you are not

03:46.880 --> 03:54.160
guaranteed to return, and so the return probability is less than 1. In such a case, we call it a

03:54.160 --> 04:02.240
transient state. Because you are either guaranteed or not guaranteed to return, a state is either

04:02.240 --> 04:10.320
recurrent or transient, there is no third option. It turns out that in 2D, the origin is a recurrent

04:10.320 --> 04:16.640
state, and just because this state is recurrent, we can already infer that it does not matter

04:16.640 --> 04:22.880
whether drunk man started, he will go back to the origin. That takes one or two lines of reasoning,

04:22.880 --> 04:29.520
which I encourage you to do in the comments. On the other hand, the origin in the 3D random walk

04:29.520 --> 04:37.200
is a transient state, so the drunk bird may never return. The problem now is how to determine that

04:37.200 --> 04:44.240
return probability. Here's the trick. We consider a quantity v, which is the number of returns to

04:44.240 --> 04:51.200
the origin. The implicit assumption is to run the random walk to infinity, even if you have returned.

04:51.840 --> 04:58.000
Our focus is the expectation of v, i.e. the expected number of returns.

04:58.720 --> 05:04.960
Now for the recurrent case, we are guaranteed to go back, but if we continue to run the random walk

05:04.960 --> 05:11.840
by the Markov property, we should have forgotten that we have returned, and then we will for sure

05:11.840 --> 05:20.480
return to the origin again and again. So the number of returns v is guaranteed to be infinite,

05:20.480 --> 05:27.840
and the expectation would also be infinite. That is when the state is recurrent. What if the state

05:27.840 --> 05:35.680
is transient? Here is a very clever general trick. By definition, the expectation is zero times the

05:35.680 --> 05:42.560
probability that v is zero plus one times the probability that v is one and so on. But what if

05:42.560 --> 05:50.480
we write, for instance, this two times probability as actually the sum of two copies, and also do

05:50.480 --> 05:58.160
this similarly for the other terms in the sum. Then instead of summing these row by row, we can

05:58.160 --> 06:05.600
sum these column by column. For the first column, it is exactly the probability that v is at least

06:05.600 --> 06:12.560
one. Similarly, the next column gives the probability that v is at least two and so on.

06:13.360 --> 06:21.040
The reason this method is useful is that if v is at least one, that exactly means you return to

06:21.040 --> 06:27.920
the origin at some point, and so this is actually the return probability. And for simplicity,

06:27.920 --> 06:35.760
we denote it as r. What about the probability of returning at least twice? Well, the probability

06:35.760 --> 06:42.240
that you will return is r, but again by the Markov property, you should forget that you have returned

06:42.320 --> 06:49.600
and so the probability that you return again is to multiply by another r. And in general,

06:49.600 --> 06:57.520
the probability of returning at least k times is r to the k. So if we go back to the expectation,

06:58.080 --> 07:05.120
each column actually sums up to a power of r, and the expectation is a geometric series.

07:05.760 --> 07:11.520
So we know what this should sum to if that return probability is less than one,

07:11.520 --> 07:18.480
which is precisely the case when the state is transient. For now, we only need to know that

07:18.480 --> 07:26.160
this is finite if r is less than one. However, we have also deduced that for a recurrent state,

07:26.160 --> 07:33.840
this expected number of returns is infinite. So if the state is recurrent, we have the expectation

07:33.840 --> 07:40.960
to be infinite, and if the state is transient, the expectation is finite. Because the state is

07:41.040 --> 07:48.000
either recurrent or transient, no third option, if say the expectation is infinite,

07:48.000 --> 07:55.840
the state could not possibly have been transient, and so we can infer that the state is recurrent.

07:56.640 --> 08:03.600
So if we know whether this expectation is infinite, we are done. To do that, we need a final trick.

08:04.400 --> 08:09.680
Another way of thinking about v, the number of returns, is that it is a tally.

08:10.240 --> 08:17.280
We first ask the questions, have you returned at step n? If yes, then we add one to the tally,

08:17.280 --> 08:23.760
and if not, then we don't do anything to it. Once we answered all these questions,

08:23.760 --> 08:29.680
we add up the total, and we obtain v by just adding up all these plus ones.

08:30.400 --> 08:37.920
If we are asking for the expected value of v, we add up the expected value of the first question,

08:38.000 --> 08:45.920
the expected value of the second question, and so on. The expected value of these yes-no questions

08:45.920 --> 08:53.360
are much easier to handle. The expected value of this question would be, by definition,

08:53.360 --> 08:59.600
one times the probability that you answer yes, plus zero times that you answer no.

09:00.320 --> 09:06.320
Well, that's just the probability that you answer yes. We usually denote this probability

09:06.320 --> 09:13.920
as p with subscript zero zero and superscript one. The double zero denotes going from the origin to

09:13.920 --> 09:20.880
the origin, i.e. revisiting the origin. And the superscript just represents at which step you

09:20.880 --> 09:28.240
revisit. These probabilities will be the expected value for each question, and the sum of these

09:28.240 --> 09:36.160
probabilities gives us the expected value of v. So we now have a way of explicitly computing

09:36.160 --> 09:43.280
the expected value of v. Previously, we deduced that we can simply use the expected value of v

09:43.280 --> 09:50.800
to infer whether the state is recurrent, and now we also know how to explicitly compute

09:50.800 --> 09:58.240
that expected value. So once we know whether this series converges, then we know whether the state

09:58.240 --> 10:05.200
is recurrent or transient. This whole argument actually works for all Markov chains, including

10:05.200 --> 10:15.520
the random walk we are considering. In the previous chapter, we essentially devised a test for recurrence

10:15.520 --> 10:23.040
or transient. A 2D random walk is recurrent, so we want to prove that the series diverges.

10:23.040 --> 10:29.760
A 3D random walk is transient, so we want to prove that it converges. But whichever the case,

10:29.760 --> 10:36.320
we need to compute each term. In the case of a random walk, you can't possibly go back to the

10:36.320 --> 10:42.720
origin after one step, and so this probability is zero. This should not be too surprising,

10:42.720 --> 10:48.880
because any return paths would have the same number of steps to the left as those to the right,

10:48.880 --> 10:55.760
and of course, same number of steps downwards as those upwards. And so the total number of steps

10:55.760 --> 11:05.040
here has to be even. So this nth step probability is zero, if n is odd, and so we can just focus on

11:05.040 --> 11:13.120
finding the even two nth step probabilities. Let's say we want a total of 18 steps, then this is a

11:13.120 --> 11:20.720
specific possible return path. This specific path has a probability of one quarter to the 18,

11:20.720 --> 11:28.720
because each step is chosen with probability one quarter, and there are 18 steps you need to take.

11:28.720 --> 11:35.760
Of course, this is not the only possible path that returns to the origin after 18 steps. This one

11:35.760 --> 11:43.040
also does. Even though it has returned to the origin already, as long as you return at step 18,

11:43.040 --> 11:48.560
it still counts, because you will still answer yes in the previous argument.

11:49.200 --> 11:56.320
Anyway, the new path also has probability one quarter to the 18. More explicitly,

11:56.320 --> 12:03.360
when calculating the 18th step return probability, we just add up these probabilities.

12:04.000 --> 12:10.000
This part is the probability of getting a specific return path, and we then multiply

12:10.000 --> 12:18.000
by the number of return paths to get the overall probability. So it all boils down to counting

12:18.080 --> 12:24.960
the number of return paths. For a total of two n steps, a return path should have the same number

12:24.960 --> 12:31.680
of steps to the left and to the right, and the number of steps upwards is the same as downwards.

12:32.480 --> 12:40.640
Because the total number of steps is two n, we can express j in terms of i. For a return path,

12:40.640 --> 12:47.200
we can imagine it as a sequence of moves. Because there are a total of two n steps,

12:47.200 --> 12:55.280
in total there are two n factorial arrangements of these moves. However, if for example,

12:55.280 --> 13:02.400
we just rearrange these two left moves, then the resulting path quite literally hasn't changed,

13:02.400 --> 13:09.760
yet it counted as different in those two n factorial arrangements. Because there are i steps to the

13:09.760 --> 13:17.280
left, there are i factorial different permutations in between them, and of course the other directions

13:17.280 --> 13:25.280
have similar results. These interpermutations all counted as different in those two n factorial

13:25.280 --> 13:33.040
arrangements, yet they should have been counted the same, and so the number of return paths for a

13:33.040 --> 13:41.600
fixed value of i is two n factorial divided by all these factorials. So given a value of i,

13:42.240 --> 13:52.080
this is the number, and i can range from zero to n, and so the final total number of return paths

13:52.080 --> 14:01.600
needs to add these up for i ranging from zero to n. Finally, this two n step return probability

14:01.680 --> 14:10.240
is one quarter to the two n times the total number of return paths. That's just the case in 2D.

14:10.240 --> 14:17.840
What about the 3D random walk? The only difference is that transition probabilities are all one sixth,

14:17.840 --> 14:24.240
and there is one more pair of directions to consider. The two n step probabilities would be

14:24.320 --> 14:32.000
replacing one quarter by one sixth, and for the number of return paths we use a similar method,

14:32.000 --> 14:39.040
with the only difference being there are three pairs of directions now. So similarly, for fixed

14:39.040 --> 14:47.600
values of i and j, this is the number of return paths, and adding all these for i and j, ranging

14:47.600 --> 14:55.360
from zero to n, will give the total number of return paths. And finally, for the two n step

14:55.360 --> 15:02.560
return probability, we simply multiply by one sixth to the two n. But perhaps it might be

15:02.560 --> 15:09.280
useful to remind ourselves why we care about this. We have demonstrated that a state being

15:09.280 --> 15:16.400
recurrent or transient implies whether the series diverges or converges. So by knowing

15:16.400 --> 15:23.040
whether this series converges, we know whether the state is transient. In this chapter,

15:23.040 --> 15:29.520
we calculated those terms in the series with explicit expressions for the 2D and 3D cases

15:29.520 --> 15:37.360
respectively. And what remains is to show that the series formed by the 2D case diverges,

15:37.360 --> 15:44.800
and the 3D case converges. The very quick reason for it is that each term here scales

15:44.800 --> 15:52.160
like 1 over n, so it diverges because the harmonic series diverges. And the other term would scale

15:52.160 --> 15:58.400
like 1 over n to 3 halves, and this will lead to a convergent series instead.

15:59.360 --> 16:05.440
Originally I wanted to make it into the main video, but this is a bit too much manipulation

16:05.440 --> 16:11.520
of expressions, and so I ended up making it a second channel video. But I want to give a bit

16:11.520 --> 16:18.960
more no calculation explanation here. We can always think of an inner and an outer region in

16:18.960 --> 16:26.400
any dimension, but in higher dimensions there is much more space in the outer region, so once you

16:26.400 --> 16:34.560
have gone out, it will be less likely for you to go back in higher dimensions. The exact cutoff

16:34.560 --> 16:42.560
turns out to be between 2 and 3 dimensions. However, does this inner outer region explanation

16:42.560 --> 16:48.560
and the cutoff between 2 and 3 dimensions look familiar? If you have watched my previous video

16:48.560 --> 16:54.960
on Stein's paradox, then you might remember that whether the ordinary estimator is admissible

16:54.960 --> 17:02.240
has a cutoff between 2 and 3 dimensions, and we found out that recurrence of random walks

17:02.240 --> 17:08.320
also has the same cutoff. It turns out that this is not a coincidence.

17:09.120 --> 17:14.880
Larry Brown wrote in 1971 about the connection between these two problems,

17:15.440 --> 17:22.000
but this is way too involved in a YouTube video. I will put a link in the description for those

17:22.000 --> 17:27.760
who want to know more. Before you go, I just want to thank you for your support, because now I have

17:27.760 --> 17:34.480
100k subs. To celebrate this milestone, I am going to make a Q&A video. You can ask me any

17:34.480 --> 17:40.720
question, but do so in the google form below. And just so you are prepared for the next video,

17:41.280 --> 17:47.360
and you might have also guessed from these two videos, I am currently a fourth year Cambridge

17:47.360 --> 17:54.960
Math student. We call ourselves Mathmos for some unknown reason. So yes, be prepared for some

17:55.040 --> 18:00.560
Cambridge related Math video in the future. Please consider giving on Patreon,

18:00.560 --> 18:06.720
and thanks to the Patrons for making this video possible. As always, subscribe with the bell on,

18:06.720 --> 18:11.200
like, comment, and share this video. See you next time!

