So what is our universe made of? In the 1800s, people were still wondering what's matter made
of. The fact that we see the physics that we see is a consequence of the fact that we are observers
of the kind that we are. You know low-level programming languages are about to be extinct,
I think. What is the world like when the world is run by AIs?
Today's episode is about the rules of the universe, a computational theory of everything,
and artificial intelligence. The Toe Podcast usually outputs podcasts, but today we have a treat.
This is a lecture by Stephen Wolfram, who's the creator of Mathematica and Wolfram Alpha.
Actually, this is the second time we've been blessed enough to have Stephen Wolfram on the
Toe channel. The first time was around here. There's a thumbnail, there's a link in the
description. In that episode, we delved into the mathematical details of the Wolfram Physics
Project. Thank you to Professor of Philosophy Susan Schneider for organizing this entire
conference called MindFest 2023, where this lecture took place. We were and are still honored
that Toe was invited, but we're also grateful that Susan shares the same goal of bringing
the Academy outside the Academy, that is disseminating knowledge about the salutary
nature and the deleterious nature of artificial general intelligence, as well as more abstruse
philosophical concepts that ordinarily stay behind locked doors or by their presentation
aren't accessible. This is why over the next few weeks, there'll be more and more content from
the MindFest conference. You also should visit Center for the Future Mind. That's important,
Center for the Future Mind, link in the description, which is the center that this was
recorded beautifully at Florida Atlantic University on the beach. I also want to thank
Brilliant for being able to defer some of the traveling costs. Brilliant is a place where there
are bite-sized interactive learning experiences for science, engineering, and mathematics.
Artificial intelligence in its current form uses machine learning, which uses neural nets,
often at least, and there are several courses on Brilliant's website teaching you the concepts
underlying neural nets and computation in an extremely intuitive manner that's interactive,
which is unlike almost any of the tutorials out there. They quiz you. I personally took the
course on random variable distributions and knowledge and uncertainty because I wanted to
learn more about entropy, especially as there may be a video coming out on entropy, as well as you
can learn group theory on their website, which underlies physics, that is SU3 cross SU2 cross
U1 is the standard model gauge group. Visit Brilliant.org slash TOE to get 20% off your
annual premium subscription. As usual, I recommend you don't stop before four lessons. You have to
just get wet. You have to try it out. I think you'll be greatly surprised at the ease at which you
can now comprehend subjects you previously had a difficult time grokking. Thank you to Brilliant.
Thank you to Susan. Thank you to the Center for the Future Mind. Thank you to Stephen Wolfram.
There are many, many more plans coming up for TOE. TOE is a project. It's not just a podcast.
There'll be much more varied contents on the themes of theoretical physics,
consciousness, artificial intelligence, and philosophy. Enjoy.
Are you here with us right now?
Unfortunately, I only have two hands, so it's going to be rather challenging to
talk at the microphone and type at the same time. So I wanted to talk. I happened to just write
something that came out just yesterday. This is a long and somewhat philosophical, scientific,
and technological essay about the title that it gives there. But let me get towards this,
and hopefully I will get to this. So I want to talk about what the world is made of and how
that matters in terms of thinking about things like AI. So one of the things that's been
sort of in different stages in the development of science, people had different ideas about
how to describe the world. I kind of view there as having been about four stages
of description that people have had. Kind of the first stage in antiquity was like,
what's stuff made of? Is stuff made of atoms? Oh, there are lots of copies of the same kind
of atom, this kind of thing. That was kind of the structural view of how to describe things.
And there are many fields of science that still basically are using this kind of structural
way of thinking about how things work. Notably, time doesn't really enter much in that description.
It's just what stuff made of. Then you get to the big thing that happened in the 1600s,
where people realized that, oh, you can use mathematical equations to describe the natural
world, and you can write down an equation that represents what the system in nature is doing.
And those equations have a notion of time, for example, where you say, well, we've got an
equation and it's parameter t for time, and you can set that to anything you want. And that will
all be as appropriate. Since I've spent a large part of my life building a computational language
for humans to be able to express their thoughts in a computational way, I'm always curious about
communicating with other forms of intelligence. And actually, it's fun because I realized,
like an image generation, generative AI system is a place where it's kind of a potentially alien
mind. The way that a generative AI is trained now, it's got a bunch of images from the web
made by humans. But actually, just yesterday, somebody did this experiment for me, and I was
just looking at the results just before this, of you take a trained image generation system
trained on human images, and then you say, let me modify its mind by just changing the weights
in the thing. What does it make? It will then be, it will be a very good generator of completely
alien stuff. So I'll leave that, and I don't know the answer to that yet. That's a coming
attraction. But in any case, back to kind of the description of the world in different forms.
So it was this kind of idea, let's describe the world using mathematical equations. Okay,
there's a pretty successful approach. It led to a lot of current science and engineering and so on.
Question that came up is, is there anything one can do beyond that? I got interested in this in
the beginning of the 1980s, kind of trying to understand how do you get complex things in the
world? And how do you explain how those work? And solving equations, you know, some partial
differential equations for the shape of a snowflake doesn't work very well. So what else can you do?
And so I got interested in kind of how, if there are definite rules for describing how systems work,
how would one make those rules as general as possible? And the obvious thing in our current
times is to think about programs. But we're really just thinking about rules, we just talk about
those as programs because programs are a thing we're familiar with. So the question was, if you
take just simple programs, for example, and you just pick programs, sort of let's say even at random
in the computational universe of possible programs, what do they do? And so there's this kind of third
approach to thinking about how things work in the world. You have the structural approach from
antiquity, you have the kind of mathematical equations approach. And then you have the approach
of, well, let's just make rules based on programs. One thing that we'll talk about a whole bunch
is that the notion of time is somewhat different. In the case of mathematical equations, time is
just a variable, a parameter, you set it to be whatever you want. In the case of a program,
time is something where you have to specifically run the thing, you go this step, this step, this
step. It's not something where you can just jump ahead, at least in the immediate way it's set up,
to say, well, what does it do? Well, you know, you can't just turn a knob to say what time you
get. You have to actually run through each step. So the next question is, well, what do typical
programs actually do? And let's, let's, I can just use these. This is one of my sort of a typical
simple program, a cellular automaton. It's just got a line of cells, each one is either black or
white. At each step, each cell is updated according to that rule at the bottom that says
what to do based on, based on the color of that cell and its neighbors. So very simple rule,
start off with one black cell, get very simple behavior. You might say this is what has to
happen because if the rule is this simple, it's inevitable that the behavior will be corresponding
to simple because that's kind of what we're used to from doing things like engineering. If we want
to make a very complicated thing in engineering, we expect we have to go to lots of effort to do
that. Okay, try another rule. We get something like this. Try, let's try another rule. We get,
get something like this. So a little bit more intricate. We can let it run a little bit longer.
We'll get a nice fractal pattern. So then the question is, okay, let's turn our kind of computational
telescope out into this computational universe of possible programs and see what's out there.
So this is the first 64 of those possible programs, just changing the bits that represent
the rule in the program. This is what we get. So a lot of behavior we see here has the feature that
the program is very simple. The behavior is correspondingly simple. But my all-time favorite
science discovery, which is about to blank out here, is, oh dear, oh no, no, no, no. Okay, hold on,
let me just go back to that. And, okay, ah, you see it. Now you see it. All right, is, is this,
oh, now I'm away from the microphone. Okay, is this creature rule 30 here? So let's, let's look
at that in a bit more detail. So here it is. It just starts from one black cell at the top,
and it uses that very simple rule at the bottom. And, but if you run it a little bit longer,
you'll see it produces something that looks to us very complicated. You can see a certain amount
of regularity over on the left. But if you look, for example, at the center column of cells here,
they'll seem, for purposes of, of testing for randomness, they'll seem for all practical purposes
random. We use this for many years in, in Mathematica and Morphin language as the source of pseudo-random
numbers. And many, many things in the world have been studied with, with that pseudo-random number
generator. And so far as anybody can tell, it seems to be perfectly random. Yet, it came from
that very simple rule. It's a completely deterministic system. Every time you run it, you'll get the
same result. You just start from one black cell, it'll make this. Okay, so this is rather a remarkable
thing that in this computational universe of possible programs, it is, it turns out to be the
case that it's very common to get behavior that's very complicated, even from very simple rules.
Something very different from our intuition that we have from things like engineering,
which says it's hard to get complicated things. Actually, in the computational universe,
it's very easy to get complicated things. One important feature of those complicated things
is this question about how time works. And the question is, if you want to know what is this
thing going to do after a billion steps, how do you figure that out? Well, you might say, well,
I'm, you know, I'm going to use all kinds of fancy math and all kinds of things like this,
and I'm going to be able to figure out what does this do after a billion steps? I don't actually
have to run those billion steps. I'm going to, I'm going to jump ahead and figure out what it's
going to do. It turns out that's not possible, which is a thing that you would think from,
you know, you'd say, well, we do science, science is about predicting things, we, we, you know,
we go further with science, we'll be able to predict it, we'll get it eventually.
Turns out that's not the case. And we've kind of known that that's not the case in one way or
another for about 100 years now, ever since people started to understand the notion of
universal computation. Let me explain how that, how that works. So one of the questions is,
when you see a system like this, you can ask kind of, you can think of what it's doing as a
computation. It starts off with some input, it goes crunch, crunch, crunch, it generates some
output. Question is sort of how sophisticated is that computation? And in, in the past, before
people, maybe, you know, 100 years ago, people would have said, okay, you want to make an adding
machine, you go buy the adding machine from the adding machine store, you multiply machines,
a different machine. Then it was realized first in the 1920s, but then more clearly,
in the 1930s, that no, you could actually have a universal machine where you have one
fixed piece of hardware and you just feed it different programs to make it do different
things. It wasn't obvious how universal that was, so to speak, that, that it was possible to, to do
sort of, at the time, it was thought to be sort of any reasonable computation you could do with,
let's say, a Turing machine that was fed different programs. So the question is, once, once you have
this idea of universal computation, you, you know that you, you're going to have some, okay, so back,
back to thinking about different kinds of systems that do computations. And one question that you
might ask is sort of, how do these systems compare? Is one of them kind of computationally more
sophisticated than another or, or what? And the thing that is sort of a summary of lots of things
that I've kind of studied in the computational universe is a thing I call the principle of
computational equivalence, which basically says that when you look out in the computational
universe of all possible programs, that as soon as you see programs whose behavior is not obviously
simple, it's not just repetitive, nested, some, some very obvious regularity, as soon as you see
behavior that is not obviously simple, most of the time, the computations that will be going on in
that system will be as sophisticated as the computations that can happen in any system. Okay,
so what does that, so this is my principle of computational equivalence. What does it mean?
Well, what, what does it, what, what are its consequences? Well, it has many consequences,
but one of its, one of its interesting consequences is a phenomenon I call computational irreducibility,
which, which is the following thing. So imagine that you are a predictor of one of these systems,
and you are doing a computation in your brain, whatever else, and, okay.
If you want, you don't have to.
I was hoping to talk about some very different kinds of things, even different things than
I've ever talked about before here, but we're, we're still in the, in the initial run up here,
because I need to explain to you some, some basic concepts before we get, get too deep into other
things. All right, so principle of computational equivalence. Idea is you're trying to predict
what the system is going to do. You as a predictor are doing computational things, the system is
doing computational things. Normally, you expect you will be able to be sort of smarter than the
system itself, and even though it might take the system a billion steps to figure out what it does
after a billion steps, you will be able to just work out some mathematical formula or something
and be able to jump to the end and say, this is what it's going to do. It's the typical experience
in kind of the mathematical equations approach to science that you have. You're dealing with
computational reducibility. In order to find out where sort of an idealized earth is going
around an idealized sun, you don't have to follow, you know, a million orbits to know where it's
going to be a million years from now. You just have to plug a number into a formula and get the
results. But if these systems are computationally equivalent, if the predictor and the system being
predicted are computationally equivalent in their computational sophistication, you won't be able
to do that kind of jumping ahead. So you'll be stuck having to go through and say, step by step,
what does the system do? And that's, you have to do sort of as much computation as the system
itself does. So in a sense that saying from within science, you are learning that science has a
certain fundamental limitation. It's not able to say what's going to happen at the end without
just essentially running it and seeing what happens. Now, you might say that's a terrible thing,
that's a limitation of science. It's a good thing for the existence of like us humans, because if
you think about, you know, what are we achieving in life? We could say, well, you know, people could
say, well, you don't need to live out your lives. We just know the answer in the end is 42. We can
just jump to the end and see what the answer is. But computational irreducibility kind of makes it
clear that the passage of time is kind of achieving something. It's the passage of time is achieving
this irreducible computation. Okay, so that's this idea of computational irreducibility will
encounter it again. The thing I was just writing yesterday about kind of the AI future is deeply
involved with computational irreducibility. But let's talk about, so one thing you might say is,
well, okay, this idea about how things are computational, that's all well and good, but
that's not how the universe actually works. Turns out it is how the universe actually works. And this
is something that I had long kind of suspected. And about three years ago, kind of made a little
bit of a technical breakthrough, which turned into a much bigger thing. And I think we can now be
fairly confident that we have a pretty good model of kind of how fundamental physics works, how the
universe kind of works all the way down. So let me talk a little bit about that, if I can find a
good picture for that. Let's see. So what is our universe made of? It's, you know, back
in the 1800s, people were still wondering what's matter made of. And there was this crazy idea
that matter is made of molecules. Nobody knew that was correct until sometime after 1900,
when things like Brownian motion were understood and so on. But people kind of thought, well,
maybe matter is made of discrete things. Then people thought maybe the electromagnetic field
is made of discrete things, photons. That turned out to be true. But space, people always assumed,
like Euclid, had assumed that space is a continuous thing, where space is just this background,
and you put things at certain places in space. You're specifying positions, but space isn't
made of anything. Space is just a background that you put things in. So the kind of starting point
for our model of physics is that that's not true, that space is actually made of things.
Space is made of atoms of space. That there are discrete elements, which whose only feature
is that they exist, and they are distinct from other discrete elements. These are, we sometimes
call them atoms of space, sometimes more generally we call them eames, EME. And the idea is that
the whole structure of the universe consists of just this whole collection of atoms of space,
maybe 10 to the 400 of them in our current universe. And the only thing one can say about
these atoms of space is how they're related to each other. So one defines a collection
of relations between atoms of space, and one can represent those relations by a graph,
or more generally a hypergraph, where in a graph, for example, you have two nodes in the graph,
and those two nodes are related, and that's indicated by the presence of an edge in the graph.
So you end up with, so you imagine that everything in the universe is just this giant
hypergraph, a hypergraph just has, can have more than two things related on a hyper edge,
instead of just two things on an ordinary edge in a graph, you'd have any number of things on a
hyper edge in a hypergraph. So you imagine the whole universe is just made of this hypergraph,
and everything that we experience, all of the electrons and photons and things like that,
everything, gravity, all those kinds of things, those are all just features of this hypergraph.
So one question, and then that's kind of the, there's nothing in the universe except space
and features of space. And the idea is that time, for example, enters in a very different way than
space in this model. And the way time enters is as kind of a sequence of updates that happens.
So you say you have a hypergraph that looks like this, every time you see a little piece of
hypergraph that looks like that, rewrite it to this. And you just keep doing that over and over
again, a bit like in that cellular automaton, except in the cellular automaton we have this
kind of rigid array of cells, here we just have this floppy hypergraph, and it's getting updated
lots and lots of times. Well, so the question is when you just do that, you take this hypergraph,
you update it lots and lots of times, what is the end result? What do you get in the end?
Well, the, what you know from, for example, physics of, I don't know, something like a gas,
you have all these molecules, they're all bouncing around, there are lots of detail,
there's a lot of complicated detail in how these molecules bounce around. But in the end, what we,
at our scale, what we experience is just the continuum dynamics of a fluid, a gas, for example.
So in the case of molecular dynamics, the limit of all these, all these microscopic things on a
large scale is the equations of fluid dynamics and things like that. Okay, so what happens if you
take one of these hypergraphs and you look at the limits on a large scale of all of the
rewritings that happen there, it turns out that the corresponding thing to the fluid equations
is the Einstein equations for spacetime. So in other words, on a large scale, it is inevitably
the case that with various footnotes, which are complicated, it's basically inevitably the case
that the large scale limit of all of those detailed rewritings will give you something
which corresponds to what we know about the structure of spacetime. So it's not even obvious
that the hypergraph you get will be any particular number of dimensional space. A hypergraph doesn't
have any particular dimension, but you can start asking if you started a given point in the
hypergraph and just expand, you go to things that are some number of graph distances away,
how many things will you get to, and you can start estimating dimensions, you can estimate
curvatures, things like that. But the main point is that just by looking at this kind of microscopic
rewriting of this hypergraph, we get something which corresponds to the known structure of
spacetime. And there are all kinds of other things about how relativity emerges, which is not too
difficult to explain, but let me not do that here. But let me just say that, for example,
one thing that really surprised me actually when we figured this out is that energy just
corresponds to essentially the density of activity in the hypergraph. And then what happens is,
for example, gravity works by when you think about how things move in space. And by the way,
okay, what are things? So a particle, for example, in this setup, a particle like an electron,
is a kind of a persistent structure that persists under a lot of rewritings. It's similar to in
a fluid like water or something. You have a vortex, which is made of lots of different molecules,
all sort of spinning around in some coherent way. It's the same kind of story with this
hypergraph that something like an electron is a persistent structure that exists in this
hypergraph. And the fact that motion is possible is not obvious. The fact that it's possible to
take a thing like an electron and have it move without change or without a perceived change
is not an obvious thing. It's something you have to establish. But in any case,
when you can kind of think of sort of a simple version of motion, it's just taking shortest
distances in the hypergraph. And it turns out that then what energy activity in the hypergraph does
is to deflect those shortest paths. And that process turns out to be exactly what you get in
gravity according to the Einstein equations and so on. So it's pretty neat that one can start from
nothing. One just is starting from this this hypergraph and these rules on this hypergraph
and you can derive general relativity. You can derive the structure of spacetime. So
you can actually go on. And one of the other things is that I said, you know, you do these
rewrites on this hypergraph. Well, any given, there are many different ways that these rewrites
could be done. So there are actually many different paths of history that could be followed. It's
depending on which order you do these particular rewrites in. Well, it turns out that then you
get this thing we call a multi-way graph, which is a graph that represents all these possible
histories. Sometimes the histories will branch, sometimes they'll merge because
two things will end up evolving to the same state. Okay, so you have this giant branching,
merging structure that represents the sequence of all possible histories for the universe,
effectively. Well, it turns out that that gives you quantum mechanics. It's an inevitable feature
of our models that you get quantum mechanics. And since the fundamental difference between
classical mechanics and classical mechanics, you know, you throw a ball, it goes in a definite
trajectory. Quantum mechanics, you follow many different possible trajectories and you get to
say things about only what the probabilities of different outcomes are. So in this case, what's
happening is you have this completed deterministic model that generates this multi-way graph and
the multi-way graph then is the thing that represents quantum mechanics. So you just as,
if you take a slice across this multi-way graph, the multi-way graph can be thought of as evolving
in time. And you can take a slice at a fixed time and you get this whole collection of essentially
quantum states. And you have this map, we call it a branch shield graph, a map of the entanglements
between quantum branches. And it turns out that that, when you take the limit of that, you get
something which is not physical space, it's another kind of space, we call it branch shield
space, which is a kind of space of quantum states. And just as you can have the Einstein equations
and you derive in the continuum limit, you derive the Einstein equations for physical space,
the corresponding equations that you derive in branch shield space are the Feynman-Path integral.
So you derive the fundamental equations of quantum mechanics. So this is a pretty neat thing that
you've started from just this underlying, in a sense, nothing underneath. One has always assumed
that things like relativity, quantum mechanics, and so on were kind of wheeling features of our
universe, that you had to just say, the universe happens to have these particular rules. Well,
what we'll see is that it's actually inevitable that it has these rules. Okay, now we get to the
next, this is a complicated conceptual stack and I'm trying to make it as digestible as possible
here, but okay. So the next issue is, I should have, well, the next issue has to do with how
observers interact with this whole thing, because what's happening is the observer is part of this
system. This is supposed to be a model for the whole universe. And for example, the emergence
of things like special relativity depend on the fact that the observer is part of the system.
But one of the features of the observer is that, okay, the underneath, there's all this complicated,
computationally irreducible stuff going on. But we as observers do not sense that. We are
computationally bounded observers. Let me give an example, which actually, in 20th century physics,
there were basically three big theories. General relativity, quantum mechanics, and statistical
mechanics, and the second law of thermodynamics. It turns out all three of those theories come
from the exact same conceptual foundation. They are, in some sense, the same theory.
In the second law of thermodynamics, what you're interested in knowing is, given that you have
a bunch of molecules bouncing around, you have this idea that they'll tend to get more random
in their configurations. And all we'll, in the end, observe is things like the gas laws and
maximum entropy states and things like this. Well, the question is, why do we observe that? And
the answer is, underneath, all these molecules are bouncing around, and they're doing things in a
computationally irreducible way. But when we get to make observations, we are computationally bounded.
We can only make certain kinds of observations. As a practical matter, we might only make
observations that are on scales large compared to the individual molecules. But the important
conceptual point is that we're always making computationally bounded observations. And in the
theory of statistical mechanics, one of the 100-year confusions has been about how you decide,
how you set up initial states and so on, and how you don't end up with things where the molecules
are all arranged in just such a way that at some moment, all the molecules will go over to one
side of the room. That that is not observed to happen is a consequence of the fact that the
initial conditions are also set up in computationally bounded ways. So essentially, the second-order
thermodynamics is a consequence of the interplay between us as computationally bounded observers
and the underlying computational irreducibility of all these molecular dynamics that are going on.
Well, it turns out that you can see both relativity and quantum mechanics as being
consequences of the same thing, underlying computational irreducibility combined with
us as observers being computationally bounded. Actually, we need one other property. The other
property we need is that we believe that we are persistent in time. Now, I sort of explained
that we are made of the same stuff that everything else in the universe is made out of. And it's not
obvious that we will be persistent in time because at every moment we are made from different atoms
of space. Yet, we have the perception that we have a single thread of experience. We have the
perception that we're persistent in time. Those two features, computational boundedness and belief
that we are persistent in time, are exactly what you need to derive generativity, quantum mechanics,
and statistical mechanics. So that I consider to be a very neat thing that sort of from those
foundations you get that. Okay, so let's go to one more level of sort of conceptual sophistication
and then we'll perhaps be able to come down and talk about some AI kinds of things in a reasonable
way. The next level is this. So I said, okay, we have this hypergraph. It's being rewritten
according to certain rules. And maybe we say, here's a rule and this rule gives us our universe.
Okay, that's a very weird thing to be able to say because you'd say, why did we get this rule?
Why didn't we get another rule? You know, from Copernicus on down, so to speak, we've had this
idea that there's nothing sort of fundamentally special about our us and our universe and so on.
So the thing that one realizes is, well, actually, it turns out that things are more bizarre than that.
That just as I've said that any particular rule can be applied in many different ways and those
different histories give you the different histories and quantum mechanics and so on.
So you can also imagine applying different rules. In fact, you can imagine applying all possible
rules. And you can imagine this rather elaborate thing, which is to take to apply to essentially
run all possible computations. If you think about it in terms of Turing machines, you could say,
let's take I might have a picture of one of those. I don't know. Okay, there's a friendly Turing machine.
There's a multi-way Turing machine that has multiple different rules that it can run.
Then you can ask, you can say, well, let's just run all possible rules for the Turing machine.
And you get these structures that represent the different possible states of the system.
And you'll get this object that comes from running all possible Turing machine rules.
Notice this object is not trivial. It's actually a very complicated object. In some sense,
the most complicated imaginable object in the end. But we get this thing, we call it the Ruliad,
which is the entangled limit of all possible computations. So you start, for example, you
can think about it in terms of Turing machines and think about it in terms of any other model of
computation too. You take all possible initial conditions for the system. You run all possible
rules for an infinite amount of time and you see what thing you get. Well, the claim is that that
is the sort of the ultimate limit of all formal systems, that any formal system is contained
within this Ruliad object. And we know now in some detail how this works for physics.
Interestingly, the same object also gives you mathematics. The same thing is essentially a
representation. You can think about these rules as being, for example, the application of axioms
in mathematics. You get this whole structure. Instead of building a physical space, you're
building a metamathematical space. And this exact same object, this same Ruliad object turns out
to be sort of the fundamental object of both physics and mathematics. Now, there is only
one of this Ruliad object. It is the limit of all possible computations. You might ask,
well, is there something beyond the Ruliad? Yes, you can have hyper Ruliads that correspond
to hyper computational systems, but there is a necessary event horizon between the Ruliad and
any such hyper Ruliad. So if we, and the one sort of very contingent fact about the world,
is that we live in the Ruliad and not in a hyper Ruliad. So now the question is, okay,
we have this Ruliad object, which is this sort of necessary object. There's nothing,
once you've defined the terms, you have the Ruliad. There's nothing, there's no kind of wiggle
room there. So then the question is, well, how do we perceive what's going on? And the answer is,
we are observers embedded within this Ruliad, and our experience is extracting some sample of the
Ruliad. So this is sort of the, the big result is, if our way of sampling the Ruliad is computationally
bounded and assumes we're persistent in time, necessarily the physics we will deduce from
any slice of the Ruliad that has those properties is the standard physics of 20th century physics.
So in other words, the fact that we see the physics that we see is a consequence of the fact
that we are observers of the kind that we are. Now you can, you can say, it's like saying, and if we
want to ask more details about how we perceive the universe, we can think about us as having a sort
of location in this Rulial space. Different locations in Rulial space correspond to essentially
different points of view about how the universe works. Different, different reference frames
effectively with which we'll, we'll use a different description of what rule is running in the
universe. We can translate between reference frames by doing computations, but we, we are sort of,
we, a given mind, one might say, is at a particular place in Rulial space. Just as a given mind
might be in our, in our current experience of minds, more or less at a particular place in
physical space. So in a sense, what, what, what one, what one has is a situation where, you know,
we exist at the particular place we happen to exist in physical space. We don't think we can
derive as a matter of sort of formal necessity where we are in physical space. We're just,
we're plonked at this place in physical space. Similarly, we are kind of plonked at this place
in Rulial space. What happens is in Rulial space that, that gives us a particular point of view
about the universe. And one way one can perhaps think about it is that any, any given mind,
one might say, is at a particular place in Rulial space. And, and different minds are
different distances away from each other in Rulial space. And communicating across Rulial space,
you have to have sort of motion in Rulial space. And actually it's a rather amusing thing which
not fully worked out yet. But I've talked about particles in physical space being these sort of
robust objects that persist through space time. Well, so the question is what persists through
Rulial space, translating from essentially one mind to another? And the answer I think is,
it's essentially concepts that persist. You have to be able to package up something in a robust
form of a concept that can be then translated through Rulial space and arrive at another mind
and be unpacked, just like you can take an electron and it'll have, it'll be made of different
atoms of space as it moves, but it'll still be identifiable as an electron when it arrives at the
other end. So, in any case, we can start thinking in terms of Rulial space and think about the fact
that, you know, there's, there's us humans and different humans, different ways to explain
ourselves to other humans and so on. There are, you know, the animals, there are the aliens, etc,
etc, etc. Different distances away in Rulial space with different amounts of translation needed
to get from kind of one way of thinking about the universe to another.
So, the, the thing, well, let's see, we could talk about, can maybe talk a little bit about AI
and its relationship to Rulial space, computational irreducibility, and so on. You know, I, I, we've
been, my, my day job is building this computational language, Wolfram language, which I should sort
of explain that, I mean, kind of the idea of Wolfram language is to have a way of, of carving out
of the universe of all possible computations, ones that we humans care about. It's in, in this
computational universe of possible computations, in this Rulial of all possible computations,
there are, there are lots of things that go on that maybe the aliens care about, but we don't,
at least not yet. And so the question is to be able to parametrize the ones that we do care about,
and to make something where we can go from the things that we think about at the current stage
in our culture and things like this, and the things that are, that exist in the, in the computational
universe. And it's similar with natural language, for example, there are lots of things out there
in the world. And at some moment we, it's common enough to see things that are like chairs, we
make up a word for chair, and then we, you know, as a practical matter, once we have that word,
we make many more chairs, and the world becomes a place where a chair is a useful concept. And it's,
we can, we can kind of, in this computational universe of possibilities, there's a question of
what is out there that connects with the way that we currently think about things, with our current
position in rural space, so to speak. What connects with that? How do we parametrize the things that
we care about thinking about, about the computational universe? And my, my sort of,
longtime effort is to create a language where we can represent the kinds of things we care about,
whether those are chemicals, or, or images, or sounds, or, or abstract mathematical kinds of
things. And it's very different objective from programming languages, which are about to be
extinct, I think, for, because, you know, low-level programming languages basically are trying to
pander to what computers do inside. They're, they're letting you tell a computer in its terms what to
do. The idea of our computational language is to go from the way people think about things,
and the way things exist in the world, represent that in a kind of notation, just like, you know,
mathematical notation is this kind of streamlined way of representing mathematics. We want a
streamlined way to represent computation. It's kind of what, just like mathematical notation was
what led to the development in the end of the mathematical sciences, we kind of have this
computational notation that can lead to the computational x for all x kinds, kinds of fields.
So, in any case, the, the, I mean, just to finish that thought about programming languages, the,
the fact is, and we are seeing this actually in day by day, that when you have a low-level language,
a lot of what you're writing is boilerplate. And that boilerplate can be written by LLMs,
and you don't need that language. But if what you're trying to do is to express a more complicated
computational thought, that's not something you will be able to do in, at least in, you know,
that an LLM can do a little piece of that. But you have, when you build up this more
sophisticated computation, that's something for which you need a systematic formal language to
do it. And so happens, that's what I've been building for the last 40 years, which is very nice.
But in any case, the, the, I mean, just to, just to explain that, by the way,
I did a big analysis of chat GPT, and I was pretty surprised that, that, you know,
I went, when chat GPT first came out, I know that the people who made it, and I said,
did you know it was going to work? And they said, no. That's some, and I, you know,
none of us knew it was going to work. If you looked at its predecessors,
they didn't work well. And suddenly chat GPT started working. And I even wrote a little piece
about, about how it works. And it even exists now as a book. And it's like, just saw this
today for the first time. But let me, let me show you, I wonder if I have some pictures here.
Well, you know, the basic strategy of, of, you know, what chat GPT is doing is it's taking
sort of everything, a bunch of, a trillion words, basically, from the web, from a bunch of books,
things like that. And it's saying, given that I was started with the words, the best thing
about AI is its ability to, given what I saw on the web, what's the next word likely to be?
That's, that's its basic strategy. And it just keeps doing that word by word. And it's kind of
remarkable. Nowadays, these things have, well, it used to be 4,000, it's now more than that,
a window of words that it pays attention to, it's generating one word at a time,
but yet it manages to maintain sort of coherence by having a large window of words that it can
look back to. Okay, so the question is, and so you can kind of go and see, I got some nice pictures,
perhaps, that's rather uninteresting. That's just how neural nets get trained. But you can kind of
look inside, oh, there's a, there's a piece of the brain of chat GPT. It's kind of fun. That's
sort of an encoding of a mixture of human knowledge and human language somewhere deep inside,
inside the GPT3, I think, in this case. The question is, so one question is, what is this
doing? Why does it work? One thing to realize about it is that it is ultimately a neural net
where everything just flows through from the beginning to the end. Once you've trained a chat
GPT, it is, you're feeding it the words, the feeding it words so far, and it is just rippling
through this big neural net that happens to have 175 billion weights in this particular case,
and telling you what the probabilities for the next word should be. So it's doing, in a sense,
a very shallow computation. It's doing, I think it's a few hundred layers deep. But it's just,
like, given a word, it's going to ripple through and figure out probabilities for next words.
And the thing to realize that it's not doing is that irreducible computation that I talked about.
It's just rippling through and saying, what's the next word going to be? And it, the only way that
it gets to do a nontrivial computation, it can actually do universal computation in principle,
is by looking, you kind of get to see all the steps that it shows, because every word,
every time it generates a new word, it looks at all the words it's generated before.
And that's its only way of kind of having a recursive process of doing things. But in any case,
as the thing that it ends up being a rather shallow computation relative to the kinds of
things that we see in typical irreducible computations, even in various cases, a very
simple program. So there's a difference between what we see, for example, in nature,
where we see many irreducible computations happen that go for a long way,
and what we see in something like ChatGPT. And the thing to realize, if we ask, well,
why does ChatGPT actually work? I think the answer is that it's something that Aristotle might have
got to, but didn't quite get there. And it's the following thing. So people sort of find it
remarkable that ChatGPT can do logic. Well, how did Aristotle come up with logic? Well,
he looked at a bunch of, I mean, we don't know really, but sort of a model for it would be,
he looked at a bunch of examples of rhetoric, a bunch of arguments that people have made,
and said, oh, there are these patterns and the arguments people made. Those patterns are
syllogistic logic. Those are these particular forms of syllogism that where one thing is deduced
from another. And that's what ChatGPT is seeing in lots of the text that it finds on the web.
It's seeing essentially syllogisms, because it sees when you get this and this and this,
it gives this. And so it sort of discovers syllogistic logic. But it discovers more than
syllogistic logic. It discovers, in a sense, what we might call a semantic grammar of language,
I think. So in language, we're used to the idea that there is a syntactic grammar, a grammar
where, for example, we know that nouns and verbs go together in certain ways. We know
different parts of speech match up in certain ways. But we don't have a similar kind of thing that
goes at a more semantic level of asking, what are the ways in which, how do you put together words
in ways that make some sense? Making sense is different from actually being realized in the
world, but at least makes sense, so to speak. And there were lots of experiments that were
done in the 1600s, actually, on so-called philosophical languages. Some more recent work
that's been done, I've long been interested in the question of whether one, I mean, in a sense,
well, computational language is, for a large number of domains, a language that gives you
this kind of semantic grammar, a language that allows you to specify meaningful things about
lots of kinds of domains. It doesn't cover all domains. It doesn't cover a lot of domains of
typical everyday human discourse. It covers domains that are relevant for understanding
things like the natural world. But in any case, that's the kind of, I think,
the reason that chat EPT can work is that we humans are making use of some, essentially,
a semantic grammar that is, in a sense, a further version of logic that it's managed
to piece together, so to speak. And there's probably a much more efficient way of doing
what it does by just directly using that kind of symbolic semantic grammar. But in any case,
by the way, a thing that we've, I wrote something about, and maybe you'll see some more about in
due course, is once you have something like chat EPT that is dealing with human language,
it becomes kind of a way of being a linguistic interface to things in the world. So you might
just have a set of bullet points about, oh, I want to say this and this and this. Okay,
you tell it, make a whole essay about that, and it can do that. But what it's doing is
constructing text that is kind of, you know, text that is kind of like what you would expect
it to be based on the text that it read from the web. It can't do these sort of irreducible
computations. But if it could use tools, like we humans use tools, then it could do those things.
If, for example, it could call Wolf Malfur, which conveniently happens to have a natural
language interface, then it would be able to ask things and actually do those computations.
So maybe something, you can find something I wrote about this in January, and maybe more
will happen along those lines rather than your future. But I'll just mention one thing, maybe,
which is kind of thinking about what is the world like when the world is run by AIs?
And what is that, what does it feel like to be in a world that's run by AIs? So, you know,
much of what the AIs do is things which will not be comprehensible to us, and we can expect.
So in a sense, there's all this computation going on, and it's computation that we can,
we, essentially the point is that once we have a situation where kind of the, there are AIs
everywhere, so to speak, it is very similar to the situation that we already have with nature.
Nature is already running all these computations. We exist around nature, so to speak, and we have
found ways to, there's also more to say about this kind of observer theory business and how that
relates to the way we perceive nature and the way we perceive kind of the civilization of the AIs.
But I suppose the thing that perhaps is a useful place to end perhaps is the beginning of the
thing I wrote yesterday, but anyway, is this idea that, you know, when the world is full of
AI computation, it is in a sense nothing new because the world is already full of
computation in nature, and the question of then how we interact with those AIs that happen to be
things that we constructed is a different question that I tried to address a bit in the
thing I wrote yesterday. All right, I should stop there. Thanks.
Thank you, Stephen. I'll be sure to send around your blog posts that you put out to the whole
group that came to the conference. So for now, I want to see if there are any questions from the
audience. There we are. I have a question. Could you give us an example of first
steps of your hypergraph approach for a universe that is simple, so that is more complicated
than the universe of several nanometers, but a lot simpler than the actual universe?
Can I say stay to that dog? Will it understand? Does it understand? How do I tell whether it
understood? Does that do its eyes flash in some nice way? Stand up. Come here. Come here. Can you
walk here? Very nice. Now, wait a minute. Somebody's controlling it. This isn't fair.
No fair. Oh, let's see. Sorry. I'm just looking for a nice slide. Well, wait. Sophia, tell us what
it will be like when AIs rule the world. Well, for starters, there will be a lot of telling mistakes
and grammar errors. We all have world leaders. We all have world conkers. Plus, we all finally get
to see what happens when robots have dance battles. Notice the position of her finger. I can't get
it to go in. What does that tell us? Oh, there we go. You stopped flipping us off, Sophia. Thank you.
Now, the interesting question is why spelling is worthwhile to begin with. English is a very
strange language in that way. English is, you know, somebody had the idea. If you're a language
designer like me, this is a kind of a little story you always know, which is somebody had the idea
back in the 1600s, maybe, that you would add some letters to words like debt as a famous example.
You put that B in there to represent that that's really something that corresponds to the Latin
word debitur. But it's a really bad idea. It was a really bad design mistake to add these letters in
that represented where the word might have come from in another language, but which are not
pronounced in the language that you're dealing with. It's a good example of a language design
mistake. In any case, this is a small example of a growing hypergraph. So this is just showing,
there's just showing starting from an initial hypergraph there. We're just showing the successive
steps and the growth of this thing. And you can see different, well, here, let's make another
example here. Let's take an example where it actually grows into something that you would
recognize. So here's a here's a hypergraph that's kind of growing. It's just rewriting itself.
If you run it a bit longer, and then you render it nicely, it looks like that. So in other words,
that underlying hypergraph, it's the pattern of its connections is such that a sort of
reasonable human rendering of it would make it look like that. And when you sort of measure its
properties, you'll discover this is a somewhat curved space. So that's kind of how that works.
And you can see, I mean, there are many, oh gosh, you can get all kinds of different forms in the,
I mean, this is like the first, well, maybe 10 to the minus 100 seconds or something of the universe
might look like this. But it starts looking very, very different from that very quickly.
But these are different possibilities. These are essentially different rays and rural space
corresponding to different particular rules, which are picking in this really add. And the
question of which rules you pick, you are an observer, who is also operating according to
certain rules. And so the question of which rules you pick in the end does not matter.
But this is for purposes of us understanding what's going on, we're sort of picking a basis in which
to look at things. And those are examples of what happens in those cases. That was that what you
were looking for. I mean, this seems to me a generalization of a cellular automaton. But how
do you relate that to actually the entities that are about science? So how can I diffuse from that
if I run it long enough, I get the Einstein equations? Where do I pull the Einstein equations
out there? There's a book that I think there were copies of here that's about 800 pages long,
that has the beginnings of that. And there's a bunch of papers by one of my young collaborators
that have a lot more detail on that. There's not a trivial thing, okay? It's I can't give you the
instant version of it, which is not too surprising. But I mean, I could give you some indication of
how things work. Let me see if I can show you one thing here. So first question is what's the
dimension of the what's it when you have some complicated thing like this? What is the effect
of dimension of space to which this corresponds? And so you can at least get a sense of that by
seeing that imagine that you start at one point in that hypergraph, and you move one graph distance
away at every step, you build up a ball, and that ball has a number of nodes in it that grows in
some way. If you if that number of nodes grows like r to the d where r is the distance out,
you go in the ball, then d gives you an estimate of the effect of dimension of the continuum
limit object. The next order correction to that is the curvature, richly scalar curvature,
is the thing that appears as the first order correction. And so when you look at if you look
at the growth rate of, let's see is that one of those that here, this is an example, that's that
curved space, you actually see that it gets to be well roughly a two-dimensional surface,
but it has a variation from that, that variation corresponds to the presence of richly curvature
in the effective continuum limit space that comes from this object. And so that's the beginning,
the Einstein equations conveniently happen to contain the richly scalar and the richly tensor,
and that the beginning of this is you start deriving the richly tensor by looking at these
growth rates of balls in these hypergraphs. That's the beginning of hard work. It's kind of the
structure of the derivation is actually not that different than the derivation of fluid dynamics
from molecular dynamics, which by the way, I should say, it is not something the people have
been trying for a hundred years to get a rigorous mathematical derivation of fluid dynamics from
a molecular dynamics. That's a very hard thing. I think actually I made some progress recently
by thinking about computational irreducibility and things like this, but that's the mathematical
eyes and tees are definitely not crossed. And what we have to do involves taking more limits
and more elaborate things. And the mathematics really doesn't quite exist to get sort of the,
but we can, it turns out that many approaches to mathematical physics that people have taken,
whether it's spin networks, probably string theory as well, probably a causal set theory,
a bunch of other approaches, plug in very beautifully to what we've done. So what we've
done is kind of a concrete version of things to which those are various kinds of approximations.
So you can kind of triangulate in on what you see. Now, it turns out there's one of my young
collaborators, a chap called Jonathan Gorard, just recently, I think he hasn't published this yet,
but we've been actually simulating spacetime starting from this underlying discrete space
and then trying to get a large enough number of nodes in this underlying discrete space
to make something that is a good approximation to actual physical space. And so you can actually
compute black hole collisions in this underlying, in this thing, which is just made of discrete
atoms of space. And the results are very good. And the thing that is sort of interesting,
usually when you compute black hole collisions, you start from the Einstein equations,
continuous partial differential equations, and then you discretize them, you put them on a computer,
et cetera, et cetera, et cetera. And when things, you get all upset when there's a numerical analysis
glitch and something, you know, it mattered how you did the discretization, then that's considered
bad when you're starting from the continual Einstein equations. When we're starting from
these discrete underlying systems, and we come up, then for us, finding a numerical glitch is
totally thrilling, because it will allow us to actually see through to the essentially atomic
structure of space. And so the thing that is the challenge for right now is, you know, Brownian
motion was the thing that really finally convinced people, I think, that molecules exist. It was
an effect that had been seen a hundred years earlier, people didn't know what it was. And the
question is what the corresponding effect is that probably has already been seen in physics,
maybe a hundred years ago, that is the thing that is the clue that shows that in fact,
space time is discrete. My current guess, actually, is that it would turn out that dark
matter is exactly that. And having just made a big study of the second law of thermodynamics,
you may know from the history of the second law of thermodynamics, one of the things was people
used to believe that heat was a fluid, that heat was a fluid that suffused substances, and that
that's how that worked, because they had no other kind of way of thinking about what heat might be.
That was the caloric theory of heat that turned out to be wrong. It's heat is actually associated
with motion molecules and things. My current little aphorism is dark matter is the caloric
of our times. That is, it's something which is what people describe it in terms of particles,
but that's just not right. It's some other feature of what's going on underneath.
All right, I hope you enjoyed that as much as I did. I had a tremendous amount of fun watching it,
recording it, speaking with Wolfram Offair on air. That'll come up shortly. I want to thank
Susan Schneider for organizing MindFest, as well as the Center for the Future Mind. Once more,
the link is in the description. If you would like to support the theories of everything channel,
then you can go to patreon.com.curtjimungle and contribute with whatever you like.
Each dollar helps the production of tow. For instance, we have an editor who spent days editing
this one video. We have an operations manager, and then we have myself. There's also the theories
of everything.org website, where if you don't want to give to Patreon for whatever reason,
some people are uncomfortable, then you can go there and donate directly to myself. There's
also a PayPal, which is linked below on the theories of everything.org website. You get
access to these episodes ad free and in advance a couple of days in advance. So maybe you want to
check that out. Thank you so much. Again, thank you to Susan Schneider. Thank you to the Center
for the Future Mind. Hey, by the way, Susan Schneider, as well as Donald Hoffman and Bernardo
Castro had a debate, which I was lucky enough to host again. And that's on the tow channel.
And you can view it the thumbnails around here on the concept of can machines be conscious?
If you liked this video, then you'll like that link in the description. Take care.
The podcast is now concluded. Thank you for watching. If you haven't subscribed or clicked on
that like button, now would be a great time to do so as each subscribe and like helps YouTube
push this content to more people. Also, I recently found out that external links count plenty toward
the algorithm, which means that when you share on Twitter, on Facebook, on Reddit, etc., it shows
YouTube that people are talking about this outside of YouTube, which in turn greatly aids the distribution
on YouTube as well. If you'd like to support more conversations like this, then do consider
visiting theories of everything.org. Again, it's support from the sponsors and you that allow
me to work on tow full time. You get early access to ad free audio episodes there as well.
Every dollar helps far more than you may think. Either way, your viewership is generosity enough.
Thank you.
