1
00:00:00,000 --> 00:00:02,800
All right.

2
00:00:02,800 --> 00:00:07,240
OK, first machines they can understand,

3
00:00:07,240 --> 00:00:09,760
which means also reason and plan.

4
00:00:09,760 --> 00:00:12,800
It's going to be a lot of overlap with what Josh said,

5
00:00:12,800 --> 00:00:19,200
at least in terms of motivation, but not in terms of solutions.

6
00:00:19,200 --> 00:00:23,640
OK, first statement is that machine learning sucks,

7
00:00:23,640 --> 00:00:26,680
certainly compared to what we observe in humans and animals

8
00:00:26,680 --> 00:00:31,280
and their ability to learn and learn efficiently.

9
00:00:31,280 --> 00:00:33,600
You know, until recently, most of machine learning

10
00:00:33,600 --> 00:00:37,120
was based on supervised learning,

11
00:00:37,120 --> 00:00:41,760
required enormous amounts of label samples.

12
00:00:41,760 --> 00:00:45,520
What has taken over the last few years

13
00:00:45,520 --> 00:00:47,200
is self-supervised learning, which does not

14
00:00:47,200 --> 00:00:49,440
require as many label samples, but still requires

15
00:00:49,440 --> 00:00:52,440
a huge amount of samples.

16
00:00:52,440 --> 00:00:56,040
And in the end, those systems of still relatively brittle

17
00:00:56,040 --> 00:00:59,120
makes stupid mistakes, do not reason or plan,

18
00:00:59,120 --> 00:01:01,480
compared to humans and animals that can learn

19
00:01:01,480 --> 00:01:04,560
new tasks extremely quickly, because they understand

20
00:01:04,560 --> 00:01:09,240
how the world works, presumably, and they can reason and plan,

21
00:01:09,240 --> 00:01:12,440
and have certainly some level of common sense.

22
00:01:12,440 --> 00:01:17,400
So in our systems of today, most of them anyway,

23
00:01:17,400 --> 00:01:20,520
not absolutely all of them, but many of them,

24
00:01:20,520 --> 00:01:23,240
have a constant number of computational steps

25
00:01:23,240 --> 00:01:25,640
between their input and output, which

26
00:01:25,640 --> 00:01:28,200
means that whatever reasoning they do

27
00:01:28,200 --> 00:01:30,200
does not change, depending on whether it's

28
00:01:30,200 --> 00:01:33,160
a difficult problem they're trying to solve or not.

29
00:01:33,160 --> 00:01:34,400
They cannot really plan.

30
00:01:34,400 --> 00:01:36,200
The only systems they can plan at the moment

31
00:01:36,200 --> 00:01:38,320
are the ones that are designed to play games

32
00:01:38,320 --> 00:01:40,640
or to control robots.

33
00:01:40,640 --> 00:01:44,760
But things like LLMs do not plan.

34
00:01:44,760 --> 00:01:47,000
So how do we get machines to do like humans,

35
00:01:47,000 --> 00:01:48,640
which is to understand how the world works,

36
00:01:48,640 --> 00:01:50,920
predict the consequences of actions

37
00:01:50,920 --> 00:01:54,680
they might take, perform chains of reasoning

38
00:01:54,680 --> 00:01:57,040
with a potentially unlimited number of steps,

39
00:01:57,040 --> 00:01:59,720
and plan a complex task by decomposing them

40
00:01:59,720 --> 00:02:01,720
into sequences or subtasks.

41
00:02:01,720 --> 00:02:05,200
So let me start with this idea of self-supervised running,

42
00:02:05,200 --> 00:02:08,480
which really has taken over the world of AI

43
00:02:08,480 --> 00:02:11,080
over the last few years.

44
00:02:11,080 --> 00:02:14,680
And it's the basic idea of essentially presenting

45
00:02:14,680 --> 00:02:17,960
an input to a system, let's say a text, a window of text,

46
00:02:17,960 --> 00:02:24,040
or video, or a few images, and hiding part of it,

47
00:02:24,040 --> 00:02:26,480
and then training the system to capture the dependencies

48
00:02:26,480 --> 00:02:30,600
between what is observed and what is not yet observed,

49
00:02:30,600 --> 00:02:33,080
but eventually will be observed, whether it's

50
00:02:33,080 --> 00:02:37,120
the future of a video or a different view of the same scene

51
00:02:37,120 --> 00:02:43,680
from an image or words that have been obscured.

52
00:02:43,680 --> 00:02:45,400
And I say capture the dependency.

53
00:02:45,400 --> 00:02:47,160
I don't say predict because I'm going

54
00:02:47,160 --> 00:02:49,520
to talk about models that don't actually predict,

55
00:02:49,520 --> 00:02:51,160
but capture the dependencies.

56
00:02:51,160 --> 00:02:56,160
So a very successful example is language models.

57
00:02:56,160 --> 00:02:58,840
So self-supervised language models.

58
00:02:58,840 --> 00:03:01,280
And the idea goes back a long time to do this.

59
00:03:01,280 --> 00:03:03,440
I think the first paper to really kind of experiment

60
00:03:03,440 --> 00:03:10,520
with this was paper in around 2010 by Colbert and Weston,

61
00:03:10,520 --> 00:03:13,360
where they had this idea of essentially taking

62
00:03:13,360 --> 00:03:18,240
a piece of text, corrupting it in some ways.

63
00:03:18,680 --> 00:03:22,040
In modern versions, it consists in removing some words

64
00:03:22,040 --> 00:03:27,440
from the text, and then training some giant neural net

65
00:03:27,440 --> 00:03:30,240
to predict the words that are missing,

66
00:03:30,240 --> 00:03:35,160
or just merely to tell you whether the text that is here

67
00:03:35,160 --> 00:03:37,240
is legit or not legit.

68
00:03:37,240 --> 00:03:39,480
That's a different way of doing it.

69
00:03:39,480 --> 00:03:42,280
So this is how every modern NLP system

70
00:03:42,280 --> 00:03:44,840
over the last four or five years has been trained.

71
00:03:44,840 --> 00:03:48,600
And that has completely revolutionized not just

72
00:03:48,600 --> 00:03:50,800
the research in NLP, but also the practice of it.

73
00:03:50,800 --> 00:03:55,160
So all of translation, content moderation,

74
00:03:55,160 --> 00:03:58,440
hate speech detection, all that stuff from social networks,

75
00:03:58,440 --> 00:04:00,880
it all uses this kind of stuff.

76
00:04:00,880 --> 00:04:04,280
And performance went up by a huge amount.

77
00:04:04,280 --> 00:04:10,880
OK, so a special case of this is generative LLMs.

78
00:04:11,760 --> 00:04:16,000
And similar things are used in images and video.

79
00:04:16,000 --> 00:04:19,160
And there, the part of the text that you're hiding

80
00:04:19,160 --> 00:04:20,240
is just the last word.

81
00:04:20,240 --> 00:04:22,720
So you train a giant neural net to just predict

82
00:04:22,720 --> 00:04:25,400
the last word in a sequence.

83
00:04:25,400 --> 00:04:28,200
And then you can use this to produce outputs

84
00:04:28,200 --> 00:04:32,320
auto-regressively, which means you give a window of text,

85
00:04:32,320 --> 00:04:34,360
you get a system to produce a word,

86
00:04:34,360 --> 00:04:37,360
and then you shift that word into the input

87
00:04:37,360 --> 00:04:39,400
by shifting everything by one.

88
00:04:39,400 --> 00:04:41,400
Predict the next, next word, shift that in.

89
00:04:41,400 --> 00:04:44,080
Predict the next, next, next word, shift that in, et cetera.

90
00:04:44,080 --> 00:04:46,160
That's auto-regressive prediction.

91
00:04:46,160 --> 00:04:48,160
It's a major flaw with this approach.

92
00:04:48,160 --> 00:04:52,640
This is how every single LLM today works.

93
00:04:52,640 --> 00:04:55,120
But we should call them auto-regressive LLMs,

94
00:04:55,120 --> 00:04:58,440
because I think future LLMs are not going to be like this.

95
00:04:58,440 --> 00:05:01,160
But basically, every single one of them, some of which

96
00:05:01,160 --> 00:05:03,680
you've probably never heard of.

97
00:05:03,680 --> 00:05:07,000
So the ones from Faire, Blenderbot, Galactica, Lama,

98
00:05:07,000 --> 00:05:09,520
Alpeca, which is fine tuning of Lama.

99
00:05:09,520 --> 00:05:13,160
There is a new one now also.

100
00:05:13,160 --> 00:05:16,120
Lambda, Bard from Google, Shinshila from DeepMind,

101
00:05:16,120 --> 00:05:18,320
Chai GPT, GPT4, et cetera.

102
00:05:18,320 --> 00:05:22,280
They're all auto-regressive LLMs.

103
00:05:22,280 --> 00:05:26,120
And they train on gigantic amounts of data.

104
00:05:26,120 --> 00:05:28,840
So we're talking one trillion tokens or something like this.

105
00:05:28,840 --> 00:05:31,480
It would take a human reading eight hours a day,

106
00:05:31,480 --> 00:05:34,400
something like 22,000 years to read this.

107
00:05:34,400 --> 00:05:36,800
So obviously, those things can swallow a lot more

108
00:05:36,800 --> 00:05:39,560
and digest a lot more data than any human.

109
00:05:39,560 --> 00:05:42,400
And the performance is nothing short of amazing.

110
00:05:42,400 --> 00:05:44,360
But they do make stupid mistakes.

111
00:05:44,360 --> 00:05:45,880
They are extremely fluent.

112
00:05:45,880 --> 00:05:48,720
So we can use them to generate text.

113
00:05:48,720 --> 00:05:55,200
But they make factual errors, logical errors, inconsistencies.

114
00:05:55,200 --> 00:05:56,760
They have limited reasoning ability.

115
00:05:56,760 --> 00:05:59,800
There is no way to control for things like toxicity

116
00:05:59,800 --> 00:06:01,560
and stuff like that.

117
00:06:01,560 --> 00:06:04,720
And they really have no knowledge of the underlying reality,

118
00:06:04,720 --> 00:06:08,440
except in one case, because, of course, they only

119
00:06:08,440 --> 00:06:10,640
train from text, except in one case.

120
00:06:10,640 --> 00:06:14,960
And that case is code generation.

121
00:06:14,960 --> 00:06:17,480
And they work really, really well for code generation.

122
00:06:17,480 --> 00:06:20,000
And the reason they work well is that the underlying reality

123
00:06:20,000 --> 00:06:21,160
of code is very simple.

124
00:06:21,160 --> 00:06:21,960
It's deterministic.

125
00:06:21,960 --> 00:06:24,360
It's just the state of variables of a program.

126
00:06:24,360 --> 00:06:27,560
And so that's fully observable, deterministic,

127
00:06:27,560 --> 00:06:28,080
and everything.

128
00:06:28,080 --> 00:06:31,680
So it works really well.

129
00:06:31,680 --> 00:06:34,200
And they can generate fluent text.

130
00:06:34,200 --> 00:06:36,560
But in this particular case, this is a joke

131
00:06:36,560 --> 00:06:39,280
that my colleagues did on me.

132
00:06:39,280 --> 00:06:41,640
It's completely made up.

133
00:06:41,640 --> 00:06:46,800
I never actually did a rap album.

134
00:06:46,800 --> 00:06:48,480
Raw personal.

135
00:06:48,480 --> 00:06:51,480
Yeah.

136
00:06:51,480 --> 00:06:54,000
I asked them if they, I don't actually like rap that well.

137
00:06:54,000 --> 00:06:56,520
So I'm over a jazz person.

138
00:06:56,520 --> 00:06:58,400
So I asked them to do the same thing with jazz.

139
00:06:58,400 --> 00:07:02,480
And they say, there's not enough training data.

140
00:07:02,480 --> 00:07:05,480
And I cried.

141
00:07:05,480 --> 00:07:06,720
OK, so what are they good for?

142
00:07:06,720 --> 00:07:09,320
They're good for writing assistance, generating

143
00:07:09,320 --> 00:07:12,800
first draft, producing a style.

144
00:07:12,800 --> 00:07:15,800
Code writing assistance, obviously, very efficient for that.

145
00:07:15,800 --> 00:07:18,920
They're not good for producing factual and consistent answers

146
00:07:18,920 --> 00:07:21,520
because of aducinations.

147
00:07:21,520 --> 00:07:23,360
And they're not good for taking into account

148
00:07:23,360 --> 00:07:24,800
recent information, because you need

149
00:07:24,800 --> 00:07:27,920
to retrain the entire system to take into account yesterday

150
00:07:27,920 --> 00:07:29,080
in real time.

151
00:07:29,080 --> 00:07:32,560
And that's just not practical.

152
00:07:32,560 --> 00:07:35,800
They don't behave properly, or at least they're

153
00:07:35,800 --> 00:07:37,680
hard to control to do so.

154
00:07:37,680 --> 00:07:38,760
They don't do reasoning.

155
00:07:38,760 --> 00:07:39,520
They don't do planning.

156
00:07:39,520 --> 00:07:42,880
They don't do math, as we saw this morning.

157
00:07:42,880 --> 00:07:44,440
They're being modified to use tools,

158
00:07:44,440 --> 00:07:47,440
such as search engines, calculators, stuff like that.

159
00:07:47,440 --> 00:07:54,120
But currently, it's kind of like using duct tape and staples.

160
00:07:54,120 --> 00:07:56,360
And we're easily fooled by their fluency into thinking

161
00:07:56,360 --> 00:07:59,600
that they are smart, but they are not that smart.

162
00:07:59,600 --> 00:08:02,080
Now, there is a major flaw with this autoregressive

163
00:08:02,080 --> 00:08:05,560
generation, which is that it's an exponentially

164
00:08:05,560 --> 00:08:07,360
diverging diffusion process.

165
00:08:07,360 --> 00:08:10,840
So if there is the probability, e,

166
00:08:10,840 --> 00:08:13,920
for every token that is produced to be outside

167
00:08:13,920 --> 00:08:15,560
of the set of correct answers, let's

168
00:08:15,560 --> 00:08:18,440
assume that errors are being independent,

169
00:08:18,440 --> 00:08:20,440
then if we generate a sequence of n tokens,

170
00:08:20,440 --> 00:08:23,440
the probability for that sequence to be correct

171
00:08:23,440 --> 00:08:25,080
is 1 minus e to the power n.

172
00:08:25,080 --> 00:08:28,480
And that decreases exponentially.

173
00:08:28,480 --> 00:08:30,440
So those things just don't work.

174
00:08:30,440 --> 00:08:31,840
They just don't work.

175
00:08:31,840 --> 00:08:33,960
My prediction is that five years from now,

176
00:08:33,960 --> 00:08:37,840
nobody in that right mind would be using autoregressive LNMs.

177
00:08:37,840 --> 00:08:41,000
It's just a bad phase.

178
00:08:41,000 --> 00:08:41,960
They are useful, though.

179
00:08:41,960 --> 00:08:42,600
They're very useful.

180
00:08:45,360 --> 00:08:47,600
So as I said, they have a constant number

181
00:08:47,600 --> 00:08:49,640
of computational steps between input and output

182
00:08:49,640 --> 00:08:52,480
for each token generated.

183
00:08:52,480 --> 00:08:54,880
They do not reason and plan.

184
00:08:54,920 --> 00:08:57,920
Jake Browning, who will be talking Wednesday,

185
00:08:57,920 --> 00:09:00,280
and I wrote a philosophy paper.

186
00:09:00,280 --> 00:09:04,560
I mean, he wrote it on the fact that there

187
00:09:04,560 --> 00:09:09,320
are limitations to the purported intelligence

188
00:09:09,320 --> 00:09:12,240
of systems that are purely trained from text,

189
00:09:12,240 --> 00:09:17,400
because I would argue that most of human knowledge

190
00:09:17,400 --> 00:09:18,800
is not textual.

191
00:09:18,800 --> 00:09:20,360
I mean, certainly most of what babies

192
00:09:21,320 --> 00:09:25,160
are on before six months is non-textual.

193
00:09:25,160 --> 00:09:27,720
And everything that animals learn is non-textual.

194
00:09:27,720 --> 00:09:34,440
So that knowledge is still unattainable to current AI

195
00:09:34,440 --> 00:09:36,120
systems.

196
00:09:36,120 --> 00:09:39,080
So how do we get machines to understand how the world works

197
00:09:39,080 --> 00:09:41,240
and predict the consequences of their actions?

198
00:09:41,240 --> 00:09:43,000
All the limitations have been pointed out

199
00:09:43,000 --> 00:09:47,560
by a number of different papers, including one

200
00:09:47,560 --> 00:09:52,840
by the MIT crowd, that fluency is really not

201
00:09:52,840 --> 00:09:57,040
the same as thinking.

202
00:09:57,040 --> 00:09:59,080
And basically, you could argue for the fact

203
00:09:59,080 --> 00:10:02,520
that what LLMs are good for is perhaps modeling

204
00:10:02,520 --> 00:10:05,240
the Browning-Key and Boracay areas,

205
00:10:05,240 --> 00:10:07,040
but not much else in the brain.

206
00:10:07,040 --> 00:10:11,080
And that's like tiny little areas on the side of the brain.

207
00:10:14,240 --> 00:10:17,280
So we need something else.

208
00:10:17,320 --> 00:10:19,600
What are we missing?

209
00:10:19,600 --> 00:10:21,040
This is a chart that I like to show.

210
00:10:21,040 --> 00:10:23,040
Oops, the animation is bad.

211
00:10:23,040 --> 00:10:27,680
But it was put together by Emmanuel Dupu, who

212
00:10:27,680 --> 00:10:30,480
kind of tends to indicate at what age babies

213
00:10:30,480 --> 00:10:34,160
learn basic concepts, like object permanence, for example.

214
00:10:34,160 --> 00:10:36,840
Liz was talking about that.

215
00:10:36,840 --> 00:10:39,240
Stability and support and intuitive physics,

216
00:10:39,240 --> 00:10:44,400
which only comes up fairly late, actually, around nine months.

217
00:10:44,400 --> 00:10:46,400
And the question is, what type of learning

218
00:10:46,440 --> 00:10:48,040
is taking place there?

219
00:10:48,040 --> 00:10:51,360
No AI systems today really kind of does this properly,

220
00:10:51,360 --> 00:10:55,480
although there's been several attempts by a few of us.

221
00:10:55,480 --> 00:10:58,320
So I think perhaps it's this type of learning

222
00:10:58,320 --> 00:11:00,400
that is the basis of common sense.

223
00:11:00,400 --> 00:11:02,920
And we should really try to figure out

224
00:11:02,920 --> 00:11:05,520
how to reproduce this with machines.

225
00:11:05,520 --> 00:11:07,840
So I think there's three challenges for AI research

226
00:11:07,840 --> 00:11:10,560
today, learning representations and predictive models

227
00:11:10,560 --> 00:11:14,920
of the world, allowing machines to predict what's going to happen,

228
00:11:14,960 --> 00:11:17,000
perhaps as a consequence of their actions.

229
00:11:17,000 --> 00:11:18,040
Learning to reason.

230
00:11:18,040 --> 00:11:21,480
So this is more like Daniel Kahneman's System 2.

231
00:11:21,480 --> 00:11:24,440
Current autoregressive LLMs are basically System 1.

232
00:11:24,440 --> 00:11:26,600
They just view one word after the other

233
00:11:26,600 --> 00:11:30,680
without really planning ahead.

234
00:11:30,680 --> 00:11:33,800
And so that is the question of making reasoning compatible

235
00:11:33,800 --> 00:11:36,240
with learning.

236
00:11:36,240 --> 00:11:37,920
Josh has a particular proposal for this,

237
00:11:37,920 --> 00:11:44,520
which I don't agree with, but that goes in the right direction.

238
00:11:44,560 --> 00:11:47,600
And then learning to plan complex action sequences.

239
00:11:47,600 --> 00:11:51,400
So I made a proposal for this almost a year ago now,

240
00:11:51,400 --> 00:11:55,560
which I posted on this website so people can make comments

241
00:11:55,560 --> 00:12:00,720
and tell me I'm wrong and which references I missed.

242
00:12:00,720 --> 00:12:03,560
I guess several technical talks about it as well.

243
00:12:03,560 --> 00:12:06,680
And basically it's sort of a modular organization

244
00:12:06,680 --> 00:12:11,640
of an AI system that would be capable of reasoning and planning.

245
00:12:11,640 --> 00:12:15,120
And I can't say that I've built it,

246
00:12:15,120 --> 00:12:18,040
but we're kind of building pieces of it.

247
00:12:18,040 --> 00:12:19,160
So it's composed.

248
00:12:19,160 --> 00:12:22,040
It's basically centered around the award model, which

249
00:12:22,040 --> 00:12:24,480
will allow the system to predict ahead

250
00:12:24,480 --> 00:12:27,440
what the consequences of its actions would be.

251
00:12:27,440 --> 00:12:32,400
And it has a cost module.

252
00:12:32,400 --> 00:12:35,840
Think of it as kind of visual ganglia stuff.

253
00:12:35,840 --> 00:12:37,440
And the only purpose of the system

254
00:12:37,440 --> 00:12:39,560
is to optimize that cost.

255
00:12:39,560 --> 00:12:41,520
Some of those costs are essentially

256
00:12:41,520 --> 00:12:44,320
intrinsic, hard-wired, immutable costs

257
00:12:44,320 --> 00:12:47,800
that sort of drive the basic behavior of the system.

258
00:12:47,800 --> 00:12:49,960
And some of them are trainable costs

259
00:12:49,960 --> 00:12:53,960
that the system learns as it goes.

260
00:12:53,960 --> 00:12:57,040
And what the system does is that it

261
00:12:57,040 --> 00:13:00,240
plans a sequence of actions that, according to its model,

262
00:13:00,240 --> 00:13:03,920
will minimize those costs.

263
00:13:03,920 --> 00:13:05,600
And of course, it needs to be able to estimate

264
00:13:05,600 --> 00:13:07,960
the current state of the world, which is done through perception

265
00:13:08,400 --> 00:13:10,000
and maybe access to a memory.

266
00:13:10,000 --> 00:13:13,440
And then depending on the task that the system is focusing on,

267
00:13:13,440 --> 00:13:15,440
it can be entirely configured by a configurator

268
00:13:15,440 --> 00:13:18,760
that will sort of focus the system on the task at hand.

269
00:13:18,760 --> 00:13:21,520
So that's a cognitive architecture, which

270
00:13:21,520 --> 00:13:24,440
some people in classical AI have been proposing,

271
00:13:24,440 --> 00:13:27,760
but in sort of different forms.

272
00:13:27,760 --> 00:13:29,160
And there's two ways to use it.

273
00:13:29,160 --> 00:13:33,400
Mode 1, which is just a reactive perception action cycle,

274
00:13:33,400 --> 00:13:35,120
get an idea of the state of the world,

275
00:13:35,120 --> 00:13:37,720
encoding into an abstract representation

276
00:13:37,720 --> 00:13:39,840
of the state of the world as 0, and then running

277
00:13:39,840 --> 00:13:42,760
through some other neural net that produces an action

278
00:13:42,760 --> 00:13:44,240
reactively.

279
00:13:44,240 --> 00:13:46,280
But the more interesting mode is mode 2,

280
00:13:46,280 --> 00:13:48,840
which is like Kettiman's System 2,

281
00:13:48,840 --> 00:13:51,640
where you make an estimate of the state of the world,

282
00:13:51,640 --> 00:13:54,840
and then using your word model, predict ahead of time

283
00:13:54,840 --> 00:13:57,640
what's going to happen according to an imagined sequence

284
00:13:57,640 --> 00:14:00,800
of actions that you might take.

285
00:14:00,800 --> 00:14:04,960
And then the agent would optimize that sequence of actions.

286
00:14:05,000 --> 00:14:08,000
So as to minimize a particular cost function,

287
00:14:08,000 --> 00:14:14,200
representing the tax to be fulfilled.

288
00:14:14,200 --> 00:14:16,520
And then it would just take the first action

289
00:14:16,520 --> 00:14:19,440
and actually send it to the actuator,

290
00:14:19,440 --> 00:14:21,240
or maybe the first few actions.

291
00:14:21,240 --> 00:14:23,440
So this is completely classical in optimal control.

292
00:14:23,440 --> 00:14:26,480
It's called model predictive control.

293
00:14:26,480 --> 00:14:30,480
Except the problem here is how you learn the model.

294
00:14:30,480 --> 00:14:32,680
There's a way to kind of turn System 2 into System 1,

295
00:14:32,720 --> 00:14:35,120
which I'm not going to go into.

296
00:14:35,120 --> 00:14:38,520
OK, so how do we train the world model?

297
00:14:38,520 --> 00:14:42,480
Only for the fact that the world is not deterministic

298
00:14:42,480 --> 00:14:46,040
or not entirely predictable, even if it is deterministic.

299
00:14:46,040 --> 00:14:50,240
So we're not going to have a neural net observe the input

300
00:14:50,240 --> 00:14:54,640
and just predict why, and then minimizing a prediction error.

301
00:14:54,640 --> 00:14:56,240
That's not going to work, because that can only

302
00:14:56,240 --> 00:14:58,120
make one prediction.

303
00:14:58,120 --> 00:15:00,920
So in fact, if you train a big neural net

304
00:15:00,920 --> 00:15:05,400
to predict like these are cars from a top-down view

305
00:15:05,400 --> 00:15:07,040
of a highway, if you train a neural net

306
00:15:07,040 --> 00:15:09,200
to try to predict what's going to happen in this video,

307
00:15:09,200 --> 00:15:11,720
you get blurry predictions.

308
00:15:11,720 --> 00:15:14,120
Because the system cannot predict if a particular car is

309
00:15:14,120 --> 00:15:17,880
going to break or accelerate or turn left or right,

310
00:15:17,880 --> 00:15:20,720
and so it makes these blurry predictions.

311
00:15:20,720 --> 00:15:22,040
Same for a natural video.

312
00:15:22,040 --> 00:15:26,920
That's an old work on video prediction.

313
00:15:26,920 --> 00:15:29,720
So you have to account for the fact

314
00:15:29,720 --> 00:15:32,680
that the world is not completely predictable.

315
00:15:32,680 --> 00:15:35,120
And you have two solutions there.

316
00:15:35,120 --> 00:15:37,280
Either you build an architecture with latent variables

317
00:15:37,280 --> 00:15:41,320
that parameterizes the set of possible predictions,

318
00:15:41,320 --> 00:15:44,960
or, and those two are not incompatible,

319
00:15:44,960 --> 00:15:47,240
or you abandon the idea that you're

320
00:15:47,240 --> 00:15:49,400
going to predict everything about the world.

321
00:15:49,400 --> 00:15:51,440
And so this is what I'm suggesting.

322
00:15:51,440 --> 00:15:54,080
So this is a generative architecture.

323
00:15:54,080 --> 00:15:57,280
Generative architecture observes x, encodes it,

324
00:15:57,280 --> 00:16:00,880
then predicts y, the variable whose dependency you're

325
00:16:00,880 --> 00:16:02,600
trying to predict.

326
00:16:02,600 --> 00:16:04,760
And then you measure the prediction error.

327
00:16:04,760 --> 00:16:07,800
You mean my side by training, et cetera.

328
00:16:07,800 --> 00:16:11,360
What I'm proposing is a joint embedding architecture

329
00:16:11,360 --> 00:16:15,840
where both x and y go through encoders, neural nets,

330
00:16:15,840 --> 00:16:19,400
and the prediction takes place in representation space.

331
00:16:19,400 --> 00:16:22,320
What that allows the system to do is basically

332
00:16:22,320 --> 00:16:25,640
eliminate a lot of irrelevant information from y

333
00:16:25,640 --> 00:16:29,000
when it encodes it into SY so that it doesn't

334
00:16:29,000 --> 00:16:32,000
have to predict all the details.

335
00:16:32,000 --> 00:16:35,400
So there's a lot of things here and a lot of information

336
00:16:35,400 --> 00:16:39,360
in this room that we cannot possibly remember or predict

337
00:16:39,360 --> 00:16:44,680
the precise texture of the wood on the floor, things like that.

338
00:16:44,680 --> 00:16:47,320
But it's kind of irrelevant.

339
00:16:47,320 --> 00:16:50,800
We only need to have sort of a relatively abstract

340
00:16:50,800 --> 00:16:52,360
representation of it.

341
00:16:52,360 --> 00:16:53,920
So I'm basically recommending to abandon

342
00:16:53,920 --> 00:16:56,720
the whole idea of generative models,

343
00:16:56,720 --> 00:16:59,400
unless you want to produce pictures or produce text.

344
00:16:59,400 --> 00:17:01,360
But if you want to learn how the world works,

345
00:17:01,360 --> 00:17:03,720
you should not reconstruct.

346
00:17:03,720 --> 00:17:05,000
There's actually several versions

347
00:17:05,000 --> 00:17:07,520
of those joint embedding architectures,

348
00:17:07,520 --> 00:17:12,640
the simple one, deterministic ones that can predict,

349
00:17:12,640 --> 00:17:14,200
and then nondeterministic ones that

350
00:17:14,200 --> 00:17:19,440
can predict where the predictor can have latent variables.

351
00:17:19,440 --> 00:17:22,720
So that's kind of the most general architecture.

352
00:17:22,720 --> 00:17:28,400
And the latent variable A here can be a latent variable you

353
00:17:28,400 --> 00:17:30,200
infer or it could be an action.

354
00:17:30,200 --> 00:17:32,080
So imagine that this is a world model.

355
00:17:32,080 --> 00:17:33,520
This is the current state of the world

356
00:17:33,520 --> 00:17:35,640
that you observe, you encode.

357
00:17:35,640 --> 00:17:37,600
This is an action you might take in the world,

358
00:17:37,600 --> 00:17:39,920
maybe combined with some latent variable which

359
00:17:39,920 --> 00:17:42,120
represent what you don't know about the world.

360
00:17:42,120 --> 00:17:45,560
And then you make a prediction, and then

361
00:17:45,560 --> 00:17:47,760
you can compare that prediction with what actually occurs

362
00:17:47,760 --> 00:17:49,960
if you want to train your model.

363
00:17:49,960 --> 00:17:51,320
And that's a predictive model that

364
00:17:51,320 --> 00:17:53,440
will allow you to predict what's going

365
00:17:53,440 --> 00:17:55,920
to happen as a consequence of your actions.

366
00:17:55,920 --> 00:18:00,080
Now, because we're not generating anything,

367
00:18:00,080 --> 00:18:02,360
and because we can't turn a model of this type

368
00:18:02,360 --> 00:18:05,480
into a probabilistic model of t of y given x,

369
00:18:05,480 --> 00:18:08,600
we have to abandon the whole idea of probabilistic modeling.

370
00:18:08,600 --> 00:18:13,120
And now Josh is going, oh my god.

371
00:18:13,120 --> 00:18:15,320
Isn't it just approximate probability at that point?

372
00:18:15,320 --> 00:18:15,960
Isn't it?

373
00:18:15,960 --> 00:18:16,460
No.

374
00:18:16,460 --> 00:18:16,960
No?

375
00:18:16,960 --> 00:18:18,840
No.

376
00:18:18,840 --> 00:18:19,960
It's energies, OK?

377
00:18:20,000 --> 00:18:24,320
So basically the name of the game here

378
00:18:24,320 --> 00:18:27,960
is that you need to understand the system as computing

379
00:18:27,960 --> 00:18:30,960
an energy function that captures the dependency between x and y.

380
00:18:30,960 --> 00:18:35,880
So imagine the data points are those black spheres.

381
00:18:35,880 --> 00:18:38,160
The energy function should take low values around the black

382
00:18:38,160 --> 00:18:40,880
spheres and higher values outside.

383
00:18:40,880 --> 00:18:44,640
And whether this energy function represents

384
00:18:44,640 --> 00:18:49,480
the unnormalized log of some probability,

385
00:18:49,480 --> 00:18:51,840
is irrelevant, you just want the energy

386
00:18:51,840 --> 00:18:54,320
to be higher outside of the manifold of data.

387
00:18:54,320 --> 00:18:57,200
And it will have captured the dependency between the variables.

388
00:18:57,200 --> 00:19:00,120
And there's nothing more you need.

389
00:19:00,120 --> 00:19:02,600
Now, the next question is, how do you train a system

390
00:19:02,600 --> 00:19:06,240
to give low energy to stuff you observe and high energy

391
00:19:06,240 --> 00:19:07,400
to stuff you don't observe?

392
00:19:07,400 --> 00:19:09,600
And there are two methods, contrastive methods,

393
00:19:09,600 --> 00:19:12,840
which consist in generating fake contrastive points whose

394
00:19:12,840 --> 00:19:14,320
energy is going to push up.

395
00:19:14,320 --> 00:19:15,640
And then regularized methods, which

396
00:19:15,640 --> 00:19:17,240
I'm going to explain in a second.

397
00:19:17,240 --> 00:19:21,120
So let's say you have training samples.

398
00:19:21,120 --> 00:19:22,800
Your system currently gives low energy

399
00:19:22,800 --> 00:19:26,640
to this sort of peak area here.

400
00:19:26,640 --> 00:19:30,720
And it's not a good model of the data here,

401
00:19:30,720 --> 00:19:33,280
because it gives high energy to data points and low energy

402
00:19:33,280 --> 00:19:36,080
to areas that have no points.

403
00:19:36,080 --> 00:19:39,000
So what you can do is generate green points here whose energy

404
00:19:39,000 --> 00:19:40,720
you're going to push up.

405
00:19:40,720 --> 00:19:44,080
And the energy function is going to take the right shape.

406
00:19:44,080 --> 00:19:46,720
Or you could use some sort of regularizer

407
00:19:46,720 --> 00:19:50,440
that minimizes the volume of space that can take low energy.

408
00:19:50,440 --> 00:19:52,720
So that whenever you push down on the energy of some regions,

409
00:19:52,720 --> 00:19:54,200
the rest has to go up, because there

410
00:19:54,200 --> 00:19:59,920
is a limited amount of volume that can take low energy.

411
00:19:59,920 --> 00:20:03,880
So in the context of joint embedding architecture,

412
00:20:03,880 --> 00:20:05,680
I kind of invented the contrastive methods.

413
00:20:05,680 --> 00:20:07,840
That's called sine is net in the old days.

414
00:20:07,840 --> 00:20:13,520
But I'm now arguing against that in favor of regularized methods.

415
00:20:13,520 --> 00:20:15,520
And the big question is, how do we train them?

416
00:20:15,560 --> 00:20:17,640
I'll tell you about that in a minute.

417
00:20:17,640 --> 00:20:19,720
But I'm asking you to abandon generative models,

418
00:20:19,720 --> 00:20:23,440
abandon probabilistic models, probabilistic modeling in general,

419
00:20:23,440 --> 00:20:24,960
abandon contrastive methods.

420
00:20:24,960 --> 00:20:26,480
And of course, abandon reinforcement learning.

421
00:20:26,480 --> 00:20:31,240
But that, I've been saying this for years.

422
00:20:31,240 --> 00:20:34,560
Those are four of the main pillars of machine learning.

423
00:20:34,560 --> 00:20:40,200
That makes me super popular among my colleagues.

424
00:20:40,200 --> 00:20:42,040
OK, so what are those regularized methods

425
00:20:42,040 --> 00:20:44,040
for joint embedding architectures?

426
00:20:44,080 --> 00:20:46,360
So essentially, there is a big issue

427
00:20:46,360 --> 00:20:50,360
that you have to fix, which is that when you train a system

428
00:20:50,360 --> 00:20:52,360
like this, one of those JEPA architecture,

429
00:20:52,360 --> 00:20:54,720
joint embedding predictive architectures,

430
00:20:54,720 --> 00:20:56,840
you show it an example of x and y.

431
00:20:56,840 --> 00:21:00,040
And you tell it just train all the weights of all those neural

432
00:21:00,040 --> 00:21:04,640
nets so as to minimize the prediction error, it collapses.

433
00:21:04,640 --> 00:21:07,280
Basically, what it says is that, well, I can just

434
00:21:07,280 --> 00:21:12,800
set Sx and Sy to constants and set the prediction,

435
00:21:12,840 --> 00:21:15,520
set the predictor to some constant thing

436
00:21:15,520 --> 00:21:17,640
and ignore x and y all together.

437
00:21:17,640 --> 00:21:20,200
And that would be a collapse system that gives zero energy

438
00:21:20,200 --> 00:21:22,120
to everything in your space.

439
00:21:22,120 --> 00:21:23,560
You have to prevent that from happening.

440
00:21:23,560 --> 00:21:25,560
And one way to prevent that from happening

441
00:21:25,560 --> 00:21:29,600
is finding a way to maximize the information content

442
00:21:29,600 --> 00:21:34,120
of the representations that come out of the encoders.

443
00:21:34,120 --> 00:21:37,120
That actually has the effect of minimizing the volume of stuff

444
00:21:37,120 --> 00:21:42,520
that can take your energy indirectly.

445
00:21:42,520 --> 00:21:47,240
So one way to prevent the outputs from being constant

446
00:21:47,240 --> 00:21:50,160
is that you can force the variance to be non-zero.

447
00:21:50,160 --> 00:21:52,840
So you put a cost function on top of this vector here

448
00:21:52,840 --> 00:21:55,160
that says, over a batch of samples,

449
00:21:55,160 --> 00:21:58,760
I want the variance of each variable coming out of that neural

450
00:21:58,760 --> 00:22:00,880
net to be non-zero, to be above one, let's say.

451
00:22:00,880 --> 00:22:02,840
So that's a hinge loss that says the variance

452
00:22:02,840 --> 00:22:04,320
needs to be above one.

453
00:22:04,320 --> 00:22:05,800
It's not enough because the system

454
00:22:05,800 --> 00:22:10,640
can still cheat by making all the variables the same

455
00:22:10,640 --> 00:22:12,280
or very highly correlated.

456
00:22:12,280 --> 00:22:13,800
So you have another cost that says,

457
00:22:13,800 --> 00:22:16,440
I want them to be decorrelated.

458
00:22:16,440 --> 00:22:18,160
So basically, this has the effect

459
00:22:18,160 --> 00:22:21,920
of enforcing the covariance matrix of that those Sx

460
00:22:21,920 --> 00:22:26,840
vectors over a batch to be close to the identity.

461
00:22:26,840 --> 00:22:29,440
And it's not enough because the variables

462
00:22:29,440 --> 00:22:34,240
can be non-collapsed and correlated

463
00:22:34,240 --> 00:22:36,240
but still dependent.

464
00:22:36,240 --> 00:22:37,920
And so there's another trick that we do,

465
00:22:37,920 --> 00:22:41,360
and we have some theory that shows that it's not stupid,

466
00:22:41,360 --> 00:22:42,760
which is that you take the Sx vector,

467
00:22:42,760 --> 00:22:45,280
you run it to some neural net that expands the dimension,

468
00:22:45,280 --> 00:22:47,960
and then you apply those criteria on the covariance

469
00:22:47,960 --> 00:22:49,080
matrix to the output.

470
00:22:49,080 --> 00:22:52,680
And that makes the variables of Sx kind of more independent.

471
00:22:52,680 --> 00:22:55,240
Now, there's a major flaw with this, which is,

472
00:22:55,240 --> 00:22:57,920
and that's the theory which I'm not going to talk about.

473
00:22:57,920 --> 00:23:01,200
There's a flaw with all of this, which is that we're basically,

474
00:23:01,200 --> 00:23:03,120
we have an upper bound on information content,

475
00:23:03,120 --> 00:23:05,960
and we're pushing it up, hoping that the actual information

476
00:23:05,960 --> 00:23:08,080
content will follow.

477
00:23:08,080 --> 00:23:09,680
And it's stupid, but it kind of works.

478
00:23:11,600 --> 00:23:14,240
OK, so you can test those pre-training

479
00:23:14,240 --> 00:23:15,240
for image recognition.

480
00:23:15,240 --> 00:23:18,080
For example, you show two different views of the same image,

481
00:23:18,080 --> 00:23:21,440
train the network to produce identical representations

482
00:23:21,440 --> 00:23:23,440
for two different views of the same image,

483
00:23:23,440 --> 00:23:27,240
and then you freeze the network and basically train

484
00:23:27,240 --> 00:23:29,640
a linear classifier on top with ImageNet

485
00:23:29,640 --> 00:23:31,120
and measure the performance.

486
00:23:31,120 --> 00:23:36,240
And this Vcrag method that I just described

487
00:23:36,240 --> 00:23:41,200
works just as well as isn't the top pack, let's say.

488
00:23:41,240 --> 00:23:42,560
There's a bunch of different methods

489
00:23:42,560 --> 00:23:44,440
that have similar performance.

490
00:23:44,440 --> 00:23:45,800
And they are in the top pack.

491
00:23:45,800 --> 00:23:47,160
I'm not going to bore you with details.

492
00:23:47,160 --> 00:23:49,600
You can try to do segmentation as well.

493
00:23:49,600 --> 00:23:51,720
Here's another method, somewhat similar,

494
00:23:51,720 --> 00:23:54,880
but closer to the JEPA idea, which

495
00:23:54,880 --> 00:23:57,200
uses a different criterion to prevent collapse, which

496
00:23:57,200 --> 00:23:58,680
I'm not going to explain.

497
00:23:58,680 --> 00:24:03,120
And this one takes a partially masked input image

498
00:24:03,120 --> 00:24:05,440
together with a full input image, runs both of them

499
00:24:05,440 --> 00:24:08,440
through encoders, and then trains a predictor

500
00:24:08,440 --> 00:24:13,280
to basically predict the representation of the full image

501
00:24:13,280 --> 00:24:16,240
from the representation computed

502
00:24:16,240 --> 00:24:19,800
from the partially masked image.

503
00:24:19,800 --> 00:24:21,800
This is called IJEPA, ImageJEPA.

504
00:24:21,800 --> 00:24:23,000
And it works amazingly well.

505
00:24:23,000 --> 00:24:29,320
And it's really fast to train very good performance.

506
00:24:29,320 --> 00:24:34,200
In terms of performance, even though this type of masking

507
00:24:34,200 --> 00:24:36,680
does not require any knowledge about the nature of the input,

508
00:24:36,720 --> 00:24:39,760
essentially, or very little, the still

509
00:24:39,760 --> 00:24:41,040
you get the same kind of performance

510
00:24:41,040 --> 00:24:44,760
that you would get if you used a self-supervised learning

511
00:24:44,760 --> 00:24:47,800
method that exploits the fact that you're

512
00:24:47,800 --> 00:24:51,680
doing image recognition, like Dino or Ibot or Simclear,

513
00:24:51,680 --> 00:24:52,200
for example.

514
00:24:55,080 --> 00:24:59,480
OK, now, how are you going to use this in the end?

515
00:24:59,480 --> 00:25:05,120
What I'm really interested in is to use JEPAs as world models

516
00:25:05,120 --> 00:25:06,040
inside of the system.

517
00:25:06,040 --> 00:25:08,320
They can do system two type planning,

518
00:25:08,320 --> 00:25:12,520
but even better than this, they can do hierarchical planning.

519
00:25:12,520 --> 00:25:18,920
And the idea there is that when you think about a task,

520
00:25:18,920 --> 00:25:22,720
you're not planning this task at the lowest level

521
00:25:22,720 --> 00:25:26,720
in terms of millisecond by millisecond muscle control.

522
00:25:26,720 --> 00:25:28,200
You're playing a task like, I want

523
00:25:28,200 --> 00:25:31,640
to go from Santa Fe to New York, or let's say

524
00:25:31,640 --> 00:25:35,400
from New York to Santa Fe, that's a better example.

525
00:25:35,400 --> 00:25:38,920
So you first decompose this into two sub-tasks.

526
00:25:38,920 --> 00:25:41,200
First thing I need to do is go to the airport

527
00:25:41,200 --> 00:25:43,200
and catch a plane.

528
00:25:43,200 --> 00:25:44,280
How do I go to the airport?

529
00:25:44,280 --> 00:25:45,360
Well, to go to the airport, I need

530
00:25:45,360 --> 00:25:46,960
to get on the street and have a taxi,

531
00:25:46,960 --> 00:25:51,040
which you can do in New York City, not in Santa Fe.

532
00:25:51,040 --> 00:25:52,360
How do I get down in the street?

533
00:25:52,360 --> 00:25:55,880
I need to get out of the building I'm in, et cetera.

534
00:25:55,880 --> 00:25:57,040
How do I get out of the building?

535
00:25:57,040 --> 00:25:59,560
I need to stand up from my chair, walk to the door.

536
00:25:59,560 --> 00:26:01,240
How do I get up from my chair?

537
00:26:01,240 --> 00:26:03,000
So you kind of decompose this all the way down

538
00:26:03,000 --> 00:26:06,320
to the lowest level millisecond muscle control.

539
00:26:06,320 --> 00:26:10,720
But you're not going to plan the entire task of going

540
00:26:10,720 --> 00:26:16,720
from New York to Santa Fe all the way down to millisecond

541
00:26:16,720 --> 00:26:17,960
by millisecond muscle control.

542
00:26:17,960 --> 00:26:20,280
You do a hierarchical planning.

543
00:26:20,280 --> 00:26:22,760
We think humans, that we are the only ones who can do this.

544
00:26:22,760 --> 00:26:24,680
Animals do this, too.

545
00:26:24,680 --> 00:26:26,720
You observe the cat planning a trajectory

546
00:26:26,720 --> 00:26:29,480
to jump on a piece of furniture.

547
00:26:29,520 --> 00:26:31,440
They definitely do a hierarchical planning.

548
00:26:31,440 --> 00:26:34,280
So basically, what you do, what you need for this

549
00:26:34,280 --> 00:26:39,200
is a sort of hierarchy of JPA architectures of predictors

550
00:26:39,200 --> 00:26:41,760
that progressively produce more and more

551
00:26:41,760 --> 00:26:44,840
abstract representations of the state of the world,

552
00:26:44,840 --> 00:26:50,720
so that in the very abstract space of representations,

553
00:26:50,720 --> 00:26:52,120
you can make long-term predictions.

554
00:26:52,120 --> 00:26:55,560
Whereas in the sort of lower levels of abstraction,

555
00:26:55,560 --> 00:26:57,640
you can make shorter term prediction,

556
00:26:57,640 --> 00:26:59,960
but they're more accurate in the short term.

557
00:26:59,960 --> 00:27:02,200
So this is a two-level architecture.

558
00:27:02,200 --> 00:27:04,240
Low-level, you can make short-term predictions.

559
00:27:04,240 --> 00:27:06,240
High-level, you can make longer-term prediction

560
00:27:06,240 --> 00:27:08,800
in a more abstract space that has less details

561
00:27:08,800 --> 00:27:11,200
about how the world works.

562
00:27:11,200 --> 00:27:16,000
Now, we've been able to train a particular instance of JPA

563
00:27:16,000 --> 00:27:18,080
that simultaneously learns teachers that

564
00:27:18,080 --> 00:27:22,240
are good for image recognition and motion prediction

565
00:27:22,240 --> 00:27:23,680
in images.

566
00:27:23,680 --> 00:27:25,920
And I'm not going to go into the details of how

567
00:27:25,920 --> 00:27:26,920
this is pretty hairy.

568
00:27:26,920 --> 00:27:29,160
But it's kind of hierarchical.

569
00:27:29,160 --> 00:27:34,840
And it's got predictors that make pretty strong assumptions

570
00:27:34,840 --> 00:27:37,680
about the type of prediction that can occur.

571
00:27:37,680 --> 00:27:39,840
And simultaneously learns invariant features

572
00:27:39,840 --> 00:27:42,040
for image recognition.

573
00:27:42,040 --> 00:27:45,560
And this works really well for things like image segmentation,

574
00:27:45,560 --> 00:27:49,000
depth estimation, tracking, et cetera.

575
00:27:49,000 --> 00:27:55,320
It's called MCJPA, which means motion and content.

576
00:27:55,760 --> 00:27:58,760
And with this, hopefully, one day,

577
00:27:58,760 --> 00:28:01,680
we'll be able to build architectures that

578
00:28:01,680 --> 00:28:03,760
can perform hierarchical tanning of the type that I

579
00:28:03,760 --> 00:28:05,800
was telling you about.

580
00:28:05,800 --> 00:28:11,680
So observe the world, compute the abstract representation,

581
00:28:11,680 --> 00:28:13,360
and even more abstract representation,

582
00:28:13,360 --> 00:28:15,480
even more abstract representation,

583
00:28:15,480 --> 00:28:18,120
make a prediction to minimize a particular cost

584
00:28:18,120 --> 00:28:20,560
function that defines your task.

585
00:28:20,560 --> 00:28:22,640
I'm assuming this cost function is differentiable,

586
00:28:22,640 --> 00:28:26,560
so we can do this inference by gradient descent.

587
00:28:26,560 --> 00:28:27,880
In first, some latent variable that

588
00:28:27,880 --> 00:28:29,920
may represent the macro action you're going to take,

589
00:28:29,920 --> 00:28:33,720
or some unknown variable about the world.

590
00:28:33,720 --> 00:28:35,520
And then the state you're going to obtain

591
00:28:35,520 --> 00:28:36,960
through the first prediction is going

592
00:28:36,960 --> 00:28:40,480
to constitute a cost function for the lowest level.

593
00:28:40,480 --> 00:28:45,440
So the first predictor at the top tells me

594
00:28:45,440 --> 00:28:47,320
I should be at the airport.

595
00:28:47,320 --> 00:28:48,760
I started from New York.

596
00:28:48,760 --> 00:28:50,160
I should be at the airport.

597
00:28:50,200 --> 00:28:54,360
The cost function below measures how far I am from the airport.

598
00:28:54,360 --> 00:28:57,640
And so the second predictor says, go down in the street.

599
00:28:57,640 --> 00:29:00,360
Take a cab to the airport.

600
00:29:00,360 --> 00:29:03,520
And so the cost function at the bottom here

601
00:29:03,520 --> 00:29:06,120
says, am I on the street?

602
00:29:06,120 --> 00:29:08,960
Likely to catch a taxi, and all the way down

603
00:29:08,960 --> 00:29:12,520
to the actual actions that you can take in the real world.

604
00:29:12,520 --> 00:29:16,120
All right, coming to the conclusion.

605
00:29:16,160 --> 00:29:20,680
So steps towards autonomous AI systems.

606
00:29:20,680 --> 00:29:21,600
Self-supervised learning.

607
00:29:21,600 --> 00:29:24,240
We need a recipe that allows us to train systems

608
00:29:24,240 --> 00:29:25,960
to learn how the world works on video.

609
00:29:25,960 --> 00:29:28,440
I can't claim that we have achieved this.

610
00:29:28,440 --> 00:29:32,120
We're kind of partially there.

611
00:29:32,120 --> 00:29:34,400
And legal uncertainty in the prediction,

612
00:29:34,400 --> 00:29:38,840
and that's with a combination of this JEPA architecture

613
00:29:38,840 --> 00:29:42,520
understood within the context of energy-based model,

614
00:29:42,520 --> 00:29:47,480
potentially with latent variables, which I didn't talk about.

615
00:29:47,480 --> 00:29:50,120
That would allow us to learn world models from observation,

616
00:29:50,120 --> 00:29:53,200
hopefully hierarchical world models, possibly

617
00:29:53,200 --> 00:29:56,920
with interaction as well, and exploration.

618
00:29:56,920 --> 00:30:00,320
And now what we have is an architecture capable of reasoning

619
00:30:00,320 --> 00:30:00,960
and planning.

620
00:30:00,960 --> 00:30:03,080
I mean, the whole architecture I presented

621
00:30:03,080 --> 00:30:05,240
is kind of this idea of system two,

622
00:30:05,240 --> 00:30:07,920
that you can decompose complex tasks into simpler ones,

623
00:30:07,920 --> 00:30:09,360
and then plan a sequence of actions

624
00:30:09,360 --> 00:30:11,560
before you take the action.

625
00:30:11,600 --> 00:30:15,800
Something that's sort of missing from current autoregressive systems.

626
00:30:19,320 --> 00:30:27,880
So is this a potential path towards sort of human-level AI?

627
00:30:27,880 --> 00:30:30,160
Possibly yes, but it's certainly not for tomorrow.

628
00:30:30,160 --> 00:30:33,120
This is maybe a 10-year plan, maybe

629
00:30:33,120 --> 00:30:37,160
to get to cat-level intelligence or something like that.

630
00:30:37,160 --> 00:30:40,200
Now interestingly, those machines will have inevitably

631
00:30:40,240 --> 00:30:43,520
some sort of emotion consciousness.

632
00:30:43,520 --> 00:30:45,840
Forget about this, but emotions certainly,

633
00:30:45,840 --> 00:30:50,560
because emotions are kind of an anticipation of outcome, most of them.

634
00:30:50,560 --> 00:30:55,760
I mean, some of them are immediate perception of outcome,

635
00:30:55,760 --> 00:30:57,360
like pain and things like that.

636
00:30:57,360 --> 00:30:59,400
But most of them are anticipation of outcome,

637
00:30:59,400 --> 00:31:02,320
and this cost function is exactly what this is.

638
00:31:02,320 --> 00:31:06,640
And so if the system sort of predicts a particular set of outcome

639
00:31:06,640 --> 00:31:15,920
that results in a bad outcome, it might feel something similar

640
00:31:15,920 --> 00:31:17,200
to fear or something of that type.

641
00:31:19,960 --> 00:31:25,640
Anyway, so common sense is a collection of world models,

642
00:31:25,640 --> 00:31:29,000
or perhaps a single world model that is configurable.

643
00:31:29,000 --> 00:31:32,320
I'll come to this in one second.

644
00:31:32,320 --> 00:31:35,920
Understanding really means being able to predict.

645
00:31:35,920 --> 00:31:39,080
I think prediction is really the essence of intelligence here,

646
00:31:39,080 --> 00:31:44,440
and better mental models need to better understanding,

647
00:31:44,440 --> 00:31:49,880
or other substrate, if you want, of understanding.

648
00:31:49,880 --> 00:31:55,680
And as a consequence, also of good reasoning and action planning.

649
00:31:55,680 --> 00:31:58,240
The complex part in all of this is going

650
00:31:58,240 --> 00:32:00,480
to be to design intrinsic cost functions that

651
00:32:00,480 --> 00:32:05,040
drive the system towards learning appropriate things.

652
00:32:05,080 --> 00:32:11,840
And it's quite possible that, in the case of leaving things,

653
00:32:11,840 --> 00:32:16,320
it's easier for evolution to hardwire your cost functions into us

654
00:32:16,320 --> 00:32:18,440
than to hardwire your behavior.

655
00:32:18,440 --> 00:32:23,320
Hardwiring behavior and physical models and whatever is super hard.

656
00:32:23,320 --> 00:32:27,000
Like, as a neural net person, I would have no idea

657
00:32:27,000 --> 00:32:29,400
how to architect neural nets to do this.

658
00:32:29,400 --> 00:32:33,280
But I can certainly design a cost function that, if minimized,

659
00:32:33,280 --> 00:32:37,120
the system will learn those basic concepts.

660
00:32:37,120 --> 00:32:42,240
And that, there is a lot of hardwiring in there, no question.

661
00:32:42,240 --> 00:32:44,760
So one module I didn't talk much about is the configurator.

662
00:32:44,760 --> 00:32:47,880
And what it's supposed to do is configure all the modules

663
00:32:47,880 --> 00:32:51,840
in this architecture for a particular sub-task

664
00:32:51,840 --> 00:32:55,760
that the system needs to be focusing on at the moment.

665
00:32:55,760 --> 00:32:57,880
And I'm imagining that there is actually

666
00:32:57,880 --> 00:33:01,400
a single world model engine in this architecture that

667
00:33:01,400 --> 00:33:04,320
is reconfigurable for the task at hand.

668
00:33:04,320 --> 00:33:07,680
But it's not like the system would have multiple world models

669
00:33:07,680 --> 00:33:08,960
for different situations.

670
00:33:08,960 --> 00:33:10,800
It's got a single one that's configurable.

671
00:33:10,800 --> 00:33:15,960
The advantage of doing this, I mean, for humans and animals,

672
00:33:15,960 --> 00:33:18,960
is that it might actually fit in your skull.

673
00:33:18,960 --> 00:33:22,320
But there is another algorithmic advantage,

674
00:33:22,320 --> 00:33:27,080
or epistemic advantage, which is that a single one model can

675
00:33:27,080 --> 00:33:30,120
share knowledge between different situations.

676
00:33:30,600 --> 00:33:34,080
Whereas if you had a separate world model for different situations,

677
00:33:34,080 --> 00:33:36,360
you would have to retrain it independently

678
00:33:36,360 --> 00:33:38,040
for each of those situations.

679
00:33:38,040 --> 00:33:40,800
So how to make this configurator work, I have no idea.

680
00:33:40,800 --> 00:33:43,840
But that's a good hypothesis.

681
00:33:43,840 --> 00:33:46,480
So that would explain the fact that there is a single world model.

682
00:33:46,480 --> 00:33:49,360
It would explain why humans and many animals

683
00:33:49,360 --> 00:33:53,120
can only focus on the single conscious task at any one time.

684
00:33:53,120 --> 00:33:57,760
Because we only have one world model.

685
00:33:57,800 --> 00:34:03,120
We can only do system two on one task at a time.

686
00:34:03,120 --> 00:34:05,040
And I just leave the question for.

