WEBVTT

00:00.000 --> 00:08.160
Perhaps Catherine should just introduce ourselves. So we were at Brown University for several decades

00:08.160 --> 00:13.840
in the Department of Cognitive and Holistic Science there. I was also joined in Computer Science.

00:14.880 --> 00:22.080
Moved to Australia about 14 years ago to Macquarie University and at the same time I think we started

00:22.080 --> 00:31.600
becoming external faculty here at SFI. Catherine got a gigantic mega grant that required hiring a

00:31.600 --> 00:38.400
lot of people doing a lot of work there. I started going into the startup route

00:39.840 --> 00:45.360
and then what with that and then with COVID I think we let the external faculty connection lapse

00:46.160 --> 00:52.160
but now we'd like to restart the connection with SFI again.

00:54.560 --> 00:58.240
The startup stuff that I was doing was actually in the chatbot space.

01:00.240 --> 01:05.600
We got acquired by Oracle Corporation. That's the reason for this disclaimer down here at

01:05.600 --> 01:12.000
the bottom right. So nothing that I'm saying represents anything of any of our employers.

01:12.720 --> 01:17.760
The other reason for this comment here is because the whole field of large language

01:17.760 --> 01:25.200
bottles is really changing incredibly quickly. I don't know how much of what I'm going to say

01:25.200 --> 01:32.080
right now is going to be true or relevant in six months time. So nothing, I'm not saying anything

01:32.080 --> 01:38.640
that I believe is false but it could well, some of the effort could well be wasted.

01:38.960 --> 01:48.640
So I'm going to try and give you guys the high level executive overview talk. This talk was

01:48.640 --> 01:54.480
really actually aimed originally at academic researchers in natural language processing

01:54.480 --> 02:01.120
and so I'm going to skip some of those some of those details but you know you had to have been

02:01.120 --> 02:06.800
underneath a rock if you didn't notice the large language models who radically changed the field

02:06.800 --> 02:16.320
in the last year or two and so I'll be talking about that. We'll also be talking about suggestions.

02:16.320 --> 02:22.640
You know how should academics actually respond to this changing world? So we'll be talking a little

02:22.640 --> 02:28.960
bit about what these large language models may mean or may not mean for human language acquisition,

02:29.040 --> 02:36.480
the study of that. There's a lot of interest in neuro-symbolic models that integrate large

02:36.480 --> 02:43.200
language models and more traditional kinds of AI models and I'll also talk a little bit about

02:43.200 --> 02:52.800
alignment of LLMs and I'll end talking a bit about the connections between large language models

02:52.800 --> 02:59.680
and the social implications of them and lessons from the first industrial revolution.

03:01.040 --> 03:06.240
Katherine and I just spent two months at the University of Edinburgh and it was very interesting

03:06.240 --> 03:11.360
there because of course the first industrial revolution really happened in Northern England

03:11.360 --> 03:20.320
and around Scotland and actually it's a fascinating thing to read about that you know James Watt,

03:20.320 --> 03:25.280
you know the inventor of the steam engine actually did a lot of his work at the University of Glasgow

03:26.320 --> 03:30.880
so that's sort of really interesting. Okay so how deep learning changes NLP?

03:33.840 --> 03:38.720
You know people like Katherine and I have been in the field long enough to have actually seen

03:38.720 --> 03:44.800
really a switch from symbolic parameters, statistical NLP and then finally now to deep

03:44.800 --> 03:52.480
learning and large language models and I guess I would say that the really huge difference is that

03:52.480 --> 04:00.800
large language models can understand context much much longer contexts than anything that I thought

04:00.800 --> 04:07.520
was even possible. In fact I actually thought there were good theoretical arguments that it should

04:07.520 --> 04:13.760
not be possible to try and model a context as long as a sentence or a dialogue or an entire

04:13.760 --> 04:20.960
paragraph yet large language models can do that perfectly well and if you want to I can go through

04:20.960 --> 04:30.080
what those counting arguments would be but just you know when I actually look back at the

04:30.080 --> 04:36.640
pre-deep learning work you know I did a lot of work on syntactic parsing which would involve

04:36.640 --> 04:42.720
recovering pastries that looked something like this and I think the motivation for dealing with

04:43.440 --> 04:52.560
you know wanting structures like these these things make local a lot of dependencies which

04:52.560 --> 05:00.000
are non-local in the string here right so you know it's a flight through Denver right so

05:00.800 --> 05:07.200
this relationship here is making that local the same thing also we're talking about a preference

05:07.200 --> 05:13.680
for a flight for example that's making that dependency local why do you want to do that you

05:13.680 --> 05:21.040
want to do that if you think that you can only really model pairs pairwise interactions large

05:21.040 --> 05:27.840
language models as I said are happy modeling contexts of you know in the hundreds or the

05:27.840 --> 05:34.880
thousands of tokens and so I actually think you know these theories are correct but they're just

05:34.880 --> 05:40.160
no longer required so to do natural language understanding I don't actually have to recover

05:40.160 --> 05:46.800
this sort of structure anymore okay so strengths of large language models we just talked about

05:46.800 --> 05:53.680
how there's so much better at working with incredibly long contexts I think the other you

05:53.680 --> 06:00.320
know I'm giving you the executive summary here the other really amazing thing is that you know

06:00.320 --> 06:05.440
if you showed these large language models to people from the first neural network revolution

06:05.440 --> 06:12.640
from the 1980s actually everything would be really quite familiar to them right there's

06:12.640 --> 06:19.600
there's a slight twist with the attention mechanism but you could explain that in about 15 minutes

06:19.600 --> 06:26.000
and those guys understand it all you know and Sutton has this thing which he calls the bitter

06:26.080 --> 06:32.720
lesson which is that you know for people like me who work on lots of really interesting algorithms

06:32.720 --> 06:38.640
and models and structures and you know things like that actually really none of it really

06:38.640 --> 06:46.960
seemed to be relevant if you just got data and compute and scaled everything up that that really

06:46.960 --> 06:52.560
seems to be the secret and I actually do think that these large language models are largely just

06:52.640 --> 06:58.640
sort of brute forcing linguistic generalization so they're covering rather than really capturing them

06:59.200 --> 07:04.080
but it's really interesting how well they work right that in fact actually with enough compute

07:04.080 --> 07:11.440
and enough data you do seem to be able to produce something that's really quite amazing okay weaknesses

07:11.440 --> 07:17.200
of large language models I mean I think this is probably everybody here's already heard about

07:17.200 --> 07:23.520
this stuff you know Emily Bender's stochastic parrots you know or as I guess Gertrude Stein would

07:23.520 --> 07:30.000
say there's no there there and I think actually these large language models just you know they

07:30.000 --> 07:37.040
respond reflexively in fact I actually think if we're talking about human models these large

07:37.040 --> 07:43.280
language models maybe actually aren't such so bad for the purely reflexive processing that maybe

07:43.280 --> 07:50.080
humans might be able to do but scaled up you know whereas humans maybe have a context window

07:50.080 --> 07:54.960
of five or something like that these large language models have a context window of five

07:54.960 --> 08:02.000
thousand for example and I think actually that's also because they have no beliefs desires or

08:02.000 --> 08:11.440
intentions they have no you know guiding you know thoughts that's why hallucinations such a big

08:11.440 --> 08:19.120
problem you know they're just trying to produce plausible output I think it's really interesting

08:19.120 --> 08:25.040
to actually wonder about you know these large language models now becoming multimodal how will

08:25.040 --> 08:30.880
that change things I actually think if you think the fundamental problem is a problem of symbol

08:30.880 --> 08:39.440
grounding then arguments like Searle's Chinese room argument still are just as valid against

08:39.440 --> 08:45.200
multimodal input on the other hand I actually think the multimodal inputs incredibly interesting

08:45.200 --> 08:51.040
might be incredibly powerful yes so naive questions what is multimodal training oh so here

08:51.040 --> 08:56.720
what I really think is vision and language I should have made that clear vision and language

08:58.160 --> 09:05.360
but these days in fact people are now starting to train off movies as well right so that's I mean

09:05.360 --> 09:11.760
arguably that is one big difference between these large language models and

09:14.800 --> 09:19.600
you know the way humans might work right and what is Searle's Chinese room

09:21.200 --> 09:28.480
that's a really interesting thought experiment which I think for reasons of time I might just

09:28.560 --> 09:36.880
skip here but it's a it's basically where he asks he asks us to imagine a blind symbol

09:36.880 --> 09:42.720
following system and he says so can you really do you really want to say that this blind symbol

09:42.720 --> 09:48.880
for you know rule following system actually has any understanding it doesn't really seem to make

09:48.880 --> 09:58.080
sense to attribute any any understanding to it anyway I do actually think you know there's a lot

09:58.080 --> 10:03.440
of people now that are also wondering are these large language models are they really intelligent

10:04.160 --> 10:11.520
could they be a AGI artificial general intelligence that maybe might take over the entire world

10:12.240 --> 10:19.040
I think they currently can't because they do lack you know beliefs desires intentions the ability to

10:19.040 --> 10:25.440
form long-term memories for example but I'm I'm actually not so sure that that's such a huge

10:25.520 --> 10:33.200
technological barrier I actually do think that it's possible that that could be relatively easy

10:33.200 --> 10:39.520
now you know we've been in this situation before where we thought oh the thing that's really missing

10:39.520 --> 10:45.520
from this machine being intelligent is x you add x to the machine and all of a sudden you discover

10:45.520 --> 10:51.040
well actually you know there's this other thing why that's missing as well and so that may happen

10:51.040 --> 10:58.400
here too but there is a chance that in fact if we just added episodic memory to these large language

10:58.400 --> 11:04.880
models then they may actually become intelligent things the other okay another high level point

11:04.880 --> 11:11.680
that I want to make is that these large language models are manufactured systems in the same way

11:11.680 --> 11:16.560
remember I was talking about the first industrial revolution in the same way that a steam engine

11:16.640 --> 11:22.400
is a very highly engineered manufactured system you know you wouldn't want to be trying to

11:23.040 --> 11:29.040
infer the laws of thermodynamics or even the ideal gas laws by examining a steam engine

11:29.760 --> 11:36.080
I think the same thing's true also about these large language models you know they're they're

11:36.080 --> 11:43.520
trained in a way which is really an engineered product you know and certainly as somebody being

11:43.520 --> 11:50.160
in industry I can tell you that you know the people in industry that are building these things

11:50.160 --> 11:54.560
really don't care about doing science they're trying to build a product that they can sell

11:54.560 --> 12:06.240
and that's really what they care about okay so just an aside about you know the relationship

12:06.320 --> 12:16.080
between I I actually think a natural language understanding and say you know the science of

12:16.080 --> 12:23.360
of linguistics or language acquisition so there's the scientific side and the technological side

12:23.360 --> 12:29.280
I think the technology is currently outstripping the science and I think that has happened in

12:29.280 --> 12:33.440
times before in fact I think it actually happened with the first industrial revolution

12:33.440 --> 12:40.720
so I won't spend too much time here but my understanding is and maybe there's people

12:41.280 --> 12:48.080
historians of science in this room that know more about this than I do but that you know

12:48.080 --> 12:54.000
really in fact actually people started to invent steam engines long before they had the scientific

12:54.000 --> 13:00.720
understanding first of all of ideal gas laws and then ultimately right through thermodynamics

13:00.720 --> 13:07.360
and statistical mechanics that took centuries to the scientific for the scientific side to

13:08.080 --> 13:14.480
emerge and in fact actually ideas of things like entropy really actually had to be developed

13:15.280 --> 13:21.680
to answer questions about why it was not possible to build steam engines above a certain level of

13:21.680 --> 13:28.960
efficiency for example and I suspect the same thing may be true today that our science of

13:29.040 --> 13:40.160
say language and psychology is actually behind the technology. Okay one of the things that I

13:40.160 --> 13:45.520
actually quite like is you know this comment here natural language is the new programming language

13:47.760 --> 13:55.280
and that yeah I mean certainly for those of us in industry LLMs are really changing the way in

13:55.280 --> 14:04.560
which we do our work right whereas it used to take a team of real experts to build say for

14:04.560 --> 14:09.680
example a device which would identify all the financial products that are mentioned

14:10.800 --> 14:16.480
you know in a particular document. Now you can just simply ask a large language model to do that

14:16.480 --> 14:24.080
for you and it does a pretty good job maybe not as good as the very best hand-built natural language

14:24.080 --> 14:30.720
processing system so those are still actually better but you know they take months or years to

14:30.720 --> 14:39.440
develop whereas it takes you know maybe hours to use a new large language model so I actually think

14:39.440 --> 14:44.640
that particularly in terms of the commercial implications the commercial deployment of natural

14:44.640 --> 14:52.080
language processing in industry that's going to change completely. It's not clear we'll need nearly

14:52.080 --> 14:59.360
as many experts in natural language understanding for example for the industrial applications.

15:01.760 --> 15:08.000
I did want to mention a little bit you know I think that one of the really interesting

15:09.520 --> 15:13.760
things that's happening in the field is taking these large language models

15:13.760 --> 15:21.760
and then combining them with other components. The first component that people started to look at

15:21.760 --> 15:28.160
was combining large language models with what's called a vector store or a retrieval system

15:28.880 --> 15:36.880
and that's just simply something whereby when you ask a question instead of just directly asking

15:36.880 --> 15:44.240
the large language model to respond to that question you retrieve a set of relevant documents

15:44.240 --> 15:49.520
to that question feed those in as part of the input to the large language model you can see

15:49.520 --> 15:54.400
that's what I'm suggesting that you do over here and then you then tell the large language model

15:54.400 --> 16:02.320
to use to produce an answer that just simply references those documents and that's sometimes

16:02.320 --> 16:12.880
called the reader retrieval model or retrieval augmented generation. That idea can get even

16:12.880 --> 16:18.160
more power when you start to think well maybe in fact the large language model can actually decide

16:19.120 --> 16:24.560
what information to do a search for and then when you then started to think well should it

16:24.560 --> 16:29.440
decide what information to do a search for maybe in fact it could also call other tools

16:30.080 --> 16:35.600
so these large language models are infamous for not being able to do numerical calculations very

16:35.600 --> 16:40.960
well but maybe in fact what we should be doing is giving the large language model the ability

16:40.960 --> 16:47.040
to call a calculator and there's just in the same ways which if I was to ask any of you guys to do

16:47.120 --> 16:52.880
a complex task and involve something some numeric calculation I'd want you to be

16:53.920 --> 17:00.400
also using a calculator rather than trying to do it longhand. Okay so in terms of research

17:00.400 --> 17:08.080
directions inside of an LLM world so the very first comment to make is that it is very challenging

17:08.080 --> 17:14.240
for academics to do research in large language models you know the ideal thing would be to have

17:14.240 --> 17:22.400
something like an ideal gas experiment set up but and you certainly can build small

17:23.200 --> 17:29.840
versions of these large language models there's some disagreement about whether or not though

17:29.840 --> 17:36.880
whether there's well there certainly seem to be emergent capabilities so the bigger the model

17:36.880 --> 17:42.240
the more things that it can do there's big arguments about whether or not this emergence

17:42.320 --> 17:52.560
is like a phase change or whether it's really more incremental and I again I'd be happy to talk

17:52.560 --> 17:59.440
about that later we could spend hours talking a bit about that but I think there's enough

18:00.320 --> 18:06.160
lack of clarity about what emergence is but if you wanted to do academic research you really

18:06.160 --> 18:12.080
do want to get access to you know the large language the larger large language models

18:12.800 --> 18:17.680
and the problem is that these you know the best large language models are really complicated

18:17.680 --> 18:26.000
commercial products as we were talking about before and it's what's actually even worse

18:26.000 --> 18:32.800
is that these days for in you know proprietary commercial reasons the companies aren't even

18:32.800 --> 18:38.160
actually telling us all the details of exactly what they're doing so that actually does make

18:38.160 --> 18:45.200
it very hard for academics to really do any sort of academic research in in our own papers

18:45.200 --> 18:51.680
you know I'm collaborating with people at the University of Edinburgh what we wind up doing

18:51.680 --> 19:01.360
is saying we're actually not going to test the closed commercial systems but we will work with

19:01.360 --> 19:07.760
the largest open source systems that are available I think that's not a bad thing to do but it does

19:07.760 --> 19:13.840
mean that you're cutting yourself off from a lot of the really cool systems there yes

19:14.880 --> 19:21.120
this is a very interesting question on that slide about how quickly they degrade as you move just

19:21.120 --> 19:27.120
as we moved from commercial ones to smaller and smaller ones yes just a steady degradation

19:27.200 --> 19:34.000
or is there a sudden drop so sorry and in fact actually this also gets back to the emergence

19:34.000 --> 19:40.400
question so let me just say I actually do think that a lot depends on exactly how you measure it

19:41.040 --> 19:49.600
so I don't have to tell people particularly at the Santa Fe Institute right that quite often

19:49.600 --> 19:55.120
what you'll actually see is a lot of small changes all of a sudden reaching a tipping point

19:55.120 --> 20:01.280
that is basically like a phase change and you know when you think about these large language models

20:01.280 --> 20:08.400
I mean they generate output token by token if the correct answer is just slightly less probable

20:08.400 --> 20:14.560
than some mistake right well then as the output gets very very long then the correct answer can be

20:14.560 --> 20:21.920
incredibly improbable right so if you're just looking at the output of 100 tokens or more

20:22.880 --> 20:26.480
you're just looking at the output you're just asking is the output right or wrong you'll go

20:26.480 --> 20:33.360
wrong wrong wrong wrong wrong right and then all of a sudden as the correct token probability

20:33.360 --> 20:40.560
just nudges above the incorrect tokens you know all of a sudden the output flips and all of a

20:40.560 --> 20:47.200
sudden it's just magically all correct but if you measure the per token probabilities for example

20:47.280 --> 20:50.160
then you'd actually discover a much more continuous change

20:53.200 --> 20:57.760
you know so I think that's actually where a lot of emergence happens and in fact that's I think

20:58.480 --> 21:03.760
the white there's a little academic dispute about whether or not these models have emergent

21:03.760 --> 21:08.400
behavior or not and that's at least my understanding of how you'd resolve that

21:10.560 --> 21:15.360
so I actually do think that a lot of the ideas that people here would have would actually be

21:15.360 --> 21:23.920
very useful for the community to have as well okay all right you know I actually think there's

21:23.920 --> 21:29.600
lots of really interesting questions also you know can we understand what these large language

21:29.600 --> 21:36.160
models are really doing I think it's you know I mean we know something about how language is

21:36.160 --> 21:44.080
processed in the human brain you know we know that none of these models really are realistic

21:46.480 --> 21:51.680
just even understanding you know what these large language models are doing how can you

21:52.320 --> 21:58.160
be sure that they know a syntactic rule or make it even simpler that they know a particular word

21:58.160 --> 22:06.400
so right now and in fact actually I think another really interesting question is

22:07.600 --> 22:14.240
if if these large language models are basically just you know gigantic neural nets as I said before

22:14.240 --> 22:20.800
of a relatively generic type why is it that only human beings can acquire language are they are

22:20.800 --> 22:26.000
you know why is it humans are the only animals that can acquire language right I mean you know

22:26.000 --> 22:30.960
we don't have the biggest brains there are animals with bigger brains if it's just merely

22:32.400 --> 22:36.320
the size you know the number of neurons that we have sitting inside of our skulls

22:36.880 --> 22:43.120
if that's all that determines our ability to do something like learn language why why don't

22:43.120 --> 22:50.160
other animals why don't they have that ability it's very popular now to talk about analyzing

22:50.160 --> 22:56.240
large language models using psychological or psychological linguistic methods I think that's

22:56.240 --> 23:02.640
about the best that I know how to do but I think a lot of these methods were really designed to

23:02.640 --> 23:09.840
work on humans at least agents that have beliefs and again you know in the sense a large language

23:09.840 --> 23:19.760
model doesn't have a belief it's just got reflexes okay so you know just to emphasize

23:19.760 --> 23:26.960
the differences between large language models and and humans right so you know children start

23:26.960 --> 23:35.440
and end learning from much smaller data sets they generalize in particular ways that we actually

23:35.440 --> 23:42.400
understand to new unseen forms I think we don't really know actually how these large language

23:42.400 --> 23:48.400
models generalize I mean they do I think they do generalize but it's very difficult to tell exactly

23:48.400 --> 23:54.880
how they generalize children also actually over generalizing characteristically so Catherine's

23:54.880 --> 24:00.400
an expert in this area but you know these are just a couple of examples that she pointed out you know

24:00.400 --> 24:07.920
where children have taken irregular verbs and either inflicted them in a regular fashion or

24:07.920 --> 24:17.360
over generalize the irregular form she's giggling me you know that makes sense if you think of

24:17.360 --> 24:26.400
giggling as being a verb a bit like tickling for example language learning you know by the time

24:26.400 --> 24:32.640
you're three or four you're a competent speaker of your native language usually but then there's

24:32.640 --> 24:40.560
also some part of language learning that's not really complete until the early teens right and

24:40.560 --> 24:46.400
then you know just in terms of the pragmatics of doing research on large language models right

24:47.040 --> 24:53.280
the time scale of research projects are different so it might take a couple of years for a student

24:53.280 --> 25:00.960
to do a research project studying say human language learning if they're studying something

25:00.960 --> 25:09.920
which was inspired by GPT-4 well in two years time we're probably in GPT-6 you know and the

25:10.400 --> 25:13.920
the the inspiration might be actually sort of completely different

25:16.960 --> 25:23.600
I also this is essentially that same thing as as I was saying before right so evaluation and

25:23.600 --> 25:29.760
testing I think is really a huge challenge that was always difficult inside of natural

25:29.760 --> 25:35.920
language processing but it gets even worse because the inputs to large language models

25:35.920 --> 25:41.360
now instead of just again because this context is so much longer the input is not just a

25:41.360 --> 25:46.880
single sentence it's an entire conversation or entire story or something else like that

25:47.520 --> 25:55.920
so if you want to really evaluate the performance of one of these systems you want to vary not just

25:55.920 --> 26:02.800
you know the last sentence you want to vary the entire context as well from a commercial point of

26:02.800 --> 26:10.080
view I think actually testing is really super important I mean you know you've probably all saw

26:10.800 --> 26:21.120
the Microsoft Bing chatbot which when it was you know the New York Times reporter studied

26:21.840 --> 26:28.080
that chatbot and it announced to the reporter that in fact actually that it preferred to be called

26:28.080 --> 26:35.600
Sydney rather than Bing and then also suggested that really you know the reporter should divorce

26:35.600 --> 26:43.920
his wife I'm sure behaviors that Microsoft was really not too proud of right and I actually

26:43.920 --> 26:50.720
think for commercial purposes it's super important to be able to detect and you know

26:51.600 --> 26:57.360
guaranteed that such behaviors really aren't lurking beneath the surface of your large language

26:57.360 --> 27:07.200
model. Constraint decoding that's just an NLP topic I won't spend too much time on but I actually

27:07.200 --> 27:11.680
do think that there's really interesting work there to look at different ways of actually

27:11.680 --> 27:17.600
constraining the output of a large language model and some real challenges there I think there's

27:17.600 --> 27:24.160
really interesting work about how one actually trains these models as well so I mentioned that the

27:24.160 --> 27:29.360
training procedure is itself actually a very complicated one typically what happens is that

27:29.360 --> 27:34.320
they start up by training with what's called the language model training objective which is where

27:34.320 --> 27:40.960
effectively you're just simply training the model to predict the very next word but a model which

27:40.960 --> 27:47.040
is just simply trained with this large language model training objective on its own doesn't really

27:47.040 --> 27:52.320
engage in useful conversation doesn't really follow instructions very well so it's actually

27:52.320 --> 28:01.200
very typical to follow that up with an additional training step that is you know that involves well

28:01.200 --> 28:06.320
as I said there's reinforcement learning with human feedback and I think that's a really interesting

28:06.320 --> 28:13.040
question I've actually got some theories myself about you know when you want to use one sort of

28:13.040 --> 28:18.400
training objective versus another and if there's people that'll like to talk about that more generally

28:18.400 --> 28:24.000
I think there's a really interesting question which is how do you align the LLM behavior

28:25.360 --> 28:31.120
with well how do you get the LLM to behave the way that you want it to right so you've got these

28:31.120 --> 28:36.240
very general alignment goals like you know follow commands that run right through to don't destroy

28:36.240 --> 28:43.040
humanity so ultimately it's the training data and the training training procedure which is going to

28:43.040 --> 28:51.840
determine the LLM behavior so how exactly do we do that right and I actually do think that there's

28:51.840 --> 28:56.960
good academic research that can be done there largely because the fine-tuning step that I mentioned

28:56.960 --> 29:03.600
this sort of multi-stage training that's pretty modest right tens of thousands of examples or less

29:05.600 --> 29:12.560
and it can be done on sort of fairly modest hardware so I actually think that's a that's

29:12.560 --> 29:19.600
that's a great academic research topic I feel a little bit guilty here because I just mentioned

29:19.600 --> 29:26.800
to you that maybe the only thing which is standing between us and you know a artificial general

29:26.800 --> 29:31.920
intelligence is the ability to have episodic memory and then now I'm going to suggest to you

29:32.560 --> 29:38.880
how we might actually do that and it's you know the most obvious way to do that is to actually

29:38.880 --> 29:44.400
take that retrieval augmented generator that I mentioned before and basically let the large

29:44.400 --> 29:52.880
language model write its own memories and this is basically a suggestion about how you might do that

29:54.560 --> 30:04.800
more generally I actually think that you know people like me have spent decades trying to come up

30:04.800 --> 30:10.800
with specialized knowledge representation systems and specialized inference systems

30:13.440 --> 30:21.600
you know so and this essentially is like a specialized logic you know so knowledge graphs

30:21.600 --> 30:28.960
and one example of that where you try to encode information in entity relation triples for example

30:29.680 --> 30:36.480
but I actually think with LLMs you know one real possibility is that you actually let the

30:38.080 --> 30:41.600
you let the primitive statements actually be natural language statements

30:43.520 --> 30:50.480
so you just have represented inside of your system stored inside of a vector store for example

30:50.480 --> 30:56.240
something like insomnia is a typical symptom of diabetes and then you'd actually let the

30:56.240 --> 31:01.440
large language model itself decide the relationship between these atomic propositions

31:03.280 --> 31:08.800
and so instead of having a specialized knowledge representation language a specialized logic

31:08.800 --> 31:13.920
you'd use natural language and you'd let the large language model actually pass information

31:13.920 --> 31:22.880
from one atomic proposition to another for those of you that are as old as I am you know I mean I

31:22.880 --> 31:32.160
loved prologue and you know very simple horn clause inference procedures so what I just tried to do

31:32.160 --> 31:40.720
here was take take that and sort of show how I might replace first order logic in there with

31:40.720 --> 31:49.600
natural language statements but otherwise you've got proof rules proof structures and so this is in

31:49.600 --> 31:56.640
fact actually a standard you know textbook example of how uh you might wind up doing

31:56.640 --> 32:01.760
inference here so you've asked the question you know can Sam get a degree and you've got a series

32:01.760 --> 32:09.120
of facts about what courses Sam has taken and a series of rules but the difference is all this

32:09.120 --> 32:17.760
is all expressed in natural language rather than in some first order logic form okay all right so

32:17.760 --> 32:23.520
just talking a little bit more about the social implications of all of this

32:25.120 --> 32:30.240
so I think to understand the social implications I think one of the things you probably want to

32:30.240 --> 32:34.880
understand is try and make some guesses about how the field itself might actually evolve

32:36.800 --> 32:45.360
I can see sort of two possible futures one is where we wind up getting ever larger

32:45.360 --> 32:52.240
proprietary monolithic close large language models that you effectively interact with via

32:52.240 --> 32:58.320
web APIs that is the actual model itself the training data everything is kept proprietary

32:58.880 --> 33:06.720
but you can just simply call it over the web another future is that there will be open sourced

33:06.720 --> 33:11.040
language models and the weights will actually be available and you'll be able to do things

33:11.040 --> 33:17.520
like fine tune those weights yourself and right now you know we're in the world where there's both

33:17.520 --> 33:24.000
of these kinds of large language models and the proprietary models are better than the open source

33:24.000 --> 33:30.720
models and I think really the big question about the development of the field is whether or not

33:30.720 --> 33:37.040
fine tuning will turn out to take the open source models and make them competitive with the closed

33:37.040 --> 33:44.320
proprietary models and I call that the 64 billion dollar question because that's probably about the

33:44.320 --> 33:50.080
amount of money that the companies that are investing here sort of have invested

33:51.680 --> 33:59.440
you know the language models are becoming increasingly capital intensive it costs

34:00.000 --> 34:06.800
huge amounts million many millions of dollars to collect the data and actually do the training

34:06.880 --> 34:13.360
of these things and capital intensive industries tend to concentrate you know you just look at the

34:13.360 --> 34:20.000
chip manufacturing where I think there's only one or two fabrication factories in the entire

34:20.000 --> 34:28.320
world that make the top-end chips that we all have in all of our devices so if that is actually

34:28.320 --> 34:34.640
what the future of LLMs is then probably we will see that same sort of concentration

34:35.280 --> 34:42.320
into just a couple of places I suspect the training data will become increasingly important

34:42.320 --> 34:49.520
people are already talking about training data as being the ultimate you know limiting factor

34:50.640 --> 34:54.640
and I think it will become the major differentiator particularly if you want to do things

34:55.280 --> 35:01.920
like build LLMs for very specialized domains like healthcare finance other things like that

35:02.480 --> 35:09.840
but and I actually think data and LLM quality control which goes back to that issue about

35:09.840 --> 35:14.320
testing and evaluation I was talking about before that's going to become increasingly

35:14.320 --> 35:20.480
important in fact I actually when I think about what will somebody like me in industry be doing

35:20.480 --> 35:27.920
in five years time quite possibly you know testing and evaluation will actually be you know

35:27.920 --> 35:36.080
90% of what we do you know we know that fine-tuning can mask a poor large language model right so

35:36.080 --> 35:42.320
we know that you can take take Sydney and do a little bit of fine-tuning and have it at least

35:42.320 --> 35:51.200
superficially call itself being but then Sydney reemerges in the right context as well so I think

35:51.200 --> 35:57.120
one way we'll get around that is we'll start seeing things like certificates of origin

35:57.840 --> 36:05.120
you know we'll be saying you know I guarantee that my large language model has been trained on

36:05.680 --> 36:11.120
just high quality data and the same way as you see certificates of origin for you know

36:11.120 --> 36:18.560
fancy cheeses and things like that you know the cows grazed on grass organically raised on the

36:18.560 --> 36:25.040
south's southern meadows and all that sort of stuff and particularly if in the if that open source

36:25.040 --> 36:32.880
world that I was talking about before if that comes into play I you know I see that has been sort

36:32.880 --> 36:38.480
of really one of the really big challenges I've I've seen how data vendors small startups are

36:38.480 --> 36:43.360
really under incredible pressure to produce something which they can sell because they're

36:43.360 --> 36:50.480
usually cash constrained and the same thing may be true for startups that are producing large

36:50.560 --> 36:58.080
language models they'll be under huge pressure to take somebody else's model and do a few tweaks to

36:58.080 --> 37:06.000
it and try to present it as something that's completely new and yeah impact on nlp jobs

37:07.040 --> 37:15.200
I I actually do think that it's not too far off when we'll be able to say something like

37:15.200 --> 37:20.480
give an instruction to a large language model it's like deploy a chatbot the task is informing

37:20.480 --> 37:26.560
users about the products that you'll find listed in this database over here I want you to interact

37:26.560 --> 37:32.800
with users in a professional tone emphasize customer service rather than price and politely

37:32.800 --> 37:37.840
decline to talk about topics that are related to the products and that will be it that will

37:38.560 --> 37:47.600
build you a chatbot you won't need an expert development team you know I do think that however

37:47.600 --> 37:53.600
that that's not going to come up immediately we will for the next say five years or so we will

37:53.600 --> 37:58.960
need people that can create training data and fine-tune models and as I said before I think

37:58.960 --> 38:05.280
evaluation and testing is just going to get more and more important brought to social impacts

38:05.840 --> 38:12.880
right so I think we already know that deep fakes and fraud are just going to get supercharged by

38:12.880 --> 38:20.960
this sort of technology and yes I think that's true I think we're going to see automation of

38:20.960 --> 38:27.360
jobs not previously automated Krugman has an interesting article in the New York Times just a

38:27.360 --> 38:33.920
couple of days ago where you know he makes the point that it doesn't really matter whether these

38:33.920 --> 38:40.160
LLMs really are intelligent or not that even a souped up auto correct can actually have

38:40.800 --> 38:46.320
quite major implications in terms of productivity he's actually really quite positive he seems to

38:46.320 --> 38:56.080
think that actually these things might you know level society somewhat and they might I mean there's

38:56.080 --> 39:05.120
some evidence that in fact the GPT-4 for example enables poorer workers to work at a higher standard

39:06.800 --> 39:12.240
whereas the best workers are helped less by GPT-4 maybe that's the case

39:14.480 --> 39:21.680
I think there's a number of risks I you know I think we are seeing you know AI models being

39:21.680 --> 39:27.520
trained on public domain data that the creators when they made their data public really had no

39:28.080 --> 39:34.720
intention no one no expectation of their data would be used in this way we're seeing a political

39:34.720 --> 39:39.680
fight right now between media companies and tech companies about the use of data I think that's

39:39.680 --> 39:48.160
still mainly about search rather than training AI models but that same fight I think will continue

39:48.160 --> 39:55.280
I looking back to the first industrial revolution and things like the tragedy of the commons I don't

39:55.280 --> 40:00.560
see any particular reason to expect a socially optimal outcome although I think the writer's

40:00.560 --> 40:06.560
Guild of America settlement actually sounds like it's a pretty forward-looking one and

40:07.520 --> 40:14.400
I'm very pleased to see that I I do think you know I I'm not one of these people that that

40:14.400 --> 40:22.400
poo-poo's the people that are worried about you know AGI and misalignment I don't think we're likely

40:22.400 --> 40:28.640
to be made extinction but to be made extinct but I do think we should be worrying about that

40:28.640 --> 40:32.560
and the final point I'd like to make is that I think these things are economic and political

40:32.560 --> 40:40.160
choices not really technical choices so it's an interesting question so those of us that actually

40:40.160 --> 40:45.680
have the technical expertise probably are in a position to have our voices heard more than what

40:45.680 --> 40:52.560
they would normally be so we should probably make use of that but I actually really do think that

40:53.120 --> 41:00.560
it's not just up to the the tech companies in particular to try and make the important decisions

41:00.560 --> 41:10.480
here so conclusions I think LLM's are here to stay a lot of my people my age remember the AI

41:10.480 --> 41:15.920
winter of the last century I don't think there's going to be an AI winter just simply because

41:17.040 --> 41:23.760
these things are actually way too useful for students intellectual revolutions are a great

41:23.760 --> 41:29.680
time to enter the field because in fact actually the amount of knowledge you need to have to become

41:29.680 --> 41:37.920
an expert is much much less I think LLM's open up new interesting scientific research questions

41:37.920 --> 41:46.400
and directions NLP I think will have less emphasis on clever new algorithms and more on

41:47.040 --> 41:53.440
yeah interaction and integration of models applications data design and training and

41:53.440 --> 41:59.120
much more emphasis on evaluation so thank you very much

42:05.840 --> 42:12.320
any questions or comments yes hi thank you for great talk super interesting I just wanted to

42:13.520 --> 42:17.280
ask you about one thing in the middle of the talk which is about

42:18.080 --> 42:25.680
this kind of neuro symbolic integration and you had this you had this kind of proposal that LLM's

42:25.680 --> 42:32.480
are going to give you parts of bits and then you're going to use those inputs into a logic model

42:33.920 --> 42:37.840
and I wondered like why do you think I mean personally I like

42:38.480 --> 42:44.560
neuro symbolic learning so but I'm interested today like why why do you think that's a good idea

42:44.560 --> 42:50.320
or good approach like why not just let the LLM do the entire thing like what what is it that

42:51.120 --> 42:57.120
so certainly certainly there are people that are betting yeah um you know let's just let the LLM

42:57.120 --> 43:05.760
do the entire thing yeah um I guess the answer I would give there is that there are a lot of

43:05.760 --> 43:12.080
academics and in fact I'm sure many of you've seen this stuff right it's it's now quite you know

43:12.160 --> 43:19.120
there's like a little mini industry of people coming up with things like you know chat GPT

43:19.120 --> 43:29.920
cannot understand negation GPT for does not understand X is Y statements you know and in fact

43:29.920 --> 43:39.200
actually I don't think I'm ashamed but we have a paper that is claiming that LLM's cannot

43:39.200 --> 43:45.680
understand you know do not really properly understand entailment you know that walking

43:45.680 --> 43:54.560
entails moving but moving does not always entail walking um so if you really believe that stuff

43:54.560 --> 44:00.000
if you really believe you know and if you believe it enough so that you actually think that GPT 5

44:00.000 --> 44:04.240
or whatever the next model is that comes out is going to have to exactly that same weakness

44:04.960 --> 44:12.720
the idea then is build a symbolic component that addresses whatever weakness you happen to think

44:12.720 --> 44:19.680
these large language models have but it isn't like I'll admit it is rather risky because these

44:19.680 --> 44:29.040
things are improving so rapidly and I'm not so sure I mean it's it's a risk if you're a grad

44:29.040 --> 44:35.200
student to say all right I'm going to commit the next year or two of my life to working on solving

44:35.200 --> 44:42.560
you know the problem of negation in large language models and halfway through your research project

44:43.920 --> 44:49.600
you know someone discovers that just by scaling up the training data another order of magnitude

44:49.600 --> 44:55.760
all of a sudden now it's going to handle negation just perfectly and that's what I meant by in fact

44:55.760 --> 45:02.000
I think that was one of the one of the slides that Kate added was saying that it's particularly if

45:02.000 --> 45:07.680
you wanted to do something some sort of behavioral research or something else like that you know where

45:08.640 --> 45:14.880
this the timescale which LLMs are changing versus the timescale of doing behavioral research is

45:15.760 --> 45:21.760
the LLMs are just changing so fast that if you if you looked at today's LLMs and said okay you're

45:21.760 --> 45:27.280
inspired by then here are some interesting behavioral predictions that they're making I'm

45:27.280 --> 45:31.760
going to go out and start running some experiments with kids or something like that

45:34.000 --> 45:38.560
yeah you know by the time you've collected a quarter of your data there's another model out

45:38.560 --> 45:45.600
there and it's got it's making different behavioral predictions it's just I don't know what the answer

45:45.600 --> 45:59.440
there is except to say that yeah yeah you might just um ask a a follow-up question so if if you

45:59.440 --> 46:12.080
have a chat system I think that's actually for the zooms but I they also said there's a whole array

46:12.240 --> 46:20.880
microphone inside the box so well um I'll just talk about um if as opposed to um adding a symbolic

46:20.880 --> 46:30.320
system to to a chat system or you know an LLM um what about fine tuning and just doing lots of fine

46:30.320 --> 46:37.280
tuning instead I mean that's adding more data but if you fine tune it yes with that kind of data as

46:37.280 --> 46:42.880
opposed to going as symbolic group that's right wouldn't that be I think that's I think that's

46:42.880 --> 46:51.360
very true and in fact that I think that's a good question is you know if so right now what was it

46:51.360 --> 46:58.880
you know Gary Marcus is picking up on the fact that uh somebody wrote a paper that said that

46:59.600 --> 47:07.200
uh oh look there's a whole lot of cases where the large language models uh will quite happily say

47:07.200 --> 47:14.800
that oh let's see all right I don't know enough about celebrities unfortunately but you know

47:14.800 --> 47:23.440
so-and-so and so is Tom Cruise's mother right okay it accepts that statement and you then ask the

47:23.440 --> 47:30.640
other you then ask the question who is Tom Cruise's mother and it says I have no idea so

47:31.520 --> 47:37.440
and it seems like oh well if I've actually got a couple of comments there so first of all it seems

47:37.440 --> 47:44.560
like x is y that looks an awful lot to a math mathematical person as being like x equals y

47:45.120 --> 47:52.560
and we know that what was it uh equality is what is it commutative you know so if x equals y then

47:52.560 --> 48:04.960
y equals x um in fact actually x is a y actually really isn't commutative um you know uh you know

48:05.360 --> 48:14.800
uh what was it you know chicken salad is a wonderful meal you know that can be true but

48:14.800 --> 48:21.680
a wonderful meal doesn't always have to be chicken salad right um you know so uh

48:26.960 --> 48:29.280
but anyway so might you know the yeah

48:29.280 --> 48:39.360
do you think uh do you think there's going to be uh kind of I mean what what do you think about

48:39.360 --> 48:47.520
like the kind of vision vision language models where you're kind of train it on like a ton of

48:47.520 --> 48:53.120
images and then it generates a load of you know you can generate these images we actually did

48:53.760 --> 48:58.640
you know before I did the startups just before I did the startups I think the last student I

48:58.640 --> 49:05.680
worked with was working on image captioning um and I'd very much like to go back to I I think

49:05.680 --> 49:10.960
that's really incredibly interesting so while I think it doesn't actually solve the chart the

49:10.960 --> 49:18.560
cell Chinese rule Chinese room objection you know I mean ultimately the input to all these models

49:18.560 --> 49:27.120
are really just activation patterns and you know the models got no reason to suspect that images

49:28.640 --> 49:35.840
closely you know more connected to the world than languages but I think just in practice there's a

49:35.840 --> 49:43.680
very good chance that there may be really interesting correlations that can be learned by correlating

49:44.560 --> 49:51.360
you know images with language and of course the only issue there is that the

49:51.520 --> 49:58.880
um the amount of compute that's needed to do this is just really enormous and it's

50:00.160 --> 50:07.200
you know you'd have to do some have to get some deal probably with one of the major tech

50:07.840 --> 50:12.800
companies to get your access to enough compute yeah to do it

50:13.520 --> 50:22.960
and that's that's sort of the problem with with a lot of this research now and in fact I think one

50:22.960 --> 50:27.120
of the comments that I wrote there is you know maybe in fact we should be thinking about something

50:27.120 --> 50:33.760
you know physics has been very successful in getting funding for big science maybe we should

50:33.760 --> 50:40.400
be thinking about ways of getting funding for academic big science as well for dealing with this stuff

50:40.640 --> 50:47.120
well and there if you think of images still images that's a certain amount of compute power

50:47.680 --> 50:53.680
I think of visual no ongoing visual scenes and movies that's a lot more

50:54.960 --> 51:04.400
and yet presumably real world learners are you know taking advantage of the visual scene

51:04.400 --> 51:14.000
as it moves by um and uh although learning can take place in blind people as well and that's

51:14.000 --> 51:19.760
a whole another research area so you know you don't need vision to learn language but it certainly

51:19.760 --> 51:29.680
can facilitate aspects of yeah and then yeah and I guess also I mean like looking forward you'd

51:29.680 --> 51:38.560
want these systems to actually be kind of situated in the physical world somehow I guess right well

51:38.560 --> 51:48.720
so that so that's the certainly lots of people have got that uh you know feeling that in fact

51:48.720 --> 51:57.840
actually that that we need situated you know situated models but I mean isn't the input to

51:57.840 --> 52:04.960
a model always really just an activation pattern isn't it really always just uh I mean couldn't

52:04.960 --> 52:11.920
you always run you know so you know there was the question what is silver's chinese room argument

52:11.920 --> 52:18.400
right there was that basically you know uh supposing you you come up with a computer program

52:18.400 --> 52:25.920
which can translate english into chinese or sorry translate chinese into english so you give that

52:26.000 --> 52:32.400
to a person that's sitting you know inside of a room and you just simply tell him you know here's

52:32.400 --> 52:40.800
a set of symbols follow these instructions and give me the output that you you you obtained

52:40.800 --> 52:48.880
by following these instructions and so's point is that even if this thing does actually produce

52:48.880 --> 52:57.600
good english as an output you really can't say that the person understands chinese you know that just

52:58.320 --> 53:06.640
they don't understand chinese you know they're just following these rules and uh so his argument

53:06.640 --> 53:12.880
really is that there's something else that a pure rule following system really doesn't have

53:12.880 --> 53:18.080
understanding that something else is required now lots of people wind up saying oh well what you

53:18.080 --> 53:26.800
really need is grounding you really do need these you know you need the symbols to be connected somehow

53:26.800 --> 53:33.440
to the real world but I think the model never really knows I don't see any way for our current

53:34.080 --> 53:39.440
computational models to know that the bit patterns that we're feeding into them

53:40.240 --> 53:42.800
yeah correspond to anything in the world

53:44.240 --> 53:49.680
yeah I mean yeah I would agree on that but um I guess for that kind of symbol grounding you

53:50.560 --> 53:57.360
like some some people argue you you need to have a kind of community right language users so

53:58.480 --> 54:08.960
maybe that uh all kind of do you know um and and then the grounding kind of comes out of

54:10.400 --> 54:15.760
people using the same symbol in the same way yes right yeah yes no no no I mean you're

54:16.480 --> 54:21.840
kripke you know so philosophers of language like kripke have argued that in fact actually

54:25.120 --> 54:29.840
that and you know in fact this is sort of very true of me because I grew up in the southern

54:29.840 --> 54:38.320
hemisphere I don't know a lot of the northern trees so I'm not sure I can with catherine's

54:38.400 --> 54:43.520
help now I can recognize aspens but you know I'm not really sure about the difference between

54:43.520 --> 54:50.240
oaks and elms and the rest of them right but kripke you would say that I can still talk about

54:50.240 --> 54:57.200
all of those things and when I talk about oaks I mean oak trees even though I might not be able

54:57.200 --> 55:03.680
to actually identify an oak tree reliably and so kripke's story there is exactly what you're

55:03.680 --> 55:10.000
saying it's a community of language learners sorry language users and I am willing basically I'm

55:10.000 --> 55:17.600
agreeing to the authority of language users and I effectively what I'm saying is when I use oak

55:18.240 --> 55:24.640
I use it to mean whatever the rest of you mean that you know that grew up and presumably know

55:24.640 --> 55:30.800
exactly what an oak tree is right and he says that in fact actually with a lot of particularly

55:30.800 --> 55:38.240
scientific terminology we wind up using it that way right many of us may not be able to define

55:38.240 --> 55:43.920
exactly what the difference is between the different types of neutrinos or whatever but

55:44.560 --> 55:51.040
we rely on experts within our community to be able to ground those things

55:52.640 --> 55:57.520
all I can say is I don't even know how you'd even tell a large language model that it's part of

55:58.480 --> 56:03.680
yeah yeah no I was sort of yeah kind of thinking that sort of thing myself like how

56:04.400 --> 56:11.120
well what I mean does that does the community there just mean literally that actually just the

56:11.120 --> 56:20.960
text documents that have been fed into it is that I mean I guess with the the instruction

56:21.680 --> 56:24.800
tuning I guess you get a little bit of that right

56:28.640 --> 56:36.080
so anyone else have questions at all yeah so that it's almost just that you could have a community

56:36.080 --> 56:45.120
of users of a particular model yeah that would have a sort of various types of queries within a

56:45.120 --> 56:56.400
particular domain that might help train up that model then to become more a realistic conversational

56:56.400 --> 57:05.680
agent within that particular domain perhaps yeah yeah so maybe that's kind of yeah yeah yeah

57:06.800 --> 57:12.880
who knows maybe a year or two from now you know those things will start to emerge yeah

57:12.880 --> 57:17.920
yeah so I think one of the things that Catherine and I are hoping to get out of this is to find

57:17.920 --> 57:27.280
out more about you know work at SFI that we might connect with right but a sort of general interest

57:27.280 --> 57:38.960
in you know language learning psychological aspects computational aspects you know so

57:38.960 --> 57:52.160
so we're here until Tuesday afternoon Tuesday evening so please please contact us right

57:52.160 --> 57:55.360
okay thanks

