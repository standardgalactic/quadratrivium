1
00:00:00,000 --> 00:00:05,680
up in the kitchen. Very excited to introduce Donald Martin, Donald's the head of societal context

2
00:00:05,680 --> 00:00:10,400
and understanding tools and solutions at Google. We don't do much for introductions here, but

3
00:00:10,400 --> 00:00:18,240
Donald let me just turn it over to you. You won't appreciate it. Hello everybody. It's really an

4
00:00:18,240 --> 00:00:22,560
honor and pleasure to be here at Santa Fe Institute. I actually have been dreaming about visiting

5
00:00:22,640 --> 00:00:29,680
SF5 for, I don't know, almost 16 years. I read the book Complexity back in 2007, about 2005. I was

6
00:00:29,680 --> 00:00:36,240
like, I maybe want to go there. And so I'm now here. I'm here. Thanks to Will. Thanks for your

7
00:00:36,240 --> 00:00:43,280
time and attention. The title of my talk is Epistemic Uncertainty, the AI problem understanding

8
00:00:43,280 --> 00:00:48,640
chasm. I kind of updated that from a gap to a chasm, but it's a really big problem. And the necessity

9
00:00:48,720 --> 00:00:55,920
of structured societal context knowledge for safe and robust AI. Now me and my team, which also

10
00:00:55,920 --> 00:01:01,840
goes by scouts, we believe that bridging this problem understanding chasm is critical to being

11
00:01:01,840 --> 00:01:08,560
able to realize our goal of responsibility for this AI. And so we're kicking off a campaign

12
00:01:09,440 --> 00:01:15,360
to try to imply more participation by others in trying to understand and solve this problem.

13
00:01:16,320 --> 00:01:21,680
And so we've got something we call our flywheel. That starts off with increasing awareness just

14
00:01:21,680 --> 00:01:28,080
about the chasm and what the causes are. We hope that drives more foundational and applied research,

15
00:01:29,200 --> 00:01:33,040
particularly in some of the techniques we're going to talk about causal theory model, for example.

16
00:01:33,840 --> 00:01:40,480
And of course, we want that to result in real impact in the world. The use case that we focus on is

17
00:01:40,480 --> 00:01:46,880
health equity. And so right now, we're here's where we are. And so we're kicking this off and

18
00:01:47,840 --> 00:01:52,480
you know, bridging the problem understanding chasm as you will see, we think requires embracing

19
00:01:52,480 --> 00:01:58,720
complexity. So I figured what better place to start talking about this in the Santa Fe Institute.

20
00:02:02,400 --> 00:02:09,200
Few calls to action because I know folks in this room, SFI is influential. So we would love for

21
00:02:10,160 --> 00:02:15,040
after the talk, people to spread the word about this chasm, right, and how important it is to budget.

22
00:02:16,720 --> 00:02:22,640
Hoping that people will also take on the task of embracing prototyping problems as complex

23
00:02:22,640 --> 00:02:27,840
adaptive systems before intervening things like data science and AI because we think that's critical

24
00:02:27,840 --> 00:02:34,400
for proactively mitigating bias in these systems that we're building. And then finally,

25
00:02:35,280 --> 00:02:40,480
a key component of bridging this gap is investing and problem prototyping

26
00:02:41,200 --> 00:02:45,760
trust and capability in historically marginalized communities. That's going to be a key source of

27
00:02:45,760 --> 00:02:50,160
the knowledge that we think is missing from AI based product development.

28
00:02:53,200 --> 00:02:55,520
Okay, so with that, I'm going to set just a little bit of context.

29
00:02:56,880 --> 00:03:00,880
I gave this talk a part of this talk last year, I had to kind of explain there's this industrial

30
00:03:00,960 --> 00:03:04,480
revolution. AI is going to be really important. I don't think I have to really do that anymore.

31
00:03:05,200 --> 00:03:09,760
I think you realize we're in the midst of this transformation that people were calling the

32
00:03:09,760 --> 00:03:15,360
fourth industrial revolution and that AI is like a core technology within it. And of course,

33
00:03:15,360 --> 00:03:20,800
you know, we have high hopes that AI is going to do these amazing things and transform healthcare

34
00:03:20,800 --> 00:03:28,160
and medical care. But we're also very cautious, rightly so because of the big conflict in the room,

35
00:03:28,720 --> 00:03:31,920
harmful bias that can be propagated really easily by these systems.

36
00:03:33,120 --> 00:03:36,400
And so it's part of my job to pay attention to all the headlines that come out every day

37
00:03:37,040 --> 00:03:44,000
about instance of bias and all sorts of domains. But I pay particular attention to

38
00:03:46,320 --> 00:03:51,680
headlines that involve healthcare and medical care. And that's really because of my mom.

39
00:03:51,760 --> 00:03:56,800
With my mom, Betty Martin, she died five years ago this month from cancer.

40
00:03:58,160 --> 00:04:04,240
And as I navigated that harrowing journey with her, I experienced bias directly in the healthcare

41
00:04:04,240 --> 00:04:10,240
system. And that really informs how I think about what it's going to take to mitigate bias in AI

42
00:04:10,240 --> 00:04:15,200
systems applied in high stakes domains like healthcare. And so as I said, she died from cancer,

43
00:04:15,200 --> 00:04:20,960
started off as breast cancer, turned into lung cancer. At one point she had to get part of whatever

44
00:04:20,960 --> 00:04:26,800
lungs removed. And as a result of that, she developed atrial fibrillation. And so that can

45
00:04:26,800 --> 00:04:33,600
cause blood clots and strokes. So she had to get prescribed a blood thinner. But the blood

46
00:04:33,600 --> 00:04:39,360
thinner that the doctor wanted her to take would require her to stop eating vitamin K foods,

47
00:04:40,080 --> 00:04:46,800
leafy greens, spinach, kale, collard greens. I love that stuff. That was like a staple in her diet.

48
00:04:46,800 --> 00:04:49,840
And it would also require her to have to go to the hospital

49
00:04:50,480 --> 00:04:54,240
every week to get her blood tested to make sure this blood thinner wasn't harming her in some way.

50
00:04:55,840 --> 00:05:00,240
And so my mom, she balked at this. She's like, ah, you sure there's not another

51
00:05:00,880 --> 00:05:04,320
option? Is there a better blood thinner where I don't have to shift my lifestyle?

52
00:05:05,200 --> 00:05:10,480
And the doctor's response was, you should just, you know, be grateful because there are many

53
00:05:10,480 --> 00:05:15,200
people in third world countries who would die to have access to this particular blood thinner.

54
00:05:15,920 --> 00:05:18,720
My mom was like, well, you know, I don't know what that has to do with me, but

55
00:05:20,400 --> 00:05:24,320
and you all know my mom, of course, but she didn't accept that as an answer.

56
00:05:25,120 --> 00:05:30,480
She talked to me and my sister, my dad. My sister happens to be a medical doctor, a psychiatrist.

57
00:05:30,480 --> 00:05:34,880
So she was able to find very quickly an alternative blood thinner that wouldn't have

58
00:05:34,880 --> 00:05:41,280
required my mom to change her lifestyle when I owed her. And so, and we could easily afford it

59
00:05:41,280 --> 00:05:46,160
with my dad's health care insurance. So we went back to the doctor. We said, hey, we found another

60
00:05:46,160 --> 00:05:51,360
alternative, but we ought to make sure we're not missing something. Like, why didn't you

61
00:05:52,000 --> 00:05:55,360
recommend this particular blood thinner because it doesn't have any of these side effects?

62
00:05:56,240 --> 00:06:01,680
The doctor's response was, I assumed that she wouldn't be able to afford the alternative.

63
00:06:03,520 --> 00:06:08,000
I assumed she wouldn't be able to afford the alternative. So that's this bias kind of

64
00:06:08,080 --> 00:06:12,800
rearing its head right in the middle of the situation. So that's why this kind of informs,

65
00:06:13,760 --> 00:06:17,280
you know, my thought process about how we're going to, what it's going to take to mitigate

66
00:06:17,920 --> 00:06:25,520
these kind of really nuanced implicit biases. And so the real issue here is that the doctor ignored

67
00:06:25,520 --> 00:06:31,760
and abstracted away my mom's, her situation, her circumstances would all call her a societal

68
00:06:31,760 --> 00:06:37,120
context. Didn't consider that when they made that intervention recommendation. So I want to look at

69
00:06:37,120 --> 00:06:42,720
my mom's societal context in a different way. There's mom, born and raised in the USA, doesn't

70
00:06:42,720 --> 00:06:47,520
know anything about, you know, what happened in third world countries. She loved leafy greens.

71
00:06:48,720 --> 00:06:53,760
This is her second bout with cancer. So she, you know, had already experienced having to go to the

72
00:06:53,760 --> 00:06:57,920
hospital every week to get chemotherapy and get stuck with the nail show. So she would avoid that

73
00:06:59,040 --> 00:07:04,480
anywhere she could if it was possible. And then, you know, she had us as a family, which included

74
00:07:04,480 --> 00:07:11,040
my sister, medical doctor knew how to navigate the drug industry very well. My dad, a double retiree

75
00:07:11,040 --> 00:07:18,640
from US Army and the Postal Service, great healthcare insurance. So we had this, this other option.

76
00:07:19,520 --> 00:07:24,320
But the doctor ignored and abstracted away all this societal context, didn't ask about it,

77
00:07:24,320 --> 00:07:29,120
didn't consider it and then made this intervention recommendation that conflicted with my mom's

78
00:07:29,200 --> 00:07:30,080
societal context.

79
00:07:31,680 --> 00:07:38,320
Conversely, do you think that the doctor thought he was being anti-biased by making

80
00:07:38,320 --> 00:07:42,080
this mistake and thus, in fact, being very biased?

81
00:07:42,640 --> 00:07:50,560
You know, we really have, I have no idea. I don't, so we all have biases, right? We can't

82
00:07:50,560 --> 00:07:54,720
tell what someone's intention is, right? I'm sure there's not a negative intention.

83
00:07:55,520 --> 00:08:01,600
But, but this is the result that when you don't seek to understand like the situation or

84
00:08:01,600 --> 00:08:07,680
circumstances, when you make assumptions, it can lead to these problems. And so we really

85
00:08:07,680 --> 00:08:12,800
use this high-level definition of societal context as this dynamic and complex collection of

86
00:08:13,600 --> 00:08:20,000
social, cultural, economic, political, historical circumstances that surround people,

87
00:08:20,000 --> 00:08:27,600
places and things. And interventions that ignore or abstract away societal context can lead to

88
00:08:27,600 --> 00:08:32,160
unintended and unnecessary harm. So this is the key takeaway for the talk, like if you don't

89
00:08:32,160 --> 00:08:36,480
remember anything else, please remember this and try to get that in as you're intervening in

90
00:08:36,480 --> 00:08:43,440
technologies. So now I'm going to dive into the core of the talk. Here's, here's the five parts

91
00:08:43,440 --> 00:08:46,880
I'm going to walk through. First, I want to talk about how we abstract away societal context when

92
00:08:46,880 --> 00:08:51,920
we do ML and AI based product development. Then I'm going to introduce this problem, understanding

93
00:08:51,920 --> 00:08:58,640
chasm. I think it's the root cause of why we do this, abstracting away. And then the last three

94
00:08:58,640 --> 00:09:03,600
seconds are about what we think some of the elements of bridging this chasm are. One is we

95
00:09:03,600 --> 00:09:10,880
think we need some sort of reference frame or model of societal context. Second, we think we need

96
00:09:10,960 --> 00:09:17,920
a way of producing societal context knowledge and we think prototyping problems as complex

97
00:09:17,920 --> 00:09:22,560
adaptive systems could be a way to do that. And community-based system dynamics involves

98
00:09:22,560 --> 00:09:26,960
communities in doing that work. And then finally, even if you have this great technique,

99
00:09:26,960 --> 00:09:32,240
you have to have capability in the world in order to like execute it. So let me dive into the first

100
00:09:32,240 --> 00:09:37,760
part. I'm going to start this off with a quote from a great paper called Fairness and Abstraction in

101
00:09:37,760 --> 00:09:44,560
Social Technical Systems by Andrew Selps, who's a leader, not leader in the ML Fairness community.

102
00:09:44,560 --> 00:09:48,240
If you haven't read this paper, even if you don't care about ML Fairness, I highly recommend it.

103
00:09:49,360 --> 00:09:54,560
But this quote sums it up really well. Bedrock concepts in computer science such as abstraction,

104
00:09:54,560 --> 00:10:00,240
render technical interventions, effective and accurate and sometimes dangerously misguided

105
00:10:00,240 --> 00:10:05,280
when they enter the societal context that surrounds decision-making systems. So you could apply this

106
00:10:05,280 --> 00:10:10,240
directly to what happened with my mom. In this case, we're talking about building products and

107
00:10:10,240 --> 00:10:16,960
systems. So I'm going to just walk through a really high-level view of typical AI product

108
00:10:16,960 --> 00:10:20,960
development lifecycle. I'm using a product development context because I work at Google

109
00:10:20,960 --> 00:10:26,640
when we build AI-based products. The key thing they're recognizing this is that this whole process

110
00:10:27,360 --> 00:10:32,240
is driven by human decision-making from end to end. A lot of times people forget that.

111
00:10:33,200 --> 00:10:39,840
And the main output, of this lifecycle, is a model-based product that generates

112
00:10:39,840 --> 00:10:46,000
outputs, typically predictions, that somehow can help you automate or automate a process,

113
00:10:46,000 --> 00:10:50,480
solve some sort of problem. Of course, to produce that product, we have to train and

114
00:10:50,480 --> 00:10:55,440
tune and evaluate that model. To do that, we have to select and prepare some training data.

115
00:10:56,560 --> 00:11:01,760
To do that, we actually have to actually formulate this problem for machine learning and AI.

116
00:11:01,760 --> 00:11:06,080
We have to take that problem and make it fit the hammer that we have, that we want to use to

117
00:11:06,080 --> 00:11:09,520
solve the problem. And then in order to do that, well, we have to actually have a deep

118
00:11:09,520 --> 00:11:15,280
understanding of the problem itself. So that's typically kind of where this diagram ends.

119
00:11:15,920 --> 00:11:19,280
But of course, there's a key input that drives this whole thing, and that's

120
00:11:19,840 --> 00:11:27,600
observations and data. But sometimes we forget about where those observations and data come from.

121
00:11:27,680 --> 00:11:30,320
They're coming from this thing we're calling societal context.

122
00:11:31,280 --> 00:11:35,440
And then the machine learning fairness, sometimes we call it the social-technical environment.

123
00:11:36,560 --> 00:11:40,160
Computer scientists and engineers and practitioners who are trying to get things done,

124
00:11:40,160 --> 00:11:45,680
they have many different words for, you know, that area, this kind of ambiguous area,

125
00:11:45,680 --> 00:11:49,760
amorphous area. It's the real world where this stuff is going to end up one of these days.

126
00:11:49,760 --> 00:11:56,800
It's the environment. It's the data-generating process. It's where we have to look to kind of

127
00:11:56,880 --> 00:12:02,160
understand the hypothesis space. It's that problem domain. It's where that background

128
00:12:02,160 --> 00:12:08,960
knowledge is going to come from. It's the broader context. And then we have some characteristics

129
00:12:08,960 --> 00:12:17,360
that we associated with this amorphous thing. It's messy. It's complex. It's qualitative.

130
00:12:18,320 --> 00:12:27,760
It's causal and nonlinear. It's subjective. I need to work on objective things. It's dynamic,

131
00:12:28,720 --> 00:12:32,080
and it's historical. Why do I have to worry about what happened decades ago?

132
00:12:33,840 --> 00:12:38,800
Now, the issue is that those characteristics don't align very well with the statistical

133
00:12:38,800 --> 00:12:46,480
processes and methods that we use to build AML models. So that's why we tend to

134
00:12:47,360 --> 00:12:51,920
put up this abstraction boundary and abstract it away. And we have particular practices and

135
00:12:51,920 --> 00:12:59,920
assumptions that reinforce that. One of those is the IID assumption, which assumes that the

136
00:12:59,920 --> 00:13:04,000
variables we're going to try to learn in the training data assumes that there's no

137
00:13:05,200 --> 00:13:08,480
that there's no relationship between them. There's no causal relationship between them,

138
00:13:08,480 --> 00:13:14,800
among other things. And so that assumption, you make that assumption, you're ignoring

139
00:13:14,800 --> 00:13:20,080
abstracting away the very nature of societal context, which is supplying all the data.

140
00:13:24,080 --> 00:13:28,640
Now, the other thing that happens when we extract away the societal context is easy to

141
00:13:28,640 --> 00:13:32,880
forget that eventually you're going to be done with training, the products are going to be in

142
00:13:32,880 --> 00:13:36,400
the real world, and the inputs aren't going to be the training data anymore. They're going to

143
00:13:36,400 --> 00:13:41,280
come from the real world. And those inputs are not going to follow the rules, the assumptions

144
00:13:41,280 --> 00:13:45,520
that we made when we built the systems. And then those outputs are not going to be in the

145
00:13:45,520 --> 00:13:51,040
laboratory anymore or in the evaluation suite. They're actually going to be interventions

146
00:13:52,080 --> 00:13:55,200
on that societal context on society. They're going to cause impact.

147
00:13:58,640 --> 00:14:04,160
And then, of course, those interventions can have an impact on the next round of observations

148
00:14:04,160 --> 00:14:10,880
and data we sample in order to build the next version of that product. So you've got this feedback

149
00:14:10,880 --> 00:14:16,560
loop that can be vicious if we're not careful about the outputs and the quality of the outputs.

150
00:14:18,880 --> 00:14:22,400
Now, the other thing that happens when you're abstracting away

151
00:14:23,440 --> 00:14:27,760
societal context and understanding the details of the data-generating process and the problem

152
00:14:27,760 --> 00:14:34,640
domain is it leads to a lack of knowledge on the other side of that boundary, lack of knowledge

153
00:14:34,640 --> 00:14:39,040
about the problem domain and the data-generating process, things you have to understand to formulate

154
00:14:39,040 --> 00:14:46,080
the problems well. Now, another term we use for that in machine learning AI is epistemic

155
00:14:46,080 --> 00:14:50,720
uncertainty, just a fancy word for lack of knowledge. Usually, we're talking about lack of

156
00:14:50,720 --> 00:14:57,760
knowledge about the model by the model, but that lack of knowledge also applies to all the decision

157
00:14:57,760 --> 00:15:01,680
makers in this ecosystem, which include the humans who are making all these critical decisions.

158
00:15:03,040 --> 00:15:08,960
And in reality, models are basically inheriting the epistemic uncertainty of the decision makers

159
00:15:09,120 --> 00:15:15,440
who are making all the pipeline decisions that are going to impact how that model performs it with

160
00:15:15,440 --> 00:15:22,880
the outputs. Now, it turns out that epistemic uncertainty is the root cause of two big problems

161
00:15:22,880 --> 00:15:29,360
in AI. One is harmful bias propagation, which I'm going to talk about in more detail in a second,

162
00:15:29,360 --> 00:15:37,280
but the other is deep learning robustness. And this also goes by out of distribution

163
00:15:37,280 --> 00:15:42,560
generalization. Distribution shift fits into this category. So, I'm going to drill into that for

164
00:15:42,560 --> 00:15:48,560
just a second to show how experts in this problem are, when I read them, they're saying, we need

165
00:15:48,560 --> 00:15:56,800
societal context knowledge to solve this problem. So, the fundamentals of this deep learning

166
00:15:56,800 --> 00:16:02,080
robustness problem is this example where you have a classifier that's trained to identify

167
00:16:02,080 --> 00:16:09,280
objects and images, gets really good at it, high performance accuracy, really good at identifying

168
00:16:09,280 --> 00:16:16,720
this pig. But if you just change very slightly the input data, the image, in ways that would be

169
00:16:16,720 --> 00:16:21,840
imperceptible to a human, the classification changes to like, ah, this is an airliner.

170
00:16:22,800 --> 00:16:27,760
Right? So, that's the kind of classic example. And so, the issue here is that the classification

171
00:16:27,760 --> 00:16:33,920
model doesn't have an equivalent conceptual model that we have. And these are kind of structures

172
00:16:33,920 --> 00:16:40,480
in our heads that allow us to reliably identify objects under really noisy conditions.

173
00:16:42,000 --> 00:16:47,760
And so, the experts who are trying to solve this problem, and one of them is Yashio Benjo,

174
00:16:47,760 --> 00:16:54,080
who is considered one of the so-called godfathers of AI. When I read what they're saying,

175
00:16:54,080 --> 00:16:58,560
when I read their words, I see they're saying, we need societal context to help us to solve this

176
00:16:58,560 --> 00:17:05,120
problem. So, just to read a quote from another really good paper called Tor Causal Representation

177
00:17:05,120 --> 00:17:11,360
Learning, we argue that causality with its focus on representing structural knowledge about the

178
00:17:11,360 --> 00:17:17,040
data generating process that allows interventions and changes can contribute towards understanding

179
00:17:17,040 --> 00:17:23,040
and resolving some key limitations like robustness and current machine learning methods. So,

180
00:17:23,040 --> 00:17:28,720
when I read that, I see we need structural knowledge about that amorphous thing, the data

181
00:17:28,720 --> 00:17:34,800
generating process, the problem domain. It's causal. So, I think there's a direct connection

182
00:17:34,800 --> 00:17:39,280
between what they're asking for and what we're seeking in terms of being able to fill that

183
00:17:39,280 --> 00:17:45,040
knowledge gap, fill that epistemic uncertainty gap. So, now I'm going to introduce this problem

184
00:17:45,040 --> 00:17:52,640
understanding chasm using this use case, this real-world example of racial bias discovered

185
00:17:53,280 --> 00:17:56,560
in a medical algorithm that's widely used throughout the U.S. healthcare system.

186
00:17:57,600 --> 00:18:03,360
This paper came out about four years ago. The purpose of this algorithm was to identify patients

187
00:18:03,360 --> 00:18:10,000
with the most complex healthcare needs so they could be given access to special programs early on

188
00:18:10,720 --> 00:18:14,320
so that you could ultimately reduce the overall cost in the healthcare system.

189
00:18:14,320 --> 00:18:17,840
And it's important to remember that their motivation was to reduce the overall cost

190
00:18:17,840 --> 00:18:23,520
in the healthcare system. Now, unfortunately, people not selected for the special programs by

191
00:18:23,520 --> 00:18:29,360
this algorithm suffer from nearly 50,000 more chronic diseases than the people who were selected

192
00:18:30,560 --> 00:18:34,800
and the people who were not selected were disproportionately Black Americans. So, this

193
00:18:34,800 --> 00:18:39,520
is why the algorithm was deemed to be racially biased. So, the question is like why did this

194
00:18:39,520 --> 00:18:45,600
happen? What led to this? And the researchers who discovered this had unprecedented access

195
00:18:45,600 --> 00:18:50,800
to every aspect of how this was built, which is not typical. The training data, the machine

196
00:18:50,800 --> 00:18:57,520
learning architecture, the training algorithm, you know, the performance metrics, the actual outputs,

197
00:18:58,480 --> 00:19:03,520
access to everybody who made decisions to build the system. And their conclusion was that the root

198
00:19:03,520 --> 00:19:09,120
cause of this failure was incomplete problem understanding. That's what it boiled down to.

199
00:19:10,480 --> 00:19:16,080
And remember, understanding the problem and formulating the problem for AI is at the very

200
00:19:16,080 --> 00:19:23,040
beginning of this process. But in reality, we really don't spend that much time studying and

201
00:19:23,040 --> 00:19:28,240
trying to improve that part of this process. We spend most of our time on the latter stages,

202
00:19:28,240 --> 00:19:35,280
the later life cycles. But remember, yeah, go ahead. Is that the algorithm that used the amount

203
00:19:35,280 --> 00:19:40,480
of money you spent on healthcare as a as an input? Yeah, exactly. And I'm going to I'm going to get

204
00:19:40,480 --> 00:19:47,600
into that right now. Yeah. So, the key thing I want to go back to is that human decision of choices

205
00:19:47,600 --> 00:19:52,960
drive this entire life cycle and really critical ones happening at the very beginning of the life

206
00:19:52,960 --> 00:19:59,440
cycle. But right now, those processes are pretty ad hoc and informal. And that's what's leading to

207
00:20:00,720 --> 00:20:06,800
my mind, this epistemic uncertainty. So there's a quote that I couldn't resist putting in. And

208
00:20:06,800 --> 00:20:12,960
once I read it, this is one of the papers that we reference and read when we're thinking about how

209
00:20:12,960 --> 00:20:17,680
humans make decisions and choices, because we said, Oh, this is a really critical part of this

210
00:20:17,680 --> 00:20:22,240
process. We should understand how humans make decisions. But this quote jumped out of me because

211
00:20:22,240 --> 00:20:27,600
it says choices do not merely identify one option among a set of possibilities. Choosing is an

212
00:20:27,600 --> 00:20:34,960
intervention, an action that changes the world. That's particularly true when decisions are being

213
00:20:34,960 --> 00:20:40,640
made within these AI product development life cycles, because those decisions impact directly

214
00:20:40,640 --> 00:20:45,760
what ends up getting produced by the machine learning models, which are themselves interventions

215
00:20:45,760 --> 00:20:50,240
on that societal context. So the criticality of this decision making and how we make choices

216
00:20:51,040 --> 00:20:56,800
is very high. And so we spent a lot of time looking at this and the research that we looked at

217
00:20:57,680 --> 00:21:03,920
pointed out that when we make decisions as humans, we leverage our causal inference capabilities.

218
00:21:04,880 --> 00:21:11,120
And those are shaped by strong top down prior dollars that we have in the form of what they

219
00:21:11,120 --> 00:21:15,360
call intuitive theories, as called them causal theories. And causal theories are these models

220
00:21:15,360 --> 00:21:22,560
we build up over time, as we navigate complex realities, face problems, solve problems,

221
00:21:22,560 --> 00:21:27,520
we form theories about why they're happening. And that informs our goals and our strategies to solve.

222
00:21:29,120 --> 00:21:34,640
So let's look at what role causal theories played in the failure of this algorithm.

223
00:21:35,440 --> 00:21:40,720
And remember that the problem was predict which patients will have the most complex healthcare

224
00:21:40,720 --> 00:21:45,840
needs, because we're going to give them access to these special programs. And the causal theory

225
00:21:45,840 --> 00:21:51,520
that they leveraged wasn't explicit at the time, but the one they leveraged was that more complex

226
00:21:51,520 --> 00:21:57,680
healthcare needs is going to lead to an increased spending on healthcare. And so they chose that

227
00:21:57,680 --> 00:22:02,720
as the target variable or their predictor, or the proxy for complex healthcare needs.

228
00:22:03,360 --> 00:22:06,800
They said if we can predict who's going to spend more money on healthcare in the future,

229
00:22:06,800 --> 00:22:09,840
we'll be able to predict who's going to have the most complex healthcare needs.

230
00:22:09,840 --> 00:22:14,240
This is where the algorithm failed. And the reason it failed is because this

231
00:22:14,240 --> 00:22:20,320
causal theory is woefully incomplete, right? It leaves out critical factors that impact

232
00:22:20,320 --> 00:22:26,800
how much black Americans spend on healthcare, even when they have more complex healthcare needs.

233
00:22:26,800 --> 00:22:31,680
These are factors like under diagnosis due to bias, lack of trust in the healthcare system,

234
00:22:31,680 --> 00:22:35,760
wealth and income disparities, and lack of access to affordable healthcare.

235
00:22:36,560 --> 00:22:40,560
Now take it, all those factors actually decrease how much black Americans spend on

236
00:22:40,560 --> 00:22:44,880
healthcare, even when they have more complex healthcare need. And then taken together,

237
00:22:44,880 --> 00:22:51,040
those factors represent a subset of the structural inequities that exist in the US healthcare system.

238
00:22:52,400 --> 00:22:57,280
And those structures were revealed by COVID-19, right? It showed all those structures.

239
00:22:57,920 --> 00:23:02,880
But those structures actually increase how much complex healthcare need there is in that community

240
00:23:02,880 --> 00:23:06,400
while simultaneously decreasing how much they spend on healthcare.

241
00:23:06,400 --> 00:23:13,280
So the causal theory wasn't even close to having the reality of the problem domain that it was

242
00:23:13,280 --> 00:23:21,200
trying to solve. So this is what we call the problem understanding chasm. This difference in

243
00:23:21,200 --> 00:23:28,400
understanding of the key problem from these two different perspectives in this chasm between them.

244
00:23:28,640 --> 00:23:30,560
I'm talking more about what contributes to that.

245
00:23:33,040 --> 00:23:36,880
This lack of knowledge on the right-hand side led directly to that

246
00:23:36,880 --> 00:23:42,320
harmful intervention, that lack of understanding of that additional societal context.

247
00:23:44,720 --> 00:23:47,680
Now one of the key root causes of this chasm is

248
00:23:49,040 --> 00:23:56,000
divergences between communities that are trying to intervene and how proximate they are to the

249
00:23:56,000 --> 00:24:03,520
problems and then their ways of knowing and explaining problems. So on the civil society side,

250
00:24:03,520 --> 00:24:08,320
these are generalizations, but folks trying to solve those problems on the civil society side

251
00:24:08,320 --> 00:24:13,840
are typically very proximate to the problems. They're deeply entrenched in the problem domains.

252
00:24:13,840 --> 00:24:18,320
On the product side, we're less entrenched in the problem domains, we're less proximate to them.

253
00:24:20,800 --> 00:24:24,640
Civil society folks that have a high stake in like solving the societal problem itself.

254
00:24:26,160 --> 00:24:30,320
Related to the broader domain. On the product side, we're typically interested in the business

255
00:24:30,320 --> 00:24:37,920
problem related to that domain. And then finally, on the civil society side, since you're really

256
00:24:37,920 --> 00:24:44,480
proximate to the problem and the humans that are suffering from it tend to prioritize a qualitative

257
00:24:45,360 --> 00:24:52,560
human perspective on the problem factors. So I have a question just trying to blend it to

258
00:24:53,520 --> 00:24:56,240
things like the gravity project, which are trying to determine

259
00:24:57,600 --> 00:25:03,360
codes for social determinants and valves and related projects. Would that be maybe some mitigating

260
00:25:04,320 --> 00:25:10,160
factor where you've got the civil society data informing this decision, I think, rather than

261
00:25:10,800 --> 00:25:15,200
as an organization trying to decide for ourselves what we think is important?

262
00:25:15,200 --> 00:25:19,280
Yeah, that would be exactly right. The thing you want to do is get to the point where

263
00:25:20,000 --> 00:25:22,800
you have some cooperation and understanding the problem. Because you have two different

264
00:25:22,800 --> 00:25:28,640
perspectives. Now, the right hand perspective is not wrong. It's just a different lens in that problem

265
00:25:28,640 --> 00:25:33,280
domain. And solely using that lens, you're going to miss critical things. And the same on the other

266
00:25:33,280 --> 00:25:39,200
side as well. Even on the civil society side, we're trying to solve problems. We're using methods,

267
00:25:39,200 --> 00:25:45,760
sociology, etc. But you also have problems with uptake of those solutions. Statistical methods

268
00:25:45,760 --> 00:25:52,880
get used on that side as well. I have to mention as well. So the idea is how do we bridge this kind

269
00:25:52,880 --> 00:25:58,880
of chasm in terms of how different groups think about and understand problems and get to a shared

270
00:25:58,880 --> 00:26:02,640
understanding helps us get closer to the reality that we're trying to intervene on.

271
00:26:05,600 --> 00:26:10,080
And then I just wanted to show this one more time using that initial diagram, that problem

272
00:26:10,080 --> 00:26:17,440
understanding chasm, right? This gap between societal context and all that messiness and how

273
00:26:17,440 --> 00:26:23,840
we're thinking about the problem on the product development side. Now, just to go back to this,

274
00:26:24,560 --> 00:26:30,000
our goal is, the thing I'm focused on within Google is how do we reduce this epistemic uncertainty?

275
00:26:30,640 --> 00:26:35,840
And to reduce epistemic uncertainty, you need knowledge. But we have to figure out a way to

276
00:26:35,920 --> 00:26:42,720
have that knowledge across that chasm and be useful during these workflows. And so people are

277
00:26:42,720 --> 00:26:52,000
using all sorts of tools to make decisions. And we can't expect them to read to Andrew Self's paper

278
00:26:52,000 --> 00:26:55,520
and figure out what they're supposed to do, read the latest research. It's just not going to happen,

279
00:26:55,520 --> 00:27:02,800
right? So we have to find a way to get knowledge across that chasm and make it available to these

280
00:27:02,800 --> 00:27:09,040
decision makers during every step of that workflow. So this is what our hypothesis is,

281
00:27:09,040 --> 00:27:14,560
is that to bridge that problem understanding chasm in a responsible way, these decision makers need

282
00:27:14,560 --> 00:27:20,800
tools that put community validated structure to societal context knowledge about complex

283
00:27:20,800 --> 00:27:27,520
societal problems at their fingertips, especially during problem understanding at the very beginning,

284
00:27:27,520 --> 00:27:31,200
but also throughout the entire lifecycle, because you should also be leveraging that

285
00:27:31,200 --> 00:27:35,360
understanding when you're evaluating a product for whether or not it's performing well,

286
00:27:35,360 --> 00:27:39,600
like what does that mean, right? It has to be relative to the context in which it's going to

287
00:27:39,600 --> 00:27:49,360
operate it. But we have to remember a couple of key things. And one is that it's factual that

288
00:27:49,360 --> 00:27:54,400
historically marginalized communities are disproportionately negatively impacted when

289
00:27:54,400 --> 00:28:01,920
things go wrong with AI bias. Those are just the facts, right? And you can look at example after

290
00:28:01,920 --> 00:28:07,600
example. And then the other fact is that the lived experience expertise of those folks

291
00:28:08,640 --> 00:28:14,240
is usually excluded from that, from those problem understanding or problem formulation steps.

292
00:28:16,400 --> 00:28:23,440
And so if we are focusing on also addressing these issues, then we're not, we're not

293
00:28:23,440 --> 00:28:29,760
responsibly solving this problem. So we think there are two key ingredients for bridging this

294
00:28:29,760 --> 00:28:35,520
chasm. One is in order to transmit knowledge across that chasm, we think we need to have some sort of

295
00:28:35,520 --> 00:28:41,360
model or reference frame for structured societal context knowledge. Yes. Oh, sorry, I didn't see

296
00:28:41,360 --> 00:28:46,640
you. Yeah. The slide where you said your hypothesis, what do you mean by knowledge?

297
00:28:47,360 --> 00:28:55,600
So that's a good question. That's a good question. So what we need, what we mean by knowledge is

298
00:28:57,120 --> 00:29:10,080
awareness of factors that make up the circumstances that in which you're going to be

299
00:29:10,160 --> 00:29:17,440
deploying models and that originate the problem you're trying to solve. So knowledge is like a very,

300
00:29:17,440 --> 00:29:22,240
you know, that's a big, deep philosophical question that you asked me. But we just mean

301
00:29:22,240 --> 00:29:28,640
awareness of these, these factors that make up these circumstances. Does that answer your question?

302
00:29:28,640 --> 00:29:35,840
I'm sure. Yeah, I just couldn't tell if you meant like data or theories or societal facts or all of

303
00:29:36,400 --> 00:29:40,720
those. Yeah, I mean, as we look at it, you'll see it's kind of like a combination of that,

304
00:29:40,720 --> 00:29:45,120
right? So we're, so we've got lots of data, but we don't have a lot of context for the data

305
00:29:45,120 --> 00:29:52,320
to help us interpret it. So what we're looking for are those, those additional facts that help

306
00:29:52,320 --> 00:29:57,200
make up the circumstances that led to the data existing. And oftentimes those circumstances

307
00:29:57,200 --> 00:30:01,200
will tell you that you're missing data, like you think you have the right data to understand

308
00:30:01,200 --> 00:30:05,360
this problem, but you don't even have that data yet. And so it starts with some understanding of

309
00:30:05,360 --> 00:30:10,400
what these other factors are you should consider. Yeah, exactly. I see, I feel like the biggest

310
00:30:10,400 --> 00:30:16,800
thing is the processes that are generating the data. So you may have known some of Rahid Ghani's

311
00:30:16,800 --> 00:30:24,160
work in Carnegie Mellon. Now he's there where he runs this data science for social good program.

312
00:30:24,160 --> 00:30:30,480
He brings students in and students, for example, go with the cops. They go, they go with the ambulance

313
00:30:30,480 --> 00:30:35,120
driver. And like one of the examples he was saying was that this ambulance driver who already

314
00:30:35,200 --> 00:30:40,400
had two car shins, so he was on probation, he didn't take a call because he's like,

315
00:30:40,400 --> 00:30:46,080
I don't want to risk my job because if I go take this call, I may do something wrong and then

316
00:30:46,080 --> 00:30:50,720
they're going to fire me, right? Your data is not going to say that unless you're actually

317
00:30:50,720 --> 00:30:56,000
riding with the ambulance driver and seeing the ambulance driver park his car under the bridge

318
00:30:56,000 --> 00:31:03,280
to and not accept that call. You're not seeing that. And so exactly. And so it's very difficult to

319
00:31:03,280 --> 00:31:08,560
say, I have good enough societal context knowledge. That's exactly right. You have to

320
00:31:08,560 --> 00:31:15,360
understand these situations and circumstances of key actors and agents. And so we came up with a

321
00:31:15,360 --> 00:31:19,680
way to start to tackle how to model this kind of knowledge. And so I'm going to get to that in a

322
00:31:19,680 --> 00:31:27,200
few slides. So I think I was here. Yeah, so this goes right back to the question about knowledge,

323
00:31:27,200 --> 00:31:33,120
right? So you'll see what we mean by it in a few couple slides. But we think the other key

324
00:31:33,120 --> 00:31:39,520
aspect to address those facts I just laid out is that we need participatory, non-extractive

325
00:31:39,520 --> 00:31:44,480
methods. And by non-extractive, I mean, you know, there'll be a demand for this knowledge. We're

326
00:31:44,480 --> 00:31:49,120
going to make sure we're not just going into communities, gathering knowledge and then taking

327
00:31:49,120 --> 00:31:55,760
it away. Like there's got to be mutual benefit. So and then you also need capacity, right,

328
00:31:55,760 --> 00:32:00,480
for communities to participate directly and own their knowledge and be able to benefit from it.

329
00:32:03,120 --> 00:32:10,080
Yeah, Chris. So some of the things that the community can share are not the sorts of things

330
00:32:10,080 --> 00:32:16,400
that are going to become data on the other side, right? Like that example is not that you're going

331
00:32:16,400 --> 00:32:21,920
to start adding as a factor, you know, are people refusing to take calls because they are on probation.

332
00:32:24,880 --> 00:32:31,440
And so I guess part of the question here is like, which things that the community knows

333
00:32:31,680 --> 00:32:40,560
are translatable into something that then a more principled set of AI creators can use to make a

334
00:32:40,560 --> 00:32:47,840
more humane and fair algorithm? And which ones of them are reasons to be modest about the uses

335
00:32:47,840 --> 00:32:53,920
of AI and to say that, well, actually, what we want is AI products that say, well, here's what

336
00:32:53,920 --> 00:32:59,920
we suggest based on what little this algorithm knows. But there's only a suggestion and you

337
00:33:00,880 --> 00:33:06,000
decision, you human decision maker being advised on this probably know a lot more about this case

338
00:33:06,000 --> 00:33:11,600
than the algorithm does. So the algorithm is presenting its results and assign kind of with

339
00:33:11,600 --> 00:33:19,360
some humility. And yeah, I mean, that's, that's interesting too. The if as well as the how

340
00:33:20,320 --> 00:33:24,720
of representing this knowledge. Yeah, yeah, I mean, the thing I'll highlight there is that,

341
00:33:25,520 --> 00:33:29,520
you know, one of the things you want to happen is you want the models to be aware of their

342
00:33:29,520 --> 00:33:35,280
epistemic uncertainty. You know, I don't I don't know a lot about this. Right. And so even if I

343
00:33:35,280 --> 00:33:41,680
don't end up incorporating, you know, a particular factor directly in the data set or the decision

344
00:33:41,680 --> 00:33:46,880
making variables, I need to have a better understanding of like, I don't know a lot about this space,

345
00:33:46,880 --> 00:33:51,200
maybe we shouldn't even be using AI for this. Right. Right. That's why you want to really

346
00:33:52,160 --> 00:33:57,280
dig deeply into this into the problem understanding and formulation phase before you start building

347
00:33:58,000 --> 00:34:00,960
machine learning systems, right, for a particular problem.

348
00:34:03,280 --> 00:34:06,880
Does that answer your question? It's a longer discussion. I think it's a great

349
00:34:06,880 --> 00:34:12,000
it's a great set of questions like, you know, something like, Dr, here's our recommendation,

350
00:34:12,000 --> 00:34:16,400
but please keep in mind, there are lots of good reasons why this patient might be an exception

351
00:34:16,480 --> 00:34:21,360
to this recommendation. And I I don't know how to convey this partly gets into human

352
00:34:21,360 --> 00:34:27,200
computer interface questions and psychology questions. How do we convey uncertainty and

353
00:34:27,200 --> 00:34:36,240
humility to the people being advised by these algorithms? Yeah. And, you know, yeah. Yeah.

354
00:34:36,240 --> 00:34:40,000
And I think it's just it's it's not the same part as the explainability problem that everybody's

355
00:34:40,000 --> 00:34:46,560
trying to solve, explaining the results of the output. We also be need to be able to explain

356
00:34:46,560 --> 00:34:50,960
like, what we don't know, right, and the risks associated with taking the advice.

357
00:34:52,160 --> 00:34:58,480
Sorry, if I may, this also goes to the incentive. So as an AI tool developer, right, you don't want

358
00:34:58,480 --> 00:35:04,560
to develop a tool that says, I don't know, right. And so first, so for example, Karina Cortez,

359
00:35:04,560 --> 00:35:09,040
Google Research, right, she had a couple of really nice papers on learning with abstention,

360
00:35:09,760 --> 00:35:14,720
where the machine learning system says, I don't know, right. But your software engineers,

361
00:35:14,720 --> 00:35:20,480
you're, they wouldn't want to give a tool that would say, I don't know, right. And so that's

362
00:35:20,480 --> 00:35:30,000
changing the incentives of like, I don't know. Yep. I agree 100% of the agents that are making

363
00:35:30,000 --> 00:35:34,640
decisions is like a key part of that overall societal context. But you'll see right now,

364
00:35:34,640 --> 00:35:40,160
even like, some things happen with the generative AI, and like hallucinations happening,

365
00:35:41,520 --> 00:35:48,800
products are saying, yeah, bullshitting, products are, you see product services saying,

366
00:35:48,800 --> 00:35:54,000
we don't know, don't trust us, we're not sure. Oh, that's great, then. Yeah. Yeah. I mean,

367
00:35:54,000 --> 00:35:59,520
because, and that's being forced by, by this, this becoming an intervention at society that's

368
00:35:59,520 --> 00:36:07,120
getting a lot of kind of feedback. Right. So, you know, so we've got this situation,

369
00:36:08,240 --> 00:36:14,880
we've got this messiness that can't be transmitted over that, that chasm, if, and we're saying,

370
00:36:14,880 --> 00:36:18,560
hey, we need to organize information and knowledge about societal context in some way, this big,

371
00:36:18,560 --> 00:36:23,520
scary thing. So we need some kind of model of it. So it's kind of ironic, right, you're trying to,

372
00:36:23,520 --> 00:36:27,040
you're trying to reduce people abstracting away, but in order to do that, you need some kind of

373
00:36:27,120 --> 00:36:31,680
abstraction, right, that people can use to kind of cope with this knowledge in some way.

374
00:36:33,200 --> 00:36:40,240
And so we're, I was really inspired by some work by a sociologist named Dr. Walter F. Buckley,

375
00:36:40,960 --> 00:36:47,120
who back in the late 60s made the connection between trying to understand society and

376
00:36:47,120 --> 00:36:54,400
sociocultural systems, and between that and systems theory. And so he was like,

377
00:36:54,400 --> 00:36:58,480
we should think of society as a complex adaptive system. When I talk about societal context,

378
00:36:58,480 --> 00:37:04,080
let's face it, we're talking about like, you know, just a, you know, some conception of society as

379
00:37:04,080 --> 00:37:09,120
a whole, how do we think about that? And so our work was based on using complex adaptive systems

380
00:37:09,120 --> 00:37:13,840
theory as the basis. If you look at the characteristics of a complex adaptive systems,

381
00:37:14,480 --> 00:37:19,360
a complex adaptive system, you'll, you'll see some of these characteristics that are in common

382
00:37:19,360 --> 00:37:25,360
with how we were describing societal context. It's complex, adaptive and dynamic. There's this

383
00:37:25,360 --> 00:37:30,720
nonlinearity. I talked about these feedback loops of the ecosystem that we're dealing with.

384
00:37:31,520 --> 00:37:36,720
We talk about things being historical, time delays goes to that complex adaptive systems have

385
00:37:36,720 --> 00:37:44,400
history as well. So we looked at the canon of work associated with this. John Holland is like

386
00:37:44,400 --> 00:37:50,640
a giant in this space. Scott Page, who I think doesn't work with SFI is also somebody I love to

387
00:37:50,640 --> 00:37:56,800
read. So we synthesize some work from these folks to come up with, you know, an attempt, right,

388
00:37:56,800 --> 00:38:01,360
to start to wrap our, our heads and arms around this complex thing. So we came up with a reference

389
00:38:01,360 --> 00:38:09,760
frame or model has three key elements, agents, precepts and artifacts, agents. When we thought

390
00:38:09,760 --> 00:38:13,360
about agents, these could be human or nonhuman, but I'm going to talk about this through the human

391
00:38:13,360 --> 00:38:18,400
lens. So through human lens, these are eight, these are individuals and institutions.

392
00:38:19,680 --> 00:38:24,000
And the ideas that all agents have and essentially are their precepts,

393
00:38:25,920 --> 00:38:31,120
which are our conception, our beliefs, our values, our stereotypes,

394
00:38:32,080 --> 00:38:34,320
our, how we perceive needs and problems.

395
00:38:36,560 --> 00:38:42,400
The causal theories that we talked about, these are all these rules that constrain and drive the

396
00:38:42,400 --> 00:38:48,240
behavior of agents. And these can go all the way down to, you know, the fundamental rules for

397
00:38:48,240 --> 00:38:52,880
building, building a human, right? But at the societal level, we're talking about these kind

398
00:38:52,880 --> 00:38:59,040
of beliefs and values and goals that constrain and drive the behavior of agents. And when agents

399
00:38:59,040 --> 00:39:06,800
behave, their behavior manifests itself in the real world as artifacts. Indeed, they're just

400
00:39:06,880 --> 00:39:12,800
all the things agents produce and create, language, data, laws, institutions, right?

401
00:39:12,800 --> 00:39:17,200
There's some multiple, multiple inheritance. Some, you know, agents can produce artifacts or

402
00:39:17,200 --> 00:39:24,720
also agents, for example, policies, AI models or artifacts were creating products and we're

403
00:39:24,720 --> 00:39:30,640
producing problems as well. And then there's this other important relationship between precepts and

404
00:39:30,640 --> 00:39:36,640
artifacts. This is kind of well-known in the way people think about HCI, for example, is that

405
00:39:36,640 --> 00:39:41,840
the things we produce are a reflection of our values, right? So the artifacts are in some way

406
00:39:41,840 --> 00:39:45,440
a reflection of the precepts that led to them existing in the first place. And of course,

407
00:39:45,440 --> 00:39:50,640
those precepts are influenced by the artifacts that the agent is surrounded by in the world,

408
00:39:50,640 --> 00:39:56,240
right? I read a book, my precepts get updated, then I do some more acting in the world, gets

409
00:39:56,240 --> 00:40:03,440
reflected in more artifacts. But we also wanted to organize this and represent it as a taxonomic

410
00:40:03,440 --> 00:40:09,360
model because ultimately we're envisioning, creating knowledge graphs and databases so that

411
00:40:09,360 --> 00:40:17,600
we can transmit this data across that chasm and make it available to tools and workflows, right?

412
00:40:17,600 --> 00:40:20,960
So somehow or another, we have to have a structured representation of this knowledge.

413
00:40:22,720 --> 00:40:27,920
Now, when we develop this initial model, we hypothesize that these perceived problems and

414
00:40:27,920 --> 00:40:31,760
causal theories were going to be really important if we're thinking about product development.

415
00:40:32,720 --> 00:40:37,360
And when we actually did this work, it was before that Ziad Obermeyer paper came out

416
00:40:37,360 --> 00:40:43,040
about racial bias and medical algorithms. So we were kind of excited that, yeah,

417
00:40:43,040 --> 00:40:47,760
we were on the right track, we think, because he pointed out, they pointed out how critical

418
00:40:47,760 --> 00:40:53,280
these causal theories were and how they directly led to harmful bias in an important system.

419
00:40:55,920 --> 00:40:59,200
And so now the left-hand side looks a little bit more

420
00:40:59,920 --> 00:41:03,920
organized, maybe we can think about societal context in a little bit more of an organized way.

421
00:41:05,440 --> 00:41:09,760
But the key question is, how do we responsibly acquire these causal theories

422
00:41:11,520 --> 00:41:18,800
and make them into structured knowledge? So this is where this practice called community-based

423
00:41:18,800 --> 00:41:25,040
system dynamics came into play. I have some friends in the system dynamics community.

424
00:41:26,000 --> 00:41:30,560
I know the person who wrote the book on community-based system dynamics, and this seemed like

425
00:41:30,560 --> 00:41:36,720
it could be a good way to produce societal context knowledge through the act of prototyping

426
00:41:38,000 --> 00:41:42,000
problems, which are like this core part of societal context, but prototyping them

427
00:41:42,000 --> 00:41:47,200
as systems, as complex adaptive systems. And community-based system dynamics

428
00:41:47,200 --> 00:41:56,320
is a participatory practice for prototyping problems that centers communities and empowering

429
00:41:56,320 --> 00:42:01,920
communities to fully participate and take ownership of their own models of the problems

430
00:42:01,920 --> 00:42:10,560
that they're faced with. But it's grounded in the feedback perspective of system dynamics,

431
00:42:10,560 --> 00:42:18,640
which is invented almost over 60 years ago at MBT. Now SD itself, system that dynamic itself,

432
00:42:18,640 --> 00:42:24,880
leverages visual tools and simulation to support group model building, building prototyping

433
00:42:24,880 --> 00:42:31,840
problems together, and developing collaborative, developing causal theories. And the other words

434
00:42:31,840 --> 00:42:36,960
we use for that are problem models or problem prototypes. And it allows people, health people

435
00:42:36,960 --> 00:42:42,640
do this both qualitatively and quantitatively, and it has a nice bridge between the qualitative

436
00:42:42,640 --> 00:42:47,360
and the quantitative, which we think is important for being able to do this kind of work. And then

437
00:42:47,360 --> 00:42:52,000
the last thing to emphasize is that these prototypes or simulations of these underlying

438
00:42:52,000 --> 00:42:57,360
structures can be used to test interventions on a problem before you actually try to do it in the

439
00:42:57,360 --> 00:43:06,640
real world. And so I'm going to walk through at a high level the basic visual notation that

440
00:43:06,720 --> 00:43:13,200
utilized to prototype problems and system dynamics. One representation is called a causal loop

441
00:43:13,200 --> 00:43:17,920
representation. You might have heard of causal loop diagrams or causal maps, causal maps. The

442
00:43:17,920 --> 00:43:24,000
other is a stock and flow representation. Causal loop diagrams are fully qualitative. Stock and

443
00:43:24,000 --> 00:43:29,760
flow diagrams are qualitative and quantitative. They're provide that bridge into the quantitative

444
00:43:31,040 --> 00:43:34,560
world. So I'm going to walk through this example with a simple loan scenario model.

445
00:43:34,640 --> 00:43:38,800
So the sort of factors you have when you're talking about loans are average credit score,

446
00:43:39,440 --> 00:43:45,360
loans received, borrowers, who's defaulting on loans, who's making their payments.

447
00:43:46,720 --> 00:43:52,480
So you have all those factors specified in language and words. And then you have an arrow

448
00:43:52,480 --> 00:44:00,160
between factors with either plus sign or minus sign on top of it that illustrates the hypothesized

449
00:44:00,160 --> 00:44:06,640
causal relationship between two factors. And the plus sign just illustrates the direction

450
00:44:06,640 --> 00:44:12,000
of the impact that one factor is going to have on another. So to give an example, the plus sign

451
00:44:12,640 --> 00:44:17,120
at the end of the average credit score arrow means that as the average credit score goes up in this

452
00:44:17,120 --> 00:44:23,120
system, the number of loans received is also going to go up. But it also means as the average credit

453
00:44:23,120 --> 00:44:28,160
score goes down, the number of loans is going to go down. So it just means that the impact is in the

454
00:44:28,160 --> 00:44:33,360
same direction. The minus sign means the opposite. So the relationship between loan defaults and

455
00:44:33,360 --> 00:44:37,760
average credit score has the minus sign on the arrow. That means as the number of loan defaults

456
00:44:37,760 --> 00:44:42,160
goes up, the average credit score is going to go down. It's going to go in the opposite direction

457
00:44:42,160 --> 00:44:47,040
as opposed to the same direction. If the number of loan defaults goes down, the average credit score

458
00:44:47,040 --> 00:44:53,280
is going to go up in the system. You can also illustrate time delays, right? Because when one

459
00:44:53,280 --> 00:45:00,160
factor impacts another, it may not happen right away. So in this system, we're saying as number

460
00:45:00,160 --> 00:45:05,040
payments go up in a system, the average credit score in the system should go up. But that's not

461
00:45:05,040 --> 00:45:08,560
going to happen right away. It'll take a while for the credit score, average credit score in the

462
00:45:08,560 --> 00:45:14,720
system to be impacted. You can illustrate that that way. And then you have feedback loops, right?

463
00:45:14,720 --> 00:45:19,520
This really important aspect of complex adaptive systems. There's two types of feedback loops.

464
00:45:19,520 --> 00:45:26,240
One is a reinforcing feedback loop. These are basically indicators of exponential growth,

465
00:45:27,200 --> 00:45:33,680
either vicious cycles or virtuous cycles. And so you can identify parts of the system that

466
00:45:33,680 --> 00:45:39,520
could be driving exponential growth. In this case, the combination of borrowers increasing

467
00:45:39,520 --> 00:45:44,720
and payments being made and average credit score going up, that could drive an exponential increase

468
00:45:44,720 --> 00:45:51,120
in the number of loans in the system. So that's what the reinforcing loop represents. But the

469
00:45:51,120 --> 00:45:56,000
balancing loop is for balancing out this exponential growth. So that's illustrated by the loop that

470
00:45:56,000 --> 00:46:02,000
includes loan defaults. Because that's the one place that even as borrowers are increasing,

471
00:46:02,000 --> 00:46:06,240
yeah, that leads to more payments, but it also leads to more defaults, which leads to a decrease

472
00:46:06,240 --> 00:46:11,600
in average credit score, which taps down the growth in loans being given out. Can I ask? Yeah.

473
00:46:11,600 --> 00:46:15,600
Because you're saying average credit score, this is meant to model what's happening in an

474
00:46:15,600 --> 00:46:20,880
entire population. Yeah. Because of course, these feedbacks also take place in individuals' lives.

475
00:46:20,880 --> 00:46:28,800
And so, yeah. Yeah. Yeah. I mean, system dynamics tends to take a macro view, right? It's looking

476
00:46:28,800 --> 00:46:34,480
at populations as a whole. And oftentimes people use a combination of system dynamics for the macro

477
00:46:34,480 --> 00:46:40,160
view and then agent-based modeling to have more specific kind of ways to model like individual

478
00:46:41,600 --> 00:46:46,320
behavior. But this model might have different outcomes within different demographic groups.

479
00:46:46,320 --> 00:46:51,280
Oh, yeah. Totally. Totally. Right. And again, this is just a way to kind of start to hypothesize

480
00:46:51,280 --> 00:46:56,720
about these factors and you can get more detailed or less detailed. Yeah. You care if the average

481
00:46:56,720 --> 00:47:04,320
credit score is a black box, but you cannot see it. Yeah. I mean, I see what's happening. Do I?

482
00:47:04,320 --> 00:47:10,800
Yeah. Yeah. Yeah. I care. So we put this up just to kind of illustrate like the elements of it,

483
00:47:10,800 --> 00:47:16,080
but as you do this work, you'll see that what happens more and more, you drill into these

484
00:47:16,080 --> 00:47:21,680
black boxes and you try to expose the mechanisms under them. Sometimes you're not able to because

485
00:47:21,680 --> 00:47:27,040
there's someone in control of that. Yeah. Because I was working with Citibank in terms of who they

486
00:47:27,040 --> 00:47:32,480
should give loans to and fair loans, et cetera, et cetera. Yeah. And they're getting some information

487
00:47:32,480 --> 00:47:37,920
from third-party folks, which they don't know. They do not know how the FICO score is being

488
00:47:37,920 --> 00:47:43,280
computed. Yeah. And so it is a black box and they're going to use it. Yeah. It's a model.

489
00:47:43,280 --> 00:47:47,520
It's a model. Right. That is a model that is being used to kind of make decisions.

490
00:47:48,480 --> 00:47:52,720
Right. Yeah. Right. So then the question would be like, how do you actually quantify that in your

491
00:47:52,720 --> 00:48:00,400
epistemic uncertainty? How much weight do you give to it? Yeah. I mean, that's a question. Right.

492
00:48:00,400 --> 00:48:08,720
You have to, and this is something that the modelers contend with and negotiate. And this is why

493
00:48:08,720 --> 00:48:15,600
you want to try to include in this modeling the people who are building those who are building

494
00:48:15,600 --> 00:48:20,000
these credit score models. It's very, you know, it's, depending on what problem domain you're

495
00:48:20,000 --> 00:48:24,560
working at, it's very hard to have participation from everybody that you want to have participation

496
00:48:24,560 --> 00:48:34,960
from. Yeah. And then the stock and flow representation has all those same features.

497
00:48:35,760 --> 00:48:40,960
You just represent those factors with quote, unquote, stocks and flows. What important aspect

498
00:48:40,960 --> 00:48:45,760
of the stocks is it allows you to take into consideration accumulations in a system,

499
00:48:45,760 --> 00:48:51,680
which really allows you to be able to consider the history of the system, the initial conditions

500
00:48:51,680 --> 00:48:56,080
and how things grow and decrease over time and really get into that detail in terms of

501
00:48:56,080 --> 00:49:00,800
relationship between factors. And this is where you start to get into the quantification,

502
00:49:00,800 --> 00:49:05,600
because the way you represent those relationships is with differential equations, for example.

503
00:49:05,600 --> 00:49:09,760
And that allows you to get to the point where you could start to simulate your hypothesis.

504
00:49:09,760 --> 00:49:14,320
And that's, and system dynamics is not about, it's really not like, I'm going to solve this

505
00:49:14,320 --> 00:49:20,240
problem. I'm trying to learn the problem. I'm trying to learn the problem system and increase my

506
00:49:20,240 --> 00:49:25,360
confidence that I understand it well enough to start to intervene on it in the real world.

507
00:49:28,160 --> 00:49:33,840
I just threw this in just to give an indication of what you have put this together. You can start

508
00:49:33,840 --> 00:49:40,800
to kind of look at how these impacts show up in a simulation. This is made with something called

509
00:49:40,800 --> 00:49:47,760
loopy. Anybody can play around with this. This is like a really oversimplified way to model these

510
00:49:48,080 --> 00:49:53,040
sorts of systems, but at least it lets people to start to engage with it. There's really expensive,

511
00:49:53,040 --> 00:49:57,520
hard to use software as well. And one of the issues is that we have to improve the accessibility of

512
00:49:57,520 --> 00:50:02,320
these sorts of tools if we really want to build capability and capacity in communities. So now

513
00:50:02,320 --> 00:50:07,920
we've got this kind of approach that we're like, ah, this might be a good way for us to start to

514
00:50:07,920 --> 00:50:16,240
tackle producing societal context knowledge from places where we're embedded and proximate to

515
00:50:16,240 --> 00:50:21,520
problems. But now we have to have some actual capability in the world to use these techniques

516
00:50:21,520 --> 00:50:26,720
if we're going to have any chance of producing useful knowledge. So now I'm going to talk about

517
00:50:27,520 --> 00:50:30,800
some experience that we've had over the past few years of trying to build that kind of trust

518
00:50:30,800 --> 00:50:35,920
and capability. I'm just just putting up this reminder of the fact that, you know, the lived

519
00:50:35,920 --> 00:50:39,920
experience expertise of historical marginalized groups is usually not involved in this process.

520
00:50:39,920 --> 00:50:42,640
So when I started this back in 2017, I intentionally

521
00:50:43,920 --> 00:50:48,640
wanted to work with folks from historically marginalized communities who are trying to

522
00:50:49,280 --> 00:50:52,400
leverage data and math and science to understand and solve problems.

523
00:50:53,760 --> 00:51:00,000
And there's a group called Data for Black Lives who does exactly that. It's a collection of activists

524
00:51:00,000 --> 00:51:05,120
and organizers and mathematicians and scientists who are trying to leverage data science for good

525
00:51:05,120 --> 00:51:09,760
and also try to make sure data sciences does not end up unintentionally making things worse

526
00:51:09,760 --> 00:51:16,000
when we're applying it into high stakes domains. And so this started with me just going to the

527
00:51:16,000 --> 00:51:21,040
first conference, not knowing what to expect back in 2017. And this led to a two and a half year journey

528
00:51:21,600 --> 00:51:27,200
that resulted in a research paper and a causal theory that's starting to have an impact on

529
00:51:27,920 --> 00:51:32,800
how Google evaluates and think about health diagnostic algorithms. So I just went to the

530
00:51:32,800 --> 00:51:38,400
first conference with no expectations, not asking anybody for anything, just trying to figure out

531
00:51:38,400 --> 00:51:41,840
what people were trying to do, what problems they were working on, what problems they were trying to

532
00:51:41,840 --> 00:51:47,520
solve. And that's interesting people. Some of them famous now, you probably recognize.

533
00:51:50,480 --> 00:51:55,280
And what I discovered there was that there was this need for people to be able to see

534
00:51:55,280 --> 00:52:01,440
how the problems they were separately working on were connected. And now everyone was talking about,

535
00:52:01,440 --> 00:52:04,800
you know, we have to understand the system, the system, but there was no real

536
00:52:05,760 --> 00:52:10,000
concrete way to actually start to tackle that. So I said, Hey, this could be a good place to

537
00:52:10,000 --> 00:52:16,160
introduce system dynamics and systems thinking. I talked to the founder. Yes, he mill in Europe.

538
00:52:16,160 --> 00:52:21,040
She didn't know me from Adam. And so I had to convince her that this could be useful. So I

539
00:52:21,040 --> 00:52:25,200
introduced her to it with a mini workshop. And then we agreed to set a goal of actually

540
00:52:25,760 --> 00:52:32,080
delivering a workshop to that's larger community at the next conference that they were going to

541
00:52:32,080 --> 00:52:37,280
have, which was about a year away. But we had one principle, we said, the people who are going to

542
00:52:37,280 --> 00:52:43,200
teach this workshop to this community, they have to have similar lived experiences to the people

543
00:52:43,200 --> 00:52:51,920
that are going to be receiving this knowledge. And not a lot of diversity in the world of system

544
00:52:51,920 --> 00:52:56,480
dynamics when we started this has changed over time. So we said, we have to, we have to teach

545
00:52:56,480 --> 00:53:01,040
the teachers, we have to create some teachers that can deliver this workshop. And so we did a

546
00:53:01,040 --> 00:53:05,040
learning lab that included people from that community, as well as people from Google to

547
00:53:05,040 --> 00:53:08,480
teach them these techniques so that they could teach the bigger workshop.

548
00:53:10,880 --> 00:53:14,800
We did that over a couple of months, they were able to teach this workshop and these techniques

549
00:53:15,360 --> 00:53:19,680
to a larger group of about 75 people was received very well.

550
00:53:20,640 --> 00:53:25,520
And then we had this capability, and we're like, okay, and a little bit of capacity. What should

551
00:53:25,520 --> 00:53:31,200
we do with it to try to further this journey? And so we decided to do a problem prototyping

552
00:53:31,200 --> 00:53:38,400
experiment around a problem domain that Google cares about and data for Black Lives Care is about.

553
00:53:38,400 --> 00:53:44,080
And that was health diagnosis. Data for Black Lives Care about it because under diagnosis can

554
00:53:44,080 --> 00:53:48,880
lead to health disparities. We talked about that earlier. Google cares about it because,

555
00:53:48,880 --> 00:53:53,200
you know, we're working on building health care diagnostic algorithms. We want them to be

556
00:53:53,200 --> 00:53:57,680
accurate and work well. But there's a lack of training data that can lead to inaccuracy.

557
00:53:58,480 --> 00:54:07,600
And so we thought this would be a good area to jointly prototype a problem that relates

558
00:54:07,600 --> 00:54:15,200
algorithms and lack of data and health care disparities as a way to start to bridge that

559
00:54:15,200 --> 00:54:20,960
chasm. And so we brought people together. We got about nine people together from that initial

560
00:54:20,960 --> 00:54:26,720
workshop. They developed one of these college loop diagrams for that problem domain. I'm not going

561
00:54:26,720 --> 00:54:30,480
to go through this in detail, but you can look at the paper, look at all the data.

562
00:54:32,240 --> 00:54:36,320
But one key thing is that it highlighted trust in medical care as a key factor

563
00:54:36,320 --> 00:54:40,800
that drove multiple balancing and reinforcing feedback loops in this system.

564
00:54:40,800 --> 00:54:48,560
They also created a stock and flow model for this. Trust shows up as this kind of

565
00:54:49,200 --> 00:54:56,240
really big substructure that you can double click in. And the thing about doing this work is that

566
00:54:56,240 --> 00:55:03,120
it allows you to think about these soft variables and include them and approach

567
00:55:04,800 --> 00:55:08,720
bridging that qualitative quantitative gap, quantifying them in some way,

568
00:55:08,720 --> 00:55:14,560
but informed by people that approximate to the problem, but also informed by previous research

569
00:55:14,560 --> 00:55:20,080
that's happened. And so they developed a really detailed trust substructure, which I think actually

570
00:55:20,080 --> 00:55:28,960
should be in a whole lot of models. And that allowed them to develop a simulation and actually

571
00:55:29,520 --> 00:55:32,320
test some of the interventions that they had in mind that might make things

572
00:55:32,960 --> 00:55:35,680
better to include the reference model that we were solving for.

573
00:55:38,240 --> 00:55:44,320
Now, the key, my key takeaway from this was that, hey, you can do community-based research and

574
00:55:44,320 --> 00:55:52,000
actually produce useful research that comes from a partnership with community and people that are

575
00:55:52,000 --> 00:55:54,960
approximate to problems and better than problems. So this paper actually got

576
00:55:55,680 --> 00:56:00,880
accepted, presented at the System Dynamics Conference, what an honorable mission award.

577
00:56:03,280 --> 00:56:09,680
But the key thing I learned is that you can go after if we want to build capability in any place,

578
00:56:09,680 --> 00:56:15,360
but you cannot do that without building trust at the same time. And there's no way to rush this,

579
00:56:16,080 --> 00:56:22,880
right? It takes determination and patience and investment, but that's going to be one of the

580
00:56:22,880 --> 00:56:27,040
key ingredients if we are going to have any sort of chance to bridge this chasm, in my opinion.

581
00:56:27,360 --> 00:56:34,720
So I'm going to conclude with, first, that key takeaway that I started with. Interventions that

582
00:56:34,720 --> 00:56:42,160
ignore abstract way societal context can lead to unintended and unnecessary harm. I think they're

583
00:56:42,160 --> 00:56:50,080
likely to. And then those calls to action. If we're going to drive more attention and research in

584
00:56:50,080 --> 00:56:57,200
this space, we need folks like you all to spread the word. And then we also are really calling

585
00:56:57,200 --> 00:57:03,680
for people to embrace this concept of prototyping problems before you prototype solutions, prototype

586
00:57:03,680 --> 00:57:10,880
problems as systems, embrace that complexity before intervening with data science or AI,

587
00:57:11,440 --> 00:57:17,280
really with anything. And we think this is really one of the only ways to really proactively mitigate

588
00:57:17,920 --> 00:57:23,360
bias in these high stakes domains. And then finally, invest some of your time and energy. And if you

589
00:57:23,360 --> 00:57:29,840
have dollars, dollars in building problem prototyping, trust and capability, like in historically

590
00:57:29,840 --> 00:57:34,560
marginalized communities. With that, I'll close. Thank you, everybody.

591
00:57:39,200 --> 00:57:41,520
So we are missing one time, but there are a few questions.

592
00:57:41,760 --> 00:57:46,160
Yeah. Did you just work at Google? I'm sorry. Did you used to work at Google? No, no.

593
00:57:48,560 --> 00:57:55,360
So I'm wondering, great talk. Thank you. So I'm like studying in public health. And so one thing

594
00:57:55,360 --> 00:58:01,120
we're finding I'm thinking is that the problems start outside the hospital. So those precepts

595
00:58:01,120 --> 00:58:06,640
and the clinicians come from outside the hospital and it becomes a problem once we get in. So how

596
00:58:06,640 --> 00:58:12,240
much of the modeling should we think about outside hospitals in terms of like social and public

597
00:58:12,240 --> 00:58:19,600
investments, housing investment, basically treating that environment in the society and having that

598
00:58:19,600 --> 00:58:25,280
gradient sort of diffuse into the health care industry somehow. I think trying to solve the

599
00:58:25,280 --> 00:58:31,360
health care problem is a hard one. And then it seems like you have to go outside a little bit.

600
00:58:32,320 --> 00:58:37,360
Like we look at your way or somewhere like they invest in their public more than any of us probably

601
00:58:37,360 --> 00:58:41,680
in health care. And we do the opposite. Yeah, we're having this problem. I agree. I agree 100%.

602
00:58:41,680 --> 00:58:47,520
Like, I don't know. I may not have been clear in this talk, but what I think of things I emphasize

603
00:58:47,520 --> 00:58:52,720
in Google is that we have to start with trying to understand the problems that people care about

604
00:58:52,720 --> 00:58:58,320
outside of Google, independent of like our business problems. If we really want to actually,

605
00:58:58,400 --> 00:59:03,520
I think this actually leads to better business as well, start outside. What are the people outside

606
00:59:03,520 --> 00:59:07,920
in society? What problems do they care about and why? And how do we get to the point where we

607
00:59:07,920 --> 00:59:12,640
understand those problems? And then you're going to find some intersection in that problem domain

608
00:59:15,120 --> 00:59:20,160
between the problems that society cares about and the problems that we may have some hammers and

609
00:59:20,160 --> 00:59:26,000
nails to try to solve technology. But you have to start outside of that in order to really start

610
00:59:26,000 --> 00:59:30,960
to get any kind of ground truth. Yeah, it's like, you need to understand what the problem is. Yeah,

611
00:59:32,000 --> 00:59:35,680
I agree. That's why that's why I'm trying to talk about this problem understanding chasm because

612
00:59:35,680 --> 00:59:41,040
of this lack of understanding of the problem, particularly on the product development side.

613
00:59:41,040 --> 00:59:45,120
But but there's multiple ways to understand the problem. I think there are ways to leverage

614
00:59:45,120 --> 00:59:51,040
technology in good ways. But unless we have a deeper understanding that's shared between

615
00:59:51,040 --> 00:59:55,600
these two perspectives, we're not we're not going to be able to do it in a responsible way.

616
00:59:59,360 --> 01:00:04,080
I guess one thing you didn't mention that I expected you mentioning would be accountability,

617
01:00:04,080 --> 01:00:12,080
right? So for example, perhaps the reason some of the AI developers now are okay with the system

618
01:00:12,080 --> 01:00:19,600
saying no is Google search was always retrieving. Now with gen AI is generating stuff. So if you're

619
01:00:19,680 --> 01:00:25,440
generating something that causes me harm, then there's accountability to be had, right? And

620
01:00:25,440 --> 01:00:29,920
the laws are still not there. We'll see, right? Like for example, in Europe, in certain European

621
01:00:29,920 --> 01:00:35,040
countries, they abandon these kinds of large language models because people harm themselves

622
01:00:35,040 --> 01:00:40,960
based on what was told to them, what was generated as opposed to retrieval. So what are some of your

623
01:00:41,600 --> 01:00:48,960
thoughts on accountability here? Who's accountable if the system if the algorithm causes harm to

624
01:00:48,960 --> 01:00:54,160
the community? Well, so you're trying to give me to answer one of these questions that are that's

625
01:00:55,040 --> 01:01:00,560
you can pass it. But it's it's it isn't it you we are all part of the community, right? So we

626
01:01:00,560 --> 01:01:06,720
it's yes, I'm a computer scientist, but I'm also part of the community. Yeah. Yeah, I mean, I think

627
01:01:06,720 --> 01:01:15,360
so. So I think there's accountability in multiple places. I think the works that that's

628
01:01:15,440 --> 01:01:24,160
happening, that's emphasizing, you know, how we govern these, the development of these tools

629
01:01:24,160 --> 01:01:30,400
and products is really critical. If you pay attention to what's happening, what's coming out of

630
01:01:31,440 --> 01:01:40,320
the national out of NIST, they've got a framework that emphasizes that the very first step that

631
01:01:40,400 --> 01:01:46,480
has to happen for companies developing these systems to be responsible and accountable,

632
01:01:46,480 --> 01:01:52,080
it has to start with understanding context, which I think aligns directly with what we're talking

633
01:01:52,080 --> 01:01:57,760
about here in terms of understanding societal context. But I but again, like you said, there's

634
01:01:57,760 --> 01:02:05,280
all these incentive structures that will kind of resist that. And in the in the product development

635
01:02:05,280 --> 01:02:11,920
companies, but even outside, right? Right? Yeah, because the accountability is tied to the incentives

636
01:02:11,920 --> 01:02:18,080
and incentives then are tied to gaming the system, including the the the algorithm, the tool, and the

637
01:02:18,080 --> 01:02:21,920
bigger picture that you had, right? And so they ground around the goals. That would be interesting

638
01:02:21,920 --> 01:02:26,320
to model. You're like, you're like, you're that would be interesting to see what that, you know,

639
01:02:26,320 --> 01:02:30,000
I would love to see how you would hypothesize what that relationship is between those faculty.

640
01:02:30,000 --> 01:02:35,840
Oh, I could definitely do that. I'm a I'm a professor. Yeah. Yeah.

641
01:02:39,920 --> 01:02:45,600
Talk. And I'm typically interested for the final point about gathering a trust from the

642
01:02:45,600 --> 01:02:50,720
historical margin of people and getting and getting involved all the domain experts about the

643
01:02:50,720 --> 01:02:58,080
particular problem. And that maybe that you came from Google, and it's like a big company,

644
01:02:58,080 --> 01:03:02,320
which can we have to deal which have a capacity and maybe some resources to

645
01:03:02,880 --> 01:03:07,120
open a workshop or getting a workshop and getting involved for many. But

646
01:03:08,320 --> 01:03:13,040
even if we just concentrate on the historical margin with people, there's so many different

647
01:03:13,040 --> 01:03:18,240
people around there. And maybe you have to think about so many different parts of other people's

648
01:03:18,240 --> 01:03:23,840
and maybe not every company or production, the product maker have a resources or capacity to

649
01:03:24,560 --> 01:03:30,640
do build a certain kind of trust like that, although it is very important. Yeah. And so

650
01:03:30,640 --> 01:03:36,000
will be will there be any kind of like a substitute or other options to incorporate this or like an

651
01:03:36,000 --> 01:03:43,760
accelerating these procedures to, well, generally like an accessible to more companies or more

652
01:03:43,760 --> 01:03:49,680
industrial parts of this person, rather than getting getting like an opening a workshop and

653
01:03:49,760 --> 01:03:54,160
getting a building a relationship like an after like a more than five or 10 years.

654
01:03:54,160 --> 01:03:59,360
I think if it if there's some kind of other options or like if we can encourage other

655
01:03:59,360 --> 01:04:05,680
entrepreneurs to do this things more, I think it'll impact a lot. I would just know your

656
01:04:05,680 --> 01:04:11,280
like opinion about this. Yeah. I mean, so I agree with you 100% like it's a really hard problem.

657
01:04:11,280 --> 01:04:18,800
And I think this has got to require investment like not just for not just from product companies,

658
01:04:18,800 --> 01:04:27,760
because in some way, you know, we're not we're not in a great position to really be fully embedded

659
01:04:28,400 --> 01:04:34,480
in the problems that society cares about surely. Right. And so I think this is the option where

660
01:04:34,480 --> 01:04:39,040
places like Google need to see some leadership and some ownership in this space and kind of

661
01:04:39,040 --> 01:04:47,040
invest in capacity outside. So we have a gigantic sector of philanthropy, foundations, etc.

662
01:04:47,760 --> 01:04:52,560
who I think need to play like a critical role in this. One of the reasons we're trying to

663
01:04:52,560 --> 01:04:57,680
raise the awareness of this is just for what you described to get other people thinking about

664
01:04:58,240 --> 01:05:06,160
solutions to these really hard problems of like, you know, scale and and dealing with

665
01:05:06,160 --> 01:05:11,680
incentives that have to do with speed. Right. So not easy things to solve. But so this is why

666
01:05:11,680 --> 01:05:16,800
we're like, let's raise awareness, get people talking about this, and have more people

667
01:05:17,600 --> 01:05:23,280
applying their mind to finding solutions that are kind of responsible, non extractive,

668
01:05:23,280 --> 01:05:28,080
can accelerate things. I think there are ways to solve these. This is kind of like a moonshot.

669
01:05:28,080 --> 01:05:32,880
There are things you would have to invent that haven't been invented yet. We're trying to kick

670
01:05:33,280 --> 01:05:36,400
off the flywheel to get those conversations going. Yes.

671
01:05:38,400 --> 01:05:46,160
So one thing I'm interested in is what kind of pushback. It's like one phrase you didn't use,

672
01:05:46,160 --> 01:05:54,480
but I'm sure that we agree on it. It's also one of our favorite things to rail against

673
01:05:54,480 --> 01:06:01,280
is domain agnosticism. Right. Like, like, oh, I'm the clever technical person, send me your data,

674
01:06:01,280 --> 01:06:05,600
send me your zeros and ones. I don't even need to know what the zeros and ones mean.

675
01:06:05,600 --> 01:06:10,160
I have cool algorithms and then I'll help. Right. And so part of the goal is like

676
01:06:10,880 --> 01:06:17,840
breaking that as, you know, what I think still for a lot of engineers and engineering students,

677
01:06:18,400 --> 01:06:23,760
it's still kind of a norm and sometimes even held up as a virtual. So I mean, I'm totally,

678
01:06:24,720 --> 01:06:31,600
I love what you're pointing out. I guess I'm interested in two things,

679
01:06:31,600 --> 01:06:40,160
and I'm looking forward to our conversation later. One is the move here is to work with the community

680
01:06:40,800 --> 01:06:45,760
to generate models at first qualitative, but with the ambition of being quantitative.

681
01:06:46,960 --> 01:06:51,280
And of course, that's an interesting move. It's not the only move. And

682
01:06:52,080 --> 01:06:59,120
and then once you have these models, then lots of other tricky questions come up. Like, do we

683
01:06:59,120 --> 01:07:04,080
really think these models are going to be predictively accurate or are they just sort of scenario

684
01:07:04,080 --> 01:07:09,200
explanation tools or kind of, you know, are they quantitative models that we're actually trying

685
01:07:09,200 --> 01:07:15,840
to learn about qualitative effects of possible interventions? So that's, that's, I think that's

686
01:07:15,840 --> 01:07:22,720
an interesting and I like it. But I also I'm curious about its limitations. Yeah. And one of

687
01:07:22,720 --> 01:07:29,440
the limitations that I expect also some engineers might push back on. So like, in the criminal

688
01:07:29,440 --> 01:07:37,040
justice field, right, everybody says, absolutely, arrest is not the same as crime. It's not just

689
01:07:37,040 --> 01:07:42,480
a very noisy signal. It's also a systematically biased signal. And some people say, well, could we

690
01:07:42,480 --> 01:07:49,360
use conviction? It's like, well, that's also a noisy and biased signal. And of course, you know,

691
01:07:49,360 --> 01:07:57,760
society, neither the government nor engineers have access to did a crime actually occur.

692
01:07:59,440 --> 01:08:06,000
And so you get this sort of defensive reaction that, well, but this is the data we have, you know,

693
01:08:06,000 --> 01:08:10,320
so like, I think we've all been to a million talks where there's one slide at the beginning

694
01:08:10,320 --> 01:08:15,040
saying, arrest is not the same as crime, but this is the data we have, you know, and then,

695
01:08:15,040 --> 01:08:20,800
and then plunging into the technical, you know, the statistics of that as data. And I don't know,

696
01:08:20,800 --> 01:08:25,760
that's, I mean, maybe this just gets back to the humility question again about, you know,

697
01:08:26,800 --> 01:08:31,520
hey, we don't know if this data means what we think it means, we don't know if it's measuring

698
01:08:31,520 --> 01:08:39,280
we think it measures. And therefore, we're presenting our suggestions, our recommendations to

699
01:08:39,280 --> 01:08:47,680
a judge or a doctor with some humility, but then I don't know, I don't know how to wrestle with that,

700
01:08:47,680 --> 01:08:53,120
especially because part of the argument for doing all this is that humans clinical judgment is also

701
01:08:53,120 --> 01:09:02,560
very biased or can be. And so I don't know, there's like, which things can cross the the

702
01:09:02,640 --> 01:09:13,280
chasm into something which is structured enough that to to mathematics, and then

703
01:09:14,160 --> 01:09:20,560
to improve the algorithm, and which things can't really, and then the algorithm just has to say,

704
01:09:20,560 --> 01:09:25,520
hey, I don't know about that and keep in mind that I don't know about that, which as you said,

705
01:09:25,520 --> 01:09:29,120
as Tina said, is, you know, vendors are not usually incentivized to do.

706
01:09:30,080 --> 01:09:33,360
Yeah, I don't know, so that's like where that boundary is, I think is really interesting.

707
01:09:34,320 --> 01:09:36,800
Yeah, yeah, and you and you pointed out one of the

708
01:09:38,400 --> 01:09:44,960
the key root causes of this of this of this chasm, and that's this, this is the data that I have.

709
01:09:48,080 --> 01:09:52,240
Don't force me to kind of think about that messy stuff, right, on the other side of this

710
01:09:52,240 --> 01:09:57,600
boundary, the techniques that I have, I'm incentivized by those techniques, and they

711
01:09:57,760 --> 01:10:04,000
incentivize me to abstract those that stuff away. Now, and also the conversations and then it always

712
01:10:04,000 --> 01:10:10,720
goes to, I've spent a year talking to engineers about like, ah, we can, we can, we're gonna work

713
01:10:10,720 --> 01:10:14,320
on building these models. And the first question I get, but yeah, but how do you know they're

714
01:10:14,320 --> 01:10:20,400
accurate? How do you know they're predictably accurate? And then my pushback is, how do I know

715
01:10:20,400 --> 01:10:26,560
yours are predictably accurate? Right, you're very confident in all the assumptions you're making

716
01:10:26,560 --> 01:10:31,680
throughout that life cycle. No one's looking at them, they're not made explicit, right? And so

717
01:10:31,680 --> 01:10:36,320
eventually you actually get over that hump where people can say, oh, I can see the utility at least

718
01:10:36,320 --> 01:10:42,240
in the beginning of this, of me being more informed. And one of the key things about system dynamics,

719
01:10:42,240 --> 01:10:49,440
it emphasizes, don't try to start with data. Don't try to start with the data set. Start with your

720
01:10:49,440 --> 01:10:56,240
intuition and the intuition of others about what's important in this system, and start off by trying

721
01:10:56,240 --> 01:11:02,960
to get some sort of, you know, shared hypothesis. And then that should drive what data do I need if

722
01:11:02,960 --> 01:11:08,080
I'm trying to intervene on a problem in this system. And if I don't have data about this factor,

723
01:11:08,080 --> 01:11:13,440
maybe I shouldn't be trying to intervene on this particular problem right with this technology.

724
01:11:14,320 --> 01:11:18,320
And so this is why we're, you know, this is why we're kind of purposely focusing on

725
01:11:18,560 --> 01:11:26,480
high stakes domain, healthcare because of, you know, the issue of health equity,

726
01:11:27,280 --> 01:11:33,280
healthcare industry is kind of, I think, a little bit more mature in terms of looking for systemic

727
01:11:35,520 --> 01:11:40,800
causes of disparity and kind of embracing that. So there's kind of more openness about like,

728
01:11:40,800 --> 01:11:45,600
hey, we're trying to, we want to use these algorithms, but there's really a lot of

729
01:11:45,600 --> 01:11:49,680
worry and caution by clinicians and others because they know how dangerous they are.

730
01:11:50,400 --> 01:11:55,200
I think this is again where the accountability comes in, right? Because the doctor can use a tool,

731
01:11:55,200 --> 01:11:59,680
but at the end of the day, he's accountable, right? And to the question that I always get is like,

732
01:11:59,680 --> 01:12:06,880
well, the system is, but your AI system is biased, but the person who's making the decision is also

733
01:12:06,880 --> 01:12:11,360
biased. Yes, but the person who's making the decision, he can go to jail. I will sue his

734
01:12:11,360 --> 01:12:17,760
ass from here all the way to Boston, right? That AI system, who am I suing? Who's responsible?

735
01:12:17,760 --> 01:12:24,480
Yeah, right? It's not clear. And that is important, right? It is important to figure out who is

736
01:12:24,480 --> 01:12:28,880
accountable, especially in America, right? We like accountability, like, who do I blame?

737
01:12:30,640 --> 01:12:37,600
Nobody. I go to jail, nothing happens, right? Because the system said I'm high risk. And that's

738
01:12:37,600 --> 01:12:42,720
it. So the systems are being used as expert witnesses without being cross-examined and not

739
01:12:42,720 --> 01:12:48,320
being held accountable. I think in healthcare, it's a really good domain to pick, because at the end

740
01:12:48,320 --> 01:12:54,400
of the day, it's a doctor and the malpractice insurance he has. And so they have, you know,

741
01:12:54,400 --> 01:12:57,760
like you said, there's a vested interest in making sure these things work out. And I think there's

742
01:12:57,760 --> 01:13:02,720
also like, you know, the Hippocratic Oath as well, like doctors don't want to harm people, right?

743
01:13:03,440 --> 01:13:04,000
I'm large.

744
01:13:05,280 --> 01:13:10,800
But so what you're pointing out is like, you know, these technologies are like

745
01:13:10,800 --> 01:13:17,200
really, really serious interventions on society. And you're talking about like, we have to update

746
01:13:17,200 --> 01:13:23,760
our, the cannons of law, right? For these entities, these systems.

747
01:13:24,800 --> 01:13:29,360
Yeah. And I think especially now with the movement on democratization, right? I want to

748
01:13:29,360 --> 01:13:36,160
democratize X. Well, then this kid can go and develop clear view AI with like $200,000. No

749
01:13:36,160 --> 01:13:41,440
accountability. He didn't know about ethics or nothing. You know, it's what, wait, what?

750
01:13:42,320 --> 01:13:48,720
Yeah. So there are issues. I mean, I love your research area. I mean, it's similar to research

751
01:13:48,720 --> 01:13:54,240
area I've been pursuing. But we cannot not talk about accountability. Like we have to talk about

752
01:13:54,800 --> 01:13:59,840
Yeah, I agree. And I think we also have to make sure that at least for me, like

753
01:14:01,680 --> 01:14:07,280
I'm not, I'm not like, I, I can like, at least me personally begin to kind of understand like,

754
01:14:07,280 --> 01:14:11,040
all right, what do we, what do we have to do to update the cannons of law? Like they're experts,

755
01:14:11,040 --> 01:14:16,240
like that's one of the things I like about Andrew Selbs, his work, right? He's a law professor,

756
01:14:16,240 --> 01:14:20,800
right? So, and that's also why I like about his work about abstract and waste societal context.

757
01:14:20,800 --> 01:14:26,160
Because I think hopefully that means that kind of perspective will make its way into the laws.

758
01:14:26,160 --> 01:14:30,960
You see that perspective making its way into some of the frameworks that are coming out of

759
01:14:30,960 --> 01:14:34,960
the government in terms of how we're going to regulate these things. So it gives me some hope.

760
01:14:34,960 --> 01:14:39,840
But I do know that even if you come up with a law that says you have to understand the societal

761
01:14:39,840 --> 01:14:47,760
context, we are not in any position to actually represent it, present it, whether it's in a,

762
01:14:47,760 --> 01:14:54,160
you know, court of law or in a product development life cycle. And so we have to build capacity.

763
01:14:54,160 --> 01:14:59,600
Whereas somebody has to pay millions and millions of dollars, then people will, will move.

764
01:14:59,600 --> 01:15:03,600
Unless they have billions of dollars, then they, maybe we'll charge them billions.

765
01:15:03,600 --> 01:15:05,760
Billions. Billions to get to billions. I want billions.

766
01:15:08,240 --> 01:15:11,840
Sadly, we're coming up on time. But Donald, I want to thank you again for a great talk.

767
01:15:12,160 --> 01:15:14,480
Thank you for your time.

768
01:15:16,960 --> 01:15:20,160
For tomorrow afternoon, I know a number of you already signed up for meetings,

769
01:15:20,160 --> 01:15:24,960
but if you haven't, he's down in Pod C. So you're welcome to go and

