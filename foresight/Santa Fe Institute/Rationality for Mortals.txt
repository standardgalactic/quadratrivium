And the director of Hardin's really great because Gerd is one of the originator of the
theory of ecological rationality at the simple computational models of rules, algorithms
that people use to make decisions in different contexts.
And he's basically introduced complex system science to psychology, cognitive science because
his main thesis is that we cannot understand cognition without looking at its social and
physical environment, only by careful analysis of both the environment and the algorithms
people use to make decisions and solve various problems.
We can understand how people behave and why people behave in different situations.
Gerd is a recipient of numerous awards.
He is member of the Academy of German Academy of Sciences, author of numerous books translated
in many different languages.
And he plays banjo.
So we need to have a closer look at the question.
All right, thank you.
Thanks Mirta.
Today I will talk about how to make good decisions.
If you open a book on rationality in neoclassical economics, behavioral economics, or psychology
of philosophy, you likely encounter the following message.
Good decisions follow the laws of logic, the calculus of Bayesian probability and the maximization
of expected utility.
This is beautiful mathematical theory, but it does not describe how most people actually
make decisions, not even those who write these books.
Let me start with a story.
A professor of decision theory at Columbia University in New York had an offer from a
rival university.
It was Harvard.
And he could not make up his mind.
Should he stay?
Should he leave?
Should he accept?
Reject.
A colleague took him aside and said, what's your problem?
Just maximize your subjective expected utility.
You always write about that.
Exasperated, the professor responded, come on, this is serious.
I'll invite you today in a short tour in our research at the Max Planck Institute for
Human Development on how people actually make decisions.
And I'll start with a key distinction between risk and uncertainty.
A situation of risk is a well-defined, stable, small world where the agents know the exhaustive
and mutually exclusive set of all future states and the exhaustive and mutually exclusive
set of their actions, of the consequences given their actions and states.
Jimmy Savage, who is often referred to as the father of modern Bayesian decision theory,
called this a small world.
There are examples for the small worlds.
If this evening you go to the casino in Santa Fe, if there is one, and play roulette, you
are in a small world.
You know every possible state that can happen, from zero to 36, all the consequences and
probabilities.
The tools for dealing with risk or maximizing expected utility, Bayesian probability updating
and many more.
In a small world, by definition, nothing new can happen.
I had the quote from Fjodor Dostoevsky, who said, in this world, if everyone would be
rational, he said, nothing would happen.
With amazing foresight, he had seen the development of rationality.
Now there are two, most of the problems have to do with, however, with some aspect and some
amount of uncertainty.
Uncertainty is a vast sea of situations where there is no small world where we either cannot
know all future possible states or their consequences.
It's not just about probabilities, it's about the state space.
Researchers have two ways to go.
One is to take an everyday problem, like investment, whom to hire, or whom to marry.
These are all situations of uncertainty because things can happen that you had not anticipated.
Then go one way, reduce it to a small world where everything is known and you can maximize.
That's the way that most theories in neoclassical economics, also behavioral economics, at
least the standards, are going.
The other option is to take uncertainty seriously, face it, and develop and study the tools that
actual people use to make decisions in an uncertain world.
Here by definition, optimization is not possible.
You cannot construct even a subjective probability distribution over a set that you don't know,
that you don't know completely.
I will talk today about one class of tools that are useful under uncertainty, and these
are heuristics.
Heuristics are rules that embody the art to focus on the important and ignore the rest.
Heuristics can lead to good decisions in a world where we cannot forecast the best decisions.
Examples are Herbert Simon's satisfying, Boston Fruity Trees, imitation, agent-based
models who use simple rules like the flocking of starlings, the beautiful models.
I will today go a different way than most of the models are going.
For instance, even behavioral economics, who criticizes neoclassical economics, criticizes
not normative standard, not homo-economics, not the maximization of expected utility,
rather takes a standard to evaluate people, and if there is a discrepancy, the blame is
never on the theory.
It's always on the people.
And then you get the list of biases that are all in your minds, and explain why you all
make these strange decisions, and stubbornly have no insight in your failures.
This is not my message.
I want to study how people make decisions and formalize the heuristics, and then find
out in what situation do they work and where do they not work.
That's the question of ecological rationality.
Rationality is not in the mind, but in the adaptation of mental strategies to certain
environments.
Let me start with an example that makes the difference clear between a model made for
risk and model made for uncertainty.
Every Markowitz got the Nobel Memorial Prize in economics for solving a certain problem.
The problem is you have a number of N assets, and you want to invest your money, and you
want to diversify, not put everything in one pocket, but how?
The answer is the so-called mean variance model.
It's a standard probability model.
You need to estimate all the future returns, their variances, and covariances.
When Harry Markowitz made his own decisions about the time of his retirement, he used
his Nobel Prize winning optimization method.
So we might think, no, he did not.
He used a simple heuristic that we call 1 over N. N is the number of assets or options.
So if it's 2, you do 50-50.
If it's 3, a third, a third, a third, and so on.
Now 1 over N is a heuristic.
The mean variance model is an optimization model.
The mean variance model assumes we are in a world that is stable, where we can estimate
the parameters to some degree of precision.
Number of studies have looked at 1 over N.
And for instance, the study by D. Miguel here has looked at seven real-world investments
and found that in six of the seven cases, 1 over N made more money than Markowitz's optimization
as measured by sharp ratios and similar ones.
The results have been found when exchange-traded funds were tested, ETFs which are close to
1 over N, they're very hard to beat.
Now there is a battle in the literature, which one is better?
Complex optimization or simple heuristic?
This is the wrong question.
None of this is better.
No algorithm is the best.
We need to ask a different question.
So can we identify the environment, the conditions where a simple heuristic like that does better
than another one, another model, and where it's the opposite?
The answer to this question of ecological rationality is not known.
Some of you might figure it out.
Here's some hypothesis.
So if you think in terms of the bias variance decomposition, then 1 over N has probably
a strong bias but makes no error due to variance because it doesn't estimate any parameters.
So the error due to variance means that you get different results depending on what sample
you make your estimates.
So the real question to solve here is when is the bias that 1 over N has larger than
the total error that mean variance makes, that is both bias and variance.
So this is the way I would like to think here.
Excuse me, did Markowitz ever say why he chose 1 over N?
Yes, he did.
He had a psychological explanation.
He said if I would, his choice was between the typical retirement situation, just stocks
or bonds.
If I would put everything in bonds and they would go down, I would feel bad.
If the opposite, I would also feel bad.
I would have regret.
So I just did 50-50 to avoid regret, but it's very different.
He was actually very interested when we and others showed that he wasn't at least where
his experiments are.
It's just stocks, a slightly different situation that was shown that simply can do better.
So the confusion between situations of uncertainty with risk, the idea that every situation of
what uncertainty is one of risk is called the turkey illusion.
So why?
Assume you are a turkey.
It is the first day of your life and a man comes in and you fear he will kill me, but
he feeds you.
The second day of your life, the man comes again, you fear he might kill me, but he feeds
you.
Third day of your life, same thing.
If you do Bayesian updating or any similar model, the probability that the man feeds
you and doesn't kill you gets higher every day.
And on day 100, it is higher than ever before, but it's the day before Thanksgiving and you
are dead meat.
So the turkey was not in a situation of risk.
He missed important information.
There was an option he could not think.
Now the turkey illusion is probably not so often committed by turkeys, but more often
by people.
And here's one example.
In 2003, in his presidential address to the American Economic Association, Robert Lucas,
the macroeconomist, argued that one has learned from earlier depressions and macroeconomics
models take care.
So he said, my thesis in this lecture is that macroeconomics has succeeded.
Its central problem of depression prevention has been solved for all practical purposes.
Now what you get is an illusion of certainty if you apply the models that are based on
assumptions of risk to one of uncertainty.
I've shown you just the illustration here.
In the year 2003, when he gave this talk, the Volateli Index, we are X, went down and
down and down.
So it got better and better and better and better, safer, until shortly before the crisis
hit.
And that's the same thing as the mass modeling models they used.
By the way, the models that Lucas described in his presidential address all assumed a
stable and unchanging structure of the economy.
So what's the alternative?
And here's the research program I and my research group has been following up.
And it has three questions.
The first one is descriptive.
What is in the adaptive toolbox of an individual, an organization, or a species?
So what are the heuristics they use, the cognitive capacities that the heuristics exploit?
And what's the building blocks?
And the goal is to use, to develop algorithmic models of these heuristics, not labels.
We have since the 1970s a tradition in the heuristics and biases program of labels, like
availability representative, nobody knows what exactly they mean.
And they used never to make predictions, but always to explain something after the fact.
The same is with system one and system two, which is just a list of dichotomies.
And the dichotomies is where a science should start and go to good models.
And this is a case where we went from models to dichotomies, to backwards.
Trusky had models of heuristics like elimination bias.
But then after Trusky died, Kahneman sided with this dual system theories that has been
long criticized in psychology before.
Alan Newell said, in a famous paper in 1957, you can't ask 20 questions to nature.
And criticizing the tendency to not do models, but to do dichotomies, serial versus parallel,
unconscious versus conscious, fast versus slow, and so on.
And all this is so intuitive that one forgets that one does science.
So the second question is, no, it's a prescriptive question.
When should we use heuristics?
And when not?
And what heuristic?
And then, certainly, it's heuristics all the way down.
There's no option.
The real question is, what is a smart?
Is a smart to imitate?
The peers, when searching for a research topic, or should I think myself?
So here the question is, can we prove mathematical or at least with computer simulations show
under which condition, for instance, one way in works, where does imitation work, and so on?
And the final question is one of application.
It's doing intuitive design using all these insights.
So I'll start with the adaptive toolbox.
And here is a few on part of the toolbox.
There are core capacities that humans have.
On that basis, the heuristics can be simple.
The core capacities are quite complex, meaning it's very hard to build a brain that can't do that.
Let me start.
So there are, I can only give you some examples today.
And on the left side, there are social heuristics like imitation, word of mouse, which are highly powerful.
There would be no culture without imitation.
Mike Tomasella has shown in his beautiful research that children, human children, imitate much more precise
and much more general than primate infants.
And it's one of our big advantages that builds culture that one doesn't have to learn from scratch all the time.
It's also one of our greatest dangers.
So let me start with recognition.
Recognition is a fundamental ability of the human mind.
We can recognize faces.
We can recognize names.
And recognition is different from recall.
If you cannot recall the name of a good friend, that happens.
Particularly when you get older or when you're a jet lag like me at this moment.
But if you do not recognize the name of a good friend, then you're a clinical case.
So the recognition heuristic exploits this capability to make inferences in situations where you don't know much.
Let me do a demonstration with you.
Are you ready?
Okay, assume you are in a TV show called Who Wants to be a Millionaire?
And you made it through the end.
And here is the million dollar question.
Which US city has more inhabitants?
Detroit or Milwaukee?
Time is ticking.
Who of you thinks it's Detroit?
Hands up.
Who of you thinks it's Milwaukee?
Hands up.
A rough estimate, 60% of you got it right.
It's Detroit.
When we did experiments with undergraduates at the University of Chicago, who think there is more or less in the country,
we got the same result.
60%.
And then Dane Goldstein and I asked students in Germany, what do you think?
So not just this one question, but all questions about large American cities.
What do you think?
How many Germans got the right?
More than 60.
More than 60.
And why do you think that?
Because otherwise I wouldn't ask you.
They relied on the heuristic of recognition.
Right, yeah.
And trust that a lot too.
You're right.
Or this is when you give a talk with two people who have already your work.
OK.
The point is, many of the Germans have never heard of Milwaukee.
And the recognition heuristic tells you, choose the option you recognize.
And they were right.
Now, let me test you the other way around.
OK, ready?
Which German city has more inhabitants?
Bielefeld or Hannover?
Who is for Bielefeld?
You.
Who is for Hannover?
You got it right.
I think the two suspected that I want to fool them.
You shouldn't think too much.
If I ask the same question to Germans, many of them get it wrong.
Because they had heard of both.
And they need to retrieve knowledge about that.
See the point?
And we had just a less-is-more effect here.
You got more questions right about the German cities than about America.
Although you know more about the Americans than the Germans.
So how does this work?
So here's the study of ecological rationality.
So think you have a test of a hundred of these questions.
Maybe the hundred largest cities in the US.
And consider the lowest coefficient.
If you haven't heard of any of the cities, your performance is 50%.
Yeah, you just guess.
If you've heard of all of them on the other side, it's also 50%.
Because you cannot use the recognition tool.
You're seeing both cases that have no knowledge.
Now assume the person has knowledge.
And the knowledge is defined here.
It's defined as the number of answers you get right in cases where you've heard of both.
Like before, it was 60%.
And actually, the curve here, the second curve, is about knowledge relative of 60%.
Person B recognizes all American cities like you and gets 60% right.
This is knowledge.
Person A recognizes only half of them.
Can apply the recognition here and gets more right.
The curves are drawn for recognition validity of 0.8.
Meaning that the number of correct inferences in all cases,
where you have recognized one and not the other.
So you can measure all these things.
There's no parameter fitting game here.
And if the knowledge validity is at least as big as the recognition validity,
there is no lesses more effect in it.
So the lesses more effect you find on this three curves below,
it means that on the right side of the curve, performance goes down,
despite you either no more or recognize more.
So this is an illustration how we can model and can actually make quantitative predictions.
And we have used this, for instance, in predicting maybe more interesting things in cities,
unless you are on a million dollar question.
Like you can how to predict the outcomes of all Wimbledon gentlemen's single games.
There are 128 contestants, there are 127 games.
And you make a prediction for everyone.
And you can do this with the ADP rankings or with the Wimbledon seedings or with ignorant people.
Exactly partially ignorant, because fully ignorant cannot use the heuristics.
And again, from here you can see, you need to find people that have heard about half of them.
And not if there was one.
And these were German amateur players and they made in two studies
more correct predictions than the ADP rankings or the Wimbledon experts.
And if you change that to those who recognize less, then they will not make more predictions.
So this is just an example how one can analyze the ecological rationality of a heuristic.
You can easily tell when the heuristic is worthless,
when the recognition validity is a change level, that is noting.
And we also have a way to make earlier concepts like availability into something precise.
That is useful and also illustrates some of the amazing capabilities of the human mind
to exploit one's own ignorance.
Second example, fluency.
Fluency presumes recognition.
And it can be measured by the time an option comes to your mind.
And the point I'm making with the next example is that a human intuition using fluency,
creates no speed accuracy trade-off.
What is the speed accuracy trade-off?
That's usually something that if you make judgments fast, then you lose on accuracy.
That's the story about fast versus slow thinking fast versus slow and so on.
A speed accuracy trade-off, you get them if you test undergraduates with problems I've never heard.
But if you test experts, it's the opposite.
So here's the example.
The example is there are experienced handball players and some of my postdocs put them in
the uniform with a ball in the hand in front of a video.
And the instruction was there will be a top handball game, running for 10 seconds, then
it will be frozen, and tell me immediately what the player with the ball should do.
So they looked at that, it was frozen, and they would say I'll pass to the left here
or shoot at the goal or something else.
Then they had 45 seconds more time to study the still frozen image carefully.
And they were instructed, if other options come to your mind, please tell us.
And then one of them said, oh, I didn't see the guy on the right side, passed, that would
be a good one.
And after all of this was done, the options were plotted against the quality, as measured
by the best coaches we have in the country.
And what you see is very interesting.
On average, the first option that comes to your mind is the best one, the second is the
second best, the third is the, and so on.
That means if an experienced player goes with the first option, he or she is most likely
right.
And we also found that when we asked them at the end of the 45 seconds they had studied,
what would you do now, about 40% change their mind, and got into a worse option.
So here we have a case where we have no speed, accuracy, trade-off.
Country, it's better, on average, to act on your first impulse.
In a game you have to do this.
But in the experiment, you have to change the thing longer, but it doesn't help.
It regularly hurts.
Remember, this only works for experts.
If you have amateur players, the curves, they're not going down, they're just stator.
So this has an important insight.
So intuition, as I define it, is based on years of experience, what you want to do comes
quickly to your mind, and you don't know why.
And the fluency heuristic is an explanation how this works.
The reason why the research has come to the conclusion that what the general speed, accuracy,
trade-off is mainly because one studies undergraduates with problems they've never seen before.
Gary Klein, who studies fire furthers, finds the same results.
So the third and last category, I want to give you examples, is an interesting one,
where it is about the human capacity to order things in importance, and these are heuristics
that are based, that base the decision on a single cue.
And let's give you an example about hiring.
When Elon Musk was still young, and Tesla was young, Elon Musk did the hiring himself,
and he reported how he did it.
He did not get an assessment center.
He did not study CVs, but he, according to himself, he relied only on a single cue.
Just a person possesses some exceptional ability, like a banjo player.
And if not, no hire, if yes, hire.
Now you might wonder why not more cues.
But look, exceptional ability is something that has some kind of redundancy with other
things.
So a person who has this to get there needs to know how to persevere, how to concentrate,
how to sweat, and not give up.
And so that's the spirit behind that.
It also illustrates that these heuristics are adapted to certain problems that will not
work well when the company, like Tesla, had grown.
Because then you need people who do routine work.
One needs to adapt that, and that's the idea in the adaptive toolbox.
Now here is another hiring heuristic, and that's what we call a fast and frugal tree.
A fast and frugal tree is an incomplete tree that has an exit on each of the questions
or cues.
So if the number of cues or questions are n, it has n plus 1 exits, while a complete
tree is in the case of dichotomous 2 to the n.
So when Abbason was young, a Jeff Bezos also reported about how he did make the description,
and we reconstructed that in the form of a fast and frugal tree.
So note it's sequential.
It doesn't integrate as rational theories usually assume.
Interestingly, his first question was the same.
So does the person have an exceptional ability?
But if yes, it wasn't enough.
Second question, will you admire this person, which is a quite unusual question?
We explained that if he, Bezos, admires a person, he will learn from that person.
But that was also not enough, and only if that person raises the average level in the
unit where he or she will be, that's being hired.
So the properties of these trees are of a lexicographic nature.
A lexicographic strategist cannot be mapped into a utility, into a smooth utility function.
And that's why classical books like Luce and Wreifer have always ignored them and looked
down at them.
But people use these strategies, and we have shown that there are certain situations where
a fast and frugal tree can outperform random forests.
And the interesting question is, again, can we prove this situation?
Can we identify them where that works?
So I'll show you one way to, about how to model such a tree.
If you use these three questions in the order, how many trees are possible by changing the
exits?
Sorry.
Four.
Oh.
It's four.
You tell me later, the other two.
The same and changing.
Yeah, the same, yeah?
The same cues in the same order, yeah?
Otherwise, it would be more.
Correct.
Bezos tree is on the left side.
So any classification can make one of two errors, like making an offer to a person whom
you should not have made an offer that's a false positive, or not making an offer to
a person whom you should, that's a miss.
The Bezos tree minimizes false positives, yeah?
He only makes an offer after long, yeah?
These four trees do a balance between false positives and misses.
On the right side, you have a tree that maximizes false positives relative to miss.
And in the middle, you have very interesting trees that have a zigzag structure.
An example for a tree on the right side, I'll show you from our work with the Bank
of England.
So the Bank of England has a problem how to identify vulnerable banks.
How do you do that?
Now the classical method is in the so-called Basel or Basel two and three programs, and
where the banks calculate a value at risk.
And if you've ever done that, so if you are the CEO of a large bank, you have to estimate
thousands of risk factors.
That means a covariance matrix in the order of millions.
And you can't use more than five, ten years of data because before something strange
would happen.
These calculations are impressive.
They have prevented, they've never prevented any crisis and probably will never prevent
any crisis.
It's another Turkey illusion.
This standard probability theory applied to the situation of uncertainty.
I once gave a talk to the European Central Bank and said, the calculation that you do
border on astrology.
And I was waiting for someone to say, no, the answer was, yes, but what else should
we do?
Here is what else could be done and systemically be studied.
So this is now a fast and frugal tree.
That's the same structure as the one that's seen before.
And it asks just three questions, for instance, leverage ratio below a certain percentage
and if it's lower, red flag.
If not, it goes on and asks two more questions.
I'm not going into the details here, but you see the structure.
It's important that it works very different from standard rational models.
So for instance, the Swiss bank, UBS, had failed before the crisis on the first item.
So the leverage ratio was much less than half of that.
That would have identified it immediately.
UBS did marvelous on the other criteria, but the trees are non-compensatory.
It's like the human body.
If your heart fails, a good lung doesn't help you.
You can't compensate that.
So it's a very different model.
And the reason why this type of strategy cannot be mapped into a smooth utility function,
who cares about that?
It has to work and not to follow some mathematical doctrine.
Again importantly, it reflects sequential thinking and has a few other advantages.
Bankers can understand that.
And also, the banks, the regulators are more safe that the banks cannot game the system.
If you have to estimate millions of co-variants, you have plenty of room to game.
Here, it's much easier.
It's much more difficult to game the system.
So can I just ask you something unconfused?
So this is a risk-minimizing structure that can be calculated based on known states.
Earlier, you started by talking about heuristics as being solutions to the uncertainty problem.
This looks like a risk problem.
The uncertainty is in what banks are doing.
So the model is a heuristic.
It's not an optimizing model.
And your right, it goes, it's the rightmost tree.
It tries to minimize the misses.
You want not to overseas.
And the costs are false alarms.
That's what the decision is.
How we build the trees is a mixture between, so the economists at the Bank of England,
they identify the variables on base and we check them down on empirical data.
So for instance, the cutoff points, it's all empirical data.
We deliver the structure.
Now, don't put this in a regression.
Think about this structure.
That's the way this is being done.
And then it needs to be tested how well it works.
The advantage is transparency.
That it can easily be understood and it can be changed in cases there is a problem.
And it's an alternative that can be systematically studied.
Yeah.
Well, I'm not going to, the states are still all nine points, but the state outside of the
system or the framework.
Whereas when you had the opening definition of risk and uncertainty, there's something
about the small world about these states.
It's a good question.
So the world of finance here has incredibly and also unknown uncertainties.
So the numbers that the Bank of England gets from the banks, you can count that they're
not the real numbers, they're polished.
The other source of uncertainty is in what's actually happening in the financial systems.
And there could be another Russian default or something.
This is the uncertainty I mean.
And this is a model how to deal with that uncertainty.
And there's no claim about optimization or maximization for anything.
It's just an attempt to do better than what one has.
Here is a book we've done, which is how to build these trees from empirical data.
And what's the alternative in case that's of interest.
But the uncertainty in the financial system is enormous.
Is that, sorry, let's take it a little bit.
So I was sympathetic to your critique of the dichotomizing approach that you've adopted
it yourself in the heuristics versus the optimizations.
And is it possible actually, given the observation that we just made, in fact, what you're calling
a heuristic is just on the spectrum towards full optimization.
Yeah.
Yes.
Okay.
Yeah.
I mean, if you do Bayesian, they're Bayesian usual optimization model, then you start with
uniform priors.
That's a heuristic.
You're totally right.
It is, I just make it simple.
But it's very clear that one should pay attention, I think, to the heuristics.
Yeah?
There's a bit of a category error here.
You actually mentioned ROC curves.
The whole point of ROC curves is, which are middle of standard machine learning, is that
they are an alternative.
You don't come up with one.
You set your own level on an ROC curve.
And you're simply saying, go on ROC curve in a way that you're less likely to screw
up.
Sure.
That makes perfect sense.
But I don't think it's really got to do with optimization versus heuristics, just using
an ROC curve over, and the points that David is making in the fellow over there, is it's
a very high level coarse grain space as opposed to these banks are being stupid and are using
a fine grain space where you can't measure the variables.
But the point is, is an ROC curve doesn't give you a single answer.
You're putting in that I don't like the chance of screwing up.
Therefore, I'm going to set my threshold in the ROC curve and go over to this side.
Okay.
What you picked this point here, the one-game map, the fast and frugal degrees onto an ROC
curve, onto four points.
And that's a direct connection between an optimization model.
Then the question is, whether the optimization model helps you for a certain problem.
The name and Pearson theory, which is below the foundation of the ROC curves, makes a
number of assumptions about the distributions which are not necessary here.
I see a point here, and also it's difficult for the ROC curves to deal with more than
one variable.
Can be done, gets very complex.
Here it's relatively easy to deal with them.
So there are differences between these two curves, but you also write and we showed that
in this paper here, that the heuristic can be mapped into an optimization model, right?
So I'll end with a few remarks about ecological rationality.
So the idea of ecological rationality is from Herbert Simon, and that's a famous quote from
him where he says, this is a analogy.
It basically says, in order to evaluate the rationality of behavior, you need to look
both at cognition, the strategies, the heuristics, anything else, and the environment, and how
they match.
It also means the standard definition of rationality is following the laws, say, axioms of consistency
or maximization of expected utility is an internal definition.
It doesn't take account how these things work in the world outside.
And for instance, that part of behavioral economics, who has adopted the heuristics
and biases program, uses a single internal definition.
There is to be a law of logic or probability, and people are measured against that.
And what I'm arguing, we need to measure behavior against the real world, against the
measure heuristic against the real world or the success.
What I'll show you now at the end is just a simple and intuitive answer to the question,
can we, I've shown you a few heuristics that just rely on one reason.
Can we identify the conditions under which relying on one reason cannot be beaten by
a linear equation that has more reasons, including the one reason?
Clear?
So do you have an idea?
I'm just asking you before, because afterwards it's so evident that we knew it all along.
To be clear, it's a subset of that larger space, right?
Yeah, it's a subset.
So think about it.
A binary decision, higher or not higher, there is Elon Musk has one reason.
The question is, under what conditions will relying on this one reason always lead to
the same results that a linear model that includes many valid reasons, they're all valid,
including the one.
Clear?
It's a subset.
Yeah?
Here's a simple condition, and it's if there is a dominant queue.
But it means that the weights of binary queues form a dominant queue structure.
And so the weights are like regression weights, it's the additional contribution to the first
one.
So if the weight of the first one is larger than the sum of all the others, you can intuitively
see it will not get to any other decision.
And then the next question is, how often does this happen?
We have looked at machine learning data sets, and it happens quite often, astonishingly
often.
So this is not the only condition, it's just a sufficient conditions.
And we've looked at three conditions, and in the median value is that in 90% of all
comparisons in the data set, this condition holds.
So if you take half of the data sets, in 90% or more, it holds, and the other is not.
And then the next question is, what are the other conditions?
And I'm not going into this at this moment, but end with a more general view that's probably
known to many of us.
So this is a standard justification for why people use heuristics.
It's an accuracy effort trade-off.
So people are a little bit lazy, or they don't make the effort, and they pay for that in
accuracy.
That you will find in the Kahneman book, on justifying why biases need to be eliminated,
or in the Kahneman-Siperni and Sunstein book, by noses.
So we have read a very interesting exchange between two David's here, and the Kahneman-Siperni
and Sunstein, where David Wolpert and David Krocker point out, really, that noise has
often a function, and we should not try to eliminate that.
Your examples were mainly from not about the examples that Kahneman has in mind, but they
apply equally to the key topics of the noise book, which is sentencing, so how much, what's
the punishment for a certain crime, and also pricing annuities.
And it is not that sentencing is like Bull's Eye, where there is a right answer, and different
experts with different opinions should converge on one.
You can have many things.
You want to punish the person.
You want to be low on things and give the person a chance.
You can have many reasons, and it's also an issue where variability is a motor about
change in societies.
And so, again, both of these views assume that there is a truce that is singular, that
is known, and the biases, the difference to the truce, and also the variability on the
truce should all be canceled, and that's the key idea behind that.
And the answer, this is not the case.
We have a bias variance trade-off, and where we have not only a bias, but also variance
plus noise, it's illustrated here.
You have two darts, and on the left side, the player throws systematically too far to
the right and low, but the variance is, the variability is small.
On the right side, there is a player who has no bias.
Bias is zero, meaning that on average, the darts are in the middle, but only on average.
Variability is high, and we can see that the real idea is not to reduce bias to zero,
as the typical messages, but to find some reasonable compromise between the trembling
hand and the bias.
Juristics, if they have zero free parameter, like one over n, have a bias, like on the
left side, but no error due to variance, they always hit the same point in this image.
And that's a way to understand why simple can help.
And it's also a way to understand when simple does not help.
For instance, in that thing, if you have really large amounts of data and the world is stable,
then the simple solution will have too much bias, because the amount of data will reduce
variance.
Can you say a little bit about, in bias variance, if I just go from bias and variance or expectation
values over both your decision algorithm and also the real world, if I go one over n and
the real world is varying, I can have a very high variance, I'm not sure in what sense.
One over n gives you, it doesn't do estimations, so if your algorithm doesn't estimate anything,
you cannot make estimation errors.
But we can talk afterwards.
Okay, let's do this.
From how it's, yeah.
It's districts.
Though I'm happy to talk about that, but in this image, the variability is due to different
samples on which the same algorithm estimates its parameters.
And if you change the samples, you get different ones, in particular if the samples are small,
the variability is higher.
And that's the idea behind it.
The bias variance dilemma, and that may also behind your question, applies to situations
that are situations of risk, where we only have to estimate the probabilities.
It's assumed as stable.
In this sense, it's just an analogy to understand what's, why the heuristics, when they fail
and when they work.
Let me finish with three methodology principles.
It is very important to have algorithmic model of heuristics, not labels, not system one
and other things, that we do not know what they predict.
Second, it is very important to do competitive testing.
Like to test a heuristic model against maybe some machine learning model.
We have still parts in, for instance, economics, but it's very little competitive testing.
At best, you eliminate a factor from your regression.
But really, to test against a different class of models, really be done.
And particularly, it's important to learn whether a very highly complicated algorithm really pays.
And we often have a complexity illusion.
And think that something that is simple is nothing worse.
For instance, Harry Markowitz would have not gotten an economics Nobel Prize for one over N.
And finally, it's also important to do not only out of sample prediction as it is regularly
done in machine learning, but also out of population prediction.
So the studies that I reported that being, you know, of investment where one over N is very simple.
They are not out of sample, but they are real forecasting in the future.
And the mean variance model or the Bayesian models in, they are not,
their parameters are not done by out of sample.
Because out of sample creates a stable world, a fairly stable world.
But they are estimated in the first time interval, and then they're predicted in the next one.
And as we know from certain machine learning, techniques that prove very successful in diagnosing
may lung diseases.
When they were used in a different hospital, they failed.
And that's a problem with out of sample prediction.
And predicting, doing the prediction in a different hospital, then is out of population.
So I was talking today about three misconceptions.
Complex models are always better than simple heuristics.
They are sometimes better, but not always.
Slow thinking is always more accurate than fast, intuitive decision making.
No, particularly not by experts.
Even chess masters, when the best, so the international masters have been studied,
for instance, when they have time constraints, they do about as well as without time constraints.
If you have excellent chess players, but not at this level, then the difference gets bigger.
And finally, it is not correct that people rely on heuristics because they're biased,
lazy, or irrational, as the typical message still is.
So I invited you today to a voyage into our studies on Homo heuristicius.
And I've made three points.
Risk is not the same as uncertainty, although often the term uncertainty is used for risk.
For instance, most of behavioral economics talks about uncertainty, but it's all problems of risk.
And you detect the problem of risk immediately because the reason to know what's right in search.
And the uncertainty, you don't know that.
Second, logically rationality, the consistency axioms, the expected utility maximization
is not the same as the ecological rationality.
The ecological rationality is much harder to model, to analyze, and it's about the adaptation to the real world.
And in, under uncertainty, less can be more.
And one way to understand that is it's not an accuracy effort trade-off, but a bias variance trade-off.
And finally, human intelligence evolve to deal with situations of uncertainty.
And so did animal.
Not with risk.
And one of the reason is that we are not very good at calculating.
Let's just show a start here.
And also, some people think we need to get rid of uncertainty.
I've often heard from well-known economists, so when Reinhard Zeldin had his 80s birthday,
I gave one of the four keynotes, and another economist who, the Nobel laureate,
afterwards came to me and said, interesting talk, but I don't like uncertainty.
That's a matter of taste, but just imagine if we would know everything, if we would live in a world
where every possible situation that can occur, the consequence that everything is known,
and all is just about the probabilities, which were all the uncertainty strain.
Would that be an interesting world to live?
It would be, as Frank Knight has pointed out, would be no innovation, no profit.
And there would be no fun.
And life would be like in the movie where you wake up in the morning and everything goes back.
Or as Dostoevsky said, if everything would be rational, nothing would happen.
Thank you for your attention.
Thank you, Gerd.
We are actually at time, so who needs to go, let's go.
And otherwise, if you have any questions, maybe you can take them.
What should I do?
Take your own questions, if you like.
No, no, you take them.
OK, all right.
Good stuff.
I think there's really supposed to be a lot of that, really.
So I'm wondering, actually, so you've got your statistics and you showed, like, that you might have different ones.
And I'm thinking, in any one situation, you can apply a multitude of your statistics.
So my question is, is there any computational principle by which you decide which heuristic is allocated control of behavior?
Because they are operating in parallel or subsets of them are operating.
They may conflict in terms of the final behavior or choice that's exerted.
So is there any framework for deciding the allocation of control of which heuristic gets to control?
This is a very good question.
So there are two or three models about how to model that.
One is reinforcement learning, so that you learn a hierarchy of heuristics, like imitate or do something else.
And depending on the reinforcement and the ego, and that makes the order.
That is, there's a paper by Rieskamp and Otto on that showing that an experimental paper.
The selection problem is not really being solved here.
And you might think about a heuristic that decides about the application of heuristic.
We may just do reinforcement learning or something else.
Unfortunately, I won't be able to meet with you if I had time to meet with you.
This is what I would ask.
So in the mathematics and computer science of risk assessment in criminal justice, for instance,
there are very similar debates.
And you may know of a computer scientist named Cynthia Rudin argues that we should use simple decision trends rather than more sophisticated algorithms.
And her argument is partly because partly that more sophisticated algorithms in that domain are typically only marginally better in accuracy than the simple decision trees.
But also, as you referred to, in that you made, there's a demand for transparency.
If your government is going to put you in jail, you deserve a very simple, clear explanation of why.
And so I guess I wanted to ask you, so I think with some of the other commenters that in some sense what you're talking about is simply a different space of models that can be optimized.
I mean, with, you know, you can choose the structure of the tree with their hands and orders, whether they're, you know, the order of the questions in the banking example.
There were parameters that I assume had to be set in some way, the threshold parameters.
But so I guess my question is, when do you think that, in what domains do you think there is a strong argument for simplicity and transparency, even if something more complicated is somewhat more accurate?
And by the way, I also agree with you that that ladder plane is often made when it is not true.
Yeah.
But in the cases when it is true, sometimes we still prefer the simpler thing.
So I agree with Cynthia Rudin.
And she and us, we have shown that in sentencing or bail decisions, very simple arguments that look at three various typically like age and previous sentences.
Are about as good as complicated things or often black box, like compass and where nobody knows exactly what's in it and what's not.
And also that these are situations where transparency is another important thing.
So, for instance, in the European Union at this moment, there's a decision being made about the AI Act.
And the AI Act is in part about ask for transparency of algorithms that are so-called high-risk algorithm and sentences is one of these examples.
And the EU Union, if they don't change now things, they're voting about it, will not allow black box algorithm.
And I've worked with the German shoe farm, which is like the American feet, so credit rating.
They have immensely complex algorithms.
If you make them simple, simple arguments do as well.
And so in many of your questions, when can we expect that complex AI will work and not.
It's very similar to the distinction between risk and uncertainty.
If a situation is stable and well-defined, AI is much better than human.
So like a classical example or a chess and go.
If a situation is highly uncertain, like how can you predict whether bail decisions about whether the guy will actually come back when they.
That's very difficult.
So many factors, uncertainty in these cases, we have little evidence that say deep neural networks would do consider better than humans.
But we know that it's black box.
Right, I guess my question is, in some cases, we want, we should use the simpler thing, even if the more complicated is more accurate.
For other reasons, because maximizing accuracy is not the least that matters.
Right, and also, so for instance, in my, in our work with the shoe farm, so this is the German fecal.
It turns out that they're using variables that I'd better not telling anyone just to get a Yota male accuracy.
For instance, when you ask for your credit score, your credit score goes down.
Yes, we don't want to tell the speed, right?
So it is not acceptable, but it helps to predict.
So the compromise will be get rid of these aggressions, make them simpler, and you may lose an Yota in prediction, but there's more than just predicting accuracy.
There are human rights about transparency.
So this is very, very, I have a slightly provocative question that it sort of hints at what we are doing.
You like the logical rationality is not equal to ecological rationality.
I don't mean this just semantically, but aren't you letting people off a bit too lightly with this distinction?
I mean, surely there must be something wrong with logical rationality if it leads you to ecologically irrational decisions.
I feel that our program looks at logical rationality and asks, come on, there is something wrong with this because it doesn't, basically it doesn't work in the real world.
So are you being diplomatic by making this distinction?
Do you really mean there's something wrong with logical rationality and we should really think of rationality as a different thing?
Yes.
So logic is a system that's very useful, but not for everything.
That's the point.
You wouldn't even understand language.
When Kahneman and Tversky asked people about Linda, the bank teller, what is more probable?
The term probable does not map into mathematical probability.
It is, if you look in the OECD, it says, is there evidence?
No, there's no evidence that she's a bank teller.
Does it make sense?
No.
So to impose logical rationality on everything, that's me.
This is no good.
And it, and it mistakes the human intelligence.
We make an inference from the content.
What do these problems terms mean?
Similarly, Linda is a bank teller and a feminist.
So an end in everyday language, the end means sometimes logically ends, sometimes something different.
And we have an amazing intuitive capability to infer it.
For instance, when I say this evening, I invite friends and colleagues.
You don't think it's the intersection.
It's the logical or.
So these are the real interesting questions.
How does the human mind make this stunning inferences?
Rather than declare it's an error, it's conjunction fallacy.
Although what you're saying there is about language and not about actions.
This is about language.
So I have one example that is becoming a favorite of mine.
We can show that a, so this is totally small world logical rationality.
If you like, someone who maximizes expected utility does not maximize utility over time.
To me is a catastrophe in the logical structure that you're building a theory where you say,
well, we can't really tell you what you should do until you tell me what it is that you really want.
And that is your utility.
And then we give you a protocol.
Now I start behaving according to this protocol and I don't get the thing that I asked for.
So I don't maximize my utility over time because we're using the wrong, you know, what happens over time is not what happens in expectation.
So these to me are real problems, logical problems within logical rationality.
It's about taking the expectation of the logarithm instead as with fitness.
It depends on what you just depends on what your dynamics are.
Yeah, I mean, that's an example.
There are a number of papers by biologists who come to a similar conclusion that if animals would maximize something,
it's always, well, what do you maximize?
Then they actually lose on fitness.
Yes, sir.
Thank you again.
That was great.
You can leave it to informal discussions and.
