1
00:00:00,000 --> 00:00:08,520
And the director of Hardin's really great because Gerd is one of the originator of the

2
00:00:08,520 --> 00:00:13,520
theory of ecological rationality at the simple computational models of rules, algorithms

3
00:00:13,520 --> 00:00:17,240
that people use to make decisions in different contexts.

4
00:00:17,240 --> 00:00:22,960
And he's basically introduced complex system science to psychology, cognitive science because

5
00:00:22,960 --> 00:00:28,680
his main thesis is that we cannot understand cognition without looking at its social and

6
00:00:28,680 --> 00:00:34,080
physical environment, only by careful analysis of both the environment and the algorithms

7
00:00:34,080 --> 00:00:36,760
people use to make decisions and solve various problems.

8
00:00:36,760 --> 00:00:40,880
We can understand how people behave and why people behave in different situations.

9
00:00:40,880 --> 00:00:45,640
Gerd is a recipient of numerous awards.

10
00:00:45,640 --> 00:00:51,600
He is member of the Academy of German Academy of Sciences, author of numerous books translated

11
00:00:51,600 --> 00:00:52,600
in many different languages.

12
00:00:52,600 --> 00:00:53,600
And he plays banjo.

13
00:00:53,600 --> 00:01:00,600
So we need to have a closer look at the question.

14
00:01:00,600 --> 00:01:07,200
All right, thank you.

15
00:01:07,200 --> 00:01:10,200
Thanks Mirta.

16
00:01:10,200 --> 00:01:15,280
Today I will talk about how to make good decisions.

17
00:01:15,280 --> 00:01:23,240
If you open a book on rationality in neoclassical economics, behavioral economics, or psychology

18
00:01:23,240 --> 00:01:29,080
of philosophy, you likely encounter the following message.

19
00:01:29,080 --> 00:01:36,120
Good decisions follow the laws of logic, the calculus of Bayesian probability and the maximization

20
00:01:36,120 --> 00:01:39,320
of expected utility.

21
00:01:39,320 --> 00:01:46,840
This is beautiful mathematical theory, but it does not describe how most people actually

22
00:01:46,840 --> 00:01:53,680
make decisions, not even those who write these books.

23
00:01:53,680 --> 00:01:56,120
Let me start with a story.

24
00:01:56,120 --> 00:02:03,120
A professor of decision theory at Columbia University in New York had an offer from a

25
00:02:03,120 --> 00:02:05,400
rival university.

26
00:02:05,400 --> 00:02:07,320
It was Harvard.

27
00:02:07,320 --> 00:02:10,360
And he could not make up his mind.

28
00:02:10,360 --> 00:02:11,360
Should he stay?

29
00:02:11,360 --> 00:02:12,360
Should he leave?

30
00:02:12,360 --> 00:02:13,360
Should he accept?

31
00:02:13,360 --> 00:02:14,360
Reject.

32
00:02:15,040 --> 00:02:20,040
A colleague took him aside and said, what's your problem?

33
00:02:20,040 --> 00:02:24,520
Just maximize your subjective expected utility.

34
00:02:24,520 --> 00:02:28,640
You always write about that.

35
00:02:28,640 --> 00:02:37,960
Exasperated, the professor responded, come on, this is serious.

36
00:02:37,960 --> 00:02:44,520
I'll invite you today in a short tour in our research at the Max Planck Institute for

37
00:02:44,520 --> 00:02:49,640
Human Development on how people actually make decisions.

38
00:02:49,640 --> 00:03:00,760
And I'll start with a key distinction between risk and uncertainty.

39
00:03:00,760 --> 00:03:10,200
A situation of risk is a well-defined, stable, small world where the agents know the exhaustive

40
00:03:10,200 --> 00:03:16,680
and mutually exclusive set of all future states and the exhaustive and mutually exclusive

41
00:03:16,680 --> 00:03:25,920
set of their actions, of the consequences given their actions and states.

42
00:03:25,920 --> 00:03:33,640
Jimmy Savage, who is often referred to as the father of modern Bayesian decision theory,

43
00:03:33,640 --> 00:03:39,800
called this a small world.

44
00:03:39,800 --> 00:03:41,760
There are examples for the small worlds.

45
00:03:41,760 --> 00:03:50,960
If this evening you go to the casino in Santa Fe, if there is one, and play roulette, you

46
00:03:50,960 --> 00:03:53,480
are in a small world.

47
00:03:53,480 --> 00:04:01,400
You know every possible state that can happen, from zero to 36, all the consequences and

48
00:04:01,400 --> 00:04:05,400
probabilities.

49
00:04:05,400 --> 00:04:13,240
The tools for dealing with risk or maximizing expected utility, Bayesian probability updating

50
00:04:13,240 --> 00:04:17,160
and many more.

51
00:04:17,160 --> 00:04:24,280
In a small world, by definition, nothing new can happen.

52
00:04:24,280 --> 00:04:33,320
I had the quote from Fjodor Dostoevsky, who said, in this world, if everyone would be

53
00:04:33,320 --> 00:04:36,560
rational, he said, nothing would happen.

54
00:04:36,560 --> 00:04:43,400
With amazing foresight, he had seen the development of rationality.

55
00:04:43,400 --> 00:04:50,280
Now there are two, most of the problems have to do with, however, with some aspect and some

56
00:04:50,280 --> 00:04:53,080
amount of uncertainty.

57
00:04:53,080 --> 00:05:01,400
Uncertainty is a vast sea of situations where there is no small world where we either cannot

58
00:05:01,400 --> 00:05:07,800
know all future possible states or their consequences.

59
00:05:07,800 --> 00:05:15,200
It's not just about probabilities, it's about the state space.

60
00:05:15,200 --> 00:05:17,680
Researchers have two ways to go.

61
00:05:17,680 --> 00:05:28,360
One is to take an everyday problem, like investment, whom to hire, or whom to marry.

62
00:05:28,360 --> 00:05:36,360
These are all situations of uncertainty because things can happen that you had not anticipated.

63
00:05:36,360 --> 00:05:45,240
Then go one way, reduce it to a small world where everything is known and you can maximize.

64
00:05:45,240 --> 00:05:53,120
That's the way that most theories in neoclassical economics, also behavioral economics, at

65
00:05:53,120 --> 00:05:56,000
least the standards, are going.

66
00:05:56,000 --> 00:06:05,560
The other option is to take uncertainty seriously, face it, and develop and study the tools that

67
00:06:05,560 --> 00:06:11,920
actual people use to make decisions in an uncertain world.

68
00:06:11,920 --> 00:06:16,600
Here by definition, optimization is not possible.

69
00:06:16,600 --> 00:06:26,520
You cannot construct even a subjective probability distribution over a set that you don't know,

70
00:06:26,520 --> 00:06:30,160
that you don't know completely.

71
00:06:30,160 --> 00:06:37,960
I will talk today about one class of tools that are useful under uncertainty, and these

72
00:06:37,960 --> 00:06:41,120
are heuristics.

73
00:06:41,120 --> 00:06:52,360
Heuristics are rules that embody the art to focus on the important and ignore the rest.

74
00:06:52,360 --> 00:07:03,600
Heuristics can lead to good decisions in a world where we cannot forecast the best decisions.

75
00:07:03,600 --> 00:07:10,600
Examples are Herbert Simon's satisfying, Boston Fruity Trees, imitation, agent-based

76
00:07:10,600 --> 00:07:21,680
models who use simple rules like the flocking of starlings, the beautiful models.

77
00:07:21,680 --> 00:07:28,480
I will today go a different way than most of the models are going.

78
00:07:28,480 --> 00:07:35,600
For instance, even behavioral economics, who criticizes neoclassical economics, criticizes

79
00:07:35,600 --> 00:07:45,600
not normative standard, not homo-economics, not the maximization of expected utility,

80
00:07:45,600 --> 00:07:53,160
rather takes a standard to evaluate people, and if there is a discrepancy, the blame is

81
00:07:53,160 --> 00:07:55,760
never on the theory.

82
00:07:55,760 --> 00:07:58,560
It's always on the people.

83
00:07:58,560 --> 00:08:05,480
And then you get the list of biases that are all in your minds, and explain why you all

84
00:08:05,480 --> 00:08:13,200
make these strange decisions, and stubbornly have no insight in your failures.

85
00:08:13,200 --> 00:08:15,120
This is not my message.

86
00:08:15,120 --> 00:08:22,440
I want to study how people make decisions and formalize the heuristics, and then find

87
00:08:22,440 --> 00:08:29,120
out in what situation do they work and where do they not work.

88
00:08:29,120 --> 00:08:33,680
That's the question of ecological rationality.

89
00:08:33,680 --> 00:08:41,760
Rationality is not in the mind, but in the adaptation of mental strategies to certain

90
00:08:41,760 --> 00:08:43,640
environments.

91
00:08:43,640 --> 00:08:50,400
Let me start with an example that makes the difference clear between a model made for

92
00:08:50,400 --> 00:08:58,400
risk and model made for uncertainty.

93
00:08:58,400 --> 00:09:07,240
Every Markowitz got the Nobel Memorial Prize in economics for solving a certain problem.

94
00:09:07,240 --> 00:09:15,640
The problem is you have a number of N assets, and you want to invest your money, and you

95
00:09:15,640 --> 00:09:21,200
want to diversify, not put everything in one pocket, but how?

96
00:09:21,200 --> 00:09:25,040
The answer is the so-called mean variance model.

97
00:09:25,040 --> 00:09:27,440
It's a standard probability model.

98
00:09:27,440 --> 00:09:35,520
You need to estimate all the future returns, their variances, and covariances.

99
00:09:35,520 --> 00:09:42,080
When Harry Markowitz made his own decisions about the time of his retirement, he used

100
00:09:42,080 --> 00:09:45,800
his Nobel Prize winning optimization method.

101
00:09:45,800 --> 00:09:50,680
So we might think, no, he did not.

102
00:09:50,680 --> 00:09:59,840
He used a simple heuristic that we call 1 over N. N is the number of assets or options.

103
00:09:59,840 --> 00:10:04,280
So if it's 2, you do 50-50.

104
00:10:04,280 --> 00:10:09,840
If it's 3, a third, a third, a third, and so on.

105
00:10:09,840 --> 00:10:15,040
Now 1 over N is a heuristic.

106
00:10:15,040 --> 00:10:19,040
The mean variance model is an optimization model.

107
00:10:19,040 --> 00:10:25,200
The mean variance model assumes we are in a world that is stable, where we can estimate

108
00:10:25,200 --> 00:10:31,320
the parameters to some degree of precision.

109
00:10:31,320 --> 00:10:37,280
Number of studies have looked at 1 over N.

110
00:10:37,280 --> 00:10:45,240
And for instance, the study by D. Miguel here has looked at seven real-world investments

111
00:10:45,240 --> 00:10:54,360
and found that in six of the seven cases, 1 over N made more money than Markowitz's optimization

112
00:10:54,360 --> 00:11:00,280
as measured by sharp ratios and similar ones.

113
00:11:00,280 --> 00:11:08,880
The results have been found when exchange-traded funds were tested, ETFs which are close to

114
00:11:08,880 --> 00:11:13,080
1 over N, they're very hard to beat.

115
00:11:13,080 --> 00:11:18,760
Now there is a battle in the literature, which one is better?

116
00:11:18,760 --> 00:11:21,440
Complex optimization or simple heuristic?

117
00:11:21,440 --> 00:11:24,280
This is the wrong question.

118
00:11:24,280 --> 00:11:25,840
None of this is better.

119
00:11:25,840 --> 00:11:28,000
No algorithm is the best.

120
00:11:28,000 --> 00:11:30,520
We need to ask a different question.

121
00:11:30,520 --> 00:11:41,200
So can we identify the environment, the conditions where a simple heuristic like that does better

122
00:11:41,200 --> 00:11:47,760
than another one, another model, and where it's the opposite?

123
00:11:47,760 --> 00:11:54,160
The answer to this question of ecological rationality is not known.

124
00:11:54,280 --> 00:11:56,120
Some of you might figure it out.

125
00:11:56,120 --> 00:11:59,000
Here's some hypothesis.

126
00:11:59,000 --> 00:12:09,080
So if you think in terms of the bias variance decomposition, then 1 over N has probably

127
00:12:09,080 --> 00:12:19,600
a strong bias but makes no error due to variance because it doesn't estimate any parameters.

128
00:12:19,600 --> 00:12:25,680
So the error due to variance means that you get different results depending on what sample

129
00:12:25,680 --> 00:12:28,600
you make your estimates.

130
00:12:28,600 --> 00:12:39,640
So the real question to solve here is when is the bias that 1 over N has larger than

131
00:12:39,640 --> 00:12:50,200
the total error that mean variance makes, that is both bias and variance.

132
00:12:50,200 --> 00:12:54,840
So this is the way I would like to think here.

133
00:12:54,840 --> 00:12:59,000
Excuse me, did Markowitz ever say why he chose 1 over N?

134
00:12:59,000 --> 00:13:00,000
Yes, he did.

135
00:13:00,000 --> 00:13:03,760
He had a psychological explanation.

136
00:13:03,760 --> 00:13:13,200
He said if I would, his choice was between the typical retirement situation, just stocks

137
00:13:13,200 --> 00:13:14,200
or bonds.

138
00:13:14,200 --> 00:13:21,320
If I would put everything in bonds and they would go down, I would feel bad.

139
00:13:21,320 --> 00:13:23,120
If the opposite, I would also feel bad.

140
00:13:23,120 --> 00:13:24,960
I would have regret.

141
00:13:24,960 --> 00:13:33,240
So I just did 50-50 to avoid regret, but it's very different.

142
00:13:33,240 --> 00:13:41,120
He was actually very interested when we and others showed that he wasn't at least where

143
00:13:41,120 --> 00:13:42,120
his experiments are.

144
00:13:42,120 --> 00:13:51,280
It's just stocks, a slightly different situation that was shown that simply can do better.

145
00:13:51,280 --> 00:14:01,440
So the confusion between situations of uncertainty with risk, the idea that every situation of

146
00:14:01,440 --> 00:14:07,960
what uncertainty is one of risk is called the turkey illusion.

147
00:14:07,960 --> 00:14:09,960
So why?

148
00:14:09,960 --> 00:14:13,240
Assume you are a turkey.

149
00:14:13,240 --> 00:14:20,880
It is the first day of your life and a man comes in and you fear he will kill me, but

150
00:14:20,880 --> 00:14:21,880
he feeds you.

151
00:14:21,880 --> 00:14:27,680
The second day of your life, the man comes again, you fear he might kill me, but he feeds

152
00:14:27,680 --> 00:14:28,680
you.

153
00:14:29,320 --> 00:14:31,480
Third day of your life, same thing.

154
00:14:31,480 --> 00:14:38,800
If you do Bayesian updating or any similar model, the probability that the man feeds

155
00:14:38,800 --> 00:14:43,320
you and doesn't kill you gets higher every day.

156
00:14:43,320 --> 00:14:51,200
And on day 100, it is higher than ever before, but it's the day before Thanksgiving and you

157
00:14:51,200 --> 00:14:53,200
are dead meat.

158
00:14:53,200 --> 00:14:57,880
So the turkey was not in a situation of risk.

159
00:14:57,880 --> 00:15:01,600
He missed important information.

160
00:15:01,600 --> 00:15:04,520
There was an option he could not think.

161
00:15:04,520 --> 00:15:13,360
Now the turkey illusion is probably not so often committed by turkeys, but more often

162
00:15:13,360 --> 00:15:15,680
by people.

163
00:15:15,680 --> 00:15:21,360
And here's one example.

164
00:15:21,360 --> 00:15:29,480
In 2003, in his presidential address to the American Economic Association, Robert Lucas,

165
00:15:29,480 --> 00:15:37,040
the macroeconomist, argued that one has learned from earlier depressions and macroeconomics

166
00:15:37,040 --> 00:15:38,880
models take care.

167
00:15:38,880 --> 00:15:44,880
So he said, my thesis in this lecture is that macroeconomics has succeeded.

168
00:15:44,880 --> 00:15:51,720
Its central problem of depression prevention has been solved for all practical purposes.

169
00:15:51,720 --> 00:16:01,560
Now what you get is an illusion of certainty if you apply the models that are based on

170
00:16:01,560 --> 00:16:04,640
assumptions of risk to one of uncertainty.

171
00:16:04,640 --> 00:16:07,640
I've shown you just the illustration here.

172
00:16:07,640 --> 00:16:15,920
In the year 2003, when he gave this talk, the Volateli Index, we are X, went down and

173
00:16:15,920 --> 00:16:16,920
down and down.

174
00:16:16,920 --> 00:16:22,280
So it got better and better and better and better, safer, until shortly before the crisis

175
00:16:22,280 --> 00:16:24,960
hit.

176
00:16:24,960 --> 00:16:30,240
And that's the same thing as the mass modeling models they used.

177
00:16:30,240 --> 00:16:36,760
By the way, the models that Lucas described in his presidential address all assumed a

178
00:16:36,760 --> 00:16:42,640
stable and unchanging structure of the economy.

179
00:16:42,640 --> 00:16:45,600
So what's the alternative?

180
00:16:45,600 --> 00:16:51,000
And here's the research program I and my research group has been following up.

181
00:16:51,000 --> 00:16:52,640
And it has three questions.

182
00:16:52,640 --> 00:16:54,880
The first one is descriptive.

183
00:16:54,880 --> 00:17:03,200
What is in the adaptive toolbox of an individual, an organization, or a species?

184
00:17:03,200 --> 00:17:10,440
So what are the heuristics they use, the cognitive capacities that the heuristics exploit?

185
00:17:10,440 --> 00:17:13,320
And what's the building blocks?

186
00:17:13,320 --> 00:17:22,200
And the goal is to use, to develop algorithmic models of these heuristics, not labels.

187
00:17:22,200 --> 00:17:29,560
We have since the 1970s a tradition in the heuristics and biases program of labels, like

188
00:17:29,560 --> 00:17:34,080
availability representative, nobody knows what exactly they mean.

189
00:17:34,080 --> 00:17:41,160
And they used never to make predictions, but always to explain something after the fact.

190
00:17:41,160 --> 00:17:48,680
The same is with system one and system two, which is just a list of dichotomies.

191
00:17:48,680 --> 00:17:58,520
And the dichotomies is where a science should start and go to good models.

192
00:17:58,520 --> 00:18:05,280
And this is a case where we went from models to dichotomies, to backwards.

193
00:18:05,280 --> 00:18:09,560
Trusky had models of heuristics like elimination bias.

194
00:18:09,560 --> 00:18:18,400
But then after Trusky died, Kahneman sided with this dual system theories that has been

195
00:18:18,480 --> 00:18:22,760
long criticized in psychology before.

196
00:18:22,760 --> 00:18:33,960
Alan Newell said, in a famous paper in 1957, you can't ask 20 questions to nature.

197
00:18:33,960 --> 00:18:42,200
And criticizing the tendency to not do models, but to do dichotomies, serial versus parallel,

198
00:18:42,200 --> 00:18:47,640
unconscious versus conscious, fast versus slow, and so on.

199
00:18:47,640 --> 00:18:53,920
And all this is so intuitive that one forgets that one does science.

200
00:18:53,920 --> 00:18:59,120
So the second question is, no, it's a prescriptive question.

201
00:18:59,120 --> 00:19:01,640
When should we use heuristics?

202
00:19:01,640 --> 00:19:02,480
And when not?

203
00:19:02,480 --> 00:19:04,560
And what heuristic?

204
00:19:04,560 --> 00:19:08,040
And then, certainly, it's heuristics all the way down.

205
00:19:08,040 --> 00:19:09,840
There's no option.

206
00:19:09,840 --> 00:19:11,880
The real question is, what is a smart?

207
00:19:11,880 --> 00:19:15,520
Is a smart to imitate?

208
00:19:15,520 --> 00:19:22,400
The peers, when searching for a research topic, or should I think myself?

209
00:19:22,400 --> 00:19:30,400
So here the question is, can we prove mathematical or at least with computer simulations show

210
00:19:30,400 --> 00:19:36,720
under which condition, for instance, one way in works, where does imitation work, and so on?

211
00:19:36,720 --> 00:19:39,600
And the final question is one of application.

212
00:19:39,600 --> 00:19:45,480
It's doing intuitive design using all these insights.

213
00:19:45,480 --> 00:19:48,920
So I'll start with the adaptive toolbox.

214
00:19:48,920 --> 00:19:55,000
And here is a few on part of the toolbox.

215
00:19:55,000 --> 00:19:58,840
There are core capacities that humans have.

216
00:19:58,840 --> 00:20:02,880
On that basis, the heuristics can be simple.

217
00:20:02,880 --> 00:20:12,440
The core capacities are quite complex, meaning it's very hard to build a brain that can't do that.

218
00:20:12,440 --> 00:20:13,840
Let me start.

219
00:20:13,840 --> 00:20:18,800
So there are, I can only give you some examples today.

220
00:20:18,800 --> 00:20:27,560
And on the left side, there are social heuristics like imitation, word of mouse, which are highly powerful.

221
00:20:27,560 --> 00:20:30,080
There would be no culture without imitation.

222
00:20:30,080 --> 00:20:39,000
Mike Tomasella has shown in his beautiful research that children, human children, imitate much more precise

223
00:20:39,000 --> 00:20:43,040
and much more general than primate infants.

224
00:20:43,040 --> 00:20:50,680
And it's one of our big advantages that builds culture that one doesn't have to learn from scratch all the time.

225
00:20:50,680 --> 00:20:55,320
It's also one of our greatest dangers.

226
00:20:55,320 --> 00:20:59,760
So let me start with recognition.

227
00:20:59,760 --> 00:21:04,160
Recognition is a fundamental ability of the human mind.

228
00:21:04,160 --> 00:21:06,240
We can recognize faces.

229
00:21:06,240 --> 00:21:09,200
We can recognize names.

230
00:21:09,200 --> 00:21:13,080
And recognition is different from recall.

231
00:21:13,080 --> 00:21:18,200
If you cannot recall the name of a good friend, that happens.

232
00:21:18,200 --> 00:21:24,280
Particularly when you get older or when you're a jet lag like me at this moment.

233
00:21:24,920 --> 00:21:33,080
But if you do not recognize the name of a good friend, then you're a clinical case.

234
00:21:33,080 --> 00:21:46,480
So the recognition heuristic exploits this capability to make inferences in situations where you don't know much.

235
00:21:46,480 --> 00:21:49,120
Let me do a demonstration with you.

236
00:21:49,120 --> 00:21:51,120
Are you ready?

237
00:21:51,120 --> 00:22:03,040
Okay, assume you are in a TV show called Who Wants to be a Millionaire?

238
00:22:03,040 --> 00:22:04,280
And you made it through the end.

239
00:22:04,280 --> 00:22:08,040
And here is the million dollar question.

240
00:22:08,040 --> 00:22:10,840
Which US city has more inhabitants?

241
00:22:10,840 --> 00:22:13,120
Detroit or Milwaukee?

242
00:22:13,120 --> 00:22:14,480
Time is ticking.

243
00:22:14,480 --> 00:22:16,560
Who of you thinks it's Detroit?

244
00:22:16,560 --> 00:22:18,560
Hands up.

245
00:22:18,560 --> 00:22:21,600
Who of you thinks it's Milwaukee?

246
00:22:21,600 --> 00:22:24,400
Hands up.

247
00:22:24,400 --> 00:22:29,440
A rough estimate, 60% of you got it right.

248
00:22:29,440 --> 00:22:33,280
It's Detroit.

249
00:22:33,280 --> 00:22:43,480
When we did experiments with undergraduates at the University of Chicago, who think there is more or less in the country,

250
00:22:43,480 --> 00:22:45,480
we got the same result.

251
00:22:45,480 --> 00:22:47,200
60%.

252
00:22:47,200 --> 00:22:55,760
And then Dane Goldstein and I asked students in Germany, what do you think?

253
00:22:55,760 --> 00:23:02,640
So not just this one question, but all questions about large American cities.

254
00:23:02,640 --> 00:23:04,040
What do you think?

255
00:23:04,040 --> 00:23:08,920
How many Germans got the right?

256
00:23:08,920 --> 00:23:11,760
More than 60.

257
00:23:11,760 --> 00:23:13,040
More than 60.

258
00:23:13,040 --> 00:23:14,680
And why do you think that?

259
00:23:14,680 --> 00:23:16,720
Because otherwise I wouldn't ask you.

260
00:23:16,720 --> 00:23:20,520
They relied on the heuristic of recognition.

261
00:23:20,520 --> 00:23:21,240
Right, yeah.

262
00:23:21,240 --> 00:23:23,400
And trust that a lot too.

263
00:23:23,400 --> 00:23:24,720
You're right.

264
00:23:24,720 --> 00:23:30,320
Or this is when you give a talk with two people who have already your work.

265
00:23:30,320 --> 00:23:31,560
OK.

266
00:23:31,560 --> 00:23:37,040
The point is, many of the Germans have never heard of Milwaukee.

267
00:23:37,040 --> 00:23:42,040
And the recognition heuristic tells you, choose the option you recognize.

268
00:23:42,040 --> 00:23:44,040
And they were right.

269
00:23:44,040 --> 00:23:47,080
Now, let me test you the other way around.

270
00:23:47,080 --> 00:23:48,680
OK, ready?

271
00:23:48,680 --> 00:23:52,360
Which German city has more inhabitants?

272
00:23:52,360 --> 00:23:54,680
Bielefeld or Hannover?

273
00:23:54,680 --> 00:23:57,440
Who is for Bielefeld?

274
00:23:57,440 --> 00:23:58,480
You.

275
00:23:58,480 --> 00:24:00,400
Who is for Hannover?

276
00:24:00,400 --> 00:24:02,360
You got it right.

277
00:24:02,360 --> 00:24:06,760
I think the two suspected that I want to fool them.

278
00:24:06,760 --> 00:24:10,840
You shouldn't think too much.

279
00:24:10,840 --> 00:24:16,000
If I ask the same question to Germans, many of them get it wrong.

280
00:24:16,000 --> 00:24:17,440
Because they had heard of both.

281
00:24:17,440 --> 00:24:21,600
And they need to retrieve knowledge about that.

282
00:24:21,600 --> 00:24:22,920
See the point?

283
00:24:22,920 --> 00:24:25,960
And we had just a less-is-more effect here.

284
00:24:25,960 --> 00:24:33,840
You got more questions right about the German cities than about America.

285
00:24:33,840 --> 00:24:37,040
Although you know more about the Americans than the Germans.

286
00:24:37,040 --> 00:24:38,880
So how does this work?

287
00:24:38,880 --> 00:24:41,800
So here's the study of ecological rationality.

288
00:24:41,800 --> 00:24:45,240
So think you have a test of a hundred of these questions.

289
00:24:45,240 --> 00:24:48,160
Maybe the hundred largest cities in the US.

290
00:24:48,160 --> 00:24:52,280
And consider the lowest coefficient.

291
00:24:52,280 --> 00:24:59,520
If you haven't heard of any of the cities, your performance is 50%.

292
00:24:59,520 --> 00:25:01,440
Yeah, you just guess.

293
00:25:01,440 --> 00:25:06,320
If you've heard of all of them on the other side, it's also 50%.

294
00:25:06,320 --> 00:25:09,160
Because you cannot use the recognition tool.

295
00:25:09,160 --> 00:25:11,880
You're seeing both cases that have no knowledge.

296
00:25:11,880 --> 00:25:15,320
Now assume the person has knowledge.

297
00:25:15,320 --> 00:25:17,960
And the knowledge is defined here.

298
00:25:17,960 --> 00:25:24,840
It's defined as the number of answers you get right in cases where you've heard of both.

299
00:25:24,840 --> 00:25:26,840
Like before, it was 60%.

300
00:25:26,840 --> 00:25:32,120
And actually, the curve here, the second curve, is about knowledge relative of 60%.

301
00:25:32,120 --> 00:25:40,280
Person B recognizes all American cities like you and gets 60% right.

302
00:25:40,280 --> 00:25:42,520
This is knowledge.

303
00:25:42,520 --> 00:25:46,800
Person A recognizes only half of them.

304
00:25:46,800 --> 00:25:52,080
Can apply the recognition here and gets more right.

305
00:25:52,080 --> 00:25:58,520
The curves are drawn for recognition validity of 0.8.

306
00:25:58,520 --> 00:26:04,760
Meaning that the number of correct inferences in all cases,

307
00:26:04,760 --> 00:26:09,240
where you have recognized one and not the other.

308
00:26:09,240 --> 00:26:11,120
So you can measure all these things.

309
00:26:11,120 --> 00:26:14,680
There's no parameter fitting game here.

310
00:26:14,680 --> 00:26:25,280
And if the knowledge validity is at least as big as the recognition validity,

311
00:26:25,280 --> 00:26:28,600
there is no lesses more effect in it.

312
00:26:28,600 --> 00:26:33,240
So the lesses more effect you find on this three curves below,

313
00:26:33,240 --> 00:26:38,960
it means that on the right side of the curve, performance goes down,

314
00:26:38,960 --> 00:26:42,560
despite you either no more or recognize more.

315
00:26:42,560 --> 00:26:50,960
So this is an illustration how we can model and can actually make quantitative predictions.

316
00:26:50,960 --> 00:26:58,560
And we have used this, for instance, in predicting maybe more interesting things in cities,

317
00:26:58,560 --> 00:27:01,880
unless you are on a million dollar question.

318
00:27:01,880 --> 00:27:13,000
Like you can how to predict the outcomes of all Wimbledon gentlemen's single games.

319
00:27:13,000 --> 00:27:19,160
There are 128 contestants, there are 127 games.

320
00:27:19,200 --> 00:27:21,000
And you make a prediction for everyone.

321
00:27:21,000 --> 00:27:28,240
And you can do this with the ADP rankings or with the Wimbledon seedings or with ignorant people.

322
00:27:28,240 --> 00:27:33,880
Exactly partially ignorant, because fully ignorant cannot use the heuristics.

323
00:27:33,880 --> 00:27:39,040
And again, from here you can see, you need to find people that have heard about half of them.

324
00:27:39,040 --> 00:27:40,480
And not if there was one.

325
00:27:40,480 --> 00:27:47,040
And these were German amateur players and they made in two studies

326
00:27:47,040 --> 00:27:54,160
more correct predictions than the ADP rankings or the Wimbledon experts.

327
00:27:54,160 --> 00:28:03,480
And if you change that to those who recognize less, then they will not make more predictions.

328
00:28:03,480 --> 00:28:09,560
So this is just an example how one can analyze the ecological rationality of a heuristic.

329
00:28:09,560 --> 00:28:13,320
You can easily tell when the heuristic is worthless,

330
00:28:13,320 --> 00:28:19,760
when the recognition validity is a change level, that is noting.

331
00:28:19,760 --> 00:28:29,000
And we also have a way to make earlier concepts like availability into something precise.

332
00:28:29,000 --> 00:28:36,560
That is useful and also illustrates some of the amazing capabilities of the human mind

333
00:28:36,560 --> 00:28:41,440
to exploit one's own ignorance.

334
00:28:41,440 --> 00:28:44,840
Second example, fluency.

335
00:28:44,840 --> 00:28:49,320
Fluency presumes recognition.

336
00:28:49,320 --> 00:28:57,520
And it can be measured by the time an option comes to your mind.

337
00:28:57,520 --> 00:29:11,400
And the point I'm making with the next example is that a human intuition using fluency,

338
00:29:12,360 --> 00:29:16,720
creates no speed accuracy trade-off.

339
00:29:16,720 --> 00:29:18,920
What is the speed accuracy trade-off?

340
00:29:18,920 --> 00:29:25,640
That's usually something that if you make judgments fast, then you lose on accuracy.

341
00:29:25,640 --> 00:29:31,560
That's the story about fast versus slow thinking fast versus slow and so on.

342
00:29:31,560 --> 00:29:39,560
A speed accuracy trade-off, you get them if you test undergraduates with problems I've never heard.

343
00:29:39,560 --> 00:29:47,360
But if you test experts, it's the opposite.

344
00:29:47,360 --> 00:29:50,480
So here's the example.

345
00:29:50,480 --> 00:30:04,600
The example is there are experienced handball players and some of my postdocs put them in

346
00:30:04,600 --> 00:30:08,760
the uniform with a ball in the hand in front of a video.

347
00:30:08,760 --> 00:30:14,800
And the instruction was there will be a top handball game, running for 10 seconds, then

348
00:30:14,800 --> 00:30:21,960
it will be frozen, and tell me immediately what the player with the ball should do.

349
00:30:21,960 --> 00:30:27,880
So they looked at that, it was frozen, and they would say I'll pass to the left here

350
00:30:27,880 --> 00:30:31,560
or shoot at the goal or something else.

351
00:30:31,560 --> 00:30:42,160
Then they had 45 seconds more time to study the still frozen image carefully.

352
00:30:42,160 --> 00:30:48,760
And they were instructed, if other options come to your mind, please tell us.

353
00:30:48,760 --> 00:30:53,080
And then one of them said, oh, I didn't see the guy on the right side, passed, that would

354
00:30:53,080 --> 00:30:54,760
be a good one.

355
00:30:54,760 --> 00:31:04,360
And after all of this was done, the options were plotted against the quality, as measured

356
00:31:04,360 --> 00:31:08,440
by the best coaches we have in the country.

357
00:31:08,440 --> 00:31:10,640
And what you see is very interesting.

358
00:31:10,640 --> 00:31:14,560
On average, the first option that comes to your mind is the best one, the second is the

359
00:31:14,560 --> 00:31:17,960
second best, the third is the, and so on.

360
00:31:17,960 --> 00:31:26,560
That means if an experienced player goes with the first option, he or she is most likely

361
00:31:26,560 --> 00:31:28,760
right.

362
00:31:28,760 --> 00:31:34,320
And we also found that when we asked them at the end of the 45 seconds they had studied,

363
00:31:34,320 --> 00:31:41,080
what would you do now, about 40% change their mind, and got into a worse option.

364
00:31:41,080 --> 00:31:46,600
So here we have a case where we have no speed, accuracy, trade-off.

365
00:31:46,600 --> 00:31:54,040
Country, it's better, on average, to act on your first impulse.

366
00:31:54,040 --> 00:31:56,960
In a game you have to do this.

367
00:31:56,960 --> 00:32:03,240
But in the experiment, you have to change the thing longer, but it doesn't help.

368
00:32:03,240 --> 00:32:05,280
It regularly hurts.

369
00:32:05,280 --> 00:32:08,560
Remember, this only works for experts.

370
00:32:08,560 --> 00:32:14,960
If you have amateur players, the curves, they're not going down, they're just stator.

371
00:32:14,960 --> 00:32:19,600
So this has an important insight.

372
00:32:19,600 --> 00:32:29,120
So intuition, as I define it, is based on years of experience, what you want to do comes

373
00:32:29,120 --> 00:32:32,760
quickly to your mind, and you don't know why.

374
00:32:32,760 --> 00:32:38,480
And the fluency heuristic is an explanation how this works.

375
00:32:39,400 --> 00:32:45,720
The reason why the research has come to the conclusion that what the general speed, accuracy,

376
00:32:45,720 --> 00:32:53,200
trade-off is mainly because one studies undergraduates with problems they've never seen before.

377
00:32:53,200 --> 00:33:01,120
Gary Klein, who studies fire furthers, finds the same results.

378
00:33:01,120 --> 00:33:07,520
So the third and last category, I want to give you examples, is an interesting one,

379
00:33:07,520 --> 00:33:19,840
where it is about the human capacity to order things in importance, and these are heuristics

380
00:33:19,840 --> 00:33:26,960
that are based, that base the decision on a single cue.

381
00:33:26,960 --> 00:33:32,120
And let's give you an example about hiring.

382
00:33:32,120 --> 00:33:41,160
When Elon Musk was still young, and Tesla was young, Elon Musk did the hiring himself,

383
00:33:41,160 --> 00:33:44,280
and he reported how he did it.

384
00:33:44,280 --> 00:33:47,920
He did not get an assessment center.

385
00:33:47,920 --> 00:33:55,720
He did not study CVs, but he, according to himself, he relied only on a single cue.

386
00:33:55,720 --> 00:34:05,520
Just a person possesses some exceptional ability, like a banjo player.

387
00:34:05,520 --> 00:34:11,400
And if not, no hire, if yes, hire.

388
00:34:11,400 --> 00:34:15,920
Now you might wonder why not more cues.

389
00:34:15,920 --> 00:34:25,360
But look, exceptional ability is something that has some kind of redundancy with other

390
00:34:25,360 --> 00:34:26,640
things.

391
00:34:26,640 --> 00:34:34,720
So a person who has this to get there needs to know how to persevere, how to concentrate,

392
00:34:34,720 --> 00:34:39,320
how to sweat, and not give up.

393
00:34:39,320 --> 00:34:46,000
And so that's the spirit behind that.

394
00:34:46,000 --> 00:34:54,480
It also illustrates that these heuristics are adapted to certain problems that will not

395
00:34:54,480 --> 00:34:59,160
work well when the company, like Tesla, had grown.

396
00:34:59,160 --> 00:35:03,360
Because then you need people who do routine work.

397
00:35:03,360 --> 00:35:08,640
One needs to adapt that, and that's the idea in the adaptive toolbox.

398
00:35:08,640 --> 00:35:20,760
Now here is another hiring heuristic, and that's what we call a fast and frugal tree.

399
00:35:20,760 --> 00:35:28,240
A fast and frugal tree is an incomplete tree that has an exit on each of the questions

400
00:35:28,240 --> 00:35:30,400
or cues.

401
00:35:30,400 --> 00:35:37,000
So if the number of cues or questions are n, it has n plus 1 exits, while a complete

402
00:35:37,000 --> 00:35:42,760
tree is in the case of dichotomous 2 to the n.

403
00:35:42,760 --> 00:35:49,280
So when Abbason was young, a Jeff Bezos also reported about how he did make the description,

404
00:35:49,280 --> 00:35:53,920
and we reconstructed that in the form of a fast and frugal tree.

405
00:35:53,920 --> 00:35:56,040
So note it's sequential.

406
00:35:56,040 --> 00:36:01,600
It doesn't integrate as rational theories usually assume.

407
00:36:01,600 --> 00:36:05,720
Interestingly, his first question was the same.

408
00:36:05,720 --> 00:36:08,440
So does the person have an exceptional ability?

409
00:36:08,440 --> 00:36:11,040
But if yes, it wasn't enough.

410
00:36:11,040 --> 00:36:17,080
Second question, will you admire this person, which is a quite unusual question?

411
00:36:17,800 --> 00:36:25,440
We explained that if he, Bezos, admires a person, he will learn from that person.

412
00:36:25,440 --> 00:36:33,840
But that was also not enough, and only if that person raises the average level in the

413
00:36:33,840 --> 00:36:38,440
unit where he or she will be, that's being hired.

414
00:36:38,440 --> 00:36:46,480
So the properties of these trees are of a lexicographic nature.

415
00:36:46,480 --> 00:36:55,000
A lexicographic strategist cannot be mapped into a utility, into a smooth utility function.

416
00:36:55,000 --> 00:37:02,560
And that's why classical books like Luce and Wreifer have always ignored them and looked

417
00:37:02,560 --> 00:37:04,600
down at them.

418
00:37:04,600 --> 00:37:11,280
But people use these strategies, and we have shown that there are certain situations where

419
00:37:11,280 --> 00:37:15,840
a fast and frugal tree can outperform random forests.

420
00:37:15,840 --> 00:37:20,720
And the interesting question is, again, can we prove this situation?

421
00:37:20,720 --> 00:37:25,000
Can we identify them where that works?

422
00:37:25,000 --> 00:37:31,680
So I'll show you one way to, about how to model such a tree.

423
00:37:31,680 --> 00:37:42,080
If you use these three questions in the order, how many trees are possible by changing the

424
00:37:42,080 --> 00:37:43,080
exits?

425
00:37:43,080 --> 00:37:44,080
Sorry.

426
00:37:44,080 --> 00:37:45,080
Four.

427
00:37:45,080 --> 00:37:46,080
Oh.

428
00:37:46,080 --> 00:37:47,080
It's four.

429
00:37:47,080 --> 00:37:49,600
You tell me later, the other two.

430
00:37:49,600 --> 00:37:50,600
The same and changing.

431
00:37:50,600 --> 00:37:51,600
Yeah, the same, yeah?

432
00:37:51,600 --> 00:37:52,600
The same cues in the same order, yeah?

433
00:37:52,600 --> 00:37:53,600
Otherwise, it would be more.

434
00:37:53,600 --> 00:37:54,600
Correct.

435
00:37:54,600 --> 00:38:00,720
Bezos tree is on the left side.

436
00:38:00,720 --> 00:38:07,520
So any classification can make one of two errors, like making an offer to a person whom

437
00:38:07,520 --> 00:38:13,840
you should not have made an offer that's a false positive, or not making an offer to

438
00:38:13,840 --> 00:38:17,440
a person whom you should, that's a miss.

439
00:38:17,440 --> 00:38:23,360
The Bezos tree minimizes false positives, yeah?

440
00:38:23,360 --> 00:38:27,840
He only makes an offer after long, yeah?

441
00:38:27,840 --> 00:38:34,520
These four trees do a balance between false positives and misses.

442
00:38:34,520 --> 00:38:42,520
On the right side, you have a tree that maximizes false positives relative to miss.

443
00:38:43,520 --> 00:38:51,960
And in the middle, you have very interesting trees that have a zigzag structure.

444
00:38:51,960 --> 00:38:58,640
An example for a tree on the right side, I'll show you from our work with the Bank

445
00:38:58,640 --> 00:38:59,640
of England.

446
00:38:59,640 --> 00:39:06,640
So the Bank of England has a problem how to identify vulnerable banks.

447
00:39:06,640 --> 00:39:08,920
How do you do that?

448
00:39:08,920 --> 00:39:19,680
Now the classical method is in the so-called Basel or Basel two and three programs, and

449
00:39:19,680 --> 00:39:25,280
where the banks calculate a value at risk.

450
00:39:25,280 --> 00:39:32,680
And if you've ever done that, so if you are the CEO of a large bank, you have to estimate

451
00:39:32,680 --> 00:39:34,760
thousands of risk factors.

452
00:39:34,760 --> 00:39:38,120
That means a covariance matrix in the order of millions.

453
00:39:38,120 --> 00:39:44,520
And you can't use more than five, ten years of data because before something strange

454
00:39:44,520 --> 00:39:49,160
would happen.

455
00:39:49,160 --> 00:39:52,200
These calculations are impressive.

456
00:39:52,200 --> 00:39:58,920
They have prevented, they've never prevented any crisis and probably will never prevent

457
00:39:58,920 --> 00:40:00,560
any crisis.

458
00:40:00,560 --> 00:40:03,520
It's another Turkey illusion.

459
00:40:03,520 --> 00:40:09,920
This standard probability theory applied to the situation of uncertainty.

460
00:40:09,920 --> 00:40:20,520
I once gave a talk to the European Central Bank and said, the calculation that you do

461
00:40:20,520 --> 00:40:24,800
border on astrology.

462
00:40:24,800 --> 00:40:32,440
And I was waiting for someone to say, no, the answer was, yes, but what else should

463
00:40:32,440 --> 00:40:34,680
we do?

464
00:40:34,680 --> 00:40:39,400
Here is what else could be done and systemically be studied.

465
00:40:39,400 --> 00:40:41,280
So this is now a fast and frugal tree.

466
00:40:41,280 --> 00:40:44,520
That's the same structure as the one that's seen before.

467
00:40:44,520 --> 00:40:52,800
And it asks just three questions, for instance, leverage ratio below a certain percentage

468
00:40:52,800 --> 00:40:57,040
and if it's lower, red flag.

469
00:40:57,040 --> 00:41:01,000
If not, it goes on and asks two more questions.

470
00:41:01,000 --> 00:41:05,480
I'm not going into the details here, but you see the structure.

471
00:41:05,480 --> 00:41:11,200
It's important that it works very different from standard rational models.

472
00:41:11,200 --> 00:41:20,520
So for instance, the Swiss bank, UBS, had failed before the crisis on the first item.

473
00:41:20,520 --> 00:41:26,360
So the leverage ratio was much less than half of that.

474
00:41:26,360 --> 00:41:29,880
That would have identified it immediately.

475
00:41:29,880 --> 00:41:40,680
UBS did marvelous on the other criteria, but the trees are non-compensatory.

476
00:41:40,680 --> 00:41:43,520
It's like the human body.

477
00:41:43,520 --> 00:41:46,920
If your heart fails, a good lung doesn't help you.

478
00:41:46,920 --> 00:41:48,920
You can't compensate that.

479
00:41:48,920 --> 00:41:51,560
So it's a very different model.

480
00:41:51,560 --> 00:41:56,920
And the reason why this type of strategy cannot be mapped into a smooth utility function,

481
00:41:56,920 --> 00:41:59,840
who cares about that?

482
00:41:59,840 --> 00:42:08,840
It has to work and not to follow some mathematical doctrine.

483
00:42:08,840 --> 00:42:16,880
Again importantly, it reflects sequential thinking and has a few other advantages.

484
00:42:16,880 --> 00:42:19,360
Bankers can understand that.

485
00:42:19,360 --> 00:42:28,560
And also, the banks, the regulators are more safe that the banks cannot game the system.

486
00:42:28,560 --> 00:42:34,440
If you have to estimate millions of co-variants, you have plenty of room to game.

487
00:42:34,440 --> 00:42:37,320
Here, it's much easier.

488
00:42:37,320 --> 00:42:42,280
It's much more difficult to game the system.

489
00:42:42,280 --> 00:42:46,160
So can I just ask you something unconfused?

490
00:42:46,160 --> 00:42:52,720
So this is a risk-minimizing structure that can be calculated based on known states.

491
00:42:52,720 --> 00:42:59,400
Earlier, you started by talking about heuristics as being solutions to the uncertainty problem.

492
00:42:59,400 --> 00:43:02,960
This looks like a risk problem.

493
00:43:02,960 --> 00:43:08,080
The uncertainty is in what banks are doing.

494
00:43:08,080 --> 00:43:13,280
So the model is a heuristic.

495
00:43:13,280 --> 00:43:17,360
It's not an optimizing model.

496
00:43:17,360 --> 00:43:22,440
And your right, it goes, it's the rightmost tree.

497
00:43:22,440 --> 00:43:26,280
It tries to minimize the misses.

498
00:43:26,280 --> 00:43:28,400
You want not to overseas.

499
00:43:28,400 --> 00:43:31,240
And the costs are false alarms.

500
00:43:31,240 --> 00:43:34,040
That's what the decision is.

501
00:43:34,040 --> 00:43:41,760
How we build the trees is a mixture between, so the economists at the Bank of England,

502
00:43:41,760 --> 00:43:48,680
they identify the variables on base and we check them down on empirical data.

503
00:43:48,680 --> 00:43:53,560
So for instance, the cutoff points, it's all empirical data.

504
00:43:53,560 --> 00:43:55,520
We deliver the structure.

505
00:43:55,520 --> 00:43:58,640
Now, don't put this in a regression.

506
00:43:58,640 --> 00:44:00,120
Think about this structure.

507
00:44:00,120 --> 00:44:04,240
That's the way this is being done.

508
00:44:04,240 --> 00:44:07,760
And then it needs to be tested how well it works.

509
00:44:07,760 --> 00:44:11,440
The advantage is transparency.

510
00:44:11,440 --> 00:44:19,800
That it can easily be understood and it can be changed in cases there is a problem.

511
00:44:19,800 --> 00:44:24,320
And it's an alternative that can be systematically studied.

512
00:44:24,320 --> 00:44:25,320
Yeah.

513
00:44:25,320 --> 00:44:33,800
Well, I'm not going to, the states are still all nine points, but the state outside of the

514
00:44:33,800 --> 00:44:35,880
system or the framework.

515
00:44:35,880 --> 00:44:41,400
Whereas when you had the opening definition of risk and uncertainty, there's something

516
00:44:41,400 --> 00:44:45,560
about the small world about these states.

517
00:44:45,560 --> 00:44:47,560
It's a good question.

518
00:44:47,560 --> 00:44:58,480
So the world of finance here has incredibly and also unknown uncertainties.

519
00:44:58,480 --> 00:45:04,520
So the numbers that the Bank of England gets from the banks, you can count that they're

520
00:45:04,520 --> 00:45:08,720
not the real numbers, they're polished.

521
00:45:09,280 --> 00:45:15,840
The other source of uncertainty is in what's actually happening in the financial systems.

522
00:45:15,840 --> 00:45:19,440
And there could be another Russian default or something.

523
00:45:19,440 --> 00:45:21,680
This is the uncertainty I mean.

524
00:45:21,680 --> 00:45:28,200
And this is a model how to deal with that uncertainty.

525
00:45:28,200 --> 00:45:33,320
And there's no claim about optimization or maximization for anything.

526
00:45:33,320 --> 00:45:37,520
It's just an attempt to do better than what one has.

527
00:45:37,880 --> 00:45:49,720
Here is a book we've done, which is how to build these trees from empirical data.

528
00:45:49,720 --> 00:45:53,440
And what's the alternative in case that's of interest.

529
00:45:53,440 --> 00:45:59,000
But the uncertainty in the financial system is enormous.

530
00:45:59,000 --> 00:46:02,640
Is that, sorry, let's take it a little bit.

531
00:46:02,760 --> 00:46:09,920
So I was sympathetic to your critique of the dichotomizing approach that you've adopted

532
00:46:09,920 --> 00:46:15,200
it yourself in the heuristics versus the optimizations.

533
00:46:15,200 --> 00:46:21,600
And is it possible actually, given the observation that we just made, in fact, what you're calling

534
00:46:21,600 --> 00:46:26,240
a heuristic is just on the spectrum towards full optimization.

535
00:46:26,240 --> 00:46:27,240
Yeah.

536
00:46:27,240 --> 00:46:28,240
Yes.

537
00:46:28,240 --> 00:46:29,240
Okay.

538
00:46:29,240 --> 00:46:30,240
Yeah.

539
00:46:30,240 --> 00:46:36,480
I mean, if you do Bayesian, they're Bayesian usual optimization model, then you start with

540
00:46:36,480 --> 00:46:37,480
uniform priors.

541
00:46:37,480 --> 00:46:38,480
That's a heuristic.

542
00:46:38,480 --> 00:46:41,440
You're totally right.

543
00:46:41,440 --> 00:46:46,480
It is, I just make it simple.

544
00:46:46,480 --> 00:46:51,320
But it's very clear that one should pay attention, I think, to the heuristics.

545
00:46:51,320 --> 00:46:52,320
Yeah?

546
00:46:52,320 --> 00:46:55,920
There's a bit of a category error here.

547
00:46:55,920 --> 00:46:57,800
You actually mentioned ROC curves.

548
00:46:57,800 --> 00:47:06,000
The whole point of ROC curves is, which are middle of standard machine learning, is that

549
00:47:06,000 --> 00:47:07,000
they are an alternative.

550
00:47:07,000 --> 00:47:08,840
You don't come up with one.

551
00:47:08,840 --> 00:47:12,160
You set your own level on an ROC curve.

552
00:47:12,160 --> 00:47:17,680
And you're simply saying, go on ROC curve in a way that you're less likely to screw

553
00:47:17,680 --> 00:47:18,680
up.

554
00:47:18,680 --> 00:47:19,680
Sure.

555
00:47:19,680 --> 00:47:23,840
That makes perfect sense.

556
00:47:23,840 --> 00:47:29,160
But I don't think it's really got to do with optimization versus heuristics, just using

557
00:47:29,160 --> 00:47:34,360
an ROC curve over, and the points that David is making in the fellow over there, is it's

558
00:47:34,360 --> 00:47:41,000
a very high level coarse grain space as opposed to these banks are being stupid and are using

559
00:47:41,000 --> 00:47:43,960
a fine grain space where you can't measure the variables.

560
00:47:43,960 --> 00:47:49,160
But the point is, is an ROC curve doesn't give you a single answer.

561
00:47:49,160 --> 00:47:53,360
You're putting in that I don't like the chance of screwing up.

562
00:47:53,360 --> 00:47:59,480
Therefore, I'm going to set my threshold in the ROC curve and go over to this side.

563
00:47:59,480 --> 00:48:00,880
Okay.

564
00:48:00,880 --> 00:48:06,880
What you picked this point here, the one-game map, the fast and frugal degrees onto an ROC

565
00:48:06,880 --> 00:48:08,400
curve, onto four points.

566
00:48:08,400 --> 00:48:13,560
And that's a direct connection between an optimization model.

567
00:48:13,560 --> 00:48:19,840
Then the question is, whether the optimization model helps you for a certain problem.

568
00:48:19,840 --> 00:48:28,960
The name and Pearson theory, which is below the foundation of the ROC curves, makes a

569
00:48:28,960 --> 00:48:35,480
number of assumptions about the distributions which are not necessary here.

570
00:48:35,480 --> 00:48:47,440
I see a point here, and also it's difficult for the ROC curves to deal with more than

571
00:48:47,440 --> 00:48:48,440
one variable.

572
00:48:48,440 --> 00:48:52,200
Can be done, gets very complex.

573
00:48:52,200 --> 00:48:57,080
Here it's relatively easy to deal with them.

574
00:48:57,080 --> 00:49:02,080
So there are differences between these two curves, but you also write and we showed that

575
00:49:02,080 --> 00:49:13,320
in this paper here, that the heuristic can be mapped into an optimization model, right?

576
00:49:13,320 --> 00:49:18,600
So I'll end with a few remarks about ecological rationality.

577
00:49:18,600 --> 00:49:27,360
So the idea of ecological rationality is from Herbert Simon, and that's a famous quote from

578
00:49:27,360 --> 00:49:31,160
him where he says, this is a analogy.

579
00:49:31,160 --> 00:49:36,640
It basically says, in order to evaluate the rationality of behavior, you need to look

580
00:49:36,640 --> 00:49:43,080
both at cognition, the strategies, the heuristics, anything else, and the environment, and how

581
00:49:43,080 --> 00:49:45,520
they match.

582
00:49:45,520 --> 00:49:58,680
It also means the standard definition of rationality is following the laws, say, axioms of consistency

583
00:49:58,680 --> 00:50:06,760
or maximization of expected utility is an internal definition.

584
00:50:06,760 --> 00:50:12,560
It doesn't take account how these things work in the world outside.

585
00:50:12,560 --> 00:50:20,800
And for instance, that part of behavioral economics, who has adopted the heuristics

586
00:50:20,800 --> 00:50:25,360
and biases program, uses a single internal definition.

587
00:50:25,360 --> 00:50:31,400
There is to be a law of logic or probability, and people are measured against that.

588
00:50:31,400 --> 00:50:38,520
And what I'm arguing, we need to measure behavior against the real world, against the

589
00:50:38,520 --> 00:50:43,360
measure heuristic against the real world or the success.

590
00:50:43,360 --> 00:50:52,680
What I'll show you now at the end is just a simple and intuitive answer to the question,

591
00:50:52,680 --> 00:50:57,400
can we, I've shown you a few heuristics that just rely on one reason.

592
00:50:57,400 --> 00:51:07,200
Can we identify the conditions under which relying on one reason cannot be beaten by

593
00:51:07,200 --> 00:51:13,560
a linear equation that has more reasons, including the one reason?

594
00:51:13,560 --> 00:51:16,920
Clear?

595
00:51:16,920 --> 00:51:24,800
So do you have an idea?

596
00:51:24,800 --> 00:51:29,920
I'm just asking you before, because afterwards it's so evident that we knew it all along.

597
00:51:29,920 --> 00:51:33,680
To be clear, it's a subset of that larger space, right?

598
00:51:33,680 --> 00:51:36,520
Yeah, it's a subset.

599
00:51:36,520 --> 00:51:38,440
So think about it.

600
00:51:38,440 --> 00:51:46,320
A binary decision, higher or not higher, there is Elon Musk has one reason.

601
00:51:46,320 --> 00:51:55,560
The question is, under what conditions will relying on this one reason always lead to

602
00:51:55,560 --> 00:52:06,640
the same results that a linear model that includes many valid reasons, they're all valid,

603
00:52:06,640 --> 00:52:07,640
including the one.

604
00:52:07,640 --> 00:52:08,640
Clear?

605
00:52:08,640 --> 00:52:09,640
It's a subset.

606
00:52:09,640 --> 00:52:10,640
Yeah?

607
00:52:11,640 --> 00:52:17,600
Here's a simple condition, and it's if there is a dominant queue.

608
00:52:17,600 --> 00:52:25,080
But it means that the weights of binary queues form a dominant queue structure.

609
00:52:25,080 --> 00:52:30,440
And so the weights are like regression weights, it's the additional contribution to the first

610
00:52:30,440 --> 00:52:31,440
one.

611
00:52:31,440 --> 00:52:37,280
So if the weight of the first one is larger than the sum of all the others, you can intuitively

612
00:52:37,280 --> 00:52:42,720
see it will not get to any other decision.

613
00:52:42,720 --> 00:52:46,240
And then the next question is, how often does this happen?

614
00:52:46,240 --> 00:52:54,480
We have looked at machine learning data sets, and it happens quite often, astonishingly

615
00:52:54,480 --> 00:52:55,640
often.

616
00:52:55,640 --> 00:53:02,400
So this is not the only condition, it's just a sufficient conditions.

617
00:53:02,400 --> 00:53:10,920
And we've looked at three conditions, and in the median value is that in 90% of all

618
00:53:10,920 --> 00:53:14,720
comparisons in the data set, this condition holds.

619
00:53:14,720 --> 00:53:24,040
So if you take half of the data sets, in 90% or more, it holds, and the other is not.

620
00:53:24,040 --> 00:53:30,640
And then the next question is, what are the other conditions?

621
00:53:30,640 --> 00:53:39,880
And I'm not going into this at this moment, but end with a more general view that's probably

622
00:53:39,880 --> 00:53:41,760
known to many of us.

623
00:53:41,760 --> 00:53:46,200
So this is a standard justification for why people use heuristics.

624
00:53:46,200 --> 00:53:49,040
It's an accuracy effort trade-off.

625
00:53:49,040 --> 00:53:55,640
So people are a little bit lazy, or they don't make the effort, and they pay for that in

626
00:53:55,640 --> 00:53:57,360
accuracy.

627
00:53:57,360 --> 00:54:06,760
That you will find in the Kahneman book, on justifying why biases need to be eliminated,

628
00:54:06,760 --> 00:54:11,600
or in the Kahneman-Siperni and Sunstein book, by noses.

629
00:54:11,600 --> 00:54:21,480
So we have read a very interesting exchange between two David's here, and the Kahneman-Siperni

630
00:54:21,480 --> 00:54:31,560
and Sunstein, where David Wolpert and David Krocker point out, really, that noise has

631
00:54:31,560 --> 00:54:36,920
often a function, and we should not try to eliminate that.

632
00:54:36,920 --> 00:54:45,040
Your examples were mainly from not about the examples that Kahneman has in mind, but they

633
00:54:45,040 --> 00:54:58,760
apply equally to the key topics of the noise book, which is sentencing, so how much, what's

634
00:54:58,760 --> 00:55:07,880
the punishment for a certain crime, and also pricing annuities.

635
00:55:07,880 --> 00:55:20,360
And it is not that sentencing is like Bull's Eye, where there is a right answer, and different

636
00:55:20,360 --> 00:55:25,120
experts with different opinions should converge on one.

637
00:55:25,120 --> 00:55:26,520
You can have many things.

638
00:55:26,520 --> 00:55:28,280
You want to punish the person.

639
00:55:28,280 --> 00:55:32,400
You want to be low on things and give the person a chance.

640
00:55:32,400 --> 00:55:40,040
You can have many reasons, and it's also an issue where variability is a motor about

641
00:55:40,040 --> 00:55:43,160
change in societies.

642
00:55:43,160 --> 00:55:54,720
And so, again, both of these views assume that there is a truce that is singular, that

643
00:55:54,720 --> 00:56:02,280
is known, and the biases, the difference to the truce, and also the variability on the

644
00:56:02,280 --> 00:56:09,800
truce should all be canceled, and that's the key idea behind that.

645
00:56:09,800 --> 00:56:13,480
And the answer, this is not the case.

646
00:56:13,480 --> 00:56:18,960
We have a bias variance trade-off, and where we have not only a bias, but also variance

647
00:56:18,960 --> 00:56:22,440
plus noise, it's illustrated here.

648
00:56:22,440 --> 00:56:29,200
You have two darts, and on the left side, the player throws systematically too far to

649
00:56:29,200 --> 00:56:35,200
the right and low, but the variance is, the variability is small.

650
00:56:35,200 --> 00:56:42,240
On the right side, there is a player who has no bias.

651
00:56:42,240 --> 00:56:48,000
Bias is zero, meaning that on average, the darts are in the middle, but only on average.

652
00:56:48,000 --> 00:56:58,960
Variability is high, and we can see that the real idea is not to reduce bias to zero,

653
00:56:59,720 --> 00:57:08,280
as the typical messages, but to find some reasonable compromise between the trembling

654
00:57:08,280 --> 00:57:10,520
hand and the bias.

655
00:57:10,520 --> 00:57:19,800
Juristics, if they have zero free parameter, like one over n, have a bias, like on the

656
00:57:19,800 --> 00:57:28,120
left side, but no error due to variance, they always hit the same point in this image.

657
00:57:28,120 --> 00:57:32,720
And that's a way to understand why simple can help.

658
00:57:32,720 --> 00:57:36,920
And it's also a way to understand when simple does not help.

659
00:57:36,920 --> 00:57:43,480
For instance, in that thing, if you have really large amounts of data and the world is stable,

660
00:57:43,480 --> 00:57:51,200
then the simple solution will have too much bias, because the amount of data will reduce

661
00:57:51,200 --> 00:57:52,200
variance.

662
00:57:52,280 --> 00:57:59,680
Can you say a little bit about, in bias variance, if I just go from bias and variance or expectation

663
00:57:59,680 --> 00:58:08,320
values over both your decision algorithm and also the real world, if I go one over n and

664
00:58:08,320 --> 00:58:14,680
the real world is varying, I can have a very high variance, I'm not sure in what sense.

665
00:58:14,680 --> 00:58:21,680
One over n gives you, it doesn't do estimations, so if your algorithm doesn't estimate anything,

666
00:58:21,680 --> 00:58:23,960
you cannot make estimation errors.

667
00:58:23,960 --> 00:58:27,480
But we can talk afterwards.

668
00:58:27,480 --> 00:58:28,960
Okay, let's do this.

669
00:58:28,960 --> 00:58:30,960
From how it's, yeah.

670
00:58:30,960 --> 00:58:31,960
It's districts.

671
00:58:31,960 --> 00:58:42,640
Though I'm happy to talk about that, but in this image, the variability is due to different

672
00:58:42,640 --> 00:58:47,880
samples on which the same algorithm estimates its parameters.

673
00:58:47,880 --> 00:58:53,080
And if you change the samples, you get different ones, in particular if the samples are small,

674
00:58:53,080 --> 00:58:55,160
the variability is higher.

675
00:58:55,160 --> 00:58:58,200
And that's the idea behind it.

676
00:58:58,200 --> 00:59:04,760
The bias variance dilemma, and that may also behind your question, applies to situations

677
00:59:04,760 --> 00:59:10,920
that are situations of risk, where we only have to estimate the probabilities.

678
00:59:10,920 --> 00:59:12,400
It's assumed as stable.

679
00:59:12,400 --> 00:59:24,120
In this sense, it's just an analogy to understand what's, why the heuristics, when they fail

680
00:59:24,120 --> 00:59:25,880
and when they work.

681
00:59:25,880 --> 00:59:32,360
Let me finish with three methodology principles.

682
00:59:32,360 --> 00:59:38,440
It is very important to have algorithmic model of heuristics, not labels, not system one

683
00:59:38,440 --> 00:59:43,160
and other things, that we do not know what they predict.

684
00:59:43,160 --> 00:59:48,000
Second, it is very important to do competitive testing.

685
00:59:48,000 --> 00:59:55,320
Like to test a heuristic model against maybe some machine learning model.

686
00:59:55,320 --> 01:00:02,320
We have still parts in, for instance, economics, but it's very little competitive testing.

687
01:00:03,120 --> 01:00:08,920
At best, you eliminate a factor from your regression.

688
01:00:08,920 --> 01:00:14,520
But really, to test against a different class of models, really be done.

689
01:00:14,520 --> 01:00:23,240
And particularly, it's important to learn whether a very highly complicated algorithm really pays.

690
01:00:23,240 --> 01:00:26,440
And we often have a complexity illusion.

691
01:00:26,440 --> 01:00:30,760
And think that something that is simple is nothing worse.

692
01:00:30,760 --> 01:00:39,440
For instance, Harry Markowitz would have not gotten an economics Nobel Prize for one over N.

693
01:00:43,520 --> 01:00:53,320
And finally, it's also important to do not only out of sample prediction as it is regularly

694
01:00:53,320 --> 01:00:59,360
done in machine learning, but also out of population prediction.

695
01:00:59,360 --> 01:01:06,440
So the studies that I reported that being, you know, of investment where one over N is very simple.

696
01:01:06,440 --> 01:01:14,880
They are not out of sample, but they are real forecasting in the future.

697
01:01:14,880 --> 01:01:21,840
And the mean variance model or the Bayesian models in, they are not,

698
01:01:21,840 --> 01:01:26,720
their parameters are not done by out of sample.

699
01:01:26,720 --> 01:01:30,520
Because out of sample creates a stable world, a fairly stable world.

700
01:01:30,520 --> 01:01:39,040
But they are estimated in the first time interval, and then they're predicted in the next one.

701
01:01:39,040 --> 01:01:46,800
And as we know from certain machine learning, techniques that prove very successful in diagnosing

702
01:01:46,800 --> 01:01:50,320
may lung diseases.

703
01:01:50,320 --> 01:01:54,960
When they were used in a different hospital, they failed.

704
01:01:54,960 --> 01:01:59,120
And that's a problem with out of sample prediction.

705
01:01:59,120 --> 01:02:08,400
And predicting, doing the prediction in a different hospital, then is out of population.

706
01:02:08,400 --> 01:02:12,480
So I was talking today about three misconceptions.

707
01:02:12,480 --> 01:02:15,400
Complex models are always better than simple heuristics.

708
01:02:15,400 --> 01:02:19,320
They are sometimes better, but not always.

709
01:02:19,320 --> 01:02:24,280
Slow thinking is always more accurate than fast, intuitive decision making.

710
01:02:24,280 --> 01:02:27,520
No, particularly not by experts.

711
01:02:27,520 --> 01:02:37,120
Even chess masters, when the best, so the international masters have been studied,

712
01:02:37,120 --> 01:02:45,240
for instance, when they have time constraints, they do about as well as without time constraints.

713
01:02:45,280 --> 01:02:52,600
If you have excellent chess players, but not at this level, then the difference gets bigger.

714
01:02:52,600 --> 01:02:58,400
And finally, it is not correct that people rely on heuristics because they're biased,

715
01:02:58,400 --> 01:03:04,880
lazy, or irrational, as the typical message still is.

716
01:03:04,880 --> 01:03:13,960
So I invited you today to a voyage into our studies on Homo heuristicius.

717
01:03:13,960 --> 01:03:16,840
And I've made three points.

718
01:03:16,840 --> 01:03:23,320
Risk is not the same as uncertainty, although often the term uncertainty is used for risk.

719
01:03:23,320 --> 01:03:28,600
For instance, most of behavioral economics talks about uncertainty, but it's all problems of risk.

720
01:03:28,600 --> 01:03:35,440
And you detect the problem of risk immediately because the reason to know what's right in search.

721
01:03:35,440 --> 01:03:37,920
And the uncertainty, you don't know that.

722
01:03:37,920 --> 01:03:46,160
Second, logically rationality, the consistency axioms, the expected utility maximization

723
01:03:46,160 --> 01:03:49,360
is not the same as the ecological rationality.

724
01:03:49,360 --> 01:04:00,160
The ecological rationality is much harder to model, to analyze, and it's about the adaptation to the real world.

725
01:04:00,160 --> 01:04:03,760
And in, under uncertainty, less can be more.

726
01:04:03,760 --> 01:04:11,320
And one way to understand that is it's not an accuracy effort trade-off, but a bias variance trade-off.

727
01:04:11,320 --> 01:04:18,240
And finally, human intelligence evolve to deal with situations of uncertainty.

728
01:04:18,240 --> 01:04:21,080
And so did animal.

729
01:04:21,080 --> 01:04:23,240
Not with risk.

730
01:04:23,240 --> 01:04:29,360
And one of the reason is that we are not very good at calculating.

731
01:04:29,360 --> 01:04:31,520
Let's just show a start here.

732
01:04:31,600 --> 01:04:38,880
And also, some people think we need to get rid of uncertainty.

733
01:04:38,880 --> 01:04:47,560
I've often heard from well-known economists, so when Reinhard Zeldin had his 80s birthday,

734
01:04:47,560 --> 01:04:53,760
I gave one of the four keynotes, and another economist who, the Nobel laureate,

735
01:04:53,760 --> 01:05:02,800
afterwards came to me and said, interesting talk, but I don't like uncertainty.

736
01:05:02,800 --> 01:05:11,040
That's a matter of taste, but just imagine if we would know everything, if we would live in a world

737
01:05:11,040 --> 01:05:18,640
where every possible situation that can occur, the consequence that everything is known,

738
01:05:18,640 --> 01:05:24,000
and all is just about the probabilities, which were all the uncertainty strain.

739
01:05:24,000 --> 01:05:28,440
Would that be an interesting world to live?

740
01:05:28,440 --> 01:05:34,440
It would be, as Frank Knight has pointed out, would be no innovation, no profit.

741
01:05:34,440 --> 01:05:36,760
And there would be no fun.

742
01:05:36,760 --> 01:05:46,440
And life would be like in the movie where you wake up in the morning and everything goes back.

743
01:05:46,440 --> 01:05:53,680
Or as Dostoevsky said, if everything would be rational, nothing would happen.

744
01:05:53,680 --> 01:05:55,440
Thank you for your attention.

745
01:06:01,880 --> 01:06:02,680
Thank you, Gerd.

746
01:06:02,680 --> 01:06:05,680
We are actually at time, so who needs to go, let's go.

747
01:06:05,680 --> 01:06:09,520
And otherwise, if you have any questions, maybe you can take them.

748
01:06:09,520 --> 01:06:10,400
What should I do?

749
01:06:10,400 --> 01:06:12,760
Take your own questions, if you like.

750
01:06:12,760 --> 01:06:14,680
No, no, you take them.

751
01:06:14,680 --> 01:06:18,600
OK, all right.

752
01:06:18,600 --> 01:06:20,160
Good stuff.

753
01:06:20,160 --> 01:06:25,000
I think there's really supposed to be a lot of that, really.

754
01:06:25,000 --> 01:06:32,400
So I'm wondering, actually, so you've got your statistics and you showed, like, that you might have different ones.

755
01:06:32,400 --> 01:06:40,280
And I'm thinking, in any one situation, you can apply a multitude of your statistics.

756
01:06:40,280 --> 01:06:50,520
So my question is, is there any computational principle by which you decide which heuristic is allocated control of behavior?

757
01:06:50,520 --> 01:06:57,040
Because they are operating in parallel or subsets of them are operating.

758
01:06:57,040 --> 01:07:01,280
They may conflict in terms of the final behavior or choice that's exerted.

759
01:07:01,280 --> 01:07:08,200
So is there any framework for deciding the allocation of control of which heuristic gets to control?

760
01:07:08,240 --> 01:07:10,360
This is a very good question.

761
01:07:10,360 --> 01:07:14,800
So there are two or three models about how to model that.

762
01:07:14,800 --> 01:07:24,000
One is reinforcement learning, so that you learn a hierarchy of heuristics, like imitate or do something else.

763
01:07:24,000 --> 01:07:32,720
And depending on the reinforcement and the ego, and that makes the order.

764
01:07:32,720 --> 01:07:39,840
That is, there's a paper by Rieskamp and Otto on that showing that an experimental paper.

765
01:07:39,840 --> 01:07:45,720
The selection problem is not really being solved here.

766
01:07:45,720 --> 01:07:51,200
And you might think about a heuristic that decides about the application of heuristic.

767
01:07:51,200 --> 01:07:54,240
We may just do reinforcement learning or something else.

768
01:07:59,240 --> 01:08:02,200
Unfortunately, I won't be able to meet with you if I had time to meet with you.

769
01:08:02,200 --> 01:08:03,920
This is what I would ask.

770
01:08:03,920 --> 01:08:12,080
So in the mathematics and computer science of risk assessment in criminal justice, for instance,

771
01:08:12,080 --> 01:08:13,560
there are very similar debates.

772
01:08:13,560 --> 01:08:26,400
And you may know of a computer scientist named Cynthia Rudin argues that we should use simple decision trends rather than more sophisticated algorithms.

773
01:08:26,400 --> 01:08:41,320
And her argument is partly because partly that more sophisticated algorithms in that domain are typically only marginally better in accuracy than the simple decision trees.

774
01:08:41,320 --> 01:08:47,040
But also, as you referred to, in that you made, there's a demand for transparency.

775
01:08:47,040 --> 01:08:55,280
If your government is going to put you in jail, you deserve a very simple, clear explanation of why.

776
01:08:55,280 --> 01:09:06,720
And so I guess I wanted to ask you, so I think with some of the other commenters that in some sense what you're talking about is simply a different space of models that can be optimized.

777
01:09:06,720 --> 01:09:17,680
I mean, with, you know, you can choose the structure of the tree with their hands and orders, whether they're, you know, the order of the questions in the banking example.

778
01:09:17,680 --> 01:09:26,680
There were parameters that I assume had to be set in some way, the threshold parameters.

779
01:09:26,680 --> 01:09:46,160
But so I guess my question is, when do you think that, in what domains do you think there is a strong argument for simplicity and transparency, even if something more complicated is somewhat more accurate?

780
01:09:46,160 --> 01:09:51,840
And by the way, I also agree with you that that ladder plane is often made when it is not true.

781
01:09:51,840 --> 01:09:52,400
Yeah.

782
01:09:52,400 --> 01:09:57,120
But in the cases when it is true, sometimes we still prefer the simpler thing.

783
01:09:57,120 --> 01:10:00,480
So I agree with Cynthia Rudin.

784
01:10:00,480 --> 01:10:16,120
And she and us, we have shown that in sentencing or bail decisions, very simple arguments that look at three various typically like age and previous sentences.

785
01:10:16,960 --> 01:10:31,880
Are about as good as complicated things or often black box, like compass and where nobody knows exactly what's in it and what's not.

786
01:10:31,880 --> 01:10:38,160
And also that these are situations where transparency is another important thing.

787
01:10:38,160 --> 01:10:45,800
So, for instance, in the European Union at this moment, there's a decision being made about the AI Act.

788
01:10:45,800 --> 01:10:59,720
And the AI Act is in part about ask for transparency of algorithms that are so-called high-risk algorithm and sentences is one of these examples.

789
01:10:59,720 --> 01:11:09,680
And the EU Union, if they don't change now things, they're voting about it, will not allow black box algorithm.

790
01:11:09,680 --> 01:11:18,000
And I've worked with the German shoe farm, which is like the American feet, so credit rating.

791
01:11:18,000 --> 01:11:20,720
They have immensely complex algorithms.

792
01:11:20,720 --> 01:11:27,040
If you make them simple, simple arguments do as well.

793
01:11:27,040 --> 01:11:38,280
And so in many of your questions, when can we expect that complex AI will work and not.

794
01:11:38,320 --> 01:11:41,840
It's very similar to the distinction between risk and uncertainty.

795
01:11:41,840 --> 01:11:48,760
If a situation is stable and well-defined, AI is much better than human.

796
01:11:48,760 --> 01:11:53,200
So like a classical example or a chess and go.

797
01:11:53,200 --> 01:12:02,680
If a situation is highly uncertain, like how can you predict whether bail decisions about whether the guy will actually come back when they.

798
01:12:02,680 --> 01:12:04,280
That's very difficult.

799
01:12:04,280 --> 01:12:18,840
So many factors, uncertainty in these cases, we have little evidence that say deep neural networks would do consider better than humans.

800
01:12:18,840 --> 01:12:22,560
But we know that it's black box.

801
01:12:22,560 --> 01:12:31,480
Right, I guess my question is, in some cases, we want, we should use the simpler thing, even if the more complicated is more accurate.

802
01:12:31,480 --> 01:12:35,720
For other reasons, because maximizing accuracy is not the least that matters.

803
01:12:35,720 --> 01:12:43,320
Right, and also, so for instance, in my, in our work with the shoe farm, so this is the German fecal.

804
01:12:43,320 --> 01:12:51,680
It turns out that they're using variables that I'd better not telling anyone just to get a Yota male accuracy.

805
01:12:51,680 --> 01:12:56,840
For instance, when you ask for your credit score, your credit score goes down.

806
01:12:56,840 --> 01:12:59,560
Yes, we don't want to tell the speed, right?

807
01:12:59,560 --> 01:13:06,440
So it is not acceptable, but it helps to predict.

808
01:13:06,440 --> 01:13:22,120
So the compromise will be get rid of these aggressions, make them simpler, and you may lose an Yota in prediction, but there's more than just predicting accuracy.

809
01:13:22,120 --> 01:13:27,880
There are human rights about transparency.

810
01:13:27,880 --> 01:13:38,160
So this is very, very, I have a slightly provocative question that it sort of hints at what we are doing.

811
01:13:38,160 --> 01:13:42,440
You like the logical rationality is not equal to ecological rationality.

812
01:13:42,440 --> 01:13:49,880
I don't mean this just semantically, but aren't you letting people off a bit too lightly with this distinction?

813
01:13:49,880 --> 01:14:00,520
I mean, surely there must be something wrong with logical rationality if it leads you to ecologically irrational decisions.

814
01:14:00,520 --> 01:14:15,080
I feel that our program looks at logical rationality and asks, come on, there is something wrong with this because it doesn't, basically it doesn't work in the real world.

815
01:14:15,080 --> 01:14:19,640
So are you being diplomatic by making this distinction?

816
01:14:19,640 --> 01:14:27,880
Do you really mean there's something wrong with logical rationality and we should really think of rationality as a different thing?

817
01:14:27,880 --> 01:14:30,440
Yes.

818
01:14:30,440 --> 01:14:36,480
So logic is a system that's very useful, but not for everything.

819
01:14:36,480 --> 01:14:38,640
That's the point.

820
01:14:38,640 --> 01:14:41,200
You wouldn't even understand language.

821
01:14:41,200 --> 01:14:51,560
When Kahneman and Tversky asked people about Linda, the bank teller, what is more probable?

822
01:14:51,560 --> 01:14:55,800
The term probable does not map into mathematical probability.

823
01:14:55,800 --> 01:14:59,320
It is, if you look in the OECD, it says, is there evidence?

824
01:14:59,320 --> 01:15:02,000
No, there's no evidence that she's a bank teller.

825
01:15:02,000 --> 01:15:02,960
Does it make sense?

826
01:15:02,960 --> 01:15:03,920
No.

827
01:15:03,920 --> 01:15:10,240
So to impose logical rationality on everything, that's me.

828
01:15:10,240 --> 01:15:12,320
This is no good.

829
01:15:12,320 --> 01:15:15,440
And it, and it mistakes the human intelligence.

830
01:15:15,440 --> 01:15:17,360
We make an inference from the content.

831
01:15:17,360 --> 01:15:20,520
What do these problems terms mean?

832
01:15:20,520 --> 01:15:23,960
Similarly, Linda is a bank teller and a feminist.

833
01:15:23,960 --> 01:15:33,480
So an end in everyday language, the end means sometimes logically ends, sometimes something different.

834
01:15:33,520 --> 01:15:38,240
And we have an amazing intuitive capability to infer it.

835
01:15:38,240 --> 01:15:43,600
For instance, when I say this evening, I invite friends and colleagues.

836
01:15:43,600 --> 01:15:46,840
You don't think it's the intersection.

837
01:15:46,840 --> 01:15:48,880
It's the logical or.

838
01:15:48,880 --> 01:15:52,320
So these are the real interesting questions.

839
01:15:52,320 --> 01:15:57,480
How does the human mind make this stunning inferences?

840
01:15:57,480 --> 01:16:01,400
Rather than declare it's an error, it's conjunction fallacy.

841
01:16:01,400 --> 01:16:06,520
Although what you're saying there is about language and not about actions.

842
01:16:06,520 --> 01:16:08,480
This is about language.

843
01:16:08,480 --> 01:16:14,800
So I have one example that is becoming a favorite of mine.

844
01:16:14,800 --> 01:16:20,800
We can show that a, so this is totally small world logical rationality.

845
01:16:20,800 --> 01:16:29,120
If you like, someone who maximizes expected utility does not maximize utility over time.

846
01:16:29,280 --> 01:16:36,200
To me is a catastrophe in the logical structure that you're building a theory where you say,

847
01:16:36,200 --> 01:16:40,280
well, we can't really tell you what you should do until you tell me what it is that you really want.

848
01:16:40,280 --> 01:16:41,560
And that is your utility.

849
01:16:41,560 --> 01:16:43,120
And then we give you a protocol.

850
01:16:43,120 --> 01:16:50,800
Now I start behaving according to this protocol and I don't get the thing that I asked for.

851
01:16:50,800 --> 01:16:57,520
So I don't maximize my utility over time because we're using the wrong, you know, what happens over time is not what happens in expectation.

852
01:16:57,520 --> 01:17:03,680
So these to me are real problems, logical problems within logical rationality.

853
01:17:03,680 --> 01:17:07,800
It's about taking the expectation of the logarithm instead as with fitness.

854
01:17:07,800 --> 01:17:10,520
It depends on what you just depends on what your dynamics are.

855
01:17:10,520 --> 01:17:12,040
Yeah, I mean, that's an example.

856
01:17:12,040 --> 01:17:19,720
There are a number of papers by biologists who come to a similar conclusion that if animals would maximize something,

857
01:17:19,720 --> 01:17:22,360
it's always, well, what do you maximize?

858
01:17:22,360 --> 01:17:24,760
Then they actually lose on fitness.

859
01:17:24,760 --> 01:17:26,680
Yes, sir.

860
01:17:26,680 --> 01:17:27,400
Thank you again.

861
01:17:27,400 --> 01:17:28,800
That was great.

862
01:17:28,800 --> 01:17:31,720
You can leave it to informal discussions and.

