start	end	text
0	5680	up in the kitchen. Very excited to introduce Donald Martin, Donald's the head of societal context
5680	10400	and understanding tools and solutions at Google. We don't do much for introductions here, but
10400	18240	Donald let me just turn it over to you. You won't appreciate it. Hello everybody. It's really an
18240	22560	honor and pleasure to be here at Santa Fe Institute. I actually have been dreaming about visiting
22640	29680	SF5 for, I don't know, almost 16 years. I read the book Complexity back in 2007, about 2005. I was
29680	36240	like, I maybe want to go there. And so I'm now here. I'm here. Thanks to Will. Thanks for your
36240	43280	time and attention. The title of my talk is Epistemic Uncertainty, the AI problem understanding
43280	48640	chasm. I kind of updated that from a gap to a chasm, but it's a really big problem. And the necessity
48720	55920	of structured societal context knowledge for safe and robust AI. Now me and my team, which also
55920	61840	goes by scouts, we believe that bridging this problem understanding chasm is critical to being
61840	68560	able to realize our goal of responsibility for this AI. And so we're kicking off a campaign
69440	75360	to try to imply more participation by others in trying to understand and solve this problem.
76320	81680	And so we've got something we call our flywheel. That starts off with increasing awareness just
81680	88080	about the chasm and what the causes are. We hope that drives more foundational and applied research,
89200	93040	particularly in some of the techniques we're going to talk about causal theory model, for example.
93840	100480	And of course, we want that to result in real impact in the world. The use case that we focus on is
100480	106880	health equity. And so right now, we're here's where we are. And so we're kicking this off and
107840	112480	you know, bridging the problem understanding chasm as you will see, we think requires embracing
112480	118720	complexity. So I figured what better place to start talking about this in the Santa Fe Institute.
122400	129200	Few calls to action because I know folks in this room, SFI is influential. So we would love for
130160	135040	after the talk, people to spread the word about this chasm, right, and how important it is to budget.
136720	142640	Hoping that people will also take on the task of embracing prototyping problems as complex
142640	147840	adaptive systems before intervening things like data science and AI because we think that's critical
147840	154400	for proactively mitigating bias in these systems that we're building. And then finally,
155280	160480	a key component of bridging this gap is investing and problem prototyping
161200	165760	trust and capability in historically marginalized communities. That's going to be a key source of
165760	170160	the knowledge that we think is missing from AI based product development.
173200	175520	Okay, so with that, I'm going to set just a little bit of context.
176880	180880	I gave this talk a part of this talk last year, I had to kind of explain there's this industrial
180960	184480	revolution. AI is going to be really important. I don't think I have to really do that anymore.
185200	189760	I think you realize we're in the midst of this transformation that people were calling the
189760	195360	fourth industrial revolution and that AI is like a core technology within it. And of course,
195360	200800	you know, we have high hopes that AI is going to do these amazing things and transform healthcare
200800	208160	and medical care. But we're also very cautious, rightly so because of the big conflict in the room,
208720	211920	harmful bias that can be propagated really easily by these systems.
213120	216400	And so it's part of my job to pay attention to all the headlines that come out every day
217040	224000	about instance of bias and all sorts of domains. But I pay particular attention to
226320	231680	headlines that involve healthcare and medical care. And that's really because of my mom.
231760	236800	With my mom, Betty Martin, she died five years ago this month from cancer.
238160	244240	And as I navigated that harrowing journey with her, I experienced bias directly in the healthcare
244240	250240	system. And that really informs how I think about what it's going to take to mitigate bias in AI
250240	255200	systems applied in high stakes domains like healthcare. And so as I said, she died from cancer,
255200	260960	started off as breast cancer, turned into lung cancer. At one point she had to get part of whatever
260960	266800	lungs removed. And as a result of that, she developed atrial fibrillation. And so that can
266800	273600	cause blood clots and strokes. So she had to get prescribed a blood thinner. But the blood
273600	279360	thinner that the doctor wanted her to take would require her to stop eating vitamin K foods,
280080	286800	leafy greens, spinach, kale, collard greens. I love that stuff. That was like a staple in her diet.
286800	289840	And it would also require her to have to go to the hospital
290480	294240	every week to get her blood tested to make sure this blood thinner wasn't harming her in some way.
295840	300240	And so my mom, she balked at this. She's like, ah, you sure there's not another
300880	304320	option? Is there a better blood thinner where I don't have to shift my lifestyle?
305200	310480	And the doctor's response was, you should just, you know, be grateful because there are many
310480	315200	people in third world countries who would die to have access to this particular blood thinner.
315920	318720	My mom was like, well, you know, I don't know what that has to do with me, but
320400	324320	and you all know my mom, of course, but she didn't accept that as an answer.
325120	330480	She talked to me and my sister, my dad. My sister happens to be a medical doctor, a psychiatrist.
330480	334880	So she was able to find very quickly an alternative blood thinner that wouldn't have
334880	341280	required my mom to change her lifestyle when I owed her. And so, and we could easily afford it
341280	346160	with my dad's health care insurance. So we went back to the doctor. We said, hey, we found another
346160	351360	alternative, but we ought to make sure we're not missing something. Like, why didn't you
352000	355360	recommend this particular blood thinner because it doesn't have any of these side effects?
356240	361680	The doctor's response was, I assumed that she wouldn't be able to afford the alternative.
363520	368000	I assumed she wouldn't be able to afford the alternative. So that's this bias kind of
368080	372800	rearing its head right in the middle of the situation. So that's why this kind of informs,
373760	377280	you know, my thought process about how we're going to, what it's going to take to mitigate
377920	385520	these kind of really nuanced implicit biases. And so the real issue here is that the doctor ignored
385520	391760	and abstracted away my mom's, her situation, her circumstances would all call her a societal
391760	397120	context. Didn't consider that when they made that intervention recommendation. So I want to look at
397120	402720	my mom's societal context in a different way. There's mom, born and raised in the USA, doesn't
402720	407520	know anything about, you know, what happened in third world countries. She loved leafy greens.
408720	413760	This is her second bout with cancer. So she, you know, had already experienced having to go to the
413760	417920	hospital every week to get chemotherapy and get stuck with the nail show. So she would avoid that
419040	424480	anywhere she could if it was possible. And then, you know, she had us as a family, which included
424480	431040	my sister, medical doctor knew how to navigate the drug industry very well. My dad, a double retiree
431040	438640	from US Army and the Postal Service, great healthcare insurance. So we had this, this other option.
439520	444320	But the doctor ignored and abstracted away all this societal context, didn't ask about it,
444320	449120	didn't consider it and then made this intervention recommendation that conflicted with my mom's
449200	450080	societal context.
451680	458320	Conversely, do you think that the doctor thought he was being anti-biased by making
458320	462080	this mistake and thus, in fact, being very biased?
462640	470560	You know, we really have, I have no idea. I don't, so we all have biases, right? We can't
470560	474720	tell what someone's intention is, right? I'm sure there's not a negative intention.
475520	481600	But, but this is the result that when you don't seek to understand like the situation or
481600	487680	circumstances, when you make assumptions, it can lead to these problems. And so we really
487680	492800	use this high-level definition of societal context as this dynamic and complex collection of
493600	500000	social, cultural, economic, political, historical circumstances that surround people,
500000	507600	places and things. And interventions that ignore or abstract away societal context can lead to
507600	512160	unintended and unnecessary harm. So this is the key takeaway for the talk, like if you don't
512160	516480	remember anything else, please remember this and try to get that in as you're intervening in
516480	523440	technologies. So now I'm going to dive into the core of the talk. Here's, here's the five parts
523440	526880	I'm going to walk through. First, I want to talk about how we abstract away societal context when
526880	531920	we do ML and AI based product development. Then I'm going to introduce this problem, understanding
531920	538640	chasm. I think it's the root cause of why we do this, abstracting away. And then the last three
538640	543600	seconds are about what we think some of the elements of bridging this chasm are. One is we
543600	550880	think we need some sort of reference frame or model of societal context. Second, we think we need
550960	557920	a way of producing societal context knowledge and we think prototyping problems as complex
557920	562560	adaptive systems could be a way to do that. And community-based system dynamics involves
562560	566960	communities in doing that work. And then finally, even if you have this great technique,
566960	572240	you have to have capability in the world in order to like execute it. So let me dive into the first
572240	577760	part. I'm going to start this off with a quote from a great paper called Fairness and Abstraction in
577760	584560	Social Technical Systems by Andrew Selps, who's a leader, not leader in the ML Fairness community.
584560	588240	If you haven't read this paper, even if you don't care about ML Fairness, I highly recommend it.
589360	594560	But this quote sums it up really well. Bedrock concepts in computer science such as abstraction,
594560	600240	render technical interventions, effective and accurate and sometimes dangerously misguided
600240	605280	when they enter the societal context that surrounds decision-making systems. So you could apply this
605280	610240	directly to what happened with my mom. In this case, we're talking about building products and
610240	616960	systems. So I'm going to just walk through a really high-level view of typical AI product
616960	620960	development lifecycle. I'm using a product development context because I work at Google
620960	626640	when we build AI-based products. The key thing they're recognizing this is that this whole process
627360	632240	is driven by human decision-making from end to end. A lot of times people forget that.
633200	639840	And the main output, of this lifecycle, is a model-based product that generates
639840	646000	outputs, typically predictions, that somehow can help you automate or automate a process,
646000	650480	solve some sort of problem. Of course, to produce that product, we have to train and
650480	655440	tune and evaluate that model. To do that, we have to select and prepare some training data.
656560	661760	To do that, we actually have to actually formulate this problem for machine learning and AI.
661760	666080	We have to take that problem and make it fit the hammer that we have, that we want to use to
666080	669520	solve the problem. And then in order to do that, well, we have to actually have a deep
669520	675280	understanding of the problem itself. So that's typically kind of where this diagram ends.
675920	679280	But of course, there's a key input that drives this whole thing, and that's
679840	687600	observations and data. But sometimes we forget about where those observations and data come from.
687680	690320	They're coming from this thing we're calling societal context.
691280	695440	And then the machine learning fairness, sometimes we call it the social-technical environment.
696560	700160	Computer scientists and engineers and practitioners who are trying to get things done,
700160	705680	they have many different words for, you know, that area, this kind of ambiguous area,
705680	709760	amorphous area. It's the real world where this stuff is going to end up one of these days.
709760	716800	It's the environment. It's the data-generating process. It's where we have to look to kind of
716880	722160	understand the hypothesis space. It's that problem domain. It's where that background
722160	728960	knowledge is going to come from. It's the broader context. And then we have some characteristics
728960	737360	that we associated with this amorphous thing. It's messy. It's complex. It's qualitative.
738320	747760	It's causal and nonlinear. It's subjective. I need to work on objective things. It's dynamic,
748720	752080	and it's historical. Why do I have to worry about what happened decades ago?
753840	758800	Now, the issue is that those characteristics don't align very well with the statistical
758800	766480	processes and methods that we use to build AML models. So that's why we tend to
767360	771920	put up this abstraction boundary and abstract it away. And we have particular practices and
771920	779920	assumptions that reinforce that. One of those is the IID assumption, which assumes that the
779920	784000	variables we're going to try to learn in the training data assumes that there's no
785200	788480	that there's no relationship between them. There's no causal relationship between them,
788480	794800	among other things. And so that assumption, you make that assumption, you're ignoring
794800	800080	abstracting away the very nature of societal context, which is supplying all the data.
804080	808640	Now, the other thing that happens when we extract away the societal context is easy to
808640	812880	forget that eventually you're going to be done with training, the products are going to be in
812880	816400	the real world, and the inputs aren't going to be the training data anymore. They're going to
816400	821280	come from the real world. And those inputs are not going to follow the rules, the assumptions
821280	825520	that we made when we built the systems. And then those outputs are not going to be in the
825520	831040	laboratory anymore or in the evaluation suite. They're actually going to be interventions
832080	835200	on that societal context on society. They're going to cause impact.
838640	844160	And then, of course, those interventions can have an impact on the next round of observations
844160	850880	and data we sample in order to build the next version of that product. So you've got this feedback
850880	856560	loop that can be vicious if we're not careful about the outputs and the quality of the outputs.
858880	862400	Now, the other thing that happens when you're abstracting away
863440	867760	societal context and understanding the details of the data-generating process and the problem
867760	874640	domain is it leads to a lack of knowledge on the other side of that boundary, lack of knowledge
874640	879040	about the problem domain and the data-generating process, things you have to understand to formulate
879040	886080	the problems well. Now, another term we use for that in machine learning AI is epistemic
886080	890720	uncertainty, just a fancy word for lack of knowledge. Usually, we're talking about lack of
890720	897760	knowledge about the model by the model, but that lack of knowledge also applies to all the decision
897760	901680	makers in this ecosystem, which include the humans who are making all these critical decisions.
903040	908960	And in reality, models are basically inheriting the epistemic uncertainty of the decision makers
909120	915440	who are making all the pipeline decisions that are going to impact how that model performs it with
915440	922880	the outputs. Now, it turns out that epistemic uncertainty is the root cause of two big problems
922880	929360	in AI. One is harmful bias propagation, which I'm going to talk about in more detail in a second,
929360	937280	but the other is deep learning robustness. And this also goes by out of distribution
937280	942560	generalization. Distribution shift fits into this category. So, I'm going to drill into that for
942560	948560	just a second to show how experts in this problem are, when I read them, they're saying, we need
948560	956800	societal context knowledge to solve this problem. So, the fundamentals of this deep learning
956800	962080	robustness problem is this example where you have a classifier that's trained to identify
962080	969280	objects and images, gets really good at it, high performance accuracy, really good at identifying
969280	976720	this pig. But if you just change very slightly the input data, the image, in ways that would be
976720	981840	imperceptible to a human, the classification changes to like, ah, this is an airliner.
982800	987760	Right? So, that's the kind of classic example. And so, the issue here is that the classification
987760	993920	model doesn't have an equivalent conceptual model that we have. And these are kind of structures
993920	1000480	in our heads that allow us to reliably identify objects under really noisy conditions.
1002000	1007760	And so, the experts who are trying to solve this problem, and one of them is Yashio Benjo,
1007760	1014080	who is considered one of the so-called godfathers of AI. When I read what they're saying,
1014080	1018560	when I read their words, I see they're saying, we need societal context to help us to solve this
1018560	1025120	problem. So, just to read a quote from another really good paper called Tor Causal Representation
1025120	1031360	Learning, we argue that causality with its focus on representing structural knowledge about the
1031360	1037040	data generating process that allows interventions and changes can contribute towards understanding
1037040	1043040	and resolving some key limitations like robustness and current machine learning methods. So,
1043040	1048720	when I read that, I see we need structural knowledge about that amorphous thing, the data
1048720	1054800	generating process, the problem domain. It's causal. So, I think there's a direct connection
1054800	1059280	between what they're asking for and what we're seeking in terms of being able to fill that
1059280	1065040	knowledge gap, fill that epistemic uncertainty gap. So, now I'm going to introduce this problem
1065040	1072640	understanding chasm using this use case, this real-world example of racial bias discovered
1073280	1076560	in a medical algorithm that's widely used throughout the U.S. healthcare system.
1077600	1083360	This paper came out about four years ago. The purpose of this algorithm was to identify patients
1083360	1090000	with the most complex healthcare needs so they could be given access to special programs early on
1090720	1094320	so that you could ultimately reduce the overall cost in the healthcare system.
1094320	1097840	And it's important to remember that their motivation was to reduce the overall cost
1097840	1103520	in the healthcare system. Now, unfortunately, people not selected for the special programs by
1103520	1109360	this algorithm suffer from nearly 50,000 more chronic diseases than the people who were selected
1110560	1114800	and the people who were not selected were disproportionately Black Americans. So, this
1114800	1119520	is why the algorithm was deemed to be racially biased. So, the question is like why did this
1119520	1125600	happen? What led to this? And the researchers who discovered this had unprecedented access
1125600	1130800	to every aspect of how this was built, which is not typical. The training data, the machine
1130800	1137520	learning architecture, the training algorithm, you know, the performance metrics, the actual outputs,
1138480	1143520	access to everybody who made decisions to build the system. And their conclusion was that the root
1143520	1149120	cause of this failure was incomplete problem understanding. That's what it boiled down to.
1150480	1156080	And remember, understanding the problem and formulating the problem for AI is at the very
1156080	1163040	beginning of this process. But in reality, we really don't spend that much time studying and
1163040	1168240	trying to improve that part of this process. We spend most of our time on the latter stages,
1168240	1175280	the later life cycles. But remember, yeah, go ahead. Is that the algorithm that used the amount
1175280	1180480	of money you spent on healthcare as a as an input? Yeah, exactly. And I'm going to I'm going to get
1180480	1187600	into that right now. Yeah. So, the key thing I want to go back to is that human decision of choices
1187600	1192960	drive this entire life cycle and really critical ones happening at the very beginning of the life
1192960	1199440	cycle. But right now, those processes are pretty ad hoc and informal. And that's what's leading to
1200720	1206800	my mind, this epistemic uncertainty. So there's a quote that I couldn't resist putting in. And
1206800	1212960	once I read it, this is one of the papers that we reference and read when we're thinking about how
1212960	1217680	humans make decisions and choices, because we said, Oh, this is a really critical part of this
1217680	1222240	process. We should understand how humans make decisions. But this quote jumped out of me because
1222240	1227600	it says choices do not merely identify one option among a set of possibilities. Choosing is an
1227600	1234960	intervention, an action that changes the world. That's particularly true when decisions are being
1234960	1240640	made within these AI product development life cycles, because those decisions impact directly
1240640	1245760	what ends up getting produced by the machine learning models, which are themselves interventions
1245760	1250240	on that societal context. So the criticality of this decision making and how we make choices
1251040	1256800	is very high. And so we spent a lot of time looking at this and the research that we looked at
1257680	1263920	pointed out that when we make decisions as humans, we leverage our causal inference capabilities.
1264880	1271120	And those are shaped by strong top down prior dollars that we have in the form of what they
1271120	1275360	call intuitive theories, as called them causal theories. And causal theories are these models
1275360	1282560	we build up over time, as we navigate complex realities, face problems, solve problems,
1282560	1287520	we form theories about why they're happening. And that informs our goals and our strategies to solve.
1289120	1294640	So let's look at what role causal theories played in the failure of this algorithm.
1295440	1300720	And remember that the problem was predict which patients will have the most complex healthcare
1300720	1305840	needs, because we're going to give them access to these special programs. And the causal theory
1305840	1311520	that they leveraged wasn't explicit at the time, but the one they leveraged was that more complex
1311520	1317680	healthcare needs is going to lead to an increased spending on healthcare. And so they chose that
1317680	1322720	as the target variable or their predictor, or the proxy for complex healthcare needs.
1323360	1326800	They said if we can predict who's going to spend more money on healthcare in the future,
1326800	1329840	we'll be able to predict who's going to have the most complex healthcare needs.
1329840	1334240	This is where the algorithm failed. And the reason it failed is because this
1334240	1340320	causal theory is woefully incomplete, right? It leaves out critical factors that impact
1340320	1346800	how much black Americans spend on healthcare, even when they have more complex healthcare needs.
1346800	1351680	These are factors like under diagnosis due to bias, lack of trust in the healthcare system,
1351680	1355760	wealth and income disparities, and lack of access to affordable healthcare.
1356560	1360560	Now take it, all those factors actually decrease how much black Americans spend on
1360560	1364880	healthcare, even when they have more complex healthcare need. And then taken together,
1364880	1371040	those factors represent a subset of the structural inequities that exist in the US healthcare system.
1372400	1377280	And those structures were revealed by COVID-19, right? It showed all those structures.
1377920	1382880	But those structures actually increase how much complex healthcare need there is in that community
1382880	1386400	while simultaneously decreasing how much they spend on healthcare.
1386400	1393280	So the causal theory wasn't even close to having the reality of the problem domain that it was
1393280	1401200	trying to solve. So this is what we call the problem understanding chasm. This difference in
1401200	1408400	understanding of the key problem from these two different perspectives in this chasm between them.
1408640	1410560	I'm talking more about what contributes to that.
1413040	1416880	This lack of knowledge on the right-hand side led directly to that
1416880	1422320	harmful intervention, that lack of understanding of that additional societal context.
1424720	1427680	Now one of the key root causes of this chasm is
1429040	1436000	divergences between communities that are trying to intervene and how proximate they are to the
1436000	1443520	problems and then their ways of knowing and explaining problems. So on the civil society side,
1443520	1448320	these are generalizations, but folks trying to solve those problems on the civil society side
1448320	1453840	are typically very proximate to the problems. They're deeply entrenched in the problem domains.
1453840	1458320	On the product side, we're less entrenched in the problem domains, we're less proximate to them.
1460800	1464640	Civil society folks that have a high stake in like solving the societal problem itself.
1466160	1470320	Related to the broader domain. On the product side, we're typically interested in the business
1470320	1477920	problem related to that domain. And then finally, on the civil society side, since you're really
1477920	1484480	proximate to the problem and the humans that are suffering from it tend to prioritize a qualitative
1485360	1492560	human perspective on the problem factors. So I have a question just trying to blend it to
1493520	1496240	things like the gravity project, which are trying to determine
1497600	1503360	codes for social determinants and valves and related projects. Would that be maybe some mitigating
1504320	1510160	factor where you've got the civil society data informing this decision, I think, rather than
1510800	1515200	as an organization trying to decide for ourselves what we think is important?
1515200	1519280	Yeah, that would be exactly right. The thing you want to do is get to the point where
1520000	1522800	you have some cooperation and understanding the problem. Because you have two different
1522800	1528640	perspectives. Now, the right hand perspective is not wrong. It's just a different lens in that problem
1528640	1533280	domain. And solely using that lens, you're going to miss critical things. And the same on the other
1533280	1539200	side as well. Even on the civil society side, we're trying to solve problems. We're using methods,
1539200	1545760	sociology, etc. But you also have problems with uptake of those solutions. Statistical methods
1545760	1552880	get used on that side as well. I have to mention as well. So the idea is how do we bridge this kind
1552880	1558880	of chasm in terms of how different groups think about and understand problems and get to a shared
1558880	1562640	understanding helps us get closer to the reality that we're trying to intervene on.
1565600	1570080	And then I just wanted to show this one more time using that initial diagram, that problem
1570080	1577440	understanding chasm, right? This gap between societal context and all that messiness and how
1577440	1583840	we're thinking about the problem on the product development side. Now, just to go back to this,
1584560	1590000	our goal is, the thing I'm focused on within Google is how do we reduce this epistemic uncertainty?
1590640	1595840	And to reduce epistemic uncertainty, you need knowledge. But we have to figure out a way to
1595920	1602720	have that knowledge across that chasm and be useful during these workflows. And so people are
1602720	1612000	using all sorts of tools to make decisions. And we can't expect them to read to Andrew Self's paper
1612000	1615520	and figure out what they're supposed to do, read the latest research. It's just not going to happen,
1615520	1622800	right? So we have to find a way to get knowledge across that chasm and make it available to these
1622800	1629040	decision makers during every step of that workflow. So this is what our hypothesis is,
1629040	1634560	is that to bridge that problem understanding chasm in a responsible way, these decision makers need
1634560	1640800	tools that put community validated structure to societal context knowledge about complex
1640800	1647520	societal problems at their fingertips, especially during problem understanding at the very beginning,
1647520	1651200	but also throughout the entire lifecycle, because you should also be leveraging that
1651200	1655360	understanding when you're evaluating a product for whether or not it's performing well,
1655360	1659600	like what does that mean, right? It has to be relative to the context in which it's going to
1659600	1669360	operate it. But we have to remember a couple of key things. And one is that it's factual that
1669360	1674400	historically marginalized communities are disproportionately negatively impacted when
1674400	1681920	things go wrong with AI bias. Those are just the facts, right? And you can look at example after
1681920	1687600	example. And then the other fact is that the lived experience expertise of those folks
1688640	1694240	is usually excluded from that, from those problem understanding or problem formulation steps.
1696400	1703440	And so if we are focusing on also addressing these issues, then we're not, we're not
1703440	1709760	responsibly solving this problem. So we think there are two key ingredients for bridging this
1709760	1715520	chasm. One is in order to transmit knowledge across that chasm, we think we need to have some sort of
1715520	1721360	model or reference frame for structured societal context knowledge. Yes. Oh, sorry, I didn't see
1721360	1726640	you. Yeah. The slide where you said your hypothesis, what do you mean by knowledge?
1727360	1735600	So that's a good question. That's a good question. So what we need, what we mean by knowledge is
1737120	1750080	awareness of factors that make up the circumstances that in which you're going to be
1750160	1757440	deploying models and that originate the problem you're trying to solve. So knowledge is like a very,
1757440	1762240	you know, that's a big, deep philosophical question that you asked me. But we just mean
1762240	1768640	awareness of these, these factors that make up these circumstances. Does that answer your question?
1768640	1775840	I'm sure. Yeah, I just couldn't tell if you meant like data or theories or societal facts or all of
1776400	1780720	those. Yeah, I mean, as we look at it, you'll see it's kind of like a combination of that,
1780720	1785120	right? So we're, so we've got lots of data, but we don't have a lot of context for the data
1785120	1792320	to help us interpret it. So what we're looking for are those, those additional facts that help
1792320	1797200	make up the circumstances that led to the data existing. And oftentimes those circumstances
1797200	1801200	will tell you that you're missing data, like you think you have the right data to understand
1801200	1805360	this problem, but you don't even have that data yet. And so it starts with some understanding of
1805360	1810400	what these other factors are you should consider. Yeah, exactly. I see, I feel like the biggest
1810400	1816800	thing is the processes that are generating the data. So you may have known some of Rahid Ghani's
1816800	1824160	work in Carnegie Mellon. Now he's there where he runs this data science for social good program.
1824160	1830480	He brings students in and students, for example, go with the cops. They go, they go with the ambulance
1830480	1835120	driver. And like one of the examples he was saying was that this ambulance driver who already
1835200	1840400	had two car shins, so he was on probation, he didn't take a call because he's like,
1840400	1846080	I don't want to risk my job because if I go take this call, I may do something wrong and then
1846080	1850720	they're going to fire me, right? Your data is not going to say that unless you're actually
1850720	1856000	riding with the ambulance driver and seeing the ambulance driver park his car under the bridge
1856000	1863280	to and not accept that call. You're not seeing that. And so exactly. And so it's very difficult to
1863280	1868560	say, I have good enough societal context knowledge. That's exactly right. You have to
1868560	1875360	understand these situations and circumstances of key actors and agents. And so we came up with a
1875360	1879680	way to start to tackle how to model this kind of knowledge. And so I'm going to get to that in a
1879680	1887200	few slides. So I think I was here. Yeah, so this goes right back to the question about knowledge,
1887200	1893120	right? So you'll see what we mean by it in a few couple slides. But we think the other key
1893120	1899520	aspect to address those facts I just laid out is that we need participatory, non-extractive
1899520	1904480	methods. And by non-extractive, I mean, you know, there'll be a demand for this knowledge. We're
1904480	1909120	going to make sure we're not just going into communities, gathering knowledge and then taking
1909120	1915760	it away. Like there's got to be mutual benefit. So and then you also need capacity, right,
1915760	1920480	for communities to participate directly and own their knowledge and be able to benefit from it.
1923120	1930080	Yeah, Chris. So some of the things that the community can share are not the sorts of things
1930080	1936400	that are going to become data on the other side, right? Like that example is not that you're going
1936400	1941920	to start adding as a factor, you know, are people refusing to take calls because they are on probation.
1944880	1951440	And so I guess part of the question here is like, which things that the community knows
1951680	1960560	are translatable into something that then a more principled set of AI creators can use to make a
1960560	1967840	more humane and fair algorithm? And which ones of them are reasons to be modest about the uses
1967840	1973920	of AI and to say that, well, actually, what we want is AI products that say, well, here's what
1973920	1979920	we suggest based on what little this algorithm knows. But there's only a suggestion and you
1980880	1986000	decision, you human decision maker being advised on this probably know a lot more about this case
1986000	1991600	than the algorithm does. So the algorithm is presenting its results and assign kind of with
1991600	1999360	some humility. And yeah, I mean, that's, that's interesting too. The if as well as the how
2000320	2004720	of representing this knowledge. Yeah, yeah, I mean, the thing I'll highlight there is that,
2005520	2009520	you know, one of the things you want to happen is you want the models to be aware of their
2009520	2015280	epistemic uncertainty. You know, I don't I don't know a lot about this. Right. And so even if I
2015280	2021680	don't end up incorporating, you know, a particular factor directly in the data set or the decision
2021680	2026880	making variables, I need to have a better understanding of like, I don't know a lot about this space,
2026880	2031200	maybe we shouldn't even be using AI for this. Right. Right. That's why you want to really
2032160	2037280	dig deeply into this into the problem understanding and formulation phase before you start building
2038000	2040960	machine learning systems, right, for a particular problem.
2043280	2046880	Does that answer your question? It's a longer discussion. I think it's a great
2046880	2052000	it's a great set of questions like, you know, something like, Dr, here's our recommendation,
2052000	2056400	but please keep in mind, there are lots of good reasons why this patient might be an exception
2056480	2061360	to this recommendation. And I I don't know how to convey this partly gets into human
2061360	2067200	computer interface questions and psychology questions. How do we convey uncertainty and
2067200	2076240	humility to the people being advised by these algorithms? Yeah. And, you know, yeah. Yeah.
2076240	2080000	And I think it's just it's it's not the same part as the explainability problem that everybody's
2080000	2086560	trying to solve, explaining the results of the output. We also be need to be able to explain
2086560	2090960	like, what we don't know, right, and the risks associated with taking the advice.
2092160	2098480	Sorry, if I may, this also goes to the incentive. So as an AI tool developer, right, you don't want
2098480	2104560	to develop a tool that says, I don't know, right. And so first, so for example, Karina Cortez,
2104560	2109040	Google Research, right, she had a couple of really nice papers on learning with abstention,
2109760	2114720	where the machine learning system says, I don't know, right. But your software engineers,
2114720	2120480	you're, they wouldn't want to give a tool that would say, I don't know, right. And so that's
2120480	2130000	changing the incentives of like, I don't know. Yep. I agree 100% of the agents that are making
2130000	2134640	decisions is like a key part of that overall societal context. But you'll see right now,
2134640	2140160	even like, some things happen with the generative AI, and like hallucinations happening,
2141520	2148800	products are saying, yeah, bullshitting, products are, you see product services saying,
2148800	2154000	we don't know, don't trust us, we're not sure. Oh, that's great, then. Yeah. Yeah. I mean,
2154000	2159520	because, and that's being forced by, by this, this becoming an intervention at society that's
2159520	2167120	getting a lot of kind of feedback. Right. So, you know, so we've got this situation,
2168240	2174880	we've got this messiness that can't be transmitted over that, that chasm, if, and we're saying,
2174880	2178560	hey, we need to organize information and knowledge about societal context in some way, this big,
2178560	2183520	scary thing. So we need some kind of model of it. So it's kind of ironic, right, you're trying to,
2183520	2187040	you're trying to reduce people abstracting away, but in order to do that, you need some kind of
2187120	2191680	abstraction, right, that people can use to kind of cope with this knowledge in some way.
2193200	2200240	And so we're, I was really inspired by some work by a sociologist named Dr. Walter F. Buckley,
2200960	2207120	who back in the late 60s made the connection between trying to understand society and
2207120	2214400	sociocultural systems, and between that and systems theory. And so he was like,
2214400	2218480	we should think of society as a complex adaptive system. When I talk about societal context,
2218480	2224080	let's face it, we're talking about like, you know, just a, you know, some conception of society as
2224080	2229120	a whole, how do we think about that? And so our work was based on using complex adaptive systems
2229120	2233840	theory as the basis. If you look at the characteristics of a complex adaptive systems,
2234480	2239360	a complex adaptive system, you'll, you'll see some of these characteristics that are in common
2239360	2245360	with how we were describing societal context. It's complex, adaptive and dynamic. There's this
2245360	2250720	nonlinearity. I talked about these feedback loops of the ecosystem that we're dealing with.
2251520	2256720	We talk about things being historical, time delays goes to that complex adaptive systems have
2256720	2264400	history as well. So we looked at the canon of work associated with this. John Holland is like
2264400	2270640	a giant in this space. Scott Page, who I think doesn't work with SFI is also somebody I love to
2270640	2276800	read. So we synthesize some work from these folks to come up with, you know, an attempt, right,
2276800	2281360	to start to wrap our, our heads and arms around this complex thing. So we came up with a reference
2281360	2289760	frame or model has three key elements, agents, precepts and artifacts, agents. When we thought
2289760	2293360	about agents, these could be human or nonhuman, but I'm going to talk about this through the human
2293360	2298400	lens. So through human lens, these are eight, these are individuals and institutions.
2299680	2304000	And the ideas that all agents have and essentially are their precepts,
2305920	2311120	which are our conception, our beliefs, our values, our stereotypes,
2312080	2314320	our, how we perceive needs and problems.
2316560	2322400	The causal theories that we talked about, these are all these rules that constrain and drive the
2322400	2328240	behavior of agents. And these can go all the way down to, you know, the fundamental rules for
2328240	2332880	building, building a human, right? But at the societal level, we're talking about these kind
2332880	2339040	of beliefs and values and goals that constrain and drive the behavior of agents. And when agents
2339040	2346800	behave, their behavior manifests itself in the real world as artifacts. Indeed, they're just
2346880	2352800	all the things agents produce and create, language, data, laws, institutions, right?
2352800	2357200	There's some multiple, multiple inheritance. Some, you know, agents can produce artifacts or
2357200	2364720	also agents, for example, policies, AI models or artifacts were creating products and we're
2364720	2370640	producing problems as well. And then there's this other important relationship between precepts and
2370640	2376640	artifacts. This is kind of well-known in the way people think about HCI, for example, is that
2376640	2381840	the things we produce are a reflection of our values, right? So the artifacts are in some way
2381840	2385440	a reflection of the precepts that led to them existing in the first place. And of course,
2385440	2390640	those precepts are influenced by the artifacts that the agent is surrounded by in the world,
2390640	2396240	right? I read a book, my precepts get updated, then I do some more acting in the world, gets
2396240	2403440	reflected in more artifacts. But we also wanted to organize this and represent it as a taxonomic
2403440	2409360	model because ultimately we're envisioning, creating knowledge graphs and databases so that
2409360	2417600	we can transmit this data across that chasm and make it available to tools and workflows, right?
2417600	2420960	So somehow or another, we have to have a structured representation of this knowledge.
2422720	2427920	Now, when we develop this initial model, we hypothesize that these perceived problems and
2427920	2431760	causal theories were going to be really important if we're thinking about product development.
2432720	2437360	And when we actually did this work, it was before that Ziad Obermeyer paper came out
2437360	2443040	about racial bias and medical algorithms. So we were kind of excited that, yeah,
2443040	2447760	we were on the right track, we think, because he pointed out, they pointed out how critical
2447760	2453280	these causal theories were and how they directly led to harmful bias in an important system.
2455920	2459200	And so now the left-hand side looks a little bit more
2459920	2463920	organized, maybe we can think about societal context in a little bit more of an organized way.
2465440	2469760	But the key question is, how do we responsibly acquire these causal theories
2471520	2478800	and make them into structured knowledge? So this is where this practice called community-based
2478800	2485040	system dynamics came into play. I have some friends in the system dynamics community.
2486000	2490560	I know the person who wrote the book on community-based system dynamics, and this seemed like
2490560	2496720	it could be a good way to produce societal context knowledge through the act of prototyping
2498000	2502000	problems, which are like this core part of societal context, but prototyping them
2502000	2507200	as systems, as complex adaptive systems. And community-based system dynamics
2507200	2516320	is a participatory practice for prototyping problems that centers communities and empowering
2516320	2521920	communities to fully participate and take ownership of their own models of the problems
2521920	2530560	that they're faced with. But it's grounded in the feedback perspective of system dynamics,
2530560	2538640	which is invented almost over 60 years ago at MBT. Now SD itself, system that dynamic itself,
2538640	2544880	leverages visual tools and simulation to support group model building, building prototyping
2544880	2551840	problems together, and developing collaborative, developing causal theories. And the other words
2551840	2556960	we use for that are problem models or problem prototypes. And it allows people, health people
2556960	2562640	do this both qualitatively and quantitatively, and it has a nice bridge between the qualitative
2562640	2567360	and the quantitative, which we think is important for being able to do this kind of work. And then
2567360	2572000	the last thing to emphasize is that these prototypes or simulations of these underlying
2572000	2577360	structures can be used to test interventions on a problem before you actually try to do it in the
2577360	2586640	real world. And so I'm going to walk through at a high level the basic visual notation that
2586720	2593200	utilized to prototype problems and system dynamics. One representation is called a causal loop
2593200	2597920	representation. You might have heard of causal loop diagrams or causal maps, causal maps. The
2597920	2604000	other is a stock and flow representation. Causal loop diagrams are fully qualitative. Stock and
2604000	2609760	flow diagrams are qualitative and quantitative. They're provide that bridge into the quantitative
2611040	2614560	world. So I'm going to walk through this example with a simple loan scenario model.
2614640	2618800	So the sort of factors you have when you're talking about loans are average credit score,
2619440	2625360	loans received, borrowers, who's defaulting on loans, who's making their payments.
2626720	2632480	So you have all those factors specified in language and words. And then you have an arrow
2632480	2640160	between factors with either plus sign or minus sign on top of it that illustrates the hypothesized
2640160	2646640	causal relationship between two factors. And the plus sign just illustrates the direction
2646640	2652000	of the impact that one factor is going to have on another. So to give an example, the plus sign
2652640	2657120	at the end of the average credit score arrow means that as the average credit score goes up in this
2657120	2663120	system, the number of loans received is also going to go up. But it also means as the average credit
2663120	2668160	score goes down, the number of loans is going to go down. So it just means that the impact is in the
2668160	2673360	same direction. The minus sign means the opposite. So the relationship between loan defaults and
2673360	2677760	average credit score has the minus sign on the arrow. That means as the number of loan defaults
2677760	2682160	goes up, the average credit score is going to go down. It's going to go in the opposite direction
2682160	2687040	as opposed to the same direction. If the number of loan defaults goes down, the average credit score
2687040	2693280	is going to go up in the system. You can also illustrate time delays, right? Because when one
2693280	2700160	factor impacts another, it may not happen right away. So in this system, we're saying as number
2700160	2705040	payments go up in a system, the average credit score in the system should go up. But that's not
2705040	2708560	going to happen right away. It'll take a while for the credit score, average credit score in the
2708560	2714720	system to be impacted. You can illustrate that that way. And then you have feedback loops, right?
2714720	2719520	This really important aspect of complex adaptive systems. There's two types of feedback loops.
2719520	2726240	One is a reinforcing feedback loop. These are basically indicators of exponential growth,
2727200	2733680	either vicious cycles or virtuous cycles. And so you can identify parts of the system that
2733680	2739520	could be driving exponential growth. In this case, the combination of borrowers increasing
2739520	2744720	and payments being made and average credit score going up, that could drive an exponential increase
2744720	2751120	in the number of loans in the system. So that's what the reinforcing loop represents. But the
2751120	2756000	balancing loop is for balancing out this exponential growth. So that's illustrated by the loop that
2756000	2762000	includes loan defaults. Because that's the one place that even as borrowers are increasing,
2762000	2766240	yeah, that leads to more payments, but it also leads to more defaults, which leads to a decrease
2766240	2771600	in average credit score, which taps down the growth in loans being given out. Can I ask? Yeah.
2771600	2775600	Because you're saying average credit score, this is meant to model what's happening in an
2775600	2780880	entire population. Yeah. Because of course, these feedbacks also take place in individuals' lives.
2780880	2788800	And so, yeah. Yeah. Yeah. I mean, system dynamics tends to take a macro view, right? It's looking
2788800	2794480	at populations as a whole. And oftentimes people use a combination of system dynamics for the macro
2794480	2800160	view and then agent-based modeling to have more specific kind of ways to model like individual
2801600	2806320	behavior. But this model might have different outcomes within different demographic groups.
2806320	2811280	Oh, yeah. Totally. Totally. Right. And again, this is just a way to kind of start to hypothesize
2811280	2816720	about these factors and you can get more detailed or less detailed. Yeah. You care if the average
2816720	2824320	credit score is a black box, but you cannot see it. Yeah. I mean, I see what's happening. Do I?
2824320	2830800	Yeah. Yeah. Yeah. I care. So we put this up just to kind of illustrate like the elements of it,
2830800	2836080	but as you do this work, you'll see that what happens more and more, you drill into these
2836080	2841680	black boxes and you try to expose the mechanisms under them. Sometimes you're not able to because
2841680	2847040	there's someone in control of that. Yeah. Because I was working with Citibank in terms of who they
2847040	2852480	should give loans to and fair loans, et cetera, et cetera. Yeah. And they're getting some information
2852480	2857920	from third-party folks, which they don't know. They do not know how the FICO score is being
2857920	2863280	computed. Yeah. And so it is a black box and they're going to use it. Yeah. It's a model.
2863280	2867520	It's a model. Right. That is a model that is being used to kind of make decisions.
2868480	2872720	Right. Yeah. Right. So then the question would be like, how do you actually quantify that in your
2872720	2880400	epistemic uncertainty? How much weight do you give to it? Yeah. I mean, that's a question. Right.
2880400	2888720	You have to, and this is something that the modelers contend with and negotiate. And this is why
2888720	2895600	you want to try to include in this modeling the people who are building those who are building
2895600	2900000	these credit score models. It's very, you know, it's, depending on what problem domain you're
2900000	2904560	working at, it's very hard to have participation from everybody that you want to have participation
2904560	2914960	from. Yeah. And then the stock and flow representation has all those same features.
2915760	2920960	You just represent those factors with quote, unquote, stocks and flows. What important aspect
2920960	2925760	of the stocks is it allows you to take into consideration accumulations in a system,
2925760	2931680	which really allows you to be able to consider the history of the system, the initial conditions
2931680	2936080	and how things grow and decrease over time and really get into that detail in terms of
2936080	2940800	relationship between factors. And this is where you start to get into the quantification,
2940800	2945600	because the way you represent those relationships is with differential equations, for example.
2945600	2949760	And that allows you to get to the point where you could start to simulate your hypothesis.
2949760	2954320	And that's, and system dynamics is not about, it's really not like, I'm going to solve this
2954320	2960240	problem. I'm trying to learn the problem. I'm trying to learn the problem system and increase my
2960240	2965360	confidence that I understand it well enough to start to intervene on it in the real world.
2968160	2973840	I just threw this in just to give an indication of what you have put this together. You can start
2973840	2980800	to kind of look at how these impacts show up in a simulation. This is made with something called
2980800	2987760	loopy. Anybody can play around with this. This is like a really oversimplified way to model these
2988080	2993040	sorts of systems, but at least it lets people to start to engage with it. There's really expensive,
2993040	2997520	hard to use software as well. And one of the issues is that we have to improve the accessibility of
2997520	3002320	these sorts of tools if we really want to build capability and capacity in communities. So now
3002320	3007920	we've got this kind of approach that we're like, ah, this might be a good way for us to start to
3007920	3016240	tackle producing societal context knowledge from places where we're embedded and proximate to
3016240	3021520	problems. But now we have to have some actual capability in the world to use these techniques
3021520	3026720	if we're going to have any chance of producing useful knowledge. So now I'm going to talk about
3027520	3030800	some experience that we've had over the past few years of trying to build that kind of trust
3030800	3035920	and capability. I'm just just putting up this reminder of the fact that, you know, the lived
3035920	3039920	experience expertise of historical marginalized groups is usually not involved in this process.
3039920	3042640	So when I started this back in 2017, I intentionally
3043920	3048640	wanted to work with folks from historically marginalized communities who are trying to
3049280	3052400	leverage data and math and science to understand and solve problems.
3053760	3060000	And there's a group called Data for Black Lives who does exactly that. It's a collection of activists
3060000	3065120	and organizers and mathematicians and scientists who are trying to leverage data science for good
3065120	3069760	and also try to make sure data sciences does not end up unintentionally making things worse
3069760	3076000	when we're applying it into high stakes domains. And so this started with me just going to the
3076000	3081040	first conference, not knowing what to expect back in 2017. And this led to a two and a half year journey
3081600	3087200	that resulted in a research paper and a causal theory that's starting to have an impact on
3087920	3092800	how Google evaluates and think about health diagnostic algorithms. So I just went to the
3092800	3098400	first conference with no expectations, not asking anybody for anything, just trying to figure out
3098400	3101840	what people were trying to do, what problems they were working on, what problems they were trying to
3101840	3107520	solve. And that's interesting people. Some of them famous now, you probably recognize.
3110480	3115280	And what I discovered there was that there was this need for people to be able to see
3115280	3121440	how the problems they were separately working on were connected. And now everyone was talking about,
3121440	3124800	you know, we have to understand the system, the system, but there was no real
3125760	3130000	concrete way to actually start to tackle that. So I said, Hey, this could be a good place to
3130000	3136160	introduce system dynamics and systems thinking. I talked to the founder. Yes, he mill in Europe.
3136160	3141040	She didn't know me from Adam. And so I had to convince her that this could be useful. So I
3141040	3145200	introduced her to it with a mini workshop. And then we agreed to set a goal of actually
3145760	3152080	delivering a workshop to that's larger community at the next conference that they were going to
3152080	3157280	have, which was about a year away. But we had one principle, we said, the people who are going to
3157280	3163200	teach this workshop to this community, they have to have similar lived experiences to the people
3163200	3171920	that are going to be receiving this knowledge. And not a lot of diversity in the world of system
3171920	3176480	dynamics when we started this has changed over time. So we said, we have to, we have to teach
3176480	3181040	the teachers, we have to create some teachers that can deliver this workshop. And so we did a
3181040	3185040	learning lab that included people from that community, as well as people from Google to
3185040	3188480	teach them these techniques so that they could teach the bigger workshop.
3190880	3194800	We did that over a couple of months, they were able to teach this workshop and these techniques
3195360	3199680	to a larger group of about 75 people was received very well.
3200640	3205520	And then we had this capability, and we're like, okay, and a little bit of capacity. What should
3205520	3211200	we do with it to try to further this journey? And so we decided to do a problem prototyping
3211200	3218400	experiment around a problem domain that Google cares about and data for Black Lives Care is about.
3218400	3224080	And that was health diagnosis. Data for Black Lives Care about it because under diagnosis can
3224080	3228880	lead to health disparities. We talked about that earlier. Google cares about it because,
3228880	3233200	you know, we're working on building health care diagnostic algorithms. We want them to be
3233200	3237680	accurate and work well. But there's a lack of training data that can lead to inaccuracy.
3238480	3247600	And so we thought this would be a good area to jointly prototype a problem that relates
3247600	3255200	algorithms and lack of data and health care disparities as a way to start to bridge that
3255200	3260960	chasm. And so we brought people together. We got about nine people together from that initial
3260960	3266720	workshop. They developed one of these college loop diagrams for that problem domain. I'm not going
3266720	3270480	to go through this in detail, but you can look at the paper, look at all the data.
3272240	3276320	But one key thing is that it highlighted trust in medical care as a key factor
3276320	3280800	that drove multiple balancing and reinforcing feedback loops in this system.
3280800	3288560	They also created a stock and flow model for this. Trust shows up as this kind of
3289200	3296240	really big substructure that you can double click in. And the thing about doing this work is that
3296240	3303120	it allows you to think about these soft variables and include them and approach
3304800	3308720	bridging that qualitative quantitative gap, quantifying them in some way,
3308720	3314560	but informed by people that approximate to the problem, but also informed by previous research
3314560	3320080	that's happened. And so they developed a really detailed trust substructure, which I think actually
3320080	3328960	should be in a whole lot of models. And that allowed them to develop a simulation and actually
3329520	3332320	test some of the interventions that they had in mind that might make things
3332960	3335680	better to include the reference model that we were solving for.
3338240	3344320	Now, the key, my key takeaway from this was that, hey, you can do community-based research and
3344320	3352000	actually produce useful research that comes from a partnership with community and people that are
3352000	3354960	approximate to problems and better than problems. So this paper actually got
3355680	3360880	accepted, presented at the System Dynamics Conference, what an honorable mission award.
3363280	3369680	But the key thing I learned is that you can go after if we want to build capability in any place,
3369680	3375360	but you cannot do that without building trust at the same time. And there's no way to rush this,
3376080	3382880	right? It takes determination and patience and investment, but that's going to be one of the
3382880	3387040	key ingredients if we are going to have any sort of chance to bridge this chasm, in my opinion.
3387360	3394720	So I'm going to conclude with, first, that key takeaway that I started with. Interventions that
3394720	3402160	ignore abstract way societal context can lead to unintended and unnecessary harm. I think they're
3402160	3410080	likely to. And then those calls to action. If we're going to drive more attention and research in
3410080	3417200	this space, we need folks like you all to spread the word. And then we also are really calling
3417200	3423680	for people to embrace this concept of prototyping problems before you prototype solutions, prototype
3423680	3430880	problems as systems, embrace that complexity before intervening with data science or AI,
3431440	3437280	really with anything. And we think this is really one of the only ways to really proactively mitigate
3437920	3443360	bias in these high stakes domains. And then finally, invest some of your time and energy. And if you
3443360	3449840	have dollars, dollars in building problem prototyping, trust and capability, like in historically
3449840	3454560	marginalized communities. With that, I'll close. Thank you, everybody.
3459200	3461520	So we are missing one time, but there are a few questions.
3461760	3466160	Yeah. Did you just work at Google? I'm sorry. Did you used to work at Google? No, no.
3468560	3475360	So I'm wondering, great talk. Thank you. So I'm like studying in public health. And so one thing
3475360	3481120	we're finding I'm thinking is that the problems start outside the hospital. So those precepts
3481120	3486640	and the clinicians come from outside the hospital and it becomes a problem once we get in. So how
3486640	3492240	much of the modeling should we think about outside hospitals in terms of like social and public
3492240	3499600	investments, housing investment, basically treating that environment in the society and having that
3499600	3505280	gradient sort of diffuse into the health care industry somehow. I think trying to solve the
3505280	3511360	health care problem is a hard one. And then it seems like you have to go outside a little bit.
3512320	3517360	Like we look at your way or somewhere like they invest in their public more than any of us probably
3517360	3521680	in health care. And we do the opposite. Yeah, we're having this problem. I agree. I agree 100%.
3521680	3527520	Like, I don't know. I may not have been clear in this talk, but what I think of things I emphasize
3527520	3532720	in Google is that we have to start with trying to understand the problems that people care about
3532720	3538320	outside of Google, independent of like our business problems. If we really want to actually,
3538400	3543520	I think this actually leads to better business as well, start outside. What are the people outside
3543520	3547920	in society? What problems do they care about and why? And how do we get to the point where we
3547920	3552640	understand those problems? And then you're going to find some intersection in that problem domain
3555120	3560160	between the problems that society cares about and the problems that we may have some hammers and
3560160	3566000	nails to try to solve technology. But you have to start outside of that in order to really start
3566000	3570960	to get any kind of ground truth. Yeah, it's like, you need to understand what the problem is. Yeah,
3572000	3575680	I agree. That's why that's why I'm trying to talk about this problem understanding chasm because
3575680	3581040	of this lack of understanding of the problem, particularly on the product development side.
3581040	3585120	But but there's multiple ways to understand the problem. I think there are ways to leverage
3585120	3591040	technology in good ways. But unless we have a deeper understanding that's shared between
3591040	3595600	these two perspectives, we're not we're not going to be able to do it in a responsible way.
3599360	3604080	I guess one thing you didn't mention that I expected you mentioning would be accountability,
3604080	3612080	right? So for example, perhaps the reason some of the AI developers now are okay with the system
3612080	3619600	saying no is Google search was always retrieving. Now with gen AI is generating stuff. So if you're
3619680	3625440	generating something that causes me harm, then there's accountability to be had, right? And
3625440	3629920	the laws are still not there. We'll see, right? Like for example, in Europe, in certain European
3629920	3635040	countries, they abandon these kinds of large language models because people harm themselves
3635040	3640960	based on what was told to them, what was generated as opposed to retrieval. So what are some of your
3641600	3648960	thoughts on accountability here? Who's accountable if the system if the algorithm causes harm to
3648960	3654160	the community? Well, so you're trying to give me to answer one of these questions that are that's
3655040	3660560	you can pass it. But it's it's it isn't it you we are all part of the community, right? So we
3660560	3666720	it's yes, I'm a computer scientist, but I'm also part of the community. Yeah. Yeah, I mean, I think
3666720	3675360	so. So I think there's accountability in multiple places. I think the works that that's
3675440	3684160	happening, that's emphasizing, you know, how we govern these, the development of these tools
3684160	3690400	and products is really critical. If you pay attention to what's happening, what's coming out of
3691440	3700320	the national out of NIST, they've got a framework that emphasizes that the very first step that
3700400	3706480	has to happen for companies developing these systems to be responsible and accountable,
3706480	3712080	it has to start with understanding context, which I think aligns directly with what we're talking
3712080	3717760	about here in terms of understanding societal context. But I but again, like you said, there's
3717760	3725280	all these incentive structures that will kind of resist that. And in the in the product development
3725280	3731920	companies, but even outside, right? Right? Yeah, because the accountability is tied to the incentives
3731920	3738080	and incentives then are tied to gaming the system, including the the the algorithm, the tool, and the
3738080	3741920	bigger picture that you had, right? And so they ground around the goals. That would be interesting
3741920	3746320	to model. You're like, you're like, you're that would be interesting to see what that, you know,
3746320	3750000	I would love to see how you would hypothesize what that relationship is between those faculty.
3750000	3755840	Oh, I could definitely do that. I'm a I'm a professor. Yeah. Yeah.
3759920	3765600	Talk. And I'm typically interested for the final point about gathering a trust from the
3765600	3770720	historical margin of people and getting and getting involved all the domain experts about the
3770720	3778080	particular problem. And that maybe that you came from Google, and it's like a big company,
3778080	3782320	which can we have to deal which have a capacity and maybe some resources to
3782880	3787120	open a workshop or getting a workshop and getting involved for many. But
3788320	3793040	even if we just concentrate on the historical margin with people, there's so many different
3793040	3798240	people around there. And maybe you have to think about so many different parts of other people's
3798240	3803840	and maybe not every company or production, the product maker have a resources or capacity to
3804560	3810640	do build a certain kind of trust like that, although it is very important. Yeah. And so
3810640	3816000	will be will there be any kind of like a substitute or other options to incorporate this or like an
3816000	3823760	accelerating these procedures to, well, generally like an accessible to more companies or more
3823760	3829680	industrial parts of this person, rather than getting getting like an opening a workshop and
3829760	3834160	getting a building a relationship like an after like a more than five or 10 years.
3834160	3839360	I think if it if there's some kind of other options or like if we can encourage other
3839360	3845680	entrepreneurs to do this things more, I think it'll impact a lot. I would just know your
3845680	3851280	like opinion about this. Yeah. I mean, so I agree with you 100% like it's a really hard problem.
3851280	3858800	And I think this has got to require investment like not just for not just from product companies,
3858800	3867760	because in some way, you know, we're not we're not in a great position to really be fully embedded
3868400	3874480	in the problems that society cares about surely. Right. And so I think this is the option where
3874480	3879040	places like Google need to see some leadership and some ownership in this space and kind of
3879040	3887040	invest in capacity outside. So we have a gigantic sector of philanthropy, foundations, etc.
3887760	3892560	who I think need to play like a critical role in this. One of the reasons we're trying to
3892560	3897680	raise the awareness of this is just for what you described to get other people thinking about
3898240	3906160	solutions to these really hard problems of like, you know, scale and and dealing with
3906160	3911680	incentives that have to do with speed. Right. So not easy things to solve. But so this is why
3911680	3916800	we're like, let's raise awareness, get people talking about this, and have more people
3917600	3923280	applying their mind to finding solutions that are kind of responsible, non extractive,
3923280	3928080	can accelerate things. I think there are ways to solve these. This is kind of like a moonshot.
3928080	3932880	There are things you would have to invent that haven't been invented yet. We're trying to kick
3933280	3936400	off the flywheel to get those conversations going. Yes.
3938400	3946160	So one thing I'm interested in is what kind of pushback. It's like one phrase you didn't use,
3946160	3954480	but I'm sure that we agree on it. It's also one of our favorite things to rail against
3954480	3961280	is domain agnosticism. Right. Like, like, oh, I'm the clever technical person, send me your data,
3961280	3965600	send me your zeros and ones. I don't even need to know what the zeros and ones mean.
3965600	3970160	I have cool algorithms and then I'll help. Right. And so part of the goal is like
3970880	3977840	breaking that as, you know, what I think still for a lot of engineers and engineering students,
3978400	3983760	it's still kind of a norm and sometimes even held up as a virtual. So I mean, I'm totally,
3984720	3991600	I love what you're pointing out. I guess I'm interested in two things,
3991600	4000160	and I'm looking forward to our conversation later. One is the move here is to work with the community
4000800	4005760	to generate models at first qualitative, but with the ambition of being quantitative.
4006960	4011280	And of course, that's an interesting move. It's not the only move. And
4012080	4019120	and then once you have these models, then lots of other tricky questions come up. Like, do we
4019120	4024080	really think these models are going to be predictively accurate or are they just sort of scenario
4024080	4029200	explanation tools or kind of, you know, are they quantitative models that we're actually trying
4029200	4035840	to learn about qualitative effects of possible interventions? So that's, that's, I think that's
4035840	4042720	an interesting and I like it. But I also I'm curious about its limitations. Yeah. And one of
4042720	4049440	the limitations that I expect also some engineers might push back on. So like, in the criminal
4049440	4057040	justice field, right, everybody says, absolutely, arrest is not the same as crime. It's not just
4057040	4062480	a very noisy signal. It's also a systematically biased signal. And some people say, well, could we
4062480	4069360	use conviction? It's like, well, that's also a noisy and biased signal. And of course, you know,
4069360	4077760	society, neither the government nor engineers have access to did a crime actually occur.
4079440	4086000	And so you get this sort of defensive reaction that, well, but this is the data we have, you know,
4086000	4090320	so like, I think we've all been to a million talks where there's one slide at the beginning
4090320	4095040	saying, arrest is not the same as crime, but this is the data we have, you know, and then,
4095040	4100800	and then plunging into the technical, you know, the statistics of that as data. And I don't know,
4100800	4105760	that's, I mean, maybe this just gets back to the humility question again about, you know,
4106800	4111520	hey, we don't know if this data means what we think it means, we don't know if it's measuring
4111520	4119280	we think it measures. And therefore, we're presenting our suggestions, our recommendations to
4119280	4127680	a judge or a doctor with some humility, but then I don't know, I don't know how to wrestle with that,
4127680	4133120	especially because part of the argument for doing all this is that humans clinical judgment is also
4133120	4142560	very biased or can be. And so I don't know, there's like, which things can cross the the
4142640	4153280	chasm into something which is structured enough that to to mathematics, and then
4154160	4160560	to improve the algorithm, and which things can't really, and then the algorithm just has to say,
4160560	4165520	hey, I don't know about that and keep in mind that I don't know about that, which as you said,
4165520	4169120	as Tina said, is, you know, vendors are not usually incentivized to do.
4170080	4173360	Yeah, I don't know, so that's like where that boundary is, I think is really interesting.
4174320	4176800	Yeah, yeah, and you and you pointed out one of the
4178400	4184960	the key root causes of this of this of this chasm, and that's this, this is the data that I have.
4188080	4192240	Don't force me to kind of think about that messy stuff, right, on the other side of this
4192240	4197600	boundary, the techniques that I have, I'm incentivized by those techniques, and they
4197760	4204000	incentivize me to abstract those that stuff away. Now, and also the conversations and then it always
4204000	4210720	goes to, I've spent a year talking to engineers about like, ah, we can, we can, we're gonna work
4210720	4214320	on building these models. And the first question I get, but yeah, but how do you know they're
4214320	4220400	accurate? How do you know they're predictably accurate? And then my pushback is, how do I know
4220400	4226560	yours are predictably accurate? Right, you're very confident in all the assumptions you're making
4226560	4231680	throughout that life cycle. No one's looking at them, they're not made explicit, right? And so
4231680	4236320	eventually you actually get over that hump where people can say, oh, I can see the utility at least
4236320	4242240	in the beginning of this, of me being more informed. And one of the key things about system dynamics,
4242240	4249440	it emphasizes, don't try to start with data. Don't try to start with the data set. Start with your
4249440	4256240	intuition and the intuition of others about what's important in this system, and start off by trying
4256240	4262960	to get some sort of, you know, shared hypothesis. And then that should drive what data do I need if
4262960	4268080	I'm trying to intervene on a problem in this system. And if I don't have data about this factor,
4268080	4273440	maybe I shouldn't be trying to intervene on this particular problem right with this technology.
4274320	4278320	And so this is why we're, you know, this is why we're kind of purposely focusing on
4278560	4286480	high stakes domain, healthcare because of, you know, the issue of health equity,
4287280	4293280	healthcare industry is kind of, I think, a little bit more mature in terms of looking for systemic
4295520	4300800	causes of disparity and kind of embracing that. So there's kind of more openness about like,
4300800	4305600	hey, we're trying to, we want to use these algorithms, but there's really a lot of
4305600	4309680	worry and caution by clinicians and others because they know how dangerous they are.
4310400	4315200	I think this is again where the accountability comes in, right? Because the doctor can use a tool,
4315200	4319680	but at the end of the day, he's accountable, right? And to the question that I always get is like,
4319680	4326880	well, the system is, but your AI system is biased, but the person who's making the decision is also
4326880	4331360	biased. Yes, but the person who's making the decision, he can go to jail. I will sue his
4331360	4337760	ass from here all the way to Boston, right? That AI system, who am I suing? Who's responsible?
4337760	4344480	Yeah, right? It's not clear. And that is important, right? It is important to figure out who is
4344480	4348880	accountable, especially in America, right? We like accountability, like, who do I blame?
4350640	4357600	Nobody. I go to jail, nothing happens, right? Because the system said I'm high risk. And that's
4357600	4362720	it. So the systems are being used as expert witnesses without being cross-examined and not
4362720	4368320	being held accountable. I think in healthcare, it's a really good domain to pick, because at the end
4368320	4374400	of the day, it's a doctor and the malpractice insurance he has. And so they have, you know,
4374400	4377760	like you said, there's a vested interest in making sure these things work out. And I think there's
4377760	4382720	also like, you know, the Hippocratic Oath as well, like doctors don't want to harm people, right?
4383440	4384000	I'm large.
4385280	4390800	But so what you're pointing out is like, you know, these technologies are like
4390800	4397200	really, really serious interventions on society. And you're talking about like, we have to update
4397200	4403760	our, the cannons of law, right? For these entities, these systems.
4404800	4409360	Yeah. And I think especially now with the movement on democratization, right? I want to
4409360	4416160	democratize X. Well, then this kid can go and develop clear view AI with like $200,000. No
4416160	4421440	accountability. He didn't know about ethics or nothing. You know, it's what, wait, what?
4422320	4428720	Yeah. So there are issues. I mean, I love your research area. I mean, it's similar to research
4428720	4434240	area I've been pursuing. But we cannot not talk about accountability. Like we have to talk about
4434800	4439840	Yeah, I agree. And I think we also have to make sure that at least for me, like
4441680	4447280	I'm not, I'm not like, I, I can like, at least me personally begin to kind of understand like,
4447280	4451040	all right, what do we, what do we have to do to update the cannons of law? Like they're experts,
4451040	4456240	like that's one of the things I like about Andrew Selbs, his work, right? He's a law professor,
4456240	4460800	right? So, and that's also why I like about his work about abstract and waste societal context.
4460800	4466160	Because I think hopefully that means that kind of perspective will make its way into the laws.
4466160	4470960	You see that perspective making its way into some of the frameworks that are coming out of
4470960	4474960	the government in terms of how we're going to regulate these things. So it gives me some hope.
4474960	4479840	But I do know that even if you come up with a law that says you have to understand the societal
4479840	4487760	context, we are not in any position to actually represent it, present it, whether it's in a,
4487760	4494160	you know, court of law or in a product development life cycle. And so we have to build capacity.
4494160	4499600	Whereas somebody has to pay millions and millions of dollars, then people will, will move.
4499600	4503600	Unless they have billions of dollars, then they, maybe we'll charge them billions.
4503600	4505760	Billions. Billions to get to billions. I want billions.
4508240	4511840	Sadly, we're coming up on time. But Donald, I want to thank you again for a great talk.
4512160	4514480	Thank you for your time.
4516960	4520160	For tomorrow afternoon, I know a number of you already signed up for meetings,
4520160	4524960	but if you haven't, he's down in Pod C. So you're welcome to go and
