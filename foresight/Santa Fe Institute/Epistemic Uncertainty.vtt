WEBVTT

00:00.000 --> 00:05.680
up in the kitchen. Very excited to introduce Donald Martin, Donald's the head of societal context

00:05.680 --> 00:10.400
and understanding tools and solutions at Google. We don't do much for introductions here, but

00:10.400 --> 00:18.240
Donald let me just turn it over to you. You won't appreciate it. Hello everybody. It's really an

00:18.240 --> 00:22.560
honor and pleasure to be here at Santa Fe Institute. I actually have been dreaming about visiting

00:22.640 --> 00:29.680
SF5 for, I don't know, almost 16 years. I read the book Complexity back in 2007, about 2005. I was

00:29.680 --> 00:36.240
like, I maybe want to go there. And so I'm now here. I'm here. Thanks to Will. Thanks for your

00:36.240 --> 00:43.280
time and attention. The title of my talk is Epistemic Uncertainty, the AI problem understanding

00:43.280 --> 00:48.640
chasm. I kind of updated that from a gap to a chasm, but it's a really big problem. And the necessity

00:48.720 --> 00:55.920
of structured societal context knowledge for safe and robust AI. Now me and my team, which also

00:55.920 --> 01:01.840
goes by scouts, we believe that bridging this problem understanding chasm is critical to being

01:01.840 --> 01:08.560
able to realize our goal of responsibility for this AI. And so we're kicking off a campaign

01:09.440 --> 01:15.360
to try to imply more participation by others in trying to understand and solve this problem.

01:16.320 --> 01:21.680
And so we've got something we call our flywheel. That starts off with increasing awareness just

01:21.680 --> 01:28.080
about the chasm and what the causes are. We hope that drives more foundational and applied research,

01:29.200 --> 01:33.040
particularly in some of the techniques we're going to talk about causal theory model, for example.

01:33.840 --> 01:40.480
And of course, we want that to result in real impact in the world. The use case that we focus on is

01:40.480 --> 01:46.880
health equity. And so right now, we're here's where we are. And so we're kicking this off and

01:47.840 --> 01:52.480
you know, bridging the problem understanding chasm as you will see, we think requires embracing

01:52.480 --> 01:58.720
complexity. So I figured what better place to start talking about this in the Santa Fe Institute.

02:02.400 --> 02:09.200
Few calls to action because I know folks in this room, SFI is influential. So we would love for

02:10.160 --> 02:15.040
after the talk, people to spread the word about this chasm, right, and how important it is to budget.

02:16.720 --> 02:22.640
Hoping that people will also take on the task of embracing prototyping problems as complex

02:22.640 --> 02:27.840
adaptive systems before intervening things like data science and AI because we think that's critical

02:27.840 --> 02:34.400
for proactively mitigating bias in these systems that we're building. And then finally,

02:35.280 --> 02:40.480
a key component of bridging this gap is investing and problem prototyping

02:41.200 --> 02:45.760
trust and capability in historically marginalized communities. That's going to be a key source of

02:45.760 --> 02:50.160
the knowledge that we think is missing from AI based product development.

02:53.200 --> 02:55.520
Okay, so with that, I'm going to set just a little bit of context.

02:56.880 --> 03:00.880
I gave this talk a part of this talk last year, I had to kind of explain there's this industrial

03:00.960 --> 03:04.480
revolution. AI is going to be really important. I don't think I have to really do that anymore.

03:05.200 --> 03:09.760
I think you realize we're in the midst of this transformation that people were calling the

03:09.760 --> 03:15.360
fourth industrial revolution and that AI is like a core technology within it. And of course,

03:15.360 --> 03:20.800
you know, we have high hopes that AI is going to do these amazing things and transform healthcare

03:20.800 --> 03:28.160
and medical care. But we're also very cautious, rightly so because of the big conflict in the room,

03:28.720 --> 03:31.920
harmful bias that can be propagated really easily by these systems.

03:33.120 --> 03:36.400
And so it's part of my job to pay attention to all the headlines that come out every day

03:37.040 --> 03:44.000
about instance of bias and all sorts of domains. But I pay particular attention to

03:46.320 --> 03:51.680
headlines that involve healthcare and medical care. And that's really because of my mom.

03:51.760 --> 03:56.800
With my mom, Betty Martin, she died five years ago this month from cancer.

03:58.160 --> 04:04.240
And as I navigated that harrowing journey with her, I experienced bias directly in the healthcare

04:04.240 --> 04:10.240
system. And that really informs how I think about what it's going to take to mitigate bias in AI

04:10.240 --> 04:15.200
systems applied in high stakes domains like healthcare. And so as I said, she died from cancer,

04:15.200 --> 04:20.960
started off as breast cancer, turned into lung cancer. At one point she had to get part of whatever

04:20.960 --> 04:26.800
lungs removed. And as a result of that, she developed atrial fibrillation. And so that can

04:26.800 --> 04:33.600
cause blood clots and strokes. So she had to get prescribed a blood thinner. But the blood

04:33.600 --> 04:39.360
thinner that the doctor wanted her to take would require her to stop eating vitamin K foods,

04:40.080 --> 04:46.800
leafy greens, spinach, kale, collard greens. I love that stuff. That was like a staple in her diet.

04:46.800 --> 04:49.840
And it would also require her to have to go to the hospital

04:50.480 --> 04:54.240
every week to get her blood tested to make sure this blood thinner wasn't harming her in some way.

04:55.840 --> 05:00.240
And so my mom, she balked at this. She's like, ah, you sure there's not another

05:00.880 --> 05:04.320
option? Is there a better blood thinner where I don't have to shift my lifestyle?

05:05.200 --> 05:10.480
And the doctor's response was, you should just, you know, be grateful because there are many

05:10.480 --> 05:15.200
people in third world countries who would die to have access to this particular blood thinner.

05:15.920 --> 05:18.720
My mom was like, well, you know, I don't know what that has to do with me, but

05:20.400 --> 05:24.320
and you all know my mom, of course, but she didn't accept that as an answer.

05:25.120 --> 05:30.480
She talked to me and my sister, my dad. My sister happens to be a medical doctor, a psychiatrist.

05:30.480 --> 05:34.880
So she was able to find very quickly an alternative blood thinner that wouldn't have

05:34.880 --> 05:41.280
required my mom to change her lifestyle when I owed her. And so, and we could easily afford it

05:41.280 --> 05:46.160
with my dad's health care insurance. So we went back to the doctor. We said, hey, we found another

05:46.160 --> 05:51.360
alternative, but we ought to make sure we're not missing something. Like, why didn't you

05:52.000 --> 05:55.360
recommend this particular blood thinner because it doesn't have any of these side effects?

05:56.240 --> 06:01.680
The doctor's response was, I assumed that she wouldn't be able to afford the alternative.

06:03.520 --> 06:08.000
I assumed she wouldn't be able to afford the alternative. So that's this bias kind of

06:08.080 --> 06:12.800
rearing its head right in the middle of the situation. So that's why this kind of informs,

06:13.760 --> 06:17.280
you know, my thought process about how we're going to, what it's going to take to mitigate

06:17.920 --> 06:25.520
these kind of really nuanced implicit biases. And so the real issue here is that the doctor ignored

06:25.520 --> 06:31.760
and abstracted away my mom's, her situation, her circumstances would all call her a societal

06:31.760 --> 06:37.120
context. Didn't consider that when they made that intervention recommendation. So I want to look at

06:37.120 --> 06:42.720
my mom's societal context in a different way. There's mom, born and raised in the USA, doesn't

06:42.720 --> 06:47.520
know anything about, you know, what happened in third world countries. She loved leafy greens.

06:48.720 --> 06:53.760
This is her second bout with cancer. So she, you know, had already experienced having to go to the

06:53.760 --> 06:57.920
hospital every week to get chemotherapy and get stuck with the nail show. So she would avoid that

06:59.040 --> 07:04.480
anywhere she could if it was possible. And then, you know, she had us as a family, which included

07:04.480 --> 07:11.040
my sister, medical doctor knew how to navigate the drug industry very well. My dad, a double retiree

07:11.040 --> 07:18.640
from US Army and the Postal Service, great healthcare insurance. So we had this, this other option.

07:19.520 --> 07:24.320
But the doctor ignored and abstracted away all this societal context, didn't ask about it,

07:24.320 --> 07:29.120
didn't consider it and then made this intervention recommendation that conflicted with my mom's

07:29.200 --> 07:30.080
societal context.

07:31.680 --> 07:38.320
Conversely, do you think that the doctor thought he was being anti-biased by making

07:38.320 --> 07:42.080
this mistake and thus, in fact, being very biased?

07:42.640 --> 07:50.560
You know, we really have, I have no idea. I don't, so we all have biases, right? We can't

07:50.560 --> 07:54.720
tell what someone's intention is, right? I'm sure there's not a negative intention.

07:55.520 --> 08:01.600
But, but this is the result that when you don't seek to understand like the situation or

08:01.600 --> 08:07.680
circumstances, when you make assumptions, it can lead to these problems. And so we really

08:07.680 --> 08:12.800
use this high-level definition of societal context as this dynamic and complex collection of

08:13.600 --> 08:20.000
social, cultural, economic, political, historical circumstances that surround people,

08:20.000 --> 08:27.600
places and things. And interventions that ignore or abstract away societal context can lead to

08:27.600 --> 08:32.160
unintended and unnecessary harm. So this is the key takeaway for the talk, like if you don't

08:32.160 --> 08:36.480
remember anything else, please remember this and try to get that in as you're intervening in

08:36.480 --> 08:43.440
technologies. So now I'm going to dive into the core of the talk. Here's, here's the five parts

08:43.440 --> 08:46.880
I'm going to walk through. First, I want to talk about how we abstract away societal context when

08:46.880 --> 08:51.920
we do ML and AI based product development. Then I'm going to introduce this problem, understanding

08:51.920 --> 08:58.640
chasm. I think it's the root cause of why we do this, abstracting away. And then the last three

08:58.640 --> 09:03.600
seconds are about what we think some of the elements of bridging this chasm are. One is we

09:03.600 --> 09:10.880
think we need some sort of reference frame or model of societal context. Second, we think we need

09:10.960 --> 09:17.920
a way of producing societal context knowledge and we think prototyping problems as complex

09:17.920 --> 09:22.560
adaptive systems could be a way to do that. And community-based system dynamics involves

09:22.560 --> 09:26.960
communities in doing that work. And then finally, even if you have this great technique,

09:26.960 --> 09:32.240
you have to have capability in the world in order to like execute it. So let me dive into the first

09:32.240 --> 09:37.760
part. I'm going to start this off with a quote from a great paper called Fairness and Abstraction in

09:37.760 --> 09:44.560
Social Technical Systems by Andrew Selps, who's a leader, not leader in the ML Fairness community.

09:44.560 --> 09:48.240
If you haven't read this paper, even if you don't care about ML Fairness, I highly recommend it.

09:49.360 --> 09:54.560
But this quote sums it up really well. Bedrock concepts in computer science such as abstraction,

09:54.560 --> 10:00.240
render technical interventions, effective and accurate and sometimes dangerously misguided

10:00.240 --> 10:05.280
when they enter the societal context that surrounds decision-making systems. So you could apply this

10:05.280 --> 10:10.240
directly to what happened with my mom. In this case, we're talking about building products and

10:10.240 --> 10:16.960
systems. So I'm going to just walk through a really high-level view of typical AI product

10:16.960 --> 10:20.960
development lifecycle. I'm using a product development context because I work at Google

10:20.960 --> 10:26.640
when we build AI-based products. The key thing they're recognizing this is that this whole process

10:27.360 --> 10:32.240
is driven by human decision-making from end to end. A lot of times people forget that.

10:33.200 --> 10:39.840
And the main output, of this lifecycle, is a model-based product that generates

10:39.840 --> 10:46.000
outputs, typically predictions, that somehow can help you automate or automate a process,

10:46.000 --> 10:50.480
solve some sort of problem. Of course, to produce that product, we have to train and

10:50.480 --> 10:55.440
tune and evaluate that model. To do that, we have to select and prepare some training data.

10:56.560 --> 11:01.760
To do that, we actually have to actually formulate this problem for machine learning and AI.

11:01.760 --> 11:06.080
We have to take that problem and make it fit the hammer that we have, that we want to use to

11:06.080 --> 11:09.520
solve the problem. And then in order to do that, well, we have to actually have a deep

11:09.520 --> 11:15.280
understanding of the problem itself. So that's typically kind of where this diagram ends.

11:15.920 --> 11:19.280
But of course, there's a key input that drives this whole thing, and that's

11:19.840 --> 11:27.600
observations and data. But sometimes we forget about where those observations and data come from.

11:27.680 --> 11:30.320
They're coming from this thing we're calling societal context.

11:31.280 --> 11:35.440
And then the machine learning fairness, sometimes we call it the social-technical environment.

11:36.560 --> 11:40.160
Computer scientists and engineers and practitioners who are trying to get things done,

11:40.160 --> 11:45.680
they have many different words for, you know, that area, this kind of ambiguous area,

11:45.680 --> 11:49.760
amorphous area. It's the real world where this stuff is going to end up one of these days.

11:49.760 --> 11:56.800
It's the environment. It's the data-generating process. It's where we have to look to kind of

11:56.880 --> 12:02.160
understand the hypothesis space. It's that problem domain. It's where that background

12:02.160 --> 12:08.960
knowledge is going to come from. It's the broader context. And then we have some characteristics

12:08.960 --> 12:17.360
that we associated with this amorphous thing. It's messy. It's complex. It's qualitative.

12:18.320 --> 12:27.760
It's causal and nonlinear. It's subjective. I need to work on objective things. It's dynamic,

12:28.720 --> 12:32.080
and it's historical. Why do I have to worry about what happened decades ago?

12:33.840 --> 12:38.800
Now, the issue is that those characteristics don't align very well with the statistical

12:38.800 --> 12:46.480
processes and methods that we use to build AML models. So that's why we tend to

12:47.360 --> 12:51.920
put up this abstraction boundary and abstract it away. And we have particular practices and

12:51.920 --> 12:59.920
assumptions that reinforce that. One of those is the IID assumption, which assumes that the

12:59.920 --> 13:04.000
variables we're going to try to learn in the training data assumes that there's no

13:05.200 --> 13:08.480
that there's no relationship between them. There's no causal relationship between them,

13:08.480 --> 13:14.800
among other things. And so that assumption, you make that assumption, you're ignoring

13:14.800 --> 13:20.080
abstracting away the very nature of societal context, which is supplying all the data.

13:24.080 --> 13:28.640
Now, the other thing that happens when we extract away the societal context is easy to

13:28.640 --> 13:32.880
forget that eventually you're going to be done with training, the products are going to be in

13:32.880 --> 13:36.400
the real world, and the inputs aren't going to be the training data anymore. They're going to

13:36.400 --> 13:41.280
come from the real world. And those inputs are not going to follow the rules, the assumptions

13:41.280 --> 13:45.520
that we made when we built the systems. And then those outputs are not going to be in the

13:45.520 --> 13:51.040
laboratory anymore or in the evaluation suite. They're actually going to be interventions

13:52.080 --> 13:55.200
on that societal context on society. They're going to cause impact.

13:58.640 --> 14:04.160
And then, of course, those interventions can have an impact on the next round of observations

14:04.160 --> 14:10.880
and data we sample in order to build the next version of that product. So you've got this feedback

14:10.880 --> 14:16.560
loop that can be vicious if we're not careful about the outputs and the quality of the outputs.

14:18.880 --> 14:22.400
Now, the other thing that happens when you're abstracting away

14:23.440 --> 14:27.760
societal context and understanding the details of the data-generating process and the problem

14:27.760 --> 14:34.640
domain is it leads to a lack of knowledge on the other side of that boundary, lack of knowledge

14:34.640 --> 14:39.040
about the problem domain and the data-generating process, things you have to understand to formulate

14:39.040 --> 14:46.080
the problems well. Now, another term we use for that in machine learning AI is epistemic

14:46.080 --> 14:50.720
uncertainty, just a fancy word for lack of knowledge. Usually, we're talking about lack of

14:50.720 --> 14:57.760
knowledge about the model by the model, but that lack of knowledge also applies to all the decision

14:57.760 --> 15:01.680
makers in this ecosystem, which include the humans who are making all these critical decisions.

15:03.040 --> 15:08.960
And in reality, models are basically inheriting the epistemic uncertainty of the decision makers

15:09.120 --> 15:15.440
who are making all the pipeline decisions that are going to impact how that model performs it with

15:15.440 --> 15:22.880
the outputs. Now, it turns out that epistemic uncertainty is the root cause of two big problems

15:22.880 --> 15:29.360
in AI. One is harmful bias propagation, which I'm going to talk about in more detail in a second,

15:29.360 --> 15:37.280
but the other is deep learning robustness. And this also goes by out of distribution

15:37.280 --> 15:42.560
generalization. Distribution shift fits into this category. So, I'm going to drill into that for

15:42.560 --> 15:48.560
just a second to show how experts in this problem are, when I read them, they're saying, we need

15:48.560 --> 15:56.800
societal context knowledge to solve this problem. So, the fundamentals of this deep learning

15:56.800 --> 16:02.080
robustness problem is this example where you have a classifier that's trained to identify

16:02.080 --> 16:09.280
objects and images, gets really good at it, high performance accuracy, really good at identifying

16:09.280 --> 16:16.720
this pig. But if you just change very slightly the input data, the image, in ways that would be

16:16.720 --> 16:21.840
imperceptible to a human, the classification changes to like, ah, this is an airliner.

16:22.800 --> 16:27.760
Right? So, that's the kind of classic example. And so, the issue here is that the classification

16:27.760 --> 16:33.920
model doesn't have an equivalent conceptual model that we have. And these are kind of structures

16:33.920 --> 16:40.480
in our heads that allow us to reliably identify objects under really noisy conditions.

16:42.000 --> 16:47.760
And so, the experts who are trying to solve this problem, and one of them is Yashio Benjo,

16:47.760 --> 16:54.080
who is considered one of the so-called godfathers of AI. When I read what they're saying,

16:54.080 --> 16:58.560
when I read their words, I see they're saying, we need societal context to help us to solve this

16:58.560 --> 17:05.120
problem. So, just to read a quote from another really good paper called Tor Causal Representation

17:05.120 --> 17:11.360
Learning, we argue that causality with its focus on representing structural knowledge about the

17:11.360 --> 17:17.040
data generating process that allows interventions and changes can contribute towards understanding

17:17.040 --> 17:23.040
and resolving some key limitations like robustness and current machine learning methods. So,

17:23.040 --> 17:28.720
when I read that, I see we need structural knowledge about that amorphous thing, the data

17:28.720 --> 17:34.800
generating process, the problem domain. It's causal. So, I think there's a direct connection

17:34.800 --> 17:39.280
between what they're asking for and what we're seeking in terms of being able to fill that

17:39.280 --> 17:45.040
knowledge gap, fill that epistemic uncertainty gap. So, now I'm going to introduce this problem

17:45.040 --> 17:52.640
understanding chasm using this use case, this real-world example of racial bias discovered

17:53.280 --> 17:56.560
in a medical algorithm that's widely used throughout the U.S. healthcare system.

17:57.600 --> 18:03.360
This paper came out about four years ago. The purpose of this algorithm was to identify patients

18:03.360 --> 18:10.000
with the most complex healthcare needs so they could be given access to special programs early on

18:10.720 --> 18:14.320
so that you could ultimately reduce the overall cost in the healthcare system.

18:14.320 --> 18:17.840
And it's important to remember that their motivation was to reduce the overall cost

18:17.840 --> 18:23.520
in the healthcare system. Now, unfortunately, people not selected for the special programs by

18:23.520 --> 18:29.360
this algorithm suffer from nearly 50,000 more chronic diseases than the people who were selected

18:30.560 --> 18:34.800
and the people who were not selected were disproportionately Black Americans. So, this

18:34.800 --> 18:39.520
is why the algorithm was deemed to be racially biased. So, the question is like why did this

18:39.520 --> 18:45.600
happen? What led to this? And the researchers who discovered this had unprecedented access

18:45.600 --> 18:50.800
to every aspect of how this was built, which is not typical. The training data, the machine

18:50.800 --> 18:57.520
learning architecture, the training algorithm, you know, the performance metrics, the actual outputs,

18:58.480 --> 19:03.520
access to everybody who made decisions to build the system. And their conclusion was that the root

19:03.520 --> 19:09.120
cause of this failure was incomplete problem understanding. That's what it boiled down to.

19:10.480 --> 19:16.080
And remember, understanding the problem and formulating the problem for AI is at the very

19:16.080 --> 19:23.040
beginning of this process. But in reality, we really don't spend that much time studying and

19:23.040 --> 19:28.240
trying to improve that part of this process. We spend most of our time on the latter stages,

19:28.240 --> 19:35.280
the later life cycles. But remember, yeah, go ahead. Is that the algorithm that used the amount

19:35.280 --> 19:40.480
of money you spent on healthcare as a as an input? Yeah, exactly. And I'm going to I'm going to get

19:40.480 --> 19:47.600
into that right now. Yeah. So, the key thing I want to go back to is that human decision of choices

19:47.600 --> 19:52.960
drive this entire life cycle and really critical ones happening at the very beginning of the life

19:52.960 --> 19:59.440
cycle. But right now, those processes are pretty ad hoc and informal. And that's what's leading to

20:00.720 --> 20:06.800
my mind, this epistemic uncertainty. So there's a quote that I couldn't resist putting in. And

20:06.800 --> 20:12.960
once I read it, this is one of the papers that we reference and read when we're thinking about how

20:12.960 --> 20:17.680
humans make decisions and choices, because we said, Oh, this is a really critical part of this

20:17.680 --> 20:22.240
process. We should understand how humans make decisions. But this quote jumped out of me because

20:22.240 --> 20:27.600
it says choices do not merely identify one option among a set of possibilities. Choosing is an

20:27.600 --> 20:34.960
intervention, an action that changes the world. That's particularly true when decisions are being

20:34.960 --> 20:40.640
made within these AI product development life cycles, because those decisions impact directly

20:40.640 --> 20:45.760
what ends up getting produced by the machine learning models, which are themselves interventions

20:45.760 --> 20:50.240
on that societal context. So the criticality of this decision making and how we make choices

20:51.040 --> 20:56.800
is very high. And so we spent a lot of time looking at this and the research that we looked at

20:57.680 --> 21:03.920
pointed out that when we make decisions as humans, we leverage our causal inference capabilities.

21:04.880 --> 21:11.120
And those are shaped by strong top down prior dollars that we have in the form of what they

21:11.120 --> 21:15.360
call intuitive theories, as called them causal theories. And causal theories are these models

21:15.360 --> 21:22.560
we build up over time, as we navigate complex realities, face problems, solve problems,

21:22.560 --> 21:27.520
we form theories about why they're happening. And that informs our goals and our strategies to solve.

21:29.120 --> 21:34.640
So let's look at what role causal theories played in the failure of this algorithm.

21:35.440 --> 21:40.720
And remember that the problem was predict which patients will have the most complex healthcare

21:40.720 --> 21:45.840
needs, because we're going to give them access to these special programs. And the causal theory

21:45.840 --> 21:51.520
that they leveraged wasn't explicit at the time, but the one they leveraged was that more complex

21:51.520 --> 21:57.680
healthcare needs is going to lead to an increased spending on healthcare. And so they chose that

21:57.680 --> 22:02.720
as the target variable or their predictor, or the proxy for complex healthcare needs.

22:03.360 --> 22:06.800
They said if we can predict who's going to spend more money on healthcare in the future,

22:06.800 --> 22:09.840
we'll be able to predict who's going to have the most complex healthcare needs.

22:09.840 --> 22:14.240
This is where the algorithm failed. And the reason it failed is because this

22:14.240 --> 22:20.320
causal theory is woefully incomplete, right? It leaves out critical factors that impact

22:20.320 --> 22:26.800
how much black Americans spend on healthcare, even when they have more complex healthcare needs.

22:26.800 --> 22:31.680
These are factors like under diagnosis due to bias, lack of trust in the healthcare system,

22:31.680 --> 22:35.760
wealth and income disparities, and lack of access to affordable healthcare.

22:36.560 --> 22:40.560
Now take it, all those factors actually decrease how much black Americans spend on

22:40.560 --> 22:44.880
healthcare, even when they have more complex healthcare need. And then taken together,

22:44.880 --> 22:51.040
those factors represent a subset of the structural inequities that exist in the US healthcare system.

22:52.400 --> 22:57.280
And those structures were revealed by COVID-19, right? It showed all those structures.

22:57.920 --> 23:02.880
But those structures actually increase how much complex healthcare need there is in that community

23:02.880 --> 23:06.400
while simultaneously decreasing how much they spend on healthcare.

23:06.400 --> 23:13.280
So the causal theory wasn't even close to having the reality of the problem domain that it was

23:13.280 --> 23:21.200
trying to solve. So this is what we call the problem understanding chasm. This difference in

23:21.200 --> 23:28.400
understanding of the key problem from these two different perspectives in this chasm between them.

23:28.640 --> 23:30.560
I'm talking more about what contributes to that.

23:33.040 --> 23:36.880
This lack of knowledge on the right-hand side led directly to that

23:36.880 --> 23:42.320
harmful intervention, that lack of understanding of that additional societal context.

23:44.720 --> 23:47.680
Now one of the key root causes of this chasm is

23:49.040 --> 23:56.000
divergences between communities that are trying to intervene and how proximate they are to the

23:56.000 --> 24:03.520
problems and then their ways of knowing and explaining problems. So on the civil society side,

24:03.520 --> 24:08.320
these are generalizations, but folks trying to solve those problems on the civil society side

24:08.320 --> 24:13.840
are typically very proximate to the problems. They're deeply entrenched in the problem domains.

24:13.840 --> 24:18.320
On the product side, we're less entrenched in the problem domains, we're less proximate to them.

24:20.800 --> 24:24.640
Civil society folks that have a high stake in like solving the societal problem itself.

24:26.160 --> 24:30.320
Related to the broader domain. On the product side, we're typically interested in the business

24:30.320 --> 24:37.920
problem related to that domain. And then finally, on the civil society side, since you're really

24:37.920 --> 24:44.480
proximate to the problem and the humans that are suffering from it tend to prioritize a qualitative

24:45.360 --> 24:52.560
human perspective on the problem factors. So I have a question just trying to blend it to

24:53.520 --> 24:56.240
things like the gravity project, which are trying to determine

24:57.600 --> 25:03.360
codes for social determinants and valves and related projects. Would that be maybe some mitigating

25:04.320 --> 25:10.160
factor where you've got the civil society data informing this decision, I think, rather than

25:10.800 --> 25:15.200
as an organization trying to decide for ourselves what we think is important?

25:15.200 --> 25:19.280
Yeah, that would be exactly right. The thing you want to do is get to the point where

25:20.000 --> 25:22.800
you have some cooperation and understanding the problem. Because you have two different

25:22.800 --> 25:28.640
perspectives. Now, the right hand perspective is not wrong. It's just a different lens in that problem

25:28.640 --> 25:33.280
domain. And solely using that lens, you're going to miss critical things. And the same on the other

25:33.280 --> 25:39.200
side as well. Even on the civil society side, we're trying to solve problems. We're using methods,

25:39.200 --> 25:45.760
sociology, etc. But you also have problems with uptake of those solutions. Statistical methods

25:45.760 --> 25:52.880
get used on that side as well. I have to mention as well. So the idea is how do we bridge this kind

25:52.880 --> 25:58.880
of chasm in terms of how different groups think about and understand problems and get to a shared

25:58.880 --> 26:02.640
understanding helps us get closer to the reality that we're trying to intervene on.

26:05.600 --> 26:10.080
And then I just wanted to show this one more time using that initial diagram, that problem

26:10.080 --> 26:17.440
understanding chasm, right? This gap between societal context and all that messiness and how

26:17.440 --> 26:23.840
we're thinking about the problem on the product development side. Now, just to go back to this,

26:24.560 --> 26:30.000
our goal is, the thing I'm focused on within Google is how do we reduce this epistemic uncertainty?

26:30.640 --> 26:35.840
And to reduce epistemic uncertainty, you need knowledge. But we have to figure out a way to

26:35.920 --> 26:42.720
have that knowledge across that chasm and be useful during these workflows. And so people are

26:42.720 --> 26:52.000
using all sorts of tools to make decisions. And we can't expect them to read to Andrew Self's paper

26:52.000 --> 26:55.520
and figure out what they're supposed to do, read the latest research. It's just not going to happen,

26:55.520 --> 27:02.800
right? So we have to find a way to get knowledge across that chasm and make it available to these

27:02.800 --> 27:09.040
decision makers during every step of that workflow. So this is what our hypothesis is,

27:09.040 --> 27:14.560
is that to bridge that problem understanding chasm in a responsible way, these decision makers need

27:14.560 --> 27:20.800
tools that put community validated structure to societal context knowledge about complex

27:20.800 --> 27:27.520
societal problems at their fingertips, especially during problem understanding at the very beginning,

27:27.520 --> 27:31.200
but also throughout the entire lifecycle, because you should also be leveraging that

27:31.200 --> 27:35.360
understanding when you're evaluating a product for whether or not it's performing well,

27:35.360 --> 27:39.600
like what does that mean, right? It has to be relative to the context in which it's going to

27:39.600 --> 27:49.360
operate it. But we have to remember a couple of key things. And one is that it's factual that

27:49.360 --> 27:54.400
historically marginalized communities are disproportionately negatively impacted when

27:54.400 --> 28:01.920
things go wrong with AI bias. Those are just the facts, right? And you can look at example after

28:01.920 --> 28:07.600
example. And then the other fact is that the lived experience expertise of those folks

28:08.640 --> 28:14.240
is usually excluded from that, from those problem understanding or problem formulation steps.

28:16.400 --> 28:23.440
And so if we are focusing on also addressing these issues, then we're not, we're not

28:23.440 --> 28:29.760
responsibly solving this problem. So we think there are two key ingredients for bridging this

28:29.760 --> 28:35.520
chasm. One is in order to transmit knowledge across that chasm, we think we need to have some sort of

28:35.520 --> 28:41.360
model or reference frame for structured societal context knowledge. Yes. Oh, sorry, I didn't see

28:41.360 --> 28:46.640
you. Yeah. The slide where you said your hypothesis, what do you mean by knowledge?

28:47.360 --> 28:55.600
So that's a good question. That's a good question. So what we need, what we mean by knowledge is

28:57.120 --> 29:10.080
awareness of factors that make up the circumstances that in which you're going to be

29:10.160 --> 29:17.440
deploying models and that originate the problem you're trying to solve. So knowledge is like a very,

29:17.440 --> 29:22.240
you know, that's a big, deep philosophical question that you asked me. But we just mean

29:22.240 --> 29:28.640
awareness of these, these factors that make up these circumstances. Does that answer your question?

29:28.640 --> 29:35.840
I'm sure. Yeah, I just couldn't tell if you meant like data or theories or societal facts or all of

29:36.400 --> 29:40.720
those. Yeah, I mean, as we look at it, you'll see it's kind of like a combination of that,

29:40.720 --> 29:45.120
right? So we're, so we've got lots of data, but we don't have a lot of context for the data

29:45.120 --> 29:52.320
to help us interpret it. So what we're looking for are those, those additional facts that help

29:52.320 --> 29:57.200
make up the circumstances that led to the data existing. And oftentimes those circumstances

29:57.200 --> 30:01.200
will tell you that you're missing data, like you think you have the right data to understand

30:01.200 --> 30:05.360
this problem, but you don't even have that data yet. And so it starts with some understanding of

30:05.360 --> 30:10.400
what these other factors are you should consider. Yeah, exactly. I see, I feel like the biggest

30:10.400 --> 30:16.800
thing is the processes that are generating the data. So you may have known some of Rahid Ghani's

30:16.800 --> 30:24.160
work in Carnegie Mellon. Now he's there where he runs this data science for social good program.

30:24.160 --> 30:30.480
He brings students in and students, for example, go with the cops. They go, they go with the ambulance

30:30.480 --> 30:35.120
driver. And like one of the examples he was saying was that this ambulance driver who already

30:35.200 --> 30:40.400
had two car shins, so he was on probation, he didn't take a call because he's like,

30:40.400 --> 30:46.080
I don't want to risk my job because if I go take this call, I may do something wrong and then

30:46.080 --> 30:50.720
they're going to fire me, right? Your data is not going to say that unless you're actually

30:50.720 --> 30:56.000
riding with the ambulance driver and seeing the ambulance driver park his car under the bridge

30:56.000 --> 31:03.280
to and not accept that call. You're not seeing that. And so exactly. And so it's very difficult to

31:03.280 --> 31:08.560
say, I have good enough societal context knowledge. That's exactly right. You have to

31:08.560 --> 31:15.360
understand these situations and circumstances of key actors and agents. And so we came up with a

31:15.360 --> 31:19.680
way to start to tackle how to model this kind of knowledge. And so I'm going to get to that in a

31:19.680 --> 31:27.200
few slides. So I think I was here. Yeah, so this goes right back to the question about knowledge,

31:27.200 --> 31:33.120
right? So you'll see what we mean by it in a few couple slides. But we think the other key

31:33.120 --> 31:39.520
aspect to address those facts I just laid out is that we need participatory, non-extractive

31:39.520 --> 31:44.480
methods. And by non-extractive, I mean, you know, there'll be a demand for this knowledge. We're

31:44.480 --> 31:49.120
going to make sure we're not just going into communities, gathering knowledge and then taking

31:49.120 --> 31:55.760
it away. Like there's got to be mutual benefit. So and then you also need capacity, right,

31:55.760 --> 32:00.480
for communities to participate directly and own their knowledge and be able to benefit from it.

32:03.120 --> 32:10.080
Yeah, Chris. So some of the things that the community can share are not the sorts of things

32:10.080 --> 32:16.400
that are going to become data on the other side, right? Like that example is not that you're going

32:16.400 --> 32:21.920
to start adding as a factor, you know, are people refusing to take calls because they are on probation.

32:24.880 --> 32:31.440
And so I guess part of the question here is like, which things that the community knows

32:31.680 --> 32:40.560
are translatable into something that then a more principled set of AI creators can use to make a

32:40.560 --> 32:47.840
more humane and fair algorithm? And which ones of them are reasons to be modest about the uses

32:47.840 --> 32:53.920
of AI and to say that, well, actually, what we want is AI products that say, well, here's what

32:53.920 --> 32:59.920
we suggest based on what little this algorithm knows. But there's only a suggestion and you

33:00.880 --> 33:06.000
decision, you human decision maker being advised on this probably know a lot more about this case

33:06.000 --> 33:11.600
than the algorithm does. So the algorithm is presenting its results and assign kind of with

33:11.600 --> 33:19.360
some humility. And yeah, I mean, that's, that's interesting too. The if as well as the how

33:20.320 --> 33:24.720
of representing this knowledge. Yeah, yeah, I mean, the thing I'll highlight there is that,

33:25.520 --> 33:29.520
you know, one of the things you want to happen is you want the models to be aware of their

33:29.520 --> 33:35.280
epistemic uncertainty. You know, I don't I don't know a lot about this. Right. And so even if I

33:35.280 --> 33:41.680
don't end up incorporating, you know, a particular factor directly in the data set or the decision

33:41.680 --> 33:46.880
making variables, I need to have a better understanding of like, I don't know a lot about this space,

33:46.880 --> 33:51.200
maybe we shouldn't even be using AI for this. Right. Right. That's why you want to really

33:52.160 --> 33:57.280
dig deeply into this into the problem understanding and formulation phase before you start building

33:58.000 --> 34:00.960
machine learning systems, right, for a particular problem.

34:03.280 --> 34:06.880
Does that answer your question? It's a longer discussion. I think it's a great

34:06.880 --> 34:12.000
it's a great set of questions like, you know, something like, Dr, here's our recommendation,

34:12.000 --> 34:16.400
but please keep in mind, there are lots of good reasons why this patient might be an exception

34:16.480 --> 34:21.360
to this recommendation. And I I don't know how to convey this partly gets into human

34:21.360 --> 34:27.200
computer interface questions and psychology questions. How do we convey uncertainty and

34:27.200 --> 34:36.240
humility to the people being advised by these algorithms? Yeah. And, you know, yeah. Yeah.

34:36.240 --> 34:40.000
And I think it's just it's it's not the same part as the explainability problem that everybody's

34:40.000 --> 34:46.560
trying to solve, explaining the results of the output. We also be need to be able to explain

34:46.560 --> 34:50.960
like, what we don't know, right, and the risks associated with taking the advice.

34:52.160 --> 34:58.480
Sorry, if I may, this also goes to the incentive. So as an AI tool developer, right, you don't want

34:58.480 --> 35:04.560
to develop a tool that says, I don't know, right. And so first, so for example, Karina Cortez,

35:04.560 --> 35:09.040
Google Research, right, she had a couple of really nice papers on learning with abstention,

35:09.760 --> 35:14.720
where the machine learning system says, I don't know, right. But your software engineers,

35:14.720 --> 35:20.480
you're, they wouldn't want to give a tool that would say, I don't know, right. And so that's

35:20.480 --> 35:30.000
changing the incentives of like, I don't know. Yep. I agree 100% of the agents that are making

35:30.000 --> 35:34.640
decisions is like a key part of that overall societal context. But you'll see right now,

35:34.640 --> 35:40.160
even like, some things happen with the generative AI, and like hallucinations happening,

35:41.520 --> 35:48.800
products are saying, yeah, bullshitting, products are, you see product services saying,

35:48.800 --> 35:54.000
we don't know, don't trust us, we're not sure. Oh, that's great, then. Yeah. Yeah. I mean,

35:54.000 --> 35:59.520
because, and that's being forced by, by this, this becoming an intervention at society that's

35:59.520 --> 36:07.120
getting a lot of kind of feedback. Right. So, you know, so we've got this situation,

36:08.240 --> 36:14.880
we've got this messiness that can't be transmitted over that, that chasm, if, and we're saying,

36:14.880 --> 36:18.560
hey, we need to organize information and knowledge about societal context in some way, this big,

36:18.560 --> 36:23.520
scary thing. So we need some kind of model of it. So it's kind of ironic, right, you're trying to,

36:23.520 --> 36:27.040
you're trying to reduce people abstracting away, but in order to do that, you need some kind of

36:27.120 --> 36:31.680
abstraction, right, that people can use to kind of cope with this knowledge in some way.

36:33.200 --> 36:40.240
And so we're, I was really inspired by some work by a sociologist named Dr. Walter F. Buckley,

36:40.960 --> 36:47.120
who back in the late 60s made the connection between trying to understand society and

36:47.120 --> 36:54.400
sociocultural systems, and between that and systems theory. And so he was like,

36:54.400 --> 36:58.480
we should think of society as a complex adaptive system. When I talk about societal context,

36:58.480 --> 37:04.080
let's face it, we're talking about like, you know, just a, you know, some conception of society as

37:04.080 --> 37:09.120
a whole, how do we think about that? And so our work was based on using complex adaptive systems

37:09.120 --> 37:13.840
theory as the basis. If you look at the characteristics of a complex adaptive systems,

37:14.480 --> 37:19.360
a complex adaptive system, you'll, you'll see some of these characteristics that are in common

37:19.360 --> 37:25.360
with how we were describing societal context. It's complex, adaptive and dynamic. There's this

37:25.360 --> 37:30.720
nonlinearity. I talked about these feedback loops of the ecosystem that we're dealing with.

37:31.520 --> 37:36.720
We talk about things being historical, time delays goes to that complex adaptive systems have

37:36.720 --> 37:44.400
history as well. So we looked at the canon of work associated with this. John Holland is like

37:44.400 --> 37:50.640
a giant in this space. Scott Page, who I think doesn't work with SFI is also somebody I love to

37:50.640 --> 37:56.800
read. So we synthesize some work from these folks to come up with, you know, an attempt, right,

37:56.800 --> 38:01.360
to start to wrap our, our heads and arms around this complex thing. So we came up with a reference

38:01.360 --> 38:09.760
frame or model has three key elements, agents, precepts and artifacts, agents. When we thought

38:09.760 --> 38:13.360
about agents, these could be human or nonhuman, but I'm going to talk about this through the human

38:13.360 --> 38:18.400
lens. So through human lens, these are eight, these are individuals and institutions.

38:19.680 --> 38:24.000
And the ideas that all agents have and essentially are their precepts,

38:25.920 --> 38:31.120
which are our conception, our beliefs, our values, our stereotypes,

38:32.080 --> 38:34.320
our, how we perceive needs and problems.

38:36.560 --> 38:42.400
The causal theories that we talked about, these are all these rules that constrain and drive the

38:42.400 --> 38:48.240
behavior of agents. And these can go all the way down to, you know, the fundamental rules for

38:48.240 --> 38:52.880
building, building a human, right? But at the societal level, we're talking about these kind

38:52.880 --> 38:59.040
of beliefs and values and goals that constrain and drive the behavior of agents. And when agents

38:59.040 --> 39:06.800
behave, their behavior manifests itself in the real world as artifacts. Indeed, they're just

39:06.880 --> 39:12.800
all the things agents produce and create, language, data, laws, institutions, right?

39:12.800 --> 39:17.200
There's some multiple, multiple inheritance. Some, you know, agents can produce artifacts or

39:17.200 --> 39:24.720
also agents, for example, policies, AI models or artifacts were creating products and we're

39:24.720 --> 39:30.640
producing problems as well. And then there's this other important relationship between precepts and

39:30.640 --> 39:36.640
artifacts. This is kind of well-known in the way people think about HCI, for example, is that

39:36.640 --> 39:41.840
the things we produce are a reflection of our values, right? So the artifacts are in some way

39:41.840 --> 39:45.440
a reflection of the precepts that led to them existing in the first place. And of course,

39:45.440 --> 39:50.640
those precepts are influenced by the artifacts that the agent is surrounded by in the world,

39:50.640 --> 39:56.240
right? I read a book, my precepts get updated, then I do some more acting in the world, gets

39:56.240 --> 40:03.440
reflected in more artifacts. But we also wanted to organize this and represent it as a taxonomic

40:03.440 --> 40:09.360
model because ultimately we're envisioning, creating knowledge graphs and databases so that

40:09.360 --> 40:17.600
we can transmit this data across that chasm and make it available to tools and workflows, right?

40:17.600 --> 40:20.960
So somehow or another, we have to have a structured representation of this knowledge.

40:22.720 --> 40:27.920
Now, when we develop this initial model, we hypothesize that these perceived problems and

40:27.920 --> 40:31.760
causal theories were going to be really important if we're thinking about product development.

40:32.720 --> 40:37.360
And when we actually did this work, it was before that Ziad Obermeyer paper came out

40:37.360 --> 40:43.040
about racial bias and medical algorithms. So we were kind of excited that, yeah,

40:43.040 --> 40:47.760
we were on the right track, we think, because he pointed out, they pointed out how critical

40:47.760 --> 40:53.280
these causal theories were and how they directly led to harmful bias in an important system.

40:55.920 --> 40:59.200
And so now the left-hand side looks a little bit more

40:59.920 --> 41:03.920
organized, maybe we can think about societal context in a little bit more of an organized way.

41:05.440 --> 41:09.760
But the key question is, how do we responsibly acquire these causal theories

41:11.520 --> 41:18.800
and make them into structured knowledge? So this is where this practice called community-based

41:18.800 --> 41:25.040
system dynamics came into play. I have some friends in the system dynamics community.

41:26.000 --> 41:30.560
I know the person who wrote the book on community-based system dynamics, and this seemed like

41:30.560 --> 41:36.720
it could be a good way to produce societal context knowledge through the act of prototyping

41:38.000 --> 41:42.000
problems, which are like this core part of societal context, but prototyping them

41:42.000 --> 41:47.200
as systems, as complex adaptive systems. And community-based system dynamics

41:47.200 --> 41:56.320
is a participatory practice for prototyping problems that centers communities and empowering

41:56.320 --> 42:01.920
communities to fully participate and take ownership of their own models of the problems

42:01.920 --> 42:10.560
that they're faced with. But it's grounded in the feedback perspective of system dynamics,

42:10.560 --> 42:18.640
which is invented almost over 60 years ago at MBT. Now SD itself, system that dynamic itself,

42:18.640 --> 42:24.880
leverages visual tools and simulation to support group model building, building prototyping

42:24.880 --> 42:31.840
problems together, and developing collaborative, developing causal theories. And the other words

42:31.840 --> 42:36.960
we use for that are problem models or problem prototypes. And it allows people, health people

42:36.960 --> 42:42.640
do this both qualitatively and quantitatively, and it has a nice bridge between the qualitative

42:42.640 --> 42:47.360
and the quantitative, which we think is important for being able to do this kind of work. And then

42:47.360 --> 42:52.000
the last thing to emphasize is that these prototypes or simulations of these underlying

42:52.000 --> 42:57.360
structures can be used to test interventions on a problem before you actually try to do it in the

42:57.360 --> 43:06.640
real world. And so I'm going to walk through at a high level the basic visual notation that

43:06.720 --> 43:13.200
utilized to prototype problems and system dynamics. One representation is called a causal loop

43:13.200 --> 43:17.920
representation. You might have heard of causal loop diagrams or causal maps, causal maps. The

43:17.920 --> 43:24.000
other is a stock and flow representation. Causal loop diagrams are fully qualitative. Stock and

43:24.000 --> 43:29.760
flow diagrams are qualitative and quantitative. They're provide that bridge into the quantitative

43:31.040 --> 43:34.560
world. So I'm going to walk through this example with a simple loan scenario model.

43:34.640 --> 43:38.800
So the sort of factors you have when you're talking about loans are average credit score,

43:39.440 --> 43:45.360
loans received, borrowers, who's defaulting on loans, who's making their payments.

43:46.720 --> 43:52.480
So you have all those factors specified in language and words. And then you have an arrow

43:52.480 --> 44:00.160
between factors with either plus sign or minus sign on top of it that illustrates the hypothesized

44:00.160 --> 44:06.640
causal relationship between two factors. And the plus sign just illustrates the direction

44:06.640 --> 44:12.000
of the impact that one factor is going to have on another. So to give an example, the plus sign

44:12.640 --> 44:17.120
at the end of the average credit score arrow means that as the average credit score goes up in this

44:17.120 --> 44:23.120
system, the number of loans received is also going to go up. But it also means as the average credit

44:23.120 --> 44:28.160
score goes down, the number of loans is going to go down. So it just means that the impact is in the

44:28.160 --> 44:33.360
same direction. The minus sign means the opposite. So the relationship between loan defaults and

44:33.360 --> 44:37.760
average credit score has the minus sign on the arrow. That means as the number of loan defaults

44:37.760 --> 44:42.160
goes up, the average credit score is going to go down. It's going to go in the opposite direction

44:42.160 --> 44:47.040
as opposed to the same direction. If the number of loan defaults goes down, the average credit score

44:47.040 --> 44:53.280
is going to go up in the system. You can also illustrate time delays, right? Because when one

44:53.280 --> 45:00.160
factor impacts another, it may not happen right away. So in this system, we're saying as number

45:00.160 --> 45:05.040
payments go up in a system, the average credit score in the system should go up. But that's not

45:05.040 --> 45:08.560
going to happen right away. It'll take a while for the credit score, average credit score in the

45:08.560 --> 45:14.720
system to be impacted. You can illustrate that that way. And then you have feedback loops, right?

45:14.720 --> 45:19.520
This really important aspect of complex adaptive systems. There's two types of feedback loops.

45:19.520 --> 45:26.240
One is a reinforcing feedback loop. These are basically indicators of exponential growth,

45:27.200 --> 45:33.680
either vicious cycles or virtuous cycles. And so you can identify parts of the system that

45:33.680 --> 45:39.520
could be driving exponential growth. In this case, the combination of borrowers increasing

45:39.520 --> 45:44.720
and payments being made and average credit score going up, that could drive an exponential increase

45:44.720 --> 45:51.120
in the number of loans in the system. So that's what the reinforcing loop represents. But the

45:51.120 --> 45:56.000
balancing loop is for balancing out this exponential growth. So that's illustrated by the loop that

45:56.000 --> 46:02.000
includes loan defaults. Because that's the one place that even as borrowers are increasing,

46:02.000 --> 46:06.240
yeah, that leads to more payments, but it also leads to more defaults, which leads to a decrease

46:06.240 --> 46:11.600
in average credit score, which taps down the growth in loans being given out. Can I ask? Yeah.

46:11.600 --> 46:15.600
Because you're saying average credit score, this is meant to model what's happening in an

46:15.600 --> 46:20.880
entire population. Yeah. Because of course, these feedbacks also take place in individuals' lives.

46:20.880 --> 46:28.800
And so, yeah. Yeah. Yeah. I mean, system dynamics tends to take a macro view, right? It's looking

46:28.800 --> 46:34.480
at populations as a whole. And oftentimes people use a combination of system dynamics for the macro

46:34.480 --> 46:40.160
view and then agent-based modeling to have more specific kind of ways to model like individual

46:41.600 --> 46:46.320
behavior. But this model might have different outcomes within different demographic groups.

46:46.320 --> 46:51.280
Oh, yeah. Totally. Totally. Right. And again, this is just a way to kind of start to hypothesize

46:51.280 --> 46:56.720
about these factors and you can get more detailed or less detailed. Yeah. You care if the average

46:56.720 --> 47:04.320
credit score is a black box, but you cannot see it. Yeah. I mean, I see what's happening. Do I?

47:04.320 --> 47:10.800
Yeah. Yeah. Yeah. I care. So we put this up just to kind of illustrate like the elements of it,

47:10.800 --> 47:16.080
but as you do this work, you'll see that what happens more and more, you drill into these

47:16.080 --> 47:21.680
black boxes and you try to expose the mechanisms under them. Sometimes you're not able to because

47:21.680 --> 47:27.040
there's someone in control of that. Yeah. Because I was working with Citibank in terms of who they

47:27.040 --> 47:32.480
should give loans to and fair loans, et cetera, et cetera. Yeah. And they're getting some information

47:32.480 --> 47:37.920
from third-party folks, which they don't know. They do not know how the FICO score is being

47:37.920 --> 47:43.280
computed. Yeah. And so it is a black box and they're going to use it. Yeah. It's a model.

47:43.280 --> 47:47.520
It's a model. Right. That is a model that is being used to kind of make decisions.

47:48.480 --> 47:52.720
Right. Yeah. Right. So then the question would be like, how do you actually quantify that in your

47:52.720 --> 48:00.400
epistemic uncertainty? How much weight do you give to it? Yeah. I mean, that's a question. Right.

48:00.400 --> 48:08.720
You have to, and this is something that the modelers contend with and negotiate. And this is why

48:08.720 --> 48:15.600
you want to try to include in this modeling the people who are building those who are building

48:15.600 --> 48:20.000
these credit score models. It's very, you know, it's, depending on what problem domain you're

48:20.000 --> 48:24.560
working at, it's very hard to have participation from everybody that you want to have participation

48:24.560 --> 48:34.960
from. Yeah. And then the stock and flow representation has all those same features.

48:35.760 --> 48:40.960
You just represent those factors with quote, unquote, stocks and flows. What important aspect

48:40.960 --> 48:45.760
of the stocks is it allows you to take into consideration accumulations in a system,

48:45.760 --> 48:51.680
which really allows you to be able to consider the history of the system, the initial conditions

48:51.680 --> 48:56.080
and how things grow and decrease over time and really get into that detail in terms of

48:56.080 --> 49:00.800
relationship between factors. And this is where you start to get into the quantification,

49:00.800 --> 49:05.600
because the way you represent those relationships is with differential equations, for example.

49:05.600 --> 49:09.760
And that allows you to get to the point where you could start to simulate your hypothesis.

49:09.760 --> 49:14.320
And that's, and system dynamics is not about, it's really not like, I'm going to solve this

49:14.320 --> 49:20.240
problem. I'm trying to learn the problem. I'm trying to learn the problem system and increase my

49:20.240 --> 49:25.360
confidence that I understand it well enough to start to intervene on it in the real world.

49:28.160 --> 49:33.840
I just threw this in just to give an indication of what you have put this together. You can start

49:33.840 --> 49:40.800
to kind of look at how these impacts show up in a simulation. This is made with something called

49:40.800 --> 49:47.760
loopy. Anybody can play around with this. This is like a really oversimplified way to model these

49:48.080 --> 49:53.040
sorts of systems, but at least it lets people to start to engage with it. There's really expensive,

49:53.040 --> 49:57.520
hard to use software as well. And one of the issues is that we have to improve the accessibility of

49:57.520 --> 50:02.320
these sorts of tools if we really want to build capability and capacity in communities. So now

50:02.320 --> 50:07.920
we've got this kind of approach that we're like, ah, this might be a good way for us to start to

50:07.920 --> 50:16.240
tackle producing societal context knowledge from places where we're embedded and proximate to

50:16.240 --> 50:21.520
problems. But now we have to have some actual capability in the world to use these techniques

50:21.520 --> 50:26.720
if we're going to have any chance of producing useful knowledge. So now I'm going to talk about

50:27.520 --> 50:30.800
some experience that we've had over the past few years of trying to build that kind of trust

50:30.800 --> 50:35.920
and capability. I'm just just putting up this reminder of the fact that, you know, the lived

50:35.920 --> 50:39.920
experience expertise of historical marginalized groups is usually not involved in this process.

50:39.920 --> 50:42.640
So when I started this back in 2017, I intentionally

50:43.920 --> 50:48.640
wanted to work with folks from historically marginalized communities who are trying to

50:49.280 --> 50:52.400
leverage data and math and science to understand and solve problems.

50:53.760 --> 51:00.000
And there's a group called Data for Black Lives who does exactly that. It's a collection of activists

51:00.000 --> 51:05.120
and organizers and mathematicians and scientists who are trying to leverage data science for good

51:05.120 --> 51:09.760
and also try to make sure data sciences does not end up unintentionally making things worse

51:09.760 --> 51:16.000
when we're applying it into high stakes domains. And so this started with me just going to the

51:16.000 --> 51:21.040
first conference, not knowing what to expect back in 2017. And this led to a two and a half year journey

51:21.600 --> 51:27.200
that resulted in a research paper and a causal theory that's starting to have an impact on

51:27.920 --> 51:32.800
how Google evaluates and think about health diagnostic algorithms. So I just went to the

51:32.800 --> 51:38.400
first conference with no expectations, not asking anybody for anything, just trying to figure out

51:38.400 --> 51:41.840
what people were trying to do, what problems they were working on, what problems they were trying to

51:41.840 --> 51:47.520
solve. And that's interesting people. Some of them famous now, you probably recognize.

51:50.480 --> 51:55.280
And what I discovered there was that there was this need for people to be able to see

51:55.280 --> 52:01.440
how the problems they were separately working on were connected. And now everyone was talking about,

52:01.440 --> 52:04.800
you know, we have to understand the system, the system, but there was no real

52:05.760 --> 52:10.000
concrete way to actually start to tackle that. So I said, Hey, this could be a good place to

52:10.000 --> 52:16.160
introduce system dynamics and systems thinking. I talked to the founder. Yes, he mill in Europe.

52:16.160 --> 52:21.040
She didn't know me from Adam. And so I had to convince her that this could be useful. So I

52:21.040 --> 52:25.200
introduced her to it with a mini workshop. And then we agreed to set a goal of actually

52:25.760 --> 52:32.080
delivering a workshop to that's larger community at the next conference that they were going to

52:32.080 --> 52:37.280
have, which was about a year away. But we had one principle, we said, the people who are going to

52:37.280 --> 52:43.200
teach this workshop to this community, they have to have similar lived experiences to the people

52:43.200 --> 52:51.920
that are going to be receiving this knowledge. And not a lot of diversity in the world of system

52:51.920 --> 52:56.480
dynamics when we started this has changed over time. So we said, we have to, we have to teach

52:56.480 --> 53:01.040
the teachers, we have to create some teachers that can deliver this workshop. And so we did a

53:01.040 --> 53:05.040
learning lab that included people from that community, as well as people from Google to

53:05.040 --> 53:08.480
teach them these techniques so that they could teach the bigger workshop.

53:10.880 --> 53:14.800
We did that over a couple of months, they were able to teach this workshop and these techniques

53:15.360 --> 53:19.680
to a larger group of about 75 people was received very well.

53:20.640 --> 53:25.520
And then we had this capability, and we're like, okay, and a little bit of capacity. What should

53:25.520 --> 53:31.200
we do with it to try to further this journey? And so we decided to do a problem prototyping

53:31.200 --> 53:38.400
experiment around a problem domain that Google cares about and data for Black Lives Care is about.

53:38.400 --> 53:44.080
And that was health diagnosis. Data for Black Lives Care about it because under diagnosis can

53:44.080 --> 53:48.880
lead to health disparities. We talked about that earlier. Google cares about it because,

53:48.880 --> 53:53.200
you know, we're working on building health care diagnostic algorithms. We want them to be

53:53.200 --> 53:57.680
accurate and work well. But there's a lack of training data that can lead to inaccuracy.

53:58.480 --> 54:07.600
And so we thought this would be a good area to jointly prototype a problem that relates

54:07.600 --> 54:15.200
algorithms and lack of data and health care disparities as a way to start to bridge that

54:15.200 --> 54:20.960
chasm. And so we brought people together. We got about nine people together from that initial

54:20.960 --> 54:26.720
workshop. They developed one of these college loop diagrams for that problem domain. I'm not going

54:26.720 --> 54:30.480
to go through this in detail, but you can look at the paper, look at all the data.

54:32.240 --> 54:36.320
But one key thing is that it highlighted trust in medical care as a key factor

54:36.320 --> 54:40.800
that drove multiple balancing and reinforcing feedback loops in this system.

54:40.800 --> 54:48.560
They also created a stock and flow model for this. Trust shows up as this kind of

54:49.200 --> 54:56.240
really big substructure that you can double click in. And the thing about doing this work is that

54:56.240 --> 55:03.120
it allows you to think about these soft variables and include them and approach

55:04.800 --> 55:08.720
bridging that qualitative quantitative gap, quantifying them in some way,

55:08.720 --> 55:14.560
but informed by people that approximate to the problem, but also informed by previous research

55:14.560 --> 55:20.080
that's happened. And so they developed a really detailed trust substructure, which I think actually

55:20.080 --> 55:28.960
should be in a whole lot of models. And that allowed them to develop a simulation and actually

55:29.520 --> 55:32.320
test some of the interventions that they had in mind that might make things

55:32.960 --> 55:35.680
better to include the reference model that we were solving for.

55:38.240 --> 55:44.320
Now, the key, my key takeaway from this was that, hey, you can do community-based research and

55:44.320 --> 55:52.000
actually produce useful research that comes from a partnership with community and people that are

55:52.000 --> 55:54.960
approximate to problems and better than problems. So this paper actually got

55:55.680 --> 56:00.880
accepted, presented at the System Dynamics Conference, what an honorable mission award.

56:03.280 --> 56:09.680
But the key thing I learned is that you can go after if we want to build capability in any place,

56:09.680 --> 56:15.360
but you cannot do that without building trust at the same time. And there's no way to rush this,

56:16.080 --> 56:22.880
right? It takes determination and patience and investment, but that's going to be one of the

56:22.880 --> 56:27.040
key ingredients if we are going to have any sort of chance to bridge this chasm, in my opinion.

56:27.360 --> 56:34.720
So I'm going to conclude with, first, that key takeaway that I started with. Interventions that

56:34.720 --> 56:42.160
ignore abstract way societal context can lead to unintended and unnecessary harm. I think they're

56:42.160 --> 56:50.080
likely to. And then those calls to action. If we're going to drive more attention and research in

56:50.080 --> 56:57.200
this space, we need folks like you all to spread the word. And then we also are really calling

56:57.200 --> 57:03.680
for people to embrace this concept of prototyping problems before you prototype solutions, prototype

57:03.680 --> 57:10.880
problems as systems, embrace that complexity before intervening with data science or AI,

57:11.440 --> 57:17.280
really with anything. And we think this is really one of the only ways to really proactively mitigate

57:17.920 --> 57:23.360
bias in these high stakes domains. And then finally, invest some of your time and energy. And if you

57:23.360 --> 57:29.840
have dollars, dollars in building problem prototyping, trust and capability, like in historically

57:29.840 --> 57:34.560
marginalized communities. With that, I'll close. Thank you, everybody.

57:39.200 --> 57:41.520
So we are missing one time, but there are a few questions.

57:41.760 --> 57:46.160
Yeah. Did you just work at Google? I'm sorry. Did you used to work at Google? No, no.

57:48.560 --> 57:55.360
So I'm wondering, great talk. Thank you. So I'm like studying in public health. And so one thing

57:55.360 --> 58:01.120
we're finding I'm thinking is that the problems start outside the hospital. So those precepts

58:01.120 --> 58:06.640
and the clinicians come from outside the hospital and it becomes a problem once we get in. So how

58:06.640 --> 58:12.240
much of the modeling should we think about outside hospitals in terms of like social and public

58:12.240 --> 58:19.600
investments, housing investment, basically treating that environment in the society and having that

58:19.600 --> 58:25.280
gradient sort of diffuse into the health care industry somehow. I think trying to solve the

58:25.280 --> 58:31.360
health care problem is a hard one. And then it seems like you have to go outside a little bit.

58:32.320 --> 58:37.360
Like we look at your way or somewhere like they invest in their public more than any of us probably

58:37.360 --> 58:41.680
in health care. And we do the opposite. Yeah, we're having this problem. I agree. I agree 100%.

58:41.680 --> 58:47.520
Like, I don't know. I may not have been clear in this talk, but what I think of things I emphasize

58:47.520 --> 58:52.720
in Google is that we have to start with trying to understand the problems that people care about

58:52.720 --> 58:58.320
outside of Google, independent of like our business problems. If we really want to actually,

58:58.400 --> 59:03.520
I think this actually leads to better business as well, start outside. What are the people outside

59:03.520 --> 59:07.920
in society? What problems do they care about and why? And how do we get to the point where we

59:07.920 --> 59:12.640
understand those problems? And then you're going to find some intersection in that problem domain

59:15.120 --> 59:20.160
between the problems that society cares about and the problems that we may have some hammers and

59:20.160 --> 59:26.000
nails to try to solve technology. But you have to start outside of that in order to really start

59:26.000 --> 59:30.960
to get any kind of ground truth. Yeah, it's like, you need to understand what the problem is. Yeah,

59:32.000 --> 59:35.680
I agree. That's why that's why I'm trying to talk about this problem understanding chasm because

59:35.680 --> 59:41.040
of this lack of understanding of the problem, particularly on the product development side.

59:41.040 --> 59:45.120
But but there's multiple ways to understand the problem. I think there are ways to leverage

59:45.120 --> 59:51.040
technology in good ways. But unless we have a deeper understanding that's shared between

59:51.040 --> 59:55.600
these two perspectives, we're not we're not going to be able to do it in a responsible way.

59:59.360 --> 01:00:04.080
I guess one thing you didn't mention that I expected you mentioning would be accountability,

01:00:04.080 --> 01:00:12.080
right? So for example, perhaps the reason some of the AI developers now are okay with the system

01:00:12.080 --> 01:00:19.600
saying no is Google search was always retrieving. Now with gen AI is generating stuff. So if you're

01:00:19.680 --> 01:00:25.440
generating something that causes me harm, then there's accountability to be had, right? And

01:00:25.440 --> 01:00:29.920
the laws are still not there. We'll see, right? Like for example, in Europe, in certain European

01:00:29.920 --> 01:00:35.040
countries, they abandon these kinds of large language models because people harm themselves

01:00:35.040 --> 01:00:40.960
based on what was told to them, what was generated as opposed to retrieval. So what are some of your

01:00:41.600 --> 01:00:48.960
thoughts on accountability here? Who's accountable if the system if the algorithm causes harm to

01:00:48.960 --> 01:00:54.160
the community? Well, so you're trying to give me to answer one of these questions that are that's

01:00:55.040 --> 01:01:00.560
you can pass it. But it's it's it isn't it you we are all part of the community, right? So we

01:01:00.560 --> 01:01:06.720
it's yes, I'm a computer scientist, but I'm also part of the community. Yeah. Yeah, I mean, I think

01:01:06.720 --> 01:01:15.360
so. So I think there's accountability in multiple places. I think the works that that's

01:01:15.440 --> 01:01:24.160
happening, that's emphasizing, you know, how we govern these, the development of these tools

01:01:24.160 --> 01:01:30.400
and products is really critical. If you pay attention to what's happening, what's coming out of

01:01:31.440 --> 01:01:40.320
the national out of NIST, they've got a framework that emphasizes that the very first step that

01:01:40.400 --> 01:01:46.480
has to happen for companies developing these systems to be responsible and accountable,

01:01:46.480 --> 01:01:52.080
it has to start with understanding context, which I think aligns directly with what we're talking

01:01:52.080 --> 01:01:57.760
about here in terms of understanding societal context. But I but again, like you said, there's

01:01:57.760 --> 01:02:05.280
all these incentive structures that will kind of resist that. And in the in the product development

01:02:05.280 --> 01:02:11.920
companies, but even outside, right? Right? Yeah, because the accountability is tied to the incentives

01:02:11.920 --> 01:02:18.080
and incentives then are tied to gaming the system, including the the the algorithm, the tool, and the

01:02:18.080 --> 01:02:21.920
bigger picture that you had, right? And so they ground around the goals. That would be interesting

01:02:21.920 --> 01:02:26.320
to model. You're like, you're like, you're that would be interesting to see what that, you know,

01:02:26.320 --> 01:02:30.000
I would love to see how you would hypothesize what that relationship is between those faculty.

01:02:30.000 --> 01:02:35.840
Oh, I could definitely do that. I'm a I'm a professor. Yeah. Yeah.

01:02:39.920 --> 01:02:45.600
Talk. And I'm typically interested for the final point about gathering a trust from the

01:02:45.600 --> 01:02:50.720
historical margin of people and getting and getting involved all the domain experts about the

01:02:50.720 --> 01:02:58.080
particular problem. And that maybe that you came from Google, and it's like a big company,

01:02:58.080 --> 01:03:02.320
which can we have to deal which have a capacity and maybe some resources to

01:03:02.880 --> 01:03:07.120
open a workshop or getting a workshop and getting involved for many. But

01:03:08.320 --> 01:03:13.040
even if we just concentrate on the historical margin with people, there's so many different

01:03:13.040 --> 01:03:18.240
people around there. And maybe you have to think about so many different parts of other people's

01:03:18.240 --> 01:03:23.840
and maybe not every company or production, the product maker have a resources or capacity to

01:03:24.560 --> 01:03:30.640
do build a certain kind of trust like that, although it is very important. Yeah. And so

01:03:30.640 --> 01:03:36.000
will be will there be any kind of like a substitute or other options to incorporate this or like an

01:03:36.000 --> 01:03:43.760
accelerating these procedures to, well, generally like an accessible to more companies or more

01:03:43.760 --> 01:03:49.680
industrial parts of this person, rather than getting getting like an opening a workshop and

01:03:49.760 --> 01:03:54.160
getting a building a relationship like an after like a more than five or 10 years.

01:03:54.160 --> 01:03:59.360
I think if it if there's some kind of other options or like if we can encourage other

01:03:59.360 --> 01:04:05.680
entrepreneurs to do this things more, I think it'll impact a lot. I would just know your

01:04:05.680 --> 01:04:11.280
like opinion about this. Yeah. I mean, so I agree with you 100% like it's a really hard problem.

01:04:11.280 --> 01:04:18.800
And I think this has got to require investment like not just for not just from product companies,

01:04:18.800 --> 01:04:27.760
because in some way, you know, we're not we're not in a great position to really be fully embedded

01:04:28.400 --> 01:04:34.480
in the problems that society cares about surely. Right. And so I think this is the option where

01:04:34.480 --> 01:04:39.040
places like Google need to see some leadership and some ownership in this space and kind of

01:04:39.040 --> 01:04:47.040
invest in capacity outside. So we have a gigantic sector of philanthropy, foundations, etc.

01:04:47.760 --> 01:04:52.560
who I think need to play like a critical role in this. One of the reasons we're trying to

01:04:52.560 --> 01:04:57.680
raise the awareness of this is just for what you described to get other people thinking about

01:04:58.240 --> 01:05:06.160
solutions to these really hard problems of like, you know, scale and and dealing with

01:05:06.160 --> 01:05:11.680
incentives that have to do with speed. Right. So not easy things to solve. But so this is why

01:05:11.680 --> 01:05:16.800
we're like, let's raise awareness, get people talking about this, and have more people

01:05:17.600 --> 01:05:23.280
applying their mind to finding solutions that are kind of responsible, non extractive,

01:05:23.280 --> 01:05:28.080
can accelerate things. I think there are ways to solve these. This is kind of like a moonshot.

01:05:28.080 --> 01:05:32.880
There are things you would have to invent that haven't been invented yet. We're trying to kick

01:05:33.280 --> 01:05:36.400
off the flywheel to get those conversations going. Yes.

01:05:38.400 --> 01:05:46.160
So one thing I'm interested in is what kind of pushback. It's like one phrase you didn't use,

01:05:46.160 --> 01:05:54.480
but I'm sure that we agree on it. It's also one of our favorite things to rail against

01:05:54.480 --> 01:06:01.280
is domain agnosticism. Right. Like, like, oh, I'm the clever technical person, send me your data,

01:06:01.280 --> 01:06:05.600
send me your zeros and ones. I don't even need to know what the zeros and ones mean.

01:06:05.600 --> 01:06:10.160
I have cool algorithms and then I'll help. Right. And so part of the goal is like

01:06:10.880 --> 01:06:17.840
breaking that as, you know, what I think still for a lot of engineers and engineering students,

01:06:18.400 --> 01:06:23.760
it's still kind of a norm and sometimes even held up as a virtual. So I mean, I'm totally,

01:06:24.720 --> 01:06:31.600
I love what you're pointing out. I guess I'm interested in two things,

01:06:31.600 --> 01:06:40.160
and I'm looking forward to our conversation later. One is the move here is to work with the community

01:06:40.800 --> 01:06:45.760
to generate models at first qualitative, but with the ambition of being quantitative.

01:06:46.960 --> 01:06:51.280
And of course, that's an interesting move. It's not the only move. And

01:06:52.080 --> 01:06:59.120
and then once you have these models, then lots of other tricky questions come up. Like, do we

01:06:59.120 --> 01:07:04.080
really think these models are going to be predictively accurate or are they just sort of scenario

01:07:04.080 --> 01:07:09.200
explanation tools or kind of, you know, are they quantitative models that we're actually trying

01:07:09.200 --> 01:07:15.840
to learn about qualitative effects of possible interventions? So that's, that's, I think that's

01:07:15.840 --> 01:07:22.720
an interesting and I like it. But I also I'm curious about its limitations. Yeah. And one of

01:07:22.720 --> 01:07:29.440
the limitations that I expect also some engineers might push back on. So like, in the criminal

01:07:29.440 --> 01:07:37.040
justice field, right, everybody says, absolutely, arrest is not the same as crime. It's not just

01:07:37.040 --> 01:07:42.480
a very noisy signal. It's also a systematically biased signal. And some people say, well, could we

01:07:42.480 --> 01:07:49.360
use conviction? It's like, well, that's also a noisy and biased signal. And of course, you know,

01:07:49.360 --> 01:07:57.760
society, neither the government nor engineers have access to did a crime actually occur.

01:07:59.440 --> 01:08:06.000
And so you get this sort of defensive reaction that, well, but this is the data we have, you know,

01:08:06.000 --> 01:08:10.320
so like, I think we've all been to a million talks where there's one slide at the beginning

01:08:10.320 --> 01:08:15.040
saying, arrest is not the same as crime, but this is the data we have, you know, and then,

01:08:15.040 --> 01:08:20.800
and then plunging into the technical, you know, the statistics of that as data. And I don't know,

01:08:20.800 --> 01:08:25.760
that's, I mean, maybe this just gets back to the humility question again about, you know,

01:08:26.800 --> 01:08:31.520
hey, we don't know if this data means what we think it means, we don't know if it's measuring

01:08:31.520 --> 01:08:39.280
we think it measures. And therefore, we're presenting our suggestions, our recommendations to

01:08:39.280 --> 01:08:47.680
a judge or a doctor with some humility, but then I don't know, I don't know how to wrestle with that,

01:08:47.680 --> 01:08:53.120
especially because part of the argument for doing all this is that humans clinical judgment is also

01:08:53.120 --> 01:09:02.560
very biased or can be. And so I don't know, there's like, which things can cross the the

01:09:02.640 --> 01:09:13.280
chasm into something which is structured enough that to to mathematics, and then

01:09:14.160 --> 01:09:20.560
to improve the algorithm, and which things can't really, and then the algorithm just has to say,

01:09:20.560 --> 01:09:25.520
hey, I don't know about that and keep in mind that I don't know about that, which as you said,

01:09:25.520 --> 01:09:29.120
as Tina said, is, you know, vendors are not usually incentivized to do.

01:09:30.080 --> 01:09:33.360
Yeah, I don't know, so that's like where that boundary is, I think is really interesting.

01:09:34.320 --> 01:09:36.800
Yeah, yeah, and you and you pointed out one of the

01:09:38.400 --> 01:09:44.960
the key root causes of this of this of this chasm, and that's this, this is the data that I have.

01:09:48.080 --> 01:09:52.240
Don't force me to kind of think about that messy stuff, right, on the other side of this

01:09:52.240 --> 01:09:57.600
boundary, the techniques that I have, I'm incentivized by those techniques, and they

01:09:57.760 --> 01:10:04.000
incentivize me to abstract those that stuff away. Now, and also the conversations and then it always

01:10:04.000 --> 01:10:10.720
goes to, I've spent a year talking to engineers about like, ah, we can, we can, we're gonna work

01:10:10.720 --> 01:10:14.320
on building these models. And the first question I get, but yeah, but how do you know they're

01:10:14.320 --> 01:10:20.400
accurate? How do you know they're predictably accurate? And then my pushback is, how do I know

01:10:20.400 --> 01:10:26.560
yours are predictably accurate? Right, you're very confident in all the assumptions you're making

01:10:26.560 --> 01:10:31.680
throughout that life cycle. No one's looking at them, they're not made explicit, right? And so

01:10:31.680 --> 01:10:36.320
eventually you actually get over that hump where people can say, oh, I can see the utility at least

01:10:36.320 --> 01:10:42.240
in the beginning of this, of me being more informed. And one of the key things about system dynamics,

01:10:42.240 --> 01:10:49.440
it emphasizes, don't try to start with data. Don't try to start with the data set. Start with your

01:10:49.440 --> 01:10:56.240
intuition and the intuition of others about what's important in this system, and start off by trying

01:10:56.240 --> 01:11:02.960
to get some sort of, you know, shared hypothesis. And then that should drive what data do I need if

01:11:02.960 --> 01:11:08.080
I'm trying to intervene on a problem in this system. And if I don't have data about this factor,

01:11:08.080 --> 01:11:13.440
maybe I shouldn't be trying to intervene on this particular problem right with this technology.

01:11:14.320 --> 01:11:18.320
And so this is why we're, you know, this is why we're kind of purposely focusing on

01:11:18.560 --> 01:11:26.480
high stakes domain, healthcare because of, you know, the issue of health equity,

01:11:27.280 --> 01:11:33.280
healthcare industry is kind of, I think, a little bit more mature in terms of looking for systemic

01:11:35.520 --> 01:11:40.800
causes of disparity and kind of embracing that. So there's kind of more openness about like,

01:11:40.800 --> 01:11:45.600
hey, we're trying to, we want to use these algorithms, but there's really a lot of

01:11:45.600 --> 01:11:49.680
worry and caution by clinicians and others because they know how dangerous they are.

01:11:50.400 --> 01:11:55.200
I think this is again where the accountability comes in, right? Because the doctor can use a tool,

01:11:55.200 --> 01:11:59.680
but at the end of the day, he's accountable, right? And to the question that I always get is like,

01:11:59.680 --> 01:12:06.880
well, the system is, but your AI system is biased, but the person who's making the decision is also

01:12:06.880 --> 01:12:11.360
biased. Yes, but the person who's making the decision, he can go to jail. I will sue his

01:12:11.360 --> 01:12:17.760
ass from here all the way to Boston, right? That AI system, who am I suing? Who's responsible?

01:12:17.760 --> 01:12:24.480
Yeah, right? It's not clear. And that is important, right? It is important to figure out who is

01:12:24.480 --> 01:12:28.880
accountable, especially in America, right? We like accountability, like, who do I blame?

01:12:30.640 --> 01:12:37.600
Nobody. I go to jail, nothing happens, right? Because the system said I'm high risk. And that's

01:12:37.600 --> 01:12:42.720
it. So the systems are being used as expert witnesses without being cross-examined and not

01:12:42.720 --> 01:12:48.320
being held accountable. I think in healthcare, it's a really good domain to pick, because at the end

01:12:48.320 --> 01:12:54.400
of the day, it's a doctor and the malpractice insurance he has. And so they have, you know,

01:12:54.400 --> 01:12:57.760
like you said, there's a vested interest in making sure these things work out. And I think there's

01:12:57.760 --> 01:13:02.720
also like, you know, the Hippocratic Oath as well, like doctors don't want to harm people, right?

01:13:03.440 --> 01:13:04.000
I'm large.

01:13:05.280 --> 01:13:10.800
But so what you're pointing out is like, you know, these technologies are like

01:13:10.800 --> 01:13:17.200
really, really serious interventions on society. And you're talking about like, we have to update

01:13:17.200 --> 01:13:23.760
our, the cannons of law, right? For these entities, these systems.

01:13:24.800 --> 01:13:29.360
Yeah. And I think especially now with the movement on democratization, right? I want to

01:13:29.360 --> 01:13:36.160
democratize X. Well, then this kid can go and develop clear view AI with like $200,000. No

01:13:36.160 --> 01:13:41.440
accountability. He didn't know about ethics or nothing. You know, it's what, wait, what?

01:13:42.320 --> 01:13:48.720
Yeah. So there are issues. I mean, I love your research area. I mean, it's similar to research

01:13:48.720 --> 01:13:54.240
area I've been pursuing. But we cannot not talk about accountability. Like we have to talk about

01:13:54.800 --> 01:13:59.840
Yeah, I agree. And I think we also have to make sure that at least for me, like

01:14:01.680 --> 01:14:07.280
I'm not, I'm not like, I, I can like, at least me personally begin to kind of understand like,

01:14:07.280 --> 01:14:11.040
all right, what do we, what do we have to do to update the cannons of law? Like they're experts,

01:14:11.040 --> 01:14:16.240
like that's one of the things I like about Andrew Selbs, his work, right? He's a law professor,

01:14:16.240 --> 01:14:20.800
right? So, and that's also why I like about his work about abstract and waste societal context.

01:14:20.800 --> 01:14:26.160
Because I think hopefully that means that kind of perspective will make its way into the laws.

01:14:26.160 --> 01:14:30.960
You see that perspective making its way into some of the frameworks that are coming out of

01:14:30.960 --> 01:14:34.960
the government in terms of how we're going to regulate these things. So it gives me some hope.

01:14:34.960 --> 01:14:39.840
But I do know that even if you come up with a law that says you have to understand the societal

01:14:39.840 --> 01:14:47.760
context, we are not in any position to actually represent it, present it, whether it's in a,

01:14:47.760 --> 01:14:54.160
you know, court of law or in a product development life cycle. And so we have to build capacity.

01:14:54.160 --> 01:14:59.600
Whereas somebody has to pay millions and millions of dollars, then people will, will move.

01:14:59.600 --> 01:15:03.600
Unless they have billions of dollars, then they, maybe we'll charge them billions.

01:15:03.600 --> 01:15:05.760
Billions. Billions to get to billions. I want billions.

01:15:08.240 --> 01:15:11.840
Sadly, we're coming up on time. But Donald, I want to thank you again for a great talk.

01:15:12.160 --> 01:15:14.480
Thank you for your time.

01:15:16.960 --> 01:15:20.160
For tomorrow afternoon, I know a number of you already signed up for meetings,

01:15:20.160 --> 01:15:24.960
but if you haven't, he's down in Pod C. So you're welcome to go and

