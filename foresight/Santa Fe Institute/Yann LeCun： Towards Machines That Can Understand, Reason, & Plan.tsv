start	end	text
0	2800	All right.
2800	7240	OK, first machines they can understand,
7240	9760	which means also reason and plan.
9760	12800	It's going to be a lot of overlap with what Josh said,
12800	19200	at least in terms of motivation, but not in terms of solutions.
19200	23640	OK, first statement is that machine learning sucks,
23640	26680	certainly compared to what we observe in humans and animals
26680	31280	and their ability to learn and learn efficiently.
31280	33600	You know, until recently, most of machine learning
33600	37120	was based on supervised learning,
37120	41760	required enormous amounts of label samples.
41760	45520	What has taken over the last few years
45520	47200	is self-supervised learning, which does not
47200	49440	require as many label samples, but still requires
49440	52440	a huge amount of samples.
52440	56040	And in the end, those systems of still relatively brittle
56040	59120	makes stupid mistakes, do not reason or plan,
59120	61480	compared to humans and animals that can learn
61480	64560	new tasks extremely quickly, because they understand
64560	69240	how the world works, presumably, and they can reason and plan,
69240	72440	and have certainly some level of common sense.
72440	77400	So in our systems of today, most of them anyway,
77400	80520	not absolutely all of them, but many of them,
80520	83240	have a constant number of computational steps
83240	85640	between their input and output, which
85640	88200	means that whatever reasoning they do
88200	90200	does not change, depending on whether it's
90200	93160	a difficult problem they're trying to solve or not.
93160	94400	They cannot really plan.
94400	96200	The only systems they can plan at the moment
96200	98320	are the ones that are designed to play games
98320	100640	or to control robots.
100640	104760	But things like LLMs do not plan.
104760	107000	So how do we get machines to do like humans,
107000	108640	which is to understand how the world works,
108640	110920	predict the consequences of actions
110920	114680	they might take, perform chains of reasoning
114680	117040	with a potentially unlimited number of steps,
117040	119720	and plan a complex task by decomposing them
119720	121720	into sequences or subtasks.
121720	125200	So let me start with this idea of self-supervised running,
125200	128480	which really has taken over the world of AI
128480	131080	over the last few years.
131080	134680	And it's the basic idea of essentially presenting
134680	137960	an input to a system, let's say a text, a window of text,
137960	144040	or video, or a few images, and hiding part of it,
144040	146480	and then training the system to capture the dependencies
146480	150600	between what is observed and what is not yet observed,
150600	153080	but eventually will be observed, whether it's
153080	157120	the future of a video or a different view of the same scene
157120	163680	from an image or words that have been obscured.
163680	165400	And I say capture the dependency.
165400	167160	I don't say predict because I'm going
167160	169520	to talk about models that don't actually predict,
169520	171160	but capture the dependencies.
171160	176160	So a very successful example is language models.
176160	178840	So self-supervised language models.
178840	181280	And the idea goes back a long time to do this.
181280	183440	I think the first paper to really kind of experiment
183440	190520	with this was paper in around 2010 by Colbert and Weston,
190520	193360	where they had this idea of essentially taking
193360	198240	a piece of text, corrupting it in some ways.
198680	202040	In modern versions, it consists in removing some words
202040	207440	from the text, and then training some giant neural net
207440	210240	to predict the words that are missing,
210240	215160	or just merely to tell you whether the text that is here
215160	217240	is legit or not legit.
217240	219480	That's a different way of doing it.
219480	222280	So this is how every modern NLP system
222280	224840	over the last four or five years has been trained.
224840	228600	And that has completely revolutionized not just
228600	230800	the research in NLP, but also the practice of it.
230800	235160	So all of translation, content moderation,
235160	238440	hate speech detection, all that stuff from social networks,
238440	240880	it all uses this kind of stuff.
240880	244280	And performance went up by a huge amount.
244280	250880	OK, so a special case of this is generative LLMs.
251760	256000	And similar things are used in images and video.
256000	259160	And there, the part of the text that you're hiding
259160	260240	is just the last word.
260240	262720	So you train a giant neural net to just predict
262720	265400	the last word in a sequence.
265400	268200	And then you can use this to produce outputs
268200	272320	auto-regressively, which means you give a window of text,
272320	274360	you get a system to produce a word,
274360	277360	and then you shift that word into the input
277360	279400	by shifting everything by one.
279400	281400	Predict the next, next word, shift that in.
281400	284080	Predict the next, next, next word, shift that in, et cetera.
284080	286160	That's auto-regressive prediction.
286160	288160	It's a major flaw with this approach.
288160	292640	This is how every single LLM today works.
292640	295120	But we should call them auto-regressive LLMs,
295120	298440	because I think future LLMs are not going to be like this.
298440	301160	But basically, every single one of them, some of which
301160	303680	you've probably never heard of.
303680	307000	So the ones from Faire, Blenderbot, Galactica, Lama,
307000	309520	Alpeca, which is fine tuning of Lama.
309520	313160	There is a new one now also.
313160	316120	Lambda, Bard from Google, Shinshila from DeepMind,
316120	318320	Chai GPT, GPT4, et cetera.
318320	322280	They're all auto-regressive LLMs.
322280	326120	And they train on gigantic amounts of data.
326120	328840	So we're talking one trillion tokens or something like this.
328840	331480	It would take a human reading eight hours a day,
331480	334400	something like 22,000 years to read this.
334400	336800	So obviously, those things can swallow a lot more
336800	339560	and digest a lot more data than any human.
339560	342400	And the performance is nothing short of amazing.
342400	344360	But they do make stupid mistakes.
344360	345880	They are extremely fluent.
345880	348720	So we can use them to generate text.
348720	355200	But they make factual errors, logical errors, inconsistencies.
355200	356760	They have limited reasoning ability.
356760	359800	There is no way to control for things like toxicity
359800	361560	and stuff like that.
361560	364720	And they really have no knowledge of the underlying reality,
364720	368440	except in one case, because, of course, they only
368440	370640	train from text, except in one case.
370640	374960	And that case is code generation.
374960	377480	And they work really, really well for code generation.
377480	380000	And the reason they work well is that the underlying reality
380000	381160	of code is very simple.
381160	381960	It's deterministic.
381960	384360	It's just the state of variables of a program.
384360	387560	And so that's fully observable, deterministic,
387560	388080	and everything.
388080	391680	So it works really well.
391680	394200	And they can generate fluent text.
394200	396560	But in this particular case, this is a joke
396560	399280	that my colleagues did on me.
399280	401640	It's completely made up.
401640	406800	I never actually did a rap album.
406800	408480	Raw personal.
408480	411480	Yeah.
411480	414000	I asked them if they, I don't actually like rap that well.
414000	416520	So I'm over a jazz person.
416520	418400	So I asked them to do the same thing with jazz.
418400	422480	And they say, there's not enough training data.
422480	425480	And I cried.
425480	426720	OK, so what are they good for?
426720	429320	They're good for writing assistance, generating
429320	432800	first draft, producing a style.
432800	435800	Code writing assistance, obviously, very efficient for that.
435800	438920	They're not good for producing factual and consistent answers
438920	441520	because of aducinations.
441520	443360	And they're not good for taking into account
443360	444800	recent information, because you need
444800	447920	to retrain the entire system to take into account yesterday
447920	449080	in real time.
449080	452560	And that's just not practical.
452560	455800	They don't behave properly, or at least they're
455800	457680	hard to control to do so.
457680	458760	They don't do reasoning.
458760	459520	They don't do planning.
459520	462880	They don't do math, as we saw this morning.
462880	464440	They're being modified to use tools,
464440	467440	such as search engines, calculators, stuff like that.
467440	474120	But currently, it's kind of like using duct tape and staples.
474120	476360	And we're easily fooled by their fluency into thinking
476360	479600	that they are smart, but they are not that smart.
479600	482080	Now, there is a major flaw with this autoregressive
482080	485560	generation, which is that it's an exponentially
485560	487360	diverging diffusion process.
487360	490840	So if there is the probability, e,
490840	493920	for every token that is produced to be outside
493920	495560	of the set of correct answers, let's
495560	498440	assume that errors are being independent,
498440	500440	then if we generate a sequence of n tokens,
500440	503440	the probability for that sequence to be correct
503440	505080	is 1 minus e to the power n.
505080	508480	And that decreases exponentially.
508480	510440	So those things just don't work.
510440	511840	They just don't work.
511840	513960	My prediction is that five years from now,
513960	517840	nobody in that right mind would be using autoregressive LNMs.
517840	521000	It's just a bad phase.
521000	521960	They are useful, though.
521960	522600	They're very useful.
525360	527600	So as I said, they have a constant number
527600	529640	of computational steps between input and output
529640	532480	for each token generated.
532480	534880	They do not reason and plan.
534920	537920	Jake Browning, who will be talking Wednesday,
537920	540280	and I wrote a philosophy paper.
540280	544560	I mean, he wrote it on the fact that there
544560	549320	are limitations to the purported intelligence
549320	552240	of systems that are purely trained from text,
552240	557400	because I would argue that most of human knowledge
557400	558800	is not textual.
558800	560360	I mean, certainly most of what babies
561320	565160	are on before six months is non-textual.
565160	567720	And everything that animals learn is non-textual.
567720	574440	So that knowledge is still unattainable to current AI
574440	576120	systems.
576120	579080	So how do we get machines to understand how the world works
579080	581240	and predict the consequences of their actions?
581240	583000	All the limitations have been pointed out
583000	587560	by a number of different papers, including one
587560	592840	by the MIT crowd, that fluency is really not
592840	597040	the same as thinking.
597040	599080	And basically, you could argue for the fact
599080	602520	that what LLMs are good for is perhaps modeling
602520	605240	the Browning-Key and Boracay areas,
605240	607040	but not much else in the brain.
607040	611080	And that's like tiny little areas on the side of the brain.
614240	617280	So we need something else.
617320	619600	What are we missing?
619600	621040	This is a chart that I like to show.
621040	623040	Oops, the animation is bad.
623040	627680	But it was put together by Emmanuel Dupu, who
627680	630480	kind of tends to indicate at what age babies
630480	634160	learn basic concepts, like object permanence, for example.
634160	636840	Liz was talking about that.
636840	639240	Stability and support and intuitive physics,
639240	644400	which only comes up fairly late, actually, around nine months.
644400	646400	And the question is, what type of learning
646440	648040	is taking place there?
648040	651360	No AI systems today really kind of does this properly,
651360	655480	although there's been several attempts by a few of us.
655480	658320	So I think perhaps it's this type of learning
658320	660400	that is the basis of common sense.
660400	662920	And we should really try to figure out
662920	665520	how to reproduce this with machines.
665520	667840	So I think there's three challenges for AI research
667840	670560	today, learning representations and predictive models
670560	674920	of the world, allowing machines to predict what's going to happen,
674960	677000	perhaps as a consequence of their actions.
677000	678040	Learning to reason.
678040	681480	So this is more like Daniel Kahneman's System 2.
681480	684440	Current autoregressive LLMs are basically System 1.
684440	686600	They just view one word after the other
686600	690680	without really planning ahead.
690680	693800	And so that is the question of making reasoning compatible
693800	696240	with learning.
696240	697920	Josh has a particular proposal for this,
697920	704520	which I don't agree with, but that goes in the right direction.
704560	707600	And then learning to plan complex action sequences.
707600	711400	So I made a proposal for this almost a year ago now,
711400	715560	which I posted on this website so people can make comments
715560	720720	and tell me I'm wrong and which references I missed.
720720	723560	I guess several technical talks about it as well.
723560	726680	And basically it's sort of a modular organization
726680	731640	of an AI system that would be capable of reasoning and planning.
731640	735120	And I can't say that I've built it,
735120	738040	but we're kind of building pieces of it.
738040	739160	So it's composed.
739160	742040	It's basically centered around the award model, which
742040	744480	will allow the system to predict ahead
744480	747440	what the consequences of its actions would be.
747440	752400	And it has a cost module.
752400	755840	Think of it as kind of visual ganglia stuff.
755840	757440	And the only purpose of the system
757440	759560	is to optimize that cost.
759560	761520	Some of those costs are essentially
761520	764320	intrinsic, hard-wired, immutable costs
764320	767800	that sort of drive the basic behavior of the system.
767800	769960	And some of them are trainable costs
769960	773960	that the system learns as it goes.
773960	777040	And what the system does is that it
777040	780240	plans a sequence of actions that, according to its model,
780240	783920	will minimize those costs.
783920	785600	And of course, it needs to be able to estimate
785600	787960	the current state of the world, which is done through perception
788400	790000	and maybe access to a memory.
790000	793440	And then depending on the task that the system is focusing on,
793440	795440	it can be entirely configured by a configurator
795440	798760	that will sort of focus the system on the task at hand.
798760	801520	So that's a cognitive architecture, which
801520	804440	some people in classical AI have been proposing,
804440	807760	but in sort of different forms.
807760	809160	And there's two ways to use it.
809160	813400	Mode 1, which is just a reactive perception action cycle,
813400	815120	get an idea of the state of the world,
815120	817720	encoding into an abstract representation
817720	819840	of the state of the world as 0, and then running
819840	822760	through some other neural net that produces an action
822760	824240	reactively.
824240	826280	But the more interesting mode is mode 2,
826280	828840	which is like Kettiman's System 2,
828840	831640	where you make an estimate of the state of the world,
831640	834840	and then using your word model, predict ahead of time
834840	837640	what's going to happen according to an imagined sequence
837640	840800	of actions that you might take.
840800	844960	And then the agent would optimize that sequence of actions.
845000	848000	So as to minimize a particular cost function,
848000	854200	representing the tax to be fulfilled.
854200	856520	And then it would just take the first action
856520	859440	and actually send it to the actuator,
859440	861240	or maybe the first few actions.
861240	863440	So this is completely classical in optimal control.
863440	866480	It's called model predictive control.
866480	870480	Except the problem here is how you learn the model.
870480	872680	There's a way to kind of turn System 2 into System 1,
872720	875120	which I'm not going to go into.
875120	878520	OK, so how do we train the world model?
878520	882480	Only for the fact that the world is not deterministic
882480	886040	or not entirely predictable, even if it is deterministic.
886040	890240	So we're not going to have a neural net observe the input
890240	894640	and just predict why, and then minimizing a prediction error.
894640	896240	That's not going to work, because that can only
896240	898120	make one prediction.
898120	900920	So in fact, if you train a big neural net
900920	905400	to predict like these are cars from a top-down view
905400	907040	of a highway, if you train a neural net
907040	909200	to try to predict what's going to happen in this video,
909200	911720	you get blurry predictions.
911720	914120	Because the system cannot predict if a particular car is
914120	917880	going to break or accelerate or turn left or right,
917880	920720	and so it makes these blurry predictions.
920720	922040	Same for a natural video.
922040	926920	That's an old work on video prediction.
926920	929720	So you have to account for the fact
929720	932680	that the world is not completely predictable.
932680	935120	And you have two solutions there.
935120	937280	Either you build an architecture with latent variables
937280	941320	that parameterizes the set of possible predictions,
941320	944960	or, and those two are not incompatible,
944960	947240	or you abandon the idea that you're
947240	949400	going to predict everything about the world.
949400	951440	And so this is what I'm suggesting.
951440	954080	So this is a generative architecture.
954080	957280	Generative architecture observes x, encodes it,
957280	960880	then predicts y, the variable whose dependency you're
960880	962600	trying to predict.
962600	964760	And then you measure the prediction error.
964760	967800	You mean my side by training, et cetera.
967800	971360	What I'm proposing is a joint embedding architecture
971360	975840	where both x and y go through encoders, neural nets,
975840	979400	and the prediction takes place in representation space.
979400	982320	What that allows the system to do is basically
982320	985640	eliminate a lot of irrelevant information from y
985640	989000	when it encodes it into SY so that it doesn't
989000	992000	have to predict all the details.
992000	995400	So there's a lot of things here and a lot of information
995400	999360	in this room that we cannot possibly remember or predict
999360	1004680	the precise texture of the wood on the floor, things like that.
1004680	1007320	But it's kind of irrelevant.
1007320	1010800	We only need to have sort of a relatively abstract
1010800	1012360	representation of it.
1012360	1013920	So I'm basically recommending to abandon
1013920	1016720	the whole idea of generative models,
1016720	1019400	unless you want to produce pictures or produce text.
1019400	1021360	But if you want to learn how the world works,
1021360	1023720	you should not reconstruct.
1023720	1025000	There's actually several versions
1025000	1027520	of those joint embedding architectures,
1027520	1032640	the simple one, deterministic ones that can predict,
1032640	1034200	and then nondeterministic ones that
1034200	1039440	can predict where the predictor can have latent variables.
1039440	1042720	So that's kind of the most general architecture.
1042720	1048400	And the latent variable A here can be a latent variable you
1048400	1050200	infer or it could be an action.
1050200	1052080	So imagine that this is a world model.
1052080	1053520	This is the current state of the world
1053520	1055640	that you observe, you encode.
1055640	1057600	This is an action you might take in the world,
1057600	1059920	maybe combined with some latent variable which
1059920	1062120	represent what you don't know about the world.
1062120	1065560	And then you make a prediction, and then
1065560	1067760	you can compare that prediction with what actually occurs
1067760	1069960	if you want to train your model.
1069960	1071320	And that's a predictive model that
1071320	1073440	will allow you to predict what's going
1073440	1075920	to happen as a consequence of your actions.
1075920	1080080	Now, because we're not generating anything,
1080080	1082360	and because we can't turn a model of this type
1082360	1085480	into a probabilistic model of t of y given x,
1085480	1088600	we have to abandon the whole idea of probabilistic modeling.
1088600	1093120	And now Josh is going, oh my god.
1093120	1095320	Isn't it just approximate probability at that point?
1095320	1095960	Isn't it?
1095960	1096460	No.
1096460	1096960	No?
1096960	1098840	No.
1098840	1099960	It's energies, OK?
1100000	1104320	So basically the name of the game here
1104320	1107960	is that you need to understand the system as computing
1107960	1110960	an energy function that captures the dependency between x and y.
1110960	1115880	So imagine the data points are those black spheres.
1115880	1118160	The energy function should take low values around the black
1118160	1120880	spheres and higher values outside.
1120880	1124640	And whether this energy function represents
1124640	1129480	the unnormalized log of some probability,
1129480	1131840	is irrelevant, you just want the energy
1131840	1134320	to be higher outside of the manifold of data.
1134320	1137200	And it will have captured the dependency between the variables.
1137200	1140120	And there's nothing more you need.
1140120	1142600	Now, the next question is, how do you train a system
1142600	1146240	to give low energy to stuff you observe and high energy
1146240	1147400	to stuff you don't observe?
1147400	1149600	And there are two methods, contrastive methods,
1149600	1152840	which consist in generating fake contrastive points whose
1152840	1154320	energy is going to push up.
1154320	1155640	And then regularized methods, which
1155640	1157240	I'm going to explain in a second.
1157240	1161120	So let's say you have training samples.
1161120	1162800	Your system currently gives low energy
1162800	1166640	to this sort of peak area here.
1166640	1170720	And it's not a good model of the data here,
1170720	1173280	because it gives high energy to data points and low energy
1173280	1176080	to areas that have no points.
1176080	1179000	So what you can do is generate green points here whose energy
1179000	1180720	you're going to push up.
1180720	1184080	And the energy function is going to take the right shape.
1184080	1186720	Or you could use some sort of regularizer
1186720	1190440	that minimizes the volume of space that can take low energy.
1190440	1192720	So that whenever you push down on the energy of some regions,
1192720	1194200	the rest has to go up, because there
1194200	1199920	is a limited amount of volume that can take low energy.
1199920	1203880	So in the context of joint embedding architecture,
1203880	1205680	I kind of invented the contrastive methods.
1205680	1207840	That's called sine is net in the old days.
1207840	1213520	But I'm now arguing against that in favor of regularized methods.
1213520	1215520	And the big question is, how do we train them?
1215560	1217640	I'll tell you about that in a minute.
1217640	1219720	But I'm asking you to abandon generative models,
1219720	1223440	abandon probabilistic models, probabilistic modeling in general,
1223440	1224960	abandon contrastive methods.
1224960	1226480	And of course, abandon reinforcement learning.
1226480	1231240	But that, I've been saying this for years.
1231240	1234560	Those are four of the main pillars of machine learning.
1234560	1240200	That makes me super popular among my colleagues.
1240200	1242040	OK, so what are those regularized methods
1242040	1244040	for joint embedding architectures?
1244080	1246360	So essentially, there is a big issue
1246360	1250360	that you have to fix, which is that when you train a system
1250360	1252360	like this, one of those JEPA architecture,
1252360	1254720	joint embedding predictive architectures,
1254720	1256840	you show it an example of x and y.
1256840	1260040	And you tell it just train all the weights of all those neural
1260040	1264640	nets so as to minimize the prediction error, it collapses.
1264640	1267280	Basically, what it says is that, well, I can just
1267280	1272800	set Sx and Sy to constants and set the prediction,
1272840	1275520	set the predictor to some constant thing
1275520	1277640	and ignore x and y all together.
1277640	1280200	And that would be a collapse system that gives zero energy
1280200	1282120	to everything in your space.
1282120	1283560	You have to prevent that from happening.
1283560	1285560	And one way to prevent that from happening
1285560	1289600	is finding a way to maximize the information content
1289600	1294120	of the representations that come out of the encoders.
1294120	1297120	That actually has the effect of minimizing the volume of stuff
1297120	1302520	that can take your energy indirectly.
1302520	1307240	So one way to prevent the outputs from being constant
1307240	1310160	is that you can force the variance to be non-zero.
1310160	1312840	So you put a cost function on top of this vector here
1312840	1315160	that says, over a batch of samples,
1315160	1318760	I want the variance of each variable coming out of that neural
1318760	1320880	net to be non-zero, to be above one, let's say.
1320880	1322840	So that's a hinge loss that says the variance
1322840	1324320	needs to be above one.
1324320	1325800	It's not enough because the system
1325800	1330640	can still cheat by making all the variables the same
1330640	1332280	or very highly correlated.
1332280	1333800	So you have another cost that says,
1333800	1336440	I want them to be decorrelated.
1336440	1338160	So basically, this has the effect
1338160	1341920	of enforcing the covariance matrix of that those Sx
1341920	1346840	vectors over a batch to be close to the identity.
1346840	1349440	And it's not enough because the variables
1349440	1354240	can be non-collapsed and correlated
1354240	1356240	but still dependent.
1356240	1357920	And so there's another trick that we do,
1357920	1361360	and we have some theory that shows that it's not stupid,
1361360	1362760	which is that you take the Sx vector,
1362760	1365280	you run it to some neural net that expands the dimension,
1365280	1367960	and then you apply those criteria on the covariance
1367960	1369080	matrix to the output.
1369080	1372680	And that makes the variables of Sx kind of more independent.
1372680	1375240	Now, there's a major flaw with this, which is,
1375240	1377920	and that's the theory which I'm not going to talk about.
1377920	1381200	There's a flaw with all of this, which is that we're basically,
1381200	1383120	we have an upper bound on information content,
1383120	1385960	and we're pushing it up, hoping that the actual information
1385960	1388080	content will follow.
1388080	1389680	And it's stupid, but it kind of works.
1391600	1394240	OK, so you can test those pre-training
1394240	1395240	for image recognition.
1395240	1398080	For example, you show two different views of the same image,
1398080	1401440	train the network to produce identical representations
1401440	1403440	for two different views of the same image,
1403440	1407240	and then you freeze the network and basically train
1407240	1409640	a linear classifier on top with ImageNet
1409640	1411120	and measure the performance.
1411120	1416240	And this Vcrag method that I just described
1416240	1421200	works just as well as isn't the top pack, let's say.
1421240	1422560	There's a bunch of different methods
1422560	1424440	that have similar performance.
1424440	1425800	And they are in the top pack.
1425800	1427160	I'm not going to bore you with details.
1427160	1429600	You can try to do segmentation as well.
1429600	1431720	Here's another method, somewhat similar,
1431720	1434880	but closer to the JEPA idea, which
1434880	1437200	uses a different criterion to prevent collapse, which
1437200	1438680	I'm not going to explain.
1438680	1443120	And this one takes a partially masked input image
1443120	1445440	together with a full input image, runs both of them
1445440	1448440	through encoders, and then trains a predictor
1448440	1453280	to basically predict the representation of the full image
1453280	1456240	from the representation computed
1456240	1459800	from the partially masked image.
1459800	1461800	This is called IJEPA, ImageJEPA.
1461800	1463000	And it works amazingly well.
1463000	1469320	And it's really fast to train very good performance.
1469320	1474200	In terms of performance, even though this type of masking
1474200	1476680	does not require any knowledge about the nature of the input,
1476720	1479760	essentially, or very little, the still
1479760	1481040	you get the same kind of performance
1481040	1484760	that you would get if you used a self-supervised learning
1484760	1487800	method that exploits the fact that you're
1487800	1491680	doing image recognition, like Dino or Ibot or Simclear,
1491680	1492200	for example.
1495080	1499480	OK, now, how are you going to use this in the end?
1499480	1505120	What I'm really interested in is to use JEPAs as world models
1505120	1506040	inside of the system.
1506040	1508320	They can do system two type planning,
1508320	1512520	but even better than this, they can do hierarchical planning.
1512520	1518920	And the idea there is that when you think about a task,
1518920	1522720	you're not planning this task at the lowest level
1522720	1526720	in terms of millisecond by millisecond muscle control.
1526720	1528200	You're playing a task like, I want
1528200	1531640	to go from Santa Fe to New York, or let's say
1531640	1535400	from New York to Santa Fe, that's a better example.
1535400	1538920	So you first decompose this into two sub-tasks.
1538920	1541200	First thing I need to do is go to the airport
1541200	1543200	and catch a plane.
1543200	1544280	How do I go to the airport?
1544280	1545360	Well, to go to the airport, I need
1545360	1546960	to get on the street and have a taxi,
1546960	1551040	which you can do in New York City, not in Santa Fe.
1551040	1552360	How do I get down in the street?
1552360	1555880	I need to get out of the building I'm in, et cetera.
1555880	1557040	How do I get out of the building?
1557040	1559560	I need to stand up from my chair, walk to the door.
1559560	1561240	How do I get up from my chair?
1561240	1563000	So you kind of decompose this all the way down
1563000	1566320	to the lowest level millisecond muscle control.
1566320	1570720	But you're not going to plan the entire task of going
1570720	1576720	from New York to Santa Fe all the way down to millisecond
1576720	1577960	by millisecond muscle control.
1577960	1580280	You do a hierarchical planning.
1580280	1582760	We think humans, that we are the only ones who can do this.
1582760	1584680	Animals do this, too.
1584680	1586720	You observe the cat planning a trajectory
1586720	1589480	to jump on a piece of furniture.
1589520	1591440	They definitely do a hierarchical planning.
1591440	1594280	So basically, what you do, what you need for this
1594280	1599200	is a sort of hierarchy of JPA architectures of predictors
1599200	1601760	that progressively produce more and more
1601760	1604840	abstract representations of the state of the world,
1604840	1610720	so that in the very abstract space of representations,
1610720	1612120	you can make long-term predictions.
1612120	1615560	Whereas in the sort of lower levels of abstraction,
1615560	1617640	you can make shorter term prediction,
1617640	1619960	but they're more accurate in the short term.
1619960	1622200	So this is a two-level architecture.
1622200	1624240	Low-level, you can make short-term predictions.
1624240	1626240	High-level, you can make longer-term prediction
1626240	1628800	in a more abstract space that has less details
1628800	1631200	about how the world works.
1631200	1636000	Now, we've been able to train a particular instance of JPA
1636000	1638080	that simultaneously learns teachers that
1638080	1642240	are good for image recognition and motion prediction
1642240	1643680	in images.
1643680	1645920	And I'm not going to go into the details of how
1645920	1646920	this is pretty hairy.
1646920	1649160	But it's kind of hierarchical.
1649160	1654840	And it's got predictors that make pretty strong assumptions
1654840	1657680	about the type of prediction that can occur.
1657680	1659840	And simultaneously learns invariant features
1659840	1662040	for image recognition.
1662040	1665560	And this works really well for things like image segmentation,
1665560	1669000	depth estimation, tracking, et cetera.
1669000	1675320	It's called MCJPA, which means motion and content.
1675760	1678760	And with this, hopefully, one day,
1678760	1681680	we'll be able to build architectures that
1681680	1683760	can perform hierarchical tanning of the type that I
1683760	1685800	was telling you about.
1685800	1691680	So observe the world, compute the abstract representation,
1691680	1693360	and even more abstract representation,
1693360	1695480	even more abstract representation,
1695480	1698120	make a prediction to minimize a particular cost
1698120	1700560	function that defines your task.
1700560	1702640	I'm assuming this cost function is differentiable,
1702640	1706560	so we can do this inference by gradient descent.
1706560	1707880	In first, some latent variable that
1707880	1709920	may represent the macro action you're going to take,
1709920	1713720	or some unknown variable about the world.
1713720	1715520	And then the state you're going to obtain
1715520	1716960	through the first prediction is going
1716960	1720480	to constitute a cost function for the lowest level.
1720480	1725440	So the first predictor at the top tells me
1725440	1727320	I should be at the airport.
1727320	1728760	I started from New York.
1728760	1730160	I should be at the airport.
1730200	1734360	The cost function below measures how far I am from the airport.
1734360	1737640	And so the second predictor says, go down in the street.
1737640	1740360	Take a cab to the airport.
1740360	1743520	And so the cost function at the bottom here
1743520	1746120	says, am I on the street?
1746120	1748960	Likely to catch a taxi, and all the way down
1748960	1752520	to the actual actions that you can take in the real world.
1752520	1756120	All right, coming to the conclusion.
1756160	1760680	So steps towards autonomous AI systems.
1760680	1761600	Self-supervised learning.
1761600	1764240	We need a recipe that allows us to train systems
1764240	1765960	to learn how the world works on video.
1765960	1768440	I can't claim that we have achieved this.
1768440	1772120	We're kind of partially there.
1772120	1774400	And legal uncertainty in the prediction,
1774400	1778840	and that's with a combination of this JEPA architecture
1778840	1782520	understood within the context of energy-based model,
1782520	1787480	potentially with latent variables, which I didn't talk about.
1787480	1790120	That would allow us to learn world models from observation,
1790120	1793200	hopefully hierarchical world models, possibly
1793200	1796920	with interaction as well, and exploration.
1796920	1800320	And now what we have is an architecture capable of reasoning
1800320	1800960	and planning.
1800960	1803080	I mean, the whole architecture I presented
1803080	1805240	is kind of this idea of system two,
1805240	1807920	that you can decompose complex tasks into simpler ones,
1807920	1809360	and then plan a sequence of actions
1809360	1811560	before you take the action.
1811600	1815800	Something that's sort of missing from current autoregressive systems.
1819320	1827880	So is this a potential path towards sort of human-level AI?
1827880	1830160	Possibly yes, but it's certainly not for tomorrow.
1830160	1833120	This is maybe a 10-year plan, maybe
1833120	1837160	to get to cat-level intelligence or something like that.
1837160	1840200	Now interestingly, those machines will have inevitably
1840240	1843520	some sort of emotion consciousness.
1843520	1845840	Forget about this, but emotions certainly,
1845840	1850560	because emotions are kind of an anticipation of outcome, most of them.
1850560	1855760	I mean, some of them are immediate perception of outcome,
1855760	1857360	like pain and things like that.
1857360	1859400	But most of them are anticipation of outcome,
1859400	1862320	and this cost function is exactly what this is.
1862320	1866640	And so if the system sort of predicts a particular set of outcome
1866640	1875920	that results in a bad outcome, it might feel something similar
1875920	1877200	to fear or something of that type.
1879960	1885640	Anyway, so common sense is a collection of world models,
1885640	1889000	or perhaps a single world model that is configurable.
1889000	1892320	I'll come to this in one second.
1892320	1895920	Understanding really means being able to predict.
1895920	1899080	I think prediction is really the essence of intelligence here,
1899080	1904440	and better mental models need to better understanding,
1904440	1909880	or other substrate, if you want, of understanding.
1909880	1915680	And as a consequence, also of good reasoning and action planning.
1915680	1918240	The complex part in all of this is going
1918240	1920480	to be to design intrinsic cost functions that
1920480	1925040	drive the system towards learning appropriate things.
1925080	1931840	And it's quite possible that, in the case of leaving things,
1931840	1936320	it's easier for evolution to hardwire your cost functions into us
1936320	1938440	than to hardwire your behavior.
1938440	1943320	Hardwiring behavior and physical models and whatever is super hard.
1943320	1947000	Like, as a neural net person, I would have no idea
1947000	1949400	how to architect neural nets to do this.
1949400	1953280	But I can certainly design a cost function that, if minimized,
1953280	1957120	the system will learn those basic concepts.
1957120	1962240	And that, there is a lot of hardwiring in there, no question.
1962240	1964760	So one module I didn't talk much about is the configurator.
1964760	1967880	And what it's supposed to do is configure all the modules
1967880	1971840	in this architecture for a particular sub-task
1971840	1975760	that the system needs to be focusing on at the moment.
1975760	1977880	And I'm imagining that there is actually
1977880	1981400	a single world model engine in this architecture that
1981400	1984320	is reconfigurable for the task at hand.
1984320	1987680	But it's not like the system would have multiple world models
1987680	1988960	for different situations.
1988960	1990800	It's got a single one that's configurable.
1990800	1995960	The advantage of doing this, I mean, for humans and animals,
1995960	1998960	is that it might actually fit in your skull.
1998960	2002320	But there is another algorithmic advantage,
2002320	2007080	or epistemic advantage, which is that a single one model can
2007080	2010120	share knowledge between different situations.
2010600	2014080	Whereas if you had a separate world model for different situations,
2014080	2016360	you would have to retrain it independently
2016360	2018040	for each of those situations.
2018040	2020800	So how to make this configurator work, I have no idea.
2020800	2023840	But that's a good hypothesis.
2023840	2026480	So that would explain the fact that there is a single world model.
2026480	2029360	It would explain why humans and many animals
2029360	2033120	can only focus on the single conscious task at any one time.
2033120	2037760	Because we only have one world model.
2037800	2043120	We can only do system two on one task at a time.
2043120	2045040	And I just leave the question for.
