WEBVTT

00:00.000 --> 00:08.520
And the director of Hardin's really great because Gerd is one of the originator of the

00:08.520 --> 00:13.520
theory of ecological rationality at the simple computational models of rules, algorithms

00:13.520 --> 00:17.240
that people use to make decisions in different contexts.

00:17.240 --> 00:22.960
And he's basically introduced complex system science to psychology, cognitive science because

00:22.960 --> 00:28.680
his main thesis is that we cannot understand cognition without looking at its social and

00:28.680 --> 00:34.080
physical environment, only by careful analysis of both the environment and the algorithms

00:34.080 --> 00:36.760
people use to make decisions and solve various problems.

00:36.760 --> 00:40.880
We can understand how people behave and why people behave in different situations.

00:40.880 --> 00:45.640
Gerd is a recipient of numerous awards.

00:45.640 --> 00:51.600
He is member of the Academy of German Academy of Sciences, author of numerous books translated

00:51.600 --> 00:52.600
in many different languages.

00:52.600 --> 00:53.600
And he plays banjo.

00:53.600 --> 01:00.600
So we need to have a closer look at the question.

01:00.600 --> 01:07.200
All right, thank you.

01:07.200 --> 01:10.200
Thanks Mirta.

01:10.200 --> 01:15.280
Today I will talk about how to make good decisions.

01:15.280 --> 01:23.240
If you open a book on rationality in neoclassical economics, behavioral economics, or psychology

01:23.240 --> 01:29.080
of philosophy, you likely encounter the following message.

01:29.080 --> 01:36.120
Good decisions follow the laws of logic, the calculus of Bayesian probability and the maximization

01:36.120 --> 01:39.320
of expected utility.

01:39.320 --> 01:46.840
This is beautiful mathematical theory, but it does not describe how most people actually

01:46.840 --> 01:53.680
make decisions, not even those who write these books.

01:53.680 --> 01:56.120
Let me start with a story.

01:56.120 --> 02:03.120
A professor of decision theory at Columbia University in New York had an offer from a

02:03.120 --> 02:05.400
rival university.

02:05.400 --> 02:07.320
It was Harvard.

02:07.320 --> 02:10.360
And he could not make up his mind.

02:10.360 --> 02:11.360
Should he stay?

02:11.360 --> 02:12.360
Should he leave?

02:12.360 --> 02:13.360
Should he accept?

02:13.360 --> 02:14.360
Reject.

02:15.040 --> 02:20.040
A colleague took him aside and said, what's your problem?

02:20.040 --> 02:24.520
Just maximize your subjective expected utility.

02:24.520 --> 02:28.640
You always write about that.

02:28.640 --> 02:37.960
Exasperated, the professor responded, come on, this is serious.

02:37.960 --> 02:44.520
I'll invite you today in a short tour in our research at the Max Planck Institute for

02:44.520 --> 02:49.640
Human Development on how people actually make decisions.

02:49.640 --> 03:00.760
And I'll start with a key distinction between risk and uncertainty.

03:00.760 --> 03:10.200
A situation of risk is a well-defined, stable, small world where the agents know the exhaustive

03:10.200 --> 03:16.680
and mutually exclusive set of all future states and the exhaustive and mutually exclusive

03:16.680 --> 03:25.920
set of their actions, of the consequences given their actions and states.

03:25.920 --> 03:33.640
Jimmy Savage, who is often referred to as the father of modern Bayesian decision theory,

03:33.640 --> 03:39.800
called this a small world.

03:39.800 --> 03:41.760
There are examples for the small worlds.

03:41.760 --> 03:50.960
If this evening you go to the casino in Santa Fe, if there is one, and play roulette, you

03:50.960 --> 03:53.480
are in a small world.

03:53.480 --> 04:01.400
You know every possible state that can happen, from zero to 36, all the consequences and

04:01.400 --> 04:05.400
probabilities.

04:05.400 --> 04:13.240
The tools for dealing with risk or maximizing expected utility, Bayesian probability updating

04:13.240 --> 04:17.160
and many more.

04:17.160 --> 04:24.280
In a small world, by definition, nothing new can happen.

04:24.280 --> 04:33.320
I had the quote from Fjodor Dostoevsky, who said, in this world, if everyone would be

04:33.320 --> 04:36.560
rational, he said, nothing would happen.

04:36.560 --> 04:43.400
With amazing foresight, he had seen the development of rationality.

04:43.400 --> 04:50.280
Now there are two, most of the problems have to do with, however, with some aspect and some

04:50.280 --> 04:53.080
amount of uncertainty.

04:53.080 --> 05:01.400
Uncertainty is a vast sea of situations where there is no small world where we either cannot

05:01.400 --> 05:07.800
know all future possible states or their consequences.

05:07.800 --> 05:15.200
It's not just about probabilities, it's about the state space.

05:15.200 --> 05:17.680
Researchers have two ways to go.

05:17.680 --> 05:28.360
One is to take an everyday problem, like investment, whom to hire, or whom to marry.

05:28.360 --> 05:36.360
These are all situations of uncertainty because things can happen that you had not anticipated.

05:36.360 --> 05:45.240
Then go one way, reduce it to a small world where everything is known and you can maximize.

05:45.240 --> 05:53.120
That's the way that most theories in neoclassical economics, also behavioral economics, at

05:53.120 --> 05:56.000
least the standards, are going.

05:56.000 --> 06:05.560
The other option is to take uncertainty seriously, face it, and develop and study the tools that

06:05.560 --> 06:11.920
actual people use to make decisions in an uncertain world.

06:11.920 --> 06:16.600
Here by definition, optimization is not possible.

06:16.600 --> 06:26.520
You cannot construct even a subjective probability distribution over a set that you don't know,

06:26.520 --> 06:30.160
that you don't know completely.

06:30.160 --> 06:37.960
I will talk today about one class of tools that are useful under uncertainty, and these

06:37.960 --> 06:41.120
are heuristics.

06:41.120 --> 06:52.360
Heuristics are rules that embody the art to focus on the important and ignore the rest.

06:52.360 --> 07:03.600
Heuristics can lead to good decisions in a world where we cannot forecast the best decisions.

07:03.600 --> 07:10.600
Examples are Herbert Simon's satisfying, Boston Fruity Trees, imitation, agent-based

07:10.600 --> 07:21.680
models who use simple rules like the flocking of starlings, the beautiful models.

07:21.680 --> 07:28.480
I will today go a different way than most of the models are going.

07:28.480 --> 07:35.600
For instance, even behavioral economics, who criticizes neoclassical economics, criticizes

07:35.600 --> 07:45.600
not normative standard, not homo-economics, not the maximization of expected utility,

07:45.600 --> 07:53.160
rather takes a standard to evaluate people, and if there is a discrepancy, the blame is

07:53.160 --> 07:55.760
never on the theory.

07:55.760 --> 07:58.560
It's always on the people.

07:58.560 --> 08:05.480
And then you get the list of biases that are all in your minds, and explain why you all

08:05.480 --> 08:13.200
make these strange decisions, and stubbornly have no insight in your failures.

08:13.200 --> 08:15.120
This is not my message.

08:15.120 --> 08:22.440
I want to study how people make decisions and formalize the heuristics, and then find

08:22.440 --> 08:29.120
out in what situation do they work and where do they not work.

08:29.120 --> 08:33.680
That's the question of ecological rationality.

08:33.680 --> 08:41.760
Rationality is not in the mind, but in the adaptation of mental strategies to certain

08:41.760 --> 08:43.640
environments.

08:43.640 --> 08:50.400
Let me start with an example that makes the difference clear between a model made for

08:50.400 --> 08:58.400
risk and model made for uncertainty.

08:58.400 --> 09:07.240
Every Markowitz got the Nobel Memorial Prize in economics for solving a certain problem.

09:07.240 --> 09:15.640
The problem is you have a number of N assets, and you want to invest your money, and you

09:15.640 --> 09:21.200
want to diversify, not put everything in one pocket, but how?

09:21.200 --> 09:25.040
The answer is the so-called mean variance model.

09:25.040 --> 09:27.440
It's a standard probability model.

09:27.440 --> 09:35.520
You need to estimate all the future returns, their variances, and covariances.

09:35.520 --> 09:42.080
When Harry Markowitz made his own decisions about the time of his retirement, he used

09:42.080 --> 09:45.800
his Nobel Prize winning optimization method.

09:45.800 --> 09:50.680
So we might think, no, he did not.

09:50.680 --> 09:59.840
He used a simple heuristic that we call 1 over N. N is the number of assets or options.

09:59.840 --> 10:04.280
So if it's 2, you do 50-50.

10:04.280 --> 10:09.840
If it's 3, a third, a third, a third, and so on.

10:09.840 --> 10:15.040
Now 1 over N is a heuristic.

10:15.040 --> 10:19.040
The mean variance model is an optimization model.

10:19.040 --> 10:25.200
The mean variance model assumes we are in a world that is stable, where we can estimate

10:25.200 --> 10:31.320
the parameters to some degree of precision.

10:31.320 --> 10:37.280
Number of studies have looked at 1 over N.

10:37.280 --> 10:45.240
And for instance, the study by D. Miguel here has looked at seven real-world investments

10:45.240 --> 10:54.360
and found that in six of the seven cases, 1 over N made more money than Markowitz's optimization

10:54.360 --> 11:00.280
as measured by sharp ratios and similar ones.

11:00.280 --> 11:08.880
The results have been found when exchange-traded funds were tested, ETFs which are close to

11:08.880 --> 11:13.080
1 over N, they're very hard to beat.

11:13.080 --> 11:18.760
Now there is a battle in the literature, which one is better?

11:18.760 --> 11:21.440
Complex optimization or simple heuristic?

11:21.440 --> 11:24.280
This is the wrong question.

11:24.280 --> 11:25.840
None of this is better.

11:25.840 --> 11:28.000
No algorithm is the best.

11:28.000 --> 11:30.520
We need to ask a different question.

11:30.520 --> 11:41.200
So can we identify the environment, the conditions where a simple heuristic like that does better

11:41.200 --> 11:47.760
than another one, another model, and where it's the opposite?

11:47.760 --> 11:54.160
The answer to this question of ecological rationality is not known.

11:54.280 --> 11:56.120
Some of you might figure it out.

11:56.120 --> 11:59.000
Here's some hypothesis.

11:59.000 --> 12:09.080
So if you think in terms of the bias variance decomposition, then 1 over N has probably

12:09.080 --> 12:19.600
a strong bias but makes no error due to variance because it doesn't estimate any parameters.

12:19.600 --> 12:25.680
So the error due to variance means that you get different results depending on what sample

12:25.680 --> 12:28.600
you make your estimates.

12:28.600 --> 12:39.640
So the real question to solve here is when is the bias that 1 over N has larger than

12:39.640 --> 12:50.200
the total error that mean variance makes, that is both bias and variance.

12:50.200 --> 12:54.840
So this is the way I would like to think here.

12:54.840 --> 12:59.000
Excuse me, did Markowitz ever say why he chose 1 over N?

12:59.000 --> 13:00.000
Yes, he did.

13:00.000 --> 13:03.760
He had a psychological explanation.

13:03.760 --> 13:13.200
He said if I would, his choice was between the typical retirement situation, just stocks

13:13.200 --> 13:14.200
or bonds.

13:14.200 --> 13:21.320
If I would put everything in bonds and they would go down, I would feel bad.

13:21.320 --> 13:23.120
If the opposite, I would also feel bad.

13:23.120 --> 13:24.960
I would have regret.

13:24.960 --> 13:33.240
So I just did 50-50 to avoid regret, but it's very different.

13:33.240 --> 13:41.120
He was actually very interested when we and others showed that he wasn't at least where

13:41.120 --> 13:42.120
his experiments are.

13:42.120 --> 13:51.280
It's just stocks, a slightly different situation that was shown that simply can do better.

13:51.280 --> 14:01.440
So the confusion between situations of uncertainty with risk, the idea that every situation of

14:01.440 --> 14:07.960
what uncertainty is one of risk is called the turkey illusion.

14:07.960 --> 14:09.960
So why?

14:09.960 --> 14:13.240
Assume you are a turkey.

14:13.240 --> 14:20.880
It is the first day of your life and a man comes in and you fear he will kill me, but

14:20.880 --> 14:21.880
he feeds you.

14:21.880 --> 14:27.680
The second day of your life, the man comes again, you fear he might kill me, but he feeds

14:27.680 --> 14:28.680
you.

14:29.320 --> 14:31.480
Third day of your life, same thing.

14:31.480 --> 14:38.800
If you do Bayesian updating or any similar model, the probability that the man feeds

14:38.800 --> 14:43.320
you and doesn't kill you gets higher every day.

14:43.320 --> 14:51.200
And on day 100, it is higher than ever before, but it's the day before Thanksgiving and you

14:51.200 --> 14:53.200
are dead meat.

14:53.200 --> 14:57.880
So the turkey was not in a situation of risk.

14:57.880 --> 15:01.600
He missed important information.

15:01.600 --> 15:04.520
There was an option he could not think.

15:04.520 --> 15:13.360
Now the turkey illusion is probably not so often committed by turkeys, but more often

15:13.360 --> 15:15.680
by people.

15:15.680 --> 15:21.360
And here's one example.

15:21.360 --> 15:29.480
In 2003, in his presidential address to the American Economic Association, Robert Lucas,

15:29.480 --> 15:37.040
the macroeconomist, argued that one has learned from earlier depressions and macroeconomics

15:37.040 --> 15:38.880
models take care.

15:38.880 --> 15:44.880
So he said, my thesis in this lecture is that macroeconomics has succeeded.

15:44.880 --> 15:51.720
Its central problem of depression prevention has been solved for all practical purposes.

15:51.720 --> 16:01.560
Now what you get is an illusion of certainty if you apply the models that are based on

16:01.560 --> 16:04.640
assumptions of risk to one of uncertainty.

16:04.640 --> 16:07.640
I've shown you just the illustration here.

16:07.640 --> 16:15.920
In the year 2003, when he gave this talk, the Volateli Index, we are X, went down and

16:15.920 --> 16:16.920
down and down.

16:16.920 --> 16:22.280
So it got better and better and better and better, safer, until shortly before the crisis

16:22.280 --> 16:24.960
hit.

16:24.960 --> 16:30.240
And that's the same thing as the mass modeling models they used.

16:30.240 --> 16:36.760
By the way, the models that Lucas described in his presidential address all assumed a

16:36.760 --> 16:42.640
stable and unchanging structure of the economy.

16:42.640 --> 16:45.600
So what's the alternative?

16:45.600 --> 16:51.000
And here's the research program I and my research group has been following up.

16:51.000 --> 16:52.640
And it has three questions.

16:52.640 --> 16:54.880
The first one is descriptive.

16:54.880 --> 17:03.200
What is in the adaptive toolbox of an individual, an organization, or a species?

17:03.200 --> 17:10.440
So what are the heuristics they use, the cognitive capacities that the heuristics exploit?

17:10.440 --> 17:13.320
And what's the building blocks?

17:13.320 --> 17:22.200
And the goal is to use, to develop algorithmic models of these heuristics, not labels.

17:22.200 --> 17:29.560
We have since the 1970s a tradition in the heuristics and biases program of labels, like

17:29.560 --> 17:34.080
availability representative, nobody knows what exactly they mean.

17:34.080 --> 17:41.160
And they used never to make predictions, but always to explain something after the fact.

17:41.160 --> 17:48.680
The same is with system one and system two, which is just a list of dichotomies.

17:48.680 --> 17:58.520
And the dichotomies is where a science should start and go to good models.

17:58.520 --> 18:05.280
And this is a case where we went from models to dichotomies, to backwards.

18:05.280 --> 18:09.560
Trusky had models of heuristics like elimination bias.

18:09.560 --> 18:18.400
But then after Trusky died, Kahneman sided with this dual system theories that has been

18:18.480 --> 18:22.760
long criticized in psychology before.

18:22.760 --> 18:33.960
Alan Newell said, in a famous paper in 1957, you can't ask 20 questions to nature.

18:33.960 --> 18:42.200
And criticizing the tendency to not do models, but to do dichotomies, serial versus parallel,

18:42.200 --> 18:47.640
unconscious versus conscious, fast versus slow, and so on.

18:47.640 --> 18:53.920
And all this is so intuitive that one forgets that one does science.

18:53.920 --> 18:59.120
So the second question is, no, it's a prescriptive question.

18:59.120 --> 19:01.640
When should we use heuristics?

19:01.640 --> 19:02.480
And when not?

19:02.480 --> 19:04.560
And what heuristic?

19:04.560 --> 19:08.040
And then, certainly, it's heuristics all the way down.

19:08.040 --> 19:09.840
There's no option.

19:09.840 --> 19:11.880
The real question is, what is a smart?

19:11.880 --> 19:15.520
Is a smart to imitate?

19:15.520 --> 19:22.400
The peers, when searching for a research topic, or should I think myself?

19:22.400 --> 19:30.400
So here the question is, can we prove mathematical or at least with computer simulations show

19:30.400 --> 19:36.720
under which condition, for instance, one way in works, where does imitation work, and so on?

19:36.720 --> 19:39.600
And the final question is one of application.

19:39.600 --> 19:45.480
It's doing intuitive design using all these insights.

19:45.480 --> 19:48.920
So I'll start with the adaptive toolbox.

19:48.920 --> 19:55.000
And here is a few on part of the toolbox.

19:55.000 --> 19:58.840
There are core capacities that humans have.

19:58.840 --> 20:02.880
On that basis, the heuristics can be simple.

20:02.880 --> 20:12.440
The core capacities are quite complex, meaning it's very hard to build a brain that can't do that.

20:12.440 --> 20:13.840
Let me start.

20:13.840 --> 20:18.800
So there are, I can only give you some examples today.

20:18.800 --> 20:27.560
And on the left side, there are social heuristics like imitation, word of mouse, which are highly powerful.

20:27.560 --> 20:30.080
There would be no culture without imitation.

20:30.080 --> 20:39.000
Mike Tomasella has shown in his beautiful research that children, human children, imitate much more precise

20:39.000 --> 20:43.040
and much more general than primate infants.

20:43.040 --> 20:50.680
And it's one of our big advantages that builds culture that one doesn't have to learn from scratch all the time.

20:50.680 --> 20:55.320
It's also one of our greatest dangers.

20:55.320 --> 20:59.760
So let me start with recognition.

20:59.760 --> 21:04.160
Recognition is a fundamental ability of the human mind.

21:04.160 --> 21:06.240
We can recognize faces.

21:06.240 --> 21:09.200
We can recognize names.

21:09.200 --> 21:13.080
And recognition is different from recall.

21:13.080 --> 21:18.200
If you cannot recall the name of a good friend, that happens.

21:18.200 --> 21:24.280
Particularly when you get older or when you're a jet lag like me at this moment.

21:24.920 --> 21:33.080
But if you do not recognize the name of a good friend, then you're a clinical case.

21:33.080 --> 21:46.480
So the recognition heuristic exploits this capability to make inferences in situations where you don't know much.

21:46.480 --> 21:49.120
Let me do a demonstration with you.

21:49.120 --> 21:51.120
Are you ready?

21:51.120 --> 22:03.040
Okay, assume you are in a TV show called Who Wants to be a Millionaire?

22:03.040 --> 22:04.280
And you made it through the end.

22:04.280 --> 22:08.040
And here is the million dollar question.

22:08.040 --> 22:10.840
Which US city has more inhabitants?

22:10.840 --> 22:13.120
Detroit or Milwaukee?

22:13.120 --> 22:14.480
Time is ticking.

22:14.480 --> 22:16.560
Who of you thinks it's Detroit?

22:16.560 --> 22:18.560
Hands up.

22:18.560 --> 22:21.600
Who of you thinks it's Milwaukee?

22:21.600 --> 22:24.400
Hands up.

22:24.400 --> 22:29.440
A rough estimate, 60% of you got it right.

22:29.440 --> 22:33.280
It's Detroit.

22:33.280 --> 22:43.480
When we did experiments with undergraduates at the University of Chicago, who think there is more or less in the country,

22:43.480 --> 22:45.480
we got the same result.

22:45.480 --> 22:47.200
60%.

22:47.200 --> 22:55.760
And then Dane Goldstein and I asked students in Germany, what do you think?

22:55.760 --> 23:02.640
So not just this one question, but all questions about large American cities.

23:02.640 --> 23:04.040
What do you think?

23:04.040 --> 23:08.920
How many Germans got the right?

23:08.920 --> 23:11.760
More than 60.

23:11.760 --> 23:13.040
More than 60.

23:13.040 --> 23:14.680
And why do you think that?

23:14.680 --> 23:16.720
Because otherwise I wouldn't ask you.

23:16.720 --> 23:20.520
They relied on the heuristic of recognition.

23:20.520 --> 23:21.240
Right, yeah.

23:21.240 --> 23:23.400
And trust that a lot too.

23:23.400 --> 23:24.720
You're right.

23:24.720 --> 23:30.320
Or this is when you give a talk with two people who have already your work.

23:30.320 --> 23:31.560
OK.

23:31.560 --> 23:37.040
The point is, many of the Germans have never heard of Milwaukee.

23:37.040 --> 23:42.040
And the recognition heuristic tells you, choose the option you recognize.

23:42.040 --> 23:44.040
And they were right.

23:44.040 --> 23:47.080
Now, let me test you the other way around.

23:47.080 --> 23:48.680
OK, ready?

23:48.680 --> 23:52.360
Which German city has more inhabitants?

23:52.360 --> 23:54.680
Bielefeld or Hannover?

23:54.680 --> 23:57.440
Who is for Bielefeld?

23:57.440 --> 23:58.480
You.

23:58.480 --> 24:00.400
Who is for Hannover?

24:00.400 --> 24:02.360
You got it right.

24:02.360 --> 24:06.760
I think the two suspected that I want to fool them.

24:06.760 --> 24:10.840
You shouldn't think too much.

24:10.840 --> 24:16.000
If I ask the same question to Germans, many of them get it wrong.

24:16.000 --> 24:17.440
Because they had heard of both.

24:17.440 --> 24:21.600
And they need to retrieve knowledge about that.

24:21.600 --> 24:22.920
See the point?

24:22.920 --> 24:25.960
And we had just a less-is-more effect here.

24:25.960 --> 24:33.840
You got more questions right about the German cities than about America.

24:33.840 --> 24:37.040
Although you know more about the Americans than the Germans.

24:37.040 --> 24:38.880
So how does this work?

24:38.880 --> 24:41.800
So here's the study of ecological rationality.

24:41.800 --> 24:45.240
So think you have a test of a hundred of these questions.

24:45.240 --> 24:48.160
Maybe the hundred largest cities in the US.

24:48.160 --> 24:52.280
And consider the lowest coefficient.

24:52.280 --> 24:59.520
If you haven't heard of any of the cities, your performance is 50%.

24:59.520 --> 25:01.440
Yeah, you just guess.

25:01.440 --> 25:06.320
If you've heard of all of them on the other side, it's also 50%.

25:06.320 --> 25:09.160
Because you cannot use the recognition tool.

25:09.160 --> 25:11.880
You're seeing both cases that have no knowledge.

25:11.880 --> 25:15.320
Now assume the person has knowledge.

25:15.320 --> 25:17.960
And the knowledge is defined here.

25:17.960 --> 25:24.840
It's defined as the number of answers you get right in cases where you've heard of both.

25:24.840 --> 25:26.840
Like before, it was 60%.

25:26.840 --> 25:32.120
And actually, the curve here, the second curve, is about knowledge relative of 60%.

25:32.120 --> 25:40.280
Person B recognizes all American cities like you and gets 60% right.

25:40.280 --> 25:42.520
This is knowledge.

25:42.520 --> 25:46.800
Person A recognizes only half of them.

25:46.800 --> 25:52.080
Can apply the recognition here and gets more right.

25:52.080 --> 25:58.520
The curves are drawn for recognition validity of 0.8.

25:58.520 --> 26:04.760
Meaning that the number of correct inferences in all cases,

26:04.760 --> 26:09.240
where you have recognized one and not the other.

26:09.240 --> 26:11.120
So you can measure all these things.

26:11.120 --> 26:14.680
There's no parameter fitting game here.

26:14.680 --> 26:25.280
And if the knowledge validity is at least as big as the recognition validity,

26:25.280 --> 26:28.600
there is no lesses more effect in it.

26:28.600 --> 26:33.240
So the lesses more effect you find on this three curves below,

26:33.240 --> 26:38.960
it means that on the right side of the curve, performance goes down,

26:38.960 --> 26:42.560
despite you either no more or recognize more.

26:42.560 --> 26:50.960
So this is an illustration how we can model and can actually make quantitative predictions.

26:50.960 --> 26:58.560
And we have used this, for instance, in predicting maybe more interesting things in cities,

26:58.560 --> 27:01.880
unless you are on a million dollar question.

27:01.880 --> 27:13.000
Like you can how to predict the outcomes of all Wimbledon gentlemen's single games.

27:13.000 --> 27:19.160
There are 128 contestants, there are 127 games.

27:19.200 --> 27:21.000
And you make a prediction for everyone.

27:21.000 --> 27:28.240
And you can do this with the ADP rankings or with the Wimbledon seedings or with ignorant people.

27:28.240 --> 27:33.880
Exactly partially ignorant, because fully ignorant cannot use the heuristics.

27:33.880 --> 27:39.040
And again, from here you can see, you need to find people that have heard about half of them.

27:39.040 --> 27:40.480
And not if there was one.

27:40.480 --> 27:47.040
And these were German amateur players and they made in two studies

27:47.040 --> 27:54.160
more correct predictions than the ADP rankings or the Wimbledon experts.

27:54.160 --> 28:03.480
And if you change that to those who recognize less, then they will not make more predictions.

28:03.480 --> 28:09.560
So this is just an example how one can analyze the ecological rationality of a heuristic.

28:09.560 --> 28:13.320
You can easily tell when the heuristic is worthless,

28:13.320 --> 28:19.760
when the recognition validity is a change level, that is noting.

28:19.760 --> 28:29.000
And we also have a way to make earlier concepts like availability into something precise.

28:29.000 --> 28:36.560
That is useful and also illustrates some of the amazing capabilities of the human mind

28:36.560 --> 28:41.440
to exploit one's own ignorance.

28:41.440 --> 28:44.840
Second example, fluency.

28:44.840 --> 28:49.320
Fluency presumes recognition.

28:49.320 --> 28:57.520
And it can be measured by the time an option comes to your mind.

28:57.520 --> 29:11.400
And the point I'm making with the next example is that a human intuition using fluency,

29:12.360 --> 29:16.720
creates no speed accuracy trade-off.

29:16.720 --> 29:18.920
What is the speed accuracy trade-off?

29:18.920 --> 29:25.640
That's usually something that if you make judgments fast, then you lose on accuracy.

29:25.640 --> 29:31.560
That's the story about fast versus slow thinking fast versus slow and so on.

29:31.560 --> 29:39.560
A speed accuracy trade-off, you get them if you test undergraduates with problems I've never heard.

29:39.560 --> 29:47.360
But if you test experts, it's the opposite.

29:47.360 --> 29:50.480
So here's the example.

29:50.480 --> 30:04.600
The example is there are experienced handball players and some of my postdocs put them in

30:04.600 --> 30:08.760
the uniform with a ball in the hand in front of a video.

30:08.760 --> 30:14.800
And the instruction was there will be a top handball game, running for 10 seconds, then

30:14.800 --> 30:21.960
it will be frozen, and tell me immediately what the player with the ball should do.

30:21.960 --> 30:27.880
So they looked at that, it was frozen, and they would say I'll pass to the left here

30:27.880 --> 30:31.560
or shoot at the goal or something else.

30:31.560 --> 30:42.160
Then they had 45 seconds more time to study the still frozen image carefully.

30:42.160 --> 30:48.760
And they were instructed, if other options come to your mind, please tell us.

30:48.760 --> 30:53.080
And then one of them said, oh, I didn't see the guy on the right side, passed, that would

30:53.080 --> 30:54.760
be a good one.

30:54.760 --> 31:04.360
And after all of this was done, the options were plotted against the quality, as measured

31:04.360 --> 31:08.440
by the best coaches we have in the country.

31:08.440 --> 31:10.640
And what you see is very interesting.

31:10.640 --> 31:14.560
On average, the first option that comes to your mind is the best one, the second is the

31:14.560 --> 31:17.960
second best, the third is the, and so on.

31:17.960 --> 31:26.560
That means if an experienced player goes with the first option, he or she is most likely

31:26.560 --> 31:28.760
right.

31:28.760 --> 31:34.320
And we also found that when we asked them at the end of the 45 seconds they had studied,

31:34.320 --> 31:41.080
what would you do now, about 40% change their mind, and got into a worse option.

31:41.080 --> 31:46.600
So here we have a case where we have no speed, accuracy, trade-off.

31:46.600 --> 31:54.040
Country, it's better, on average, to act on your first impulse.

31:54.040 --> 31:56.960
In a game you have to do this.

31:56.960 --> 32:03.240
But in the experiment, you have to change the thing longer, but it doesn't help.

32:03.240 --> 32:05.280
It regularly hurts.

32:05.280 --> 32:08.560
Remember, this only works for experts.

32:08.560 --> 32:14.960
If you have amateur players, the curves, they're not going down, they're just stator.

32:14.960 --> 32:19.600
So this has an important insight.

32:19.600 --> 32:29.120
So intuition, as I define it, is based on years of experience, what you want to do comes

32:29.120 --> 32:32.760
quickly to your mind, and you don't know why.

32:32.760 --> 32:38.480
And the fluency heuristic is an explanation how this works.

32:39.400 --> 32:45.720
The reason why the research has come to the conclusion that what the general speed, accuracy,

32:45.720 --> 32:53.200
trade-off is mainly because one studies undergraduates with problems they've never seen before.

32:53.200 --> 33:01.120
Gary Klein, who studies fire furthers, finds the same results.

33:01.120 --> 33:07.520
So the third and last category, I want to give you examples, is an interesting one,

33:07.520 --> 33:19.840
where it is about the human capacity to order things in importance, and these are heuristics

33:19.840 --> 33:26.960
that are based, that base the decision on a single cue.

33:26.960 --> 33:32.120
And let's give you an example about hiring.

33:32.120 --> 33:41.160
When Elon Musk was still young, and Tesla was young, Elon Musk did the hiring himself,

33:41.160 --> 33:44.280
and he reported how he did it.

33:44.280 --> 33:47.920
He did not get an assessment center.

33:47.920 --> 33:55.720
He did not study CVs, but he, according to himself, he relied only on a single cue.

33:55.720 --> 34:05.520
Just a person possesses some exceptional ability, like a banjo player.

34:05.520 --> 34:11.400
And if not, no hire, if yes, hire.

34:11.400 --> 34:15.920
Now you might wonder why not more cues.

34:15.920 --> 34:25.360
But look, exceptional ability is something that has some kind of redundancy with other

34:25.360 --> 34:26.640
things.

34:26.640 --> 34:34.720
So a person who has this to get there needs to know how to persevere, how to concentrate,

34:34.720 --> 34:39.320
how to sweat, and not give up.

34:39.320 --> 34:46.000
And so that's the spirit behind that.

34:46.000 --> 34:54.480
It also illustrates that these heuristics are adapted to certain problems that will not

34:54.480 --> 34:59.160
work well when the company, like Tesla, had grown.

34:59.160 --> 35:03.360
Because then you need people who do routine work.

35:03.360 --> 35:08.640
One needs to adapt that, and that's the idea in the adaptive toolbox.

35:08.640 --> 35:20.760
Now here is another hiring heuristic, and that's what we call a fast and frugal tree.

35:20.760 --> 35:28.240
A fast and frugal tree is an incomplete tree that has an exit on each of the questions

35:28.240 --> 35:30.400
or cues.

35:30.400 --> 35:37.000
So if the number of cues or questions are n, it has n plus 1 exits, while a complete

35:37.000 --> 35:42.760
tree is in the case of dichotomous 2 to the n.

35:42.760 --> 35:49.280
So when Abbason was young, a Jeff Bezos also reported about how he did make the description,

35:49.280 --> 35:53.920
and we reconstructed that in the form of a fast and frugal tree.

35:53.920 --> 35:56.040
So note it's sequential.

35:56.040 --> 36:01.600
It doesn't integrate as rational theories usually assume.

36:01.600 --> 36:05.720
Interestingly, his first question was the same.

36:05.720 --> 36:08.440
So does the person have an exceptional ability?

36:08.440 --> 36:11.040
But if yes, it wasn't enough.

36:11.040 --> 36:17.080
Second question, will you admire this person, which is a quite unusual question?

36:17.800 --> 36:25.440
We explained that if he, Bezos, admires a person, he will learn from that person.

36:25.440 --> 36:33.840
But that was also not enough, and only if that person raises the average level in the

36:33.840 --> 36:38.440
unit where he or she will be, that's being hired.

36:38.440 --> 36:46.480
So the properties of these trees are of a lexicographic nature.

36:46.480 --> 36:55.000
A lexicographic strategist cannot be mapped into a utility, into a smooth utility function.

36:55.000 --> 37:02.560
And that's why classical books like Luce and Wreifer have always ignored them and looked

37:02.560 --> 37:04.600
down at them.

37:04.600 --> 37:11.280
But people use these strategies, and we have shown that there are certain situations where

37:11.280 --> 37:15.840
a fast and frugal tree can outperform random forests.

37:15.840 --> 37:20.720
And the interesting question is, again, can we prove this situation?

37:20.720 --> 37:25.000
Can we identify them where that works?

37:25.000 --> 37:31.680
So I'll show you one way to, about how to model such a tree.

37:31.680 --> 37:42.080
If you use these three questions in the order, how many trees are possible by changing the

37:42.080 --> 37:43.080
exits?

37:43.080 --> 37:44.080
Sorry.

37:44.080 --> 37:45.080
Four.

37:45.080 --> 37:46.080
Oh.

37:46.080 --> 37:47.080
It's four.

37:47.080 --> 37:49.600
You tell me later, the other two.

37:49.600 --> 37:50.600
The same and changing.

37:50.600 --> 37:51.600
Yeah, the same, yeah?

37:51.600 --> 37:52.600
The same cues in the same order, yeah?

37:52.600 --> 37:53.600
Otherwise, it would be more.

37:53.600 --> 37:54.600
Correct.

37:54.600 --> 38:00.720
Bezos tree is on the left side.

38:00.720 --> 38:07.520
So any classification can make one of two errors, like making an offer to a person whom

38:07.520 --> 38:13.840
you should not have made an offer that's a false positive, or not making an offer to

38:13.840 --> 38:17.440
a person whom you should, that's a miss.

38:17.440 --> 38:23.360
The Bezos tree minimizes false positives, yeah?

38:23.360 --> 38:27.840
He only makes an offer after long, yeah?

38:27.840 --> 38:34.520
These four trees do a balance between false positives and misses.

38:34.520 --> 38:42.520
On the right side, you have a tree that maximizes false positives relative to miss.

38:43.520 --> 38:51.960
And in the middle, you have very interesting trees that have a zigzag structure.

38:51.960 --> 38:58.640
An example for a tree on the right side, I'll show you from our work with the Bank

38:58.640 --> 38:59.640
of England.

38:59.640 --> 39:06.640
So the Bank of England has a problem how to identify vulnerable banks.

39:06.640 --> 39:08.920
How do you do that?

39:08.920 --> 39:19.680
Now the classical method is in the so-called Basel or Basel two and three programs, and

39:19.680 --> 39:25.280
where the banks calculate a value at risk.

39:25.280 --> 39:32.680
And if you've ever done that, so if you are the CEO of a large bank, you have to estimate

39:32.680 --> 39:34.760
thousands of risk factors.

39:34.760 --> 39:38.120
That means a covariance matrix in the order of millions.

39:38.120 --> 39:44.520
And you can't use more than five, ten years of data because before something strange

39:44.520 --> 39:49.160
would happen.

39:49.160 --> 39:52.200
These calculations are impressive.

39:52.200 --> 39:58.920
They have prevented, they've never prevented any crisis and probably will never prevent

39:58.920 --> 40:00.560
any crisis.

40:00.560 --> 40:03.520
It's another Turkey illusion.

40:03.520 --> 40:09.920
This standard probability theory applied to the situation of uncertainty.

40:09.920 --> 40:20.520
I once gave a talk to the European Central Bank and said, the calculation that you do

40:20.520 --> 40:24.800
border on astrology.

40:24.800 --> 40:32.440
And I was waiting for someone to say, no, the answer was, yes, but what else should

40:32.440 --> 40:34.680
we do?

40:34.680 --> 40:39.400
Here is what else could be done and systemically be studied.

40:39.400 --> 40:41.280
So this is now a fast and frugal tree.

40:41.280 --> 40:44.520
That's the same structure as the one that's seen before.

40:44.520 --> 40:52.800
And it asks just three questions, for instance, leverage ratio below a certain percentage

40:52.800 --> 40:57.040
and if it's lower, red flag.

40:57.040 --> 41:01.000
If not, it goes on and asks two more questions.

41:01.000 --> 41:05.480
I'm not going into the details here, but you see the structure.

41:05.480 --> 41:11.200
It's important that it works very different from standard rational models.

41:11.200 --> 41:20.520
So for instance, the Swiss bank, UBS, had failed before the crisis on the first item.

41:20.520 --> 41:26.360
So the leverage ratio was much less than half of that.

41:26.360 --> 41:29.880
That would have identified it immediately.

41:29.880 --> 41:40.680
UBS did marvelous on the other criteria, but the trees are non-compensatory.

41:40.680 --> 41:43.520
It's like the human body.

41:43.520 --> 41:46.920
If your heart fails, a good lung doesn't help you.

41:46.920 --> 41:48.920
You can't compensate that.

41:48.920 --> 41:51.560
So it's a very different model.

41:51.560 --> 41:56.920
And the reason why this type of strategy cannot be mapped into a smooth utility function,

41:56.920 --> 41:59.840
who cares about that?

41:59.840 --> 42:08.840
It has to work and not to follow some mathematical doctrine.

42:08.840 --> 42:16.880
Again importantly, it reflects sequential thinking and has a few other advantages.

42:16.880 --> 42:19.360
Bankers can understand that.

42:19.360 --> 42:28.560
And also, the banks, the regulators are more safe that the banks cannot game the system.

42:28.560 --> 42:34.440
If you have to estimate millions of co-variants, you have plenty of room to game.

42:34.440 --> 42:37.320
Here, it's much easier.

42:37.320 --> 42:42.280
It's much more difficult to game the system.

42:42.280 --> 42:46.160
So can I just ask you something unconfused?

42:46.160 --> 42:52.720
So this is a risk-minimizing structure that can be calculated based on known states.

42:52.720 --> 42:59.400
Earlier, you started by talking about heuristics as being solutions to the uncertainty problem.

42:59.400 --> 43:02.960
This looks like a risk problem.

43:02.960 --> 43:08.080
The uncertainty is in what banks are doing.

43:08.080 --> 43:13.280
So the model is a heuristic.

43:13.280 --> 43:17.360
It's not an optimizing model.

43:17.360 --> 43:22.440
And your right, it goes, it's the rightmost tree.

43:22.440 --> 43:26.280
It tries to minimize the misses.

43:26.280 --> 43:28.400
You want not to overseas.

43:28.400 --> 43:31.240
And the costs are false alarms.

43:31.240 --> 43:34.040
That's what the decision is.

43:34.040 --> 43:41.760
How we build the trees is a mixture between, so the economists at the Bank of England,

43:41.760 --> 43:48.680
they identify the variables on base and we check them down on empirical data.

43:48.680 --> 43:53.560
So for instance, the cutoff points, it's all empirical data.

43:53.560 --> 43:55.520
We deliver the structure.

43:55.520 --> 43:58.640
Now, don't put this in a regression.

43:58.640 --> 44:00.120
Think about this structure.

44:00.120 --> 44:04.240
That's the way this is being done.

44:04.240 --> 44:07.760
And then it needs to be tested how well it works.

44:07.760 --> 44:11.440
The advantage is transparency.

44:11.440 --> 44:19.800
That it can easily be understood and it can be changed in cases there is a problem.

44:19.800 --> 44:24.320
And it's an alternative that can be systematically studied.

44:24.320 --> 44:25.320
Yeah.

44:25.320 --> 44:33.800
Well, I'm not going to, the states are still all nine points, but the state outside of the

44:33.800 --> 44:35.880
system or the framework.

44:35.880 --> 44:41.400
Whereas when you had the opening definition of risk and uncertainty, there's something

44:41.400 --> 44:45.560
about the small world about these states.

44:45.560 --> 44:47.560
It's a good question.

44:47.560 --> 44:58.480
So the world of finance here has incredibly and also unknown uncertainties.

44:58.480 --> 45:04.520
So the numbers that the Bank of England gets from the banks, you can count that they're

45:04.520 --> 45:08.720
not the real numbers, they're polished.

45:09.280 --> 45:15.840
The other source of uncertainty is in what's actually happening in the financial systems.

45:15.840 --> 45:19.440
And there could be another Russian default or something.

45:19.440 --> 45:21.680
This is the uncertainty I mean.

45:21.680 --> 45:28.200
And this is a model how to deal with that uncertainty.

45:28.200 --> 45:33.320
And there's no claim about optimization or maximization for anything.

45:33.320 --> 45:37.520
It's just an attempt to do better than what one has.

45:37.880 --> 45:49.720
Here is a book we've done, which is how to build these trees from empirical data.

45:49.720 --> 45:53.440
And what's the alternative in case that's of interest.

45:53.440 --> 45:59.000
But the uncertainty in the financial system is enormous.

45:59.000 --> 46:02.640
Is that, sorry, let's take it a little bit.

46:02.760 --> 46:09.920
So I was sympathetic to your critique of the dichotomizing approach that you've adopted

46:09.920 --> 46:15.200
it yourself in the heuristics versus the optimizations.

46:15.200 --> 46:21.600
And is it possible actually, given the observation that we just made, in fact, what you're calling

46:21.600 --> 46:26.240
a heuristic is just on the spectrum towards full optimization.

46:26.240 --> 46:27.240
Yeah.

46:27.240 --> 46:28.240
Yes.

46:28.240 --> 46:29.240
Okay.

46:29.240 --> 46:30.240
Yeah.

46:30.240 --> 46:36.480
I mean, if you do Bayesian, they're Bayesian usual optimization model, then you start with

46:36.480 --> 46:37.480
uniform priors.

46:37.480 --> 46:38.480
That's a heuristic.

46:38.480 --> 46:41.440
You're totally right.

46:41.440 --> 46:46.480
It is, I just make it simple.

46:46.480 --> 46:51.320
But it's very clear that one should pay attention, I think, to the heuristics.

46:51.320 --> 46:52.320
Yeah?

46:52.320 --> 46:55.920
There's a bit of a category error here.

46:55.920 --> 46:57.800
You actually mentioned ROC curves.

46:57.800 --> 47:06.000
The whole point of ROC curves is, which are middle of standard machine learning, is that

47:06.000 --> 47:07.000
they are an alternative.

47:07.000 --> 47:08.840
You don't come up with one.

47:08.840 --> 47:12.160
You set your own level on an ROC curve.

47:12.160 --> 47:17.680
And you're simply saying, go on ROC curve in a way that you're less likely to screw

47:17.680 --> 47:18.680
up.

47:18.680 --> 47:19.680
Sure.

47:19.680 --> 47:23.840
That makes perfect sense.

47:23.840 --> 47:29.160
But I don't think it's really got to do with optimization versus heuristics, just using

47:29.160 --> 47:34.360
an ROC curve over, and the points that David is making in the fellow over there, is it's

47:34.360 --> 47:41.000
a very high level coarse grain space as opposed to these banks are being stupid and are using

47:41.000 --> 47:43.960
a fine grain space where you can't measure the variables.

47:43.960 --> 47:49.160
But the point is, is an ROC curve doesn't give you a single answer.

47:49.160 --> 47:53.360
You're putting in that I don't like the chance of screwing up.

47:53.360 --> 47:59.480
Therefore, I'm going to set my threshold in the ROC curve and go over to this side.

47:59.480 --> 48:00.880
Okay.

48:00.880 --> 48:06.880
What you picked this point here, the one-game map, the fast and frugal degrees onto an ROC

48:06.880 --> 48:08.400
curve, onto four points.

48:08.400 --> 48:13.560
And that's a direct connection between an optimization model.

48:13.560 --> 48:19.840
Then the question is, whether the optimization model helps you for a certain problem.

48:19.840 --> 48:28.960
The name and Pearson theory, which is below the foundation of the ROC curves, makes a

48:28.960 --> 48:35.480
number of assumptions about the distributions which are not necessary here.

48:35.480 --> 48:47.440
I see a point here, and also it's difficult for the ROC curves to deal with more than

48:47.440 --> 48:48.440
one variable.

48:48.440 --> 48:52.200
Can be done, gets very complex.

48:52.200 --> 48:57.080
Here it's relatively easy to deal with them.

48:57.080 --> 49:02.080
So there are differences between these two curves, but you also write and we showed that

49:02.080 --> 49:13.320
in this paper here, that the heuristic can be mapped into an optimization model, right?

49:13.320 --> 49:18.600
So I'll end with a few remarks about ecological rationality.

49:18.600 --> 49:27.360
So the idea of ecological rationality is from Herbert Simon, and that's a famous quote from

49:27.360 --> 49:31.160
him where he says, this is a analogy.

49:31.160 --> 49:36.640
It basically says, in order to evaluate the rationality of behavior, you need to look

49:36.640 --> 49:43.080
both at cognition, the strategies, the heuristics, anything else, and the environment, and how

49:43.080 --> 49:45.520
they match.

49:45.520 --> 49:58.680
It also means the standard definition of rationality is following the laws, say, axioms of consistency

49:58.680 --> 50:06.760
or maximization of expected utility is an internal definition.

50:06.760 --> 50:12.560
It doesn't take account how these things work in the world outside.

50:12.560 --> 50:20.800
And for instance, that part of behavioral economics, who has adopted the heuristics

50:20.800 --> 50:25.360
and biases program, uses a single internal definition.

50:25.360 --> 50:31.400
There is to be a law of logic or probability, and people are measured against that.

50:31.400 --> 50:38.520
And what I'm arguing, we need to measure behavior against the real world, against the

50:38.520 --> 50:43.360
measure heuristic against the real world or the success.

50:43.360 --> 50:52.680
What I'll show you now at the end is just a simple and intuitive answer to the question,

50:52.680 --> 50:57.400
can we, I've shown you a few heuristics that just rely on one reason.

50:57.400 --> 51:07.200
Can we identify the conditions under which relying on one reason cannot be beaten by

51:07.200 --> 51:13.560
a linear equation that has more reasons, including the one reason?

51:13.560 --> 51:16.920
Clear?

51:16.920 --> 51:24.800
So do you have an idea?

51:24.800 --> 51:29.920
I'm just asking you before, because afterwards it's so evident that we knew it all along.

51:29.920 --> 51:33.680
To be clear, it's a subset of that larger space, right?

51:33.680 --> 51:36.520
Yeah, it's a subset.

51:36.520 --> 51:38.440
So think about it.

51:38.440 --> 51:46.320
A binary decision, higher or not higher, there is Elon Musk has one reason.

51:46.320 --> 51:55.560
The question is, under what conditions will relying on this one reason always lead to

51:55.560 --> 52:06.640
the same results that a linear model that includes many valid reasons, they're all valid,

52:06.640 --> 52:07.640
including the one.

52:07.640 --> 52:08.640
Clear?

52:08.640 --> 52:09.640
It's a subset.

52:09.640 --> 52:10.640
Yeah?

52:11.640 --> 52:17.600
Here's a simple condition, and it's if there is a dominant queue.

52:17.600 --> 52:25.080
But it means that the weights of binary queues form a dominant queue structure.

52:25.080 --> 52:30.440
And so the weights are like regression weights, it's the additional contribution to the first

52:30.440 --> 52:31.440
one.

52:31.440 --> 52:37.280
So if the weight of the first one is larger than the sum of all the others, you can intuitively

52:37.280 --> 52:42.720
see it will not get to any other decision.

52:42.720 --> 52:46.240
And then the next question is, how often does this happen?

52:46.240 --> 52:54.480
We have looked at machine learning data sets, and it happens quite often, astonishingly

52:54.480 --> 52:55.640
often.

52:55.640 --> 53:02.400
So this is not the only condition, it's just a sufficient conditions.

53:02.400 --> 53:10.920
And we've looked at three conditions, and in the median value is that in 90% of all

53:10.920 --> 53:14.720
comparisons in the data set, this condition holds.

53:14.720 --> 53:24.040
So if you take half of the data sets, in 90% or more, it holds, and the other is not.

53:24.040 --> 53:30.640
And then the next question is, what are the other conditions?

53:30.640 --> 53:39.880
And I'm not going into this at this moment, but end with a more general view that's probably

53:39.880 --> 53:41.760
known to many of us.

53:41.760 --> 53:46.200
So this is a standard justification for why people use heuristics.

53:46.200 --> 53:49.040
It's an accuracy effort trade-off.

53:49.040 --> 53:55.640
So people are a little bit lazy, or they don't make the effort, and they pay for that in

53:55.640 --> 53:57.360
accuracy.

53:57.360 --> 54:06.760
That you will find in the Kahneman book, on justifying why biases need to be eliminated,

54:06.760 --> 54:11.600
or in the Kahneman-Siperni and Sunstein book, by noses.

54:11.600 --> 54:21.480
So we have read a very interesting exchange between two David's here, and the Kahneman-Siperni

54:21.480 --> 54:31.560
and Sunstein, where David Wolpert and David Krocker point out, really, that noise has

54:31.560 --> 54:36.920
often a function, and we should not try to eliminate that.

54:36.920 --> 54:45.040
Your examples were mainly from not about the examples that Kahneman has in mind, but they

54:45.040 --> 54:58.760
apply equally to the key topics of the noise book, which is sentencing, so how much, what's

54:58.760 --> 55:07.880
the punishment for a certain crime, and also pricing annuities.

55:07.880 --> 55:20.360
And it is not that sentencing is like Bull's Eye, where there is a right answer, and different

55:20.360 --> 55:25.120
experts with different opinions should converge on one.

55:25.120 --> 55:26.520
You can have many things.

55:26.520 --> 55:28.280
You want to punish the person.

55:28.280 --> 55:32.400
You want to be low on things and give the person a chance.

55:32.400 --> 55:40.040
You can have many reasons, and it's also an issue where variability is a motor about

55:40.040 --> 55:43.160
change in societies.

55:43.160 --> 55:54.720
And so, again, both of these views assume that there is a truce that is singular, that

55:54.720 --> 56:02.280
is known, and the biases, the difference to the truce, and also the variability on the

56:02.280 --> 56:09.800
truce should all be canceled, and that's the key idea behind that.

56:09.800 --> 56:13.480
And the answer, this is not the case.

56:13.480 --> 56:18.960
We have a bias variance trade-off, and where we have not only a bias, but also variance

56:18.960 --> 56:22.440
plus noise, it's illustrated here.

56:22.440 --> 56:29.200
You have two darts, and on the left side, the player throws systematically too far to

56:29.200 --> 56:35.200
the right and low, but the variance is, the variability is small.

56:35.200 --> 56:42.240
On the right side, there is a player who has no bias.

56:42.240 --> 56:48.000
Bias is zero, meaning that on average, the darts are in the middle, but only on average.

56:48.000 --> 56:58.960
Variability is high, and we can see that the real idea is not to reduce bias to zero,

56:59.720 --> 57:08.280
as the typical messages, but to find some reasonable compromise between the trembling

57:08.280 --> 57:10.520
hand and the bias.

57:10.520 --> 57:19.800
Juristics, if they have zero free parameter, like one over n, have a bias, like on the

57:19.800 --> 57:28.120
left side, but no error due to variance, they always hit the same point in this image.

57:28.120 --> 57:32.720
And that's a way to understand why simple can help.

57:32.720 --> 57:36.920
And it's also a way to understand when simple does not help.

57:36.920 --> 57:43.480
For instance, in that thing, if you have really large amounts of data and the world is stable,

57:43.480 --> 57:51.200
then the simple solution will have too much bias, because the amount of data will reduce

57:51.200 --> 57:52.200
variance.

57:52.280 --> 57:59.680
Can you say a little bit about, in bias variance, if I just go from bias and variance or expectation

57:59.680 --> 58:08.320
values over both your decision algorithm and also the real world, if I go one over n and

58:08.320 --> 58:14.680
the real world is varying, I can have a very high variance, I'm not sure in what sense.

58:14.680 --> 58:21.680
One over n gives you, it doesn't do estimations, so if your algorithm doesn't estimate anything,

58:21.680 --> 58:23.960
you cannot make estimation errors.

58:23.960 --> 58:27.480
But we can talk afterwards.

58:27.480 --> 58:28.960
Okay, let's do this.

58:28.960 --> 58:30.960
From how it's, yeah.

58:30.960 --> 58:31.960
It's districts.

58:31.960 --> 58:42.640
Though I'm happy to talk about that, but in this image, the variability is due to different

58:42.640 --> 58:47.880
samples on which the same algorithm estimates its parameters.

58:47.880 --> 58:53.080
And if you change the samples, you get different ones, in particular if the samples are small,

58:53.080 --> 58:55.160
the variability is higher.

58:55.160 --> 58:58.200
And that's the idea behind it.

58:58.200 --> 59:04.760
The bias variance dilemma, and that may also behind your question, applies to situations

59:04.760 --> 59:10.920
that are situations of risk, where we only have to estimate the probabilities.

59:10.920 --> 59:12.400
It's assumed as stable.

59:12.400 --> 59:24.120
In this sense, it's just an analogy to understand what's, why the heuristics, when they fail

59:24.120 --> 59:25.880
and when they work.

59:25.880 --> 59:32.360
Let me finish with three methodology principles.

59:32.360 --> 59:38.440
It is very important to have algorithmic model of heuristics, not labels, not system one

59:38.440 --> 59:43.160
and other things, that we do not know what they predict.

59:43.160 --> 59:48.000
Second, it is very important to do competitive testing.

59:48.000 --> 59:55.320
Like to test a heuristic model against maybe some machine learning model.

59:55.320 --> 01:00:02.320
We have still parts in, for instance, economics, but it's very little competitive testing.

01:00:03.120 --> 01:00:08.920
At best, you eliminate a factor from your regression.

01:00:08.920 --> 01:00:14.520
But really, to test against a different class of models, really be done.

01:00:14.520 --> 01:00:23.240
And particularly, it's important to learn whether a very highly complicated algorithm really pays.

01:00:23.240 --> 01:00:26.440
And we often have a complexity illusion.

01:00:26.440 --> 01:00:30.760
And think that something that is simple is nothing worse.

01:00:30.760 --> 01:00:39.440
For instance, Harry Markowitz would have not gotten an economics Nobel Prize for one over N.

01:00:43.520 --> 01:00:53.320
And finally, it's also important to do not only out of sample prediction as it is regularly

01:00:53.320 --> 01:00:59.360
done in machine learning, but also out of population prediction.

01:00:59.360 --> 01:01:06.440
So the studies that I reported that being, you know, of investment where one over N is very simple.

01:01:06.440 --> 01:01:14.880
They are not out of sample, but they are real forecasting in the future.

01:01:14.880 --> 01:01:21.840
And the mean variance model or the Bayesian models in, they are not,

01:01:21.840 --> 01:01:26.720
their parameters are not done by out of sample.

01:01:26.720 --> 01:01:30.520
Because out of sample creates a stable world, a fairly stable world.

01:01:30.520 --> 01:01:39.040
But they are estimated in the first time interval, and then they're predicted in the next one.

01:01:39.040 --> 01:01:46.800
And as we know from certain machine learning, techniques that prove very successful in diagnosing

01:01:46.800 --> 01:01:50.320
may lung diseases.

01:01:50.320 --> 01:01:54.960
When they were used in a different hospital, they failed.

01:01:54.960 --> 01:01:59.120
And that's a problem with out of sample prediction.

01:01:59.120 --> 01:02:08.400
And predicting, doing the prediction in a different hospital, then is out of population.

01:02:08.400 --> 01:02:12.480
So I was talking today about three misconceptions.

01:02:12.480 --> 01:02:15.400
Complex models are always better than simple heuristics.

01:02:15.400 --> 01:02:19.320
They are sometimes better, but not always.

01:02:19.320 --> 01:02:24.280
Slow thinking is always more accurate than fast, intuitive decision making.

01:02:24.280 --> 01:02:27.520
No, particularly not by experts.

01:02:27.520 --> 01:02:37.120
Even chess masters, when the best, so the international masters have been studied,

01:02:37.120 --> 01:02:45.240
for instance, when they have time constraints, they do about as well as without time constraints.

01:02:45.280 --> 01:02:52.600
If you have excellent chess players, but not at this level, then the difference gets bigger.

01:02:52.600 --> 01:02:58.400
And finally, it is not correct that people rely on heuristics because they're biased,

01:02:58.400 --> 01:03:04.880
lazy, or irrational, as the typical message still is.

01:03:04.880 --> 01:03:13.960
So I invited you today to a voyage into our studies on Homo heuristicius.

01:03:13.960 --> 01:03:16.840
And I've made three points.

01:03:16.840 --> 01:03:23.320
Risk is not the same as uncertainty, although often the term uncertainty is used for risk.

01:03:23.320 --> 01:03:28.600
For instance, most of behavioral economics talks about uncertainty, but it's all problems of risk.

01:03:28.600 --> 01:03:35.440
And you detect the problem of risk immediately because the reason to know what's right in search.

01:03:35.440 --> 01:03:37.920
And the uncertainty, you don't know that.

01:03:37.920 --> 01:03:46.160
Second, logically rationality, the consistency axioms, the expected utility maximization

01:03:46.160 --> 01:03:49.360
is not the same as the ecological rationality.

01:03:49.360 --> 01:04:00.160
The ecological rationality is much harder to model, to analyze, and it's about the adaptation to the real world.

01:04:00.160 --> 01:04:03.760
And in, under uncertainty, less can be more.

01:04:03.760 --> 01:04:11.320
And one way to understand that is it's not an accuracy effort trade-off, but a bias variance trade-off.

01:04:11.320 --> 01:04:18.240
And finally, human intelligence evolve to deal with situations of uncertainty.

01:04:18.240 --> 01:04:21.080
And so did animal.

01:04:21.080 --> 01:04:23.240
Not with risk.

01:04:23.240 --> 01:04:29.360
And one of the reason is that we are not very good at calculating.

01:04:29.360 --> 01:04:31.520
Let's just show a start here.

01:04:31.600 --> 01:04:38.880
And also, some people think we need to get rid of uncertainty.

01:04:38.880 --> 01:04:47.560
I've often heard from well-known economists, so when Reinhard Zeldin had his 80s birthday,

01:04:47.560 --> 01:04:53.760
I gave one of the four keynotes, and another economist who, the Nobel laureate,

01:04:53.760 --> 01:05:02.800
afterwards came to me and said, interesting talk, but I don't like uncertainty.

01:05:02.800 --> 01:05:11.040
That's a matter of taste, but just imagine if we would know everything, if we would live in a world

01:05:11.040 --> 01:05:18.640
where every possible situation that can occur, the consequence that everything is known,

01:05:18.640 --> 01:05:24.000
and all is just about the probabilities, which were all the uncertainty strain.

01:05:24.000 --> 01:05:28.440
Would that be an interesting world to live?

01:05:28.440 --> 01:05:34.440
It would be, as Frank Knight has pointed out, would be no innovation, no profit.

01:05:34.440 --> 01:05:36.760
And there would be no fun.

01:05:36.760 --> 01:05:46.440
And life would be like in the movie where you wake up in the morning and everything goes back.

01:05:46.440 --> 01:05:53.680
Or as Dostoevsky said, if everything would be rational, nothing would happen.

01:05:53.680 --> 01:05:55.440
Thank you for your attention.

01:06:01.880 --> 01:06:02.680
Thank you, Gerd.

01:06:02.680 --> 01:06:05.680
We are actually at time, so who needs to go, let's go.

01:06:05.680 --> 01:06:09.520
And otherwise, if you have any questions, maybe you can take them.

01:06:09.520 --> 01:06:10.400
What should I do?

01:06:10.400 --> 01:06:12.760
Take your own questions, if you like.

01:06:12.760 --> 01:06:14.680
No, no, you take them.

01:06:14.680 --> 01:06:18.600
OK, all right.

01:06:18.600 --> 01:06:20.160
Good stuff.

01:06:20.160 --> 01:06:25.000
I think there's really supposed to be a lot of that, really.

01:06:25.000 --> 01:06:32.400
So I'm wondering, actually, so you've got your statistics and you showed, like, that you might have different ones.

01:06:32.400 --> 01:06:40.280
And I'm thinking, in any one situation, you can apply a multitude of your statistics.

01:06:40.280 --> 01:06:50.520
So my question is, is there any computational principle by which you decide which heuristic is allocated control of behavior?

01:06:50.520 --> 01:06:57.040
Because they are operating in parallel or subsets of them are operating.

01:06:57.040 --> 01:07:01.280
They may conflict in terms of the final behavior or choice that's exerted.

01:07:01.280 --> 01:07:08.200
So is there any framework for deciding the allocation of control of which heuristic gets to control?

01:07:08.240 --> 01:07:10.360
This is a very good question.

01:07:10.360 --> 01:07:14.800
So there are two or three models about how to model that.

01:07:14.800 --> 01:07:24.000
One is reinforcement learning, so that you learn a hierarchy of heuristics, like imitate or do something else.

01:07:24.000 --> 01:07:32.720
And depending on the reinforcement and the ego, and that makes the order.

01:07:32.720 --> 01:07:39.840
That is, there's a paper by Rieskamp and Otto on that showing that an experimental paper.

01:07:39.840 --> 01:07:45.720
The selection problem is not really being solved here.

01:07:45.720 --> 01:07:51.200
And you might think about a heuristic that decides about the application of heuristic.

01:07:51.200 --> 01:07:54.240
We may just do reinforcement learning or something else.

01:07:59.240 --> 01:08:02.200
Unfortunately, I won't be able to meet with you if I had time to meet with you.

01:08:02.200 --> 01:08:03.920
This is what I would ask.

01:08:03.920 --> 01:08:12.080
So in the mathematics and computer science of risk assessment in criminal justice, for instance,

01:08:12.080 --> 01:08:13.560
there are very similar debates.

01:08:13.560 --> 01:08:26.400
And you may know of a computer scientist named Cynthia Rudin argues that we should use simple decision trends rather than more sophisticated algorithms.

01:08:26.400 --> 01:08:41.320
And her argument is partly because partly that more sophisticated algorithms in that domain are typically only marginally better in accuracy than the simple decision trees.

01:08:41.320 --> 01:08:47.040
But also, as you referred to, in that you made, there's a demand for transparency.

01:08:47.040 --> 01:08:55.280
If your government is going to put you in jail, you deserve a very simple, clear explanation of why.

01:08:55.280 --> 01:09:06.720
And so I guess I wanted to ask you, so I think with some of the other commenters that in some sense what you're talking about is simply a different space of models that can be optimized.

01:09:06.720 --> 01:09:17.680
I mean, with, you know, you can choose the structure of the tree with their hands and orders, whether they're, you know, the order of the questions in the banking example.

01:09:17.680 --> 01:09:26.680
There were parameters that I assume had to be set in some way, the threshold parameters.

01:09:26.680 --> 01:09:46.160
But so I guess my question is, when do you think that, in what domains do you think there is a strong argument for simplicity and transparency, even if something more complicated is somewhat more accurate?

01:09:46.160 --> 01:09:51.840
And by the way, I also agree with you that that ladder plane is often made when it is not true.

01:09:51.840 --> 01:09:52.400
Yeah.

01:09:52.400 --> 01:09:57.120
But in the cases when it is true, sometimes we still prefer the simpler thing.

01:09:57.120 --> 01:10:00.480
So I agree with Cynthia Rudin.

01:10:00.480 --> 01:10:16.120
And she and us, we have shown that in sentencing or bail decisions, very simple arguments that look at three various typically like age and previous sentences.

01:10:16.960 --> 01:10:31.880
Are about as good as complicated things or often black box, like compass and where nobody knows exactly what's in it and what's not.

01:10:31.880 --> 01:10:38.160
And also that these are situations where transparency is another important thing.

01:10:38.160 --> 01:10:45.800
So, for instance, in the European Union at this moment, there's a decision being made about the AI Act.

01:10:45.800 --> 01:10:59.720
And the AI Act is in part about ask for transparency of algorithms that are so-called high-risk algorithm and sentences is one of these examples.

01:10:59.720 --> 01:11:09.680
And the EU Union, if they don't change now things, they're voting about it, will not allow black box algorithm.

01:11:09.680 --> 01:11:18.000
And I've worked with the German shoe farm, which is like the American feet, so credit rating.

01:11:18.000 --> 01:11:20.720
They have immensely complex algorithms.

01:11:20.720 --> 01:11:27.040
If you make them simple, simple arguments do as well.

01:11:27.040 --> 01:11:38.280
And so in many of your questions, when can we expect that complex AI will work and not.

01:11:38.320 --> 01:11:41.840
It's very similar to the distinction between risk and uncertainty.

01:11:41.840 --> 01:11:48.760
If a situation is stable and well-defined, AI is much better than human.

01:11:48.760 --> 01:11:53.200
So like a classical example or a chess and go.

01:11:53.200 --> 01:12:02.680
If a situation is highly uncertain, like how can you predict whether bail decisions about whether the guy will actually come back when they.

01:12:02.680 --> 01:12:04.280
That's very difficult.

01:12:04.280 --> 01:12:18.840
So many factors, uncertainty in these cases, we have little evidence that say deep neural networks would do consider better than humans.

01:12:18.840 --> 01:12:22.560
But we know that it's black box.

01:12:22.560 --> 01:12:31.480
Right, I guess my question is, in some cases, we want, we should use the simpler thing, even if the more complicated is more accurate.

01:12:31.480 --> 01:12:35.720
For other reasons, because maximizing accuracy is not the least that matters.

01:12:35.720 --> 01:12:43.320
Right, and also, so for instance, in my, in our work with the shoe farm, so this is the German fecal.

01:12:43.320 --> 01:12:51.680
It turns out that they're using variables that I'd better not telling anyone just to get a Yota male accuracy.

01:12:51.680 --> 01:12:56.840
For instance, when you ask for your credit score, your credit score goes down.

01:12:56.840 --> 01:12:59.560
Yes, we don't want to tell the speed, right?

01:12:59.560 --> 01:13:06.440
So it is not acceptable, but it helps to predict.

01:13:06.440 --> 01:13:22.120
So the compromise will be get rid of these aggressions, make them simpler, and you may lose an Yota in prediction, but there's more than just predicting accuracy.

01:13:22.120 --> 01:13:27.880
There are human rights about transparency.

01:13:27.880 --> 01:13:38.160
So this is very, very, I have a slightly provocative question that it sort of hints at what we are doing.

01:13:38.160 --> 01:13:42.440
You like the logical rationality is not equal to ecological rationality.

01:13:42.440 --> 01:13:49.880
I don't mean this just semantically, but aren't you letting people off a bit too lightly with this distinction?

01:13:49.880 --> 01:14:00.520
I mean, surely there must be something wrong with logical rationality if it leads you to ecologically irrational decisions.

01:14:00.520 --> 01:14:15.080
I feel that our program looks at logical rationality and asks, come on, there is something wrong with this because it doesn't, basically it doesn't work in the real world.

01:14:15.080 --> 01:14:19.640
So are you being diplomatic by making this distinction?

01:14:19.640 --> 01:14:27.880
Do you really mean there's something wrong with logical rationality and we should really think of rationality as a different thing?

01:14:27.880 --> 01:14:30.440
Yes.

01:14:30.440 --> 01:14:36.480
So logic is a system that's very useful, but not for everything.

01:14:36.480 --> 01:14:38.640
That's the point.

01:14:38.640 --> 01:14:41.200
You wouldn't even understand language.

01:14:41.200 --> 01:14:51.560
When Kahneman and Tversky asked people about Linda, the bank teller, what is more probable?

01:14:51.560 --> 01:14:55.800
The term probable does not map into mathematical probability.

01:14:55.800 --> 01:14:59.320
It is, if you look in the OECD, it says, is there evidence?

01:14:59.320 --> 01:15:02.000
No, there's no evidence that she's a bank teller.

01:15:02.000 --> 01:15:02.960
Does it make sense?

01:15:02.960 --> 01:15:03.920
No.

01:15:03.920 --> 01:15:10.240
So to impose logical rationality on everything, that's me.

01:15:10.240 --> 01:15:12.320
This is no good.

01:15:12.320 --> 01:15:15.440
And it, and it mistakes the human intelligence.

01:15:15.440 --> 01:15:17.360
We make an inference from the content.

01:15:17.360 --> 01:15:20.520
What do these problems terms mean?

01:15:20.520 --> 01:15:23.960
Similarly, Linda is a bank teller and a feminist.

01:15:23.960 --> 01:15:33.480
So an end in everyday language, the end means sometimes logically ends, sometimes something different.

01:15:33.520 --> 01:15:38.240
And we have an amazing intuitive capability to infer it.

01:15:38.240 --> 01:15:43.600
For instance, when I say this evening, I invite friends and colleagues.

01:15:43.600 --> 01:15:46.840
You don't think it's the intersection.

01:15:46.840 --> 01:15:48.880
It's the logical or.

01:15:48.880 --> 01:15:52.320
So these are the real interesting questions.

01:15:52.320 --> 01:15:57.480
How does the human mind make this stunning inferences?

01:15:57.480 --> 01:16:01.400
Rather than declare it's an error, it's conjunction fallacy.

01:16:01.400 --> 01:16:06.520
Although what you're saying there is about language and not about actions.

01:16:06.520 --> 01:16:08.480
This is about language.

01:16:08.480 --> 01:16:14.800
So I have one example that is becoming a favorite of mine.

01:16:14.800 --> 01:16:20.800
We can show that a, so this is totally small world logical rationality.

01:16:20.800 --> 01:16:29.120
If you like, someone who maximizes expected utility does not maximize utility over time.

01:16:29.280 --> 01:16:36.200
To me is a catastrophe in the logical structure that you're building a theory where you say,

01:16:36.200 --> 01:16:40.280
well, we can't really tell you what you should do until you tell me what it is that you really want.

01:16:40.280 --> 01:16:41.560
And that is your utility.

01:16:41.560 --> 01:16:43.120
And then we give you a protocol.

01:16:43.120 --> 01:16:50.800
Now I start behaving according to this protocol and I don't get the thing that I asked for.

01:16:50.800 --> 01:16:57.520
So I don't maximize my utility over time because we're using the wrong, you know, what happens over time is not what happens in expectation.

01:16:57.520 --> 01:17:03.680
So these to me are real problems, logical problems within logical rationality.

01:17:03.680 --> 01:17:07.800
It's about taking the expectation of the logarithm instead as with fitness.

01:17:07.800 --> 01:17:10.520
It depends on what you just depends on what your dynamics are.

01:17:10.520 --> 01:17:12.040
Yeah, I mean, that's an example.

01:17:12.040 --> 01:17:19.720
There are a number of papers by biologists who come to a similar conclusion that if animals would maximize something,

01:17:19.720 --> 01:17:22.360
it's always, well, what do you maximize?

01:17:22.360 --> 01:17:24.760
Then they actually lose on fitness.

01:17:24.760 --> 01:17:26.680
Yes, sir.

01:17:26.680 --> 01:17:27.400
Thank you again.

01:17:27.400 --> 01:17:28.800
That was great.

01:17:28.800 --> 01:17:31.720
You can leave it to informal discussions and.

