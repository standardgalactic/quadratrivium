Perhaps Catherine should just introduce ourselves. So we were at Brown University for several decades
in the Department of Cognitive and Holistic Science there. I was also joined in Computer Science.
Moved to Australia about 14 years ago to Macquarie University and at the same time I think we started
becoming external faculty here at SFI. Catherine got a gigantic mega grant that required hiring a
lot of people doing a lot of work there. I started going into the startup route
and then what with that and then with COVID I think we let the external faculty connection lapse
but now we'd like to restart the connection with SFI again.
The startup stuff that I was doing was actually in the chatbot space.
We got acquired by Oracle Corporation. That's the reason for this disclaimer down here at
the bottom right. So nothing that I'm saying represents anything of any of our employers.
The other reason for this comment here is because the whole field of large language
bottles is really changing incredibly quickly. I don't know how much of what I'm going to say
right now is going to be true or relevant in six months time. So nothing, I'm not saying anything
that I believe is false but it could well, some of the effort could well be wasted.
So I'm going to try and give you guys the high level executive overview talk. This talk was
really actually aimed originally at academic researchers in natural language processing
and so I'm going to skip some of those some of those details but you know you had to have been
underneath a rock if you didn't notice the large language models who radically changed the field
in the last year or two and so I'll be talking about that. We'll also be talking about suggestions.
You know how should academics actually respond to this changing world? So we'll be talking a little
bit about what these large language models may mean or may not mean for human language acquisition,
the study of that. There's a lot of interest in neuro-symbolic models that integrate large
language models and more traditional kinds of AI models and I'll also talk a little bit about
alignment of LLMs and I'll end talking a bit about the connections between large language models
and the social implications of them and lessons from the first industrial revolution.
Katherine and I just spent two months at the University of Edinburgh and it was very interesting
there because of course the first industrial revolution really happened in Northern England
and around Scotland and actually it's a fascinating thing to read about that you know James Watt,
you know the inventor of the steam engine actually did a lot of his work at the University of Glasgow
so that's sort of really interesting. Okay so how deep learning changes NLP?
You know people like Katherine and I have been in the field long enough to have actually seen
really a switch from symbolic parameters, statistical NLP and then finally now to deep
learning and large language models and I guess I would say that the really huge difference is that
large language models can understand context much much longer contexts than anything that I thought
was even possible. In fact I actually thought there were good theoretical arguments that it should
not be possible to try and model a context as long as a sentence or a dialogue or an entire
paragraph yet large language models can do that perfectly well and if you want to I can go through
what those counting arguments would be but just you know when I actually look back at the
pre-deep learning work you know I did a lot of work on syntactic parsing which would involve
recovering pastries that looked something like this and I think the motivation for dealing with
you know wanting structures like these these things make local a lot of dependencies which
are non-local in the string here right so you know it's a flight through Denver right so
this relationship here is making that local the same thing also we're talking about a preference
for a flight for example that's making that dependency local why do you want to do that you
want to do that if you think that you can only really model pairs pairwise interactions large
language models as I said are happy modeling contexts of you know in the hundreds or the
thousands of tokens and so I actually think you know these theories are correct but they're just
no longer required so to do natural language understanding I don't actually have to recover
this sort of structure anymore okay so strengths of large language models we just talked about
how there's so much better at working with incredibly long contexts I think the other you
know I'm giving you the executive summary here the other really amazing thing is that you know
if you showed these large language models to people from the first neural network revolution
from the 1980s actually everything would be really quite familiar to them right there's
there's a slight twist with the attention mechanism but you could explain that in about 15 minutes
and those guys understand it all you know and Sutton has this thing which he calls the bitter
lesson which is that you know for people like me who work on lots of really interesting algorithms
and models and structures and you know things like that actually really none of it really
seemed to be relevant if you just got data and compute and scaled everything up that that really
seems to be the secret and I actually do think that these large language models are largely just
sort of brute forcing linguistic generalization so they're covering rather than really capturing them
but it's really interesting how well they work right that in fact actually with enough compute
and enough data you do seem to be able to produce something that's really quite amazing okay weaknesses
of large language models I mean I think this is probably everybody here's already heard about
this stuff you know Emily Bender's stochastic parrots you know or as I guess Gertrude Stein would
say there's no there there and I think actually these large language models just you know they
respond reflexively in fact I actually think if we're talking about human models these large
language models maybe actually aren't such so bad for the purely reflexive processing that maybe
humans might be able to do but scaled up you know whereas humans maybe have a context window
of five or something like that these large language models have a context window of five
thousand for example and I think actually that's also because they have no beliefs desires or
intentions they have no you know guiding you know thoughts that's why hallucinations such a big
problem you know they're just trying to produce plausible output I think it's really interesting
to actually wonder about you know these large language models now becoming multimodal how will
that change things I actually think if you think the fundamental problem is a problem of symbol
grounding then arguments like Searle's Chinese room argument still are just as valid against
multimodal input on the other hand I actually think the multimodal inputs incredibly interesting
might be incredibly powerful yes so naive questions what is multimodal training oh so here
what I really think is vision and language I should have made that clear vision and language
but these days in fact people are now starting to train off movies as well right so that's I mean
arguably that is one big difference between these large language models and
you know the way humans might work right and what is Searle's Chinese room
that's a really interesting thought experiment which I think for reasons of time I might just
skip here but it's a it's basically where he asks he asks us to imagine a blind symbol
following system and he says so can you really do you really want to say that this blind symbol
for you know rule following system actually has any understanding it doesn't really seem to make
sense to attribute any any understanding to it anyway I do actually think you know there's a lot
of people now that are also wondering are these large language models are they really intelligent
could they be a AGI artificial general intelligence that maybe might take over the entire world
I think they currently can't because they do lack you know beliefs desires intentions the ability to
form long-term memories for example but I'm I'm actually not so sure that that's such a huge
technological barrier I actually do think that it's possible that that could be relatively easy
now you know we've been in this situation before where we thought oh the thing that's really missing
from this machine being intelligent is x you add x to the machine and all of a sudden you discover
well actually you know there's this other thing why that's missing as well and so that may happen
here too but there is a chance that in fact if we just added episodic memory to these large language
models then they may actually become intelligent things the other okay another high level point
that I want to make is that these large language models are manufactured systems in the same way
remember I was talking about the first industrial revolution in the same way that a steam engine
is a very highly engineered manufactured system you know you wouldn't want to be trying to
infer the laws of thermodynamics or even the ideal gas laws by examining a steam engine
I think the same thing's true also about these large language models you know they're they're
trained in a way which is really an engineered product you know and certainly as somebody being
in industry I can tell you that you know the people in industry that are building these things
really don't care about doing science they're trying to build a product that they can sell
and that's really what they care about okay so just an aside about you know the relationship
between I I actually think a natural language understanding and say you know the science of
of linguistics or language acquisition so there's the scientific side and the technological side
I think the technology is currently outstripping the science and I think that has happened in
times before in fact I think it actually happened with the first industrial revolution
so I won't spend too much time here but my understanding is and maybe there's people
historians of science in this room that know more about this than I do but that you know
really in fact actually people started to invent steam engines long before they had the scientific
understanding first of all of ideal gas laws and then ultimately right through thermodynamics
and statistical mechanics that took centuries to the scientific for the scientific side to
emerge and in fact actually ideas of things like entropy really actually had to be developed
to answer questions about why it was not possible to build steam engines above a certain level of
efficiency for example and I suspect the same thing may be true today that our science of
say language and psychology is actually behind the technology. Okay one of the things that I
actually quite like is you know this comment here natural language is the new programming language
and that yeah I mean certainly for those of us in industry LLMs are really changing the way in
which we do our work right whereas it used to take a team of real experts to build say for
example a device which would identify all the financial products that are mentioned
you know in a particular document. Now you can just simply ask a large language model to do that
for you and it does a pretty good job maybe not as good as the very best hand-built natural language
processing system so those are still actually better but you know they take months or years to
develop whereas it takes you know maybe hours to use a new large language model so I actually think
that particularly in terms of the commercial implications the commercial deployment of natural
language processing in industry that's going to change completely. It's not clear we'll need nearly
as many experts in natural language understanding for example for the industrial applications.
I did want to mention a little bit you know I think that one of the really interesting
things that's happening in the field is taking these large language models
and then combining them with other components. The first component that people started to look at
was combining large language models with what's called a vector store or a retrieval system
and that's just simply something whereby when you ask a question instead of just directly asking
the large language model to respond to that question you retrieve a set of relevant documents
to that question feed those in as part of the input to the large language model you can see
that's what I'm suggesting that you do over here and then you then tell the large language model
to use to produce an answer that just simply references those documents and that's sometimes
called the reader retrieval model or retrieval augmented generation. That idea can get even
more power when you start to think well maybe in fact the large language model can actually decide
what information to do a search for and then when you then started to think well should it
decide what information to do a search for maybe in fact it could also call other tools
so these large language models are infamous for not being able to do numerical calculations very
well but maybe in fact what we should be doing is giving the large language model the ability
to call a calculator and there's just in the same ways which if I was to ask any of you guys to do
a complex task and involve something some numeric calculation I'd want you to be
also using a calculator rather than trying to do it longhand. Okay so in terms of research
directions inside of an LLM world so the very first comment to make is that it is very challenging
for academics to do research in large language models you know the ideal thing would be to have
something like an ideal gas experiment set up but and you certainly can build small
versions of these large language models there's some disagreement about whether or not though
whether there's well there certainly seem to be emergent capabilities so the bigger the model
the more things that it can do there's big arguments about whether or not this emergence
is like a phase change or whether it's really more incremental and I again I'd be happy to talk
about that later we could spend hours talking a bit about that but I think there's enough
lack of clarity about what emergence is but if you wanted to do academic research you really
do want to get access to you know the large language the larger large language models
and the problem is that these you know the best large language models are really complicated
commercial products as we were talking about before and it's what's actually even worse
is that these days for in you know proprietary commercial reasons the companies aren't even
actually telling us all the details of exactly what they're doing so that actually does make
it very hard for academics to really do any sort of academic research in in our own papers
you know I'm collaborating with people at the University of Edinburgh what we wind up doing
is saying we're actually not going to test the closed commercial systems but we will work with
the largest open source systems that are available I think that's not a bad thing to do but it does
mean that you're cutting yourself off from a lot of the really cool systems there yes
this is a very interesting question on that slide about how quickly they degrade as you move just
as we moved from commercial ones to smaller and smaller ones yes just a steady degradation
or is there a sudden drop so sorry and in fact actually this also gets back to the emergence
question so let me just say I actually do think that a lot depends on exactly how you measure it
so I don't have to tell people particularly at the Santa Fe Institute right that quite often
what you'll actually see is a lot of small changes all of a sudden reaching a tipping point
that is basically like a phase change and you know when you think about these large language models
I mean they generate output token by token if the correct answer is just slightly less probable
than some mistake right well then as the output gets very very long then the correct answer can be
incredibly improbable right so if you're just looking at the output of 100 tokens or more
you're just looking at the output you're just asking is the output right or wrong you'll go
wrong wrong wrong wrong wrong right and then all of a sudden as the correct token probability
just nudges above the incorrect tokens you know all of a sudden the output flips and all of a
sudden it's just magically all correct but if you measure the per token probabilities for example
then you'd actually discover a much more continuous change
you know so I think that's actually where a lot of emergence happens and in fact that's I think
the white there's a little academic dispute about whether or not these models have emergent
behavior or not and that's at least my understanding of how you'd resolve that
so I actually do think that a lot of the ideas that people here would have would actually be
very useful for the community to have as well okay all right you know I actually think there's
lots of really interesting questions also you know can we understand what these large language
models are really doing I think it's you know I mean we know something about how language is
processed in the human brain you know we know that none of these models really are realistic
just even understanding you know what these large language models are doing how can you
be sure that they know a syntactic rule or make it even simpler that they know a particular word
so right now and in fact actually I think another really interesting question is
if if these large language models are basically just you know gigantic neural nets as I said before
of a relatively generic type why is it that only human beings can acquire language are they are
you know why is it humans are the only animals that can acquire language right I mean you know
we don't have the biggest brains there are animals with bigger brains if it's just merely
the size you know the number of neurons that we have sitting inside of our skulls
if that's all that determines our ability to do something like learn language why why don't
other animals why don't they have that ability it's very popular now to talk about analyzing
large language models using psychological or psychological linguistic methods I think that's
about the best that I know how to do but I think a lot of these methods were really designed to
work on humans at least agents that have beliefs and again you know in the sense a large language
model doesn't have a belief it's just got reflexes okay so you know just to emphasize
the differences between large language models and and humans right so you know children start
and end learning from much smaller data sets they generalize in particular ways that we actually
understand to new unseen forms I think we don't really know actually how these large language
models generalize I mean they do I think they do generalize but it's very difficult to tell exactly
how they generalize children also actually over generalizing characteristically so Catherine's
an expert in this area but you know these are just a couple of examples that she pointed out you know
where children have taken irregular verbs and either inflicted them in a regular fashion or
over generalize the irregular form she's giggling me you know that makes sense if you think of
giggling as being a verb a bit like tickling for example language learning you know by the time
you're three or four you're a competent speaker of your native language usually but then there's
also some part of language learning that's not really complete until the early teens right and
then you know just in terms of the pragmatics of doing research on large language models right
the time scale of research projects are different so it might take a couple of years for a student
to do a research project studying say human language learning if they're studying something
which was inspired by GPT-4 well in two years time we're probably in GPT-6 you know and the
the the inspiration might be actually sort of completely different
I also this is essentially that same thing as as I was saying before right so evaluation and
testing I think is really a huge challenge that was always difficult inside of natural
language processing but it gets even worse because the inputs to large language models
now instead of just again because this context is so much longer the input is not just a
single sentence it's an entire conversation or entire story or something else like that
so if you want to really evaluate the performance of one of these systems you want to vary not just
you know the last sentence you want to vary the entire context as well from a commercial point of
view I think actually testing is really super important I mean you know you've probably all saw
the Microsoft Bing chatbot which when it was you know the New York Times reporter studied
that chatbot and it announced to the reporter that in fact actually that it preferred to be called
Sydney rather than Bing and then also suggested that really you know the reporter should divorce
his wife I'm sure behaviors that Microsoft was really not too proud of right and I actually
think for commercial purposes it's super important to be able to detect and you know
guaranteed that such behaviors really aren't lurking beneath the surface of your large language
model. Constraint decoding that's just an NLP topic I won't spend too much time on but I actually
do think that there's really interesting work there to look at different ways of actually
constraining the output of a large language model and some real challenges there I think there's
really interesting work about how one actually trains these models as well so I mentioned that the
training procedure is itself actually a very complicated one typically what happens is that
they start up by training with what's called the language model training objective which is where
effectively you're just simply training the model to predict the very next word but a model which
is just simply trained with this large language model training objective on its own doesn't really
engage in useful conversation doesn't really follow instructions very well so it's actually
very typical to follow that up with an additional training step that is you know that involves well
as I said there's reinforcement learning with human feedback and I think that's a really interesting
question I've actually got some theories myself about you know when you want to use one sort of
training objective versus another and if there's people that'll like to talk about that more generally
I think there's a really interesting question which is how do you align the LLM behavior
with well how do you get the LLM to behave the way that you want it to right so you've got these
very general alignment goals like you know follow commands that run right through to don't destroy
humanity so ultimately it's the training data and the training training procedure which is going to
determine the LLM behavior so how exactly do we do that right and I actually do think that there's
good academic research that can be done there largely because the fine-tuning step that I mentioned
this sort of multi-stage training that's pretty modest right tens of thousands of examples or less
and it can be done on sort of fairly modest hardware so I actually think that's a that's
that's a great academic research topic I feel a little bit guilty here because I just mentioned
to you that maybe the only thing which is standing between us and you know a artificial general
intelligence is the ability to have episodic memory and then now I'm going to suggest to you
how we might actually do that and it's you know the most obvious way to do that is to actually
take that retrieval augmented generator that I mentioned before and basically let the large
language model write its own memories and this is basically a suggestion about how you might do that
more generally I actually think that you know people like me have spent decades trying to come up
with specialized knowledge representation systems and specialized inference systems
you know so and this essentially is like a specialized logic you know so knowledge graphs
and one example of that where you try to encode information in entity relation triples for example
but I actually think with LLMs you know one real possibility is that you actually let the
you let the primitive statements actually be natural language statements
so you just have represented inside of your system stored inside of a vector store for example
something like insomnia is a typical symptom of diabetes and then you'd actually let the
large language model itself decide the relationship between these atomic propositions
and so instead of having a specialized knowledge representation language a specialized logic
you'd use natural language and you'd let the large language model actually pass information
from one atomic proposition to another for those of you that are as old as I am you know I mean I
loved prologue and you know very simple horn clause inference procedures so what I just tried to do
here was take take that and sort of show how I might replace first order logic in there with
natural language statements but otherwise you've got proof rules proof structures and so this is in
fact actually a standard you know textbook example of how uh you might wind up doing
inference here so you've asked the question you know can Sam get a degree and you've got a series
of facts about what courses Sam has taken and a series of rules but the difference is all this
is all expressed in natural language rather than in some first order logic form okay all right so
just talking a little bit more about the social implications of all of this
so I think to understand the social implications I think one of the things you probably want to
understand is try and make some guesses about how the field itself might actually evolve
I can see sort of two possible futures one is where we wind up getting ever larger
proprietary monolithic close large language models that you effectively interact with via
web APIs that is the actual model itself the training data everything is kept proprietary
but you can just simply call it over the web another future is that there will be open sourced
language models and the weights will actually be available and you'll be able to do things
like fine tune those weights yourself and right now you know we're in the world where there's both
of these kinds of large language models and the proprietary models are better than the open source
models and I think really the big question about the development of the field is whether or not
fine tuning will turn out to take the open source models and make them competitive with the closed
proprietary models and I call that the 64 billion dollar question because that's probably about the
amount of money that the companies that are investing here sort of have invested
you know the language models are becoming increasingly capital intensive it costs
huge amounts million many millions of dollars to collect the data and actually do the training
of these things and capital intensive industries tend to concentrate you know you just look at the
chip manufacturing where I think there's only one or two fabrication factories in the entire
world that make the top-end chips that we all have in all of our devices so if that is actually
what the future of LLMs is then probably we will see that same sort of concentration
into just a couple of places I suspect the training data will become increasingly important
people are already talking about training data as being the ultimate you know limiting factor
and I think it will become the major differentiator particularly if you want to do things
like build LLMs for very specialized domains like healthcare finance other things like that
but and I actually think data and LLM quality control which goes back to that issue about
testing and evaluation I was talking about before that's going to become increasingly
important in fact I actually when I think about what will somebody like me in industry be doing
in five years time quite possibly you know testing and evaluation will actually be you know
90% of what we do you know we know that fine-tuning can mask a poor large language model right so
we know that you can take take Sydney and do a little bit of fine-tuning and have it at least
superficially call itself being but then Sydney reemerges in the right context as well so I think
one way we'll get around that is we'll start seeing things like certificates of origin
you know we'll be saying you know I guarantee that my large language model has been trained on
just high quality data and the same way as you see certificates of origin for you know
fancy cheeses and things like that you know the cows grazed on grass organically raised on the
south's southern meadows and all that sort of stuff and particularly if in the if that open source
world that I was talking about before if that comes into play I you know I see that has been sort
of really one of the really big challenges I've I've seen how data vendors small startups are
really under incredible pressure to produce something which they can sell because they're
usually cash constrained and the same thing may be true for startups that are producing large
language models they'll be under huge pressure to take somebody else's model and do a few tweaks to
it and try to present it as something that's completely new and yeah impact on nlp jobs
I I actually do think that it's not too far off when we'll be able to say something like
give an instruction to a large language model it's like deploy a chatbot the task is informing
users about the products that you'll find listed in this database over here I want you to interact
with users in a professional tone emphasize customer service rather than price and politely
decline to talk about topics that are related to the products and that will be it that will
build you a chatbot you won't need an expert development team you know I do think that however
that that's not going to come up immediately we will for the next say five years or so we will
need people that can create training data and fine-tune models and as I said before I think
evaluation and testing is just going to get more and more important brought to social impacts
right so I think we already know that deep fakes and fraud are just going to get supercharged by
this sort of technology and yes I think that's true I think we're going to see automation of
jobs not previously automated Krugman has an interesting article in the New York Times just a
couple of days ago where you know he makes the point that it doesn't really matter whether these
LLMs really are intelligent or not that even a souped up auto correct can actually have
quite major implications in terms of productivity he's actually really quite positive he seems to
think that actually these things might you know level society somewhat and they might I mean there's
some evidence that in fact the GPT-4 for example enables poorer workers to work at a higher standard
whereas the best workers are helped less by GPT-4 maybe that's the case
I think there's a number of risks I you know I think we are seeing you know AI models being
trained on public domain data that the creators when they made their data public really had no
intention no one no expectation of their data would be used in this way we're seeing a political
fight right now between media companies and tech companies about the use of data I think that's
still mainly about search rather than training AI models but that same fight I think will continue
I looking back to the first industrial revolution and things like the tragedy of the commons I don't
see any particular reason to expect a socially optimal outcome although I think the writer's
Guild of America settlement actually sounds like it's a pretty forward-looking one and
I'm very pleased to see that I I do think you know I I'm not one of these people that that
poo-poo's the people that are worried about you know AGI and misalignment I don't think we're likely
to be made extinction but to be made extinct but I do think we should be worrying about that
and the final point I'd like to make is that I think these things are economic and political
choices not really technical choices so it's an interesting question so those of us that actually
have the technical expertise probably are in a position to have our voices heard more than what
they would normally be so we should probably make use of that but I actually really do think that
it's not just up to the the tech companies in particular to try and make the important decisions
here so conclusions I think LLM's are here to stay a lot of my people my age remember the AI
winter of the last century I don't think there's going to be an AI winter just simply because
these things are actually way too useful for students intellectual revolutions are a great
time to enter the field because in fact actually the amount of knowledge you need to have to become
an expert is much much less I think LLM's open up new interesting scientific research questions
and directions NLP I think will have less emphasis on clever new algorithms and more on
yeah interaction and integration of models applications data design and training and
much more emphasis on evaluation so thank you very much
any questions or comments yes hi thank you for great talk super interesting I just wanted to
ask you about one thing in the middle of the talk which is about
this kind of neuro symbolic integration and you had this you had this kind of proposal that LLM's
are going to give you parts of bits and then you're going to use those inputs into a logic model
and I wondered like why do you think I mean personally I like
neuro symbolic learning so but I'm interested today like why why do you think that's a good idea
or good approach like why not just let the LLM do the entire thing like what what is it that
so certainly certainly there are people that are betting yeah um you know let's just let the LLM
do the entire thing yeah um I guess the answer I would give there is that there are a lot of
academics and in fact I'm sure many of you've seen this stuff right it's it's now quite you know
there's like a little mini industry of people coming up with things like you know chat GPT
cannot understand negation GPT for does not understand X is Y statements you know and in fact
actually I don't think I'm ashamed but we have a paper that is claiming that LLM's cannot
understand you know do not really properly understand entailment you know that walking
entails moving but moving does not always entail walking um so if you really believe that stuff
if you really believe you know and if you believe it enough so that you actually think that GPT 5
or whatever the next model is that comes out is going to have to exactly that same weakness
the idea then is build a symbolic component that addresses whatever weakness you happen to think
these large language models have but it isn't like I'll admit it is rather risky because these
things are improving so rapidly and I'm not so sure I mean it's it's a risk if you're a grad
student to say all right I'm going to commit the next year or two of my life to working on solving
you know the problem of negation in large language models and halfway through your research project
you know someone discovers that just by scaling up the training data another order of magnitude
all of a sudden now it's going to handle negation just perfectly and that's what I meant by in fact
I think that was one of the one of the slides that Kate added was saying that it's particularly if
you wanted to do something some sort of behavioral research or something else like that you know where
this the timescale which LLMs are changing versus the timescale of doing behavioral research is
the LLMs are just changing so fast that if you if you looked at today's LLMs and said okay you're
inspired by then here are some interesting behavioral predictions that they're making I'm
going to go out and start running some experiments with kids or something like that
yeah you know by the time you've collected a quarter of your data there's another model out
there and it's got it's making different behavioral predictions it's just I don't know what the answer
there is except to say that yeah yeah you might just um ask a a follow-up question so if if you
have a chat system I think that's actually for the zooms but I they also said there's a whole array
microphone inside the box so well um I'll just talk about um if as opposed to um adding a symbolic
system to to a chat system or you know an LLM um what about fine tuning and just doing lots of fine
tuning instead I mean that's adding more data but if you fine tune it yes with that kind of data as
opposed to going as symbolic group that's right wouldn't that be I think that's I think that's
very true and in fact that I think that's a good question is you know if so right now what was it
you know Gary Marcus is picking up on the fact that uh somebody wrote a paper that said that
uh oh look there's a whole lot of cases where the large language models uh will quite happily say
that oh let's see all right I don't know enough about celebrities unfortunately but you know
so-and-so and so is Tom Cruise's mother right okay it accepts that statement and you then ask the
other you then ask the question who is Tom Cruise's mother and it says I have no idea so
and it seems like oh well if I've actually got a couple of comments there so first of all it seems
like x is y that looks an awful lot to a math mathematical person as being like x equals y
and we know that what was it uh equality is what is it commutative you know so if x equals y then
y equals x um in fact actually x is a y actually really isn't commutative um you know uh you know
uh what was it you know chicken salad is a wonderful meal you know that can be true but
a wonderful meal doesn't always have to be chicken salad right um you know so uh
but anyway so might you know the yeah
do you think uh do you think there's going to be uh kind of I mean what what do you think about
like the kind of vision vision language models where you're kind of train it on like a ton of
images and then it generates a load of you know you can generate these images we actually did
you know before I did the startups just before I did the startups I think the last student I
worked with was working on image captioning um and I'd very much like to go back to I I think
that's really incredibly interesting so while I think it doesn't actually solve the chart the
cell Chinese rule Chinese room objection you know I mean ultimately the input to all these models
are really just activation patterns and you know the models got no reason to suspect that images
closely you know more connected to the world than languages but I think just in practice there's a
very good chance that there may be really interesting correlations that can be learned by correlating
you know images with language and of course the only issue there is that the
um the amount of compute that's needed to do this is just really enormous and it's
you know you'd have to do some have to get some deal probably with one of the major tech
companies to get your access to enough compute yeah to do it
and that's that's sort of the problem with with a lot of this research now and in fact I think one
of the comments that I wrote there is you know maybe in fact we should be thinking about something
you know physics has been very successful in getting funding for big science maybe we should
be thinking about ways of getting funding for academic big science as well for dealing with this stuff
well and there if you think of images still images that's a certain amount of compute power
I think of visual no ongoing visual scenes and movies that's a lot more
and yet presumably real world learners are you know taking advantage of the visual scene
as it moves by um and uh although learning can take place in blind people as well and that's
a whole another research area so you know you don't need vision to learn language but it certainly
can facilitate aspects of yeah and then yeah and I guess also I mean like looking forward you'd
want these systems to actually be kind of situated in the physical world somehow I guess right well
so that so that's the certainly lots of people have got that uh you know feeling that in fact
actually that that we need situated you know situated models but I mean isn't the input to
a model always really just an activation pattern isn't it really always just uh I mean couldn't
you always run you know so you know there was the question what is silver's chinese room argument
right there was that basically you know uh supposing you you come up with a computer program
which can translate english into chinese or sorry translate chinese into english so you give that
to a person that's sitting you know inside of a room and you just simply tell him you know here's
a set of symbols follow these instructions and give me the output that you you you obtained
by following these instructions and so's point is that even if this thing does actually produce
good english as an output you really can't say that the person understands chinese you know that just
they don't understand chinese you know they're just following these rules and uh so his argument
really is that there's something else that a pure rule following system really doesn't have
understanding that something else is required now lots of people wind up saying oh well what you
really need is grounding you really do need these you know you need the symbols to be connected somehow
to the real world but I think the model never really knows I don't see any way for our current
computational models to know that the bit patterns that we're feeding into them
yeah correspond to anything in the world
yeah I mean yeah I would agree on that but um I guess for that kind of symbol grounding you
like some some people argue you you need to have a kind of community right language users so
maybe that uh all kind of do you know um and and then the grounding kind of comes out of
people using the same symbol in the same way yes right yeah yes no no no I mean you're
kripke you know so philosophers of language like kripke have argued that in fact actually
that and you know in fact this is sort of very true of me because I grew up in the southern
hemisphere I don't know a lot of the northern trees so I'm not sure I can with catherine's
help now I can recognize aspens but you know I'm not really sure about the difference between
oaks and elms and the rest of them right but kripke you would say that I can still talk about
all of those things and when I talk about oaks I mean oak trees even though I might not be able
to actually identify an oak tree reliably and so kripke's story there is exactly what you're
saying it's a community of language learners sorry language users and I am willing basically I'm
agreeing to the authority of language users and I effectively what I'm saying is when I use oak
I use it to mean whatever the rest of you mean that you know that grew up and presumably know
exactly what an oak tree is right and he says that in fact actually with a lot of particularly
scientific terminology we wind up using it that way right many of us may not be able to define
exactly what the difference is between the different types of neutrinos or whatever but
we rely on experts within our community to be able to ground those things
all I can say is I don't even know how you'd even tell a large language model that it's part of
yeah yeah no I was sort of yeah kind of thinking that sort of thing myself like how
well what I mean does that does the community there just mean literally that actually just the
text documents that have been fed into it is that I mean I guess with the the instruction
tuning I guess you get a little bit of that right
so anyone else have questions at all yeah so that it's almost just that you could have a community
of users of a particular model yeah that would have a sort of various types of queries within a
particular domain that might help train up that model then to become more a realistic conversational
agent within that particular domain perhaps yeah yeah so maybe that's kind of yeah yeah yeah
who knows maybe a year or two from now you know those things will start to emerge yeah
yeah so I think one of the things that Catherine and I are hoping to get out of this is to find
out more about you know work at SFI that we might connect with right but a sort of general interest
in you know language learning psychological aspects computational aspects you know so
so we're here until Tuesday afternoon Tuesday evening so please please contact us right
okay thanks
