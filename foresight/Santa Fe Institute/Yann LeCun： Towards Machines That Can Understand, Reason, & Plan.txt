All right.
OK, first machines they can understand,
which means also reason and plan.
It's going to be a lot of overlap with what Josh said,
at least in terms of motivation, but not in terms of solutions.
OK, first statement is that machine learning sucks,
certainly compared to what we observe in humans and animals
and their ability to learn and learn efficiently.
You know, until recently, most of machine learning
was based on supervised learning,
required enormous amounts of label samples.
What has taken over the last few years
is self-supervised learning, which does not
require as many label samples, but still requires
a huge amount of samples.
And in the end, those systems of still relatively brittle
makes stupid mistakes, do not reason or plan,
compared to humans and animals that can learn
new tasks extremely quickly, because they understand
how the world works, presumably, and they can reason and plan,
and have certainly some level of common sense.
So in our systems of today, most of them anyway,
not absolutely all of them, but many of them,
have a constant number of computational steps
between their input and output, which
means that whatever reasoning they do
does not change, depending on whether it's
a difficult problem they're trying to solve or not.
They cannot really plan.
The only systems they can plan at the moment
are the ones that are designed to play games
or to control robots.
But things like LLMs do not plan.
So how do we get machines to do like humans,
which is to understand how the world works,
predict the consequences of actions
they might take, perform chains of reasoning
with a potentially unlimited number of steps,
and plan a complex task by decomposing them
into sequences or subtasks.
So let me start with this idea of self-supervised running,
which really has taken over the world of AI
over the last few years.
And it's the basic idea of essentially presenting
an input to a system, let's say a text, a window of text,
or video, or a few images, and hiding part of it,
and then training the system to capture the dependencies
between what is observed and what is not yet observed,
but eventually will be observed, whether it's
the future of a video or a different view of the same scene
from an image or words that have been obscured.
And I say capture the dependency.
I don't say predict because I'm going
to talk about models that don't actually predict,
but capture the dependencies.
So a very successful example is language models.
So self-supervised language models.
And the idea goes back a long time to do this.
I think the first paper to really kind of experiment
with this was paper in around 2010 by Colbert and Weston,
where they had this idea of essentially taking
a piece of text, corrupting it in some ways.
In modern versions, it consists in removing some words
from the text, and then training some giant neural net
to predict the words that are missing,
or just merely to tell you whether the text that is here
is legit or not legit.
That's a different way of doing it.
So this is how every modern NLP system
over the last four or five years has been trained.
And that has completely revolutionized not just
the research in NLP, but also the practice of it.
So all of translation, content moderation,
hate speech detection, all that stuff from social networks,
it all uses this kind of stuff.
And performance went up by a huge amount.
OK, so a special case of this is generative LLMs.
And similar things are used in images and video.
And there, the part of the text that you're hiding
is just the last word.
So you train a giant neural net to just predict
the last word in a sequence.
And then you can use this to produce outputs
auto-regressively, which means you give a window of text,
you get a system to produce a word,
and then you shift that word into the input
by shifting everything by one.
Predict the next, next word, shift that in.
Predict the next, next, next word, shift that in, et cetera.
That's auto-regressive prediction.
It's a major flaw with this approach.
This is how every single LLM today works.
But we should call them auto-regressive LLMs,
because I think future LLMs are not going to be like this.
But basically, every single one of them, some of which
you've probably never heard of.
So the ones from Faire, Blenderbot, Galactica, Lama,
Alpeca, which is fine tuning of Lama.
There is a new one now also.
Lambda, Bard from Google, Shinshila from DeepMind,
Chai GPT, GPT4, et cetera.
They're all auto-regressive LLMs.
And they train on gigantic amounts of data.
So we're talking one trillion tokens or something like this.
It would take a human reading eight hours a day,
something like 22,000 years to read this.
So obviously, those things can swallow a lot more
and digest a lot more data than any human.
And the performance is nothing short of amazing.
But they do make stupid mistakes.
They are extremely fluent.
So we can use them to generate text.
But they make factual errors, logical errors, inconsistencies.
They have limited reasoning ability.
There is no way to control for things like toxicity
and stuff like that.
And they really have no knowledge of the underlying reality,
except in one case, because, of course, they only
train from text, except in one case.
And that case is code generation.
And they work really, really well for code generation.
And the reason they work well is that the underlying reality
of code is very simple.
It's deterministic.
It's just the state of variables of a program.
And so that's fully observable, deterministic,
and everything.
So it works really well.
And they can generate fluent text.
But in this particular case, this is a joke
that my colleagues did on me.
It's completely made up.
I never actually did a rap album.
Raw personal.
Yeah.
I asked them if they, I don't actually like rap that well.
So I'm over a jazz person.
So I asked them to do the same thing with jazz.
And they say, there's not enough training data.
And I cried.
OK, so what are they good for?
They're good for writing assistance, generating
first draft, producing a style.
Code writing assistance, obviously, very efficient for that.
They're not good for producing factual and consistent answers
because of aducinations.
And they're not good for taking into account
recent information, because you need
to retrain the entire system to take into account yesterday
in real time.
And that's just not practical.
They don't behave properly, or at least they're
hard to control to do so.
They don't do reasoning.
They don't do planning.
They don't do math, as we saw this morning.
They're being modified to use tools,
such as search engines, calculators, stuff like that.
But currently, it's kind of like using duct tape and staples.
And we're easily fooled by their fluency into thinking
that they are smart, but they are not that smart.
Now, there is a major flaw with this autoregressive
generation, which is that it's an exponentially
diverging diffusion process.
So if there is the probability, e,
for every token that is produced to be outside
of the set of correct answers, let's
assume that errors are being independent,
then if we generate a sequence of n tokens,
the probability for that sequence to be correct
is 1 minus e to the power n.
And that decreases exponentially.
So those things just don't work.
They just don't work.
My prediction is that five years from now,
nobody in that right mind would be using autoregressive LNMs.
It's just a bad phase.
They are useful, though.
They're very useful.
So as I said, they have a constant number
of computational steps between input and output
for each token generated.
They do not reason and plan.
Jake Browning, who will be talking Wednesday,
and I wrote a philosophy paper.
I mean, he wrote it on the fact that there
are limitations to the purported intelligence
of systems that are purely trained from text,
because I would argue that most of human knowledge
is not textual.
I mean, certainly most of what babies
are on before six months is non-textual.
And everything that animals learn is non-textual.
So that knowledge is still unattainable to current AI
systems.
So how do we get machines to understand how the world works
and predict the consequences of their actions?
All the limitations have been pointed out
by a number of different papers, including one
by the MIT crowd, that fluency is really not
the same as thinking.
And basically, you could argue for the fact
that what LLMs are good for is perhaps modeling
the Browning-Key and Boracay areas,
but not much else in the brain.
And that's like tiny little areas on the side of the brain.
So we need something else.
What are we missing?
This is a chart that I like to show.
Oops, the animation is bad.
But it was put together by Emmanuel Dupu, who
kind of tends to indicate at what age babies
learn basic concepts, like object permanence, for example.
Liz was talking about that.
Stability and support and intuitive physics,
which only comes up fairly late, actually, around nine months.
And the question is, what type of learning
is taking place there?
No AI systems today really kind of does this properly,
although there's been several attempts by a few of us.
So I think perhaps it's this type of learning
that is the basis of common sense.
And we should really try to figure out
how to reproduce this with machines.
So I think there's three challenges for AI research
today, learning representations and predictive models
of the world, allowing machines to predict what's going to happen,
perhaps as a consequence of their actions.
Learning to reason.
So this is more like Daniel Kahneman's System 2.
Current autoregressive LLMs are basically System 1.
They just view one word after the other
without really planning ahead.
And so that is the question of making reasoning compatible
with learning.
Josh has a particular proposal for this,
which I don't agree with, but that goes in the right direction.
And then learning to plan complex action sequences.
So I made a proposal for this almost a year ago now,
which I posted on this website so people can make comments
and tell me I'm wrong and which references I missed.
I guess several technical talks about it as well.
And basically it's sort of a modular organization
of an AI system that would be capable of reasoning and planning.
And I can't say that I've built it,
but we're kind of building pieces of it.
So it's composed.
It's basically centered around the award model, which
will allow the system to predict ahead
what the consequences of its actions would be.
And it has a cost module.
Think of it as kind of visual ganglia stuff.
And the only purpose of the system
is to optimize that cost.
Some of those costs are essentially
intrinsic, hard-wired, immutable costs
that sort of drive the basic behavior of the system.
And some of them are trainable costs
that the system learns as it goes.
And what the system does is that it
plans a sequence of actions that, according to its model,
will minimize those costs.
And of course, it needs to be able to estimate
the current state of the world, which is done through perception
and maybe access to a memory.
And then depending on the task that the system is focusing on,
it can be entirely configured by a configurator
that will sort of focus the system on the task at hand.
So that's a cognitive architecture, which
some people in classical AI have been proposing,
but in sort of different forms.
And there's two ways to use it.
Mode 1, which is just a reactive perception action cycle,
get an idea of the state of the world,
encoding into an abstract representation
of the state of the world as 0, and then running
through some other neural net that produces an action
reactively.
But the more interesting mode is mode 2,
which is like Kettiman's System 2,
where you make an estimate of the state of the world,
and then using your word model, predict ahead of time
what's going to happen according to an imagined sequence
of actions that you might take.
And then the agent would optimize that sequence of actions.
So as to minimize a particular cost function,
representing the tax to be fulfilled.
And then it would just take the first action
and actually send it to the actuator,
or maybe the first few actions.
So this is completely classical in optimal control.
It's called model predictive control.
Except the problem here is how you learn the model.
There's a way to kind of turn System 2 into System 1,
which I'm not going to go into.
OK, so how do we train the world model?
Only for the fact that the world is not deterministic
or not entirely predictable, even if it is deterministic.
So we're not going to have a neural net observe the input
and just predict why, and then minimizing a prediction error.
That's not going to work, because that can only
make one prediction.
So in fact, if you train a big neural net
to predict like these are cars from a top-down view
of a highway, if you train a neural net
to try to predict what's going to happen in this video,
you get blurry predictions.
Because the system cannot predict if a particular car is
going to break or accelerate or turn left or right,
and so it makes these blurry predictions.
Same for a natural video.
That's an old work on video prediction.
So you have to account for the fact
that the world is not completely predictable.
And you have two solutions there.
Either you build an architecture with latent variables
that parameterizes the set of possible predictions,
or, and those two are not incompatible,
or you abandon the idea that you're
going to predict everything about the world.
And so this is what I'm suggesting.
So this is a generative architecture.
Generative architecture observes x, encodes it,
then predicts y, the variable whose dependency you're
trying to predict.
And then you measure the prediction error.
You mean my side by training, et cetera.
What I'm proposing is a joint embedding architecture
where both x and y go through encoders, neural nets,
and the prediction takes place in representation space.
What that allows the system to do is basically
eliminate a lot of irrelevant information from y
when it encodes it into SY so that it doesn't
have to predict all the details.
So there's a lot of things here and a lot of information
in this room that we cannot possibly remember or predict
the precise texture of the wood on the floor, things like that.
But it's kind of irrelevant.
We only need to have sort of a relatively abstract
representation of it.
So I'm basically recommending to abandon
the whole idea of generative models,
unless you want to produce pictures or produce text.
But if you want to learn how the world works,
you should not reconstruct.
There's actually several versions
of those joint embedding architectures,
the simple one, deterministic ones that can predict,
and then nondeterministic ones that
can predict where the predictor can have latent variables.
So that's kind of the most general architecture.
And the latent variable A here can be a latent variable you
infer or it could be an action.
So imagine that this is a world model.
This is the current state of the world
that you observe, you encode.
This is an action you might take in the world,
maybe combined with some latent variable which
represent what you don't know about the world.
And then you make a prediction, and then
you can compare that prediction with what actually occurs
if you want to train your model.
And that's a predictive model that
will allow you to predict what's going
to happen as a consequence of your actions.
Now, because we're not generating anything,
and because we can't turn a model of this type
into a probabilistic model of t of y given x,
we have to abandon the whole idea of probabilistic modeling.
And now Josh is going, oh my god.
Isn't it just approximate probability at that point?
Isn't it?
No.
No?
No.
It's energies, OK?
So basically the name of the game here
is that you need to understand the system as computing
an energy function that captures the dependency between x and y.
So imagine the data points are those black spheres.
The energy function should take low values around the black
spheres and higher values outside.
And whether this energy function represents
the unnormalized log of some probability,
is irrelevant, you just want the energy
to be higher outside of the manifold of data.
And it will have captured the dependency between the variables.
And there's nothing more you need.
Now, the next question is, how do you train a system
to give low energy to stuff you observe and high energy
to stuff you don't observe?
And there are two methods, contrastive methods,
which consist in generating fake contrastive points whose
energy is going to push up.
And then regularized methods, which
I'm going to explain in a second.
So let's say you have training samples.
Your system currently gives low energy
to this sort of peak area here.
And it's not a good model of the data here,
because it gives high energy to data points and low energy
to areas that have no points.
So what you can do is generate green points here whose energy
you're going to push up.
And the energy function is going to take the right shape.
Or you could use some sort of regularizer
that minimizes the volume of space that can take low energy.
So that whenever you push down on the energy of some regions,
the rest has to go up, because there
is a limited amount of volume that can take low energy.
So in the context of joint embedding architecture,
I kind of invented the contrastive methods.
That's called sine is net in the old days.
But I'm now arguing against that in favor of regularized methods.
And the big question is, how do we train them?
I'll tell you about that in a minute.
But I'm asking you to abandon generative models,
abandon probabilistic models, probabilistic modeling in general,
abandon contrastive methods.
And of course, abandon reinforcement learning.
But that, I've been saying this for years.
Those are four of the main pillars of machine learning.
That makes me super popular among my colleagues.
OK, so what are those regularized methods
for joint embedding architectures?
So essentially, there is a big issue
that you have to fix, which is that when you train a system
like this, one of those JEPA architecture,
joint embedding predictive architectures,
you show it an example of x and y.
And you tell it just train all the weights of all those neural
nets so as to minimize the prediction error, it collapses.
Basically, what it says is that, well, I can just
set Sx and Sy to constants and set the prediction,
set the predictor to some constant thing
and ignore x and y all together.
And that would be a collapse system that gives zero energy
to everything in your space.
You have to prevent that from happening.
And one way to prevent that from happening
is finding a way to maximize the information content
of the representations that come out of the encoders.
That actually has the effect of minimizing the volume of stuff
that can take your energy indirectly.
So one way to prevent the outputs from being constant
is that you can force the variance to be non-zero.
So you put a cost function on top of this vector here
that says, over a batch of samples,
I want the variance of each variable coming out of that neural
net to be non-zero, to be above one, let's say.
So that's a hinge loss that says the variance
needs to be above one.
It's not enough because the system
can still cheat by making all the variables the same
or very highly correlated.
So you have another cost that says,
I want them to be decorrelated.
So basically, this has the effect
of enforcing the covariance matrix of that those Sx
vectors over a batch to be close to the identity.
And it's not enough because the variables
can be non-collapsed and correlated
but still dependent.
And so there's another trick that we do,
and we have some theory that shows that it's not stupid,
which is that you take the Sx vector,
you run it to some neural net that expands the dimension,
and then you apply those criteria on the covariance
matrix to the output.
And that makes the variables of Sx kind of more independent.
Now, there's a major flaw with this, which is,
and that's the theory which I'm not going to talk about.
There's a flaw with all of this, which is that we're basically,
we have an upper bound on information content,
and we're pushing it up, hoping that the actual information
content will follow.
And it's stupid, but it kind of works.
OK, so you can test those pre-training
for image recognition.
For example, you show two different views of the same image,
train the network to produce identical representations
for two different views of the same image,
and then you freeze the network and basically train
a linear classifier on top with ImageNet
and measure the performance.
And this Vcrag method that I just described
works just as well as isn't the top pack, let's say.
There's a bunch of different methods
that have similar performance.
And they are in the top pack.
I'm not going to bore you with details.
You can try to do segmentation as well.
Here's another method, somewhat similar,
but closer to the JEPA idea, which
uses a different criterion to prevent collapse, which
I'm not going to explain.
And this one takes a partially masked input image
together with a full input image, runs both of them
through encoders, and then trains a predictor
to basically predict the representation of the full image
from the representation computed
from the partially masked image.
This is called IJEPA, ImageJEPA.
And it works amazingly well.
And it's really fast to train very good performance.
In terms of performance, even though this type of masking
does not require any knowledge about the nature of the input,
essentially, or very little, the still
you get the same kind of performance
that you would get if you used a self-supervised learning
method that exploits the fact that you're
doing image recognition, like Dino or Ibot or Simclear,
for example.
OK, now, how are you going to use this in the end?
What I'm really interested in is to use JEPAs as world models
inside of the system.
They can do system two type planning,
but even better than this, they can do hierarchical planning.
And the idea there is that when you think about a task,
you're not planning this task at the lowest level
in terms of millisecond by millisecond muscle control.
You're playing a task like, I want
to go from Santa Fe to New York, or let's say
from New York to Santa Fe, that's a better example.
So you first decompose this into two sub-tasks.
First thing I need to do is go to the airport
and catch a plane.
How do I go to the airport?
Well, to go to the airport, I need
to get on the street and have a taxi,
which you can do in New York City, not in Santa Fe.
How do I get down in the street?
I need to get out of the building I'm in, et cetera.
How do I get out of the building?
I need to stand up from my chair, walk to the door.
How do I get up from my chair?
So you kind of decompose this all the way down
to the lowest level millisecond muscle control.
But you're not going to plan the entire task of going
from New York to Santa Fe all the way down to millisecond
by millisecond muscle control.
You do a hierarchical planning.
We think humans, that we are the only ones who can do this.
Animals do this, too.
You observe the cat planning a trajectory
to jump on a piece of furniture.
They definitely do a hierarchical planning.
So basically, what you do, what you need for this
is a sort of hierarchy of JPA architectures of predictors
that progressively produce more and more
abstract representations of the state of the world,
so that in the very abstract space of representations,
you can make long-term predictions.
Whereas in the sort of lower levels of abstraction,
you can make shorter term prediction,
but they're more accurate in the short term.
So this is a two-level architecture.
Low-level, you can make short-term predictions.
High-level, you can make longer-term prediction
in a more abstract space that has less details
about how the world works.
Now, we've been able to train a particular instance of JPA
that simultaneously learns teachers that
are good for image recognition and motion prediction
in images.
And I'm not going to go into the details of how
this is pretty hairy.
But it's kind of hierarchical.
And it's got predictors that make pretty strong assumptions
about the type of prediction that can occur.
And simultaneously learns invariant features
for image recognition.
And this works really well for things like image segmentation,
depth estimation, tracking, et cetera.
It's called MCJPA, which means motion and content.
And with this, hopefully, one day,
we'll be able to build architectures that
can perform hierarchical tanning of the type that I
was telling you about.
So observe the world, compute the abstract representation,
and even more abstract representation,
even more abstract representation,
make a prediction to minimize a particular cost
function that defines your task.
I'm assuming this cost function is differentiable,
so we can do this inference by gradient descent.
In first, some latent variable that
may represent the macro action you're going to take,
or some unknown variable about the world.
And then the state you're going to obtain
through the first prediction is going
to constitute a cost function for the lowest level.
So the first predictor at the top tells me
I should be at the airport.
I started from New York.
I should be at the airport.
The cost function below measures how far I am from the airport.
And so the second predictor says, go down in the street.
Take a cab to the airport.
And so the cost function at the bottom here
says, am I on the street?
Likely to catch a taxi, and all the way down
to the actual actions that you can take in the real world.
All right, coming to the conclusion.
So steps towards autonomous AI systems.
Self-supervised learning.
We need a recipe that allows us to train systems
to learn how the world works on video.
I can't claim that we have achieved this.
We're kind of partially there.
And legal uncertainty in the prediction,
and that's with a combination of this JEPA architecture
understood within the context of energy-based model,
potentially with latent variables, which I didn't talk about.
That would allow us to learn world models from observation,
hopefully hierarchical world models, possibly
with interaction as well, and exploration.
And now what we have is an architecture capable of reasoning
and planning.
I mean, the whole architecture I presented
is kind of this idea of system two,
that you can decompose complex tasks into simpler ones,
and then plan a sequence of actions
before you take the action.
Something that's sort of missing from current autoregressive systems.
So is this a potential path towards sort of human-level AI?
Possibly yes, but it's certainly not for tomorrow.
This is maybe a 10-year plan, maybe
to get to cat-level intelligence or something like that.
Now interestingly, those machines will have inevitably
some sort of emotion consciousness.
Forget about this, but emotions certainly,
because emotions are kind of an anticipation of outcome, most of them.
I mean, some of them are immediate perception of outcome,
like pain and things like that.
But most of them are anticipation of outcome,
and this cost function is exactly what this is.
And so if the system sort of predicts a particular set of outcome
that results in a bad outcome, it might feel something similar
to fear or something of that type.
Anyway, so common sense is a collection of world models,
or perhaps a single world model that is configurable.
I'll come to this in one second.
Understanding really means being able to predict.
I think prediction is really the essence of intelligence here,
and better mental models need to better understanding,
or other substrate, if you want, of understanding.
And as a consequence, also of good reasoning and action planning.
The complex part in all of this is going
to be to design intrinsic cost functions that
drive the system towards learning appropriate things.
And it's quite possible that, in the case of leaving things,
it's easier for evolution to hardwire your cost functions into us
than to hardwire your behavior.
Hardwiring behavior and physical models and whatever is super hard.
Like, as a neural net person, I would have no idea
how to architect neural nets to do this.
But I can certainly design a cost function that, if minimized,
the system will learn those basic concepts.
And that, there is a lot of hardwiring in there, no question.
So one module I didn't talk much about is the configurator.
And what it's supposed to do is configure all the modules
in this architecture for a particular sub-task
that the system needs to be focusing on at the moment.
And I'm imagining that there is actually
a single world model engine in this architecture that
is reconfigurable for the task at hand.
But it's not like the system would have multiple world models
for different situations.
It's got a single one that's configurable.
The advantage of doing this, I mean, for humans and animals,
is that it might actually fit in your skull.
But there is another algorithmic advantage,
or epistemic advantage, which is that a single one model can
share knowledge between different situations.
Whereas if you had a separate world model for different situations,
you would have to retrain it independently
for each of those situations.
So how to make this configurator work, I have no idea.
But that's a good hypothesis.
So that would explain the fact that there is a single world model.
It would explain why humans and many animals
can only focus on the single conscious task at any one time.
Because we only have one world model.
We can only do system two on one task at a time.
And I just leave the question for.
