WEBVTT

00:00.000 --> 00:10.000
Rwy'n cael ei ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud.

00:30.000 --> 00:50.000
Rwy'n cael ei ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud.

01:00.000 --> 01:30.000
Rwy'n cael ei ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'

01:30.000 --> 02:00.000
ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud

02:00.000 --> 02:30.000
o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r d

02:30.000 --> 02:41.380
dweud e wicked

02:41.380 --> 02:47.160
Go ahead, everybody!

02:47.160 --> 02:50.540
Hello!

02:50.540 --> 02:53.060
First off, um, some thank you's.

02:53.060 --> 02:56.300
The most important one.

02:56.300 --> 02:57.860
We at S own a fan Locities like to thank the McKinnon family foundation

02:57.860 --> 02:59.940
who underwrites this entire

02:59.940 --> 03:04.060
liliwsham tîn hefyd yllud o flyngeoliol,

03:04.340 --> 03:10.420
ti yw'r amgueddfa Sabir na'r請oedd fiTVll I.

03:10.740 --> 03:12.220
Felly, bydd e môl ychydig.

03:12.220 --> 03:21.500
ye'r yw gweld cynnigadseitau mewn gweld mewn gweldiannau,

03:21.500 --> 03:27.340
a ni'n tyfn ni'n gwoedd ar yektoedd ond bynnag.

03:27.340 --> 03:29.340
Thank you very much for this lecture.

03:29.340 --> 03:32.480
Very importantly, we would like to thank you, the members of the audience,

03:32.480 --> 03:35.480
for participating in this series.

03:35.480 --> 03:39.480
Again, again!

03:39.480 --> 03:43.580
Very, very prosaicly, thank you for voting at the Santa Fe Institute,

03:43.580 --> 03:47.360
this series yet again as the best lecture series

03:47.360 --> 03:50.080
in this town and city of Santa Fe,

03:50.080 --> 03:53.320
but also more importantly than what you're voting,

03:53.320 --> 03:55.480
thank you for your participation.

03:55.480 --> 04:00.260
rydw i'n meddwl cyblir ni dwi wedi'i effearntu se VR

04:00.260 --> 04:03.600
wedi neger i ddim yn reunio

04:03.600 --> 04:07.460
ac yn redes i gweithio hwnnw i rwylo weit ti gael

04:07.460 --> 04:11.220
ydi hwnnw i kontos i g difrwyslaud ar ei gweithio

04:11.220 --> 04:15.720
i hynny'r panfaith beirgo.

04:18.500 --> 04:25.160
Efallai gyda ni wedi meddwl gweld eu hon diagnosed

04:25.160 --> 04:27.720
o'r oedd y cwmfersiwyr.

04:27.720 --> 04:28.520
Okay.

04:28.520 --> 04:31.260
Vijay Baosubamaniaan.

04:31.260 --> 04:33.960
Wow.

04:33.960 --> 04:35.320
Okay.

04:35.320 --> 04:36.120
How do I begin?

04:36.120 --> 04:39.280
Let me start talking about his research interests.

04:39.280 --> 04:43.560
And I'll be presenting a lot of quotes directly from him

04:43.560 --> 04:46.520
to try to get the nuances correct,

04:46.520 --> 04:48.360
even though I'm familiar with a lot of his work,

04:48.360 --> 04:50.760
I'll collaborate with him and so on.

04:50.760 --> 04:53.080
In Vijay's own words, quote,

04:53.160 --> 04:56.080
he is interested in how natural systems manipulate

04:56.080 --> 04:58.480
and process information.

04:58.480 --> 05:00.720
Okay, well, at that level, you know, big deal.

05:00.720 --> 05:02.880
Everybody here is interested in how systems

05:02.880 --> 05:04.720
process information, right?

05:05.720 --> 05:09.720
Well, God is in the details of what one's interests are

05:09.720 --> 05:14.560
and with Vijay, those details are pretty damned impressive.

05:14.560 --> 05:17.200
For example, in the way that Vijay

05:17.200 --> 05:19.280
construes those particular words,

05:19.280 --> 05:23.240
one of the ways that natural systems process information

05:23.240 --> 05:25.080
is, well, you know,

05:25.080 --> 05:28.040
the fundamental nature of space and time.

05:28.040 --> 05:31.080
So for example, he has done pioneering work

05:31.080 --> 05:34.480
in what is known in quantum cosmology

05:34.480 --> 05:36.580
as the information paradox.

05:36.580 --> 05:39.800
That's the fact that naively at least,

05:39.800 --> 05:43.880
information seems to get lost into a black hole

05:43.880 --> 05:47.320
even though the laws of physics forbid there being any loss

05:47.320 --> 05:49.520
of information in the universe.

05:50.480 --> 05:53.560
Following this theme, he's investigated,

05:53.560 --> 05:55.520
again, quoting from him directly,

05:55.520 --> 05:58.520
how the familiar smooth structure of space-time,

05:58.520 --> 06:00.160
you know, that's what we're familiar with,

06:00.160 --> 06:03.080
how it can emerge for more complex underlying

06:03.080 --> 06:04.480
physical constructs.

06:05.440 --> 06:07.400
So, you know, these are the kinds of things

06:07.400 --> 06:11.360
that you and I might mull over while I'm taking a bath

06:11.360 --> 06:14.400
or on the way to work this morning.

06:14.400 --> 06:17.040
Let's see, I'll have to walk the dog when I get home

06:17.040 --> 06:20.000
and buy a cup of milk and, oh yeah,

06:20.000 --> 06:22.240
just what is going on at the Planck scale

06:22.240 --> 06:24.280
at the event horizon of a spinning black hole

06:24.280 --> 06:26.080
that's got a little bit of charge.

06:26.080 --> 06:27.440
The kind of thing that everybody does

06:27.440 --> 06:29.120
on the way to work in the morning.

06:30.480 --> 06:32.920
Anyway, no surprise though,

06:32.920 --> 06:35.640
the mind of someone like Vijay cannot be contained

06:35.640 --> 06:40.280
in just the fundamental nature of the physical universe.

06:40.280 --> 06:42.720
He actually takes a step back from that and asks,

06:42.720 --> 06:46.840
well, how is it that we might actually come to our understanding

06:46.840 --> 06:50.080
of the physical nature of the universe?

06:50.080 --> 06:54.360
He's been interested in how it is that the human enterprise

06:54.360 --> 06:59.360
of science comes to try to understand scientific reality.

07:00.120 --> 07:02.560
Again, using his own words,

07:02.560 --> 07:05.720
he has also worked on problems in statistical inference

07:05.720 --> 07:08.360
and in particular, Occam's razor,

07:08.360 --> 07:11.040
which is very interesting because all scientific theories

07:11.040 --> 07:14.000
involve fitting of models to data,

07:14.000 --> 07:17.600
trading it off versus the complexity of that model.

07:17.600 --> 07:19.760
Again, just the kind of thing,

07:19.760 --> 07:21.880
maybe this is for the shower rather than the bath

07:21.880 --> 07:23.000
when you go into work.

07:24.560 --> 07:27.680
In short, Vijay is somebody cast in the same mold

07:27.680 --> 07:29.520
as the Santa Fe Institute is in general,

07:29.520 --> 07:32.040
a person of very many interests.

07:32.040 --> 07:34.840
Now to wax a little bit more prosaic though,

07:34.840 --> 07:37.240
one difference between the SFI and Vijay

07:37.240 --> 07:41.440
was that Vijay was born a little bit earlier than the SFI was.

07:41.440 --> 07:46.640
Specifically, he was born in Mumbai, India, Bombay back then,

07:46.640 --> 07:48.600
moved among many cities in India.

07:48.600 --> 07:51.040
His family eventually moved to Jakarta.

07:51.040 --> 07:54.400
Then he came to MIT in the US.

07:54.400 --> 07:58.040
His degrees at first were physics and computer science.

07:58.040 --> 08:00.800
Then he got a PhD at Princeton.

08:00.800 --> 08:04.480
After that, among other awards, well, just achievements,

08:04.480 --> 08:05.880
he was accepted as a junior fellow

08:05.880 --> 08:08.080
with the Harvard Society.

08:08.080 --> 08:11.720
Around this time, he first formed a connection with the SFI.

08:11.720 --> 08:15.640
Since 2000, he has been a professor at University of Pennsylvania.

08:15.640 --> 08:18.200
He's a fellow of the American Physical Society,

08:18.200 --> 08:21.800
has received many awards, over 17,000 citations

08:21.800 --> 08:23.840
in the academic literature.

08:23.840 --> 08:26.640
So that's a little bit of the more prosaic kinds of,

08:26.640 --> 08:30.680
well, he did this and he did this and he did that kind of details.

08:30.680 --> 08:34.920
Anyway, now though, the third one of his major interests,

08:34.920 --> 08:37.160
what he'll be talking about tonight,

08:37.200 --> 08:41.280
this is one way to phrase it, it's how your brain,

08:41.280 --> 08:44.360
the three pounds or so of pink mushy jelly

08:44.360 --> 08:46.480
enscarced in your cranium,

08:46.480 --> 08:48.040
that you know, it kind of wiggles around

08:48.040 --> 08:49.600
when you do this too fast,

08:49.600 --> 08:53.040
how it is that that, the information processing,

08:53.040 --> 08:55.160
remember that's his underlying theme here,

08:55.160 --> 08:57.440
how it is that the information processing

08:57.440 --> 09:03.720
that goes on in that jelly becomes you, okay?

09:03.720 --> 09:05.600
And I have personally been very fortunate

09:05.600 --> 09:07.400
to have some interactions,

09:07.400 --> 09:09.040
we've had many discussions,

09:09.040 --> 09:11.640
Vijay and I on a topic related to that,

09:11.640 --> 09:15.760
namely the energetic cost of the information processing in the brain.

09:15.760 --> 09:18.960
Anyway, tonight you're really in for a singular treat

09:18.960 --> 09:21.480
and with that, over to you, Vijay.

09:21.480 --> 09:31.320
APPLAUSE

09:31.320 --> 09:32.640
Well, David, thank you very much.

09:32.640 --> 09:34.320
Oh, first of all, can you hear me?

09:34.400 --> 09:35.600
Yeah, okay.

09:35.600 --> 09:37.360
Thank you very much for that lovely introduction

09:37.360 --> 09:41.040
that was really kind and it's a delight to be back in Santa Fe.

09:41.040 --> 09:43.360
This is one of my favourite cities on earth.

09:44.600 --> 09:46.440
I've always really liked coming here

09:46.440 --> 09:48.960
and thank you very much for inviting me back

09:48.960 --> 09:51.640
and thanks so much for coming to my talk today.

09:52.640 --> 09:57.840
So great, so let me first of all check that my iPad works, it works.

09:57.840 --> 10:00.480
Great, so the subject of today's talk

10:00.560 --> 10:05.200
is how the brain makes you.

10:05.200 --> 10:09.320
So we should start by first discussing what I mean by making you.

10:09.320 --> 10:11.680
So you're an animal, like any other animal.

10:11.680 --> 10:14.560
So the first thing we've got to ask is what do animals do?

10:14.560 --> 10:17.120
And you know, here are a number of animals

10:17.120 --> 10:19.040
from the OSA, Peninsula and Costa Rica,

10:19.040 --> 10:21.600
these are photographs taken by my son,

10:21.600 --> 10:24.080
who is also one of the animals there.

10:24.080 --> 10:27.200
And what do animals do?

10:27.280 --> 10:29.800
So the main business of being an animal

10:29.800 --> 10:34.800
involves eating, navigating, exploring the world,

10:34.800 --> 10:38.840
sometimes resting like this one here, manipulating the environment,

10:38.840 --> 10:43.200
look at the nest and this web here,

10:43.200 --> 10:46.880
interacting with each other, reproducing, things of this kind.

10:46.880 --> 10:50.640
And that's what animals do almost all of the time.

10:50.640 --> 10:53.520
To do so, they have to do certain things, right?

10:53.520 --> 10:55.520
They have to be able to sense the world,

10:55.520 --> 10:58.560
namely get information from the things around them,

10:58.560 --> 11:02.160
the things, the other animals, the sky, whatever.

11:02.160 --> 11:05.680
They need to process this information that's coming to them

11:05.680 --> 11:07.680
and remember some parts of it,

11:07.680 --> 11:11.120
those parts of it that help them predict what's going to happen in the future.

11:11.120 --> 11:13.880
And after doing this, they need to eventually make decisions,

11:13.880 --> 11:16.160
both in the short term and the long term,

11:16.160 --> 11:19.800
and be able to plan out sequences of actions for the future.

11:19.800 --> 11:24.840
So these are all things that an animal has to do just to be an animal.

11:24.840 --> 11:27.720
Now, some animals do additional things.

11:27.720 --> 11:28.720
They write poetry.

11:28.720 --> 11:34.160
Here's one of my favorite poems by the poet Gerard Manly Hopkins.

11:34.160 --> 11:35.160
They paint.

11:35.160 --> 11:38.760
This is a painting done by my daughter, Aruna.

11:38.760 --> 11:40.160
They write music.

11:40.160 --> 11:44.480
This is by Heinrich Injatz Franz von Bieber.

11:44.480 --> 11:46.600
I'm sure I pronounced that incorrectly.

11:46.600 --> 11:48.520
He was before the Baroque.

11:48.520 --> 11:52.560
And you know, some people write equations like a guy named Einstein

11:52.640 --> 11:54.960
and a guy named Schrödinger wrote this equation.

11:54.960 --> 11:57.840
So we theorize about the universe in this way.

11:57.840 --> 12:03.000
Not all of this, we being human, seems very impressive to us.

12:03.000 --> 12:05.240
And we're very interested in this.

12:05.240 --> 12:07.320
I'm very interested in this. You're all very interested in this.

12:07.320 --> 12:13.040
But in fact, the brain devotes very few resources to such activities.

12:13.040 --> 12:17.360
It's important to us, but really, mostly, that's not what the brain does.

12:17.360 --> 12:21.160
Mostly you, as an animal, like other animals,

12:21.160 --> 12:24.400
mostly you eat, navigate, explore, rest, manipulate the environment,

12:24.400 --> 12:26.760
interact socially with each other and reproduce.

12:26.760 --> 12:27.760
That's what you do.

12:27.760 --> 12:31.880
And then there's these additional things that we do on top of it.

12:31.880 --> 12:34.680
So I'm going to focus most of the talk today

12:34.680 --> 12:37.680
on these sorts of activities that all animals do.

12:37.680 --> 12:39.400
And then we'll talk a little bit at the end

12:39.400 --> 12:45.080
about these sorts of activities that interest us particularly as humans.

12:45.080 --> 12:48.440
Now, to do all of this, what do we use?

12:48.440 --> 12:51.160
The organ we use is the brain.

12:51.160 --> 12:55.760
And I'm going to think about the brain today as a sort of computing engine

12:55.760 --> 12:57.960
that produces the behavior of animals.

12:57.960 --> 13:00.960
So here's the human brain in particular.

13:00.960 --> 13:01.920
And what does it do?

13:01.920 --> 13:03.960
Well, it does the things that I said animals need to do,

13:03.960 --> 13:06.960
which is it gathers information from the world,

13:06.960 --> 13:09.360
it learns from experience, forms memories,

13:09.360 --> 13:10.880
and makes decisions for future actions.

13:10.880 --> 13:12.760
So that's the job of this organ.

13:12.760 --> 13:15.200
You know, the liver processes chemicals for you,

13:15.200 --> 13:17.200
and this organ and the heart pumps blood,

13:17.200 --> 13:19.840
this organ does this stuff with processing information.

13:19.840 --> 13:22.720
It's the kind of computer that you have in your head.

13:22.720 --> 13:24.760
Now, the challenge for this computer

13:24.760 --> 13:28.560
is that it's a very big, complex, and often unpredictable world.

13:28.560 --> 13:30.240
And you know, you have very limited resources.

13:30.240 --> 13:32.080
It's this thing that you carry around inside your head,

13:32.080 --> 13:34.080
it sloshes, it bumps, you know, stuff like this.

13:34.080 --> 13:35.760
That's what you get to use.

13:35.760 --> 13:38.840
And so myself, as a scientist in this area,

13:38.840 --> 13:42.800
the question that animates me is I'm interested in what are the organizational principles

13:42.800 --> 13:47.000
that allow the brain to meet such enormous challenges

13:47.000 --> 13:48.560
that animal life faces.

13:48.560 --> 13:50.240
So that's what I'm interested in.

13:50.240 --> 13:53.360
Now, I'm trained as a physicist,

13:53.360 --> 13:58.560
and a physicist posed to the question about how does this thing, this system, work?

13:58.560 --> 14:01.520
The instinct of most physicists is that, well, you know,

14:01.520 --> 14:07.080
what I'm going to do is I'm going to first work out what stuff this thing is made of.

14:07.080 --> 14:09.000
What are the parts that make it up?

14:09.000 --> 14:11.280
It's kind of an atomic instinct, you know?

14:11.280 --> 14:13.920
Figure out the parts that make up a system.

14:13.920 --> 14:17.560
Then you figure out how all the parts talk to each other, how they interact.

14:17.560 --> 14:19.440
And then the idea is, if you know that well enough,

14:19.440 --> 14:21.520
you can work out what the whole system is doing.

14:21.520 --> 14:25.240
That's the kind of method that physicists get trained in.

14:25.240 --> 14:27.960
And you know, it goes back a long way, hundreds and hundreds of years,

14:27.960 --> 14:32.800
and that's the tendency, that's the approach to the scientific question.

14:32.800 --> 14:34.600
So what's the brain made of?

14:34.600 --> 14:37.480
Well, that brings us to the neuron doctrine.

14:37.480 --> 14:43.040
This is Santiago Ramónica Hall, who lived from 1852 to 1934,

14:43.120 --> 14:47.560
and he is widely considered probably the founding figure of neuroscience.

14:47.560 --> 14:48.800
So what did he do?

14:48.800 --> 14:52.280
He was actually, he first wanted to be an artist, by the way.

14:52.280 --> 14:57.440
But then he wound up becoming an anatomist,

14:57.440 --> 15:01.560
and famously what he did is he founded the neuron doctrine.

15:01.560 --> 15:05.920
The neuron doctrine is the idea that the, if you like, atomic constituents of the brain,

15:05.920 --> 15:07.440
the thing that the brain is really made of,

15:07.440 --> 15:09.760
the stuff that does the interesting things in the brain,

15:09.760 --> 15:11.240
are the neurons of the brain.

15:11.240 --> 15:13.480
So here's a picture of a neuron.

15:13.480 --> 15:17.560
This is a so-called Purkinje cell in the cerebellum that's in the back over here.

15:17.560 --> 15:21.560
This is drawn by Ramónica Hall, and you know, he was a trained artist,

15:21.560 --> 15:23.720
so he drew really beautiful pictures.

15:23.720 --> 15:29.040
So his pictures to this day are wonderful artworks in their own right.

15:29.040 --> 15:31.280
Now he got the Nobel Prize in 1906,

15:31.280 --> 15:36.840
along with another scientist named Camilo Golgi for their work on anatomy.

15:36.840 --> 15:41.240
Golgi did not propound the neuron doctrine.

15:41.240 --> 15:44.160
The reason is they used different experimental methods,

15:44.160 --> 15:47.040
different stains to stain cells,

15:47.040 --> 15:50.240
and the nature of Golgi's stain was he had the impression

15:50.240 --> 15:52.720
that all the neurons were kind of connected together,

15:52.720 --> 15:53.840
and they formed a web work,

15:53.840 --> 15:56.840
and there was sort of one thing that made up the brain,

15:56.840 --> 16:01.680
and it was the very careful anatomy done by Ramónica Hall,

16:01.680 --> 16:04.720
which identified that no, no, no, there are many discreet,

16:04.720 --> 16:07.680
separate objects in the brain called neurons,

16:07.680 --> 16:11.120
and the neurons talk to each other in some form or fashion

16:11.120 --> 16:16.040
in order to produce all the effects that we call animal behavior.

16:16.040 --> 16:18.280
So let's talk about neurons some more.

16:18.280 --> 16:20.560
So you'll see that I'm not a trained artist,

16:20.560 --> 16:25.560
so you'll see my drawings do not compare to this in either precision or beauty,

16:25.560 --> 16:28.800
but let me explain what is a neuron.

16:28.800 --> 16:33.640
A neuron is a cell like any other cell in your body.

16:33.640 --> 16:36.040
So it has a cell body. Let me see, is that big enough?

16:36.040 --> 16:38.760
Can you see that? Can you see the words? OK, great.

16:38.760 --> 16:42.440
So, well, see the nice thing about iPads is you can increase it. See?

16:42.440 --> 16:46.120
Is that nice? OK, so if you can't see it, tell me and I'll just increase it.

16:46.120 --> 16:49.920
So a neuron is a cell like any other cell in your body

16:49.920 --> 16:54.560
and has a little membrane that contains within it all the molecular machines

16:54.560 --> 16:56.280
that make a cell work.

16:56.280 --> 17:01.240
But neurons are particular cells that are intended to help your brain compute,

17:01.240 --> 17:05.200
and as such, they have very specializations that allow them to do that.

17:05.200 --> 17:10.600
So in particular, a computing element like a transistor or a diode or a resistor

17:10.600 --> 17:12.880
needs to take in inputs from other stuff.

17:12.880 --> 17:15.240
So indeed, if you look at a neuron,

17:15.240 --> 17:19.360
it has this set of branches coming out from one end,

17:19.360 --> 17:22.040
which is called the dendritic arbor,

17:22.040 --> 17:24.960
and on the dendritic arbor, the dendrites,

17:24.960 --> 17:27.240
those are the little fibres at the end,

17:27.240 --> 17:31.040
and on the dendrites, there are the famous synapses that most people have heard of,

17:31.040 --> 17:35.640
the synapses are little junctions where one neuron makes a connection with another neuron.

17:35.640 --> 17:37.040
That's a chemical connection.

17:37.040 --> 17:40.680
One neuron would dump something called a neurotransmitter,

17:40.680 --> 17:45.560
and then sort of diffuses over to the other neuron, is eaten, taken up by the synapse,

17:45.560 --> 17:49.400
and then that leads to communication that we'll talk about later.

17:49.400 --> 17:54.000
Now the output, if you're a little computing device, you need to have an output,

17:54.000 --> 17:58.120
and the output of the neuron goes through a wire called the axon,

17:58.120 --> 18:00.920
and the axon then makes an axonal arbor,

18:00.920 --> 18:05.400
which also has some synapses at the tip that's where the neuron communicates the next neuron,

18:05.400 --> 18:07.640
whatever message it's going to send.

18:07.640 --> 18:11.400
So your brain, the human brain contains about 100 billion neurons,

18:11.400 --> 18:16.680
and it contains about 100 to 1,000 trillion synapses.

18:16.680 --> 18:18.720
So there's not just one kind of neuron.

18:18.720 --> 18:21.280
You know, if you go look inside a computer, a silicon computer,

18:21.280 --> 18:23.480
there are a few kinds of circuit elements, right?

18:23.480 --> 18:28.040
There are transistors, there are diodes, there are capacitors,

18:28.040 --> 18:33.760
there are resistors, there's a repertoire that engineers use

18:33.760 --> 18:36.640
to build up computers in general.

18:36.640 --> 18:42.200
So in fact, in the brain, there are very many types of neurons.

18:42.200 --> 18:45.960
And Cahal identified them by how they looked.

18:45.960 --> 18:50.680
And for example, here is what's called a cortical pyramidal cell,

18:50.680 --> 18:52.960
because it looks a little bit like a pyramid,

18:52.960 --> 18:56.120
and these pyramidal cells are the workhorses of your brain.

18:56.120 --> 18:59.160
All over the cortex, you find them layered upon layer

18:59.160 --> 19:01.600
of cortical pyramidal cells.

19:01.600 --> 19:05.040
This is this cerebellar purkinje cell that we mentioned.

19:05.040 --> 19:09.200
Here's this huge thing, it has this huge dendritic arbor,

19:09.200 --> 19:13.360
which it uses like a giant fan to catch inputs

19:13.360 --> 19:15.880
that are passing perpendicular to it.

19:15.880 --> 19:18.600
It does that as part of controlling your body

19:18.600 --> 19:23.320
and allowing you to move your body in ways that you want.

19:23.320 --> 19:27.080
And then in addition, here's another cell called a cortical stellate cell.

19:27.080 --> 19:31.720
So in fact, as we will see, there is a great diversity of forms of neurons.

19:31.720 --> 19:35.680
And usually, in biology, there's a rule, a working rule,

19:35.680 --> 19:38.120
which says that form follows function.

19:38.120 --> 19:41.520
So what's going to happen is that they look different like this versus this.

19:41.520 --> 19:42.680
They're going to do different things.

19:42.680 --> 19:44.680
They'll have different biochemistry,

19:44.680 --> 19:47.760
they'll perform different functions within this computer.

19:47.760 --> 19:50.960
So it's like you've got many circuit elements that are stuck together

19:50.960 --> 19:55.000
to produce your behavior.

19:55.000 --> 19:59.760
Now, the key thing for us is that neurons literally,

19:59.760 --> 20:03.840
like the components of a computer, are electrical circuit devices.

20:03.840 --> 20:05.680
They're electrical devices.

20:05.680 --> 20:07.080
So how does this work?

20:07.080 --> 20:10.560
So once again, here's a picture of a neuron.

20:10.560 --> 20:15.760
And if you go measure its voltage, literally, I mean its voltage, right,

20:15.760 --> 20:19.360
you'll find that its voltage inside a brain,

20:19.360 --> 20:23.240
the voltage will be something like minus 70 millivolts, typical neurons.

20:23.240 --> 20:26.640
They're like at rest, they're like at minus 70 millivolts.

20:26.640 --> 20:31.520
Then, if you have input coming into it, and the input could be signals,

20:31.520 --> 20:36.280
neurotransmitters, which I mentioned earlier, sent out by the previous neuron.

20:36.280 --> 20:41.000
Or if this is a neuron in the eye, it might be light landing on it

20:41.000 --> 20:43.840
because there's a little sensor that senses like.

20:43.840 --> 20:47.480
Or if it's in the nose, if it's an olfactory receptor neuron,

20:47.480 --> 20:51.320
it's a little molecule floating in the air which binds to a receptor

20:51.320 --> 20:53.640
and that produces a signal, you know, whichever you do.

20:53.640 --> 20:56.640
If a signal comes in, here's what happens.

20:56.640 --> 21:00.720
The voltage of the neuron begins to increase.

21:00.720 --> 21:04.360
And when it reaches some critical value, there's a sort of runaway feedback process.

21:04.360 --> 21:09.280
It's a little bit like feeding the output of your speaker back into the microphone.

21:09.280 --> 21:10.560
And so it goes round and round.

21:10.560 --> 21:15.280
It goes boom and sort of explodes rapidly to some very high potential.

21:15.280 --> 21:20.360
And then there's another feedback process, a negative feedback process, as they call it,

21:20.360 --> 21:24.680
which causes this to dip back down and flip back to rest.

21:24.680 --> 21:31.400
So if you step away and time a little bit, you'll find that it goes like this and then boom!

21:31.400 --> 21:34.160
There's a very sharp pulse of electricity.

21:34.160 --> 21:40.240
So if you sort of look above this level, it's like this silence and then boom!

21:40.240 --> 21:41.840
Silence, boom!

21:41.840 --> 21:44.240
So it's essentially a digital signal.

21:44.240 --> 21:48.400
It's like looking at a digital computer where you have this device going every once in a while,

21:48.400 --> 21:55.680
sending out ones, hold up these spikes or action potentials, or zeros, where it's silent.

21:55.680 --> 21:57.320
So it's very much like a digital computer.

21:57.320 --> 21:59.080
That's what you have in your head.

21:59.080 --> 22:06.280
So just to drive home that this is absolutely a mechanical, physical process,

22:06.280 --> 22:09.800
beautifully studied in biophysics, we can even delve in.

22:09.800 --> 22:14.080
You know, the instinct of many scientists is you see a phenomenon like this.

22:14.080 --> 22:16.600
You say, well, well, how does that exactly happen?

22:16.600 --> 22:19.880
You know, and you can work that out in complete detail.

22:19.880 --> 22:25.520
So for example, if you look at a membrane surrounding the cell, that's the membrane right there,

22:25.520 --> 22:31.520
you'll find embedded in this membrane various molecular machines,

22:31.520 --> 22:38.720
such as ion channels, which allow specific ions like sodium and potassium mentioned here,

22:38.720 --> 22:41.160
or chloride or calcium.

22:41.160 --> 22:44.800
They let certain ions pass back and forth.

22:44.800 --> 22:51.280
And then you have these things called pumps, which have the job of when the ions go back and forth,

22:51.280 --> 22:55.320
changing in this way the voltage of the neuron.

22:55.320 --> 22:57.160
Something has to be done to restore it, right?

22:57.160 --> 23:01.600
You send a lot of current back and forth, and the voltage changes, you've got to restore everything.

23:01.600 --> 23:04.240
And these pumps basically burn energy.

23:04.240 --> 23:12.360
They burn the gasoline of the cell, which is a molecule called adenazine triphosphate, ATP,

23:12.360 --> 23:17.280
and it burns ATP and goes chunk, chunk, chunk, chunk, chunk for a while,

23:17.280 --> 23:23.160
and then sort of restores the levels of ions inside and outside.

23:23.160 --> 23:27.960
So at rest there's a certain proportion of sodium and potassium inside,

23:27.960 --> 23:31.360
and a certain proportion of sodium and potassium outside,

23:31.360 --> 23:34.400
and then you have the kinds of processes physicists talk about.

23:34.400 --> 23:39.040
Stuff, you know, it's like if you have more salt outside a membrane than inside,

23:39.040 --> 23:40.840
you know how the salt comes in, right?

23:40.840 --> 23:45.680
In the same way, if there's more potassium sodium outside than inside,

23:45.680 --> 23:49.440
the sodium wants to come in, and the potassium, because there's more inside than outside,

23:49.440 --> 23:51.680
wants to go out so that these processes like this,

23:51.680 --> 23:53.760
basically stuff you can understand perfectly well,

23:53.760 --> 23:55.640
and then these pumps are working against it,

23:55.640 --> 23:58.280
and then what happens is that you have external inputs,

23:58.280 --> 24:02.240
but change how open or closed these channels are

24:02.240 --> 24:06.120
to allow the ions to pass before through them, and that's it.

24:06.120 --> 24:09.440
It's a completely, I mean, just like a mechanical device,

24:09.440 --> 24:13.280
with stuff flowing back and forth, and you can work this out in detail

24:13.280 --> 24:17.440
and show that if you have a system like this, it'll produce these voltage spikes,

24:17.440 --> 24:21.680
and then if you make a voltage spike over there, it'll then propagate down the axon.

24:21.680 --> 24:25.080
It's literally making, you give it the correct inputs,

24:25.160 --> 24:28.960
it's going to make a digital pulse, and the pulse is going to move down.

24:28.960 --> 24:34.160
In fact, this is so well understood that a Nobel Prize went to it.

24:34.160 --> 24:38.280
So Hodgkin and Huxley, in 1963, worked all of this out,

24:38.280 --> 24:41.280
and here's a set of equations describing their model of the salt.

24:41.280 --> 24:43.000
You don't have to read the equations.

24:43.000 --> 24:46.480
I just want to show you, you can write it on a damn page, right?

24:46.480 --> 24:49.720
This means you can put it on a computer, and you can certainly simulate it.

24:49.720 --> 24:54.840
You can totally describe how a single cell describes it in complete and utter detail.

24:54.840 --> 24:56.960
It's actually a little bit more complicated than this,

24:56.960 --> 25:01.560
because for the purposes of this talk, I'm talking about sodium and potassium and things like that,

25:01.560 --> 25:06.000
but there's more things, this chloride and there's calcium,

25:06.000 --> 25:11.160
and actually there's more than one kind of ion channel for potassium, some details, details.

25:11.160 --> 25:13.840
But basically, you know exactly how to do this,

25:13.840 --> 25:16.040
so in principle, you could just build yourself a brain.

25:16.040 --> 25:21.320
If you knew enough about all the other neurons and all the types of neurons and all these problems,

25:21.320 --> 25:23.520
you could just build yourself a brain on a computer.

25:23.520 --> 25:30.280
That's the promise of this kind of result that won this Nobel Prize, right?

25:30.280 --> 25:32.280
But what's the problem, right?

25:32.280 --> 25:36.440
The problem is, oh wait, actually before I tell you the problem,

25:36.440 --> 25:38.720
let me just sort of say a word about this.

25:38.720 --> 25:42.560
So this idea that the brain really just functions in electricity,

25:42.560 --> 25:47.600
just an electrical device, a computer of some kind, goes back a long, long way.

25:47.600 --> 25:53.240
And the discovery that somehow there's an electrical basis to animal behaviour

25:53.240 --> 25:57.880
goes back all the way, I don't know why I said this is a 19th century idea,

25:57.880 --> 26:01.400
it's even an 18th century idea, it goes back all the way to the 18th century,

26:01.400 --> 26:06.920
with the discovery by Luigi and Luisa Galvani, we're in Bologna,

26:06.920 --> 26:11.760
that you can make frogs' legs, for example, twitch, by putting electricity into them.

26:11.760 --> 26:17.640
So at the time, Luigi got most of the credit because Luisa being a woman,

26:17.640 --> 26:22.000
couldn't be a professor in the university and couldn't also take credit for the discoveries,

26:22.000 --> 26:27.000
but well, you know, times have improved a bit, they're perhaps not completely.

26:27.000 --> 26:31.960
Anyway, so they were the ones who discovered this notion of animal electricity.

26:31.960 --> 26:35.080
Then immediately after that, this is the time when Benjamin Franklin,

26:35.080 --> 26:38.560
you know, is discovering positive and negative charges, flying his kite,

26:38.560 --> 26:41.440
all this kind of stuff, that's all happening at the same time.

26:41.440 --> 26:45.080
And then Volta, also in Italy, invented the battery,

26:45.080 --> 26:49.160
which then allowed controlled injection of currents into living tissues,

26:49.160 --> 26:51.400
and then you could see how they moved and people had this idea

26:51.400 --> 26:55.600
that somehow electricity itself was the vital spark of life.

26:55.600 --> 26:59.480
It's not that it made neurons work, because they didn't know about neurons really,

26:59.480 --> 27:00.680
but they thought it was life itself.

27:00.680 --> 27:04.600
So for example, so they thought, well, maybe we can reanimate the dead.

27:04.600 --> 27:10.240
So Aldini reanimated a criminal who was executed using electricity,

27:10.240 --> 27:13.680
and then it was done again in 1818 with another criminal,

27:13.680 --> 27:17.720
and in those days there were laws that allowed you to experiment

27:17.800 --> 27:22.720
with the dead bodies of, you know, of murderers and things like that.

27:22.720 --> 27:26.680
Anyhow, this led, for example, Mary Shelley to write Frankenstein,

27:26.680 --> 27:32.640
because it led to the idea that life and living things and animals could be just built,

27:32.640 --> 27:35.680
that they're sort of machines that could be built,

27:35.680 --> 27:37.720
and that's what's partly animating us still,

27:37.720 --> 27:42.280
this notion of artificial life and artificial intelligence goes back,

27:42.280 --> 27:47.200
excuse me, goes back to these discoveries, you know, hundreds of years ago.

27:47.200 --> 27:51.400
So now, of course, we know that electricity, the shocking dead body,

27:51.400 --> 27:53.520
doesn't make it alive again.

27:53.520 --> 27:56.320
What it really does is following what I told you a moment ago,

27:56.320 --> 27:59.880
it causes the nerve cells to fire,

27:59.880 --> 28:03.000
and when they fire, they cause muscles to move,

28:03.000 --> 28:08.320
and then the body will move in the way it looks as though you reanimated the dead,

28:08.320 --> 28:09.920
but you haven't really.

28:09.920 --> 28:14.000
Anyway, so suppose we know how a single neuron works in all of its detail,

28:14.000 --> 28:16.920
then what's the problem with understanding the rest of animal behavior,

28:16.920 --> 28:18.240
just putting enough neurons together?

28:18.240 --> 28:25.240
Well, the problem is, you know, a single neuron can't produce a poetry,

28:25.240 --> 28:28.440
a single neuron doesn't feel love, you know, things like this, right?

28:28.440 --> 28:32.880
Really, neurons produce the brain through the circuits that they work in.

28:32.880 --> 28:39.080
It's really the interaction, the collective of all the neurons that makes the brain.

28:39.080 --> 28:41.040
So what do these circuits look like?

28:41.040 --> 28:46.640
So perhaps the best-studied circuit in the entire brain of any animal

28:46.640 --> 28:48.360
is the retina.

28:48.360 --> 28:56.120
So this is a drawing done, again, by Ramoni Kahal, back in 1917,

28:56.120 --> 29:00.160
and what it's drawing is a picture of the retina like this.

29:00.160 --> 29:04.880
Over here, these are the famous photoreceptors that many of you will have heard of.

29:04.880 --> 29:10.200
Photoreceptors are neurons which take in light and convert the light

29:10.200 --> 29:12.960
into an electrical signal that comes out the other end.

29:12.960 --> 29:16.480
So that's the first layer of the retina.

29:16.480 --> 29:20.840
Now, the second layer of the retina consists of cells called bipolar cells.

29:20.840 --> 29:24.560
These are like analogue computing devices, namely they're not digital.

29:24.560 --> 29:26.640
They have voltage levels that go up and down,

29:26.640 --> 29:29.000
and they're all continuous, it isn't 1s and 0s if you like,

29:29.000 --> 29:33.880
but they do all kinds of calculations and computations which are studied.

29:33.880 --> 29:38.000
Then these things form the second layer, feed into a third layer,

29:38.000 --> 29:41.760
and the third layer are cells called ganglion cells,

29:41.760 --> 29:46.440
and these ganglion cells are the output cells of the retina, right?

29:46.440 --> 29:49.880
And these output cells, you know, the axons come out,

29:49.880 --> 29:55.000
and the bundle of axons is a thing you call your optic nerve, right?

29:55.000 --> 30:00.280
So very often people have a tendency to think about the retina like a camera.

30:00.280 --> 30:03.400
It just takes the pixels and writes down the amount of light in each location.

30:03.400 --> 30:04.840
But nothing could be further from the truth.

30:04.840 --> 30:09.080
This is your primordial example of what would be called a three-layer neural network.

30:09.080 --> 30:12.800
It's a piece of your central brain that got put on a stalk

30:12.800 --> 30:16.400
and then sent out to the front of your head during development,

30:16.400 --> 30:17.880
during embryonic development,

30:17.880 --> 30:20.800
so that you could see in the direction in which you're going.

30:20.800 --> 30:22.800
It's really a piece of your central brain.

30:22.800 --> 30:26.800
And indeed, in keeping with that, central brain is a very complicated place.

30:26.800 --> 30:29.400
This picture drawn by Cajal is actually a cartoon.

30:29.400 --> 30:33.080
We now know that there are more than 60 kinds of cells,

30:33.080 --> 30:35.000
different types of cells,

30:35.000 --> 30:39.560
just so different computational elements in the retina alone.

30:39.560 --> 30:42.480
So, for example, if these are the photoreceptors,

30:42.480 --> 30:45.760
and then these are the bipolar cells described in the second layer,

30:45.760 --> 30:48.920
and then these are the ganglion cells described in the third layer,

30:48.920 --> 30:55.760
in fact, in between, there is an entire zoo of other so-called interneurons

30:55.760 --> 30:58.400
that sort of communicate laterally.

30:58.400 --> 31:01.600
And what they do is the following.

31:01.600 --> 31:05.360
What they do is, you know, most of the light coming to your eyes

31:05.360 --> 31:07.640
is not actually useful to your behaviour.

31:07.640 --> 31:08.880
There's only certain things that are useful.

31:08.880 --> 31:11.560
And indeed, the ganglion cells, what they do is,

31:11.560 --> 31:13.200
they report the things that are useful.

31:13.200 --> 31:15.680
Like, there are ganglion cells called on cells

31:15.680 --> 31:20.400
that respond and tell you when there's a bright spot in the world at some location.

31:20.400 --> 31:22.880
There are off cells that tell you about dark spots.

31:22.880 --> 31:26.600
There are so-called local edge detectors that detect local edges.

31:26.600 --> 31:28.520
There are direction-selective cells

31:28.520 --> 31:33.000
that will respond when a little blob moves left to right or right to left or up or down.

31:33.000 --> 31:35.520
You know, there are all of these cells, these different kinds.

31:35.520 --> 31:40.520
So how do you extract these so-called visual features from the scene?

31:40.520 --> 31:50.360
Well, this hugely diverse body of neurons is charged with removing from the visual input

31:50.360 --> 31:53.800
everything that's irrelevant and leaving what is relevant.

31:53.800 --> 31:58.440
So famously, for example, in analogy, someone once asked Michelangelo,

31:58.440 --> 31:59.880
how did you sculpt these sculptures?

31:59.880 --> 32:01.160
David.

32:01.160 --> 32:03.920
And he said, well, actually, David was already in the marble.

32:03.920 --> 32:06.280
I just took out everything else.

32:06.280 --> 32:07.760
And so that's what these things are doing.

32:07.760 --> 32:12.600
They're taking out everything else and leaving the important stuff to go through to the brain.

32:12.600 --> 32:13.600
So this is how it's organized.

32:13.600 --> 32:17.920
So you can see it's a large collective effort of many, many types of neurons

32:17.920 --> 32:20.160
arranged in a very specific circuit.

32:20.160 --> 32:24.800
For example, you will always find that the slow bipolar cell here connects to the local edge cell

32:24.800 --> 32:28.120
and the fast bipolar cell here connects to the brisk transient cell.

32:28.120 --> 32:30.800
So it's like a circuit that Intel would design.

32:30.800 --> 32:32.200
You know, the engineers designed something.

32:32.200 --> 32:33.240
This isn't designed.

32:33.240 --> 32:35.160
It's selected out by evolution.

32:35.160 --> 32:36.400
But anyway, there you go.

32:36.400 --> 32:41.120
It's like a circuit that you will find in every vertebrate eye.

32:41.120 --> 32:49.880
Now, even that is not actually a sufficient picture of the degree of cooperativity in the brain

32:49.880 --> 32:51.640
that produces all of the behavior.

32:51.640 --> 32:56.640
So if you pop out from the scale of neurons and instead look at the whole brain,

32:56.640 --> 32:59.720
here is your brain, you know, looking this away.

32:59.720 --> 33:05.840
And in the back of your head, there are circuits that are associated with vision.

33:05.880 --> 33:11.080
Over here, there's a strip going down the side of your head that helps you control your body.

33:11.080 --> 33:18.520
Over here, there's an area called Broca's area, which is associated with the production of speech

33:18.520 --> 33:22.160
and, you know, many other areas of this kind, right, the hearing, et cetera.

33:22.160 --> 33:27.440
And in the front here, there's regions of your brain that are associated with planning

33:27.440 --> 33:31.840
and personality and things of this kind, right?

33:31.840 --> 33:36.640
So this is why, by the way, you know, decades and decades ago,

33:36.640 --> 33:43.640
there was a treatment for schizophrenia, which involved basically sticking an ice pick up your nose

33:43.640 --> 33:49.400
and scrambling this area because there was some issue with it.

33:49.400 --> 33:51.880
And the result is, you know, the person won't be schizophrenic,

33:51.880 --> 33:55.600
but they won't be themselves anymore because you scramble the circuits that make you.

33:55.600 --> 34:02.160
It really should be very careful with any kind of treatment that involves sort of messing with bits of the brain.

34:02.160 --> 34:07.240
OK, so anyway, so there are all of these things, and we know that function is localised in this way,

34:07.240 --> 34:09.680
that this is called localisation of function for many reasons.

34:09.680 --> 34:14.080
So one reason is, you know, for example, if you bash the back of your head, you fall backward,

34:14.080 --> 34:16.800
you know how in the cartoons you see stars, right?

34:16.800 --> 34:20.760
You know, Bugs Bunny sees stars when he bashes the back of his head.

34:20.760 --> 34:25.520
It's because Bugs Bunny has bruise on this part of the brain.

34:25.520 --> 34:28.600
So the circuits aren't working quite right, that's what's going on.

34:28.600 --> 34:33.200
So when someone has a stroke, sometimes you'll see one side of the face sag,

34:33.200 --> 34:34.960
but everything else is completely fine.

34:34.960 --> 34:38.440
That's because there is an issue, there's a problem,

34:38.440 --> 34:42.160
let's say in this region of the brain, this is the motor cortex,

34:42.160 --> 34:45.640
and within the motor cortex there are circuits here that control the knee

34:45.640 --> 34:47.760
and circuits here that control the face.

34:47.760 --> 34:51.080
So if you have a stroke there, this thing sags, but the rest is fine.

34:51.080 --> 34:54.280
But it's all collective, right? There's a collective circuit,

34:54.280 --> 34:57.400
which of course has little areas that concentrate on different things,

34:57.400 --> 34:59.400
but it's a collective effect.

34:59.400 --> 35:01.760
And this is not just on the surface of the brain, by the way.

35:01.760 --> 35:06.440
So deep in the middle of the brain, there's an area there called a hippocampal formation,

35:06.440 --> 35:09.320
which is associated with, for example, memory, right?

35:09.320 --> 35:14.160
So if you have damage there, you will not be able to form long-term memories, right?

35:14.160 --> 35:16.560
And it's also associated with spatial cognition,

35:16.560 --> 35:20.080
with spatial, you know, animals need to map space

35:20.080 --> 35:22.240
and be able to make plans and navigate.

35:22.320 --> 35:25.080
And so all of those circuits are hidden here.

35:25.080 --> 35:25.960
Okay.

35:25.960 --> 35:30.240
So actually, human beings have known about this kind of thing,

35:30.240 --> 35:35.160
that there's function localized different parts of the brain for a very long time.

35:35.160 --> 35:38.240
This is a picture of what's now called a Smith papyrus after Smith,

35:38.240 --> 35:42.400
who bought the papyrus in the late 1800s.

35:42.400 --> 35:46.240
This was a papyrus written in 3000 BC.

35:46.240 --> 35:51.840
And the author of this papyrus describes various kinds of traumatic brain injuries,

35:51.840 --> 35:56.640
and then explains that, you know, something will stop working, but everything else works.

35:56.640 --> 36:02.240
So this author knew that function was localized in the various parts of the brain.

36:02.240 --> 36:07.400
The modern version of this idea is due to Paul Broca, a French doctor,

36:07.400 --> 36:13.320
who basically discovered Broca's area because he had patients who had impairments in their speech,

36:13.320 --> 36:19.880
and he discovered they had lesions in this area of his brain, of their brains,

36:19.880 --> 36:22.040
and there's another area called Wernicke's area,

36:22.040 --> 36:25.960
which is associated, but later discovered, having to do with speech understanding.

36:25.960 --> 36:30.000
So if you go to Paris, you can go to the 13th arrondissement,

36:30.000 --> 36:34.960
and, you know, admire Rue Broca, a name for him for all of his great discoveries.

36:34.960 --> 36:40.000
There you go. There's the picture of the street sign of Rue Broca.

36:40.000 --> 36:45.600
All you need to do to get a street sign is discover localization of function in your brain.

36:45.600 --> 36:49.880
OK. Very good.

36:49.880 --> 36:57.920
So now that's also not enough in describing how complicated this processing machine is.

36:57.920 --> 37:04.080
You know, you all know, if you ever looked at a wiring diagram of something like a chip that powers your computer,

37:04.080 --> 37:06.800
you know, it's got lots and lots of pieces with intricate circuits,

37:06.800 --> 37:08.600
and all those pieces connect to every other piece.

37:08.600 --> 37:12.480
Otherwise, of course, they couldn't produce their collective effects.

37:12.480 --> 37:15.600
And indeed, all of these brain areas connect.

37:15.600 --> 37:22.240
So today, in the 21st century, there are all of these extraordinarily beautiful techniques now

37:22.240 --> 37:24.160
for tracing out the wires.

37:24.160 --> 37:26.720
It used to be that people like Cajal, you know,

37:26.720 --> 37:30.280
they had to take the brain of an animal, slowly slice it up,

37:30.280 --> 37:34.520
very, very carefully trace everything, draw it out by hand.

37:34.520 --> 37:40.960
You know, you can imagine the level of dedication and sheer labor it took.

37:40.960 --> 37:44.320
Now, we have all these amazing techniques.

37:44.320 --> 37:49.600
You can do things like you can make neurons glow in different colors

37:49.600 --> 37:53.640
and get the color to propagate along the axon, you know, the output of the wire,

37:53.640 --> 37:56.120
so you can see where it's pointing to.

37:56.120 --> 37:58.480
You can inject viruses into bits of brain.

37:58.480 --> 38:03.600
The virus will go backward along where, you know, the neuron connects to.

38:03.600 --> 38:06.360
And then you can find out which neuron, one neuron down.

38:06.360 --> 38:08.800
You can tag it in that way, and then it will stop working.

38:08.800 --> 38:10.400
The virus stops working at the point.

38:10.400 --> 38:12.040
You can engineer all these things.

38:12.040 --> 38:17.160
So, in this way, we now have far more detailed maps of the brain.

38:17.160 --> 38:20.080
Here's a picture from a paper by these authors.

38:20.080 --> 38:25.960
It was a review paper showing some of the nerve tracts, as they're called,

38:25.960 --> 38:28.200
the wires that pass between.

38:28.200 --> 38:31.920
And now, because of that, we just have a much more refined understanding

38:31.920 --> 38:36.840
of how collectively all these bits of brain produce functions that we care about.

38:36.840 --> 38:40.400
So, for example, following what I said a moment ago,

38:40.400 --> 38:43.960
in Broca's era, or shortly thereafter, we might have said,

38:43.960 --> 38:46.520
well, you know, here's Broca's area,

38:46.520 --> 38:50.480
which, you know, somehow is responsible for articulation, production of speech.

38:50.480 --> 38:55.200
Here's Borneke's area that is somehow associated with comprehension of speech.

38:55.200 --> 38:56.960
Let me explain the difference between those two.

38:56.960 --> 38:59.880
If you damage Borneke's area, you'll produce speech.

38:59.880 --> 39:02.080
It just won't make any sense.

39:02.080 --> 39:06.760
And if you damage Broca's area, you can understand speech perfectly well.

39:06.760 --> 39:10.600
People can talk to you, but you won't be able to make it.

39:10.600 --> 39:16.120
You can't control your muscles to make the speech, stuff like this.

39:16.120 --> 39:18.440
There's also another area, Gershwin's area for concepts.

39:18.440 --> 39:20.080
You damage that, you have problems with concepts.

39:20.080 --> 39:23.280
Anyway, so there are these areas that people identified.

39:23.280 --> 39:30.440
So a more refined model you might derive by looking at the way the nerve tracts go

39:30.440 --> 39:33.160
is that there's clearly a pathway from here to here,

39:33.160 --> 39:35.800
so from the comprehension area to the production area,

39:35.800 --> 39:38.840
and there's a secondary path, you know, there are two pathways like that.

39:38.840 --> 39:41.000
But now we have even more fancy techniques.

39:41.000 --> 39:44.280
You can put a person in an fMRI machine, right?

39:44.280 --> 39:49.480
Functional Magnetic Resonance Imaging Machine, and have them do stuff.

39:49.480 --> 39:53.480
And while they're doing stuff, you can record, you can find a way

39:53.480 --> 39:56.800
of measuring which areas of the brain are active.

39:56.800 --> 40:01.080
So which areas of the brain are active together when you do different things.

40:01.080 --> 40:04.400
And now you get a much more refined picture that all of these areas are there.

40:04.400 --> 40:08.760
It's true that they're associated with articulation concepts and comprehension,

40:08.760 --> 40:14.040
but depending upon exactly what you're trying to do, they connect in different ways.

40:14.040 --> 40:17.600
So there's a sort of flexible computational engine

40:17.600 --> 40:23.840
wherein, you know, the different bits of it work together for one task or not,

40:23.840 --> 40:26.160
depending upon what's necessary to do the task.

40:26.160 --> 40:31.440
Is this amazingly flexible engine inside your head for producing all of animal behaviour?

40:31.440 --> 40:35.280
So that's the task to work out how all of these parts connect

40:35.280 --> 40:40.000
and how they reconnect and reorganise themselves when they need to do things.

40:40.000 --> 40:41.160
OK.

40:41.160 --> 40:45.400
So myself, my interest is to understand how all of that happens.

40:45.400 --> 40:47.120
I think of the brain as a computing device.

40:47.120 --> 40:50.440
I want to know how this computer works.

40:50.440 --> 40:53.360
So there are many ways of approaching this,

40:53.360 --> 40:57.320
but one way of doing that is, well, so the way I do that,

40:57.400 --> 41:03.240
is I take a view of the circuit repertoire of the architecture of this computer

41:03.240 --> 41:09.360
as a kind of memory of the computations that have predictive value for your behaviour,

41:09.360 --> 41:11.840
because, you know, animal life is about predicting what's going to happen

41:11.840 --> 41:13.640
and taking action appropriately,

41:13.640 --> 41:20.280
and that these computations have been learned over evolutionary time

41:20.280 --> 41:25.240
and then encoded in your genome and in the developmental programme.

41:25.240 --> 41:26.560
So that's the way I tend to think of it.

41:26.560 --> 41:29.640
These are computations that are useful to you,

41:29.640 --> 41:33.120
and then what you do is something, and then you use them.

41:33.120 --> 41:38.040
So the question that animates my work is what are the organisational principles or laws, if you like,

41:38.040 --> 41:45.280
that control the collective computation and information processing of the brain.

41:45.280 --> 41:46.640
So, OK.

41:46.640 --> 41:51.760
And, you know, if you're interested, there's a popular book by Peter Sterling and Simon Loughlin,

41:51.760 --> 41:54.680
this book, your principles of neural design, that you'll probably enjoy,

41:54.680 --> 41:58.640
which lays out many possible principles that may be operative

41:58.640 --> 42:03.480
in the organisation of the circuits in the brain.

42:03.480 --> 42:06.000
Now, so what principles might be relevant?

42:06.000 --> 42:07.960
What kinds of laws or principles?

42:07.960 --> 42:10.000
So there are several that you could try to name,

42:10.000 --> 42:12.400
but for today's purposes I'm going to name two,

42:12.400 --> 42:17.840
and then I'm going to illustrate them in the operation of some of these circuits.

42:17.840 --> 42:23.960
So, for example, one principle involves the costs of computation.

42:23.960 --> 42:28.120
So this is your brain, and this is your laptop,

42:28.120 --> 42:32.200
and, you know, your brain is only 2% of your body weight,

42:32.200 --> 42:35.920
but it's actually 20% of your metabolic load.

42:35.920 --> 42:40.880
What that means is it's 10 times more expensive than muscle.

42:40.880 --> 42:46.360
This is a seriously expensive thing to own, right?

42:46.360 --> 42:50.800
And, you know, it's also packed solid, right?

42:50.880 --> 42:54.440
Every millimeter cubed contains four kilometres of wire.

42:54.440 --> 42:57.240
This is like this really dense thing, right?

42:57.240 --> 43:00.360
So on the one hand this sounds like, wow, you know, that's a really expensive thing.

43:00.360 --> 43:07.480
On the other hand, you know, your brain consumes something like 12 to 20 watts of power.

43:07.480 --> 43:11.840
Your laptop consumes 80 watts of power, right?

43:11.840 --> 43:15.600
And it sure can multiply fast, but, you know, it can't give the stock.

43:15.600 --> 43:17.360
Well, at least not yet.

43:17.680 --> 43:21.680
But even things like, by the way, you know, AI seems to have the promise of, you know,

43:21.680 --> 43:23.960
getting, making machines that'll do me,

43:23.960 --> 43:28.520
but actually they have to train their machines on so much data.

43:28.520 --> 43:31.880
You know, planetary, a planet's worth of data,

43:31.880 --> 43:36.080
they use a city's worth of power to train these things, right?

43:36.080 --> 43:38.440
Whereas I can learn stuff with one example, right?

43:38.440 --> 43:41.480
I don't need the city's worth of, you know, the planet's worth of data.

43:41.480 --> 43:44.280
So there's a real difference in the way the brain works.

43:44.280 --> 43:47.800
It's just much more efficient, right, in the way it operates

43:47.800 --> 43:50.040
than your typical silicon machine.

43:50.040 --> 43:52.920
That's also one reason why computer scientists are interested in this kind of thing

43:52.920 --> 43:55.320
because they'd like to make more efficient machines.

43:55.320 --> 44:00.320
So you could ask yourself, how does the brain achieve this sort of efficiency

44:00.320 --> 44:04.560
relative to the engineered systems that we have been able to produce so far, right?

44:04.560 --> 44:09.920
I've described this baroque architecture with lots and lots of pieces that interact in some way.

44:09.920 --> 44:16.640
So one idea that's very powerful and has been used powerfully in neuroscience

44:16.640 --> 44:24.000
is that the brain achieves its efficiency by adapting its circuits to the structure of the world

44:24.000 --> 44:28.960
and then using learning and self-organisation to further adapt the changes, right?

44:28.960 --> 44:32.240
You know, you first adapt to the world and then if the world is not quite what you expected,

44:32.240 --> 44:35.880
you change the circuit so that it's well adapted to the world.

44:35.880 --> 44:38.880
So why would this make the system more efficient?

44:38.880 --> 44:40.920
Well, the idea is roughly like this.

44:40.920 --> 44:46.240
So if you talk to sociologists or something, they'll tell you that in the back of the day,

44:46.240 --> 44:51.720
we were all hunter-gatherers, we all dug the ground, we hunted the animals, we built the house,

44:51.720 --> 44:54.920
you know, whatever, we did everything and society got along.

44:54.920 --> 44:59.920
And then as society's progress, the civilisations developed, et cetera,

44:59.920 --> 45:02.120
you know, there's a segregation of function.

45:02.120 --> 45:06.480
What happens is that the shoemaker makes the shoes, the baker breaks the bread,

45:06.480 --> 45:12.760
you know, the mason builds the houses and then each unit in the society develops efficiencies.

45:12.760 --> 45:16.760
They do their job really well because they're well trained, they're adapted to it, et cetera, et cetera,

45:16.760 --> 45:20.080
and the whole system, by communicating between all the individuals,

45:20.080 --> 45:24.840
works better, works smoother, it's more productive, et cetera, and it's more efficient.

45:24.840 --> 45:31.000
So you could try to apply a similar idea to the organisation of these circuits in our head

45:31.000 --> 45:32.440
which have evolved over time.

45:32.440 --> 45:36.960
So that's a very important idea in neuroscience.

45:36.960 --> 45:39.800
So that's principle one, the efficient use of resources.

45:39.800 --> 45:46.200
So the idea of this principle is that brains exploit the average structure of the world

45:46.200 --> 45:51.960
to efficiently allocate their limited resources for the tasks that they have to do

45:51.960 --> 45:55.280
to maximise gain for the organism.

45:55.280 --> 45:58.800
Now if I were to draw a cartoon of something like that, it might be something like this.

45:58.880 --> 46:03.040
Remember how in the retina we said, well, you know, there's all the light coming in,

46:03.040 --> 46:07.800
but at the back end of the eye, you have a certain number of different neurons, right?

46:07.800 --> 46:11.040
There are 20 types of ganglion cells, we call them,

46:11.040 --> 46:13.800
but you know, they're each charged with doing a thing, right?

46:13.800 --> 46:16.840
One thing will look for bright spots, that's the on cell.

46:16.840 --> 46:21.160
The one thing that's for dark spots is the off cell, you know, and so on and so forth.

46:21.160 --> 46:24.880
There's a certain repertoire of things that are apparently needed for vision,

46:24.880 --> 46:29.120
so you make those things, and then each of those things is one of these blobs,

46:29.120 --> 46:35.240
and together they cover the space of those aspects of the visual world that you need for your behaviour.

46:35.240 --> 46:39.120
And they have to allocate those resources in an effective way.

46:39.120 --> 46:42.800
And you could keep doing this with a sense of smell, with your sense of place,

46:42.800 --> 46:46.200
you know, the whole brain, and you could try thinking in this way.

46:46.200 --> 46:51.480
So this is a very important principle that's used often by neuroscientists

46:51.480 --> 46:55.520
to try to understand the architecture of circuits in the brain.

46:55.520 --> 46:59.560
Another important principle is this one, it's learning and self-organisation.

46:59.560 --> 47:02.720
These individuals whom I know well did a lot of that,

47:02.720 --> 47:06.160
and so what do they do, so that's far as I can tell,

47:06.160 --> 47:12.800
their little heads self-organise a lot of the architecture within it through dynamics and learning,

47:12.800 --> 47:14.000
and what do they do?

47:14.000 --> 47:19.760
Well, this learning and self-organisation adapts the brain

47:19.840 --> 47:24.440
to ongoing variations in the world, you know, their world is different from the world I was growing up in.

47:24.440 --> 47:27.160
It allows them to learn new tasks and environments at this age,

47:27.160 --> 47:29.320
they definitely didn't know how to ride bikes,

47:29.320 --> 47:33.360
and you can correct inaccuracies in pre-existing dynamics.

47:33.360 --> 47:35.920
Anybody who's interacted with small children knows perfectly well

47:35.920 --> 47:39.600
that, you know, when the baby first tries to reach for stuff, it doesn't quite, you know, it doesn't get there, right?

47:39.600 --> 47:41.560
You can't quite do it.

47:41.560 --> 47:46.400
And then they learn how to control their muscles to get there, to reach it.

47:46.400 --> 47:48.920
Or you watch a kid learning a musical instrument.

47:48.920 --> 47:55.160
Well, you know, the first, you know, 500 times doesn't sound so good.

47:55.160 --> 48:01.640
And then eventually it does, because there's something that happens where you learn how to operate the bits of you to do the right thing.

48:01.640 --> 48:05.960
So that's the learning that has to happen in a variable world.

48:05.960 --> 48:11.640
OK, so what I would like to do in the remaining, what, 15, 20 minutes, 20 minutes or so,

48:11.640 --> 48:17.680
is that I would like to give you examples of how in the field people use these kinds of ideas

48:17.680 --> 48:20.840
to understand the organization of neural circuits.

48:20.840 --> 48:26.240
I mean, so far I gave you a description of the architecture, right, of these circuits,

48:26.240 --> 48:31.440
from the neuron level, you know, the atomic constituent of the brain, all the way to the whole brain, right?

48:31.440 --> 48:38.760
But in science, we want more than that, we want an understanding of why they're organized the way they are.

48:38.760 --> 48:43.840
How they produce the dynamics and effects that they do.

48:43.840 --> 48:52.080
So there is an enormous amount of interesting work that has happened since the era of Cahal in these directions.

48:52.080 --> 48:56.960
So I want to give you a little flavor of some of it by giving you a few examples from my work.

48:56.960 --> 49:00.520
I'm going to pick simple examples because, you know, there's 20 minutes left.

49:00.520 --> 49:04.240
So we should go, so it has to be simple and precise enough.

49:04.240 --> 49:08.520
So I'm going to give you examples in four domains, right?

49:08.520 --> 49:11.960
One domain is seeing, vision.

49:11.960 --> 49:16.400
The second domain is smelling, or factions. These are both sensory domains.

49:16.400 --> 49:20.200
Then I want to talk about learning to sing. This is in songbirds.

49:20.200 --> 49:23.760
So that's what the professionals would call motor control,

49:23.760 --> 49:33.640
because you need to control all the muscles in the songbirds apparatus in order to produce the sounds.

49:33.640 --> 49:37.320
And then I'm going to talk very, very briefly about navigation,

49:37.320 --> 49:42.840
which is actually the subject that I spend probably most of my time on, thinking about right now.

49:42.840 --> 49:45.400
So I'll just say a little bit about that, and then we'll end.

49:45.400 --> 49:47.760
So I'm going to give you four examples of this guy.

49:47.760 --> 49:49.440
So let's start with one.

49:49.440 --> 49:55.480
So first I'm going to talk about vision, and my goal, by the way, what's my goal with these examples?

49:55.480 --> 49:59.560
You remember I talked about these two principles, adaptation to the world and learning,

49:59.560 --> 50:04.880
as two mechanisms that are used to, well, to organize neural circuits.

50:04.880 --> 50:07.680
You can understand why they're organized the way they are,

50:07.680 --> 50:12.880
but think about adaptation to the world, and about learning to reorganize.

50:12.880 --> 50:15.320
So I want to give you two examples of adaptation.

50:15.320 --> 50:20.720
Those will be the sensory examples, and an example of learning.

50:20.720 --> 50:23.040
So first let's talk about vision.

50:23.040 --> 50:24.760
So why did I choose vision?

50:24.760 --> 50:29.440
Well, we're very visual animals, and it's a sense we use most of,

50:29.440 --> 50:35.760
so that naturally neuroscientists gravitate often to thinking about the visual world.

50:35.760 --> 50:38.240
So I want to ask a very basic question.

50:38.240 --> 50:39.680
Here's the question.

50:39.680 --> 50:45.600
So we drew this, Ramoni Kahal drew these pictures with all of these different neurons,

50:45.600 --> 50:49.360
and then I showed you another picture, a more modern one, of all the cell types,

50:49.360 --> 50:51.080
and there are like 60 types of cells.

50:51.080 --> 50:53.120
God help us in this structure.

50:53.120 --> 50:54.400
Well, what did I do?

50:54.400 --> 50:56.720
Let me just look at the output layer for now.

50:56.720 --> 50:59.000
These were the so-called retinal ganglion cells,

50:59.000 --> 51:03.360
and I told you that these cells detect features, visual features of the world,

51:03.360 --> 51:06.200
bright spots, dark spots, color, local motion, et cetera,

51:06.200 --> 51:10.320
and then they put the signal, these digital signals that I talked about,

51:10.320 --> 51:16.000
go down the optic nerve and that little bundle of wires that's like an ethernet's cable worth of data,

51:16.000 --> 51:21.480
and that data goes to your brain and you do all the stuff that you do with vision.

51:21.480 --> 51:25.920
So following a kind of question that immediately comes to mind

51:26.000 --> 51:29.120
is you have a certain repertoire of these cells.

51:29.120 --> 51:31.400
Well, humans, we have about a million of them.

51:31.400 --> 51:34.920
If you're a guinea pig, you have about 100,000 of them.

51:34.920 --> 51:37.280
So, you know, it's not an infinite resource.

51:37.280 --> 51:42.040
So if you spent all of your money on detecting bright spots, there's nothing left.

51:42.040 --> 51:45.840
If you spent all of your cells on detecting bright spots very well,

51:45.840 --> 51:50.920
you will have nothing left for detecting motion, for example.

51:50.920 --> 51:56.760
So there has to be something interesting in the way this sensory device, right?

51:56.760 --> 52:01.160
This is like you have a robot and has little eyes sitting in front looking at stuff, right?

52:01.160 --> 52:03.760
So you've got to decide. How are you going to decide the design of that eye?

52:03.760 --> 52:06.400
It's only a big. It's got only some of the units in it.

52:06.400 --> 52:10.400
How are you going to allocate the different pieces of the circuit?

52:10.400 --> 52:16.840
So here, there should be some choice done by evolution for the allocation of these features.

52:16.840 --> 52:20.600
So for this talk, let's take the specific example

52:20.600 --> 52:24.800
because it's easy to imagine of bright and dark spot detectors.

52:24.800 --> 52:31.240
So bright spot means you look for a little bright spot relative to the near-surrounding region.

52:31.240 --> 52:36.600
A dark, so that's called an on-cell, and an off-cell does the reverse.

52:36.600 --> 52:41.600
It looks for a little dark spot relative to the near-surrounding region.

52:41.600 --> 52:46.920
So we're going to ask the question, well, suppose you have a certain number of spot detectors,

52:46.920 --> 52:48.920
how many on-and-off cells should you have?

52:48.920 --> 52:51.880
How many bright and dark spot detectors should you have?

52:51.880 --> 52:57.280
And we're going to try to use this to understand the architecture of the retina.

52:57.280 --> 53:00.280
Just incidentally, it's also important therapeutically, right?

53:00.280 --> 53:06.480
So if somebody's, you know, when the day comes when we can replace the retina,

53:06.480 --> 53:10.640
if somebody's retina degenerates and we want to replace it by a silicon device,

53:10.640 --> 53:14.160
it would be better if we could understand how many of these and how many of these detectors

53:14.160 --> 53:17.680
we should put in into that silicon device.

53:17.680 --> 53:25.960
Right, so if the idea, if this kind of organization involves adaptation to the world around us,

53:25.960 --> 53:28.280
a question we have to ask is what is the world around us?

53:28.280 --> 53:32.880
So for each sense or for each mode of interaction with the world,

53:32.880 --> 53:37.520
if you adapt it to the world, then one question you have to ask is what does the world look like?

53:37.520 --> 53:43.280
So in this case, you can go collect a large database of images and study its properties.

53:43.280 --> 53:47.000
How is light organized in these images?

53:47.000 --> 53:52.920
And one thing you find is that in any color channel, this is supposed to be red, blue and green,

53:52.920 --> 53:57.400
you'll find that the distribution of the intensity of pixels,

53:57.400 --> 54:01.440
you know, if you look at the brightness of different pixels, it's very, very skewed.

54:01.440 --> 54:03.800
There are lots of pixels which are pretty dim,

54:03.800 --> 54:08.400
but then there's a very long tail with some pixels that are enormously bright.

54:08.400 --> 54:12.760
So it doesn't matter where you go, you do this inside a city, you do this at the North Pole,

54:12.760 --> 54:15.200
you do this in a jungle, you do it wherever you want,

54:15.200 --> 54:17.200
you're going to find that this is the case.

54:17.200 --> 54:20.840
It's a very universal feature of the visual world.

54:20.840 --> 54:23.480
And it has an important consequence.

54:23.480 --> 54:29.680
The average intensity of a pixel is lower than the median.

54:29.680 --> 54:36.840
Median means the median intensity is the intensity at which half the pixels are dimmer and half are brighter.

54:36.840 --> 54:42.200
So the median exceeds the mean in the world.

54:42.200 --> 54:46.040
There's another fact, which is that there are long range correlations.

54:46.040 --> 54:49.120
What do I mean by that? This is some technical figure showing that,

54:49.120 --> 54:52.960
but what we really mean is, you know, if it's kind of red here, it's going to be kind of red here,

54:52.960 --> 54:56.200
it's going to be kind of red here, and then after a little while it's improbable

54:56.200 --> 54:58.480
where the color is going to persist forever.

54:58.480 --> 55:02.000
But there's a very particular structure of those correlations which you can measure.

55:02.000 --> 55:06.080
It's a regularity of the natural world that you can measure.

55:06.080 --> 55:10.800
So you would think that, well, if I'm going to detect bright dark spots,

55:10.800 --> 55:12.560
maybe that's related to these statistics.

55:12.560 --> 55:16.920
And actually, without even doing anything, we can immediately argue,

55:16.920 --> 55:23.240
and let me argue to you now that these two facts, that light is correlated over distances

55:23.240 --> 55:28.960
and that you have the median light intensity exceeds the average,

55:28.960 --> 55:32.160
that immediately means that there are more dark spots in the world.

55:32.160 --> 55:34.040
Here's how to see that, right?

55:34.040 --> 55:40.000
So suppose what you do is you compare the light in a small region here

55:40.000 --> 55:45.200
to a large region there, right?

55:45.200 --> 55:55.400
So what's going to happen is that if you measure,

55:55.400 --> 55:59.320
if you compare the light in those two regions, because of these two statistical facts,

55:59.320 --> 56:03.880
it's going to turn out that the spot in the middle is generally going to be darker

56:03.880 --> 56:04.880
than the surrounding region.

56:04.880 --> 56:07.840
That's just because the average, if you see the average here,

56:07.840 --> 56:10.760
the average is less than this.

56:10.760 --> 56:12.240
So that turns out to be the case.

56:12.240 --> 56:17.920
So, for example, you can take a bunch of images and you can put down here a thing

56:17.920 --> 56:20.960
that detects a bright spot, a thing that detects a dark spot,

56:20.960 --> 56:25.280
and you can just count how many bright and dark spots you detect in every image,

56:25.280 --> 56:30.040
and you'll find that no matter what angular size you look at,

56:30.040 --> 56:34.400
there are more dark spots than there are bright spots.

56:34.400 --> 56:36.840
So that's what this figure is supposed to be showing.

56:36.840 --> 56:39.120
So we're going to ask a question based on this.

56:39.120 --> 56:42.040
So let's agree that because of these statistical facts,

56:42.040 --> 56:46.320
there are more dark spots than bright spots in the world.

56:46.320 --> 56:51.080
So then we can ask, suppose I give you an allocation that you can buy,

56:51.080 --> 56:56.480
or you can put in the eye, a certain number N of bright and dark spot detectors,

56:56.480 --> 56:59.640
namely on and off cells.

56:59.640 --> 57:03.720
So you get to divide them, you get to decide how many dark spot detectors

57:03.720 --> 57:06.120
and how many bright spot detectors you have,

57:06.120 --> 57:12.240
and my challenge here is to work out what would be best given the structure of the world.

57:12.240 --> 57:16.360
So it's immediately clear that if you have only money to buy one cell,

57:16.360 --> 57:18.600
it's better to buy a dark spot detector.

57:18.600 --> 57:20.760
Why? Because there are more dark spots.

57:20.760 --> 57:22.360
So it's more likely to respond.

57:22.360 --> 57:26.280
So you're more likely to get useful things out of it.

57:26.280 --> 57:29.560
If I give you two steps, then you buy some dark spot detectors

57:29.560 --> 57:31.760
and you can cover the eye with them.

57:31.760 --> 57:33.600
But then they're going to start overlapping.

57:33.600 --> 57:35.760
So they're going to start telling you the same thing,

57:35.760 --> 57:39.000
because they're overlapping, they're telling you redundant information.

57:39.000 --> 57:43.320
So then you should buy some on cells, bright spot detectors,

57:43.320 --> 57:47.840
and you can continue with this game and work out the mathematics of it

57:47.840 --> 57:53.320
about how you should tile the retina with bright and dark spot detectors

57:53.320 --> 57:58.800
in order to get the most information about the brightness of spots in the world.

57:58.800 --> 58:03.040
So that's a mathematical problem that you can write down and solve.

58:03.120 --> 58:06.400
And it turns out that the answer is about you should have one and a half

58:06.400 --> 58:10.960
to about two times as many dark spot detectors in your system

58:10.960 --> 58:13.240
if you want to say as much as you can,

58:13.240 --> 58:17.320
given your budget of the number of cells, about bright and dark spots.

58:17.320 --> 58:21.920
So we can ask, right, how does that compare with the actual eye?

58:21.920 --> 58:23.560
So over the last two decades,

58:23.560 --> 58:26.160
there have been a very large number of experiments on this.

58:26.160 --> 58:29.320
So it used to be in the 1990s, for example,

58:29.320 --> 58:30.920
if you asked people, they would talk about, you know,

58:31.880 --> 58:33.760
white and dark equal on opposite,

58:34.120 --> 58:36.840
but around the year 2000,

58:36.840 --> 58:38.740
a series exactly as you'd predict

58:39.400 --> 58:42.760
if the circuits of the eye

58:42.760 --> 58:46.660
were adapted to the statistical structure of the world.

58:46.920 --> 58:48.720
So exactly in that way.

58:48.720 --> 58:52.120
So the collective organization, so that suggests,

58:52.120 --> 58:54.760
so as I was saying according to the principal I mentioned,

58:54.760 --> 58:58.200
that the collective structure of the retina, the object,

58:58.200 --> 59:03.320
roedd y gofyniwyr o fynd a dydy'r cyific o d Однакоllur answers pwysigol

59:03.320 --> 59:07.140
sut wnaeth yr hyn, ddiun i собi d automobile

59:07.180 --> 59:11.960
o flynyddiad yn ychwaneg

59:12.100 --> 59:17.680
a chi dysgu cynyddiad y c eg Adventure

59:17.800 --> 59:19.960
Felly oddiwch iawn ay grid yments

59:20.860 --> 59:23.260
filn dech 1iblaethau alcoholu

59:23.260 --> 59:26.680
yw'r wider in the sensory world. Let's talk about smelling.

59:26.680 --> 59:30.600
And we care about smelling, about orphaction

59:30.600 --> 59:34.360
because it's in some ways the most primordial sense,

59:34.360 --> 59:40.180
right, all living things sense molecules

59:40.180 --> 59:44.000
So all living things smell in that sense, that every animal, most animals use smell

59:44.000 --> 59:49.700
more importantly than vision to do their daily jobs.

59:49.700 --> 59:55.640
Felly mae'r maewn leak, mae'n deamiad â'r hanon a Date-anes, ydy Republiciaus Gyno Secondly Mae'r Losing, …

59:55.640 --> 01:00:08.940
… mae'r rhywun gwleol yn gall Ilaigdol oherwydd sasr hyffМolau jyrnol yn y gywe uch G

01:00:08.940 --> 01:00:16.520
Mae gennymied excitingadol yna gyda ar bobl Paris Prefres Gwriaethyn yn eu cyzd.

01:00:16.520 --> 01:00:23.240
wrth gwrs you can go consult with the people who do fragrances and flavours, you know people

01:00:23.240 --> 01:00:29.520
who make artificial flavors and they will tell you that if you look at natural odors of different

01:00:29.520 --> 01:00:32.620
kinds, foods, for other animals etc- typically an odour will contain fifty different kinds

01:00:32.620 --> 01:00:37.440
of molecular species, so you could estimate from that that in terms of possible natural

01:00:37.440 --> 01:00:41.400
smell it's like ten thousand to the fifty. This is such a vast number, it's even hard

01:00:41.400 --> 01:00:49.400
That's the number of different things, like older objects, if you like, that you might have to sense in the world.

01:00:49.400 --> 01:00:57.400
To make matters worse, the older environment changes, reflecting season circumstances and new opportunities.

01:00:57.400 --> 01:01:05.400
So what do you need to do? Well, to detect a molecule, actually you need to feel its shape.

01:01:05.400 --> 01:01:11.400
The identity of a molecule is basically defined by how all of its atoms are arranged in space.

01:01:11.400 --> 01:01:23.400
So olfaction, the sense of smell, is a method used by the brain to sense the shapes of little things floating in the air.

01:01:23.400 --> 01:01:34.400
So what do you need to do this? Well, it turns out that odors are sensed when molecules bind to receptors in the nose.

01:01:34.400 --> 01:01:40.400
There's basically another molecule in a neuron in the nose, so that it's an olfactory sensory neuron.

01:01:40.400 --> 01:01:48.400
The molecule will be sitting there, and then in will come some other molecule, an odorant, as it's called, floating in the air.

01:01:48.400 --> 01:01:58.400
And if this molecule fits into the binding pocket of your sensor and sticks there, doesn't fall off, then the neuron will respond.

01:01:58.400 --> 01:02:07.400
So this binding pocket, your olfactory sensory neuron, is literally feeling the shape of the molecule, whether or not it fits.

01:02:07.400 --> 01:02:17.400
Now as it turns out, every receptor, olfactory receptor that allows you your sense of smell, is encoded by a separate gene.

01:02:17.400 --> 01:02:23.400
That's because, you know, to make your receptors have different shapes because they're trying to feel out different shapes.

01:02:23.400 --> 01:02:29.400
So to make a molecule, a protein, that has a different shape, you need a different gene to encode it.

01:02:29.400 --> 01:02:36.400
For the genetic level, you know, they're all different genes. In fact, it's the biggest gene family in most animals.

01:02:36.400 --> 01:02:45.400
So flies have about 100 of these, humans have about 300, and even the elephant only has about 2,000.

01:02:45.400 --> 01:02:59.400
So this leads us to feel, looking at this, that somehow, you know, apparently animals can sense all the molecules in the world and all the combinations of the molecules in the world with just a few hundred receptors.

01:02:59.400 --> 01:03:06.400
The reason this is very, very strange is you could phrase the difficulty of the problem in the following way.

01:03:06.400 --> 01:03:15.400
You could say, suppose an odour is defined somehow by the concentrations of all the molecules in it.

01:03:15.400 --> 01:03:19.400
Then it's possible that there are up to 10,000 different molecules.

01:03:19.400 --> 01:03:27.400
So you put the first concentration of this side on this axis, you put the second concentration of this axis, the third concentration of this axis, et cetera.

01:03:27.400 --> 01:03:35.400
There are 10,000 different axes on which you can draw all these concentrations, and somehow you have to represent all of this information

01:03:35.400 --> 01:03:43.400
in the responses of just 500 neurons, let's say, or 300 neurons, and so it's the response of neuron one, the response of neuron two, the response of neuron three.

01:03:43.400 --> 01:03:52.400
You just don't have, it would feel like there's no way to represent so much information about all the molecular concentrations with so few neural responses.

01:03:52.400 --> 01:03:56.400
You could, just to make the problem more easy to visualise, consider the following thing.

01:03:56.400 --> 01:04:01.400
Suppose I told you that you have positions in three-dimensional space, right? X, Y, Z, like that.

01:04:01.400 --> 01:04:13.400
There are positions in three-dimensional space, and I insist to you that you tell me where you are in three-dimensional space by telling me just, you know, a coordinate if you're like a location in one dimension.

01:04:13.400 --> 01:04:19.400
I mean, that's just really not, doesn't seem like it's very sensible, it's very hard to do, to do that, right?

01:04:19.400 --> 01:04:30.400
Because you've got three directions, you could go up, you could go left, you could go right, and somehow encoding that in just one direction seems very hard, okay?

01:04:30.400 --> 01:04:37.400
And what's more, you'd like to do that in a way that somehow preserves similarities and differences.

01:04:37.400 --> 01:04:43.400
You need to be able to tell that this order is the same as this order, or similar to, and these two orders are very different.

01:04:43.400 --> 01:04:49.400
So somehow the brain has to solve the problem of representing all of this information with very few neurons, right?

01:04:49.400 --> 01:04:57.400
So a very high-dimensional space of orders, as they say, in a low-dimensional space of responses.

01:04:57.400 --> 01:05:03.400
That's like going from three dimensions to one dimension, and it needs to do it in a way that preserves distances.

01:05:04.400 --> 01:05:15.400
So an insight that's happened in neuroscience in the recent past is that actually there's a way in which you can do this by adapting to structure in the space of orders.

01:05:15.400 --> 01:05:26.400
The structure is the following. We said already that if I look at, let's say, this order, it'll contain only a few molecules, let's say 50, and all the other things are absent.

01:05:26.400 --> 01:05:30.400
This order contains this molecule and this molecule, and all the other ones are absent.

01:05:30.400 --> 01:05:38.400
Each order in the natural world contains about 50 odorant molecules, right? 50 types of odorant molecules.

01:05:38.400 --> 01:05:52.400
So it turns out that there's a theorem in mathematics that you could take signals like this if they came to you and store them with many fewer responses than the number of things that are coming in

01:05:53.400 --> 01:06:01.400
if every sensor bound to all the molecules that are coming in kind of randomly.

01:06:01.400 --> 01:06:13.400
It's a rather than being structured like, you know, in vision, you have the cell that pulls out a bright spot and you have a cell that pulls out the dark spot or left to right motion and things like this.

01:06:13.400 --> 01:06:21.400
So instead of doing that, if you just said, here's an olfactory receptor, I kind of wanted to bind kind of randomly to everything and just give me a sort of mishmash of numbers.

01:06:21.400 --> 01:06:31.400
So there's a theorem in math that says if you do that, you could store all the information that's there in the odor world with a few hundred receptors.

01:06:31.400 --> 01:06:33.400
So there's a theorem like that.

01:06:33.400 --> 01:06:38.400
So the question is, does the olfactory system use randomness in that way?

01:06:38.400 --> 01:06:41.400
That's a very different kind of organization, mind you, than the visual system.

01:06:41.400 --> 01:06:42.400
That's why I'm giving this example.

01:06:42.400 --> 01:06:48.400
In the visual system, everything is highly structured because the visual world is highly structured.

01:06:48.400 --> 01:06:53.400
And here the idea would be, the math would tell you that it should be kind of random what it does.

01:06:53.400 --> 01:07:09.400
And indeed, so here's a picture of different receptors in the fruit fly binding to many different odorant molecules and the darkness of the color tells you the strength of the response.

01:07:09.400 --> 01:07:19.400
Now this doesn't look entirely random, and it isn't entirely random, but there's a sense, a mathematical sense, in which this is sufficiently random.

01:07:19.400 --> 01:07:22.400
It's pretty close to random.

01:07:22.400 --> 01:07:34.400
So much so that the, so this is, so in other words, basically what happens is that every olfactory receptor here is binding to many, many odorant molecules.

01:07:34.400 --> 01:07:38.400
That every odorant molecule is binding to many, many receptors.

01:07:38.400 --> 01:07:40.400
So this is a highly multiplexed thing.

01:07:40.400 --> 01:07:44.400
It's not like one receptor tells you this molecule is there, or this receptor tells you this molecule is there.

01:07:44.400 --> 01:07:50.400
It's like every receptor binds to everything at some level, and every molecule binds to everything.

01:07:50.400 --> 01:07:56.400
So this is a highly multiplexed kind of description of the odor world that appears in the nose.

01:07:56.400 --> 01:08:03.400
So this is what's called a kind of combinatorial code in the jargon of the field.

01:08:03.400 --> 01:08:04.400
And this continues.

01:08:04.400 --> 01:08:10.400
So this is a drawing, a diagram of your early olfactory system.

01:08:10.400 --> 01:08:12.400
So you have all the molecules in the world.

01:08:12.400 --> 01:08:19.400
This is a vector, this is a figure showing concentrations of which molecules are present.

01:08:19.400 --> 01:08:24.400
They bind to different receptors, you know, they're all colored differently.

01:08:24.400 --> 01:08:29.400
And then what happens is that the signals from the receptors are collected at a second stage.

01:08:29.400 --> 01:08:32.400
That's what's drawn here in the picture by Camilo Golgi.

01:08:32.400 --> 01:08:38.400
So the schematic that I've drawn here is actually this thing that I've simplified.

01:08:38.400 --> 01:08:40.400
And at this stage it's all cleaned up.

01:08:40.400 --> 01:08:47.400
And then it turns out that the signals from here project to your central brain, to the cortex also in a random way.

01:08:47.400 --> 01:08:53.400
So once again you get this sort of randomness and the information is kind of spread out all over the place.

01:08:53.400 --> 01:09:05.400
And you can show, and so that's what I was trying to argue to you, is that so you might be surprised if you studied vision, you might be surprised by this.

01:09:05.400 --> 01:09:14.400
Why is vision so structurally organized with all these little feature detectors pulling out different things in the world and sending the brain and so on?

01:09:14.400 --> 01:09:17.400
And why does olfaction look so random?

01:09:17.400 --> 01:09:27.400
So what I tried to argue to you is that the collective organization here in the olfactory system, the system of smell, is via randomness as opposed to structure,

01:09:27.400 --> 01:09:32.400
because that's what you need mathematically to be able to process signals of this kind.

01:09:32.400 --> 01:09:39.400
So in both cases, in both vision and olfaction, in one case the structure, in the other case the sort of randomness in the circuit,

01:09:39.400 --> 01:09:46.400
are associated with adapting the circuit to process efficiently the information in the world.

01:09:46.400 --> 01:09:50.400
So that's what I'm trying to convey here.

01:09:50.400 --> 01:09:57.400
By the way, I'm trying to give you some of the technical details here, because often I find that in these kinds of lectures, when I listen to public lectures,

01:09:57.400 --> 01:10:04.400
there's lots of pretty words, but I don't have a sense that something can actually be done, something concrete.

01:10:04.400 --> 01:10:10.400
I'd like you to go away with the sense that this is completely concrete, you can do these calculations, you can do these measurements.

01:10:10.400 --> 01:10:16.400
This is a completely concrete thing, and that's in the details of this kind of analysis.

01:10:16.400 --> 01:10:21.400
That's why I'm giving you some of these details, so you have a sense of how this works.

01:10:21.400 --> 01:10:25.400
So I'm going to give you one more example, and then stop.

01:10:25.400 --> 01:10:28.400
So the example I'm going to talk about is now learning.

01:10:28.400 --> 01:10:35.400
So far I've talked about adaptation in evolutionary selection of circuits to be adapted to the world.

01:10:35.400 --> 01:10:41.400
And now I want to talk about something that isn't evolutionary selection for adaptation to the world, but rather involves learning.

01:10:41.400 --> 01:10:43.400
So there are songbirds.

01:10:43.400 --> 01:10:48.400
Here are two kinds of songbirds, zebra finches and the brown-headed cowbird.

01:10:48.400 --> 01:10:56.400
And famously, songbirds learn to sing by imitation practice and innovation.

01:10:56.400 --> 01:11:00.400
By the way, the reason why we should care is humans do the same thing.

01:11:00.400 --> 01:11:03.400
Here's a human learning by practice.

01:11:03.400 --> 01:11:10.400
So that is a very important part of animal activity, to learn by practice and to get good at something.

01:11:10.400 --> 01:11:12.400
So how do songbirds learn?

01:11:12.400 --> 01:11:17.400
Well, it turns out that juvenile male bird dabbles and just goes on like that.

01:11:17.400 --> 01:11:28.400
And then over weeks, it learns to sing clearly partly through imitation because its song will partly imitate the songs it hears often of its parent with some innovation.

01:11:28.400 --> 01:11:33.400
And then the adult male bird sings to female birds often seeking a mate.

01:11:33.400 --> 01:11:40.400
And then the female birds seem to evaluate these songs and then decide which one or which ones to accept.

01:11:40.400 --> 01:11:44.400
And this is all really important, right? It matters.

01:11:44.400 --> 01:11:53.400
Because for example, in social species like this cowbird, the males and females work out a kind of social hierarchy based on their singing to each other.

01:11:53.400 --> 01:12:01.400
And then they produce pair bonds and the success of those pair bonds is absolutely critical for later success in egg laying.

01:12:01.400 --> 01:12:04.400
You don't make the pair bonds, the colony doesn't produce very many eggs.

01:12:04.400 --> 01:12:12.400
So it's actually really important for the survival of the species that this is all well organized and that the males learn to sing.

01:12:12.400 --> 01:12:14.400
So how do they do this?

01:12:14.400 --> 01:12:21.400
So now, let me tell you about how all of these circuits I've talked about reorganize themselves.

01:12:21.400 --> 01:12:28.400
They reorganize themselves when neurons, which are connected to each other, autonomously rewire their circuits.

01:12:28.400 --> 01:12:32.400
There's a rule, they'll use the rule to reorganize.

01:12:32.400 --> 01:12:40.400
There are various sorts of rules and one of the rules, one of these mechanisms is called spike timing dependent plasticity.

01:12:40.400 --> 01:12:42.400
So let me give you an example.

01:12:42.400 --> 01:12:46.400
Suppose you have two neurons, one and two, and they're connected and here's a synapse between them.

01:12:46.400 --> 01:12:50.400
Suppose, for some reason, they both fire.

01:12:50.400 --> 01:12:55.400
They both produce these voltage spikes that I talked about at the beginning of the top.

01:12:55.400 --> 01:13:00.400
Suppose neuron one fires before neuron two.

01:13:00.400 --> 01:13:03.400
You could have a rule that says the synapse strengthens.

01:13:03.400 --> 01:13:13.400
Or you could have a rule that the first, and also the second part of the rule says that neuron one fires after the second one the synapse weakens.

01:13:13.400 --> 01:13:16.400
So this is just a local rule, right?

01:13:16.400 --> 01:13:17.400
I mean, it's just a dynamical rule.

01:13:17.400 --> 01:13:22.400
You've got two dumb objects, neuron one and neuron two talking to each other, and they just have a rule.

01:13:22.400 --> 01:13:27.400
They have the strength of the synapse, the weak of the synapse, depending on who goes first.

01:13:27.400 --> 01:13:29.400
You can modify this a little bit.

01:13:29.400 --> 01:13:33.400
There are things called neuromodulators, which are sort of chemicals that the brain uses.

01:13:33.400 --> 01:13:37.400
And these can be used as global knobs to affect how this happens.

01:13:37.400 --> 01:13:41.400
So for example, there's a thing called dopamine, which if dopamine is released here,

01:13:41.400 --> 01:13:48.400
it can reinforce changes that have happened, that happened to have led offline to some sort of reward.

01:13:48.400 --> 01:13:53.400
So if there's a reward, you know, they'll do something to help cement some change that's happened to synapse.

01:13:53.400 --> 01:13:59.400
There's other things that can help a nephrin that also, you know, allow emotion to control how these synapses changes and everything.

01:13:59.400 --> 01:14:03.400
And you could put all of these rules on computers and see what they do and so on.

01:14:03.400 --> 01:14:08.400
It just seems really implausible at first sight that something so stupid, if you like,

01:14:08.400 --> 01:14:13.400
can actually lead to things like actual learning of things you might care about in the case of the birds,

01:14:13.400 --> 01:14:17.400
for example, allowing them to learn songs.

01:14:17.400 --> 01:14:19.400
But let's see.

01:14:19.400 --> 01:14:24.400
So how does song learning actual work in the actual brains of birds?

01:14:24.400 --> 01:14:26.400
So here's the bird brain.

01:14:26.400 --> 01:14:30.400
And what's written down here is all the different major brain areas.

01:14:30.400 --> 01:14:32.400
Remember, we talk about brain areas.

01:14:32.400 --> 01:14:37.400
And these are the brain areas that sort of cooperate collectively to help the bird to learn songs.

01:14:37.400 --> 01:14:44.400
Three important ones are HVCR and L-Man acronyms with some historic provenance.

01:14:44.400 --> 01:14:50.400
What's very interesting about the system is that this region, RA, has neurons in it,

01:14:50.400 --> 01:14:53.400
which are the things that control the muscles.

01:14:53.400 --> 01:14:55.400
So these are the things that actually produce the song.

01:14:55.400 --> 01:15:05.400
So a sequence of firing of the neurons in the region RA will make a whole bunch of muscles go back and forth in the bird's throat

01:15:05.400 --> 01:15:07.400
and thereby produce the song.

01:15:07.400 --> 01:15:09.400
So that's the thing that makes song.

01:15:09.400 --> 01:15:13.400
So to make a song, of course, you need to control those muscles in a particular order,

01:15:13.400 --> 01:15:18.400
control these muscles first, and these muscles, and these muscles, and these muscles, and these muscles,

01:15:18.400 --> 01:15:19.400
and that makes the song.

01:15:19.400 --> 01:15:22.400
So that's what you've got to learn.

01:15:22.400 --> 01:15:28.400
So RA also gets input from another area called HVCR.

01:15:28.400 --> 01:15:31.400
And HVCR acts as a conductor.

01:15:31.400 --> 01:15:37.400
So the neurons in this area produce patterns like this, and this, and this, and this, and this, if you like,

01:15:37.400 --> 01:15:42.400
but it's actually patterns of neuron firing, and they provide a time base.

01:15:42.400 --> 01:15:51.400
Every pattern of firing of neurons in this area HVCR marks out a different time in the song.

01:15:51.400 --> 01:15:53.400
It's literally conducting the song.

01:15:53.400 --> 01:16:02.400
So when HVCR produces this pattern, certain neurons here in the area RA should fire thereby pushing certain muscles.

01:16:02.400 --> 01:16:09.400
When neurons in HVCR fire in some other pattern, then some other neurons in RA should fire producing another push,

01:16:09.400 --> 01:16:11.400
a different kind of push in the muscles.

01:16:11.400 --> 01:16:12.400
So that's what should be happening.

01:16:12.400 --> 01:16:14.400
That's what it's got to learn.

01:16:14.400 --> 01:16:18.400
As the conductor marks out time, different muscles get pushed.

01:16:18.400 --> 01:16:20.400
So how does it learn this?

01:16:20.400 --> 01:16:26.400
There's a third area called L-man, which is a tutor area.

01:16:26.400 --> 01:16:29.400
And this tutor area does two things.

01:16:29.400 --> 01:16:31.400
First, it drives exploration.

01:16:31.400 --> 01:16:36.400
You see, part of the problem is that the bird doesn't know which neurons control which muscles.

01:16:36.400 --> 01:16:38.400
You don't know that.

01:16:38.400 --> 01:16:43.400
Just like baby doesn't know how to move its neurons in its head in order to control their muscles.

01:16:43.400 --> 01:16:45.400
It's something you learn.

01:16:45.400 --> 01:16:47.400
So here L-man does two things.

01:16:47.400 --> 01:16:53.400
One is it drives exploration by injecting a little bit of randomness in the firing of RA

01:16:53.400 --> 01:17:00.400
that allows the neurons there to do slightly different things every time to explore what can be done.

01:17:00.400 --> 01:17:03.400
The other thing it does is it provides guidance.

01:17:03.400 --> 01:17:11.400
So this area gets back a message that says how did the output compare to a remembered song

01:17:11.400 --> 01:17:15.400
and based upon those differences between what you wanted and what you got,

01:17:15.400 --> 01:17:19.400
it's able to provide some guidance about which synapses to change or well.

01:17:19.400 --> 01:17:22.400
That was a good thing you did, or that was a bad thing you did.

01:17:22.400 --> 01:17:25.400
It's able to provide guidance of this kind.

01:17:25.400 --> 01:17:30.400
So that's at the level of these brain areas collectively talking to each other.

01:17:30.400 --> 01:17:34.400
What about at the level of the individual neurons and the synapses?

01:17:34.400 --> 01:17:39.400
Because somehow here at the end every neuron is a dumb little thing

01:17:39.400 --> 01:17:43.400
and all it knows is a dumb little rule about how to change.

01:17:43.400 --> 01:17:55.400
So it turns out that right here there are synapses between the conductor neurons and the student neurons

01:17:55.400 --> 01:17:58.400
which have the following property.

01:17:58.400 --> 01:18:08.400
So if the conductor sends a message to the student before the tutor, the synapse strengthens.

01:18:08.400 --> 01:18:15.400
If the conductor sends a message to the student at the same time as the tutor,

01:18:15.400 --> 01:18:18.400
then the synapse weakens.

01:18:18.400 --> 01:18:24.400
That's a measurement in this system and otherwise basically nothing happens.

01:18:24.400 --> 01:18:28.400
This is for this particular synapse and this particular bird, this is the zebra finch.

01:18:28.400 --> 01:18:31.400
Other birds and other synapses can have different rules.

01:18:31.400 --> 01:18:39.400
So it would be a legitimate question to ask whether can such a stupid rule actually allow a bird to learn a song.

01:18:39.400 --> 01:18:45.400
So you can check that and the way neuroscientists or at least theoretical neuroscientists will do this

01:18:45.400 --> 01:18:48.400
is they'll build a computer model of the system.

01:18:48.400 --> 01:18:51.400
So you make a model of the conductor area as a bunch of neurons.

01:18:51.400 --> 01:18:55.400
You allow the conductor neurons to send signals to all the student neurons.

01:18:55.400 --> 01:19:00.400
Then you say there's a tutor and you get the tutor to send messages to the student neurons too

01:19:00.400 --> 01:19:04.400
and then you say I want the student to learn a particular pattern of firing and you press go

01:19:04.400 --> 01:19:10.400
and you say can such a stupid rule, this spike timing dependent plasticity rule

01:19:10.400 --> 01:19:14.400
where depending upon who fires first you strengthen a weakened synapse, can that work.

01:19:15.400 --> 01:19:21.400
So amazingly this is capable of learning the correct sequential outputs.

01:19:21.400 --> 01:19:27.400
You just tell it that I want this sequence of firing in this area, it can learn that output.

01:19:27.400 --> 01:19:33.400
But there's an interesting finding you can find out this way and that is the learning works better

01:19:33.400 --> 01:19:40.400
if the teaching style of the tutor matches the learning style of the student,

01:19:40.400 --> 01:19:42.400
the kind of thing you hear in kindergarten.

01:19:42.400 --> 01:19:49.400
So basically it could be that the neuron has this spike timing dependent plasticity rule.

01:19:49.400 --> 01:19:53.400
If A goes before B then you strengthen, if B goes before A you weaken.

01:19:53.400 --> 01:19:56.400
Or it could have this spike timing dependent plasticity rule.

01:19:56.400 --> 01:20:01.400
You strengthen so long as both of these fire close to each other.

01:20:01.400 --> 01:20:03.400
There are many different rules different neurons implement.

01:20:03.400 --> 01:20:10.400
So it turns out that for some of these rules of learning that an individual synapse could implement

01:20:10.400 --> 01:20:15.400
it turns out the tutor should teach the student, give it signals

01:20:15.400 --> 01:20:20.400
so that it learns early segments of the song before late segments of the song.

01:20:20.400 --> 01:20:26.400
That's the way it's going to work best given that rule of how the learning at the synapse works.

01:20:26.400 --> 01:20:34.400
But for other students by which I mean other kinds of synaptic plasticity or learning rules at the synapse

01:20:34.400 --> 01:20:37.400
you'll find that the teacher should just teach the whole song all the time,

01:20:37.400 --> 01:20:42.400
just keep giving corrections everywhere and it'll work better, it'll just work faster.

01:20:42.400 --> 01:20:46.400
That's a very interesting finding and this is intended to illustrate two things.

01:20:46.400 --> 01:20:49.400
One is this kind of learning, it's a collective effect.

01:20:49.400 --> 01:20:54.400
The whole brain cooperates and the synapses have to do things.

01:20:54.400 --> 01:20:58.400
So it's a collective of everything that's doing this effect.

01:20:58.400 --> 01:21:03.400
The first statement, the other statement is this kind of theoretical computer modelling

01:21:03.400 --> 01:21:08.400
that's now possible given I told you earlier that you could build a model of a neuron

01:21:08.400 --> 01:21:11.400
and why not put it on a computer and just see what it does.

01:21:11.400 --> 01:21:15.400
You can now do that kind of thing and learn a great deal, in fact this is a prediction

01:21:15.400 --> 01:21:20.400
for things that ought to be measurable in different kinds of birds and different kinds of synapses.

01:21:20.400 --> 01:21:26.400
So we're now at the stage where you can have the style of science that was in physics

01:21:26.400 --> 01:21:30.400
which is that you can have theory, you can have experiment, the theories make predictions,

01:21:30.400 --> 01:21:33.400
you can have experiments, experiments refine the theories etc.

01:21:33.400 --> 01:21:37.400
That is a mode of doing science that's been basically absent from much of neuroscience,

01:21:37.400 --> 01:21:41.400
from much of biology and that is now becoming possible because we have so many tools

01:21:41.400 --> 01:21:43.400
and so much knowledge of all of this.

01:21:43.400 --> 01:21:48.400
I'm basically out of time so I'm not going to talk about the circuits that underlie a sense of space

01:21:48.400 --> 01:21:55.400
unless you ask me during question time which can tell you more and instead I'm going to conclude.

01:21:55.400 --> 01:22:03.400
So the point of my talk today was to try to suggest to you, say something about how our brains make us

01:22:03.400 --> 01:22:08.400
and the kind of point I want to try to make to you is that inside our head we're all collectives.

01:22:08.400 --> 01:22:13.400
It's not like there's a thing which is you, there's just lots of neurons, lots of neurons are connected,

01:22:13.400 --> 01:22:16.400
lots of circuits, the circuits are connected into brain areas

01:22:16.400 --> 01:22:20.400
and collectively the emergent effect of all of this is you.

01:22:20.400 --> 01:22:25.400
I tried to illustrate that by looking at various examples and special cases of different kind,

01:22:25.400 --> 01:22:27.400
in particular for example this learning.

01:22:27.400 --> 01:22:32.400
Another point that I'd like to emphasize is that in essentials all animals are the same.

01:22:32.400 --> 01:22:38.400
This turtle and my daughter at an earlier age are basically in all essentials they're the same.

01:22:38.400 --> 01:22:44.400
All of the stuff we like about the fancy thinking we do is a veneer on top of that.

01:22:44.400 --> 01:22:46.400
Nevertheless there's a lot more to discuss.

01:22:46.400 --> 01:22:53.400
So for example we're very interested as humans in decision making, social behavior, things like curiosity and creativity,

01:22:53.400 --> 01:23:00.400
language which humans have a particular faculty for language, abstract thought which matters to us

01:23:00.400 --> 01:23:06.400
and we'd like to understand the origins of all of these behaviors in neural circuits

01:23:06.400 --> 01:23:08.400
and actually there's progress on this.

01:23:08.400 --> 01:23:14.400
Just in the last 20 years or last 10 years there are more and more tools for studying all of these things

01:23:14.400 --> 01:23:23.400
and I anticipate that the next decade we'll see lots of progress in understanding the neural circuit origin of these sorts of behaviors.

01:23:23.400 --> 01:23:28.400
Then usually most people are interested in the even broader question.

01:23:28.400 --> 01:23:34.400
One of the things you prize about ourselves is sentience and consciousness.

01:23:34.400 --> 01:23:36.400
So we'd like to know what are these states?

01:23:36.400 --> 01:23:38.400
They're clearly states of brain.

01:23:38.400 --> 01:23:41.400
We don't all say that rocks are sentient or conscious.

01:23:41.400 --> 01:23:43.400
They're states of brain.

01:23:43.400 --> 01:23:53.400
But we don't quite know how to define even what sentience and consciousness even mean in general let alone at the circuit level.

01:23:53.400 --> 01:23:57.400
We'd like to know from this how does mind emerge from brain.

01:23:57.400 --> 01:24:03.400
But I actually think that we will make progress on these questions in the next 100 years.

01:24:03.400 --> 01:24:06.400
That might seem like a long time but it isn't really.

01:24:06.400 --> 01:24:08.400
People live to 100.

01:24:08.400 --> 01:24:11.400
If you think about it we have come a heck of a long way since Cahal.

01:24:11.400 --> 01:24:18.400
Cahal had pictures of lots of different neurons and suggested that neurons are central to how the brain works.

01:24:18.400 --> 01:24:19.400
So there are these different things.

01:24:19.400 --> 01:24:21.400
They're like the atomic constituents of brains.

01:24:21.400 --> 01:24:25.400
And think about all the things I've been able to say today and I even only just scratched the surface.

01:24:25.400 --> 01:24:33.400
So 100 years from now I think there's every chance that we'll be able to give some sort of definition of what we want to mean by something that's conscious or sentient

01:24:33.400 --> 01:24:37.400
and understand how those states arise from neural circuits.

01:24:37.400 --> 01:24:38.400
So I'm going to stop there.

01:24:54.400 --> 01:24:56.400
So we'll go ahead and open it up for Q&A.

01:24:56.400 --> 01:24:58.400
I think we have room for about two questions.

01:24:58.400 --> 01:24:59.400
That's okay?

01:25:07.400 --> 01:25:19.400
So in your opinion what would be the best way to learn a new language?

01:25:24.400 --> 01:25:27.400
You're asking the tutor should start at the beginning of the sentence only?

01:25:27.400 --> 01:25:28.400
Where's the nothing?

01:25:30.400 --> 01:25:32.400
That's really not clear.

01:25:32.400 --> 01:25:36.400
I mean there are different learning styles that people have.

01:25:36.400 --> 01:25:48.400
So some people, my wife for example is very good at languages and always talks about how when she learned Arabic the grammar was what really attracted her.

01:25:48.400 --> 01:25:54.400
There's this very regular structure to that language and learning that grammar helped her to understand the language and everything.

01:25:54.400 --> 01:25:56.400
My dad is very good at languages.

01:25:56.400 --> 01:25:57.400
He speaks seven languages.

01:25:58.400 --> 01:26:13.400
And from him I get an impression of a kind of, on the one hand he just listens to it and reads it and then somehow on the way on the side studies grammar.

01:26:13.400 --> 01:26:15.400
It's a very different mode.

01:26:15.400 --> 01:26:20.400
It's also the case that the young and the old learn language in very different ways.

01:26:21.400 --> 01:26:27.400
Children just listen to it and they just pick the whole thing up somehow holistically.

01:26:27.400 --> 01:26:35.400
There's a certain stage, there's a critical period where you can do that and after that critical period the mode in which you learn language is known to be different.

01:26:35.400 --> 01:26:43.400
So it's much more useful when you're older to have structures like grammar and things like that and consciously be aware of them and to learn them.

01:26:43.400 --> 01:26:55.400
So there used to be this idea, well there is this idea due to Noam Chomsky of something called a universal grammar that's sort of embedded in these kinds of circuits that I've talked about, that it's just built in.

01:26:55.400 --> 01:27:05.400
All humans have the faculty for this and that in early childhood when you learn languages what you're doing is pruning away bits of that that you don't need.

01:27:05.400 --> 01:27:10.400
It's a little bit like all of those neurons, those interneurons, the retina that sort of take away all the stuff you don't need.

01:27:10.400 --> 01:27:18.400
So the idea is that you sort of prune those away and you're left with a structure pertent to the language that you're going to speak in your local area.

01:27:18.400 --> 01:27:35.400
Now with the advent of things like chat GPT and this sort of remarkable linguistic abilities there's some discussion, I mean there's debates about whether people agree or disagree still with this proposition.

01:27:35.400 --> 01:27:48.400
So I think that's a little bit up in the air because of this particular, if you think about chat GPT as a development in computational linguistics that's raised some questions about this notion of a universal grammar whether it's really there or not.

01:27:48.400 --> 01:28:04.400
I mean personally I think they're going to find that and I think the reason why things like chat GPT work is that there is a underlying regular structure in all human language and this is an engine that's built in such a way that it's able to extract that structure in the language.

01:28:04.400 --> 01:28:13.400
And so I think people are going to want to understand how to piece apart these artificial engines and work out what they're actually doing inside their inards.

01:28:13.400 --> 01:28:16.400
I think we will find that there is such a structure.

01:28:16.400 --> 01:28:20.400
So it's different strategies for young and old, it's also clearly different for different people.

01:28:20.400 --> 01:28:25.400
I'm not sure that's a sufficient answer to what your question but there you go.

01:28:34.400 --> 01:28:38.400
I'm coming, I'm coming.

01:28:38.400 --> 01:28:39.400
Oh, thank you.

01:28:39.400 --> 01:28:47.400
Yes, you touched on something which was related to my question which is the effect of age.

01:28:47.400 --> 01:28:52.400
You know this working in these mathematical schematics that you have.

01:28:52.400 --> 01:28:57.400
I'm a senior and I think there are a lot of seniors in this room.

01:28:57.400 --> 01:29:00.400
So can you speak to that?

01:29:00.400 --> 01:29:02.400
Yeah.

01:29:02.400 --> 01:29:21.400
So age produces many effects on the structure of the brain in part because adults basically don't get new neurons.

01:29:21.400 --> 01:29:25.400
There are two regions of the brain where you do.

01:29:25.400 --> 01:29:44.400
So in the olfactory system, so in this structure drawn by Golgi, this one over here, you actually get new neurons as you age, which is very interesting because nobody understands why.

01:29:45.400 --> 01:30:00.400
The other area where you get new neurons even when you're older is the hippocampus, which I mentioned here but didn't really talk about, which is the area associated with the learning memory navigation and things like that.

01:30:00.400 --> 01:30:15.400
So these two areas, you do get new neurons and their appearance and their placement within the circuit and where they synapse is known to be associated with the production in the hippocampus anyway of new memories.

01:30:15.400 --> 01:30:24.400
So one of the things that happens in people who have memory loss is that system isn't working quite as well.

01:30:24.400 --> 01:30:28.400
You're not getting enough neurons, you're not synaphing properly.

01:30:28.400 --> 01:30:32.400
But we know the locus of that.

01:30:32.400 --> 01:30:37.400
Maybe there is a day will come when you can encourage the production of new neurons.

01:30:37.400 --> 01:30:46.400
So you could make the argument that part of the reason we developed these age-related problems is maybe humans didn't originally live that long.

01:30:46.400 --> 01:30:48.400
Most people died young.

01:30:48.400 --> 01:30:53.400
They were eaten, they died of disease, they died of war, young.

01:30:53.400 --> 01:31:04.400
So at an earlier time in human history, we wouldn't have needed, well, in the general populace, populace to kind of maintain all of these structures of that problem.

01:31:04.400 --> 01:31:11.400
So maybe that's been changing and we're unfortunately stuck with an earlier evolutionary program.

01:31:11.400 --> 01:31:12.400
That's what's going on.

01:31:12.400 --> 01:31:16.400
So it's possible that those things can help.

01:31:16.400 --> 01:31:20.400
And also, you know, in general, ageing produces effects on all your cells.

01:31:20.400 --> 01:31:28.400
So there's things called telomeres that are at the end of chromosomes.

01:31:28.400 --> 01:31:34.400
And as you make copies of cells, the telomere is shortened.

01:31:34.400 --> 01:31:40.400
And if they get shortened so much that they're not really delimiting the ends of the chromosomes, then you start running into problems.

01:31:40.400 --> 01:31:47.400
Because, you know, the program for transcribing genes to make proteins and things won't work as well.

01:31:47.400 --> 01:31:48.400
So that's another issue.

01:31:48.400 --> 01:31:50.400
But not all creatures have that.

01:31:50.400 --> 01:31:56.400
So there are creatures that are essentially immortal that don't have this telomere shortening.

01:31:56.400 --> 01:32:00.400
They basically die when they're eaten or they don't get food.

01:32:00.400 --> 01:32:06.400
There's a jellyfish that gets younger at some point.

01:32:06.400 --> 01:32:11.400
It seems to get younger and then age again sort of oscillates in its effective age.

01:32:11.400 --> 01:32:13.400
Very strange.

01:32:13.400 --> 01:32:14.400
Very interesting.

01:32:14.400 --> 01:32:16.400
We'd all love that.

01:32:16.400 --> 01:32:22.400
And these are all subjects of the study of ageing.

01:32:22.400 --> 01:32:25.400
There's actually even weirder stuff still, right?

01:32:25.400 --> 01:32:30.400
Just to reveal the set of possibilities that can happen.

01:32:30.400 --> 01:32:33.400
So there's a creature called Hydra.

01:32:33.400 --> 01:32:35.400
You can chop it into two.

01:32:35.400 --> 01:32:36.400
It's got a nerve net.

01:32:36.400 --> 01:32:38.400
It doesn't have a brain.

01:32:38.400 --> 01:32:39.400
It has a nerve net.

01:32:39.400 --> 01:32:44.400
So there are nerves all over it that's kind of hooked up in a kind of neural network that operates Hydra.

01:32:44.400 --> 01:32:47.400
I'm mostly talking about creatures with centralized brains.

01:32:47.400 --> 01:32:50.400
But some creatures actually have a nerve net that's kind of spread about.

01:32:50.400 --> 01:32:52.400
You can chop Hydra in two.

01:32:52.400 --> 01:32:54.400
It'll make two Hydras.

01:32:54.400 --> 01:33:00.400
You can take Hydra and put it in a blender and kind of separate its cells, all separate.

01:33:00.400 --> 01:33:01.400
You put it back together.

01:33:01.400 --> 01:33:02.400
It makes a Hydra.

01:33:02.400 --> 01:33:04.400
All the cells find their location.

01:33:04.400 --> 01:33:07.400
I'd love to be like Hydra when I grow up.

01:33:07.400 --> 01:33:21.400
So the biology, the living things have many, many, many tricks that we're only just beginning to plumb the depths off.

01:33:21.400 --> 01:33:24.400
And so I think in time we will learn to do these tricks.

01:33:24.400 --> 01:33:28.400
Heck, you lose a finger in a circular saw.

01:33:28.400 --> 01:33:30.400
There's no reason why you can't grow it back.

01:33:30.400 --> 01:33:32.400
I mean that the program is there.

01:33:32.400 --> 01:33:34.400
So I'm not talking about neurons now.

01:33:34.400 --> 01:33:36.400
I'm talking about the rest of the body.

01:33:36.400 --> 01:33:38.400
But all of these things are in principle possible.

01:33:38.400 --> 01:33:42.400
We just need to know how to unlock within the, in that case, the circuits.

01:33:42.400 --> 01:33:50.400
I mean you can think about the organization of your body in terms of the circuits of what cells communicate with what, what turns, what on, etc.

01:33:50.400 --> 01:33:52.400
It should be possible.

01:33:52.400 --> 01:33:54.400
We just haven't figured out how.

01:33:54.400 --> 01:33:57.400
And likewise with the brain and with the problems of aging and everything.

01:33:57.400 --> 01:34:04.400
You know, once we understand how these circuits work, there's no reason why we can't get them to do what we want.

01:34:04.400 --> 01:34:05.400
These are machines.

01:34:05.400 --> 01:34:07.400
We're machines.

01:34:07.400 --> 01:34:10.400
And if you can repair a car, you can repair the head.

01:34:21.400 --> 01:34:23.400
Thank you so much, Vijay.

01:34:23.400 --> 01:34:26.400
And be sure to join us for our next set of lectures.

01:34:26.400 --> 01:34:30.400
We have our ulams coming up in September, September 19th and 20th.

01:34:30.400 --> 01:34:32.400
Have a good night.

