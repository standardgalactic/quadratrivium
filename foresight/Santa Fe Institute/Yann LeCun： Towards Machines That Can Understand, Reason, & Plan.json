{"text": " All right. OK, first machines they can understand, which means also reason and plan. It's going to be a lot of overlap with what Josh said, at least in terms of motivation, but not in terms of solutions. OK, first statement is that machine learning sucks, certainly compared to what we observe in humans and animals and their ability to learn and learn efficiently. You know, until recently, most of machine learning was based on supervised learning, required enormous amounts of label samples. What has taken over the last few years is self-supervised learning, which does not require as many label samples, but still requires a huge amount of samples. And in the end, those systems of still relatively brittle makes stupid mistakes, do not reason or plan, compared to humans and animals that can learn new tasks extremely quickly, because they understand how the world works, presumably, and they can reason and plan, and have certainly some level of common sense. So in our systems of today, most of them anyway, not absolutely all of them, but many of them, have a constant number of computational steps between their input and output, which means that whatever reasoning they do does not change, depending on whether it's a difficult problem they're trying to solve or not. They cannot really plan. The only systems they can plan at the moment are the ones that are designed to play games or to control robots. But things like LLMs do not plan. So how do we get machines to do like humans, which is to understand how the world works, predict the consequences of actions they might take, perform chains of reasoning with a potentially unlimited number of steps, and plan a complex task by decomposing them into sequences or subtasks. So let me start with this idea of self-supervised running, which really has taken over the world of AI over the last few years. And it's the basic idea of essentially presenting an input to a system, let's say a text, a window of text, or video, or a few images, and hiding part of it, and then training the system to capture the dependencies between what is observed and what is not yet observed, but eventually will be observed, whether it's the future of a video or a different view of the same scene from an image or words that have been obscured. And I say capture the dependency. I don't say predict because I'm going to talk about models that don't actually predict, but capture the dependencies. So a very successful example is language models. So self-supervised language models. And the idea goes back a long time to do this. I think the first paper to really kind of experiment with this was paper in around 2010 by Colbert and Weston, where they had this idea of essentially taking a piece of text, corrupting it in some ways. In modern versions, it consists in removing some words from the text, and then training some giant neural net to predict the words that are missing, or just merely to tell you whether the text that is here is legit or not legit. That's a different way of doing it. So this is how every modern NLP system over the last four or five years has been trained. And that has completely revolutionized not just the research in NLP, but also the practice of it. So all of translation, content moderation, hate speech detection, all that stuff from social networks, it all uses this kind of stuff. And performance went up by a huge amount. OK, so a special case of this is generative LLMs. And similar things are used in images and video. And there, the part of the text that you're hiding is just the last word. So you train a giant neural net to just predict the last word in a sequence. And then you can use this to produce outputs auto-regressively, which means you give a window of text, you get a system to produce a word, and then you shift that word into the input by shifting everything by one. Predict the next, next word, shift that in. Predict the next, next, next word, shift that in, et cetera. That's auto-regressive prediction. It's a major flaw with this approach. This is how every single LLM today works. But we should call them auto-regressive LLMs, because I think future LLMs are not going to be like this. But basically, every single one of them, some of which you've probably never heard of. So the ones from Faire, Blenderbot, Galactica, Lama, Alpeca, which is fine tuning of Lama. There is a new one now also. Lambda, Bard from Google, Shinshila from DeepMind, Chai GPT, GPT4, et cetera. They're all auto-regressive LLMs. And they train on gigantic amounts of data. So we're talking one trillion tokens or something like this. It would take a human reading eight hours a day, something like 22,000 years to read this. So obviously, those things can swallow a lot more and digest a lot more data than any human. And the performance is nothing short of amazing. But they do make stupid mistakes. They are extremely fluent. So we can use them to generate text. But they make factual errors, logical errors, inconsistencies. They have limited reasoning ability. There is no way to control for things like toxicity and stuff like that. And they really have no knowledge of the underlying reality, except in one case, because, of course, they only train from text, except in one case. And that case is code generation. And they work really, really well for code generation. And the reason they work well is that the underlying reality of code is very simple. It's deterministic. It's just the state of variables of a program. And so that's fully observable, deterministic, and everything. So it works really well. And they can generate fluent text. But in this particular case, this is a joke that my colleagues did on me. It's completely made up. I never actually did a rap album. Raw personal. Yeah. I asked them if they, I don't actually like rap that well. So I'm over a jazz person. So I asked them to do the same thing with jazz. And they say, there's not enough training data. And I cried. OK, so what are they good for? They're good for writing assistance, generating first draft, producing a style. Code writing assistance, obviously, very efficient for that. They're not good for producing factual and consistent answers because of aducinations. And they're not good for taking into account recent information, because you need to retrain the entire system to take into account yesterday in real time. And that's just not practical. They don't behave properly, or at least they're hard to control to do so. They don't do reasoning. They don't do planning. They don't do math, as we saw this morning. They're being modified to use tools, such as search engines, calculators, stuff like that. But currently, it's kind of like using duct tape and staples. And we're easily fooled by their fluency into thinking that they are smart, but they are not that smart. Now, there is a major flaw with this autoregressive generation, which is that it's an exponentially diverging diffusion process. So if there is the probability, e, for every token that is produced to be outside of the set of correct answers, let's assume that errors are being independent, then if we generate a sequence of n tokens, the probability for that sequence to be correct is 1 minus e to the power n. And that decreases exponentially. So those things just don't work. They just don't work. My prediction is that five years from now, nobody in that right mind would be using autoregressive LNMs. It's just a bad phase. They are useful, though. They're very useful. So as I said, they have a constant number of computational steps between input and output for each token generated. They do not reason and plan. Jake Browning, who will be talking Wednesday, and I wrote a philosophy paper. I mean, he wrote it on the fact that there are limitations to the purported intelligence of systems that are purely trained from text, because I would argue that most of human knowledge is not textual. I mean, certainly most of what babies are on before six months is non-textual. And everything that animals learn is non-textual. So that knowledge is still unattainable to current AI systems. So how do we get machines to understand how the world works and predict the consequences of their actions? All the limitations have been pointed out by a number of different papers, including one by the MIT crowd, that fluency is really not the same as thinking. And basically, you could argue for the fact that what LLMs are good for is perhaps modeling the Browning-Key and Boracay areas, but not much else in the brain. And that's like tiny little areas on the side of the brain. So we need something else. What are we missing? This is a chart that I like to show. Oops, the animation is bad. But it was put together by Emmanuel Dupu, who kind of tends to indicate at what age babies learn basic concepts, like object permanence, for example. Liz was talking about that. Stability and support and intuitive physics, which only comes up fairly late, actually, around nine months. And the question is, what type of learning is taking place there? No AI systems today really kind of does this properly, although there's been several attempts by a few of us. So I think perhaps it's this type of learning that is the basis of common sense. And we should really try to figure out how to reproduce this with machines. So I think there's three challenges for AI research today, learning representations and predictive models of the world, allowing machines to predict what's going to happen, perhaps as a consequence of their actions. Learning to reason. So this is more like Daniel Kahneman's System 2. Current autoregressive LLMs are basically System 1. They just view one word after the other without really planning ahead. And so that is the question of making reasoning compatible with learning. Josh has a particular proposal for this, which I don't agree with, but that goes in the right direction. And then learning to plan complex action sequences. So I made a proposal for this almost a year ago now, which I posted on this website so people can make comments and tell me I'm wrong and which references I missed. I guess several technical talks about it as well. And basically it's sort of a modular organization of an AI system that would be capable of reasoning and planning. And I can't say that I've built it, but we're kind of building pieces of it. So it's composed. It's basically centered around the award model, which will allow the system to predict ahead what the consequences of its actions would be. And it has a cost module. Think of it as kind of visual ganglia stuff. And the only purpose of the system is to optimize that cost. Some of those costs are essentially intrinsic, hard-wired, immutable costs that sort of drive the basic behavior of the system. And some of them are trainable costs that the system learns as it goes. And what the system does is that it plans a sequence of actions that, according to its model, will minimize those costs. And of course, it needs to be able to estimate the current state of the world, which is done through perception and maybe access to a memory. And then depending on the task that the system is focusing on, it can be entirely configured by a configurator that will sort of focus the system on the task at hand. So that's a cognitive architecture, which some people in classical AI have been proposing, but in sort of different forms. And there's two ways to use it. Mode 1, which is just a reactive perception action cycle, get an idea of the state of the world, encoding into an abstract representation of the state of the world as 0, and then running through some other neural net that produces an action reactively. But the more interesting mode is mode 2, which is like Kettiman's System 2, where you make an estimate of the state of the world, and then using your word model, predict ahead of time what's going to happen according to an imagined sequence of actions that you might take. And then the agent would optimize that sequence of actions. So as to minimize a particular cost function, representing the tax to be fulfilled. And then it would just take the first action and actually send it to the actuator, or maybe the first few actions. So this is completely classical in optimal control. It's called model predictive control. Except the problem here is how you learn the model. There's a way to kind of turn System 2 into System 1, which I'm not going to go into. OK, so how do we train the world model? Only for the fact that the world is not deterministic or not entirely predictable, even if it is deterministic. So we're not going to have a neural net observe the input and just predict why, and then minimizing a prediction error. That's not going to work, because that can only make one prediction. So in fact, if you train a big neural net to predict like these are cars from a top-down view of a highway, if you train a neural net to try to predict what's going to happen in this video, you get blurry predictions. Because the system cannot predict if a particular car is going to break or accelerate or turn left or right, and so it makes these blurry predictions. Same for a natural video. That's an old work on video prediction. So you have to account for the fact that the world is not completely predictable. And you have two solutions there. Either you build an architecture with latent variables that parameterizes the set of possible predictions, or, and those two are not incompatible, or you abandon the idea that you're going to predict everything about the world. And so this is what I'm suggesting. So this is a generative architecture. Generative architecture observes x, encodes it, then predicts y, the variable whose dependency you're trying to predict. And then you measure the prediction error. You mean my side by training, et cetera. What I'm proposing is a joint embedding architecture where both x and y go through encoders, neural nets, and the prediction takes place in representation space. What that allows the system to do is basically eliminate a lot of irrelevant information from y when it encodes it into SY so that it doesn't have to predict all the details. So there's a lot of things here and a lot of information in this room that we cannot possibly remember or predict the precise texture of the wood on the floor, things like that. But it's kind of irrelevant. We only need to have sort of a relatively abstract representation of it. So I'm basically recommending to abandon the whole idea of generative models, unless you want to produce pictures or produce text. But if you want to learn how the world works, you should not reconstruct. There's actually several versions of those joint embedding architectures, the simple one, deterministic ones that can predict, and then nondeterministic ones that can predict where the predictor can have latent variables. So that's kind of the most general architecture. And the latent variable A here can be a latent variable you infer or it could be an action. So imagine that this is a world model. This is the current state of the world that you observe, you encode. This is an action you might take in the world, maybe combined with some latent variable which represent what you don't know about the world. And then you make a prediction, and then you can compare that prediction with what actually occurs if you want to train your model. And that's a predictive model that will allow you to predict what's going to happen as a consequence of your actions. Now, because we're not generating anything, and because we can't turn a model of this type into a probabilistic model of t of y given x, we have to abandon the whole idea of probabilistic modeling. And now Josh is going, oh my god. Isn't it just approximate probability at that point? Isn't it? No. No? No. It's energies, OK? So basically the name of the game here is that you need to understand the system as computing an energy function that captures the dependency between x and y. So imagine the data points are those black spheres. The energy function should take low values around the black spheres and higher values outside. And whether this energy function represents the unnormalized log of some probability, is irrelevant, you just want the energy to be higher outside of the manifold of data. And it will have captured the dependency between the variables. And there's nothing more you need. Now, the next question is, how do you train a system to give low energy to stuff you observe and high energy to stuff you don't observe? And there are two methods, contrastive methods, which consist in generating fake contrastive points whose energy is going to push up. And then regularized methods, which I'm going to explain in a second. So let's say you have training samples. Your system currently gives low energy to this sort of peak area here. And it's not a good model of the data here, because it gives high energy to data points and low energy to areas that have no points. So what you can do is generate green points here whose energy you're going to push up. And the energy function is going to take the right shape. Or you could use some sort of regularizer that minimizes the volume of space that can take low energy. So that whenever you push down on the energy of some regions, the rest has to go up, because there is a limited amount of volume that can take low energy. So in the context of joint embedding architecture, I kind of invented the contrastive methods. That's called sine is net in the old days. But I'm now arguing against that in favor of regularized methods. And the big question is, how do we train them? I'll tell you about that in a minute. But I'm asking you to abandon generative models, abandon probabilistic models, probabilistic modeling in general, abandon contrastive methods. And of course, abandon reinforcement learning. But that, I've been saying this for years. Those are four of the main pillars of machine learning. That makes me super popular among my colleagues. OK, so what are those regularized methods for joint embedding architectures? So essentially, there is a big issue that you have to fix, which is that when you train a system like this, one of those JEPA architecture, joint embedding predictive architectures, you show it an example of x and y. And you tell it just train all the weights of all those neural nets so as to minimize the prediction error, it collapses. Basically, what it says is that, well, I can just set Sx and Sy to constants and set the prediction, set the predictor to some constant thing and ignore x and y all together. And that would be a collapse system that gives zero energy to everything in your space. You have to prevent that from happening. And one way to prevent that from happening is finding a way to maximize the information content of the representations that come out of the encoders. That actually has the effect of minimizing the volume of stuff that can take your energy indirectly. So one way to prevent the outputs from being constant is that you can force the variance to be non-zero. So you put a cost function on top of this vector here that says, over a batch of samples, I want the variance of each variable coming out of that neural net to be non-zero, to be above one, let's say. So that's a hinge loss that says the variance needs to be above one. It's not enough because the system can still cheat by making all the variables the same or very highly correlated. So you have another cost that says, I want them to be decorrelated. So basically, this has the effect of enforcing the covariance matrix of that those Sx vectors over a batch to be close to the identity. And it's not enough because the variables can be non-collapsed and correlated but still dependent. And so there's another trick that we do, and we have some theory that shows that it's not stupid, which is that you take the Sx vector, you run it to some neural net that expands the dimension, and then you apply those criteria on the covariance matrix to the output. And that makes the variables of Sx kind of more independent. Now, there's a major flaw with this, which is, and that's the theory which I'm not going to talk about. There's a flaw with all of this, which is that we're basically, we have an upper bound on information content, and we're pushing it up, hoping that the actual information content will follow. And it's stupid, but it kind of works. OK, so you can test those pre-training for image recognition. For example, you show two different views of the same image, train the network to produce identical representations for two different views of the same image, and then you freeze the network and basically train a linear classifier on top with ImageNet and measure the performance. And this Vcrag method that I just described works just as well as isn't the top pack, let's say. There's a bunch of different methods that have similar performance. And they are in the top pack. I'm not going to bore you with details. You can try to do segmentation as well. Here's another method, somewhat similar, but closer to the JEPA idea, which uses a different criterion to prevent collapse, which I'm not going to explain. And this one takes a partially masked input image together with a full input image, runs both of them through encoders, and then trains a predictor to basically predict the representation of the full image from the representation computed from the partially masked image. This is called IJEPA, ImageJEPA. And it works amazingly well. And it's really fast to train very good performance. In terms of performance, even though this type of masking does not require any knowledge about the nature of the input, essentially, or very little, the still you get the same kind of performance that you would get if you used a self-supervised learning method that exploits the fact that you're doing image recognition, like Dino or Ibot or Simclear, for example. OK, now, how are you going to use this in the end? What I'm really interested in is to use JEPAs as world models inside of the system. They can do system two type planning, but even better than this, they can do hierarchical planning. And the idea there is that when you think about a task, you're not planning this task at the lowest level in terms of millisecond by millisecond muscle control. You're playing a task like, I want to go from Santa Fe to New York, or let's say from New York to Santa Fe, that's a better example. So you first decompose this into two sub-tasks. First thing I need to do is go to the airport and catch a plane. How do I go to the airport? Well, to go to the airport, I need to get on the street and have a taxi, which you can do in New York City, not in Santa Fe. How do I get down in the street? I need to get out of the building I'm in, et cetera. How do I get out of the building? I need to stand up from my chair, walk to the door. How do I get up from my chair? So you kind of decompose this all the way down to the lowest level millisecond muscle control. But you're not going to plan the entire task of going from New York to Santa Fe all the way down to millisecond by millisecond muscle control. You do a hierarchical planning. We think humans, that we are the only ones who can do this. Animals do this, too. You observe the cat planning a trajectory to jump on a piece of furniture. They definitely do a hierarchical planning. So basically, what you do, what you need for this is a sort of hierarchy of JPA architectures of predictors that progressively produce more and more abstract representations of the state of the world, so that in the very abstract space of representations, you can make long-term predictions. Whereas in the sort of lower levels of abstraction, you can make shorter term prediction, but they're more accurate in the short term. So this is a two-level architecture. Low-level, you can make short-term predictions. High-level, you can make longer-term prediction in a more abstract space that has less details about how the world works. Now, we've been able to train a particular instance of JPA that simultaneously learns teachers that are good for image recognition and motion prediction in images. And I'm not going to go into the details of how this is pretty hairy. But it's kind of hierarchical. And it's got predictors that make pretty strong assumptions about the type of prediction that can occur. And simultaneously learns invariant features for image recognition. And this works really well for things like image segmentation, depth estimation, tracking, et cetera. It's called MCJPA, which means motion and content. And with this, hopefully, one day, we'll be able to build architectures that can perform hierarchical tanning of the type that I was telling you about. So observe the world, compute the abstract representation, and even more abstract representation, even more abstract representation, make a prediction to minimize a particular cost function that defines your task. I'm assuming this cost function is differentiable, so we can do this inference by gradient descent. In first, some latent variable that may represent the macro action you're going to take, or some unknown variable about the world. And then the state you're going to obtain through the first prediction is going to constitute a cost function for the lowest level. So the first predictor at the top tells me I should be at the airport. I started from New York. I should be at the airport. The cost function below measures how far I am from the airport. And so the second predictor says, go down in the street. Take a cab to the airport. And so the cost function at the bottom here says, am I on the street? Likely to catch a taxi, and all the way down to the actual actions that you can take in the real world. All right, coming to the conclusion. So steps towards autonomous AI systems. Self-supervised learning. We need a recipe that allows us to train systems to learn how the world works on video. I can't claim that we have achieved this. We're kind of partially there. And legal uncertainty in the prediction, and that's with a combination of this JEPA architecture understood within the context of energy-based model, potentially with latent variables, which I didn't talk about. That would allow us to learn world models from observation, hopefully hierarchical world models, possibly with interaction as well, and exploration. And now what we have is an architecture capable of reasoning and planning. I mean, the whole architecture I presented is kind of this idea of system two, that you can decompose complex tasks into simpler ones, and then plan a sequence of actions before you take the action. Something that's sort of missing from current autoregressive systems. So is this a potential path towards sort of human-level AI? Possibly yes, but it's certainly not for tomorrow. This is maybe a 10-year plan, maybe to get to cat-level intelligence or something like that. Now interestingly, those machines will have inevitably some sort of emotion consciousness. Forget about this, but emotions certainly, because emotions are kind of an anticipation of outcome, most of them. I mean, some of them are immediate perception of outcome, like pain and things like that. But most of them are anticipation of outcome, and this cost function is exactly what this is. And so if the system sort of predicts a particular set of outcome that results in a bad outcome, it might feel something similar to fear or something of that type. Anyway, so common sense is a collection of world models, or perhaps a single world model that is configurable. I'll come to this in one second. Understanding really means being able to predict. I think prediction is really the essence of intelligence here, and better mental models need to better understanding, or other substrate, if you want, of understanding. And as a consequence, also of good reasoning and action planning. The complex part in all of this is going to be to design intrinsic cost functions that drive the system towards learning appropriate things. And it's quite possible that, in the case of leaving things, it's easier for evolution to hardwire your cost functions into us than to hardwire your behavior. Hardwiring behavior and physical models and whatever is super hard. Like, as a neural net person, I would have no idea how to architect neural nets to do this. But I can certainly design a cost function that, if minimized, the system will learn those basic concepts. And that, there is a lot of hardwiring in there, no question. So one module I didn't talk much about is the configurator. And what it's supposed to do is configure all the modules in this architecture for a particular sub-task that the system needs to be focusing on at the moment. And I'm imagining that there is actually a single world model engine in this architecture that is reconfigurable for the task at hand. But it's not like the system would have multiple world models for different situations. It's got a single one that's configurable. The advantage of doing this, I mean, for humans and animals, is that it might actually fit in your skull. But there is another algorithmic advantage, or epistemic advantage, which is that a single one model can share knowledge between different situations. Whereas if you had a separate world model for different situations, you would have to retrain it independently for each of those situations. So how to make this configurator work, I have no idea. But that's a good hypothesis. So that would explain the fact that there is a single world model. It would explain why humans and many animals can only focus on the single conscious task at any one time. Because we only have one world model. We can only do system two on one task at a time. And I just leave the question for.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 2.8000000000000003, "text": " All right.", "tokens": [50364, 1057, 558, 13, 50504], "temperature": 0.0, "avg_logprob": -0.2528003832189048, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.0033665515948086977}, {"id": 1, "seek": 0, "start": 2.8000000000000003, "end": 7.24, "text": " OK, first machines they can understand,", "tokens": [50504, 2264, 11, 700, 8379, 436, 393, 1223, 11, 50726], "temperature": 0.0, "avg_logprob": -0.2528003832189048, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.0033665515948086977}, {"id": 2, "seek": 0, "start": 7.24, "end": 9.76, "text": " which means also reason and plan.", "tokens": [50726, 597, 1355, 611, 1778, 293, 1393, 13, 50852], "temperature": 0.0, "avg_logprob": -0.2528003832189048, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.0033665515948086977}, {"id": 3, "seek": 0, "start": 9.76, "end": 12.8, "text": " It's going to be a lot of overlap with what Josh said,", "tokens": [50852, 467, 311, 516, 281, 312, 257, 688, 295, 19959, 365, 437, 9785, 848, 11, 51004], "temperature": 0.0, "avg_logprob": -0.2528003832189048, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.0033665515948086977}, {"id": 4, "seek": 0, "start": 12.8, "end": 19.2, "text": " at least in terms of motivation, but not in terms of solutions.", "tokens": [51004, 412, 1935, 294, 2115, 295, 12335, 11, 457, 406, 294, 2115, 295, 6547, 13, 51324], "temperature": 0.0, "avg_logprob": -0.2528003832189048, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.0033665515948086977}, {"id": 5, "seek": 0, "start": 19.2, "end": 23.64, "text": " OK, first statement is that machine learning sucks,", "tokens": [51324, 2264, 11, 700, 5629, 307, 300, 3479, 2539, 15846, 11, 51546], "temperature": 0.0, "avg_logprob": -0.2528003832189048, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.0033665515948086977}, {"id": 6, "seek": 0, "start": 23.64, "end": 26.68, "text": " certainly compared to what we observe in humans and animals", "tokens": [51546, 3297, 5347, 281, 437, 321, 11441, 294, 6255, 293, 4882, 51698], "temperature": 0.0, "avg_logprob": -0.2528003832189048, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.0033665515948086977}, {"id": 7, "seek": 2668, "start": 26.68, "end": 31.28, "text": " and their ability to learn and learn efficiently.", "tokens": [50364, 293, 641, 3485, 281, 1466, 293, 1466, 19621, 13, 50594], "temperature": 0.0, "avg_logprob": -0.1794676979382833, "compression_ratio": 1.724890829694323, "no_speech_prob": 0.004743547178804874}, {"id": 8, "seek": 2668, "start": 31.28, "end": 33.6, "text": " You know, until recently, most of machine learning", "tokens": [50594, 509, 458, 11, 1826, 3938, 11, 881, 295, 3479, 2539, 50710], "temperature": 0.0, "avg_logprob": -0.1794676979382833, "compression_ratio": 1.724890829694323, "no_speech_prob": 0.004743547178804874}, {"id": 9, "seek": 2668, "start": 33.6, "end": 37.12, "text": " was based on supervised learning,", "tokens": [50710, 390, 2361, 322, 46533, 2539, 11, 50886], "temperature": 0.0, "avg_logprob": -0.1794676979382833, "compression_ratio": 1.724890829694323, "no_speech_prob": 0.004743547178804874}, {"id": 10, "seek": 2668, "start": 37.12, "end": 41.76, "text": " required enormous amounts of label samples.", "tokens": [50886, 4739, 11322, 11663, 295, 7645, 10938, 13, 51118], "temperature": 0.0, "avg_logprob": -0.1794676979382833, "compression_ratio": 1.724890829694323, "no_speech_prob": 0.004743547178804874}, {"id": 11, "seek": 2668, "start": 41.76, "end": 45.519999999999996, "text": " What has taken over the last few years", "tokens": [51118, 708, 575, 2726, 670, 264, 1036, 1326, 924, 51306], "temperature": 0.0, "avg_logprob": -0.1794676979382833, "compression_ratio": 1.724890829694323, "no_speech_prob": 0.004743547178804874}, {"id": 12, "seek": 2668, "start": 45.519999999999996, "end": 47.2, "text": " is self-supervised learning, which does not", "tokens": [51306, 307, 2698, 12, 48172, 24420, 2539, 11, 597, 775, 406, 51390], "temperature": 0.0, "avg_logprob": -0.1794676979382833, "compression_ratio": 1.724890829694323, "no_speech_prob": 0.004743547178804874}, {"id": 13, "seek": 2668, "start": 47.2, "end": 49.44, "text": " require as many label samples, but still requires", "tokens": [51390, 3651, 382, 867, 7645, 10938, 11, 457, 920, 7029, 51502], "temperature": 0.0, "avg_logprob": -0.1794676979382833, "compression_ratio": 1.724890829694323, "no_speech_prob": 0.004743547178804874}, {"id": 14, "seek": 2668, "start": 49.44, "end": 52.44, "text": " a huge amount of samples.", "tokens": [51502, 257, 2603, 2372, 295, 10938, 13, 51652], "temperature": 0.0, "avg_logprob": -0.1794676979382833, "compression_ratio": 1.724890829694323, "no_speech_prob": 0.004743547178804874}, {"id": 15, "seek": 2668, "start": 52.44, "end": 56.04, "text": " And in the end, those systems of still relatively brittle", "tokens": [51652, 400, 294, 264, 917, 11, 729, 3652, 295, 920, 7226, 49325, 51832], "temperature": 0.0, "avg_logprob": -0.1794676979382833, "compression_ratio": 1.724890829694323, "no_speech_prob": 0.004743547178804874}, {"id": 16, "seek": 5604, "start": 56.04, "end": 59.12, "text": " makes stupid mistakes, do not reason or plan,", "tokens": [50364, 1669, 6631, 8038, 11, 360, 406, 1778, 420, 1393, 11, 50518], "temperature": 0.0, "avg_logprob": -0.16702669971394088, "compression_ratio": 1.6848249027237354, "no_speech_prob": 0.0005348461563698947}, {"id": 17, "seek": 5604, "start": 59.12, "end": 61.48, "text": " compared to humans and animals that can learn", "tokens": [50518, 5347, 281, 6255, 293, 4882, 300, 393, 1466, 50636], "temperature": 0.0, "avg_logprob": -0.16702669971394088, "compression_ratio": 1.6848249027237354, "no_speech_prob": 0.0005348461563698947}, {"id": 18, "seek": 5604, "start": 61.48, "end": 64.56, "text": " new tasks extremely quickly, because they understand", "tokens": [50636, 777, 9608, 4664, 2661, 11, 570, 436, 1223, 50790], "temperature": 0.0, "avg_logprob": -0.16702669971394088, "compression_ratio": 1.6848249027237354, "no_speech_prob": 0.0005348461563698947}, {"id": 19, "seek": 5604, "start": 64.56, "end": 69.24, "text": " how the world works, presumably, and they can reason and plan,", "tokens": [50790, 577, 264, 1002, 1985, 11, 26742, 11, 293, 436, 393, 1778, 293, 1393, 11, 51024], "temperature": 0.0, "avg_logprob": -0.16702669971394088, "compression_ratio": 1.6848249027237354, "no_speech_prob": 0.0005348461563698947}, {"id": 20, "seek": 5604, "start": 69.24, "end": 72.44, "text": " and have certainly some level of common sense.", "tokens": [51024, 293, 362, 3297, 512, 1496, 295, 2689, 2020, 13, 51184], "temperature": 0.0, "avg_logprob": -0.16702669971394088, "compression_ratio": 1.6848249027237354, "no_speech_prob": 0.0005348461563698947}, {"id": 21, "seek": 5604, "start": 72.44, "end": 77.4, "text": " So in our systems of today, most of them anyway,", "tokens": [51184, 407, 294, 527, 3652, 295, 965, 11, 881, 295, 552, 4033, 11, 51432], "temperature": 0.0, "avg_logprob": -0.16702669971394088, "compression_ratio": 1.6848249027237354, "no_speech_prob": 0.0005348461563698947}, {"id": 22, "seek": 5604, "start": 77.4, "end": 80.52, "text": " not absolutely all of them, but many of them,", "tokens": [51432, 406, 3122, 439, 295, 552, 11, 457, 867, 295, 552, 11, 51588], "temperature": 0.0, "avg_logprob": -0.16702669971394088, "compression_ratio": 1.6848249027237354, "no_speech_prob": 0.0005348461563698947}, {"id": 23, "seek": 5604, "start": 80.52, "end": 83.24, "text": " have a constant number of computational steps", "tokens": [51588, 362, 257, 5754, 1230, 295, 28270, 4439, 51724], "temperature": 0.0, "avg_logprob": -0.16702669971394088, "compression_ratio": 1.6848249027237354, "no_speech_prob": 0.0005348461563698947}, {"id": 24, "seek": 5604, "start": 83.24, "end": 85.64, "text": " between their input and output, which", "tokens": [51724, 1296, 641, 4846, 293, 5598, 11, 597, 51844], "temperature": 0.0, "avg_logprob": -0.16702669971394088, "compression_ratio": 1.6848249027237354, "no_speech_prob": 0.0005348461563698947}, {"id": 25, "seek": 8564, "start": 85.64, "end": 88.2, "text": " means that whatever reasoning they do", "tokens": [50364, 1355, 300, 2035, 21577, 436, 360, 50492], "temperature": 0.0, "avg_logprob": -0.13370983775069073, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.001242277561686933}, {"id": 26, "seek": 8564, "start": 88.2, "end": 90.2, "text": " does not change, depending on whether it's", "tokens": [50492, 775, 406, 1319, 11, 5413, 322, 1968, 309, 311, 50592], "temperature": 0.0, "avg_logprob": -0.13370983775069073, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.001242277561686933}, {"id": 27, "seek": 8564, "start": 90.2, "end": 93.16, "text": " a difficult problem they're trying to solve or not.", "tokens": [50592, 257, 2252, 1154, 436, 434, 1382, 281, 5039, 420, 406, 13, 50740], "temperature": 0.0, "avg_logprob": -0.13370983775069073, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.001242277561686933}, {"id": 28, "seek": 8564, "start": 93.16, "end": 94.4, "text": " They cannot really plan.", "tokens": [50740, 814, 2644, 534, 1393, 13, 50802], "temperature": 0.0, "avg_logprob": -0.13370983775069073, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.001242277561686933}, {"id": 29, "seek": 8564, "start": 94.4, "end": 96.2, "text": " The only systems they can plan at the moment", "tokens": [50802, 440, 787, 3652, 436, 393, 1393, 412, 264, 1623, 50892], "temperature": 0.0, "avg_logprob": -0.13370983775069073, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.001242277561686933}, {"id": 30, "seek": 8564, "start": 96.2, "end": 98.32, "text": " are the ones that are designed to play games", "tokens": [50892, 366, 264, 2306, 300, 366, 4761, 281, 862, 2813, 50998], "temperature": 0.0, "avg_logprob": -0.13370983775069073, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.001242277561686933}, {"id": 31, "seek": 8564, "start": 98.32, "end": 100.64, "text": " or to control robots.", "tokens": [50998, 420, 281, 1969, 14733, 13, 51114], "temperature": 0.0, "avg_logprob": -0.13370983775069073, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.001242277561686933}, {"id": 32, "seek": 8564, "start": 100.64, "end": 104.76, "text": " But things like LLMs do not plan.", "tokens": [51114, 583, 721, 411, 441, 43, 26386, 360, 406, 1393, 13, 51320], "temperature": 0.0, "avg_logprob": -0.13370983775069073, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.001242277561686933}, {"id": 33, "seek": 8564, "start": 104.76, "end": 107.0, "text": " So how do we get machines to do like humans,", "tokens": [51320, 407, 577, 360, 321, 483, 8379, 281, 360, 411, 6255, 11, 51432], "temperature": 0.0, "avg_logprob": -0.13370983775069073, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.001242277561686933}, {"id": 34, "seek": 8564, "start": 107.0, "end": 108.64, "text": " which is to understand how the world works,", "tokens": [51432, 597, 307, 281, 1223, 577, 264, 1002, 1985, 11, 51514], "temperature": 0.0, "avg_logprob": -0.13370983775069073, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.001242277561686933}, {"id": 35, "seek": 8564, "start": 108.64, "end": 110.92, "text": " predict the consequences of actions", "tokens": [51514, 6069, 264, 10098, 295, 5909, 51628], "temperature": 0.0, "avg_logprob": -0.13370983775069073, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.001242277561686933}, {"id": 36, "seek": 8564, "start": 110.92, "end": 114.68, "text": " they might take, perform chains of reasoning", "tokens": [51628, 436, 1062, 747, 11, 2042, 12626, 295, 21577, 51816], "temperature": 0.0, "avg_logprob": -0.13370983775069073, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.001242277561686933}, {"id": 37, "seek": 11468, "start": 114.68, "end": 117.04, "text": " with a potentially unlimited number of steps,", "tokens": [50364, 365, 257, 7263, 21950, 1230, 295, 4439, 11, 50482], "temperature": 0.0, "avg_logprob": -0.13737652365085298, "compression_ratio": 1.6184738955823292, "no_speech_prob": 0.0009813997894525528}, {"id": 38, "seek": 11468, "start": 117.04, "end": 119.72000000000001, "text": " and plan a complex task by decomposing them", "tokens": [50482, 293, 1393, 257, 3997, 5633, 538, 22867, 6110, 552, 50616], "temperature": 0.0, "avg_logprob": -0.13737652365085298, "compression_ratio": 1.6184738955823292, "no_speech_prob": 0.0009813997894525528}, {"id": 39, "seek": 11468, "start": 119.72000000000001, "end": 121.72000000000001, "text": " into sequences or subtasks.", "tokens": [50616, 666, 22978, 420, 7257, 296, 1694, 13, 50716], "temperature": 0.0, "avg_logprob": -0.13737652365085298, "compression_ratio": 1.6184738955823292, "no_speech_prob": 0.0009813997894525528}, {"id": 40, "seek": 11468, "start": 121.72000000000001, "end": 125.2, "text": " So let me start with this idea of self-supervised running,", "tokens": [50716, 407, 718, 385, 722, 365, 341, 1558, 295, 2698, 12, 48172, 24420, 2614, 11, 50890], "temperature": 0.0, "avg_logprob": -0.13737652365085298, "compression_ratio": 1.6184738955823292, "no_speech_prob": 0.0009813997894525528}, {"id": 41, "seek": 11468, "start": 125.2, "end": 128.48000000000002, "text": " which really has taken over the world of AI", "tokens": [50890, 597, 534, 575, 2726, 670, 264, 1002, 295, 7318, 51054], "temperature": 0.0, "avg_logprob": -0.13737652365085298, "compression_ratio": 1.6184738955823292, "no_speech_prob": 0.0009813997894525528}, {"id": 42, "seek": 11468, "start": 128.48000000000002, "end": 131.08, "text": " over the last few years.", "tokens": [51054, 670, 264, 1036, 1326, 924, 13, 51184], "temperature": 0.0, "avg_logprob": -0.13737652365085298, "compression_ratio": 1.6184738955823292, "no_speech_prob": 0.0009813997894525528}, {"id": 43, "seek": 11468, "start": 131.08, "end": 134.68, "text": " And it's the basic idea of essentially presenting", "tokens": [51184, 400, 309, 311, 264, 3875, 1558, 295, 4476, 15578, 51364], "temperature": 0.0, "avg_logprob": -0.13737652365085298, "compression_ratio": 1.6184738955823292, "no_speech_prob": 0.0009813997894525528}, {"id": 44, "seek": 11468, "start": 134.68, "end": 137.96, "text": " an input to a system, let's say a text, a window of text,", "tokens": [51364, 364, 4846, 281, 257, 1185, 11, 718, 311, 584, 257, 2487, 11, 257, 4910, 295, 2487, 11, 51528], "temperature": 0.0, "avg_logprob": -0.13737652365085298, "compression_ratio": 1.6184738955823292, "no_speech_prob": 0.0009813997894525528}, {"id": 45, "seek": 11468, "start": 137.96, "end": 144.04000000000002, "text": " or video, or a few images, and hiding part of it,", "tokens": [51528, 420, 960, 11, 420, 257, 1326, 5267, 11, 293, 10596, 644, 295, 309, 11, 51832], "temperature": 0.0, "avg_logprob": -0.13737652365085298, "compression_ratio": 1.6184738955823292, "no_speech_prob": 0.0009813997894525528}, {"id": 46, "seek": 14404, "start": 144.04, "end": 146.48, "text": " and then training the system to capture the dependencies", "tokens": [50364, 293, 550, 3097, 264, 1185, 281, 7983, 264, 36606, 50486], "temperature": 0.0, "avg_logprob": -0.1269135016661424, "compression_ratio": 1.7896995708154506, "no_speech_prob": 0.000855151389259845}, {"id": 47, "seek": 14404, "start": 146.48, "end": 150.6, "text": " between what is observed and what is not yet observed,", "tokens": [50486, 1296, 437, 307, 13095, 293, 437, 307, 406, 1939, 13095, 11, 50692], "temperature": 0.0, "avg_logprob": -0.1269135016661424, "compression_ratio": 1.7896995708154506, "no_speech_prob": 0.000855151389259845}, {"id": 48, "seek": 14404, "start": 150.6, "end": 153.07999999999998, "text": " but eventually will be observed, whether it's", "tokens": [50692, 457, 4728, 486, 312, 13095, 11, 1968, 309, 311, 50816], "temperature": 0.0, "avg_logprob": -0.1269135016661424, "compression_ratio": 1.7896995708154506, "no_speech_prob": 0.000855151389259845}, {"id": 49, "seek": 14404, "start": 153.07999999999998, "end": 157.12, "text": " the future of a video or a different view of the same scene", "tokens": [50816, 264, 2027, 295, 257, 960, 420, 257, 819, 1910, 295, 264, 912, 4145, 51018], "temperature": 0.0, "avg_logprob": -0.1269135016661424, "compression_ratio": 1.7896995708154506, "no_speech_prob": 0.000855151389259845}, {"id": 50, "seek": 14404, "start": 157.12, "end": 163.68, "text": " from an image or words that have been obscured.", "tokens": [51018, 490, 364, 3256, 420, 2283, 300, 362, 668, 22082, 3831, 13, 51346], "temperature": 0.0, "avg_logprob": -0.1269135016661424, "compression_ratio": 1.7896995708154506, "no_speech_prob": 0.000855151389259845}, {"id": 51, "seek": 14404, "start": 163.68, "end": 165.39999999999998, "text": " And I say capture the dependency.", "tokens": [51346, 400, 286, 584, 7983, 264, 33621, 13, 51432], "temperature": 0.0, "avg_logprob": -0.1269135016661424, "compression_ratio": 1.7896995708154506, "no_speech_prob": 0.000855151389259845}, {"id": 52, "seek": 14404, "start": 165.39999999999998, "end": 167.16, "text": " I don't say predict because I'm going", "tokens": [51432, 286, 500, 380, 584, 6069, 570, 286, 478, 516, 51520], "temperature": 0.0, "avg_logprob": -0.1269135016661424, "compression_ratio": 1.7896995708154506, "no_speech_prob": 0.000855151389259845}, {"id": 53, "seek": 14404, "start": 167.16, "end": 169.51999999999998, "text": " to talk about models that don't actually predict,", "tokens": [51520, 281, 751, 466, 5245, 300, 500, 380, 767, 6069, 11, 51638], "temperature": 0.0, "avg_logprob": -0.1269135016661424, "compression_ratio": 1.7896995708154506, "no_speech_prob": 0.000855151389259845}, {"id": 54, "seek": 14404, "start": 169.51999999999998, "end": 171.16, "text": " but capture the dependencies.", "tokens": [51638, 457, 7983, 264, 36606, 13, 51720], "temperature": 0.0, "avg_logprob": -0.1269135016661424, "compression_ratio": 1.7896995708154506, "no_speech_prob": 0.000855151389259845}, {"id": 55, "seek": 17116, "start": 171.16, "end": 176.16, "text": " So a very successful example is language models.", "tokens": [50364, 407, 257, 588, 4406, 1365, 307, 2856, 5245, 13, 50614], "temperature": 0.0, "avg_logprob": -0.19153723556004212, "compression_ratio": 1.5321100917431192, "no_speech_prob": 0.00018485311011318117}, {"id": 56, "seek": 17116, "start": 176.16, "end": 178.84, "text": " So self-supervised language models.", "tokens": [50614, 407, 2698, 12, 48172, 24420, 2856, 5245, 13, 50748], "temperature": 0.0, "avg_logprob": -0.19153723556004212, "compression_ratio": 1.5321100917431192, "no_speech_prob": 0.00018485311011318117}, {"id": 57, "seek": 17116, "start": 178.84, "end": 181.28, "text": " And the idea goes back a long time to do this.", "tokens": [50748, 400, 264, 1558, 1709, 646, 257, 938, 565, 281, 360, 341, 13, 50870], "temperature": 0.0, "avg_logprob": -0.19153723556004212, "compression_ratio": 1.5321100917431192, "no_speech_prob": 0.00018485311011318117}, {"id": 58, "seek": 17116, "start": 181.28, "end": 183.44, "text": " I think the first paper to really kind of experiment", "tokens": [50870, 286, 519, 264, 700, 3035, 281, 534, 733, 295, 5120, 50978], "temperature": 0.0, "avg_logprob": -0.19153723556004212, "compression_ratio": 1.5321100917431192, "no_speech_prob": 0.00018485311011318117}, {"id": 59, "seek": 17116, "start": 183.44, "end": 190.51999999999998, "text": " with this was paper in around 2010 by Colbert and Weston,", "tokens": [50978, 365, 341, 390, 3035, 294, 926, 9657, 538, 4004, 4290, 293, 4055, 266, 11, 51332], "temperature": 0.0, "avg_logprob": -0.19153723556004212, "compression_ratio": 1.5321100917431192, "no_speech_prob": 0.00018485311011318117}, {"id": 60, "seek": 17116, "start": 190.51999999999998, "end": 193.35999999999999, "text": " where they had this idea of essentially taking", "tokens": [51332, 689, 436, 632, 341, 1558, 295, 4476, 1940, 51474], "temperature": 0.0, "avg_logprob": -0.19153723556004212, "compression_ratio": 1.5321100917431192, "no_speech_prob": 0.00018485311011318117}, {"id": 61, "seek": 17116, "start": 193.35999999999999, "end": 198.24, "text": " a piece of text, corrupting it in some ways.", "tokens": [51474, 257, 2522, 295, 2487, 11, 17366, 278, 309, 294, 512, 2098, 13, 51718], "temperature": 0.0, "avg_logprob": -0.19153723556004212, "compression_ratio": 1.5321100917431192, "no_speech_prob": 0.00018485311011318117}, {"id": 62, "seek": 19824, "start": 198.68, "end": 202.04000000000002, "text": " In modern versions, it consists in removing some words", "tokens": [50386, 682, 4363, 9606, 11, 309, 14689, 294, 12720, 512, 2283, 50554], "temperature": 0.0, "avg_logprob": -0.14148411852248172, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.0006163217476569116}, {"id": 63, "seek": 19824, "start": 202.04000000000002, "end": 207.44, "text": " from the text, and then training some giant neural net", "tokens": [50554, 490, 264, 2487, 11, 293, 550, 3097, 512, 7410, 18161, 2533, 50824], "temperature": 0.0, "avg_logprob": -0.14148411852248172, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.0006163217476569116}, {"id": 64, "seek": 19824, "start": 207.44, "end": 210.24, "text": " to predict the words that are missing,", "tokens": [50824, 281, 6069, 264, 2283, 300, 366, 5361, 11, 50964], "temperature": 0.0, "avg_logprob": -0.14148411852248172, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.0006163217476569116}, {"id": 65, "seek": 19824, "start": 210.24, "end": 215.16000000000003, "text": " or just merely to tell you whether the text that is here", "tokens": [50964, 420, 445, 17003, 281, 980, 291, 1968, 264, 2487, 300, 307, 510, 51210], "temperature": 0.0, "avg_logprob": -0.14148411852248172, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.0006163217476569116}, {"id": 66, "seek": 19824, "start": 215.16000000000003, "end": 217.24, "text": " is legit or not legit.", "tokens": [51210, 307, 10275, 420, 406, 10275, 13, 51314], "temperature": 0.0, "avg_logprob": -0.14148411852248172, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.0006163217476569116}, {"id": 67, "seek": 19824, "start": 217.24, "end": 219.48000000000002, "text": " That's a different way of doing it.", "tokens": [51314, 663, 311, 257, 819, 636, 295, 884, 309, 13, 51426], "temperature": 0.0, "avg_logprob": -0.14148411852248172, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.0006163217476569116}, {"id": 68, "seek": 19824, "start": 219.48000000000002, "end": 222.28, "text": " So this is how every modern NLP system", "tokens": [51426, 407, 341, 307, 577, 633, 4363, 426, 45196, 1185, 51566], "temperature": 0.0, "avg_logprob": -0.14148411852248172, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.0006163217476569116}, {"id": 69, "seek": 19824, "start": 222.28, "end": 224.84, "text": " over the last four or five years has been trained.", "tokens": [51566, 670, 264, 1036, 1451, 420, 1732, 924, 575, 668, 8895, 13, 51694], "temperature": 0.0, "avg_logprob": -0.14148411852248172, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.0006163217476569116}, {"id": 70, "seek": 22484, "start": 224.84, "end": 228.6, "text": " And that has completely revolutionized not just", "tokens": [50364, 400, 300, 575, 2584, 8894, 1602, 406, 445, 50552], "temperature": 0.0, "avg_logprob": -0.1761200102892789, "compression_ratio": 1.5069767441860464, "no_speech_prob": 0.0003400236018933356}, {"id": 71, "seek": 22484, "start": 228.6, "end": 230.8, "text": " the research in NLP, but also the practice of it.", "tokens": [50552, 264, 2132, 294, 426, 45196, 11, 457, 611, 264, 3124, 295, 309, 13, 50662], "temperature": 0.0, "avg_logprob": -0.1761200102892789, "compression_ratio": 1.5069767441860464, "no_speech_prob": 0.0003400236018933356}, {"id": 72, "seek": 22484, "start": 230.8, "end": 235.16, "text": " So all of translation, content moderation,", "tokens": [50662, 407, 439, 295, 12853, 11, 2701, 49471, 11, 50880], "temperature": 0.0, "avg_logprob": -0.1761200102892789, "compression_ratio": 1.5069767441860464, "no_speech_prob": 0.0003400236018933356}, {"id": 73, "seek": 22484, "start": 235.16, "end": 238.44, "text": " hate speech detection, all that stuff from social networks,", "tokens": [50880, 4700, 6218, 17784, 11, 439, 300, 1507, 490, 2093, 9590, 11, 51044], "temperature": 0.0, "avg_logprob": -0.1761200102892789, "compression_ratio": 1.5069767441860464, "no_speech_prob": 0.0003400236018933356}, {"id": 74, "seek": 22484, "start": 238.44, "end": 240.88, "text": " it all uses this kind of stuff.", "tokens": [51044, 309, 439, 4960, 341, 733, 295, 1507, 13, 51166], "temperature": 0.0, "avg_logprob": -0.1761200102892789, "compression_ratio": 1.5069767441860464, "no_speech_prob": 0.0003400236018933356}, {"id": 75, "seek": 22484, "start": 240.88, "end": 244.28, "text": " And performance went up by a huge amount.", "tokens": [51166, 400, 3389, 1437, 493, 538, 257, 2603, 2372, 13, 51336], "temperature": 0.0, "avg_logprob": -0.1761200102892789, "compression_ratio": 1.5069767441860464, "no_speech_prob": 0.0003400236018933356}, {"id": 76, "seek": 22484, "start": 244.28, "end": 250.88, "text": " OK, so a special case of this is generative LLMs.", "tokens": [51336, 2264, 11, 370, 257, 2121, 1389, 295, 341, 307, 1337, 1166, 441, 43, 26386, 13, 51666], "temperature": 0.0, "avg_logprob": -0.1761200102892789, "compression_ratio": 1.5069767441860464, "no_speech_prob": 0.0003400236018933356}, {"id": 77, "seek": 25088, "start": 251.76, "end": 256.0, "text": " And similar things are used in images and video.", "tokens": [50408, 400, 2531, 721, 366, 1143, 294, 5267, 293, 960, 13, 50620], "temperature": 0.0, "avg_logprob": -0.14735246741253397, "compression_ratio": 1.7280334728033473, "no_speech_prob": 0.0017538175452500582}, {"id": 78, "seek": 25088, "start": 256.0, "end": 259.15999999999997, "text": " And there, the part of the text that you're hiding", "tokens": [50620, 400, 456, 11, 264, 644, 295, 264, 2487, 300, 291, 434, 10596, 50778], "temperature": 0.0, "avg_logprob": -0.14735246741253397, "compression_ratio": 1.7280334728033473, "no_speech_prob": 0.0017538175452500582}, {"id": 79, "seek": 25088, "start": 259.15999999999997, "end": 260.24, "text": " is just the last word.", "tokens": [50778, 307, 445, 264, 1036, 1349, 13, 50832], "temperature": 0.0, "avg_logprob": -0.14735246741253397, "compression_ratio": 1.7280334728033473, "no_speech_prob": 0.0017538175452500582}, {"id": 80, "seek": 25088, "start": 260.24, "end": 262.71999999999997, "text": " So you train a giant neural net to just predict", "tokens": [50832, 407, 291, 3847, 257, 7410, 18161, 2533, 281, 445, 6069, 50956], "temperature": 0.0, "avg_logprob": -0.14735246741253397, "compression_ratio": 1.7280334728033473, "no_speech_prob": 0.0017538175452500582}, {"id": 81, "seek": 25088, "start": 262.71999999999997, "end": 265.4, "text": " the last word in a sequence.", "tokens": [50956, 264, 1036, 1349, 294, 257, 8310, 13, 51090], "temperature": 0.0, "avg_logprob": -0.14735246741253397, "compression_ratio": 1.7280334728033473, "no_speech_prob": 0.0017538175452500582}, {"id": 82, "seek": 25088, "start": 265.4, "end": 268.2, "text": " And then you can use this to produce outputs", "tokens": [51090, 400, 550, 291, 393, 764, 341, 281, 5258, 23930, 51230], "temperature": 0.0, "avg_logprob": -0.14735246741253397, "compression_ratio": 1.7280334728033473, "no_speech_prob": 0.0017538175452500582}, {"id": 83, "seek": 25088, "start": 268.2, "end": 272.32, "text": " auto-regressively, which means you give a window of text,", "tokens": [51230, 8399, 12, 3375, 735, 3413, 11, 597, 1355, 291, 976, 257, 4910, 295, 2487, 11, 51436], "temperature": 0.0, "avg_logprob": -0.14735246741253397, "compression_ratio": 1.7280334728033473, "no_speech_prob": 0.0017538175452500582}, {"id": 84, "seek": 25088, "start": 272.32, "end": 274.36, "text": " you get a system to produce a word,", "tokens": [51436, 291, 483, 257, 1185, 281, 5258, 257, 1349, 11, 51538], "temperature": 0.0, "avg_logprob": -0.14735246741253397, "compression_ratio": 1.7280334728033473, "no_speech_prob": 0.0017538175452500582}, {"id": 85, "seek": 25088, "start": 274.36, "end": 277.36, "text": " and then you shift that word into the input", "tokens": [51538, 293, 550, 291, 5513, 300, 1349, 666, 264, 4846, 51688], "temperature": 0.0, "avg_logprob": -0.14735246741253397, "compression_ratio": 1.7280334728033473, "no_speech_prob": 0.0017538175452500582}, {"id": 86, "seek": 25088, "start": 277.36, "end": 279.4, "text": " by shifting everything by one.", "tokens": [51688, 538, 17573, 1203, 538, 472, 13, 51790], "temperature": 0.0, "avg_logprob": -0.14735246741253397, "compression_ratio": 1.7280334728033473, "no_speech_prob": 0.0017538175452500582}, {"id": 87, "seek": 27940, "start": 279.4, "end": 281.4, "text": " Predict the next, next word, shift that in.", "tokens": [50364, 430, 24945, 264, 958, 11, 958, 1349, 11, 5513, 300, 294, 13, 50464], "temperature": 0.0, "avg_logprob": -0.2140040397644043, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.0006555885192938149}, {"id": 88, "seek": 27940, "start": 281.4, "end": 284.08, "text": " Predict the next, next, next word, shift that in, et cetera.", "tokens": [50464, 430, 24945, 264, 958, 11, 958, 11, 958, 1349, 11, 5513, 300, 294, 11, 1030, 11458, 13, 50598], "temperature": 0.0, "avg_logprob": -0.2140040397644043, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.0006555885192938149}, {"id": 89, "seek": 27940, "start": 284.08, "end": 286.15999999999997, "text": " That's auto-regressive prediction.", "tokens": [50598, 663, 311, 8399, 12, 3375, 22733, 17630, 13, 50702], "temperature": 0.0, "avg_logprob": -0.2140040397644043, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.0006555885192938149}, {"id": 90, "seek": 27940, "start": 286.15999999999997, "end": 288.15999999999997, "text": " It's a major flaw with this approach.", "tokens": [50702, 467, 311, 257, 2563, 13717, 365, 341, 3109, 13, 50802], "temperature": 0.0, "avg_logprob": -0.2140040397644043, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.0006555885192938149}, {"id": 91, "seek": 27940, "start": 288.15999999999997, "end": 292.64, "text": " This is how every single LLM today works.", "tokens": [50802, 639, 307, 577, 633, 2167, 441, 43, 44, 965, 1985, 13, 51026], "temperature": 0.0, "avg_logprob": -0.2140040397644043, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.0006555885192938149}, {"id": 92, "seek": 27940, "start": 292.64, "end": 295.12, "text": " But we should call them auto-regressive LLMs,", "tokens": [51026, 583, 321, 820, 818, 552, 8399, 12, 3375, 22733, 441, 43, 26386, 11, 51150], "temperature": 0.0, "avg_logprob": -0.2140040397644043, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.0006555885192938149}, {"id": 93, "seek": 27940, "start": 295.12, "end": 298.44, "text": " because I think future LLMs are not going to be like this.", "tokens": [51150, 570, 286, 519, 2027, 441, 43, 26386, 366, 406, 516, 281, 312, 411, 341, 13, 51316], "temperature": 0.0, "avg_logprob": -0.2140040397644043, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.0006555885192938149}, {"id": 94, "seek": 27940, "start": 298.44, "end": 301.15999999999997, "text": " But basically, every single one of them, some of which", "tokens": [51316, 583, 1936, 11, 633, 2167, 472, 295, 552, 11, 512, 295, 597, 51452], "temperature": 0.0, "avg_logprob": -0.2140040397644043, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.0006555885192938149}, {"id": 95, "seek": 27940, "start": 301.15999999999997, "end": 303.67999999999995, "text": " you've probably never heard of.", "tokens": [51452, 291, 600, 1391, 1128, 2198, 295, 13, 51578], "temperature": 0.0, "avg_logprob": -0.2140040397644043, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.0006555885192938149}, {"id": 96, "seek": 27940, "start": 303.67999999999995, "end": 307.0, "text": " So the ones from Faire, Blenderbot, Galactica, Lama,", "tokens": [51578, 407, 264, 2306, 490, 479, 9020, 11, 2177, 3216, 18870, 11, 7336, 578, 2262, 11, 441, 2404, 11, 51744], "temperature": 0.0, "avg_logprob": -0.2140040397644043, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.0006555885192938149}, {"id": 97, "seek": 30700, "start": 307.0, "end": 309.52, "text": " Alpeca, which is fine tuning of Lama.", "tokens": [50364, 967, 494, 496, 11, 597, 307, 2489, 15164, 295, 441, 2404, 13, 50490], "temperature": 0.0, "avg_logprob": -0.21344871232003876, "compression_ratio": 1.5197132616487454, "no_speech_prob": 0.0007286785985343158}, {"id": 98, "seek": 30700, "start": 309.52, "end": 313.16, "text": " There is a new one now also.", "tokens": [50490, 821, 307, 257, 777, 472, 586, 611, 13, 50672], "temperature": 0.0, "avg_logprob": -0.21344871232003876, "compression_ratio": 1.5197132616487454, "no_speech_prob": 0.0007286785985343158}, {"id": 99, "seek": 30700, "start": 313.16, "end": 316.12, "text": " Lambda, Bard from Google, Shinshila from DeepMind,", "tokens": [50672, 45691, 11, 26841, 490, 3329, 11, 1160, 1292, 71, 7371, 490, 14895, 44, 471, 11, 50820], "temperature": 0.0, "avg_logprob": -0.21344871232003876, "compression_ratio": 1.5197132616487454, "no_speech_prob": 0.0007286785985343158}, {"id": 100, "seek": 30700, "start": 316.12, "end": 318.32, "text": " Chai GPT, GPT4, et cetera.", "tokens": [50820, 761, 1301, 26039, 51, 11, 26039, 51, 19, 11, 1030, 11458, 13, 50930], "temperature": 0.0, "avg_logprob": -0.21344871232003876, "compression_ratio": 1.5197132616487454, "no_speech_prob": 0.0007286785985343158}, {"id": 101, "seek": 30700, "start": 318.32, "end": 322.28, "text": " They're all auto-regressive LLMs.", "tokens": [50930, 814, 434, 439, 8399, 12, 3375, 22733, 441, 43, 26386, 13, 51128], "temperature": 0.0, "avg_logprob": -0.21344871232003876, "compression_ratio": 1.5197132616487454, "no_speech_prob": 0.0007286785985343158}, {"id": 102, "seek": 30700, "start": 322.28, "end": 326.12, "text": " And they train on gigantic amounts of data.", "tokens": [51128, 400, 436, 3847, 322, 26800, 11663, 295, 1412, 13, 51320], "temperature": 0.0, "avg_logprob": -0.21344871232003876, "compression_ratio": 1.5197132616487454, "no_speech_prob": 0.0007286785985343158}, {"id": 103, "seek": 30700, "start": 326.12, "end": 328.84, "text": " So we're talking one trillion tokens or something like this.", "tokens": [51320, 407, 321, 434, 1417, 472, 18723, 22667, 420, 746, 411, 341, 13, 51456], "temperature": 0.0, "avg_logprob": -0.21344871232003876, "compression_ratio": 1.5197132616487454, "no_speech_prob": 0.0007286785985343158}, {"id": 104, "seek": 30700, "start": 328.84, "end": 331.48, "text": " It would take a human reading eight hours a day,", "tokens": [51456, 467, 576, 747, 257, 1952, 3760, 3180, 2496, 257, 786, 11, 51588], "temperature": 0.0, "avg_logprob": -0.21344871232003876, "compression_ratio": 1.5197132616487454, "no_speech_prob": 0.0007286785985343158}, {"id": 105, "seek": 30700, "start": 331.48, "end": 334.4, "text": " something like 22,000 years to read this.", "tokens": [51588, 746, 411, 5853, 11, 1360, 924, 281, 1401, 341, 13, 51734], "temperature": 0.0, "avg_logprob": -0.21344871232003876, "compression_ratio": 1.5197132616487454, "no_speech_prob": 0.0007286785985343158}, {"id": 106, "seek": 30700, "start": 334.4, "end": 336.8, "text": " So obviously, those things can swallow a lot more", "tokens": [51734, 407, 2745, 11, 729, 721, 393, 20099, 257, 688, 544, 51854], "temperature": 0.0, "avg_logprob": -0.21344871232003876, "compression_ratio": 1.5197132616487454, "no_speech_prob": 0.0007286785985343158}, {"id": 107, "seek": 33680, "start": 336.8, "end": 339.56, "text": " and digest a lot more data than any human.", "tokens": [50364, 293, 13884, 257, 688, 544, 1412, 813, 604, 1952, 13, 50502], "temperature": 0.0, "avg_logprob": -0.1683990264607367, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.0002652432885952294}, {"id": 108, "seek": 33680, "start": 339.56, "end": 342.40000000000003, "text": " And the performance is nothing short of amazing.", "tokens": [50502, 400, 264, 3389, 307, 1825, 2099, 295, 2243, 13, 50644], "temperature": 0.0, "avg_logprob": -0.1683990264607367, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.0002652432885952294}, {"id": 109, "seek": 33680, "start": 342.40000000000003, "end": 344.36, "text": " But they do make stupid mistakes.", "tokens": [50644, 583, 436, 360, 652, 6631, 8038, 13, 50742], "temperature": 0.0, "avg_logprob": -0.1683990264607367, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.0002652432885952294}, {"id": 110, "seek": 33680, "start": 344.36, "end": 345.88, "text": " They are extremely fluent.", "tokens": [50742, 814, 366, 4664, 40799, 13, 50818], "temperature": 0.0, "avg_logprob": -0.1683990264607367, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.0002652432885952294}, {"id": 111, "seek": 33680, "start": 345.88, "end": 348.72, "text": " So we can use them to generate text.", "tokens": [50818, 407, 321, 393, 764, 552, 281, 8460, 2487, 13, 50960], "temperature": 0.0, "avg_logprob": -0.1683990264607367, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.0002652432885952294}, {"id": 112, "seek": 33680, "start": 348.72, "end": 355.2, "text": " But they make factual errors, logical errors, inconsistencies.", "tokens": [50960, 583, 436, 652, 48029, 13603, 11, 14978, 13603, 11, 22039, 4821, 4629, 13, 51284], "temperature": 0.0, "avg_logprob": -0.1683990264607367, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.0002652432885952294}, {"id": 113, "seek": 33680, "start": 355.2, "end": 356.76, "text": " They have limited reasoning ability.", "tokens": [51284, 814, 362, 5567, 21577, 3485, 13, 51362], "temperature": 0.0, "avg_logprob": -0.1683990264607367, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.0002652432885952294}, {"id": 114, "seek": 33680, "start": 356.76, "end": 359.8, "text": " There is no way to control for things like toxicity", "tokens": [51362, 821, 307, 572, 636, 281, 1969, 337, 721, 411, 45866, 51514], "temperature": 0.0, "avg_logprob": -0.1683990264607367, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.0002652432885952294}, {"id": 115, "seek": 33680, "start": 359.8, "end": 361.56, "text": " and stuff like that.", "tokens": [51514, 293, 1507, 411, 300, 13, 51602], "temperature": 0.0, "avg_logprob": -0.1683990264607367, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.0002652432885952294}, {"id": 116, "seek": 33680, "start": 361.56, "end": 364.72, "text": " And they really have no knowledge of the underlying reality,", "tokens": [51602, 400, 436, 534, 362, 572, 3601, 295, 264, 14217, 4103, 11, 51760], "temperature": 0.0, "avg_logprob": -0.1683990264607367, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.0002652432885952294}, {"id": 117, "seek": 36472, "start": 364.72, "end": 368.44000000000005, "text": " except in one case, because, of course, they only", "tokens": [50364, 3993, 294, 472, 1389, 11, 570, 11, 295, 1164, 11, 436, 787, 50550], "temperature": 0.0, "avg_logprob": -0.1893426513671875, "compression_ratio": 1.9067796610169492, "no_speech_prob": 0.0008556826505810022}, {"id": 118, "seek": 36472, "start": 368.44000000000005, "end": 370.64000000000004, "text": " train from text, except in one case.", "tokens": [50550, 3847, 490, 2487, 11, 3993, 294, 472, 1389, 13, 50660], "temperature": 0.0, "avg_logprob": -0.1893426513671875, "compression_ratio": 1.9067796610169492, "no_speech_prob": 0.0008556826505810022}, {"id": 119, "seek": 36472, "start": 370.64000000000004, "end": 374.96000000000004, "text": " And that case is code generation.", "tokens": [50660, 400, 300, 1389, 307, 3089, 5125, 13, 50876], "temperature": 0.0, "avg_logprob": -0.1893426513671875, "compression_ratio": 1.9067796610169492, "no_speech_prob": 0.0008556826505810022}, {"id": 120, "seek": 36472, "start": 374.96000000000004, "end": 377.48, "text": " And they work really, really well for code generation.", "tokens": [50876, 400, 436, 589, 534, 11, 534, 731, 337, 3089, 5125, 13, 51002], "temperature": 0.0, "avg_logprob": -0.1893426513671875, "compression_ratio": 1.9067796610169492, "no_speech_prob": 0.0008556826505810022}, {"id": 121, "seek": 36472, "start": 377.48, "end": 380.0, "text": " And the reason they work well is that the underlying reality", "tokens": [51002, 400, 264, 1778, 436, 589, 731, 307, 300, 264, 14217, 4103, 51128], "temperature": 0.0, "avg_logprob": -0.1893426513671875, "compression_ratio": 1.9067796610169492, "no_speech_prob": 0.0008556826505810022}, {"id": 122, "seek": 36472, "start": 380.0, "end": 381.16, "text": " of code is very simple.", "tokens": [51128, 295, 3089, 307, 588, 2199, 13, 51186], "temperature": 0.0, "avg_logprob": -0.1893426513671875, "compression_ratio": 1.9067796610169492, "no_speech_prob": 0.0008556826505810022}, {"id": 123, "seek": 36472, "start": 381.16, "end": 381.96000000000004, "text": " It's deterministic.", "tokens": [51186, 467, 311, 15957, 3142, 13, 51226], "temperature": 0.0, "avg_logprob": -0.1893426513671875, "compression_ratio": 1.9067796610169492, "no_speech_prob": 0.0008556826505810022}, {"id": 124, "seek": 36472, "start": 381.96000000000004, "end": 384.36, "text": " It's just the state of variables of a program.", "tokens": [51226, 467, 311, 445, 264, 1785, 295, 9102, 295, 257, 1461, 13, 51346], "temperature": 0.0, "avg_logprob": -0.1893426513671875, "compression_ratio": 1.9067796610169492, "no_speech_prob": 0.0008556826505810022}, {"id": 125, "seek": 36472, "start": 384.36, "end": 387.56, "text": " And so that's fully observable, deterministic,", "tokens": [51346, 400, 370, 300, 311, 4498, 9951, 712, 11, 15957, 3142, 11, 51506], "temperature": 0.0, "avg_logprob": -0.1893426513671875, "compression_ratio": 1.9067796610169492, "no_speech_prob": 0.0008556826505810022}, {"id": 126, "seek": 36472, "start": 387.56, "end": 388.08000000000004, "text": " and everything.", "tokens": [51506, 293, 1203, 13, 51532], "temperature": 0.0, "avg_logprob": -0.1893426513671875, "compression_ratio": 1.9067796610169492, "no_speech_prob": 0.0008556826505810022}, {"id": 127, "seek": 36472, "start": 388.08000000000004, "end": 391.68, "text": " So it works really well.", "tokens": [51532, 407, 309, 1985, 534, 731, 13, 51712], "temperature": 0.0, "avg_logprob": -0.1893426513671875, "compression_ratio": 1.9067796610169492, "no_speech_prob": 0.0008556826505810022}, {"id": 128, "seek": 36472, "start": 391.68, "end": 394.20000000000005, "text": " And they can generate fluent text.", "tokens": [51712, 400, 436, 393, 8460, 40799, 2487, 13, 51838], "temperature": 0.0, "avg_logprob": -0.1893426513671875, "compression_ratio": 1.9067796610169492, "no_speech_prob": 0.0008556826505810022}, {"id": 129, "seek": 39420, "start": 394.2, "end": 396.56, "text": " But in this particular case, this is a joke", "tokens": [50364, 583, 294, 341, 1729, 1389, 11, 341, 307, 257, 7647, 50482], "temperature": 0.0, "avg_logprob": -0.2483835403735821, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.00221461383625865}, {"id": 130, "seek": 39420, "start": 396.56, "end": 399.28, "text": " that my colleagues did on me.", "tokens": [50482, 300, 452, 7734, 630, 322, 385, 13, 50618], "temperature": 0.0, "avg_logprob": -0.2483835403735821, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.00221461383625865}, {"id": 131, "seek": 39420, "start": 399.28, "end": 401.64, "text": " It's completely made up.", "tokens": [50618, 467, 311, 2584, 1027, 493, 13, 50736], "temperature": 0.0, "avg_logprob": -0.2483835403735821, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.00221461383625865}, {"id": 132, "seek": 39420, "start": 401.64, "end": 406.8, "text": " I never actually did a rap album.", "tokens": [50736, 286, 1128, 767, 630, 257, 5099, 6030, 13, 50994], "temperature": 0.0, "avg_logprob": -0.2483835403735821, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.00221461383625865}, {"id": 133, "seek": 39420, "start": 406.8, "end": 408.48, "text": " Raw personal.", "tokens": [50994, 23732, 2973, 13, 51078], "temperature": 0.0, "avg_logprob": -0.2483835403735821, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.00221461383625865}, {"id": 134, "seek": 39420, "start": 408.48, "end": 411.48, "text": " Yeah.", "tokens": [51078, 865, 13, 51228], "temperature": 0.0, "avg_logprob": -0.2483835403735821, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.00221461383625865}, {"id": 135, "seek": 39420, "start": 411.48, "end": 414.0, "text": " I asked them if they, I don't actually like rap that well.", "tokens": [51228, 286, 2351, 552, 498, 436, 11, 286, 500, 380, 767, 411, 5099, 300, 731, 13, 51354], "temperature": 0.0, "avg_logprob": -0.2483835403735821, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.00221461383625865}, {"id": 136, "seek": 39420, "start": 414.0, "end": 416.52, "text": " So I'm over a jazz person.", "tokens": [51354, 407, 286, 478, 670, 257, 15066, 954, 13, 51480], "temperature": 0.0, "avg_logprob": -0.2483835403735821, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.00221461383625865}, {"id": 137, "seek": 39420, "start": 416.52, "end": 418.4, "text": " So I asked them to do the same thing with jazz.", "tokens": [51480, 407, 286, 2351, 552, 281, 360, 264, 912, 551, 365, 15066, 13, 51574], "temperature": 0.0, "avg_logprob": -0.2483835403735821, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.00221461383625865}, {"id": 138, "seek": 39420, "start": 418.4, "end": 422.48, "text": " And they say, there's not enough training data.", "tokens": [51574, 400, 436, 584, 11, 456, 311, 406, 1547, 3097, 1412, 13, 51778], "temperature": 0.0, "avg_logprob": -0.2483835403735821, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.00221461383625865}, {"id": 139, "seek": 42248, "start": 422.48, "end": 425.48, "text": " And I cried.", "tokens": [50364, 400, 286, 16266, 13, 50514], "temperature": 0.0, "avg_logprob": -0.226557555707913, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.0005701783229596913}, {"id": 140, "seek": 42248, "start": 425.48, "end": 426.72, "text": " OK, so what are they good for?", "tokens": [50514, 2264, 11, 370, 437, 366, 436, 665, 337, 30, 50576], "temperature": 0.0, "avg_logprob": -0.226557555707913, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.0005701783229596913}, {"id": 141, "seek": 42248, "start": 426.72, "end": 429.32, "text": " They're good for writing assistance, generating", "tokens": [50576, 814, 434, 665, 337, 3579, 9683, 11, 17746, 50706], "temperature": 0.0, "avg_logprob": -0.226557555707913, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.0005701783229596913}, {"id": 142, "seek": 42248, "start": 429.32, "end": 432.8, "text": " first draft, producing a style.", "tokens": [50706, 700, 11206, 11, 10501, 257, 3758, 13, 50880], "temperature": 0.0, "avg_logprob": -0.226557555707913, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.0005701783229596913}, {"id": 143, "seek": 42248, "start": 432.8, "end": 435.8, "text": " Code writing assistance, obviously, very efficient for that.", "tokens": [50880, 15549, 3579, 9683, 11, 2745, 11, 588, 7148, 337, 300, 13, 51030], "temperature": 0.0, "avg_logprob": -0.226557555707913, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.0005701783229596913}, {"id": 144, "seek": 42248, "start": 435.8, "end": 438.92, "text": " They're not good for producing factual and consistent answers", "tokens": [51030, 814, 434, 406, 665, 337, 10501, 48029, 293, 8398, 6338, 51186], "temperature": 0.0, "avg_logprob": -0.226557555707913, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.0005701783229596913}, {"id": 145, "seek": 42248, "start": 438.92, "end": 441.52000000000004, "text": " because of aducinations.", "tokens": [51186, 570, 295, 614, 1311, 10325, 13, 51316], "temperature": 0.0, "avg_logprob": -0.226557555707913, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.0005701783229596913}, {"id": 146, "seek": 42248, "start": 441.52000000000004, "end": 443.36, "text": " And they're not good for taking into account", "tokens": [51316, 400, 436, 434, 406, 665, 337, 1940, 666, 2696, 51408], "temperature": 0.0, "avg_logprob": -0.226557555707913, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.0005701783229596913}, {"id": 147, "seek": 42248, "start": 443.36, "end": 444.8, "text": " recent information, because you need", "tokens": [51408, 5162, 1589, 11, 570, 291, 643, 51480], "temperature": 0.0, "avg_logprob": -0.226557555707913, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.0005701783229596913}, {"id": 148, "seek": 42248, "start": 444.8, "end": 447.92, "text": " to retrain the entire system to take into account yesterday", "tokens": [51480, 281, 1533, 7146, 264, 2302, 1185, 281, 747, 666, 2696, 5186, 51636], "temperature": 0.0, "avg_logprob": -0.226557555707913, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.0005701783229596913}, {"id": 149, "seek": 44792, "start": 447.92, "end": 449.08000000000004, "text": " in real time.", "tokens": [50364, 294, 957, 565, 13, 50422], "temperature": 0.0, "avg_logprob": -0.17790572623896406, "compression_ratio": 1.7032520325203253, "no_speech_prob": 0.0029329147655516863}, {"id": 150, "seek": 44792, "start": 449.08000000000004, "end": 452.56, "text": " And that's just not practical.", "tokens": [50422, 400, 300, 311, 445, 406, 8496, 13, 50596], "temperature": 0.0, "avg_logprob": -0.17790572623896406, "compression_ratio": 1.7032520325203253, "no_speech_prob": 0.0029329147655516863}, {"id": 151, "seek": 44792, "start": 452.56, "end": 455.8, "text": " They don't behave properly, or at least they're", "tokens": [50596, 814, 500, 380, 15158, 6108, 11, 420, 412, 1935, 436, 434, 50758], "temperature": 0.0, "avg_logprob": -0.17790572623896406, "compression_ratio": 1.7032520325203253, "no_speech_prob": 0.0029329147655516863}, {"id": 152, "seek": 44792, "start": 455.8, "end": 457.68, "text": " hard to control to do so.", "tokens": [50758, 1152, 281, 1969, 281, 360, 370, 13, 50852], "temperature": 0.0, "avg_logprob": -0.17790572623896406, "compression_ratio": 1.7032520325203253, "no_speech_prob": 0.0029329147655516863}, {"id": 153, "seek": 44792, "start": 457.68, "end": 458.76, "text": " They don't do reasoning.", "tokens": [50852, 814, 500, 380, 360, 21577, 13, 50906], "temperature": 0.0, "avg_logprob": -0.17790572623896406, "compression_ratio": 1.7032520325203253, "no_speech_prob": 0.0029329147655516863}, {"id": 154, "seek": 44792, "start": 458.76, "end": 459.52000000000004, "text": " They don't do planning.", "tokens": [50906, 814, 500, 380, 360, 5038, 13, 50944], "temperature": 0.0, "avg_logprob": -0.17790572623896406, "compression_ratio": 1.7032520325203253, "no_speech_prob": 0.0029329147655516863}, {"id": 155, "seek": 44792, "start": 459.52000000000004, "end": 462.88, "text": " They don't do math, as we saw this morning.", "tokens": [50944, 814, 500, 380, 360, 5221, 11, 382, 321, 1866, 341, 2446, 13, 51112], "temperature": 0.0, "avg_logprob": -0.17790572623896406, "compression_ratio": 1.7032520325203253, "no_speech_prob": 0.0029329147655516863}, {"id": 156, "seek": 44792, "start": 462.88, "end": 464.44, "text": " They're being modified to use tools,", "tokens": [51112, 814, 434, 885, 15873, 281, 764, 3873, 11, 51190], "temperature": 0.0, "avg_logprob": -0.17790572623896406, "compression_ratio": 1.7032520325203253, "no_speech_prob": 0.0029329147655516863}, {"id": 157, "seek": 44792, "start": 464.44, "end": 467.44, "text": " such as search engines, calculators, stuff like that.", "tokens": [51190, 1270, 382, 3164, 12982, 11, 4322, 3391, 11, 1507, 411, 300, 13, 51340], "temperature": 0.0, "avg_logprob": -0.17790572623896406, "compression_ratio": 1.7032520325203253, "no_speech_prob": 0.0029329147655516863}, {"id": 158, "seek": 44792, "start": 467.44, "end": 474.12, "text": " But currently, it's kind of like using duct tape and staples.", "tokens": [51340, 583, 4362, 11, 309, 311, 733, 295, 411, 1228, 25954, 7314, 293, 11135, 2622, 13, 51674], "temperature": 0.0, "avg_logprob": -0.17790572623896406, "compression_ratio": 1.7032520325203253, "no_speech_prob": 0.0029329147655516863}, {"id": 159, "seek": 44792, "start": 474.12, "end": 476.36, "text": " And we're easily fooled by their fluency into thinking", "tokens": [51674, 400, 321, 434, 3612, 33372, 538, 641, 5029, 3020, 666, 1953, 51786], "temperature": 0.0, "avg_logprob": -0.17790572623896406, "compression_ratio": 1.7032520325203253, "no_speech_prob": 0.0029329147655516863}, {"id": 160, "seek": 47636, "start": 476.36, "end": 479.6, "text": " that they are smart, but they are not that smart.", "tokens": [50364, 300, 436, 366, 4069, 11, 457, 436, 366, 406, 300, 4069, 13, 50526], "temperature": 0.0, "avg_logprob": -0.1351746589906754, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.00029130076291039586}, {"id": 161, "seek": 47636, "start": 479.6, "end": 482.08000000000004, "text": " Now, there is a major flaw with this autoregressive", "tokens": [50526, 823, 11, 456, 307, 257, 2563, 13717, 365, 341, 1476, 418, 3091, 488, 50650], "temperature": 0.0, "avg_logprob": -0.1351746589906754, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.00029130076291039586}, {"id": 162, "seek": 47636, "start": 482.08000000000004, "end": 485.56, "text": " generation, which is that it's an exponentially", "tokens": [50650, 5125, 11, 597, 307, 300, 309, 311, 364, 37330, 50824], "temperature": 0.0, "avg_logprob": -0.1351746589906754, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.00029130076291039586}, {"id": 163, "seek": 47636, "start": 485.56, "end": 487.36, "text": " diverging diffusion process.", "tokens": [50824, 18558, 3249, 25242, 1399, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1351746589906754, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.00029130076291039586}, {"id": 164, "seek": 47636, "start": 487.36, "end": 490.84000000000003, "text": " So if there is the probability, e,", "tokens": [50914, 407, 498, 456, 307, 264, 8482, 11, 308, 11, 51088], "temperature": 0.0, "avg_logprob": -0.1351746589906754, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.00029130076291039586}, {"id": 165, "seek": 47636, "start": 490.84000000000003, "end": 493.92, "text": " for every token that is produced to be outside", "tokens": [51088, 337, 633, 14862, 300, 307, 7126, 281, 312, 2380, 51242], "temperature": 0.0, "avg_logprob": -0.1351746589906754, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.00029130076291039586}, {"id": 166, "seek": 47636, "start": 493.92, "end": 495.56, "text": " of the set of correct answers, let's", "tokens": [51242, 295, 264, 992, 295, 3006, 6338, 11, 718, 311, 51324], "temperature": 0.0, "avg_logprob": -0.1351746589906754, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.00029130076291039586}, {"id": 167, "seek": 47636, "start": 495.56, "end": 498.44, "text": " assume that errors are being independent,", "tokens": [51324, 6552, 300, 13603, 366, 885, 6695, 11, 51468], "temperature": 0.0, "avg_logprob": -0.1351746589906754, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.00029130076291039586}, {"id": 168, "seek": 47636, "start": 498.44, "end": 500.44, "text": " then if we generate a sequence of n tokens,", "tokens": [51468, 550, 498, 321, 8460, 257, 8310, 295, 297, 22667, 11, 51568], "temperature": 0.0, "avg_logprob": -0.1351746589906754, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.00029130076291039586}, {"id": 169, "seek": 47636, "start": 500.44, "end": 503.44, "text": " the probability for that sequence to be correct", "tokens": [51568, 264, 8482, 337, 300, 8310, 281, 312, 3006, 51718], "temperature": 0.0, "avg_logprob": -0.1351746589906754, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.00029130076291039586}, {"id": 170, "seek": 47636, "start": 503.44, "end": 505.08000000000004, "text": " is 1 minus e to the power n.", "tokens": [51718, 307, 502, 3175, 308, 281, 264, 1347, 297, 13, 51800], "temperature": 0.0, "avg_logprob": -0.1351746589906754, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.00029130076291039586}, {"id": 171, "seek": 50508, "start": 505.08, "end": 508.47999999999996, "text": " And that decreases exponentially.", "tokens": [50364, 400, 300, 24108, 37330, 13, 50534], "temperature": 0.0, "avg_logprob": -0.18772783849993321, "compression_ratio": 1.5960784313725491, "no_speech_prob": 0.00013131642481312156}, {"id": 172, "seek": 50508, "start": 508.47999999999996, "end": 510.44, "text": " So those things just don't work.", "tokens": [50534, 407, 729, 721, 445, 500, 380, 589, 13, 50632], "temperature": 0.0, "avg_logprob": -0.18772783849993321, "compression_ratio": 1.5960784313725491, "no_speech_prob": 0.00013131642481312156}, {"id": 173, "seek": 50508, "start": 510.44, "end": 511.84, "text": " They just don't work.", "tokens": [50632, 814, 445, 500, 380, 589, 13, 50702], "temperature": 0.0, "avg_logprob": -0.18772783849993321, "compression_ratio": 1.5960784313725491, "no_speech_prob": 0.00013131642481312156}, {"id": 174, "seek": 50508, "start": 511.84, "end": 513.96, "text": " My prediction is that five years from now,", "tokens": [50702, 1222, 17630, 307, 300, 1732, 924, 490, 586, 11, 50808], "temperature": 0.0, "avg_logprob": -0.18772783849993321, "compression_ratio": 1.5960784313725491, "no_speech_prob": 0.00013131642481312156}, {"id": 175, "seek": 50508, "start": 513.96, "end": 517.84, "text": " nobody in that right mind would be using autoregressive LNMs.", "tokens": [50808, 5079, 294, 300, 558, 1575, 576, 312, 1228, 1476, 418, 3091, 488, 441, 45, 26386, 13, 51002], "temperature": 0.0, "avg_logprob": -0.18772783849993321, "compression_ratio": 1.5960784313725491, "no_speech_prob": 0.00013131642481312156}, {"id": 176, "seek": 50508, "start": 517.84, "end": 521.0, "text": " It's just a bad phase.", "tokens": [51002, 467, 311, 445, 257, 1578, 5574, 13, 51160], "temperature": 0.0, "avg_logprob": -0.18772783849993321, "compression_ratio": 1.5960784313725491, "no_speech_prob": 0.00013131642481312156}, {"id": 177, "seek": 50508, "start": 521.0, "end": 521.96, "text": " They are useful, though.", "tokens": [51160, 814, 366, 4420, 11, 1673, 13, 51208], "temperature": 0.0, "avg_logprob": -0.18772783849993321, "compression_ratio": 1.5960784313725491, "no_speech_prob": 0.00013131642481312156}, {"id": 178, "seek": 50508, "start": 521.96, "end": 522.6, "text": " They're very useful.", "tokens": [51208, 814, 434, 588, 4420, 13, 51240], "temperature": 0.0, "avg_logprob": -0.18772783849993321, "compression_ratio": 1.5960784313725491, "no_speech_prob": 0.00013131642481312156}, {"id": 179, "seek": 50508, "start": 525.36, "end": 527.6, "text": " So as I said, they have a constant number", "tokens": [51378, 407, 382, 286, 848, 11, 436, 362, 257, 5754, 1230, 51490], "temperature": 0.0, "avg_logprob": -0.18772783849993321, "compression_ratio": 1.5960784313725491, "no_speech_prob": 0.00013131642481312156}, {"id": 180, "seek": 50508, "start": 527.6, "end": 529.64, "text": " of computational steps between input and output", "tokens": [51490, 295, 28270, 4439, 1296, 4846, 293, 5598, 51592], "temperature": 0.0, "avg_logprob": -0.18772783849993321, "compression_ratio": 1.5960784313725491, "no_speech_prob": 0.00013131642481312156}, {"id": 181, "seek": 50508, "start": 529.64, "end": 532.48, "text": " for each token generated.", "tokens": [51592, 337, 1184, 14862, 10833, 13, 51734], "temperature": 0.0, "avg_logprob": -0.18772783849993321, "compression_ratio": 1.5960784313725491, "no_speech_prob": 0.00013131642481312156}, {"id": 182, "seek": 50508, "start": 532.48, "end": 534.88, "text": " They do not reason and plan.", "tokens": [51734, 814, 360, 406, 1778, 293, 1393, 13, 51854], "temperature": 0.0, "avg_logprob": -0.18772783849993321, "compression_ratio": 1.5960784313725491, "no_speech_prob": 0.00013131642481312156}, {"id": 183, "seek": 53488, "start": 534.92, "end": 537.92, "text": " Jake Browning, who will be talking Wednesday,", "tokens": [50366, 15822, 8030, 278, 11, 567, 486, 312, 1417, 10579, 11, 50516], "temperature": 0.0, "avg_logprob": -0.18814622093649472, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.0010000726906582713}, {"id": 184, "seek": 53488, "start": 537.92, "end": 540.28, "text": " and I wrote a philosophy paper.", "tokens": [50516, 293, 286, 4114, 257, 10675, 3035, 13, 50634], "temperature": 0.0, "avg_logprob": -0.18814622093649472, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.0010000726906582713}, {"id": 185, "seek": 53488, "start": 540.28, "end": 544.56, "text": " I mean, he wrote it on the fact that there", "tokens": [50634, 286, 914, 11, 415, 4114, 309, 322, 264, 1186, 300, 456, 50848], "temperature": 0.0, "avg_logprob": -0.18814622093649472, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.0010000726906582713}, {"id": 186, "seek": 53488, "start": 544.56, "end": 549.32, "text": " are limitations to the purported intelligence", "tokens": [50848, 366, 15705, 281, 264, 1864, 2707, 292, 7599, 51086], "temperature": 0.0, "avg_logprob": -0.18814622093649472, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.0010000726906582713}, {"id": 187, "seek": 53488, "start": 549.32, "end": 552.24, "text": " of systems that are purely trained from text,", "tokens": [51086, 295, 3652, 300, 366, 17491, 8895, 490, 2487, 11, 51232], "temperature": 0.0, "avg_logprob": -0.18814622093649472, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.0010000726906582713}, {"id": 188, "seek": 53488, "start": 552.24, "end": 557.4, "text": " because I would argue that most of human knowledge", "tokens": [51232, 570, 286, 576, 9695, 300, 881, 295, 1952, 3601, 51490], "temperature": 0.0, "avg_logprob": -0.18814622093649472, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.0010000726906582713}, {"id": 189, "seek": 53488, "start": 557.4, "end": 558.8, "text": " is not textual.", "tokens": [51490, 307, 406, 2487, 901, 13, 51560], "temperature": 0.0, "avg_logprob": -0.18814622093649472, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.0010000726906582713}, {"id": 190, "seek": 53488, "start": 558.8, "end": 560.36, "text": " I mean, certainly most of what babies", "tokens": [51560, 286, 914, 11, 3297, 881, 295, 437, 10917, 51638], "temperature": 0.0, "avg_logprob": -0.18814622093649472, "compression_ratio": 1.5388349514563107, "no_speech_prob": 0.0010000726906582713}, {"id": 191, "seek": 56036, "start": 561.32, "end": 565.16, "text": " are on before six months is non-textual.", "tokens": [50412, 366, 322, 949, 2309, 2493, 307, 2107, 12, 25111, 901, 13, 50604], "temperature": 0.0, "avg_logprob": -0.2057801793130596, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.0012060515582561493}, {"id": 192, "seek": 56036, "start": 565.16, "end": 567.72, "text": " And everything that animals learn is non-textual.", "tokens": [50604, 400, 1203, 300, 4882, 1466, 307, 2107, 12, 25111, 901, 13, 50732], "temperature": 0.0, "avg_logprob": -0.2057801793130596, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.0012060515582561493}, {"id": 193, "seek": 56036, "start": 567.72, "end": 574.44, "text": " So that knowledge is still unattainable to current AI", "tokens": [50732, 407, 300, 3601, 307, 920, 47316, 491, 712, 281, 2190, 7318, 51068], "temperature": 0.0, "avg_logprob": -0.2057801793130596, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.0012060515582561493}, {"id": 194, "seek": 56036, "start": 574.44, "end": 576.12, "text": " systems.", "tokens": [51068, 3652, 13, 51152], "temperature": 0.0, "avg_logprob": -0.2057801793130596, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.0012060515582561493}, {"id": 195, "seek": 56036, "start": 576.12, "end": 579.08, "text": " So how do we get machines to understand how the world works", "tokens": [51152, 407, 577, 360, 321, 483, 8379, 281, 1223, 577, 264, 1002, 1985, 51300], "temperature": 0.0, "avg_logprob": -0.2057801793130596, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.0012060515582561493}, {"id": 196, "seek": 56036, "start": 579.08, "end": 581.24, "text": " and predict the consequences of their actions?", "tokens": [51300, 293, 6069, 264, 10098, 295, 641, 5909, 30, 51408], "temperature": 0.0, "avg_logprob": -0.2057801793130596, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.0012060515582561493}, {"id": 197, "seek": 56036, "start": 581.24, "end": 583.0, "text": " All the limitations have been pointed out", "tokens": [51408, 1057, 264, 15705, 362, 668, 10932, 484, 51496], "temperature": 0.0, "avg_logprob": -0.2057801793130596, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.0012060515582561493}, {"id": 198, "seek": 56036, "start": 583.0, "end": 587.5600000000001, "text": " by a number of different papers, including one", "tokens": [51496, 538, 257, 1230, 295, 819, 10577, 11, 3009, 472, 51724], "temperature": 0.0, "avg_logprob": -0.2057801793130596, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.0012060515582561493}, {"id": 199, "seek": 58756, "start": 587.56, "end": 592.8399999999999, "text": " by the MIT crowd, that fluency is really not", "tokens": [50364, 538, 264, 13100, 6919, 11, 300, 5029, 3020, 307, 534, 406, 50628], "temperature": 0.0, "avg_logprob": -0.2757534270590924, "compression_ratio": 1.5343137254901962, "no_speech_prob": 0.00040431355591863394}, {"id": 200, "seek": 58756, "start": 592.8399999999999, "end": 597.04, "text": " the same as thinking.", "tokens": [50628, 264, 912, 382, 1953, 13, 50838], "temperature": 0.0, "avg_logprob": -0.2757534270590924, "compression_ratio": 1.5343137254901962, "no_speech_prob": 0.00040431355591863394}, {"id": 201, "seek": 58756, "start": 597.04, "end": 599.0799999999999, "text": " And basically, you could argue for the fact", "tokens": [50838, 400, 1936, 11, 291, 727, 9695, 337, 264, 1186, 50940], "temperature": 0.0, "avg_logprob": -0.2757534270590924, "compression_ratio": 1.5343137254901962, "no_speech_prob": 0.00040431355591863394}, {"id": 202, "seek": 58756, "start": 599.0799999999999, "end": 602.52, "text": " that what LLMs are good for is perhaps modeling", "tokens": [50940, 300, 437, 441, 43, 26386, 366, 665, 337, 307, 4317, 15983, 51112], "temperature": 0.0, "avg_logprob": -0.2757534270590924, "compression_ratio": 1.5343137254901962, "no_speech_prob": 0.00040431355591863394}, {"id": 203, "seek": 58756, "start": 602.52, "end": 605.2399999999999, "text": " the Browning-Key and Boracay areas,", "tokens": [51112, 264, 8030, 278, 12, 42, 2030, 293, 13739, 326, 320, 3179, 11, 51248], "temperature": 0.0, "avg_logprob": -0.2757534270590924, "compression_ratio": 1.5343137254901962, "no_speech_prob": 0.00040431355591863394}, {"id": 204, "seek": 58756, "start": 605.2399999999999, "end": 607.04, "text": " but not much else in the brain.", "tokens": [51248, 457, 406, 709, 1646, 294, 264, 3567, 13, 51338], "temperature": 0.0, "avg_logprob": -0.2757534270590924, "compression_ratio": 1.5343137254901962, "no_speech_prob": 0.00040431355591863394}, {"id": 205, "seek": 58756, "start": 607.04, "end": 611.0799999999999, "text": " And that's like tiny little areas on the side of the brain.", "tokens": [51338, 400, 300, 311, 411, 5870, 707, 3179, 322, 264, 1252, 295, 264, 3567, 13, 51540], "temperature": 0.0, "avg_logprob": -0.2757534270590924, "compression_ratio": 1.5343137254901962, "no_speech_prob": 0.00040431355591863394}, {"id": 206, "seek": 58756, "start": 614.2399999999999, "end": 617.28, "text": " So we need something else.", "tokens": [51698, 407, 321, 643, 746, 1646, 13, 51850], "temperature": 0.0, "avg_logprob": -0.2757534270590924, "compression_ratio": 1.5343137254901962, "no_speech_prob": 0.00040431355591863394}, {"id": 207, "seek": 61728, "start": 617.3199999999999, "end": 619.6, "text": " What are we missing?", "tokens": [50366, 708, 366, 321, 5361, 30, 50480], "temperature": 0.0, "avg_logprob": -0.22918864900031977, "compression_ratio": 1.510948905109489, "no_speech_prob": 0.0016953622689470649}, {"id": 208, "seek": 61728, "start": 619.6, "end": 621.04, "text": " This is a chart that I like to show.", "tokens": [50480, 639, 307, 257, 6927, 300, 286, 411, 281, 855, 13, 50552], "temperature": 0.0, "avg_logprob": -0.22918864900031977, "compression_ratio": 1.510948905109489, "no_speech_prob": 0.0016953622689470649}, {"id": 209, "seek": 61728, "start": 621.04, "end": 623.04, "text": " Oops, the animation is bad.", "tokens": [50552, 21726, 11, 264, 9603, 307, 1578, 13, 50652], "temperature": 0.0, "avg_logprob": -0.22918864900031977, "compression_ratio": 1.510948905109489, "no_speech_prob": 0.0016953622689470649}, {"id": 210, "seek": 61728, "start": 623.04, "end": 627.68, "text": " But it was put together by Emmanuel Dupu, who", "tokens": [50652, 583, 309, 390, 829, 1214, 538, 44421, 413, 1010, 84, 11, 567, 50884], "temperature": 0.0, "avg_logprob": -0.22918864900031977, "compression_ratio": 1.510948905109489, "no_speech_prob": 0.0016953622689470649}, {"id": 211, "seek": 61728, "start": 627.68, "end": 630.48, "text": " kind of tends to indicate at what age babies", "tokens": [50884, 733, 295, 12258, 281, 13330, 412, 437, 3205, 10917, 51024], "temperature": 0.0, "avg_logprob": -0.22918864900031977, "compression_ratio": 1.510948905109489, "no_speech_prob": 0.0016953622689470649}, {"id": 212, "seek": 61728, "start": 630.48, "end": 634.16, "text": " learn basic concepts, like object permanence, for example.", "tokens": [51024, 1466, 3875, 10392, 11, 411, 2657, 8105, 655, 11, 337, 1365, 13, 51208], "temperature": 0.0, "avg_logprob": -0.22918864900031977, "compression_ratio": 1.510948905109489, "no_speech_prob": 0.0016953622689470649}, {"id": 213, "seek": 61728, "start": 634.16, "end": 636.8399999999999, "text": " Liz was talking about that.", "tokens": [51208, 16480, 390, 1417, 466, 300, 13, 51342], "temperature": 0.0, "avg_logprob": -0.22918864900031977, "compression_ratio": 1.510948905109489, "no_speech_prob": 0.0016953622689470649}, {"id": 214, "seek": 61728, "start": 636.8399999999999, "end": 639.24, "text": " Stability and support and intuitive physics,", "tokens": [51342, 745, 2310, 293, 1406, 293, 21769, 10649, 11, 51462], "temperature": 0.0, "avg_logprob": -0.22918864900031977, "compression_ratio": 1.510948905109489, "no_speech_prob": 0.0016953622689470649}, {"id": 215, "seek": 61728, "start": 639.24, "end": 644.4, "text": " which only comes up fairly late, actually, around nine months.", "tokens": [51462, 597, 787, 1487, 493, 6457, 3469, 11, 767, 11, 926, 4949, 2493, 13, 51720], "temperature": 0.0, "avg_logprob": -0.22918864900031977, "compression_ratio": 1.510948905109489, "no_speech_prob": 0.0016953622689470649}, {"id": 216, "seek": 61728, "start": 644.4, "end": 646.4, "text": " And the question is, what type of learning", "tokens": [51720, 400, 264, 1168, 307, 11, 437, 2010, 295, 2539, 51820], "temperature": 0.0, "avg_logprob": -0.22918864900031977, "compression_ratio": 1.510948905109489, "no_speech_prob": 0.0016953622689470649}, {"id": 217, "seek": 64640, "start": 646.4399999999999, "end": 648.04, "text": " is taking place there?", "tokens": [50366, 307, 1940, 1081, 456, 30, 50446], "temperature": 0.0, "avg_logprob": -0.14266415646201686, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.0005353211890906096}, {"id": 218, "seek": 64640, "start": 648.04, "end": 651.36, "text": " No AI systems today really kind of does this properly,", "tokens": [50446, 883, 7318, 3652, 965, 534, 733, 295, 775, 341, 6108, 11, 50612], "temperature": 0.0, "avg_logprob": -0.14266415646201686, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.0005353211890906096}, {"id": 219, "seek": 64640, "start": 651.36, "end": 655.48, "text": " although there's been several attempts by a few of us.", "tokens": [50612, 4878, 456, 311, 668, 2940, 15257, 538, 257, 1326, 295, 505, 13, 50818], "temperature": 0.0, "avg_logprob": -0.14266415646201686, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.0005353211890906096}, {"id": 220, "seek": 64640, "start": 655.48, "end": 658.3199999999999, "text": " So I think perhaps it's this type of learning", "tokens": [50818, 407, 286, 519, 4317, 309, 311, 341, 2010, 295, 2539, 50960], "temperature": 0.0, "avg_logprob": -0.14266415646201686, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.0005353211890906096}, {"id": 221, "seek": 64640, "start": 658.3199999999999, "end": 660.4, "text": " that is the basis of common sense.", "tokens": [50960, 300, 307, 264, 5143, 295, 2689, 2020, 13, 51064], "temperature": 0.0, "avg_logprob": -0.14266415646201686, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.0005353211890906096}, {"id": 222, "seek": 64640, "start": 660.4, "end": 662.92, "text": " And we should really try to figure out", "tokens": [51064, 400, 321, 820, 534, 853, 281, 2573, 484, 51190], "temperature": 0.0, "avg_logprob": -0.14266415646201686, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.0005353211890906096}, {"id": 223, "seek": 64640, "start": 662.92, "end": 665.52, "text": " how to reproduce this with machines.", "tokens": [51190, 577, 281, 29501, 341, 365, 8379, 13, 51320], "temperature": 0.0, "avg_logprob": -0.14266415646201686, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.0005353211890906096}, {"id": 224, "seek": 64640, "start": 665.52, "end": 667.84, "text": " So I think there's three challenges for AI research", "tokens": [51320, 407, 286, 519, 456, 311, 1045, 4759, 337, 7318, 2132, 51436], "temperature": 0.0, "avg_logprob": -0.14266415646201686, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.0005353211890906096}, {"id": 225, "seek": 64640, "start": 667.84, "end": 670.56, "text": " today, learning representations and predictive models", "tokens": [51436, 965, 11, 2539, 33358, 293, 35521, 5245, 51572], "temperature": 0.0, "avg_logprob": -0.14266415646201686, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.0005353211890906096}, {"id": 226, "seek": 64640, "start": 670.56, "end": 674.92, "text": " of the world, allowing machines to predict what's going to happen,", "tokens": [51572, 295, 264, 1002, 11, 8293, 8379, 281, 6069, 437, 311, 516, 281, 1051, 11, 51790], "temperature": 0.0, "avg_logprob": -0.14266415646201686, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.0005353211890906096}, {"id": 227, "seek": 67492, "start": 674.9599999999999, "end": 677.0, "text": " perhaps as a consequence of their actions.", "tokens": [50366, 4317, 382, 257, 18326, 295, 641, 5909, 13, 50468], "temperature": 0.0, "avg_logprob": -0.21572749986561066, "compression_ratio": 1.5353159851301115, "no_speech_prob": 0.0007317941635847092}, {"id": 228, "seek": 67492, "start": 677.0, "end": 678.04, "text": " Learning to reason.", "tokens": [50468, 15205, 281, 1778, 13, 50520], "temperature": 0.0, "avg_logprob": -0.21572749986561066, "compression_ratio": 1.5353159851301115, "no_speech_prob": 0.0007317941635847092}, {"id": 229, "seek": 67492, "start": 678.04, "end": 681.4799999999999, "text": " So this is more like Daniel Kahneman's System 2.", "tokens": [50520, 407, 341, 307, 544, 411, 8033, 591, 12140, 15023, 311, 8910, 568, 13, 50692], "temperature": 0.0, "avg_logprob": -0.21572749986561066, "compression_ratio": 1.5353159851301115, "no_speech_prob": 0.0007317941635847092}, {"id": 230, "seek": 67492, "start": 681.4799999999999, "end": 684.4399999999999, "text": " Current autoregressive LLMs are basically System 1.", "tokens": [50692, 15629, 1476, 418, 3091, 488, 441, 43, 26386, 366, 1936, 8910, 502, 13, 50840], "temperature": 0.0, "avg_logprob": -0.21572749986561066, "compression_ratio": 1.5353159851301115, "no_speech_prob": 0.0007317941635847092}, {"id": 231, "seek": 67492, "start": 684.4399999999999, "end": 686.5999999999999, "text": " They just view one word after the other", "tokens": [50840, 814, 445, 1910, 472, 1349, 934, 264, 661, 50948], "temperature": 0.0, "avg_logprob": -0.21572749986561066, "compression_ratio": 1.5353159851301115, "no_speech_prob": 0.0007317941635847092}, {"id": 232, "seek": 67492, "start": 686.5999999999999, "end": 690.68, "text": " without really planning ahead.", "tokens": [50948, 1553, 534, 5038, 2286, 13, 51152], "temperature": 0.0, "avg_logprob": -0.21572749986561066, "compression_ratio": 1.5353159851301115, "no_speech_prob": 0.0007317941635847092}, {"id": 233, "seek": 67492, "start": 690.68, "end": 693.8, "text": " And so that is the question of making reasoning compatible", "tokens": [51152, 400, 370, 300, 307, 264, 1168, 295, 1455, 21577, 18218, 51308], "temperature": 0.0, "avg_logprob": -0.21572749986561066, "compression_ratio": 1.5353159851301115, "no_speech_prob": 0.0007317941635847092}, {"id": 234, "seek": 67492, "start": 693.8, "end": 696.24, "text": " with learning.", "tokens": [51308, 365, 2539, 13, 51430], "temperature": 0.0, "avg_logprob": -0.21572749986561066, "compression_ratio": 1.5353159851301115, "no_speech_prob": 0.0007317941635847092}, {"id": 235, "seek": 67492, "start": 696.24, "end": 697.92, "text": " Josh has a particular proposal for this,", "tokens": [51430, 9785, 575, 257, 1729, 11494, 337, 341, 11, 51514], "temperature": 0.0, "avg_logprob": -0.21572749986561066, "compression_ratio": 1.5353159851301115, "no_speech_prob": 0.0007317941635847092}, {"id": 236, "seek": 67492, "start": 697.92, "end": 704.52, "text": " which I don't agree with, but that goes in the right direction.", "tokens": [51514, 597, 286, 500, 380, 3986, 365, 11, 457, 300, 1709, 294, 264, 558, 3513, 13, 51844], "temperature": 0.0, "avg_logprob": -0.21572749986561066, "compression_ratio": 1.5353159851301115, "no_speech_prob": 0.0007317941635847092}, {"id": 237, "seek": 70452, "start": 704.56, "end": 707.6, "text": " And then learning to plan complex action sequences.", "tokens": [50366, 400, 550, 2539, 281, 1393, 3997, 3069, 22978, 13, 50518], "temperature": 0.0, "avg_logprob": -0.12391274975192162, "compression_ratio": 1.5551020408163265, "no_speech_prob": 0.00034555429010652006}, {"id": 238, "seek": 70452, "start": 707.6, "end": 711.4, "text": " So I made a proposal for this almost a year ago now,", "tokens": [50518, 407, 286, 1027, 257, 11494, 337, 341, 1920, 257, 1064, 2057, 586, 11, 50708], "temperature": 0.0, "avg_logprob": -0.12391274975192162, "compression_ratio": 1.5551020408163265, "no_speech_prob": 0.00034555429010652006}, {"id": 239, "seek": 70452, "start": 711.4, "end": 715.56, "text": " which I posted on this website so people can make comments", "tokens": [50708, 597, 286, 9437, 322, 341, 3144, 370, 561, 393, 652, 3053, 50916], "temperature": 0.0, "avg_logprob": -0.12391274975192162, "compression_ratio": 1.5551020408163265, "no_speech_prob": 0.00034555429010652006}, {"id": 240, "seek": 70452, "start": 715.56, "end": 720.72, "text": " and tell me I'm wrong and which references I missed.", "tokens": [50916, 293, 980, 385, 286, 478, 2085, 293, 597, 15400, 286, 6721, 13, 51174], "temperature": 0.0, "avg_logprob": -0.12391274975192162, "compression_ratio": 1.5551020408163265, "no_speech_prob": 0.00034555429010652006}, {"id": 241, "seek": 70452, "start": 720.72, "end": 723.56, "text": " I guess several technical talks about it as well.", "tokens": [51174, 286, 2041, 2940, 6191, 6686, 466, 309, 382, 731, 13, 51316], "temperature": 0.0, "avg_logprob": -0.12391274975192162, "compression_ratio": 1.5551020408163265, "no_speech_prob": 0.00034555429010652006}, {"id": 242, "seek": 70452, "start": 723.56, "end": 726.68, "text": " And basically it's sort of a modular organization", "tokens": [51316, 400, 1936, 309, 311, 1333, 295, 257, 31111, 4475, 51472], "temperature": 0.0, "avg_logprob": -0.12391274975192162, "compression_ratio": 1.5551020408163265, "no_speech_prob": 0.00034555429010652006}, {"id": 243, "seek": 70452, "start": 726.68, "end": 731.64, "text": " of an AI system that would be capable of reasoning and planning.", "tokens": [51472, 295, 364, 7318, 1185, 300, 576, 312, 8189, 295, 21577, 293, 5038, 13, 51720], "temperature": 0.0, "avg_logprob": -0.12391274975192162, "compression_ratio": 1.5551020408163265, "no_speech_prob": 0.00034555429010652006}, {"id": 244, "seek": 73164, "start": 731.64, "end": 735.12, "text": " And I can't say that I've built it,", "tokens": [50364, 400, 286, 393, 380, 584, 300, 286, 600, 3094, 309, 11, 50538], "temperature": 0.0, "avg_logprob": -0.17986884344191778, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.000708981417119503}, {"id": 245, "seek": 73164, "start": 735.12, "end": 738.04, "text": " but we're kind of building pieces of it.", "tokens": [50538, 457, 321, 434, 733, 295, 2390, 3755, 295, 309, 13, 50684], "temperature": 0.0, "avg_logprob": -0.17986884344191778, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.000708981417119503}, {"id": 246, "seek": 73164, "start": 738.04, "end": 739.16, "text": " So it's composed.", "tokens": [50684, 407, 309, 311, 18204, 13, 50740], "temperature": 0.0, "avg_logprob": -0.17986884344191778, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.000708981417119503}, {"id": 247, "seek": 73164, "start": 739.16, "end": 742.04, "text": " It's basically centered around the award model, which", "tokens": [50740, 467, 311, 1936, 18988, 926, 264, 7130, 2316, 11, 597, 50884], "temperature": 0.0, "avg_logprob": -0.17986884344191778, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.000708981417119503}, {"id": 248, "seek": 73164, "start": 742.04, "end": 744.48, "text": " will allow the system to predict ahead", "tokens": [50884, 486, 2089, 264, 1185, 281, 6069, 2286, 51006], "temperature": 0.0, "avg_logprob": -0.17986884344191778, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.000708981417119503}, {"id": 249, "seek": 73164, "start": 744.48, "end": 747.4399999999999, "text": " what the consequences of its actions would be.", "tokens": [51006, 437, 264, 10098, 295, 1080, 5909, 576, 312, 13, 51154], "temperature": 0.0, "avg_logprob": -0.17986884344191778, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.000708981417119503}, {"id": 250, "seek": 73164, "start": 747.4399999999999, "end": 752.4, "text": " And it has a cost module.", "tokens": [51154, 400, 309, 575, 257, 2063, 10088, 13, 51402], "temperature": 0.0, "avg_logprob": -0.17986884344191778, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.000708981417119503}, {"id": 251, "seek": 73164, "start": 752.4, "end": 755.84, "text": " Think of it as kind of visual ganglia stuff.", "tokens": [51402, 6557, 295, 309, 382, 733, 295, 5056, 10145, 14218, 1507, 13, 51574], "temperature": 0.0, "avg_logprob": -0.17986884344191778, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.000708981417119503}, {"id": 252, "seek": 73164, "start": 755.84, "end": 757.4399999999999, "text": " And the only purpose of the system", "tokens": [51574, 400, 264, 787, 4334, 295, 264, 1185, 51654], "temperature": 0.0, "avg_logprob": -0.17986884344191778, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.000708981417119503}, {"id": 253, "seek": 73164, "start": 757.4399999999999, "end": 759.56, "text": " is to optimize that cost.", "tokens": [51654, 307, 281, 19719, 300, 2063, 13, 51760], "temperature": 0.0, "avg_logprob": -0.17986884344191778, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.000708981417119503}, {"id": 254, "seek": 75956, "start": 759.56, "end": 761.52, "text": " Some of those costs are essentially", "tokens": [50364, 2188, 295, 729, 5497, 366, 4476, 50462], "temperature": 0.0, "avg_logprob": -0.1691887104405766, "compression_ratio": 1.7925311203319503, "no_speech_prob": 0.00018517345597501844}, {"id": 255, "seek": 75956, "start": 761.52, "end": 764.3199999999999, "text": " intrinsic, hard-wired, immutable costs", "tokens": [50462, 35698, 11, 1152, 12, 86, 1824, 11, 3397, 32148, 5497, 50602], "temperature": 0.0, "avg_logprob": -0.1691887104405766, "compression_ratio": 1.7925311203319503, "no_speech_prob": 0.00018517345597501844}, {"id": 256, "seek": 75956, "start": 764.3199999999999, "end": 767.8, "text": " that sort of drive the basic behavior of the system.", "tokens": [50602, 300, 1333, 295, 3332, 264, 3875, 5223, 295, 264, 1185, 13, 50776], "temperature": 0.0, "avg_logprob": -0.1691887104405766, "compression_ratio": 1.7925311203319503, "no_speech_prob": 0.00018517345597501844}, {"id": 257, "seek": 75956, "start": 767.8, "end": 769.9599999999999, "text": " And some of them are trainable costs", "tokens": [50776, 400, 512, 295, 552, 366, 3847, 712, 5497, 50884], "temperature": 0.0, "avg_logprob": -0.1691887104405766, "compression_ratio": 1.7925311203319503, "no_speech_prob": 0.00018517345597501844}, {"id": 258, "seek": 75956, "start": 769.9599999999999, "end": 773.9599999999999, "text": " that the system learns as it goes.", "tokens": [50884, 300, 264, 1185, 27152, 382, 309, 1709, 13, 51084], "temperature": 0.0, "avg_logprob": -0.1691887104405766, "compression_ratio": 1.7925311203319503, "no_speech_prob": 0.00018517345597501844}, {"id": 259, "seek": 75956, "start": 773.9599999999999, "end": 777.04, "text": " And what the system does is that it", "tokens": [51084, 400, 437, 264, 1185, 775, 307, 300, 309, 51238], "temperature": 0.0, "avg_logprob": -0.1691887104405766, "compression_ratio": 1.7925311203319503, "no_speech_prob": 0.00018517345597501844}, {"id": 260, "seek": 75956, "start": 777.04, "end": 780.2399999999999, "text": " plans a sequence of actions that, according to its model,", "tokens": [51238, 5482, 257, 8310, 295, 5909, 300, 11, 4650, 281, 1080, 2316, 11, 51398], "temperature": 0.0, "avg_logprob": -0.1691887104405766, "compression_ratio": 1.7925311203319503, "no_speech_prob": 0.00018517345597501844}, {"id": 261, "seek": 75956, "start": 780.2399999999999, "end": 783.92, "text": " will minimize those costs.", "tokens": [51398, 486, 17522, 729, 5497, 13, 51582], "temperature": 0.0, "avg_logprob": -0.1691887104405766, "compression_ratio": 1.7925311203319503, "no_speech_prob": 0.00018517345597501844}, {"id": 262, "seek": 75956, "start": 783.92, "end": 785.5999999999999, "text": " And of course, it needs to be able to estimate", "tokens": [51582, 400, 295, 1164, 11, 309, 2203, 281, 312, 1075, 281, 12539, 51666], "temperature": 0.0, "avg_logprob": -0.1691887104405766, "compression_ratio": 1.7925311203319503, "no_speech_prob": 0.00018517345597501844}, {"id": 263, "seek": 75956, "start": 785.5999999999999, "end": 787.9599999999999, "text": " the current state of the world, which is done through perception", "tokens": [51666, 264, 2190, 1785, 295, 264, 1002, 11, 597, 307, 1096, 807, 12860, 51784], "temperature": 0.0, "avg_logprob": -0.1691887104405766, "compression_ratio": 1.7925311203319503, "no_speech_prob": 0.00018517345597501844}, {"id": 264, "seek": 78796, "start": 788.4000000000001, "end": 790.0, "text": " and maybe access to a memory.", "tokens": [50386, 293, 1310, 2105, 281, 257, 4675, 13, 50466], "temperature": 0.0, "avg_logprob": -0.1481538820667427, "compression_ratio": 1.6905660377358491, "no_speech_prob": 0.0008037482621148229}, {"id": 265, "seek": 78796, "start": 790.0, "end": 793.44, "text": " And then depending on the task that the system is focusing on,", "tokens": [50466, 400, 550, 5413, 322, 264, 5633, 300, 264, 1185, 307, 8416, 322, 11, 50638], "temperature": 0.0, "avg_logprob": -0.1481538820667427, "compression_ratio": 1.6905660377358491, "no_speech_prob": 0.0008037482621148229}, {"id": 266, "seek": 78796, "start": 793.44, "end": 795.44, "text": " it can be entirely configured by a configurator", "tokens": [50638, 309, 393, 312, 7696, 30538, 538, 257, 22192, 1639, 50738], "temperature": 0.0, "avg_logprob": -0.1481538820667427, "compression_ratio": 1.6905660377358491, "no_speech_prob": 0.0008037482621148229}, {"id": 267, "seek": 78796, "start": 795.44, "end": 798.76, "text": " that will sort of focus the system on the task at hand.", "tokens": [50738, 300, 486, 1333, 295, 1879, 264, 1185, 322, 264, 5633, 412, 1011, 13, 50904], "temperature": 0.0, "avg_logprob": -0.1481538820667427, "compression_ratio": 1.6905660377358491, "no_speech_prob": 0.0008037482621148229}, {"id": 268, "seek": 78796, "start": 798.76, "end": 801.52, "text": " So that's a cognitive architecture, which", "tokens": [50904, 407, 300, 311, 257, 15605, 9482, 11, 597, 51042], "temperature": 0.0, "avg_logprob": -0.1481538820667427, "compression_ratio": 1.6905660377358491, "no_speech_prob": 0.0008037482621148229}, {"id": 269, "seek": 78796, "start": 801.52, "end": 804.44, "text": " some people in classical AI have been proposing,", "tokens": [51042, 512, 561, 294, 13735, 7318, 362, 668, 29939, 11, 51188], "temperature": 0.0, "avg_logprob": -0.1481538820667427, "compression_ratio": 1.6905660377358491, "no_speech_prob": 0.0008037482621148229}, {"id": 270, "seek": 78796, "start": 804.44, "end": 807.76, "text": " but in sort of different forms.", "tokens": [51188, 457, 294, 1333, 295, 819, 6422, 13, 51354], "temperature": 0.0, "avg_logprob": -0.1481538820667427, "compression_ratio": 1.6905660377358491, "no_speech_prob": 0.0008037482621148229}, {"id": 271, "seek": 78796, "start": 807.76, "end": 809.1600000000001, "text": " And there's two ways to use it.", "tokens": [51354, 400, 456, 311, 732, 2098, 281, 764, 309, 13, 51424], "temperature": 0.0, "avg_logprob": -0.1481538820667427, "compression_ratio": 1.6905660377358491, "no_speech_prob": 0.0008037482621148229}, {"id": 272, "seek": 78796, "start": 809.1600000000001, "end": 813.4000000000001, "text": " Mode 1, which is just a reactive perception action cycle,", "tokens": [51424, 20500, 502, 11, 597, 307, 445, 257, 28897, 12860, 3069, 6586, 11, 51636], "temperature": 0.0, "avg_logprob": -0.1481538820667427, "compression_ratio": 1.6905660377358491, "no_speech_prob": 0.0008037482621148229}, {"id": 273, "seek": 78796, "start": 813.4000000000001, "end": 815.12, "text": " get an idea of the state of the world,", "tokens": [51636, 483, 364, 1558, 295, 264, 1785, 295, 264, 1002, 11, 51722], "temperature": 0.0, "avg_logprob": -0.1481538820667427, "compression_ratio": 1.6905660377358491, "no_speech_prob": 0.0008037482621148229}, {"id": 274, "seek": 81512, "start": 815.12, "end": 817.72, "text": " encoding into an abstract representation", "tokens": [50364, 43430, 666, 364, 12649, 10290, 50494], "temperature": 0.0, "avg_logprob": -0.23352867994851212, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.00025308949989266694}, {"id": 275, "seek": 81512, "start": 817.72, "end": 819.84, "text": " of the state of the world as 0, and then running", "tokens": [50494, 295, 264, 1785, 295, 264, 1002, 382, 1958, 11, 293, 550, 2614, 50600], "temperature": 0.0, "avg_logprob": -0.23352867994851212, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.00025308949989266694}, {"id": 276, "seek": 81512, "start": 819.84, "end": 822.76, "text": " through some other neural net that produces an action", "tokens": [50600, 807, 512, 661, 18161, 2533, 300, 14725, 364, 3069, 50746], "temperature": 0.0, "avg_logprob": -0.23352867994851212, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.00025308949989266694}, {"id": 277, "seek": 81512, "start": 822.76, "end": 824.24, "text": " reactively.", "tokens": [50746, 4515, 3413, 13, 50820], "temperature": 0.0, "avg_logprob": -0.23352867994851212, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.00025308949989266694}, {"id": 278, "seek": 81512, "start": 824.24, "end": 826.28, "text": " But the more interesting mode is mode 2,", "tokens": [50820, 583, 264, 544, 1880, 4391, 307, 4391, 568, 11, 50922], "temperature": 0.0, "avg_logprob": -0.23352867994851212, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.00025308949989266694}, {"id": 279, "seek": 81512, "start": 826.28, "end": 828.84, "text": " which is like Kettiman's System 2,", "tokens": [50922, 597, 307, 411, 591, 3093, 25504, 311, 8910, 568, 11, 51050], "temperature": 0.0, "avg_logprob": -0.23352867994851212, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.00025308949989266694}, {"id": 280, "seek": 81512, "start": 828.84, "end": 831.64, "text": " where you make an estimate of the state of the world,", "tokens": [51050, 689, 291, 652, 364, 12539, 295, 264, 1785, 295, 264, 1002, 11, 51190], "temperature": 0.0, "avg_logprob": -0.23352867994851212, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.00025308949989266694}, {"id": 281, "seek": 81512, "start": 831.64, "end": 834.84, "text": " and then using your word model, predict ahead of time", "tokens": [51190, 293, 550, 1228, 428, 1349, 2316, 11, 6069, 2286, 295, 565, 51350], "temperature": 0.0, "avg_logprob": -0.23352867994851212, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.00025308949989266694}, {"id": 282, "seek": 81512, "start": 834.84, "end": 837.64, "text": " what's going to happen according to an imagined sequence", "tokens": [51350, 437, 311, 516, 281, 1051, 4650, 281, 364, 16590, 8310, 51490], "temperature": 0.0, "avg_logprob": -0.23352867994851212, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.00025308949989266694}, {"id": 283, "seek": 81512, "start": 837.64, "end": 840.8, "text": " of actions that you might take.", "tokens": [51490, 295, 5909, 300, 291, 1062, 747, 13, 51648], "temperature": 0.0, "avg_logprob": -0.23352867994851212, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.00025308949989266694}, {"id": 284, "seek": 81512, "start": 840.8, "end": 844.96, "text": " And then the agent would optimize that sequence of actions.", "tokens": [51648, 400, 550, 264, 9461, 576, 19719, 300, 8310, 295, 5909, 13, 51856], "temperature": 0.0, "avg_logprob": -0.23352867994851212, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.00025308949989266694}, {"id": 285, "seek": 84496, "start": 845.0, "end": 848.0, "text": " So as to minimize a particular cost function,", "tokens": [50366, 407, 382, 281, 17522, 257, 1729, 2063, 2445, 11, 50516], "temperature": 0.0, "avg_logprob": -0.20829539205513747, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0002530994242988527}, {"id": 286, "seek": 84496, "start": 848.0, "end": 854.2, "text": " representing the tax to be fulfilled.", "tokens": [50516, 13460, 264, 3366, 281, 312, 21380, 13, 50826], "temperature": 0.0, "avg_logprob": -0.20829539205513747, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0002530994242988527}, {"id": 287, "seek": 84496, "start": 854.2, "end": 856.52, "text": " And then it would just take the first action", "tokens": [50826, 400, 550, 309, 576, 445, 747, 264, 700, 3069, 50942], "temperature": 0.0, "avg_logprob": -0.20829539205513747, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0002530994242988527}, {"id": 288, "seek": 84496, "start": 856.52, "end": 859.44, "text": " and actually send it to the actuator,", "tokens": [50942, 293, 767, 2845, 309, 281, 264, 34964, 1639, 11, 51088], "temperature": 0.0, "avg_logprob": -0.20829539205513747, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0002530994242988527}, {"id": 289, "seek": 84496, "start": 859.44, "end": 861.24, "text": " or maybe the first few actions.", "tokens": [51088, 420, 1310, 264, 700, 1326, 5909, 13, 51178], "temperature": 0.0, "avg_logprob": -0.20829539205513747, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0002530994242988527}, {"id": 290, "seek": 84496, "start": 861.24, "end": 863.44, "text": " So this is completely classical in optimal control.", "tokens": [51178, 407, 341, 307, 2584, 13735, 294, 16252, 1969, 13, 51288], "temperature": 0.0, "avg_logprob": -0.20829539205513747, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0002530994242988527}, {"id": 291, "seek": 84496, "start": 863.44, "end": 866.48, "text": " It's called model predictive control.", "tokens": [51288, 467, 311, 1219, 2316, 35521, 1969, 13, 51440], "temperature": 0.0, "avg_logprob": -0.20829539205513747, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0002530994242988527}, {"id": 292, "seek": 84496, "start": 866.48, "end": 870.48, "text": " Except the problem here is how you learn the model.", "tokens": [51440, 16192, 264, 1154, 510, 307, 577, 291, 1466, 264, 2316, 13, 51640], "temperature": 0.0, "avg_logprob": -0.20829539205513747, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0002530994242988527}, {"id": 293, "seek": 84496, "start": 870.48, "end": 872.6800000000001, "text": " There's a way to kind of turn System 2 into System 1,", "tokens": [51640, 821, 311, 257, 636, 281, 733, 295, 1261, 8910, 568, 666, 8910, 502, 11, 51750], "temperature": 0.0, "avg_logprob": -0.20829539205513747, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0002530994242988527}, {"id": 294, "seek": 87268, "start": 872.7199999999999, "end": 875.12, "text": " which I'm not going to go into.", "tokens": [50366, 597, 286, 478, 406, 516, 281, 352, 666, 13, 50486], "temperature": 0.0, "avg_logprob": -0.1568557337710732, "compression_ratio": 1.7542372881355932, "no_speech_prob": 0.00015595451986882836}, {"id": 295, "seek": 87268, "start": 875.12, "end": 878.52, "text": " OK, so how do we train the world model?", "tokens": [50486, 2264, 11, 370, 577, 360, 321, 3847, 264, 1002, 2316, 30, 50656], "temperature": 0.0, "avg_logprob": -0.1568557337710732, "compression_ratio": 1.7542372881355932, "no_speech_prob": 0.00015595451986882836}, {"id": 296, "seek": 87268, "start": 878.52, "end": 882.4799999999999, "text": " Only for the fact that the world is not deterministic", "tokens": [50656, 5686, 337, 264, 1186, 300, 264, 1002, 307, 406, 15957, 3142, 50854], "temperature": 0.0, "avg_logprob": -0.1568557337710732, "compression_ratio": 1.7542372881355932, "no_speech_prob": 0.00015595451986882836}, {"id": 297, "seek": 87268, "start": 882.4799999999999, "end": 886.04, "text": " or not entirely predictable, even if it is deterministic.", "tokens": [50854, 420, 406, 7696, 27737, 11, 754, 498, 309, 307, 15957, 3142, 13, 51032], "temperature": 0.0, "avg_logprob": -0.1568557337710732, "compression_ratio": 1.7542372881355932, "no_speech_prob": 0.00015595451986882836}, {"id": 298, "seek": 87268, "start": 886.04, "end": 890.2399999999999, "text": " So we're not going to have a neural net observe the input", "tokens": [51032, 407, 321, 434, 406, 516, 281, 362, 257, 18161, 2533, 11441, 264, 4846, 51242], "temperature": 0.0, "avg_logprob": -0.1568557337710732, "compression_ratio": 1.7542372881355932, "no_speech_prob": 0.00015595451986882836}, {"id": 299, "seek": 87268, "start": 890.2399999999999, "end": 894.64, "text": " and just predict why, and then minimizing a prediction error.", "tokens": [51242, 293, 445, 6069, 983, 11, 293, 550, 46608, 257, 17630, 6713, 13, 51462], "temperature": 0.0, "avg_logprob": -0.1568557337710732, "compression_ratio": 1.7542372881355932, "no_speech_prob": 0.00015595451986882836}, {"id": 300, "seek": 87268, "start": 894.64, "end": 896.2399999999999, "text": " That's not going to work, because that can only", "tokens": [51462, 663, 311, 406, 516, 281, 589, 11, 570, 300, 393, 787, 51542], "temperature": 0.0, "avg_logprob": -0.1568557337710732, "compression_ratio": 1.7542372881355932, "no_speech_prob": 0.00015595451986882836}, {"id": 301, "seek": 87268, "start": 896.2399999999999, "end": 898.12, "text": " make one prediction.", "tokens": [51542, 652, 472, 17630, 13, 51636], "temperature": 0.0, "avg_logprob": -0.1568557337710732, "compression_ratio": 1.7542372881355932, "no_speech_prob": 0.00015595451986882836}, {"id": 302, "seek": 87268, "start": 898.12, "end": 900.92, "text": " So in fact, if you train a big neural net", "tokens": [51636, 407, 294, 1186, 11, 498, 291, 3847, 257, 955, 18161, 2533, 51776], "temperature": 0.0, "avg_logprob": -0.1568557337710732, "compression_ratio": 1.7542372881355932, "no_speech_prob": 0.00015595451986882836}, {"id": 303, "seek": 90092, "start": 900.92, "end": 905.4, "text": " to predict like these are cars from a top-down view", "tokens": [50364, 281, 6069, 411, 613, 366, 5163, 490, 257, 1192, 12, 5093, 1910, 50588], "temperature": 0.0, "avg_logprob": -0.15168419218899912, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0003858821000903845}, {"id": 304, "seek": 90092, "start": 905.4, "end": 907.04, "text": " of a highway, if you train a neural net", "tokens": [50588, 295, 257, 17205, 11, 498, 291, 3847, 257, 18161, 2533, 50670], "temperature": 0.0, "avg_logprob": -0.15168419218899912, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0003858821000903845}, {"id": 305, "seek": 90092, "start": 907.04, "end": 909.1999999999999, "text": " to try to predict what's going to happen in this video,", "tokens": [50670, 281, 853, 281, 6069, 437, 311, 516, 281, 1051, 294, 341, 960, 11, 50778], "temperature": 0.0, "avg_logprob": -0.15168419218899912, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0003858821000903845}, {"id": 306, "seek": 90092, "start": 909.1999999999999, "end": 911.7199999999999, "text": " you get blurry predictions.", "tokens": [50778, 291, 483, 37644, 21264, 13, 50904], "temperature": 0.0, "avg_logprob": -0.15168419218899912, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0003858821000903845}, {"id": 307, "seek": 90092, "start": 911.7199999999999, "end": 914.12, "text": " Because the system cannot predict if a particular car is", "tokens": [50904, 1436, 264, 1185, 2644, 6069, 498, 257, 1729, 1032, 307, 51024], "temperature": 0.0, "avg_logprob": -0.15168419218899912, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0003858821000903845}, {"id": 308, "seek": 90092, "start": 914.12, "end": 917.88, "text": " going to break or accelerate or turn left or right,", "tokens": [51024, 516, 281, 1821, 420, 21341, 420, 1261, 1411, 420, 558, 11, 51212], "temperature": 0.0, "avg_logprob": -0.15168419218899912, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0003858821000903845}, {"id": 309, "seek": 90092, "start": 917.88, "end": 920.7199999999999, "text": " and so it makes these blurry predictions.", "tokens": [51212, 293, 370, 309, 1669, 613, 37644, 21264, 13, 51354], "temperature": 0.0, "avg_logprob": -0.15168419218899912, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0003858821000903845}, {"id": 310, "seek": 90092, "start": 920.7199999999999, "end": 922.04, "text": " Same for a natural video.", "tokens": [51354, 10635, 337, 257, 3303, 960, 13, 51420], "temperature": 0.0, "avg_logprob": -0.15168419218899912, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0003858821000903845}, {"id": 311, "seek": 90092, "start": 922.04, "end": 926.92, "text": " That's an old work on video prediction.", "tokens": [51420, 663, 311, 364, 1331, 589, 322, 960, 17630, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15168419218899912, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0003858821000903845}, {"id": 312, "seek": 90092, "start": 926.92, "end": 929.7199999999999, "text": " So you have to account for the fact", "tokens": [51664, 407, 291, 362, 281, 2696, 337, 264, 1186, 51804], "temperature": 0.0, "avg_logprob": -0.15168419218899912, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0003858821000903845}, {"id": 313, "seek": 92972, "start": 929.72, "end": 932.6800000000001, "text": " that the world is not completely predictable.", "tokens": [50364, 300, 264, 1002, 307, 406, 2584, 27737, 13, 50512], "temperature": 0.0, "avg_logprob": -0.1580116201978211, "compression_ratio": 1.7439024390243902, "no_speech_prob": 0.0002652566763572395}, {"id": 314, "seek": 92972, "start": 932.6800000000001, "end": 935.12, "text": " And you have two solutions there.", "tokens": [50512, 400, 291, 362, 732, 6547, 456, 13, 50634], "temperature": 0.0, "avg_logprob": -0.1580116201978211, "compression_ratio": 1.7439024390243902, "no_speech_prob": 0.0002652566763572395}, {"id": 315, "seek": 92972, "start": 935.12, "end": 937.28, "text": " Either you build an architecture with latent variables", "tokens": [50634, 13746, 291, 1322, 364, 9482, 365, 48994, 9102, 50742], "temperature": 0.0, "avg_logprob": -0.1580116201978211, "compression_ratio": 1.7439024390243902, "no_speech_prob": 0.0002652566763572395}, {"id": 316, "seek": 92972, "start": 937.28, "end": 941.32, "text": " that parameterizes the set of possible predictions,", "tokens": [50742, 300, 13075, 5660, 264, 992, 295, 1944, 21264, 11, 50944], "temperature": 0.0, "avg_logprob": -0.1580116201978211, "compression_ratio": 1.7439024390243902, "no_speech_prob": 0.0002652566763572395}, {"id": 317, "seek": 92972, "start": 941.32, "end": 944.96, "text": " or, and those two are not incompatible,", "tokens": [50944, 420, 11, 293, 729, 732, 366, 406, 40393, 267, 964, 11, 51126], "temperature": 0.0, "avg_logprob": -0.1580116201978211, "compression_ratio": 1.7439024390243902, "no_speech_prob": 0.0002652566763572395}, {"id": 318, "seek": 92972, "start": 944.96, "end": 947.24, "text": " or you abandon the idea that you're", "tokens": [51126, 420, 291, 9072, 264, 1558, 300, 291, 434, 51240], "temperature": 0.0, "avg_logprob": -0.1580116201978211, "compression_ratio": 1.7439024390243902, "no_speech_prob": 0.0002652566763572395}, {"id": 319, "seek": 92972, "start": 947.24, "end": 949.4, "text": " going to predict everything about the world.", "tokens": [51240, 516, 281, 6069, 1203, 466, 264, 1002, 13, 51348], "temperature": 0.0, "avg_logprob": -0.1580116201978211, "compression_ratio": 1.7439024390243902, "no_speech_prob": 0.0002652566763572395}, {"id": 320, "seek": 92972, "start": 949.4, "end": 951.44, "text": " And so this is what I'm suggesting.", "tokens": [51348, 400, 370, 341, 307, 437, 286, 478, 18094, 13, 51450], "temperature": 0.0, "avg_logprob": -0.1580116201978211, "compression_ratio": 1.7439024390243902, "no_speech_prob": 0.0002652566763572395}, {"id": 321, "seek": 92972, "start": 951.44, "end": 954.08, "text": " So this is a generative architecture.", "tokens": [51450, 407, 341, 307, 257, 1337, 1166, 9482, 13, 51582], "temperature": 0.0, "avg_logprob": -0.1580116201978211, "compression_ratio": 1.7439024390243902, "no_speech_prob": 0.0002652566763572395}, {"id": 322, "seek": 92972, "start": 954.08, "end": 957.28, "text": " Generative architecture observes x, encodes it,", "tokens": [51582, 15409, 1166, 9482, 3181, 9054, 2031, 11, 2058, 4789, 309, 11, 51742], "temperature": 0.0, "avg_logprob": -0.1580116201978211, "compression_ratio": 1.7439024390243902, "no_speech_prob": 0.0002652566763572395}, {"id": 323, "seek": 95728, "start": 957.28, "end": 960.88, "text": " then predicts y, the variable whose dependency you're", "tokens": [50364, 550, 6069, 82, 288, 11, 264, 7006, 6104, 33621, 291, 434, 50544], "temperature": 0.0, "avg_logprob": -0.15630355390530187, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.00010389668750576675}, {"id": 324, "seek": 95728, "start": 960.88, "end": 962.6, "text": " trying to predict.", "tokens": [50544, 1382, 281, 6069, 13, 50630], "temperature": 0.0, "avg_logprob": -0.15630355390530187, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.00010389668750576675}, {"id": 325, "seek": 95728, "start": 962.6, "end": 964.76, "text": " And then you measure the prediction error.", "tokens": [50630, 400, 550, 291, 3481, 264, 17630, 6713, 13, 50738], "temperature": 0.0, "avg_logprob": -0.15630355390530187, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.00010389668750576675}, {"id": 326, "seek": 95728, "start": 964.76, "end": 967.8, "text": " You mean my side by training, et cetera.", "tokens": [50738, 509, 914, 452, 1252, 538, 3097, 11, 1030, 11458, 13, 50890], "temperature": 0.0, "avg_logprob": -0.15630355390530187, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.00010389668750576675}, {"id": 327, "seek": 95728, "start": 967.8, "end": 971.36, "text": " What I'm proposing is a joint embedding architecture", "tokens": [50890, 708, 286, 478, 29939, 307, 257, 7225, 12240, 3584, 9482, 51068], "temperature": 0.0, "avg_logprob": -0.15630355390530187, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.00010389668750576675}, {"id": 328, "seek": 95728, "start": 971.36, "end": 975.8399999999999, "text": " where both x and y go through encoders, neural nets,", "tokens": [51068, 689, 1293, 2031, 293, 288, 352, 807, 2058, 378, 433, 11, 18161, 36170, 11, 51292], "temperature": 0.0, "avg_logprob": -0.15630355390530187, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.00010389668750576675}, {"id": 329, "seek": 95728, "start": 975.8399999999999, "end": 979.4, "text": " and the prediction takes place in representation space.", "tokens": [51292, 293, 264, 17630, 2516, 1081, 294, 10290, 1901, 13, 51470], "temperature": 0.0, "avg_logprob": -0.15630355390530187, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.00010389668750576675}, {"id": 330, "seek": 95728, "start": 979.4, "end": 982.3199999999999, "text": " What that allows the system to do is basically", "tokens": [51470, 708, 300, 4045, 264, 1185, 281, 360, 307, 1936, 51616], "temperature": 0.0, "avg_logprob": -0.15630355390530187, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.00010389668750576675}, {"id": 331, "seek": 95728, "start": 982.3199999999999, "end": 985.64, "text": " eliminate a lot of irrelevant information from y", "tokens": [51616, 13819, 257, 688, 295, 28682, 1589, 490, 288, 51782], "temperature": 0.0, "avg_logprob": -0.15630355390530187, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.00010389668750576675}, {"id": 332, "seek": 98564, "start": 985.64, "end": 989.0, "text": " when it encodes it into SY so that it doesn't", "tokens": [50364, 562, 309, 2058, 4789, 309, 666, 32624, 370, 300, 309, 1177, 380, 50532], "temperature": 0.0, "avg_logprob": -0.17086674616887018, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.00021989857486914843}, {"id": 333, "seek": 98564, "start": 989.0, "end": 992.0, "text": " have to predict all the details.", "tokens": [50532, 362, 281, 6069, 439, 264, 4365, 13, 50682], "temperature": 0.0, "avg_logprob": -0.17086674616887018, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.00021989857486914843}, {"id": 334, "seek": 98564, "start": 992.0, "end": 995.4, "text": " So there's a lot of things here and a lot of information", "tokens": [50682, 407, 456, 311, 257, 688, 295, 721, 510, 293, 257, 688, 295, 1589, 50852], "temperature": 0.0, "avg_logprob": -0.17086674616887018, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.00021989857486914843}, {"id": 335, "seek": 98564, "start": 995.4, "end": 999.36, "text": " in this room that we cannot possibly remember or predict", "tokens": [50852, 294, 341, 1808, 300, 321, 2644, 6264, 1604, 420, 6069, 51050], "temperature": 0.0, "avg_logprob": -0.17086674616887018, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.00021989857486914843}, {"id": 336, "seek": 98564, "start": 999.36, "end": 1004.68, "text": " the precise texture of the wood on the floor, things like that.", "tokens": [51050, 264, 13600, 8091, 295, 264, 4576, 322, 264, 4123, 11, 721, 411, 300, 13, 51316], "temperature": 0.0, "avg_logprob": -0.17086674616887018, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.00021989857486914843}, {"id": 337, "seek": 98564, "start": 1004.68, "end": 1007.3199999999999, "text": " But it's kind of irrelevant.", "tokens": [51316, 583, 309, 311, 733, 295, 28682, 13, 51448], "temperature": 0.0, "avg_logprob": -0.17086674616887018, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.00021989857486914843}, {"id": 338, "seek": 98564, "start": 1007.3199999999999, "end": 1010.8, "text": " We only need to have sort of a relatively abstract", "tokens": [51448, 492, 787, 643, 281, 362, 1333, 295, 257, 7226, 12649, 51622], "temperature": 0.0, "avg_logprob": -0.17086674616887018, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.00021989857486914843}, {"id": 339, "seek": 98564, "start": 1010.8, "end": 1012.36, "text": " representation of it.", "tokens": [51622, 10290, 295, 309, 13, 51700], "temperature": 0.0, "avg_logprob": -0.17086674616887018, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.00021989857486914843}, {"id": 340, "seek": 98564, "start": 1012.36, "end": 1013.92, "text": " So I'm basically recommending to abandon", "tokens": [51700, 407, 286, 478, 1936, 30559, 281, 9072, 51778], "temperature": 0.0, "avg_logprob": -0.17086674616887018, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.00021989857486914843}, {"id": 341, "seek": 101392, "start": 1013.92, "end": 1016.7199999999999, "text": " the whole idea of generative models,", "tokens": [50364, 264, 1379, 1558, 295, 1337, 1166, 5245, 11, 50504], "temperature": 0.0, "avg_logprob": -0.17574665763161398, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.00011232982069486752}, {"id": 342, "seek": 101392, "start": 1016.7199999999999, "end": 1019.4, "text": " unless you want to produce pictures or produce text.", "tokens": [50504, 5969, 291, 528, 281, 5258, 5242, 420, 5258, 2487, 13, 50638], "temperature": 0.0, "avg_logprob": -0.17574665763161398, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.00011232982069486752}, {"id": 343, "seek": 101392, "start": 1019.4, "end": 1021.36, "text": " But if you want to learn how the world works,", "tokens": [50638, 583, 498, 291, 528, 281, 1466, 577, 264, 1002, 1985, 11, 50736], "temperature": 0.0, "avg_logprob": -0.17574665763161398, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.00011232982069486752}, {"id": 344, "seek": 101392, "start": 1021.36, "end": 1023.7199999999999, "text": " you should not reconstruct.", "tokens": [50736, 291, 820, 406, 31499, 13, 50854], "temperature": 0.0, "avg_logprob": -0.17574665763161398, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.00011232982069486752}, {"id": 345, "seek": 101392, "start": 1023.7199999999999, "end": 1025.0, "text": " There's actually several versions", "tokens": [50854, 821, 311, 767, 2940, 9606, 50918], "temperature": 0.0, "avg_logprob": -0.17574665763161398, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.00011232982069486752}, {"id": 346, "seek": 101392, "start": 1025.0, "end": 1027.52, "text": " of those joint embedding architectures,", "tokens": [50918, 295, 729, 7225, 12240, 3584, 6331, 1303, 11, 51044], "temperature": 0.0, "avg_logprob": -0.17574665763161398, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.00011232982069486752}, {"id": 347, "seek": 101392, "start": 1027.52, "end": 1032.6399999999999, "text": " the simple one, deterministic ones that can predict,", "tokens": [51044, 264, 2199, 472, 11, 15957, 3142, 2306, 300, 393, 6069, 11, 51300], "temperature": 0.0, "avg_logprob": -0.17574665763161398, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.00011232982069486752}, {"id": 348, "seek": 101392, "start": 1032.6399999999999, "end": 1034.2, "text": " and then nondeterministic ones that", "tokens": [51300, 293, 550, 297, 684, 35344, 259, 3142, 2306, 300, 51378], "temperature": 0.0, "avg_logprob": -0.17574665763161398, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.00011232982069486752}, {"id": 349, "seek": 101392, "start": 1034.2, "end": 1039.44, "text": " can predict where the predictor can have latent variables.", "tokens": [51378, 393, 6069, 689, 264, 6069, 284, 393, 362, 48994, 9102, 13, 51640], "temperature": 0.0, "avg_logprob": -0.17574665763161398, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.00011232982069486752}, {"id": 350, "seek": 101392, "start": 1039.44, "end": 1042.72, "text": " So that's kind of the most general architecture.", "tokens": [51640, 407, 300, 311, 733, 295, 264, 881, 2674, 9482, 13, 51804], "temperature": 0.0, "avg_logprob": -0.17574665763161398, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.00011232982069486752}, {"id": 351, "seek": 104272, "start": 1042.72, "end": 1048.4, "text": " And the latent variable A here can be a latent variable you", "tokens": [50364, 400, 264, 48994, 7006, 316, 510, 393, 312, 257, 48994, 7006, 291, 50648], "temperature": 0.0, "avg_logprob": -0.1702671918002042, "compression_ratio": 1.9651162790697674, "no_speech_prob": 7.721123256487772e-05}, {"id": 352, "seek": 104272, "start": 1048.4, "end": 1050.2, "text": " infer or it could be an action.", "tokens": [50648, 13596, 420, 309, 727, 312, 364, 3069, 13, 50738], "temperature": 0.0, "avg_logprob": -0.1702671918002042, "compression_ratio": 1.9651162790697674, "no_speech_prob": 7.721123256487772e-05}, {"id": 353, "seek": 104272, "start": 1050.2, "end": 1052.08, "text": " So imagine that this is a world model.", "tokens": [50738, 407, 3811, 300, 341, 307, 257, 1002, 2316, 13, 50832], "temperature": 0.0, "avg_logprob": -0.1702671918002042, "compression_ratio": 1.9651162790697674, "no_speech_prob": 7.721123256487772e-05}, {"id": 354, "seek": 104272, "start": 1052.08, "end": 1053.52, "text": " This is the current state of the world", "tokens": [50832, 639, 307, 264, 2190, 1785, 295, 264, 1002, 50904], "temperature": 0.0, "avg_logprob": -0.1702671918002042, "compression_ratio": 1.9651162790697674, "no_speech_prob": 7.721123256487772e-05}, {"id": 355, "seek": 104272, "start": 1053.52, "end": 1055.64, "text": " that you observe, you encode.", "tokens": [50904, 300, 291, 11441, 11, 291, 2058, 1429, 13, 51010], "temperature": 0.0, "avg_logprob": -0.1702671918002042, "compression_ratio": 1.9651162790697674, "no_speech_prob": 7.721123256487772e-05}, {"id": 356, "seek": 104272, "start": 1055.64, "end": 1057.6000000000001, "text": " This is an action you might take in the world,", "tokens": [51010, 639, 307, 364, 3069, 291, 1062, 747, 294, 264, 1002, 11, 51108], "temperature": 0.0, "avg_logprob": -0.1702671918002042, "compression_ratio": 1.9651162790697674, "no_speech_prob": 7.721123256487772e-05}, {"id": 357, "seek": 104272, "start": 1057.6000000000001, "end": 1059.92, "text": " maybe combined with some latent variable which", "tokens": [51108, 1310, 9354, 365, 512, 48994, 7006, 597, 51224], "temperature": 0.0, "avg_logprob": -0.1702671918002042, "compression_ratio": 1.9651162790697674, "no_speech_prob": 7.721123256487772e-05}, {"id": 358, "seek": 104272, "start": 1059.92, "end": 1062.1200000000001, "text": " represent what you don't know about the world.", "tokens": [51224, 2906, 437, 291, 500, 380, 458, 466, 264, 1002, 13, 51334], "temperature": 0.0, "avg_logprob": -0.1702671918002042, "compression_ratio": 1.9651162790697674, "no_speech_prob": 7.721123256487772e-05}, {"id": 359, "seek": 104272, "start": 1062.1200000000001, "end": 1065.56, "text": " And then you make a prediction, and then", "tokens": [51334, 400, 550, 291, 652, 257, 17630, 11, 293, 550, 51506], "temperature": 0.0, "avg_logprob": -0.1702671918002042, "compression_ratio": 1.9651162790697674, "no_speech_prob": 7.721123256487772e-05}, {"id": 360, "seek": 104272, "start": 1065.56, "end": 1067.76, "text": " you can compare that prediction with what actually occurs", "tokens": [51506, 291, 393, 6794, 300, 17630, 365, 437, 767, 11843, 51616], "temperature": 0.0, "avg_logprob": -0.1702671918002042, "compression_ratio": 1.9651162790697674, "no_speech_prob": 7.721123256487772e-05}, {"id": 361, "seek": 104272, "start": 1067.76, "end": 1069.96, "text": " if you want to train your model.", "tokens": [51616, 498, 291, 528, 281, 3847, 428, 2316, 13, 51726], "temperature": 0.0, "avg_logprob": -0.1702671918002042, "compression_ratio": 1.9651162790697674, "no_speech_prob": 7.721123256487772e-05}, {"id": 362, "seek": 104272, "start": 1069.96, "end": 1071.32, "text": " And that's a predictive model that", "tokens": [51726, 400, 300, 311, 257, 35521, 2316, 300, 51794], "temperature": 0.0, "avg_logprob": -0.1702671918002042, "compression_ratio": 1.9651162790697674, "no_speech_prob": 7.721123256487772e-05}, {"id": 363, "seek": 107132, "start": 1071.32, "end": 1073.4399999999998, "text": " will allow you to predict what's going", "tokens": [50364, 486, 2089, 291, 281, 6069, 437, 311, 516, 50470], "temperature": 0.0, "avg_logprob": -0.22302911376953125, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.000501575879752636}, {"id": 364, "seek": 107132, "start": 1073.4399999999998, "end": 1075.9199999999998, "text": " to happen as a consequence of your actions.", "tokens": [50470, 281, 1051, 382, 257, 18326, 295, 428, 5909, 13, 50594], "temperature": 0.0, "avg_logprob": -0.22302911376953125, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.000501575879752636}, {"id": 365, "seek": 107132, "start": 1075.9199999999998, "end": 1080.08, "text": " Now, because we're not generating anything,", "tokens": [50594, 823, 11, 570, 321, 434, 406, 17746, 1340, 11, 50802], "temperature": 0.0, "avg_logprob": -0.22302911376953125, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.000501575879752636}, {"id": 366, "seek": 107132, "start": 1080.08, "end": 1082.36, "text": " and because we can't turn a model of this type", "tokens": [50802, 293, 570, 321, 393, 380, 1261, 257, 2316, 295, 341, 2010, 50916], "temperature": 0.0, "avg_logprob": -0.22302911376953125, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.000501575879752636}, {"id": 367, "seek": 107132, "start": 1082.36, "end": 1085.48, "text": " into a probabilistic model of t of y given x,", "tokens": [50916, 666, 257, 31959, 3142, 2316, 295, 256, 295, 288, 2212, 2031, 11, 51072], "temperature": 0.0, "avg_logprob": -0.22302911376953125, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.000501575879752636}, {"id": 368, "seek": 107132, "start": 1085.48, "end": 1088.6, "text": " we have to abandon the whole idea of probabilistic modeling.", "tokens": [51072, 321, 362, 281, 9072, 264, 1379, 1558, 295, 31959, 3142, 15983, 13, 51228], "temperature": 0.0, "avg_logprob": -0.22302911376953125, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.000501575879752636}, {"id": 369, "seek": 107132, "start": 1088.6, "end": 1093.12, "text": " And now Josh is going, oh my god.", "tokens": [51228, 400, 586, 9785, 307, 516, 11, 1954, 452, 3044, 13, 51454], "temperature": 0.0, "avg_logprob": -0.22302911376953125, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.000501575879752636}, {"id": 370, "seek": 107132, "start": 1093.12, "end": 1095.32, "text": " Isn't it just approximate probability at that point?", "tokens": [51454, 6998, 380, 309, 445, 30874, 8482, 412, 300, 935, 30, 51564], "temperature": 0.0, "avg_logprob": -0.22302911376953125, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.000501575879752636}, {"id": 371, "seek": 107132, "start": 1095.32, "end": 1095.96, "text": " Isn't it?", "tokens": [51564, 6998, 380, 309, 30, 51596], "temperature": 0.0, "avg_logprob": -0.22302911376953125, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.000501575879752636}, {"id": 372, "seek": 107132, "start": 1095.96, "end": 1096.46, "text": " No.", "tokens": [51596, 883, 13, 51621], "temperature": 0.0, "avg_logprob": -0.22302911376953125, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.000501575879752636}, {"id": 373, "seek": 107132, "start": 1096.46, "end": 1096.96, "text": " No?", "tokens": [51621, 883, 30, 51646], "temperature": 0.0, "avg_logprob": -0.22302911376953125, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.000501575879752636}, {"id": 374, "seek": 107132, "start": 1096.96, "end": 1098.84, "text": " No.", "tokens": [51646, 883, 13, 51740], "temperature": 0.0, "avg_logprob": -0.22302911376953125, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.000501575879752636}, {"id": 375, "seek": 107132, "start": 1098.84, "end": 1099.96, "text": " It's energies, OK?", "tokens": [51740, 467, 311, 25737, 11, 2264, 30, 51796], "temperature": 0.0, "avg_logprob": -0.22302911376953125, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.000501575879752636}, {"id": 376, "seek": 109996, "start": 1100.0, "end": 1104.32, "text": " So basically the name of the game here", "tokens": [50366, 407, 1936, 264, 1315, 295, 264, 1216, 510, 50582], "temperature": 0.0, "avg_logprob": -0.19526770945345417, "compression_ratio": 1.7, "no_speech_prob": 0.00010227998427581042}, {"id": 377, "seek": 109996, "start": 1104.32, "end": 1107.96, "text": " is that you need to understand the system as computing", "tokens": [50582, 307, 300, 291, 643, 281, 1223, 264, 1185, 382, 15866, 50764], "temperature": 0.0, "avg_logprob": -0.19526770945345417, "compression_ratio": 1.7, "no_speech_prob": 0.00010227998427581042}, {"id": 378, "seek": 109996, "start": 1107.96, "end": 1110.96, "text": " an energy function that captures the dependency between x and y.", "tokens": [50764, 364, 2281, 2445, 300, 27986, 264, 33621, 1296, 2031, 293, 288, 13, 50914], "temperature": 0.0, "avg_logprob": -0.19526770945345417, "compression_ratio": 1.7, "no_speech_prob": 0.00010227998427581042}, {"id": 379, "seek": 109996, "start": 1110.96, "end": 1115.88, "text": " So imagine the data points are those black spheres.", "tokens": [50914, 407, 3811, 264, 1412, 2793, 366, 729, 2211, 41225, 13, 51160], "temperature": 0.0, "avg_logprob": -0.19526770945345417, "compression_ratio": 1.7, "no_speech_prob": 0.00010227998427581042}, {"id": 380, "seek": 109996, "start": 1115.88, "end": 1118.16, "text": " The energy function should take low values around the black", "tokens": [51160, 440, 2281, 2445, 820, 747, 2295, 4190, 926, 264, 2211, 51274], "temperature": 0.0, "avg_logprob": -0.19526770945345417, "compression_ratio": 1.7, "no_speech_prob": 0.00010227998427581042}, {"id": 381, "seek": 109996, "start": 1118.16, "end": 1120.88, "text": " spheres and higher values outside.", "tokens": [51274, 41225, 293, 2946, 4190, 2380, 13, 51410], "temperature": 0.0, "avg_logprob": -0.19526770945345417, "compression_ratio": 1.7, "no_speech_prob": 0.00010227998427581042}, {"id": 382, "seek": 109996, "start": 1120.88, "end": 1124.64, "text": " And whether this energy function represents", "tokens": [51410, 400, 1968, 341, 2281, 2445, 8855, 51598], "temperature": 0.0, "avg_logprob": -0.19526770945345417, "compression_ratio": 1.7, "no_speech_prob": 0.00010227998427581042}, {"id": 383, "seek": 109996, "start": 1124.64, "end": 1129.48, "text": " the unnormalized log of some probability,", "tokens": [51598, 264, 517, 23157, 1602, 3565, 295, 512, 8482, 11, 51840], "temperature": 0.0, "avg_logprob": -0.19526770945345417, "compression_ratio": 1.7, "no_speech_prob": 0.00010227998427581042}, {"id": 384, "seek": 112948, "start": 1129.48, "end": 1131.84, "text": " is irrelevant, you just want the energy", "tokens": [50364, 307, 28682, 11, 291, 445, 528, 264, 2281, 50482], "temperature": 0.0, "avg_logprob": -0.19957169364480412, "compression_ratio": 1.8103448275862069, "no_speech_prob": 0.00023770933330524713}, {"id": 385, "seek": 112948, "start": 1131.84, "end": 1134.32, "text": " to be higher outside of the manifold of data.", "tokens": [50482, 281, 312, 2946, 2380, 295, 264, 47138, 295, 1412, 13, 50606], "temperature": 0.0, "avg_logprob": -0.19957169364480412, "compression_ratio": 1.8103448275862069, "no_speech_prob": 0.00023770933330524713}, {"id": 386, "seek": 112948, "start": 1134.32, "end": 1137.2, "text": " And it will have captured the dependency between the variables.", "tokens": [50606, 400, 309, 486, 362, 11828, 264, 33621, 1296, 264, 9102, 13, 50750], "temperature": 0.0, "avg_logprob": -0.19957169364480412, "compression_ratio": 1.8103448275862069, "no_speech_prob": 0.00023770933330524713}, {"id": 387, "seek": 112948, "start": 1137.2, "end": 1140.1200000000001, "text": " And there's nothing more you need.", "tokens": [50750, 400, 456, 311, 1825, 544, 291, 643, 13, 50896], "temperature": 0.0, "avg_logprob": -0.19957169364480412, "compression_ratio": 1.8103448275862069, "no_speech_prob": 0.00023770933330524713}, {"id": 388, "seek": 112948, "start": 1140.1200000000001, "end": 1142.6, "text": " Now, the next question is, how do you train a system", "tokens": [50896, 823, 11, 264, 958, 1168, 307, 11, 577, 360, 291, 3847, 257, 1185, 51020], "temperature": 0.0, "avg_logprob": -0.19957169364480412, "compression_ratio": 1.8103448275862069, "no_speech_prob": 0.00023770933330524713}, {"id": 389, "seek": 112948, "start": 1142.6, "end": 1146.24, "text": " to give low energy to stuff you observe and high energy", "tokens": [51020, 281, 976, 2295, 2281, 281, 1507, 291, 11441, 293, 1090, 2281, 51202], "temperature": 0.0, "avg_logprob": -0.19957169364480412, "compression_ratio": 1.8103448275862069, "no_speech_prob": 0.00023770933330524713}, {"id": 390, "seek": 112948, "start": 1146.24, "end": 1147.4, "text": " to stuff you don't observe?", "tokens": [51202, 281, 1507, 291, 500, 380, 11441, 30, 51260], "temperature": 0.0, "avg_logprob": -0.19957169364480412, "compression_ratio": 1.8103448275862069, "no_speech_prob": 0.00023770933330524713}, {"id": 391, "seek": 112948, "start": 1147.4, "end": 1149.6, "text": " And there are two methods, contrastive methods,", "tokens": [51260, 400, 456, 366, 732, 7150, 11, 8712, 488, 7150, 11, 51370], "temperature": 0.0, "avg_logprob": -0.19957169364480412, "compression_ratio": 1.8103448275862069, "no_speech_prob": 0.00023770933330524713}, {"id": 392, "seek": 112948, "start": 1149.6, "end": 1152.84, "text": " which consist in generating fake contrastive points whose", "tokens": [51370, 597, 4603, 294, 17746, 7592, 8712, 488, 2793, 6104, 51532], "temperature": 0.0, "avg_logprob": -0.19957169364480412, "compression_ratio": 1.8103448275862069, "no_speech_prob": 0.00023770933330524713}, {"id": 393, "seek": 112948, "start": 1152.84, "end": 1154.32, "text": " energy is going to push up.", "tokens": [51532, 2281, 307, 516, 281, 2944, 493, 13, 51606], "temperature": 0.0, "avg_logprob": -0.19957169364480412, "compression_ratio": 1.8103448275862069, "no_speech_prob": 0.00023770933330524713}, {"id": 394, "seek": 112948, "start": 1154.32, "end": 1155.64, "text": " And then regularized methods, which", "tokens": [51606, 400, 550, 3890, 1602, 7150, 11, 597, 51672], "temperature": 0.0, "avg_logprob": -0.19957169364480412, "compression_ratio": 1.8103448275862069, "no_speech_prob": 0.00023770933330524713}, {"id": 395, "seek": 112948, "start": 1155.64, "end": 1157.24, "text": " I'm going to explain in a second.", "tokens": [51672, 286, 478, 516, 281, 2903, 294, 257, 1150, 13, 51752], "temperature": 0.0, "avg_logprob": -0.19957169364480412, "compression_ratio": 1.8103448275862069, "no_speech_prob": 0.00023770933330524713}, {"id": 396, "seek": 115724, "start": 1157.24, "end": 1161.1200000000001, "text": " So let's say you have training samples.", "tokens": [50364, 407, 718, 311, 584, 291, 362, 3097, 10938, 13, 50558], "temperature": 0.0, "avg_logprob": -0.15632985985797385, "compression_ratio": 1.7479674796747968, "no_speech_prob": 2.9309630917850882e-05}, {"id": 397, "seek": 115724, "start": 1161.1200000000001, "end": 1162.8, "text": " Your system currently gives low energy", "tokens": [50558, 2260, 1185, 4362, 2709, 2295, 2281, 50642], "temperature": 0.0, "avg_logprob": -0.15632985985797385, "compression_ratio": 1.7479674796747968, "no_speech_prob": 2.9309630917850882e-05}, {"id": 398, "seek": 115724, "start": 1162.8, "end": 1166.64, "text": " to this sort of peak area here.", "tokens": [50642, 281, 341, 1333, 295, 10651, 1859, 510, 13, 50834], "temperature": 0.0, "avg_logprob": -0.15632985985797385, "compression_ratio": 1.7479674796747968, "no_speech_prob": 2.9309630917850882e-05}, {"id": 399, "seek": 115724, "start": 1166.64, "end": 1170.72, "text": " And it's not a good model of the data here,", "tokens": [50834, 400, 309, 311, 406, 257, 665, 2316, 295, 264, 1412, 510, 11, 51038], "temperature": 0.0, "avg_logprob": -0.15632985985797385, "compression_ratio": 1.7479674796747968, "no_speech_prob": 2.9309630917850882e-05}, {"id": 400, "seek": 115724, "start": 1170.72, "end": 1173.28, "text": " because it gives high energy to data points and low energy", "tokens": [51038, 570, 309, 2709, 1090, 2281, 281, 1412, 2793, 293, 2295, 2281, 51166], "temperature": 0.0, "avg_logprob": -0.15632985985797385, "compression_ratio": 1.7479674796747968, "no_speech_prob": 2.9309630917850882e-05}, {"id": 401, "seek": 115724, "start": 1173.28, "end": 1176.08, "text": " to areas that have no points.", "tokens": [51166, 281, 3179, 300, 362, 572, 2793, 13, 51306], "temperature": 0.0, "avg_logprob": -0.15632985985797385, "compression_ratio": 1.7479674796747968, "no_speech_prob": 2.9309630917850882e-05}, {"id": 402, "seek": 115724, "start": 1176.08, "end": 1179.0, "text": " So what you can do is generate green points here whose energy", "tokens": [51306, 407, 437, 291, 393, 360, 307, 8460, 3092, 2793, 510, 6104, 2281, 51452], "temperature": 0.0, "avg_logprob": -0.15632985985797385, "compression_ratio": 1.7479674796747968, "no_speech_prob": 2.9309630917850882e-05}, {"id": 403, "seek": 115724, "start": 1179.0, "end": 1180.72, "text": " you're going to push up.", "tokens": [51452, 291, 434, 516, 281, 2944, 493, 13, 51538], "temperature": 0.0, "avg_logprob": -0.15632985985797385, "compression_ratio": 1.7479674796747968, "no_speech_prob": 2.9309630917850882e-05}, {"id": 404, "seek": 115724, "start": 1180.72, "end": 1184.08, "text": " And the energy function is going to take the right shape.", "tokens": [51538, 400, 264, 2281, 2445, 307, 516, 281, 747, 264, 558, 3909, 13, 51706], "temperature": 0.0, "avg_logprob": -0.15632985985797385, "compression_ratio": 1.7479674796747968, "no_speech_prob": 2.9309630917850882e-05}, {"id": 405, "seek": 115724, "start": 1184.08, "end": 1186.72, "text": " Or you could use some sort of regularizer", "tokens": [51706, 1610, 291, 727, 764, 512, 1333, 295, 3890, 6545, 51838], "temperature": 0.0, "avg_logprob": -0.15632985985797385, "compression_ratio": 1.7479674796747968, "no_speech_prob": 2.9309630917850882e-05}, {"id": 406, "seek": 118672, "start": 1186.72, "end": 1190.44, "text": " that minimizes the volume of space that can take low energy.", "tokens": [50364, 300, 4464, 5660, 264, 5523, 295, 1901, 300, 393, 747, 2295, 2281, 13, 50550], "temperature": 0.0, "avg_logprob": -0.1707244500881288, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.00021637394092977047}, {"id": 407, "seek": 118672, "start": 1190.44, "end": 1192.72, "text": " So that whenever you push down on the energy of some regions,", "tokens": [50550, 407, 300, 5699, 291, 2944, 760, 322, 264, 2281, 295, 512, 10682, 11, 50664], "temperature": 0.0, "avg_logprob": -0.1707244500881288, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.00021637394092977047}, {"id": 408, "seek": 118672, "start": 1192.72, "end": 1194.2, "text": " the rest has to go up, because there", "tokens": [50664, 264, 1472, 575, 281, 352, 493, 11, 570, 456, 50738], "temperature": 0.0, "avg_logprob": -0.1707244500881288, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.00021637394092977047}, {"id": 409, "seek": 118672, "start": 1194.2, "end": 1199.92, "text": " is a limited amount of volume that can take low energy.", "tokens": [50738, 307, 257, 5567, 2372, 295, 5523, 300, 393, 747, 2295, 2281, 13, 51024], "temperature": 0.0, "avg_logprob": -0.1707244500881288, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.00021637394092977047}, {"id": 410, "seek": 118672, "start": 1199.92, "end": 1203.88, "text": " So in the context of joint embedding architecture,", "tokens": [51024, 407, 294, 264, 4319, 295, 7225, 12240, 3584, 9482, 11, 51222], "temperature": 0.0, "avg_logprob": -0.1707244500881288, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.00021637394092977047}, {"id": 411, "seek": 118672, "start": 1203.88, "end": 1205.68, "text": " I kind of invented the contrastive methods.", "tokens": [51222, 286, 733, 295, 14479, 264, 8712, 488, 7150, 13, 51312], "temperature": 0.0, "avg_logprob": -0.1707244500881288, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.00021637394092977047}, {"id": 412, "seek": 118672, "start": 1205.68, "end": 1207.84, "text": " That's called sine is net in the old days.", "tokens": [51312, 663, 311, 1219, 18609, 307, 2533, 294, 264, 1331, 1708, 13, 51420], "temperature": 0.0, "avg_logprob": -0.1707244500881288, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.00021637394092977047}, {"id": 413, "seek": 118672, "start": 1207.84, "end": 1213.52, "text": " But I'm now arguing against that in favor of regularized methods.", "tokens": [51420, 583, 286, 478, 586, 19697, 1970, 300, 294, 2294, 295, 3890, 1602, 7150, 13, 51704], "temperature": 0.0, "avg_logprob": -0.1707244500881288, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.00021637394092977047}, {"id": 414, "seek": 118672, "start": 1213.52, "end": 1215.52, "text": " And the big question is, how do we train them?", "tokens": [51704, 400, 264, 955, 1168, 307, 11, 577, 360, 321, 3847, 552, 30, 51804], "temperature": 0.0, "avg_logprob": -0.1707244500881288, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.00021637394092977047}, {"id": 415, "seek": 121552, "start": 1215.56, "end": 1217.6399999999999, "text": " I'll tell you about that in a minute.", "tokens": [50366, 286, 603, 980, 291, 466, 300, 294, 257, 3456, 13, 50470], "temperature": 0.0, "avg_logprob": -0.19371086254454495, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.00012928545766044408}, {"id": 416, "seek": 121552, "start": 1217.6399999999999, "end": 1219.72, "text": " But I'm asking you to abandon generative models,", "tokens": [50470, 583, 286, 478, 3365, 291, 281, 9072, 1337, 1166, 5245, 11, 50574], "temperature": 0.0, "avg_logprob": -0.19371086254454495, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.00012928545766044408}, {"id": 417, "seek": 121552, "start": 1219.72, "end": 1223.44, "text": " abandon probabilistic models, probabilistic modeling in general,", "tokens": [50574, 9072, 31959, 3142, 5245, 11, 31959, 3142, 15983, 294, 2674, 11, 50760], "temperature": 0.0, "avg_logprob": -0.19371086254454495, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.00012928545766044408}, {"id": 418, "seek": 121552, "start": 1223.44, "end": 1224.96, "text": " abandon contrastive methods.", "tokens": [50760, 9072, 8712, 488, 7150, 13, 50836], "temperature": 0.0, "avg_logprob": -0.19371086254454495, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.00012928545766044408}, {"id": 419, "seek": 121552, "start": 1224.96, "end": 1226.48, "text": " And of course, abandon reinforcement learning.", "tokens": [50836, 400, 295, 1164, 11, 9072, 29280, 2539, 13, 50912], "temperature": 0.0, "avg_logprob": -0.19371086254454495, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.00012928545766044408}, {"id": 420, "seek": 121552, "start": 1226.48, "end": 1231.24, "text": " But that, I've been saying this for years.", "tokens": [50912, 583, 300, 11, 286, 600, 668, 1566, 341, 337, 924, 13, 51150], "temperature": 0.0, "avg_logprob": -0.19371086254454495, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.00012928545766044408}, {"id": 421, "seek": 121552, "start": 1231.24, "end": 1234.56, "text": " Those are four of the main pillars of machine learning.", "tokens": [51150, 3950, 366, 1451, 295, 264, 2135, 26729, 295, 3479, 2539, 13, 51316], "temperature": 0.0, "avg_logprob": -0.19371086254454495, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.00012928545766044408}, {"id": 422, "seek": 121552, "start": 1234.56, "end": 1240.2, "text": " That makes me super popular among my colleagues.", "tokens": [51316, 663, 1669, 385, 1687, 3743, 3654, 452, 7734, 13, 51598], "temperature": 0.0, "avg_logprob": -0.19371086254454495, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.00012928545766044408}, {"id": 423, "seek": 121552, "start": 1240.2, "end": 1242.04, "text": " OK, so what are those regularized methods", "tokens": [51598, 2264, 11, 370, 437, 366, 729, 3890, 1602, 7150, 51690], "temperature": 0.0, "avg_logprob": -0.19371086254454495, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.00012928545766044408}, {"id": 424, "seek": 121552, "start": 1242.04, "end": 1244.04, "text": " for joint embedding architectures?", "tokens": [51690, 337, 7225, 12240, 3584, 6331, 1303, 30, 51790], "temperature": 0.0, "avg_logprob": -0.19371086254454495, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.00012928545766044408}, {"id": 425, "seek": 124404, "start": 1244.08, "end": 1246.36, "text": " So essentially, there is a big issue", "tokens": [50366, 407, 4476, 11, 456, 307, 257, 955, 2734, 50480], "temperature": 0.0, "avg_logprob": -0.19165315628051757, "compression_ratio": 1.6755725190839694, "no_speech_prob": 9.459007560508326e-05}, {"id": 426, "seek": 124404, "start": 1246.36, "end": 1250.36, "text": " that you have to fix, which is that when you train a system", "tokens": [50480, 300, 291, 362, 281, 3191, 11, 597, 307, 300, 562, 291, 3847, 257, 1185, 50680], "temperature": 0.0, "avg_logprob": -0.19165315628051757, "compression_ratio": 1.6755725190839694, "no_speech_prob": 9.459007560508326e-05}, {"id": 427, "seek": 124404, "start": 1250.36, "end": 1252.36, "text": " like this, one of those JEPA architecture,", "tokens": [50680, 411, 341, 11, 472, 295, 729, 508, 8929, 32, 9482, 11, 50780], "temperature": 0.0, "avg_logprob": -0.19165315628051757, "compression_ratio": 1.6755725190839694, "no_speech_prob": 9.459007560508326e-05}, {"id": 428, "seek": 124404, "start": 1252.36, "end": 1254.72, "text": " joint embedding predictive architectures,", "tokens": [50780, 7225, 12240, 3584, 35521, 6331, 1303, 11, 50898], "temperature": 0.0, "avg_logprob": -0.19165315628051757, "compression_ratio": 1.6755725190839694, "no_speech_prob": 9.459007560508326e-05}, {"id": 429, "seek": 124404, "start": 1254.72, "end": 1256.84, "text": " you show it an example of x and y.", "tokens": [50898, 291, 855, 309, 364, 1365, 295, 2031, 293, 288, 13, 51004], "temperature": 0.0, "avg_logprob": -0.19165315628051757, "compression_ratio": 1.6755725190839694, "no_speech_prob": 9.459007560508326e-05}, {"id": 430, "seek": 124404, "start": 1256.84, "end": 1260.04, "text": " And you tell it just train all the weights of all those neural", "tokens": [51004, 400, 291, 980, 309, 445, 3847, 439, 264, 17443, 295, 439, 729, 18161, 51164], "temperature": 0.0, "avg_logprob": -0.19165315628051757, "compression_ratio": 1.6755725190839694, "no_speech_prob": 9.459007560508326e-05}, {"id": 431, "seek": 124404, "start": 1260.04, "end": 1264.6399999999999, "text": " nets so as to minimize the prediction error, it collapses.", "tokens": [51164, 36170, 370, 382, 281, 17522, 264, 17630, 6713, 11, 309, 48765, 13, 51394], "temperature": 0.0, "avg_logprob": -0.19165315628051757, "compression_ratio": 1.6755725190839694, "no_speech_prob": 9.459007560508326e-05}, {"id": 432, "seek": 124404, "start": 1264.6399999999999, "end": 1267.28, "text": " Basically, what it says is that, well, I can just", "tokens": [51394, 8537, 11, 437, 309, 1619, 307, 300, 11, 731, 11, 286, 393, 445, 51526], "temperature": 0.0, "avg_logprob": -0.19165315628051757, "compression_ratio": 1.6755725190839694, "no_speech_prob": 9.459007560508326e-05}, {"id": 433, "seek": 124404, "start": 1267.28, "end": 1272.8, "text": " set Sx and Sy to constants and set the prediction,", "tokens": [51526, 992, 318, 87, 293, 3902, 281, 35870, 293, 992, 264, 17630, 11, 51802], "temperature": 0.0, "avg_logprob": -0.19165315628051757, "compression_ratio": 1.6755725190839694, "no_speech_prob": 9.459007560508326e-05}, {"id": 434, "seek": 127280, "start": 1272.84, "end": 1275.52, "text": " set the predictor to some constant thing", "tokens": [50366, 992, 264, 6069, 284, 281, 512, 5754, 551, 50500], "temperature": 0.0, "avg_logprob": -0.16975541548295456, "compression_ratio": 1.812, "no_speech_prob": 0.00048019347013905644}, {"id": 435, "seek": 127280, "start": 1275.52, "end": 1277.6399999999999, "text": " and ignore x and y all together.", "tokens": [50500, 293, 11200, 2031, 293, 288, 439, 1214, 13, 50606], "temperature": 0.0, "avg_logprob": -0.16975541548295456, "compression_ratio": 1.812, "no_speech_prob": 0.00048019347013905644}, {"id": 436, "seek": 127280, "start": 1277.6399999999999, "end": 1280.2, "text": " And that would be a collapse system that gives zero energy", "tokens": [50606, 400, 300, 576, 312, 257, 15584, 1185, 300, 2709, 4018, 2281, 50734], "temperature": 0.0, "avg_logprob": -0.16975541548295456, "compression_ratio": 1.812, "no_speech_prob": 0.00048019347013905644}, {"id": 437, "seek": 127280, "start": 1280.2, "end": 1282.12, "text": " to everything in your space.", "tokens": [50734, 281, 1203, 294, 428, 1901, 13, 50830], "temperature": 0.0, "avg_logprob": -0.16975541548295456, "compression_ratio": 1.812, "no_speech_prob": 0.00048019347013905644}, {"id": 438, "seek": 127280, "start": 1282.12, "end": 1283.56, "text": " You have to prevent that from happening.", "tokens": [50830, 509, 362, 281, 4871, 300, 490, 2737, 13, 50902], "temperature": 0.0, "avg_logprob": -0.16975541548295456, "compression_ratio": 1.812, "no_speech_prob": 0.00048019347013905644}, {"id": 439, "seek": 127280, "start": 1283.56, "end": 1285.56, "text": " And one way to prevent that from happening", "tokens": [50902, 400, 472, 636, 281, 4871, 300, 490, 2737, 51002], "temperature": 0.0, "avg_logprob": -0.16975541548295456, "compression_ratio": 1.812, "no_speech_prob": 0.00048019347013905644}, {"id": 440, "seek": 127280, "start": 1285.56, "end": 1289.6, "text": " is finding a way to maximize the information content", "tokens": [51002, 307, 5006, 257, 636, 281, 19874, 264, 1589, 2701, 51204], "temperature": 0.0, "avg_logprob": -0.16975541548295456, "compression_ratio": 1.812, "no_speech_prob": 0.00048019347013905644}, {"id": 441, "seek": 127280, "start": 1289.6, "end": 1294.12, "text": " of the representations that come out of the encoders.", "tokens": [51204, 295, 264, 33358, 300, 808, 484, 295, 264, 2058, 378, 433, 13, 51430], "temperature": 0.0, "avg_logprob": -0.16975541548295456, "compression_ratio": 1.812, "no_speech_prob": 0.00048019347013905644}, {"id": 442, "seek": 127280, "start": 1294.12, "end": 1297.12, "text": " That actually has the effect of minimizing the volume of stuff", "tokens": [51430, 663, 767, 575, 264, 1802, 295, 46608, 264, 5523, 295, 1507, 51580], "temperature": 0.0, "avg_logprob": -0.16975541548295456, "compression_ratio": 1.812, "no_speech_prob": 0.00048019347013905644}, {"id": 443, "seek": 127280, "start": 1297.12, "end": 1302.52, "text": " that can take your energy indirectly.", "tokens": [51580, 300, 393, 747, 428, 2281, 37779, 13, 51850], "temperature": 0.0, "avg_logprob": -0.16975541548295456, "compression_ratio": 1.812, "no_speech_prob": 0.00048019347013905644}, {"id": 444, "seek": 130252, "start": 1302.52, "end": 1307.24, "text": " So one way to prevent the outputs from being constant", "tokens": [50364, 407, 472, 636, 281, 4871, 264, 23930, 490, 885, 5754, 50600], "temperature": 0.0, "avg_logprob": -0.15273378856146513, "compression_ratio": 1.8246268656716418, "no_speech_prob": 5.143055386724882e-05}, {"id": 445, "seek": 130252, "start": 1307.24, "end": 1310.16, "text": " is that you can force the variance to be non-zero.", "tokens": [50600, 307, 300, 291, 393, 3464, 264, 21977, 281, 312, 2107, 12, 32226, 13, 50746], "temperature": 0.0, "avg_logprob": -0.15273378856146513, "compression_ratio": 1.8246268656716418, "no_speech_prob": 5.143055386724882e-05}, {"id": 446, "seek": 130252, "start": 1310.16, "end": 1312.84, "text": " So you put a cost function on top of this vector here", "tokens": [50746, 407, 291, 829, 257, 2063, 2445, 322, 1192, 295, 341, 8062, 510, 50880], "temperature": 0.0, "avg_logprob": -0.15273378856146513, "compression_ratio": 1.8246268656716418, "no_speech_prob": 5.143055386724882e-05}, {"id": 447, "seek": 130252, "start": 1312.84, "end": 1315.16, "text": " that says, over a batch of samples,", "tokens": [50880, 300, 1619, 11, 670, 257, 15245, 295, 10938, 11, 50996], "temperature": 0.0, "avg_logprob": -0.15273378856146513, "compression_ratio": 1.8246268656716418, "no_speech_prob": 5.143055386724882e-05}, {"id": 448, "seek": 130252, "start": 1315.16, "end": 1318.76, "text": " I want the variance of each variable coming out of that neural", "tokens": [50996, 286, 528, 264, 21977, 295, 1184, 7006, 1348, 484, 295, 300, 18161, 51176], "temperature": 0.0, "avg_logprob": -0.15273378856146513, "compression_ratio": 1.8246268656716418, "no_speech_prob": 5.143055386724882e-05}, {"id": 449, "seek": 130252, "start": 1318.76, "end": 1320.8799999999999, "text": " net to be non-zero, to be above one, let's say.", "tokens": [51176, 2533, 281, 312, 2107, 12, 32226, 11, 281, 312, 3673, 472, 11, 718, 311, 584, 13, 51282], "temperature": 0.0, "avg_logprob": -0.15273378856146513, "compression_ratio": 1.8246268656716418, "no_speech_prob": 5.143055386724882e-05}, {"id": 450, "seek": 130252, "start": 1320.8799999999999, "end": 1322.84, "text": " So that's a hinge loss that says the variance", "tokens": [51282, 407, 300, 311, 257, 28822, 4470, 300, 1619, 264, 21977, 51380], "temperature": 0.0, "avg_logprob": -0.15273378856146513, "compression_ratio": 1.8246268656716418, "no_speech_prob": 5.143055386724882e-05}, {"id": 451, "seek": 130252, "start": 1322.84, "end": 1324.32, "text": " needs to be above one.", "tokens": [51380, 2203, 281, 312, 3673, 472, 13, 51454], "temperature": 0.0, "avg_logprob": -0.15273378856146513, "compression_ratio": 1.8246268656716418, "no_speech_prob": 5.143055386724882e-05}, {"id": 452, "seek": 130252, "start": 1324.32, "end": 1325.8, "text": " It's not enough because the system", "tokens": [51454, 467, 311, 406, 1547, 570, 264, 1185, 51528], "temperature": 0.0, "avg_logprob": -0.15273378856146513, "compression_ratio": 1.8246268656716418, "no_speech_prob": 5.143055386724882e-05}, {"id": 453, "seek": 130252, "start": 1325.8, "end": 1330.6399999999999, "text": " can still cheat by making all the variables the same", "tokens": [51528, 393, 920, 17470, 538, 1455, 439, 264, 9102, 264, 912, 51770], "temperature": 0.0, "avg_logprob": -0.15273378856146513, "compression_ratio": 1.8246268656716418, "no_speech_prob": 5.143055386724882e-05}, {"id": 454, "seek": 130252, "start": 1330.6399999999999, "end": 1332.28, "text": " or very highly correlated.", "tokens": [51770, 420, 588, 5405, 38574, 13, 51852], "temperature": 0.0, "avg_logprob": -0.15273378856146513, "compression_ratio": 1.8246268656716418, "no_speech_prob": 5.143055386724882e-05}, {"id": 455, "seek": 133228, "start": 1332.28, "end": 1333.8, "text": " So you have another cost that says,", "tokens": [50364, 407, 291, 362, 1071, 2063, 300, 1619, 11, 50440], "temperature": 0.0, "avg_logprob": -0.18121111709459692, "compression_ratio": 1.6528925619834711, "no_speech_prob": 4.0691847971174866e-05}, {"id": 456, "seek": 133228, "start": 1333.8, "end": 1336.44, "text": " I want them to be decorrelated.", "tokens": [50440, 286, 528, 552, 281, 312, 979, 284, 12004, 13, 50572], "temperature": 0.0, "avg_logprob": -0.18121111709459692, "compression_ratio": 1.6528925619834711, "no_speech_prob": 4.0691847971174866e-05}, {"id": 457, "seek": 133228, "start": 1336.44, "end": 1338.16, "text": " So basically, this has the effect", "tokens": [50572, 407, 1936, 11, 341, 575, 264, 1802, 50658], "temperature": 0.0, "avg_logprob": -0.18121111709459692, "compression_ratio": 1.6528925619834711, "no_speech_prob": 4.0691847971174866e-05}, {"id": 458, "seek": 133228, "start": 1338.16, "end": 1341.92, "text": " of enforcing the covariance matrix of that those Sx", "tokens": [50658, 295, 25495, 2175, 264, 49851, 719, 8141, 295, 300, 729, 318, 87, 50846], "temperature": 0.0, "avg_logprob": -0.18121111709459692, "compression_ratio": 1.6528925619834711, "no_speech_prob": 4.0691847971174866e-05}, {"id": 459, "seek": 133228, "start": 1341.92, "end": 1346.84, "text": " vectors over a batch to be close to the identity.", "tokens": [50846, 18875, 670, 257, 15245, 281, 312, 1998, 281, 264, 6575, 13, 51092], "temperature": 0.0, "avg_logprob": -0.18121111709459692, "compression_ratio": 1.6528925619834711, "no_speech_prob": 4.0691847971174866e-05}, {"id": 460, "seek": 133228, "start": 1346.84, "end": 1349.44, "text": " And it's not enough because the variables", "tokens": [51092, 400, 309, 311, 406, 1547, 570, 264, 9102, 51222], "temperature": 0.0, "avg_logprob": -0.18121111709459692, "compression_ratio": 1.6528925619834711, "no_speech_prob": 4.0691847971174866e-05}, {"id": 461, "seek": 133228, "start": 1349.44, "end": 1354.24, "text": " can be non-collapsed and correlated", "tokens": [51222, 393, 312, 2107, 12, 33891, 2382, 292, 293, 38574, 51462], "temperature": 0.0, "avg_logprob": -0.18121111709459692, "compression_ratio": 1.6528925619834711, "no_speech_prob": 4.0691847971174866e-05}, {"id": 462, "seek": 133228, "start": 1354.24, "end": 1356.24, "text": " but still dependent.", "tokens": [51462, 457, 920, 12334, 13, 51562], "temperature": 0.0, "avg_logprob": -0.18121111709459692, "compression_ratio": 1.6528925619834711, "no_speech_prob": 4.0691847971174866e-05}, {"id": 463, "seek": 133228, "start": 1356.24, "end": 1357.92, "text": " And so there's another trick that we do,", "tokens": [51562, 400, 370, 456, 311, 1071, 4282, 300, 321, 360, 11, 51646], "temperature": 0.0, "avg_logprob": -0.18121111709459692, "compression_ratio": 1.6528925619834711, "no_speech_prob": 4.0691847971174866e-05}, {"id": 464, "seek": 133228, "start": 1357.92, "end": 1361.36, "text": " and we have some theory that shows that it's not stupid,", "tokens": [51646, 293, 321, 362, 512, 5261, 300, 3110, 300, 309, 311, 406, 6631, 11, 51818], "temperature": 0.0, "avg_logprob": -0.18121111709459692, "compression_ratio": 1.6528925619834711, "no_speech_prob": 4.0691847971174866e-05}, {"id": 465, "seek": 136136, "start": 1361.36, "end": 1362.76, "text": " which is that you take the Sx vector,", "tokens": [50364, 597, 307, 300, 291, 747, 264, 318, 87, 8062, 11, 50434], "temperature": 0.0, "avg_logprob": -0.16697238327620864, "compression_ratio": 1.7879746835443038, "no_speech_prob": 0.00024913036031648517}, {"id": 466, "seek": 136136, "start": 1362.76, "end": 1365.28, "text": " you run it to some neural net that expands the dimension,", "tokens": [50434, 291, 1190, 309, 281, 512, 18161, 2533, 300, 33706, 264, 10139, 11, 50560], "temperature": 0.0, "avg_logprob": -0.16697238327620864, "compression_ratio": 1.7879746835443038, "no_speech_prob": 0.00024913036031648517}, {"id": 467, "seek": 136136, "start": 1365.28, "end": 1367.9599999999998, "text": " and then you apply those criteria on the covariance", "tokens": [50560, 293, 550, 291, 3079, 729, 11101, 322, 264, 49851, 719, 50694], "temperature": 0.0, "avg_logprob": -0.16697238327620864, "compression_ratio": 1.7879746835443038, "no_speech_prob": 0.00024913036031648517}, {"id": 468, "seek": 136136, "start": 1367.9599999999998, "end": 1369.08, "text": " matrix to the output.", "tokens": [50694, 8141, 281, 264, 5598, 13, 50750], "temperature": 0.0, "avg_logprob": -0.16697238327620864, "compression_ratio": 1.7879746835443038, "no_speech_prob": 0.00024913036031648517}, {"id": 469, "seek": 136136, "start": 1369.08, "end": 1372.6799999999998, "text": " And that makes the variables of Sx kind of more independent.", "tokens": [50750, 400, 300, 1669, 264, 9102, 295, 318, 87, 733, 295, 544, 6695, 13, 50930], "temperature": 0.0, "avg_logprob": -0.16697238327620864, "compression_ratio": 1.7879746835443038, "no_speech_prob": 0.00024913036031648517}, {"id": 470, "seek": 136136, "start": 1372.6799999999998, "end": 1375.24, "text": " Now, there's a major flaw with this, which is,", "tokens": [50930, 823, 11, 456, 311, 257, 2563, 13717, 365, 341, 11, 597, 307, 11, 51058], "temperature": 0.0, "avg_logprob": -0.16697238327620864, "compression_ratio": 1.7879746835443038, "no_speech_prob": 0.00024913036031648517}, {"id": 471, "seek": 136136, "start": 1375.24, "end": 1377.9199999999998, "text": " and that's the theory which I'm not going to talk about.", "tokens": [51058, 293, 300, 311, 264, 5261, 597, 286, 478, 406, 516, 281, 751, 466, 13, 51192], "temperature": 0.0, "avg_logprob": -0.16697238327620864, "compression_ratio": 1.7879746835443038, "no_speech_prob": 0.00024913036031648517}, {"id": 472, "seek": 136136, "start": 1377.9199999999998, "end": 1381.1999999999998, "text": " There's a flaw with all of this, which is that we're basically,", "tokens": [51192, 821, 311, 257, 13717, 365, 439, 295, 341, 11, 597, 307, 300, 321, 434, 1936, 11, 51356], "temperature": 0.0, "avg_logprob": -0.16697238327620864, "compression_ratio": 1.7879746835443038, "no_speech_prob": 0.00024913036031648517}, {"id": 473, "seek": 136136, "start": 1381.1999999999998, "end": 1383.12, "text": " we have an upper bound on information content,", "tokens": [51356, 321, 362, 364, 6597, 5472, 322, 1589, 2701, 11, 51452], "temperature": 0.0, "avg_logprob": -0.16697238327620864, "compression_ratio": 1.7879746835443038, "no_speech_prob": 0.00024913036031648517}, {"id": 474, "seek": 136136, "start": 1383.12, "end": 1385.9599999999998, "text": " and we're pushing it up, hoping that the actual information", "tokens": [51452, 293, 321, 434, 7380, 309, 493, 11, 7159, 300, 264, 3539, 1589, 51594], "temperature": 0.0, "avg_logprob": -0.16697238327620864, "compression_ratio": 1.7879746835443038, "no_speech_prob": 0.00024913036031648517}, {"id": 475, "seek": 136136, "start": 1385.9599999999998, "end": 1388.08, "text": " content will follow.", "tokens": [51594, 2701, 486, 1524, 13, 51700], "temperature": 0.0, "avg_logprob": -0.16697238327620864, "compression_ratio": 1.7879746835443038, "no_speech_prob": 0.00024913036031648517}, {"id": 476, "seek": 136136, "start": 1388.08, "end": 1389.6799999999998, "text": " And it's stupid, but it kind of works.", "tokens": [51700, 400, 309, 311, 6631, 11, 457, 309, 733, 295, 1985, 13, 51780], "temperature": 0.0, "avg_logprob": -0.16697238327620864, "compression_ratio": 1.7879746835443038, "no_speech_prob": 0.00024913036031648517}, {"id": 477, "seek": 139136, "start": 1391.6, "end": 1394.24, "text": " OK, so you can test those pre-training", "tokens": [50376, 2264, 11, 370, 291, 393, 1500, 729, 659, 12, 17227, 1760, 50508], "temperature": 0.0, "avg_logprob": -0.18825288404498183, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.00011058768723160028}, {"id": 478, "seek": 139136, "start": 1394.24, "end": 1395.24, "text": " for image recognition.", "tokens": [50508, 337, 3256, 11150, 13, 50558], "temperature": 0.0, "avg_logprob": -0.18825288404498183, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.00011058768723160028}, {"id": 479, "seek": 139136, "start": 1395.24, "end": 1398.08, "text": " For example, you show two different views of the same image,", "tokens": [50558, 1171, 1365, 11, 291, 855, 732, 819, 6809, 295, 264, 912, 3256, 11, 50700], "temperature": 0.0, "avg_logprob": -0.18825288404498183, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.00011058768723160028}, {"id": 480, "seek": 139136, "start": 1398.08, "end": 1401.4399999999998, "text": " train the network to produce identical representations", "tokens": [50700, 3847, 264, 3209, 281, 5258, 14800, 33358, 50868], "temperature": 0.0, "avg_logprob": -0.18825288404498183, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.00011058768723160028}, {"id": 481, "seek": 139136, "start": 1401.4399999999998, "end": 1403.4399999999998, "text": " for two different views of the same image,", "tokens": [50868, 337, 732, 819, 6809, 295, 264, 912, 3256, 11, 50968], "temperature": 0.0, "avg_logprob": -0.18825288404498183, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.00011058768723160028}, {"id": 482, "seek": 139136, "start": 1403.4399999999998, "end": 1407.24, "text": " and then you freeze the network and basically train", "tokens": [50968, 293, 550, 291, 15959, 264, 3209, 293, 1936, 3847, 51158], "temperature": 0.0, "avg_logprob": -0.18825288404498183, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.00011058768723160028}, {"id": 483, "seek": 139136, "start": 1407.24, "end": 1409.6399999999999, "text": " a linear classifier on top with ImageNet", "tokens": [51158, 257, 8213, 1508, 9902, 322, 1192, 365, 29903, 31890, 51278], "temperature": 0.0, "avg_logprob": -0.18825288404498183, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.00011058768723160028}, {"id": 484, "seek": 139136, "start": 1409.6399999999999, "end": 1411.12, "text": " and measure the performance.", "tokens": [51278, 293, 3481, 264, 3389, 13, 51352], "temperature": 0.0, "avg_logprob": -0.18825288404498183, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.00011058768723160028}, {"id": 485, "seek": 139136, "start": 1411.12, "end": 1416.24, "text": " And this Vcrag method that I just described", "tokens": [51352, 400, 341, 691, 66, 3731, 3170, 300, 286, 445, 7619, 51608], "temperature": 0.0, "avg_logprob": -0.18825288404498183, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.00011058768723160028}, {"id": 486, "seek": 139136, "start": 1416.24, "end": 1421.1999999999998, "text": " works just as well as isn't the top pack, let's say.", "tokens": [51608, 1985, 445, 382, 731, 382, 1943, 380, 264, 1192, 2844, 11, 718, 311, 584, 13, 51856], "temperature": 0.0, "avg_logprob": -0.18825288404498183, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.00011058768723160028}, {"id": 487, "seek": 142120, "start": 1421.24, "end": 1422.56, "text": " There's a bunch of different methods", "tokens": [50366, 821, 311, 257, 3840, 295, 819, 7150, 50432], "temperature": 0.0, "avg_logprob": -0.20352335922590647, "compression_ratio": 1.6529209621993126, "no_speech_prob": 0.0004371392715256661}, {"id": 488, "seek": 142120, "start": 1422.56, "end": 1424.44, "text": " that have similar performance.", "tokens": [50432, 300, 362, 2531, 3389, 13, 50526], "temperature": 0.0, "avg_logprob": -0.20352335922590647, "compression_ratio": 1.6529209621993126, "no_speech_prob": 0.0004371392715256661}, {"id": 489, "seek": 142120, "start": 1424.44, "end": 1425.8, "text": " And they are in the top pack.", "tokens": [50526, 400, 436, 366, 294, 264, 1192, 2844, 13, 50594], "temperature": 0.0, "avg_logprob": -0.20352335922590647, "compression_ratio": 1.6529209621993126, "no_speech_prob": 0.0004371392715256661}, {"id": 490, "seek": 142120, "start": 1425.8, "end": 1427.16, "text": " I'm not going to bore you with details.", "tokens": [50594, 286, 478, 406, 516, 281, 26002, 291, 365, 4365, 13, 50662], "temperature": 0.0, "avg_logprob": -0.20352335922590647, "compression_ratio": 1.6529209621993126, "no_speech_prob": 0.0004371392715256661}, {"id": 491, "seek": 142120, "start": 1427.16, "end": 1429.6000000000001, "text": " You can try to do segmentation as well.", "tokens": [50662, 509, 393, 853, 281, 360, 9469, 399, 382, 731, 13, 50784], "temperature": 0.0, "avg_logprob": -0.20352335922590647, "compression_ratio": 1.6529209621993126, "no_speech_prob": 0.0004371392715256661}, {"id": 492, "seek": 142120, "start": 1429.6000000000001, "end": 1431.72, "text": " Here's another method, somewhat similar,", "tokens": [50784, 1692, 311, 1071, 3170, 11, 8344, 2531, 11, 50890], "temperature": 0.0, "avg_logprob": -0.20352335922590647, "compression_ratio": 1.6529209621993126, "no_speech_prob": 0.0004371392715256661}, {"id": 493, "seek": 142120, "start": 1431.72, "end": 1434.88, "text": " but closer to the JEPA idea, which", "tokens": [50890, 457, 4966, 281, 264, 508, 8929, 32, 1558, 11, 597, 51048], "temperature": 0.0, "avg_logprob": -0.20352335922590647, "compression_ratio": 1.6529209621993126, "no_speech_prob": 0.0004371392715256661}, {"id": 494, "seek": 142120, "start": 1434.88, "end": 1437.2, "text": " uses a different criterion to prevent collapse, which", "tokens": [51048, 4960, 257, 819, 46691, 281, 4871, 15584, 11, 597, 51164], "temperature": 0.0, "avg_logprob": -0.20352335922590647, "compression_ratio": 1.6529209621993126, "no_speech_prob": 0.0004371392715256661}, {"id": 495, "seek": 142120, "start": 1437.2, "end": 1438.68, "text": " I'm not going to explain.", "tokens": [51164, 286, 478, 406, 516, 281, 2903, 13, 51238], "temperature": 0.0, "avg_logprob": -0.20352335922590647, "compression_ratio": 1.6529209621993126, "no_speech_prob": 0.0004371392715256661}, {"id": 496, "seek": 142120, "start": 1438.68, "end": 1443.1200000000001, "text": " And this one takes a partially masked input image", "tokens": [51238, 400, 341, 472, 2516, 257, 18886, 45249, 4846, 3256, 51460], "temperature": 0.0, "avg_logprob": -0.20352335922590647, "compression_ratio": 1.6529209621993126, "no_speech_prob": 0.0004371392715256661}, {"id": 497, "seek": 142120, "start": 1443.1200000000001, "end": 1445.44, "text": " together with a full input image, runs both of them", "tokens": [51460, 1214, 365, 257, 1577, 4846, 3256, 11, 6676, 1293, 295, 552, 51576], "temperature": 0.0, "avg_logprob": -0.20352335922590647, "compression_ratio": 1.6529209621993126, "no_speech_prob": 0.0004371392715256661}, {"id": 498, "seek": 142120, "start": 1445.44, "end": 1448.44, "text": " through encoders, and then trains a predictor", "tokens": [51576, 807, 2058, 378, 433, 11, 293, 550, 16329, 257, 6069, 284, 51726], "temperature": 0.0, "avg_logprob": -0.20352335922590647, "compression_ratio": 1.6529209621993126, "no_speech_prob": 0.0004371392715256661}, {"id": 499, "seek": 144844, "start": 1448.44, "end": 1453.28, "text": " to basically predict the representation of the full image", "tokens": [50364, 281, 1936, 6069, 264, 10290, 295, 264, 1577, 3256, 50606], "temperature": 0.0, "avg_logprob": -0.16448515786064996, "compression_ratio": 1.5982142857142858, "no_speech_prob": 0.0005789668648503721}, {"id": 500, "seek": 144844, "start": 1453.28, "end": 1456.24, "text": " from the representation computed", "tokens": [50606, 490, 264, 10290, 40610, 50754], "temperature": 0.0, "avg_logprob": -0.16448515786064996, "compression_ratio": 1.5982142857142858, "no_speech_prob": 0.0005789668648503721}, {"id": 501, "seek": 144844, "start": 1456.24, "end": 1459.8, "text": " from the partially masked image.", "tokens": [50754, 490, 264, 18886, 45249, 3256, 13, 50932], "temperature": 0.0, "avg_logprob": -0.16448515786064996, "compression_ratio": 1.5982142857142858, "no_speech_prob": 0.0005789668648503721}, {"id": 502, "seek": 144844, "start": 1459.8, "end": 1461.8, "text": " This is called IJEPA, ImageJEPA.", "tokens": [50932, 639, 307, 1219, 286, 41, 8929, 32, 11, 29903, 41, 8929, 32, 13, 51032], "temperature": 0.0, "avg_logprob": -0.16448515786064996, "compression_ratio": 1.5982142857142858, "no_speech_prob": 0.0005789668648503721}, {"id": 503, "seek": 144844, "start": 1461.8, "end": 1463.0, "text": " And it works amazingly well.", "tokens": [51032, 400, 309, 1985, 31762, 731, 13, 51092], "temperature": 0.0, "avg_logprob": -0.16448515786064996, "compression_ratio": 1.5982142857142858, "no_speech_prob": 0.0005789668648503721}, {"id": 504, "seek": 144844, "start": 1463.0, "end": 1469.3200000000002, "text": " And it's really fast to train very good performance.", "tokens": [51092, 400, 309, 311, 534, 2370, 281, 3847, 588, 665, 3389, 13, 51408], "temperature": 0.0, "avg_logprob": -0.16448515786064996, "compression_ratio": 1.5982142857142858, "no_speech_prob": 0.0005789668648503721}, {"id": 505, "seek": 144844, "start": 1469.3200000000002, "end": 1474.2, "text": " In terms of performance, even though this type of masking", "tokens": [51408, 682, 2115, 295, 3389, 11, 754, 1673, 341, 2010, 295, 31226, 51652], "temperature": 0.0, "avg_logprob": -0.16448515786064996, "compression_ratio": 1.5982142857142858, "no_speech_prob": 0.0005789668648503721}, {"id": 506, "seek": 144844, "start": 1474.2, "end": 1476.68, "text": " does not require any knowledge about the nature of the input,", "tokens": [51652, 775, 406, 3651, 604, 3601, 466, 264, 3687, 295, 264, 4846, 11, 51776], "temperature": 0.0, "avg_logprob": -0.16448515786064996, "compression_ratio": 1.5982142857142858, "no_speech_prob": 0.0005789668648503721}, {"id": 507, "seek": 147668, "start": 1476.72, "end": 1479.76, "text": " essentially, or very little, the still", "tokens": [50366, 4476, 11, 420, 588, 707, 11, 264, 920, 50518], "temperature": 0.0, "avg_logprob": -0.21824819901410272, "compression_ratio": 1.5191489361702128, "no_speech_prob": 0.0003624240052886307}, {"id": 508, "seek": 147668, "start": 1479.76, "end": 1481.04, "text": " you get the same kind of performance", "tokens": [50518, 291, 483, 264, 912, 733, 295, 3389, 50582], "temperature": 0.0, "avg_logprob": -0.21824819901410272, "compression_ratio": 1.5191489361702128, "no_speech_prob": 0.0003624240052886307}, {"id": 509, "seek": 147668, "start": 1481.04, "end": 1484.76, "text": " that you would get if you used a self-supervised learning", "tokens": [50582, 300, 291, 576, 483, 498, 291, 1143, 257, 2698, 12, 48172, 24420, 2539, 50768], "temperature": 0.0, "avg_logprob": -0.21824819901410272, "compression_ratio": 1.5191489361702128, "no_speech_prob": 0.0003624240052886307}, {"id": 510, "seek": 147668, "start": 1484.76, "end": 1487.8, "text": " method that exploits the fact that you're", "tokens": [50768, 3170, 300, 12382, 1208, 264, 1186, 300, 291, 434, 50920], "temperature": 0.0, "avg_logprob": -0.21824819901410272, "compression_ratio": 1.5191489361702128, "no_speech_prob": 0.0003624240052886307}, {"id": 511, "seek": 147668, "start": 1487.8, "end": 1491.68, "text": " doing image recognition, like Dino or Ibot or Simclear,", "tokens": [50920, 884, 3256, 11150, 11, 411, 413, 2982, 420, 286, 18870, 420, 3998, 43679, 11, 51114], "temperature": 0.0, "avg_logprob": -0.21824819901410272, "compression_ratio": 1.5191489361702128, "no_speech_prob": 0.0003624240052886307}, {"id": 512, "seek": 147668, "start": 1491.68, "end": 1492.2, "text": " for example.", "tokens": [51114, 337, 1365, 13, 51140], "temperature": 0.0, "avg_logprob": -0.21824819901410272, "compression_ratio": 1.5191489361702128, "no_speech_prob": 0.0003624240052886307}, {"id": 513, "seek": 147668, "start": 1495.0800000000002, "end": 1499.48, "text": " OK, now, how are you going to use this in the end?", "tokens": [51284, 2264, 11, 586, 11, 577, 366, 291, 516, 281, 764, 341, 294, 264, 917, 30, 51504], "temperature": 0.0, "avg_logprob": -0.21824819901410272, "compression_ratio": 1.5191489361702128, "no_speech_prob": 0.0003624240052886307}, {"id": 514, "seek": 147668, "start": 1499.48, "end": 1505.1200000000001, "text": " What I'm really interested in is to use JEPAs as world models", "tokens": [51504, 708, 286, 478, 534, 3102, 294, 307, 281, 764, 508, 8929, 10884, 382, 1002, 5245, 51786], "temperature": 0.0, "avg_logprob": -0.21824819901410272, "compression_ratio": 1.5191489361702128, "no_speech_prob": 0.0003624240052886307}, {"id": 515, "seek": 150512, "start": 1505.12, "end": 1506.04, "text": " inside of the system.", "tokens": [50364, 1854, 295, 264, 1185, 13, 50410], "temperature": 0.0, "avg_logprob": -0.1855315399169922, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.0008035074570216238}, {"id": 516, "seek": 150512, "start": 1506.04, "end": 1508.32, "text": " They can do system two type planning,", "tokens": [50410, 814, 393, 360, 1185, 732, 2010, 5038, 11, 50524], "temperature": 0.0, "avg_logprob": -0.1855315399169922, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.0008035074570216238}, {"id": 517, "seek": 150512, "start": 1508.32, "end": 1512.52, "text": " but even better than this, they can do hierarchical planning.", "tokens": [50524, 457, 754, 1101, 813, 341, 11, 436, 393, 360, 35250, 804, 5038, 13, 50734], "temperature": 0.0, "avg_logprob": -0.1855315399169922, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.0008035074570216238}, {"id": 518, "seek": 150512, "start": 1512.52, "end": 1518.9199999999998, "text": " And the idea there is that when you think about a task,", "tokens": [50734, 400, 264, 1558, 456, 307, 300, 562, 291, 519, 466, 257, 5633, 11, 51054], "temperature": 0.0, "avg_logprob": -0.1855315399169922, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.0008035074570216238}, {"id": 519, "seek": 150512, "start": 1518.9199999999998, "end": 1522.7199999999998, "text": " you're not planning this task at the lowest level", "tokens": [51054, 291, 434, 406, 5038, 341, 5633, 412, 264, 12437, 1496, 51244], "temperature": 0.0, "avg_logprob": -0.1855315399169922, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.0008035074570216238}, {"id": 520, "seek": 150512, "start": 1522.7199999999998, "end": 1526.7199999999998, "text": " in terms of millisecond by millisecond muscle control.", "tokens": [51244, 294, 2115, 295, 27940, 18882, 538, 27940, 18882, 8679, 1969, 13, 51444], "temperature": 0.0, "avg_logprob": -0.1855315399169922, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.0008035074570216238}, {"id": 521, "seek": 150512, "start": 1526.7199999999998, "end": 1528.1999999999998, "text": " You're playing a task like, I want", "tokens": [51444, 509, 434, 2433, 257, 5633, 411, 11, 286, 528, 51518], "temperature": 0.0, "avg_logprob": -0.1855315399169922, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.0008035074570216238}, {"id": 522, "seek": 150512, "start": 1528.1999999999998, "end": 1531.6399999999999, "text": " to go from Santa Fe to New York, or let's say", "tokens": [51518, 281, 352, 490, 9933, 3697, 281, 1873, 3609, 11, 420, 718, 311, 584, 51690], "temperature": 0.0, "avg_logprob": -0.1855315399169922, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.0008035074570216238}, {"id": 523, "seek": 153164, "start": 1531.64, "end": 1535.4, "text": " from New York to Santa Fe, that's a better example.", "tokens": [50364, 490, 1873, 3609, 281, 9933, 3697, 11, 300, 311, 257, 1101, 1365, 13, 50552], "temperature": 0.0, "avg_logprob": -0.15712122945390508, "compression_ratio": 1.9696969696969697, "no_speech_prob": 0.001263796235434711}, {"id": 524, "seek": 153164, "start": 1535.4, "end": 1538.92, "text": " So you first decompose this into two sub-tasks.", "tokens": [50552, 407, 291, 700, 22867, 541, 341, 666, 732, 1422, 12, 83, 296, 1694, 13, 50728], "temperature": 0.0, "avg_logprob": -0.15712122945390508, "compression_ratio": 1.9696969696969697, "no_speech_prob": 0.001263796235434711}, {"id": 525, "seek": 153164, "start": 1538.92, "end": 1541.2, "text": " First thing I need to do is go to the airport", "tokens": [50728, 2386, 551, 286, 643, 281, 360, 307, 352, 281, 264, 10155, 50842], "temperature": 0.0, "avg_logprob": -0.15712122945390508, "compression_ratio": 1.9696969696969697, "no_speech_prob": 0.001263796235434711}, {"id": 526, "seek": 153164, "start": 1541.2, "end": 1543.2, "text": " and catch a plane.", "tokens": [50842, 293, 3745, 257, 5720, 13, 50942], "temperature": 0.0, "avg_logprob": -0.15712122945390508, "compression_ratio": 1.9696969696969697, "no_speech_prob": 0.001263796235434711}, {"id": 527, "seek": 153164, "start": 1543.2, "end": 1544.2800000000002, "text": " How do I go to the airport?", "tokens": [50942, 1012, 360, 286, 352, 281, 264, 10155, 30, 50996], "temperature": 0.0, "avg_logprob": -0.15712122945390508, "compression_ratio": 1.9696969696969697, "no_speech_prob": 0.001263796235434711}, {"id": 528, "seek": 153164, "start": 1544.2800000000002, "end": 1545.3600000000001, "text": " Well, to go to the airport, I need", "tokens": [50996, 1042, 11, 281, 352, 281, 264, 10155, 11, 286, 643, 51050], "temperature": 0.0, "avg_logprob": -0.15712122945390508, "compression_ratio": 1.9696969696969697, "no_speech_prob": 0.001263796235434711}, {"id": 529, "seek": 153164, "start": 1545.3600000000001, "end": 1546.96, "text": " to get on the street and have a taxi,", "tokens": [51050, 281, 483, 322, 264, 4838, 293, 362, 257, 18984, 11, 51130], "temperature": 0.0, "avg_logprob": -0.15712122945390508, "compression_ratio": 1.9696969696969697, "no_speech_prob": 0.001263796235434711}, {"id": 530, "seek": 153164, "start": 1546.96, "end": 1551.0400000000002, "text": " which you can do in New York City, not in Santa Fe.", "tokens": [51130, 597, 291, 393, 360, 294, 1873, 3609, 4392, 11, 406, 294, 9933, 3697, 13, 51334], "temperature": 0.0, "avg_logprob": -0.15712122945390508, "compression_ratio": 1.9696969696969697, "no_speech_prob": 0.001263796235434711}, {"id": 531, "seek": 153164, "start": 1551.0400000000002, "end": 1552.3600000000001, "text": " How do I get down in the street?", "tokens": [51334, 1012, 360, 286, 483, 760, 294, 264, 4838, 30, 51400], "temperature": 0.0, "avg_logprob": -0.15712122945390508, "compression_ratio": 1.9696969696969697, "no_speech_prob": 0.001263796235434711}, {"id": 532, "seek": 153164, "start": 1552.3600000000001, "end": 1555.88, "text": " I need to get out of the building I'm in, et cetera.", "tokens": [51400, 286, 643, 281, 483, 484, 295, 264, 2390, 286, 478, 294, 11, 1030, 11458, 13, 51576], "temperature": 0.0, "avg_logprob": -0.15712122945390508, "compression_ratio": 1.9696969696969697, "no_speech_prob": 0.001263796235434711}, {"id": 533, "seek": 153164, "start": 1555.88, "end": 1557.0400000000002, "text": " How do I get out of the building?", "tokens": [51576, 1012, 360, 286, 483, 484, 295, 264, 2390, 30, 51634], "temperature": 0.0, "avg_logprob": -0.15712122945390508, "compression_ratio": 1.9696969696969697, "no_speech_prob": 0.001263796235434711}, {"id": 534, "seek": 153164, "start": 1557.0400000000002, "end": 1559.5600000000002, "text": " I need to stand up from my chair, walk to the door.", "tokens": [51634, 286, 643, 281, 1463, 493, 490, 452, 6090, 11, 1792, 281, 264, 2853, 13, 51760], "temperature": 0.0, "avg_logprob": -0.15712122945390508, "compression_ratio": 1.9696969696969697, "no_speech_prob": 0.001263796235434711}, {"id": 535, "seek": 153164, "start": 1559.5600000000002, "end": 1561.24, "text": " How do I get up from my chair?", "tokens": [51760, 1012, 360, 286, 483, 493, 490, 452, 6090, 30, 51844], "temperature": 0.0, "avg_logprob": -0.15712122945390508, "compression_ratio": 1.9696969696969697, "no_speech_prob": 0.001263796235434711}, {"id": 536, "seek": 156124, "start": 1561.24, "end": 1563.0, "text": " So you kind of decompose this all the way down", "tokens": [50364, 407, 291, 733, 295, 22867, 541, 341, 439, 264, 636, 760, 50452], "temperature": 0.0, "avg_logprob": -0.16447065616476125, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.0007299348362721503}, {"id": 537, "seek": 156124, "start": 1563.0, "end": 1566.32, "text": " to the lowest level millisecond muscle control.", "tokens": [50452, 281, 264, 12437, 1496, 27940, 18882, 8679, 1969, 13, 50618], "temperature": 0.0, "avg_logprob": -0.16447065616476125, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.0007299348362721503}, {"id": 538, "seek": 156124, "start": 1566.32, "end": 1570.72, "text": " But you're not going to plan the entire task of going", "tokens": [50618, 583, 291, 434, 406, 516, 281, 1393, 264, 2302, 5633, 295, 516, 50838], "temperature": 0.0, "avg_logprob": -0.16447065616476125, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.0007299348362721503}, {"id": 539, "seek": 156124, "start": 1570.72, "end": 1576.72, "text": " from New York to Santa Fe all the way down to millisecond", "tokens": [50838, 490, 1873, 3609, 281, 9933, 3697, 439, 264, 636, 760, 281, 27940, 18882, 51138], "temperature": 0.0, "avg_logprob": -0.16447065616476125, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.0007299348362721503}, {"id": 540, "seek": 156124, "start": 1576.72, "end": 1577.96, "text": " by millisecond muscle control.", "tokens": [51138, 538, 27940, 18882, 8679, 1969, 13, 51200], "temperature": 0.0, "avg_logprob": -0.16447065616476125, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.0007299348362721503}, {"id": 541, "seek": 156124, "start": 1577.96, "end": 1580.28, "text": " You do a hierarchical planning.", "tokens": [51200, 509, 360, 257, 35250, 804, 5038, 13, 51316], "temperature": 0.0, "avg_logprob": -0.16447065616476125, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.0007299348362721503}, {"id": 542, "seek": 156124, "start": 1580.28, "end": 1582.76, "text": " We think humans, that we are the only ones who can do this.", "tokens": [51316, 492, 519, 6255, 11, 300, 321, 366, 264, 787, 2306, 567, 393, 360, 341, 13, 51440], "temperature": 0.0, "avg_logprob": -0.16447065616476125, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.0007299348362721503}, {"id": 543, "seek": 156124, "start": 1582.76, "end": 1584.68, "text": " Animals do this, too.", "tokens": [51440, 47164, 360, 341, 11, 886, 13, 51536], "temperature": 0.0, "avg_logprob": -0.16447065616476125, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.0007299348362721503}, {"id": 544, "seek": 156124, "start": 1584.68, "end": 1586.72, "text": " You observe the cat planning a trajectory", "tokens": [51536, 509, 11441, 264, 3857, 5038, 257, 21512, 51638], "temperature": 0.0, "avg_logprob": -0.16447065616476125, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.0007299348362721503}, {"id": 545, "seek": 156124, "start": 1586.72, "end": 1589.48, "text": " to jump on a piece of furniture.", "tokens": [51638, 281, 3012, 322, 257, 2522, 295, 15671, 13, 51776], "temperature": 0.0, "avg_logprob": -0.16447065616476125, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.0007299348362721503}, {"id": 546, "seek": 158948, "start": 1589.52, "end": 1591.44, "text": " They definitely do a hierarchical planning.", "tokens": [50366, 814, 2138, 360, 257, 35250, 804, 5038, 13, 50462], "temperature": 0.0, "avg_logprob": -0.21370130372278898, "compression_ratio": 1.8558951965065502, "no_speech_prob": 0.0002867782022804022}, {"id": 547, "seek": 158948, "start": 1591.44, "end": 1594.28, "text": " So basically, what you do, what you need for this", "tokens": [50462, 407, 1936, 11, 437, 291, 360, 11, 437, 291, 643, 337, 341, 50604], "temperature": 0.0, "avg_logprob": -0.21370130372278898, "compression_ratio": 1.8558951965065502, "no_speech_prob": 0.0002867782022804022}, {"id": 548, "seek": 158948, "start": 1594.28, "end": 1599.2, "text": " is a sort of hierarchy of JPA architectures of predictors", "tokens": [50604, 307, 257, 1333, 295, 22333, 295, 508, 10297, 6331, 1303, 295, 6069, 830, 50850], "temperature": 0.0, "avg_logprob": -0.21370130372278898, "compression_ratio": 1.8558951965065502, "no_speech_prob": 0.0002867782022804022}, {"id": 549, "seek": 158948, "start": 1599.2, "end": 1601.76, "text": " that progressively produce more and more", "tokens": [50850, 300, 46667, 5258, 544, 293, 544, 50978], "temperature": 0.0, "avg_logprob": -0.21370130372278898, "compression_ratio": 1.8558951965065502, "no_speech_prob": 0.0002867782022804022}, {"id": 550, "seek": 158948, "start": 1601.76, "end": 1604.84, "text": " abstract representations of the state of the world,", "tokens": [50978, 12649, 33358, 295, 264, 1785, 295, 264, 1002, 11, 51132], "temperature": 0.0, "avg_logprob": -0.21370130372278898, "compression_ratio": 1.8558951965065502, "no_speech_prob": 0.0002867782022804022}, {"id": 551, "seek": 158948, "start": 1604.84, "end": 1610.72, "text": " so that in the very abstract space of representations,", "tokens": [51132, 370, 300, 294, 264, 588, 12649, 1901, 295, 33358, 11, 51426], "temperature": 0.0, "avg_logprob": -0.21370130372278898, "compression_ratio": 1.8558951965065502, "no_speech_prob": 0.0002867782022804022}, {"id": 552, "seek": 158948, "start": 1610.72, "end": 1612.1200000000001, "text": " you can make long-term predictions.", "tokens": [51426, 291, 393, 652, 938, 12, 7039, 21264, 13, 51496], "temperature": 0.0, "avg_logprob": -0.21370130372278898, "compression_ratio": 1.8558951965065502, "no_speech_prob": 0.0002867782022804022}, {"id": 553, "seek": 158948, "start": 1612.1200000000001, "end": 1615.56, "text": " Whereas in the sort of lower levels of abstraction,", "tokens": [51496, 13813, 294, 264, 1333, 295, 3126, 4358, 295, 37765, 11, 51668], "temperature": 0.0, "avg_logprob": -0.21370130372278898, "compression_ratio": 1.8558951965065502, "no_speech_prob": 0.0002867782022804022}, {"id": 554, "seek": 158948, "start": 1615.56, "end": 1617.64, "text": " you can make shorter term prediction,", "tokens": [51668, 291, 393, 652, 11639, 1433, 17630, 11, 51772], "temperature": 0.0, "avg_logprob": -0.21370130372278898, "compression_ratio": 1.8558951965065502, "no_speech_prob": 0.0002867782022804022}, {"id": 555, "seek": 161764, "start": 1617.64, "end": 1619.96, "text": " but they're more accurate in the short term.", "tokens": [50364, 457, 436, 434, 544, 8559, 294, 264, 2099, 1433, 13, 50480], "temperature": 0.0, "avg_logprob": -0.16250289036677434, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.00019100184726994485}, {"id": 556, "seek": 161764, "start": 1619.96, "end": 1622.2, "text": " So this is a two-level architecture.", "tokens": [50480, 407, 341, 307, 257, 732, 12, 12418, 9482, 13, 50592], "temperature": 0.0, "avg_logprob": -0.16250289036677434, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.00019100184726994485}, {"id": 557, "seek": 161764, "start": 1622.2, "end": 1624.24, "text": " Low-level, you can make short-term predictions.", "tokens": [50592, 17078, 12, 12418, 11, 291, 393, 652, 2099, 12, 7039, 21264, 13, 50694], "temperature": 0.0, "avg_logprob": -0.16250289036677434, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.00019100184726994485}, {"id": 558, "seek": 161764, "start": 1624.24, "end": 1626.24, "text": " High-level, you can make longer-term prediction", "tokens": [50694, 5229, 12, 12418, 11, 291, 393, 652, 2854, 12, 7039, 17630, 50794], "temperature": 0.0, "avg_logprob": -0.16250289036677434, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.00019100184726994485}, {"id": 559, "seek": 161764, "start": 1626.24, "end": 1628.8000000000002, "text": " in a more abstract space that has less details", "tokens": [50794, 294, 257, 544, 12649, 1901, 300, 575, 1570, 4365, 50922], "temperature": 0.0, "avg_logprob": -0.16250289036677434, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.00019100184726994485}, {"id": 560, "seek": 161764, "start": 1628.8000000000002, "end": 1631.2, "text": " about how the world works.", "tokens": [50922, 466, 577, 264, 1002, 1985, 13, 51042], "temperature": 0.0, "avg_logprob": -0.16250289036677434, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.00019100184726994485}, {"id": 561, "seek": 161764, "start": 1631.2, "end": 1636.0, "text": " Now, we've been able to train a particular instance of JPA", "tokens": [51042, 823, 11, 321, 600, 668, 1075, 281, 3847, 257, 1729, 5197, 295, 508, 10297, 51282], "temperature": 0.0, "avg_logprob": -0.16250289036677434, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.00019100184726994485}, {"id": 562, "seek": 161764, "start": 1636.0, "end": 1638.0800000000002, "text": " that simultaneously learns teachers that", "tokens": [51282, 300, 16561, 27152, 6023, 300, 51386], "temperature": 0.0, "avg_logprob": -0.16250289036677434, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.00019100184726994485}, {"id": 563, "seek": 161764, "start": 1638.0800000000002, "end": 1642.24, "text": " are good for image recognition and motion prediction", "tokens": [51386, 366, 665, 337, 3256, 11150, 293, 5394, 17630, 51594], "temperature": 0.0, "avg_logprob": -0.16250289036677434, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.00019100184726994485}, {"id": 564, "seek": 161764, "start": 1642.24, "end": 1643.68, "text": " in images.", "tokens": [51594, 294, 5267, 13, 51666], "temperature": 0.0, "avg_logprob": -0.16250289036677434, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.00019100184726994485}, {"id": 565, "seek": 161764, "start": 1643.68, "end": 1645.92, "text": " And I'm not going to go into the details of how", "tokens": [51666, 400, 286, 478, 406, 516, 281, 352, 666, 264, 4365, 295, 577, 51778], "temperature": 0.0, "avg_logprob": -0.16250289036677434, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.00019100184726994485}, {"id": 566, "seek": 161764, "start": 1645.92, "end": 1646.92, "text": " this is pretty hairy.", "tokens": [51778, 341, 307, 1238, 42346, 13, 51828], "temperature": 0.0, "avg_logprob": -0.16250289036677434, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.00019100184726994485}, {"id": 567, "seek": 164692, "start": 1646.92, "end": 1649.16, "text": " But it's kind of hierarchical.", "tokens": [50364, 583, 309, 311, 733, 295, 35250, 804, 13, 50476], "temperature": 0.0, "avg_logprob": -0.1880969786911868, "compression_ratio": 1.5478260869565217, "no_speech_prob": 0.00010880234185606241}, {"id": 568, "seek": 164692, "start": 1649.16, "end": 1654.8400000000001, "text": " And it's got predictors that make pretty strong assumptions", "tokens": [50476, 400, 309, 311, 658, 6069, 830, 300, 652, 1238, 2068, 17695, 50760], "temperature": 0.0, "avg_logprob": -0.1880969786911868, "compression_ratio": 1.5478260869565217, "no_speech_prob": 0.00010880234185606241}, {"id": 569, "seek": 164692, "start": 1654.8400000000001, "end": 1657.68, "text": " about the type of prediction that can occur.", "tokens": [50760, 466, 264, 2010, 295, 17630, 300, 393, 5160, 13, 50902], "temperature": 0.0, "avg_logprob": -0.1880969786911868, "compression_ratio": 1.5478260869565217, "no_speech_prob": 0.00010880234185606241}, {"id": 570, "seek": 164692, "start": 1657.68, "end": 1659.8400000000001, "text": " And simultaneously learns invariant features", "tokens": [50902, 400, 16561, 27152, 33270, 394, 4122, 51010], "temperature": 0.0, "avg_logprob": -0.1880969786911868, "compression_ratio": 1.5478260869565217, "no_speech_prob": 0.00010880234185606241}, {"id": 571, "seek": 164692, "start": 1659.8400000000001, "end": 1662.04, "text": " for image recognition.", "tokens": [51010, 337, 3256, 11150, 13, 51120], "temperature": 0.0, "avg_logprob": -0.1880969786911868, "compression_ratio": 1.5478260869565217, "no_speech_prob": 0.00010880234185606241}, {"id": 572, "seek": 164692, "start": 1662.04, "end": 1665.5600000000002, "text": " And this works really well for things like image segmentation,", "tokens": [51120, 400, 341, 1985, 534, 731, 337, 721, 411, 3256, 9469, 399, 11, 51296], "temperature": 0.0, "avg_logprob": -0.1880969786911868, "compression_ratio": 1.5478260869565217, "no_speech_prob": 0.00010880234185606241}, {"id": 573, "seek": 164692, "start": 1665.5600000000002, "end": 1669.0, "text": " depth estimation, tracking, et cetera.", "tokens": [51296, 7161, 35701, 11, 11603, 11, 1030, 11458, 13, 51468], "temperature": 0.0, "avg_logprob": -0.1880969786911868, "compression_ratio": 1.5478260869565217, "no_speech_prob": 0.00010880234185606241}, {"id": 574, "seek": 164692, "start": 1669.0, "end": 1675.3200000000002, "text": " It's called MCJPA, which means motion and content.", "tokens": [51468, 467, 311, 1219, 8797, 41, 10297, 11, 597, 1355, 5394, 293, 2701, 13, 51784], "temperature": 0.0, "avg_logprob": -0.1880969786911868, "compression_ratio": 1.5478260869565217, "no_speech_prob": 0.00010880234185606241}, {"id": 575, "seek": 167532, "start": 1675.76, "end": 1678.76, "text": " And with this, hopefully, one day,", "tokens": [50386, 400, 365, 341, 11, 4696, 11, 472, 786, 11, 50536], "temperature": 0.0, "avg_logprob": -0.25107865286345527, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0004953966126777232}, {"id": 576, "seek": 167532, "start": 1678.76, "end": 1681.6799999999998, "text": " we'll be able to build architectures that", "tokens": [50536, 321, 603, 312, 1075, 281, 1322, 6331, 1303, 300, 50682], "temperature": 0.0, "avg_logprob": -0.25107865286345527, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0004953966126777232}, {"id": 577, "seek": 167532, "start": 1681.6799999999998, "end": 1683.76, "text": " can perform hierarchical tanning of the type that I", "tokens": [50682, 393, 2042, 35250, 804, 7603, 773, 295, 264, 2010, 300, 286, 50786], "temperature": 0.0, "avg_logprob": -0.25107865286345527, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0004953966126777232}, {"id": 578, "seek": 167532, "start": 1683.76, "end": 1685.8, "text": " was telling you about.", "tokens": [50786, 390, 3585, 291, 466, 13, 50888], "temperature": 0.0, "avg_logprob": -0.25107865286345527, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0004953966126777232}, {"id": 579, "seek": 167532, "start": 1685.8, "end": 1691.6799999999998, "text": " So observe the world, compute the abstract representation,", "tokens": [50888, 407, 11441, 264, 1002, 11, 14722, 264, 12649, 10290, 11, 51182], "temperature": 0.0, "avg_logprob": -0.25107865286345527, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0004953966126777232}, {"id": 580, "seek": 167532, "start": 1691.6799999999998, "end": 1693.36, "text": " and even more abstract representation,", "tokens": [51182, 293, 754, 544, 12649, 10290, 11, 51266], "temperature": 0.0, "avg_logprob": -0.25107865286345527, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0004953966126777232}, {"id": 581, "seek": 167532, "start": 1693.36, "end": 1695.48, "text": " even more abstract representation,", "tokens": [51266, 754, 544, 12649, 10290, 11, 51372], "temperature": 0.0, "avg_logprob": -0.25107865286345527, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0004953966126777232}, {"id": 582, "seek": 167532, "start": 1695.48, "end": 1698.12, "text": " make a prediction to minimize a particular cost", "tokens": [51372, 652, 257, 17630, 281, 17522, 257, 1729, 2063, 51504], "temperature": 0.0, "avg_logprob": -0.25107865286345527, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0004953966126777232}, {"id": 583, "seek": 167532, "start": 1698.12, "end": 1700.56, "text": " function that defines your task.", "tokens": [51504, 2445, 300, 23122, 428, 5633, 13, 51626], "temperature": 0.0, "avg_logprob": -0.25107865286345527, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0004953966126777232}, {"id": 584, "seek": 167532, "start": 1700.56, "end": 1702.6399999999999, "text": " I'm assuming this cost function is differentiable,", "tokens": [51626, 286, 478, 11926, 341, 2063, 2445, 307, 819, 9364, 11, 51730], "temperature": 0.0, "avg_logprob": -0.25107865286345527, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0004953966126777232}, {"id": 585, "seek": 170264, "start": 1702.64, "end": 1706.5600000000002, "text": " so we can do this inference by gradient descent.", "tokens": [50364, 370, 321, 393, 360, 341, 38253, 538, 16235, 23475, 13, 50560], "temperature": 0.0, "avg_logprob": -0.19039956797724183, "compression_ratio": 1.8049792531120332, "no_speech_prob": 0.00026936130598187447}, {"id": 586, "seek": 170264, "start": 1706.5600000000002, "end": 1707.88, "text": " In first, some latent variable that", "tokens": [50560, 682, 700, 11, 512, 48994, 7006, 300, 50626], "temperature": 0.0, "avg_logprob": -0.19039956797724183, "compression_ratio": 1.8049792531120332, "no_speech_prob": 0.00026936130598187447}, {"id": 587, "seek": 170264, "start": 1707.88, "end": 1709.92, "text": " may represent the macro action you're going to take,", "tokens": [50626, 815, 2906, 264, 18887, 3069, 291, 434, 516, 281, 747, 11, 50728], "temperature": 0.0, "avg_logprob": -0.19039956797724183, "compression_ratio": 1.8049792531120332, "no_speech_prob": 0.00026936130598187447}, {"id": 588, "seek": 170264, "start": 1709.92, "end": 1713.72, "text": " or some unknown variable about the world.", "tokens": [50728, 420, 512, 9841, 7006, 466, 264, 1002, 13, 50918], "temperature": 0.0, "avg_logprob": -0.19039956797724183, "compression_ratio": 1.8049792531120332, "no_speech_prob": 0.00026936130598187447}, {"id": 589, "seek": 170264, "start": 1713.72, "end": 1715.5200000000002, "text": " And then the state you're going to obtain", "tokens": [50918, 400, 550, 264, 1785, 291, 434, 516, 281, 12701, 51008], "temperature": 0.0, "avg_logprob": -0.19039956797724183, "compression_ratio": 1.8049792531120332, "no_speech_prob": 0.00026936130598187447}, {"id": 590, "seek": 170264, "start": 1715.5200000000002, "end": 1716.96, "text": " through the first prediction is going", "tokens": [51008, 807, 264, 700, 17630, 307, 516, 51080], "temperature": 0.0, "avg_logprob": -0.19039956797724183, "compression_ratio": 1.8049792531120332, "no_speech_prob": 0.00026936130598187447}, {"id": 591, "seek": 170264, "start": 1716.96, "end": 1720.48, "text": " to constitute a cost function for the lowest level.", "tokens": [51080, 281, 41658, 257, 2063, 2445, 337, 264, 12437, 1496, 13, 51256], "temperature": 0.0, "avg_logprob": -0.19039956797724183, "compression_ratio": 1.8049792531120332, "no_speech_prob": 0.00026936130598187447}, {"id": 592, "seek": 170264, "start": 1720.48, "end": 1725.44, "text": " So the first predictor at the top tells me", "tokens": [51256, 407, 264, 700, 6069, 284, 412, 264, 1192, 5112, 385, 51504], "temperature": 0.0, "avg_logprob": -0.19039956797724183, "compression_ratio": 1.8049792531120332, "no_speech_prob": 0.00026936130598187447}, {"id": 593, "seek": 170264, "start": 1725.44, "end": 1727.3200000000002, "text": " I should be at the airport.", "tokens": [51504, 286, 820, 312, 412, 264, 10155, 13, 51598], "temperature": 0.0, "avg_logprob": -0.19039956797724183, "compression_ratio": 1.8049792531120332, "no_speech_prob": 0.00026936130598187447}, {"id": 594, "seek": 170264, "start": 1727.3200000000002, "end": 1728.76, "text": " I started from New York.", "tokens": [51598, 286, 1409, 490, 1873, 3609, 13, 51670], "temperature": 0.0, "avg_logprob": -0.19039956797724183, "compression_ratio": 1.8049792531120332, "no_speech_prob": 0.00026936130598187447}, {"id": 595, "seek": 170264, "start": 1728.76, "end": 1730.16, "text": " I should be at the airport.", "tokens": [51670, 286, 820, 312, 412, 264, 10155, 13, 51740], "temperature": 0.0, "avg_logprob": -0.19039956797724183, "compression_ratio": 1.8049792531120332, "no_speech_prob": 0.00026936130598187447}, {"id": 596, "seek": 173016, "start": 1730.2, "end": 1734.3600000000001, "text": " The cost function below measures how far I am from the airport.", "tokens": [50366, 440, 2063, 2445, 2507, 8000, 577, 1400, 286, 669, 490, 264, 10155, 13, 50574], "temperature": 0.0, "avg_logprob": -0.16898503023035386, "compression_ratio": 1.7047619047619047, "no_speech_prob": 7.841953629394993e-05}, {"id": 597, "seek": 173016, "start": 1734.3600000000001, "end": 1737.64, "text": " And so the second predictor says, go down in the street.", "tokens": [50574, 400, 370, 264, 1150, 6069, 284, 1619, 11, 352, 760, 294, 264, 4838, 13, 50738], "temperature": 0.0, "avg_logprob": -0.16898503023035386, "compression_ratio": 1.7047619047619047, "no_speech_prob": 7.841953629394993e-05}, {"id": 598, "seek": 173016, "start": 1737.64, "end": 1740.3600000000001, "text": " Take a cab to the airport.", "tokens": [50738, 3664, 257, 5487, 281, 264, 10155, 13, 50874], "temperature": 0.0, "avg_logprob": -0.16898503023035386, "compression_ratio": 1.7047619047619047, "no_speech_prob": 7.841953629394993e-05}, {"id": 599, "seek": 173016, "start": 1740.3600000000001, "end": 1743.52, "text": " And so the cost function at the bottom here", "tokens": [50874, 400, 370, 264, 2063, 2445, 412, 264, 2767, 510, 51032], "temperature": 0.0, "avg_logprob": -0.16898503023035386, "compression_ratio": 1.7047619047619047, "no_speech_prob": 7.841953629394993e-05}, {"id": 600, "seek": 173016, "start": 1743.52, "end": 1746.1200000000001, "text": " says, am I on the street?", "tokens": [51032, 1619, 11, 669, 286, 322, 264, 4838, 30, 51162], "temperature": 0.0, "avg_logprob": -0.16898503023035386, "compression_ratio": 1.7047619047619047, "no_speech_prob": 7.841953629394993e-05}, {"id": 601, "seek": 173016, "start": 1746.1200000000001, "end": 1748.96, "text": " Likely to catch a taxi, and all the way down", "tokens": [51162, 1743, 356, 281, 3745, 257, 18984, 11, 293, 439, 264, 636, 760, 51304], "temperature": 0.0, "avg_logprob": -0.16898503023035386, "compression_ratio": 1.7047619047619047, "no_speech_prob": 7.841953629394993e-05}, {"id": 602, "seek": 173016, "start": 1748.96, "end": 1752.52, "text": " to the actual actions that you can take in the real world.", "tokens": [51304, 281, 264, 3539, 5909, 300, 291, 393, 747, 294, 264, 957, 1002, 13, 51482], "temperature": 0.0, "avg_logprob": -0.16898503023035386, "compression_ratio": 1.7047619047619047, "no_speech_prob": 7.841953629394993e-05}, {"id": 603, "seek": 173016, "start": 1752.52, "end": 1756.1200000000001, "text": " All right, coming to the conclusion.", "tokens": [51482, 1057, 558, 11, 1348, 281, 264, 10063, 13, 51662], "temperature": 0.0, "avg_logprob": -0.16898503023035386, "compression_ratio": 1.7047619047619047, "no_speech_prob": 7.841953629394993e-05}, {"id": 604, "seek": 175612, "start": 1756.1599999999999, "end": 1760.6799999999998, "text": " So steps towards autonomous AI systems.", "tokens": [50366, 407, 4439, 3030, 23797, 7318, 3652, 13, 50592], "temperature": 0.0, "avg_logprob": -0.22890122082768655, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.00031463420600630343}, {"id": 605, "seek": 175612, "start": 1760.6799999999998, "end": 1761.6, "text": " Self-supervised learning.", "tokens": [50592, 16348, 12, 48172, 24420, 2539, 13, 50638], "temperature": 0.0, "avg_logprob": -0.22890122082768655, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.00031463420600630343}, {"id": 606, "seek": 175612, "start": 1761.6, "end": 1764.2399999999998, "text": " We need a recipe that allows us to train systems", "tokens": [50638, 492, 643, 257, 6782, 300, 4045, 505, 281, 3847, 3652, 50770], "temperature": 0.0, "avg_logprob": -0.22890122082768655, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.00031463420600630343}, {"id": 607, "seek": 175612, "start": 1764.2399999999998, "end": 1765.9599999999998, "text": " to learn how the world works on video.", "tokens": [50770, 281, 1466, 577, 264, 1002, 1985, 322, 960, 13, 50856], "temperature": 0.0, "avg_logprob": -0.22890122082768655, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.00031463420600630343}, {"id": 608, "seek": 175612, "start": 1765.9599999999998, "end": 1768.4399999999998, "text": " I can't claim that we have achieved this.", "tokens": [50856, 286, 393, 380, 3932, 300, 321, 362, 11042, 341, 13, 50980], "temperature": 0.0, "avg_logprob": -0.22890122082768655, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.00031463420600630343}, {"id": 609, "seek": 175612, "start": 1768.4399999999998, "end": 1772.12, "text": " We're kind of partially there.", "tokens": [50980, 492, 434, 733, 295, 18886, 456, 13, 51164], "temperature": 0.0, "avg_logprob": -0.22890122082768655, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.00031463420600630343}, {"id": 610, "seek": 175612, "start": 1772.12, "end": 1774.3999999999999, "text": " And legal uncertainty in the prediction,", "tokens": [51164, 400, 5089, 15697, 294, 264, 17630, 11, 51278], "temperature": 0.0, "avg_logprob": -0.22890122082768655, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.00031463420600630343}, {"id": 611, "seek": 175612, "start": 1774.3999999999999, "end": 1778.84, "text": " and that's with a combination of this JEPA architecture", "tokens": [51278, 293, 300, 311, 365, 257, 6562, 295, 341, 508, 8929, 32, 9482, 51500], "temperature": 0.0, "avg_logprob": -0.22890122082768655, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.00031463420600630343}, {"id": 612, "seek": 175612, "start": 1778.84, "end": 1782.52, "text": " understood within the context of energy-based model,", "tokens": [51500, 7320, 1951, 264, 4319, 295, 2281, 12, 6032, 2316, 11, 51684], "temperature": 0.0, "avg_logprob": -0.22890122082768655, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.00031463420600630343}, {"id": 613, "seek": 178252, "start": 1782.52, "end": 1787.48, "text": " potentially with latent variables, which I didn't talk about.", "tokens": [50364, 7263, 365, 48994, 9102, 11, 597, 286, 994, 380, 751, 466, 13, 50612], "temperature": 0.0, "avg_logprob": -0.17127314184465978, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0003099612658843398}, {"id": 614, "seek": 178252, "start": 1787.48, "end": 1790.12, "text": " That would allow us to learn world models from observation,", "tokens": [50612, 663, 576, 2089, 505, 281, 1466, 1002, 5245, 490, 14816, 11, 50744], "temperature": 0.0, "avg_logprob": -0.17127314184465978, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0003099612658843398}, {"id": 615, "seek": 178252, "start": 1790.12, "end": 1793.2, "text": " hopefully hierarchical world models, possibly", "tokens": [50744, 4696, 35250, 804, 1002, 5245, 11, 6264, 50898], "temperature": 0.0, "avg_logprob": -0.17127314184465978, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0003099612658843398}, {"id": 616, "seek": 178252, "start": 1793.2, "end": 1796.92, "text": " with interaction as well, and exploration.", "tokens": [50898, 365, 9285, 382, 731, 11, 293, 16197, 13, 51084], "temperature": 0.0, "avg_logprob": -0.17127314184465978, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0003099612658843398}, {"id": 617, "seek": 178252, "start": 1796.92, "end": 1800.32, "text": " And now what we have is an architecture capable of reasoning", "tokens": [51084, 400, 586, 437, 321, 362, 307, 364, 9482, 8189, 295, 21577, 51254], "temperature": 0.0, "avg_logprob": -0.17127314184465978, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0003099612658843398}, {"id": 618, "seek": 178252, "start": 1800.32, "end": 1800.96, "text": " and planning.", "tokens": [51254, 293, 5038, 13, 51286], "temperature": 0.0, "avg_logprob": -0.17127314184465978, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0003099612658843398}, {"id": 619, "seek": 178252, "start": 1800.96, "end": 1803.08, "text": " I mean, the whole architecture I presented", "tokens": [51286, 286, 914, 11, 264, 1379, 9482, 286, 8212, 51392], "temperature": 0.0, "avg_logprob": -0.17127314184465978, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0003099612658843398}, {"id": 620, "seek": 178252, "start": 1803.08, "end": 1805.24, "text": " is kind of this idea of system two,", "tokens": [51392, 307, 733, 295, 341, 1558, 295, 1185, 732, 11, 51500], "temperature": 0.0, "avg_logprob": -0.17127314184465978, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0003099612658843398}, {"id": 621, "seek": 178252, "start": 1805.24, "end": 1807.92, "text": " that you can decompose complex tasks into simpler ones,", "tokens": [51500, 300, 291, 393, 22867, 541, 3997, 9608, 666, 18587, 2306, 11, 51634], "temperature": 0.0, "avg_logprob": -0.17127314184465978, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0003099612658843398}, {"id": 622, "seek": 178252, "start": 1807.92, "end": 1809.36, "text": " and then plan a sequence of actions", "tokens": [51634, 293, 550, 1393, 257, 8310, 295, 5909, 51706], "temperature": 0.0, "avg_logprob": -0.17127314184465978, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0003099612658843398}, {"id": 623, "seek": 178252, "start": 1809.36, "end": 1811.56, "text": " before you take the action.", "tokens": [51706, 949, 291, 747, 264, 3069, 13, 51816], "temperature": 0.0, "avg_logprob": -0.17127314184465978, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0003099612658843398}, {"id": 624, "seek": 181156, "start": 1811.6, "end": 1815.8, "text": " Something that's sort of missing from current autoregressive systems.", "tokens": [50366, 6595, 300, 311, 1333, 295, 5361, 490, 2190, 1476, 418, 3091, 488, 3652, 13, 50576], "temperature": 0.0, "avg_logprob": -0.22066127686273485, "compression_ratio": 1.4841628959276019, "no_speech_prob": 0.0006555194850079715}, {"id": 625, "seek": 181156, "start": 1819.32, "end": 1827.8799999999999, "text": " So is this a potential path towards sort of human-level AI?", "tokens": [50752, 407, 307, 341, 257, 3995, 3100, 3030, 1333, 295, 1952, 12, 12418, 7318, 30, 51180], "temperature": 0.0, "avg_logprob": -0.22066127686273485, "compression_ratio": 1.4841628959276019, "no_speech_prob": 0.0006555194850079715}, {"id": 626, "seek": 181156, "start": 1827.8799999999999, "end": 1830.1599999999999, "text": " Possibly yes, but it's certainly not for tomorrow.", "tokens": [51180, 33112, 3545, 2086, 11, 457, 309, 311, 3297, 406, 337, 4153, 13, 51294], "temperature": 0.0, "avg_logprob": -0.22066127686273485, "compression_ratio": 1.4841628959276019, "no_speech_prob": 0.0006555194850079715}, {"id": 627, "seek": 181156, "start": 1830.1599999999999, "end": 1833.12, "text": " This is maybe a 10-year plan, maybe", "tokens": [51294, 639, 307, 1310, 257, 1266, 12, 5294, 1393, 11, 1310, 51442], "temperature": 0.0, "avg_logprob": -0.22066127686273485, "compression_ratio": 1.4841628959276019, "no_speech_prob": 0.0006555194850079715}, {"id": 628, "seek": 181156, "start": 1833.12, "end": 1837.1599999999999, "text": " to get to cat-level intelligence or something like that.", "tokens": [51442, 281, 483, 281, 3857, 12, 12418, 7599, 420, 746, 411, 300, 13, 51644], "temperature": 0.0, "avg_logprob": -0.22066127686273485, "compression_ratio": 1.4841628959276019, "no_speech_prob": 0.0006555194850079715}, {"id": 629, "seek": 181156, "start": 1837.1599999999999, "end": 1840.2, "text": " Now interestingly, those machines will have inevitably", "tokens": [51644, 823, 25873, 11, 729, 8379, 486, 362, 28171, 51796], "temperature": 0.0, "avg_logprob": -0.22066127686273485, "compression_ratio": 1.4841628959276019, "no_speech_prob": 0.0006555194850079715}, {"id": 630, "seek": 184020, "start": 1840.24, "end": 1843.52, "text": " some sort of emotion consciousness.", "tokens": [50366, 512, 1333, 295, 8913, 10081, 13, 50530], "temperature": 0.0, "avg_logprob": -0.1745235287413305, "compression_ratio": 1.8302752293577982, "no_speech_prob": 0.0007093348540365696}, {"id": 631, "seek": 184020, "start": 1843.52, "end": 1845.8400000000001, "text": " Forget about this, but emotions certainly,", "tokens": [50530, 18675, 466, 341, 11, 457, 8462, 3297, 11, 50646], "temperature": 0.0, "avg_logprob": -0.1745235287413305, "compression_ratio": 1.8302752293577982, "no_speech_prob": 0.0007093348540365696}, {"id": 632, "seek": 184020, "start": 1845.8400000000001, "end": 1850.56, "text": " because emotions are kind of an anticipation of outcome, most of them.", "tokens": [50646, 570, 8462, 366, 733, 295, 364, 35979, 295, 9700, 11, 881, 295, 552, 13, 50882], "temperature": 0.0, "avg_logprob": -0.1745235287413305, "compression_ratio": 1.8302752293577982, "no_speech_prob": 0.0007093348540365696}, {"id": 633, "seek": 184020, "start": 1850.56, "end": 1855.76, "text": " I mean, some of them are immediate perception of outcome,", "tokens": [50882, 286, 914, 11, 512, 295, 552, 366, 11629, 12860, 295, 9700, 11, 51142], "temperature": 0.0, "avg_logprob": -0.1745235287413305, "compression_ratio": 1.8302752293577982, "no_speech_prob": 0.0007093348540365696}, {"id": 634, "seek": 184020, "start": 1855.76, "end": 1857.3600000000001, "text": " like pain and things like that.", "tokens": [51142, 411, 1822, 293, 721, 411, 300, 13, 51222], "temperature": 0.0, "avg_logprob": -0.1745235287413305, "compression_ratio": 1.8302752293577982, "no_speech_prob": 0.0007093348540365696}, {"id": 635, "seek": 184020, "start": 1857.3600000000001, "end": 1859.4, "text": " But most of them are anticipation of outcome,", "tokens": [51222, 583, 881, 295, 552, 366, 35979, 295, 9700, 11, 51324], "temperature": 0.0, "avg_logprob": -0.1745235287413305, "compression_ratio": 1.8302752293577982, "no_speech_prob": 0.0007093348540365696}, {"id": 636, "seek": 184020, "start": 1859.4, "end": 1862.32, "text": " and this cost function is exactly what this is.", "tokens": [51324, 293, 341, 2063, 2445, 307, 2293, 437, 341, 307, 13, 51470], "temperature": 0.0, "avg_logprob": -0.1745235287413305, "compression_ratio": 1.8302752293577982, "no_speech_prob": 0.0007093348540365696}, {"id": 637, "seek": 184020, "start": 1862.32, "end": 1866.64, "text": " And so if the system sort of predicts a particular set of outcome", "tokens": [51470, 400, 370, 498, 264, 1185, 1333, 295, 6069, 82, 257, 1729, 992, 295, 9700, 51686], "temperature": 0.0, "avg_logprob": -0.1745235287413305, "compression_ratio": 1.8302752293577982, "no_speech_prob": 0.0007093348540365696}, {"id": 638, "seek": 186664, "start": 1866.64, "end": 1875.92, "text": " that results in a bad outcome, it might feel something similar", "tokens": [50364, 300, 3542, 294, 257, 1578, 9700, 11, 309, 1062, 841, 746, 2531, 50828], "temperature": 0.0, "avg_logprob": -0.1632410900012867, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.0002377044002059847}, {"id": 639, "seek": 186664, "start": 1875.92, "end": 1877.2, "text": " to fear or something of that type.", "tokens": [50828, 281, 4240, 420, 746, 295, 300, 2010, 13, 50892], "temperature": 0.0, "avg_logprob": -0.1632410900012867, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.0002377044002059847}, {"id": 640, "seek": 186664, "start": 1879.96, "end": 1885.64, "text": " Anyway, so common sense is a collection of world models,", "tokens": [51030, 5684, 11, 370, 2689, 2020, 307, 257, 5765, 295, 1002, 5245, 11, 51314], "temperature": 0.0, "avg_logprob": -0.1632410900012867, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.0002377044002059847}, {"id": 641, "seek": 186664, "start": 1885.64, "end": 1889.0, "text": " or perhaps a single world model that is configurable.", "tokens": [51314, 420, 4317, 257, 2167, 1002, 2316, 300, 307, 22192, 712, 13, 51482], "temperature": 0.0, "avg_logprob": -0.1632410900012867, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.0002377044002059847}, {"id": 642, "seek": 186664, "start": 1889.0, "end": 1892.3200000000002, "text": " I'll come to this in one second.", "tokens": [51482, 286, 603, 808, 281, 341, 294, 472, 1150, 13, 51648], "temperature": 0.0, "avg_logprob": -0.1632410900012867, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.0002377044002059847}, {"id": 643, "seek": 186664, "start": 1892.3200000000002, "end": 1895.92, "text": " Understanding really means being able to predict.", "tokens": [51648, 36858, 534, 1355, 885, 1075, 281, 6069, 13, 51828], "temperature": 0.0, "avg_logprob": -0.1632410900012867, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.0002377044002059847}, {"id": 644, "seek": 189592, "start": 1895.92, "end": 1899.0800000000002, "text": " I think prediction is really the essence of intelligence here,", "tokens": [50364, 286, 519, 17630, 307, 534, 264, 12801, 295, 7599, 510, 11, 50522], "temperature": 0.0, "avg_logprob": -0.17867273442885456, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.000314411852741614}, {"id": 645, "seek": 189592, "start": 1899.0800000000002, "end": 1904.44, "text": " and better mental models need to better understanding,", "tokens": [50522, 293, 1101, 4973, 5245, 643, 281, 1101, 3701, 11, 50790], "temperature": 0.0, "avg_logprob": -0.17867273442885456, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.000314411852741614}, {"id": 646, "seek": 189592, "start": 1904.44, "end": 1909.88, "text": " or other substrate, if you want, of understanding.", "tokens": [50790, 420, 661, 27585, 11, 498, 291, 528, 11, 295, 3701, 13, 51062], "temperature": 0.0, "avg_logprob": -0.17867273442885456, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.000314411852741614}, {"id": 647, "seek": 189592, "start": 1909.88, "end": 1915.68, "text": " And as a consequence, also of good reasoning and action planning.", "tokens": [51062, 400, 382, 257, 18326, 11, 611, 295, 665, 21577, 293, 3069, 5038, 13, 51352], "temperature": 0.0, "avg_logprob": -0.17867273442885456, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.000314411852741614}, {"id": 648, "seek": 189592, "start": 1915.68, "end": 1918.24, "text": " The complex part in all of this is going", "tokens": [51352, 440, 3997, 644, 294, 439, 295, 341, 307, 516, 51480], "temperature": 0.0, "avg_logprob": -0.17867273442885456, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.000314411852741614}, {"id": 649, "seek": 189592, "start": 1918.24, "end": 1920.48, "text": " to be to design intrinsic cost functions that", "tokens": [51480, 281, 312, 281, 1715, 35698, 2063, 6828, 300, 51592], "temperature": 0.0, "avg_logprob": -0.17867273442885456, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.000314411852741614}, {"id": 650, "seek": 189592, "start": 1920.48, "end": 1925.04, "text": " drive the system towards learning appropriate things.", "tokens": [51592, 3332, 264, 1185, 3030, 2539, 6854, 721, 13, 51820], "temperature": 0.0, "avg_logprob": -0.17867273442885456, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.000314411852741614}, {"id": 651, "seek": 192504, "start": 1925.08, "end": 1931.84, "text": " And it's quite possible that, in the case of leaving things,", "tokens": [50366, 400, 309, 311, 1596, 1944, 300, 11, 294, 264, 1389, 295, 5012, 721, 11, 50704], "temperature": 0.0, "avg_logprob": -0.16474981982298573, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.00014421954983845353}, {"id": 652, "seek": 192504, "start": 1931.84, "end": 1936.32, "text": " it's easier for evolution to hardwire your cost functions into us", "tokens": [50704, 309, 311, 3571, 337, 9303, 281, 1152, 42689, 428, 2063, 6828, 666, 505, 50928], "temperature": 0.0, "avg_logprob": -0.16474981982298573, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.00014421954983845353}, {"id": 653, "seek": 192504, "start": 1936.32, "end": 1938.44, "text": " than to hardwire your behavior.", "tokens": [50928, 813, 281, 1152, 42689, 428, 5223, 13, 51034], "temperature": 0.0, "avg_logprob": -0.16474981982298573, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.00014421954983845353}, {"id": 654, "seek": 192504, "start": 1938.44, "end": 1943.32, "text": " Hardwiring behavior and physical models and whatever is super hard.", "tokens": [51034, 11817, 86, 5057, 5223, 293, 4001, 5245, 293, 2035, 307, 1687, 1152, 13, 51278], "temperature": 0.0, "avg_logprob": -0.16474981982298573, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.00014421954983845353}, {"id": 655, "seek": 192504, "start": 1943.32, "end": 1947.0, "text": " Like, as a neural net person, I would have no idea", "tokens": [51278, 1743, 11, 382, 257, 18161, 2533, 954, 11, 286, 576, 362, 572, 1558, 51462], "temperature": 0.0, "avg_logprob": -0.16474981982298573, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.00014421954983845353}, {"id": 656, "seek": 192504, "start": 1947.0, "end": 1949.3999999999999, "text": " how to architect neural nets to do this.", "tokens": [51462, 577, 281, 6331, 18161, 36170, 281, 360, 341, 13, 51582], "temperature": 0.0, "avg_logprob": -0.16474981982298573, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.00014421954983845353}, {"id": 657, "seek": 192504, "start": 1949.3999999999999, "end": 1953.28, "text": " But I can certainly design a cost function that, if minimized,", "tokens": [51582, 583, 286, 393, 3297, 1715, 257, 2063, 2445, 300, 11, 498, 4464, 1602, 11, 51776], "temperature": 0.0, "avg_logprob": -0.16474981982298573, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.00014421954983845353}, {"id": 658, "seek": 195328, "start": 1953.28, "end": 1957.12, "text": " the system will learn those basic concepts.", "tokens": [50364, 264, 1185, 486, 1466, 729, 3875, 10392, 13, 50556], "temperature": 0.0, "avg_logprob": -0.1279692120022244, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.00017126812599599361}, {"id": 659, "seek": 195328, "start": 1957.12, "end": 1962.24, "text": " And that, there is a lot of hardwiring in there, no question.", "tokens": [50556, 400, 300, 11, 456, 307, 257, 688, 295, 1152, 86, 5057, 294, 456, 11, 572, 1168, 13, 50812], "temperature": 0.0, "avg_logprob": -0.1279692120022244, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.00017126812599599361}, {"id": 660, "seek": 195328, "start": 1962.24, "end": 1964.76, "text": " So one module I didn't talk much about is the configurator.", "tokens": [50812, 407, 472, 10088, 286, 994, 380, 751, 709, 466, 307, 264, 22192, 1639, 13, 50938], "temperature": 0.0, "avg_logprob": -0.1279692120022244, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.00017126812599599361}, {"id": 661, "seek": 195328, "start": 1964.76, "end": 1967.8799999999999, "text": " And what it's supposed to do is configure all the modules", "tokens": [50938, 400, 437, 309, 311, 3442, 281, 360, 307, 22162, 439, 264, 16679, 51094], "temperature": 0.0, "avg_logprob": -0.1279692120022244, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.00017126812599599361}, {"id": 662, "seek": 195328, "start": 1967.8799999999999, "end": 1971.84, "text": " in this architecture for a particular sub-task", "tokens": [51094, 294, 341, 9482, 337, 257, 1729, 1422, 12, 83, 3863, 51292], "temperature": 0.0, "avg_logprob": -0.1279692120022244, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.00017126812599599361}, {"id": 663, "seek": 195328, "start": 1971.84, "end": 1975.76, "text": " that the system needs to be focusing on at the moment.", "tokens": [51292, 300, 264, 1185, 2203, 281, 312, 8416, 322, 412, 264, 1623, 13, 51488], "temperature": 0.0, "avg_logprob": -0.1279692120022244, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.00017126812599599361}, {"id": 664, "seek": 195328, "start": 1975.76, "end": 1977.8799999999999, "text": " And I'm imagining that there is actually", "tokens": [51488, 400, 286, 478, 27798, 300, 456, 307, 767, 51594], "temperature": 0.0, "avg_logprob": -0.1279692120022244, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.00017126812599599361}, {"id": 665, "seek": 195328, "start": 1977.8799999999999, "end": 1981.3999999999999, "text": " a single world model engine in this architecture that", "tokens": [51594, 257, 2167, 1002, 2316, 2848, 294, 341, 9482, 300, 51770], "temperature": 0.0, "avg_logprob": -0.1279692120022244, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.00017126812599599361}, {"id": 666, "seek": 198140, "start": 1981.4, "end": 1984.3200000000002, "text": " is reconfigurable for the task at hand.", "tokens": [50364, 307, 9993, 20646, 25863, 337, 264, 5633, 412, 1011, 13, 50510], "temperature": 0.0, "avg_logprob": -0.17254803158821316, "compression_ratio": 1.7357723577235773, "no_speech_prob": 0.000687652500346303}, {"id": 667, "seek": 198140, "start": 1984.3200000000002, "end": 1987.68, "text": " But it's not like the system would have multiple world models", "tokens": [50510, 583, 309, 311, 406, 411, 264, 1185, 576, 362, 3866, 1002, 5245, 50678], "temperature": 0.0, "avg_logprob": -0.17254803158821316, "compression_ratio": 1.7357723577235773, "no_speech_prob": 0.000687652500346303}, {"id": 668, "seek": 198140, "start": 1987.68, "end": 1988.96, "text": " for different situations.", "tokens": [50678, 337, 819, 6851, 13, 50742], "temperature": 0.0, "avg_logprob": -0.17254803158821316, "compression_ratio": 1.7357723577235773, "no_speech_prob": 0.000687652500346303}, {"id": 669, "seek": 198140, "start": 1988.96, "end": 1990.8000000000002, "text": " It's got a single one that's configurable.", "tokens": [50742, 467, 311, 658, 257, 2167, 472, 300, 311, 22192, 712, 13, 50834], "temperature": 0.0, "avg_logprob": -0.17254803158821316, "compression_ratio": 1.7357723577235773, "no_speech_prob": 0.000687652500346303}, {"id": 670, "seek": 198140, "start": 1990.8000000000002, "end": 1995.96, "text": " The advantage of doing this, I mean, for humans and animals,", "tokens": [50834, 440, 5002, 295, 884, 341, 11, 286, 914, 11, 337, 6255, 293, 4882, 11, 51092], "temperature": 0.0, "avg_logprob": -0.17254803158821316, "compression_ratio": 1.7357723577235773, "no_speech_prob": 0.000687652500346303}, {"id": 671, "seek": 198140, "start": 1995.96, "end": 1998.96, "text": " is that it might actually fit in your skull.", "tokens": [51092, 307, 300, 309, 1062, 767, 3318, 294, 428, 11743, 13, 51242], "temperature": 0.0, "avg_logprob": -0.17254803158821316, "compression_ratio": 1.7357723577235773, "no_speech_prob": 0.000687652500346303}, {"id": 672, "seek": 198140, "start": 1998.96, "end": 2002.3200000000002, "text": " But there is another algorithmic advantage,", "tokens": [51242, 583, 456, 307, 1071, 9284, 299, 5002, 11, 51410], "temperature": 0.0, "avg_logprob": -0.17254803158821316, "compression_ratio": 1.7357723577235773, "no_speech_prob": 0.000687652500346303}, {"id": 673, "seek": 198140, "start": 2002.3200000000002, "end": 2007.0800000000002, "text": " or epistemic advantage, which is that a single one model can", "tokens": [51410, 420, 2388, 468, 3438, 5002, 11, 597, 307, 300, 257, 2167, 472, 2316, 393, 51648], "temperature": 0.0, "avg_logprob": -0.17254803158821316, "compression_ratio": 1.7357723577235773, "no_speech_prob": 0.000687652500346303}, {"id": 674, "seek": 198140, "start": 2007.0800000000002, "end": 2010.1200000000001, "text": " share knowledge between different situations.", "tokens": [51648, 2073, 3601, 1296, 819, 6851, 13, 51800], "temperature": 0.0, "avg_logprob": -0.17254803158821316, "compression_ratio": 1.7357723577235773, "no_speech_prob": 0.000687652500346303}, {"id": 675, "seek": 201012, "start": 2010.6, "end": 2014.08, "text": " Whereas if you had a separate world model for different situations,", "tokens": [50388, 13813, 498, 291, 632, 257, 4994, 1002, 2316, 337, 819, 6851, 11, 50562], "temperature": 0.0, "avg_logprob": -0.19049911499023436, "compression_ratio": 1.7370517928286853, "no_speech_prob": 0.0002530587953515351}, {"id": 676, "seek": 201012, "start": 2014.08, "end": 2016.36, "text": " you would have to retrain it independently", "tokens": [50562, 291, 576, 362, 281, 1533, 7146, 309, 21761, 50676], "temperature": 0.0, "avg_logprob": -0.19049911499023436, "compression_ratio": 1.7370517928286853, "no_speech_prob": 0.0002530587953515351}, {"id": 677, "seek": 201012, "start": 2016.36, "end": 2018.04, "text": " for each of those situations.", "tokens": [50676, 337, 1184, 295, 729, 6851, 13, 50760], "temperature": 0.0, "avg_logprob": -0.19049911499023436, "compression_ratio": 1.7370517928286853, "no_speech_prob": 0.0002530587953515351}, {"id": 678, "seek": 201012, "start": 2018.04, "end": 2020.8, "text": " So how to make this configurator work, I have no idea.", "tokens": [50760, 407, 577, 281, 652, 341, 22192, 1639, 589, 11, 286, 362, 572, 1558, 13, 50898], "temperature": 0.0, "avg_logprob": -0.19049911499023436, "compression_ratio": 1.7370517928286853, "no_speech_prob": 0.0002530587953515351}, {"id": 679, "seek": 201012, "start": 2020.8, "end": 2023.84, "text": " But that's a good hypothesis.", "tokens": [50898, 583, 300, 311, 257, 665, 17291, 13, 51050], "temperature": 0.0, "avg_logprob": -0.19049911499023436, "compression_ratio": 1.7370517928286853, "no_speech_prob": 0.0002530587953515351}, {"id": 680, "seek": 201012, "start": 2023.84, "end": 2026.4799999999998, "text": " So that would explain the fact that there is a single world model.", "tokens": [51050, 407, 300, 576, 2903, 264, 1186, 300, 456, 307, 257, 2167, 1002, 2316, 13, 51182], "temperature": 0.0, "avg_logprob": -0.19049911499023436, "compression_ratio": 1.7370517928286853, "no_speech_prob": 0.0002530587953515351}, {"id": 681, "seek": 201012, "start": 2026.4799999999998, "end": 2029.36, "text": " It would explain why humans and many animals", "tokens": [51182, 467, 576, 2903, 983, 6255, 293, 867, 4882, 51326], "temperature": 0.0, "avg_logprob": -0.19049911499023436, "compression_ratio": 1.7370517928286853, "no_speech_prob": 0.0002530587953515351}, {"id": 682, "seek": 201012, "start": 2029.36, "end": 2033.12, "text": " can only focus on the single conscious task at any one time.", "tokens": [51326, 393, 787, 1879, 322, 264, 2167, 6648, 5633, 412, 604, 472, 565, 13, 51514], "temperature": 0.0, "avg_logprob": -0.19049911499023436, "compression_ratio": 1.7370517928286853, "no_speech_prob": 0.0002530587953515351}, {"id": 683, "seek": 201012, "start": 2033.12, "end": 2037.76, "text": " Because we only have one world model.", "tokens": [51514, 1436, 321, 787, 362, 472, 1002, 2316, 13, 51746], "temperature": 0.0, "avg_logprob": -0.19049911499023436, "compression_ratio": 1.7370517928286853, "no_speech_prob": 0.0002530587953515351}, {"id": 684, "seek": 203776, "start": 2037.8, "end": 2043.12, "text": " We can only do system two on one task at a time.", "tokens": [50366, 492, 393, 787, 360, 1185, 732, 322, 472, 5633, 412, 257, 565, 13, 50632], "temperature": 0.0, "avg_logprob": -0.3191719055175781, "compression_ratio": 1.0375, "no_speech_prob": 0.0007303618476726115}, {"id": 685, "seek": 203776, "start": 2043.12, "end": 2045.04, "text": " And I just leave the question for.", "tokens": [50632, 400, 286, 445, 1856, 264, 1168, 337, 13, 50728], "temperature": 0.0, "avg_logprob": -0.3191719055175781, "compression_ratio": 1.0375, "no_speech_prob": 0.0007303618476726115}], "language": "en"}