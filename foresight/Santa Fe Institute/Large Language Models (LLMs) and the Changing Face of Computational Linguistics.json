{"text": " Perhaps Catherine should just introduce ourselves. So we were at Brown University for several decades in the Department of Cognitive and Holistic Science there. I was also joined in Computer Science. Moved to Australia about 14 years ago to Macquarie University and at the same time I think we started becoming external faculty here at SFI. Catherine got a gigantic mega grant that required hiring a lot of people doing a lot of work there. I started going into the startup route and then what with that and then with COVID I think we let the external faculty connection lapse but now we'd like to restart the connection with SFI again. The startup stuff that I was doing was actually in the chatbot space. We got acquired by Oracle Corporation. That's the reason for this disclaimer down here at the bottom right. So nothing that I'm saying represents anything of any of our employers. The other reason for this comment here is because the whole field of large language bottles is really changing incredibly quickly. I don't know how much of what I'm going to say right now is going to be true or relevant in six months time. So nothing, I'm not saying anything that I believe is false but it could well, some of the effort could well be wasted. So I'm going to try and give you guys the high level executive overview talk. This talk was really actually aimed originally at academic researchers in natural language processing and so I'm going to skip some of those some of those details but you know you had to have been underneath a rock if you didn't notice the large language models who radically changed the field in the last year or two and so I'll be talking about that. We'll also be talking about suggestions. You know how should academics actually respond to this changing world? So we'll be talking a little bit about what these large language models may mean or may not mean for human language acquisition, the study of that. There's a lot of interest in neuro-symbolic models that integrate large language models and more traditional kinds of AI models and I'll also talk a little bit about alignment of LLMs and I'll end talking a bit about the connections between large language models and the social implications of them and lessons from the first industrial revolution. Katherine and I just spent two months at the University of Edinburgh and it was very interesting there because of course the first industrial revolution really happened in Northern England and around Scotland and actually it's a fascinating thing to read about that you know James Watt, you know the inventor of the steam engine actually did a lot of his work at the University of Glasgow so that's sort of really interesting. Okay so how deep learning changes NLP? You know people like Katherine and I have been in the field long enough to have actually seen really a switch from symbolic parameters, statistical NLP and then finally now to deep learning and large language models and I guess I would say that the really huge difference is that large language models can understand context much much longer contexts than anything that I thought was even possible. In fact I actually thought there were good theoretical arguments that it should not be possible to try and model a context as long as a sentence or a dialogue or an entire paragraph yet large language models can do that perfectly well and if you want to I can go through what those counting arguments would be but just you know when I actually look back at the pre-deep learning work you know I did a lot of work on syntactic parsing which would involve recovering pastries that looked something like this and I think the motivation for dealing with you know wanting structures like these these things make local a lot of dependencies which are non-local in the string here right so you know it's a flight through Denver right so this relationship here is making that local the same thing also we're talking about a preference for a flight for example that's making that dependency local why do you want to do that you want to do that if you think that you can only really model pairs pairwise interactions large language models as I said are happy modeling contexts of you know in the hundreds or the thousands of tokens and so I actually think you know these theories are correct but they're just no longer required so to do natural language understanding I don't actually have to recover this sort of structure anymore okay so strengths of large language models we just talked about how there's so much better at working with incredibly long contexts I think the other you know I'm giving you the executive summary here the other really amazing thing is that you know if you showed these large language models to people from the first neural network revolution from the 1980s actually everything would be really quite familiar to them right there's there's a slight twist with the attention mechanism but you could explain that in about 15 minutes and those guys understand it all you know and Sutton has this thing which he calls the bitter lesson which is that you know for people like me who work on lots of really interesting algorithms and models and structures and you know things like that actually really none of it really seemed to be relevant if you just got data and compute and scaled everything up that that really seems to be the secret and I actually do think that these large language models are largely just sort of brute forcing linguistic generalization so they're covering rather than really capturing them but it's really interesting how well they work right that in fact actually with enough compute and enough data you do seem to be able to produce something that's really quite amazing okay weaknesses of large language models I mean I think this is probably everybody here's already heard about this stuff you know Emily Bender's stochastic parrots you know or as I guess Gertrude Stein would say there's no there there and I think actually these large language models just you know they respond reflexively in fact I actually think if we're talking about human models these large language models maybe actually aren't such so bad for the purely reflexive processing that maybe humans might be able to do but scaled up you know whereas humans maybe have a context window of five or something like that these large language models have a context window of five thousand for example and I think actually that's also because they have no beliefs desires or intentions they have no you know guiding you know thoughts that's why hallucinations such a big problem you know they're just trying to produce plausible output I think it's really interesting to actually wonder about you know these large language models now becoming multimodal how will that change things I actually think if you think the fundamental problem is a problem of symbol grounding then arguments like Searle's Chinese room argument still are just as valid against multimodal input on the other hand I actually think the multimodal inputs incredibly interesting might be incredibly powerful yes so naive questions what is multimodal training oh so here what I really think is vision and language I should have made that clear vision and language but these days in fact people are now starting to train off movies as well right so that's I mean arguably that is one big difference between these large language models and you know the way humans might work right and what is Searle's Chinese room that's a really interesting thought experiment which I think for reasons of time I might just skip here but it's a it's basically where he asks he asks us to imagine a blind symbol following system and he says so can you really do you really want to say that this blind symbol for you know rule following system actually has any understanding it doesn't really seem to make sense to attribute any any understanding to it anyway I do actually think you know there's a lot of people now that are also wondering are these large language models are they really intelligent could they be a AGI artificial general intelligence that maybe might take over the entire world I think they currently can't because they do lack you know beliefs desires intentions the ability to form long-term memories for example but I'm I'm actually not so sure that that's such a huge technological barrier I actually do think that it's possible that that could be relatively easy now you know we've been in this situation before where we thought oh the thing that's really missing from this machine being intelligent is x you add x to the machine and all of a sudden you discover well actually you know there's this other thing why that's missing as well and so that may happen here too but there is a chance that in fact if we just added episodic memory to these large language models then they may actually become intelligent things the other okay another high level point that I want to make is that these large language models are manufactured systems in the same way remember I was talking about the first industrial revolution in the same way that a steam engine is a very highly engineered manufactured system you know you wouldn't want to be trying to infer the laws of thermodynamics or even the ideal gas laws by examining a steam engine I think the same thing's true also about these large language models you know they're they're trained in a way which is really an engineered product you know and certainly as somebody being in industry I can tell you that you know the people in industry that are building these things really don't care about doing science they're trying to build a product that they can sell and that's really what they care about okay so just an aside about you know the relationship between I I actually think a natural language understanding and say you know the science of of linguistics or language acquisition so there's the scientific side and the technological side I think the technology is currently outstripping the science and I think that has happened in times before in fact I think it actually happened with the first industrial revolution so I won't spend too much time here but my understanding is and maybe there's people historians of science in this room that know more about this than I do but that you know really in fact actually people started to invent steam engines long before they had the scientific understanding first of all of ideal gas laws and then ultimately right through thermodynamics and statistical mechanics that took centuries to the scientific for the scientific side to emerge and in fact actually ideas of things like entropy really actually had to be developed to answer questions about why it was not possible to build steam engines above a certain level of efficiency for example and I suspect the same thing may be true today that our science of say language and psychology is actually behind the technology. Okay one of the things that I actually quite like is you know this comment here natural language is the new programming language and that yeah I mean certainly for those of us in industry LLMs are really changing the way in which we do our work right whereas it used to take a team of real experts to build say for example a device which would identify all the financial products that are mentioned you know in a particular document. Now you can just simply ask a large language model to do that for you and it does a pretty good job maybe not as good as the very best hand-built natural language processing system so those are still actually better but you know they take months or years to develop whereas it takes you know maybe hours to use a new large language model so I actually think that particularly in terms of the commercial implications the commercial deployment of natural language processing in industry that's going to change completely. It's not clear we'll need nearly as many experts in natural language understanding for example for the industrial applications. I did want to mention a little bit you know I think that one of the really interesting things that's happening in the field is taking these large language models and then combining them with other components. The first component that people started to look at was combining large language models with what's called a vector store or a retrieval system and that's just simply something whereby when you ask a question instead of just directly asking the large language model to respond to that question you retrieve a set of relevant documents to that question feed those in as part of the input to the large language model you can see that's what I'm suggesting that you do over here and then you then tell the large language model to use to produce an answer that just simply references those documents and that's sometimes called the reader retrieval model or retrieval augmented generation. That idea can get even more power when you start to think well maybe in fact the large language model can actually decide what information to do a search for and then when you then started to think well should it decide what information to do a search for maybe in fact it could also call other tools so these large language models are infamous for not being able to do numerical calculations very well but maybe in fact what we should be doing is giving the large language model the ability to call a calculator and there's just in the same ways which if I was to ask any of you guys to do a complex task and involve something some numeric calculation I'd want you to be also using a calculator rather than trying to do it longhand. Okay so in terms of research directions inside of an LLM world so the very first comment to make is that it is very challenging for academics to do research in large language models you know the ideal thing would be to have something like an ideal gas experiment set up but and you certainly can build small versions of these large language models there's some disagreement about whether or not though whether there's well there certainly seem to be emergent capabilities so the bigger the model the more things that it can do there's big arguments about whether or not this emergence is like a phase change or whether it's really more incremental and I again I'd be happy to talk about that later we could spend hours talking a bit about that but I think there's enough lack of clarity about what emergence is but if you wanted to do academic research you really do want to get access to you know the large language the larger large language models and the problem is that these you know the best large language models are really complicated commercial products as we were talking about before and it's what's actually even worse is that these days for in you know proprietary commercial reasons the companies aren't even actually telling us all the details of exactly what they're doing so that actually does make it very hard for academics to really do any sort of academic research in in our own papers you know I'm collaborating with people at the University of Edinburgh what we wind up doing is saying we're actually not going to test the closed commercial systems but we will work with the largest open source systems that are available I think that's not a bad thing to do but it does mean that you're cutting yourself off from a lot of the really cool systems there yes this is a very interesting question on that slide about how quickly they degrade as you move just as we moved from commercial ones to smaller and smaller ones yes just a steady degradation or is there a sudden drop so sorry and in fact actually this also gets back to the emergence question so let me just say I actually do think that a lot depends on exactly how you measure it so I don't have to tell people particularly at the Santa Fe Institute right that quite often what you'll actually see is a lot of small changes all of a sudden reaching a tipping point that is basically like a phase change and you know when you think about these large language models I mean they generate output token by token if the correct answer is just slightly less probable than some mistake right well then as the output gets very very long then the correct answer can be incredibly improbable right so if you're just looking at the output of 100 tokens or more you're just looking at the output you're just asking is the output right or wrong you'll go wrong wrong wrong wrong wrong right and then all of a sudden as the correct token probability just nudges above the incorrect tokens you know all of a sudden the output flips and all of a sudden it's just magically all correct but if you measure the per token probabilities for example then you'd actually discover a much more continuous change you know so I think that's actually where a lot of emergence happens and in fact that's I think the white there's a little academic dispute about whether or not these models have emergent behavior or not and that's at least my understanding of how you'd resolve that so I actually do think that a lot of the ideas that people here would have would actually be very useful for the community to have as well okay all right you know I actually think there's lots of really interesting questions also you know can we understand what these large language models are really doing I think it's you know I mean we know something about how language is processed in the human brain you know we know that none of these models really are realistic just even understanding you know what these large language models are doing how can you be sure that they know a syntactic rule or make it even simpler that they know a particular word so right now and in fact actually I think another really interesting question is if if these large language models are basically just you know gigantic neural nets as I said before of a relatively generic type why is it that only human beings can acquire language are they are you know why is it humans are the only animals that can acquire language right I mean you know we don't have the biggest brains there are animals with bigger brains if it's just merely the size you know the number of neurons that we have sitting inside of our skulls if that's all that determines our ability to do something like learn language why why don't other animals why don't they have that ability it's very popular now to talk about analyzing large language models using psychological or psychological linguistic methods I think that's about the best that I know how to do but I think a lot of these methods were really designed to work on humans at least agents that have beliefs and again you know in the sense a large language model doesn't have a belief it's just got reflexes okay so you know just to emphasize the differences between large language models and and humans right so you know children start and end learning from much smaller data sets they generalize in particular ways that we actually understand to new unseen forms I think we don't really know actually how these large language models generalize I mean they do I think they do generalize but it's very difficult to tell exactly how they generalize children also actually over generalizing characteristically so Catherine's an expert in this area but you know these are just a couple of examples that she pointed out you know where children have taken irregular verbs and either inflicted them in a regular fashion or over generalize the irregular form she's giggling me you know that makes sense if you think of giggling as being a verb a bit like tickling for example language learning you know by the time you're three or four you're a competent speaker of your native language usually but then there's also some part of language learning that's not really complete until the early teens right and then you know just in terms of the pragmatics of doing research on large language models right the time scale of research projects are different so it might take a couple of years for a student to do a research project studying say human language learning if they're studying something which was inspired by GPT-4 well in two years time we're probably in GPT-6 you know and the the the inspiration might be actually sort of completely different I also this is essentially that same thing as as I was saying before right so evaluation and testing I think is really a huge challenge that was always difficult inside of natural language processing but it gets even worse because the inputs to large language models now instead of just again because this context is so much longer the input is not just a single sentence it's an entire conversation or entire story or something else like that so if you want to really evaluate the performance of one of these systems you want to vary not just you know the last sentence you want to vary the entire context as well from a commercial point of view I think actually testing is really super important I mean you know you've probably all saw the Microsoft Bing chatbot which when it was you know the New York Times reporter studied that chatbot and it announced to the reporter that in fact actually that it preferred to be called Sydney rather than Bing and then also suggested that really you know the reporter should divorce his wife I'm sure behaviors that Microsoft was really not too proud of right and I actually think for commercial purposes it's super important to be able to detect and you know guaranteed that such behaviors really aren't lurking beneath the surface of your large language model. Constraint decoding that's just an NLP topic I won't spend too much time on but I actually do think that there's really interesting work there to look at different ways of actually constraining the output of a large language model and some real challenges there I think there's really interesting work about how one actually trains these models as well so I mentioned that the training procedure is itself actually a very complicated one typically what happens is that they start up by training with what's called the language model training objective which is where effectively you're just simply training the model to predict the very next word but a model which is just simply trained with this large language model training objective on its own doesn't really engage in useful conversation doesn't really follow instructions very well so it's actually very typical to follow that up with an additional training step that is you know that involves well as I said there's reinforcement learning with human feedback and I think that's a really interesting question I've actually got some theories myself about you know when you want to use one sort of training objective versus another and if there's people that'll like to talk about that more generally I think there's a really interesting question which is how do you align the LLM behavior with well how do you get the LLM to behave the way that you want it to right so you've got these very general alignment goals like you know follow commands that run right through to don't destroy humanity so ultimately it's the training data and the training training procedure which is going to determine the LLM behavior so how exactly do we do that right and I actually do think that there's good academic research that can be done there largely because the fine-tuning step that I mentioned this sort of multi-stage training that's pretty modest right tens of thousands of examples or less and it can be done on sort of fairly modest hardware so I actually think that's a that's that's a great academic research topic I feel a little bit guilty here because I just mentioned to you that maybe the only thing which is standing between us and you know a artificial general intelligence is the ability to have episodic memory and then now I'm going to suggest to you how we might actually do that and it's you know the most obvious way to do that is to actually take that retrieval augmented generator that I mentioned before and basically let the large language model write its own memories and this is basically a suggestion about how you might do that more generally I actually think that you know people like me have spent decades trying to come up with specialized knowledge representation systems and specialized inference systems you know so and this essentially is like a specialized logic you know so knowledge graphs and one example of that where you try to encode information in entity relation triples for example but I actually think with LLMs you know one real possibility is that you actually let the you let the primitive statements actually be natural language statements so you just have represented inside of your system stored inside of a vector store for example something like insomnia is a typical symptom of diabetes and then you'd actually let the large language model itself decide the relationship between these atomic propositions and so instead of having a specialized knowledge representation language a specialized logic you'd use natural language and you'd let the large language model actually pass information from one atomic proposition to another for those of you that are as old as I am you know I mean I loved prologue and you know very simple horn clause inference procedures so what I just tried to do here was take take that and sort of show how I might replace first order logic in there with natural language statements but otherwise you've got proof rules proof structures and so this is in fact actually a standard you know textbook example of how uh you might wind up doing inference here so you've asked the question you know can Sam get a degree and you've got a series of facts about what courses Sam has taken and a series of rules but the difference is all this is all expressed in natural language rather than in some first order logic form okay all right so just talking a little bit more about the social implications of all of this so I think to understand the social implications I think one of the things you probably want to understand is try and make some guesses about how the field itself might actually evolve I can see sort of two possible futures one is where we wind up getting ever larger proprietary monolithic close large language models that you effectively interact with via web APIs that is the actual model itself the training data everything is kept proprietary but you can just simply call it over the web another future is that there will be open sourced language models and the weights will actually be available and you'll be able to do things like fine tune those weights yourself and right now you know we're in the world where there's both of these kinds of large language models and the proprietary models are better than the open source models and I think really the big question about the development of the field is whether or not fine tuning will turn out to take the open source models and make them competitive with the closed proprietary models and I call that the 64 billion dollar question because that's probably about the amount of money that the companies that are investing here sort of have invested you know the language models are becoming increasingly capital intensive it costs huge amounts million many millions of dollars to collect the data and actually do the training of these things and capital intensive industries tend to concentrate you know you just look at the chip manufacturing where I think there's only one or two fabrication factories in the entire world that make the top-end chips that we all have in all of our devices so if that is actually what the future of LLMs is then probably we will see that same sort of concentration into just a couple of places I suspect the training data will become increasingly important people are already talking about training data as being the ultimate you know limiting factor and I think it will become the major differentiator particularly if you want to do things like build LLMs for very specialized domains like healthcare finance other things like that but and I actually think data and LLM quality control which goes back to that issue about testing and evaluation I was talking about before that's going to become increasingly important in fact I actually when I think about what will somebody like me in industry be doing in five years time quite possibly you know testing and evaluation will actually be you know 90% of what we do you know we know that fine-tuning can mask a poor large language model right so we know that you can take take Sydney and do a little bit of fine-tuning and have it at least superficially call itself being but then Sydney reemerges in the right context as well so I think one way we'll get around that is we'll start seeing things like certificates of origin you know we'll be saying you know I guarantee that my large language model has been trained on just high quality data and the same way as you see certificates of origin for you know fancy cheeses and things like that you know the cows grazed on grass organically raised on the south's southern meadows and all that sort of stuff and particularly if in the if that open source world that I was talking about before if that comes into play I you know I see that has been sort of really one of the really big challenges I've I've seen how data vendors small startups are really under incredible pressure to produce something which they can sell because they're usually cash constrained and the same thing may be true for startups that are producing large language models they'll be under huge pressure to take somebody else's model and do a few tweaks to it and try to present it as something that's completely new and yeah impact on nlp jobs I I actually do think that it's not too far off when we'll be able to say something like give an instruction to a large language model it's like deploy a chatbot the task is informing users about the products that you'll find listed in this database over here I want you to interact with users in a professional tone emphasize customer service rather than price and politely decline to talk about topics that are related to the products and that will be it that will build you a chatbot you won't need an expert development team you know I do think that however that that's not going to come up immediately we will for the next say five years or so we will need people that can create training data and fine-tune models and as I said before I think evaluation and testing is just going to get more and more important brought to social impacts right so I think we already know that deep fakes and fraud are just going to get supercharged by this sort of technology and yes I think that's true I think we're going to see automation of jobs not previously automated Krugman has an interesting article in the New York Times just a couple of days ago where you know he makes the point that it doesn't really matter whether these LLMs really are intelligent or not that even a souped up auto correct can actually have quite major implications in terms of productivity he's actually really quite positive he seems to think that actually these things might you know level society somewhat and they might I mean there's some evidence that in fact the GPT-4 for example enables poorer workers to work at a higher standard whereas the best workers are helped less by GPT-4 maybe that's the case I think there's a number of risks I you know I think we are seeing you know AI models being trained on public domain data that the creators when they made their data public really had no intention no one no expectation of their data would be used in this way we're seeing a political fight right now between media companies and tech companies about the use of data I think that's still mainly about search rather than training AI models but that same fight I think will continue I looking back to the first industrial revolution and things like the tragedy of the commons I don't see any particular reason to expect a socially optimal outcome although I think the writer's Guild of America settlement actually sounds like it's a pretty forward-looking one and I'm very pleased to see that I I do think you know I I'm not one of these people that that poo-poo's the people that are worried about you know AGI and misalignment I don't think we're likely to be made extinction but to be made extinct but I do think we should be worrying about that and the final point I'd like to make is that I think these things are economic and political choices not really technical choices so it's an interesting question so those of us that actually have the technical expertise probably are in a position to have our voices heard more than what they would normally be so we should probably make use of that but I actually really do think that it's not just up to the the tech companies in particular to try and make the important decisions here so conclusions I think LLM's are here to stay a lot of my people my age remember the AI winter of the last century I don't think there's going to be an AI winter just simply because these things are actually way too useful for students intellectual revolutions are a great time to enter the field because in fact actually the amount of knowledge you need to have to become an expert is much much less I think LLM's open up new interesting scientific research questions and directions NLP I think will have less emphasis on clever new algorithms and more on yeah interaction and integration of models applications data design and training and much more emphasis on evaluation so thank you very much any questions or comments yes hi thank you for great talk super interesting I just wanted to ask you about one thing in the middle of the talk which is about this kind of neuro symbolic integration and you had this you had this kind of proposal that LLM's are going to give you parts of bits and then you're going to use those inputs into a logic model and I wondered like why do you think I mean personally I like neuro symbolic learning so but I'm interested today like why why do you think that's a good idea or good approach like why not just let the LLM do the entire thing like what what is it that so certainly certainly there are people that are betting yeah um you know let's just let the LLM do the entire thing yeah um I guess the answer I would give there is that there are a lot of academics and in fact I'm sure many of you've seen this stuff right it's it's now quite you know there's like a little mini industry of people coming up with things like you know chat GPT cannot understand negation GPT for does not understand X is Y statements you know and in fact actually I don't think I'm ashamed but we have a paper that is claiming that LLM's cannot understand you know do not really properly understand entailment you know that walking entails moving but moving does not always entail walking um so if you really believe that stuff if you really believe you know and if you believe it enough so that you actually think that GPT 5 or whatever the next model is that comes out is going to have to exactly that same weakness the idea then is build a symbolic component that addresses whatever weakness you happen to think these large language models have but it isn't like I'll admit it is rather risky because these things are improving so rapidly and I'm not so sure I mean it's it's a risk if you're a grad student to say all right I'm going to commit the next year or two of my life to working on solving you know the problem of negation in large language models and halfway through your research project you know someone discovers that just by scaling up the training data another order of magnitude all of a sudden now it's going to handle negation just perfectly and that's what I meant by in fact I think that was one of the one of the slides that Kate added was saying that it's particularly if you wanted to do something some sort of behavioral research or something else like that you know where this the timescale which LLMs are changing versus the timescale of doing behavioral research is the LLMs are just changing so fast that if you if you looked at today's LLMs and said okay you're inspired by then here are some interesting behavioral predictions that they're making I'm going to go out and start running some experiments with kids or something like that yeah you know by the time you've collected a quarter of your data there's another model out there and it's got it's making different behavioral predictions it's just I don't know what the answer there is except to say that yeah yeah you might just um ask a a follow-up question so if if you have a chat system I think that's actually for the zooms but I they also said there's a whole array microphone inside the box so well um I'll just talk about um if as opposed to um adding a symbolic system to to a chat system or you know an LLM um what about fine tuning and just doing lots of fine tuning instead I mean that's adding more data but if you fine tune it yes with that kind of data as opposed to going as symbolic group that's right wouldn't that be I think that's I think that's very true and in fact that I think that's a good question is you know if so right now what was it you know Gary Marcus is picking up on the fact that uh somebody wrote a paper that said that uh oh look there's a whole lot of cases where the large language models uh will quite happily say that oh let's see all right I don't know enough about celebrities unfortunately but you know so-and-so and so is Tom Cruise's mother right okay it accepts that statement and you then ask the other you then ask the question who is Tom Cruise's mother and it says I have no idea so and it seems like oh well if I've actually got a couple of comments there so first of all it seems like x is y that looks an awful lot to a math mathematical person as being like x equals y and we know that what was it uh equality is what is it commutative you know so if x equals y then y equals x um in fact actually x is a y actually really isn't commutative um you know uh you know uh what was it you know chicken salad is a wonderful meal you know that can be true but a wonderful meal doesn't always have to be chicken salad right um you know so uh but anyway so might you know the yeah do you think uh do you think there's going to be uh kind of I mean what what do you think about like the kind of vision vision language models where you're kind of train it on like a ton of images and then it generates a load of you know you can generate these images we actually did you know before I did the startups just before I did the startups I think the last student I worked with was working on image captioning um and I'd very much like to go back to I I think that's really incredibly interesting so while I think it doesn't actually solve the chart the cell Chinese rule Chinese room objection you know I mean ultimately the input to all these models are really just activation patterns and you know the models got no reason to suspect that images closely you know more connected to the world than languages but I think just in practice there's a very good chance that there may be really interesting correlations that can be learned by correlating you know images with language and of course the only issue there is that the um the amount of compute that's needed to do this is just really enormous and it's you know you'd have to do some have to get some deal probably with one of the major tech companies to get your access to enough compute yeah to do it and that's that's sort of the problem with with a lot of this research now and in fact I think one of the comments that I wrote there is you know maybe in fact we should be thinking about something you know physics has been very successful in getting funding for big science maybe we should be thinking about ways of getting funding for academic big science as well for dealing with this stuff well and there if you think of images still images that's a certain amount of compute power I think of visual no ongoing visual scenes and movies that's a lot more and yet presumably real world learners are you know taking advantage of the visual scene as it moves by um and uh although learning can take place in blind people as well and that's a whole another research area so you know you don't need vision to learn language but it certainly can facilitate aspects of yeah and then yeah and I guess also I mean like looking forward you'd want these systems to actually be kind of situated in the physical world somehow I guess right well so that so that's the certainly lots of people have got that uh you know feeling that in fact actually that that we need situated you know situated models but I mean isn't the input to a model always really just an activation pattern isn't it really always just uh I mean couldn't you always run you know so you know there was the question what is silver's chinese room argument right there was that basically you know uh supposing you you come up with a computer program which can translate english into chinese or sorry translate chinese into english so you give that to a person that's sitting you know inside of a room and you just simply tell him you know here's a set of symbols follow these instructions and give me the output that you you you obtained by following these instructions and so's point is that even if this thing does actually produce good english as an output you really can't say that the person understands chinese you know that just they don't understand chinese you know they're just following these rules and uh so his argument really is that there's something else that a pure rule following system really doesn't have understanding that something else is required now lots of people wind up saying oh well what you really need is grounding you really do need these you know you need the symbols to be connected somehow to the real world but I think the model never really knows I don't see any way for our current computational models to know that the bit patterns that we're feeding into them yeah correspond to anything in the world yeah I mean yeah I would agree on that but um I guess for that kind of symbol grounding you like some some people argue you you need to have a kind of community right language users so maybe that uh all kind of do you know um and and then the grounding kind of comes out of people using the same symbol in the same way yes right yeah yes no no no I mean you're kripke you know so philosophers of language like kripke have argued that in fact actually that and you know in fact this is sort of very true of me because I grew up in the southern hemisphere I don't know a lot of the northern trees so I'm not sure I can with catherine's help now I can recognize aspens but you know I'm not really sure about the difference between oaks and elms and the rest of them right but kripke you would say that I can still talk about all of those things and when I talk about oaks I mean oak trees even though I might not be able to actually identify an oak tree reliably and so kripke's story there is exactly what you're saying it's a community of language learners sorry language users and I am willing basically I'm agreeing to the authority of language users and I effectively what I'm saying is when I use oak I use it to mean whatever the rest of you mean that you know that grew up and presumably know exactly what an oak tree is right and he says that in fact actually with a lot of particularly scientific terminology we wind up using it that way right many of us may not be able to define exactly what the difference is between the different types of neutrinos or whatever but we rely on experts within our community to be able to ground those things all I can say is I don't even know how you'd even tell a large language model that it's part of yeah yeah no I was sort of yeah kind of thinking that sort of thing myself like how well what I mean does that does the community there just mean literally that actually just the text documents that have been fed into it is that I mean I guess with the the instruction tuning I guess you get a little bit of that right so anyone else have questions at all yeah so that it's almost just that you could have a community of users of a particular model yeah that would have a sort of various types of queries within a particular domain that might help train up that model then to become more a realistic conversational agent within that particular domain perhaps yeah yeah so maybe that's kind of yeah yeah yeah who knows maybe a year or two from now you know those things will start to emerge yeah yeah so I think one of the things that Catherine and I are hoping to get out of this is to find out more about you know work at SFI that we might connect with right but a sort of general interest in you know language learning psychological aspects computational aspects you know so so we're here until Tuesday afternoon Tuesday evening so please please contact us right okay thanks", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.16, "text": " Perhaps Catherine should just introduce ourselves. So we were at Brown University for several decades", "tokens": [50364, 10517, 23098, 820, 445, 5366, 4175, 13, 407, 321, 645, 412, 8030, 3535, 337, 2940, 7878, 50772], "temperature": 0.0, "avg_logprob": -0.24035670152351038, "compression_ratio": 1.4333333333333333, "no_speech_prob": 0.006969159469008446}, {"id": 1, "seek": 0, "start": 8.16, "end": 13.84, "text": " in the Department of Cognitive and Holistic Science there. I was also joined in Computer Science.", "tokens": [50772, 294, 264, 5982, 295, 383, 2912, 2187, 293, 11086, 3142, 8976, 456, 13, 286, 390, 611, 6869, 294, 22289, 8976, 13, 51056], "temperature": 0.0, "avg_logprob": -0.24035670152351038, "compression_ratio": 1.4333333333333333, "no_speech_prob": 0.006969159469008446}, {"id": 2, "seek": 0, "start": 14.88, "end": 22.080000000000002, "text": " Moved to Australia about 14 years ago to Macquarie University and at the same time I think we started", "tokens": [51108, 3335, 937, 281, 7060, 466, 3499, 924, 2057, 281, 5707, 16177, 414, 3535, 293, 412, 264, 912, 565, 286, 519, 321, 1409, 51468], "temperature": 0.0, "avg_logprob": -0.24035670152351038, "compression_ratio": 1.4333333333333333, "no_speech_prob": 0.006969159469008446}, {"id": 3, "seek": 2208, "start": 22.08, "end": 31.599999999999998, "text": " becoming external faculty here at SFI. Catherine got a gigantic mega grant that required hiring a", "tokens": [50364, 5617, 8320, 6389, 510, 412, 31095, 40, 13, 23098, 658, 257, 26800, 17986, 6386, 300, 4739, 15335, 257, 50840], "temperature": 0.0, "avg_logprob": -0.11310731387529217, "compression_ratio": 1.583815028901734, "no_speech_prob": 0.24849741160869598}, {"id": 4, "seek": 2208, "start": 31.599999999999998, "end": 38.4, "text": " lot of people doing a lot of work there. I started going into the startup route", "tokens": [50840, 688, 295, 561, 884, 257, 688, 295, 589, 456, 13, 286, 1409, 516, 666, 264, 18578, 7955, 51180], "temperature": 0.0, "avg_logprob": -0.11310731387529217, "compression_ratio": 1.583815028901734, "no_speech_prob": 0.24849741160869598}, {"id": 5, "seek": 2208, "start": 39.84, "end": 45.36, "text": " and then what with that and then with COVID I think we let the external faculty connection lapse", "tokens": [51252, 293, 550, 437, 365, 300, 293, 550, 365, 4566, 286, 519, 321, 718, 264, 8320, 6389, 4984, 49757, 51528], "temperature": 0.0, "avg_logprob": -0.11310731387529217, "compression_ratio": 1.583815028901734, "no_speech_prob": 0.24849741160869598}, {"id": 6, "seek": 4536, "start": 46.16, "end": 52.16, "text": " but now we'd like to restart the connection with SFI again.", "tokens": [50404, 457, 586, 321, 1116, 411, 281, 21022, 264, 4984, 365, 31095, 40, 797, 13, 50704], "temperature": 0.0, "avg_logprob": -0.09501609931121001, "compression_ratio": 1.5, "no_speech_prob": 0.017105111852288246}, {"id": 7, "seek": 4536, "start": 54.56, "end": 58.24, "text": " The startup stuff that I was doing was actually in the chatbot space.", "tokens": [50824, 440, 18578, 1507, 300, 286, 390, 884, 390, 767, 294, 264, 5081, 18870, 1901, 13, 51008], "temperature": 0.0, "avg_logprob": -0.09501609931121001, "compression_ratio": 1.5, "no_speech_prob": 0.017105111852288246}, {"id": 8, "seek": 4536, "start": 60.24, "end": 65.6, "text": " We got acquired by Oracle Corporation. That's the reason for this disclaimer down here at", "tokens": [51108, 492, 658, 17554, 538, 25654, 26464, 13, 663, 311, 264, 1778, 337, 341, 40896, 760, 510, 412, 51376], "temperature": 0.0, "avg_logprob": -0.09501609931121001, "compression_ratio": 1.5, "no_speech_prob": 0.017105111852288246}, {"id": 9, "seek": 4536, "start": 65.6, "end": 72.0, "text": " the bottom right. So nothing that I'm saying represents anything of any of our employers.", "tokens": [51376, 264, 2767, 558, 13, 407, 1825, 300, 286, 478, 1566, 8855, 1340, 295, 604, 295, 527, 16744, 13, 51696], "temperature": 0.0, "avg_logprob": -0.09501609931121001, "compression_ratio": 1.5, "no_speech_prob": 0.017105111852288246}, {"id": 10, "seek": 7200, "start": 72.72, "end": 77.76, "text": " The other reason for this comment here is because the whole field of large language", "tokens": [50400, 440, 661, 1778, 337, 341, 2871, 510, 307, 570, 264, 1379, 2519, 295, 2416, 2856, 50652], "temperature": 0.0, "avg_logprob": -0.10141977044038994, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.003583562560379505}, {"id": 11, "seek": 7200, "start": 77.76, "end": 85.2, "text": " bottles is really changing incredibly quickly. I don't know how much of what I'm going to say", "tokens": [50652, 15923, 307, 534, 4473, 6252, 2661, 13, 286, 500, 380, 458, 577, 709, 295, 437, 286, 478, 516, 281, 584, 51024], "temperature": 0.0, "avg_logprob": -0.10141977044038994, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.003583562560379505}, {"id": 12, "seek": 7200, "start": 85.2, "end": 92.08, "text": " right now is going to be true or relevant in six months time. So nothing, I'm not saying anything", "tokens": [51024, 558, 586, 307, 516, 281, 312, 2074, 420, 7340, 294, 2309, 2493, 565, 13, 407, 1825, 11, 286, 478, 406, 1566, 1340, 51368], "temperature": 0.0, "avg_logprob": -0.10141977044038994, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.003583562560379505}, {"id": 13, "seek": 7200, "start": 92.08, "end": 98.64, "text": " that I believe is false but it could well, some of the effort could well be wasted.", "tokens": [51368, 300, 286, 1697, 307, 7908, 457, 309, 727, 731, 11, 512, 295, 264, 4630, 727, 731, 312, 19496, 13, 51696], "temperature": 0.0, "avg_logprob": -0.10141977044038994, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.003583562560379505}, {"id": 14, "seek": 9864, "start": 98.96, "end": 108.64, "text": " So I'm going to try and give you guys the high level executive overview talk. This talk was", "tokens": [50380, 407, 286, 478, 516, 281, 853, 293, 976, 291, 1074, 264, 1090, 1496, 10140, 12492, 751, 13, 639, 751, 390, 50864], "temperature": 0.0, "avg_logprob": -0.10901123285293579, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0004286159237381071}, {"id": 15, "seek": 9864, "start": 108.64, "end": 114.48, "text": " really actually aimed originally at academic researchers in natural language processing", "tokens": [50864, 534, 767, 20540, 7993, 412, 7778, 10309, 294, 3303, 2856, 9007, 51156], "temperature": 0.0, "avg_logprob": -0.10901123285293579, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0004286159237381071}, {"id": 16, "seek": 9864, "start": 114.48, "end": 121.12, "text": " and so I'm going to skip some of those some of those details but you know you had to have been", "tokens": [51156, 293, 370, 286, 478, 516, 281, 10023, 512, 295, 729, 512, 295, 729, 4365, 457, 291, 458, 291, 632, 281, 362, 668, 51488], "temperature": 0.0, "avg_logprob": -0.10901123285293579, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0004286159237381071}, {"id": 17, "seek": 9864, "start": 121.12, "end": 126.8, "text": " underneath a rock if you didn't notice the large language models who radically changed the field", "tokens": [51488, 7223, 257, 3727, 498, 291, 994, 380, 3449, 264, 2416, 2856, 5245, 567, 35508, 3105, 264, 2519, 51772], "temperature": 0.0, "avg_logprob": -0.10901123285293579, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0004286159237381071}, {"id": 18, "seek": 12680, "start": 126.8, "end": 136.32, "text": " in the last year or two and so I'll be talking about that. We'll also be talking about suggestions.", "tokens": [50364, 294, 264, 1036, 1064, 420, 732, 293, 370, 286, 603, 312, 1417, 466, 300, 13, 492, 603, 611, 312, 1417, 466, 13396, 13, 50840], "temperature": 0.0, "avg_logprob": -0.15807184051064885, "compression_ratio": 1.5989304812834224, "no_speech_prob": 0.00095200608484447}, {"id": 19, "seek": 12680, "start": 136.32, "end": 142.64, "text": " You know how should academics actually respond to this changing world? So we'll be talking a little", "tokens": [50840, 509, 458, 577, 820, 25695, 767, 4196, 281, 341, 4473, 1002, 30, 407, 321, 603, 312, 1417, 257, 707, 51156], "temperature": 0.0, "avg_logprob": -0.15807184051064885, "compression_ratio": 1.5989304812834224, "no_speech_prob": 0.00095200608484447}, {"id": 20, "seek": 12680, "start": 142.64, "end": 148.96, "text": " bit about what these large language models may mean or may not mean for human language acquisition,", "tokens": [51156, 857, 466, 437, 613, 2416, 2856, 5245, 815, 914, 420, 815, 406, 914, 337, 1952, 2856, 21668, 11, 51472], "temperature": 0.0, "avg_logprob": -0.15807184051064885, "compression_ratio": 1.5989304812834224, "no_speech_prob": 0.00095200608484447}, {"id": 21, "seek": 14896, "start": 149.04000000000002, "end": 156.48000000000002, "text": " the study of that. There's a lot of interest in neuro-symbolic models that integrate large", "tokens": [50368, 264, 2979, 295, 300, 13, 821, 311, 257, 688, 295, 1179, 294, 16499, 12, 3187, 5612, 299, 5245, 300, 13365, 2416, 50740], "temperature": 0.0, "avg_logprob": -0.09565877191948169, "compression_ratio": 1.6529411764705881, "no_speech_prob": 0.017609482631087303}, {"id": 22, "seek": 14896, "start": 156.48000000000002, "end": 163.20000000000002, "text": " language models and more traditional kinds of AI models and I'll also talk a little bit about", "tokens": [50740, 2856, 5245, 293, 544, 5164, 3685, 295, 7318, 5245, 293, 286, 603, 611, 751, 257, 707, 857, 466, 51076], "temperature": 0.0, "avg_logprob": -0.09565877191948169, "compression_ratio": 1.6529411764705881, "no_speech_prob": 0.017609482631087303}, {"id": 23, "seek": 14896, "start": 163.20000000000002, "end": 172.8, "text": " alignment of LLMs and I'll end talking a bit about the connections between large language models", "tokens": [51076, 18515, 295, 441, 43, 26386, 293, 286, 603, 917, 1417, 257, 857, 466, 264, 9271, 1296, 2416, 2856, 5245, 51556], "temperature": 0.0, "avg_logprob": -0.09565877191948169, "compression_ratio": 1.6529411764705881, "no_speech_prob": 0.017609482631087303}, {"id": 24, "seek": 17280, "start": 172.8, "end": 179.68, "text": " and the social implications of them and lessons from the first industrial revolution.", "tokens": [50364, 293, 264, 2093, 16602, 295, 552, 293, 8820, 490, 264, 700, 9987, 8894, 13, 50708], "temperature": 0.0, "avg_logprob": -0.1084337363371978, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.0005095850792713463}, {"id": 25, "seek": 17280, "start": 181.04000000000002, "end": 186.24, "text": " Katherine and I just spent two months at the University of Edinburgh and it was very interesting", "tokens": [50776, 33478, 293, 286, 445, 4418, 732, 2493, 412, 264, 3535, 295, 41215, 293, 309, 390, 588, 1880, 51036], "temperature": 0.0, "avg_logprob": -0.1084337363371978, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.0005095850792713463}, {"id": 26, "seek": 17280, "start": 186.24, "end": 191.36, "text": " there because of course the first industrial revolution really happened in Northern England", "tokens": [51036, 456, 570, 295, 1164, 264, 700, 9987, 8894, 534, 2011, 294, 14335, 8196, 51292], "temperature": 0.0, "avg_logprob": -0.1084337363371978, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.0005095850792713463}, {"id": 27, "seek": 17280, "start": 191.36, "end": 200.32000000000002, "text": " and around Scotland and actually it's a fascinating thing to read about that you know James Watt,", "tokens": [51292, 293, 926, 11180, 293, 767, 309, 311, 257, 10343, 551, 281, 1401, 466, 300, 291, 458, 5678, 343, 1591, 11, 51740], "temperature": 0.0, "avg_logprob": -0.1084337363371978, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.0005095850792713463}, {"id": 28, "seek": 20032, "start": 200.32, "end": 205.28, "text": " you know the inventor of the steam engine actually did a lot of his work at the University of Glasgow", "tokens": [50364, 291, 458, 264, 41593, 295, 264, 11952, 2848, 767, 630, 257, 688, 295, 702, 589, 412, 264, 3535, 295, 40457, 50612], "temperature": 0.0, "avg_logprob": -0.09517551940164448, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.0024310972075909376}, {"id": 29, "seek": 20032, "start": 206.32, "end": 210.88, "text": " so that's sort of really interesting. Okay so how deep learning changes NLP?", "tokens": [50664, 370, 300, 311, 1333, 295, 534, 1880, 13, 1033, 370, 577, 2452, 2539, 2962, 426, 45196, 30, 50892], "temperature": 0.0, "avg_logprob": -0.09517551940164448, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.0024310972075909376}, {"id": 30, "seek": 20032, "start": 213.84, "end": 218.72, "text": " You know people like Katherine and I have been in the field long enough to have actually seen", "tokens": [51040, 509, 458, 561, 411, 33478, 293, 286, 362, 668, 294, 264, 2519, 938, 1547, 281, 362, 767, 1612, 51284], "temperature": 0.0, "avg_logprob": -0.09517551940164448, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.0024310972075909376}, {"id": 31, "seek": 20032, "start": 218.72, "end": 224.79999999999998, "text": " really a switch from symbolic parameters, statistical NLP and then finally now to deep", "tokens": [51284, 534, 257, 3679, 490, 25755, 9834, 11, 22820, 426, 45196, 293, 550, 2721, 586, 281, 2452, 51588], "temperature": 0.0, "avg_logprob": -0.09517551940164448, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.0024310972075909376}, {"id": 32, "seek": 22480, "start": 224.8, "end": 232.48000000000002, "text": " learning and large language models and I guess I would say that the really huge difference is that", "tokens": [50364, 2539, 293, 2416, 2856, 5245, 293, 286, 2041, 286, 576, 584, 300, 264, 534, 2603, 2649, 307, 300, 50748], "temperature": 0.0, "avg_logprob": -0.08250776529312134, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.006883991416543722}, {"id": 33, "seek": 22480, "start": 232.48000000000002, "end": 240.8, "text": " large language models can understand context much much longer contexts than anything that I thought", "tokens": [50748, 2416, 2856, 5245, 393, 1223, 4319, 709, 709, 2854, 30628, 813, 1340, 300, 286, 1194, 51164], "temperature": 0.0, "avg_logprob": -0.08250776529312134, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.006883991416543722}, {"id": 34, "seek": 22480, "start": 240.8, "end": 247.52, "text": " was even possible. In fact I actually thought there were good theoretical arguments that it should", "tokens": [51164, 390, 754, 1944, 13, 682, 1186, 286, 767, 1194, 456, 645, 665, 20864, 12869, 300, 309, 820, 51500], "temperature": 0.0, "avg_logprob": -0.08250776529312134, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.006883991416543722}, {"id": 35, "seek": 22480, "start": 247.52, "end": 253.76000000000002, "text": " not be possible to try and model a context as long as a sentence or a dialogue or an entire", "tokens": [51500, 406, 312, 1944, 281, 853, 293, 2316, 257, 4319, 382, 938, 382, 257, 8174, 420, 257, 10221, 420, 364, 2302, 51812], "temperature": 0.0, "avg_logprob": -0.08250776529312134, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.006883991416543722}, {"id": 36, "seek": 25376, "start": 253.76, "end": 260.96, "text": " paragraph yet large language models can do that perfectly well and if you want to I can go through", "tokens": [50364, 18865, 1939, 2416, 2856, 5245, 393, 360, 300, 6239, 731, 293, 498, 291, 528, 281, 286, 393, 352, 807, 50724], "temperature": 0.0, "avg_logprob": -0.09838297280920558, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.00043555328738875687}, {"id": 37, "seek": 25376, "start": 260.96, "end": 270.08, "text": " what those counting arguments would be but just you know when I actually look back at the", "tokens": [50724, 437, 729, 13251, 12869, 576, 312, 457, 445, 291, 458, 562, 286, 767, 574, 646, 412, 264, 51180], "temperature": 0.0, "avg_logprob": -0.09838297280920558, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.00043555328738875687}, {"id": 38, "seek": 25376, "start": 270.08, "end": 276.64, "text": " pre-deep learning work you know I did a lot of work on syntactic parsing which would involve", "tokens": [51180, 659, 12, 38422, 2539, 589, 291, 458, 286, 630, 257, 688, 295, 589, 322, 23980, 19892, 21156, 278, 597, 576, 9494, 51508], "temperature": 0.0, "avg_logprob": -0.09838297280920558, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.00043555328738875687}, {"id": 39, "seek": 25376, "start": 276.64, "end": 282.71999999999997, "text": " recovering pastries that looked something like this and I think the motivation for dealing with", "tokens": [51508, 29180, 1791, 2244, 300, 2956, 746, 411, 341, 293, 286, 519, 264, 12335, 337, 6260, 365, 51812], "temperature": 0.0, "avg_logprob": -0.09838297280920558, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.00043555328738875687}, {"id": 40, "seek": 28272, "start": 283.44000000000005, "end": 292.56, "text": " you know wanting structures like these these things make local a lot of dependencies which", "tokens": [50400, 291, 458, 7935, 9227, 411, 613, 613, 721, 652, 2654, 257, 688, 295, 36606, 597, 50856], "temperature": 0.0, "avg_logprob": -0.10239637875166095, "compression_ratio": 1.6331360946745561, "no_speech_prob": 0.001452133757993579}, {"id": 41, "seek": 28272, "start": 292.56, "end": 300.0, "text": " are non-local in the string here right so you know it's a flight through Denver right so", "tokens": [50856, 366, 2107, 12, 5842, 304, 294, 264, 6798, 510, 558, 370, 291, 458, 309, 311, 257, 7018, 807, 26270, 558, 370, 51228], "temperature": 0.0, "avg_logprob": -0.10239637875166095, "compression_ratio": 1.6331360946745561, "no_speech_prob": 0.001452133757993579}, {"id": 42, "seek": 28272, "start": 300.8, "end": 307.20000000000005, "text": " this relationship here is making that local the same thing also we're talking about a preference", "tokens": [51268, 341, 2480, 510, 307, 1455, 300, 2654, 264, 912, 551, 611, 321, 434, 1417, 466, 257, 17502, 51588], "temperature": 0.0, "avg_logprob": -0.10239637875166095, "compression_ratio": 1.6331360946745561, "no_speech_prob": 0.001452133757993579}, {"id": 43, "seek": 30720, "start": 307.2, "end": 313.68, "text": " for a flight for example that's making that dependency local why do you want to do that you", "tokens": [50364, 337, 257, 7018, 337, 1365, 300, 311, 1455, 300, 33621, 2654, 983, 360, 291, 528, 281, 360, 300, 291, 50688], "temperature": 0.0, "avg_logprob": -0.07654790761994153, "compression_ratio": 1.75, "no_speech_prob": 0.0055481749586761}, {"id": 44, "seek": 30720, "start": 313.68, "end": 321.03999999999996, "text": " want to do that if you think that you can only really model pairs pairwise interactions large", "tokens": [50688, 528, 281, 360, 300, 498, 291, 519, 300, 291, 393, 787, 534, 2316, 15494, 6119, 3711, 13280, 2416, 51056], "temperature": 0.0, "avg_logprob": -0.07654790761994153, "compression_ratio": 1.75, "no_speech_prob": 0.0055481749586761}, {"id": 45, "seek": 30720, "start": 321.03999999999996, "end": 327.84, "text": " language models as I said are happy modeling contexts of you know in the hundreds or the", "tokens": [51056, 2856, 5245, 382, 286, 848, 366, 2055, 15983, 30628, 295, 291, 458, 294, 264, 6779, 420, 264, 51396], "temperature": 0.0, "avg_logprob": -0.07654790761994153, "compression_ratio": 1.75, "no_speech_prob": 0.0055481749586761}, {"id": 46, "seek": 30720, "start": 327.84, "end": 334.88, "text": " thousands of tokens and so I actually think you know these theories are correct but they're just", "tokens": [51396, 5383, 295, 22667, 293, 370, 286, 767, 519, 291, 458, 613, 13667, 366, 3006, 457, 436, 434, 445, 51748], "temperature": 0.0, "avg_logprob": -0.07654790761994153, "compression_ratio": 1.75, "no_speech_prob": 0.0055481749586761}, {"id": 47, "seek": 33488, "start": 334.88, "end": 340.15999999999997, "text": " no longer required so to do natural language understanding I don't actually have to recover", "tokens": [50364, 572, 2854, 4739, 370, 281, 360, 3303, 2856, 3701, 286, 500, 380, 767, 362, 281, 8114, 50628], "temperature": 0.0, "avg_logprob": -0.051524564817354276, "compression_ratio": 1.6130434782608696, "no_speech_prob": 0.0004043467342853546}, {"id": 48, "seek": 33488, "start": 340.15999999999997, "end": 346.8, "text": " this sort of structure anymore okay so strengths of large language models we just talked about", "tokens": [50628, 341, 1333, 295, 3877, 3602, 1392, 370, 16986, 295, 2416, 2856, 5245, 321, 445, 2825, 466, 50960], "temperature": 0.0, "avg_logprob": -0.051524564817354276, "compression_ratio": 1.6130434782608696, "no_speech_prob": 0.0004043467342853546}, {"id": 49, "seek": 33488, "start": 346.8, "end": 353.68, "text": " how there's so much better at working with incredibly long contexts I think the other you", "tokens": [50960, 577, 456, 311, 370, 709, 1101, 412, 1364, 365, 6252, 938, 30628, 286, 519, 264, 661, 291, 51304], "temperature": 0.0, "avg_logprob": -0.051524564817354276, "compression_ratio": 1.6130434782608696, "no_speech_prob": 0.0004043467342853546}, {"id": 50, "seek": 33488, "start": 353.68, "end": 360.32, "text": " know I'm giving you the executive summary here the other really amazing thing is that you know", "tokens": [51304, 458, 286, 478, 2902, 291, 264, 10140, 12691, 510, 264, 661, 534, 2243, 551, 307, 300, 291, 458, 51636], "temperature": 0.0, "avg_logprob": -0.051524564817354276, "compression_ratio": 1.6130434782608696, "no_speech_prob": 0.0004043467342853546}, {"id": 51, "seek": 36032, "start": 360.32, "end": 365.44, "text": " if you showed these large language models to people from the first neural network revolution", "tokens": [50364, 498, 291, 4712, 613, 2416, 2856, 5245, 281, 561, 490, 264, 700, 18161, 3209, 8894, 50620], "temperature": 0.0, "avg_logprob": -0.06429637395418607, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.0074445512145757675}, {"id": 52, "seek": 36032, "start": 365.44, "end": 372.64, "text": " from the 1980s actually everything would be really quite familiar to them right there's", "tokens": [50620, 490, 264, 13626, 82, 767, 1203, 576, 312, 534, 1596, 4963, 281, 552, 558, 456, 311, 50980], "temperature": 0.0, "avg_logprob": -0.06429637395418607, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.0074445512145757675}, {"id": 53, "seek": 36032, "start": 372.64, "end": 379.6, "text": " there's a slight twist with the attention mechanism but you could explain that in about 15 minutes", "tokens": [50980, 456, 311, 257, 4036, 8203, 365, 264, 3202, 7513, 457, 291, 727, 2903, 300, 294, 466, 2119, 2077, 51328], "temperature": 0.0, "avg_logprob": -0.06429637395418607, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.0074445512145757675}, {"id": 54, "seek": 36032, "start": 379.6, "end": 386.0, "text": " and those guys understand it all you know and Sutton has this thing which he calls the bitter", "tokens": [51328, 293, 729, 1074, 1223, 309, 439, 291, 458, 293, 40492, 1756, 575, 341, 551, 597, 415, 5498, 264, 13871, 51648], "temperature": 0.0, "avg_logprob": -0.06429637395418607, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.0074445512145757675}, {"id": 55, "seek": 38600, "start": 386.08, "end": 392.72, "text": " lesson which is that you know for people like me who work on lots of really interesting algorithms", "tokens": [50368, 6898, 597, 307, 300, 291, 458, 337, 561, 411, 385, 567, 589, 322, 3195, 295, 534, 1880, 14642, 50700], "temperature": 0.0, "avg_logprob": -0.07746810913085937, "compression_ratio": 1.8365384615384615, "no_speech_prob": 0.012401435524225235}, {"id": 56, "seek": 38600, "start": 392.72, "end": 398.64, "text": " and models and structures and you know things like that actually really none of it really", "tokens": [50700, 293, 5245, 293, 9227, 293, 291, 458, 721, 411, 300, 767, 534, 6022, 295, 309, 534, 50996], "temperature": 0.0, "avg_logprob": -0.07746810913085937, "compression_ratio": 1.8365384615384615, "no_speech_prob": 0.012401435524225235}, {"id": 57, "seek": 38600, "start": 398.64, "end": 406.96, "text": " seemed to be relevant if you just got data and compute and scaled everything up that that really", "tokens": [50996, 6576, 281, 312, 7340, 498, 291, 445, 658, 1412, 293, 14722, 293, 36039, 1203, 493, 300, 300, 534, 51412], "temperature": 0.0, "avg_logprob": -0.07746810913085937, "compression_ratio": 1.8365384615384615, "no_speech_prob": 0.012401435524225235}, {"id": 58, "seek": 38600, "start": 406.96, "end": 412.56, "text": " seems to be the secret and I actually do think that these large language models are largely just", "tokens": [51412, 2544, 281, 312, 264, 4054, 293, 286, 767, 360, 519, 300, 613, 2416, 2856, 5245, 366, 11611, 445, 51692], "temperature": 0.0, "avg_logprob": -0.07746810913085937, "compression_ratio": 1.8365384615384615, "no_speech_prob": 0.012401435524225235}, {"id": 59, "seek": 41256, "start": 412.64, "end": 418.64, "text": " sort of brute forcing linguistic generalization so they're covering rather than really capturing them", "tokens": [50368, 1333, 295, 47909, 19030, 43002, 2674, 2144, 370, 436, 434, 10322, 2831, 813, 534, 23384, 552, 50668], "temperature": 0.0, "avg_logprob": -0.07414273974261706, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.002246217569336295}, {"id": 60, "seek": 41256, "start": 419.2, "end": 424.08, "text": " but it's really interesting how well they work right that in fact actually with enough compute", "tokens": [50696, 457, 309, 311, 534, 1880, 577, 731, 436, 589, 558, 300, 294, 1186, 767, 365, 1547, 14722, 50940], "temperature": 0.0, "avg_logprob": -0.07414273974261706, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.002246217569336295}, {"id": 61, "seek": 41256, "start": 424.08, "end": 431.44, "text": " and enough data you do seem to be able to produce something that's really quite amazing okay weaknesses", "tokens": [50940, 293, 1547, 1412, 291, 360, 1643, 281, 312, 1075, 281, 5258, 746, 300, 311, 534, 1596, 2243, 1392, 24381, 51308], "temperature": 0.0, "avg_logprob": -0.07414273974261706, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.002246217569336295}, {"id": 62, "seek": 41256, "start": 431.44, "end": 437.2, "text": " of large language models I mean I think this is probably everybody here's already heard about", "tokens": [51308, 295, 2416, 2856, 5245, 286, 914, 286, 519, 341, 307, 1391, 2201, 510, 311, 1217, 2198, 466, 51596], "temperature": 0.0, "avg_logprob": -0.07414273974261706, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.002246217569336295}, {"id": 63, "seek": 43720, "start": 437.2, "end": 443.52, "text": " this stuff you know Emily Bender's stochastic parrots you know or as I guess Gertrude Stein would", "tokens": [50364, 341, 1507, 291, 458, 15034, 363, 3216, 311, 342, 8997, 2750, 971, 81, 1971, 291, 458, 420, 382, 286, 2041, 460, 911, 81, 2303, 29453, 576, 50680], "temperature": 0.0, "avg_logprob": -0.08293913169340654, "compression_ratio": 1.76036866359447, "no_speech_prob": 0.02512560598552227}, {"id": 64, "seek": 43720, "start": 443.52, "end": 450.0, "text": " say there's no there there and I think actually these large language models just you know they", "tokens": [50680, 584, 456, 311, 572, 456, 456, 293, 286, 519, 767, 613, 2416, 2856, 5245, 445, 291, 458, 436, 51004], "temperature": 0.0, "avg_logprob": -0.08293913169340654, "compression_ratio": 1.76036866359447, "no_speech_prob": 0.02512560598552227}, {"id": 65, "seek": 43720, "start": 450.0, "end": 457.03999999999996, "text": " respond reflexively in fact I actually think if we're talking about human models these large", "tokens": [51004, 4196, 23802, 3413, 294, 1186, 286, 767, 519, 498, 321, 434, 1417, 466, 1952, 5245, 613, 2416, 51356], "temperature": 0.0, "avg_logprob": -0.08293913169340654, "compression_ratio": 1.76036866359447, "no_speech_prob": 0.02512560598552227}, {"id": 66, "seek": 43720, "start": 457.03999999999996, "end": 463.28, "text": " language models maybe actually aren't such so bad for the purely reflexive processing that maybe", "tokens": [51356, 2856, 5245, 1310, 767, 3212, 380, 1270, 370, 1578, 337, 264, 17491, 23802, 488, 9007, 300, 1310, 51668], "temperature": 0.0, "avg_logprob": -0.08293913169340654, "compression_ratio": 1.76036866359447, "no_speech_prob": 0.02512560598552227}, {"id": 67, "seek": 46328, "start": 463.28, "end": 470.08, "text": " humans might be able to do but scaled up you know whereas humans maybe have a context window", "tokens": [50364, 6255, 1062, 312, 1075, 281, 360, 457, 36039, 493, 291, 458, 9735, 6255, 1310, 362, 257, 4319, 4910, 50704], "temperature": 0.0, "avg_logprob": -0.08425418636466883, "compression_ratio": 1.7751196172248804, "no_speech_prob": 0.0015723605174571276}, {"id": 68, "seek": 46328, "start": 470.08, "end": 474.96, "text": " of five or something like that these large language models have a context window of five", "tokens": [50704, 295, 1732, 420, 746, 411, 300, 613, 2416, 2856, 5245, 362, 257, 4319, 4910, 295, 1732, 50948], "temperature": 0.0, "avg_logprob": -0.08425418636466883, "compression_ratio": 1.7751196172248804, "no_speech_prob": 0.0015723605174571276}, {"id": 69, "seek": 46328, "start": 474.96, "end": 482.0, "text": " thousand for example and I think actually that's also because they have no beliefs desires or", "tokens": [50948, 4714, 337, 1365, 293, 286, 519, 767, 300, 311, 611, 570, 436, 362, 572, 13585, 18005, 420, 51300], "temperature": 0.0, "avg_logprob": -0.08425418636466883, "compression_ratio": 1.7751196172248804, "no_speech_prob": 0.0015723605174571276}, {"id": 70, "seek": 46328, "start": 482.0, "end": 491.44, "text": " intentions they have no you know guiding you know thoughts that's why hallucinations such a big", "tokens": [51300, 19354, 436, 362, 572, 291, 458, 25061, 291, 458, 4598, 300, 311, 983, 35212, 10325, 1270, 257, 955, 51772], "temperature": 0.0, "avg_logprob": -0.08425418636466883, "compression_ratio": 1.7751196172248804, "no_speech_prob": 0.0015723605174571276}, {"id": 71, "seek": 49144, "start": 491.44, "end": 499.12, "text": " problem you know they're just trying to produce plausible output I think it's really interesting", "tokens": [50364, 1154, 291, 458, 436, 434, 445, 1382, 281, 5258, 39925, 5598, 286, 519, 309, 311, 534, 1880, 50748], "temperature": 0.0, "avg_logprob": -0.10069294718952922, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.003117613960057497}, {"id": 72, "seek": 49144, "start": 499.12, "end": 505.04, "text": " to actually wonder about you know these large language models now becoming multimodal how will", "tokens": [50748, 281, 767, 2441, 466, 291, 458, 613, 2416, 2856, 5245, 586, 5617, 32972, 378, 304, 577, 486, 51044], "temperature": 0.0, "avg_logprob": -0.10069294718952922, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.003117613960057497}, {"id": 73, "seek": 49144, "start": 505.04, "end": 510.88, "text": " that change things I actually think if you think the fundamental problem is a problem of symbol", "tokens": [51044, 300, 1319, 721, 286, 767, 519, 498, 291, 519, 264, 8088, 1154, 307, 257, 1154, 295, 5986, 51336], "temperature": 0.0, "avg_logprob": -0.10069294718952922, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.003117613960057497}, {"id": 74, "seek": 49144, "start": 510.88, "end": 519.44, "text": " grounding then arguments like Searle's Chinese room argument still are just as valid against", "tokens": [51336, 46727, 550, 12869, 411, 1100, 36153, 311, 4649, 1808, 6770, 920, 366, 445, 382, 7363, 1970, 51764], "temperature": 0.0, "avg_logprob": -0.10069294718952922, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.003117613960057497}, {"id": 75, "seek": 51944, "start": 519.44, "end": 525.2, "text": " multimodal input on the other hand I actually think the multimodal inputs incredibly interesting", "tokens": [50364, 32972, 378, 304, 4846, 322, 264, 661, 1011, 286, 767, 519, 264, 32972, 378, 304, 15743, 6252, 1880, 50652], "temperature": 0.0, "avg_logprob": -0.09125212014439595, "compression_ratio": 1.7830188679245282, "no_speech_prob": 0.007227522786706686}, {"id": 76, "seek": 51944, "start": 525.2, "end": 531.0400000000001, "text": " might be incredibly powerful yes so naive questions what is multimodal training oh so here", "tokens": [50652, 1062, 312, 6252, 4005, 2086, 370, 29052, 1651, 437, 307, 32972, 378, 304, 3097, 1954, 370, 510, 50944], "temperature": 0.0, "avg_logprob": -0.09125212014439595, "compression_ratio": 1.7830188679245282, "no_speech_prob": 0.007227522786706686}, {"id": 77, "seek": 51944, "start": 531.0400000000001, "end": 536.72, "text": " what I really think is vision and language I should have made that clear vision and language", "tokens": [50944, 437, 286, 534, 519, 307, 5201, 293, 2856, 286, 820, 362, 1027, 300, 1850, 5201, 293, 2856, 51228], "temperature": 0.0, "avg_logprob": -0.09125212014439595, "compression_ratio": 1.7830188679245282, "no_speech_prob": 0.007227522786706686}, {"id": 78, "seek": 51944, "start": 538.1600000000001, "end": 545.36, "text": " but these days in fact people are now starting to train off movies as well right so that's I mean", "tokens": [51300, 457, 613, 1708, 294, 1186, 561, 366, 586, 2891, 281, 3847, 766, 6233, 382, 731, 558, 370, 300, 311, 286, 914, 51660], "temperature": 0.0, "avg_logprob": -0.09125212014439595, "compression_ratio": 1.7830188679245282, "no_speech_prob": 0.007227522786706686}, {"id": 79, "seek": 54536, "start": 545.36, "end": 551.76, "text": " arguably that is one big difference between these large language models and", "tokens": [50364, 26771, 300, 307, 472, 955, 2649, 1296, 613, 2416, 2856, 5245, 293, 50684], "temperature": 0.0, "avg_logprob": -0.1353788825700868, "compression_ratio": 1.4698795180722892, "no_speech_prob": 0.0011113439686596394}, {"id": 80, "seek": 54536, "start": 554.8000000000001, "end": 559.6, "text": " you know the way humans might work right and what is Searle's Chinese room", "tokens": [50836, 291, 458, 264, 636, 6255, 1062, 589, 558, 293, 437, 307, 1100, 36153, 311, 4649, 1808, 51076], "temperature": 0.0, "avg_logprob": -0.1353788825700868, "compression_ratio": 1.4698795180722892, "no_speech_prob": 0.0011113439686596394}, {"id": 81, "seek": 54536, "start": 561.2, "end": 568.48, "text": " that's a really interesting thought experiment which I think for reasons of time I might just", "tokens": [51156, 300, 311, 257, 534, 1880, 1194, 5120, 597, 286, 519, 337, 4112, 295, 565, 286, 1062, 445, 51520], "temperature": 0.0, "avg_logprob": -0.1353788825700868, "compression_ratio": 1.4698795180722892, "no_speech_prob": 0.0011113439686596394}, {"id": 82, "seek": 56848, "start": 568.5600000000001, "end": 576.88, "text": " skip here but it's a it's basically where he asks he asks us to imagine a blind symbol", "tokens": [50368, 10023, 510, 457, 309, 311, 257, 309, 311, 1936, 689, 415, 8962, 415, 8962, 505, 281, 3811, 257, 6865, 5986, 50784], "temperature": 0.0, "avg_logprob": -0.10802238688749426, "compression_ratio": 1.9086294416243654, "no_speech_prob": 0.044521164149045944}, {"id": 83, "seek": 56848, "start": 576.88, "end": 582.72, "text": " following system and he says so can you really do you really want to say that this blind symbol", "tokens": [50784, 3480, 1185, 293, 415, 1619, 370, 393, 291, 534, 360, 291, 534, 528, 281, 584, 300, 341, 6865, 5986, 51076], "temperature": 0.0, "avg_logprob": -0.10802238688749426, "compression_ratio": 1.9086294416243654, "no_speech_prob": 0.044521164149045944}, {"id": 84, "seek": 56848, "start": 582.72, "end": 588.88, "text": " for you know rule following system actually has any understanding it doesn't really seem to make", "tokens": [51076, 337, 291, 458, 4978, 3480, 1185, 767, 575, 604, 3701, 309, 1177, 380, 534, 1643, 281, 652, 51384], "temperature": 0.0, "avg_logprob": -0.10802238688749426, "compression_ratio": 1.9086294416243654, "no_speech_prob": 0.044521164149045944}, {"id": 85, "seek": 56848, "start": 588.88, "end": 598.08, "text": " sense to attribute any any understanding to it anyway I do actually think you know there's a lot", "tokens": [51384, 2020, 281, 19667, 604, 604, 3701, 281, 309, 4033, 286, 360, 767, 519, 291, 458, 456, 311, 257, 688, 51844], "temperature": 0.0, "avg_logprob": -0.10802238688749426, "compression_ratio": 1.9086294416243654, "no_speech_prob": 0.044521164149045944}, {"id": 86, "seek": 59808, "start": 598.08, "end": 603.44, "text": " of people now that are also wondering are these large language models are they really intelligent", "tokens": [50364, 295, 561, 586, 300, 366, 611, 6359, 366, 613, 2416, 2856, 5245, 366, 436, 534, 13232, 50632], "temperature": 0.0, "avg_logprob": -0.07788994226110987, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.0021109404042363167}, {"id": 87, "seek": 59808, "start": 604.1600000000001, "end": 611.5200000000001, "text": " could they be a AGI artificial general intelligence that maybe might take over the entire world", "tokens": [50668, 727, 436, 312, 257, 316, 26252, 11677, 2674, 7599, 300, 1310, 1062, 747, 670, 264, 2302, 1002, 51036], "temperature": 0.0, "avg_logprob": -0.07788994226110987, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.0021109404042363167}, {"id": 88, "seek": 59808, "start": 612.24, "end": 619.0400000000001, "text": " I think they currently can't because they do lack you know beliefs desires intentions the ability to", "tokens": [51072, 286, 519, 436, 4362, 393, 380, 570, 436, 360, 5011, 291, 458, 13585, 18005, 19354, 264, 3485, 281, 51412], "temperature": 0.0, "avg_logprob": -0.07788994226110987, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.0021109404042363167}, {"id": 89, "seek": 59808, "start": 619.0400000000001, "end": 625.44, "text": " form long-term memories for example but I'm I'm actually not so sure that that's such a huge", "tokens": [51412, 1254, 938, 12, 7039, 8495, 337, 1365, 457, 286, 478, 286, 478, 767, 406, 370, 988, 300, 300, 311, 1270, 257, 2603, 51732], "temperature": 0.0, "avg_logprob": -0.07788994226110987, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.0021109404042363167}, {"id": 90, "seek": 62544, "start": 625.5200000000001, "end": 633.2, "text": " technological barrier I actually do think that it's possible that that could be relatively easy", "tokens": [50368, 18439, 13357, 286, 767, 360, 519, 300, 309, 311, 1944, 300, 300, 727, 312, 7226, 1858, 50752], "temperature": 0.0, "avg_logprob": -0.05722755055094874, "compression_ratio": 1.7623318385650224, "no_speech_prob": 0.002713084453716874}, {"id": 91, "seek": 62544, "start": 633.2, "end": 639.5200000000001, "text": " now you know we've been in this situation before where we thought oh the thing that's really missing", "tokens": [50752, 586, 291, 458, 321, 600, 668, 294, 341, 2590, 949, 689, 321, 1194, 1954, 264, 551, 300, 311, 534, 5361, 51068], "temperature": 0.0, "avg_logprob": -0.05722755055094874, "compression_ratio": 1.7623318385650224, "no_speech_prob": 0.002713084453716874}, {"id": 92, "seek": 62544, "start": 639.5200000000001, "end": 645.5200000000001, "text": " from this machine being intelligent is x you add x to the machine and all of a sudden you discover", "tokens": [51068, 490, 341, 3479, 885, 13232, 307, 2031, 291, 909, 2031, 281, 264, 3479, 293, 439, 295, 257, 3990, 291, 4411, 51368], "temperature": 0.0, "avg_logprob": -0.05722755055094874, "compression_ratio": 1.7623318385650224, "no_speech_prob": 0.002713084453716874}, {"id": 93, "seek": 62544, "start": 645.5200000000001, "end": 651.0400000000001, "text": " well actually you know there's this other thing why that's missing as well and so that may happen", "tokens": [51368, 731, 767, 291, 458, 456, 311, 341, 661, 551, 983, 300, 311, 5361, 382, 731, 293, 370, 300, 815, 1051, 51644], "temperature": 0.0, "avg_logprob": -0.05722755055094874, "compression_ratio": 1.7623318385650224, "no_speech_prob": 0.002713084453716874}, {"id": 94, "seek": 65104, "start": 651.04, "end": 658.4, "text": " here too but there is a chance that in fact if we just added episodic memory to these large language", "tokens": [50364, 510, 886, 457, 456, 307, 257, 2931, 300, 294, 1186, 498, 321, 445, 3869, 39200, 299, 4675, 281, 613, 2416, 2856, 50732], "temperature": 0.0, "avg_logprob": -0.04390442812884295, "compression_ratio": 1.7972350230414746, "no_speech_prob": 0.002047288930043578}, {"id": 95, "seek": 65104, "start": 658.4, "end": 664.88, "text": " models then they may actually become intelligent things the other okay another high level point", "tokens": [50732, 5245, 550, 436, 815, 767, 1813, 13232, 721, 264, 661, 1392, 1071, 1090, 1496, 935, 51056], "temperature": 0.0, "avg_logprob": -0.04390442812884295, "compression_ratio": 1.7972350230414746, "no_speech_prob": 0.002047288930043578}, {"id": 96, "seek": 65104, "start": 664.88, "end": 671.68, "text": " that I want to make is that these large language models are manufactured systems in the same way", "tokens": [51056, 300, 286, 528, 281, 652, 307, 300, 613, 2416, 2856, 5245, 366, 25738, 3652, 294, 264, 912, 636, 51396], "temperature": 0.0, "avg_logprob": -0.04390442812884295, "compression_ratio": 1.7972350230414746, "no_speech_prob": 0.002047288930043578}, {"id": 97, "seek": 65104, "start": 671.68, "end": 676.56, "text": " remember I was talking about the first industrial revolution in the same way that a steam engine", "tokens": [51396, 1604, 286, 390, 1417, 466, 264, 700, 9987, 8894, 294, 264, 912, 636, 300, 257, 11952, 2848, 51640], "temperature": 0.0, "avg_logprob": -0.04390442812884295, "compression_ratio": 1.7972350230414746, "no_speech_prob": 0.002047288930043578}, {"id": 98, "seek": 67656, "start": 676.64, "end": 682.4, "text": " is a very highly engineered manufactured system you know you wouldn't want to be trying to", "tokens": [50368, 307, 257, 588, 5405, 38648, 25738, 1185, 291, 458, 291, 2759, 380, 528, 281, 312, 1382, 281, 50656], "temperature": 0.0, "avg_logprob": -0.07989848852157592, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.00636674789711833}, {"id": 99, "seek": 67656, "start": 683.04, "end": 689.04, "text": " infer the laws of thermodynamics or even the ideal gas laws by examining a steam engine", "tokens": [50688, 13596, 264, 6064, 295, 8810, 35483, 420, 754, 264, 7157, 4211, 6064, 538, 34662, 257, 11952, 2848, 50988], "temperature": 0.0, "avg_logprob": -0.07989848852157592, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.00636674789711833}, {"id": 100, "seek": 67656, "start": 689.76, "end": 696.0799999999999, "text": " I think the same thing's true also about these large language models you know they're they're", "tokens": [51024, 286, 519, 264, 912, 551, 311, 2074, 611, 466, 613, 2416, 2856, 5245, 291, 458, 436, 434, 436, 434, 51340], "temperature": 0.0, "avg_logprob": -0.07989848852157592, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.00636674789711833}, {"id": 101, "seek": 67656, "start": 696.0799999999999, "end": 703.52, "text": " trained in a way which is really an engineered product you know and certainly as somebody being", "tokens": [51340, 8895, 294, 257, 636, 597, 307, 534, 364, 38648, 1674, 291, 458, 293, 3297, 382, 2618, 885, 51712], "temperature": 0.0, "avg_logprob": -0.07989848852157592, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.00636674789711833}, {"id": 102, "seek": 70352, "start": 703.52, "end": 710.16, "text": " in industry I can tell you that you know the people in industry that are building these things", "tokens": [50364, 294, 3518, 286, 393, 980, 291, 300, 291, 458, 264, 561, 294, 3518, 300, 366, 2390, 613, 721, 50696], "temperature": 0.0, "avg_logprob": -0.05347136143715151, "compression_ratio": 1.759493670886076, "no_speech_prob": 0.0044595771469175816}, {"id": 103, "seek": 70352, "start": 710.16, "end": 714.56, "text": " really don't care about doing science they're trying to build a product that they can sell", "tokens": [50696, 534, 500, 380, 1127, 466, 884, 3497, 436, 434, 1382, 281, 1322, 257, 1674, 300, 436, 393, 3607, 50916], "temperature": 0.0, "avg_logprob": -0.05347136143715151, "compression_ratio": 1.759493670886076, "no_speech_prob": 0.0044595771469175816}, {"id": 104, "seek": 70352, "start": 714.56, "end": 726.24, "text": " and that's really what they care about okay so just an aside about you know the relationship", "tokens": [50916, 293, 300, 311, 534, 437, 436, 1127, 466, 1392, 370, 445, 364, 7359, 466, 291, 458, 264, 2480, 51500], "temperature": 0.0, "avg_logprob": -0.05347136143715151, "compression_ratio": 1.759493670886076, "no_speech_prob": 0.0044595771469175816}, {"id": 105, "seek": 72624, "start": 726.32, "end": 736.08, "text": " between I I actually think a natural language understanding and say you know the science of", "tokens": [50368, 1296, 286, 286, 767, 519, 257, 3303, 2856, 3701, 293, 584, 291, 458, 264, 3497, 295, 50856], "temperature": 0.0, "avg_logprob": -0.06818283081054688, "compression_ratio": 1.8, "no_speech_prob": 0.03353704512119293}, {"id": 106, "seek": 72624, "start": 736.08, "end": 743.36, "text": " of linguistics or language acquisition so there's the scientific side and the technological side", "tokens": [50856, 295, 21766, 6006, 420, 2856, 21668, 370, 456, 311, 264, 8134, 1252, 293, 264, 18439, 1252, 51220], "temperature": 0.0, "avg_logprob": -0.06818283081054688, "compression_ratio": 1.8, "no_speech_prob": 0.03353704512119293}, {"id": 107, "seek": 72624, "start": 743.36, "end": 749.28, "text": " I think the technology is currently outstripping the science and I think that has happened in", "tokens": [51220, 286, 519, 264, 2899, 307, 4362, 484, 372, 470, 3759, 264, 3497, 293, 286, 519, 300, 575, 2011, 294, 51516], "temperature": 0.0, "avg_logprob": -0.06818283081054688, "compression_ratio": 1.8, "no_speech_prob": 0.03353704512119293}, {"id": 108, "seek": 72624, "start": 749.28, "end": 753.44, "text": " times before in fact I think it actually happened with the first industrial revolution", "tokens": [51516, 1413, 949, 294, 1186, 286, 519, 309, 767, 2011, 365, 264, 700, 9987, 8894, 51724], "temperature": 0.0, "avg_logprob": -0.06818283081054688, "compression_ratio": 1.8, "no_speech_prob": 0.03353704512119293}, {"id": 109, "seek": 75344, "start": 753.44, "end": 760.72, "text": " so I won't spend too much time here but my understanding is and maybe there's people", "tokens": [50364, 370, 286, 1582, 380, 3496, 886, 709, 565, 510, 457, 452, 3701, 307, 293, 1310, 456, 311, 561, 50728], "temperature": 0.0, "avg_logprob": -0.09820141111101423, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.00026937638176605105}, {"id": 110, "seek": 75344, "start": 761.2800000000001, "end": 768.08, "text": " historians of science in this room that know more about this than I do but that you know", "tokens": [50756, 26442, 295, 3497, 294, 341, 1808, 300, 458, 544, 466, 341, 813, 286, 360, 457, 300, 291, 458, 51096], "temperature": 0.0, "avg_logprob": -0.09820141111101423, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.00026937638176605105}, {"id": 111, "seek": 75344, "start": 768.08, "end": 774.0, "text": " really in fact actually people started to invent steam engines long before they had the scientific", "tokens": [51096, 534, 294, 1186, 767, 561, 1409, 281, 7962, 11952, 12982, 938, 949, 436, 632, 264, 8134, 51392], "temperature": 0.0, "avg_logprob": -0.09820141111101423, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.00026937638176605105}, {"id": 112, "seek": 75344, "start": 774.0, "end": 780.72, "text": " understanding first of all of ideal gas laws and then ultimately right through thermodynamics", "tokens": [51392, 3701, 700, 295, 439, 295, 7157, 4211, 6064, 293, 550, 6284, 558, 807, 8810, 35483, 51728], "temperature": 0.0, "avg_logprob": -0.09820141111101423, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.00026937638176605105}, {"id": 113, "seek": 78072, "start": 780.72, "end": 787.36, "text": " and statistical mechanics that took centuries to the scientific for the scientific side to", "tokens": [50364, 293, 22820, 12939, 300, 1890, 13926, 281, 264, 8134, 337, 264, 8134, 1252, 281, 50696], "temperature": 0.0, "avg_logprob": -0.06955780029296875, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.0018068557837978005}, {"id": 114, "seek": 78072, "start": 788.08, "end": 794.48, "text": " emerge and in fact actually ideas of things like entropy really actually had to be developed", "tokens": [50732, 21511, 293, 294, 1186, 767, 3487, 295, 721, 411, 30867, 534, 767, 632, 281, 312, 4743, 51052], "temperature": 0.0, "avg_logprob": -0.06955780029296875, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.0018068557837978005}, {"id": 115, "seek": 78072, "start": 795.28, "end": 801.6800000000001, "text": " to answer questions about why it was not possible to build steam engines above a certain level of", "tokens": [51092, 281, 1867, 1651, 466, 983, 309, 390, 406, 1944, 281, 1322, 11952, 12982, 3673, 257, 1629, 1496, 295, 51412], "temperature": 0.0, "avg_logprob": -0.06955780029296875, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.0018068557837978005}, {"id": 116, "seek": 78072, "start": 801.6800000000001, "end": 808.96, "text": " efficiency for example and I suspect the same thing may be true today that our science of", "tokens": [51412, 10493, 337, 1365, 293, 286, 9091, 264, 912, 551, 815, 312, 2074, 965, 300, 527, 3497, 295, 51776], "temperature": 0.0, "avg_logprob": -0.06955780029296875, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.0018068557837978005}, {"id": 117, "seek": 80896, "start": 809.0400000000001, "end": 820.1600000000001, "text": " say language and psychology is actually behind the technology. Okay one of the things that I", "tokens": [50368, 584, 2856, 293, 15105, 307, 767, 2261, 264, 2899, 13, 1033, 472, 295, 264, 721, 300, 286, 50924], "temperature": 0.0, "avg_logprob": -0.1424940324598743, "compression_ratio": 1.580110497237569, "no_speech_prob": 0.00045037458767183125}, {"id": 118, "seek": 80896, "start": 820.1600000000001, "end": 825.52, "text": " actually quite like is you know this comment here natural language is the new programming language", "tokens": [50924, 767, 1596, 411, 307, 291, 458, 341, 2871, 510, 3303, 2856, 307, 264, 777, 9410, 2856, 51192], "temperature": 0.0, "avg_logprob": -0.1424940324598743, "compression_ratio": 1.580110497237569, "no_speech_prob": 0.00045037458767183125}, {"id": 119, "seek": 80896, "start": 827.76, "end": 835.2800000000001, "text": " and that yeah I mean certainly for those of us in industry LLMs are really changing the way in", "tokens": [51304, 293, 300, 1338, 286, 914, 3297, 337, 729, 295, 505, 294, 3518, 441, 43, 26386, 366, 534, 4473, 264, 636, 294, 51680], "temperature": 0.0, "avg_logprob": -0.1424940324598743, "compression_ratio": 1.580110497237569, "no_speech_prob": 0.00045037458767183125}, {"id": 120, "seek": 83528, "start": 835.28, "end": 844.56, "text": " which we do our work right whereas it used to take a team of real experts to build say for", "tokens": [50364, 597, 321, 360, 527, 589, 558, 9735, 309, 1143, 281, 747, 257, 1469, 295, 957, 8572, 281, 1322, 584, 337, 50828], "temperature": 0.0, "avg_logprob": -0.07272778118357939, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.0045260898768901825}, {"id": 121, "seek": 83528, "start": 844.56, "end": 849.68, "text": " example a device which would identify all the financial products that are mentioned", "tokens": [50828, 1365, 257, 4302, 597, 576, 5876, 439, 264, 4669, 3383, 300, 366, 2835, 51084], "temperature": 0.0, "avg_logprob": -0.07272778118357939, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.0045260898768901825}, {"id": 122, "seek": 83528, "start": 850.8, "end": 856.48, "text": " you know in a particular document. Now you can just simply ask a large language model to do that", "tokens": [51140, 291, 458, 294, 257, 1729, 4166, 13, 823, 291, 393, 445, 2935, 1029, 257, 2416, 2856, 2316, 281, 360, 300, 51424], "temperature": 0.0, "avg_logprob": -0.07272778118357939, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.0045260898768901825}, {"id": 123, "seek": 83528, "start": 856.48, "end": 864.0799999999999, "text": " for you and it does a pretty good job maybe not as good as the very best hand-built natural language", "tokens": [51424, 337, 291, 293, 309, 775, 257, 1238, 665, 1691, 1310, 406, 382, 665, 382, 264, 588, 1151, 1011, 12, 23018, 3303, 2856, 51804], "temperature": 0.0, "avg_logprob": -0.07272778118357939, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.0045260898768901825}, {"id": 124, "seek": 86408, "start": 864.08, "end": 870.72, "text": " processing system so those are still actually better but you know they take months or years to", "tokens": [50364, 9007, 1185, 370, 729, 366, 920, 767, 1101, 457, 291, 458, 436, 747, 2493, 420, 924, 281, 50696], "temperature": 0.0, "avg_logprob": -0.07381829848656288, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.001320247771218419}, {"id": 125, "seek": 86408, "start": 870.72, "end": 879.44, "text": " develop whereas it takes you know maybe hours to use a new large language model so I actually think", "tokens": [50696, 1499, 9735, 309, 2516, 291, 458, 1310, 2496, 281, 764, 257, 777, 2416, 2856, 2316, 370, 286, 767, 519, 51132], "temperature": 0.0, "avg_logprob": -0.07381829848656288, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.001320247771218419}, {"id": 126, "seek": 86408, "start": 879.44, "end": 884.64, "text": " that particularly in terms of the commercial implications the commercial deployment of natural", "tokens": [51132, 300, 4098, 294, 2115, 295, 264, 6841, 16602, 264, 6841, 19317, 295, 3303, 51392], "temperature": 0.0, "avg_logprob": -0.07381829848656288, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.001320247771218419}, {"id": 127, "seek": 86408, "start": 884.64, "end": 892.08, "text": " language processing in industry that's going to change completely. It's not clear we'll need nearly", "tokens": [51392, 2856, 9007, 294, 3518, 300, 311, 516, 281, 1319, 2584, 13, 467, 311, 406, 1850, 321, 603, 643, 6217, 51764], "temperature": 0.0, "avg_logprob": -0.07381829848656288, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.001320247771218419}, {"id": 128, "seek": 89208, "start": 892.08, "end": 899.36, "text": " as many experts in natural language understanding for example for the industrial applications.", "tokens": [50364, 382, 867, 8572, 294, 3303, 2856, 3701, 337, 1365, 337, 264, 9987, 5821, 13, 50728], "temperature": 0.0, "avg_logprob": -0.04476242926385668, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.0003237810160499066}, {"id": 129, "seek": 89208, "start": 901.76, "end": 908.0, "text": " I did want to mention a little bit you know I think that one of the really interesting", "tokens": [50848, 286, 630, 528, 281, 2152, 257, 707, 857, 291, 458, 286, 519, 300, 472, 295, 264, 534, 1880, 51160], "temperature": 0.0, "avg_logprob": -0.04476242926385668, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.0003237810160499066}, {"id": 130, "seek": 89208, "start": 909.5200000000001, "end": 913.76, "text": " things that's happening in the field is taking these large language models", "tokens": [51236, 721, 300, 311, 2737, 294, 264, 2519, 307, 1940, 613, 2416, 2856, 5245, 51448], "temperature": 0.0, "avg_logprob": -0.04476242926385668, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.0003237810160499066}, {"id": 131, "seek": 89208, "start": 913.76, "end": 921.76, "text": " and then combining them with other components. The first component that people started to look at", "tokens": [51448, 293, 550, 21928, 552, 365, 661, 6677, 13, 440, 700, 6542, 300, 561, 1409, 281, 574, 412, 51848], "temperature": 0.0, "avg_logprob": -0.04476242926385668, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.0003237810160499066}, {"id": 132, "seek": 92176, "start": 921.76, "end": 928.16, "text": " was combining large language models with what's called a vector store or a retrieval system", "tokens": [50364, 390, 21928, 2416, 2856, 5245, 365, 437, 311, 1219, 257, 8062, 3531, 420, 257, 19817, 3337, 1185, 50684], "temperature": 0.0, "avg_logprob": -0.050288215468201454, "compression_ratio": 1.87, "no_speech_prob": 0.0005787700065411627}, {"id": 133, "seek": 92176, "start": 928.88, "end": 936.88, "text": " and that's just simply something whereby when you ask a question instead of just directly asking", "tokens": [50720, 293, 300, 311, 445, 2935, 746, 36998, 562, 291, 1029, 257, 1168, 2602, 295, 445, 3838, 3365, 51120], "temperature": 0.0, "avg_logprob": -0.050288215468201454, "compression_ratio": 1.87, "no_speech_prob": 0.0005787700065411627}, {"id": 134, "seek": 92176, "start": 936.88, "end": 944.24, "text": " the large language model to respond to that question you retrieve a set of relevant documents", "tokens": [51120, 264, 2416, 2856, 2316, 281, 4196, 281, 300, 1168, 291, 30254, 257, 992, 295, 7340, 8512, 51488], "temperature": 0.0, "avg_logprob": -0.050288215468201454, "compression_ratio": 1.87, "no_speech_prob": 0.0005787700065411627}, {"id": 135, "seek": 92176, "start": 944.24, "end": 949.52, "text": " to that question feed those in as part of the input to the large language model you can see", "tokens": [51488, 281, 300, 1168, 3154, 729, 294, 382, 644, 295, 264, 4846, 281, 264, 2416, 2856, 2316, 291, 393, 536, 51752], "temperature": 0.0, "avg_logprob": -0.050288215468201454, "compression_ratio": 1.87, "no_speech_prob": 0.0005787700065411627}, {"id": 136, "seek": 94952, "start": 949.52, "end": 954.4, "text": " that's what I'm suggesting that you do over here and then you then tell the large language model", "tokens": [50364, 300, 311, 437, 286, 478, 18094, 300, 291, 360, 670, 510, 293, 550, 291, 550, 980, 264, 2416, 2856, 2316, 50608], "temperature": 0.0, "avg_logprob": -0.06688050870542173, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.0008421398815698922}, {"id": 137, "seek": 94952, "start": 954.4, "end": 962.3199999999999, "text": " to use to produce an answer that just simply references those documents and that's sometimes", "tokens": [50608, 281, 764, 281, 5258, 364, 1867, 300, 445, 2935, 15400, 729, 8512, 293, 300, 311, 2171, 51004], "temperature": 0.0, "avg_logprob": -0.06688050870542173, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.0008421398815698922}, {"id": 138, "seek": 94952, "start": 962.3199999999999, "end": 972.88, "text": " called the reader retrieval model or retrieval augmented generation. That idea can get even", "tokens": [51004, 1219, 264, 15149, 19817, 3337, 2316, 420, 19817, 3337, 36155, 5125, 13, 663, 1558, 393, 483, 754, 51532], "temperature": 0.0, "avg_logprob": -0.06688050870542173, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.0008421398815698922}, {"id": 139, "seek": 94952, "start": 972.88, "end": 978.16, "text": " more power when you start to think well maybe in fact the large language model can actually decide", "tokens": [51532, 544, 1347, 562, 291, 722, 281, 519, 731, 1310, 294, 1186, 264, 2416, 2856, 2316, 393, 767, 4536, 51796], "temperature": 0.0, "avg_logprob": -0.06688050870542173, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.0008421398815698922}, {"id": 140, "seek": 97816, "start": 979.12, "end": 984.56, "text": " what information to do a search for and then when you then started to think well should it", "tokens": [50412, 437, 1589, 281, 360, 257, 3164, 337, 293, 550, 562, 291, 550, 1409, 281, 519, 731, 820, 309, 50684], "temperature": 0.0, "avg_logprob": -0.07832502637590681, "compression_ratio": 1.9581589958158996, "no_speech_prob": 0.0015221864450722933}, {"id": 141, "seek": 97816, "start": 984.56, "end": 989.4399999999999, "text": " decide what information to do a search for maybe in fact it could also call other tools", "tokens": [50684, 4536, 437, 1589, 281, 360, 257, 3164, 337, 1310, 294, 1186, 309, 727, 611, 818, 661, 3873, 50928], "temperature": 0.0, "avg_logprob": -0.07832502637590681, "compression_ratio": 1.9581589958158996, "no_speech_prob": 0.0015221864450722933}, {"id": 142, "seek": 97816, "start": 990.0799999999999, "end": 995.6, "text": " so these large language models are infamous for not being able to do numerical calculations very", "tokens": [50960, 370, 613, 2416, 2856, 5245, 366, 30769, 337, 406, 885, 1075, 281, 360, 29054, 20448, 588, 51236], "temperature": 0.0, "avg_logprob": -0.07832502637590681, "compression_ratio": 1.9581589958158996, "no_speech_prob": 0.0015221864450722933}, {"id": 143, "seek": 97816, "start": 995.6, "end": 1000.9599999999999, "text": " well but maybe in fact what we should be doing is giving the large language model the ability", "tokens": [51236, 731, 457, 1310, 294, 1186, 437, 321, 820, 312, 884, 307, 2902, 264, 2416, 2856, 2316, 264, 3485, 51504], "temperature": 0.0, "avg_logprob": -0.07832502637590681, "compression_ratio": 1.9581589958158996, "no_speech_prob": 0.0015221864450722933}, {"id": 144, "seek": 97816, "start": 1000.9599999999999, "end": 1007.04, "text": " to call a calculator and there's just in the same ways which if I was to ask any of you guys to do", "tokens": [51504, 281, 818, 257, 24993, 293, 456, 311, 445, 294, 264, 912, 2098, 597, 498, 286, 390, 281, 1029, 604, 295, 291, 1074, 281, 360, 51808], "temperature": 0.0, "avg_logprob": -0.07832502637590681, "compression_ratio": 1.9581589958158996, "no_speech_prob": 0.0015221864450722933}, {"id": 145, "seek": 100704, "start": 1007.12, "end": 1012.88, "text": " a complex task and involve something some numeric calculation I'd want you to be", "tokens": [50368, 257, 3997, 5633, 293, 9494, 746, 512, 7866, 299, 17108, 286, 1116, 528, 291, 281, 312, 50656], "temperature": 0.0, "avg_logprob": -0.07802993910653251, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.0007427732925862074}, {"id": 146, "seek": 100704, "start": 1013.92, "end": 1020.4, "text": " also using a calculator rather than trying to do it longhand. Okay so in terms of research", "tokens": [50708, 611, 1228, 257, 24993, 2831, 813, 1382, 281, 360, 309, 938, 5543, 13, 1033, 370, 294, 2115, 295, 2132, 51032], "temperature": 0.0, "avg_logprob": -0.07802993910653251, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.0007427732925862074}, {"id": 147, "seek": 100704, "start": 1020.4, "end": 1028.08, "text": " directions inside of an LLM world so the very first comment to make is that it is very challenging", "tokens": [51032, 11095, 1854, 295, 364, 441, 43, 44, 1002, 370, 264, 588, 700, 2871, 281, 652, 307, 300, 309, 307, 588, 7595, 51416], "temperature": 0.0, "avg_logprob": -0.07802993910653251, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.0007427732925862074}, {"id": 148, "seek": 100704, "start": 1028.08, "end": 1034.24, "text": " for academics to do research in large language models you know the ideal thing would be to have", "tokens": [51416, 337, 25695, 281, 360, 2132, 294, 2416, 2856, 5245, 291, 458, 264, 7157, 551, 576, 312, 281, 362, 51724], "temperature": 0.0, "avg_logprob": -0.07802993910653251, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.0007427732925862074}, {"id": 149, "seek": 103424, "start": 1034.24, "end": 1042.4, "text": " something like an ideal gas experiment set up but and you certainly can build small", "tokens": [50364, 746, 411, 364, 7157, 4211, 5120, 992, 493, 457, 293, 291, 3297, 393, 1322, 1359, 50772], "temperature": 0.0, "avg_logprob": -0.10028420267878352, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0009088012739084661}, {"id": 150, "seek": 103424, "start": 1043.2, "end": 1049.84, "text": " versions of these large language models there's some disagreement about whether or not though", "tokens": [50812, 9606, 295, 613, 2416, 2856, 5245, 456, 311, 512, 38947, 466, 1968, 420, 406, 1673, 51144], "temperature": 0.0, "avg_logprob": -0.10028420267878352, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0009088012739084661}, {"id": 151, "seek": 103424, "start": 1049.84, "end": 1056.88, "text": " whether there's well there certainly seem to be emergent capabilities so the bigger the model", "tokens": [51144, 1968, 456, 311, 731, 456, 3297, 1643, 281, 312, 4345, 6930, 10862, 370, 264, 3801, 264, 2316, 51496], "temperature": 0.0, "avg_logprob": -0.10028420267878352, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0009088012739084661}, {"id": 152, "seek": 103424, "start": 1056.88, "end": 1062.24, "text": " the more things that it can do there's big arguments about whether or not this emergence", "tokens": [51496, 264, 544, 721, 300, 309, 393, 360, 456, 311, 955, 12869, 466, 1968, 420, 406, 341, 36211, 51764], "temperature": 0.0, "avg_logprob": -0.10028420267878352, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0009088012739084661}, {"id": 153, "seek": 106224, "start": 1062.32, "end": 1072.56, "text": " is like a phase change or whether it's really more incremental and I again I'd be happy to talk", "tokens": [50368, 307, 411, 257, 5574, 1319, 420, 1968, 309, 311, 534, 544, 35759, 293, 286, 797, 286, 1116, 312, 2055, 281, 751, 50880], "temperature": 0.0, "avg_logprob": -0.09372168034315109, "compression_ratio": 1.553072625698324, "no_speech_prob": 0.0014752799179404974}, {"id": 154, "seek": 106224, "start": 1072.56, "end": 1079.44, "text": " about that later we could spend hours talking a bit about that but I think there's enough", "tokens": [50880, 466, 300, 1780, 321, 727, 3496, 2496, 1417, 257, 857, 466, 300, 457, 286, 519, 456, 311, 1547, 51224], "temperature": 0.0, "avg_logprob": -0.09372168034315109, "compression_ratio": 1.553072625698324, "no_speech_prob": 0.0014752799179404974}, {"id": 155, "seek": 106224, "start": 1080.32, "end": 1086.16, "text": " lack of clarity about what emergence is but if you wanted to do academic research you really", "tokens": [51268, 5011, 295, 16992, 466, 437, 36211, 307, 457, 498, 291, 1415, 281, 360, 7778, 2132, 291, 534, 51560], "temperature": 0.0, "avg_logprob": -0.09372168034315109, "compression_ratio": 1.553072625698324, "no_speech_prob": 0.0014752799179404974}, {"id": 156, "seek": 108616, "start": 1086.16, "end": 1092.0800000000002, "text": " do want to get access to you know the large language the larger large language models", "tokens": [50364, 360, 528, 281, 483, 2105, 281, 291, 458, 264, 2416, 2856, 264, 4833, 2416, 2856, 5245, 50660], "temperature": 0.0, "avg_logprob": -0.07093417966688001, "compression_ratio": 1.914438502673797, "no_speech_prob": 0.01824188232421875}, {"id": 157, "seek": 108616, "start": 1092.8000000000002, "end": 1097.68, "text": " and the problem is that these you know the best large language models are really complicated", "tokens": [50696, 293, 264, 1154, 307, 300, 613, 291, 458, 264, 1151, 2416, 2856, 5245, 366, 534, 6179, 50940], "temperature": 0.0, "avg_logprob": -0.07093417966688001, "compression_ratio": 1.914438502673797, "no_speech_prob": 0.01824188232421875}, {"id": 158, "seek": 108616, "start": 1097.68, "end": 1106.0, "text": " commercial products as we were talking about before and it's what's actually even worse", "tokens": [50940, 6841, 3383, 382, 321, 645, 1417, 466, 949, 293, 309, 311, 437, 311, 767, 754, 5324, 51356], "temperature": 0.0, "avg_logprob": -0.07093417966688001, "compression_ratio": 1.914438502673797, "no_speech_prob": 0.01824188232421875}, {"id": 159, "seek": 108616, "start": 1106.0, "end": 1112.8000000000002, "text": " is that these days for in you know proprietary commercial reasons the companies aren't even", "tokens": [51356, 307, 300, 613, 1708, 337, 294, 291, 458, 38992, 6841, 4112, 264, 3431, 3212, 380, 754, 51696], "temperature": 0.0, "avg_logprob": -0.07093417966688001, "compression_ratio": 1.914438502673797, "no_speech_prob": 0.01824188232421875}, {"id": 160, "seek": 111280, "start": 1112.8, "end": 1118.1599999999999, "text": " actually telling us all the details of exactly what they're doing so that actually does make", "tokens": [50364, 767, 3585, 505, 439, 264, 4365, 295, 2293, 437, 436, 434, 884, 370, 300, 767, 775, 652, 50632], "temperature": 0.0, "avg_logprob": -0.05492026805877685, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.0012433669762685895}, {"id": 161, "seek": 111280, "start": 1118.1599999999999, "end": 1125.2, "text": " it very hard for academics to really do any sort of academic research in in our own papers", "tokens": [50632, 309, 588, 1152, 337, 25695, 281, 534, 360, 604, 1333, 295, 7778, 2132, 294, 294, 527, 1065, 10577, 50984], "temperature": 0.0, "avg_logprob": -0.05492026805877685, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.0012433669762685895}, {"id": 162, "seek": 111280, "start": 1125.2, "end": 1131.68, "text": " you know I'm collaborating with people at the University of Edinburgh what we wind up doing", "tokens": [50984, 291, 458, 286, 478, 30188, 365, 561, 412, 264, 3535, 295, 41215, 437, 321, 2468, 493, 884, 51308], "temperature": 0.0, "avg_logprob": -0.05492026805877685, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.0012433669762685895}, {"id": 163, "seek": 111280, "start": 1131.68, "end": 1141.36, "text": " is saying we're actually not going to test the closed commercial systems but we will work with", "tokens": [51308, 307, 1566, 321, 434, 767, 406, 516, 281, 1500, 264, 5395, 6841, 3652, 457, 321, 486, 589, 365, 51792], "temperature": 0.0, "avg_logprob": -0.05492026805877685, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.0012433669762685895}, {"id": 164, "seek": 114136, "start": 1141.36, "end": 1147.76, "text": " the largest open source systems that are available I think that's not a bad thing to do but it does", "tokens": [50364, 264, 6443, 1269, 4009, 3652, 300, 366, 2435, 286, 519, 300, 311, 406, 257, 1578, 551, 281, 360, 457, 309, 775, 50684], "temperature": 0.0, "avg_logprob": -0.09856203377964985, "compression_ratio": 1.7077625570776256, "no_speech_prob": 0.003366272896528244}, {"id": 165, "seek": 114136, "start": 1147.76, "end": 1153.84, "text": " mean that you're cutting yourself off from a lot of the really cool systems there yes", "tokens": [50684, 914, 300, 291, 434, 6492, 1803, 766, 490, 257, 688, 295, 264, 534, 1627, 3652, 456, 2086, 50988], "temperature": 0.0, "avg_logprob": -0.09856203377964985, "compression_ratio": 1.7077625570776256, "no_speech_prob": 0.003366272896528244}, {"id": 166, "seek": 114136, "start": 1154.8799999999999, "end": 1161.12, "text": " this is a very interesting question on that slide about how quickly they degrade as you move just", "tokens": [51040, 341, 307, 257, 588, 1880, 1168, 322, 300, 4137, 466, 577, 2661, 436, 368, 8692, 382, 291, 1286, 445, 51352], "temperature": 0.0, "avg_logprob": -0.09856203377964985, "compression_ratio": 1.7077625570776256, "no_speech_prob": 0.003366272896528244}, {"id": 167, "seek": 114136, "start": 1161.12, "end": 1167.12, "text": " as we moved from commercial ones to smaller and smaller ones yes just a steady degradation", "tokens": [51352, 382, 321, 4259, 490, 6841, 2306, 281, 4356, 293, 4356, 2306, 2086, 445, 257, 13211, 40519, 51652], "temperature": 0.0, "avg_logprob": -0.09856203377964985, "compression_ratio": 1.7077625570776256, "no_speech_prob": 0.003366272896528244}, {"id": 168, "seek": 116712, "start": 1167.1999999999998, "end": 1174.0, "text": " or is there a sudden drop so sorry and in fact actually this also gets back to the emergence", "tokens": [50368, 420, 307, 456, 257, 3990, 3270, 370, 2597, 293, 294, 1186, 767, 341, 611, 2170, 646, 281, 264, 36211, 50708], "temperature": 0.0, "avg_logprob": -0.10506346059399982, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.007672835141420364}, {"id": 169, "seek": 116712, "start": 1174.0, "end": 1180.3999999999999, "text": " question so let me just say I actually do think that a lot depends on exactly how you measure it", "tokens": [50708, 1168, 370, 718, 385, 445, 584, 286, 767, 360, 519, 300, 257, 688, 5946, 322, 2293, 577, 291, 3481, 309, 51028], "temperature": 0.0, "avg_logprob": -0.10506346059399982, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.007672835141420364}, {"id": 170, "seek": 116712, "start": 1181.04, "end": 1189.6, "text": " so I don't have to tell people particularly at the Santa Fe Institute right that quite often", "tokens": [51060, 370, 286, 500, 380, 362, 281, 980, 561, 4098, 412, 264, 9933, 3697, 9446, 558, 300, 1596, 2049, 51488], "temperature": 0.0, "avg_logprob": -0.10506346059399982, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.007672835141420364}, {"id": 171, "seek": 116712, "start": 1189.6, "end": 1195.12, "text": " what you'll actually see is a lot of small changes all of a sudden reaching a tipping point", "tokens": [51488, 437, 291, 603, 767, 536, 307, 257, 688, 295, 1359, 2962, 439, 295, 257, 3990, 9906, 257, 41625, 935, 51764], "temperature": 0.0, "avg_logprob": -0.10506346059399982, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.007672835141420364}, {"id": 172, "seek": 119512, "start": 1195.12, "end": 1201.28, "text": " that is basically like a phase change and you know when you think about these large language models", "tokens": [50364, 300, 307, 1936, 411, 257, 5574, 1319, 293, 291, 458, 562, 291, 519, 466, 613, 2416, 2856, 5245, 50672], "temperature": 0.0, "avg_logprob": -0.07359921214092209, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.0018049998907372355}, {"id": 173, "seek": 119512, "start": 1201.28, "end": 1208.3999999999999, "text": " I mean they generate output token by token if the correct answer is just slightly less probable", "tokens": [50672, 286, 914, 436, 8460, 5598, 14862, 538, 14862, 498, 264, 3006, 1867, 307, 445, 4748, 1570, 21759, 51028], "temperature": 0.0, "avg_logprob": -0.07359921214092209, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.0018049998907372355}, {"id": 174, "seek": 119512, "start": 1208.3999999999999, "end": 1214.56, "text": " than some mistake right well then as the output gets very very long then the correct answer can be", "tokens": [51028, 813, 512, 6146, 558, 731, 550, 382, 264, 5598, 2170, 588, 588, 938, 550, 264, 3006, 1867, 393, 312, 51336], "temperature": 0.0, "avg_logprob": -0.07359921214092209, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.0018049998907372355}, {"id": 175, "seek": 119512, "start": 1214.56, "end": 1221.9199999999998, "text": " incredibly improbable right so if you're just looking at the output of 100 tokens or more", "tokens": [51336, 6252, 2530, 65, 712, 558, 370, 498, 291, 434, 445, 1237, 412, 264, 5598, 295, 2319, 22667, 420, 544, 51704], "temperature": 0.0, "avg_logprob": -0.07359921214092209, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.0018049998907372355}, {"id": 176, "seek": 122192, "start": 1222.88, "end": 1226.48, "text": " you're just looking at the output you're just asking is the output right or wrong you'll go", "tokens": [50412, 291, 434, 445, 1237, 412, 264, 5598, 291, 434, 445, 3365, 307, 264, 5598, 558, 420, 2085, 291, 603, 352, 50592], "temperature": 0.0, "avg_logprob": -0.08266454083578927, "compression_ratio": 2.0944444444444446, "no_speech_prob": 0.001921652932651341}, {"id": 177, "seek": 122192, "start": 1226.48, "end": 1233.3600000000001, "text": " wrong wrong wrong wrong wrong right and then all of a sudden as the correct token probability", "tokens": [50592, 2085, 2085, 2085, 2085, 2085, 558, 293, 550, 439, 295, 257, 3990, 382, 264, 3006, 14862, 8482, 50936], "temperature": 0.0, "avg_logprob": -0.08266454083578927, "compression_ratio": 2.0944444444444446, "no_speech_prob": 0.001921652932651341}, {"id": 178, "seek": 122192, "start": 1233.3600000000001, "end": 1240.5600000000002, "text": " just nudges above the incorrect tokens you know all of a sudden the output flips and all of a", "tokens": [50936, 445, 40045, 2880, 3673, 264, 18424, 22667, 291, 458, 439, 295, 257, 3990, 264, 5598, 40249, 293, 439, 295, 257, 51296], "temperature": 0.0, "avg_logprob": -0.08266454083578927, "compression_ratio": 2.0944444444444446, "no_speech_prob": 0.001921652932651341}, {"id": 179, "seek": 122192, "start": 1240.5600000000002, "end": 1247.2, "text": " sudden it's just magically all correct but if you measure the per token probabilities for example", "tokens": [51296, 3990, 309, 311, 445, 39763, 439, 3006, 457, 498, 291, 3481, 264, 680, 14862, 33783, 337, 1365, 51628], "temperature": 0.0, "avg_logprob": -0.08266454083578927, "compression_ratio": 2.0944444444444446, "no_speech_prob": 0.001921652932651341}, {"id": 180, "seek": 124720, "start": 1247.28, "end": 1250.16, "text": " then you'd actually discover a much more continuous change", "tokens": [50368, 550, 291, 1116, 767, 4411, 257, 709, 544, 10957, 1319, 50512], "temperature": 0.0, "avg_logprob": -0.0883876599763569, "compression_ratio": 1.8660714285714286, "no_speech_prob": 0.005991611164063215}, {"id": 181, "seek": 124720, "start": 1253.2, "end": 1257.76, "text": " you know so I think that's actually where a lot of emergence happens and in fact that's I think", "tokens": [50664, 291, 458, 370, 286, 519, 300, 311, 767, 689, 257, 688, 295, 36211, 2314, 293, 294, 1186, 300, 311, 286, 519, 50892], "temperature": 0.0, "avg_logprob": -0.0883876599763569, "compression_ratio": 1.8660714285714286, "no_speech_prob": 0.005991611164063215}, {"id": 182, "seek": 124720, "start": 1258.48, "end": 1263.76, "text": " the white there's a little academic dispute about whether or not these models have emergent", "tokens": [50928, 264, 2418, 456, 311, 257, 707, 7778, 25379, 466, 1968, 420, 406, 613, 5245, 362, 4345, 6930, 51192], "temperature": 0.0, "avg_logprob": -0.0883876599763569, "compression_ratio": 1.8660714285714286, "no_speech_prob": 0.005991611164063215}, {"id": 183, "seek": 124720, "start": 1263.76, "end": 1268.4, "text": " behavior or not and that's at least my understanding of how you'd resolve that", "tokens": [51192, 5223, 420, 406, 293, 300, 311, 412, 1935, 452, 3701, 295, 577, 291, 1116, 14151, 300, 51424], "temperature": 0.0, "avg_logprob": -0.0883876599763569, "compression_ratio": 1.8660714285714286, "no_speech_prob": 0.005991611164063215}, {"id": 184, "seek": 124720, "start": 1270.56, "end": 1275.3600000000001, "text": " so I actually do think that a lot of the ideas that people here would have would actually be", "tokens": [51532, 370, 286, 767, 360, 519, 300, 257, 688, 295, 264, 3487, 300, 561, 510, 576, 362, 576, 767, 312, 51772], "temperature": 0.0, "avg_logprob": -0.0883876599763569, "compression_ratio": 1.8660714285714286, "no_speech_prob": 0.005991611164063215}, {"id": 185, "seek": 127536, "start": 1275.36, "end": 1283.9199999999998, "text": " very useful for the community to have as well okay all right you know I actually think there's", "tokens": [50364, 588, 4420, 337, 264, 1768, 281, 362, 382, 731, 1392, 439, 558, 291, 458, 286, 767, 519, 456, 311, 50792], "temperature": 0.0, "avg_logprob": -0.051224708557128906, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.001522851292975247}, {"id": 186, "seek": 127536, "start": 1283.9199999999998, "end": 1289.6, "text": " lots of really interesting questions also you know can we understand what these large language", "tokens": [50792, 3195, 295, 534, 1880, 1651, 611, 291, 458, 393, 321, 1223, 437, 613, 2416, 2856, 51076], "temperature": 0.0, "avg_logprob": -0.051224708557128906, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.001522851292975247}, {"id": 187, "seek": 127536, "start": 1289.6, "end": 1296.1599999999999, "text": " models are really doing I think it's you know I mean we know something about how language is", "tokens": [51076, 5245, 366, 534, 884, 286, 519, 309, 311, 291, 458, 286, 914, 321, 458, 746, 466, 577, 2856, 307, 51404], "temperature": 0.0, "avg_logprob": -0.051224708557128906, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.001522851292975247}, {"id": 188, "seek": 129616, "start": 1296.16, "end": 1304.0800000000002, "text": " processed in the human brain you know we know that none of these models really are realistic", "tokens": [50364, 18846, 294, 264, 1952, 3567, 291, 458, 321, 458, 300, 6022, 295, 613, 5245, 534, 366, 12465, 50760], "temperature": 0.0, "avg_logprob": -0.06078019936879476, "compression_ratio": 1.7421383647798743, "no_speech_prob": 0.026725826784968376}, {"id": 189, "seek": 129616, "start": 1306.48, "end": 1311.68, "text": " just even understanding you know what these large language models are doing how can you", "tokens": [50880, 445, 754, 3701, 291, 458, 437, 613, 2416, 2856, 5245, 366, 884, 577, 393, 291, 51140], "temperature": 0.0, "avg_logprob": -0.06078019936879476, "compression_ratio": 1.7421383647798743, "no_speech_prob": 0.026725826784968376}, {"id": 190, "seek": 129616, "start": 1312.3200000000002, "end": 1318.16, "text": " be sure that they know a syntactic rule or make it even simpler that they know a particular word", "tokens": [51172, 312, 988, 300, 436, 458, 257, 23980, 19892, 4978, 420, 652, 309, 754, 18587, 300, 436, 458, 257, 1729, 1349, 51464], "temperature": 0.0, "avg_logprob": -0.06078019936879476, "compression_ratio": 1.7421383647798743, "no_speech_prob": 0.026725826784968376}, {"id": 191, "seek": 131816, "start": 1318.16, "end": 1326.4, "text": " so right now and in fact actually I think another really interesting question is", "tokens": [50364, 370, 558, 586, 293, 294, 1186, 767, 286, 519, 1071, 534, 1880, 1168, 307, 50776], "temperature": 0.0, "avg_logprob": -0.12641060503223275, "compression_ratio": 1.7922705314009661, "no_speech_prob": 0.0049759927205741405}, {"id": 192, "seek": 131816, "start": 1327.6000000000001, "end": 1334.24, "text": " if if these large language models are basically just you know gigantic neural nets as I said before", "tokens": [50836, 498, 498, 613, 2416, 2856, 5245, 366, 1936, 445, 291, 458, 26800, 18161, 36170, 382, 286, 848, 949, 51168], "temperature": 0.0, "avg_logprob": -0.12641060503223275, "compression_ratio": 1.7922705314009661, "no_speech_prob": 0.0049759927205741405}, {"id": 193, "seek": 131816, "start": 1334.24, "end": 1340.8000000000002, "text": " of a relatively generic type why is it that only human beings can acquire language are they are", "tokens": [51168, 295, 257, 7226, 19577, 2010, 983, 307, 309, 300, 787, 1952, 8958, 393, 20001, 2856, 366, 436, 366, 51496], "temperature": 0.0, "avg_logprob": -0.12641060503223275, "compression_ratio": 1.7922705314009661, "no_speech_prob": 0.0049759927205741405}, {"id": 194, "seek": 131816, "start": 1340.8000000000002, "end": 1346.0, "text": " you know why is it humans are the only animals that can acquire language right I mean you know", "tokens": [51496, 291, 458, 983, 307, 309, 6255, 366, 264, 787, 4882, 300, 393, 20001, 2856, 558, 286, 914, 291, 458, 51756], "temperature": 0.0, "avg_logprob": -0.12641060503223275, "compression_ratio": 1.7922705314009661, "no_speech_prob": 0.0049759927205741405}, {"id": 195, "seek": 134600, "start": 1346.0, "end": 1350.96, "text": " we don't have the biggest brains there are animals with bigger brains if it's just merely", "tokens": [50364, 321, 500, 380, 362, 264, 3880, 15442, 456, 366, 4882, 365, 3801, 15442, 498, 309, 311, 445, 17003, 50612], "temperature": 0.0, "avg_logprob": -0.11908435821533203, "compression_ratio": 1.7536945812807883, "no_speech_prob": 0.0011320823105052114}, {"id": 196, "seek": 134600, "start": 1352.4, "end": 1356.32, "text": " the size you know the number of neurons that we have sitting inside of our skulls", "tokens": [50684, 264, 2744, 291, 458, 264, 1230, 295, 22027, 300, 321, 362, 3798, 1854, 295, 527, 11743, 82, 50880], "temperature": 0.0, "avg_logprob": -0.11908435821533203, "compression_ratio": 1.7536945812807883, "no_speech_prob": 0.0011320823105052114}, {"id": 197, "seek": 134600, "start": 1356.88, "end": 1363.12, "text": " if that's all that determines our ability to do something like learn language why why don't", "tokens": [50908, 498, 300, 311, 439, 300, 24799, 527, 3485, 281, 360, 746, 411, 1466, 2856, 983, 983, 500, 380, 51220], "temperature": 0.0, "avg_logprob": -0.11908435821533203, "compression_ratio": 1.7536945812807883, "no_speech_prob": 0.0011320823105052114}, {"id": 198, "seek": 134600, "start": 1363.12, "end": 1370.16, "text": " other animals why don't they have that ability it's very popular now to talk about analyzing", "tokens": [51220, 661, 4882, 983, 500, 380, 436, 362, 300, 3485, 309, 311, 588, 3743, 586, 281, 751, 466, 23663, 51572], "temperature": 0.0, "avg_logprob": -0.11908435821533203, "compression_ratio": 1.7536945812807883, "no_speech_prob": 0.0011320823105052114}, {"id": 199, "seek": 137016, "start": 1370.16, "end": 1376.24, "text": " large language models using psychological or psychological linguistic methods I think that's", "tokens": [50364, 2416, 2856, 5245, 1228, 14346, 420, 14346, 43002, 7150, 286, 519, 300, 311, 50668], "temperature": 0.0, "avg_logprob": -0.06171833596578458, "compression_ratio": 1.7383177570093458, "no_speech_prob": 0.011846586130559444}, {"id": 200, "seek": 137016, "start": 1376.24, "end": 1382.64, "text": " about the best that I know how to do but I think a lot of these methods were really designed to", "tokens": [50668, 466, 264, 1151, 300, 286, 458, 577, 281, 360, 457, 286, 519, 257, 688, 295, 613, 7150, 645, 534, 4761, 281, 50988], "temperature": 0.0, "avg_logprob": -0.06171833596578458, "compression_ratio": 1.7383177570093458, "no_speech_prob": 0.011846586130559444}, {"id": 201, "seek": 137016, "start": 1382.64, "end": 1389.8400000000001, "text": " work on humans at least agents that have beliefs and again you know in the sense a large language", "tokens": [50988, 589, 322, 6255, 412, 1935, 12554, 300, 362, 13585, 293, 797, 291, 458, 294, 264, 2020, 257, 2416, 2856, 51348], "temperature": 0.0, "avg_logprob": -0.06171833596578458, "compression_ratio": 1.7383177570093458, "no_speech_prob": 0.011846586130559444}, {"id": 202, "seek": 137016, "start": 1389.8400000000001, "end": 1399.76, "text": " model doesn't have a belief it's just got reflexes okay so you know just to emphasize", "tokens": [51348, 2316, 1177, 380, 362, 257, 7107, 309, 311, 445, 658, 23802, 279, 1392, 370, 291, 458, 445, 281, 16078, 51844], "temperature": 0.0, "avg_logprob": -0.06171833596578458, "compression_ratio": 1.7383177570093458, "no_speech_prob": 0.011846586130559444}, {"id": 203, "seek": 139976, "start": 1399.76, "end": 1406.96, "text": " the differences between large language models and and humans right so you know children start", "tokens": [50364, 264, 7300, 1296, 2416, 2856, 5245, 293, 293, 6255, 558, 370, 291, 458, 2227, 722, 50724], "temperature": 0.0, "avg_logprob": -0.04217605888843536, "compression_ratio": 1.7534246575342465, "no_speech_prob": 0.0005881063407287002}, {"id": 204, "seek": 139976, "start": 1406.96, "end": 1415.44, "text": " and end learning from much smaller data sets they generalize in particular ways that we actually", "tokens": [50724, 293, 917, 2539, 490, 709, 4356, 1412, 6352, 436, 2674, 1125, 294, 1729, 2098, 300, 321, 767, 51148], "temperature": 0.0, "avg_logprob": -0.04217605888843536, "compression_ratio": 1.7534246575342465, "no_speech_prob": 0.0005881063407287002}, {"id": 205, "seek": 139976, "start": 1415.44, "end": 1422.4, "text": " understand to new unseen forms I think we don't really know actually how these large language", "tokens": [51148, 1223, 281, 777, 40608, 6422, 286, 519, 321, 500, 380, 534, 458, 767, 577, 613, 2416, 2856, 51496], "temperature": 0.0, "avg_logprob": -0.04217605888843536, "compression_ratio": 1.7534246575342465, "no_speech_prob": 0.0005881063407287002}, {"id": 206, "seek": 139976, "start": 1422.4, "end": 1428.4, "text": " models generalize I mean they do I think they do generalize but it's very difficult to tell exactly", "tokens": [51496, 5245, 2674, 1125, 286, 914, 436, 360, 286, 519, 436, 360, 2674, 1125, 457, 309, 311, 588, 2252, 281, 980, 2293, 51796], "temperature": 0.0, "avg_logprob": -0.04217605888843536, "compression_ratio": 1.7534246575342465, "no_speech_prob": 0.0005881063407287002}, {"id": 207, "seek": 142840, "start": 1428.4, "end": 1434.88, "text": " how they generalize children also actually over generalizing characteristically so Catherine's", "tokens": [50364, 577, 436, 2674, 1125, 2227, 611, 767, 670, 2674, 3319, 2517, 20458, 370, 23098, 311, 50688], "temperature": 0.0, "avg_logprob": -0.10757246250059546, "compression_ratio": 1.7731481481481481, "no_speech_prob": 0.0011144423624500632}, {"id": 208, "seek": 142840, "start": 1434.88, "end": 1440.4, "text": " an expert in this area but you know these are just a couple of examples that she pointed out you know", "tokens": [50688, 364, 5844, 294, 341, 1859, 457, 291, 458, 613, 366, 445, 257, 1916, 295, 5110, 300, 750, 10932, 484, 291, 458, 50964], "temperature": 0.0, "avg_logprob": -0.10757246250059546, "compression_ratio": 1.7731481481481481, "no_speech_prob": 0.0011144423624500632}, {"id": 209, "seek": 142840, "start": 1440.4, "end": 1447.92, "text": " where children have taken irregular verbs and either inflicted them in a regular fashion or", "tokens": [50964, 689, 2227, 362, 2726, 29349, 30051, 293, 2139, 38137, 292, 552, 294, 257, 3890, 6700, 420, 51340], "temperature": 0.0, "avg_logprob": -0.10757246250059546, "compression_ratio": 1.7731481481481481, "no_speech_prob": 0.0011144423624500632}, {"id": 210, "seek": 142840, "start": 1447.92, "end": 1457.3600000000001, "text": " over generalize the irregular form she's giggling me you know that makes sense if you think of", "tokens": [51340, 670, 2674, 1125, 264, 29349, 1254, 750, 311, 290, 24542, 385, 291, 458, 300, 1669, 2020, 498, 291, 519, 295, 51812], "temperature": 0.0, "avg_logprob": -0.10757246250059546, "compression_ratio": 1.7731481481481481, "no_speech_prob": 0.0011144423624500632}, {"id": 211, "seek": 145736, "start": 1457.36, "end": 1466.3999999999999, "text": " giggling as being a verb a bit like tickling for example language learning you know by the time", "tokens": [50364, 290, 24542, 382, 885, 257, 9595, 257, 857, 411, 5204, 1688, 337, 1365, 2856, 2539, 291, 458, 538, 264, 565, 50816], "temperature": 0.0, "avg_logprob": -0.06240227643181296, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.0005349060520529747}, {"id": 212, "seek": 145736, "start": 1466.3999999999999, "end": 1472.6399999999999, "text": " you're three or four you're a competent speaker of your native language usually but then there's", "tokens": [50816, 291, 434, 1045, 420, 1451, 291, 434, 257, 29998, 8145, 295, 428, 8470, 2856, 2673, 457, 550, 456, 311, 51128], "temperature": 0.0, "avg_logprob": -0.06240227643181296, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.0005349060520529747}, {"id": 213, "seek": 145736, "start": 1472.6399999999999, "end": 1480.56, "text": " also some part of language learning that's not really complete until the early teens right and", "tokens": [51128, 611, 512, 644, 295, 2856, 2539, 300, 311, 406, 534, 3566, 1826, 264, 2440, 24849, 558, 293, 51524], "temperature": 0.0, "avg_logprob": -0.06240227643181296, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.0005349060520529747}, {"id": 214, "seek": 145736, "start": 1480.56, "end": 1486.3999999999999, "text": " then you know just in terms of the pragmatics of doing research on large language models right", "tokens": [51524, 550, 291, 458, 445, 294, 2115, 295, 264, 33394, 15677, 1167, 295, 884, 2132, 322, 2416, 2856, 5245, 558, 51816], "temperature": 0.0, "avg_logprob": -0.06240227643181296, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.0005349060520529747}, {"id": 215, "seek": 148640, "start": 1487.0400000000002, "end": 1493.2800000000002, "text": " the time scale of research projects are different so it might take a couple of years for a student", "tokens": [50396, 264, 565, 4373, 295, 2132, 4455, 366, 819, 370, 309, 1062, 747, 257, 1916, 295, 924, 337, 257, 3107, 50708], "temperature": 0.0, "avg_logprob": -0.09769127262172414, "compression_ratio": 1.5243243243243243, "no_speech_prob": 0.0005601056618615985}, {"id": 216, "seek": 148640, "start": 1493.2800000000002, "end": 1500.96, "text": " to do a research project studying say human language learning if they're studying something", "tokens": [50708, 281, 360, 257, 2132, 1716, 7601, 584, 1952, 2856, 2539, 498, 436, 434, 7601, 746, 51092], "temperature": 0.0, "avg_logprob": -0.09769127262172414, "compression_ratio": 1.5243243243243243, "no_speech_prob": 0.0005601056618615985}, {"id": 217, "seek": 148640, "start": 1500.96, "end": 1509.92, "text": " which was inspired by GPT-4 well in two years time we're probably in GPT-6 you know and the", "tokens": [51092, 597, 390, 7547, 538, 26039, 51, 12, 19, 731, 294, 732, 924, 565, 321, 434, 1391, 294, 26039, 51, 12, 21, 291, 458, 293, 264, 51540], "temperature": 0.0, "avg_logprob": -0.09769127262172414, "compression_ratio": 1.5243243243243243, "no_speech_prob": 0.0005601056618615985}, {"id": 218, "seek": 150992, "start": 1510.4, "end": 1513.92, "text": " the the inspiration might be actually sort of completely different", "tokens": [50388, 264, 264, 10249, 1062, 312, 767, 1333, 295, 2584, 819, 50564], "temperature": 0.0, "avg_logprob": -0.12090613237067835, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.004678305704146624}, {"id": 219, "seek": 150992, "start": 1516.96, "end": 1523.6000000000001, "text": " I also this is essentially that same thing as as I was saying before right so evaluation and", "tokens": [50716, 286, 611, 341, 307, 4476, 300, 912, 551, 382, 382, 286, 390, 1566, 949, 558, 370, 13344, 293, 51048], "temperature": 0.0, "avg_logprob": -0.12090613237067835, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.004678305704146624}, {"id": 220, "seek": 150992, "start": 1523.6000000000001, "end": 1529.76, "text": " testing I think is really a huge challenge that was always difficult inside of natural", "tokens": [51048, 4997, 286, 519, 307, 534, 257, 2603, 3430, 300, 390, 1009, 2252, 1854, 295, 3303, 51356], "temperature": 0.0, "avg_logprob": -0.12090613237067835, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.004678305704146624}, {"id": 221, "seek": 150992, "start": 1529.76, "end": 1535.92, "text": " language processing but it gets even worse because the inputs to large language models", "tokens": [51356, 2856, 9007, 457, 309, 2170, 754, 5324, 570, 264, 15743, 281, 2416, 2856, 5245, 51664], "temperature": 0.0, "avg_logprob": -0.12090613237067835, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.004678305704146624}, {"id": 222, "seek": 153592, "start": 1535.92, "end": 1541.3600000000001, "text": " now instead of just again because this context is so much longer the input is not just a", "tokens": [50364, 586, 2602, 295, 445, 797, 570, 341, 4319, 307, 370, 709, 2854, 264, 4846, 307, 406, 445, 257, 50636], "temperature": 0.0, "avg_logprob": -0.05050115469025403, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0005440078093670309}, {"id": 223, "seek": 153592, "start": 1541.3600000000001, "end": 1546.88, "text": " single sentence it's an entire conversation or entire story or something else like that", "tokens": [50636, 2167, 8174, 309, 311, 364, 2302, 3761, 420, 2302, 1657, 420, 746, 1646, 411, 300, 50912], "temperature": 0.0, "avg_logprob": -0.05050115469025403, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0005440078093670309}, {"id": 224, "seek": 153592, "start": 1547.52, "end": 1555.92, "text": " so if you want to really evaluate the performance of one of these systems you want to vary not just", "tokens": [50944, 370, 498, 291, 528, 281, 534, 13059, 264, 3389, 295, 472, 295, 613, 3652, 291, 528, 281, 10559, 406, 445, 51364], "temperature": 0.0, "avg_logprob": -0.05050115469025403, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0005440078093670309}, {"id": 225, "seek": 153592, "start": 1555.92, "end": 1562.8000000000002, "text": " you know the last sentence you want to vary the entire context as well from a commercial point of", "tokens": [51364, 291, 458, 264, 1036, 8174, 291, 528, 281, 10559, 264, 2302, 4319, 382, 731, 490, 257, 6841, 935, 295, 51708], "temperature": 0.0, "avg_logprob": -0.05050115469025403, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0005440078093670309}, {"id": 226, "seek": 156280, "start": 1562.8, "end": 1570.08, "text": " view I think actually testing is really super important I mean you know you've probably all saw", "tokens": [50364, 1910, 286, 519, 767, 4997, 307, 534, 1687, 1021, 286, 914, 291, 458, 291, 600, 1391, 439, 1866, 50728], "temperature": 0.0, "avg_logprob": -0.09733963781787504, "compression_ratio": 1.5865921787709498, "no_speech_prob": 0.007318770047277212}, {"id": 227, "seek": 156280, "start": 1570.8, "end": 1581.12, "text": " the Microsoft Bing chatbot which when it was you know the New York Times reporter studied", "tokens": [50764, 264, 8116, 30755, 5081, 18870, 597, 562, 309, 390, 291, 458, 264, 1873, 3609, 11366, 19152, 9454, 51280], "temperature": 0.0, "avg_logprob": -0.09733963781787504, "compression_ratio": 1.5865921787709498, "no_speech_prob": 0.007318770047277212}, {"id": 228, "seek": 156280, "start": 1581.84, "end": 1588.08, "text": " that chatbot and it announced to the reporter that in fact actually that it preferred to be called", "tokens": [51316, 300, 5081, 18870, 293, 309, 7548, 281, 264, 19152, 300, 294, 1186, 767, 300, 309, 16494, 281, 312, 1219, 51628], "temperature": 0.0, "avg_logprob": -0.09733963781787504, "compression_ratio": 1.5865921787709498, "no_speech_prob": 0.007318770047277212}, {"id": 229, "seek": 158808, "start": 1588.08, "end": 1595.6, "text": " Sydney rather than Bing and then also suggested that really you know the reporter should divorce", "tokens": [50364, 21065, 2831, 813, 30755, 293, 550, 611, 10945, 300, 534, 291, 458, 264, 19152, 820, 16052, 50740], "temperature": 0.0, "avg_logprob": -0.08975886043749358, "compression_ratio": 1.6043478260869566, "no_speech_prob": 0.004108082968741655}, {"id": 230, "seek": 158808, "start": 1595.6, "end": 1603.9199999999998, "text": " his wife I'm sure behaviors that Microsoft was really not too proud of right and I actually", "tokens": [50740, 702, 3836, 286, 478, 988, 15501, 300, 8116, 390, 534, 406, 886, 4570, 295, 558, 293, 286, 767, 51156], "temperature": 0.0, "avg_logprob": -0.08975886043749358, "compression_ratio": 1.6043478260869566, "no_speech_prob": 0.004108082968741655}, {"id": 231, "seek": 158808, "start": 1603.9199999999998, "end": 1610.72, "text": " think for commercial purposes it's super important to be able to detect and you know", "tokens": [51156, 519, 337, 6841, 9932, 309, 311, 1687, 1021, 281, 312, 1075, 281, 5531, 293, 291, 458, 51496], "temperature": 0.0, "avg_logprob": -0.08975886043749358, "compression_ratio": 1.6043478260869566, "no_speech_prob": 0.004108082968741655}, {"id": 232, "seek": 158808, "start": 1611.6, "end": 1617.36, "text": " guaranteed that such behaviors really aren't lurking beneath the surface of your large language", "tokens": [51540, 18031, 300, 1270, 15501, 534, 3212, 380, 35583, 5092, 17149, 264, 3753, 295, 428, 2416, 2856, 51828], "temperature": 0.0, "avg_logprob": -0.08975886043749358, "compression_ratio": 1.6043478260869566, "no_speech_prob": 0.004108082968741655}, {"id": 233, "seek": 161736, "start": 1617.36, "end": 1627.1999999999998, "text": " model. Constraint decoding that's just an NLP topic I won't spend too much time on but I actually", "tokens": [50364, 2316, 13, 8574, 424, 686, 979, 8616, 300, 311, 445, 364, 426, 45196, 4829, 286, 1582, 380, 3496, 886, 709, 565, 322, 457, 286, 767, 50856], "temperature": 0.0, "avg_logprob": -0.0720018508822419, "compression_ratio": 1.7568807339449541, "no_speech_prob": 0.0007196355727501214}, {"id": 234, "seek": 161736, "start": 1627.1999999999998, "end": 1631.6799999999998, "text": " do think that there's really interesting work there to look at different ways of actually", "tokens": [50856, 360, 519, 300, 456, 311, 534, 1880, 589, 456, 281, 574, 412, 819, 2098, 295, 767, 51080], "temperature": 0.0, "avg_logprob": -0.0720018508822419, "compression_ratio": 1.7568807339449541, "no_speech_prob": 0.0007196355727501214}, {"id": 235, "seek": 161736, "start": 1631.6799999999998, "end": 1637.6, "text": " constraining the output of a large language model and some real challenges there I think there's", "tokens": [51080, 11525, 1760, 264, 5598, 295, 257, 2416, 2856, 2316, 293, 512, 957, 4759, 456, 286, 519, 456, 311, 51376], "temperature": 0.0, "avg_logprob": -0.0720018508822419, "compression_ratio": 1.7568807339449541, "no_speech_prob": 0.0007196355727501214}, {"id": 236, "seek": 161736, "start": 1637.6, "end": 1644.1599999999999, "text": " really interesting work about how one actually trains these models as well so I mentioned that the", "tokens": [51376, 534, 1880, 589, 466, 577, 472, 767, 16329, 613, 5245, 382, 731, 370, 286, 2835, 300, 264, 51704], "temperature": 0.0, "avg_logprob": -0.0720018508822419, "compression_ratio": 1.7568807339449541, "no_speech_prob": 0.0007196355727501214}, {"id": 237, "seek": 164416, "start": 1644.16, "end": 1649.3600000000001, "text": " training procedure is itself actually a very complicated one typically what happens is that", "tokens": [50364, 3097, 10747, 307, 2564, 767, 257, 588, 6179, 472, 5850, 437, 2314, 307, 300, 50624], "temperature": 0.0, "avg_logprob": -0.06043110611618206, "compression_ratio": 1.9670781893004115, "no_speech_prob": 0.0014773141592741013}, {"id": 238, "seek": 164416, "start": 1649.3600000000001, "end": 1654.3200000000002, "text": " they start up by training with what's called the language model training objective which is where", "tokens": [50624, 436, 722, 493, 538, 3097, 365, 437, 311, 1219, 264, 2856, 2316, 3097, 10024, 597, 307, 689, 50872], "temperature": 0.0, "avg_logprob": -0.06043110611618206, "compression_ratio": 1.9670781893004115, "no_speech_prob": 0.0014773141592741013}, {"id": 239, "seek": 164416, "start": 1654.3200000000002, "end": 1660.96, "text": " effectively you're just simply training the model to predict the very next word but a model which", "tokens": [50872, 8659, 291, 434, 445, 2935, 3097, 264, 2316, 281, 6069, 264, 588, 958, 1349, 457, 257, 2316, 597, 51204], "temperature": 0.0, "avg_logprob": -0.06043110611618206, "compression_ratio": 1.9670781893004115, "no_speech_prob": 0.0014773141592741013}, {"id": 240, "seek": 164416, "start": 1660.96, "end": 1667.0400000000002, "text": " is just simply trained with this large language model training objective on its own doesn't really", "tokens": [51204, 307, 445, 2935, 8895, 365, 341, 2416, 2856, 2316, 3097, 10024, 322, 1080, 1065, 1177, 380, 534, 51508], "temperature": 0.0, "avg_logprob": -0.06043110611618206, "compression_ratio": 1.9670781893004115, "no_speech_prob": 0.0014773141592741013}, {"id": 241, "seek": 164416, "start": 1667.0400000000002, "end": 1672.3200000000002, "text": " engage in useful conversation doesn't really follow instructions very well so it's actually", "tokens": [51508, 4683, 294, 4420, 3761, 1177, 380, 534, 1524, 9415, 588, 731, 370, 309, 311, 767, 51772], "temperature": 0.0, "avg_logprob": -0.06043110611618206, "compression_ratio": 1.9670781893004115, "no_speech_prob": 0.0014773141592741013}, {"id": 242, "seek": 167232, "start": 1672.32, "end": 1681.2, "text": " very typical to follow that up with an additional training step that is you know that involves well", "tokens": [50364, 588, 7476, 281, 1524, 300, 493, 365, 364, 4497, 3097, 1823, 300, 307, 291, 458, 300, 11626, 731, 50808], "temperature": 0.0, "avg_logprob": -0.08134732763451266, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.0018063760362565517}, {"id": 243, "seek": 167232, "start": 1681.2, "end": 1686.32, "text": " as I said there's reinforcement learning with human feedback and I think that's a really interesting", "tokens": [50808, 382, 286, 848, 456, 311, 29280, 2539, 365, 1952, 5824, 293, 286, 519, 300, 311, 257, 534, 1880, 51064], "temperature": 0.0, "avg_logprob": -0.08134732763451266, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.0018063760362565517}, {"id": 244, "seek": 167232, "start": 1686.32, "end": 1693.04, "text": " question I've actually got some theories myself about you know when you want to use one sort of", "tokens": [51064, 1168, 286, 600, 767, 658, 512, 13667, 2059, 466, 291, 458, 562, 291, 528, 281, 764, 472, 1333, 295, 51400], "temperature": 0.0, "avg_logprob": -0.08134732763451266, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.0018063760362565517}, {"id": 245, "seek": 167232, "start": 1693.04, "end": 1698.3999999999999, "text": " training objective versus another and if there's people that'll like to talk about that more generally", "tokens": [51400, 3097, 10024, 5717, 1071, 293, 498, 456, 311, 561, 300, 603, 411, 281, 751, 466, 300, 544, 5101, 51668], "temperature": 0.0, "avg_logprob": -0.08134732763451266, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.0018063760362565517}, {"id": 246, "seek": 169840, "start": 1698.4, "end": 1704.0, "text": " I think there's a really interesting question which is how do you align the LLM behavior", "tokens": [50364, 286, 519, 456, 311, 257, 534, 1880, 1168, 597, 307, 577, 360, 291, 7975, 264, 441, 43, 44, 5223, 50644], "temperature": 0.0, "avg_logprob": -0.09541028196161444, "compression_ratio": 1.7534246575342465, "no_speech_prob": 0.0009528130758553743}, {"id": 247, "seek": 169840, "start": 1705.3600000000001, "end": 1711.1200000000001, "text": " with well how do you get the LLM to behave the way that you want it to right so you've got these", "tokens": [50712, 365, 731, 577, 360, 291, 483, 264, 441, 43, 44, 281, 15158, 264, 636, 300, 291, 528, 309, 281, 558, 370, 291, 600, 658, 613, 51000], "temperature": 0.0, "avg_logprob": -0.09541028196161444, "compression_ratio": 1.7534246575342465, "no_speech_prob": 0.0009528130758553743}, {"id": 248, "seek": 169840, "start": 1711.1200000000001, "end": 1716.24, "text": " very general alignment goals like you know follow commands that run right through to don't destroy", "tokens": [51000, 588, 2674, 18515, 5493, 411, 291, 458, 1524, 16901, 300, 1190, 558, 807, 281, 500, 380, 5293, 51256], "temperature": 0.0, "avg_logprob": -0.09541028196161444, "compression_ratio": 1.7534246575342465, "no_speech_prob": 0.0009528130758553743}, {"id": 249, "seek": 169840, "start": 1716.24, "end": 1723.0400000000002, "text": " humanity so ultimately it's the training data and the training training procedure which is going to", "tokens": [51256, 10243, 370, 6284, 309, 311, 264, 3097, 1412, 293, 264, 3097, 3097, 10747, 597, 307, 516, 281, 51596], "temperature": 0.0, "avg_logprob": -0.09541028196161444, "compression_ratio": 1.7534246575342465, "no_speech_prob": 0.0009528130758553743}, {"id": 250, "seek": 172304, "start": 1723.04, "end": 1731.84, "text": " determine the LLM behavior so how exactly do we do that right and I actually do think that there's", "tokens": [50364, 6997, 264, 441, 43, 44, 5223, 370, 577, 2293, 360, 321, 360, 300, 558, 293, 286, 767, 360, 519, 300, 456, 311, 50804], "temperature": 0.0, "avg_logprob": -0.07313419448004829, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.002711622044444084}, {"id": 251, "seek": 172304, "start": 1731.84, "end": 1736.96, "text": " good academic research that can be done there largely because the fine-tuning step that I mentioned", "tokens": [50804, 665, 7778, 2132, 300, 393, 312, 1096, 456, 11611, 570, 264, 2489, 12, 83, 37726, 1823, 300, 286, 2835, 51060], "temperature": 0.0, "avg_logprob": -0.07313419448004829, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.002711622044444084}, {"id": 252, "seek": 172304, "start": 1736.96, "end": 1743.6, "text": " this sort of multi-stage training that's pretty modest right tens of thousands of examples or less", "tokens": [51060, 341, 1333, 295, 4825, 12, 17882, 3097, 300, 311, 1238, 25403, 558, 10688, 295, 5383, 295, 5110, 420, 1570, 51392], "temperature": 0.0, "avg_logprob": -0.07313419448004829, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.002711622044444084}, {"id": 253, "seek": 172304, "start": 1745.6, "end": 1752.56, "text": " and it can be done on sort of fairly modest hardware so I actually think that's a that's", "tokens": [51492, 293, 309, 393, 312, 1096, 322, 1333, 295, 6457, 25403, 8837, 370, 286, 767, 519, 300, 311, 257, 300, 311, 51840], "temperature": 0.0, "avg_logprob": -0.07313419448004829, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.002711622044444084}, {"id": 254, "seek": 175256, "start": 1752.56, "end": 1759.6, "text": " that's a great academic research topic I feel a little bit guilty here because I just mentioned", "tokens": [50364, 300, 311, 257, 869, 7778, 2132, 4829, 286, 841, 257, 707, 857, 12341, 510, 570, 286, 445, 2835, 50716], "temperature": 0.0, "avg_logprob": -0.05798309348350347, "compression_ratio": 1.669603524229075, "no_speech_prob": 7.835381984477863e-05}, {"id": 255, "seek": 175256, "start": 1759.6, "end": 1766.8, "text": " to you that maybe the only thing which is standing between us and you know a artificial general", "tokens": [50716, 281, 291, 300, 1310, 264, 787, 551, 597, 307, 4877, 1296, 505, 293, 291, 458, 257, 11677, 2674, 51076], "temperature": 0.0, "avg_logprob": -0.05798309348350347, "compression_ratio": 1.669603524229075, "no_speech_prob": 7.835381984477863e-05}, {"id": 256, "seek": 175256, "start": 1766.8, "end": 1771.9199999999998, "text": " intelligence is the ability to have episodic memory and then now I'm going to suggest to you", "tokens": [51076, 7599, 307, 264, 3485, 281, 362, 39200, 299, 4675, 293, 550, 586, 286, 478, 516, 281, 3402, 281, 291, 51332], "temperature": 0.0, "avg_logprob": -0.05798309348350347, "compression_ratio": 1.669603524229075, "no_speech_prob": 7.835381984477863e-05}, {"id": 257, "seek": 175256, "start": 1772.56, "end": 1778.8799999999999, "text": " how we might actually do that and it's you know the most obvious way to do that is to actually", "tokens": [51364, 577, 321, 1062, 767, 360, 300, 293, 309, 311, 291, 458, 264, 881, 6322, 636, 281, 360, 300, 307, 281, 767, 51680], "temperature": 0.0, "avg_logprob": -0.05798309348350347, "compression_ratio": 1.669603524229075, "no_speech_prob": 7.835381984477863e-05}, {"id": 258, "seek": 177888, "start": 1778.88, "end": 1784.4, "text": " take that retrieval augmented generator that I mentioned before and basically let the large", "tokens": [50364, 747, 300, 19817, 3337, 36155, 19265, 300, 286, 2835, 949, 293, 1936, 718, 264, 2416, 50640], "temperature": 0.0, "avg_logprob": -0.09077436641111213, "compression_ratio": 1.6022099447513811, "no_speech_prob": 0.0008025762508623302}, {"id": 259, "seek": 177888, "start": 1784.4, "end": 1792.88, "text": " language model write its own memories and this is basically a suggestion about how you might do that", "tokens": [50640, 2856, 2316, 2464, 1080, 1065, 8495, 293, 341, 307, 1936, 257, 16541, 466, 577, 291, 1062, 360, 300, 51064], "temperature": 0.0, "avg_logprob": -0.09077436641111213, "compression_ratio": 1.6022099447513811, "no_speech_prob": 0.0008025762508623302}, {"id": 260, "seek": 177888, "start": 1794.5600000000002, "end": 1804.8000000000002, "text": " more generally I actually think that you know people like me have spent decades trying to come up", "tokens": [51148, 544, 5101, 286, 767, 519, 300, 291, 458, 561, 411, 385, 362, 4418, 7878, 1382, 281, 808, 493, 51660], "temperature": 0.0, "avg_logprob": -0.09077436641111213, "compression_ratio": 1.6022099447513811, "no_speech_prob": 0.0008025762508623302}, {"id": 261, "seek": 180480, "start": 1804.8, "end": 1810.8, "text": " with specialized knowledge representation systems and specialized inference systems", "tokens": [50364, 365, 19813, 3601, 10290, 3652, 293, 19813, 38253, 3652, 50664], "temperature": 0.0, "avg_logprob": -0.09749273153451773, "compression_ratio": 1.8013245033112584, "no_speech_prob": 0.0013232664205133915}, {"id": 262, "seek": 180480, "start": 1813.44, "end": 1821.6, "text": " you know so and this essentially is like a specialized logic you know so knowledge graphs", "tokens": [50796, 291, 458, 370, 293, 341, 4476, 307, 411, 257, 19813, 9952, 291, 458, 370, 3601, 24877, 51204], "temperature": 0.0, "avg_logprob": -0.09749273153451773, "compression_ratio": 1.8013245033112584, "no_speech_prob": 0.0013232664205133915}, {"id": 263, "seek": 180480, "start": 1821.6, "end": 1828.96, "text": " and one example of that where you try to encode information in entity relation triples for example", "tokens": [51204, 293, 472, 1365, 295, 300, 689, 291, 853, 281, 2058, 1429, 1589, 294, 13977, 9721, 1376, 2622, 337, 1365, 51572], "temperature": 0.0, "avg_logprob": -0.09749273153451773, "compression_ratio": 1.8013245033112584, "no_speech_prob": 0.0013232664205133915}, {"id": 264, "seek": 182896, "start": 1829.68, "end": 1836.48, "text": " but I actually think with LLMs you know one real possibility is that you actually let the", "tokens": [50400, 457, 286, 767, 519, 365, 441, 43, 26386, 291, 458, 472, 957, 7959, 307, 300, 291, 767, 718, 264, 50740], "temperature": 0.0, "avg_logprob": -0.11503470433901435, "compression_ratio": 1.7386934673366834, "no_speech_prob": 0.0006352502387017012}, {"id": 265, "seek": 182896, "start": 1838.08, "end": 1841.6000000000001, "text": " you let the primitive statements actually be natural language statements", "tokens": [50820, 291, 718, 264, 28540, 12363, 767, 312, 3303, 2856, 12363, 50996], "temperature": 0.0, "avg_logprob": -0.11503470433901435, "compression_ratio": 1.7386934673366834, "no_speech_prob": 0.0006352502387017012}, {"id": 266, "seek": 182896, "start": 1843.52, "end": 1850.48, "text": " so you just have represented inside of your system stored inside of a vector store for example", "tokens": [51092, 370, 291, 445, 362, 10379, 1854, 295, 428, 1185, 12187, 1854, 295, 257, 8062, 3531, 337, 1365, 51440], "temperature": 0.0, "avg_logprob": -0.11503470433901435, "compression_ratio": 1.7386934673366834, "no_speech_prob": 0.0006352502387017012}, {"id": 267, "seek": 182896, "start": 1850.48, "end": 1856.24, "text": " something like insomnia is a typical symptom of diabetes and then you'd actually let the", "tokens": [51440, 746, 411, 1028, 45438, 307, 257, 7476, 29370, 295, 13881, 293, 550, 291, 1116, 767, 718, 264, 51728], "temperature": 0.0, "avg_logprob": -0.11503470433901435, "compression_ratio": 1.7386934673366834, "no_speech_prob": 0.0006352502387017012}, {"id": 268, "seek": 185624, "start": 1856.24, "end": 1861.44, "text": " large language model itself decide the relationship between these atomic propositions", "tokens": [50364, 2416, 2856, 2316, 2564, 4536, 264, 2480, 1296, 613, 22275, 7532, 2451, 50624], "temperature": 0.0, "avg_logprob": -0.04246891361393341, "compression_ratio": 1.812807881773399, "no_speech_prob": 0.00020015586051158607}, {"id": 269, "seek": 185624, "start": 1863.28, "end": 1868.8, "text": " and so instead of having a specialized knowledge representation language a specialized logic", "tokens": [50716, 293, 370, 2602, 295, 1419, 257, 19813, 3601, 10290, 2856, 257, 19813, 9952, 50992], "temperature": 0.0, "avg_logprob": -0.04246891361393341, "compression_ratio": 1.812807881773399, "no_speech_prob": 0.00020015586051158607}, {"id": 270, "seek": 185624, "start": 1868.8, "end": 1873.92, "text": " you'd use natural language and you'd let the large language model actually pass information", "tokens": [50992, 291, 1116, 764, 3303, 2856, 293, 291, 1116, 718, 264, 2416, 2856, 2316, 767, 1320, 1589, 51248], "temperature": 0.0, "avg_logprob": -0.04246891361393341, "compression_ratio": 1.812807881773399, "no_speech_prob": 0.00020015586051158607}, {"id": 271, "seek": 185624, "start": 1873.92, "end": 1882.88, "text": " from one atomic proposition to another for those of you that are as old as I am you know I mean I", "tokens": [51248, 490, 472, 22275, 24830, 281, 1071, 337, 729, 295, 291, 300, 366, 382, 1331, 382, 286, 669, 291, 458, 286, 914, 286, 51696], "temperature": 0.0, "avg_logprob": -0.04246891361393341, "compression_ratio": 1.812807881773399, "no_speech_prob": 0.00020015586051158607}, {"id": 272, "seek": 188288, "start": 1882.88, "end": 1892.16, "text": " loved prologue and you know very simple horn clause inference procedures so what I just tried to do", "tokens": [50364, 4333, 447, 4987, 622, 293, 291, 458, 588, 2199, 13482, 25925, 38253, 13846, 370, 437, 286, 445, 3031, 281, 360, 50828], "temperature": 0.0, "avg_logprob": -0.09299177676439285, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.004602564033120871}, {"id": 273, "seek": 188288, "start": 1892.16, "end": 1900.72, "text": " here was take take that and sort of show how I might replace first order logic in there with", "tokens": [50828, 510, 390, 747, 747, 300, 293, 1333, 295, 855, 577, 286, 1062, 7406, 700, 1668, 9952, 294, 456, 365, 51256], "temperature": 0.0, "avg_logprob": -0.09299177676439285, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.004602564033120871}, {"id": 274, "seek": 188288, "start": 1900.72, "end": 1909.6000000000001, "text": " natural language statements but otherwise you've got proof rules proof structures and so this is in", "tokens": [51256, 3303, 2856, 12363, 457, 5911, 291, 600, 658, 8177, 4474, 8177, 9227, 293, 370, 341, 307, 294, 51700], "temperature": 0.0, "avg_logprob": -0.09299177676439285, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.004602564033120871}, {"id": 275, "seek": 190960, "start": 1909.6, "end": 1916.6399999999999, "text": " fact actually a standard you know textbook example of how uh you might wind up doing", "tokens": [50364, 1186, 767, 257, 3832, 291, 458, 25591, 1365, 295, 577, 2232, 291, 1062, 2468, 493, 884, 50716], "temperature": 0.0, "avg_logprob": -0.07884544417971656, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00045807124115526676}, {"id": 276, "seek": 190960, "start": 1916.6399999999999, "end": 1921.76, "text": " inference here so you've asked the question you know can Sam get a degree and you've got a series", "tokens": [50716, 38253, 510, 370, 291, 600, 2351, 264, 1168, 291, 458, 393, 4832, 483, 257, 4314, 293, 291, 600, 658, 257, 2638, 50972], "temperature": 0.0, "avg_logprob": -0.07884544417971656, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00045807124115526676}, {"id": 277, "seek": 190960, "start": 1921.76, "end": 1929.12, "text": " of facts about what courses Sam has taken and a series of rules but the difference is all this", "tokens": [50972, 295, 9130, 466, 437, 7712, 4832, 575, 2726, 293, 257, 2638, 295, 4474, 457, 264, 2649, 307, 439, 341, 51340], "temperature": 0.0, "avg_logprob": -0.07884544417971656, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00045807124115526676}, {"id": 278, "seek": 190960, "start": 1929.12, "end": 1937.76, "text": " is all expressed in natural language rather than in some first order logic form okay all right so", "tokens": [51340, 307, 439, 12675, 294, 3303, 2856, 2831, 813, 294, 512, 700, 1668, 9952, 1254, 1392, 439, 558, 370, 51772], "temperature": 0.0, "avg_logprob": -0.07884544417971656, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00045807124115526676}, {"id": 279, "seek": 193776, "start": 1937.76, "end": 1943.52, "text": " just talking a little bit more about the social implications of all of this", "tokens": [50364, 445, 1417, 257, 707, 857, 544, 466, 264, 2093, 16602, 295, 439, 295, 341, 50652], "temperature": 0.0, "avg_logprob": -0.06873010300301216, "compression_ratio": 1.7064676616915422, "no_speech_prob": 0.001014843350276351}, {"id": 280, "seek": 193776, "start": 1945.12, "end": 1950.24, "text": " so I think to understand the social implications I think one of the things you probably want to", "tokens": [50732, 370, 286, 519, 281, 1223, 264, 2093, 16602, 286, 519, 472, 295, 264, 721, 291, 1391, 528, 281, 50988], "temperature": 0.0, "avg_logprob": -0.06873010300301216, "compression_ratio": 1.7064676616915422, "no_speech_prob": 0.001014843350276351}, {"id": 281, "seek": 193776, "start": 1950.24, "end": 1954.8799999999999, "text": " understand is try and make some guesses about how the field itself might actually evolve", "tokens": [50988, 1223, 307, 853, 293, 652, 512, 42703, 466, 577, 264, 2519, 2564, 1062, 767, 16693, 51220], "temperature": 0.0, "avg_logprob": -0.06873010300301216, "compression_ratio": 1.7064676616915422, "no_speech_prob": 0.001014843350276351}, {"id": 282, "seek": 193776, "start": 1956.8, "end": 1965.36, "text": " I can see sort of two possible futures one is where we wind up getting ever larger", "tokens": [51316, 286, 393, 536, 1333, 295, 732, 1944, 26071, 472, 307, 689, 321, 2468, 493, 1242, 1562, 4833, 51744], "temperature": 0.0, "avg_logprob": -0.06873010300301216, "compression_ratio": 1.7064676616915422, "no_speech_prob": 0.001014843350276351}, {"id": 283, "seek": 196536, "start": 1965.36, "end": 1972.24, "text": " proprietary monolithic close large language models that you effectively interact with via", "tokens": [50364, 38992, 1108, 42878, 1998, 2416, 2856, 5245, 300, 291, 8659, 4648, 365, 5766, 50708], "temperature": 0.0, "avg_logprob": -0.09341741561889648, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.0011867055436596274}, {"id": 284, "seek": 196536, "start": 1972.24, "end": 1978.32, "text": " web APIs that is the actual model itself the training data everything is kept proprietary", "tokens": [50708, 3670, 21445, 300, 307, 264, 3539, 2316, 2564, 264, 3097, 1412, 1203, 307, 4305, 38992, 51012], "temperature": 0.0, "avg_logprob": -0.09341741561889648, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.0011867055436596274}, {"id": 285, "seek": 196536, "start": 1978.8799999999999, "end": 1986.7199999999998, "text": " but you can just simply call it over the web another future is that there will be open sourced", "tokens": [51040, 457, 291, 393, 445, 2935, 818, 309, 670, 264, 3670, 1071, 2027, 307, 300, 456, 486, 312, 1269, 11006, 1232, 51432], "temperature": 0.0, "avg_logprob": -0.09341741561889648, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.0011867055436596274}, {"id": 286, "seek": 196536, "start": 1986.7199999999998, "end": 1991.04, "text": " language models and the weights will actually be available and you'll be able to do things", "tokens": [51432, 2856, 5245, 293, 264, 17443, 486, 767, 312, 2435, 293, 291, 603, 312, 1075, 281, 360, 721, 51648], "temperature": 0.0, "avg_logprob": -0.09341741561889648, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.0011867055436596274}, {"id": 287, "seek": 199104, "start": 1991.04, "end": 1997.52, "text": " like fine tune those weights yourself and right now you know we're in the world where there's both", "tokens": [50364, 411, 2489, 10864, 729, 17443, 1803, 293, 558, 586, 291, 458, 321, 434, 294, 264, 1002, 689, 456, 311, 1293, 50688], "temperature": 0.0, "avg_logprob": -0.05830513712871505, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.0012823754223063588}, {"id": 288, "seek": 199104, "start": 1997.52, "end": 2004.0, "text": " of these kinds of large language models and the proprietary models are better than the open source", "tokens": [50688, 295, 613, 3685, 295, 2416, 2856, 5245, 293, 264, 38992, 5245, 366, 1101, 813, 264, 1269, 4009, 51012], "temperature": 0.0, "avg_logprob": -0.05830513712871505, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.0012823754223063588}, {"id": 289, "seek": 199104, "start": 2004.0, "end": 2010.72, "text": " models and I think really the big question about the development of the field is whether or not", "tokens": [51012, 5245, 293, 286, 519, 534, 264, 955, 1168, 466, 264, 3250, 295, 264, 2519, 307, 1968, 420, 406, 51348], "temperature": 0.0, "avg_logprob": -0.05830513712871505, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.0012823754223063588}, {"id": 290, "seek": 199104, "start": 2010.72, "end": 2017.04, "text": " fine tuning will turn out to take the open source models and make them competitive with the closed", "tokens": [51348, 2489, 15164, 486, 1261, 484, 281, 747, 264, 1269, 4009, 5245, 293, 652, 552, 10043, 365, 264, 5395, 51664], "temperature": 0.0, "avg_logprob": -0.05830513712871505, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.0012823754223063588}, {"id": 291, "seek": 201704, "start": 2017.04, "end": 2024.32, "text": " proprietary models and I call that the 64 billion dollar question because that's probably about the", "tokens": [50364, 38992, 5245, 293, 286, 818, 300, 264, 12145, 5218, 7241, 1168, 570, 300, 311, 1391, 466, 264, 50728], "temperature": 0.0, "avg_logprob": -0.09634977838267451, "compression_ratio": 1.6919431279620853, "no_speech_prob": 0.0013446860248222947}, {"id": 292, "seek": 201704, "start": 2024.32, "end": 2030.08, "text": " amount of money that the companies that are investing here sort of have invested", "tokens": [50728, 2372, 295, 1460, 300, 264, 3431, 300, 366, 10978, 510, 1333, 295, 362, 13104, 51016], "temperature": 0.0, "avg_logprob": -0.09634977838267451, "compression_ratio": 1.6919431279620853, "no_speech_prob": 0.0013446860248222947}, {"id": 293, "seek": 201704, "start": 2031.68, "end": 2039.44, "text": " you know the language models are becoming increasingly capital intensive it costs", "tokens": [51096, 291, 458, 264, 2856, 5245, 366, 5617, 12980, 4238, 18957, 309, 5497, 51484], "temperature": 0.0, "avg_logprob": -0.09634977838267451, "compression_ratio": 1.6919431279620853, "no_speech_prob": 0.0013446860248222947}, {"id": 294, "seek": 201704, "start": 2040.0, "end": 2046.8, "text": " huge amounts million many millions of dollars to collect the data and actually do the training", "tokens": [51512, 2603, 11663, 2459, 867, 6803, 295, 3808, 281, 2500, 264, 1412, 293, 767, 360, 264, 3097, 51852], "temperature": 0.0, "avg_logprob": -0.09634977838267451, "compression_ratio": 1.6919431279620853, "no_speech_prob": 0.0013446860248222947}, {"id": 295, "seek": 204680, "start": 2046.8799999999999, "end": 2053.36, "text": " of these things and capital intensive industries tend to concentrate you know you just look at the", "tokens": [50368, 295, 613, 721, 293, 4238, 18957, 13284, 3928, 281, 18089, 291, 458, 291, 445, 574, 412, 264, 50692], "temperature": 0.0, "avg_logprob": -0.07278776743325842, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.0004233720537740737}, {"id": 296, "seek": 204680, "start": 2053.36, "end": 2060.0, "text": " chip manufacturing where I think there's only one or two fabrication factories in the entire", "tokens": [50692, 11409, 11096, 689, 286, 519, 456, 311, 787, 472, 420, 732, 44820, 24813, 294, 264, 2302, 51024], "temperature": 0.0, "avg_logprob": -0.07278776743325842, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.0004233720537740737}, {"id": 297, "seek": 204680, "start": 2060.0, "end": 2068.32, "text": " world that make the top-end chips that we all have in all of our devices so if that is actually", "tokens": [51024, 1002, 300, 652, 264, 1192, 12, 521, 11583, 300, 321, 439, 362, 294, 439, 295, 527, 5759, 370, 498, 300, 307, 767, 51440], "temperature": 0.0, "avg_logprob": -0.07278776743325842, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.0004233720537740737}, {"id": 298, "seek": 204680, "start": 2068.32, "end": 2074.64, "text": " what the future of LLMs is then probably we will see that same sort of concentration", "tokens": [51440, 437, 264, 2027, 295, 441, 43, 26386, 307, 550, 1391, 321, 486, 536, 300, 912, 1333, 295, 9856, 51756], "temperature": 0.0, "avg_logprob": -0.07278776743325842, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.0004233720537740737}, {"id": 299, "seek": 207464, "start": 2075.2799999999997, "end": 2082.3199999999997, "text": " into just a couple of places I suspect the training data will become increasingly important", "tokens": [50396, 666, 445, 257, 1916, 295, 3190, 286, 9091, 264, 3097, 1412, 486, 1813, 12980, 1021, 50748], "temperature": 0.0, "avg_logprob": -0.07435939736562232, "compression_ratio": 1.660633484162896, "no_speech_prob": 0.003990510944277048}, {"id": 300, "seek": 207464, "start": 2082.3199999999997, "end": 2089.52, "text": " people are already talking about training data as being the ultimate you know limiting factor", "tokens": [50748, 561, 366, 1217, 1417, 466, 3097, 1412, 382, 885, 264, 9705, 291, 458, 22083, 5952, 51108], "temperature": 0.0, "avg_logprob": -0.07435939736562232, "compression_ratio": 1.660633484162896, "no_speech_prob": 0.003990510944277048}, {"id": 301, "seek": 207464, "start": 2090.64, "end": 2094.64, "text": " and I think it will become the major differentiator particularly if you want to do things", "tokens": [51164, 293, 286, 519, 309, 486, 1813, 264, 2563, 27372, 1639, 4098, 498, 291, 528, 281, 360, 721, 51364], "temperature": 0.0, "avg_logprob": -0.07435939736562232, "compression_ratio": 1.660633484162896, "no_speech_prob": 0.003990510944277048}, {"id": 302, "seek": 207464, "start": 2095.2799999999997, "end": 2101.92, "text": " like build LLMs for very specialized domains like healthcare finance other things like that", "tokens": [51396, 411, 1322, 441, 43, 26386, 337, 588, 19813, 25514, 411, 8884, 10719, 661, 721, 411, 300, 51728], "temperature": 0.0, "avg_logprob": -0.07435939736562232, "compression_ratio": 1.660633484162896, "no_speech_prob": 0.003990510944277048}, {"id": 303, "seek": 210192, "start": 2102.48, "end": 2109.84, "text": " but and I actually think data and LLM quality control which goes back to that issue about", "tokens": [50392, 457, 293, 286, 767, 519, 1412, 293, 441, 43, 44, 3125, 1969, 597, 1709, 646, 281, 300, 2734, 466, 50760], "temperature": 0.0, "avg_logprob": -0.11245545473965732, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.00042993322131223977}, {"id": 304, "seek": 210192, "start": 2109.84, "end": 2114.32, "text": " testing and evaluation I was talking about before that's going to become increasingly", "tokens": [50760, 4997, 293, 13344, 286, 390, 1417, 466, 949, 300, 311, 516, 281, 1813, 12980, 50984], "temperature": 0.0, "avg_logprob": -0.11245545473965732, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.00042993322131223977}, {"id": 305, "seek": 210192, "start": 2114.32, "end": 2120.48, "text": " important in fact I actually when I think about what will somebody like me in industry be doing", "tokens": [50984, 1021, 294, 1186, 286, 767, 562, 286, 519, 466, 437, 486, 2618, 411, 385, 294, 3518, 312, 884, 51292], "temperature": 0.0, "avg_logprob": -0.11245545473965732, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.00042993322131223977}, {"id": 306, "seek": 210192, "start": 2120.48, "end": 2127.92, "text": " in five years time quite possibly you know testing and evaluation will actually be you know", "tokens": [51292, 294, 1732, 924, 565, 1596, 6264, 291, 458, 4997, 293, 13344, 486, 767, 312, 291, 458, 51664], "temperature": 0.0, "avg_logprob": -0.11245545473965732, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.00042993322131223977}, {"id": 307, "seek": 212792, "start": 2127.92, "end": 2136.08, "text": " 90% of what we do you know we know that fine-tuning can mask a poor large language model right so", "tokens": [50364, 4289, 4, 295, 437, 321, 360, 291, 458, 321, 458, 300, 2489, 12, 83, 37726, 393, 6094, 257, 4716, 2416, 2856, 2316, 558, 370, 50772], "temperature": 0.0, "avg_logprob": -0.07806006390997704, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.005721117369830608}, {"id": 308, "seek": 212792, "start": 2136.08, "end": 2142.32, "text": " we know that you can take take Sydney and do a little bit of fine-tuning and have it at least", "tokens": [50772, 321, 458, 300, 291, 393, 747, 747, 21065, 293, 360, 257, 707, 857, 295, 2489, 12, 83, 37726, 293, 362, 309, 412, 1935, 51084], "temperature": 0.0, "avg_logprob": -0.07806006390997704, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.005721117369830608}, {"id": 309, "seek": 212792, "start": 2142.32, "end": 2151.2000000000003, "text": " superficially call itself being but then Sydney reemerges in the right context as well so I think", "tokens": [51084, 23881, 2270, 818, 2564, 885, 457, 550, 21065, 319, 29660, 2880, 294, 264, 558, 4319, 382, 731, 370, 286, 519, 51528], "temperature": 0.0, "avg_logprob": -0.07806006390997704, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.005721117369830608}, {"id": 310, "seek": 212792, "start": 2151.2000000000003, "end": 2157.12, "text": " one way we'll get around that is we'll start seeing things like certificates of origin", "tokens": [51528, 472, 636, 321, 603, 483, 926, 300, 307, 321, 603, 722, 2577, 721, 411, 32941, 295, 4957, 51824], "temperature": 0.0, "avg_logprob": -0.07806006390997704, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.005721117369830608}, {"id": 311, "seek": 215712, "start": 2157.8399999999997, "end": 2165.12, "text": " you know we'll be saying you know I guarantee that my large language model has been trained on", "tokens": [50400, 291, 458, 321, 603, 312, 1566, 291, 458, 286, 10815, 300, 452, 2416, 2856, 2316, 575, 668, 8895, 322, 50764], "temperature": 0.0, "avg_logprob": -0.08408167750336402, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.0003913940454367548}, {"id": 312, "seek": 215712, "start": 2165.68, "end": 2171.12, "text": " just high quality data and the same way as you see certificates of origin for you know", "tokens": [50792, 445, 1090, 3125, 1412, 293, 264, 912, 636, 382, 291, 536, 32941, 295, 4957, 337, 291, 458, 51064], "temperature": 0.0, "avg_logprob": -0.08408167750336402, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.0003913940454367548}, {"id": 313, "seek": 215712, "start": 2171.12, "end": 2178.56, "text": " fancy cheeses and things like that you know the cows grazed on grass organically raised on the", "tokens": [51064, 10247, 947, 23639, 293, 721, 411, 300, 291, 458, 264, 19148, 1295, 11312, 322, 8054, 1798, 984, 6005, 322, 264, 51436], "temperature": 0.0, "avg_logprob": -0.08408167750336402, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.0003913940454367548}, {"id": 314, "seek": 215712, "start": 2178.56, "end": 2185.04, "text": " south's southern meadows and all that sort of stuff and particularly if in the if that open source", "tokens": [51436, 7377, 311, 13456, 385, 33480, 293, 439, 300, 1333, 295, 1507, 293, 4098, 498, 294, 264, 498, 300, 1269, 4009, 51760], "temperature": 0.0, "avg_logprob": -0.08408167750336402, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.0003913940454367548}, {"id": 315, "seek": 218504, "start": 2185.04, "end": 2192.88, "text": " world that I was talking about before if that comes into play I you know I see that has been sort", "tokens": [50364, 1002, 300, 286, 390, 1417, 466, 949, 498, 300, 1487, 666, 862, 286, 291, 458, 286, 536, 300, 575, 668, 1333, 50756], "temperature": 0.0, "avg_logprob": -0.05424138307571411, "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.001097693108022213}, {"id": 316, "seek": 218504, "start": 2192.88, "end": 2198.48, "text": " of really one of the really big challenges I've I've seen how data vendors small startups are", "tokens": [50756, 295, 534, 472, 295, 264, 534, 955, 4759, 286, 600, 286, 600, 1612, 577, 1412, 22056, 1359, 28041, 366, 51036], "temperature": 0.0, "avg_logprob": -0.05424138307571411, "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.001097693108022213}, {"id": 317, "seek": 218504, "start": 2198.48, "end": 2203.36, "text": " really under incredible pressure to produce something which they can sell because they're", "tokens": [51036, 534, 833, 4651, 3321, 281, 5258, 746, 597, 436, 393, 3607, 570, 436, 434, 51280], "temperature": 0.0, "avg_logprob": -0.05424138307571411, "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.001097693108022213}, {"id": 318, "seek": 218504, "start": 2203.36, "end": 2210.48, "text": " usually cash constrained and the same thing may be true for startups that are producing large", "tokens": [51280, 2673, 6388, 38901, 293, 264, 912, 551, 815, 312, 2074, 337, 28041, 300, 366, 10501, 2416, 51636], "temperature": 0.0, "avg_logprob": -0.05424138307571411, "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.001097693108022213}, {"id": 319, "seek": 221048, "start": 2210.56, "end": 2218.08, "text": " language models they'll be under huge pressure to take somebody else's model and do a few tweaks to", "tokens": [50368, 2856, 5245, 436, 603, 312, 833, 2603, 3321, 281, 747, 2618, 1646, 311, 2316, 293, 360, 257, 1326, 46664, 281, 50744], "temperature": 0.0, "avg_logprob": -0.08321387525917827, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.005981163587421179}, {"id": 320, "seek": 221048, "start": 2218.08, "end": 2226.0, "text": " it and try to present it as something that's completely new and yeah impact on nlp jobs", "tokens": [50744, 309, 293, 853, 281, 1974, 309, 382, 746, 300, 311, 2584, 777, 293, 1338, 2712, 322, 297, 75, 79, 4782, 51140], "temperature": 0.0, "avg_logprob": -0.08321387525917827, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.005981163587421179}, {"id": 321, "seek": 221048, "start": 2227.04, "end": 2235.2, "text": " I I actually do think that it's not too far off when we'll be able to say something like", "tokens": [51192, 286, 286, 767, 360, 519, 300, 309, 311, 406, 886, 1400, 766, 562, 321, 603, 312, 1075, 281, 584, 746, 411, 51600], "temperature": 0.0, "avg_logprob": -0.08321387525917827, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.005981163587421179}, {"id": 322, "seek": 223520, "start": 2235.2, "end": 2240.48, "text": " give an instruction to a large language model it's like deploy a chatbot the task is informing", "tokens": [50364, 976, 364, 10951, 281, 257, 2416, 2856, 2316, 309, 311, 411, 7274, 257, 5081, 18870, 264, 5633, 307, 43969, 50628], "temperature": 0.0, "avg_logprob": -0.06363039252198772, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.03672443702816963}, {"id": 323, "seek": 223520, "start": 2240.48, "end": 2246.56, "text": " users about the products that you'll find listed in this database over here I want you to interact", "tokens": [50628, 5022, 466, 264, 3383, 300, 291, 603, 915, 10052, 294, 341, 8149, 670, 510, 286, 528, 291, 281, 4648, 50932], "temperature": 0.0, "avg_logprob": -0.06363039252198772, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.03672443702816963}, {"id": 324, "seek": 223520, "start": 2246.56, "end": 2252.7999999999997, "text": " with users in a professional tone emphasize customer service rather than price and politely", "tokens": [50932, 365, 5022, 294, 257, 4843, 8027, 16078, 5474, 2643, 2831, 813, 3218, 293, 1180, 1959, 51244], "temperature": 0.0, "avg_logprob": -0.06363039252198772, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.03672443702816963}, {"id": 325, "seek": 223520, "start": 2252.7999999999997, "end": 2257.8399999999997, "text": " decline to talk about topics that are related to the products and that will be it that will", "tokens": [51244, 15635, 281, 751, 466, 8378, 300, 366, 4077, 281, 264, 3383, 293, 300, 486, 312, 309, 300, 486, 51496], "temperature": 0.0, "avg_logprob": -0.06363039252198772, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.03672443702816963}, {"id": 326, "seek": 225784, "start": 2258.56, "end": 2267.6000000000004, "text": " build you a chatbot you won't need an expert development team you know I do think that however", "tokens": [50400, 1322, 291, 257, 5081, 18870, 291, 1582, 380, 643, 364, 5844, 3250, 1469, 291, 458, 286, 360, 519, 300, 4461, 50852], "temperature": 0.0, "avg_logprob": -0.08730525531988034, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.01939752884209156}, {"id": 327, "seek": 225784, "start": 2267.6000000000004, "end": 2273.6000000000004, "text": " that that's not going to come up immediately we will for the next say five years or so we will", "tokens": [50852, 300, 300, 311, 406, 516, 281, 808, 493, 4258, 321, 486, 337, 264, 958, 584, 1732, 924, 420, 370, 321, 486, 51152], "temperature": 0.0, "avg_logprob": -0.08730525531988034, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.01939752884209156}, {"id": 328, "seek": 225784, "start": 2273.6000000000004, "end": 2278.96, "text": " need people that can create training data and fine-tune models and as I said before I think", "tokens": [51152, 643, 561, 300, 393, 1884, 3097, 1412, 293, 2489, 12, 83, 2613, 5245, 293, 382, 286, 848, 949, 286, 519, 51420], "temperature": 0.0, "avg_logprob": -0.08730525531988034, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.01939752884209156}, {"id": 329, "seek": 225784, "start": 2278.96, "end": 2285.28, "text": " evaluation and testing is just going to get more and more important brought to social impacts", "tokens": [51420, 13344, 293, 4997, 307, 445, 516, 281, 483, 544, 293, 544, 1021, 3038, 281, 2093, 11606, 51736], "temperature": 0.0, "avg_logprob": -0.08730525531988034, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.01939752884209156}, {"id": 330, "seek": 228528, "start": 2285.84, "end": 2292.88, "text": " right so I think we already know that deep fakes and fraud are just going to get supercharged by", "tokens": [50392, 558, 370, 286, 519, 321, 1217, 458, 300, 2452, 283, 3419, 293, 14560, 366, 445, 516, 281, 483, 1687, 25064, 538, 50744], "temperature": 0.0, "avg_logprob": -0.09729114445773038, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.002467102138325572}, {"id": 331, "seek": 228528, "start": 2292.88, "end": 2300.96, "text": " this sort of technology and yes I think that's true I think we're going to see automation of", "tokens": [50744, 341, 1333, 295, 2899, 293, 2086, 286, 519, 300, 311, 2074, 286, 519, 321, 434, 516, 281, 536, 17769, 295, 51148], "temperature": 0.0, "avg_logprob": -0.09729114445773038, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.002467102138325572}, {"id": 332, "seek": 228528, "start": 2300.96, "end": 2307.36, "text": " jobs not previously automated Krugman has an interesting article in the New York Times just a", "tokens": [51148, 4782, 406, 8046, 18473, 6332, 697, 1601, 575, 364, 1880, 7222, 294, 264, 1873, 3609, 11366, 445, 257, 51468], "temperature": 0.0, "avg_logprob": -0.09729114445773038, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.002467102138325572}, {"id": 333, "seek": 228528, "start": 2307.36, "end": 2313.92, "text": " couple of days ago where you know he makes the point that it doesn't really matter whether these", "tokens": [51468, 1916, 295, 1708, 2057, 689, 291, 458, 415, 1669, 264, 935, 300, 309, 1177, 380, 534, 1871, 1968, 613, 51796], "temperature": 0.0, "avg_logprob": -0.09729114445773038, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.002467102138325572}, {"id": 334, "seek": 231392, "start": 2313.92, "end": 2320.16, "text": " LLMs really are intelligent or not that even a souped up auto correct can actually have", "tokens": [50364, 441, 43, 26386, 534, 366, 13232, 420, 406, 300, 754, 257, 7884, 292, 493, 8399, 3006, 393, 767, 362, 50676], "temperature": 0.0, "avg_logprob": -0.10568138028754563, "compression_ratio": 1.580110497237569, "no_speech_prob": 0.0036260862834751606}, {"id": 335, "seek": 231392, "start": 2320.8, "end": 2326.32, "text": " quite major implications in terms of productivity he's actually really quite positive he seems to", "tokens": [50708, 1596, 2563, 16602, 294, 2115, 295, 15604, 415, 311, 767, 534, 1596, 3353, 415, 2544, 281, 50984], "temperature": 0.0, "avg_logprob": -0.10568138028754563, "compression_ratio": 1.580110497237569, "no_speech_prob": 0.0036260862834751606}, {"id": 336, "seek": 231392, "start": 2326.32, "end": 2336.08, "text": " think that actually these things might you know level society somewhat and they might I mean there's", "tokens": [50984, 519, 300, 767, 613, 721, 1062, 291, 458, 1496, 4086, 8344, 293, 436, 1062, 286, 914, 456, 311, 51472], "temperature": 0.0, "avg_logprob": -0.10568138028754563, "compression_ratio": 1.580110497237569, "no_speech_prob": 0.0036260862834751606}, {"id": 337, "seek": 233608, "start": 2336.08, "end": 2345.12, "text": " some evidence that in fact the GPT-4 for example enables poorer workers to work at a higher standard", "tokens": [50364, 512, 4467, 300, 294, 1186, 264, 26039, 51, 12, 19, 337, 1365, 17077, 49740, 5600, 281, 589, 412, 257, 2946, 3832, 50816], "temperature": 0.0, "avg_logprob": -0.07753230564629854, "compression_ratio": 1.5260115606936415, "no_speech_prob": 0.014340400695800781}, {"id": 338, "seek": 233608, "start": 2346.7999999999997, "end": 2352.24, "text": " whereas the best workers are helped less by GPT-4 maybe that's the case", "tokens": [50900, 9735, 264, 1151, 5600, 366, 4254, 1570, 538, 26039, 51, 12, 19, 1310, 300, 311, 264, 1389, 51172], "temperature": 0.0, "avg_logprob": -0.07753230564629854, "compression_ratio": 1.5260115606936415, "no_speech_prob": 0.014340400695800781}, {"id": 339, "seek": 233608, "start": 2354.48, "end": 2361.68, "text": " I think there's a number of risks I you know I think we are seeing you know AI models being", "tokens": [51284, 286, 519, 456, 311, 257, 1230, 295, 10888, 286, 291, 458, 286, 519, 321, 366, 2577, 291, 458, 7318, 5245, 885, 51644], "temperature": 0.0, "avg_logprob": -0.07753230564629854, "compression_ratio": 1.5260115606936415, "no_speech_prob": 0.014340400695800781}, {"id": 340, "seek": 236168, "start": 2361.68, "end": 2367.52, "text": " trained on public domain data that the creators when they made their data public really had no", "tokens": [50364, 8895, 322, 1908, 9274, 1412, 300, 264, 16039, 562, 436, 1027, 641, 1412, 1908, 534, 632, 572, 50656], "temperature": 0.0, "avg_logprob": -0.06970715817110038, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.01821649633347988}, {"id": 341, "seek": 236168, "start": 2368.08, "end": 2374.72, "text": " intention no one no expectation of their data would be used in this way we're seeing a political", "tokens": [50684, 7789, 572, 472, 572, 14334, 295, 641, 1412, 576, 312, 1143, 294, 341, 636, 321, 434, 2577, 257, 3905, 51016], "temperature": 0.0, "avg_logprob": -0.06970715817110038, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.01821649633347988}, {"id": 342, "seek": 236168, "start": 2374.72, "end": 2379.68, "text": " fight right now between media companies and tech companies about the use of data I think that's", "tokens": [51016, 2092, 558, 586, 1296, 3021, 3431, 293, 7553, 3431, 466, 264, 764, 295, 1412, 286, 519, 300, 311, 51264], "temperature": 0.0, "avg_logprob": -0.06970715817110038, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.01821649633347988}, {"id": 343, "seek": 236168, "start": 2379.68, "end": 2388.16, "text": " still mainly about search rather than training AI models but that same fight I think will continue", "tokens": [51264, 920, 8704, 466, 3164, 2831, 813, 3097, 7318, 5245, 457, 300, 912, 2092, 286, 519, 486, 2354, 51688], "temperature": 0.0, "avg_logprob": -0.06970715817110038, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.01821649633347988}, {"id": 344, "seek": 238816, "start": 2388.16, "end": 2395.2799999999997, "text": " I looking back to the first industrial revolution and things like the tragedy of the commons I don't", "tokens": [50364, 286, 1237, 646, 281, 264, 700, 9987, 8894, 293, 721, 411, 264, 18563, 295, 264, 800, 892, 286, 500, 380, 50720], "temperature": 0.0, "avg_logprob": -0.15949816984288834, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.0067707630805671215}, {"id": 345, "seek": 238816, "start": 2395.2799999999997, "end": 2400.56, "text": " see any particular reason to expect a socially optimal outcome although I think the writer's", "tokens": [50720, 536, 604, 1729, 1778, 281, 2066, 257, 21397, 16252, 9700, 4878, 286, 519, 264, 9936, 311, 50984], "temperature": 0.0, "avg_logprob": -0.15949816984288834, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.0067707630805671215}, {"id": 346, "seek": 238816, "start": 2400.56, "end": 2406.56, "text": " Guild of America settlement actually sounds like it's a pretty forward-looking one and", "tokens": [50984, 38968, 295, 3374, 18130, 767, 3263, 411, 309, 311, 257, 1238, 2128, 12, 16129, 472, 293, 51284], "temperature": 0.0, "avg_logprob": -0.15949816984288834, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.0067707630805671215}, {"id": 347, "seek": 238816, "start": 2407.52, "end": 2414.3999999999996, "text": " I'm very pleased to see that I I do think you know I I'm not one of these people that that", "tokens": [51332, 286, 478, 588, 10587, 281, 536, 300, 286, 286, 360, 519, 291, 458, 286, 286, 478, 406, 472, 295, 613, 561, 300, 300, 51676], "temperature": 0.0, "avg_logprob": -0.15949816984288834, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.0067707630805671215}, {"id": 348, "seek": 241440, "start": 2414.4, "end": 2422.4, "text": " poo-poo's the people that are worried about you know AGI and misalignment I don't think we're likely", "tokens": [50364, 36743, 12, 79, 1986, 311, 264, 561, 300, 366, 5804, 466, 291, 458, 316, 26252, 293, 3346, 304, 41134, 286, 500, 380, 519, 321, 434, 3700, 50764], "temperature": 0.0, "avg_logprob": -0.08241533447097946, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.005094556137919426}, {"id": 349, "seek": 241440, "start": 2422.4, "end": 2428.64, "text": " to be made extinction but to be made extinct but I do think we should be worrying about that", "tokens": [50764, 281, 312, 1027, 33163, 457, 281, 312, 1027, 35094, 457, 286, 360, 519, 321, 820, 312, 18788, 466, 300, 51076], "temperature": 0.0, "avg_logprob": -0.08241533447097946, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.005094556137919426}, {"id": 350, "seek": 241440, "start": 2428.64, "end": 2432.56, "text": " and the final point I'd like to make is that I think these things are economic and political", "tokens": [51076, 293, 264, 2572, 935, 286, 1116, 411, 281, 652, 307, 300, 286, 519, 613, 721, 366, 4836, 293, 3905, 51272], "temperature": 0.0, "avg_logprob": -0.08241533447097946, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.005094556137919426}, {"id": 351, "seek": 241440, "start": 2432.56, "end": 2440.1600000000003, "text": " choices not really technical choices so it's an interesting question so those of us that actually", "tokens": [51272, 7994, 406, 534, 6191, 7994, 370, 309, 311, 364, 1880, 1168, 370, 729, 295, 505, 300, 767, 51652], "temperature": 0.0, "avg_logprob": -0.08241533447097946, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.005094556137919426}, {"id": 352, "seek": 244016, "start": 2440.16, "end": 2445.68, "text": " have the technical expertise probably are in a position to have our voices heard more than what", "tokens": [50364, 362, 264, 6191, 11769, 1391, 366, 294, 257, 2535, 281, 362, 527, 9802, 2198, 544, 813, 437, 50640], "temperature": 0.0, "avg_logprob": -0.06520785225762261, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.004114761482924223}, {"id": 353, "seek": 244016, "start": 2445.68, "end": 2452.56, "text": " they would normally be so we should probably make use of that but I actually really do think that", "tokens": [50640, 436, 576, 5646, 312, 370, 321, 820, 1391, 652, 764, 295, 300, 457, 286, 767, 534, 360, 519, 300, 50984], "temperature": 0.0, "avg_logprob": -0.06520785225762261, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.004114761482924223}, {"id": 354, "seek": 244016, "start": 2453.12, "end": 2460.56, "text": " it's not just up to the the tech companies in particular to try and make the important decisions", "tokens": [51012, 309, 311, 406, 445, 493, 281, 264, 264, 7553, 3431, 294, 1729, 281, 853, 293, 652, 264, 1021, 5327, 51384], "temperature": 0.0, "avg_logprob": -0.06520785225762261, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.004114761482924223}, {"id": 355, "seek": 246056, "start": 2460.56, "end": 2470.48, "text": " here so conclusions I think LLM's are here to stay a lot of my people my age remember the AI", "tokens": [50364, 510, 370, 22865, 286, 519, 441, 43, 44, 311, 366, 510, 281, 1754, 257, 688, 295, 452, 561, 452, 3205, 1604, 264, 7318, 50860], "temperature": 0.0, "avg_logprob": -0.07497005901117434, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.007562373764812946}, {"id": 356, "seek": 246056, "start": 2470.48, "end": 2475.92, "text": " winter of the last century I don't think there's going to be an AI winter just simply because", "tokens": [50860, 6355, 295, 264, 1036, 4901, 286, 500, 380, 519, 456, 311, 516, 281, 312, 364, 7318, 6355, 445, 2935, 570, 51132], "temperature": 0.0, "avg_logprob": -0.07497005901117434, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.007562373764812946}, {"id": 357, "seek": 246056, "start": 2477.04, "end": 2483.7599999999998, "text": " these things are actually way too useful for students intellectual revolutions are a great", "tokens": [51188, 613, 721, 366, 767, 636, 886, 4420, 337, 1731, 12576, 3698, 15892, 366, 257, 869, 51524], "temperature": 0.0, "avg_logprob": -0.07497005901117434, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.007562373764812946}, {"id": 358, "seek": 246056, "start": 2483.7599999999998, "end": 2489.68, "text": " time to enter the field because in fact actually the amount of knowledge you need to have to become", "tokens": [51524, 565, 281, 3242, 264, 2519, 570, 294, 1186, 767, 264, 2372, 295, 3601, 291, 643, 281, 362, 281, 1813, 51820], "temperature": 0.0, "avg_logprob": -0.07497005901117434, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.007562373764812946}, {"id": 359, "seek": 248968, "start": 2489.68, "end": 2497.9199999999996, "text": " an expert is much much less I think LLM's open up new interesting scientific research questions", "tokens": [50364, 364, 5844, 307, 709, 709, 1570, 286, 519, 441, 43, 44, 311, 1269, 493, 777, 1880, 8134, 2132, 1651, 50776], "temperature": 0.0, "avg_logprob": -0.08186531066894531, "compression_ratio": 1.5857988165680474, "no_speech_prob": 0.0012566206278279424}, {"id": 360, "seek": 248968, "start": 2497.9199999999996, "end": 2506.3999999999996, "text": " and directions NLP I think will have less emphasis on clever new algorithms and more on", "tokens": [50776, 293, 11095, 426, 45196, 286, 519, 486, 362, 1570, 16271, 322, 13494, 777, 14642, 293, 544, 322, 51200], "temperature": 0.0, "avg_logprob": -0.08186531066894531, "compression_ratio": 1.5857988165680474, "no_speech_prob": 0.0012566206278279424}, {"id": 361, "seek": 248968, "start": 2507.04, "end": 2513.44, "text": " yeah interaction and integration of models applications data design and training and", "tokens": [51232, 1338, 9285, 293, 10980, 295, 5245, 5821, 1412, 1715, 293, 3097, 293, 51552], "temperature": 0.0, "avg_logprob": -0.08186531066894531, "compression_ratio": 1.5857988165680474, "no_speech_prob": 0.0012566206278279424}, {"id": 362, "seek": 251344, "start": 2513.44, "end": 2519.12, "text": " much more emphasis on evaluation so thank you very much", "tokens": [50364, 709, 544, 16271, 322, 13344, 370, 1309, 291, 588, 709, 50648], "temperature": 0.0, "avg_logprob": -0.16377842183015784, "compression_ratio": 1.5106382978723405, "no_speech_prob": 0.003970496356487274}, {"id": 363, "seek": 251344, "start": 2525.84, "end": 2532.32, "text": " any questions or comments yes hi thank you for great talk super interesting I just wanted to", "tokens": [50984, 604, 1651, 420, 3053, 2086, 4879, 1309, 291, 337, 869, 751, 1687, 1880, 286, 445, 1415, 281, 51308], "temperature": 0.0, "avg_logprob": -0.16377842183015784, "compression_ratio": 1.5106382978723405, "no_speech_prob": 0.003970496356487274}, {"id": 364, "seek": 251344, "start": 2533.52, "end": 2537.28, "text": " ask you about one thing in the middle of the talk which is about", "tokens": [51368, 1029, 291, 466, 472, 551, 294, 264, 2808, 295, 264, 751, 597, 307, 466, 51556], "temperature": 0.0, "avg_logprob": -0.16377842183015784, "compression_ratio": 1.5106382978723405, "no_speech_prob": 0.003970496356487274}, {"id": 365, "seek": 253728, "start": 2538.0800000000004, "end": 2545.6800000000003, "text": " this kind of neuro symbolic integration and you had this you had this kind of proposal that LLM's", "tokens": [50404, 341, 733, 295, 16499, 25755, 10980, 293, 291, 632, 341, 291, 632, 341, 733, 295, 11494, 300, 441, 43, 44, 311, 50784], "temperature": 0.0, "avg_logprob": -0.1299004498650046, "compression_ratio": 1.7738693467336684, "no_speech_prob": 0.10995904356241226}, {"id": 366, "seek": 253728, "start": 2545.6800000000003, "end": 2552.48, "text": " are going to give you parts of bits and then you're going to use those inputs into a logic model", "tokens": [50784, 366, 516, 281, 976, 291, 3166, 295, 9239, 293, 550, 291, 434, 516, 281, 764, 729, 15743, 666, 257, 9952, 2316, 51124], "temperature": 0.0, "avg_logprob": -0.1299004498650046, "compression_ratio": 1.7738693467336684, "no_speech_prob": 0.10995904356241226}, {"id": 367, "seek": 253728, "start": 2553.92, "end": 2557.84, "text": " and I wondered like why do you think I mean personally I like", "tokens": [51196, 293, 286, 17055, 411, 983, 360, 291, 519, 286, 914, 5665, 286, 411, 51392], "temperature": 0.0, "avg_logprob": -0.1299004498650046, "compression_ratio": 1.7738693467336684, "no_speech_prob": 0.10995904356241226}, {"id": 368, "seek": 253728, "start": 2558.48, "end": 2564.5600000000004, "text": " neuro symbolic learning so but I'm interested today like why why do you think that's a good idea", "tokens": [51424, 16499, 25755, 2539, 370, 457, 286, 478, 3102, 965, 411, 983, 983, 360, 291, 519, 300, 311, 257, 665, 1558, 51728], "temperature": 0.0, "avg_logprob": -0.1299004498650046, "compression_ratio": 1.7738693467336684, "no_speech_prob": 0.10995904356241226}, {"id": 369, "seek": 256456, "start": 2564.56, "end": 2570.32, "text": " or good approach like why not just let the LLM do the entire thing like what what is it that", "tokens": [50364, 420, 665, 3109, 411, 983, 406, 445, 718, 264, 441, 43, 44, 360, 264, 2302, 551, 411, 437, 437, 307, 309, 300, 50652], "temperature": 0.0, "avg_logprob": -0.09793492936596428, "compression_ratio": 1.8221153846153846, "no_speech_prob": 0.012862653471529484}, {"id": 370, "seek": 256456, "start": 2571.12, "end": 2577.12, "text": " so certainly certainly there are people that are betting yeah um you know let's just let the LLM", "tokens": [50692, 370, 3297, 3297, 456, 366, 561, 300, 366, 34246, 1338, 1105, 291, 458, 718, 311, 445, 718, 264, 441, 43, 44, 50992], "temperature": 0.0, "avg_logprob": -0.09793492936596428, "compression_ratio": 1.8221153846153846, "no_speech_prob": 0.012862653471529484}, {"id": 371, "seek": 256456, "start": 2577.12, "end": 2585.7599999999998, "text": " do the entire thing yeah um I guess the answer I would give there is that there are a lot of", "tokens": [50992, 360, 264, 2302, 551, 1338, 1105, 286, 2041, 264, 1867, 286, 576, 976, 456, 307, 300, 456, 366, 257, 688, 295, 51424], "temperature": 0.0, "avg_logprob": -0.09793492936596428, "compression_ratio": 1.8221153846153846, "no_speech_prob": 0.012862653471529484}, {"id": 372, "seek": 256456, "start": 2585.7599999999998, "end": 2592.08, "text": " academics and in fact I'm sure many of you've seen this stuff right it's it's now quite you know", "tokens": [51424, 25695, 293, 294, 1186, 286, 478, 988, 867, 295, 291, 600, 1612, 341, 1507, 558, 309, 311, 309, 311, 586, 1596, 291, 458, 51740], "temperature": 0.0, "avg_logprob": -0.09793492936596428, "compression_ratio": 1.8221153846153846, "no_speech_prob": 0.012862653471529484}, {"id": 373, "seek": 259208, "start": 2592.16, "end": 2599.12, "text": " there's like a little mini industry of people coming up with things like you know chat GPT", "tokens": [50368, 456, 311, 411, 257, 707, 8382, 3518, 295, 561, 1348, 493, 365, 721, 411, 291, 458, 5081, 26039, 51, 50716], "temperature": 0.0, "avg_logprob": -0.11608377624960507, "compression_ratio": 1.4972677595628416, "no_speech_prob": 0.0065117329359054565}, {"id": 374, "seek": 259208, "start": 2599.12, "end": 2609.92, "text": " cannot understand negation GPT for does not understand X is Y statements you know and in fact", "tokens": [50716, 2644, 1223, 2485, 399, 26039, 51, 337, 775, 406, 1223, 1783, 307, 398, 12363, 291, 458, 293, 294, 1186, 51256], "temperature": 0.0, "avg_logprob": -0.11608377624960507, "compression_ratio": 1.4972677595628416, "no_speech_prob": 0.0065117329359054565}, {"id": 375, "seek": 259208, "start": 2609.92, "end": 2619.2, "text": " actually I don't think I'm ashamed but we have a paper that is claiming that LLM's cannot", "tokens": [51256, 767, 286, 500, 380, 519, 286, 478, 19489, 457, 321, 362, 257, 3035, 300, 307, 19232, 300, 441, 43, 44, 311, 2644, 51720], "temperature": 0.0, "avg_logprob": -0.11608377624960507, "compression_ratio": 1.4972677595628416, "no_speech_prob": 0.0065117329359054565}, {"id": 376, "seek": 261920, "start": 2619.2, "end": 2625.68, "text": " understand you know do not really properly understand entailment you know that walking", "tokens": [50364, 1223, 291, 458, 360, 406, 534, 6108, 1223, 948, 864, 518, 291, 458, 300, 4494, 50688], "temperature": 0.0, "avg_logprob": -0.10003672576532131, "compression_ratio": 1.878787878787879, "no_speech_prob": 0.0007407472003251314}, {"id": 377, "seek": 261920, "start": 2625.68, "end": 2634.56, "text": " entails moving but moving does not always entail walking um so if you really believe that stuff", "tokens": [50688, 50133, 2684, 457, 2684, 775, 406, 1009, 948, 864, 4494, 1105, 370, 498, 291, 534, 1697, 300, 1507, 51132], "temperature": 0.0, "avg_logprob": -0.10003672576532131, "compression_ratio": 1.878787878787879, "no_speech_prob": 0.0007407472003251314}, {"id": 378, "seek": 261920, "start": 2634.56, "end": 2640.0, "text": " if you really believe you know and if you believe it enough so that you actually think that GPT 5", "tokens": [51132, 498, 291, 534, 1697, 291, 458, 293, 498, 291, 1697, 309, 1547, 370, 300, 291, 767, 519, 300, 26039, 51, 1025, 51404], "temperature": 0.0, "avg_logprob": -0.10003672576532131, "compression_ratio": 1.878787878787879, "no_speech_prob": 0.0007407472003251314}, {"id": 379, "seek": 261920, "start": 2640.0, "end": 2644.24, "text": " or whatever the next model is that comes out is going to have to exactly that same weakness", "tokens": [51404, 420, 2035, 264, 958, 2316, 307, 300, 1487, 484, 307, 516, 281, 362, 281, 2293, 300, 912, 12772, 51616], "temperature": 0.0, "avg_logprob": -0.10003672576532131, "compression_ratio": 1.878787878787879, "no_speech_prob": 0.0007407472003251314}, {"id": 380, "seek": 264424, "start": 2644.9599999999996, "end": 2652.72, "text": " the idea then is build a symbolic component that addresses whatever weakness you happen to think", "tokens": [50400, 264, 1558, 550, 307, 1322, 257, 25755, 6542, 300, 16862, 2035, 12772, 291, 1051, 281, 519, 50788], "temperature": 0.0, "avg_logprob": -0.08440784198134693, "compression_ratio": 1.5519125683060109, "no_speech_prob": 0.012383915483951569}, {"id": 381, "seek": 264424, "start": 2652.72, "end": 2659.68, "text": " these large language models have but it isn't like I'll admit it is rather risky because these", "tokens": [50788, 613, 2416, 2856, 5245, 362, 457, 309, 1943, 380, 411, 286, 603, 9796, 309, 307, 2831, 21137, 570, 613, 51136], "temperature": 0.0, "avg_logprob": -0.08440784198134693, "compression_ratio": 1.5519125683060109, "no_speech_prob": 0.012383915483951569}, {"id": 382, "seek": 264424, "start": 2659.68, "end": 2669.04, "text": " things are improving so rapidly and I'm not so sure I mean it's it's a risk if you're a grad", "tokens": [51136, 721, 366, 11470, 370, 12910, 293, 286, 478, 406, 370, 988, 286, 914, 309, 311, 309, 311, 257, 3148, 498, 291, 434, 257, 2771, 51604], "temperature": 0.0, "avg_logprob": -0.08440784198134693, "compression_ratio": 1.5519125683060109, "no_speech_prob": 0.012383915483951569}, {"id": 383, "seek": 266904, "start": 2669.04, "end": 2675.2, "text": " student to say all right I'm going to commit the next year or two of my life to working on solving", "tokens": [50364, 3107, 281, 584, 439, 558, 286, 478, 516, 281, 5599, 264, 958, 1064, 420, 732, 295, 452, 993, 281, 1364, 322, 12606, 50672], "temperature": 0.0, "avg_logprob": -0.06194538419896906, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.020235680043697357}, {"id": 384, "seek": 266904, "start": 2675.2, "end": 2682.56, "text": " you know the problem of negation in large language models and halfway through your research project", "tokens": [50672, 291, 458, 264, 1154, 295, 2485, 399, 294, 2416, 2856, 5245, 293, 15461, 807, 428, 2132, 1716, 51040], "temperature": 0.0, "avg_logprob": -0.06194538419896906, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.020235680043697357}, {"id": 385, "seek": 266904, "start": 2683.92, "end": 2689.6, "text": " you know someone discovers that just by scaling up the training data another order of magnitude", "tokens": [51108, 291, 458, 1580, 44522, 300, 445, 538, 21589, 493, 264, 3097, 1412, 1071, 1668, 295, 15668, 51392], "temperature": 0.0, "avg_logprob": -0.06194538419896906, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.020235680043697357}, {"id": 386, "seek": 266904, "start": 2689.6, "end": 2695.7599999999998, "text": " all of a sudden now it's going to handle negation just perfectly and that's what I meant by in fact", "tokens": [51392, 439, 295, 257, 3990, 586, 309, 311, 516, 281, 4813, 2485, 399, 445, 6239, 293, 300, 311, 437, 286, 4140, 538, 294, 1186, 51700], "temperature": 0.0, "avg_logprob": -0.06194538419896906, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.020235680043697357}, {"id": 387, "seek": 269576, "start": 2695.76, "end": 2702.0, "text": " I think that was one of the one of the slides that Kate added was saying that it's particularly if", "tokens": [50364, 286, 519, 300, 390, 472, 295, 264, 472, 295, 264, 9788, 300, 16251, 3869, 390, 1566, 300, 309, 311, 4098, 498, 50676], "temperature": 0.0, "avg_logprob": -0.12120004410439349, "compression_ratio": 1.8632075471698113, "no_speech_prob": 0.009679603390395641}, {"id": 388, "seek": 269576, "start": 2702.0, "end": 2707.6800000000003, "text": " you wanted to do something some sort of behavioral research or something else like that you know where", "tokens": [50676, 291, 1415, 281, 360, 746, 512, 1333, 295, 19124, 2132, 420, 746, 1646, 411, 300, 291, 458, 689, 50960], "temperature": 0.0, "avg_logprob": -0.12120004410439349, "compression_ratio": 1.8632075471698113, "no_speech_prob": 0.009679603390395641}, {"id": 389, "seek": 269576, "start": 2708.6400000000003, "end": 2714.88, "text": " this the timescale which LLMs are changing versus the timescale of doing behavioral research is", "tokens": [51008, 341, 264, 1413, 37088, 597, 441, 43, 26386, 366, 4473, 5717, 264, 1413, 37088, 295, 884, 19124, 2132, 307, 51320], "temperature": 0.0, "avg_logprob": -0.12120004410439349, "compression_ratio": 1.8632075471698113, "no_speech_prob": 0.009679603390395641}, {"id": 390, "seek": 269576, "start": 2715.76, "end": 2721.76, "text": " the LLMs are just changing so fast that if you if you looked at today's LLMs and said okay you're", "tokens": [51364, 264, 441, 43, 26386, 366, 445, 4473, 370, 2370, 300, 498, 291, 498, 291, 2956, 412, 965, 311, 441, 43, 26386, 293, 848, 1392, 291, 434, 51664], "temperature": 0.0, "avg_logprob": -0.12120004410439349, "compression_ratio": 1.8632075471698113, "no_speech_prob": 0.009679603390395641}, {"id": 391, "seek": 272176, "start": 2721.76, "end": 2727.28, "text": " inspired by then here are some interesting behavioral predictions that they're making I'm", "tokens": [50364, 7547, 538, 550, 510, 366, 512, 1880, 19124, 21264, 300, 436, 434, 1455, 286, 478, 50640], "temperature": 0.0, "avg_logprob": -0.08281959295272827, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.004569939337670803}, {"id": 392, "seek": 272176, "start": 2727.28, "end": 2731.76, "text": " going to go out and start running some experiments with kids or something like that", "tokens": [50640, 516, 281, 352, 484, 293, 722, 2614, 512, 12050, 365, 2301, 420, 746, 411, 300, 50864], "temperature": 0.0, "avg_logprob": -0.08281959295272827, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.004569939337670803}, {"id": 393, "seek": 272176, "start": 2734.0, "end": 2738.5600000000004, "text": " yeah you know by the time you've collected a quarter of your data there's another model out", "tokens": [50976, 1338, 291, 458, 538, 264, 565, 291, 600, 11087, 257, 6555, 295, 428, 1412, 456, 311, 1071, 2316, 484, 51204], "temperature": 0.0, "avg_logprob": -0.08281959295272827, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.004569939337670803}, {"id": 394, "seek": 272176, "start": 2738.5600000000004, "end": 2745.6000000000004, "text": " there and it's got it's making different behavioral predictions it's just I don't know what the answer", "tokens": [51204, 456, 293, 309, 311, 658, 309, 311, 1455, 819, 19124, 21264, 309, 311, 445, 286, 500, 380, 458, 437, 264, 1867, 51556], "temperature": 0.0, "avg_logprob": -0.08281959295272827, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.004569939337670803}, {"id": 395, "seek": 274560, "start": 2745.6, "end": 2759.44, "text": " there is except to say that yeah yeah you might just um ask a a follow-up question so if if you", "tokens": [50364, 456, 307, 3993, 281, 584, 300, 1338, 1338, 291, 1062, 445, 1105, 1029, 257, 257, 1524, 12, 1010, 1168, 370, 498, 498, 291, 51056], "temperature": 0.0, "avg_logprob": -0.2170250415802002, "compression_ratio": 1.4028776978417266, "no_speech_prob": 0.006438771262764931}, {"id": 396, "seek": 274560, "start": 2759.44, "end": 2772.08, "text": " have a chat system I think that's actually for the zooms but I they also said there's a whole array", "tokens": [51056, 362, 257, 5081, 1185, 286, 519, 300, 311, 767, 337, 264, 5721, 4785, 457, 286, 436, 611, 848, 456, 311, 257, 1379, 10225, 51688], "temperature": 0.0, "avg_logprob": -0.2170250415802002, "compression_ratio": 1.4028776978417266, "no_speech_prob": 0.006438771262764931}, {"id": 397, "seek": 277208, "start": 2772.24, "end": 2780.88, "text": " microphone inside the box so well um I'll just talk about um if as opposed to um adding a symbolic", "tokens": [50372, 10952, 1854, 264, 2424, 370, 731, 1105, 286, 603, 445, 751, 466, 1105, 498, 382, 8851, 281, 1105, 5127, 257, 25755, 50804], "temperature": 0.0, "avg_logprob": -0.17895721435546874, "compression_ratio": 1.6373626373626373, "no_speech_prob": 0.026332953944802284}, {"id": 398, "seek": 277208, "start": 2780.88, "end": 2790.3199999999997, "text": " system to to a chat system or you know an LLM um what about fine tuning and just doing lots of fine", "tokens": [50804, 1185, 281, 281, 257, 5081, 1185, 420, 291, 458, 364, 441, 43, 44, 1105, 437, 466, 2489, 15164, 293, 445, 884, 3195, 295, 2489, 51276], "temperature": 0.0, "avg_logprob": -0.17895721435546874, "compression_ratio": 1.6373626373626373, "no_speech_prob": 0.026332953944802284}, {"id": 399, "seek": 277208, "start": 2790.3199999999997, "end": 2797.2799999999997, "text": " tuning instead I mean that's adding more data but if you fine tune it yes with that kind of data as", "tokens": [51276, 15164, 2602, 286, 914, 300, 311, 5127, 544, 1412, 457, 498, 291, 2489, 10864, 309, 2086, 365, 300, 733, 295, 1412, 382, 51624], "temperature": 0.0, "avg_logprob": -0.17895721435546874, "compression_ratio": 1.6373626373626373, "no_speech_prob": 0.026332953944802284}, {"id": 400, "seek": 279728, "start": 2797.28, "end": 2802.88, "text": " opposed to going as symbolic group that's right wouldn't that be I think that's I think that's", "tokens": [50364, 8851, 281, 516, 382, 25755, 1594, 300, 311, 558, 2759, 380, 300, 312, 286, 519, 300, 311, 286, 519, 300, 311, 50644], "temperature": 0.0, "avg_logprob": -0.16448609258087588, "compression_ratio": 1.7065868263473054, "no_speech_prob": 0.0020767436362802982}, {"id": 401, "seek": 279728, "start": 2802.88, "end": 2811.36, "text": " very true and in fact that I think that's a good question is you know if so right now what was it", "tokens": [50644, 588, 2074, 293, 294, 1186, 300, 286, 519, 300, 311, 257, 665, 1168, 307, 291, 458, 498, 370, 558, 586, 437, 390, 309, 51068], "temperature": 0.0, "avg_logprob": -0.16448609258087588, "compression_ratio": 1.7065868263473054, "no_speech_prob": 0.0020767436362802982}, {"id": 402, "seek": 279728, "start": 2811.36, "end": 2818.88, "text": " you know Gary Marcus is picking up on the fact that uh somebody wrote a paper that said that", "tokens": [51068, 291, 458, 13788, 26574, 307, 8867, 493, 322, 264, 1186, 300, 2232, 2618, 4114, 257, 3035, 300, 848, 300, 51444], "temperature": 0.0, "avg_logprob": -0.16448609258087588, "compression_ratio": 1.7065868263473054, "no_speech_prob": 0.0020767436362802982}, {"id": 403, "seek": 281888, "start": 2819.6, "end": 2827.2000000000003, "text": " uh oh look there's a whole lot of cases where the large language models uh will quite happily say", "tokens": [50400, 2232, 1954, 574, 456, 311, 257, 1379, 688, 295, 3331, 689, 264, 2416, 2856, 5245, 2232, 486, 1596, 19909, 584, 50780], "temperature": 0.0, "avg_logprob": -0.13764471247576285, "compression_ratio": 1.5078534031413613, "no_speech_prob": 0.008943464607000351}, {"id": 404, "seek": 281888, "start": 2827.2000000000003, "end": 2834.8, "text": " that oh let's see all right I don't know enough about celebrities unfortunately but you know", "tokens": [50780, 300, 1954, 718, 311, 536, 439, 558, 286, 500, 380, 458, 1547, 466, 23200, 7015, 457, 291, 458, 51160], "temperature": 0.0, "avg_logprob": -0.13764471247576285, "compression_ratio": 1.5078534031413613, "no_speech_prob": 0.008943464607000351}, {"id": 405, "seek": 281888, "start": 2834.8, "end": 2843.44, "text": " so-and-so and so is Tom Cruise's mother right okay it accepts that statement and you then ask the", "tokens": [51160, 370, 12, 474, 12, 539, 293, 370, 307, 5041, 39165, 311, 2895, 558, 1392, 309, 33538, 300, 5629, 293, 291, 550, 1029, 264, 51592], "temperature": 0.0, "avg_logprob": -0.13764471247576285, "compression_ratio": 1.5078534031413613, "no_speech_prob": 0.008943464607000351}, {"id": 406, "seek": 284344, "start": 2843.44, "end": 2850.64, "text": " other you then ask the question who is Tom Cruise's mother and it says I have no idea so", "tokens": [50364, 661, 291, 550, 1029, 264, 1168, 567, 307, 5041, 39165, 311, 2895, 293, 309, 1619, 286, 362, 572, 1558, 370, 50724], "temperature": 0.0, "avg_logprob": -0.12330800465175083, "compression_ratio": 1.5617977528089888, "no_speech_prob": 0.0024661875795572996}, {"id": 407, "seek": 284344, "start": 2851.52, "end": 2857.44, "text": " and it seems like oh well if I've actually got a couple of comments there so first of all it seems", "tokens": [50768, 293, 309, 2544, 411, 1954, 731, 498, 286, 600, 767, 658, 257, 1916, 295, 3053, 456, 370, 700, 295, 439, 309, 2544, 51064], "temperature": 0.0, "avg_logprob": -0.12330800465175083, "compression_ratio": 1.5617977528089888, "no_speech_prob": 0.0024661875795572996}, {"id": 408, "seek": 284344, "start": 2857.44, "end": 2864.56, "text": " like x is y that looks an awful lot to a math mathematical person as being like x equals y", "tokens": [51064, 411, 2031, 307, 288, 300, 1542, 364, 11232, 688, 281, 257, 5221, 18894, 954, 382, 885, 411, 2031, 6915, 288, 51420], "temperature": 0.0, "avg_logprob": -0.12330800465175083, "compression_ratio": 1.5617977528089888, "no_speech_prob": 0.0024661875795572996}, {"id": 409, "seek": 286456, "start": 2865.12, "end": 2872.56, "text": " and we know that what was it uh equality is what is it commutative you know so if x equals y then", "tokens": [50392, 293, 321, 458, 300, 437, 390, 309, 2232, 14949, 307, 437, 307, 309, 800, 325, 1166, 291, 458, 370, 498, 2031, 6915, 288, 550, 50764], "temperature": 0.0, "avg_logprob": -0.10469706853230794, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.022590013220906258}, {"id": 410, "seek": 286456, "start": 2872.56, "end": 2884.96, "text": " y equals x um in fact actually x is a y actually really isn't commutative um you know uh you know", "tokens": [50764, 288, 6915, 2031, 1105, 294, 1186, 767, 2031, 307, 257, 288, 767, 534, 1943, 380, 800, 325, 1166, 1105, 291, 458, 2232, 291, 458, 51384], "temperature": 0.0, "avg_logprob": -0.10469706853230794, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.022590013220906258}, {"id": 411, "seek": 288496, "start": 2885.36, "end": 2894.8, "text": " uh what was it you know chicken salad is a wonderful meal you know that can be true but", "tokens": [50384, 2232, 437, 390, 309, 291, 458, 4662, 12604, 307, 257, 3715, 6791, 291, 458, 300, 393, 312, 2074, 457, 50856], "temperature": 0.0, "avg_logprob": -0.17781732632563665, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.00525316596031189}, {"id": 412, "seek": 288496, "start": 2894.8, "end": 2901.68, "text": " a wonderful meal doesn't always have to be chicken salad right um you know so uh", "tokens": [50856, 257, 3715, 6791, 1177, 380, 1009, 362, 281, 312, 4662, 12604, 558, 1105, 291, 458, 370, 2232, 51200], "temperature": 0.0, "avg_logprob": -0.17781732632563665, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.00525316596031189}, {"id": 413, "seek": 288496, "start": 2906.96, "end": 2909.28, "text": " but anyway so might you know the yeah", "tokens": [51464, 457, 4033, 370, 1062, 291, 458, 264, 1338, 51580], "temperature": 0.0, "avg_logprob": -0.17781732632563665, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.00525316596031189}, {"id": 414, "seek": 290928, "start": 2909.28, "end": 2919.36, "text": " do you think uh do you think there's going to be uh kind of I mean what what do you think about", "tokens": [50364, 360, 291, 519, 2232, 360, 291, 519, 456, 311, 516, 281, 312, 2232, 733, 295, 286, 914, 437, 437, 360, 291, 519, 466, 50868], "temperature": 0.0, "avg_logprob": -0.16860005060831706, "compression_ratio": 1.9183673469387754, "no_speech_prob": 0.01474273856729269}, {"id": 415, "seek": 290928, "start": 2919.36, "end": 2927.52, "text": " like the kind of vision vision language models where you're kind of train it on like a ton of", "tokens": [50868, 411, 264, 733, 295, 5201, 5201, 2856, 5245, 689, 291, 434, 733, 295, 3847, 309, 322, 411, 257, 2952, 295, 51276], "temperature": 0.0, "avg_logprob": -0.16860005060831706, "compression_ratio": 1.9183673469387754, "no_speech_prob": 0.01474273856729269}, {"id": 416, "seek": 290928, "start": 2927.52, "end": 2933.1200000000003, "text": " images and then it generates a load of you know you can generate these images we actually did", "tokens": [51276, 5267, 293, 550, 309, 23815, 257, 3677, 295, 291, 458, 291, 393, 8460, 613, 5267, 321, 767, 630, 51556], "temperature": 0.0, "avg_logprob": -0.16860005060831706, "compression_ratio": 1.9183673469387754, "no_speech_prob": 0.01474273856729269}, {"id": 417, "seek": 290928, "start": 2933.76, "end": 2938.6400000000003, "text": " you know before I did the startups just before I did the startups I think the last student I", "tokens": [51588, 291, 458, 949, 286, 630, 264, 28041, 445, 949, 286, 630, 264, 28041, 286, 519, 264, 1036, 3107, 286, 51832], "temperature": 0.0, "avg_logprob": -0.16860005060831706, "compression_ratio": 1.9183673469387754, "no_speech_prob": 0.01474273856729269}, {"id": 418, "seek": 293864, "start": 2938.64, "end": 2945.68, "text": " worked with was working on image captioning um and I'd very much like to go back to I I think", "tokens": [50364, 2732, 365, 390, 1364, 322, 3256, 31974, 278, 1105, 293, 286, 1116, 588, 709, 411, 281, 352, 646, 281, 286, 286, 519, 50716], "temperature": 0.0, "avg_logprob": -0.12775934033277558, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.002421613782644272}, {"id": 419, "seek": 293864, "start": 2945.68, "end": 2950.96, "text": " that's really incredibly interesting so while I think it doesn't actually solve the chart the", "tokens": [50716, 300, 311, 534, 6252, 1880, 370, 1339, 286, 519, 309, 1177, 380, 767, 5039, 264, 6927, 264, 50980], "temperature": 0.0, "avg_logprob": -0.12775934033277558, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.002421613782644272}, {"id": 420, "seek": 293864, "start": 2950.96, "end": 2958.56, "text": " cell Chinese rule Chinese room objection you know I mean ultimately the input to all these models", "tokens": [50980, 2815, 4649, 4978, 4649, 1808, 35756, 291, 458, 286, 914, 6284, 264, 4846, 281, 439, 613, 5245, 51360], "temperature": 0.0, "avg_logprob": -0.12775934033277558, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.002421613782644272}, {"id": 421, "seek": 293864, "start": 2958.56, "end": 2967.12, "text": " are really just activation patterns and you know the models got no reason to suspect that images", "tokens": [51360, 366, 534, 445, 24433, 8294, 293, 291, 458, 264, 5245, 658, 572, 1778, 281, 9091, 300, 5267, 51788], "temperature": 0.0, "avg_logprob": -0.12775934033277558, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.002421613782644272}, {"id": 422, "seek": 296864, "start": 2968.64, "end": 2975.8399999999997, "text": " closely you know more connected to the world than languages but I think just in practice there's a", "tokens": [50364, 8185, 291, 458, 544, 4582, 281, 264, 1002, 813, 8650, 457, 286, 519, 445, 294, 3124, 456, 311, 257, 50724], "temperature": 0.0, "avg_logprob": -0.07527018388112386, "compression_ratio": 1.658682634730539, "no_speech_prob": 0.0010343499016016722}, {"id": 423, "seek": 296864, "start": 2975.8399999999997, "end": 2983.68, "text": " very good chance that there may be really interesting correlations that can be learned by correlating", "tokens": [50724, 588, 665, 2931, 300, 456, 815, 312, 534, 1880, 13983, 763, 300, 393, 312, 3264, 538, 13983, 990, 51116], "temperature": 0.0, "avg_logprob": -0.07527018388112386, "compression_ratio": 1.658682634730539, "no_speech_prob": 0.0010343499016016722}, {"id": 424, "seek": 296864, "start": 2984.56, "end": 2991.3599999999997, "text": " you know images with language and of course the only issue there is that the", "tokens": [51160, 291, 458, 5267, 365, 2856, 293, 295, 1164, 264, 787, 2734, 456, 307, 300, 264, 51500], "temperature": 0.0, "avg_logprob": -0.07527018388112386, "compression_ratio": 1.658682634730539, "no_speech_prob": 0.0010343499016016722}, {"id": 425, "seek": 299136, "start": 2991.52, "end": 2998.88, "text": " um the amount of compute that's needed to do this is just really enormous and it's", "tokens": [50372, 1105, 264, 2372, 295, 14722, 300, 311, 2978, 281, 360, 341, 307, 445, 534, 11322, 293, 309, 311, 50740], "temperature": 0.0, "avg_logprob": -0.18241155558619007, "compression_ratio": 1.5890410958904109, "no_speech_prob": 0.0037947900127619505}, {"id": 426, "seek": 299136, "start": 3000.1600000000003, "end": 3007.2000000000003, "text": " you know you'd have to do some have to get some deal probably with one of the major tech", "tokens": [50804, 291, 458, 291, 1116, 362, 281, 360, 512, 362, 281, 483, 512, 2028, 1391, 365, 472, 295, 264, 2563, 7553, 51156], "temperature": 0.0, "avg_logprob": -0.18241155558619007, "compression_ratio": 1.5890410958904109, "no_speech_prob": 0.0037947900127619505}, {"id": 427, "seek": 299136, "start": 3007.84, "end": 3012.8, "text": " companies to get your access to enough compute yeah to do it", "tokens": [51188, 3431, 281, 483, 428, 2105, 281, 1547, 14722, 1338, 281, 360, 309, 51436], "temperature": 0.0, "avg_logprob": -0.18241155558619007, "compression_ratio": 1.5890410958904109, "no_speech_prob": 0.0037947900127619505}, {"id": 428, "seek": 301280, "start": 3013.52, "end": 3022.96, "text": " and that's that's sort of the problem with with a lot of this research now and in fact I think one", "tokens": [50400, 293, 300, 311, 300, 311, 1333, 295, 264, 1154, 365, 365, 257, 688, 295, 341, 2132, 586, 293, 294, 1186, 286, 519, 472, 50872], "temperature": 0.0, "avg_logprob": -0.16705377711806185, "compression_ratio": 1.9077669902912622, "no_speech_prob": 0.004808390978723764}, {"id": 429, "seek": 301280, "start": 3022.96, "end": 3027.1200000000003, "text": " of the comments that I wrote there is you know maybe in fact we should be thinking about something", "tokens": [50872, 295, 264, 3053, 300, 286, 4114, 456, 307, 291, 458, 1310, 294, 1186, 321, 820, 312, 1953, 466, 746, 51080], "temperature": 0.0, "avg_logprob": -0.16705377711806185, "compression_ratio": 1.9077669902912622, "no_speech_prob": 0.004808390978723764}, {"id": 430, "seek": 301280, "start": 3027.1200000000003, "end": 3033.76, "text": " you know physics has been very successful in getting funding for big science maybe we should", "tokens": [51080, 291, 458, 10649, 575, 668, 588, 4406, 294, 1242, 6137, 337, 955, 3497, 1310, 321, 820, 51412], "temperature": 0.0, "avg_logprob": -0.16705377711806185, "compression_ratio": 1.9077669902912622, "no_speech_prob": 0.004808390978723764}, {"id": 431, "seek": 301280, "start": 3033.76, "end": 3040.4, "text": " be thinking about ways of getting funding for academic big science as well for dealing with this stuff", "tokens": [51412, 312, 1953, 466, 2098, 295, 1242, 6137, 337, 7778, 955, 3497, 382, 731, 337, 6260, 365, 341, 1507, 51744], "temperature": 0.0, "avg_logprob": -0.16705377711806185, "compression_ratio": 1.9077669902912622, "no_speech_prob": 0.004808390978723764}, {"id": 432, "seek": 304040, "start": 3040.64, "end": 3047.12, "text": " well and there if you think of images still images that's a certain amount of compute power", "tokens": [50376, 731, 293, 456, 498, 291, 519, 295, 5267, 920, 5267, 300, 311, 257, 1629, 2372, 295, 14722, 1347, 50700], "temperature": 0.0, "avg_logprob": -0.25233001368386404, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00033507836633361876}, {"id": 433, "seek": 304040, "start": 3047.6800000000003, "end": 3053.6800000000003, "text": " I think of visual no ongoing visual scenes and movies that's a lot more", "tokens": [50728, 286, 519, 295, 5056, 572, 10452, 5056, 8026, 293, 6233, 300, 311, 257, 688, 544, 51028], "temperature": 0.0, "avg_logprob": -0.25233001368386404, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00033507836633361876}, {"id": 434, "seek": 304040, "start": 3054.96, "end": 3064.4, "text": " and yet presumably real world learners are you know taking advantage of the visual scene", "tokens": [51092, 293, 1939, 26742, 957, 1002, 23655, 366, 291, 458, 1940, 5002, 295, 264, 5056, 4145, 51564], "temperature": 0.0, "avg_logprob": -0.25233001368386404, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00033507836633361876}, {"id": 435, "seek": 306440, "start": 3064.4, "end": 3074.0, "text": " as it moves by um and uh although learning can take place in blind people as well and that's", "tokens": [50364, 382, 309, 6067, 538, 1105, 293, 2232, 4878, 2539, 393, 747, 1081, 294, 6865, 561, 382, 731, 293, 300, 311, 50844], "temperature": 0.0, "avg_logprob": -0.12318021600896661, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.001000463031232357}, {"id": 436, "seek": 306440, "start": 3074.0, "end": 3079.76, "text": " a whole another research area so you know you don't need vision to learn language but it certainly", "tokens": [50844, 257, 1379, 1071, 2132, 1859, 370, 291, 458, 291, 500, 380, 643, 5201, 281, 1466, 2856, 457, 309, 3297, 51132], "temperature": 0.0, "avg_logprob": -0.12318021600896661, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.001000463031232357}, {"id": 437, "seek": 306440, "start": 3079.76, "end": 3089.6800000000003, "text": " can facilitate aspects of yeah and then yeah and I guess also I mean like looking forward you'd", "tokens": [51132, 393, 20207, 7270, 295, 1338, 293, 550, 1338, 293, 286, 2041, 611, 286, 914, 411, 1237, 2128, 291, 1116, 51628], "temperature": 0.0, "avg_logprob": -0.12318021600896661, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.001000463031232357}, {"id": 438, "seek": 308968, "start": 3089.68, "end": 3098.56, "text": " want these systems to actually be kind of situated in the physical world somehow I guess right well", "tokens": [50364, 528, 613, 3652, 281, 767, 312, 733, 295, 30143, 294, 264, 4001, 1002, 6063, 286, 2041, 558, 731, 50808], "temperature": 0.0, "avg_logprob": -0.09070504456758499, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.003663281910121441}, {"id": 439, "seek": 308968, "start": 3098.56, "end": 3108.72, "text": " so that so that's the certainly lots of people have got that uh you know feeling that in fact", "tokens": [50808, 370, 300, 370, 300, 311, 264, 3297, 3195, 295, 561, 362, 658, 300, 2232, 291, 458, 2633, 300, 294, 1186, 51316], "temperature": 0.0, "avg_logprob": -0.09070504456758499, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.003663281910121441}, {"id": 440, "seek": 308968, "start": 3108.72, "end": 3117.8399999999997, "text": " actually that that we need situated you know situated models but I mean isn't the input to", "tokens": [51316, 767, 300, 300, 321, 643, 30143, 291, 458, 30143, 5245, 457, 286, 914, 1943, 380, 264, 4846, 281, 51772], "temperature": 0.0, "avg_logprob": -0.09070504456758499, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.003663281910121441}, {"id": 441, "seek": 311784, "start": 3117.84, "end": 3124.96, "text": " a model always really just an activation pattern isn't it really always just uh I mean couldn't", "tokens": [50364, 257, 2316, 1009, 534, 445, 364, 24433, 5102, 1943, 380, 309, 534, 1009, 445, 2232, 286, 914, 2809, 380, 50720], "temperature": 0.0, "avg_logprob": -0.13242368000309643, "compression_ratio": 1.819905213270142, "no_speech_prob": 0.004123921040445566}, {"id": 442, "seek": 311784, "start": 3124.96, "end": 3131.92, "text": " you always run you know so you know there was the question what is silver's chinese room argument", "tokens": [50720, 291, 1009, 1190, 291, 458, 370, 291, 458, 456, 390, 264, 1168, 437, 307, 8753, 311, 47272, 1808, 6770, 51068], "temperature": 0.0, "avg_logprob": -0.13242368000309643, "compression_ratio": 1.819905213270142, "no_speech_prob": 0.004123921040445566}, {"id": 443, "seek": 311784, "start": 3131.92, "end": 3138.4, "text": " right there was that basically you know uh supposing you you come up with a computer program", "tokens": [51068, 558, 456, 390, 300, 1936, 291, 458, 2232, 1003, 6110, 291, 291, 808, 493, 365, 257, 3820, 1461, 51392], "temperature": 0.0, "avg_logprob": -0.13242368000309643, "compression_ratio": 1.819905213270142, "no_speech_prob": 0.004123921040445566}, {"id": 444, "seek": 311784, "start": 3138.4, "end": 3145.92, "text": " which can translate english into chinese or sorry translate chinese into english so you give that", "tokens": [51392, 597, 393, 13799, 32169, 666, 47272, 420, 2597, 13799, 47272, 666, 32169, 370, 291, 976, 300, 51768], "temperature": 0.0, "avg_logprob": -0.13242368000309643, "compression_ratio": 1.819905213270142, "no_speech_prob": 0.004123921040445566}, {"id": 445, "seek": 314592, "start": 3146.0, "end": 3152.4, "text": " to a person that's sitting you know inside of a room and you just simply tell him you know here's", "tokens": [50368, 281, 257, 954, 300, 311, 3798, 291, 458, 1854, 295, 257, 1808, 293, 291, 445, 2935, 980, 796, 291, 458, 510, 311, 50688], "temperature": 0.0, "avg_logprob": -0.09900473803281784, "compression_ratio": 1.7065868263473054, "no_speech_prob": 0.008659787476062775}, {"id": 446, "seek": 314592, "start": 3152.4, "end": 3160.8, "text": " a set of symbols follow these instructions and give me the output that you you you obtained", "tokens": [50688, 257, 992, 295, 16944, 1524, 613, 9415, 293, 976, 385, 264, 5598, 300, 291, 291, 291, 14879, 51108], "temperature": 0.0, "avg_logprob": -0.09900473803281784, "compression_ratio": 1.7065868263473054, "no_speech_prob": 0.008659787476062775}, {"id": 447, "seek": 314592, "start": 3160.8, "end": 3168.88, "text": " by following these instructions and so's point is that even if this thing does actually produce", "tokens": [51108, 538, 3480, 613, 9415, 293, 370, 311, 935, 307, 300, 754, 498, 341, 551, 775, 767, 5258, 51512], "temperature": 0.0, "avg_logprob": -0.09900473803281784, "compression_ratio": 1.7065868263473054, "no_speech_prob": 0.008659787476062775}, {"id": 448, "seek": 316888, "start": 3168.88, "end": 3177.6, "text": " good english as an output you really can't say that the person understands chinese you know that just", "tokens": [50364, 665, 32169, 382, 364, 5598, 291, 534, 393, 380, 584, 300, 264, 954, 15146, 47272, 291, 458, 300, 445, 50800], "temperature": 0.0, "avg_logprob": -0.08811915951010621, "compression_ratio": 1.8695652173913044, "no_speech_prob": 0.1087966188788414}, {"id": 449, "seek": 316888, "start": 3178.32, "end": 3186.6400000000003, "text": " they don't understand chinese you know they're just following these rules and uh so his argument", "tokens": [50836, 436, 500, 380, 1223, 47272, 291, 458, 436, 434, 445, 3480, 613, 4474, 293, 2232, 370, 702, 6770, 51252], "temperature": 0.0, "avg_logprob": -0.08811915951010621, "compression_ratio": 1.8695652173913044, "no_speech_prob": 0.1087966188788414}, {"id": 450, "seek": 316888, "start": 3186.6400000000003, "end": 3192.88, "text": " really is that there's something else that a pure rule following system really doesn't have", "tokens": [51252, 534, 307, 300, 456, 311, 746, 1646, 300, 257, 6075, 4978, 3480, 1185, 534, 1177, 380, 362, 51564], "temperature": 0.0, "avg_logprob": -0.08811915951010621, "compression_ratio": 1.8695652173913044, "no_speech_prob": 0.1087966188788414}, {"id": 451, "seek": 316888, "start": 3192.88, "end": 3198.08, "text": " understanding that something else is required now lots of people wind up saying oh well what you", "tokens": [51564, 3701, 300, 746, 1646, 307, 4739, 586, 3195, 295, 561, 2468, 493, 1566, 1954, 731, 437, 291, 51824], "temperature": 0.0, "avg_logprob": -0.08811915951010621, "compression_ratio": 1.8695652173913044, "no_speech_prob": 0.1087966188788414}, {"id": 452, "seek": 319808, "start": 3198.08, "end": 3206.7999999999997, "text": " really need is grounding you really do need these you know you need the symbols to be connected somehow", "tokens": [50364, 534, 643, 307, 46727, 291, 534, 360, 643, 613, 291, 458, 291, 643, 264, 16944, 281, 312, 4582, 6063, 50800], "temperature": 0.0, "avg_logprob": -0.08379933141892956, "compression_ratio": 1.6547619047619047, "no_speech_prob": 0.021102124825119972}, {"id": 453, "seek": 319808, "start": 3206.7999999999997, "end": 3213.44, "text": " to the real world but I think the model never really knows I don't see any way for our current", "tokens": [50800, 281, 264, 957, 1002, 457, 286, 519, 264, 2316, 1128, 534, 3255, 286, 500, 380, 536, 604, 636, 337, 527, 2190, 51132], "temperature": 0.0, "avg_logprob": -0.08379933141892956, "compression_ratio": 1.6547619047619047, "no_speech_prob": 0.021102124825119972}, {"id": 454, "seek": 319808, "start": 3214.08, "end": 3219.44, "text": " computational models to know that the bit patterns that we're feeding into them", "tokens": [51164, 28270, 5245, 281, 458, 300, 264, 857, 8294, 300, 321, 434, 12919, 666, 552, 51432], "temperature": 0.0, "avg_logprob": -0.08379933141892956, "compression_ratio": 1.6547619047619047, "no_speech_prob": 0.021102124825119972}, {"id": 455, "seek": 321944, "start": 3220.2400000000002, "end": 3222.8, "text": " yeah correspond to anything in the world", "tokens": [50404, 1338, 6805, 281, 1340, 294, 264, 1002, 50532], "temperature": 0.0, "avg_logprob": -0.2652214668892525, "compression_ratio": 1.715846994535519, "no_speech_prob": 0.04994962736964226}, {"id": 456, "seek": 321944, "start": 3224.2400000000002, "end": 3229.68, "text": " yeah I mean yeah I would agree on that but um I guess for that kind of symbol grounding you", "tokens": [50604, 1338, 286, 914, 1338, 286, 576, 3986, 322, 300, 457, 1105, 286, 2041, 337, 300, 733, 295, 5986, 46727, 291, 50876], "temperature": 0.0, "avg_logprob": -0.2652214668892525, "compression_ratio": 1.715846994535519, "no_speech_prob": 0.04994962736964226}, {"id": 457, "seek": 321944, "start": 3230.56, "end": 3237.36, "text": " like some some people argue you you need to have a kind of community right language users so", "tokens": [50920, 411, 512, 512, 561, 9695, 291, 291, 643, 281, 362, 257, 733, 295, 1768, 558, 2856, 5022, 370, 51260], "temperature": 0.0, "avg_logprob": -0.2652214668892525, "compression_ratio": 1.715846994535519, "no_speech_prob": 0.04994962736964226}, {"id": 458, "seek": 321944, "start": 3238.48, "end": 3248.96, "text": " maybe that uh all kind of do you know um and and then the grounding kind of comes out of", "tokens": [51316, 1310, 300, 2232, 439, 733, 295, 360, 291, 458, 1105, 293, 293, 550, 264, 46727, 733, 295, 1487, 484, 295, 51840], "temperature": 0.0, "avg_logprob": -0.2652214668892525, "compression_ratio": 1.715846994535519, "no_speech_prob": 0.04994962736964226}, {"id": 459, "seek": 324944, "start": 3250.4, "end": 3255.76, "text": " people using the same symbol in the same way yes right yeah yes no no no I mean you're", "tokens": [50412, 561, 1228, 264, 912, 5986, 294, 264, 912, 636, 2086, 558, 1338, 2086, 572, 572, 572, 286, 914, 291, 434, 50680], "temperature": 0.0, "avg_logprob": -0.2440108009006666, "compression_ratio": 1.7259615384615385, "no_speech_prob": 0.002488507190719247}, {"id": 460, "seek": 324944, "start": 3256.48, "end": 3261.84, "text": " kripke you know so philosophers of language like kripke have argued that in fact actually", "tokens": [50716, 350, 8400, 330, 291, 458, 370, 36839, 295, 2856, 411, 350, 8400, 330, 362, 20219, 300, 294, 1186, 767, 50984], "temperature": 0.0, "avg_logprob": -0.2440108009006666, "compression_ratio": 1.7259615384615385, "no_speech_prob": 0.002488507190719247}, {"id": 461, "seek": 324944, "start": 3265.12, "end": 3269.84, "text": " that and you know in fact this is sort of very true of me because I grew up in the southern", "tokens": [51148, 300, 293, 291, 458, 294, 1186, 341, 307, 1333, 295, 588, 2074, 295, 385, 570, 286, 6109, 493, 294, 264, 13456, 51384], "temperature": 0.0, "avg_logprob": -0.2440108009006666, "compression_ratio": 1.7259615384615385, "no_speech_prob": 0.002488507190719247}, {"id": 462, "seek": 324944, "start": 3269.84, "end": 3278.32, "text": " hemisphere I don't know a lot of the northern trees so I'm not sure I can with catherine's", "tokens": [51384, 38453, 286, 500, 380, 458, 257, 688, 295, 264, 14197, 5852, 370, 286, 478, 406, 988, 286, 393, 365, 269, 16351, 311, 51808], "temperature": 0.0, "avg_logprob": -0.2440108009006666, "compression_ratio": 1.7259615384615385, "no_speech_prob": 0.002488507190719247}, {"id": 463, "seek": 327832, "start": 3278.4, "end": 3283.52, "text": " help now I can recognize aspens but you know I'm not really sure about the difference between", "tokens": [50368, 854, 586, 286, 393, 5521, 16817, 694, 457, 291, 458, 286, 478, 406, 534, 988, 466, 264, 2649, 1296, 50624], "temperature": 0.0, "avg_logprob": -0.0632573027359812, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.0018905742326751351}, {"id": 464, "seek": 327832, "start": 3283.52, "end": 3290.2400000000002, "text": " oaks and elms and the rest of them right but kripke you would say that I can still talk about", "tokens": [50624, 277, 5461, 293, 806, 2592, 293, 264, 1472, 295, 552, 558, 457, 350, 8400, 330, 291, 576, 584, 300, 286, 393, 920, 751, 466, 50960], "temperature": 0.0, "avg_logprob": -0.0632573027359812, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.0018905742326751351}, {"id": 465, "seek": 327832, "start": 3290.2400000000002, "end": 3297.2000000000003, "text": " all of those things and when I talk about oaks I mean oak trees even though I might not be able", "tokens": [50960, 439, 295, 729, 721, 293, 562, 286, 751, 466, 277, 5461, 286, 914, 31322, 5852, 754, 1673, 286, 1062, 406, 312, 1075, 51308], "temperature": 0.0, "avg_logprob": -0.0632573027359812, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.0018905742326751351}, {"id": 466, "seek": 327832, "start": 3297.2000000000003, "end": 3303.6800000000003, "text": " to actually identify an oak tree reliably and so kripke's story there is exactly what you're", "tokens": [51308, 281, 767, 5876, 364, 31322, 4230, 49927, 293, 370, 350, 8400, 330, 311, 1657, 456, 307, 2293, 437, 291, 434, 51632], "temperature": 0.0, "avg_logprob": -0.0632573027359812, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.0018905742326751351}, {"id": 467, "seek": 330368, "start": 3303.68, "end": 3310.0, "text": " saying it's a community of language learners sorry language users and I am willing basically I'm", "tokens": [50364, 1566, 309, 311, 257, 1768, 295, 2856, 23655, 2597, 2856, 5022, 293, 286, 669, 4950, 1936, 286, 478, 50680], "temperature": 0.0, "avg_logprob": -0.044150346933409225, "compression_ratio": 1.7720930232558139, "no_speech_prob": 0.022814657539129257}, {"id": 468, "seek": 330368, "start": 3310.0, "end": 3317.6, "text": " agreeing to the authority of language users and I effectively what I'm saying is when I use oak", "tokens": [50680, 36900, 281, 264, 8281, 295, 2856, 5022, 293, 286, 8659, 437, 286, 478, 1566, 307, 562, 286, 764, 31322, 51060], "temperature": 0.0, "avg_logprob": -0.044150346933409225, "compression_ratio": 1.7720930232558139, "no_speech_prob": 0.022814657539129257}, {"id": 469, "seek": 330368, "start": 3318.24, "end": 3324.64, "text": " I use it to mean whatever the rest of you mean that you know that grew up and presumably know", "tokens": [51092, 286, 764, 309, 281, 914, 2035, 264, 1472, 295, 291, 914, 300, 291, 458, 300, 6109, 493, 293, 26742, 458, 51412], "temperature": 0.0, "avg_logprob": -0.044150346933409225, "compression_ratio": 1.7720930232558139, "no_speech_prob": 0.022814657539129257}, {"id": 470, "seek": 330368, "start": 3324.64, "end": 3330.7999999999997, "text": " exactly what an oak tree is right and he says that in fact actually with a lot of particularly", "tokens": [51412, 2293, 437, 364, 31322, 4230, 307, 558, 293, 415, 1619, 300, 294, 1186, 767, 365, 257, 688, 295, 4098, 51720], "temperature": 0.0, "avg_logprob": -0.044150346933409225, "compression_ratio": 1.7720930232558139, "no_speech_prob": 0.022814657539129257}, {"id": 471, "seek": 333080, "start": 3330.8, "end": 3338.2400000000002, "text": " scientific terminology we wind up using it that way right many of us may not be able to define", "tokens": [50364, 8134, 27575, 321, 2468, 493, 1228, 309, 300, 636, 558, 867, 295, 505, 815, 406, 312, 1075, 281, 6964, 50736], "temperature": 0.0, "avg_logprob": -0.0763286912297628, "compression_ratio": 1.614678899082569, "no_speech_prob": 0.004924135748296976}, {"id": 472, "seek": 333080, "start": 3338.2400000000002, "end": 3343.92, "text": " exactly what the difference is between the different types of neutrinos or whatever but", "tokens": [50736, 2293, 437, 264, 2649, 307, 1296, 264, 819, 3467, 295, 7989, 12629, 329, 420, 2035, 457, 51020], "temperature": 0.0, "avg_logprob": -0.0763286912297628, "compression_ratio": 1.614678899082569, "no_speech_prob": 0.004924135748296976}, {"id": 473, "seek": 333080, "start": 3344.5600000000004, "end": 3351.04, "text": " we rely on experts within our community to be able to ground those things", "tokens": [51052, 321, 10687, 322, 8572, 1951, 527, 1768, 281, 312, 1075, 281, 2727, 729, 721, 51376], "temperature": 0.0, "avg_logprob": -0.0763286912297628, "compression_ratio": 1.614678899082569, "no_speech_prob": 0.004924135748296976}, {"id": 474, "seek": 333080, "start": 3352.6400000000003, "end": 3357.52, "text": " all I can say is I don't even know how you'd even tell a large language model that it's part of", "tokens": [51456, 439, 286, 393, 584, 307, 286, 500, 380, 754, 458, 577, 291, 1116, 754, 980, 257, 2416, 2856, 2316, 300, 309, 311, 644, 295, 51700], "temperature": 0.0, "avg_logprob": -0.0763286912297628, "compression_ratio": 1.614678899082569, "no_speech_prob": 0.004924135748296976}, {"id": 475, "seek": 335752, "start": 3358.48, "end": 3363.68, "text": " yeah yeah no I was sort of yeah kind of thinking that sort of thing myself like how", "tokens": [50412, 1338, 1338, 572, 286, 390, 1333, 295, 1338, 733, 295, 1953, 300, 1333, 295, 551, 2059, 411, 577, 50672], "temperature": 0.0, "avg_logprob": -0.2395421012503202, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.014498444274067879}, {"id": 476, "seek": 335752, "start": 3364.4, "end": 3371.12, "text": " well what I mean does that does the community there just mean literally that actually just the", "tokens": [50708, 731, 437, 286, 914, 775, 300, 775, 264, 1768, 456, 445, 914, 3736, 300, 767, 445, 264, 51044], "temperature": 0.0, "avg_logprob": -0.2395421012503202, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.014498444274067879}, {"id": 477, "seek": 335752, "start": 3371.12, "end": 3380.96, "text": " text documents that have been fed into it is that I mean I guess with the the instruction", "tokens": [51044, 2487, 8512, 300, 362, 668, 4636, 666, 309, 307, 300, 286, 914, 286, 2041, 365, 264, 264, 10951, 51536], "temperature": 0.0, "avg_logprob": -0.2395421012503202, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.014498444274067879}, {"id": 478, "seek": 338096, "start": 3381.68, "end": 3384.8, "text": " tuning I guess you get a little bit of that right", "tokens": [50400, 15164, 286, 2041, 291, 483, 257, 707, 857, 295, 300, 558, 50556], "temperature": 0.0, "avg_logprob": -0.2371478738455937, "compression_ratio": 1.5844155844155845, "no_speech_prob": 0.006477434653788805}, {"id": 479, "seek": 338096, "start": 3388.64, "end": 3396.08, "text": " so anyone else have questions at all yeah so that it's almost just that you could have a community", "tokens": [50748, 370, 2878, 1646, 362, 1651, 412, 439, 1338, 370, 300, 309, 311, 1920, 445, 300, 291, 727, 362, 257, 1768, 51120], "temperature": 0.0, "avg_logprob": -0.2371478738455937, "compression_ratio": 1.5844155844155845, "no_speech_prob": 0.006477434653788805}, {"id": 480, "seek": 338096, "start": 3396.08, "end": 3405.12, "text": " of users of a particular model yeah that would have a sort of various types of queries within a", "tokens": [51120, 295, 5022, 295, 257, 1729, 2316, 1338, 300, 576, 362, 257, 1333, 295, 3683, 3467, 295, 24109, 1951, 257, 51572], "temperature": 0.0, "avg_logprob": -0.2371478738455937, "compression_ratio": 1.5844155844155845, "no_speech_prob": 0.006477434653788805}, {"id": 481, "seek": 340512, "start": 3405.12, "end": 3416.4, "text": " particular domain that might help train up that model then to become more a realistic conversational", "tokens": [50364, 1729, 9274, 300, 1062, 854, 3847, 493, 300, 2316, 550, 281, 1813, 544, 257, 12465, 2615, 1478, 50928], "temperature": 0.0, "avg_logprob": -0.10831549962361654, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.016362348571419716}, {"id": 482, "seek": 340512, "start": 3416.4, "end": 3425.68, "text": " agent within that particular domain perhaps yeah yeah so maybe that's kind of yeah yeah yeah", "tokens": [50928, 9461, 1951, 300, 1729, 9274, 4317, 1338, 1338, 370, 1310, 300, 311, 733, 295, 1338, 1338, 1338, 51392], "temperature": 0.0, "avg_logprob": -0.10831549962361654, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.016362348571419716}, {"id": 483, "seek": 340512, "start": 3426.7999999999997, "end": 3432.88, "text": " who knows maybe a year or two from now you know those things will start to emerge yeah", "tokens": [51448, 567, 3255, 1310, 257, 1064, 420, 732, 490, 586, 291, 458, 729, 721, 486, 722, 281, 21511, 1338, 51752], "temperature": 0.0, "avg_logprob": -0.10831549962361654, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.016362348571419716}, {"id": 484, "seek": 343288, "start": 3432.88, "end": 3437.92, "text": " yeah so I think one of the things that Catherine and I are hoping to get out of this is to find", "tokens": [50364, 1338, 370, 286, 519, 472, 295, 264, 721, 300, 23098, 293, 286, 366, 7159, 281, 483, 484, 295, 341, 307, 281, 915, 50616], "temperature": 0.0, "avg_logprob": -0.14736923338874938, "compression_ratio": 1.6149425287356323, "no_speech_prob": 0.0005867608124390244}, {"id": 485, "seek": 343288, "start": 3437.92, "end": 3447.28, "text": " out more about you know work at SFI that we might connect with right but a sort of general interest", "tokens": [50616, 484, 544, 466, 291, 458, 589, 412, 31095, 40, 300, 321, 1062, 1745, 365, 558, 457, 257, 1333, 295, 2674, 1179, 51084], "temperature": 0.0, "avg_logprob": -0.14736923338874938, "compression_ratio": 1.6149425287356323, "no_speech_prob": 0.0005867608124390244}, {"id": 486, "seek": 343288, "start": 3447.28, "end": 3458.96, "text": " in you know language learning psychological aspects computational aspects you know so", "tokens": [51084, 294, 291, 458, 2856, 2539, 14346, 7270, 28270, 7270, 291, 458, 370, 51668], "temperature": 0.0, "avg_logprob": -0.14736923338874938, "compression_ratio": 1.6149425287356323, "no_speech_prob": 0.0005867608124390244}, {"id": 487, "seek": 345896, "start": 3458.96, "end": 3472.16, "text": " so we're here until Tuesday afternoon Tuesday evening so please please contact us right", "tokens": [50364, 370, 321, 434, 510, 1826, 10017, 6499, 10017, 5634, 370, 1767, 1767, 3385, 505, 558, 51024], "temperature": 0.0, "avg_logprob": -0.46950470773797287, "compression_ratio": 1.16, "no_speech_prob": 0.005563012789934874}, {"id": 488, "seek": 347216, "start": 3472.16, "end": 3475.3599999999997, "text": " okay thanks", "tokens": [50400, 1392, 3231, 50524], "temperature": 0.0, "avg_logprob": -0.8879996299743652, "compression_ratio": 0.5789473684210527, "no_speech_prob": 0.15802639722824097}], "language": "en"}