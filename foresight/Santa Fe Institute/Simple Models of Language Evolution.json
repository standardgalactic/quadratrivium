{"text": " Are you still a reader in mathematical neuroscience? Is that still accurate? I've changed all our titles a little bit. Okay, so a man of many titles. Whatever I was at Bristol, I know you were the head of the Computational Neuroscience Unit, and he was also an examiner for my PhD five. So super, super happy to have him here today. Thank you, Connor. Brilliant. Sorry, one second. Cool. Well, I'm super excited to be here. This has always been kind of a legendary location for me. I've always wanted to visit, and it's really blown me away. It's an amazing looking place. It's my first time in New Mexico, and oh my god, the countryside here is just incredible. I assume that it's traditional to start talks here with some reminiscence about the time that you met Murray Gaumann. So I met Murray Gaumann 20 years ago. I was still a particle physicist then. He was coming through 20 College Dublin, where I was working at the time, and he asked me where I was from, and I said I'm from Galway, which is true. And he said, oh yes, Galway, home of this, Galway, off whose coast the Spanish Armada founded in the storm, saving the British Navy the trouble of sinking it. Home too, he said, to the myth that the lusty sailors thus wrecked, swam ashore, and bred with the local women to create the black haired Irish. Something he said that is not true, the black haired Irish are presumably the remnants of the indigenous population, the population there before and accounts of population whose traditions and cultures we know from archaeology, but whose language is wholly lost and unknown. And I was just incredibly impressed by the whole thing. I mean, he knew where Galway was. He knew that we had this link with Spain. He knew about the Spanish Armada sinking. He knew that we all believed, as it turns out incorrectly, that the Spanish sailors had created these sort of black haired folk in Galway. And he told me this interesting thing about how these black haired folk are probably the remnants of the indigenous population. And I was always intrigued by this idea that there was a language that they spoke, which is now completely lost. The idea of these completely lost languages is lovely. It also struck me with something of a show off, but that's true too. But it was very impressive. And so I'm very, very, very glad to be in the Murray Gilman building. I was, I did start life as a particle physicist and then became a neuroscientist and only started working on language recently. So I don't know that much about it. But the reason I started working on language was I read this kind of bizarre quote from Novichonsky, which says, in their essential properties, and even down to fine details, languages are cast in the same mold. At the Martian scientists, my reasons we conclude that there's a single human language with differences only at the margins. And that just seemed so, so wrong to me. And, you know, I thought that maybe that is an accident of not, you know, thinking about how different languages are. It seemed to me, you know, the languages are incredibly different from each other. And I'm looking with where I'm from to know a little bit of Irish. And Irish is, you know, although an inter-European language is more different to, you know, English than maybe French and so on is. I mean, for starters, it has a different word order. One of the more striking features is the, it doesn't have a verb possession. So here, this is the Irish for, I have a newspaper, Thon, Newton, Ogham. And what's happening there is that there's no, the Thon is just the verb is. So the literal translation of that sentence is the newspaper is at me. The Ogham is a preposition combined with the pronoun, which is a feature of Irish. And prepositions do a lot of work in Irish that it's done by other parts of speech in other languages. So instead of saying that you have a newspaper, you say the newspaper is at you. And similarly, Thon, Newton, Ogham, the new newspapers from me is how you'd say that you wanted the newspaper. So it's strongly that, you know, that languages are very different from each other. And that it's wrong to presuppose that languages all spring from the brain in the way that Chomsky is suggesting. Because, you know, his idea is that the languages bear the imprint of the human mechanism. Whereas, for me, as a neuroscientist, at the time, somebody knew about how vision worked and so on, it seemed that languages were much more likely to just be the unfolding of some search for statistical structure. And maybe something to do with the properties of the world around us, the fact that there are actions, and there are, and hence verbs, and there are objects, and hence nouns. But of course, I'm no longer sure that I was right in thinking that. And I've begun to appreciate something of the wisdom of what Chomsky said. And of course, another example from Irish Thon, Newton Thum, the newspaper is under me. That's how you'd say the newspaper is about me. But there you can see that even in English, the same work is being done by the prepositional construction as is being done in the Irish. In other words, although, you know, the Irish uses a different preposition and prepositions tend to vary as you probably know a lot from language to language. The idea of using a preposition where you might otherwise have a different verb is not so unique to Irish. And so maybe languages are somewhat similar to each other. And it struck me, or strikes me, as I'm sure it strikes you, that this is a really important question. I love this sign because it says that the vehicles will be prosecuted without warning. But this is a warning sign. So of course, it makes it impossible for them to do that. And I have here started to remind me to comment on the idea that language is complicated because some aspects of language are to do with what's in the world. The structure of language is structured by its use case, by communication. And some parts of language presumably are to do with the human ability to communicate, or our ability to perceive, or the calculations we do in producing, interpreting meaning and in producing meaning. And some parts might be more idiosyncratic to do with some linguistic mechanism. And deciding between these, deciding, you know, recognizing the fact that we can say something like this, which logically makes no sense at all, but which, you know, in a Wisconsinian fashion, quite clearly communicates something. It is a property, you know, is something that's deeply rooted in who we are, and our language is a part of how we conceive of ourselves as conscious individuals. And separating what's special about language, what's special about us, what's special about, with, you know, what language necessarily has as a communication medium, seems an interesting and an important challenge. So, of course, as a neuroscientist, when I decided I was interested in this, the first inclination was to do EEG experiments. And I wanted to start talking a little bit about that before talking a little bit more about models of language evolution. I started off actually trying to do EEG experiments on Irish speakers. I don't know how much you know about Ireland that Irish speakers, but it turns out that trying to do EEG experiments on Irish speakers is very difficult, because lots of people who claim they can speak Irish actually really can't. As I did a whole series of experiments on people who claim they can speak Irish and discovered, in fact, the EEG just told me that they were lying. So I went back to working on English speakers. And I was interested in testing this idea that the brain does privilege grammatical structure over mere statistical structure. Now, if any of you have done EEG experiments, you'll know just how damn difficult they are. I mean, the idea that we detect electrical fields outside the brain is actually slightly surprising, because you'd expect all the electrical fields to cancel out. And the only reason we can detect anything at all is that there's a slight preponderance of synapses pointing one way around another. And then, of course, you bathe the whole thing in salty fluid, which makes it hard to detect anything. For cognitive experiments, the difficulty is still greater still. And I'll refer to this a few times. This EEG data is obviously showing somebody having an epileptic fit and they were induced by tooth brushing, so they started brushing their teeth, and here they have the fit. And you can see that it's a very pronounced activity. But for cognitive tasks, of course, not only is the activity not so pronounced, I mean, epilepsy is distinguished by the synchrony of neural activity. So that's exactly the thing that's recently easy to detect. Cognitive things aren't so distinguished. And secondly, of course, when you're doing an EEG experiment, when you're a participant, you tend to sort of think other things. You're there, the stimulus is being played to you. You're supposed to be thinking about the stimulus, but in fact, you're thinking, oh my god, I wish I wasn't doing this experiment anymore. Oh my god, this is really boring. Oh my god, all people are immortal. I'm going to die someday. I wonder what I'll have for dinner. I mean, it's very hard to detect cognitive stuff with EEG. So the thing that we did, again, following a paradigm that was introduced by David Peeble and Naiding and his co-workers, was a frequency tag experiment. So in a frequency tag experiment, to try and separate a signal from noise, you concentrate your stimulus at a particular frequency. And so that you know, noise tends to stuff that you're not interested in, people thinking about their mortal end or the end of the experiment, that happens at all sorts of frequencies. But if your stimulus is at a particular frequency, then you can use Fourier transform or whatever to concentrate on the thing that you're interested in. So in this experiment, we play people adjective noun sentences. We've choose the adjectives and nouns, so they're single syllable. We record them and then coerce them a little bit so they're all exactly the same length. We play the syllables at 3.125 hertz. That just turns out to be a particularly good frequency for, it's a comfortable frequency for listening to syllables. We have long, long streams of these things. I mean, we have old rats, that man, ill wife. It goes on and on forever. And we choose the adjective nouns quite carefully so that the bigrams between old and rat and rat and sad are roughly the same. You can't get them exactly the same, but you try and keep the statistical structure the same. And then what you see here is that obviously there's going to be a response in the EG to the stimulus at 3.125 hertz. But there's something else happening, which is the noun phrase. So if there's a response at 1.5625 hertz, that's a response to something that's not directly in the stimulus, but is related to the meaning of the words. And what interpretation is that that's because the brain privileges noun phrases over not noun phrases. And the other example is an adjective verb stream. In the adjective verb stream, we make sure we have the same sort of biogram structures as we had for the adjective noun. It's likely to come beside each other as old and man or whatever it was I had before. But now there is no grammatical structure. So if we see a response at 1.5625 hertz for the first stimulus and not for the second, that's indicative of the brain responding hopefully. It's indicative to the brain privileging a grammatical structure, which was I guess the one thing to be interested in. It turns out these data are quite hard to analyze. You know, you think all you have to do then is take the Fourier transform and look at the size of the peak at those particular frequencies. But EG is extremely noisy. And so if you do that, you don't see anything. What you have to do is look for the phase locked component of the response. So you play lots of these streams, so you make each stream five seconds long. You play repeatedly the stream. And then you look for that portion of the response, you know, once you've taken the transform, that portion of the response which has a similar phase. And the complication, of course, is that the overall phase is not so important. It depends on how big your head is and where the electrode is and all sorts of other things. And so the actual phase of people's responses at whatever frequency, at one point, whatever it is, isn't important. And you can see that here. So this is just each of these dots, each of these lines corresponds to a participant. We had 16 participants in this experiment. And then for each participant, we've got all the different electrodes. And this is the average phase of the response averaged across. It was 10 trials for each participant for each stimulus. So this is the adjective noun, stimulus. And in the dot is for the 32 electrodes. And just for convenience, for ease of comparison, three of the electrodes have been picked out and are cut at the same across the participants. And basically all you can see is that the actual overall phase is quite different. And so what's interesting is, you know, each of these dots hides the fact that there's 20 trials. Each trial has its own phase. And the signal is in the degree of alignment of that phase. And so to analyze these data, well, the easiest way to do it, it turns out, is to make some complicated Bayesian models. So we imagine the phase is being drawn from some distribution. In this case, it's circular Cauchy distribution, which turns out to be the nicest. The circular Cauchy distribution is quite cool. It's just a wrapped Cauchy distribution. And when you do the wrapping, you can set some of the theories and come up with an analytic formula for it. You say the data is kind of noisy, right? Immensely. Yeah. So the Fourier transform is also like pretty noisy. Yes. Yes. So you have to go through several steps. So, you know, obviously, for start using the frequency tag experiment makes it less noisy because, you know, the other stuff is happening at different frequencies. But even then you have to do this sort of phase lock analysis, which I'm briefly reviewing. I don't want to spend too long on it. If you're interested in the analysis of EEG data, we've been thinking about it a lot. And I can talk to you about it afterwards. But basically, in summary, we make a big Bayesian model of the results. What we're looking for, we imagine the phases have been drawn from a wrapped distribution. We have some prior for the variance of that distribution, some prior for the mean, that doesn't matter. And so the signal will be in this, the posterior distribution of this phase of this variance for the phase. And so this basically is the result. Here, we're looking at the, it basically is the inverse of this variance, what they call the mean circular resultant against frequency. And so, sorry, I'm standing in front of the camera. And so what we're seeing here is the different frequencies. This is the frequency of the syllables. This is the frequency of the phase. For the adjective verb condition, you see that there's a big response at the syllable rate, as there is for all of these conditions. These are other conditions. This is mixed lexical, mixed phrase, random. We did six different conditions, but we'll just concentrate on these two. For both the adjective noun and the adjective verb, there's a response at the syllable rate, showing up as a reduction in the variance of the phases. But for only the adjective noun, are you seeing a response at the noun, at the, at the phrase rate? This, on the right, this is some basic equivalent of the usual bar and star type graph. All it's really indicating is that the adjective noun condition shows substantially, or you might say significantly more response, which is, again, a significantly more mean resultant, just like the inverse of variance, than these other conditions, which is basically showing that the brain has a response to the grammar. That we can't just think of the brain as performing a statistical inference on the sentences, trying to extract meaning and so on. It does something every time it hears a noun phrase, and it doesn't do anything every time it hears an adjective verb word pairing, because that isn't a grammatical object, or at least that's the interpretation. So, you know, having done this, I was, I was kind of amazed. It shows that there is sort of stuff happening in the brain that, that is more formal or more, more akin to a grammatical manipulation than, than you might have expected, and shows sort of the presence in, in, in this, you know, in this discussion as to what part of language is about the world, what part of language is about communication, what part of language is about the machinery the brain has to deal, to deal with language. There's more in that sort of brain part than you might think, and it's certainly, will it be of interest to start to consider grammar, and the brain's view of grammar. Now, of course, these, these stimuli that we were dealing with are, are quite new that we were using, as I said, following this idea of people in a ding, this frequency tagged paradigm, where you are playing the stimulus at a set frequency in order to be able to extract the EG signal of interest. But it's very hard to, to, to come up with, you know, there's lots of things that you can't do with that. We were able to examine the presence of noun phrases, etc. But what you'd really like is to use free text like this, with like complicated, you know, sentences, different types of grammar, and so on. This would be better as well, because one problem with the, the frequency tagged experiments is that they are quite, quite boring and annoying to, to, to be a participant in those experiments. And it doesn't encourage this kind of thinking away from what you're supposed to be thinking. The problem here is the, is the difficulty in, in, in analyzing the data and, and trying to show a relationship between the EG response and, and, and what's going on. So, you know, what, what we'd like to do, I think, is to understand what grammar looks like to the brain. You know, we, we know what Grammaticians think grammar looks like, but we don't know, you know, but I often think about the phrenology skills that people were interested in in the 19th century. The pseudoscience phenology divided the brain up into regions of the brain that did different things, which is actually true. There are regions of the brain that do different things, but they got it completely wrong. You know, they, they thought that the cerebellum was the organ of amethyst, which is bizarre. In fact, it had some role to do with predicting the consequence of motor commands or something. So, you know, the, the, the ideas we have about grammar might be quite different from what grammar actually looks like. And to, to probe that, about one thing certainly to do would be to, to, to do EG experiments with free text. So, Connor, like, so just to kind of get an idea of the experiment itself. So you'd have like, I know you said this is going to be like really complicated, but in principle, you'd have your participants in a room reading this out loud and you'd just be looking at the brain signals. Is that the, you would be reading it to them and you'd be reading the two brain signals. Yeah. So, so, I mean, here, here's the results. So, I mean, there are people, you know, it's not just us that's trying to do this. There are lots of labs doing experiments along these lines. Stephen Frank in Namegun, for example. You have measures of kind of comprehensive, like some behavioral output, you know, did like, what is, what's the point of the brain responding to these things? Are you, are you measuring that as well? Like in the previous experiment, you showed that there was a response better, but like, why, right? Like, what is that, what is that, what is that doing for? I mean, what we do do is we make sure people are paying attention. Okay. So we have a detention trap. We introduce particular words that they're supposed to press a button to show that they're still paying attention. We, one advantage of the Bayesian analysis is you can check the participants are, you know, you can, there's terms in the, in the, you know, you have a big Bayesian model. There's a term to do with the participants, you know, attentiveness. You can see that some of the participants are clearly not paying attention. You can look at them through the window and see they're not paying attention as well. You know, I guess I'm asking like a deeper question about, you know, you're asking what, what is language for, what's the brain for? Just like some of these responses, I'm always skeptical, particularly, I mean, I'd know the fMRI literature much better than the EEG literature of where you just record a response and you say, okay, this brain, this part of the brain, there's a response to something, but without some sort of behavioral output that you're measuring connected to that response, I'm always skeptical of how much, how much does that actually tell us about, you know, what's actually happening and why, why we should care that there's a response there. I mean, we should care there's a response there, because it shows that we are responding to the noun phrases. And that's part of a story where, you know, we want to understand how the brain understands language. How do we answer that? Well, I'm not sure. And, and you, you know, people try behavioral experiments, they try manipulations to see how it changes people's understanding and so on. There's, you know, there's a long history of this. But it hasn't yet produced an account of language. And I'm not to answer your question going to produce one either. But that's absolutely what we need to do. I mean, so I guess my idea at the moment is that we start, that we use the EEG response as some sort of proxy for, for what's happening. And we try and see, well, just let me say what we've done here, for example. So again, we're looking at the EEG response to free text. We're doing some big regression, and we're trying to see how people are responding to different types of words. And what we can see what this is showing is that for part of the EEG response, there is a difference in how people are responding to words according to the categorization of the words into function and content. And so we can see that people's brains respond differently to some particular categorization of word. And so what you might be optimistic about is that if you've got better at this, you could try different categorizations and match them to different ideas about how the brain might approach parsing language. But I agree. You can hear that then to maybe the grammars of the linguistics. Yeah. And then separately, of course. Mel asked me not to mention transformers, but then separately, you look at, you can probe how large language models deal with language. You can probe the grammatician's approach to language. You can probe the whole linguistic tradition of our accounts of how language is dealt with. And then this gives a sort of neural account, the neural view of what grammar is like. But whether that's going to work or exactly the details of that, I don't know. And so certainly what I would advocate is that we could collect a big data set, you know, with lots of dots of different languages. The little prince, as maybe you know, is the non-religious text that has been translated into the most languages. It exists in 300 languages. It exists as an audiobook in 50 or 60 languages. And so it's been suggested by these folk here, Jinxing Lee, Brennan, Haile and some of their co-workers, that we collect a large corpus of different varying qualities, some, you know, some MEG, which is, you know, very high quality, some consumer EEG, but with many more participants across lots of languages. And we start trying to understand, from the point of view of the brain, what grammar looks like. So far, this hasn't been done. And I guess people are trying to raise money to do it and have failed. So that's, you know, that's just by way of sort of motivation. That's where I came into thinking about language. And, you know, my interest in language is trying to understand, you know, how is language, what's special about language, what makes language, language. And of course, when you start thinking about this, you start asking these questions, how do we do these experiments? And it strikes you that there's a whole sort of separate story to language, which is to do with evolution. And maybe the hope that if we think a little bit about evolution, about how languages arise from evolution, the species point of view, and then separately, how languages change, it would be, it might tell us something about the innate structure of language. And maybe we can then think about how language, why languages might have to be the way they are, compared to other ways they could be. And I think this is, you know, a very interesting question. One of the sort of striking things is that, well, you know, we know from Creole languages and sign languages and so on that languages do arise with a large amount of their structure already present. But it's still an open question as to whether there are parts of language that develop through time, you know, as languages evolve, do they, do they change in a consistent way? Or are they the same from the very start? So near where I work, there's a museum, the Bristol Museum, and Art Gallery, I left that bit out. And they have some of these freezes from Nimrod. So the palace at Nimrod was broken up by people who went there and they took the panels and sent them all around the world in, I guess, an act of gross theft, although what was left in Nimrod has since been destroyed, so in the way it was quite lucky. And these panels do have this kind of cuneiform script written across them. It's a standard inscription. And it's Ashnirah. Ashnirah is basically showing off in this description. So this standard inscription here in cuneiform explains what a great person he is. And some of it's quite sort of bloody as this bit is, there are men young and old, I took prisoners of some I cut off their feet and hands of others I cut off the ears, noses and lips of the young men's ears. I made a heap of the old men's beads I made him heads, I made him in a rash, et cetera, et cetera, et cetera. Some of it's much nicer. It's about making pleasure palaces and beautiful things, et cetera. But I couldn't find any of that in an easily cut and pasted form. So I had to get this rather bloody bit instead. It says something about our civilization. But the point anyway, is that what is striking in the text is the lack of sort of the normal clause structure. There's very few instances or no instances of the sort of clauses that you might expect where there's who and which is and so on linking together the sentences. Instead, it's this rolling list, this very sort of list like structure. And so you wonder is that because the language has not at this point evolved all the structures of modern language, the sort of merge and clause structures that we have now, or is it just that that's how they like to write in their ceremonial functions. So just striking that there's a lot to be gained from trying to understand something about the evolution of language. And so that's what I wanted to talk a little bit about now. And so the first thing I wanted to talk about is really is to urge a return considering the iterative language model that Simon Kirby and his co-workers came up with about 20 years ago. So I don't know if you know the iterative language model, it's a language, it's a model for the evolution of languages. The story is that Kirby and his co-workers discovered it. They simulated a little bit, but they came quickly hard up against the computational limitations at the time. It is quite computationally expensive. And so they worked on it and then they kind of abandoned it and went on to try and do the same experiment that they'd done in simulation in real people. And so Simon Kirby has very successfully spent the last 20 years using toy languages and toy language learning as a pro into how people learn languages, but hasn't considered much beyond the original work, the iterative language model itself. And now that we've got faster computers and so on, it might I think be very interesting to go back and think about this much more. So in the iterative language model, basically you have a teacher and the teacher teaches a pupil and then the pupil becomes the teacher and teaches another pupil and so on. So it's a chain of learnings, teachings and learnings. And the idea is that the language progresses or changes through this teaching and learning. The hope is that we might learn something about how the structured language arises by seeing if it arises through this simulation of the teaching and learning process. And the crucial point, as we'll see, is that there is a bottleneck. So the agent has a language, the teacher, they teach only a number of exemplars to the learner, to the pupil. And then the pupil in turn, the pupil becomes the teacher, the pupil is teaching the next learner and they have to extrapolate from the, well, they've extrapolated from the few examples, exemplars they've been taught, a whole language, and then they choose other exemplars from that language to teach the next pupil. And it's this process of bottleneck and extrapolation from the bottleneck that Simon Kirby hoped and in fact found did produce some of the properties that we believe languages should have. And so again, this just summarizes it. The teacher provides signals and meanings. So they say, you know, cat and then shows a cat, just like in cartoons. The cartoon idea about how we teach children, although people who work on children point out that that's not actually what happens, that we very rarely teach children language in this supervised way. But here, we do have this naive picture. The teacher provides signals and meanings. The learner learns the mapping from signals to meanings. And then when they reach maturity, they use that to, they then use a version, which I'm going to talk about in a minute, to invert that map. So they get a map from meanings back to signals. And then they choose some random meanings, produce the signals for, as an example, to the new learner. And the process continues. And the idea is that the language that's produced should have some of these nice properties. So the properties they have that we're to look for are expressivity, stability and compositionality. Expressivity is basically, can all meanings be expressed? In other words, if you map signals, set of signals onto the space of meanings, how onto is that map? If a completely expressive language is one where the map from signals to meanings is onto, if the signals all map to the same meaning, that would be not at all expressive. And expressivity is just a sort of counting of, I mean, basically it's a counting of the map of the signal space into the, into the meaning space divided by the size of the meaning space. Stability, that's just how after the languages have time to mature, is it roughly stable from, from iteration from generation to generation. And then compositionality, of course, is the sort of more difficult one. That's what makes languages languages, what makes a language a language, the idea that a part of the signal should consistently code for some aspect of meaning. So, you know, in this case here, we have the word for orange. I think funny to make this as an example when I was making these slides, but of course it's the worst possible example. So, so the idea is we have a word for orange, the color. You can see my problem, I'm really screwed myself, but yeah, we have a word for orange, the color, and it's used here to describe orange, the fruit, and then we have blue used to describe the blueness of the orange on the right. And the idea of compositionality is that the word orange, it means the color orange when referring to the color of an orange, or it means orange when referring to the color of a high-vis jacket. And one of the things, you know, the people have learned in trying to make agent models of language evolution is that compositionality is actually quite hard to enforce. So, you know, if you have two reinforcement learning agents and they're playing what they call the Lewis signaling game, they're trying to learn a way of telling each other about things, what tends to happen is that, you know, if you don't, if you restrict their signal space, so they just don't have a separate word for every possible combination of attribute and object, et cetera, but rather they're forced to have some symbol for attribution, some symbol for what's being described, they have to have a word for the color and a word for the object. You can do that, but they don't consistently use the same color word. You know, they might use orange to mean orange when describing oranges, but they might use blue to mean orange when describing high-vis jackets. It's enough for the communication between agents, you know, reinforcement learning agents, that there is a word to distinguish potential colors of the orange fruit. But that word, the word for orange, doesn't have to be the same word. It doesn't have to match to the same color as when you're using it to describe something else. You know, we do this a little bit ourselves. I mean, you know, when we're talking about horses, we use the word chestnut for what we would call brown in other instances. But generally speaking, the main property of language is compositionality, and the idea is to seek that here. So in this version, in the simple version of the iterated language model, we just have eight bits of meaning and eight bits of signal. So the meaning space is just 256 potential meanings, and the signal space is 256 potential signals. And then the learner has a simple neural network. You can see that this all dates back to the year 2000. So it's a two-layer network. Signals come in. So the teacher says a signal and then provides a meaning. And then the learner, perhaps using a sigmoid non-linearity. So probability of ones and zeros for all the potential meanings compares it to the actual meaning backpropagates and thus learns to map from signals to meaning. So that's the plot. And of course, the thing about this is that this neural net has been trained on only these exemplars, the small part of the space of meanings that form the teaching event from the teacher to the learner. But the mechanism, of course, provides a map from any signal to a meaning. So that's the first part of the teaching. But then the next part, which is that the learner has to also get a map from meanings to signals. And so the way that's done is using a version without worrying about it too much. Here's a two-bit example. So here we have a signal mapping to a meaning, using the neural net. So what the learner is learning is the correct mapping. So maybe the correct mapping here, the mapping that the teacher is trying to teach, is that one zero goes to one one. But literally speaking, the learner can provide from this mapping, coming from their neural net, a probability for all four potential meanings. And this is a contrived example, so that the probabilities are 0.1, 0.1, 0.7, and 0.1, if you obviously multiply P1 by P2, or 1 minus P2, or whatever. And so in that way, the learner can produce a table of all possible maps from signal to meaning. So here are the signals, and these are the probabilities that give meanings. And in a version, all you do, you run the table the other way. So basically, if you're given one one one, so if you see the signal one zero, your neural net tells you these are the probabilities of the different meanings. And to get a map from the other way from a meaning to a signal, you look across this way and see that 0.7 is the largest number. And then you decide that in your aversion, in your inverting of the map for meanings to signals, you'll map one one to one zero, because that's associated with the highest probability. So that's the process of aversion. You read across each of these lines, you put a one there, and then this gives you the map. So zero zero will map to one one. So if the learner wants to randomly decides to teach the next learner the signal from zero zero, they'll say zero zero, sorry, they'll say one one, that's the signal, and they'll point out that that corresponds to the meaning zero zero. So that's the that's the iterated language model. You can see that this aspect of it is completely unrealistic and something that you might want to get rid of. But it's the thing that seems to work. And what the reason, of course, it's unrealistic is that it requires that the learner on reaching maturity goes through all potential, all potential signals and all potential meanings, works out the corresponding probabilities, and then does this aversion map. And that's obviously not true of language. The whole point of language is that you can't go through, you know, you don't have to go through all meanings and all signals. And even from a computational point of view, it's extremely resource heavy. And even with modern computers, it means that you couldn't. So the hope is that you could look at this iterated language model far beyond the sort of eight bit examples they're doing here, maybe even use it, you know, on language itself. So replace the set of meanings with actual sentences and force the agents to come up with their own internal languages and see what happens. But so anyway, with this aversion process, you do get a way of mapping. And so you can run the iterated language model. This is just us recapitulating what Kirby and co-workers saw 20 years ago. You can see that if you do this, you know, with a suitable size of bottleneck, so 256 meanings, you allow 50 of them to be taught, you run it across generations, the expressivity goes up, stability. So this is the instability, the opposite of stability, that goes down. And more remarkably still, the compositionality increases. So this I think is intriguing and worth sort of reminding us. Sorry, this is just some, you know, interview. There's two different measures here going on of compositionality. But it's basically measure of positionality based on entropy. But I mean, I do think it's remarkable. And I think it's worth us going back to the iterated language model and trying to see what it tells us about language evolution. To get rid of the inverter, it turns out, you think it's going to be easy. So for example, you think that you can just add another, you know, in the learning process, you can think that the learner could have, you know, a meaning and then also learn its inversion at the same time, stick in as an objective function, getting the right meaning, but also recovering the original signal. That seems like it would almost certainly work. And I guess this is one of the sort of few original results we have in this area. And it's a sort of a negative one, which is that it turns out that a version, although clunky, something that you hope might just be a convenient way of doing the inversion is necessary. It creates some sort of pressure which drives apart the mapping and makes the mapping from signals to meanings onto and produces expressivity. So if you just replace it with a recurrent neural network, well, what you might call a recurrent neural network, with this architecture here, you lose that expressivity. I think that it's, so what we're seeing basically is that the bottleneck, which forces the neural net to generalize or sorry, the agents to generalize is producing compositionality. But the aversion is also required for expressivity. The bottleneck also tends to cause this pushing together of the mapping, which loses the impressiveness of the language. And so I think there's ways around that. I think, you know, maybe if you think the whole thing is a sort of Bayesian reconstruction problem, you can come up with a new objective function, which has a sort of contrastive term, which forces it not only to get the right mapping from signals to meanings, but also punish it for other meanings being close in probability. When you started doing that, it hasn't worked yet. It's still not producing the expressive map. It goes up for a while and then something happens. I'm sure we can sort that out. But the plan here, the hope, the conclusion is that there's a lot going on in trying to understand how compositionality and expressivity kind of rise in these simple agent models. It would be nice to have a sort of working example first and then try and decide, you know, could we generalize it to much larger, more difficult cases? But I think, you know, the problem is that, this slide is just to remind me to sort of announce the problem, which is that, you know, with these models, you're starting to worry about how much you just follow your own footsteps. You start putting things into the models to produce some sort of behavior. And then you start to worry about exactly what we can compare to when we look, you know, how we compare the model to the actual behavior of languages, how easy it will be to decide, you know, whether the model is a good one or not, and then work out from whether the model is a good one or not, you know, what is it about it that it tells it? What does it tell us about the brain? I mean, one thing we sort of, one of the things we try, obviously, is look at how it affects language, language synchrony between different communities. So here we've, we've taken the iterated language model, and we've stopped it being a simple learner to, to, teacher-to-learner interaction, but rather have some sort of web of interactions where a learner has a privileged teacher, but also learns from, from their community. And then we take networks where there's greater connectivity within, within a cave. It's, you know, it's one of these cave people graphs. So there's greater connectivity inside a cave, then across the caves, and we look at how the languages evolve and stabilize. And we discovered that you can, you know, depending on your parameter choices, you can end up with, you know, five different languages in six caves, this cave, they don't even have their own language, or a case where there's, everybody speaks the same language, or where some languages are shared and some of them are different, or where there's two different languages. And so you can use this model to try and understand properties of the distribution or the sizes of language, languages. But the problem there, of course, is that there's millions of parameters, you have to work out how to, how to, you know, what the network should look like, how to structure the teaching events, how to, how to structure the size of the bottlenecks compared to all possible being single pairs, etc. And so the model, although intended as a very simple one, when you start trying to apply it in ways that can give you data that you could compare to the real world, the model becomes still simple, too simple to produce something, you know, directly comparable to language, but much too complicated to draw easy conclusions from. And so it struck me at this point that we should try and think of what the very simplest model of language evolution is. And that's what I wanted to finish by talking about briefly. So the idea, here, is that we want to look at language change and look at the, the, the most personmonious description of what goes on as languages change. And so, obviously, one property that languages have to have is alignment. If you're talking to someone, you want their language to be almost the same as your own, or else you won't have, you won't be able to understand them. And the closer their language is to yours, the less sort of cognitive low communication will play, will place on you. So I think the first properties of languages as they change is that they should, should align. The second property is the converse, this inclination towards change. You know, obviously, teenagers do this all the time. I have teenagers, they, they, they use words in different ways. They, they delight in language invention. I mean, weirdly, my, my mother is almost the very opposite of a teenager. It's the same. She's forever making up new words and new ways of saying things, often based on puns. And then the puns turn into words. And then she just uses them in everyday conversation, expects people to, to follow. And she just does it in a, out of a sheer delight in language invention. There's also a kind of a more strategic point to language invention, which is to find shorthands for saying things, auxiliary words are pushed in with the words that their exiliaries to pronunciations are changed. People say small phrases to mean more complicated, large phrase things, et cetera. So there, there, there, there are these two sort of competing forces that work in language changes. The third one as well, which is an inclination towards consistency. That's exactly the sort of thing that the bottom leg in the iterated language model deals with. And I'm not going to deal with that one here. That's the idea that, you know, if, if in one sentence you put, you know, if at some point you decide that the verb goes at the end of the sentence, it does that for, for, you know, that can sit that, that rule doesn't just apply to one, one's type of sentence, but rolls through the language. So once something starts changing, other parts of language change to suit it. So that's an inclination towards consistency. You know, and it's obviously kind of amazing. I mean, if you think of how how, how noun and verb modifications work with Arabic and Hebrew, I mean, it's just incredible the way that, you know, everything has three, three consonants, and then you've got the verbs and the endings and, you know, how did that happen? I mean, it's just some, some small changes then became rules. And so that is obviously an important part, but not one we'll talk about now. So obviously agreement alignment between speakers and spontaneous change. You probably guessed when we're going with this, that that's a nising model. So in the nising model, it's a model of magnetism, as you'll all know, at different lattice sites, you've got a spin, the spin goes upwards or downwards. And there's an energy associated with, with the alignment of the spins. It's a lower energy state. If the spins are aligned, then if they're not aligned. And so you would obviously expect an nising model to evolve towards total alignment. But there's also in the nising model thermal effect. So in an nising model, this is the metropolis formulation of the nising model, what you do to evolve the model is you choose a random, you choose a site, and then you consider what would happen if you, if you flip the spin. So we'll label the spins plus one and minus one. And if the, if the consequence of flipping the spin can be written down like that, I hope I have the signs right there. But basically, if the spins become more aligned on average with the neighbors, so the one site, these are the four, for example, neighbors of the middle site, if it becomes an average more aligned with them, then a DE will be negative, and you'll accept the change. If it becomes less aligned, you'll still accept the change. And this is the terrible part with some probability given by the exponential of minus DE divided by T. And so DE is positive in this case. If DE is negative, as I said, you always accept the change. T is the temperature. In the case of when you're modeling magnetism, it's literally the temperature. If T is very large, then this is always one, you always accept the change. And so you just get a random up and down. If T is very small, then this will be near to zero. You'll never accept the change. And you'll just get it flowing to total alignment. And so you get these different patterns. This is the very random. This is the very aligned. And this is the critical point in between. So this is a model of a phase change. And at the phase point, at the critical point, you have scaling behavior at the cluster size. And that's what people are interested in the IC model. So obviously, we can map these two things, the movement towards alignment, towards the idea that you're trying to lower the energy. And the spontaneous change is like the thermal fluctuations. But clearly, of course, if you had only one spin, then you'd have only two languages, the upper language and the down language. And that wouldn't be very interesting. So what we do instead is we've got a vector of spins, d dimensions, where d is some number, you have to be chosen. And we imagine that these are coding for different properties of the language. So one spin might be, how do you say father? And obviously in lots of languages like Latin, German, Irish, the word is almost the same as that would be plus one. Do you use derivative of pattern for the word for father? And minus one would be languages where you don't. I don't know any languages that don't use pattern like derivatives. Anyway, not a good example. Or it could be something to do with word order. So here you see the word order in English, subject followed by, followed by verb and adjective followed by noun, whereas in Irish, it's verb followed by subject, noun followed by adjective, and they would be, you know, correspond to two further spins. And then a further complication is that that choice tends to be the same. So if the subject comes before the verb, the adjective comes before the the noun. If the subject comes after the verb, it's the other way around. And so you can imagine there might be two spins, one for this part of the word order, one for that part of the word order, and then in some elaborations model, there'd be some relationship between them. But that's basically the model we have. We have, as it were, in this first idea, we've got D independent ising models, and a given set of plus ones and minus ones constitutes a language, we choose some temperature, we run it forward and we get some behavior. I do have a graph in a minute, I'll show you. But the main thing that you do see is that because it's lots of interlocking patches, one for each of the D dimensions, you tend to see language continuum. And language continuum are a property of languages. So, you know, these days, everything's a bit more complicated, national boundaries and official, you know, government documents and radio stations and so on. But in the olden days, you tended to have no hard language barriers, boundaries. So if you walked from, you know, Portugal all the way to Sicily, well, Portuguese would be very different from Sicilian, but as you do it, you'd never actually be somewhere where we're, well, depending on your route, and that's going to be the point, if you go this way, you're never going to be somewhere where people in nearby villages can't understand each other. The languages gradually change one to the other. And so that property of language distributions is well reflected by this ising model of language, and obviously the temperature determines how big these clusters are and so on. And that's something that you might try and fit against some knowledge of the distribution of languages. But the problem, as you could probably anticipate, occurs here, which is the Basque country. And if you, if you instead of walking from Portugal through Galician and Castilian and Arganese, you kept along the coast as well, you might, particularly if you're a pilgrim. Well, if you're a pilgrim, you're going the other way. But either way, if you strayed into the Basque country, you would encounter a linguistic barrier. Basque is quite, quite different from, it's not an Indo-European language. This is a sign, remarkably, in French Gascon, which is a Pocotin language. And Basque, the Basque is this here. And you can see that it's very different from the others. This is Basque here as well. And so there is, in real languages, a language barrier. There is the possibility of language barriers. And also, of course, you know that we don't have to be able to talk to everybody. You know, we, we can have neighbors who speak a different language. And we can talk to them without necessarily aligning our language to theirs. We can speak to them, for example, through a lingua franca. These days, using translation by using somebody who's bilingual or being bilingual ourselves. This is an example of Kiswahili, which is a language that, you know, 100 million people, that's not a bit much, a large number of 10 million people can speak as a second language, but only a million as a first language. And because it exists as a lingua franca, allowing people who don't have a mutual language to live beside each other. So the next version of the language evolution model is this preference sizing model. So the idea here is to run the same sort of dynamics, alignment and thermal change, but only allow or only have each side interact with which of the ever of the four sides around it, in the simplest case, has the most similar language to its own. So, so the idea is that for each of the, for each pair of, of sites, each pair of speakers, I guess, you can work out the difference between their languages. And then you could, you need to, you only run the sizing model between the speaker you've randomly chosen to consider changing one of their plus ones and minus ones, and whichever neighbor is closest to it. And so that is the idea being that you only speak to the people who speak the same language as you. And so that's the new version, there's a paper about it there, in ALI. And it kind of works. So there's lots more to be done on this, but this is, this is the basic idea. This is the originalizing model. And here is a histogram of, so this is only five dimensions, you can run it in far more, you know, five different language attributes, you can run in far higher dimensions. But in the case of the first sizing model, you can see that this is the number of different, average number of differences between the speaker and their neighbor. And you can see that the speakers and their neighbors tend to have very similar languages. And very few people, very few pairs of people have languages that have very little in common. In other words, this version of the model does not allow for linguistic borders. Whereas in this preference model, where a comparison, the dynamics is only relative to whichever neighbor has the closest language to you, you do have lots of pairs where people speak the same language or very similar languages, but you do have pairs where people speak very different languages. So this is the, this is my proposal of the simplest possible model of language evolution. This work has only just started, but the idea is to consider, you know, consider different structures of preference and how many neighbors you interact with and so on, and then try and find different temperatures and then find the structure of the size of language groups there and compare it to real day clear. And so that's something that we'll do in the future. And that's it. Thank you very much. So I was assuming throughout this presentation that the bottleneck in the iterative model kind of corresponds to Chomsky's poverty of stimulus argument about language learning. Is that right? Corresponds to, I mean, it's, it's, it's meant to be representative of the fact that people, children only like experience a very limited amount of, but it adds something extra to what Chomsky says. So Chomsky uses the poverty of stimulus as evidence that the brain must have, you know, linguistic things, linguistic things. Whereas here, the poverty of, the poverty of stimulus is a mechanism for, it says that the structure of language is a response to the poverty of stimulus. So it changes that. I mean, that's what I think is really nice about this model. It turns that language upside down and says that in fact, it's not that the brain has some special language mechanism, it's that language has, has evolved so that we can generalize from a, from a poverty of stimulus. I was interested in, because I can imagine a more innate style person saying, you know, it's this convergence from the statistics to just a definite mapping in the aversion process that is leading to compositionality, but maybe there is some sort of innate mechanism, which is responsible for something that has the same effect as aversion. Yeah, no, but do you think that the, the work you're doing with this model, is there something that makes you lean more towards the, the empiricist take on this? So, I mean, I think one of, one of the things that we're learning at the moment, you know, you know, obviously, you know, the objection to transformers as a model of language is that they require this massive stimulus. But conversely, they, they, they are very sophisticated at learning grammar. You can see that if you back away from the, from the learning aspect, and you just look at the ability of these models to perform grammatical tasks, it's quite incredible. We do, we do, you know, we do do experiments now where we ask, you know, a transformer or an LSTM or something, can you learn gender agreement? And so I have a student Priyanka who's teaching new words to, to an LSTM. So she takes an LSTM, it's pre-trained. She freezes everything but the representations. She teaches it a new word. She gives it some grammatical context, gender context. So she doesn't French, obviously. So she says, you know, la, we use the word trilobie for some reason, because it's the least common word in the vocabulary list that we're using. So we take the word trilobie, we cut off its representations, we reintroduce it somewhere else in the representation space, and we train just the representation space on a few examples where you use the word la or la, describing trilobie, and then you ask it to do other gender agreement tasks and it does it completely well. So even though it's learning one aspect of gender, it's been taught the gender of the word through one aspect of gender, and it's been tested on another, it has abstracted the, the abstract category of gender. And so, you know, the initial bias that you had, that you needed, you needed special mechanisms, you know, Chomsky and mechanisms to allow the brain to perform the form of manipulations associated with grammar. That, I think, has been demonstrably made false by, by large language models. They, they have this very, I mean, the structure of transformers seems completely bizarrely crappy, right? And yet they do these amazing things. And so, it's quite possible for these, you know, simple network models that are doing statistical learning to learn this stuff. But that's not the same as saying that it can learn it against a poverty of stimulus. But what I think the iterated language model is maybe indicating, and again, we need to start using the iterated language model on much bigger examples. It's, it's indicating that you don't have to put much more in, you know, you, you, you, you, once you start thinking about a version or exactly what the objective function is, and the needs to, to generalize, it's possible that it's not just that the, it's possible that the language that evolved, that you could evolve the language where you can learn from a, from a poverty of stimulus. Does that make sense? I said that in the very roundabout way, but I mean, I think I'm essentially agreeing with your initial, initial point. Feel where you're coming from. Thank you. Thanks for the lovely talk. I think we could have done this at home some day. No, no, no. But yeah, I was wondering, are you, are you thinking with the Schelling model, are you thinking of maybe looking at the, or maybe already have the different patterns of, like actually taking different patterns of like verb order and, and so on, and kind of encoding them and plugging them into this model and seeing what you get out. Because I think, I mean, one, like one thing is, I think with, you know, subject verb, object ordering, there are certain patterns and some patterns are more frequent than others. For example, I don't know. And I guess that wouldn't. I mean, it's, it's very hard to know because we tend to think that the patterns represented by the Indo-European languages are, are much more common. I mean, just to answer that specific point, it is, you know, so you think, for, well, for example, the, the claim is that verb object is much more common than anything else. But that's only if you can't buy speakers, if you can't buy languages, that's maybe not so obviously true. Okay. And Irish, of course, that was one of my original interests is a language where verb and objects aren't beside each other. So, you know, I mean, I think the point with the ISI model is to make the simplest possible model. And so we, you know, I don't think you can retain that advantage to the model, while at the same time tying your coding to specific features, but rather the idea would be to introduce into these dynamics some abstract version of these features. So, so you could include some sense of consistency, which would be an interaction between the spins. So not only are, is a speaker interacting with the, with their neighbors, but there's also an interaction within the spins themselves. And that would be the idea that once you flip one thing, other things should flip as well. But, but I think, you know, there's sort of two, two types of models here. There's one that you might, the iterated language model, where you might actually try and use that to probe actual properties of actual languages. But with the difficulty that, you know, it's, it's, it's not a completely parsimonious model. You always have the problem when you're doing ancient modeling of deciding whether you're looking at your own model or you're looking at the world. And then there's the ISI model, which is supposed to be that there's a simplest possible language, a model of language evolution. And the idea there would be to look at very simple properties such as cluster size, cluster distribution, et cetera, and compare them to real languages, which hasn't been done yet. But that's, that's where we're going. So it could be wrong in assuming this, but sort of tying together the start and the middle of the talk. And in the iterated learning model, you have this mapping between meaning and sensations. And in the EEG experiments, I guess, is, is the, is the purpose there to try to uncover how that mapping is established in the brain and how, you know, maybe, you know, neurophysiology or segregation in the brain biases or influences that mapping itself. Like, is that what you're trying to get at with the EEG experiments? Yeah. I mean, as, as he pointed out, we're a long way from actually doing any of this, but the, that the sort of overall picture is you language has structure. We can discover that structure. How do we discover that structure? Probably, I think the best way to do it is by looking at EEG responses. You know, we do have commentaries on language invented by grammaticians. That's really them trying to impose what they've learned about Latin onto other languages. And it's probably sort of quite naive compared to how the brain considers parts of speech and the relationships between parts of speech. But the parts of speech and their relationships are probably important either as a reflection of, you know, and again, the discussion there is relevant of the mechanisms the brain uses for producing language and understanding language, or the language that the, that's the languages that have had to evolve so that we can learn them despite the poverty of stimulus that we experience as children. And so the iterated language model is trying to find out what, I mean, potentially is trying to find out what those properties of language might be. And we'd like to compare them to the real properties of language, which we discover through EEG. That's the big plan. Where my thoughts were going with that, I was just wondering if there are any, you know, takeaways from that EEG research or from, you know, the neuroscience realm that could perhaps be brought back over to the iterated learning model, but specifically, so in that bottleneck layer and your representational bottleneck in the meaning. Can you take any principles from neuroscience to apply, you know, to constrain that, you know, the structure of the bottleneck? But I mean, the story of my life is that when I started working in neuroscience, I thought, you know, we really want to map from actual neural networks to what they're doing, you know, we want to go from, you know, this is a neuron and this is what it's connected to, this is how it works. And so, you know, I ended up working on tadpoles, because I thought tadpoles are really simple creatures, maybe we can understand tadpoles. And tadpoles, neonatal tadpoles make a decision. So basically, older tadpoles are fearsome hunting animals, but the very, very young ones, when they've just emerged from the egg, and they're still carrying, they carry with them kind of an egg pouch. So the material from the egg that they grew in, so they don't have to eat for a couple of days. And these animals are very, very simple. So you touch them and they swim away. You grab them, they struggle. So there's exactly one decision they have to make, which is whether they've been grabbed or touched. And I thought this is the simplest decision that any creature makes. And there's like eight different neurons involved in this. Maybe we could go from the network to understanding the decision. And I had a PhD student who worked on this model for, you know, four years, reduced it all to two dimensions so we could draw a phase diagrams. And it was like just the most unbelievable mess. And in the end, you know, we just knew nothing about tadpoles, yet alone, you know. So I think, you know, I just don't think that's, I think we're so far away from, you know, something as complicated as language and mapping anything but the broadest principles of neural dynamics to the cognitive dynamics. I think the gulf between, you know, the neural substrate and the cognitive function is so great that I would be trying to stay away from that. The idea is rather to look at the structure of language and ask, look for the structure of language in EEG. So the mechanism there is only to try and find out what the brain regards as grammar. And then to compare that to the iterated language model and how it might, or some other model and how it might treat grammar, basically. Which is not to say that people are, you know, I mean, maybe the cerebellum, you know, maybe the hippocampus. But if you can't understand tadpoles, what could you do? I mean, basically we disproved the existence of tadpoles, I think. I'm sorry, Seth. Do you worry that expanding kind of this type of EEG work to a much broader set of structures would result in the same sort of mess? Yes. I mean, I think it's a good strategy. I agree with you that it seems promising and much simpler than that or some of the stuff that's been done mapping kind of semantic content into the brain with MEG and fMRI. But it turns out like the tadpoles, it's a mess. Yeah, it seems more promising. But I mean, who knows? I mean, yeah, I mean, we, what we've done compared to what we want to do is tiny. So it seems to me to be the most straightforward strategy. But, you know, at the moment, I can't even get my little prince experiment funded. And not only an eye, but these are greater people who are trying to get money for the same thing and get it done. So, but, you know, yeah, I said they're building in Bristol an instrumented museum, an instrumented cinema, so it'll be possible to do simultaneous EEG experiments on 100 people at once with consumer level EEG. So, and I think they're building this thing without any clue what they're going to use it for. So I think, I think there's a potential to persuade, you know, to get hundreds of recordings of people listening to the little prince and lots of different languages. And you can help it feel that you can discover something that way. Because languages are just so different, you know, I mean, you know, if we could get Malay speakers, they don't, they don't, it's an isolating language, they don't change the words for the plural, you know, if we can get speakers of, you know, Irish, they put the verb at a different place. I mean, it's just, there's such a variety, it'll be really interesting to see what we can see. But I agree, you can probably turn into a mess. And I think the way that you can turn into a mess is to try and find things that are too detailed. We have to try and find the very broad principles, you know, because even that's unknown. Is this cinema? Like... Yeah, yeah, they're building this, they've built it. They got funding for something called the Future Institute. And it's good to have this instrumented cinema where people wear EEG and they'll watch films, basically. So, like, when all the new films come out, you can have like the EEG response. Exactly. Yeah, exactly. I mean, I think that's how they solve it. Yeah. I mean, it's nothing to do with me. I mean, it's people in the psychology department. And the idea is that I think they'll have, you know, all sorts of instrumentation, right? They'll have, you know, we have to work out how sweaty people are and the heart rates and, you know, how much they're breathing, you know, and their EEG. So, I mean, it sounds like one of those things that sounds really cool. So, they were able to get money for it. But I think when it comes down to it, they're going to struggle to find experiments. So, hopefully, they'll do experiments for me. Nightmare giving a talk in that room. You can track in real time. Yeah, there's lots of cool things you can do like that. Yeah. Cool. Thanks for coming in.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.64, "text": " Are you still a reader in mathematical neuroscience? Is that still accurate?", "tokens": [50364, 2014, 291, 920, 257, 15149, 294, 18894, 42762, 30, 1119, 300, 920, 8559, 30, 50596], "temperature": 0.0, "avg_logprob": -0.2568456566852072, "compression_ratio": 1.55956678700361, "no_speech_prob": 0.25761809945106506}, {"id": 1, "seek": 0, "start": 4.64, "end": 7.5200000000000005, "text": " I've changed all our titles a little bit. Okay, so a man of many titles.", "tokens": [50596, 286, 600, 3105, 439, 527, 12992, 257, 707, 857, 13, 1033, 11, 370, 257, 587, 295, 867, 12992, 13, 50740], "temperature": 0.0, "avg_logprob": -0.2568456566852072, "compression_ratio": 1.55956678700361, "no_speech_prob": 0.25761809945106506}, {"id": 2, "seek": 0, "start": 9.44, "end": 13.52, "text": " Whatever I was at Bristol, I know you were the head of the Computational Neuroscience Unit,", "tokens": [50836, 8541, 286, 390, 412, 41208, 11, 286, 458, 291, 645, 264, 1378, 295, 264, 37804, 1478, 1734, 8977, 6699, 27894, 11, 51040], "temperature": 0.0, "avg_logprob": -0.2568456566852072, "compression_ratio": 1.55956678700361, "no_speech_prob": 0.25761809945106506}, {"id": 3, "seek": 0, "start": 13.52, "end": 19.52, "text": " and he was also an examiner for my PhD five. So super, super happy to have him here today.", "tokens": [51040, 293, 415, 390, 611, 364, 1139, 4564, 337, 452, 14476, 1732, 13, 407, 1687, 11, 1687, 2055, 281, 362, 796, 510, 965, 13, 51340], "temperature": 0.0, "avg_logprob": -0.2568456566852072, "compression_ratio": 1.55956678700361, "no_speech_prob": 0.25761809945106506}, {"id": 4, "seek": 0, "start": 19.52, "end": 25.04, "text": " Thank you, Connor. Brilliant. Sorry, one second. Cool. Well, I'm super excited to be here. This has", "tokens": [51340, 1044, 291, 11, 33133, 13, 34007, 13, 4919, 11, 472, 1150, 13, 8561, 13, 1042, 11, 286, 478, 1687, 2919, 281, 312, 510, 13, 639, 575, 51616], "temperature": 0.0, "avg_logprob": -0.2568456566852072, "compression_ratio": 1.55956678700361, "no_speech_prob": 0.25761809945106506}, {"id": 5, "seek": 2504, "start": 25.04, "end": 28.72, "text": " always been kind of a legendary location for me. I've always wanted to visit, and", "tokens": [50364, 1009, 668, 733, 295, 257, 16698, 4914, 337, 385, 13, 286, 600, 1009, 1415, 281, 3441, 11, 293, 50548], "temperature": 0.0, "avg_logprob": -0.14942631105176446, "compression_ratio": 1.6918429003021147, "no_speech_prob": 0.10006748139858246}, {"id": 6, "seek": 2504, "start": 30.32, "end": 34.879999999999995, "text": " it's really blown me away. It's an amazing looking place. It's my first time in New Mexico, and oh", "tokens": [50628, 309, 311, 534, 16479, 385, 1314, 13, 467, 311, 364, 2243, 1237, 1081, 13, 467, 311, 452, 700, 565, 294, 1873, 8612, 11, 293, 1954, 50856], "temperature": 0.0, "avg_logprob": -0.14942631105176446, "compression_ratio": 1.6918429003021147, "no_speech_prob": 0.10006748139858246}, {"id": 7, "seek": 2504, "start": 34.879999999999995, "end": 40.08, "text": " my god, the countryside here is just incredible. I assume that it's traditional to start talks here", "tokens": [50856, 452, 3044, 11, 264, 28252, 510, 307, 445, 4651, 13, 286, 6552, 300, 309, 311, 5164, 281, 722, 6686, 510, 51116], "temperature": 0.0, "avg_logprob": -0.14942631105176446, "compression_ratio": 1.6918429003021147, "no_speech_prob": 0.10006748139858246}, {"id": 8, "seek": 2504, "start": 40.08, "end": 45.36, "text": " with some reminiscence about the time that you met Murray Gaumann. So I met Murray Gaumann 20", "tokens": [51116, 365, 512, 890, 259, 5606, 655, 466, 264, 565, 300, 291, 1131, 27291, 10384, 449, 969, 13, 407, 286, 1131, 27291, 10384, 449, 969, 945, 51380], "temperature": 0.0, "avg_logprob": -0.14942631105176446, "compression_ratio": 1.6918429003021147, "no_speech_prob": 0.10006748139858246}, {"id": 9, "seek": 2504, "start": 45.36, "end": 49.36, "text": " years ago. I was still a particle physicist then. He was coming through 20 College Dublin,", "tokens": [51380, 924, 2057, 13, 286, 390, 920, 257, 12359, 42466, 550, 13, 634, 390, 1348, 807, 945, 6745, 42323, 11, 51580], "temperature": 0.0, "avg_logprob": -0.14942631105176446, "compression_ratio": 1.6918429003021147, "no_speech_prob": 0.10006748139858246}, {"id": 10, "seek": 2504, "start": 49.36, "end": 53.04, "text": " where I was working at the time, and he asked me where I was from, and I said I'm from Galway,", "tokens": [51580, 689, 286, 390, 1364, 412, 264, 565, 11, 293, 415, 2351, 385, 689, 286, 390, 490, 11, 293, 286, 848, 286, 478, 490, 7336, 676, 11, 51764], "temperature": 0.0, "avg_logprob": -0.14942631105176446, "compression_ratio": 1.6918429003021147, "no_speech_prob": 0.10006748139858246}, {"id": 11, "seek": 5304, "start": 53.04, "end": 59.12, "text": " which is true. And he said, oh yes, Galway, home of this, Galway, off whose coast the Spanish Armada", "tokens": [50364, 597, 307, 2074, 13, 400, 415, 848, 11, 1954, 2086, 11, 7336, 676, 11, 1280, 295, 341, 11, 7336, 676, 11, 766, 6104, 8684, 264, 8058, 11893, 1538, 50668], "temperature": 0.0, "avg_logprob": -0.16994987099857653, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.006918629631400108}, {"id": 12, "seek": 5304, "start": 59.68, "end": 64.16, "text": " founded in the storm, saving the British Navy the trouble of sinking it. Home too, he said,", "tokens": [50696, 13234, 294, 264, 7679, 11, 6816, 264, 6221, 15659, 264, 5253, 295, 28148, 309, 13, 8719, 886, 11, 415, 848, 11, 50920], "temperature": 0.0, "avg_logprob": -0.16994987099857653, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.006918629631400108}, {"id": 13, "seek": 5304, "start": 64.16, "end": 69.84, "text": " to the myth that the lusty sailors thus wrecked, swam ashore, and bred with the local women to", "tokens": [50920, 281, 264, 9474, 300, 264, 24672, 88, 42036, 8807, 21478, 292, 11, 1693, 335, 12588, 418, 11, 293, 34133, 365, 264, 2654, 2266, 281, 51204], "temperature": 0.0, "avg_logprob": -0.16994987099857653, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.006918629631400108}, {"id": 14, "seek": 5304, "start": 69.84, "end": 77.03999999999999, "text": " create the black haired Irish. Something he said that is not true, the black haired Irish are", "tokens": [51204, 1884, 264, 2211, 324, 1824, 16801, 13, 6595, 415, 848, 300, 307, 406, 2074, 11, 264, 2211, 324, 1824, 16801, 366, 51564], "temperature": 0.0, "avg_logprob": -0.16994987099857653, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.006918629631400108}, {"id": 15, "seek": 5304, "start": 77.03999999999999, "end": 81.84, "text": " presumably the remnants of the indigenous population, the population there before", "tokens": [51564, 26742, 264, 44652, 295, 264, 15511, 4415, 11, 264, 4415, 456, 949, 51804], "temperature": 0.0, "avg_logprob": -0.16994987099857653, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.006918629631400108}, {"id": 16, "seek": 8184, "start": 81.92, "end": 86.96000000000001, "text": " and accounts of population whose traditions and cultures we know from archaeology,", "tokens": [50368, 293, 9402, 295, 4415, 6104, 15643, 293, 12951, 321, 458, 490, 21894, 1793, 11, 50620], "temperature": 0.0, "avg_logprob": -0.1219810052351518, "compression_ratio": 1.6985294117647058, "no_speech_prob": 0.0048151686787605286}, {"id": 17, "seek": 8184, "start": 86.96000000000001, "end": 91.28, "text": " but whose language is wholly lost and unknown. And I was just incredibly impressed by the whole", "tokens": [50620, 457, 6104, 2856, 307, 45157, 2731, 293, 9841, 13, 400, 286, 390, 445, 6252, 11679, 538, 264, 1379, 50836], "temperature": 0.0, "avg_logprob": -0.1219810052351518, "compression_ratio": 1.6985294117647058, "no_speech_prob": 0.0048151686787605286}, {"id": 18, "seek": 8184, "start": 91.28, "end": 96.24000000000001, "text": " thing. I mean, he knew where Galway was. He knew that we had this link with Spain. He knew about", "tokens": [50836, 551, 13, 286, 914, 11, 415, 2586, 689, 7336, 676, 390, 13, 634, 2586, 300, 321, 632, 341, 2113, 365, 12838, 13, 634, 2586, 466, 51084], "temperature": 0.0, "avg_logprob": -0.1219810052351518, "compression_ratio": 1.6985294117647058, "no_speech_prob": 0.0048151686787605286}, {"id": 19, "seek": 8184, "start": 96.24000000000001, "end": 100.32000000000001, "text": " the Spanish Armada sinking. He knew that we all believed, as it turns out incorrectly, that the", "tokens": [51084, 264, 8058, 11893, 1538, 28148, 13, 634, 2586, 300, 321, 439, 7847, 11, 382, 309, 4523, 484, 42892, 11, 300, 264, 51288], "temperature": 0.0, "avg_logprob": -0.1219810052351518, "compression_ratio": 1.6985294117647058, "no_speech_prob": 0.0048151686787605286}, {"id": 20, "seek": 8184, "start": 100.32000000000001, "end": 108.80000000000001, "text": " Spanish sailors had created these sort of black haired folk in Galway. And he told me this", "tokens": [51288, 8058, 42036, 632, 2942, 613, 1333, 295, 2211, 324, 1824, 15748, 294, 7336, 676, 13, 400, 415, 1907, 385, 341, 51712], "temperature": 0.0, "avg_logprob": -0.1219810052351518, "compression_ratio": 1.6985294117647058, "no_speech_prob": 0.0048151686787605286}, {"id": 21, "seek": 10880, "start": 108.8, "end": 112.56, "text": " interesting thing about how these black haired folk are probably the remnants of the indigenous", "tokens": [50364, 1880, 551, 466, 577, 613, 2211, 324, 1824, 15748, 366, 1391, 264, 44652, 295, 264, 15511, 50552], "temperature": 0.0, "avg_logprob": -0.1519611737200322, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.009805905632674694}, {"id": 22, "seek": 10880, "start": 112.56, "end": 117.52, "text": " population. And I was always intrigued by this idea that there was a language that they spoke,", "tokens": [50552, 4415, 13, 400, 286, 390, 1009, 35140, 538, 341, 1558, 300, 456, 390, 257, 2856, 300, 436, 7179, 11, 50800], "temperature": 0.0, "avg_logprob": -0.1519611737200322, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.009805905632674694}, {"id": 23, "seek": 10880, "start": 117.52, "end": 121.67999999999999, "text": " which is now completely lost. The idea of these completely lost languages is lovely.", "tokens": [50800, 597, 307, 586, 2584, 2731, 13, 440, 1558, 295, 613, 2584, 2731, 8650, 307, 7496, 13, 51008], "temperature": 0.0, "avg_logprob": -0.1519611737200322, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.009805905632674694}, {"id": 24, "seek": 10880, "start": 121.67999999999999, "end": 126.88, "text": " It also struck me with something of a show off, but that's true too. But it was very impressive.", "tokens": [51008, 467, 611, 13159, 385, 365, 746, 295, 257, 855, 766, 11, 457, 300, 311, 2074, 886, 13, 583, 309, 390, 588, 8992, 13, 51268], "temperature": 0.0, "avg_logprob": -0.1519611737200322, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.009805905632674694}, {"id": 25, "seek": 10880, "start": 126.88, "end": 133.51999999999998, "text": " And so I'm very, very, very glad to be in the Murray Gilman building. I was, I did start life", "tokens": [51268, 400, 370, 286, 478, 588, 11, 588, 11, 588, 5404, 281, 312, 294, 264, 27291, 17654, 1601, 2390, 13, 286, 390, 11, 286, 630, 722, 993, 51600], "temperature": 0.0, "avg_logprob": -0.1519611737200322, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.009805905632674694}, {"id": 26, "seek": 10880, "start": 133.51999999999998, "end": 137.44, "text": " as a particle physicist and then became a neuroscientist and only started working on language", "tokens": [51600, 382, 257, 12359, 42466, 293, 550, 3062, 257, 28813, 5412, 468, 293, 787, 1409, 1364, 322, 2856, 51796], "temperature": 0.0, "avg_logprob": -0.1519611737200322, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.009805905632674694}, {"id": 27, "seek": 13744, "start": 137.44, "end": 142.56, "text": " recently. So I don't know that much about it. But the reason I started working on language was I", "tokens": [50364, 3938, 13, 407, 286, 500, 380, 458, 300, 709, 466, 309, 13, 583, 264, 1778, 286, 1409, 1364, 322, 2856, 390, 286, 50620], "temperature": 0.0, "avg_logprob": -0.15737755234177048, "compression_ratio": 1.6, "no_speech_prob": 0.007956712506711483}, {"id": 28, "seek": 13744, "start": 142.56, "end": 147.84, "text": " read this kind of bizarre quote from Novichonsky, which says, in their essential properties,", "tokens": [50620, 1401, 341, 733, 295, 18265, 6513, 490, 31948, 480, 892, 4133, 11, 597, 1619, 11, 294, 641, 7115, 7221, 11, 50884], "temperature": 0.0, "avg_logprob": -0.15737755234177048, "compression_ratio": 1.6, "no_speech_prob": 0.007956712506711483}, {"id": 29, "seek": 13744, "start": 147.84, "end": 151.92, "text": " and even down to fine details, languages are cast in the same mold. At the Martian scientists,", "tokens": [50884, 293, 754, 760, 281, 2489, 4365, 11, 8650, 366, 4193, 294, 264, 912, 11102, 13, 1711, 264, 5807, 952, 7708, 11, 51088], "temperature": 0.0, "avg_logprob": -0.15737755234177048, "compression_ratio": 1.6, "no_speech_prob": 0.007956712506711483}, {"id": 30, "seek": 13744, "start": 151.92, "end": 156.07999999999998, "text": " my reasons we conclude that there's a single human language with differences only at the margins.", "tokens": [51088, 452, 4112, 321, 16886, 300, 456, 311, 257, 2167, 1952, 2856, 365, 7300, 787, 412, 264, 30317, 13, 51296], "temperature": 0.0, "avg_logprob": -0.15737755234177048, "compression_ratio": 1.6, "no_speech_prob": 0.007956712506711483}, {"id": 31, "seek": 13744, "start": 156.07999999999998, "end": 161.92, "text": " And that just seemed so, so wrong to me. And, you know, I thought", "tokens": [51296, 400, 300, 445, 6576, 370, 11, 370, 2085, 281, 385, 13, 400, 11, 291, 458, 11, 286, 1194, 51588], "temperature": 0.0, "avg_logprob": -0.15737755234177048, "compression_ratio": 1.6, "no_speech_prob": 0.007956712506711483}, {"id": 32, "seek": 16192, "start": 162.88, "end": 169.83999999999997, "text": " that maybe that is an accident of not, you know, thinking about how different languages are. It", "tokens": [50412, 300, 1310, 300, 307, 364, 6398, 295, 406, 11, 291, 458, 11, 1953, 466, 577, 819, 8650, 366, 13, 467, 50760], "temperature": 0.0, "avg_logprob": -0.16183146783861063, "compression_ratio": 1.7900763358778626, "no_speech_prob": 0.006154784467071295}, {"id": 33, "seek": 16192, "start": 169.83999999999997, "end": 173.04, "text": " seemed to me, you know, the languages are incredibly different from each other. And I'm", "tokens": [50760, 6576, 281, 385, 11, 291, 458, 11, 264, 8650, 366, 6252, 819, 490, 1184, 661, 13, 400, 286, 478, 50920], "temperature": 0.0, "avg_logprob": -0.16183146783861063, "compression_ratio": 1.7900763358778626, "no_speech_prob": 0.006154784467071295}, {"id": 34, "seek": 16192, "start": 173.04, "end": 177.67999999999998, "text": " looking with where I'm from to know a little bit of Irish. And Irish is, you know, although an", "tokens": [50920, 1237, 365, 689, 286, 478, 490, 281, 458, 257, 707, 857, 295, 16801, 13, 400, 16801, 307, 11, 291, 458, 11, 4878, 364, 51152], "temperature": 0.0, "avg_logprob": -0.16183146783861063, "compression_ratio": 1.7900763358778626, "no_speech_prob": 0.006154784467071295}, {"id": 35, "seek": 16192, "start": 177.67999999999998, "end": 184.72, "text": " inter-European language is more different to, you know, English than maybe French and so on is.", "tokens": [51152, 728, 12, 32293, 282, 2856, 307, 544, 819, 281, 11, 291, 458, 11, 3669, 813, 1310, 5522, 293, 370, 322, 307, 13, 51504], "temperature": 0.0, "avg_logprob": -0.16183146783861063, "compression_ratio": 1.7900763358778626, "no_speech_prob": 0.006154784467071295}, {"id": 36, "seek": 16192, "start": 184.72, "end": 188.32, "text": " I mean, for starters, it has a different word order. One of the more striking features is the,", "tokens": [51504, 286, 914, 11, 337, 35131, 11, 309, 575, 257, 819, 1349, 1668, 13, 1485, 295, 264, 544, 18559, 4122, 307, 264, 11, 51684], "temperature": 0.0, "avg_logprob": -0.16183146783861063, "compression_ratio": 1.7900763358778626, "no_speech_prob": 0.006154784467071295}, {"id": 37, "seek": 18832, "start": 189.28, "end": 194.48, "text": " it doesn't have a verb possession. So here, this is the Irish for, I have a newspaper, Thon,", "tokens": [50412, 309, 1177, 380, 362, 257, 9595, 20935, 13, 407, 510, 11, 341, 307, 264, 16801, 337, 11, 286, 362, 257, 13669, 11, 334, 266, 11, 50672], "temperature": 0.0, "avg_logprob": -0.15737610771542504, "compression_ratio": 1.7564575645756457, "no_speech_prob": 0.0017566286260262132}, {"id": 38, "seek": 18832, "start": 194.48, "end": 201.28, "text": " Newton, Ogham. And what's happening there is that there's no, the Thon is just the verb is. So the", "tokens": [50672, 19541, 11, 14883, 4822, 13, 400, 437, 311, 2737, 456, 307, 300, 456, 311, 572, 11, 264, 334, 266, 307, 445, 264, 9595, 307, 13, 407, 264, 51012], "temperature": 0.0, "avg_logprob": -0.15737610771542504, "compression_ratio": 1.7564575645756457, "no_speech_prob": 0.0017566286260262132}, {"id": 39, "seek": 18832, "start": 201.28, "end": 206.64, "text": " literal translation of that sentence is the newspaper is at me. The Ogham is a preposition", "tokens": [51012, 20411, 12853, 295, 300, 8174, 307, 264, 13669, 307, 412, 385, 13, 440, 14883, 4822, 307, 257, 2666, 5830, 51280], "temperature": 0.0, "avg_logprob": -0.15737610771542504, "compression_ratio": 1.7564575645756457, "no_speech_prob": 0.0017566286260262132}, {"id": 40, "seek": 18832, "start": 206.64, "end": 210.88, "text": " combined with the pronoun, which is a feature of Irish. And prepositions do a lot of work in Irish", "tokens": [51280, 9354, 365, 264, 14144, 11, 597, 307, 257, 4111, 295, 16801, 13, 400, 2666, 329, 2451, 360, 257, 688, 295, 589, 294, 16801, 51492], "temperature": 0.0, "avg_logprob": -0.15737610771542504, "compression_ratio": 1.7564575645756457, "no_speech_prob": 0.0017566286260262132}, {"id": 41, "seek": 18832, "start": 210.88, "end": 215.35999999999999, "text": " that it's done by other parts of speech in other languages. So instead of saying that you have", "tokens": [51492, 300, 309, 311, 1096, 538, 661, 3166, 295, 6218, 294, 661, 8650, 13, 407, 2602, 295, 1566, 300, 291, 362, 51716], "temperature": 0.0, "avg_logprob": -0.15737610771542504, "compression_ratio": 1.7564575645756457, "no_speech_prob": 0.0017566286260262132}, {"id": 42, "seek": 21536, "start": 215.36, "end": 220.32000000000002, "text": " a newspaper, you say the newspaper is at you. And similarly, Thon, Newton, Ogham, the new", "tokens": [50364, 257, 13669, 11, 291, 584, 264, 13669, 307, 412, 291, 13, 400, 14138, 11, 334, 266, 11, 19541, 11, 14883, 4822, 11, 264, 777, 50612], "temperature": 0.0, "avg_logprob": -0.12461945284967837, "compression_ratio": 1.7184466019417475, "no_speech_prob": 0.010447660461068153}, {"id": 43, "seek": 21536, "start": 220.32000000000002, "end": 224.16000000000003, "text": " newspapers from me is how you'd say that you wanted the newspaper. So it's strongly that,", "tokens": [50612, 20781, 490, 385, 307, 577, 291, 1116, 584, 300, 291, 1415, 264, 13669, 13, 407, 309, 311, 10613, 300, 11, 50804], "temperature": 0.0, "avg_logprob": -0.12461945284967837, "compression_ratio": 1.7184466019417475, "no_speech_prob": 0.010447660461068153}, {"id": 44, "seek": 21536, "start": 224.16000000000003, "end": 231.04000000000002, "text": " you know, that languages are very different from each other. And that it's wrong to", "tokens": [50804, 291, 458, 11, 300, 8650, 366, 588, 819, 490, 1184, 661, 13, 400, 300, 309, 311, 2085, 281, 51148], "temperature": 0.0, "avg_logprob": -0.12461945284967837, "compression_ratio": 1.7184466019417475, "no_speech_prob": 0.010447660461068153}, {"id": 45, "seek": 21536, "start": 231.04000000000002, "end": 237.60000000000002, "text": " presuppose that languages all spring from the brain in the way that Chomsky is suggesting.", "tokens": [51148, 1183, 10504, 541, 300, 8650, 439, 5587, 490, 264, 3567, 294, 264, 636, 300, 761, 4785, 4133, 307, 18094, 13, 51476], "temperature": 0.0, "avg_logprob": -0.12461945284967837, "compression_ratio": 1.7184466019417475, "no_speech_prob": 0.010447660461068153}, {"id": 46, "seek": 23760, "start": 238.32, "end": 246.79999999999998, "text": " Because, you know, his idea is that the languages bear the imprint of the human mechanism. Whereas,", "tokens": [50400, 1436, 11, 291, 458, 11, 702, 1558, 307, 300, 264, 8650, 6155, 264, 44615, 295, 264, 1952, 7513, 13, 13813, 11, 50824], "temperature": 0.0, "avg_logprob": -0.12101359800858931, "compression_ratio": 1.6985294117647058, "no_speech_prob": 0.016921451315283775}, {"id": 47, "seek": 23760, "start": 246.79999999999998, "end": 250.79999999999998, "text": " for me, as a neuroscientist, at the time, somebody knew about how vision worked and so on,", "tokens": [50824, 337, 385, 11, 382, 257, 28813, 5412, 468, 11, 412, 264, 565, 11, 2618, 2586, 466, 577, 5201, 2732, 293, 370, 322, 11, 51024], "temperature": 0.0, "avg_logprob": -0.12101359800858931, "compression_ratio": 1.6985294117647058, "no_speech_prob": 0.016921451315283775}, {"id": 48, "seek": 23760, "start": 250.79999999999998, "end": 254.32, "text": " it seemed that languages were much more likely to just be the unfolding of some", "tokens": [51024, 309, 6576, 300, 8650, 645, 709, 544, 3700, 281, 445, 312, 264, 44586, 295, 512, 51200], "temperature": 0.0, "avg_logprob": -0.12101359800858931, "compression_ratio": 1.6985294117647058, "no_speech_prob": 0.016921451315283775}, {"id": 49, "seek": 23760, "start": 254.88, "end": 260.56, "text": " search for statistical structure. And maybe something to do with the properties of the world", "tokens": [51228, 3164, 337, 22820, 3877, 13, 400, 1310, 746, 281, 360, 365, 264, 7221, 295, 264, 1002, 51512], "temperature": 0.0, "avg_logprob": -0.12101359800858931, "compression_ratio": 1.6985294117647058, "no_speech_prob": 0.016921451315283775}, {"id": 50, "seek": 23760, "start": 260.56, "end": 265.2, "text": " around us, the fact that there are actions, and there are, and hence verbs, and there are objects,", "tokens": [51512, 926, 505, 11, 264, 1186, 300, 456, 366, 5909, 11, 293, 456, 366, 11, 293, 16678, 30051, 11, 293, 456, 366, 6565, 11, 51744], "temperature": 0.0, "avg_logprob": -0.12101359800858931, "compression_ratio": 1.6985294117647058, "no_speech_prob": 0.016921451315283775}, {"id": 51, "seek": 26520, "start": 265.2, "end": 270.8, "text": " and hence nouns. But of course, I'm no longer sure that I was right in thinking that. And I've", "tokens": [50364, 293, 16678, 48184, 13, 583, 295, 1164, 11, 286, 478, 572, 2854, 988, 300, 286, 390, 558, 294, 1953, 300, 13, 400, 286, 600, 50644], "temperature": 0.0, "avg_logprob": -0.10460574509667568, "compression_ratio": 1.675, "no_speech_prob": 0.010760488919913769}, {"id": 52, "seek": 26520, "start": 270.8, "end": 275.44, "text": " begun to appreciate something of the wisdom of what Chomsky said. And of course, another example", "tokens": [50644, 16009, 281, 4449, 746, 295, 264, 10712, 295, 437, 761, 4785, 4133, 848, 13, 400, 295, 1164, 11, 1071, 1365, 50876], "temperature": 0.0, "avg_logprob": -0.10460574509667568, "compression_ratio": 1.675, "no_speech_prob": 0.010760488919913769}, {"id": 53, "seek": 26520, "start": 275.44, "end": 280.48, "text": " from Irish Thon, Newton Thum, the newspaper is under me. That's how you'd say the newspaper is", "tokens": [50876, 490, 16801, 334, 266, 11, 19541, 334, 449, 11, 264, 13669, 307, 833, 385, 13, 663, 311, 577, 291, 1116, 584, 264, 13669, 307, 51128], "temperature": 0.0, "avg_logprob": -0.10460574509667568, "compression_ratio": 1.675, "no_speech_prob": 0.010760488919913769}, {"id": 54, "seek": 26520, "start": 280.48, "end": 286.0, "text": " about me. But there you can see that even in English, the same work is being done by the", "tokens": [51128, 466, 385, 13, 583, 456, 291, 393, 536, 300, 754, 294, 3669, 11, 264, 912, 589, 307, 885, 1096, 538, 264, 51404], "temperature": 0.0, "avg_logprob": -0.10460574509667568, "compression_ratio": 1.675, "no_speech_prob": 0.010760488919913769}, {"id": 55, "seek": 26520, "start": 286.0, "end": 292.24, "text": " prepositional construction as is being done in the Irish. In other words, although, you know,", "tokens": [51404, 2666, 329, 2628, 6435, 382, 307, 885, 1096, 294, 264, 16801, 13, 682, 661, 2283, 11, 4878, 11, 291, 458, 11, 51716], "temperature": 0.0, "avg_logprob": -0.10460574509667568, "compression_ratio": 1.675, "no_speech_prob": 0.010760488919913769}, {"id": 56, "seek": 29224, "start": 292.24, "end": 296.0, "text": " the Irish uses a different preposition and prepositions tend to vary as you probably know", "tokens": [50364, 264, 16801, 4960, 257, 819, 2666, 5830, 293, 2666, 329, 2451, 3928, 281, 10559, 382, 291, 1391, 458, 50552], "temperature": 0.0, "avg_logprob": -0.11344803896817295, "compression_ratio": 1.7306273062730628, "no_speech_prob": 0.008233639411628246}, {"id": 57, "seek": 29224, "start": 296.0, "end": 301.28000000000003, "text": " a lot from language to language. The idea of using a preposition where you might otherwise", "tokens": [50552, 257, 688, 490, 2856, 281, 2856, 13, 440, 1558, 295, 1228, 257, 2666, 5830, 689, 291, 1062, 5911, 50816], "temperature": 0.0, "avg_logprob": -0.11344803896817295, "compression_ratio": 1.7306273062730628, "no_speech_prob": 0.008233639411628246}, {"id": 58, "seek": 29224, "start": 301.28000000000003, "end": 309.84000000000003, "text": " have a different verb is not so unique to Irish. And so maybe languages are somewhat similar to", "tokens": [50816, 362, 257, 819, 9595, 307, 406, 370, 3845, 281, 16801, 13, 400, 370, 1310, 8650, 366, 8344, 2531, 281, 51244], "temperature": 0.0, "avg_logprob": -0.11344803896817295, "compression_ratio": 1.7306273062730628, "no_speech_prob": 0.008233639411628246}, {"id": 59, "seek": 29224, "start": 309.84000000000003, "end": 313.84000000000003, "text": " each other. And it struck me, or strikes me, as I'm sure it strikes you, that this is a really", "tokens": [51244, 1184, 661, 13, 400, 309, 13159, 385, 11, 420, 16750, 385, 11, 382, 286, 478, 988, 309, 16750, 291, 11, 300, 341, 307, 257, 534, 51444], "temperature": 0.0, "avg_logprob": -0.11344803896817295, "compression_ratio": 1.7306273062730628, "no_speech_prob": 0.008233639411628246}, {"id": 60, "seek": 29224, "start": 313.84000000000003, "end": 319.68, "text": " important question. I love this sign because it says that the vehicles will be prosecuted without", "tokens": [51444, 1021, 1168, 13, 286, 959, 341, 1465, 570, 309, 1619, 300, 264, 8948, 486, 312, 22382, 4866, 1553, 51736], "temperature": 0.0, "avg_logprob": -0.11344803896817295, "compression_ratio": 1.7306273062730628, "no_speech_prob": 0.008233639411628246}, {"id": 61, "seek": 31968, "start": 319.68, "end": 324.48, "text": " warning. But this is a warning sign. So of course, it makes it impossible for them to do that.", "tokens": [50364, 9164, 13, 583, 341, 307, 257, 9164, 1465, 13, 407, 295, 1164, 11, 309, 1669, 309, 6243, 337, 552, 281, 360, 300, 13, 50604], "temperature": 0.0, "avg_logprob": -0.13216376834445528, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.005589461885392666}, {"id": 62, "seek": 31968, "start": 324.48, "end": 331.36, "text": " And I have here started to remind me to comment on the idea that language is complicated because", "tokens": [50604, 400, 286, 362, 510, 1409, 281, 4160, 385, 281, 2871, 322, 264, 1558, 300, 2856, 307, 6179, 570, 50948], "temperature": 0.0, "avg_logprob": -0.13216376834445528, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.005589461885392666}, {"id": 63, "seek": 31968, "start": 331.36, "end": 337.44, "text": " some aspects of language are to do with what's in the world. The structure of language is structured", "tokens": [50948, 512, 7270, 295, 2856, 366, 281, 360, 365, 437, 311, 294, 264, 1002, 13, 440, 3877, 295, 2856, 307, 18519, 51252], "temperature": 0.0, "avg_logprob": -0.13216376834445528, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.005589461885392666}, {"id": 64, "seek": 31968, "start": 337.44, "end": 343.6, "text": " by its use case, by communication. And some parts of language presumably are to do with the", "tokens": [51252, 538, 1080, 764, 1389, 11, 538, 6101, 13, 400, 512, 3166, 295, 2856, 26742, 366, 281, 360, 365, 264, 51560], "temperature": 0.0, "avg_logprob": -0.13216376834445528, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.005589461885392666}, {"id": 65, "seek": 34360, "start": 343.68, "end": 351.52000000000004, "text": " human ability to communicate, or our ability to perceive, or the calculations we do in", "tokens": [50368, 1952, 3485, 281, 7890, 11, 420, 527, 3485, 281, 20281, 11, 420, 264, 20448, 321, 360, 294, 50760], "temperature": 0.0, "avg_logprob": -0.13852250432393637, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.015110031701624393}, {"id": 66, "seek": 34360, "start": 351.52000000000004, "end": 357.12, "text": " producing, interpreting meaning and in producing meaning. And some parts might be more idiosyncratic", "tokens": [50760, 10501, 11, 37395, 3620, 293, 294, 10501, 3620, 13, 400, 512, 3166, 1062, 312, 544, 4496, 2717, 2534, 10757, 2399, 51040], "temperature": 0.0, "avg_logprob": -0.13852250432393637, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.015110031701624393}, {"id": 67, "seek": 34360, "start": 357.12, "end": 362.88, "text": " to do with some linguistic mechanism. And deciding between these, deciding, you know,", "tokens": [51040, 281, 360, 365, 512, 43002, 7513, 13, 400, 17990, 1296, 613, 11, 17990, 11, 291, 458, 11, 51328], "temperature": 0.0, "avg_logprob": -0.13852250432393637, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.015110031701624393}, {"id": 68, "seek": 34360, "start": 362.88, "end": 368.16, "text": " recognizing the fact that we can say something like this, which logically makes no sense at all,", "tokens": [51328, 18538, 264, 1186, 300, 321, 393, 584, 746, 411, 341, 11, 597, 38887, 1669, 572, 2020, 412, 439, 11, 51592], "temperature": 0.0, "avg_logprob": -0.13852250432393637, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.015110031701624393}, {"id": 69, "seek": 36816, "start": 368.16, "end": 374.24, "text": " but which, you know, in a Wisconsinian fashion, quite clearly communicates something. It is a", "tokens": [50364, 457, 597, 11, 291, 458, 11, 294, 257, 17977, 952, 6700, 11, 1596, 4448, 3363, 1024, 746, 13, 467, 307, 257, 50668], "temperature": 0.0, "avg_logprob": -0.1774506838816517, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.003058723174035549}, {"id": 70, "seek": 36816, "start": 374.24, "end": 380.56, "text": " property, you know, is something that's deeply rooted in who we are, and our language is a part", "tokens": [50668, 4707, 11, 291, 458, 11, 307, 746, 300, 311, 8760, 25277, 294, 567, 321, 366, 11, 293, 527, 2856, 307, 257, 644, 50984], "temperature": 0.0, "avg_logprob": -0.1774506838816517, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.003058723174035549}, {"id": 71, "seek": 36816, "start": 380.56, "end": 385.76000000000005, "text": " of how we conceive of ourselves as conscious individuals. And separating what's special", "tokens": [50984, 295, 577, 321, 48605, 295, 4175, 382, 6648, 5346, 13, 400, 29279, 437, 311, 2121, 51244], "temperature": 0.0, "avg_logprob": -0.1774506838816517, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.003058723174035549}, {"id": 72, "seek": 36816, "start": 385.76000000000005, "end": 390.72, "text": " about language, what's special about us, what's special about, with, you know, what language", "tokens": [51244, 466, 2856, 11, 437, 311, 2121, 466, 505, 11, 437, 311, 2121, 466, 11, 365, 11, 291, 458, 11, 437, 2856, 51492], "temperature": 0.0, "avg_logprob": -0.1774506838816517, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.003058723174035549}, {"id": 73, "seek": 36816, "start": 390.72, "end": 396.40000000000003, "text": " necessarily has as a communication medium, seems an interesting and an important challenge.", "tokens": [51492, 4725, 575, 382, 257, 6101, 6399, 11, 2544, 364, 1880, 293, 364, 1021, 3430, 13, 51776], "temperature": 0.0, "avg_logprob": -0.1774506838816517, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.003058723174035549}, {"id": 74, "seek": 39640, "start": 396.88, "end": 403.2, "text": " So, of course, as a neuroscientist, when I decided I was interested in this, the first", "tokens": [50388, 407, 11, 295, 1164, 11, 382, 257, 28813, 5412, 468, 11, 562, 286, 3047, 286, 390, 3102, 294, 341, 11, 264, 700, 50704], "temperature": 0.0, "avg_logprob": -0.09213613580774378, "compression_ratio": 1.883817427385892, "no_speech_prob": 0.002146952785551548}, {"id": 75, "seek": 39640, "start": 403.2, "end": 410.23999999999995, "text": " inclination was to do EEG experiments. And I wanted to start talking a little bit about that", "tokens": [50704, 37070, 2486, 390, 281, 360, 33685, 38, 12050, 13, 400, 286, 1415, 281, 722, 1417, 257, 707, 857, 466, 300, 51056], "temperature": 0.0, "avg_logprob": -0.09213613580774378, "compression_ratio": 1.883817427385892, "no_speech_prob": 0.002146952785551548}, {"id": 76, "seek": 39640, "start": 410.23999999999995, "end": 416.23999999999995, "text": " before talking a little bit more about models of language evolution. I started off actually", "tokens": [51056, 949, 1417, 257, 707, 857, 544, 466, 5245, 295, 2856, 9303, 13, 286, 1409, 766, 767, 51356], "temperature": 0.0, "avg_logprob": -0.09213613580774378, "compression_ratio": 1.883817427385892, "no_speech_prob": 0.002146952785551548}, {"id": 77, "seek": 39640, "start": 416.23999999999995, "end": 420.71999999999997, "text": " trying to do EEG experiments on Irish speakers. I don't know how much you know about Ireland", "tokens": [51356, 1382, 281, 360, 33685, 38, 12050, 322, 16801, 9518, 13, 286, 500, 380, 458, 577, 709, 291, 458, 466, 15880, 51580], "temperature": 0.0, "avg_logprob": -0.09213613580774378, "compression_ratio": 1.883817427385892, "no_speech_prob": 0.002146952785551548}, {"id": 78, "seek": 39640, "start": 420.71999999999997, "end": 424.47999999999996, "text": " that Irish speakers, but it turns out that trying to do EEG experiments on Irish speakers", "tokens": [51580, 300, 16801, 9518, 11, 457, 309, 4523, 484, 300, 1382, 281, 360, 33685, 38, 12050, 322, 16801, 9518, 51768], "temperature": 0.0, "avg_logprob": -0.09213613580774378, "compression_ratio": 1.883817427385892, "no_speech_prob": 0.002146952785551548}, {"id": 79, "seek": 42448, "start": 424.48, "end": 427.92, "text": " is very difficult, because lots of people who claim they can speak Irish actually really can't.", "tokens": [50364, 307, 588, 2252, 11, 570, 3195, 295, 561, 567, 3932, 436, 393, 1710, 16801, 767, 534, 393, 380, 13, 50536], "temperature": 0.0, "avg_logprob": -0.09164713323116302, "compression_ratio": 1.7973421926910298, "no_speech_prob": 0.004918148275464773}, {"id": 80, "seek": 42448, "start": 429.44, "end": 432.24, "text": " As I did a whole series of experiments on people who claim they can speak Irish and", "tokens": [50612, 1018, 286, 630, 257, 1379, 2638, 295, 12050, 322, 561, 567, 3932, 436, 393, 1710, 16801, 293, 50752], "temperature": 0.0, "avg_logprob": -0.09164713323116302, "compression_ratio": 1.7973421926910298, "no_speech_prob": 0.004918148275464773}, {"id": 81, "seek": 42448, "start": 432.24, "end": 437.68, "text": " discovered, in fact, the EEG just told me that they were lying. So I went back to working on", "tokens": [50752, 6941, 11, 294, 1186, 11, 264, 33685, 38, 445, 1907, 385, 300, 436, 645, 8493, 13, 407, 286, 1437, 646, 281, 1364, 322, 51024], "temperature": 0.0, "avg_logprob": -0.09164713323116302, "compression_ratio": 1.7973421926910298, "no_speech_prob": 0.004918148275464773}, {"id": 82, "seek": 42448, "start": 437.68, "end": 443.6, "text": " English speakers. And I was interested in testing this idea that the brain does privilege grammatical", "tokens": [51024, 3669, 9518, 13, 400, 286, 390, 3102, 294, 4997, 341, 1558, 300, 264, 3567, 775, 12122, 17570, 267, 804, 51320], "temperature": 0.0, "avg_logprob": -0.09164713323116302, "compression_ratio": 1.7973421926910298, "no_speech_prob": 0.004918148275464773}, {"id": 83, "seek": 42448, "start": 443.6, "end": 448.48, "text": " structure over mere statistical structure. Now, if any of you have done EEG experiments,", "tokens": [51320, 3877, 670, 8401, 22820, 3877, 13, 823, 11, 498, 604, 295, 291, 362, 1096, 33685, 38, 12050, 11, 51564], "temperature": 0.0, "avg_logprob": -0.09164713323116302, "compression_ratio": 1.7973421926910298, "no_speech_prob": 0.004918148275464773}, {"id": 84, "seek": 42448, "start": 448.48, "end": 451.68, "text": " you'll know just how damn difficult they are. I mean, the idea that we detect", "tokens": [51564, 291, 603, 458, 445, 577, 8151, 2252, 436, 366, 13, 286, 914, 11, 264, 1558, 300, 321, 5531, 51724], "temperature": 0.0, "avg_logprob": -0.09164713323116302, "compression_ratio": 1.7973421926910298, "no_speech_prob": 0.004918148275464773}, {"id": 85, "seek": 45168, "start": 452.56, "end": 456.32, "text": " electrical fields outside the brain is actually slightly surprising, because you'd expect", "tokens": [50408, 12147, 7909, 2380, 264, 3567, 307, 767, 4748, 8830, 11, 570, 291, 1116, 2066, 50596], "temperature": 0.0, "avg_logprob": -0.12316055297851562, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.006095143500715494}, {"id": 86, "seek": 45168, "start": 456.32, "end": 460.40000000000003, "text": " all the electrical fields to cancel out. And the only reason we can detect anything at all", "tokens": [50596, 439, 264, 12147, 7909, 281, 10373, 484, 13, 400, 264, 787, 1778, 321, 393, 5531, 1340, 412, 439, 50800], "temperature": 0.0, "avg_logprob": -0.12316055297851562, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.006095143500715494}, {"id": 87, "seek": 45168, "start": 460.40000000000003, "end": 464.08, "text": " is that there's a slight preponderance of synapses pointing one way around another.", "tokens": [50800, 307, 300, 456, 311, 257, 4036, 2666, 8548, 719, 295, 5451, 2382, 279, 12166, 472, 636, 926, 1071, 13, 50984], "temperature": 0.0, "avg_logprob": -0.12316055297851562, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.006095143500715494}, {"id": 88, "seek": 45168, "start": 464.08, "end": 470.24, "text": " And then, of course, you bathe the whole thing in salty fluid, which makes it hard to detect", "tokens": [50984, 400, 550, 11, 295, 1164, 11, 291, 7362, 675, 264, 1379, 551, 294, 18443, 9113, 11, 597, 1669, 309, 1152, 281, 5531, 51292], "temperature": 0.0, "avg_logprob": -0.12316055297851562, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.006095143500715494}, {"id": 89, "seek": 45168, "start": 470.24, "end": 476.0, "text": " anything. For cognitive experiments, the difficulty is still greater still. And I'll refer to this a", "tokens": [51292, 1340, 13, 1171, 15605, 12050, 11, 264, 10360, 307, 920, 5044, 920, 13, 400, 286, 603, 2864, 281, 341, 257, 51580], "temperature": 0.0, "avg_logprob": -0.12316055297851562, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.006095143500715494}, {"id": 90, "seek": 47600, "start": 476.0, "end": 482.64, "text": " few times. This EEG data is obviously showing somebody having an epileptic fit and they were", "tokens": [50364, 1326, 1413, 13, 639, 33685, 38, 1412, 307, 2745, 4099, 2618, 1419, 364, 41855, 32307, 3318, 293, 436, 645, 50696], "temperature": 0.0, "avg_logprob": -0.12315835592881688, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.006292865611612797}, {"id": 91, "seek": 47600, "start": 482.64, "end": 486.48, "text": " induced by tooth brushing, so they started brushing their teeth, and here they have the fit.", "tokens": [50696, 33991, 538, 11680, 33130, 11, 370, 436, 1409, 33130, 641, 7798, 11, 293, 510, 436, 362, 264, 3318, 13, 50888], "temperature": 0.0, "avg_logprob": -0.12315835592881688, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.006292865611612797}, {"id": 92, "seek": 47600, "start": 486.48, "end": 491.36, "text": " And you can see that it's a very pronounced activity. But for cognitive tasks, of course,", "tokens": [50888, 400, 291, 393, 536, 300, 309, 311, 257, 588, 23155, 5191, 13, 583, 337, 15605, 9608, 11, 295, 1164, 11, 51132], "temperature": 0.0, "avg_logprob": -0.12315835592881688, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.006292865611612797}, {"id": 93, "seek": 47600, "start": 491.36, "end": 496.4, "text": " not only is the activity not so pronounced, I mean, epilepsy is distinguished by the", "tokens": [51132, 406, 787, 307, 264, 5191, 406, 370, 23155, 11, 286, 914, 11, 49680, 307, 21702, 538, 264, 51384], "temperature": 0.0, "avg_logprob": -0.12315835592881688, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.006292865611612797}, {"id": 94, "seek": 47600, "start": 496.4, "end": 502.08, "text": " synchrony of neural activity. So that's exactly the thing that's recently easy to detect.", "tokens": [51384, 19331, 88, 295, 18161, 5191, 13, 407, 300, 311, 2293, 264, 551, 300, 311, 3938, 1858, 281, 5531, 13, 51668], "temperature": 0.0, "avg_logprob": -0.12315835592881688, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.006292865611612797}, {"id": 95, "seek": 50208, "start": 502.08, "end": 507.52, "text": " Cognitive things aren't so distinguished. And secondly, of course, when you're", "tokens": [50364, 383, 2912, 2187, 721, 3212, 380, 370, 21702, 13, 400, 26246, 11, 295, 1164, 11, 562, 291, 434, 50636], "temperature": 0.0, "avg_logprob": -0.12126006561071695, "compression_ratio": 1.7587301587301587, "no_speech_prob": 0.005927952937781811}, {"id": 96, "seek": 50208, "start": 507.52, "end": 512.4, "text": " doing an EEG experiment, when you're a participant, you tend to sort of think other things. You're", "tokens": [50636, 884, 364, 33685, 38, 5120, 11, 562, 291, 434, 257, 24950, 11, 291, 3928, 281, 1333, 295, 519, 661, 721, 13, 509, 434, 50880], "temperature": 0.0, "avg_logprob": -0.12126006561071695, "compression_ratio": 1.7587301587301587, "no_speech_prob": 0.005927952937781811}, {"id": 97, "seek": 50208, "start": 512.4, "end": 515.76, "text": " there, the stimulus is being played to you. You're supposed to be thinking about the stimulus,", "tokens": [50880, 456, 11, 264, 21366, 307, 885, 3737, 281, 291, 13, 509, 434, 3442, 281, 312, 1953, 466, 264, 21366, 11, 51048], "temperature": 0.0, "avg_logprob": -0.12126006561071695, "compression_ratio": 1.7587301587301587, "no_speech_prob": 0.005927952937781811}, {"id": 98, "seek": 50208, "start": 515.76, "end": 519.6, "text": " but in fact, you're thinking, oh my god, I wish I wasn't doing this experiment anymore. Oh my god,", "tokens": [51048, 457, 294, 1186, 11, 291, 434, 1953, 11, 1954, 452, 3044, 11, 286, 3172, 286, 2067, 380, 884, 341, 5120, 3602, 13, 876, 452, 3044, 11, 51240], "temperature": 0.0, "avg_logprob": -0.12126006561071695, "compression_ratio": 1.7587301587301587, "no_speech_prob": 0.005927952937781811}, {"id": 99, "seek": 50208, "start": 519.6, "end": 523.1999999999999, "text": " this is really boring. Oh my god, all people are immortal. I'm going to die someday. I wonder", "tokens": [51240, 341, 307, 534, 9989, 13, 876, 452, 3044, 11, 439, 561, 366, 31414, 13, 286, 478, 516, 281, 978, 19412, 13, 286, 2441, 51420], "temperature": 0.0, "avg_logprob": -0.12126006561071695, "compression_ratio": 1.7587301587301587, "no_speech_prob": 0.005927952937781811}, {"id": 100, "seek": 50208, "start": 523.1999999999999, "end": 530.4, "text": " what I'll have for dinner. I mean, it's very hard to detect cognitive stuff with EEG. So", "tokens": [51420, 437, 286, 603, 362, 337, 6148, 13, 286, 914, 11, 309, 311, 588, 1152, 281, 5531, 15605, 1507, 365, 33685, 38, 13, 407, 51780], "temperature": 0.0, "avg_logprob": -0.12126006561071695, "compression_ratio": 1.7587301587301587, "no_speech_prob": 0.005927952937781811}, {"id": 101, "seek": 53040, "start": 531.36, "end": 537.4399999999999, "text": " the thing that we did, again, following a paradigm that was introduced by David Peeble and Naiding", "tokens": [50412, 264, 551, 300, 321, 630, 11, 797, 11, 3480, 257, 24709, 300, 390, 7268, 538, 4389, 430, 1653, 638, 293, 6056, 327, 278, 50716], "temperature": 0.0, "avg_logprob": -0.12782444033706397, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.00020930256869178265}, {"id": 102, "seek": 53040, "start": 537.4399999999999, "end": 543.04, "text": " and his co-workers, was a frequency tag experiment. So in a frequency tag experiment, to try and", "tokens": [50716, 293, 702, 598, 12, 37101, 11, 390, 257, 7893, 6162, 5120, 13, 407, 294, 257, 7893, 6162, 5120, 11, 281, 853, 293, 50996], "temperature": 0.0, "avg_logprob": -0.12782444033706397, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.00020930256869178265}, {"id": 103, "seek": 53040, "start": 543.04, "end": 550.56, "text": " separate a signal from noise, you concentrate your stimulus at a particular frequency. And so that", "tokens": [50996, 4994, 257, 6358, 490, 5658, 11, 291, 18089, 428, 21366, 412, 257, 1729, 7893, 13, 400, 370, 300, 51372], "temperature": 0.0, "avg_logprob": -0.12782444033706397, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.00020930256869178265}, {"id": 104, "seek": 53040, "start": 550.56, "end": 555.36, "text": " you know, noise tends to stuff that you're not interested in, people thinking about their mortal", "tokens": [51372, 291, 458, 11, 5658, 12258, 281, 1507, 300, 291, 434, 406, 3102, 294, 11, 561, 1953, 466, 641, 27624, 51612], "temperature": 0.0, "avg_logprob": -0.12782444033706397, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.00020930256869178265}, {"id": 105, "seek": 53040, "start": 555.36, "end": 560.0, "text": " end or the end of the experiment, that happens at all sorts of frequencies. But if your stimulus", "tokens": [51612, 917, 420, 264, 917, 295, 264, 5120, 11, 300, 2314, 412, 439, 7527, 295, 20250, 13, 583, 498, 428, 21366, 51844], "temperature": 0.0, "avg_logprob": -0.12782444033706397, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.00020930256869178265}, {"id": 106, "seek": 56000, "start": 560.0, "end": 564.48, "text": " is at a particular frequency, then you can use Fourier transform or whatever to concentrate", "tokens": [50364, 307, 412, 257, 1729, 7893, 11, 550, 291, 393, 764, 36810, 4088, 420, 2035, 281, 18089, 50588], "temperature": 0.0, "avg_logprob": -0.15832266054655375, "compression_ratio": 1.7577464788732395, "no_speech_prob": 0.001777180121280253}, {"id": 107, "seek": 56000, "start": 564.48, "end": 567.76, "text": " on the thing that you're interested in. So in this experiment, we play people", "tokens": [50588, 322, 264, 551, 300, 291, 434, 3102, 294, 13, 407, 294, 341, 5120, 11, 321, 862, 561, 50752], "temperature": 0.0, "avg_logprob": -0.15832266054655375, "compression_ratio": 1.7577464788732395, "no_speech_prob": 0.001777180121280253}, {"id": 108, "seek": 56000, "start": 568.4, "end": 572.56, "text": " adjective noun sentences. We've choose the adjectives and nouns, so they're single syllable.", "tokens": [50784, 44129, 23307, 16579, 13, 492, 600, 2826, 264, 29378, 1539, 293, 48184, 11, 370, 436, 434, 2167, 40151, 13, 50992], "temperature": 0.0, "avg_logprob": -0.15832266054655375, "compression_ratio": 1.7577464788732395, "no_speech_prob": 0.001777180121280253}, {"id": 109, "seek": 56000, "start": 572.56, "end": 576.24, "text": " We record them and then coerce them a little bit so they're all exactly the same length.", "tokens": [50992, 492, 2136, 552, 293, 550, 598, 260, 384, 552, 257, 707, 857, 370, 436, 434, 439, 2293, 264, 912, 4641, 13, 51176], "temperature": 0.0, "avg_logprob": -0.15832266054655375, "compression_ratio": 1.7577464788732395, "no_speech_prob": 0.001777180121280253}, {"id": 110, "seek": 56000, "start": 576.24, "end": 580.48, "text": " We play the syllables at 3.125 hertz. That just turns out to be a particularly good", "tokens": [51176, 492, 862, 264, 45364, 412, 805, 13, 48804, 45830, 13, 663, 445, 4523, 484, 281, 312, 257, 4098, 665, 51388], "temperature": 0.0, "avg_logprob": -0.15832266054655375, "compression_ratio": 1.7577464788732395, "no_speech_prob": 0.001777180121280253}, {"id": 111, "seek": 56000, "start": 580.48, "end": 584.8, "text": " frequency for, it's a comfortable frequency for listening to syllables. We have long, long streams", "tokens": [51388, 7893, 337, 11, 309, 311, 257, 4619, 7893, 337, 4764, 281, 45364, 13, 492, 362, 938, 11, 938, 15842, 51604], "temperature": 0.0, "avg_logprob": -0.15832266054655375, "compression_ratio": 1.7577464788732395, "no_speech_prob": 0.001777180121280253}, {"id": 112, "seek": 56000, "start": 584.8, "end": 588.48, "text": " of these things. I mean, we have old rats, that man, ill wife. It goes on and on forever.", "tokens": [51604, 295, 613, 721, 13, 286, 914, 11, 321, 362, 1331, 25691, 11, 300, 587, 11, 3171, 3836, 13, 467, 1709, 322, 293, 322, 5680, 13, 51788], "temperature": 0.0, "avg_logprob": -0.15832266054655375, "compression_ratio": 1.7577464788732395, "no_speech_prob": 0.001777180121280253}, {"id": 113, "seek": 58848, "start": 589.44, "end": 593.12, "text": " And we choose the adjective nouns quite carefully so that the", "tokens": [50412, 400, 321, 2826, 264, 44129, 48184, 1596, 7500, 370, 300, 264, 50596], "temperature": 0.0, "avg_logprob": -0.12866668190274919, "compression_ratio": 1.6653992395437263, "no_speech_prob": 0.0014550340129062533}, {"id": 114, "seek": 58848, "start": 594.4, "end": 599.12, "text": " bigrams between old and rat and rat and sad are roughly the same. You can't get them exactly", "tokens": [50660, 955, 2356, 82, 1296, 1331, 293, 5937, 293, 5937, 293, 4227, 366, 9810, 264, 912, 13, 509, 393, 380, 483, 552, 2293, 50896], "temperature": 0.0, "avg_logprob": -0.12866668190274919, "compression_ratio": 1.6653992395437263, "no_speech_prob": 0.0014550340129062533}, {"id": 115, "seek": 58848, "start": 599.12, "end": 603.2, "text": " the same, but you try and keep the statistical structure the same. And then what you see here", "tokens": [50896, 264, 912, 11, 457, 291, 853, 293, 1066, 264, 22820, 3877, 264, 912, 13, 400, 550, 437, 291, 536, 510, 51100], "temperature": 0.0, "avg_logprob": -0.12866668190274919, "compression_ratio": 1.6653992395437263, "no_speech_prob": 0.0014550340129062533}, {"id": 116, "seek": 58848, "start": 603.2, "end": 608.64, "text": " is that obviously there's going to be a response in the EG to the stimulus at 3.125 hertz. But", "tokens": [51100, 307, 300, 2745, 456, 311, 516, 281, 312, 257, 4134, 294, 264, 462, 38, 281, 264, 21366, 412, 805, 13, 48804, 45830, 13, 583, 51372], "temperature": 0.0, "avg_logprob": -0.12866668190274919, "compression_ratio": 1.6653992395437263, "no_speech_prob": 0.0014550340129062533}, {"id": 117, "seek": 58848, "start": 608.64, "end": 614.64, "text": " there's something else happening, which is the noun phrase. So if there's a response at 1.5625", "tokens": [51372, 456, 311, 746, 1646, 2737, 11, 597, 307, 264, 23307, 9535, 13, 407, 498, 456, 311, 257, 4134, 412, 502, 13, 18317, 6074, 51672], "temperature": 0.0, "avg_logprob": -0.12866668190274919, "compression_ratio": 1.6653992395437263, "no_speech_prob": 0.0014550340129062533}, {"id": 118, "seek": 61464, "start": 614.64, "end": 619.84, "text": " hertz, that's a response to something that's not directly in the stimulus, but is related to the", "tokens": [50364, 45830, 11, 300, 311, 257, 4134, 281, 746, 300, 311, 406, 3838, 294, 264, 21366, 11, 457, 307, 4077, 281, 264, 50624], "temperature": 0.0, "avg_logprob": -0.13150126654822547, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.003622033866122365}, {"id": 119, "seek": 61464, "start": 619.84, "end": 625.1999999999999, "text": " meaning of the words. And what interpretation is that that's because the brain privileges noun phrases", "tokens": [50624, 3620, 295, 264, 2283, 13, 400, 437, 14174, 307, 300, 300, 311, 570, 264, 3567, 32588, 23307, 20312, 50892], "temperature": 0.0, "avg_logprob": -0.13150126654822547, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.003622033866122365}, {"id": 120, "seek": 61464, "start": 625.1999999999999, "end": 631.84, "text": " over not noun phrases. And the other example is an adjective verb stream. In the adjective verb", "tokens": [50892, 670, 406, 23307, 20312, 13, 400, 264, 661, 1365, 307, 364, 44129, 9595, 4309, 13, 682, 264, 44129, 9595, 51224], "temperature": 0.0, "avg_logprob": -0.13150126654822547, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.003622033866122365}, {"id": 121, "seek": 61464, "start": 631.84, "end": 638.88, "text": " stream, we make sure we have the same sort of biogram structures as we had for the adjective", "tokens": [51224, 4309, 11, 321, 652, 988, 321, 362, 264, 912, 1333, 295, 3228, 12820, 9227, 382, 321, 632, 337, 264, 44129, 51576], "temperature": 0.0, "avg_logprob": -0.13150126654822547, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.003622033866122365}, {"id": 122, "seek": 61464, "start": 638.88, "end": 643.52, "text": " noun. It's likely to come beside each other as old and man or whatever it was I had before.", "tokens": [51576, 23307, 13, 467, 311, 3700, 281, 808, 15726, 1184, 661, 382, 1331, 293, 587, 420, 2035, 309, 390, 286, 632, 949, 13, 51808], "temperature": 0.0, "avg_logprob": -0.13150126654822547, "compression_ratio": 1.791044776119403, "no_speech_prob": 0.003622033866122365}, {"id": 123, "seek": 64464, "start": 644.96, "end": 652.64, "text": " But now there is no grammatical structure. So if we see a response at 1.5625 hertz", "tokens": [50380, 583, 586, 456, 307, 572, 17570, 267, 804, 3877, 13, 407, 498, 321, 536, 257, 4134, 412, 502, 13, 18317, 6074, 45830, 50764], "temperature": 0.0, "avg_logprob": -0.11213724462835638, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0004687004548031837}, {"id": 124, "seek": 64464, "start": 654.4, "end": 659.76, "text": " for the first stimulus and not for the second, that's indicative of the brain responding hopefully.", "tokens": [50852, 337, 264, 700, 21366, 293, 406, 337, 264, 1150, 11, 300, 311, 47513, 295, 264, 3567, 16670, 4696, 13, 51120], "temperature": 0.0, "avg_logprob": -0.11213724462835638, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0004687004548031837}, {"id": 125, "seek": 64464, "start": 659.76, "end": 664.48, "text": " It's indicative to the brain privileging a grammatical structure, which was I guess the", "tokens": [51120, 467, 311, 47513, 281, 264, 3567, 8670, 3249, 257, 17570, 267, 804, 3877, 11, 597, 390, 286, 2041, 264, 51356], "temperature": 0.0, "avg_logprob": -0.11213724462835638, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0004687004548031837}, {"id": 126, "seek": 64464, "start": 664.48, "end": 669.2, "text": " one thing to be interested in. It turns out these data are quite hard to analyze.", "tokens": [51356, 472, 551, 281, 312, 3102, 294, 13, 467, 4523, 484, 613, 1412, 366, 1596, 1152, 281, 12477, 13, 51592], "temperature": 0.0, "avg_logprob": -0.11213724462835638, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0004687004548031837}, {"id": 127, "seek": 64464, "start": 670.0, "end": 673.52, "text": " You know, you think all you have to do then is take the Fourier transform and look at the size", "tokens": [51632, 509, 458, 11, 291, 519, 439, 291, 362, 281, 360, 550, 307, 747, 264, 36810, 4088, 293, 574, 412, 264, 2744, 51808], "temperature": 0.0, "avg_logprob": -0.11213724462835638, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0004687004548031837}, {"id": 128, "seek": 67352, "start": 673.52, "end": 680.72, "text": " of the peak at those particular frequencies. But EG is extremely noisy. And so if you do that,", "tokens": [50364, 295, 264, 10651, 412, 729, 1729, 20250, 13, 583, 462, 38, 307, 4664, 24518, 13, 400, 370, 498, 291, 360, 300, 11, 50724], "temperature": 0.0, "avg_logprob": -0.12359829743703206, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.0028829562943428755}, {"id": 129, "seek": 67352, "start": 680.72, "end": 687.1999999999999, "text": " you don't see anything. What you have to do is look for the phase locked component of the response.", "tokens": [50724, 291, 500, 380, 536, 1340, 13, 708, 291, 362, 281, 360, 307, 574, 337, 264, 5574, 9376, 6542, 295, 264, 4134, 13, 51048], "temperature": 0.0, "avg_logprob": -0.12359829743703206, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.0028829562943428755}, {"id": 130, "seek": 67352, "start": 687.1999999999999, "end": 693.76, "text": " So you play lots of these streams, so you make each stream five seconds long. You play repeatedly", "tokens": [51048, 407, 291, 862, 3195, 295, 613, 15842, 11, 370, 291, 652, 1184, 4309, 1732, 3949, 938, 13, 509, 862, 18227, 51376], "temperature": 0.0, "avg_logprob": -0.12359829743703206, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.0028829562943428755}, {"id": 131, "seek": 67352, "start": 693.76, "end": 698.48, "text": " the stream. And then you look for that portion of the response, you know, once you've taken the", "tokens": [51376, 264, 4309, 13, 400, 550, 291, 574, 337, 300, 8044, 295, 264, 4134, 11, 291, 458, 11, 1564, 291, 600, 2726, 264, 51612], "temperature": 0.0, "avg_logprob": -0.12359829743703206, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.0028829562943428755}, {"id": 132, "seek": 69848, "start": 698.96, "end": 705.28, "text": " transform, that portion of the response which has a similar phase. And the complication, of course,", "tokens": [50388, 4088, 11, 300, 8044, 295, 264, 4134, 597, 575, 257, 2531, 5574, 13, 400, 264, 1209, 8758, 11, 295, 1164, 11, 50704], "temperature": 0.0, "avg_logprob": -0.11491304828274634, "compression_ratio": 1.7130044843049328, "no_speech_prob": 0.0074921720661222935}, {"id": 133, "seek": 69848, "start": 705.28, "end": 711.12, "text": " is that the overall phase is not so important. It depends on how big your head is and where the", "tokens": [50704, 307, 300, 264, 4787, 5574, 307, 406, 370, 1021, 13, 467, 5946, 322, 577, 955, 428, 1378, 307, 293, 689, 264, 50996], "temperature": 0.0, "avg_logprob": -0.11491304828274634, "compression_ratio": 1.7130044843049328, "no_speech_prob": 0.0074921720661222935}, {"id": 134, "seek": 69848, "start": 711.12, "end": 719.12, "text": " electrode is and all sorts of other things. And so the actual phase of people's responses", "tokens": [50996, 38346, 307, 293, 439, 7527, 295, 661, 721, 13, 400, 370, 264, 3539, 5574, 295, 561, 311, 13019, 51396], "temperature": 0.0, "avg_logprob": -0.11491304828274634, "compression_ratio": 1.7130044843049328, "no_speech_prob": 0.0074921720661222935}, {"id": 135, "seek": 69848, "start": 719.84, "end": 725.9200000000001, "text": " at whatever frequency, at one point, whatever it is, isn't important. And you can see that here.", "tokens": [51432, 412, 2035, 7893, 11, 412, 472, 935, 11, 2035, 309, 307, 11, 1943, 380, 1021, 13, 400, 291, 393, 536, 300, 510, 13, 51736], "temperature": 0.0, "avg_logprob": -0.11491304828274634, "compression_ratio": 1.7130044843049328, "no_speech_prob": 0.0074921720661222935}, {"id": 136, "seek": 72592, "start": 725.92, "end": 730.4, "text": " So this is just each of these dots, each of these lines corresponds to a participant.", "tokens": [50364, 407, 341, 307, 445, 1184, 295, 613, 15026, 11, 1184, 295, 613, 3876, 23249, 281, 257, 24950, 13, 50588], "temperature": 0.0, "avg_logprob": -0.13227095297717173, "compression_ratio": 1.954732510288066, "no_speech_prob": 0.002428121631965041}, {"id": 137, "seek": 72592, "start": 730.4, "end": 734.9599999999999, "text": " We had 16 participants in this experiment. And then for each participant, we've got all the", "tokens": [50588, 492, 632, 3165, 10503, 294, 341, 5120, 13, 400, 550, 337, 1184, 24950, 11, 321, 600, 658, 439, 264, 50816], "temperature": 0.0, "avg_logprob": -0.13227095297717173, "compression_ratio": 1.954732510288066, "no_speech_prob": 0.002428121631965041}, {"id": 138, "seek": 72592, "start": 734.9599999999999, "end": 740.9599999999999, "text": " different electrodes. And this is the average phase of the response averaged across. It was 10 trials", "tokens": [50816, 819, 47824, 13, 400, 341, 307, 264, 4274, 5574, 295, 264, 4134, 18247, 2980, 2108, 13, 467, 390, 1266, 12450, 51116], "temperature": 0.0, "avg_logprob": -0.13227095297717173, "compression_ratio": 1.954732510288066, "no_speech_prob": 0.002428121631965041}, {"id": 139, "seek": 72592, "start": 742.64, "end": 749.12, "text": " for each participant for each stimulus. So this is the adjective noun, stimulus. And in the dot", "tokens": [51200, 337, 1184, 24950, 337, 1184, 21366, 13, 407, 341, 307, 264, 44129, 23307, 11, 21366, 13, 400, 294, 264, 5893, 51524], "temperature": 0.0, "avg_logprob": -0.13227095297717173, "compression_ratio": 1.954732510288066, "no_speech_prob": 0.002428121631965041}, {"id": 140, "seek": 72592, "start": 749.12, "end": 754.48, "text": " is for the 32 electrodes. And just for convenience, for ease of comparison, three of the electrodes", "tokens": [51524, 307, 337, 264, 8858, 47824, 13, 400, 445, 337, 19283, 11, 337, 12708, 295, 9660, 11, 1045, 295, 264, 47824, 51792], "temperature": 0.0, "avg_logprob": -0.13227095297717173, "compression_ratio": 1.954732510288066, "no_speech_prob": 0.002428121631965041}, {"id": 141, "seek": 75448, "start": 754.48, "end": 758.8000000000001, "text": " have been picked out and are cut at the same across the participants. And basically all you can", "tokens": [50364, 362, 668, 6183, 484, 293, 366, 1723, 412, 264, 912, 2108, 264, 10503, 13, 400, 1936, 439, 291, 393, 50580], "temperature": 0.0, "avg_logprob": -0.08592328531988735, "compression_ratio": 1.667857142857143, "no_speech_prob": 0.0008600231958553195}, {"id": 142, "seek": 75448, "start": 758.8000000000001, "end": 764.16, "text": " see is that the actual overall phase is quite different. And so what's interesting is, you", "tokens": [50580, 536, 307, 300, 264, 3539, 4787, 5574, 307, 1596, 819, 13, 400, 370, 437, 311, 1880, 307, 11, 291, 50848], "temperature": 0.0, "avg_logprob": -0.08592328531988735, "compression_ratio": 1.667857142857143, "no_speech_prob": 0.0008600231958553195}, {"id": 143, "seek": 75448, "start": 764.16, "end": 769.76, "text": " know, each of these dots hides the fact that there's 20 trials. Each trial has its own phase.", "tokens": [50848, 458, 11, 1184, 295, 613, 15026, 35953, 264, 1186, 300, 456, 311, 945, 12450, 13, 6947, 7308, 575, 1080, 1065, 5574, 13, 51128], "temperature": 0.0, "avg_logprob": -0.08592328531988735, "compression_ratio": 1.667857142857143, "no_speech_prob": 0.0008600231958553195}, {"id": 144, "seek": 75448, "start": 769.76, "end": 776.24, "text": " And the signal is in the degree of alignment of that phase. And so to analyze these data,", "tokens": [51128, 400, 264, 6358, 307, 294, 264, 4314, 295, 18515, 295, 300, 5574, 13, 400, 370, 281, 12477, 613, 1412, 11, 51452], "temperature": 0.0, "avg_logprob": -0.08592328531988735, "compression_ratio": 1.667857142857143, "no_speech_prob": 0.0008600231958553195}, {"id": 145, "seek": 75448, "start": 776.24, "end": 780.24, "text": " well, the easiest way to do it, it turns out, is to make some complicated Bayesian models. So we", "tokens": [51452, 731, 11, 264, 12889, 636, 281, 360, 309, 11, 309, 4523, 484, 11, 307, 281, 652, 512, 6179, 7840, 42434, 5245, 13, 407, 321, 51652], "temperature": 0.0, "avg_logprob": -0.08592328531988735, "compression_ratio": 1.667857142857143, "no_speech_prob": 0.0008600231958553195}, {"id": 146, "seek": 78024, "start": 780.24, "end": 784.4, "text": " imagine the phase is being drawn from some distribution. In this case, it's circular", "tokens": [50364, 3811, 264, 5574, 307, 885, 10117, 490, 512, 7316, 13, 682, 341, 1389, 11, 309, 311, 16476, 50572], "temperature": 0.0, "avg_logprob": -0.19211450087285675, "compression_ratio": 1.6996197718631179, "no_speech_prob": 0.0018491344526410103}, {"id": 147, "seek": 78024, "start": 784.4, "end": 789.28, "text": " Cauchy distribution, which turns out to be the nicest. The circular Cauchy distribution is quite", "tokens": [50572, 7544, 625, 88, 7316, 11, 597, 4523, 484, 281, 312, 264, 45516, 13, 440, 16476, 7544, 625, 88, 7316, 307, 1596, 50816], "temperature": 0.0, "avg_logprob": -0.19211450087285675, "compression_ratio": 1.6996197718631179, "no_speech_prob": 0.0018491344526410103}, {"id": 148, "seek": 78024, "start": 789.28, "end": 795.12, "text": " cool. It's just a wrapped Cauchy distribution. And when you do the wrapping, you can set some", "tokens": [50816, 1627, 13, 467, 311, 445, 257, 14226, 7544, 625, 88, 7316, 13, 400, 562, 291, 360, 264, 21993, 11, 291, 393, 992, 512, 51108], "temperature": 0.0, "avg_logprob": -0.19211450087285675, "compression_ratio": 1.6996197718631179, "no_speech_prob": 0.0018491344526410103}, {"id": 149, "seek": 78024, "start": 795.12, "end": 802.32, "text": " of the theories and come up with an analytic formula for it. You say the data is kind of noisy,", "tokens": [51108, 295, 264, 13667, 293, 808, 493, 365, 364, 40358, 8513, 337, 309, 13, 509, 584, 264, 1412, 307, 733, 295, 24518, 11, 51468], "temperature": 0.0, "avg_logprob": -0.19211450087285675, "compression_ratio": 1.6996197718631179, "no_speech_prob": 0.0018491344526410103}, {"id": 150, "seek": 78024, "start": 802.32, "end": 807.28, "text": " right? Immensely. Yeah. So the Fourier transform is also like pretty noisy.", "tokens": [51468, 558, 30, 17322, 694, 736, 13, 865, 13, 407, 264, 36810, 4088, 307, 611, 411, 1238, 24518, 13, 51716], "temperature": 0.0, "avg_logprob": -0.19211450087285675, "compression_ratio": 1.6996197718631179, "no_speech_prob": 0.0018491344526410103}, {"id": 151, "seek": 80728, "start": 807.28, "end": 811.4399999999999, "text": " Yes. Yes. So you have to go through several steps. So, you know, obviously,", "tokens": [50364, 1079, 13, 1079, 13, 407, 291, 362, 281, 352, 807, 2940, 4439, 13, 407, 11, 291, 458, 11, 2745, 11, 50572], "temperature": 0.0, "avg_logprob": -0.12971054183112252, "compression_ratio": 1.6686217008797655, "no_speech_prob": 0.0005436940118670464}, {"id": 152, "seek": 80728, "start": 812.24, "end": 816.48, "text": " for start using the frequency tag experiment makes it less noisy because, you know, the other stuff", "tokens": [50612, 337, 722, 1228, 264, 7893, 6162, 5120, 1669, 309, 1570, 24518, 570, 11, 291, 458, 11, 264, 661, 1507, 50824], "temperature": 0.0, "avg_logprob": -0.12971054183112252, "compression_ratio": 1.6686217008797655, "no_speech_prob": 0.0005436940118670464}, {"id": 153, "seek": 80728, "start": 816.48, "end": 820.72, "text": " is happening at different frequencies. But even then you have to do this sort of phase lock analysis,", "tokens": [50824, 307, 2737, 412, 819, 20250, 13, 583, 754, 550, 291, 362, 281, 360, 341, 1333, 295, 5574, 4017, 5215, 11, 51036], "temperature": 0.0, "avg_logprob": -0.12971054183112252, "compression_ratio": 1.6686217008797655, "no_speech_prob": 0.0005436940118670464}, {"id": 154, "seek": 80728, "start": 820.72, "end": 825.76, "text": " which I'm briefly reviewing. I don't want to spend too long on it. If you're interested in the", "tokens": [51036, 597, 286, 478, 10515, 19576, 13, 286, 500, 380, 528, 281, 3496, 886, 938, 322, 309, 13, 759, 291, 434, 3102, 294, 264, 51288], "temperature": 0.0, "avg_logprob": -0.12971054183112252, "compression_ratio": 1.6686217008797655, "no_speech_prob": 0.0005436940118670464}, {"id": 155, "seek": 80728, "start": 825.76, "end": 830.48, "text": " analysis of EEG data, we've been thinking about it a lot. And I can talk to you about it afterwards.", "tokens": [51288, 5215, 295, 33685, 38, 1412, 11, 321, 600, 668, 1953, 466, 309, 257, 688, 13, 400, 286, 393, 751, 281, 291, 466, 309, 10543, 13, 51524], "temperature": 0.0, "avg_logprob": -0.12971054183112252, "compression_ratio": 1.6686217008797655, "no_speech_prob": 0.0005436940118670464}, {"id": 156, "seek": 80728, "start": 830.48, "end": 836.48, "text": " But basically, in summary, we make a big Bayesian model of the results. What we're looking for,", "tokens": [51524, 583, 1936, 11, 294, 12691, 11, 321, 652, 257, 955, 7840, 42434, 2316, 295, 264, 3542, 13, 708, 321, 434, 1237, 337, 11, 51824], "temperature": 0.0, "avg_logprob": -0.12971054183112252, "compression_ratio": 1.6686217008797655, "no_speech_prob": 0.0005436940118670464}, {"id": 157, "seek": 83648, "start": 836.5600000000001, "end": 843.04, "text": " we imagine the phases have been drawn from a wrapped distribution. We have some prior for the", "tokens": [50368, 321, 3811, 264, 18764, 362, 668, 10117, 490, 257, 14226, 7316, 13, 492, 362, 512, 4059, 337, 264, 50692], "temperature": 0.0, "avg_logprob": -0.14863310076973654, "compression_ratio": 1.8423645320197044, "no_speech_prob": 0.002457328839227557}, {"id": 158, "seek": 83648, "start": 843.04, "end": 847.44, "text": " variance of that distribution, some prior for the mean, that doesn't matter. And so the signal", "tokens": [50692, 21977, 295, 300, 7316, 11, 512, 4059, 337, 264, 914, 11, 300, 1177, 380, 1871, 13, 400, 370, 264, 6358, 50912], "temperature": 0.0, "avg_logprob": -0.14863310076973654, "compression_ratio": 1.8423645320197044, "no_speech_prob": 0.002457328839227557}, {"id": 159, "seek": 83648, "start": 847.44, "end": 854.32, "text": " will be in this, the posterior distribution of this phase of this variance for the phase.", "tokens": [50912, 486, 312, 294, 341, 11, 264, 33529, 7316, 295, 341, 5574, 295, 341, 21977, 337, 264, 5574, 13, 51256], "temperature": 0.0, "avg_logprob": -0.14863310076973654, "compression_ratio": 1.8423645320197044, "no_speech_prob": 0.002457328839227557}, {"id": 160, "seek": 83648, "start": 854.32, "end": 862.24, "text": " And so this basically is the result. Here, we're looking at the, it basically is the inverse of", "tokens": [51256, 400, 370, 341, 1936, 307, 264, 1874, 13, 1692, 11, 321, 434, 1237, 412, 264, 11, 309, 1936, 307, 264, 17340, 295, 51652], "temperature": 0.0, "avg_logprob": -0.14863310076973654, "compression_ratio": 1.8423645320197044, "no_speech_prob": 0.002457328839227557}, {"id": 161, "seek": 86224, "start": 862.24, "end": 868.32, "text": " this variance, what they call the mean circular resultant against frequency. And so, sorry,", "tokens": [50364, 341, 21977, 11, 437, 436, 818, 264, 914, 16476, 1874, 394, 1970, 7893, 13, 400, 370, 11, 2597, 11, 50668], "temperature": 0.0, "avg_logprob": -0.08459404662803367, "compression_ratio": 1.8360655737704918, "no_speech_prob": 0.0026045574340969324}, {"id": 162, "seek": 86224, "start": 868.32, "end": 872.88, "text": " I'm standing in front of the camera. And so what we're seeing here is the", "tokens": [50668, 286, 478, 4877, 294, 1868, 295, 264, 2799, 13, 400, 370, 437, 321, 434, 2577, 510, 307, 264, 50896], "temperature": 0.0, "avg_logprob": -0.08459404662803367, "compression_ratio": 1.8360655737704918, "no_speech_prob": 0.0026045574340969324}, {"id": 163, "seek": 86224, "start": 874.64, "end": 879.28, "text": " different frequencies. This is the frequency of the syllables. This is the frequency of the phase.", "tokens": [50984, 819, 20250, 13, 639, 307, 264, 7893, 295, 264, 45364, 13, 639, 307, 264, 7893, 295, 264, 5574, 13, 51216], "temperature": 0.0, "avg_logprob": -0.08459404662803367, "compression_ratio": 1.8360655737704918, "no_speech_prob": 0.0026045574340969324}, {"id": 164, "seek": 86224, "start": 879.84, "end": 885.04, "text": " For the adjective verb condition, you see that there's a big response at the syllable rate,", "tokens": [51244, 1171, 264, 44129, 9595, 4188, 11, 291, 536, 300, 456, 311, 257, 955, 4134, 412, 264, 40151, 3314, 11, 51504], "temperature": 0.0, "avg_logprob": -0.08459404662803367, "compression_ratio": 1.8360655737704918, "no_speech_prob": 0.0026045574340969324}, {"id": 165, "seek": 86224, "start": 885.04, "end": 888.72, "text": " as there is for all of these conditions. These are other conditions. This is mixed lexical,", "tokens": [51504, 382, 456, 307, 337, 439, 295, 613, 4487, 13, 1981, 366, 661, 4487, 13, 639, 307, 7467, 476, 87, 804, 11, 51688], "temperature": 0.0, "avg_logprob": -0.08459404662803367, "compression_ratio": 1.8360655737704918, "no_speech_prob": 0.0026045574340969324}, {"id": 166, "seek": 88872, "start": 888.72, "end": 894.24, "text": " mixed phrase, random. We did six different conditions, but we'll just concentrate on these two.", "tokens": [50364, 7467, 9535, 11, 4974, 13, 492, 630, 2309, 819, 4487, 11, 457, 321, 603, 445, 18089, 322, 613, 732, 13, 50640], "temperature": 0.0, "avg_logprob": -0.11061965567725045, "compression_ratio": 1.7228464419475655, "no_speech_prob": 0.0015046343905851245}, {"id": 167, "seek": 88872, "start": 895.28, "end": 899.52, "text": " For both the adjective noun and the adjective verb, there's a response at the syllable rate,", "tokens": [50692, 1171, 1293, 264, 44129, 23307, 293, 264, 44129, 9595, 11, 456, 311, 257, 4134, 412, 264, 40151, 3314, 11, 50904], "temperature": 0.0, "avg_logprob": -0.11061965567725045, "compression_ratio": 1.7228464419475655, "no_speech_prob": 0.0015046343905851245}, {"id": 168, "seek": 88872, "start": 900.32, "end": 905.9200000000001, "text": " showing up as a reduction in the variance of the phases. But for only the adjective noun,", "tokens": [50944, 4099, 493, 382, 257, 11004, 294, 264, 21977, 295, 264, 18764, 13, 583, 337, 787, 264, 44129, 23307, 11, 51224], "temperature": 0.0, "avg_logprob": -0.11061965567725045, "compression_ratio": 1.7228464419475655, "no_speech_prob": 0.0015046343905851245}, {"id": 169, "seek": 88872, "start": 905.9200000000001, "end": 912.4, "text": " are you seeing a response at the noun, at the, at the phrase rate? This, on the right,", "tokens": [51224, 366, 291, 2577, 257, 4134, 412, 264, 23307, 11, 412, 264, 11, 412, 264, 9535, 3314, 30, 639, 11, 322, 264, 558, 11, 51548], "temperature": 0.0, "avg_logprob": -0.11061965567725045, "compression_ratio": 1.7228464419475655, "no_speech_prob": 0.0015046343905851245}, {"id": 170, "seek": 88872, "start": 912.4, "end": 918.1600000000001, "text": " this is some basic equivalent of the usual bar and star type graph. All it's really indicating", "tokens": [51548, 341, 307, 512, 3875, 10344, 295, 264, 7713, 2159, 293, 3543, 2010, 4295, 13, 1057, 309, 311, 534, 25604, 51836], "temperature": 0.0, "avg_logprob": -0.11061965567725045, "compression_ratio": 1.7228464419475655, "no_speech_prob": 0.0015046343905851245}, {"id": 171, "seek": 91816, "start": 918.16, "end": 923.92, "text": " is that the adjective noun condition shows substantially, or you might say significantly more", "tokens": [50364, 307, 300, 264, 44129, 23307, 4188, 3110, 30797, 11, 420, 291, 1062, 584, 10591, 544, 50652], "temperature": 0.0, "avg_logprob": -0.14222721994659046, "compression_ratio": 1.737327188940092, "no_speech_prob": 0.0009414397645741701}, {"id": 172, "seek": 91816, "start": 926.8, "end": 932.8, "text": " response, which is, again, a significantly more mean resultant, just like the inverse of variance,", "tokens": [50796, 4134, 11, 597, 307, 11, 797, 11, 257, 10591, 544, 914, 1874, 394, 11, 445, 411, 264, 17340, 295, 21977, 11, 51096], "temperature": 0.0, "avg_logprob": -0.14222721994659046, "compression_ratio": 1.737327188940092, "no_speech_prob": 0.0009414397645741701}, {"id": 173, "seek": 91816, "start": 933.76, "end": 940.7199999999999, "text": " than these other conditions, which is basically showing that the brain has a response to the", "tokens": [51144, 813, 613, 661, 4487, 11, 597, 307, 1936, 4099, 300, 264, 3567, 575, 257, 4134, 281, 264, 51492], "temperature": 0.0, "avg_logprob": -0.14222721994659046, "compression_ratio": 1.737327188940092, "no_speech_prob": 0.0009414397645741701}, {"id": 174, "seek": 91816, "start": 940.7199999999999, "end": 946.3199999999999, "text": " grammar. That we can't just think of the brain as performing a statistical inference on the", "tokens": [51492, 22317, 13, 663, 321, 393, 380, 445, 519, 295, 264, 3567, 382, 10205, 257, 22820, 38253, 322, 264, 51772], "temperature": 0.0, "avg_logprob": -0.14222721994659046, "compression_ratio": 1.737327188940092, "no_speech_prob": 0.0009414397645741701}, {"id": 175, "seek": 94632, "start": 946.32, "end": 953.12, "text": " sentences, trying to extract meaning and so on. It does something every time it hears a noun phrase,", "tokens": [50364, 16579, 11, 1382, 281, 8947, 3620, 293, 370, 322, 13, 467, 775, 746, 633, 565, 309, 25688, 257, 23307, 9535, 11, 50704], "temperature": 0.0, "avg_logprob": -0.09978992422831427, "compression_ratio": 1.685589519650655, "no_speech_prob": 0.0009062562021426857}, {"id": 176, "seek": 94632, "start": 953.12, "end": 959.12, "text": " and it doesn't do anything every time it hears an adjective verb word pairing, because that", "tokens": [50704, 293, 309, 1177, 380, 360, 1340, 633, 565, 309, 25688, 364, 44129, 9595, 1349, 32735, 11, 570, 300, 51004], "temperature": 0.0, "avg_logprob": -0.09978992422831427, "compression_ratio": 1.685589519650655, "no_speech_prob": 0.0009062562021426857}, {"id": 177, "seek": 94632, "start": 959.12, "end": 966.1600000000001, "text": " isn't a grammatical object, or at least that's the interpretation. So, you know, having done this,", "tokens": [51004, 1943, 380, 257, 17570, 267, 804, 2657, 11, 420, 412, 1935, 300, 311, 264, 14174, 13, 407, 11, 291, 458, 11, 1419, 1096, 341, 11, 51356], "temperature": 0.0, "avg_logprob": -0.09978992422831427, "compression_ratio": 1.685589519650655, "no_speech_prob": 0.0009062562021426857}, {"id": 178, "seek": 94632, "start": 966.1600000000001, "end": 971.44, "text": " I was, I was kind of amazed. It shows that there is sort of stuff happening in the brain that,", "tokens": [51356, 286, 390, 11, 286, 390, 733, 295, 20507, 13, 467, 3110, 300, 456, 307, 1333, 295, 1507, 2737, 294, 264, 3567, 300, 11, 51620], "temperature": 0.0, "avg_logprob": -0.09978992422831427, "compression_ratio": 1.685589519650655, "no_speech_prob": 0.0009062562021426857}, {"id": 179, "seek": 97144, "start": 971.44, "end": 978.48, "text": " that is more formal or more, more akin to a grammatical manipulation than, than you might", "tokens": [50364, 300, 307, 544, 9860, 420, 544, 11, 544, 47540, 281, 257, 17570, 267, 804, 26475, 813, 11, 813, 291, 1062, 50716], "temperature": 0.0, "avg_logprob": -0.10789037362123147, "compression_ratio": 1.9832635983263598, "no_speech_prob": 0.0024084774777293205}, {"id": 180, "seek": 97144, "start": 978.48, "end": 985.2800000000001, "text": " have expected, and shows sort of the presence in, in, in this, you know, in this discussion as to", "tokens": [50716, 362, 5176, 11, 293, 3110, 1333, 295, 264, 6814, 294, 11, 294, 11, 294, 341, 11, 291, 458, 11, 294, 341, 5017, 382, 281, 51056], "temperature": 0.0, "avg_logprob": -0.10789037362123147, "compression_ratio": 1.9832635983263598, "no_speech_prob": 0.0024084774777293205}, {"id": 181, "seek": 97144, "start": 985.2800000000001, "end": 988.96, "text": " what part of language is about the world, what part of language is about communication, what", "tokens": [51056, 437, 644, 295, 2856, 307, 466, 264, 1002, 11, 437, 644, 295, 2856, 307, 466, 6101, 11, 437, 51240], "temperature": 0.0, "avg_logprob": -0.10789037362123147, "compression_ratio": 1.9832635983263598, "no_speech_prob": 0.0024084774777293205}, {"id": 182, "seek": 97144, "start": 988.96, "end": 993.5200000000001, "text": " part of language is about the machinery the brain has to deal, to deal with language. There's more", "tokens": [51240, 644, 295, 2856, 307, 466, 264, 27302, 264, 3567, 575, 281, 2028, 11, 281, 2028, 365, 2856, 13, 821, 311, 544, 51468], "temperature": 0.0, "avg_logprob": -0.10789037362123147, "compression_ratio": 1.9832635983263598, "no_speech_prob": 0.0024084774777293205}, {"id": 183, "seek": 97144, "start": 993.5200000000001, "end": 997.9200000000001, "text": " in that sort of brain part than you might think, and it's certainly, will it be of interest to", "tokens": [51468, 294, 300, 1333, 295, 3567, 644, 813, 291, 1062, 519, 11, 293, 309, 311, 3297, 11, 486, 309, 312, 295, 1179, 281, 51688], "temperature": 0.0, "avg_logprob": -0.10789037362123147, "compression_ratio": 1.9832635983263598, "no_speech_prob": 0.0024084774777293205}, {"id": 184, "seek": 99792, "start": 997.92, "end": 1004.64, "text": " start to consider grammar, and the brain's view of grammar. Now, of course, these,", "tokens": [50364, 722, 281, 1949, 22317, 11, 293, 264, 3567, 311, 1910, 295, 22317, 13, 823, 11, 295, 1164, 11, 613, 11, 50700], "temperature": 0.0, "avg_logprob": -0.19003863864474826, "compression_ratio": 1.6116071428571428, "no_speech_prob": 0.004588369280099869}, {"id": 185, "seek": 99792, "start": 1006.4799999999999, "end": 1011.52, "text": " these stimuli that we were dealing with are, are quite new that we were using, as I said,", "tokens": [50792, 613, 47752, 300, 321, 645, 6260, 365, 366, 11, 366, 1596, 777, 300, 321, 645, 1228, 11, 382, 286, 848, 11, 51044], "temperature": 0.0, "avg_logprob": -0.19003863864474826, "compression_ratio": 1.6116071428571428, "no_speech_prob": 0.004588369280099869}, {"id": 186, "seek": 99792, "start": 1011.52, "end": 1018.16, "text": " following this idea of people in a ding, this frequency tagged paradigm, where you are playing", "tokens": [51044, 3480, 341, 1558, 295, 561, 294, 257, 21211, 11, 341, 7893, 40239, 24709, 11, 689, 291, 366, 2433, 51376], "temperature": 0.0, "avg_logprob": -0.19003863864474826, "compression_ratio": 1.6116071428571428, "no_speech_prob": 0.004588369280099869}, {"id": 187, "seek": 99792, "start": 1018.16, "end": 1024.96, "text": " the stimulus at a set frequency in order to be able to extract the EG signal of interest. But", "tokens": [51376, 264, 21366, 412, 257, 992, 7893, 294, 1668, 281, 312, 1075, 281, 8947, 264, 462, 38, 6358, 295, 1179, 13, 583, 51716], "temperature": 0.0, "avg_logprob": -0.19003863864474826, "compression_ratio": 1.6116071428571428, "no_speech_prob": 0.004588369280099869}, {"id": 188, "seek": 102496, "start": 1024.96, "end": 1029.28, "text": " it's very hard to, to, to come up with, you know, there's lots of things that you can't do with that.", "tokens": [50364, 309, 311, 588, 1152, 281, 11, 281, 11, 281, 808, 493, 365, 11, 291, 458, 11, 456, 311, 3195, 295, 721, 300, 291, 393, 380, 360, 365, 300, 13, 50580], "temperature": 0.0, "avg_logprob": -0.12884351683825981, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.00044192862696945667}, {"id": 189, "seek": 102496, "start": 1029.28, "end": 1036.0, "text": " We were able to examine the presence of noun phrases, etc. But what you'd really like is to", "tokens": [50580, 492, 645, 1075, 281, 17496, 264, 6814, 295, 23307, 20312, 11, 5183, 13, 583, 437, 291, 1116, 534, 411, 307, 281, 50916], "temperature": 0.0, "avg_logprob": -0.12884351683825981, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.00044192862696945667}, {"id": 190, "seek": 102496, "start": 1036.0, "end": 1041.52, "text": " use free text like this, with like complicated, you know, sentences, different types of grammar,", "tokens": [50916, 764, 1737, 2487, 411, 341, 11, 365, 411, 6179, 11, 291, 458, 11, 16579, 11, 819, 3467, 295, 22317, 11, 51192], "temperature": 0.0, "avg_logprob": -0.12884351683825981, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.00044192862696945667}, {"id": 191, "seek": 102496, "start": 1041.52, "end": 1049.28, "text": " and so on. This would be better as well, because one problem with the, the frequency tagged", "tokens": [51192, 293, 370, 322, 13, 639, 576, 312, 1101, 382, 731, 11, 570, 472, 1154, 365, 264, 11, 264, 7893, 40239, 51580], "temperature": 0.0, "avg_logprob": -0.12884351683825981, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.00044192862696945667}, {"id": 192, "seek": 102496, "start": 1049.28, "end": 1054.24, "text": " experiments is that they are quite, quite boring and annoying to, to, to be a participant in those", "tokens": [51580, 12050, 307, 300, 436, 366, 1596, 11, 1596, 9989, 293, 11304, 281, 11, 281, 11, 281, 312, 257, 24950, 294, 729, 51828], "temperature": 0.0, "avg_logprob": -0.12884351683825981, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.00044192862696945667}, {"id": 193, "seek": 105424, "start": 1054.24, "end": 1059.6, "text": " experiments. And it doesn't encourage this kind of thinking away from what you're supposed to be", "tokens": [50364, 12050, 13, 400, 309, 1177, 380, 5373, 341, 733, 295, 1953, 1314, 490, 437, 291, 434, 3442, 281, 312, 50632], "temperature": 0.0, "avg_logprob": -0.08934170592064951, "compression_ratio": 1.6623931623931625, "no_speech_prob": 0.0010092317825183272}, {"id": 194, "seek": 105424, "start": 1059.6, "end": 1067.76, "text": " thinking. The problem here is the, is the difficulty in, in, in analyzing the data and, and trying to", "tokens": [50632, 1953, 13, 440, 1154, 510, 307, 264, 11, 307, 264, 10360, 294, 11, 294, 11, 294, 23663, 264, 1412, 293, 11, 293, 1382, 281, 51040], "temperature": 0.0, "avg_logprob": -0.08934170592064951, "compression_ratio": 1.6623931623931625, "no_speech_prob": 0.0010092317825183272}, {"id": 195, "seek": 105424, "start": 1067.76, "end": 1074.08, "text": " show a relationship between the EG response and, and, and what's going on. So, you know, what,", "tokens": [51040, 855, 257, 2480, 1296, 264, 462, 38, 4134, 293, 11, 293, 11, 293, 437, 311, 516, 322, 13, 407, 11, 291, 458, 11, 437, 11, 51356], "temperature": 0.0, "avg_logprob": -0.08934170592064951, "compression_ratio": 1.6623931623931625, "no_speech_prob": 0.0010092317825183272}, {"id": 196, "seek": 105424, "start": 1074.08, "end": 1079.6, "text": " what we'd like to do, I think, is to understand what grammar looks like to the brain. You know,", "tokens": [51356, 437, 321, 1116, 411, 281, 360, 11, 286, 519, 11, 307, 281, 1223, 437, 22317, 1542, 411, 281, 264, 3567, 13, 509, 458, 11, 51632], "temperature": 0.0, "avg_logprob": -0.08934170592064951, "compression_ratio": 1.6623931623931625, "no_speech_prob": 0.0010092317825183272}, {"id": 197, "seek": 107960, "start": 1079.6, "end": 1084.56, "text": " we, we know what Grammaticians think grammar looks like, but we don't know, you know, but", "tokens": [50364, 321, 11, 321, 458, 437, 22130, 15677, 8455, 519, 22317, 1542, 411, 11, 457, 321, 500, 380, 458, 11, 291, 458, 11, 457, 50612], "temperature": 0.0, "avg_logprob": -0.12589289987687585, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.0092091616243124}, {"id": 198, "seek": 107960, "start": 1084.56, "end": 1088.6399999999999, "text": " I often think about the phrenology skills that people were interested in in the 19th century.", "tokens": [50612, 286, 2049, 519, 466, 264, 903, 1095, 1793, 3942, 300, 561, 645, 3102, 294, 294, 264, 1294, 392, 4901, 13, 50816], "temperature": 0.0, "avg_logprob": -0.12589289987687585, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.0092091616243124}, {"id": 199, "seek": 107960, "start": 1088.6399999999999, "end": 1093.12, "text": " The pseudoscience phenology divided the brain up into regions of the brain that did different", "tokens": [50816, 440, 25505, 35063, 6699, 903, 268, 1793, 6666, 264, 3567, 493, 666, 10682, 295, 264, 3567, 300, 630, 819, 51040], "temperature": 0.0, "avg_logprob": -0.12589289987687585, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.0092091616243124}, {"id": 200, "seek": 107960, "start": 1093.12, "end": 1096.56, "text": " things, which is actually true. There are regions of the brain that do different things,", "tokens": [51040, 721, 11, 597, 307, 767, 2074, 13, 821, 366, 10682, 295, 264, 3567, 300, 360, 819, 721, 11, 51212], "temperature": 0.0, "avg_logprob": -0.12589289987687585, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.0092091616243124}, {"id": 201, "seek": 107960, "start": 1096.56, "end": 1100.1599999999999, "text": " but they got it completely wrong. You know, they, they thought that the cerebellum was the organ", "tokens": [51212, 457, 436, 658, 309, 2584, 2085, 13, 509, 458, 11, 436, 11, 436, 1194, 300, 264, 11643, 7100, 449, 390, 264, 1798, 51392], "temperature": 0.0, "avg_logprob": -0.12589289987687585, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.0092091616243124}, {"id": 202, "seek": 107960, "start": 1100.1599999999999, "end": 1105.6, "text": " of amethyst, which is bizarre. In fact, it had some role to do with predicting the consequence of", "tokens": [51392, 295, 669, 3293, 38593, 11, 597, 307, 18265, 13, 682, 1186, 11, 309, 632, 512, 3090, 281, 360, 365, 32884, 264, 18326, 295, 51664], "temperature": 0.0, "avg_logprob": -0.12589289987687585, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.0092091616243124}, {"id": 203, "seek": 110560, "start": 1105.6, "end": 1110.56, "text": " motor commands or something. So, you know, the, the, the ideas we have about grammar might be", "tokens": [50364, 5932, 16901, 420, 746, 13, 407, 11, 291, 458, 11, 264, 11, 264, 11, 264, 3487, 321, 362, 466, 22317, 1062, 312, 50612], "temperature": 0.0, "avg_logprob": -0.15771622966519364, "compression_ratio": 1.7348242811501597, "no_speech_prob": 0.012766815721988678}, {"id": 204, "seek": 110560, "start": 1110.56, "end": 1115.4399999999998, "text": " quite different from what grammar actually looks like. And to, to probe that, about one thing", "tokens": [50612, 1596, 819, 490, 437, 22317, 767, 1542, 411, 13, 400, 281, 11, 281, 22715, 300, 11, 466, 472, 551, 50856], "temperature": 0.0, "avg_logprob": -0.15771622966519364, "compression_ratio": 1.7348242811501597, "no_speech_prob": 0.012766815721988678}, {"id": 205, "seek": 110560, "start": 1115.4399999999998, "end": 1119.28, "text": " certainly to do would be to, to, to do EG experiments with free text.", "tokens": [50856, 3297, 281, 360, 576, 312, 281, 11, 281, 11, 281, 360, 462, 38, 12050, 365, 1737, 2487, 13, 51048], "temperature": 0.0, "avg_logprob": -0.15771622966519364, "compression_ratio": 1.7348242811501597, "no_speech_prob": 0.012766815721988678}, {"id": 206, "seek": 110560, "start": 1119.28, "end": 1125.28, "text": " So, Connor, like, so just to kind of get an idea of the experiment itself. So you'd have like,", "tokens": [51048, 407, 11, 33133, 11, 411, 11, 370, 445, 281, 733, 295, 483, 364, 1558, 295, 264, 5120, 2564, 13, 407, 291, 1116, 362, 411, 11, 51348], "temperature": 0.0, "avg_logprob": -0.15771622966519364, "compression_ratio": 1.7348242811501597, "no_speech_prob": 0.012766815721988678}, {"id": 207, "seek": 110560, "start": 1125.28, "end": 1128.32, "text": " I know you said this is going to be like really complicated, but in principle, you'd have your", "tokens": [51348, 286, 458, 291, 848, 341, 307, 516, 281, 312, 411, 534, 6179, 11, 457, 294, 8665, 11, 291, 1116, 362, 428, 51500], "temperature": 0.0, "avg_logprob": -0.15771622966519364, "compression_ratio": 1.7348242811501597, "no_speech_prob": 0.012766815721988678}, {"id": 208, "seek": 110560, "start": 1128.32, "end": 1132.8, "text": " participants in a room reading this out loud and you'd just be looking at the brain signals. Is", "tokens": [51500, 10503, 294, 257, 1808, 3760, 341, 484, 6588, 293, 291, 1116, 445, 312, 1237, 412, 264, 3567, 12354, 13, 1119, 51724], "temperature": 0.0, "avg_logprob": -0.15771622966519364, "compression_ratio": 1.7348242811501597, "no_speech_prob": 0.012766815721988678}, {"id": 209, "seek": 113280, "start": 1132.8, "end": 1136.72, "text": " that the, you would be reading it to them and you'd be reading the two brain signals. Yeah.", "tokens": [50364, 300, 264, 11, 291, 576, 312, 3760, 309, 281, 552, 293, 291, 1116, 312, 3760, 264, 732, 3567, 12354, 13, 865, 13, 50560], "temperature": 0.0, "avg_logprob": -0.2052677342149078, "compression_ratio": 1.6962962962962962, "no_speech_prob": 0.003821626538410783}, {"id": 210, "seek": 113280, "start": 1136.72, "end": 1141.04, "text": " So, so, I mean, here, here's the results. So, I mean, there are people, you know,", "tokens": [50560, 407, 11, 370, 11, 286, 914, 11, 510, 11, 510, 311, 264, 3542, 13, 407, 11, 286, 914, 11, 456, 366, 561, 11, 291, 458, 11, 50776], "temperature": 0.0, "avg_logprob": -0.2052677342149078, "compression_ratio": 1.6962962962962962, "no_speech_prob": 0.003821626538410783}, {"id": 211, "seek": 113280, "start": 1141.04, "end": 1145.68, "text": " it's not just us that's trying to do this. There are lots of labs doing experiments along these", "tokens": [50776, 309, 311, 406, 445, 505, 300, 311, 1382, 281, 360, 341, 13, 821, 366, 3195, 295, 20339, 884, 12050, 2051, 613, 51008], "temperature": 0.0, "avg_logprob": -0.2052677342149078, "compression_ratio": 1.6962962962962962, "no_speech_prob": 0.003821626538410783}, {"id": 212, "seek": 113280, "start": 1145.68, "end": 1153.04, "text": " lines. Stephen Frank in Namegun, for example. You have measures of kind of comprehensive,", "tokens": [51008, 3876, 13, 13391, 6823, 294, 13866, 7414, 11, 337, 1365, 13, 509, 362, 8000, 295, 733, 295, 13914, 11, 51376], "temperature": 0.0, "avg_logprob": -0.2052677342149078, "compression_ratio": 1.6962962962962962, "no_speech_prob": 0.003821626538410783}, {"id": 213, "seek": 113280, "start": 1153.04, "end": 1158.1599999999999, "text": " like some behavioral output, you know, did like, what is, what's the point of the brain responding", "tokens": [51376, 411, 512, 19124, 5598, 11, 291, 458, 11, 630, 411, 11, 437, 307, 11, 437, 311, 264, 935, 295, 264, 3567, 16670, 51632], "temperature": 0.0, "avg_logprob": -0.2052677342149078, "compression_ratio": 1.6962962962962962, "no_speech_prob": 0.003821626538410783}, {"id": 214, "seek": 115816, "start": 1158.24, "end": 1163.76, "text": " to these things? Are you, are you measuring that as well? Like in the previous experiment,", "tokens": [50368, 281, 613, 721, 30, 2014, 291, 11, 366, 291, 13389, 300, 382, 731, 30, 1743, 294, 264, 3894, 5120, 11, 50644], "temperature": 0.0, "avg_logprob": -0.11981261187586291, "compression_ratio": 1.7644787644787645, "no_speech_prob": 0.032568156719207764}, {"id": 215, "seek": 115816, "start": 1163.76, "end": 1167.3600000000001, "text": " you showed that there was a response better, but like, why, right? Like, what is that,", "tokens": [50644, 291, 4712, 300, 456, 390, 257, 4134, 1101, 11, 457, 411, 11, 983, 11, 558, 30, 1743, 11, 437, 307, 300, 11, 50824], "temperature": 0.0, "avg_logprob": -0.11981261187586291, "compression_ratio": 1.7644787644787645, "no_speech_prob": 0.032568156719207764}, {"id": 216, "seek": 115816, "start": 1168.0, "end": 1171.44, "text": " what is that, what is that doing for? I mean, what we do do is we make sure people are paying", "tokens": [50856, 437, 307, 300, 11, 437, 307, 300, 884, 337, 30, 286, 914, 11, 437, 321, 360, 360, 307, 321, 652, 988, 561, 366, 6229, 51028], "temperature": 0.0, "avg_logprob": -0.11981261187586291, "compression_ratio": 1.7644787644787645, "no_speech_prob": 0.032568156719207764}, {"id": 217, "seek": 115816, "start": 1171.44, "end": 1176.72, "text": " attention. Okay. So we have a detention trap. We introduce particular words that they're supposed", "tokens": [51028, 3202, 13, 1033, 13, 407, 321, 362, 257, 31291, 11487, 13, 492, 5366, 1729, 2283, 300, 436, 434, 3442, 51292], "temperature": 0.0, "avg_logprob": -0.11981261187586291, "compression_ratio": 1.7644787644787645, "no_speech_prob": 0.032568156719207764}, {"id": 218, "seek": 115816, "start": 1176.72, "end": 1182.4, "text": " to press a button to show that they're still paying attention. We, one advantage of the", "tokens": [51292, 281, 1886, 257, 2960, 281, 855, 300, 436, 434, 920, 6229, 3202, 13, 492, 11, 472, 5002, 295, 264, 51576], "temperature": 0.0, "avg_logprob": -0.11981261187586291, "compression_ratio": 1.7644787644787645, "no_speech_prob": 0.032568156719207764}, {"id": 219, "seek": 118240, "start": 1182.4, "end": 1188.16, "text": " Bayesian analysis is you can check the participants are, you know, you can, there's terms in the,", "tokens": [50364, 7840, 42434, 5215, 307, 291, 393, 1520, 264, 10503, 366, 11, 291, 458, 11, 291, 393, 11, 456, 311, 2115, 294, 264, 11, 50652], "temperature": 0.0, "avg_logprob": -0.13914459292628184, "compression_ratio": 1.9579831932773109, "no_speech_prob": 0.0073443264700472355}, {"id": 220, "seek": 118240, "start": 1188.16, "end": 1191.44, "text": " in the, you know, you have a big Bayesian model. There's a term to do with the participants,", "tokens": [50652, 294, 264, 11, 291, 458, 11, 291, 362, 257, 955, 7840, 42434, 2316, 13, 821, 311, 257, 1433, 281, 360, 365, 264, 10503, 11, 50816], "temperature": 0.0, "avg_logprob": -0.13914459292628184, "compression_ratio": 1.9579831932773109, "no_speech_prob": 0.0073443264700472355}, {"id": 221, "seek": 118240, "start": 1191.44, "end": 1195.52, "text": " you know, attentiveness. You can see that some of the participants are clearly not paying attention.", "tokens": [50816, 291, 458, 11, 30980, 8477, 13, 509, 393, 536, 300, 512, 295, 264, 10503, 366, 4448, 406, 6229, 3202, 13, 51020], "temperature": 0.0, "avg_logprob": -0.13914459292628184, "compression_ratio": 1.9579831932773109, "no_speech_prob": 0.0073443264700472355}, {"id": 222, "seek": 118240, "start": 1195.52, "end": 1198.0, "text": " You can look at them through the window and see they're not paying attention as well.", "tokens": [51020, 509, 393, 574, 412, 552, 807, 264, 4910, 293, 536, 436, 434, 406, 6229, 3202, 382, 731, 13, 51144], "temperature": 0.0, "avg_logprob": -0.13914459292628184, "compression_ratio": 1.9579831932773109, "no_speech_prob": 0.0073443264700472355}, {"id": 223, "seek": 118240, "start": 1199.0400000000002, "end": 1207.8400000000001, "text": " You know, I guess I'm asking like a deeper question about, you know, you're asking what,", "tokens": [51196, 509, 458, 11, 286, 2041, 286, 478, 3365, 411, 257, 7731, 1168, 466, 11, 291, 458, 11, 291, 434, 3365, 437, 11, 51636], "temperature": 0.0, "avg_logprob": -0.13914459292628184, "compression_ratio": 1.9579831932773109, "no_speech_prob": 0.0073443264700472355}, {"id": 224, "seek": 120784, "start": 1207.84, "end": 1212.0, "text": " what is language for, what's the brain for? Just like some of these responses,", "tokens": [50364, 437, 307, 2856, 337, 11, 437, 311, 264, 3567, 337, 30, 1449, 411, 512, 295, 613, 13019, 11, 50572], "temperature": 0.0, "avg_logprob": -0.12317816149286863, "compression_ratio": 1.7749077490774907, "no_speech_prob": 0.037316013127565384}, {"id": 225, "seek": 120784, "start": 1213.9199999999998, "end": 1218.24, "text": " I'm always skeptical, particularly, I mean, I'd know the fMRI literature much better than the", "tokens": [50668, 286, 478, 1009, 28601, 11, 4098, 11, 286, 914, 11, 286, 1116, 458, 264, 283, 44, 5577, 10394, 709, 1101, 813, 264, 50884], "temperature": 0.0, "avg_logprob": -0.12317816149286863, "compression_ratio": 1.7749077490774907, "no_speech_prob": 0.037316013127565384}, {"id": 226, "seek": 120784, "start": 1218.24, "end": 1222.3999999999999, "text": " EEG literature of where you just record a response and you say, okay, this brain,", "tokens": [50884, 33685, 38, 10394, 295, 689, 291, 445, 2136, 257, 4134, 293, 291, 584, 11, 1392, 11, 341, 3567, 11, 51092], "temperature": 0.0, "avg_logprob": -0.12317816149286863, "compression_ratio": 1.7749077490774907, "no_speech_prob": 0.037316013127565384}, {"id": 227, "seek": 120784, "start": 1222.3999999999999, "end": 1226.9599999999998, "text": " this part of the brain, there's a response to something, but without some sort of behavioral", "tokens": [51092, 341, 644, 295, 264, 3567, 11, 456, 311, 257, 4134, 281, 746, 11, 457, 1553, 512, 1333, 295, 19124, 51320], "temperature": 0.0, "avg_logprob": -0.12317816149286863, "compression_ratio": 1.7749077490774907, "no_speech_prob": 0.037316013127565384}, {"id": 228, "seek": 120784, "start": 1226.9599999999998, "end": 1230.0, "text": " output that you're measuring connected to that response, I'm always skeptical of", "tokens": [51320, 5598, 300, 291, 434, 13389, 4582, 281, 300, 4134, 11, 286, 478, 1009, 28601, 295, 51472], "temperature": 0.0, "avg_logprob": -0.12317816149286863, "compression_ratio": 1.7749077490774907, "no_speech_prob": 0.037316013127565384}, {"id": 229, "seek": 120784, "start": 1230.8, "end": 1232.8799999999999, "text": " how much, how much does that actually tell us about,", "tokens": [51512, 577, 709, 11, 577, 709, 775, 300, 767, 980, 505, 466, 11, 51616], "temperature": 0.0, "avg_logprob": -0.12317816149286863, "compression_ratio": 1.7749077490774907, "no_speech_prob": 0.037316013127565384}, {"id": 230, "seek": 123288, "start": 1233.1200000000001, "end": 1240.4, "text": " you know, what's actually happening and why, why we should care that there's a response there.", "tokens": [50376, 291, 458, 11, 437, 311, 767, 2737, 293, 983, 11, 983, 321, 820, 1127, 300, 456, 311, 257, 4134, 456, 13, 50740], "temperature": 0.0, "avg_logprob": -0.11171040863826356, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.0010956390760838985}, {"id": 231, "seek": 123288, "start": 1240.4, "end": 1244.0, "text": " I mean, we should care there's a response there, because it shows that we are responding to the", "tokens": [50740, 286, 914, 11, 321, 820, 1127, 456, 311, 257, 4134, 456, 11, 570, 309, 3110, 300, 321, 366, 16670, 281, 264, 50920], "temperature": 0.0, "avg_logprob": -0.11171040863826356, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.0010956390760838985}, {"id": 232, "seek": 123288, "start": 1244.0, "end": 1250.0, "text": " noun phrases. And that's part of a story where, you know, we want to understand how the brain", "tokens": [50920, 23307, 20312, 13, 400, 300, 311, 644, 295, 257, 1657, 689, 11, 291, 458, 11, 321, 528, 281, 1223, 577, 264, 3567, 51220], "temperature": 0.0, "avg_logprob": -0.11171040863826356, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.0010956390760838985}, {"id": 233, "seek": 123288, "start": 1250.0, "end": 1256.3200000000002, "text": " understands language. How do we answer that? Well, I'm not sure. And, and you, you know,", "tokens": [51220, 15146, 2856, 13, 1012, 360, 321, 1867, 300, 30, 1042, 11, 286, 478, 406, 988, 13, 400, 11, 293, 291, 11, 291, 458, 11, 51536], "temperature": 0.0, "avg_logprob": -0.11171040863826356, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.0010956390760838985}, {"id": 234, "seek": 123288, "start": 1256.3200000000002, "end": 1261.1200000000001, "text": " people try behavioral experiments, they try manipulations to see how it changes people's", "tokens": [51536, 561, 853, 19124, 12050, 11, 436, 853, 9258, 4136, 281, 536, 577, 309, 2962, 561, 311, 51776], "temperature": 0.0, "avg_logprob": -0.11171040863826356, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.0010956390760838985}, {"id": 235, "seek": 126112, "start": 1261.12, "end": 1266.08, "text": " understanding and so on. There's, you know, there's a long history of this. But it hasn't", "tokens": [50364, 3701, 293, 370, 322, 13, 821, 311, 11, 291, 458, 11, 456, 311, 257, 938, 2503, 295, 341, 13, 583, 309, 6132, 380, 50612], "temperature": 0.0, "avg_logprob": -0.07088492615054352, "compression_ratio": 1.5546218487394958, "no_speech_prob": 0.00263643404468894}, {"id": 236, "seek": 126112, "start": 1266.08, "end": 1273.12, "text": " yet produced an account of language. And I'm not to answer your question going to produce one either.", "tokens": [50612, 1939, 7126, 364, 2696, 295, 2856, 13, 400, 286, 478, 406, 281, 1867, 428, 1168, 516, 281, 5258, 472, 2139, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07088492615054352, "compression_ratio": 1.5546218487394958, "no_speech_prob": 0.00263643404468894}, {"id": 237, "seek": 126112, "start": 1274.9599999999998, "end": 1282.32, "text": " But that's absolutely what we need to do. I mean, so I guess my idea at the moment", "tokens": [51056, 583, 300, 311, 3122, 437, 321, 643, 281, 360, 13, 286, 914, 11, 370, 286, 2041, 452, 1558, 412, 264, 1623, 51424], "temperature": 0.0, "avg_logprob": -0.07088492615054352, "compression_ratio": 1.5546218487394958, "no_speech_prob": 0.00263643404468894}, {"id": 238, "seek": 126112, "start": 1282.32, "end": 1290.3999999999999, "text": " is that we start, that we use the EEG response as some sort of proxy for, for what's happening.", "tokens": [51424, 307, 300, 321, 722, 11, 300, 321, 764, 264, 33685, 38, 4134, 382, 512, 1333, 295, 29690, 337, 11, 337, 437, 311, 2737, 13, 51828], "temperature": 0.0, "avg_logprob": -0.07088492615054352, "compression_ratio": 1.5546218487394958, "no_speech_prob": 0.00263643404468894}, {"id": 239, "seek": 129040, "start": 1290.4, "end": 1297.1200000000001, "text": " And we try and see, well, just let me say what we've done here, for example. So again, we're", "tokens": [50364, 400, 321, 853, 293, 536, 11, 731, 11, 445, 718, 385, 584, 437, 321, 600, 1096, 510, 11, 337, 1365, 13, 407, 797, 11, 321, 434, 50700], "temperature": 0.0, "avg_logprob": -0.10216604868570964, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.004409994930028915}, {"id": 240, "seek": 129040, "start": 1297.1200000000001, "end": 1303.2, "text": " looking at the EEG response to free text. We're doing some big regression, and we're trying to see", "tokens": [50700, 1237, 412, 264, 33685, 38, 4134, 281, 1737, 2487, 13, 492, 434, 884, 512, 955, 24590, 11, 293, 321, 434, 1382, 281, 536, 51004], "temperature": 0.0, "avg_logprob": -0.10216604868570964, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.004409994930028915}, {"id": 241, "seek": 129040, "start": 1303.8400000000001, "end": 1308.4, "text": " how people are responding to different types of words. And what we can see what this is showing", "tokens": [51036, 577, 561, 366, 16670, 281, 819, 3467, 295, 2283, 13, 400, 437, 321, 393, 536, 437, 341, 307, 4099, 51264], "temperature": 0.0, "avg_logprob": -0.10216604868570964, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.004409994930028915}, {"id": 242, "seek": 129040, "start": 1308.4, "end": 1313.68, "text": " is that for part of the EEG response, there is a difference in how people are responding to", "tokens": [51264, 307, 300, 337, 644, 295, 264, 33685, 38, 4134, 11, 456, 307, 257, 2649, 294, 577, 561, 366, 16670, 281, 51528], "temperature": 0.0, "avg_logprob": -0.10216604868570964, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.004409994930028915}, {"id": 243, "seek": 129040, "start": 1313.68, "end": 1319.1200000000001, "text": " words according to the categorization of the words into function and content. And so we can see that", "tokens": [51528, 2283, 4650, 281, 264, 19250, 2144, 295, 264, 2283, 666, 2445, 293, 2701, 13, 400, 370, 321, 393, 536, 300, 51800], "temperature": 0.0, "avg_logprob": -0.10216604868570964, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.004409994930028915}, {"id": 244, "seek": 131912, "start": 1319.9199999999998, "end": 1325.1999999999998, "text": " people's brains respond differently to some particular categorization of word. And so", "tokens": [50404, 561, 311, 15442, 4196, 7614, 281, 512, 1729, 19250, 2144, 295, 1349, 13, 400, 370, 50668], "temperature": 0.0, "avg_logprob": -0.14894411610622033, "compression_ratio": 1.6844106463878328, "no_speech_prob": 0.0025416072458028793}, {"id": 245, "seek": 131912, "start": 1325.1999999999998, "end": 1329.6799999999998, "text": " what you might be optimistic about is that if you've got better at this, you could try different", "tokens": [50668, 437, 291, 1062, 312, 19397, 466, 307, 300, 498, 291, 600, 658, 1101, 412, 341, 11, 291, 727, 853, 819, 50892], "temperature": 0.0, "avg_logprob": -0.14894411610622033, "compression_ratio": 1.6844106463878328, "no_speech_prob": 0.0025416072458028793}, {"id": 246, "seek": 131912, "start": 1329.6799999999998, "end": 1337.28, "text": " categorizations and match them to different ideas about how the brain might approach parsing language.", "tokens": [50892, 19250, 14455, 293, 2995, 552, 281, 819, 3487, 466, 577, 264, 3567, 1062, 3109, 21156, 278, 2856, 13, 51272], "temperature": 0.0, "avg_logprob": -0.14894411610622033, "compression_ratio": 1.6844106463878328, "no_speech_prob": 0.0025416072458028793}, {"id": 247, "seek": 131912, "start": 1338.56, "end": 1344.1599999999999, "text": " But I agree. You can hear that then to maybe the grammars of the linguistics.", "tokens": [51336, 583, 286, 3986, 13, 509, 393, 1568, 300, 550, 281, 1310, 264, 17570, 685, 295, 264, 21766, 6006, 13, 51616], "temperature": 0.0, "avg_logprob": -0.14894411610622033, "compression_ratio": 1.6844106463878328, "no_speech_prob": 0.0025416072458028793}, {"id": 248, "seek": 131912, "start": 1344.1599999999999, "end": 1348.08, "text": " Yeah. And then separately, of course. Mel asked me not to mention transformers,", "tokens": [51616, 865, 13, 400, 550, 14759, 11, 295, 1164, 13, 7375, 2351, 385, 406, 281, 2152, 4088, 433, 11, 51812], "temperature": 0.0, "avg_logprob": -0.14894411610622033, "compression_ratio": 1.6844106463878328, "no_speech_prob": 0.0025416072458028793}, {"id": 249, "seek": 134808, "start": 1348.08, "end": 1354.3999999999999, "text": " but then separately, you look at, you can probe how large language models deal with language. You", "tokens": [50364, 457, 550, 14759, 11, 291, 574, 412, 11, 291, 393, 22715, 577, 2416, 2856, 5245, 2028, 365, 2856, 13, 509, 50680], "temperature": 0.0, "avg_logprob": -0.10609943802292282, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.0009789119940251112}, {"id": 250, "seek": 134808, "start": 1354.3999999999999, "end": 1359.9199999999998, "text": " can probe the grammatician's approach to language. You can probe the whole linguistic", "tokens": [50680, 393, 22715, 264, 17570, 2399, 952, 311, 3109, 281, 2856, 13, 509, 393, 22715, 264, 1379, 43002, 50956], "temperature": 0.0, "avg_logprob": -0.10609943802292282, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.0009789119940251112}, {"id": 251, "seek": 134808, "start": 1359.9199999999998, "end": 1364.96, "text": " tradition of our accounts of how language is dealt with. And then this gives a sort of neural", "tokens": [50956, 6994, 295, 527, 9402, 295, 577, 2856, 307, 15991, 365, 13, 400, 550, 341, 2709, 257, 1333, 295, 18161, 51208], "temperature": 0.0, "avg_logprob": -0.10609943802292282, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.0009789119940251112}, {"id": 252, "seek": 134808, "start": 1364.96, "end": 1370.1599999999999, "text": " account, the neural view of what grammar is like. But whether that's going to work or exactly the", "tokens": [51208, 2696, 11, 264, 18161, 1910, 295, 437, 22317, 307, 411, 13, 583, 1968, 300, 311, 516, 281, 589, 420, 2293, 264, 51468], "temperature": 0.0, "avg_logprob": -0.10609943802292282, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.0009789119940251112}, {"id": 253, "seek": 134808, "start": 1370.1599999999999, "end": 1375.9199999999998, "text": " details of that, I don't know. And so certainly what I would advocate is that we could collect a", "tokens": [51468, 4365, 295, 300, 11, 286, 500, 380, 458, 13, 400, 370, 3297, 437, 286, 576, 14608, 307, 300, 321, 727, 2500, 257, 51756], "temperature": 0.0, "avg_logprob": -0.10609943802292282, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.0009789119940251112}, {"id": 254, "seek": 137592, "start": 1375.92, "end": 1383.8400000000001, "text": " big data set, you know, with lots of dots of different languages. The little prince, as maybe", "tokens": [50364, 955, 1412, 992, 11, 291, 458, 11, 365, 3195, 295, 15026, 295, 819, 8650, 13, 440, 707, 16467, 11, 382, 1310, 50760], "temperature": 0.0, "avg_logprob": -0.18147613978621982, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.004848570097237825}, {"id": 255, "seek": 137592, "start": 1383.8400000000001, "end": 1389.04, "text": " you know, is the non-religious text that has been translated into the most languages. It exists in", "tokens": [50760, 291, 458, 11, 307, 264, 2107, 12, 4419, 21883, 2487, 300, 575, 668, 16805, 666, 264, 881, 8650, 13, 467, 8198, 294, 51020], "temperature": 0.0, "avg_logprob": -0.18147613978621982, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.004848570097237825}, {"id": 256, "seek": 137592, "start": 1389.04, "end": 1394.96, "text": " 300 languages. It exists as an audiobook in 50 or 60 languages. And so it's been suggested by these", "tokens": [51020, 6641, 8650, 13, 467, 8198, 382, 364, 40031, 294, 2625, 420, 4060, 8650, 13, 400, 370, 309, 311, 668, 10945, 538, 613, 51316], "temperature": 0.0, "avg_logprob": -0.18147613978621982, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.004848570097237825}, {"id": 257, "seek": 137592, "start": 1394.96, "end": 1401.04, "text": " folk here, Jinxing Lee, Brennan, Haile and some of their co-workers, that we collect a large", "tokens": [51316, 15748, 510, 11, 10617, 87, 278, 6957, 11, 31200, 17622, 11, 4064, 794, 293, 512, 295, 641, 598, 12, 37101, 11, 300, 321, 2500, 257, 2416, 51620], "temperature": 0.0, "avg_logprob": -0.18147613978621982, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.004848570097237825}, {"id": 258, "seek": 140104, "start": 1401.68, "end": 1406.08, "text": " corpus of different varying qualities, some, you know, some MEG, which is, you know,", "tokens": [50396, 1181, 31624, 295, 819, 22984, 16477, 11, 512, 11, 291, 458, 11, 512, 12003, 38, 11, 597, 307, 11, 291, 458, 11, 50616], "temperature": 0.0, "avg_logprob": -0.15333409653496496, "compression_ratio": 1.5532786885245902, "no_speech_prob": 0.024829968810081482}, {"id": 259, "seek": 140104, "start": 1406.08, "end": 1411.28, "text": " very high quality, some consumer EEG, but with many more participants across lots of languages.", "tokens": [50616, 588, 1090, 3125, 11, 512, 9711, 33685, 38, 11, 457, 365, 867, 544, 10503, 2108, 3195, 295, 8650, 13, 50876], "temperature": 0.0, "avg_logprob": -0.15333409653496496, "compression_ratio": 1.5532786885245902, "no_speech_prob": 0.024829968810081482}, {"id": 260, "seek": 140104, "start": 1411.28, "end": 1416.8799999999999, "text": " And we start trying to understand, from the point of view of the brain, what grammar looks like.", "tokens": [50876, 400, 321, 722, 1382, 281, 1223, 11, 490, 264, 935, 295, 1910, 295, 264, 3567, 11, 437, 22317, 1542, 411, 13, 51156], "temperature": 0.0, "avg_logprob": -0.15333409653496496, "compression_ratio": 1.5532786885245902, "no_speech_prob": 0.024829968810081482}, {"id": 261, "seek": 140104, "start": 1418.08, "end": 1423.84, "text": " So far, this hasn't been done. And I guess people are trying to raise money to do it and have failed.", "tokens": [51216, 407, 1400, 11, 341, 6132, 380, 668, 1096, 13, 400, 286, 2041, 561, 366, 1382, 281, 5300, 1460, 281, 360, 309, 293, 362, 7612, 13, 51504], "temperature": 0.0, "avg_logprob": -0.15333409653496496, "compression_ratio": 1.5532786885245902, "no_speech_prob": 0.024829968810081482}, {"id": 262, "seek": 142384, "start": 1424.8, "end": 1431.52, "text": " So that's, you know, that's just by way of sort of motivation. That's where I came into thinking", "tokens": [50412, 407, 300, 311, 11, 291, 458, 11, 300, 311, 445, 538, 636, 295, 1333, 295, 12335, 13, 663, 311, 689, 286, 1361, 666, 1953, 50748], "temperature": 0.0, "avg_logprob": -0.12287057947229456, "compression_ratio": 1.9821428571428572, "no_speech_prob": 0.008308928459882736}, {"id": 263, "seek": 142384, "start": 1431.52, "end": 1436.48, "text": " about language. And, you know, my interest in language is trying to understand, you know,", "tokens": [50748, 466, 2856, 13, 400, 11, 291, 458, 11, 452, 1179, 294, 2856, 307, 1382, 281, 1223, 11, 291, 458, 11, 50996], "temperature": 0.0, "avg_logprob": -0.12287057947229456, "compression_ratio": 1.9821428571428572, "no_speech_prob": 0.008308928459882736}, {"id": 264, "seek": 142384, "start": 1436.48, "end": 1442.24, "text": " how is language, what's special about language, what makes language, language. And of course,", "tokens": [50996, 577, 307, 2856, 11, 437, 311, 2121, 466, 2856, 11, 437, 1669, 2856, 11, 2856, 13, 400, 295, 1164, 11, 51284], "temperature": 0.0, "avg_logprob": -0.12287057947229456, "compression_ratio": 1.9821428571428572, "no_speech_prob": 0.008308928459882736}, {"id": 265, "seek": 142384, "start": 1442.24, "end": 1445.52, "text": " when you start thinking about this, you start asking these questions, how do we do these", "tokens": [51284, 562, 291, 722, 1953, 466, 341, 11, 291, 722, 3365, 613, 1651, 11, 577, 360, 321, 360, 613, 51448], "temperature": 0.0, "avg_logprob": -0.12287057947229456, "compression_ratio": 1.9821428571428572, "no_speech_prob": 0.008308928459882736}, {"id": 266, "seek": 142384, "start": 1445.52, "end": 1448.9599999999998, "text": " experiments? And it strikes you that there's a whole sort of separate story to language,", "tokens": [51448, 12050, 30, 400, 309, 16750, 291, 300, 456, 311, 257, 1379, 1333, 295, 4994, 1657, 281, 2856, 11, 51620], "temperature": 0.0, "avg_logprob": -0.12287057947229456, "compression_ratio": 1.9821428571428572, "no_speech_prob": 0.008308928459882736}, {"id": 267, "seek": 142384, "start": 1448.9599999999998, "end": 1453.52, "text": " which is to do with evolution. And maybe the hope that if we think a little bit about evolution,", "tokens": [51620, 597, 307, 281, 360, 365, 9303, 13, 400, 1310, 264, 1454, 300, 498, 321, 519, 257, 707, 857, 466, 9303, 11, 51848], "temperature": 0.0, "avg_logprob": -0.12287057947229456, "compression_ratio": 1.9821428571428572, "no_speech_prob": 0.008308928459882736}, {"id": 268, "seek": 145352, "start": 1453.6, "end": 1459.12, "text": " about how languages arise from evolution, the species point of view, and then separately,", "tokens": [50368, 466, 577, 8650, 20288, 490, 9303, 11, 264, 6172, 935, 295, 1910, 11, 293, 550, 14759, 11, 50644], "temperature": 0.0, "avg_logprob": -0.13121101591322157, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.0026822786312550306}, {"id": 269, "seek": 145352, "start": 1459.12, "end": 1466.24, "text": " how languages change, it would be, it might tell us something about the innate structure of language.", "tokens": [50644, 577, 8650, 1319, 11, 309, 576, 312, 11, 309, 1062, 980, 505, 746, 466, 264, 41766, 3877, 295, 2856, 13, 51000], "temperature": 0.0, "avg_logprob": -0.13121101591322157, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.0026822786312550306}, {"id": 270, "seek": 145352, "start": 1466.24, "end": 1470.48, "text": " And maybe we can then think about how language, why languages might have to be the way they are,", "tokens": [51000, 400, 1310, 321, 393, 550, 519, 466, 577, 2856, 11, 983, 8650, 1062, 362, 281, 312, 264, 636, 436, 366, 11, 51212], "temperature": 0.0, "avg_logprob": -0.13121101591322157, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.0026822786312550306}, {"id": 271, "seek": 145352, "start": 1470.48, "end": 1476.48, "text": " compared to other ways they could be. And I think this is, you know, a very interesting question.", "tokens": [51212, 5347, 281, 661, 2098, 436, 727, 312, 13, 400, 286, 519, 341, 307, 11, 291, 458, 11, 257, 588, 1880, 1168, 13, 51512], "temperature": 0.0, "avg_logprob": -0.13121101591322157, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.0026822786312550306}, {"id": 272, "seek": 147648, "start": 1476.48, "end": 1482.72, "text": " One of the sort of striking things is that, well, you know, we know from Creole languages and", "tokens": [50364, 1485, 295, 264, 1333, 295, 18559, 721, 307, 300, 11, 731, 11, 291, 458, 11, 321, 458, 490, 9549, 4812, 8650, 293, 50676], "temperature": 0.0, "avg_logprob": -0.10898122020151424, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.0173233300447464}, {"id": 273, "seek": 147648, "start": 1484.72, "end": 1491.3600000000001, "text": " sign languages and so on that languages do arise with a large amount of their structure", "tokens": [50776, 1465, 8650, 293, 370, 322, 300, 8650, 360, 20288, 365, 257, 2416, 2372, 295, 641, 3877, 51108], "temperature": 0.0, "avg_logprob": -0.10898122020151424, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.0173233300447464}, {"id": 274, "seek": 147648, "start": 1491.3600000000001, "end": 1495.92, "text": " already present. But it's still an open question as to whether there are parts of language", "tokens": [51108, 1217, 1974, 13, 583, 309, 311, 920, 364, 1269, 1168, 382, 281, 1968, 456, 366, 3166, 295, 2856, 51336], "temperature": 0.0, "avg_logprob": -0.10898122020151424, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.0173233300447464}, {"id": 275, "seek": 147648, "start": 1495.92, "end": 1501.52, "text": " that develop through time, you know, as languages evolve, do they, do they change in a consistent", "tokens": [51336, 300, 1499, 807, 565, 11, 291, 458, 11, 382, 8650, 16693, 11, 360, 436, 11, 360, 436, 1319, 294, 257, 8398, 51616], "temperature": 0.0, "avg_logprob": -0.10898122020151424, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.0173233300447464}, {"id": 276, "seek": 150152, "start": 1501.52, "end": 1508.48, "text": " way? Or are they the same from the very start? So near where I work, there's a museum, the Bristol", "tokens": [50364, 636, 30, 1610, 366, 436, 264, 912, 490, 264, 588, 722, 30, 407, 2651, 689, 286, 589, 11, 456, 311, 257, 8441, 11, 264, 41208, 50712], "temperature": 0.0, "avg_logprob": -0.15789978249559125, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.036323431879282}, {"id": 277, "seek": 150152, "start": 1508.48, "end": 1515.68, "text": " Museum, and Art Gallery, I left that bit out. And they have some of these freezes from Nimrod.", "tokens": [50712, 10967, 11, 293, 5735, 29733, 11, 286, 1411, 300, 857, 484, 13, 400, 436, 362, 512, 295, 613, 1737, 12214, 490, 45251, 11452, 13, 51072], "temperature": 0.0, "avg_logprob": -0.15789978249559125, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.036323431879282}, {"id": 278, "seek": 150152, "start": 1515.68, "end": 1521.04, "text": " So the palace at Nimrod was broken up by people who went there and they took the panels and sent", "tokens": [51072, 407, 264, 15207, 412, 45251, 11452, 390, 5463, 493, 538, 561, 567, 1437, 456, 293, 436, 1890, 264, 13419, 293, 2279, 51340], "temperature": 0.0, "avg_logprob": -0.15789978249559125, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.036323431879282}, {"id": 279, "seek": 150152, "start": 1521.04, "end": 1527.44, "text": " them all around the world in, I guess, an act of gross theft, although what was left in Nimrod", "tokens": [51340, 552, 439, 926, 264, 1002, 294, 11, 286, 2041, 11, 364, 605, 295, 11367, 28508, 11, 4878, 437, 390, 1411, 294, 45251, 11452, 51660], "temperature": 0.0, "avg_logprob": -0.15789978249559125, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.036323431879282}, {"id": 280, "seek": 152744, "start": 1527.44, "end": 1532.8, "text": " has since been destroyed, so in the way it was quite lucky. And these panels do have this kind", "tokens": [50364, 575, 1670, 668, 8937, 11, 370, 294, 264, 636, 309, 390, 1596, 6356, 13, 400, 613, 13419, 360, 362, 341, 733, 50632], "temperature": 0.0, "avg_logprob": -0.21515437761942546, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.008329630829393864}, {"id": 281, "seek": 152744, "start": 1532.8, "end": 1539.3600000000001, "text": " of cuneiform script written across them. It's a standard inscription. And it's Ashnirah. Ashnirah", "tokens": [50632, 295, 269, 2613, 8629, 5755, 3720, 2108, 552, 13, 467, 311, 257, 3832, 49882, 13, 400, 309, 311, 10279, 77, 347, 545, 13, 10279, 77, 347, 545, 50960], "temperature": 0.0, "avg_logprob": -0.21515437761942546, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.008329630829393864}, {"id": 282, "seek": 152744, "start": 1540.88, "end": 1546.3200000000002, "text": " is basically showing off in this description. So this standard inscription here in cuneiform", "tokens": [51036, 307, 1936, 4099, 766, 294, 341, 3855, 13, 407, 341, 3832, 49882, 510, 294, 269, 2613, 8629, 51308], "temperature": 0.0, "avg_logprob": -0.21515437761942546, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.008329630829393864}, {"id": 283, "seek": 152744, "start": 1546.3200000000002, "end": 1551.3600000000001, "text": " explains what a great person he is. And some of it's quite sort of bloody as this bit is,", "tokens": [51308, 13948, 437, 257, 869, 954, 415, 307, 13, 400, 512, 295, 309, 311, 1596, 1333, 295, 18938, 382, 341, 857, 307, 11, 51560], "temperature": 0.0, "avg_logprob": -0.21515437761942546, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.008329630829393864}, {"id": 284, "seek": 152744, "start": 1551.3600000000001, "end": 1555.3600000000001, "text": " there are men young and old, I took prisoners of some I cut off their feet and hands of others", "tokens": [51560, 456, 366, 1706, 2037, 293, 1331, 11, 286, 1890, 20417, 295, 512, 286, 1723, 766, 641, 3521, 293, 2377, 295, 2357, 51760], "temperature": 0.0, "avg_logprob": -0.21515437761942546, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.008329630829393864}, {"id": 285, "seek": 155536, "start": 1555.36, "end": 1560.3999999999999, "text": " I cut off the ears, noses and lips of the young men's ears. I made a heap of the old men's beads", "tokens": [50364, 286, 1723, 766, 264, 8798, 11, 3269, 279, 293, 10118, 295, 264, 2037, 1706, 311, 8798, 13, 286, 1027, 257, 33591, 295, 264, 1331, 1706, 311, 20369, 50616], "temperature": 0.0, "avg_logprob": -0.1647239261203342, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.003391908248886466}, {"id": 286, "seek": 155536, "start": 1560.3999999999999, "end": 1565.36, "text": " I made him heads, I made him in a rash, et cetera, et cetera, et cetera. Some of it's much nicer.", "tokens": [50616, 286, 1027, 796, 8050, 11, 286, 1027, 796, 294, 257, 40357, 11, 1030, 11458, 11, 1030, 11458, 11, 1030, 11458, 13, 2188, 295, 309, 311, 709, 22842, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1647239261203342, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.003391908248886466}, {"id": 287, "seek": 155536, "start": 1565.36, "end": 1571.04, "text": " It's about making pleasure palaces and beautiful things, et cetera. But I couldn't find any of", "tokens": [50864, 467, 311, 466, 1455, 6834, 3984, 2116, 293, 2238, 721, 11, 1030, 11458, 13, 583, 286, 2809, 380, 915, 604, 295, 51148], "temperature": 0.0, "avg_logprob": -0.1647239261203342, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.003391908248886466}, {"id": 288, "seek": 155536, "start": 1571.04, "end": 1575.52, "text": " that in an easily cut and pasted form. So I had to get this rather bloody bit instead.", "tokens": [51148, 300, 294, 364, 3612, 1723, 293, 1791, 292, 1254, 13, 407, 286, 632, 281, 483, 341, 2831, 18938, 857, 2602, 13, 51372], "temperature": 0.0, "avg_logprob": -0.1647239261203342, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.003391908248886466}, {"id": 289, "seek": 155536, "start": 1576.4799999999998, "end": 1583.4399999999998, "text": " It says something about our civilization. But the point anyway, is that what is striking", "tokens": [51420, 467, 1619, 746, 466, 527, 18036, 13, 583, 264, 935, 4033, 11, 307, 300, 437, 307, 18559, 51768], "temperature": 0.0, "avg_logprob": -0.1647239261203342, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.003391908248886466}, {"id": 290, "seek": 158344, "start": 1584.0800000000002, "end": 1591.04, "text": " in the text is the lack of sort of the normal clause structure. There's very few instances or", "tokens": [50396, 294, 264, 2487, 307, 264, 5011, 295, 1333, 295, 264, 2710, 25925, 3877, 13, 821, 311, 588, 1326, 14519, 420, 50744], "temperature": 0.0, "avg_logprob": -0.13852136702764603, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.005879512056708336}, {"id": 291, "seek": 158344, "start": 1591.04, "end": 1595.3600000000001, "text": " no instances of the sort of clauses that you might expect where there's who and which is", "tokens": [50744, 572, 14519, 295, 264, 1333, 295, 49072, 300, 291, 1062, 2066, 689, 456, 311, 567, 293, 597, 307, 50960], "temperature": 0.0, "avg_logprob": -0.13852136702764603, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.005879512056708336}, {"id": 292, "seek": 158344, "start": 1596.0, "end": 1602.4, "text": " and so on linking together the sentences. Instead, it's this rolling list, this very sort of list", "tokens": [50992, 293, 370, 322, 25775, 1214, 264, 16579, 13, 7156, 11, 309, 311, 341, 9439, 1329, 11, 341, 588, 1333, 295, 1329, 51312], "temperature": 0.0, "avg_logprob": -0.13852136702764603, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.005879512056708336}, {"id": 293, "seek": 158344, "start": 1602.4, "end": 1607.28, "text": " like structure. And so you wonder is that because the language has not at this point", "tokens": [51312, 411, 3877, 13, 400, 370, 291, 2441, 307, 300, 570, 264, 2856, 575, 406, 412, 341, 935, 51556], "temperature": 0.0, "avg_logprob": -0.13852136702764603, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.005879512056708336}, {"id": 294, "seek": 158344, "start": 1607.28, "end": 1611.52, "text": " evolved all the structures of modern language, the sort of merge and clause structures that we", "tokens": [51556, 14178, 439, 264, 9227, 295, 4363, 2856, 11, 264, 1333, 295, 22183, 293, 25925, 9227, 300, 321, 51768], "temperature": 0.0, "avg_logprob": -0.13852136702764603, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.005879512056708336}, {"id": 295, "seek": 161152, "start": 1611.52, "end": 1618.24, "text": " have now, or is it just that that's how they like to write in their ceremonial functions.", "tokens": [50364, 362, 586, 11, 420, 307, 309, 445, 300, 300, 311, 577, 436, 411, 281, 2464, 294, 641, 25920, 831, 6828, 13, 50700], "temperature": 0.0, "avg_logprob": -0.10494063621343569, "compression_ratio": 1.6745283018867925, "no_speech_prob": 0.004674259573221207}, {"id": 296, "seek": 161152, "start": 1618.24, "end": 1622.48, "text": " So just striking that there's a lot to be gained from trying to understand", "tokens": [50700, 407, 445, 18559, 300, 456, 311, 257, 688, 281, 312, 12634, 490, 1382, 281, 1223, 50912], "temperature": 0.0, "avg_logprob": -0.10494063621343569, "compression_ratio": 1.6745283018867925, "no_speech_prob": 0.004674259573221207}, {"id": 297, "seek": 161152, "start": 1622.48, "end": 1627.92, "text": " something about the evolution of language. And so that's what I wanted to talk a little bit about", "tokens": [50912, 746, 466, 264, 9303, 295, 2856, 13, 400, 370, 300, 311, 437, 286, 1415, 281, 751, 257, 707, 857, 466, 51184], "temperature": 0.0, "avg_logprob": -0.10494063621343569, "compression_ratio": 1.6745283018867925, "no_speech_prob": 0.004674259573221207}, {"id": 298, "seek": 161152, "start": 1628.6399999999999, "end": 1635.52, "text": " now. And so the first thing I wanted to talk about is really is to urge a return considering", "tokens": [51220, 586, 13, 400, 370, 264, 700, 551, 286, 1415, 281, 751, 466, 307, 534, 307, 281, 19029, 257, 2736, 8079, 51564], "temperature": 0.0, "avg_logprob": -0.10494063621343569, "compression_ratio": 1.6745283018867925, "no_speech_prob": 0.004674259573221207}, {"id": 299, "seek": 163552, "start": 1635.52, "end": 1642.32, "text": " the iterative language model that Simon Kirby and his co-workers came up with about 20 years ago.", "tokens": [50364, 264, 17138, 1166, 2856, 2316, 300, 13193, 37423, 293, 702, 598, 12, 37101, 1361, 493, 365, 466, 945, 924, 2057, 13, 50704], "temperature": 0.0, "avg_logprob": -0.10330235827100145, "compression_ratio": 1.7546296296296295, "no_speech_prob": 0.045626286417245865}, {"id": 300, "seek": 163552, "start": 1642.32, "end": 1648.0, "text": " So I don't know if you know the iterative language model, it's a language, it's a model for the", "tokens": [50704, 407, 286, 500, 380, 458, 498, 291, 458, 264, 17138, 1166, 2856, 2316, 11, 309, 311, 257, 2856, 11, 309, 311, 257, 2316, 337, 264, 50988], "temperature": 0.0, "avg_logprob": -0.10330235827100145, "compression_ratio": 1.7546296296296295, "no_speech_prob": 0.045626286417245865}, {"id": 301, "seek": 163552, "start": 1648.0, "end": 1657.52, "text": " evolution of languages. The story is that Kirby and his co-workers discovered it. They simulated", "tokens": [50988, 9303, 295, 8650, 13, 440, 1657, 307, 300, 37423, 293, 702, 598, 12, 37101, 6941, 309, 13, 814, 41713, 51464], "temperature": 0.0, "avg_logprob": -0.10330235827100145, "compression_ratio": 1.7546296296296295, "no_speech_prob": 0.045626286417245865}, {"id": 302, "seek": 163552, "start": 1657.52, "end": 1663.2, "text": " a little bit, but they came quickly hard up against the computational limitations at the", "tokens": [51464, 257, 707, 857, 11, 457, 436, 1361, 2661, 1152, 493, 1970, 264, 28270, 15705, 412, 264, 51748], "temperature": 0.0, "avg_logprob": -0.10330235827100145, "compression_ratio": 1.7546296296296295, "no_speech_prob": 0.045626286417245865}, {"id": 303, "seek": 166320, "start": 1663.2, "end": 1670.48, "text": " time. It is quite computationally expensive. And so they worked on it and then they kind of abandoned", "tokens": [50364, 565, 13, 467, 307, 1596, 24903, 379, 5124, 13, 400, 370, 436, 2732, 322, 309, 293, 550, 436, 733, 295, 13732, 50728], "temperature": 0.0, "avg_logprob": -0.0881880234027731, "compression_ratio": 1.651063829787234, "no_speech_prob": 0.004184111021459103}, {"id": 304, "seek": 166320, "start": 1670.48, "end": 1676.32, "text": " it and went on to try and do the same experiment that they'd done in simulation in real people.", "tokens": [50728, 309, 293, 1437, 322, 281, 853, 293, 360, 264, 912, 5120, 300, 436, 1116, 1096, 294, 16575, 294, 957, 561, 13, 51020], "temperature": 0.0, "avg_logprob": -0.0881880234027731, "compression_ratio": 1.651063829787234, "no_speech_prob": 0.004184111021459103}, {"id": 305, "seek": 166320, "start": 1676.32, "end": 1681.92, "text": " And so Simon Kirby has very successfully spent the last 20 years using toy languages and toy", "tokens": [51020, 400, 370, 13193, 37423, 575, 588, 10727, 4418, 264, 1036, 945, 924, 1228, 12058, 8650, 293, 12058, 51300], "temperature": 0.0, "avg_logprob": -0.0881880234027731, "compression_ratio": 1.651063829787234, "no_speech_prob": 0.004184111021459103}, {"id": 306, "seek": 166320, "start": 1681.92, "end": 1689.28, "text": " language learning as a pro into how people learn languages, but hasn't considered much beyond the", "tokens": [51300, 2856, 2539, 382, 257, 447, 666, 577, 561, 1466, 8650, 11, 457, 6132, 380, 4888, 709, 4399, 264, 51668], "temperature": 0.0, "avg_logprob": -0.0881880234027731, "compression_ratio": 1.651063829787234, "no_speech_prob": 0.004184111021459103}, {"id": 307, "seek": 168928, "start": 1689.28, "end": 1694.0, "text": " original work, the iterative language model itself. And now that we've got faster computers and so on,", "tokens": [50364, 3380, 589, 11, 264, 17138, 1166, 2856, 2316, 2564, 13, 400, 586, 300, 321, 600, 658, 4663, 10807, 293, 370, 322, 11, 50600], "temperature": 0.0, "avg_logprob": -0.08538144879636511, "compression_ratio": 1.921875, "no_speech_prob": 0.01029270701110363}, {"id": 308, "seek": 168928, "start": 1694.0, "end": 1699.12, "text": " it might I think be very interesting to go back and think about this much more. So in the iterative", "tokens": [50600, 309, 1062, 286, 519, 312, 588, 1880, 281, 352, 646, 293, 519, 466, 341, 709, 544, 13, 407, 294, 264, 17138, 1166, 50856], "temperature": 0.0, "avg_logprob": -0.08538144879636511, "compression_ratio": 1.921875, "no_speech_prob": 0.01029270701110363}, {"id": 309, "seek": 168928, "start": 1699.12, "end": 1704.8799999999999, "text": " language model, basically you have a teacher and the teacher teaches a pupil and then the pupil", "tokens": [50856, 2856, 2316, 11, 1936, 291, 362, 257, 5027, 293, 264, 5027, 16876, 257, 44533, 293, 550, 264, 44533, 51144], "temperature": 0.0, "avg_logprob": -0.08538144879636511, "compression_ratio": 1.921875, "no_speech_prob": 0.01029270701110363}, {"id": 310, "seek": 168928, "start": 1704.8799999999999, "end": 1712.3999999999999, "text": " becomes the teacher and teaches another pupil and so on. So it's a chain of learnings, teachings", "tokens": [51144, 3643, 264, 5027, 293, 16876, 1071, 44533, 293, 370, 322, 13, 407, 309, 311, 257, 5021, 295, 2539, 82, 11, 21037, 51520], "temperature": 0.0, "avg_logprob": -0.08538144879636511, "compression_ratio": 1.921875, "no_speech_prob": 0.01029270701110363}, {"id": 311, "seek": 168928, "start": 1712.3999999999999, "end": 1717.12, "text": " and learnings. And the idea is that the language progresses or changes through this teaching and", "tokens": [51520, 293, 2539, 82, 13, 400, 264, 1558, 307, 300, 264, 2856, 41929, 420, 2962, 807, 341, 4571, 293, 51756], "temperature": 0.0, "avg_logprob": -0.08538144879636511, "compression_ratio": 1.921875, "no_speech_prob": 0.01029270701110363}, {"id": 312, "seek": 171712, "start": 1717.12, "end": 1723.52, "text": " learning. The hope is that we might learn something about how the structured language arises by seeing", "tokens": [50364, 2539, 13, 440, 1454, 307, 300, 321, 1062, 1466, 746, 466, 577, 264, 18519, 2856, 27388, 538, 2577, 50684], "temperature": 0.0, "avg_logprob": -0.09413464525912671, "compression_ratio": 1.7623318385650224, "no_speech_prob": 0.006354630459100008}, {"id": 313, "seek": 171712, "start": 1723.52, "end": 1728.8799999999999, "text": " if it arises through this simulation of the teaching and learning process. And the crucial point,", "tokens": [50684, 498, 309, 27388, 807, 341, 16575, 295, 264, 4571, 293, 2539, 1399, 13, 400, 264, 11462, 935, 11, 50952], "temperature": 0.0, "avg_logprob": -0.09413464525912671, "compression_ratio": 1.7623318385650224, "no_speech_prob": 0.006354630459100008}, {"id": 314, "seek": 171712, "start": 1728.8799999999999, "end": 1735.6799999999998, "text": " as we'll see, is that there is a bottleneck. So the agent has a language, the teacher, they teach", "tokens": [50952, 382, 321, 603, 536, 11, 307, 300, 456, 307, 257, 44641, 547, 13, 407, 264, 9461, 575, 257, 2856, 11, 264, 5027, 11, 436, 2924, 51292], "temperature": 0.0, "avg_logprob": -0.09413464525912671, "compression_ratio": 1.7623318385650224, "no_speech_prob": 0.006354630459100008}, {"id": 315, "seek": 171712, "start": 1735.6799999999998, "end": 1744.0, "text": " only a number of exemplars to the learner, to the pupil. And then the pupil in turn, the pupil", "tokens": [51292, 787, 257, 1230, 295, 24112, 685, 281, 264, 33347, 11, 281, 264, 44533, 13, 400, 550, 264, 44533, 294, 1261, 11, 264, 44533, 51708], "temperature": 0.0, "avg_logprob": -0.09413464525912671, "compression_ratio": 1.7623318385650224, "no_speech_prob": 0.006354630459100008}, {"id": 316, "seek": 174400, "start": 1744.0, "end": 1749.76, "text": " becomes the teacher, the pupil is teaching the next learner and they have to extrapolate from the,", "tokens": [50364, 3643, 264, 5027, 11, 264, 44533, 307, 4571, 264, 958, 33347, 293, 436, 362, 281, 48224, 473, 490, 264, 11, 50652], "temperature": 0.0, "avg_logprob": -0.10370097317538418, "compression_ratio": 1.827906976744186, "no_speech_prob": 0.025251563638448715}, {"id": 317, "seek": 174400, "start": 1749.76, "end": 1754.72, "text": " well, they've extrapolated from the few examples, exemplars they've been taught, a whole language,", "tokens": [50652, 731, 11, 436, 600, 48224, 770, 490, 264, 1326, 5110, 11, 24112, 685, 436, 600, 668, 5928, 11, 257, 1379, 2856, 11, 50900], "temperature": 0.0, "avg_logprob": -0.10370097317538418, "compression_ratio": 1.827906976744186, "no_speech_prob": 0.025251563638448715}, {"id": 318, "seek": 174400, "start": 1754.72, "end": 1760.56, "text": " and then they choose other exemplars from that language to teach the next pupil. And it's this", "tokens": [50900, 293, 550, 436, 2826, 661, 24112, 685, 490, 300, 2856, 281, 2924, 264, 958, 44533, 13, 400, 309, 311, 341, 51192], "temperature": 0.0, "avg_logprob": -0.10370097317538418, "compression_ratio": 1.827906976744186, "no_speech_prob": 0.025251563638448715}, {"id": 319, "seek": 174400, "start": 1760.56, "end": 1767.44, "text": " process of bottleneck and extrapolation from the bottleneck that Simon Kirby hoped and in fact found", "tokens": [51192, 1399, 295, 44641, 547, 293, 48224, 399, 490, 264, 44641, 547, 300, 13193, 37423, 19737, 293, 294, 1186, 1352, 51536], "temperature": 0.0, "avg_logprob": -0.10370097317538418, "compression_ratio": 1.827906976744186, "no_speech_prob": 0.025251563638448715}, {"id": 320, "seek": 176744, "start": 1767.44, "end": 1774.16, "text": " did produce some of the properties that we believe languages should have. And so again,", "tokens": [50364, 630, 5258, 512, 295, 264, 7221, 300, 321, 1697, 8650, 820, 362, 13, 400, 370, 797, 11, 50700], "temperature": 0.0, "avg_logprob": -0.1381694067092169, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.20669378340244293}, {"id": 321, "seek": 176744, "start": 1774.16, "end": 1779.2, "text": " this just summarizes it. The teacher provides signals and meanings. So they say, you know,", "tokens": [50700, 341, 445, 14611, 5660, 309, 13, 440, 5027, 6417, 12354, 293, 28138, 13, 407, 436, 584, 11, 291, 458, 11, 50952], "temperature": 0.0, "avg_logprob": -0.1381694067092169, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.20669378340244293}, {"id": 322, "seek": 176744, "start": 1779.2, "end": 1785.2, "text": " cat and then shows a cat, just like in cartoons. The cartoon idea about how we teach children,", "tokens": [50952, 3857, 293, 550, 3110, 257, 3857, 11, 445, 411, 294, 34855, 13, 440, 18569, 1558, 466, 577, 321, 2924, 2227, 11, 51252], "temperature": 0.0, "avg_logprob": -0.1381694067092169, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.20669378340244293}, {"id": 323, "seek": 176744, "start": 1785.2, "end": 1788.8, "text": " although people who work on children point out that that's not actually what happens,", "tokens": [51252, 4878, 561, 567, 589, 322, 2227, 935, 484, 300, 300, 311, 406, 767, 437, 2314, 11, 51432], "temperature": 0.0, "avg_logprob": -0.1381694067092169, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.20669378340244293}, {"id": 324, "seek": 176744, "start": 1788.8, "end": 1795.76, "text": " that we very rarely teach children language in this supervised way. But here, we do have this", "tokens": [51432, 300, 321, 588, 13752, 2924, 2227, 2856, 294, 341, 46533, 636, 13, 583, 510, 11, 321, 360, 362, 341, 51780], "temperature": 0.0, "avg_logprob": -0.1381694067092169, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.20669378340244293}, {"id": 325, "seek": 179576, "start": 1795.76, "end": 1801.12, "text": " naive picture. The teacher provides signals and meanings. The learner learns the mapping from signals", "tokens": [50364, 29052, 3036, 13, 440, 5027, 6417, 12354, 293, 28138, 13, 440, 33347, 27152, 264, 18350, 490, 12354, 50632], "temperature": 0.0, "avg_logprob": -0.15951946648684415, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.009164153598248959}, {"id": 326, "seek": 179576, "start": 1801.12, "end": 1806.16, "text": " to meanings. And then when they reach maturity, they use that to, they then use a version,", "tokens": [50632, 281, 28138, 13, 400, 550, 562, 436, 2524, 28874, 11, 436, 764, 300, 281, 11, 436, 550, 764, 257, 3037, 11, 50884], "temperature": 0.0, "avg_logprob": -0.15951946648684415, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.009164153598248959}, {"id": 327, "seek": 179576, "start": 1806.16, "end": 1809.92, "text": " which I'm going to talk about in a minute, to invert that map. So they get a map from", "tokens": [50884, 597, 286, 478, 516, 281, 751, 466, 294, 257, 3456, 11, 281, 33966, 300, 4471, 13, 407, 436, 483, 257, 4471, 490, 51072], "temperature": 0.0, "avg_logprob": -0.15951946648684415, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.009164153598248959}, {"id": 328, "seek": 179576, "start": 1809.92, "end": 1815.68, "text": " meanings back to signals. And then they choose some random meanings, produce the signals for,", "tokens": [51072, 28138, 646, 281, 12354, 13, 400, 550, 436, 2826, 512, 4974, 28138, 11, 5258, 264, 12354, 337, 11, 51360], "temperature": 0.0, "avg_logprob": -0.15951946648684415, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.009164153598248959}, {"id": 329, "seek": 181568, "start": 1816.64, "end": 1823.76, "text": " as an example, to the new learner. And the process continues. And", "tokens": [50412, 382, 364, 1365, 11, 281, 264, 777, 33347, 13, 400, 264, 1399, 6515, 13, 400, 50768], "temperature": 0.0, "avg_logprob": -0.16536396305735518, "compression_ratio": 1.6157407407407407, "no_speech_prob": 0.010212871246039867}, {"id": 330, "seek": 181568, "start": 1825.2, "end": 1829.8400000000001, "text": " the idea is that the language that's produced should have some of these nice properties. So", "tokens": [50840, 264, 1558, 307, 300, 264, 2856, 300, 311, 7126, 820, 362, 512, 295, 613, 1481, 7221, 13, 407, 51072], "temperature": 0.0, "avg_logprob": -0.16536396305735518, "compression_ratio": 1.6157407407407407, "no_speech_prob": 0.010212871246039867}, {"id": 331, "seek": 181568, "start": 1829.8400000000001, "end": 1836.64, "text": " the properties they have that we're to look for are expressivity, stability and compositionality.", "tokens": [51072, 264, 7221, 436, 362, 300, 321, 434, 281, 574, 337, 366, 5109, 4253, 11, 11826, 293, 12686, 1860, 13, 51412], "temperature": 0.0, "avg_logprob": -0.16536396305735518, "compression_ratio": 1.6157407407407407, "no_speech_prob": 0.010212871246039867}, {"id": 332, "seek": 181568, "start": 1836.64, "end": 1843.68, "text": " Expressivity is basically, can all meanings be expressed? In other words, if you map signals,", "tokens": [51412, 20212, 4253, 307, 1936, 11, 393, 439, 28138, 312, 12675, 30, 682, 661, 2283, 11, 498, 291, 4471, 12354, 11, 51764], "temperature": 0.0, "avg_logprob": -0.16536396305735518, "compression_ratio": 1.6157407407407407, "no_speech_prob": 0.010212871246039867}, {"id": 333, "seek": 184368, "start": 1844.64, "end": 1848.88, "text": " set of signals onto the space of meanings, how onto is that map? If a completely expressive", "tokens": [50412, 992, 295, 12354, 3911, 264, 1901, 295, 28138, 11, 577, 3911, 307, 300, 4471, 30, 759, 257, 2584, 40189, 50624], "temperature": 0.0, "avg_logprob": -0.15317053543893913, "compression_ratio": 1.9707112970711298, "no_speech_prob": 0.0031447794754058123}, {"id": 334, "seek": 184368, "start": 1848.88, "end": 1854.24, "text": " language is one where the map from signals to meanings is onto, if the signals all map to the", "tokens": [50624, 2856, 307, 472, 689, 264, 4471, 490, 12354, 281, 28138, 307, 3911, 11, 498, 264, 12354, 439, 4471, 281, 264, 50892], "temperature": 0.0, "avg_logprob": -0.15317053543893913, "compression_ratio": 1.9707112970711298, "no_speech_prob": 0.0031447794754058123}, {"id": 335, "seek": 184368, "start": 1854.24, "end": 1859.3600000000001, "text": " same meaning, that would be not at all expressive. And expressivity is just a sort of counting of,", "tokens": [50892, 912, 3620, 11, 300, 576, 312, 406, 412, 439, 40189, 13, 400, 5109, 4253, 307, 445, 257, 1333, 295, 13251, 295, 11, 51148], "temperature": 0.0, "avg_logprob": -0.15317053543893913, "compression_ratio": 1.9707112970711298, "no_speech_prob": 0.0031447794754058123}, {"id": 336, "seek": 184368, "start": 1859.3600000000001, "end": 1864.0800000000002, "text": " I mean, basically it's a counting of the map of the signal space into the, into the meaning space", "tokens": [51148, 286, 914, 11, 1936, 309, 311, 257, 13251, 295, 264, 4471, 295, 264, 6358, 1901, 666, 264, 11, 666, 264, 3620, 1901, 51384], "temperature": 0.0, "avg_logprob": -0.15317053543893913, "compression_ratio": 1.9707112970711298, "no_speech_prob": 0.0031447794754058123}, {"id": 337, "seek": 184368, "start": 1864.0800000000002, "end": 1869.6000000000001, "text": " divided by the size of the meaning space. Stability, that's just how after the languages", "tokens": [51384, 6666, 538, 264, 2744, 295, 264, 3620, 1901, 13, 745, 2310, 11, 300, 311, 445, 577, 934, 264, 8650, 51660], "temperature": 0.0, "avg_logprob": -0.15317053543893913, "compression_ratio": 1.9707112970711298, "no_speech_prob": 0.0031447794754058123}, {"id": 338, "seek": 186960, "start": 1870.1599999999999, "end": 1875.76, "text": " have time to mature, is it roughly stable from, from iteration from generation to generation.", "tokens": [50392, 362, 565, 281, 14442, 11, 307, 309, 9810, 8351, 490, 11, 490, 24784, 490, 5125, 281, 5125, 13, 50672], "temperature": 0.0, "avg_logprob": -0.14066621109291358, "compression_ratio": 1.739463601532567, "no_speech_prob": 0.004040692467242479}, {"id": 339, "seek": 186960, "start": 1875.76, "end": 1881.4399999999998, "text": " And then compositionality, of course, is the sort of more difficult one. That's what makes", "tokens": [50672, 400, 550, 12686, 1860, 11, 295, 1164, 11, 307, 264, 1333, 295, 544, 2252, 472, 13, 663, 311, 437, 1669, 50956], "temperature": 0.0, "avg_logprob": -0.14066621109291358, "compression_ratio": 1.739463601532567, "no_speech_prob": 0.004040692467242479}, {"id": 340, "seek": 186960, "start": 1881.4399999999998, "end": 1886.9599999999998, "text": " languages languages, what makes a language a language, the idea that a part of the signal", "tokens": [50956, 8650, 8650, 11, 437, 1669, 257, 2856, 257, 2856, 11, 264, 1558, 300, 257, 644, 295, 264, 6358, 51232], "temperature": 0.0, "avg_logprob": -0.14066621109291358, "compression_ratio": 1.739463601532567, "no_speech_prob": 0.004040692467242479}, {"id": 341, "seek": 186960, "start": 1886.9599999999998, "end": 1891.9199999999998, "text": " should consistently code for some aspect of meaning. So, you know, in this case here,", "tokens": [51232, 820, 14961, 3089, 337, 512, 4171, 295, 3620, 13, 407, 11, 291, 458, 11, 294, 341, 1389, 510, 11, 51480], "temperature": 0.0, "avg_logprob": -0.14066621109291358, "compression_ratio": 1.739463601532567, "no_speech_prob": 0.004040692467242479}, {"id": 342, "seek": 186960, "start": 1891.9199999999998, "end": 1898.8, "text": " we have the word for orange. I think funny to make this as an example when I was making these", "tokens": [51480, 321, 362, 264, 1349, 337, 7671, 13, 286, 519, 4074, 281, 652, 341, 382, 364, 1365, 562, 286, 390, 1455, 613, 51824], "temperature": 0.0, "avg_logprob": -0.14066621109291358, "compression_ratio": 1.739463601532567, "no_speech_prob": 0.004040692467242479}, {"id": 343, "seek": 189880, "start": 1898.8, "end": 1903.76, "text": " slides, but of course it's the worst possible example. So, so the idea is we have a word for", "tokens": [50364, 9788, 11, 457, 295, 1164, 309, 311, 264, 5855, 1944, 1365, 13, 407, 11, 370, 264, 1558, 307, 321, 362, 257, 1349, 337, 50612], "temperature": 0.0, "avg_logprob": -0.1459886016845703, "compression_ratio": 1.9510204081632654, "no_speech_prob": 0.006840958725661039}, {"id": 344, "seek": 189880, "start": 1903.76, "end": 1908.72, "text": " orange, the color. You can see my problem, I'm really screwed myself, but yeah, we have a word", "tokens": [50612, 7671, 11, 264, 2017, 13, 509, 393, 536, 452, 1154, 11, 286, 478, 534, 20331, 2059, 11, 457, 1338, 11, 321, 362, 257, 1349, 50860], "temperature": 0.0, "avg_logprob": -0.1459886016845703, "compression_ratio": 1.9510204081632654, "no_speech_prob": 0.006840958725661039}, {"id": 345, "seek": 189880, "start": 1908.72, "end": 1913.28, "text": " for orange, the color, and it's used here to describe orange, the fruit, and then we have blue", "tokens": [50860, 337, 7671, 11, 264, 2017, 11, 293, 309, 311, 1143, 510, 281, 6786, 7671, 11, 264, 6773, 11, 293, 550, 321, 362, 3344, 51088], "temperature": 0.0, "avg_logprob": -0.1459886016845703, "compression_ratio": 1.9510204081632654, "no_speech_prob": 0.006840958725661039}, {"id": 346, "seek": 189880, "start": 1914.0, "end": 1920.72, "text": " used to describe the blueness of the orange on the right. And the idea of compositionality is that", "tokens": [51124, 1143, 281, 6786, 264, 888, 7801, 442, 295, 264, 7671, 322, 264, 558, 13, 400, 264, 1558, 295, 12686, 1860, 307, 300, 51460], "temperature": 0.0, "avg_logprob": -0.1459886016845703, "compression_ratio": 1.9510204081632654, "no_speech_prob": 0.006840958725661039}, {"id": 347, "seek": 189880, "start": 1920.72, "end": 1926.8799999999999, "text": " the word orange, it means the color orange when referring to the color of an orange, or it means", "tokens": [51460, 264, 1349, 7671, 11, 309, 1355, 264, 2017, 7671, 562, 13761, 281, 264, 2017, 295, 364, 7671, 11, 420, 309, 1355, 51768], "temperature": 0.0, "avg_logprob": -0.1459886016845703, "compression_ratio": 1.9510204081632654, "no_speech_prob": 0.006840958725661039}, {"id": 348, "seek": 192688, "start": 1926.88, "end": 1931.1200000000001, "text": " orange when referring to the color of a high-vis jacket. And one of the things, you know, the", "tokens": [50364, 7671, 562, 13761, 281, 264, 2017, 295, 257, 1090, 12, 4938, 11781, 13, 400, 472, 295, 264, 721, 11, 291, 458, 11, 264, 50576], "temperature": 0.0, "avg_logprob": -0.11430307856777258, "compression_ratio": 1.8022388059701493, "no_speech_prob": 0.0038728213403373957}, {"id": 349, "seek": 192688, "start": 1931.1200000000001, "end": 1937.1200000000001, "text": " people have learned in trying to make agent models of language evolution is that compositionality is", "tokens": [50576, 561, 362, 3264, 294, 1382, 281, 652, 9461, 5245, 295, 2856, 9303, 307, 300, 12686, 1860, 307, 50876], "temperature": 0.0, "avg_logprob": -0.11430307856777258, "compression_ratio": 1.8022388059701493, "no_speech_prob": 0.0038728213403373957}, {"id": 350, "seek": 192688, "start": 1937.1200000000001, "end": 1942.5600000000002, "text": " actually quite hard to enforce. So, you know, if you have two reinforcement learning agents and", "tokens": [50876, 767, 1596, 1152, 281, 24825, 13, 407, 11, 291, 458, 11, 498, 291, 362, 732, 29280, 2539, 12554, 293, 51148], "temperature": 0.0, "avg_logprob": -0.11430307856777258, "compression_ratio": 1.8022388059701493, "no_speech_prob": 0.0038728213403373957}, {"id": 351, "seek": 192688, "start": 1942.5600000000002, "end": 1947.2, "text": " they're playing what they call the Lewis signaling game, they're trying to learn a way of telling", "tokens": [51148, 436, 434, 2433, 437, 436, 818, 264, 17412, 38639, 1216, 11, 436, 434, 1382, 281, 1466, 257, 636, 295, 3585, 51380], "temperature": 0.0, "avg_logprob": -0.11430307856777258, "compression_ratio": 1.8022388059701493, "no_speech_prob": 0.0038728213403373957}, {"id": 352, "seek": 192688, "start": 1947.2, "end": 1952.88, "text": " each other about things, what tends to happen is that, you know, if you don't, if you restrict", "tokens": [51380, 1184, 661, 466, 721, 11, 437, 12258, 281, 1051, 307, 300, 11, 291, 458, 11, 498, 291, 500, 380, 11, 498, 291, 7694, 51664], "temperature": 0.0, "avg_logprob": -0.11430307856777258, "compression_ratio": 1.8022388059701493, "no_speech_prob": 0.0038728213403373957}, {"id": 353, "seek": 195288, "start": 1952.96, "end": 1957.1200000000001, "text": " their signal space, so they just don't have a separate word for every possible combination of", "tokens": [50368, 641, 6358, 1901, 11, 370, 436, 445, 500, 380, 362, 257, 4994, 1349, 337, 633, 1944, 6562, 295, 50576], "temperature": 0.0, "avg_logprob": -0.1030892481846092, "compression_ratio": 1.963265306122449, "no_speech_prob": 0.04440084099769592}, {"id": 354, "seek": 195288, "start": 1957.1200000000001, "end": 1963.44, "text": " attribute and object, et cetera, but rather they're forced to have some symbol for attribution, some", "tokens": [50576, 19667, 293, 2657, 11, 1030, 11458, 11, 457, 2831, 436, 434, 7579, 281, 362, 512, 5986, 337, 9080, 1448, 11, 512, 50892], "temperature": 0.0, "avg_logprob": -0.1030892481846092, "compression_ratio": 1.963265306122449, "no_speech_prob": 0.04440084099769592}, {"id": 355, "seek": 195288, "start": 1963.44, "end": 1968.0, "text": " symbol for what's being described, they have to have a word for the color and a word for the", "tokens": [50892, 5986, 337, 437, 311, 885, 7619, 11, 436, 362, 281, 362, 257, 1349, 337, 264, 2017, 293, 257, 1349, 337, 264, 51120], "temperature": 0.0, "avg_logprob": -0.1030892481846092, "compression_ratio": 1.963265306122449, "no_speech_prob": 0.04440084099769592}, {"id": 356, "seek": 195288, "start": 1968.0, "end": 1974.72, "text": " object. You can do that, but they don't consistently use the same color word. You know, they might use", "tokens": [51120, 2657, 13, 509, 393, 360, 300, 11, 457, 436, 500, 380, 14961, 764, 264, 912, 2017, 1349, 13, 509, 458, 11, 436, 1062, 764, 51456], "temperature": 0.0, "avg_logprob": -0.1030892481846092, "compression_ratio": 1.963265306122449, "no_speech_prob": 0.04440084099769592}, {"id": 357, "seek": 195288, "start": 1974.72, "end": 1978.3200000000002, "text": " orange to mean orange when describing oranges, but they might use blue to mean orange when", "tokens": [51456, 7671, 281, 914, 7671, 562, 16141, 35474, 11, 457, 436, 1062, 764, 3344, 281, 914, 7671, 562, 51636], "temperature": 0.0, "avg_logprob": -0.1030892481846092, "compression_ratio": 1.963265306122449, "no_speech_prob": 0.04440084099769592}, {"id": 358, "seek": 197832, "start": 1978.8799999999999, "end": 1985.6799999999998, "text": " describing high-vis jackets. It's enough for the communication between agents, you know,", "tokens": [50392, 16141, 1090, 12, 4938, 34612, 13, 467, 311, 1547, 337, 264, 6101, 1296, 12554, 11, 291, 458, 11, 50732], "temperature": 0.0, "avg_logprob": -0.1351348646394499, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.00425250269472599}, {"id": 359, "seek": 197832, "start": 1985.6799999999998, "end": 1992.8799999999999, "text": " reinforcement learning agents, that there is a word to distinguish potential colors of the", "tokens": [50732, 29280, 2539, 12554, 11, 300, 456, 307, 257, 1349, 281, 20206, 3995, 4577, 295, 264, 51092], "temperature": 0.0, "avg_logprob": -0.1351348646394499, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.00425250269472599}, {"id": 360, "seek": 197832, "start": 1992.8799999999999, "end": 2000.08, "text": " orange fruit. But that word, the word for orange, doesn't have to be the same word. It doesn't have", "tokens": [51092, 7671, 6773, 13, 583, 300, 1349, 11, 264, 1349, 337, 7671, 11, 1177, 380, 362, 281, 312, 264, 912, 1349, 13, 467, 1177, 380, 362, 51452], "temperature": 0.0, "avg_logprob": -0.1351348646394499, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.00425250269472599}, {"id": 361, "seek": 197832, "start": 2000.08, "end": 2005.2, "text": " to match to the same color as when you're using it to describe something else. You know, we do this", "tokens": [51452, 281, 2995, 281, 264, 912, 2017, 382, 562, 291, 434, 1228, 309, 281, 6786, 746, 1646, 13, 509, 458, 11, 321, 360, 341, 51708], "temperature": 0.0, "avg_logprob": -0.1351348646394499, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.00425250269472599}, {"id": 362, "seek": 200520, "start": 2005.2, "end": 2012.48, "text": " a little bit ourselves. I mean, you know, when we're talking about horses, we use the word chestnut", "tokens": [50364, 257, 707, 857, 4175, 13, 286, 914, 11, 291, 458, 11, 562, 321, 434, 1417, 466, 13112, 11, 321, 764, 264, 1349, 7443, 18316, 50728], "temperature": 0.0, "avg_logprob": -0.13969853494019635, "compression_ratio": 1.7664233576642336, "no_speech_prob": 0.0023556286469101906}, {"id": 363, "seek": 200520, "start": 2013.68, "end": 2019.2, "text": " for what we would call brown in other instances. But generally speaking, the main property of", "tokens": [50788, 337, 437, 321, 576, 818, 6292, 294, 661, 14519, 13, 583, 5101, 4124, 11, 264, 2135, 4707, 295, 51064], "temperature": 0.0, "avg_logprob": -0.13969853494019635, "compression_ratio": 1.7664233576642336, "no_speech_prob": 0.0023556286469101906}, {"id": 364, "seek": 200520, "start": 2019.2, "end": 2024.4, "text": " language is compositionality, and the idea is to seek that here. So in this version, in the simple", "tokens": [51064, 2856, 307, 12686, 1860, 11, 293, 264, 1558, 307, 281, 8075, 300, 510, 13, 407, 294, 341, 3037, 11, 294, 264, 2199, 51324], "temperature": 0.0, "avg_logprob": -0.13969853494019635, "compression_ratio": 1.7664233576642336, "no_speech_prob": 0.0023556286469101906}, {"id": 365, "seek": 200520, "start": 2024.4, "end": 2028.56, "text": " version of the iterated language model, we just have eight bits of meaning and eight bits of signal.", "tokens": [51324, 3037, 295, 264, 17138, 770, 2856, 2316, 11, 321, 445, 362, 3180, 9239, 295, 3620, 293, 3180, 9239, 295, 6358, 13, 51532], "temperature": 0.0, "avg_logprob": -0.13969853494019635, "compression_ratio": 1.7664233576642336, "no_speech_prob": 0.0023556286469101906}, {"id": 366, "seek": 200520, "start": 2029.2, "end": 2034.96, "text": " So the meaning space is just 256 potential meanings, and the signal space is 256 potential", "tokens": [51564, 407, 264, 3620, 1901, 307, 445, 38882, 3995, 28138, 11, 293, 264, 6358, 1901, 307, 38882, 3995, 51852], "temperature": 0.0, "avg_logprob": -0.13969853494019635, "compression_ratio": 1.7664233576642336, "no_speech_prob": 0.0023556286469101906}, {"id": 367, "seek": 203520, "start": 2035.44, "end": 2041.8400000000001, "text": " signals. And then the learner has a simple neural network. You can see that this all dates back to", "tokens": [50376, 12354, 13, 400, 550, 264, 33347, 575, 257, 2199, 18161, 3209, 13, 509, 393, 536, 300, 341, 439, 11691, 646, 281, 50696], "temperature": 0.0, "avg_logprob": -0.18321890472083963, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.001929706777445972}, {"id": 368, "seek": 203520, "start": 2041.8400000000001, "end": 2048.8, "text": " the year 2000. So it's a two-layer network. Signals come in. So the teacher says a signal", "tokens": [50696, 264, 1064, 8132, 13, 407, 309, 311, 257, 732, 12, 8376, 260, 3209, 13, 13515, 1124, 808, 294, 13, 407, 264, 5027, 1619, 257, 6358, 51044], "temperature": 0.0, "avg_logprob": -0.18321890472083963, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.001929706777445972}, {"id": 369, "seek": 203520, "start": 2049.44, "end": 2056.56, "text": " and then provides a meaning. And then the learner, perhaps using a sigmoid", "tokens": [51076, 293, 550, 6417, 257, 3620, 13, 400, 550, 264, 33347, 11, 4317, 1228, 257, 4556, 3280, 327, 51432], "temperature": 0.0, "avg_logprob": -0.18321890472083963, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.001929706777445972}, {"id": 370, "seek": 203520, "start": 2057.12, "end": 2062.7200000000003, "text": " non-linearity. So probability of ones and zeros for all the potential meanings compares it to the", "tokens": [51460, 2107, 12, 1889, 17409, 13, 407, 8482, 295, 2306, 293, 35193, 337, 439, 264, 3995, 28138, 38334, 309, 281, 264, 51740], "temperature": 0.0, "avg_logprob": -0.18321890472083963, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.001929706777445972}, {"id": 371, "seek": 206272, "start": 2062.72, "end": 2070.3199999999997, "text": " actual meaning backpropagates and thus learns to map from signals to meaning. So that's the plot.", "tokens": [50364, 3539, 3620, 646, 79, 1513, 559, 1024, 293, 8807, 27152, 281, 4471, 490, 12354, 281, 3620, 13, 407, 300, 311, 264, 7542, 13, 50744], "temperature": 0.0, "avg_logprob": -0.09799392327018407, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.002742999931797385}, {"id": 372, "seek": 206272, "start": 2070.3199999999997, "end": 2076.3199999999997, "text": " And of course, the thing about this is that this neural net has been trained on only these", "tokens": [50744, 400, 295, 1164, 11, 264, 551, 466, 341, 307, 300, 341, 18161, 2533, 575, 668, 8895, 322, 787, 613, 51044], "temperature": 0.0, "avg_logprob": -0.09799392327018407, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.002742999931797385}, {"id": 373, "seek": 206272, "start": 2076.3199999999997, "end": 2083.6, "text": " exemplars, the small part of the space of meanings that form the teaching event from the teacher to", "tokens": [51044, 24112, 685, 11, 264, 1359, 644, 295, 264, 1901, 295, 28138, 300, 1254, 264, 4571, 2280, 490, 264, 5027, 281, 51408], "temperature": 0.0, "avg_logprob": -0.09799392327018407, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.002742999931797385}, {"id": 374, "seek": 206272, "start": 2083.6, "end": 2089.6, "text": " the learner. But the mechanism, of course, provides a map from any signal to a meaning.", "tokens": [51408, 264, 33347, 13, 583, 264, 7513, 11, 295, 1164, 11, 6417, 257, 4471, 490, 604, 6358, 281, 257, 3620, 13, 51708], "temperature": 0.0, "avg_logprob": -0.09799392327018407, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.002742999931797385}, {"id": 375, "seek": 208960, "start": 2090.4, "end": 2095.44, "text": " So that's the first part of the teaching. But then the next part, which is that the", "tokens": [50404, 407, 300, 311, 264, 700, 644, 295, 264, 4571, 13, 583, 550, 264, 958, 644, 11, 597, 307, 300, 264, 50656], "temperature": 0.0, "avg_logprob": -0.08575008312861125, "compression_ratio": 1.5280898876404494, "no_speech_prob": 0.0005601020529866219}, {"id": 376, "seek": 208960, "start": 2096.72, "end": 2103.68, "text": " learner has to also get a map from meanings to signals. And so the way that's done is using a", "tokens": [50720, 33347, 575, 281, 611, 483, 257, 4471, 490, 28138, 281, 12354, 13, 400, 370, 264, 636, 300, 311, 1096, 307, 1228, 257, 51068], "temperature": 0.0, "avg_logprob": -0.08575008312861125, "compression_ratio": 1.5280898876404494, "no_speech_prob": 0.0005601020529866219}, {"id": 377, "seek": 208960, "start": 2103.68, "end": 2111.2, "text": " version without worrying about it too much. Here's a two-bit example. So here we have a signal", "tokens": [51068, 3037, 1553, 18788, 466, 309, 886, 709, 13, 1692, 311, 257, 732, 12, 5260, 1365, 13, 407, 510, 321, 362, 257, 6358, 51444], "temperature": 0.0, "avg_logprob": -0.08575008312861125, "compression_ratio": 1.5280898876404494, "no_speech_prob": 0.0005601020529866219}, {"id": 378, "seek": 211120, "start": 2111.2, "end": 2119.4399999999996, "text": " mapping to a meaning, using the neural net. So what the learner is learning is the correct", "tokens": [50364, 18350, 281, 257, 3620, 11, 1228, 264, 18161, 2533, 13, 407, 437, 264, 33347, 307, 2539, 307, 264, 3006, 50776], "temperature": 0.0, "avg_logprob": -0.08983354108879366, "compression_ratio": 1.8418367346938775, "no_speech_prob": 0.03182582929730415}, {"id": 379, "seek": 211120, "start": 2119.4399999999996, "end": 2123.2799999999997, "text": " mapping. So maybe the correct mapping here, the mapping that the teacher is trying to teach,", "tokens": [50776, 18350, 13, 407, 1310, 264, 3006, 18350, 510, 11, 264, 18350, 300, 264, 5027, 307, 1382, 281, 2924, 11, 50968], "temperature": 0.0, "avg_logprob": -0.08983354108879366, "compression_ratio": 1.8418367346938775, "no_speech_prob": 0.03182582929730415}, {"id": 380, "seek": 211120, "start": 2123.2799999999997, "end": 2132.3199999999997, "text": " is that one zero goes to one one. But literally speaking, the learner can provide from this", "tokens": [50968, 307, 300, 472, 4018, 1709, 281, 472, 472, 13, 583, 3736, 4124, 11, 264, 33347, 393, 2893, 490, 341, 51420], "temperature": 0.0, "avg_logprob": -0.08983354108879366, "compression_ratio": 1.8418367346938775, "no_speech_prob": 0.03182582929730415}, {"id": 381, "seek": 211120, "start": 2132.3199999999997, "end": 2138.64, "text": " mapping, coming from their neural net, a probability for all four potential meanings.", "tokens": [51420, 18350, 11, 1348, 490, 641, 18161, 2533, 11, 257, 8482, 337, 439, 1451, 3995, 28138, 13, 51736], "temperature": 0.0, "avg_logprob": -0.08983354108879366, "compression_ratio": 1.8418367346938775, "no_speech_prob": 0.03182582929730415}, {"id": 382, "seek": 213864, "start": 2138.72, "end": 2146.0, "text": " And this is a contrived example, so that the probabilities are 0.1, 0.1, 0.7, and 0.1, if you", "tokens": [50368, 400, 341, 307, 257, 660, 470, 937, 1365, 11, 370, 300, 264, 33783, 366, 1958, 13, 16, 11, 1958, 13, 16, 11, 1958, 13, 22, 11, 293, 1958, 13, 16, 11, 498, 291, 50732], "temperature": 0.0, "avg_logprob": -0.1415726071312314, "compression_ratio": 1.6238532110091743, "no_speech_prob": 0.0017194550018757582}, {"id": 383, "seek": 213864, "start": 2146.0, "end": 2152.8799999999997, "text": " obviously multiply P1 by P2, or 1 minus P2, or whatever. And so in that way, the learner can", "tokens": [50732, 2745, 12972, 430, 16, 538, 430, 17, 11, 420, 502, 3175, 430, 17, 11, 420, 2035, 13, 400, 370, 294, 300, 636, 11, 264, 33347, 393, 51076], "temperature": 0.0, "avg_logprob": -0.1415726071312314, "compression_ratio": 1.6238532110091743, "no_speech_prob": 0.0017194550018757582}, {"id": 384, "seek": 213864, "start": 2152.8799999999997, "end": 2162.8799999999997, "text": " produce a table of all possible maps from signal to meaning. So here are the signals,", "tokens": [51076, 5258, 257, 3199, 295, 439, 1944, 11317, 490, 6358, 281, 3620, 13, 407, 510, 366, 264, 12354, 11, 51576], "temperature": 0.0, "avg_logprob": -0.1415726071312314, "compression_ratio": 1.6238532110091743, "no_speech_prob": 0.0017194550018757582}, {"id": 385, "seek": 213864, "start": 2162.8799999999997, "end": 2166.96, "text": " and these are the probabilities that give meanings. And in a version, all you do,", "tokens": [51576, 293, 613, 366, 264, 33783, 300, 976, 28138, 13, 400, 294, 257, 3037, 11, 439, 291, 360, 11, 51780], "temperature": 0.0, "avg_logprob": -0.1415726071312314, "compression_ratio": 1.6238532110091743, "no_speech_prob": 0.0017194550018757582}, {"id": 386, "seek": 216696, "start": 2167.44, "end": 2174.48, "text": " you run the table the other way. So basically, if you're given one one one, so if you see the", "tokens": [50388, 291, 1190, 264, 3199, 264, 661, 636, 13, 407, 1936, 11, 498, 291, 434, 2212, 472, 472, 472, 11, 370, 498, 291, 536, 264, 50740], "temperature": 0.0, "avg_logprob": -0.12018257141113281, "compression_ratio": 1.7399103139013452, "no_speech_prob": 0.003266469808295369}, {"id": 387, "seek": 216696, "start": 2174.48, "end": 2179.12, "text": " signal one zero, your neural net tells you these are the probabilities of the different meanings.", "tokens": [50740, 6358, 472, 4018, 11, 428, 18161, 2533, 5112, 291, 613, 366, 264, 33783, 295, 264, 819, 28138, 13, 50972], "temperature": 0.0, "avg_logprob": -0.12018257141113281, "compression_ratio": 1.7399103139013452, "no_speech_prob": 0.003266469808295369}, {"id": 388, "seek": 216696, "start": 2179.12, "end": 2184.64, "text": " And to get a map from the other way from a meaning to a signal, you look across this way and see", "tokens": [50972, 400, 281, 483, 257, 4471, 490, 264, 661, 636, 490, 257, 3620, 281, 257, 6358, 11, 291, 574, 2108, 341, 636, 293, 536, 51248], "temperature": 0.0, "avg_logprob": -0.12018257141113281, "compression_ratio": 1.7399103139013452, "no_speech_prob": 0.003266469808295369}, {"id": 389, "seek": 216696, "start": 2184.64, "end": 2192.64, "text": " that 0.7 is the largest number. And then you decide that in your aversion, in your inverting of the", "tokens": [51248, 300, 1958, 13, 22, 307, 264, 6443, 1230, 13, 400, 550, 291, 4536, 300, 294, 428, 257, 29153, 11, 294, 428, 28653, 783, 295, 264, 51648], "temperature": 0.0, "avg_logprob": -0.12018257141113281, "compression_ratio": 1.7399103139013452, "no_speech_prob": 0.003266469808295369}, {"id": 390, "seek": 219264, "start": 2192.64, "end": 2198.08, "text": " map for meanings to signals, you'll map one one to one zero, because that's associated with the", "tokens": [50364, 4471, 337, 28138, 281, 12354, 11, 291, 603, 4471, 472, 472, 281, 472, 4018, 11, 570, 300, 311, 6615, 365, 264, 50636], "temperature": 0.0, "avg_logprob": -0.12501114670948316, "compression_ratio": 1.7031963470319635, "no_speech_prob": 0.004897561389952898}, {"id": 391, "seek": 219264, "start": 2198.08, "end": 2203.7599999999998, "text": " highest probability. So that's the process of aversion. You read across each of these lines,", "tokens": [50636, 6343, 8482, 13, 407, 300, 311, 264, 1399, 295, 257, 29153, 13, 509, 1401, 2108, 1184, 295, 613, 3876, 11, 50920], "temperature": 0.0, "avg_logprob": -0.12501114670948316, "compression_ratio": 1.7031963470319635, "no_speech_prob": 0.004897561389952898}, {"id": 392, "seek": 219264, "start": 2203.7599999999998, "end": 2209.2, "text": " you put a one there, and then this gives you the map. So zero zero will map to one one. So if the", "tokens": [50920, 291, 829, 257, 472, 456, 11, 293, 550, 341, 2709, 291, 264, 4471, 13, 407, 4018, 4018, 486, 4471, 281, 472, 472, 13, 407, 498, 264, 51192], "temperature": 0.0, "avg_logprob": -0.12501114670948316, "compression_ratio": 1.7031963470319635, "no_speech_prob": 0.004897561389952898}, {"id": 393, "seek": 219264, "start": 2209.2, "end": 2218.3199999999997, "text": " learner wants to randomly decides to teach the next learner the signal from zero zero,", "tokens": [51192, 33347, 2738, 281, 16979, 14898, 281, 2924, 264, 958, 33347, 264, 6358, 490, 4018, 4018, 11, 51648], "temperature": 0.0, "avg_logprob": -0.12501114670948316, "compression_ratio": 1.7031963470319635, "no_speech_prob": 0.004897561389952898}, {"id": 394, "seek": 221832, "start": 2218.32, "end": 2222.88, "text": " they'll say zero zero, sorry, they'll say one one, that's the signal, and they'll point out that", "tokens": [50364, 436, 603, 584, 4018, 4018, 11, 2597, 11, 436, 603, 584, 472, 472, 11, 300, 311, 264, 6358, 11, 293, 436, 603, 935, 484, 300, 50592], "temperature": 0.0, "avg_logprob": -0.08874065097015683, "compression_ratio": 1.8009049773755657, "no_speech_prob": 0.0018236110918223858}, {"id": 395, "seek": 221832, "start": 2222.88, "end": 2229.28, "text": " that corresponds to the meaning zero zero. So that's the that's the iterated language model. You can", "tokens": [50592, 300, 23249, 281, 264, 3620, 4018, 4018, 13, 407, 300, 311, 264, 300, 311, 264, 17138, 770, 2856, 2316, 13, 509, 393, 50912], "temperature": 0.0, "avg_logprob": -0.08874065097015683, "compression_ratio": 1.8009049773755657, "no_speech_prob": 0.0018236110918223858}, {"id": 396, "seek": 221832, "start": 2229.28, "end": 2233.84, "text": " see that this aspect of it is completely unrealistic and something that you might want to get rid of.", "tokens": [50912, 536, 300, 341, 4171, 295, 309, 307, 2584, 42867, 293, 746, 300, 291, 1062, 528, 281, 483, 3973, 295, 13, 51140], "temperature": 0.0, "avg_logprob": -0.08874065097015683, "compression_ratio": 1.8009049773755657, "no_speech_prob": 0.0018236110918223858}, {"id": 397, "seek": 221832, "start": 2234.88, "end": 2241.52, "text": " But it's the thing that seems to work. And what the reason, of course, it's unrealistic is that it", "tokens": [51192, 583, 309, 311, 264, 551, 300, 2544, 281, 589, 13, 400, 437, 264, 1778, 11, 295, 1164, 11, 309, 311, 42867, 307, 300, 309, 51524], "temperature": 0.0, "avg_logprob": -0.08874065097015683, "compression_ratio": 1.8009049773755657, "no_speech_prob": 0.0018236110918223858}, {"id": 398, "seek": 224152, "start": 2241.52, "end": 2250.0, "text": " requires that the learner on reaching maturity goes through all potential, all potential signals", "tokens": [50364, 7029, 300, 264, 33347, 322, 9906, 28874, 1709, 807, 439, 3995, 11, 439, 3995, 12354, 50788], "temperature": 0.0, "avg_logprob": -0.14121722066125206, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.009790276177227497}, {"id": 399, "seek": 224152, "start": 2250.0, "end": 2255.6, "text": " and all potential meanings, works out the corresponding probabilities, and then does this aversion map.", "tokens": [50788, 293, 439, 3995, 28138, 11, 1985, 484, 264, 11760, 33783, 11, 293, 550, 775, 341, 257, 29153, 4471, 13, 51068], "temperature": 0.0, "avg_logprob": -0.14121722066125206, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.009790276177227497}, {"id": 400, "seek": 224152, "start": 2255.6, "end": 2260.88, "text": " And that's obviously not true of language. The whole point of language is that you can't go through,", "tokens": [51068, 400, 300, 311, 2745, 406, 2074, 295, 2856, 13, 440, 1379, 935, 295, 2856, 307, 300, 291, 393, 380, 352, 807, 11, 51332], "temperature": 0.0, "avg_logprob": -0.14121722066125206, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.009790276177227497}, {"id": 401, "seek": 224152, "start": 2260.88, "end": 2265.28, "text": " you know, you don't have to go through all meanings and all signals. And even from a", "tokens": [51332, 291, 458, 11, 291, 500, 380, 362, 281, 352, 807, 439, 28138, 293, 439, 12354, 13, 400, 754, 490, 257, 51552], "temperature": 0.0, "avg_logprob": -0.14121722066125206, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.009790276177227497}, {"id": 402, "seek": 226528, "start": 2265.28, "end": 2271.84, "text": " computational point of view, it's extremely resource heavy. And even with modern computers,", "tokens": [50364, 28270, 935, 295, 1910, 11, 309, 311, 4664, 7684, 4676, 13, 400, 754, 365, 4363, 10807, 11, 50692], "temperature": 0.0, "avg_logprob": -0.08501638280283107, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.005843380466103554}, {"id": 403, "seek": 226528, "start": 2271.84, "end": 2276.2400000000002, "text": " it means that you couldn't. So the hope is that you could look at this iterated language model", "tokens": [50692, 309, 1355, 300, 291, 2809, 380, 13, 407, 264, 1454, 307, 300, 291, 727, 574, 412, 341, 17138, 770, 2856, 2316, 50912], "temperature": 0.0, "avg_logprob": -0.08501638280283107, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.005843380466103554}, {"id": 404, "seek": 226528, "start": 2276.2400000000002, "end": 2280.0800000000004, "text": " far beyond the sort of eight bit examples they're doing here, maybe even use it,", "tokens": [50912, 1400, 4399, 264, 1333, 295, 3180, 857, 5110, 436, 434, 884, 510, 11, 1310, 754, 764, 309, 11, 51104], "temperature": 0.0, "avg_logprob": -0.08501638280283107, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.005843380466103554}, {"id": 405, "seek": 226528, "start": 2280.0800000000004, "end": 2285.1200000000003, "text": " you know, on language itself. So replace the set of meanings with actual sentences and", "tokens": [51104, 291, 458, 11, 322, 2856, 2564, 13, 407, 7406, 264, 992, 295, 28138, 365, 3539, 16579, 293, 51356], "temperature": 0.0, "avg_logprob": -0.08501638280283107, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.005843380466103554}, {"id": 406, "seek": 226528, "start": 2285.1200000000003, "end": 2288.88, "text": " force the agents to come up with their own internal languages and see what happens.", "tokens": [51356, 3464, 264, 12554, 281, 808, 493, 365, 641, 1065, 6920, 8650, 293, 536, 437, 2314, 13, 51544], "temperature": 0.0, "avg_logprob": -0.08501638280283107, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.005843380466103554}, {"id": 407, "seek": 228888, "start": 2289.44, "end": 2298.1600000000003, "text": " But so anyway, with this aversion process, you do get a way of mapping. And so you can run the", "tokens": [50392, 583, 370, 4033, 11, 365, 341, 257, 29153, 1399, 11, 291, 360, 483, 257, 636, 295, 18350, 13, 400, 370, 291, 393, 1190, 264, 50828], "temperature": 0.0, "avg_logprob": -0.1314210330738741, "compression_ratio": 1.5158730158730158, "no_speech_prob": 0.005172487813979387}, {"id": 408, "seek": 228888, "start": 2298.1600000000003, "end": 2306.0, "text": " iterated language model. This is just us recapitulating what Kirby and co-workers saw 20 years ago.", "tokens": [50828, 17138, 770, 2856, 2316, 13, 639, 307, 445, 505, 20928, 270, 12162, 437, 37423, 293, 598, 12, 37101, 1866, 945, 924, 2057, 13, 51220], "temperature": 0.0, "avg_logprob": -0.1314210330738741, "compression_ratio": 1.5158730158730158, "no_speech_prob": 0.005172487813979387}, {"id": 409, "seek": 228888, "start": 2306.0, "end": 2312.1600000000003, "text": " You can see that if you do this, you know, with a suitable size of bottleneck, so 256 meanings,", "tokens": [51220, 509, 393, 536, 300, 498, 291, 360, 341, 11, 291, 458, 11, 365, 257, 12873, 2744, 295, 44641, 547, 11, 370, 38882, 28138, 11, 51528], "temperature": 0.0, "avg_logprob": -0.1314210330738741, "compression_ratio": 1.5158730158730158, "no_speech_prob": 0.005172487813979387}, {"id": 410, "seek": 228888, "start": 2312.1600000000003, "end": 2317.76, "text": " you allow 50 of them to be taught, you run it across generations, the expressivity goes up,", "tokens": [51528, 291, 2089, 2625, 295, 552, 281, 312, 5928, 11, 291, 1190, 309, 2108, 10593, 11, 264, 5109, 4253, 1709, 493, 11, 51808], "temperature": 0.0, "avg_logprob": -0.1314210330738741, "compression_ratio": 1.5158730158730158, "no_speech_prob": 0.005172487813979387}, {"id": 411, "seek": 231776, "start": 2317.84, "end": 2322.8, "text": " stability. So this is the instability, the opposite of stability, that goes down.", "tokens": [50368, 11826, 13, 407, 341, 307, 264, 34379, 11, 264, 6182, 295, 11826, 11, 300, 1709, 760, 13, 50616], "temperature": 0.0, "avg_logprob": -0.1895740650318287, "compression_ratio": 1.8380566801619433, "no_speech_prob": 0.0020633023232221603}, {"id": 412, "seek": 231776, "start": 2323.36, "end": 2330.88, "text": " And more remarkably still, the compositionality increases. So this I think is intriguing and", "tokens": [50644, 400, 544, 37381, 920, 11, 264, 12686, 1860, 8637, 13, 407, 341, 286, 519, 307, 32503, 293, 51020], "temperature": 0.0, "avg_logprob": -0.1895740650318287, "compression_ratio": 1.8380566801619433, "no_speech_prob": 0.0020633023232221603}, {"id": 413, "seek": 231776, "start": 2330.88, "end": 2335.1200000000003, "text": " worth sort of reminding us. Sorry, this is just some, you know, interview. There's two different", "tokens": [51020, 3163, 1333, 295, 27639, 505, 13, 4919, 11, 341, 307, 445, 512, 11, 291, 458, 11, 4049, 13, 821, 311, 732, 819, 51232], "temperature": 0.0, "avg_logprob": -0.1895740650318287, "compression_ratio": 1.8380566801619433, "no_speech_prob": 0.0020633023232221603}, {"id": 414, "seek": 231776, "start": 2335.1200000000003, "end": 2341.76, "text": " measures here going on of compositionality. But it's basically measure of positionality", "tokens": [51232, 8000, 510, 516, 322, 295, 12686, 1860, 13, 583, 309, 311, 1936, 3481, 295, 2535, 1860, 51564], "temperature": 0.0, "avg_logprob": -0.1895740650318287, "compression_ratio": 1.8380566801619433, "no_speech_prob": 0.0020633023232221603}, {"id": 415, "seek": 231776, "start": 2341.76, "end": 2346.88, "text": " based on entropy. But I mean, I do think it's remarkable. And I think it's worth us going back", "tokens": [51564, 2361, 322, 30867, 13, 583, 286, 914, 11, 286, 360, 519, 309, 311, 12802, 13, 400, 286, 519, 309, 311, 3163, 505, 516, 646, 51820], "temperature": 0.0, "avg_logprob": -0.1895740650318287, "compression_ratio": 1.8380566801619433, "no_speech_prob": 0.0020633023232221603}, {"id": 416, "seek": 234688, "start": 2346.88, "end": 2351.36, "text": " to the iterated language model and trying to see what it tells us about language evolution.", "tokens": [50364, 281, 264, 17138, 770, 2856, 2316, 293, 1382, 281, 536, 437, 309, 5112, 505, 466, 2856, 9303, 13, 50588], "temperature": 0.0, "avg_logprob": -0.0960177700076483, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.0014965110458433628}, {"id": 417, "seek": 234688, "start": 2352.48, "end": 2357.44, "text": " To get rid of the inverter, it turns out, you think it's going to be easy. So for example,", "tokens": [50644, 1407, 483, 3973, 295, 264, 47201, 11, 309, 4523, 484, 11, 291, 519, 309, 311, 516, 281, 312, 1858, 13, 407, 337, 1365, 11, 50892], "temperature": 0.0, "avg_logprob": -0.0960177700076483, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.0014965110458433628}, {"id": 418, "seek": 234688, "start": 2357.44, "end": 2362.56, "text": " you think that you can just add another, you know, in the learning process, you can think that the", "tokens": [50892, 291, 519, 300, 291, 393, 445, 909, 1071, 11, 291, 458, 11, 294, 264, 2539, 1399, 11, 291, 393, 519, 300, 264, 51148], "temperature": 0.0, "avg_logprob": -0.0960177700076483, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.0014965110458433628}, {"id": 419, "seek": 234688, "start": 2362.56, "end": 2368.2400000000002, "text": " learner could have, you know, a meaning and then also learn its inversion at the same time,", "tokens": [51148, 33347, 727, 362, 11, 291, 458, 11, 257, 3620, 293, 550, 611, 1466, 1080, 43576, 412, 264, 912, 565, 11, 51432], "temperature": 0.0, "avg_logprob": -0.0960177700076483, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.0014965110458433628}, {"id": 420, "seek": 234688, "start": 2368.2400000000002, "end": 2372.88, "text": " stick in as an objective function, getting the right meaning, but also recovering the original", "tokens": [51432, 2897, 294, 382, 364, 10024, 2445, 11, 1242, 264, 558, 3620, 11, 457, 611, 29180, 264, 3380, 51664], "temperature": 0.0, "avg_logprob": -0.0960177700076483, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.0014965110458433628}, {"id": 421, "seek": 237288, "start": 2372.88, "end": 2379.36, "text": " signal. That seems like it would almost certainly work. And I guess this is one of the sort of", "tokens": [50364, 6358, 13, 663, 2544, 411, 309, 576, 1920, 3297, 589, 13, 400, 286, 2041, 341, 307, 472, 295, 264, 1333, 295, 50688], "temperature": 0.0, "avg_logprob": -0.15210000584634503, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.0042745014652609825}, {"id": 422, "seek": 237288, "start": 2379.36, "end": 2384.96, "text": " few original results we have in this area. And it's a sort of a negative one, which is that", "tokens": [50688, 1326, 3380, 3542, 321, 362, 294, 341, 1859, 13, 400, 309, 311, 257, 1333, 295, 257, 3671, 472, 11, 597, 307, 300, 50968], "temperature": 0.0, "avg_logprob": -0.15210000584634503, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.0042745014652609825}, {"id": 423, "seek": 237288, "start": 2385.52, "end": 2391.12, "text": " it turns out that a version, although clunky, something that you hope might just be a convenient", "tokens": [50996, 309, 4523, 484, 300, 257, 3037, 11, 4878, 596, 25837, 11, 746, 300, 291, 1454, 1062, 445, 312, 257, 10851, 51276], "temperature": 0.0, "avg_logprob": -0.15210000584634503, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.0042745014652609825}, {"id": 424, "seek": 237288, "start": 2391.12, "end": 2398.8, "text": " way of doing the inversion is necessary. It creates some sort of pressure which drives", "tokens": [51276, 636, 295, 884, 264, 43576, 307, 4818, 13, 467, 7829, 512, 1333, 295, 3321, 597, 11754, 51660], "temperature": 0.0, "avg_logprob": -0.15210000584634503, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.0042745014652609825}, {"id": 425, "seek": 239880, "start": 2398.88, "end": 2405.52, "text": " apart the mapping and makes the mapping from signals to meanings onto and produces expressivity.", "tokens": [50368, 4936, 264, 18350, 293, 1669, 264, 18350, 490, 12354, 281, 28138, 3911, 293, 14725, 5109, 4253, 13, 50700], "temperature": 0.0, "avg_logprob": -0.1405818638978181, "compression_ratio": 1.81640625, "no_speech_prob": 0.011476614512503147}, {"id": 426, "seek": 239880, "start": 2405.52, "end": 2409.92, "text": " So if you just replace it with a recurrent neural network, well, what you might call a recurrent", "tokens": [50700, 407, 498, 291, 445, 7406, 309, 365, 257, 18680, 1753, 18161, 3209, 11, 731, 11, 437, 291, 1062, 818, 257, 18680, 1753, 50920], "temperature": 0.0, "avg_logprob": -0.1405818638978181, "compression_ratio": 1.81640625, "no_speech_prob": 0.011476614512503147}, {"id": 427, "seek": 239880, "start": 2409.92, "end": 2415.76, "text": " neural network, with this architecture here, you lose that expressivity. I think that it's,", "tokens": [50920, 18161, 3209, 11, 365, 341, 9482, 510, 11, 291, 3624, 300, 5109, 4253, 13, 286, 519, 300, 309, 311, 11, 51212], "temperature": 0.0, "avg_logprob": -0.1405818638978181, "compression_ratio": 1.81640625, "no_speech_prob": 0.011476614512503147}, {"id": 428, "seek": 239880, "start": 2416.7200000000003, "end": 2421.36, "text": " so what we're seeing basically is that the bottleneck, which forces the neural net to", "tokens": [51260, 370, 437, 321, 434, 2577, 1936, 307, 300, 264, 44641, 547, 11, 597, 5874, 264, 18161, 2533, 281, 51492], "temperature": 0.0, "avg_logprob": -0.1405818638978181, "compression_ratio": 1.81640625, "no_speech_prob": 0.011476614512503147}, {"id": 429, "seek": 239880, "start": 2421.36, "end": 2426.88, "text": " generalize or sorry, the agents to generalize is producing compositionality. But the aversion", "tokens": [51492, 2674, 1125, 420, 2597, 11, 264, 12554, 281, 2674, 1125, 307, 10501, 12686, 1860, 13, 583, 264, 257, 29153, 51768], "temperature": 0.0, "avg_logprob": -0.1405818638978181, "compression_ratio": 1.81640625, "no_speech_prob": 0.011476614512503147}, {"id": 430, "seek": 242688, "start": 2426.88, "end": 2433.12, "text": " is also required for expressivity. The bottleneck also tends to cause this pushing together of the", "tokens": [50364, 307, 611, 4739, 337, 5109, 4253, 13, 440, 44641, 547, 611, 12258, 281, 3082, 341, 7380, 1214, 295, 264, 50676], "temperature": 0.0, "avg_logprob": -0.08899606856624637, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0017702163895592093}, {"id": 431, "seek": 242688, "start": 2433.12, "end": 2437.84, "text": " mapping, which loses the impressiveness of the language. And so I think there's ways around", "tokens": [50676, 18350, 11, 597, 18293, 264, 6729, 8477, 295, 264, 2856, 13, 400, 370, 286, 519, 456, 311, 2098, 926, 50912], "temperature": 0.0, "avg_logprob": -0.08899606856624637, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0017702163895592093}, {"id": 432, "seek": 242688, "start": 2437.84, "end": 2441.12, "text": " that. I think, you know, maybe if you think the whole thing is a sort of Bayesian reconstruction", "tokens": [50912, 300, 13, 286, 519, 11, 291, 458, 11, 1310, 498, 291, 519, 264, 1379, 551, 307, 257, 1333, 295, 7840, 42434, 31565, 51076], "temperature": 0.0, "avg_logprob": -0.08899606856624637, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0017702163895592093}, {"id": 433, "seek": 242688, "start": 2441.12, "end": 2447.6800000000003, "text": " problem, you can come up with a new objective function, which has a sort of contrastive term,", "tokens": [51076, 1154, 11, 291, 393, 808, 493, 365, 257, 777, 10024, 2445, 11, 597, 575, 257, 1333, 295, 8712, 488, 1433, 11, 51404], "temperature": 0.0, "avg_logprob": -0.08899606856624637, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0017702163895592093}, {"id": 434, "seek": 242688, "start": 2447.6800000000003, "end": 2453.44, "text": " which forces it not only to get the right mapping from signals to meanings, but also punish it for", "tokens": [51404, 597, 5874, 309, 406, 787, 281, 483, 264, 558, 18350, 490, 12354, 281, 28138, 11, 457, 611, 9842, 309, 337, 51692], "temperature": 0.0, "avg_logprob": -0.08899606856624637, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0017702163895592093}, {"id": 435, "seek": 245344, "start": 2454.0, "end": 2459.52, "text": " other meanings being close in probability. When you started doing that, it hasn't worked yet.", "tokens": [50392, 661, 28138, 885, 1998, 294, 8482, 13, 1133, 291, 1409, 884, 300, 11, 309, 6132, 380, 2732, 1939, 13, 50668], "temperature": 0.0, "avg_logprob": -0.1888544342734597, "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.002534546423703432}, {"id": 436, "seek": 245344, "start": 2460.08, "end": 2463.28, "text": " It's still not producing the expressive map. It goes up for a while and then something happens.", "tokens": [50696, 467, 311, 920, 406, 10501, 264, 40189, 4471, 13, 467, 1709, 493, 337, 257, 1339, 293, 550, 746, 2314, 13, 50856], "temperature": 0.0, "avg_logprob": -0.1888544342734597, "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.002534546423703432}, {"id": 437, "seek": 245344, "start": 2463.92, "end": 2470.56, "text": " I'm sure we can sort that out. But the plan here, the hope, the conclusion is that there's", "tokens": [50888, 286, 478, 988, 321, 393, 1333, 300, 484, 13, 583, 264, 1393, 510, 11, 264, 1454, 11, 264, 10063, 307, 300, 456, 311, 51220], "temperature": 0.0, "avg_logprob": -0.1888544342734597, "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.002534546423703432}, {"id": 438, "seek": 245344, "start": 2471.84, "end": 2476.88, "text": " a lot going on in trying to understand how compositionality and expressivity kind of", "tokens": [51284, 257, 688, 516, 322, 294, 1382, 281, 1223, 577, 12686, 1860, 293, 5109, 4253, 733, 295, 51536], "temperature": 0.0, "avg_logprob": -0.1888544342734597, "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.002534546423703432}, {"id": 439, "seek": 247688, "start": 2476.88, "end": 2483.2000000000003, "text": " rise in these simple agent models. It would be nice to have a sort of working example first and", "tokens": [50364, 6272, 294, 613, 2199, 9461, 5245, 13, 467, 576, 312, 1481, 281, 362, 257, 1333, 295, 1364, 1365, 700, 293, 50680], "temperature": 0.0, "avg_logprob": -0.12688173763993857, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.035942304879426956}, {"id": 440, "seek": 247688, "start": 2483.2000000000003, "end": 2488.48, "text": " then try and decide, you know, could we generalize it to much larger, more difficult cases? But I", "tokens": [50680, 550, 853, 293, 4536, 11, 291, 458, 11, 727, 321, 2674, 1125, 309, 281, 709, 4833, 11, 544, 2252, 3331, 30, 583, 286, 50944], "temperature": 0.0, "avg_logprob": -0.12688173763993857, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.035942304879426956}, {"id": 441, "seek": 247688, "start": 2488.48, "end": 2492.96, "text": " think, you know, the problem is that, this slide is just to remind me to sort of announce the problem,", "tokens": [50944, 519, 11, 291, 458, 11, 264, 1154, 307, 300, 11, 341, 4137, 307, 445, 281, 4160, 385, 281, 1333, 295, 7478, 264, 1154, 11, 51168], "temperature": 0.0, "avg_logprob": -0.12688173763993857, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.035942304879426956}, {"id": 442, "seek": 247688, "start": 2492.96, "end": 2497.04, "text": " which is that, you know, with these models, you're starting to worry about how much you just", "tokens": [51168, 597, 307, 300, 11, 291, 458, 11, 365, 613, 5245, 11, 291, 434, 2891, 281, 3292, 466, 577, 709, 291, 445, 51372], "temperature": 0.0, "avg_logprob": -0.12688173763993857, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.035942304879426956}, {"id": 443, "seek": 247688, "start": 2497.04, "end": 2500.96, "text": " follow your own footsteps. You start putting things into the models to produce some sort of", "tokens": [51372, 1524, 428, 1065, 26883, 13, 509, 722, 3372, 721, 666, 264, 5245, 281, 5258, 512, 1333, 295, 51568], "temperature": 0.0, "avg_logprob": -0.12688173763993857, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.035942304879426956}, {"id": 444, "seek": 247688, "start": 2501.52, "end": 2506.48, "text": " behavior. And then you start to worry about exactly what we can compare to when we look,", "tokens": [51596, 5223, 13, 400, 550, 291, 722, 281, 3292, 466, 2293, 437, 321, 393, 6794, 281, 562, 321, 574, 11, 51844], "temperature": 0.0, "avg_logprob": -0.12688173763993857, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.035942304879426956}, {"id": 445, "seek": 250648, "start": 2506.48, "end": 2513.36, "text": " you know, how we compare the model to the actual behavior of languages, how easy it will be to", "tokens": [50364, 291, 458, 11, 577, 321, 6794, 264, 2316, 281, 264, 3539, 5223, 295, 8650, 11, 577, 1858, 309, 486, 312, 281, 50708], "temperature": 0.0, "avg_logprob": -0.11830398523918936, "compression_ratio": 1.8584905660377358, "no_speech_prob": 0.0007015234441496432}, {"id": 446, "seek": 250648, "start": 2514.2400000000002, "end": 2520.72, "text": " decide, you know, whether the model is a good one or not, and then work out from whether the model", "tokens": [50752, 4536, 11, 291, 458, 11, 1968, 264, 2316, 307, 257, 665, 472, 420, 406, 11, 293, 550, 589, 484, 490, 1968, 264, 2316, 51076], "temperature": 0.0, "avg_logprob": -0.11830398523918936, "compression_ratio": 1.8584905660377358, "no_speech_prob": 0.0007015234441496432}, {"id": 447, "seek": 250648, "start": 2520.72, "end": 2526.32, "text": " is a good one or not, you know, what is it about it that it tells it? What does it tell us about", "tokens": [51076, 307, 257, 665, 472, 420, 406, 11, 291, 458, 11, 437, 307, 309, 466, 309, 300, 309, 5112, 309, 30, 708, 775, 309, 980, 505, 466, 51356], "temperature": 0.0, "avg_logprob": -0.11830398523918936, "compression_ratio": 1.8584905660377358, "no_speech_prob": 0.0007015234441496432}, {"id": 448, "seek": 250648, "start": 2526.32, "end": 2532.16, "text": " the brain? I mean, one thing we sort of, one of the things we try, obviously, is look at how it affects", "tokens": [51356, 264, 3567, 30, 286, 914, 11, 472, 551, 321, 1333, 295, 11, 472, 295, 264, 721, 321, 853, 11, 2745, 11, 307, 574, 412, 577, 309, 11807, 51648], "temperature": 0.0, "avg_logprob": -0.11830398523918936, "compression_ratio": 1.8584905660377358, "no_speech_prob": 0.0007015234441496432}, {"id": 449, "seek": 253216, "start": 2533.12, "end": 2538.8799999999997, "text": " language, language synchrony between different communities. So here we've, we've taken the", "tokens": [50412, 2856, 11, 2856, 19331, 88, 1296, 819, 4456, 13, 407, 510, 321, 600, 11, 321, 600, 2726, 264, 50700], "temperature": 0.0, "avg_logprob": -0.12866705817145271, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.005679077468812466}, {"id": 450, "seek": 253216, "start": 2538.8799999999997, "end": 2545.7599999999998, "text": " iterated language model, and we've stopped it being a simple learner to, to, teacher-to-learner", "tokens": [50700, 17138, 770, 2856, 2316, 11, 293, 321, 600, 5936, 309, 885, 257, 2199, 33347, 281, 11, 281, 11, 5027, 12, 1353, 12, 306, 22916, 51044], "temperature": 0.0, "avg_logprob": -0.12866705817145271, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.005679077468812466}, {"id": 451, "seek": 253216, "start": 2545.7599999999998, "end": 2549.92, "text": " interaction, but rather have some sort of web of interactions where a learner has a privileged", "tokens": [51044, 9285, 11, 457, 2831, 362, 512, 1333, 295, 3670, 295, 13280, 689, 257, 33347, 575, 257, 25293, 51252], "temperature": 0.0, "avg_logprob": -0.12866705817145271, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.005679077468812466}, {"id": 452, "seek": 253216, "start": 2549.92, "end": 2554.3199999999997, "text": " teacher, but also learns from, from their community. And then we take networks where there's", "tokens": [51252, 5027, 11, 457, 611, 27152, 490, 11, 490, 641, 1768, 13, 400, 550, 321, 747, 9590, 689, 456, 311, 51472], "temperature": 0.0, "avg_logprob": -0.12866705817145271, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.005679077468812466}, {"id": 453, "seek": 253216, "start": 2554.3199999999997, "end": 2559.52, "text": " greater connectivity within, within a cave. It's, you know, it's one of these cave people", "tokens": [51472, 5044, 21095, 1951, 11, 1951, 257, 11730, 13, 467, 311, 11, 291, 458, 11, 309, 311, 472, 295, 613, 11730, 561, 51732], "temperature": 0.0, "avg_logprob": -0.12866705817145271, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.005679077468812466}, {"id": 454, "seek": 255952, "start": 2560.16, "end": 2564.8, "text": " graphs. So there's greater connectivity inside a cave, then across the caves, and we look at", "tokens": [50396, 24877, 13, 407, 456, 311, 5044, 21095, 1854, 257, 11730, 11, 550, 2108, 264, 32288, 11, 293, 321, 574, 412, 50628], "temperature": 0.0, "avg_logprob": -0.10866447887589446, "compression_ratio": 1.8818897637795275, "no_speech_prob": 0.0051152678206563}, {"id": 455, "seek": 255952, "start": 2564.8, "end": 2569.44, "text": " how the languages evolve and stabilize. And we discovered that you can, you know, depending", "tokens": [50628, 577, 264, 8650, 16693, 293, 31870, 13, 400, 321, 6941, 300, 291, 393, 11, 291, 458, 11, 5413, 50860], "temperature": 0.0, "avg_logprob": -0.10866447887589446, "compression_ratio": 1.8818897637795275, "no_speech_prob": 0.0051152678206563}, {"id": 456, "seek": 255952, "start": 2569.44, "end": 2575.68, "text": " on your parameter choices, you can end up with, you know, five different languages in six caves,", "tokens": [50860, 322, 428, 13075, 7994, 11, 291, 393, 917, 493, 365, 11, 291, 458, 11, 1732, 819, 8650, 294, 2309, 32288, 11, 51172], "temperature": 0.0, "avg_logprob": -0.10866447887589446, "compression_ratio": 1.8818897637795275, "no_speech_prob": 0.0051152678206563}, {"id": 457, "seek": 255952, "start": 2575.68, "end": 2579.92, "text": " this cave, they don't even have their own language, or a case where there's, everybody speaks the", "tokens": [51172, 341, 11730, 11, 436, 500, 380, 754, 362, 641, 1065, 2856, 11, 420, 257, 1389, 689, 456, 311, 11, 2201, 10789, 264, 51384], "temperature": 0.0, "avg_logprob": -0.10866447887589446, "compression_ratio": 1.8818897637795275, "no_speech_prob": 0.0051152678206563}, {"id": 458, "seek": 255952, "start": 2579.92, "end": 2585.28, "text": " same language, or where some languages are shared and some of them are different, or where there's", "tokens": [51384, 912, 2856, 11, 420, 689, 512, 8650, 366, 5507, 293, 512, 295, 552, 366, 819, 11, 420, 689, 456, 311, 51652], "temperature": 0.0, "avg_logprob": -0.10866447887589446, "compression_ratio": 1.8818897637795275, "no_speech_prob": 0.0051152678206563}, {"id": 459, "seek": 258528, "start": 2585.36, "end": 2591.92, "text": " two different languages. And so you can use this model to try and understand properties of the", "tokens": [50368, 732, 819, 8650, 13, 400, 370, 291, 393, 764, 341, 2316, 281, 853, 293, 1223, 7221, 295, 264, 50696], "temperature": 0.0, "avg_logprob": -0.10085673124893851, "compression_ratio": 1.7970479704797049, "no_speech_prob": 0.004448182415217161}, {"id": 460, "seek": 258528, "start": 2591.92, "end": 2597.2000000000003, "text": " distribution or the sizes of language, languages. But the problem there, of course, is that there's", "tokens": [50696, 7316, 420, 264, 11602, 295, 2856, 11, 8650, 13, 583, 264, 1154, 456, 11, 295, 1164, 11, 307, 300, 456, 311, 50960], "temperature": 0.0, "avg_logprob": -0.10085673124893851, "compression_ratio": 1.7970479704797049, "no_speech_prob": 0.004448182415217161}, {"id": 461, "seek": 258528, "start": 2597.2000000000003, "end": 2601.6800000000003, "text": " millions of parameters, you have to work out how to, how to, you know, what the network should", "tokens": [50960, 6803, 295, 9834, 11, 291, 362, 281, 589, 484, 577, 281, 11, 577, 281, 11, 291, 458, 11, 437, 264, 3209, 820, 51184], "temperature": 0.0, "avg_logprob": -0.10085673124893851, "compression_ratio": 1.7970479704797049, "no_speech_prob": 0.004448182415217161}, {"id": 462, "seek": 258528, "start": 2601.6800000000003, "end": 2607.92, "text": " look like, how to structure the teaching events, how to, how to structure the size of the bottlenecks", "tokens": [51184, 574, 411, 11, 577, 281, 3877, 264, 4571, 3931, 11, 577, 281, 11, 577, 281, 3877, 264, 2744, 295, 264, 44641, 2761, 51496], "temperature": 0.0, "avg_logprob": -0.10085673124893851, "compression_ratio": 1.7970479704797049, "no_speech_prob": 0.004448182415217161}, {"id": 463, "seek": 258528, "start": 2607.92, "end": 2612.5600000000004, "text": " compared to all possible being single pairs, etc. And so the model, although intended as a very", "tokens": [51496, 5347, 281, 439, 1944, 885, 2167, 15494, 11, 5183, 13, 400, 370, 264, 2316, 11, 4878, 10226, 382, 257, 588, 51728], "temperature": 0.0, "avg_logprob": -0.10085673124893851, "compression_ratio": 1.7970479704797049, "no_speech_prob": 0.004448182415217161}, {"id": 464, "seek": 261256, "start": 2612.56, "end": 2616.64, "text": " simple one, when you start trying to apply it in ways that can give you data that you could", "tokens": [50364, 2199, 472, 11, 562, 291, 722, 1382, 281, 3079, 309, 294, 2098, 300, 393, 976, 291, 1412, 300, 291, 727, 50568], "temperature": 0.0, "avg_logprob": -0.06718250187960538, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.0035639943089336157}, {"id": 465, "seek": 261256, "start": 2616.64, "end": 2623.52, "text": " compare to the real world, the model becomes still simple, too simple to produce something,", "tokens": [50568, 6794, 281, 264, 957, 1002, 11, 264, 2316, 3643, 920, 2199, 11, 886, 2199, 281, 5258, 746, 11, 50912], "temperature": 0.0, "avg_logprob": -0.06718250187960538, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.0035639943089336157}, {"id": 466, "seek": 261256, "start": 2623.52, "end": 2630.0, "text": " you know, directly comparable to language, but much too complicated to draw easy conclusions from.", "tokens": [50912, 291, 458, 11, 3838, 25323, 281, 2856, 11, 457, 709, 886, 6179, 281, 2642, 1858, 22865, 490, 13, 51236], "temperature": 0.0, "avg_logprob": -0.06718250187960538, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.0035639943089336157}, {"id": 467, "seek": 261256, "start": 2630.0, "end": 2636.96, "text": " And so it struck me at this point that we should try and think of what the very simplest model", "tokens": [51236, 400, 370, 309, 13159, 385, 412, 341, 935, 300, 321, 820, 853, 293, 519, 295, 437, 264, 588, 22811, 2316, 51584], "temperature": 0.0, "avg_logprob": -0.06718250187960538, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.0035639943089336157}, {"id": 468, "seek": 261256, "start": 2636.96, "end": 2641.7599999999998, "text": " of language evolution is. And that's what I wanted to finish by talking about briefly. So the idea,", "tokens": [51584, 295, 2856, 9303, 307, 13, 400, 300, 311, 437, 286, 1415, 281, 2413, 538, 1417, 466, 10515, 13, 407, 264, 1558, 11, 51824], "temperature": 0.0, "avg_logprob": -0.06718250187960538, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.0035639943089336157}, {"id": 469, "seek": 264176, "start": 2641.76, "end": 2649.92, "text": " here, is that we want to look at language change and look at the, the, the most personmonious", "tokens": [50364, 510, 11, 307, 300, 321, 528, 281, 574, 412, 2856, 1319, 293, 574, 412, 264, 11, 264, 11, 264, 881, 954, 3317, 851, 50772], "temperature": 0.0, "avg_logprob": -0.12914957079971046, "compression_ratio": 1.7992277992277992, "no_speech_prob": 0.0029226779006421566}, {"id": 470, "seek": 264176, "start": 2649.92, "end": 2655.84, "text": " description of what goes on as languages change. And so, obviously, one property that languages", "tokens": [50772, 3855, 295, 437, 1709, 322, 382, 8650, 1319, 13, 400, 370, 11, 2745, 11, 472, 4707, 300, 8650, 51068], "temperature": 0.0, "avg_logprob": -0.12914957079971046, "compression_ratio": 1.7992277992277992, "no_speech_prob": 0.0029226779006421566}, {"id": 471, "seek": 264176, "start": 2655.84, "end": 2660.32, "text": " have to have is alignment. If you're talking to someone, you want their language to be almost", "tokens": [51068, 362, 281, 362, 307, 18515, 13, 759, 291, 434, 1417, 281, 1580, 11, 291, 528, 641, 2856, 281, 312, 1920, 51292], "temperature": 0.0, "avg_logprob": -0.12914957079971046, "compression_ratio": 1.7992277992277992, "no_speech_prob": 0.0029226779006421566}, {"id": 472, "seek": 264176, "start": 2660.32, "end": 2664.2400000000002, "text": " the same as your own, or else you won't have, you won't be able to understand them. And the closer", "tokens": [51292, 264, 912, 382, 428, 1065, 11, 420, 1646, 291, 1582, 380, 362, 11, 291, 1582, 380, 312, 1075, 281, 1223, 552, 13, 400, 264, 4966, 51488], "temperature": 0.0, "avg_logprob": -0.12914957079971046, "compression_ratio": 1.7992277992277992, "no_speech_prob": 0.0029226779006421566}, {"id": 473, "seek": 264176, "start": 2664.2400000000002, "end": 2668.8, "text": " their language is to yours, the less sort of cognitive low communication will play,", "tokens": [51488, 641, 2856, 307, 281, 6342, 11, 264, 1570, 1333, 295, 15605, 2295, 6101, 486, 862, 11, 51716], "temperature": 0.0, "avg_logprob": -0.12914957079971046, "compression_ratio": 1.7992277992277992, "no_speech_prob": 0.0029226779006421566}, {"id": 474, "seek": 266880, "start": 2669.44, "end": 2675.1200000000003, "text": " will place on you. So I think the first properties of languages as they change is that they should,", "tokens": [50396, 486, 1081, 322, 291, 13, 407, 286, 519, 264, 700, 7221, 295, 8650, 382, 436, 1319, 307, 300, 436, 820, 11, 50680], "temperature": 0.0, "avg_logprob": -0.13923639892249978, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.005801627412438393}, {"id": 475, "seek": 266880, "start": 2675.1200000000003, "end": 2682.88, "text": " should align. The second property is the converse, this inclination towards change. You know,", "tokens": [50680, 820, 7975, 13, 440, 1150, 4707, 307, 264, 416, 4308, 11, 341, 37070, 2486, 3030, 1319, 13, 509, 458, 11, 51068], "temperature": 0.0, "avg_logprob": -0.13923639892249978, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.005801627412438393}, {"id": 476, "seek": 266880, "start": 2682.88, "end": 2688.88, "text": " obviously, teenagers do this all the time. I have teenagers, they, they, they use words in", "tokens": [51068, 2745, 11, 23618, 360, 341, 439, 264, 565, 13, 286, 362, 23618, 11, 436, 11, 436, 11, 436, 764, 2283, 294, 51368], "temperature": 0.0, "avg_logprob": -0.13923639892249978, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.005801627412438393}, {"id": 477, "seek": 266880, "start": 2688.88, "end": 2694.7200000000003, "text": " different ways. They, they delight in language invention. I mean, weirdly, my, my mother is", "tokens": [51368, 819, 2098, 13, 814, 11, 436, 11627, 294, 2856, 22265, 13, 286, 914, 11, 48931, 11, 452, 11, 452, 2895, 307, 51660], "temperature": 0.0, "avg_logprob": -0.13923639892249978, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.005801627412438393}, {"id": 478, "seek": 269472, "start": 2694.7999999999997, "end": 2700.08, "text": " almost the very opposite of a teenager. It's the same. She's forever making up new words and", "tokens": [50368, 1920, 264, 588, 6182, 295, 257, 21440, 13, 467, 311, 264, 912, 13, 1240, 311, 5680, 1455, 493, 777, 2283, 293, 50632], "temperature": 0.0, "avg_logprob": -0.14200090957900224, "compression_ratio": 1.7445255474452555, "no_speech_prob": 0.05698651447892189}, {"id": 479, "seek": 269472, "start": 2700.08, "end": 2704.24, "text": " new ways of saying things, often based on puns. And then the puns turn into words. And then she", "tokens": [50632, 777, 2098, 295, 1566, 721, 11, 2049, 2361, 322, 4468, 82, 13, 400, 550, 264, 4468, 82, 1261, 666, 2283, 13, 400, 550, 750, 50840], "temperature": 0.0, "avg_logprob": -0.14200090957900224, "compression_ratio": 1.7445255474452555, "no_speech_prob": 0.05698651447892189}, {"id": 480, "seek": 269472, "start": 2704.24, "end": 2708.56, "text": " just uses them in everyday conversation, expects people to, to follow. And she just does it in", "tokens": [50840, 445, 4960, 552, 294, 7429, 3761, 11, 33280, 561, 281, 11, 281, 1524, 13, 400, 750, 445, 775, 309, 294, 51056], "temperature": 0.0, "avg_logprob": -0.14200090957900224, "compression_ratio": 1.7445255474452555, "no_speech_prob": 0.05698651447892189}, {"id": 481, "seek": 269472, "start": 2708.56, "end": 2714.08, "text": " a, out of a sheer delight in language invention. There's also a kind of a more strategic point", "tokens": [51056, 257, 11, 484, 295, 257, 23061, 11627, 294, 2856, 22265, 13, 821, 311, 611, 257, 733, 295, 257, 544, 10924, 935, 51332], "temperature": 0.0, "avg_logprob": -0.14200090957900224, "compression_ratio": 1.7445255474452555, "no_speech_prob": 0.05698651447892189}, {"id": 482, "seek": 269472, "start": 2714.72, "end": 2719.7599999999998, "text": " to language invention, which is to find shorthands for saying things, auxiliary words are pushed in", "tokens": [51364, 281, 2856, 22265, 11, 597, 307, 281, 915, 402, 2652, 2967, 337, 1566, 721, 11, 43741, 2283, 366, 9152, 294, 51616], "temperature": 0.0, "avg_logprob": -0.14200090957900224, "compression_ratio": 1.7445255474452555, "no_speech_prob": 0.05698651447892189}, {"id": 483, "seek": 271976, "start": 2719.76, "end": 2725.6800000000003, "text": " with the words that their exiliaries to pronunciations are changed. People say small phrases to mean", "tokens": [50364, 365, 264, 2283, 300, 641, 454, 4600, 530, 281, 7569, 11228, 763, 366, 3105, 13, 3432, 584, 1359, 20312, 281, 914, 50660], "temperature": 0.0, "avg_logprob": -0.11937382346705387, "compression_ratio": 1.7695167286245352, "no_speech_prob": 0.026622729375958443}, {"id": 484, "seek": 271976, "start": 2725.6800000000003, "end": 2729.76, "text": " more complicated, large phrase things, et cetera. So there, there, there, there are these two sort", "tokens": [50660, 544, 6179, 11, 2416, 9535, 721, 11, 1030, 11458, 13, 407, 456, 11, 456, 11, 456, 11, 456, 366, 613, 732, 1333, 50864], "temperature": 0.0, "avg_logprob": -0.11937382346705387, "compression_ratio": 1.7695167286245352, "no_speech_prob": 0.026622729375958443}, {"id": 485, "seek": 271976, "start": 2729.76, "end": 2735.2000000000003, "text": " of competing forces that work in language changes. The third one as well, which is an", "tokens": [50864, 295, 15439, 5874, 300, 589, 294, 2856, 2962, 13, 440, 2636, 472, 382, 731, 11, 597, 307, 364, 51136], "temperature": 0.0, "avg_logprob": -0.11937382346705387, "compression_ratio": 1.7695167286245352, "no_speech_prob": 0.026622729375958443}, {"id": 486, "seek": 271976, "start": 2735.2000000000003, "end": 2739.76, "text": " inclination towards consistency. That's exactly the sort of thing that the bottom leg in the", "tokens": [51136, 37070, 2486, 3030, 14416, 13, 663, 311, 2293, 264, 1333, 295, 551, 300, 264, 2767, 1676, 294, 264, 51364], "temperature": 0.0, "avg_logprob": -0.11937382346705387, "compression_ratio": 1.7695167286245352, "no_speech_prob": 0.026622729375958443}, {"id": 487, "seek": 271976, "start": 2739.76, "end": 2744.1600000000003, "text": " iterated language model deals with. And I'm not going to deal with that one here. That's the idea", "tokens": [51364, 17138, 770, 2856, 2316, 11215, 365, 13, 400, 286, 478, 406, 516, 281, 2028, 365, 300, 472, 510, 13, 663, 311, 264, 1558, 51584], "temperature": 0.0, "avg_logprob": -0.11937382346705387, "compression_ratio": 1.7695167286245352, "no_speech_prob": 0.026622729375958443}, {"id": 488, "seek": 274416, "start": 2744.16, "end": 2750.24, "text": " that, you know, if, if in one sentence you put, you know, if at some point you decide that the", "tokens": [50364, 300, 11, 291, 458, 11, 498, 11, 498, 294, 472, 8174, 291, 829, 11, 291, 458, 11, 498, 412, 512, 935, 291, 4536, 300, 264, 50668], "temperature": 0.0, "avg_logprob": -0.10321439361572265, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.010173718445003033}, {"id": 489, "seek": 274416, "start": 2750.24, "end": 2755.44, "text": " verb goes at the end of the sentence, it does that for, for, you know, that can sit that,", "tokens": [50668, 9595, 1709, 412, 264, 917, 295, 264, 8174, 11, 309, 775, 300, 337, 11, 337, 11, 291, 458, 11, 300, 393, 1394, 300, 11, 50928], "temperature": 0.0, "avg_logprob": -0.10321439361572265, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.010173718445003033}, {"id": 490, "seek": 274416, "start": 2755.44, "end": 2760.0, "text": " that rule doesn't just apply to one, one's type of sentence, but rolls through the language. So", "tokens": [50928, 300, 4978, 1177, 380, 445, 3079, 281, 472, 11, 472, 311, 2010, 295, 8174, 11, 457, 15767, 807, 264, 2856, 13, 407, 51156], "temperature": 0.0, "avg_logprob": -0.10321439361572265, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.010173718445003033}, {"id": 491, "seek": 274416, "start": 2760.0, "end": 2765.68, "text": " once something starts changing, other parts of language change to suit it. So that's an inclination", "tokens": [51156, 1564, 746, 3719, 4473, 11, 661, 3166, 295, 2856, 1319, 281, 5722, 309, 13, 407, 300, 311, 364, 37070, 2486, 51440], "temperature": 0.0, "avg_logprob": -0.10321439361572265, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.010173718445003033}, {"id": 492, "seek": 274416, "start": 2765.68, "end": 2771.68, "text": " towards consistency. You know, and it's obviously kind of amazing. I mean, if you think of how", "tokens": [51440, 3030, 14416, 13, 509, 458, 11, 293, 309, 311, 2745, 733, 295, 2243, 13, 286, 914, 11, 498, 291, 519, 295, 577, 51740], "temperature": 0.0, "avg_logprob": -0.10321439361572265, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.010173718445003033}, {"id": 493, "seek": 277168, "start": 2772.0, "end": 2779.68, "text": " how, how noun and verb modifications work with Arabic and Hebrew, I mean, it's just incredible", "tokens": [50380, 577, 11, 577, 23307, 293, 9595, 26881, 589, 365, 19938, 293, 17895, 11, 286, 914, 11, 309, 311, 445, 4651, 50764], "temperature": 0.0, "avg_logprob": -0.1256865611118553, "compression_ratio": 1.690391459074733, "no_speech_prob": 0.002651890739798546}, {"id": 494, "seek": 277168, "start": 2779.68, "end": 2783.7599999999998, "text": " the way that, you know, everything has three, three consonants, and then you've got the verbs and", "tokens": [50764, 264, 636, 300, 11, 291, 458, 11, 1203, 575, 1045, 11, 1045, 30843, 1719, 11, 293, 550, 291, 600, 658, 264, 30051, 293, 50968], "temperature": 0.0, "avg_logprob": -0.1256865611118553, "compression_ratio": 1.690391459074733, "no_speech_prob": 0.002651890739798546}, {"id": 495, "seek": 277168, "start": 2783.7599999999998, "end": 2788.08, "text": " the endings and, you know, how did that happen? I mean, it's just some, some small changes then", "tokens": [50968, 264, 42474, 293, 11, 291, 458, 11, 577, 630, 300, 1051, 30, 286, 914, 11, 309, 311, 445, 512, 11, 512, 1359, 2962, 550, 51184], "temperature": 0.0, "avg_logprob": -0.1256865611118553, "compression_ratio": 1.690391459074733, "no_speech_prob": 0.002651890739798546}, {"id": 496, "seek": 277168, "start": 2788.08, "end": 2793.12, "text": " became rules. And so that is obviously an important part, but not one we'll talk about now. So", "tokens": [51184, 3062, 4474, 13, 400, 370, 300, 307, 2745, 364, 1021, 644, 11, 457, 406, 472, 321, 603, 751, 466, 586, 13, 407, 51436], "temperature": 0.0, "avg_logprob": -0.1256865611118553, "compression_ratio": 1.690391459074733, "no_speech_prob": 0.002651890739798546}, {"id": 497, "seek": 277168, "start": 2793.12, "end": 2799.2, "text": " obviously agreement alignment between speakers and spontaneous change. You probably guessed", "tokens": [51436, 2745, 8106, 18515, 1296, 9518, 293, 32744, 1319, 13, 509, 1391, 21852, 51740], "temperature": 0.0, "avg_logprob": -0.1256865611118553, "compression_ratio": 1.690391459074733, "no_speech_prob": 0.002651890739798546}, {"id": 498, "seek": 279920, "start": 2799.2, "end": 2805.2, "text": " when we're going with this, that that's a nising model. So in the nising model, it's a model of", "tokens": [50364, 562, 321, 434, 516, 365, 341, 11, 300, 300, 311, 257, 297, 3436, 2316, 13, 407, 294, 264, 297, 3436, 2316, 11, 309, 311, 257, 2316, 295, 50664], "temperature": 0.0, "avg_logprob": -0.16407079035692876, "compression_ratio": 1.7452830188679245, "no_speech_prob": 0.010441872291266918}, {"id": 499, "seek": 279920, "start": 2805.2, "end": 2811.7599999999998, "text": " magnetism, as you'll all know, at different lattice sites, you've got a spin, the spin goes", "tokens": [50664, 15211, 1434, 11, 382, 291, 603, 439, 458, 11, 412, 819, 34011, 7533, 11, 291, 600, 658, 257, 6060, 11, 264, 6060, 1709, 50992], "temperature": 0.0, "avg_logprob": -0.16407079035692876, "compression_ratio": 1.7452830188679245, "no_speech_prob": 0.010441872291266918}, {"id": 500, "seek": 279920, "start": 2811.7599999999998, "end": 2817.12, "text": " upwards or downwards. And there's an energy associated with, with the alignment of the", "tokens": [50992, 22167, 420, 39880, 13, 400, 456, 311, 364, 2281, 6615, 365, 11, 365, 264, 18515, 295, 264, 51260], "temperature": 0.0, "avg_logprob": -0.16407079035692876, "compression_ratio": 1.7452830188679245, "no_speech_prob": 0.010441872291266918}, {"id": 501, "seek": 279920, "start": 2817.12, "end": 2823.8399999999997, "text": " spins. It's a lower energy state. If the spins are aligned, then if they're not aligned. And so", "tokens": [51260, 31587, 13, 467, 311, 257, 3126, 2281, 1785, 13, 759, 264, 31587, 366, 17962, 11, 550, 498, 436, 434, 406, 17962, 13, 400, 370, 51596], "temperature": 0.0, "avg_logprob": -0.16407079035692876, "compression_ratio": 1.7452830188679245, "no_speech_prob": 0.010441872291266918}, {"id": 502, "seek": 282384, "start": 2824.6400000000003, "end": 2830.2400000000002, "text": " you would obviously expect an nising model to evolve towards total alignment. But there's also", "tokens": [50404, 291, 576, 2745, 2066, 364, 297, 3436, 2316, 281, 16693, 3030, 3217, 18515, 13, 583, 456, 311, 611, 50684], "temperature": 0.0, "avg_logprob": -0.12937349185608982, "compression_ratio": 1.8724279835390947, "no_speech_prob": 0.00306770415045321}, {"id": 503, "seek": 282384, "start": 2830.2400000000002, "end": 2835.6000000000004, "text": " in the nising model thermal effect. So in an nising model, this is the metropolis", "tokens": [50684, 294, 264, 297, 3436, 2316, 15070, 1802, 13, 407, 294, 364, 297, 3436, 2316, 11, 341, 307, 264, 1131, 46470, 50952], "temperature": 0.0, "avg_logprob": -0.12937349185608982, "compression_ratio": 1.8724279835390947, "no_speech_prob": 0.00306770415045321}, {"id": 504, "seek": 282384, "start": 2836.4, "end": 2840.6400000000003, "text": " formulation of the nising model, what you do to evolve the model is you choose a random, you", "tokens": [50992, 37642, 295, 264, 297, 3436, 2316, 11, 437, 291, 360, 281, 16693, 264, 2316, 307, 291, 2826, 257, 4974, 11, 291, 51204], "temperature": 0.0, "avg_logprob": -0.12937349185608982, "compression_ratio": 1.8724279835390947, "no_speech_prob": 0.00306770415045321}, {"id": 505, "seek": 282384, "start": 2840.6400000000003, "end": 2844.96, "text": " choose a site, and then you consider what would happen if you, if you flip the spin. So we'll", "tokens": [51204, 2826, 257, 3621, 11, 293, 550, 291, 1949, 437, 576, 1051, 498, 291, 11, 498, 291, 7929, 264, 6060, 13, 407, 321, 603, 51420], "temperature": 0.0, "avg_logprob": -0.12937349185608982, "compression_ratio": 1.8724279835390947, "no_speech_prob": 0.00306770415045321}, {"id": 506, "seek": 282384, "start": 2844.96, "end": 2850.56, "text": " label the spins plus one and minus one. And if the, if the consequence of flipping the spin", "tokens": [51420, 7645, 264, 31587, 1804, 472, 293, 3175, 472, 13, 400, 498, 264, 11, 498, 264, 18326, 295, 26886, 264, 6060, 51700], "temperature": 0.0, "avg_logprob": -0.12937349185608982, "compression_ratio": 1.8724279835390947, "no_speech_prob": 0.00306770415045321}, {"id": 507, "seek": 285056, "start": 2850.56, "end": 2857.52, "text": " can be written down like that, I hope I have the signs right there. But basically, if the spins", "tokens": [50364, 393, 312, 3720, 760, 411, 300, 11, 286, 1454, 286, 362, 264, 7880, 558, 456, 13, 583, 1936, 11, 498, 264, 31587, 50712], "temperature": 0.0, "avg_logprob": -0.11877300028215375, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.008651425130665302}, {"id": 508, "seek": 285056, "start": 2857.52, "end": 2863.2799999999997, "text": " become more aligned on average with the neighbors, so the one site, these are the four, for example,", "tokens": [50712, 1813, 544, 17962, 322, 4274, 365, 264, 12512, 11, 370, 264, 472, 3621, 11, 613, 366, 264, 1451, 11, 337, 1365, 11, 51000], "temperature": 0.0, "avg_logprob": -0.11877300028215375, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.008651425130665302}, {"id": 509, "seek": 285056, "start": 2863.2799999999997, "end": 2868.16, "text": " neighbors of the middle site, if it becomes an average more aligned with them, then a DE will", "tokens": [51000, 12512, 295, 264, 2808, 3621, 11, 498, 309, 3643, 364, 4274, 544, 17962, 365, 552, 11, 550, 257, 10113, 486, 51244], "temperature": 0.0, "avg_logprob": -0.11877300028215375, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.008651425130665302}, {"id": 510, "seek": 285056, "start": 2868.16, "end": 2872.88, "text": " be negative, and you'll accept the change. If it becomes less aligned, you'll still accept the", "tokens": [51244, 312, 3671, 11, 293, 291, 603, 3241, 264, 1319, 13, 759, 309, 3643, 1570, 17962, 11, 291, 603, 920, 3241, 264, 51480], "temperature": 0.0, "avg_logprob": -0.11877300028215375, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.008651425130665302}, {"id": 511, "seek": 285056, "start": 2872.88, "end": 2877.68, "text": " change. And this is the terrible part with some probability given by the exponential of minus", "tokens": [51480, 1319, 13, 400, 341, 307, 264, 6237, 644, 365, 512, 8482, 2212, 538, 264, 21510, 295, 3175, 51720], "temperature": 0.0, "avg_logprob": -0.11877300028215375, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.008651425130665302}, {"id": 512, "seek": 287768, "start": 2877.68, "end": 2883.8399999999997, "text": " DE divided by T. And so DE is positive in this case. If DE is negative, as I said, you always", "tokens": [50364, 10113, 6666, 538, 314, 13, 400, 370, 10113, 307, 3353, 294, 341, 1389, 13, 759, 10113, 307, 3671, 11, 382, 286, 848, 11, 291, 1009, 50672], "temperature": 0.0, "avg_logprob": -0.10552848875522614, "compression_ratio": 1.9047619047619047, "no_speech_prob": 0.002971051959320903}, {"id": 513, "seek": 287768, "start": 2883.8399999999997, "end": 2889.12, "text": " accept the change. T is the temperature. In the case of when you're modeling magnetism, it's", "tokens": [50672, 3241, 264, 1319, 13, 314, 307, 264, 4292, 13, 682, 264, 1389, 295, 562, 291, 434, 15983, 15211, 1434, 11, 309, 311, 50936], "temperature": 0.0, "avg_logprob": -0.10552848875522614, "compression_ratio": 1.9047619047619047, "no_speech_prob": 0.002971051959320903}, {"id": 514, "seek": 287768, "start": 2889.12, "end": 2893.6, "text": " literally the temperature. If T is very large, then this is always one, you always accept the", "tokens": [50936, 3736, 264, 4292, 13, 759, 314, 307, 588, 2416, 11, 550, 341, 307, 1009, 472, 11, 291, 1009, 3241, 264, 51160], "temperature": 0.0, "avg_logprob": -0.10552848875522614, "compression_ratio": 1.9047619047619047, "no_speech_prob": 0.002971051959320903}, {"id": 515, "seek": 287768, "start": 2893.6, "end": 2900.0, "text": " change. And so you just get a random up and down. If T is very small, then this will be near to zero.", "tokens": [51160, 1319, 13, 400, 370, 291, 445, 483, 257, 4974, 493, 293, 760, 13, 759, 314, 307, 588, 1359, 11, 550, 341, 486, 312, 2651, 281, 4018, 13, 51480], "temperature": 0.0, "avg_logprob": -0.10552848875522614, "compression_ratio": 1.9047619047619047, "no_speech_prob": 0.002971051959320903}, {"id": 516, "seek": 287768, "start": 2900.0, "end": 2904.24, "text": " You'll never accept the change. And you'll just get it flowing to total alignment. And so you get", "tokens": [51480, 509, 603, 1128, 3241, 264, 1319, 13, 400, 291, 603, 445, 483, 309, 13974, 281, 3217, 18515, 13, 400, 370, 291, 483, 51692], "temperature": 0.0, "avg_logprob": -0.10552848875522614, "compression_ratio": 1.9047619047619047, "no_speech_prob": 0.002971051959320903}, {"id": 517, "seek": 290424, "start": 2904.24, "end": 2911.3599999999997, "text": " these different patterns. This is the very random. This is the very aligned. And this is the critical", "tokens": [50364, 613, 819, 8294, 13, 639, 307, 264, 588, 4974, 13, 639, 307, 264, 588, 17962, 13, 400, 341, 307, 264, 4924, 50720], "temperature": 0.0, "avg_logprob": -0.10037293054361258, "compression_ratio": 1.8, "no_speech_prob": 0.0034542083740234375}, {"id": 518, "seek": 290424, "start": 2911.3599999999997, "end": 2917.68, "text": " point in between. So this is a model of a phase change. And at the phase point, at the critical", "tokens": [50720, 935, 294, 1296, 13, 407, 341, 307, 257, 2316, 295, 257, 5574, 1319, 13, 400, 412, 264, 5574, 935, 11, 412, 264, 4924, 51036], "temperature": 0.0, "avg_logprob": -0.10037293054361258, "compression_ratio": 1.8, "no_speech_prob": 0.0034542083740234375}, {"id": 519, "seek": 290424, "start": 2917.68, "end": 2922.56, "text": " point, you have scaling behavior at the cluster size. And that's what people are interested in", "tokens": [51036, 935, 11, 291, 362, 21589, 5223, 412, 264, 13630, 2744, 13, 400, 300, 311, 437, 561, 366, 3102, 294, 51280], "temperature": 0.0, "avg_logprob": -0.10037293054361258, "compression_ratio": 1.8, "no_speech_prob": 0.0034542083740234375}, {"id": 520, "seek": 290424, "start": 2922.56, "end": 2927.68, "text": " the IC model. So obviously, we can map these two things, the movement towards alignment,", "tokens": [51280, 264, 14360, 2316, 13, 407, 2745, 11, 321, 393, 4471, 613, 732, 721, 11, 264, 3963, 3030, 18515, 11, 51536], "temperature": 0.0, "avg_logprob": -0.10037293054361258, "compression_ratio": 1.8, "no_speech_prob": 0.0034542083740234375}, {"id": 521, "seek": 290424, "start": 2927.68, "end": 2932.16, "text": " towards the idea that you're trying to lower the energy. And the spontaneous change is like the", "tokens": [51536, 3030, 264, 1558, 300, 291, 434, 1382, 281, 3126, 264, 2281, 13, 400, 264, 32744, 1319, 307, 411, 264, 51760], "temperature": 0.0, "avg_logprob": -0.10037293054361258, "compression_ratio": 1.8, "no_speech_prob": 0.0034542083740234375}, {"id": 522, "seek": 293216, "start": 2932.16, "end": 2938.8799999999997, "text": " thermal fluctuations. But clearly, of course, if you had only one spin, then you'd have only two", "tokens": [50364, 15070, 45276, 13, 583, 4448, 11, 295, 1164, 11, 498, 291, 632, 787, 472, 6060, 11, 550, 291, 1116, 362, 787, 732, 50700], "temperature": 0.0, "avg_logprob": -0.11256675382631015, "compression_ratio": 1.6985294117647058, "no_speech_prob": 0.008687279187142849}, {"id": 523, "seek": 293216, "start": 2938.8799999999997, "end": 2942.56, "text": " languages, the upper language and the down language. And that wouldn't be very interesting.", "tokens": [50700, 8650, 11, 264, 6597, 2856, 293, 264, 760, 2856, 13, 400, 300, 2759, 380, 312, 588, 1880, 13, 50884], "temperature": 0.0, "avg_logprob": -0.11256675382631015, "compression_ratio": 1.6985294117647058, "no_speech_prob": 0.008687279187142849}, {"id": 524, "seek": 293216, "start": 2942.56, "end": 2948.24, "text": " So what we do instead is we've got a vector of spins, d dimensions, where d is some number,", "tokens": [50884, 407, 437, 321, 360, 2602, 307, 321, 600, 658, 257, 8062, 295, 31587, 11, 274, 12819, 11, 689, 274, 307, 512, 1230, 11, 51168], "temperature": 0.0, "avg_logprob": -0.11256675382631015, "compression_ratio": 1.6985294117647058, "no_speech_prob": 0.008687279187142849}, {"id": 525, "seek": 293216, "start": 2948.24, "end": 2953.2, "text": " you have to be chosen. And we imagine that these are coding for different properties", "tokens": [51168, 291, 362, 281, 312, 8614, 13, 400, 321, 3811, 300, 613, 366, 17720, 337, 819, 7221, 51416], "temperature": 0.0, "avg_logprob": -0.11256675382631015, "compression_ratio": 1.6985294117647058, "no_speech_prob": 0.008687279187142849}, {"id": 526, "seek": 293216, "start": 2953.8399999999997, "end": 2959.92, "text": " of the language. So one spin might be, how do you say father? And obviously in lots of languages", "tokens": [51448, 295, 264, 2856, 13, 407, 472, 6060, 1062, 312, 11, 577, 360, 291, 584, 3086, 30, 400, 2745, 294, 3195, 295, 8650, 51752], "temperature": 0.0, "avg_logprob": -0.11256675382631015, "compression_ratio": 1.6985294117647058, "no_speech_prob": 0.008687279187142849}, {"id": 527, "seek": 295992, "start": 2959.92, "end": 2967.44, "text": " like Latin, German, Irish, the word is almost the same as that would be plus one. Do you use", "tokens": [50364, 411, 10803, 11, 6521, 11, 16801, 11, 264, 1349, 307, 1920, 264, 912, 382, 300, 576, 312, 1804, 472, 13, 1144, 291, 764, 50740], "temperature": 0.0, "avg_logprob": -0.14944198006077816, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.006986348889768124}, {"id": 528, "seek": 295992, "start": 2967.44, "end": 2972.56, "text": " derivative of pattern for the word for father? And minus one would be languages where you don't.", "tokens": [50740, 13760, 295, 5102, 337, 264, 1349, 337, 3086, 30, 400, 3175, 472, 576, 312, 8650, 689, 291, 500, 380, 13, 50996], "temperature": 0.0, "avg_logprob": -0.14944198006077816, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.006986348889768124}, {"id": 529, "seek": 295992, "start": 2974.08, "end": 2980.16, "text": " I don't know any languages that don't use pattern like derivatives. Anyway, not a good example.", "tokens": [51072, 286, 500, 380, 458, 604, 8650, 300, 500, 380, 764, 5102, 411, 33733, 13, 5684, 11, 406, 257, 665, 1365, 13, 51376], "temperature": 0.0, "avg_logprob": -0.14944198006077816, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.006986348889768124}, {"id": 530, "seek": 295992, "start": 2980.16, "end": 2985.2000000000003, "text": " Or it could be something to do with word order. So here you see the word order in English,", "tokens": [51376, 1610, 309, 727, 312, 746, 281, 360, 365, 1349, 1668, 13, 407, 510, 291, 536, 264, 1349, 1668, 294, 3669, 11, 51628], "temperature": 0.0, "avg_logprob": -0.14944198006077816, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.006986348889768124}, {"id": 531, "seek": 298520, "start": 2986.0, "end": 2992.48, "text": " subject followed by, followed by verb and adjective followed by noun, whereas in Irish,", "tokens": [50404, 3983, 6263, 538, 11, 6263, 538, 9595, 293, 44129, 6263, 538, 23307, 11, 9735, 294, 16801, 11, 50728], "temperature": 0.0, "avg_logprob": -0.12880931992128672, "compression_ratio": 1.873015873015873, "no_speech_prob": 0.030054885894060135}, {"id": 532, "seek": 298520, "start": 2992.48, "end": 2998.8799999999997, "text": " it's verb followed by subject, noun followed by adjective, and they would be, you know,", "tokens": [50728, 309, 311, 9595, 6263, 538, 3983, 11, 23307, 6263, 538, 44129, 11, 293, 436, 576, 312, 11, 291, 458, 11, 51048], "temperature": 0.0, "avg_logprob": -0.12880931992128672, "compression_ratio": 1.873015873015873, "no_speech_prob": 0.030054885894060135}, {"id": 533, "seek": 298520, "start": 2998.8799999999997, "end": 3002.48, "text": " correspond to two further spins. And then a further complication is that that choice", "tokens": [51048, 6805, 281, 732, 3052, 31587, 13, 400, 550, 257, 3052, 1209, 8758, 307, 300, 300, 3922, 51228], "temperature": 0.0, "avg_logprob": -0.12880931992128672, "compression_ratio": 1.873015873015873, "no_speech_prob": 0.030054885894060135}, {"id": 534, "seek": 298520, "start": 3002.48, "end": 3008.3199999999997, "text": " tends to be the same. So if the subject comes before the verb, the adjective comes before the", "tokens": [51228, 12258, 281, 312, 264, 912, 13, 407, 498, 264, 3983, 1487, 949, 264, 9595, 11, 264, 44129, 1487, 949, 264, 51520], "temperature": 0.0, "avg_logprob": -0.12880931992128672, "compression_ratio": 1.873015873015873, "no_speech_prob": 0.030054885894060135}, {"id": 535, "seek": 300832, "start": 3009.28, "end": 3014.88, "text": " the noun. If the subject comes after the verb, it's the other way around.", "tokens": [50412, 264, 23307, 13, 759, 264, 3983, 1487, 934, 264, 9595, 11, 309, 311, 264, 661, 636, 926, 13, 50692], "temperature": 0.0, "avg_logprob": -0.1244489149613814, "compression_ratio": 1.7178423236514522, "no_speech_prob": 0.009664955548942089}, {"id": 536, "seek": 300832, "start": 3016.7200000000003, "end": 3019.92, "text": " And so you can imagine there might be two spins, one for this part of the word order,", "tokens": [50784, 400, 370, 291, 393, 3811, 456, 1062, 312, 732, 31587, 11, 472, 337, 341, 644, 295, 264, 1349, 1668, 11, 50944], "temperature": 0.0, "avg_logprob": -0.1244489149613814, "compression_ratio": 1.7178423236514522, "no_speech_prob": 0.009664955548942089}, {"id": 537, "seek": 300832, "start": 3019.92, "end": 3022.56, "text": " one for that part of the word order, and then in some elaborations model,", "tokens": [50944, 472, 337, 300, 644, 295, 264, 1349, 1668, 11, 293, 550, 294, 512, 16298, 763, 2316, 11, 51076], "temperature": 0.0, "avg_logprob": -0.1244489149613814, "compression_ratio": 1.7178423236514522, "no_speech_prob": 0.009664955548942089}, {"id": 538, "seek": 300832, "start": 3022.56, "end": 3026.32, "text": " there'd be some relationship between them. But that's basically the model we have.", "tokens": [51076, 456, 1116, 312, 512, 2480, 1296, 552, 13, 583, 300, 311, 1936, 264, 2316, 321, 362, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1244489149613814, "compression_ratio": 1.7178423236514522, "no_speech_prob": 0.009664955548942089}, {"id": 539, "seek": 300832, "start": 3027.28, "end": 3034.0, "text": " We have, as it were, in this first idea, we've got D independent ising models, and a given set of", "tokens": [51312, 492, 362, 11, 382, 309, 645, 11, 294, 341, 700, 1558, 11, 321, 600, 658, 413, 6695, 307, 278, 5245, 11, 293, 257, 2212, 992, 295, 51648], "temperature": 0.0, "avg_logprob": -0.1244489149613814, "compression_ratio": 1.7178423236514522, "no_speech_prob": 0.009664955548942089}, {"id": 540, "seek": 303400, "start": 3034.0, "end": 3038.48, "text": " plus ones and minus ones constitutes a language, we choose some temperature,", "tokens": [50364, 1804, 2306, 293, 3175, 2306, 44204, 257, 2856, 11, 321, 2826, 512, 4292, 11, 50588], "temperature": 0.0, "avg_logprob": -0.1393860370740978, "compression_ratio": 1.6743295019157087, "no_speech_prob": 0.008811990730464458}, {"id": 541, "seek": 303400, "start": 3038.48, "end": 3043.12, "text": " we run it forward and we get some behavior. I do have a graph in a minute, I'll show you.", "tokens": [50588, 321, 1190, 309, 2128, 293, 321, 483, 512, 5223, 13, 286, 360, 362, 257, 4295, 294, 257, 3456, 11, 286, 603, 855, 291, 13, 50820], "temperature": 0.0, "avg_logprob": -0.1393860370740978, "compression_ratio": 1.6743295019157087, "no_speech_prob": 0.008811990730464458}, {"id": 542, "seek": 303400, "start": 3043.12, "end": 3049.04, "text": " But the main thing that you do see is that because it's lots of interlocking patches,", "tokens": [50820, 583, 264, 2135, 551, 300, 291, 360, 536, 307, 300, 570, 309, 311, 3195, 295, 728, 4102, 278, 26531, 11, 51116], "temperature": 0.0, "avg_logprob": -0.1393860370740978, "compression_ratio": 1.6743295019157087, "no_speech_prob": 0.008811990730464458}, {"id": 543, "seek": 303400, "start": 3049.04, "end": 3054.08, "text": " one for each of the D dimensions, you tend to see language continuum. And language", "tokens": [51116, 472, 337, 1184, 295, 264, 413, 12819, 11, 291, 3928, 281, 536, 2856, 36120, 13, 400, 2856, 51368], "temperature": 0.0, "avg_logprob": -0.1393860370740978, "compression_ratio": 1.6743295019157087, "no_speech_prob": 0.008811990730464458}, {"id": 544, "seek": 303400, "start": 3054.08, "end": 3060.24, "text": " continuum are a property of languages. So, you know, these days, everything's a bit more complicated,", "tokens": [51368, 36120, 366, 257, 4707, 295, 8650, 13, 407, 11, 291, 458, 11, 613, 1708, 11, 1203, 311, 257, 857, 544, 6179, 11, 51676], "temperature": 0.0, "avg_logprob": -0.1393860370740978, "compression_ratio": 1.6743295019157087, "no_speech_prob": 0.008811990730464458}, {"id": 545, "seek": 306024, "start": 3060.3199999999997, "end": 3066.64, "text": " national boundaries and official, you know, government documents and radio stations and so on.", "tokens": [50368, 4048, 13180, 293, 4783, 11, 291, 458, 11, 2463, 8512, 293, 6477, 13390, 293, 370, 322, 13, 50684], "temperature": 0.0, "avg_logprob": -0.11358548813507337, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.01580696739256382}, {"id": 546, "seek": 306024, "start": 3066.64, "end": 3072.7999999999997, "text": " But in the olden days, you tended to have no hard language barriers, boundaries. So if you", "tokens": [50684, 583, 294, 264, 1331, 268, 1708, 11, 291, 34732, 281, 362, 572, 1152, 2856, 13565, 11, 13180, 13, 407, 498, 291, 50992], "temperature": 0.0, "avg_logprob": -0.11358548813507337, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.01580696739256382}, {"id": 547, "seek": 306024, "start": 3072.7999999999997, "end": 3078.64, "text": " walked from, you know, Portugal all the way to Sicily, well, Portuguese would be very different", "tokens": [50992, 7628, 490, 11, 291, 458, 11, 23011, 439, 264, 636, 281, 39155, 953, 11, 731, 11, 22759, 576, 312, 588, 819, 51284], "temperature": 0.0, "avg_logprob": -0.11358548813507337, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.01580696739256382}, {"id": 548, "seek": 306024, "start": 3078.64, "end": 3083.4399999999996, "text": " from Sicilian, but as you do it, you'd never actually be somewhere where we're, well, depending", "tokens": [51284, 490, 39155, 48666, 11, 457, 382, 291, 360, 309, 11, 291, 1116, 1128, 767, 312, 4079, 689, 321, 434, 11, 731, 11, 5413, 51524], "temperature": 0.0, "avg_logprob": -0.11358548813507337, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.01580696739256382}, {"id": 549, "seek": 306024, "start": 3083.4399999999996, "end": 3087.04, "text": " on your route, and that's going to be the point, if you go this way, you're never going to be", "tokens": [51524, 322, 428, 7955, 11, 293, 300, 311, 516, 281, 312, 264, 935, 11, 498, 291, 352, 341, 636, 11, 291, 434, 1128, 516, 281, 312, 51704], "temperature": 0.0, "avg_logprob": -0.11358548813507337, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.01580696739256382}, {"id": 550, "seek": 308704, "start": 3087.04, "end": 3091.12, "text": " somewhere where people in nearby villages can't understand each other. The languages gradually", "tokens": [50364, 4079, 689, 561, 294, 11184, 20444, 393, 380, 1223, 1184, 661, 13, 440, 8650, 13145, 50568], "temperature": 0.0, "avg_logprob": -0.11289470711934198, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.009581356309354305}, {"id": 551, "seek": 308704, "start": 3091.12, "end": 3099.2, "text": " change one to the other. And so that property of language distributions is well reflected by", "tokens": [50568, 1319, 472, 281, 264, 661, 13, 400, 370, 300, 4707, 295, 2856, 37870, 307, 731, 15502, 538, 50972], "temperature": 0.0, "avg_logprob": -0.11289470711934198, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.009581356309354305}, {"id": 552, "seek": 308704, "start": 3100.0, "end": 3104.8, "text": " this ising model of language, and obviously the temperature determines how big these clusters", "tokens": [51012, 341, 307, 278, 2316, 295, 2856, 11, 293, 2745, 264, 4292, 24799, 577, 955, 613, 23313, 51252], "temperature": 0.0, "avg_logprob": -0.11289470711934198, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.009581356309354305}, {"id": 553, "seek": 308704, "start": 3104.8, "end": 3108.24, "text": " are and so on. And that's something that you might try and fit against some knowledge of", "tokens": [51252, 366, 293, 370, 322, 13, 400, 300, 311, 746, 300, 291, 1062, 853, 293, 3318, 1970, 512, 3601, 295, 51424], "temperature": 0.0, "avg_logprob": -0.11289470711934198, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.009581356309354305}, {"id": 554, "seek": 308704, "start": 3108.24, "end": 3114.24, "text": " the distribution of languages. But the problem, as you could probably anticipate, occurs here,", "tokens": [51424, 264, 7316, 295, 8650, 13, 583, 264, 1154, 11, 382, 291, 727, 1391, 21685, 11, 11843, 510, 11, 51724], "temperature": 0.0, "avg_logprob": -0.11289470711934198, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.009581356309354305}, {"id": 555, "seek": 311424, "start": 3114.24, "end": 3118.16, "text": " which is the Basque country. And if you, if you instead of walking from", "tokens": [50364, 597, 307, 264, 5859, 1077, 1941, 13, 400, 498, 291, 11, 498, 291, 2602, 295, 4494, 490, 50560], "temperature": 0.0, "avg_logprob": -0.16299892457063533, "compression_ratio": 1.7099236641221374, "no_speech_prob": 0.031194575130939484}, {"id": 556, "seek": 311424, "start": 3118.16, "end": 3123.2799999999997, "text": " Portugal through Galician and Castilian and Arganese, you kept along the coast as well,", "tokens": [50560, 23011, 807, 7336, 9027, 293, 11019, 48666, 293, 1587, 1275, 1130, 11, 291, 4305, 2051, 264, 8684, 382, 731, 11, 50816], "temperature": 0.0, "avg_logprob": -0.16299892457063533, "compression_ratio": 1.7099236641221374, "no_speech_prob": 0.031194575130939484}, {"id": 557, "seek": 311424, "start": 3123.2799999999997, "end": 3127.4399999999996, "text": " you might, particularly if you're a pilgrim. Well, if you're a pilgrim, you're going the other way.", "tokens": [50816, 291, 1062, 11, 4098, 498, 291, 434, 257, 30760, 332, 13, 1042, 11, 498, 291, 434, 257, 30760, 332, 11, 291, 434, 516, 264, 661, 636, 13, 51024], "temperature": 0.0, "avg_logprob": -0.16299892457063533, "compression_ratio": 1.7099236641221374, "no_speech_prob": 0.031194575130939484}, {"id": 558, "seek": 311424, "start": 3127.4399999999996, "end": 3132.3999999999996, "text": " But either way, if you strayed into the Basque country, you would encounter a linguistic barrier.", "tokens": [51024, 583, 2139, 636, 11, 498, 291, 36219, 292, 666, 264, 5859, 1077, 1941, 11, 291, 576, 8593, 257, 43002, 13357, 13, 51272], "temperature": 0.0, "avg_logprob": -0.16299892457063533, "compression_ratio": 1.7099236641221374, "no_speech_prob": 0.031194575130939484}, {"id": 559, "seek": 311424, "start": 3133.2799999999997, "end": 3138.08, "text": " Basque is quite, quite different from, it's not an Indo-European language. This is a sign,", "tokens": [51316, 5859, 1077, 307, 1596, 11, 1596, 819, 490, 11, 309, 311, 406, 364, 46489, 12, 32293, 282, 2856, 13, 639, 307, 257, 1465, 11, 51556], "temperature": 0.0, "avg_logprob": -0.16299892457063533, "compression_ratio": 1.7099236641221374, "no_speech_prob": 0.031194575130939484}, {"id": 560, "seek": 313808, "start": 3138.72, "end": 3148.16, "text": " remarkably, in French Gascon, which is a Pocotin language. And Basque, the Basque is this here.", "tokens": [50396, 37381, 11, 294, 5522, 24025, 1671, 11, 597, 307, 257, 430, 905, 310, 259, 2856, 13, 400, 5859, 1077, 11, 264, 5859, 1077, 307, 341, 510, 13, 50868], "temperature": 0.0, "avg_logprob": -0.16155704498291015, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.02792137861251831}, {"id": 561, "seek": 313808, "start": 3148.16, "end": 3152.48, "text": " And you can see that it's very different from the others. This is Basque here as well.", "tokens": [50868, 400, 291, 393, 536, 300, 309, 311, 588, 819, 490, 264, 2357, 13, 639, 307, 5859, 1077, 510, 382, 731, 13, 51084], "temperature": 0.0, "avg_logprob": -0.16155704498291015, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.02792137861251831}, {"id": 562, "seek": 313808, "start": 3154.16, "end": 3160.0, "text": " And so there is, in real languages, a language barrier. There is the possibility of language", "tokens": [51168, 400, 370, 456, 307, 11, 294, 957, 8650, 11, 257, 2856, 13357, 13, 821, 307, 264, 7959, 295, 2856, 51460], "temperature": 0.0, "avg_logprob": -0.16155704498291015, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.02792137861251831}, {"id": 563, "seek": 313808, "start": 3160.0, "end": 3166.08, "text": " barriers. And also, of course, you know that we don't have to be able to talk to everybody.", "tokens": [51460, 13565, 13, 400, 611, 11, 295, 1164, 11, 291, 458, 300, 321, 500, 380, 362, 281, 312, 1075, 281, 751, 281, 2201, 13, 51764], "temperature": 0.0, "avg_logprob": -0.16155704498291015, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.02792137861251831}, {"id": 564, "seek": 316608, "start": 3166.88, "end": 3171.7599999999998, "text": " You know, we, we can have neighbors who speak a different language. And we can talk to them", "tokens": [50404, 509, 458, 11, 321, 11, 321, 393, 362, 12512, 567, 1710, 257, 819, 2856, 13, 400, 321, 393, 751, 281, 552, 50648], "temperature": 0.0, "avg_logprob": -0.16118251659252025, "compression_ratio": 1.8288590604026846, "no_speech_prob": 0.004725984763354063}, {"id": 565, "seek": 316608, "start": 3171.7599999999998, "end": 3176.4, "text": " without necessarily aligning our language to theirs. We can speak to them, for example,", "tokens": [50648, 1553, 4725, 419, 9676, 527, 2856, 281, 22760, 13, 492, 393, 1710, 281, 552, 11, 337, 1365, 11, 50880], "temperature": 0.0, "avg_logprob": -0.16118251659252025, "compression_ratio": 1.8288590604026846, "no_speech_prob": 0.004725984763354063}, {"id": 566, "seek": 316608, "start": 3176.4, "end": 3180.72, "text": " through a lingua franca. These days, using translation by using somebody who's bilingual", "tokens": [50880, 807, 257, 22949, 4398, 431, 40835, 13, 1981, 1708, 11, 1228, 12853, 538, 1228, 2618, 567, 311, 48757, 51096], "temperature": 0.0, "avg_logprob": -0.16118251659252025, "compression_ratio": 1.8288590604026846, "no_speech_prob": 0.004725984763354063}, {"id": 567, "seek": 316608, "start": 3180.72, "end": 3185.44, "text": " or being bilingual ourselves. This is an example of Kiswahili, which is a language that, you know,", "tokens": [51096, 420, 885, 48757, 4175, 13, 639, 307, 364, 1365, 295, 591, 271, 37360, 2312, 11, 597, 307, 257, 2856, 300, 11, 291, 458, 11, 51332], "temperature": 0.0, "avg_logprob": -0.16118251659252025, "compression_ratio": 1.8288590604026846, "no_speech_prob": 0.004725984763354063}, {"id": 568, "seek": 316608, "start": 3185.44, "end": 3190.0, "text": " 100 million people, that's not a bit much, a large number of 10 million people can speak", "tokens": [51332, 2319, 2459, 561, 11, 300, 311, 406, 257, 857, 709, 11, 257, 2416, 1230, 295, 1266, 2459, 561, 393, 1710, 51560], "temperature": 0.0, "avg_logprob": -0.16118251659252025, "compression_ratio": 1.8288590604026846, "no_speech_prob": 0.004725984763354063}, {"id": 569, "seek": 316608, "start": 3190.0, "end": 3194.4, "text": " as a second language, but only a million as a first language. And because it exists as a", "tokens": [51560, 382, 257, 1150, 2856, 11, 457, 787, 257, 2459, 382, 257, 700, 2856, 13, 400, 570, 309, 8198, 382, 257, 51780], "temperature": 0.0, "avg_logprob": -0.16118251659252025, "compression_ratio": 1.8288590604026846, "no_speech_prob": 0.004725984763354063}, {"id": 570, "seek": 319440, "start": 3194.4, "end": 3198.88, "text": " lingua franca, allowing people who don't have a mutual language to live beside each other.", "tokens": [50364, 22949, 4398, 431, 40835, 11, 8293, 561, 567, 500, 380, 362, 257, 16917, 2856, 281, 1621, 15726, 1184, 661, 13, 50588], "temperature": 0.0, "avg_logprob": -0.12092850717265954, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.004948682617396116}, {"id": 571, "seek": 319440, "start": 3198.88, "end": 3206.2400000000002, "text": " So the next version of the language evolution model is this preference sizing model. So the idea", "tokens": [50588, 407, 264, 958, 3037, 295, 264, 2856, 9303, 2316, 307, 341, 17502, 45435, 2316, 13, 407, 264, 1558, 50956], "temperature": 0.0, "avg_logprob": -0.12092850717265954, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.004948682617396116}, {"id": 572, "seek": 319440, "start": 3206.2400000000002, "end": 3215.12, "text": " here is to run the same sort of dynamics, alignment and thermal change, but only allow or only have", "tokens": [50956, 510, 307, 281, 1190, 264, 912, 1333, 295, 15679, 11, 18515, 293, 15070, 1319, 11, 457, 787, 2089, 420, 787, 362, 51400], "temperature": 0.0, "avg_logprob": -0.12092850717265954, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.004948682617396116}, {"id": 573, "seek": 319440, "start": 3215.12, "end": 3220.7200000000003, "text": " each side interact with which of the ever of the four sides around it, in the simplest case,", "tokens": [51400, 1184, 1252, 4648, 365, 597, 295, 264, 1562, 295, 264, 1451, 4881, 926, 309, 11, 294, 264, 22811, 1389, 11, 51680], "temperature": 0.0, "avg_logprob": -0.12092850717265954, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.004948682617396116}, {"id": 574, "seek": 322072, "start": 3220.72, "end": 3227.8399999999997, "text": " has the most similar language to its own. So, so the idea is that for each of the, for each pair", "tokens": [50364, 575, 264, 881, 2531, 2856, 281, 1080, 1065, 13, 407, 11, 370, 264, 1558, 307, 300, 337, 1184, 295, 264, 11, 337, 1184, 6119, 50720], "temperature": 0.0, "avg_logprob": -0.10265982254691745, "compression_ratio": 1.828793774319066, "no_speech_prob": 0.006889763288199902}, {"id": 575, "seek": 322072, "start": 3227.8399999999997, "end": 3232.8799999999997, "text": " of, of sites, each pair of speakers, I guess, you can work out the difference between their", "tokens": [50720, 295, 11, 295, 7533, 11, 1184, 6119, 295, 9518, 11, 286, 2041, 11, 291, 393, 589, 484, 264, 2649, 1296, 641, 50972], "temperature": 0.0, "avg_logprob": -0.10265982254691745, "compression_ratio": 1.828793774319066, "no_speech_prob": 0.006889763288199902}, {"id": 576, "seek": 322072, "start": 3232.8799999999997, "end": 3237.7599999999998, "text": " languages. And then you could, you need to, you only run the sizing model between the speaker", "tokens": [50972, 8650, 13, 400, 550, 291, 727, 11, 291, 643, 281, 11, 291, 787, 1190, 264, 45435, 2316, 1296, 264, 8145, 51216], "temperature": 0.0, "avg_logprob": -0.10265982254691745, "compression_ratio": 1.828793774319066, "no_speech_prob": 0.006889763288199902}, {"id": 577, "seek": 322072, "start": 3237.7599999999998, "end": 3243.4399999999996, "text": " you've randomly chosen to consider changing one of their plus ones and minus ones, and whichever", "tokens": [51216, 291, 600, 16979, 8614, 281, 1949, 4473, 472, 295, 641, 1804, 2306, 293, 3175, 2306, 11, 293, 24123, 51500], "temperature": 0.0, "avg_logprob": -0.10265982254691745, "compression_ratio": 1.828793774319066, "no_speech_prob": 0.006889763288199902}, {"id": 578, "seek": 322072, "start": 3243.4399999999996, "end": 3247.6, "text": " neighbor is closest to it. And so that is the idea being that you only speak to the people", "tokens": [51500, 5987, 307, 13699, 281, 309, 13, 400, 370, 300, 307, 264, 1558, 885, 300, 291, 787, 1710, 281, 264, 561, 51708], "temperature": 0.0, "avg_logprob": -0.10265982254691745, "compression_ratio": 1.828793774319066, "no_speech_prob": 0.006889763288199902}, {"id": 579, "seek": 324760, "start": 3247.6, "end": 3251.8399999999997, "text": " who speak the same language as you. And so that's the new version, there's a paper about it there,", "tokens": [50364, 567, 1710, 264, 912, 2856, 382, 291, 13, 400, 370, 300, 311, 264, 777, 3037, 11, 456, 311, 257, 3035, 466, 309, 456, 11, 50576], "temperature": 0.0, "avg_logprob": -0.1281047643617142, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.006846048403531313}, {"id": 580, "seek": 324760, "start": 3251.8399999999997, "end": 3259.44, "text": " in ALI. And it kind of works. So there's lots more to be done on this, but this is, this is the basic", "tokens": [50576, 294, 7056, 40, 13, 400, 309, 733, 295, 1985, 13, 407, 456, 311, 3195, 544, 281, 312, 1096, 322, 341, 11, 457, 341, 307, 11, 341, 307, 264, 3875, 50956], "temperature": 0.0, "avg_logprob": -0.1281047643617142, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.006846048403531313}, {"id": 581, "seek": 324760, "start": 3259.44, "end": 3266.16, "text": " idea. This is the originalizing model. And here is a histogram of, so this is only five dimensions,", "tokens": [50956, 1558, 13, 639, 307, 264, 3380, 3319, 2316, 13, 400, 510, 307, 257, 49816, 295, 11, 370, 341, 307, 787, 1732, 12819, 11, 51292], "temperature": 0.0, "avg_logprob": -0.1281047643617142, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.006846048403531313}, {"id": 582, "seek": 324760, "start": 3266.16, "end": 3270.3199999999997, "text": " you can run it in far more, you know, five different language attributes, you can run in", "tokens": [51292, 291, 393, 1190, 309, 294, 1400, 544, 11, 291, 458, 11, 1732, 819, 2856, 17212, 11, 291, 393, 1190, 294, 51500], "temperature": 0.0, "avg_logprob": -0.1281047643617142, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.006846048403531313}, {"id": 583, "seek": 324760, "start": 3270.3199999999997, "end": 3276.96, "text": " far higher dimensions. But in the case of the first sizing model, you can see that this is the", "tokens": [51500, 1400, 2946, 12819, 13, 583, 294, 264, 1389, 295, 264, 700, 45435, 2316, 11, 291, 393, 536, 300, 341, 307, 264, 51832], "temperature": 0.0, "avg_logprob": -0.1281047643617142, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.006846048403531313}, {"id": 584, "seek": 327696, "start": 3276.96, "end": 3281.12, "text": " number of different, average number of differences between the speaker and their neighbor. And you", "tokens": [50364, 1230, 295, 819, 11, 4274, 1230, 295, 7300, 1296, 264, 8145, 293, 641, 5987, 13, 400, 291, 50572], "temperature": 0.0, "avg_logprob": -0.10027239799499511, "compression_ratio": 1.8455598455598456, "no_speech_prob": 0.002949951682239771}, {"id": 585, "seek": 327696, "start": 3281.12, "end": 3287.2, "text": " can see that the speakers and their neighbors tend to have very similar languages. And very few", "tokens": [50572, 393, 536, 300, 264, 9518, 293, 641, 12512, 3928, 281, 362, 588, 2531, 8650, 13, 400, 588, 1326, 50876], "temperature": 0.0, "avg_logprob": -0.10027239799499511, "compression_ratio": 1.8455598455598456, "no_speech_prob": 0.002949951682239771}, {"id": 586, "seek": 327696, "start": 3287.2, "end": 3293.2, "text": " people, very few pairs of people have languages that have very little in common. In other words,", "tokens": [50876, 561, 11, 588, 1326, 15494, 295, 561, 362, 8650, 300, 362, 588, 707, 294, 2689, 13, 682, 661, 2283, 11, 51176], "temperature": 0.0, "avg_logprob": -0.10027239799499511, "compression_ratio": 1.8455598455598456, "no_speech_prob": 0.002949951682239771}, {"id": 587, "seek": 327696, "start": 3293.2, "end": 3298.0, "text": " this version of the model does not allow for linguistic borders. Whereas in this preference", "tokens": [51176, 341, 3037, 295, 264, 2316, 775, 406, 2089, 337, 43002, 16287, 13, 13813, 294, 341, 17502, 51416], "temperature": 0.0, "avg_logprob": -0.10027239799499511, "compression_ratio": 1.8455598455598456, "no_speech_prob": 0.002949951682239771}, {"id": 588, "seek": 327696, "start": 3298.0, "end": 3304.2400000000002, "text": " model, where a comparison, the dynamics is only relative to whichever neighbor has the closest", "tokens": [51416, 2316, 11, 689, 257, 9660, 11, 264, 15679, 307, 787, 4972, 281, 24123, 5987, 575, 264, 13699, 51728], "temperature": 0.0, "avg_logprob": -0.10027239799499511, "compression_ratio": 1.8455598455598456, "no_speech_prob": 0.002949951682239771}, {"id": 589, "seek": 330424, "start": 3304.24, "end": 3308.9599999999996, "text": " language to you, you do have lots of pairs where people speak the same language or very", "tokens": [50364, 2856, 281, 291, 11, 291, 360, 362, 3195, 295, 15494, 689, 561, 1710, 264, 912, 2856, 420, 588, 50600], "temperature": 0.0, "avg_logprob": -0.10188093530126364, "compression_ratio": 1.8487804878048781, "no_speech_prob": 0.005597847513854504}, {"id": 590, "seek": 330424, "start": 3308.9599999999996, "end": 3313.4399999999996, "text": " similar languages, but you do have pairs where people speak very different languages. So this is", "tokens": [50600, 2531, 8650, 11, 457, 291, 360, 362, 15494, 689, 561, 1710, 588, 819, 8650, 13, 407, 341, 307, 50824], "temperature": 0.0, "avg_logprob": -0.10188093530126364, "compression_ratio": 1.8487804878048781, "no_speech_prob": 0.005597847513854504}, {"id": 591, "seek": 330424, "start": 3313.4399999999996, "end": 3321.12, "text": " the, this is my proposal of the simplest possible model of language evolution. This work has only", "tokens": [50824, 264, 11, 341, 307, 452, 11494, 295, 264, 22811, 1944, 2316, 295, 2856, 9303, 13, 639, 589, 575, 787, 51208], "temperature": 0.0, "avg_logprob": -0.10188093530126364, "compression_ratio": 1.8487804878048781, "no_speech_prob": 0.005597847513854504}, {"id": 592, "seek": 330424, "start": 3321.12, "end": 3329.3599999999997, "text": " just started, but the idea is to consider, you know, consider different structures of preference", "tokens": [51208, 445, 1409, 11, 457, 264, 1558, 307, 281, 1949, 11, 291, 458, 11, 1949, 819, 9227, 295, 17502, 51620], "temperature": 0.0, "avg_logprob": -0.10188093530126364, "compression_ratio": 1.8487804878048781, "no_speech_prob": 0.005597847513854504}, {"id": 593, "seek": 332936, "start": 3329.36, "end": 3335.6800000000003, "text": " and how many neighbors you interact with and so on, and then try and find different temperatures", "tokens": [50364, 293, 577, 867, 12512, 291, 4648, 365, 293, 370, 322, 11, 293, 550, 853, 293, 915, 819, 12633, 50680], "temperature": 0.0, "avg_logprob": -0.16006623143735138, "compression_ratio": 1.5519125683060109, "no_speech_prob": 0.009515573270618916}, {"id": 594, "seek": 332936, "start": 3335.6800000000003, "end": 3340.8, "text": " and then find the structure of the size of language groups there and compare it to real", "tokens": [50680, 293, 550, 915, 264, 3877, 295, 264, 2744, 295, 2856, 3935, 456, 293, 6794, 309, 281, 957, 50936], "temperature": 0.0, "avg_logprob": -0.16006623143735138, "compression_ratio": 1.5519125683060109, "no_speech_prob": 0.009515573270618916}, {"id": 595, "seek": 332936, "start": 3340.8, "end": 3345.6800000000003, "text": " day clear. And so that's something that we'll do in the future. And that's it. Thank you very much.", "tokens": [50936, 786, 1850, 13, 400, 370, 300, 311, 746, 300, 321, 603, 360, 294, 264, 2027, 13, 400, 300, 311, 309, 13, 1044, 291, 588, 709, 13, 51180], "temperature": 0.0, "avg_logprob": -0.16006623143735138, "compression_ratio": 1.5519125683060109, "no_speech_prob": 0.009515573270618916}, {"id": 596, "seek": 334568, "start": 3345.8399999999997, "end": 3361.04, "text": " So I was assuming throughout this presentation that the bottleneck in the iterative model kind of", "tokens": [50372, 407, 286, 390, 11926, 3710, 341, 5860, 300, 264, 44641, 547, 294, 264, 17138, 1166, 2316, 733, 295, 51132], "temperature": 0.0, "avg_logprob": -0.1629034070407643, "compression_ratio": 1.5164835164835164, "no_speech_prob": 0.01970602013170719}, {"id": 597, "seek": 334568, "start": 3361.04, "end": 3366.3199999999997, "text": " corresponds to Chomsky's poverty of stimulus argument about language learning. Is that", "tokens": [51132, 23249, 281, 761, 4785, 4133, 311, 10958, 295, 21366, 6770, 466, 2856, 2539, 13, 1119, 300, 51396], "temperature": 0.0, "avg_logprob": -0.1629034070407643, "compression_ratio": 1.5164835164835164, "no_speech_prob": 0.01970602013170719}, {"id": 598, "seek": 334568, "start": 3367.6, "end": 3373.9199999999996, "text": " right? Corresponds to, I mean, it's, it's, it's meant to be representative of the fact that", "tokens": [51460, 558, 30, 3925, 6663, 82, 281, 11, 286, 914, 11, 309, 311, 11, 309, 311, 11, 309, 311, 4140, 281, 312, 12424, 295, 264, 1186, 300, 51776], "temperature": 0.0, "avg_logprob": -0.1629034070407643, "compression_ratio": 1.5164835164835164, "no_speech_prob": 0.01970602013170719}, {"id": 599, "seek": 337392, "start": 3374.8, "end": 3380.2400000000002, "text": " people, children only like experience a very limited amount of, but it adds something extra", "tokens": [50408, 561, 11, 2227, 787, 411, 1752, 257, 588, 5567, 2372, 295, 11, 457, 309, 10860, 746, 2857, 50680], "temperature": 0.0, "avg_logprob": -0.12583242995398386, "compression_ratio": 1.8352941176470587, "no_speech_prob": 0.001387836062349379}, {"id": 600, "seek": 337392, "start": 3380.2400000000002, "end": 3386.88, "text": " to what Chomsky says. So Chomsky uses the poverty of stimulus as evidence that the brain must have,", "tokens": [50680, 281, 437, 761, 4785, 4133, 1619, 13, 407, 761, 4785, 4133, 4960, 264, 10958, 295, 21366, 382, 4467, 300, 264, 3567, 1633, 362, 11, 51012], "temperature": 0.0, "avg_logprob": -0.12583242995398386, "compression_ratio": 1.8352941176470587, "no_speech_prob": 0.001387836062349379}, {"id": 601, "seek": 337392, "start": 3386.88, "end": 3391.76, "text": " you know, linguistic things, linguistic things. Whereas here, the poverty of, the poverty of", "tokens": [51012, 291, 458, 11, 43002, 721, 11, 43002, 721, 13, 13813, 510, 11, 264, 10958, 295, 11, 264, 10958, 295, 51256], "temperature": 0.0, "avg_logprob": -0.12583242995398386, "compression_ratio": 1.8352941176470587, "no_speech_prob": 0.001387836062349379}, {"id": 602, "seek": 337392, "start": 3391.76, "end": 3398.0, "text": " stimulus is a mechanism for, it says that the structure of language is a response to the", "tokens": [51256, 21366, 307, 257, 7513, 337, 11, 309, 1619, 300, 264, 3877, 295, 2856, 307, 257, 4134, 281, 264, 51568], "temperature": 0.0, "avg_logprob": -0.12583242995398386, "compression_ratio": 1.8352941176470587, "no_speech_prob": 0.001387836062349379}, {"id": 603, "seek": 337392, "start": 3398.0, "end": 3402.08, "text": " poverty of stimulus. So it changes that. I mean, that's what I think is really nice about this", "tokens": [51568, 10958, 295, 21366, 13, 407, 309, 2962, 300, 13, 286, 914, 11, 300, 311, 437, 286, 519, 307, 534, 1481, 466, 341, 51772], "temperature": 0.0, "avg_logprob": -0.12583242995398386, "compression_ratio": 1.8352941176470587, "no_speech_prob": 0.001387836062349379}, {"id": 604, "seek": 340208, "start": 3402.08, "end": 3407.2, "text": " model. It turns that language upside down and says that in fact, it's not that the brain has some", "tokens": [50364, 2316, 13, 467, 4523, 300, 2856, 14119, 760, 293, 1619, 300, 294, 1186, 11, 309, 311, 406, 300, 264, 3567, 575, 512, 50620], "temperature": 0.0, "avg_logprob": -0.1033419422481371, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.008835038170218468}, {"id": 605, "seek": 340208, "start": 3407.2, "end": 3414.64, "text": " special language mechanism, it's that language has, has evolved so that we can generalize from a,", "tokens": [50620, 2121, 2856, 7513, 11, 309, 311, 300, 2856, 575, 11, 575, 14178, 370, 300, 321, 393, 2674, 1125, 490, 257, 11, 50992], "temperature": 0.0, "avg_logprob": -0.1033419422481371, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.008835038170218468}, {"id": 606, "seek": 340208, "start": 3414.64, "end": 3420.72, "text": " from a poverty of stimulus. I was interested in, because I can imagine a more innate style person", "tokens": [50992, 490, 257, 10958, 295, 21366, 13, 286, 390, 3102, 294, 11, 570, 286, 393, 3811, 257, 544, 41766, 3758, 954, 51296], "temperature": 0.0, "avg_logprob": -0.1033419422481371, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.008835038170218468}, {"id": 607, "seek": 340208, "start": 3421.84, "end": 3429.92, "text": " saying, you know, it's this convergence from the statistics to just a definite mapping in the", "tokens": [51352, 1566, 11, 291, 458, 11, 309, 311, 341, 32181, 490, 264, 12523, 281, 445, 257, 25131, 18350, 294, 264, 51756], "temperature": 0.0, "avg_logprob": -0.1033419422481371, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.008835038170218468}, {"id": 608, "seek": 342992, "start": 3430.0, "end": 3436.4, "text": " aversion process that is leading to compositionality, but maybe there is some sort of innate", "tokens": [50368, 257, 29153, 1399, 300, 307, 5775, 281, 12686, 1860, 11, 457, 1310, 456, 307, 512, 1333, 295, 41766, 50688], "temperature": 0.0, "avg_logprob": -0.11808789494525955, "compression_ratio": 1.6407766990291262, "no_speech_prob": 0.0039437380619347095}, {"id": 609, "seek": 342992, "start": 3436.4, "end": 3441.44, "text": " mechanism, which is responsible for something that has the same effect as aversion.", "tokens": [50688, 7513, 11, 597, 307, 6250, 337, 746, 300, 575, 264, 912, 1802, 382, 257, 29153, 13, 50940], "temperature": 0.0, "avg_logprob": -0.11808789494525955, "compression_ratio": 1.6407766990291262, "no_speech_prob": 0.0039437380619347095}, {"id": 610, "seek": 342992, "start": 3443.04, "end": 3447.04, "text": " Yeah, no, but do you think that the, the work you're doing with this model,", "tokens": [51020, 865, 11, 572, 11, 457, 360, 291, 519, 300, 264, 11, 264, 589, 291, 434, 884, 365, 341, 2316, 11, 51220], "temperature": 0.0, "avg_logprob": -0.11808789494525955, "compression_ratio": 1.6407766990291262, "no_speech_prob": 0.0039437380619347095}, {"id": 611, "seek": 342992, "start": 3448.08, "end": 3454.8, "text": " is there something that makes you lean more towards the, the empiricist take on this?", "tokens": [51272, 307, 456, 746, 300, 1669, 291, 11659, 544, 3030, 264, 11, 264, 25790, 299, 468, 747, 322, 341, 30, 51608], "temperature": 0.0, "avg_logprob": -0.11808789494525955, "compression_ratio": 1.6407766990291262, "no_speech_prob": 0.0039437380619347095}, {"id": 612, "seek": 345480, "start": 3455.6800000000003, "end": 3464.0800000000004, "text": " So, I mean, I think one of, one of the things that we're learning at the moment, you know,", "tokens": [50408, 407, 11, 286, 914, 11, 286, 519, 472, 295, 11, 472, 295, 264, 721, 300, 321, 434, 2539, 412, 264, 1623, 11, 291, 458, 11, 50828], "temperature": 0.0, "avg_logprob": -0.17924068398671608, "compression_ratio": 1.6436781609195403, "no_speech_prob": 0.0013839919120073318}, {"id": 613, "seek": 345480, "start": 3467.36, "end": 3471.76, "text": " you know, obviously, you know, the objection to transformers as a model of language is that", "tokens": [50992, 291, 458, 11, 2745, 11, 291, 458, 11, 264, 35756, 281, 4088, 433, 382, 257, 2316, 295, 2856, 307, 300, 51212], "temperature": 0.0, "avg_logprob": -0.17924068398671608, "compression_ratio": 1.6436781609195403, "no_speech_prob": 0.0013839919120073318}, {"id": 614, "seek": 345480, "start": 3471.76, "end": 3479.2000000000003, "text": " they require this massive stimulus. But conversely, they, they, they are very sophisticated at learning", "tokens": [51212, 436, 3651, 341, 5994, 21366, 13, 583, 2615, 736, 11, 436, 11, 436, 11, 436, 366, 588, 16950, 412, 2539, 51584], "temperature": 0.0, "avg_logprob": -0.17924068398671608, "compression_ratio": 1.6436781609195403, "no_speech_prob": 0.0013839919120073318}, {"id": 615, "seek": 347920, "start": 3479.2, "end": 3485.4399999999996, "text": " grammar. You can see that if you back away from the, from the learning aspect, and you just look", "tokens": [50364, 22317, 13, 509, 393, 536, 300, 498, 291, 646, 1314, 490, 264, 11, 490, 264, 2539, 4171, 11, 293, 291, 445, 574, 50676], "temperature": 0.0, "avg_logprob": -0.08083635163538665, "compression_ratio": 1.5691056910569106, "no_speech_prob": 0.014815659262239933}, {"id": 616, "seek": 347920, "start": 3485.4399999999996, "end": 3491.7599999999998, "text": " at the ability of these models to perform grammatical tasks, it's quite incredible. We do, we do, you", "tokens": [50676, 412, 264, 3485, 295, 613, 5245, 281, 2042, 17570, 267, 804, 9608, 11, 309, 311, 1596, 4651, 13, 492, 360, 11, 321, 360, 11, 291, 50992], "temperature": 0.0, "avg_logprob": -0.08083635163538665, "compression_ratio": 1.5691056910569106, "no_speech_prob": 0.014815659262239933}, {"id": 617, "seek": 347920, "start": 3491.7599999999998, "end": 3497.6, "text": " know, we do do experiments now where we ask, you know, a transformer or an LSTM or something,", "tokens": [50992, 458, 11, 321, 360, 360, 12050, 586, 689, 321, 1029, 11, 291, 458, 11, 257, 31782, 420, 364, 441, 6840, 44, 420, 746, 11, 51284], "temperature": 0.0, "avg_logprob": -0.08083635163538665, "compression_ratio": 1.5691056910569106, "no_speech_prob": 0.014815659262239933}, {"id": 618, "seek": 347920, "start": 3497.6, "end": 3504.56, "text": " can you learn gender agreement? And so I have a student Priyanka who's teaching new words to,", "tokens": [51284, 393, 291, 1466, 7898, 8106, 30, 400, 370, 286, 362, 257, 3107, 8087, 88, 21729, 567, 311, 4571, 777, 2283, 281, 11, 51632], "temperature": 0.0, "avg_logprob": -0.08083635163538665, "compression_ratio": 1.5691056910569106, "no_speech_prob": 0.014815659262239933}, {"id": 619, "seek": 350456, "start": 3504.56, "end": 3509.2799999999997, "text": " to an LSTM. So she takes an LSTM, it's pre-trained. She freezes everything but the", "tokens": [50364, 281, 364, 441, 6840, 44, 13, 407, 750, 2516, 364, 441, 6840, 44, 11, 309, 311, 659, 12, 17227, 2001, 13, 1240, 1737, 12214, 1203, 457, 264, 50600], "temperature": 0.0, "avg_logprob": -0.12871678173542023, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.03831038996577263}, {"id": 620, "seek": 350456, "start": 3509.2799999999997, "end": 3516.08, "text": " representations. She teaches it a new word. She gives it some grammatical context, gender context.", "tokens": [50600, 33358, 13, 1240, 16876, 309, 257, 777, 1349, 13, 1240, 2709, 309, 512, 17570, 267, 804, 4319, 11, 7898, 4319, 13, 50940], "temperature": 0.0, "avg_logprob": -0.12871678173542023, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.03831038996577263}, {"id": 621, "seek": 350456, "start": 3516.08, "end": 3522.08, "text": " So she doesn't French, obviously. So she says, you know, la, we use the word trilobie for some", "tokens": [50940, 407, 750, 1177, 380, 5522, 11, 2745, 13, 407, 750, 1619, 11, 291, 458, 11, 635, 11, 321, 764, 264, 1349, 26120, 996, 414, 337, 512, 51240], "temperature": 0.0, "avg_logprob": -0.12871678173542023, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.03831038996577263}, {"id": 622, "seek": 350456, "start": 3522.08, "end": 3526.32, "text": " reason, because it's the least common word in the vocabulary list that we're using. So we take", "tokens": [51240, 1778, 11, 570, 309, 311, 264, 1935, 2689, 1349, 294, 264, 19864, 1329, 300, 321, 434, 1228, 13, 407, 321, 747, 51452], "temperature": 0.0, "avg_logprob": -0.12871678173542023, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.03831038996577263}, {"id": 623, "seek": 350456, "start": 3526.32, "end": 3530.56, "text": " the word trilobie, we cut off its representations, we reintroduce it somewhere else in the", "tokens": [51452, 264, 1349, 26120, 996, 414, 11, 321, 1723, 766, 1080, 33358, 11, 321, 319, 38132, 384, 309, 4079, 1646, 294, 264, 51664], "temperature": 0.0, "avg_logprob": -0.12871678173542023, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.03831038996577263}, {"id": 624, "seek": 353056, "start": 3530.56, "end": 3536.56, "text": " representation space, and we train just the representation space on a few examples where", "tokens": [50364, 10290, 1901, 11, 293, 321, 3847, 445, 264, 10290, 1901, 322, 257, 1326, 5110, 689, 50664], "temperature": 0.0, "avg_logprob": -0.11144394533974784, "compression_ratio": 1.852, "no_speech_prob": 0.0028072441928088665}, {"id": 625, "seek": 353056, "start": 3536.56, "end": 3541.44, "text": " you use the word la or la, describing trilobie, and then you ask it to do other gender agreement", "tokens": [50664, 291, 764, 264, 1349, 635, 420, 635, 11, 16141, 26120, 996, 414, 11, 293, 550, 291, 1029, 309, 281, 360, 661, 7898, 8106, 50908], "temperature": 0.0, "avg_logprob": -0.11144394533974784, "compression_ratio": 1.852, "no_speech_prob": 0.0028072441928088665}, {"id": 626, "seek": 353056, "start": 3541.44, "end": 3546.4, "text": " tasks and it does it completely well. So even though it's learning one aspect of gender,", "tokens": [50908, 9608, 293, 309, 775, 309, 2584, 731, 13, 407, 754, 1673, 309, 311, 2539, 472, 4171, 295, 7898, 11, 51156], "temperature": 0.0, "avg_logprob": -0.11144394533974784, "compression_ratio": 1.852, "no_speech_prob": 0.0028072441928088665}, {"id": 627, "seek": 353056, "start": 3546.4, "end": 3550.0, "text": " it's been taught the gender of the word through one aspect of gender, and it's been tested on", "tokens": [51156, 309, 311, 668, 5928, 264, 7898, 295, 264, 1349, 807, 472, 4171, 295, 7898, 11, 293, 309, 311, 668, 8246, 322, 51336], "temperature": 0.0, "avg_logprob": -0.11144394533974784, "compression_ratio": 1.852, "no_speech_prob": 0.0028072441928088665}, {"id": 628, "seek": 353056, "start": 3550.0, "end": 3555.36, "text": " another, it has abstracted the, the abstract category of gender. And so, you know, the initial", "tokens": [51336, 1071, 11, 309, 575, 12649, 292, 264, 11, 264, 12649, 7719, 295, 7898, 13, 400, 370, 11, 291, 458, 11, 264, 5883, 51604], "temperature": 0.0, "avg_logprob": -0.11144394533974784, "compression_ratio": 1.852, "no_speech_prob": 0.0028072441928088665}, {"id": 629, "seek": 355536, "start": 3555.76, "end": 3564.96, "text": " bias that you had, that you needed, you needed special mechanisms, you know, Chomsky and", "tokens": [50384, 12577, 300, 291, 632, 11, 300, 291, 2978, 11, 291, 2978, 2121, 15902, 11, 291, 458, 11, 761, 4785, 4133, 293, 50844], "temperature": 0.0, "avg_logprob": -0.16556948080830192, "compression_ratio": 1.5982142857142858, "no_speech_prob": 0.0004378397425170988}, {"id": 630, "seek": 355536, "start": 3565.52, "end": 3571.28, "text": " mechanisms to allow the brain to perform the form of manipulations associated with grammar.", "tokens": [50872, 15902, 281, 2089, 264, 3567, 281, 2042, 264, 1254, 295, 9258, 4136, 6615, 365, 22317, 13, 51160], "temperature": 0.0, "avg_logprob": -0.16556948080830192, "compression_ratio": 1.5982142857142858, "no_speech_prob": 0.0004378397425170988}, {"id": 631, "seek": 355536, "start": 3571.28, "end": 3576.56, "text": " That, I think, has been demonstrably made false by, by large language models. They,", "tokens": [51160, 663, 11, 286, 519, 11, 575, 668, 5516, 5305, 356, 1027, 7908, 538, 11, 538, 2416, 2856, 5245, 13, 814, 11, 51424], "temperature": 0.0, "avg_logprob": -0.16556948080830192, "compression_ratio": 1.5982142857142858, "no_speech_prob": 0.0004378397425170988}, {"id": 632, "seek": 355536, "start": 3576.56, "end": 3581.6800000000003, "text": " they have this very, I mean, the structure of transformers seems completely bizarrely crappy,", "tokens": [51424, 436, 362, 341, 588, 11, 286, 914, 11, 264, 3877, 295, 4088, 433, 2544, 2584, 18265, 356, 36531, 11, 51680], "temperature": 0.0, "avg_logprob": -0.16556948080830192, "compression_ratio": 1.5982142857142858, "no_speech_prob": 0.0004378397425170988}, {"id": 633, "seek": 358168, "start": 3581.7599999999998, "end": 3588.64, "text": " right? And yet they do these amazing things. And so, it's quite possible for these, you know,", "tokens": [50368, 558, 30, 400, 1939, 436, 360, 613, 2243, 721, 13, 400, 370, 11, 309, 311, 1596, 1944, 337, 613, 11, 291, 458, 11, 50712], "temperature": 0.0, "avg_logprob": -0.10073377945843864, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.0036976828705519438}, {"id": 634, "seek": 358168, "start": 3588.64, "end": 3593.12, "text": " simple network models that are doing statistical learning to learn this stuff.", "tokens": [50712, 2199, 3209, 5245, 300, 366, 884, 22820, 2539, 281, 1466, 341, 1507, 13, 50936], "temperature": 0.0, "avg_logprob": -0.10073377945843864, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.0036976828705519438}, {"id": 635, "seek": 358168, "start": 3594.3999999999996, "end": 3599.04, "text": " But that's not the same as saying that it can learn it against a poverty of stimulus.", "tokens": [51000, 583, 300, 311, 406, 264, 912, 382, 1566, 300, 309, 393, 1466, 309, 1970, 257, 10958, 295, 21366, 13, 51232], "temperature": 0.0, "avg_logprob": -0.10073377945843864, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.0036976828705519438}, {"id": 636, "seek": 358168, "start": 3602.64, "end": 3606.48, "text": " But what I think the iterated language model is maybe indicating, and again, we need to start", "tokens": [51412, 583, 437, 286, 519, 264, 17138, 770, 2856, 2316, 307, 1310, 25604, 11, 293, 797, 11, 321, 643, 281, 722, 51604], "temperature": 0.0, "avg_logprob": -0.10073377945843864, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.0036976828705519438}, {"id": 637, "seek": 360648, "start": 3606.48, "end": 3611.68, "text": " using the iterated language model on much bigger examples. It's, it's indicating that you don't", "tokens": [50364, 1228, 264, 17138, 770, 2856, 2316, 322, 709, 3801, 5110, 13, 467, 311, 11, 309, 311, 25604, 300, 291, 500, 380, 50624], "temperature": 0.0, "avg_logprob": -0.10627990040352674, "compression_ratio": 1.8127340823970037, "no_speech_prob": 0.014243043959140778}, {"id": 638, "seek": 360648, "start": 3611.68, "end": 3617.68, "text": " have to put much more in, you know, you, you, you, you, once you start thinking about a version or", "tokens": [50624, 362, 281, 829, 709, 544, 294, 11, 291, 458, 11, 291, 11, 291, 11, 291, 11, 291, 11, 1564, 291, 722, 1953, 466, 257, 3037, 420, 50924], "temperature": 0.0, "avg_logprob": -0.10627990040352674, "compression_ratio": 1.8127340823970037, "no_speech_prob": 0.014243043959140778}, {"id": 639, "seek": 360648, "start": 3617.68, "end": 3622.88, "text": " exactly what the objective function is, and the needs to, to generalize, it's possible that it's", "tokens": [50924, 2293, 437, 264, 10024, 2445, 307, 11, 293, 264, 2203, 281, 11, 281, 2674, 1125, 11, 309, 311, 1944, 300, 309, 311, 51184], "temperature": 0.0, "avg_logprob": -0.10627990040352674, "compression_ratio": 1.8127340823970037, "no_speech_prob": 0.014243043959140778}, {"id": 640, "seek": 360648, "start": 3622.88, "end": 3628.08, "text": " not just that the, it's possible that the language that evolved, that you could evolve the language", "tokens": [51184, 406, 445, 300, 264, 11, 309, 311, 1944, 300, 264, 2856, 300, 14178, 11, 300, 291, 727, 16693, 264, 2856, 51444], "temperature": 0.0, "avg_logprob": -0.10627990040352674, "compression_ratio": 1.8127340823970037, "no_speech_prob": 0.014243043959140778}, {"id": 641, "seek": 360648, "start": 3628.08, "end": 3634.16, "text": " where you can learn from a, from a poverty of stimulus. Does that make sense? I said that in", "tokens": [51444, 689, 291, 393, 1466, 490, 257, 11, 490, 257, 10958, 295, 21366, 13, 4402, 300, 652, 2020, 30, 286, 848, 300, 294, 51748], "temperature": 0.0, "avg_logprob": -0.10627990040352674, "compression_ratio": 1.8127340823970037, "no_speech_prob": 0.014243043959140778}, {"id": 642, "seek": 363416, "start": 3634.16, "end": 3637.44, "text": " the very roundabout way, but I mean, I think I'm essentially agreeing with your initial, initial", "tokens": [50364, 264, 588, 3098, 21970, 636, 11, 457, 286, 914, 11, 286, 519, 286, 478, 4476, 36900, 365, 428, 5883, 11, 5883, 50528], "temperature": 0.0, "avg_logprob": -0.21987342834472656, "compression_ratio": 1.6824034334763949, "no_speech_prob": 0.003653909545391798}, {"id": 643, "seek": 363416, "start": 3637.44, "end": 3647.2, "text": " point. Feel where you're coming from. Thank you. Thanks for the lovely talk. I think we could have", "tokens": [50528, 935, 13, 14113, 689, 291, 434, 1348, 490, 13, 1044, 291, 13, 2561, 337, 264, 7496, 751, 13, 286, 519, 321, 727, 362, 51016], "temperature": 0.0, "avg_logprob": -0.21987342834472656, "compression_ratio": 1.6824034334763949, "no_speech_prob": 0.003653909545391798}, {"id": 644, "seek": 363416, "start": 3647.2, "end": 3653.8399999999997, "text": " done this at home some day. No, no, no. But yeah, I was wondering, are you, are you thinking with", "tokens": [51016, 1096, 341, 412, 1280, 512, 786, 13, 883, 11, 572, 11, 572, 13, 583, 1338, 11, 286, 390, 6359, 11, 366, 291, 11, 366, 291, 1953, 365, 51348], "temperature": 0.0, "avg_logprob": -0.21987342834472656, "compression_ratio": 1.6824034334763949, "no_speech_prob": 0.003653909545391798}, {"id": 645, "seek": 363416, "start": 3653.8399999999997, "end": 3659.68, "text": " the Schelling model, are you thinking of maybe looking at the, or maybe already have the different", "tokens": [51348, 264, 2065, 11073, 2316, 11, 366, 291, 1953, 295, 1310, 1237, 412, 264, 11, 420, 1310, 1217, 362, 264, 819, 51640], "temperature": 0.0, "avg_logprob": -0.21987342834472656, "compression_ratio": 1.6824034334763949, "no_speech_prob": 0.003653909545391798}, {"id": 646, "seek": 365968, "start": 3659.68, "end": 3666.8799999999997, "text": " patterns of, like actually taking different patterns of like verb order and, and so on, and", "tokens": [50364, 8294, 295, 11, 411, 767, 1940, 819, 8294, 295, 411, 9595, 1668, 293, 11, 293, 370, 322, 11, 293, 50724], "temperature": 0.0, "avg_logprob": -0.13331088613956532, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.016080321744084358}, {"id": 647, "seek": 365968, "start": 3666.8799999999997, "end": 3672.08, "text": " kind of encoding them and plugging them into this model and seeing what you get out. Because I think,", "tokens": [50724, 733, 295, 43430, 552, 293, 42975, 552, 666, 341, 2316, 293, 2577, 437, 291, 483, 484, 13, 1436, 286, 519, 11, 50984], "temperature": 0.0, "avg_logprob": -0.13331088613956532, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.016080321744084358}, {"id": 648, "seek": 365968, "start": 3672.7999999999997, "end": 3679.52, "text": " I mean, one, like one thing is, I think with, you know, subject verb, object ordering, there are", "tokens": [51020, 286, 914, 11, 472, 11, 411, 472, 551, 307, 11, 286, 519, 365, 11, 291, 458, 11, 3983, 9595, 11, 2657, 21739, 11, 456, 366, 51356], "temperature": 0.0, "avg_logprob": -0.13331088613956532, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.016080321744084358}, {"id": 649, "seek": 365968, "start": 3679.52, "end": 3685.8399999999997, "text": " certain patterns and some patterns are more frequent than others. For example, I don't know.", "tokens": [51356, 1629, 8294, 293, 512, 8294, 366, 544, 18004, 813, 2357, 13, 1171, 1365, 11, 286, 500, 380, 458, 13, 51672], "temperature": 0.0, "avg_logprob": -0.13331088613956532, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.016080321744084358}, {"id": 650, "seek": 368584, "start": 3685.84, "end": 3690.8, "text": " And I guess that wouldn't. I mean, it's, it's very hard to know because we tend to think that the", "tokens": [50364, 400, 286, 2041, 300, 2759, 380, 13, 286, 914, 11, 309, 311, 11, 309, 311, 588, 1152, 281, 458, 570, 321, 3928, 281, 519, 300, 264, 50612], "temperature": 0.0, "avg_logprob": -0.13981873830159505, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.0050056008622050285}, {"id": 651, "seek": 368584, "start": 3690.8, "end": 3694.96, "text": " patterns represented by the Indo-European languages are, are much more common. I mean,", "tokens": [50612, 8294, 10379, 538, 264, 46489, 12, 32293, 282, 8650, 366, 11, 366, 709, 544, 2689, 13, 286, 914, 11, 50820], "temperature": 0.0, "avg_logprob": -0.13981873830159505, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.0050056008622050285}, {"id": 652, "seek": 368584, "start": 3694.96, "end": 3700.8, "text": " just to answer that specific point, it is, you know, so you think, for, well, for example, the,", "tokens": [50820, 445, 281, 1867, 300, 2685, 935, 11, 309, 307, 11, 291, 458, 11, 370, 291, 519, 11, 337, 11, 731, 11, 337, 1365, 11, 264, 11, 51112], "temperature": 0.0, "avg_logprob": -0.13981873830159505, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.0050056008622050285}, {"id": 653, "seek": 368584, "start": 3701.36, "end": 3705.44, "text": " the claim is that verb object is much more common than anything else. But that's only if you can't", "tokens": [51140, 264, 3932, 307, 300, 9595, 2657, 307, 709, 544, 2689, 813, 1340, 1646, 13, 583, 300, 311, 787, 498, 291, 393, 380, 51344], "temperature": 0.0, "avg_logprob": -0.13981873830159505, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.0050056008622050285}, {"id": 654, "seek": 368584, "start": 3705.44, "end": 3710.6400000000003, "text": " buy speakers, if you can't buy languages, that's maybe not so obviously true. Okay. And Irish,", "tokens": [51344, 2256, 9518, 11, 498, 291, 393, 380, 2256, 8650, 11, 300, 311, 1310, 406, 370, 2745, 2074, 13, 1033, 13, 400, 16801, 11, 51604], "temperature": 0.0, "avg_logprob": -0.13981873830159505, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.0050056008622050285}, {"id": 655, "seek": 368584, "start": 3710.6400000000003, "end": 3715.1200000000003, "text": " of course, that was one of my original interests is a language where verb and objects aren't beside", "tokens": [51604, 295, 1164, 11, 300, 390, 472, 295, 452, 3380, 8847, 307, 257, 2856, 689, 9595, 293, 6565, 3212, 380, 15726, 51828], "temperature": 0.0, "avg_logprob": -0.13981873830159505, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.0050056008622050285}, {"id": 656, "seek": 371512, "start": 3715.12, "end": 3722.24, "text": " each other. So, you know, I mean, I think the point with the ISI model is to make the simplest", "tokens": [50364, 1184, 661, 13, 407, 11, 291, 458, 11, 286, 914, 11, 286, 519, 264, 935, 365, 264, 6205, 40, 2316, 307, 281, 652, 264, 22811, 50720], "temperature": 0.0, "avg_logprob": -0.11039554807874892, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.0021817099768668413}, {"id": 657, "seek": 371512, "start": 3722.24, "end": 3727.2799999999997, "text": " possible model. And so we, you know, I don't think you can retain that advantage to the model,", "tokens": [50720, 1944, 2316, 13, 400, 370, 321, 11, 291, 458, 11, 286, 500, 380, 519, 291, 393, 18340, 300, 5002, 281, 264, 2316, 11, 50972], "temperature": 0.0, "avg_logprob": -0.11039554807874892, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.0021817099768668413}, {"id": 658, "seek": 371512, "start": 3727.92, "end": 3733.68, "text": " while at the same time tying your coding to specific features, but rather the idea would be", "tokens": [51004, 1339, 412, 264, 912, 565, 32405, 428, 17720, 281, 2685, 4122, 11, 457, 2831, 264, 1558, 576, 312, 51292], "temperature": 0.0, "avg_logprob": -0.11039554807874892, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.0021817099768668413}, {"id": 659, "seek": 371512, "start": 3733.68, "end": 3742.0, "text": " to introduce into these dynamics some abstract version of these features. So,", "tokens": [51292, 281, 5366, 666, 613, 15679, 512, 12649, 3037, 295, 613, 4122, 13, 407, 11, 51708], "temperature": 0.0, "avg_logprob": -0.11039554807874892, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.0021817099768668413}, {"id": 660, "seek": 374200, "start": 3742.56, "end": 3749.84, "text": " so you could include some sense of consistency, which would be an interaction between the spins.", "tokens": [50392, 370, 291, 727, 4090, 512, 2020, 295, 14416, 11, 597, 576, 312, 364, 9285, 1296, 264, 31587, 13, 50756], "temperature": 0.0, "avg_logprob": -0.14281057274859885, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.0009820141131058335}, {"id": 661, "seek": 374200, "start": 3749.84, "end": 3755.2, "text": " So not only are, is a speaker interacting with the, with their neighbors, but there's also an", "tokens": [50756, 407, 406, 787, 366, 11, 307, 257, 8145, 18017, 365, 264, 11, 365, 641, 12512, 11, 457, 456, 311, 611, 364, 51024], "temperature": 0.0, "avg_logprob": -0.14281057274859885, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.0009820141131058335}, {"id": 662, "seek": 374200, "start": 3755.2, "end": 3760.24, "text": " interaction within the spins themselves. And that would be the idea that once you flip one thing,", "tokens": [51024, 9285, 1951, 264, 31587, 2969, 13, 400, 300, 576, 312, 264, 1558, 300, 1564, 291, 7929, 472, 551, 11, 51276], "temperature": 0.0, "avg_logprob": -0.14281057274859885, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.0009820141131058335}, {"id": 663, "seek": 374200, "start": 3760.24, "end": 3766.8, "text": " other things should flip as well. But, but I think, you know, there's sort of two, two types", "tokens": [51276, 661, 721, 820, 7929, 382, 731, 13, 583, 11, 457, 286, 519, 11, 291, 458, 11, 456, 311, 1333, 295, 732, 11, 732, 3467, 51604], "temperature": 0.0, "avg_logprob": -0.14281057274859885, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.0009820141131058335}, {"id": 664, "seek": 374200, "start": 3766.8, "end": 3769.84, "text": " of models here. There's one that you might, the iterated language model, where you might actually", "tokens": [51604, 295, 5245, 510, 13, 821, 311, 472, 300, 291, 1062, 11, 264, 17138, 770, 2856, 2316, 11, 689, 291, 1062, 767, 51756], "temperature": 0.0, "avg_logprob": -0.14281057274859885, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.0009820141131058335}, {"id": 665, "seek": 376984, "start": 3769.84, "end": 3779.52, "text": " try and use that to probe actual properties of actual languages. But with the difficulty that,", "tokens": [50364, 853, 293, 764, 300, 281, 22715, 3539, 7221, 295, 3539, 8650, 13, 583, 365, 264, 10360, 300, 11, 50848], "temperature": 0.0, "avg_logprob": -0.10270076885558012, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.002346646972000599}, {"id": 666, "seek": 376984, "start": 3780.7200000000003, "end": 3787.36, "text": " you know, it's, it's, it's not a completely parsimonious model. You always have the problem", "tokens": [50908, 291, 458, 11, 309, 311, 11, 309, 311, 11, 309, 311, 406, 257, 2584, 21156, 25098, 851, 2316, 13, 509, 1009, 362, 264, 1154, 51240], "temperature": 0.0, "avg_logprob": -0.10270076885558012, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.002346646972000599}, {"id": 667, "seek": 376984, "start": 3787.36, "end": 3791.1200000000003, "text": " when you're doing ancient modeling of deciding whether you're looking at your own model or", "tokens": [51240, 562, 291, 434, 884, 7832, 15983, 295, 17990, 1968, 291, 434, 1237, 412, 428, 1065, 2316, 420, 51428], "temperature": 0.0, "avg_logprob": -0.10270076885558012, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.002346646972000599}, {"id": 668, "seek": 376984, "start": 3791.1200000000003, "end": 3794.48, "text": " you're looking at the world. And then there's the ISI model, which is supposed to be that there's", "tokens": [51428, 291, 434, 1237, 412, 264, 1002, 13, 400, 550, 456, 311, 264, 6205, 40, 2316, 11, 597, 307, 3442, 281, 312, 300, 456, 311, 51596], "temperature": 0.0, "avg_logprob": -0.10270076885558012, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.002346646972000599}, {"id": 669, "seek": 376984, "start": 3794.48, "end": 3798.7200000000003, "text": " a simplest possible language, a model of language evolution. And the idea there would be to look", "tokens": [51596, 257, 22811, 1944, 2856, 11, 257, 2316, 295, 2856, 9303, 13, 400, 264, 1558, 456, 576, 312, 281, 574, 51808], "temperature": 0.0, "avg_logprob": -0.10270076885558012, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.002346646972000599}, {"id": 670, "seek": 379872, "start": 3798.8799999999997, "end": 3802.7999999999997, "text": " at very simple properties such as cluster size, cluster distribution, et cetera,", "tokens": [50372, 412, 588, 2199, 7221, 1270, 382, 13630, 2744, 11, 13630, 7316, 11, 1030, 11458, 11, 50568], "temperature": 0.0, "avg_logprob": -0.1330823474460178, "compression_ratio": 1.5732217573221758, "no_speech_prob": 0.0015196624444797635}, {"id": 671, "seek": 379872, "start": 3802.7999999999997, "end": 3807.68, "text": " and compare them to real languages, which hasn't been done yet. But that's, that's where we're going.", "tokens": [50568, 293, 6794, 552, 281, 957, 8650, 11, 597, 6132, 380, 668, 1096, 1939, 13, 583, 300, 311, 11, 300, 311, 689, 321, 434, 516, 13, 50812], "temperature": 0.0, "avg_logprob": -0.1330823474460178, "compression_ratio": 1.5732217573221758, "no_speech_prob": 0.0015196624444797635}, {"id": 672, "seek": 379872, "start": 3810.16, "end": 3815.7599999999998, "text": " So it could be wrong in assuming this, but sort of tying together the start and the middle of the", "tokens": [50936, 407, 309, 727, 312, 2085, 294, 11926, 341, 11, 457, 1333, 295, 32405, 1214, 264, 722, 293, 264, 2808, 295, 264, 51216], "temperature": 0.0, "avg_logprob": -0.1330823474460178, "compression_ratio": 1.5732217573221758, "no_speech_prob": 0.0015196624444797635}, {"id": 673, "seek": 379872, "start": 3815.7599999999998, "end": 3823.2, "text": " talk. And in the iterated learning model, you have this mapping between meaning and sensations.", "tokens": [51216, 751, 13, 400, 294, 264, 17138, 770, 2539, 2316, 11, 291, 362, 341, 18350, 1296, 3620, 293, 36642, 13, 51588], "temperature": 0.0, "avg_logprob": -0.1330823474460178, "compression_ratio": 1.5732217573221758, "no_speech_prob": 0.0015196624444797635}, {"id": 674, "seek": 382320, "start": 3823.9199999999996, "end": 3835.3599999999997, "text": " And in the EEG experiments, I guess, is, is the, is the purpose there to try to uncover", "tokens": [50400, 400, 294, 264, 33685, 38, 12050, 11, 286, 2041, 11, 307, 11, 307, 264, 11, 307, 264, 4334, 456, 281, 853, 281, 21694, 50972], "temperature": 0.0, "avg_logprob": -0.14230666748464924, "compression_ratio": 1.6374269005847952, "no_speech_prob": 0.005906975362449884}, {"id": 675, "seek": 382320, "start": 3836.56, "end": 3844.24, "text": " how that mapping is established in the brain and how, you know, maybe, you know, neurophysiology or", "tokens": [51032, 577, 300, 18350, 307, 7545, 294, 264, 3567, 293, 577, 11, 291, 458, 11, 1310, 11, 291, 458, 11, 16499, 950, 749, 46457, 420, 51416], "temperature": 0.0, "avg_logprob": -0.14230666748464924, "compression_ratio": 1.6374269005847952, "no_speech_prob": 0.005906975362449884}, {"id": 676, "seek": 382320, "start": 3845.12, "end": 3850.48, "text": " segregation in the brain biases or influences that mapping itself. Like, is that what you're", "tokens": [51460, 34317, 294, 264, 3567, 32152, 420, 21222, 300, 18350, 2564, 13, 1743, 11, 307, 300, 437, 291, 434, 51728], "temperature": 0.0, "avg_logprob": -0.14230666748464924, "compression_ratio": 1.6374269005847952, "no_speech_prob": 0.005906975362449884}, {"id": 677, "seek": 385048, "start": 3850.48, "end": 3855.44, "text": " trying to get at with the EEG experiments? Yeah. I mean, as, as he pointed out, we're a long way", "tokens": [50364, 1382, 281, 483, 412, 365, 264, 33685, 38, 12050, 30, 865, 13, 286, 914, 11, 382, 11, 382, 415, 10932, 484, 11, 321, 434, 257, 938, 636, 50612], "temperature": 0.0, "avg_logprob": -0.11445431709289551, "compression_ratio": 1.65625, "no_speech_prob": 0.0010305145988240838}, {"id": 678, "seek": 385048, "start": 3855.44, "end": 3862.72, "text": " from actually doing any of this, but the, that the sort of overall picture is you language has", "tokens": [50612, 490, 767, 884, 604, 295, 341, 11, 457, 264, 11, 300, 264, 1333, 295, 4787, 3036, 307, 291, 2856, 575, 50976], "temperature": 0.0, "avg_logprob": -0.11445431709289551, "compression_ratio": 1.65625, "no_speech_prob": 0.0010305145988240838}, {"id": 679, "seek": 385048, "start": 3862.72, "end": 3867.36, "text": " structure. We can discover that structure. How do we discover that structure? Probably, I think", "tokens": [50976, 3877, 13, 492, 393, 4411, 300, 3877, 13, 1012, 360, 321, 4411, 300, 3877, 30, 9210, 11, 286, 519, 51208], "temperature": 0.0, "avg_logprob": -0.11445431709289551, "compression_ratio": 1.65625, "no_speech_prob": 0.0010305145988240838}, {"id": 680, "seek": 385048, "start": 3867.36, "end": 3872.16, "text": " the best way to do it is by looking at EEG responses. You know, we do have commentaries on", "tokens": [51208, 264, 1151, 636, 281, 360, 309, 307, 538, 1237, 412, 33685, 38, 13019, 13, 509, 458, 11, 321, 360, 362, 2871, 4889, 322, 51448], "temperature": 0.0, "avg_logprob": -0.11445431709289551, "compression_ratio": 1.65625, "no_speech_prob": 0.0010305145988240838}, {"id": 681, "seek": 385048, "start": 3872.16, "end": 3878.56, "text": " language invented by grammaticians. That's really them trying to impose what they've learned about", "tokens": [51448, 2856, 14479, 538, 17570, 267, 8455, 13, 663, 311, 534, 552, 1382, 281, 26952, 437, 436, 600, 3264, 466, 51768], "temperature": 0.0, "avg_logprob": -0.11445431709289551, "compression_ratio": 1.65625, "no_speech_prob": 0.0010305145988240838}, {"id": 682, "seek": 387856, "start": 3878.56, "end": 3883.7599999999998, "text": " Latin onto other languages. And it's probably sort of quite naive compared to how the brain", "tokens": [50364, 10803, 3911, 661, 8650, 13, 400, 309, 311, 1391, 1333, 295, 1596, 29052, 5347, 281, 577, 264, 3567, 50624], "temperature": 0.0, "avg_logprob": -0.08585567667026713, "compression_ratio": 1.896, "no_speech_prob": 0.005601737182587385}, {"id": 683, "seek": 387856, "start": 3883.7599999999998, "end": 3889.36, "text": " considers parts of speech and the relationships between parts of speech. But the parts of speech", "tokens": [50624, 33095, 3166, 295, 6218, 293, 264, 6159, 1296, 3166, 295, 6218, 13, 583, 264, 3166, 295, 6218, 50904], "temperature": 0.0, "avg_logprob": -0.08585567667026713, "compression_ratio": 1.896, "no_speech_prob": 0.005601737182587385}, {"id": 684, "seek": 387856, "start": 3889.36, "end": 3895.12, "text": " and their relationships are probably important either as a reflection of, you know, and again,", "tokens": [50904, 293, 641, 6159, 366, 1391, 1021, 2139, 382, 257, 12914, 295, 11, 291, 458, 11, 293, 797, 11, 51192], "temperature": 0.0, "avg_logprob": -0.08585567667026713, "compression_ratio": 1.896, "no_speech_prob": 0.005601737182587385}, {"id": 685, "seek": 387856, "start": 3895.12, "end": 3900.48, "text": " the discussion there is relevant of the mechanisms the brain uses for producing language and", "tokens": [51192, 264, 5017, 456, 307, 7340, 295, 264, 15902, 264, 3567, 4960, 337, 10501, 2856, 293, 51460], "temperature": 0.0, "avg_logprob": -0.08585567667026713, "compression_ratio": 1.896, "no_speech_prob": 0.005601737182587385}, {"id": 686, "seek": 387856, "start": 3900.48, "end": 3905.92, "text": " understanding language, or the language that the, that's the languages that have had to evolve so", "tokens": [51460, 3701, 2856, 11, 420, 264, 2856, 300, 264, 11, 300, 311, 264, 8650, 300, 362, 632, 281, 16693, 370, 51732], "temperature": 0.0, "avg_logprob": -0.08585567667026713, "compression_ratio": 1.896, "no_speech_prob": 0.005601737182587385}, {"id": 687, "seek": 390592, "start": 3905.92, "end": 3911.04, "text": " that we can learn them despite the poverty of stimulus that we experience as children.", "tokens": [50364, 300, 321, 393, 1466, 552, 7228, 264, 10958, 295, 21366, 300, 321, 1752, 382, 2227, 13, 50620], "temperature": 0.0, "avg_logprob": -0.10215871674673897, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0010623157722875476}, {"id": 688, "seek": 390592, "start": 3912.56, "end": 3918.4, "text": " And so the iterated language model is trying to find out what, I mean, potentially is trying to", "tokens": [50696, 400, 370, 264, 17138, 770, 2856, 2316, 307, 1382, 281, 915, 484, 437, 11, 286, 914, 11, 7263, 307, 1382, 281, 50988], "temperature": 0.0, "avg_logprob": -0.10215871674673897, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0010623157722875476}, {"id": 689, "seek": 390592, "start": 3918.4, "end": 3922.16, "text": " find out what those properties of language might be. And we'd like to compare them to the real", "tokens": [50988, 915, 484, 437, 729, 7221, 295, 2856, 1062, 312, 13, 400, 321, 1116, 411, 281, 6794, 552, 281, 264, 957, 51176], "temperature": 0.0, "avg_logprob": -0.10215871674673897, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0010623157722875476}, {"id": 690, "seek": 390592, "start": 3922.16, "end": 3929.28, "text": " properties of language, which we discover through EEG. That's the big plan. Where my thoughts were", "tokens": [51176, 7221, 295, 2856, 11, 597, 321, 4411, 807, 33685, 38, 13, 663, 311, 264, 955, 1393, 13, 2305, 452, 4598, 645, 51532], "temperature": 0.0, "avg_logprob": -0.10215871674673897, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0010623157722875476}, {"id": 691, "seek": 390592, "start": 3929.28, "end": 3933.76, "text": " going with that, I was just wondering if there are any, you know, takeaways from that EEG research", "tokens": [51532, 516, 365, 300, 11, 286, 390, 445, 6359, 498, 456, 366, 604, 11, 291, 458, 11, 45584, 490, 300, 33685, 38, 2132, 51756], "temperature": 0.0, "avg_logprob": -0.10215871674673897, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0010623157722875476}, {"id": 692, "seek": 393376, "start": 3933.76, "end": 3939.5200000000004, "text": " or from, you know, the neuroscience realm that could perhaps be brought back over to the", "tokens": [50364, 420, 490, 11, 291, 458, 11, 264, 42762, 15355, 300, 727, 4317, 312, 3038, 646, 670, 281, 264, 50652], "temperature": 0.0, "avg_logprob": -0.20666956079417262, "compression_ratio": 1.5535714285714286, "no_speech_prob": 0.009993154555559158}, {"id": 693, "seek": 393376, "start": 3940.88, "end": 3945.28, "text": " iterated learning model, but specifically, so in that bottleneck layer and your", "tokens": [50720, 17138, 770, 2539, 2316, 11, 457, 4682, 11, 370, 294, 300, 44641, 547, 4583, 293, 428, 50940], "temperature": 0.0, "avg_logprob": -0.20666956079417262, "compression_ratio": 1.5535714285714286, "no_speech_prob": 0.009993154555559158}, {"id": 694, "seek": 393376, "start": 3945.28, "end": 3953.5200000000004, "text": " representational bottleneck in the meaning. Can you take any principles from neuroscience to", "tokens": [50940, 2906, 1478, 44641, 547, 294, 264, 3620, 13, 1664, 291, 747, 604, 9156, 490, 42762, 281, 51352], "temperature": 0.0, "avg_logprob": -0.20666956079417262, "compression_ratio": 1.5535714285714286, "no_speech_prob": 0.009993154555559158}, {"id": 695, "seek": 395352, "start": 3953.52, "end": 3958.4, "text": " apply, you know, to constrain that, you know, the structure of the bottleneck?", "tokens": [50364, 3079, 11, 291, 458, 11, 281, 1817, 7146, 300, 11, 291, 458, 11, 264, 3877, 295, 264, 44641, 547, 30, 50608], "temperature": 0.0, "avg_logprob": -0.16506052017211914, "compression_ratio": 1.7864077669902914, "no_speech_prob": 0.23003414273262024}, {"id": 696, "seek": 395352, "start": 3958.4, "end": 3968.0, "text": " But I mean, the story of my life is that when I started working in neuroscience, I thought, you", "tokens": [50608, 583, 286, 914, 11, 264, 1657, 295, 452, 993, 307, 300, 562, 286, 1409, 1364, 294, 42762, 11, 286, 1194, 11, 291, 51088], "temperature": 0.0, "avg_logprob": -0.16506052017211914, "compression_ratio": 1.7864077669902914, "no_speech_prob": 0.23003414273262024}, {"id": 697, "seek": 395352, "start": 3968.0, "end": 3974.4, "text": " know, we really want to map from actual neural networks to what they're doing, you know, we want", "tokens": [51088, 458, 11, 321, 534, 528, 281, 4471, 490, 3539, 18161, 9590, 281, 437, 436, 434, 884, 11, 291, 458, 11, 321, 528, 51408], "temperature": 0.0, "avg_logprob": -0.16506052017211914, "compression_ratio": 1.7864077669902914, "no_speech_prob": 0.23003414273262024}, {"id": 698, "seek": 395352, "start": 3974.4, "end": 3980.16, "text": " to go from, you know, this is a neuron and this is what it's connected to, this is how it works.", "tokens": [51408, 281, 352, 490, 11, 291, 458, 11, 341, 307, 257, 34090, 293, 341, 307, 437, 309, 311, 4582, 281, 11, 341, 307, 577, 309, 1985, 13, 51696], "temperature": 0.0, "avg_logprob": -0.16506052017211914, "compression_ratio": 1.7864077669902914, "no_speech_prob": 0.23003414273262024}, {"id": 699, "seek": 398016, "start": 3980.24, "end": 3985.12, "text": " And so, you know, I ended up working on tadpoles, because I thought tadpoles are really simple", "tokens": [50368, 400, 370, 11, 291, 458, 11, 286, 4590, 493, 1364, 322, 29622, 79, 7456, 11, 570, 286, 1194, 29622, 79, 7456, 366, 534, 2199, 50612], "temperature": 0.0, "avg_logprob": -0.11140254974365234, "compression_ratio": 1.825242718446602, "no_speech_prob": 0.0015035454416647553}, {"id": 700, "seek": 398016, "start": 3985.12, "end": 3991.3599999999997, "text": " creatures, maybe we can understand tadpoles. And tadpoles, neonatal tadpoles make a decision. So", "tokens": [50612, 12281, 11, 1310, 321, 393, 1223, 29622, 79, 7456, 13, 400, 29622, 79, 7456, 11, 30820, 40478, 29622, 79, 7456, 652, 257, 3537, 13, 407, 50924], "temperature": 0.0, "avg_logprob": -0.11140254974365234, "compression_ratio": 1.825242718446602, "no_speech_prob": 0.0015035454416647553}, {"id": 701, "seek": 398016, "start": 3991.3599999999997, "end": 3995.7599999999998, "text": " basically, older tadpoles are fearsome hunting animals, but the very, very young ones, when they've", "tokens": [50924, 1936, 11, 4906, 29622, 79, 7456, 366, 15649, 423, 12599, 4882, 11, 457, 264, 588, 11, 588, 2037, 2306, 11, 562, 436, 600, 51144], "temperature": 0.0, "avg_logprob": -0.11140254974365234, "compression_ratio": 1.825242718446602, "no_speech_prob": 0.0015035454416647553}, {"id": 702, "seek": 398016, "start": 3995.7599999999998, "end": 4001.52, "text": " just emerged from the egg, and they're still carrying, they carry with them kind of an egg pouch.", "tokens": [51144, 445, 20178, 490, 264, 3777, 11, 293, 436, 434, 920, 9792, 11, 436, 3985, 365, 552, 733, 295, 364, 3777, 27781, 13, 51432], "temperature": 0.0, "avg_logprob": -0.11140254974365234, "compression_ratio": 1.825242718446602, "no_speech_prob": 0.0015035454416647553}, {"id": 703, "seek": 398016, "start": 4001.52, "end": 4005.04, "text": " So the material from the egg that they grew in, so they don't have to eat for a couple of days.", "tokens": [51432, 407, 264, 2527, 490, 264, 3777, 300, 436, 6109, 294, 11, 370, 436, 500, 380, 362, 281, 1862, 337, 257, 1916, 295, 1708, 13, 51608], "temperature": 0.0, "avg_logprob": -0.11140254974365234, "compression_ratio": 1.825242718446602, "no_speech_prob": 0.0015035454416647553}, {"id": 704, "seek": 398016, "start": 4005.6, "end": 4009.52, "text": " And these animals are very, very simple. So you touch them and they swim away.", "tokens": [51636, 400, 613, 4882, 366, 588, 11, 588, 2199, 13, 407, 291, 2557, 552, 293, 436, 7110, 1314, 13, 51832], "temperature": 0.0, "avg_logprob": -0.11140254974365234, "compression_ratio": 1.825242718446602, "no_speech_prob": 0.0015035454416647553}, {"id": 705, "seek": 400952, "start": 4009.6, "end": 4014.4, "text": " You grab them, they struggle. So there's exactly one decision they have to make,", "tokens": [50368, 509, 4444, 552, 11, 436, 7799, 13, 407, 456, 311, 2293, 472, 3537, 436, 362, 281, 652, 11, 50608], "temperature": 0.0, "avg_logprob": -0.12543544315156482, "compression_ratio": 1.692063492063492, "no_speech_prob": 0.001201766193844378}, {"id": 706, "seek": 400952, "start": 4014.4, "end": 4018.32, "text": " which is whether they've been grabbed or touched. And I thought this is the simplest", "tokens": [50608, 597, 307, 1968, 436, 600, 668, 18607, 420, 9828, 13, 400, 286, 1194, 341, 307, 264, 22811, 50804], "temperature": 0.0, "avg_logprob": -0.12543544315156482, "compression_ratio": 1.692063492063492, "no_speech_prob": 0.001201766193844378}, {"id": 707, "seek": 400952, "start": 4018.88, "end": 4023.12, "text": " decision that any creature makes. And there's like eight different neurons involved in this.", "tokens": [50832, 3537, 300, 604, 12797, 1669, 13, 400, 456, 311, 411, 3180, 819, 22027, 3288, 294, 341, 13, 51044], "temperature": 0.0, "avg_logprob": -0.12543544315156482, "compression_ratio": 1.692063492063492, "no_speech_prob": 0.001201766193844378}, {"id": 708, "seek": 400952, "start": 4023.12, "end": 4027.44, "text": " Maybe we could go from the network to understanding the decision. And I had a PhD student who", "tokens": [51044, 2704, 321, 727, 352, 490, 264, 3209, 281, 3701, 264, 3537, 13, 400, 286, 632, 257, 14476, 3107, 567, 51260], "temperature": 0.0, "avg_logprob": -0.12543544315156482, "compression_ratio": 1.692063492063492, "no_speech_prob": 0.001201766193844378}, {"id": 709, "seek": 400952, "start": 4027.44, "end": 4033.2, "text": " worked on this model for, you know, four years, reduced it all to two dimensions so we could", "tokens": [51260, 2732, 322, 341, 2316, 337, 11, 291, 458, 11, 1451, 924, 11, 9212, 309, 439, 281, 732, 12819, 370, 321, 727, 51548], "temperature": 0.0, "avg_logprob": -0.12543544315156482, "compression_ratio": 1.692063492063492, "no_speech_prob": 0.001201766193844378}, {"id": 710, "seek": 400952, "start": 4033.2, "end": 4037.68, "text": " draw a phase diagrams. And it was like just the most unbelievable mess. And in the end,", "tokens": [51548, 2642, 257, 5574, 36709, 13, 400, 309, 390, 411, 445, 264, 881, 16605, 2082, 13, 400, 294, 264, 917, 11, 51772], "temperature": 0.0, "avg_logprob": -0.12543544315156482, "compression_ratio": 1.692063492063492, "no_speech_prob": 0.001201766193844378}, {"id": 711, "seek": 403768, "start": 4037.68, "end": 4042.64, "text": " you know, we just knew nothing about tadpoles, yet alone, you know. So I think, you know, I just", "tokens": [50364, 291, 458, 11, 321, 445, 2586, 1825, 466, 29622, 79, 7456, 11, 1939, 3312, 11, 291, 458, 13, 407, 286, 519, 11, 291, 458, 11, 286, 445, 50612], "temperature": 0.0, "avg_logprob": -0.10241437960071724, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.00215625436976552}, {"id": 712, "seek": 403768, "start": 4042.64, "end": 4047.6, "text": " don't think that's, I think we're so far away from, you know, something as complicated as language", "tokens": [50612, 500, 380, 519, 300, 311, 11, 286, 519, 321, 434, 370, 1400, 1314, 490, 11, 291, 458, 11, 746, 382, 6179, 382, 2856, 50860], "temperature": 0.0, "avg_logprob": -0.10241437960071724, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.00215625436976552}, {"id": 713, "seek": 403768, "start": 4047.6, "end": 4054.7999999999997, "text": " and mapping anything but the broadest principles of neural dynamics to the cognitive dynamics.", "tokens": [50860, 293, 18350, 1340, 457, 264, 4152, 377, 9156, 295, 18161, 15679, 281, 264, 15605, 15679, 13, 51220], "temperature": 0.0, "avg_logprob": -0.10241437960071724, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.00215625436976552}, {"id": 714, "seek": 403768, "start": 4054.7999999999997, "end": 4062.16, "text": " I think the gulf between, you know, the neural substrate and the cognitive function is so great", "tokens": [51220, 286, 519, 264, 290, 5757, 1296, 11, 291, 458, 11, 264, 18161, 27585, 293, 264, 15605, 2445, 307, 370, 869, 51588], "temperature": 0.0, "avg_logprob": -0.10241437960071724, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.00215625436976552}, {"id": 715, "seek": 403768, "start": 4062.16, "end": 4066.7999999999997, "text": " that I would be trying to stay away from that. The idea is rather to look at the structure of", "tokens": [51588, 300, 286, 576, 312, 1382, 281, 1754, 1314, 490, 300, 13, 440, 1558, 307, 2831, 281, 574, 412, 264, 3877, 295, 51820], "temperature": 0.0, "avg_logprob": -0.10241437960071724, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.00215625436976552}, {"id": 716, "seek": 406680, "start": 4066.8, "end": 4071.52, "text": " language and ask, look for the structure of language in EEG. So the mechanism there is only", "tokens": [50364, 2856, 293, 1029, 11, 574, 337, 264, 3877, 295, 2856, 294, 33685, 38, 13, 407, 264, 7513, 456, 307, 787, 50600], "temperature": 0.0, "avg_logprob": -0.1264583062434542, "compression_ratio": 1.7585034013605443, "no_speech_prob": 0.0006985343061387539}, {"id": 717, "seek": 406680, "start": 4071.52, "end": 4078.0800000000004, "text": " to try and find out what the brain regards as grammar. And then to compare that to the", "tokens": [50600, 281, 853, 293, 915, 484, 437, 264, 3567, 14258, 382, 22317, 13, 400, 550, 281, 6794, 300, 281, 264, 50928], "temperature": 0.0, "avg_logprob": -0.1264583062434542, "compression_ratio": 1.7585034013605443, "no_speech_prob": 0.0006985343061387539}, {"id": 718, "seek": 406680, "start": 4078.0800000000004, "end": 4082.32, "text": " iterated language model and how it might, or some other model and how it might treat grammar,", "tokens": [50928, 17138, 770, 2856, 2316, 293, 577, 309, 1062, 11, 420, 512, 661, 2316, 293, 577, 309, 1062, 2387, 22317, 11, 51140], "temperature": 0.0, "avg_logprob": -0.1264583062434542, "compression_ratio": 1.7585034013605443, "no_speech_prob": 0.0006985343061387539}, {"id": 719, "seek": 406680, "start": 4082.32, "end": 4086.4, "text": " basically. Which is not to say that people are, you know, I mean, maybe the cerebellum, you know,", "tokens": [51140, 1936, 13, 3013, 307, 406, 281, 584, 300, 561, 366, 11, 291, 458, 11, 286, 914, 11, 1310, 264, 11643, 7100, 449, 11, 291, 458, 11, 51344], "temperature": 0.0, "avg_logprob": -0.1264583062434542, "compression_ratio": 1.7585034013605443, "no_speech_prob": 0.0006985343061387539}, {"id": 720, "seek": 406680, "start": 4086.4, "end": 4090.4, "text": " maybe the hippocampus. But if you can't understand tadpoles, what could you do?", "tokens": [51344, 1310, 264, 27745, 905, 1215, 301, 13, 583, 498, 291, 393, 380, 1223, 29622, 79, 7456, 11, 437, 727, 291, 360, 30, 51544], "temperature": 0.0, "avg_logprob": -0.1264583062434542, "compression_ratio": 1.7585034013605443, "no_speech_prob": 0.0006985343061387539}, {"id": 721, "seek": 406680, "start": 4091.92, "end": 4094.88, "text": " I mean, basically we disproved the existence of tadpoles, I think.", "tokens": [51620, 286, 914, 11, 1936, 321, 717, 4318, 937, 264, 9123, 295, 29622, 79, 7456, 11, 286, 519, 13, 51768], "temperature": 0.0, "avg_logprob": -0.1264583062434542, "compression_ratio": 1.7585034013605443, "no_speech_prob": 0.0006985343061387539}, {"id": 722, "seek": 409680, "start": 4097.360000000001, "end": 4107.52, "text": " I'm sorry, Seth. Do you worry that expanding kind of this type of EEG work to a much broader set of", "tokens": [50392, 286, 478, 2597, 11, 25353, 13, 1144, 291, 3292, 300, 14702, 733, 295, 341, 2010, 295, 33685, 38, 589, 281, 257, 709, 13227, 992, 295, 50900], "temperature": 0.0, "avg_logprob": -0.21022130503798975, "compression_ratio": 1.4152046783625731, "no_speech_prob": 0.0021154657006263733}, {"id": 723, "seek": 409680, "start": 4108.400000000001, "end": 4111.12, "text": " structures would result in the same sort of mess?", "tokens": [50944, 9227, 576, 1874, 294, 264, 912, 1333, 295, 2082, 30, 51080], "temperature": 0.0, "avg_logprob": -0.21022130503798975, "compression_ratio": 1.4152046783625731, "no_speech_prob": 0.0021154657006263733}, {"id": 724, "seek": 409680, "start": 4112.96, "end": 4122.4800000000005, "text": " Yes. I mean, I think it's a good strategy. I agree with you that it seems promising and much", "tokens": [51172, 1079, 13, 286, 914, 11, 286, 519, 309, 311, 257, 665, 5206, 13, 286, 3986, 365, 291, 300, 309, 2544, 20257, 293, 709, 51648], "temperature": 0.0, "avg_logprob": -0.21022130503798975, "compression_ratio": 1.4152046783625731, "no_speech_prob": 0.0021154657006263733}, {"id": 725, "seek": 412248, "start": 4123.04, "end": 4128.719999999999, "text": " simpler than that or some of the stuff that's been done mapping kind of semantic content into the", "tokens": [50392, 18587, 813, 300, 420, 512, 295, 264, 1507, 300, 311, 668, 1096, 18350, 733, 295, 47982, 2701, 666, 264, 50676], "temperature": 0.0, "avg_logprob": -0.14445481264501586, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.0101612014696002}, {"id": 726, "seek": 412248, "start": 4128.719999999999, "end": 4135.599999999999, "text": " brain with MEG and fMRI. But it turns out like the tadpoles, it's a mess. Yeah, it seems more", "tokens": [50676, 3567, 365, 12003, 38, 293, 283, 44, 5577, 13, 583, 309, 4523, 484, 411, 264, 29622, 79, 7456, 11, 309, 311, 257, 2082, 13, 865, 11, 309, 2544, 544, 51020], "temperature": 0.0, "avg_logprob": -0.14445481264501586, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.0101612014696002}, {"id": 727, "seek": 412248, "start": 4135.599999999999, "end": 4141.5199999999995, "text": " promising. But I mean, who knows? I mean, yeah, I mean, we, what we've done compared to what we", "tokens": [51020, 20257, 13, 583, 286, 914, 11, 567, 3255, 30, 286, 914, 11, 1338, 11, 286, 914, 11, 321, 11, 437, 321, 600, 1096, 5347, 281, 437, 321, 51316], "temperature": 0.0, "avg_logprob": -0.14445481264501586, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.0101612014696002}, {"id": 728, "seek": 412248, "start": 4141.5199999999995, "end": 4146.5599999999995, "text": " want to do is tiny. So it seems to me to be the most straightforward strategy. But, you know,", "tokens": [51316, 528, 281, 360, 307, 5870, 13, 407, 309, 2544, 281, 385, 281, 312, 264, 881, 15325, 5206, 13, 583, 11, 291, 458, 11, 51568], "temperature": 0.0, "avg_logprob": -0.14445481264501586, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.0101612014696002}, {"id": 729, "seek": 412248, "start": 4146.5599999999995, "end": 4151.759999999999, "text": " at the moment, I can't even get my little prince experiment funded. And not only an eye, but these", "tokens": [51568, 412, 264, 1623, 11, 286, 393, 380, 754, 483, 452, 707, 16467, 5120, 14385, 13, 400, 406, 787, 364, 3313, 11, 457, 613, 51828], "temperature": 0.0, "avg_logprob": -0.14445481264501586, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.0101612014696002}, {"id": 730, "seek": 415176, "start": 4151.76, "end": 4156.56, "text": " are greater people who are trying to get money for the same thing and get it done. So, but, you know,", "tokens": [50364, 366, 5044, 561, 567, 366, 1382, 281, 483, 1460, 337, 264, 912, 551, 293, 483, 309, 1096, 13, 407, 11, 457, 11, 291, 458, 11, 50604], "temperature": 0.0, "avg_logprob": -0.1416274299306318, "compression_ratio": 1.749090909090909, "no_speech_prob": 0.002269153716042638}, {"id": 731, "seek": 415176, "start": 4159.84, "end": 4167.6, "text": " yeah, I said they're building in Bristol an instrumented museum, an instrumented cinema,", "tokens": [50768, 1338, 11, 286, 848, 436, 434, 2390, 294, 41208, 364, 7198, 292, 8441, 11, 364, 7198, 292, 17178, 11, 51156], "temperature": 0.0, "avg_logprob": -0.1416274299306318, "compression_ratio": 1.749090909090909, "no_speech_prob": 0.002269153716042638}, {"id": 732, "seek": 415176, "start": 4167.6, "end": 4172.16, "text": " so it'll be possible to do simultaneous EEG experiments on 100 people at once with consumer", "tokens": [51156, 370, 309, 603, 312, 1944, 281, 360, 46218, 33685, 38, 12050, 322, 2319, 561, 412, 1564, 365, 9711, 51384], "temperature": 0.0, "avg_logprob": -0.1416274299306318, "compression_ratio": 1.749090909090909, "no_speech_prob": 0.002269153716042638}, {"id": 733, "seek": 415176, "start": 4172.16, "end": 4176.64, "text": " level EEG. So, and I think they're building this thing without any clue what they're going to use", "tokens": [51384, 1496, 33685, 38, 13, 407, 11, 293, 286, 519, 436, 434, 2390, 341, 551, 1553, 604, 13602, 437, 436, 434, 516, 281, 764, 51608], "temperature": 0.0, "avg_logprob": -0.1416274299306318, "compression_ratio": 1.749090909090909, "no_speech_prob": 0.002269153716042638}, {"id": 734, "seek": 415176, "start": 4176.64, "end": 4181.12, "text": " it for. So I think, I think there's a potential to persuade, you know, to get hundreds of recordings", "tokens": [51608, 309, 337, 13, 407, 286, 519, 11, 286, 519, 456, 311, 257, 3995, 281, 31781, 11, 291, 458, 11, 281, 483, 6779, 295, 25162, 51832], "temperature": 0.0, "avg_logprob": -0.1416274299306318, "compression_ratio": 1.749090909090909, "no_speech_prob": 0.002269153716042638}, {"id": 735, "seek": 418112, "start": 4181.12, "end": 4184.4, "text": " of people listening to the little prince and lots of different languages. And you can help", "tokens": [50364, 295, 561, 4764, 281, 264, 707, 16467, 293, 3195, 295, 819, 8650, 13, 400, 291, 393, 854, 50528], "temperature": 0.0, "avg_logprob": -0.12250470461910717, "compression_ratio": 1.9081632653061225, "no_speech_prob": 0.0033676971215754747}, {"id": 736, "seek": 418112, "start": 4184.4, "end": 4189.599999999999, "text": " it feel that you can discover something that way. Because languages are just so different, you know,", "tokens": [50528, 309, 841, 300, 291, 393, 4411, 746, 300, 636, 13, 1436, 8650, 366, 445, 370, 819, 11, 291, 458, 11, 50788], "temperature": 0.0, "avg_logprob": -0.12250470461910717, "compression_ratio": 1.9081632653061225, "no_speech_prob": 0.0033676971215754747}, {"id": 737, "seek": 418112, "start": 4189.599999999999, "end": 4197.36, "text": " I mean, you know, if we could get Malay speakers, they don't, they don't, it's an isolating language,", "tokens": [50788, 286, 914, 11, 291, 458, 11, 498, 321, 727, 483, 5746, 320, 9518, 11, 436, 500, 380, 11, 436, 500, 380, 11, 309, 311, 364, 48912, 2856, 11, 51176], "temperature": 0.0, "avg_logprob": -0.12250470461910717, "compression_ratio": 1.9081632653061225, "no_speech_prob": 0.0033676971215754747}, {"id": 738, "seek": 418112, "start": 4197.36, "end": 4202.08, "text": " they don't change the words for the plural, you know, if we can get speakers of, you know,", "tokens": [51176, 436, 500, 380, 1319, 264, 2283, 337, 264, 25377, 11, 291, 458, 11, 498, 321, 393, 483, 9518, 295, 11, 291, 458, 11, 51412], "temperature": 0.0, "avg_logprob": -0.12250470461910717, "compression_ratio": 1.9081632653061225, "no_speech_prob": 0.0033676971215754747}, {"id": 739, "seek": 418112, "start": 4202.08, "end": 4206.24, "text": " Irish, they put the verb at a different place. I mean, it's just, there's such a variety,", "tokens": [51412, 16801, 11, 436, 829, 264, 9595, 412, 257, 819, 1081, 13, 286, 914, 11, 309, 311, 445, 11, 456, 311, 1270, 257, 5673, 11, 51620], "temperature": 0.0, "avg_logprob": -0.12250470461910717, "compression_ratio": 1.9081632653061225, "no_speech_prob": 0.0033676971215754747}, {"id": 740, "seek": 418112, "start": 4206.24, "end": 4210.72, "text": " it'll be really interesting to see what we can see. But I agree, you can probably turn", "tokens": [51620, 309, 603, 312, 534, 1880, 281, 536, 437, 321, 393, 536, 13, 583, 286, 3986, 11, 291, 393, 1391, 1261, 51844], "temperature": 0.0, "avg_logprob": -0.12250470461910717, "compression_ratio": 1.9081632653061225, "no_speech_prob": 0.0033676971215754747}, {"id": 741, "seek": 421072, "start": 4210.72, "end": 4214.0, "text": " into a mess. And I think the way that you can turn into a mess is to try and", "tokens": [50364, 666, 257, 2082, 13, 400, 286, 519, 264, 636, 300, 291, 393, 1261, 666, 257, 2082, 307, 281, 853, 293, 50528], "temperature": 0.0, "avg_logprob": -0.17129451887948172, "compression_ratio": 1.6407407407407408, "no_speech_prob": 0.025197003036737442}, {"id": 742, "seek": 421072, "start": 4216.0, "end": 4221.360000000001, "text": " find things that are too detailed. We have to try and find the very broad principles,", "tokens": [50628, 915, 721, 300, 366, 886, 9942, 13, 492, 362, 281, 853, 293, 915, 264, 588, 4152, 9156, 11, 50896], "temperature": 0.0, "avg_logprob": -0.17129451887948172, "compression_ratio": 1.6407407407407408, "no_speech_prob": 0.025197003036737442}, {"id": 743, "seek": 421072, "start": 4221.360000000001, "end": 4227.280000000001, "text": " you know, because even that's unknown. Is this cinema? Like... Yeah, yeah, they're building this,", "tokens": [50896, 291, 458, 11, 570, 754, 300, 311, 9841, 13, 1119, 341, 17178, 30, 1743, 485, 865, 11, 1338, 11, 436, 434, 2390, 341, 11, 51192], "temperature": 0.0, "avg_logprob": -0.17129451887948172, "compression_ratio": 1.6407407407407408, "no_speech_prob": 0.025197003036737442}, {"id": 744, "seek": 421072, "start": 4227.280000000001, "end": 4233.360000000001, "text": " they've built it. They got funding for something called the Future Institute. And it's good to", "tokens": [51192, 436, 600, 3094, 309, 13, 814, 658, 6137, 337, 746, 1219, 264, 20805, 9446, 13, 400, 309, 311, 665, 281, 51496], "temperature": 0.0, "avg_logprob": -0.17129451887948172, "compression_ratio": 1.6407407407407408, "no_speech_prob": 0.025197003036737442}, {"id": 745, "seek": 421072, "start": 4233.360000000001, "end": 4238.8, "text": " have this instrumented cinema where people wear EEG and they'll watch films, basically.", "tokens": [51496, 362, 341, 7198, 292, 17178, 689, 561, 3728, 33685, 38, 293, 436, 603, 1159, 7796, 11, 1936, 13, 51768], "temperature": 0.0, "avg_logprob": -0.17129451887948172, "compression_ratio": 1.6407407407407408, "no_speech_prob": 0.025197003036737442}, {"id": 746, "seek": 423880, "start": 4238.8, "end": 4242.400000000001, "text": " So, like, when all the new films come out, you can have like the EEG response.", "tokens": [50364, 407, 11, 411, 11, 562, 439, 264, 777, 7796, 808, 484, 11, 291, 393, 362, 411, 264, 33685, 38, 4134, 13, 50544], "temperature": 0.0, "avg_logprob": -0.19473028486701333, "compression_ratio": 1.859531772575251, "no_speech_prob": 0.01023433543741703}, {"id": 747, "seek": 423880, "start": 4242.400000000001, "end": 4246.16, "text": " Exactly. Yeah, exactly. I mean, I think that's how they solve it. Yeah. I mean, it's nothing to do", "tokens": [50544, 7587, 13, 865, 11, 2293, 13, 286, 914, 11, 286, 519, 300, 311, 577, 436, 5039, 309, 13, 865, 13, 286, 914, 11, 309, 311, 1825, 281, 360, 50732], "temperature": 0.0, "avg_logprob": -0.19473028486701333, "compression_ratio": 1.859531772575251, "no_speech_prob": 0.01023433543741703}, {"id": 748, "seek": 423880, "start": 4246.16, "end": 4250.96, "text": " with me. I mean, it's people in the psychology department. And the idea is that I think they'll", "tokens": [50732, 365, 385, 13, 286, 914, 11, 309, 311, 561, 294, 264, 15105, 5882, 13, 400, 264, 1558, 307, 300, 286, 519, 436, 603, 50972], "temperature": 0.0, "avg_logprob": -0.19473028486701333, "compression_ratio": 1.859531772575251, "no_speech_prob": 0.01023433543741703}, {"id": 749, "seek": 423880, "start": 4250.96, "end": 4255.04, "text": " have, you know, all sorts of instrumentation, right? They'll have, you know, we have to work out how", "tokens": [50972, 362, 11, 291, 458, 11, 439, 7527, 295, 7198, 399, 11, 558, 30, 814, 603, 362, 11, 291, 458, 11, 321, 362, 281, 589, 484, 577, 51176], "temperature": 0.0, "avg_logprob": -0.19473028486701333, "compression_ratio": 1.859531772575251, "no_speech_prob": 0.01023433543741703}, {"id": 750, "seek": 423880, "start": 4255.04, "end": 4259.2, "text": " sweaty people are and the heart rates and, you know, how much they're breathing, you know,", "tokens": [51176, 36044, 561, 366, 293, 264, 1917, 6846, 293, 11, 291, 458, 11, 577, 709, 436, 434, 9570, 11, 291, 458, 11, 51384], "temperature": 0.0, "avg_logprob": -0.19473028486701333, "compression_ratio": 1.859531772575251, "no_speech_prob": 0.01023433543741703}, {"id": 751, "seek": 423880, "start": 4261.52, "end": 4266.0, "text": " and their EEG. So, I mean, it sounds like one of those things that sounds really cool. So,", "tokens": [51500, 293, 641, 33685, 38, 13, 407, 11, 286, 914, 11, 309, 3263, 411, 472, 295, 729, 721, 300, 3263, 534, 1627, 13, 407, 11, 51724], "temperature": 0.0, "avg_logprob": -0.19473028486701333, "compression_ratio": 1.859531772575251, "no_speech_prob": 0.01023433543741703}, {"id": 752, "seek": 426600, "start": 4266.0, "end": 4269.04, "text": " they were able to get money for it. But I think when it comes down to it, they're going to", "tokens": [50364, 436, 645, 1075, 281, 483, 1460, 337, 309, 13, 583, 286, 519, 562, 309, 1487, 760, 281, 309, 11, 436, 434, 516, 281, 50516], "temperature": 0.0, "avg_logprob": -0.2142386043772978, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.006643170025199652}, {"id": 753, "seek": 426600, "start": 4269.04, "end": 4272.24, "text": " struggle to find experiments. So, hopefully, they'll do experiments for me.", "tokens": [50516, 7799, 281, 915, 12050, 13, 407, 11, 4696, 11, 436, 603, 360, 12050, 337, 385, 13, 50676], "temperature": 0.0, "avg_logprob": -0.2142386043772978, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.006643170025199652}, {"id": 754, "seek": 426600, "start": 4273.92, "end": 4276.48, "text": " Nightmare giving a talk in that room. You can track in real time.", "tokens": [50760, 10190, 15455, 2902, 257, 751, 294, 300, 1808, 13, 509, 393, 2837, 294, 957, 565, 13, 50888], "temperature": 0.0, "avg_logprob": -0.2142386043772978, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.006643170025199652}, {"id": 755, "seek": 426600, "start": 4279.92, "end": 4282.16, "text": " Yeah, there's lots of cool things you can do like that. Yeah.", "tokens": [51060, 865, 11, 456, 311, 3195, 295, 1627, 721, 291, 393, 360, 411, 300, 13, 865, 13, 51172], "temperature": 0.0, "avg_logprob": -0.2142386043772978, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.006643170025199652}, {"id": 756, "seek": 426600, "start": 4286.96, "end": 4287.6, "text": " Cool.", "tokens": [51412, 8561, 13, 51444], "temperature": 0.0, "avg_logprob": -0.2142386043772978, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.006643170025199652}, {"id": 757, "seek": 428760, "start": 4287.6, "end": 4289.200000000001, "text": " Thanks for coming in.", "tokens": [50392, 2561, 337, 1348, 294, 13, 50444], "temperature": 0.0, "avg_logprob": -0.8365730047225952, "compression_ratio": 0.7241379310344828, "no_speech_prob": 0.5832878351211548}], "language": "en"}