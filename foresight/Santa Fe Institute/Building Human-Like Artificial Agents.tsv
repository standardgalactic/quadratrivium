start	end	text
0	5760	dynamic decision-making laboratory. She has made various contributions to theories models and empirical
5760	10960	research on individual and group decision-making, in particular in complex and dynamic situations,
10960	17040	which is what drew me to her work and also your texture as one of the most important
17040	22320	quality scientists today. And I think it's been working also a lot, and she's writing to Human
22320	29280	Air Coordination and Cybersecurity, and she co-directs one of these new NSF national air
29280	35200	research institutes. This one is called Alliance for Societal Decision-Making. She is a fellow
35200	40160	authority science society, human factors ergonomic society editor of various leading journals,
40160	44400	and she has been leading a wide range of multi-million and multi-year collaborative
44400	49680	efforts to guarantee industry, such as multi-university research, initiative grants from
49680	54880	Army research laboratories in Army research office, and large collaborative projects
54880	59840	with Dartmouth. And she has visited SFI a number of times. It's one of the lead authors on the
59840	63040	paper on collective adaptation. That is the background for the workshop. That was just
63040	70000	happening at SFI, and I'm glad to hear your talk. Thank you. Thank you so very much for having me
70000	76560	here. I've been to Santa Fe in the past, and every time I come here, I'm like in heaven. I feel that
76560	84320	I can relax, finally, even for a little short time. So what I'm going to talk about today is
85120	92240	work that basically I've been doing all my life, all my research life, and it's this idea of building
92880	102880	agents that look at, are like humans in terms of the thinking and the cognition and the decisions
103680	113040	these agents make. But in particular, I am interested in studying very complex dynamic
113040	121200	situations. We call these situations situations of dynamic decision making. So in conditions like
121200	131040	the examples you see here, there are more than one decisions that need to be made. We make multiple
131040	139920	interdependent decisions under a lot of stress and time constraints. There is usually high uncertainty
139920	147200	on the way we make decisions and take, for example, the disaster management case. So there are multiple
147200	154240	actors working in trying to allocate resources in a very constrained environment. Those resources
154480	162240	are usually limited. And so figuring out how to allocate those resources on time to save the victims
162240	168720	is a real challenge. Obviously, a challenge in which the individual human cognitive abilities
168720	177440	also play a big role. We cannot remember everything. We have to sleep. We are stressed. We feel the
177520	185440	time pressure, et cetera. So these are the type of situations I'm interested in helping humans
185440	195520	make better decisions. And AI, at least one of the initial goals of AI, was to create agents
195520	201760	that would be undistinguishable from humans. So be able to create agents that would make decisions
201760	209840	like humans do. But in reality, the current view of AI, at least, is about machine learning
209840	218720	algorithms that are trained on data. And they are trained to optimize decisions that is very much
218720	227840	not human-like because humans are not optimal at how we make decisions. So the goal of these AI
227840	236400	tools has been to create these elements of optimal decision-making, faster and better than humans.
236400	244960	And in fact, many times, we see competitions between humans and AI. And of course, they are
244960	252880	amazing and they are fantastic and we need them. And they have amazing applications now with
252960	262880	increasingly complex data sets and features. And be able now, obviously, to generate content
262880	271600	from PROM, which is really incredible. So things are advancing very fast. However, I still
272320	280080	believe that to be able to address complex problems like the one I have been talking about,
281040	291200	we need more than algorithms that are optimal. I believe that these generation patterns can help
291200	297760	in situations like this, but they are really not capable of making decisions on their own,
298320	305120	at least not in the way humans would do it. Because the kind of situations humans have to confront
305840	312720	are more than just economic, are more than just optimizing an objective value.
313280	320400	They are about various factors and consequences, including ethical and moral trade-offs,
320400	330160	that are very hard to emulate or to optimize. So I think the ultimate responsibility for
330160	337280	decision making in these type of situations is human. And therefore, we need to figure out
337280	345520	how we can use these tools together with tools that emulate human behavior so that we can help
345520	353840	humans in these situations. So our goal, should I take questions now? Okay, go ahead.
353840	368640	Yeah, I was just wondering about the previous slide and how you said that ethical and moral
368640	373200	considerations. It seems that there is a fundamental assumption that you cannot quantify it or you
373200	377600	cannot encode it as part of an objective function, but I wonder where does it come from?
378560	385280	Yeah, I wouldn't say that we cannot. It's simply we haven't found out how. How to
386320	391920	consider all the different trade-offs and the different type of demands that humans need to
393440	400160	live when they are making decisions in these kind of situations. So that's precisely where we need
400160	409600	to make progress. I just wanted to follow up. The idea about ethics and morals though is that
409600	414640	they kind of can't be quantified because they are subjective and relative and it really is contextual
414640	420640	and you define things very down to make it morally right or wrong. I guess that's a huge like complex
421360	426720	web of trade-offs, I guess. But I guess like you said, it's not impossible. Yeah, I'm not saying
426720	433600	it's impossible. I think we are going to get there, but I think we need more research on precisely
433600	440480	how humans make decisions in these complex situations before we even attempt to create any
440480	447680	computer program that tries to emulate that process. So that has been my goal, yes.
447840	457840	I make complex decisions where I don't exactly know what are the outcomes. It also depends
457840	464640	on my risk attitudes and how important it is. I mean people, for example, during pregnancy,
464640	469360	there are very strong opinions and you are not allowed to do this and this varies over countries.
469360	476720	But at the end, Emily Oster wrote this nice book just to know what are the facts and then
477280	485040	everyone has to judge on his own or her own how to trade drinking wine against the low risk of,
485040	490640	you know, whatever that would be. So these things are probably not part of...
491680	499360	No, they are all part. They are all part. But you are going to see next how I make a distinction
499360	507680	between sort of the traditional risk assessment and dynamic situations and how I believe we make
507680	517040	decisions in dynamic tasks. Across individuals. Across individuals, across situations, absolutely.
519200	527040	So my goal has been to improve and speed up human dynamic decision making. And we want to do that
527040	534000	by building algorithms that emulate the human cognitive processes and that they can inform
534880	544240	other tools like AI optimization algorithms so that we together can make better decisions
544240	552320	in these kind of situations. I plan or I want to go beyond just being inspired by humans,
552320	559520	which is what usually, you know, computer scientists would do. They do care about humans and
559520	566560	they do observe what humans do, but usually they take humans as an inspiration only.
567200	573120	What I'm saying is we need to go beyond just taking humans as an inspiration, but really
573120	581520	understanding their cognitive processes and being able to emulate that process that is creating this
582560	591920	algorithms that can emulate the decision processes, including the biases and errors.
591920	599360	So I actually believe that being able to replicate human biases is essential,
599360	607520	because the only way we can on bias humans is by being able to understand where the biases come
607520	614880	from and then be able to use other tools to help humans make more correct decisions.
615600	622960	So sometimes the process of using simple heuristics has been shown to be more effective
622960	631680	in these dynamic complex tasks than even very complex reasoning or data sets that, you know,
632400	640240	large amounts of data and algorithms that come from large amount of data often are not as good
640240	651200	as human experience. So this is sort of the idea of using not the traditional approach to AI,
651200	658560	but really to promote the idea of we need representations that can replicate the cognitive
658560	666880	process. Why cognitive algorithms? Well, it's a little bit repetitive, I guess, here is we
666880	673200	are not trying to create super humans, but we're trying to imitate cognition. And here are some
673200	681280	of the advantages and some ways in which these type of algorithms can be used. So they can help
681280	688160	explain the cognitive process of decisions on their uncertainty. They are also dynamic and they
688160	697760	can learn, presumably like humans learn to make better decisions. They can inform other decision
697760	707440	aids or other autonomous systems to help humans. They can also be synchronized with the humans.
707440	716560	So for example, if I want to emulate Mirta's decision processes, I will need some information
716560	725280	about Mirta's past experience, but I can trace Mirta's decisions over time to be able to predict
725280	730880	what Mirta is going to do in a particular situation. So we can do that with these models.
731440	738800	And we can also make them be collaborators with others. So if I can emulate Mirta's decision,
738800	744160	then I can predict what Mirta is going to do when she collaborates with me. So that's in general
744960	750800	the idea of why cognitive algorithms are different from traditional or at least
750800	759280	currently traditional AI. So I have been trying to pursue these two essential questions. My whole
760000	766560	research career is about how do human make decisions learn and adapt in dynamic tasks.
766560	772800	And the second is how can I represent such process computationally. So I'm going to go with the first
772800	780800	one too. And when I came to Carnegie Mellon, which was late 90s and beginning 2000, I started as a
780800	789920	professor. The concept that I observed about decision making was very different from what I had in
789920	798320	mind. It was static decision making, economics decision making. But I was inspired by many
798320	803760	people, including this quote by Edward Edwards, where he said that static decision
804480	812240	theories have only a limited future. Human beings learn and probabilities and values change.
812880	819840	These facts mean that the really applicable kinds of decision theories will be dynamic and not
819840	829120	static. So I was stubborn enough to pursue the idea that we want to study dynamic decision making
829120	832960	even though nobody in my department understood what the hell I was talking about.
834560	840640	This is what they were talking about. And this is a more traditional approach of classical
840640	847360	decision making. It's a linear process where you have alternatives. The alternatives are usually
847360	853520	well established, meaning they are obviously provided to you. You have option A and option
853520	860400	B, usually represented as a decision tree, and then calculated based on expected value what
860400	868080	humans should do. Expected value concept, of course, is an extremely useful today and always will be,
868080	875280	I think, a concept on optimal decision making. But of course, very soon people realize that
876000	884160	humans don't make decisions optimally. And so this famous theory by Kanieman and Taversky,
884160	892400	which is still very popular and very influential today, what they essentially did was to modify
892400	899680	the concept of expected value into functions that are more human-like. And of course, they
899680	905760	supported these functions based on human behavior. So humans are not optimal. Instead,
905760	912240	humans are behaving according to these other functions. And that's how you can find out what
912240	920480	humans are going to do. And yeah, that's fantastic and still influential. But this type of theory
921120	926560	is just not applicable to dynamic situations that are constantly
927440	936320	adapting according to various factors over time. Now, if we go completely to the other extreme
936320	944560	of what I was studying early 2000, I worked with Gary Klein in a very big project for the
944560	950800	army research laboratories. And Gary was the opposite of all the behavioral economics. At
950800	957920	that time, it wasn't behavioral, it was just economics. So Gary Klein was studying naturalistic
957920	964800	situations, in particular firefighters. So there are books about naturalistic decision making.
964800	970960	There is a conference. There are lots of people interested in this area. But essentially what
971040	978080	you do in this case is you actually go to the place where decisions are made. Let's say
978080	984960	firefighters. And he describes in his book how he would get on the trucks with the firefighters to
984960	991360	find out how they make decisions in those very constrained tasks. And then he found out that
992240	998240	they don't make decisions. At least that's what they express the firefighters. They just said,
998240	1005600	I just know what to do. I just get in there. I've seen these situations so many times in the past.
1005600	1015680	I just know what to do. So he documents in his book all the stories about the many firefighters
1015680	1023120	making decisions. And this type of approach has been used in hospitals and in many other
1023120	1032080	naturalistic situations. So out of that work came a model that he called the recognition
1032080	1038640	prime decision model, which was very influential to me. The essential element of recognition
1038640	1045120	prime decision model is recognition. What is recognition is our ability to determine the
1045120	1051600	similarity between a situation we are confronting and what we have confronted in the past.
1052560	1060400	And so he proposes how such a match determines whether we know a situation is typical or not
1060400	1069440	typical. And then based on that, we engage in some sort of mental simulation to figure out
1070480	1077120	different courses of action. This presumably happens very fast. And then you are able to
1077120	1085360	finally decide what to do and implement a course of action. So very inspiring. It made
1085360	1093440	a lot of sense to me, but it wasn't computational. And it hasn't been as far as I know up to now.
1093440	1100800	There has been many attempts to make this sort of theory computational, but to my knowledge,
1100800	1109840	none of them have been really successful. So what I do is sort of in the middle of these two
1109840	1116000	extremes. Dynamic decision making, I sort of realized that I'm not going to make a lot of
1116000	1122720	progress if I go and get on the trucks with firefighters. So instead, I'm going to take
1122720	1132080	the approach that others have taken like Brent Bremmer and Joaquin Funke to create micro worlds.
1132800	1141600	So these are reductions of these real world situations in which we can actually use experimentation.
1141600	1147600	And to me, it was very important to be able to see the cause and effect relationships,
1147600	1157600	not only rely on observations. And so this is just a whole lot of the many micro worlds we
1157600	1166560	have developed and used in my lab through the various years. Initially, I used and developed
1166560	1176240	this and it took many, many years to develop. It's a water purification plant task that has
1176320	1182320	all the major elements of dynamic decision making. It's a dynamic resource allocation with
1182320	1189040	limitations of time, et cetera. So we developed a theory, we gathered a lot of information based
1189040	1195360	on that task. But then the main question was like, well, whatever you are doing is only applicable
1195360	1200560	to this task. You haven't shown me that what your models and your things that you are doing
1201360	1209040	can be applicable to many tasks. So then we went wild and then we just developed all kinds of
1209040	1215760	micro worlds and started to apply to many other tasks to demonstrate that our theory and our ideas
1215760	1226240	were more general than just that particular task. Now, this is a summary of behavioral phenomena
1226320	1233280	that came out of experimental work with many different micro worlds. So I'm going to go over it
1233280	1241680	but relatively quickly given the time. So the first thing is when I arrived to this area, the
1243280	1253680	picture was very frustratingly negative. So all humans are very poor at making decisions
1253680	1260640	in dynamic environments. Even when you give them full feedback and you give them unlimited time
1260640	1267760	incentives, extensive practice, we are born with something that doesn't allow us to make
1268400	1276880	good decisions in these very complex environments. People are generally poor at handling systems
1276880	1284720	with long feedback delays. A lot of people where Bremer, including basically almost all his work,
1284720	1291760	was about demonstrating of long feedback delays and the effect of that on decision making. So the
1291760	1300880	longer the delay is, the poorer decisions we make. So, okay, I was taking all that and I was in
1300880	1307520	agreement with all that, but at the same time, I could observe that people in the real world,
1307520	1313840	some of them are pretty good at making very complex decisions. So how can we explain that?
1314560	1322880	And my response to that is that it's in the learning process. It's in understanding how we go from not
1322880	1330000	knowing how to make decisions to really making very good decisions in really complex environments.
1330880	1338240	So I started to study different things. For example, I found that if we give headroom, meaning
1338240	1344800	space for learning to individual decision makers, then they are able to adapt to more
1345360	1353120	difficult decision situations. For example, putting someone in a low time constraint is going to help
1353840	1360480	for that person to perform under high time constraints more than those that are always
1360480	1365920	trained under high time constraints. So the headroom is needed for learning.
1367120	1375600	Heterogeneity, experiencing different variety of situations are going to help us to adapt
1375600	1383760	to novel situations and we have several studies on that. The ability to pattern match is extremely
1383760	1391600	important. At that time, we collected our ability to pattern match with the Raven progressive matrices
1392240	1399760	test, which is essentially a pattern matching test. And we demonstrated how the score in that test
1399760	1405920	is very predictive of the quality of the decisions that human makes in these micro worlds.
1406560	1411920	And we also found how to provide feedback in a way that would be more useful
1412960	1419600	to individuals by particularly providing observing behavior of an expert was extremely
1419600	1425520	helpful for people to learn and to adapt even after removing that feedback.
1426320	1431440	So this is a list of some of the phenomena and all the things that we have found.
1433280	1440720	I think all over that list, there are two essential elements of dynamic decision making and
1440720	1449520	they keep coming over and over. One of them was recognition. So I was convinced that similarity
1449600	1457280	and our ability to detect similarity is essential for making good decisions and memory.
1457280	1466240	That is our ability to create context specific knowledge. So very different from traditional
1466240	1472880	cognitive theories that believe that humans with experience generate heuristics. I actually
1472880	1478560	believed it was the other way around that humans use heuristics when they don't have knowledge.
1479280	1485760	And as you acquire more context specific knowledge, you actually depart from those
1485760	1492960	heuristics that are not good to start with. They are only approximate. And so you move away from
1492960	1500960	those heuristics and you start to apply context specific knowledge. So let me move to the second
1500960	1508800	question which is how to represent those things computationally. My approach is on cognitive
1508800	1522240	architectures and in large part because I was at CMU. And in CMU is the birth part place
1522240	1530000	of cognitive architectures and in many ways of AI too. So in particular, I was inspired by the work
1530000	1536960	of Allen Newell and Herb Simon that wanted to do had this idea of creating unified theories of
1536960	1547760	cognition. They imagine or envision this complete program that would be capable of making all the
1549040	1556880	activities that human mind was able to make. So they imagine to represent all these cognitive
1556960	1565120	steps and be able to explain all the components of the mind and how they work and produce cognition
1565120	1574400	together. Many books that are very inspiring coming from this tradition. My personal view of this
1574400	1582560	is that it is very utopic. It is an extremely complex problem and therefore is very hard
1582560	1591360	to accomplish. So it was accomplished partly by the actor unified theory of cognition.
1592160	1598800	So John Anderson at Carnegie Mellon and Christian LeVier, one of John Anderson's students and then
1598800	1606720	postdoc and now faculty, research faculty has been working on this idea from Allen Newell
1606720	1615440	and Herb Simon. And if you look at the history of actor, you are going to see that it has
1615440	1623120	just become more complex with over the years. And in my personal opinion, also less useful.
1624640	1633840	So what I did was to grab what I felt was useful from that cognitive architecture. And this is
1633840	1640080	essentially the way knowledge is represented in declarative and procedural forms that is with
1640080	1649520	facts or with rules and in symbolic or sub symbolic ways. That is with formulas that would
1649520	1658560	explain how those facts or rules are going to evolve and change over time. So that's what I
1658560	1666080	did and basically I took that to develop my own mini architecture, if you will. So this
1666880	1673440	theory called instance-based learning theory is essentially a dynamic decision making process
1673440	1680720	that is represented in a learning loop. So I'm going to go over this process and this is sort of
1680720	1687840	the corresponding picture of RPM, the recognition prime, but my own and there are also many
1687840	1696000	differences obviously. So the first step is again decisions are made by recognizing the
1696000	1704640	similar situations and mapping with decisions that have been made in the past to be able to retrieve
1704640	1711840	something that worked in the past. And that happens in this recognition process. We evaluate the new
1711840	1719200	actions according to the utility of the past decisions which are retrieved from memory.
1720160	1729040	And then we explore mentally all the possible alternatives. As you can see this is very similar
1729040	1736720	to the mental simulation that Gary Klein was talking about. And then finally we make execute
1737440	1746400	decision that is the highest up to that point and that execution is going to modify the environment.
1746400	1754160	And finally whenever we get feedback we can reevaluate the utility of those decisions we
1754160	1761760	have made in the past. So that's conceptually what the theory, the learning loop is about.
1762480	1772080	Now let me tell you about the representations. So the idea is that decisions are stored over time
1772080	1779840	in memory in the form of instances and those instances are triplets. Those triplets are the
1779840	1788640	associations of the features that are necessary for that decision, the action that is taken
1789280	1797120	and the outcome. The outcome can be the observed or the expected utility that is calculated
1797120	1804880	about making that decision. Now for each potential action, action one or action two,
1805440	1812560	instances when you try to make a new decision, instances will be blended according to the
1812640	1823600	similarity of the features. And then the action that has the maximum blended value,
1823600	1832560	in this case let's assume it's action two, is chosen. At that moment a new instance is created
1832560	1841280	with the blended value. And then when feedback is received that blended value is changed for the
1841280	1848000	actual feedback outcome that was received. So that's the algorithm in the in the
1848000	1855920	representational form. And this is the algorithm in the mathematical form. So the most important
1855920	1864480	equation I think in this algorithm is the activation, which is not developed by me at all,
1864480	1874080	is borrowed from ACTAAR. So ACTAAR has backed up evidence for each part of this equation
1874080	1882080	with a lot of experiments regarding how humans process information. So this activation equation
1882080	1888720	has three parts. This part is called the base level equation. This part is the partial matching
1889600	1898000	idea and this part is noise. And essentially this equation takes into account the frequency
1898000	1906000	of events. So we tend to believe that something is going to happen more often and actually to
1906000	1914960	retrieve more that information faster when it happens more often. So frequency of events matters
1914960	1921360	a lot for our ability to retrieve information from memory. And so that is represented here.
1922240	1928880	We also tend to forget. So things that happened yesterday I can remember faster than things that
1928880	1938320	happened many years ago. And that is represented by this nonlinear decay function. And then this
1938400	1945280	part here represents the partial matching equation for each feature in the instance. We can
1945920	1954800	determine a particular similarity function, which is then aggregated across all the features
1954800	1965280	and then sort of penalized or exacerbated with a partial matching parameter. And then finally
1965280	1970480	the noise, which this is just noise. Right now there is not a lot of theory about
1971440	1979280	what doesn't fit in this part. This part here is a draw from a random distribution or
1979840	1988880	other type of distributions. And this one is one of the parameters. So for each instance
1988960	1999680	you calculate the activation at each point of time. And then you can calculate the probability
1999680	2007440	of retrieving that instance from memory, which is essentially the activation relative to the
2007440	2016880	activation of all the instances in memory. And then this is the magic I guess of IVLT. What we
2016880	2024320	do is essentially combine all those instances that belong to a particular choice option
2025200	2031200	in the form of an expected value. So this is the probability times the outcome.
2032560	2038960	But obviously this probability is a cognitive probability. It's not the actual probability
2038960	2043760	of an event. And it's a probability that is calculated all the way from here.
2044160	2052160	Right. So in this way we are accounting for the cognition of how do we forget information,
2052160	2059680	how similar the information is, etc. Okay. And we create this expected utility value that we
2059680	2066480	call blending. And then we select the option that has the maximum expected value. So that's it.
2067280	2077760	Now I have been interested as I said before in figuring out how general this theory is
2078560	2087840	across many decision-making situations. And how human-like this algorithm is by comparing
2088800	2098880	the predictions from an IVL model to the results from an experiment in a particular task.
2100480	2107920	What I'm going to show you now is a set of examples of human likeness of IVL agents.
2109360	2116800	And it is important to know that all these examples that I selected here do not fit data.
2116880	2122960	That is in none of these examples, I first collect the data and then I fit the model to the data and
2122960	2130480	then I present the results. Anybody can fit data. All these examples come from the theory.
2130480	2138080	So we do simulations with our theory and then we look at the data and we see how close we are
2138080	2144880	to the data. Okay. But we don't fit data. That being said, doesn't mean that we cannot fit data.
2144880	2150720	We of course can fit data. And that is only going to make things better, right? So we can
2150720	2156800	fit data and we can fit data at the global level, at the individual level, whatever level you want.
2156800	2160560	We can fit the data, but that's not the point of the examples I'm going to show you.
2161680	2168320	So because I started with a very complicated task and nobody understood what the guy was doing,
2168560	2176080	something happened on the way that made me realize I need to simplify things to make things
2176080	2182880	understandable. And so I went to the root of decision making, which is binary choice.
2184000	2189520	But in this case, it was not going to be just the decision tree. It was going to be a dynamic
2189520	2195200	binary choice task. By the way, what I'm going to show since even that there are many examples,
2195760	2203360	it's just a snippet of the various examples. And if you want to go in more detail in any of them,
2203360	2210800	I can, but I wanted to show a good variety of these examples. So the first one is binary choice.
2210800	2216800	And essentially in this case, this is sort of an example of the task. People just have two
2216800	2224480	buttons and they receive an outcome after clicking on a button. And then they go on and on each of
2224480	2231920	these buttons have a particular distribution of outcomes assigned to it. And so the idea is that
2231920	2237840	over time, people are going to learn how to obtain more points out of the right button, the bottom
2237840	2247200	that is the maximum expected value, right, but from experience. So what this figure is, of course,
2247200	2255520	not readable, but the idea is to demonstrate how each of these squares are different problems
2255520	2263760	with different distributions in the two buttons. And it shows the proportion of maximization
2264480	2272800	is a Y axis. And it shows two lines in each of these things. The dotted line is the observed.
2273760	2280560	Actually, it's not the maximization, it's the risky rate. So the proportion of times you choose a
2280560	2290080	risky option. So the dotted line is the human data observed in an experiment. And the black line is
2290080	2297680	ideal predictions. And so the idea of this figure is just to show that in a large variety of problems
2297680	2308320	of these tasks, the model just from simulation is able to predict the learning process that humans
2308960	2320960	follow in this particular task. So the next sort of complication in this journey was to actually
2320960	2331120	demonstrate that if the probabilities change over time, the model was still going to be able to
2331120	2339200	predict that trend that humans would be able to do when they were performing a changing task.
2339760	2347680	So in this particular example, this shows the binary options and the change in probability
2347760	2354320	during a particular time period. And so humans are doing still that binary choice task,
2354320	2363200	but the probability changes of one of the options. And so the figures show the rate of
2363200	2370560	observed choices, again in the dotted line, from human behavior and the predictions from the IVL
2370640	2379680	model. Numerically, we can calculate the actual relationship between human and model with some
2379680	2389360	metrics, but I'm going to come back to that later on. Another example is a collaboration,
2389360	2397040	a cooperation between two agents in the prisoner's dilemma. This is some of the data I presented
2397040	2405280	also the other day in the workshop, but I'm expanding these figures now a little bit. So the
2405280	2414320	idea was that if we can emulate the choice, binary choice in this case of a particular individual,
2414880	2422160	can we emulate the behavior, the collective behavior in this case, cooperation,
2422160	2429120	emergent cooperation of two individuals working together in particular in the prisoner's dilemma
2429120	2437680	in this example. So we created copies of the IVL model, one representing player one, the other
2437680	2444480	one representing player two, and we put them to work in the prisoner's dilemma, and we did
2444480	2450960	different levels of information provided to the players. So what we observe here
2451600	2458080	is again the model, in this case the model is the lighter color and the human behavior,
2458080	2465920	and this is the proportion of the collective cooperation of the pair. And what you see here
2465920	2472960	is actually the strategies, the sequential strategies. So for example, mistrust is the
2472960	2480080	number of times that a player defects after the two players have defected. Forgiveness is the
2480080	2488240	proportion of cooperation after the other player defected even though I cooperated, etc. So the
2488240	2495520	point of this is that the model also captures the sequential strategies over time. So it's not
2495520	2503360	just capturing the outcome metric, but also the effects of the sequential strategies and also does
2503440	2513120	it with different levels of information. We did have to change the blending equation to capture
2513120	2518320	the descriptive information. I can tell you more about that. I explained that in our talk the other
2518320	2527840	day. Okay, so the next sort of escalation of these examples is whether it can capture the effects,
2527840	2536320	the collective effects of groups. So for this what we did was to create different networks
2536880	2544560	with different numbers of connections, and we used two different pay-off matrices,
2544560	2551920	and this came from the work of Valier and colleagues on a taxonomy of two by two games.
2552480	2560560	So we chose the extremes of those two by two games, the independence and the interdependence
2561120	2570160	games, and we run simulations with our IVL models to play with 16 players in this case,
2571040	2579280	making collective decisions, and then we aggregated their results. And what we observe is that in
2579280	2587920	the independence matrix, the collectives are able to figure out what is the best option for
2587920	2597200	everybody, that is this 5.5, actually faster the more connections there are. So they are pretty
2597200	2604000	good at finding the right option pretty quickly, almost immediately we can see this, but it's faster
2604000	2611120	the more connections there are. However, when there is the interdependence that is when the outcome
2611120	2618720	I'm going to get depends on the other person, that's not the case, that is more connections actually
2618720	2627600	resulted in less ability or inability to determine what is the best action for both of us, the AA
2628080	2636880	action. So this was curious for us, and you know this particular degradation of performance based
2636880	2645440	on the number of connections that the agents had, and so we wanted to see whether this in fact happens
2645440	2654720	with actual humans. So we run the study with teams, yes. What is the mechanism?
2654800	2658880	What is the mechanism? Why is interdependence so bad on?
2660400	2667840	Yeah, so the why which you know I'm not going to show you all the evidence for that, but the why has
2667840	2678400	to do with our ability to know the one-to-one correspondence of the actions. So if I play
2678480	2687760	with all of you, I need to keep track of what you did to me last time, and what Henrik did to me
2687760	2696080	last time, and what everybody did to me. It's very hard to learn that, right? So next time when I
2696080	2704880	meet you again, then I need to figure it out or remember, right? Oh, she did B, I should do A with
2704880	2712800	her, right? It's very hard to keep track of that, the more connections you have. The first one is not
2712800	2717680	difficult because it's very easy to find that regardless of how many connections you have,
2717680	2724560	if you pick A, that's what everybody should pick. But doesn't it depend on the payoff structure
2724560	2732240	specifically of this interdependence pattern? I think it does. Okay, I secure myself. You could
2732240	2737600	have a slightly different payoff structure, you could have the default. I don't remember what she
2737600	2746560	did, but it does in a pro-social way. Yeah, it does. It would go up. It does, and I mean, this is
2746560	2754080	an example that it does, right? But in addition to that, Valier et al. have a whole taxonomy
2754080	2760480	of two by two games, and we run this model in the whole taxonomy, and this was the only,
2760560	2766320	there were other things that happened, but this was the only one in which we saw this effect.
2767120	2772800	So this paper is still, I haven't published this paper, but it's still in process.
2774400	2779920	By the way, sorry, I had to step out of it. That was my doctor call. So the mechanism you just
2779920	2787760	described seems to depend sensitively on the internal structure of these computational agents
2788480	2792960	on their memory capacity. I mean, if they have a representation which does
2793760	2802560	simply record what every other player did, then this wouldn't happen. So there are some choices here
2803680	2809280	of kind of the parameters and the structure, and maybe even your definition of similarity, right?
2809280	2819200	So, I mean, similarity would only ask what's the, what happened the last time that Catrine played
2819200	2826720	that way as opposed to recognizing that I can learn about my interactions from Catrine,
2826720	2834640	from my interactions with Iname, Hi, Iname. So, I mean, the, so there are a lot of choices
2834640	2841440	underneath here. I mean, the choices are very transparent in the equations of the model.
2841440	2849760	That's the choices. In this particular task, the binary choice, you don't even have attributes.
2849760	2856160	You know A in an outcome. So there is not even a similarity in this case, but the frequency
2856160	2862960	and the recency play a role, and that's just organic in the model. There is no, yeah,
2862960	2869200	there is a choice of the parameter of the K, but we don't in, I forgot to say that, in one of these
2869200	2875600	results, we are manipulating that parameter. We are using the default parameter that is used in
2875600	2881440	Act R. So it's just coming from, these predictions are coming from the theory.
2882400	2890240	But in this setting, aren't there some choices about how I represent past events?
2890320	2897120	Very good. Yeah, in every setting. And that is the major point. I'm going to come back to that
2897120	2903520	later if you don't mind. But that is exactly the major, one of the major things that we need to
2903520	2910080	improve. The main choice is the designer of the model on what to represent in the instance.
2911200	2917600	We say that we don't represent anything that is not fully available to the human.
2917600	2923360	And it's true. That's what we try to be very committed to do. But still there are choices
2923360	2929120	on what to represent in the instance. This case is not that case because it's very simple.
2929120	2938320	You just see A or B. There is no attributes and the outcome. I can come back to the issue of
2938320	2943920	representation. I know there are many questions. Do you think you could wrap up in like five minutes?
2944160	2951760	Okay. And then so there is still time for more discussion. Then here is where my strategy to
2951760	2958960	cut a lot of stuff comes. So I'm going to show a few more things. I'm going to wrap up in five
2958960	2971680	minutes then. Okay. So again, we run the exercise with human participants in groups. And the human
2971760	2978640	participants in this case were groups of six because running 16 was a lot more complicated.
2978640	2984320	But we managed to gather a good number of groups in each of the conditions. And this is what we
2984320	2992240	observed. So the main effect that we predicted was verified with the human data.
2993920	3000480	Okay. The next example is a little bit more complicated and is more realistic too. And in
3000480	3007680	this example, there are many features. It's about phishing. And it's about our human decision to
3008720	3016720	whether let pass phishing email or not. So we use a data set that was collected by someone else,
3016720	3023680	not us. And that data set was collected in this form. So there were a bunch of emails that they
3023680	3029520	created and some of them were real. So real phishing and real ham. Those are real emails.
3029520	3038960	And then they simulated some phishing and ham emails. And then they asked people to rate the
3039760	3047680	phishing, the emails, whether they were definitely safe to definitely suspicious. And so what we did
3047680	3057920	is we plotted the human data. It's here. We emulated the same decision with our model. And it's here.
3057920	3064160	And we can see that the distributions of the model are very similar to that of the human.
3065120	3071040	And finally, an even more complex task because this is a very popular theme of research is
3071040	3078720	cybersecurity. And in this particular case, we are emulating the decisions of a defender in an
3078800	3089600	individual task. And there is a network that the defender needs to keep safe. And there are some
3090240	3103360	strategies of red agents or red attackers. Two strategies are implemented. And the strategies
3103360	3110160	vary in the way they explore the network to reach the objective, which is this operational server.
3111040	3120080	The interface that the humans use to do this task is what you see here. And we again run
3120080	3129120	and publish a paper with just the predictions of what the model does against as defending against
3129440	3142400	two attacker strategies, the meander and the other more direct strategy. And this is our results
3142400	3150800	from the human experiment. Again, we cannot run 2000 episodes in that case. So we run seven episodes,
3150800	3158240	but we see how our human experiment emulates the predictions from that model.
3159520	3165760	So, okay, given the time, I think I would like to conclude with this, which is
3167040	3176560	our reflection of human likeness. So I think what I have been talking about now regarding human
3176560	3186480	likeness is based on outcome metrics. So I emulate the decisions of the human, and then I compare
3186480	3193600	the outcome of the model to the outcome of the human. Usually we use generic metrics like MSE.
3194320	3200320	Often they are used at the aggregate level to be able to say, hey, our model is doing very well,
3200320	3205920	and that's what every cognitive modeler does. But I think that's wrong. I think we really need to
3205920	3215120	improve this metric, especially that we have the mechanisms to be able to compare at a much lower
3215120	3222400	level. We should always at least compare at the individual level and be able to evaluate the steps,
3222400	3236960	each of the steps at the individual level. Let me say just that we are using now these models
3237520	3246480	more directly in some applications to be able to help prevent the biases that humans have,
3247040	3253760	and to be able to help humans make better decisions. For example, connecting these models
3253760	3262400	with machine learning models and optimization models, and using our cognitive models as human
3262480	3273280	collaborators. This is one of the ways in which we are actually using our models in cybersecurity.
3274480	3283360	By emulating the actions of an attacker or an end user in the case of phishing, we can actually
3283360	3291520	provide the predictions of what these humans are going to do to heavy machine learning algorithms
3291520	3299520	that are being used as defender strategy. By providing these predictions, these defender
3299520	3307520	strategies are being adapted to be able to modify, in these cases, a stalker per security game,
3307520	3315600	modify the allocation of defense resources. The other way in which we are using these models
3315600	3325040	is by making them work with the human, along with the human. Instead of having one individual
3325040	3334000	making decisions in the cybersecurity task, now we have a team. The team is an AI and a human
3334000	3341920	making decisions in this type of security task. They collaborate with each other and they are
3341920	3352880	able to accomplish the task better than a human alone can do. In conclusion, our current focus of
3352960	3363920	AI, I thought I had it. I'm almost finished though, but I thought I had it. Maybe I never,
3364080	3366960	oh, it's not connected.
3380080	3387600	Our current focus on AI is on algorithms that aim at making faster and better decisions. The
3387600	3394720	current focus of AI is on creating algorithms that compete with the human and that are able to
3394720	3402400	make faster and better decisions than humans do. Our goal in contrast is to build learning
3402400	3408480	algorithms that can emulate the cognitive processes and can inform those optimization
3408480	3415280	algorithms to be able to speed up the human decision process. There is evidence of human
3415280	3422960	likeness in these IPL algorithms, but we know that we need to improve the metrics of human likeness.
3422960	3431760	What do we really mean by human likeness? We need to develop this evaluation of human
3431760	3440160	likeness at the cognitive level steps. That is what I think is required to demonstrate some of the
3440160	3447440	usefulness of these models. Yeah, thank you.
3452160	3454960	Maybe some people may believe, but who can stay, please go ahead.
3454960	3469120	Thank you for this talk. You mentioned the prospect theory of Kahneman and Tversky,
3469120	3476000	and one of the strengths of that was not only trying to match human data on specific examples,
3476000	3482800	but also being able to abstract out very general biases, human cognitive biases.
3483760	3494000	Kahneman wrote a whole book about it. Can this also abstract out from your data,
3494000	3497520	very general human biases in these dynamic situations?
3497520	3503280	Some, but we haven't done a lot of work on that, but confirmation bias, for example,
3503280	3510320	is something that we can definitely predict. The big difference is that we are interested in
3510320	3518320	biases from experience. That's in very sharp contrast, because in their theory, they rely on
3518320	3526720	descriptions of the options. Having the descriptions is very different than just acting and observing
3526720	3532160	the outcomes. We only focus on biases from experience, and one of them is confirmation
3532160	3538480	biases, and yes, our model can predict that. There is quite a lot of work to do on characterizing
3538480	3542960	those biases that we can predict from experience that we haven't done.
3542960	3544960	There's a question from Tenda.
3544960	3552560	Can I just go up on this? One of the things that Kahneman and Tversky most rarely or
3552560	3556720	ever implemented computational is basically just a verbal re-description of what's going on
3557440	3562320	and something really disputed. But this basically, you are showing computational what can emerge,
3562320	3564880	and maybe there is no general class even for some of them.
3565760	3572080	I'm a little frustrated with the heuristic and biases situation, because it cannot
3572080	3577520	grow longer because there is no more paper, I guess. I don't know. It just keeps growing and
3577520	3584720	growing, and new biases are emerging. I was very much in line with the initial idea. I really
3584720	3590480	like it, and I think it's still powerful. But now everybody wants to bring a bias out. It's like
3590800	3601360	the 20 questions, right? The idea of Alain Newell is we are doing all these very tiny experiments,
3601360	3608240	but we are forgetting about the global picture. Definitely, there is a lot more work to do on
3608240	3609760	biases from experience.
3609760	3618560	I was wondering about the previous slide of where I don't...
3619840	3621120	Write the previous to this one?
3624400	3631280	So I'm wondering if there's a little bit of cluster clarity, but in a way, on the one hand,
3631280	3635760	we want machines to make that for humans. On the other hand, we're
3636480	3639680	evaluating machines, and there's similarity with human decision-making.
3644160	3647520	So it seems that there needs to be some kind of decomposition of human decision-making in
3647520	3652000	terms of what do we actually want to extract from it? What comes from human limitations,
3652000	3656560	just cognitive limitations, not being able to evaluate everything? How do we split that, I guess?
3657680	3665520	I had a lot of slides going step by step in the process. Actually, a paper that
3665520	3672000	is coming out in PPS does that, so if you are interested, I can send you that.
3672800	3682560	But yeah, so basically, I would break out... I broke out each of the steps in the IVLT process
3682560	3688400	to analyze what we know and what we need to know. And definitely, one of those things are
3689280	3696960	evidence for particular similarity metrics. So in our model, for example, we often use just
3696960	3704080	linear similarity, and sometimes if it doesn't work well, we change it. There is not a lot of
3704080	3710320	theory, even though there is quite a lot of work on similarity judgments, but how to translate
3710320	3716720	that empirical warning to a computational mathematical form and then be able to test it
3716720	3722240	within the realm of the model, we haven't done that work, and that's one of the areas we need to
3722240	3731600	work on. Koti, there is a question for Daniel then. Yes. Thank you for a very interesting paper.
3731600	3742720	There are two points that I wasn't sure I understood. First of all, I want to applaud the idea
3743520	3751120	that you want to enhance human decision-making and make it rather than make it
3752160	3761840	slavishly dependent on AI decision-making. This is a theme that I've been discussing for years.
3764000	3770080	I call it the difference between the Nautilus machine and the bulldozer.
3771040	3778720	The bulldozer lets you move mountains, but you're still a 98-pound weakling. The Nautilus machine,
3780400	3786480	you actually develop the strength yourself. The technology is being used to improve your
3786480	3796080	abilities and make you less dependent on the technology. So I picked that point up. I want
3796080	3804320	to see if you stress it the same way I do. But the question that I really want to ask you about is
3810720	3819760	consider the notorious, rueful, reflective remark of somebody who says, well, it seemed like a good
3819760	3827920	idea at the time. And this requires memory of your past decision-making in detail
3829360	3837040	and memory of your evaluation and the grounds for that evaluation. Now that seems to me to
3837040	3844240	be a very important part of human decision-making, that is the capacity to be self-critical and
3844240	3854000	reflective and to learn from your mistakes by being able to debug your past performances.
3854640	3865360	I didn't think that I saw any sign of that reflective capacity in your presentation,
3865360	3872720	but maybe I just missed it the way you were saying it. Yeah, let me address the second point before I
3872720	3883440	forget. My memory has a large decay. So yes, you are correct that I didn't address that point
3883440	3890480	very concretely in my presentation. But that idea is essentially
3890640	3903120	expressive. You can see this slide in the feedback mechanism. And so what happens is that
3904800	3910960	usually the feedback is delayed and there is nothing we can do about it. And furthermore,
3910960	3918560	we can make many decisions and then get one outcome. And we don't know which of the decisions we
3918560	3926640	made is responsible for that outcome. So we use a concept that is well-known in AI of credit
3926640	3935840	assignment. So how do we associate that outcome to the various decisions that were made? This is
3935840	3941280	the other major area of research. We have a paper that we've been having trouble to
3941840	3949040	publish, but it's made available online where we test various mechanisms of credit assignment,
3949040	3959120	including the TD mechanism in reinforcement learning. So this is a very important part of
3959120	3966640	learning because it's the only way that you can modify the expected utility with the actual utility
3966640	3974160	of the decisions you are making and how you do that process will determine a lot how the learning
3974160	3983280	is done. So we usually just do equal credit until this paper that we are still exploring and
3983280	3989520	figuring out different credit assignments and trying to figure out how humans do that credit
3989520	3996320	assignment. And so there's quite a lot of research that is needed in that area. And to go back to
3996320	4005040	your first point of the bulldozer and dependency on technology, I think we are going to be dependent
4005040	4012080	on technology no matter what. And I think that's a good thing. I don't have anything against
4012800	4023280	technology, but the essence of human life is the human and the essence of any decision making
4023280	4030640	in any complex environments I think will continue to be the human. So I think we should use the
4030640	4039040	technology to empower the human to be able to help the human improve their decisions. And learning
4039040	4046240	mechanisms is essential for that, but figuring out how. So it's not like I give you what you want now.
4046640	4054400	It's like a spoon feeding you on what you need right now. It needs to be a lot more global than
4054400	4073680	that. Thank you. Let me just comment. I think if you try, I'm trying to imagine your systems
4074560	4085200	acting as sort of tutors for human beings and tutors that have good models of how the human
4085200	4093440	beings are making decisions because they are also models about how the tutors are making decisions.
4094320	4105840	And so that in effect the tutor can say, yeah, I'm making this up now. Yeah, I was tempted by that
4107120	4114640	way of thinking about this problem. You are absolutely correct. One of the most successful
4114720	4122160	programs from ACTAAR has been on cognitive tutors. So being able to use ACTAAR to
4123280	4130080	help children learn mathematics. So you are absolutely right. We are using tutors of a
4130080	4136160	different kind, we can say. It's not about mathematics. It's about making choices in some
4136160	4143600	complex environments. But that is exactly how we are using it. We can trace, as I was saying
4143600	4150880	before, we can trace specific students, decision makers and their history and therefore we can
4150880	4157520	make predictions about the decisions that particular person is going to make in the next
4158160	4166400	opportunity. And then we can use other tools, machine learning included or any other AI tools
4166400	4169040	to support that decision making process.
4172800	4178560	There are maybe a couple more questions from people who want to. You choose.
4178560	4185520	We've talked about crisis and errors and we can only say if you see that as something
4185520	4194400	like ACTAAR, in a way they have to agree with the evidence. They have an adaptation to the world
4194400	4202480	in which we live or have lived. So in your conception of a dynamic environment, is it also
4202480	4214800	part of the idea to change the environment to have what we call biases not occur as biases
4214800	4221520	in shortcomings? We will have interventions on those biases. Yes. So a particular program
4221520	4230480	which hasn't started yet is from IRPA. So I am about to start a very big program in IRPA on
4230480	4238400	cybersecurity and it's about predicting the biases of the attackers. The idea is, of course,
4238400	4247120	that we can modify the situation from the defense side so that we can trap the attacker. And given
4247120	4254880	that the attackers are as humans as the defenders are, we can trace down and predict when they are
4254880	4261440	falling trap of certain biases and then be able to react according to that. So yeah, I think
4261440	4267200	biases are emergent. Again, there are some biases that are not emergent related to Melanie's
4268480	4274960	question. There are some biases that are based on information, the explicit information, but the
4274960	4281680	ones that we are able to handle are the ones that are emergent from experience. And yeah,
4281680	4287840	we are working on that too. Yeah, it's a question about decision making and science.
4289200	4295680	So you are trying to emulate the way people actually make decisions, but we lack a lot of
4295680	4300400	detailed knowledge. We wish we had about the neurophysiological mechanisms that realize the
4300400	4306960	procedures that make the decisions. And so you are necessarily sort of, you have to go beyond
4307840	4310880	that and you are making decisions that are under-determined by evidence.
4311920	4318560	And I wonder what kind of commitments or maybe even heuristics you use when it's, you have to
4318560	4324880	make these decisions about what kind of a learner or what kind of a decision maker a human is.
4326160	4331600	Does that question make sense? Let me try to interpret it. I am going to answer based on what
4331600	4341920	I understood. Our type of modeling is symbolic. And so what that means is I don't really go into
4341920	4352160	the neuro part of it. ATAR, that's right. In my personal opinion, that is why it became very complex
4352880	4361920	because now ATAR is trying to map particular mechanisms to activation in certain part of the
4361920	4368880	brains. While that can be very important and necessary, that is not what I do. So I stay at
4368880	4375440	this symbolic level. So that is one thing, but there was another part in your question.
4376400	4381760	I know Kelly, you will meet with Koti now. Maybe these two guys could go.
4382800	4384960	Because maybe you would not be able to talk for a while.
4386880	4394480	Unfortunately, my afternoon is full. So here's my question. So one way to learn to imitate
4394480	4399360	human decision makers would be machine learning. And I know some people are doing that too.
4400080	4405200	And just treat the human's behavior as a thing to be predicted.
4407600	4413520	And I generally much prefer mechanistic models to machine learning. On the other hand, when you
4413520	4420800	have a mechanistic model, then you are in a position of having to either defend the specific
4420880	4431760	mechanism or argue that you're only trying to understand the typical behavior of a wide
4431760	4437520	variety of mechanisms. So when I look at the model, the mathematical model you showed,
4438800	4443600	and maybe there's something I'm missing, I'm not an expert in this area, it looked like a kind of
4444560	4451280	standard online learning model, but with the delay from time. And I know that scientists
4451280	4457360	have tried to figure out, do people forget things exponentially fast, or is it a power law, etc.
4458880	4464880	But then at least some of the examples you showed, like shifting this one where the
4464880	4473200	expected outcomes of the two alternatives were changing. Honestly, I felt like a very
4473200	4481120	wide variety of models that would update their behavior and forget old data to some extent
4481120	4489840	would have produced very similar things. So that made me not so convinced that the specific
4489840	4497360	mechanistic model you wrote down is really the right one. Although certainly some aspects of it,
4497360	4501840	like forgetting the past so you can learn about new things, is surely part of it.
4501920	4508960	But I guess that on the other hand, when I think about humans in dynamic situations,
4510960	4518560	I think that their notion of similarity can be much more general than, for instance,
4518560	4524800	looking at kind of individual attributes of the instance. People do all kinds of analogy making
4524800	4530400	and, you know, like, oh, that bit of grass which is still smoldering, which might flare up again.
4530960	4535200	Oh, that's kind of like there might be a tiger there, or it's kind of like
4536080	4543920	maybe there's an ambush being played by my enemies or whatever. So I don't know, the mechanism seems
4545680	4555040	both impoverished in a way, and maybe over committed to specificity in a way. So at the same
4555840	4559120	time, I'm glad that you're doing something mechanistic instead of just saying,
4559840	4563680	I'll have a year old network, watch for human, and then predict what the human would do,
4564400	4570480	which is the kind of thing which is far too popular nowadays. So I'm such a long question,
4570480	4576960	and maybe it's a long answer. No, I love these type of questions. Let me just answer with what
4576960	4583760	other two people have said. The first one is all models are wrong, right? And I totally
4585920	4594240	no accept a priority that this model cannot be perfect in any way, right? So all models are
4594240	4600320	wrong, but some models are useful. So I like to concentrate on the usefulness of these models,
4600320	4609360	what it can teach us, what it can do, right? And then the other quote is about using models as
4609360	4617280	your toothbrush. And yes, we have the tendency to use our models only and forget about other people's
4617280	4628800	models. We have done comparison, model comparison, and there are including a site review paper of
4628800	4634960	the comparison of our model with another 16 models. I mean, the proliferation of models
4634960	4641520	for this kind of thing is huge, right? So many. But there was a competition, modeling competition,
4642240	4648720	and we tested our model against the winners of that competition and many other of the more
4648720	4655920	typical models. And our model comes to be more general, and it comes to be more predictive than
4656000	4665520	most of them. So again, am I using my own toothbrush? Maybe. I like to use my own toothbrush.
4666160	4673440	The other part is regarding the machine learning. Yeah, I think human likeness depends on what you
4673440	4681600	want to do, right? So if you just want to predict a particular outcome, of course, having a large
4681600	4689680	amount of data can help. As I said, everything I presented is predictions from theory. I didn't
4689680	4696400	use any data before I made the predictions. No human machine learning algorithm would be able
4696400	4701920	to do that. They need the data. In fact, the less data they have, the worse they are, right?
4702480	4705680	Okay. Thank you. Thanks for your time.
4709600	4713440	I want to ask, in a different way, the same question that was already asked three times
4713440	4718000	at the beginning, which is the question of ethics. Because, well, two things. It seems that this is
4718000	4723920	a visibly important question in decision-making and situations of warfare or juridical situations.
4724640	4731760	And secondly, it belongs to philosophy, right? To the Western tradition that runs from Aristotle,
4731760	4738000	through Emmanuel Levinas, and so on. And the one consensus that the big players within this
4738000	4744080	tradition all share, it seems to me, is that from the Greek, you know, Pyrenees and their idea of
4744080	4751040	Epoche is as vertiginous suspension of all decidability all the way through to Walter Benjamin's
4751040	4760800	idea of mystical origins of justice. The consensus seems to be that the ethical or just decision
4760800	4769360	can only come from a place of radical undecidability, right? From non-calculability, right? Law is
4769360	4776240	about calculus, but justice comes from the incalculable. So, in that sense, the distinction
4776240	4781440	to be made would not be between humans and machines. It would be between calculability
4781440	4788080	and non-calculability. If you can calculate ethics or justice, it's not ethics or justice
4788080	4794800	anymore. It's just control, right? I just wanted, that would be the provocation I would put to you
4795440	4800400	in the name of philosophy, right? So, it's a different language game that we're working
4800400	4805040	at using. As soon as you deal with ethics, necessarily you're taking that on board, right?
4805040	4811840	I think that is a world or a controlled world. I guess that is a philosophical question
4811920	4819200	for our model to be able to account for ethical issues. There needs to be
4819200	4832160	calculability. And so, you know, philosophically, that moves us out of the ethics realm. I would
4832160	4836560	accept it because there is nothing else we can do without calculability.
4836560	4839280	Thank you very much.
