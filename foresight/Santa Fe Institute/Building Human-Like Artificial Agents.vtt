WEBVTT

00:00.000 --> 00:05.760
dynamic decision-making laboratory. She has made various contributions to theories models and empirical

00:05.760 --> 00:10.960
research on individual and group decision-making, in particular in complex and dynamic situations,

00:10.960 --> 00:17.040
which is what drew me to her work and also your texture as one of the most important

00:17.040 --> 00:22.320
quality scientists today. And I think it's been working also a lot, and she's writing to Human

00:22.320 --> 00:29.280
Air Coordination and Cybersecurity, and she co-directs one of these new NSF national air

00:29.280 --> 00:35.200
research institutes. This one is called Alliance for Societal Decision-Making. She is a fellow

00:35.200 --> 00:40.160
authority science society, human factors ergonomic society editor of various leading journals,

00:40.160 --> 00:44.400
and she has been leading a wide range of multi-million and multi-year collaborative

00:44.400 --> 00:49.680
efforts to guarantee industry, such as multi-university research, initiative grants from

00:49.680 --> 00:54.880
Army research laboratories in Army research office, and large collaborative projects

00:54.880 --> 00:59.840
with Dartmouth. And she has visited SFI a number of times. It's one of the lead authors on the

00:59.840 --> 01:03.040
paper on collective adaptation. That is the background for the workshop. That was just

01:03.040 --> 01:10.000
happening at SFI, and I'm glad to hear your talk. Thank you. Thank you so very much for having me

01:10.000 --> 01:16.560
here. I've been to Santa Fe in the past, and every time I come here, I'm like in heaven. I feel that

01:16.560 --> 01:24.320
I can relax, finally, even for a little short time. So what I'm going to talk about today is

01:25.120 --> 01:32.240
work that basically I've been doing all my life, all my research life, and it's this idea of building

01:32.880 --> 01:42.880
agents that look at, are like humans in terms of the thinking and the cognition and the decisions

01:43.680 --> 01:53.040
these agents make. But in particular, I am interested in studying very complex dynamic

01:53.040 --> 02:01.200
situations. We call these situations situations of dynamic decision making. So in conditions like

02:01.200 --> 02:11.040
the examples you see here, there are more than one decisions that need to be made. We make multiple

02:11.040 --> 02:19.920
interdependent decisions under a lot of stress and time constraints. There is usually high uncertainty

02:19.920 --> 02:27.200
on the way we make decisions and take, for example, the disaster management case. So there are multiple

02:27.200 --> 02:34.240
actors working in trying to allocate resources in a very constrained environment. Those resources

02:34.480 --> 02:42.240
are usually limited. And so figuring out how to allocate those resources on time to save the victims

02:42.240 --> 02:48.720
is a real challenge. Obviously, a challenge in which the individual human cognitive abilities

02:48.720 --> 02:57.440
also play a big role. We cannot remember everything. We have to sleep. We are stressed. We feel the

02:57.520 --> 03:05.440
time pressure, et cetera. So these are the type of situations I'm interested in helping humans

03:05.440 --> 03:15.520
make better decisions. And AI, at least one of the initial goals of AI, was to create agents

03:15.520 --> 03:21.760
that would be undistinguishable from humans. So be able to create agents that would make decisions

03:21.760 --> 03:29.840
like humans do. But in reality, the current view of AI, at least, is about machine learning

03:29.840 --> 03:38.720
algorithms that are trained on data. And they are trained to optimize decisions that is very much

03:38.720 --> 03:47.840
not human-like because humans are not optimal at how we make decisions. So the goal of these AI

03:47.840 --> 03:56.400
tools has been to create these elements of optimal decision-making, faster and better than humans.

03:56.400 --> 04:04.960
And in fact, many times, we see competitions between humans and AI. And of course, they are

04:04.960 --> 04:12.880
amazing and they are fantastic and we need them. And they have amazing applications now with

04:12.960 --> 04:22.880
increasingly complex data sets and features. And be able now, obviously, to generate content

04:22.880 --> 04:31.600
from PROM, which is really incredible. So things are advancing very fast. However, I still

04:32.320 --> 04:40.080
believe that to be able to address complex problems like the one I have been talking about,

04:41.040 --> 04:51.200
we need more than algorithms that are optimal. I believe that these generation patterns can help

04:51.200 --> 04:57.760
in situations like this, but they are really not capable of making decisions on their own,

04:58.320 --> 05:05.120
at least not in the way humans would do it. Because the kind of situations humans have to confront

05:05.840 --> 05:12.720
are more than just economic, are more than just optimizing an objective value.

05:13.280 --> 05:20.400
They are about various factors and consequences, including ethical and moral trade-offs,

05:20.400 --> 05:30.160
that are very hard to emulate or to optimize. So I think the ultimate responsibility for

05:30.160 --> 05:37.280
decision making in these type of situations is human. And therefore, we need to figure out

05:37.280 --> 05:45.520
how we can use these tools together with tools that emulate human behavior so that we can help

05:45.520 --> 05:53.840
humans in these situations. So our goal, should I take questions now? Okay, go ahead.

05:53.840 --> 06:08.640
Yeah, I was just wondering about the previous slide and how you said that ethical and moral

06:08.640 --> 06:13.200
considerations. It seems that there is a fundamental assumption that you cannot quantify it or you

06:13.200 --> 06:17.600
cannot encode it as part of an objective function, but I wonder where does it come from?

06:18.560 --> 06:25.280
Yeah, I wouldn't say that we cannot. It's simply we haven't found out how. How to

06:26.320 --> 06:31.920
consider all the different trade-offs and the different type of demands that humans need to

06:33.440 --> 06:40.160
live when they are making decisions in these kind of situations. So that's precisely where we need

06:40.160 --> 06:49.600
to make progress. I just wanted to follow up. The idea about ethics and morals though is that

06:49.600 --> 06:54.640
they kind of can't be quantified because they are subjective and relative and it really is contextual

06:54.640 --> 07:00.640
and you define things very down to make it morally right or wrong. I guess that's a huge like complex

07:01.360 --> 07:06.720
web of trade-offs, I guess. But I guess like you said, it's not impossible. Yeah, I'm not saying

07:06.720 --> 07:13.600
it's impossible. I think we are going to get there, but I think we need more research on precisely

07:13.600 --> 07:20.480
how humans make decisions in these complex situations before we even attempt to create any

07:20.480 --> 07:27.680
computer program that tries to emulate that process. So that has been my goal, yes.

07:27.840 --> 07:37.840
I make complex decisions where I don't exactly know what are the outcomes. It also depends

07:37.840 --> 07:44.640
on my risk attitudes and how important it is. I mean people, for example, during pregnancy,

07:44.640 --> 07:49.360
there are very strong opinions and you are not allowed to do this and this varies over countries.

07:49.360 --> 07:56.720
But at the end, Emily Oster wrote this nice book just to know what are the facts and then

07:57.280 --> 08:05.040
everyone has to judge on his own or her own how to trade drinking wine against the low risk of,

08:05.040 --> 08:10.640
you know, whatever that would be. So these things are probably not part of...

08:11.680 --> 08:19.360
No, they are all part. They are all part. But you are going to see next how I make a distinction

08:19.360 --> 08:27.680
between sort of the traditional risk assessment and dynamic situations and how I believe we make

08:27.680 --> 08:37.040
decisions in dynamic tasks. Across individuals. Across individuals, across situations, absolutely.

08:39.200 --> 08:47.040
So my goal has been to improve and speed up human dynamic decision making. And we want to do that

08:47.040 --> 08:54.000
by building algorithms that emulate the human cognitive processes and that they can inform

08:54.880 --> 09:04.240
other tools like AI optimization algorithms so that we together can make better decisions

09:04.240 --> 09:12.320
in these kind of situations. I plan or I want to go beyond just being inspired by humans,

09:12.320 --> 09:19.520
which is what usually, you know, computer scientists would do. They do care about humans and

09:19.520 --> 09:26.560
they do observe what humans do, but usually they take humans as an inspiration only.

09:27.200 --> 09:33.120
What I'm saying is we need to go beyond just taking humans as an inspiration, but really

09:33.120 --> 09:41.520
understanding their cognitive processes and being able to emulate that process that is creating this

09:42.560 --> 09:51.920
algorithms that can emulate the decision processes, including the biases and errors.

09:51.920 --> 09:59.360
So I actually believe that being able to replicate human biases is essential,

09:59.360 --> 10:07.520
because the only way we can on bias humans is by being able to understand where the biases come

10:07.520 --> 10:14.880
from and then be able to use other tools to help humans make more correct decisions.

10:15.600 --> 10:22.960
So sometimes the process of using simple heuristics has been shown to be more effective

10:22.960 --> 10:31.680
in these dynamic complex tasks than even very complex reasoning or data sets that, you know,

10:32.400 --> 10:40.240
large amounts of data and algorithms that come from large amount of data often are not as good

10:40.240 --> 10:51.200
as human experience. So this is sort of the idea of using not the traditional approach to AI,

10:51.200 --> 10:58.560
but really to promote the idea of we need representations that can replicate the cognitive

10:58.560 --> 11:06.880
process. Why cognitive algorithms? Well, it's a little bit repetitive, I guess, here is we

11:06.880 --> 11:13.200
are not trying to create super humans, but we're trying to imitate cognition. And here are some

11:13.200 --> 11:21.280
of the advantages and some ways in which these type of algorithms can be used. So they can help

11:21.280 --> 11:28.160
explain the cognitive process of decisions on their uncertainty. They are also dynamic and they

11:28.160 --> 11:37.760
can learn, presumably like humans learn to make better decisions. They can inform other decision

11:37.760 --> 11:47.440
aids or other autonomous systems to help humans. They can also be synchronized with the humans.

11:47.440 --> 11:56.560
So for example, if I want to emulate Mirta's decision processes, I will need some information

11:56.560 --> 12:05.280
about Mirta's past experience, but I can trace Mirta's decisions over time to be able to predict

12:05.280 --> 12:10.880
what Mirta is going to do in a particular situation. So we can do that with these models.

12:11.440 --> 12:18.800
And we can also make them be collaborators with others. So if I can emulate Mirta's decision,

12:18.800 --> 12:24.160
then I can predict what Mirta is going to do when she collaborates with me. So that's in general

12:24.960 --> 12:30.800
the idea of why cognitive algorithms are different from traditional or at least

12:30.800 --> 12:39.280
currently traditional AI. So I have been trying to pursue these two essential questions. My whole

12:40.000 --> 12:46.560
research career is about how do human make decisions learn and adapt in dynamic tasks.

12:46.560 --> 12:52.800
And the second is how can I represent such process computationally. So I'm going to go with the first

12:52.800 --> 13:00.800
one too. And when I came to Carnegie Mellon, which was late 90s and beginning 2000, I started as a

13:00.800 --> 13:09.920
professor. The concept that I observed about decision making was very different from what I had in

13:09.920 --> 13:18.320
mind. It was static decision making, economics decision making. But I was inspired by many

13:18.320 --> 13:23.760
people, including this quote by Edward Edwards, where he said that static decision

13:24.480 --> 13:32.240
theories have only a limited future. Human beings learn and probabilities and values change.

13:32.880 --> 13:39.840
These facts mean that the really applicable kinds of decision theories will be dynamic and not

13:39.840 --> 13:49.120
static. So I was stubborn enough to pursue the idea that we want to study dynamic decision making

13:49.120 --> 13:52.960
even though nobody in my department understood what the hell I was talking about.

13:54.560 --> 14:00.640
This is what they were talking about. And this is a more traditional approach of classical

14:00.640 --> 14:07.360
decision making. It's a linear process where you have alternatives. The alternatives are usually

14:07.360 --> 14:13.520
well established, meaning they are obviously provided to you. You have option A and option

14:13.520 --> 14:20.400
B, usually represented as a decision tree, and then calculated based on expected value what

14:20.400 --> 14:28.080
humans should do. Expected value concept, of course, is an extremely useful today and always will be,

14:28.080 --> 14:35.280
I think, a concept on optimal decision making. But of course, very soon people realize that

14:36.000 --> 14:44.160
humans don't make decisions optimally. And so this famous theory by Kanieman and Taversky,

14:44.160 --> 14:52.400
which is still very popular and very influential today, what they essentially did was to modify

14:52.400 --> 14:59.680
the concept of expected value into functions that are more human-like. And of course, they

14:59.680 --> 15:05.760
supported these functions based on human behavior. So humans are not optimal. Instead,

15:05.760 --> 15:12.240
humans are behaving according to these other functions. And that's how you can find out what

15:12.240 --> 15:20.480
humans are going to do. And yeah, that's fantastic and still influential. But this type of theory

15:21.120 --> 15:26.560
is just not applicable to dynamic situations that are constantly

15:27.440 --> 15:36.320
adapting according to various factors over time. Now, if we go completely to the other extreme

15:36.320 --> 15:44.560
of what I was studying early 2000, I worked with Gary Klein in a very big project for the

15:44.560 --> 15:50.800
army research laboratories. And Gary was the opposite of all the behavioral economics. At

15:50.800 --> 15:57.920
that time, it wasn't behavioral, it was just economics. So Gary Klein was studying naturalistic

15:57.920 --> 16:04.800
situations, in particular firefighters. So there are books about naturalistic decision making.

16:04.800 --> 16:10.960
There is a conference. There are lots of people interested in this area. But essentially what

16:11.040 --> 16:18.080
you do in this case is you actually go to the place where decisions are made. Let's say

16:18.080 --> 16:24.960
firefighters. And he describes in his book how he would get on the trucks with the firefighters to

16:24.960 --> 16:31.360
find out how they make decisions in those very constrained tasks. And then he found out that

16:32.240 --> 16:38.240
they don't make decisions. At least that's what they express the firefighters. They just said,

16:38.240 --> 16:45.600
I just know what to do. I just get in there. I've seen these situations so many times in the past.

16:45.600 --> 16:55.680
I just know what to do. So he documents in his book all the stories about the many firefighters

16:55.680 --> 17:03.120
making decisions. And this type of approach has been used in hospitals and in many other

17:03.120 --> 17:12.080
naturalistic situations. So out of that work came a model that he called the recognition

17:12.080 --> 17:18.640
prime decision model, which was very influential to me. The essential element of recognition

17:18.640 --> 17:25.120
prime decision model is recognition. What is recognition is our ability to determine the

17:25.120 --> 17:31.600
similarity between a situation we are confronting and what we have confronted in the past.

17:32.560 --> 17:40.400
And so he proposes how such a match determines whether we know a situation is typical or not

17:40.400 --> 17:49.440
typical. And then based on that, we engage in some sort of mental simulation to figure out

17:50.480 --> 17:57.120
different courses of action. This presumably happens very fast. And then you are able to

17:57.120 --> 18:05.360
finally decide what to do and implement a course of action. So very inspiring. It made

18:05.360 --> 18:13.440
a lot of sense to me, but it wasn't computational. And it hasn't been as far as I know up to now.

18:13.440 --> 18:20.800
There has been many attempts to make this sort of theory computational, but to my knowledge,

18:20.800 --> 18:29.840
none of them have been really successful. So what I do is sort of in the middle of these two

18:29.840 --> 18:36.000
extremes. Dynamic decision making, I sort of realized that I'm not going to make a lot of

18:36.000 --> 18:42.720
progress if I go and get on the trucks with firefighters. So instead, I'm going to take

18:42.720 --> 18:52.080
the approach that others have taken like Brent Bremmer and Joaquin Funke to create micro worlds.

18:52.800 --> 19:01.600
So these are reductions of these real world situations in which we can actually use experimentation.

19:01.600 --> 19:07.600
And to me, it was very important to be able to see the cause and effect relationships,

19:07.600 --> 19:17.600
not only rely on observations. And so this is just a whole lot of the many micro worlds we

19:17.600 --> 19:26.560
have developed and used in my lab through the various years. Initially, I used and developed

19:26.560 --> 19:36.240
this and it took many, many years to develop. It's a water purification plant task that has

19:36.320 --> 19:42.320
all the major elements of dynamic decision making. It's a dynamic resource allocation with

19:42.320 --> 19:49.040
limitations of time, et cetera. So we developed a theory, we gathered a lot of information based

19:49.040 --> 19:55.360
on that task. But then the main question was like, well, whatever you are doing is only applicable

19:55.360 --> 20:00.560
to this task. You haven't shown me that what your models and your things that you are doing

20:01.360 --> 20:09.040
can be applicable to many tasks. So then we went wild and then we just developed all kinds of

20:09.040 --> 20:15.760
micro worlds and started to apply to many other tasks to demonstrate that our theory and our ideas

20:15.760 --> 20:26.240
were more general than just that particular task. Now, this is a summary of behavioral phenomena

20:26.320 --> 20:33.280
that came out of experimental work with many different micro worlds. So I'm going to go over it

20:33.280 --> 20:41.680
but relatively quickly given the time. So the first thing is when I arrived to this area, the

20:43.280 --> 20:53.680
picture was very frustratingly negative. So all humans are very poor at making decisions

20:53.680 --> 21:00.640
in dynamic environments. Even when you give them full feedback and you give them unlimited time

21:00.640 --> 21:07.760
incentives, extensive practice, we are born with something that doesn't allow us to make

21:08.400 --> 21:16.880
good decisions in these very complex environments. People are generally poor at handling systems

21:16.880 --> 21:24.720
with long feedback delays. A lot of people where Bremer, including basically almost all his work,

21:24.720 --> 21:31.760
was about demonstrating of long feedback delays and the effect of that on decision making. So the

21:31.760 --> 21:40.880
longer the delay is, the poorer decisions we make. So, okay, I was taking all that and I was in

21:40.880 --> 21:47.520
agreement with all that, but at the same time, I could observe that people in the real world,

21:47.520 --> 21:53.840
some of them are pretty good at making very complex decisions. So how can we explain that?

21:54.560 --> 22:02.880
And my response to that is that it's in the learning process. It's in understanding how we go from not

22:02.880 --> 22:10.000
knowing how to make decisions to really making very good decisions in really complex environments.

22:10.880 --> 22:18.240
So I started to study different things. For example, I found that if we give headroom, meaning

22:18.240 --> 22:24.800
space for learning to individual decision makers, then they are able to adapt to more

22:25.360 --> 22:33.120
difficult decision situations. For example, putting someone in a low time constraint is going to help

22:33.840 --> 22:40.480
for that person to perform under high time constraints more than those that are always

22:40.480 --> 22:45.920
trained under high time constraints. So the headroom is needed for learning.

22:47.120 --> 22:55.600
Heterogeneity, experiencing different variety of situations are going to help us to adapt

22:55.600 --> 23:03.760
to novel situations and we have several studies on that. The ability to pattern match is extremely

23:03.760 --> 23:11.600
important. At that time, we collected our ability to pattern match with the Raven progressive matrices

23:12.240 --> 23:19.760
test, which is essentially a pattern matching test. And we demonstrated how the score in that test

23:19.760 --> 23:25.920
is very predictive of the quality of the decisions that human makes in these micro worlds.

23:26.560 --> 23:31.920
And we also found how to provide feedback in a way that would be more useful

23:32.960 --> 23:39.600
to individuals by particularly providing observing behavior of an expert was extremely

23:39.600 --> 23:45.520
helpful for people to learn and to adapt even after removing that feedback.

23:46.320 --> 23:51.440
So this is a list of some of the phenomena and all the things that we have found.

23:53.280 --> 24:00.720
I think all over that list, there are two essential elements of dynamic decision making and

24:00.720 --> 24:09.520
they keep coming over and over. One of them was recognition. So I was convinced that similarity

24:09.600 --> 24:17.280
and our ability to detect similarity is essential for making good decisions and memory.

24:17.280 --> 24:26.240
That is our ability to create context specific knowledge. So very different from traditional

24:26.240 --> 24:32.880
cognitive theories that believe that humans with experience generate heuristics. I actually

24:32.880 --> 24:38.560
believed it was the other way around that humans use heuristics when they don't have knowledge.

24:39.280 --> 24:45.760
And as you acquire more context specific knowledge, you actually depart from those

24:45.760 --> 24:52.960
heuristics that are not good to start with. They are only approximate. And so you move away from

24:52.960 --> 25:00.960
those heuristics and you start to apply context specific knowledge. So let me move to the second

25:00.960 --> 25:08.800
question which is how to represent those things computationally. My approach is on cognitive

25:08.800 --> 25:22.240
architectures and in large part because I was at CMU. And in CMU is the birth part place

25:22.240 --> 25:30.000
of cognitive architectures and in many ways of AI too. So in particular, I was inspired by the work

25:30.000 --> 25:36.960
of Allen Newell and Herb Simon that wanted to do had this idea of creating unified theories of

25:36.960 --> 25:47.760
cognition. They imagine or envision this complete program that would be capable of making all the

25:49.040 --> 25:56.880
activities that human mind was able to make. So they imagine to represent all these cognitive

25:56.960 --> 26:05.120
steps and be able to explain all the components of the mind and how they work and produce cognition

26:05.120 --> 26:14.400
together. Many books that are very inspiring coming from this tradition. My personal view of this

26:14.400 --> 26:22.560
is that it is very utopic. It is an extremely complex problem and therefore is very hard

26:22.560 --> 26:31.360
to accomplish. So it was accomplished partly by the actor unified theory of cognition.

26:32.160 --> 26:38.800
So John Anderson at Carnegie Mellon and Christian LeVier, one of John Anderson's students and then

26:38.800 --> 26:46.720
postdoc and now faculty, research faculty has been working on this idea from Allen Newell

26:46.720 --> 26:55.440
and Herb Simon. And if you look at the history of actor, you are going to see that it has

26:55.440 --> 27:03.120
just become more complex with over the years. And in my personal opinion, also less useful.

27:04.640 --> 27:13.840
So what I did was to grab what I felt was useful from that cognitive architecture. And this is

27:13.840 --> 27:20.080
essentially the way knowledge is represented in declarative and procedural forms that is with

27:20.080 --> 27:29.520
facts or with rules and in symbolic or sub symbolic ways. That is with formulas that would

27:29.520 --> 27:38.560
explain how those facts or rules are going to evolve and change over time. So that's what I

27:38.560 --> 27:46.080
did and basically I took that to develop my own mini architecture, if you will. So this

27:46.880 --> 27:53.440
theory called instance-based learning theory is essentially a dynamic decision making process

27:53.440 --> 28:00.720
that is represented in a learning loop. So I'm going to go over this process and this is sort of

28:00.720 --> 28:07.840
the corresponding picture of RPM, the recognition prime, but my own and there are also many

28:07.840 --> 28:16.000
differences obviously. So the first step is again decisions are made by recognizing the

28:16.000 --> 28:24.640
similar situations and mapping with decisions that have been made in the past to be able to retrieve

28:24.640 --> 28:31.840
something that worked in the past. And that happens in this recognition process. We evaluate the new

28:31.840 --> 28:39.200
actions according to the utility of the past decisions which are retrieved from memory.

28:40.160 --> 28:49.040
And then we explore mentally all the possible alternatives. As you can see this is very similar

28:49.040 --> 28:56.720
to the mental simulation that Gary Klein was talking about. And then finally we make execute

28:57.440 --> 29:06.400
decision that is the highest up to that point and that execution is going to modify the environment.

29:06.400 --> 29:14.160
And finally whenever we get feedback we can reevaluate the utility of those decisions we

29:14.160 --> 29:21.760
have made in the past. So that's conceptually what the theory, the learning loop is about.

29:22.480 --> 29:32.080
Now let me tell you about the representations. So the idea is that decisions are stored over time

29:32.080 --> 29:39.840
in memory in the form of instances and those instances are triplets. Those triplets are the

29:39.840 --> 29:48.640
associations of the features that are necessary for that decision, the action that is taken

29:49.280 --> 29:57.120
and the outcome. The outcome can be the observed or the expected utility that is calculated

29:57.120 --> 30:04.880
about making that decision. Now for each potential action, action one or action two,

30:05.440 --> 30:12.560
instances when you try to make a new decision, instances will be blended according to the

30:12.640 --> 30:23.600
similarity of the features. And then the action that has the maximum blended value,

30:23.600 --> 30:32.560
in this case let's assume it's action two, is chosen. At that moment a new instance is created

30:32.560 --> 30:41.280
with the blended value. And then when feedback is received that blended value is changed for the

30:41.280 --> 30:48.000
actual feedback outcome that was received. So that's the algorithm in the in the

30:48.000 --> 30:55.920
representational form. And this is the algorithm in the mathematical form. So the most important

30:55.920 --> 31:04.480
equation I think in this algorithm is the activation, which is not developed by me at all,

31:04.480 --> 31:14.080
is borrowed from ACTAAR. So ACTAAR has backed up evidence for each part of this equation

31:14.080 --> 31:22.080
with a lot of experiments regarding how humans process information. So this activation equation

31:22.080 --> 31:28.720
has three parts. This part is called the base level equation. This part is the partial matching

31:29.600 --> 31:38.000
idea and this part is noise. And essentially this equation takes into account the frequency

31:38.000 --> 31:46.000
of events. So we tend to believe that something is going to happen more often and actually to

31:46.000 --> 31:54.960
retrieve more that information faster when it happens more often. So frequency of events matters

31:54.960 --> 32:01.360
a lot for our ability to retrieve information from memory. And so that is represented here.

32:02.240 --> 32:08.880
We also tend to forget. So things that happened yesterday I can remember faster than things that

32:08.880 --> 32:18.320
happened many years ago. And that is represented by this nonlinear decay function. And then this

32:18.400 --> 32:25.280
part here represents the partial matching equation for each feature in the instance. We can

32:25.920 --> 32:34.800
determine a particular similarity function, which is then aggregated across all the features

32:34.800 --> 32:45.280
and then sort of penalized or exacerbated with a partial matching parameter. And then finally

32:45.280 --> 32:50.480
the noise, which this is just noise. Right now there is not a lot of theory about

32:51.440 --> 32:59.280
what doesn't fit in this part. This part here is a draw from a random distribution or

32:59.840 --> 33:08.880
other type of distributions. And this one is one of the parameters. So for each instance

33:08.960 --> 33:19.680
you calculate the activation at each point of time. And then you can calculate the probability

33:19.680 --> 33:27.440
of retrieving that instance from memory, which is essentially the activation relative to the

33:27.440 --> 33:36.880
activation of all the instances in memory. And then this is the magic I guess of IVLT. What we

33:36.880 --> 33:44.320
do is essentially combine all those instances that belong to a particular choice option

33:45.200 --> 33:51.200
in the form of an expected value. So this is the probability times the outcome.

33:52.560 --> 33:58.960
But obviously this probability is a cognitive probability. It's not the actual probability

33:58.960 --> 34:03.760
of an event. And it's a probability that is calculated all the way from here.

34:04.160 --> 34:12.160
Right. So in this way we are accounting for the cognition of how do we forget information,

34:12.160 --> 34:19.680
how similar the information is, etc. Okay. And we create this expected utility value that we

34:19.680 --> 34:26.480
call blending. And then we select the option that has the maximum expected value. So that's it.

34:27.280 --> 34:37.760
Now I have been interested as I said before in figuring out how general this theory is

34:38.560 --> 34:47.840
across many decision-making situations. And how human-like this algorithm is by comparing

34:48.800 --> 34:58.880
the predictions from an IVL model to the results from an experiment in a particular task.

35:00.480 --> 35:07.920
What I'm going to show you now is a set of examples of human likeness of IVL agents.

35:09.360 --> 35:16.800
And it is important to know that all these examples that I selected here do not fit data.

35:16.880 --> 35:22.960
That is in none of these examples, I first collect the data and then I fit the model to the data and

35:22.960 --> 35:30.480
then I present the results. Anybody can fit data. All these examples come from the theory.

35:30.480 --> 35:38.080
So we do simulations with our theory and then we look at the data and we see how close we are

35:38.080 --> 35:44.880
to the data. Okay. But we don't fit data. That being said, doesn't mean that we cannot fit data.

35:44.880 --> 35:50.720
We of course can fit data. And that is only going to make things better, right? So we can

35:50.720 --> 35:56.800
fit data and we can fit data at the global level, at the individual level, whatever level you want.

35:56.800 --> 36:00.560
We can fit the data, but that's not the point of the examples I'm going to show you.

36:01.680 --> 36:08.320
So because I started with a very complicated task and nobody understood what the guy was doing,

36:08.560 --> 36:16.080
something happened on the way that made me realize I need to simplify things to make things

36:16.080 --> 36:22.880
understandable. And so I went to the root of decision making, which is binary choice.

36:24.000 --> 36:29.520
But in this case, it was not going to be just the decision tree. It was going to be a dynamic

36:29.520 --> 36:35.200
binary choice task. By the way, what I'm going to show since even that there are many examples,

36:35.760 --> 36:43.360
it's just a snippet of the various examples. And if you want to go in more detail in any of them,

36:43.360 --> 36:50.800
I can, but I wanted to show a good variety of these examples. So the first one is binary choice.

36:50.800 --> 36:56.800
And essentially in this case, this is sort of an example of the task. People just have two

36:56.800 --> 37:04.480
buttons and they receive an outcome after clicking on a button. And then they go on and on each of

37:04.480 --> 37:11.920
these buttons have a particular distribution of outcomes assigned to it. And so the idea is that

37:11.920 --> 37:17.840
over time, people are going to learn how to obtain more points out of the right button, the bottom

37:17.840 --> 37:27.200
that is the maximum expected value, right, but from experience. So what this figure is, of course,

37:27.200 --> 37:35.520
not readable, but the idea is to demonstrate how each of these squares are different problems

37:35.520 --> 37:43.760
with different distributions in the two buttons. And it shows the proportion of maximization

37:44.480 --> 37:52.800
is a Y axis. And it shows two lines in each of these things. The dotted line is the observed.

37:53.760 --> 38:00.560
Actually, it's not the maximization, it's the risky rate. So the proportion of times you choose a

38:00.560 --> 38:10.080
risky option. So the dotted line is the human data observed in an experiment. And the black line is

38:10.080 --> 38:17.680
ideal predictions. And so the idea of this figure is just to show that in a large variety of problems

38:17.680 --> 38:28.320
of these tasks, the model just from simulation is able to predict the learning process that humans

38:28.960 --> 38:40.960
follow in this particular task. So the next sort of complication in this journey was to actually

38:40.960 --> 38:51.120
demonstrate that if the probabilities change over time, the model was still going to be able to

38:51.120 --> 38:59.200
predict that trend that humans would be able to do when they were performing a changing task.

38:59.760 --> 39:07.680
So in this particular example, this shows the binary options and the change in probability

39:07.760 --> 39:14.320
during a particular time period. And so humans are doing still that binary choice task,

39:14.320 --> 39:23.200
but the probability changes of one of the options. And so the figures show the rate of

39:23.200 --> 39:30.560
observed choices, again in the dotted line, from human behavior and the predictions from the IVL

39:30.640 --> 39:39.680
model. Numerically, we can calculate the actual relationship between human and model with some

39:39.680 --> 39:49.360
metrics, but I'm going to come back to that later on. Another example is a collaboration,

39:49.360 --> 39:57.040
a cooperation between two agents in the prisoner's dilemma. This is some of the data I presented

39:57.040 --> 40:05.280
also the other day in the workshop, but I'm expanding these figures now a little bit. So the

40:05.280 --> 40:14.320
idea was that if we can emulate the choice, binary choice in this case of a particular individual,

40:14.880 --> 40:22.160
can we emulate the behavior, the collective behavior in this case, cooperation,

40:22.160 --> 40:29.120
emergent cooperation of two individuals working together in particular in the prisoner's dilemma

40:29.120 --> 40:37.680
in this example. So we created copies of the IVL model, one representing player one, the other

40:37.680 --> 40:44.480
one representing player two, and we put them to work in the prisoner's dilemma, and we did

40:44.480 --> 40:50.960
different levels of information provided to the players. So what we observe here

40:51.600 --> 40:58.080
is again the model, in this case the model is the lighter color and the human behavior,

40:58.080 --> 41:05.920
and this is the proportion of the collective cooperation of the pair. And what you see here

41:05.920 --> 41:12.960
is actually the strategies, the sequential strategies. So for example, mistrust is the

41:12.960 --> 41:20.080
number of times that a player defects after the two players have defected. Forgiveness is the

41:20.080 --> 41:28.240
proportion of cooperation after the other player defected even though I cooperated, etc. So the

41:28.240 --> 41:35.520
point of this is that the model also captures the sequential strategies over time. So it's not

41:35.520 --> 41:43.360
just capturing the outcome metric, but also the effects of the sequential strategies and also does

41:43.440 --> 41:53.120
it with different levels of information. We did have to change the blending equation to capture

41:53.120 --> 41:58.320
the descriptive information. I can tell you more about that. I explained that in our talk the other

41:58.320 --> 42:07.840
day. Okay, so the next sort of escalation of these examples is whether it can capture the effects,

42:07.840 --> 42:16.320
the collective effects of groups. So for this what we did was to create different networks

42:16.880 --> 42:24.560
with different numbers of connections, and we used two different pay-off matrices,

42:24.560 --> 42:31.920
and this came from the work of Valier and colleagues on a taxonomy of two by two games.

42:32.480 --> 42:40.560
So we chose the extremes of those two by two games, the independence and the interdependence

42:41.120 --> 42:50.160
games, and we run simulations with our IVL models to play with 16 players in this case,

42:51.040 --> 42:59.280
making collective decisions, and then we aggregated their results. And what we observe is that in

42:59.280 --> 43:07.920
the independence matrix, the collectives are able to figure out what is the best option for

43:07.920 --> 43:17.200
everybody, that is this 5.5, actually faster the more connections there are. So they are pretty

43:17.200 --> 43:24.000
good at finding the right option pretty quickly, almost immediately we can see this, but it's faster

43:24.000 --> 43:31.120
the more connections there are. However, when there is the interdependence that is when the outcome

43:31.120 --> 43:38.720
I'm going to get depends on the other person, that's not the case, that is more connections actually

43:38.720 --> 43:47.600
resulted in less ability or inability to determine what is the best action for both of us, the AA

43:48.080 --> 43:56.880
action. So this was curious for us, and you know this particular degradation of performance based

43:56.880 --> 44:05.440
on the number of connections that the agents had, and so we wanted to see whether this in fact happens

44:05.440 --> 44:14.720
with actual humans. So we run the study with teams, yes. What is the mechanism?

44:14.800 --> 44:18.880
What is the mechanism? Why is interdependence so bad on?

44:20.400 --> 44:27.840
Yeah, so the why which you know I'm not going to show you all the evidence for that, but the why has

44:27.840 --> 44:38.400
to do with our ability to know the one-to-one correspondence of the actions. So if I play

44:38.480 --> 44:47.760
with all of you, I need to keep track of what you did to me last time, and what Henrik did to me

44:47.760 --> 44:56.080
last time, and what everybody did to me. It's very hard to learn that, right? So next time when I

44:56.080 --> 45:04.880
meet you again, then I need to figure it out or remember, right? Oh, she did B, I should do A with

45:04.880 --> 45:12.800
her, right? It's very hard to keep track of that, the more connections you have. The first one is not

45:12.800 --> 45:17.680
difficult because it's very easy to find that regardless of how many connections you have,

45:17.680 --> 45:24.560
if you pick A, that's what everybody should pick. But doesn't it depend on the payoff structure

45:24.560 --> 45:32.240
specifically of this interdependence pattern? I think it does. Okay, I secure myself. You could

45:32.240 --> 45:37.600
have a slightly different payoff structure, you could have the default. I don't remember what she

45:37.600 --> 45:46.560
did, but it does in a pro-social way. Yeah, it does. It would go up. It does, and I mean, this is

45:46.560 --> 45:54.080
an example that it does, right? But in addition to that, Valier et al. have a whole taxonomy

45:54.080 --> 46:00.480
of two by two games, and we run this model in the whole taxonomy, and this was the only,

46:00.560 --> 46:06.320
there were other things that happened, but this was the only one in which we saw this effect.

46:07.120 --> 46:12.800
So this paper is still, I haven't published this paper, but it's still in process.

46:14.400 --> 46:19.920
By the way, sorry, I had to step out of it. That was my doctor call. So the mechanism you just

46:19.920 --> 46:27.760
described seems to depend sensitively on the internal structure of these computational agents

46:28.480 --> 46:32.960
on their memory capacity. I mean, if they have a representation which does

46:33.760 --> 46:42.560
simply record what every other player did, then this wouldn't happen. So there are some choices here

46:43.680 --> 46:49.280
of kind of the parameters and the structure, and maybe even your definition of similarity, right?

46:49.280 --> 46:59.200
So, I mean, similarity would only ask what's the, what happened the last time that Catrine played

46:59.200 --> 47:06.720
that way as opposed to recognizing that I can learn about my interactions from Catrine,

47:06.720 --> 47:14.640
from my interactions with Iname, Hi, Iname. So, I mean, the, so there are a lot of choices

47:14.640 --> 47:21.440
underneath here. I mean, the choices are very transparent in the equations of the model.

47:21.440 --> 47:29.760
That's the choices. In this particular task, the binary choice, you don't even have attributes.

47:29.760 --> 47:36.160
You know A in an outcome. So there is not even a similarity in this case, but the frequency

47:36.160 --> 47:42.960
and the recency play a role, and that's just organic in the model. There is no, yeah,

47:42.960 --> 47:49.200
there is a choice of the parameter of the K, but we don't in, I forgot to say that, in one of these

47:49.200 --> 47:55.600
results, we are manipulating that parameter. We are using the default parameter that is used in

47:55.600 --> 48:01.440
Act R. So it's just coming from, these predictions are coming from the theory.

48:02.400 --> 48:10.240
But in this setting, aren't there some choices about how I represent past events?

48:10.320 --> 48:17.120
Very good. Yeah, in every setting. And that is the major point. I'm going to come back to that

48:17.120 --> 48:23.520
later if you don't mind. But that is exactly the major, one of the major things that we need to

48:23.520 --> 48:30.080
improve. The main choice is the designer of the model on what to represent in the instance.

48:31.200 --> 48:37.600
We say that we don't represent anything that is not fully available to the human.

48:37.600 --> 48:43.360
And it's true. That's what we try to be very committed to do. But still there are choices

48:43.360 --> 48:49.120
on what to represent in the instance. This case is not that case because it's very simple.

48:49.120 --> 48:58.320
You just see A or B. There is no attributes and the outcome. I can come back to the issue of

48:58.320 --> 49:03.920
representation. I know there are many questions. Do you think you could wrap up in like five minutes?

49:04.160 --> 49:11.760
Okay. And then so there is still time for more discussion. Then here is where my strategy to

49:11.760 --> 49:18.960
cut a lot of stuff comes. So I'm going to show a few more things. I'm going to wrap up in five

49:18.960 --> 49:31.680
minutes then. Okay. So again, we run the exercise with human participants in groups. And the human

49:31.760 --> 49:38.640
participants in this case were groups of six because running 16 was a lot more complicated.

49:38.640 --> 49:44.320
But we managed to gather a good number of groups in each of the conditions. And this is what we

49:44.320 --> 49:52.240
observed. So the main effect that we predicted was verified with the human data.

49:53.920 --> 50:00.480
Okay. The next example is a little bit more complicated and is more realistic too. And in

50:00.480 --> 50:07.680
this example, there are many features. It's about phishing. And it's about our human decision to

50:08.720 --> 50:16.720
whether let pass phishing email or not. So we use a data set that was collected by someone else,

50:16.720 --> 50:23.680
not us. And that data set was collected in this form. So there were a bunch of emails that they

50:23.680 --> 50:29.520
created and some of them were real. So real phishing and real ham. Those are real emails.

50:29.520 --> 50:38.960
And then they simulated some phishing and ham emails. And then they asked people to rate the

50:39.760 --> 50:47.680
phishing, the emails, whether they were definitely safe to definitely suspicious. And so what we did

50:47.680 --> 50:57.920
is we plotted the human data. It's here. We emulated the same decision with our model. And it's here.

50:57.920 --> 51:04.160
And we can see that the distributions of the model are very similar to that of the human.

51:05.120 --> 51:11.040
And finally, an even more complex task because this is a very popular theme of research is

51:11.040 --> 51:18.720
cybersecurity. And in this particular case, we are emulating the decisions of a defender in an

51:18.800 --> 51:29.600
individual task. And there is a network that the defender needs to keep safe. And there are some

51:30.240 --> 51:43.360
strategies of red agents or red attackers. Two strategies are implemented. And the strategies

51:43.360 --> 51:50.160
vary in the way they explore the network to reach the objective, which is this operational server.

51:51.040 --> 52:00.080
The interface that the humans use to do this task is what you see here. And we again run

52:00.080 --> 52:09.120
and publish a paper with just the predictions of what the model does against as defending against

52:09.440 --> 52:22.400
two attacker strategies, the meander and the other more direct strategy. And this is our results

52:22.400 --> 52:30.800
from the human experiment. Again, we cannot run 2000 episodes in that case. So we run seven episodes,

52:30.800 --> 52:38.240
but we see how our human experiment emulates the predictions from that model.

52:39.520 --> 52:45.760
So, okay, given the time, I think I would like to conclude with this, which is

52:47.040 --> 52:56.560
our reflection of human likeness. So I think what I have been talking about now regarding human

52:56.560 --> 53:06.480
likeness is based on outcome metrics. So I emulate the decisions of the human, and then I compare

53:06.480 --> 53:13.600
the outcome of the model to the outcome of the human. Usually we use generic metrics like MSE.

53:14.320 --> 53:20.320
Often they are used at the aggregate level to be able to say, hey, our model is doing very well,

53:20.320 --> 53:25.920
and that's what every cognitive modeler does. But I think that's wrong. I think we really need to

53:25.920 --> 53:35.120
improve this metric, especially that we have the mechanisms to be able to compare at a much lower

53:35.120 --> 53:42.400
level. We should always at least compare at the individual level and be able to evaluate the steps,

53:42.400 --> 53:56.960
each of the steps at the individual level. Let me say just that we are using now these models

53:57.520 --> 54:06.480
more directly in some applications to be able to help prevent the biases that humans have,

54:07.040 --> 54:13.760
and to be able to help humans make better decisions. For example, connecting these models

54:13.760 --> 54:22.400
with machine learning models and optimization models, and using our cognitive models as human

54:22.480 --> 54:33.280
collaborators. This is one of the ways in which we are actually using our models in cybersecurity.

54:34.480 --> 54:43.360
By emulating the actions of an attacker or an end user in the case of phishing, we can actually

54:43.360 --> 54:51.520
provide the predictions of what these humans are going to do to heavy machine learning algorithms

54:51.520 --> 54:59.520
that are being used as defender strategy. By providing these predictions, these defender

54:59.520 --> 55:07.520
strategies are being adapted to be able to modify, in these cases, a stalker per security game,

55:07.520 --> 55:15.600
modify the allocation of defense resources. The other way in which we are using these models

55:15.600 --> 55:25.040
is by making them work with the human, along with the human. Instead of having one individual

55:25.040 --> 55:34.000
making decisions in the cybersecurity task, now we have a team. The team is an AI and a human

55:34.000 --> 55:41.920
making decisions in this type of security task. They collaborate with each other and they are

55:41.920 --> 55:52.880
able to accomplish the task better than a human alone can do. In conclusion, our current focus of

55:52.960 --> 56:03.920
AI, I thought I had it. I'm almost finished though, but I thought I had it. Maybe I never,

56:04.080 --> 56:06.960
oh, it's not connected.

56:20.080 --> 56:27.600
Our current focus on AI is on algorithms that aim at making faster and better decisions. The

56:27.600 --> 56:34.720
current focus of AI is on creating algorithms that compete with the human and that are able to

56:34.720 --> 56:42.400
make faster and better decisions than humans do. Our goal in contrast is to build learning

56:42.400 --> 56:48.480
algorithms that can emulate the cognitive processes and can inform those optimization

56:48.480 --> 56:55.280
algorithms to be able to speed up the human decision process. There is evidence of human

56:55.280 --> 57:02.960
likeness in these IPL algorithms, but we know that we need to improve the metrics of human likeness.

57:02.960 --> 57:11.760
What do we really mean by human likeness? We need to develop this evaluation of human

57:11.760 --> 57:20.160
likeness at the cognitive level steps. That is what I think is required to demonstrate some of the

57:20.160 --> 57:27.440
usefulness of these models. Yeah, thank you.

57:32.160 --> 57:34.960
Maybe some people may believe, but who can stay, please go ahead.

57:34.960 --> 57:49.120
Thank you for this talk. You mentioned the prospect theory of Kahneman and Tversky,

57:49.120 --> 57:56.000
and one of the strengths of that was not only trying to match human data on specific examples,

57:56.000 --> 58:02.800
but also being able to abstract out very general biases, human cognitive biases.

58:03.760 --> 58:14.000
Kahneman wrote a whole book about it. Can this also abstract out from your data,

58:14.000 --> 58:17.520
very general human biases in these dynamic situations?

58:17.520 --> 58:23.280
Some, but we haven't done a lot of work on that, but confirmation bias, for example,

58:23.280 --> 58:30.320
is something that we can definitely predict. The big difference is that we are interested in

58:30.320 --> 58:38.320
biases from experience. That's in very sharp contrast, because in their theory, they rely on

58:38.320 --> 58:46.720
descriptions of the options. Having the descriptions is very different than just acting and observing

58:46.720 --> 58:52.160
the outcomes. We only focus on biases from experience, and one of them is confirmation

58:52.160 --> 58:58.480
biases, and yes, our model can predict that. There is quite a lot of work to do on characterizing

58:58.480 --> 59:02.960
those biases that we can predict from experience that we haven't done.

59:02.960 --> 59:04.960
There's a question from Tenda.

59:04.960 --> 59:12.560
Can I just go up on this? One of the things that Kahneman and Tversky most rarely or

59:12.560 --> 59:16.720
ever implemented computational is basically just a verbal re-description of what's going on

59:17.440 --> 59:22.320
and something really disputed. But this basically, you are showing computational what can emerge,

59:22.320 --> 59:24.880
and maybe there is no general class even for some of them.

59:25.760 --> 59:32.080
I'm a little frustrated with the heuristic and biases situation, because it cannot

59:32.080 --> 59:37.520
grow longer because there is no more paper, I guess. I don't know. It just keeps growing and

59:37.520 --> 59:44.720
growing, and new biases are emerging. I was very much in line with the initial idea. I really

59:44.720 --> 59:50.480
like it, and I think it's still powerful. But now everybody wants to bring a bias out. It's like

59:50.800 --> 01:00:01.360
the 20 questions, right? The idea of Alain Newell is we are doing all these very tiny experiments,

01:00:01.360 --> 01:00:08.240
but we are forgetting about the global picture. Definitely, there is a lot more work to do on

01:00:08.240 --> 01:00:09.760
biases from experience.

01:00:09.760 --> 01:00:18.560
I was wondering about the previous slide of where I don't...

01:00:19.840 --> 01:00:21.120
Write the previous to this one?

01:00:24.400 --> 01:00:31.280
So I'm wondering if there's a little bit of cluster clarity, but in a way, on the one hand,

01:00:31.280 --> 01:00:35.760
we want machines to make that for humans. On the other hand, we're

01:00:36.480 --> 01:00:39.680
evaluating machines, and there's similarity with human decision-making.

01:00:44.160 --> 01:00:47.520
So it seems that there needs to be some kind of decomposition of human decision-making in

01:00:47.520 --> 01:00:52.000
terms of what do we actually want to extract from it? What comes from human limitations,

01:00:52.000 --> 01:00:56.560
just cognitive limitations, not being able to evaluate everything? How do we split that, I guess?

01:00:57.680 --> 01:01:05.520
I had a lot of slides going step by step in the process. Actually, a paper that

01:01:05.520 --> 01:01:12.000
is coming out in PPS does that, so if you are interested, I can send you that.

01:01:12.800 --> 01:01:22.560
But yeah, so basically, I would break out... I broke out each of the steps in the IVLT process

01:01:22.560 --> 01:01:28.400
to analyze what we know and what we need to know. And definitely, one of those things are

01:01:29.280 --> 01:01:36.960
evidence for particular similarity metrics. So in our model, for example, we often use just

01:01:36.960 --> 01:01:44.080
linear similarity, and sometimes if it doesn't work well, we change it. There is not a lot of

01:01:44.080 --> 01:01:50.320
theory, even though there is quite a lot of work on similarity judgments, but how to translate

01:01:50.320 --> 01:01:56.720
that empirical warning to a computational mathematical form and then be able to test it

01:01:56.720 --> 01:02:02.240
within the realm of the model, we haven't done that work, and that's one of the areas we need to

01:02:02.240 --> 01:02:11.600
work on. Koti, there is a question for Daniel then. Yes. Thank you for a very interesting paper.

01:02:11.600 --> 01:02:22.720
There are two points that I wasn't sure I understood. First of all, I want to applaud the idea

01:02:23.520 --> 01:02:31.120
that you want to enhance human decision-making and make it rather than make it

01:02:32.160 --> 01:02:41.840
slavishly dependent on AI decision-making. This is a theme that I've been discussing for years.

01:02:44.000 --> 01:02:50.080
I call it the difference between the Nautilus machine and the bulldozer.

01:02:51.040 --> 01:02:58.720
The bulldozer lets you move mountains, but you're still a 98-pound weakling. The Nautilus machine,

01:03:00.400 --> 01:03:06.480
you actually develop the strength yourself. The technology is being used to improve your

01:03:06.480 --> 01:03:16.080
abilities and make you less dependent on the technology. So I picked that point up. I want

01:03:16.080 --> 01:03:24.320
to see if you stress it the same way I do. But the question that I really want to ask you about is

01:03:30.720 --> 01:03:39.760
consider the notorious, rueful, reflective remark of somebody who says, well, it seemed like a good

01:03:39.760 --> 01:03:47.920
idea at the time. And this requires memory of your past decision-making in detail

01:03:49.360 --> 01:03:57.040
and memory of your evaluation and the grounds for that evaluation. Now that seems to me to

01:03:57.040 --> 01:04:04.240
be a very important part of human decision-making, that is the capacity to be self-critical and

01:04:04.240 --> 01:04:14.000
reflective and to learn from your mistakes by being able to debug your past performances.

01:04:14.640 --> 01:04:25.360
I didn't think that I saw any sign of that reflective capacity in your presentation,

01:04:25.360 --> 01:04:32.720
but maybe I just missed it the way you were saying it. Yeah, let me address the second point before I

01:04:32.720 --> 01:04:43.440
forget. My memory has a large decay. So yes, you are correct that I didn't address that point

01:04:43.440 --> 01:04:50.480
very concretely in my presentation. But that idea is essentially

01:04:50.640 --> 01:05:03.120
expressive. You can see this slide in the feedback mechanism. And so what happens is that

01:05:04.800 --> 01:05:10.960
usually the feedback is delayed and there is nothing we can do about it. And furthermore,

01:05:10.960 --> 01:05:18.560
we can make many decisions and then get one outcome. And we don't know which of the decisions we

01:05:18.560 --> 01:05:26.640
made is responsible for that outcome. So we use a concept that is well-known in AI of credit

01:05:26.640 --> 01:05:35.840
assignment. So how do we associate that outcome to the various decisions that were made? This is

01:05:35.840 --> 01:05:41.280
the other major area of research. We have a paper that we've been having trouble to

01:05:41.840 --> 01:05:49.040
publish, but it's made available online where we test various mechanisms of credit assignment,

01:05:49.040 --> 01:05:59.120
including the TD mechanism in reinforcement learning. So this is a very important part of

01:05:59.120 --> 01:06:06.640
learning because it's the only way that you can modify the expected utility with the actual utility

01:06:06.640 --> 01:06:14.160
of the decisions you are making and how you do that process will determine a lot how the learning

01:06:14.160 --> 01:06:23.280
is done. So we usually just do equal credit until this paper that we are still exploring and

01:06:23.280 --> 01:06:29.520
figuring out different credit assignments and trying to figure out how humans do that credit

01:06:29.520 --> 01:06:36.320
assignment. And so there's quite a lot of research that is needed in that area. And to go back to

01:06:36.320 --> 01:06:45.040
your first point of the bulldozer and dependency on technology, I think we are going to be dependent

01:06:45.040 --> 01:06:52.080
on technology no matter what. And I think that's a good thing. I don't have anything against

01:06:52.800 --> 01:07:03.280
technology, but the essence of human life is the human and the essence of any decision making

01:07:03.280 --> 01:07:10.640
in any complex environments I think will continue to be the human. So I think we should use the

01:07:10.640 --> 01:07:19.040
technology to empower the human to be able to help the human improve their decisions. And learning

01:07:19.040 --> 01:07:26.240
mechanisms is essential for that, but figuring out how. So it's not like I give you what you want now.

01:07:26.640 --> 01:07:34.400
It's like a spoon feeding you on what you need right now. It needs to be a lot more global than

01:07:34.400 --> 01:07:53.680
that. Thank you. Let me just comment. I think if you try, I'm trying to imagine your systems

01:07:54.560 --> 01:08:05.200
acting as sort of tutors for human beings and tutors that have good models of how the human

01:08:05.200 --> 01:08:13.440
beings are making decisions because they are also models about how the tutors are making decisions.

01:08:14.320 --> 01:08:25.840
And so that in effect the tutor can say, yeah, I'm making this up now. Yeah, I was tempted by that

01:08:27.120 --> 01:08:34.640
way of thinking about this problem. You are absolutely correct. One of the most successful

01:08:34.720 --> 01:08:42.160
programs from ACTAAR has been on cognitive tutors. So being able to use ACTAAR to

01:08:43.280 --> 01:08:50.080
help children learn mathematics. So you are absolutely right. We are using tutors of a

01:08:50.080 --> 01:08:56.160
different kind, we can say. It's not about mathematics. It's about making choices in some

01:08:56.160 --> 01:09:03.600
complex environments. But that is exactly how we are using it. We can trace, as I was saying

01:09:03.600 --> 01:09:10.880
before, we can trace specific students, decision makers and their history and therefore we can

01:09:10.880 --> 01:09:17.520
make predictions about the decisions that particular person is going to make in the next

01:09:18.160 --> 01:09:26.400
opportunity. And then we can use other tools, machine learning included or any other AI tools

01:09:26.400 --> 01:09:29.040
to support that decision making process.

01:09:32.800 --> 01:09:38.560
There are maybe a couple more questions from people who want to. You choose.

01:09:38.560 --> 01:09:45.520
We've talked about crisis and errors and we can only say if you see that as something

01:09:45.520 --> 01:09:54.400
like ACTAAR, in a way they have to agree with the evidence. They have an adaptation to the world

01:09:54.400 --> 01:10:02.480
in which we live or have lived. So in your conception of a dynamic environment, is it also

01:10:02.480 --> 01:10:14.800
part of the idea to change the environment to have what we call biases not occur as biases

01:10:14.800 --> 01:10:21.520
in shortcomings? We will have interventions on those biases. Yes. So a particular program

01:10:21.520 --> 01:10:30.480
which hasn't started yet is from IRPA. So I am about to start a very big program in IRPA on

01:10:30.480 --> 01:10:38.400
cybersecurity and it's about predicting the biases of the attackers. The idea is, of course,

01:10:38.400 --> 01:10:47.120
that we can modify the situation from the defense side so that we can trap the attacker. And given

01:10:47.120 --> 01:10:54.880
that the attackers are as humans as the defenders are, we can trace down and predict when they are

01:10:54.880 --> 01:11:01.440
falling trap of certain biases and then be able to react according to that. So yeah, I think

01:11:01.440 --> 01:11:07.200
biases are emergent. Again, there are some biases that are not emergent related to Melanie's

01:11:08.480 --> 01:11:14.960
question. There are some biases that are based on information, the explicit information, but the

01:11:14.960 --> 01:11:21.680
ones that we are able to handle are the ones that are emergent from experience. And yeah,

01:11:21.680 --> 01:11:27.840
we are working on that too. Yeah, it's a question about decision making and science.

01:11:29.200 --> 01:11:35.680
So you are trying to emulate the way people actually make decisions, but we lack a lot of

01:11:35.680 --> 01:11:40.400
detailed knowledge. We wish we had about the neurophysiological mechanisms that realize the

01:11:40.400 --> 01:11:46.960
procedures that make the decisions. And so you are necessarily sort of, you have to go beyond

01:11:47.840 --> 01:11:50.880
that and you are making decisions that are under-determined by evidence.

01:11:51.920 --> 01:11:58.560
And I wonder what kind of commitments or maybe even heuristics you use when it's, you have to

01:11:58.560 --> 01:12:04.880
make these decisions about what kind of a learner or what kind of a decision maker a human is.

01:12:06.160 --> 01:12:11.600
Does that question make sense? Let me try to interpret it. I am going to answer based on what

01:12:11.600 --> 01:12:21.920
I understood. Our type of modeling is symbolic. And so what that means is I don't really go into

01:12:21.920 --> 01:12:32.160
the neuro part of it. ATAR, that's right. In my personal opinion, that is why it became very complex

01:12:32.880 --> 01:12:41.920
because now ATAR is trying to map particular mechanisms to activation in certain part of the

01:12:41.920 --> 01:12:48.880
brains. While that can be very important and necessary, that is not what I do. So I stay at

01:12:48.880 --> 01:12:55.440
this symbolic level. So that is one thing, but there was another part in your question.

01:12:56.400 --> 01:13:01.760
I know Kelly, you will meet with Koti now. Maybe these two guys could go.

01:13:02.800 --> 01:13:04.960
Because maybe you would not be able to talk for a while.

01:13:06.880 --> 01:13:14.480
Unfortunately, my afternoon is full. So here's my question. So one way to learn to imitate

01:13:14.480 --> 01:13:19.360
human decision makers would be machine learning. And I know some people are doing that too.

01:13:20.080 --> 01:13:25.200
And just treat the human's behavior as a thing to be predicted.

01:13:27.600 --> 01:13:33.520
And I generally much prefer mechanistic models to machine learning. On the other hand, when you

01:13:33.520 --> 01:13:40.800
have a mechanistic model, then you are in a position of having to either defend the specific

01:13:40.880 --> 01:13:51.760
mechanism or argue that you're only trying to understand the typical behavior of a wide

01:13:51.760 --> 01:13:57.520
variety of mechanisms. So when I look at the model, the mathematical model you showed,

01:13:58.800 --> 01:14:03.600
and maybe there's something I'm missing, I'm not an expert in this area, it looked like a kind of

01:14:04.560 --> 01:14:11.280
standard online learning model, but with the delay from time. And I know that scientists

01:14:11.280 --> 01:14:17.360
have tried to figure out, do people forget things exponentially fast, or is it a power law, etc.

01:14:18.880 --> 01:14:24.880
But then at least some of the examples you showed, like shifting this one where the

01:14:24.880 --> 01:14:33.200
expected outcomes of the two alternatives were changing. Honestly, I felt like a very

01:14:33.200 --> 01:14:41.120
wide variety of models that would update their behavior and forget old data to some extent

01:14:41.120 --> 01:14:49.840
would have produced very similar things. So that made me not so convinced that the specific

01:14:49.840 --> 01:14:57.360
mechanistic model you wrote down is really the right one. Although certainly some aspects of it,

01:14:57.360 --> 01:15:01.840
like forgetting the past so you can learn about new things, is surely part of it.

01:15:01.920 --> 01:15:08.960
But I guess that on the other hand, when I think about humans in dynamic situations,

01:15:10.960 --> 01:15:18.560
I think that their notion of similarity can be much more general than, for instance,

01:15:18.560 --> 01:15:24.800
looking at kind of individual attributes of the instance. People do all kinds of analogy making

01:15:24.800 --> 01:15:30.400
and, you know, like, oh, that bit of grass which is still smoldering, which might flare up again.

01:15:30.960 --> 01:15:35.200
Oh, that's kind of like there might be a tiger there, or it's kind of like

01:15:36.080 --> 01:15:43.920
maybe there's an ambush being played by my enemies or whatever. So I don't know, the mechanism seems

01:15:45.680 --> 01:15:55.040
both impoverished in a way, and maybe over committed to specificity in a way. So at the same

01:15:55.840 --> 01:15:59.120
time, I'm glad that you're doing something mechanistic instead of just saying,

01:15:59.840 --> 01:16:03.680
I'll have a year old network, watch for human, and then predict what the human would do,

01:16:04.400 --> 01:16:10.480
which is the kind of thing which is far too popular nowadays. So I'm such a long question,

01:16:10.480 --> 01:16:16.960
and maybe it's a long answer. No, I love these type of questions. Let me just answer with what

01:16:16.960 --> 01:16:23.760
other two people have said. The first one is all models are wrong, right? And I totally

01:16:25.920 --> 01:16:34.240
no accept a priority that this model cannot be perfect in any way, right? So all models are

01:16:34.240 --> 01:16:40.320
wrong, but some models are useful. So I like to concentrate on the usefulness of these models,

01:16:40.320 --> 01:16:49.360
what it can teach us, what it can do, right? And then the other quote is about using models as

01:16:49.360 --> 01:16:57.280
your toothbrush. And yes, we have the tendency to use our models only and forget about other people's

01:16:57.280 --> 01:17:08.800
models. We have done comparison, model comparison, and there are including a site review paper of

01:17:08.800 --> 01:17:14.960
the comparison of our model with another 16 models. I mean, the proliferation of models

01:17:14.960 --> 01:17:21.520
for this kind of thing is huge, right? So many. But there was a competition, modeling competition,

01:17:22.240 --> 01:17:28.720
and we tested our model against the winners of that competition and many other of the more

01:17:28.720 --> 01:17:35.920
typical models. And our model comes to be more general, and it comes to be more predictive than

01:17:36.000 --> 01:17:45.520
most of them. So again, am I using my own toothbrush? Maybe. I like to use my own toothbrush.

01:17:46.160 --> 01:17:53.440
The other part is regarding the machine learning. Yeah, I think human likeness depends on what you

01:17:53.440 --> 01:18:01.600
want to do, right? So if you just want to predict a particular outcome, of course, having a large

01:18:01.600 --> 01:18:09.680
amount of data can help. As I said, everything I presented is predictions from theory. I didn't

01:18:09.680 --> 01:18:16.400
use any data before I made the predictions. No human machine learning algorithm would be able

01:18:16.400 --> 01:18:21.920
to do that. They need the data. In fact, the less data they have, the worse they are, right?

01:18:22.480 --> 01:18:25.680
Okay. Thank you. Thanks for your time.

01:18:29.600 --> 01:18:33.440
I want to ask, in a different way, the same question that was already asked three times

01:18:33.440 --> 01:18:38.000
at the beginning, which is the question of ethics. Because, well, two things. It seems that this is

01:18:38.000 --> 01:18:43.920
a visibly important question in decision-making and situations of warfare or juridical situations.

01:18:44.640 --> 01:18:51.760
And secondly, it belongs to philosophy, right? To the Western tradition that runs from Aristotle,

01:18:51.760 --> 01:18:58.000
through Emmanuel Levinas, and so on. And the one consensus that the big players within this

01:18:58.000 --> 01:19:04.080
tradition all share, it seems to me, is that from the Greek, you know, Pyrenees and their idea of

01:19:04.080 --> 01:19:11.040
Epoche is as vertiginous suspension of all decidability all the way through to Walter Benjamin's

01:19:11.040 --> 01:19:20.800
idea of mystical origins of justice. The consensus seems to be that the ethical or just decision

01:19:20.800 --> 01:19:29.360
can only come from a place of radical undecidability, right? From non-calculability, right? Law is

01:19:29.360 --> 01:19:36.240
about calculus, but justice comes from the incalculable. So, in that sense, the distinction

01:19:36.240 --> 01:19:41.440
to be made would not be between humans and machines. It would be between calculability

01:19:41.440 --> 01:19:48.080
and non-calculability. If you can calculate ethics or justice, it's not ethics or justice

01:19:48.080 --> 01:19:54.800
anymore. It's just control, right? I just wanted, that would be the provocation I would put to you

01:19:55.440 --> 01:20:00.400
in the name of philosophy, right? So, it's a different language game that we're working

01:20:00.400 --> 01:20:05.040
at using. As soon as you deal with ethics, necessarily you're taking that on board, right?

01:20:05.040 --> 01:20:11.840
I think that is a world or a controlled world. I guess that is a philosophical question

01:20:11.920 --> 01:20:19.200
for our model to be able to account for ethical issues. There needs to be

01:20:19.200 --> 01:20:32.160
calculability. And so, you know, philosophically, that moves us out of the ethics realm. I would

01:20:32.160 --> 01:20:36.560
accept it because there is nothing else we can do without calculability.

01:20:36.560 --> 01:20:39.280
Thank you very much.

