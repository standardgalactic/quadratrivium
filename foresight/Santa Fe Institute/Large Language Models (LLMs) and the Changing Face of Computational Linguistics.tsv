start	end	text
0	8160	Perhaps Catherine should just introduce ourselves. So we were at Brown University for several decades
8160	13840	in the Department of Cognitive and Holistic Science there. I was also joined in Computer Science.
14880	22080	Moved to Australia about 14 years ago to Macquarie University and at the same time I think we started
22080	31600	becoming external faculty here at SFI. Catherine got a gigantic mega grant that required hiring a
31600	38400	lot of people doing a lot of work there. I started going into the startup route
39840	45360	and then what with that and then with COVID I think we let the external faculty connection lapse
46160	52160	but now we'd like to restart the connection with SFI again.
54560	58240	The startup stuff that I was doing was actually in the chatbot space.
60240	65600	We got acquired by Oracle Corporation. That's the reason for this disclaimer down here at
65600	72000	the bottom right. So nothing that I'm saying represents anything of any of our employers.
72720	77760	The other reason for this comment here is because the whole field of large language
77760	85200	bottles is really changing incredibly quickly. I don't know how much of what I'm going to say
85200	92080	right now is going to be true or relevant in six months time. So nothing, I'm not saying anything
92080	98640	that I believe is false but it could well, some of the effort could well be wasted.
98960	108640	So I'm going to try and give you guys the high level executive overview talk. This talk was
108640	114480	really actually aimed originally at academic researchers in natural language processing
114480	121120	and so I'm going to skip some of those some of those details but you know you had to have been
121120	126800	underneath a rock if you didn't notice the large language models who radically changed the field
126800	136320	in the last year or two and so I'll be talking about that. We'll also be talking about suggestions.
136320	142640	You know how should academics actually respond to this changing world? So we'll be talking a little
142640	148960	bit about what these large language models may mean or may not mean for human language acquisition,
149040	156480	the study of that. There's a lot of interest in neuro-symbolic models that integrate large
156480	163200	language models and more traditional kinds of AI models and I'll also talk a little bit about
163200	172800	alignment of LLMs and I'll end talking a bit about the connections between large language models
172800	179680	and the social implications of them and lessons from the first industrial revolution.
181040	186240	Katherine and I just spent two months at the University of Edinburgh and it was very interesting
186240	191360	there because of course the first industrial revolution really happened in Northern England
191360	200320	and around Scotland and actually it's a fascinating thing to read about that you know James Watt,
200320	205280	you know the inventor of the steam engine actually did a lot of his work at the University of Glasgow
206320	210880	so that's sort of really interesting. Okay so how deep learning changes NLP?
213840	218720	You know people like Katherine and I have been in the field long enough to have actually seen
218720	224800	really a switch from symbolic parameters, statistical NLP and then finally now to deep
224800	232480	learning and large language models and I guess I would say that the really huge difference is that
232480	240800	large language models can understand context much much longer contexts than anything that I thought
240800	247520	was even possible. In fact I actually thought there were good theoretical arguments that it should
247520	253760	not be possible to try and model a context as long as a sentence or a dialogue or an entire
253760	260960	paragraph yet large language models can do that perfectly well and if you want to I can go through
260960	270080	what those counting arguments would be but just you know when I actually look back at the
270080	276640	pre-deep learning work you know I did a lot of work on syntactic parsing which would involve
276640	282720	recovering pastries that looked something like this and I think the motivation for dealing with
283440	292560	you know wanting structures like these these things make local a lot of dependencies which
292560	300000	are non-local in the string here right so you know it's a flight through Denver right so
300800	307200	this relationship here is making that local the same thing also we're talking about a preference
307200	313680	for a flight for example that's making that dependency local why do you want to do that you
313680	321040	want to do that if you think that you can only really model pairs pairwise interactions large
321040	327840	language models as I said are happy modeling contexts of you know in the hundreds or the
327840	334880	thousands of tokens and so I actually think you know these theories are correct but they're just
334880	340160	no longer required so to do natural language understanding I don't actually have to recover
340160	346800	this sort of structure anymore okay so strengths of large language models we just talked about
346800	353680	how there's so much better at working with incredibly long contexts I think the other you
353680	360320	know I'm giving you the executive summary here the other really amazing thing is that you know
360320	365440	if you showed these large language models to people from the first neural network revolution
365440	372640	from the 1980s actually everything would be really quite familiar to them right there's
372640	379600	there's a slight twist with the attention mechanism but you could explain that in about 15 minutes
379600	386000	and those guys understand it all you know and Sutton has this thing which he calls the bitter
386080	392720	lesson which is that you know for people like me who work on lots of really interesting algorithms
392720	398640	and models and structures and you know things like that actually really none of it really
398640	406960	seemed to be relevant if you just got data and compute and scaled everything up that that really
406960	412560	seems to be the secret and I actually do think that these large language models are largely just
412640	418640	sort of brute forcing linguistic generalization so they're covering rather than really capturing them
419200	424080	but it's really interesting how well they work right that in fact actually with enough compute
424080	431440	and enough data you do seem to be able to produce something that's really quite amazing okay weaknesses
431440	437200	of large language models I mean I think this is probably everybody here's already heard about
437200	443520	this stuff you know Emily Bender's stochastic parrots you know or as I guess Gertrude Stein would
443520	450000	say there's no there there and I think actually these large language models just you know they
450000	457040	respond reflexively in fact I actually think if we're talking about human models these large
457040	463280	language models maybe actually aren't such so bad for the purely reflexive processing that maybe
463280	470080	humans might be able to do but scaled up you know whereas humans maybe have a context window
470080	474960	of five or something like that these large language models have a context window of five
474960	482000	thousand for example and I think actually that's also because they have no beliefs desires or
482000	491440	intentions they have no you know guiding you know thoughts that's why hallucinations such a big
491440	499120	problem you know they're just trying to produce plausible output I think it's really interesting
499120	505040	to actually wonder about you know these large language models now becoming multimodal how will
505040	510880	that change things I actually think if you think the fundamental problem is a problem of symbol
510880	519440	grounding then arguments like Searle's Chinese room argument still are just as valid against
519440	525200	multimodal input on the other hand I actually think the multimodal inputs incredibly interesting
525200	531040	might be incredibly powerful yes so naive questions what is multimodal training oh so here
531040	536720	what I really think is vision and language I should have made that clear vision and language
538160	545360	but these days in fact people are now starting to train off movies as well right so that's I mean
545360	551760	arguably that is one big difference between these large language models and
554800	559600	you know the way humans might work right and what is Searle's Chinese room
561200	568480	that's a really interesting thought experiment which I think for reasons of time I might just
568560	576880	skip here but it's a it's basically where he asks he asks us to imagine a blind symbol
576880	582720	following system and he says so can you really do you really want to say that this blind symbol
582720	588880	for you know rule following system actually has any understanding it doesn't really seem to make
588880	598080	sense to attribute any any understanding to it anyway I do actually think you know there's a lot
598080	603440	of people now that are also wondering are these large language models are they really intelligent
604160	611520	could they be a AGI artificial general intelligence that maybe might take over the entire world
612240	619040	I think they currently can't because they do lack you know beliefs desires intentions the ability to
619040	625440	form long-term memories for example but I'm I'm actually not so sure that that's such a huge
625520	633200	technological barrier I actually do think that it's possible that that could be relatively easy
633200	639520	now you know we've been in this situation before where we thought oh the thing that's really missing
639520	645520	from this machine being intelligent is x you add x to the machine and all of a sudden you discover
645520	651040	well actually you know there's this other thing why that's missing as well and so that may happen
651040	658400	here too but there is a chance that in fact if we just added episodic memory to these large language
658400	664880	models then they may actually become intelligent things the other okay another high level point
664880	671680	that I want to make is that these large language models are manufactured systems in the same way
671680	676560	remember I was talking about the first industrial revolution in the same way that a steam engine
676640	682400	is a very highly engineered manufactured system you know you wouldn't want to be trying to
683040	689040	infer the laws of thermodynamics or even the ideal gas laws by examining a steam engine
689760	696080	I think the same thing's true also about these large language models you know they're they're
696080	703520	trained in a way which is really an engineered product you know and certainly as somebody being
703520	710160	in industry I can tell you that you know the people in industry that are building these things
710160	714560	really don't care about doing science they're trying to build a product that they can sell
714560	726240	and that's really what they care about okay so just an aside about you know the relationship
726320	736080	between I I actually think a natural language understanding and say you know the science of
736080	743360	of linguistics or language acquisition so there's the scientific side and the technological side
743360	749280	I think the technology is currently outstripping the science and I think that has happened in
749280	753440	times before in fact I think it actually happened with the first industrial revolution
753440	760720	so I won't spend too much time here but my understanding is and maybe there's people
761280	768080	historians of science in this room that know more about this than I do but that you know
768080	774000	really in fact actually people started to invent steam engines long before they had the scientific
774000	780720	understanding first of all of ideal gas laws and then ultimately right through thermodynamics
780720	787360	and statistical mechanics that took centuries to the scientific for the scientific side to
788080	794480	emerge and in fact actually ideas of things like entropy really actually had to be developed
795280	801680	to answer questions about why it was not possible to build steam engines above a certain level of
801680	808960	efficiency for example and I suspect the same thing may be true today that our science of
809040	820160	say language and psychology is actually behind the technology. Okay one of the things that I
820160	825520	actually quite like is you know this comment here natural language is the new programming language
827760	835280	and that yeah I mean certainly for those of us in industry LLMs are really changing the way in
835280	844560	which we do our work right whereas it used to take a team of real experts to build say for
844560	849680	example a device which would identify all the financial products that are mentioned
850800	856480	you know in a particular document. Now you can just simply ask a large language model to do that
856480	864080	for you and it does a pretty good job maybe not as good as the very best hand-built natural language
864080	870720	processing system so those are still actually better but you know they take months or years to
870720	879440	develop whereas it takes you know maybe hours to use a new large language model so I actually think
879440	884640	that particularly in terms of the commercial implications the commercial deployment of natural
884640	892080	language processing in industry that's going to change completely. It's not clear we'll need nearly
892080	899360	as many experts in natural language understanding for example for the industrial applications.
901760	908000	I did want to mention a little bit you know I think that one of the really interesting
909520	913760	things that's happening in the field is taking these large language models
913760	921760	and then combining them with other components. The first component that people started to look at
921760	928160	was combining large language models with what's called a vector store or a retrieval system
928880	936880	and that's just simply something whereby when you ask a question instead of just directly asking
936880	944240	the large language model to respond to that question you retrieve a set of relevant documents
944240	949520	to that question feed those in as part of the input to the large language model you can see
949520	954400	that's what I'm suggesting that you do over here and then you then tell the large language model
954400	962320	to use to produce an answer that just simply references those documents and that's sometimes
962320	972880	called the reader retrieval model or retrieval augmented generation. That idea can get even
972880	978160	more power when you start to think well maybe in fact the large language model can actually decide
979120	984560	what information to do a search for and then when you then started to think well should it
984560	989440	decide what information to do a search for maybe in fact it could also call other tools
990080	995600	so these large language models are infamous for not being able to do numerical calculations very
995600	1000960	well but maybe in fact what we should be doing is giving the large language model the ability
1000960	1007040	to call a calculator and there's just in the same ways which if I was to ask any of you guys to do
1007120	1012880	a complex task and involve something some numeric calculation I'd want you to be
1013920	1020400	also using a calculator rather than trying to do it longhand. Okay so in terms of research
1020400	1028080	directions inside of an LLM world so the very first comment to make is that it is very challenging
1028080	1034240	for academics to do research in large language models you know the ideal thing would be to have
1034240	1042400	something like an ideal gas experiment set up but and you certainly can build small
1043200	1049840	versions of these large language models there's some disagreement about whether or not though
1049840	1056880	whether there's well there certainly seem to be emergent capabilities so the bigger the model
1056880	1062240	the more things that it can do there's big arguments about whether or not this emergence
1062320	1072560	is like a phase change or whether it's really more incremental and I again I'd be happy to talk
1072560	1079440	about that later we could spend hours talking a bit about that but I think there's enough
1080320	1086160	lack of clarity about what emergence is but if you wanted to do academic research you really
1086160	1092080	do want to get access to you know the large language the larger large language models
1092800	1097680	and the problem is that these you know the best large language models are really complicated
1097680	1106000	commercial products as we were talking about before and it's what's actually even worse
1106000	1112800	is that these days for in you know proprietary commercial reasons the companies aren't even
1112800	1118160	actually telling us all the details of exactly what they're doing so that actually does make
1118160	1125200	it very hard for academics to really do any sort of academic research in in our own papers
1125200	1131680	you know I'm collaborating with people at the University of Edinburgh what we wind up doing
1131680	1141360	is saying we're actually not going to test the closed commercial systems but we will work with
1141360	1147760	the largest open source systems that are available I think that's not a bad thing to do but it does
1147760	1153840	mean that you're cutting yourself off from a lot of the really cool systems there yes
1154880	1161120	this is a very interesting question on that slide about how quickly they degrade as you move just
1161120	1167120	as we moved from commercial ones to smaller and smaller ones yes just a steady degradation
1167200	1174000	or is there a sudden drop so sorry and in fact actually this also gets back to the emergence
1174000	1180400	question so let me just say I actually do think that a lot depends on exactly how you measure it
1181040	1189600	so I don't have to tell people particularly at the Santa Fe Institute right that quite often
1189600	1195120	what you'll actually see is a lot of small changes all of a sudden reaching a tipping point
1195120	1201280	that is basically like a phase change and you know when you think about these large language models
1201280	1208400	I mean they generate output token by token if the correct answer is just slightly less probable
1208400	1214560	than some mistake right well then as the output gets very very long then the correct answer can be
1214560	1221920	incredibly improbable right so if you're just looking at the output of 100 tokens or more
1222880	1226480	you're just looking at the output you're just asking is the output right or wrong you'll go
1226480	1233360	wrong wrong wrong wrong wrong right and then all of a sudden as the correct token probability
1233360	1240560	just nudges above the incorrect tokens you know all of a sudden the output flips and all of a
1240560	1247200	sudden it's just magically all correct but if you measure the per token probabilities for example
1247280	1250160	then you'd actually discover a much more continuous change
1253200	1257760	you know so I think that's actually where a lot of emergence happens and in fact that's I think
1258480	1263760	the white there's a little academic dispute about whether or not these models have emergent
1263760	1268400	behavior or not and that's at least my understanding of how you'd resolve that
1270560	1275360	so I actually do think that a lot of the ideas that people here would have would actually be
1275360	1283920	very useful for the community to have as well okay all right you know I actually think there's
1283920	1289600	lots of really interesting questions also you know can we understand what these large language
1289600	1296160	models are really doing I think it's you know I mean we know something about how language is
1296160	1304080	processed in the human brain you know we know that none of these models really are realistic
1306480	1311680	just even understanding you know what these large language models are doing how can you
1312320	1318160	be sure that they know a syntactic rule or make it even simpler that they know a particular word
1318160	1326400	so right now and in fact actually I think another really interesting question is
1327600	1334240	if if these large language models are basically just you know gigantic neural nets as I said before
1334240	1340800	of a relatively generic type why is it that only human beings can acquire language are they are
1340800	1346000	you know why is it humans are the only animals that can acquire language right I mean you know
1346000	1350960	we don't have the biggest brains there are animals with bigger brains if it's just merely
1352400	1356320	the size you know the number of neurons that we have sitting inside of our skulls
1356880	1363120	if that's all that determines our ability to do something like learn language why why don't
1363120	1370160	other animals why don't they have that ability it's very popular now to talk about analyzing
1370160	1376240	large language models using psychological or psychological linguistic methods I think that's
1376240	1382640	about the best that I know how to do but I think a lot of these methods were really designed to
1382640	1389840	work on humans at least agents that have beliefs and again you know in the sense a large language
1389840	1399760	model doesn't have a belief it's just got reflexes okay so you know just to emphasize
1399760	1406960	the differences between large language models and and humans right so you know children start
1406960	1415440	and end learning from much smaller data sets they generalize in particular ways that we actually
1415440	1422400	understand to new unseen forms I think we don't really know actually how these large language
1422400	1428400	models generalize I mean they do I think they do generalize but it's very difficult to tell exactly
1428400	1434880	how they generalize children also actually over generalizing characteristically so Catherine's
1434880	1440400	an expert in this area but you know these are just a couple of examples that she pointed out you know
1440400	1447920	where children have taken irregular verbs and either inflicted them in a regular fashion or
1447920	1457360	over generalize the irregular form she's giggling me you know that makes sense if you think of
1457360	1466400	giggling as being a verb a bit like tickling for example language learning you know by the time
1466400	1472640	you're three or four you're a competent speaker of your native language usually but then there's
1472640	1480560	also some part of language learning that's not really complete until the early teens right and
1480560	1486400	then you know just in terms of the pragmatics of doing research on large language models right
1487040	1493280	the time scale of research projects are different so it might take a couple of years for a student
1493280	1500960	to do a research project studying say human language learning if they're studying something
1500960	1509920	which was inspired by GPT-4 well in two years time we're probably in GPT-6 you know and the
1510400	1513920	the the inspiration might be actually sort of completely different
1516960	1523600	I also this is essentially that same thing as as I was saying before right so evaluation and
1523600	1529760	testing I think is really a huge challenge that was always difficult inside of natural
1529760	1535920	language processing but it gets even worse because the inputs to large language models
1535920	1541360	now instead of just again because this context is so much longer the input is not just a
1541360	1546880	single sentence it's an entire conversation or entire story or something else like that
1547520	1555920	so if you want to really evaluate the performance of one of these systems you want to vary not just
1555920	1562800	you know the last sentence you want to vary the entire context as well from a commercial point of
1562800	1570080	view I think actually testing is really super important I mean you know you've probably all saw
1570800	1581120	the Microsoft Bing chatbot which when it was you know the New York Times reporter studied
1581840	1588080	that chatbot and it announced to the reporter that in fact actually that it preferred to be called
1588080	1595600	Sydney rather than Bing and then also suggested that really you know the reporter should divorce
1595600	1603920	his wife I'm sure behaviors that Microsoft was really not too proud of right and I actually
1603920	1610720	think for commercial purposes it's super important to be able to detect and you know
1611600	1617360	guaranteed that such behaviors really aren't lurking beneath the surface of your large language
1617360	1627200	model. Constraint decoding that's just an NLP topic I won't spend too much time on but I actually
1627200	1631680	do think that there's really interesting work there to look at different ways of actually
1631680	1637600	constraining the output of a large language model and some real challenges there I think there's
1637600	1644160	really interesting work about how one actually trains these models as well so I mentioned that the
1644160	1649360	training procedure is itself actually a very complicated one typically what happens is that
1649360	1654320	they start up by training with what's called the language model training objective which is where
1654320	1660960	effectively you're just simply training the model to predict the very next word but a model which
1660960	1667040	is just simply trained with this large language model training objective on its own doesn't really
1667040	1672320	engage in useful conversation doesn't really follow instructions very well so it's actually
1672320	1681200	very typical to follow that up with an additional training step that is you know that involves well
1681200	1686320	as I said there's reinforcement learning with human feedback and I think that's a really interesting
1686320	1693040	question I've actually got some theories myself about you know when you want to use one sort of
1693040	1698400	training objective versus another and if there's people that'll like to talk about that more generally
1698400	1704000	I think there's a really interesting question which is how do you align the LLM behavior
1705360	1711120	with well how do you get the LLM to behave the way that you want it to right so you've got these
1711120	1716240	very general alignment goals like you know follow commands that run right through to don't destroy
1716240	1723040	humanity so ultimately it's the training data and the training training procedure which is going to
1723040	1731840	determine the LLM behavior so how exactly do we do that right and I actually do think that there's
1731840	1736960	good academic research that can be done there largely because the fine-tuning step that I mentioned
1736960	1743600	this sort of multi-stage training that's pretty modest right tens of thousands of examples or less
1745600	1752560	and it can be done on sort of fairly modest hardware so I actually think that's a that's
1752560	1759600	that's a great academic research topic I feel a little bit guilty here because I just mentioned
1759600	1766800	to you that maybe the only thing which is standing between us and you know a artificial general
1766800	1771920	intelligence is the ability to have episodic memory and then now I'm going to suggest to you
1772560	1778880	how we might actually do that and it's you know the most obvious way to do that is to actually
1778880	1784400	take that retrieval augmented generator that I mentioned before and basically let the large
1784400	1792880	language model write its own memories and this is basically a suggestion about how you might do that
1794560	1804800	more generally I actually think that you know people like me have spent decades trying to come up
1804800	1810800	with specialized knowledge representation systems and specialized inference systems
1813440	1821600	you know so and this essentially is like a specialized logic you know so knowledge graphs
1821600	1828960	and one example of that where you try to encode information in entity relation triples for example
1829680	1836480	but I actually think with LLMs you know one real possibility is that you actually let the
1838080	1841600	you let the primitive statements actually be natural language statements
1843520	1850480	so you just have represented inside of your system stored inside of a vector store for example
1850480	1856240	something like insomnia is a typical symptom of diabetes and then you'd actually let the
1856240	1861440	large language model itself decide the relationship between these atomic propositions
1863280	1868800	and so instead of having a specialized knowledge representation language a specialized logic
1868800	1873920	you'd use natural language and you'd let the large language model actually pass information
1873920	1882880	from one atomic proposition to another for those of you that are as old as I am you know I mean I
1882880	1892160	loved prologue and you know very simple horn clause inference procedures so what I just tried to do
1892160	1900720	here was take take that and sort of show how I might replace first order logic in there with
1900720	1909600	natural language statements but otherwise you've got proof rules proof structures and so this is in
1909600	1916640	fact actually a standard you know textbook example of how uh you might wind up doing
1916640	1921760	inference here so you've asked the question you know can Sam get a degree and you've got a series
1921760	1929120	of facts about what courses Sam has taken and a series of rules but the difference is all this
1929120	1937760	is all expressed in natural language rather than in some first order logic form okay all right so
1937760	1943520	just talking a little bit more about the social implications of all of this
1945120	1950240	so I think to understand the social implications I think one of the things you probably want to
1950240	1954880	understand is try and make some guesses about how the field itself might actually evolve
1956800	1965360	I can see sort of two possible futures one is where we wind up getting ever larger
1965360	1972240	proprietary monolithic close large language models that you effectively interact with via
1972240	1978320	web APIs that is the actual model itself the training data everything is kept proprietary
1978880	1986720	but you can just simply call it over the web another future is that there will be open sourced
1986720	1991040	language models and the weights will actually be available and you'll be able to do things
1991040	1997520	like fine tune those weights yourself and right now you know we're in the world where there's both
1997520	2004000	of these kinds of large language models and the proprietary models are better than the open source
2004000	2010720	models and I think really the big question about the development of the field is whether or not
2010720	2017040	fine tuning will turn out to take the open source models and make them competitive with the closed
2017040	2024320	proprietary models and I call that the 64 billion dollar question because that's probably about the
2024320	2030080	amount of money that the companies that are investing here sort of have invested
2031680	2039440	you know the language models are becoming increasingly capital intensive it costs
2040000	2046800	huge amounts million many millions of dollars to collect the data and actually do the training
2046880	2053360	of these things and capital intensive industries tend to concentrate you know you just look at the
2053360	2060000	chip manufacturing where I think there's only one or two fabrication factories in the entire
2060000	2068320	world that make the top-end chips that we all have in all of our devices so if that is actually
2068320	2074640	what the future of LLMs is then probably we will see that same sort of concentration
2075280	2082320	into just a couple of places I suspect the training data will become increasingly important
2082320	2089520	people are already talking about training data as being the ultimate you know limiting factor
2090640	2094640	and I think it will become the major differentiator particularly if you want to do things
2095280	2101920	like build LLMs for very specialized domains like healthcare finance other things like that
2102480	2109840	but and I actually think data and LLM quality control which goes back to that issue about
2109840	2114320	testing and evaluation I was talking about before that's going to become increasingly
2114320	2120480	important in fact I actually when I think about what will somebody like me in industry be doing
2120480	2127920	in five years time quite possibly you know testing and evaluation will actually be you know
2127920	2136080	90% of what we do you know we know that fine-tuning can mask a poor large language model right so
2136080	2142320	we know that you can take take Sydney and do a little bit of fine-tuning and have it at least
2142320	2151200	superficially call itself being but then Sydney reemerges in the right context as well so I think
2151200	2157120	one way we'll get around that is we'll start seeing things like certificates of origin
2157840	2165120	you know we'll be saying you know I guarantee that my large language model has been trained on
2165680	2171120	just high quality data and the same way as you see certificates of origin for you know
2171120	2178560	fancy cheeses and things like that you know the cows grazed on grass organically raised on the
2178560	2185040	south's southern meadows and all that sort of stuff and particularly if in the if that open source
2185040	2192880	world that I was talking about before if that comes into play I you know I see that has been sort
2192880	2198480	of really one of the really big challenges I've I've seen how data vendors small startups are
2198480	2203360	really under incredible pressure to produce something which they can sell because they're
2203360	2210480	usually cash constrained and the same thing may be true for startups that are producing large
2210560	2218080	language models they'll be under huge pressure to take somebody else's model and do a few tweaks to
2218080	2226000	it and try to present it as something that's completely new and yeah impact on nlp jobs
2227040	2235200	I I actually do think that it's not too far off when we'll be able to say something like
2235200	2240480	give an instruction to a large language model it's like deploy a chatbot the task is informing
2240480	2246560	users about the products that you'll find listed in this database over here I want you to interact
2246560	2252800	with users in a professional tone emphasize customer service rather than price and politely
2252800	2257840	decline to talk about topics that are related to the products and that will be it that will
2258560	2267600	build you a chatbot you won't need an expert development team you know I do think that however
2267600	2273600	that that's not going to come up immediately we will for the next say five years or so we will
2273600	2278960	need people that can create training data and fine-tune models and as I said before I think
2278960	2285280	evaluation and testing is just going to get more and more important brought to social impacts
2285840	2292880	right so I think we already know that deep fakes and fraud are just going to get supercharged by
2292880	2300960	this sort of technology and yes I think that's true I think we're going to see automation of
2300960	2307360	jobs not previously automated Krugman has an interesting article in the New York Times just a
2307360	2313920	couple of days ago where you know he makes the point that it doesn't really matter whether these
2313920	2320160	LLMs really are intelligent or not that even a souped up auto correct can actually have
2320800	2326320	quite major implications in terms of productivity he's actually really quite positive he seems to
2326320	2336080	think that actually these things might you know level society somewhat and they might I mean there's
2336080	2345120	some evidence that in fact the GPT-4 for example enables poorer workers to work at a higher standard
2346800	2352240	whereas the best workers are helped less by GPT-4 maybe that's the case
2354480	2361680	I think there's a number of risks I you know I think we are seeing you know AI models being
2361680	2367520	trained on public domain data that the creators when they made their data public really had no
2368080	2374720	intention no one no expectation of their data would be used in this way we're seeing a political
2374720	2379680	fight right now between media companies and tech companies about the use of data I think that's
2379680	2388160	still mainly about search rather than training AI models but that same fight I think will continue
2388160	2395280	I looking back to the first industrial revolution and things like the tragedy of the commons I don't
2395280	2400560	see any particular reason to expect a socially optimal outcome although I think the writer's
2400560	2406560	Guild of America settlement actually sounds like it's a pretty forward-looking one and
2407520	2414400	I'm very pleased to see that I I do think you know I I'm not one of these people that that
2414400	2422400	poo-poo's the people that are worried about you know AGI and misalignment I don't think we're likely
2422400	2428640	to be made extinction but to be made extinct but I do think we should be worrying about that
2428640	2432560	and the final point I'd like to make is that I think these things are economic and political
2432560	2440160	choices not really technical choices so it's an interesting question so those of us that actually
2440160	2445680	have the technical expertise probably are in a position to have our voices heard more than what
2445680	2452560	they would normally be so we should probably make use of that but I actually really do think that
2453120	2460560	it's not just up to the the tech companies in particular to try and make the important decisions
2460560	2470480	here so conclusions I think LLM's are here to stay a lot of my people my age remember the AI
2470480	2475920	winter of the last century I don't think there's going to be an AI winter just simply because
2477040	2483760	these things are actually way too useful for students intellectual revolutions are a great
2483760	2489680	time to enter the field because in fact actually the amount of knowledge you need to have to become
2489680	2497920	an expert is much much less I think LLM's open up new interesting scientific research questions
2497920	2506400	and directions NLP I think will have less emphasis on clever new algorithms and more on
2507040	2513440	yeah interaction and integration of models applications data design and training and
2513440	2519120	much more emphasis on evaluation so thank you very much
2525840	2532320	any questions or comments yes hi thank you for great talk super interesting I just wanted to
2533520	2537280	ask you about one thing in the middle of the talk which is about
2538080	2545680	this kind of neuro symbolic integration and you had this you had this kind of proposal that LLM's
2545680	2552480	are going to give you parts of bits and then you're going to use those inputs into a logic model
2553920	2557840	and I wondered like why do you think I mean personally I like
2558480	2564560	neuro symbolic learning so but I'm interested today like why why do you think that's a good idea
2564560	2570320	or good approach like why not just let the LLM do the entire thing like what what is it that
2571120	2577120	so certainly certainly there are people that are betting yeah um you know let's just let the LLM
2577120	2585760	do the entire thing yeah um I guess the answer I would give there is that there are a lot of
2585760	2592080	academics and in fact I'm sure many of you've seen this stuff right it's it's now quite you know
2592160	2599120	there's like a little mini industry of people coming up with things like you know chat GPT
2599120	2609920	cannot understand negation GPT for does not understand X is Y statements you know and in fact
2609920	2619200	actually I don't think I'm ashamed but we have a paper that is claiming that LLM's cannot
2619200	2625680	understand you know do not really properly understand entailment you know that walking
2625680	2634560	entails moving but moving does not always entail walking um so if you really believe that stuff
2634560	2640000	if you really believe you know and if you believe it enough so that you actually think that GPT 5
2640000	2644240	or whatever the next model is that comes out is going to have to exactly that same weakness
2644960	2652720	the idea then is build a symbolic component that addresses whatever weakness you happen to think
2652720	2659680	these large language models have but it isn't like I'll admit it is rather risky because these
2659680	2669040	things are improving so rapidly and I'm not so sure I mean it's it's a risk if you're a grad
2669040	2675200	student to say all right I'm going to commit the next year or two of my life to working on solving
2675200	2682560	you know the problem of negation in large language models and halfway through your research project
2683920	2689600	you know someone discovers that just by scaling up the training data another order of magnitude
2689600	2695760	all of a sudden now it's going to handle negation just perfectly and that's what I meant by in fact
2695760	2702000	I think that was one of the one of the slides that Kate added was saying that it's particularly if
2702000	2707680	you wanted to do something some sort of behavioral research or something else like that you know where
2708640	2714880	this the timescale which LLMs are changing versus the timescale of doing behavioral research is
2715760	2721760	the LLMs are just changing so fast that if you if you looked at today's LLMs and said okay you're
2721760	2727280	inspired by then here are some interesting behavioral predictions that they're making I'm
2727280	2731760	going to go out and start running some experiments with kids or something like that
2734000	2738560	yeah you know by the time you've collected a quarter of your data there's another model out
2738560	2745600	there and it's got it's making different behavioral predictions it's just I don't know what the answer
2745600	2759440	there is except to say that yeah yeah you might just um ask a a follow-up question so if if you
2759440	2772080	have a chat system I think that's actually for the zooms but I they also said there's a whole array
2772240	2780880	microphone inside the box so well um I'll just talk about um if as opposed to um adding a symbolic
2780880	2790320	system to to a chat system or you know an LLM um what about fine tuning and just doing lots of fine
2790320	2797280	tuning instead I mean that's adding more data but if you fine tune it yes with that kind of data as
2797280	2802880	opposed to going as symbolic group that's right wouldn't that be I think that's I think that's
2802880	2811360	very true and in fact that I think that's a good question is you know if so right now what was it
2811360	2818880	you know Gary Marcus is picking up on the fact that uh somebody wrote a paper that said that
2819600	2827200	uh oh look there's a whole lot of cases where the large language models uh will quite happily say
2827200	2834800	that oh let's see all right I don't know enough about celebrities unfortunately but you know
2834800	2843440	so-and-so and so is Tom Cruise's mother right okay it accepts that statement and you then ask the
2843440	2850640	other you then ask the question who is Tom Cruise's mother and it says I have no idea so
2851520	2857440	and it seems like oh well if I've actually got a couple of comments there so first of all it seems
2857440	2864560	like x is y that looks an awful lot to a math mathematical person as being like x equals y
2865120	2872560	and we know that what was it uh equality is what is it commutative you know so if x equals y then
2872560	2884960	y equals x um in fact actually x is a y actually really isn't commutative um you know uh you know
2885360	2894800	uh what was it you know chicken salad is a wonderful meal you know that can be true but
2894800	2901680	a wonderful meal doesn't always have to be chicken salad right um you know so uh
2906960	2909280	but anyway so might you know the yeah
2909280	2919360	do you think uh do you think there's going to be uh kind of I mean what what do you think about
2919360	2927520	like the kind of vision vision language models where you're kind of train it on like a ton of
2927520	2933120	images and then it generates a load of you know you can generate these images we actually did
2933760	2938640	you know before I did the startups just before I did the startups I think the last student I
2938640	2945680	worked with was working on image captioning um and I'd very much like to go back to I I think
2945680	2950960	that's really incredibly interesting so while I think it doesn't actually solve the chart the
2950960	2958560	cell Chinese rule Chinese room objection you know I mean ultimately the input to all these models
2958560	2967120	are really just activation patterns and you know the models got no reason to suspect that images
2968640	2975840	closely you know more connected to the world than languages but I think just in practice there's a
2975840	2983680	very good chance that there may be really interesting correlations that can be learned by correlating
2984560	2991360	you know images with language and of course the only issue there is that the
2991520	2998880	um the amount of compute that's needed to do this is just really enormous and it's
3000160	3007200	you know you'd have to do some have to get some deal probably with one of the major tech
3007840	3012800	companies to get your access to enough compute yeah to do it
3013520	3022960	and that's that's sort of the problem with with a lot of this research now and in fact I think one
3022960	3027120	of the comments that I wrote there is you know maybe in fact we should be thinking about something
3027120	3033760	you know physics has been very successful in getting funding for big science maybe we should
3033760	3040400	be thinking about ways of getting funding for academic big science as well for dealing with this stuff
3040640	3047120	well and there if you think of images still images that's a certain amount of compute power
3047680	3053680	I think of visual no ongoing visual scenes and movies that's a lot more
3054960	3064400	and yet presumably real world learners are you know taking advantage of the visual scene
3064400	3074000	as it moves by um and uh although learning can take place in blind people as well and that's
3074000	3079760	a whole another research area so you know you don't need vision to learn language but it certainly
3079760	3089680	can facilitate aspects of yeah and then yeah and I guess also I mean like looking forward you'd
3089680	3098560	want these systems to actually be kind of situated in the physical world somehow I guess right well
3098560	3108720	so that so that's the certainly lots of people have got that uh you know feeling that in fact
3108720	3117840	actually that that we need situated you know situated models but I mean isn't the input to
3117840	3124960	a model always really just an activation pattern isn't it really always just uh I mean couldn't
3124960	3131920	you always run you know so you know there was the question what is silver's chinese room argument
3131920	3138400	right there was that basically you know uh supposing you you come up with a computer program
3138400	3145920	which can translate english into chinese or sorry translate chinese into english so you give that
3146000	3152400	to a person that's sitting you know inside of a room and you just simply tell him you know here's
3152400	3160800	a set of symbols follow these instructions and give me the output that you you you obtained
3160800	3168880	by following these instructions and so's point is that even if this thing does actually produce
3168880	3177600	good english as an output you really can't say that the person understands chinese you know that just
3178320	3186640	they don't understand chinese you know they're just following these rules and uh so his argument
3186640	3192880	really is that there's something else that a pure rule following system really doesn't have
3192880	3198080	understanding that something else is required now lots of people wind up saying oh well what you
3198080	3206800	really need is grounding you really do need these you know you need the symbols to be connected somehow
3206800	3213440	to the real world but I think the model never really knows I don't see any way for our current
3214080	3219440	computational models to know that the bit patterns that we're feeding into them
3220240	3222800	yeah correspond to anything in the world
3224240	3229680	yeah I mean yeah I would agree on that but um I guess for that kind of symbol grounding you
3230560	3237360	like some some people argue you you need to have a kind of community right language users so
3238480	3248960	maybe that uh all kind of do you know um and and then the grounding kind of comes out of
3250400	3255760	people using the same symbol in the same way yes right yeah yes no no no I mean you're
3256480	3261840	kripke you know so philosophers of language like kripke have argued that in fact actually
3265120	3269840	that and you know in fact this is sort of very true of me because I grew up in the southern
3269840	3278320	hemisphere I don't know a lot of the northern trees so I'm not sure I can with catherine's
3278400	3283520	help now I can recognize aspens but you know I'm not really sure about the difference between
3283520	3290240	oaks and elms and the rest of them right but kripke you would say that I can still talk about
3290240	3297200	all of those things and when I talk about oaks I mean oak trees even though I might not be able
3297200	3303680	to actually identify an oak tree reliably and so kripke's story there is exactly what you're
3303680	3310000	saying it's a community of language learners sorry language users and I am willing basically I'm
3310000	3317600	agreeing to the authority of language users and I effectively what I'm saying is when I use oak
3318240	3324640	I use it to mean whatever the rest of you mean that you know that grew up and presumably know
3324640	3330800	exactly what an oak tree is right and he says that in fact actually with a lot of particularly
3330800	3338240	scientific terminology we wind up using it that way right many of us may not be able to define
3338240	3343920	exactly what the difference is between the different types of neutrinos or whatever but
3344560	3351040	we rely on experts within our community to be able to ground those things
3352640	3357520	all I can say is I don't even know how you'd even tell a large language model that it's part of
3358480	3363680	yeah yeah no I was sort of yeah kind of thinking that sort of thing myself like how
3364400	3371120	well what I mean does that does the community there just mean literally that actually just the
3371120	3380960	text documents that have been fed into it is that I mean I guess with the the instruction
3381680	3384800	tuning I guess you get a little bit of that right
3388640	3396080	so anyone else have questions at all yeah so that it's almost just that you could have a community
3396080	3405120	of users of a particular model yeah that would have a sort of various types of queries within a
3405120	3416400	particular domain that might help train up that model then to become more a realistic conversational
3416400	3425680	agent within that particular domain perhaps yeah yeah so maybe that's kind of yeah yeah yeah
3426800	3432880	who knows maybe a year or two from now you know those things will start to emerge yeah
3432880	3437920	yeah so I think one of the things that Catherine and I are hoping to get out of this is to find
3437920	3447280	out more about you know work at SFI that we might connect with right but a sort of general interest
3447280	3458960	in you know language learning psychological aspects computational aspects you know so
3458960	3472160	so we're here until Tuesday afternoon Tuesday evening so please please contact us right
3472160	3475360	okay thanks
