start	end	text
0	4640	Are you still a reader in mathematical neuroscience? Is that still accurate?
4640	7520	I've changed all our titles a little bit. Okay, so a man of many titles.
9440	13520	Whatever I was at Bristol, I know you were the head of the Computational Neuroscience Unit,
13520	19520	and he was also an examiner for my PhD five. So super, super happy to have him here today.
19520	25040	Thank you, Connor. Brilliant. Sorry, one second. Cool. Well, I'm super excited to be here. This has
25040	28720	always been kind of a legendary location for me. I've always wanted to visit, and
30320	34880	it's really blown me away. It's an amazing looking place. It's my first time in New Mexico, and oh
34880	40080	my god, the countryside here is just incredible. I assume that it's traditional to start talks here
40080	45360	with some reminiscence about the time that you met Murray Gaumann. So I met Murray Gaumann 20
45360	49360	years ago. I was still a particle physicist then. He was coming through 20 College Dublin,
49360	53040	where I was working at the time, and he asked me where I was from, and I said I'm from Galway,
53040	59120	which is true. And he said, oh yes, Galway, home of this, Galway, off whose coast the Spanish Armada
59680	64160	founded in the storm, saving the British Navy the trouble of sinking it. Home too, he said,
64160	69840	to the myth that the lusty sailors thus wrecked, swam ashore, and bred with the local women to
69840	77040	create the black haired Irish. Something he said that is not true, the black haired Irish are
77040	81840	presumably the remnants of the indigenous population, the population there before
81920	86960	and accounts of population whose traditions and cultures we know from archaeology,
86960	91280	but whose language is wholly lost and unknown. And I was just incredibly impressed by the whole
91280	96240	thing. I mean, he knew where Galway was. He knew that we had this link with Spain. He knew about
96240	100320	the Spanish Armada sinking. He knew that we all believed, as it turns out incorrectly, that the
100320	108800	Spanish sailors had created these sort of black haired folk in Galway. And he told me this
108800	112560	interesting thing about how these black haired folk are probably the remnants of the indigenous
112560	117520	population. And I was always intrigued by this idea that there was a language that they spoke,
117520	121680	which is now completely lost. The idea of these completely lost languages is lovely.
121680	126880	It also struck me with something of a show off, but that's true too. But it was very impressive.
126880	133520	And so I'm very, very, very glad to be in the Murray Gilman building. I was, I did start life
133520	137440	as a particle physicist and then became a neuroscientist and only started working on language
137440	142560	recently. So I don't know that much about it. But the reason I started working on language was I
142560	147840	read this kind of bizarre quote from Novichonsky, which says, in their essential properties,
147840	151920	and even down to fine details, languages are cast in the same mold. At the Martian scientists,
151920	156080	my reasons we conclude that there's a single human language with differences only at the margins.
156080	161920	And that just seemed so, so wrong to me. And, you know, I thought
162880	169840	that maybe that is an accident of not, you know, thinking about how different languages are. It
169840	173040	seemed to me, you know, the languages are incredibly different from each other. And I'm
173040	177680	looking with where I'm from to know a little bit of Irish. And Irish is, you know, although an
177680	184720	inter-European language is more different to, you know, English than maybe French and so on is.
184720	188320	I mean, for starters, it has a different word order. One of the more striking features is the,
189280	194480	it doesn't have a verb possession. So here, this is the Irish for, I have a newspaper, Thon,
194480	201280	Newton, Ogham. And what's happening there is that there's no, the Thon is just the verb is. So the
201280	206640	literal translation of that sentence is the newspaper is at me. The Ogham is a preposition
206640	210880	combined with the pronoun, which is a feature of Irish. And prepositions do a lot of work in Irish
210880	215360	that it's done by other parts of speech in other languages. So instead of saying that you have
215360	220320	a newspaper, you say the newspaper is at you. And similarly, Thon, Newton, Ogham, the new
220320	224160	newspapers from me is how you'd say that you wanted the newspaper. So it's strongly that,
224160	231040	you know, that languages are very different from each other. And that it's wrong to
231040	237600	presuppose that languages all spring from the brain in the way that Chomsky is suggesting.
238320	246800	Because, you know, his idea is that the languages bear the imprint of the human mechanism. Whereas,
246800	250800	for me, as a neuroscientist, at the time, somebody knew about how vision worked and so on,
250800	254320	it seemed that languages were much more likely to just be the unfolding of some
254880	260560	search for statistical structure. And maybe something to do with the properties of the world
260560	265200	around us, the fact that there are actions, and there are, and hence verbs, and there are objects,
265200	270800	and hence nouns. But of course, I'm no longer sure that I was right in thinking that. And I've
270800	275440	begun to appreciate something of the wisdom of what Chomsky said. And of course, another example
275440	280480	from Irish Thon, Newton Thum, the newspaper is under me. That's how you'd say the newspaper is
280480	286000	about me. But there you can see that even in English, the same work is being done by the
286000	292240	prepositional construction as is being done in the Irish. In other words, although, you know,
292240	296000	the Irish uses a different preposition and prepositions tend to vary as you probably know
296000	301280	a lot from language to language. The idea of using a preposition where you might otherwise
301280	309840	have a different verb is not so unique to Irish. And so maybe languages are somewhat similar to
309840	313840	each other. And it struck me, or strikes me, as I'm sure it strikes you, that this is a really
313840	319680	important question. I love this sign because it says that the vehicles will be prosecuted without
319680	324480	warning. But this is a warning sign. So of course, it makes it impossible for them to do that.
324480	331360	And I have here started to remind me to comment on the idea that language is complicated because
331360	337440	some aspects of language are to do with what's in the world. The structure of language is structured
337440	343600	by its use case, by communication. And some parts of language presumably are to do with the
343680	351520	human ability to communicate, or our ability to perceive, or the calculations we do in
351520	357120	producing, interpreting meaning and in producing meaning. And some parts might be more idiosyncratic
357120	362880	to do with some linguistic mechanism. And deciding between these, deciding, you know,
362880	368160	recognizing the fact that we can say something like this, which logically makes no sense at all,
368160	374240	but which, you know, in a Wisconsinian fashion, quite clearly communicates something. It is a
374240	380560	property, you know, is something that's deeply rooted in who we are, and our language is a part
380560	385760	of how we conceive of ourselves as conscious individuals. And separating what's special
385760	390720	about language, what's special about us, what's special about, with, you know, what language
390720	396400	necessarily has as a communication medium, seems an interesting and an important challenge.
396880	403200	So, of course, as a neuroscientist, when I decided I was interested in this, the first
403200	410240	inclination was to do EEG experiments. And I wanted to start talking a little bit about that
410240	416240	before talking a little bit more about models of language evolution. I started off actually
416240	420720	trying to do EEG experiments on Irish speakers. I don't know how much you know about Ireland
420720	424480	that Irish speakers, but it turns out that trying to do EEG experiments on Irish speakers
424480	427920	is very difficult, because lots of people who claim they can speak Irish actually really can't.
429440	432240	As I did a whole series of experiments on people who claim they can speak Irish and
432240	437680	discovered, in fact, the EEG just told me that they were lying. So I went back to working on
437680	443600	English speakers. And I was interested in testing this idea that the brain does privilege grammatical
443600	448480	structure over mere statistical structure. Now, if any of you have done EEG experiments,
448480	451680	you'll know just how damn difficult they are. I mean, the idea that we detect
452560	456320	electrical fields outside the brain is actually slightly surprising, because you'd expect
456320	460400	all the electrical fields to cancel out. And the only reason we can detect anything at all
460400	464080	is that there's a slight preponderance of synapses pointing one way around another.
464080	470240	And then, of course, you bathe the whole thing in salty fluid, which makes it hard to detect
470240	476000	anything. For cognitive experiments, the difficulty is still greater still. And I'll refer to this a
476000	482640	few times. This EEG data is obviously showing somebody having an epileptic fit and they were
482640	486480	induced by tooth brushing, so they started brushing their teeth, and here they have the fit.
486480	491360	And you can see that it's a very pronounced activity. But for cognitive tasks, of course,
491360	496400	not only is the activity not so pronounced, I mean, epilepsy is distinguished by the
496400	502080	synchrony of neural activity. So that's exactly the thing that's recently easy to detect.
502080	507520	Cognitive things aren't so distinguished. And secondly, of course, when you're
507520	512400	doing an EEG experiment, when you're a participant, you tend to sort of think other things. You're
512400	515760	there, the stimulus is being played to you. You're supposed to be thinking about the stimulus,
515760	519600	but in fact, you're thinking, oh my god, I wish I wasn't doing this experiment anymore. Oh my god,
519600	523200	this is really boring. Oh my god, all people are immortal. I'm going to die someday. I wonder
523200	530400	what I'll have for dinner. I mean, it's very hard to detect cognitive stuff with EEG. So
531360	537440	the thing that we did, again, following a paradigm that was introduced by David Peeble and Naiding
537440	543040	and his co-workers, was a frequency tag experiment. So in a frequency tag experiment, to try and
543040	550560	separate a signal from noise, you concentrate your stimulus at a particular frequency. And so that
550560	555360	you know, noise tends to stuff that you're not interested in, people thinking about their mortal
555360	560000	end or the end of the experiment, that happens at all sorts of frequencies. But if your stimulus
560000	564480	is at a particular frequency, then you can use Fourier transform or whatever to concentrate
564480	567760	on the thing that you're interested in. So in this experiment, we play people
568400	572560	adjective noun sentences. We've choose the adjectives and nouns, so they're single syllable.
572560	576240	We record them and then coerce them a little bit so they're all exactly the same length.
576240	580480	We play the syllables at 3.125 hertz. That just turns out to be a particularly good
580480	584800	frequency for, it's a comfortable frequency for listening to syllables. We have long, long streams
584800	588480	of these things. I mean, we have old rats, that man, ill wife. It goes on and on forever.
589440	593120	And we choose the adjective nouns quite carefully so that the
594400	599120	bigrams between old and rat and rat and sad are roughly the same. You can't get them exactly
599120	603200	the same, but you try and keep the statistical structure the same. And then what you see here
603200	608640	is that obviously there's going to be a response in the EG to the stimulus at 3.125 hertz. But
608640	614640	there's something else happening, which is the noun phrase. So if there's a response at 1.5625
614640	619840	hertz, that's a response to something that's not directly in the stimulus, but is related to the
619840	625200	meaning of the words. And what interpretation is that that's because the brain privileges noun phrases
625200	631840	over not noun phrases. And the other example is an adjective verb stream. In the adjective verb
631840	638880	stream, we make sure we have the same sort of biogram structures as we had for the adjective
638880	643520	noun. It's likely to come beside each other as old and man or whatever it was I had before.
644960	652640	But now there is no grammatical structure. So if we see a response at 1.5625 hertz
654400	659760	for the first stimulus and not for the second, that's indicative of the brain responding hopefully.
659760	664480	It's indicative to the brain privileging a grammatical structure, which was I guess the
664480	669200	one thing to be interested in. It turns out these data are quite hard to analyze.
670000	673520	You know, you think all you have to do then is take the Fourier transform and look at the size
673520	680720	of the peak at those particular frequencies. But EG is extremely noisy. And so if you do that,
680720	687200	you don't see anything. What you have to do is look for the phase locked component of the response.
687200	693760	So you play lots of these streams, so you make each stream five seconds long. You play repeatedly
693760	698480	the stream. And then you look for that portion of the response, you know, once you've taken the
698960	705280	transform, that portion of the response which has a similar phase. And the complication, of course,
705280	711120	is that the overall phase is not so important. It depends on how big your head is and where the
711120	719120	electrode is and all sorts of other things. And so the actual phase of people's responses
719840	725920	at whatever frequency, at one point, whatever it is, isn't important. And you can see that here.
725920	730400	So this is just each of these dots, each of these lines corresponds to a participant.
730400	734960	We had 16 participants in this experiment. And then for each participant, we've got all the
734960	740960	different electrodes. And this is the average phase of the response averaged across. It was 10 trials
742640	749120	for each participant for each stimulus. So this is the adjective noun, stimulus. And in the dot
749120	754480	is for the 32 electrodes. And just for convenience, for ease of comparison, three of the electrodes
754480	758800	have been picked out and are cut at the same across the participants. And basically all you can
758800	764160	see is that the actual overall phase is quite different. And so what's interesting is, you
764160	769760	know, each of these dots hides the fact that there's 20 trials. Each trial has its own phase.
769760	776240	And the signal is in the degree of alignment of that phase. And so to analyze these data,
776240	780240	well, the easiest way to do it, it turns out, is to make some complicated Bayesian models. So we
780240	784400	imagine the phase is being drawn from some distribution. In this case, it's circular
784400	789280	Cauchy distribution, which turns out to be the nicest. The circular Cauchy distribution is quite
789280	795120	cool. It's just a wrapped Cauchy distribution. And when you do the wrapping, you can set some
795120	802320	of the theories and come up with an analytic formula for it. You say the data is kind of noisy,
802320	807280	right? Immensely. Yeah. So the Fourier transform is also like pretty noisy.
807280	811440	Yes. Yes. So you have to go through several steps. So, you know, obviously,
812240	816480	for start using the frequency tag experiment makes it less noisy because, you know, the other stuff
816480	820720	is happening at different frequencies. But even then you have to do this sort of phase lock analysis,
820720	825760	which I'm briefly reviewing. I don't want to spend too long on it. If you're interested in the
825760	830480	analysis of EEG data, we've been thinking about it a lot. And I can talk to you about it afterwards.
830480	836480	But basically, in summary, we make a big Bayesian model of the results. What we're looking for,
836560	843040	we imagine the phases have been drawn from a wrapped distribution. We have some prior for the
843040	847440	variance of that distribution, some prior for the mean, that doesn't matter. And so the signal
847440	854320	will be in this, the posterior distribution of this phase of this variance for the phase.
854320	862240	And so this basically is the result. Here, we're looking at the, it basically is the inverse of
862240	868320	this variance, what they call the mean circular resultant against frequency. And so, sorry,
868320	872880	I'm standing in front of the camera. And so what we're seeing here is the
874640	879280	different frequencies. This is the frequency of the syllables. This is the frequency of the phase.
879840	885040	For the adjective verb condition, you see that there's a big response at the syllable rate,
885040	888720	as there is for all of these conditions. These are other conditions. This is mixed lexical,
888720	894240	mixed phrase, random. We did six different conditions, but we'll just concentrate on these two.
895280	899520	For both the adjective noun and the adjective verb, there's a response at the syllable rate,
900320	905920	showing up as a reduction in the variance of the phases. But for only the adjective noun,
905920	912400	are you seeing a response at the noun, at the, at the phrase rate? This, on the right,
912400	918160	this is some basic equivalent of the usual bar and star type graph. All it's really indicating
918160	923920	is that the adjective noun condition shows substantially, or you might say significantly more
926800	932800	response, which is, again, a significantly more mean resultant, just like the inverse of variance,
933760	940720	than these other conditions, which is basically showing that the brain has a response to the
940720	946320	grammar. That we can't just think of the brain as performing a statistical inference on the
946320	953120	sentences, trying to extract meaning and so on. It does something every time it hears a noun phrase,
953120	959120	and it doesn't do anything every time it hears an adjective verb word pairing, because that
959120	966160	isn't a grammatical object, or at least that's the interpretation. So, you know, having done this,
966160	971440	I was, I was kind of amazed. It shows that there is sort of stuff happening in the brain that,
971440	978480	that is more formal or more, more akin to a grammatical manipulation than, than you might
978480	985280	have expected, and shows sort of the presence in, in, in this, you know, in this discussion as to
985280	988960	what part of language is about the world, what part of language is about communication, what
988960	993520	part of language is about the machinery the brain has to deal, to deal with language. There's more
993520	997920	in that sort of brain part than you might think, and it's certainly, will it be of interest to
997920	1004640	start to consider grammar, and the brain's view of grammar. Now, of course, these,
1006480	1011520	these stimuli that we were dealing with are, are quite new that we were using, as I said,
1011520	1018160	following this idea of people in a ding, this frequency tagged paradigm, where you are playing
1018160	1024960	the stimulus at a set frequency in order to be able to extract the EG signal of interest. But
1024960	1029280	it's very hard to, to, to come up with, you know, there's lots of things that you can't do with that.
1029280	1036000	We were able to examine the presence of noun phrases, etc. But what you'd really like is to
1036000	1041520	use free text like this, with like complicated, you know, sentences, different types of grammar,
1041520	1049280	and so on. This would be better as well, because one problem with the, the frequency tagged
1049280	1054240	experiments is that they are quite, quite boring and annoying to, to, to be a participant in those
1054240	1059600	experiments. And it doesn't encourage this kind of thinking away from what you're supposed to be
1059600	1067760	thinking. The problem here is the, is the difficulty in, in, in analyzing the data and, and trying to
1067760	1074080	show a relationship between the EG response and, and, and what's going on. So, you know, what,
1074080	1079600	what we'd like to do, I think, is to understand what grammar looks like to the brain. You know,
1079600	1084560	we, we know what Grammaticians think grammar looks like, but we don't know, you know, but
1084560	1088640	I often think about the phrenology skills that people were interested in in the 19th century.
1088640	1093120	The pseudoscience phenology divided the brain up into regions of the brain that did different
1093120	1096560	things, which is actually true. There are regions of the brain that do different things,
1096560	1100160	but they got it completely wrong. You know, they, they thought that the cerebellum was the organ
1100160	1105600	of amethyst, which is bizarre. In fact, it had some role to do with predicting the consequence of
1105600	1110560	motor commands or something. So, you know, the, the, the ideas we have about grammar might be
1110560	1115440	quite different from what grammar actually looks like. And to, to probe that, about one thing
1115440	1119280	certainly to do would be to, to, to do EG experiments with free text.
1119280	1125280	So, Connor, like, so just to kind of get an idea of the experiment itself. So you'd have like,
1125280	1128320	I know you said this is going to be like really complicated, but in principle, you'd have your
1128320	1132800	participants in a room reading this out loud and you'd just be looking at the brain signals. Is
1132800	1136720	that the, you would be reading it to them and you'd be reading the two brain signals. Yeah.
1136720	1141040	So, so, I mean, here, here's the results. So, I mean, there are people, you know,
1141040	1145680	it's not just us that's trying to do this. There are lots of labs doing experiments along these
1145680	1153040	lines. Stephen Frank in Namegun, for example. You have measures of kind of comprehensive,
1153040	1158160	like some behavioral output, you know, did like, what is, what's the point of the brain responding
1158240	1163760	to these things? Are you, are you measuring that as well? Like in the previous experiment,
1163760	1167360	you showed that there was a response better, but like, why, right? Like, what is that,
1168000	1171440	what is that, what is that doing for? I mean, what we do do is we make sure people are paying
1171440	1176720	attention. Okay. So we have a detention trap. We introduce particular words that they're supposed
1176720	1182400	to press a button to show that they're still paying attention. We, one advantage of the
1182400	1188160	Bayesian analysis is you can check the participants are, you know, you can, there's terms in the,
1188160	1191440	in the, you know, you have a big Bayesian model. There's a term to do with the participants,
1191440	1195520	you know, attentiveness. You can see that some of the participants are clearly not paying attention.
1195520	1198000	You can look at them through the window and see they're not paying attention as well.
1199040	1207840	You know, I guess I'm asking like a deeper question about, you know, you're asking what,
1207840	1212000	what is language for, what's the brain for? Just like some of these responses,
1213920	1218240	I'm always skeptical, particularly, I mean, I'd know the fMRI literature much better than the
1218240	1222400	EEG literature of where you just record a response and you say, okay, this brain,
1222400	1226960	this part of the brain, there's a response to something, but without some sort of behavioral
1226960	1230000	output that you're measuring connected to that response, I'm always skeptical of
1230800	1232880	how much, how much does that actually tell us about,
1233120	1240400	you know, what's actually happening and why, why we should care that there's a response there.
1240400	1244000	I mean, we should care there's a response there, because it shows that we are responding to the
1244000	1250000	noun phrases. And that's part of a story where, you know, we want to understand how the brain
1250000	1256320	understands language. How do we answer that? Well, I'm not sure. And, and you, you know,
1256320	1261120	people try behavioral experiments, they try manipulations to see how it changes people's
1261120	1266080	understanding and so on. There's, you know, there's a long history of this. But it hasn't
1266080	1273120	yet produced an account of language. And I'm not to answer your question going to produce one either.
1274960	1282320	But that's absolutely what we need to do. I mean, so I guess my idea at the moment
1282320	1290400	is that we start, that we use the EEG response as some sort of proxy for, for what's happening.
1290400	1297120	And we try and see, well, just let me say what we've done here, for example. So again, we're
1297120	1303200	looking at the EEG response to free text. We're doing some big regression, and we're trying to see
1303840	1308400	how people are responding to different types of words. And what we can see what this is showing
1308400	1313680	is that for part of the EEG response, there is a difference in how people are responding to
1313680	1319120	words according to the categorization of the words into function and content. And so we can see that
1319920	1325200	people's brains respond differently to some particular categorization of word. And so
1325200	1329680	what you might be optimistic about is that if you've got better at this, you could try different
1329680	1337280	categorizations and match them to different ideas about how the brain might approach parsing language.
1338560	1344160	But I agree. You can hear that then to maybe the grammars of the linguistics.
1344160	1348080	Yeah. And then separately, of course. Mel asked me not to mention transformers,
1348080	1354400	but then separately, you look at, you can probe how large language models deal with language. You
1354400	1359920	can probe the grammatician's approach to language. You can probe the whole linguistic
1359920	1364960	tradition of our accounts of how language is dealt with. And then this gives a sort of neural
1364960	1370160	account, the neural view of what grammar is like. But whether that's going to work or exactly the
1370160	1375920	details of that, I don't know. And so certainly what I would advocate is that we could collect a
1375920	1383840	big data set, you know, with lots of dots of different languages. The little prince, as maybe
1383840	1389040	you know, is the non-religious text that has been translated into the most languages. It exists in
1389040	1394960	300 languages. It exists as an audiobook in 50 or 60 languages. And so it's been suggested by these
1394960	1401040	folk here, Jinxing Lee, Brennan, Haile and some of their co-workers, that we collect a large
1401680	1406080	corpus of different varying qualities, some, you know, some MEG, which is, you know,
1406080	1411280	very high quality, some consumer EEG, but with many more participants across lots of languages.
1411280	1416880	And we start trying to understand, from the point of view of the brain, what grammar looks like.
1418080	1423840	So far, this hasn't been done. And I guess people are trying to raise money to do it and have failed.
1424800	1431520	So that's, you know, that's just by way of sort of motivation. That's where I came into thinking
1431520	1436480	about language. And, you know, my interest in language is trying to understand, you know,
1436480	1442240	how is language, what's special about language, what makes language, language. And of course,
1442240	1445520	when you start thinking about this, you start asking these questions, how do we do these
1445520	1448960	experiments? And it strikes you that there's a whole sort of separate story to language,
1448960	1453520	which is to do with evolution. And maybe the hope that if we think a little bit about evolution,
1453600	1459120	about how languages arise from evolution, the species point of view, and then separately,
1459120	1466240	how languages change, it would be, it might tell us something about the innate structure of language.
1466240	1470480	And maybe we can then think about how language, why languages might have to be the way they are,
1470480	1476480	compared to other ways they could be. And I think this is, you know, a very interesting question.
1476480	1482720	One of the sort of striking things is that, well, you know, we know from Creole languages and
1484720	1491360	sign languages and so on that languages do arise with a large amount of their structure
1491360	1495920	already present. But it's still an open question as to whether there are parts of language
1495920	1501520	that develop through time, you know, as languages evolve, do they, do they change in a consistent
1501520	1508480	way? Or are they the same from the very start? So near where I work, there's a museum, the Bristol
1508480	1515680	Museum, and Art Gallery, I left that bit out. And they have some of these freezes from Nimrod.
1515680	1521040	So the palace at Nimrod was broken up by people who went there and they took the panels and sent
1521040	1527440	them all around the world in, I guess, an act of gross theft, although what was left in Nimrod
1527440	1532800	has since been destroyed, so in the way it was quite lucky. And these panels do have this kind
1532800	1539360	of cuneiform script written across them. It's a standard inscription. And it's Ashnirah. Ashnirah
1540880	1546320	is basically showing off in this description. So this standard inscription here in cuneiform
1546320	1551360	explains what a great person he is. And some of it's quite sort of bloody as this bit is,
1551360	1555360	there are men young and old, I took prisoners of some I cut off their feet and hands of others
1555360	1560400	I cut off the ears, noses and lips of the young men's ears. I made a heap of the old men's beads
1560400	1565360	I made him heads, I made him in a rash, et cetera, et cetera, et cetera. Some of it's much nicer.
1565360	1571040	It's about making pleasure palaces and beautiful things, et cetera. But I couldn't find any of
1571040	1575520	that in an easily cut and pasted form. So I had to get this rather bloody bit instead.
1576480	1583440	It says something about our civilization. But the point anyway, is that what is striking
1584080	1591040	in the text is the lack of sort of the normal clause structure. There's very few instances or
1591040	1595360	no instances of the sort of clauses that you might expect where there's who and which is
1596000	1602400	and so on linking together the sentences. Instead, it's this rolling list, this very sort of list
1602400	1607280	like structure. And so you wonder is that because the language has not at this point
1607280	1611520	evolved all the structures of modern language, the sort of merge and clause structures that we
1611520	1618240	have now, or is it just that that's how they like to write in their ceremonial functions.
1618240	1622480	So just striking that there's a lot to be gained from trying to understand
1622480	1627920	something about the evolution of language. And so that's what I wanted to talk a little bit about
1628640	1635520	now. And so the first thing I wanted to talk about is really is to urge a return considering
1635520	1642320	the iterative language model that Simon Kirby and his co-workers came up with about 20 years ago.
1642320	1648000	So I don't know if you know the iterative language model, it's a language, it's a model for the
1648000	1657520	evolution of languages. The story is that Kirby and his co-workers discovered it. They simulated
1657520	1663200	a little bit, but they came quickly hard up against the computational limitations at the
1663200	1670480	time. It is quite computationally expensive. And so they worked on it and then they kind of abandoned
1670480	1676320	it and went on to try and do the same experiment that they'd done in simulation in real people.
1676320	1681920	And so Simon Kirby has very successfully spent the last 20 years using toy languages and toy
1681920	1689280	language learning as a pro into how people learn languages, but hasn't considered much beyond the
1689280	1694000	original work, the iterative language model itself. And now that we've got faster computers and so on,
1694000	1699120	it might I think be very interesting to go back and think about this much more. So in the iterative
1699120	1704880	language model, basically you have a teacher and the teacher teaches a pupil and then the pupil
1704880	1712400	becomes the teacher and teaches another pupil and so on. So it's a chain of learnings, teachings
1712400	1717120	and learnings. And the idea is that the language progresses or changes through this teaching and
1717120	1723520	learning. The hope is that we might learn something about how the structured language arises by seeing
1723520	1728880	if it arises through this simulation of the teaching and learning process. And the crucial point,
1728880	1735680	as we'll see, is that there is a bottleneck. So the agent has a language, the teacher, they teach
1735680	1744000	only a number of exemplars to the learner, to the pupil. And then the pupil in turn, the pupil
1744000	1749760	becomes the teacher, the pupil is teaching the next learner and they have to extrapolate from the,
1749760	1754720	well, they've extrapolated from the few examples, exemplars they've been taught, a whole language,
1754720	1760560	and then they choose other exemplars from that language to teach the next pupil. And it's this
1760560	1767440	process of bottleneck and extrapolation from the bottleneck that Simon Kirby hoped and in fact found
1767440	1774160	did produce some of the properties that we believe languages should have. And so again,
1774160	1779200	this just summarizes it. The teacher provides signals and meanings. So they say, you know,
1779200	1785200	cat and then shows a cat, just like in cartoons. The cartoon idea about how we teach children,
1785200	1788800	although people who work on children point out that that's not actually what happens,
1788800	1795760	that we very rarely teach children language in this supervised way. But here, we do have this
1795760	1801120	naive picture. The teacher provides signals and meanings. The learner learns the mapping from signals
1801120	1806160	to meanings. And then when they reach maturity, they use that to, they then use a version,
1806160	1809920	which I'm going to talk about in a minute, to invert that map. So they get a map from
1809920	1815680	meanings back to signals. And then they choose some random meanings, produce the signals for,
1816640	1823760	as an example, to the new learner. And the process continues. And
1825200	1829840	the idea is that the language that's produced should have some of these nice properties. So
1829840	1836640	the properties they have that we're to look for are expressivity, stability and compositionality.
1836640	1843680	Expressivity is basically, can all meanings be expressed? In other words, if you map signals,
1844640	1848880	set of signals onto the space of meanings, how onto is that map? If a completely expressive
1848880	1854240	language is one where the map from signals to meanings is onto, if the signals all map to the
1854240	1859360	same meaning, that would be not at all expressive. And expressivity is just a sort of counting of,
1859360	1864080	I mean, basically it's a counting of the map of the signal space into the, into the meaning space
1864080	1869600	divided by the size of the meaning space. Stability, that's just how after the languages
1870160	1875760	have time to mature, is it roughly stable from, from iteration from generation to generation.
1875760	1881440	And then compositionality, of course, is the sort of more difficult one. That's what makes
1881440	1886960	languages languages, what makes a language a language, the idea that a part of the signal
1886960	1891920	should consistently code for some aspect of meaning. So, you know, in this case here,
1891920	1898800	we have the word for orange. I think funny to make this as an example when I was making these
1898800	1903760	slides, but of course it's the worst possible example. So, so the idea is we have a word for
1903760	1908720	orange, the color. You can see my problem, I'm really screwed myself, but yeah, we have a word
1908720	1913280	for orange, the color, and it's used here to describe orange, the fruit, and then we have blue
1914000	1920720	used to describe the blueness of the orange on the right. And the idea of compositionality is that
1920720	1926880	the word orange, it means the color orange when referring to the color of an orange, or it means
1926880	1931120	orange when referring to the color of a high-vis jacket. And one of the things, you know, the
1931120	1937120	people have learned in trying to make agent models of language evolution is that compositionality is
1937120	1942560	actually quite hard to enforce. So, you know, if you have two reinforcement learning agents and
1942560	1947200	they're playing what they call the Lewis signaling game, they're trying to learn a way of telling
1947200	1952880	each other about things, what tends to happen is that, you know, if you don't, if you restrict
1952960	1957120	their signal space, so they just don't have a separate word for every possible combination of
1957120	1963440	attribute and object, et cetera, but rather they're forced to have some symbol for attribution, some
1963440	1968000	symbol for what's being described, they have to have a word for the color and a word for the
1968000	1974720	object. You can do that, but they don't consistently use the same color word. You know, they might use
1974720	1978320	orange to mean orange when describing oranges, but they might use blue to mean orange when
1978880	1985680	describing high-vis jackets. It's enough for the communication between agents, you know,
1985680	1992880	reinforcement learning agents, that there is a word to distinguish potential colors of the
1992880	2000080	orange fruit. But that word, the word for orange, doesn't have to be the same word. It doesn't have
2000080	2005200	to match to the same color as when you're using it to describe something else. You know, we do this
2005200	2012480	a little bit ourselves. I mean, you know, when we're talking about horses, we use the word chestnut
2013680	2019200	for what we would call brown in other instances. But generally speaking, the main property of
2019200	2024400	language is compositionality, and the idea is to seek that here. So in this version, in the simple
2024400	2028560	version of the iterated language model, we just have eight bits of meaning and eight bits of signal.
2029200	2034960	So the meaning space is just 256 potential meanings, and the signal space is 256 potential
2035440	2041840	signals. And then the learner has a simple neural network. You can see that this all dates back to
2041840	2048800	the year 2000. So it's a two-layer network. Signals come in. So the teacher says a signal
2049440	2056560	and then provides a meaning. And then the learner, perhaps using a sigmoid
2057120	2062720	non-linearity. So probability of ones and zeros for all the potential meanings compares it to the
2062720	2070320	actual meaning backpropagates and thus learns to map from signals to meaning. So that's the plot.
2070320	2076320	And of course, the thing about this is that this neural net has been trained on only these
2076320	2083600	exemplars, the small part of the space of meanings that form the teaching event from the teacher to
2083600	2089600	the learner. But the mechanism, of course, provides a map from any signal to a meaning.
2090400	2095440	So that's the first part of the teaching. But then the next part, which is that the
2096720	2103680	learner has to also get a map from meanings to signals. And so the way that's done is using a
2103680	2111200	version without worrying about it too much. Here's a two-bit example. So here we have a signal
2111200	2119440	mapping to a meaning, using the neural net. So what the learner is learning is the correct
2119440	2123280	mapping. So maybe the correct mapping here, the mapping that the teacher is trying to teach,
2123280	2132320	is that one zero goes to one one. But literally speaking, the learner can provide from this
2132320	2138640	mapping, coming from their neural net, a probability for all four potential meanings.
2138720	2146000	And this is a contrived example, so that the probabilities are 0.1, 0.1, 0.7, and 0.1, if you
2146000	2152880	obviously multiply P1 by P2, or 1 minus P2, or whatever. And so in that way, the learner can
2152880	2162880	produce a table of all possible maps from signal to meaning. So here are the signals,
2162880	2166960	and these are the probabilities that give meanings. And in a version, all you do,
2167440	2174480	you run the table the other way. So basically, if you're given one one one, so if you see the
2174480	2179120	signal one zero, your neural net tells you these are the probabilities of the different meanings.
2179120	2184640	And to get a map from the other way from a meaning to a signal, you look across this way and see
2184640	2192640	that 0.7 is the largest number. And then you decide that in your aversion, in your inverting of the
2192640	2198080	map for meanings to signals, you'll map one one to one zero, because that's associated with the
2198080	2203760	highest probability. So that's the process of aversion. You read across each of these lines,
2203760	2209200	you put a one there, and then this gives you the map. So zero zero will map to one one. So if the
2209200	2218320	learner wants to randomly decides to teach the next learner the signal from zero zero,
2218320	2222880	they'll say zero zero, sorry, they'll say one one, that's the signal, and they'll point out that
2222880	2229280	that corresponds to the meaning zero zero. So that's the that's the iterated language model. You can
2229280	2233840	see that this aspect of it is completely unrealistic and something that you might want to get rid of.
2234880	2241520	But it's the thing that seems to work. And what the reason, of course, it's unrealistic is that it
2241520	2250000	requires that the learner on reaching maturity goes through all potential, all potential signals
2250000	2255600	and all potential meanings, works out the corresponding probabilities, and then does this aversion map.
2255600	2260880	And that's obviously not true of language. The whole point of language is that you can't go through,
2260880	2265280	you know, you don't have to go through all meanings and all signals. And even from a
2265280	2271840	computational point of view, it's extremely resource heavy. And even with modern computers,
2271840	2276240	it means that you couldn't. So the hope is that you could look at this iterated language model
2276240	2280080	far beyond the sort of eight bit examples they're doing here, maybe even use it,
2280080	2285120	you know, on language itself. So replace the set of meanings with actual sentences and
2285120	2288880	force the agents to come up with their own internal languages and see what happens.
2289440	2298160	But so anyway, with this aversion process, you do get a way of mapping. And so you can run the
2298160	2306000	iterated language model. This is just us recapitulating what Kirby and co-workers saw 20 years ago.
2306000	2312160	You can see that if you do this, you know, with a suitable size of bottleneck, so 256 meanings,
2312160	2317760	you allow 50 of them to be taught, you run it across generations, the expressivity goes up,
2317840	2322800	stability. So this is the instability, the opposite of stability, that goes down.
2323360	2330880	And more remarkably still, the compositionality increases. So this I think is intriguing and
2330880	2335120	worth sort of reminding us. Sorry, this is just some, you know, interview. There's two different
2335120	2341760	measures here going on of compositionality. But it's basically measure of positionality
2341760	2346880	based on entropy. But I mean, I do think it's remarkable. And I think it's worth us going back
2346880	2351360	to the iterated language model and trying to see what it tells us about language evolution.
2352480	2357440	To get rid of the inverter, it turns out, you think it's going to be easy. So for example,
2357440	2362560	you think that you can just add another, you know, in the learning process, you can think that the
2362560	2368240	learner could have, you know, a meaning and then also learn its inversion at the same time,
2368240	2372880	stick in as an objective function, getting the right meaning, but also recovering the original
2372880	2379360	signal. That seems like it would almost certainly work. And I guess this is one of the sort of
2379360	2384960	few original results we have in this area. And it's a sort of a negative one, which is that
2385520	2391120	it turns out that a version, although clunky, something that you hope might just be a convenient
2391120	2398800	way of doing the inversion is necessary. It creates some sort of pressure which drives
2398880	2405520	apart the mapping and makes the mapping from signals to meanings onto and produces expressivity.
2405520	2409920	So if you just replace it with a recurrent neural network, well, what you might call a recurrent
2409920	2415760	neural network, with this architecture here, you lose that expressivity. I think that it's,
2416720	2421360	so what we're seeing basically is that the bottleneck, which forces the neural net to
2421360	2426880	generalize or sorry, the agents to generalize is producing compositionality. But the aversion
2426880	2433120	is also required for expressivity. The bottleneck also tends to cause this pushing together of the
2433120	2437840	mapping, which loses the impressiveness of the language. And so I think there's ways around
2437840	2441120	that. I think, you know, maybe if you think the whole thing is a sort of Bayesian reconstruction
2441120	2447680	problem, you can come up with a new objective function, which has a sort of contrastive term,
2447680	2453440	which forces it not only to get the right mapping from signals to meanings, but also punish it for
2454000	2459520	other meanings being close in probability. When you started doing that, it hasn't worked yet.
2460080	2463280	It's still not producing the expressive map. It goes up for a while and then something happens.
2463920	2470560	I'm sure we can sort that out. But the plan here, the hope, the conclusion is that there's
2471840	2476880	a lot going on in trying to understand how compositionality and expressivity kind of
2476880	2483200	rise in these simple agent models. It would be nice to have a sort of working example first and
2483200	2488480	then try and decide, you know, could we generalize it to much larger, more difficult cases? But I
2488480	2492960	think, you know, the problem is that, this slide is just to remind me to sort of announce the problem,
2492960	2497040	which is that, you know, with these models, you're starting to worry about how much you just
2497040	2500960	follow your own footsteps. You start putting things into the models to produce some sort of
2501520	2506480	behavior. And then you start to worry about exactly what we can compare to when we look,
2506480	2513360	you know, how we compare the model to the actual behavior of languages, how easy it will be to
2514240	2520720	decide, you know, whether the model is a good one or not, and then work out from whether the model
2520720	2526320	is a good one or not, you know, what is it about it that it tells it? What does it tell us about
2526320	2532160	the brain? I mean, one thing we sort of, one of the things we try, obviously, is look at how it affects
2533120	2538880	language, language synchrony between different communities. So here we've, we've taken the
2538880	2545760	iterated language model, and we've stopped it being a simple learner to, to, teacher-to-learner
2545760	2549920	interaction, but rather have some sort of web of interactions where a learner has a privileged
2549920	2554320	teacher, but also learns from, from their community. And then we take networks where there's
2554320	2559520	greater connectivity within, within a cave. It's, you know, it's one of these cave people
2560160	2564800	graphs. So there's greater connectivity inside a cave, then across the caves, and we look at
2564800	2569440	how the languages evolve and stabilize. And we discovered that you can, you know, depending
2569440	2575680	on your parameter choices, you can end up with, you know, five different languages in six caves,
2575680	2579920	this cave, they don't even have their own language, or a case where there's, everybody speaks the
2579920	2585280	same language, or where some languages are shared and some of them are different, or where there's
2585360	2591920	two different languages. And so you can use this model to try and understand properties of the
2591920	2597200	distribution or the sizes of language, languages. But the problem there, of course, is that there's
2597200	2601680	millions of parameters, you have to work out how to, how to, you know, what the network should
2601680	2607920	look like, how to structure the teaching events, how to, how to structure the size of the bottlenecks
2607920	2612560	compared to all possible being single pairs, etc. And so the model, although intended as a very
2612560	2616640	simple one, when you start trying to apply it in ways that can give you data that you could
2616640	2623520	compare to the real world, the model becomes still simple, too simple to produce something,
2623520	2630000	you know, directly comparable to language, but much too complicated to draw easy conclusions from.
2630000	2636960	And so it struck me at this point that we should try and think of what the very simplest model
2636960	2641760	of language evolution is. And that's what I wanted to finish by talking about briefly. So the idea,
2641760	2649920	here, is that we want to look at language change and look at the, the, the most personmonious
2649920	2655840	description of what goes on as languages change. And so, obviously, one property that languages
2655840	2660320	have to have is alignment. If you're talking to someone, you want their language to be almost
2660320	2664240	the same as your own, or else you won't have, you won't be able to understand them. And the closer
2664240	2668800	their language is to yours, the less sort of cognitive low communication will play,
2669440	2675120	will place on you. So I think the first properties of languages as they change is that they should,
2675120	2682880	should align. The second property is the converse, this inclination towards change. You know,
2682880	2688880	obviously, teenagers do this all the time. I have teenagers, they, they, they use words in
2688880	2694720	different ways. They, they delight in language invention. I mean, weirdly, my, my mother is
2694800	2700080	almost the very opposite of a teenager. It's the same. She's forever making up new words and
2700080	2704240	new ways of saying things, often based on puns. And then the puns turn into words. And then she
2704240	2708560	just uses them in everyday conversation, expects people to, to follow. And she just does it in
2708560	2714080	a, out of a sheer delight in language invention. There's also a kind of a more strategic point
2714720	2719760	to language invention, which is to find shorthands for saying things, auxiliary words are pushed in
2719760	2725680	with the words that their exiliaries to pronunciations are changed. People say small phrases to mean
2725680	2729760	more complicated, large phrase things, et cetera. So there, there, there, there are these two sort
2729760	2735200	of competing forces that work in language changes. The third one as well, which is an
2735200	2739760	inclination towards consistency. That's exactly the sort of thing that the bottom leg in the
2739760	2744160	iterated language model deals with. And I'm not going to deal with that one here. That's the idea
2744160	2750240	that, you know, if, if in one sentence you put, you know, if at some point you decide that the
2750240	2755440	verb goes at the end of the sentence, it does that for, for, you know, that can sit that,
2755440	2760000	that rule doesn't just apply to one, one's type of sentence, but rolls through the language. So
2760000	2765680	once something starts changing, other parts of language change to suit it. So that's an inclination
2765680	2771680	towards consistency. You know, and it's obviously kind of amazing. I mean, if you think of how
2772000	2779680	how, how noun and verb modifications work with Arabic and Hebrew, I mean, it's just incredible
2779680	2783760	the way that, you know, everything has three, three consonants, and then you've got the verbs and
2783760	2788080	the endings and, you know, how did that happen? I mean, it's just some, some small changes then
2788080	2793120	became rules. And so that is obviously an important part, but not one we'll talk about now. So
2793120	2799200	obviously agreement alignment between speakers and spontaneous change. You probably guessed
2799200	2805200	when we're going with this, that that's a nising model. So in the nising model, it's a model of
2805200	2811760	magnetism, as you'll all know, at different lattice sites, you've got a spin, the spin goes
2811760	2817120	upwards or downwards. And there's an energy associated with, with the alignment of the
2817120	2823840	spins. It's a lower energy state. If the spins are aligned, then if they're not aligned. And so
2824640	2830240	you would obviously expect an nising model to evolve towards total alignment. But there's also
2830240	2835600	in the nising model thermal effect. So in an nising model, this is the metropolis
2836400	2840640	formulation of the nising model, what you do to evolve the model is you choose a random, you
2840640	2844960	choose a site, and then you consider what would happen if you, if you flip the spin. So we'll
2844960	2850560	label the spins plus one and minus one. And if the, if the consequence of flipping the spin
2850560	2857520	can be written down like that, I hope I have the signs right there. But basically, if the spins
2857520	2863280	become more aligned on average with the neighbors, so the one site, these are the four, for example,
2863280	2868160	neighbors of the middle site, if it becomes an average more aligned with them, then a DE will
2868160	2872880	be negative, and you'll accept the change. If it becomes less aligned, you'll still accept the
2872880	2877680	change. And this is the terrible part with some probability given by the exponential of minus
2877680	2883840	DE divided by T. And so DE is positive in this case. If DE is negative, as I said, you always
2883840	2889120	accept the change. T is the temperature. In the case of when you're modeling magnetism, it's
2889120	2893600	literally the temperature. If T is very large, then this is always one, you always accept the
2893600	2900000	change. And so you just get a random up and down. If T is very small, then this will be near to zero.
2900000	2904240	You'll never accept the change. And you'll just get it flowing to total alignment. And so you get
2904240	2911360	these different patterns. This is the very random. This is the very aligned. And this is the critical
2911360	2917680	point in between. So this is a model of a phase change. And at the phase point, at the critical
2917680	2922560	point, you have scaling behavior at the cluster size. And that's what people are interested in
2922560	2927680	the IC model. So obviously, we can map these two things, the movement towards alignment,
2927680	2932160	towards the idea that you're trying to lower the energy. And the spontaneous change is like the
2932160	2938880	thermal fluctuations. But clearly, of course, if you had only one spin, then you'd have only two
2938880	2942560	languages, the upper language and the down language. And that wouldn't be very interesting.
2942560	2948240	So what we do instead is we've got a vector of spins, d dimensions, where d is some number,
2948240	2953200	you have to be chosen. And we imagine that these are coding for different properties
2953840	2959920	of the language. So one spin might be, how do you say father? And obviously in lots of languages
2959920	2967440	like Latin, German, Irish, the word is almost the same as that would be plus one. Do you use
2967440	2972560	derivative of pattern for the word for father? And minus one would be languages where you don't.
2974080	2980160	I don't know any languages that don't use pattern like derivatives. Anyway, not a good example.
2980160	2985200	Or it could be something to do with word order. So here you see the word order in English,
2986000	2992480	subject followed by, followed by verb and adjective followed by noun, whereas in Irish,
2992480	2998880	it's verb followed by subject, noun followed by adjective, and they would be, you know,
2998880	3002480	correspond to two further spins. And then a further complication is that that choice
3002480	3008320	tends to be the same. So if the subject comes before the verb, the adjective comes before the
3009280	3014880	the noun. If the subject comes after the verb, it's the other way around.
3016720	3019920	And so you can imagine there might be two spins, one for this part of the word order,
3019920	3022560	one for that part of the word order, and then in some elaborations model,
3022560	3026320	there'd be some relationship between them. But that's basically the model we have.
3027280	3034000	We have, as it were, in this first idea, we've got D independent ising models, and a given set of
3034000	3038480	plus ones and minus ones constitutes a language, we choose some temperature,
3038480	3043120	we run it forward and we get some behavior. I do have a graph in a minute, I'll show you.
3043120	3049040	But the main thing that you do see is that because it's lots of interlocking patches,
3049040	3054080	one for each of the D dimensions, you tend to see language continuum. And language
3054080	3060240	continuum are a property of languages. So, you know, these days, everything's a bit more complicated,
3060320	3066640	national boundaries and official, you know, government documents and radio stations and so on.
3066640	3072800	But in the olden days, you tended to have no hard language barriers, boundaries. So if you
3072800	3078640	walked from, you know, Portugal all the way to Sicily, well, Portuguese would be very different
3078640	3083440	from Sicilian, but as you do it, you'd never actually be somewhere where we're, well, depending
3083440	3087040	on your route, and that's going to be the point, if you go this way, you're never going to be
3087040	3091120	somewhere where people in nearby villages can't understand each other. The languages gradually
3091120	3099200	change one to the other. And so that property of language distributions is well reflected by
3100000	3104800	this ising model of language, and obviously the temperature determines how big these clusters
3104800	3108240	are and so on. And that's something that you might try and fit against some knowledge of
3108240	3114240	the distribution of languages. But the problem, as you could probably anticipate, occurs here,
3114240	3118160	which is the Basque country. And if you, if you instead of walking from
3118160	3123280	Portugal through Galician and Castilian and Arganese, you kept along the coast as well,
3123280	3127440	you might, particularly if you're a pilgrim. Well, if you're a pilgrim, you're going the other way.
3127440	3132400	But either way, if you strayed into the Basque country, you would encounter a linguistic barrier.
3133280	3138080	Basque is quite, quite different from, it's not an Indo-European language. This is a sign,
3138720	3148160	remarkably, in French Gascon, which is a Pocotin language. And Basque, the Basque is this here.
3148160	3152480	And you can see that it's very different from the others. This is Basque here as well.
3154160	3160000	And so there is, in real languages, a language barrier. There is the possibility of language
3160000	3166080	barriers. And also, of course, you know that we don't have to be able to talk to everybody.
3166880	3171760	You know, we, we can have neighbors who speak a different language. And we can talk to them
3171760	3176400	without necessarily aligning our language to theirs. We can speak to them, for example,
3176400	3180720	through a lingua franca. These days, using translation by using somebody who's bilingual
3180720	3185440	or being bilingual ourselves. This is an example of Kiswahili, which is a language that, you know,
3185440	3190000	100 million people, that's not a bit much, a large number of 10 million people can speak
3190000	3194400	as a second language, but only a million as a first language. And because it exists as a
3194400	3198880	lingua franca, allowing people who don't have a mutual language to live beside each other.
3198880	3206240	So the next version of the language evolution model is this preference sizing model. So the idea
3206240	3215120	here is to run the same sort of dynamics, alignment and thermal change, but only allow or only have
3215120	3220720	each side interact with which of the ever of the four sides around it, in the simplest case,
3220720	3227840	has the most similar language to its own. So, so the idea is that for each of the, for each pair
3227840	3232880	of, of sites, each pair of speakers, I guess, you can work out the difference between their
3232880	3237760	languages. And then you could, you need to, you only run the sizing model between the speaker
3237760	3243440	you've randomly chosen to consider changing one of their plus ones and minus ones, and whichever
3243440	3247600	neighbor is closest to it. And so that is the idea being that you only speak to the people
3247600	3251840	who speak the same language as you. And so that's the new version, there's a paper about it there,
3251840	3259440	in ALI. And it kind of works. So there's lots more to be done on this, but this is, this is the basic
3259440	3266160	idea. This is the originalizing model. And here is a histogram of, so this is only five dimensions,
3266160	3270320	you can run it in far more, you know, five different language attributes, you can run in
3270320	3276960	far higher dimensions. But in the case of the first sizing model, you can see that this is the
3276960	3281120	number of different, average number of differences between the speaker and their neighbor. And you
3281120	3287200	can see that the speakers and their neighbors tend to have very similar languages. And very few
3287200	3293200	people, very few pairs of people have languages that have very little in common. In other words,
3293200	3298000	this version of the model does not allow for linguistic borders. Whereas in this preference
3298000	3304240	model, where a comparison, the dynamics is only relative to whichever neighbor has the closest
3304240	3308960	language to you, you do have lots of pairs where people speak the same language or very
3308960	3313440	similar languages, but you do have pairs where people speak very different languages. So this is
3313440	3321120	the, this is my proposal of the simplest possible model of language evolution. This work has only
3321120	3329360	just started, but the idea is to consider, you know, consider different structures of preference
3329360	3335680	and how many neighbors you interact with and so on, and then try and find different temperatures
3335680	3340800	and then find the structure of the size of language groups there and compare it to real
3340800	3345680	day clear. And so that's something that we'll do in the future. And that's it. Thank you very much.
3345840	3361040	So I was assuming throughout this presentation that the bottleneck in the iterative model kind of
3361040	3366320	corresponds to Chomsky's poverty of stimulus argument about language learning. Is that
3367600	3373920	right? Corresponds to, I mean, it's, it's, it's meant to be representative of the fact that
3374800	3380240	people, children only like experience a very limited amount of, but it adds something extra
3380240	3386880	to what Chomsky says. So Chomsky uses the poverty of stimulus as evidence that the brain must have,
3386880	3391760	you know, linguistic things, linguistic things. Whereas here, the poverty of, the poverty of
3391760	3398000	stimulus is a mechanism for, it says that the structure of language is a response to the
3398000	3402080	poverty of stimulus. So it changes that. I mean, that's what I think is really nice about this
3402080	3407200	model. It turns that language upside down and says that in fact, it's not that the brain has some
3407200	3414640	special language mechanism, it's that language has, has evolved so that we can generalize from a,
3414640	3420720	from a poverty of stimulus. I was interested in, because I can imagine a more innate style person
3421840	3429920	saying, you know, it's this convergence from the statistics to just a definite mapping in the
3430000	3436400	aversion process that is leading to compositionality, but maybe there is some sort of innate
3436400	3441440	mechanism, which is responsible for something that has the same effect as aversion.
3443040	3447040	Yeah, no, but do you think that the, the work you're doing with this model,
3448080	3454800	is there something that makes you lean more towards the, the empiricist take on this?
3455680	3464080	So, I mean, I think one of, one of the things that we're learning at the moment, you know,
3467360	3471760	you know, obviously, you know, the objection to transformers as a model of language is that
3471760	3479200	they require this massive stimulus. But conversely, they, they, they are very sophisticated at learning
3479200	3485440	grammar. You can see that if you back away from the, from the learning aspect, and you just look
3485440	3491760	at the ability of these models to perform grammatical tasks, it's quite incredible. We do, we do, you
3491760	3497600	know, we do do experiments now where we ask, you know, a transformer or an LSTM or something,
3497600	3504560	can you learn gender agreement? And so I have a student Priyanka who's teaching new words to,
3504560	3509280	to an LSTM. So she takes an LSTM, it's pre-trained. She freezes everything but the
3509280	3516080	representations. She teaches it a new word. She gives it some grammatical context, gender context.
3516080	3522080	So she doesn't French, obviously. So she says, you know, la, we use the word trilobie for some
3522080	3526320	reason, because it's the least common word in the vocabulary list that we're using. So we take
3526320	3530560	the word trilobie, we cut off its representations, we reintroduce it somewhere else in the
3530560	3536560	representation space, and we train just the representation space on a few examples where
3536560	3541440	you use the word la or la, describing trilobie, and then you ask it to do other gender agreement
3541440	3546400	tasks and it does it completely well. So even though it's learning one aspect of gender,
3546400	3550000	it's been taught the gender of the word through one aspect of gender, and it's been tested on
3550000	3555360	another, it has abstracted the, the abstract category of gender. And so, you know, the initial
3555760	3564960	bias that you had, that you needed, you needed special mechanisms, you know, Chomsky and
3565520	3571280	mechanisms to allow the brain to perform the form of manipulations associated with grammar.
3571280	3576560	That, I think, has been demonstrably made false by, by large language models. They,
3576560	3581680	they have this very, I mean, the structure of transformers seems completely bizarrely crappy,
3581760	3588640	right? And yet they do these amazing things. And so, it's quite possible for these, you know,
3588640	3593120	simple network models that are doing statistical learning to learn this stuff.
3594400	3599040	But that's not the same as saying that it can learn it against a poverty of stimulus.
3602640	3606480	But what I think the iterated language model is maybe indicating, and again, we need to start
3606480	3611680	using the iterated language model on much bigger examples. It's, it's indicating that you don't
3611680	3617680	have to put much more in, you know, you, you, you, you, once you start thinking about a version or
3617680	3622880	exactly what the objective function is, and the needs to, to generalize, it's possible that it's
3622880	3628080	not just that the, it's possible that the language that evolved, that you could evolve the language
3628080	3634160	where you can learn from a, from a poverty of stimulus. Does that make sense? I said that in
3634160	3637440	the very roundabout way, but I mean, I think I'm essentially agreeing with your initial, initial
3637440	3647200	point. Feel where you're coming from. Thank you. Thanks for the lovely talk. I think we could have
3647200	3653840	done this at home some day. No, no, no. But yeah, I was wondering, are you, are you thinking with
3653840	3659680	the Schelling model, are you thinking of maybe looking at the, or maybe already have the different
3659680	3666880	patterns of, like actually taking different patterns of like verb order and, and so on, and
3666880	3672080	kind of encoding them and plugging them into this model and seeing what you get out. Because I think,
3672800	3679520	I mean, one, like one thing is, I think with, you know, subject verb, object ordering, there are
3679520	3685840	certain patterns and some patterns are more frequent than others. For example, I don't know.
3685840	3690800	And I guess that wouldn't. I mean, it's, it's very hard to know because we tend to think that the
3690800	3694960	patterns represented by the Indo-European languages are, are much more common. I mean,
3694960	3700800	just to answer that specific point, it is, you know, so you think, for, well, for example, the,
3701360	3705440	the claim is that verb object is much more common than anything else. But that's only if you can't
3705440	3710640	buy speakers, if you can't buy languages, that's maybe not so obviously true. Okay. And Irish,
3710640	3715120	of course, that was one of my original interests is a language where verb and objects aren't beside
3715120	3722240	each other. So, you know, I mean, I think the point with the ISI model is to make the simplest
3722240	3727280	possible model. And so we, you know, I don't think you can retain that advantage to the model,
3727920	3733680	while at the same time tying your coding to specific features, but rather the idea would be
3733680	3742000	to introduce into these dynamics some abstract version of these features. So,
3742560	3749840	so you could include some sense of consistency, which would be an interaction between the spins.
3749840	3755200	So not only are, is a speaker interacting with the, with their neighbors, but there's also an
3755200	3760240	interaction within the spins themselves. And that would be the idea that once you flip one thing,
3760240	3766800	other things should flip as well. But, but I think, you know, there's sort of two, two types
3766800	3769840	of models here. There's one that you might, the iterated language model, where you might actually
3769840	3779520	try and use that to probe actual properties of actual languages. But with the difficulty that,
3780720	3787360	you know, it's, it's, it's not a completely parsimonious model. You always have the problem
3787360	3791120	when you're doing ancient modeling of deciding whether you're looking at your own model or
3791120	3794480	you're looking at the world. And then there's the ISI model, which is supposed to be that there's
3794480	3798720	a simplest possible language, a model of language evolution. And the idea there would be to look
3798880	3802800	at very simple properties such as cluster size, cluster distribution, et cetera,
3802800	3807680	and compare them to real languages, which hasn't been done yet. But that's, that's where we're going.
3810160	3815760	So it could be wrong in assuming this, but sort of tying together the start and the middle of the
3815760	3823200	talk. And in the iterated learning model, you have this mapping between meaning and sensations.
3823920	3835360	And in the EEG experiments, I guess, is, is the, is the purpose there to try to uncover
3836560	3844240	how that mapping is established in the brain and how, you know, maybe, you know, neurophysiology or
3845120	3850480	segregation in the brain biases or influences that mapping itself. Like, is that what you're
3850480	3855440	trying to get at with the EEG experiments? Yeah. I mean, as, as he pointed out, we're a long way
3855440	3862720	from actually doing any of this, but the, that the sort of overall picture is you language has
3862720	3867360	structure. We can discover that structure. How do we discover that structure? Probably, I think
3867360	3872160	the best way to do it is by looking at EEG responses. You know, we do have commentaries on
3872160	3878560	language invented by grammaticians. That's really them trying to impose what they've learned about
3878560	3883760	Latin onto other languages. And it's probably sort of quite naive compared to how the brain
3883760	3889360	considers parts of speech and the relationships between parts of speech. But the parts of speech
3889360	3895120	and their relationships are probably important either as a reflection of, you know, and again,
3895120	3900480	the discussion there is relevant of the mechanisms the brain uses for producing language and
3900480	3905920	understanding language, or the language that the, that's the languages that have had to evolve so
3905920	3911040	that we can learn them despite the poverty of stimulus that we experience as children.
3912560	3918400	And so the iterated language model is trying to find out what, I mean, potentially is trying to
3918400	3922160	find out what those properties of language might be. And we'd like to compare them to the real
3922160	3929280	properties of language, which we discover through EEG. That's the big plan. Where my thoughts were
3929280	3933760	going with that, I was just wondering if there are any, you know, takeaways from that EEG research
3933760	3939520	or from, you know, the neuroscience realm that could perhaps be brought back over to the
3940880	3945280	iterated learning model, but specifically, so in that bottleneck layer and your
3945280	3953520	representational bottleneck in the meaning. Can you take any principles from neuroscience to
3953520	3958400	apply, you know, to constrain that, you know, the structure of the bottleneck?
3958400	3968000	But I mean, the story of my life is that when I started working in neuroscience, I thought, you
3968000	3974400	know, we really want to map from actual neural networks to what they're doing, you know, we want
3974400	3980160	to go from, you know, this is a neuron and this is what it's connected to, this is how it works.
3980240	3985120	And so, you know, I ended up working on tadpoles, because I thought tadpoles are really simple
3985120	3991360	creatures, maybe we can understand tadpoles. And tadpoles, neonatal tadpoles make a decision. So
3991360	3995760	basically, older tadpoles are fearsome hunting animals, but the very, very young ones, when they've
3995760	4001520	just emerged from the egg, and they're still carrying, they carry with them kind of an egg pouch.
4001520	4005040	So the material from the egg that they grew in, so they don't have to eat for a couple of days.
4005600	4009520	And these animals are very, very simple. So you touch them and they swim away.
4009600	4014400	You grab them, they struggle. So there's exactly one decision they have to make,
4014400	4018320	which is whether they've been grabbed or touched. And I thought this is the simplest
4018880	4023120	decision that any creature makes. And there's like eight different neurons involved in this.
4023120	4027440	Maybe we could go from the network to understanding the decision. And I had a PhD student who
4027440	4033200	worked on this model for, you know, four years, reduced it all to two dimensions so we could
4033200	4037680	draw a phase diagrams. And it was like just the most unbelievable mess. And in the end,
4037680	4042640	you know, we just knew nothing about tadpoles, yet alone, you know. So I think, you know, I just
4042640	4047600	don't think that's, I think we're so far away from, you know, something as complicated as language
4047600	4054800	and mapping anything but the broadest principles of neural dynamics to the cognitive dynamics.
4054800	4062160	I think the gulf between, you know, the neural substrate and the cognitive function is so great
4062160	4066800	that I would be trying to stay away from that. The idea is rather to look at the structure of
4066800	4071520	language and ask, look for the structure of language in EEG. So the mechanism there is only
4071520	4078080	to try and find out what the brain regards as grammar. And then to compare that to the
4078080	4082320	iterated language model and how it might, or some other model and how it might treat grammar,
4082320	4086400	basically. Which is not to say that people are, you know, I mean, maybe the cerebellum, you know,
4086400	4090400	maybe the hippocampus. But if you can't understand tadpoles, what could you do?
4091920	4094880	I mean, basically we disproved the existence of tadpoles, I think.
4097360	4107520	I'm sorry, Seth. Do you worry that expanding kind of this type of EEG work to a much broader set of
4108400	4111120	structures would result in the same sort of mess?
4112960	4122480	Yes. I mean, I think it's a good strategy. I agree with you that it seems promising and much
4123040	4128720	simpler than that or some of the stuff that's been done mapping kind of semantic content into the
4128720	4135600	brain with MEG and fMRI. But it turns out like the tadpoles, it's a mess. Yeah, it seems more
4135600	4141520	promising. But I mean, who knows? I mean, yeah, I mean, we, what we've done compared to what we
4141520	4146560	want to do is tiny. So it seems to me to be the most straightforward strategy. But, you know,
4146560	4151760	at the moment, I can't even get my little prince experiment funded. And not only an eye, but these
4151760	4156560	are greater people who are trying to get money for the same thing and get it done. So, but, you know,
4159840	4167600	yeah, I said they're building in Bristol an instrumented museum, an instrumented cinema,
4167600	4172160	so it'll be possible to do simultaneous EEG experiments on 100 people at once with consumer
4172160	4176640	level EEG. So, and I think they're building this thing without any clue what they're going to use
4176640	4181120	it for. So I think, I think there's a potential to persuade, you know, to get hundreds of recordings
4181120	4184400	of people listening to the little prince and lots of different languages. And you can help
4184400	4189600	it feel that you can discover something that way. Because languages are just so different, you know,
4189600	4197360	I mean, you know, if we could get Malay speakers, they don't, they don't, it's an isolating language,
4197360	4202080	they don't change the words for the plural, you know, if we can get speakers of, you know,
4202080	4206240	Irish, they put the verb at a different place. I mean, it's just, there's such a variety,
4206240	4210720	it'll be really interesting to see what we can see. But I agree, you can probably turn
4210720	4214000	into a mess. And I think the way that you can turn into a mess is to try and
4216000	4221360	find things that are too detailed. We have to try and find the very broad principles,
4221360	4227280	you know, because even that's unknown. Is this cinema? Like... Yeah, yeah, they're building this,
4227280	4233360	they've built it. They got funding for something called the Future Institute. And it's good to
4233360	4238800	have this instrumented cinema where people wear EEG and they'll watch films, basically.
4238800	4242400	So, like, when all the new films come out, you can have like the EEG response.
4242400	4246160	Exactly. Yeah, exactly. I mean, I think that's how they solve it. Yeah. I mean, it's nothing to do
4246160	4250960	with me. I mean, it's people in the psychology department. And the idea is that I think they'll
4250960	4255040	have, you know, all sorts of instrumentation, right? They'll have, you know, we have to work out how
4255040	4259200	sweaty people are and the heart rates and, you know, how much they're breathing, you know,
4261520	4266000	and their EEG. So, I mean, it sounds like one of those things that sounds really cool. So,
4266000	4269040	they were able to get money for it. But I think when it comes down to it, they're going to
4269040	4272240	struggle to find experiments. So, hopefully, they'll do experiments for me.
4273920	4276480	Nightmare giving a talk in that room. You can track in real time.
4279920	4282160	Yeah, there's lots of cool things you can do like that. Yeah.
4286960	4287600	Cool.
4287600	4289200	Thanks for coming in.
