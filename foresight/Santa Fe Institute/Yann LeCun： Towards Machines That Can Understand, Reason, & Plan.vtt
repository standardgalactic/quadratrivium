WEBVTT

00:00.000 --> 00:02.800
All right.

00:02.800 --> 00:07.240
OK, first machines they can understand,

00:07.240 --> 00:09.760
which means also reason and plan.

00:09.760 --> 00:12.800
It's going to be a lot of overlap with what Josh said,

00:12.800 --> 00:19.200
at least in terms of motivation, but not in terms of solutions.

00:19.200 --> 00:23.640
OK, first statement is that machine learning sucks,

00:23.640 --> 00:26.680
certainly compared to what we observe in humans and animals

00:26.680 --> 00:31.280
and their ability to learn and learn efficiently.

00:31.280 --> 00:33.600
You know, until recently, most of machine learning

00:33.600 --> 00:37.120
was based on supervised learning,

00:37.120 --> 00:41.760
required enormous amounts of label samples.

00:41.760 --> 00:45.520
What has taken over the last few years

00:45.520 --> 00:47.200
is self-supervised learning, which does not

00:47.200 --> 00:49.440
require as many label samples, but still requires

00:49.440 --> 00:52.440
a huge amount of samples.

00:52.440 --> 00:56.040
And in the end, those systems of still relatively brittle

00:56.040 --> 00:59.120
makes stupid mistakes, do not reason or plan,

00:59.120 --> 01:01.480
compared to humans and animals that can learn

01:01.480 --> 01:04.560
new tasks extremely quickly, because they understand

01:04.560 --> 01:09.240
how the world works, presumably, and they can reason and plan,

01:09.240 --> 01:12.440
and have certainly some level of common sense.

01:12.440 --> 01:17.400
So in our systems of today, most of them anyway,

01:17.400 --> 01:20.520
not absolutely all of them, but many of them,

01:20.520 --> 01:23.240
have a constant number of computational steps

01:23.240 --> 01:25.640
between their input and output, which

01:25.640 --> 01:28.200
means that whatever reasoning they do

01:28.200 --> 01:30.200
does not change, depending on whether it's

01:30.200 --> 01:33.160
a difficult problem they're trying to solve or not.

01:33.160 --> 01:34.400
They cannot really plan.

01:34.400 --> 01:36.200
The only systems they can plan at the moment

01:36.200 --> 01:38.320
are the ones that are designed to play games

01:38.320 --> 01:40.640
or to control robots.

01:40.640 --> 01:44.760
But things like LLMs do not plan.

01:44.760 --> 01:47.000
So how do we get machines to do like humans,

01:47.000 --> 01:48.640
which is to understand how the world works,

01:48.640 --> 01:50.920
predict the consequences of actions

01:50.920 --> 01:54.680
they might take, perform chains of reasoning

01:54.680 --> 01:57.040
with a potentially unlimited number of steps,

01:57.040 --> 01:59.720
and plan a complex task by decomposing them

01:59.720 --> 02:01.720
into sequences or subtasks.

02:01.720 --> 02:05.200
So let me start with this idea of self-supervised running,

02:05.200 --> 02:08.480
which really has taken over the world of AI

02:08.480 --> 02:11.080
over the last few years.

02:11.080 --> 02:14.680
And it's the basic idea of essentially presenting

02:14.680 --> 02:17.960
an input to a system, let's say a text, a window of text,

02:17.960 --> 02:24.040
or video, or a few images, and hiding part of it,

02:24.040 --> 02:26.480
and then training the system to capture the dependencies

02:26.480 --> 02:30.600
between what is observed and what is not yet observed,

02:30.600 --> 02:33.080
but eventually will be observed, whether it's

02:33.080 --> 02:37.120
the future of a video or a different view of the same scene

02:37.120 --> 02:43.680
from an image or words that have been obscured.

02:43.680 --> 02:45.400
And I say capture the dependency.

02:45.400 --> 02:47.160
I don't say predict because I'm going

02:47.160 --> 02:49.520
to talk about models that don't actually predict,

02:49.520 --> 02:51.160
but capture the dependencies.

02:51.160 --> 02:56.160
So a very successful example is language models.

02:56.160 --> 02:58.840
So self-supervised language models.

02:58.840 --> 03:01.280
And the idea goes back a long time to do this.

03:01.280 --> 03:03.440
I think the first paper to really kind of experiment

03:03.440 --> 03:10.520
with this was paper in around 2010 by Colbert and Weston,

03:10.520 --> 03:13.360
where they had this idea of essentially taking

03:13.360 --> 03:18.240
a piece of text, corrupting it in some ways.

03:18.680 --> 03:22.040
In modern versions, it consists in removing some words

03:22.040 --> 03:27.440
from the text, and then training some giant neural net

03:27.440 --> 03:30.240
to predict the words that are missing,

03:30.240 --> 03:35.160
or just merely to tell you whether the text that is here

03:35.160 --> 03:37.240
is legit or not legit.

03:37.240 --> 03:39.480
That's a different way of doing it.

03:39.480 --> 03:42.280
So this is how every modern NLP system

03:42.280 --> 03:44.840
over the last four or five years has been trained.

03:44.840 --> 03:48.600
And that has completely revolutionized not just

03:48.600 --> 03:50.800
the research in NLP, but also the practice of it.

03:50.800 --> 03:55.160
So all of translation, content moderation,

03:55.160 --> 03:58.440
hate speech detection, all that stuff from social networks,

03:58.440 --> 04:00.880
it all uses this kind of stuff.

04:00.880 --> 04:04.280
And performance went up by a huge amount.

04:04.280 --> 04:10.880
OK, so a special case of this is generative LLMs.

04:11.760 --> 04:16.000
And similar things are used in images and video.

04:16.000 --> 04:19.160
And there, the part of the text that you're hiding

04:19.160 --> 04:20.240
is just the last word.

04:20.240 --> 04:22.720
So you train a giant neural net to just predict

04:22.720 --> 04:25.400
the last word in a sequence.

04:25.400 --> 04:28.200
And then you can use this to produce outputs

04:28.200 --> 04:32.320
auto-regressively, which means you give a window of text,

04:32.320 --> 04:34.360
you get a system to produce a word,

04:34.360 --> 04:37.360
and then you shift that word into the input

04:37.360 --> 04:39.400
by shifting everything by one.

04:39.400 --> 04:41.400
Predict the next, next word, shift that in.

04:41.400 --> 04:44.080
Predict the next, next, next word, shift that in, et cetera.

04:44.080 --> 04:46.160
That's auto-regressive prediction.

04:46.160 --> 04:48.160
It's a major flaw with this approach.

04:48.160 --> 04:52.640
This is how every single LLM today works.

04:52.640 --> 04:55.120
But we should call them auto-regressive LLMs,

04:55.120 --> 04:58.440
because I think future LLMs are not going to be like this.

04:58.440 --> 05:01.160
But basically, every single one of them, some of which

05:01.160 --> 05:03.680
you've probably never heard of.

05:03.680 --> 05:07.000
So the ones from Faire, Blenderbot, Galactica, Lama,

05:07.000 --> 05:09.520
Alpeca, which is fine tuning of Lama.

05:09.520 --> 05:13.160
There is a new one now also.

05:13.160 --> 05:16.120
Lambda, Bard from Google, Shinshila from DeepMind,

05:16.120 --> 05:18.320
Chai GPT, GPT4, et cetera.

05:18.320 --> 05:22.280
They're all auto-regressive LLMs.

05:22.280 --> 05:26.120
And they train on gigantic amounts of data.

05:26.120 --> 05:28.840
So we're talking one trillion tokens or something like this.

05:28.840 --> 05:31.480
It would take a human reading eight hours a day,

05:31.480 --> 05:34.400
something like 22,000 years to read this.

05:34.400 --> 05:36.800
So obviously, those things can swallow a lot more

05:36.800 --> 05:39.560
and digest a lot more data than any human.

05:39.560 --> 05:42.400
And the performance is nothing short of amazing.

05:42.400 --> 05:44.360
But they do make stupid mistakes.

05:44.360 --> 05:45.880
They are extremely fluent.

05:45.880 --> 05:48.720
So we can use them to generate text.

05:48.720 --> 05:55.200
But they make factual errors, logical errors, inconsistencies.

05:55.200 --> 05:56.760
They have limited reasoning ability.

05:56.760 --> 05:59.800
There is no way to control for things like toxicity

05:59.800 --> 06:01.560
and stuff like that.

06:01.560 --> 06:04.720
And they really have no knowledge of the underlying reality,

06:04.720 --> 06:08.440
except in one case, because, of course, they only

06:08.440 --> 06:10.640
train from text, except in one case.

06:10.640 --> 06:14.960
And that case is code generation.

06:14.960 --> 06:17.480
And they work really, really well for code generation.

06:17.480 --> 06:20.000
And the reason they work well is that the underlying reality

06:20.000 --> 06:21.160
of code is very simple.

06:21.160 --> 06:21.960
It's deterministic.

06:21.960 --> 06:24.360
It's just the state of variables of a program.

06:24.360 --> 06:27.560
And so that's fully observable, deterministic,

06:27.560 --> 06:28.080
and everything.

06:28.080 --> 06:31.680
So it works really well.

06:31.680 --> 06:34.200
And they can generate fluent text.

06:34.200 --> 06:36.560
But in this particular case, this is a joke

06:36.560 --> 06:39.280
that my colleagues did on me.

06:39.280 --> 06:41.640
It's completely made up.

06:41.640 --> 06:46.800
I never actually did a rap album.

06:46.800 --> 06:48.480
Raw personal.

06:48.480 --> 06:51.480
Yeah.

06:51.480 --> 06:54.000
I asked them if they, I don't actually like rap that well.

06:54.000 --> 06:56.520
So I'm over a jazz person.

06:56.520 --> 06:58.400
So I asked them to do the same thing with jazz.

06:58.400 --> 07:02.480
And they say, there's not enough training data.

07:02.480 --> 07:05.480
And I cried.

07:05.480 --> 07:06.720
OK, so what are they good for?

07:06.720 --> 07:09.320
They're good for writing assistance, generating

07:09.320 --> 07:12.800
first draft, producing a style.

07:12.800 --> 07:15.800
Code writing assistance, obviously, very efficient for that.

07:15.800 --> 07:18.920
They're not good for producing factual and consistent answers

07:18.920 --> 07:21.520
because of aducinations.

07:21.520 --> 07:23.360
And they're not good for taking into account

07:23.360 --> 07:24.800
recent information, because you need

07:24.800 --> 07:27.920
to retrain the entire system to take into account yesterday

07:27.920 --> 07:29.080
in real time.

07:29.080 --> 07:32.560
And that's just not practical.

07:32.560 --> 07:35.800
They don't behave properly, or at least they're

07:35.800 --> 07:37.680
hard to control to do so.

07:37.680 --> 07:38.760
They don't do reasoning.

07:38.760 --> 07:39.520
They don't do planning.

07:39.520 --> 07:42.880
They don't do math, as we saw this morning.

07:42.880 --> 07:44.440
They're being modified to use tools,

07:44.440 --> 07:47.440
such as search engines, calculators, stuff like that.

07:47.440 --> 07:54.120
But currently, it's kind of like using duct tape and staples.

07:54.120 --> 07:56.360
And we're easily fooled by their fluency into thinking

07:56.360 --> 07:59.600
that they are smart, but they are not that smart.

07:59.600 --> 08:02.080
Now, there is a major flaw with this autoregressive

08:02.080 --> 08:05.560
generation, which is that it's an exponentially

08:05.560 --> 08:07.360
diverging diffusion process.

08:07.360 --> 08:10.840
So if there is the probability, e,

08:10.840 --> 08:13.920
for every token that is produced to be outside

08:13.920 --> 08:15.560
of the set of correct answers, let's

08:15.560 --> 08:18.440
assume that errors are being independent,

08:18.440 --> 08:20.440
then if we generate a sequence of n tokens,

08:20.440 --> 08:23.440
the probability for that sequence to be correct

08:23.440 --> 08:25.080
is 1 minus e to the power n.

08:25.080 --> 08:28.480
And that decreases exponentially.

08:28.480 --> 08:30.440
So those things just don't work.

08:30.440 --> 08:31.840
They just don't work.

08:31.840 --> 08:33.960
My prediction is that five years from now,

08:33.960 --> 08:37.840
nobody in that right mind would be using autoregressive LNMs.

08:37.840 --> 08:41.000
It's just a bad phase.

08:41.000 --> 08:41.960
They are useful, though.

08:41.960 --> 08:42.600
They're very useful.

08:45.360 --> 08:47.600
So as I said, they have a constant number

08:47.600 --> 08:49.640
of computational steps between input and output

08:49.640 --> 08:52.480
for each token generated.

08:52.480 --> 08:54.880
They do not reason and plan.

08:54.920 --> 08:57.920
Jake Browning, who will be talking Wednesday,

08:57.920 --> 09:00.280
and I wrote a philosophy paper.

09:00.280 --> 09:04.560
I mean, he wrote it on the fact that there

09:04.560 --> 09:09.320
are limitations to the purported intelligence

09:09.320 --> 09:12.240
of systems that are purely trained from text,

09:12.240 --> 09:17.400
because I would argue that most of human knowledge

09:17.400 --> 09:18.800
is not textual.

09:18.800 --> 09:20.360
I mean, certainly most of what babies

09:21.320 --> 09:25.160
are on before six months is non-textual.

09:25.160 --> 09:27.720
And everything that animals learn is non-textual.

09:27.720 --> 09:34.440
So that knowledge is still unattainable to current AI

09:34.440 --> 09:36.120
systems.

09:36.120 --> 09:39.080
So how do we get machines to understand how the world works

09:39.080 --> 09:41.240
and predict the consequences of their actions?

09:41.240 --> 09:43.000
All the limitations have been pointed out

09:43.000 --> 09:47.560
by a number of different papers, including one

09:47.560 --> 09:52.840
by the MIT crowd, that fluency is really not

09:52.840 --> 09:57.040
the same as thinking.

09:57.040 --> 09:59.080
And basically, you could argue for the fact

09:59.080 --> 10:02.520
that what LLMs are good for is perhaps modeling

10:02.520 --> 10:05.240
the Browning-Key and Boracay areas,

10:05.240 --> 10:07.040
but not much else in the brain.

10:07.040 --> 10:11.080
And that's like tiny little areas on the side of the brain.

10:14.240 --> 10:17.280
So we need something else.

10:17.320 --> 10:19.600
What are we missing?

10:19.600 --> 10:21.040
This is a chart that I like to show.

10:21.040 --> 10:23.040
Oops, the animation is bad.

10:23.040 --> 10:27.680
But it was put together by Emmanuel Dupu, who

10:27.680 --> 10:30.480
kind of tends to indicate at what age babies

10:30.480 --> 10:34.160
learn basic concepts, like object permanence, for example.

10:34.160 --> 10:36.840
Liz was talking about that.

10:36.840 --> 10:39.240
Stability and support and intuitive physics,

10:39.240 --> 10:44.400
which only comes up fairly late, actually, around nine months.

10:44.400 --> 10:46.400
And the question is, what type of learning

10:46.440 --> 10:48.040
is taking place there?

10:48.040 --> 10:51.360
No AI systems today really kind of does this properly,

10:51.360 --> 10:55.480
although there's been several attempts by a few of us.

10:55.480 --> 10:58.320
So I think perhaps it's this type of learning

10:58.320 --> 11:00.400
that is the basis of common sense.

11:00.400 --> 11:02.920
And we should really try to figure out

11:02.920 --> 11:05.520
how to reproduce this with machines.

11:05.520 --> 11:07.840
So I think there's three challenges for AI research

11:07.840 --> 11:10.560
today, learning representations and predictive models

11:10.560 --> 11:14.920
of the world, allowing machines to predict what's going to happen,

11:14.960 --> 11:17.000
perhaps as a consequence of their actions.

11:17.000 --> 11:18.040
Learning to reason.

11:18.040 --> 11:21.480
So this is more like Daniel Kahneman's System 2.

11:21.480 --> 11:24.440
Current autoregressive LLMs are basically System 1.

11:24.440 --> 11:26.600
They just view one word after the other

11:26.600 --> 11:30.680
without really planning ahead.

11:30.680 --> 11:33.800
And so that is the question of making reasoning compatible

11:33.800 --> 11:36.240
with learning.

11:36.240 --> 11:37.920
Josh has a particular proposal for this,

11:37.920 --> 11:44.520
which I don't agree with, but that goes in the right direction.

11:44.560 --> 11:47.600
And then learning to plan complex action sequences.

11:47.600 --> 11:51.400
So I made a proposal for this almost a year ago now,

11:51.400 --> 11:55.560
which I posted on this website so people can make comments

11:55.560 --> 12:00.720
and tell me I'm wrong and which references I missed.

12:00.720 --> 12:03.560
I guess several technical talks about it as well.

12:03.560 --> 12:06.680
And basically it's sort of a modular organization

12:06.680 --> 12:11.640
of an AI system that would be capable of reasoning and planning.

12:11.640 --> 12:15.120
And I can't say that I've built it,

12:15.120 --> 12:18.040
but we're kind of building pieces of it.

12:18.040 --> 12:19.160
So it's composed.

12:19.160 --> 12:22.040
It's basically centered around the award model, which

12:22.040 --> 12:24.480
will allow the system to predict ahead

12:24.480 --> 12:27.440
what the consequences of its actions would be.

12:27.440 --> 12:32.400
And it has a cost module.

12:32.400 --> 12:35.840
Think of it as kind of visual ganglia stuff.

12:35.840 --> 12:37.440
And the only purpose of the system

12:37.440 --> 12:39.560
is to optimize that cost.

12:39.560 --> 12:41.520
Some of those costs are essentially

12:41.520 --> 12:44.320
intrinsic, hard-wired, immutable costs

12:44.320 --> 12:47.800
that sort of drive the basic behavior of the system.

12:47.800 --> 12:49.960
And some of them are trainable costs

12:49.960 --> 12:53.960
that the system learns as it goes.

12:53.960 --> 12:57.040
And what the system does is that it

12:57.040 --> 13:00.240
plans a sequence of actions that, according to its model,

13:00.240 --> 13:03.920
will minimize those costs.

13:03.920 --> 13:05.600
And of course, it needs to be able to estimate

13:05.600 --> 13:07.960
the current state of the world, which is done through perception

13:08.400 --> 13:10.000
and maybe access to a memory.

13:10.000 --> 13:13.440
And then depending on the task that the system is focusing on,

13:13.440 --> 13:15.440
it can be entirely configured by a configurator

13:15.440 --> 13:18.760
that will sort of focus the system on the task at hand.

13:18.760 --> 13:21.520
So that's a cognitive architecture, which

13:21.520 --> 13:24.440
some people in classical AI have been proposing,

13:24.440 --> 13:27.760
but in sort of different forms.

13:27.760 --> 13:29.160
And there's two ways to use it.

13:29.160 --> 13:33.400
Mode 1, which is just a reactive perception action cycle,

13:33.400 --> 13:35.120
get an idea of the state of the world,

13:35.120 --> 13:37.720
encoding into an abstract representation

13:37.720 --> 13:39.840
of the state of the world as 0, and then running

13:39.840 --> 13:42.760
through some other neural net that produces an action

13:42.760 --> 13:44.240
reactively.

13:44.240 --> 13:46.280
But the more interesting mode is mode 2,

13:46.280 --> 13:48.840
which is like Kettiman's System 2,

13:48.840 --> 13:51.640
where you make an estimate of the state of the world,

13:51.640 --> 13:54.840
and then using your word model, predict ahead of time

13:54.840 --> 13:57.640
what's going to happen according to an imagined sequence

13:57.640 --> 14:00.800
of actions that you might take.

14:00.800 --> 14:04.960
And then the agent would optimize that sequence of actions.

14:05.000 --> 14:08.000
So as to minimize a particular cost function,

14:08.000 --> 14:14.200
representing the tax to be fulfilled.

14:14.200 --> 14:16.520
And then it would just take the first action

14:16.520 --> 14:19.440
and actually send it to the actuator,

14:19.440 --> 14:21.240
or maybe the first few actions.

14:21.240 --> 14:23.440
So this is completely classical in optimal control.

14:23.440 --> 14:26.480
It's called model predictive control.

14:26.480 --> 14:30.480
Except the problem here is how you learn the model.

14:30.480 --> 14:32.680
There's a way to kind of turn System 2 into System 1,

14:32.720 --> 14:35.120
which I'm not going to go into.

14:35.120 --> 14:38.520
OK, so how do we train the world model?

14:38.520 --> 14:42.480
Only for the fact that the world is not deterministic

14:42.480 --> 14:46.040
or not entirely predictable, even if it is deterministic.

14:46.040 --> 14:50.240
So we're not going to have a neural net observe the input

14:50.240 --> 14:54.640
and just predict why, and then minimizing a prediction error.

14:54.640 --> 14:56.240
That's not going to work, because that can only

14:56.240 --> 14:58.120
make one prediction.

14:58.120 --> 15:00.920
So in fact, if you train a big neural net

15:00.920 --> 15:05.400
to predict like these are cars from a top-down view

15:05.400 --> 15:07.040
of a highway, if you train a neural net

15:07.040 --> 15:09.200
to try to predict what's going to happen in this video,

15:09.200 --> 15:11.720
you get blurry predictions.

15:11.720 --> 15:14.120
Because the system cannot predict if a particular car is

15:14.120 --> 15:17.880
going to break or accelerate or turn left or right,

15:17.880 --> 15:20.720
and so it makes these blurry predictions.

15:20.720 --> 15:22.040
Same for a natural video.

15:22.040 --> 15:26.920
That's an old work on video prediction.

15:26.920 --> 15:29.720
So you have to account for the fact

15:29.720 --> 15:32.680
that the world is not completely predictable.

15:32.680 --> 15:35.120
And you have two solutions there.

15:35.120 --> 15:37.280
Either you build an architecture with latent variables

15:37.280 --> 15:41.320
that parameterizes the set of possible predictions,

15:41.320 --> 15:44.960
or, and those two are not incompatible,

15:44.960 --> 15:47.240
or you abandon the idea that you're

15:47.240 --> 15:49.400
going to predict everything about the world.

15:49.400 --> 15:51.440
And so this is what I'm suggesting.

15:51.440 --> 15:54.080
So this is a generative architecture.

15:54.080 --> 15:57.280
Generative architecture observes x, encodes it,

15:57.280 --> 16:00.880
then predicts y, the variable whose dependency you're

16:00.880 --> 16:02.600
trying to predict.

16:02.600 --> 16:04.760
And then you measure the prediction error.

16:04.760 --> 16:07.800
You mean my side by training, et cetera.

16:07.800 --> 16:11.360
What I'm proposing is a joint embedding architecture

16:11.360 --> 16:15.840
where both x and y go through encoders, neural nets,

16:15.840 --> 16:19.400
and the prediction takes place in representation space.

16:19.400 --> 16:22.320
What that allows the system to do is basically

16:22.320 --> 16:25.640
eliminate a lot of irrelevant information from y

16:25.640 --> 16:29.000
when it encodes it into SY so that it doesn't

16:29.000 --> 16:32.000
have to predict all the details.

16:32.000 --> 16:35.400
So there's a lot of things here and a lot of information

16:35.400 --> 16:39.360
in this room that we cannot possibly remember or predict

16:39.360 --> 16:44.680
the precise texture of the wood on the floor, things like that.

16:44.680 --> 16:47.320
But it's kind of irrelevant.

16:47.320 --> 16:50.800
We only need to have sort of a relatively abstract

16:50.800 --> 16:52.360
representation of it.

16:52.360 --> 16:53.920
So I'm basically recommending to abandon

16:53.920 --> 16:56.720
the whole idea of generative models,

16:56.720 --> 16:59.400
unless you want to produce pictures or produce text.

16:59.400 --> 17:01.360
But if you want to learn how the world works,

17:01.360 --> 17:03.720
you should not reconstruct.

17:03.720 --> 17:05.000
There's actually several versions

17:05.000 --> 17:07.520
of those joint embedding architectures,

17:07.520 --> 17:12.640
the simple one, deterministic ones that can predict,

17:12.640 --> 17:14.200
and then nondeterministic ones that

17:14.200 --> 17:19.440
can predict where the predictor can have latent variables.

17:19.440 --> 17:22.720
So that's kind of the most general architecture.

17:22.720 --> 17:28.400
And the latent variable A here can be a latent variable you

17:28.400 --> 17:30.200
infer or it could be an action.

17:30.200 --> 17:32.080
So imagine that this is a world model.

17:32.080 --> 17:33.520
This is the current state of the world

17:33.520 --> 17:35.640
that you observe, you encode.

17:35.640 --> 17:37.600
This is an action you might take in the world,

17:37.600 --> 17:39.920
maybe combined with some latent variable which

17:39.920 --> 17:42.120
represent what you don't know about the world.

17:42.120 --> 17:45.560
And then you make a prediction, and then

17:45.560 --> 17:47.760
you can compare that prediction with what actually occurs

17:47.760 --> 17:49.960
if you want to train your model.

17:49.960 --> 17:51.320
And that's a predictive model that

17:51.320 --> 17:53.440
will allow you to predict what's going

17:53.440 --> 17:55.920
to happen as a consequence of your actions.

17:55.920 --> 18:00.080
Now, because we're not generating anything,

18:00.080 --> 18:02.360
and because we can't turn a model of this type

18:02.360 --> 18:05.480
into a probabilistic model of t of y given x,

18:05.480 --> 18:08.600
we have to abandon the whole idea of probabilistic modeling.

18:08.600 --> 18:13.120
And now Josh is going, oh my god.

18:13.120 --> 18:15.320
Isn't it just approximate probability at that point?

18:15.320 --> 18:15.960
Isn't it?

18:15.960 --> 18:16.460
No.

18:16.460 --> 18:16.960
No?

18:16.960 --> 18:18.840
No.

18:18.840 --> 18:19.960
It's energies, OK?

18:20.000 --> 18:24.320
So basically the name of the game here

18:24.320 --> 18:27.960
is that you need to understand the system as computing

18:27.960 --> 18:30.960
an energy function that captures the dependency between x and y.

18:30.960 --> 18:35.880
So imagine the data points are those black spheres.

18:35.880 --> 18:38.160
The energy function should take low values around the black

18:38.160 --> 18:40.880
spheres and higher values outside.

18:40.880 --> 18:44.640
And whether this energy function represents

18:44.640 --> 18:49.480
the unnormalized log of some probability,

18:49.480 --> 18:51.840
is irrelevant, you just want the energy

18:51.840 --> 18:54.320
to be higher outside of the manifold of data.

18:54.320 --> 18:57.200
And it will have captured the dependency between the variables.

18:57.200 --> 19:00.120
And there's nothing more you need.

19:00.120 --> 19:02.600
Now, the next question is, how do you train a system

19:02.600 --> 19:06.240
to give low energy to stuff you observe and high energy

19:06.240 --> 19:07.400
to stuff you don't observe?

19:07.400 --> 19:09.600
And there are two methods, contrastive methods,

19:09.600 --> 19:12.840
which consist in generating fake contrastive points whose

19:12.840 --> 19:14.320
energy is going to push up.

19:14.320 --> 19:15.640
And then regularized methods, which

19:15.640 --> 19:17.240
I'm going to explain in a second.

19:17.240 --> 19:21.120
So let's say you have training samples.

19:21.120 --> 19:22.800
Your system currently gives low energy

19:22.800 --> 19:26.640
to this sort of peak area here.

19:26.640 --> 19:30.720
And it's not a good model of the data here,

19:30.720 --> 19:33.280
because it gives high energy to data points and low energy

19:33.280 --> 19:36.080
to areas that have no points.

19:36.080 --> 19:39.000
So what you can do is generate green points here whose energy

19:39.000 --> 19:40.720
you're going to push up.

19:40.720 --> 19:44.080
And the energy function is going to take the right shape.

19:44.080 --> 19:46.720
Or you could use some sort of regularizer

19:46.720 --> 19:50.440
that minimizes the volume of space that can take low energy.

19:50.440 --> 19:52.720
So that whenever you push down on the energy of some regions,

19:52.720 --> 19:54.200
the rest has to go up, because there

19:54.200 --> 19:59.920
is a limited amount of volume that can take low energy.

19:59.920 --> 20:03.880
So in the context of joint embedding architecture,

20:03.880 --> 20:05.680
I kind of invented the contrastive methods.

20:05.680 --> 20:07.840
That's called sine is net in the old days.

20:07.840 --> 20:13.520
But I'm now arguing against that in favor of regularized methods.

20:13.520 --> 20:15.520
And the big question is, how do we train them?

20:15.560 --> 20:17.640
I'll tell you about that in a minute.

20:17.640 --> 20:19.720
But I'm asking you to abandon generative models,

20:19.720 --> 20:23.440
abandon probabilistic models, probabilistic modeling in general,

20:23.440 --> 20:24.960
abandon contrastive methods.

20:24.960 --> 20:26.480
And of course, abandon reinforcement learning.

20:26.480 --> 20:31.240
But that, I've been saying this for years.

20:31.240 --> 20:34.560
Those are four of the main pillars of machine learning.

20:34.560 --> 20:40.200
That makes me super popular among my colleagues.

20:40.200 --> 20:42.040
OK, so what are those regularized methods

20:42.040 --> 20:44.040
for joint embedding architectures?

20:44.080 --> 20:46.360
So essentially, there is a big issue

20:46.360 --> 20:50.360
that you have to fix, which is that when you train a system

20:50.360 --> 20:52.360
like this, one of those JEPA architecture,

20:52.360 --> 20:54.720
joint embedding predictive architectures,

20:54.720 --> 20:56.840
you show it an example of x and y.

20:56.840 --> 21:00.040
And you tell it just train all the weights of all those neural

21:00.040 --> 21:04.640
nets so as to minimize the prediction error, it collapses.

21:04.640 --> 21:07.280
Basically, what it says is that, well, I can just

21:07.280 --> 21:12.800
set Sx and Sy to constants and set the prediction,

21:12.840 --> 21:15.520
set the predictor to some constant thing

21:15.520 --> 21:17.640
and ignore x and y all together.

21:17.640 --> 21:20.200
And that would be a collapse system that gives zero energy

21:20.200 --> 21:22.120
to everything in your space.

21:22.120 --> 21:23.560
You have to prevent that from happening.

21:23.560 --> 21:25.560
And one way to prevent that from happening

21:25.560 --> 21:29.600
is finding a way to maximize the information content

21:29.600 --> 21:34.120
of the representations that come out of the encoders.

21:34.120 --> 21:37.120
That actually has the effect of minimizing the volume of stuff

21:37.120 --> 21:42.520
that can take your energy indirectly.

21:42.520 --> 21:47.240
So one way to prevent the outputs from being constant

21:47.240 --> 21:50.160
is that you can force the variance to be non-zero.

21:50.160 --> 21:52.840
So you put a cost function on top of this vector here

21:52.840 --> 21:55.160
that says, over a batch of samples,

21:55.160 --> 21:58.760
I want the variance of each variable coming out of that neural

21:58.760 --> 22:00.880
net to be non-zero, to be above one, let's say.

22:00.880 --> 22:02.840
So that's a hinge loss that says the variance

22:02.840 --> 22:04.320
needs to be above one.

22:04.320 --> 22:05.800
It's not enough because the system

22:05.800 --> 22:10.640
can still cheat by making all the variables the same

22:10.640 --> 22:12.280
or very highly correlated.

22:12.280 --> 22:13.800
So you have another cost that says,

22:13.800 --> 22:16.440
I want them to be decorrelated.

22:16.440 --> 22:18.160
So basically, this has the effect

22:18.160 --> 22:21.920
of enforcing the covariance matrix of that those Sx

22:21.920 --> 22:26.840
vectors over a batch to be close to the identity.

22:26.840 --> 22:29.440
And it's not enough because the variables

22:29.440 --> 22:34.240
can be non-collapsed and correlated

22:34.240 --> 22:36.240
but still dependent.

22:36.240 --> 22:37.920
And so there's another trick that we do,

22:37.920 --> 22:41.360
and we have some theory that shows that it's not stupid,

22:41.360 --> 22:42.760
which is that you take the Sx vector,

22:42.760 --> 22:45.280
you run it to some neural net that expands the dimension,

22:45.280 --> 22:47.960
and then you apply those criteria on the covariance

22:47.960 --> 22:49.080
matrix to the output.

22:49.080 --> 22:52.680
And that makes the variables of Sx kind of more independent.

22:52.680 --> 22:55.240
Now, there's a major flaw with this, which is,

22:55.240 --> 22:57.920
and that's the theory which I'm not going to talk about.

22:57.920 --> 23:01.200
There's a flaw with all of this, which is that we're basically,

23:01.200 --> 23:03.120
we have an upper bound on information content,

23:03.120 --> 23:05.960
and we're pushing it up, hoping that the actual information

23:05.960 --> 23:08.080
content will follow.

23:08.080 --> 23:09.680
And it's stupid, but it kind of works.

23:11.600 --> 23:14.240
OK, so you can test those pre-training

23:14.240 --> 23:15.240
for image recognition.

23:15.240 --> 23:18.080
For example, you show two different views of the same image,

23:18.080 --> 23:21.440
train the network to produce identical representations

23:21.440 --> 23:23.440
for two different views of the same image,

23:23.440 --> 23:27.240
and then you freeze the network and basically train

23:27.240 --> 23:29.640
a linear classifier on top with ImageNet

23:29.640 --> 23:31.120
and measure the performance.

23:31.120 --> 23:36.240
And this Vcrag method that I just described

23:36.240 --> 23:41.200
works just as well as isn't the top pack, let's say.

23:41.240 --> 23:42.560
There's a bunch of different methods

23:42.560 --> 23:44.440
that have similar performance.

23:44.440 --> 23:45.800
And they are in the top pack.

23:45.800 --> 23:47.160
I'm not going to bore you with details.

23:47.160 --> 23:49.600
You can try to do segmentation as well.

23:49.600 --> 23:51.720
Here's another method, somewhat similar,

23:51.720 --> 23:54.880
but closer to the JEPA idea, which

23:54.880 --> 23:57.200
uses a different criterion to prevent collapse, which

23:57.200 --> 23:58.680
I'm not going to explain.

23:58.680 --> 24:03.120
And this one takes a partially masked input image

24:03.120 --> 24:05.440
together with a full input image, runs both of them

24:05.440 --> 24:08.440
through encoders, and then trains a predictor

24:08.440 --> 24:13.280
to basically predict the representation of the full image

24:13.280 --> 24:16.240
from the representation computed

24:16.240 --> 24:19.800
from the partially masked image.

24:19.800 --> 24:21.800
This is called IJEPA, ImageJEPA.

24:21.800 --> 24:23.000
And it works amazingly well.

24:23.000 --> 24:29.320
And it's really fast to train very good performance.

24:29.320 --> 24:34.200
In terms of performance, even though this type of masking

24:34.200 --> 24:36.680
does not require any knowledge about the nature of the input,

24:36.720 --> 24:39.760
essentially, or very little, the still

24:39.760 --> 24:41.040
you get the same kind of performance

24:41.040 --> 24:44.760
that you would get if you used a self-supervised learning

24:44.760 --> 24:47.800
method that exploits the fact that you're

24:47.800 --> 24:51.680
doing image recognition, like Dino or Ibot or Simclear,

24:51.680 --> 24:52.200
for example.

24:55.080 --> 24:59.480
OK, now, how are you going to use this in the end?

24:59.480 --> 25:05.120
What I'm really interested in is to use JEPAs as world models

25:05.120 --> 25:06.040
inside of the system.

25:06.040 --> 25:08.320
They can do system two type planning,

25:08.320 --> 25:12.520
but even better than this, they can do hierarchical planning.

25:12.520 --> 25:18.920
And the idea there is that when you think about a task,

25:18.920 --> 25:22.720
you're not planning this task at the lowest level

25:22.720 --> 25:26.720
in terms of millisecond by millisecond muscle control.

25:26.720 --> 25:28.200
You're playing a task like, I want

25:28.200 --> 25:31.640
to go from Santa Fe to New York, or let's say

25:31.640 --> 25:35.400
from New York to Santa Fe, that's a better example.

25:35.400 --> 25:38.920
So you first decompose this into two sub-tasks.

25:38.920 --> 25:41.200
First thing I need to do is go to the airport

25:41.200 --> 25:43.200
and catch a plane.

25:43.200 --> 25:44.280
How do I go to the airport?

25:44.280 --> 25:45.360
Well, to go to the airport, I need

25:45.360 --> 25:46.960
to get on the street and have a taxi,

25:46.960 --> 25:51.040
which you can do in New York City, not in Santa Fe.

25:51.040 --> 25:52.360
How do I get down in the street?

25:52.360 --> 25:55.880
I need to get out of the building I'm in, et cetera.

25:55.880 --> 25:57.040
How do I get out of the building?

25:57.040 --> 25:59.560
I need to stand up from my chair, walk to the door.

25:59.560 --> 26:01.240
How do I get up from my chair?

26:01.240 --> 26:03.000
So you kind of decompose this all the way down

26:03.000 --> 26:06.320
to the lowest level millisecond muscle control.

26:06.320 --> 26:10.720
But you're not going to plan the entire task of going

26:10.720 --> 26:16.720
from New York to Santa Fe all the way down to millisecond

26:16.720 --> 26:17.960
by millisecond muscle control.

26:17.960 --> 26:20.280
You do a hierarchical planning.

26:20.280 --> 26:22.760
We think humans, that we are the only ones who can do this.

26:22.760 --> 26:24.680
Animals do this, too.

26:24.680 --> 26:26.720
You observe the cat planning a trajectory

26:26.720 --> 26:29.480
to jump on a piece of furniture.

26:29.520 --> 26:31.440
They definitely do a hierarchical planning.

26:31.440 --> 26:34.280
So basically, what you do, what you need for this

26:34.280 --> 26:39.200
is a sort of hierarchy of JPA architectures of predictors

26:39.200 --> 26:41.760
that progressively produce more and more

26:41.760 --> 26:44.840
abstract representations of the state of the world,

26:44.840 --> 26:50.720
so that in the very abstract space of representations,

26:50.720 --> 26:52.120
you can make long-term predictions.

26:52.120 --> 26:55.560
Whereas in the sort of lower levels of abstraction,

26:55.560 --> 26:57.640
you can make shorter term prediction,

26:57.640 --> 26:59.960
but they're more accurate in the short term.

26:59.960 --> 27:02.200
So this is a two-level architecture.

27:02.200 --> 27:04.240
Low-level, you can make short-term predictions.

27:04.240 --> 27:06.240
High-level, you can make longer-term prediction

27:06.240 --> 27:08.800
in a more abstract space that has less details

27:08.800 --> 27:11.200
about how the world works.

27:11.200 --> 27:16.000
Now, we've been able to train a particular instance of JPA

27:16.000 --> 27:18.080
that simultaneously learns teachers that

27:18.080 --> 27:22.240
are good for image recognition and motion prediction

27:22.240 --> 27:23.680
in images.

27:23.680 --> 27:25.920
And I'm not going to go into the details of how

27:25.920 --> 27:26.920
this is pretty hairy.

27:26.920 --> 27:29.160
But it's kind of hierarchical.

27:29.160 --> 27:34.840
And it's got predictors that make pretty strong assumptions

27:34.840 --> 27:37.680
about the type of prediction that can occur.

27:37.680 --> 27:39.840
And simultaneously learns invariant features

27:39.840 --> 27:42.040
for image recognition.

27:42.040 --> 27:45.560
And this works really well for things like image segmentation,

27:45.560 --> 27:49.000
depth estimation, tracking, et cetera.

27:49.000 --> 27:55.320
It's called MCJPA, which means motion and content.

27:55.760 --> 27:58.760
And with this, hopefully, one day,

27:58.760 --> 28:01.680
we'll be able to build architectures that

28:01.680 --> 28:03.760
can perform hierarchical tanning of the type that I

28:03.760 --> 28:05.800
was telling you about.

28:05.800 --> 28:11.680
So observe the world, compute the abstract representation,

28:11.680 --> 28:13.360
and even more abstract representation,

28:13.360 --> 28:15.480
even more abstract representation,

28:15.480 --> 28:18.120
make a prediction to minimize a particular cost

28:18.120 --> 28:20.560
function that defines your task.

28:20.560 --> 28:22.640
I'm assuming this cost function is differentiable,

28:22.640 --> 28:26.560
so we can do this inference by gradient descent.

28:26.560 --> 28:27.880
In first, some latent variable that

28:27.880 --> 28:29.920
may represent the macro action you're going to take,

28:29.920 --> 28:33.720
or some unknown variable about the world.

28:33.720 --> 28:35.520
And then the state you're going to obtain

28:35.520 --> 28:36.960
through the first prediction is going

28:36.960 --> 28:40.480
to constitute a cost function for the lowest level.

28:40.480 --> 28:45.440
So the first predictor at the top tells me

28:45.440 --> 28:47.320
I should be at the airport.

28:47.320 --> 28:48.760
I started from New York.

28:48.760 --> 28:50.160
I should be at the airport.

28:50.200 --> 28:54.360
The cost function below measures how far I am from the airport.

28:54.360 --> 28:57.640
And so the second predictor says, go down in the street.

28:57.640 --> 29:00.360
Take a cab to the airport.

29:00.360 --> 29:03.520
And so the cost function at the bottom here

29:03.520 --> 29:06.120
says, am I on the street?

29:06.120 --> 29:08.960
Likely to catch a taxi, and all the way down

29:08.960 --> 29:12.520
to the actual actions that you can take in the real world.

29:12.520 --> 29:16.120
All right, coming to the conclusion.

29:16.160 --> 29:20.680
So steps towards autonomous AI systems.

29:20.680 --> 29:21.600
Self-supervised learning.

29:21.600 --> 29:24.240
We need a recipe that allows us to train systems

29:24.240 --> 29:25.960
to learn how the world works on video.

29:25.960 --> 29:28.440
I can't claim that we have achieved this.

29:28.440 --> 29:32.120
We're kind of partially there.

29:32.120 --> 29:34.400
And legal uncertainty in the prediction,

29:34.400 --> 29:38.840
and that's with a combination of this JEPA architecture

29:38.840 --> 29:42.520
understood within the context of energy-based model,

29:42.520 --> 29:47.480
potentially with latent variables, which I didn't talk about.

29:47.480 --> 29:50.120
That would allow us to learn world models from observation,

29:50.120 --> 29:53.200
hopefully hierarchical world models, possibly

29:53.200 --> 29:56.920
with interaction as well, and exploration.

29:56.920 --> 30:00.320
And now what we have is an architecture capable of reasoning

30:00.320 --> 30:00.960
and planning.

30:00.960 --> 30:03.080
I mean, the whole architecture I presented

30:03.080 --> 30:05.240
is kind of this idea of system two,

30:05.240 --> 30:07.920
that you can decompose complex tasks into simpler ones,

30:07.920 --> 30:09.360
and then plan a sequence of actions

30:09.360 --> 30:11.560
before you take the action.

30:11.600 --> 30:15.800
Something that's sort of missing from current autoregressive systems.

30:19.320 --> 30:27.880
So is this a potential path towards sort of human-level AI?

30:27.880 --> 30:30.160
Possibly yes, but it's certainly not for tomorrow.

30:30.160 --> 30:33.120
This is maybe a 10-year plan, maybe

30:33.120 --> 30:37.160
to get to cat-level intelligence or something like that.

30:37.160 --> 30:40.200
Now interestingly, those machines will have inevitably

30:40.240 --> 30:43.520
some sort of emotion consciousness.

30:43.520 --> 30:45.840
Forget about this, but emotions certainly,

30:45.840 --> 30:50.560
because emotions are kind of an anticipation of outcome, most of them.

30:50.560 --> 30:55.760
I mean, some of them are immediate perception of outcome,

30:55.760 --> 30:57.360
like pain and things like that.

30:57.360 --> 30:59.400
But most of them are anticipation of outcome,

30:59.400 --> 31:02.320
and this cost function is exactly what this is.

31:02.320 --> 31:06.640
And so if the system sort of predicts a particular set of outcome

31:06.640 --> 31:15.920
that results in a bad outcome, it might feel something similar

31:15.920 --> 31:17.200
to fear or something of that type.

31:19.960 --> 31:25.640
Anyway, so common sense is a collection of world models,

31:25.640 --> 31:29.000
or perhaps a single world model that is configurable.

31:29.000 --> 31:32.320
I'll come to this in one second.

31:32.320 --> 31:35.920
Understanding really means being able to predict.

31:35.920 --> 31:39.080
I think prediction is really the essence of intelligence here,

31:39.080 --> 31:44.440
and better mental models need to better understanding,

31:44.440 --> 31:49.880
or other substrate, if you want, of understanding.

31:49.880 --> 31:55.680
And as a consequence, also of good reasoning and action planning.

31:55.680 --> 31:58.240
The complex part in all of this is going

31:58.240 --> 32:00.480
to be to design intrinsic cost functions that

32:00.480 --> 32:05.040
drive the system towards learning appropriate things.

32:05.080 --> 32:11.840
And it's quite possible that, in the case of leaving things,

32:11.840 --> 32:16.320
it's easier for evolution to hardwire your cost functions into us

32:16.320 --> 32:18.440
than to hardwire your behavior.

32:18.440 --> 32:23.320
Hardwiring behavior and physical models and whatever is super hard.

32:23.320 --> 32:27.000
Like, as a neural net person, I would have no idea

32:27.000 --> 32:29.400
how to architect neural nets to do this.

32:29.400 --> 32:33.280
But I can certainly design a cost function that, if minimized,

32:33.280 --> 32:37.120
the system will learn those basic concepts.

32:37.120 --> 32:42.240
And that, there is a lot of hardwiring in there, no question.

32:42.240 --> 32:44.760
So one module I didn't talk much about is the configurator.

32:44.760 --> 32:47.880
And what it's supposed to do is configure all the modules

32:47.880 --> 32:51.840
in this architecture for a particular sub-task

32:51.840 --> 32:55.760
that the system needs to be focusing on at the moment.

32:55.760 --> 32:57.880
And I'm imagining that there is actually

32:57.880 --> 33:01.400
a single world model engine in this architecture that

33:01.400 --> 33:04.320
is reconfigurable for the task at hand.

33:04.320 --> 33:07.680
But it's not like the system would have multiple world models

33:07.680 --> 33:08.960
for different situations.

33:08.960 --> 33:10.800
It's got a single one that's configurable.

33:10.800 --> 33:15.960
The advantage of doing this, I mean, for humans and animals,

33:15.960 --> 33:18.960
is that it might actually fit in your skull.

33:18.960 --> 33:22.320
But there is another algorithmic advantage,

33:22.320 --> 33:27.080
or epistemic advantage, which is that a single one model can

33:27.080 --> 33:30.120
share knowledge between different situations.

33:30.600 --> 33:34.080
Whereas if you had a separate world model for different situations,

33:34.080 --> 33:36.360
you would have to retrain it independently

33:36.360 --> 33:38.040
for each of those situations.

33:38.040 --> 33:40.800
So how to make this configurator work, I have no idea.

33:40.800 --> 33:43.840
But that's a good hypothesis.

33:43.840 --> 33:46.480
So that would explain the fact that there is a single world model.

33:46.480 --> 33:49.360
It would explain why humans and many animals

33:49.360 --> 33:53.120
can only focus on the single conscious task at any one time.

33:53.120 --> 33:57.760
Because we only have one world model.

33:57.800 --> 34:03.120
We can only do system two on one task at a time.

34:03.120 --> 34:05.040
And I just leave the question for.

