1
00:00:00,000 --> 00:00:06,720
This public lecture series.

2
00:00:06,720 --> 00:00:11,840
Many of the most cherished books in science, Einstein's relativity, Schrodinger's What

3
00:00:11,840 --> 00:00:17,820
Is Like, Richard Feynman's QED, were all based on public lectures.

4
00:00:17,820 --> 00:00:24,420
In this spirit, we want to welcome you here tonight, this public lecture.

5
00:00:24,420 --> 00:00:29,800
In the special series beginning tonight, the ULAM lectures, we bring a notable scientist

6
00:00:29,800 --> 00:00:36,000
to illuminate a cutting-edge topic in honor of the late theoretical mathematician Stanislaw

7
00:00:36,000 --> 00:00:37,560
Ulam.

8
00:00:37,560 --> 00:00:42,280
All of our public lectures are underwritten by the McKinnon Family Foundation, who allow

9
00:00:42,280 --> 00:00:47,080
us to continue to provide the lectures at no cost, and we want to take a moment to thank

10
00:00:47,080 --> 00:00:54,840
them first.

11
00:00:54,840 --> 00:00:59,160
We are additionally supported by the Santa Fe Reporter and this Lenzick Performing Arts

12
00:00:59,160 --> 00:01:05,680
Center.

13
00:01:05,680 --> 00:01:10,600
Stanislaw Ulam was long associated with Los Alamos National Laboratory, and is highly

14
00:01:10,600 --> 00:01:14,520
regarded by the Santa Fe Institute scientific community.

15
00:01:14,520 --> 00:01:19,840
Former SFI vice president Mike Simmons said, the enormous range of Ulam's scientific thought

16
00:01:19,840 --> 00:01:25,680
encompassed not only mathematics, but also physics, computation, biology, and much else.

17
00:01:25,680 --> 00:01:30,000
He would have been very much at home in the present day Santa Fe Institute.

18
00:01:30,000 --> 00:01:36,060
In this tradition, I am proud to introduce Ricard Soleil, our speaker tonight.

19
00:01:36,060 --> 00:01:41,640
Simply put, Ricard is one of the most abundant and interesting theorists alive.

20
00:01:41,640 --> 00:01:48,780
Ricard leads Icreia, the complex systems lab at the Universitat Pompeo Fabra in Barcelona.

21
00:01:48,780 --> 00:01:53,760
He is also an external faculty member here at the Santa Fe Institute.

22
00:01:53,760 --> 00:01:58,780
Ricard originally studied both physics and biology and undergraduate before achieving

23
00:01:58,780 --> 00:02:00,900
a Ph.D. in physics.

24
00:02:00,900 --> 00:02:05,900
His work touches on everything from how life originated to present day ecology to the very

25
00:02:05,900 --> 00:02:07,920
nature of thought.

26
00:02:07,920 --> 00:02:12,800
In the late 90s, Ricard connected ecology and mathematics by demonstrating the fractal

27
00:02:12,800 --> 00:02:18,040
structure of forest canopies, and that these structures emerge from the statistical dynamics

28
00:02:18,040 --> 00:02:20,060
of self-organization.

29
00:02:20,060 --> 00:02:25,860
This is a fundamental idea in ecology, and the full extent of the impact of these ideas

30
00:02:25,860 --> 00:02:29,940
are still being woven into our study and predictions of forests today.

31
00:02:29,940 --> 00:02:34,900
They are ideas that inform and inspire my own work constantly.

32
00:02:34,900 --> 00:02:40,540
Ricard has worked on other fundamental mathematical questions related to complex networks.

33
00:02:40,540 --> 00:02:45,540
More broadly, he is interested in how life works, spanning from how cells first originated

34
00:02:45,540 --> 00:02:51,200
in our past, to how brains developed to know and observe those same cells today, to how

35
00:02:51,200 --> 00:02:58,720
technological societies network brains to design new cells and engage in global bioengineering.

36
00:02:58,720 --> 00:03:03,480
And it is necessary to point out that Ricard's aggregate breadth of study is never at the

37
00:03:03,480 --> 00:03:05,360
expense of depth.

38
00:03:05,360 --> 00:03:10,560
This combination of expansive topics pursued with deep rigor is one of the rarest talents

39
00:03:10,560 --> 00:03:12,520
in science.

40
00:03:12,520 --> 00:03:16,860
In my own interactions with Ricard, we often discuss literature, and I am the fortunate

41
00:03:16,860 --> 00:03:23,020
recipient of many quotes from novels, biographies, and historical scientific works.

42
00:03:23,020 --> 00:03:27,980
The wonderful drawings that you are looking at tonight are made by the man himself.

43
00:03:27,980 --> 00:03:32,420
This shows that his imagination is constantly in motion, and it is a pleasure to discuss

44
00:03:32,420 --> 00:03:34,080
any topic with him.

45
00:03:34,080 --> 00:03:38,780
He is a true polymath and an ideal speaker for this lecture series.

46
00:03:38,780 --> 00:03:43,000
With that, please help me welcome Ricard Soleil.

47
00:03:43,000 --> 00:03:45,000
Hello.

48
00:03:45,000 --> 00:03:51,120
You hear me?

49
00:03:51,120 --> 00:03:55,040
Thanks for coming tonight.

50
00:03:55,040 --> 00:04:00,520
It's a real honor to be here delivering the Olam lectures this year.

51
00:04:00,520 --> 00:04:04,720
Thanks, Chris, for this great introduction.

52
00:04:05,080 --> 00:04:12,060
My talks have to do with a very exciting domain of research, which is mapping the space of

53
00:04:12,060 --> 00:04:14,020
cognitions.

54
00:04:14,020 --> 00:04:19,820
Not only the cognitions that we are familiar with, what I call the solid brains, but trying

55
00:04:19,820 --> 00:04:26,340
to understand what is there, what is what I call the cognitive biosphere, what is there,

56
00:04:26,340 --> 00:04:32,340
how much is complex, and whether or not we can understand the evolutionary dynamics that

57
00:04:32,360 --> 00:04:39,360
brings us, and if we can maybe go beyond evolution and try to engineer that complexity.

58
00:04:40,880 --> 00:04:45,520
So, before I go to that, since this is an Olam lecture, I want to say a little thing

59
00:04:45,520 --> 00:04:52,720
about that, because when I was in high school, I stumbled into this book, The Monte Carlo

60
00:04:52,720 --> 00:04:53,720
Method.

61
00:04:54,700 --> 00:05:04,700
It was a tiny book edited by the former Soviet Union in a series of little books with all

62
00:05:04,700 --> 00:05:11,200
kinds of things, where I discovered that Stan Olam, along with John von Neumann, invented

63
00:05:11,200 --> 00:05:17,220
this method that physicists use all the time to actually model complexity that has to do

64
00:05:17,220 --> 00:05:20,540
with some kind of randomness.

65
00:05:20,540 --> 00:05:23,880
And that brought me into play myself.

66
00:05:23,880 --> 00:05:31,040
It was the time of, there were no computers, so I'm old enough to say that, and so I had

67
00:05:31,040 --> 00:05:36,400
a calculator, I have a coin, and I did a model of a gas.

68
00:05:36,400 --> 00:05:43,960
You can see this is written with a typewriter, and I elaborated with that using that book.

69
00:05:43,960 --> 00:05:49,960
And one of the things that I learned is that there's a lot of power in being able to approach

70
00:05:49,960 --> 00:05:55,340
reality using, let's say, synthetic approximations.

71
00:05:55,340 --> 00:06:00,940
And the other thing I learned is that it's something about popularity.

72
00:06:00,940 --> 00:06:07,540
This series is called Popular Lessons in Mathematics.

73
00:06:07,540 --> 00:06:10,340
Did that make me popular at all?

74
00:06:10,340 --> 00:06:11,340
No?

75
00:06:11,720 --> 00:06:16,960
And I understood that popularity is not exactly that kind of stuff.

76
00:06:16,960 --> 00:06:23,760
But the other thing that I did connect is, I was mentioning Ulam and von Neumann, which

77
00:06:23,760 --> 00:06:25,800
you can see there on the left.

78
00:06:25,800 --> 00:06:29,040
In the middle we have Richard Feynman, another big name.

79
00:06:29,040 --> 00:06:32,080
It was important in my life.

80
00:06:32,080 --> 00:06:39,720
And von Neumann, and I will mention him today and tomorrow, brought me also into something

81
00:06:39,720 --> 00:06:40,720
else.

82
00:06:41,100 --> 00:06:46,420
This book by Michael Arby, Brains, Machines, and Mathematics, and again, I was still in

83
00:06:46,420 --> 00:06:47,420
high school.

84
00:06:47,420 --> 00:06:50,940
I found out that there was people thinking in the brains using mathematics that you could

85
00:06:50,940 --> 00:06:54,300
approach actually brain complexity using mathematics.

86
00:06:54,300 --> 00:06:58,060
I found out extremely fascinating.

87
00:06:58,060 --> 00:07:06,300
And over time, I got involved in trying to figure out how to solve some of the interesting

88
00:07:06,300 --> 00:07:07,300
questions.

89
00:07:08,280 --> 00:07:14,520
There are plenty of good things that we would like to understand from the question of why

90
00:07:14,520 --> 00:07:15,520
brains?

91
00:07:15,520 --> 00:07:22,400
Why brains is part of a more broad complex, a more general question on complexity?

92
00:07:22,400 --> 00:07:28,440
Because some people said that why the biosphere is not just made of microorganisms.

93
00:07:28,440 --> 00:07:29,440
They are cheap.

94
00:07:29,440 --> 00:07:30,440
They reproduce.

95
00:07:30,440 --> 00:07:33,040
They propagate and proliferate.

96
00:07:33,100 --> 00:07:40,580
Thinking that it adds complexity and brain is a lot of complexity with a cost seems to

97
00:07:40,580 --> 00:07:41,580
be superfluous.

98
00:07:41,580 --> 00:07:43,820
Why actually is any complexity?

99
00:07:43,820 --> 00:07:46,660
And brains are especially important.

100
00:07:46,660 --> 00:07:47,660
What kind of brains?

101
00:07:47,660 --> 00:07:51,700
That's one of the big questions we'll try to bring.

102
00:07:51,700 --> 00:07:53,380
Are there other minds?

103
00:07:53,380 --> 00:07:55,900
It's a very hot topic these days.

104
00:07:55,900 --> 00:07:59,820
What is a mind and how do we define and what we can find out?

105
00:07:59,820 --> 00:08:02,900
All the way up into the artificial.

106
00:08:02,900 --> 00:08:09,040
So whether or not beyond evolution, beyond what we see, beyond what we can infer about

107
00:08:09,040 --> 00:08:16,600
how nature happened to construct complexity, whether we can actually create new things.

108
00:08:16,600 --> 00:08:21,840
And that connects with the big question of the major evolutionary transitions, namely

109
00:08:21,840 --> 00:08:26,160
how the big innovation happens in the real world.

110
00:08:26,160 --> 00:08:32,600
Many years ago, Eosia Smadi and Don Minersmith made this list of transitions that had to

111
00:08:32,600 --> 00:08:38,540
do with the origins of cells, the origins of information, the code, et cetera, et cetera,

112
00:08:38,540 --> 00:08:42,020
all the way up to language.

113
00:08:42,020 --> 00:08:44,060
Of course, the list is more complex.

114
00:08:44,060 --> 00:08:48,100
You could ask yourself, what is the origin of consciousness?

115
00:08:48,100 --> 00:08:49,900
Why do we have consciousness?

116
00:08:49,900 --> 00:08:50,900
I will address that.

117
00:08:50,900 --> 00:08:56,700
I don't have an answer, which might be wrong, but you'll see.

118
00:08:56,700 --> 00:09:01,940
And in the middle of all this discussion about what is the nature of innovation and how innovations

119
00:09:01,960 --> 00:09:09,600
happen, is precisely this special thing of biology, the special status of information

120
00:09:09,600 --> 00:09:10,920
in biology.

121
00:09:10,920 --> 00:09:20,040
And Eva Javlonka and Marion Lam made this beautiful work where they bring precisely this idea.

122
00:09:20,040 --> 00:09:22,680
Cognitive agents bring something extraordinary.

123
00:09:22,680 --> 00:09:27,720
In evolution, you have genetic information dominating the story of life for a very long

124
00:09:27,720 --> 00:09:28,720
time.

125
00:09:28,900 --> 00:09:33,100
Information that is based on something that is not genetic has enormous advantages.

126
00:09:33,100 --> 00:09:38,060
Information that can be shared, can be propagated, and for us humans can be propagated beyond

127
00:09:38,060 --> 00:09:40,460
ourselves.

128
00:09:40,460 --> 00:09:43,660
So that's the main thing I want to address.

129
00:09:43,660 --> 00:09:54,020
And the first part of the story is to actually ask ourselves, how likely is a brain to happen?

130
00:09:54,020 --> 00:09:59,880
And that connects with something that is fundamental in evolutionary biology, but goes beyond that

131
00:09:59,880 --> 00:10:06,240
and percolates into many other areas of knowledge, which is the role of randomness and contingency

132
00:10:06,240 --> 00:10:13,720
and history versus the possibility that there are very strong laws that constrain a lot

133
00:10:13,720 --> 00:10:15,360
what is possible.

134
00:10:15,360 --> 00:10:19,600
And I put two views here.

135
00:10:19,600 --> 00:10:24,780
Here on the left, Stephen Jay Gould was a strong advocate of the idea that evolution

136
00:10:24,780 --> 00:10:32,660
is so historical that any kind of little change that you put in place will modify the outcome.

137
00:10:32,660 --> 00:10:38,420
And he made, he used this mental experiment based on this movie that, sure, you have seen.

138
00:10:38,420 --> 00:10:45,060
I mean, I don't know here, but in Barcelona, you see every single Christmas, we have wonderful

139
00:10:45,060 --> 00:10:48,420
life at some point, right, on TV.

140
00:10:48,420 --> 00:10:50,440
And I remember for you the story.

141
00:10:50,440 --> 00:10:55,400
It's this guy, a very, very nice guy who had a lot of trouble for a very unfair reason

142
00:10:55,400 --> 00:11:00,640
that at some point he decides that his life is worthless and says to himself, the world

143
00:11:00,640 --> 00:11:03,280
will be better if I haven't been born.

144
00:11:03,280 --> 00:11:09,480
And then comes this angel, which is a kind of quite annoying character, which is proposing

145
00:11:09,480 --> 00:11:12,760
the experiment of, okay, let's do it, let's do the experiment.

146
00:11:12,760 --> 00:11:14,160
You haven't been born, right?

147
00:11:14,160 --> 00:11:15,280
What happens?

148
00:11:15,280 --> 00:11:17,660
And he shows this, all this chain of events.

149
00:11:17,660 --> 00:11:22,540
He didn't save his brother, who fell in a frozen lake and died.

150
00:11:22,540 --> 00:11:28,740
And because of that, his brother in the war, in the second war, couldn't save a whole company.

151
00:11:28,740 --> 00:11:30,500
And so on and so forth.

152
00:11:30,500 --> 00:11:35,740
And eventually, the city where they live is very crappy.

153
00:11:35,740 --> 00:11:40,660
So the message is, any single event can change everything.

154
00:11:40,660 --> 00:11:48,120
And Steve Gould used that in the context of evolution, saying if we were able to replay

155
00:11:48,120 --> 00:11:54,080
again the tape of evolution, like starting 600 million years ago, the biosphere that

156
00:11:54,080 --> 00:11:57,080
we'll see now will be totally different, right?

157
00:11:57,080 --> 00:11:59,800
It will be an alien biosphere.

158
00:11:59,800 --> 00:12:01,400
I don't think that's the case, right?

159
00:12:01,400 --> 00:12:02,960
But this is the idea.

160
00:12:02,960 --> 00:12:07,720
Other people like Jack Monod also brought the idea that randomness plays a very, very

161
00:12:07,720 --> 00:12:10,420
important role in evolution.

162
00:12:10,420 --> 00:12:16,220
And more recently, I'm very much a book person and a movie person, and my students know that

163
00:12:16,220 --> 00:12:20,100
very well, and I'll make recommendations.

164
00:12:20,100 --> 00:12:26,740
Recently, Sean Carroll, the biologist, wrote this beautiful book where he kind of puts

165
00:12:26,740 --> 00:12:29,500
these random events that seem to be relevant.

166
00:12:29,500 --> 00:12:32,500
But this randomness really is so important.

167
00:12:32,500 --> 00:12:34,780
Let me show you the other part of the story.

168
00:12:34,780 --> 00:12:41,040
In fact, when we look at the natural world, we find out that very often the same solutions

169
00:12:41,040 --> 00:12:48,800
appear again and again and again independently in very different groups of organisms.

170
00:12:48,800 --> 00:12:50,640
I put the eyes here.

171
00:12:50,640 --> 00:12:57,040
The complex eye that we have, this camera eye, has been invented in evolution probably

172
00:12:57,040 --> 00:13:01,080
about 25 times in a totally independent way.

173
00:13:01,080 --> 00:13:06,860
We have this eye, and Octopy, for example, they have an eye that is essentially the same.

174
00:13:06,860 --> 00:13:10,420
In fact, it's better than ours, right?

175
00:13:10,420 --> 00:13:16,460
And this is what we call convergence, that the reason that these solutions appear again

176
00:13:16,460 --> 00:13:21,500
and again and again is that there are very, very strong constraints, even maybe mathematical

177
00:13:21,500 --> 00:13:24,820
laws that limit the possible, right?

178
00:13:24,820 --> 00:13:29,900
This is a book, Life Solution, sounds like a self-help book, but this is a book of evolution.

179
00:13:29,920 --> 00:13:35,600
It gives a lot of very interesting examples that go from cells and codes, genetic codes,

180
00:13:35,600 --> 00:13:37,480
to minds, right?

181
00:13:37,480 --> 00:13:45,160
And here is the Catalan scientist, Per Alberg, who unfortunately I met him years ago and

182
00:13:45,160 --> 00:13:50,640
died very, very soon, and he made the argument that even when you look at the structure of

183
00:13:50,640 --> 00:13:56,720
monstrosities, when you take the terephthalgists in, for example, in nature, you see that they

184
00:13:56,720 --> 00:13:58,000
are very well organized.

185
00:13:58,000 --> 00:14:04,420
You can make a taxonomy that is very well organized because not everything is possible.

186
00:14:04,420 --> 00:14:12,380
So in the context of brains, we wanted to do something about that because, and as one

187
00:14:12,380 --> 00:14:18,420
of the ambitions of the Santa Fe Institute, right, that think in really broad terms and

188
00:14:18,420 --> 00:14:20,740
ask ourselves difficult questions.

189
00:14:20,740 --> 00:14:26,760
So at some point, we talk about the idea of why not to try to make a kind of a space

190
00:14:26,760 --> 00:14:30,040
of brains, of cognitions.

191
00:14:30,040 --> 00:14:37,040
And in particular, it was clear that we have these solid brains, right?

192
00:14:37,040 --> 00:14:42,360
Brains that I'm going to go into that in a moment, brains that are made by neurons located

193
00:14:42,360 --> 00:14:48,880
in specific positions, right, and connected, and everything happens in the connections.

194
00:14:48,880 --> 00:14:54,700
But uncolonies are a kind of brain of brains that are moving around, they're liquid.

195
00:14:54,700 --> 00:15:01,540
Your immune system is a class of network that in many ways is like a neural network, right?

196
00:15:01,540 --> 00:15:04,940
But the cells are moving all the time.

197
00:15:04,940 --> 00:15:05,940
And so on and so forth.

198
00:15:05,940 --> 00:15:09,220
So what happens with all these cognitions?

199
00:15:09,220 --> 00:15:11,580
How likely are they?

200
00:15:11,580 --> 00:15:14,180
How powerful they can be, all right?

201
00:15:14,180 --> 00:15:19,800
So we decided to organize a little workshop with my colleagues, Melanie Moses and Stephanie

202
00:15:19,800 --> 00:15:22,800
Forrest, about liquid brains, solid brains.

203
00:15:22,800 --> 00:15:28,240
This is kind of a very ambitious idea because we brought together this group of very, very

204
00:15:28,240 --> 00:15:29,760
interesting people.

205
00:15:29,760 --> 00:15:35,120
But of course, we didn't know what was the outcome of this because we didn't have even

206
00:15:35,120 --> 00:15:38,480
a definition of liquid brains, right?

207
00:15:38,480 --> 00:15:46,820
So it was like our St. Patron Colma McCarthy said once, we met together to have more fun

208
00:15:46,820 --> 00:15:48,520
that should be legal.

209
00:15:48,520 --> 00:15:56,180
But the idea was actually to come out from that, from a first roadmap of cognitions.

210
00:15:56,180 --> 00:16:02,180
I put it here, this was a special issue that came from this, a list of examples, right,

211
00:16:02,180 --> 00:16:07,180
that include the microbiome, include plants, we'll talk about plants also because they're

212
00:16:07,180 --> 00:16:10,200
kind of a solid organism.

213
00:16:10,200 --> 00:16:16,920
And in around everything, we look for regularities.

214
00:16:16,920 --> 00:16:21,880
They are common laws that we can use, common languages that we can use, and networks appear

215
00:16:21,880 --> 00:16:25,000
to be the key here.

216
00:16:25,000 --> 00:16:32,320
In a standard brain, right, you have the, as I was saying, fixed positions for neurons.

217
00:16:32,320 --> 00:16:37,260
In a non-colony, your individual scarring brains are moving around, right?

218
00:16:37,260 --> 00:16:42,100
So they are no constant connections, the connections are broken and formed all the time.

219
00:16:42,100 --> 00:16:45,180
And the same happens for the immune system.

220
00:16:45,180 --> 00:16:49,560
So first question, why brains, right?

221
00:16:49,560 --> 00:16:56,980
What is the evolutionary force that actually pushes things towards complexity towards brains?

222
00:16:56,980 --> 00:17:04,000
This is one very nice hypothesis that we'll use in several times, which is the moving hypothesis.

223
00:17:04,000 --> 00:17:10,280
It essentially says that if you live in an environment, when you need to search, an uncertain

224
00:17:10,280 --> 00:17:14,640
environment, you look to search for resources, you don't know what resources are, you need

225
00:17:14,640 --> 00:17:19,280
to move, you need to detect, you need to sense, brains are great for that, right?

226
00:17:19,280 --> 00:17:24,700
They centralize information and centralize the way you move, okay?

227
00:17:24,720 --> 00:17:27,600
So that's important for a number of reasons.

228
00:17:27,600 --> 00:17:33,520
And the other way of looking at that is that brains, to a very large extent, are prediction

229
00:17:33,520 --> 00:17:34,520
systems.

230
00:17:34,520 --> 00:17:40,020
Some people say prediction machines, but they want to avoid the machine metaphor in the

231
00:17:40,020 --> 00:17:43,640
sense that prediction is absolutely fundamental.

232
00:17:43,640 --> 00:17:46,920
We predict all the time, right?

233
00:17:46,920 --> 00:17:53,960
Okay, so first of all, and it will be totally unfair with our brains, right?

234
00:17:53,980 --> 00:17:55,660
Because we don't have time.

235
00:17:55,660 --> 00:17:58,820
But I brought a brain, okay?

236
00:17:58,820 --> 00:18:04,540
Yeah, my family was so happy when I said that I had a brain, finally.

237
00:18:04,540 --> 00:18:08,020
So this is kind of a very good model, right?

238
00:18:08,020 --> 00:18:11,980
It kind of weights kind of a real brain.

239
00:18:11,980 --> 00:18:17,340
The human brain is a spectacular combination of accidents.

240
00:18:17,340 --> 00:18:21,420
So there are things that have been occurring over time.

241
00:18:21,420 --> 00:18:24,240
Many things we don't understand, right?

242
00:18:24,240 --> 00:18:25,240
But also of optimization.

243
00:18:25,240 --> 00:18:27,600
There's optimized circuits.

244
00:18:27,600 --> 00:18:32,600
As you probably know, it consumes a lot of the energy that we bring into our bodies,

245
00:18:32,600 --> 00:18:37,560
about 25%, which means that it has to be important, right?

246
00:18:37,560 --> 00:18:42,240
Not everyone uses that much, but it's important, right?

247
00:18:42,240 --> 00:18:48,760
And it's made of, we know now, 86 billion neurons.

248
00:18:49,180 --> 00:18:53,940
It was the Susana Eculano in Brazil, found out the way of counting.

249
00:18:53,940 --> 00:18:58,820
And it works in a very dynamical way, right?

250
00:18:58,820 --> 00:19:02,300
At any time, you record the activity of the brain,

251
00:19:02,300 --> 00:19:04,860
you will see waves moving around.

252
00:19:04,860 --> 00:19:08,980
We know now that these waves happen to occur between order and disorder,

253
00:19:08,980 --> 00:19:11,020
in what we call a critical point.

254
00:19:11,020 --> 00:19:13,660
And allow synchronize the system so

255
00:19:13,680 --> 00:19:19,480
that you can actually put together information for different places, right?

256
00:19:19,480 --> 00:19:23,200
And so have specialized areas working together, right?

257
00:19:23,200 --> 00:19:26,760
It's a compromise between try to modularize the system and

258
00:19:26,760 --> 00:19:29,800
try to put everything in place.

259
00:19:29,800 --> 00:19:34,720
So there are several brains that correspond to this solid picture.

260
00:19:34,720 --> 00:19:39,720
Very simple brains like Hydra, which is kind of a simple network.

261
00:19:40,700 --> 00:19:46,060
Some more complex nervous systems, right?

262
00:19:46,060 --> 00:19:51,140
But brains by themselves come later on in the evolutionary history.

263
00:19:51,140 --> 00:19:57,140
And again, going into this idea of randomness versus strong laws.

264
00:19:59,180 --> 00:20:00,540
Is the brain unique?

265
00:20:00,540 --> 00:20:04,140
Could be very different ways of generating brains.

266
00:20:04,140 --> 00:20:08,540
And in the recent years, people have been trying to think about that.

267
00:20:08,560 --> 00:20:12,600
Well, we can make a theory of that and try to understand what makes the brain

268
00:20:12,600 --> 00:20:18,480
special or maybe what makes the brain inevitable, all right?

269
00:20:18,480 --> 00:20:22,480
So we could say, but if you want to build a revolutionary story of brains,

270
00:20:22,480 --> 00:20:24,880
that's a difficult task, isn't it?

271
00:20:24,880 --> 00:20:30,040
Because, for example, in terms of language or

272
00:20:30,040 --> 00:20:34,640
the mind, you could say language, which is very central.

273
00:20:34,660 --> 00:20:37,620
In the next lecture, we'll see how important is this and

274
00:20:37,620 --> 00:20:41,300
how robots can help understand the origins of language.

275
00:20:41,300 --> 00:20:47,260
But language, for example, somebody could say, it doesn't leave fossils.

276
00:20:47,260 --> 00:20:48,500
The mind doesn't leave fossils.

277
00:20:48,500 --> 00:20:50,860
Well, it's not completely true.

278
00:20:50,860 --> 00:20:54,260
This is an example that I wanted to bring because it's simple.

279
00:20:54,260 --> 00:20:58,420
But it brings an idea of what kind of things we can recognize.

280
00:20:58,420 --> 00:21:00,900
Does anybody see that there's a strong regularity here?

281
00:21:05,620 --> 00:21:09,460
These are hands of different people, right?

282
00:21:09,460 --> 00:21:14,220
That painted is the cave of the hands in Argentina.

283
00:21:14,220 --> 00:21:20,860
And what happens here is that if you look, it's all left hands, all of them, right?

284
00:21:20,860 --> 00:21:26,900
Because the people who use whatever they use to paint, you're using the right hand, right?

285
00:21:26,900 --> 00:21:32,460
And you know that nowadays, most people is right handed, okay?

286
00:21:32,680 --> 00:21:35,520
And that symmetry was there already.

287
00:21:35,520 --> 00:21:39,080
So we can see there's a trace of that particular thing.

288
00:21:39,080 --> 00:21:41,120
But brains do spectacular things.

289
00:21:41,120 --> 00:21:45,480
And particularly human brain with a visual cortex,

290
00:21:45,480 --> 00:21:52,280
which is an amazing system that we have been using to do a lot of

291
00:21:52,280 --> 00:21:54,400
very important things that we will discuss.

292
00:21:54,400 --> 00:21:57,640
But one particular experiment we can bring and

293
00:21:57,640 --> 00:22:03,060
that allows me to go into how we make theories of the brain is this, right?

294
00:22:03,060 --> 00:22:08,300
In the left, you have a picture which has been kind of pixelated and

295
00:22:08,300 --> 00:22:12,620
you only retain part of the texture, right?

296
00:22:12,620 --> 00:22:13,780
Everybody sees what is there?

297
00:22:15,460 --> 00:22:16,780
There's a dock.

298
00:22:18,380 --> 00:22:21,100
The usual thing, it's a pattern here, right?

299
00:22:21,100 --> 00:22:22,500
We don't know exactly why.

300
00:22:22,500 --> 00:22:25,100
The pattern is some people already know.

301
00:22:25,120 --> 00:22:27,880
Some people see the dock, right?

302
00:22:27,880 --> 00:22:30,240
It's kind of like a Dalmatian.

303
00:22:30,240 --> 00:22:34,680
Some people doesn't see anything, but when you say there's a dock, right?

304
00:22:34,680 --> 00:22:37,200
The brain finds out, okay?

305
00:22:37,200 --> 00:22:40,800
And then there's people who don't see it, right?

306
00:22:40,800 --> 00:22:42,320
It's okay.

307
00:22:42,320 --> 00:22:46,600
But the thing is that that kind of pattern recognition system,

308
00:22:46,600 --> 00:22:52,640
which is extremely effective, we use constantly since we are toddlers almost,

309
00:22:52,660 --> 00:22:57,220
is impossible to simulate with a standard program.

310
00:22:57,220 --> 00:23:00,580
You cannot write a computer program to do this, right?

311
00:23:02,700 --> 00:23:10,740
On the right part, what I'm putting is also the fact that once you recognize

312
00:23:10,740 --> 00:23:16,660
that particular pattern here, it's going to be stuck in your brain forever.

313
00:23:16,660 --> 00:23:22,020
So one day you go, in ten years ahead, you go to the house of a friend and

314
00:23:22,040 --> 00:23:27,440
on top of the table is this picture covered in part, right?

315
00:23:27,440 --> 00:23:31,080
It's very likely that those of you who recognize that say,

316
00:23:31,080 --> 00:23:37,200
look, this is from that great lecture I went, right?

317
00:23:38,720 --> 00:23:42,240
So how do we actually make a model of this, right?

318
00:23:42,240 --> 00:23:46,520
It would appear that it has to be a very complex, very complicated model.

319
00:23:46,520 --> 00:23:53,180
And everything starts and comes from the work of these two amazing characters,

320
00:23:53,180 --> 00:23:59,140
Warren McCulloch and Walter Pitts, who actually came about with

321
00:23:59,140 --> 00:24:02,820
the first formal model of a neuron.

322
00:24:02,820 --> 00:24:08,740
And from that comes out all the neural networks that we use in

323
00:24:08,740 --> 00:24:12,820
all the artificial neural applications, chat GPT and everything else.

324
00:24:13,760 --> 00:24:18,200
We don't have time to talk about them, but if you search a little bit,

325
00:24:18,200 --> 00:24:24,760
in particular Pete's on the right, he was a child prodigy.

326
00:24:24,760 --> 00:24:27,120
He has a very, very interesting story.

327
00:24:27,120 --> 00:24:28,560
They figure out how to do it.

328
00:24:28,560 --> 00:24:31,480
And what they did was to transform a neuron,

329
00:24:31,480 --> 00:24:36,400
which is a really complex system itself, into a mathematical representation.

330
00:24:36,400 --> 00:24:40,200
To make the long story short, the idea is you have a neuron,

331
00:24:40,220 --> 00:24:43,780
you have inputs from other neurons that set signals.

332
00:24:43,780 --> 00:24:48,300
The signals can be positive, trying to make you to activate, or

333
00:24:48,300 --> 00:24:52,340
can be negative, try to make you put down, getting active.

334
00:24:52,340 --> 00:24:54,180
And you wait everything.

335
00:24:54,180 --> 00:24:56,820
And once you wait, you have a threshold.

336
00:24:56,820 --> 00:25:00,060
And if you go beyond the threshold, you activate, right?

337
00:25:00,060 --> 00:25:04,460
And then the neuron sends another signal somewhere else, okay?

338
00:25:04,460 --> 00:25:08,740
You can bring that mathematically in a very elegant way, okay?

339
00:25:08,740 --> 00:25:12,200
And you can use now that for modeling a lot of things.

340
00:25:12,200 --> 00:25:17,320
For example, John Hopfield, this amazing model,

341
00:25:17,320 --> 00:25:22,240
which I will put in a nutshell as follows.

342
00:25:22,240 --> 00:25:24,560
Imagine you have a collection of neurons.

343
00:25:24,560 --> 00:25:29,200
For me, my neurons will be elements that they just are on and off, right?

344
00:25:29,200 --> 00:25:31,360
They are active or inactive.

345
00:25:31,360 --> 00:25:34,920
And I can put them to the dimensional layer,

346
00:25:34,940 --> 00:25:39,260
like in a retina, like it's something that detects images, okay?

347
00:25:40,420 --> 00:25:44,500
Now, it's possible to show, I connect everyone with everyone.

348
00:25:44,500 --> 00:25:48,340
I use the maculok pits, threshold units, right?

349
00:25:48,340 --> 00:25:52,860
And then it's possible to train the network, show images,

350
00:25:52,860 --> 00:25:55,140
letters, whatever it is, okay?

351
00:25:55,140 --> 00:26:01,460
So the network learns how, well, the rule is very simple.

352
00:26:01,460 --> 00:26:04,860
If two neurons receive the same information, for example,

353
00:26:04,860 --> 00:26:09,240
two black pixels, they reinforce their connection.

354
00:26:09,240 --> 00:26:14,680
If they receive contradictory signals, black pixel, white pixel,

355
00:26:14,680 --> 00:26:17,640
the connection between them is reduced.

356
00:26:19,000 --> 00:26:24,120
That's all, no long computer program, nothing.

357
00:26:24,120 --> 00:26:28,960
And you can show, using this, that this network is capable of doing

358
00:26:28,960 --> 00:26:31,960
precisely what I was showing you before, right?

359
00:26:31,980 --> 00:26:35,780
It's possible to show that in a space that is highly dimensional,

360
00:26:35,780 --> 00:26:38,180
that depends on the number of connections, right?

361
00:26:38,180 --> 00:26:41,900
You create kind of valleys, right?

362
00:26:41,900 --> 00:26:47,300
This abstract space, valleys where the bottom of the valleys are the memories.

363
00:26:47,300 --> 00:26:51,820
The things that you have made the network to learn.

364
00:26:51,820 --> 00:26:58,060
For example, imagine that I have trained the network to learn only two images, right?

365
00:26:58,060 --> 00:26:59,940
This one here, that one here.

366
00:26:59,960 --> 00:27:02,400
You will create two valleys, and

367
00:27:02,400 --> 00:27:05,440
in the bottom of the valleys you have the memories.

368
00:27:05,440 --> 00:27:06,560
In what sense?

369
00:27:06,560 --> 00:27:11,760
In the sense that, if now I show an image that is incomplete,

370
00:27:11,760 --> 00:27:14,240
is the form, is distorted, right?

371
00:27:14,240 --> 00:27:17,000
The network, just using the dynamics of my Culloch and

372
00:27:17,000 --> 00:27:20,520
Pitts goes down the valley, right?

373
00:27:20,520 --> 00:27:23,120
And reconstruct all the information, right?

374
00:27:24,320 --> 00:27:27,400
I think this is totally amazing, right?

375
00:27:27,400 --> 00:27:31,580
And shows the power of artificial neural networks.

376
00:27:31,580 --> 00:27:36,780
But of course, this corresponds to a given model for

377
00:27:36,780 --> 00:27:38,860
a given class of brains.

378
00:27:38,860 --> 00:27:43,900
What about the potential universe possibilities, right?

379
00:27:43,900 --> 00:27:45,020
I could imagine things.

380
00:27:45,020 --> 00:27:49,980
These are, my drawing is about four things that we don't observe,

381
00:27:49,980 --> 00:27:51,660
don't exist, right?

382
00:27:51,660 --> 00:27:52,220
We'll offer them.

383
00:27:52,220 --> 00:27:57,040
And in my lecture today,

384
00:27:57,040 --> 00:28:02,760
I want to explore this idea of what kind of brains are there.

385
00:28:04,360 --> 00:28:06,560
Whether there are very strong constraints, whether or

386
00:28:06,560 --> 00:28:12,680
not brains are expected or there's a lot of possibilities.

387
00:28:12,680 --> 00:28:16,040
And I'm going to use, there's a huge amount of examples here.

388
00:28:16,040 --> 00:28:19,400
I'm going to use ants as one example.

389
00:28:19,400 --> 00:28:23,420
Fissarum, which is kind of a very alien creature.

390
00:28:24,700 --> 00:28:27,140
Discuss about plants, because there's been a lot of discussion in

391
00:28:27,140 --> 00:28:29,900
the recent years about plant intelligence.

392
00:28:29,900 --> 00:28:34,900
And show you that we can start to think in a space of possible

393
00:28:34,900 --> 00:28:37,780
conditions, and how we do it, right?

394
00:28:39,020 --> 00:28:44,660
So a very important point, liquid versus solid, right?

395
00:28:45,080 --> 00:28:51,560
On the right hand side, you have a very small part of a neural

396
00:28:51,560 --> 00:28:53,600
network of neurons connected.

397
00:28:53,600 --> 00:28:56,880
Again, their locations remain the same over time.

398
00:28:56,880 --> 00:29:06,000
And on the left, it's a very tiny part of a swarm of army ants, right?

399
00:29:06,000 --> 00:29:08,760
I was very impressed when I was in Panama years ago,

400
00:29:08,760 --> 00:29:12,560
seeing the army ants, which are blind.

401
00:29:12,560 --> 00:29:17,940
They communicate in simple ways with their nest mates.

402
00:29:17,940 --> 00:29:24,100
But forming these huge forms that, if you look from the distance,

403
00:29:24,100 --> 00:29:27,620
look like a single organism moving around, right?

404
00:29:27,620 --> 00:29:30,340
Swarming in the middle of the forest, quite a thing.

405
00:29:31,460 --> 00:29:37,580
And so we want to see how we approach all these problems.

406
00:29:37,580 --> 00:29:41,580
And in particular, one good question is, is a liquid brain

407
00:29:41,600 --> 00:29:44,640
able to be as complex as a solid brain?

408
00:29:46,140 --> 00:29:50,000
And the question is, is it relevant for a number of reasons?

409
00:29:51,800 --> 00:29:56,200
One of the reasons is, let me see this, oops, there's a movie here.

410
00:29:58,200 --> 00:30:00,720
Oh, yes, okay, sorry.

411
00:30:06,320 --> 00:30:06,840
Let me see.

412
00:30:06,840 --> 00:30:12,180
And the question is, in particular, because one good comparison we

413
00:30:12,180 --> 00:30:14,620
can make is about us, right?

414
00:30:14,620 --> 00:30:18,780
The, oh, man, what's happened?

415
00:30:23,280 --> 00:30:23,780
Let me see.

416
00:30:26,980 --> 00:30:27,500
Get into me.

417
00:30:27,500 --> 00:30:36,820
And the question, of course, for a colony of ants is who is in charge?

418
00:30:37,140 --> 00:30:39,860
I mean, ants are moving around, right?

419
00:30:39,860 --> 00:30:44,300
It's not like a centralized system saying you have to do these and

420
00:30:44,300 --> 00:30:45,540
that and that, right?

421
00:30:45,540 --> 00:30:52,140
It's not such a thing as a queen giving orders to everyone else, right?

422
00:30:52,140 --> 00:30:58,540
The queens in a way, as in European monarchies, are very useless,

423
00:30:58,540 --> 00:31:01,700
except in this case, to put X, right?

424
00:31:01,700 --> 00:31:03,380
So how do you control that?

425
00:31:03,380 --> 00:31:09,200
And this sentence by Deborah Gordon, I think, is very to the point, right?

426
00:31:09,200 --> 00:31:11,960
And for those of you who are fans of Richard Feynman,

427
00:31:11,960 --> 00:31:15,360
Richard Feynman himself was interested in ants and

428
00:31:15,360 --> 00:31:20,040
started to bring quite interesting questions about how ants work, right?

429
00:31:20,040 --> 00:31:21,200
Well, two important things to say.

430
00:31:23,040 --> 00:31:26,280
What makes ants and termites and social insects,

431
00:31:26,280 --> 00:31:30,040
I can only address a few things, special.

432
00:31:30,040 --> 00:31:35,100
And what do they do that is close to the brains that we were discussing,

433
00:31:35,100 --> 00:31:36,820
the human brain, for example?

434
00:31:36,820 --> 00:31:41,860
Well, on the one hand, they have managed to modify their environments.

435
00:31:41,860 --> 00:31:45,340
They do what we call extended minds, right?

436
00:31:45,340 --> 00:31:50,860
They create superstructures and the nests are the clear example.

437
00:31:50,860 --> 00:31:54,580
And the nests can be extremely large compared with the single organisms.

438
00:31:54,580 --> 00:31:58,780
In a termite nest, in some cases, you can have the very tiny,

439
00:31:58,800 --> 00:32:05,160
homilimetric organisms, whereas the nest for termites in some parts of

440
00:32:05,160 --> 00:32:08,520
Africa can be three, four meters high, right?

441
00:32:08,520 --> 00:32:11,640
So ten orders of magnitude larger.

442
00:32:11,640 --> 00:32:13,440
How do you build that?

443
00:32:13,440 --> 00:32:16,280
Of course, termites don't know anything about that.

444
00:32:16,280 --> 00:32:17,840
Again, they are blind.

445
00:32:17,840 --> 00:32:21,120
They communicate in simple ways with chemistry.

446
00:32:21,120 --> 00:32:26,320
So whatever creates the organization comes out from a collective phenomenon,

447
00:32:26,340 --> 00:32:31,780
from self-organization, or what we call in complexity, emerging phenomena.

448
00:32:31,780 --> 00:32:36,580
Phenomena that we can describe that even scale, network architecture,

449
00:32:36,580 --> 00:32:39,900
or in our brains, consciousness, memory.

450
00:32:39,900 --> 00:32:44,460
And that cannot be reduced to the properties of the individual parts.

451
00:32:44,460 --> 00:32:47,940
You can spend your whole life studying single termites.

452
00:32:47,940 --> 00:32:50,900
You'll never understand how they build the nests, right?

453
00:32:50,900 --> 00:32:56,020
The key is that the collective behavior, the fact that the interaction between,

454
00:32:56,020 --> 00:33:01,600
for example, termites and the material they use creates amplification phenomena

455
00:33:01,600 --> 00:33:06,840
that lead to self-organization, in particular, to order patterns.

456
00:33:06,840 --> 00:33:13,920
That picture there is a small part of the fungi factory that you have in

457
00:33:13,920 --> 00:33:16,360
termite nest, in some cases, right?

458
00:33:16,360 --> 00:33:21,240
And the explanation of the mathematics comes from what's called tuning structures.

459
00:33:21,240 --> 00:33:24,080
But it emerges from the interactions, right?

460
00:33:24,080 --> 00:33:25,520
In a liquid brain.

461
00:33:25,520 --> 00:33:28,340
Another thing that is quite fascinating is,

462
00:33:28,340 --> 00:33:32,460
ants can solve the problem of finding out the shortest distance.

463
00:33:32,460 --> 00:33:34,960
How?

464
00:33:34,960 --> 00:33:38,140
Imagine you have the ants like in the movie, right?

465
00:33:38,140 --> 00:33:43,060
That are in the lab, you put a food source somewhere, and here is the nest.

466
00:33:43,060 --> 00:33:48,820
If some ant detects that there's food here, they deliver a chemical signal,

467
00:33:48,820 --> 00:33:54,060
which other ants find out reinforce, and eventually you create a signal

468
00:33:54,060 --> 00:33:56,460
that goes beyond the individuals, right?

469
00:33:56,460 --> 00:33:59,740
And individuals in the signal interact in nonlinear ways, and

470
00:33:59,740 --> 00:34:04,100
you make this trail of ants that exploit the source very quickly.

471
00:34:04,100 --> 00:34:05,140
What happens if you put two?

472
00:34:06,300 --> 00:34:11,900
Well, the things can also be dependent on its scenario and its species.

473
00:34:11,900 --> 00:34:15,140
But for example, if I have more food here than here,

474
00:34:15,140 --> 00:34:22,340
they split at the beginning, but the source that is more abandoned is reinforced.

475
00:34:22,340 --> 00:34:25,900
What happens if you make an experiment like the following?

476
00:34:25,900 --> 00:34:30,700
Imagine you have the nest, you have the resource, the food here,

477
00:34:30,700 --> 00:34:32,900
and you have a double bridge, right?

478
00:34:32,900 --> 00:34:35,700
So there are two branches, and one is longer than the other.

479
00:34:36,740 --> 00:34:42,540
Okay, individual ants are unable to know that, if this is longer or shorter.

480
00:34:42,540 --> 00:34:45,580
But since they leave a chemical trail, right?

481
00:34:45,580 --> 00:34:50,220
The longer part will lose, because of course,

482
00:34:50,220 --> 00:34:55,740
the chemical signal is dissipated, is evaporated, will lose more by evaporation.

483
00:34:55,740 --> 00:35:00,140
And eventually, everyone will go into the shorter chain, right?

484
00:35:00,140 --> 00:35:04,620
So you need a collective phenomenon here to actually solve the problem of

485
00:35:04,620 --> 00:35:06,100
the shortest path, okay?

486
00:35:07,700 --> 00:35:09,660
And then you go into the question.

487
00:35:10,700 --> 00:35:16,380
Is an an colony going to be as complex in terms of cognition as a brain?

488
00:35:18,380 --> 00:35:20,140
I think the answer is no.

489
00:35:20,140 --> 00:35:24,340
And the reason is, you can represent ants in different ways.

490
00:35:24,340 --> 00:35:28,740
And I want to remember you that the ants have brains, right?

491
00:35:28,740 --> 00:35:31,540
They are not neurons, they are brains.

492
00:35:31,540 --> 00:35:35,740
But in brains, they can be half a million neurons, so it's not small.

493
00:35:37,260 --> 00:35:41,460
But interestingly, when you make models of how the ants solve the shortest path

494
00:35:41,460 --> 00:35:45,940
problem, how they build their nest, you can represent the ants in extremely simple

495
00:35:45,940 --> 00:35:46,820
ways, right?

496
00:35:46,820 --> 00:35:50,500
Sometimes even in on and off systems.

497
00:35:50,500 --> 00:35:55,380
And so you can use that, you can represent the ants in this way.

498
00:35:55,380 --> 00:36:00,940
And instead of using the McCulloch-Pitts model where neurons exchange things

499
00:36:00,940 --> 00:36:06,900
in a fixed way, you can actually make what we call a liquid brain.

500
00:36:06,900 --> 00:36:11,340
Here, for me, each ant can be, for example, an active or

501
00:36:11,340 --> 00:36:15,060
an inactive neuron, they move around, right?

502
00:36:15,060 --> 00:36:19,620
And over time, they interact exactly the same way that neural networks,

503
00:36:19,620 --> 00:36:22,940
except that now they are changing in time, they are moving.

504
00:36:22,940 --> 00:36:29,980
And one of the beautiful examples that we have explored comes from a war by

505
00:36:29,980 --> 00:36:34,180
Deborah Gordon and colleagues, where actually they have these ants that

506
00:36:34,180 --> 00:36:39,420
live in the desert, and you can see the ants doing special tasks, right?

507
00:36:39,420 --> 00:36:43,420
They are exactly the same morphological identical, but they do different tasks.

508
00:36:43,420 --> 00:36:48,380
They can forage, they can have nest maintenance, they patrol.

509
00:36:48,380 --> 00:36:53,660
And you see that the same ants, over time, they might change task, right?

510
00:36:53,660 --> 00:36:59,540
And also, if you are a bad person and you take, for example, all the foragers, right?

511
00:36:59,540 --> 00:37:00,820
And see what happens.

512
00:37:00,820 --> 00:37:03,660
What happens is the colony reorganizes.

513
00:37:03,660 --> 00:37:10,660
So some ants that maybe were just making nest maintenance become foragers.

514
00:37:10,660 --> 00:37:15,620
It reorganizes in such a way that it optimized the number of individuals that

515
00:37:15,620 --> 00:37:17,340
do each task.

516
00:37:17,340 --> 00:37:21,340
But then if you represent that with a mathematical model, etc., what you realize

517
00:37:21,340 --> 00:37:27,860
is, remember the model I showed you with these ballies that were the memories, right?

518
00:37:27,860 --> 00:37:29,860
Here you have also ballies.

519
00:37:29,860 --> 00:37:35,380
But in this landscape, what is in the bottom is the number of ants that

520
00:37:35,380 --> 00:37:37,340
are involved in each task.

521
00:37:37,340 --> 00:37:42,500
Which is something that is much, much, much less rich, right?

522
00:37:42,500 --> 00:37:48,900
You are not exploring a hyper-dimensional space of connections, because the connections

523
00:37:48,900 --> 00:37:51,060
are destroyed all the time.

524
00:37:51,060 --> 00:37:55,260
You generate something that has to do with the average number of things needed so that

525
00:37:55,260 --> 00:37:56,860
the colony works.

526
00:37:56,860 --> 00:38:06,740
Which for us, suggests that things like that depend on liquidity may be very limited.

527
00:38:06,740 --> 00:38:11,860
In the science fiction literature, or in the movies, for example, Star Trek, I'm not

528
00:38:11,860 --> 00:38:15,220
a tricky person, but it's an interesting example.

529
00:38:15,220 --> 00:38:16,820
They propose this idea, the Borgs.

530
00:38:16,820 --> 00:38:24,420
The Borgs is kind of a race of cyborgs, each one with a big brain, right?

531
00:38:24,420 --> 00:38:30,620
With a queen that has a big brain and controls some things, right?

532
00:38:30,620 --> 00:38:38,580
But the thing is, right, why do we don't see, for example, what I call brainy ants?

533
00:38:38,580 --> 00:38:41,860
We don't see ants with a large brain, right?

534
00:38:41,860 --> 00:38:44,980
It's a possibility in the space of what we can imagine.

535
00:38:44,980 --> 00:38:50,660
And what we find out is, although the theory has to be developed, is that actually, if

536
00:38:50,660 --> 00:38:56,380
you look closely, you find some things that are extremely interesting.

537
00:38:56,380 --> 00:39:05,620
For example, it seems to be a trend for colonies that in a nutshell, the pattern is, you have

538
00:39:05,620 --> 00:39:09,740
a very small colony, individuals can be complex.

539
00:39:09,740 --> 00:39:11,540
I saw them in Panama also.

540
00:39:11,540 --> 00:39:18,660
I saw these groups of ants, colonies with 100 individuals only, very large ants, everybody

541
00:39:18,660 --> 00:39:21,380
warned us, don't touch them, right?

542
00:39:21,380 --> 00:39:26,260
Because they are called 24 hours, that's the time we'll suffer the pain.

543
00:39:26,260 --> 00:39:33,500
So I believe that, one of my colleagues didn't, and the experimental method.

544
00:39:33,500 --> 00:39:37,580
And it was interesting because you approach the colony and the ants were outside with

545
00:39:37,580 --> 00:39:41,300
big eyes, and clearly they saw us.

546
00:39:41,300 --> 00:39:46,260
And if you look at the individuals, they were more or less making their decisions without

547
00:39:46,260 --> 00:39:48,620
much worrying about anything else.

548
00:39:48,620 --> 00:39:54,500
But if you take very large colonies where the colony itself can do extremely complex

549
00:39:54,580 --> 00:40:00,060
things, interestingly, individuals get more and more and more dumb.

550
00:40:00,060 --> 00:40:02,620
As if there was a trade of fear.

551
00:40:02,620 --> 00:40:06,500
This has been called the complexity drain.

552
00:40:06,500 --> 00:40:09,780
The complexity drain is something that we need to develop the theory of that.

553
00:40:09,780 --> 00:40:15,740
Essentially, we'll say that the more complex the society, the less complex the individuals,

554
00:40:15,740 --> 00:40:16,740
right?

555
00:40:16,740 --> 00:40:20,940
Don't try to apply that to our societies, okay?

556
00:40:20,940 --> 00:40:27,940
Anyways, my second example, my second example has to do with an extraordinary organism.

557
00:40:28,140 --> 00:40:30,380
And I wanted to start with this.

558
00:40:30,380 --> 00:40:37,380
This is a labyrinth that is in Barcelona in the orta quarter that is a replica of the

559
00:40:37,380 --> 00:40:42,300
famous labyrinth of Gnosis, the minotaur, right?

560
00:40:42,300 --> 00:40:46,740
Labyrinths have been something that mathematicians and all kinds of people have been fascinated

561
00:40:46,740 --> 00:40:47,740
in.

562
00:40:47,740 --> 00:40:49,540
How do you escape from a labyrinth?

563
00:40:49,540 --> 00:40:53,780
Or is this the entrance and is this the exit?

564
00:40:53,780 --> 00:40:57,300
How do you find the shortest path, for example, okay?

565
00:40:57,300 --> 00:40:59,300
So Fissarum is able to do that.

566
00:40:59,300 --> 00:41:06,300
Fissarum is not an animal, not a plant, is a slime mold, is a very simple creature.

567
00:41:08,060 --> 00:41:15,060
And actually, some people describe it as a single cell, is a whole thing with many nuclei

568
00:41:16,060 --> 00:41:22,900
inside, but essentially it is a single cell, except that it is extremely large, right?

569
00:41:22,900 --> 00:41:24,100
You can see it in the forest.

570
00:41:24,100 --> 00:41:30,700
And actually, when it was found many years ago in the time of the Sputnik, the people

571
00:41:30,700 --> 00:41:35,540
who found out that blob, which is yellowish as it is like this, and it can be large as

572
00:41:35,540 --> 00:41:40,460
my hand, found in the forest, they thought that it was kind of an alien thing, right?

573
00:41:40,460 --> 00:41:43,100
That came with a spacecraft, of course.

574
00:41:43,460 --> 00:41:49,500
Fissarum is amazing, it shifts all the time, has these network structures, sometimes looks

575
00:41:49,500 --> 00:41:55,180
like a neural network, and searches in space looking for resources, right?

576
00:41:55,180 --> 00:41:59,700
And if part of Fissarum finds something that is very rich, and he finds out that it's not

577
00:41:59,700 --> 00:42:03,180
so rich, it deviates everything in this direction.

578
00:42:03,180 --> 00:42:09,100
So you have all these tubes that pulsate over time, it's quite a thing.

579
00:42:09,100 --> 00:42:13,900
And that gets thicker and thicker as you approach something that is richer.

580
00:42:13,900 --> 00:42:18,340
And somebody used that in a very clever way, right?

581
00:42:18,340 --> 00:42:26,580
So since Fissarum is so easy to cultivate, one thing I can do is take pieces of Fissarum,

582
00:42:26,580 --> 00:42:31,900
put it within a labyrinth, an amaze, right, like here.

583
00:42:31,900 --> 00:42:38,140
And then I'm going to use what Fissarum likes a lot, which is flakes, right?

584
00:42:38,140 --> 00:42:45,620
One at the entrance, A, one at the exit, B. And over time, what you see in the movie

585
00:42:45,620 --> 00:42:48,580
is that Fissarum is detecting two sources, right?

586
00:42:48,580 --> 00:42:54,020
It's totally distributed, there's no centralized mind, there's no neurons at all, okay?

587
00:42:54,020 --> 00:42:57,180
And what happens is that it's an amplification phenomenon.

588
00:42:57,180 --> 00:43:02,180
As you go over time, you see that close to entrance and exit, right?

589
00:43:02,180 --> 00:43:08,980
The tubes that Fissarum forms, that is what they need to actually push forward the detection

590
00:43:08,980 --> 00:43:11,340
and exploitation, right?

591
00:43:11,340 --> 00:43:14,220
They have these nice waves.

592
00:43:14,220 --> 00:43:21,820
Eventually what happens is that you get in a single tube that goes all over the place

593
00:43:21,820 --> 00:43:26,820
in the shortest path from the entrance to the exit, okay?

594
00:43:26,820 --> 00:43:30,940
But if you want to model Fissarum, it's possible to do it.

595
00:43:30,940 --> 00:43:33,420
You model it with a network, a network of what?

596
00:43:33,420 --> 00:43:36,660
Of tubes connected like a fluid, all right?

597
00:43:36,660 --> 00:43:41,780
And interestingly, the mathematics that is behind is a threshold network, right?

598
00:43:41,780 --> 00:43:46,940
Even in that case, you need to use one mathematics that might be universal.

599
00:43:46,940 --> 00:43:50,420
Fissarum has been used in a number of applications, mazes.

600
00:43:50,420 --> 00:43:55,820
You can actually find out a network of roads.

601
00:43:55,820 --> 00:44:01,180
You take a country, for example, you put flakes in the locations of different cities.

602
00:44:01,180 --> 00:44:04,460
Fissarum is distributed all over the place.

603
00:44:04,460 --> 00:44:11,540
And then the tubes that reinforce the connections between different pairs of sources, right?

604
00:44:11,540 --> 00:44:17,820
Eventually draw this map, which you find out that is more optimal than the ones that engineers

605
00:44:17,820 --> 00:44:19,060
built.

606
00:44:19,060 --> 00:44:23,900
And it's been also used to actually map dark matter, right?

607
00:44:23,900 --> 00:44:28,940
Some astrophysicists find out the way of actually use Fissarum to make a large-scale

608
00:44:28,940 --> 00:44:34,180
model of the universe and infer the distribution of dark matter.

609
00:44:34,180 --> 00:44:36,300
But I want to make a point.

610
00:44:36,300 --> 00:44:41,540
Very often, and you can make logic gates and many things, very often it's said, look, Fissarum

611
00:44:41,540 --> 00:44:44,860
can solve complex mathematical models.

612
00:44:44,860 --> 00:44:50,980
And that is a distortion a little bit of what really happens.

613
00:44:50,980 --> 00:44:57,380
We exploit the properties of Fissarum, this extraordinary capacity of searching around.

614
00:44:57,380 --> 00:45:03,420
And this special capacity that, and that's very important, is based in a way of computing

615
00:45:03,420 --> 00:45:08,020
things, a computation that has nothing to do with the standard computation we use.

616
00:45:08,020 --> 00:45:11,020
The computation is the form, the shape.

617
00:45:11,020 --> 00:45:14,700
The final shape is what is being computed.

618
00:45:14,700 --> 00:45:18,820
But of course, we, the humans, we put what in physics, we say the boundary conditions

619
00:45:18,820 --> 00:45:19,820
or in mathematics, right?

620
00:45:19,820 --> 00:45:26,620
We put in place things and Fissarum just goes on with its dynamics, right?

621
00:45:26,620 --> 00:45:29,660
So Fissarum doesn't solve problems in nature.

622
00:45:29,660 --> 00:45:34,620
Solve problems that have to do with resources, but not mathematics, okay?

623
00:45:34,620 --> 00:45:36,900
What about plants, right?

624
00:45:36,900 --> 00:45:41,380
I hope there are not so many enthusiasts here of plant intelligence.

625
00:45:41,380 --> 00:45:44,100
I'm a skeptic.

626
00:45:44,100 --> 00:45:49,380
In the literature also, we have this, I don't know if everyone knows, the day of the triphids.

627
00:45:49,380 --> 00:45:52,220
It's a classic novel science fiction.

628
00:45:52,220 --> 00:45:56,660
We have these plants that are capable of moving, right?

629
00:45:56,660 --> 00:46:01,340
And that, well, I don't want to spoil anything.

630
00:46:01,340 --> 00:46:02,340
Just read it.

631
00:46:02,340 --> 00:46:04,140
It's really cool.

632
00:46:04,140 --> 00:46:07,060
So are plants intelligent?

633
00:46:07,060 --> 00:46:11,300
Well, let me say first something.

634
00:46:11,300 --> 00:46:12,780
Plants are extraordinary.

635
00:46:12,780 --> 00:46:15,900
They have really transformed completely the planet.

636
00:46:15,900 --> 00:46:19,340
When they invade land, they invented the sorts.

637
00:46:19,340 --> 00:46:21,300
They created the forests.

638
00:46:21,300 --> 00:46:26,140
They have this system of photosynthesis that creates quite an amazing super molecular system

639
00:46:26,140 --> 00:46:29,660
with quantum properties that we're still trying to understand.

640
00:46:29,660 --> 00:46:32,140
So they are amazing by themselves, right?

641
00:46:32,140 --> 00:46:36,340
Do we need them to like Mozart?

642
00:46:36,340 --> 00:46:37,340
Maybe not.

643
00:46:37,340 --> 00:46:38,340
Maybe not.

644
00:46:38,340 --> 00:46:44,420
Have in mind that plants, on the one hand, have this extraordinary capacity of changing

645
00:46:44,420 --> 00:46:49,700
morphology, of adapting in a way that animals cannot do.

646
00:46:49,700 --> 00:46:52,660
They give them a lot of advantage.

647
00:46:52,660 --> 00:46:58,980
The thing is, when you look at plants and plants in the context, right?

648
00:46:58,980 --> 00:47:01,060
Like in a forest.

649
00:47:01,060 --> 00:47:06,500
Of course, there's a lot of complexity that has to do with things that we know from ecology.

650
00:47:06,580 --> 00:47:15,180
Competition, competition, mutually is in place a very, very important role in many ways.

651
00:47:15,180 --> 00:47:19,380
We start to uncover a lot of complexities there.

652
00:47:19,380 --> 00:47:23,460
But is really this connected to cognition?

653
00:47:23,460 --> 00:47:26,220
And I think it's important to go and look closely.

654
00:47:26,220 --> 00:47:28,340
What do we have?

655
00:47:28,340 --> 00:47:34,300
Plant cells, and there's a big constant with animal cells, are very rigid.

656
00:47:34,300 --> 00:47:41,540
We have this wall that makes connections between them extremely constrained, right?

657
00:47:41,540 --> 00:47:48,140
Connections, plasma of this matter, like that on the right in the upper picture is an electron

658
00:47:48,140 --> 00:47:53,060
microscope picture of a channel that connects to plant cells.

659
00:47:53,060 --> 00:47:56,460
But the architecture constrains a lot what happens there.

660
00:47:56,460 --> 00:48:01,980
Of course, there are no neurons, it's been told that there are analogies, but no neurons.

661
00:48:02,980 --> 00:48:05,780
Everything very much goes into two directions.

662
00:48:05,780 --> 00:48:07,500
One is growth.

663
00:48:07,500 --> 00:48:10,820
I have to grow and grow in a plastic way.

664
00:48:10,820 --> 00:48:12,980
Another is defense, right?

665
00:48:12,980 --> 00:48:21,420
Plants have developed a huge battery of chemical signals that connect them with the challenge

666
00:48:21,420 --> 00:48:24,780
that we have from insect herbivores in particular, right?

667
00:48:24,780 --> 00:48:26,580
There's a lot of investment in that.

668
00:48:26,580 --> 00:48:30,740
And it shows, it shows very much.

669
00:48:30,740 --> 00:48:36,780
On the other hand, one thing I wanted to mention, we will discuss that in the second lecture,

670
00:48:36,780 --> 00:48:43,820
but John von Neumann, the mathematician that I showed you before, in his studies about

671
00:48:43,820 --> 00:48:49,460
brains, unfortunately it wasn't at the end of his life, of brains versus computers, he

672
00:48:49,460 --> 00:48:52,060
made this the following point.

673
00:48:52,060 --> 00:48:57,500
At that time, the computers, these very big electronic computers, were very prone to fail

674
00:48:57,500 --> 00:49:01,820
because the basic components were not much reliable, right?

675
00:49:01,820 --> 00:49:06,500
And if a vacuum tube failed, the computer could fail.

676
00:49:06,500 --> 00:49:08,820
And they knew that brains don't work like that.

677
00:49:08,820 --> 00:49:12,100
Your neurons, every day you lose neurons.

678
00:49:12,100 --> 00:49:18,380
And you can even have a big loss of neurons and the brains capable of have plasticity

679
00:49:18,380 --> 00:49:23,420
to return the system to the previous state, not computers.

680
00:49:23,540 --> 00:49:29,540
He ended up in a conclusion which was, maybe we need systems that are very redundant, right?

681
00:49:29,540 --> 00:49:30,620
Very, very redundant.

682
00:49:30,620 --> 00:49:32,420
That's part of the solution, really.

683
00:49:32,420 --> 00:49:36,100
But if you look at plants and compare with animals, what is the difference?

684
00:49:36,100 --> 00:49:37,980
Well, many differences, right?

685
00:49:37,980 --> 00:49:42,260
On the one hand, they stack in the same place, right?

686
00:49:42,260 --> 00:49:47,140
So you think in the moving hypothesis, if I have to move, I need brains.

687
00:49:47,140 --> 00:49:52,340
If I don't, maybe I don't need brains.

688
00:49:52,340 --> 00:49:54,780
On the other hand, for example, you think in organs.

689
00:49:54,780 --> 00:50:00,260
If I ask you how many organs you have, you are not going to say, well, I don't know exactly.

690
00:50:00,260 --> 00:50:02,140
You do know, right?

691
00:50:02,140 --> 00:50:03,860
You do.

692
00:50:03,860 --> 00:50:06,500
One heart, two kidneys, et cetera, et cetera.

693
00:50:06,500 --> 00:50:10,380
And all of this is decided in embryogenesis.

694
00:50:10,380 --> 00:50:12,220
In plants, we see a very different situation.

695
00:50:12,220 --> 00:50:15,060
How many organs we have?

696
00:50:15,060 --> 00:50:16,060
Many.

697
00:50:16,060 --> 00:50:18,620
Every single leaf is an organ, right?

698
00:50:19,500 --> 00:50:24,260
They are formed and degrade and happen all the time.

699
00:50:24,260 --> 00:50:29,660
And also, for example, at the level of leaves, we have discovered that if you analyze the

700
00:50:29,660 --> 00:50:35,100
network of transport within leaves, which is a beautiful structure, you can see that

701
00:50:35,100 --> 00:50:38,740
it's optimized for doing two things.

702
00:50:38,740 --> 00:50:45,940
One, deliver the nutrients everywhere, right, in the most efficient way.

703
00:50:46,700 --> 00:50:52,060
To protect themselves from damage, an insect can make a hole.

704
00:50:52,060 --> 00:50:57,340
You have seen, for sure, leaves that are damaged in different ways, but you want to warranty

705
00:50:57,340 --> 00:50:58,620
that you get there.

706
00:50:58,620 --> 00:51:01,060
The transport keeps going well.

707
00:51:01,060 --> 00:51:04,660
If you have loops, the right amount of loops, you can do it.

708
00:51:04,660 --> 00:51:12,660
And this is a picture of one of these damage experiments where using a kind of fluorescent

709
00:51:12,660 --> 00:51:13,780
marker.

710
00:51:13,780 --> 00:51:19,220
You can see how it propagates and goes into the whole leaf again using the loops.

711
00:51:19,220 --> 00:51:20,220
And it's optimized.

712
00:51:20,220 --> 00:51:21,900
You can do a theory of that.

713
00:51:21,900 --> 00:51:29,100
So you have many organs that can be lost, are essentially redundant, so you don't really

714
00:51:29,100 --> 00:51:32,780
need to have a central control system.

715
00:51:32,780 --> 00:51:36,940
In the second lecture, I'll try to convince you that this is probably the case, right,

716
00:51:36,940 --> 00:51:42,180
that plants, because of they don't need that, and they are extraordinarily well-adapted

717
00:51:42,180 --> 00:51:47,940
in different ways, right, might not have anything like brains or intelligence.

718
00:51:47,940 --> 00:51:55,580
So fissurum, plants, brains, ants, how do we put this together?

719
00:51:55,580 --> 00:52:02,020
And the idea, still work in progress, right, is to create what we call a morphospace, a

720
00:52:02,020 --> 00:52:04,100
space of possibilities.

721
00:52:04,100 --> 00:52:09,060
In this particular example that we used some years ago, I used three axes.

722
00:52:09,060 --> 00:52:11,980
The vertical axis is how important is development.

723
00:52:11,980 --> 00:52:13,300
Design is very important.

724
00:52:13,300 --> 00:52:15,180
Nature constructs things, right?

725
00:52:15,180 --> 00:52:20,540
You have embryos that develop and get into complex architectures.

726
00:52:20,540 --> 00:52:27,500
In the horizontal axis is the liquid to solid, right, the state of matter that we have.

727
00:52:27,500 --> 00:52:33,380
And in the other axis is how complex is your cognition, how complex are your decision-making

728
00:52:33,380 --> 00:52:38,620
and how diverse is the way you actually sense and respond to the environment.

729
00:52:38,620 --> 00:52:43,700
We locate things here in relative positions, okay?

730
00:52:43,700 --> 00:52:50,740
For example, organs are kind of simple cognitive systems, right, or we could put, they are

731
00:52:50,740 --> 00:52:56,740
the outcome of development, they might be just responding to simple signals, or they

732
00:52:56,740 --> 00:53:00,580
might involve feedbacks that are more complex.

733
00:53:00,580 --> 00:53:03,820
This is the artificial part of the story.

734
00:53:03,820 --> 00:53:08,980
Organisms and organs have a counterpart in bioengineering, which is organoids, right,

735
00:53:08,980 --> 00:53:14,780
which opens the possibilities that we will discuss in the second lecture.

736
00:53:14,780 --> 00:53:21,620
Of course, brains, and of course, you can see I put a single sphere here representing

737
00:53:21,620 --> 00:53:26,900
all solid neural networks, right, this is an oversimplification.

738
00:53:26,900 --> 00:53:33,620
The immune system, which is a system that can learn where cells can have memories, where

739
00:53:33,620 --> 00:53:39,980
collectively you have phenomena that remind us a brain, right, but they are liquid, they

740
00:53:39,980 --> 00:53:40,980
move around, right.

741
00:53:40,980 --> 00:53:45,940
We are living in a very interesting time now where it seems that because our understanding

742
00:53:45,940 --> 00:53:50,580
of these networks, we might actually fight cancer and other things in a very effective

743
00:53:50,580 --> 00:53:51,580
way.

744
00:53:51,580 --> 00:53:57,020
And of course, I put hands in the middle between liquid and solid, high development because

745
00:53:57,020 --> 00:54:03,020
an uncolline actually, if you look at how it generates, it comes from the queen, the

746
00:54:03,020 --> 00:54:11,020
first X, the first individuals, the structure that goes emerges in a totally predictable

747
00:54:11,020 --> 00:54:16,420
way and ends up into something that is the structure plus the colony that is inside kind

748
00:54:16,420 --> 00:54:21,020
of a liquid system, right.

749
00:54:21,020 --> 00:54:28,780
The microbiome, the millions and millions of bacteria that we carry out that makes us

750
00:54:28,780 --> 00:54:34,580
something that is not anymore a single species, the idea that we are one species, we have

751
00:54:34,580 --> 00:54:38,540
to abandon because we really are much more than that.

752
00:54:38,540 --> 00:54:43,420
The microbiome, we'll talk about that also in the second lecture, has been co-evolving

753
00:54:43,420 --> 00:54:45,460
with us.

754
00:54:45,460 --> 00:54:50,940
The more we know, the more clear it is that many diseases we couldn't understand but it

755
00:54:50,940 --> 00:54:56,060
was how they work are connected with the microbiome and the great thing is that we can intervene,

756
00:54:56,060 --> 00:55:01,260
change the microbiome and maybe fight those diseases, right.

757
00:55:01,260 --> 00:55:06,740
And this microbiome interacts with the immune system and with the brain, right, which means

758
00:55:06,740 --> 00:55:10,820
that we have a lot of things at play.

759
00:55:10,820 --> 00:55:16,020
Fissarum, somewhere also in the middle between liquid and solid, okay.

760
00:55:16,020 --> 00:55:21,420
And this is kind of the big picture and they want to bring to your attention something

761
00:55:21,420 --> 00:55:27,540
that is quite visible, is this big sphere that occupies a lot of space.

762
00:55:27,540 --> 00:55:34,460
This big sphere is what we say is a void in the morpho space, meaning that when you look

763
00:55:34,460 --> 00:55:39,540
at the natural world, you don't see anything there.

764
00:55:39,540 --> 00:55:40,940
Why is that?

765
00:55:40,940 --> 00:55:44,300
Because it's forbidden.

766
00:55:44,300 --> 00:55:51,060
Because evolution for some reason is unable to get there, that's a big question.

767
00:55:51,060 --> 00:55:58,100
For many of us, it is an indication that is a lot, is plenty of space to explore and

768
00:55:58,100 --> 00:56:03,260
that if you are able to engineer things, we might go right there, right.

769
00:56:03,260 --> 00:56:05,500
And I'll show you examples soon.

770
00:56:05,500 --> 00:56:07,340
Okay.

771
00:56:07,340 --> 00:56:11,500
So now I just give you all these examples but I'm sure that some of you are thinking,

772
00:56:11,500 --> 00:56:16,260
yeah, yeah, but there's one particular example that you are not talking about, right.

773
00:56:16,260 --> 00:56:21,900
And that is quite bizarre, right.

774
00:56:21,900 --> 00:56:25,740
Of course, yeah, octopus.

775
00:56:25,740 --> 00:56:31,660
Octopus has been receiving a lot of attention over the last two decades.

776
00:56:31,660 --> 00:56:32,660
Why?

777
00:56:32,660 --> 00:56:38,740
Well, it's on the one hand is an extraordinary example of how evolution creates in a totally

778
00:56:38,740 --> 00:56:49,140
different trajectory in the tree of life, a mind that is remarkable, right.

779
00:56:49,140 --> 00:56:53,820
Octopi, they can learn, they have memory.

780
00:56:53,820 --> 00:56:57,900
You can see that they repeat the same words, right.

781
00:56:57,900 --> 00:57:00,940
But they also have clearly curiosity.

782
00:57:00,940 --> 00:57:06,460
There's been described many times that the octopus can be really interested and engaged

783
00:57:06,580 --> 00:57:15,580
into approaching humans, for example, and trying to, I don't know, figure out what we're doing.

784
00:57:15,580 --> 00:57:21,780
In the lab, it's been discovered that they recognize particular people, or sometimes

785
00:57:21,780 --> 00:57:26,980
that they can escape from the fish tank by night because there's a camera recording.

786
00:57:26,980 --> 00:57:31,140
Go to another fish tank to visit someone, I guess.

787
00:57:31,140 --> 00:57:33,820
And get back.

788
00:57:33,820 --> 00:57:34,820
Why they get back?

789
00:57:35,580 --> 00:57:39,740
There's plenty of things that we don't understand.

790
00:57:39,740 --> 00:57:44,700
And not surprisingly, it has been used often in the context of science fiction.

791
00:57:44,700 --> 00:57:54,740
This comes from a snapshot from the movie The Arrival, where this is the question of

792
00:57:54,740 --> 00:57:56,740
the language we'll discuss tomorrow.

793
00:57:56,740 --> 00:57:59,740
Language is the big thing, right.

794
00:57:59,740 --> 00:58:04,800
And you could say this is totally bizarre, totally different, totally alien.

795
00:58:05,720 --> 00:58:08,720
But, well, not that much.

796
00:58:08,720 --> 00:58:13,720
The interesting thing I want to bring is, if you analyze the brain of an octopus, right,

797
00:58:13,720 --> 00:58:18,280
you can have the microscope, you make slices, see what's the architecture.

798
00:58:18,280 --> 00:58:21,320
If you ask a histologist, what do you see here, right?

799
00:58:21,320 --> 00:58:25,080
You might not have seen any time an octopus' brain.

800
00:58:25,080 --> 00:58:33,240
But he will say, okay, this is a brain, probably some vertebrate, because I see the neurons.

801
00:58:33,240 --> 00:58:37,920
The neurons are these amazing structures that have the polarity, you have this special

802
00:58:37,920 --> 00:58:38,920
structure.

803
00:58:38,920 --> 00:58:41,560
When you look at the shape of a neuron, what do you see?

804
00:58:41,560 --> 00:58:44,760
You see a cell that is trying to connect, right?

805
00:58:44,760 --> 00:58:46,680
This is a big function.

806
00:58:46,680 --> 00:58:48,920
You see multilayers.

807
00:58:48,920 --> 00:58:55,480
So interestingly, in a different universe of possibilities, in invertebrates, a totally

808
00:58:55,480 --> 00:59:02,680
different branch, you generate an animal that has the same kind of eyes we have, that has

809
00:59:03,640 --> 00:59:09,280
a neural architecture that resembles things that we are very familiar with, okay?

810
00:59:09,280 --> 00:59:15,120
Of course, they are peculiarities, they have the central brain and something that acts

811
00:59:15,120 --> 00:59:21,280
like eight autonomous brains, one for each tentacle, right?

812
00:59:21,280 --> 00:59:28,800
But what this brings is that, again, it looks like maybe the space of possibilities is not

813
00:59:28,800 --> 00:59:29,800
that big.

814
00:59:29,840 --> 00:59:33,200
Even in that case, where you have this amazing animal, right?

815
00:59:33,200 --> 00:59:37,560
So why octopus are not more complex, right?

816
00:59:37,560 --> 00:59:38,560
They are interesting.

817
00:59:38,560 --> 00:59:39,960
They are clearly interesting.

818
00:59:39,960 --> 00:59:47,720
But for example, why cephalopod in general have not gone into using tools, for example.

819
00:59:47,720 --> 00:59:53,680
And it comes this interesting constraint that has to do with the life of these animals.

820
00:59:53,680 --> 00:59:57,040
Unfortunately, octopi don't live much.

821
00:59:57,040 --> 01:00:03,680
In some species, one year, in some two years, maybe three, but that's all.

822
01:00:03,680 --> 01:00:12,200
So thinking in the bioengineering, the possibilities in the future, you cannot avoid to think.

823
01:00:12,200 --> 01:00:17,040
What if we were able to make an octopus to live more, right?

824
01:00:17,040 --> 01:00:23,120
An animal that clearly learns over time has a brain that has this potential, right?

825
01:00:23,360 --> 01:00:31,240
And that brings me into, and I am trying to attract you to the second lecture, okay?

826
01:00:31,240 --> 01:00:39,160
Of course, to answer the questions I'm making about how brains originated, why brains will

827
01:00:39,160 --> 01:00:43,600
require to have a picture of evolution, what happened, really.

828
01:00:43,600 --> 01:00:45,760
We don't have a time machine.

829
01:00:45,760 --> 01:00:52,800
But we have an alternative, which is extremely interesting, and I will show you, provides

830
01:00:52,800 --> 01:00:59,200
very new fundamental questions that I think in the future might solve the questions that

831
01:00:59,200 --> 01:01:02,280
we have been making before.

832
01:01:02,280 --> 01:01:03,280
And that will be all.

833
01:01:03,280 --> 01:01:04,280
Thank you.

834
01:01:04,280 --> 01:01:17,480
Do we have time for questions?

835
01:01:17,480 --> 01:01:29,880
Maybe I can try, oh, there we go.

836
01:01:29,880 --> 01:01:32,080
So we have time for some questions.

837
01:01:32,080 --> 01:01:52,360
I would like to know how that plant or whatever it was, you're not to laugh at me, got in

838
01:01:52,360 --> 01:01:54,240
and out of that maze.

839
01:01:54,240 --> 01:01:59,600
How did something go into the maze and find its way out?

840
01:01:59,600 --> 01:02:02,600
You mean for the organism I was mentioning?

841
01:02:02,600 --> 01:02:08,240
Yeah, there was Fissarum, it was this single cell organism.

842
01:02:08,240 --> 01:02:15,080
And in nature, you have, for example, imagine you put Fissarum in some place, you have different

843
01:02:15,080 --> 01:02:17,400
sources of food, right?

844
01:02:17,400 --> 01:02:23,720
Fissarum is all the time expanding, like I'm searching around space.

845
01:02:23,720 --> 01:02:29,000
And then this information about the different resources comes to the collective, it's kind

846
01:02:29,000 --> 01:02:34,720
of integrating information all the time and making decisions about which one is richer

847
01:02:34,720 --> 01:02:37,400
and minimizing the trajectories.

848
01:02:37,400 --> 01:02:43,520
You don't want to make a lot of channels and invest energy and resources to create tubes.

849
01:02:43,520 --> 01:02:46,400
You want to, well, this is necessary.

850
01:02:46,400 --> 01:02:51,680
So what happened in the maze is that you put these two resources in those particular places

851
01:02:51,680 --> 01:02:54,560
that have sense for us.

852
01:02:54,560 --> 01:02:56,560
We know this is the entrance, this is the exit.

853
01:02:56,560 --> 01:02:58,520
Fissarum doesn't know anything.

854
01:02:58,520 --> 01:03:03,880
But then the amplification that is made here creates these tubes that are very strong here,

855
01:03:03,880 --> 01:03:04,880
right?

856
01:03:04,880 --> 01:03:06,480
And the same principle applies.

857
01:03:06,480 --> 01:03:12,280
In the end, you try to exploit which is really rich, right?

858
01:03:12,280 --> 01:03:18,480
And connect these sources in the shortest path you can make.

859
01:03:18,480 --> 01:03:20,520
And that solves the maze.

860
01:03:20,520 --> 01:03:25,640
But again, have in mind that we humans put the maze there, right?

861
01:03:25,640 --> 01:03:45,720
To prepare the problem, it's an important difference.

862
01:03:45,720 --> 01:03:47,200
Thanks for a brilliant lecture.

863
01:03:47,200 --> 01:03:49,760
Your drawings are like Leonardo da Vinci, really.

864
01:03:49,760 --> 01:03:50,760
Thank you.

865
01:03:50,760 --> 01:03:51,760
Fantastic.

866
01:03:52,160 --> 01:03:56,920
This is a question that's very stupid, but I've been asking it since I was six years old,

867
01:03:56,920 --> 01:04:01,800
which is today, of course, it seems completely obvious to all of us that we think with our

868
01:04:01,800 --> 01:04:03,200
brains.

869
01:04:03,200 --> 01:04:09,000
But I wonder whether early man knew that they think with their brains.

870
01:04:09,000 --> 01:04:14,520
Because if you look at all the cave paintings we have, you have the hands, you have animals,

871
01:04:14,520 --> 01:04:15,520
you have human figures.

872
01:04:15,960 --> 01:04:23,480
But you don't have heads necessarily as being more important than any other body feature.

873
01:04:23,480 --> 01:04:28,760
So I just wonder if there's any history in the scientific research of when we began to

874
01:04:28,760 --> 01:04:33,200
realize that we think with our brain.

875
01:04:33,200 --> 01:04:37,520
You mean as rational creatures that we knew that there was a brain?

876
01:04:37,520 --> 01:04:41,440
Or when the brain become became relevant?

877
01:04:41,440 --> 01:04:44,000
I think back to earliest human beings.

878
01:04:44,000 --> 01:04:49,600
The earliest development of communication, language, hunting, coordination, whatever

879
01:04:49,600 --> 01:04:55,080
it was, did they realize that this was what was working?

880
01:04:55,080 --> 01:04:56,080
Okay.

881
01:04:56,080 --> 01:04:57,080
Okay.

882
01:04:57,080 --> 01:04:58,080
It's not a stupid question.

883
01:04:58,080 --> 01:05:01,360
It always happens in all the talks when somebody brings it.

884
01:05:01,360 --> 01:05:06,160
This is going to be a stupid question and it's not at all.

885
01:05:06,160 --> 01:05:11,520
In fact, tomorrow I'm going to bring a little bit about what part of the singularity of

886
01:05:11,520 --> 01:05:16,320
the human brain is there.

887
01:05:16,320 --> 01:05:20,600
I will make a bit of spoilers here.

888
01:05:20,600 --> 01:05:26,160
The human brain is interesting for a number of reasons and had a very important impact

889
01:05:26,160 --> 01:05:28,280
in an evolutionary history.

890
01:05:28,280 --> 01:05:30,760
Just to mention a few things.

891
01:05:30,760 --> 01:05:35,400
One is language, of course.

892
01:05:35,400 --> 01:05:37,560
Language is a pretty extraordinary piece.

893
01:05:37,640 --> 01:05:46,160
These days of chat GPT, chat GPT for a number of reasons is not intelligent but brings some

894
01:05:46,160 --> 01:05:53,840
interesting ideas about the importance of language in evolving reasoning.

895
01:05:53,840 --> 01:05:59,000
One thing that I find extraordinary is very, very important and we also bring that tomorrow.

896
01:05:59,000 --> 01:06:00,500
It's time.

897
01:06:00,500 --> 01:06:06,280
Somebody said that we are mental time travelers.

898
01:06:07,280 --> 01:06:09,400
You remember what we mentioned before.

899
01:06:09,400 --> 01:06:11,720
We are prediction machines.

900
01:06:11,720 --> 01:06:15,360
The brain is something that tries to predict what's going next.

901
01:06:15,360 --> 01:06:21,200
This is very, very important because in the end what makes brains worth is that we are

902
01:06:21,200 --> 01:06:24,880
able to reduce uncertainty.

903
01:06:24,880 --> 01:06:32,920
We make us more prepared to actually be understanding what's next, what's going to happen.

904
01:06:33,040 --> 01:06:37,640
We had this cortex that expanded so much.

905
01:06:37,640 --> 01:06:44,120
This kind of understanding of time became something that was the narrative.

906
01:06:44,120 --> 01:06:49,880
We became able to make narratives, not only one feature.

907
01:06:49,880 --> 01:06:55,320
We can imagine many possible features.

908
01:06:55,320 --> 01:06:58,200
It's very interesting to see how it happens in evolution.

909
01:06:58,200 --> 01:07:03,480
Somebody said you all know that memory is faulty.

910
01:07:03,480 --> 01:07:05,520
Memory sometimes fails.

911
01:07:05,520 --> 01:07:12,720
Sometimes you have memories that are not real, memories that are being constructed.

912
01:07:12,720 --> 01:07:15,160
Somebody can say, why is that?

913
01:07:15,160 --> 01:07:18,520
Because natural selection doesn't care about that.

914
01:07:18,520 --> 01:07:21,120
Natural selection wants you to predict the future.

915
01:07:21,120 --> 01:07:23,360
The future is important.

916
01:07:23,360 --> 01:07:27,920
If you remember well or not the past is not so important.

917
01:07:27,920 --> 01:07:30,520
An important thing that connects maybe more with your question.

918
01:07:30,520 --> 01:07:38,440
One of the things that made us successful, as ants are successful because they cooperate,

919
01:07:38,440 --> 01:07:42,080
some days it doesn't seem so, but we are cooperators.

920
01:07:42,080 --> 01:07:45,520
Being a cooperative species made a big difference.

921
01:07:45,520 --> 01:07:51,960
One of the drivers of that was our amazing capacity of understanding the mind of the

922
01:07:51,960 --> 01:07:53,960
other.

923
01:07:53,960 --> 01:08:02,720
Understanding that somebody that is looking at me is suffering or is scared or is something

924
01:08:02,720 --> 01:08:04,400
that can create trouble.

925
01:08:04,400 --> 01:08:07,800
I can put myself in the mind of the other.

926
01:08:07,800 --> 01:08:15,080
When you combine all this stuff, we have a singularity plus a lot of other things like

927
01:08:15,080 --> 01:08:16,680
mental diseases.

928
01:08:16,680 --> 01:08:18,880
But it's a whole story.

929
01:08:18,880 --> 01:08:22,800
I don't want to spoil because as I was saying, one of the things I want to bring tomorrow

930
01:08:22,800 --> 01:08:30,320
is when you try to approach the complexities of the human mind, there's a kind of a synthetic

931
01:08:30,320 --> 01:08:46,120
or artificial path that can bring a lot of understanding.

932
01:08:46,120 --> 01:08:51,600
At one point, you were talking about ants, I think, and their colonies and that the

933
01:08:51,600 --> 01:09:00,080
more complex the society, the less complex the individuals needed to be.

934
01:09:00,080 --> 01:09:04,200
But we shouldn't apply that to humans.

935
01:09:04,200 --> 01:09:07,200
But you said we shouldn't apply that to humans.

936
01:09:07,200 --> 01:09:09,360
I think, why?

937
01:09:09,360 --> 01:09:18,320
Well, I mean, not every day I feel like the human race deserves good words.

938
01:09:18,320 --> 01:09:28,360
But individual humans are spectacularly complex, provided that you're being immersed in the

939
01:09:28,360 --> 01:09:29,360
cultural thing.

940
01:09:29,360 --> 01:09:34,240
Our brains are nothing unless you are in a society that goes from language to almost

941
01:09:34,240 --> 01:09:37,520
everything else.

942
01:09:37,520 --> 01:09:44,720
But I think that on the one hand, because of the evolutionary pressures that apply for

943
01:09:44,720 --> 01:09:54,360
ant colonies, that had to do with the warranty of the colony works, and eventually that reproduces.

944
01:09:54,360 --> 01:09:57,920
It's a different story for us in many ways.

945
01:09:57,920 --> 01:10:03,160
I also try to bring that tomorrow and make a good comparison.

946
01:10:03,160 --> 01:10:09,040
I must say that this trend we observed, but we still don't have a good theory for the

947
01:10:09,040 --> 01:10:11,480
complexity drain.

948
01:10:11,480 --> 01:10:17,960
But definitely, I remember that Michael Lachman, which was a faculty also at Santa Fe Institute

949
01:10:17,960 --> 01:10:23,560
one day, we were discussing about this, the brain of humans, and he brought me to something

950
01:10:23,560 --> 01:10:32,120
that is, in a way, is trivial, but it's interesting, is that you isolate a human from society and

951
01:10:32,240 --> 01:10:37,960
the brain, this extraordinary potential machine is worthless.

952
01:10:37,960 --> 01:10:44,880
You might survive, maybe, as some kids survive with wolves, but all the potential of the

953
01:10:44,880 --> 01:10:47,840
brain is never going to develop.

954
01:10:47,840 --> 01:10:53,720
So it again says something about the fact that we are also cultural animals, and that

955
01:10:53,760 --> 01:10:55,760
makes a difference.

956
01:10:55,760 --> 01:11:02,760
I understand the utilitarian and the intellectual reasons, like that you do experiments with

957
01:11:25,760 --> 01:11:33,760
animals on octopus, octopi, but aren't there ethical considerations on how we do all these

958
01:11:33,760 --> 01:11:37,520
studies on sentient beings that aren't us?

959
01:11:37,520 --> 01:11:44,640
Yeah, that's a very good point, and I'm sure you know that there's a hot topic these days.

960
01:11:44,640 --> 01:11:50,760
The more we know about some species, the more clearly we need to have ethic criteria of

961
01:11:50,760 --> 01:11:56,760
what is reasonable and what is not for the next experiments.

962
01:11:56,760 --> 01:12:02,760
Even if we will say that, because we don't know, to what extent we can talk about that

963
01:12:02,760 --> 01:12:05,760
sentience and consciousness, etc., we still don't know.

964
01:12:05,760 --> 01:12:12,760
But the precursors of a complex mind, clearly we are seeing it in many species, right?

965
01:12:12,760 --> 01:12:18,760
The elephants clearly mourn, they kind of feel the loss of others.

966
01:12:18,760 --> 01:12:23,760
In the octopi, we see that kind of extraordinary curiosity.

967
01:12:23,760 --> 01:12:25,760
What brings that there?

968
01:12:25,760 --> 01:12:27,760
So this is on the table.

969
01:12:27,760 --> 01:12:33,760
It's a hot discussion, because one thing, as you can imagine, is that to make you want

970
01:12:33,760 --> 01:12:39,760
decisions about what is ethically reasonable or not, we first need to actually have good

971
01:12:39,760 --> 01:12:44,760
definitions of whether or not you have sentience.

972
01:12:44,760 --> 01:12:47,760
Can these be measured in some way?

973
01:12:47,760 --> 01:13:02,760
So it's a good point and it's a relevant research problem now.

974
01:13:02,760 --> 01:13:04,760
I have a question too.

975
01:13:04,760 --> 01:13:07,760
Are you wondering about instinct?

976
01:13:07,760 --> 01:13:14,760
Is that also part of the brain or is the instinct separate from the brain?

977
01:13:14,760 --> 01:13:19,760
Which you have kind of not mentioned in your lecture so far.

978
01:13:19,760 --> 01:13:20,760
I don't know if you understand.

979
01:13:20,760 --> 01:13:22,760
You say, insect brains?

980
01:13:22,760 --> 01:13:27,760
The instinct, the instinct that living organisms have.

981
01:13:27,760 --> 01:13:35,760
I mean, is that part of the brain or is that something else?

982
01:13:35,760 --> 01:13:38,760
You mean for, you say, insect?

983
01:13:38,760 --> 01:13:39,760
Instinct.

984
01:13:39,760 --> 01:13:40,760
Oh, instinct.

985
01:13:40,760 --> 01:13:41,760
Sorry, sorry, sorry.

986
01:13:41,760 --> 01:13:48,760
But you have to, I mean, I came here, you don't know that, but I came straight from

987
01:13:48,760 --> 01:13:53,760
Barcelona, 26 hours, and I was brought here.

988
01:13:53,760 --> 01:13:57,760
My brain is not as good as it should.

989
01:13:57,760 --> 01:13:59,760
Yeah, actually, that's a good point.

990
01:13:59,760 --> 01:14:05,760
Instinct clearly is something that is part of the machinery, right?

991
01:14:05,760 --> 01:14:12,760
In many cases, you see that working in a very almost algorithmic way, right?

992
01:14:12,760 --> 01:14:25,760
This is a famous example of these wasps that kill, no, sorry, that use their stings to

993
01:14:25,760 --> 01:14:30,760
actually put the eggs inside prey that are paralyzed, right?

994
01:14:30,760 --> 01:14:33,760
Could be spiders, could be something else.

995
01:14:33,760 --> 01:14:42,760
And they first make a hole, and then they hunt, they take, I don't know, could be larvae

996
01:14:42,760 --> 01:14:44,760
or whatever it is.

997
01:14:44,760 --> 01:14:49,760
They go there, they leave the prey there, which is paralyzed.

998
01:14:49,760 --> 01:14:52,760
They go inside, check out that everything is okay.

999
01:14:52,760 --> 01:14:56,760
Go out, take the larvae, put it inside.

1000
01:14:56,760 --> 01:15:02,760
But then what happens is, if you, while this is happening, imagine that you are a mean entomologist.

1001
01:15:03,760 --> 01:15:11,760
And you are looking, and then when it goes inside, you move the prey, which is paralyzed, right?

1002
01:15:11,760 --> 01:15:12,760
Somewhere else.

1003
01:15:12,760 --> 01:15:17,760
So you see the wasps coming out, finding out that something has changed.

1004
01:15:17,760 --> 01:15:19,760
They kind of managed to touch a little bit.

1005
01:15:19,760 --> 01:15:22,760
They go inside again.

1006
01:15:22,760 --> 01:15:26,760
And if you can repeat that operation as much as you want.

1007
01:15:26,760 --> 01:15:30,760
It's like an algorithm which clearly goes from instinct, right?

1008
01:15:30,760 --> 01:15:36,760
But on the other hand, in insects, we also been finding in the last 20 years many unexpected things.

1009
01:15:36,760 --> 01:15:40,760
Like some wasps recognize the face of others.

1010
01:15:40,760 --> 01:15:44,760
There are asymmetries, apparently, in the brains of some ants.

1011
01:15:44,760 --> 01:15:48,760
So it's plenty of still of things we need to know.

1012
01:15:51,760 --> 01:15:56,760
So it was a question here, and another there.

1013
01:15:57,760 --> 01:16:02,760
You defined, you defined some really interesting questions that are very simple.

1014
01:16:02,760 --> 01:16:05,760
Like why brains, and what kinds of brains.

1015
01:16:05,760 --> 01:16:12,760
But I didn't hear you say what constitutes being a brain.

1016
01:16:12,760 --> 01:16:15,760
Like what's your definition of a brain?

1017
01:16:15,760 --> 01:16:17,760
So how do you define a brain?

1018
01:16:17,760 --> 01:16:18,760
Okay.

1019
01:16:18,760 --> 01:16:23,760
I try to avoid that question, right?

1020
01:16:24,760 --> 01:16:38,760
I mean, in a simple way, a brain is a collective of neurons that centralizes the activity of an organism, right?

1021
01:16:38,760 --> 01:16:43,760
So for a hydra, for example, you have a net of neurons,

1022
01:16:43,760 --> 01:16:47,760
or for a jellyfish, you have a ring and a distributed thing.

1023
01:16:47,760 --> 01:16:55,760
But you don't have really a core of neurons that are playing a role of kind of integrating information, right?

1024
01:16:55,760 --> 01:17:03,760
So that would be a kind of, I think, a potential definition, but it's disputed, right?

1025
01:17:03,760 --> 01:17:06,760
I think for me, it's satisfactory.

1026
01:17:07,760 --> 01:17:24,760
When you were showing the example of the ants who managed to figure out the shortest route to the food,

1027
01:17:24,760 --> 01:17:33,760
or the termites who end up creating something that is a benefit to the society,

1028
01:17:33,760 --> 01:17:41,760
or the single-celled organisms that find the shortest route to the flakes in the maze,

1029
01:17:41,760 --> 01:17:49,760
it seems like more often than not, in these experiments, you have similar sort of outcomes

1030
01:17:49,760 --> 01:17:55,760
that the ants are successful in doing this, the termites are successful in doing this,

1031
01:17:55,760 --> 01:18:00,760
and the single-celled organisms are successful in doing this.

1032
01:18:00,760 --> 01:18:08,760
But when you take humans acting like a liquid brain,

1033
01:18:08,760 --> 01:18:17,760
they often come out to solutions that are harmful to the organism as a whole,

1034
01:18:17,760 --> 01:18:29,760
especially in the area of finance, where if you have a lot of individuals here coming to conclusions,

1035
01:18:29,760 --> 01:18:37,760
they're more likely to come to this wrong conclusion, which ends up making a few people very rich

1036
01:18:37,760 --> 01:18:45,760
and other people very poor, and I'm puzzled by this.

1037
01:18:45,760 --> 01:18:48,760
Okay.

1038
01:18:48,760 --> 01:18:58,760
Well, it's not an easy question, so it requires a long answer, so close the doors.

1039
01:18:58,760 --> 01:19:01,760
No, I am now seriously.

1040
01:19:01,760 --> 01:19:04,760
Yeah, when we go into humans, it's interesting to see two things.

1041
01:19:04,760 --> 01:19:15,760
One is what you mentioned, because humans, as you move beyond pure commission and you have society,

1042
01:19:15,760 --> 01:19:21,760
with all the biases that some can deal with, incomplete information,

1043
01:19:21,760 --> 01:19:24,760
plus the big problems we have about polarization and everything,

1044
01:19:24,760 --> 01:19:29,760
which is something that in complex systems we want to solve, but it looks like very, very difficult.

1045
01:19:29,760 --> 01:19:37,760
But on the other hand, it's just a small part of research, but it's interesting for the insight it brings.

1046
01:19:37,760 --> 01:19:47,760
When humans are under the situation of panic, you can make very well-defined models,

1047
01:19:47,760 --> 01:19:54,760
which essentially is like particles moving by forces and amplifying phenomena.

1048
01:19:54,760 --> 01:19:56,760
Things that we see and have consequences.

1049
01:19:56,760 --> 01:20:05,760
For example, in a stadium, you have two exits, and this has happened unfortunately a number of times.

1050
01:20:05,760 --> 01:20:07,760
It's panic.

1051
01:20:07,760 --> 01:20:08,760
It's a panic attack.

1052
01:20:08,760 --> 01:20:10,760
We don't reason there.

1053
01:20:10,760 --> 01:20:12,760
We just want to survive.

1054
01:20:12,760 --> 01:20:21,760
A typical trend is that the more people go into one exit, the more people go there, which of course is harmful,

1055
01:20:21,760 --> 01:20:23,760
because there's another exit.

1056
01:20:23,760 --> 01:20:31,760
You can model that, and it's interesting to see that you do the same experiment of panic with ants and with humans,

1057
01:20:31,760 --> 01:20:35,760
and that's one place where you can compare.

1058
01:20:35,760 --> 01:20:39,760
Very unfortunately, we behave like ants.

1059
01:20:39,760 --> 01:20:43,760
If we move beyond that, and it's a legitimate question,

1060
01:20:43,760 --> 01:20:51,760
under what conditions we can actually be described as systems like collective intelligence in ants,

1061
01:20:51,760 --> 01:20:59,760
and what is the threshold that separates from that and gets into society and all the conflicts that you're mentioning?

1062
01:20:59,760 --> 01:21:02,760
But this is a whole area of research.

1063
01:21:02,760 --> 01:21:04,760
So, good point.

1064
01:21:04,760 --> 01:21:13,760
One thing that we need to actually figure out is to what extent these ideas of collective intelligence apply to humans.

1065
01:21:13,760 --> 01:21:23,760
And if theory develops, whether we could actually explore the insight of the theory to actually help us to exploit common knowledge,

1066
01:21:23,760 --> 01:21:25,760
that clearly now is underexploited.

1067
01:21:25,760 --> 01:21:29,760
So maybe a third lecture on third day?

1068
01:21:29,760 --> 01:21:33,760
Thirdly, you have to be in Barcelona.

1069
01:21:33,760 --> 01:21:49,760
Okay, we have time for one more question.

1070
01:21:49,760 --> 01:21:56,760
Hi, I was interested in your chart where you have your three dimensions,

1071
01:21:56,760 --> 01:22:00,760
and you have this big sphere of the unknown or the unexplored,

1072
01:22:00,760 --> 01:22:04,760
and I wanted to know, it was like, yes, that's the one.

1073
01:22:04,760 --> 01:22:13,760
So, what we're missing is cognitively complex and developmentally complex as well, or maybe that doesn't matter.

1074
01:22:13,760 --> 01:22:17,760
But I'm wondering what you think that could look like.

1075
01:22:17,760 --> 01:22:22,760
You said that we might be able to manufacture it, something in that space.

1076
01:22:22,760 --> 01:22:24,760
I said what, sorry.

1077
01:22:24,760 --> 01:22:32,760
I thought I caught you saying, sneakily, maybe that in this unexplored space,

1078
01:22:32,760 --> 01:22:39,760
maybe that's where we have space to manufacture some sort of brain-like thing.

1079
01:22:39,760 --> 01:22:43,760
I'm just wondering what you think is in that space that we're missing.

1080
01:22:43,760 --> 01:22:50,760
Okay, if you come to the second lecture, I think I have the answer for that.

1081
01:22:50,760 --> 01:22:54,760
I don't want to make more spoilers.

1082
01:22:54,760 --> 01:22:56,760
Alright, thank you so much.

