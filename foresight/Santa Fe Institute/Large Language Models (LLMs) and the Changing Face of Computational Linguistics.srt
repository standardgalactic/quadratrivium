1
00:00:00,000 --> 00:00:08,160
Perhaps Catherine should just introduce ourselves. So we were at Brown University for several decades

2
00:00:08,160 --> 00:00:13,840
in the Department of Cognitive and Holistic Science there. I was also joined in Computer Science.

3
00:00:14,880 --> 00:00:22,080
Moved to Australia about 14 years ago to Macquarie University and at the same time I think we started

4
00:00:22,080 --> 00:00:31,600
becoming external faculty here at SFI. Catherine got a gigantic mega grant that required hiring a

5
00:00:31,600 --> 00:00:38,400
lot of people doing a lot of work there. I started going into the startup route

6
00:00:39,840 --> 00:00:45,360
and then what with that and then with COVID I think we let the external faculty connection lapse

7
00:00:46,160 --> 00:00:52,160
but now we'd like to restart the connection with SFI again.

8
00:00:54,560 --> 00:00:58,240
The startup stuff that I was doing was actually in the chatbot space.

9
00:01:00,240 --> 00:01:05,600
We got acquired by Oracle Corporation. That's the reason for this disclaimer down here at

10
00:01:05,600 --> 00:01:12,000
the bottom right. So nothing that I'm saying represents anything of any of our employers.

11
00:01:12,720 --> 00:01:17,760
The other reason for this comment here is because the whole field of large language

12
00:01:17,760 --> 00:01:25,200
bottles is really changing incredibly quickly. I don't know how much of what I'm going to say

13
00:01:25,200 --> 00:01:32,080
right now is going to be true or relevant in six months time. So nothing, I'm not saying anything

14
00:01:32,080 --> 00:01:38,640
that I believe is false but it could well, some of the effort could well be wasted.

15
00:01:38,960 --> 00:01:48,640
So I'm going to try and give you guys the high level executive overview talk. This talk was

16
00:01:48,640 --> 00:01:54,480
really actually aimed originally at academic researchers in natural language processing

17
00:01:54,480 --> 00:02:01,120
and so I'm going to skip some of those some of those details but you know you had to have been

18
00:02:01,120 --> 00:02:06,800
underneath a rock if you didn't notice the large language models who radically changed the field

19
00:02:06,800 --> 00:02:16,320
in the last year or two and so I'll be talking about that. We'll also be talking about suggestions.

20
00:02:16,320 --> 00:02:22,640
You know how should academics actually respond to this changing world? So we'll be talking a little

21
00:02:22,640 --> 00:02:28,960
bit about what these large language models may mean or may not mean for human language acquisition,

22
00:02:29,040 --> 00:02:36,480
the study of that. There's a lot of interest in neuro-symbolic models that integrate large

23
00:02:36,480 --> 00:02:43,200
language models and more traditional kinds of AI models and I'll also talk a little bit about

24
00:02:43,200 --> 00:02:52,800
alignment of LLMs and I'll end talking a bit about the connections between large language models

25
00:02:52,800 --> 00:02:59,680
and the social implications of them and lessons from the first industrial revolution.

26
00:03:01,040 --> 00:03:06,240
Katherine and I just spent two months at the University of Edinburgh and it was very interesting

27
00:03:06,240 --> 00:03:11,360
there because of course the first industrial revolution really happened in Northern England

28
00:03:11,360 --> 00:03:20,320
and around Scotland and actually it's a fascinating thing to read about that you know James Watt,

29
00:03:20,320 --> 00:03:25,280
you know the inventor of the steam engine actually did a lot of his work at the University of Glasgow

30
00:03:26,320 --> 00:03:30,880
so that's sort of really interesting. Okay so how deep learning changes NLP?

31
00:03:33,840 --> 00:03:38,720
You know people like Katherine and I have been in the field long enough to have actually seen

32
00:03:38,720 --> 00:03:44,800
really a switch from symbolic parameters, statistical NLP and then finally now to deep

33
00:03:44,800 --> 00:03:52,480
learning and large language models and I guess I would say that the really huge difference is that

34
00:03:52,480 --> 00:04:00,800
large language models can understand context much much longer contexts than anything that I thought

35
00:04:00,800 --> 00:04:07,520
was even possible. In fact I actually thought there were good theoretical arguments that it should

36
00:04:07,520 --> 00:04:13,760
not be possible to try and model a context as long as a sentence or a dialogue or an entire

37
00:04:13,760 --> 00:04:20,960
paragraph yet large language models can do that perfectly well and if you want to I can go through

38
00:04:20,960 --> 00:04:30,080
what those counting arguments would be but just you know when I actually look back at the

39
00:04:30,080 --> 00:04:36,640
pre-deep learning work you know I did a lot of work on syntactic parsing which would involve

40
00:04:36,640 --> 00:04:42,720
recovering pastries that looked something like this and I think the motivation for dealing with

41
00:04:43,440 --> 00:04:52,560
you know wanting structures like these these things make local a lot of dependencies which

42
00:04:52,560 --> 00:05:00,000
are non-local in the string here right so you know it's a flight through Denver right so

43
00:05:00,800 --> 00:05:07,200
this relationship here is making that local the same thing also we're talking about a preference

44
00:05:07,200 --> 00:05:13,680
for a flight for example that's making that dependency local why do you want to do that you

45
00:05:13,680 --> 00:05:21,040
want to do that if you think that you can only really model pairs pairwise interactions large

46
00:05:21,040 --> 00:05:27,840
language models as I said are happy modeling contexts of you know in the hundreds or the

47
00:05:27,840 --> 00:05:34,880
thousands of tokens and so I actually think you know these theories are correct but they're just

48
00:05:34,880 --> 00:05:40,160
no longer required so to do natural language understanding I don't actually have to recover

49
00:05:40,160 --> 00:05:46,800
this sort of structure anymore okay so strengths of large language models we just talked about

50
00:05:46,800 --> 00:05:53,680
how there's so much better at working with incredibly long contexts I think the other you

51
00:05:53,680 --> 00:06:00,320
know I'm giving you the executive summary here the other really amazing thing is that you know

52
00:06:00,320 --> 00:06:05,440
if you showed these large language models to people from the first neural network revolution

53
00:06:05,440 --> 00:06:12,640
from the 1980s actually everything would be really quite familiar to them right there's

54
00:06:12,640 --> 00:06:19,600
there's a slight twist with the attention mechanism but you could explain that in about 15 minutes

55
00:06:19,600 --> 00:06:26,000
and those guys understand it all you know and Sutton has this thing which he calls the bitter

56
00:06:26,080 --> 00:06:32,720
lesson which is that you know for people like me who work on lots of really interesting algorithms

57
00:06:32,720 --> 00:06:38,640
and models and structures and you know things like that actually really none of it really

58
00:06:38,640 --> 00:06:46,960
seemed to be relevant if you just got data and compute and scaled everything up that that really

59
00:06:46,960 --> 00:06:52,560
seems to be the secret and I actually do think that these large language models are largely just

60
00:06:52,640 --> 00:06:58,640
sort of brute forcing linguistic generalization so they're covering rather than really capturing them

61
00:06:59,200 --> 00:07:04,080
but it's really interesting how well they work right that in fact actually with enough compute

62
00:07:04,080 --> 00:07:11,440
and enough data you do seem to be able to produce something that's really quite amazing okay weaknesses

63
00:07:11,440 --> 00:07:17,200
of large language models I mean I think this is probably everybody here's already heard about

64
00:07:17,200 --> 00:07:23,520
this stuff you know Emily Bender's stochastic parrots you know or as I guess Gertrude Stein would

65
00:07:23,520 --> 00:07:30,000
say there's no there there and I think actually these large language models just you know they

66
00:07:30,000 --> 00:07:37,040
respond reflexively in fact I actually think if we're talking about human models these large

67
00:07:37,040 --> 00:07:43,280
language models maybe actually aren't such so bad for the purely reflexive processing that maybe

68
00:07:43,280 --> 00:07:50,080
humans might be able to do but scaled up you know whereas humans maybe have a context window

69
00:07:50,080 --> 00:07:54,960
of five or something like that these large language models have a context window of five

70
00:07:54,960 --> 00:08:02,000
thousand for example and I think actually that's also because they have no beliefs desires or

71
00:08:02,000 --> 00:08:11,440
intentions they have no you know guiding you know thoughts that's why hallucinations such a big

72
00:08:11,440 --> 00:08:19,120
problem you know they're just trying to produce plausible output I think it's really interesting

73
00:08:19,120 --> 00:08:25,040
to actually wonder about you know these large language models now becoming multimodal how will

74
00:08:25,040 --> 00:08:30,880
that change things I actually think if you think the fundamental problem is a problem of symbol

75
00:08:30,880 --> 00:08:39,440
grounding then arguments like Searle's Chinese room argument still are just as valid against

76
00:08:39,440 --> 00:08:45,200
multimodal input on the other hand I actually think the multimodal inputs incredibly interesting

77
00:08:45,200 --> 00:08:51,040
might be incredibly powerful yes so naive questions what is multimodal training oh so here

78
00:08:51,040 --> 00:08:56,720
what I really think is vision and language I should have made that clear vision and language

79
00:08:58,160 --> 00:09:05,360
but these days in fact people are now starting to train off movies as well right so that's I mean

80
00:09:05,360 --> 00:09:11,760
arguably that is one big difference between these large language models and

81
00:09:14,800 --> 00:09:19,600
you know the way humans might work right and what is Searle's Chinese room

82
00:09:21,200 --> 00:09:28,480
that's a really interesting thought experiment which I think for reasons of time I might just

83
00:09:28,560 --> 00:09:36,880
skip here but it's a it's basically where he asks he asks us to imagine a blind symbol

84
00:09:36,880 --> 00:09:42,720
following system and he says so can you really do you really want to say that this blind symbol

85
00:09:42,720 --> 00:09:48,880
for you know rule following system actually has any understanding it doesn't really seem to make

86
00:09:48,880 --> 00:09:58,080
sense to attribute any any understanding to it anyway I do actually think you know there's a lot

87
00:09:58,080 --> 00:10:03,440
of people now that are also wondering are these large language models are they really intelligent

88
00:10:04,160 --> 00:10:11,520
could they be a AGI artificial general intelligence that maybe might take over the entire world

89
00:10:12,240 --> 00:10:19,040
I think they currently can't because they do lack you know beliefs desires intentions the ability to

90
00:10:19,040 --> 00:10:25,440
form long-term memories for example but I'm I'm actually not so sure that that's such a huge

91
00:10:25,520 --> 00:10:33,200
technological barrier I actually do think that it's possible that that could be relatively easy

92
00:10:33,200 --> 00:10:39,520
now you know we've been in this situation before where we thought oh the thing that's really missing

93
00:10:39,520 --> 00:10:45,520
from this machine being intelligent is x you add x to the machine and all of a sudden you discover

94
00:10:45,520 --> 00:10:51,040
well actually you know there's this other thing why that's missing as well and so that may happen

95
00:10:51,040 --> 00:10:58,400
here too but there is a chance that in fact if we just added episodic memory to these large language

96
00:10:58,400 --> 00:11:04,880
models then they may actually become intelligent things the other okay another high level point

97
00:11:04,880 --> 00:11:11,680
that I want to make is that these large language models are manufactured systems in the same way

98
00:11:11,680 --> 00:11:16,560
remember I was talking about the first industrial revolution in the same way that a steam engine

99
00:11:16,640 --> 00:11:22,400
is a very highly engineered manufactured system you know you wouldn't want to be trying to

100
00:11:23,040 --> 00:11:29,040
infer the laws of thermodynamics or even the ideal gas laws by examining a steam engine

101
00:11:29,760 --> 00:11:36,080
I think the same thing's true also about these large language models you know they're they're

102
00:11:36,080 --> 00:11:43,520
trained in a way which is really an engineered product you know and certainly as somebody being

103
00:11:43,520 --> 00:11:50,160
in industry I can tell you that you know the people in industry that are building these things

104
00:11:50,160 --> 00:11:54,560
really don't care about doing science they're trying to build a product that they can sell

105
00:11:54,560 --> 00:12:06,240
and that's really what they care about okay so just an aside about you know the relationship

106
00:12:06,320 --> 00:12:16,080
between I I actually think a natural language understanding and say you know the science of

107
00:12:16,080 --> 00:12:23,360
of linguistics or language acquisition so there's the scientific side and the technological side

108
00:12:23,360 --> 00:12:29,280
I think the technology is currently outstripping the science and I think that has happened in

109
00:12:29,280 --> 00:12:33,440
times before in fact I think it actually happened with the first industrial revolution

110
00:12:33,440 --> 00:12:40,720
so I won't spend too much time here but my understanding is and maybe there's people

111
00:12:41,280 --> 00:12:48,080
historians of science in this room that know more about this than I do but that you know

112
00:12:48,080 --> 00:12:54,000
really in fact actually people started to invent steam engines long before they had the scientific

113
00:12:54,000 --> 00:13:00,720
understanding first of all of ideal gas laws and then ultimately right through thermodynamics

114
00:13:00,720 --> 00:13:07,360
and statistical mechanics that took centuries to the scientific for the scientific side to

115
00:13:08,080 --> 00:13:14,480
emerge and in fact actually ideas of things like entropy really actually had to be developed

116
00:13:15,280 --> 00:13:21,680
to answer questions about why it was not possible to build steam engines above a certain level of

117
00:13:21,680 --> 00:13:28,960
efficiency for example and I suspect the same thing may be true today that our science of

118
00:13:29,040 --> 00:13:40,160
say language and psychology is actually behind the technology. Okay one of the things that I

119
00:13:40,160 --> 00:13:45,520
actually quite like is you know this comment here natural language is the new programming language

120
00:13:47,760 --> 00:13:55,280
and that yeah I mean certainly for those of us in industry LLMs are really changing the way in

121
00:13:55,280 --> 00:14:04,560
which we do our work right whereas it used to take a team of real experts to build say for

122
00:14:04,560 --> 00:14:09,680
example a device which would identify all the financial products that are mentioned

123
00:14:10,800 --> 00:14:16,480
you know in a particular document. Now you can just simply ask a large language model to do that

124
00:14:16,480 --> 00:14:24,080
for you and it does a pretty good job maybe not as good as the very best hand-built natural language

125
00:14:24,080 --> 00:14:30,720
processing system so those are still actually better but you know they take months or years to

126
00:14:30,720 --> 00:14:39,440
develop whereas it takes you know maybe hours to use a new large language model so I actually think

127
00:14:39,440 --> 00:14:44,640
that particularly in terms of the commercial implications the commercial deployment of natural

128
00:14:44,640 --> 00:14:52,080
language processing in industry that's going to change completely. It's not clear we'll need nearly

129
00:14:52,080 --> 00:14:59,360
as many experts in natural language understanding for example for the industrial applications.

130
00:15:01,760 --> 00:15:08,000
I did want to mention a little bit you know I think that one of the really interesting

131
00:15:09,520 --> 00:15:13,760
things that's happening in the field is taking these large language models

132
00:15:13,760 --> 00:15:21,760
and then combining them with other components. The first component that people started to look at

133
00:15:21,760 --> 00:15:28,160
was combining large language models with what's called a vector store or a retrieval system

134
00:15:28,880 --> 00:15:36,880
and that's just simply something whereby when you ask a question instead of just directly asking

135
00:15:36,880 --> 00:15:44,240
the large language model to respond to that question you retrieve a set of relevant documents

136
00:15:44,240 --> 00:15:49,520
to that question feed those in as part of the input to the large language model you can see

137
00:15:49,520 --> 00:15:54,400
that's what I'm suggesting that you do over here and then you then tell the large language model

138
00:15:54,400 --> 00:16:02,320
to use to produce an answer that just simply references those documents and that's sometimes

139
00:16:02,320 --> 00:16:12,880
called the reader retrieval model or retrieval augmented generation. That idea can get even

140
00:16:12,880 --> 00:16:18,160
more power when you start to think well maybe in fact the large language model can actually decide

141
00:16:19,120 --> 00:16:24,560
what information to do a search for and then when you then started to think well should it

142
00:16:24,560 --> 00:16:29,440
decide what information to do a search for maybe in fact it could also call other tools

143
00:16:30,080 --> 00:16:35,600
so these large language models are infamous for not being able to do numerical calculations very

144
00:16:35,600 --> 00:16:40,960
well but maybe in fact what we should be doing is giving the large language model the ability

145
00:16:40,960 --> 00:16:47,040
to call a calculator and there's just in the same ways which if I was to ask any of you guys to do

146
00:16:47,120 --> 00:16:52,880
a complex task and involve something some numeric calculation I'd want you to be

147
00:16:53,920 --> 00:17:00,400
also using a calculator rather than trying to do it longhand. Okay so in terms of research

148
00:17:00,400 --> 00:17:08,080
directions inside of an LLM world so the very first comment to make is that it is very challenging

149
00:17:08,080 --> 00:17:14,240
for academics to do research in large language models you know the ideal thing would be to have

150
00:17:14,240 --> 00:17:22,400
something like an ideal gas experiment set up but and you certainly can build small

151
00:17:23,200 --> 00:17:29,840
versions of these large language models there's some disagreement about whether or not though

152
00:17:29,840 --> 00:17:36,880
whether there's well there certainly seem to be emergent capabilities so the bigger the model

153
00:17:36,880 --> 00:17:42,240
the more things that it can do there's big arguments about whether or not this emergence

154
00:17:42,320 --> 00:17:52,560
is like a phase change or whether it's really more incremental and I again I'd be happy to talk

155
00:17:52,560 --> 00:17:59,440
about that later we could spend hours talking a bit about that but I think there's enough

156
00:18:00,320 --> 00:18:06,160
lack of clarity about what emergence is but if you wanted to do academic research you really

157
00:18:06,160 --> 00:18:12,080
do want to get access to you know the large language the larger large language models

158
00:18:12,800 --> 00:18:17,680
and the problem is that these you know the best large language models are really complicated

159
00:18:17,680 --> 00:18:26,000
commercial products as we were talking about before and it's what's actually even worse

160
00:18:26,000 --> 00:18:32,800
is that these days for in you know proprietary commercial reasons the companies aren't even

161
00:18:32,800 --> 00:18:38,160
actually telling us all the details of exactly what they're doing so that actually does make

162
00:18:38,160 --> 00:18:45,200
it very hard for academics to really do any sort of academic research in in our own papers

163
00:18:45,200 --> 00:18:51,680
you know I'm collaborating with people at the University of Edinburgh what we wind up doing

164
00:18:51,680 --> 00:19:01,360
is saying we're actually not going to test the closed commercial systems but we will work with

165
00:19:01,360 --> 00:19:07,760
the largest open source systems that are available I think that's not a bad thing to do but it does

166
00:19:07,760 --> 00:19:13,840
mean that you're cutting yourself off from a lot of the really cool systems there yes

167
00:19:14,880 --> 00:19:21,120
this is a very interesting question on that slide about how quickly they degrade as you move just

168
00:19:21,120 --> 00:19:27,120
as we moved from commercial ones to smaller and smaller ones yes just a steady degradation

169
00:19:27,200 --> 00:19:34,000
or is there a sudden drop so sorry and in fact actually this also gets back to the emergence

170
00:19:34,000 --> 00:19:40,400
question so let me just say I actually do think that a lot depends on exactly how you measure it

171
00:19:41,040 --> 00:19:49,600
so I don't have to tell people particularly at the Santa Fe Institute right that quite often

172
00:19:49,600 --> 00:19:55,120
what you'll actually see is a lot of small changes all of a sudden reaching a tipping point

173
00:19:55,120 --> 00:20:01,280
that is basically like a phase change and you know when you think about these large language models

174
00:20:01,280 --> 00:20:08,400
I mean they generate output token by token if the correct answer is just slightly less probable

175
00:20:08,400 --> 00:20:14,560
than some mistake right well then as the output gets very very long then the correct answer can be

176
00:20:14,560 --> 00:20:21,920
incredibly improbable right so if you're just looking at the output of 100 tokens or more

177
00:20:22,880 --> 00:20:26,480
you're just looking at the output you're just asking is the output right or wrong you'll go

178
00:20:26,480 --> 00:20:33,360
wrong wrong wrong wrong wrong right and then all of a sudden as the correct token probability

179
00:20:33,360 --> 00:20:40,560
just nudges above the incorrect tokens you know all of a sudden the output flips and all of a

180
00:20:40,560 --> 00:20:47,200
sudden it's just magically all correct but if you measure the per token probabilities for example

181
00:20:47,280 --> 00:20:50,160
then you'd actually discover a much more continuous change

182
00:20:53,200 --> 00:20:57,760
you know so I think that's actually where a lot of emergence happens and in fact that's I think

183
00:20:58,480 --> 00:21:03,760
the white there's a little academic dispute about whether or not these models have emergent

184
00:21:03,760 --> 00:21:08,400
behavior or not and that's at least my understanding of how you'd resolve that

185
00:21:10,560 --> 00:21:15,360
so I actually do think that a lot of the ideas that people here would have would actually be

186
00:21:15,360 --> 00:21:23,920
very useful for the community to have as well okay all right you know I actually think there's

187
00:21:23,920 --> 00:21:29,600
lots of really interesting questions also you know can we understand what these large language

188
00:21:29,600 --> 00:21:36,160
models are really doing I think it's you know I mean we know something about how language is

189
00:21:36,160 --> 00:21:44,080
processed in the human brain you know we know that none of these models really are realistic

190
00:21:46,480 --> 00:21:51,680
just even understanding you know what these large language models are doing how can you

191
00:21:52,320 --> 00:21:58,160
be sure that they know a syntactic rule or make it even simpler that they know a particular word

192
00:21:58,160 --> 00:22:06,400
so right now and in fact actually I think another really interesting question is

193
00:22:07,600 --> 00:22:14,240
if if these large language models are basically just you know gigantic neural nets as I said before

194
00:22:14,240 --> 00:22:20,800
of a relatively generic type why is it that only human beings can acquire language are they are

195
00:22:20,800 --> 00:22:26,000
you know why is it humans are the only animals that can acquire language right I mean you know

196
00:22:26,000 --> 00:22:30,960
we don't have the biggest brains there are animals with bigger brains if it's just merely

197
00:22:32,400 --> 00:22:36,320
the size you know the number of neurons that we have sitting inside of our skulls

198
00:22:36,880 --> 00:22:43,120
if that's all that determines our ability to do something like learn language why why don't

199
00:22:43,120 --> 00:22:50,160
other animals why don't they have that ability it's very popular now to talk about analyzing

200
00:22:50,160 --> 00:22:56,240
large language models using psychological or psychological linguistic methods I think that's

201
00:22:56,240 --> 00:23:02,640
about the best that I know how to do but I think a lot of these methods were really designed to

202
00:23:02,640 --> 00:23:09,840
work on humans at least agents that have beliefs and again you know in the sense a large language

203
00:23:09,840 --> 00:23:19,760
model doesn't have a belief it's just got reflexes okay so you know just to emphasize

204
00:23:19,760 --> 00:23:26,960
the differences between large language models and and humans right so you know children start

205
00:23:26,960 --> 00:23:35,440
and end learning from much smaller data sets they generalize in particular ways that we actually

206
00:23:35,440 --> 00:23:42,400
understand to new unseen forms I think we don't really know actually how these large language

207
00:23:42,400 --> 00:23:48,400
models generalize I mean they do I think they do generalize but it's very difficult to tell exactly

208
00:23:48,400 --> 00:23:54,880
how they generalize children also actually over generalizing characteristically so Catherine's

209
00:23:54,880 --> 00:24:00,400
an expert in this area but you know these are just a couple of examples that she pointed out you know

210
00:24:00,400 --> 00:24:07,920
where children have taken irregular verbs and either inflicted them in a regular fashion or

211
00:24:07,920 --> 00:24:17,360
over generalize the irregular form she's giggling me you know that makes sense if you think of

212
00:24:17,360 --> 00:24:26,400
giggling as being a verb a bit like tickling for example language learning you know by the time

213
00:24:26,400 --> 00:24:32,640
you're three or four you're a competent speaker of your native language usually but then there's

214
00:24:32,640 --> 00:24:40,560
also some part of language learning that's not really complete until the early teens right and

215
00:24:40,560 --> 00:24:46,400
then you know just in terms of the pragmatics of doing research on large language models right

216
00:24:47,040 --> 00:24:53,280
the time scale of research projects are different so it might take a couple of years for a student

217
00:24:53,280 --> 00:25:00,960
to do a research project studying say human language learning if they're studying something

218
00:25:00,960 --> 00:25:09,920
which was inspired by GPT-4 well in two years time we're probably in GPT-6 you know and the

219
00:25:10,400 --> 00:25:13,920
the the inspiration might be actually sort of completely different

220
00:25:16,960 --> 00:25:23,600
I also this is essentially that same thing as as I was saying before right so evaluation and

221
00:25:23,600 --> 00:25:29,760
testing I think is really a huge challenge that was always difficult inside of natural

222
00:25:29,760 --> 00:25:35,920
language processing but it gets even worse because the inputs to large language models

223
00:25:35,920 --> 00:25:41,360
now instead of just again because this context is so much longer the input is not just a

224
00:25:41,360 --> 00:25:46,880
single sentence it's an entire conversation or entire story or something else like that

225
00:25:47,520 --> 00:25:55,920
so if you want to really evaluate the performance of one of these systems you want to vary not just

226
00:25:55,920 --> 00:26:02,800
you know the last sentence you want to vary the entire context as well from a commercial point of

227
00:26:02,800 --> 00:26:10,080
view I think actually testing is really super important I mean you know you've probably all saw

228
00:26:10,800 --> 00:26:21,120
the Microsoft Bing chatbot which when it was you know the New York Times reporter studied

229
00:26:21,840 --> 00:26:28,080
that chatbot and it announced to the reporter that in fact actually that it preferred to be called

230
00:26:28,080 --> 00:26:35,600
Sydney rather than Bing and then also suggested that really you know the reporter should divorce

231
00:26:35,600 --> 00:26:43,920
his wife I'm sure behaviors that Microsoft was really not too proud of right and I actually

232
00:26:43,920 --> 00:26:50,720
think for commercial purposes it's super important to be able to detect and you know

233
00:26:51,600 --> 00:26:57,360
guaranteed that such behaviors really aren't lurking beneath the surface of your large language

234
00:26:57,360 --> 00:27:07,200
model. Constraint decoding that's just an NLP topic I won't spend too much time on but I actually

235
00:27:07,200 --> 00:27:11,680
do think that there's really interesting work there to look at different ways of actually

236
00:27:11,680 --> 00:27:17,600
constraining the output of a large language model and some real challenges there I think there's

237
00:27:17,600 --> 00:27:24,160
really interesting work about how one actually trains these models as well so I mentioned that the

238
00:27:24,160 --> 00:27:29,360
training procedure is itself actually a very complicated one typically what happens is that

239
00:27:29,360 --> 00:27:34,320
they start up by training with what's called the language model training objective which is where

240
00:27:34,320 --> 00:27:40,960
effectively you're just simply training the model to predict the very next word but a model which

241
00:27:40,960 --> 00:27:47,040
is just simply trained with this large language model training objective on its own doesn't really

242
00:27:47,040 --> 00:27:52,320
engage in useful conversation doesn't really follow instructions very well so it's actually

243
00:27:52,320 --> 00:28:01,200
very typical to follow that up with an additional training step that is you know that involves well

244
00:28:01,200 --> 00:28:06,320
as I said there's reinforcement learning with human feedback and I think that's a really interesting

245
00:28:06,320 --> 00:28:13,040
question I've actually got some theories myself about you know when you want to use one sort of

246
00:28:13,040 --> 00:28:18,400
training objective versus another and if there's people that'll like to talk about that more generally

247
00:28:18,400 --> 00:28:24,000
I think there's a really interesting question which is how do you align the LLM behavior

248
00:28:25,360 --> 00:28:31,120
with well how do you get the LLM to behave the way that you want it to right so you've got these

249
00:28:31,120 --> 00:28:36,240
very general alignment goals like you know follow commands that run right through to don't destroy

250
00:28:36,240 --> 00:28:43,040
humanity so ultimately it's the training data and the training training procedure which is going to

251
00:28:43,040 --> 00:28:51,840
determine the LLM behavior so how exactly do we do that right and I actually do think that there's

252
00:28:51,840 --> 00:28:56,960
good academic research that can be done there largely because the fine-tuning step that I mentioned

253
00:28:56,960 --> 00:29:03,600
this sort of multi-stage training that's pretty modest right tens of thousands of examples or less

254
00:29:05,600 --> 00:29:12,560
and it can be done on sort of fairly modest hardware so I actually think that's a that's

255
00:29:12,560 --> 00:29:19,600
that's a great academic research topic I feel a little bit guilty here because I just mentioned

256
00:29:19,600 --> 00:29:26,800
to you that maybe the only thing which is standing between us and you know a artificial general

257
00:29:26,800 --> 00:29:31,920
intelligence is the ability to have episodic memory and then now I'm going to suggest to you

258
00:29:32,560 --> 00:29:38,880
how we might actually do that and it's you know the most obvious way to do that is to actually

259
00:29:38,880 --> 00:29:44,400
take that retrieval augmented generator that I mentioned before and basically let the large

260
00:29:44,400 --> 00:29:52,880
language model write its own memories and this is basically a suggestion about how you might do that

261
00:29:54,560 --> 00:30:04,800
more generally I actually think that you know people like me have spent decades trying to come up

262
00:30:04,800 --> 00:30:10,800
with specialized knowledge representation systems and specialized inference systems

263
00:30:13,440 --> 00:30:21,600
you know so and this essentially is like a specialized logic you know so knowledge graphs

264
00:30:21,600 --> 00:30:28,960
and one example of that where you try to encode information in entity relation triples for example

265
00:30:29,680 --> 00:30:36,480
but I actually think with LLMs you know one real possibility is that you actually let the

266
00:30:38,080 --> 00:30:41,600
you let the primitive statements actually be natural language statements

267
00:30:43,520 --> 00:30:50,480
so you just have represented inside of your system stored inside of a vector store for example

268
00:30:50,480 --> 00:30:56,240
something like insomnia is a typical symptom of diabetes and then you'd actually let the

269
00:30:56,240 --> 00:31:01,440
large language model itself decide the relationship between these atomic propositions

270
00:31:03,280 --> 00:31:08,800
and so instead of having a specialized knowledge representation language a specialized logic

271
00:31:08,800 --> 00:31:13,920
you'd use natural language and you'd let the large language model actually pass information

272
00:31:13,920 --> 00:31:22,880
from one atomic proposition to another for those of you that are as old as I am you know I mean I

273
00:31:22,880 --> 00:31:32,160
loved prologue and you know very simple horn clause inference procedures so what I just tried to do

274
00:31:32,160 --> 00:31:40,720
here was take take that and sort of show how I might replace first order logic in there with

275
00:31:40,720 --> 00:31:49,600
natural language statements but otherwise you've got proof rules proof structures and so this is in

276
00:31:49,600 --> 00:31:56,640
fact actually a standard you know textbook example of how uh you might wind up doing

277
00:31:56,640 --> 00:32:01,760
inference here so you've asked the question you know can Sam get a degree and you've got a series

278
00:32:01,760 --> 00:32:09,120
of facts about what courses Sam has taken and a series of rules but the difference is all this

279
00:32:09,120 --> 00:32:17,760
is all expressed in natural language rather than in some first order logic form okay all right so

280
00:32:17,760 --> 00:32:23,520
just talking a little bit more about the social implications of all of this

281
00:32:25,120 --> 00:32:30,240
so I think to understand the social implications I think one of the things you probably want to

282
00:32:30,240 --> 00:32:34,880
understand is try and make some guesses about how the field itself might actually evolve

283
00:32:36,800 --> 00:32:45,360
I can see sort of two possible futures one is where we wind up getting ever larger

284
00:32:45,360 --> 00:32:52,240
proprietary monolithic close large language models that you effectively interact with via

285
00:32:52,240 --> 00:32:58,320
web APIs that is the actual model itself the training data everything is kept proprietary

286
00:32:58,880 --> 00:33:06,720
but you can just simply call it over the web another future is that there will be open sourced

287
00:33:06,720 --> 00:33:11,040
language models and the weights will actually be available and you'll be able to do things

288
00:33:11,040 --> 00:33:17,520
like fine tune those weights yourself and right now you know we're in the world where there's both

289
00:33:17,520 --> 00:33:24,000
of these kinds of large language models and the proprietary models are better than the open source

290
00:33:24,000 --> 00:33:30,720
models and I think really the big question about the development of the field is whether or not

291
00:33:30,720 --> 00:33:37,040
fine tuning will turn out to take the open source models and make them competitive with the closed

292
00:33:37,040 --> 00:33:44,320
proprietary models and I call that the 64 billion dollar question because that's probably about the

293
00:33:44,320 --> 00:33:50,080
amount of money that the companies that are investing here sort of have invested

294
00:33:51,680 --> 00:33:59,440
you know the language models are becoming increasingly capital intensive it costs

295
00:34:00,000 --> 00:34:06,800
huge amounts million many millions of dollars to collect the data and actually do the training

296
00:34:06,880 --> 00:34:13,360
of these things and capital intensive industries tend to concentrate you know you just look at the

297
00:34:13,360 --> 00:34:20,000
chip manufacturing where I think there's only one or two fabrication factories in the entire

298
00:34:20,000 --> 00:34:28,320
world that make the top-end chips that we all have in all of our devices so if that is actually

299
00:34:28,320 --> 00:34:34,640
what the future of LLMs is then probably we will see that same sort of concentration

300
00:34:35,280 --> 00:34:42,320
into just a couple of places I suspect the training data will become increasingly important

301
00:34:42,320 --> 00:34:49,520
people are already talking about training data as being the ultimate you know limiting factor

302
00:34:50,640 --> 00:34:54,640
and I think it will become the major differentiator particularly if you want to do things

303
00:34:55,280 --> 00:35:01,920
like build LLMs for very specialized domains like healthcare finance other things like that

304
00:35:02,480 --> 00:35:09,840
but and I actually think data and LLM quality control which goes back to that issue about

305
00:35:09,840 --> 00:35:14,320
testing and evaluation I was talking about before that's going to become increasingly

306
00:35:14,320 --> 00:35:20,480
important in fact I actually when I think about what will somebody like me in industry be doing

307
00:35:20,480 --> 00:35:27,920
in five years time quite possibly you know testing and evaluation will actually be you know

308
00:35:27,920 --> 00:35:36,080
90% of what we do you know we know that fine-tuning can mask a poor large language model right so

309
00:35:36,080 --> 00:35:42,320
we know that you can take take Sydney and do a little bit of fine-tuning and have it at least

310
00:35:42,320 --> 00:35:51,200
superficially call itself being but then Sydney reemerges in the right context as well so I think

311
00:35:51,200 --> 00:35:57,120
one way we'll get around that is we'll start seeing things like certificates of origin

312
00:35:57,840 --> 00:36:05,120
you know we'll be saying you know I guarantee that my large language model has been trained on

313
00:36:05,680 --> 00:36:11,120
just high quality data and the same way as you see certificates of origin for you know

314
00:36:11,120 --> 00:36:18,560
fancy cheeses and things like that you know the cows grazed on grass organically raised on the

315
00:36:18,560 --> 00:36:25,040
south's southern meadows and all that sort of stuff and particularly if in the if that open source

316
00:36:25,040 --> 00:36:32,880
world that I was talking about before if that comes into play I you know I see that has been sort

317
00:36:32,880 --> 00:36:38,480
of really one of the really big challenges I've I've seen how data vendors small startups are

318
00:36:38,480 --> 00:36:43,360
really under incredible pressure to produce something which they can sell because they're

319
00:36:43,360 --> 00:36:50,480
usually cash constrained and the same thing may be true for startups that are producing large

320
00:36:50,560 --> 00:36:58,080
language models they'll be under huge pressure to take somebody else's model and do a few tweaks to

321
00:36:58,080 --> 00:37:06,000
it and try to present it as something that's completely new and yeah impact on nlp jobs

322
00:37:07,040 --> 00:37:15,200
I I actually do think that it's not too far off when we'll be able to say something like

323
00:37:15,200 --> 00:37:20,480
give an instruction to a large language model it's like deploy a chatbot the task is informing

324
00:37:20,480 --> 00:37:26,560
users about the products that you'll find listed in this database over here I want you to interact

325
00:37:26,560 --> 00:37:32,800
with users in a professional tone emphasize customer service rather than price and politely

326
00:37:32,800 --> 00:37:37,840
decline to talk about topics that are related to the products and that will be it that will

327
00:37:38,560 --> 00:37:47,600
build you a chatbot you won't need an expert development team you know I do think that however

328
00:37:47,600 --> 00:37:53,600
that that's not going to come up immediately we will for the next say five years or so we will

329
00:37:53,600 --> 00:37:58,960
need people that can create training data and fine-tune models and as I said before I think

330
00:37:58,960 --> 00:38:05,280
evaluation and testing is just going to get more and more important brought to social impacts

331
00:38:05,840 --> 00:38:12,880
right so I think we already know that deep fakes and fraud are just going to get supercharged by

332
00:38:12,880 --> 00:38:20,960
this sort of technology and yes I think that's true I think we're going to see automation of

333
00:38:20,960 --> 00:38:27,360
jobs not previously automated Krugman has an interesting article in the New York Times just a

334
00:38:27,360 --> 00:38:33,920
couple of days ago where you know he makes the point that it doesn't really matter whether these

335
00:38:33,920 --> 00:38:40,160
LLMs really are intelligent or not that even a souped up auto correct can actually have

336
00:38:40,800 --> 00:38:46,320
quite major implications in terms of productivity he's actually really quite positive he seems to

337
00:38:46,320 --> 00:38:56,080
think that actually these things might you know level society somewhat and they might I mean there's

338
00:38:56,080 --> 00:39:05,120
some evidence that in fact the GPT-4 for example enables poorer workers to work at a higher standard

339
00:39:06,800 --> 00:39:12,240
whereas the best workers are helped less by GPT-4 maybe that's the case

340
00:39:14,480 --> 00:39:21,680
I think there's a number of risks I you know I think we are seeing you know AI models being

341
00:39:21,680 --> 00:39:27,520
trained on public domain data that the creators when they made their data public really had no

342
00:39:28,080 --> 00:39:34,720
intention no one no expectation of their data would be used in this way we're seeing a political

343
00:39:34,720 --> 00:39:39,680
fight right now between media companies and tech companies about the use of data I think that's

344
00:39:39,680 --> 00:39:48,160
still mainly about search rather than training AI models but that same fight I think will continue

345
00:39:48,160 --> 00:39:55,280
I looking back to the first industrial revolution and things like the tragedy of the commons I don't

346
00:39:55,280 --> 00:40:00,560
see any particular reason to expect a socially optimal outcome although I think the writer's

347
00:40:00,560 --> 00:40:06,560
Guild of America settlement actually sounds like it's a pretty forward-looking one and

348
00:40:07,520 --> 00:40:14,400
I'm very pleased to see that I I do think you know I I'm not one of these people that that

349
00:40:14,400 --> 00:40:22,400
poo-poo's the people that are worried about you know AGI and misalignment I don't think we're likely

350
00:40:22,400 --> 00:40:28,640
to be made extinction but to be made extinct but I do think we should be worrying about that

351
00:40:28,640 --> 00:40:32,560
and the final point I'd like to make is that I think these things are economic and political

352
00:40:32,560 --> 00:40:40,160
choices not really technical choices so it's an interesting question so those of us that actually

353
00:40:40,160 --> 00:40:45,680
have the technical expertise probably are in a position to have our voices heard more than what

354
00:40:45,680 --> 00:40:52,560
they would normally be so we should probably make use of that but I actually really do think that

355
00:40:53,120 --> 00:41:00,560
it's not just up to the the tech companies in particular to try and make the important decisions

356
00:41:00,560 --> 00:41:10,480
here so conclusions I think LLM's are here to stay a lot of my people my age remember the AI

357
00:41:10,480 --> 00:41:15,920
winter of the last century I don't think there's going to be an AI winter just simply because

358
00:41:17,040 --> 00:41:23,760
these things are actually way too useful for students intellectual revolutions are a great

359
00:41:23,760 --> 00:41:29,680
time to enter the field because in fact actually the amount of knowledge you need to have to become

360
00:41:29,680 --> 00:41:37,920
an expert is much much less I think LLM's open up new interesting scientific research questions

361
00:41:37,920 --> 00:41:46,400
and directions NLP I think will have less emphasis on clever new algorithms and more on

362
00:41:47,040 --> 00:41:53,440
yeah interaction and integration of models applications data design and training and

363
00:41:53,440 --> 00:41:59,120
much more emphasis on evaluation so thank you very much

364
00:42:05,840 --> 00:42:12,320
any questions or comments yes hi thank you for great talk super interesting I just wanted to

365
00:42:13,520 --> 00:42:17,280
ask you about one thing in the middle of the talk which is about

366
00:42:18,080 --> 00:42:25,680
this kind of neuro symbolic integration and you had this you had this kind of proposal that LLM's

367
00:42:25,680 --> 00:42:32,480
are going to give you parts of bits and then you're going to use those inputs into a logic model

368
00:42:33,920 --> 00:42:37,840
and I wondered like why do you think I mean personally I like

369
00:42:38,480 --> 00:42:44,560
neuro symbolic learning so but I'm interested today like why why do you think that's a good idea

370
00:42:44,560 --> 00:42:50,320
or good approach like why not just let the LLM do the entire thing like what what is it that

371
00:42:51,120 --> 00:42:57,120
so certainly certainly there are people that are betting yeah um you know let's just let the LLM

372
00:42:57,120 --> 00:43:05,760
do the entire thing yeah um I guess the answer I would give there is that there are a lot of

373
00:43:05,760 --> 00:43:12,080
academics and in fact I'm sure many of you've seen this stuff right it's it's now quite you know

374
00:43:12,160 --> 00:43:19,120
there's like a little mini industry of people coming up with things like you know chat GPT

375
00:43:19,120 --> 00:43:29,920
cannot understand negation GPT for does not understand X is Y statements you know and in fact

376
00:43:29,920 --> 00:43:39,200
actually I don't think I'm ashamed but we have a paper that is claiming that LLM's cannot

377
00:43:39,200 --> 00:43:45,680
understand you know do not really properly understand entailment you know that walking

378
00:43:45,680 --> 00:43:54,560
entails moving but moving does not always entail walking um so if you really believe that stuff

379
00:43:54,560 --> 00:44:00,000
if you really believe you know and if you believe it enough so that you actually think that GPT 5

380
00:44:00,000 --> 00:44:04,240
or whatever the next model is that comes out is going to have to exactly that same weakness

381
00:44:04,960 --> 00:44:12,720
the idea then is build a symbolic component that addresses whatever weakness you happen to think

382
00:44:12,720 --> 00:44:19,680
these large language models have but it isn't like I'll admit it is rather risky because these

383
00:44:19,680 --> 00:44:29,040
things are improving so rapidly and I'm not so sure I mean it's it's a risk if you're a grad

384
00:44:29,040 --> 00:44:35,200
student to say all right I'm going to commit the next year or two of my life to working on solving

385
00:44:35,200 --> 00:44:42,560
you know the problem of negation in large language models and halfway through your research project

386
00:44:43,920 --> 00:44:49,600
you know someone discovers that just by scaling up the training data another order of magnitude

387
00:44:49,600 --> 00:44:55,760
all of a sudden now it's going to handle negation just perfectly and that's what I meant by in fact

388
00:44:55,760 --> 00:45:02,000
I think that was one of the one of the slides that Kate added was saying that it's particularly if

389
00:45:02,000 --> 00:45:07,680
you wanted to do something some sort of behavioral research or something else like that you know where

390
00:45:08,640 --> 00:45:14,880
this the timescale which LLMs are changing versus the timescale of doing behavioral research is

391
00:45:15,760 --> 00:45:21,760
the LLMs are just changing so fast that if you if you looked at today's LLMs and said okay you're

392
00:45:21,760 --> 00:45:27,280
inspired by then here are some interesting behavioral predictions that they're making I'm

393
00:45:27,280 --> 00:45:31,760
going to go out and start running some experiments with kids or something like that

394
00:45:34,000 --> 00:45:38,560
yeah you know by the time you've collected a quarter of your data there's another model out

395
00:45:38,560 --> 00:45:45,600
there and it's got it's making different behavioral predictions it's just I don't know what the answer

396
00:45:45,600 --> 00:45:59,440
there is except to say that yeah yeah you might just um ask a a follow-up question so if if you

397
00:45:59,440 --> 00:46:12,080
have a chat system I think that's actually for the zooms but I they also said there's a whole array

398
00:46:12,240 --> 00:46:20,880
microphone inside the box so well um I'll just talk about um if as opposed to um adding a symbolic

399
00:46:20,880 --> 00:46:30,320
system to to a chat system or you know an LLM um what about fine tuning and just doing lots of fine

400
00:46:30,320 --> 00:46:37,280
tuning instead I mean that's adding more data but if you fine tune it yes with that kind of data as

401
00:46:37,280 --> 00:46:42,880
opposed to going as symbolic group that's right wouldn't that be I think that's I think that's

402
00:46:42,880 --> 00:46:51,360
very true and in fact that I think that's a good question is you know if so right now what was it

403
00:46:51,360 --> 00:46:58,880
you know Gary Marcus is picking up on the fact that uh somebody wrote a paper that said that

404
00:46:59,600 --> 00:47:07,200
uh oh look there's a whole lot of cases where the large language models uh will quite happily say

405
00:47:07,200 --> 00:47:14,800
that oh let's see all right I don't know enough about celebrities unfortunately but you know

406
00:47:14,800 --> 00:47:23,440
so-and-so and so is Tom Cruise's mother right okay it accepts that statement and you then ask the

407
00:47:23,440 --> 00:47:30,640
other you then ask the question who is Tom Cruise's mother and it says I have no idea so

408
00:47:31,520 --> 00:47:37,440
and it seems like oh well if I've actually got a couple of comments there so first of all it seems

409
00:47:37,440 --> 00:47:44,560
like x is y that looks an awful lot to a math mathematical person as being like x equals y

410
00:47:45,120 --> 00:47:52,560
and we know that what was it uh equality is what is it commutative you know so if x equals y then

411
00:47:52,560 --> 00:48:04,960
y equals x um in fact actually x is a y actually really isn't commutative um you know uh you know

412
00:48:05,360 --> 00:48:14,800
uh what was it you know chicken salad is a wonderful meal you know that can be true but

413
00:48:14,800 --> 00:48:21,680
a wonderful meal doesn't always have to be chicken salad right um you know so uh

414
00:48:26,960 --> 00:48:29,280
but anyway so might you know the yeah

415
00:48:29,280 --> 00:48:39,360
do you think uh do you think there's going to be uh kind of I mean what what do you think about

416
00:48:39,360 --> 00:48:47,520
like the kind of vision vision language models where you're kind of train it on like a ton of

417
00:48:47,520 --> 00:48:53,120
images and then it generates a load of you know you can generate these images we actually did

418
00:48:53,760 --> 00:48:58,640
you know before I did the startups just before I did the startups I think the last student I

419
00:48:58,640 --> 00:49:05,680
worked with was working on image captioning um and I'd very much like to go back to I I think

420
00:49:05,680 --> 00:49:10,960
that's really incredibly interesting so while I think it doesn't actually solve the chart the

421
00:49:10,960 --> 00:49:18,560
cell Chinese rule Chinese room objection you know I mean ultimately the input to all these models

422
00:49:18,560 --> 00:49:27,120
are really just activation patterns and you know the models got no reason to suspect that images

423
00:49:28,640 --> 00:49:35,840
closely you know more connected to the world than languages but I think just in practice there's a

424
00:49:35,840 --> 00:49:43,680
very good chance that there may be really interesting correlations that can be learned by correlating

425
00:49:44,560 --> 00:49:51,360
you know images with language and of course the only issue there is that the

426
00:49:51,520 --> 00:49:58,880
um the amount of compute that's needed to do this is just really enormous and it's

427
00:50:00,160 --> 00:50:07,200
you know you'd have to do some have to get some deal probably with one of the major tech

428
00:50:07,840 --> 00:50:12,800
companies to get your access to enough compute yeah to do it

429
00:50:13,520 --> 00:50:22,960
and that's that's sort of the problem with with a lot of this research now and in fact I think one

430
00:50:22,960 --> 00:50:27,120
of the comments that I wrote there is you know maybe in fact we should be thinking about something

431
00:50:27,120 --> 00:50:33,760
you know physics has been very successful in getting funding for big science maybe we should

432
00:50:33,760 --> 00:50:40,400
be thinking about ways of getting funding for academic big science as well for dealing with this stuff

433
00:50:40,640 --> 00:50:47,120
well and there if you think of images still images that's a certain amount of compute power

434
00:50:47,680 --> 00:50:53,680
I think of visual no ongoing visual scenes and movies that's a lot more

435
00:50:54,960 --> 00:51:04,400
and yet presumably real world learners are you know taking advantage of the visual scene

436
00:51:04,400 --> 00:51:14,000
as it moves by um and uh although learning can take place in blind people as well and that's

437
00:51:14,000 --> 00:51:19,760
a whole another research area so you know you don't need vision to learn language but it certainly

438
00:51:19,760 --> 00:51:29,680
can facilitate aspects of yeah and then yeah and I guess also I mean like looking forward you'd

439
00:51:29,680 --> 00:51:38,560
want these systems to actually be kind of situated in the physical world somehow I guess right well

440
00:51:38,560 --> 00:51:48,720
so that so that's the certainly lots of people have got that uh you know feeling that in fact

441
00:51:48,720 --> 00:51:57,840
actually that that we need situated you know situated models but I mean isn't the input to

442
00:51:57,840 --> 00:52:04,960
a model always really just an activation pattern isn't it really always just uh I mean couldn't

443
00:52:04,960 --> 00:52:11,920
you always run you know so you know there was the question what is silver's chinese room argument

444
00:52:11,920 --> 00:52:18,400
right there was that basically you know uh supposing you you come up with a computer program

445
00:52:18,400 --> 00:52:25,920
which can translate english into chinese or sorry translate chinese into english so you give that

446
00:52:26,000 --> 00:52:32,400
to a person that's sitting you know inside of a room and you just simply tell him you know here's

447
00:52:32,400 --> 00:52:40,800
a set of symbols follow these instructions and give me the output that you you you obtained

448
00:52:40,800 --> 00:52:48,880
by following these instructions and so's point is that even if this thing does actually produce

449
00:52:48,880 --> 00:52:57,600
good english as an output you really can't say that the person understands chinese you know that just

450
00:52:58,320 --> 00:53:06,640
they don't understand chinese you know they're just following these rules and uh so his argument

451
00:53:06,640 --> 00:53:12,880
really is that there's something else that a pure rule following system really doesn't have

452
00:53:12,880 --> 00:53:18,080
understanding that something else is required now lots of people wind up saying oh well what you

453
00:53:18,080 --> 00:53:26,800
really need is grounding you really do need these you know you need the symbols to be connected somehow

454
00:53:26,800 --> 00:53:33,440
to the real world but I think the model never really knows I don't see any way for our current

455
00:53:34,080 --> 00:53:39,440
computational models to know that the bit patterns that we're feeding into them

456
00:53:40,240 --> 00:53:42,800
yeah correspond to anything in the world

457
00:53:44,240 --> 00:53:49,680
yeah I mean yeah I would agree on that but um I guess for that kind of symbol grounding you

458
00:53:50,560 --> 00:53:57,360
like some some people argue you you need to have a kind of community right language users so

459
00:53:58,480 --> 00:54:08,960
maybe that uh all kind of do you know um and and then the grounding kind of comes out of

460
00:54:10,400 --> 00:54:15,760
people using the same symbol in the same way yes right yeah yes no no no I mean you're

461
00:54:16,480 --> 00:54:21,840
kripke you know so philosophers of language like kripke have argued that in fact actually

462
00:54:25,120 --> 00:54:29,840
that and you know in fact this is sort of very true of me because I grew up in the southern

463
00:54:29,840 --> 00:54:38,320
hemisphere I don't know a lot of the northern trees so I'm not sure I can with catherine's

464
00:54:38,400 --> 00:54:43,520
help now I can recognize aspens but you know I'm not really sure about the difference between

465
00:54:43,520 --> 00:54:50,240
oaks and elms and the rest of them right but kripke you would say that I can still talk about

466
00:54:50,240 --> 00:54:57,200
all of those things and when I talk about oaks I mean oak trees even though I might not be able

467
00:54:57,200 --> 00:55:03,680
to actually identify an oak tree reliably and so kripke's story there is exactly what you're

468
00:55:03,680 --> 00:55:10,000
saying it's a community of language learners sorry language users and I am willing basically I'm

469
00:55:10,000 --> 00:55:17,600
agreeing to the authority of language users and I effectively what I'm saying is when I use oak

470
00:55:18,240 --> 00:55:24,640
I use it to mean whatever the rest of you mean that you know that grew up and presumably know

471
00:55:24,640 --> 00:55:30,800
exactly what an oak tree is right and he says that in fact actually with a lot of particularly

472
00:55:30,800 --> 00:55:38,240
scientific terminology we wind up using it that way right many of us may not be able to define

473
00:55:38,240 --> 00:55:43,920
exactly what the difference is between the different types of neutrinos or whatever but

474
00:55:44,560 --> 00:55:51,040
we rely on experts within our community to be able to ground those things

475
00:55:52,640 --> 00:55:57,520
all I can say is I don't even know how you'd even tell a large language model that it's part of

476
00:55:58,480 --> 00:56:03,680
yeah yeah no I was sort of yeah kind of thinking that sort of thing myself like how

477
00:56:04,400 --> 00:56:11,120
well what I mean does that does the community there just mean literally that actually just the

478
00:56:11,120 --> 00:56:20,960
text documents that have been fed into it is that I mean I guess with the the instruction

479
00:56:21,680 --> 00:56:24,800
tuning I guess you get a little bit of that right

480
00:56:28,640 --> 00:56:36,080
so anyone else have questions at all yeah so that it's almost just that you could have a community

481
00:56:36,080 --> 00:56:45,120
of users of a particular model yeah that would have a sort of various types of queries within a

482
00:56:45,120 --> 00:56:56,400
particular domain that might help train up that model then to become more a realistic conversational

483
00:56:56,400 --> 00:57:05,680
agent within that particular domain perhaps yeah yeah so maybe that's kind of yeah yeah yeah

484
00:57:06,800 --> 00:57:12,880
who knows maybe a year or two from now you know those things will start to emerge yeah

485
00:57:12,880 --> 00:57:17,920
yeah so I think one of the things that Catherine and I are hoping to get out of this is to find

486
00:57:17,920 --> 00:57:27,280
out more about you know work at SFI that we might connect with right but a sort of general interest

487
00:57:27,280 --> 00:57:38,960
in you know language learning psychological aspects computational aspects you know so

488
00:57:38,960 --> 00:57:52,160
so we're here until Tuesday afternoon Tuesday evening so please please contact us right

489
00:57:52,160 --> 00:57:55,360
okay thanks

