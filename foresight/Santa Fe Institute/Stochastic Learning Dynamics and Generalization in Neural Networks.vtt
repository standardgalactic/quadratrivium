WEBVTT

00:00.000 --> 00:04.320
What I'm going to tell you about is some recent work

00:04.320 --> 00:06.560
we have done in trying to understand the learning

00:06.560 --> 00:10.160
dynamics in the neural network, feed forward neural network,

00:10.160 --> 00:12.880
from your based on a statistical physics point of view.

00:12.880 --> 00:16.080
So you will see.

00:16.080 --> 00:18.480
Got it, OK.

00:18.480 --> 00:23.240
OK, so I will start with some basic introduction.

00:23.240 --> 00:26.600
What I call the central dogma of machine learning.

00:26.600 --> 00:28.520
Focus on two main things, but I think

00:28.520 --> 00:31.040
is very important, which I think physicists

00:31.040 --> 00:34.640
can contribute to optimization and generalization,

00:34.640 --> 00:36.000
to understand these two things.

00:36.000 --> 00:39.080
And then I'll talk along and talk about the topic.

00:39.080 --> 00:42.280
One is really about the dynamics of learning in feed

00:42.280 --> 00:43.960
forward neural network.

