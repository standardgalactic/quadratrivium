WEBVTT

00:00.000 --> 00:06.720
This public lecture series.

00:06.720 --> 00:11.840
Many of the most cherished books in science, Einstein's relativity, Schrodinger's What

00:11.840 --> 00:17.820
Is Like, Richard Feynman's QED, were all based on public lectures.

00:17.820 --> 00:24.420
In this spirit, we want to welcome you here tonight, this public lecture.

00:24.420 --> 00:29.800
In the special series beginning tonight, the ULAM lectures, we bring a notable scientist

00:29.800 --> 00:36.000
to illuminate a cutting-edge topic in honor of the late theoretical mathematician Stanislaw

00:36.000 --> 00:37.560
Ulam.

00:37.560 --> 00:42.280
All of our public lectures are underwritten by the McKinnon Family Foundation, who allow

00:42.280 --> 00:47.080
us to continue to provide the lectures at no cost, and we want to take a moment to thank

00:47.080 --> 00:54.840
them first.

00:54.840 --> 00:59.160
We are additionally supported by the Santa Fe Reporter and this Lenzick Performing Arts

00:59.160 --> 01:05.680
Center.

01:05.680 --> 01:10.600
Stanislaw Ulam was long associated with Los Alamos National Laboratory, and is highly

01:10.600 --> 01:14.520
regarded by the Santa Fe Institute scientific community.

01:14.520 --> 01:19.840
Former SFI vice president Mike Simmons said, the enormous range of Ulam's scientific thought

01:19.840 --> 01:25.680
encompassed not only mathematics, but also physics, computation, biology, and much else.

01:25.680 --> 01:30.000
He would have been very much at home in the present day Santa Fe Institute.

01:30.000 --> 01:36.060
In this tradition, I am proud to introduce Ricard Soleil, our speaker tonight.

01:36.060 --> 01:41.640
Simply put, Ricard is one of the most abundant and interesting theorists alive.

01:41.640 --> 01:48.780
Ricard leads Icreia, the complex systems lab at the Universitat Pompeo Fabra in Barcelona.

01:48.780 --> 01:53.760
He is also an external faculty member here at the Santa Fe Institute.

01:53.760 --> 01:58.780
Ricard originally studied both physics and biology and undergraduate before achieving

01:58.780 --> 02:00.900
a Ph.D. in physics.

02:00.900 --> 02:05.900
His work touches on everything from how life originated to present day ecology to the very

02:05.900 --> 02:07.920
nature of thought.

02:07.920 --> 02:12.800
In the late 90s, Ricard connected ecology and mathematics by demonstrating the fractal

02:12.800 --> 02:18.040
structure of forest canopies, and that these structures emerge from the statistical dynamics

02:18.040 --> 02:20.060
of self-organization.

02:20.060 --> 02:25.860
This is a fundamental idea in ecology, and the full extent of the impact of these ideas

02:25.860 --> 02:29.940
are still being woven into our study and predictions of forests today.

02:29.940 --> 02:34.900
They are ideas that inform and inspire my own work constantly.

02:34.900 --> 02:40.540
Ricard has worked on other fundamental mathematical questions related to complex networks.

02:40.540 --> 02:45.540
More broadly, he is interested in how life works, spanning from how cells first originated

02:45.540 --> 02:51.200
in our past, to how brains developed to know and observe those same cells today, to how

02:51.200 --> 02:58.720
technological societies network brains to design new cells and engage in global bioengineering.

02:58.720 --> 03:03.480
And it is necessary to point out that Ricard's aggregate breadth of study is never at the

03:03.480 --> 03:05.360
expense of depth.

03:05.360 --> 03:10.560
This combination of expansive topics pursued with deep rigor is one of the rarest talents

03:10.560 --> 03:12.520
in science.

03:12.520 --> 03:16.860
In my own interactions with Ricard, we often discuss literature, and I am the fortunate

03:16.860 --> 03:23.020
recipient of many quotes from novels, biographies, and historical scientific works.

03:23.020 --> 03:27.980
The wonderful drawings that you are looking at tonight are made by the man himself.

03:27.980 --> 03:32.420
This shows that his imagination is constantly in motion, and it is a pleasure to discuss

03:32.420 --> 03:34.080
any topic with him.

03:34.080 --> 03:38.780
He is a true polymath and an ideal speaker for this lecture series.

03:38.780 --> 03:43.000
With that, please help me welcome Ricard Soleil.

03:43.000 --> 03:45.000
Hello.

03:45.000 --> 03:51.120
You hear me?

03:51.120 --> 03:55.040
Thanks for coming tonight.

03:55.040 --> 04:00.520
It's a real honor to be here delivering the Olam lectures this year.

04:00.520 --> 04:04.720
Thanks, Chris, for this great introduction.

04:05.080 --> 04:12.060
My talks have to do with a very exciting domain of research, which is mapping the space of

04:12.060 --> 04:14.020
cognitions.

04:14.020 --> 04:19.820
Not only the cognitions that we are familiar with, what I call the solid brains, but trying

04:19.820 --> 04:26.340
to understand what is there, what is what I call the cognitive biosphere, what is there,

04:26.340 --> 04:32.340
how much is complex, and whether or not we can understand the evolutionary dynamics that

04:32.360 --> 04:39.360
brings us, and if we can maybe go beyond evolution and try to engineer that complexity.

04:40.880 --> 04:45.520
So, before I go to that, since this is an Olam lecture, I want to say a little thing

04:45.520 --> 04:52.720
about that, because when I was in high school, I stumbled into this book, The Monte Carlo

04:52.720 --> 04:53.720
Method.

04:54.700 --> 05:04.700
It was a tiny book edited by the former Soviet Union in a series of little books with all

05:04.700 --> 05:11.200
kinds of things, where I discovered that Stan Olam, along with John von Neumann, invented

05:11.200 --> 05:17.220
this method that physicists use all the time to actually model complexity that has to do

05:17.220 --> 05:20.540
with some kind of randomness.

05:20.540 --> 05:23.880
And that brought me into play myself.

05:23.880 --> 05:31.040
It was the time of, there were no computers, so I'm old enough to say that, and so I had

05:31.040 --> 05:36.400
a calculator, I have a coin, and I did a model of a gas.

05:36.400 --> 05:43.960
You can see this is written with a typewriter, and I elaborated with that using that book.

05:43.960 --> 05:49.960
And one of the things that I learned is that there's a lot of power in being able to approach

05:49.960 --> 05:55.340
reality using, let's say, synthetic approximations.

05:55.340 --> 06:00.940
And the other thing I learned is that it's something about popularity.

06:00.940 --> 06:07.540
This series is called Popular Lessons in Mathematics.

06:07.540 --> 06:10.340
Did that make me popular at all?

06:10.340 --> 06:11.340
No?

06:11.720 --> 06:16.960
And I understood that popularity is not exactly that kind of stuff.

06:16.960 --> 06:23.760
But the other thing that I did connect is, I was mentioning Ulam and von Neumann, which

06:23.760 --> 06:25.800
you can see there on the left.

06:25.800 --> 06:29.040
In the middle we have Richard Feynman, another big name.

06:29.040 --> 06:32.080
It was important in my life.

06:32.080 --> 06:39.720
And von Neumann, and I will mention him today and tomorrow, brought me also into something

06:39.720 --> 06:40.720
else.

06:41.100 --> 06:46.420
This book by Michael Arby, Brains, Machines, and Mathematics, and again, I was still in

06:46.420 --> 06:47.420
high school.

06:47.420 --> 06:50.940
I found out that there was people thinking in the brains using mathematics that you could

06:50.940 --> 06:54.300
approach actually brain complexity using mathematics.

06:54.300 --> 06:58.060
I found out extremely fascinating.

06:58.060 --> 07:06.300
And over time, I got involved in trying to figure out how to solve some of the interesting

07:06.300 --> 07:07.300
questions.

07:08.280 --> 07:14.520
There are plenty of good things that we would like to understand from the question of why

07:14.520 --> 07:15.520
brains?

07:15.520 --> 07:22.400
Why brains is part of a more broad complex, a more general question on complexity?

07:22.400 --> 07:28.440
Because some people said that why the biosphere is not just made of microorganisms.

07:28.440 --> 07:29.440
They are cheap.

07:29.440 --> 07:30.440
They reproduce.

07:30.440 --> 07:33.040
They propagate and proliferate.

07:33.100 --> 07:40.580
Thinking that it adds complexity and brain is a lot of complexity with a cost seems to

07:40.580 --> 07:41.580
be superfluous.

07:41.580 --> 07:43.820
Why actually is any complexity?

07:43.820 --> 07:46.660
And brains are especially important.

07:46.660 --> 07:47.660
What kind of brains?

07:47.660 --> 07:51.700
That's one of the big questions we'll try to bring.

07:51.700 --> 07:53.380
Are there other minds?

07:53.380 --> 07:55.900
It's a very hot topic these days.

07:55.900 --> 07:59.820
What is a mind and how do we define and what we can find out?

07:59.820 --> 08:02.900
All the way up into the artificial.

08:02.900 --> 08:09.040
So whether or not beyond evolution, beyond what we see, beyond what we can infer about

08:09.040 --> 08:16.600
how nature happened to construct complexity, whether we can actually create new things.

08:16.600 --> 08:21.840
And that connects with the big question of the major evolutionary transitions, namely

08:21.840 --> 08:26.160
how the big innovation happens in the real world.

08:26.160 --> 08:32.600
Many years ago, Eosia Smadi and Don Minersmith made this list of transitions that had to

08:32.600 --> 08:38.540
do with the origins of cells, the origins of information, the code, et cetera, et cetera,

08:38.540 --> 08:42.020
all the way up to language.

08:42.020 --> 08:44.060
Of course, the list is more complex.

08:44.060 --> 08:48.100
You could ask yourself, what is the origin of consciousness?

08:48.100 --> 08:49.900
Why do we have consciousness?

08:49.900 --> 08:50.900
I will address that.

08:50.900 --> 08:56.700
I don't have an answer, which might be wrong, but you'll see.

08:56.700 --> 09:01.940
And in the middle of all this discussion about what is the nature of innovation and how innovations

09:01.960 --> 09:09.600
happen, is precisely this special thing of biology, the special status of information

09:09.600 --> 09:10.920
in biology.

09:10.920 --> 09:20.040
And Eva Javlonka and Marion Lam made this beautiful work where they bring precisely this idea.

09:20.040 --> 09:22.680
Cognitive agents bring something extraordinary.

09:22.680 --> 09:27.720
In evolution, you have genetic information dominating the story of life for a very long

09:27.720 --> 09:28.720
time.

09:28.900 --> 09:33.100
Information that is based on something that is not genetic has enormous advantages.

09:33.100 --> 09:38.060
Information that can be shared, can be propagated, and for us humans can be propagated beyond

09:38.060 --> 09:40.460
ourselves.

09:40.460 --> 09:43.660
So that's the main thing I want to address.

09:43.660 --> 09:54.020
And the first part of the story is to actually ask ourselves, how likely is a brain to happen?

09:54.020 --> 09:59.880
And that connects with something that is fundamental in evolutionary biology, but goes beyond that

09:59.880 --> 10:06.240
and percolates into many other areas of knowledge, which is the role of randomness and contingency

10:06.240 --> 10:13.720
and history versus the possibility that there are very strong laws that constrain a lot

10:13.720 --> 10:15.360
what is possible.

10:15.360 --> 10:19.600
And I put two views here.

10:19.600 --> 10:24.780
Here on the left, Stephen Jay Gould was a strong advocate of the idea that evolution

10:24.780 --> 10:32.660
is so historical that any kind of little change that you put in place will modify the outcome.

10:32.660 --> 10:38.420
And he made, he used this mental experiment based on this movie that, sure, you have seen.

10:38.420 --> 10:45.060
I mean, I don't know here, but in Barcelona, you see every single Christmas, we have wonderful

10:45.060 --> 10:48.420
life at some point, right, on TV.

10:48.420 --> 10:50.440
And I remember for you the story.

10:50.440 --> 10:55.400
It's this guy, a very, very nice guy who had a lot of trouble for a very unfair reason

10:55.400 --> 11:00.640
that at some point he decides that his life is worthless and says to himself, the world

11:00.640 --> 11:03.280
will be better if I haven't been born.

11:03.280 --> 11:09.480
And then comes this angel, which is a kind of quite annoying character, which is proposing

11:09.480 --> 11:12.760
the experiment of, okay, let's do it, let's do the experiment.

11:12.760 --> 11:14.160
You haven't been born, right?

11:14.160 --> 11:15.280
What happens?

11:15.280 --> 11:17.660
And he shows this, all this chain of events.

11:17.660 --> 11:22.540
He didn't save his brother, who fell in a frozen lake and died.

11:22.540 --> 11:28.740
And because of that, his brother in the war, in the second war, couldn't save a whole company.

11:28.740 --> 11:30.500
And so on and so forth.

11:30.500 --> 11:35.740
And eventually, the city where they live is very crappy.

11:35.740 --> 11:40.660
So the message is, any single event can change everything.

11:40.660 --> 11:48.120
And Steve Gould used that in the context of evolution, saying if we were able to replay

11:48.120 --> 11:54.080
again the tape of evolution, like starting 600 million years ago, the biosphere that

11:54.080 --> 11:57.080
we'll see now will be totally different, right?

11:57.080 --> 11:59.800
It will be an alien biosphere.

11:59.800 --> 12:01.400
I don't think that's the case, right?

12:01.400 --> 12:02.960
But this is the idea.

12:02.960 --> 12:07.720
Other people like Jack Monod also brought the idea that randomness plays a very, very

12:07.720 --> 12:10.420
important role in evolution.

12:10.420 --> 12:16.220
And more recently, I'm very much a book person and a movie person, and my students know that

12:16.220 --> 12:20.100
very well, and I'll make recommendations.

12:20.100 --> 12:26.740
Recently, Sean Carroll, the biologist, wrote this beautiful book where he kind of puts

12:26.740 --> 12:29.500
these random events that seem to be relevant.

12:29.500 --> 12:32.500
But this randomness really is so important.

12:32.500 --> 12:34.780
Let me show you the other part of the story.

12:34.780 --> 12:41.040
In fact, when we look at the natural world, we find out that very often the same solutions

12:41.040 --> 12:48.800
appear again and again and again independently in very different groups of organisms.

12:48.800 --> 12:50.640
I put the eyes here.

12:50.640 --> 12:57.040
The complex eye that we have, this camera eye, has been invented in evolution probably

12:57.040 --> 13:01.080
about 25 times in a totally independent way.

13:01.080 --> 13:06.860
We have this eye, and Octopy, for example, they have an eye that is essentially the same.

13:06.860 --> 13:10.420
In fact, it's better than ours, right?

13:10.420 --> 13:16.460
And this is what we call convergence, that the reason that these solutions appear again

13:16.460 --> 13:21.500
and again and again is that there are very, very strong constraints, even maybe mathematical

13:21.500 --> 13:24.820
laws that limit the possible, right?

13:24.820 --> 13:29.900
This is a book, Life Solution, sounds like a self-help book, but this is a book of evolution.

13:29.920 --> 13:35.600
It gives a lot of very interesting examples that go from cells and codes, genetic codes,

13:35.600 --> 13:37.480
to minds, right?

13:37.480 --> 13:45.160
And here is the Catalan scientist, Per Alberg, who unfortunately I met him years ago and

13:45.160 --> 13:50.640
died very, very soon, and he made the argument that even when you look at the structure of

13:50.640 --> 13:56.720
monstrosities, when you take the terephthalgists in, for example, in nature, you see that they

13:56.720 --> 13:58.000
are very well organized.

13:58.000 --> 14:04.420
You can make a taxonomy that is very well organized because not everything is possible.

14:04.420 --> 14:12.380
So in the context of brains, we wanted to do something about that because, and as one

14:12.380 --> 14:18.420
of the ambitions of the Santa Fe Institute, right, that think in really broad terms and

14:18.420 --> 14:20.740
ask ourselves difficult questions.

14:20.740 --> 14:26.760
So at some point, we talk about the idea of why not to try to make a kind of a space

14:26.760 --> 14:30.040
of brains, of cognitions.

14:30.040 --> 14:37.040
And in particular, it was clear that we have these solid brains, right?

14:37.040 --> 14:42.360
Brains that I'm going to go into that in a moment, brains that are made by neurons located

14:42.360 --> 14:48.880
in specific positions, right, and connected, and everything happens in the connections.

14:48.880 --> 14:54.700
But uncolonies are a kind of brain of brains that are moving around, they're liquid.

14:54.700 --> 15:01.540
Your immune system is a class of network that in many ways is like a neural network, right?

15:01.540 --> 15:04.940
But the cells are moving all the time.

15:04.940 --> 15:05.940
And so on and so forth.

15:05.940 --> 15:09.220
So what happens with all these cognitions?

15:09.220 --> 15:11.580
How likely are they?

15:11.580 --> 15:14.180
How powerful they can be, all right?

15:14.180 --> 15:19.800
So we decided to organize a little workshop with my colleagues, Melanie Moses and Stephanie

15:19.800 --> 15:22.800
Forrest, about liquid brains, solid brains.

15:22.800 --> 15:28.240
This is kind of a very ambitious idea because we brought together this group of very, very

15:28.240 --> 15:29.760
interesting people.

15:29.760 --> 15:35.120
But of course, we didn't know what was the outcome of this because we didn't have even

15:35.120 --> 15:38.480
a definition of liquid brains, right?

15:38.480 --> 15:46.820
So it was like our St. Patron Colma McCarthy said once, we met together to have more fun

15:46.820 --> 15:48.520
that should be legal.

15:48.520 --> 15:56.180
But the idea was actually to come out from that, from a first roadmap of cognitions.

15:56.180 --> 16:02.180
I put it here, this was a special issue that came from this, a list of examples, right,

16:02.180 --> 16:07.180
that include the microbiome, include plants, we'll talk about plants also because they're

16:07.180 --> 16:10.200
kind of a solid organism.

16:10.200 --> 16:16.920
And in around everything, we look for regularities.

16:16.920 --> 16:21.880
They are common laws that we can use, common languages that we can use, and networks appear

16:21.880 --> 16:25.000
to be the key here.

16:25.000 --> 16:32.320
In a standard brain, right, you have the, as I was saying, fixed positions for neurons.

16:32.320 --> 16:37.260
In a non-colony, your individual scarring brains are moving around, right?

16:37.260 --> 16:42.100
So they are no constant connections, the connections are broken and formed all the time.

16:42.100 --> 16:45.180
And the same happens for the immune system.

16:45.180 --> 16:49.560
So first question, why brains, right?

16:49.560 --> 16:56.980
What is the evolutionary force that actually pushes things towards complexity towards brains?

16:56.980 --> 17:04.000
This is one very nice hypothesis that we'll use in several times, which is the moving hypothesis.

17:04.000 --> 17:10.280
It essentially says that if you live in an environment, when you need to search, an uncertain

17:10.280 --> 17:14.640
environment, you look to search for resources, you don't know what resources are, you need

17:14.640 --> 17:19.280
to move, you need to detect, you need to sense, brains are great for that, right?

17:19.280 --> 17:24.700
They centralize information and centralize the way you move, okay?

17:24.720 --> 17:27.600
So that's important for a number of reasons.

17:27.600 --> 17:33.520
And the other way of looking at that is that brains, to a very large extent, are prediction

17:33.520 --> 17:34.520
systems.

17:34.520 --> 17:40.020
Some people say prediction machines, but they want to avoid the machine metaphor in the

17:40.020 --> 17:43.640
sense that prediction is absolutely fundamental.

17:43.640 --> 17:46.920
We predict all the time, right?

17:46.920 --> 17:53.960
Okay, so first of all, and it will be totally unfair with our brains, right?

17:53.980 --> 17:55.660
Because we don't have time.

17:55.660 --> 17:58.820
But I brought a brain, okay?

17:58.820 --> 18:04.540
Yeah, my family was so happy when I said that I had a brain, finally.

18:04.540 --> 18:08.020
So this is kind of a very good model, right?

18:08.020 --> 18:11.980
It kind of weights kind of a real brain.

18:11.980 --> 18:17.340
The human brain is a spectacular combination of accidents.

18:17.340 --> 18:21.420
So there are things that have been occurring over time.

18:21.420 --> 18:24.240
Many things we don't understand, right?

18:24.240 --> 18:25.240
But also of optimization.

18:25.240 --> 18:27.600
There's optimized circuits.

18:27.600 --> 18:32.600
As you probably know, it consumes a lot of the energy that we bring into our bodies,

18:32.600 --> 18:37.560
about 25%, which means that it has to be important, right?

18:37.560 --> 18:42.240
Not everyone uses that much, but it's important, right?

18:42.240 --> 18:48.760
And it's made of, we know now, 86 billion neurons.

18:49.180 --> 18:53.940
It was the Susana Eculano in Brazil, found out the way of counting.

18:53.940 --> 18:58.820
And it works in a very dynamical way, right?

18:58.820 --> 19:02.300
At any time, you record the activity of the brain,

19:02.300 --> 19:04.860
you will see waves moving around.

19:04.860 --> 19:08.980
We know now that these waves happen to occur between order and disorder,

19:08.980 --> 19:11.020
in what we call a critical point.

19:11.020 --> 19:13.660
And allow synchronize the system so

19:13.680 --> 19:19.480
that you can actually put together information for different places, right?

19:19.480 --> 19:23.200
And so have specialized areas working together, right?

19:23.200 --> 19:26.760
It's a compromise between try to modularize the system and

19:26.760 --> 19:29.800
try to put everything in place.

19:29.800 --> 19:34.720
So there are several brains that correspond to this solid picture.

19:34.720 --> 19:39.720
Very simple brains like Hydra, which is kind of a simple network.

19:40.700 --> 19:46.060
Some more complex nervous systems, right?

19:46.060 --> 19:51.140
But brains by themselves come later on in the evolutionary history.

19:51.140 --> 19:57.140
And again, going into this idea of randomness versus strong laws.

19:59.180 --> 20:00.540
Is the brain unique?

20:00.540 --> 20:04.140
Could be very different ways of generating brains.

20:04.140 --> 20:08.540
And in the recent years, people have been trying to think about that.

20:08.560 --> 20:12.600
Well, we can make a theory of that and try to understand what makes the brain

20:12.600 --> 20:18.480
special or maybe what makes the brain inevitable, all right?

20:18.480 --> 20:22.480
So we could say, but if you want to build a revolutionary story of brains,

20:22.480 --> 20:24.880
that's a difficult task, isn't it?

20:24.880 --> 20:30.040
Because, for example, in terms of language or

20:30.040 --> 20:34.640
the mind, you could say language, which is very central.

20:34.660 --> 20:37.620
In the next lecture, we'll see how important is this and

20:37.620 --> 20:41.300
how robots can help understand the origins of language.

20:41.300 --> 20:47.260
But language, for example, somebody could say, it doesn't leave fossils.

20:47.260 --> 20:48.500
The mind doesn't leave fossils.

20:48.500 --> 20:50.860
Well, it's not completely true.

20:50.860 --> 20:54.260
This is an example that I wanted to bring because it's simple.

20:54.260 --> 20:58.420
But it brings an idea of what kind of things we can recognize.

20:58.420 --> 21:00.900
Does anybody see that there's a strong regularity here?

21:05.620 --> 21:09.460
These are hands of different people, right?

21:09.460 --> 21:14.220
That painted is the cave of the hands in Argentina.

21:14.220 --> 21:20.860
And what happens here is that if you look, it's all left hands, all of them, right?

21:20.860 --> 21:26.900
Because the people who use whatever they use to paint, you're using the right hand, right?

21:26.900 --> 21:32.460
And you know that nowadays, most people is right handed, okay?

21:32.680 --> 21:35.520
And that symmetry was there already.

21:35.520 --> 21:39.080
So we can see there's a trace of that particular thing.

21:39.080 --> 21:41.120
But brains do spectacular things.

21:41.120 --> 21:45.480
And particularly human brain with a visual cortex,

21:45.480 --> 21:52.280
which is an amazing system that we have been using to do a lot of

21:52.280 --> 21:54.400
very important things that we will discuss.

21:54.400 --> 21:57.640
But one particular experiment we can bring and

21:57.640 --> 22:03.060
that allows me to go into how we make theories of the brain is this, right?

22:03.060 --> 22:08.300
In the left, you have a picture which has been kind of pixelated and

22:08.300 --> 22:12.620
you only retain part of the texture, right?

22:12.620 --> 22:13.780
Everybody sees what is there?

22:15.460 --> 22:16.780
There's a dock.

22:18.380 --> 22:21.100
The usual thing, it's a pattern here, right?

22:21.100 --> 22:22.500
We don't know exactly why.

22:22.500 --> 22:25.100
The pattern is some people already know.

22:25.120 --> 22:27.880
Some people see the dock, right?

22:27.880 --> 22:30.240
It's kind of like a Dalmatian.

22:30.240 --> 22:34.680
Some people doesn't see anything, but when you say there's a dock, right?

22:34.680 --> 22:37.200
The brain finds out, okay?

22:37.200 --> 22:40.800
And then there's people who don't see it, right?

22:40.800 --> 22:42.320
It's okay.

22:42.320 --> 22:46.600
But the thing is that that kind of pattern recognition system,

22:46.600 --> 22:52.640
which is extremely effective, we use constantly since we are toddlers almost,

22:52.660 --> 22:57.220
is impossible to simulate with a standard program.

22:57.220 --> 23:00.580
You cannot write a computer program to do this, right?

23:02.700 --> 23:10.740
On the right part, what I'm putting is also the fact that once you recognize

23:10.740 --> 23:16.660
that particular pattern here, it's going to be stuck in your brain forever.

23:16.660 --> 23:22.020
So one day you go, in ten years ahead, you go to the house of a friend and

23:22.040 --> 23:27.440
on top of the table is this picture covered in part, right?

23:27.440 --> 23:31.080
It's very likely that those of you who recognize that say,

23:31.080 --> 23:37.200
look, this is from that great lecture I went, right?

23:38.720 --> 23:42.240
So how do we actually make a model of this, right?

23:42.240 --> 23:46.520
It would appear that it has to be a very complex, very complicated model.

23:46.520 --> 23:53.180
And everything starts and comes from the work of these two amazing characters,

23:53.180 --> 23:59.140
Warren McCulloch and Walter Pitts, who actually came about with

23:59.140 --> 24:02.820
the first formal model of a neuron.

24:02.820 --> 24:08.740
And from that comes out all the neural networks that we use in

24:08.740 --> 24:12.820
all the artificial neural applications, chat GPT and everything else.

24:13.760 --> 24:18.200
We don't have time to talk about them, but if you search a little bit,

24:18.200 --> 24:24.760
in particular Pete's on the right, he was a child prodigy.

24:24.760 --> 24:27.120
He has a very, very interesting story.

24:27.120 --> 24:28.560
They figure out how to do it.

24:28.560 --> 24:31.480
And what they did was to transform a neuron,

24:31.480 --> 24:36.400
which is a really complex system itself, into a mathematical representation.

24:36.400 --> 24:40.200
To make the long story short, the idea is you have a neuron,

24:40.220 --> 24:43.780
you have inputs from other neurons that set signals.

24:43.780 --> 24:48.300
The signals can be positive, trying to make you to activate, or

24:48.300 --> 24:52.340
can be negative, try to make you put down, getting active.

24:52.340 --> 24:54.180
And you wait everything.

24:54.180 --> 24:56.820
And once you wait, you have a threshold.

24:56.820 --> 25:00.060
And if you go beyond the threshold, you activate, right?

25:00.060 --> 25:04.460
And then the neuron sends another signal somewhere else, okay?

25:04.460 --> 25:08.740
You can bring that mathematically in a very elegant way, okay?

25:08.740 --> 25:12.200
And you can use now that for modeling a lot of things.

25:12.200 --> 25:17.320
For example, John Hopfield, this amazing model,

25:17.320 --> 25:22.240
which I will put in a nutshell as follows.

25:22.240 --> 25:24.560
Imagine you have a collection of neurons.

25:24.560 --> 25:29.200
For me, my neurons will be elements that they just are on and off, right?

25:29.200 --> 25:31.360
They are active or inactive.

25:31.360 --> 25:34.920
And I can put them to the dimensional layer,

25:34.940 --> 25:39.260
like in a retina, like it's something that detects images, okay?

25:40.420 --> 25:44.500
Now, it's possible to show, I connect everyone with everyone.

25:44.500 --> 25:48.340
I use the maculok pits, threshold units, right?

25:48.340 --> 25:52.860
And then it's possible to train the network, show images,

25:52.860 --> 25:55.140
letters, whatever it is, okay?

25:55.140 --> 26:01.460
So the network learns how, well, the rule is very simple.

26:01.460 --> 26:04.860
If two neurons receive the same information, for example,

26:04.860 --> 26:09.240
two black pixels, they reinforce their connection.

26:09.240 --> 26:14.680
If they receive contradictory signals, black pixel, white pixel,

26:14.680 --> 26:17.640
the connection between them is reduced.

26:19.000 --> 26:24.120
That's all, no long computer program, nothing.

26:24.120 --> 26:28.960
And you can show, using this, that this network is capable of doing

26:28.960 --> 26:31.960
precisely what I was showing you before, right?

26:31.980 --> 26:35.780
It's possible to show that in a space that is highly dimensional,

26:35.780 --> 26:38.180
that depends on the number of connections, right?

26:38.180 --> 26:41.900
You create kind of valleys, right?

26:41.900 --> 26:47.300
This abstract space, valleys where the bottom of the valleys are the memories.

26:47.300 --> 26:51.820
The things that you have made the network to learn.

26:51.820 --> 26:58.060
For example, imagine that I have trained the network to learn only two images, right?

26:58.060 --> 26:59.940
This one here, that one here.

26:59.960 --> 27:02.400
You will create two valleys, and

27:02.400 --> 27:05.440
in the bottom of the valleys you have the memories.

27:05.440 --> 27:06.560
In what sense?

27:06.560 --> 27:11.760
In the sense that, if now I show an image that is incomplete,

27:11.760 --> 27:14.240
is the form, is distorted, right?

27:14.240 --> 27:17.000
The network, just using the dynamics of my Culloch and

27:17.000 --> 27:20.520
Pitts goes down the valley, right?

27:20.520 --> 27:23.120
And reconstruct all the information, right?

27:24.320 --> 27:27.400
I think this is totally amazing, right?

27:27.400 --> 27:31.580
And shows the power of artificial neural networks.

27:31.580 --> 27:36.780
But of course, this corresponds to a given model for

27:36.780 --> 27:38.860
a given class of brains.

27:38.860 --> 27:43.900
What about the potential universe possibilities, right?

27:43.900 --> 27:45.020
I could imagine things.

27:45.020 --> 27:49.980
These are, my drawing is about four things that we don't observe,

27:49.980 --> 27:51.660
don't exist, right?

27:51.660 --> 27:52.220
We'll offer them.

27:52.220 --> 27:57.040
And in my lecture today,

27:57.040 --> 28:02.760
I want to explore this idea of what kind of brains are there.

28:04.360 --> 28:06.560
Whether there are very strong constraints, whether or

28:06.560 --> 28:12.680
not brains are expected or there's a lot of possibilities.

28:12.680 --> 28:16.040
And I'm going to use, there's a huge amount of examples here.

28:16.040 --> 28:19.400
I'm going to use ants as one example.

28:19.400 --> 28:23.420
Fissarum, which is kind of a very alien creature.

28:24.700 --> 28:27.140
Discuss about plants, because there's been a lot of discussion in

28:27.140 --> 28:29.900
the recent years about plant intelligence.

28:29.900 --> 28:34.900
And show you that we can start to think in a space of possible

28:34.900 --> 28:37.780
conditions, and how we do it, right?

28:39.020 --> 28:44.660
So a very important point, liquid versus solid, right?

28:45.080 --> 28:51.560
On the right hand side, you have a very small part of a neural

28:51.560 --> 28:53.600
network of neurons connected.

28:53.600 --> 28:56.880
Again, their locations remain the same over time.

28:56.880 --> 29:06.000
And on the left, it's a very tiny part of a swarm of army ants, right?

29:06.000 --> 29:08.760
I was very impressed when I was in Panama years ago,

29:08.760 --> 29:12.560
seeing the army ants, which are blind.

29:12.560 --> 29:17.940
They communicate in simple ways with their nest mates.

29:17.940 --> 29:24.100
But forming these huge forms that, if you look from the distance,

29:24.100 --> 29:27.620
look like a single organism moving around, right?

29:27.620 --> 29:30.340
Swarming in the middle of the forest, quite a thing.

29:31.460 --> 29:37.580
And so we want to see how we approach all these problems.

29:37.580 --> 29:41.580
And in particular, one good question is, is a liquid brain

29:41.600 --> 29:44.640
able to be as complex as a solid brain?

29:46.140 --> 29:50.000
And the question is, is it relevant for a number of reasons?

29:51.800 --> 29:56.200
One of the reasons is, let me see this, oops, there's a movie here.

29:58.200 --> 30:00.720
Oh, yes, okay, sorry.

30:06.320 --> 30:06.840
Let me see.

30:06.840 --> 30:12.180
And the question is, in particular, because one good comparison we

30:12.180 --> 30:14.620
can make is about us, right?

30:14.620 --> 30:18.780
The, oh, man, what's happened?

30:23.280 --> 30:23.780
Let me see.

30:26.980 --> 30:27.500
Get into me.

30:27.500 --> 30:36.820
And the question, of course, for a colony of ants is who is in charge?

30:37.140 --> 30:39.860
I mean, ants are moving around, right?

30:39.860 --> 30:44.300
It's not like a centralized system saying you have to do these and

30:44.300 --> 30:45.540
that and that, right?

30:45.540 --> 30:52.140
It's not such a thing as a queen giving orders to everyone else, right?

30:52.140 --> 30:58.540
The queens in a way, as in European monarchies, are very useless,

30:58.540 --> 31:01.700
except in this case, to put X, right?

31:01.700 --> 31:03.380
So how do you control that?

31:03.380 --> 31:09.200
And this sentence by Deborah Gordon, I think, is very to the point, right?

31:09.200 --> 31:11.960
And for those of you who are fans of Richard Feynman,

31:11.960 --> 31:15.360
Richard Feynman himself was interested in ants and

31:15.360 --> 31:20.040
started to bring quite interesting questions about how ants work, right?

31:20.040 --> 31:21.200
Well, two important things to say.

31:23.040 --> 31:26.280
What makes ants and termites and social insects,

31:26.280 --> 31:30.040
I can only address a few things, special.

31:30.040 --> 31:35.100
And what do they do that is close to the brains that we were discussing,

31:35.100 --> 31:36.820
the human brain, for example?

31:36.820 --> 31:41.860
Well, on the one hand, they have managed to modify their environments.

31:41.860 --> 31:45.340
They do what we call extended minds, right?

31:45.340 --> 31:50.860
They create superstructures and the nests are the clear example.

31:50.860 --> 31:54.580
And the nests can be extremely large compared with the single organisms.

31:54.580 --> 31:58.780
In a termite nest, in some cases, you can have the very tiny,

31:58.800 --> 32:05.160
homilimetric organisms, whereas the nest for termites in some parts of

32:05.160 --> 32:08.520
Africa can be three, four meters high, right?

32:08.520 --> 32:11.640
So ten orders of magnitude larger.

32:11.640 --> 32:13.440
How do you build that?

32:13.440 --> 32:16.280
Of course, termites don't know anything about that.

32:16.280 --> 32:17.840
Again, they are blind.

32:17.840 --> 32:21.120
They communicate in simple ways with chemistry.

32:21.120 --> 32:26.320
So whatever creates the organization comes out from a collective phenomenon,

32:26.340 --> 32:31.780
from self-organization, or what we call in complexity, emerging phenomena.

32:31.780 --> 32:36.580
Phenomena that we can describe that even scale, network architecture,

32:36.580 --> 32:39.900
or in our brains, consciousness, memory.

32:39.900 --> 32:44.460
And that cannot be reduced to the properties of the individual parts.

32:44.460 --> 32:47.940
You can spend your whole life studying single termites.

32:47.940 --> 32:50.900
You'll never understand how they build the nests, right?

32:50.900 --> 32:56.020
The key is that the collective behavior, the fact that the interaction between,

32:56.020 --> 33:01.600
for example, termites and the material they use creates amplification phenomena

33:01.600 --> 33:06.840
that lead to self-organization, in particular, to order patterns.

33:06.840 --> 33:13.920
That picture there is a small part of the fungi factory that you have in

33:13.920 --> 33:16.360
termite nest, in some cases, right?

33:16.360 --> 33:21.240
And the explanation of the mathematics comes from what's called tuning structures.

33:21.240 --> 33:24.080
But it emerges from the interactions, right?

33:24.080 --> 33:25.520
In a liquid brain.

33:25.520 --> 33:28.340
Another thing that is quite fascinating is,

33:28.340 --> 33:32.460
ants can solve the problem of finding out the shortest distance.

33:32.460 --> 33:34.960
How?

33:34.960 --> 33:38.140
Imagine you have the ants like in the movie, right?

33:38.140 --> 33:43.060
That are in the lab, you put a food source somewhere, and here is the nest.

33:43.060 --> 33:48.820
If some ant detects that there's food here, they deliver a chemical signal,

33:48.820 --> 33:54.060
which other ants find out reinforce, and eventually you create a signal

33:54.060 --> 33:56.460
that goes beyond the individuals, right?

33:56.460 --> 33:59.740
And individuals in the signal interact in nonlinear ways, and

33:59.740 --> 34:04.100
you make this trail of ants that exploit the source very quickly.

34:04.100 --> 34:05.140
What happens if you put two?

34:06.300 --> 34:11.900
Well, the things can also be dependent on its scenario and its species.

34:11.900 --> 34:15.140
But for example, if I have more food here than here,

34:15.140 --> 34:22.340
they split at the beginning, but the source that is more abandoned is reinforced.

34:22.340 --> 34:25.900
What happens if you make an experiment like the following?

34:25.900 --> 34:30.700
Imagine you have the nest, you have the resource, the food here,

34:30.700 --> 34:32.900
and you have a double bridge, right?

34:32.900 --> 34:35.700
So there are two branches, and one is longer than the other.

34:36.740 --> 34:42.540
Okay, individual ants are unable to know that, if this is longer or shorter.

34:42.540 --> 34:45.580
But since they leave a chemical trail, right?

34:45.580 --> 34:50.220
The longer part will lose, because of course,

34:50.220 --> 34:55.740
the chemical signal is dissipated, is evaporated, will lose more by evaporation.

34:55.740 --> 35:00.140
And eventually, everyone will go into the shorter chain, right?

35:00.140 --> 35:04.620
So you need a collective phenomenon here to actually solve the problem of

35:04.620 --> 35:06.100
the shortest path, okay?

35:07.700 --> 35:09.660
And then you go into the question.

35:10.700 --> 35:16.380
Is an an colony going to be as complex in terms of cognition as a brain?

35:18.380 --> 35:20.140
I think the answer is no.

35:20.140 --> 35:24.340
And the reason is, you can represent ants in different ways.

35:24.340 --> 35:28.740
And I want to remember you that the ants have brains, right?

35:28.740 --> 35:31.540
They are not neurons, they are brains.

35:31.540 --> 35:35.740
But in brains, they can be half a million neurons, so it's not small.

35:37.260 --> 35:41.460
But interestingly, when you make models of how the ants solve the shortest path

35:41.460 --> 35:45.940
problem, how they build their nest, you can represent the ants in extremely simple

35:45.940 --> 35:46.820
ways, right?

35:46.820 --> 35:50.500
Sometimes even in on and off systems.

35:50.500 --> 35:55.380
And so you can use that, you can represent the ants in this way.

35:55.380 --> 36:00.940
And instead of using the McCulloch-Pitts model where neurons exchange things

36:00.940 --> 36:06.900
in a fixed way, you can actually make what we call a liquid brain.

36:06.900 --> 36:11.340
Here, for me, each ant can be, for example, an active or

36:11.340 --> 36:15.060
an inactive neuron, they move around, right?

36:15.060 --> 36:19.620
And over time, they interact exactly the same way that neural networks,

36:19.620 --> 36:22.940
except that now they are changing in time, they are moving.

36:22.940 --> 36:29.980
And one of the beautiful examples that we have explored comes from a war by

36:29.980 --> 36:34.180
Deborah Gordon and colleagues, where actually they have these ants that

36:34.180 --> 36:39.420
live in the desert, and you can see the ants doing special tasks, right?

36:39.420 --> 36:43.420
They are exactly the same morphological identical, but they do different tasks.

36:43.420 --> 36:48.380
They can forage, they can have nest maintenance, they patrol.

36:48.380 --> 36:53.660
And you see that the same ants, over time, they might change task, right?

36:53.660 --> 36:59.540
And also, if you are a bad person and you take, for example, all the foragers, right?

36:59.540 --> 37:00.820
And see what happens.

37:00.820 --> 37:03.660
What happens is the colony reorganizes.

37:03.660 --> 37:10.660
So some ants that maybe were just making nest maintenance become foragers.

37:10.660 --> 37:15.620
It reorganizes in such a way that it optimized the number of individuals that

37:15.620 --> 37:17.340
do each task.

37:17.340 --> 37:21.340
But then if you represent that with a mathematical model, etc., what you realize

37:21.340 --> 37:27.860
is, remember the model I showed you with these ballies that were the memories, right?

37:27.860 --> 37:29.860
Here you have also ballies.

37:29.860 --> 37:35.380
But in this landscape, what is in the bottom is the number of ants that

37:35.380 --> 37:37.340
are involved in each task.

37:37.340 --> 37:42.500
Which is something that is much, much, much less rich, right?

37:42.500 --> 37:48.900
You are not exploring a hyper-dimensional space of connections, because the connections

37:48.900 --> 37:51.060
are destroyed all the time.

37:51.060 --> 37:55.260
You generate something that has to do with the average number of things needed so that

37:55.260 --> 37:56.860
the colony works.

37:56.860 --> 38:06.740
Which for us, suggests that things like that depend on liquidity may be very limited.

38:06.740 --> 38:11.860
In the science fiction literature, or in the movies, for example, Star Trek, I'm not

38:11.860 --> 38:15.220
a tricky person, but it's an interesting example.

38:15.220 --> 38:16.820
They propose this idea, the Borgs.

38:16.820 --> 38:24.420
The Borgs is kind of a race of cyborgs, each one with a big brain, right?

38:24.420 --> 38:30.620
With a queen that has a big brain and controls some things, right?

38:30.620 --> 38:38.580
But the thing is, right, why do we don't see, for example, what I call brainy ants?

38:38.580 --> 38:41.860
We don't see ants with a large brain, right?

38:41.860 --> 38:44.980
It's a possibility in the space of what we can imagine.

38:44.980 --> 38:50.660
And what we find out is, although the theory has to be developed, is that actually, if

38:50.660 --> 38:56.380
you look closely, you find some things that are extremely interesting.

38:56.380 --> 39:05.620
For example, it seems to be a trend for colonies that in a nutshell, the pattern is, you have

39:05.620 --> 39:09.740
a very small colony, individuals can be complex.

39:09.740 --> 39:11.540
I saw them in Panama also.

39:11.540 --> 39:18.660
I saw these groups of ants, colonies with 100 individuals only, very large ants, everybody

39:18.660 --> 39:21.380
warned us, don't touch them, right?

39:21.380 --> 39:26.260
Because they are called 24 hours, that's the time we'll suffer the pain.

39:26.260 --> 39:33.500
So I believe that, one of my colleagues didn't, and the experimental method.

39:33.500 --> 39:37.580
And it was interesting because you approach the colony and the ants were outside with

39:37.580 --> 39:41.300
big eyes, and clearly they saw us.

39:41.300 --> 39:46.260
And if you look at the individuals, they were more or less making their decisions without

39:46.260 --> 39:48.620
much worrying about anything else.

39:48.620 --> 39:54.500
But if you take very large colonies where the colony itself can do extremely complex

39:54.580 --> 40:00.060
things, interestingly, individuals get more and more and more dumb.

40:00.060 --> 40:02.620
As if there was a trade of fear.

40:02.620 --> 40:06.500
This has been called the complexity drain.

40:06.500 --> 40:09.780
The complexity drain is something that we need to develop the theory of that.

40:09.780 --> 40:15.740
Essentially, we'll say that the more complex the society, the less complex the individuals,

40:15.740 --> 40:16.740
right?

40:16.740 --> 40:20.940
Don't try to apply that to our societies, okay?

40:20.940 --> 40:27.940
Anyways, my second example, my second example has to do with an extraordinary organism.

40:28.140 --> 40:30.380
And I wanted to start with this.

40:30.380 --> 40:37.380
This is a labyrinth that is in Barcelona in the orta quarter that is a replica of the

40:37.380 --> 40:42.300
famous labyrinth of Gnosis, the minotaur, right?

40:42.300 --> 40:46.740
Labyrinths have been something that mathematicians and all kinds of people have been fascinated

40:46.740 --> 40:47.740
in.

40:47.740 --> 40:49.540
How do you escape from a labyrinth?

40:49.540 --> 40:53.780
Or is this the entrance and is this the exit?

40:53.780 --> 40:57.300
How do you find the shortest path, for example, okay?

40:57.300 --> 40:59.300
So Fissarum is able to do that.

40:59.300 --> 41:06.300
Fissarum is not an animal, not a plant, is a slime mold, is a very simple creature.

41:08.060 --> 41:15.060
And actually, some people describe it as a single cell, is a whole thing with many nuclei

41:16.060 --> 41:22.900
inside, but essentially it is a single cell, except that it is extremely large, right?

41:22.900 --> 41:24.100
You can see it in the forest.

41:24.100 --> 41:30.700
And actually, when it was found many years ago in the time of the Sputnik, the people

41:30.700 --> 41:35.540
who found out that blob, which is yellowish as it is like this, and it can be large as

41:35.540 --> 41:40.460
my hand, found in the forest, they thought that it was kind of an alien thing, right?

41:40.460 --> 41:43.100
That came with a spacecraft, of course.

41:43.460 --> 41:49.500
Fissarum is amazing, it shifts all the time, has these network structures, sometimes looks

41:49.500 --> 41:55.180
like a neural network, and searches in space looking for resources, right?

41:55.180 --> 41:59.700
And if part of Fissarum finds something that is very rich, and he finds out that it's not

41:59.700 --> 42:03.180
so rich, it deviates everything in this direction.

42:03.180 --> 42:09.100
So you have all these tubes that pulsate over time, it's quite a thing.

42:09.100 --> 42:13.900
And that gets thicker and thicker as you approach something that is richer.

42:13.900 --> 42:18.340
And somebody used that in a very clever way, right?

42:18.340 --> 42:26.580
So since Fissarum is so easy to cultivate, one thing I can do is take pieces of Fissarum,

42:26.580 --> 42:31.900
put it within a labyrinth, an amaze, right, like here.

42:31.900 --> 42:38.140
And then I'm going to use what Fissarum likes a lot, which is flakes, right?

42:38.140 --> 42:45.620
One at the entrance, A, one at the exit, B. And over time, what you see in the movie

42:45.620 --> 42:48.580
is that Fissarum is detecting two sources, right?

42:48.580 --> 42:54.020
It's totally distributed, there's no centralized mind, there's no neurons at all, okay?

42:54.020 --> 42:57.180
And what happens is that it's an amplification phenomenon.

42:57.180 --> 43:02.180
As you go over time, you see that close to entrance and exit, right?

43:02.180 --> 43:08.980
The tubes that Fissarum forms, that is what they need to actually push forward the detection

43:08.980 --> 43:11.340
and exploitation, right?

43:11.340 --> 43:14.220
They have these nice waves.

43:14.220 --> 43:21.820
Eventually what happens is that you get in a single tube that goes all over the place

43:21.820 --> 43:26.820
in the shortest path from the entrance to the exit, okay?

43:26.820 --> 43:30.940
But if you want to model Fissarum, it's possible to do it.

43:30.940 --> 43:33.420
You model it with a network, a network of what?

43:33.420 --> 43:36.660
Of tubes connected like a fluid, all right?

43:36.660 --> 43:41.780
And interestingly, the mathematics that is behind is a threshold network, right?

43:41.780 --> 43:46.940
Even in that case, you need to use one mathematics that might be universal.

43:46.940 --> 43:50.420
Fissarum has been used in a number of applications, mazes.

43:50.420 --> 43:55.820
You can actually find out a network of roads.

43:55.820 --> 44:01.180
You take a country, for example, you put flakes in the locations of different cities.

44:01.180 --> 44:04.460
Fissarum is distributed all over the place.

44:04.460 --> 44:11.540
And then the tubes that reinforce the connections between different pairs of sources, right?

44:11.540 --> 44:17.820
Eventually draw this map, which you find out that is more optimal than the ones that engineers

44:17.820 --> 44:19.060
built.

44:19.060 --> 44:23.900
And it's been also used to actually map dark matter, right?

44:23.900 --> 44:28.940
Some astrophysicists find out the way of actually use Fissarum to make a large-scale

44:28.940 --> 44:34.180
model of the universe and infer the distribution of dark matter.

44:34.180 --> 44:36.300
But I want to make a point.

44:36.300 --> 44:41.540
Very often, and you can make logic gates and many things, very often it's said, look, Fissarum

44:41.540 --> 44:44.860
can solve complex mathematical models.

44:44.860 --> 44:50.980
And that is a distortion a little bit of what really happens.

44:50.980 --> 44:57.380
We exploit the properties of Fissarum, this extraordinary capacity of searching around.

44:57.380 --> 45:03.420
And this special capacity that, and that's very important, is based in a way of computing

45:03.420 --> 45:08.020
things, a computation that has nothing to do with the standard computation we use.

45:08.020 --> 45:11.020
The computation is the form, the shape.

45:11.020 --> 45:14.700
The final shape is what is being computed.

45:14.700 --> 45:18.820
But of course, we, the humans, we put what in physics, we say the boundary conditions

45:18.820 --> 45:19.820
or in mathematics, right?

45:19.820 --> 45:26.620
We put in place things and Fissarum just goes on with its dynamics, right?

45:26.620 --> 45:29.660
So Fissarum doesn't solve problems in nature.

45:29.660 --> 45:34.620
Solve problems that have to do with resources, but not mathematics, okay?

45:34.620 --> 45:36.900
What about plants, right?

45:36.900 --> 45:41.380
I hope there are not so many enthusiasts here of plant intelligence.

45:41.380 --> 45:44.100
I'm a skeptic.

45:44.100 --> 45:49.380
In the literature also, we have this, I don't know if everyone knows, the day of the triphids.

45:49.380 --> 45:52.220
It's a classic novel science fiction.

45:52.220 --> 45:56.660
We have these plants that are capable of moving, right?

45:56.660 --> 46:01.340
And that, well, I don't want to spoil anything.

46:01.340 --> 46:02.340
Just read it.

46:02.340 --> 46:04.140
It's really cool.

46:04.140 --> 46:07.060
So are plants intelligent?

46:07.060 --> 46:11.300
Well, let me say first something.

46:11.300 --> 46:12.780
Plants are extraordinary.

46:12.780 --> 46:15.900
They have really transformed completely the planet.

46:15.900 --> 46:19.340
When they invade land, they invented the sorts.

46:19.340 --> 46:21.300
They created the forests.

46:21.300 --> 46:26.140
They have this system of photosynthesis that creates quite an amazing super molecular system

46:26.140 --> 46:29.660
with quantum properties that we're still trying to understand.

46:29.660 --> 46:32.140
So they are amazing by themselves, right?

46:32.140 --> 46:36.340
Do we need them to like Mozart?

46:36.340 --> 46:37.340
Maybe not.

46:37.340 --> 46:38.340
Maybe not.

46:38.340 --> 46:44.420
Have in mind that plants, on the one hand, have this extraordinary capacity of changing

46:44.420 --> 46:49.700
morphology, of adapting in a way that animals cannot do.

46:49.700 --> 46:52.660
They give them a lot of advantage.

46:52.660 --> 46:58.980
The thing is, when you look at plants and plants in the context, right?

46:58.980 --> 47:01.060
Like in a forest.

47:01.060 --> 47:06.500
Of course, there's a lot of complexity that has to do with things that we know from ecology.

47:06.580 --> 47:15.180
Competition, competition, mutually is in place a very, very important role in many ways.

47:15.180 --> 47:19.380
We start to uncover a lot of complexities there.

47:19.380 --> 47:23.460
But is really this connected to cognition?

47:23.460 --> 47:26.220
And I think it's important to go and look closely.

47:26.220 --> 47:28.340
What do we have?

47:28.340 --> 47:34.300
Plant cells, and there's a big constant with animal cells, are very rigid.

47:34.300 --> 47:41.540
We have this wall that makes connections between them extremely constrained, right?

47:41.540 --> 47:48.140
Connections, plasma of this matter, like that on the right in the upper picture is an electron

47:48.140 --> 47:53.060
microscope picture of a channel that connects to plant cells.

47:53.060 --> 47:56.460
But the architecture constrains a lot what happens there.

47:56.460 --> 48:01.980
Of course, there are no neurons, it's been told that there are analogies, but no neurons.

48:02.980 --> 48:05.780
Everything very much goes into two directions.

48:05.780 --> 48:07.500
One is growth.

48:07.500 --> 48:10.820
I have to grow and grow in a plastic way.

48:10.820 --> 48:12.980
Another is defense, right?

48:12.980 --> 48:21.420
Plants have developed a huge battery of chemical signals that connect them with the challenge

48:21.420 --> 48:24.780
that we have from insect herbivores in particular, right?

48:24.780 --> 48:26.580
There's a lot of investment in that.

48:26.580 --> 48:30.740
And it shows, it shows very much.

48:30.740 --> 48:36.780
On the other hand, one thing I wanted to mention, we will discuss that in the second lecture,

48:36.780 --> 48:43.820
but John von Neumann, the mathematician that I showed you before, in his studies about

48:43.820 --> 48:49.460
brains, unfortunately it wasn't at the end of his life, of brains versus computers, he

48:49.460 --> 48:52.060
made this the following point.

48:52.060 --> 48:57.500
At that time, the computers, these very big electronic computers, were very prone to fail

48:57.500 --> 49:01.820
because the basic components were not much reliable, right?

49:01.820 --> 49:06.500
And if a vacuum tube failed, the computer could fail.

49:06.500 --> 49:08.820
And they knew that brains don't work like that.

49:08.820 --> 49:12.100
Your neurons, every day you lose neurons.

49:12.100 --> 49:18.380
And you can even have a big loss of neurons and the brains capable of have plasticity

49:18.380 --> 49:23.420
to return the system to the previous state, not computers.

49:23.540 --> 49:29.540
He ended up in a conclusion which was, maybe we need systems that are very redundant, right?

49:29.540 --> 49:30.620
Very, very redundant.

49:30.620 --> 49:32.420
That's part of the solution, really.

49:32.420 --> 49:36.100
But if you look at plants and compare with animals, what is the difference?

49:36.100 --> 49:37.980
Well, many differences, right?

49:37.980 --> 49:42.260
On the one hand, they stack in the same place, right?

49:42.260 --> 49:47.140
So you think in the moving hypothesis, if I have to move, I need brains.

49:47.140 --> 49:52.340
If I don't, maybe I don't need brains.

49:52.340 --> 49:54.780
On the other hand, for example, you think in organs.

49:54.780 --> 50:00.260
If I ask you how many organs you have, you are not going to say, well, I don't know exactly.

50:00.260 --> 50:02.140
You do know, right?

50:02.140 --> 50:03.860
You do.

50:03.860 --> 50:06.500
One heart, two kidneys, et cetera, et cetera.

50:06.500 --> 50:10.380
And all of this is decided in embryogenesis.

50:10.380 --> 50:12.220
In plants, we see a very different situation.

50:12.220 --> 50:15.060
How many organs we have?

50:15.060 --> 50:16.060
Many.

50:16.060 --> 50:18.620
Every single leaf is an organ, right?

50:19.500 --> 50:24.260
They are formed and degrade and happen all the time.

50:24.260 --> 50:29.660
And also, for example, at the level of leaves, we have discovered that if you analyze the

50:29.660 --> 50:35.100
network of transport within leaves, which is a beautiful structure, you can see that

50:35.100 --> 50:38.740
it's optimized for doing two things.

50:38.740 --> 50:45.940
One, deliver the nutrients everywhere, right, in the most efficient way.

50:46.700 --> 50:52.060
To protect themselves from damage, an insect can make a hole.

50:52.060 --> 50:57.340
You have seen, for sure, leaves that are damaged in different ways, but you want to warranty

50:57.340 --> 50:58.620
that you get there.

50:58.620 --> 51:01.060
The transport keeps going well.

51:01.060 --> 51:04.660
If you have loops, the right amount of loops, you can do it.

51:04.660 --> 51:12.660
And this is a picture of one of these damage experiments where using a kind of fluorescent

51:12.660 --> 51:13.780
marker.

51:13.780 --> 51:19.220
You can see how it propagates and goes into the whole leaf again using the loops.

51:19.220 --> 51:20.220
And it's optimized.

51:20.220 --> 51:21.900
You can do a theory of that.

51:21.900 --> 51:29.100
So you have many organs that can be lost, are essentially redundant, so you don't really

51:29.100 --> 51:32.780
need to have a central control system.

51:32.780 --> 51:36.940
In the second lecture, I'll try to convince you that this is probably the case, right,

51:36.940 --> 51:42.180
that plants, because of they don't need that, and they are extraordinarily well-adapted

51:42.180 --> 51:47.940
in different ways, right, might not have anything like brains or intelligence.

51:47.940 --> 51:55.580
So fissurum, plants, brains, ants, how do we put this together?

51:55.580 --> 52:02.020
And the idea, still work in progress, right, is to create what we call a morphospace, a

52:02.020 --> 52:04.100
space of possibilities.

52:04.100 --> 52:09.060
In this particular example that we used some years ago, I used three axes.

52:09.060 --> 52:11.980
The vertical axis is how important is development.

52:11.980 --> 52:13.300
Design is very important.

52:13.300 --> 52:15.180
Nature constructs things, right?

52:15.180 --> 52:20.540
You have embryos that develop and get into complex architectures.

52:20.540 --> 52:27.500
In the horizontal axis is the liquid to solid, right, the state of matter that we have.

52:27.500 --> 52:33.380
And in the other axis is how complex is your cognition, how complex are your decision-making

52:33.380 --> 52:38.620
and how diverse is the way you actually sense and respond to the environment.

52:38.620 --> 52:43.700
We locate things here in relative positions, okay?

52:43.700 --> 52:50.740
For example, organs are kind of simple cognitive systems, right, or we could put, they are

52:50.740 --> 52:56.740
the outcome of development, they might be just responding to simple signals, or they

52:56.740 --> 53:00.580
might involve feedbacks that are more complex.

53:00.580 --> 53:03.820
This is the artificial part of the story.

53:03.820 --> 53:08.980
Organisms and organs have a counterpart in bioengineering, which is organoids, right,

53:08.980 --> 53:14.780
which opens the possibilities that we will discuss in the second lecture.

53:14.780 --> 53:21.620
Of course, brains, and of course, you can see I put a single sphere here representing

53:21.620 --> 53:26.900
all solid neural networks, right, this is an oversimplification.

53:26.900 --> 53:33.620
The immune system, which is a system that can learn where cells can have memories, where

53:33.620 --> 53:39.980
collectively you have phenomena that remind us a brain, right, but they are liquid, they

53:39.980 --> 53:40.980
move around, right.

53:40.980 --> 53:45.940
We are living in a very interesting time now where it seems that because our understanding

53:45.940 --> 53:50.580
of these networks, we might actually fight cancer and other things in a very effective

53:50.580 --> 53:51.580
way.

53:51.580 --> 53:57.020
And of course, I put hands in the middle between liquid and solid, high development because

53:57.020 --> 54:03.020
an uncolline actually, if you look at how it generates, it comes from the queen, the

54:03.020 --> 54:11.020
first X, the first individuals, the structure that goes emerges in a totally predictable

54:11.020 --> 54:16.420
way and ends up into something that is the structure plus the colony that is inside kind

54:16.420 --> 54:21.020
of a liquid system, right.

54:21.020 --> 54:28.780
The microbiome, the millions and millions of bacteria that we carry out that makes us

54:28.780 --> 54:34.580
something that is not anymore a single species, the idea that we are one species, we have

54:34.580 --> 54:38.540
to abandon because we really are much more than that.

54:38.540 --> 54:43.420
The microbiome, we'll talk about that also in the second lecture, has been co-evolving

54:43.420 --> 54:45.460
with us.

54:45.460 --> 54:50.940
The more we know, the more clear it is that many diseases we couldn't understand but it

54:50.940 --> 54:56.060
was how they work are connected with the microbiome and the great thing is that we can intervene,

54:56.060 --> 55:01.260
change the microbiome and maybe fight those diseases, right.

55:01.260 --> 55:06.740
And this microbiome interacts with the immune system and with the brain, right, which means

55:06.740 --> 55:10.820
that we have a lot of things at play.

55:10.820 --> 55:16.020
Fissarum, somewhere also in the middle between liquid and solid, okay.

55:16.020 --> 55:21.420
And this is kind of the big picture and they want to bring to your attention something

55:21.420 --> 55:27.540
that is quite visible, is this big sphere that occupies a lot of space.

55:27.540 --> 55:34.460
This big sphere is what we say is a void in the morpho space, meaning that when you look

55:34.460 --> 55:39.540
at the natural world, you don't see anything there.

55:39.540 --> 55:40.940
Why is that?

55:40.940 --> 55:44.300
Because it's forbidden.

55:44.300 --> 55:51.060
Because evolution for some reason is unable to get there, that's a big question.

55:51.060 --> 55:58.100
For many of us, it is an indication that is a lot, is plenty of space to explore and

55:58.100 --> 56:03.260
that if you are able to engineer things, we might go right there, right.

56:03.260 --> 56:05.500
And I'll show you examples soon.

56:05.500 --> 56:07.340
Okay.

56:07.340 --> 56:11.500
So now I just give you all these examples but I'm sure that some of you are thinking,

56:11.500 --> 56:16.260
yeah, yeah, but there's one particular example that you are not talking about, right.

56:16.260 --> 56:21.900
And that is quite bizarre, right.

56:21.900 --> 56:25.740
Of course, yeah, octopus.

56:25.740 --> 56:31.660
Octopus has been receiving a lot of attention over the last two decades.

56:31.660 --> 56:32.660
Why?

56:32.660 --> 56:38.740
Well, it's on the one hand is an extraordinary example of how evolution creates in a totally

56:38.740 --> 56:49.140
different trajectory in the tree of life, a mind that is remarkable, right.

56:49.140 --> 56:53.820
Octopi, they can learn, they have memory.

56:53.820 --> 56:57.900
You can see that they repeat the same words, right.

56:57.900 --> 57:00.940
But they also have clearly curiosity.

57:00.940 --> 57:06.460
There's been described many times that the octopus can be really interested and engaged

57:06.580 --> 57:15.580
into approaching humans, for example, and trying to, I don't know, figure out what we're doing.

57:15.580 --> 57:21.780
In the lab, it's been discovered that they recognize particular people, or sometimes

57:21.780 --> 57:26.980
that they can escape from the fish tank by night because there's a camera recording.

57:26.980 --> 57:31.140
Go to another fish tank to visit someone, I guess.

57:31.140 --> 57:33.820
And get back.

57:33.820 --> 57:34.820
Why they get back?

57:35.580 --> 57:39.740
There's plenty of things that we don't understand.

57:39.740 --> 57:44.700
And not surprisingly, it has been used often in the context of science fiction.

57:44.700 --> 57:54.740
This comes from a snapshot from the movie The Arrival, where this is the question of

57:54.740 --> 57:56.740
the language we'll discuss tomorrow.

57:56.740 --> 57:59.740
Language is the big thing, right.

57:59.740 --> 58:04.800
And you could say this is totally bizarre, totally different, totally alien.

58:05.720 --> 58:08.720
But, well, not that much.

58:08.720 --> 58:13.720
The interesting thing I want to bring is, if you analyze the brain of an octopus, right,

58:13.720 --> 58:18.280
you can have the microscope, you make slices, see what's the architecture.

58:18.280 --> 58:21.320
If you ask a histologist, what do you see here, right?

58:21.320 --> 58:25.080
You might not have seen any time an octopus' brain.

58:25.080 --> 58:33.240
But he will say, okay, this is a brain, probably some vertebrate, because I see the neurons.

58:33.240 --> 58:37.920
The neurons are these amazing structures that have the polarity, you have this special

58:37.920 --> 58:38.920
structure.

58:38.920 --> 58:41.560
When you look at the shape of a neuron, what do you see?

58:41.560 --> 58:44.760
You see a cell that is trying to connect, right?

58:44.760 --> 58:46.680
This is a big function.

58:46.680 --> 58:48.920
You see multilayers.

58:48.920 --> 58:55.480
So interestingly, in a different universe of possibilities, in invertebrates, a totally

58:55.480 --> 59:02.680
different branch, you generate an animal that has the same kind of eyes we have, that has

59:03.640 --> 59:09.280
a neural architecture that resembles things that we are very familiar with, okay?

59:09.280 --> 59:15.120
Of course, they are peculiarities, they have the central brain and something that acts

59:15.120 --> 59:21.280
like eight autonomous brains, one for each tentacle, right?

59:21.280 --> 59:28.800
But what this brings is that, again, it looks like maybe the space of possibilities is not

59:28.800 --> 59:29.800
that big.

59:29.840 --> 59:33.200
Even in that case, where you have this amazing animal, right?

59:33.200 --> 59:37.560
So why octopus are not more complex, right?

59:37.560 --> 59:38.560
They are interesting.

59:38.560 --> 59:39.960
They are clearly interesting.

59:39.960 --> 59:47.720
But for example, why cephalopod in general have not gone into using tools, for example.

59:47.720 --> 59:53.680
And it comes this interesting constraint that has to do with the life of these animals.

59:53.680 --> 59:57.040
Unfortunately, octopi don't live much.

59:57.040 --> 01:00:03.680
In some species, one year, in some two years, maybe three, but that's all.

01:00:03.680 --> 01:00:12.200
So thinking in the bioengineering, the possibilities in the future, you cannot avoid to think.

01:00:12.200 --> 01:00:17.040
What if we were able to make an octopus to live more, right?

01:00:17.040 --> 01:00:23.120
An animal that clearly learns over time has a brain that has this potential, right?

01:00:23.360 --> 01:00:31.240
And that brings me into, and I am trying to attract you to the second lecture, okay?

01:00:31.240 --> 01:00:39.160
Of course, to answer the questions I'm making about how brains originated, why brains will

01:00:39.160 --> 01:00:43.600
require to have a picture of evolution, what happened, really.

01:00:43.600 --> 01:00:45.760
We don't have a time machine.

01:00:45.760 --> 01:00:52.800
But we have an alternative, which is extremely interesting, and I will show you, provides

01:00:52.800 --> 01:00:59.200
very new fundamental questions that I think in the future might solve the questions that

01:00:59.200 --> 01:01:02.280
we have been making before.

01:01:02.280 --> 01:01:03.280
And that will be all.

01:01:03.280 --> 01:01:04.280
Thank you.

01:01:04.280 --> 01:01:17.480
Do we have time for questions?

01:01:17.480 --> 01:01:29.880
Maybe I can try, oh, there we go.

01:01:29.880 --> 01:01:32.080
So we have time for some questions.

01:01:32.080 --> 01:01:52.360
I would like to know how that plant or whatever it was, you're not to laugh at me, got in

01:01:52.360 --> 01:01:54.240
and out of that maze.

01:01:54.240 --> 01:01:59.600
How did something go into the maze and find its way out?

01:01:59.600 --> 01:02:02.600
You mean for the organism I was mentioning?

01:02:02.600 --> 01:02:08.240
Yeah, there was Fissarum, it was this single cell organism.

01:02:08.240 --> 01:02:15.080
And in nature, you have, for example, imagine you put Fissarum in some place, you have different

01:02:15.080 --> 01:02:17.400
sources of food, right?

01:02:17.400 --> 01:02:23.720
Fissarum is all the time expanding, like I'm searching around space.

01:02:23.720 --> 01:02:29.000
And then this information about the different resources comes to the collective, it's kind

01:02:29.000 --> 01:02:34.720
of integrating information all the time and making decisions about which one is richer

01:02:34.720 --> 01:02:37.400
and minimizing the trajectories.

01:02:37.400 --> 01:02:43.520
You don't want to make a lot of channels and invest energy and resources to create tubes.

01:02:43.520 --> 01:02:46.400
You want to, well, this is necessary.

01:02:46.400 --> 01:02:51.680
So what happened in the maze is that you put these two resources in those particular places

01:02:51.680 --> 01:02:54.560
that have sense for us.

01:02:54.560 --> 01:02:56.560
We know this is the entrance, this is the exit.

01:02:56.560 --> 01:02:58.520
Fissarum doesn't know anything.

01:02:58.520 --> 01:03:03.880
But then the amplification that is made here creates these tubes that are very strong here,

01:03:03.880 --> 01:03:04.880
right?

01:03:04.880 --> 01:03:06.480
And the same principle applies.

01:03:06.480 --> 01:03:12.280
In the end, you try to exploit which is really rich, right?

01:03:12.280 --> 01:03:18.480
And connect these sources in the shortest path you can make.

01:03:18.480 --> 01:03:20.520
And that solves the maze.

01:03:20.520 --> 01:03:25.640
But again, have in mind that we humans put the maze there, right?

01:03:25.640 --> 01:03:45.720
To prepare the problem, it's an important difference.

01:03:45.720 --> 01:03:47.200
Thanks for a brilliant lecture.

01:03:47.200 --> 01:03:49.760
Your drawings are like Leonardo da Vinci, really.

01:03:49.760 --> 01:03:50.760
Thank you.

01:03:50.760 --> 01:03:51.760
Fantastic.

01:03:52.160 --> 01:03:56.920
This is a question that's very stupid, but I've been asking it since I was six years old,

01:03:56.920 --> 01:04:01.800
which is today, of course, it seems completely obvious to all of us that we think with our

01:04:01.800 --> 01:04:03.200
brains.

01:04:03.200 --> 01:04:09.000
But I wonder whether early man knew that they think with their brains.

01:04:09.000 --> 01:04:14.520
Because if you look at all the cave paintings we have, you have the hands, you have animals,

01:04:14.520 --> 01:04:15.520
you have human figures.

01:04:15.960 --> 01:04:23.480
But you don't have heads necessarily as being more important than any other body feature.

01:04:23.480 --> 01:04:28.760
So I just wonder if there's any history in the scientific research of when we began to

01:04:28.760 --> 01:04:33.200
realize that we think with our brain.

01:04:33.200 --> 01:04:37.520
You mean as rational creatures that we knew that there was a brain?

01:04:37.520 --> 01:04:41.440
Or when the brain become became relevant?

01:04:41.440 --> 01:04:44.000
I think back to earliest human beings.

01:04:44.000 --> 01:04:49.600
The earliest development of communication, language, hunting, coordination, whatever

01:04:49.600 --> 01:04:55.080
it was, did they realize that this was what was working?

01:04:55.080 --> 01:04:56.080
Okay.

01:04:56.080 --> 01:04:57.080
Okay.

01:04:57.080 --> 01:04:58.080
It's not a stupid question.

01:04:58.080 --> 01:05:01.360
It always happens in all the talks when somebody brings it.

01:05:01.360 --> 01:05:06.160
This is going to be a stupid question and it's not at all.

01:05:06.160 --> 01:05:11.520
In fact, tomorrow I'm going to bring a little bit about what part of the singularity of

01:05:11.520 --> 01:05:16.320
the human brain is there.

01:05:16.320 --> 01:05:20.600
I will make a bit of spoilers here.

01:05:20.600 --> 01:05:26.160
The human brain is interesting for a number of reasons and had a very important impact

01:05:26.160 --> 01:05:28.280
in an evolutionary history.

01:05:28.280 --> 01:05:30.760
Just to mention a few things.

01:05:30.760 --> 01:05:35.400
One is language, of course.

01:05:35.400 --> 01:05:37.560
Language is a pretty extraordinary piece.

01:05:37.640 --> 01:05:46.160
These days of chat GPT, chat GPT for a number of reasons is not intelligent but brings some

01:05:46.160 --> 01:05:53.840
interesting ideas about the importance of language in evolving reasoning.

01:05:53.840 --> 01:05:59.000
One thing that I find extraordinary is very, very important and we also bring that tomorrow.

01:05:59.000 --> 01:06:00.500
It's time.

01:06:00.500 --> 01:06:06.280
Somebody said that we are mental time travelers.

01:06:07.280 --> 01:06:09.400
You remember what we mentioned before.

01:06:09.400 --> 01:06:11.720
We are prediction machines.

01:06:11.720 --> 01:06:15.360
The brain is something that tries to predict what's going next.

01:06:15.360 --> 01:06:21.200
This is very, very important because in the end what makes brains worth is that we are

01:06:21.200 --> 01:06:24.880
able to reduce uncertainty.

01:06:24.880 --> 01:06:32.920
We make us more prepared to actually be understanding what's next, what's going to happen.

01:06:33.040 --> 01:06:37.640
We had this cortex that expanded so much.

01:06:37.640 --> 01:06:44.120
This kind of understanding of time became something that was the narrative.

01:06:44.120 --> 01:06:49.880
We became able to make narratives, not only one feature.

01:06:49.880 --> 01:06:55.320
We can imagine many possible features.

01:06:55.320 --> 01:06:58.200
It's very interesting to see how it happens in evolution.

01:06:58.200 --> 01:07:03.480
Somebody said you all know that memory is faulty.

01:07:03.480 --> 01:07:05.520
Memory sometimes fails.

01:07:05.520 --> 01:07:12.720
Sometimes you have memories that are not real, memories that are being constructed.

01:07:12.720 --> 01:07:15.160
Somebody can say, why is that?

01:07:15.160 --> 01:07:18.520
Because natural selection doesn't care about that.

01:07:18.520 --> 01:07:21.120
Natural selection wants you to predict the future.

01:07:21.120 --> 01:07:23.360
The future is important.

01:07:23.360 --> 01:07:27.920
If you remember well or not the past is not so important.

01:07:27.920 --> 01:07:30.520
An important thing that connects maybe more with your question.

01:07:30.520 --> 01:07:38.440
One of the things that made us successful, as ants are successful because they cooperate,

01:07:38.440 --> 01:07:42.080
some days it doesn't seem so, but we are cooperators.

01:07:42.080 --> 01:07:45.520
Being a cooperative species made a big difference.

01:07:45.520 --> 01:07:51.960
One of the drivers of that was our amazing capacity of understanding the mind of the

01:07:51.960 --> 01:07:53.960
other.

01:07:53.960 --> 01:08:02.720
Understanding that somebody that is looking at me is suffering or is scared or is something

01:08:02.720 --> 01:08:04.400
that can create trouble.

01:08:04.400 --> 01:08:07.800
I can put myself in the mind of the other.

01:08:07.800 --> 01:08:15.080
When you combine all this stuff, we have a singularity plus a lot of other things like

01:08:15.080 --> 01:08:16.680
mental diseases.

01:08:16.680 --> 01:08:18.880
But it's a whole story.

01:08:18.880 --> 01:08:22.800
I don't want to spoil because as I was saying, one of the things I want to bring tomorrow

01:08:22.800 --> 01:08:30.320
is when you try to approach the complexities of the human mind, there's a kind of a synthetic

01:08:30.320 --> 01:08:46.120
or artificial path that can bring a lot of understanding.

01:08:46.120 --> 01:08:51.600
At one point, you were talking about ants, I think, and their colonies and that the

01:08:51.600 --> 01:09:00.080
more complex the society, the less complex the individuals needed to be.

01:09:00.080 --> 01:09:04.200
But we shouldn't apply that to humans.

01:09:04.200 --> 01:09:07.200
But you said we shouldn't apply that to humans.

01:09:07.200 --> 01:09:09.360
I think, why?

01:09:09.360 --> 01:09:18.320
Well, I mean, not every day I feel like the human race deserves good words.

01:09:18.320 --> 01:09:28.360
But individual humans are spectacularly complex, provided that you're being immersed in the

01:09:28.360 --> 01:09:29.360
cultural thing.

01:09:29.360 --> 01:09:34.240
Our brains are nothing unless you are in a society that goes from language to almost

01:09:34.240 --> 01:09:37.520
everything else.

01:09:37.520 --> 01:09:44.720
But I think that on the one hand, because of the evolutionary pressures that apply for

01:09:44.720 --> 01:09:54.360
ant colonies, that had to do with the warranty of the colony works, and eventually that reproduces.

01:09:54.360 --> 01:09:57.920
It's a different story for us in many ways.

01:09:57.920 --> 01:10:03.160
I also try to bring that tomorrow and make a good comparison.

01:10:03.160 --> 01:10:09.040
I must say that this trend we observed, but we still don't have a good theory for the

01:10:09.040 --> 01:10:11.480
complexity drain.

01:10:11.480 --> 01:10:17.960
But definitely, I remember that Michael Lachman, which was a faculty also at Santa Fe Institute

01:10:17.960 --> 01:10:23.560
one day, we were discussing about this, the brain of humans, and he brought me to something

01:10:23.560 --> 01:10:32.120
that is, in a way, is trivial, but it's interesting, is that you isolate a human from society and

01:10:32.240 --> 01:10:37.960
the brain, this extraordinary potential machine is worthless.

01:10:37.960 --> 01:10:44.880
You might survive, maybe, as some kids survive with wolves, but all the potential of the

01:10:44.880 --> 01:10:47.840
brain is never going to develop.

01:10:47.840 --> 01:10:53.720
So it again says something about the fact that we are also cultural animals, and that

01:10:53.760 --> 01:10:55.760
makes a difference.

01:10:55.760 --> 01:11:02.760
I understand the utilitarian and the intellectual reasons, like that you do experiments with

01:11:25.760 --> 01:11:33.760
animals on octopus, octopi, but aren't there ethical considerations on how we do all these

01:11:33.760 --> 01:11:37.520
studies on sentient beings that aren't us?

01:11:37.520 --> 01:11:44.640
Yeah, that's a very good point, and I'm sure you know that there's a hot topic these days.

01:11:44.640 --> 01:11:50.760
The more we know about some species, the more clearly we need to have ethic criteria of

01:11:50.760 --> 01:11:56.760
what is reasonable and what is not for the next experiments.

01:11:56.760 --> 01:12:02.760
Even if we will say that, because we don't know, to what extent we can talk about that

01:12:02.760 --> 01:12:05.760
sentience and consciousness, etc., we still don't know.

01:12:05.760 --> 01:12:12.760
But the precursors of a complex mind, clearly we are seeing it in many species, right?

01:12:12.760 --> 01:12:18.760
The elephants clearly mourn, they kind of feel the loss of others.

01:12:18.760 --> 01:12:23.760
In the octopi, we see that kind of extraordinary curiosity.

01:12:23.760 --> 01:12:25.760
What brings that there?

01:12:25.760 --> 01:12:27.760
So this is on the table.

01:12:27.760 --> 01:12:33.760
It's a hot discussion, because one thing, as you can imagine, is that to make you want

01:12:33.760 --> 01:12:39.760
decisions about what is ethically reasonable or not, we first need to actually have good

01:12:39.760 --> 01:12:44.760
definitions of whether or not you have sentience.

01:12:44.760 --> 01:12:47.760
Can these be measured in some way?

01:12:47.760 --> 01:13:02.760
So it's a good point and it's a relevant research problem now.

01:13:02.760 --> 01:13:04.760
I have a question too.

01:13:04.760 --> 01:13:07.760
Are you wondering about instinct?

01:13:07.760 --> 01:13:14.760
Is that also part of the brain or is the instinct separate from the brain?

01:13:14.760 --> 01:13:19.760
Which you have kind of not mentioned in your lecture so far.

01:13:19.760 --> 01:13:20.760
I don't know if you understand.

01:13:20.760 --> 01:13:22.760
You say, insect brains?

01:13:22.760 --> 01:13:27.760
The instinct, the instinct that living organisms have.

01:13:27.760 --> 01:13:35.760
I mean, is that part of the brain or is that something else?

01:13:35.760 --> 01:13:38.760
You mean for, you say, insect?

01:13:38.760 --> 01:13:39.760
Instinct.

01:13:39.760 --> 01:13:40.760
Oh, instinct.

01:13:40.760 --> 01:13:41.760
Sorry, sorry, sorry.

01:13:41.760 --> 01:13:48.760
But you have to, I mean, I came here, you don't know that, but I came straight from

01:13:48.760 --> 01:13:53.760
Barcelona, 26 hours, and I was brought here.

01:13:53.760 --> 01:13:57.760
My brain is not as good as it should.

01:13:57.760 --> 01:13:59.760
Yeah, actually, that's a good point.

01:13:59.760 --> 01:14:05.760
Instinct clearly is something that is part of the machinery, right?

01:14:05.760 --> 01:14:12.760
In many cases, you see that working in a very almost algorithmic way, right?

01:14:12.760 --> 01:14:25.760
This is a famous example of these wasps that kill, no, sorry, that use their stings to

01:14:25.760 --> 01:14:30.760
actually put the eggs inside prey that are paralyzed, right?

01:14:30.760 --> 01:14:33.760
Could be spiders, could be something else.

01:14:33.760 --> 01:14:42.760
And they first make a hole, and then they hunt, they take, I don't know, could be larvae

01:14:42.760 --> 01:14:44.760
or whatever it is.

01:14:44.760 --> 01:14:49.760
They go there, they leave the prey there, which is paralyzed.

01:14:49.760 --> 01:14:52.760
They go inside, check out that everything is okay.

01:14:52.760 --> 01:14:56.760
Go out, take the larvae, put it inside.

01:14:56.760 --> 01:15:02.760
But then what happens is, if you, while this is happening, imagine that you are a mean entomologist.

01:15:03.760 --> 01:15:11.760
And you are looking, and then when it goes inside, you move the prey, which is paralyzed, right?

01:15:11.760 --> 01:15:12.760
Somewhere else.

01:15:12.760 --> 01:15:17.760
So you see the wasps coming out, finding out that something has changed.

01:15:17.760 --> 01:15:19.760
They kind of managed to touch a little bit.

01:15:19.760 --> 01:15:22.760
They go inside again.

01:15:22.760 --> 01:15:26.760
And if you can repeat that operation as much as you want.

01:15:26.760 --> 01:15:30.760
It's like an algorithm which clearly goes from instinct, right?

01:15:30.760 --> 01:15:36.760
But on the other hand, in insects, we also been finding in the last 20 years many unexpected things.

01:15:36.760 --> 01:15:40.760
Like some wasps recognize the face of others.

01:15:40.760 --> 01:15:44.760
There are asymmetries, apparently, in the brains of some ants.

01:15:44.760 --> 01:15:48.760
So it's plenty of still of things we need to know.

01:15:51.760 --> 01:15:56.760
So it was a question here, and another there.

01:15:57.760 --> 01:16:02.760
You defined, you defined some really interesting questions that are very simple.

01:16:02.760 --> 01:16:05.760
Like why brains, and what kinds of brains.

01:16:05.760 --> 01:16:12.760
But I didn't hear you say what constitutes being a brain.

01:16:12.760 --> 01:16:15.760
Like what's your definition of a brain?

01:16:15.760 --> 01:16:17.760
So how do you define a brain?

01:16:17.760 --> 01:16:18.760
Okay.

01:16:18.760 --> 01:16:23.760
I try to avoid that question, right?

01:16:24.760 --> 01:16:38.760
I mean, in a simple way, a brain is a collective of neurons that centralizes the activity of an organism, right?

01:16:38.760 --> 01:16:43.760
So for a hydra, for example, you have a net of neurons,

01:16:43.760 --> 01:16:47.760
or for a jellyfish, you have a ring and a distributed thing.

01:16:47.760 --> 01:16:55.760
But you don't have really a core of neurons that are playing a role of kind of integrating information, right?

01:16:55.760 --> 01:17:03.760
So that would be a kind of, I think, a potential definition, but it's disputed, right?

01:17:03.760 --> 01:17:06.760
I think for me, it's satisfactory.

01:17:07.760 --> 01:17:24.760
When you were showing the example of the ants who managed to figure out the shortest route to the food,

01:17:24.760 --> 01:17:33.760
or the termites who end up creating something that is a benefit to the society,

01:17:33.760 --> 01:17:41.760
or the single-celled organisms that find the shortest route to the flakes in the maze,

01:17:41.760 --> 01:17:49.760
it seems like more often than not, in these experiments, you have similar sort of outcomes

01:17:49.760 --> 01:17:55.760
that the ants are successful in doing this, the termites are successful in doing this,

01:17:55.760 --> 01:18:00.760
and the single-celled organisms are successful in doing this.

01:18:00.760 --> 01:18:08.760
But when you take humans acting like a liquid brain,

01:18:08.760 --> 01:18:17.760
they often come out to solutions that are harmful to the organism as a whole,

01:18:17.760 --> 01:18:29.760
especially in the area of finance, where if you have a lot of individuals here coming to conclusions,

01:18:29.760 --> 01:18:37.760
they're more likely to come to this wrong conclusion, which ends up making a few people very rich

01:18:37.760 --> 01:18:45.760
and other people very poor, and I'm puzzled by this.

01:18:45.760 --> 01:18:48.760
Okay.

01:18:48.760 --> 01:18:58.760
Well, it's not an easy question, so it requires a long answer, so close the doors.

01:18:58.760 --> 01:19:01.760
No, I am now seriously.

01:19:01.760 --> 01:19:04.760
Yeah, when we go into humans, it's interesting to see two things.

01:19:04.760 --> 01:19:15.760
One is what you mentioned, because humans, as you move beyond pure commission and you have society,

01:19:15.760 --> 01:19:21.760
with all the biases that some can deal with, incomplete information,

01:19:21.760 --> 01:19:24.760
plus the big problems we have about polarization and everything,

01:19:24.760 --> 01:19:29.760
which is something that in complex systems we want to solve, but it looks like very, very difficult.

01:19:29.760 --> 01:19:37.760
But on the other hand, it's just a small part of research, but it's interesting for the insight it brings.

01:19:37.760 --> 01:19:47.760
When humans are under the situation of panic, you can make very well-defined models,

01:19:47.760 --> 01:19:54.760
which essentially is like particles moving by forces and amplifying phenomena.

01:19:54.760 --> 01:19:56.760
Things that we see and have consequences.

01:19:56.760 --> 01:20:05.760
For example, in a stadium, you have two exits, and this has happened unfortunately a number of times.

01:20:05.760 --> 01:20:07.760
It's panic.

01:20:07.760 --> 01:20:08.760
It's a panic attack.

01:20:08.760 --> 01:20:10.760
We don't reason there.

01:20:10.760 --> 01:20:12.760
We just want to survive.

01:20:12.760 --> 01:20:21.760
A typical trend is that the more people go into one exit, the more people go there, which of course is harmful,

01:20:21.760 --> 01:20:23.760
because there's another exit.

01:20:23.760 --> 01:20:31.760
You can model that, and it's interesting to see that you do the same experiment of panic with ants and with humans,

01:20:31.760 --> 01:20:35.760
and that's one place where you can compare.

01:20:35.760 --> 01:20:39.760
Very unfortunately, we behave like ants.

01:20:39.760 --> 01:20:43.760
If we move beyond that, and it's a legitimate question,

01:20:43.760 --> 01:20:51.760
under what conditions we can actually be described as systems like collective intelligence in ants,

01:20:51.760 --> 01:20:59.760
and what is the threshold that separates from that and gets into society and all the conflicts that you're mentioning?

01:20:59.760 --> 01:21:02.760
But this is a whole area of research.

01:21:02.760 --> 01:21:04.760
So, good point.

01:21:04.760 --> 01:21:13.760
One thing that we need to actually figure out is to what extent these ideas of collective intelligence apply to humans.

01:21:13.760 --> 01:21:23.760
And if theory develops, whether we could actually explore the insight of the theory to actually help us to exploit common knowledge,

01:21:23.760 --> 01:21:25.760
that clearly now is underexploited.

01:21:25.760 --> 01:21:29.760
So maybe a third lecture on third day?

01:21:29.760 --> 01:21:33.760
Thirdly, you have to be in Barcelona.

01:21:33.760 --> 01:21:49.760
Okay, we have time for one more question.

01:21:49.760 --> 01:21:56.760
Hi, I was interested in your chart where you have your three dimensions,

01:21:56.760 --> 01:22:00.760
and you have this big sphere of the unknown or the unexplored,

01:22:00.760 --> 01:22:04.760
and I wanted to know, it was like, yes, that's the one.

01:22:04.760 --> 01:22:13.760
So, what we're missing is cognitively complex and developmentally complex as well, or maybe that doesn't matter.

01:22:13.760 --> 01:22:17.760
But I'm wondering what you think that could look like.

01:22:17.760 --> 01:22:22.760
You said that we might be able to manufacture it, something in that space.

01:22:22.760 --> 01:22:24.760
I said what, sorry.

01:22:24.760 --> 01:22:32.760
I thought I caught you saying, sneakily, maybe that in this unexplored space,

01:22:32.760 --> 01:22:39.760
maybe that's where we have space to manufacture some sort of brain-like thing.

01:22:39.760 --> 01:22:43.760
I'm just wondering what you think is in that space that we're missing.

01:22:43.760 --> 01:22:50.760
Okay, if you come to the second lecture, I think I have the answer for that.

01:22:50.760 --> 01:22:54.760
I don't want to make more spoilers.

01:22:54.760 --> 01:22:56.760
Alright, thank you so much.

