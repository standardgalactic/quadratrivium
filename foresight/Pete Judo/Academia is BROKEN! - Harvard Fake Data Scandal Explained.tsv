start	end	text
0	6080	Academia is broken. Universities are broken. The way that academic research is published
6080	10720	is broken. That's the message that's come through loud and clear over the last few weeks,
10720	15440	thanks to three articles concerning the research of Francesca Geno. If you don't know what I'm
15440	20320	talking about, let me explain. Francesca Geno is a professor of behavioral science at Harvard
20320	25120	University. She is extremely well known in the field. I've talked about her research to clients
25120	29920	before. I've recommended books on this channel to you guys that use her work as a key reference,
30000	34560	I've used her research before as references in my own essays and work that I did at university
34560	39520	when it comes to academic fame, Francesca Geno is up there, as you would expect from someone
39520	44080	who is a professor at Harvard. However, the reason why she's so well known is because her
44080	48880	research tends to bring out a lot of very surprising findings. Now, some people just think this
48880	52960	research is cool and don't think much more about it, but a lot of people in the industry have been
52960	58080	quite skeptical of Francesca Geno and her work because her results just seem a little bit too
58080	62320	good. Her hypotheses are really wacky, but yet they always seem to be proved correct,
62320	67680	the effect sizes from her study seem to be really large, and her statistical significance just seem
67680	71600	a little bit too significant. So while some of us have been skeptical of her work for a while,
71600	76000	nobody has taken the time to actually investigate her research and go into her data to see if they
76000	82000	can find anything fishy... until now. These three guys, Yuri, Joe and Leif, are also professors
82000	86560	of behavioral science and other related subjects from different universities across the world,
86560	91120	and they took it upon themselves to investigate Francesca Geno and her data to see if there was
91120	95760	anything fishy going on. And spoiler alert, they found a lot of fishy stuff in the data,
95760	99600	and that's what the three articles that they released are talking about. Each article relates
99600	103440	to a different study by Francesca Geno, and in this video I'm going to be taking you through
103440	108240	each one. The results of their investigation are shocking, damning for Francesca Geno,
108240	112080	but I think they speak even louder volumes about the state of academia in general,
112080	115920	and that's what I'm going to be concluding on at the end of this video. So without further ado,
116000	119920	let's jump into the first study. So this first article is called clusterfake, and it's referring
119920	125520	to a paper written by Geno in 2012, along with their collaborators Shu, Nina Mazar, Dan Ariely,
125520	129520	and Max Bazeman. Given the fact that I know the first names of all of those researchers with the
129520	133760	exception of Shu, I should tell you that all of these researchers are very well-known people in
133760	138400	the field of behavioral science. So in this study, they were trying to get participants to be more
138400	142960	honest, and their hypothesis was that if you put an honesty pledge at the top of a form,
142960	147280	that'll make people more honest when they then fill out the rest of the form. So all of the studies
147280	152000	in this paper by these authors were looking at this idea, that if you put an honesty pledge at the
152000	156320	top of a form, people will be more honest than if you put the honesty pledge at the bottom of a form.
156320	161200	Now the first study in this paper was led by Francesca Geno, our protagonist. So in this study,
161200	166000	students were brought into a lab to complete 20 math puzzles in five minutes. The students were
166000	170560	told that they would be paid one dollar for each math puzzle they solved correctly, and the way that
170560	174800	this worked was that when students walked into the room, there were two pieces of paper. They had
174800	179120	their work paper and their report paper. So on the work paper, they'd write down their workings for
179120	183040	the math questions and of course their answers, and then on the report paper, they would then have
183040	187600	to report how many answers they got correctly and therefore how much they should get paid. The students
187600	191520	were then told that before handing in their report paper to the researchers and getting paid, that
191520	196480	they should shred their original work paper. The idea behind this is that by shredding their work
196480	200960	paper, there's then a stronger incentive for them to cheat on the report paper and lie about
200960	205040	how many answers they got correct, since the researchers in theory should never know how
205040	209040	many answers they got right on the work paper. But what the students didn't know was that the
209040	213440	shredder at the back of the room was not a normal shredder. What the people in the experiment don't
213440	219120	know is that the shredder has been fixed. So the shredder only shreds the sides of the page,
219120	225120	but the main body of the page remains intact. Now in order to test the hypothesis of the researchers,
225120	229600	on the reporting paper, the participants were split into two groups. Half of them had an honesty
229600	233840	pledge at the top of the paper, and half of them had an honesty pledge at the bottom of the paper,
233840	237440	with the idea being of course that those who signed the honesty pledge at the top would
237440	243440	then cheat less going forward. So what was the result? Well the result showed a massive effect
243440	247200	from this simple intervention. According to what was published in the study originally,
247200	252000	for the students who signed the honesty pledge at the top of the form, only 37% of them lied,
252000	257680	but when students signed at the bottom of the form, 79% of students lied. This is a massive
257680	262480	effect size that the researchers are reporting, and as a result of that, this study gained a lot
262480	266960	of public attention, and I have talked about it with many people in the past before because it is
266960	272880	so surprising. But that's why these vigilantes were suspicious. The results just seem a bit too good.
272880	277040	Can it really be the case that simply moving an honesty pledge from the bottom to the top of a
277040	281920	form can have such a dramatic effect on the amount of cheating that happens? It seems pretty
281920	287600	unlikely. So our vigilantes managed to source the original data set that was published by the
287600	292560	authors of the study, and when they looked into the data it just seemed a little bit fishy. If you
292560	298000	look at this table and specifically look at the left hand column, the P hash column, this is referring
298000	303920	to participant ID. This is the unique ID given to each participant in a study, and as it's highlighted
303920	308080	in yellow, there are some weird anomalies in the way that this data has been sorted. Because
308080	312480	when you look at this data, it seems obvious that this has been sorted by first the condition, so
312480	316640	all of condition one are together, and all of condition two are together, and then in ascending
316640	321920	order of the participant ID, which means that the numbers should consistently get bigger as you go
321920	326560	down the line, and there should be no duplicates. Remember, each participant has a unique ID. So
326560	330720	when you look at this data, it's a bit weird. We've got two 49s here, that's a duplicate,
330720	334720	that should never happen, and then at the end of the condition one set of participants,
334720	340720	you have participant 51 coming after 95, then 12, then 101, like that sequence doesn't make any
340720	345840	sense. And similarly, when you get to condition two, we start with seven, then 91, then 52,
345840	350160	then all the way back down to five again. These entries in the data set look suspicious. They
350160	354640	look like they're out of sequence, which suggests that somebody maybe has tampered with them. So
354640	359840	our vigilantes are suspicious of these rows. So then you have to ask the question, why would the
359840	364560	researchers want to tamper with the data? Well, it's because they would want to show a bigger
364560	369760	effect than those actually seen in the real data. The more dramatic the effect of the intervention
369760	374240	is, the more surprising the result of the study is, and therefore the more likely it is to get
374240	378320	published in the top journal, the more likely it is that this will make a lot of press headlines,
378320	382160	that they will get lots of interviews and work off the back of it. And so there's a strong incentive
382160	387200	for the researchers to fudge the data a little bit, make the effect seem larger than it really is.
387200	392000	And so that's what our vigilantes were looking for. They wanted to see if these suspicious rows
392000	397200	in the data set showed a bigger effect than the normal data that wasn't suspicious. And sure
397200	402000	enough, that's exactly what they found. If you look at this graph, the red circles with the cross
402000	406880	show the suspicious data and the blue dots show the unsuspicious data. And as you can see, the
406880	411680	circles with the red crosses are the most extreme ones, meaning that these few data points are
411680	417120	inflating the effect size. Now the article goes on to show how our vigilantes did some very clever
417120	421600	work to unpack the Excel file that this data was stored in, and they were able to show quite clearly
421600	426800	that these suspicious rows were manually resorted in the data set. I won't go into it on this video
426800	430400	because it's quite technical, but I'll have a link to all of these articles in the description if
430400	435600	you want to read them in full. But as you'll soon see, this theme of suspicious data and then those
435600	441200	data showing extremely strong effect sizes will be a recurring pattern. So let's move on to study
441200	446080	two. Now this second article is called My Class Year is Harvard and you'll see why in a second.
446080	451520	They're looking at a study from 2015 written by Francesca Geno as well as Kuchaki and Galinsky,
451520	456320	again two fairly well known researchers in the field. Now the hypothesis for this study, in my
456320	461360	opinion, is pretty stupid. The hypothesis is that if you argue against something that you really
461360	467200	believe in, that makes you feel dirty, which then increases your desire for cleansing products,
467200	472080	which is kind of silly in my opinion. But nevertheless, this is what they were researching.
472080	477440	So this study was done at Harvard University with almost 500 students and what they asked the
477440	481520	participants to do was the following. So students of Harvard University were brought into the lab
481520	485360	and then asked how they felt about this thing called the Q Guide. I don't really know what the Q
485360	489200	Guide is, but apparently it's a hot topic at Harvard and it's very controversial. Some people are for
489200	492720	it, some people are against it. So when they were brought into the lab, they were asked how do you
492720	496720	feel about the Q Guide and they either said they were for or against it and then the participants
496720	501360	were split into two groups. Half the participants were asked to write an essay supporting the
501360	505600	view that they just gave. So if they said I'm for the Q Guide, they had to then write an essay
505600	509680	explaining why they were for the Q Guide. But then half the participants were asked to write an essay
509680	514080	arguing opposite to the point that they just gave. So if they said I'm for the Q Guide, they would
514080	518800	then have to write an essay explaining why they should be against the Q Guide. Again, the idea
518800	523120	being that those who are writing an essay against what they actually believe in would make them feel
523120	527680	dirty. Because after they'd written this essay, they were then shown five different cleansing
527680	532080	products. And the participants in the study had to rate how desirable they felt these cleansing
532080	537920	products were on a scale of one to seven, with one being completely undesirable and seven being
537920	543920	completely desirable. And again, the authors found a strong effect. You can see here that the p value
543920	549280	is less than 0.0001. And for those of you who haven't had any academic training in statistics,
549280	552720	basically when you're doing a study like this, you're looking for a p value that's less than
552720	558240	0.05. That's the industry standard. If it's less than 0.05, you say, yes, I'm confident that the
558240	564000	effect that I'm seeing is caused by the manipulation that I just did. So less than 0.0001 is an
564000	569760	extremely strong effect. You're basically 100% confident that what you're seeing in the data
569760	575040	is caused by the manipulation that you did. So once again, our vigilantes are suspicious of this
575040	579360	very strong effect sites. So they managed to source the data online and do a little bit of
579360	583920	investigating. And what they find are some weird anomalies in the kind of demographic data that
583920	587520	the participants have to give when they enter the study. And this is very common in psychological
587520	591440	studies that participants have to give a little bit of demographic data about themselves, which
591440	595440	gives the researchers a little bit more flexibility about how they cut up the data later on. So in
595440	599280	this particular study, the participants were asked a number of demographic questions, including their
599280	603840	age, their gender, and then number six was what year in school they were. Now the way this question
603840	608000	is structured isn't very good, in my opinion, in terms of research design. But nevertheless,
608000	612160	there are a number of acceptable answers that you can give to year in school. Because Harvard is an
612160	616160	American school, you might say, I'm a senior, right, which is a common thing, or a sophomore,
616160	621760	you might write the year that you're supposed to graduate, 2015, 2016, etc. Or you might indicate a
621760	626080	one, a two, a three, a four, or a five to indicate how many years of school that you've been in there.
626080	629760	These are all different answers, but they're all acceptable and make sense in the context of being
629760	634000	asked what year in school you. And so when our vigilantes go into the data, that's exactly what
634000	637920	they saw in this column, a range of different answers that were all acceptable, except for
637920	642880	one, there were 20 entries in this data set, where the answer to the question what year in school
642880	650080	are you was Harvard. That doesn't make any sense. What year in school are you? Harvard. What? Right,
650080	653680	that doesn't make any sense. And the other thing that was suspicious about these Harvard entries
653680	658800	is that they were all grouped together within 35 rows. Again, this was a data set of nearly 500
658800	664400	different participants, and yet all of these weird Harvard answers were within 35 rows. So once again,
664400	669840	our vigilantes treat these Harvard answers as suspicious data entries. They mark them in red
669840	675520	circles with crosses. And as you can see, the ones that are suspicious are again, the most
675520	681200	extreme answers supporting the hypothesis of the researchers, with the exception of this one. But
681200	685680	come on, it's most suspicious when you look at the ones on argued other side. So these are the
685680	690000	people who wrote an essay arguing against what they didn't believe in, and therefore was supposed
690080	694880	to feel more dirty and find cleansing products more appealing. All of these suspicious entries on
694880	699360	that side of the manipulation went for seven, that they found all of the cleaning products
699360	704240	completely desirable. And so what our vigilantes go on to say is that these were just the 20 entries
704240	708640	in the data set that looked suspicious because of this Harvard answer to the demographic question.
708640	713120	But who's to say that the other data in the data set was not also tampered with, but just they were
713120	717040	more careful when they filled in this column and didn't put Harvard. Since it seems pretty clear
717040	721360	that at least these 20 entries were manipulated and tampered with some way, it probably means that
721360	725280	there are other entries within this data set that were also tampered with. Are you shocked yet? I
725280	729280	hope you are, but it's about to get worse because there's a third article to do with Francesca
729280	733760	Geno. So this third article was released literally yesterday, the day before I'm filming this video,
733760	738720	and it's called the cheaters are out of order. This is written by Francesca Geno and a guy called
738720	743920	Wilta Muth. I don't know Wilta Muth, but again, I find it incredibly ironic that all of this
743920	747760	cheating and fake data is being conducted by researchers who are studying
747760	753680	the science of honesty. It is incredibly ironic. So in this third study, Geno and her co-author
753680	759360	are investigating the idea that people who cheat, people that lie, who are dishonest,
759360	765200	are actually more creative. And they call the paper evil genius, how dishonesty can lead to
765200	772480	greater creativity. So let's quickly go through how the study worked. Participants were brought
772480	777520	into a lab where they were sat at a machine with a virtual coin flipping mechanism. What the
777520	782960	participants are asked to do is to predict whether the coin will flip heads or tails, and then they
782960	787120	would push a button to actually flip the coin. And if they had predicted correctly about whether
787120	791600	it would go heads or tails, then they would get a dollar. So again, there's a strong incentive to
791600	794960	cheat. So the participants would write down on a piece of paper how many predictions they got
794960	798480	correct, and then they would hand that to the researcher in order to get paid. Then of course,
798480	802400	the researcher would then go back and look at the machine that they were flipping the coin on
802400	806640	to see how many they actually got correct. And then they were able to tell how many times that
806640	811280	participant had cheated. So after they completed the coin flipping task, they were then given a
811280	816720	creativity task. And the creativity task was how many different uses can you think of for a
816720	821040	piece of newspaper? So in psychology, this is a pretty common technique for testing creativity.
821040	825280	You give somebody an inanimate object, and then you say how many uses can you think of
825280	831040	for this inanimate object. And again, with this study, we see a very strong effect size. Remember,
831040	838400	the magic number that academics look for is P less than 0.05. And here we have P less than 0.001.
838400	842320	So basically what that means is that there's an extremely high likelihood that the effect that
842320	847440	the academics are seeing is caused by the manipulation that they did. So again, our vigilantes
847440	851840	are suspicious. But this one is interesting because our vigilantes were able to actually
851920	857280	get the data set from Gino several years ago. So they got this data set directly from Gino.
857280	862160	So again, when our vigilantes look into the data, they find some weird things going on.
862160	866720	As you can see, it seems to be sorted by two things. Firstly, by the number of times the
866720	871040	participant cheated. So all of the people who didn't cheat at all are zeros. And then the
871040	875120	number of responses is the number of different uses for a newspaper that that participant could
875120	879040	come up with. And those are clearly ranked in ascending order. But as you can see from this
879040	883600	next screenshot, some of the cheaters are out of order. So these are the people who cheated once,
883600	887600	who basically over reported one time, and the number of uses that they could come up with for
887600	893680	the newspaper are out of sequence. Here we have three, four, 13, then nine, and then back down to
893680	898480	five again, then back up to nine, then five, then nine, then eight, then nine is just a total mess,
898480	902240	right? So these ones that are highlighted in yellow are the suspicious ones. They're the ones
902240	906720	that are out of order, according to how the data appears to have been sorted. So what our
906720	911440	vigilantes did was they basically took this data set and then made a new column and they called
911440	916480	it imputed low or imputed high. What that basically means is that rather than taking the number of
916480	920240	responses that are written down in this original data set, they're going to say, well, where does
920240	925520	this entry sit in the ranking order? And so we're going to replace the value that is given here
925520	929440	with what the value should really be. So if it's between four and five, then that number should
929440	933760	be either four or five, whether it's imputed low or imputed high. Does that make sense? So once
933840	939040	again, our researchers plotted the data, suspicious entries are marked with a circle and a cross.
939600	944320	And as you can see, the suspicious entries are the ones that deviate from the pattern that you see
944320	948080	in the non cheaters, the blue line. So in other words, the ones that are out of order,
948080	953040	the suspicious entries, they're the ones showing the effect. But when you use the imputed position,
953040	958000	so that's the number that is implied by the row that the entry was in, then suddenly the entire
958000	962480	effect disappears. And the group of cheaters seem to show a very similar pattern to the group of
962480	967600	non cheaters. And the result of this statistically speaking is significant. Remember the original
967600	973680	p value for this study was p less than 0.0001. But once you use the data that's implied by the
973680	980480	row, suddenly the significance completely disappears. It then goes to p equals 0.292 or p equals 0.180,
980480	984640	depending on whether you're imputing low or high. Remember, in order for an academic study to be
984640	991120	significant, the standard is p less than 0.05. And here the p is clearly more than 0.05. Again,
991120	995120	this article goes on. The vigilantes do a little bit more research to really back up the point and
995120	1000160	really drive home the fact that this data is very suspicious. I won't go into the details now. Again,
1000160	1003600	all of these articles are linked in the description. Go check them out. And you'll notice that these
1003600	1007600	were all called part one, part two, part three. And that's because this is actually a four part
1007600	1012080	series. So I'm expecting a fourth article to come out after this video is published looking at yet
1012080	1016880	another study from Francesca Geno. But I hope by this point, you get the picture, there's a number
1016880	1021200	of studies conducted by Francesca Geno with very suspicious looking data. So at this point,
1021200	1026160	you're probably wondering how did Harvard allow this? And the short answer is, well,
1026160	1030160	they don't really seem to have done. If you go on Francesca Geno's page on the Harvard website,
1030160	1034880	it shows that she's on administrative leave. I think we all know what that means. And Harvard,
1034880	1039360	who have even more access to Francesca Geno's data than our vigilantes do, have since asked for
1039360	1043280	several of Francesca Geno's papers to be retracted from the journals that they were originally
1043360	1048080	published in. Now, this is a bad look for Francesca Geno, right? And we can't be sure that it was
1048080	1052160	Francesca Geno who was doing this manipulation. It could be one of her co-authors. But given that
1052160	1056080	she's the common thread between all of these different papers, it seems pretty likely that
1056080	1061520	it was her in the world of psychology and writing good quality academic papers. This is really bad.
1061520	1066400	It's not only bad for Francesca Geno, but it's bad for the field as a whole. It casts doubt over
1066400	1070800	the entire field of behavioral science, because we don't know the extent of the damage that bad
1070800	1075360	actors like Geno have been causing in the field and for how long. Like I said, Francesca Geno has
1075360	1079840	been a prominent name in the field for years, gaining a position at one of the top universities,
1079840	1084240	Harvard. So who's to say that this isn't a problem that is rife amongst many other researchers
1084240	1088800	in the field? We certainly hope not. But you can't really know when somebody so high profile like
1088800	1092720	this has been engaging in this kind of behavior for years and getting away with it. It also looks
1092720	1097440	bad for people like me who work in the industry, who trust these academics to publish good quality
1097520	1101840	research that we then use to try and influence real world change in businesses, in government,
1101840	1106320	and so on and so forth. Like I said, I've used Geno's work before to make recommendations to my
1106320	1110240	clients. And I've recommended to you guys to read Dan Ariely's book, The Honest Truth About
1110240	1114400	Dishonesty in the Past, a book which I no longer recommend since the paper that was talked about
1114400	1118240	in the first article here is used heavily as a reference for a lot of the claims that Ariely
1118240	1122480	is making in that book. And while it's tempting here to just completely lay into Francesca Geno
1122480	1126480	and just, you know, really have a go at her for this kind of bad behavior, I actually kind of
1126560	1130560	understand why she did it, right? If you're an academic at a top institution like Harvard,
1130560	1134960	you are under an enormous amount of pressure to publish surprising results and consistently.
1134960	1139200	Surprising results with big effect sizes are more likely to get published in top journals when you
1139200	1143760	more press interviews and basically cement your position there at a top university like Harvard.
1143760	1147680	So there is a strong incentive for academics to fudge data like this and come up with more
1147680	1152080	surprising results in order to try and maintain their position. I'm not condoning the behavior in
1152080	1156640	the slightest. It's completely unacceptable that an academic would do this, but I can somewhat
1156640	1160880	empathize that she's under a lot of pressure and can see how the incentives are working against
1160880	1165120	the practice of following good science. But what do you guys think of Francesca Geno and all of
1165120	1168800	this nonsense? Let me know in the comments below. Please go read the articles that are in the
1168800	1173040	description. Thank you to Yuri, Joe, and Leigh for publishing this research. You guys are absolute
1173040	1176160	legends. And Francesca Geno, if you're watching this video, I know you must be going through a
1176160	1180560	really rough time right now to have your sort of entire career ripped away from you so publicly
1180560	1184400	like this. While I think that what you did is completely unacceptable, please don't do anything
1184400	1188560	stupid with your own life. You're still a valuable human being. But thank you guys so much for watching
1188560	1190000	and I'll see you next time. Bye bye.
