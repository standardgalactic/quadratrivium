1
00:00:00,000 --> 00:00:06,080
Academia is broken. Universities are broken. The way that academic research is published

2
00:00:06,080 --> 00:00:10,720
is broken. That's the message that's come through loud and clear over the last few weeks,

3
00:00:10,720 --> 00:00:15,440
thanks to three articles concerning the research of Francesca Geno. If you don't know what I'm

4
00:00:15,440 --> 00:00:20,320
talking about, let me explain. Francesca Geno is a professor of behavioral science at Harvard

5
00:00:20,320 --> 00:00:25,120
University. She is extremely well known in the field. I've talked about her research to clients

6
00:00:25,120 --> 00:00:29,920
before. I've recommended books on this channel to you guys that use her work as a key reference,

7
00:00:30,000 --> 00:00:34,560
I've used her research before as references in my own essays and work that I did at university

8
00:00:34,560 --> 00:00:39,520
when it comes to academic fame, Francesca Geno is up there, as you would expect from someone

9
00:00:39,520 --> 00:00:44,080
who is a professor at Harvard. However, the reason why she's so well known is because her

10
00:00:44,080 --> 00:00:48,880
research tends to bring out a lot of very surprising findings. Now, some people just think this

11
00:00:48,880 --> 00:00:52,960
research is cool and don't think much more about it, but a lot of people in the industry have been

12
00:00:52,960 --> 00:00:58,080
quite skeptical of Francesca Geno and her work because her results just seem a little bit too

13
00:00:58,080 --> 00:01:02,320
good. Her hypotheses are really wacky, but yet they always seem to be proved correct,

14
00:01:02,320 --> 00:01:07,680
the effect sizes from her study seem to be really large, and her statistical significance just seem

15
00:01:07,680 --> 00:01:11,600
a little bit too significant. So while some of us have been skeptical of her work for a while,

16
00:01:11,600 --> 00:01:16,000
nobody has taken the time to actually investigate her research and go into her data to see if they

17
00:01:16,000 --> 00:01:22,000
can find anything fishy... until now. These three guys, Yuri, Joe and Leif, are also professors

18
00:01:22,000 --> 00:01:26,560
of behavioral science and other related subjects from different universities across the world,

19
00:01:26,560 --> 00:01:31,120
and they took it upon themselves to investigate Francesca Geno and her data to see if there was

20
00:01:31,120 --> 00:01:35,760
anything fishy going on. And spoiler alert, they found a lot of fishy stuff in the data,

21
00:01:35,760 --> 00:01:39,600
and that's what the three articles that they released are talking about. Each article relates

22
00:01:39,600 --> 00:01:43,440
to a different study by Francesca Geno, and in this video I'm going to be taking you through

23
00:01:43,440 --> 00:01:48,240
each one. The results of their investigation are shocking, damning for Francesca Geno,

24
00:01:48,240 --> 00:01:52,080
but I think they speak even louder volumes about the state of academia in general,

25
00:01:52,080 --> 00:01:55,920
and that's what I'm going to be concluding on at the end of this video. So without further ado,

26
00:01:56,000 --> 00:01:59,920
let's jump into the first study. So this first article is called clusterfake, and it's referring

27
00:01:59,920 --> 00:02:05,520
to a paper written by Geno in 2012, along with their collaborators Shu, Nina Mazar, Dan Ariely,

28
00:02:05,520 --> 00:02:09,520
and Max Bazeman. Given the fact that I know the first names of all of those researchers with the

29
00:02:09,520 --> 00:02:13,760
exception of Shu, I should tell you that all of these researchers are very well-known people in

30
00:02:13,760 --> 00:02:18,400
the field of behavioral science. So in this study, they were trying to get participants to be more

31
00:02:18,400 --> 00:02:22,960
honest, and their hypothesis was that if you put an honesty pledge at the top of a form,

32
00:02:22,960 --> 00:02:27,280
that'll make people more honest when they then fill out the rest of the form. So all of the studies

33
00:02:27,280 --> 00:02:32,000
in this paper by these authors were looking at this idea, that if you put an honesty pledge at the

34
00:02:32,000 --> 00:02:36,320
top of a form, people will be more honest than if you put the honesty pledge at the bottom of a form.

35
00:02:36,320 --> 00:02:41,200
Now the first study in this paper was led by Francesca Geno, our protagonist. So in this study,

36
00:02:41,200 --> 00:02:46,000
students were brought into a lab to complete 20 math puzzles in five minutes. The students were

37
00:02:46,000 --> 00:02:50,560
told that they would be paid one dollar for each math puzzle they solved correctly, and the way that

38
00:02:50,560 --> 00:02:54,800
this worked was that when students walked into the room, there were two pieces of paper. They had

39
00:02:54,800 --> 00:02:59,120
their work paper and their report paper. So on the work paper, they'd write down their workings for

40
00:02:59,120 --> 00:03:03,040
the math questions and of course their answers, and then on the report paper, they would then have

41
00:03:03,040 --> 00:03:07,600
to report how many answers they got correctly and therefore how much they should get paid. The students

42
00:03:07,600 --> 00:03:11,520
were then told that before handing in their report paper to the researchers and getting paid, that

43
00:03:11,520 --> 00:03:16,480
they should shred their original work paper. The idea behind this is that by shredding their work

44
00:03:16,480 --> 00:03:20,960
paper, there's then a stronger incentive for them to cheat on the report paper and lie about

45
00:03:20,960 --> 00:03:25,040
how many answers they got correct, since the researchers in theory should never know how

46
00:03:25,040 --> 00:03:29,040
many answers they got right on the work paper. But what the students didn't know was that the

47
00:03:29,040 --> 00:03:33,440
shredder at the back of the room was not a normal shredder. What the people in the experiment don't

48
00:03:33,440 --> 00:03:39,120
know is that the shredder has been fixed. So the shredder only shreds the sides of the page,

49
00:03:39,120 --> 00:03:45,120
but the main body of the page remains intact. Now in order to test the hypothesis of the researchers,

50
00:03:45,120 --> 00:03:49,600
on the reporting paper, the participants were split into two groups. Half of them had an honesty

51
00:03:49,600 --> 00:03:53,840
pledge at the top of the paper, and half of them had an honesty pledge at the bottom of the paper,

52
00:03:53,840 --> 00:03:57,440
with the idea being of course that those who signed the honesty pledge at the top would

53
00:03:57,440 --> 00:04:03,440
then cheat less going forward. So what was the result? Well the result showed a massive effect

54
00:04:03,440 --> 00:04:07,200
from this simple intervention. According to what was published in the study originally,

55
00:04:07,200 --> 00:04:12,000
for the students who signed the honesty pledge at the top of the form, only 37% of them lied,

56
00:04:12,000 --> 00:04:17,680
but when students signed at the bottom of the form, 79% of students lied. This is a massive

57
00:04:17,680 --> 00:04:22,480
effect size that the researchers are reporting, and as a result of that, this study gained a lot

58
00:04:22,480 --> 00:04:26,960
of public attention, and I have talked about it with many people in the past before because it is

59
00:04:26,960 --> 00:04:32,880
so surprising. But that's why these vigilantes were suspicious. The results just seem a bit too good.

60
00:04:32,880 --> 00:04:37,040
Can it really be the case that simply moving an honesty pledge from the bottom to the top of a

61
00:04:37,040 --> 00:04:41,920
form can have such a dramatic effect on the amount of cheating that happens? It seems pretty

62
00:04:41,920 --> 00:04:47,600
unlikely. So our vigilantes managed to source the original data set that was published by the

63
00:04:47,600 --> 00:04:52,560
authors of the study, and when they looked into the data it just seemed a little bit fishy. If you

64
00:04:52,560 --> 00:04:58,000
look at this table and specifically look at the left hand column, the P hash column, this is referring

65
00:04:58,000 --> 00:05:03,920
to participant ID. This is the unique ID given to each participant in a study, and as it's highlighted

66
00:05:03,920 --> 00:05:08,080
in yellow, there are some weird anomalies in the way that this data has been sorted. Because

67
00:05:08,080 --> 00:05:12,480
when you look at this data, it seems obvious that this has been sorted by first the condition, so

68
00:05:12,480 --> 00:05:16,640
all of condition one are together, and all of condition two are together, and then in ascending

69
00:05:16,640 --> 00:05:21,920
order of the participant ID, which means that the numbers should consistently get bigger as you go

70
00:05:21,920 --> 00:05:26,560
down the line, and there should be no duplicates. Remember, each participant has a unique ID. So

71
00:05:26,560 --> 00:05:30,720
when you look at this data, it's a bit weird. We've got two 49s here, that's a duplicate,

72
00:05:30,720 --> 00:05:34,720
that should never happen, and then at the end of the condition one set of participants,

73
00:05:34,720 --> 00:05:40,720
you have participant 51 coming after 95, then 12, then 101, like that sequence doesn't make any

74
00:05:40,720 --> 00:05:45,840
sense. And similarly, when you get to condition two, we start with seven, then 91, then 52,

75
00:05:45,840 --> 00:05:50,160
then all the way back down to five again. These entries in the data set look suspicious. They

76
00:05:50,160 --> 00:05:54,640
look like they're out of sequence, which suggests that somebody maybe has tampered with them. So

77
00:05:54,640 --> 00:05:59,840
our vigilantes are suspicious of these rows. So then you have to ask the question, why would the

78
00:05:59,840 --> 00:06:04,560
researchers want to tamper with the data? Well, it's because they would want to show a bigger

79
00:06:04,560 --> 00:06:09,760
effect than those actually seen in the real data. The more dramatic the effect of the intervention

80
00:06:09,760 --> 00:06:14,240
is, the more surprising the result of the study is, and therefore the more likely it is to get

81
00:06:14,240 --> 00:06:18,320
published in the top journal, the more likely it is that this will make a lot of press headlines,

82
00:06:18,320 --> 00:06:22,160
that they will get lots of interviews and work off the back of it. And so there's a strong incentive

83
00:06:22,160 --> 00:06:27,200
for the researchers to fudge the data a little bit, make the effect seem larger than it really is.

84
00:06:27,200 --> 00:06:32,000
And so that's what our vigilantes were looking for. They wanted to see if these suspicious rows

85
00:06:32,000 --> 00:06:37,200
in the data set showed a bigger effect than the normal data that wasn't suspicious. And sure

86
00:06:37,200 --> 00:06:42,000
enough, that's exactly what they found. If you look at this graph, the red circles with the cross

87
00:06:42,000 --> 00:06:46,880
show the suspicious data and the blue dots show the unsuspicious data. And as you can see, the

88
00:06:46,880 --> 00:06:51,680
circles with the red crosses are the most extreme ones, meaning that these few data points are

89
00:06:51,680 --> 00:06:57,120
inflating the effect size. Now the article goes on to show how our vigilantes did some very clever

90
00:06:57,120 --> 00:07:01,600
work to unpack the Excel file that this data was stored in, and they were able to show quite clearly

91
00:07:01,600 --> 00:07:06,800
that these suspicious rows were manually resorted in the data set. I won't go into it on this video

92
00:07:06,800 --> 00:07:10,400
because it's quite technical, but I'll have a link to all of these articles in the description if

93
00:07:10,400 --> 00:07:15,600
you want to read them in full. But as you'll soon see, this theme of suspicious data and then those

94
00:07:15,600 --> 00:07:21,200
data showing extremely strong effect sizes will be a recurring pattern. So let's move on to study

95
00:07:21,200 --> 00:07:26,080
two. Now this second article is called My Class Year is Harvard and you'll see why in a second.

96
00:07:26,080 --> 00:07:31,520
They're looking at a study from 2015 written by Francesca Geno as well as Kuchaki and Galinsky,

97
00:07:31,520 --> 00:07:36,320
again two fairly well known researchers in the field. Now the hypothesis for this study, in my

98
00:07:36,320 --> 00:07:41,360
opinion, is pretty stupid. The hypothesis is that if you argue against something that you really

99
00:07:41,360 --> 00:07:47,200
believe in, that makes you feel dirty, which then increases your desire for cleansing products,

100
00:07:47,200 --> 00:07:52,080
which is kind of silly in my opinion. But nevertheless, this is what they were researching.

101
00:07:52,080 --> 00:07:57,440
So this study was done at Harvard University with almost 500 students and what they asked the

102
00:07:57,440 --> 00:08:01,520
participants to do was the following. So students of Harvard University were brought into the lab

103
00:08:01,520 --> 00:08:05,360
and then asked how they felt about this thing called the Q Guide. I don't really know what the Q

104
00:08:05,360 --> 00:08:09,200
Guide is, but apparently it's a hot topic at Harvard and it's very controversial. Some people are for

105
00:08:09,200 --> 00:08:12,720
it, some people are against it. So when they were brought into the lab, they were asked how do you

106
00:08:12,720 --> 00:08:16,720
feel about the Q Guide and they either said they were for or against it and then the participants

107
00:08:16,720 --> 00:08:21,360
were split into two groups. Half the participants were asked to write an essay supporting the

108
00:08:21,360 --> 00:08:25,600
view that they just gave. So if they said I'm for the Q Guide, they had to then write an essay

109
00:08:25,600 --> 00:08:29,680
explaining why they were for the Q Guide. But then half the participants were asked to write an essay

110
00:08:29,680 --> 00:08:34,080
arguing opposite to the point that they just gave. So if they said I'm for the Q Guide, they would

111
00:08:34,080 --> 00:08:38,800
then have to write an essay explaining why they should be against the Q Guide. Again, the idea

112
00:08:38,800 --> 00:08:43,120
being that those who are writing an essay against what they actually believe in would make them feel

113
00:08:43,120 --> 00:08:47,680
dirty. Because after they'd written this essay, they were then shown five different cleansing

114
00:08:47,680 --> 00:08:52,080
products. And the participants in the study had to rate how desirable they felt these cleansing

115
00:08:52,080 --> 00:08:57,920
products were on a scale of one to seven, with one being completely undesirable and seven being

116
00:08:57,920 --> 00:09:03,920
completely desirable. And again, the authors found a strong effect. You can see here that the p value

117
00:09:03,920 --> 00:09:09,280
is less than 0.0001. And for those of you who haven't had any academic training in statistics,

118
00:09:09,280 --> 00:09:12,720
basically when you're doing a study like this, you're looking for a p value that's less than

119
00:09:12,720 --> 00:09:18,240
0.05. That's the industry standard. If it's less than 0.05, you say, yes, I'm confident that the

120
00:09:18,240 --> 00:09:24,000
effect that I'm seeing is caused by the manipulation that I just did. So less than 0.0001 is an

121
00:09:24,000 --> 00:09:29,760
extremely strong effect. You're basically 100% confident that what you're seeing in the data

122
00:09:29,760 --> 00:09:35,040
is caused by the manipulation that you did. So once again, our vigilantes are suspicious of this

123
00:09:35,040 --> 00:09:39,360
very strong effect sites. So they managed to source the data online and do a little bit of

124
00:09:39,360 --> 00:09:43,920
investigating. And what they find are some weird anomalies in the kind of demographic data that

125
00:09:43,920 --> 00:09:47,520
the participants have to give when they enter the study. And this is very common in psychological

126
00:09:47,520 --> 00:09:51,440
studies that participants have to give a little bit of demographic data about themselves, which

127
00:09:51,440 --> 00:09:55,440
gives the researchers a little bit more flexibility about how they cut up the data later on. So in

128
00:09:55,440 --> 00:09:59,280
this particular study, the participants were asked a number of demographic questions, including their

129
00:09:59,280 --> 00:10:03,840
age, their gender, and then number six was what year in school they were. Now the way this question

130
00:10:03,840 --> 00:10:08,000
is structured isn't very good, in my opinion, in terms of research design. But nevertheless,

131
00:10:08,000 --> 00:10:12,160
there are a number of acceptable answers that you can give to year in school. Because Harvard is an

132
00:10:12,160 --> 00:10:16,160
American school, you might say, I'm a senior, right, which is a common thing, or a sophomore,

133
00:10:16,160 --> 00:10:21,760
you might write the year that you're supposed to graduate, 2015, 2016, etc. Or you might indicate a

134
00:10:21,760 --> 00:10:26,080
one, a two, a three, a four, or a five to indicate how many years of school that you've been in there.

135
00:10:26,080 --> 00:10:29,760
These are all different answers, but they're all acceptable and make sense in the context of being

136
00:10:29,760 --> 00:10:34,000
asked what year in school you. And so when our vigilantes go into the data, that's exactly what

137
00:10:34,000 --> 00:10:37,920
they saw in this column, a range of different answers that were all acceptable, except for

138
00:10:37,920 --> 00:10:42,880
one, there were 20 entries in this data set, where the answer to the question what year in school

139
00:10:42,880 --> 00:10:50,080
are you was Harvard. That doesn't make any sense. What year in school are you? Harvard. What? Right,

140
00:10:50,080 --> 00:10:53,680
that doesn't make any sense. And the other thing that was suspicious about these Harvard entries

141
00:10:53,680 --> 00:10:58,800
is that they were all grouped together within 35 rows. Again, this was a data set of nearly 500

142
00:10:58,800 --> 00:11:04,400
different participants, and yet all of these weird Harvard answers were within 35 rows. So once again,

143
00:11:04,400 --> 00:11:09,840
our vigilantes treat these Harvard answers as suspicious data entries. They mark them in red

144
00:11:09,840 --> 00:11:15,520
circles with crosses. And as you can see, the ones that are suspicious are again, the most

145
00:11:15,520 --> 00:11:21,200
extreme answers supporting the hypothesis of the researchers, with the exception of this one. But

146
00:11:21,200 --> 00:11:25,680
come on, it's most suspicious when you look at the ones on argued other side. So these are the

147
00:11:25,680 --> 00:11:30,000
people who wrote an essay arguing against what they didn't believe in, and therefore was supposed

148
00:11:30,080 --> 00:11:34,880
to feel more dirty and find cleansing products more appealing. All of these suspicious entries on

149
00:11:34,880 --> 00:11:39,360
that side of the manipulation went for seven, that they found all of the cleaning products

150
00:11:39,360 --> 00:11:44,240
completely desirable. And so what our vigilantes go on to say is that these were just the 20 entries

151
00:11:44,240 --> 00:11:48,640
in the data set that looked suspicious because of this Harvard answer to the demographic question.

152
00:11:48,640 --> 00:11:53,120
But who's to say that the other data in the data set was not also tampered with, but just they were

153
00:11:53,120 --> 00:11:57,040
more careful when they filled in this column and didn't put Harvard. Since it seems pretty clear

154
00:11:57,040 --> 00:12:01,360
that at least these 20 entries were manipulated and tampered with some way, it probably means that

155
00:12:01,360 --> 00:12:05,280
there are other entries within this data set that were also tampered with. Are you shocked yet? I

156
00:12:05,280 --> 00:12:09,280
hope you are, but it's about to get worse because there's a third article to do with Francesca

157
00:12:09,280 --> 00:12:13,760
Geno. So this third article was released literally yesterday, the day before I'm filming this video,

158
00:12:13,760 --> 00:12:18,720
and it's called the cheaters are out of order. This is written by Francesca Geno and a guy called

159
00:12:18,720 --> 00:12:23,920
Wilta Muth. I don't know Wilta Muth, but again, I find it incredibly ironic that all of this

160
00:12:23,920 --> 00:12:27,760
cheating and fake data is being conducted by researchers who are studying

161
00:12:27,760 --> 00:12:33,680
the science of honesty. It is incredibly ironic. So in this third study, Geno and her co-author

162
00:12:33,680 --> 00:12:39,360
are investigating the idea that people who cheat, people that lie, who are dishonest,

163
00:12:39,360 --> 00:12:45,200
are actually more creative. And they call the paper evil genius, how dishonesty can lead to

164
00:12:45,200 --> 00:12:52,480
greater creativity. So let's quickly go through how the study worked. Participants were brought

165
00:12:52,480 --> 00:12:57,520
into a lab where they were sat at a machine with a virtual coin flipping mechanism. What the

166
00:12:57,520 --> 00:13:02,960
participants are asked to do is to predict whether the coin will flip heads or tails, and then they

167
00:13:02,960 --> 00:13:07,120
would push a button to actually flip the coin. And if they had predicted correctly about whether

168
00:13:07,120 --> 00:13:11,600
it would go heads or tails, then they would get a dollar. So again, there's a strong incentive to

169
00:13:11,600 --> 00:13:14,960
cheat. So the participants would write down on a piece of paper how many predictions they got

170
00:13:14,960 --> 00:13:18,480
correct, and then they would hand that to the researcher in order to get paid. Then of course,

171
00:13:18,480 --> 00:13:22,400
the researcher would then go back and look at the machine that they were flipping the coin on

172
00:13:22,400 --> 00:13:26,640
to see how many they actually got correct. And then they were able to tell how many times that

173
00:13:26,640 --> 00:13:31,280
participant had cheated. So after they completed the coin flipping task, they were then given a

174
00:13:31,280 --> 00:13:36,720
creativity task. And the creativity task was how many different uses can you think of for a

175
00:13:36,720 --> 00:13:41,040
piece of newspaper? So in psychology, this is a pretty common technique for testing creativity.

176
00:13:41,040 --> 00:13:45,280
You give somebody an inanimate object, and then you say how many uses can you think of

177
00:13:45,280 --> 00:13:51,040
for this inanimate object. And again, with this study, we see a very strong effect size. Remember,

178
00:13:51,040 --> 00:13:58,400
the magic number that academics look for is P less than 0.05. And here we have P less than 0.001.

179
00:13:58,400 --> 00:14:02,320
So basically what that means is that there's an extremely high likelihood that the effect that

180
00:14:02,320 --> 00:14:07,440
the academics are seeing is caused by the manipulation that they did. So again, our vigilantes

181
00:14:07,440 --> 00:14:11,840
are suspicious. But this one is interesting because our vigilantes were able to actually

182
00:14:11,920 --> 00:14:17,280
get the data set from Gino several years ago. So they got this data set directly from Gino.

183
00:14:17,280 --> 00:14:22,160
So again, when our vigilantes look into the data, they find some weird things going on.

184
00:14:22,160 --> 00:14:26,720
As you can see, it seems to be sorted by two things. Firstly, by the number of times the

185
00:14:26,720 --> 00:14:31,040
participant cheated. So all of the people who didn't cheat at all are zeros. And then the

186
00:14:31,040 --> 00:14:35,120
number of responses is the number of different uses for a newspaper that that participant could

187
00:14:35,120 --> 00:14:39,040
come up with. And those are clearly ranked in ascending order. But as you can see from this

188
00:14:39,040 --> 00:14:43,600
next screenshot, some of the cheaters are out of order. So these are the people who cheated once,

189
00:14:43,600 --> 00:14:47,600
who basically over reported one time, and the number of uses that they could come up with for

190
00:14:47,600 --> 00:14:53,680
the newspaper are out of sequence. Here we have three, four, 13, then nine, and then back down to

191
00:14:53,680 --> 00:14:58,480
five again, then back up to nine, then five, then nine, then eight, then nine is just a total mess,

192
00:14:58,480 --> 00:15:02,240
right? So these ones that are highlighted in yellow are the suspicious ones. They're the ones

193
00:15:02,240 --> 00:15:06,720
that are out of order, according to how the data appears to have been sorted. So what our

194
00:15:06,720 --> 00:15:11,440
vigilantes did was they basically took this data set and then made a new column and they called

195
00:15:11,440 --> 00:15:16,480
it imputed low or imputed high. What that basically means is that rather than taking the number of

196
00:15:16,480 --> 00:15:20,240
responses that are written down in this original data set, they're going to say, well, where does

197
00:15:20,240 --> 00:15:25,520
this entry sit in the ranking order? And so we're going to replace the value that is given here

198
00:15:25,520 --> 00:15:29,440
with what the value should really be. So if it's between four and five, then that number should

199
00:15:29,440 --> 00:15:33,760
be either four or five, whether it's imputed low or imputed high. Does that make sense? So once

200
00:15:33,840 --> 00:15:39,040
again, our researchers plotted the data, suspicious entries are marked with a circle and a cross.

201
00:15:39,600 --> 00:15:44,320
And as you can see, the suspicious entries are the ones that deviate from the pattern that you see

202
00:15:44,320 --> 00:15:48,080
in the non cheaters, the blue line. So in other words, the ones that are out of order,

203
00:15:48,080 --> 00:15:53,040
the suspicious entries, they're the ones showing the effect. But when you use the imputed position,

204
00:15:53,040 --> 00:15:58,000
so that's the number that is implied by the row that the entry was in, then suddenly the entire

205
00:15:58,000 --> 00:16:02,480
effect disappears. And the group of cheaters seem to show a very similar pattern to the group of

206
00:16:02,480 --> 00:16:07,600
non cheaters. And the result of this statistically speaking is significant. Remember the original

207
00:16:07,600 --> 00:16:13,680
p value for this study was p less than 0.0001. But once you use the data that's implied by the

208
00:16:13,680 --> 00:16:20,480
row, suddenly the significance completely disappears. It then goes to p equals 0.292 or p equals 0.180,

209
00:16:20,480 --> 00:16:24,640
depending on whether you're imputing low or high. Remember, in order for an academic study to be

210
00:16:24,640 --> 00:16:31,120
significant, the standard is p less than 0.05. And here the p is clearly more than 0.05. Again,

211
00:16:31,120 --> 00:16:35,120
this article goes on. The vigilantes do a little bit more research to really back up the point and

212
00:16:35,120 --> 00:16:40,160
really drive home the fact that this data is very suspicious. I won't go into the details now. Again,

213
00:16:40,160 --> 00:16:43,600
all of these articles are linked in the description. Go check them out. And you'll notice that these

214
00:16:43,600 --> 00:16:47,600
were all called part one, part two, part three. And that's because this is actually a four part

215
00:16:47,600 --> 00:16:52,080
series. So I'm expecting a fourth article to come out after this video is published looking at yet

216
00:16:52,080 --> 00:16:56,880
another study from Francesca Geno. But I hope by this point, you get the picture, there's a number

217
00:16:56,880 --> 00:17:01,200
of studies conducted by Francesca Geno with very suspicious looking data. So at this point,

218
00:17:01,200 --> 00:17:06,160
you're probably wondering how did Harvard allow this? And the short answer is, well,

219
00:17:06,160 --> 00:17:10,160
they don't really seem to have done. If you go on Francesca Geno's page on the Harvard website,

220
00:17:10,160 --> 00:17:14,880
it shows that she's on administrative leave. I think we all know what that means. And Harvard,

221
00:17:14,880 --> 00:17:19,360
who have even more access to Francesca Geno's data than our vigilantes do, have since asked for

222
00:17:19,360 --> 00:17:23,280
several of Francesca Geno's papers to be retracted from the journals that they were originally

223
00:17:23,360 --> 00:17:28,080
published in. Now, this is a bad look for Francesca Geno, right? And we can't be sure that it was

224
00:17:28,080 --> 00:17:32,160
Francesca Geno who was doing this manipulation. It could be one of her co-authors. But given that

225
00:17:32,160 --> 00:17:36,080
she's the common thread between all of these different papers, it seems pretty likely that

226
00:17:36,080 --> 00:17:41,520
it was her in the world of psychology and writing good quality academic papers. This is really bad.

227
00:17:41,520 --> 00:17:46,400
It's not only bad for Francesca Geno, but it's bad for the field as a whole. It casts doubt over

228
00:17:46,400 --> 00:17:50,800
the entire field of behavioral science, because we don't know the extent of the damage that bad

229
00:17:50,800 --> 00:17:55,360
actors like Geno have been causing in the field and for how long. Like I said, Francesca Geno has

230
00:17:55,360 --> 00:17:59,840
been a prominent name in the field for years, gaining a position at one of the top universities,

231
00:17:59,840 --> 00:18:04,240
Harvard. So who's to say that this isn't a problem that is rife amongst many other researchers

232
00:18:04,240 --> 00:18:08,800
in the field? We certainly hope not. But you can't really know when somebody so high profile like

233
00:18:08,800 --> 00:18:12,720
this has been engaging in this kind of behavior for years and getting away with it. It also looks

234
00:18:12,720 --> 00:18:17,440
bad for people like me who work in the industry, who trust these academics to publish good quality

235
00:18:17,520 --> 00:18:21,840
research that we then use to try and influence real world change in businesses, in government,

236
00:18:21,840 --> 00:18:26,320
and so on and so forth. Like I said, I've used Geno's work before to make recommendations to my

237
00:18:26,320 --> 00:18:30,240
clients. And I've recommended to you guys to read Dan Ariely's book, The Honest Truth About

238
00:18:30,240 --> 00:18:34,400
Dishonesty in the Past, a book which I no longer recommend since the paper that was talked about

239
00:18:34,400 --> 00:18:38,240
in the first article here is used heavily as a reference for a lot of the claims that Ariely

240
00:18:38,240 --> 00:18:42,480
is making in that book. And while it's tempting here to just completely lay into Francesca Geno

241
00:18:42,480 --> 00:18:46,480
and just, you know, really have a go at her for this kind of bad behavior, I actually kind of

242
00:18:46,560 --> 00:18:50,560
understand why she did it, right? If you're an academic at a top institution like Harvard,

243
00:18:50,560 --> 00:18:54,960
you are under an enormous amount of pressure to publish surprising results and consistently.

244
00:18:54,960 --> 00:18:59,200
Surprising results with big effect sizes are more likely to get published in top journals when you

245
00:18:59,200 --> 00:19:03,760
more press interviews and basically cement your position there at a top university like Harvard.

246
00:19:03,760 --> 00:19:07,680
So there is a strong incentive for academics to fudge data like this and come up with more

247
00:19:07,680 --> 00:19:12,080
surprising results in order to try and maintain their position. I'm not condoning the behavior in

248
00:19:12,080 --> 00:19:16,640
the slightest. It's completely unacceptable that an academic would do this, but I can somewhat

249
00:19:16,640 --> 00:19:20,880
empathize that she's under a lot of pressure and can see how the incentives are working against

250
00:19:20,880 --> 00:19:25,120
the practice of following good science. But what do you guys think of Francesca Geno and all of

251
00:19:25,120 --> 00:19:28,800
this nonsense? Let me know in the comments below. Please go read the articles that are in the

252
00:19:28,800 --> 00:19:33,040
description. Thank you to Yuri, Joe, and Leigh for publishing this research. You guys are absolute

253
00:19:33,040 --> 00:19:36,160
legends. And Francesca Geno, if you're watching this video, I know you must be going through a

254
00:19:36,160 --> 00:19:40,560
really rough time right now to have your sort of entire career ripped away from you so publicly

255
00:19:40,560 --> 00:19:44,400
like this. While I think that what you did is completely unacceptable, please don't do anything

256
00:19:44,400 --> 00:19:48,560
stupid with your own life. You're still a valuable human being. But thank you guys so much for watching

257
00:19:48,560 --> 00:19:50,000
and I'll see you next time. Bye bye.

