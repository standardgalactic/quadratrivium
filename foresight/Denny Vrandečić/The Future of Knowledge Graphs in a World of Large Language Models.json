{"text": " I was invited by Fran\u00e7ois Chavre to keynote one of the days of the Knowledge Graph Conference in May 2023 in New York, New York. This is a post-conference recording of the talk. Fran\u00e7ois asked me what topic I want to talk about. I said wiki data and wiki functions and he said, do you have something else to talk about? You talked about it the last time you were here at the conference in 2019 and I said there's one topic everyone talks to me about and this is what about knowledge graphs and large language models and he said perfect do that and I thought great given how much I have been talking and thinking about it I probably should have a good idea what I want to say by May. Well it's May now and I'm still full of doubts about what to say. I recently stumbled upon this quote looking at books for my daughter and it gives me some hope for the talk today. When in doubt you can't be wrong. So let this be my first disclaimer for today. I am most certainly in doubt. I don't know what the future of knowledge graphs is but I have been talking about this for a decade now for a lot of very smart people so I hope that what I say will prove your time. There is one thing I have no doubts about. Something big is happening. See this is from the book Diffusion of Innovation. It shows how technology is adopted. Well my brother who lives in a small village of 180 people on a Croatian island he is roughly here on this chart but he called me last fall to talk about stable diffusion the image generation model. Chatchity P a large language model reached 100 million users two months after launch. This is a record in how fast the technology is spreading. If there's one thing we don't need to be in doubt about is that these technology are having a large impact on the world. Last year in September that's two months before the release of Chatchity P a number of researchers and practitioners in the knowledge graph field met in Dachstuhl in Germany. Our goal was to discuss the role of knowledge engineering in the 21st century like we were almost under a kind of shock talking about it processing and trying to figure out what it really means for us. I'm trying to distill some of the results from that seminar and also from any other conversations I had in this talk but you can also just go and read the report directly. My second disclaimer nothing in this talk is generated by an AI. And this explicitly marked so or you see a screenshot from an AI. This is not one of those talks where we have a reveal at the end that all of this was generated. And finally the last disclaimer this is a focused talk it is only about knowledge graphs and large language models. It is not about the ethical questions of large language models. It's not about blunders of current LLMs. It's not of the that those systems will have about the impact on jobs and the economy. It's not about questions of copyright or whether LLMs are a path to destruction of the human race. All of these questions are interesting and deserve their own talks and I will not talk about them today. So what makes me believe that there is a future for knowledge graphs in the world of large language models? About six or seven years ago I was working at Google on the knowledge graph. Back then the knowledge graph wasn't necessarily a generic term yet. It was a big question Google had to ask itself. The knowledge graph costs a lot of resources, a lot of money, a lot of manpower to run and maintain. A lot of people were working on the knowledge graph. Is all of this work going to be obsolete? Should we stop investing in that? I give you the answer that I came up with back then. Let's think first principles. So what is a knowledge graph? A knowledge graph represents things and the relations usually stored in a graph database. WikiData is an example. WikiData is the largest publicly available free knowledge graph that anyone can edit. What is a large language model? Model here means that we have a neural network, nodes connected with each other with weights. The weights in this neural networks are trained on some input and desired outputs in order to create good outputs on novel inputs. They are language models because they are trained on natural language texts and it is a large language model because it is trained on lots and lots of natural language texts. GPT is a family of large language models which have been created by OpenAI. GPT-4 is the current version but OpenAI has not published much data about it. So the following data is for GPT-3. GPT-3 has 96 such layers in the picture and it had 175 billion parameters. That is weights. As a large language model, GPT-3 has been trained to provide appropriate answers to a given prompt to our input. So for example, given the following input or prompt, who created the School of Athens. JGDP answers, the School of Athens is a famous fresco painted by the Italian Renaissance artist Raphael. It was painted between 1509 and 1511 as part of a commission for the Vatican's Apostolic Palace in Rome. The fresco is widely regarded as one of the greatest works of art of the high Renaissance. This answer is not only correct, it is brilliant. There's absolutely nothing to complain about. It gives context, it answers the question. It's great. If you go to Google and ask the same question, the answer is the same, Raphael. But the context is even more amazing. A picture from Raphael, his biography, data about him, who influenced him, other artists that are being searched for. The answer is just beautiful and amazing. You can also go to Wikidata and write a sparkle query, asking for the creator of the School of Athens. Again, you will get the result, Raphael. Not as pretty as Google, not as much context as with JGDP. But here's one thing, JGDP took about five seconds to answer my question. Admittedly, it gave a lot of context I didn't ask for, but even just passing the question took close to a second. Google took half a second, including passing, but it also got a lot of great context and gave me results from the search index, even though they didn't make it to a screenshot. Wikidata took about a quarter of a second to produce the answer, and that's not all. JGDP is running on the fastest and most expensive hardware you can buy. Google is running on the fastest and most expensive hardware you cannot buy. And the Wikidata query service is running abandonware on a server somewhere. And if you think about this, it is not surprising. The query I asked had six tokens of input. A token is roughly a word for a natural language model. And the answer produced 60 tokens. Now, if the answer would just have been Raphael, it would still have been at least two tokens. Every single token runs for the 96 layers of JGDP, branching out to 175 billion weights and multiplying matrices and soft maximum results. Whereas in Wikidata, we look up an item out of 100 million and find a key out of 10,000 to get the results values. Those are both logarithmic operations. It's just much cheaper. Given this paper, getting and hosting your own copy of Wikidata, 1,000 Wikidata queries, like the ones I just asked, would cost me about 14 cents in the cloud. The authors thought this was too expensive and bought their own hardware to make it even cheaper. Whereas if I look at Azure pricing for GPT-4, a thousand queries, as asked before, would cost $7.56. So that's 14 cents compared to more than $7, so a factor of 50. 50 times more expense can make a difference. Now, to make it very clear, I don't know how robust these numbers are. I would love to see more robust numbers and I expect the cost for inference to go down. But even Sam Altman, the CEO of OpenAI, the company making JetGDP, describes the compute cost of JetGDP as eye-watering. And don't forget, he made a really good deal with Microsoft about running it on Azure. John Hennessey, chairman of Google and former computer science professor at Stanford, so he really knows what he's talking about, said that running at JetGDP, like Google, would be 10 times more expensive. And that's just talking about the inference costs. Each of these models also need to be trained. And current state-of-the-art language models cost millions of dollars to train. For GPT-4, the cost was given as more than $100 million. The cost for GPT-3 was around $4 million. Good news is, you probably don't have to do this training. But you can just hopefully reuse an existing open source model that you can find due to your task. This will be considerably cheaper to train a model from the foundation on. But not for inference. The same cost that we saw earlier remains. Here's some interesting new thoughts. Some folks, like Sam Altman, think that the age of large language models is already over. OpenAI says it's because of diminishing returns of further reading. Guess what? After reading a million books worth of text, you don't seem to learn too much new stuff. He said that OpenAI is not working on GPT-5, but that they want to explore new ideas. And that's great. Because with models that are more readily available out there, such as Metas Llama, we can see new ideas happening very fast. For example, when Llama was leaked within days, someone managed to run it on consumer hardware. A few days later, we got it running on a phone, and then a Raspberry Pi. It was very slow, but it ran. And by the way, as ridiculous as that sounds, this still means that these giant models are more expensive to run than a knowledge graph. And really, it's not just cost, right? There are a few challenges that large language models need to overcome. We need new ideas for those. Let's take a look. Here's one thing that surprised me a lot. A few weeks ago, I stumbled into one of those Wikipedia rabbit holes where I got temporarily obsessed with figuring out one specific fact. The correct place of birth of a not-too-well-known Croatian actress, Anna Begovic. I was surprised that Google and Wikipedia Wikidata were giving different answers, so I tried to figure out the truth and fix it. Here's a screenshot of Google answering the question. Begovic was born in Terpan. Wikipedia used to say split. After a bit of hunting through sources, I figured out that it mostly likely is Terpan, though. And that a lot of the sources were copying from Wikipedia and Wikidata and became contaminated by Wikipedia and Wikidata. This is an example of our knowledge ecosystem being substantial danger, by the way, and that long before we have LLM syndemics. Now, if you go to Bing Chat, which is powered by OpenAI's GPT, but has access to web search and ask for the place of birth of Anna Begovic, it also answers me Terpan, which is great, and it gives me free sources for that answer. It's just very disappointing that if you follow one of the references to Wikipedia, you will actually find it says split. When I asked ChatGDP instead of Bing Chat, I get a different answer. It answers split. This is not too surprising. After all, Wikipedia was claiming split until just a few weeks ago, and ChatGDP was trained on a 2021 copy of the web. Corrections to Wikipedia done in 2023 would not show up. Split is totally expected as an answer here. What is interesting, though, is if I ask the same question in Croatian, I get a different wrong answer. Zagreb. Now, that's an interesting answer, because it demonstrates us two things. First, knowledge in ChatGDP seems to be not stored in a language independent way, but is stored within each individual language. Depending on the language I ask the question in, I receive a different answer. Also, the English answer at least has support in the training data. Wikipedia did say split. The Croatian answer, where does Zagreb come from? I can find a single source that says that Anna Begovic was born in Zagreb. But here's the thing. What if ChatGDP is actually just guessing, it's just making it up? What is the probability of Zagreb given the prior of Croatian actress? In order to figure that out, I want to check Wikipedia. I ask ChatGDP to give me a sparkle query for Croatian actors in the places of birth. The query it returns to me is good enough for our purpose. I can just copy and paste it. It is subtly wrong if you read it in detail. It does not ask for Croatian actors, but for actors born in the place in Croatia. But again, good enough for our purposes. But still, this is fascinating. ChatGDP got out of the box all of the QIDs right in this query. For Croatia, for actor, it got a property ID right. It can make sparkle queries that are syntactically right. And all of that with zero shot extra training. There was no look up on the web. ChatGDP just knows these QIDs by heart. In fact, you can totally ask ChatGDP to make you a table of all the countries of the European Union and their QIDs. It has a lot of QIDs memorized. Just you never know if maybe one of them is wrong or not. The query can be copied and pasted right into the Wikipedia query service. And it runs giving 445 answers. And you can already see in the screenshot that the first few are all in Zagreb. So now we can see that of the 445 Croatian actors for place of birth, 154 are born in Zagreb, about a third. This gives a pretty good conditional probability for the birthplace of Croatian actors. ChatGDP is guessing the place of birth for Anna Begovic in Croatian, even though it knows it in English. This leads me to something my manager at Google used to say. You can machine learn Obama's birthplace every time you need it, but it costs a lot and you're never sure it is correct. Another interesting observation is that GPT isn't particularly good with knowing what it knows. For example, here we are asking for cities with mayors born after 1998. I wanted to ask something that I expected to be not trivially Google-able, where there wouldn't be a listicle or table on the map. So ChatGDP correctly points out that that would make them younger than 23 at its cutoff 80 date of 2021. So it is unlikely that anyone would be mayor at that age. Asking Bing Chat, it also says that it doesn't know anything about mayors born after 1998. Let's check Wikidata. I again used ChatGDP to help me write a query, although this time I had to make a few fixes. And indeed, Wikidata has an answer. It tells me about Christian von Waldenfelds, who was born in April 2000, mayor of Lichtenberg in Bavaria, Germany. He became mayor in 2020, well before the cutoff date of Chaterity, by the way. This is no endorsement of his politics or his platform he is running, by the way. Now that we know the answers, we can guide Bing Chat and get the mayor of Lichtenberg and his age. And obviously this answer is inconsistent with its previous answer, but ChatGDP or BingJDP are both completely unaware of this inconsistency and don't care. Large language models are not yet graded being consistent, whether about individual facts or across different languages. They are also not always very good at math. But even if they were good at math, we have to answer the same question that we do for looking up facts in a knowledge graph. Why would you ever use 96 layers, 175 billion parameters model, to do multiplication? When that's something you can do in a single operation on your CPU. Why internalized knowledge in an LLM if you can externalize it in a graph store and look it up when needed. Don't get bedazzled by the capabilities of large language models. Autoregressive transformer models such as ChatGDP are touring complete and they are just a very expensive reiteration of touring's carpet. You can do everything with them, but it doesn't mean you should. Use LLMs where they are efficient and use other things where they are better. One possible solution to this capability gap are so-called augmented language models. Toolform are being a particularly well-known example. Or for ChatGDP, that's what ChatGDP plugins are there for. The idea is that we can enrich large language models with additional systems which are good and efficient at specific tasks, such as math or other functions, from wiki functions, or looking up facts or query results in a knowledge graph such as wiki data. I mean, given that ChatGDP already can, zero shot, create queries against wiki data, there isn't that much to do to make them work together. Some folks think that some mapping of strategic nodes into a knowledge graph with embeddings, we can easily connect a knowledge graph directly to a large language model. But we don't even need to do that, we can just use the Sparkle query generation ability directly and ask queries against wiki data. And not only can we connect LLMs to a knowledge graph, but also to a repository of functions, such as wiki functions. Both knowledge graphs and functions would be tools the LLM can learn to use. There is work in trying to understand how knowledge is stored in the parameters of a large language model. And when we look at this work, we start to understand why large language models are large. Do they really have to be this large? Let's compare with stable diffusion one. Stable diffusion one is a text to image generator. It has to understand natural language prompts, just as GPT does. It can make an image out of basically any prompt. It can also generate the image of a good number of celebrities. Here, for example, I'm asking for the Pope, Geleta Thunberg, Idris Elba, Michelle Yeo, Helen Mirren, and Yanle Kuhn to explain knowledge graphs with the help of a whiteboard. So GPT-3 had 175 billion parameters. How many parameters do you think are in stable diffusion one, knowing all these people being able to generate them? 890 million parameters. Now, I think 890 million is a lot. But GPT-3 is about 200 times larger than stable diffusion one. And it's no surprise. Think of it. All the knowledge that we were using for questions answering so far. The embedded QIDs, what are the member countries of the EU, the place of birth of individual people. I mean, if it has NABegovitch, I'm sure GPT-3 knows the place of birth of millions of individuals. All this taught in many different languages. All of this is taught in those hundreds of billions of parameters. You don't need that for text generation. But you need it if you want to answer all these questions we have been asking. And indeed, Metas Lama, which came out a few weeks ago, is considerably smaller than GPT-3, about 25 times smaller. But it seems to be rather competitive in terms of language understanding and fluency. So yes, we could internalize all the 1.4 billion statements in Wikidata into a large language model. But what if we go the other way around and try to externalize the knowledge model instead? If we leave the language model to deal with language, but push the knowledge to a knowledge graph or a different knowledge model, in a world where language models can generate infinite content, knowledge becomes valuable. And that takes us back to Jamie Taylor's rule. We don't want to machine learn Obama's place of birth every time we need it. We want to store it once and for all. And that's what knowledge graphs are good for. To keep you valuable knowledge safe. I am of the strict belief there is no reason to ever again manually enter the place of birth for Anna Begovic. This makes knowledge for Wikidata both valuable and a public good. The knowledge graph provides you with the ground truth for your language models. By the way, the other way around is also true. Large language models can be an amazing tool to speed up the creation of a knowledge graph. They are probably the best tool for knowledge extraction we have seen developed in a decade or two. We want to extract knowledge into a symbolic form. We want the system to overfit for truth. And this is why it makes so much sense to store the knowledge in a symbolic system. One that can be edited, audited, curated, understood. We can cover the long tail by simply adding new notes to the knowledge graph. One we don't train to return knowledge with a certain probability to make stuff up on the fly. But one where we can simply look it up. And maybe not all of the pieces are in place to make this happen just yet. There are questions around identity and embeddings. How exactly do they talk with each other? But there are good ideas to help with those problems. And knowledge graphs themselves should probably also evolve. I want to make one particular suggestion here. Freebase, the Google Knowledge Graph, Wikidata, they all have two kinds of special values or special statements. The third one is the possibility to say that a specific statement has no value. Here, for example, we are saying that Elizabeth I has no children. The second special value is the unknown value. That is, we know that there is a value for it, but we don't know what the value is. It's like a question mark in the graph. For example, we don't know who Adam Smith's father is, but we know he has one. It could be one of the existing notes. It could be one that we didn't represent yet. We have no idea. My suggestion is to introduce a third special value. It's complicated. I usually get people laughing when I make this suggestion, but I'm really serious. It's complicated is what you would use if the answer cannot be stated with the expressivity of your knowledge graph. This helps with maintaining the graph to mark difficult spots explicitly. This helps with avoiding embarrassing wrong or flat out dangerous answers. Given the interaction with LLMs, this can in particular mark areas of knowledge where we say, don't trust the graph. Can we instead train the LLM harder on this particular question and assign a few extra parameters for that? But really, what we want to be able to say are more expressive statements. In order to build a much more expressive ground truth, to be able to say sentences like these, Jupiter is the largest planet in the solar system. That's what we are working on right now. With abstract Wikipedia and leaky functions, we aim to vastly extend the limited expressivity of Viki data so that complicated things become stateable. This way, we hope to provide a ground truth for large language models. In summary, large language models are truly awesome. They are particularly awesome as an incredibly enabling UX tool. It's just breathtaking, honestly, things are happening which I didn't think possible in my lifetime. But they hallucinate. They need ground truth. They just make up stuff. They are expensive to train and to run. They're difficult to fix and repair, which is great if you have to explain someone, sorry, I cannot fix your problem. The thing is making a mistake, but I don't have a clue how to make it better. They are hard to audit and explain, which in areas like finance and medicine is crucial. They give inconsistent answers. They struggle with low resource languages. And they have a coverage gap on long tail entities, which is not easily overcome. All of these problems can be solved with knowledge graphs, which is why I think that the future of knowledge graphs is brighter than ever, especially thanks to a world that has large language models in it. Thank you for your attention. Thanks to all the people who helped me clarify my thinking around this topic. And if you have any questions, feel free to put them in the comments.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.84, "text": " I was invited by Fran\u00e7ois Chavre to keynote one of the days of the Knowledge Graph", "tokens": [50364, 286, 390, 9185, 538, 1526, 12368, 7376, 761, 706, 265, 281, 33896, 472, 295, 264, 1708, 295, 264, 32906, 21884, 50606], "temperature": 0.0, "avg_logprob": -0.23472568476311514, "compression_ratio": 1.597609561752988, "no_speech_prob": 0.04728812724351883}, {"id": 1, "seek": 0, "start": 4.84, "end": 11.44, "text": " Conference in May 2023 in New York, New York. This is a post-conference recording", "tokens": [50606, 22131, 294, 1891, 44377, 294, 1873, 3609, 11, 1873, 3609, 13, 639, 307, 257, 2183, 12, 1671, 5158, 6613, 50936], "temperature": 0.0, "avg_logprob": -0.23472568476311514, "compression_ratio": 1.597609561752988, "no_speech_prob": 0.04728812724351883}, {"id": 2, "seek": 0, "start": 11.44, "end": 18.12, "text": " of the talk. Fran\u00e7ois asked me what topic I want to talk about. I said wiki data", "tokens": [50936, 295, 264, 751, 13, 1526, 12368, 7376, 2351, 385, 437, 4829, 286, 528, 281, 751, 466, 13, 286, 848, 261, 9850, 1412, 51270], "temperature": 0.0, "avg_logprob": -0.23472568476311514, "compression_ratio": 1.597609561752988, "no_speech_prob": 0.04728812724351883}, {"id": 3, "seek": 0, "start": 18.12, "end": 23.04, "text": " and wiki functions and he said, do you have something else to talk about? You", "tokens": [51270, 293, 261, 9850, 6828, 293, 415, 848, 11, 360, 291, 362, 746, 1646, 281, 751, 466, 30, 509, 51516], "temperature": 0.0, "avg_logprob": -0.23472568476311514, "compression_ratio": 1.597609561752988, "no_speech_prob": 0.04728812724351883}, {"id": 4, "seek": 0, "start": 23.04, "end": 27.8, "text": " talked about it the last time you were here at the conference in 2019 and I", "tokens": [51516, 2825, 466, 309, 264, 1036, 565, 291, 645, 510, 412, 264, 7586, 294, 6071, 293, 286, 51754], "temperature": 0.0, "avg_logprob": -0.23472568476311514, "compression_ratio": 1.597609561752988, "no_speech_prob": 0.04728812724351883}, {"id": 5, "seek": 2780, "start": 27.8, "end": 35.96, "text": " said there's one topic everyone talks to me about and this is what about", "tokens": [50364, 848, 456, 311, 472, 4829, 1518, 6686, 281, 385, 466, 293, 341, 307, 437, 466, 50772], "temperature": 0.0, "avg_logprob": -0.2259877871160638, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.007693402469158173}, {"id": 6, "seek": 2780, "start": 35.96, "end": 42.8, "text": " knowledge graphs and large language models and he said perfect do that and", "tokens": [50772, 3601, 24877, 293, 2416, 2856, 5245, 293, 415, 848, 2176, 360, 300, 293, 51114], "temperature": 0.0, "avg_logprob": -0.2259877871160638, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.007693402469158173}, {"id": 7, "seek": 2780, "start": 42.8, "end": 46.8, "text": " I thought great given how much I have been talking and thinking about it I", "tokens": [51114, 286, 1194, 869, 2212, 577, 709, 286, 362, 668, 1417, 293, 1953, 466, 309, 286, 51314], "temperature": 0.0, "avg_logprob": -0.2259877871160638, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.007693402469158173}, {"id": 8, "seek": 2780, "start": 46.8, "end": 53.24, "text": " probably should have a good idea what I want to say by May. Well it's May now and", "tokens": [51314, 1391, 820, 362, 257, 665, 1558, 437, 286, 528, 281, 584, 538, 1891, 13, 1042, 309, 311, 1891, 586, 293, 51636], "temperature": 0.0, "avg_logprob": -0.2259877871160638, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.007693402469158173}, {"id": 9, "seek": 5324, "start": 53.4, "end": 59.84, "text": " I'm still full of doubts about what to say. I recently stumbled upon this quote", "tokens": [50372, 286, 478, 920, 1577, 295, 22618, 466, 437, 281, 584, 13, 286, 3938, 36668, 3564, 341, 6513, 50694], "temperature": 0.0, "avg_logprob": -0.18075334548950195, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.007457738742232323}, {"id": 10, "seek": 5324, "start": 59.84, "end": 64.64, "text": " looking at books for my daughter and it gives me some hope for the talk today.", "tokens": [50694, 1237, 412, 3642, 337, 452, 4653, 293, 309, 2709, 385, 512, 1454, 337, 264, 751, 965, 13, 50934], "temperature": 0.0, "avg_logprob": -0.18075334548950195, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.007457738742232323}, {"id": 11, "seek": 5324, "start": 64.64, "end": 72.68, "text": " When in doubt you can't be wrong. So let this be my first disclaimer for today. I", "tokens": [50934, 1133, 294, 6385, 291, 393, 380, 312, 2085, 13, 407, 718, 341, 312, 452, 700, 40896, 337, 965, 13, 286, 51336], "temperature": 0.0, "avg_logprob": -0.18075334548950195, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.007457738742232323}, {"id": 12, "seek": 5324, "start": 72.68, "end": 78.08, "text": " am most certainly in doubt. I don't know what the future of knowledge graphs is", "tokens": [51336, 669, 881, 3297, 294, 6385, 13, 286, 500, 380, 458, 437, 264, 2027, 295, 3601, 24877, 307, 51606], "temperature": 0.0, "avg_logprob": -0.18075334548950195, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.007457738742232323}, {"id": 13, "seek": 5324, "start": 78.08, "end": 83.12, "text": " but I have been talking about this for a decade now for a lot of very smart", "tokens": [51606, 457, 286, 362, 668, 1417, 466, 341, 337, 257, 10378, 586, 337, 257, 688, 295, 588, 4069, 51858], "temperature": 0.0, "avg_logprob": -0.18075334548950195, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.007457738742232323}, {"id": 14, "seek": 8312, "start": 83.12, "end": 88.72, "text": " people so I hope that what I say will prove your time. There is one thing I", "tokens": [50364, 561, 370, 286, 1454, 300, 437, 286, 584, 486, 7081, 428, 565, 13, 821, 307, 472, 551, 286, 50644], "temperature": 0.0, "avg_logprob": -0.19806681397140666, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.0024340678937733173}, {"id": 15, "seek": 8312, "start": 88.72, "end": 95.04, "text": " have no doubts about. Something big is happening. See this is from the book", "tokens": [50644, 362, 572, 22618, 466, 13, 6595, 955, 307, 2737, 13, 3008, 341, 307, 490, 264, 1446, 50960], "temperature": 0.0, "avg_logprob": -0.19806681397140666, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.0024340678937733173}, {"id": 16, "seek": 8312, "start": 95.04, "end": 100.32000000000001, "text": " Diffusion of Innovation. It shows how technology is adopted. Well my brother", "tokens": [50960, 413, 3661, 5704, 295, 27092, 13, 467, 3110, 577, 2899, 307, 12175, 13, 1042, 452, 3708, 51224], "temperature": 0.0, "avg_logprob": -0.19806681397140666, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.0024340678937733173}, {"id": 17, "seek": 8312, "start": 100.32000000000001, "end": 104.84, "text": " who lives in a small village of 180 people on a Croatian island he is", "tokens": [51224, 567, 2909, 294, 257, 1359, 7288, 295, 11971, 561, 322, 257, 37614, 952, 6077, 415, 307, 51450], "temperature": 0.0, "avg_logprob": -0.19806681397140666, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.0024340678937733173}, {"id": 18, "seek": 8312, "start": 104.84, "end": 111.48, "text": " roughly here on this chart but he called me last fall to talk about stable", "tokens": [51450, 9810, 510, 322, 341, 6927, 457, 415, 1219, 385, 1036, 2100, 281, 751, 466, 8351, 51782], "temperature": 0.0, "avg_logprob": -0.19806681397140666, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.0024340678937733173}, {"id": 19, "seek": 11148, "start": 111.48, "end": 116.48, "text": " diffusion the image generation model. Chatchity P a large language model", "tokens": [50364, 25242, 264, 3256, 5125, 2316, 13, 761, 852, 507, 430, 257, 2416, 2856, 2316, 50614], "temperature": 0.0, "avg_logprob": -0.24084017826960638, "compression_ratio": 1.6266094420600858, "no_speech_prob": 0.0028007272630929947}, {"id": 20, "seek": 11148, "start": 116.48, "end": 124.52000000000001, "text": " reached 100 million users two months after launch. This is a record in how", "tokens": [50614, 6488, 2319, 2459, 5022, 732, 2493, 934, 4025, 13, 639, 307, 257, 2136, 294, 577, 51016], "temperature": 0.0, "avg_logprob": -0.24084017826960638, "compression_ratio": 1.6266094420600858, "no_speech_prob": 0.0028007272630929947}, {"id": 21, "seek": 11148, "start": 124.52000000000001, "end": 128.76, "text": " fast the technology is spreading. If there's one thing we don't need to be", "tokens": [51016, 2370, 264, 2899, 307, 15232, 13, 759, 456, 311, 472, 551, 321, 500, 380, 643, 281, 312, 51228], "temperature": 0.0, "avg_logprob": -0.24084017826960638, "compression_ratio": 1.6266094420600858, "no_speech_prob": 0.0028007272630929947}, {"id": 22, "seek": 11148, "start": 128.76, "end": 134.6, "text": " in doubt about is that these technology are having a large impact on the world.", "tokens": [51228, 294, 6385, 466, 307, 300, 613, 2899, 366, 1419, 257, 2416, 2712, 322, 264, 1002, 13, 51520], "temperature": 0.0, "avg_logprob": -0.24084017826960638, "compression_ratio": 1.6266094420600858, "no_speech_prob": 0.0028007272630929947}, {"id": 23, "seek": 11148, "start": 134.6, "end": 139.68, "text": " Last year in September that's two months before the release of Chatchity P a", "tokens": [51520, 5264, 1064, 294, 7216, 300, 311, 732, 2493, 949, 264, 4374, 295, 761, 852, 507, 430, 257, 51774], "temperature": 0.0, "avg_logprob": -0.24084017826960638, "compression_ratio": 1.6266094420600858, "no_speech_prob": 0.0028007272630929947}, {"id": 24, "seek": 13968, "start": 139.8, "end": 143.88, "text": " number of researchers and practitioners in the knowledge graph field met in", "tokens": [50370, 1230, 295, 10309, 293, 25742, 294, 264, 3601, 4295, 2519, 1131, 294, 50574], "temperature": 0.0, "avg_logprob": -0.16555617196219308, "compression_ratio": 1.6423357664233578, "no_speech_prob": 0.002018705941736698}, {"id": 25, "seek": 13968, "start": 143.88, "end": 148.48000000000002, "text": " Dachstuhl in Germany. Our goal was to discuss the role of knowledge", "tokens": [50574, 413, 608, 372, 3232, 75, 294, 7244, 13, 2621, 3387, 390, 281, 2248, 264, 3090, 295, 3601, 50804], "temperature": 0.0, "avg_logprob": -0.16555617196219308, "compression_ratio": 1.6423357664233578, "no_speech_prob": 0.002018705941736698}, {"id": 26, "seek": 13968, "start": 148.48000000000002, "end": 152.84, "text": " engineering in the 21st century like we were almost under a kind of shock", "tokens": [50804, 7043, 294, 264, 5080, 372, 4901, 411, 321, 645, 1920, 833, 257, 733, 295, 5588, 51022], "temperature": 0.0, "avg_logprob": -0.16555617196219308, "compression_ratio": 1.6423357664233578, "no_speech_prob": 0.002018705941736698}, {"id": 27, "seek": 13968, "start": 152.84, "end": 156.92000000000002, "text": " talking about it processing and trying to figure out what it really means for", "tokens": [51022, 1417, 466, 309, 9007, 293, 1382, 281, 2573, 484, 437, 309, 534, 1355, 337, 51226], "temperature": 0.0, "avg_logprob": -0.16555617196219308, "compression_ratio": 1.6423357664233578, "no_speech_prob": 0.002018705941736698}, {"id": 28, "seek": 13968, "start": 156.92000000000002, "end": 162.36, "text": " us. I'm trying to distill some of the results from that seminar and also from", "tokens": [51226, 505, 13, 286, 478, 1382, 281, 42923, 512, 295, 264, 3542, 490, 300, 29235, 293, 611, 490, 51498], "temperature": 0.0, "avg_logprob": -0.16555617196219308, "compression_ratio": 1.6423357664233578, "no_speech_prob": 0.002018705941736698}, {"id": 29, "seek": 13968, "start": 162.36, "end": 167.04000000000002, "text": " any other conversations I had in this talk but you can also just go and read", "tokens": [51498, 604, 661, 7315, 286, 632, 294, 341, 751, 457, 291, 393, 611, 445, 352, 293, 1401, 51732], "temperature": 0.0, "avg_logprob": -0.16555617196219308, "compression_ratio": 1.6423357664233578, "no_speech_prob": 0.002018705941736698}, {"id": 30, "seek": 16704, "start": 167.04, "end": 175.07999999999998, "text": " the report directly. My second disclaimer nothing in this talk is generated by an AI.", "tokens": [50364, 264, 2275, 3838, 13, 1222, 1150, 40896, 1825, 294, 341, 751, 307, 10833, 538, 364, 7318, 13, 50766], "temperature": 0.0, "avg_logprob": -0.2249457041422526, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.0041982196271419525}, {"id": 31, "seek": 16704, "start": 175.07999999999998, "end": 180.68, "text": " And this explicitly marked so or you see a screenshot from an AI. This is not one", "tokens": [50766, 400, 341, 20803, 12658, 370, 420, 291, 536, 257, 27712, 490, 364, 7318, 13, 639, 307, 406, 472, 51046], "temperature": 0.0, "avg_logprob": -0.2249457041422526, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.0041982196271419525}, {"id": 32, "seek": 16704, "start": 180.68, "end": 184.68, "text": " of those talks where we have a reveal at the end that all of this was generated.", "tokens": [51046, 295, 729, 6686, 689, 321, 362, 257, 10658, 412, 264, 917, 300, 439, 295, 341, 390, 10833, 13, 51246], "temperature": 0.0, "avg_logprob": -0.2249457041422526, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.0041982196271419525}, {"id": 33, "seek": 16704, "start": 184.68, "end": 192.79999999999998, "text": " And finally the last disclaimer this is a focused talk it is only about knowledge", "tokens": [51246, 400, 2721, 264, 1036, 40896, 341, 307, 257, 5178, 751, 309, 307, 787, 466, 3601, 51652], "temperature": 0.0, "avg_logprob": -0.2249457041422526, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.0041982196271419525}, {"id": 34, "seek": 19280, "start": 192.8, "end": 198.08, "text": " graphs and large language models. It is not about the ethical questions of", "tokens": [50364, 24877, 293, 2416, 2856, 5245, 13, 467, 307, 406, 466, 264, 18890, 1651, 295, 50628], "temperature": 0.0, "avg_logprob": -0.18647599471242804, "compression_ratio": 1.8443396226415094, "no_speech_prob": 0.14215821027755737}, {"id": 35, "seek": 19280, "start": 198.08, "end": 202.96, "text": " large language models. It's not about blunders of current LLMs. It's not of the", "tokens": [50628, 2416, 2856, 5245, 13, 467, 311, 406, 466, 888, 997, 433, 295, 2190, 441, 43, 26386, 13, 467, 311, 406, 295, 264, 50872], "temperature": 0.0, "avg_logprob": -0.18647599471242804, "compression_ratio": 1.8443396226415094, "no_speech_prob": 0.14215821027755737}, {"id": 36, "seek": 19280, "start": 202.96, "end": 209.32000000000002, "text": " that those systems will have about the impact on jobs and the economy. It's not", "tokens": [50872, 300, 729, 3652, 486, 362, 466, 264, 2712, 322, 4782, 293, 264, 5010, 13, 467, 311, 406, 51190], "temperature": 0.0, "avg_logprob": -0.18647599471242804, "compression_ratio": 1.8443396226415094, "no_speech_prob": 0.14215821027755737}, {"id": 37, "seek": 19280, "start": 209.32000000000002, "end": 213.36, "text": " about questions of copyright or whether LLMs are a path to destruction of the", "tokens": [51190, 466, 1651, 295, 17996, 420, 1968, 441, 43, 26386, 366, 257, 3100, 281, 13563, 295, 264, 51392], "temperature": 0.0, "avg_logprob": -0.18647599471242804, "compression_ratio": 1.8443396226415094, "no_speech_prob": 0.14215821027755737}, {"id": 38, "seek": 19280, "start": 213.36, "end": 217.76000000000002, "text": " human race. All of these questions are interesting and deserve their own talks", "tokens": [51392, 1952, 4569, 13, 1057, 295, 613, 1651, 366, 1880, 293, 9948, 641, 1065, 6686, 51612], "temperature": 0.0, "avg_logprob": -0.18647599471242804, "compression_ratio": 1.8443396226415094, "no_speech_prob": 0.14215821027755737}, {"id": 39, "seek": 21776, "start": 217.76, "end": 224.84, "text": " and I will not talk about them today. So what makes me believe that there is a", "tokens": [50364, 293, 286, 486, 406, 751, 466, 552, 965, 13, 407, 437, 1669, 385, 1697, 300, 456, 307, 257, 50718], "temperature": 0.0, "avg_logprob": -0.17139365825247257, "compression_ratio": 1.6982758620689655, "no_speech_prob": 0.010985614731907845}, {"id": 40, "seek": 21776, "start": 224.84, "end": 229.84, "text": " future for knowledge graphs in the world of large language models? About six or", "tokens": [50718, 2027, 337, 3601, 24877, 294, 264, 1002, 295, 2416, 2856, 5245, 30, 7769, 2309, 420, 50968], "temperature": 0.0, "avg_logprob": -0.17139365825247257, "compression_ratio": 1.6982758620689655, "no_speech_prob": 0.010985614731907845}, {"id": 41, "seek": 21776, "start": 229.84, "end": 234.64, "text": " seven years ago I was working at Google on the knowledge graph. Back then the", "tokens": [50968, 3407, 924, 2057, 286, 390, 1364, 412, 3329, 322, 264, 3601, 4295, 13, 5833, 550, 264, 51208], "temperature": 0.0, "avg_logprob": -0.17139365825247257, "compression_ratio": 1.6982758620689655, "no_speech_prob": 0.010985614731907845}, {"id": 42, "seek": 21776, "start": 234.64, "end": 239.6, "text": " knowledge graph wasn't necessarily a generic term yet. It was a big question", "tokens": [51208, 3601, 4295, 2067, 380, 4725, 257, 19577, 1433, 1939, 13, 467, 390, 257, 955, 1168, 51456], "temperature": 0.0, "avg_logprob": -0.17139365825247257, "compression_ratio": 1.6982758620689655, "no_speech_prob": 0.010985614731907845}, {"id": 43, "seek": 21776, "start": 239.6, "end": 244.64, "text": " Google had to ask itself. The knowledge graph costs a lot of resources, a lot of", "tokens": [51456, 3329, 632, 281, 1029, 2564, 13, 440, 3601, 4295, 5497, 257, 688, 295, 3593, 11, 257, 688, 295, 51708], "temperature": 0.0, "avg_logprob": -0.17139365825247257, "compression_ratio": 1.6982758620689655, "no_speech_prob": 0.010985614731907845}, {"id": 44, "seek": 24464, "start": 244.64, "end": 249.2, "text": " money, a lot of manpower to run and maintain. A lot of people were working on", "tokens": [50364, 1460, 11, 257, 688, 295, 587, 9513, 281, 1190, 293, 6909, 13, 316, 688, 295, 561, 645, 1364, 322, 50592], "temperature": 0.0, "avg_logprob": -0.15920177258943258, "compression_ratio": 1.7216117216117217, "no_speech_prob": 0.003222239436581731}, {"id": 45, "seek": 24464, "start": 249.2, "end": 253.64, "text": " the knowledge graph. Is all of this work going to be obsolete? Should we stop", "tokens": [50592, 264, 3601, 4295, 13, 1119, 439, 295, 341, 589, 516, 281, 312, 46333, 30, 6454, 321, 1590, 50814], "temperature": 0.0, "avg_logprob": -0.15920177258943258, "compression_ratio": 1.7216117216117217, "no_speech_prob": 0.003222239436581731}, {"id": 46, "seek": 24464, "start": 253.64, "end": 259.03999999999996, "text": " investing in that? I give you the answer that I came up with back then. Let's", "tokens": [50814, 10978, 294, 300, 30, 286, 976, 291, 264, 1867, 300, 286, 1361, 493, 365, 646, 550, 13, 961, 311, 51084], "temperature": 0.0, "avg_logprob": -0.15920177258943258, "compression_ratio": 1.7216117216117217, "no_speech_prob": 0.003222239436581731}, {"id": 47, "seek": 24464, "start": 259.03999999999996, "end": 265.64, "text": " think first principles. So what is a knowledge graph? A knowledge graph represents", "tokens": [51084, 519, 700, 9156, 13, 407, 437, 307, 257, 3601, 4295, 30, 316, 3601, 4295, 8855, 51414], "temperature": 0.0, "avg_logprob": -0.15920177258943258, "compression_ratio": 1.7216117216117217, "no_speech_prob": 0.003222239436581731}, {"id": 48, "seek": 24464, "start": 265.64, "end": 270.4, "text": " things and the relations usually stored in a graph database. WikiData is an", "tokens": [51414, 721, 293, 264, 2299, 2673, 12187, 294, 257, 4295, 8149, 13, 35892, 35, 3274, 307, 364, 51652], "temperature": 0.0, "avg_logprob": -0.15920177258943258, "compression_ratio": 1.7216117216117217, "no_speech_prob": 0.003222239436581731}, {"id": 49, "seek": 24464, "start": 270.4, "end": 274.4, "text": " example. WikiData is the largest publicly available free knowledge graph that", "tokens": [51652, 1365, 13, 35892, 35, 3274, 307, 264, 6443, 14843, 2435, 1737, 3601, 4295, 300, 51852], "temperature": 0.0, "avg_logprob": -0.15920177258943258, "compression_ratio": 1.7216117216117217, "no_speech_prob": 0.003222239436581731}, {"id": 50, "seek": 27440, "start": 274.4, "end": 279.91999999999996, "text": " anyone can edit. What is a large language model? Model here means that we have a", "tokens": [50364, 2878, 393, 8129, 13, 708, 307, 257, 2416, 2856, 2316, 30, 17105, 510, 1355, 300, 321, 362, 257, 50640], "temperature": 0.0, "avg_logprob": -0.1454806601864168, "compression_ratio": 1.8767772511848342, "no_speech_prob": 0.002757393755018711}, {"id": 51, "seek": 27440, "start": 279.91999999999996, "end": 284.91999999999996, "text": " neural network, nodes connected with each other with weights. The weights in", "tokens": [50640, 18161, 3209, 11, 13891, 4582, 365, 1184, 661, 365, 17443, 13, 440, 17443, 294, 50890], "temperature": 0.0, "avg_logprob": -0.1454806601864168, "compression_ratio": 1.8767772511848342, "no_speech_prob": 0.002757393755018711}, {"id": 52, "seek": 27440, "start": 284.91999999999996, "end": 289.0, "text": " this neural networks are trained on some input and desired outputs in order to", "tokens": [50890, 341, 18161, 9590, 366, 8895, 322, 512, 4846, 293, 14721, 23930, 294, 1668, 281, 51094], "temperature": 0.0, "avg_logprob": -0.1454806601864168, "compression_ratio": 1.8767772511848342, "no_speech_prob": 0.002757393755018711}, {"id": 53, "seek": 27440, "start": 289.0, "end": 294.15999999999997, "text": " create good outputs on novel inputs. They are language models because they are", "tokens": [51094, 1884, 665, 23930, 322, 7613, 15743, 13, 814, 366, 2856, 5245, 570, 436, 366, 51352], "temperature": 0.0, "avg_logprob": -0.1454806601864168, "compression_ratio": 1.8767772511848342, "no_speech_prob": 0.002757393755018711}, {"id": 54, "seek": 27440, "start": 294.15999999999997, "end": 299.56, "text": " trained on natural language texts and it is a large language model because it is", "tokens": [51352, 8895, 322, 3303, 2856, 15765, 293, 309, 307, 257, 2416, 2856, 2316, 570, 309, 307, 51622], "temperature": 0.0, "avg_logprob": -0.1454806601864168, "compression_ratio": 1.8767772511848342, "no_speech_prob": 0.002757393755018711}, {"id": 55, "seek": 29956, "start": 299.56, "end": 304.96, "text": " trained on lots and lots of natural language texts. GPT is a family of large", "tokens": [50364, 8895, 322, 3195, 293, 3195, 295, 3303, 2856, 15765, 13, 26039, 51, 307, 257, 1605, 295, 2416, 50634], "temperature": 0.0, "avg_logprob": -0.15420924595424107, "compression_ratio": 1.6083333333333334, "no_speech_prob": 0.006589195691049099}, {"id": 56, "seek": 29956, "start": 304.96, "end": 310.08, "text": " language models which have been created by OpenAI. GPT-4 is the current version", "tokens": [50634, 2856, 5245, 597, 362, 668, 2942, 538, 7238, 48698, 13, 26039, 51, 12, 19, 307, 264, 2190, 3037, 50890], "temperature": 0.0, "avg_logprob": -0.15420924595424107, "compression_ratio": 1.6083333333333334, "no_speech_prob": 0.006589195691049099}, {"id": 57, "seek": 29956, "start": 310.08, "end": 313.88, "text": " but OpenAI has not published much data about it. So the following data is for", "tokens": [50890, 457, 7238, 48698, 575, 406, 6572, 709, 1412, 466, 309, 13, 407, 264, 3480, 1412, 307, 337, 51080], "temperature": 0.0, "avg_logprob": -0.15420924595424107, "compression_ratio": 1.6083333333333334, "no_speech_prob": 0.006589195691049099}, {"id": 58, "seek": 29956, "start": 313.88, "end": 322.4, "text": " GPT-3. GPT-3 has 96 such layers in the picture and it had 175 billion", "tokens": [51080, 26039, 51, 12, 18, 13, 26039, 51, 12, 18, 575, 24124, 1270, 7914, 294, 264, 3036, 293, 309, 632, 41165, 5218, 51506], "temperature": 0.0, "avg_logprob": -0.15420924595424107, "compression_ratio": 1.6083333333333334, "no_speech_prob": 0.006589195691049099}, {"id": 59, "seek": 29956, "start": 322.4, "end": 329.12, "text": " parameters. That is weights. As a large language model, GPT-3 has been trained to", "tokens": [51506, 9834, 13, 663, 307, 17443, 13, 1018, 257, 2416, 2856, 2316, 11, 26039, 51, 12, 18, 575, 668, 8895, 281, 51842], "temperature": 0.0, "avg_logprob": -0.15420924595424107, "compression_ratio": 1.6083333333333334, "no_speech_prob": 0.006589195691049099}, {"id": 60, "seek": 32912, "start": 329.12, "end": 334.92, "text": " provide appropriate answers to a given prompt to our input. So for example, given", "tokens": [50364, 2893, 6854, 6338, 281, 257, 2212, 12391, 281, 527, 4846, 13, 407, 337, 1365, 11, 2212, 50654], "temperature": 0.0, "avg_logprob": -0.21795281030798472, "compression_ratio": 1.551440329218107, "no_speech_prob": 0.0024723524693399668}, {"id": 61, "seek": 32912, "start": 334.92, "end": 338.64, "text": " the following input or prompt, who created the School of Athens.", "tokens": [50654, 264, 3480, 4846, 420, 12391, 11, 567, 2942, 264, 5070, 295, 32530, 13, 50840], "temperature": 0.0, "avg_logprob": -0.21795281030798472, "compression_ratio": 1.551440329218107, "no_speech_prob": 0.0024723524693399668}, {"id": 62, "seek": 32912, "start": 338.64, "end": 344.68, "text": " JGDP answers, the School of Athens is a famous fresco painted by the Italian", "tokens": [50840, 508, 38, 11373, 6338, 11, 264, 5070, 295, 32530, 307, 257, 4618, 25235, 1291, 11797, 538, 264, 10003, 51142], "temperature": 0.0, "avg_logprob": -0.21795281030798472, "compression_ratio": 1.551440329218107, "no_speech_prob": 0.0024723524693399668}, {"id": 63, "seek": 32912, "start": 344.68, "end": 350.52, "text": " Renaissance artist Raphael. It was painted between 1509 and 1511 as part of a", "tokens": [51142, 32642, 5748, 49690, 338, 13, 467, 390, 11797, 1296, 8451, 24, 293, 2119, 5348, 382, 644, 295, 257, 51434], "temperature": 0.0, "avg_logprob": -0.21795281030798472, "compression_ratio": 1.551440329218107, "no_speech_prob": 0.0024723524693399668}, {"id": 64, "seek": 32912, "start": 350.52, "end": 354.92, "text": " commission for the Vatican's Apostolic Palace in Rome. The fresco is widely", "tokens": [51434, 9221, 337, 264, 37904, 311, 31467, 7940, 19121, 294, 12043, 13, 440, 25235, 1291, 307, 13371, 51654], "temperature": 0.0, "avg_logprob": -0.21795281030798472, "compression_ratio": 1.551440329218107, "no_speech_prob": 0.0024723524693399668}, {"id": 65, "seek": 35492, "start": 354.92, "end": 361.32, "text": " regarded as one of the greatest works of art of the high Renaissance. This answer", "tokens": [50364, 26047, 382, 472, 295, 264, 6636, 1985, 295, 1523, 295, 264, 1090, 32642, 13, 639, 1867, 50684], "temperature": 0.0, "avg_logprob": -0.18294349519333036, "compression_ratio": 1.6518218623481782, "no_speech_prob": 0.01970713958144188}, {"id": 66, "seek": 35492, "start": 361.32, "end": 365.64000000000004, "text": " is not only correct, it is brilliant. There's absolutely nothing to complain", "tokens": [50684, 307, 406, 787, 3006, 11, 309, 307, 10248, 13, 821, 311, 3122, 1825, 281, 11024, 50900], "temperature": 0.0, "avg_logprob": -0.18294349519333036, "compression_ratio": 1.6518218623481782, "no_speech_prob": 0.01970713958144188}, {"id": 67, "seek": 35492, "start": 365.64000000000004, "end": 372.56, "text": " about. It gives context, it answers the question. It's great. If you go to Google", "tokens": [50900, 466, 13, 467, 2709, 4319, 11, 309, 6338, 264, 1168, 13, 467, 311, 869, 13, 759, 291, 352, 281, 3329, 51246], "temperature": 0.0, "avg_logprob": -0.18294349519333036, "compression_ratio": 1.6518218623481782, "no_speech_prob": 0.01970713958144188}, {"id": 68, "seek": 35492, "start": 372.56, "end": 377.20000000000005, "text": " and ask the same question, the answer is the same, Raphael. But the context is even", "tokens": [51246, 293, 1029, 264, 912, 1168, 11, 264, 1867, 307, 264, 912, 11, 49690, 338, 13, 583, 264, 4319, 307, 754, 51478], "temperature": 0.0, "avg_logprob": -0.18294349519333036, "compression_ratio": 1.6518218623481782, "no_speech_prob": 0.01970713958144188}, {"id": 69, "seek": 35492, "start": 377.20000000000005, "end": 384.24, "text": " more amazing. A picture from Raphael, his biography, data about him, who influenced", "tokens": [51478, 544, 2243, 13, 316, 3036, 490, 49690, 338, 11, 702, 37062, 11, 1412, 466, 796, 11, 567, 15269, 51830], "temperature": 0.0, "avg_logprob": -0.18294349519333036, "compression_ratio": 1.6518218623481782, "no_speech_prob": 0.01970713958144188}, {"id": 70, "seek": 38424, "start": 384.24, "end": 389.04, "text": " him, other artists that are being searched for. The answer is just beautiful and", "tokens": [50364, 796, 11, 661, 6910, 300, 366, 885, 22961, 337, 13, 440, 1867, 307, 445, 2238, 293, 50604], "temperature": 0.0, "avg_logprob": -0.1935603752564848, "compression_ratio": 1.4336283185840708, "no_speech_prob": 0.002714412519708276}, {"id": 71, "seek": 38424, "start": 389.04, "end": 395.32, "text": " amazing. You can also go to Wikidata and write a sparkle query, asking for the", "tokens": [50604, 2243, 13, 509, 393, 611, 352, 281, 23377, 327, 3274, 293, 2464, 257, 48558, 14581, 11, 3365, 337, 264, 50918], "temperature": 0.0, "avg_logprob": -0.1935603752564848, "compression_ratio": 1.4336283185840708, "no_speech_prob": 0.002714412519708276}, {"id": 72, "seek": 38424, "start": 395.32, "end": 401.76, "text": " creator of the School of Athens. Again, you will get the result, Raphael. Not as", "tokens": [50918, 14181, 295, 264, 5070, 295, 32530, 13, 3764, 11, 291, 486, 483, 264, 1874, 11, 49690, 338, 13, 1726, 382, 51240], "temperature": 0.0, "avg_logprob": -0.1935603752564848, "compression_ratio": 1.4336283185840708, "no_speech_prob": 0.002714412519708276}, {"id": 73, "seek": 38424, "start": 401.76, "end": 411.48, "text": " pretty as Google, not as much context as with JGDP. But here's one thing, JGDP took", "tokens": [51240, 1238, 382, 3329, 11, 406, 382, 709, 4319, 382, 365, 508, 38, 11373, 13, 583, 510, 311, 472, 551, 11, 508, 38, 11373, 1890, 51726], "temperature": 0.0, "avg_logprob": -0.1935603752564848, "compression_ratio": 1.4336283185840708, "no_speech_prob": 0.002714412519708276}, {"id": 74, "seek": 41148, "start": 411.56, "end": 416.72, "text": " about five seconds to answer my question. Admittedly, it gave a lot of context I", "tokens": [50368, 466, 1732, 3949, 281, 1867, 452, 1168, 13, 46292, 3944, 356, 11, 309, 2729, 257, 688, 295, 4319, 286, 50626], "temperature": 0.0, "avg_logprob": -0.1262811118481206, "compression_ratio": 1.728448275862069, "no_speech_prob": 0.00609711604192853}, {"id": 75, "seek": 41148, "start": 416.72, "end": 422.72, "text": " didn't ask for, but even just passing the question took close to a second. Google", "tokens": [50626, 994, 380, 1029, 337, 11, 457, 754, 445, 8437, 264, 1168, 1890, 1998, 281, 257, 1150, 13, 3329, 50926], "temperature": 0.0, "avg_logprob": -0.1262811118481206, "compression_ratio": 1.728448275862069, "no_speech_prob": 0.00609711604192853}, {"id": 76, "seek": 41148, "start": 422.72, "end": 427.20000000000005, "text": " took half a second, including passing, but it also got a lot of great context and", "tokens": [50926, 1890, 1922, 257, 1150, 11, 3009, 8437, 11, 457, 309, 611, 658, 257, 688, 295, 869, 4319, 293, 51150], "temperature": 0.0, "avg_logprob": -0.1262811118481206, "compression_ratio": 1.728448275862069, "no_speech_prob": 0.00609711604192853}, {"id": 77, "seek": 41148, "start": 427.20000000000005, "end": 429.96000000000004, "text": " gave me results from the search index, even though they didn't make it to a", "tokens": [51150, 2729, 385, 3542, 490, 264, 3164, 8186, 11, 754, 1673, 436, 994, 380, 652, 309, 281, 257, 51288], "temperature": 0.0, "avg_logprob": -0.1262811118481206, "compression_ratio": 1.728448275862069, "no_speech_prob": 0.00609711604192853}, {"id": 78, "seek": 41148, "start": 429.96000000000004, "end": 436.36, "text": " screenshot. Wikidata took about a quarter of a second to produce the answer, and", "tokens": [51288, 27712, 13, 23377, 327, 3274, 1890, 466, 257, 6555, 295, 257, 1150, 281, 5258, 264, 1867, 11, 293, 51608], "temperature": 0.0, "avg_logprob": -0.1262811118481206, "compression_ratio": 1.728448275862069, "no_speech_prob": 0.00609711604192853}, {"id": 79, "seek": 43636, "start": 436.36, "end": 441.8, "text": " that's not all. JGDP is running on the fastest and most expensive hardware you", "tokens": [50364, 300, 311, 406, 439, 13, 508, 38, 11373, 307, 2614, 322, 264, 14573, 293, 881, 5124, 8837, 291, 50636], "temperature": 0.0, "avg_logprob": -0.11467157665051912, "compression_ratio": 1.7695852534562213, "no_speech_prob": 0.0012448078487068415}, {"id": 80, "seek": 43636, "start": 441.8, "end": 446.76, "text": " can buy. Google is running on the fastest and most expensive hardware you", "tokens": [50636, 393, 2256, 13, 3329, 307, 2614, 322, 264, 14573, 293, 881, 5124, 8837, 291, 50884], "temperature": 0.0, "avg_logprob": -0.11467157665051912, "compression_ratio": 1.7695852534562213, "no_speech_prob": 0.0012448078487068415}, {"id": 81, "seek": 43636, "start": 446.76, "end": 452.04, "text": " cannot buy. And the Wikidata query service is running abandonware on a", "tokens": [50884, 2644, 2256, 13, 400, 264, 23377, 327, 3274, 14581, 2643, 307, 2614, 9072, 3039, 322, 257, 51148], "temperature": 0.0, "avg_logprob": -0.11467157665051912, "compression_ratio": 1.7695852534562213, "no_speech_prob": 0.0012448078487068415}, {"id": 82, "seek": 43636, "start": 452.04, "end": 459.0, "text": " server somewhere. And if you think about this, it is not surprising. The query I", "tokens": [51148, 7154, 4079, 13, 400, 498, 291, 519, 466, 341, 11, 309, 307, 406, 8830, 13, 440, 14581, 286, 51496], "temperature": 0.0, "avg_logprob": -0.11467157665051912, "compression_ratio": 1.7695852534562213, "no_speech_prob": 0.0012448078487068415}, {"id": 83, "seek": 43636, "start": 459.0, "end": 463.56, "text": " asked had six tokens of input. A token is roughly a word for a natural language", "tokens": [51496, 2351, 632, 2309, 22667, 295, 4846, 13, 316, 14862, 307, 9810, 257, 1349, 337, 257, 3303, 2856, 51724], "temperature": 0.0, "avg_logprob": -0.11467157665051912, "compression_ratio": 1.7695852534562213, "no_speech_prob": 0.0012448078487068415}, {"id": 84, "seek": 46356, "start": 463.56, "end": 468.6, "text": " model. And the answer produced 60 tokens. Now, if the answer would just have been", "tokens": [50364, 2316, 13, 400, 264, 1867, 7126, 4060, 22667, 13, 823, 11, 498, 264, 1867, 576, 445, 362, 668, 50616], "temperature": 0.0, "avg_logprob": -0.12373089322856828, "compression_ratio": 1.546875, "no_speech_prob": 0.002714762929826975}, {"id": 85, "seek": 46356, "start": 468.6, "end": 473.4, "text": " Raphael, it would still have been at least two tokens. Every single token runs for", "tokens": [50616, 49690, 338, 11, 309, 576, 920, 362, 668, 412, 1935, 732, 22667, 13, 2048, 2167, 14862, 6676, 337, 50856], "temperature": 0.0, "avg_logprob": -0.12373089322856828, "compression_ratio": 1.546875, "no_speech_prob": 0.002714762929826975}, {"id": 86, "seek": 46356, "start": 473.4, "end": 480.12, "text": " the 96 layers of JGDP, branching out to 175 billion weights and multiplying", "tokens": [50856, 264, 24124, 7914, 295, 508, 38, 11373, 11, 9819, 278, 484, 281, 41165, 5218, 17443, 293, 30955, 51192], "temperature": 0.0, "avg_logprob": -0.12373089322856828, "compression_ratio": 1.546875, "no_speech_prob": 0.002714762929826975}, {"id": 87, "seek": 46356, "start": 480.12, "end": 486.84000000000003, "text": " matrices and soft maximum results. Whereas in Wikidata, we look up an item out", "tokens": [51192, 32284, 293, 2787, 6674, 3542, 13, 13813, 294, 23377, 327, 3274, 11, 321, 574, 493, 364, 3174, 484, 51528], "temperature": 0.0, "avg_logprob": -0.12373089322856828, "compression_ratio": 1.546875, "no_speech_prob": 0.002714762929826975}, {"id": 88, "seek": 46356, "start": 486.84000000000003, "end": 491.72, "text": " of 100 million and find a key out of 10,000 to get the results values. Those", "tokens": [51528, 295, 2319, 2459, 293, 915, 257, 2141, 484, 295, 1266, 11, 1360, 281, 483, 264, 3542, 4190, 13, 3950, 51772], "temperature": 0.0, "avg_logprob": -0.12373089322856828, "compression_ratio": 1.546875, "no_speech_prob": 0.002714762929826975}, {"id": 89, "seek": 49172, "start": 491.72, "end": 498.04, "text": " are both logarithmic operations. It's just much cheaper. Given this paper,", "tokens": [50364, 366, 1293, 41473, 355, 13195, 7705, 13, 467, 311, 445, 709, 12284, 13, 18600, 341, 3035, 11, 50680], "temperature": 0.0, "avg_logprob": -0.11009522584768441, "compression_ratio": 1.5447470817120623, "no_speech_prob": 0.0005883948761038482}, {"id": 90, "seek": 49172, "start": 498.04, "end": 502.92, "text": " getting and hosting your own copy of Wikidata, 1,000 Wikidata queries, like", "tokens": [50680, 1242, 293, 16058, 428, 1065, 5055, 295, 23377, 327, 3274, 11, 502, 11, 1360, 23377, 327, 3274, 24109, 11, 411, 50924], "temperature": 0.0, "avg_logprob": -0.11009522584768441, "compression_ratio": 1.5447470817120623, "no_speech_prob": 0.0005883948761038482}, {"id": 91, "seek": 49172, "start": 502.92, "end": 509.24, "text": " the ones I just asked, would cost me about 14 cents in the cloud. The authors thought", "tokens": [50924, 264, 2306, 286, 445, 2351, 11, 576, 2063, 385, 466, 3499, 14941, 294, 264, 4588, 13, 440, 16552, 1194, 51240], "temperature": 0.0, "avg_logprob": -0.11009522584768441, "compression_ratio": 1.5447470817120623, "no_speech_prob": 0.0005883948761038482}, {"id": 92, "seek": 49172, "start": 509.24, "end": 512.6800000000001, "text": " this was too expensive and bought their own hardware to make it even cheaper.", "tokens": [51240, 341, 390, 886, 5124, 293, 4243, 641, 1065, 8837, 281, 652, 309, 754, 12284, 13, 51412], "temperature": 0.0, "avg_logprob": -0.11009522584768441, "compression_ratio": 1.5447470817120623, "no_speech_prob": 0.0005883948761038482}, {"id": 93, "seek": 49172, "start": 512.6800000000001, "end": 518.76, "text": " Whereas if I look at Azure pricing for GPT-4, a thousand queries, as asked before,", "tokens": [51412, 13813, 498, 286, 574, 412, 11969, 17621, 337, 26039, 51, 12, 19, 11, 257, 4714, 24109, 11, 382, 2351, 949, 11, 51716], "temperature": 0.0, "avg_logprob": -0.11009522584768441, "compression_ratio": 1.5447470817120623, "no_speech_prob": 0.0005883948761038482}, {"id": 94, "seek": 51876, "start": 518.76, "end": 528.4399999999999, "text": " would cost $7.56. So that's 14 cents compared to more than $7, so a factor of 50.", "tokens": [50364, 576, 2063, 1848, 22, 13, 18317, 13, 407, 300, 311, 3499, 14941, 5347, 281, 544, 813, 1848, 22, 11, 370, 257, 5952, 295, 2625, 13, 50848], "temperature": 0.0, "avg_logprob": -0.1508745890791698, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.007814974524080753}, {"id": 95, "seek": 51876, "start": 528.4399999999999, "end": 534.6, "text": " 50 times more expense can make a difference. Now, to make it very clear, I don't know", "tokens": [50848, 2625, 1413, 544, 18406, 393, 652, 257, 2649, 13, 823, 11, 281, 652, 309, 588, 1850, 11, 286, 500, 380, 458, 51156], "temperature": 0.0, "avg_logprob": -0.1508745890791698, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.007814974524080753}, {"id": 96, "seek": 51876, "start": 534.6, "end": 538.68, "text": " how robust these numbers are. I would love to see more robust numbers and I expect", "tokens": [51156, 577, 13956, 613, 3547, 366, 13, 286, 576, 959, 281, 536, 544, 13956, 3547, 293, 286, 2066, 51360], "temperature": 0.0, "avg_logprob": -0.1508745890791698, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.007814974524080753}, {"id": 97, "seek": 51876, "start": 538.68, "end": 544.2, "text": " the cost for inference to go down. But even Sam Altman, the CEO of OpenAI,", "tokens": [51360, 264, 2063, 337, 38253, 281, 352, 760, 13, 583, 754, 4832, 15992, 1601, 11, 264, 9282, 295, 7238, 48698, 11, 51636], "temperature": 0.0, "avg_logprob": -0.1508745890791698, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.007814974524080753}, {"id": 98, "seek": 54420, "start": 544.2, "end": 549.72, "text": " the company making JetGDP, describes the compute cost of JetGDP as eye-watering.", "tokens": [50364, 264, 2237, 1455, 28730, 38, 11373, 11, 15626, 264, 14722, 2063, 295, 28730, 38, 11373, 382, 3313, 12, 8002, 278, 13, 50640], "temperature": 0.0, "avg_logprob": -0.12642771226388436, "compression_ratio": 1.5888888888888888, "no_speech_prob": 0.005469026509672403}, {"id": 99, "seek": 54420, "start": 550.76, "end": 554.6, "text": " And don't forget, he made a really good deal with Microsoft about running it on Azure.", "tokens": [50692, 400, 500, 380, 2870, 11, 415, 1027, 257, 534, 665, 2028, 365, 8116, 466, 2614, 309, 322, 11969, 13, 50884], "temperature": 0.0, "avg_logprob": -0.12642771226388436, "compression_ratio": 1.5888888888888888, "no_speech_prob": 0.005469026509672403}, {"id": 100, "seek": 54420, "start": 555.88, "end": 560.0400000000001, "text": " John Hennessey, chairman of Google and former computer science professor at Stanford,", "tokens": [50948, 2619, 389, 1857, 7357, 88, 11, 22770, 295, 3329, 293, 5819, 3820, 3497, 8304, 412, 20374, 11, 51156], "temperature": 0.0, "avg_logprob": -0.12642771226388436, "compression_ratio": 1.5888888888888888, "no_speech_prob": 0.005469026509672403}, {"id": 101, "seek": 54420, "start": 560.0400000000001, "end": 565.08, "text": " so he really knows what he's talking about, said that running at JetGDP, like Google,", "tokens": [51156, 370, 415, 534, 3255, 437, 415, 311, 1417, 466, 11, 848, 300, 2614, 412, 28730, 38, 11373, 11, 411, 3329, 11, 51408], "temperature": 0.0, "avg_logprob": -0.12642771226388436, "compression_ratio": 1.5888888888888888, "no_speech_prob": 0.005469026509672403}, {"id": 102, "seek": 54420, "start": 565.08, "end": 571.8000000000001, "text": " would be 10 times more expensive. And that's just talking about the inference costs. Each", "tokens": [51408, 576, 312, 1266, 1413, 544, 5124, 13, 400, 300, 311, 445, 1417, 466, 264, 38253, 5497, 13, 6947, 51744], "temperature": 0.0, "avg_logprob": -0.12642771226388436, "compression_ratio": 1.5888888888888888, "no_speech_prob": 0.005469026509672403}, {"id": 103, "seek": 57180, "start": 571.8, "end": 577.3199999999999, "text": " of these models also need to be trained. And current state-of-the-art language models cost", "tokens": [50364, 295, 613, 5245, 611, 643, 281, 312, 8895, 13, 400, 2190, 1785, 12, 2670, 12, 3322, 12, 446, 2856, 5245, 2063, 50640], "temperature": 0.0, "avg_logprob": -0.09620349605878194, "compression_ratio": 1.5474137931034482, "no_speech_prob": 0.0045381588861346245}, {"id": 104, "seek": 57180, "start": 577.3199999999999, "end": 584.5999999999999, "text": " millions of dollars to train. For GPT-4, the cost was given as more than $100 million.", "tokens": [50640, 6803, 295, 3808, 281, 3847, 13, 1171, 26039, 51, 12, 19, 11, 264, 2063, 390, 2212, 382, 544, 813, 1848, 6879, 2459, 13, 51004], "temperature": 0.0, "avg_logprob": -0.09620349605878194, "compression_ratio": 1.5474137931034482, "no_speech_prob": 0.0045381588861346245}, {"id": 105, "seek": 57180, "start": 586.5999999999999, "end": 593.24, "text": " The cost for GPT-3 was around $4 million. Good news is, you probably don't have to do this", "tokens": [51104, 440, 2063, 337, 26039, 51, 12, 18, 390, 926, 1848, 19, 2459, 13, 2205, 2583, 307, 11, 291, 1391, 500, 380, 362, 281, 360, 341, 51436], "temperature": 0.0, "avg_logprob": -0.09620349605878194, "compression_ratio": 1.5474137931034482, "no_speech_prob": 0.0045381588861346245}, {"id": 106, "seek": 57180, "start": 593.24, "end": 598.68, "text": " training. But you can just hopefully reuse an existing open source model that you can find", "tokens": [51436, 3097, 13, 583, 291, 393, 445, 4696, 26225, 364, 6741, 1269, 4009, 2316, 300, 291, 393, 915, 51708], "temperature": 0.0, "avg_logprob": -0.09620349605878194, "compression_ratio": 1.5474137931034482, "no_speech_prob": 0.0045381588861346245}, {"id": 107, "seek": 59868, "start": 598.68, "end": 604.1999999999999, "text": " due to your task. This will be considerably cheaper to train a model from the foundation on.", "tokens": [50364, 3462, 281, 428, 5633, 13, 639, 486, 312, 31308, 12284, 281, 3847, 257, 2316, 490, 264, 7030, 322, 13, 50640], "temperature": 0.0, "avg_logprob": -0.11837318708311836, "compression_ratio": 1.5578947368421052, "no_speech_prob": 0.005641411989927292}, {"id": 108, "seek": 59868, "start": 604.92, "end": 608.68, "text": " But not for inference. The same cost that we saw earlier remains.", "tokens": [50676, 583, 406, 337, 38253, 13, 440, 912, 2063, 300, 321, 1866, 3071, 7023, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11837318708311836, "compression_ratio": 1.5578947368421052, "no_speech_prob": 0.005641411989927292}, {"id": 109, "seek": 59868, "start": 610.92, "end": 615.8, "text": " Here's some interesting new thoughts. Some folks, like Sam Altman, think that the age of", "tokens": [50976, 1692, 311, 512, 1880, 777, 4598, 13, 2188, 4024, 11, 411, 4832, 15992, 1601, 11, 519, 300, 264, 3205, 295, 51220], "temperature": 0.0, "avg_logprob": -0.11837318708311836, "compression_ratio": 1.5578947368421052, "no_speech_prob": 0.005641411989927292}, {"id": 110, "seek": 59868, "start": 615.8, "end": 621.7199999999999, "text": " large language models is already over. OpenAI says it's because of diminishing returns of further", "tokens": [51220, 2416, 2856, 5245, 307, 1217, 670, 13, 7238, 48698, 1619, 309, 311, 570, 295, 15739, 3807, 11247, 295, 3052, 51516], "temperature": 0.0, "avg_logprob": -0.11837318708311836, "compression_ratio": 1.5578947368421052, "no_speech_prob": 0.005641411989927292}, {"id": 111, "seek": 59868, "start": 621.7199999999999, "end": 626.68, "text": " reading. Guess what? After reading a million books worth of text, you don't seem to learn too much", "tokens": [51516, 3760, 13, 17795, 437, 30, 2381, 3760, 257, 2459, 3642, 3163, 295, 2487, 11, 291, 500, 380, 1643, 281, 1466, 886, 709, 51764], "temperature": 0.0, "avg_logprob": -0.11837318708311836, "compression_ratio": 1.5578947368421052, "no_speech_prob": 0.005641411989927292}, {"id": 112, "seek": 62668, "start": 626.68, "end": 634.8399999999999, "text": " new stuff. He said that OpenAI is not working on GPT-5, but that they want to explore new ideas.", "tokens": [50364, 777, 1507, 13, 634, 848, 300, 7238, 48698, 307, 406, 1364, 322, 26039, 51, 12, 20, 11, 457, 300, 436, 528, 281, 6839, 777, 3487, 13, 50772], "temperature": 0.0, "avg_logprob": -0.090852954600117, "compression_ratio": 1.5058823529411764, "no_speech_prob": 0.005554581992328167}, {"id": 113, "seek": 62668, "start": 634.8399999999999, "end": 639.88, "text": " And that's great. Because with models that are more readily available out there, such as Metas", "tokens": [50772, 400, 300, 311, 869, 13, 1436, 365, 5245, 300, 366, 544, 26336, 2435, 484, 456, 11, 1270, 382, 6377, 296, 51024], "temperature": 0.0, "avg_logprob": -0.090852954600117, "compression_ratio": 1.5058823529411764, "no_speech_prob": 0.005554581992328167}, {"id": 114, "seek": 62668, "start": 639.88, "end": 647.0, "text": " Llama, we can see new ideas happening very fast. For example, when Llama was leaked within days,", "tokens": [51024, 32717, 2404, 11, 321, 393, 536, 777, 3487, 2737, 588, 2370, 13, 1171, 1365, 11, 562, 32717, 2404, 390, 31779, 1951, 1708, 11, 51380], "temperature": 0.0, "avg_logprob": -0.090852954600117, "compression_ratio": 1.5058823529411764, "no_speech_prob": 0.005554581992328167}, {"id": 115, "seek": 62668, "start": 647.0, "end": 652.8399999999999, "text": " someone managed to run it on consumer hardware. A few days later, we got it running on a phone,", "tokens": [51380, 1580, 6453, 281, 1190, 309, 322, 9711, 8837, 13, 316, 1326, 1708, 1780, 11, 321, 658, 309, 2614, 322, 257, 2593, 11, 51672], "temperature": 0.0, "avg_logprob": -0.090852954600117, "compression_ratio": 1.5058823529411764, "no_speech_prob": 0.005554581992328167}, {"id": 116, "seek": 65284, "start": 652.84, "end": 660.12, "text": " and then a Raspberry Pi. It was very slow, but it ran. And by the way, as ridiculous as that sounds,", "tokens": [50364, 293, 550, 257, 41154, 17741, 13, 467, 390, 588, 2964, 11, 457, 309, 5872, 13, 400, 538, 264, 636, 11, 382, 11083, 382, 300, 3263, 11, 50728], "temperature": 0.0, "avg_logprob": -0.08155048939219692, "compression_ratio": 1.5774647887323943, "no_speech_prob": 0.0006070637609809637}, {"id": 117, "seek": 65284, "start": 660.12, "end": 664.0400000000001, "text": " this still means that these giant models are more expensive to run than a knowledge graph.", "tokens": [50728, 341, 920, 1355, 300, 613, 7410, 5245, 366, 544, 5124, 281, 1190, 813, 257, 3601, 4295, 13, 50924], "temperature": 0.0, "avg_logprob": -0.08155048939219692, "compression_ratio": 1.5774647887323943, "no_speech_prob": 0.0006070637609809637}, {"id": 118, "seek": 65284, "start": 664.84, "end": 669.1600000000001, "text": " And really, it's not just cost, right? There are a few challenges that large", "tokens": [50964, 400, 534, 11, 309, 311, 406, 445, 2063, 11, 558, 30, 821, 366, 257, 1326, 4759, 300, 2416, 51180], "temperature": 0.0, "avg_logprob": -0.08155048939219692, "compression_ratio": 1.5774647887323943, "no_speech_prob": 0.0006070637609809637}, {"id": 119, "seek": 65284, "start": 669.1600000000001, "end": 673.8000000000001, "text": " language models need to overcome. We need new ideas for those. Let's take a look.", "tokens": [51180, 2856, 5245, 643, 281, 10473, 13, 492, 643, 777, 3487, 337, 729, 13, 961, 311, 747, 257, 574, 13, 51412], "temperature": 0.0, "avg_logprob": -0.08155048939219692, "compression_ratio": 1.5774647887323943, "no_speech_prob": 0.0006070637609809637}, {"id": 120, "seek": 65284, "start": 675.72, "end": 681.72, "text": " Here's one thing that surprised me a lot. A few weeks ago, I stumbled into one of those Wikipedia", "tokens": [51508, 1692, 311, 472, 551, 300, 6100, 385, 257, 688, 13, 316, 1326, 3259, 2057, 11, 286, 36668, 666, 472, 295, 729, 28999, 51808], "temperature": 0.0, "avg_logprob": -0.08155048939219692, "compression_ratio": 1.5774647887323943, "no_speech_prob": 0.0006070637609809637}, {"id": 121, "seek": 68172, "start": 681.72, "end": 688.12, "text": " rabbit holes where I got temporarily obsessed with figuring out one specific fact. The correct", "tokens": [50364, 19509, 8118, 689, 286, 658, 23750, 16923, 365, 15213, 484, 472, 2685, 1186, 13, 440, 3006, 50684], "temperature": 0.0, "avg_logprob": -0.12460667292277018, "compression_ratio": 1.5269709543568464, "no_speech_prob": 0.0027574363630264997}, {"id": 122, "seek": 68172, "start": 688.12, "end": 695.1600000000001, "text": " place of birth of a not-too-well-known Croatian actress, Anna Begovic. I was surprised that", "tokens": [50684, 1081, 295, 3965, 295, 257, 406, 12, 32599, 12, 6326, 12, 6861, 37614, 952, 15410, 11, 12899, 879, 16089, 299, 13, 286, 390, 6100, 300, 51036], "temperature": 0.0, "avg_logprob": -0.12460667292277018, "compression_ratio": 1.5269709543568464, "no_speech_prob": 0.0027574363630264997}, {"id": 123, "seek": 68172, "start": 695.1600000000001, "end": 700.12, "text": " Google and Wikipedia Wikidata were giving different answers, so I tried to figure out", "tokens": [51036, 3329, 293, 28999, 23377, 327, 3274, 645, 2902, 819, 6338, 11, 370, 286, 3031, 281, 2573, 484, 51284], "temperature": 0.0, "avg_logprob": -0.12460667292277018, "compression_ratio": 1.5269709543568464, "no_speech_prob": 0.0027574363630264997}, {"id": 124, "seek": 68172, "start": 700.12, "end": 706.9200000000001, "text": " the truth and fix it. Here's a screenshot of Google answering the question. Begovic was born in", "tokens": [51284, 264, 3494, 293, 3191, 309, 13, 1692, 311, 257, 27712, 295, 3329, 13430, 264, 1168, 13, 879, 16089, 299, 390, 4232, 294, 51624], "temperature": 0.0, "avg_logprob": -0.12460667292277018, "compression_ratio": 1.5269709543568464, "no_speech_prob": 0.0027574363630264997}, {"id": 125, "seek": 70692, "start": 706.92, "end": 713.9599999999999, "text": " Terpan. Wikipedia used to say split. After a bit of hunting through sources, I figured out that it", "tokens": [50364, 6564, 6040, 13, 28999, 1143, 281, 584, 7472, 13, 2381, 257, 857, 295, 12599, 807, 7139, 11, 286, 8932, 484, 300, 309, 50716], "temperature": 0.0, "avg_logprob": -0.12978617350260416, "compression_ratio": 1.5885416666666667, "no_speech_prob": 0.0012253817403689027}, {"id": 126, "seek": 70692, "start": 713.9599999999999, "end": 721.7199999999999, "text": " mostly likely is Terpan, though. And that a lot of the sources were copying from Wikipedia and Wikidata", "tokens": [50716, 5240, 3700, 307, 6564, 6040, 11, 1673, 13, 400, 300, 257, 688, 295, 264, 7139, 645, 27976, 490, 28999, 293, 23377, 327, 3274, 51104], "temperature": 0.0, "avg_logprob": -0.12978617350260416, "compression_ratio": 1.5885416666666667, "no_speech_prob": 0.0012253817403689027}, {"id": 127, "seek": 70692, "start": 721.7199999999999, "end": 728.36, "text": " and became contaminated by Wikipedia and Wikidata. This is an example of our knowledge ecosystem being", "tokens": [51104, 293, 3062, 34492, 538, 28999, 293, 23377, 327, 3274, 13, 639, 307, 364, 1365, 295, 527, 3601, 11311, 885, 51436], "temperature": 0.0, "avg_logprob": -0.12978617350260416, "compression_ratio": 1.5885416666666667, "no_speech_prob": 0.0012253817403689027}, {"id": 128, "seek": 72836, "start": 728.44, "end": 733.8000000000001, "text": " substantial danger, by the way, and that long before we have LLM syndemics.", "tokens": [50368, 16726, 4330, 11, 538, 264, 636, 11, 293, 300, 938, 949, 321, 362, 441, 43, 44, 15198, 38014, 13, 50636], "temperature": 0.0, "avg_logprob": -0.15851550741293996, "compression_ratio": 1.4852320675105486, "no_speech_prob": 0.013427249155938625}, {"id": 129, "seek": 72836, "start": 735.88, "end": 742.36, "text": " Now, if you go to Bing Chat, which is powered by OpenAI's GPT, but has access to web search", "tokens": [50740, 823, 11, 498, 291, 352, 281, 30755, 27503, 11, 597, 307, 17786, 538, 7238, 48698, 311, 26039, 51, 11, 457, 575, 2105, 281, 3670, 3164, 51064], "temperature": 0.0, "avg_logprob": -0.15851550741293996, "compression_ratio": 1.4852320675105486, "no_speech_prob": 0.013427249155938625}, {"id": 130, "seek": 72836, "start": 743.08, "end": 748.76, "text": " and ask for the place of birth of Anna Begovic, it also answers me Terpan, which is great,", "tokens": [51100, 293, 1029, 337, 264, 1081, 295, 3965, 295, 12899, 879, 16089, 299, 11, 309, 611, 6338, 385, 6564, 6040, 11, 597, 307, 869, 11, 51384], "temperature": 0.0, "avg_logprob": -0.15851550741293996, "compression_ratio": 1.4852320675105486, "no_speech_prob": 0.013427249155938625}, {"id": 131, "seek": 72836, "start": 748.76, "end": 754.44, "text": " and it gives me free sources for that answer. It's just very disappointing that if you follow", "tokens": [51384, 293, 309, 2709, 385, 1737, 7139, 337, 300, 1867, 13, 467, 311, 445, 588, 25054, 300, 498, 291, 1524, 51668], "temperature": 0.0, "avg_logprob": -0.15851550741293996, "compression_ratio": 1.4852320675105486, "no_speech_prob": 0.013427249155938625}, {"id": 132, "seek": 75444, "start": 754.44, "end": 759.08, "text": " one of the references to Wikipedia, you will actually find it says split.", "tokens": [50364, 472, 295, 264, 15400, 281, 28999, 11, 291, 486, 767, 915, 309, 1619, 7472, 13, 50596], "temperature": 0.0, "avg_logprob": -0.12968141183085824, "compression_ratio": 1.5401785714285714, "no_speech_prob": 0.0005274623399600387}, {"id": 133, "seek": 75444, "start": 760.2, "end": 765.48, "text": " When I asked ChatGDP instead of Bing Chat, I get a different answer. It answers split.", "tokens": [50652, 1133, 286, 2351, 27503, 38, 11373, 2602, 295, 30755, 27503, 11, 286, 483, 257, 819, 1867, 13, 467, 6338, 7472, 13, 50916], "temperature": 0.0, "avg_logprob": -0.12968141183085824, "compression_ratio": 1.5401785714285714, "no_speech_prob": 0.0005274623399600387}, {"id": 134, "seek": 75444, "start": 766.2, "end": 771.5600000000001, "text": " This is not too surprising. After all, Wikipedia was claiming split until just a few weeks ago,", "tokens": [50952, 639, 307, 406, 886, 8830, 13, 2381, 439, 11, 28999, 390, 19232, 7472, 1826, 445, 257, 1326, 3259, 2057, 11, 51220], "temperature": 0.0, "avg_logprob": -0.12968141183085824, "compression_ratio": 1.5401785714285714, "no_speech_prob": 0.0005274623399600387}, {"id": 135, "seek": 75444, "start": 771.5600000000001, "end": 777.72, "text": " and ChatGDP was trained on a 2021 copy of the web. Corrections to Wikipedia done in 2023", "tokens": [51220, 293, 27503, 38, 11373, 390, 8895, 322, 257, 7201, 5055, 295, 264, 3670, 13, 12753, 626, 281, 28999, 1096, 294, 44377, 51528], "temperature": 0.0, "avg_logprob": -0.12968141183085824, "compression_ratio": 1.5401785714285714, "no_speech_prob": 0.0005274623399600387}, {"id": 136, "seek": 77772, "start": 777.72, "end": 781.5600000000001, "text": " would not show up. Split is totally expected as an answer here.", "tokens": [50364, 576, 406, 855, 493, 13, 45111, 307, 3879, 5176, 382, 364, 1867, 510, 13, 50556], "temperature": 0.0, "avg_logprob": -0.08558743596076965, "compression_ratio": 1.4567307692307692, "no_speech_prob": 0.009707219898700714}, {"id": 137, "seek": 77772, "start": 782.36, "end": 787.1600000000001, "text": " What is interesting, though, is if I ask the same question in Croatian,", "tokens": [50596, 708, 307, 1880, 11, 1673, 11, 307, 498, 286, 1029, 264, 912, 1168, 294, 37614, 952, 11, 50836], "temperature": 0.0, "avg_logprob": -0.08558743596076965, "compression_ratio": 1.4567307692307692, "no_speech_prob": 0.009707219898700714}, {"id": 138, "seek": 77772, "start": 788.28, "end": 796.28, "text": " I get a different wrong answer. Zagreb. Now, that's an interesting answer,", "tokens": [50892, 286, 483, 257, 819, 2085, 1867, 13, 1176, 559, 22692, 13, 823, 11, 300, 311, 364, 1880, 1867, 11, 51292], "temperature": 0.0, "avg_logprob": -0.08558743596076965, "compression_ratio": 1.4567307692307692, "no_speech_prob": 0.009707219898700714}, {"id": 139, "seek": 77772, "start": 796.28, "end": 804.0400000000001, "text": " because it demonstrates us two things. First, knowledge in ChatGDP seems to be not stored in", "tokens": [51292, 570, 309, 31034, 505, 732, 721, 13, 2386, 11, 3601, 294, 27503, 38, 11373, 2544, 281, 312, 406, 12187, 294, 51680], "temperature": 0.0, "avg_logprob": -0.08558743596076965, "compression_ratio": 1.4567307692307692, "no_speech_prob": 0.009707219898700714}, {"id": 140, "seek": 80404, "start": 804.04, "end": 810.1999999999999, "text": " a language independent way, but is stored within each individual language. Depending on the language", "tokens": [50364, 257, 2856, 6695, 636, 11, 457, 307, 12187, 1951, 1184, 2609, 2856, 13, 22539, 322, 264, 2856, 50672], "temperature": 0.0, "avg_logprob": -0.0986300772363013, "compression_ratio": 1.5458333333333334, "no_speech_prob": 0.002434232272207737}, {"id": 141, "seek": 80404, "start": 810.1999999999999, "end": 816.92, "text": " I ask the question in, I receive a different answer. Also, the English answer at least has", "tokens": [50672, 286, 1029, 264, 1168, 294, 11, 286, 4774, 257, 819, 1867, 13, 2743, 11, 264, 3669, 1867, 412, 1935, 575, 51008], "temperature": 0.0, "avg_logprob": -0.0986300772363013, "compression_ratio": 1.5458333333333334, "no_speech_prob": 0.002434232272207737}, {"id": 142, "seek": 80404, "start": 816.92, "end": 823.64, "text": " support in the training data. Wikipedia did say split. The Croatian answer, where does Zagreb come", "tokens": [51008, 1406, 294, 264, 3097, 1412, 13, 28999, 630, 584, 7472, 13, 440, 37614, 952, 1867, 11, 689, 775, 1176, 559, 22692, 808, 51344], "temperature": 0.0, "avg_logprob": -0.0986300772363013, "compression_ratio": 1.5458333333333334, "no_speech_prob": 0.002434232272207737}, {"id": 143, "seek": 80404, "start": 823.64, "end": 829.4, "text": " from? I can find a single source that says that Anna Begovic was born in Zagreb.", "tokens": [51344, 490, 30, 286, 393, 915, 257, 2167, 4009, 300, 1619, 300, 12899, 879, 16089, 299, 390, 4232, 294, 1176, 559, 22692, 13, 51632], "temperature": 0.0, "avg_logprob": -0.0986300772363013, "compression_ratio": 1.5458333333333334, "no_speech_prob": 0.002434232272207737}, {"id": 144, "seek": 82940, "start": 829.64, "end": 837.56, "text": " But here's the thing. What if ChatGDP is actually just guessing, it's just making it up?", "tokens": [50376, 583, 510, 311, 264, 551, 13, 708, 498, 27503, 38, 11373, 307, 767, 445, 17939, 11, 309, 311, 445, 1455, 309, 493, 30, 50772], "temperature": 0.0, "avg_logprob": -0.1324545068943754, "compression_ratio": 1.5350877192982457, "no_speech_prob": 0.0010816415306180716}, {"id": 145, "seek": 82940, "start": 838.6, "end": 843.4, "text": " What is the probability of Zagreb given the prior of Croatian actress?", "tokens": [50824, 708, 307, 264, 8482, 295, 1176, 559, 22692, 2212, 264, 4059, 295, 37614, 952, 15410, 30, 51064], "temperature": 0.0, "avg_logprob": -0.1324545068943754, "compression_ratio": 1.5350877192982457, "no_speech_prob": 0.0010816415306180716}, {"id": 146, "seek": 82940, "start": 845.0, "end": 851.72, "text": " In order to figure that out, I want to check Wikipedia. I ask ChatGDP to give me a sparkle", "tokens": [51144, 682, 1668, 281, 2573, 300, 484, 11, 286, 528, 281, 1520, 28999, 13, 286, 1029, 27503, 38, 11373, 281, 976, 385, 257, 48558, 51480], "temperature": 0.0, "avg_logprob": -0.1324545068943754, "compression_ratio": 1.5350877192982457, "no_speech_prob": 0.0010816415306180716}, {"id": 147, "seek": 82940, "start": 851.72, "end": 858.52, "text": " query for Croatian actors in the places of birth. The query it returns to me is good enough for our", "tokens": [51480, 14581, 337, 37614, 952, 10037, 294, 264, 3190, 295, 3965, 13, 440, 14581, 309, 11247, 281, 385, 307, 665, 1547, 337, 527, 51820], "temperature": 0.0, "avg_logprob": -0.1324545068943754, "compression_ratio": 1.5350877192982457, "no_speech_prob": 0.0010816415306180716}, {"id": 148, "seek": 85852, "start": 858.52, "end": 865.64, "text": " purpose. I can just copy and paste it. It is subtly wrong if you read it in detail. It does not ask", "tokens": [50364, 4334, 13, 286, 393, 445, 5055, 293, 9163, 309, 13, 467, 307, 7257, 356, 2085, 498, 291, 1401, 309, 294, 2607, 13, 467, 775, 406, 1029, 50720], "temperature": 0.0, "avg_logprob": -0.10494278698432737, "compression_ratio": 1.528497409326425, "no_speech_prob": 0.0022168683353811502}, {"id": 149, "seek": 85852, "start": 865.64, "end": 871.72, "text": " for Croatian actors, but for actors born in the place in Croatia. But again, good enough for our", "tokens": [50720, 337, 37614, 952, 10037, 11, 457, 337, 10037, 4232, 294, 264, 1081, 294, 50186, 13, 583, 797, 11, 665, 1547, 337, 527, 51024], "temperature": 0.0, "avg_logprob": -0.10494278698432737, "compression_ratio": 1.528497409326425, "no_speech_prob": 0.0022168683353811502}, {"id": 150, "seek": 85852, "start": 871.72, "end": 881.8, "text": " purposes. But still, this is fascinating. ChatGDP got out of the box all of the QIDs right in this", "tokens": [51024, 9932, 13, 583, 920, 11, 341, 307, 10343, 13, 27503, 38, 11373, 658, 484, 295, 264, 2424, 439, 295, 264, 1249, 2777, 82, 558, 294, 341, 51528], "temperature": 0.0, "avg_logprob": -0.10494278698432737, "compression_ratio": 1.528497409326425, "no_speech_prob": 0.0022168683353811502}, {"id": 151, "seek": 88180, "start": 881.8, "end": 889.0, "text": " query. For Croatia, for actor, it got a property ID right. It can make sparkle queries that are", "tokens": [50364, 14581, 13, 1171, 50186, 11, 337, 8747, 11, 309, 658, 257, 4707, 7348, 558, 13, 467, 393, 652, 48558, 24109, 300, 366, 50724], "temperature": 0.0, "avg_logprob": -0.09745399906950176, "compression_ratio": 1.536, "no_speech_prob": 0.00831531174480915}, {"id": 152, "seek": 88180, "start": 889.0, "end": 897.0, "text": " syntactically right. And all of that with zero shot extra training. There was no look up on the web.", "tokens": [50724, 23980, 578, 984, 558, 13, 400, 439, 295, 300, 365, 4018, 3347, 2857, 3097, 13, 821, 390, 572, 574, 493, 322, 264, 3670, 13, 51124], "temperature": 0.0, "avg_logprob": -0.09745399906950176, "compression_ratio": 1.536, "no_speech_prob": 0.00831531174480915}, {"id": 153, "seek": 88180, "start": 897.0, "end": 905.24, "text": " ChatGDP just knows these QIDs by heart. In fact, you can totally ask ChatGDP to make you a table", "tokens": [51124, 27503, 38, 11373, 445, 3255, 613, 1249, 2777, 82, 538, 1917, 13, 682, 1186, 11, 291, 393, 3879, 1029, 27503, 38, 11373, 281, 652, 291, 257, 3199, 51536], "temperature": 0.0, "avg_logprob": -0.09745399906950176, "compression_ratio": 1.536, "no_speech_prob": 0.00831531174480915}, {"id": 154, "seek": 88180, "start": 905.24, "end": 911.24, "text": " of all the countries of the European Union and their QIDs. It has a lot of QIDs memorized.", "tokens": [51536, 295, 439, 264, 3517, 295, 264, 6473, 8133, 293, 641, 1249, 2777, 82, 13, 467, 575, 257, 688, 295, 1249, 2777, 82, 46677, 13, 51836], "temperature": 0.0, "avg_logprob": -0.09745399906950176, "compression_ratio": 1.536, "no_speech_prob": 0.00831531174480915}, {"id": 155, "seek": 91180, "start": 911.8, "end": 919.4, "text": " Just you never know if maybe one of them is wrong or not. The query can be copied and pasted right", "tokens": [50364, 1449, 291, 1128, 458, 498, 1310, 472, 295, 552, 307, 2085, 420, 406, 13, 440, 14581, 393, 312, 25365, 293, 1791, 292, 558, 50744], "temperature": 0.0, "avg_logprob": -0.14559842081903254, "compression_ratio": 1.5537848605577689, "no_speech_prob": 0.0004044652741868049}, {"id": 156, "seek": 91180, "start": 919.4, "end": 926.52, "text": " into the Wikipedia query service. And it runs giving 445 answers. And you can already see in the", "tokens": [50744, 666, 264, 28999, 14581, 2643, 13, 400, 309, 6676, 2902, 1017, 8465, 6338, 13, 400, 291, 393, 1217, 536, 294, 264, 51100], "temperature": 0.0, "avg_logprob": -0.14559842081903254, "compression_ratio": 1.5537848605577689, "no_speech_prob": 0.0004044652741868049}, {"id": 157, "seek": 91180, "start": 926.52, "end": 934.12, "text": " screenshot that the first few are all in Zagreb. So now we can see that of the 445 Croatian actors", "tokens": [51100, 27712, 300, 264, 700, 1326, 366, 439, 294, 1176, 559, 22692, 13, 407, 586, 321, 393, 536, 300, 295, 264, 1017, 8465, 37614, 952, 10037, 51480], "temperature": 0.0, "avg_logprob": -0.14559842081903254, "compression_ratio": 1.5537848605577689, "no_speech_prob": 0.0004044652741868049}, {"id": 158, "seek": 91180, "start": 934.12, "end": 941.0799999999999, "text": " for place of birth, 154 are born in Zagreb, about a third. This gives a pretty good conditional", "tokens": [51480, 337, 1081, 295, 3965, 11, 2119, 19, 366, 4232, 294, 1176, 559, 22692, 11, 466, 257, 2636, 13, 639, 2709, 257, 1238, 665, 27708, 51828], "temperature": 0.0, "avg_logprob": -0.14559842081903254, "compression_ratio": 1.5537848605577689, "no_speech_prob": 0.0004044652741868049}, {"id": 159, "seek": 94108, "start": 941.08, "end": 948.12, "text": " probability for the birthplace of Croatian actors. ChatGDP is guessing the place of birth", "tokens": [50364, 8482, 337, 264, 3965, 6742, 295, 37614, 952, 10037, 13, 27503, 38, 11373, 307, 17939, 264, 1081, 295, 3965, 50716], "temperature": 0.0, "avg_logprob": -0.10534529850400727, "compression_ratio": 1.5318181818181817, "no_speech_prob": 0.0018385021248832345}, {"id": 160, "seek": 94108, "start": 948.12, "end": 954.84, "text": " for Anna Begovic in Croatian, even though it knows it in English. This leads me to something my", "tokens": [50716, 337, 12899, 879, 16089, 299, 294, 37614, 952, 11, 754, 1673, 309, 3255, 309, 294, 3669, 13, 639, 6689, 385, 281, 746, 452, 51052], "temperature": 0.0, "avg_logprob": -0.10534529850400727, "compression_ratio": 1.5318181818181817, "no_speech_prob": 0.0018385021248832345}, {"id": 161, "seek": 94108, "start": 954.84, "end": 961.72, "text": " manager at Google used to say. You can machine learn Obama's birthplace every time you need it,", "tokens": [51052, 6598, 412, 3329, 1143, 281, 584, 13, 509, 393, 3479, 1466, 9560, 311, 3965, 6742, 633, 565, 291, 643, 309, 11, 51396], "temperature": 0.0, "avg_logprob": -0.10534529850400727, "compression_ratio": 1.5318181818181817, "no_speech_prob": 0.0018385021248832345}, {"id": 162, "seek": 94108, "start": 961.72, "end": 965.1600000000001, "text": " but it costs a lot and you're never sure it is correct.", "tokens": [51396, 457, 309, 5497, 257, 688, 293, 291, 434, 1128, 988, 309, 307, 3006, 13, 51568], "temperature": 0.0, "avg_logprob": -0.10534529850400727, "compression_ratio": 1.5318181818181817, "no_speech_prob": 0.0018385021248832345}, {"id": 163, "seek": 96516, "start": 965.4, "end": 972.36, "text": " Another interesting observation is that GPT isn't particularly good with knowing what it knows.", "tokens": [50376, 3996, 1880, 14816, 307, 300, 26039, 51, 1943, 380, 4098, 665, 365, 5276, 437, 309, 3255, 13, 50724], "temperature": 0.0, "avg_logprob": -0.1442912791637664, "compression_ratio": 1.5078125, "no_speech_prob": 0.002550834557041526}, {"id": 164, "seek": 96516, "start": 973.7199999999999, "end": 980.92, "text": " For example, here we are asking for cities with mayors born after 1998. I wanted to ask something", "tokens": [50792, 1171, 1365, 11, 510, 321, 366, 3365, 337, 6486, 365, 815, 830, 4232, 934, 21404, 13, 286, 1415, 281, 1029, 746, 51152], "temperature": 0.0, "avg_logprob": -0.1442912791637664, "compression_ratio": 1.5078125, "no_speech_prob": 0.002550834557041526}, {"id": 165, "seek": 96516, "start": 980.92, "end": 986.4399999999999, "text": " that I expected to be not trivially Google-able, where there wouldn't be a listicle or table on", "tokens": [51152, 300, 286, 5176, 281, 312, 406, 1376, 85, 2270, 3329, 12, 712, 11, 689, 456, 2759, 380, 312, 257, 1329, 3520, 420, 3199, 322, 51428], "temperature": 0.0, "avg_logprob": -0.1442912791637664, "compression_ratio": 1.5078125, "no_speech_prob": 0.002550834557041526}, {"id": 166, "seek": 96516, "start": 986.4399999999999, "end": 994.92, "text": " the map. So ChatGDP correctly points out that that would make them younger than 23 at its cutoff", "tokens": [51428, 264, 4471, 13, 407, 27503, 38, 11373, 8944, 2793, 484, 300, 300, 576, 652, 552, 7037, 813, 6673, 412, 1080, 1723, 4506, 51852], "temperature": 0.0, "avg_logprob": -0.1442912791637664, "compression_ratio": 1.5078125, "no_speech_prob": 0.002550834557041526}, {"id": 167, "seek": 99492, "start": 994.92, "end": 1004.04, "text": " 80 date of 2021. So it is unlikely that anyone would be mayor at that age. Asking Bing Chat,", "tokens": [50364, 4688, 4002, 295, 7201, 13, 407, 309, 307, 17518, 300, 2878, 576, 312, 10120, 412, 300, 3205, 13, 1018, 5092, 30755, 27503, 11, 50820], "temperature": 0.0, "avg_logprob": -0.1519590218861898, "compression_ratio": 1.43801652892562, "no_speech_prob": 0.0003982052148785442}, {"id": 168, "seek": 99492, "start": 1004.04, "end": 1007.88, "text": " it also says that it doesn't know anything about mayors born after 1998.", "tokens": [50820, 309, 611, 1619, 300, 309, 1177, 380, 458, 1340, 466, 815, 830, 4232, 934, 21404, 13, 51012], "temperature": 0.0, "avg_logprob": -0.1519590218861898, "compression_ratio": 1.43801652892562, "no_speech_prob": 0.0003982052148785442}, {"id": 169, "seek": 99492, "start": 1010.36, "end": 1015.4, "text": " Let's check Wikidata. I again used ChatGDP to help me write a query, although this time I", "tokens": [51136, 961, 311, 1520, 23377, 327, 3274, 13, 286, 797, 1143, 27503, 38, 11373, 281, 854, 385, 2464, 257, 14581, 11, 4878, 341, 565, 286, 51388], "temperature": 0.0, "avg_logprob": -0.1519590218861898, "compression_ratio": 1.43801652892562, "no_speech_prob": 0.0003982052148785442}, {"id": 170, "seek": 99492, "start": 1015.4, "end": 1022.5999999999999, "text": " had to make a few fixes. And indeed, Wikidata has an answer. It tells me about Christian von", "tokens": [51388, 632, 281, 652, 257, 1326, 32539, 13, 400, 6451, 11, 23377, 327, 3274, 575, 364, 1867, 13, 467, 5112, 385, 466, 5778, 2957, 51748], "temperature": 0.0, "avg_logprob": -0.1519590218861898, "compression_ratio": 1.43801652892562, "no_speech_prob": 0.0003982052148785442}, {"id": 171, "seek": 102260, "start": 1022.6, "end": 1028.3600000000001, "text": " Waldenfelds, who was born in April 2000, mayor of Lichtenberg in Bavaria, Germany.", "tokens": [50364, 9707, 1556, 25115, 82, 11, 567, 390, 4232, 294, 6929, 8132, 11, 10120, 295, 441, 24681, 6873, 294, 363, 706, 9831, 11, 7244, 13, 50652], "temperature": 0.0, "avg_logprob": -0.18823880565409756, "compression_ratio": 1.5688073394495412, "no_speech_prob": 0.008437291719019413}, {"id": 172, "seek": 102260, "start": 1029.72, "end": 1034.92, "text": " He became mayor in 2020, well before the cutoff date of Chaterity, by the way.", "tokens": [50720, 634, 3062, 10120, 294, 4808, 11, 731, 949, 264, 1723, 4506, 4002, 295, 761, 771, 507, 11, 538, 264, 636, 13, 50980], "temperature": 0.0, "avg_logprob": -0.18823880565409756, "compression_ratio": 1.5688073394495412, "no_speech_prob": 0.008437291719019413}, {"id": 173, "seek": 102260, "start": 1037.0, "end": 1041.4, "text": " This is no endorsement of his politics or his platform he is running, by the way.", "tokens": [51084, 639, 307, 572, 29228, 518, 295, 702, 7341, 420, 702, 3663, 415, 307, 2614, 11, 538, 264, 636, 13, 51304], "temperature": 0.0, "avg_logprob": -0.18823880565409756, "compression_ratio": 1.5688073394495412, "no_speech_prob": 0.008437291719019413}, {"id": 174, "seek": 102260, "start": 1043.96, "end": 1049.8, "text": " Now that we know the answers, we can guide Bing Chat and get the mayor of Lichtenberg and his age.", "tokens": [51432, 823, 300, 321, 458, 264, 6338, 11, 321, 393, 5934, 30755, 27503, 293, 483, 264, 10120, 295, 441, 24681, 6873, 293, 702, 3205, 13, 51724], "temperature": 0.0, "avg_logprob": -0.18823880565409756, "compression_ratio": 1.5688073394495412, "no_speech_prob": 0.008437291719019413}, {"id": 175, "seek": 104980, "start": 1049.8, "end": 1053.72, "text": " And obviously this answer is inconsistent with its previous answer,", "tokens": [50364, 400, 2745, 341, 1867, 307, 36891, 365, 1080, 3894, 1867, 11, 50560], "temperature": 0.0, "avg_logprob": -0.12778540146656525, "compression_ratio": 1.5176991150442478, "no_speech_prob": 0.00046550267143175006}, {"id": 176, "seek": 104980, "start": 1053.72, "end": 1058.84, "text": " but ChatGDP or BingJDP are both completely unaware of this inconsistency and don't care.", "tokens": [50560, 457, 27503, 38, 11373, 420, 30755, 41, 11373, 366, 1293, 2584, 32065, 295, 341, 22039, 468, 3020, 293, 500, 380, 1127, 13, 50816], "temperature": 0.0, "avg_logprob": -0.12778540146656525, "compression_ratio": 1.5176991150442478, "no_speech_prob": 0.00046550267143175006}, {"id": 177, "seek": 104980, "start": 1060.52, "end": 1065.48, "text": " Large language models are not yet graded being consistent, whether about individual facts", "tokens": [50900, 33092, 2856, 5245, 366, 406, 1939, 2771, 292, 885, 8398, 11, 1968, 466, 2609, 9130, 51148], "temperature": 0.0, "avg_logprob": -0.12778540146656525, "compression_ratio": 1.5176991150442478, "no_speech_prob": 0.00046550267143175006}, {"id": 178, "seek": 104980, "start": 1065.48, "end": 1073.0, "text": " or across different languages. They are also not always very good at math. But even if they were", "tokens": [51148, 420, 2108, 819, 8650, 13, 814, 366, 611, 406, 1009, 588, 665, 412, 5221, 13, 583, 754, 498, 436, 645, 51524], "temperature": 0.0, "avg_logprob": -0.12778540146656525, "compression_ratio": 1.5176991150442478, "no_speech_prob": 0.00046550267143175006}, {"id": 179, "seek": 107300, "start": 1073.0, "end": 1079.8, "text": " good at math, we have to answer the same question that we do for looking up facts in a knowledge", "tokens": [50364, 665, 412, 5221, 11, 321, 362, 281, 1867, 264, 912, 1168, 300, 321, 360, 337, 1237, 493, 9130, 294, 257, 3601, 50704], "temperature": 0.0, "avg_logprob": -0.11213015426288951, "compression_ratio": 1.537117903930131, "no_speech_prob": 0.0655885711312294}, {"id": 180, "seek": 107300, "start": 1079.8, "end": 1090.44, "text": " graph. Why would you ever use 96 layers, 175 billion parameters model, to do multiplication?", "tokens": [50704, 4295, 13, 1545, 576, 291, 1562, 764, 24124, 7914, 11, 41165, 5218, 9834, 2316, 11, 281, 360, 27290, 30, 51236], "temperature": 0.0, "avg_logprob": -0.11213015426288951, "compression_ratio": 1.537117903930131, "no_speech_prob": 0.0655885711312294}, {"id": 181, "seek": 107300, "start": 1091.08, "end": 1094.28, "text": " When that's something you can do in a single operation on your CPU.", "tokens": [51268, 1133, 300, 311, 746, 291, 393, 360, 294, 257, 2167, 6916, 322, 428, 13199, 13, 51428], "temperature": 0.0, "avg_logprob": -0.11213015426288951, "compression_ratio": 1.537117903930131, "no_speech_prob": 0.0655885711312294}, {"id": 182, "seek": 107300, "start": 1095.64, "end": 1102.2, "text": " Why internalized knowledge in an LLM if you can externalize it in a graph store and look it up", "tokens": [51496, 1545, 6920, 1602, 3601, 294, 364, 441, 43, 44, 498, 291, 393, 8320, 1125, 309, 294, 257, 4295, 3531, 293, 574, 309, 493, 51824], "temperature": 0.0, "avg_logprob": -0.11213015426288951, "compression_ratio": 1.537117903930131, "no_speech_prob": 0.0655885711312294}, {"id": 183, "seek": 110220, "start": 1102.28, "end": 1109.24, "text": " when needed. Don't get bedazzled by the capabilities of large language models.", "tokens": [50368, 562, 2978, 13, 1468, 380, 483, 2901, 9112, 1493, 538, 264, 10862, 295, 2416, 2856, 5245, 13, 50716], "temperature": 0.0, "avg_logprob": -0.17478055732194767, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.0011694440618157387}, {"id": 184, "seek": 110220, "start": 1110.92, "end": 1117.64, "text": " Autoregressive transformer models such as ChatGDP are touring complete and they are just a very", "tokens": [50800, 6049, 418, 3091, 488, 31782, 5245, 1270, 382, 27503, 38, 11373, 366, 32487, 3566, 293, 436, 366, 445, 257, 588, 51136], "temperature": 0.0, "avg_logprob": -0.17478055732194767, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.0011694440618157387}, {"id": 185, "seek": 110220, "start": 1117.64, "end": 1124.3600000000001, "text": " expensive reiteration of touring's carpet. You can do everything with them, but it doesn't mean you", "tokens": [51136, 5124, 25211, 399, 295, 32487, 311, 18119, 13, 509, 393, 360, 1203, 365, 552, 11, 457, 309, 1177, 380, 914, 291, 51472], "temperature": 0.0, "avg_logprob": -0.17478055732194767, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.0011694440618157387}, {"id": 186, "seek": 110220, "start": 1124.3600000000001, "end": 1131.4, "text": " should. Use LLMs where they are efficient and use other things where they are better.", "tokens": [51472, 820, 13, 8278, 441, 43, 26386, 689, 436, 366, 7148, 293, 764, 661, 721, 689, 436, 366, 1101, 13, 51824], "temperature": 0.0, "avg_logprob": -0.17478055732194767, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.0011694440618157387}, {"id": 187, "seek": 113220, "start": 1133.16, "end": 1139.24, "text": " One possible solution to this capability gap are so-called augmented language models.", "tokens": [50412, 1485, 1944, 3827, 281, 341, 13759, 7417, 366, 370, 12, 11880, 36155, 2856, 5245, 13, 50716], "temperature": 0.0, "avg_logprob": -0.17828569179627954, "compression_ratio": 1.5278969957081545, "no_speech_prob": 0.00018234964227303863}, {"id": 188, "seek": 113220, "start": 1140.28, "end": 1146.04, "text": " Toolform are being a particularly well-known example. Or for ChatGDP, that's what ChatGDP", "tokens": [50768, 15934, 837, 366, 885, 257, 4098, 731, 12, 6861, 1365, 13, 1610, 337, 27503, 38, 11373, 11, 300, 311, 437, 27503, 38, 11373, 51056], "temperature": 0.0, "avg_logprob": -0.17828569179627954, "compression_ratio": 1.5278969957081545, "no_speech_prob": 0.00018234964227303863}, {"id": 189, "seek": 113220, "start": 1146.04, "end": 1152.3600000000001, "text": " plugins are there for. The idea is that we can enrich large language models with additional", "tokens": [51056, 33759, 366, 456, 337, 13, 440, 1558, 307, 300, 321, 393, 18849, 2416, 2856, 5245, 365, 4497, 51372], "temperature": 0.0, "avg_logprob": -0.17828569179627954, "compression_ratio": 1.5278969957081545, "no_speech_prob": 0.00018234964227303863}, {"id": 190, "seek": 113220, "start": 1152.3600000000001, "end": 1158.28, "text": " systems which are good and efficient at specific tasks, such as math or other functions,", "tokens": [51372, 3652, 597, 366, 665, 293, 7148, 412, 2685, 9608, 11, 1270, 382, 5221, 420, 661, 6828, 11, 51668], "temperature": 0.0, "avg_logprob": -0.17828569179627954, "compression_ratio": 1.5278969957081545, "no_speech_prob": 0.00018234964227303863}, {"id": 191, "seek": 115828, "start": 1158.44, "end": 1164.12, "text": " from wiki functions, or looking up facts or query results in a knowledge graph such as wiki data.", "tokens": [50372, 490, 261, 9850, 6828, 11, 420, 1237, 493, 9130, 420, 14581, 3542, 294, 257, 3601, 4295, 1270, 382, 261, 9850, 1412, 13, 50656], "temperature": 0.0, "avg_logprob": -0.16754788768534756, "compression_ratio": 1.6585365853658536, "no_speech_prob": 0.001782117411494255}, {"id": 192, "seek": 115828, "start": 1165.24, "end": 1171.8, "text": " I mean, given that ChatGDP already can, zero shot, create queries against wiki data,", "tokens": [50712, 286, 914, 11, 2212, 300, 27503, 38, 11373, 1217, 393, 11, 4018, 3347, 11, 1884, 24109, 1970, 261, 9850, 1412, 11, 51040], "temperature": 0.0, "avg_logprob": -0.16754788768534756, "compression_ratio": 1.6585365853658536, "no_speech_prob": 0.001782117411494255}, {"id": 193, "seek": 115828, "start": 1171.8, "end": 1174.6, "text": " there isn't that much to do to make them work together.", "tokens": [51040, 456, 1943, 380, 300, 709, 281, 360, 281, 652, 552, 589, 1214, 13, 51180], "temperature": 0.0, "avg_logprob": -0.16754788768534756, "compression_ratio": 1.6585365853658536, "no_speech_prob": 0.001782117411494255}, {"id": 194, "seek": 115828, "start": 1177.08, "end": 1183.24, "text": " Some folks think that some mapping of strategic nodes into a knowledge graph with embeddings,", "tokens": [51304, 2188, 4024, 519, 300, 512, 18350, 295, 10924, 13891, 666, 257, 3601, 4295, 365, 12240, 29432, 11, 51612], "temperature": 0.0, "avg_logprob": -0.16754788768534756, "compression_ratio": 1.6585365853658536, "no_speech_prob": 0.001782117411494255}, {"id": 195, "seek": 115828, "start": 1183.8799999999999, "end": 1187.32, "text": " we can easily connect a knowledge graph directly to a large language model.", "tokens": [51644, 321, 393, 3612, 1745, 257, 3601, 4295, 3838, 281, 257, 2416, 2856, 2316, 13, 51816], "temperature": 0.0, "avg_logprob": -0.16754788768534756, "compression_ratio": 1.6585365853658536, "no_speech_prob": 0.001782117411494255}, {"id": 196, "seek": 118828, "start": 1188.52, "end": 1192.92, "text": " But we don't even need to do that, we can just use the Sparkle query generation ability", "tokens": [50376, 583, 321, 500, 380, 754, 643, 281, 360, 300, 11, 321, 393, 445, 764, 264, 23424, 306, 14581, 5125, 3485, 50596], "temperature": 0.0, "avg_logprob": -0.10933334224826687, "compression_ratio": 1.6106194690265487, "no_speech_prob": 0.00024922608281485736}, {"id": 197, "seek": 118828, "start": 1192.92, "end": 1200.28, "text": " directly and ask queries against wiki data. And not only can we connect LLMs to a knowledge", "tokens": [50596, 3838, 293, 1029, 24109, 1970, 261, 9850, 1412, 13, 400, 406, 787, 393, 321, 1745, 441, 43, 26386, 281, 257, 3601, 50964], "temperature": 0.0, "avg_logprob": -0.10933334224826687, "compression_ratio": 1.6106194690265487, "no_speech_prob": 0.00024922608281485736}, {"id": 198, "seek": 118828, "start": 1200.28, "end": 1206.36, "text": " graph, but also to a repository of functions, such as wiki functions. Both knowledge graphs and", "tokens": [50964, 4295, 11, 457, 611, 281, 257, 25841, 295, 6828, 11, 1270, 382, 261, 9850, 6828, 13, 6767, 3601, 24877, 293, 51268], "temperature": 0.0, "avg_logprob": -0.10933334224826687, "compression_ratio": 1.6106194690265487, "no_speech_prob": 0.00024922608281485736}, {"id": 199, "seek": 118828, "start": 1206.36, "end": 1214.12, "text": " functions would be tools the LLM can learn to use. There is work in trying to understand", "tokens": [51268, 6828, 576, 312, 3873, 264, 441, 43, 44, 393, 1466, 281, 764, 13, 821, 307, 589, 294, 1382, 281, 1223, 51656], "temperature": 0.0, "avg_logprob": -0.10933334224826687, "compression_ratio": 1.6106194690265487, "no_speech_prob": 0.00024922608281485736}, {"id": 200, "seek": 121412, "start": 1214.12, "end": 1218.6799999999998, "text": " how knowledge is stored in the parameters of a large language model. And when we look at this", "tokens": [50364, 577, 3601, 307, 12187, 294, 264, 9834, 295, 257, 2416, 2856, 2316, 13, 400, 562, 321, 574, 412, 341, 50592], "temperature": 0.0, "avg_logprob": -0.0885317142193134, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.0004955287440679967}, {"id": 201, "seek": 121412, "start": 1218.6799999999998, "end": 1228.28, "text": " work, we start to understand why large language models are large. Do they really have to be this", "tokens": [50592, 589, 11, 321, 722, 281, 1223, 983, 2416, 2856, 5245, 366, 2416, 13, 1144, 436, 534, 362, 281, 312, 341, 51072], "temperature": 0.0, "avg_logprob": -0.0885317142193134, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.0004955287440679967}, {"id": 202, "seek": 121412, "start": 1228.28, "end": 1237.8799999999999, "text": " large? Let's compare with stable diffusion one. Stable diffusion one is a text to image generator.", "tokens": [51072, 2416, 30, 961, 311, 6794, 365, 8351, 25242, 472, 13, 745, 712, 25242, 472, 307, 257, 2487, 281, 3256, 19265, 13, 51552], "temperature": 0.0, "avg_logprob": -0.0885317142193134, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.0004955287440679967}, {"id": 203, "seek": 121412, "start": 1237.8799999999999, "end": 1244.04, "text": " It has to understand natural language prompts, just as GPT does. It can make an image out of", "tokens": [51552, 467, 575, 281, 1223, 3303, 2856, 41095, 11, 445, 382, 26039, 51, 775, 13, 467, 393, 652, 364, 3256, 484, 295, 51860], "temperature": 0.0, "avg_logprob": -0.0885317142193134, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.0004955287440679967}, {"id": 204, "seek": 124404, "start": 1244.04, "end": 1248.76, "text": " basically any prompt. It can also generate the image of a good number of celebrities.", "tokens": [50364, 1936, 604, 12391, 13, 467, 393, 611, 8460, 264, 3256, 295, 257, 665, 1230, 295, 23200, 13, 50600], "temperature": 0.0, "avg_logprob": -0.1838175654411316, "compression_ratio": 1.4216867469879517, "no_speech_prob": 0.0015977587318047881}, {"id": 205, "seek": 124404, "start": 1248.76, "end": 1255.3999999999999, "text": " Here, for example, I'm asking for the Pope, Geleta Thunberg, Idris Elba, Michelle Yeo,", "tokens": [50600, 1692, 11, 337, 1365, 11, 286, 478, 3365, 337, 264, 19291, 11, 16142, 7664, 334, 409, 6873, 11, 11506, 5714, 2699, 4231, 11, 14933, 835, 78, 11, 50932], "temperature": 0.0, "avg_logprob": -0.1838175654411316, "compression_ratio": 1.4216867469879517, "no_speech_prob": 0.0015977587318047881}, {"id": 206, "seek": 124404, "start": 1255.3999999999999, "end": 1260.6, "text": " Helen Mirren, and Yanle Kuhn to explain knowledge graphs with the help of a whiteboard.", "tokens": [50932, 26294, 9421, 1095, 11, 293, 13633, 306, 591, 3232, 77, 281, 2903, 3601, 24877, 365, 264, 854, 295, 257, 2418, 3787, 13, 51192], "temperature": 0.0, "avg_logprob": -0.1838175654411316, "compression_ratio": 1.4216867469879517, "no_speech_prob": 0.0015977587318047881}, {"id": 207, "seek": 124404, "start": 1262.36, "end": 1271.0, "text": " So GPT-3 had 175 billion parameters. How many parameters do you think are in stable diffusion", "tokens": [51280, 407, 26039, 51, 12, 18, 632, 41165, 5218, 9834, 13, 1012, 867, 9834, 360, 291, 519, 366, 294, 8351, 25242, 51712], "temperature": 0.0, "avg_logprob": -0.1838175654411316, "compression_ratio": 1.4216867469879517, "no_speech_prob": 0.0015977587318047881}, {"id": 208, "seek": 127100, "start": 1271.0, "end": 1279.88, "text": " one, knowing all these people being able to generate them? 890 million parameters.", "tokens": [50364, 472, 11, 5276, 439, 613, 561, 885, 1075, 281, 8460, 552, 30, 1649, 7771, 2459, 9834, 13, 50808], "temperature": 0.0, "avg_logprob": -0.08565901347569056, "compression_ratio": 1.4148936170212767, "no_speech_prob": 0.0015730501618236303}, {"id": 209, "seek": 127100, "start": 1280.76, "end": 1289.96, "text": " Now, I think 890 million is a lot. But GPT-3 is about 200 times larger than stable diffusion one.", "tokens": [50852, 823, 11, 286, 519, 1649, 7771, 2459, 307, 257, 688, 13, 583, 26039, 51, 12, 18, 307, 466, 2331, 1413, 4833, 813, 8351, 25242, 472, 13, 51312], "temperature": 0.0, "avg_logprob": -0.08565901347569056, "compression_ratio": 1.4148936170212767, "no_speech_prob": 0.0015730501618236303}, {"id": 210, "seek": 127100, "start": 1291.24, "end": 1296.6, "text": " And it's no surprise. Think of it. All the knowledge that we were using for questions", "tokens": [51376, 400, 309, 311, 572, 6365, 13, 6557, 295, 309, 13, 1057, 264, 3601, 300, 321, 645, 1228, 337, 1651, 51644], "temperature": 0.0, "avg_logprob": -0.08565901347569056, "compression_ratio": 1.4148936170212767, "no_speech_prob": 0.0015730501618236303}, {"id": 211, "seek": 129660, "start": 1296.6, "end": 1302.36, "text": " answering so far. The embedded QIDs, what are the member countries of the EU,", "tokens": [50364, 13430, 370, 1400, 13, 440, 16741, 1249, 2777, 82, 11, 437, 366, 264, 4006, 3517, 295, 264, 10887, 11, 50652], "temperature": 0.0, "avg_logprob": -0.17406940460205078, "compression_ratio": 1.5741626794258374, "no_speech_prob": 0.011330228298902512}, {"id": 212, "seek": 129660, "start": 1302.36, "end": 1309.6399999999999, "text": " the place of birth of individual people. I mean, if it has NABegovitch, I'm sure GPT-3 knows the", "tokens": [50652, 264, 1081, 295, 3965, 295, 2609, 561, 13, 286, 914, 11, 498, 309, 575, 426, 13868, 1146, 5179, 1549, 11, 286, 478, 988, 26039, 51, 12, 18, 3255, 264, 51016], "temperature": 0.0, "avg_logprob": -0.17406940460205078, "compression_ratio": 1.5741626794258374, "no_speech_prob": 0.011330228298902512}, {"id": 213, "seek": 129660, "start": 1309.6399999999999, "end": 1315.6399999999999, "text": " place of birth of millions of individuals. All this taught in many different languages.", "tokens": [51016, 1081, 295, 3965, 295, 6803, 295, 5346, 13, 1057, 341, 5928, 294, 867, 819, 8650, 13, 51316], "temperature": 0.0, "avg_logprob": -0.17406940460205078, "compression_ratio": 1.5741626794258374, "no_speech_prob": 0.011330228298902512}, {"id": 214, "seek": 129660, "start": 1317.08, "end": 1321.32, "text": " All of this is taught in those hundreds of billions of parameters.", "tokens": [51388, 1057, 295, 341, 307, 5928, 294, 729, 6779, 295, 17375, 295, 9834, 13, 51600], "temperature": 0.0, "avg_logprob": -0.17406940460205078, "compression_ratio": 1.5741626794258374, "no_speech_prob": 0.011330228298902512}, {"id": 215, "seek": 132132, "start": 1321.8, "end": 1326.6, "text": " You don't need that for text generation. But you need it if you want to answer all", "tokens": [50388, 509, 500, 380, 643, 300, 337, 2487, 5125, 13, 583, 291, 643, 309, 498, 291, 528, 281, 1867, 439, 50628], "temperature": 0.0, "avg_logprob": -0.16581301853574557, "compression_ratio": 1.4506437768240343, "no_speech_prob": 0.010012511163949966}, {"id": 216, "seek": 132132, "start": 1326.6, "end": 1333.1599999999999, "text": " these questions we have been asking. And indeed, Metas Lama, which came out a few weeks ago,", "tokens": [50628, 613, 1651, 321, 362, 668, 3365, 13, 400, 6451, 11, 6377, 296, 441, 2404, 11, 597, 1361, 484, 257, 1326, 3259, 2057, 11, 50956], "temperature": 0.0, "avg_logprob": -0.16581301853574557, "compression_ratio": 1.4506437768240343, "no_speech_prob": 0.010012511163949966}, {"id": 217, "seek": 132132, "start": 1333.1599999999999, "end": 1340.6799999999998, "text": " is considerably smaller than GPT-3, about 25 times smaller. But it seems to be", "tokens": [50956, 307, 31308, 4356, 813, 26039, 51, 12, 18, 11, 466, 3552, 1413, 4356, 13, 583, 309, 2544, 281, 312, 51332], "temperature": 0.0, "avg_logprob": -0.16581301853574557, "compression_ratio": 1.4506437768240343, "no_speech_prob": 0.010012511163949966}, {"id": 218, "seek": 132132, "start": 1340.6799999999998, "end": 1347.3999999999999, "text": " rather competitive in terms of language understanding and fluency. So yes, we could", "tokens": [51332, 2831, 10043, 294, 2115, 295, 2856, 3701, 293, 5029, 3020, 13, 407, 2086, 11, 321, 727, 51668], "temperature": 0.0, "avg_logprob": -0.16581301853574557, "compression_ratio": 1.4506437768240343, "no_speech_prob": 0.010012511163949966}, {"id": 219, "seek": 134740, "start": 1347.88, "end": 1354.92, "text": " internalize all the 1.4 billion statements in Wikidata into a large language model.", "tokens": [50388, 6920, 1125, 439, 264, 502, 13, 19, 5218, 12363, 294, 23377, 327, 3274, 666, 257, 2416, 2856, 2316, 13, 50740], "temperature": 0.0, "avg_logprob": -0.12200372219085694, "compression_ratio": 1.739795918367347, "no_speech_prob": 0.0017273793928325176}, {"id": 220, "seek": 134740, "start": 1355.88, "end": 1361.5600000000002, "text": " But what if we go the other way around and try to externalize the knowledge model instead?", "tokens": [50788, 583, 437, 498, 321, 352, 264, 661, 636, 926, 293, 853, 281, 8320, 1125, 264, 3601, 2316, 2602, 30, 51072], "temperature": 0.0, "avg_logprob": -0.12200372219085694, "compression_ratio": 1.739795918367347, "no_speech_prob": 0.0017273793928325176}, {"id": 221, "seek": 134740, "start": 1362.6000000000001, "end": 1367.48, "text": " If we leave the language model to deal with language, but push the knowledge", "tokens": [51124, 759, 321, 1856, 264, 2856, 2316, 281, 2028, 365, 2856, 11, 457, 2944, 264, 3601, 51368], "temperature": 0.0, "avg_logprob": -0.12200372219085694, "compression_ratio": 1.739795918367347, "no_speech_prob": 0.0017273793928325176}, {"id": 222, "seek": 134740, "start": 1368.1200000000001, "end": 1373.96, "text": " to a knowledge graph or a different knowledge model, in a world where language models can", "tokens": [51400, 281, 257, 3601, 4295, 420, 257, 819, 3601, 2316, 11, 294, 257, 1002, 689, 2856, 5245, 393, 51692], "temperature": 0.0, "avg_logprob": -0.12200372219085694, "compression_ratio": 1.739795918367347, "no_speech_prob": 0.0017273793928325176}, {"id": 223, "seek": 137396, "start": 1373.96, "end": 1381.96, "text": " generate infinite content, knowledge becomes valuable. And that takes us back to Jamie Taylor's", "tokens": [50364, 8460, 13785, 2701, 11, 3601, 3643, 8263, 13, 400, 300, 2516, 505, 646, 281, 19309, 12060, 311, 50764], "temperature": 0.0, "avg_logprob": -0.09706844680610745, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.001115940511226654}, {"id": 224, "seek": 137396, "start": 1381.96, "end": 1388.52, "text": " rule. We don't want to machine learn Obama's place of birth every time we need it. We want to", "tokens": [50764, 4978, 13, 492, 500, 380, 528, 281, 3479, 1466, 9560, 311, 1081, 295, 3965, 633, 565, 321, 643, 309, 13, 492, 528, 281, 51092], "temperature": 0.0, "avg_logprob": -0.09706844680610745, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.001115940511226654}, {"id": 225, "seek": 137396, "start": 1388.52, "end": 1395.48, "text": " store it once and for all. And that's what knowledge graphs are good for. To keep you", "tokens": [51092, 3531, 309, 1564, 293, 337, 439, 13, 400, 300, 311, 437, 3601, 24877, 366, 665, 337, 13, 1407, 1066, 291, 51440], "temperature": 0.0, "avg_logprob": -0.09706844680610745, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.001115940511226654}, {"id": 226, "seek": 137396, "start": 1395.48, "end": 1401.8, "text": " valuable knowledge safe. I am of the strict belief there is no reason to ever again", "tokens": [51440, 8263, 3601, 3273, 13, 286, 669, 295, 264, 10910, 7107, 456, 307, 572, 1778, 281, 1562, 797, 51756], "temperature": 0.0, "avg_logprob": -0.09706844680610745, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.001115940511226654}, {"id": 227, "seek": 140180, "start": 1402.44, "end": 1409.72, "text": " manually enter the place of birth for Anna Begovic. This makes knowledge for Wikidata", "tokens": [50396, 16945, 3242, 264, 1081, 295, 3965, 337, 12899, 879, 16089, 299, 13, 639, 1669, 3601, 337, 23377, 327, 3274, 50760], "temperature": 0.0, "avg_logprob": -0.10828051073797818, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.0008295620209537446}, {"id": 228, "seek": 140180, "start": 1409.72, "end": 1417.32, "text": " both valuable and a public good. The knowledge graph provides you with the ground truth for your", "tokens": [50760, 1293, 8263, 293, 257, 1908, 665, 13, 440, 3601, 4295, 6417, 291, 365, 264, 2727, 3494, 337, 428, 51140], "temperature": 0.0, "avg_logprob": -0.10828051073797818, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.0008295620209537446}, {"id": 229, "seek": 140180, "start": 1417.32, "end": 1424.36, "text": " language models. By the way, the other way around is also true. Large language models can be an", "tokens": [51140, 2856, 5245, 13, 3146, 264, 636, 11, 264, 661, 636, 926, 307, 611, 2074, 13, 33092, 2856, 5245, 393, 312, 364, 51492], "temperature": 0.0, "avg_logprob": -0.10828051073797818, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.0008295620209537446}, {"id": 230, "seek": 140180, "start": 1424.36, "end": 1430.36, "text": " amazing tool to speed up the creation of a knowledge graph. They are probably the best tool for", "tokens": [51492, 2243, 2290, 281, 3073, 493, 264, 8016, 295, 257, 3601, 4295, 13, 814, 366, 1391, 264, 1151, 2290, 337, 51792], "temperature": 0.0, "avg_logprob": -0.10828051073797818, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.0008295620209537446}, {"id": 231, "seek": 143036, "start": 1430.36, "end": 1438.1999999999998, "text": " knowledge extraction we have seen developed in a decade or two. We want to extract knowledge into", "tokens": [50364, 3601, 30197, 321, 362, 1612, 4743, 294, 257, 10378, 420, 732, 13, 492, 528, 281, 8947, 3601, 666, 50756], "temperature": 0.0, "avg_logprob": -0.09153978589554908, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.0005976426182314754}, {"id": 232, "seek": 143036, "start": 1438.1999999999998, "end": 1448.04, "text": " a symbolic form. We want the system to overfit for truth. And this is why it makes so much sense", "tokens": [50756, 257, 25755, 1254, 13, 492, 528, 264, 1185, 281, 670, 6845, 337, 3494, 13, 400, 341, 307, 983, 309, 1669, 370, 709, 2020, 51248], "temperature": 0.0, "avg_logprob": -0.09153978589554908, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.0005976426182314754}, {"id": 233, "seek": 143036, "start": 1448.04, "end": 1456.04, "text": " to store the knowledge in a symbolic system. One that can be edited, audited, curated, understood.", "tokens": [51248, 281, 3531, 264, 3601, 294, 257, 25755, 1185, 13, 1485, 300, 393, 312, 23016, 11, 2379, 1226, 11, 47851, 11, 7320, 13, 51648], "temperature": 0.0, "avg_logprob": -0.09153978589554908, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.0005976426182314754}, {"id": 234, "seek": 145604, "start": 1456.68, "end": 1463.08, "text": " We can cover the long tail by simply adding new notes to the knowledge graph. One we don't train", "tokens": [50396, 492, 393, 2060, 264, 938, 6838, 538, 2935, 5127, 777, 5570, 281, 264, 3601, 4295, 13, 1485, 321, 500, 380, 3847, 50716], "temperature": 0.0, "avg_logprob": -0.11954215332701966, "compression_ratio": 1.5875, "no_speech_prob": 0.0030276961624622345}, {"id": 235, "seek": 145604, "start": 1463.08, "end": 1469.48, "text": " to return knowledge with a certain probability to make stuff up on the fly. But one where we can", "tokens": [50716, 281, 2736, 3601, 365, 257, 1629, 8482, 281, 652, 1507, 493, 322, 264, 3603, 13, 583, 472, 689, 321, 393, 51036], "temperature": 0.0, "avg_logprob": -0.11954215332701966, "compression_ratio": 1.5875, "no_speech_prob": 0.0030276961624622345}, {"id": 236, "seek": 145604, "start": 1469.48, "end": 1475.72, "text": " simply look it up. And maybe not all of the pieces are in place to make this happen just yet.", "tokens": [51036, 2935, 574, 309, 493, 13, 400, 1310, 406, 439, 295, 264, 3755, 366, 294, 1081, 281, 652, 341, 1051, 445, 1939, 13, 51348], "temperature": 0.0, "avg_logprob": -0.11954215332701966, "compression_ratio": 1.5875, "no_speech_prob": 0.0030276961624622345}, {"id": 237, "seek": 145604, "start": 1476.36, "end": 1481.48, "text": " There are questions around identity and embeddings. How exactly do they talk with each other?", "tokens": [51380, 821, 366, 1651, 926, 6575, 293, 12240, 29432, 13, 1012, 2293, 360, 436, 751, 365, 1184, 661, 30, 51636], "temperature": 0.0, "avg_logprob": -0.11954215332701966, "compression_ratio": 1.5875, "no_speech_prob": 0.0030276961624622345}, {"id": 238, "seek": 148148, "start": 1481.8, "end": 1489.48, "text": " But there are good ideas to help with those problems. And knowledge graphs themselves should", "tokens": [50380, 583, 456, 366, 665, 3487, 281, 854, 365, 729, 2740, 13, 400, 3601, 24877, 2969, 820, 50764], "temperature": 0.0, "avg_logprob": -0.1379359310323542, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.0006771681364625692}, {"id": 239, "seek": 148148, "start": 1489.48, "end": 1496.2, "text": " probably also evolve. I want to make one particular suggestion here. Freebase, the Google Knowledge", "tokens": [50764, 1391, 611, 16693, 13, 286, 528, 281, 652, 472, 1729, 16541, 510, 13, 11551, 17429, 11, 264, 3329, 32906, 51100], "temperature": 0.0, "avg_logprob": -0.1379359310323542, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.0006771681364625692}, {"id": 240, "seek": 148148, "start": 1496.2, "end": 1502.76, "text": " Graph, Wikidata, they all have two kinds of special values or special statements. The third one is", "tokens": [51100, 21884, 11, 23377, 327, 3274, 11, 436, 439, 362, 732, 3685, 295, 2121, 4190, 420, 2121, 12363, 13, 440, 2636, 472, 307, 51428], "temperature": 0.0, "avg_logprob": -0.1379359310323542, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.0006771681364625692}, {"id": 241, "seek": 148148, "start": 1502.76, "end": 1509.16, "text": " the possibility to say that a specific statement has no value. Here, for example, we are saying that", "tokens": [51428, 264, 7959, 281, 584, 300, 257, 2685, 5629, 575, 572, 2158, 13, 1692, 11, 337, 1365, 11, 321, 366, 1566, 300, 51748], "temperature": 0.0, "avg_logprob": -0.1379359310323542, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.0006771681364625692}, {"id": 242, "seek": 150916, "start": 1509.24, "end": 1517.0800000000002, "text": " Elizabeth I has no children. The second special value is the unknown value. That is, we know", "tokens": [50368, 12978, 286, 575, 572, 2227, 13, 440, 1150, 2121, 2158, 307, 264, 9841, 2158, 13, 663, 307, 11, 321, 458, 50760], "temperature": 0.0, "avg_logprob": -0.08475840091705322, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.0006165955564938486}, {"id": 243, "seek": 150916, "start": 1517.0800000000002, "end": 1522.52, "text": " that there is a value for it, but we don't know what the value is. It's like a question mark in", "tokens": [50760, 300, 456, 307, 257, 2158, 337, 309, 11, 457, 321, 500, 380, 458, 437, 264, 2158, 307, 13, 467, 311, 411, 257, 1168, 1491, 294, 51032], "temperature": 0.0, "avg_logprob": -0.08475840091705322, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.0006165955564938486}, {"id": 244, "seek": 150916, "start": 1522.52, "end": 1528.8400000000001, "text": " the graph. For example, we don't know who Adam Smith's father is, but we know he has one. It could", "tokens": [51032, 264, 4295, 13, 1171, 1365, 11, 321, 500, 380, 458, 567, 7938, 8538, 311, 3086, 307, 11, 457, 321, 458, 415, 575, 472, 13, 467, 727, 51348], "temperature": 0.0, "avg_logprob": -0.08475840091705322, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.0006165955564938486}, {"id": 245, "seek": 150916, "start": 1528.8400000000001, "end": 1533.24, "text": " be one of the existing notes. It could be one that we didn't represent yet. We have no idea.", "tokens": [51348, 312, 472, 295, 264, 6741, 5570, 13, 467, 727, 312, 472, 300, 321, 994, 380, 2906, 1939, 13, 492, 362, 572, 1558, 13, 51568], "temperature": 0.0, "avg_logprob": -0.08475840091705322, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.0006165955564938486}, {"id": 246, "seek": 153324, "start": 1534.2, "end": 1541.64, "text": " My suggestion is to introduce a third special value. It's complicated. I usually get people", "tokens": [50412, 1222, 16541, 307, 281, 5366, 257, 2636, 2121, 2158, 13, 467, 311, 6179, 13, 286, 2673, 483, 561, 50784], "temperature": 0.0, "avg_logprob": -0.09541827213915088, "compression_ratio": 1.6289592760180995, "no_speech_prob": 0.0015978100709617138}, {"id": 247, "seek": 153324, "start": 1541.64, "end": 1547.48, "text": " laughing when I make this suggestion, but I'm really serious. It's complicated is what you would use", "tokens": [50784, 5059, 562, 286, 652, 341, 16541, 11, 457, 286, 478, 534, 3156, 13, 467, 311, 6179, 307, 437, 291, 576, 764, 51076], "temperature": 0.0, "avg_logprob": -0.09541827213915088, "compression_ratio": 1.6289592760180995, "no_speech_prob": 0.0015978100709617138}, {"id": 248, "seek": 153324, "start": 1547.48, "end": 1551.32, "text": " if the answer cannot be stated with the expressivity of your knowledge graph.", "tokens": [51076, 498, 264, 1867, 2644, 312, 11323, 365, 264, 5109, 4253, 295, 428, 3601, 4295, 13, 51268], "temperature": 0.0, "avg_logprob": -0.09541827213915088, "compression_ratio": 1.6289592760180995, "no_speech_prob": 0.0015978100709617138}, {"id": 249, "seek": 153324, "start": 1552.36, "end": 1558.84, "text": " This helps with maintaining the graph to mark difficult spots explicitly. This helps with", "tokens": [51320, 639, 3665, 365, 14916, 264, 4295, 281, 1491, 2252, 10681, 20803, 13, 639, 3665, 365, 51644], "temperature": 0.0, "avg_logprob": -0.09541827213915088, "compression_ratio": 1.6289592760180995, "no_speech_prob": 0.0015978100709617138}, {"id": 250, "seek": 155884, "start": 1558.9199999999998, "end": 1566.84, "text": " avoiding embarrassing wrong or flat out dangerous answers. Given the interaction with LLMs,", "tokens": [50368, 20220, 17299, 2085, 420, 4962, 484, 5795, 6338, 13, 18600, 264, 9285, 365, 441, 43, 26386, 11, 50764], "temperature": 0.0, "avg_logprob": -0.16347577021672174, "compression_ratio": 1.4663212435233162, "no_speech_prob": 0.00023781890922691673}, {"id": 251, "seek": 155884, "start": 1567.3999999999999, "end": 1575.1599999999999, "text": " this can in particular mark areas of knowledge where we say, don't trust the graph. Can we instead", "tokens": [50792, 341, 393, 294, 1729, 1491, 3179, 295, 3601, 689, 321, 584, 11, 500, 380, 3361, 264, 4295, 13, 1664, 321, 2602, 51180], "temperature": 0.0, "avg_logprob": -0.16347577021672174, "compression_ratio": 1.4663212435233162, "no_speech_prob": 0.00023781890922691673}, {"id": 252, "seek": 155884, "start": 1575.1599999999999, "end": 1580.12, "text": " train the LLM harder on this particular question and assign a few extra parameters for that?", "tokens": [51180, 3847, 264, 441, 43, 44, 6081, 322, 341, 1729, 1168, 293, 6269, 257, 1326, 2857, 9834, 337, 300, 30, 51428], "temperature": 0.0, "avg_logprob": -0.16347577021672174, "compression_ratio": 1.4663212435233162, "no_speech_prob": 0.00023781890922691673}, {"id": 253, "seek": 158012, "start": 1580.28, "end": 1588.84, "text": " But really, what we want to be able to say are more expressive statements. In order to build", "tokens": [50372, 583, 534, 11, 437, 321, 528, 281, 312, 1075, 281, 584, 366, 544, 40189, 12363, 13, 682, 1668, 281, 1322, 50800], "temperature": 0.0, "avg_logprob": -0.1531358282250094, "compression_ratio": 1.628440366972477, "no_speech_prob": 0.0005112498765811324}, {"id": 254, "seek": 158012, "start": 1589.4799999999998, "end": 1595.8, "text": " a much more expressive ground truth, to be able to say sentences like these,", "tokens": [50832, 257, 709, 544, 40189, 2727, 3494, 11, 281, 312, 1075, 281, 584, 16579, 411, 613, 11, 51148], "temperature": 0.0, "avg_logprob": -0.1531358282250094, "compression_ratio": 1.628440366972477, "no_speech_prob": 0.0005112498765811324}, {"id": 255, "seek": 158012, "start": 1595.8, "end": 1601.9599999999998, "text": " Jupiter is the largest planet in the solar system. That's what we are working on right now.", "tokens": [51148, 24567, 307, 264, 6443, 5054, 294, 264, 7936, 1185, 13, 663, 311, 437, 321, 366, 1364, 322, 558, 586, 13, 51456], "temperature": 0.0, "avg_logprob": -0.1531358282250094, "compression_ratio": 1.628440366972477, "no_speech_prob": 0.0005112498765811324}, {"id": 256, "seek": 158012, "start": 1602.52, "end": 1607.8799999999999, "text": " With abstract Wikipedia and leaky functions, we aim to vastly extend the limited expressivity", "tokens": [51484, 2022, 12649, 28999, 293, 476, 15681, 6828, 11, 321, 5939, 281, 41426, 10101, 264, 5567, 5109, 4253, 51752], "temperature": 0.0, "avg_logprob": -0.1531358282250094, "compression_ratio": 1.628440366972477, "no_speech_prob": 0.0005112498765811324}, {"id": 257, "seek": 160788, "start": 1607.88, "end": 1616.6000000000001, "text": " of Viki data so that complicated things become stateable. This way, we hope to provide a ground", "tokens": [50364, 295, 691, 9850, 1412, 370, 300, 6179, 721, 1813, 1785, 712, 13, 639, 636, 11, 321, 1454, 281, 2893, 257, 2727, 50800], "temperature": 0.0, "avg_logprob": -0.1260029853336395, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.0012447871267795563}, {"id": 258, "seek": 160788, "start": 1616.6000000000001, "end": 1625.72, "text": " truth for large language models. In summary, large language models are truly awesome. They are", "tokens": [50800, 3494, 337, 2416, 2856, 5245, 13, 682, 12691, 11, 2416, 2856, 5245, 366, 4908, 3476, 13, 814, 366, 51256], "temperature": 0.0, "avg_logprob": -0.1260029853336395, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.0012447871267795563}, {"id": 259, "seek": 160788, "start": 1625.72, "end": 1631.96, "text": " particularly awesome as an incredibly enabling UX tool. It's just breathtaking, honestly,", "tokens": [51256, 4098, 3476, 382, 364, 6252, 23148, 40176, 2290, 13, 467, 311, 445, 48393, 11, 6095, 11, 51568], "temperature": 0.0, "avg_logprob": -0.1260029853336395, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.0012447871267795563}, {"id": 260, "seek": 163196, "start": 1632.3600000000001, "end": 1639.08, "text": " things are happening which I didn't think possible in my lifetime. But they hallucinate.", "tokens": [50384, 721, 366, 2737, 597, 286, 994, 380, 519, 1944, 294, 452, 11364, 13, 583, 436, 35212, 13923, 13, 50720], "temperature": 0.0, "avg_logprob": -0.16231275134616427, "compression_ratio": 1.5866666666666667, "no_speech_prob": 0.022282496094703674}, {"id": 261, "seek": 163196, "start": 1639.72, "end": 1647.24, "text": " They need ground truth. They just make up stuff. They are expensive to train and to run.", "tokens": [50752, 814, 643, 2727, 3494, 13, 814, 445, 652, 493, 1507, 13, 814, 366, 5124, 281, 3847, 293, 281, 1190, 13, 51128], "temperature": 0.0, "avg_logprob": -0.16231275134616427, "compression_ratio": 1.5866666666666667, "no_speech_prob": 0.022282496094703674}, {"id": 262, "seek": 163196, "start": 1648.2, "end": 1652.52, "text": " They're difficult to fix and repair, which is great if you have to explain someone,", "tokens": [51176, 814, 434, 2252, 281, 3191, 293, 10535, 11, 597, 307, 869, 498, 291, 362, 281, 2903, 1580, 11, 51392], "temperature": 0.0, "avg_logprob": -0.16231275134616427, "compression_ratio": 1.5866666666666667, "no_speech_prob": 0.022282496094703674}, {"id": 263, "seek": 163196, "start": 1652.52, "end": 1656.68, "text": " sorry, I cannot fix your problem. The thing is making a mistake, but I don't have a clue how to", "tokens": [51392, 2597, 11, 286, 2644, 3191, 428, 1154, 13, 440, 551, 307, 1455, 257, 6146, 11, 457, 286, 500, 380, 362, 257, 13602, 577, 281, 51600], "temperature": 0.0, "avg_logprob": -0.16231275134616427, "compression_ratio": 1.5866666666666667, "no_speech_prob": 0.022282496094703674}, {"id": 264, "seek": 165668, "start": 1656.68, "end": 1664.04, "text": " make it better. They are hard to audit and explain, which in areas like finance and medicine is crucial.", "tokens": [50364, 652, 309, 1101, 13, 814, 366, 1152, 281, 17748, 293, 2903, 11, 597, 294, 3179, 411, 10719, 293, 7195, 307, 11462, 13, 50732], "temperature": 0.0, "avg_logprob": -0.1028828039401915, "compression_ratio": 1.6157205240174672, "no_speech_prob": 0.005301557946950197}, {"id": 265, "seek": 165668, "start": 1664.76, "end": 1670.2, "text": " They give inconsistent answers. They struggle with low resource languages.", "tokens": [50768, 814, 976, 36891, 6338, 13, 814, 7799, 365, 2295, 7684, 8650, 13, 51040], "temperature": 0.0, "avg_logprob": -0.1028828039401915, "compression_ratio": 1.6157205240174672, "no_speech_prob": 0.005301557946950197}, {"id": 266, "seek": 165668, "start": 1671.5600000000002, "end": 1677.4, "text": " And they have a coverage gap on long tail entities, which is not easily overcome. All of these", "tokens": [51108, 400, 436, 362, 257, 9645, 7417, 322, 938, 6838, 16667, 11, 597, 307, 406, 3612, 10473, 13, 1057, 295, 613, 51400], "temperature": 0.0, "avg_logprob": -0.1028828039401915, "compression_ratio": 1.6157205240174672, "no_speech_prob": 0.005301557946950197}, {"id": 267, "seek": 165668, "start": 1677.4, "end": 1684.28, "text": " problems can be solved with knowledge graphs, which is why I think that the future of knowledge", "tokens": [51400, 2740, 393, 312, 13041, 365, 3601, 24877, 11, 597, 307, 983, 286, 519, 300, 264, 2027, 295, 3601, 51744], "temperature": 0.0, "avg_logprob": -0.1028828039401915, "compression_ratio": 1.6157205240174672, "no_speech_prob": 0.005301557946950197}, {"id": 268, "seek": 168428, "start": 1684.28, "end": 1691.6399999999999, "text": " graphs is brighter than ever, especially thanks to a world that has large language models in it.", "tokens": [50364, 24877, 307, 19764, 813, 1562, 11, 2318, 3231, 281, 257, 1002, 300, 575, 2416, 2856, 5245, 294, 309, 13, 50732], "temperature": 0.0, "avg_logprob": -0.14819664425320095, "compression_ratio": 1.5054945054945055, "no_speech_prob": 0.005639310926198959}, {"id": 269, "seek": 168428, "start": 1693.3999999999999, "end": 1697.6399999999999, "text": " Thank you for your attention. Thanks to all the people who helped me clarify my thinking", "tokens": [50820, 1044, 291, 337, 428, 3202, 13, 2561, 281, 439, 264, 561, 567, 4254, 385, 17594, 452, 1953, 51032], "temperature": 0.0, "avg_logprob": -0.14819664425320095, "compression_ratio": 1.5054945054945055, "no_speech_prob": 0.005639310926198959}, {"id": 270, "seek": 168428, "start": 1697.6399999999999, "end": 1706.44, "text": " around this topic. And if you have any questions, feel free to put them in the comments.", "tokens": [51032, 926, 341, 4829, 13, 400, 498, 291, 362, 604, 1651, 11, 841, 1737, 281, 829, 552, 294, 264, 3053, 13, 51472], "temperature": 0.0, "avg_logprob": -0.14819664425320095, "compression_ratio": 1.5054945054945055, "no_speech_prob": 0.005639310926198959}], "language": "en"}