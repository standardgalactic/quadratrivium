start	end	text
0	4840	I was invited by François Chavre to keynote one of the days of the Knowledge Graph
4840	11440	Conference in May 2023 in New York, New York. This is a post-conference recording
11440	18120	of the talk. François asked me what topic I want to talk about. I said wiki data
18120	23040	and wiki functions and he said, do you have something else to talk about? You
23040	27800	talked about it the last time you were here at the conference in 2019 and I
27800	35960	said there's one topic everyone talks to me about and this is what about
35960	42800	knowledge graphs and large language models and he said perfect do that and
42800	46800	I thought great given how much I have been talking and thinking about it I
46800	53240	probably should have a good idea what I want to say by May. Well it's May now and
53400	59840	I'm still full of doubts about what to say. I recently stumbled upon this quote
59840	64640	looking at books for my daughter and it gives me some hope for the talk today.
64640	72680	When in doubt you can't be wrong. So let this be my first disclaimer for today. I
72680	78080	am most certainly in doubt. I don't know what the future of knowledge graphs is
78080	83120	but I have been talking about this for a decade now for a lot of very smart
83120	88720	people so I hope that what I say will prove your time. There is one thing I
88720	95040	have no doubts about. Something big is happening. See this is from the book
95040	100320	Diffusion of Innovation. It shows how technology is adopted. Well my brother
100320	104840	who lives in a small village of 180 people on a Croatian island he is
104840	111480	roughly here on this chart but he called me last fall to talk about stable
111480	116480	diffusion the image generation model. Chatchity P a large language model
116480	124520	reached 100 million users two months after launch. This is a record in how
124520	128760	fast the technology is spreading. If there's one thing we don't need to be
128760	134600	in doubt about is that these technology are having a large impact on the world.
134600	139680	Last year in September that's two months before the release of Chatchity P a
139800	143880	number of researchers and practitioners in the knowledge graph field met in
143880	148480	Dachstuhl in Germany. Our goal was to discuss the role of knowledge
148480	152840	engineering in the 21st century like we were almost under a kind of shock
152840	156920	talking about it processing and trying to figure out what it really means for
156920	162360	us. I'm trying to distill some of the results from that seminar and also from
162360	167040	any other conversations I had in this talk but you can also just go and read
167040	175080	the report directly. My second disclaimer nothing in this talk is generated by an AI.
175080	180680	And this explicitly marked so or you see a screenshot from an AI. This is not one
180680	184680	of those talks where we have a reveal at the end that all of this was generated.
184680	192800	And finally the last disclaimer this is a focused talk it is only about knowledge
192800	198080	graphs and large language models. It is not about the ethical questions of
198080	202960	large language models. It's not about blunders of current LLMs. It's not of the
202960	209320	that those systems will have about the impact on jobs and the economy. It's not
209320	213360	about questions of copyright or whether LLMs are a path to destruction of the
213360	217760	human race. All of these questions are interesting and deserve their own talks
217760	224840	and I will not talk about them today. So what makes me believe that there is a
224840	229840	future for knowledge graphs in the world of large language models? About six or
229840	234640	seven years ago I was working at Google on the knowledge graph. Back then the
234640	239600	knowledge graph wasn't necessarily a generic term yet. It was a big question
239600	244640	Google had to ask itself. The knowledge graph costs a lot of resources, a lot of
244640	249200	money, a lot of manpower to run and maintain. A lot of people were working on
249200	253640	the knowledge graph. Is all of this work going to be obsolete? Should we stop
253640	259040	investing in that? I give you the answer that I came up with back then. Let's
259040	265640	think first principles. So what is a knowledge graph? A knowledge graph represents
265640	270400	things and the relations usually stored in a graph database. WikiData is an
270400	274400	example. WikiData is the largest publicly available free knowledge graph that
274400	279920	anyone can edit. What is a large language model? Model here means that we have a
279920	284920	neural network, nodes connected with each other with weights. The weights in
284920	289000	this neural networks are trained on some input and desired outputs in order to
289000	294160	create good outputs on novel inputs. They are language models because they are
294160	299560	trained on natural language texts and it is a large language model because it is
299560	304960	trained on lots and lots of natural language texts. GPT is a family of large
304960	310080	language models which have been created by OpenAI. GPT-4 is the current version
310080	313880	but OpenAI has not published much data about it. So the following data is for
313880	322400	GPT-3. GPT-3 has 96 such layers in the picture and it had 175 billion
322400	329120	parameters. That is weights. As a large language model, GPT-3 has been trained to
329120	334920	provide appropriate answers to a given prompt to our input. So for example, given
334920	338640	the following input or prompt, who created the School of Athens.
338640	344680	JGDP answers, the School of Athens is a famous fresco painted by the Italian
344680	350520	Renaissance artist Raphael. It was painted between 1509 and 1511 as part of a
350520	354920	commission for the Vatican's Apostolic Palace in Rome. The fresco is widely
354920	361320	regarded as one of the greatest works of art of the high Renaissance. This answer
361320	365640	is not only correct, it is brilliant. There's absolutely nothing to complain
365640	372560	about. It gives context, it answers the question. It's great. If you go to Google
372560	377200	and ask the same question, the answer is the same, Raphael. But the context is even
377200	384240	more amazing. A picture from Raphael, his biography, data about him, who influenced
384240	389040	him, other artists that are being searched for. The answer is just beautiful and
389040	395320	amazing. You can also go to Wikidata and write a sparkle query, asking for the
395320	401760	creator of the School of Athens. Again, you will get the result, Raphael. Not as
401760	411480	pretty as Google, not as much context as with JGDP. But here's one thing, JGDP took
411560	416720	about five seconds to answer my question. Admittedly, it gave a lot of context I
416720	422720	didn't ask for, but even just passing the question took close to a second. Google
422720	427200	took half a second, including passing, but it also got a lot of great context and
427200	429960	gave me results from the search index, even though they didn't make it to a
429960	436360	screenshot. Wikidata took about a quarter of a second to produce the answer, and
436360	441800	that's not all. JGDP is running on the fastest and most expensive hardware you
441800	446760	can buy. Google is running on the fastest and most expensive hardware you
446760	452040	cannot buy. And the Wikidata query service is running abandonware on a
452040	459000	server somewhere. And if you think about this, it is not surprising. The query I
459000	463560	asked had six tokens of input. A token is roughly a word for a natural language
463560	468600	model. And the answer produced 60 tokens. Now, if the answer would just have been
468600	473400	Raphael, it would still have been at least two tokens. Every single token runs for
473400	480120	the 96 layers of JGDP, branching out to 175 billion weights and multiplying
480120	486840	matrices and soft maximum results. Whereas in Wikidata, we look up an item out
486840	491720	of 100 million and find a key out of 10,000 to get the results values. Those
491720	498040	are both logarithmic operations. It's just much cheaper. Given this paper,
498040	502920	getting and hosting your own copy of Wikidata, 1,000 Wikidata queries, like
502920	509240	the ones I just asked, would cost me about 14 cents in the cloud. The authors thought
509240	512680	this was too expensive and bought their own hardware to make it even cheaper.
512680	518760	Whereas if I look at Azure pricing for GPT-4, a thousand queries, as asked before,
518760	528440	would cost $7.56. So that's 14 cents compared to more than $7, so a factor of 50.
528440	534600	50 times more expense can make a difference. Now, to make it very clear, I don't know
534600	538680	how robust these numbers are. I would love to see more robust numbers and I expect
538680	544200	the cost for inference to go down. But even Sam Altman, the CEO of OpenAI,
544200	549720	the company making JetGDP, describes the compute cost of JetGDP as eye-watering.
550760	554600	And don't forget, he made a really good deal with Microsoft about running it on Azure.
555880	560040	John Hennessey, chairman of Google and former computer science professor at Stanford,
560040	565080	so he really knows what he's talking about, said that running at JetGDP, like Google,
565080	571800	would be 10 times more expensive. And that's just talking about the inference costs. Each
571800	577320	of these models also need to be trained. And current state-of-the-art language models cost
577320	584600	millions of dollars to train. For GPT-4, the cost was given as more than $100 million.
586600	593240	The cost for GPT-3 was around $4 million. Good news is, you probably don't have to do this
593240	598680	training. But you can just hopefully reuse an existing open source model that you can find
598680	604200	due to your task. This will be considerably cheaper to train a model from the foundation on.
604920	608680	But not for inference. The same cost that we saw earlier remains.
610920	615800	Here's some interesting new thoughts. Some folks, like Sam Altman, think that the age of
615800	621720	large language models is already over. OpenAI says it's because of diminishing returns of further
621720	626680	reading. Guess what? After reading a million books worth of text, you don't seem to learn too much
626680	634840	new stuff. He said that OpenAI is not working on GPT-5, but that they want to explore new ideas.
634840	639880	And that's great. Because with models that are more readily available out there, such as Metas
639880	647000	Llama, we can see new ideas happening very fast. For example, when Llama was leaked within days,
647000	652840	someone managed to run it on consumer hardware. A few days later, we got it running on a phone,
652840	660120	and then a Raspberry Pi. It was very slow, but it ran. And by the way, as ridiculous as that sounds,
660120	664040	this still means that these giant models are more expensive to run than a knowledge graph.
664840	669160	And really, it's not just cost, right? There are a few challenges that large
669160	673800	language models need to overcome. We need new ideas for those. Let's take a look.
675720	681720	Here's one thing that surprised me a lot. A few weeks ago, I stumbled into one of those Wikipedia
681720	688120	rabbit holes where I got temporarily obsessed with figuring out one specific fact. The correct
688120	695160	place of birth of a not-too-well-known Croatian actress, Anna Begovic. I was surprised that
695160	700120	Google and Wikipedia Wikidata were giving different answers, so I tried to figure out
700120	706920	the truth and fix it. Here's a screenshot of Google answering the question. Begovic was born in
706920	713960	Terpan. Wikipedia used to say split. After a bit of hunting through sources, I figured out that it
713960	721720	mostly likely is Terpan, though. And that a lot of the sources were copying from Wikipedia and Wikidata
721720	728360	and became contaminated by Wikipedia and Wikidata. This is an example of our knowledge ecosystem being
728440	733800	substantial danger, by the way, and that long before we have LLM syndemics.
735880	742360	Now, if you go to Bing Chat, which is powered by OpenAI's GPT, but has access to web search
743080	748760	and ask for the place of birth of Anna Begovic, it also answers me Terpan, which is great,
748760	754440	and it gives me free sources for that answer. It's just very disappointing that if you follow
754440	759080	one of the references to Wikipedia, you will actually find it says split.
760200	765480	When I asked ChatGDP instead of Bing Chat, I get a different answer. It answers split.
766200	771560	This is not too surprising. After all, Wikipedia was claiming split until just a few weeks ago,
771560	777720	and ChatGDP was trained on a 2021 copy of the web. Corrections to Wikipedia done in 2023
777720	781560	would not show up. Split is totally expected as an answer here.
782360	787160	What is interesting, though, is if I ask the same question in Croatian,
788280	796280	I get a different wrong answer. Zagreb. Now, that's an interesting answer,
796280	804040	because it demonstrates us two things. First, knowledge in ChatGDP seems to be not stored in
804040	810200	a language independent way, but is stored within each individual language. Depending on the language
810200	816920	I ask the question in, I receive a different answer. Also, the English answer at least has
816920	823640	support in the training data. Wikipedia did say split. The Croatian answer, where does Zagreb come
823640	829400	from? I can find a single source that says that Anna Begovic was born in Zagreb.
829640	837560	But here's the thing. What if ChatGDP is actually just guessing, it's just making it up?
838600	843400	What is the probability of Zagreb given the prior of Croatian actress?
845000	851720	In order to figure that out, I want to check Wikipedia. I ask ChatGDP to give me a sparkle
851720	858520	query for Croatian actors in the places of birth. The query it returns to me is good enough for our
858520	865640	purpose. I can just copy and paste it. It is subtly wrong if you read it in detail. It does not ask
865640	871720	for Croatian actors, but for actors born in the place in Croatia. But again, good enough for our
871720	881800	purposes. But still, this is fascinating. ChatGDP got out of the box all of the QIDs right in this
881800	889000	query. For Croatia, for actor, it got a property ID right. It can make sparkle queries that are
889000	897000	syntactically right. And all of that with zero shot extra training. There was no look up on the web.
897000	905240	ChatGDP just knows these QIDs by heart. In fact, you can totally ask ChatGDP to make you a table
905240	911240	of all the countries of the European Union and their QIDs. It has a lot of QIDs memorized.
911800	919400	Just you never know if maybe one of them is wrong or not. The query can be copied and pasted right
919400	926520	into the Wikipedia query service. And it runs giving 445 answers. And you can already see in the
926520	934120	screenshot that the first few are all in Zagreb. So now we can see that of the 445 Croatian actors
934120	941080	for place of birth, 154 are born in Zagreb, about a third. This gives a pretty good conditional
941080	948120	probability for the birthplace of Croatian actors. ChatGDP is guessing the place of birth
948120	954840	for Anna Begovic in Croatian, even though it knows it in English. This leads me to something my
954840	961720	manager at Google used to say. You can machine learn Obama's birthplace every time you need it,
961720	965160	but it costs a lot and you're never sure it is correct.
965400	972360	Another interesting observation is that GPT isn't particularly good with knowing what it knows.
973720	980920	For example, here we are asking for cities with mayors born after 1998. I wanted to ask something
980920	986440	that I expected to be not trivially Google-able, where there wouldn't be a listicle or table on
986440	994920	the map. So ChatGDP correctly points out that that would make them younger than 23 at its cutoff
994920	1004040	80 date of 2021. So it is unlikely that anyone would be mayor at that age. Asking Bing Chat,
1004040	1007880	it also says that it doesn't know anything about mayors born after 1998.
1010360	1015400	Let's check Wikidata. I again used ChatGDP to help me write a query, although this time I
1015400	1022600	had to make a few fixes. And indeed, Wikidata has an answer. It tells me about Christian von
1022600	1028360	Waldenfelds, who was born in April 2000, mayor of Lichtenberg in Bavaria, Germany.
1029720	1034920	He became mayor in 2020, well before the cutoff date of Chaterity, by the way.
1037000	1041400	This is no endorsement of his politics or his platform he is running, by the way.
1043960	1049800	Now that we know the answers, we can guide Bing Chat and get the mayor of Lichtenberg and his age.
1049800	1053720	And obviously this answer is inconsistent with its previous answer,
1053720	1058840	but ChatGDP or BingJDP are both completely unaware of this inconsistency and don't care.
1060520	1065480	Large language models are not yet graded being consistent, whether about individual facts
1065480	1073000	or across different languages. They are also not always very good at math. But even if they were
1073000	1079800	good at math, we have to answer the same question that we do for looking up facts in a knowledge
1079800	1090440	graph. Why would you ever use 96 layers, 175 billion parameters model, to do multiplication?
1091080	1094280	When that's something you can do in a single operation on your CPU.
1095640	1102200	Why internalized knowledge in an LLM if you can externalize it in a graph store and look it up
1102280	1109240	when needed. Don't get bedazzled by the capabilities of large language models.
1110920	1117640	Autoregressive transformer models such as ChatGDP are touring complete and they are just a very
1117640	1124360	expensive reiteration of touring's carpet. You can do everything with them, but it doesn't mean you
1124360	1131400	should. Use LLMs where they are efficient and use other things where they are better.
1133160	1139240	One possible solution to this capability gap are so-called augmented language models.
1140280	1146040	Toolform are being a particularly well-known example. Or for ChatGDP, that's what ChatGDP
1146040	1152360	plugins are there for. The idea is that we can enrich large language models with additional
1152360	1158280	systems which are good and efficient at specific tasks, such as math or other functions,
1158440	1164120	from wiki functions, or looking up facts or query results in a knowledge graph such as wiki data.
1165240	1171800	I mean, given that ChatGDP already can, zero shot, create queries against wiki data,
1171800	1174600	there isn't that much to do to make them work together.
1177080	1183240	Some folks think that some mapping of strategic nodes into a knowledge graph with embeddings,
1183880	1187320	we can easily connect a knowledge graph directly to a large language model.
1188520	1192920	But we don't even need to do that, we can just use the Sparkle query generation ability
1192920	1200280	directly and ask queries against wiki data. And not only can we connect LLMs to a knowledge
1200280	1206360	graph, but also to a repository of functions, such as wiki functions. Both knowledge graphs and
1206360	1214120	functions would be tools the LLM can learn to use. There is work in trying to understand
1214120	1218680	how knowledge is stored in the parameters of a large language model. And when we look at this
1218680	1228280	work, we start to understand why large language models are large. Do they really have to be this
1228280	1237880	large? Let's compare with stable diffusion one. Stable diffusion one is a text to image generator.
1237880	1244040	It has to understand natural language prompts, just as GPT does. It can make an image out of
1244040	1248760	basically any prompt. It can also generate the image of a good number of celebrities.
1248760	1255400	Here, for example, I'm asking for the Pope, Geleta Thunberg, Idris Elba, Michelle Yeo,
1255400	1260600	Helen Mirren, and Yanle Kuhn to explain knowledge graphs with the help of a whiteboard.
1262360	1271000	So GPT-3 had 175 billion parameters. How many parameters do you think are in stable diffusion
1271000	1279880	one, knowing all these people being able to generate them? 890 million parameters.
1280760	1289960	Now, I think 890 million is a lot. But GPT-3 is about 200 times larger than stable diffusion one.
1291240	1296600	And it's no surprise. Think of it. All the knowledge that we were using for questions
1296600	1302360	answering so far. The embedded QIDs, what are the member countries of the EU,
1302360	1309640	the place of birth of individual people. I mean, if it has NABegovitch, I'm sure GPT-3 knows the
1309640	1315640	place of birth of millions of individuals. All this taught in many different languages.
1317080	1321320	All of this is taught in those hundreds of billions of parameters.
1321800	1326600	You don't need that for text generation. But you need it if you want to answer all
1326600	1333160	these questions we have been asking. And indeed, Metas Lama, which came out a few weeks ago,
1333160	1340680	is considerably smaller than GPT-3, about 25 times smaller. But it seems to be
1340680	1347400	rather competitive in terms of language understanding and fluency. So yes, we could
1347880	1354920	internalize all the 1.4 billion statements in Wikidata into a large language model.
1355880	1361560	But what if we go the other way around and try to externalize the knowledge model instead?
1362600	1367480	If we leave the language model to deal with language, but push the knowledge
1368120	1373960	to a knowledge graph or a different knowledge model, in a world where language models can
1373960	1381960	generate infinite content, knowledge becomes valuable. And that takes us back to Jamie Taylor's
1381960	1388520	rule. We don't want to machine learn Obama's place of birth every time we need it. We want to
1388520	1395480	store it once and for all. And that's what knowledge graphs are good for. To keep you
1395480	1401800	valuable knowledge safe. I am of the strict belief there is no reason to ever again
1402440	1409720	manually enter the place of birth for Anna Begovic. This makes knowledge for Wikidata
1409720	1417320	both valuable and a public good. The knowledge graph provides you with the ground truth for your
1417320	1424360	language models. By the way, the other way around is also true. Large language models can be an
1424360	1430360	amazing tool to speed up the creation of a knowledge graph. They are probably the best tool for
1430360	1438200	knowledge extraction we have seen developed in a decade or two. We want to extract knowledge into
1438200	1448040	a symbolic form. We want the system to overfit for truth. And this is why it makes so much sense
1448040	1456040	to store the knowledge in a symbolic system. One that can be edited, audited, curated, understood.
1456680	1463080	We can cover the long tail by simply adding new notes to the knowledge graph. One we don't train
1463080	1469480	to return knowledge with a certain probability to make stuff up on the fly. But one where we can
1469480	1475720	simply look it up. And maybe not all of the pieces are in place to make this happen just yet.
1476360	1481480	There are questions around identity and embeddings. How exactly do they talk with each other?
1481800	1489480	But there are good ideas to help with those problems. And knowledge graphs themselves should
1489480	1496200	probably also evolve. I want to make one particular suggestion here. Freebase, the Google Knowledge
1496200	1502760	Graph, Wikidata, they all have two kinds of special values or special statements. The third one is
1502760	1509160	the possibility to say that a specific statement has no value. Here, for example, we are saying that
1509240	1517080	Elizabeth I has no children. The second special value is the unknown value. That is, we know
1517080	1522520	that there is a value for it, but we don't know what the value is. It's like a question mark in
1522520	1528840	the graph. For example, we don't know who Adam Smith's father is, but we know he has one. It could
1528840	1533240	be one of the existing notes. It could be one that we didn't represent yet. We have no idea.
1534200	1541640	My suggestion is to introduce a third special value. It's complicated. I usually get people
1541640	1547480	laughing when I make this suggestion, but I'm really serious. It's complicated is what you would use
1547480	1551320	if the answer cannot be stated with the expressivity of your knowledge graph.
1552360	1558840	This helps with maintaining the graph to mark difficult spots explicitly. This helps with
1558920	1566840	avoiding embarrassing wrong or flat out dangerous answers. Given the interaction with LLMs,
1567400	1575160	this can in particular mark areas of knowledge where we say, don't trust the graph. Can we instead
1575160	1580120	train the LLM harder on this particular question and assign a few extra parameters for that?
1580280	1588840	But really, what we want to be able to say are more expressive statements. In order to build
1589480	1595800	a much more expressive ground truth, to be able to say sentences like these,
1595800	1601960	Jupiter is the largest planet in the solar system. That's what we are working on right now.
1602520	1607880	With abstract Wikipedia and leaky functions, we aim to vastly extend the limited expressivity
1607880	1616600	of Viki data so that complicated things become stateable. This way, we hope to provide a ground
1616600	1625720	truth for large language models. In summary, large language models are truly awesome. They are
1625720	1631960	particularly awesome as an incredibly enabling UX tool. It's just breathtaking, honestly,
1632360	1639080	things are happening which I didn't think possible in my lifetime. But they hallucinate.
1639720	1647240	They need ground truth. They just make up stuff. They are expensive to train and to run.
1648200	1652520	They're difficult to fix and repair, which is great if you have to explain someone,
1652520	1656680	sorry, I cannot fix your problem. The thing is making a mistake, but I don't have a clue how to
1656680	1664040	make it better. They are hard to audit and explain, which in areas like finance and medicine is crucial.
1664760	1670200	They give inconsistent answers. They struggle with low resource languages.
1671560	1677400	And they have a coverage gap on long tail entities, which is not easily overcome. All of these
1677400	1684280	problems can be solved with knowledge graphs, which is why I think that the future of knowledge
1684280	1691640	graphs is brighter than ever, especially thanks to a world that has large language models in it.
1693400	1697640	Thank you for your attention. Thanks to all the people who helped me clarify my thinking
1697640	1706440	around this topic. And if you have any questions, feel free to put them in the comments.
