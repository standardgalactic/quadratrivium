So this was a huge week for AI news.
Let's catch up on some of the big things that were happening.
There's a lot of rumors about the next big model coming from OpenAI.
Somebody dropped this screenshot of GPT 4.5, which was very suspicious.
But don't worry, Sam Altman is on the case.
He's going to clarify the situation for us.
Somebody asked some GPT 4.5 leak legit or no.
Sam Altman answers, nah, which again, people are a little bit confused.
Like, what does that mean?
The image is fake or the news about the new model is fake.
Either way, we'll know soon enough.
Google deep mind drops this paper.
Mathematical discoveries from program search with large language models.
And here they introduce fund search, which actually stands for searching in the function space.
There's a paper in nature that kind of goes over it.
The big headline news here is this is the first time anyone has shown that an
LLM based system can go beyond what was known by mathematicians and computer scientists.
It's not just novel.
It's more effective than anything else that exists today.
So this is yet another example of AI coming out of deep mind that is rapidly expanding
the human scientific knowledge.
In this case, it's mathematics.
In the case of Alpha fold, it's expanding our understanding of 3D structures of proteins.
The GNOME project is discovering new materials.
And a lot of this is done autonomously.
So it's AI and robots kind of doing their own thing to create this humans are more and more
out of the loop.
And just yesterday, OpenAI drops this practices for governing agentech AI systems.
In other words, AI systems that have agency, they have their own goals and also have the
ability to pursue those goals.
Agentech AI systems, AI systems that can pursue complex goal with limited direct supervision
are likely to be broadly useful if we can integrate them responsibly into our society.
And here they have this paper that talks about things that we need to think about
as a society as humanity about how to safely release these things into the wild.
They quickly define what they mean when they say AI agents.
And I think they actually invent a brand new word, agentechness.
We define the degree of agentechness in a system as the degree to which a system can
adaptably achieve complex goals in complex environments with limited direct supervision.
Agentechness as defined here thus breaks down into several components.
Goal complexity, environmental complexity.
So for example, to what extent are they cross domain multi stakeholder require operating over
a long time horizons and involve the use of multiple external tools, then adaptability,
how well can the system adapt and react to novel or unexpected circumstances and independent
execution to what extent can the system reliably achieve its goals with limited human intervention
or supervision.
So reading through this paper to me, it seems like it's mostly a way to kind of brainstorm
and get all the potential dangers out on paper, as well as some questions that we have to answer
before we allow these agents to autonomously go and act on our behalf, whether they're able to do it
when is approval required.
They talk about autonomous weapons, for example, default behaviors.
So for example, they talk about how if you're getting it to do your grocery shopping for you
or to complete some chores for you, the default should be, for example, to spend no money.
So only spend money when you have to don't just default to buying the most expensive thing right
away.
And here, interestingly, they talk about the shifting of offense defense balance.
So some tasks may be more susceptible to automation by agents than others.
So for example, in the cyber security domain, human monitoring and incident response is still
key to cyber attack mitigation.
So the feasibility of such human monitoring is predicated on the fact that the volume of attacks
is similarly constrained by the number of human attackers.
So if you have 10 guys trying to hack into somewhere and you have 10 cyber security experts
kind of defending and monitoring for those attacks, if one side increases their offense
abilities, the defense similarly can increase their defense abilities, in general, obviously.
But what happens if the offense can now use a million smart AI agents that are able to
carry out these tasks autonomously without human supervision?
Is there a limit to how many they can deploy at one time?
So in that world, that would kind of destroy cyber security, right?
But on the other hand, conversely, if agentic AI systems make monitoring a response cheaper
than producing new cyber attacks, the overall effect would be to make cyber defense cheaper
and easier, which is an interesting point, right?
Because it's kind of been this this race, you know, if the offense improves, the defense improves.
But what if we find that in the future with these AI agents, either the offense or the defense is
significantly easier and just more powerful?
That either means that the ability to hack into systems skyrockets or is more or less
completely negated?
I don't think we're quite sure yet which future we're going to live in.
So they're saying, well, it's very difficult to anticipate the net effect of these sort of AI
adoption dynamics.
And saying it behooves actors to pay close attention in identifying which equilibrium
assumptions no longer hold.
I mean, this is where kind of the conversation about AI drones really kind of comes into play.
Specifically, I mean, attack drones that are disconnected from any human operator.
What are the defense against those drones?
Does attacking them being on the offense, does that become the much more effective tactic
than waiting and trying to defend?
If that is indeed the case, then the world kind of becomes a more of a scary place.
And right around the same time that opening, I released this paper, they released another one
called weak to strong generalization.
And so it's part of their super alignment.
How do we make super intelligence safe?
And so in this paper, they're talking about an approach that seems to be having some results.
And so traditionally, you can think of this as a human supervisor.
So this dotted line, that's sort of the human level intelligence, right?
So the supervisor, the smart human is teaching the AI that is not as smart as the human.
And then super alignment, how that would look like is this idea that this human who is not as
smart as the super intelligent AI, you know, trying to teach it and tell it what to do.
This is what a lot of people have concerns with is like, how do you control something
that is far smarter than yourself?
And their approach is we start right now, before they get past the human level of intelligence,
are we able to train a small model to supervise a larger model?
And so at the end, at the very end of this paper, this is like page 47 out of 49,
they have this high level plan.
So Lakey and Sutscubar proposed the following high level plan, which they've adopted.
So one, once we have a model that is capable enough that can automate machine learning,
like AI almost training other forms of AI, more advanced forms of itself,
and so once it's able to automate that, and in particular, alignment research,
our goal will be to align that model well enough that it can safely and productively
automate alignment research.
So we're trying to create an AI that will be able to research how to create safe AI
as it gets sort of stronger and better and more beyond our understanding.
And so we will align this model using our most scalable techniques available are LHF,
reinforcement learning, human feedback, constitutional AI, scalable oversight,
adversarial training, and this new approach, the focus of this paper, weak to strong
generalization techniques, we will validate that the resulting models align using our best
evaluation tools available.
For example, red teaming.
So that's when a group of people try to do their best to try to break that model to try
to get it to do something bad and interpretability, which is our attempt to kind of understand
what it's thinking, what its thought process are, can we monitor its quote unquote thoughts
somehow.
And then they're saying number four, using a large amount of compute, we will have the
resulting model conduct research to align vastly smarter superhuman systems.
We will bootstrap from here to align arbitrarily more capable systems.
So and we've talked about this before so that there's this idea that we can take these
smaller models and by using just vast quantities of compute almost get a glimpse into what
that model would look like if it was much stronger.
And of course that costs a lot of money.
We need to have a lot of equipment to be able to do that.
But it seems to be like almost a way to get those high level answers without necessarily
creating a model that is that high level, we're able to match the capabilities over
much bigger model with a smaller model by, you know, overclocking it, giving it more
compute, you know, expanding more resources on that, that model.
So it's interesting because we've heard some of the stuff being discussed before,
not officially, but it seems like certain leaks, certain hints at what's happening.
These ideas have been out there before runway ML.
So you've probably seen some of the videos this thing is able to generate.
So it's one of the more well known AI video generation platform.
And what they're announcing is that they're starting a long term research effort around
what they call general world models.
And we've talked about this idea before that AI these neural nets that they build
these mental models of the world.
So when we feed them information, and then we ask them for to make certain inferences
certain guesses about what's going to happen.
So for example, if we feed them tons of text, then we ask them to predict what comes next
in a sentence, how we feed them a bunch of images.
And then we have them either recognize certain images or create certain images.
What we find is they build these sort of mental models about how to do that.
So for example, if you give them a lot of 2D images, some of the latest research is showing
that they build almost like a mental model of the 3D world.
So they kind of start understanding the 3D space in those 2D images.
We didn't give them any data about like the depth of field or, you know,
how far away certain things are, but they do start to gain some quote unquote understanding
about how to position objects in the 3D space, what's further away, what's closer, etc.
And so the fact that runway ML is doing that is very interesting.
So they're saying to build general world models, there are several open research challenges
that we're working on.
For one, these models will need to generate consistent maps of the environment and the
ability to navigate into rack nose environments, they need to capture not just the dynamics of
the world, but the dynamics of its inhabitants, which involves also building realistic models
of human behavior.
So to me, it almost sounds like what they're talking about is having these AIs build almost
a simulation of the world.
And then within that sort of simulation of the world, almost like taking a camera and
recording something, and then that becomes the video that then is extracted and becomes
this AI video.
So currently they have their Gen 2 system, which I've showcased some of the stuff that
you can do with it.
They're saying in order for Gen 2 to generate realistic short videos, it has developed some
understanding of physics and motion.
However, it's still very limited in its capabilities, struggling with complex camera or
object motions, amongst other things.
If it wants to generate an image of a bird flying, it has to have some understanding
of how that bird moves, the physics that are interacting with it, how the point where the
camera is, how it's picking up the movement of that bird, etc.
Very excited to hear where they're going to be going with this.
And I'm glad more companies are investing resources into this.
And this piece of news was very interesting.
So OpenAI is partnering with some news outlets to sort of pull real-time information from
them.
So when you ask ChatGPT for real-time news, it's able to pull from those news sources and
then present those to you in real time.
And then of course, they'll include attribution and links to full articles for transparency,
etc.
This might be how we get our news in the future.
We're not going to go to Google or the newspaper or TV or even a specific website.
Where's it going to ask our favorite chatbot?
Hey, what's the news today?
Mid Journey Alpha.
Mid Journey launches a brand new thing, Mid Journey Alpha.
We're able to create stuff faster, easier, and breaks it down by subjects, descriptors.
You can have it look like known artists, etc.
So this is kind of the next step in AI art generation that gives you a lot more control
and more precision in how to create your images.
If you've generated, I believe it's over 10,000 images with Mid Journey,
this should be available to you soon.
Google DeepMind drops Image in 2.
They're most advanced text to image technology.
And some of these images are extremely realistic.
I think I would be hard pressed to find any faults with it.
I mean, there's certain things that I could point to maybe this area is a little bit weird
kind of the area here, but I could not call this an AI generated image.
There's nothing here to really give it away.
So that's it for today, but I think there's a lot more to come.
I think this is kind of the calm before the storm.
I think before December is out, we're going to see some pretty incredible things.
Just a hunch.
My name is Wes Roff and thank you for watching.
