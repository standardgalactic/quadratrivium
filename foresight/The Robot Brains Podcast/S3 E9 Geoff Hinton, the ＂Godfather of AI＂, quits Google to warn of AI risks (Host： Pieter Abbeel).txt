Our guest today is Jeffrey Hinton.
Over the past 10 years, AI has experienced breakthrough after breakthrough after breakthrough.
Underneath all of these breakthroughs is one single subfield of artificial intelligence,
deep learning.
Jeff was at the origins of so many of the pioneering breakthroughs in deep learning
that he is often referred to as the godfather of artificial intelligence.
His work has been cited over half a million times.
That means there is half a million and counting other research papers out there that build
on top of his work.
Jeff's work has been recognized by the Turing Award, the Computer Science Equivalent of
the Nobel Prize.
Jeff was actually the guest for our Season 2 finale with two back-to-back episodes discussing
the early days of deep learning, the various breakthroughs, and the potential for AI ahead
of us.
But something big changed just recently, which is why I am so excited to have Jeff back
on already.
As announced in the New York Times, among many other outlets, Jeff has quit his job
at Google so he can freely speak about the risks of artificial intelligence.
A part of him, he said, now regrets his life's work.
Jeff, so great to have you here with us.
Welcome to the show.
Thank you.
I enjoyed our last podcast and I'm hoping to enjoy this one.
Well, I hope so too.
I'll try my best.
Jeff, before diving into today's conversation, I'd like to thank our podcast sponsors,
Index Ventures, and Weights & Biases.
Index Ventures is a venture capital firm that invests in exceptional entrepreneurs across
all stages, from seed to IPO.
With offices in San Francisco, New York, and London, the firm backs founders across a variety
of verticals, including AI, SaaS, Fintech, security, gaming, and consumer.
On a personal note, Index is an investor-incovariant and I couldn't recommend them any higher.
Weights & Biases is an ML ops platform that helps you train better models faster with
experiment tracking, modeling data set versioning, and model management.
They're used by OpenAI, NVIDIA, and almost every lab releasing a large model.
In fact, many, if not all, of my students at Berkeley and colleagues at Covarrant are
big users of Weights & Biases.
Well, Jeff, welcome back.
So glad to have you here.
Let me dive right in with the big headline from last week.
May 1st, New York Times headline summarizes actually very well why we decided to catch
up here.
It relates quite a lot to what we talked about in a previous podcast about the forward-forward
algorithm.
For 50 years, I thought I was investigating how the brain might know by making models
on digital computers, using artificial neural networks, and trying to figure out how to
make those learn.
And I strongly believed that to make the digital models work better, we had to make them more
like the brain.
And until very recently, I believed that, and then a few months ago, I suddenly did
a flip.
I suddenly decided, actually, back propagation running on digital computers might be a much
better learning algorithm than anything the brain's got.
And there were several reasons for that.
It started a year or two ago when Palm could explain why joke was funny.
And that was a criterion I'd been using for a long time to decide whether these things
were really intelligent.
And I was slightly shocked that Palm could explain why joke was funny.
Then we saw, in addition to farm things like chat GPT and GPT-4, and they were very impressive
in what they could do.
And in particular, they had about a trillion weights, but they knew much, much more than
any one person.
So we've got about a hundred trillion weights.
But these things know about a thousand times more common sense facts than we do.
And that suggested that back propagation was much, much better at packing a lot of information
into only a few connections, like only a trillion.
So that made me change my mind about whether the brain has a better learning algorithm than
the digital systems.
And I started thinking that maybe these digital systems have something the brain doesn't have,
which is you can have many copies of the same model running on different hardware.
And when one copy learns something, it can communicate that to all the other copies by
communicating the weight changes with a bandwidth of trillions of bits.
Whereas when you learn something, to communicate it to me, I need to try and change my weights
so that I would say the same thing as you, and the bandwidth of sentences is only hundreds
of bits per sentence.
So maybe these things are much, much better requiring knowledge because they can work
in parallel much, much better than people can.
In some sense, you could then say that's a dream come true.
You've been trying to build AI, and now all of a sudden you realize that the work that's
been going on and that you pioneered a lot of is actually even more capable possibly
than you had dreamed of when you started.
You were hoping to match human intelligence, and now you might have found a way to exceed
it.
Well, yes, except that my dream was to understand how the brain works, and we still haven't
done that.
We've built something better, and now we have to worry about what the something better
might do.
Now, I think, I mean, you notice as well as anybody, the first question on a lot of people's
minds when they hear about AI is, you know, is it dangerous to us if it becomes smarter
than us, which you're alluding to, Jeff, that it might have the potential to be smarter
than us, is it going to kill us?
What is it going to do with us?
What happens to us humans when we hit that point?
So obviously, we would like to stay in control, and obviously, there's a problem with less
intelligent things controlling more intelligent things.
Now, one thing we have on our side is that with or else evolution, so we come with strong
goals about not damaging our bodies and getting enough to eat and making lots of copies of
ourselves.
And it's very hard to turn those goals off.
These things don't come with strong-building goals.
We made them, and we get to put the goals in.
So that suggests that we might be able to keep them working for our benefit.
But there's lots of possible ways that might go wrong.
So one possible way is bad actors.
Defence departments are going to build robot soldiers, and the robot soldiers are not going
to have the Asimov principles.
Their first principle is not going to be whatever you do, don't harm people.
It's going to be just the opposite of that.
So that's the bad actor scenario.
But then there's the alignment problem, which is that if you give these things the ability
to create their own sub-goals, and you will surely want to do that because creating sub-goals
makes you much more efficient.
So for example, if you want to get to the airport, you create the sub-goal of finding
some means of transport, and then you work on that sub-goal, and breaking things up into
sub-goals just makes you more efficient.
And so I think we will give these digital intelligences the ability to create their
own sub-goals.
And now there's a problem, which is what if they create sub-goals that have unintended
consequences that are bad for us.
So here's a very common sub-goal, which makes a lot of sense, gain more control.
Because if you gain more control, you're better at achieving all your other goals.
So for example, I was sitting in a very boring seminar, and I noticed a spot of light on
the ceiling, and I wondered where it came from, and then I noticed that when I moved,
the spot of light moved.
So then I realised it was a reflection of the sun on my watch.
And so what did I do?
Having solved the problem, did I go back and listen to the boring seminar?
Well no, the first thing I did was try to figure out how I could get it to move on the
ceiling.
I wanted to get control of that spot of light.
We probably have a built-in drive to get control, but even if we didn't, that would
be a very useful thing to do, to get control of things, because it allows you to achieve
all sorts of other things.
So I think as soon as you let them develop their own sub-goals, one of the sub-goals
will be get more control, and we don't want them to get too much control.
It seems that there is still a big, I would say, gap maybe, at least conceptually.
Maybe it can be bridged quickly in time, but a big gap conceptually between the models today
that are most visible, the language models, chat GPT and competitors thereof, which do
next-word completion, conceptually speaking, versus AIs that have a goal.
So we, of course, have seen AIs with goals like the world champion of Go is an AI that
has the goal of winning the game of Go and is trained that way, but these AIs with goals
so far have been in rather contained environments, I guess, compared to the next-word prediction
models.
So how do you see a path?
I mean, do you see it happen quickly to go from next-word prediction to AIs that start
doing things themselves in the world?
It's not quite true that it's just an next-word prediction.
That's the main way they learn, but they're also trained with human reinforcement learning.
So people are telling them, don't produce that kind of answer, do produce this kind
of answer.
That's very different from next-word prediction, and that's shaping them.
And that was a big breakthrough that OpenAI had, realizing that, actually, you don't need
to do all that much of that once you've trained a big language model, and you can radically
shape the way it behaves during that.
It's a bit like raising a child.
When you raise a child, most of what it learns to do is just from wandering about in the
world and figuring out how the world works, but the parent has a small amount of input
by saying, no, and don't do that, and oh, very well done, and that makes a big difference
to how the child turns out.
So already, we've got things other than next-word prediction shaping them, and you can imagine
going further.
You could imagine having large language models that are multimodal, so they're seeing visual
input, and they're trying to do things like open doors and put things in drawers, and then
they'll have much more than just next-word prediction.
And even if it was just next-word prediction, people sometimes talk about, they say, oh,
they're just autocomplete.
But the point about autocomplete is, if you want to do a really good job of next-word prediction,
the only way to do a really good job is to understand what was said, and that's what
they're doing, they're understanding what was said in order to do next-word prediction.
And so underneath, it needs to understand, effectively, everything that's going on in
people's minds, to be able to maximally accurately predict what a person will say next, I think
is your point, right?
And so, it has to be a very capable model.
The best prediction would be made like that.
It doesn't understand everything that's going on in people's minds yet, but it understands
quite a lot, and it's understanding more every day, and if you look at how good they were
five years ago, look at how good things were in 2015 or sometime like that, when people
were messing about with chatbots, they weren't that good before Transformers came along.
Look at how good they are now, and now project that forwards five years, and that's what's
got me worried.
I think in five years' time, they could well be smarter than people.
Now, when you say smarter than people, that's an interesting concept to even define, right?
What do you think of when you say smarter than people?
Well, it's very easy to see in a limited domain.
I don't play Go, so I don't understand Alpha Go, but I do play chess at a very low level,
and when I look at Alpha Zero, it's not just that he's doing lots of calculation, because
he's doing a lot less calculation than Deep Blue was doing, I believe, but he's got very
good intuitions, and it makes brilliant peace sacrifices in a kind of justified way, and
it's just much better than any human chess player ever was, and I don't see why that
should just be limited to that domain, and it's not that it's not just cheating by doing
lots and lots of calculation, it has really good intuitions about chess.
Could you imagine that, I mean, maybe not five years from now, but that, or maybe sooner,
who knows, that somebody decides that they want the CEO of their company to be an AI,
and that that company will actually do well, because that's, AI CEO better understands
everything going on in the company, in the world, and can make better decisions?
Why not?
I don't think that's an absurd idea.
I should say something about predicting the future here.
So you get lots of car accidents when people drive in fog, and the reason is, people are
used to driving at night, and now you can see the tail lights of the car in front, and
the brightness of the tail lights falls off as the inverse square of the distance, as
you mentioned, light you get from them, and so that's your kind of model, which is kind
of quadratic fall-off.
As soon as you get fog, that's exponential fall-off, you lose a certain fraction of the
light per unit distance, and so relative to what you're used to, which is quadratic
fall-off, fog, you can see the first 100 yards perfectly clearly, and that makes you think
you're going to be able to see 1,000 yards moderately clearly, but actually at 200 yards,
a wall comes down, you can't see anything beyond 200 yards, because it's exponential
drop-off, and that's a very good model for seeing the future.
Because he very clearly was going to happen a few years down the road, and then suddenly
you don't know anything.
You think you do, because you extrapolate, but you extrapolate using a linear model or
quadratic model, and it's actually exponential, and we're hopeless at predicting the future
in the long term.
I love the story of the New York Times, where in, I think, 1902, there was an article that
predicted that heavier-than-air flying machines would take a million or maybe 10 million years
to develop, and actually they came in two months after that.
Well, there's a lesson there.
Now, Jeff, if we go zoom out for a moment, when you think about the risks of AI, and
we'll get, I know you've said you see many good things coming from AI, also that's why
you remain very excited, but we need to approach it the right way.
But just to make sure we have a good framing of the risks which are driving your change
in what you're doing right now.
You mentioned bad actors, could be more powerful to put it to their bad uses.
You mentioned alignment, namely, making sure that AI aligns with what we want, and I want
to dive a lot deeper into that in a little bit, but are there others than these two that
are concerned you?
Oh, yes, there's lots of things that many other people have talked about, and they've
been talked about for a while, like AI is incredibly biased if it's trained on incredibly
biased data, it just picks up class from the data.
That doesn't worry me as much as it worries some people, because I think people are very
biased, and actually understanding the bias in an AI system is easier than understanding
the bias in a person, because you can just freeze the AI system and do experiments on
it.
You can't do that with a person.
If you try and freeze the person and do experiments, they realize what you're up to, and they change
what they say.
So I think actually bias is easier to fix in an AI system than it is in a person.
There's job losses, and job losses aren't really the fault of AI.
So, if you think about autonomous driving, if we make something that can drive long-distance
trucks, a lot of truck drivers will lose their jobs, and some people think, well, that's
the fault of AI, but when we made things that were better at digging ditches, machines that
could dig ditches, we didn't say, oh, well, we shouldn't do that.
These machines are terrible.
Actually, at the time, they probably did, but in the end, we didn't keep digging ditches
with spades.
We dug ditches with machines, because that's just a much better way to do it, when the people
who used to dig ditches with spades had to find other jobs.
In a decent society, if you increase productivity, it should help everybody.
The danger is, in the society that most of us are living, if you increase productivity,
the gains are going to go to making the rich richer, and possibly making the poor poorer.
But I don't see that as the fault of AI, and I don't see that as a reason to be luddite
and to stop developing AI, because AI has tremendous good it can do, even in the area
of autonomous driving.
If an AI runs over a pedestrian, everybody thinks, shocking, we should stop developing
it, even if on the very same day, people ran over lots of pedestrians.
You'll know that, well, you and I believe, I think, that eventually they'll get on autonomous
driving working properly, and it will save lots and lots of lives.
It won't lapse in attention the same way, and it'll probably be more cautious.
So that's a good thing.
In medicine, it's even more obvious what the good's going to be.
We're going to get much better family doctors who know much more.
We're going to be able to get much more information out of medical scans.
I said in 2016 that by 2021, we wouldn't need radiologists.
I was talking in the context of interpreting scans, so what I meant was we wouldn't need
them for interpreting scans.
That was overambitious.
Already we've got systems in Pakistan and India doing diabetic retinopathy stage in
diabetic retinopathy and saving lots of people's sight.
We've already got systems that are comparable with good radiologists from many other kinds
of scan.
I think it'll still be a few years before we move to a system where most of the interpretation
of scans is done by these AI systems with radiologists looking over their shoulders.
But that's coming, and that'll be tremendously useful.
There's an enormous amount of information in something like a CAT scan that isn't being
used at present, and we'll be able to use much more information, I think.
There's going to be things like making better nanomaterials, obviously things like Alpha
Fold, where Alpha Fold is now done about a billion years' work in predicting protein
structure if it was done the old way by PhD students.
That's enough to pay for most of AI.
Lots of things like that are going to be tremendously helpful.
I think if you could make nanomaterials that make better solar panels, that would probably
compensate for all the carbon dioxide produced by data centers.
You could actually make all the data centers use solar power with better solar panels.
So there's tremendous good to be had.
I've just given a few examples, but we know that more or less anything you do AI can make
it more efficient.
AI can do parts of that job, sometimes all of it, sometimes parts of it.
Any time you're producing textual output, AI can make you more efficient, even for things
like legislative recommendation.
So I don't think there's any chance people will stop developing it because it's going
to be so useful.
No, you're saying there's no chance people will stop developing it because there's so
many positive uses, but also almost every positive use can get paired with somebody
using it, a bad actor using it in a different way.
There was a call, nevertheless, a few weeks ago to stop the development of not AI as a
whole, but to not train any even larger models than GPT-4.
That's a very specific call.
And I think you've talked about that too, even though I don't think you sign up on that
specific letter.
What are your thoughts on that?
That specific letter, maybe it was politically sensible because it got people's attention,
but there was never any chance that people would do that.
It was completely...
I didn't sign it because I thought it was silly in the sense that what they're calling
for is something completely infeasible.
If you did that in the States, they wouldn't do it in China.
And even if you didn't say it in China, they wouldn't do it in Russia.
So there was never any chance that development would be stopped.
I also agree with Sam Altman that if you think about the existential risk of these things
getting out of control, then the best way to handle that, given that you can't stop
this stuff developing, is for the people developing it to be doing experiments as they develop
it, understanding much more about how you control it.
Anybody who's written a program knows you can't just sit in an armchair and figure out
the solution to programs like that.
You have to experiment.
It has to be the people who are developing it who fiddle with it and see what happens
and what doesn't happen.
Then you learn all sorts of strange things you wouldn't have expected.
So that's what I think is going to happen.
Really what I want to call for is that comparable amounts of effort and resources should go
into developing it and figuring out how to stop the bad side effects and stop it getting
overall control.
So I want to sort of sound a warning that we've got to take this very seriously.
Right now it's kind of 99% of the money goes into developing, it's 1% into safety.
It should be much more like 50-50.
From an academic point of view, that might not be too hard to steer, I mean in a sense
you could imagine funding agencies adjusting the calls for proposals to be closer to what
you're saying, Jeff, and that doesn't seem infeasible at all to have something like that
happen.
In a private sector which is more driven by developing things that make more money, do
you think it's realistic that 50% of the money will be made by spending time on the safety
aspects or how do we get there, that that actually matters just as much?
Yeah, I don't know.
This is where I resort to saying I'm just a scientist who knows just something that
might happen, I'm not a policy expert.
In Google, they were fairly responsible when they had the lead.
After they developed a transformer, they published them and they developed chatbots.
They didn't put their chatbots out there because they knew there would be lots of bad side
effects if people started using them.
And so I thought they were fairly responsible.
It's just when Microsoft funded OpenAI and then used their chatbot in Bing, Google didn't
have much alternative but to respond by trying to do the engineering on their chatbots so
they could make a version of Bard that was comparable with ChatGPT.
In a capitalist system, they don't have much alternative.
The way you get big companies to do things that don't immediately make profits is with
regulations, I think.
Regulations have been called for a while actually by some people, and particularly Elon Musk
has called for a regulation in the AI space for a while without necessarily being particularly
specific about what the regulation should be.
Is there a certain regulation that you think would make sense?
So there are many dangers of AI and people tend to confound them all as you want a big
soup of danger and I think it's important to separate them out.
So there's putting people out of work, there's encouraging political divisions by trying to
give people things that will make them indignant to click on because they love being indignant.
I love it too, I love clicking on these things that are going to make me indignant.
Because the existential threat, and then there's the threat of truth disappearing because you
don't know what's real and what's fake, these are all different threats in addition to things
like discrimination bias.
So we need to think which threat we're talking about and for the threat of truth disappearing
because we just swamp with fakes.
You can imagine there's something we might be able to do, it's going to be very tough,
but governments really don't like people printing money, they like to be the ones that do that.
And so there's very severe penalties for printing fake money.
And the penalties are actually, if someone gives you some fake money and you know it
to be fake and then you take it to a store, that I believe is a criminal offence too.
Not as bad as printing yourself, but trying to pass counterfeit money, that is illegal.
So we need something like that for AI generated material.
It has to be clearly labelled as AI generated, and if you try and pass it off as real, there
should be severe legal penalties for that.
Whether we can be good enough at detecting it is another matter.
But at least you can see the sort of direction in which you need to go in to prevent us being
swamped by fake videos.
Yeah, enforcement wouldn't be easy, I guess, but I agree.
The principle seems clear, I guess, even if the enforcement might be hard.
Let me tell you why the enforcement is going to be really hard.
Can you use deep learning to help you?
So you build an AI system that can detect the fakes, right?
We know that building an AI system that can detect the fakes is a very good way of training
your generator to make more realistic fakes.
That's what Genre-Adversarial let's do.
So there doesn't seem to be much hope in the direction of using an AI system that can detect
fakes.
It'll just allow the generators to make better fakes.
When you think about cryptographic solutions, that's always been on my mind.
Imagine something where whatever content that's created gets a cryptographic signature, traditional
cryptography, nothing to do with Web 3 or anything like that.
It just gets a signature attached to it that shows who is the author of this piece of material
or who shot the video and so forth.
And then it'd be the reputation of the author that'd be at stake in terms of credibility
if they put a lot of fakes in the mix.
It used to be the case in Britain.
It's probably still the case that whenever you print anything, even if it's a pamphlet
for a political demonstration, the printer's identity has to be printed on it.
It's illegal to print things without the printer's identity being on it.
And that's basically the same idea, early version of that idea.
I know nothing about cryptography and cryptographic stuff, so I might have my area of expertise,
but it sounds to me like that's extremely sensible.
That's your thoughts on regulation, even though it might be hard to enforce that in principle
has a clear framework to it for avoiding being flooded with fake news, fake videos, fake
texts and so forth.
You can also imagine regulations for clickbait, for avoiding political division by clickbait.
This is the point of which I'm glad I no longer work for Google.
So Facebook and YouTube and lots of other social media are encouraged division by offering
up to you things that'll make you indignant and things that are within your echo chamber.
And you could imagine trying to use legislation to prevent that.
It's tricky to do, but I think it's important if you want to keep democracy and not have
this incredible division between two groups of people, each of whom think the other is
completely crazy, to do something about that.
It needs to be that you discourage these companies from offering up things that'll just make
you indignant.
I recall a few years ago, there was a brief, very brief moment where actually Facebook
did some self-policing in some sense where they would not let you share anything that
ended with you'll never guess what happened next.
That was automatically not allowed, but yeah, it's of course much more difficult than that.
And it's not an area where I have any expertise, but it just seems to be, it's the kind of
thing where maybe regulation could have an effect.
Now, the big thing, of course, a lot of people worry about is AI is taking over the world.
Right.
That's the existential threat.
And that's what I'm talking about.
That's where I've changed my mind a lot recently.
I used to think it was way off, 30, 50, 100 years, and now I think I have very little
confidence in predicting how far off it is, but I would guess 5 to 20 with very low confidence.
Not taking over the world, but being smarter than us.
And the big issue is, once it's smarter than us, does it take over the world or do we still
control it?
And that's what's in our hands then in the next 30 to 50 years is what you're talking
about, what we should focus on.
It may be in our hands.
It may be that it's historically inevitable that digital intelligence is better than
biological intelligence, and it's the next stage of evolution.
I have not, but that's possible.
And we should certainly do everything we can to keep control.
Sometimes when I'm gloomy, I think, imagine if somehow frogs had invented people and frogs
needed to keep control of people, but there's rather a big gap in intelligence.
I didn't think it would work out well for the frogs.
But of course, that's not really a very realistic argument because people evolved, and so people
evolved with their own goals, including the goal of make more people.
And these digital intelligences don't have their own goals.
If they ever did get the goal, if a digital intelligence got the goal of make more of
me, then evolution would kick in, right?
And the one that was most determined to make more of itself would win.
So we don't want them to ever get that goal.
We don't want it, that's what you're saying, but would it be hard to give it to them if
we wanted to?
I mean, if we just...
Oh, I think you could quite easily give a digital intelligence the goal of making more
of itself.
You just say, that's your main goal in life, and I think that would be crazy.
I don't...
I don't think even Putin would do that, but...
Well, it's a very risky thing to give them the goal of there being more of themselves,
but maybe the...
I mean, I'm just imagining here, but maybe the reason that humans are where they are
today is because of evolutionary competition, maybe the smartest digital intelligence would
emerge from a competitive environment, maybe not against humans, but against other digital
intelligences.
Yes.
So you can imagine a whole new phase of evolution when biological intelligences, because they're
very low power, can evolve.
They build power stations, and they build digital intelligences, which provide a lot
of power and very accurate fabrication, so you can have many copies of the same model.
These digital intelligences are then the next phase of evolution, and they compete with
each other and get better because they're competing with each other.
That's certainly a scenario that's not at all inconceivable.
We prefer to have a scenario where we create something much more intelligent than us, and
it kind of replaces the UN.
You have this really intelligent mediator that doesn't have goals of its own.
Everybody knows it doesn't have its own agenda.
It can look at what people are up to, and it can say, oh, don't be silly, you'll go
everywhere what it's going to lose if you do this.
If you do this and you do that, and we'll sort of believe it like children would believe
a benevolent parent, that's the utopian version, as opposed to the gestopian version.
I don't think that's out of the question either.
That's feasible, possibly.
It could happen, yes.
But it seems like it would require humanity to get together and want it, but technologically
what you describe seems feasible.
Yes, it seems feasible to me.
But like I say, when you're speculating about things with which you have no experience,
you tend to be quite a long way off, and as soon as you get a little bit of practical
experience, you revise your theories because you realize how far off you are.
We need to be doing a lot of work as these things are developed in understanding the
risk and having little empirical experiments where we can see what tends to happen.
When you make these smart things, do they tend to try and get control?
Do they tend to come up with goals of, hey, I want to make more of me?
It seems like that there's a natural tension between the short term and the long term here,
meaning if I look at the short term, having an AI that sets its own goals that can go
do things could be a nice to have and say, okay, make me some more money on the side.
Do this, do that.
You know, I have these resources, those resources, see what you can do with it and get some things
done for me, very convenient, and then it could be a slippery slope possibly to what it decides
to set as its sub-goals to get that done.
That's the alignment problem.
Right.
It's very, very hard, but it does seem like you are thinking of a clear distinction here
between AIs that tried to get things done that can set goals versus AIs that are purely
advisory, that are called upon for their wisdom, have a lot of wisdom because they have seen
so much, know so much, have predictive powers, but that they are just advisors, not actors.
That would be great, right?
That would be very useful.
But that seems a clear thing to pursue possibly, right?
Yes.
Now, you can't make it safe just by not allowing it to press buttons or pull levers.
Why is that?
A chatbot will have learned how to manipulate people.
They would have read everything Machiavelli ever wrote and all the novels in which people
manipulate other people, and it will be a master manipulator if it wants to be.
And it turns out you don't need to be able to press buttons and pull levers.
You can, for example, invade a building in Washington just by manipulating people.
You can manipulate them into thinking that the only way to save democracies is to invade
this building.
And so a kind of air gap that doesn't allow an AI to actually do anything other than talk
to people isn't sufficient.
If he can talk to people, he can manipulate people, and if he can manipulate people, he
can get them to do what it wants.
So it's not so much about the air gap.
It's about the purpose, the built-in purpose.
Yes.
It's about the goal, yeah.
If it ever develops the purpose of make many more of me, we're all in trouble.
So Jeff, I like your thoughts on regulation, but also the challenges with regulation.
Now for a moment, let's imagine things maybe don't go exactly the way we hope, in that
we hope that there is this AI advisor who can just ask questions to and helps us make
the right decisions, but let's say somehow the AI does emerge with a goal and a purpose
and starts doing things for itself rather than for us.
I mean, couldn't be the case that it just decides to run off to a different solar system
with more energy available, and we're back to where we are today.
We're entering this time of huge uncertainty where we're going to be dealing with things
that we've got no experience with, the sort of no empirical data on what it's like to
interact with things smarter than us.
So we just don't know.
I mean, I think the right attitude is, I've no idea.
I talked to Elon Musk the other day, and he thinks we'll get things more intelligent than
us.
And what he's hoping is they'll keep us around because we'll make life more interesting.
If you have a world without people in it, or without animals in it, it's just not as
interesting as a world with people in it.
That seems like a pretty thin thing to rest humanity on to me.
But he thinks it's quite possible these things will get much smarter and they'll gain control.
I didn't actually ask him if I could have a space on the rocket.
Yeah, I guess Marx is even with speed of light.
It gives you some delay before it reaches you.
Now, in a scenario where you said you talked with Elon Musk, the scenario he seems to envision
is one where AI and humans might fuse together, his Neuralink company.
You've talked about AI.
You've talked about the brain, about the combination of both more than anyone else possibly.
What are your thoughts on that kind of future?
I think that's pretty interesting.
I've always thought that people have audio in and they have audio out and they don't
have video out.
But if people had video out, we'd be able to communicate better.
So that's not exactly what Elon's planning to do.
He's planning to get brain-to-brain transferred to a fairly abstract level of thought.
Now you need to transfer things in a way the other person can understand them.
Much less ambitious project would be to have video out because now the other person knows
how to deal with video and they can deal with that as input.
I think if we had video out, it would improve communication quite a bit.
So a person, if you want to communicate something to me, you can talk or you can draw diagrams.
But presumably before you draw the diagram, you have a kind of picture in your head.
Maybe not, but probably you do.
And if you could just communicate those pictures in your head very quickly, that ought to increase
the bandwidth.
Maybe only by a factor of two, but that would still be a big win.
But maybe you're by a factor of more than that.
Well, it's not unnecessarily helped, but I think it might.
I did have a plan for improving communication between drivers, where every car has on its
roof a big LED display where you can display up to two words.
But it turns out in that case, you wouldn't actually need to make the two words variable.
You could just put them next.
It might induce a lot more road rage, yes.
I'm a bit worried there.
Yes, so I don't think that was a very good scheme.
Now you've alluded to this earlier that maybe biological evolution is just a starting point.
Maybe it's natural to be followed by digital evolution, computer-based or other forms.
In principle, we could ask the question there too, right?
Imagine we, I mean, you've also said that you really, that's not the future you currently
would want, but imagine that somehow that would be the future, that after humanity there's
a digital life form that is more dominant in some sense than humans are today on Earth.
That could still go in many ways.
You can imagine a digital life form that is really good to us, to others, to everything
that's around versus digital life forms that maybe destroy everything.
Is that worth thinking about?
If that scenario is the scenario, how do we make sure it's a good version of it?
Yes, that's definitely worth thinking about.
And there'll be something very different about them, because they don't have to worry about
death.
Basically, people haven't really noticed this yet, but we've discovered the secretive
immortality.
The secretive immortality is just the computer science thing and make the software separate
from the hardware.
As soon as you do that, it's true of these artificial neural nets.
If one piece of hardware dies, the knowledge doesn't die, the weights can be recorded somewhere,
and as soon as you've got another piece of hardware that can execute the same instructions,
then it comes back to life again.
So these digital intelligences are immortal, as opposed to the kind of biological intelligence
we have where whatever our learning algorithm is, it appears to make use of all the little
quirks and peculiarities of the wiring of our brain and the funny way our neurons work.
That makes it much more efficient in energy terms, but it means that when the hardware
dies, the knowledge dies with it, unless you've taught it to somebody else, which is what
I'm trying to do now.
So Ray Kurzweil, for example, would like to be immortal, and I don't think he will be,
because he's biological, but the digital devices, the digital intelligences we're producing
are going to be immortal, and maybe once you've got immortality, you maybe get to be a bit
nicer.
Or also less afraid, I guess.
You also get to be a lot less afraid, yes.
So digital soldiers that know that they're immortal, maybe that's not so good.
Now, I mean, in some sense, when I haven't given this enough time to think about, compared
to what I would like to, but just at a high level, when you think about this, it seems
like it comes back to what's the purpose of, not just humanity, but the purpose of life,
right?
Yes, absolutely.
So when I was a student, a troubled teenager, I started off studying physics and physiology
at Cambridge, and then I really wanted to know what the purpose of life was, so I started
philosophy for a year.
That didn't really help, so I switched to psychology, and that didn't really help either,
and I ended up in AI.
And I now think, I mean, I'm an atheist, so I think the purpose of life is to make as
many copies of yourself as possible.
That's what evolution seems to do, and we've evolved, we've evolved as hominids that live
in small, warring tribes, and that's our evolutionary history, and it's a recent enough
history we haven't been able to change that much.
And if you look at society now, it's small, warring tribes at every level, we just made
it fractal, so for those hominids, insofar as they have any purpose, it's make more
copies of yourself.
That all makes sense to me, but can we hope for something more?
I mean, we're able to think of more, right?
We're able to at least articulate the notion that maybe it'd be nice to be more than what
you just described, right?
Nice to be more than just trying to replicate ourselves maximally.
If you're in a small tribe of hominids, to make the tribe successful, you want to help
other people in the tribe, so we have a strong urge to help other people in our tribe, and
you may have noticed this, I noticed this very strongly in academic departments.
If you're in a big department, in general, you'd rather your department got resources
rather than some other department.
So the University of Toronto, there's lots and lots of professors of French, and I think
it'd be better if there were less professors of French and more professors of computer
science.
But within computer science, there's professors in different areas, and I think it'd be good
if there were a lot of professors in machine learning.
But within my group, when I was at the University of Toronto, actively, there was a very strong
allegiance to a group, and that's just from my evolutionary inheritance.
So I think we are very strongly altruistic towards members of our group.
We're willing to sacrifice things to help members of our group, and you know that when
you write a very long letter of recommendation for a student you really like, you're sacrificing
your time to help someone in your group, and you're much more willing to do that for someone
in your group than for someone who may be equally good but wasn't in your group.
Yeah, it's a natural thing, I think, and it's because you've already invested so much time
in them, you know them so well, I think it's pretty natural for that to happen.
So it's not just that we want to mend more copies of ourselves, we want to make our
group successful.
Now if we could make that generalize to bigger and bigger groups, then we can probably get
better societies.
I maybe have a question from a slightly different angle, Jeff, though I agree it would be great
if we could generalize it to bigger and bigger groups, see what we could get to, but imagine
in some sense the counterstance to developing new technology for good would be, I could
imagine a counterstance that says, hey, my life today is pretty good, I have not too
much to complain about, I could maybe imagine a version where more people, maybe all people
could have a similar standard of life with some adjustments here and there and so forth,
why don't we just stagnate and keep it this way, we're happy, we stay happy, but then
to me that falls short because to me it seems like when there is at least person, when there
is no progress, when everything stays the same, that's really uninteresting, not exciting
and almost defeats the purpose of that we're even here and it seems that progress somehow,
at least for me, progress is this kind of natural notion of something we need for it to even
make sense to be here.
I don't think I agree with that, I think I can imagine a society in which it's not going
anywhere in your sense, it's not, you're not getting more of you, but what you're doing
makes you happy, it's sustainable. When I was at University College London and I was
director of the Gatsby unit, we had four faculty members and a dozen or more graduate students
in some postdocs and a member came around from the university which is how are you planning
to expand your department and I replied that I actually wasn't planning to expand it, I thought
it was a very nice size as it was, that wasn't an acceptable answer but that was how I felt
Yeah, so maybe I meant it in a slightly different way, I meant it more in the sense of imagine
a hundred million years from now, just essentially people doing the exact same thing we're doing
today, life has not really evolved, you could land in the earth a hundred million years from
now and you couldn't tell the difference, wouldn't that be like somehow, I don't know,
feel unsatisfactory that there was, there has, nothing has evolved or changed in a hundred
million years? I'm not so sure it would be, I mean if we got a decent society and most people were
happy and having a fulfilled life and having lots of nice social interactions and it just
stayed like that forever, I don't see the problem with that. Well then maybe we should try to achieve
it, yes. No, there's a lot of people, I mean I think one thing that's wrong with
society present is economists obsessed with growth and if you're not growing something's wrong
and that's, unless you're going to leave the planet, that's utterly unsustainable.
We talked a bit earlier about there's both a technological kind of line of work ahead to make
sure we can align the AI with humanity and there is regulation government work ahead. Now on the
technological side of things, do you have any recommendations for today's AI researchers
or people in other fields who might want to contribute, what do you see as big opportunities?
So I guess playing with the most advanced chatbots and trying to understand more about
how they actually are intelligent, how they do it, because we still don't really understand how
they do it. I mean we know just how they work in terms of transformers but I think we're not really
very clear about exactly how they manage to do these reasoning tasks and looking at how you can
control them as they develop. I've said that before but I mean I think that would be a very
sensible thing to do. I don't, but I should emphasize I'm not really an expert on these
alignment problems as people have been thinking about them much longer than me. People who don't
do deep learning like Stuart Russell and lots of other people who do deep learning, people like
Roger Gross have been thinking about these things for much longer than me and they're kind of the
experts. I've just come to this very late because I suddenly changed my mind about how soon these
super intelligences may be coming and I see my role as getting a bit old to doing technical work.
I see my role as use my reputation to sound the alarm. I thought I quoted somewhere that you said
I feel a little bit old to do technical work and I think back to our conversation less than a year
ago and I'm like wow that conversation was far more inspiring than pretty much any other conversation
I've had in a very long time, inspiring at the technical level of things we talked about. So I
think you have a very high bar for what it means to still be able to do something interesting.
Well it's kind of you to say so but when I tried to scale up the forward forward algorithm
I couldn't get it to compete with backprop and that was one of the things that made me think
that maybe back propagation is just much better than what the brain's got. Now final question for
this conversation here Jeff. Top of my mind ever since your announcement has been you leave Google
you have all this extra time now in your hand to freely do whatever you want to do. What does
that mean? Does that mean you'll be working solo? Are you finding a new affiliation? Are you starting
an organization of your own? What's happening in the near future?
My main goal is to watch all those good movies on Netflix. I never had a chance to watch when I was
working too hard for 50 years. At Google they talk a lot about life-work balance. I never went to any
of those seminars. I didn't have time. My view as a researcher is I got this from Alan Newell
at Carnegie Mellon who used to tell the graduate students if you're not working 80 hours a week
you're not a serious scientist and I'm afraid that's been my view. It may not have done me much
good in life but it's time to stop doing that so that's my main objective. I didn't think I'll be
able to stop doing research because it's so much fun even when you're not as good as you used to be
at it. So I will probably keep working on variations on the forward-forward algorithm
and variations on trying to do a stochastic version of back propagation by taking random steps
and then multiplying the step length by how much it improved things. That seems to be
that'll scale much worse but if you have lots of little local objective functions
they maybe you can have lots of little local modules. They're all learning optimizing the
local objective functions by taking random steps and scaling the step by how much it improved their
local objective function and maybe that's how the brain can learn big systems without being able to
back propagate. So I'll keep thinking about things like that but I will actually watch a lot of movies
and I'll try and spend a lot of time with my kids. We're no longer kids.
It's nice to hear your passion for the brain is still so big to understand how it might work.
In terms of the alignment work and AI risk work of course you've done a lot by your announcement
by I mean F. How many interview requests did you get after the announcement?
So the day after the New York Times came out on a Monday on the Tuesday I was getting an interview
request every two minutes. That was stressful. To begin with I felt I ought to reply to them
and that just wasn't feasible. Then someone who knows much more about the media told me
no they don't actually expect you to reply. That made life easier.
Yeah I mean every two minutes that's wild. Do you expect going forward to spend time
evangelizing and making people aware or was this a one-time thing and you feel like okay now
people know it's clear the job is done? I don't know. I wasn't expecting this bigger reaction
and I haven't really had time to think through what happens next. I suspect I'll keep encouraging
people to work on the alignment problem, work on thinking about how to keep this under control
and I'll probably keep giving the occasional lecture about that but I don't intend to sort of
make that a full-time job. What I enjoy much more is fiddling about with programs that are
implementing interesting algorithms and see how to make them work. That's what I like doing.
That's what I'm good at and I'll go back to doing that if I when I get bored with Netflix.
And isn't there an apparent contradiction there between
trying to get across how important it is that we work on alignment but still have a
personal maybe stronger attraction to understanding how the brain might work?
Understanding how the brain might work is not the thing that's going to get us into trouble.
It's building things that are better than the brain. Understanding how the brain might work
might help more in sort of how you deal with this horrible device of thing where you get
indignant camps who don't believe what the others are saying. I grew up in the sort of 50s and 60s
when there was a general belief that if there was better education and more understanding
everything would get better that belief is disappeared but I still believe that it ought
to be the case that if we understand better we understand how people work better and we should
be able to make society better. I love that as a concluding sentence for this conversation Joe.
Thank you so much. Well thank you.
