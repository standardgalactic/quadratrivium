WEBVTT

00:00.000 --> 00:06.840
What's called AI today has departed to basically pure engineering.

00:07.600 --> 00:14.880
It's designed in such a, the large language models are designed in such a way that in

00:14.880 --> 00:21.660
principle, they can't tell you anything about language learning, cognitive

00:21.660 --> 00:28.140
processes generally, they can produce useful devices like what I'm using, but

00:28.140 --> 00:33.360
the very design ensures that you'll never understand, they'll never lead

00:33.360 --> 00:36.680
to any contribution to science.

00:37.520 --> 00:41.000
That's not a criticism anymore than I'm criticizing.

00:41.040 --> 00:43.480
Camptons this week.

00:43.480 --> 00:48.060
I talked to Noam Chomsky, one of the preeminent intellectuals of our time.

00:48.620 --> 00:54.440
Our conversation touched on the dichotomy between understanding and application

00:54.440 --> 00:56.800
in the field of artificial intelligence.

00:57.340 --> 01:02.740
Chomsky argues that AI has shifted from a science aimed at understanding

01:02.740 --> 01:09.560
cognition to a pure engineering field focused on creating useful, but not

01:09.560 --> 01:13.080
necessarily explanatory tools.

01:13.600 --> 01:19.400
He questions whether neural nets truly mirror how the brain functions and whether

01:19.400 --> 01:22.600
they exhibit any true intelligence at all.

01:23.520 --> 01:29.120
He also suggests that advanced alien life forms would likely have language

01:29.120 --> 01:34.320
structured similar to our own, allowing us to communicate.

01:35.720 --> 01:42.200
Chomsky is 94 and I reached him at home where he appeared with a clock hanging

01:42.220 --> 01:44.520
omnestly over his head.

01:45.400 --> 01:49.280
I hope you enjoy the conversation as much as I did.

01:50.040 --> 01:50.680
Well, thanks.

01:50.680 --> 01:51.880
You're in California.

01:52.400 --> 01:55.760
Actually, I'm in Arizona, which is on California time.

01:56.080 --> 01:56.440
Yeah.

01:56.560 --> 01:56.800
Yeah.

01:56.800 --> 01:57.600
Oh, wonderful.

01:58.120 --> 01:59.120
Uh, yeah.

01:59.120 --> 02:05.880
So, uh, you know, I wanted to talk to you because you have the, uh, you know, one

02:05.880 --> 02:11.640
of the few people, uh, with a deep understanding of, of, uh, linguistics and,

02:11.800 --> 02:19.000
uh, natural language processing that has the historical knowledge, uh, of, of

02:19.160 --> 02:26.080
where we are, how we got to where we are and what, uh, that might mean for the future.

02:26.560 --> 02:35.240
Uh, I, I understand the, the, uh, your criticisms of deep learning, uh, and,

02:35.280 --> 02:43.400
and what large language models are not in terms of, uh, reasoning and, and, uh, you

02:43.400 --> 02:47.680
know, understanding the, the, the underpinnings of, uh, language.

02:48.280 --> 02:53.920
But, uh, I, I thought maybe I could ask you to talk about how this developed.

02:53.920 --> 02:59.440
I mean, going back to Minsky's, uh, thesis at Princeton, when he was, you know,

02:59.440 --> 03:06.720
before he turned against the perceptron, when he was talking about, uh, nets as,

03:06.720 --> 03:11.680
uh, a possible model for, uh, biological processes in the brain.

03:11.680 --> 03:19.360
And then, you know, how did, how you see that things developed and what were

03:19.360 --> 03:25.680
the, the failures that didn't get to where presumably, uh, you would have wanted

03:25.680 --> 03:31.120
that research to go, uh, and then, and then I have some other questions.

03:31.120 --> 03:33.720
But, but, but is that enough to get started?

03:35.200 --> 03:38.360
Well, let's, let's take an analogy.

03:39.320 --> 03:50.040
Suppose you're interested in figuring out how, uh, insects navigate biological

03:50.040 --> 03:50.720
problem.

03:51.440 --> 04:00.640
So, uh, one thing you can do is say, let's try to study in detail what the

04:01.320 --> 04:07.880
desert ants are doing in my backyard, how they're using solar azimuths and so on

04:07.880 --> 04:08.600
and so forth.

04:09.600 --> 04:12.400
Something else you could do is say, look, it's easy.

04:13.480 --> 04:20.120
I'll just build an automobile which can navigate, uh, fine, does better than the

04:20.120 --> 04:20.880
desert ants.

04:21.440 --> 04:22.360
So who cares?

04:23.200 --> 04:27.600
Uh, well, those are the two forms of artificial intelligence.

04:28.640 --> 04:31.360
One is what Minsky was after.

04:32.360 --> 04:41.480
It's now kind of ridiculed as good old fashioned AI, go fi, we're past that stage.

04:42.160 --> 04:44.720
Now we just build things that do it better.

04:45.600 --> 04:46.080
Okay.

04:46.960 --> 04:51.080
Like, uh, an airplane does better than an eagle.

04:51.120 --> 04:53.200
So who cares about how eagles fly?

04:54.560 --> 04:55.640
Yeah, that's possible.

04:56.320 --> 05:01.480
But, uh, it's a difference between totally different goals.

05:02.440 --> 05:09.240
Roughly speaking, science and engineering, it's not a sharp difference, but first

05:09.240 --> 05:14.880
approximation, either you're interested in understanding something or you're just

05:14.880 --> 05:19.080
interested in building something that'll work for some purpose.

05:19.840 --> 05:24.440
So they're both fine occupations, nothing wrong with.

05:24.920 --> 05:29.680
I mean, when you say I'm criticism of the large, criticizing the large language

05:29.680 --> 05:31.080
models, that's not correct.

05:31.640 --> 05:33.200
I'm using them right now.

05:33.800 --> 05:35.200
I'm reading captions.

05:36.480 --> 05:43.040
Captions are based on deep learning, clever programming, very useful.

05:43.520 --> 05:46.040
I'm hard of hearing, so they're very helpful to me.

05:46.560 --> 05:47.520
No criticism.

05:48.200 --> 05:53.720
But if somebody comes along and says, okay, this explains language, you tell them

05:53.720 --> 06:01.000
it's kind of like saying an airplane explains how eagles fly, the wrong question.

06:01.680 --> 06:04.680
It's not intended to lead to any understanding.

06:05.240 --> 06:07.480
It's intended to be for a useful purpose.

06:08.360 --> 06:09.080
That's fine.

06:09.800 --> 06:10.800
No criticism.

06:11.520 --> 06:18.760
And what's called AI today has departed to basically pure engineering.

06:19.480 --> 06:26.600
It's designed in such a, the large language models are designed in such a way that

06:26.600 --> 06:33.560
in principle, they can't tell you anything about language, learning, cognitive

06:33.560 --> 06:39.120
processes generally, they can produce useful devices like what I'm using.

06:39.760 --> 06:45.000
But the very design ensures that you'll never understand, they'll never

06:45.000 --> 06:48.600
lead to any contribution to science.

06:49.400 --> 06:53.680
That's not a criticism anymore than I'm criticizing champions.

06:54.360 --> 07:00.600
Jeff Hinton says, you know, his goal is to understand the brain, how the brain works.

07:01.520 --> 07:13.120
And he talks about AI as we know it today, supervised learning and generative AI as

07:13.120 --> 07:22.400
useful by products, but that are not his goal or not the goal of cognitive

07:22.400 --> 07:26.320
science or computational biology.

07:28.360 --> 07:36.440
Was there a point at which you think the research lost a bead or is there research

07:37.080 --> 07:43.960
going on that people aren't paying attention to that, that is not caught up in the

07:43.960 --> 07:49.360
usefulness of these other kinds of neural nets?

07:51.800 --> 07:56.600
Well, first of all, if you're interested in how the brain works, the first question

07:56.600 --> 08:00.360
you ask is, does it work by neural nets?

08:02.040 --> 08:03.320
That's an open question.

08:04.120 --> 08:09.720
There's plenty of critical analysis that argues that neural nets are not what's

08:09.720 --> 08:12.280
involved, even in simple things like memory.

08:13.480 --> 08:20.840
Actually, these arguments that go back to Helmholtz, the neural transmission is pretty

08:20.840 --> 08:24.520
slow as compared with the ordinary memory.

08:25.000 --> 08:30.840
There's much hard for criticism by people like Randy Gallistel, cognitive

08:30.840 --> 08:37.400
neuroscientist, who's given pretty sound arguments that neural nets in principle

08:38.280 --> 08:48.760
don't have the ability to capture the core notion of a Turing machine,

08:50.520 --> 08:53.640
computational capacity, they just don't have that capacity.

08:54.440 --> 09:02.200
And he's argued that the computational capacity is in much richer computational

09:02.200 --> 09:10.200
systems in the brain, internal delves, where there's very rich computational capacity,

09:10.200 --> 09:15.720
goes wavy on neural net, some experimental evidence to support this.

09:16.360 --> 09:19.640
So if you're interested in the brain, that's the kind of thing you look at.

09:20.600 --> 09:23.640
Not just saying, can I make bigger neural nets?

09:24.840 --> 09:29.080
It's okay if you want to try it, but maybe it's the wrong place to look.

09:29.640 --> 09:33.000
So the first question is, is it even the right place to look?

09:33.960 --> 09:37.480
That's an open question in neuroscience.

09:38.360 --> 09:43.720
If you take a vote among neuroscientists, almost all of them think that neural nets

09:43.720 --> 09:49.080
are the right place to look, but you don't solve scientific questions by a vote.

09:50.360 --> 10:00.200
Yeah, one of the things that's obvious is neural nets, they may be a model,

10:00.200 --> 10:07.000
they may mimic a portion of brain activity, but there are so many other structures.

10:08.040 --> 10:13.320
There's all kind of stuff going on in the brain, way down to the cellular level,

10:13.320 --> 10:16.040
there's chemical interactions, plenty of other things.

10:16.920 --> 10:23.400
So maybe you'll learn something by studying neural nets, if you do, fine, everybody will be happy,

10:24.200 --> 10:29.880
but maybe that's not the place to look if you want to study even simple things like just

10:30.520 --> 10:32.200
memory and associations.

10:33.080 --> 10:41.240
There's now already evidence of associations internal to large cells in the hippocampus,

10:42.040 --> 10:49.240
internal to them, which means maybe something's going on at a deeper level where there's vastly

10:49.240 --> 10:51.080
more computational capacity.

10:53.240 --> 10:54.840
Those are serious questions.

10:55.720 --> 11:01.400
So there's nothing wrong with trying to construct models and learn something from them, if you can, fine.

11:02.040 --> 11:09.480
The building larger models, which is kind of the rage in the engineering side of AI right now,

11:10.440 --> 11:13.000
does produce remarkable results.

11:13.000 --> 11:16.520
I mean, what was your reaction when you saw

11:18.600 --> 11:29.000
chat GPT or GPT-4 or any of these models, that it's just a sort of clever stochastic parent

11:29.000 --> 11:32.120
or that there was something deeper?

11:32.200 --> 11:41.720
If you look at the design of the system, you can see it's like an airplane explaining flying.

11:42.600 --> 11:43.720
There's nothing to do with it.

11:44.360 --> 11:51.720
In fact, it's immediately obvious, trivially obvious, not a deep point, that it can't be

11:51.720 --> 11:52.840
teaching us anything.

11:53.560 --> 11:54.840
The reason is very simple.

11:55.800 --> 12:04.680
The large learning models work just as well for impossible languages that children can't acquire

12:05.320 --> 12:07.960
as for the languages they're trained on.

12:08.840 --> 12:16.040
So it's as if a biologist came along and said, I got a great new theory of organisms, lists a

12:16.040 --> 12:22.680
lot of organisms that possibly exist, a lot that can't possibly exist.

12:22.680 --> 12:24.760
And I can tell you nothing about the difference.

12:26.680 --> 12:30.040
I mean, that's not a contribution to biology.

12:30.840 --> 12:33.480
It doesn't meet the first minimal condition.

12:34.360 --> 12:40.760
The first minimal condition is distinguish between what's possible from what's not possible.

12:41.480 --> 12:42.440
You can't do that.

12:43.720 --> 12:45.480
It's not a contribution to science.

12:46.280 --> 12:51.560
If it was a biologist making that proposal, you'd just laugh.

12:52.600 --> 12:58.600
Why shouldn't we just laugh when an engineer from Silicon Valley says the same thing?

13:00.200 --> 13:02.200
So maybe they're fun.

13:02.200 --> 13:04.200
Maybe they're useful for something.

13:04.200 --> 13:05.320
Maybe they're harmful.

13:06.040 --> 13:10.120
Those are the kinds of questions you ask about pure technology.

13:11.080 --> 13:13.000
So take large language models.

13:13.640 --> 13:15.160
There are something they're useful.

13:16.040 --> 13:18.440
In fact, I'm using them right at this minute.

13:19.720 --> 13:20.280
Captions.

13:21.080 --> 13:23.000
It's very helpful for people like me.

13:24.760 --> 13:25.640
Are they harmful?

13:26.280 --> 13:28.040
Yeah, they can cause a lot of harm.

13:28.920 --> 13:34.440
Disinformation, defamation, brain on human gullibility.

13:35.160 --> 13:36.120
Plenty of examples.

13:37.400 --> 13:38.840
So they can cause harm.

13:38.840 --> 13:39.880
They can be of use.

13:40.840 --> 13:45.400
Those are the kinds of questions you ask about pure engineering,

13:46.120 --> 13:48.440
which can be very sophisticated and clever.

13:49.240 --> 13:54.200
I mean, the internal combustion engine is a very sophisticated device,

13:55.560 --> 14:01.160
but we don't expect it to tell us anything about how a gazelle runs.

14:02.840 --> 14:04.120
It's just the wrong question.

14:05.080 --> 14:18.120
Yeah, although I talk a lot to Jeff Hinton, and you'll be the first to concede that back propagation

14:19.560 --> 14:20.920
there's no evidence of that.

14:20.920 --> 14:26.040
And in fact, there's a lot of evidence that it wouldn't work in the brain.

14:29.080 --> 14:30.440
Reinforcement learning.

14:30.440 --> 14:39.000
You know, I spoke in a rich Sutton, that's been accepted as by a lot of people as

14:40.200 --> 14:49.160
an algorithmic model for brain activity in part of the brain, in the lower brain.

14:50.040 --> 15:02.840
So in terms of exploring the mechanisms of the brain, it seems that there is some usefulness.

15:02.840 --> 15:08.920
I mean, it says, you said there's, on the one hand, people look at the principles,

15:10.840 --> 15:16.920
and then they built through engineering, just as the analogy of a bird to an airplane,

15:17.560 --> 15:23.800
they've taken some of the principles and applied it through engineering and created something useful.

15:24.360 --> 15:33.000
But there are scientists that are looking at what's been created, like Hinton's criticism

15:33.000 --> 15:41.800
of back propagation, and are looking for other models that would fit with the principles they see

15:41.800 --> 15:46.920
in cognitive science or in the brain.

15:46.920 --> 15:52.040
And I mentioned this forward-forward algorithm, which you said you hadn't looked at.

15:52.040 --> 16:04.520
But I found it compelling in that it doesn't require signals to be passing back through

16:04.840 --> 16:18.440
the neurons. I mean, they pass back, but then stimulate other neurons as you move forward

16:18.440 --> 16:30.760
in time. But I mean, is there nothing that's been learned in the study of AI or the research

16:31.720 --> 16:32.600
of neural nets?

16:35.160 --> 16:42.920
But if you can find anything, it's great. Nothing against search, but it's just,

16:43.720 --> 16:51.160
but we have to remember what you asked about chatbots. What do we learn from them? Zero.

16:52.040 --> 17:02.280
For the simple reason that the systems work as well for impossible languages as for possible ones.

17:03.160 --> 17:10.840
So it's like the biologist with the new theory that has organisms and impossible ones and can't

17:10.840 --> 17:16.520
tell the difference. Now, maybe by the look at these systems, you'll learn something about

17:16.520 --> 17:24.280
possible organisms. Okay, great. All in favor of learning things. But there's no issues.

17:25.080 --> 17:33.480
It's just that the systems themselves, there are great claims by some of the leading figures in

17:33.480 --> 17:43.720
the field. We've solved the problem of language acquisition, namely zero contribution, because

17:43.720 --> 17:52.280
the systems work as well for impossible languages. Therefore, they can't be telling you anything about

17:52.280 --> 18:00.040
language acquisition. Period. Maybe they're useful for something else. Okay, let's take a look.

18:01.160 --> 18:09.400
Well, maybe for the audience that this is going out to, you know, I understand what you mean by

18:09.400 --> 18:17.000
impossible, impossible, but could you just give a brief synopsis of what you mean by impossible

18:17.000 --> 18:25.720
languages for people that haven't read your work? Well, I mean, there are certain general properties

18:26.600 --> 18:37.320
that every infant knows, already tested down to two years old, no evidence, couldn't have evidence.

18:38.280 --> 18:47.400
So one of the basic properties of language is that the linguistic rules apply to structures,

18:47.960 --> 18:54.280
not linear strings. So if you want to take a sentence like

18:54.600 --> 19:09.080
instinctively, birds that fly swim, it means instinctively they swim, not instinctively they

19:09.080 --> 19:18.920
fly. Well, the adverb instinctively has to find a verb to attach to. It skips the closest verb

19:19.560 --> 19:25.400
and finds the structurally closest ones. That principle turns out to be universal

19:26.040 --> 19:33.240
for all structures, all constructions, and all languages. What it means is that an infant

19:34.040 --> 19:43.560
from birth, as soon as you can test automatically, disregards linear order and disregards 100% of

19:43.560 --> 19:50.840
what it hears, notice, as all we hear is words in linear order, but you disregard that and you

19:50.840 --> 19:59.320
deal only with abstract structures in your mind, which you never hear. Take another simple example,

19:59.320 --> 20:09.240
take the friends of my brothers are in England. Who's in England? The friends of the brothers,

20:10.200 --> 20:16.280
the friends, not the brothers, the one that's adjacent, you just disregard all the linear

20:16.280 --> 20:23.640
information. It means you disregard everything you hear, everything, and you pay attention only

20:23.640 --> 20:31.160
to what your mind constructs. That's the basic, most fundamental property of language. Well,

20:31.160 --> 20:38.040
you can make up impossible languages that work with what you hear. Simple rule, take the first

20:38.760 --> 20:47.000
relevant thing, associate them. Friends of my brothers are here, brothers are the closest things,

20:47.000 --> 20:53.800
and the brothers are here. Trivial rule, much simpler than the rule we use. You can construct

20:53.800 --> 21:00.840
languages that use only those simple rules that will be based on the linear order of what we hear.

21:01.800 --> 21:09.640
Well, maybe children, people could acquire them as a puzzle somehow using non-linguistic

21:09.640 --> 21:16.360
capacities, but they're not what children, infants, reflexively construct with no evidence.

21:17.320 --> 21:24.600
Well, there's many things like this, impossible and impossible languages. Well, nobody's tried

21:24.600 --> 21:30.120
it out because it's too obvious how it's going to turn out. You take a large language model,

21:30.120 --> 21:37.800
apply it to one of these models, systems that use linear order. Of course, it's going to work fine,

21:38.360 --> 21:43.560
trivial rules. Well, that's a refutation of the system.

21:45.560 --> 21:51.000
Meaning that if you trained it on an impossible language, it would produce impossible languages.

21:51.000 --> 21:56.200
How would you mean? Well, you don't even have to train it because the rules are simple.

21:56.760 --> 22:02.840
Yeah. Rules are much simpler than the rules of language. Like taking things that are,

22:02.840 --> 22:11.080
take the example, the friends of my brother are here. The way we actually do it is we don't say,

22:11.720 --> 22:19.240
take the noun phrase that's closest. We don't do that. That would be trivial, but we don't do it.

22:19.240 --> 22:24.520
What we say is first construct the structure in your mind, friends of my brothers,

22:25.160 --> 22:31.080
then figure out that the central element in that structure is friends, not brothers.

22:31.720 --> 22:38.520
And then let's let it be talking about the head of it. It's a pretty complicated computation,

22:39.160 --> 22:45.720
but that's the one we do instantaneously and reflexively. And we ignore, and we never see it,

22:45.720 --> 22:52.520
hear it, remember? We don't hear structures. All we hear is words in linear order. What we hear

22:52.600 --> 22:59.400
is words in linear order. We never use that information. We use only the much more looks

22:59.400 --> 23:05.960
like complex. If you think about it computationally, it's actually simpler, but that's a deeper

23:05.960 --> 23:12.200
question, which is why we do it. To move to a different dimension, there's a reason for this.

23:13.160 --> 23:19.880
The reason has to do with the theory of computation. You're trying to construct

23:20.840 --> 23:28.680
an infinite array of structured expressions. Simplest way to do that, the simplest computational

23:28.680 --> 23:35.560
procedure is binary set formation. But if you use binary set formation, you're just going to get

23:35.560 --> 23:42.040
structures, not order. So what the brain is doing is the simplest computational system,

23:42.920 --> 23:50.920
which happens to be very much harder to use. Nature doesn't care about that. Nature constructs

23:50.920 --> 23:59.240
the simplest system, doesn't care about it, if it's hard to use or not. I mean, you know, nature

23:59.240 --> 24:05.880
could have saved us a lot of trouble if it had developed eight fingers instead of 10.

24:06.840 --> 24:14.040
Then we'd have a much better base for computation. But nature didn't care about that when it developed

24:14.040 --> 24:21.160
10 fingers. If you look at evolution, it pays no attention to function. It just constructs the

24:21.160 --> 24:27.320
best system at each point. There's a lot of misleading talk about that. But if you just think

24:27.320 --> 24:35.160
about the physics of evolution, say a bacterium swallows another organism,

24:36.760 --> 24:43.800
the basis for what became complex cells, and nature doesn't get the new system,

24:44.440 --> 24:50.600
it reconstructs it in the simplest possible way. It doesn't pay any attention to how

24:50.680 --> 24:59.240
complex organisms are going to behave, not what nature can do. And that's the way evolution works

24:59.240 --> 25:07.240
all the way down the line. So not surprisingly, nature constructed language so that it's

25:07.800 --> 25:16.200
computationally elegant, but dysfunctional, hard to use in many ways. Not nature's problem,

25:16.920 --> 25:22.920
just like every other aspect of nature. You can think of a way in which you can do it better,

25:22.920 --> 25:32.920
but it didn't happen stage by stage. Two questions from that. So your view is that

25:33.960 --> 25:38.920
artificial intelligence, as it's being called, and particularly generative AI,

25:39.800 --> 25:47.160
doesn't exhibit true intelligence. Is that right? I wouldn't even say that.

25:47.880 --> 25:56.680
It's irrelevant to the question of intelligence. It's not its problem. A guy who designs a jet plane

25:57.320 --> 26:06.920
is not trying to answer the question, how do eagles fly? So to say, well, it doesn't tell us how

26:06.920 --> 26:16.920
eagles fly is the wrong question to ask. It's not the goal. Except what people are struggling with

26:16.920 --> 26:25.880
right now. You've heard the existential threat argument that these models, if they get large

26:25.880 --> 26:32.600
enough, they'll actually be more intelligent than humans. That's science fiction. I mean,

26:32.600 --> 26:40.280
there is a theoretical possibility. You can give a theoretical argument that, in principle,

26:42.360 --> 26:52.520
a complex system with vast search capacity could conceivably turn into something that would start

26:52.520 --> 27:02.200
to do things that you can't predict, maybe beyond. But that's even more remote than some

27:03.880 --> 27:10.040
distant asteroid, maybe someday hitting the earth, could happen. I mean, if you read a

27:10.040 --> 27:18.120
serious scientist on this, like Max Tagmark, his book on the three levels of intelligence,

27:19.080 --> 27:27.560
does give a sound theoretical argument as to how a massive system could, say,

27:29.320 --> 27:38.280
run through all the scientific discoveries in history, maybe find out some better way of

27:40.120 --> 27:44.920
developing them and use that better way to design something new which would destroy us all.

27:45.880 --> 27:53.080
It's, in theory, possible, but it's so remote from anything that's available that it's a waste of

27:53.080 --> 28:00.040
time to think about it. Yeah, so your view is that whatever threat exists from

28:01.080 --> 28:10.520
generative AI, it's the more mundane threat of disinformation. Disinformation, defamation,

28:11.480 --> 28:20.760
gullibility, Gary Marcus has done a lot of work on this, real cases, those are problems. I mean,

28:20.760 --> 28:29.800
you may have seen that there was a, sort of as a joke, people, somebody developed a defamation of the

28:29.880 --> 28:40.840
pope, put an image of the pope, somebody could do it for you, duplicate your face so it looks more

28:40.840 --> 28:50.120
or less like your face, pretty much duplicate your voice, develop a robot that looks kind of like you,

28:50.120 --> 28:56.360
have you say some insane thing, it would be hard only an expert could tell whether it was you or

28:56.360 --> 29:03.720
none. It's like this was done already several times, but basically is a joke.

29:04.440 --> 29:09.720
When powerful institutions get started on it, it's not going to be a joke.

29:14.520 --> 29:21.640
Another argument that's swirling around these large language models is the question of

29:22.600 --> 29:30.200
a sentence of whether if the model is large enough, and this goes a little bit back to how

29:30.200 --> 29:36.520
there's a lot more going on in the brain than the neural network or the cerebral cortex, but

29:38.520 --> 29:46.280
that there is the potential for some kind of sentence, not necessarily equivalent to human

29:46.280 --> 29:55.720
sentence. These are vacuous questions. It's like asking, does a submarine really swim?

29:57.000 --> 30:03.560
You want to call that swimming? Yeah, it swims. You don't want to call it swimming? It's not a

30:03.560 --> 30:13.080
substantive question. Well, in the sense that it supports the view that there's no separation between

30:13.320 --> 30:22.200
consciousness and the material activities of the brain. There's a separation that hasn't

30:22.200 --> 30:30.840
been believed since the 17th century. John Locke, after Newton's demonstration, said, well leaves

30:30.840 --> 30:39.400
us only with the possibility that thinking is some property of organized matter. That's the 17th

30:39.400 --> 30:51.000
century. Yeah, okay. But the belief in a soul and consciousness is something separate from a

30:51.000 --> 30:59.640
material biology. It persists. The belief in all kinds of things. But within the rational part

30:59.720 --> 31:08.200
of the human species, once Newton demonstrated that the mechanical model doesn't work,

31:09.160 --> 31:17.560
there's no material universe in the only sense that was understood. Locke took the obvious conclusion,

31:17.560 --> 31:25.800
said, well, since matter, as Mr. Newton has demonstrated, has properties that we cannot

31:25.800 --> 31:32.520
conceive of. They're not part of our intuitive picture. Since matter has those properties,

31:33.800 --> 31:39.240
organized matter can also have the property of thought. This was investigated all through the

31:39.240 --> 31:48.600
18th century. Ended up finally with Joseph Priestley, a philosopher in the late 18th century,

31:48.600 --> 31:58.040
gave pretty extensive discussions of how material, organized material objects could have

31:58.040 --> 32:05.080
properties of thought. You can even find it in Darwin's early notebooks. It was kind of forgotten

32:05.080 --> 32:12.280
after that. Rediscovered in the late 20th century as some radical new discovery,

32:12.840 --> 32:20.040
astonishing hypothesis. Matter can think. Of course it can. In fact, we're doing it right now.

32:20.920 --> 32:27.160
But the only problem then is to find out what's involved in what we call thinking,

32:28.120 --> 32:34.440
what we call sentience, what are the properties of whatever matter is. We don't know what matter is,

32:34.440 --> 32:41.560
but whatever it turns out to be, whatever constitutes the world, what physicists don't

32:41.560 --> 32:49.800
know, but whatever it is, there's something organized. Elements of it can have various properties,

32:50.440 --> 32:56.920
like the properties that we are now using, properties that we call sentience. Then the question

32:56.920 --> 33:04.920
whether something else has sentience is as interesting as whether airplanes fly. If you're

33:04.920 --> 33:11.880
talking English, airplanes fly. If you're talking Hebrew, airplanes glide, they don't fly.

33:13.160 --> 33:19.560
It's not a substantive question. What metaphors do we like?

33:21.800 --> 33:30.840
But what you're saying then is that neural net may not be the engineering solution, but

33:31.800 --> 33:41.880
that eventually it may be possible to create a system outside of the human brain that can think

33:43.960 --> 33:51.640
whatever thinking means. And do what we call thinking. But whether it thinks or not is like

33:51.640 --> 34:00.520
asking the airplanes fly, not a substantive question. We shouldn't waste time on questions

34:00.520 --> 34:06.760
that are completely meaningless. Going back to the history then,

34:09.160 --> 34:14.120
you know, Minsky was very interested in the possibility of neural nets as a

34:17.720 --> 34:24.680
computational model. In Minsky's time, it looked as if neural nets were the right place to look.

34:25.480 --> 34:30.840
Now I think it's not so obvious, especially because of Galastal's work,

34:31.800 --> 34:37.720
which is not accepted by most neuroscientists, but seems to me pretty compelling.

34:38.760 --> 34:43.800
Can you talk a little bit about that because I haven't read that and I'm guessing our readers

34:43.800 --> 34:52.040
haven't, our listeners haven't. Galastal is not the only one. Roger Penrose is another

34:53.000 --> 34:58.920
Nobel Prize winning physicist, but a number of people have pointed out Galastal mostly that

34:59.800 --> 35:07.960
have argued, I think plausibly, that the basic component of a computational system,

35:09.000 --> 35:15.560
the basic element of essentially a Turing machine, cannot be constructed from neural nets.

35:16.520 --> 35:23.640
So you have to look somewhere else with a different form of computation. And he's also

35:23.640 --> 35:30.840
pointed out, but in fact, it's true that there's much richer computational capacity in the brain

35:30.840 --> 35:38.680
than neural nets, even internal to a cell. There's massive computational capacity

35:39.640 --> 35:46.040
intercellular. So maybe that's involved in computation. And then there's by now some

35:46.600 --> 35:52.920
experimental work, I think, giving some evidence for this, but it's a problem for neuroscientists

35:52.920 --> 36:00.520
to work on. I'm not an expert in the field. I'm looking at it from the outside,

36:01.240 --> 36:06.040
so don't take my opinion too seriously. But to me, it looks pretty compelling.

36:06.680 --> 36:13.320
But whatever it is, neural nets or something else, yes, some organization of them, of whatever

36:13.320 --> 36:19.800
is there, is giving us the capacity to do what we're doing. So if you're a scientist, what you do is

36:21.160 --> 36:28.200
approach it in two different ways. One is you try to find the properties of the system.

36:28.840 --> 36:35.000
What is the nature of the system? That's first step kind of thing I was talking about before with

36:35.640 --> 36:43.480
structure dependence. What are the properties of the system that an infant automatically

36:43.480 --> 36:49.960
develops in the mind? And there's a lot of work on that. From the other point of view, you can say,

36:50.760 --> 36:57.080
what can we learn about the brain that relates to this? Actually, there is some work. So there is

36:57.080 --> 37:10.920
neurophysiological studies which have shown that for artificial languages that violate the principle

37:11.640 --> 37:18.120
that I mentioned, this structure dependent principle, if you train people on those,

37:19.080 --> 37:25.480
the ordinary language centers don't function. You get diffuse functioning of the brain,

37:25.480 --> 37:33.720
means they're being treated as puzzles basically. So you can find some neurological correlates of

37:34.440 --> 37:40.360
some of the things that are discovered by looking at the nature of the phenotype.

37:41.640 --> 37:48.840
But it's very hard for humans for a number of reasons. We know a lot about human, the physiology

37:48.920 --> 37:57.480
of human vision. But the reason is because of invasive experiments with nonhumans, cats,

37:58.680 --> 38:06.200
monkeys and so on. Can't do that for language. There aren't any other organisms unique to humans.

38:07.000 --> 38:13.480
So there's no comparative studies. You can think of a lot of invasive experiments which

38:13.560 --> 38:21.240
teach you a lot. You can't do them for ethical reasons. So study of the neurophysiology of

38:22.120 --> 38:31.240
human cognition is a uniquely hard problem. In its basic elements like language,

38:31.240 --> 38:38.920
it's just unique to the species. And in fact, a very recent development in evolutionary history,

38:38.920 --> 38:45.880
probably the last couple of hundred thousand years, which is nothing. So you can't do the

38:45.880 --> 38:50.760
invasive experiments for ethical reasons. You can think of them, but you can't do them,

38:50.760 --> 38:58.120
fortunately. And there's no comparative evidence. So it's much harder to do. You have to do things

38:58.120 --> 39:07.160
like, you know, looking at a blood flow in the brain, MRI type things, electrical stimulation,

39:07.160 --> 39:13.560
looking from the outside. It's tough. It's not like doing the kind of experiments you can think of.

39:14.600 --> 39:22.200
So it's very hard to find out the neurophysiological basis for things like use of language. But

39:22.920 --> 39:27.240
it's one way to proceed. And the other way to proceed is learn more about the phenol.

39:27.800 --> 39:38.120
It's like chemistry for hundreds of years. You just postulated the existence of atoms.

39:39.560 --> 39:44.120
Nobody could see them. You know, why are they there? You know, because unless

39:44.840 --> 39:51.640
there are atoms with the Dalton's properties, you don't explain anything. Early genetics,

39:52.600 --> 39:58.120
early genetics work before anybody had any idea what a gene is. You just looked at the

39:59.800 --> 40:04.040
properties of the system, try to figure out what must be going on.

40:05.400 --> 40:12.600
It's the way astrophysics works. You know, most of science works like that. So this does too.

40:13.320 --> 40:21.000
When you talk about invasive exploration, there are tools that are increasingly

40:21.080 --> 40:28.920
sophisticated. I'm thinking of neural link, Elon Musk's startup that has these super fine

40:31.080 --> 40:39.080
electrodes that can be put into the brain without damaging individual neurons.

40:40.520 --> 40:46.280
There's actually, I think, much more advanced than that is work that's being done with

40:47.240 --> 40:54.360
patients under brain surgery. Under brain surgery, with the brain basically exposed,

40:54.360 --> 41:03.160
there are some noninvasive procedures that can be used to study what particular

41:04.600 --> 41:09.480
parts of the brain, even particular neurons are doing. It's very delicate work.

41:10.440 --> 41:18.280
But there is some work going on. One person is working on it is Andrea Moro,

41:18.280 --> 41:23.560
the same person who designed the experiments that I described before about impossible languages.

41:24.360 --> 41:33.320
That seems to me a promising direction. There's other kinds of work. I could mention some of it.

41:33.320 --> 41:43.000
Alec Moran, why you was doing interesting studies that shed some light on the very

41:43.000 --> 41:52.600
elementary function. How do words get stored in the brain? What's going on in the brain that

41:54.040 --> 42:01.400
tells us that blake is a possible word, but the nick isn't for an English speaker. It is for

42:01.480 --> 42:06.840
an Arabic speaker. What's going on in the brain that deals with that?

42:08.280 --> 42:18.040
Hard work. David Peppel, another very good neuroscientist, has found evidence

42:19.000 --> 42:28.920
for things like pharyngeal structure in the brain. But the kinds of invasive experiments

42:28.920 --> 42:36.200
you can dream of, you can think of, he's just not allowed to do. So you have to try it in much

42:36.200 --> 42:44.280
indirect ways. Do you think that understanding cognition has advanced in your lifetime?

42:45.080 --> 42:52.120
And are you hopeful that we'll eventually really understand how the brain thinks?

42:52.200 --> 43:04.360
Well, there's been vast improvement in understanding the phenotype that we know a great deal about

43:04.360 --> 43:13.320
that was not known even a few years ago. There's been some progress in the neuroscience of

43:14.120 --> 43:25.880
the relates to it, but it's much harder. Yeah. I'm just curious about where you are in,

43:26.840 --> 43:34.040
not physically you're in Arizona, but where you are in your thinking. Are you still

43:34.200 --> 43:49.320
pushing forward in trying to understand language in the brain or are you sort of retired, so to speak,

43:49.320 --> 43:56.280
at this point? No, very much involved. I mean, I don't work on the neurophysiology.

43:56.600 --> 44:08.120
A man I mentioned, Andrea Moro, happens to be a good friend. So I follow the work they're doing,

44:08.120 --> 44:15.240
we interact, but my work is just on the phenotype. What's the nature of the system?

44:15.880 --> 44:21.480
And there, I think we're learning a lot. I'm right in the middle of papers at the moment,

44:22.440 --> 44:28.840
looking at more subtle, complex properties. The idea is essentially to find

44:30.040 --> 44:38.760
what I said about binary set formation. How can we show that from the simplest

44:39.800 --> 44:49.080
computational procedures, we can account for the apparently complex and apparently varied

44:49.880 --> 44:56.200
properties of the language systems. There's a fair amount of progress on that,

44:57.640 --> 45:05.480
that was unheard of 20, 30 years ago. So this is all new. Understanding is one thing and then

45:06.120 --> 45:18.520
re-creating it through computation in external hardware is another. Is that a blind ally or do

45:18.520 --> 45:27.320
you think that? Well, at the moment, I don't see any particular point in it. If there is some point,

45:28.280 --> 45:37.240
okay. I mean, the kinds of things that we're learning about the nature of language,

45:38.360 --> 45:43.720
I suppose you could construct some sort of system that would duplicate them,

45:45.320 --> 45:54.520
but it doesn't seem any obvious point to it. It's like taking chemistry in 100 years ago and saying,

45:55.400 --> 45:58.760
can I construct models that will look sort of like,

45:59.480 --> 46:06.840
suppose you took, I was saying, a kick of the diagram for an organic molecule

46:07.960 --> 46:14.680
and study its properties. You could presumably construct a mechanical model

46:15.480 --> 46:21.880
that would do some of those things. Would it be useful? Apparently chemists didn't think so, but

46:22.520 --> 46:27.400
if it would, okay. If it wouldn't, then don't.

46:30.120 --> 46:35.080
Nonetheless, I mean, we are using neural nets even in this call.

46:37.080 --> 46:46.440
Do you see, I mean, setting inside the question of whether or not they help us understand anything

46:46.440 --> 46:54.600
about the brain. Are you excited at all in about the promise that these large

46:54.600 --> 46:58.680
models hold? I mean, because they do something very useful.

46:59.800 --> 47:07.960
They are. Like I said, I'm using it right now. I think it's fine for me, somebody who can't hear

47:08.040 --> 47:16.040
to be able to read what you're saying pretty accurately. It's an achievement, so great.

47:17.400 --> 47:25.160
I have nothing against technology. And who do you think is going to carry on

47:27.400 --> 47:34.840
your work from here? I mean, are there any students of yours who you think we should

47:34.920 --> 47:42.120
be paying attention to? Well, quite a lot. A lot of young people doing fine work.

47:43.000 --> 47:48.600
In fact, I work with a, closely with a small research group

47:50.920 --> 47:58.520
by now, spread all over the world. We meet virtually from Japan and Holland and other places

47:58.520 --> 48:02.200
regularly working on the kinds of problems I was talking about.

48:03.640 --> 48:07.960
But right now, I should say it's a pretty special interest. Most linguists aren't

48:07.960 --> 48:15.880
interested in these foundational questions. But I think that's happened to be my interest.

48:15.880 --> 48:24.360
I want to see if we can show the, ultimately try to show that language is essentially a

48:24.360 --> 48:33.960
natural object. I mean, there was an interesting paper written about the time that I started

48:33.960 --> 48:42.840
working on this by Albert Einstein in 1950. He had an article in Scientific American, which

48:43.560 --> 48:49.320
I read, but didn't appreciate at the time, began to appreciate later, in which he talked

48:49.320 --> 48:56.440
about what he called a miracle creed. He has an interesting history. It goes back to Galileo.

48:57.320 --> 49:07.080
Galileo had a maxim saying, nature is simple. It doesn't do things in a complicated way if it

49:07.080 --> 49:15.720
could do them in a simple way. Galileo's maxim couldn't prove it. But they said, I think that's

49:15.720 --> 49:22.840
the way it is. That's the task of the scientist to prove it. Well, over the centuries, it's been

49:22.840 --> 49:30.840
substantiated case after case. It shows up in Leibniz's principle of optimality. But by then,

49:30.840 --> 49:38.760
there was a lot of evidence for it. By now, it's just a norm for science. It's what Einstein called

49:38.760 --> 49:48.600
the miracle creed. Nature is simple. Our task is to show it. It says, improve it. Skeptic can say,

49:48.600 --> 49:56.920
I don't believe it. Okay. But that's the way science works. Well, this one's worked the same way for

49:56.920 --> 50:04.200
language. But you couldn't have proposed that 50 years ago, 20 years ago. I think now you can

50:04.520 --> 50:13.000
believe that maybe language is just basically a perfect computational system at its base.

50:14.120 --> 50:20.680
You look at the phenomena, it doesn't look like that. But the same was true of biology. Go back to

50:20.680 --> 50:32.200
the 1950s, 1960s, biologists assumed that organisms could vary so widely that each one has to be

50:32.200 --> 50:39.400
studied on its own without bias. By now, that's all forgotten. It's recognized that there,

50:40.040 --> 50:46.360
since the Cambrian explosion, there's virtually no variation in the kinds of organisms,

50:47.160 --> 50:55.800
fundamentally all the same. Deep homologies, and so on. So even been proposed that there's a universal

50:55.800 --> 51:03.880
genome, not totally accepted, but not considered ridiculous. Well, I think we're in the same

51:03.880 --> 51:09.640
direction as the study of language. Now, let me say again, there's not many linguists interested in

51:09.640 --> 51:17.000
this. Most linguists, like most biologists, are studying particular things, which is fine. You

51:17.000 --> 51:27.240
learn a lot that way. But I think it is possible now to formulate a plausible thesis that language is a

51:27.960 --> 51:34.040
natural object like others, which evolved in such a way as to have perfect design,

51:34.840 --> 51:40.520
but to be highly dysfunctional. Because that's true of natural objects, generally. It's part of

51:40.520 --> 51:49.400
the nature of evolution, which doesn't take into account possible functions. I mean, the last stage

51:49.400 --> 51:57.560
of evolution, the reproductive success that does take function into account, natural selection,

51:58.520 --> 52:05.560
that's a fringe of evolution. It's just the peripheral fringe, very important, not denigrated,

52:05.560 --> 52:13.720
but it's the basic part of evolution is constructing the optimal system that meets

52:13.720 --> 52:20.280
the physical conditions established by some disruption in the system. That's the core of

52:20.280 --> 52:30.840
evolution. That's what Turing studied. Darcy Thompson, others by now, I think it's understood.

52:30.840 --> 52:37.160
And I think maybe the study of this particular biology after a language is a biological object.

52:37.800 --> 52:44.440
So why should it be different? Let's see if we can show it. There's been a lot of talk in the news

52:44.440 --> 52:55.720
recently about extraterrestrial craft having been found by the government. I don't put much

52:55.720 --> 53:04.920
talk in it, but imagine that there is extraterrestrial life, advanced forms of life. Do you think that

53:04.920 --> 53:15.800
their language would have developed the same way if it's based on these simple principles? Or is it

53:16.600 --> 53:24.840
could there be other forms of language in other biological organisms that would be quote unquote

53:24.840 --> 53:33.720
impossible in the human context? Back around the 1960s, I guess, Minsky

53:35.960 --> 53:42.680
studied with one of his students, Daniel Belbrom, studied the simplest Turing machines,

53:43.560 --> 53:52.040
few estates, fewest symbols, and asked what happens if you just let them run free?

53:53.000 --> 54:02.680
Well, it turned out that most of them crash, either get into endless loops or just crash

54:03.480 --> 54:09.080
don't proceed. But the ones that didn't crash all produced the successor function.

54:10.680 --> 54:18.280
So he suggested what we're going to find if any kind of intelligence develops is

54:18.840 --> 54:25.000
it'll be based on the successor function. And if we want to try to communicate with some

54:25.960 --> 54:31.160
extraterrestrial intelligence, we should first see if they have the successor function

54:32.200 --> 54:36.600
and then maybe build up from there. Well, turns out the successor

54:38.200 --> 54:47.480
happens to be what you get from the simplest possible language. The language is one symbol

54:47.480 --> 54:53.000
and the simplest form of binary set formation basically gives it a successor function.

54:53.880 --> 54:59.480
Add a little bit more to it, you get something like arithmetic. Add a little bit more to it,

54:59.480 --> 55:07.880
you get something like the poor properties of language. So it's conceivable that if there is any

55:08.520 --> 55:14.920
extraterrestrial intelligence, it would have pursued the same course. Where it goes from there,

55:14.920 --> 55:22.920
we don't know enough to say. And back to the idea that there is no super natural realm,

55:22.920 --> 55:31.240
that the consciousness is an emergent property from the physical attributes of the brain.

55:33.880 --> 55:42.840
Do you believe in a higher intelligence behind the creation or continuation of the universe?

55:45.640 --> 55:55.320
I don't see any point in vacuous hypotheses. If you want to believe it okay, it has no consequences.

55:57.880 --> 56:03.720
But do you believe it? No, I don't see any point in believing things for

56:05.160 --> 56:11.560
which there's no evidence and do no work. And another thing I've always wanted to ask

56:12.200 --> 56:20.840
someone like you, clearly your intelligence surpasses most peoples.

56:21.960 --> 56:27.560
I don't think so. Well, that's a good, that's interesting that you would say that. You think

56:27.560 --> 56:35.160
it's just a matter of applying yourself to study throughout your career.

56:35.160 --> 56:45.880
I have certain talents, I know, like not believing things just cause people believe them.

56:48.120 --> 56:55.160
And keeping an open mind and looking for arguments and evidence, not

56:56.360 --> 57:01.160
anything we've been talking about when meaningless questions are proposed, like

57:01.160 --> 57:09.560
our other organism, sentient or the submarine swim, I say let's discard them and look at

57:09.560 --> 57:19.240
meaningful questions. If you just pursue common sense like that, I think you can make some progress.

57:19.240 --> 57:25.960
Same on the questions we're talking about language. If you think it through, there's every reason why

57:26.600 --> 57:32.920
the organic object language should be an object. If so, it should follow the

57:33.480 --> 57:40.840
general principles of evolution, which satisfy what Einstein called the miracle creed. So why

57:40.840 --> 57:48.680
shouldn't language, so let's pursue that CFR we can do. I think that's just common sense. Many

57:48.680 --> 57:55.960
people think it's superior intelligence. I don't think so. That's it for this episode. I want to

57:55.960 --> 58:02.520
thank Noam for his time. If you'd like a transcript of this conversation, you can find one on our

58:02.520 --> 58:16.040
website, I on AI, that's EYE-ON.AI. In the meantime, remember, the singularity may not be near,

58:16.040 --> 58:26.760
but AI is about to change your world. So pay attention.

