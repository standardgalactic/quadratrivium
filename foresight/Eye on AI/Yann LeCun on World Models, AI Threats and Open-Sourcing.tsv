start	end	text
0	3200	Even if you train a system to have a world model that can predict what's going to happen next,
3200	7520	the world is really complicated and there's probably all kinds of situations that the system
7520	13440	hasn't been trained on and need to, you know, fine-tune itself as it goes. The question of how
13440	18720	we organize AI research going forward which is somewhat determined by how afraid people are
18720	24320	of the consequences of AI. So if you have a rather positive view of the impact of AI on society and
24320	28880	you trust humanity and society and democracies to use it in good ways, then the best way to
28880	35680	make progress is to open research. AI might be the most important new computer technology ever.
35680	41760	It's storming every industry and literally billions of dollars are being invested. So buckle up.
42400	48240	The problem is that AI needs a lot of speed and processing power. So how do you compete
48240	54880	without cost spiraling out of control? It's time to upgrade to the next generation of the cloud,
55520	63120	Oracle Cloud Infrastructure or OCI. OCI is a single platform for your infrastructure,
63120	71120	database, application, development, and AI needs. OCI has four to eight times the bandwidth of other
71120	78720	clouds, offers one consistent price instead of variable regional pricing, and of course nobody
78720	85920	does data better than Oracle. So now you can train your AI models at twice the speed and less than
85920	93360	half the cost of other clouds. If you want to do more and spend less like Uber, eight by eight,
93920	106000	and Databricks Mosaic, take a free test drive of OCI at oracle.com slash ion AI. That's E-Y-E-O-N-A-I
106000	117040	all run together oracle.com slash ion AI. Hi, I'm Craig Smith and this is ION AI. In this episode,
117040	124480	I speak again with Yan Lacoon, one of the founders of deep learning and someone who followers of AI
124480	131680	should need no introduction to. Yan talks about his work on developing world models on why he
131680	139280	does not believe AI research poses a threat to humanity and why he thinks open source AI models
139280	146720	are the future. In the course of the conversation, we talk about a new model, Gaia 1, developed by a
146720	155040	company called Wave AI. I'll have an episode with Wave's founder to further explore that world model
155040	160960	which has produced some startling results. I hope you find the conversation with Yan
160960	168480	as enlightening as I did. First, the notion of world model is the idea that the system would get
168480	174960	some idea of the state of the world and be able to predict sort of following states of the world
174960	178960	resulting from just the natural evolution of the world or resulting from an action that the
178960	185440	agent might take. If you have an idea of the state of the world and you imagine an action that
185440	191920	you're going to take and you can predict the resulting state of the world, that means you
191920	195440	can predict what's going to happen as a consequence of a sequence of actions and that means you can
195440	201760	plan a sequence of actions to arrive at a particular goal. That's really what a world model is. At
201760	209680	least that's what the Wave people have understood the word in other contexts, like in the context of
209680	216080	optimal control and robotics and things like that. That's what a world model is. Now, there's several
216080	223520	levels of complexity of those world models, whether they model yourself, the agent, or whether they
223520	232880	model the external world, which is much more complicated. Training a world model basically
232880	238240	consists in just observing the world go by and then learning to predict what's going to happen next,
238960	245120	or observing the world taking an action and then observing the resulting effect, an action that
245120	251920	you take as an agent or an action that you see other agents taking. That establishes
252160	261920	causality essentially. You could think of this as a causal model. Those models don't need to predict
261920	267120	all the details about the world. They don't need to be generative. They don't need to predict exactly
267120	274400	every pixel in a video, for example, because what you need to be able to predict is enough details,
275280	287200	some sort of abstract representation to allow you to plan. You're assembling something out of wood
287200	294320	and you're going to put two planks together and attach them with screws. It doesn't matter the
294320	300480	details of which type of screwdriver you're using or the size of the screw within some limits and
300480	305440	things like that. There are details that in the end don't matter as to what the end result
305440	312400	will be or the precise grain of the wood and things of that type. You need to have some
312400	316800	abstract level of representation within which you can make the prediction without having to
316800	324800	predict every detail. That's why those JPA architectures I've been advocating are useful. Models like
325760	331840	the Gaia 1 model from Wave actually makes prediction in an abstract representation space.
331840	337280	There's been a lot of work in that area for years also at FAIR, but generally the
337920	343200	abstract representation were pre-trained. The encoders that would take images from videos and
343200	348000	then encode them into some representation were trained in some other way. The progress we've
348000	353840	made over the last six months in self-improvised learning for images and video is that now we
353840	361120	can train the entire system to make those predictions simultaneously. We have systems now that can
362400	369280	learn good representations of images and the basic idea is very simple. You take an image,
370560	377520	you run it through an encoder, then you corrupt that image, you mask parts of it, for example,
378080	385200	or you transform it in various ways, you blur it, you change the colors, you change the framing a
385200	390000	little bit, and you run that corrupted image through the same encoder or something very similar.
391600	397120	And then you train the encoder to predict the features of the complete image from the features
397920	405520	of the corrupted one. You're not trying to reconstruct the perfect image,
406400	411520	you're just trying to predict the representation of it. And this is different,
411520	416800	this is not generative in the sense that it does not produce pixels. And that's the secret to getting
416800	422080	self-supervisual into work in the context of images and video. You don't want to be predicting pixels,
422080	427040	it doesn't work. You can't predict pixels as an afterthought, which is what the Gaia system is
427040	432080	doing by sticking a decoder on it and with some diffusion model that will produce a nice image,
432080	437280	but that's kind of a second step. If you train the system by predicting pixels,
438000	442000	you just don't get good representations, you don't get good predictions, you get blurry
442000	448480	predictions most of the time. So that's what makes learning from images and video fundamentally
448480	454400	different from learning from text because in text you don't have that problem. It's easy to predict
454400	461040	words, even if you cannot do a perfect prediction because language is discrete. So language is
461040	472640	simple compared to the real world. And there's a lot written right now about the energy required
472720	480960	in the computational resources GPUs required to train language models. Is it less
482080	488480	in training a world model like using iJAPA architecture? Well, it's hard to tell because
488480	495760	there is no equivalent training procedure, self-supervised training procedure for video,
495760	501760	for example, that does not use JAPA. The ones that are generative don't really work.
502880	509840	Yeah. Well, but this architecture could also be applied to language, couldn't it?
510720	517520	Oh yeah, absolutely. Yeah, so you could very well use a JAPA architecture that makes prediction
517520	524400	in representation space and apply to language. Yeah, definitely. And in that case, would it be
524480	532000	less computationally intense than training a large language model?
532000	539760	It's possible. It's not entirely clear either. I mean, there is some advantage regardless of what
539760	545440	technique you're using to making those models really big. They just seem to work better if you
545440	554480	make them big. So if you make them bigger. So scaling is useful. Contrary to some claims,
554480	559680	I do not believe that scaling is sufficient. So in other words, we're not going to get anywhere
559680	570320	close to human level AI. In fact, not even any more level AI by simply scaling up language models.
571120	576320	Even multimodal language models that we applied to video, we're going to have to find new concepts,
576320	584000	new architectures. And I've written a vision paper about this a while back of a different
584000	590720	type of architecture that would be necessary for this. So scaling is necessary, but not sufficient.
591680	601120	And we're missing some basic ingredients to get to human level AI. We're fooled by the fact that
601120	606400	LLMs are fluent. And so we think that they have human level intelligence because they can manipulate
606400	616320	language. But that's false. And in fact, there's a very good symptom for this, which is that
617280	624960	we have systems that can pass the bar exam, but answering questions from text by basically
624960	632880	regurgitating what they've learned, more or less by road. But we don't have completely autonomous
632880	639360	level five cell driving cars, or at least no system that can learn to do this in about 20 hours
639520	646400	of practice, just like any 17-year-old. And we certainly don't have any domestic robot that can
647440	651200	clear up the dinner table and fill up the dishwasher attest that any 10-year-old can learn
651200	658800	in one shot. So clearly, we're missing something big. And that something is an ability to learn
658800	663360	how the world works and the world is much more complicated than language. And also being able
663360	670720	to plan and reason, basically having a mental world model that allows to plan and predict
671360	677120	consequences of actions. That's what we're missing. It takes a while before we figure this out.
678160	687840	You were on another paper that talked about augmented language models. And
687840	693680	in the embodied touring test, was that the same paper, the embodied touring test?
696160	701760	Can you talk about that? First of all, what is the embodied touring test? I didn't quite
701760	710800	understand that. Well, okay, it's a different concept. But it's basically the idea that you
711760	721280	do, it's based on the Moravec paradox, right? So Moravec many years ago noticed that things that
721280	727360	appeared difficult for humans turned out to sometimes be very easy for computers to do,
727360	733600	like playing chess, much better than humans. Or I don't know, computing integrals or whatever,
733600	739600	certainly doing arithmetic. But then there are things that we take for granted as humans that
739600	743840	we don't even consider them intelligent tasks that we are incapable of reproducing with computers.
745120	751840	And so that's where the embodied touring test comes in. Observe what a cat can do or how fast a cat
751840	761360	can learn new tricks or how a cat can plan to jump on a bunch of different furniture to get to the
761360	767680	top of wherever it wants to go. That's an amazing feat that we can't reproduce with robots today.
768640	776320	So that's kind of the embodied touring test, if you want. Like, can you make a robot that
777040	783360	can behave, have behaviors that are easily wishable from those of animals, first of all,
783360	791440	and can acquire new ones with the same efficiency as animals? Then the augmented
791440	798400	LLM paper is different. It's about how do you sort of minimally change large language models so
798400	804880	that they can use tools so they can, to some extent, plan actions. Like, you know, you need to compute
804880	809280	the product of two numbers, right? You just call a calculator and you know you're going to get the
809280	814400	product of those two numbers. And LLMs are notoriously bad for arithmetic, so they need to do
814400	819680	this kind of stuff or do a search, you know, using a search engine or database lookup or
819680	824240	something like that. So there's a lot of work on this right now and it's somewhat incremental. Like,
824240	829840	you know, how can you sort of minimally change LLM and take advantage of their current capabilities
829840	837600	but still augment them with the ability to use tools? Yeah. And I don't want to get into the
838240	844160	too much into the threat debate. But, you know, you're on one side, your colleagues,
845040	851280	Jeff and Yashor on the other. I recently saw a picture of the three of you. I think you put that
851280	861440	up on social media, saying how, you know, you can disagree but still be friends. This idea of
861440	868960	augmenting language models with stronger reasoning capabilities and the ability,
868960	876720	and agency, the ability to use tools is precisely what Jeff and Yashor are worried about.
878240	889520	Can you just, why are you not worried about that? Okay. So first of all, what you're describing
889520	898240	is not necessarily what they are afraid of. They are alerting people and various governments and
898240	904880	others about various dangers that they perceive. Okay. So one danger, one set of dangers are
905440	910480	relatively short-term. There are things like, you know, bad people will use technology for bad
910480	916880	things. What can bad people use powerful AI systems for? And one concern that, you know,
916880	924640	governments have been worried about and intelligence agencies encounter intelligence and stuff like
924640	932720	that is, you know, could value-intentioned organizations or countries use LLM to help them,
932720	939840	I don't know, design pathogens or chemical weapons or other things or cyber attacks,
939840	943840	you know, things like that, right? Now, those problems are not new. Those problems have been
943840	951200	with us for a long time. And the question is, what incremental help would AI systems bring to the
951200	960160	table? So my opinion is that as of today, AI systems are not sophisticated enough to provide
960160	967200	any significant help for such value-intentioned people because those systems are trained with
967200	971520	public data that is publicly available on the internet. And they can't really invent anything.
971520	976320	They're going to regurgitate with a little bit of interpolation if you want. But
977120	984800	they cannot produce anything that you can't get from a search engine in a few minutes.
986160	990000	So that claim is being tested at the moment. There are people who are actually kind of
990560	995280	trying to figure out, like, is it the case that you can actually do something, you're
995280	999840	unable to do something more dangerous with sort of current AI technology that you can do with a
999840	1007760	search engine results are not out yet. But my hunch is that, you know, it's not going to enable
1007760	1014160	a lot of people to do significantly bad things. Then there is the issue of things like code
1014160	1018800	generation for cyber attacks and things like this. And those problems have been with us for years.
1019760	1024080	And the interesting thing that most people should know, like, you know, also for like
1024080	1028880	disinformation or attempts to corrupt the electoral process and things like this. And what's
1029600	1034480	very important for everyone to know is that the best countermeasures that we have
1034480	1041040	against all of those attacks currently use AI massively. Okay. So AI is used as a defense
1041040	1049280	mechanism against those attacks. It's not actually used to do the attacks yet. And so now it becomes
1049280	1056560	the question of, you know, who has the better system, like other countermeasures? Is the AI
1057520	1063840	countermeasures significantly better than the AI is used by the attackers so that, you know,
1063840	1070560	the problem is satisfactorily mitigated. And that's what we are. Now, the good news is that there are
1070560	1076720	many more good guys and bad guys. They're usually much more competent. They're usually much more
1076720	1082960	sophisticated. They're usually much more better funded. And they have a strong incentive to take
1082960	1091680	down the attackers. So it's a game of cat and mouse, just like every security that's ever existed.
1092480	1100080	There's nothing new there. Okay. Nothing quite entirely new. Yeah. But then there is the question
1100080	1108400	of existential risk, right? And this is something that both Jeff and Yosha have been thinking of
1109200	1114000	fairly recently. So for Jeff, it's only sort of just before last summer that he became,
1115200	1119520	he started thinking about this because before he thought he was convinced that the kind of
1119520	1125120	algorithms that we had were significantly inferior to the kind of learning algorithm that the brain
1125120	1132400	used. And the epiphany he had was that, in fact, no, because looking at the capabilities of
1133200	1138080	large English models, they can do pretty amazing things with a relatively small number of neurons
1138080	1141840	and synapses. He said, maybe they're more efficient than the brain. And maybe the learning algorithm
1141840	1145600	that we use, back propagation, is actually better than whatever it is that the brain uses.
1146240	1149280	So he started thinking about like, you know, what are the consequences? And
1150320	1153680	but that's very recent. And in my opinion, he hasn't thought about this enough.
1154000	1163280	Yosha went to a similar epiphany last winter, where he started thinking about the long-term
1163280	1169520	consequences. And came to the conclusion also that there was a potential danger.
1170880	1175360	They're both convinced that AI has enormous potential benefits. They're just worried
1176560	1180880	about the dangers. And they're both worried about the dangers because they have some doubts
1181680	1187600	about the ability of our institutions to do the best with technology.
1189360	1196640	You know, whether they are political, economic, geopolitical, financial institutions,
1196640	1203920	or industrial, to do the right thing, to be motivated by the right thing. So
1204480	1212880	you know, if you trust the system, if you trust humanity and democracy,
1214960	1223200	you might be entitled to believe that society is going to make the best use of
1223200	1228640	future technology. If you don't believe in the solidity of those institutions,
1228640	1234640	then you might be scared. Okay. I think I'm more confident in humanity and democracy than they are.
1235280	1239040	And, and, you know, whatever current systems and they are, I've been thinking about this
1239040	1247440	problem for much longer, actually, since at least 2014. So when I started fair at Facebook at the
1247440	1253120	time, it became pretty clear, pretty early on that, you know, deploying AI systems was going to have
1253840	1259840	big consequences on people in society. And we got confronted to this very early.
1260560	1265360	And so I started thinking about those problems very early on. Things like, you know, counter
1265360	1272320	measures against like bias in AI systems, systematic bias, counter measures against attacks,
1274080	1278560	or, you know, detection of hate speech in every language, things like that. These are things that
1278560	1283760	people at fair worked on and then were eventually deployed. To just to give you an example, the
1284560	1289120	proportion of hate speech that was taken down automatically by AI systems five years ago,
1289120	1297920	you know, in 2017, was about 20 to 25%. Last year, it was 95%. And the difference is entirely due to
1297920	1303520	progress in natural language understanding, entirely grew to transformers that are pretrained
1303600	1308640	self-supervised and can essentially detect hate speech in any language. Not perfectly. Nothing
1308640	1314560	is perfect. It's ever perfect. But AI is massively there. And that's the solution. So I started
1314560	1322000	thinking about those issues, including existential risk, very early on. In fact, in 2015, early 2016,
1322000	1328160	actually, I organized a conference hosted at NYU on the future of AI, where a lot of those
1328160	1337600	questions were discussed. I invited people like, you know, Eric Schmidt and Mark Schreffer, who
1337600	1346000	was the CTO of Facebook at the time. A lot of people, both from the academic and AI research
1346000	1350880	side and from the industry side. And there were two days, a public day and kind of a more private
1350880	1355920	day. What came out of this is the creation of an institution called a partnership on AI.
1355920	1362400	So this is a discussion I had with Mr. Sabis, which was, you know, would it be useful to have a
1362400	1367840	forum where we can discuss before they happen, sort of bad things that could happen as a consequence
1367840	1375840	of deploying AI? Pretty soon, we brought on board Eric Horvitz and a bunch of other people,
1375840	1379200	and we co-founded this thing called a partnership on AI, which basically has been
1380160	1389520	funding studies about AI ethics and consequences of AI and publishing guidelines about, you know,
1389520	1394400	how you do it right to me and my time. So this is not a new thing for me. Like, I've been thinking
1394400	1399360	about this for 10 years, essentially. Whereas for Yosha and Jeff, it's much more recent.
1400160	1409360	Yeah. But nonetheless, this augmented AI or augmented language models that have stronger
1409360	1419200	reasoning and agency raises the threat, regardless of whether or not it can be countered
1419920	1427600	to a higher level. Right. Okay. So I guess the question there becomes, what is the blueprint
1428240	1434640	of future AI systems that will be capable of reasoning and planning, will understand how the
1434640	1440800	world works, will be able to, you know, use tools and have agency and things like that. Right.
1442320	1449040	And I tell you, they will not be autoregressive LLMs. So the problems that we see at the moment
1449920	1455840	of autoregressive LLM, the fact that they hallucinate, they sometimes say really stupid
1455840	1460640	things. They don't really have a good understanding of the world. People claim that they have some
1460640	1465440	simple word model, but it's very implicit and it's really not good at all. Like, for example,
1466640	1472640	you know, you can tell an LLM that A is the same as B. And then you ask if B is the same as A,
1472640	1478000	and it will say, I don't know, or no. Right. I mean, those things don't really understand
1478000	1486160	logic or anything like that. Right. So the type of system that we're talking about that might be,
1487040	1492880	that might approach any more level intelligence and let alone human level intelligence have not
1492880	1500880	been designed. They don't exist. And so discussing their danger and their potential harm is a bit
1500880	1506240	like, you know, discussing the sex of angels at the moment, or to be a little more
1506640	1512960	accurate, perhaps, it would be kind of like discussing how we're going to make transatlantic
1512960	1520640	flight at near the speed of sound safe when we haven't yet invented the turbojet in 1925.
1521200	1528400	Yeah. Yeah. Like, you know, we can speculate, but you know, how do we, how did we make turbojet
1528400	1534640	safe? It required decades of really careful engineering to make them incredibly reliable.
1534640	1541120	And, you know, now we can, you know, run like halfway around the world with the two-engine
1543360	1550960	turbojet aircraft. I mean, that's an incredible feat. And it's not like people were discussing
1550960	1555200	sort of philosophical questions about how you make turbojet safe. It's just really careful and
1555200	1564080	complicated engineering that no one, none of us would understand. So, you know,
1564320	1571040	how could we ask the AI community now to explain how AI systems are going to be safe? We haven't
1571040	1579040	invented them yet. No. Okay. That said, I have some idea about how we can design them so that
1579040	1584800	they have these capabilities. And as a consequence, how they will be safe, I call this objective
1584800	1595360	driven AI. So what that means is essentially systems that produce their answer by planning
1595360	1601120	their answer so as to satisfy an objective or a set of objectives. So this is very different
1601120	1605760	from current LLNs. Current LLNs produce one word after the other or one token, which is,
1605760	1610800	which has a board unit, doesn't matter, right? They don't really think and plan ahead as we,
1610800	1614480	as we said before. They just produce one word after the other. That's not controllable.
1615840	1620640	The only thing we can do is see if what they've produced, like check if what they've produced
1621520	1627200	satisfies some criterion or set of criteria and then not produce an answer or produce a
1627200	1634640	non-answer if the answer that was produced isn't appropriate. But we can't really force them to
1635360	1642800	produce an answer that satisfies a set of objectives. So objective driven AI is the other way,
1642800	1649920	is the opposite. The only thing that the system can produce are answers that satisfy a certain
1649920	1655520	number of objectives. So what objective would be? Did you answer the question? Another objective could
1655520	1660800	be, is your answer understandable by a 13 year old because you're talking to a 13 year old?
1661440	1668720	Another would be, is this, I don't know, terrorist propaganda or something? You know,
1668720	1673360	you can have a number of criteria like this, guardrails that would guarantee that the answer
1673360	1679680	that's produced is satisfy certain criteria, whatever they are. Okay. Same for a robot,
1679680	1683520	you could guarantee that the sequence of actions that is produced will not hurt anyone.
1684080	1687600	Like you can have very low level, you know, guardrails of this type that say,
1687920	1693440	okay, you have, you know, humans nearby and you're cooking, so you have a big knife in your hand,
1693440	1699440	don't flare your arms. Okay, that would be a very simple guardrails to impose. And you can imagine
1699440	1703520	having a whole bunch of guardrails like this that will guarantee that the behavior of those systems
1704160	1712880	would be safe and that their primary goal would be to be basically subservient to us, right? So I
1712880	1720320	do not believe that we'll have AI systems that can work that will not be subservient to us,
1721040	1724640	will define their own goals, they will define their own sub goals, but those sub goals would
1724640	1730960	be sub goals or goals that we set them and will not have all kinds of guardrails that will
1730960	1735600	guarantee the safety. And we're not going to, it's not like we're going to invent a system
1735600	1740080	and make a gigantic one that we know will have human level AI and just turning on and then from
1740640	1743440	the next minute is going to take over the world. That's completely preposterous.
1743920	1748960	What we're going to do is try with small ones, you know, maybe as smart as a mouse or something,
1748960	1753200	maybe a dog, maybe a cat, maybe a dog, maybe and work our way up and then, you know,
1753200	1759120	put some more guardrails. Basically, like we've engineered, you know, more and more powerful
1759120	1767360	and more reliable turbojets. It's an engineering problem. Yeah, yeah. You were also on a paper,
1767360	1772640	maybe this is the one that talked about the embodied Turing test on neuro AI.
1774800	1785440	Can you explain what the neuro AI is? Okay. Well, it's the idea that we should get some
1785440	1793280	inspiration from neuroscience to build AI systems and that there is something to be learned from
1793280	1802320	neuroscience and from cognitive science to drive the design of AI systems, some inspiration.
1802960	1808880	Okay. Something to be learned as well as the other way around. So what's interesting right now is
1808880	1815840	that the best models that we have of how, for example, the visual cortex works is convolutional
1815840	1820800	neural networks, which are also the models that we use to recognize images primarily
1820800	1826800	in artificial systems. So there is kind of information kind of being exchanged both ways.
1827520	1835760	There's one, you know, one way to make progress in AI is to kind of ignore nature and just,
1835760	1842240	you know, kind of try to solve problems in a sort of engineering fashion, if you want.
1843840	1850560	I found interaction with neuroscience always thought provoking. So you don't want to be
1850640	1854960	copying nature very too closely because there are details in nature that are irrelevant.
1855840	1860720	And there are principles on which, you know, natural intelligence is based that we haven't
1860720	1866400	discovered. So, but there is some inspiration to have certainly in your convolutional net for
1866400	1871600	inspired by the architecture of the visual cortex. The whole idea of neural net and deep learning
1871600	1877200	came out of the idea that, you know, intelligence can emerge from a large collection of simple
1877200	1882080	elements that are connected with each other and change the nature of their interactions.
1882080	1888800	That's the whole idea, right? So, so inspiration from neuroscience certainly has been extremely
1889680	1894800	beneficial so far. And the idea of neural AI is that you should keep going. You don't want to go
1894800	1902240	too far. So going too far, for example, is trying to reproduce the some aspect of the functioning
1902240	1909760	of neurons with electronics. I'm not sure that's a good idea. I'm skeptical about this, for example.
1911840	1917760	So your research right now, are you, your main focus is on
1919120	1925440	furthering the JEPA architecture into other modalities or where are you headed?
1926400	1934000	Yeah. So, I mean, the long term goal is, you know, to get machines to be as intelligent and learn
1934000	1939520	as efficiently as animals and humans. Okay. And the reason for this is that we need this because
1939520	1945760	we need to amplify human intelligence. And so intelligence is the most needed commodity that
1945760	1953120	we want in the world, right? And so we could, you know, possibly bring a new renaissance to humanity
1953120	1958480	if we could amplify human intelligence using machines, which we are doing already with computers,
1958480	1963200	right? I mean, that's pretty much what they've been designed to do. But even more, you know,
1963200	1972880	imagine a future where every one of us has an intelligent assistant with us at all times.
1973520	1977600	They can be smarter than us. You shouldn't feel threatened by that. We should feel
1978560	1985840	like we are like, you know, a director of a big lab or a CEO of a company that has a staff working
1985840	1990240	for them of people who are smarter than themselves. I mean, we're used to this already. I'm used to
1990240	1995360	this certainly working with people who are smarter than me. So we shouldn't feel threatened by this,
1995360	2003520	but it's going to empower a lot of us, right, and humanity as a whole. So I think that's a good
2003600	2007920	thing. That's the overall practical goal, if you want, right? Then there's a scientific
2007920	2011840	question that's behind this, which is really what is intelligence and how you build it.
2012960	2018240	And then which is, you know, how can system learn the way animals and humans seem to be
2018240	2025440	learning so efficiently? And the next thing is, how do we learn how the world works by observation,
2025440	2032080	by watching the world go by through vision and all the other senses? And animals can do this
2032080	2039360	without language, right? So it has nothing to do with language, has to do with learning from sensory
2039360	2044960	perceives and learning mostly without acting, because any action you take can kill you. So
2044960	2049200	it's better to be able to learn as much as you can without actually acting at all, just observing,
2050000	2054400	which is what babies do in the first few months of life. They can't hardly do anything, right? So
2054400	2059600	they mostly observe and learn how the world works by observation. So what kind of learning takes
2059680	2064880	place there? So that's obviously kind of self-supervised, right? It's learning by prediction. That's an
2064880	2072080	whole idea from cognitive science. And the thing is, you know, we can learn to predict videos,
2072080	2076720	but then we notice that predicting videos, predicting pixels in video, is so initially
2076720	2083360	complicated that it doesn't work. And so then came this idea of JEPA, right? Learn representations
2083360	2088080	so that you can make predictions in representation space. And that turned out to work really well
2088080	2093600	for learning image features. And now we're working on getting this to work for video. And
2093600	2100000	eventually, we'll be able to use this to learn to learn world models, where you show a piece of
2100000	2104960	video, and then you say, I'm going to take this action, predict what's going to happen next in the
2104960	2113520	world. And, you know, which is a bit where the Gaia system from Wave is doing at a high level,
2113520	2117440	but we need this at sort of various levels of abstraction, so that we can build,
2119200	2124640	you know, systems that are more general than autonomous driving. Okay. That's the...
2124640	2139360	Yeah. And it's my fault, so I won't go over the hour. But is it conceivable that someday there'll be
2140240	2153280	a model that you may be embodied in a robot that is ingesting video from its environment
2153280	2160880	and learning as it's just continuously learning and getting smarter and smarter and smarter?
2161600	2169280	Yeah. I mean, that's kind of a bit of a necessity. The reason being that, you know,
2169280	2172560	even if you train a system to have a world model that can predict what's going to happen next,
2173440	2177840	the world is really complicated. And there's probably all kinds of situations that you,
2177840	2183280	you know, the system hasn't been trained on and need to, you know, fine tune itself as it goes.
2184160	2193680	So, you know, animals and humans do this early in life by playing. So play is a way of
2194720	2200000	learning your world model in situations that basically you won't hurt you.
2201920	2207520	And, but then during life, of course, you know, when we don't drive, there's all kinds of these
2207520	2213040	mistakes that we do initially that we don't do after having some experience. And that's because
2213040	2219280	we're fine tuning our world model to some extent. We're learning a new task. We're basically just
2219280	2225680	learning a new version of our world model. Right. So, so yeah, I mean, this type of continuous,
2225680	2230960	continual learning is going to have to be present. But the overall power and the
2230960	2235520	intelligence of the system will be limited by, you know, how much a co-governor on that is using
2235520	2239280	and various other constraints, you know, computational constraints, basically.
2239600	2246000	And, you know, you're still young. And, and this not sure about that.
2246640	2250400	Well, you're younger than Jeff. Let me put it that way.
2255040	2262080	But this, the progress you've made on world models is, is fairly rapid from my point of
2262160	2271360	view, watching it. Are you, are you hopeful that within your career, you'll have
2272720	2279040	embodied robots that are, are building world models through their interaction in reality,
2279040	2283120	and, and then being able to, well, I guess the other question on world models,
2284080	2293440	do you then combine it with a language model to do reasoning or, or is the world model able to,
2293440	2298640	to do reasoning on its own? But are you hopeful that in your career, you'll, you'll get to the
2298640	2305120	point where you'll have this continuous learning in a world model? Yeah, I sure hope so. I might have
2305120	2311440	another, you know, 10, 10 useful years or something like this in research before my brain, you know,
2311440	2318560	turns into dish and male sauce, but, or something like that, you know, 15 years if I'm lucky.
2320000	2326160	So, or perhaps less. But yeah, I hope that there's going to be breakthroughs in that direction
2326160	2333600	during that time. Now, whether that will result in the kind of artifact that you're describing,
2333600	2337760	you know, robots that can, like, you know, domestic robots, for example, or,
2337760	2341520	or sort of in cars that are, they can run fairly quickly by themselves.
2342880	2349840	I don't know, because there might be all kinds of obstacles that we have not envisaged that may
2350480	2357440	appear on the way. You know, that's, it's a constant in the history of AI that you have some new idea
2358160	2361440	and a breakthrough, and you think that's going to solve all the world's problems.
2362240	2367200	And then you're going to hit limitation, and you have to go beyond that limitation. So it's like,
2367280	2371760	you know, you're climbing a mountain, you find a way to climb the mountain that you're seeing.
2372560	2378480	And you know that once you get to the top, you will have the problem solved because now it's,
2378480	2383760	you know, the gentle slope down. And once you get to the top, you realize that there is
2383760	2390080	another mountain behind it that you hadn't seen. So that's, that's, that's been the history of AI,
2390080	2395760	right, where people have come up with sort of new concepts, new ideas, new way to approach
2397680	2404640	AI reasoning, whatever, perception, and then realize that their idea basically was very limited.
2406400	2414960	And so, so, you know, this, inevitably, we're trying to figure out what's the next
2416160	2420960	revolution in AI. That's what I'm trying to figure out. So, you know, learning how the world works
2420960	2425200	from video, having systems that have world model allows systems to reason and plan.
2427520	2434960	And there's something I want to be very clear about, which is an answer to your question,
2436400	2441840	which is that you can have systems that reason and plan without manipulating language. Animals are
2441840	2449280	capable of amazing feats of planning and also to some extent reasoning. They don't have language,
2449360	2456960	at least most of them don't. And so, many of them don't have culture because they are mostly
2456960	2465520	solitary animals. So, you know, it's only the animals that have some level of culture. So,
2468000	2474480	so the idea that a system can plan and reason is not connected with the idea that you can
2474480	2479840	manipulate language. Those are two different things. It needs to be able to manipulate abstract
2480640	2485840	notions. But those notions do not necessarily correspond to linguistic entities like words
2485840	2491040	or things like that. We can have mental images if you want to things. Like you do
2492080	2496240	ask a physicist or a mathematician, you know, how they reason is very much in terms of sort of
2496240	2499920	mental models. I have nothing to do with language. Then you can turn things into
2500240	2504880	language. But that's a different story. That's the second step. So,
2508640	2512960	so, you know, we're going to have to figure out how to do this reasoning,
2512960	2519760	hierarchical planning in machines, reproduce this first. And then, of course, you know,
2519760	2524000	sticking language on top of it will help. Like, we'll make those systems smarter and be able,
2524000	2527600	you know, we will allow us to communicate with them and teach them things. And they're going to
2527600	2532560	be able to teach us things and stuff like that. But this is a different question, really.
2533120	2538560	The question of how we organize AI research going forward, which is somewhat determined by how
2538560	2545440	afraid people are of the consequences of AI. So, if you have a rather positive view of the impact
2545440	2550160	of AI on society, and you trust humanity and society and democracies to use it in good ways,
2550960	2558080	then the best way to make progress is to open research. And for the people who are
2558080	2562320	afraid of the consequences, whether they are societal or geopolitical,
2564000	2570240	they're putting pressure on governments around the world to regulate AI in ways that basically limit
2571920	2577600	access, particularly of open source code and things like that. And it's a big debate at the
2577600	2582320	moment. I'm very much on the side. So, he's met up very much on the side of open research.
2582960	2588560	Yeah, actually, that was something I was going to ask you. And now that you've brought it up.
2589440	2597120	Because there, I've been talking to people about this. And there is a view that aside from the
2597120	2604160	risks of open source, you know, again, Jeff Hinton saying, you know, would you open source
2604160	2613840	thermonuclear weapons? Aside from that is the question of as to whether open source can marshal
2613840	2625120	the resources to compete with proprietary models. And because of the tremendous resources required
2625120	2631360	for when you're scaling these models. And there's a question as to whether or not
2631360	2640160	Meta will continue to open source future versions of Lama or not continue to open source, but whether
2640160	2648160	it'll continue to invest the resources needed to push the open source models.
2649120	2653840	So what do you think about that? Okay, there's a lot to say about this. Okay, so first thing is,
2654720	2658720	there's no question that Meta will continue to invest the resources to build
2659520	2664800	better and better AI systems, because it needs it for its own products. So the resources will
2664800	2671120	be invested. Now, the next question is, do you, you know, will we continue to open source the
2671120	2677760	base models? And the answer is, you know, probably yes, because that creates an ecosystem on top of
2677760	2683120	which an entire industry can be built. And there is no point, you know, having 50 different companies
2683440	2691840	building proprietary close systems when you can have, you know, one good base open source base
2691840	2699680	model that everybody can use. It's wasteful. And it's not a good idea. And another reason for
2699680	2708160	having open source models is that it, it nobody has no entity as powerful as it thinks it is,
2708240	2714080	as a monopoly on good ideas. And so if you want people who can have good new innovative ideas
2714080	2718960	to contribute, you need an open source platform. If you want the academic world to contribute,
2718960	2722560	you need open source platforms. If you want the startup world to be able to build
2722560	2727680	customized products, you need open source base models, because they don't have the resources to
2727680	2734480	build to train large models, right? Okay, and then there is the history that shows that for,
2735040	2743760	for foundational technology, for infrastructure type technology, open source always wins,
2746640	2753440	right? It's true of the software infrastructure of the internet. In the early 90s and mid 90s,
2753440	2758560	there was a big battle between sun macro systems and Microsoft to produce the, deliver the
2759520	2763920	software infrastructure of the internet, you know, operating systems, web servers,
2765360	2770000	web browsers, and, and, you know, various servers aside and client-side frameworks, right?
2770000	2775200	They're both lost. Nobody is talking about them anymore. The entire world is,
2776960	2786560	of the web is using Linux and Apache and MySQL and JavaScript and, and, you know, and even the,
2786800	2791600	the basic core code for, for web browser is open source. So,
2793440	2800320	open source won by a huge margin. Why? Because it's safer, gathers more people to contribute.
2800320	2803360	All the features are unnecessary. It's more reliable.
2805760	2812960	Venerabilities are fixed faster. And, and it's customizable. So anybody can customize Linux
2812960	2820400	to run on whatever hardware they want, right? So open source wins. And the same, same for AI.
2820400	2825280	It's going to be the same thing. It's inevitable. The, the people now who are climbing up,
2826320	2835200	like open AI, their, their system is based on publications from all of us. Sure. And from
2836160	2838800	open platforms like, like PyTorch. Yeah.
2838800	2843280	Judgeability is built using PyTorch. PyTorch was produced originally by Meta. Now it's owned by
2843280	2849520	the Linux Foundation. It's open source. They've contributed to it, by the way. You know, their
2849520	2856160	LLM is based on transformer architectures invented at Google. Yeah. All the tricks to kind of train
2856160	2861280	all those things came out of like various papers from all kinds of different institutions,
2861280	2867760	including academia, all the fine-tuning techniques, same. So nobody works in a vacuum.
2867760	2872560	The thing is, nobody can keep their advance and their advantage
2874160	2880320	for very long if they are secretive. Yeah. Except that with these models, because they're
2880320	2887200	so compute intensive and they cost so much money to train, you need somebody like Meta that who's,
2887200	2893440	who's going to be willing to build them and open source them. And that's why I was, when I was
2893440	2903520	asking whether they'll continue, obviously Meta will continue building, you know, resource-intensive
2903520	2910000	models. But the question is whether they'll continue to open source. I mean, if- I'm telling you,
2910000	2917040	I'm telling you the only reason why Meta could stop open sourcing models are legal.
2918000	2923520	So if there is a law that adds laws, open source AI systems above a certain level of
2924800	2933200	sophistication, then of course we can do it. If there are laws that in the US or across the world
2935520	2942400	makes it illegal to use public content to train AI systems, then it's the end of AI
2942480	2948560	for everybody, not just for the open source. Okay. So, or at least the end of the type of AI
2948560	2952400	that we are talking about today might have, you know, new AI in the future, but that don't
2952400	2961360	require as much data. So, and then there is, you know, liability. If you, if you, if you, if you
2961360	2968960	believe in the kind of that someone doing something bad with an AI system that was open sourced by
2969680	2975920	by Meta, then Meta is liable, then Meta will have a big incentive not to release it, obviously.
2976720	2981840	So it's the entire question about this is around legal reasons and political decisions.
2981840	2988320	But on the idea of open source winning, don't you need more people or more companies like Meta
2988320	2994400	building the foundation models and open sourcing them? Or could it be, could an open source
2994960	3001280	ecosystem win based on a single company building the models? No, I mean, you need two or three.
3002000	3006960	And there are two or three, right? I mean, there is this hugging face. There is Mistral in France,
3006960	3013040	who's also embracing sort of an open source LLM. They're very good LLM. It's a small one, but it's
3013040	3021680	very good. There is, you know, academic efforts like Lyon. They don't have all the resources they
3021680	3026640	need, but they, you know, they collect the data that is used by everyone. So everybody can contribute.
3026640	3031440	One thing that I think is really important to understand also is that there is a future in
3031440	3037520	which I described earlier in which every one of us, every one of our interactions with the digital
3037520	3044000	world will be mediated by an AI assistant. And this is going to be for true for everyone around
3044000	3049280	the world, right? Everyone who has any kind of smart device. Eventually it's going to be in our,
3049280	3053440	you know, augmented reality glasses, but, you know, for the time being in our smartphones, right?
3055200	3062240	And so imagine that future where, you know, you are, I don't know, from
3064400	3073840	Indonesia or Senegal or France. And your entire digital diet is done through the
3074560	3081840	mediation of an AI system. Your government is not going to be happy about it. Your government
3082640	3087440	is going to want the local culture to be present in that system. It doesn't want that system to be
3087440	3096880	closed sourced and controlled by a company on the west coast of the US. So just for reasons of
3097200	3105360	preserving the diversity of culture across the world and not having or entire information
3105360	3109840	diet being biased by whatever it is that some company on the west coast of the US states,
3111360	3117360	there's going to need to be open source platforms. And they're going to be predominant
3118240	3124400	in at least outside the US for that reason. Including China, right? There is all those
3124400	3129120	talks about, oh, what if China puts their hands on our open source code? I mean, China wants control
3129120	3135760	over its own LLM because they don't want their citizen to, you know, have access to certain type
3135760	3140480	of information. So they're not going to use our LLMs. They're going to trend theirs that they already
3140480	3146640	have. And nobody is, you know, particularly ahead of anybody else by more than about a year.
3147520	3155120	Yeah. And China is pushing open source. I mean, they're very pro open source within their
3155120	3160960	ecosystems. Some of them, you know, it's there's no like unified opinion there. But
3163040	3166880	I mean, it's the same in in the West, right? There are some some governments that are too
3166880	3173520	afraid of the risks. And then or are thinking about it and some others that are all for open
3173520	3179440	source because they see this as the only way for them to have any influence on the
3182160	3188240	type of information and culture that would be mediated by those systems. So it's going to have
3188240	3197120	to be like Wikipedia, right? Wikipedia, you know, is built by millions of people who contribute to
3197120	3201280	or from all around the world in all kinds of languages. Okay. And it has a system for sort
3201360	3206720	of vetting the information. The way AI systems of the future will be taught and we'll be fine
3206720	3213360	tuned will have to be the same way will have to be quite sourced. Because something that matters to
3214720	3221600	a farmer in southern India is probably not going to be taken into account by the fine
3221600	3227360	tuning done by, you know, some some company on the west coast of the US. AI might be the most
3227360	3233440	important new computer technology ever. It's storming every industry and literally billions
3233440	3239920	of dollars are being invested. So buckle up. The problem is that AI needs a lot of speed and
3239920	3247280	processing power. So how do you compete without cost spiraling out of control? It's time to upgrade
3247280	3256080	to the next generation of the cloud oracle cloud infrastructure or OCI. OCI is a single platform
3256080	3263920	for your infrastructure, database, application, development and AI needs. OCI has four to eight
3263920	3270960	times the bandwidth of other clouds, offers one consistent price instead of variable regional
3270960	3278240	pricing. And of course, nobody does data better than oracle. So now you can train your AI models
3278240	3284960	at twice the speed and less than half the cost of other clouds. If you want to do more and spend
3284960	3294640	less like Uber, eight by eight and Databricks Mosaic, take a free test drive of OCI at oracle.com
3295200	3307040	slash I on AI. That's E Y E O N A I all run together oracle.com slash I on AI. That's it for this
3307040	3313920	episode. I want to thank Yen for his time. If you want to read a transcript of this conversation,
3313920	3323520	you can find one on our website I on AI. That's E Y E hyphen O N dot AI. And remember the singularity
3324320	3336480	may not be near, but AI is changing your world. So best pay attention.
