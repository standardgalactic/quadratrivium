WEBVTT

00:00.000 --> 00:04.320
There isn't a science around that isn't profoundly influenced by the availability of

00:04.320 --> 00:08.960
massive computing power and just greater regular computing power. It's the story of our age. It's

00:08.960 --> 00:15.680
not just the story of AI. The idea is to leverage computation to make useful things and understand

00:15.680 --> 00:20.320
the mind. These all these things need a lot of computation. It's the fact that computation has

00:20.320 --> 00:27.040
become cheaper exponentially for on the order of 100 years and can be expected to continue going

00:27.520 --> 00:33.680
It looks like doubling every two years now every 18 months and that keeps happening 18 months after

00:33.680 --> 00:39.600
18 months after 18 months and it means you double and you double and things get qualitatively different

00:39.600 --> 00:44.880
every decade and that's happened for a long time for many decades and will happen more so in the

00:44.880 --> 00:48.880
future. So we have that to look forward to. I think it's what we really should mean when we

00:48.880 --> 00:54.000
say the singularity. The singularity is that we have this exploding it's a slow explosion of

00:54.000 --> 00:59.440
computer power and that that is fundamentally changing things. Hi I'm Craig Smith and this is

00:59.440 --> 01:06.560
I on AI. In this episode I speak with Richard Sutton the father of reinforcement learning

01:06.560 --> 01:12.320
and professor at the University of Alberta. We discussed his cooperation with John Carmack

01:12.320 --> 01:19.520
on Keen a startup that vows to reach artificial general intelligence by 2030. Richard also talked

01:19.520 --> 01:25.920
about the Alberta plan his ambitious five-year research agenda focused on building embodied

01:25.920 --> 01:32.560
agents with the capability to learn and plan through interactions with their environment.

01:32.560 --> 01:38.640
Sutton provides insights into the current state of progress new algorithmic developments

01:38.640 --> 01:45.200
and trade-offs between simulated and physical environments in training and the ultimate goal

01:45.200 --> 01:52.640
of creating AGI. I hope you find the conversation as amazing as I did. So why don't you start by

01:52.640 --> 01:58.640
introducing yourself. I assume people know who you are I've had you on the podcast before

02:00.000 --> 02:08.960
but for those new listeners tell us who you are where you are and then we'll talk about the Alberta

02:08.960 --> 02:18.800
plan which I find pretty exciting. Thank you Craig I'm Richard Sutton I'm a scientist I've been

02:18.800 --> 02:26.480
studying artificial intelligence for like 45 years a long time and I'm up in north of the

02:26.480 --> 02:33.280
University of Alberta in Canada and I'm a professor in the computer science computing science department

02:33.280 --> 02:41.040
and also I'm a researcher at Keen Technologies and I got lots of titles and sub-rolls but basically

02:41.040 --> 02:46.560
I'm just trying to figure out how the mind works and I've tried to do it in a very broad and

02:46.560 --> 02:52.880
interdisciplinary way reading all the different thinkers on the subject and addressed from the

02:52.880 --> 03:00.640
point of view of psychology and how the brain might work as well as. Yeah I've read a number of the

03:00.640 --> 03:08.240
recent papers and I can see this thread developing and I don't know whether it's just that you're

03:08.240 --> 03:15.040
writing more and so the thoughts are are more developed in print or whether they're developing

03:15.040 --> 03:23.200
in your mind but from 2019 when you wrote the bitter lesson you talked about the idea that

03:23.200 --> 03:32.000
it's really increasing computation and the striving a lot of things a lot of progress

03:32.720 --> 03:43.600
that kind of coincided with open AI's scaling of the transformer model I talked to Ilya

03:43.600 --> 03:53.280
Sutskover and I asked him whether your essay had triggered their their interest in scaling and he

03:53.280 --> 04:01.600
said no it was coincidental but I kept first can we talk about that about how scale scaling and

04:01.600 --> 04:10.480
and the availability of computational resources and Moore's law has driven a lot of what's happened

04:10.560 --> 04:18.560
in artificial intelligence research almost more than novel algorithms well I think the

04:18.560 --> 04:23.920
first thing to be aware of is it's it's been driving things that are not just artificial

04:23.920 --> 04:30.240
intelligence it's been driving all the sciences and all the engineering developments in the world

04:31.520 --> 04:36.000
there isn't a science around that isn't profoundly influenced by the availability

04:36.720 --> 04:41.440
availability of massive computing power and just greater regular computing power

04:42.480 --> 04:48.480
it's it's it's the story of our age it's not just the story of AI it's not particularly the story

04:48.480 --> 04:58.080
of AI AI has always known that it needs computation the idea is to leverage computation to make useful

04:58.080 --> 05:06.720
things and understand the mind um yeah now it's true that those of us who are interested in

05:06.720 --> 05:13.200
connection to systems or distributed networks nowadays just call neural networks not particularly

05:13.200 --> 05:18.880
good terms so I always shudder a little bit when I use it but those of us that have been doing that

05:18.880 --> 05:24.320
have have those are doing learning I think that learning is important for intelligence

05:25.040 --> 05:30.960
these all these things need a lot of computation and so they're they are limited by the computation

05:30.960 --> 05:37.680
available at the time okay so let's let's be what is this thing what is the so Moore's law what's

05:37.680 --> 05:44.320
called Moore's law it's the fact computation is becoming more plentiful and cheaper exponentially

05:45.040 --> 05:51.280
for on the order of a hundred years and can be expected to continue going that way so

05:51.360 --> 05:56.880
exponentially looks like doubling every two years now each every 18 months and that keeps

05:56.880 --> 06:02.560
happening 18 months after 18 months after 18 months and it means you double and you double

06:02.560 --> 06:10.560
and things get qualitatively different uh every decade and that's happened for a long time for

06:10.560 --> 06:16.080
many decades and will happen it's more so in the future so we have that to look forward to that will

06:16.080 --> 06:25.600
continue having a tremendous influence on everything that's done on the other hand it's just normal

06:25.600 --> 06:32.880
it's just what you would expect and that those of who worked on AI for for a long time have just

06:32.880 --> 06:41.680
you know expect and plan for and um now it's coming but it's an exponential so exponentials

06:41.680 --> 06:47.440
are self-similar so that means they look the same at every point in time every every year it's

06:47.440 --> 06:52.800
you're doubling in a year and a half and so it's it's an explosion as every exponential is an

06:52.800 --> 06:58.560
explosion it's it's sort of I think it's what we really should mean when we say the singularity

06:58.560 --> 07:05.760
the singularity is that we have this exploding it's a slow explosion of computer power and that

07:06.160 --> 07:12.240
has fundamentally changed things yeah and I had a really interesting conversation almost a year

07:12.240 --> 07:20.400
ago with Aidan Gomez who was on the team that that that designed the transformer algorithm at

07:20.400 --> 07:29.120
Google and he now has a startup co-coher he's Canadian and he said an interesting thing that

07:29.840 --> 07:36.160
that he believes it could have been almost any algorithm it didn't have to be the transformer

07:36.800 --> 07:43.760
that the community got behind the transformer poured resources into it continued to scale it

07:44.560 --> 07:50.240
and it was scalable I mean that was important that it that it's a scalable architecture but

07:51.440 --> 07:57.200
that but it didn't have to be the transformer and and that made me think of you because

07:58.000 --> 08:07.840
of so transformers they the way he described it at its core it's a stock of multi-layer

08:07.840 --> 08:15.680
perceptrons with attention you scale it feed it data and it does learns to understand language

08:15.680 --> 08:22.240
or at least seems to understand language but it's got all these obvious limitations

08:23.200 --> 08:30.240
of I've been talking a lot over the last couple of years to Yamakun about world models and that

08:30.240 --> 08:41.200
to me sounded like a much more exciting direction for general intelligence because not all intelligence

08:41.200 --> 08:51.520
is is contained in language or at least most or even less so in human text and then I see you guys

08:51.600 --> 09:00.640
come along with the Alberta plan and that that that sounded even more exciting to me so

09:02.480 --> 09:09.600
how how do you so the Alberta plan you're building the ideas to build an

09:10.640 --> 09:18.480
agent ultimately an embodied agent that that has a world model or can

09:18.720 --> 09:26.720
and create a world model through interactions with its environment how is that different from

09:27.600 --> 09:38.640
Lucune's approach at a very basic level very basic level a good is that they're a very similar

09:38.640 --> 09:44.320
idea it's uh you look at the parts of his architecture and the parts of the architecture

09:44.320 --> 09:50.800
put forth in the Alberta plan they line up one one for one yeah we're trying to do the same thing

09:53.280 --> 09:57.440
we're going about it slightly different and we could talk about that but I think

09:58.080 --> 10:02.160
to just to focus on the differences might even be to distract from the big message the big

10:02.160 --> 10:08.400
message is that you have to have a goal and you have to have a model of the world and

10:09.360 --> 10:18.160
and then everything is driven by using that model to take action and to plan action at various levels

10:18.160 --> 10:27.600
of abstraction in order to to achieve the goal okay so to me this is really what intelligence is

10:27.600 --> 10:32.880
understand the world use your understanding to get to achieve to achieve your your goals

10:33.520 --> 10:38.320
I'd like to formulate the goals as as a reward and I'm super comfortable with that other people

10:39.200 --> 10:42.400
sort of grudgingly accept rewards even though it seems kind of low level

10:43.120 --> 10:48.880
but it's a it's a natural approach I think I think it's something that almost makes more

10:48.880 --> 10:53.520
sense to people who aren't steep and deep learning and to supervise learning and one thing I found

10:53.520 --> 11:00.000
interesting in in the roadmap that you've laid out for the Alberta plan you start with supervised

11:00.000 --> 11:07.680
learning and why is that is it just because it's it's easy yeah I guess we do in a sense

11:07.680 --> 11:13.120
because we want to focus on well continual learning learning continually which is sort of

11:13.120 --> 11:18.800
an obvious thing almost what learning means it has something that goes on at all times but

11:20.000 --> 11:28.800
the first steps getting continual learning with nonlinear networks is still challenging

11:28.800 --> 11:35.360
even for supervised learning and so it's natural to start at the simplest possible case which involves

11:35.360 --> 11:43.280
the fewest other factors and that's a supervised learning case yeah yeah it's funny let me just

11:43.280 --> 11:48.880
say a few words about that because there's sort of been a fight through a struggle throughout the

11:48.880 --> 11:55.280
decades between supervised learning and reinforcement learning you know there's only so much oxygen

11:55.280 --> 12:01.120
for learning methods and all the attention that's paid to supervised learning somewhat

12:01.120 --> 12:06.080
detracts from reinforcement learning so there's a there's a there's a bit of a friendly competition

12:08.160 --> 12:13.200
and supervised learning has always won the competition because supervised learning is so

12:13.200 --> 12:18.960
much more easy to put into practice and for people to use and it's sort of it's sort of

12:18.960 --> 12:23.600
less ambitious but it's really important and really those of us who do reinforcement learning

12:23.600 --> 12:30.000
or try to make whole agent architectures we are consumers of supervised learning outcomes we will

12:30.000 --> 12:35.120
use them as components of our overall architecture so we need them and we can work on them and we

12:35.120 --> 12:43.440
need to structure them for our purposes I saw one of your talks you make a distinction between

12:44.400 --> 12:53.840
AI tools and AI agents and supervised learning falls into the tool category can you sort of

12:53.840 --> 13:00.320
start and and talk about the evolution of the Alberta plan and then present to listeners

13:00.960 --> 13:08.480
what it is in in its simplest form and that'll that'll give me a structure on which to hang

13:08.560 --> 13:16.800
questions the Alberta plan is an attempt to understand intelligence as a as a primarily

13:16.800 --> 13:24.480
a learning phenomenon assists us something that comes to understand its environment and and then

13:26.000 --> 13:32.640
drives the environment to achieve goals so the first step in the Alberta plan is the structure

13:32.640 --> 13:37.280
between the agent the environment and their interaction form the interaction there's the

13:37.280 --> 13:43.200
you're not exchanging states you're exchanging observations like sensors sensors visual touch

13:43.200 --> 13:51.760
auditory it's all abstract to those particulars but it's got to be genuine observations and not

13:51.760 --> 13:59.520
state because state we don't we don't really have access to directly so that you know the

13:59.520 --> 14:04.160
principles number one principle I'm trying to remember them as I speak but number one principle

14:04.160 --> 14:12.080
is this this agent environment interaction is sacrosanct and number two is that learning or

14:12.080 --> 14:18.400
everything is is we could say continual I think we call it we say temporally uniform

14:19.520 --> 14:24.560
temporally symmetric in in the Alberta plan which means that there are no special phases

14:24.560 --> 14:31.280
where you like training and test there's just life goes on and on you get rewards or you don't get

14:32.240 --> 14:38.960
or you don't get the reward you want and you get your observations and there there is no teacher

14:38.960 --> 14:50.160
other than rewards pains and pleasures and maybe I'm not getting the four principles right but

14:50.160 --> 14:56.000
another important point is that you are going to be forming a model and so you're going to plan

14:56.640 --> 15:03.120
both trial and error learning directly from experience and learning a model and then planning

15:03.120 --> 15:09.440
with the model both these are important part of intelligence okay so those are that's the

15:09.440 --> 15:15.440
background then we outline there are 12 steps and the 12 steps really start with let's have

15:15.440 --> 15:21.520
learning that is temporally uniform let's have metal learning and metal learning maybe I should

15:21.520 --> 15:28.240
stop and on that for a moment metal learning means learning to learn not just learning one function

15:28.240 --> 15:32.800
but once you are continually learning you're learning this and you're learning that you get

15:32.800 --> 15:38.080
many many experiences learning and you can get better at learning you can use those repeated

15:38.080 --> 15:46.160
experience with repeatedly learning to make make future learning episodes more efficient so as part

15:46.240 --> 15:53.440
of that you learn representations you learn features you learn step sizes

15:55.680 --> 16:00.800
okay so continual learning and then all the algorithms and once once we add

16:00.800 --> 16:05.840
metal learning and continual learning we have to in supervised learning then we extend that to

16:07.520 --> 16:12.720
reinforcement learning which involves its own set of issues to get more interesting temporal

16:12.720 --> 16:22.080
relationships and I think like the first six steps are crafting the basic algorithms of reinforcement

16:22.080 --> 16:30.080
working through them again to be continual and meta and then we start to bring in the the challenging

16:30.080 --> 16:37.040
issues like learning off policy and learning models of the world and then planning and the

16:37.840 --> 16:40.320
just to jump to the end the last step is about

16:44.320 --> 16:49.520
AI, AI, AI's, AI intelligence augmentation

16:51.760 --> 16:59.520
where we combine computers, AI's with our own minds to make make our own minds stronger

17:00.960 --> 17:06.640
okay now one of the key steps in there was off policy learning and learning a model of the world

17:07.840 --> 17:13.440
off policy learning means you want to be able to learn about things that you're not doing or you're

17:13.440 --> 17:20.640
not because you're not doing all the way to completion so even like to recognize an object

17:20.640 --> 17:27.760
you look at the object and you say how would you you have to define that in some objective way

17:28.400 --> 17:33.280
and the best way to just do that is as a sub problem so

17:35.520 --> 17:43.440
yeah maybe maybe I'll just sort of stop there the most interesting strategy

17:45.280 --> 17:50.160
distinctive strategy by the Alberta plan is the pose is that the mind works by posing sub

17:50.160 --> 17:55.200
problems for itself and then working on them and it's it's not it's sure it's got a main

17:55.280 --> 18:00.480
problem which is to get reward but it's also has many thousands of sub problems it's also

18:00.480 --> 18:05.840
working on simultaneously and since it's not behaving it cannot behave for all thousand

18:05.840 --> 18:10.080
problems at once it has to pick one problem like perhaps the main problem and behave according to

18:10.080 --> 18:14.880
that so all the other things have to be able to learn from data that's not exactly on what they

18:14.880 --> 18:22.720
would do and this is called off policy learning and it's a key to learning to achieve auxiliary

18:22.720 --> 18:28.720
sub problems and also it's a key to efficiently learning a model of the world yeah you you have

18:29.760 --> 18:37.360
something called the horde architecture is is that where that comes in when you you break

18:37.360 --> 18:45.360
a problem down into multiple sub tasks that that you learn I was one one paper where we

18:46.480 --> 18:49.920
we worked on that idea we developed that idea the horde is the horde of sub problems

18:50.560 --> 18:58.240
each each demon in the horde which is it could be almost viewed like a single neuron in a neural

18:58.240 --> 19:04.640
network as achieving working towards a different task trying to predict a different thing or maybe

19:04.640 --> 19:11.840
trying to attain a different thing it's the view of the of the mind as decentralized there is one

19:11.840 --> 19:17.520
goal and everything is ultimately driven towards one goal but still it's a useful structure to

19:17.840 --> 19:25.920
to have different parts driving towards towards other goals how did you get together with john

19:25.920 --> 19:34.000
cormack was that primarily because you need the funding and it gives you a vehicle to raise

19:34.000 --> 19:40.560
capital oh seriously I mean you you know Jan Lacoon's got meta behind him well it's just not it's

19:40.560 --> 19:47.200
not really comparable uh john's uh john's company is great but it's still like a 20 million dollar

19:47.200 --> 19:55.840
company and uh which is which is plenty of money for what we want to do now um john and I got together

19:55.840 --> 20:02.800
because we had similar ideas about what was needed um and and also what was not needed

20:03.520 --> 20:13.200
um to get to ai or agi um yeah so I read an art newspaper article an interview that john did

20:14.320 --> 20:17.920
down in texas and uh I just could see that he was thinking about the way

20:18.640 --> 20:21.440
thinking about things the way I was even though our backgrounds were quite different

20:22.000 --> 20:27.280
you thought of intelligence you had to there's a few principles that needed to be worked out

20:27.280 --> 20:32.320
rather than so this isn't a huge program to write it's a few principles we have to figure those out

20:33.040 --> 20:39.840
um not that many maybe uh maybe 10 000 lines instead of 10 million lines of code

20:40.720 --> 20:48.240
so it's easy to get it's relatively it's still it's still it's still hard to get basic research

20:48.240 --> 20:54.400
funding in the world it's easy to get funding towards uh applications of ai large language models

20:54.400 --> 21:01.760
particularly um anyway I'm really enjoying working at keen and being able to focus on the ideas

21:02.560 --> 21:11.200
and uh it's a it's a it's a it's a calm company we um um

21:12.960 --> 21:17.760
there's a lot of thinking involved a lot of contemplation a lot there is also experiments

21:17.760 --> 21:23.200
and we're trying to get um the engineering side of it is really important uh but for me it's

21:23.200 --> 21:28.720
been really great just to be able to regroup my thoughts and think about them very carefully and

21:29.280 --> 21:35.920
push them forward but keen is is implementing the alberta plan is that right I mean that's

21:35.920 --> 21:41.520
that's uh the project well the alberta plan is a research plan it's like a five-year research plan

21:42.080 --> 21:48.880
and so research is something you don't implement research is something you conduct and and it

21:48.880 --> 21:56.480
doesn't always end up the way you want but um yeah I wouldn't say implement is the right word

21:56.480 --> 22:03.360
not yet but but the the work you're doing at keen is is informed by the alberta

22:04.080 --> 22:10.240
yeah I'm absolutely I'm working on the alberta plan uh uh and and the end goal at keen is to

22:10.240 --> 22:19.600
create the uh the embodied intelligence described by the alberta plan you don't sound very yeah

22:19.600 --> 22:25.440
very confident well a plan is just a plan and you know I think there's a good chance that it

22:25.440 --> 22:31.680
will work out as planned but you know a five-year plan you make another one after four or three years

22:33.760 --> 22:41.520
yeah so I wouldn't I wouldn't uh presume to to know how it's gonna work out but at the same time

22:41.520 --> 22:48.800
we have to make you know we have to make our bets we have to think hard about it um just knowing um

22:49.760 --> 22:56.800
you know we we may well be right but you know you your work is primarily in reinforcement

22:56.800 --> 23:02.400
learning you're you wrote the book on reinforcement learning temporal difference learning and

23:03.920 --> 23:12.640
uh lambda and all of that is is this I mean this is this seems a much more ambitious

23:13.600 --> 23:23.760
uh project is this was it the the success of the transformer scaling that that said well you know

23:24.560 --> 23:31.280
let's do that with rl let's why why are these guys uh uh you know everyone's celebrating what

23:31.280 --> 23:36.960
they're doing but but there's much more to be done no no what what you're seeing the alberta plan

23:36.960 --> 23:43.120
is is perhaps bigger than the book but this has always been the plan we've always in AI

23:43.120 --> 23:49.520
tried to understand all of the mind and reproduce it in computers and so that's an that's that is a

23:49.520 --> 23:58.240
big enormous ambition that's what it's always been so the large language models are a bit

24:00.080 --> 24:04.160
a bit disappointing in some sense I mean it's really good that people are getting excited

24:04.240 --> 24:12.080
and people are wanting to learn about it but um but it's not it's I don't envision that it's the

24:12.720 --> 24:22.240
direction um that will be most uh productive to pursue now you know who knows what I do know

24:22.240 --> 24:27.920
is it's not the most direction that's useful for me to pursue um I I'm much more interested in

24:27.920 --> 24:33.360
actions and goals and how an agent can tell what's true and what's not true all of those

24:33.360 --> 24:42.480
things are missing from large language models so uh um no I'm not they're not really what what are

24:42.480 --> 24:48.720
they what they are doing that's important is they're showing uh what you can do with uh computation

24:48.720 --> 24:57.520
and and networks um and learning that you can get enormously complex um things and you can

24:57.600 --> 25:03.440
incorporate a lot of data it just shows the power for those who needed to be shown that

25:05.360 --> 25:15.280
and and it could be uh an interface between humans and and whatever you end up creating

25:15.280 --> 25:23.840
the agents you end up creating you still need a language interface to communicate yeah but I don't

25:24.800 --> 25:29.840
I'm I doubt that what we're doing with large language models today will contribute to that

25:29.840 --> 25:35.520
oh is that right yeah I mean in other words the models that you want to build the agents you want

25:35.520 --> 25:42.400
to build would learn language uh as as part of the learning process yeah so it's like we say

25:43.120 --> 25:48.560
language language last you know language not not language first with large language models

25:48.560 --> 25:54.320
are language first we just say large language last just as Jan McCoon says we need to do you

25:54.320 --> 25:59.280
know rat level intelligence and then cat level intelligence and we have to get those figured

25:59.280 --> 26:05.440
out before we should try to make human level intelligence uh so where are you on the plan I

26:05.440 --> 26:14.480
mean you you figured out reinforcement learning you can build agents uh you there are various

26:14.480 --> 26:22.240
architectures for creating representations from from various kinds of sensory input

26:24.160 --> 26:32.800
and and at that representation level then you can plan efficiently so where in all of that

26:33.920 --> 26:38.720
are are you in your research well it's a little hard to explain non-technically

26:38.800 --> 26:45.360
but you can say some things certainly you can say that the various steps

26:46.960 --> 26:52.480
are not done entirely sequentially you you're always looking for areas of opportunity where

26:52.480 --> 26:58.400
you can make an increment of progress and those could be you know on step 10 or they could be on

26:58.400 --> 27:05.280
step three but you also I could also try to be very rough and say that we're we're at about step

27:05.360 --> 27:12.480
four now we are still doing things where we're changing the basic underlying

27:13.200 --> 27:20.640
fundamental reinforcement learning algorithms we are not done with that we need more efficient

27:20.640 --> 27:28.000
algorithms and I'm excited about some of the changes new ideas we're developing recently

27:28.000 --> 27:33.760
about how that can be done can you talk about those new ideas at all okay well one of the big

27:33.760 --> 27:40.960
things is efficient off-policy learning and the use of important sampling important sampling is

27:40.960 --> 27:45.760
where you see how likely you're to do things under your target and your behavior policies

27:45.760 --> 27:53.920
and you adjust the returns based on those the ratios of those two and for a long time I thought

27:53.920 --> 28:01.760
that was the only way to adjust the returns but now the forward correction of the returns I think

28:01.760 --> 28:12.800
can be done by by changing your expectations so like if you're expecting a good thing to happen

28:12.800 --> 28:18.320
you're expecting a good action to be taken and then a different action was taken a more exploratory

28:18.320 --> 28:26.320
action so this is a deviation from your target policy which would be more greedy and one way to

28:26.320 --> 28:30.800
take into account the deviation from the target policy is to just say oh okay now I've done something

28:31.440 --> 28:35.360
not best so I'm just going to adjust my level now you're going to expect a little a little less

28:36.080 --> 28:38.960
and there's a way there's a systematic way of doing that

28:41.280 --> 28:47.120
that's gives us a new way to handle the off-policiness of of our returns

28:47.840 --> 28:53.520
and so this gives a whole new family of algorithms so that's exciting now

28:54.320 --> 29:02.240
exciting maybe mostly for me I think maybe the most accessible direction of of of excitement

29:02.800 --> 29:08.080
of novelty is in continual right so there's I'm going to say a bunch of things and to me

29:08.080 --> 29:13.360
they're all going to have the same solution continual learning meta learning representation

29:13.360 --> 29:19.760
learning learning to learn learning how to generalize state how to construct a state

29:19.760 --> 29:27.040
representation feature finding that whole thing is is is coming and it will be a kind of

29:28.400 --> 29:33.600
it's just a new kind of a way a new kind of way of doing the learning in deep networks

29:35.680 --> 29:42.080
and I call it dynamic learning nets see a dynamic learning nets have learning at three levels

29:42.080 --> 29:47.360
whereas usually our neural networks only learn at one level they learn the level of the weights

29:48.000 --> 29:53.120
and in addition we also want to learn at the level of step sizes so all of every place you

29:53.120 --> 29:58.000
have a weight in your network you're also going to have a step size so a step size is sometimes

29:58.000 --> 30:01.440
called a learning rate it's much better to call the step size because the learning rate will be

30:01.440 --> 30:07.600
influenced by many other things so if we imagine a whole network all these weights next to each

30:07.600 --> 30:14.240
weight is a step size that is adjusted by an adaptive process that's adapted in a meta learning way

30:14.240 --> 30:20.000
a meta gradient way towards making the system learn better rather than just perform better

30:20.000 --> 30:25.760
an instantaneous moment in time learning rates or step sizes don't affect the function they don't

30:25.760 --> 30:29.120
affect some function implemented in a particular point in time they don't affect with the network

30:29.120 --> 30:34.800
does they affect what the network learns and so if you can tune the step sizes you also get

30:34.800 --> 30:41.840
learning to learn and learning to generalize well and things like that the last three the last

30:41.840 --> 30:48.800
element that we wanted to have be adaptive weights step sizes the third one is the connection pattern

30:49.600 --> 30:56.240
so who's connected to who and so this will be done by an accretive process

30:58.480 --> 31:04.160
like let's say you start with a linear unit and it learns say a value function or a policy

31:04.160 --> 31:10.160
and it does the best it can with the features available and and then it needs to induce the

31:10.160 --> 31:15.280
creation of new features because you need to learn a nonlinear function of your original

31:15.280 --> 31:22.480
signals and so you need to create new features that have become available to that linear unit

31:22.480 --> 31:28.720
and in this way you grow in a sort of organic way a system that can learn nonlinear functions

31:30.960 --> 31:38.400
and so this is just a different way of ending up with a deep network that was all learned

31:38.400 --> 31:44.560
including all the features dynamic learning that's where is the data the input data coming from

31:44.560 --> 31:49.680
well the the input data and reinforcement just comes from life from doing things seeing things

31:49.680 --> 31:54.080
right there is no labeled data set yeah maybe I should have said this from the very beginning

31:54.080 --> 32:01.360
the whole idea of I call it experiential AI is that you know what makes you data you're you you

32:01.360 --> 32:07.920
grow up as a baby and you play with things and you see things and you do things and that's the

32:07.920 --> 32:13.680
data and the trick of reinforcement learning is how do you turn that kind of data into something

32:13.680 --> 32:19.280
you can learn from and grow a mind up from so the the beauty and the limitation of supervised learning

32:19.280 --> 32:24.400
is they say well let's not worry about that for now let's assume that somehow we have a data set

32:24.400 --> 32:29.920
with labeled things and let's let's work on this sub problem that's a great idea work on a sub problem

32:29.920 --> 32:35.760
figure it out and then move on to the next thing but really we have to move on to the next thing

32:35.760 --> 32:41.360
we have to worry about how the the data set quote data set is automatically created from

32:41.360 --> 32:47.120
the the training information there isn't ever a data set data set is is is such a misleading term

32:47.120 --> 32:52.080
it suggests that it's easy to to have this thing and store this thing and curate this thing

32:52.080 --> 32:58.000
really life is full of you do things things happen and then there's one you know everything is

32:58.000 --> 33:04.880
fleeting you you don't have a record of it and it would be enormously complex and not only valuable

33:04.880 --> 33:10.880
to have a record of it the the the feeling is totally different in reinforcement learning and

33:10.880 --> 33:16.880
supervised learning and in particularly the way the way I would adjust it you know many people

33:17.680 --> 33:23.120
do reinforcement learning by creating a buffer or a record of all the experiences that have been

33:23.120 --> 33:30.880
been retained that have been occurred at least for some period of time and I think that's that's

33:31.520 --> 33:37.840
uh an appealing but but it's it's not where the action the answer is the answer is

33:39.040 --> 33:46.000
embracing the fleeting nature of data and and making most when it happens and then letting it go

33:46.560 --> 33:52.560
well that's why you want to make an embodied system so that you have all the the five senses or

33:53.360 --> 33:59.600
or more so you need you need as you say an embodied system an interactive system that that

33:59.920 --> 34:07.360
influences its its input stream its sensory stream and that you get that interaction and for a long

34:07.360 --> 34:14.960
creative time you can do this in simulation or you can do it in robotics there's still I still

34:14.960 --> 34:20.320
know what's the best way or if the best ways do both and right or maybe first one and then the other

34:21.120 --> 34:30.800
John is interested in uh having um uh learning from video and he likes his his his view of the

34:30.800 --> 34:36.880
experience is you have massive numbers of video streams like you're viewing you know 500 channels

34:36.880 --> 34:44.400
of television and then you can switch switch to look at one look at another one um uh other people

34:44.400 --> 34:52.320
in in in keen my close colleague Joseph Modial he's uh interested in robotics and he thinks the

34:52.320 --> 34:58.800
best way to get an appropriate in data stream is to actually build robotic hardware um

35:00.800 --> 35:05.520
you know it's important that the world be large and complex because the worlds we want to address

35:05.520 --> 35:12.640
are large and complex um and so you want things like video and you want large data streams um

35:14.400 --> 35:21.760
now you can use simulations to generate even video streams simulated video but inevitably

35:21.760 --> 35:27.680
those simulated worlds are really quite simple they have an underlying simplicity uh they have

35:27.680 --> 35:33.920
objects perhaps and three-dimensional straight structure maybe they're rigid objects and the

35:33.920 --> 35:40.240
vision is is is a very particular geometric form um they they are generated and they are

35:40.240 --> 35:44.880
they are made up worlds and they are generated so they're they're really the worlds are are are

35:44.880 --> 35:50.400
less complex than the agent uh their goal would be to have to spend most of the computer power

35:50.400 --> 35:56.880
working on the mind and just a little bit to just create the simulated data and and that's that's

35:56.880 --> 36:06.000
reversed the way it really is right every person is maybe has a has a complex brain but their world

36:06.000 --> 36:11.840
is much more complex not just because the world consists of all these um physics and matter

36:11.840 --> 36:16.400
but it also consists of other minds other brains and other minds out there and and what goes on

36:16.400 --> 36:20.880
in their minds matters and so the world is inherently vastly more complex than the agent

36:22.160 --> 36:26.640
and we we've reversed that when we work on simulated worlds so which is always concerning

36:27.600 --> 36:32.880
anyway those are some of the issues in the trade-offs between working with simulations or with

36:33.600 --> 36:38.480
physical worlds nonetheless you you need to develop the architecture and the algorithms

36:39.440 --> 36:46.080
before you worry about the data data stream i would think yeah but you want to develop the

36:46.080 --> 36:51.360
right algorithms and if you're working with the world it's not representative of of your target

36:51.360 --> 36:58.880
world in an important way um it can be misleading but you're right and that's what we that's what

36:58.880 --> 37:03.120
we strive to do you know i don't know if you know but i think of my own work is almost always

37:03.680 --> 37:08.480
i want to focus on some issues so i make a really simple instance of that issue like you know a

37:08.480 --> 37:16.320
five-state world and and i study the the hell out of it but i don't like try to take advantage of

37:16.320 --> 37:22.320
its smallness you know i study algorithms that are in some sense even simpler than the simple world

37:22.320 --> 37:27.520
and i i stress those algorithms and see what their abilities are so we always you know it's always

37:27.520 --> 37:33.520
part of research is we we simplify the world understand it fully just like a a physicist might

37:33.520 --> 37:38.240
you know make a simplified world with with a ball rolling down a ramp and it's it's a really

37:38.240 --> 37:44.480
simple world and you'll try to eliminate the friction and you eliminate other weird effects

37:44.480 --> 37:50.880
and just see things in their simplest form yeah have you um paid much attention to um

37:51.840 --> 37:58.160
alex kendall's work at at wave ai do you know that company it's an autonomous driving company

37:58.160 --> 38:07.440
they have a world model called gaya one um and it's it's it's similar to what yanlacoon's doing

38:07.440 --> 38:17.520
it it you know encodes representations from from video from live video and then uh plans

38:18.480 --> 38:29.440
uh based on those representations uh and and it can control a car uh from the representation space

38:29.440 --> 38:37.600
it's actually pretty remarkable so let's talk about the world model and and what what kind of

38:37.600 --> 38:46.560
world model would be appropriate for autonomous driving um so let me say some things that are

38:47.920 --> 38:53.920
mistakes they're a natural seeming but mistakes in my opinion uh the mistake would be to make

38:53.920 --> 39:00.480
like a physics model of the world or to try to make something that could simulate the world and

39:00.480 --> 39:06.480
produce the video frames you don't you don't you don't want the video frames of the future

39:06.480 --> 39:13.760
that's not the way you think um instead you think oh i could i could go to the market and

39:13.760 --> 39:20.560
maybe there would be strawberries okay you're not creating a visual uh a video you're saying

39:20.560 --> 39:27.120
you're like jumping to the market and then your strawberries could be you know different sizes

39:27.120 --> 39:34.080
and positions and and and still uh there's not a video there's an idea that will happen if you go

39:34.080 --> 39:46.160
to the market um so uh people have realized this like yon lakun used to talk about um generating

39:46.160 --> 39:53.840
video of the future and then you realize it would be blurry and and now he realizes that you need

39:53.840 --> 39:59.360
to produce outcomes of your model that are not like not at all like video streams and not like

39:59.360 --> 40:07.680
observations at all they're like um they're like constructed states um that are the outcome of the

40:07.680 --> 40:15.520
action okay so this is this is a very different from from a partial differential equation model

40:15.520 --> 40:22.400
of the world and it's so it's very different from what self-driving car companies start with

40:22.400 --> 40:28.000
self-driving car companies start with physics and geometry and uh you know things that are

40:28.000 --> 40:33.440
calibrated by human understanding engineers understanding of the world and driving but

40:34.480 --> 40:39.840
i suspect that's going to be i mean what do i know i'm not into self-driving i don't do

40:39.840 --> 40:48.240
self-driving cars but i know that that um like tesla is and elon musk is and um so their goal

40:48.240 --> 40:53.120
is to is to make some you know they they started like everyone else with engineering models but i

40:53.760 --> 40:59.520
my understanding now is that they're building uh sort of more conceptual models um that are

40:59.520 --> 41:05.600
based on the artificial neural networks okay and so rather than starting with geometry and

41:05.600 --> 41:10.480
understood things they're just getting massive amounts of data and training it to make a model

41:10.480 --> 41:16.080
we need a model that is at the level of high level consequences not at the level of low level

41:16.080 --> 41:21.120
things like pixels and video so one way you do that is you're having state features that are

41:21.120 --> 41:27.440
at a more advanced level you say oh this is a car uh rather than this is a uh a video frame

41:29.280 --> 41:38.480
and um so and then basically it's as simple as you need abstraction in both state and time

41:39.120 --> 41:44.880
abstraction in in state is like saying there will be strawberries when i get to the market

41:44.880 --> 41:51.040
and abstraction in in time is saying oh i can go to the market and then in 20 minutes i will

41:51.040 --> 41:58.640
be there probably and other things will be the same or related in natural ways

42:01.440 --> 42:05.360
so we want to be able to think about i could go to the market you also want to think oh i could

42:05.360 --> 42:10.240
pick up the coke can i could move a finger and that will have certain consequences these these

42:10.240 --> 42:14.960
all these things that we know you think uh are vastly different scales going to the market is like

42:14.960 --> 42:22.720
20 minutes um you know taking taking a new job you know might be a year uh deciding to study a

42:22.720 --> 42:28.800
topic also might be a period of time we think and we analyze the consequences like you wanted to

42:28.800 --> 42:35.360
meet with me today and you know we arranged it we set it up it was your your planning uh took

42:35.360 --> 42:42.160
you know place over weeks and some cases months and and and we assembled the the the event of

42:42.160 --> 42:48.640
this interview by by planning all that and exchanging mess high-level messages uh it all that

42:48.640 --> 42:53.680
you know it's silly to think that that's done at the level of of of imagining videos that we might

42:53.680 --> 43:00.640
see with our eyes or our audio is signals that we might hear yeah so we need models that are

43:00.640 --> 43:09.200
abstract in time and state and um as a reinforcement learning person um there's a particular set of

43:09.200 --> 43:16.640
technologies that i naturally turn towards to do that um the prediction is based on multi-step

43:16.640 --> 43:25.520
prediction by temporal difference learning um the planning is done by uh dynamic programming

43:25.520 --> 43:32.480
essentially value iteration but where the steps the are not low-level actions but they're called

43:32.480 --> 43:36.960
options they're high-level ways of behaving with that that terminate so they're there are things

43:36.960 --> 43:42.960
like going to the market and they'll terminate when you're at the market so you know at a certain

43:42.960 --> 43:52.400
conceptual level it's clear where we want to go to me um with abstract models in time and state

43:53.200 --> 43:54.640
built options and features

43:58.240 --> 44:05.920
i don't know you we did write one paper recently put published an AI journal on the the notion of

44:05.920 --> 44:16.000
planning using uh sub-problems on the stomp progression stomp means sub-task option model

44:16.000 --> 44:21.200
and planning put all those things together and you can do the full progression from from the

44:21.200 --> 44:28.560
data stream to abstract planning and that's that's what we're trying to put together yeah yeah and i

44:28.560 --> 44:35.680
i sort of misspoke talking about gaya one about that model i mean it they they the input is video

44:37.200 --> 44:45.600
it creates a representation and it plans and and and takes action in the representation

44:46.640 --> 44:54.480
plans actions in the representation space you can then decode that into video to see what

44:55.200 --> 45:02.480
what it's doing but but it's but you're not planning in the video space so the what what's

45:02.480 --> 45:09.440
your ambition with with this you'll figure out the refine the algorithms the reinforcement learning

45:09.440 --> 45:19.360
algorithms they need to be scalable once you have that uh then you move on and and uh start start

45:19.360 --> 45:27.680
scaling them with compute and and uh you know following your roadmap or am i simplifying it too

45:27.680 --> 45:32.720
much you know we want to understand how the mind works and then we're going to make a mind or some

45:32.720 --> 45:42.400
minds or some mind uh amount of mind uh and this will be useful in all the ways in all sorts of

45:42.400 --> 45:50.960
ways economically useful it'll also be useful um to to us to extend the capabilities of our own

45:50.960 --> 45:58.560
minds if we can understand how our minds work um we can we can augment them so that they can work

45:58.560 --> 46:06.880
better um yeah we're gonna the the key step is understanding and then there would be millions

46:06.880 --> 46:16.240
of uses um i don't think it's going to be as simple as making uh workers sort of like slaves for us

46:16.960 --> 46:23.760
to direct i don't think it'll be as simple as that um that maybe gives a lower bound on

46:23.760 --> 46:31.440
potential utility our sort of our story for etkin is we say that um well if you suppose you

46:31.440 --> 46:38.080
could make a virtual worker um this would be enormously useful um much of the work that we

46:38.080 --> 46:43.680
all do from day to day is doesn't require a physical presence it doesn't require a robot

46:43.680 --> 46:50.480
much of which we do is just shuffling information around we can do most things through through a

46:50.480 --> 46:59.840
video interface um so why can't we make workers that are extremely useful by playing the roles that

47:00.080 --> 47:05.440
people play in many cases that's that's that's sort of a lower bound and what can be done

47:05.440 --> 47:08.800
i think much more can be done and there'll be much more interesting things to be done

47:12.000 --> 47:18.960
and then this question of what should be done um yeah those are those are rich

47:18.960 --> 47:25.760
philosophical questions and practical questions for the economy yeah uh the the i've seen your

47:25.760 --> 47:30.800
third uh well and one thing on reinforcement learning and sort of supervised learning sort

47:30.800 --> 47:38.240
of took over for a while now it's transformer based generative uh ai but uh during the supervised

47:38.240 --> 47:47.920
learning phase uh the argument was that uh higher knowledge is all supervised learning

47:47.920 --> 47:54.880
and and the it's still supervised it's still supervised in general the ai had large language

47:54.880 --> 48:02.000
models they the the training information is the next token the next word and that's taken as

48:02.960 --> 48:10.880
as the correct action the analogy you gave me was uh you know because the analogy that that's

48:10.880 --> 48:17.840
always given is that you know a child sees an elephant the mother says that's an elephant

48:18.400 --> 48:25.440
and the child very quickly can generalize and and recognize other elements elephants maybe it

48:25.440 --> 48:32.000
makes a mistake and the mother corrects it and says no that's a cow and and that was always given

48:32.000 --> 48:36.800
as an example of supervised learning but maybe it's reinforcement learning maybe it's the child's

48:37.520 --> 48:46.000
reward from the mother praising him for remembering the label the point is that a child has

48:48.400 --> 48:56.480
well-developed concepts classes concepts um before and then and then when it's you know when

48:56.480 --> 49:03.040
its mother says that is an elephant uh there's already an extensive understanding on the child's

49:03.680 --> 49:11.520
and a part of you know what the space is what the objects are and and this this the the thing

49:12.080 --> 49:19.520
that that is being labeled um no the label is the least interesting part of that and the the

49:19.520 --> 49:26.400
the child has already learned all the all all the other most interesting parts of of what it means

49:26.960 --> 49:31.680
to have animals and moving things and objects in its world the label is the least interesting

49:31.680 --> 49:37.760
part well first of all you're talking about agents that that could be virtual workers already

49:38.320 --> 49:48.160
uh using reinforcement learning people are building agents and using large language models

49:48.160 --> 50:00.720
and knowledge bases to you know carry out tasks knowledge based tasks uh so what you're talking

50:00.720 --> 50:08.400
about is is more than uh linguistic tasks or knowledge based tasks you're talking about

50:09.200 --> 50:18.080
of physical planning and physical tasks is that right the key thing is having goals and a lot

50:19.280 --> 50:25.200
if you have an for example an assistant help you plan your day organize your day or do tasks for

50:25.200 --> 50:33.280
you um i'm thinking it's very important that the system is able to have goals and is able to

50:33.280 --> 50:40.240
understand your goals i think it's probably the most important part of an assistant is to understand

50:40.240 --> 50:47.680
the purposes involved and um large language models don't understand don't really understand

50:47.680 --> 50:55.920
purposes involved they will appear to a little bit um but the corner case has always come up and

50:55.920 --> 51:02.240
once you spend a bit of time they're always and you're always in a corner case and so an AI system

51:02.240 --> 51:08.640
is system that that after a bit does silly things and that don't respect the goals that you have or

51:08.640 --> 51:14.560
that have been given to it um that's not going to be a useful assistant so i mean i don't want to be

51:14.560 --> 51:21.200
critical of large language models um they're very very useful but it shouldn't be viewed as a criticism

51:21.200 --> 51:26.560
to say that they're also at the same time have rather important limitations it's not a competition

51:26.560 --> 51:36.000
in that sense are you concerned at all are you ascribed to the threat debate no i think the

51:36.560 --> 51:44.400
i don't i don't uh i i think the doomers are they're not just wrong i think i think they're

51:44.400 --> 51:51.280
blindingly biased the the bias is blinding them to what's going on basically AI is a broadly

51:51.280 --> 51:56.880
applicable technology it's not like it's not like nuclear weapons it's not like it's not like a

51:56.880 --> 52:04.640
bio weapons it can be used for all kinds of things and it's not it's not uh it's uh the way we deal

52:04.640 --> 52:12.000
with such things is we we uh we try to use them well and there will be people that use them

52:12.720 --> 52:17.280
for bad things and then you know this is just normal there's normal technology

52:17.280 --> 52:23.120
is it can be used by good people or bad people the the doomers the doomers are just saying oh

52:23.120 --> 52:29.760
somehow there's going to be it's going to be it's bad in the same way that nuclear weapons are bad

52:29.760 --> 52:37.280
that they and that's just they're just blinded by that metaphor by the thinking that that the AI

52:37.280 --> 52:44.880
will be out to kill them that's just it's just silly and i i don't i don't think well they the

52:44.880 --> 52:52.560
doomers don't actually give coherent reasons for what they what they believe and so it's hard to

52:52.560 --> 52:59.600
argue with them uh so maybe it's fair just to hold that they're they're biased and blind

52:59.600 --> 53:05.600
i don't accept i don't accept an argument this is a proper argument so so where you say you're

53:05.600 --> 53:16.960
maybe at stage four in the research car max says 2030 uh that's you know it's far enough out there

53:16.960 --> 53:25.680
that maybe people won't remember in 2030 that he said 2030 uh it's always uh you know i've 2030

53:25.680 --> 53:32.720
has been out there for a long time and it's it's it's uh you can't it doesn't recede it's always

53:32.720 --> 53:44.240
been 2030 for the uh computer power reaching human scale um quantities yeah but anyway 2030 is is a

53:44.240 --> 53:50.160
reasonable it's a reasonable target for us understanding everything that we need in order

53:50.160 --> 53:56.720
to make uh a real mind yeah i'm good with that yeah as you you have to be ambitious

53:58.560 --> 54:09.200
i've always said that 2030 is a 25 chance of of of achieving a real intelligence a real human

54:09.200 --> 54:15.600
level intelligence 25 chance so probably not but it's it's a big enough chunk of probability that

54:16.160 --> 54:20.800
that an ambitious person should work towards it and try to make it true and it does depend upon

54:20.800 --> 54:27.760
what we do and not just the uh unfolding of the universe so we should we should try to do that

54:27.760 --> 54:34.080
that that is a big the big thing that's happening right now is the the the public is coming to grips

54:34.080 --> 54:39.120
with what it means for there to be for us to understand the mind and to have the ability to

54:39.120 --> 54:47.680
create uh minded things uh and so that that is a big uh transformation it's a big change in our

54:47.680 --> 54:57.280
worldview um and so we absolutely need all kinds of people to uh to help us help us become easy

54:57.280 --> 55:04.880
and become have an understanding of what's happening as we uh achieve human level

55:05.920 --> 55:11.760
designed intelligence that's it for this week's episode i want to thank richard for his time

55:11.760 --> 55:18.080
if you want to read a transcript of today's conversation you can find one on our website

55:18.160 --> 55:26.320
i on ai that's e y e hyphen o n dot ai in the meantime remember the singularity may be getting

55:26.320 --> 55:34.880
closer but ai is already changing your world so pay attention

