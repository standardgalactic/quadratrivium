WEBVTT

00:00.000 --> 00:06.000
What this whole open AI saga has shown us is that, I mean, obviously we can't have something like

00:06.000 --> 00:10.720
this being developed by just like a handful of, you know, weird people, unaccountable

00:10.720 --> 00:14.480
billionaires in the Bay Area. This is actually something that I've been telling

00:14.480 --> 00:18.080
saying to people for years now. Obviously, this is not the right governance structure.

00:18.080 --> 00:22.320
So the weirdest thing I would say about this technology should be a tool. It should not be a

00:22.320 --> 00:29.360
goal in and of itself. Hi, I wanted to jump in and give a shout out to our sponsor NetSuite

00:29.360 --> 00:37.120
by Oracle. I'm a journalist and getting a single source of truth is nearly impossible.

00:37.120 --> 00:42.960
If you're a business owner, having a single source of truth is critical to running your

00:42.960 --> 00:54.320
operations. If this is you, you should know these three numbers. 36,025,1. 36,000 because that's the

00:54.320 --> 01:00.720
number of businesses that have upgraded to NetSuite by Oracle. NetSuite is the number one

01:00.720 --> 01:08.080
cloud financial system, streamlining, accounting, financial management, inventory, HR, and more.

01:08.960 --> 01:17.040
25, because NetSuite turns 25 this year. That's 25 years of helping businesses do more with less,

01:17.040 --> 01:23.920
close their books in days, not weeks, and drive down costs. One, because your business is one

01:23.920 --> 01:31.600
of a kind. So you get a customized solution for all of your KPIs in one efficient system

01:31.600 --> 01:38.880
with one source of truth. Manage risk, get reliable, forecast, and improve margins. Everything you need

01:39.440 --> 01:47.280
all in one place. As I said, I'm not the most organized person in the world, and there's real

01:47.280 --> 01:52.000
power to having all of the information in one place to make better decisions.

01:52.960 --> 02:01.440
This is an unprecedented offer by NetSuite to make that possible. Right now, download NetSuite's

02:01.440 --> 02:09.200
popular KPI checklist designed to give you consistently excellent performance, absolutely free

02:09.360 --> 02:21.600
at netsuite.com slash I on AI. That's I on AI, E-Y-E-O-N-A-I, all run together. Go to net suite.com

02:22.320 --> 02:34.880
slash I on AI to get your own KPI checklist. Again, that's net suite.com slash I on AI, E-Y-E-O-N-A-I.

02:35.120 --> 02:39.120
They support us, so let's support them.

02:42.880 --> 02:50.400
Hi, my name is Craig Smith, and this is I on AI. In this episode, I speak again with Connor Lehi.

02:51.360 --> 02:58.080
He's the founder and CEO of a startup called Conjecture that's working on AI alignment.

02:58.960 --> 03:05.360
Before that, he was one of the founders and leaders of a group called Eleuther AI

03:06.320 --> 03:16.560
that built one of the world's first open source large language models. Connor is concerned about

03:16.560 --> 03:25.360
AI safety, about where AI development is going, concerned about the push towards

03:26.320 --> 03:32.640
artificial general intelligence, and has a lot of thoughts about what we should be doing

03:33.280 --> 03:42.240
to control development so that we don't end up creating something that is harmful to humanity.

03:42.240 --> 03:47.840
I talked to him particularly because I wanted to hear his thoughts on the open AI saga,

03:48.800 --> 03:55.520
which highlighted for a lot of people the dangers of having such a small group of people

03:56.720 --> 04:06.000
controlling such a fundamentally powerful technology today. I hope you find the conversation

04:06.000 --> 04:13.280
as interesting as I did. So I'm Connor. I'm currently the CEO of Conjecture, an AI company in

04:13.280 --> 04:18.960
London focused on AI safety and building architectures for AI systems that are understandable

04:18.960 --> 04:25.520
and controllable, and various other things. I also do a bit of work in policy regulation,

04:25.520 --> 04:32.320
public messaging, that kind of stuff. Before this, I was well known as one of the founders of Eleuther

04:32.320 --> 04:39.760
AI, which was kind of one of the first, if not the first kind of like open source large language

04:39.760 --> 04:46.240
models, research, building kind of groups. And technically, even before that, I was someone who

04:46.240 --> 04:51.200
worked on open source GBT2 as I think maybe literally the first person.

04:52.480 --> 05:00.400
Yeah. And so last time we spoke was after the release of chat GBT and GBD4, and

05:01.840 --> 05:07.520
you were very concerned as were a lot of people and a lot of people continue to be about

05:08.480 --> 05:16.320
releasing these kinds of models into the public before having fully explored

05:17.760 --> 05:26.320
the safety issues or without having adequate guidelines. But it struck me during this open AI

05:26.320 --> 05:34.960
saga that we have lived through the last week, that having this kind of powerful technology

05:34.960 --> 05:45.440
in the hands of a handful of people who have different agendas and can't get along is in itself

05:45.440 --> 05:56.640
a security issue. And I would think that having these models open source where everyone can

05:57.440 --> 06:05.040
see the data they were trained on in particular, because even Llama, I've learned, doesn't

06:05.920 --> 06:14.560
make public the training data. But having the data open for all to see and having the weights

06:15.200 --> 06:24.640
of the models open so people can improve them or play with them or whatever. That to me seems

06:24.640 --> 06:32.880
like a much safer path than having proprietary models. But I wanted to hear how your thinking is

06:32.880 --> 06:41.760
involved on that. In the 1940s and in the early 1950s, the Soviet Union built a what's called a

06:41.760 --> 06:48.960
closed city around what would be known as the Mayak facility. The Mayak facility was the largest and

06:48.960 --> 06:52.800
one of the first, well I don't know if it was literally the first but it's one of the largest

06:52.800 --> 06:57.120
nuclear facilities for the Soviet Union in their breakthrough development attempt to build the

06:57.120 --> 07:03.680
nuclear bomb. To give a bit of a flavor for what Mayak and similar facilities that existed throughout

07:03.680 --> 07:11.920
the Soviet Union were like, there was a program where if you got caught by the secret police

07:11.920 --> 07:16.080
and you were being sent to a gulag for the rest of your life, you would give them an option.

07:16.960 --> 07:21.120
Either you go to Siberia, work yourself to death for the rest of your life,

07:21.120 --> 07:25.680
or you get sent to Mayak for only three months. And if you serve your term, you're free.

07:26.560 --> 07:32.560
Sounds great, doesn't it? Well, no one survived the three months. So what happened, of course, is

07:32.560 --> 07:37.040
that one of the first things that the Americans developed while developing the bomb is the HEPA

07:37.040 --> 07:43.200
filter, which is a form of air filter that is powerful enough to be able to filter radioactive

07:43.200 --> 07:48.800
material very sufficiently out of the air, making it safe for the workers. The Soviets didn't

07:48.800 --> 07:55.440
bother developing HEPA filters. Not really. So a lot of people died. To this day,

07:56.160 --> 08:01.600
Lake Karakai, which is near the Mayak facility, is one of the most radioactive places on earth,

08:02.160 --> 08:07.520
so much so that it is said that standing next to it for an hour can kill a man. These are all

08:07.520 --> 08:11.280
just some fun facts that surely have nothing to do with the topic we're talking about today.

08:11.840 --> 08:20.400
It's a story about how some people who were pretty bad people developed something or working

08:20.400 --> 08:26.000
on something pretty dangerous, and then other people got access to it, and also really bad things

08:26.000 --> 08:33.040
out because even worse people got access to it. Now, from this, I don't conclude, oh, so we should

08:33.040 --> 08:40.320
have just had everyone develop plutonium. That would have made it safer. This is not a draw from

08:40.400 --> 08:46.720
this conclusion from the story. Now, nuclear power is obviously very different from AI in many

08:46.720 --> 08:54.480
factors. So why even bring it up? So I can ask the question in the other direction, though.

08:54.480 --> 08:59.920
We're talking about AI. We're talking about AGI. So let's focus on AGI. I'm not really interested

08:59.920 --> 09:06.080
in talking about the risks of current-day models, like chat GPT or something. We can talk about

09:06.080 --> 09:09.920
those, too. They are real. There are real risks from those, but they're not the kind where I

09:09.920 --> 09:14.800
think we have to stop all publication necessarily. But let's talk about AGI systems.

09:17.120 --> 09:24.000
What reference class does AGI fall into? Is it like open source? Is it like nuclear bombs?

09:24.960 --> 09:32.960
Is it like something different? The reference class we choose forms our thinking around

09:32.960 --> 09:37.840
something which is fundamentally none of those things. AGI is in nukes. AGI is in open source.

09:38.480 --> 09:44.400
It's not Linux. It's something very different. So while it has things in common with all of those

09:44.400 --> 09:50.880
things, AGI runs on computers. Linux runs on computers. And it has other things that come

09:50.880 --> 09:54.960
with nuclear bombs. Nuclear bombs can kill everybody. AGI can kill everybody. Those are

09:54.960 --> 09:59.040
things in common. Is it more like nukes? This is more like open source. At some point, we have

09:59.040 --> 10:04.000
to actually drop down from the metaphors and into the actual models of reality. So from my

10:04.000 --> 10:10.240
perspective, I think you're completely correct. What this whole opening AGI has shown us is that

10:11.440 --> 10:17.840
obviously, we can't have something like this being developed by just a handful of weird people,

10:17.840 --> 10:22.480
unaccountable billionaires in the Bay Area. Obviously, they're not acting in humanity's

10:22.480 --> 10:28.880
best interest to no one's surprise. A couple of months ago, Sam Altman interviewed about this,

10:28.880 --> 10:32.720
and he said, oh, yeah, the board can fire me at any time if I go about a mission. I think that's

10:32.720 --> 10:39.200
important. And then they try to fire him, and he's back. And well, that didn't work. So this is

10:39.200 --> 10:47.200
actually something that I've been telling saying to people for years now. I've gotten some disagreements.

10:47.200 --> 10:51.760
He did disagreements, let us call them, with some of the people who were involved with the creation

10:51.760 --> 10:57.040
of the board or in favor of the existence of the board. And the point I always made to them was

10:58.000 --> 11:06.560
this obviously cannot control a charismatic billionaire political mastermind. Why the

11:06.560 --> 11:12.320
hell would you think it would? This is crazy. And this is exactly what we saw play out. And

11:12.320 --> 11:16.320
I'm not even trying to make a comment on Sam Altman, good Sam Altman, bad. I'm just saying,

11:16.320 --> 11:21.680
obviously, he was just not going to just say, oh gosh darn, I guess the board said no more AGI for

11:21.680 --> 11:27.520
me, I guess I'm going to stop. That's not how men like him work. And that's obviously not what was

11:27.520 --> 11:33.840
going to happen. And you think the politically unsavvy nerds could write a document that would

11:33.840 --> 11:38.080
convince someone like him to stop? No, of course not. So obviously, this is not the right governance

11:38.080 --> 11:43.760
structure. I fully agree with this. But there's a great saying, which is that reverse stupidity is

11:43.760 --> 11:48.800
not intelligence. If you take something stupid, and you take the opposite of it, it's probably also

11:48.800 --> 11:55.760
stupid. And so the fact that this governance structure doesn't work for me does not say that

11:55.760 --> 11:59.600
therefore there should be no governance structure. This to me does not follow.

12:00.880 --> 12:07.200
Yeah, but open source, I mean, you are I'm sure very familiar with Yanlacun's argument that,

12:08.000 --> 12:19.920
yeah, that open sourcing can can lead to some abuse by bad actors, but by and large,

12:21.040 --> 12:30.960
the vast majority of people that will be working on an open source model contributing to it or

12:30.960 --> 12:41.440
building products off of it will be doing so with, you know, not without nefarious intent.

12:42.000 --> 12:49.600
And that the larger the open source community, the quicker it would be able to respond to

12:50.560 --> 13:02.560
to bad actors or or misuse or or the more people available to to build guardrails and spot

13:02.960 --> 13:11.280
of weaknesses and that sort of thing. So that that argument makes a lot of sense to me. I mean,

13:11.280 --> 13:20.240
at the beginning, when the, you know, pause letter came out. And around the time that we talked about,

13:20.240 --> 13:26.560
yeah, this stuff is too dangerous to be open source. But,

13:29.920 --> 13:36.880
but I'm changing my mind. And I wanted to hear whether the this this episode has changed your

13:36.880 --> 13:44.720
mind at all. I'll make three points in reaction to that. The first one is a story.

13:44.720 --> 13:49.840
The second one is a heuristic. And the third one is a true observation from my own life.

13:50.560 --> 13:58.000
So first, the story, the story is that the smallpox virus genome is currently online.

13:58.560 --> 14:03.040
You can go download it. It's a small text file. You can just go download it to your computer.

14:04.000 --> 14:09.280
Another fact of the story is that a couple of years ago, Canadian scientists

14:10.240 --> 14:16.480
recreated an extinct version of smallpox called horsepox. They revived it. They may and it was

14:16.480 --> 14:22.720
functional and viable and infectious. And they published how to do it. Do you think either of

14:22.720 --> 14:28.000
those things are good? Now you can argue, well, if we have more eyes on the smallpox virus,

14:28.000 --> 14:32.080
then something, something, you know, good things happen. But this isn't really a model.

14:32.800 --> 14:36.640
So this brings me to the second point. The second point is offense versus defense.

14:37.600 --> 14:43.120
The way technology is work is that some favorite offense, some favorite defense,

14:43.120 --> 14:50.960
very few are symmetric. Most of the time, and most of the time offense wins. It is usually

14:50.960 --> 14:56.320
easier to destroy than it is to protect. There are exceptions to this rule, for example,

14:56.320 --> 15:01.040
cryptography is an interesting exception where defense is easier than offense.

15:02.240 --> 15:07.120
But in most cases, it is easier to build a bomb than it is to build a reactor,

15:07.680 --> 15:15.280
you know, a safe controlled burn. So all things being equal, you should expect that if you have

15:15.280 --> 15:20.480
a technology and you distribute equally, that there will be more destruction. This is the default

15:20.480 --> 15:25.440
case. This is what you should expect by default. Most technologies that are destroyed don't immediately

15:25.440 --> 15:29.840
give you a way to defend against it. Developing vaccines is harder than developing bioweapons.

15:30.480 --> 15:33.840
It is much easier to crank out a bunch of bioweapons and then you have to develop

15:33.840 --> 15:37.600
vaccines in response to that, which is already super hard because, you know,

15:37.600 --> 15:42.800
who knows how far the virus already is. So just because the technology is why the aspect does

15:42.800 --> 15:48.320
not mean it defends wins. Whether offense or defense wins is a property of reality. It is

15:48.320 --> 15:54.640
not a property of your morals or of your ideology. And the third point is an observation from my

15:54.640 --> 15:59.280
own life is that I used to work in open source. I was one of the very first people to work on it.

15:59.280 --> 16:07.040
And I had similar views to Licken and, you know, assuming he holds these views genuinely,

16:07.760 --> 16:12.960
which, you know, I hope he does. I don't know him very well. I talked to him maybe once.

16:14.160 --> 16:21.280
And I think this is just not even wrong. It's just in my experience, what happens when you

16:21.280 --> 16:25.520
build AI models and you release them open source is that the first thing that happens to get

16:25.520 --> 16:30.560
uploaded to Huggingface, and then a guy called the bloke, that's literally his name, uncensored

16:30.560 --> 16:36.160
them, undoes any RLHF training or other security training run have done, trains them all the newest

16:36.160 --> 16:42.080
data to make them more powerful, more general, more whatever, uploads them again, 4chan downloads it,

16:42.080 --> 16:46.160
down, you know, uses them for whatever their applications are, whether it's, you know,

16:46.160 --> 16:53.200
pornography mostly, or likes BAM or whatever, et cetera. And now maybe this is fine, right?

16:53.200 --> 16:58.400
Like, you know, you know, maybe we say, oh, it's okay. If people want to use their LLMs for porn,

16:58.400 --> 17:05.040
so what? That's okay. Sure. What I'm saying is, is the empirical observation is that the amount

17:05.040 --> 17:11.040
of effort that gets put into making these things safer or more controllable is absolutely pathetic

17:11.680 --> 17:15.280
compared to the amount of effort that the open source community puts into making these things

17:15.280 --> 17:21.280
more powerful, more general, and less controllable. This is just an empirical fact. This is just

17:21.280 --> 17:27.760
actually, if you go online, you pick the top 1000 LLM repos, how many of them are about controlling

17:27.760 --> 17:32.880
the models better versus making them faster, making them more efficient, distilling them,

17:32.880 --> 17:37.840
making them more, et cetera. And the fact is that the offense, like the unbalance here is like,

17:37.840 --> 17:42.240
it's not even funny. And I understand, right? And this is not to say that the people working on this

17:42.240 --> 17:46.720
technology are like morally evil. I think this is an important thing to understand. There's an

17:46.720 --> 17:52.000
incentive from people like Lacan and other like big tech, you know, people, like talking heads,

17:52.000 --> 17:58.080
to try to focus on it's only the evil people's fault because that absolves them of responsibility.

17:59.200 --> 18:04.800
Meta wants open source because it absolves them of responsibility as a corporation.

18:04.800 --> 18:09.120
They can't get sued because it owes the user's fault. And this is also what's happening in the

18:09.120 --> 18:15.200
EU AI act right now is that people like Lacan are lobbying to remove foundation models from

18:15.200 --> 18:21.680
regulation in the EU and saying instead of their uses should be regulated. This is the same thing

18:21.680 --> 18:26.720
as when, for example, plastic companies invented recycling. They invented it so that it was the

18:26.720 --> 18:32.000
user's fault that there is all this plastic pollution. Like, oh, see, we would have recycled

18:32.000 --> 18:38.240
it. But unfortunately, the users just didn't do it. This is a, this is gas lighting. And this is a

18:39.040 --> 18:45.760
complete unbalance of power. The externalities of plastic pollution should be on the ones who are

18:45.760 --> 18:50.960
most suited to addressing this externality, who are creating this externality. It shouldn't be on

18:50.960 --> 18:56.800
the user. And the same thing applies to foundation models is that these systems can do things. They

18:56.800 --> 19:01.360
can be used for many things. And we should be taking the big companies building these systems.

19:01.360 --> 19:04.880
Is that what these open source models are being built by like, you know, plucky little teenagers

19:04.880 --> 19:10.240
in their, in their, you know, rooms as a plucky teenager that did do that. I'm saying most of

19:10.240 --> 19:16.480
the ones being built now are being made by like the UAE and meta. Like, these aren't the little

19:16.480 --> 19:20.320
guys. These are big guys trying to shirk their responsibility to society.

19:21.600 --> 19:29.040
Well, then what, what's the lesson from, from the open AI saga that, that you just need a

19:29.040 --> 19:35.040
bigger board or you need a lesson is that none of these structures are correct. This is what we

19:35.040 --> 19:39.360
have governments for. This is the same lesson that we've had over and over again. It's like

19:40.080 --> 19:46.160
self-regulation does not work. It has never worked. Self-regulate. This is like tobacco

19:46.160 --> 19:50.800
companies self-regulating themselves. This does not work. And we as a society have developed a

19:50.800 --> 19:55.840
mechanism. I'm not saying it's a perfect mechanism by any means, but we do have a mechanism for

19:56.400 --> 20:01.360
intervening in systems that have extreme high externalities that are not self-regulatable.

20:01.360 --> 20:11.200
And it's called the government. Yeah. And, and I mean, there has been a lot of work at the government

20:11.200 --> 20:20.720
level, not as much in the US as in Europe. But, but how, how do I mean, obviously these

20:21.680 --> 20:26.960
models are so commercially, the potential is so commercially

20:28.560 --> 20:35.280
exciting that fines aren't going to matter. You're not going to be able to find people

20:36.480 --> 20:42.640
to behave in ways that the government wants them to. There's got to be something stronger than

20:42.640 --> 20:48.640
that. So, have you thought about, about that? I mean, how do you regulate these things?

20:51.520 --> 20:57.520
One of my, one of the most inspiring moments from the history, I think of science and society

20:58.160 --> 21:06.480
is many decades ago, biologists and chemists and so on realized that human cloning should be possible.

21:06.480 --> 21:10.480
Like it should be possible to do this. They were still far from having the actual technology

21:10.480 --> 21:15.440
to perform with human cloning, but they found out it should be possible. And they reasonably

21:15.440 --> 21:20.320
understood, wait, that might be really disabilizing. Like that could be, we don't know what the

21:20.320 --> 21:24.080
consequence is, but maybe it's great. You know, maybe there's, you know, there's many benefits

21:24.080 --> 21:30.160
from human cloning as well. But like, let's chill out. We don't know, like we don't know,

21:30.160 --> 21:35.280
and this seems huge. This doesn't, isn't just like another thing. This is not a 10% more effective,

21:35.280 --> 21:42.640
you know, cough drop. Like human cloning is a big deal. And so, heroically, long before the

21:42.640 --> 21:48.640
technology existed, they came together and banned it and said, let's have a moratorium.

21:48.640 --> 21:54.080
Let's not do this until we've had a bit more time to figure out what the hell we as a society

21:54.080 --> 21:59.840
want about this. And it wasn't one board. This wasn't one CEO being like, I will, you know,

22:00.480 --> 22:04.720
take a moratorium on this. No, it was the scientific community and governments coming

22:04.720 --> 22:10.880
together and working very, very hard to create a moratorium. A moratorium is what we do when we

22:10.880 --> 22:16.160
are faced with something which we know is huge and we don't know how to deal with. That's what

22:16.160 --> 22:20.880
scientists do. You have a moratorium. And we should have a moratorium on AGI. This is what we

22:20.880 --> 22:29.280
need to do. And can you enforce a moratorium? Yeah, I mean, it's like technically, like physically,

22:29.280 --> 22:34.880
like, yeah, obviously, like, that's not that hard. Whether people will do that, whether people

22:34.880 --> 22:39.680
want to do that, whether people can overcome the incredible political power that big tech has,

22:39.680 --> 22:44.240
that's the more interesting question. It's not like the government obviously has the ability,

22:44.320 --> 22:48.640
like the CIA can track every GPU in the country if it wants to. Like, you know, if you want,

22:48.640 --> 22:52.960
if the NSA wants to shut down, just press a button. Like, that's not the problem. You know,

22:52.960 --> 22:57.520
if you want to throw a couple CEOs in jail, like, sure, like the FBI can do that. Like,

22:57.520 --> 23:02.560
physically, this is not a problem. It's a political problem. This is not a physical problem. This is

23:02.560 --> 23:07.120
a political problem. The political problem is, well, if you have legislation around this kind of

23:07.120 --> 23:11.840
stuff, well, we just saw what happens if you try to fire Sam Altman, you think he's going to be okay

23:11.840 --> 23:17.600
with a huge GPUs away? Well, no, I expect that's going to be a hard fight. I expect, you know,

23:17.600 --> 23:22.320
Microsoft lobbyists will fight that tooth and nail. I expect many people will fight this.

23:22.320 --> 23:25.760
And this is why, like, you know, I'm not, I'm not here to give point to you,

23:25.760 --> 23:30.320
paint you a rosy picture of the future. I'm not optimistic that things are going to go well.

23:30.320 --> 23:36.560
We have an unprecedentedly huge political problem here. I think I'd like to say is the thing that's

23:36.560 --> 23:44.880
killing us right now, it's not AGI. AGI doesn't exist yet. It's people. It's politics that is

23:44.880 --> 23:52.880
killing us. Right, right. But and to that point that AGI doesn't exist, not so much all the other,

23:53.520 --> 24:03.280
I mean, yes, no doubt, the political systems are not equipped to deal with the big problems facing

24:03.280 --> 24:11.600
humanity. But in this case, AGI doesn't exist. I don't know how you would ban AGI, because

24:11.600 --> 24:19.360
no one really knows how and when it might emerge, if it ever does. At the level of the tech now,

24:19.360 --> 24:27.680
I mean, what are you, what are you suggesting? And I'm not putting on the spot. I don't expect you

24:27.680 --> 24:33.520
to. Oh, I have policy proposals. I have very concrete policy proposals here, here are three.

24:35.120 --> 24:40.240
The first one is a compute cap. There should be a limitation that no single training around no

24:40.240 --> 24:44.880
single AI system can be built with more than a certain amount of compute. So luckily, we are,

24:44.880 --> 24:49.920
so we are very lucky that current frontier AI systems, more and more general purpose systems

24:49.920 --> 24:54.400
require more and more computing resources. These computing resources are very easy to track.

24:54.480 --> 24:59.440
They're very bulky. They take lots of specialized knowledge, lots of energy. The kinds of supercomputers

24:59.440 --> 25:03.920
that can train a GPT-4 or GPT-5 are only built by like three companies in the world,

25:03.920 --> 25:10.960
and they're all in the US. So like, this is a solvable problem. And we should put a ban on,

25:10.960 --> 25:15.680
you know, there should be like a registration process for, you know, frontier models up to a

25:15.680 --> 25:20.800
certain limit. And beyond that, there should be just ban, just a moratorium. Just you are not

25:20.800 --> 25:26.320
allowed to perform any experiment that requires more than, I don't know, 10 to the 24, 10 to 25

25:26.320 --> 25:36.000
or whatever, FLOPs. FLOP being a unit of measurement for computing power. And this is easily enforceable.

25:36.000 --> 25:40.000
This is absolutely something that like technically is enforceable with, it's just a political

25:40.000 --> 25:45.680
problem. And this buys you time. Then you're, our scientists figure out, you spend time actually

25:45.680 --> 25:50.800
figuring out how far is AGI away, how dangerous is it, how do we control the things, blah, blah,

25:50.800 --> 25:54.480
then we can talk about those kinds of things. The first thing is to buy time. The second

25:54.480 --> 26:02.880
proposal, or unless you want to comment on that. Well, just on that, you're talking about limiting

26:03.680 --> 26:13.040
commercial products. But if, when you say then that gives the research community time to figure

26:13.040 --> 26:18.960
these out, things out, they're going to have to experiment with larger models. So there's got to

26:18.960 --> 26:30.000
be some. To be clear, these levels are insane. 10 to the 24, 10 to the 25, FLOP is an unimaginably

26:30.000 --> 26:34.080
large amount of computing power. There are no academic labs, basically, that need this for

26:34.080 --> 26:41.120
research, FD research. This is ridiculous. There is just no, so this is a common propaganda piece

26:41.120 --> 26:45.040
that the big labs like to say is like, oh, we need more compute to do safety research.

26:45.760 --> 26:52.400
Maybe this is true. I have not seen it. This is just not what has actually happened. Just purely

26:52.400 --> 27:00.880
empirically speaking, there is, I have seen basically no safety AGI relevant research that

27:00.880 --> 27:07.120
required more than like, you know, a GPT-3 that you couldn't have done with GPT-3 level of compute

27:07.120 --> 27:11.760
or less. I have like, maybe it exists, but I sure as hell as I have not seen it.

27:13.760 --> 27:22.240
Okay. So limiting compute is one proposal. What are the others you mentioned?

27:22.880 --> 27:28.000
Two others I would recommend. The second is strict liability for model developers.

27:28.800 --> 27:34.320
So what this means, so strict liability means that the intentions of the developer do not matter.

27:34.960 --> 27:40.640
It would matter is that if a harm is caused, the developer is liable. I think this should

27:40.640 --> 27:46.560
basically exist for the whole supply chain is that if you create externalities, you have to pay for

27:46.560 --> 27:52.400
them. And this aligns the incentives of everyone aligned on the chain. Currently, there are no

27:52.400 --> 27:58.240
incentives for developers to develop to minimize the externalities of their systems. Currently,

27:58.240 --> 28:03.520
you as an open source developer can be an arbitrarily dangerous thing that causes arbitrarily

28:03.520 --> 28:09.200
much damage. And you have no incentive to avoid this. As a concrete example, which not even going

28:09.200 --> 28:15.840
to AGI is voice cloning systems. There are right now in GitHub, systems you can just download,

28:15.840 --> 28:20.320
which take you 15 seconds of your voice, clone it perfectly. And you know, go call your kids,

28:20.320 --> 28:25.200
call your wife, you know, just manipulate them, call in a swat hit on your, on you using your own

28:25.200 --> 28:30.560
voice. This is all doable. And the people developing these systems have zero liability.

28:30.640 --> 28:35.920
They don't even feel bad about it. Because it's open source, Craig. If it's open source, it must

28:35.920 --> 28:41.520
be good. My ideology says so. And you know when your ideology tells you something is morally right,

28:41.520 --> 28:48.880
then it's good as we've seen throughout history. So it's, so we have to align incentives here

28:48.880 --> 28:56.640
somewhere along the line, you know, if a, it reminds me of cars and seatbelts in the 70s, where

28:57.520 --> 29:05.600
car manufacturers fought tooth and nail to not have seatbelts. They fought it viciously with

29:05.600 --> 29:10.720
propaganda and with lawsuits and with everything they could throw at it. Because they said, well,

29:10.720 --> 29:14.720
it's the driver's fault. If he gets into an accident, it's not our fault. Like, you know,

29:14.720 --> 29:20.800
we just build cars. If they drive it poorly and they die, well, it's not our fault. And we, you

29:20.800 --> 29:25.920
know, the people rightfully told them to go fuck themselves. Like, no, you have to build a safe

29:25.920 --> 29:31.440
product. You can't like, it's, it's not a moral question. It's kind of like the point I want to

29:31.440 --> 29:38.480
make. I'm not making an ideological point. I'm not saying my religion says that seatbelts are good.

29:38.480 --> 29:46.000
I'm like, I don't care. I care. Do seatbelts mean that less people die? And the answer is, yeah,

29:46.000 --> 29:52.880
like they make cars safer. So then I want seatbelts. Cool. And the same thing applies to open

29:52.880 --> 29:59.600
source. Does Linux being open source result in more safety? The truth is, yeah, looks pretty

29:59.600 --> 30:05.440
obviously like case. So I'm in favor of Linux being open source. Awesome. Great. You know, does,

30:05.440 --> 30:10.560
you know, some seven billion parameter model be open source positive or negative? I don't know,

30:10.560 --> 30:15.840
probably positive. Like probably so. I'm not sure. Like there's a lot of downsides there as well.

30:15.840 --> 30:22.880
But like, seems like it probably is positive. AGI being positive, you know, open source, you know,

30:22.880 --> 30:27.680
that does not seem positive to me at all. That does not, that seems like a recipe for disaster.

30:27.680 --> 30:31.120
So it's, I'm not trying to make an ideological point is what I'm starting to say. I'm not saying

30:31.760 --> 30:34.880
all these things are good. All these things are bad. I'm saying we have to look at things

30:34.880 --> 30:40.000
at a case by case basis. This is how proper regulation works. Proper regulation shouldn't

30:40.000 --> 30:46.240
be ideological. It shouldn't be everything is regulated as ARB. That would be terrible regulation.

30:46.880 --> 30:54.800
Yeah. Well, so that was the capping of the compute on training runs,

30:56.320 --> 31:01.280
shifting liability to the model developer. What was the third one?

31:01.840 --> 31:05.840
So the third one that I think should be done is that there should be a kill switch.

31:05.920 --> 31:10.000
And what I mean by this is it doesn't have to be literally a switch. What I mean is there should

31:10.000 --> 31:18.160
be a protocol that any developer of frontier AI systems needs to implement by which at a given

31:18.160 --> 31:26.880
notice, any frontier training runs or deployments can be shut down in under a minute. So the reason

31:26.880 --> 31:30.800
for this is not per se because I need, I think necessarily that this would be very helpful.

31:30.800 --> 31:35.280
The AGI actually happens. If AGI actually happens, this is probably useless. The reason I think this

31:35.280 --> 31:40.800
is good is because we should have the institutional capacity to do these kinds of things. There should

31:40.800 --> 31:44.720
be every six months, there should be a fire alarm. There should be a fire drill where everyone has

31:44.720 --> 31:49.520
to practice. In the next five minutes, all AI companies have to go offline for 60 seconds.

31:49.520 --> 31:53.920
If not, you get slapped with a huge fine. These are the kinds of protocols you want to have

31:54.560 --> 31:59.280
in worlds where you have tail risks, where things can blow up, where you can have these

31:59.280 --> 32:05.600
kind of things. And then there should be a multilateral K of N kind of system around this.

32:05.600 --> 32:11.760
Like maybe all major global powers have one of these buttons and if three or five of them push it

32:11.760 --> 32:18.880
or seven of 10 or whatever, then the system kicks in. This is the kind of institutional

32:18.880 --> 32:23.120
building which doesn't save us, but it's a hell of a lot better than nothing.

32:23.840 --> 32:34.880
And how do you see these kinds of proposals moving through the policy making frameworks?

32:35.680 --> 32:46.640
There is some advance in the European Union. The White House has come out with its

32:47.440 --> 32:52.640
executive order, which as yet doesn't have any real concrete

32:55.440 --> 33:02.800
government governance policy in it, but it sort of lays out the things that we should

33:02.800 --> 33:14.320
be thinking about. Where do you see these things going? What sort of a timeline do you think that

33:14.320 --> 33:21.840
governments are being educated enough that they can deal with this? What government is

33:21.840 --> 33:29.120
going to lead? Is it the EU? Will it be the US? Who should it be? And then of course you've got

33:29.920 --> 33:36.880
the other side of the world, Russia and China, who have very different agendas and may not want

33:36.880 --> 33:43.520
to regulate at all. So when people ask me questions like this and they're like, what's your probability

33:43.520 --> 33:50.640
of X happening? And then my follow-up question is usually, is it X conditioned on me and other

33:50.640 --> 33:56.880
people doing something about it or not? Because I expect if they're conditioned on me and other

33:56.880 --> 34:01.120
people don't do anything about it, then yeah, I just think nothing will happen if big tech wins

34:01.120 --> 34:07.520
and then we die. I think it will be very heroic or special. It will just be new products keep

34:07.520 --> 34:11.600
happening, AI keep going up, and then just one day, humanity's not in control anymore and we

34:11.600 --> 34:15.920
have no idea what's going on. And then it's just over. I don't think it will be dramatic. I think

34:15.920 --> 34:20.320
we will just get more and more confused. We won't understand what's going on anymore. Weirder and

34:20.320 --> 34:27.600
weirder things will happen, more and more politics, economics, markets, media is controlled by AI,

34:27.600 --> 34:32.400
or even just fully generated by AI. There will be no more movies or just AI generated. And then

34:32.400 --> 34:36.640
just humanity will not be in control anymore. And then one day we fall over dead for some reason,

34:36.880 --> 34:42.000
we don't understand. That's what I expect will happen by default. And along the way to be clear,

34:42.000 --> 34:48.080
big tech will pick a lot of money. So go buy that Microsoft stock. You'll get really rich

34:48.080 --> 34:55.920
just before you die. So if I could addition on someone actually doing something about this,

34:55.920 --> 35:00.640
I do think there is hope. I don't think there's a lot of hope, but there is hope. And the main

35:00.640 --> 35:12.320
hope I see from this is that the general public fucking hates AI. It's unfathomable how much

35:12.320 --> 35:19.200
normal people hate AI. They use it, of course, but they're freaked out by it, which is just completely

35:19.200 --> 35:25.040
the correct reaction. It's just these crazy bizarre weirdo tech people like you and me

35:25.040 --> 35:30.560
who are not instantly like, wait, that's actually, let's not do that. If you talk to any normal

35:30.560 --> 35:34.480
person, you're like, hey, these people are building systems that are smarter than humans.

35:34.480 --> 35:42.560
They don't do that. That seems really dangerous. Don't do that. Well, all the people are like,

35:42.560 --> 35:51.760
oh, but actually you see my proposal because we'll make it fine. Or actually universal love

35:51.760 --> 35:57.600
means that AI systems will love whatever. I don't even know what these people say anymore.

35:57.600 --> 36:01.200
I think they've given up making arguments at this point and they're just vibing.

36:02.160 --> 36:09.440
So I don't even know if there's an argument that debunked there. So from my perspective,

36:10.240 --> 36:17.760
it's we are building systems. They are going to be built by default unless we do something about it.

36:17.760 --> 36:22.960
So the general public wants these systems to not be built, or at least for us to slow down,

36:22.960 --> 36:26.160
until we can make them safe and we understand them better and they've been integrated to society,

36:26.160 --> 36:30.480
et cetera, et cetera. So now you might ask the question, okay, well, that's true.

36:31.280 --> 36:36.320
Why is fuck all happening? And that's a good question. And now we have to talk about models

36:36.320 --> 36:43.920
of policy change and like global coordination, which at least how I think about this problem

36:43.920 --> 36:51.200
generally is that the general public actually does have power in the West and like in democratic

36:51.200 --> 36:57.120
countries. It's very fashionable among elites to sneer and be like, Oh, actually, you see the

36:58.400 --> 37:04.000
populace, you know, they don't have true control, you know, we live in a whatever the words are

37:04.000 --> 37:10.800
that people like to use. And this is to a large degree true, but it's not fully true.

37:11.600 --> 37:18.640
The main problem is that the general public has extremely short attention spans

37:18.720 --> 37:26.320
and extremely discoordinated. This is the main problem. The bottleneck on policy action currently

37:26.320 --> 37:34.400
is not will of the people. It's not ability to enforce regulation. It's coordination. It's

37:34.400 --> 37:39.360
getting people to actually do something about it, you know, to actually write letters to their

37:39.360 --> 37:43.600
senators, actually put things on their desks, actually yell at them on the phone, you know,

37:43.600 --> 37:48.240
actually like, you know, talk about on social media, et cetera, et cetera. This is the kind

37:48.240 --> 37:52.880
of thing that's currently missing basically campaigning. This is the kind of stuff that

37:52.880 --> 38:01.440
is missing. And I expect that if you did this well, if you raise this to saliency about people,

38:01.440 --> 38:04.400
you wouldn't have to you wouldn't have to convince them. And I'm saying this because

38:04.400 --> 38:08.240
empirically, this has been true in my experience, like talking to people and also like doing stuff

38:08.240 --> 38:12.320
like focus groups and stuff. I found that you don't really need to convince people very much.

38:12.320 --> 38:17.040
You mostly just have to tell them facts just have to, you know, just like present them with,

38:17.040 --> 38:23.120
hey, this is what's going on right now. And then mostly they converge to the like a reasonable

38:23.120 --> 38:29.600
beliefs around like, hey, that's scary, don't do that. So I think this is currently the best

38:29.600 --> 38:37.280
path we have. I'm also, you know, excited to talk to politicians and I talked to many of them,

38:37.280 --> 38:44.320
mostly in the UK and the EU, because I'm UK based. But it's hard because, you know, politicians have

38:44.320 --> 38:47.840
similar problems. They have very little attention span, because they have so many things they need

38:47.840 --> 38:56.720
to do. There's so many things haranguing them. And my model of policymakers is basically that the

38:56.720 --> 39:06.320
ultimate goal of a politician is to not get blamed. So it's because the politician you have

39:06.320 --> 39:11.120
really like I have so any if there's any policymakers listening or any staffers or so on,

39:11.200 --> 39:17.760
I feel you, you're in a shit spot, I get it. Because like, basically the way I see it is like

39:17.760 --> 39:23.200
there's like a two by two grid of like what you do as a politician, which you can do. So

39:23.920 --> 39:30.560
the idea is that there's a default action is that in a common, in our common, you know, feelings

39:30.560 --> 39:34.880
around issue, there's something that is the default thing to do, which is usually nothing.

39:35.840 --> 39:42.000
If you do the default action, and it goes wrong, well, you're not blamed, you know,

39:42.000 --> 39:47.600
because, you know, you did the sensible thing, not your fault. If you do the default action,

39:47.600 --> 39:53.440
and it goes, well, well, great, you're a genius, you know, good job. If you do the non default

39:53.440 --> 40:00.720
action, and it goes great, cool, yeah, you're good, great. If you do the non default action,

40:00.800 --> 40:07.920
and it goes bad, then you get blamed. That's how you get blamed. So you may notice from

40:07.920 --> 40:12.480
this payoff matrix, that it is always better to take the default action rather than non default

40:12.480 --> 40:19.280
action. It is always better for the politician to not stray off the path. And this is universally

40:19.280 --> 40:24.880
true. So it's easy to yell at politicians and be like they have no spine, they have no courage

40:24.880 --> 40:30.880
and whatever. And yeah, that's true for many of them. Many of them are just, yeah, just, you

40:30.880 --> 40:36.000
know, just don't care, true. But some do, and they do go off the path and they get burned for it.

40:36.720 --> 40:42.400
And that sucks. But it is how the game is. So what we can do as the people is we have to change

40:42.400 --> 40:47.760
what the default action is. You have to change the narrative from, I guess we just keep bumbling

40:47.760 --> 40:54.800
along until we die to how the fuck dare you keep bumbling, like seize your bumbling immediately.

40:55.360 --> 40:59.440
Bumbling is no longer accepted. And that's my biggest hope at the moment.

41:00.160 --> 41:09.280
Yeah. When we spoke last time, again, right as GPT four was being released.

41:09.280 --> 41:17.600
One of your immediate concerns was that these things can be hooked up to

41:19.680 --> 41:27.600
systems that can take action. And I don't remember if we talked about auto GPT that first,

41:28.480 --> 41:36.400
I haven't looked at what's happened with that, but that first attempt to create an agent that could

41:36.400 --> 41:44.960
use LLMs. But that has developed a pace. And we're now on the cusp of seeing

41:46.640 --> 41:53.120
sort of an explosion of AI agents that can leverage the power of large language models or other

41:53.840 --> 42:05.440
other tools. I had a guy on earlier from News Guard, a company that builds databases to

42:06.720 --> 42:12.320
try and help companies, tech companies identify

42:12.720 --> 42:24.160
disinformation and combat it. And we were talking about, once you have these agents building

42:27.600 --> 42:35.040
creating disinformation, not only creating the disinformation, but distributing it on a massive

42:35.040 --> 42:44.160
scale and maybe on a massively parallel scale. The internet, public discourse, everything is

42:44.160 --> 42:51.600
going to get very confusing because you're not going to be able to tell what's real and what's

42:51.600 --> 43:00.400
not real and people, which is the majority who are not particularly careful about where they're

43:00.400 --> 43:13.520
getting their information will be manipulated. So yeah, the coming AI agent era, how do you

43:13.520 --> 43:26.080
deal with that? I mean, I don't know, get your affairs in order. A number of years ago, post

43:26.080 --> 43:34.880
GPT-2 was around GPT-3 time. That's how we mark the eras now. Instead of years, we just use GPTs

43:34.880 --> 43:45.600
now. I was invited to work kind of like just like a discussion group with some open AI people,

43:45.600 --> 43:51.600
policy people, disinformation experts and stuff like this about the potential for misinformation

43:51.600 --> 43:58.800
and so on from language models, especially before GPT-4, before chat GPT and so on. And

44:00.720 --> 44:06.960
polite lead to all these well-credentialed experts with their triple Stanford professorships or

44:06.960 --> 44:14.560
Harvard, whatever, talk about misinformation, bias and whatever. And then when it came my turn

44:14.880 --> 44:23.040
to talk, my reaction was like, holy shit, you're all so undressed. You're being so optimistic.

44:23.040 --> 44:29.280
It's so much worse than any of you. You're like, oh, it could make it easier for far writers to

44:30.240 --> 44:35.120
that's that's that's fucking children's play compared to what you could do with these things.

44:35.120 --> 44:39.040
Like you were truly you're not creative. Like if you think that's the worst that can happen,

44:39.040 --> 44:42.800
oh, they're going to generate some fake news and some like Russian digital websites. I mean,

44:42.800 --> 44:46.880
oh boy, that would be nice. That's the nice timeline. It's going to be much worse than that.

44:46.880 --> 44:51.280
It's already getting worse like that. Talk about fully automated cults with fully

44:51.280 --> 45:00.720
automated profits. Talk about all sensory, illusionary interactive systems, creating

45:00.720 --> 45:05.040
full complex narratives that are completely disconnected from reality. Talk about full

45:05.040 --> 45:12.240
epistemic collapse, the semantic apocalypse. Even if AIs don't kill us, they're going to drive us

45:12.240 --> 45:20.160
insane. So it's because it will just be harder and harder and harder to survive in a more and more

45:20.160 --> 45:24.560
adversarial informational environment. This has already been happening for a very long time.

45:24.560 --> 45:28.880
You know, we just had Thanksgiving. And as much as we love her, we all have that one

45:28.880 --> 45:36.560
aunt that get way too into QAnon a while back. And imagine so, you know, currently stuff like

45:36.560 --> 45:40.880
QAnon or like, I don't even know if QAnon is still a thing, but like whatever the newest thing is,

45:40.880 --> 45:45.760
the newest cult is, the newest whatever is, you know, that affects, you know, some percentage of

45:45.760 --> 45:49.440
the population, you know, some percentage of the more vulnerable population. I'm going to say

45:49.440 --> 45:53.520
stupid, just like, you know, maybe emotionally vulnerable or like epistemically, you know,

45:53.520 --> 45:59.120
vulnerable and for some reason, not trying to judge these people here. Now imagine the bar keeps

45:59.120 --> 46:04.000
racing. You get systems that become more and more convincing, that become more and more

46:04.000 --> 46:09.520
sophisticated, more and more targeted, and slowly, slowly, the number of people who are just

46:09.520 --> 46:17.760
functionally schizophrenic keeps going up until at some point, people cannot converge on reality

46:17.760 --> 46:24.400
anymore. And people just every person you meet is functionally schizophrenic. You cannot run a society.

46:24.400 --> 46:31.200
You cannot organize a system if you and your neighbor cannot come to a conclusion about

46:31.200 --> 46:37.920
basic reality. This is like what is possible with these kinds of systems. I'm not saying this is

46:37.920 --> 46:42.560
going to happen next year. I mean, maybe, but this is the kinds of things you couldn't do.

46:43.120 --> 46:49.920
Like the like epistemics is hard. Like this is the thing that like, there's also things like honesty

46:49.920 --> 46:54.640
is hard. This is like some people are like, Oh, just, you know, misinformation is a trivial

46:54.640 --> 46:58.240
concept. It's almost become a slur at this point. It's kind of come a joke, you know, like when

46:58.240 --> 47:01.840
people use the word answer information, like, at least in my social circles, a lot of people like

47:01.840 --> 47:07.200
rolled their eyes to be like, Oh, yeah, anything that isn't big media isn't this information,

47:07.200 --> 47:13.520
whatever. But like, it's just not that easy. Like finding out what is true and disseminating and

47:13.520 --> 47:19.840
evaluating what is true is hard. This is very hard. It takes energy. It takes effort. It takes

47:19.840 --> 47:25.280
mechanisms. It takes like it's hard. And it's going to get harder. It's going to get more expensive.

47:25.280 --> 47:31.600
But currently, like, do you really know what's happening in Ukraine right now? Really? I don't.

47:32.080 --> 47:37.680
I think I'm at a point where it is like literally impossible for me to actually know what's going

47:37.680 --> 47:43.440
on in Ukraine. It's something that affects me, you know, affects family, friends, you know, it is

47:43.440 --> 47:49.680
a huge thing. I don't think that there is any way I could actually acquire and verify

47:50.560 --> 47:57.600
that the truth of what is actually going on there. And this generalizes. This is even before we get

47:57.600 --> 48:02.960
into agents doing worse things than this. I mean, automating all jobs, obviously, you know,

48:02.960 --> 48:08.160
anything you can do at a computer, an agent will do better and faster. So there will be complete

48:08.160 --> 48:12.880
economic collapse from that. Like, obviously, there will be no more need for human jobs unless

48:12.880 --> 48:17.200
until the inference costs, you know, get too high. But you know, you can improve those back down.

48:17.760 --> 48:24.960
You'll have systems that can do harm in various ways, you know, by manipulating markets,

48:25.040 --> 48:31.200
campaigns, politics, you're going to have systems that are, you know, cybercrime, hacking, you

48:31.200 --> 48:36.160
have system like it's like when you ask a question like, what is the worst thing agent based systems

48:36.160 --> 48:40.400
are doing? You're asking the question, what are the worst intelligence systems can do?

48:40.400 --> 48:44.640
What is the worst that a human can do? The answer is a lot.

48:44.960 --> 48:50.560
Yeah. But again,

48:53.600 --> 49:04.320
yeah, I mean, you can you can see that that very bleak future. But I'm also a great believer in

49:04.400 --> 49:06.880
in how

49:10.640 --> 49:24.960
mankind, the worst case scenario generally is not what happens. And people kind of muddle along.

49:25.920 --> 49:32.720
But that survivorship bias, there was a man named Stanislav Petrov, who was a Russian soldier

49:32.720 --> 49:37.920
stationed in a nuclear bunker. And he had the command that if American missiles appear on the

49:37.920 --> 49:45.680
screen, he shoots the missile. And one day, six missiles appeared on his screen. His commands

49:45.680 --> 49:50.560
were very clear. The second guy with him there who had, you know, the other key was ready to

49:50.560 --> 49:54.720
turn and yelled at him that it's time we have to shoot back. The Americans are attacking.

49:56.400 --> 50:02.400
And Stanislav didn't. He disobeyed orders. He could have been, you know, fucking executed for that.

50:02.400 --> 50:07.200
And he disobeyed orders that day. And it's because of this one man, one Russian soldier,

50:08.240 --> 50:14.160
that you and me weren't nuked. One guy, we got lucky. So when people said, oh, but so far as

50:14.160 --> 50:16.800
I was like, what the fuck are you talking about? This is like saying, well, I've played Russian

50:16.800 --> 50:22.160
roulette five times so far and it's been great. Let me pull again. That's just not how anything

50:22.160 --> 50:28.960
works. This is not how reality works. If you play like this, then eventually you predictably lose.

50:29.920 --> 50:35.360
You have to play strategies where you can win in adversarial environments where you can play,

50:35.360 --> 50:40.480
where you can win in games where dangers exist. Our ancestors, when they were in the wild,

50:40.480 --> 50:45.600
they couldn't be like, well, oh, my forefathers survived. So I don't have to worry about bears.

50:46.160 --> 50:50.720
You know, none of my forefathers got killed about bears. No, like that's just, no,

50:50.720 --> 50:55.840
this is not how things work. The world isn't nice. There is no arc of history. There is no God

50:55.840 --> 51:01.360
that is protecting us. The fact that we are here today is because of the hard work of our ancestors.

51:02.000 --> 51:06.560
The fact that I live in this nice, you know, warm apartment, sound like safe that I have enough food

51:06.560 --> 51:13.920
to eat and so on is not God that gave me that. It's not some, you know, force of nature. It was

51:13.920 --> 51:21.840
the hard scrabble and bloody fight of my ancestors that left me this. And if I let this to rot,

51:21.840 --> 51:29.520
if me and other people don't maintain society, then it just dies. Like then entropy wins.

51:30.160 --> 51:37.200
Entropy always increases and entropy is death. So if we just sit back and hope things will go well,

51:37.760 --> 51:44.960
they will not. So, you know, I was gonna, I was thinking, well, that's a good place to end it,

51:44.960 --> 51:54.320
but I don't want to end there because our last conversation got an inordinate number of views.

51:55.280 --> 52:05.120
And I have some producers that take these and turn them into shorts and they have these sound bites

52:06.080 --> 52:15.200
from that episode that have gotten an enormous number of views because people gravitate towards

52:15.200 --> 52:26.240
these doomsday proclamations. And I don't, I mean, whether or not they're true, I want to end

52:26.960 --> 52:34.800
something more hopeful. So what, what, what should people do in your view? What should

52:34.800 --> 52:42.960
regulators be doing? What should researchers be doing? What should Microsoft be doing now?

52:43.680 --> 52:49.280
So the weirdest thing I would tell them is like, to be clear, I don't like being the doomed guy.

52:49.280 --> 52:53.680
I absolutely don't like this. I was the pecto optimist throughout my entire life. I was always

52:53.680 --> 52:57.760
the person saying, no, we can fix problems. Climate change is solvable. You know, solar

52:57.760 --> 53:01.760
powers can be exponentially cheaper. You know, we can do carbon capture with like, there are so

53:01.760 --> 53:06.320
many things we can do. I've always been saying like, no, like, you know, see how in the interest

53:06.320 --> 53:10.720
improved education, how much people are becoming, you know, better at, you know, and having more

53:10.720 --> 53:16.560
access to information. Look at how so many things are like, I was just reading the other day about

53:16.560 --> 53:23.360
how slowly over decades, just the flash freezing of frozen food has gotten better. And I've noticed

53:23.360 --> 53:28.160
this, just like my frozen broccoli, I'll make it night. It's just a little bit nicer. And you

53:28.160 --> 53:32.160
know what, that might sound like a teeny thing compared to all these other things, but I think

53:32.160 --> 53:37.920
that's beautiful. I think it's extremely beautiful that life gets better. All things being equal,

53:37.920 --> 53:44.080
life has gotten a lot better. I'm very happy to be alive when I am right now. All these small things

53:44.080 --> 53:48.640
done by these smart people, mostly done for profit. Sure, the broccoli company, they just want profit,

53:48.640 --> 53:53.280
but ultimately, they made my dinner a little bit nicer. It was already fine. Like I was already

53:53.280 --> 53:57.680
surviving, but it was a little bit nicer. And you know what, that's awesome. And it's so nice

53:57.680 --> 54:04.080
that we can live this way. The truth is, is that we are so lucky that we live in a society full of

54:04.080 --> 54:11.040
educated, smart people that for the most part, you know, not all over more angels, they're not heroes,

54:11.760 --> 54:15.280
but they want to make, they do want to leave the world better, you know, they want people to be

54:15.280 --> 54:20.400
happy, they want people to be safe. Most things being equal, you know, almost everyone, you know,

54:20.400 --> 54:24.000
given the option, if they could just help someone else and it didn't cost them anything,

54:24.000 --> 54:29.600
they'd do it. And that's really nice. So we have to leverage this. We have to leverage that we,

54:29.600 --> 54:35.200
and this is not the case everywhere in the world, I want to say. This is something that even today

54:35.200 --> 54:41.200
is not in every country. It is not in every place or in every society. But in the West and, you know,

54:41.200 --> 54:47.680
many other countries in the Far East and so on, most people are educated. Most people are decent.

54:47.760 --> 54:53.920
Again, I'm saying they're great or heroes, but they're decent. And they want the world to go well.

54:53.920 --> 54:58.880
They want their kids to grow up and have a nice life and, you know, eat nice frozen, you know,

54:59.600 --> 55:06.320
broccoli, you know, whatever, you know, they want to see art and beauty and, you know, music and so

55:06.320 --> 55:11.280
on. And we can have this. This is the important thing to understand. The important thing is,

55:11.280 --> 55:16.240
sometimes I'll talk about this, is that like this, this idea of techno optimism, quote unquote,

55:16.240 --> 55:20.560
it's just cynicism and disguise. This is a really important thing to understand.

55:20.560 --> 55:25.280
These people who put, who's talking about, oh, yeah, actually, we're, we're techno optimists,

55:25.280 --> 55:30.720
we're accelerationists or whatever. They're just cynics. They're just libertarian cynics

55:30.720 --> 55:36.720
that don't believe that society can be improved, except by just like giving themselves to this

55:36.720 --> 55:42.080
abstract process of technology. But technology is not a force of nature. It's not a thing

55:42.080 --> 55:48.240
happening to us. It's a thing that we do. It's like, it's about humanity. It's not about technology

55:48.240 --> 55:52.640
that like, sure, technology is great, it's helped humans, but I only care about technology because

55:52.640 --> 55:57.360
I care about humans, care about people. And we all care about people. We care about our families,

55:57.360 --> 56:02.400
we care about our friends. And technology should be a tool. It should not be a goal in and of itself.

56:02.400 --> 56:07.760
So when people talk about, well, AGI is inevitable, someone's going to do it. No, no, it is not.

56:08.320 --> 56:13.840
It is not inevitable. It is not a force of nature. It's a decision we make. It is a decision we make

56:13.840 --> 56:23.120
and we can do better. We can, as people, societies, as civilizations, make choices. We can say, hey,

56:23.120 --> 56:29.520
let's be a little more careful. That doesn't mean we'll do not do any AI anymore. We can just say,

56:29.520 --> 56:35.280
hey, give our scientists a couple more years, a couple more decades to understand the mathematics

56:35.280 --> 56:39.760
of interpretability better, and then maybe we'll give it another shot, you know, like we did with

56:39.760 --> 56:45.040
human cloning. These are what's important. I'm not saying that this is easy or that this is what's

56:45.040 --> 56:50.320
going to happen. It's because it's not what's going to happen by default. But it's just important

56:50.320 --> 56:57.200
that there is this poison in our society that believes that the future is already decided.

56:57.760 --> 57:04.240
And it is not. The future is not yet decided. We still have a choice. It is not yet too late

57:05.280 --> 57:12.160
for what it will be soon. Hi, I wanted to jump in and give a shout out to our sponsor,

57:12.160 --> 57:20.960
NetSuite, by Oracle. I'm a journalist and getting a single source of truth is nearly impossible.

57:20.960 --> 57:26.800
If you're a business owner, having a single source of truth is critical to running your

57:26.800 --> 57:37.360
operations. If this is you, you should know these three numbers. 36,000, 25,1. 36,000,

57:37.360 --> 57:43.920
because that's the number of businesses that have upgraded to NetSuite by Oracle. NetSuite is the

57:43.920 --> 57:50.560
number one cloud financial system, streamlining, accounting, financial management, inventory,

57:50.560 --> 57:59.600
HR, and more. 25, because NetSuite turns 25 this year. That's 25 years of helping businesses do

57:59.600 --> 58:06.880
more with less, close their books in days, not weeks, and drive down costs. One, because your

58:06.880 --> 58:14.320
business is one of a kind. So you get a customized solution for all of your KPIs in one efficient

58:14.320 --> 58:21.840
system with one source of truth. Manage risk, get reliable, forecast, and improve margins.

58:21.840 --> 58:29.120
Everything you need, all in one place. As I said, I'm not the most organized person in the

58:29.120 --> 58:35.920
world, and there's real power to having all of the information in one place to make better decisions.

58:36.880 --> 58:42.160
This is an unprecedented offer by NetSuite to make that possible.

58:43.520 --> 58:49.760
Right now, download NetSuite's popular KPI checklist, designed to give you consistently

58:49.760 --> 59:02.000
excellent performance, absolutely free at netsuite.com slash I on AI. That's I on AI, E-Y-E-O-N-A-I,

59:02.080 --> 59:10.880
all run together. Go to netsuite.com slash I on AI to get your own KPI checklist.

59:10.880 --> 59:22.880
Again, that's netsuite.com slash I on AI, E-Y-E-O-N-A-I. They support us, so let's support them.

59:23.680 --> 59:33.200
That's it for this episode. I want to thank Connor for his time. If you want to read a transcript

59:33.200 --> 59:42.160
of the conversation, you can find one on our website, I on AI. That's E-Y-E-O-N-A-I.

59:43.120 --> 59:54.080
As I always say, the singularity may not be near, but AI is changing your world, changing it rapidly,

59:54.800 --> 01:00:02.000
so pay attention.

