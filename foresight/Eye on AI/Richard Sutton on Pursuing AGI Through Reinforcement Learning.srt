1
00:00:00,000 --> 00:00:04,320
There isn't a science around that isn't profoundly influenced by the availability of

2
00:00:04,320 --> 00:00:08,960
massive computing power and just greater regular computing power. It's the story of our age. It's

3
00:00:08,960 --> 00:00:15,680
not just the story of AI. The idea is to leverage computation to make useful things and understand

4
00:00:15,680 --> 00:00:20,320
the mind. These all these things need a lot of computation. It's the fact that computation has

5
00:00:20,320 --> 00:00:27,040
become cheaper exponentially for on the order of 100 years and can be expected to continue going

6
00:00:27,520 --> 00:00:33,680
It looks like doubling every two years now every 18 months and that keeps happening 18 months after

7
00:00:33,680 --> 00:00:39,600
18 months after 18 months and it means you double and you double and things get qualitatively different

8
00:00:39,600 --> 00:00:44,880
every decade and that's happened for a long time for many decades and will happen more so in the

9
00:00:44,880 --> 00:00:48,880
future. So we have that to look forward to. I think it's what we really should mean when we

10
00:00:48,880 --> 00:00:54,000
say the singularity. The singularity is that we have this exploding it's a slow explosion of

11
00:00:54,000 --> 00:00:59,440
computer power and that that is fundamentally changing things. Hi I'm Craig Smith and this is

12
00:00:59,440 --> 00:01:06,560
I on AI. In this episode I speak with Richard Sutton the father of reinforcement learning

13
00:01:06,560 --> 00:01:12,320
and professor at the University of Alberta. We discussed his cooperation with John Carmack

14
00:01:12,320 --> 00:01:19,520
on Keen a startup that vows to reach artificial general intelligence by 2030. Richard also talked

15
00:01:19,520 --> 00:01:25,920
about the Alberta plan his ambitious five-year research agenda focused on building embodied

16
00:01:25,920 --> 00:01:32,560
agents with the capability to learn and plan through interactions with their environment.

17
00:01:32,560 --> 00:01:38,640
Sutton provides insights into the current state of progress new algorithmic developments

18
00:01:38,640 --> 00:01:45,200
and trade-offs between simulated and physical environments in training and the ultimate goal

19
00:01:45,200 --> 00:01:52,640
of creating AGI. I hope you find the conversation as amazing as I did. So why don't you start by

20
00:01:52,640 --> 00:01:58,640
introducing yourself. I assume people know who you are I've had you on the podcast before

21
00:02:00,000 --> 00:02:08,960
but for those new listeners tell us who you are where you are and then we'll talk about the Alberta

22
00:02:08,960 --> 00:02:18,800
plan which I find pretty exciting. Thank you Craig I'm Richard Sutton I'm a scientist I've been

23
00:02:18,800 --> 00:02:26,480
studying artificial intelligence for like 45 years a long time and I'm up in north of the

24
00:02:26,480 --> 00:02:33,280
University of Alberta in Canada and I'm a professor in the computer science computing science department

25
00:02:33,280 --> 00:02:41,040
and also I'm a researcher at Keen Technologies and I got lots of titles and sub-rolls but basically

26
00:02:41,040 --> 00:02:46,560
I'm just trying to figure out how the mind works and I've tried to do it in a very broad and

27
00:02:46,560 --> 00:02:52,880
interdisciplinary way reading all the different thinkers on the subject and addressed from the

28
00:02:52,880 --> 00:03:00,640
point of view of psychology and how the brain might work as well as. Yeah I've read a number of the

29
00:03:00,640 --> 00:03:08,240
recent papers and I can see this thread developing and I don't know whether it's just that you're

30
00:03:08,240 --> 00:03:15,040
writing more and so the thoughts are are more developed in print or whether they're developing

31
00:03:15,040 --> 00:03:23,200
in your mind but from 2019 when you wrote the bitter lesson you talked about the idea that

32
00:03:23,200 --> 00:03:32,000
it's really increasing computation and the striving a lot of things a lot of progress

33
00:03:32,720 --> 00:03:43,600
that kind of coincided with open AI's scaling of the transformer model I talked to Ilya

34
00:03:43,600 --> 00:03:53,280
Sutskover and I asked him whether your essay had triggered their their interest in scaling and he

35
00:03:53,280 --> 00:04:01,600
said no it was coincidental but I kept first can we talk about that about how scale scaling and

36
00:04:01,600 --> 00:04:10,480
and the availability of computational resources and Moore's law has driven a lot of what's happened

37
00:04:10,560 --> 00:04:18,560
in artificial intelligence research almost more than novel algorithms well I think the

38
00:04:18,560 --> 00:04:23,920
first thing to be aware of is it's it's been driving things that are not just artificial

39
00:04:23,920 --> 00:04:30,240
intelligence it's been driving all the sciences and all the engineering developments in the world

40
00:04:31,520 --> 00:04:36,000
there isn't a science around that isn't profoundly influenced by the availability

41
00:04:36,720 --> 00:04:41,440
availability of massive computing power and just greater regular computing power

42
00:04:42,480 --> 00:04:48,480
it's it's it's the story of our age it's not just the story of AI it's not particularly the story

43
00:04:48,480 --> 00:04:58,080
of AI AI has always known that it needs computation the idea is to leverage computation to make useful

44
00:04:58,080 --> 00:05:06,720
things and understand the mind um yeah now it's true that those of us who are interested in

45
00:05:06,720 --> 00:05:13,200
connection to systems or distributed networks nowadays just call neural networks not particularly

46
00:05:13,200 --> 00:05:18,880
good terms so I always shudder a little bit when I use it but those of us that have been doing that

47
00:05:18,880 --> 00:05:24,320
have have those are doing learning I think that learning is important for intelligence

48
00:05:25,040 --> 00:05:30,960
these all these things need a lot of computation and so they're they are limited by the computation

49
00:05:30,960 --> 00:05:37,680
available at the time okay so let's let's be what is this thing what is the so Moore's law what's

50
00:05:37,680 --> 00:05:44,320
called Moore's law it's the fact computation is becoming more plentiful and cheaper exponentially

51
00:05:45,040 --> 00:05:51,280
for on the order of a hundred years and can be expected to continue going that way so

52
00:05:51,360 --> 00:05:56,880
exponentially looks like doubling every two years now each every 18 months and that keeps

53
00:05:56,880 --> 00:06:02,560
happening 18 months after 18 months after 18 months and it means you double and you double

54
00:06:02,560 --> 00:06:10,560
and things get qualitatively different uh every decade and that's happened for a long time for

55
00:06:10,560 --> 00:06:16,080
many decades and will happen it's more so in the future so we have that to look forward to that will

56
00:06:16,080 --> 00:06:25,600
continue having a tremendous influence on everything that's done on the other hand it's just normal

57
00:06:25,600 --> 00:06:32,880
it's just what you would expect and that those of who worked on AI for for a long time have just

58
00:06:32,880 --> 00:06:41,680
you know expect and plan for and um now it's coming but it's an exponential so exponentials

59
00:06:41,680 --> 00:06:47,440
are self-similar so that means they look the same at every point in time every every year it's

60
00:06:47,440 --> 00:06:52,800
you're doubling in a year and a half and so it's it's an explosion as every exponential is an

61
00:06:52,800 --> 00:06:58,560
explosion it's it's sort of I think it's what we really should mean when we say the singularity

62
00:06:58,560 --> 00:07:05,760
the singularity is that we have this exploding it's a slow explosion of computer power and that

63
00:07:06,160 --> 00:07:12,240
has fundamentally changed things yeah and I had a really interesting conversation almost a year

64
00:07:12,240 --> 00:07:20,400
ago with Aidan Gomez who was on the team that that that designed the transformer algorithm at

65
00:07:20,400 --> 00:07:29,120
Google and he now has a startup co-coher he's Canadian and he said an interesting thing that

66
00:07:29,840 --> 00:07:36,160
that he believes it could have been almost any algorithm it didn't have to be the transformer

67
00:07:36,800 --> 00:07:43,760
that the community got behind the transformer poured resources into it continued to scale it

68
00:07:44,560 --> 00:07:50,240
and it was scalable I mean that was important that it that it's a scalable architecture but

69
00:07:51,440 --> 00:07:57,200
that but it didn't have to be the transformer and and that made me think of you because

70
00:07:58,000 --> 00:08:07,840
of so transformers they the way he described it at its core it's a stock of multi-layer

71
00:08:07,840 --> 00:08:15,680
perceptrons with attention you scale it feed it data and it does learns to understand language

72
00:08:15,680 --> 00:08:22,240
or at least seems to understand language but it's got all these obvious limitations

73
00:08:23,200 --> 00:08:30,240
of I've been talking a lot over the last couple of years to Yamakun about world models and that

74
00:08:30,240 --> 00:08:41,200
to me sounded like a much more exciting direction for general intelligence because not all intelligence

75
00:08:41,200 --> 00:08:51,520
is is contained in language or at least most or even less so in human text and then I see you guys

76
00:08:51,600 --> 00:09:00,640
come along with the Alberta plan and that that that sounded even more exciting to me so

77
00:09:02,480 --> 00:09:09,600
how how do you so the Alberta plan you're building the ideas to build an

78
00:09:10,640 --> 00:09:18,480
agent ultimately an embodied agent that that has a world model or can

79
00:09:18,720 --> 00:09:26,720
and create a world model through interactions with its environment how is that different from

80
00:09:27,600 --> 00:09:38,640
Lucune's approach at a very basic level very basic level a good is that they're a very similar

81
00:09:38,640 --> 00:09:44,320
idea it's uh you look at the parts of his architecture and the parts of the architecture

82
00:09:44,320 --> 00:09:50,800
put forth in the Alberta plan they line up one one for one yeah we're trying to do the same thing

83
00:09:53,280 --> 00:09:57,440
we're going about it slightly different and we could talk about that but I think

84
00:09:58,080 --> 00:10:02,160
to just to focus on the differences might even be to distract from the big message the big

85
00:10:02,160 --> 00:10:08,400
message is that you have to have a goal and you have to have a model of the world and

86
00:10:09,360 --> 00:10:18,160
and then everything is driven by using that model to take action and to plan action at various levels

87
00:10:18,160 --> 00:10:27,600
of abstraction in order to to achieve the goal okay so to me this is really what intelligence is

88
00:10:27,600 --> 00:10:32,880
understand the world use your understanding to get to achieve to achieve your your goals

89
00:10:33,520 --> 00:10:38,320
I'd like to formulate the goals as as a reward and I'm super comfortable with that other people

90
00:10:39,200 --> 00:10:42,400
sort of grudgingly accept rewards even though it seems kind of low level

91
00:10:43,120 --> 00:10:48,880
but it's a it's a natural approach I think I think it's something that almost makes more

92
00:10:48,880 --> 00:10:53,520
sense to people who aren't steep and deep learning and to supervise learning and one thing I found

93
00:10:53,520 --> 00:11:00,000
interesting in in the roadmap that you've laid out for the Alberta plan you start with supervised

94
00:11:00,000 --> 00:11:07,680
learning and why is that is it just because it's it's easy yeah I guess we do in a sense

95
00:11:07,680 --> 00:11:13,120
because we want to focus on well continual learning learning continually which is sort of

96
00:11:13,120 --> 00:11:18,800
an obvious thing almost what learning means it has something that goes on at all times but

97
00:11:20,000 --> 00:11:28,800
the first steps getting continual learning with nonlinear networks is still challenging

98
00:11:28,800 --> 00:11:35,360
even for supervised learning and so it's natural to start at the simplest possible case which involves

99
00:11:35,360 --> 00:11:43,280
the fewest other factors and that's a supervised learning case yeah yeah it's funny let me just

100
00:11:43,280 --> 00:11:48,880
say a few words about that because there's sort of been a fight through a struggle throughout the

101
00:11:48,880 --> 00:11:55,280
decades between supervised learning and reinforcement learning you know there's only so much oxygen

102
00:11:55,280 --> 00:12:01,120
for learning methods and all the attention that's paid to supervised learning somewhat

103
00:12:01,120 --> 00:12:06,080
detracts from reinforcement learning so there's a there's a there's a bit of a friendly competition

104
00:12:08,160 --> 00:12:13,200
and supervised learning has always won the competition because supervised learning is so

105
00:12:13,200 --> 00:12:18,960
much more easy to put into practice and for people to use and it's sort of it's sort of

106
00:12:18,960 --> 00:12:23,600
less ambitious but it's really important and really those of us who do reinforcement learning

107
00:12:23,600 --> 00:12:30,000
or try to make whole agent architectures we are consumers of supervised learning outcomes we will

108
00:12:30,000 --> 00:12:35,120
use them as components of our overall architecture so we need them and we can work on them and we

109
00:12:35,120 --> 00:12:43,440
need to structure them for our purposes I saw one of your talks you make a distinction between

110
00:12:44,400 --> 00:12:53,840
AI tools and AI agents and supervised learning falls into the tool category can you sort of

111
00:12:53,840 --> 00:13:00,320
start and and talk about the evolution of the Alberta plan and then present to listeners

112
00:13:00,960 --> 00:13:08,480
what it is in in its simplest form and that'll that'll give me a structure on which to hang

113
00:13:08,560 --> 00:13:16,800
questions the Alberta plan is an attempt to understand intelligence as a as a primarily

114
00:13:16,800 --> 00:13:24,480
a learning phenomenon assists us something that comes to understand its environment and and then

115
00:13:26,000 --> 00:13:32,640
drives the environment to achieve goals so the first step in the Alberta plan is the structure

116
00:13:32,640 --> 00:13:37,280
between the agent the environment and their interaction form the interaction there's the

117
00:13:37,280 --> 00:13:43,200
you're not exchanging states you're exchanging observations like sensors sensors visual touch

118
00:13:43,200 --> 00:13:51,760
auditory it's all abstract to those particulars but it's got to be genuine observations and not

119
00:13:51,760 --> 00:13:59,520
state because state we don't we don't really have access to directly so that you know the

120
00:13:59,520 --> 00:14:04,160
principles number one principle I'm trying to remember them as I speak but number one principle

121
00:14:04,160 --> 00:14:12,080
is this this agent environment interaction is sacrosanct and number two is that learning or

122
00:14:12,080 --> 00:14:18,400
everything is is we could say continual I think we call it we say temporally uniform

123
00:14:19,520 --> 00:14:24,560
temporally symmetric in in the Alberta plan which means that there are no special phases

124
00:14:24,560 --> 00:14:31,280
where you like training and test there's just life goes on and on you get rewards or you don't get

125
00:14:32,240 --> 00:14:38,960
or you don't get the reward you want and you get your observations and there there is no teacher

126
00:14:38,960 --> 00:14:50,160
other than rewards pains and pleasures and maybe I'm not getting the four principles right but

127
00:14:50,160 --> 00:14:56,000
another important point is that you are going to be forming a model and so you're going to plan

128
00:14:56,640 --> 00:15:03,120
both trial and error learning directly from experience and learning a model and then planning

129
00:15:03,120 --> 00:15:09,440
with the model both these are important part of intelligence okay so those are that's the

130
00:15:09,440 --> 00:15:15,440
background then we outline there are 12 steps and the 12 steps really start with let's have

131
00:15:15,440 --> 00:15:21,520
learning that is temporally uniform let's have metal learning and metal learning maybe I should

132
00:15:21,520 --> 00:15:28,240
stop and on that for a moment metal learning means learning to learn not just learning one function

133
00:15:28,240 --> 00:15:32,800
but once you are continually learning you're learning this and you're learning that you get

134
00:15:32,800 --> 00:15:38,080
many many experiences learning and you can get better at learning you can use those repeated

135
00:15:38,080 --> 00:15:46,160
experience with repeatedly learning to make make future learning episodes more efficient so as part

136
00:15:46,240 --> 00:15:53,440
of that you learn representations you learn features you learn step sizes

137
00:15:55,680 --> 00:16:00,800
okay so continual learning and then all the algorithms and once once we add

138
00:16:00,800 --> 00:16:05,840
metal learning and continual learning we have to in supervised learning then we extend that to

139
00:16:07,520 --> 00:16:12,720
reinforcement learning which involves its own set of issues to get more interesting temporal

140
00:16:12,720 --> 00:16:22,080
relationships and I think like the first six steps are crafting the basic algorithms of reinforcement

141
00:16:22,080 --> 00:16:30,080
working through them again to be continual and meta and then we start to bring in the the challenging

142
00:16:30,080 --> 00:16:37,040
issues like learning off policy and learning models of the world and then planning and the

143
00:16:37,840 --> 00:16:40,320
just to jump to the end the last step is about

144
00:16:44,320 --> 00:16:49,520
AI, AI, AI's, AI intelligence augmentation

145
00:16:51,760 --> 00:16:59,520
where we combine computers, AI's with our own minds to make make our own minds stronger

146
00:17:00,960 --> 00:17:06,640
okay now one of the key steps in there was off policy learning and learning a model of the world

147
00:17:07,840 --> 00:17:13,440
off policy learning means you want to be able to learn about things that you're not doing or you're

148
00:17:13,440 --> 00:17:20,640
not because you're not doing all the way to completion so even like to recognize an object

149
00:17:20,640 --> 00:17:27,760
you look at the object and you say how would you you have to define that in some objective way

150
00:17:28,400 --> 00:17:33,280
and the best way to just do that is as a sub problem so

151
00:17:35,520 --> 00:17:43,440
yeah maybe maybe I'll just sort of stop there the most interesting strategy

152
00:17:45,280 --> 00:17:50,160
distinctive strategy by the Alberta plan is the pose is that the mind works by posing sub

153
00:17:50,160 --> 00:17:55,200
problems for itself and then working on them and it's it's not it's sure it's got a main

154
00:17:55,280 --> 00:18:00,480
problem which is to get reward but it's also has many thousands of sub problems it's also

155
00:18:00,480 --> 00:18:05,840
working on simultaneously and since it's not behaving it cannot behave for all thousand

156
00:18:05,840 --> 00:18:10,080
problems at once it has to pick one problem like perhaps the main problem and behave according to

157
00:18:10,080 --> 00:18:14,880
that so all the other things have to be able to learn from data that's not exactly on what they

158
00:18:14,880 --> 00:18:22,720
would do and this is called off policy learning and it's a key to learning to achieve auxiliary

159
00:18:22,720 --> 00:18:28,720
sub problems and also it's a key to efficiently learning a model of the world yeah you you have

160
00:18:29,760 --> 00:18:37,360
something called the horde architecture is is that where that comes in when you you break

161
00:18:37,360 --> 00:18:45,360
a problem down into multiple sub tasks that that you learn I was one one paper where we

162
00:18:46,480 --> 00:18:49,920
we worked on that idea we developed that idea the horde is the horde of sub problems

163
00:18:50,560 --> 00:18:58,240
each each demon in the horde which is it could be almost viewed like a single neuron in a neural

164
00:18:58,240 --> 00:19:04,640
network as achieving working towards a different task trying to predict a different thing or maybe

165
00:19:04,640 --> 00:19:11,840
trying to attain a different thing it's the view of the of the mind as decentralized there is one

166
00:19:11,840 --> 00:19:17,520
goal and everything is ultimately driven towards one goal but still it's a useful structure to

167
00:19:17,840 --> 00:19:25,920
to have different parts driving towards towards other goals how did you get together with john

168
00:19:25,920 --> 00:19:34,000
cormack was that primarily because you need the funding and it gives you a vehicle to raise

169
00:19:34,000 --> 00:19:40,560
capital oh seriously I mean you you know Jan Lacoon's got meta behind him well it's just not it's

170
00:19:40,560 --> 00:19:47,200
not really comparable uh john's uh john's company is great but it's still like a 20 million dollar

171
00:19:47,200 --> 00:19:55,840
company and uh which is which is plenty of money for what we want to do now um john and I got together

172
00:19:55,840 --> 00:20:02,800
because we had similar ideas about what was needed um and and also what was not needed

173
00:20:03,520 --> 00:20:13,200
um to get to ai or agi um yeah so I read an art newspaper article an interview that john did

174
00:20:14,320 --> 00:20:17,920
down in texas and uh I just could see that he was thinking about the way

175
00:20:18,640 --> 00:20:21,440
thinking about things the way I was even though our backgrounds were quite different

176
00:20:22,000 --> 00:20:27,280
you thought of intelligence you had to there's a few principles that needed to be worked out

177
00:20:27,280 --> 00:20:32,320
rather than so this isn't a huge program to write it's a few principles we have to figure those out

178
00:20:33,040 --> 00:20:39,840
um not that many maybe uh maybe 10 000 lines instead of 10 million lines of code

179
00:20:40,720 --> 00:20:48,240
so it's easy to get it's relatively it's still it's still it's still hard to get basic research

180
00:20:48,240 --> 00:20:54,400
funding in the world it's easy to get funding towards uh applications of ai large language models

181
00:20:54,400 --> 00:21:01,760
particularly um anyway I'm really enjoying working at keen and being able to focus on the ideas

182
00:21:02,560 --> 00:21:11,200
and uh it's a it's a it's a it's a calm company we um um

183
00:21:12,960 --> 00:21:17,760
there's a lot of thinking involved a lot of contemplation a lot there is also experiments

184
00:21:17,760 --> 00:21:23,200
and we're trying to get um the engineering side of it is really important uh but for me it's

185
00:21:23,200 --> 00:21:28,720
been really great just to be able to regroup my thoughts and think about them very carefully and

186
00:21:29,280 --> 00:21:35,920
push them forward but keen is is implementing the alberta plan is that right I mean that's

187
00:21:35,920 --> 00:21:41,520
that's uh the project well the alberta plan is a research plan it's like a five-year research plan

188
00:21:42,080 --> 00:21:48,880
and so research is something you don't implement research is something you conduct and and it

189
00:21:48,880 --> 00:21:56,480
doesn't always end up the way you want but um yeah I wouldn't say implement is the right word

190
00:21:56,480 --> 00:22:03,360
not yet but but the the work you're doing at keen is is informed by the alberta

191
00:22:04,080 --> 00:22:10,240
yeah I'm absolutely I'm working on the alberta plan uh uh and and the end goal at keen is to

192
00:22:10,240 --> 00:22:19,600
create the uh the embodied intelligence described by the alberta plan you don't sound very yeah

193
00:22:19,600 --> 00:22:25,440
very confident well a plan is just a plan and you know I think there's a good chance that it

194
00:22:25,440 --> 00:22:31,680
will work out as planned but you know a five-year plan you make another one after four or three years

195
00:22:33,760 --> 00:22:41,520
yeah so I wouldn't I wouldn't uh presume to to know how it's gonna work out but at the same time

196
00:22:41,520 --> 00:22:48,800
we have to make you know we have to make our bets we have to think hard about it um just knowing um

197
00:22:49,760 --> 00:22:56,800
you know we we may well be right but you know you your work is primarily in reinforcement

198
00:22:56,800 --> 00:23:02,400
learning you're you wrote the book on reinforcement learning temporal difference learning and

199
00:23:03,920 --> 00:23:12,640
uh lambda and all of that is is this I mean this is this seems a much more ambitious

200
00:23:13,600 --> 00:23:23,760
uh project is this was it the the success of the transformer scaling that that said well you know

201
00:23:24,560 --> 00:23:31,280
let's do that with rl let's why why are these guys uh uh you know everyone's celebrating what

202
00:23:31,280 --> 00:23:36,960
they're doing but but there's much more to be done no no what what you're seeing the alberta plan

203
00:23:36,960 --> 00:23:43,120
is is perhaps bigger than the book but this has always been the plan we've always in AI

204
00:23:43,120 --> 00:23:49,520
tried to understand all of the mind and reproduce it in computers and so that's an that's that is a

205
00:23:49,520 --> 00:23:58,240
big enormous ambition that's what it's always been so the large language models are a bit

206
00:24:00,080 --> 00:24:04,160
a bit disappointing in some sense I mean it's really good that people are getting excited

207
00:24:04,240 --> 00:24:12,080
and people are wanting to learn about it but um but it's not it's I don't envision that it's the

208
00:24:12,720 --> 00:24:22,240
direction um that will be most uh productive to pursue now you know who knows what I do know

209
00:24:22,240 --> 00:24:27,920
is it's not the most direction that's useful for me to pursue um I I'm much more interested in

210
00:24:27,920 --> 00:24:33,360
actions and goals and how an agent can tell what's true and what's not true all of those

211
00:24:33,360 --> 00:24:42,480
things are missing from large language models so uh um no I'm not they're not really what what are

212
00:24:42,480 --> 00:24:48,720
they what they are doing that's important is they're showing uh what you can do with uh computation

213
00:24:48,720 --> 00:24:57,520
and and networks um and learning that you can get enormously complex um things and you can

214
00:24:57,600 --> 00:25:03,440
incorporate a lot of data it just shows the power for those who needed to be shown that

215
00:25:05,360 --> 00:25:15,280
and and it could be uh an interface between humans and and whatever you end up creating

216
00:25:15,280 --> 00:25:23,840
the agents you end up creating you still need a language interface to communicate yeah but I don't

217
00:25:24,800 --> 00:25:29,840
I'm I doubt that what we're doing with large language models today will contribute to that

218
00:25:29,840 --> 00:25:35,520
oh is that right yeah I mean in other words the models that you want to build the agents you want

219
00:25:35,520 --> 00:25:42,400
to build would learn language uh as as part of the learning process yeah so it's like we say

220
00:25:43,120 --> 00:25:48,560
language language last you know language not not language first with large language models

221
00:25:48,560 --> 00:25:54,320
are language first we just say large language last just as Jan McCoon says we need to do you

222
00:25:54,320 --> 00:25:59,280
know rat level intelligence and then cat level intelligence and we have to get those figured

223
00:25:59,280 --> 00:26:05,440
out before we should try to make human level intelligence uh so where are you on the plan I

224
00:26:05,440 --> 00:26:14,480
mean you you figured out reinforcement learning you can build agents uh you there are various

225
00:26:14,480 --> 00:26:22,240
architectures for creating representations from from various kinds of sensory input

226
00:26:24,160 --> 00:26:32,800
and and at that representation level then you can plan efficiently so where in all of that

227
00:26:33,920 --> 00:26:38,720
are are you in your research well it's a little hard to explain non-technically

228
00:26:38,800 --> 00:26:45,360
but you can say some things certainly you can say that the various steps

229
00:26:46,960 --> 00:26:52,480
are not done entirely sequentially you you're always looking for areas of opportunity where

230
00:26:52,480 --> 00:26:58,400
you can make an increment of progress and those could be you know on step 10 or they could be on

231
00:26:58,400 --> 00:27:05,280
step three but you also I could also try to be very rough and say that we're we're at about step

232
00:27:05,360 --> 00:27:12,480
four now we are still doing things where we're changing the basic underlying

233
00:27:13,200 --> 00:27:20,640
fundamental reinforcement learning algorithms we are not done with that we need more efficient

234
00:27:20,640 --> 00:27:28,000
algorithms and I'm excited about some of the changes new ideas we're developing recently

235
00:27:28,000 --> 00:27:33,760
about how that can be done can you talk about those new ideas at all okay well one of the big

236
00:27:33,760 --> 00:27:40,960
things is efficient off-policy learning and the use of important sampling important sampling is

237
00:27:40,960 --> 00:27:45,760
where you see how likely you're to do things under your target and your behavior policies

238
00:27:45,760 --> 00:27:53,920
and you adjust the returns based on those the ratios of those two and for a long time I thought

239
00:27:53,920 --> 00:28:01,760
that was the only way to adjust the returns but now the forward correction of the returns I think

240
00:28:01,760 --> 00:28:12,800
can be done by by changing your expectations so like if you're expecting a good thing to happen

241
00:28:12,800 --> 00:28:18,320
you're expecting a good action to be taken and then a different action was taken a more exploratory

242
00:28:18,320 --> 00:28:26,320
action so this is a deviation from your target policy which would be more greedy and one way to

243
00:28:26,320 --> 00:28:30,800
take into account the deviation from the target policy is to just say oh okay now I've done something

244
00:28:31,440 --> 00:28:35,360
not best so I'm just going to adjust my level now you're going to expect a little a little less

245
00:28:36,080 --> 00:28:38,960
and there's a way there's a systematic way of doing that

246
00:28:41,280 --> 00:28:47,120
that's gives us a new way to handle the off-policiness of of our returns

247
00:28:47,840 --> 00:28:53,520
and so this gives a whole new family of algorithms so that's exciting now

248
00:28:54,320 --> 00:29:02,240
exciting maybe mostly for me I think maybe the most accessible direction of of of excitement

249
00:29:02,800 --> 00:29:08,080
of novelty is in continual right so there's I'm going to say a bunch of things and to me

250
00:29:08,080 --> 00:29:13,360
they're all going to have the same solution continual learning meta learning representation

251
00:29:13,360 --> 00:29:19,760
learning learning to learn learning how to generalize state how to construct a state

252
00:29:19,760 --> 00:29:27,040
representation feature finding that whole thing is is is coming and it will be a kind of

253
00:29:28,400 --> 00:29:33,600
it's just a new kind of a way a new kind of way of doing the learning in deep networks

254
00:29:35,680 --> 00:29:42,080
and I call it dynamic learning nets see a dynamic learning nets have learning at three levels

255
00:29:42,080 --> 00:29:47,360
whereas usually our neural networks only learn at one level they learn the level of the weights

256
00:29:48,000 --> 00:29:53,120
and in addition we also want to learn at the level of step sizes so all of every place you

257
00:29:53,120 --> 00:29:58,000
have a weight in your network you're also going to have a step size so a step size is sometimes

258
00:29:58,000 --> 00:30:01,440
called a learning rate it's much better to call the step size because the learning rate will be

259
00:30:01,440 --> 00:30:07,600
influenced by many other things so if we imagine a whole network all these weights next to each

260
00:30:07,600 --> 00:30:14,240
weight is a step size that is adjusted by an adaptive process that's adapted in a meta learning way

261
00:30:14,240 --> 00:30:20,000
a meta gradient way towards making the system learn better rather than just perform better

262
00:30:20,000 --> 00:30:25,760
an instantaneous moment in time learning rates or step sizes don't affect the function they don't

263
00:30:25,760 --> 00:30:29,120
affect some function implemented in a particular point in time they don't affect with the network

264
00:30:29,120 --> 00:30:34,800
does they affect what the network learns and so if you can tune the step sizes you also get

265
00:30:34,800 --> 00:30:41,840
learning to learn and learning to generalize well and things like that the last three the last

266
00:30:41,840 --> 00:30:48,800
element that we wanted to have be adaptive weights step sizes the third one is the connection pattern

267
00:30:49,600 --> 00:30:56,240
so who's connected to who and so this will be done by an accretive process

268
00:30:58,480 --> 00:31:04,160
like let's say you start with a linear unit and it learns say a value function or a policy

269
00:31:04,160 --> 00:31:10,160
and it does the best it can with the features available and and then it needs to induce the

270
00:31:10,160 --> 00:31:15,280
creation of new features because you need to learn a nonlinear function of your original

271
00:31:15,280 --> 00:31:22,480
signals and so you need to create new features that have become available to that linear unit

272
00:31:22,480 --> 00:31:28,720
and in this way you grow in a sort of organic way a system that can learn nonlinear functions

273
00:31:30,960 --> 00:31:38,400
and so this is just a different way of ending up with a deep network that was all learned

274
00:31:38,400 --> 00:31:44,560
including all the features dynamic learning that's where is the data the input data coming from

275
00:31:44,560 --> 00:31:49,680
well the the input data and reinforcement just comes from life from doing things seeing things

276
00:31:49,680 --> 00:31:54,080
right there is no labeled data set yeah maybe I should have said this from the very beginning

277
00:31:54,080 --> 00:32:01,360
the whole idea of I call it experiential AI is that you know what makes you data you're you you

278
00:32:01,360 --> 00:32:07,920
grow up as a baby and you play with things and you see things and you do things and that's the

279
00:32:07,920 --> 00:32:13,680
data and the trick of reinforcement learning is how do you turn that kind of data into something

280
00:32:13,680 --> 00:32:19,280
you can learn from and grow a mind up from so the the beauty and the limitation of supervised learning

281
00:32:19,280 --> 00:32:24,400
is they say well let's not worry about that for now let's assume that somehow we have a data set

282
00:32:24,400 --> 00:32:29,920
with labeled things and let's let's work on this sub problem that's a great idea work on a sub problem

283
00:32:29,920 --> 00:32:35,760
figure it out and then move on to the next thing but really we have to move on to the next thing

284
00:32:35,760 --> 00:32:41,360
we have to worry about how the the data set quote data set is automatically created from

285
00:32:41,360 --> 00:32:47,120
the the training information there isn't ever a data set data set is is is such a misleading term

286
00:32:47,120 --> 00:32:52,080
it suggests that it's easy to to have this thing and store this thing and curate this thing

287
00:32:52,080 --> 00:32:58,000
really life is full of you do things things happen and then there's one you know everything is

288
00:32:58,000 --> 00:33:04,880
fleeting you you don't have a record of it and it would be enormously complex and not only valuable

289
00:33:04,880 --> 00:33:10,880
to have a record of it the the the feeling is totally different in reinforcement learning and

290
00:33:10,880 --> 00:33:16,880
supervised learning and in particularly the way the way I would adjust it you know many people

291
00:33:17,680 --> 00:33:23,120
do reinforcement learning by creating a buffer or a record of all the experiences that have been

292
00:33:23,120 --> 00:33:30,880
been retained that have been occurred at least for some period of time and I think that's that's

293
00:33:31,520 --> 00:33:37,840
uh an appealing but but it's it's not where the action the answer is the answer is

294
00:33:39,040 --> 00:33:46,000
embracing the fleeting nature of data and and making most when it happens and then letting it go

295
00:33:46,560 --> 00:33:52,560
well that's why you want to make an embodied system so that you have all the the five senses or

296
00:33:53,360 --> 00:33:59,600
or more so you need you need as you say an embodied system an interactive system that that

297
00:33:59,920 --> 00:34:07,360
influences its its input stream its sensory stream and that you get that interaction and for a long

298
00:34:07,360 --> 00:34:14,960
creative time you can do this in simulation or you can do it in robotics there's still I still

299
00:34:14,960 --> 00:34:20,320
know what's the best way or if the best ways do both and right or maybe first one and then the other

300
00:34:21,120 --> 00:34:30,800
John is interested in uh having um uh learning from video and he likes his his his view of the

301
00:34:30,800 --> 00:34:36,880
experience is you have massive numbers of video streams like you're viewing you know 500 channels

302
00:34:36,880 --> 00:34:44,400
of television and then you can switch switch to look at one look at another one um uh other people

303
00:34:44,400 --> 00:34:52,320
in in in keen my close colleague Joseph Modial he's uh interested in robotics and he thinks the

304
00:34:52,320 --> 00:34:58,800
best way to get an appropriate in data stream is to actually build robotic hardware um

305
00:35:00,800 --> 00:35:05,520
you know it's important that the world be large and complex because the worlds we want to address

306
00:35:05,520 --> 00:35:12,640
are large and complex um and so you want things like video and you want large data streams um

307
00:35:14,400 --> 00:35:21,760
now you can use simulations to generate even video streams simulated video but inevitably

308
00:35:21,760 --> 00:35:27,680
those simulated worlds are really quite simple they have an underlying simplicity uh they have

309
00:35:27,680 --> 00:35:33,920
objects perhaps and three-dimensional straight structure maybe they're rigid objects and the

310
00:35:33,920 --> 00:35:40,240
vision is is is a very particular geometric form um they they are generated and they are

311
00:35:40,240 --> 00:35:44,880
they are made up worlds and they are generated so they're they're really the worlds are are are

312
00:35:44,880 --> 00:35:50,400
less complex than the agent uh their goal would be to have to spend most of the computer power

313
00:35:50,400 --> 00:35:56,880
working on the mind and just a little bit to just create the simulated data and and that's that's

314
00:35:56,880 --> 00:36:06,000
reversed the way it really is right every person is maybe has a has a complex brain but their world

315
00:36:06,000 --> 00:36:11,840
is much more complex not just because the world consists of all these um physics and matter

316
00:36:11,840 --> 00:36:16,400
but it also consists of other minds other brains and other minds out there and and what goes on

317
00:36:16,400 --> 00:36:20,880
in their minds matters and so the world is inherently vastly more complex than the agent

318
00:36:22,160 --> 00:36:26,640
and we we've reversed that when we work on simulated worlds so which is always concerning

319
00:36:27,600 --> 00:36:32,880
anyway those are some of the issues in the trade-offs between working with simulations or with

320
00:36:33,600 --> 00:36:38,480
physical worlds nonetheless you you need to develop the architecture and the algorithms

321
00:36:39,440 --> 00:36:46,080
before you worry about the data data stream i would think yeah but you want to develop the

322
00:36:46,080 --> 00:36:51,360
right algorithms and if you're working with the world it's not representative of of your target

323
00:36:51,360 --> 00:36:58,880
world in an important way um it can be misleading but you're right and that's what we that's what

324
00:36:58,880 --> 00:37:03,120
we strive to do you know i don't know if you know but i think of my own work is almost always

325
00:37:03,680 --> 00:37:08,480
i want to focus on some issues so i make a really simple instance of that issue like you know a

326
00:37:08,480 --> 00:37:16,320
five-state world and and i study the the hell out of it but i don't like try to take advantage of

327
00:37:16,320 --> 00:37:22,320
its smallness you know i study algorithms that are in some sense even simpler than the simple world

328
00:37:22,320 --> 00:37:27,520
and i i stress those algorithms and see what their abilities are so we always you know it's always

329
00:37:27,520 --> 00:37:33,520
part of research is we we simplify the world understand it fully just like a a physicist might

330
00:37:33,520 --> 00:37:38,240
you know make a simplified world with with a ball rolling down a ramp and it's it's a really

331
00:37:38,240 --> 00:37:44,480
simple world and you'll try to eliminate the friction and you eliminate other weird effects

332
00:37:44,480 --> 00:37:50,880
and just see things in their simplest form yeah have you um paid much attention to um

333
00:37:51,840 --> 00:37:58,160
alex kendall's work at at wave ai do you know that company it's an autonomous driving company

334
00:37:58,160 --> 00:38:07,440
they have a world model called gaya one um and it's it's it's similar to what yanlacoon's doing

335
00:38:07,440 --> 00:38:17,520
it it you know encodes representations from from video from live video and then uh plans

336
00:38:18,480 --> 00:38:29,440
uh based on those representations uh and and it can control a car uh from the representation space

337
00:38:29,440 --> 00:38:37,600
it's actually pretty remarkable so let's talk about the world model and and what what kind of

338
00:38:37,600 --> 00:38:46,560
world model would be appropriate for autonomous driving um so let me say some things that are

339
00:38:47,920 --> 00:38:53,920
mistakes they're a natural seeming but mistakes in my opinion uh the mistake would be to make

340
00:38:53,920 --> 00:39:00,480
like a physics model of the world or to try to make something that could simulate the world and

341
00:39:00,480 --> 00:39:06,480
produce the video frames you don't you don't you don't want the video frames of the future

342
00:39:06,480 --> 00:39:13,760
that's not the way you think um instead you think oh i could i could go to the market and

343
00:39:13,760 --> 00:39:20,560
maybe there would be strawberries okay you're not creating a visual uh a video you're saying

344
00:39:20,560 --> 00:39:27,120
you're like jumping to the market and then your strawberries could be you know different sizes

345
00:39:27,120 --> 00:39:34,080
and positions and and and still uh there's not a video there's an idea that will happen if you go

346
00:39:34,080 --> 00:39:46,160
to the market um so uh people have realized this like yon lakun used to talk about um generating

347
00:39:46,160 --> 00:39:53,840
video of the future and then you realize it would be blurry and and now he realizes that you need

348
00:39:53,840 --> 00:39:59,360
to produce outcomes of your model that are not like not at all like video streams and not like

349
00:39:59,360 --> 00:40:07,680
observations at all they're like um they're like constructed states um that are the outcome of the

350
00:40:07,680 --> 00:40:15,520
action okay so this is this is a very different from from a partial differential equation model

351
00:40:15,520 --> 00:40:22,400
of the world and it's so it's very different from what self-driving car companies start with

352
00:40:22,400 --> 00:40:28,000
self-driving car companies start with physics and geometry and uh you know things that are

353
00:40:28,000 --> 00:40:33,440
calibrated by human understanding engineers understanding of the world and driving but

354
00:40:34,480 --> 00:40:39,840
i suspect that's going to be i mean what do i know i'm not into self-driving i don't do

355
00:40:39,840 --> 00:40:48,240
self-driving cars but i know that that um like tesla is and elon musk is and um so their goal

356
00:40:48,240 --> 00:40:53,120
is to is to make some you know they they started like everyone else with engineering models but i

357
00:40:53,760 --> 00:40:59,520
my understanding now is that they're building uh sort of more conceptual models um that are

358
00:40:59,520 --> 00:41:05,600
based on the artificial neural networks okay and so rather than starting with geometry and

359
00:41:05,600 --> 00:41:10,480
understood things they're just getting massive amounts of data and training it to make a model

360
00:41:10,480 --> 00:41:16,080
we need a model that is at the level of high level consequences not at the level of low level

361
00:41:16,080 --> 00:41:21,120
things like pixels and video so one way you do that is you're having state features that are

362
00:41:21,120 --> 00:41:27,440
at a more advanced level you say oh this is a car uh rather than this is a uh a video frame

363
00:41:29,280 --> 00:41:38,480
and um so and then basically it's as simple as you need abstraction in both state and time

364
00:41:39,120 --> 00:41:44,880
abstraction in in state is like saying there will be strawberries when i get to the market

365
00:41:44,880 --> 00:41:51,040
and abstraction in in time is saying oh i can go to the market and then in 20 minutes i will

366
00:41:51,040 --> 00:41:58,640
be there probably and other things will be the same or related in natural ways

367
00:42:01,440 --> 00:42:05,360
so we want to be able to think about i could go to the market you also want to think oh i could

368
00:42:05,360 --> 00:42:10,240
pick up the coke can i could move a finger and that will have certain consequences these these

369
00:42:10,240 --> 00:42:14,960
all these things that we know you think uh are vastly different scales going to the market is like

370
00:42:14,960 --> 00:42:22,720
20 minutes um you know taking taking a new job you know might be a year uh deciding to study a

371
00:42:22,720 --> 00:42:28,800
topic also might be a period of time we think and we analyze the consequences like you wanted to

372
00:42:28,800 --> 00:42:35,360
meet with me today and you know we arranged it we set it up it was your your planning uh took

373
00:42:35,360 --> 00:42:42,160
you know place over weeks and some cases months and and and we assembled the the the event of

374
00:42:42,160 --> 00:42:48,640
this interview by by planning all that and exchanging mess high-level messages uh it all that

375
00:42:48,640 --> 00:42:53,680
you know it's silly to think that that's done at the level of of of imagining videos that we might

376
00:42:53,680 --> 00:43:00,640
see with our eyes or our audio is signals that we might hear yeah so we need models that are

377
00:43:00,640 --> 00:43:09,200
abstract in time and state and um as a reinforcement learning person um there's a particular set of

378
00:43:09,200 --> 00:43:16,640
technologies that i naturally turn towards to do that um the prediction is based on multi-step

379
00:43:16,640 --> 00:43:25,520
prediction by temporal difference learning um the planning is done by uh dynamic programming

380
00:43:25,520 --> 00:43:32,480
essentially value iteration but where the steps the are not low-level actions but they're called

381
00:43:32,480 --> 00:43:36,960
options they're high-level ways of behaving with that that terminate so they're there are things

382
00:43:36,960 --> 00:43:42,960
like going to the market and they'll terminate when you're at the market so you know at a certain

383
00:43:42,960 --> 00:43:52,400
conceptual level it's clear where we want to go to me um with abstract models in time and state

384
00:43:53,200 --> 00:43:54,640
built options and features

385
00:43:58,240 --> 00:44:05,920
i don't know you we did write one paper recently put published an AI journal on the the notion of

386
00:44:05,920 --> 00:44:16,000
planning using uh sub-problems on the stomp progression stomp means sub-task option model

387
00:44:16,000 --> 00:44:21,200
and planning put all those things together and you can do the full progression from from the

388
00:44:21,200 --> 00:44:28,560
data stream to abstract planning and that's that's what we're trying to put together yeah yeah and i

389
00:44:28,560 --> 00:44:35,680
i sort of misspoke talking about gaya one about that model i mean it they they the input is video

390
00:44:37,200 --> 00:44:45,600
it creates a representation and it plans and and and takes action in the representation

391
00:44:46,640 --> 00:44:54,480
plans actions in the representation space you can then decode that into video to see what

392
00:44:55,200 --> 00:45:02,480
what it's doing but but it's but you're not planning in the video space so the what what's

393
00:45:02,480 --> 00:45:09,440
your ambition with with this you'll figure out the refine the algorithms the reinforcement learning

394
00:45:09,440 --> 00:45:19,360
algorithms they need to be scalable once you have that uh then you move on and and uh start start

395
00:45:19,360 --> 00:45:27,680
scaling them with compute and and uh you know following your roadmap or am i simplifying it too

396
00:45:27,680 --> 00:45:32,720
much you know we want to understand how the mind works and then we're going to make a mind or some

397
00:45:32,720 --> 00:45:42,400
minds or some mind uh amount of mind uh and this will be useful in all the ways in all sorts of

398
00:45:42,400 --> 00:45:50,960
ways economically useful it'll also be useful um to to us to extend the capabilities of our own

399
00:45:50,960 --> 00:45:58,560
minds if we can understand how our minds work um we can we can augment them so that they can work

400
00:45:58,560 --> 00:46:06,880
better um yeah we're gonna the the key step is understanding and then there would be millions

401
00:46:06,880 --> 00:46:16,240
of uses um i don't think it's going to be as simple as making uh workers sort of like slaves for us

402
00:46:16,960 --> 00:46:23,760
to direct i don't think it'll be as simple as that um that maybe gives a lower bound on

403
00:46:23,760 --> 00:46:31,440
potential utility our sort of our story for etkin is we say that um well if you suppose you

404
00:46:31,440 --> 00:46:38,080
could make a virtual worker um this would be enormously useful um much of the work that we

405
00:46:38,080 --> 00:46:43,680
all do from day to day is doesn't require a physical presence it doesn't require a robot

406
00:46:43,680 --> 00:46:50,480
much of which we do is just shuffling information around we can do most things through through a

407
00:46:50,480 --> 00:46:59,840
video interface um so why can't we make workers that are extremely useful by playing the roles that

408
00:47:00,080 --> 00:47:05,440
people play in many cases that's that's that's sort of a lower bound and what can be done

409
00:47:05,440 --> 00:47:08,800
i think much more can be done and there'll be much more interesting things to be done

410
00:47:12,000 --> 00:47:18,960
and then this question of what should be done um yeah those are those are rich

411
00:47:18,960 --> 00:47:25,760
philosophical questions and practical questions for the economy yeah uh the the i've seen your

412
00:47:25,760 --> 00:47:30,800
third uh well and one thing on reinforcement learning and sort of supervised learning sort

413
00:47:30,800 --> 00:47:38,240
of took over for a while now it's transformer based generative uh ai but uh during the supervised

414
00:47:38,240 --> 00:47:47,920
learning phase uh the argument was that uh higher knowledge is all supervised learning

415
00:47:47,920 --> 00:47:54,880
and and the it's still supervised it's still supervised in general the ai had large language

416
00:47:54,880 --> 00:48:02,000
models they the the training information is the next token the next word and that's taken as

417
00:48:02,960 --> 00:48:10,880
as the correct action the analogy you gave me was uh you know because the analogy that that's

418
00:48:10,880 --> 00:48:17,840
always given is that you know a child sees an elephant the mother says that's an elephant

419
00:48:18,400 --> 00:48:25,440
and the child very quickly can generalize and and recognize other elements elephants maybe it

420
00:48:25,440 --> 00:48:32,000
makes a mistake and the mother corrects it and says no that's a cow and and that was always given

421
00:48:32,000 --> 00:48:36,800
as an example of supervised learning but maybe it's reinforcement learning maybe it's the child's

422
00:48:37,520 --> 00:48:46,000
reward from the mother praising him for remembering the label the point is that a child has

423
00:48:48,400 --> 00:48:56,480
well-developed concepts classes concepts um before and then and then when it's you know when

424
00:48:56,480 --> 00:49:03,040
its mother says that is an elephant uh there's already an extensive understanding on the child's

425
00:49:03,680 --> 00:49:11,520
and a part of you know what the space is what the objects are and and this this the the thing

426
00:49:12,080 --> 00:49:19,520
that that is being labeled um no the label is the least interesting part of that and the the

427
00:49:19,520 --> 00:49:26,400
the child has already learned all the all all the other most interesting parts of of what it means

428
00:49:26,960 --> 00:49:31,680
to have animals and moving things and objects in its world the label is the least interesting

429
00:49:31,680 --> 00:49:37,760
part well first of all you're talking about agents that that could be virtual workers already

430
00:49:38,320 --> 00:49:48,160
uh using reinforcement learning people are building agents and using large language models

431
00:49:48,160 --> 00:50:00,720
and knowledge bases to you know carry out tasks knowledge based tasks uh so what you're talking

432
00:50:00,720 --> 00:50:08,400
about is is more than uh linguistic tasks or knowledge based tasks you're talking about

433
00:50:09,200 --> 00:50:18,080
of physical planning and physical tasks is that right the key thing is having goals and a lot

434
00:50:19,280 --> 00:50:25,200
if you have an for example an assistant help you plan your day organize your day or do tasks for

435
00:50:25,200 --> 00:50:33,280
you um i'm thinking it's very important that the system is able to have goals and is able to

436
00:50:33,280 --> 00:50:40,240
understand your goals i think it's probably the most important part of an assistant is to understand

437
00:50:40,240 --> 00:50:47,680
the purposes involved and um large language models don't understand don't really understand

438
00:50:47,680 --> 00:50:55,920
purposes involved they will appear to a little bit um but the corner case has always come up and

439
00:50:55,920 --> 00:51:02,240
once you spend a bit of time they're always and you're always in a corner case and so an AI system

440
00:51:02,240 --> 00:51:08,640
is system that that after a bit does silly things and that don't respect the goals that you have or

441
00:51:08,640 --> 00:51:14,560
that have been given to it um that's not going to be a useful assistant so i mean i don't want to be

442
00:51:14,560 --> 00:51:21,200
critical of large language models um they're very very useful but it shouldn't be viewed as a criticism

443
00:51:21,200 --> 00:51:26,560
to say that they're also at the same time have rather important limitations it's not a competition

444
00:51:26,560 --> 00:51:36,000
in that sense are you concerned at all are you ascribed to the threat debate no i think the

445
00:51:36,560 --> 00:51:44,400
i don't i don't uh i i think the doomers are they're not just wrong i think i think they're

446
00:51:44,400 --> 00:51:51,280
blindingly biased the the bias is blinding them to what's going on basically AI is a broadly

447
00:51:51,280 --> 00:51:56,880
applicable technology it's not like it's not like nuclear weapons it's not like it's not like a

448
00:51:56,880 --> 00:52:04,640
bio weapons it can be used for all kinds of things and it's not it's not uh it's uh the way we deal

449
00:52:04,640 --> 00:52:12,000
with such things is we we uh we try to use them well and there will be people that use them

450
00:52:12,720 --> 00:52:17,280
for bad things and then you know this is just normal there's normal technology

451
00:52:17,280 --> 00:52:23,120
is it can be used by good people or bad people the the doomers the doomers are just saying oh

452
00:52:23,120 --> 00:52:29,760
somehow there's going to be it's going to be it's bad in the same way that nuclear weapons are bad

453
00:52:29,760 --> 00:52:37,280
that they and that's just they're just blinded by that metaphor by the thinking that that the AI

454
00:52:37,280 --> 00:52:44,880
will be out to kill them that's just it's just silly and i i don't i don't think well they the

455
00:52:44,880 --> 00:52:52,560
doomers don't actually give coherent reasons for what they what they believe and so it's hard to

456
00:52:52,560 --> 00:52:59,600
argue with them uh so maybe it's fair just to hold that they're they're biased and blind

457
00:52:59,600 --> 00:53:05,600
i don't accept i don't accept an argument this is a proper argument so so where you say you're

458
00:53:05,600 --> 00:53:16,960
maybe at stage four in the research car max says 2030 uh that's you know it's far enough out there

459
00:53:16,960 --> 00:53:25,680
that maybe people won't remember in 2030 that he said 2030 uh it's always uh you know i've 2030

460
00:53:25,680 --> 00:53:32,720
has been out there for a long time and it's it's it's uh you can't it doesn't recede it's always

461
00:53:32,720 --> 00:53:44,240
been 2030 for the uh computer power reaching human scale um quantities yeah but anyway 2030 is is a

462
00:53:44,240 --> 00:53:50,160
reasonable it's a reasonable target for us understanding everything that we need in order

463
00:53:50,160 --> 00:53:56,720
to make uh a real mind yeah i'm good with that yeah as you you have to be ambitious

464
00:53:58,560 --> 00:54:09,200
i've always said that 2030 is a 25 chance of of of achieving a real intelligence a real human

465
00:54:09,200 --> 00:54:15,600
level intelligence 25 chance so probably not but it's it's a big enough chunk of probability that

466
00:54:16,160 --> 00:54:20,800
that an ambitious person should work towards it and try to make it true and it does depend upon

467
00:54:20,800 --> 00:54:27,760
what we do and not just the uh unfolding of the universe so we should we should try to do that

468
00:54:27,760 --> 00:54:34,080
that that is a big the big thing that's happening right now is the the the public is coming to grips

469
00:54:34,080 --> 00:54:39,120
with what it means for there to be for us to understand the mind and to have the ability to

470
00:54:39,120 --> 00:54:47,680
create uh minded things uh and so that that is a big uh transformation it's a big change in our

471
00:54:47,680 --> 00:54:57,280
worldview um and so we absolutely need all kinds of people to uh to help us help us become easy

472
00:54:57,280 --> 00:55:04,880
and become have an understanding of what's happening as we uh achieve human level

473
00:55:05,920 --> 00:55:11,760
designed intelligence that's it for this week's episode i want to thank richard for his time

474
00:55:11,760 --> 00:55:18,080
if you want to read a transcript of today's conversation you can find one on our website

475
00:55:18,160 --> 00:55:26,320
i on ai that's e y e hyphen o n dot ai in the meantime remember the singularity may be getting

476
00:55:26,320 --> 00:55:34,880
closer but ai is already changing your world so pay attention

