1
00:00:00,000 --> 00:00:03,200
Even if you train a system to have a world model that can predict what's going to happen next,

2
00:00:03,200 --> 00:00:07,520
the world is really complicated and there's probably all kinds of situations that the system

3
00:00:07,520 --> 00:00:13,440
hasn't been trained on and need to, you know, fine-tune itself as it goes. The question of how

4
00:00:13,440 --> 00:00:18,720
we organize AI research going forward which is somewhat determined by how afraid people are

5
00:00:18,720 --> 00:00:24,320
of the consequences of AI. So if you have a rather positive view of the impact of AI on society and

6
00:00:24,320 --> 00:00:28,880
you trust humanity and society and democracies to use it in good ways, then the best way to

7
00:00:28,880 --> 00:00:35,680
make progress is to open research. AI might be the most important new computer technology ever.

8
00:00:35,680 --> 00:00:41,760
It's storming every industry and literally billions of dollars are being invested. So buckle up.

9
00:00:42,400 --> 00:00:48,240
The problem is that AI needs a lot of speed and processing power. So how do you compete

10
00:00:48,240 --> 00:00:54,880
without cost spiraling out of control? It's time to upgrade to the next generation of the cloud,

11
00:00:55,520 --> 00:01:03,120
Oracle Cloud Infrastructure or OCI. OCI is a single platform for your infrastructure,

12
00:01:03,120 --> 00:01:11,120
database, application, development, and AI needs. OCI has four to eight times the bandwidth of other

13
00:01:11,120 --> 00:01:18,720
clouds, offers one consistent price instead of variable regional pricing, and of course nobody

14
00:01:18,720 --> 00:01:25,920
does data better than Oracle. So now you can train your AI models at twice the speed and less than

15
00:01:25,920 --> 00:01:33,360
half the cost of other clouds. If you want to do more and spend less like Uber, eight by eight,

16
00:01:33,920 --> 00:01:46,000
and Databricks Mosaic, take a free test drive of OCI at oracle.com slash ion AI. That's E-Y-E-O-N-A-I

17
00:01:46,000 --> 00:01:57,040
all run together oracle.com slash ion AI. Hi, I'm Craig Smith and this is ION AI. In this episode,

18
00:01:57,040 --> 00:02:04,480
I speak again with Yan Lacoon, one of the founders of deep learning and someone who followers of AI

19
00:02:04,480 --> 00:02:11,680
should need no introduction to. Yan talks about his work on developing world models on why he

20
00:02:11,680 --> 00:02:19,280
does not believe AI research poses a threat to humanity and why he thinks open source AI models

21
00:02:19,280 --> 00:02:26,720
are the future. In the course of the conversation, we talk about a new model, Gaia 1, developed by a

22
00:02:26,720 --> 00:02:35,040
company called Wave AI. I'll have an episode with Wave's founder to further explore that world model

23
00:02:35,040 --> 00:02:40,960
which has produced some startling results. I hope you find the conversation with Yan

24
00:02:40,960 --> 00:02:48,480
as enlightening as I did. First, the notion of world model is the idea that the system would get

25
00:02:48,480 --> 00:02:54,960
some idea of the state of the world and be able to predict sort of following states of the world

26
00:02:54,960 --> 00:02:58,960
resulting from just the natural evolution of the world or resulting from an action that the

27
00:02:58,960 --> 00:03:05,440
agent might take. If you have an idea of the state of the world and you imagine an action that

28
00:03:05,440 --> 00:03:11,920
you're going to take and you can predict the resulting state of the world, that means you

29
00:03:11,920 --> 00:03:15,440
can predict what's going to happen as a consequence of a sequence of actions and that means you can

30
00:03:15,440 --> 00:03:21,760
plan a sequence of actions to arrive at a particular goal. That's really what a world model is. At

31
00:03:21,760 --> 00:03:29,680
least that's what the Wave people have understood the word in other contexts, like in the context of

32
00:03:29,680 --> 00:03:36,080
optimal control and robotics and things like that. That's what a world model is. Now, there's several

33
00:03:36,080 --> 00:03:43,520
levels of complexity of those world models, whether they model yourself, the agent, or whether they

34
00:03:43,520 --> 00:03:52,880
model the external world, which is much more complicated. Training a world model basically

35
00:03:52,880 --> 00:03:58,240
consists in just observing the world go by and then learning to predict what's going to happen next,

36
00:03:58,960 --> 00:04:05,120
or observing the world taking an action and then observing the resulting effect, an action that

37
00:04:05,120 --> 00:04:11,920
you take as an agent or an action that you see other agents taking. That establishes

38
00:04:12,160 --> 00:04:21,920
causality essentially. You could think of this as a causal model. Those models don't need to predict

39
00:04:21,920 --> 00:04:27,120
all the details about the world. They don't need to be generative. They don't need to predict exactly

40
00:04:27,120 --> 00:04:34,400
every pixel in a video, for example, because what you need to be able to predict is enough details,

41
00:04:35,280 --> 00:04:47,200
some sort of abstract representation to allow you to plan. You're assembling something out of wood

42
00:04:47,200 --> 00:04:54,320
and you're going to put two planks together and attach them with screws. It doesn't matter the

43
00:04:54,320 --> 00:05:00,480
details of which type of screwdriver you're using or the size of the screw within some limits and

44
00:05:00,480 --> 00:05:05,440
things like that. There are details that in the end don't matter as to what the end result

45
00:05:05,440 --> 00:05:12,400
will be or the precise grain of the wood and things of that type. You need to have some

46
00:05:12,400 --> 00:05:16,800
abstract level of representation within which you can make the prediction without having to

47
00:05:16,800 --> 00:05:24,800
predict every detail. That's why those JPA architectures I've been advocating are useful. Models like

48
00:05:25,760 --> 00:05:31,840
the Gaia 1 model from Wave actually makes prediction in an abstract representation space.

49
00:05:31,840 --> 00:05:37,280
There's been a lot of work in that area for years also at FAIR, but generally the

50
00:05:37,920 --> 00:05:43,200
abstract representation were pre-trained. The encoders that would take images from videos and

51
00:05:43,200 --> 00:05:48,000
then encode them into some representation were trained in some other way. The progress we've

52
00:05:48,000 --> 00:05:53,840
made over the last six months in self-improvised learning for images and video is that now we

53
00:05:53,840 --> 00:06:01,120
can train the entire system to make those predictions simultaneously. We have systems now that can

54
00:06:02,400 --> 00:06:09,280
learn good representations of images and the basic idea is very simple. You take an image,

55
00:06:10,560 --> 00:06:17,520
you run it through an encoder, then you corrupt that image, you mask parts of it, for example,

56
00:06:18,080 --> 00:06:25,200
or you transform it in various ways, you blur it, you change the colors, you change the framing a

57
00:06:25,200 --> 00:06:30,000
little bit, and you run that corrupted image through the same encoder or something very similar.

58
00:06:31,600 --> 00:06:37,120
And then you train the encoder to predict the features of the complete image from the features

59
00:06:37,920 --> 00:06:45,520
of the corrupted one. You're not trying to reconstruct the perfect image,

60
00:06:46,400 --> 00:06:51,520
you're just trying to predict the representation of it. And this is different,

61
00:06:51,520 --> 00:06:56,800
this is not generative in the sense that it does not produce pixels. And that's the secret to getting

62
00:06:56,800 --> 00:07:02,080
self-supervisual into work in the context of images and video. You don't want to be predicting pixels,

63
00:07:02,080 --> 00:07:07,040
it doesn't work. You can't predict pixels as an afterthought, which is what the Gaia system is

64
00:07:07,040 --> 00:07:12,080
doing by sticking a decoder on it and with some diffusion model that will produce a nice image,

65
00:07:12,080 --> 00:07:17,280
but that's kind of a second step. If you train the system by predicting pixels,

66
00:07:18,000 --> 00:07:22,000
you just don't get good representations, you don't get good predictions, you get blurry

67
00:07:22,000 --> 00:07:28,480
predictions most of the time. So that's what makes learning from images and video fundamentally

68
00:07:28,480 --> 00:07:34,400
different from learning from text because in text you don't have that problem. It's easy to predict

69
00:07:34,400 --> 00:07:41,040
words, even if you cannot do a perfect prediction because language is discrete. So language is

70
00:07:41,040 --> 00:07:52,640
simple compared to the real world. And there's a lot written right now about the energy required

71
00:07:52,720 --> 00:08:00,960
in the computational resources GPUs required to train language models. Is it less

72
00:08:02,080 --> 00:08:08,480
in training a world model like using iJAPA architecture? Well, it's hard to tell because

73
00:08:08,480 --> 00:08:15,760
there is no equivalent training procedure, self-supervised training procedure for video,

74
00:08:15,760 --> 00:08:21,760
for example, that does not use JAPA. The ones that are generative don't really work.

75
00:08:22,880 --> 00:08:29,840
Yeah. Well, but this architecture could also be applied to language, couldn't it?

76
00:08:30,720 --> 00:08:37,520
Oh yeah, absolutely. Yeah, so you could very well use a JAPA architecture that makes prediction

77
00:08:37,520 --> 00:08:44,400
in representation space and apply to language. Yeah, definitely. And in that case, would it be

78
00:08:44,480 --> 00:08:52,000
less computationally intense than training a large language model?

79
00:08:52,000 --> 00:08:59,760
It's possible. It's not entirely clear either. I mean, there is some advantage regardless of what

80
00:08:59,760 --> 00:09:05,440
technique you're using to making those models really big. They just seem to work better if you

81
00:09:05,440 --> 00:09:14,480
make them big. So if you make them bigger. So scaling is useful. Contrary to some claims,

82
00:09:14,480 --> 00:09:19,680
I do not believe that scaling is sufficient. So in other words, we're not going to get anywhere

83
00:09:19,680 --> 00:09:30,320
close to human level AI. In fact, not even any more level AI by simply scaling up language models.

84
00:09:31,120 --> 00:09:36,320
Even multimodal language models that we applied to video, we're going to have to find new concepts,

85
00:09:36,320 --> 00:09:44,000
new architectures. And I've written a vision paper about this a while back of a different

86
00:09:44,000 --> 00:09:50,720
type of architecture that would be necessary for this. So scaling is necessary, but not sufficient.

87
00:09:51,680 --> 00:10:01,120
And we're missing some basic ingredients to get to human level AI. We're fooled by the fact that

88
00:10:01,120 --> 00:10:06,400
LLMs are fluent. And so we think that they have human level intelligence because they can manipulate

89
00:10:06,400 --> 00:10:16,320
language. But that's false. And in fact, there's a very good symptom for this, which is that

90
00:10:17,280 --> 00:10:24,960
we have systems that can pass the bar exam, but answering questions from text by basically

91
00:10:24,960 --> 00:10:32,880
regurgitating what they've learned, more or less by road. But we don't have completely autonomous

92
00:10:32,880 --> 00:10:39,360
level five cell driving cars, or at least no system that can learn to do this in about 20 hours

93
00:10:39,520 --> 00:10:46,400
of practice, just like any 17-year-old. And we certainly don't have any domestic robot that can

94
00:10:47,440 --> 00:10:51,200
clear up the dinner table and fill up the dishwasher attest that any 10-year-old can learn

95
00:10:51,200 --> 00:10:58,800
in one shot. So clearly, we're missing something big. And that something is an ability to learn

96
00:10:58,800 --> 00:11:03,360
how the world works and the world is much more complicated than language. And also being able

97
00:11:03,360 --> 00:11:10,720
to plan and reason, basically having a mental world model that allows to plan and predict

98
00:11:11,360 --> 00:11:17,120
consequences of actions. That's what we're missing. It takes a while before we figure this out.

99
00:11:18,160 --> 00:11:27,840
You were on another paper that talked about augmented language models. And

100
00:11:27,840 --> 00:11:33,680
in the embodied touring test, was that the same paper, the embodied touring test?

101
00:11:36,160 --> 00:11:41,760
Can you talk about that? First of all, what is the embodied touring test? I didn't quite

102
00:11:41,760 --> 00:11:50,800
understand that. Well, okay, it's a different concept. But it's basically the idea that you

103
00:11:51,760 --> 00:12:01,280
do, it's based on the Moravec paradox, right? So Moravec many years ago noticed that things that

104
00:12:01,280 --> 00:12:07,360
appeared difficult for humans turned out to sometimes be very easy for computers to do,

105
00:12:07,360 --> 00:12:13,600
like playing chess, much better than humans. Or I don't know, computing integrals or whatever,

106
00:12:13,600 --> 00:12:19,600
certainly doing arithmetic. But then there are things that we take for granted as humans that

107
00:12:19,600 --> 00:12:23,840
we don't even consider them intelligent tasks that we are incapable of reproducing with computers.

108
00:12:25,120 --> 00:12:31,840
And so that's where the embodied touring test comes in. Observe what a cat can do or how fast a cat

109
00:12:31,840 --> 00:12:41,360
can learn new tricks or how a cat can plan to jump on a bunch of different furniture to get to the

110
00:12:41,360 --> 00:12:47,680
top of wherever it wants to go. That's an amazing feat that we can't reproduce with robots today.

111
00:12:48,640 --> 00:12:56,320
So that's kind of the embodied touring test, if you want. Like, can you make a robot that

112
00:12:57,040 --> 00:13:03,360
can behave, have behaviors that are easily wishable from those of animals, first of all,

113
00:13:03,360 --> 00:13:11,440
and can acquire new ones with the same efficiency as animals? Then the augmented

114
00:13:11,440 --> 00:13:18,400
LLM paper is different. It's about how do you sort of minimally change large language models so

115
00:13:18,400 --> 00:13:24,880
that they can use tools so they can, to some extent, plan actions. Like, you know, you need to compute

116
00:13:24,880 --> 00:13:29,280
the product of two numbers, right? You just call a calculator and you know you're going to get the

117
00:13:29,280 --> 00:13:34,400
product of those two numbers. And LLMs are notoriously bad for arithmetic, so they need to do

118
00:13:34,400 --> 00:13:39,680
this kind of stuff or do a search, you know, using a search engine or database lookup or

119
00:13:39,680 --> 00:13:44,240
something like that. So there's a lot of work on this right now and it's somewhat incremental. Like,

120
00:13:44,240 --> 00:13:49,840
you know, how can you sort of minimally change LLM and take advantage of their current capabilities

121
00:13:49,840 --> 00:13:57,600
but still augment them with the ability to use tools? Yeah. And I don't want to get into the

122
00:13:58,240 --> 00:14:04,160
too much into the threat debate. But, you know, you're on one side, your colleagues,

123
00:14:05,040 --> 00:14:11,280
Jeff and Yashor on the other. I recently saw a picture of the three of you. I think you put that

124
00:14:11,280 --> 00:14:21,440
up on social media, saying how, you know, you can disagree but still be friends. This idea of

125
00:14:21,440 --> 00:14:28,960
augmenting language models with stronger reasoning capabilities and the ability,

126
00:14:28,960 --> 00:14:36,720
and agency, the ability to use tools is precisely what Jeff and Yashor are worried about.

127
00:14:38,240 --> 00:14:49,520
Can you just, why are you not worried about that? Okay. So first of all, what you're describing

128
00:14:49,520 --> 00:14:58,240
is not necessarily what they are afraid of. They are alerting people and various governments and

129
00:14:58,240 --> 00:15:04,880
others about various dangers that they perceive. Okay. So one danger, one set of dangers are

130
00:15:05,440 --> 00:15:10,480
relatively short-term. There are things like, you know, bad people will use technology for bad

131
00:15:10,480 --> 00:15:16,880
things. What can bad people use powerful AI systems for? And one concern that, you know,

132
00:15:16,880 --> 00:15:24,640
governments have been worried about and intelligence agencies encounter intelligence and stuff like

133
00:15:24,640 --> 00:15:32,720
that is, you know, could value-intentioned organizations or countries use LLM to help them,

134
00:15:32,720 --> 00:15:39,840
I don't know, design pathogens or chemical weapons or other things or cyber attacks,

135
00:15:39,840 --> 00:15:43,840
you know, things like that, right? Now, those problems are not new. Those problems have been

136
00:15:43,840 --> 00:15:51,200
with us for a long time. And the question is, what incremental help would AI systems bring to the

137
00:15:51,200 --> 00:16:00,160
table? So my opinion is that as of today, AI systems are not sophisticated enough to provide

138
00:16:00,160 --> 00:16:07,200
any significant help for such value-intentioned people because those systems are trained with

139
00:16:07,200 --> 00:16:11,520
public data that is publicly available on the internet. And they can't really invent anything.

140
00:16:11,520 --> 00:16:16,320
They're going to regurgitate with a little bit of interpolation if you want. But

141
00:16:17,120 --> 00:16:24,800
they cannot produce anything that you can't get from a search engine in a few minutes.

142
00:16:26,160 --> 00:16:30,000
So that claim is being tested at the moment. There are people who are actually kind of

143
00:16:30,560 --> 00:16:35,280
trying to figure out, like, is it the case that you can actually do something, you're

144
00:16:35,280 --> 00:16:39,840
unable to do something more dangerous with sort of current AI technology that you can do with a

145
00:16:39,840 --> 00:16:47,760
search engine results are not out yet. But my hunch is that, you know, it's not going to enable

146
00:16:47,760 --> 00:16:54,160
a lot of people to do significantly bad things. Then there is the issue of things like code

147
00:16:54,160 --> 00:16:58,800
generation for cyber attacks and things like this. And those problems have been with us for years.

148
00:16:59,760 --> 00:17:04,080
And the interesting thing that most people should know, like, you know, also for like

149
00:17:04,080 --> 00:17:08,880
disinformation or attempts to corrupt the electoral process and things like this. And what's

150
00:17:09,600 --> 00:17:14,480
very important for everyone to know is that the best countermeasures that we have

151
00:17:14,480 --> 00:17:21,040
against all of those attacks currently use AI massively. Okay. So AI is used as a defense

152
00:17:21,040 --> 00:17:29,280
mechanism against those attacks. It's not actually used to do the attacks yet. And so now it becomes

153
00:17:29,280 --> 00:17:36,560
the question of, you know, who has the better system, like other countermeasures? Is the AI

154
00:17:37,520 --> 00:17:43,840
countermeasures significantly better than the AI is used by the attackers so that, you know,

155
00:17:43,840 --> 00:17:50,560
the problem is satisfactorily mitigated. And that's what we are. Now, the good news is that there are

156
00:17:50,560 --> 00:17:56,720
many more good guys and bad guys. They're usually much more competent. They're usually much more

157
00:17:56,720 --> 00:18:02,960
sophisticated. They're usually much more better funded. And they have a strong incentive to take

158
00:18:02,960 --> 00:18:11,680
down the attackers. So it's a game of cat and mouse, just like every security that's ever existed.

159
00:18:12,480 --> 00:18:20,080
There's nothing new there. Okay. Nothing quite entirely new. Yeah. But then there is the question

160
00:18:20,080 --> 00:18:28,400
of existential risk, right? And this is something that both Jeff and Yosha have been thinking of

161
00:18:29,200 --> 00:18:34,000
fairly recently. So for Jeff, it's only sort of just before last summer that he became,

162
00:18:35,200 --> 00:18:39,520
he started thinking about this because before he thought he was convinced that the kind of

163
00:18:39,520 --> 00:18:45,120
algorithms that we had were significantly inferior to the kind of learning algorithm that the brain

164
00:18:45,120 --> 00:18:52,400
used. And the epiphany he had was that, in fact, no, because looking at the capabilities of

165
00:18:53,200 --> 00:18:58,080
large English models, they can do pretty amazing things with a relatively small number of neurons

166
00:18:58,080 --> 00:19:01,840
and synapses. He said, maybe they're more efficient than the brain. And maybe the learning algorithm

167
00:19:01,840 --> 00:19:05,600
that we use, back propagation, is actually better than whatever it is that the brain uses.

168
00:19:06,240 --> 00:19:09,280
So he started thinking about like, you know, what are the consequences? And

169
00:19:10,320 --> 00:19:13,680
but that's very recent. And in my opinion, he hasn't thought about this enough.

170
00:19:14,000 --> 00:19:23,280
Yosha went to a similar epiphany last winter, where he started thinking about the long-term

171
00:19:23,280 --> 00:19:29,520
consequences. And came to the conclusion also that there was a potential danger.

172
00:19:30,880 --> 00:19:35,360
They're both convinced that AI has enormous potential benefits. They're just worried

173
00:19:36,560 --> 00:19:40,880
about the dangers. And they're both worried about the dangers because they have some doubts

174
00:19:41,680 --> 00:19:47,600
about the ability of our institutions to do the best with technology.

175
00:19:49,360 --> 00:19:56,640
You know, whether they are political, economic, geopolitical, financial institutions,

176
00:19:56,640 --> 00:20:03,920
or industrial, to do the right thing, to be motivated by the right thing. So

177
00:20:04,480 --> 00:20:12,880
you know, if you trust the system, if you trust humanity and democracy,

178
00:20:14,960 --> 00:20:23,200
you might be entitled to believe that society is going to make the best use of

179
00:20:23,200 --> 00:20:28,640
future technology. If you don't believe in the solidity of those institutions,

180
00:20:28,640 --> 00:20:34,640
then you might be scared. Okay. I think I'm more confident in humanity and democracy than they are.

181
00:20:35,280 --> 00:20:39,040
And, and, you know, whatever current systems and they are, I've been thinking about this

182
00:20:39,040 --> 00:20:47,440
problem for much longer, actually, since at least 2014. So when I started fair at Facebook at the

183
00:20:47,440 --> 00:20:53,120
time, it became pretty clear, pretty early on that, you know, deploying AI systems was going to have

184
00:20:53,840 --> 00:20:59,840
big consequences on people in society. And we got confronted to this very early.

185
00:21:00,560 --> 00:21:05,360
And so I started thinking about those problems very early on. Things like, you know, counter

186
00:21:05,360 --> 00:21:12,320
measures against like bias in AI systems, systematic bias, counter measures against attacks,

187
00:21:14,080 --> 00:21:18,560
or, you know, detection of hate speech in every language, things like that. These are things that

188
00:21:18,560 --> 00:21:23,760
people at fair worked on and then were eventually deployed. To just to give you an example, the

189
00:21:24,560 --> 00:21:29,120
proportion of hate speech that was taken down automatically by AI systems five years ago,

190
00:21:29,120 --> 00:21:37,920
you know, in 2017, was about 20 to 25%. Last year, it was 95%. And the difference is entirely due to

191
00:21:37,920 --> 00:21:43,520
progress in natural language understanding, entirely grew to transformers that are pretrained

192
00:21:43,600 --> 00:21:48,640
self-supervised and can essentially detect hate speech in any language. Not perfectly. Nothing

193
00:21:48,640 --> 00:21:54,560
is perfect. It's ever perfect. But AI is massively there. And that's the solution. So I started

194
00:21:54,560 --> 00:22:02,000
thinking about those issues, including existential risk, very early on. In fact, in 2015, early 2016,

195
00:22:02,000 --> 00:22:08,160
actually, I organized a conference hosted at NYU on the future of AI, where a lot of those

196
00:22:08,160 --> 00:22:17,600
questions were discussed. I invited people like, you know, Eric Schmidt and Mark Schreffer, who

197
00:22:17,600 --> 00:22:26,000
was the CTO of Facebook at the time. A lot of people, both from the academic and AI research

198
00:22:26,000 --> 00:22:30,880
side and from the industry side. And there were two days, a public day and kind of a more private

199
00:22:30,880 --> 00:22:35,920
day. What came out of this is the creation of an institution called a partnership on AI.

200
00:22:35,920 --> 00:22:42,400
So this is a discussion I had with Mr. Sabis, which was, you know, would it be useful to have a

201
00:22:42,400 --> 00:22:47,840
forum where we can discuss before they happen, sort of bad things that could happen as a consequence

202
00:22:47,840 --> 00:22:55,840
of deploying AI? Pretty soon, we brought on board Eric Horvitz and a bunch of other people,

203
00:22:55,840 --> 00:22:59,200
and we co-founded this thing called a partnership on AI, which basically has been

204
00:23:00,160 --> 00:23:09,520
funding studies about AI ethics and consequences of AI and publishing guidelines about, you know,

205
00:23:09,520 --> 00:23:14,400
how you do it right to me and my time. So this is not a new thing for me. Like, I've been thinking

206
00:23:14,400 --> 00:23:19,360
about this for 10 years, essentially. Whereas for Yosha and Jeff, it's much more recent.

207
00:23:20,160 --> 00:23:29,360
Yeah. But nonetheless, this augmented AI or augmented language models that have stronger

208
00:23:29,360 --> 00:23:39,200
reasoning and agency raises the threat, regardless of whether or not it can be countered

209
00:23:39,920 --> 00:23:47,600
to a higher level. Right. Okay. So I guess the question there becomes, what is the blueprint

210
00:23:48,240 --> 00:23:54,640
of future AI systems that will be capable of reasoning and planning, will understand how the

211
00:23:54,640 --> 00:24:00,800
world works, will be able to, you know, use tools and have agency and things like that. Right.

212
00:24:02,320 --> 00:24:09,040
And I tell you, they will not be autoregressive LLMs. So the problems that we see at the moment

213
00:24:09,920 --> 00:24:15,840
of autoregressive LLM, the fact that they hallucinate, they sometimes say really stupid

214
00:24:15,840 --> 00:24:20,640
things. They don't really have a good understanding of the world. People claim that they have some

215
00:24:20,640 --> 00:24:25,440
simple word model, but it's very implicit and it's really not good at all. Like, for example,

216
00:24:26,640 --> 00:24:32,640
you know, you can tell an LLM that A is the same as B. And then you ask if B is the same as A,

217
00:24:32,640 --> 00:24:38,000
and it will say, I don't know, or no. Right. I mean, those things don't really understand

218
00:24:38,000 --> 00:24:46,160
logic or anything like that. Right. So the type of system that we're talking about that might be,

219
00:24:47,040 --> 00:24:52,880
that might approach any more level intelligence and let alone human level intelligence have not

220
00:24:52,880 --> 00:25:00,880
been designed. They don't exist. And so discussing their danger and their potential harm is a bit

221
00:25:00,880 --> 00:25:06,240
like, you know, discussing the sex of angels at the moment, or to be a little more

222
00:25:06,640 --> 00:25:12,960
accurate, perhaps, it would be kind of like discussing how we're going to make transatlantic

223
00:25:12,960 --> 00:25:20,640
flight at near the speed of sound safe when we haven't yet invented the turbojet in 1925.

224
00:25:21,200 --> 00:25:28,400
Yeah. Yeah. Like, you know, we can speculate, but you know, how do we, how did we make turbojet

225
00:25:28,400 --> 00:25:34,640
safe? It required decades of really careful engineering to make them incredibly reliable.

226
00:25:34,640 --> 00:25:41,120
And, you know, now we can, you know, run like halfway around the world with the two-engine

227
00:25:43,360 --> 00:25:50,960
turbojet aircraft. I mean, that's an incredible feat. And it's not like people were discussing

228
00:25:50,960 --> 00:25:55,200
sort of philosophical questions about how you make turbojet safe. It's just really careful and

229
00:25:55,200 --> 00:26:04,080
complicated engineering that no one, none of us would understand. So, you know,

230
00:26:04,320 --> 00:26:11,040
how could we ask the AI community now to explain how AI systems are going to be safe? We haven't

231
00:26:11,040 --> 00:26:19,040
invented them yet. No. Okay. That said, I have some idea about how we can design them so that

232
00:26:19,040 --> 00:26:24,800
they have these capabilities. And as a consequence, how they will be safe, I call this objective

233
00:26:24,800 --> 00:26:35,360
driven AI. So what that means is essentially systems that produce their answer by planning

234
00:26:35,360 --> 00:26:41,120
their answer so as to satisfy an objective or a set of objectives. So this is very different

235
00:26:41,120 --> 00:26:45,760
from current LLNs. Current LLNs produce one word after the other or one token, which is,

236
00:26:45,760 --> 00:26:50,800
which has a board unit, doesn't matter, right? They don't really think and plan ahead as we,

237
00:26:50,800 --> 00:26:54,480
as we said before. They just produce one word after the other. That's not controllable.

238
00:26:55,840 --> 00:27:00,640
The only thing we can do is see if what they've produced, like check if what they've produced

239
00:27:01,520 --> 00:27:07,200
satisfies some criterion or set of criteria and then not produce an answer or produce a

240
00:27:07,200 --> 00:27:14,640
non-answer if the answer that was produced isn't appropriate. But we can't really force them to

241
00:27:15,360 --> 00:27:22,800
produce an answer that satisfies a set of objectives. So objective driven AI is the other way,

242
00:27:22,800 --> 00:27:29,920
is the opposite. The only thing that the system can produce are answers that satisfy a certain

243
00:27:29,920 --> 00:27:35,520
number of objectives. So what objective would be? Did you answer the question? Another objective could

244
00:27:35,520 --> 00:27:40,800
be, is your answer understandable by a 13 year old because you're talking to a 13 year old?

245
00:27:41,440 --> 00:27:48,720
Another would be, is this, I don't know, terrorist propaganda or something? You know,

246
00:27:48,720 --> 00:27:53,360
you can have a number of criteria like this, guardrails that would guarantee that the answer

247
00:27:53,360 --> 00:27:59,680
that's produced is satisfy certain criteria, whatever they are. Okay. Same for a robot,

248
00:27:59,680 --> 00:28:03,520
you could guarantee that the sequence of actions that is produced will not hurt anyone.

249
00:28:04,080 --> 00:28:07,600
Like you can have very low level, you know, guardrails of this type that say,

250
00:28:07,920 --> 00:28:13,440
okay, you have, you know, humans nearby and you're cooking, so you have a big knife in your hand,

251
00:28:13,440 --> 00:28:19,440
don't flare your arms. Okay, that would be a very simple guardrails to impose. And you can imagine

252
00:28:19,440 --> 00:28:23,520
having a whole bunch of guardrails like this that will guarantee that the behavior of those systems

253
00:28:24,160 --> 00:28:32,880
would be safe and that their primary goal would be to be basically subservient to us, right? So I

254
00:28:32,880 --> 00:28:40,320
do not believe that we'll have AI systems that can work that will not be subservient to us,

255
00:28:41,040 --> 00:28:44,640
will define their own goals, they will define their own sub goals, but those sub goals would

256
00:28:44,640 --> 00:28:50,960
be sub goals or goals that we set them and will not have all kinds of guardrails that will

257
00:28:50,960 --> 00:28:55,600
guarantee the safety. And we're not going to, it's not like we're going to invent a system

258
00:28:55,600 --> 00:29:00,080
and make a gigantic one that we know will have human level AI and just turning on and then from

259
00:29:00,640 --> 00:29:03,440
the next minute is going to take over the world. That's completely preposterous.

260
00:29:03,920 --> 00:29:08,960
What we're going to do is try with small ones, you know, maybe as smart as a mouse or something,

261
00:29:08,960 --> 00:29:13,200
maybe a dog, maybe a cat, maybe a dog, maybe and work our way up and then, you know,

262
00:29:13,200 --> 00:29:19,120
put some more guardrails. Basically, like we've engineered, you know, more and more powerful

263
00:29:19,120 --> 00:29:27,360
and more reliable turbojets. It's an engineering problem. Yeah, yeah. You were also on a paper,

264
00:29:27,360 --> 00:29:32,640
maybe this is the one that talked about the embodied Turing test on neuro AI.

265
00:29:34,800 --> 00:29:45,440
Can you explain what the neuro AI is? Okay. Well, it's the idea that we should get some

266
00:29:45,440 --> 00:29:53,280
inspiration from neuroscience to build AI systems and that there is something to be learned from

267
00:29:53,280 --> 00:30:02,320
neuroscience and from cognitive science to drive the design of AI systems, some inspiration.

268
00:30:02,960 --> 00:30:08,880
Okay. Something to be learned as well as the other way around. So what's interesting right now is

269
00:30:08,880 --> 00:30:15,840
that the best models that we have of how, for example, the visual cortex works is convolutional

270
00:30:15,840 --> 00:30:20,800
neural networks, which are also the models that we use to recognize images primarily

271
00:30:20,800 --> 00:30:26,800
in artificial systems. So there is kind of information kind of being exchanged both ways.

272
00:30:27,520 --> 00:30:35,760
There's one, you know, one way to make progress in AI is to kind of ignore nature and just,

273
00:30:35,760 --> 00:30:42,240
you know, kind of try to solve problems in a sort of engineering fashion, if you want.

274
00:30:43,840 --> 00:30:50,560
I found interaction with neuroscience always thought provoking. So you don't want to be

275
00:30:50,640 --> 00:30:54,960
copying nature very too closely because there are details in nature that are irrelevant.

276
00:30:55,840 --> 00:31:00,720
And there are principles on which, you know, natural intelligence is based that we haven't

277
00:31:00,720 --> 00:31:06,400
discovered. So, but there is some inspiration to have certainly in your convolutional net for

278
00:31:06,400 --> 00:31:11,600
inspired by the architecture of the visual cortex. The whole idea of neural net and deep learning

279
00:31:11,600 --> 00:31:17,200
came out of the idea that, you know, intelligence can emerge from a large collection of simple

280
00:31:17,200 --> 00:31:22,080
elements that are connected with each other and change the nature of their interactions.

281
00:31:22,080 --> 00:31:28,800
That's the whole idea, right? So, so inspiration from neuroscience certainly has been extremely

282
00:31:29,680 --> 00:31:34,800
beneficial so far. And the idea of neural AI is that you should keep going. You don't want to go

283
00:31:34,800 --> 00:31:42,240
too far. So going too far, for example, is trying to reproduce the some aspect of the functioning

284
00:31:42,240 --> 00:31:49,760
of neurons with electronics. I'm not sure that's a good idea. I'm skeptical about this, for example.

285
00:31:51,840 --> 00:31:57,760
So your research right now, are you, your main focus is on

286
00:31:59,120 --> 00:32:05,440
furthering the JEPA architecture into other modalities or where are you headed?

287
00:32:06,400 --> 00:32:14,000
Yeah. So, I mean, the long term goal is, you know, to get machines to be as intelligent and learn

288
00:32:14,000 --> 00:32:19,520
as efficiently as animals and humans. Okay. And the reason for this is that we need this because

289
00:32:19,520 --> 00:32:25,760
we need to amplify human intelligence. And so intelligence is the most needed commodity that

290
00:32:25,760 --> 00:32:33,120
we want in the world, right? And so we could, you know, possibly bring a new renaissance to humanity

291
00:32:33,120 --> 00:32:38,480
if we could amplify human intelligence using machines, which we are doing already with computers,

292
00:32:38,480 --> 00:32:43,200
right? I mean, that's pretty much what they've been designed to do. But even more, you know,

293
00:32:43,200 --> 00:32:52,880
imagine a future where every one of us has an intelligent assistant with us at all times.

294
00:32:53,520 --> 00:32:57,600
They can be smarter than us. You shouldn't feel threatened by that. We should feel

295
00:32:58,560 --> 00:33:05,840
like we are like, you know, a director of a big lab or a CEO of a company that has a staff working

296
00:33:05,840 --> 00:33:10,240
for them of people who are smarter than themselves. I mean, we're used to this already. I'm used to

297
00:33:10,240 --> 00:33:15,360
this certainly working with people who are smarter than me. So we shouldn't feel threatened by this,

298
00:33:15,360 --> 00:33:23,520
but it's going to empower a lot of us, right, and humanity as a whole. So I think that's a good

299
00:33:23,600 --> 00:33:27,920
thing. That's the overall practical goal, if you want, right? Then there's a scientific

300
00:33:27,920 --> 00:33:31,840
question that's behind this, which is really what is intelligence and how you build it.

301
00:33:32,960 --> 00:33:38,240
And then which is, you know, how can system learn the way animals and humans seem to be

302
00:33:38,240 --> 00:33:45,440
learning so efficiently? And the next thing is, how do we learn how the world works by observation,

303
00:33:45,440 --> 00:33:52,080
by watching the world go by through vision and all the other senses? And animals can do this

304
00:33:52,080 --> 00:33:59,360
without language, right? So it has nothing to do with language, has to do with learning from sensory

305
00:33:59,360 --> 00:34:04,960
perceives and learning mostly without acting, because any action you take can kill you. So

306
00:34:04,960 --> 00:34:09,200
it's better to be able to learn as much as you can without actually acting at all, just observing,

307
00:34:10,000 --> 00:34:14,400
which is what babies do in the first few months of life. They can't hardly do anything, right? So

308
00:34:14,400 --> 00:34:19,600
they mostly observe and learn how the world works by observation. So what kind of learning takes

309
00:34:19,680 --> 00:34:24,880
place there? So that's obviously kind of self-supervised, right? It's learning by prediction. That's an

310
00:34:24,880 --> 00:34:32,080
whole idea from cognitive science. And the thing is, you know, we can learn to predict videos,

311
00:34:32,080 --> 00:34:36,720
but then we notice that predicting videos, predicting pixels in video, is so initially

312
00:34:36,720 --> 00:34:43,360
complicated that it doesn't work. And so then came this idea of JEPA, right? Learn representations

313
00:34:43,360 --> 00:34:48,080
so that you can make predictions in representation space. And that turned out to work really well

314
00:34:48,080 --> 00:34:53,600
for learning image features. And now we're working on getting this to work for video. And

315
00:34:53,600 --> 00:35:00,000
eventually, we'll be able to use this to learn to learn world models, where you show a piece of

316
00:35:00,000 --> 00:35:04,960
video, and then you say, I'm going to take this action, predict what's going to happen next in the

317
00:35:04,960 --> 00:35:13,520
world. And, you know, which is a bit where the Gaia system from Wave is doing at a high level,

318
00:35:13,520 --> 00:35:17,440
but we need this at sort of various levels of abstraction, so that we can build,

319
00:35:19,200 --> 00:35:24,640
you know, systems that are more general than autonomous driving. Okay. That's the...

320
00:35:24,640 --> 00:35:39,360
Yeah. And it's my fault, so I won't go over the hour. But is it conceivable that someday there'll be

321
00:35:40,240 --> 00:35:53,280
a model that you may be embodied in a robot that is ingesting video from its environment

322
00:35:53,280 --> 00:36:00,880
and learning as it's just continuously learning and getting smarter and smarter and smarter?

323
00:36:01,600 --> 00:36:09,280
Yeah. I mean, that's kind of a bit of a necessity. The reason being that, you know,

324
00:36:09,280 --> 00:36:12,560
even if you train a system to have a world model that can predict what's going to happen next,

325
00:36:13,440 --> 00:36:17,840
the world is really complicated. And there's probably all kinds of situations that you,

326
00:36:17,840 --> 00:36:23,280
you know, the system hasn't been trained on and need to, you know, fine tune itself as it goes.

327
00:36:24,160 --> 00:36:33,680
So, you know, animals and humans do this early in life by playing. So play is a way of

328
00:36:34,720 --> 00:36:40,000
learning your world model in situations that basically you won't hurt you.

329
00:36:41,920 --> 00:36:47,520
And, but then during life, of course, you know, when we don't drive, there's all kinds of these

330
00:36:47,520 --> 00:36:53,040
mistakes that we do initially that we don't do after having some experience. And that's because

331
00:36:53,040 --> 00:36:59,280
we're fine tuning our world model to some extent. We're learning a new task. We're basically just

332
00:36:59,280 --> 00:37:05,680
learning a new version of our world model. Right. So, so yeah, I mean, this type of continuous,

333
00:37:05,680 --> 00:37:10,960
continual learning is going to have to be present. But the overall power and the

334
00:37:10,960 --> 00:37:15,520
intelligence of the system will be limited by, you know, how much a co-governor on that is using

335
00:37:15,520 --> 00:37:19,280
and various other constraints, you know, computational constraints, basically.

336
00:37:19,600 --> 00:37:26,000
And, you know, you're still young. And, and this not sure about that.

337
00:37:26,640 --> 00:37:30,400
Well, you're younger than Jeff. Let me put it that way.

338
00:37:35,040 --> 00:37:42,080
But this, the progress you've made on world models is, is fairly rapid from my point of

339
00:37:42,160 --> 00:37:51,360
view, watching it. Are you, are you hopeful that within your career, you'll have

340
00:37:52,720 --> 00:37:59,040
embodied robots that are, are building world models through their interaction in reality,

341
00:37:59,040 --> 00:38:03,120
and, and then being able to, well, I guess the other question on world models,

342
00:38:04,080 --> 00:38:13,440
do you then combine it with a language model to do reasoning or, or is the world model able to,

343
00:38:13,440 --> 00:38:18,640
to do reasoning on its own? But are you hopeful that in your career, you'll, you'll get to the

344
00:38:18,640 --> 00:38:25,120
point where you'll have this continuous learning in a world model? Yeah, I sure hope so. I might have

345
00:38:25,120 --> 00:38:31,440
another, you know, 10, 10 useful years or something like this in research before my brain, you know,

346
00:38:31,440 --> 00:38:38,560
turns into dish and male sauce, but, or something like that, you know, 15 years if I'm lucky.

347
00:38:40,000 --> 00:38:46,160
So, or perhaps less. But yeah, I hope that there's going to be breakthroughs in that direction

348
00:38:46,160 --> 00:38:53,600
during that time. Now, whether that will result in the kind of artifact that you're describing,

349
00:38:53,600 --> 00:38:57,760
you know, robots that can, like, you know, domestic robots, for example, or,

350
00:38:57,760 --> 00:39:01,520
or sort of in cars that are, they can run fairly quickly by themselves.

351
00:39:02,880 --> 00:39:09,840
I don't know, because there might be all kinds of obstacles that we have not envisaged that may

352
00:39:10,480 --> 00:39:17,440
appear on the way. You know, that's, it's a constant in the history of AI that you have some new idea

353
00:39:18,160 --> 00:39:21,440
and a breakthrough, and you think that's going to solve all the world's problems.

354
00:39:22,240 --> 00:39:27,200
And then you're going to hit limitation, and you have to go beyond that limitation. So it's like,

355
00:39:27,280 --> 00:39:31,760
you know, you're climbing a mountain, you find a way to climb the mountain that you're seeing.

356
00:39:32,560 --> 00:39:38,480
And you know that once you get to the top, you will have the problem solved because now it's,

357
00:39:38,480 --> 00:39:43,760
you know, the gentle slope down. And once you get to the top, you realize that there is

358
00:39:43,760 --> 00:39:50,080
another mountain behind it that you hadn't seen. So that's, that's, that's been the history of AI,

359
00:39:50,080 --> 00:39:55,760
right, where people have come up with sort of new concepts, new ideas, new way to approach

360
00:39:57,680 --> 00:40:04,640
AI reasoning, whatever, perception, and then realize that their idea basically was very limited.

361
00:40:06,400 --> 00:40:14,960
And so, so, you know, this, inevitably, we're trying to figure out what's the next

362
00:40:16,160 --> 00:40:20,960
revolution in AI. That's what I'm trying to figure out. So, you know, learning how the world works

363
00:40:20,960 --> 00:40:25,200
from video, having systems that have world model allows systems to reason and plan.

364
00:40:27,520 --> 00:40:34,960
And there's something I want to be very clear about, which is an answer to your question,

365
00:40:36,400 --> 00:40:41,840
which is that you can have systems that reason and plan without manipulating language. Animals are

366
00:40:41,840 --> 00:40:49,280
capable of amazing feats of planning and also to some extent reasoning. They don't have language,

367
00:40:49,360 --> 00:40:56,960
at least most of them don't. And so, many of them don't have culture because they are mostly

368
00:40:56,960 --> 00:41:05,520
solitary animals. So, you know, it's only the animals that have some level of culture. So,

369
00:41:08,000 --> 00:41:14,480
so the idea that a system can plan and reason is not connected with the idea that you can

370
00:41:14,480 --> 00:41:19,840
manipulate language. Those are two different things. It needs to be able to manipulate abstract

371
00:41:20,640 --> 00:41:25,840
notions. But those notions do not necessarily correspond to linguistic entities like words

372
00:41:25,840 --> 00:41:31,040
or things like that. We can have mental images if you want to things. Like you do

373
00:41:32,080 --> 00:41:36,240
ask a physicist or a mathematician, you know, how they reason is very much in terms of sort of

374
00:41:36,240 --> 00:41:39,920
mental models. I have nothing to do with language. Then you can turn things into

375
00:41:40,240 --> 00:41:44,880
language. But that's a different story. That's the second step. So,

376
00:41:48,640 --> 00:41:52,960
so, you know, we're going to have to figure out how to do this reasoning,

377
00:41:52,960 --> 00:41:59,760
hierarchical planning in machines, reproduce this first. And then, of course, you know,

378
00:41:59,760 --> 00:42:04,000
sticking language on top of it will help. Like, we'll make those systems smarter and be able,

379
00:42:04,000 --> 00:42:07,600
you know, we will allow us to communicate with them and teach them things. And they're going to

380
00:42:07,600 --> 00:42:12,560
be able to teach us things and stuff like that. But this is a different question, really.

381
00:42:13,120 --> 00:42:18,560
The question of how we organize AI research going forward, which is somewhat determined by how

382
00:42:18,560 --> 00:42:25,440
afraid people are of the consequences of AI. So, if you have a rather positive view of the impact

383
00:42:25,440 --> 00:42:30,160
of AI on society, and you trust humanity and society and democracies to use it in good ways,

384
00:42:30,960 --> 00:42:38,080
then the best way to make progress is to open research. And for the people who are

385
00:42:38,080 --> 00:42:42,320
afraid of the consequences, whether they are societal or geopolitical,

386
00:42:44,000 --> 00:42:50,240
they're putting pressure on governments around the world to regulate AI in ways that basically limit

387
00:42:51,920 --> 00:42:57,600
access, particularly of open source code and things like that. And it's a big debate at the

388
00:42:57,600 --> 00:43:02,320
moment. I'm very much on the side. So, he's met up very much on the side of open research.

389
00:43:02,960 --> 00:43:08,560
Yeah, actually, that was something I was going to ask you. And now that you've brought it up.

390
00:43:09,440 --> 00:43:17,120
Because there, I've been talking to people about this. And there is a view that aside from the

391
00:43:17,120 --> 00:43:24,160
risks of open source, you know, again, Jeff Hinton saying, you know, would you open source

392
00:43:24,160 --> 00:43:33,840
thermonuclear weapons? Aside from that is the question of as to whether open source can marshal

393
00:43:33,840 --> 00:43:45,120
the resources to compete with proprietary models. And because of the tremendous resources required

394
00:43:45,120 --> 00:43:51,360
for when you're scaling these models. And there's a question as to whether or not

395
00:43:51,360 --> 00:44:00,160
Meta will continue to open source future versions of Lama or not continue to open source, but whether

396
00:44:00,160 --> 00:44:08,160
it'll continue to invest the resources needed to push the open source models.

397
00:44:09,120 --> 00:44:13,840
So what do you think about that? Okay, there's a lot to say about this. Okay, so first thing is,

398
00:44:14,720 --> 00:44:18,720
there's no question that Meta will continue to invest the resources to build

399
00:44:19,520 --> 00:44:24,800
better and better AI systems, because it needs it for its own products. So the resources will

400
00:44:24,800 --> 00:44:31,120
be invested. Now, the next question is, do you, you know, will we continue to open source the

401
00:44:31,120 --> 00:44:37,760
base models? And the answer is, you know, probably yes, because that creates an ecosystem on top of

402
00:44:37,760 --> 00:44:43,120
which an entire industry can be built. And there is no point, you know, having 50 different companies

403
00:44:43,440 --> 00:44:51,840
building proprietary close systems when you can have, you know, one good base open source base

404
00:44:51,840 --> 00:44:59,680
model that everybody can use. It's wasteful. And it's not a good idea. And another reason for

405
00:44:59,680 --> 00:45:08,160
having open source models is that it, it nobody has no entity as powerful as it thinks it is,

406
00:45:08,240 --> 00:45:14,080
as a monopoly on good ideas. And so if you want people who can have good new innovative ideas

407
00:45:14,080 --> 00:45:18,960
to contribute, you need an open source platform. If you want the academic world to contribute,

408
00:45:18,960 --> 00:45:22,560
you need open source platforms. If you want the startup world to be able to build

409
00:45:22,560 --> 00:45:27,680
customized products, you need open source base models, because they don't have the resources to

410
00:45:27,680 --> 00:45:34,480
build to train large models, right? Okay, and then there is the history that shows that for,

411
00:45:35,040 --> 00:45:43,760
for foundational technology, for infrastructure type technology, open source always wins,

412
00:45:46,640 --> 00:45:53,440
right? It's true of the software infrastructure of the internet. In the early 90s and mid 90s,

413
00:45:53,440 --> 00:45:58,560
there was a big battle between sun macro systems and Microsoft to produce the, deliver the

414
00:45:59,520 --> 00:46:03,920
software infrastructure of the internet, you know, operating systems, web servers,

415
00:46:05,360 --> 00:46:10,000
web browsers, and, and, you know, various servers aside and client-side frameworks, right?

416
00:46:10,000 --> 00:46:15,200
They're both lost. Nobody is talking about them anymore. The entire world is,

417
00:46:16,960 --> 00:46:26,560
of the web is using Linux and Apache and MySQL and JavaScript and, and, you know, and even the,

418
00:46:26,800 --> 00:46:31,600
the basic core code for, for web browser is open source. So,

419
00:46:33,440 --> 00:46:40,320
open source won by a huge margin. Why? Because it's safer, gathers more people to contribute.

420
00:46:40,320 --> 00:46:43,360
All the features are unnecessary. It's more reliable.

421
00:46:45,760 --> 00:46:52,960
Venerabilities are fixed faster. And, and it's customizable. So anybody can customize Linux

422
00:46:52,960 --> 00:47:00,400
to run on whatever hardware they want, right? So open source wins. And the same, same for AI.

423
00:47:00,400 --> 00:47:05,280
It's going to be the same thing. It's inevitable. The, the people now who are climbing up,

424
00:47:06,320 --> 00:47:15,200
like open AI, their, their system is based on publications from all of us. Sure. And from

425
00:47:16,160 --> 00:47:18,800
open platforms like, like PyTorch. Yeah.

426
00:47:18,800 --> 00:47:23,280
Judgeability is built using PyTorch. PyTorch was produced originally by Meta. Now it's owned by

427
00:47:23,280 --> 00:47:29,520
the Linux Foundation. It's open source. They've contributed to it, by the way. You know, their

428
00:47:29,520 --> 00:47:36,160
LLM is based on transformer architectures invented at Google. Yeah. All the tricks to kind of train

429
00:47:36,160 --> 00:47:41,280
all those things came out of like various papers from all kinds of different institutions,

430
00:47:41,280 --> 00:47:47,760
including academia, all the fine-tuning techniques, same. So nobody works in a vacuum.

431
00:47:47,760 --> 00:47:52,560
The thing is, nobody can keep their advance and their advantage

432
00:47:54,160 --> 00:48:00,320
for very long if they are secretive. Yeah. Except that with these models, because they're

433
00:48:00,320 --> 00:48:07,200
so compute intensive and they cost so much money to train, you need somebody like Meta that who's,

434
00:48:07,200 --> 00:48:13,440
who's going to be willing to build them and open source them. And that's why I was, when I was

435
00:48:13,440 --> 00:48:23,520
asking whether they'll continue, obviously Meta will continue building, you know, resource-intensive

436
00:48:23,520 --> 00:48:30,000
models. But the question is whether they'll continue to open source. I mean, if- I'm telling you,

437
00:48:30,000 --> 00:48:37,040
I'm telling you the only reason why Meta could stop open sourcing models are legal.

438
00:48:38,000 --> 00:48:43,520
So if there is a law that adds laws, open source AI systems above a certain level of

439
00:48:44,800 --> 00:48:53,200
sophistication, then of course we can do it. If there are laws that in the US or across the world

440
00:48:55,520 --> 00:49:02,400
makes it illegal to use public content to train AI systems, then it's the end of AI

441
00:49:02,480 --> 00:49:08,560
for everybody, not just for the open source. Okay. So, or at least the end of the type of AI

442
00:49:08,560 --> 00:49:12,400
that we are talking about today might have, you know, new AI in the future, but that don't

443
00:49:12,400 --> 00:49:21,360
require as much data. So, and then there is, you know, liability. If you, if you, if you, if you

444
00:49:21,360 --> 00:49:28,960
believe in the kind of that someone doing something bad with an AI system that was open sourced by

445
00:49:29,680 --> 00:49:35,920
by Meta, then Meta is liable, then Meta will have a big incentive not to release it, obviously.

446
00:49:36,720 --> 00:49:41,840
So it's the entire question about this is around legal reasons and political decisions.

447
00:49:41,840 --> 00:49:48,320
But on the idea of open source winning, don't you need more people or more companies like Meta

448
00:49:48,320 --> 00:49:54,400
building the foundation models and open sourcing them? Or could it be, could an open source

449
00:49:54,960 --> 00:50:01,280
ecosystem win based on a single company building the models? No, I mean, you need two or three.

450
00:50:02,000 --> 00:50:06,960
And there are two or three, right? I mean, there is this hugging face. There is Mistral in France,

451
00:50:06,960 --> 00:50:13,040
who's also embracing sort of an open source LLM. They're very good LLM. It's a small one, but it's

452
00:50:13,040 --> 00:50:21,680
very good. There is, you know, academic efforts like Lyon. They don't have all the resources they

453
00:50:21,680 --> 00:50:26,640
need, but they, you know, they collect the data that is used by everyone. So everybody can contribute.

454
00:50:26,640 --> 00:50:31,440
One thing that I think is really important to understand also is that there is a future in

455
00:50:31,440 --> 00:50:37,520
which I described earlier in which every one of us, every one of our interactions with the digital

456
00:50:37,520 --> 00:50:44,000
world will be mediated by an AI assistant. And this is going to be for true for everyone around

457
00:50:44,000 --> 00:50:49,280
the world, right? Everyone who has any kind of smart device. Eventually it's going to be in our,

458
00:50:49,280 --> 00:50:53,440
you know, augmented reality glasses, but, you know, for the time being in our smartphones, right?

459
00:50:55,200 --> 00:51:02,240
And so imagine that future where, you know, you are, I don't know, from

460
00:51:04,400 --> 00:51:13,840
Indonesia or Senegal or France. And your entire digital diet is done through the

461
00:51:14,560 --> 00:51:21,840
mediation of an AI system. Your government is not going to be happy about it. Your government

462
00:51:22,640 --> 00:51:27,440
is going to want the local culture to be present in that system. It doesn't want that system to be

463
00:51:27,440 --> 00:51:36,880
closed sourced and controlled by a company on the west coast of the US. So just for reasons of

464
00:51:37,200 --> 00:51:45,360
preserving the diversity of culture across the world and not having or entire information

465
00:51:45,360 --> 00:51:49,840
diet being biased by whatever it is that some company on the west coast of the US states,

466
00:51:51,360 --> 00:51:57,360
there's going to need to be open source platforms. And they're going to be predominant

467
00:51:58,240 --> 00:52:04,400
in at least outside the US for that reason. Including China, right? There is all those

468
00:52:04,400 --> 00:52:09,120
talks about, oh, what if China puts their hands on our open source code? I mean, China wants control

469
00:52:09,120 --> 00:52:15,760
over its own LLM because they don't want their citizen to, you know, have access to certain type

470
00:52:15,760 --> 00:52:20,480
of information. So they're not going to use our LLMs. They're going to trend theirs that they already

471
00:52:20,480 --> 00:52:26,640
have. And nobody is, you know, particularly ahead of anybody else by more than about a year.

472
00:52:27,520 --> 00:52:35,120
Yeah. And China is pushing open source. I mean, they're very pro open source within their

473
00:52:35,120 --> 00:52:40,960
ecosystems. Some of them, you know, it's there's no like unified opinion there. But

474
00:52:43,040 --> 00:52:46,880
I mean, it's the same in in the West, right? There are some some governments that are too

475
00:52:46,880 --> 00:52:53,520
afraid of the risks. And then or are thinking about it and some others that are all for open

476
00:52:53,520 --> 00:52:59,440
source because they see this as the only way for them to have any influence on the

477
00:53:02,160 --> 00:53:08,240
type of information and culture that would be mediated by those systems. So it's going to have

478
00:53:08,240 --> 00:53:17,120
to be like Wikipedia, right? Wikipedia, you know, is built by millions of people who contribute to

479
00:53:17,120 --> 00:53:21,280
or from all around the world in all kinds of languages. Okay. And it has a system for sort

480
00:53:21,360 --> 00:53:26,720
of vetting the information. The way AI systems of the future will be taught and we'll be fine

481
00:53:26,720 --> 00:53:33,360
tuned will have to be the same way will have to be quite sourced. Because something that matters to

482
00:53:34,720 --> 00:53:41,600
a farmer in southern India is probably not going to be taken into account by the fine

483
00:53:41,600 --> 00:53:47,360
tuning done by, you know, some some company on the west coast of the US. AI might be the most

484
00:53:47,360 --> 00:53:53,440
important new computer technology ever. It's storming every industry and literally billions

485
00:53:53,440 --> 00:53:59,920
of dollars are being invested. So buckle up. The problem is that AI needs a lot of speed and

486
00:53:59,920 --> 00:54:07,280
processing power. So how do you compete without cost spiraling out of control? It's time to upgrade

487
00:54:07,280 --> 00:54:16,080
to the next generation of the cloud oracle cloud infrastructure or OCI. OCI is a single platform

488
00:54:16,080 --> 00:54:23,920
for your infrastructure, database, application, development and AI needs. OCI has four to eight

489
00:54:23,920 --> 00:54:30,960
times the bandwidth of other clouds, offers one consistent price instead of variable regional

490
00:54:30,960 --> 00:54:38,240
pricing. And of course, nobody does data better than oracle. So now you can train your AI models

491
00:54:38,240 --> 00:54:44,960
at twice the speed and less than half the cost of other clouds. If you want to do more and spend

492
00:54:44,960 --> 00:54:54,640
less like Uber, eight by eight and Databricks Mosaic, take a free test drive of OCI at oracle.com

493
00:54:55,200 --> 00:55:07,040
slash I on AI. That's E Y E O N A I all run together oracle.com slash I on AI. That's it for this

494
00:55:07,040 --> 00:55:13,920
episode. I want to thank Yen for his time. If you want to read a transcript of this conversation,

495
00:55:13,920 --> 00:55:23,520
you can find one on our website I on AI. That's E Y E hyphen O N dot AI. And remember the singularity

496
00:55:24,320 --> 00:55:36,480
may not be near, but AI is changing your world. So best pay attention.

