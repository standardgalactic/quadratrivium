WEBVTT

00:00.000 --> 00:03.200
Even if you train a system to have a world model that can predict what's going to happen next,

00:03.200 --> 00:07.520
the world is really complicated and there's probably all kinds of situations that the system

00:07.520 --> 00:13.440
hasn't been trained on and need to, you know, fine-tune itself as it goes. The question of how

00:13.440 --> 00:18.720
we organize AI research going forward which is somewhat determined by how afraid people are

00:18.720 --> 00:24.320
of the consequences of AI. So if you have a rather positive view of the impact of AI on society and

00:24.320 --> 00:28.880
you trust humanity and society and democracies to use it in good ways, then the best way to

00:28.880 --> 00:35.680
make progress is to open research. AI might be the most important new computer technology ever.

00:35.680 --> 00:41.760
It's storming every industry and literally billions of dollars are being invested. So buckle up.

00:42.400 --> 00:48.240
The problem is that AI needs a lot of speed and processing power. So how do you compete

00:48.240 --> 00:54.880
without cost spiraling out of control? It's time to upgrade to the next generation of the cloud,

00:55.520 --> 01:03.120
Oracle Cloud Infrastructure or OCI. OCI is a single platform for your infrastructure,

01:03.120 --> 01:11.120
database, application, development, and AI needs. OCI has four to eight times the bandwidth of other

01:11.120 --> 01:18.720
clouds, offers one consistent price instead of variable regional pricing, and of course nobody

01:18.720 --> 01:25.920
does data better than Oracle. So now you can train your AI models at twice the speed and less than

01:25.920 --> 01:33.360
half the cost of other clouds. If you want to do more and spend less like Uber, eight by eight,

01:33.920 --> 01:46.000
and Databricks Mosaic, take a free test drive of OCI at oracle.com slash ion AI. That's E-Y-E-O-N-A-I

01:46.000 --> 01:57.040
all run together oracle.com slash ion AI. Hi, I'm Craig Smith and this is ION AI. In this episode,

01:57.040 --> 02:04.480
I speak again with Yan Lacoon, one of the founders of deep learning and someone who followers of AI

02:04.480 --> 02:11.680
should need no introduction to. Yan talks about his work on developing world models on why he

02:11.680 --> 02:19.280
does not believe AI research poses a threat to humanity and why he thinks open source AI models

02:19.280 --> 02:26.720
are the future. In the course of the conversation, we talk about a new model, Gaia 1, developed by a

02:26.720 --> 02:35.040
company called Wave AI. I'll have an episode with Wave's founder to further explore that world model

02:35.040 --> 02:40.960
which has produced some startling results. I hope you find the conversation with Yan

02:40.960 --> 02:48.480
as enlightening as I did. First, the notion of world model is the idea that the system would get

02:48.480 --> 02:54.960
some idea of the state of the world and be able to predict sort of following states of the world

02:54.960 --> 02:58.960
resulting from just the natural evolution of the world or resulting from an action that the

02:58.960 --> 03:05.440
agent might take. If you have an idea of the state of the world and you imagine an action that

03:05.440 --> 03:11.920
you're going to take and you can predict the resulting state of the world, that means you

03:11.920 --> 03:15.440
can predict what's going to happen as a consequence of a sequence of actions and that means you can

03:15.440 --> 03:21.760
plan a sequence of actions to arrive at a particular goal. That's really what a world model is. At

03:21.760 --> 03:29.680
least that's what the Wave people have understood the word in other contexts, like in the context of

03:29.680 --> 03:36.080
optimal control and robotics and things like that. That's what a world model is. Now, there's several

03:36.080 --> 03:43.520
levels of complexity of those world models, whether they model yourself, the agent, or whether they

03:43.520 --> 03:52.880
model the external world, which is much more complicated. Training a world model basically

03:52.880 --> 03:58.240
consists in just observing the world go by and then learning to predict what's going to happen next,

03:58.960 --> 04:05.120
or observing the world taking an action and then observing the resulting effect, an action that

04:05.120 --> 04:11.920
you take as an agent or an action that you see other agents taking. That establishes

04:12.160 --> 04:21.920
causality essentially. You could think of this as a causal model. Those models don't need to predict

04:21.920 --> 04:27.120
all the details about the world. They don't need to be generative. They don't need to predict exactly

04:27.120 --> 04:34.400
every pixel in a video, for example, because what you need to be able to predict is enough details,

04:35.280 --> 04:47.200
some sort of abstract representation to allow you to plan. You're assembling something out of wood

04:47.200 --> 04:54.320
and you're going to put two planks together and attach them with screws. It doesn't matter the

04:54.320 --> 05:00.480
details of which type of screwdriver you're using or the size of the screw within some limits and

05:00.480 --> 05:05.440
things like that. There are details that in the end don't matter as to what the end result

05:05.440 --> 05:12.400
will be or the precise grain of the wood and things of that type. You need to have some

05:12.400 --> 05:16.800
abstract level of representation within which you can make the prediction without having to

05:16.800 --> 05:24.800
predict every detail. That's why those JPA architectures I've been advocating are useful. Models like

05:25.760 --> 05:31.840
the Gaia 1 model from Wave actually makes prediction in an abstract representation space.

05:31.840 --> 05:37.280
There's been a lot of work in that area for years also at FAIR, but generally the

05:37.920 --> 05:43.200
abstract representation were pre-trained. The encoders that would take images from videos and

05:43.200 --> 05:48.000
then encode them into some representation were trained in some other way. The progress we've

05:48.000 --> 05:53.840
made over the last six months in self-improvised learning for images and video is that now we

05:53.840 --> 06:01.120
can train the entire system to make those predictions simultaneously. We have systems now that can

06:02.400 --> 06:09.280
learn good representations of images and the basic idea is very simple. You take an image,

06:10.560 --> 06:17.520
you run it through an encoder, then you corrupt that image, you mask parts of it, for example,

06:18.080 --> 06:25.200
or you transform it in various ways, you blur it, you change the colors, you change the framing a

06:25.200 --> 06:30.000
little bit, and you run that corrupted image through the same encoder or something very similar.

06:31.600 --> 06:37.120
And then you train the encoder to predict the features of the complete image from the features

06:37.920 --> 06:45.520
of the corrupted one. You're not trying to reconstruct the perfect image,

06:46.400 --> 06:51.520
you're just trying to predict the representation of it. And this is different,

06:51.520 --> 06:56.800
this is not generative in the sense that it does not produce pixels. And that's the secret to getting

06:56.800 --> 07:02.080
self-supervisual into work in the context of images and video. You don't want to be predicting pixels,

07:02.080 --> 07:07.040
it doesn't work. You can't predict pixels as an afterthought, which is what the Gaia system is

07:07.040 --> 07:12.080
doing by sticking a decoder on it and with some diffusion model that will produce a nice image,

07:12.080 --> 07:17.280
but that's kind of a second step. If you train the system by predicting pixels,

07:18.000 --> 07:22.000
you just don't get good representations, you don't get good predictions, you get blurry

07:22.000 --> 07:28.480
predictions most of the time. So that's what makes learning from images and video fundamentally

07:28.480 --> 07:34.400
different from learning from text because in text you don't have that problem. It's easy to predict

07:34.400 --> 07:41.040
words, even if you cannot do a perfect prediction because language is discrete. So language is

07:41.040 --> 07:52.640
simple compared to the real world. And there's a lot written right now about the energy required

07:52.720 --> 08:00.960
in the computational resources GPUs required to train language models. Is it less

08:02.080 --> 08:08.480
in training a world model like using iJAPA architecture? Well, it's hard to tell because

08:08.480 --> 08:15.760
there is no equivalent training procedure, self-supervised training procedure for video,

08:15.760 --> 08:21.760
for example, that does not use JAPA. The ones that are generative don't really work.

08:22.880 --> 08:29.840
Yeah. Well, but this architecture could also be applied to language, couldn't it?

08:30.720 --> 08:37.520
Oh yeah, absolutely. Yeah, so you could very well use a JAPA architecture that makes prediction

08:37.520 --> 08:44.400
in representation space and apply to language. Yeah, definitely. And in that case, would it be

08:44.480 --> 08:52.000
less computationally intense than training a large language model?

08:52.000 --> 08:59.760
It's possible. It's not entirely clear either. I mean, there is some advantage regardless of what

08:59.760 --> 09:05.440
technique you're using to making those models really big. They just seem to work better if you

09:05.440 --> 09:14.480
make them big. So if you make them bigger. So scaling is useful. Contrary to some claims,

09:14.480 --> 09:19.680
I do not believe that scaling is sufficient. So in other words, we're not going to get anywhere

09:19.680 --> 09:30.320
close to human level AI. In fact, not even any more level AI by simply scaling up language models.

09:31.120 --> 09:36.320
Even multimodal language models that we applied to video, we're going to have to find new concepts,

09:36.320 --> 09:44.000
new architectures. And I've written a vision paper about this a while back of a different

09:44.000 --> 09:50.720
type of architecture that would be necessary for this. So scaling is necessary, but not sufficient.

09:51.680 --> 10:01.120
And we're missing some basic ingredients to get to human level AI. We're fooled by the fact that

10:01.120 --> 10:06.400
LLMs are fluent. And so we think that they have human level intelligence because they can manipulate

10:06.400 --> 10:16.320
language. But that's false. And in fact, there's a very good symptom for this, which is that

10:17.280 --> 10:24.960
we have systems that can pass the bar exam, but answering questions from text by basically

10:24.960 --> 10:32.880
regurgitating what they've learned, more or less by road. But we don't have completely autonomous

10:32.880 --> 10:39.360
level five cell driving cars, or at least no system that can learn to do this in about 20 hours

10:39.520 --> 10:46.400
of practice, just like any 17-year-old. And we certainly don't have any domestic robot that can

10:47.440 --> 10:51.200
clear up the dinner table and fill up the dishwasher attest that any 10-year-old can learn

10:51.200 --> 10:58.800
in one shot. So clearly, we're missing something big. And that something is an ability to learn

10:58.800 --> 11:03.360
how the world works and the world is much more complicated than language. And also being able

11:03.360 --> 11:10.720
to plan and reason, basically having a mental world model that allows to plan and predict

11:11.360 --> 11:17.120
consequences of actions. That's what we're missing. It takes a while before we figure this out.

11:18.160 --> 11:27.840
You were on another paper that talked about augmented language models. And

11:27.840 --> 11:33.680
in the embodied touring test, was that the same paper, the embodied touring test?

11:36.160 --> 11:41.760
Can you talk about that? First of all, what is the embodied touring test? I didn't quite

11:41.760 --> 11:50.800
understand that. Well, okay, it's a different concept. But it's basically the idea that you

11:51.760 --> 12:01.280
do, it's based on the Moravec paradox, right? So Moravec many years ago noticed that things that

12:01.280 --> 12:07.360
appeared difficult for humans turned out to sometimes be very easy for computers to do,

12:07.360 --> 12:13.600
like playing chess, much better than humans. Or I don't know, computing integrals or whatever,

12:13.600 --> 12:19.600
certainly doing arithmetic. But then there are things that we take for granted as humans that

12:19.600 --> 12:23.840
we don't even consider them intelligent tasks that we are incapable of reproducing with computers.

12:25.120 --> 12:31.840
And so that's where the embodied touring test comes in. Observe what a cat can do or how fast a cat

12:31.840 --> 12:41.360
can learn new tricks or how a cat can plan to jump on a bunch of different furniture to get to the

12:41.360 --> 12:47.680
top of wherever it wants to go. That's an amazing feat that we can't reproduce with robots today.

12:48.640 --> 12:56.320
So that's kind of the embodied touring test, if you want. Like, can you make a robot that

12:57.040 --> 13:03.360
can behave, have behaviors that are easily wishable from those of animals, first of all,

13:03.360 --> 13:11.440
and can acquire new ones with the same efficiency as animals? Then the augmented

13:11.440 --> 13:18.400
LLM paper is different. It's about how do you sort of minimally change large language models so

13:18.400 --> 13:24.880
that they can use tools so they can, to some extent, plan actions. Like, you know, you need to compute

13:24.880 --> 13:29.280
the product of two numbers, right? You just call a calculator and you know you're going to get the

13:29.280 --> 13:34.400
product of those two numbers. And LLMs are notoriously bad for arithmetic, so they need to do

13:34.400 --> 13:39.680
this kind of stuff or do a search, you know, using a search engine or database lookup or

13:39.680 --> 13:44.240
something like that. So there's a lot of work on this right now and it's somewhat incremental. Like,

13:44.240 --> 13:49.840
you know, how can you sort of minimally change LLM and take advantage of their current capabilities

13:49.840 --> 13:57.600
but still augment them with the ability to use tools? Yeah. And I don't want to get into the

13:58.240 --> 14:04.160
too much into the threat debate. But, you know, you're on one side, your colleagues,

14:05.040 --> 14:11.280
Jeff and Yashor on the other. I recently saw a picture of the three of you. I think you put that

14:11.280 --> 14:21.440
up on social media, saying how, you know, you can disagree but still be friends. This idea of

14:21.440 --> 14:28.960
augmenting language models with stronger reasoning capabilities and the ability,

14:28.960 --> 14:36.720
and agency, the ability to use tools is precisely what Jeff and Yashor are worried about.

14:38.240 --> 14:49.520
Can you just, why are you not worried about that? Okay. So first of all, what you're describing

14:49.520 --> 14:58.240
is not necessarily what they are afraid of. They are alerting people and various governments and

14:58.240 --> 15:04.880
others about various dangers that they perceive. Okay. So one danger, one set of dangers are

15:05.440 --> 15:10.480
relatively short-term. There are things like, you know, bad people will use technology for bad

15:10.480 --> 15:16.880
things. What can bad people use powerful AI systems for? And one concern that, you know,

15:16.880 --> 15:24.640
governments have been worried about and intelligence agencies encounter intelligence and stuff like

15:24.640 --> 15:32.720
that is, you know, could value-intentioned organizations or countries use LLM to help them,

15:32.720 --> 15:39.840
I don't know, design pathogens or chemical weapons or other things or cyber attacks,

15:39.840 --> 15:43.840
you know, things like that, right? Now, those problems are not new. Those problems have been

15:43.840 --> 15:51.200
with us for a long time. And the question is, what incremental help would AI systems bring to the

15:51.200 --> 16:00.160
table? So my opinion is that as of today, AI systems are not sophisticated enough to provide

16:00.160 --> 16:07.200
any significant help for such value-intentioned people because those systems are trained with

16:07.200 --> 16:11.520
public data that is publicly available on the internet. And they can't really invent anything.

16:11.520 --> 16:16.320
They're going to regurgitate with a little bit of interpolation if you want. But

16:17.120 --> 16:24.800
they cannot produce anything that you can't get from a search engine in a few minutes.

16:26.160 --> 16:30.000
So that claim is being tested at the moment. There are people who are actually kind of

16:30.560 --> 16:35.280
trying to figure out, like, is it the case that you can actually do something, you're

16:35.280 --> 16:39.840
unable to do something more dangerous with sort of current AI technology that you can do with a

16:39.840 --> 16:47.760
search engine results are not out yet. But my hunch is that, you know, it's not going to enable

16:47.760 --> 16:54.160
a lot of people to do significantly bad things. Then there is the issue of things like code

16:54.160 --> 16:58.800
generation for cyber attacks and things like this. And those problems have been with us for years.

16:59.760 --> 17:04.080
And the interesting thing that most people should know, like, you know, also for like

17:04.080 --> 17:08.880
disinformation or attempts to corrupt the electoral process and things like this. And what's

17:09.600 --> 17:14.480
very important for everyone to know is that the best countermeasures that we have

17:14.480 --> 17:21.040
against all of those attacks currently use AI massively. Okay. So AI is used as a defense

17:21.040 --> 17:29.280
mechanism against those attacks. It's not actually used to do the attacks yet. And so now it becomes

17:29.280 --> 17:36.560
the question of, you know, who has the better system, like other countermeasures? Is the AI

17:37.520 --> 17:43.840
countermeasures significantly better than the AI is used by the attackers so that, you know,

17:43.840 --> 17:50.560
the problem is satisfactorily mitigated. And that's what we are. Now, the good news is that there are

17:50.560 --> 17:56.720
many more good guys and bad guys. They're usually much more competent. They're usually much more

17:56.720 --> 18:02.960
sophisticated. They're usually much more better funded. And they have a strong incentive to take

18:02.960 --> 18:11.680
down the attackers. So it's a game of cat and mouse, just like every security that's ever existed.

18:12.480 --> 18:20.080
There's nothing new there. Okay. Nothing quite entirely new. Yeah. But then there is the question

18:20.080 --> 18:28.400
of existential risk, right? And this is something that both Jeff and Yosha have been thinking of

18:29.200 --> 18:34.000
fairly recently. So for Jeff, it's only sort of just before last summer that he became,

18:35.200 --> 18:39.520
he started thinking about this because before he thought he was convinced that the kind of

18:39.520 --> 18:45.120
algorithms that we had were significantly inferior to the kind of learning algorithm that the brain

18:45.120 --> 18:52.400
used. And the epiphany he had was that, in fact, no, because looking at the capabilities of

18:53.200 --> 18:58.080
large English models, they can do pretty amazing things with a relatively small number of neurons

18:58.080 --> 19:01.840
and synapses. He said, maybe they're more efficient than the brain. And maybe the learning algorithm

19:01.840 --> 19:05.600
that we use, back propagation, is actually better than whatever it is that the brain uses.

19:06.240 --> 19:09.280
So he started thinking about like, you know, what are the consequences? And

19:10.320 --> 19:13.680
but that's very recent. And in my opinion, he hasn't thought about this enough.

19:14.000 --> 19:23.280
Yosha went to a similar epiphany last winter, where he started thinking about the long-term

19:23.280 --> 19:29.520
consequences. And came to the conclusion also that there was a potential danger.

19:30.880 --> 19:35.360
They're both convinced that AI has enormous potential benefits. They're just worried

19:36.560 --> 19:40.880
about the dangers. And they're both worried about the dangers because they have some doubts

19:41.680 --> 19:47.600
about the ability of our institutions to do the best with technology.

19:49.360 --> 19:56.640
You know, whether they are political, economic, geopolitical, financial institutions,

19:56.640 --> 20:03.920
or industrial, to do the right thing, to be motivated by the right thing. So

20:04.480 --> 20:12.880
you know, if you trust the system, if you trust humanity and democracy,

20:14.960 --> 20:23.200
you might be entitled to believe that society is going to make the best use of

20:23.200 --> 20:28.640
future technology. If you don't believe in the solidity of those institutions,

20:28.640 --> 20:34.640
then you might be scared. Okay. I think I'm more confident in humanity and democracy than they are.

20:35.280 --> 20:39.040
And, and, you know, whatever current systems and they are, I've been thinking about this

20:39.040 --> 20:47.440
problem for much longer, actually, since at least 2014. So when I started fair at Facebook at the

20:47.440 --> 20:53.120
time, it became pretty clear, pretty early on that, you know, deploying AI systems was going to have

20:53.840 --> 20:59.840
big consequences on people in society. And we got confronted to this very early.

21:00.560 --> 21:05.360
And so I started thinking about those problems very early on. Things like, you know, counter

21:05.360 --> 21:12.320
measures against like bias in AI systems, systematic bias, counter measures against attacks,

21:14.080 --> 21:18.560
or, you know, detection of hate speech in every language, things like that. These are things that

21:18.560 --> 21:23.760
people at fair worked on and then were eventually deployed. To just to give you an example, the

21:24.560 --> 21:29.120
proportion of hate speech that was taken down automatically by AI systems five years ago,

21:29.120 --> 21:37.920
you know, in 2017, was about 20 to 25%. Last year, it was 95%. And the difference is entirely due to

21:37.920 --> 21:43.520
progress in natural language understanding, entirely grew to transformers that are pretrained

21:43.600 --> 21:48.640
self-supervised and can essentially detect hate speech in any language. Not perfectly. Nothing

21:48.640 --> 21:54.560
is perfect. It's ever perfect. But AI is massively there. And that's the solution. So I started

21:54.560 --> 22:02.000
thinking about those issues, including existential risk, very early on. In fact, in 2015, early 2016,

22:02.000 --> 22:08.160
actually, I organized a conference hosted at NYU on the future of AI, where a lot of those

22:08.160 --> 22:17.600
questions were discussed. I invited people like, you know, Eric Schmidt and Mark Schreffer, who

22:17.600 --> 22:26.000
was the CTO of Facebook at the time. A lot of people, both from the academic and AI research

22:26.000 --> 22:30.880
side and from the industry side. And there were two days, a public day and kind of a more private

22:30.880 --> 22:35.920
day. What came out of this is the creation of an institution called a partnership on AI.

22:35.920 --> 22:42.400
So this is a discussion I had with Mr. Sabis, which was, you know, would it be useful to have a

22:42.400 --> 22:47.840
forum where we can discuss before they happen, sort of bad things that could happen as a consequence

22:47.840 --> 22:55.840
of deploying AI? Pretty soon, we brought on board Eric Horvitz and a bunch of other people,

22:55.840 --> 22:59.200
and we co-founded this thing called a partnership on AI, which basically has been

23:00.160 --> 23:09.520
funding studies about AI ethics and consequences of AI and publishing guidelines about, you know,

23:09.520 --> 23:14.400
how you do it right to me and my time. So this is not a new thing for me. Like, I've been thinking

23:14.400 --> 23:19.360
about this for 10 years, essentially. Whereas for Yosha and Jeff, it's much more recent.

23:20.160 --> 23:29.360
Yeah. But nonetheless, this augmented AI or augmented language models that have stronger

23:29.360 --> 23:39.200
reasoning and agency raises the threat, regardless of whether or not it can be countered

23:39.920 --> 23:47.600
to a higher level. Right. Okay. So I guess the question there becomes, what is the blueprint

23:48.240 --> 23:54.640
of future AI systems that will be capable of reasoning and planning, will understand how the

23:54.640 --> 24:00.800
world works, will be able to, you know, use tools and have agency and things like that. Right.

24:02.320 --> 24:09.040
And I tell you, they will not be autoregressive LLMs. So the problems that we see at the moment

24:09.920 --> 24:15.840
of autoregressive LLM, the fact that they hallucinate, they sometimes say really stupid

24:15.840 --> 24:20.640
things. They don't really have a good understanding of the world. People claim that they have some

24:20.640 --> 24:25.440
simple word model, but it's very implicit and it's really not good at all. Like, for example,

24:26.640 --> 24:32.640
you know, you can tell an LLM that A is the same as B. And then you ask if B is the same as A,

24:32.640 --> 24:38.000
and it will say, I don't know, or no. Right. I mean, those things don't really understand

24:38.000 --> 24:46.160
logic or anything like that. Right. So the type of system that we're talking about that might be,

24:47.040 --> 24:52.880
that might approach any more level intelligence and let alone human level intelligence have not

24:52.880 --> 25:00.880
been designed. They don't exist. And so discussing their danger and their potential harm is a bit

25:00.880 --> 25:06.240
like, you know, discussing the sex of angels at the moment, or to be a little more

25:06.640 --> 25:12.960
accurate, perhaps, it would be kind of like discussing how we're going to make transatlantic

25:12.960 --> 25:20.640
flight at near the speed of sound safe when we haven't yet invented the turbojet in 1925.

25:21.200 --> 25:28.400
Yeah. Yeah. Like, you know, we can speculate, but you know, how do we, how did we make turbojet

25:28.400 --> 25:34.640
safe? It required decades of really careful engineering to make them incredibly reliable.

25:34.640 --> 25:41.120
And, you know, now we can, you know, run like halfway around the world with the two-engine

25:43.360 --> 25:50.960
turbojet aircraft. I mean, that's an incredible feat. And it's not like people were discussing

25:50.960 --> 25:55.200
sort of philosophical questions about how you make turbojet safe. It's just really careful and

25:55.200 --> 26:04.080
complicated engineering that no one, none of us would understand. So, you know,

26:04.320 --> 26:11.040
how could we ask the AI community now to explain how AI systems are going to be safe? We haven't

26:11.040 --> 26:19.040
invented them yet. No. Okay. That said, I have some idea about how we can design them so that

26:19.040 --> 26:24.800
they have these capabilities. And as a consequence, how they will be safe, I call this objective

26:24.800 --> 26:35.360
driven AI. So what that means is essentially systems that produce their answer by planning

26:35.360 --> 26:41.120
their answer so as to satisfy an objective or a set of objectives. So this is very different

26:41.120 --> 26:45.760
from current LLNs. Current LLNs produce one word after the other or one token, which is,

26:45.760 --> 26:50.800
which has a board unit, doesn't matter, right? They don't really think and plan ahead as we,

26:50.800 --> 26:54.480
as we said before. They just produce one word after the other. That's not controllable.

26:55.840 --> 27:00.640
The only thing we can do is see if what they've produced, like check if what they've produced

27:01.520 --> 27:07.200
satisfies some criterion or set of criteria and then not produce an answer or produce a

27:07.200 --> 27:14.640
non-answer if the answer that was produced isn't appropriate. But we can't really force them to

27:15.360 --> 27:22.800
produce an answer that satisfies a set of objectives. So objective driven AI is the other way,

27:22.800 --> 27:29.920
is the opposite. The only thing that the system can produce are answers that satisfy a certain

27:29.920 --> 27:35.520
number of objectives. So what objective would be? Did you answer the question? Another objective could

27:35.520 --> 27:40.800
be, is your answer understandable by a 13 year old because you're talking to a 13 year old?

27:41.440 --> 27:48.720
Another would be, is this, I don't know, terrorist propaganda or something? You know,

27:48.720 --> 27:53.360
you can have a number of criteria like this, guardrails that would guarantee that the answer

27:53.360 --> 27:59.680
that's produced is satisfy certain criteria, whatever they are. Okay. Same for a robot,

27:59.680 --> 28:03.520
you could guarantee that the sequence of actions that is produced will not hurt anyone.

28:04.080 --> 28:07.600
Like you can have very low level, you know, guardrails of this type that say,

28:07.920 --> 28:13.440
okay, you have, you know, humans nearby and you're cooking, so you have a big knife in your hand,

28:13.440 --> 28:19.440
don't flare your arms. Okay, that would be a very simple guardrails to impose. And you can imagine

28:19.440 --> 28:23.520
having a whole bunch of guardrails like this that will guarantee that the behavior of those systems

28:24.160 --> 28:32.880
would be safe and that their primary goal would be to be basically subservient to us, right? So I

28:32.880 --> 28:40.320
do not believe that we'll have AI systems that can work that will not be subservient to us,

28:41.040 --> 28:44.640
will define their own goals, they will define their own sub goals, but those sub goals would

28:44.640 --> 28:50.960
be sub goals or goals that we set them and will not have all kinds of guardrails that will

28:50.960 --> 28:55.600
guarantee the safety. And we're not going to, it's not like we're going to invent a system

28:55.600 --> 29:00.080
and make a gigantic one that we know will have human level AI and just turning on and then from

29:00.640 --> 29:03.440
the next minute is going to take over the world. That's completely preposterous.

29:03.920 --> 29:08.960
What we're going to do is try with small ones, you know, maybe as smart as a mouse or something,

29:08.960 --> 29:13.200
maybe a dog, maybe a cat, maybe a dog, maybe and work our way up and then, you know,

29:13.200 --> 29:19.120
put some more guardrails. Basically, like we've engineered, you know, more and more powerful

29:19.120 --> 29:27.360
and more reliable turbojets. It's an engineering problem. Yeah, yeah. You were also on a paper,

29:27.360 --> 29:32.640
maybe this is the one that talked about the embodied Turing test on neuro AI.

29:34.800 --> 29:45.440
Can you explain what the neuro AI is? Okay. Well, it's the idea that we should get some

29:45.440 --> 29:53.280
inspiration from neuroscience to build AI systems and that there is something to be learned from

29:53.280 --> 30:02.320
neuroscience and from cognitive science to drive the design of AI systems, some inspiration.

30:02.960 --> 30:08.880
Okay. Something to be learned as well as the other way around. So what's interesting right now is

30:08.880 --> 30:15.840
that the best models that we have of how, for example, the visual cortex works is convolutional

30:15.840 --> 30:20.800
neural networks, which are also the models that we use to recognize images primarily

30:20.800 --> 30:26.800
in artificial systems. So there is kind of information kind of being exchanged both ways.

30:27.520 --> 30:35.760
There's one, you know, one way to make progress in AI is to kind of ignore nature and just,

30:35.760 --> 30:42.240
you know, kind of try to solve problems in a sort of engineering fashion, if you want.

30:43.840 --> 30:50.560
I found interaction with neuroscience always thought provoking. So you don't want to be

30:50.640 --> 30:54.960
copying nature very too closely because there are details in nature that are irrelevant.

30:55.840 --> 31:00.720
And there are principles on which, you know, natural intelligence is based that we haven't

31:00.720 --> 31:06.400
discovered. So, but there is some inspiration to have certainly in your convolutional net for

31:06.400 --> 31:11.600
inspired by the architecture of the visual cortex. The whole idea of neural net and deep learning

31:11.600 --> 31:17.200
came out of the idea that, you know, intelligence can emerge from a large collection of simple

31:17.200 --> 31:22.080
elements that are connected with each other and change the nature of their interactions.

31:22.080 --> 31:28.800
That's the whole idea, right? So, so inspiration from neuroscience certainly has been extremely

31:29.680 --> 31:34.800
beneficial so far. And the idea of neural AI is that you should keep going. You don't want to go

31:34.800 --> 31:42.240
too far. So going too far, for example, is trying to reproduce the some aspect of the functioning

31:42.240 --> 31:49.760
of neurons with electronics. I'm not sure that's a good idea. I'm skeptical about this, for example.

31:51.840 --> 31:57.760
So your research right now, are you, your main focus is on

31:59.120 --> 32:05.440
furthering the JEPA architecture into other modalities or where are you headed?

32:06.400 --> 32:14.000
Yeah. So, I mean, the long term goal is, you know, to get machines to be as intelligent and learn

32:14.000 --> 32:19.520
as efficiently as animals and humans. Okay. And the reason for this is that we need this because

32:19.520 --> 32:25.760
we need to amplify human intelligence. And so intelligence is the most needed commodity that

32:25.760 --> 32:33.120
we want in the world, right? And so we could, you know, possibly bring a new renaissance to humanity

32:33.120 --> 32:38.480
if we could amplify human intelligence using machines, which we are doing already with computers,

32:38.480 --> 32:43.200
right? I mean, that's pretty much what they've been designed to do. But even more, you know,

32:43.200 --> 32:52.880
imagine a future where every one of us has an intelligent assistant with us at all times.

32:53.520 --> 32:57.600
They can be smarter than us. You shouldn't feel threatened by that. We should feel

32:58.560 --> 33:05.840
like we are like, you know, a director of a big lab or a CEO of a company that has a staff working

33:05.840 --> 33:10.240
for them of people who are smarter than themselves. I mean, we're used to this already. I'm used to

33:10.240 --> 33:15.360
this certainly working with people who are smarter than me. So we shouldn't feel threatened by this,

33:15.360 --> 33:23.520
but it's going to empower a lot of us, right, and humanity as a whole. So I think that's a good

33:23.600 --> 33:27.920
thing. That's the overall practical goal, if you want, right? Then there's a scientific

33:27.920 --> 33:31.840
question that's behind this, which is really what is intelligence and how you build it.

33:32.960 --> 33:38.240
And then which is, you know, how can system learn the way animals and humans seem to be

33:38.240 --> 33:45.440
learning so efficiently? And the next thing is, how do we learn how the world works by observation,

33:45.440 --> 33:52.080
by watching the world go by through vision and all the other senses? And animals can do this

33:52.080 --> 33:59.360
without language, right? So it has nothing to do with language, has to do with learning from sensory

33:59.360 --> 34:04.960
perceives and learning mostly without acting, because any action you take can kill you. So

34:04.960 --> 34:09.200
it's better to be able to learn as much as you can without actually acting at all, just observing,

34:10.000 --> 34:14.400
which is what babies do in the first few months of life. They can't hardly do anything, right? So

34:14.400 --> 34:19.600
they mostly observe and learn how the world works by observation. So what kind of learning takes

34:19.680 --> 34:24.880
place there? So that's obviously kind of self-supervised, right? It's learning by prediction. That's an

34:24.880 --> 34:32.080
whole idea from cognitive science. And the thing is, you know, we can learn to predict videos,

34:32.080 --> 34:36.720
but then we notice that predicting videos, predicting pixels in video, is so initially

34:36.720 --> 34:43.360
complicated that it doesn't work. And so then came this idea of JEPA, right? Learn representations

34:43.360 --> 34:48.080
so that you can make predictions in representation space. And that turned out to work really well

34:48.080 --> 34:53.600
for learning image features. And now we're working on getting this to work for video. And

34:53.600 --> 35:00.000
eventually, we'll be able to use this to learn to learn world models, where you show a piece of

35:00.000 --> 35:04.960
video, and then you say, I'm going to take this action, predict what's going to happen next in the

35:04.960 --> 35:13.520
world. And, you know, which is a bit where the Gaia system from Wave is doing at a high level,

35:13.520 --> 35:17.440
but we need this at sort of various levels of abstraction, so that we can build,

35:19.200 --> 35:24.640
you know, systems that are more general than autonomous driving. Okay. That's the...

35:24.640 --> 35:39.360
Yeah. And it's my fault, so I won't go over the hour. But is it conceivable that someday there'll be

35:40.240 --> 35:53.280
a model that you may be embodied in a robot that is ingesting video from its environment

35:53.280 --> 36:00.880
and learning as it's just continuously learning and getting smarter and smarter and smarter?

36:01.600 --> 36:09.280
Yeah. I mean, that's kind of a bit of a necessity. The reason being that, you know,

36:09.280 --> 36:12.560
even if you train a system to have a world model that can predict what's going to happen next,

36:13.440 --> 36:17.840
the world is really complicated. And there's probably all kinds of situations that you,

36:17.840 --> 36:23.280
you know, the system hasn't been trained on and need to, you know, fine tune itself as it goes.

36:24.160 --> 36:33.680
So, you know, animals and humans do this early in life by playing. So play is a way of

36:34.720 --> 36:40.000
learning your world model in situations that basically you won't hurt you.

36:41.920 --> 36:47.520
And, but then during life, of course, you know, when we don't drive, there's all kinds of these

36:47.520 --> 36:53.040
mistakes that we do initially that we don't do after having some experience. And that's because

36:53.040 --> 36:59.280
we're fine tuning our world model to some extent. We're learning a new task. We're basically just

36:59.280 --> 37:05.680
learning a new version of our world model. Right. So, so yeah, I mean, this type of continuous,

37:05.680 --> 37:10.960
continual learning is going to have to be present. But the overall power and the

37:10.960 --> 37:15.520
intelligence of the system will be limited by, you know, how much a co-governor on that is using

37:15.520 --> 37:19.280
and various other constraints, you know, computational constraints, basically.

37:19.600 --> 37:26.000
And, you know, you're still young. And, and this not sure about that.

37:26.640 --> 37:30.400
Well, you're younger than Jeff. Let me put it that way.

37:35.040 --> 37:42.080
But this, the progress you've made on world models is, is fairly rapid from my point of

37:42.160 --> 37:51.360
view, watching it. Are you, are you hopeful that within your career, you'll have

37:52.720 --> 37:59.040
embodied robots that are, are building world models through their interaction in reality,

37:59.040 --> 38:03.120
and, and then being able to, well, I guess the other question on world models,

38:04.080 --> 38:13.440
do you then combine it with a language model to do reasoning or, or is the world model able to,

38:13.440 --> 38:18.640
to do reasoning on its own? But are you hopeful that in your career, you'll, you'll get to the

38:18.640 --> 38:25.120
point where you'll have this continuous learning in a world model? Yeah, I sure hope so. I might have

38:25.120 --> 38:31.440
another, you know, 10, 10 useful years or something like this in research before my brain, you know,

38:31.440 --> 38:38.560
turns into dish and male sauce, but, or something like that, you know, 15 years if I'm lucky.

38:40.000 --> 38:46.160
So, or perhaps less. But yeah, I hope that there's going to be breakthroughs in that direction

38:46.160 --> 38:53.600
during that time. Now, whether that will result in the kind of artifact that you're describing,

38:53.600 --> 38:57.760
you know, robots that can, like, you know, domestic robots, for example, or,

38:57.760 --> 39:01.520
or sort of in cars that are, they can run fairly quickly by themselves.

39:02.880 --> 39:09.840
I don't know, because there might be all kinds of obstacles that we have not envisaged that may

39:10.480 --> 39:17.440
appear on the way. You know, that's, it's a constant in the history of AI that you have some new idea

39:18.160 --> 39:21.440
and a breakthrough, and you think that's going to solve all the world's problems.

39:22.240 --> 39:27.200
And then you're going to hit limitation, and you have to go beyond that limitation. So it's like,

39:27.280 --> 39:31.760
you know, you're climbing a mountain, you find a way to climb the mountain that you're seeing.

39:32.560 --> 39:38.480
And you know that once you get to the top, you will have the problem solved because now it's,

39:38.480 --> 39:43.760
you know, the gentle slope down. And once you get to the top, you realize that there is

39:43.760 --> 39:50.080
another mountain behind it that you hadn't seen. So that's, that's, that's been the history of AI,

39:50.080 --> 39:55.760
right, where people have come up with sort of new concepts, new ideas, new way to approach

39:57.680 --> 40:04.640
AI reasoning, whatever, perception, and then realize that their idea basically was very limited.

40:06.400 --> 40:14.960
And so, so, you know, this, inevitably, we're trying to figure out what's the next

40:16.160 --> 40:20.960
revolution in AI. That's what I'm trying to figure out. So, you know, learning how the world works

40:20.960 --> 40:25.200
from video, having systems that have world model allows systems to reason and plan.

40:27.520 --> 40:34.960
And there's something I want to be very clear about, which is an answer to your question,

40:36.400 --> 40:41.840
which is that you can have systems that reason and plan without manipulating language. Animals are

40:41.840 --> 40:49.280
capable of amazing feats of planning and also to some extent reasoning. They don't have language,

40:49.360 --> 40:56.960
at least most of them don't. And so, many of them don't have culture because they are mostly

40:56.960 --> 41:05.520
solitary animals. So, you know, it's only the animals that have some level of culture. So,

41:08.000 --> 41:14.480
so the idea that a system can plan and reason is not connected with the idea that you can

41:14.480 --> 41:19.840
manipulate language. Those are two different things. It needs to be able to manipulate abstract

41:20.640 --> 41:25.840
notions. But those notions do not necessarily correspond to linguistic entities like words

41:25.840 --> 41:31.040
or things like that. We can have mental images if you want to things. Like you do

41:32.080 --> 41:36.240
ask a physicist or a mathematician, you know, how they reason is very much in terms of sort of

41:36.240 --> 41:39.920
mental models. I have nothing to do with language. Then you can turn things into

41:40.240 --> 41:44.880
language. But that's a different story. That's the second step. So,

41:48.640 --> 41:52.960
so, you know, we're going to have to figure out how to do this reasoning,

41:52.960 --> 41:59.760
hierarchical planning in machines, reproduce this first. And then, of course, you know,

41:59.760 --> 42:04.000
sticking language on top of it will help. Like, we'll make those systems smarter and be able,

42:04.000 --> 42:07.600
you know, we will allow us to communicate with them and teach them things. And they're going to

42:07.600 --> 42:12.560
be able to teach us things and stuff like that. But this is a different question, really.

42:13.120 --> 42:18.560
The question of how we organize AI research going forward, which is somewhat determined by how

42:18.560 --> 42:25.440
afraid people are of the consequences of AI. So, if you have a rather positive view of the impact

42:25.440 --> 42:30.160
of AI on society, and you trust humanity and society and democracies to use it in good ways,

42:30.960 --> 42:38.080
then the best way to make progress is to open research. And for the people who are

42:38.080 --> 42:42.320
afraid of the consequences, whether they are societal or geopolitical,

42:44.000 --> 42:50.240
they're putting pressure on governments around the world to regulate AI in ways that basically limit

42:51.920 --> 42:57.600
access, particularly of open source code and things like that. And it's a big debate at the

42:57.600 --> 43:02.320
moment. I'm very much on the side. So, he's met up very much on the side of open research.

43:02.960 --> 43:08.560
Yeah, actually, that was something I was going to ask you. And now that you've brought it up.

43:09.440 --> 43:17.120
Because there, I've been talking to people about this. And there is a view that aside from the

43:17.120 --> 43:24.160
risks of open source, you know, again, Jeff Hinton saying, you know, would you open source

43:24.160 --> 43:33.840
thermonuclear weapons? Aside from that is the question of as to whether open source can marshal

43:33.840 --> 43:45.120
the resources to compete with proprietary models. And because of the tremendous resources required

43:45.120 --> 43:51.360
for when you're scaling these models. And there's a question as to whether or not

43:51.360 --> 44:00.160
Meta will continue to open source future versions of Lama or not continue to open source, but whether

44:00.160 --> 44:08.160
it'll continue to invest the resources needed to push the open source models.

44:09.120 --> 44:13.840
So what do you think about that? Okay, there's a lot to say about this. Okay, so first thing is,

44:14.720 --> 44:18.720
there's no question that Meta will continue to invest the resources to build

44:19.520 --> 44:24.800
better and better AI systems, because it needs it for its own products. So the resources will

44:24.800 --> 44:31.120
be invested. Now, the next question is, do you, you know, will we continue to open source the

44:31.120 --> 44:37.760
base models? And the answer is, you know, probably yes, because that creates an ecosystem on top of

44:37.760 --> 44:43.120
which an entire industry can be built. And there is no point, you know, having 50 different companies

44:43.440 --> 44:51.840
building proprietary close systems when you can have, you know, one good base open source base

44:51.840 --> 44:59.680
model that everybody can use. It's wasteful. And it's not a good idea. And another reason for

44:59.680 --> 45:08.160
having open source models is that it, it nobody has no entity as powerful as it thinks it is,

45:08.240 --> 45:14.080
as a monopoly on good ideas. And so if you want people who can have good new innovative ideas

45:14.080 --> 45:18.960
to contribute, you need an open source platform. If you want the academic world to contribute,

45:18.960 --> 45:22.560
you need open source platforms. If you want the startup world to be able to build

45:22.560 --> 45:27.680
customized products, you need open source base models, because they don't have the resources to

45:27.680 --> 45:34.480
build to train large models, right? Okay, and then there is the history that shows that for,

45:35.040 --> 45:43.760
for foundational technology, for infrastructure type technology, open source always wins,

45:46.640 --> 45:53.440
right? It's true of the software infrastructure of the internet. In the early 90s and mid 90s,

45:53.440 --> 45:58.560
there was a big battle between sun macro systems and Microsoft to produce the, deliver the

45:59.520 --> 46:03.920
software infrastructure of the internet, you know, operating systems, web servers,

46:05.360 --> 46:10.000
web browsers, and, and, you know, various servers aside and client-side frameworks, right?

46:10.000 --> 46:15.200
They're both lost. Nobody is talking about them anymore. The entire world is,

46:16.960 --> 46:26.560
of the web is using Linux and Apache and MySQL and JavaScript and, and, you know, and even the,

46:26.800 --> 46:31.600
the basic core code for, for web browser is open source. So,

46:33.440 --> 46:40.320
open source won by a huge margin. Why? Because it's safer, gathers more people to contribute.

46:40.320 --> 46:43.360
All the features are unnecessary. It's more reliable.

46:45.760 --> 46:52.960
Venerabilities are fixed faster. And, and it's customizable. So anybody can customize Linux

46:52.960 --> 47:00.400
to run on whatever hardware they want, right? So open source wins. And the same, same for AI.

47:00.400 --> 47:05.280
It's going to be the same thing. It's inevitable. The, the people now who are climbing up,

47:06.320 --> 47:15.200
like open AI, their, their system is based on publications from all of us. Sure. And from

47:16.160 --> 47:18.800
open platforms like, like PyTorch. Yeah.

47:18.800 --> 47:23.280
Judgeability is built using PyTorch. PyTorch was produced originally by Meta. Now it's owned by

47:23.280 --> 47:29.520
the Linux Foundation. It's open source. They've contributed to it, by the way. You know, their

47:29.520 --> 47:36.160
LLM is based on transformer architectures invented at Google. Yeah. All the tricks to kind of train

47:36.160 --> 47:41.280
all those things came out of like various papers from all kinds of different institutions,

47:41.280 --> 47:47.760
including academia, all the fine-tuning techniques, same. So nobody works in a vacuum.

47:47.760 --> 47:52.560
The thing is, nobody can keep their advance and their advantage

47:54.160 --> 48:00.320
for very long if they are secretive. Yeah. Except that with these models, because they're

48:00.320 --> 48:07.200
so compute intensive and they cost so much money to train, you need somebody like Meta that who's,

48:07.200 --> 48:13.440
who's going to be willing to build them and open source them. And that's why I was, when I was

48:13.440 --> 48:23.520
asking whether they'll continue, obviously Meta will continue building, you know, resource-intensive

48:23.520 --> 48:30.000
models. But the question is whether they'll continue to open source. I mean, if- I'm telling you,

48:30.000 --> 48:37.040
I'm telling you the only reason why Meta could stop open sourcing models are legal.

48:38.000 --> 48:43.520
So if there is a law that adds laws, open source AI systems above a certain level of

48:44.800 --> 48:53.200
sophistication, then of course we can do it. If there are laws that in the US or across the world

48:55.520 --> 49:02.400
makes it illegal to use public content to train AI systems, then it's the end of AI

49:02.480 --> 49:08.560
for everybody, not just for the open source. Okay. So, or at least the end of the type of AI

49:08.560 --> 49:12.400
that we are talking about today might have, you know, new AI in the future, but that don't

49:12.400 --> 49:21.360
require as much data. So, and then there is, you know, liability. If you, if you, if you, if you

49:21.360 --> 49:28.960
believe in the kind of that someone doing something bad with an AI system that was open sourced by

49:29.680 --> 49:35.920
by Meta, then Meta is liable, then Meta will have a big incentive not to release it, obviously.

49:36.720 --> 49:41.840
So it's the entire question about this is around legal reasons and political decisions.

49:41.840 --> 49:48.320
But on the idea of open source winning, don't you need more people or more companies like Meta

49:48.320 --> 49:54.400
building the foundation models and open sourcing them? Or could it be, could an open source

49:54.960 --> 50:01.280
ecosystem win based on a single company building the models? No, I mean, you need two or three.

50:02.000 --> 50:06.960
And there are two or three, right? I mean, there is this hugging face. There is Mistral in France,

50:06.960 --> 50:13.040
who's also embracing sort of an open source LLM. They're very good LLM. It's a small one, but it's

50:13.040 --> 50:21.680
very good. There is, you know, academic efforts like Lyon. They don't have all the resources they

50:21.680 --> 50:26.640
need, but they, you know, they collect the data that is used by everyone. So everybody can contribute.

50:26.640 --> 50:31.440
One thing that I think is really important to understand also is that there is a future in

50:31.440 --> 50:37.520
which I described earlier in which every one of us, every one of our interactions with the digital

50:37.520 --> 50:44.000
world will be mediated by an AI assistant. And this is going to be for true for everyone around

50:44.000 --> 50:49.280
the world, right? Everyone who has any kind of smart device. Eventually it's going to be in our,

50:49.280 --> 50:53.440
you know, augmented reality glasses, but, you know, for the time being in our smartphones, right?

50:55.200 --> 51:02.240
And so imagine that future where, you know, you are, I don't know, from

51:04.400 --> 51:13.840
Indonesia or Senegal or France. And your entire digital diet is done through the

51:14.560 --> 51:21.840
mediation of an AI system. Your government is not going to be happy about it. Your government

51:22.640 --> 51:27.440
is going to want the local culture to be present in that system. It doesn't want that system to be

51:27.440 --> 51:36.880
closed sourced and controlled by a company on the west coast of the US. So just for reasons of

51:37.200 --> 51:45.360
preserving the diversity of culture across the world and not having or entire information

51:45.360 --> 51:49.840
diet being biased by whatever it is that some company on the west coast of the US states,

51:51.360 --> 51:57.360
there's going to need to be open source platforms. And they're going to be predominant

51:58.240 --> 52:04.400
in at least outside the US for that reason. Including China, right? There is all those

52:04.400 --> 52:09.120
talks about, oh, what if China puts their hands on our open source code? I mean, China wants control

52:09.120 --> 52:15.760
over its own LLM because they don't want their citizen to, you know, have access to certain type

52:15.760 --> 52:20.480
of information. So they're not going to use our LLMs. They're going to trend theirs that they already

52:20.480 --> 52:26.640
have. And nobody is, you know, particularly ahead of anybody else by more than about a year.

52:27.520 --> 52:35.120
Yeah. And China is pushing open source. I mean, they're very pro open source within their

52:35.120 --> 52:40.960
ecosystems. Some of them, you know, it's there's no like unified opinion there. But

52:43.040 --> 52:46.880
I mean, it's the same in in the West, right? There are some some governments that are too

52:46.880 --> 52:53.520
afraid of the risks. And then or are thinking about it and some others that are all for open

52:53.520 --> 52:59.440
source because they see this as the only way for them to have any influence on the

53:02.160 --> 53:08.240
type of information and culture that would be mediated by those systems. So it's going to have

53:08.240 --> 53:17.120
to be like Wikipedia, right? Wikipedia, you know, is built by millions of people who contribute to

53:17.120 --> 53:21.280
or from all around the world in all kinds of languages. Okay. And it has a system for sort

53:21.360 --> 53:26.720
of vetting the information. The way AI systems of the future will be taught and we'll be fine

53:26.720 --> 53:33.360
tuned will have to be the same way will have to be quite sourced. Because something that matters to

53:34.720 --> 53:41.600
a farmer in southern India is probably not going to be taken into account by the fine

53:41.600 --> 53:47.360
tuning done by, you know, some some company on the west coast of the US. AI might be the most

53:47.360 --> 53:53.440
important new computer technology ever. It's storming every industry and literally billions

53:53.440 --> 53:59.920
of dollars are being invested. So buckle up. The problem is that AI needs a lot of speed and

53:59.920 --> 54:07.280
processing power. So how do you compete without cost spiraling out of control? It's time to upgrade

54:07.280 --> 54:16.080
to the next generation of the cloud oracle cloud infrastructure or OCI. OCI is a single platform

54:16.080 --> 54:23.920
for your infrastructure, database, application, development and AI needs. OCI has four to eight

54:23.920 --> 54:30.960
times the bandwidth of other clouds, offers one consistent price instead of variable regional

54:30.960 --> 54:38.240
pricing. And of course, nobody does data better than oracle. So now you can train your AI models

54:38.240 --> 54:44.960
at twice the speed and less than half the cost of other clouds. If you want to do more and spend

54:44.960 --> 54:54.640
less like Uber, eight by eight and Databricks Mosaic, take a free test drive of OCI at oracle.com

54:55.200 --> 55:07.040
slash I on AI. That's E Y E O N A I all run together oracle.com slash I on AI. That's it for this

55:07.040 --> 55:13.920
episode. I want to thank Yen for his time. If you want to read a transcript of this conversation,

55:13.920 --> 55:23.520
you can find one on our website I on AI. That's E Y E hyphen O N dot AI. And remember the singularity

55:24.320 --> 55:36.480
may not be near, but AI is changing your world. So best pay attention.

