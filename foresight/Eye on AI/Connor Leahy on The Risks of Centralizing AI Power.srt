1
00:00:00,000 --> 00:00:06,000
What this whole open AI saga has shown us is that, I mean, obviously we can't have something like

2
00:00:06,000 --> 00:00:10,720
this being developed by just like a handful of, you know, weird people, unaccountable

3
00:00:10,720 --> 00:00:14,480
billionaires in the Bay Area. This is actually something that I've been telling

4
00:00:14,480 --> 00:00:18,080
saying to people for years now. Obviously, this is not the right governance structure.

5
00:00:18,080 --> 00:00:22,320
So the weirdest thing I would say about this technology should be a tool. It should not be a

6
00:00:22,320 --> 00:00:29,360
goal in and of itself. Hi, I wanted to jump in and give a shout out to our sponsor NetSuite

7
00:00:29,360 --> 00:00:37,120
by Oracle. I'm a journalist and getting a single source of truth is nearly impossible.

8
00:00:37,120 --> 00:00:42,960
If you're a business owner, having a single source of truth is critical to running your

9
00:00:42,960 --> 00:00:54,320
operations. If this is you, you should know these three numbers. 36,025,1. 36,000 because that's the

10
00:00:54,320 --> 00:01:00,720
number of businesses that have upgraded to NetSuite by Oracle. NetSuite is the number one

11
00:01:00,720 --> 00:01:08,080
cloud financial system, streamlining, accounting, financial management, inventory, HR, and more.

12
00:01:08,960 --> 00:01:17,040
25, because NetSuite turns 25 this year. That's 25 years of helping businesses do more with less,

13
00:01:17,040 --> 00:01:23,920
close their books in days, not weeks, and drive down costs. One, because your business is one

14
00:01:23,920 --> 00:01:31,600
of a kind. So you get a customized solution for all of your KPIs in one efficient system

15
00:01:31,600 --> 00:01:38,880
with one source of truth. Manage risk, get reliable, forecast, and improve margins. Everything you need

16
00:01:39,440 --> 00:01:47,280
all in one place. As I said, I'm not the most organized person in the world, and there's real

17
00:01:47,280 --> 00:01:52,000
power to having all of the information in one place to make better decisions.

18
00:01:52,960 --> 00:02:01,440
This is an unprecedented offer by NetSuite to make that possible. Right now, download NetSuite's

19
00:02:01,440 --> 00:02:09,200
popular KPI checklist designed to give you consistently excellent performance, absolutely free

20
00:02:09,360 --> 00:02:21,600
at netsuite.com slash I on AI. That's I on AI, E-Y-E-O-N-A-I, all run together. Go to net suite.com

21
00:02:22,320 --> 00:02:34,880
slash I on AI to get your own KPI checklist. Again, that's net suite.com slash I on AI, E-Y-E-O-N-A-I.

22
00:02:35,120 --> 00:02:39,120
They support us, so let's support them.

23
00:02:42,880 --> 00:02:50,400
Hi, my name is Craig Smith, and this is I on AI. In this episode, I speak again with Connor Lehi.

24
00:02:51,360 --> 00:02:58,080
He's the founder and CEO of a startup called Conjecture that's working on AI alignment.

25
00:02:58,960 --> 00:03:05,360
Before that, he was one of the founders and leaders of a group called Eleuther AI

26
00:03:06,320 --> 00:03:16,560
that built one of the world's first open source large language models. Connor is concerned about

27
00:03:16,560 --> 00:03:25,360
AI safety, about where AI development is going, concerned about the push towards

28
00:03:26,320 --> 00:03:32,640
artificial general intelligence, and has a lot of thoughts about what we should be doing

29
00:03:33,280 --> 00:03:42,240
to control development so that we don't end up creating something that is harmful to humanity.

30
00:03:42,240 --> 00:03:47,840
I talked to him particularly because I wanted to hear his thoughts on the open AI saga,

31
00:03:48,800 --> 00:03:55,520
which highlighted for a lot of people the dangers of having such a small group of people

32
00:03:56,720 --> 00:04:06,000
controlling such a fundamentally powerful technology today. I hope you find the conversation

33
00:04:06,000 --> 00:04:13,280
as interesting as I did. So I'm Connor. I'm currently the CEO of Conjecture, an AI company in

34
00:04:13,280 --> 00:04:18,960
London focused on AI safety and building architectures for AI systems that are understandable

35
00:04:18,960 --> 00:04:25,520
and controllable, and various other things. I also do a bit of work in policy regulation,

36
00:04:25,520 --> 00:04:32,320
public messaging, that kind of stuff. Before this, I was well known as one of the founders of Eleuther

37
00:04:32,320 --> 00:04:39,760
AI, which was kind of one of the first, if not the first kind of like open source large language

38
00:04:39,760 --> 00:04:46,240
models, research, building kind of groups. And technically, even before that, I was someone who

39
00:04:46,240 --> 00:04:51,200
worked on open source GBT2 as I think maybe literally the first person.

40
00:04:52,480 --> 00:05:00,400
Yeah. And so last time we spoke was after the release of chat GBT and GBD4, and

41
00:05:01,840 --> 00:05:07,520
you were very concerned as were a lot of people and a lot of people continue to be about

42
00:05:08,480 --> 00:05:16,320
releasing these kinds of models into the public before having fully explored

43
00:05:17,760 --> 00:05:26,320
the safety issues or without having adequate guidelines. But it struck me during this open AI

44
00:05:26,320 --> 00:05:34,960
saga that we have lived through the last week, that having this kind of powerful technology

45
00:05:34,960 --> 00:05:45,440
in the hands of a handful of people who have different agendas and can't get along is in itself

46
00:05:45,440 --> 00:05:56,640
a security issue. And I would think that having these models open source where everyone can

47
00:05:57,440 --> 00:06:05,040
see the data they were trained on in particular, because even Llama, I've learned, doesn't

48
00:06:05,920 --> 00:06:14,560
make public the training data. But having the data open for all to see and having the weights

49
00:06:15,200 --> 00:06:24,640
of the models open so people can improve them or play with them or whatever. That to me seems

50
00:06:24,640 --> 00:06:32,880
like a much safer path than having proprietary models. But I wanted to hear how your thinking is

51
00:06:32,880 --> 00:06:41,760
involved on that. In the 1940s and in the early 1950s, the Soviet Union built a what's called a

52
00:06:41,760 --> 00:06:48,960
closed city around what would be known as the Mayak facility. The Mayak facility was the largest and

53
00:06:48,960 --> 00:06:52,800
one of the first, well I don't know if it was literally the first but it's one of the largest

54
00:06:52,800 --> 00:06:57,120
nuclear facilities for the Soviet Union in their breakthrough development attempt to build the

55
00:06:57,120 --> 00:07:03,680
nuclear bomb. To give a bit of a flavor for what Mayak and similar facilities that existed throughout

56
00:07:03,680 --> 00:07:11,920
the Soviet Union were like, there was a program where if you got caught by the secret police

57
00:07:11,920 --> 00:07:16,080
and you were being sent to a gulag for the rest of your life, you would give them an option.

58
00:07:16,960 --> 00:07:21,120
Either you go to Siberia, work yourself to death for the rest of your life,

59
00:07:21,120 --> 00:07:25,680
or you get sent to Mayak for only three months. And if you serve your term, you're free.

60
00:07:26,560 --> 00:07:32,560
Sounds great, doesn't it? Well, no one survived the three months. So what happened, of course, is

61
00:07:32,560 --> 00:07:37,040
that one of the first things that the Americans developed while developing the bomb is the HEPA

62
00:07:37,040 --> 00:07:43,200
filter, which is a form of air filter that is powerful enough to be able to filter radioactive

63
00:07:43,200 --> 00:07:48,800
material very sufficiently out of the air, making it safe for the workers. The Soviets didn't

64
00:07:48,800 --> 00:07:55,440
bother developing HEPA filters. Not really. So a lot of people died. To this day,

65
00:07:56,160 --> 00:08:01,600
Lake Karakai, which is near the Mayak facility, is one of the most radioactive places on earth,

66
00:08:02,160 --> 00:08:07,520
so much so that it is said that standing next to it for an hour can kill a man. These are all

67
00:08:07,520 --> 00:08:11,280
just some fun facts that surely have nothing to do with the topic we're talking about today.

68
00:08:11,840 --> 00:08:20,400
It's a story about how some people who were pretty bad people developed something or working

69
00:08:20,400 --> 00:08:26,000
on something pretty dangerous, and then other people got access to it, and also really bad things

70
00:08:26,000 --> 00:08:33,040
out because even worse people got access to it. Now, from this, I don't conclude, oh, so we should

71
00:08:33,040 --> 00:08:40,320
have just had everyone develop plutonium. That would have made it safer. This is not a draw from

72
00:08:40,400 --> 00:08:46,720
this conclusion from the story. Now, nuclear power is obviously very different from AI in many

73
00:08:46,720 --> 00:08:54,480
factors. So why even bring it up? So I can ask the question in the other direction, though.

74
00:08:54,480 --> 00:08:59,920
We're talking about AI. We're talking about AGI. So let's focus on AGI. I'm not really interested

75
00:08:59,920 --> 00:09:06,080
in talking about the risks of current-day models, like chat GPT or something. We can talk about

76
00:09:06,080 --> 00:09:09,920
those, too. They are real. There are real risks from those, but they're not the kind where I

77
00:09:09,920 --> 00:09:14,800
think we have to stop all publication necessarily. But let's talk about AGI systems.

78
00:09:17,120 --> 00:09:24,000
What reference class does AGI fall into? Is it like open source? Is it like nuclear bombs?

79
00:09:24,960 --> 00:09:32,960
Is it like something different? The reference class we choose forms our thinking around

80
00:09:32,960 --> 00:09:37,840
something which is fundamentally none of those things. AGI is in nukes. AGI is in open source.

81
00:09:38,480 --> 00:09:44,400
It's not Linux. It's something very different. So while it has things in common with all of those

82
00:09:44,400 --> 00:09:50,880
things, AGI runs on computers. Linux runs on computers. And it has other things that come

83
00:09:50,880 --> 00:09:54,960
with nuclear bombs. Nuclear bombs can kill everybody. AGI can kill everybody. Those are

84
00:09:54,960 --> 00:09:59,040
things in common. Is it more like nukes? This is more like open source. At some point, we have

85
00:09:59,040 --> 00:10:04,000
to actually drop down from the metaphors and into the actual models of reality. So from my

86
00:10:04,000 --> 00:10:10,240
perspective, I think you're completely correct. What this whole opening AGI has shown us is that

87
00:10:11,440 --> 00:10:17,840
obviously, we can't have something like this being developed by just a handful of weird people,

88
00:10:17,840 --> 00:10:22,480
unaccountable billionaires in the Bay Area. Obviously, they're not acting in humanity's

89
00:10:22,480 --> 00:10:28,880
best interest to no one's surprise. A couple of months ago, Sam Altman interviewed about this,

90
00:10:28,880 --> 00:10:32,720
and he said, oh, yeah, the board can fire me at any time if I go about a mission. I think that's

91
00:10:32,720 --> 00:10:39,200
important. And then they try to fire him, and he's back. And well, that didn't work. So this is

92
00:10:39,200 --> 00:10:47,200
actually something that I've been telling saying to people for years now. I've gotten some disagreements.

93
00:10:47,200 --> 00:10:51,760
He did disagreements, let us call them, with some of the people who were involved with the creation

94
00:10:51,760 --> 00:10:57,040
of the board or in favor of the existence of the board. And the point I always made to them was

95
00:10:58,000 --> 00:11:06,560
this obviously cannot control a charismatic billionaire political mastermind. Why the

96
00:11:06,560 --> 00:11:12,320
hell would you think it would? This is crazy. And this is exactly what we saw play out. And

97
00:11:12,320 --> 00:11:16,320
I'm not even trying to make a comment on Sam Altman, good Sam Altman, bad. I'm just saying,

98
00:11:16,320 --> 00:11:21,680
obviously, he was just not going to just say, oh gosh darn, I guess the board said no more AGI for

99
00:11:21,680 --> 00:11:27,520
me, I guess I'm going to stop. That's not how men like him work. And that's obviously not what was

100
00:11:27,520 --> 00:11:33,840
going to happen. And you think the politically unsavvy nerds could write a document that would

101
00:11:33,840 --> 00:11:38,080
convince someone like him to stop? No, of course not. So obviously, this is not the right governance

102
00:11:38,080 --> 00:11:43,760
structure. I fully agree with this. But there's a great saying, which is that reverse stupidity is

103
00:11:43,760 --> 00:11:48,800
not intelligence. If you take something stupid, and you take the opposite of it, it's probably also

104
00:11:48,800 --> 00:11:55,760
stupid. And so the fact that this governance structure doesn't work for me does not say that

105
00:11:55,760 --> 00:11:59,600
therefore there should be no governance structure. This to me does not follow.

106
00:12:00,880 --> 00:12:07,200
Yeah, but open source, I mean, you are I'm sure very familiar with Yanlacun's argument that,

107
00:12:08,000 --> 00:12:19,920
yeah, that open sourcing can can lead to some abuse by bad actors, but by and large,

108
00:12:21,040 --> 00:12:30,960
the vast majority of people that will be working on an open source model contributing to it or

109
00:12:30,960 --> 00:12:41,440
building products off of it will be doing so with, you know, not without nefarious intent.

110
00:12:42,000 --> 00:12:49,600
And that the larger the open source community, the quicker it would be able to respond to

111
00:12:50,560 --> 00:13:02,560
to bad actors or or misuse or or the more people available to to build guardrails and spot

112
00:13:02,960 --> 00:13:11,280
of weaknesses and that sort of thing. So that that argument makes a lot of sense to me. I mean,

113
00:13:11,280 --> 00:13:20,240
at the beginning, when the, you know, pause letter came out. And around the time that we talked about,

114
00:13:20,240 --> 00:13:26,560
yeah, this stuff is too dangerous to be open source. But,

115
00:13:29,920 --> 00:13:36,880
but I'm changing my mind. And I wanted to hear whether the this this episode has changed your

116
00:13:36,880 --> 00:13:44,720
mind at all. I'll make three points in reaction to that. The first one is a story.

117
00:13:44,720 --> 00:13:49,840
The second one is a heuristic. And the third one is a true observation from my own life.

118
00:13:50,560 --> 00:13:58,000
So first, the story, the story is that the smallpox virus genome is currently online.

119
00:13:58,560 --> 00:14:03,040
You can go download it. It's a small text file. You can just go download it to your computer.

120
00:14:04,000 --> 00:14:09,280
Another fact of the story is that a couple of years ago, Canadian scientists

121
00:14:10,240 --> 00:14:16,480
recreated an extinct version of smallpox called horsepox. They revived it. They may and it was

122
00:14:16,480 --> 00:14:22,720
functional and viable and infectious. And they published how to do it. Do you think either of

123
00:14:22,720 --> 00:14:28,000
those things are good? Now you can argue, well, if we have more eyes on the smallpox virus,

124
00:14:28,000 --> 00:14:32,080
then something, something, you know, good things happen. But this isn't really a model.

125
00:14:32,800 --> 00:14:36,640
So this brings me to the second point. The second point is offense versus defense.

126
00:14:37,600 --> 00:14:43,120
The way technology is work is that some favorite offense, some favorite defense,

127
00:14:43,120 --> 00:14:50,960
very few are symmetric. Most of the time, and most of the time offense wins. It is usually

128
00:14:50,960 --> 00:14:56,320
easier to destroy than it is to protect. There are exceptions to this rule, for example,

129
00:14:56,320 --> 00:15:01,040
cryptography is an interesting exception where defense is easier than offense.

130
00:15:02,240 --> 00:15:07,120
But in most cases, it is easier to build a bomb than it is to build a reactor,

131
00:15:07,680 --> 00:15:15,280
you know, a safe controlled burn. So all things being equal, you should expect that if you have

132
00:15:15,280 --> 00:15:20,480
a technology and you distribute equally, that there will be more destruction. This is the default

133
00:15:20,480 --> 00:15:25,440
case. This is what you should expect by default. Most technologies that are destroyed don't immediately

134
00:15:25,440 --> 00:15:29,840
give you a way to defend against it. Developing vaccines is harder than developing bioweapons.

135
00:15:30,480 --> 00:15:33,840
It is much easier to crank out a bunch of bioweapons and then you have to develop

136
00:15:33,840 --> 00:15:37,600
vaccines in response to that, which is already super hard because, you know,

137
00:15:37,600 --> 00:15:42,800
who knows how far the virus already is. So just because the technology is why the aspect does

138
00:15:42,800 --> 00:15:48,320
not mean it defends wins. Whether offense or defense wins is a property of reality. It is

139
00:15:48,320 --> 00:15:54,640
not a property of your morals or of your ideology. And the third point is an observation from my

140
00:15:54,640 --> 00:15:59,280
own life is that I used to work in open source. I was one of the very first people to work on it.

141
00:15:59,280 --> 00:16:07,040
And I had similar views to Licken and, you know, assuming he holds these views genuinely,

142
00:16:07,760 --> 00:16:12,960
which, you know, I hope he does. I don't know him very well. I talked to him maybe once.

143
00:16:14,160 --> 00:16:21,280
And I think this is just not even wrong. It's just in my experience, what happens when you

144
00:16:21,280 --> 00:16:25,520
build AI models and you release them open source is that the first thing that happens to get

145
00:16:25,520 --> 00:16:30,560
uploaded to Huggingface, and then a guy called the bloke, that's literally his name, uncensored

146
00:16:30,560 --> 00:16:36,160
them, undoes any RLHF training or other security training run have done, trains them all the newest

147
00:16:36,160 --> 00:16:42,080
data to make them more powerful, more general, more whatever, uploads them again, 4chan downloads it,

148
00:16:42,080 --> 00:16:46,160
down, you know, uses them for whatever their applications are, whether it's, you know,

149
00:16:46,160 --> 00:16:53,200
pornography mostly, or likes BAM or whatever, et cetera. And now maybe this is fine, right?

150
00:16:53,200 --> 00:16:58,400
Like, you know, you know, maybe we say, oh, it's okay. If people want to use their LLMs for porn,

151
00:16:58,400 --> 00:17:05,040
so what? That's okay. Sure. What I'm saying is, is the empirical observation is that the amount

152
00:17:05,040 --> 00:17:11,040
of effort that gets put into making these things safer or more controllable is absolutely pathetic

153
00:17:11,680 --> 00:17:15,280
compared to the amount of effort that the open source community puts into making these things

154
00:17:15,280 --> 00:17:21,280
more powerful, more general, and less controllable. This is just an empirical fact. This is just

155
00:17:21,280 --> 00:17:27,760
actually, if you go online, you pick the top 1000 LLM repos, how many of them are about controlling

156
00:17:27,760 --> 00:17:32,880
the models better versus making them faster, making them more efficient, distilling them,

157
00:17:32,880 --> 00:17:37,840
making them more, et cetera. And the fact is that the offense, like the unbalance here is like,

158
00:17:37,840 --> 00:17:42,240
it's not even funny. And I understand, right? And this is not to say that the people working on this

159
00:17:42,240 --> 00:17:46,720
technology are like morally evil. I think this is an important thing to understand. There's an

160
00:17:46,720 --> 00:17:52,000
incentive from people like Lacan and other like big tech, you know, people, like talking heads,

161
00:17:52,000 --> 00:17:58,080
to try to focus on it's only the evil people's fault because that absolves them of responsibility.

162
00:17:59,200 --> 00:18:04,800
Meta wants open source because it absolves them of responsibility as a corporation.

163
00:18:04,800 --> 00:18:09,120
They can't get sued because it owes the user's fault. And this is also what's happening in the

164
00:18:09,120 --> 00:18:15,200
EU AI act right now is that people like Lacan are lobbying to remove foundation models from

165
00:18:15,200 --> 00:18:21,680
regulation in the EU and saying instead of their uses should be regulated. This is the same thing

166
00:18:21,680 --> 00:18:26,720
as when, for example, plastic companies invented recycling. They invented it so that it was the

167
00:18:26,720 --> 00:18:32,000
user's fault that there is all this plastic pollution. Like, oh, see, we would have recycled

168
00:18:32,000 --> 00:18:38,240
it. But unfortunately, the users just didn't do it. This is a, this is gas lighting. And this is a

169
00:18:39,040 --> 00:18:45,760
complete unbalance of power. The externalities of plastic pollution should be on the ones who are

170
00:18:45,760 --> 00:18:50,960
most suited to addressing this externality, who are creating this externality. It shouldn't be on

171
00:18:50,960 --> 00:18:56,800
the user. And the same thing applies to foundation models is that these systems can do things. They

172
00:18:56,800 --> 00:19:01,360
can be used for many things. And we should be taking the big companies building these systems.

173
00:19:01,360 --> 00:19:04,880
Is that what these open source models are being built by like, you know, plucky little teenagers

174
00:19:04,880 --> 00:19:10,240
in their, in their, you know, rooms as a plucky teenager that did do that. I'm saying most of

175
00:19:10,240 --> 00:19:16,480
the ones being built now are being made by like the UAE and meta. Like, these aren't the little

176
00:19:16,480 --> 00:19:20,320
guys. These are big guys trying to shirk their responsibility to society.

177
00:19:21,600 --> 00:19:29,040
Well, then what, what's the lesson from, from the open AI saga that, that you just need a

178
00:19:29,040 --> 00:19:35,040
bigger board or you need a lesson is that none of these structures are correct. This is what we

179
00:19:35,040 --> 00:19:39,360
have governments for. This is the same lesson that we've had over and over again. It's like

180
00:19:40,080 --> 00:19:46,160
self-regulation does not work. It has never worked. Self-regulate. This is like tobacco

181
00:19:46,160 --> 00:19:50,800
companies self-regulating themselves. This does not work. And we as a society have developed a

182
00:19:50,800 --> 00:19:55,840
mechanism. I'm not saying it's a perfect mechanism by any means, but we do have a mechanism for

183
00:19:56,400 --> 00:20:01,360
intervening in systems that have extreme high externalities that are not self-regulatable.

184
00:20:01,360 --> 00:20:11,200
And it's called the government. Yeah. And, and I mean, there has been a lot of work at the government

185
00:20:11,200 --> 00:20:20,720
level, not as much in the US as in Europe. But, but how, how do I mean, obviously these

186
00:20:21,680 --> 00:20:26,960
models are so commercially, the potential is so commercially

187
00:20:28,560 --> 00:20:35,280
exciting that fines aren't going to matter. You're not going to be able to find people

188
00:20:36,480 --> 00:20:42,640
to behave in ways that the government wants them to. There's got to be something stronger than

189
00:20:42,640 --> 00:20:48,640
that. So, have you thought about, about that? I mean, how do you regulate these things?

190
00:20:51,520 --> 00:20:57,520
One of my, one of the most inspiring moments from the history, I think of science and society

191
00:20:58,160 --> 00:21:06,480
is many decades ago, biologists and chemists and so on realized that human cloning should be possible.

192
00:21:06,480 --> 00:21:10,480
Like it should be possible to do this. They were still far from having the actual technology

193
00:21:10,480 --> 00:21:15,440
to perform with human cloning, but they found out it should be possible. And they reasonably

194
00:21:15,440 --> 00:21:20,320
understood, wait, that might be really disabilizing. Like that could be, we don't know what the

195
00:21:20,320 --> 00:21:24,080
consequence is, but maybe it's great. You know, maybe there's, you know, there's many benefits

196
00:21:24,080 --> 00:21:30,160
from human cloning as well. But like, let's chill out. We don't know, like we don't know,

197
00:21:30,160 --> 00:21:35,280
and this seems huge. This doesn't, isn't just like another thing. This is not a 10% more effective,

198
00:21:35,280 --> 00:21:42,640
you know, cough drop. Like human cloning is a big deal. And so, heroically, long before the

199
00:21:42,640 --> 00:21:48,640
technology existed, they came together and banned it and said, let's have a moratorium.

200
00:21:48,640 --> 00:21:54,080
Let's not do this until we've had a bit more time to figure out what the hell we as a society

201
00:21:54,080 --> 00:21:59,840
want about this. And it wasn't one board. This wasn't one CEO being like, I will, you know,

202
00:22:00,480 --> 00:22:04,720
take a moratorium on this. No, it was the scientific community and governments coming

203
00:22:04,720 --> 00:22:10,880
together and working very, very hard to create a moratorium. A moratorium is what we do when we

204
00:22:10,880 --> 00:22:16,160
are faced with something which we know is huge and we don't know how to deal with. That's what

205
00:22:16,160 --> 00:22:20,880
scientists do. You have a moratorium. And we should have a moratorium on AGI. This is what we

206
00:22:20,880 --> 00:22:29,280
need to do. And can you enforce a moratorium? Yeah, I mean, it's like technically, like physically,

207
00:22:29,280 --> 00:22:34,880
like, yeah, obviously, like, that's not that hard. Whether people will do that, whether people

208
00:22:34,880 --> 00:22:39,680
want to do that, whether people can overcome the incredible political power that big tech has,

209
00:22:39,680 --> 00:22:44,240
that's the more interesting question. It's not like the government obviously has the ability,

210
00:22:44,320 --> 00:22:48,640
like the CIA can track every GPU in the country if it wants to. Like, you know, if you want,

211
00:22:48,640 --> 00:22:52,960
if the NSA wants to shut down, just press a button. Like, that's not the problem. You know,

212
00:22:52,960 --> 00:22:57,520
if you want to throw a couple CEOs in jail, like, sure, like the FBI can do that. Like,

213
00:22:57,520 --> 00:23:02,560
physically, this is not a problem. It's a political problem. This is not a physical problem. This is

214
00:23:02,560 --> 00:23:07,120
a political problem. The political problem is, well, if you have legislation around this kind of

215
00:23:07,120 --> 00:23:11,840
stuff, well, we just saw what happens if you try to fire Sam Altman, you think he's going to be okay

216
00:23:11,840 --> 00:23:17,600
with a huge GPUs away? Well, no, I expect that's going to be a hard fight. I expect, you know,

217
00:23:17,600 --> 00:23:22,320
Microsoft lobbyists will fight that tooth and nail. I expect many people will fight this.

218
00:23:22,320 --> 00:23:25,760
And this is why, like, you know, I'm not, I'm not here to give point to you,

219
00:23:25,760 --> 00:23:30,320
paint you a rosy picture of the future. I'm not optimistic that things are going to go well.

220
00:23:30,320 --> 00:23:36,560
We have an unprecedentedly huge political problem here. I think I'd like to say is the thing that's

221
00:23:36,560 --> 00:23:44,880
killing us right now, it's not AGI. AGI doesn't exist yet. It's people. It's politics that is

222
00:23:44,880 --> 00:23:52,880
killing us. Right, right. But and to that point that AGI doesn't exist, not so much all the other,

223
00:23:53,520 --> 00:24:03,280
I mean, yes, no doubt, the political systems are not equipped to deal with the big problems facing

224
00:24:03,280 --> 00:24:11,600
humanity. But in this case, AGI doesn't exist. I don't know how you would ban AGI, because

225
00:24:11,600 --> 00:24:19,360
no one really knows how and when it might emerge, if it ever does. At the level of the tech now,

226
00:24:19,360 --> 00:24:27,680
I mean, what are you, what are you suggesting? And I'm not putting on the spot. I don't expect you

227
00:24:27,680 --> 00:24:33,520
to. Oh, I have policy proposals. I have very concrete policy proposals here, here are three.

228
00:24:35,120 --> 00:24:40,240
The first one is a compute cap. There should be a limitation that no single training around no

229
00:24:40,240 --> 00:24:44,880
single AI system can be built with more than a certain amount of compute. So luckily, we are,

230
00:24:44,880 --> 00:24:49,920
so we are very lucky that current frontier AI systems, more and more general purpose systems

231
00:24:49,920 --> 00:24:54,400
require more and more computing resources. These computing resources are very easy to track.

232
00:24:54,480 --> 00:24:59,440
They're very bulky. They take lots of specialized knowledge, lots of energy. The kinds of supercomputers

233
00:24:59,440 --> 00:25:03,920
that can train a GPT-4 or GPT-5 are only built by like three companies in the world,

234
00:25:03,920 --> 00:25:10,960
and they're all in the US. So like, this is a solvable problem. And we should put a ban on,

235
00:25:10,960 --> 00:25:15,680
you know, there should be like a registration process for, you know, frontier models up to a

236
00:25:15,680 --> 00:25:20,800
certain limit. And beyond that, there should be just ban, just a moratorium. Just you are not

237
00:25:20,800 --> 00:25:26,320
allowed to perform any experiment that requires more than, I don't know, 10 to the 24, 10 to 25

238
00:25:26,320 --> 00:25:36,000
or whatever, FLOPs. FLOP being a unit of measurement for computing power. And this is easily enforceable.

239
00:25:36,000 --> 00:25:40,000
This is absolutely something that like technically is enforceable with, it's just a political

240
00:25:40,000 --> 00:25:45,680
problem. And this buys you time. Then you're, our scientists figure out, you spend time actually

241
00:25:45,680 --> 00:25:50,800
figuring out how far is AGI away, how dangerous is it, how do we control the things, blah, blah,

242
00:25:50,800 --> 00:25:54,480
then we can talk about those kinds of things. The first thing is to buy time. The second

243
00:25:54,480 --> 00:26:02,880
proposal, or unless you want to comment on that. Well, just on that, you're talking about limiting

244
00:26:03,680 --> 00:26:13,040
commercial products. But if, when you say then that gives the research community time to figure

245
00:26:13,040 --> 00:26:18,960
these out, things out, they're going to have to experiment with larger models. So there's got to

246
00:26:18,960 --> 00:26:30,000
be some. To be clear, these levels are insane. 10 to the 24, 10 to the 25, FLOP is an unimaginably

247
00:26:30,000 --> 00:26:34,080
large amount of computing power. There are no academic labs, basically, that need this for

248
00:26:34,080 --> 00:26:41,120
research, FD research. This is ridiculous. There is just no, so this is a common propaganda piece

249
00:26:41,120 --> 00:26:45,040
that the big labs like to say is like, oh, we need more compute to do safety research.

250
00:26:45,760 --> 00:26:52,400
Maybe this is true. I have not seen it. This is just not what has actually happened. Just purely

251
00:26:52,400 --> 00:27:00,880
empirically speaking, there is, I have seen basically no safety AGI relevant research that

252
00:27:00,880 --> 00:27:07,120
required more than like, you know, a GPT-3 that you couldn't have done with GPT-3 level of compute

253
00:27:07,120 --> 00:27:11,760
or less. I have like, maybe it exists, but I sure as hell as I have not seen it.

254
00:27:13,760 --> 00:27:22,240
Okay. So limiting compute is one proposal. What are the others you mentioned?

255
00:27:22,880 --> 00:27:28,000
Two others I would recommend. The second is strict liability for model developers.

256
00:27:28,800 --> 00:27:34,320
So what this means, so strict liability means that the intentions of the developer do not matter.

257
00:27:34,960 --> 00:27:40,640
It would matter is that if a harm is caused, the developer is liable. I think this should

258
00:27:40,640 --> 00:27:46,560
basically exist for the whole supply chain is that if you create externalities, you have to pay for

259
00:27:46,560 --> 00:27:52,400
them. And this aligns the incentives of everyone aligned on the chain. Currently, there are no

260
00:27:52,400 --> 00:27:58,240
incentives for developers to develop to minimize the externalities of their systems. Currently,

261
00:27:58,240 --> 00:28:03,520
you as an open source developer can be an arbitrarily dangerous thing that causes arbitrarily

262
00:28:03,520 --> 00:28:09,200
much damage. And you have no incentive to avoid this. As a concrete example, which not even going

263
00:28:09,200 --> 00:28:15,840
to AGI is voice cloning systems. There are right now in GitHub, systems you can just download,

264
00:28:15,840 --> 00:28:20,320
which take you 15 seconds of your voice, clone it perfectly. And you know, go call your kids,

265
00:28:20,320 --> 00:28:25,200
call your wife, you know, just manipulate them, call in a swat hit on your, on you using your own

266
00:28:25,200 --> 00:28:30,560
voice. This is all doable. And the people developing these systems have zero liability.

267
00:28:30,640 --> 00:28:35,920
They don't even feel bad about it. Because it's open source, Craig. If it's open source, it must

268
00:28:35,920 --> 00:28:41,520
be good. My ideology says so. And you know when your ideology tells you something is morally right,

269
00:28:41,520 --> 00:28:48,880
then it's good as we've seen throughout history. So it's, so we have to align incentives here

270
00:28:48,880 --> 00:28:56,640
somewhere along the line, you know, if a, it reminds me of cars and seatbelts in the 70s, where

271
00:28:57,520 --> 00:29:05,600
car manufacturers fought tooth and nail to not have seatbelts. They fought it viciously with

272
00:29:05,600 --> 00:29:10,720
propaganda and with lawsuits and with everything they could throw at it. Because they said, well,

273
00:29:10,720 --> 00:29:14,720
it's the driver's fault. If he gets into an accident, it's not our fault. Like, you know,

274
00:29:14,720 --> 00:29:20,800
we just build cars. If they drive it poorly and they die, well, it's not our fault. And we, you

275
00:29:20,800 --> 00:29:25,920
know, the people rightfully told them to go fuck themselves. Like, no, you have to build a safe

276
00:29:25,920 --> 00:29:31,440
product. You can't like, it's, it's not a moral question. It's kind of like the point I want to

277
00:29:31,440 --> 00:29:38,480
make. I'm not making an ideological point. I'm not saying my religion says that seatbelts are good.

278
00:29:38,480 --> 00:29:46,000
I'm like, I don't care. I care. Do seatbelts mean that less people die? And the answer is, yeah,

279
00:29:46,000 --> 00:29:52,880
like they make cars safer. So then I want seatbelts. Cool. And the same thing applies to open

280
00:29:52,880 --> 00:29:59,600
source. Does Linux being open source result in more safety? The truth is, yeah, looks pretty

281
00:29:59,600 --> 00:30:05,440
obviously like case. So I'm in favor of Linux being open source. Awesome. Great. You know, does,

282
00:30:05,440 --> 00:30:10,560
you know, some seven billion parameter model be open source positive or negative? I don't know,

283
00:30:10,560 --> 00:30:15,840
probably positive. Like probably so. I'm not sure. Like there's a lot of downsides there as well.

284
00:30:15,840 --> 00:30:22,880
But like, seems like it probably is positive. AGI being positive, you know, open source, you know,

285
00:30:22,880 --> 00:30:27,680
that does not seem positive to me at all. That does not, that seems like a recipe for disaster.

286
00:30:27,680 --> 00:30:31,120
So it's, I'm not trying to make an ideological point is what I'm starting to say. I'm not saying

287
00:30:31,760 --> 00:30:34,880
all these things are good. All these things are bad. I'm saying we have to look at things

288
00:30:34,880 --> 00:30:40,000
at a case by case basis. This is how proper regulation works. Proper regulation shouldn't

289
00:30:40,000 --> 00:30:46,240
be ideological. It shouldn't be everything is regulated as ARB. That would be terrible regulation.

290
00:30:46,880 --> 00:30:54,800
Yeah. Well, so that was the capping of the compute on training runs,

291
00:30:56,320 --> 00:31:01,280
shifting liability to the model developer. What was the third one?

292
00:31:01,840 --> 00:31:05,840
So the third one that I think should be done is that there should be a kill switch.

293
00:31:05,920 --> 00:31:10,000
And what I mean by this is it doesn't have to be literally a switch. What I mean is there should

294
00:31:10,000 --> 00:31:18,160
be a protocol that any developer of frontier AI systems needs to implement by which at a given

295
00:31:18,160 --> 00:31:26,880
notice, any frontier training runs or deployments can be shut down in under a minute. So the reason

296
00:31:26,880 --> 00:31:30,800
for this is not per se because I need, I think necessarily that this would be very helpful.

297
00:31:30,800 --> 00:31:35,280
The AGI actually happens. If AGI actually happens, this is probably useless. The reason I think this

298
00:31:35,280 --> 00:31:40,800
is good is because we should have the institutional capacity to do these kinds of things. There should

299
00:31:40,800 --> 00:31:44,720
be every six months, there should be a fire alarm. There should be a fire drill where everyone has

300
00:31:44,720 --> 00:31:49,520
to practice. In the next five minutes, all AI companies have to go offline for 60 seconds.

301
00:31:49,520 --> 00:31:53,920
If not, you get slapped with a huge fine. These are the kinds of protocols you want to have

302
00:31:54,560 --> 00:31:59,280
in worlds where you have tail risks, where things can blow up, where you can have these

303
00:31:59,280 --> 00:32:05,600
kind of things. And then there should be a multilateral K of N kind of system around this.

304
00:32:05,600 --> 00:32:11,760
Like maybe all major global powers have one of these buttons and if three or five of them push it

305
00:32:11,760 --> 00:32:18,880
or seven of 10 or whatever, then the system kicks in. This is the kind of institutional

306
00:32:18,880 --> 00:32:23,120
building which doesn't save us, but it's a hell of a lot better than nothing.

307
00:32:23,840 --> 00:32:34,880
And how do you see these kinds of proposals moving through the policy making frameworks?

308
00:32:35,680 --> 00:32:46,640
There is some advance in the European Union. The White House has come out with its

309
00:32:47,440 --> 00:32:52,640
executive order, which as yet doesn't have any real concrete

310
00:32:55,440 --> 00:33:02,800
government governance policy in it, but it sort of lays out the things that we should

311
00:33:02,800 --> 00:33:14,320
be thinking about. Where do you see these things going? What sort of a timeline do you think that

312
00:33:14,320 --> 00:33:21,840
governments are being educated enough that they can deal with this? What government is

313
00:33:21,840 --> 00:33:29,120
going to lead? Is it the EU? Will it be the US? Who should it be? And then of course you've got

314
00:33:29,920 --> 00:33:36,880
the other side of the world, Russia and China, who have very different agendas and may not want

315
00:33:36,880 --> 00:33:43,520
to regulate at all. So when people ask me questions like this and they're like, what's your probability

316
00:33:43,520 --> 00:33:50,640
of X happening? And then my follow-up question is usually, is it X conditioned on me and other

317
00:33:50,640 --> 00:33:56,880
people doing something about it or not? Because I expect if they're conditioned on me and other

318
00:33:56,880 --> 00:34:01,120
people don't do anything about it, then yeah, I just think nothing will happen if big tech wins

319
00:34:01,120 --> 00:34:07,520
and then we die. I think it will be very heroic or special. It will just be new products keep

320
00:34:07,520 --> 00:34:11,600
happening, AI keep going up, and then just one day, humanity's not in control anymore and we

321
00:34:11,600 --> 00:34:15,920
have no idea what's going on. And then it's just over. I don't think it will be dramatic. I think

322
00:34:15,920 --> 00:34:20,320
we will just get more and more confused. We won't understand what's going on anymore. Weirder and

323
00:34:20,320 --> 00:34:27,600
weirder things will happen, more and more politics, economics, markets, media is controlled by AI,

324
00:34:27,600 --> 00:34:32,400
or even just fully generated by AI. There will be no more movies or just AI generated. And then

325
00:34:32,400 --> 00:34:36,640
just humanity will not be in control anymore. And then one day we fall over dead for some reason,

326
00:34:36,880 --> 00:34:42,000
we don't understand. That's what I expect will happen by default. And along the way to be clear,

327
00:34:42,000 --> 00:34:48,080
big tech will pick a lot of money. So go buy that Microsoft stock. You'll get really rich

328
00:34:48,080 --> 00:34:55,920
just before you die. So if I could addition on someone actually doing something about this,

329
00:34:55,920 --> 00:35:00,640
I do think there is hope. I don't think there's a lot of hope, but there is hope. And the main

330
00:35:00,640 --> 00:35:12,320
hope I see from this is that the general public fucking hates AI. It's unfathomable how much

331
00:35:12,320 --> 00:35:19,200
normal people hate AI. They use it, of course, but they're freaked out by it, which is just completely

332
00:35:19,200 --> 00:35:25,040
the correct reaction. It's just these crazy bizarre weirdo tech people like you and me

333
00:35:25,040 --> 00:35:30,560
who are not instantly like, wait, that's actually, let's not do that. If you talk to any normal

334
00:35:30,560 --> 00:35:34,480
person, you're like, hey, these people are building systems that are smarter than humans.

335
00:35:34,480 --> 00:35:42,560
They don't do that. That seems really dangerous. Don't do that. Well, all the people are like,

336
00:35:42,560 --> 00:35:51,760
oh, but actually you see my proposal because we'll make it fine. Or actually universal love

337
00:35:51,760 --> 00:35:57,600
means that AI systems will love whatever. I don't even know what these people say anymore.

338
00:35:57,600 --> 00:36:01,200
I think they've given up making arguments at this point and they're just vibing.

339
00:36:02,160 --> 00:36:09,440
So I don't even know if there's an argument that debunked there. So from my perspective,

340
00:36:10,240 --> 00:36:17,760
it's we are building systems. They are going to be built by default unless we do something about it.

341
00:36:17,760 --> 00:36:22,960
So the general public wants these systems to not be built, or at least for us to slow down,

342
00:36:22,960 --> 00:36:26,160
until we can make them safe and we understand them better and they've been integrated to society,

343
00:36:26,160 --> 00:36:30,480
et cetera, et cetera. So now you might ask the question, okay, well, that's true.

344
00:36:31,280 --> 00:36:36,320
Why is fuck all happening? And that's a good question. And now we have to talk about models

345
00:36:36,320 --> 00:36:43,920
of policy change and like global coordination, which at least how I think about this problem

346
00:36:43,920 --> 00:36:51,200
generally is that the general public actually does have power in the West and like in democratic

347
00:36:51,200 --> 00:36:57,120
countries. It's very fashionable among elites to sneer and be like, Oh, actually, you see the

348
00:36:58,400 --> 00:37:04,000
populace, you know, they don't have true control, you know, we live in a whatever the words are

349
00:37:04,000 --> 00:37:10,800
that people like to use. And this is to a large degree true, but it's not fully true.

350
00:37:11,600 --> 00:37:18,640
The main problem is that the general public has extremely short attention spans

351
00:37:18,720 --> 00:37:26,320
and extremely discoordinated. This is the main problem. The bottleneck on policy action currently

352
00:37:26,320 --> 00:37:34,400
is not will of the people. It's not ability to enforce regulation. It's coordination. It's

353
00:37:34,400 --> 00:37:39,360
getting people to actually do something about it, you know, to actually write letters to their

354
00:37:39,360 --> 00:37:43,600
senators, actually put things on their desks, actually yell at them on the phone, you know,

355
00:37:43,600 --> 00:37:48,240
actually like, you know, talk about on social media, et cetera, et cetera. This is the kind

356
00:37:48,240 --> 00:37:52,880
of thing that's currently missing basically campaigning. This is the kind of stuff that

357
00:37:52,880 --> 00:38:01,440
is missing. And I expect that if you did this well, if you raise this to saliency about people,

358
00:38:01,440 --> 00:38:04,400
you wouldn't have to you wouldn't have to convince them. And I'm saying this because

359
00:38:04,400 --> 00:38:08,240
empirically, this has been true in my experience, like talking to people and also like doing stuff

360
00:38:08,240 --> 00:38:12,320
like focus groups and stuff. I found that you don't really need to convince people very much.

361
00:38:12,320 --> 00:38:17,040
You mostly just have to tell them facts just have to, you know, just like present them with,

362
00:38:17,040 --> 00:38:23,120
hey, this is what's going on right now. And then mostly they converge to the like a reasonable

363
00:38:23,120 --> 00:38:29,600
beliefs around like, hey, that's scary, don't do that. So I think this is currently the best

364
00:38:29,600 --> 00:38:37,280
path we have. I'm also, you know, excited to talk to politicians and I talked to many of them,

365
00:38:37,280 --> 00:38:44,320
mostly in the UK and the EU, because I'm UK based. But it's hard because, you know, politicians have

366
00:38:44,320 --> 00:38:47,840
similar problems. They have very little attention span, because they have so many things they need

367
00:38:47,840 --> 00:38:56,720
to do. There's so many things haranguing them. And my model of policymakers is basically that the

368
00:38:56,720 --> 00:39:06,320
ultimate goal of a politician is to not get blamed. So it's because the politician you have

369
00:39:06,320 --> 00:39:11,120
really like I have so any if there's any policymakers listening or any staffers or so on,

370
00:39:11,200 --> 00:39:17,760
I feel you, you're in a shit spot, I get it. Because like, basically the way I see it is like

371
00:39:17,760 --> 00:39:23,200
there's like a two by two grid of like what you do as a politician, which you can do. So

372
00:39:23,920 --> 00:39:30,560
the idea is that there's a default action is that in a common, in our common, you know, feelings

373
00:39:30,560 --> 00:39:34,880
around issue, there's something that is the default thing to do, which is usually nothing.

374
00:39:35,840 --> 00:39:42,000
If you do the default action, and it goes wrong, well, you're not blamed, you know,

375
00:39:42,000 --> 00:39:47,600
because, you know, you did the sensible thing, not your fault. If you do the default action,

376
00:39:47,600 --> 00:39:53,440
and it goes, well, well, great, you're a genius, you know, good job. If you do the non default

377
00:39:53,440 --> 00:40:00,720
action, and it goes great, cool, yeah, you're good, great. If you do the non default action,

378
00:40:00,800 --> 00:40:07,920
and it goes bad, then you get blamed. That's how you get blamed. So you may notice from

379
00:40:07,920 --> 00:40:12,480
this payoff matrix, that it is always better to take the default action rather than non default

380
00:40:12,480 --> 00:40:19,280
action. It is always better for the politician to not stray off the path. And this is universally

381
00:40:19,280 --> 00:40:24,880
true. So it's easy to yell at politicians and be like they have no spine, they have no courage

382
00:40:24,880 --> 00:40:30,880
and whatever. And yeah, that's true for many of them. Many of them are just, yeah, just, you

383
00:40:30,880 --> 00:40:36,000
know, just don't care, true. But some do, and they do go off the path and they get burned for it.

384
00:40:36,720 --> 00:40:42,400
And that sucks. But it is how the game is. So what we can do as the people is we have to change

385
00:40:42,400 --> 00:40:47,760
what the default action is. You have to change the narrative from, I guess we just keep bumbling

386
00:40:47,760 --> 00:40:54,800
along until we die to how the fuck dare you keep bumbling, like seize your bumbling immediately.

387
00:40:55,360 --> 00:40:59,440
Bumbling is no longer accepted. And that's my biggest hope at the moment.

388
00:41:00,160 --> 00:41:09,280
Yeah. When we spoke last time, again, right as GPT four was being released.

389
00:41:09,280 --> 00:41:17,600
One of your immediate concerns was that these things can be hooked up to

390
00:41:19,680 --> 00:41:27,600
systems that can take action. And I don't remember if we talked about auto GPT that first,

391
00:41:28,480 --> 00:41:36,400
I haven't looked at what's happened with that, but that first attempt to create an agent that could

392
00:41:36,400 --> 00:41:44,960
use LLMs. But that has developed a pace. And we're now on the cusp of seeing

393
00:41:46,640 --> 00:41:53,120
sort of an explosion of AI agents that can leverage the power of large language models or other

394
00:41:53,840 --> 00:42:05,440
other tools. I had a guy on earlier from News Guard, a company that builds databases to

395
00:42:06,720 --> 00:42:12,320
try and help companies, tech companies identify

396
00:42:12,720 --> 00:42:24,160
disinformation and combat it. And we were talking about, once you have these agents building

397
00:42:27,600 --> 00:42:35,040
creating disinformation, not only creating the disinformation, but distributing it on a massive

398
00:42:35,040 --> 00:42:44,160
scale and maybe on a massively parallel scale. The internet, public discourse, everything is

399
00:42:44,160 --> 00:42:51,600
going to get very confusing because you're not going to be able to tell what's real and what's

400
00:42:51,600 --> 00:43:00,400
not real and people, which is the majority who are not particularly careful about where they're

401
00:43:00,400 --> 00:43:13,520
getting their information will be manipulated. So yeah, the coming AI agent era, how do you

402
00:43:13,520 --> 00:43:26,080
deal with that? I mean, I don't know, get your affairs in order. A number of years ago, post

403
00:43:26,080 --> 00:43:34,880
GPT-2 was around GPT-3 time. That's how we mark the eras now. Instead of years, we just use GPTs

404
00:43:34,880 --> 00:43:45,600
now. I was invited to work kind of like just like a discussion group with some open AI people,

405
00:43:45,600 --> 00:43:51,600
policy people, disinformation experts and stuff like this about the potential for misinformation

406
00:43:51,600 --> 00:43:58,800
and so on from language models, especially before GPT-4, before chat GPT and so on. And

407
00:44:00,720 --> 00:44:06,960
polite lead to all these well-credentialed experts with their triple Stanford professorships or

408
00:44:06,960 --> 00:44:14,560
Harvard, whatever, talk about misinformation, bias and whatever. And then when it came my turn

409
00:44:14,880 --> 00:44:23,040
to talk, my reaction was like, holy shit, you're all so undressed. You're being so optimistic.

410
00:44:23,040 --> 00:44:29,280
It's so much worse than any of you. You're like, oh, it could make it easier for far writers to

411
00:44:30,240 --> 00:44:35,120
that's that's that's fucking children's play compared to what you could do with these things.

412
00:44:35,120 --> 00:44:39,040
Like you were truly you're not creative. Like if you think that's the worst that can happen,

413
00:44:39,040 --> 00:44:42,800
oh, they're going to generate some fake news and some like Russian digital websites. I mean,

414
00:44:42,800 --> 00:44:46,880
oh boy, that would be nice. That's the nice timeline. It's going to be much worse than that.

415
00:44:46,880 --> 00:44:51,280
It's already getting worse like that. Talk about fully automated cults with fully

416
00:44:51,280 --> 00:45:00,720
automated profits. Talk about all sensory, illusionary interactive systems, creating

417
00:45:00,720 --> 00:45:05,040
full complex narratives that are completely disconnected from reality. Talk about full

418
00:45:05,040 --> 00:45:12,240
epistemic collapse, the semantic apocalypse. Even if AIs don't kill us, they're going to drive us

419
00:45:12,240 --> 00:45:20,160
insane. So it's because it will just be harder and harder and harder to survive in a more and more

420
00:45:20,160 --> 00:45:24,560
adversarial informational environment. This has already been happening for a very long time.

421
00:45:24,560 --> 00:45:28,880
You know, we just had Thanksgiving. And as much as we love her, we all have that one

422
00:45:28,880 --> 00:45:36,560
aunt that get way too into QAnon a while back. And imagine so, you know, currently stuff like

423
00:45:36,560 --> 00:45:40,880
QAnon or like, I don't even know if QAnon is still a thing, but like whatever the newest thing is,

424
00:45:40,880 --> 00:45:45,760
the newest cult is, the newest whatever is, you know, that affects, you know, some percentage of

425
00:45:45,760 --> 00:45:49,440
the population, you know, some percentage of the more vulnerable population. I'm going to say

426
00:45:49,440 --> 00:45:53,520
stupid, just like, you know, maybe emotionally vulnerable or like epistemically, you know,

427
00:45:53,520 --> 00:45:59,120
vulnerable and for some reason, not trying to judge these people here. Now imagine the bar keeps

428
00:45:59,120 --> 00:46:04,000
racing. You get systems that become more and more convincing, that become more and more

429
00:46:04,000 --> 00:46:09,520
sophisticated, more and more targeted, and slowly, slowly, the number of people who are just

430
00:46:09,520 --> 00:46:17,760
functionally schizophrenic keeps going up until at some point, people cannot converge on reality

431
00:46:17,760 --> 00:46:24,400
anymore. And people just every person you meet is functionally schizophrenic. You cannot run a society.

432
00:46:24,400 --> 00:46:31,200
You cannot organize a system if you and your neighbor cannot come to a conclusion about

433
00:46:31,200 --> 00:46:37,920
basic reality. This is like what is possible with these kinds of systems. I'm not saying this is

434
00:46:37,920 --> 00:46:42,560
going to happen next year. I mean, maybe, but this is the kinds of things you couldn't do.

435
00:46:43,120 --> 00:46:49,920
Like the like epistemics is hard. Like this is the thing that like, there's also things like honesty

436
00:46:49,920 --> 00:46:54,640
is hard. This is like some people are like, Oh, just, you know, misinformation is a trivial

437
00:46:54,640 --> 00:46:58,240
concept. It's almost become a slur at this point. It's kind of come a joke, you know, like when

438
00:46:58,240 --> 00:47:01,840
people use the word answer information, like, at least in my social circles, a lot of people like

439
00:47:01,840 --> 00:47:07,200
rolled their eyes to be like, Oh, yeah, anything that isn't big media isn't this information,

440
00:47:07,200 --> 00:47:13,520
whatever. But like, it's just not that easy. Like finding out what is true and disseminating and

441
00:47:13,520 --> 00:47:19,840
evaluating what is true is hard. This is very hard. It takes energy. It takes effort. It takes

442
00:47:19,840 --> 00:47:25,280
mechanisms. It takes like it's hard. And it's going to get harder. It's going to get more expensive.

443
00:47:25,280 --> 00:47:31,600
But currently, like, do you really know what's happening in Ukraine right now? Really? I don't.

444
00:47:32,080 --> 00:47:37,680
I think I'm at a point where it is like literally impossible for me to actually know what's going

445
00:47:37,680 --> 00:47:43,440
on in Ukraine. It's something that affects me, you know, affects family, friends, you know, it is

446
00:47:43,440 --> 00:47:49,680
a huge thing. I don't think that there is any way I could actually acquire and verify

447
00:47:50,560 --> 00:47:57,600
that the truth of what is actually going on there. And this generalizes. This is even before we get

448
00:47:57,600 --> 00:48:02,960
into agents doing worse things than this. I mean, automating all jobs, obviously, you know,

449
00:48:02,960 --> 00:48:08,160
anything you can do at a computer, an agent will do better and faster. So there will be complete

450
00:48:08,160 --> 00:48:12,880
economic collapse from that. Like, obviously, there will be no more need for human jobs unless

451
00:48:12,880 --> 00:48:17,200
until the inference costs, you know, get too high. But you know, you can improve those back down.

452
00:48:17,760 --> 00:48:24,960
You'll have systems that can do harm in various ways, you know, by manipulating markets,

453
00:48:25,040 --> 00:48:31,200
campaigns, politics, you're going to have systems that are, you know, cybercrime, hacking, you

454
00:48:31,200 --> 00:48:36,160
have system like it's like when you ask a question like, what is the worst thing agent based systems

455
00:48:36,160 --> 00:48:40,400
are doing? You're asking the question, what are the worst intelligence systems can do?

456
00:48:40,400 --> 00:48:44,640
What is the worst that a human can do? The answer is a lot.

457
00:48:44,960 --> 00:48:50,560
Yeah. But again,

458
00:48:53,600 --> 00:49:04,320
yeah, I mean, you can you can see that that very bleak future. But I'm also a great believer in

459
00:49:04,400 --> 00:49:06,880
in how

460
00:49:10,640 --> 00:49:24,960
mankind, the worst case scenario generally is not what happens. And people kind of muddle along.

461
00:49:25,920 --> 00:49:32,720
But that survivorship bias, there was a man named Stanislav Petrov, who was a Russian soldier

462
00:49:32,720 --> 00:49:37,920
stationed in a nuclear bunker. And he had the command that if American missiles appear on the

463
00:49:37,920 --> 00:49:45,680
screen, he shoots the missile. And one day, six missiles appeared on his screen. His commands

464
00:49:45,680 --> 00:49:50,560
were very clear. The second guy with him there who had, you know, the other key was ready to

465
00:49:50,560 --> 00:49:54,720
turn and yelled at him that it's time we have to shoot back. The Americans are attacking.

466
00:49:56,400 --> 00:50:02,400
And Stanislav didn't. He disobeyed orders. He could have been, you know, fucking executed for that.

467
00:50:02,400 --> 00:50:07,200
And he disobeyed orders that day. And it's because of this one man, one Russian soldier,

468
00:50:08,240 --> 00:50:14,160
that you and me weren't nuked. One guy, we got lucky. So when people said, oh, but so far as

469
00:50:14,160 --> 00:50:16,800
I was like, what the fuck are you talking about? This is like saying, well, I've played Russian

470
00:50:16,800 --> 00:50:22,160
roulette five times so far and it's been great. Let me pull again. That's just not how anything

471
00:50:22,160 --> 00:50:28,960
works. This is not how reality works. If you play like this, then eventually you predictably lose.

472
00:50:29,920 --> 00:50:35,360
You have to play strategies where you can win in adversarial environments where you can play,

473
00:50:35,360 --> 00:50:40,480
where you can win in games where dangers exist. Our ancestors, when they were in the wild,

474
00:50:40,480 --> 00:50:45,600
they couldn't be like, well, oh, my forefathers survived. So I don't have to worry about bears.

475
00:50:46,160 --> 00:50:50,720
You know, none of my forefathers got killed about bears. No, like that's just, no,

476
00:50:50,720 --> 00:50:55,840
this is not how things work. The world isn't nice. There is no arc of history. There is no God

477
00:50:55,840 --> 00:51:01,360
that is protecting us. The fact that we are here today is because of the hard work of our ancestors.

478
00:51:02,000 --> 00:51:06,560
The fact that I live in this nice, you know, warm apartment, sound like safe that I have enough food

479
00:51:06,560 --> 00:51:13,920
to eat and so on is not God that gave me that. It's not some, you know, force of nature. It was

480
00:51:13,920 --> 00:51:21,840
the hard scrabble and bloody fight of my ancestors that left me this. And if I let this to rot,

481
00:51:21,840 --> 00:51:29,520
if me and other people don't maintain society, then it just dies. Like then entropy wins.

482
00:51:30,160 --> 00:51:37,200
Entropy always increases and entropy is death. So if we just sit back and hope things will go well,

483
00:51:37,760 --> 00:51:44,960
they will not. So, you know, I was gonna, I was thinking, well, that's a good place to end it,

484
00:51:44,960 --> 00:51:54,320
but I don't want to end there because our last conversation got an inordinate number of views.

485
00:51:55,280 --> 00:52:05,120
And I have some producers that take these and turn them into shorts and they have these sound bites

486
00:52:06,080 --> 00:52:15,200
from that episode that have gotten an enormous number of views because people gravitate towards

487
00:52:15,200 --> 00:52:26,240
these doomsday proclamations. And I don't, I mean, whether or not they're true, I want to end

488
00:52:26,960 --> 00:52:34,800
something more hopeful. So what, what, what should people do in your view? What should

489
00:52:34,800 --> 00:52:42,960
regulators be doing? What should researchers be doing? What should Microsoft be doing now?

490
00:52:43,680 --> 00:52:49,280
So the weirdest thing I would tell them is like, to be clear, I don't like being the doomed guy.

491
00:52:49,280 --> 00:52:53,680
I absolutely don't like this. I was the pecto optimist throughout my entire life. I was always

492
00:52:53,680 --> 00:52:57,760
the person saying, no, we can fix problems. Climate change is solvable. You know, solar

493
00:52:57,760 --> 00:53:01,760
powers can be exponentially cheaper. You know, we can do carbon capture with like, there are so

494
00:53:01,760 --> 00:53:06,320
many things we can do. I've always been saying like, no, like, you know, see how in the interest

495
00:53:06,320 --> 00:53:10,720
improved education, how much people are becoming, you know, better at, you know, and having more

496
00:53:10,720 --> 00:53:16,560
access to information. Look at how so many things are like, I was just reading the other day about

497
00:53:16,560 --> 00:53:23,360
how slowly over decades, just the flash freezing of frozen food has gotten better. And I've noticed

498
00:53:23,360 --> 00:53:28,160
this, just like my frozen broccoli, I'll make it night. It's just a little bit nicer. And you

499
00:53:28,160 --> 00:53:32,160
know what, that might sound like a teeny thing compared to all these other things, but I think

500
00:53:32,160 --> 00:53:37,920
that's beautiful. I think it's extremely beautiful that life gets better. All things being equal,

501
00:53:37,920 --> 00:53:44,080
life has gotten a lot better. I'm very happy to be alive when I am right now. All these small things

502
00:53:44,080 --> 00:53:48,640
done by these smart people, mostly done for profit. Sure, the broccoli company, they just want profit,

503
00:53:48,640 --> 00:53:53,280
but ultimately, they made my dinner a little bit nicer. It was already fine. Like I was already

504
00:53:53,280 --> 00:53:57,680
surviving, but it was a little bit nicer. And you know what, that's awesome. And it's so nice

505
00:53:57,680 --> 00:54:04,080
that we can live this way. The truth is, is that we are so lucky that we live in a society full of

506
00:54:04,080 --> 00:54:11,040
educated, smart people that for the most part, you know, not all over more angels, they're not heroes,

507
00:54:11,760 --> 00:54:15,280
but they want to make, they do want to leave the world better, you know, they want people to be

508
00:54:15,280 --> 00:54:20,400
happy, they want people to be safe. Most things being equal, you know, almost everyone, you know,

509
00:54:20,400 --> 00:54:24,000
given the option, if they could just help someone else and it didn't cost them anything,

510
00:54:24,000 --> 00:54:29,600
they'd do it. And that's really nice. So we have to leverage this. We have to leverage that we,

511
00:54:29,600 --> 00:54:35,200
and this is not the case everywhere in the world, I want to say. This is something that even today

512
00:54:35,200 --> 00:54:41,200
is not in every country. It is not in every place or in every society. But in the West and, you know,

513
00:54:41,200 --> 00:54:47,680
many other countries in the Far East and so on, most people are educated. Most people are decent.

514
00:54:47,760 --> 00:54:53,920
Again, I'm saying they're great or heroes, but they're decent. And they want the world to go well.

515
00:54:53,920 --> 00:54:58,880
They want their kids to grow up and have a nice life and, you know, eat nice frozen, you know,

516
00:54:59,600 --> 00:55:06,320
broccoli, you know, whatever, you know, they want to see art and beauty and, you know, music and so

517
00:55:06,320 --> 00:55:11,280
on. And we can have this. This is the important thing to understand. The important thing is,

518
00:55:11,280 --> 00:55:16,240
sometimes I'll talk about this, is that like this, this idea of techno optimism, quote unquote,

519
00:55:16,240 --> 00:55:20,560
it's just cynicism and disguise. This is a really important thing to understand.

520
00:55:20,560 --> 00:55:25,280
These people who put, who's talking about, oh, yeah, actually, we're, we're techno optimists,

521
00:55:25,280 --> 00:55:30,720
we're accelerationists or whatever. They're just cynics. They're just libertarian cynics

522
00:55:30,720 --> 00:55:36,720
that don't believe that society can be improved, except by just like giving themselves to this

523
00:55:36,720 --> 00:55:42,080
abstract process of technology. But technology is not a force of nature. It's not a thing

524
00:55:42,080 --> 00:55:48,240
happening to us. It's a thing that we do. It's like, it's about humanity. It's not about technology

525
00:55:48,240 --> 00:55:52,640
that like, sure, technology is great, it's helped humans, but I only care about technology because

526
00:55:52,640 --> 00:55:57,360
I care about humans, care about people. And we all care about people. We care about our families,

527
00:55:57,360 --> 00:56:02,400
we care about our friends. And technology should be a tool. It should not be a goal in and of itself.

528
00:56:02,400 --> 00:56:07,760
So when people talk about, well, AGI is inevitable, someone's going to do it. No, no, it is not.

529
00:56:08,320 --> 00:56:13,840
It is not inevitable. It is not a force of nature. It's a decision we make. It is a decision we make

530
00:56:13,840 --> 00:56:23,120
and we can do better. We can, as people, societies, as civilizations, make choices. We can say, hey,

531
00:56:23,120 --> 00:56:29,520
let's be a little more careful. That doesn't mean we'll do not do any AI anymore. We can just say,

532
00:56:29,520 --> 00:56:35,280
hey, give our scientists a couple more years, a couple more decades to understand the mathematics

533
00:56:35,280 --> 00:56:39,760
of interpretability better, and then maybe we'll give it another shot, you know, like we did with

534
00:56:39,760 --> 00:56:45,040
human cloning. These are what's important. I'm not saying that this is easy or that this is what's

535
00:56:45,040 --> 00:56:50,320
going to happen. It's because it's not what's going to happen by default. But it's just important

536
00:56:50,320 --> 00:56:57,200
that there is this poison in our society that believes that the future is already decided.

537
00:56:57,760 --> 00:57:04,240
And it is not. The future is not yet decided. We still have a choice. It is not yet too late

538
00:57:05,280 --> 00:57:12,160
for what it will be soon. Hi, I wanted to jump in and give a shout out to our sponsor,

539
00:57:12,160 --> 00:57:20,960
NetSuite, by Oracle. I'm a journalist and getting a single source of truth is nearly impossible.

540
00:57:20,960 --> 00:57:26,800
If you're a business owner, having a single source of truth is critical to running your

541
00:57:26,800 --> 00:57:37,360
operations. If this is you, you should know these three numbers. 36,000, 25,1. 36,000,

542
00:57:37,360 --> 00:57:43,920
because that's the number of businesses that have upgraded to NetSuite by Oracle. NetSuite is the

543
00:57:43,920 --> 00:57:50,560
number one cloud financial system, streamlining, accounting, financial management, inventory,

544
00:57:50,560 --> 00:57:59,600
HR, and more. 25, because NetSuite turns 25 this year. That's 25 years of helping businesses do

545
00:57:59,600 --> 00:58:06,880
more with less, close their books in days, not weeks, and drive down costs. One, because your

546
00:58:06,880 --> 00:58:14,320
business is one of a kind. So you get a customized solution for all of your KPIs in one efficient

547
00:58:14,320 --> 00:58:21,840
system with one source of truth. Manage risk, get reliable, forecast, and improve margins.

548
00:58:21,840 --> 00:58:29,120
Everything you need, all in one place. As I said, I'm not the most organized person in the

549
00:58:29,120 --> 00:58:35,920
world, and there's real power to having all of the information in one place to make better decisions.

550
00:58:36,880 --> 00:58:42,160
This is an unprecedented offer by NetSuite to make that possible.

551
00:58:43,520 --> 00:58:49,760
Right now, download NetSuite's popular KPI checklist, designed to give you consistently

552
00:58:49,760 --> 00:59:02,000
excellent performance, absolutely free at netsuite.com slash I on AI. That's I on AI, E-Y-E-O-N-A-I,

553
00:59:02,080 --> 00:59:10,880
all run together. Go to netsuite.com slash I on AI to get your own KPI checklist.

554
00:59:10,880 --> 00:59:22,880
Again, that's netsuite.com slash I on AI, E-Y-E-O-N-A-I. They support us, so let's support them.

555
00:59:23,680 --> 00:59:33,200
That's it for this episode. I want to thank Connor for his time. If you want to read a transcript

556
00:59:33,200 --> 00:59:42,160
of the conversation, you can find one on our website, I on AI. That's E-Y-E-O-N-A-I.

557
00:59:43,120 --> 00:59:54,080
As I always say, the singularity may not be near, but AI is changing your world, changing it rapidly,

558
00:59:54,800 --> 01:00:02,000
so pay attention.

