1
00:00:00,000 --> 00:00:06,840
What's called AI today has departed to basically pure engineering.

2
00:00:07,600 --> 00:00:14,880
It's designed in such a, the large language models are designed in such a way that in

3
00:00:14,880 --> 00:00:21,660
principle, they can't tell you anything about language learning, cognitive

4
00:00:21,660 --> 00:00:28,140
processes generally, they can produce useful devices like what I'm using, but

5
00:00:28,140 --> 00:00:33,360
the very design ensures that you'll never understand, they'll never lead

6
00:00:33,360 --> 00:00:36,680
to any contribution to science.

7
00:00:37,520 --> 00:00:41,000
That's not a criticism anymore than I'm criticizing.

8
00:00:41,040 --> 00:00:43,480
Camptons this week.

9
00:00:43,480 --> 00:00:48,060
I talked to Noam Chomsky, one of the preeminent intellectuals of our time.

10
00:00:48,620 --> 00:00:54,440
Our conversation touched on the dichotomy between understanding and application

11
00:00:54,440 --> 00:00:56,800
in the field of artificial intelligence.

12
00:00:57,340 --> 00:01:02,740
Chomsky argues that AI has shifted from a science aimed at understanding

13
00:01:02,740 --> 00:01:09,560
cognition to a pure engineering field focused on creating useful, but not

14
00:01:09,560 --> 00:01:13,080
necessarily explanatory tools.

15
00:01:13,600 --> 00:01:19,400
He questions whether neural nets truly mirror how the brain functions and whether

16
00:01:19,400 --> 00:01:22,600
they exhibit any true intelligence at all.

17
00:01:23,520 --> 00:01:29,120
He also suggests that advanced alien life forms would likely have language

18
00:01:29,120 --> 00:01:34,320
structured similar to our own, allowing us to communicate.

19
00:01:35,720 --> 00:01:42,200
Chomsky is 94 and I reached him at home where he appeared with a clock hanging

20
00:01:42,220 --> 00:01:44,520
omnestly over his head.

21
00:01:45,400 --> 00:01:49,280
I hope you enjoy the conversation as much as I did.

22
00:01:50,040 --> 00:01:50,680
Well, thanks.

23
00:01:50,680 --> 00:01:51,880
You're in California.

24
00:01:52,400 --> 00:01:55,760
Actually, I'm in Arizona, which is on California time.

25
00:01:56,080 --> 00:01:56,440
Yeah.

26
00:01:56,560 --> 00:01:56,800
Yeah.

27
00:01:56,800 --> 00:01:57,600
Oh, wonderful.

28
00:01:58,120 --> 00:01:59,120
Uh, yeah.

29
00:01:59,120 --> 00:02:05,880
So, uh, you know, I wanted to talk to you because you have the, uh, you know, one

30
00:02:05,880 --> 00:02:11,640
of the few people, uh, with a deep understanding of, of, uh, linguistics and,

31
00:02:11,800 --> 00:02:19,000
uh, natural language processing that has the historical knowledge, uh, of, of

32
00:02:19,160 --> 00:02:26,080
where we are, how we got to where we are and what, uh, that might mean for the future.

33
00:02:26,560 --> 00:02:35,240
Uh, I, I understand the, the, uh, your criticisms of deep learning, uh, and,

34
00:02:35,280 --> 00:02:43,400
and what large language models are not in terms of, uh, reasoning and, and, uh, you

35
00:02:43,400 --> 00:02:47,680
know, understanding the, the, the underpinnings of, uh, language.

36
00:02:48,280 --> 00:02:53,920
But, uh, I, I thought maybe I could ask you to talk about how this developed.

37
00:02:53,920 --> 00:02:59,440
I mean, going back to Minsky's, uh, thesis at Princeton, when he was, you know,

38
00:02:59,440 --> 00:03:06,720
before he turned against the perceptron, when he was talking about, uh, nets as,

39
00:03:06,720 --> 00:03:11,680
uh, a possible model for, uh, biological processes in the brain.

40
00:03:11,680 --> 00:03:19,360
And then, you know, how did, how you see that things developed and what were

41
00:03:19,360 --> 00:03:25,680
the, the failures that didn't get to where presumably, uh, you would have wanted

42
00:03:25,680 --> 00:03:31,120
that research to go, uh, and then, and then I have some other questions.

43
00:03:31,120 --> 00:03:33,720
But, but, but is that enough to get started?

44
00:03:35,200 --> 00:03:38,360
Well, let's, let's take an analogy.

45
00:03:39,320 --> 00:03:50,040
Suppose you're interested in figuring out how, uh, insects navigate biological

46
00:03:50,040 --> 00:03:50,720
problem.

47
00:03:51,440 --> 00:04:00,640
So, uh, one thing you can do is say, let's try to study in detail what the

48
00:04:01,320 --> 00:04:07,880
desert ants are doing in my backyard, how they're using solar azimuths and so on

49
00:04:07,880 --> 00:04:08,600
and so forth.

50
00:04:09,600 --> 00:04:12,400
Something else you could do is say, look, it's easy.

51
00:04:13,480 --> 00:04:20,120
I'll just build an automobile which can navigate, uh, fine, does better than the

52
00:04:20,120 --> 00:04:20,880
desert ants.

53
00:04:21,440 --> 00:04:22,360
So who cares?

54
00:04:23,200 --> 00:04:27,600
Uh, well, those are the two forms of artificial intelligence.

55
00:04:28,640 --> 00:04:31,360
One is what Minsky was after.

56
00:04:32,360 --> 00:04:41,480
It's now kind of ridiculed as good old fashioned AI, go fi, we're past that stage.

57
00:04:42,160 --> 00:04:44,720
Now we just build things that do it better.

58
00:04:45,600 --> 00:04:46,080
Okay.

59
00:04:46,960 --> 00:04:51,080
Like, uh, an airplane does better than an eagle.

60
00:04:51,120 --> 00:04:53,200
So who cares about how eagles fly?

61
00:04:54,560 --> 00:04:55,640
Yeah, that's possible.

62
00:04:56,320 --> 00:05:01,480
But, uh, it's a difference between totally different goals.

63
00:05:02,440 --> 00:05:09,240
Roughly speaking, science and engineering, it's not a sharp difference, but first

64
00:05:09,240 --> 00:05:14,880
approximation, either you're interested in understanding something or you're just

65
00:05:14,880 --> 00:05:19,080
interested in building something that'll work for some purpose.

66
00:05:19,840 --> 00:05:24,440
So they're both fine occupations, nothing wrong with.

67
00:05:24,920 --> 00:05:29,680
I mean, when you say I'm criticism of the large, criticizing the large language

68
00:05:29,680 --> 00:05:31,080
models, that's not correct.

69
00:05:31,640 --> 00:05:33,200
I'm using them right now.

70
00:05:33,800 --> 00:05:35,200
I'm reading captions.

71
00:05:36,480 --> 00:05:43,040
Captions are based on deep learning, clever programming, very useful.

72
00:05:43,520 --> 00:05:46,040
I'm hard of hearing, so they're very helpful to me.

73
00:05:46,560 --> 00:05:47,520
No criticism.

74
00:05:48,200 --> 00:05:53,720
But if somebody comes along and says, okay, this explains language, you tell them

75
00:05:53,720 --> 00:06:01,000
it's kind of like saying an airplane explains how eagles fly, the wrong question.

76
00:06:01,680 --> 00:06:04,680
It's not intended to lead to any understanding.

77
00:06:05,240 --> 00:06:07,480
It's intended to be for a useful purpose.

78
00:06:08,360 --> 00:06:09,080
That's fine.

79
00:06:09,800 --> 00:06:10,800
No criticism.

80
00:06:11,520 --> 00:06:18,760
And what's called AI today has departed to basically pure engineering.

81
00:06:19,480 --> 00:06:26,600
It's designed in such a, the large language models are designed in such a way that

82
00:06:26,600 --> 00:06:33,560
in principle, they can't tell you anything about language, learning, cognitive

83
00:06:33,560 --> 00:06:39,120
processes generally, they can produce useful devices like what I'm using.

84
00:06:39,760 --> 00:06:45,000
But the very design ensures that you'll never understand, they'll never

85
00:06:45,000 --> 00:06:48,600
lead to any contribution to science.

86
00:06:49,400 --> 00:06:53,680
That's not a criticism anymore than I'm criticizing champions.

87
00:06:54,360 --> 00:07:00,600
Jeff Hinton says, you know, his goal is to understand the brain, how the brain works.

88
00:07:01,520 --> 00:07:13,120
And he talks about AI as we know it today, supervised learning and generative AI as

89
00:07:13,120 --> 00:07:22,400
useful by products, but that are not his goal or not the goal of cognitive

90
00:07:22,400 --> 00:07:26,320
science or computational biology.

91
00:07:28,360 --> 00:07:36,440
Was there a point at which you think the research lost a bead or is there research

92
00:07:37,080 --> 00:07:43,960
going on that people aren't paying attention to that, that is not caught up in the

93
00:07:43,960 --> 00:07:49,360
usefulness of these other kinds of neural nets?

94
00:07:51,800 --> 00:07:56,600
Well, first of all, if you're interested in how the brain works, the first question

95
00:07:56,600 --> 00:08:00,360
you ask is, does it work by neural nets?

96
00:08:02,040 --> 00:08:03,320
That's an open question.

97
00:08:04,120 --> 00:08:09,720
There's plenty of critical analysis that argues that neural nets are not what's

98
00:08:09,720 --> 00:08:12,280
involved, even in simple things like memory.

99
00:08:13,480 --> 00:08:20,840
Actually, these arguments that go back to Helmholtz, the neural transmission is pretty

100
00:08:20,840 --> 00:08:24,520
slow as compared with the ordinary memory.

101
00:08:25,000 --> 00:08:30,840
There's much hard for criticism by people like Randy Gallistel, cognitive

102
00:08:30,840 --> 00:08:37,400
neuroscientist, who's given pretty sound arguments that neural nets in principle

103
00:08:38,280 --> 00:08:48,760
don't have the ability to capture the core notion of a Turing machine,

104
00:08:50,520 --> 00:08:53,640
computational capacity, they just don't have that capacity.

105
00:08:54,440 --> 00:09:02,200
And he's argued that the computational capacity is in much richer computational

106
00:09:02,200 --> 00:09:10,200
systems in the brain, internal delves, where there's very rich computational capacity,

107
00:09:10,200 --> 00:09:15,720
goes wavy on neural net, some experimental evidence to support this.

108
00:09:16,360 --> 00:09:19,640
So if you're interested in the brain, that's the kind of thing you look at.

109
00:09:20,600 --> 00:09:23,640
Not just saying, can I make bigger neural nets?

110
00:09:24,840 --> 00:09:29,080
It's okay if you want to try it, but maybe it's the wrong place to look.

111
00:09:29,640 --> 00:09:33,000
So the first question is, is it even the right place to look?

112
00:09:33,960 --> 00:09:37,480
That's an open question in neuroscience.

113
00:09:38,360 --> 00:09:43,720
If you take a vote among neuroscientists, almost all of them think that neural nets

114
00:09:43,720 --> 00:09:49,080
are the right place to look, but you don't solve scientific questions by a vote.

115
00:09:50,360 --> 00:10:00,200
Yeah, one of the things that's obvious is neural nets, they may be a model,

116
00:10:00,200 --> 00:10:07,000
they may mimic a portion of brain activity, but there are so many other structures.

117
00:10:08,040 --> 00:10:13,320
There's all kind of stuff going on in the brain, way down to the cellular level,

118
00:10:13,320 --> 00:10:16,040
there's chemical interactions, plenty of other things.

119
00:10:16,920 --> 00:10:23,400
So maybe you'll learn something by studying neural nets, if you do, fine, everybody will be happy,

120
00:10:24,200 --> 00:10:29,880
but maybe that's not the place to look if you want to study even simple things like just

121
00:10:30,520 --> 00:10:32,200
memory and associations.

122
00:10:33,080 --> 00:10:41,240
There's now already evidence of associations internal to large cells in the hippocampus,

123
00:10:42,040 --> 00:10:49,240
internal to them, which means maybe something's going on at a deeper level where there's vastly

124
00:10:49,240 --> 00:10:51,080
more computational capacity.

125
00:10:53,240 --> 00:10:54,840
Those are serious questions.

126
00:10:55,720 --> 00:11:01,400
So there's nothing wrong with trying to construct models and learn something from them, if you can, fine.

127
00:11:02,040 --> 00:11:09,480
The building larger models, which is kind of the rage in the engineering side of AI right now,

128
00:11:10,440 --> 00:11:13,000
does produce remarkable results.

129
00:11:13,000 --> 00:11:16,520
I mean, what was your reaction when you saw

130
00:11:18,600 --> 00:11:29,000
chat GPT or GPT-4 or any of these models, that it's just a sort of clever stochastic parent

131
00:11:29,000 --> 00:11:32,120
or that there was something deeper?

132
00:11:32,200 --> 00:11:41,720
If you look at the design of the system, you can see it's like an airplane explaining flying.

133
00:11:42,600 --> 00:11:43,720
There's nothing to do with it.

134
00:11:44,360 --> 00:11:51,720
In fact, it's immediately obvious, trivially obvious, not a deep point, that it can't be

135
00:11:51,720 --> 00:11:52,840
teaching us anything.

136
00:11:53,560 --> 00:11:54,840
The reason is very simple.

137
00:11:55,800 --> 00:12:04,680
The large learning models work just as well for impossible languages that children can't acquire

138
00:12:05,320 --> 00:12:07,960
as for the languages they're trained on.

139
00:12:08,840 --> 00:12:16,040
So it's as if a biologist came along and said, I got a great new theory of organisms, lists a

140
00:12:16,040 --> 00:12:22,680
lot of organisms that possibly exist, a lot that can't possibly exist.

141
00:12:22,680 --> 00:12:24,760
And I can tell you nothing about the difference.

142
00:12:26,680 --> 00:12:30,040
I mean, that's not a contribution to biology.

143
00:12:30,840 --> 00:12:33,480
It doesn't meet the first minimal condition.

144
00:12:34,360 --> 00:12:40,760
The first minimal condition is distinguish between what's possible from what's not possible.

145
00:12:41,480 --> 00:12:42,440
You can't do that.

146
00:12:43,720 --> 00:12:45,480
It's not a contribution to science.

147
00:12:46,280 --> 00:12:51,560
If it was a biologist making that proposal, you'd just laugh.

148
00:12:52,600 --> 00:12:58,600
Why shouldn't we just laugh when an engineer from Silicon Valley says the same thing?

149
00:13:00,200 --> 00:13:02,200
So maybe they're fun.

150
00:13:02,200 --> 00:13:04,200
Maybe they're useful for something.

151
00:13:04,200 --> 00:13:05,320
Maybe they're harmful.

152
00:13:06,040 --> 00:13:10,120
Those are the kinds of questions you ask about pure technology.

153
00:13:11,080 --> 00:13:13,000
So take large language models.

154
00:13:13,640 --> 00:13:15,160
There are something they're useful.

155
00:13:16,040 --> 00:13:18,440
In fact, I'm using them right at this minute.

156
00:13:19,720 --> 00:13:20,280
Captions.

157
00:13:21,080 --> 00:13:23,000
It's very helpful for people like me.

158
00:13:24,760 --> 00:13:25,640
Are they harmful?

159
00:13:26,280 --> 00:13:28,040
Yeah, they can cause a lot of harm.

160
00:13:28,920 --> 00:13:34,440
Disinformation, defamation, brain on human gullibility.

161
00:13:35,160 --> 00:13:36,120
Plenty of examples.

162
00:13:37,400 --> 00:13:38,840
So they can cause harm.

163
00:13:38,840 --> 00:13:39,880
They can be of use.

164
00:13:40,840 --> 00:13:45,400
Those are the kinds of questions you ask about pure engineering,

165
00:13:46,120 --> 00:13:48,440
which can be very sophisticated and clever.

166
00:13:49,240 --> 00:13:54,200
I mean, the internal combustion engine is a very sophisticated device,

167
00:13:55,560 --> 00:14:01,160
but we don't expect it to tell us anything about how a gazelle runs.

168
00:14:02,840 --> 00:14:04,120
It's just the wrong question.

169
00:14:05,080 --> 00:14:18,120
Yeah, although I talk a lot to Jeff Hinton, and you'll be the first to concede that back propagation

170
00:14:19,560 --> 00:14:20,920
there's no evidence of that.

171
00:14:20,920 --> 00:14:26,040
And in fact, there's a lot of evidence that it wouldn't work in the brain.

172
00:14:29,080 --> 00:14:30,440
Reinforcement learning.

173
00:14:30,440 --> 00:14:39,000
You know, I spoke in a rich Sutton, that's been accepted as by a lot of people as

174
00:14:40,200 --> 00:14:49,160
an algorithmic model for brain activity in part of the brain, in the lower brain.

175
00:14:50,040 --> 00:15:02,840
So in terms of exploring the mechanisms of the brain, it seems that there is some usefulness.

176
00:15:02,840 --> 00:15:08,920
I mean, it says, you said there's, on the one hand, people look at the principles,

177
00:15:10,840 --> 00:15:16,920
and then they built through engineering, just as the analogy of a bird to an airplane,

178
00:15:17,560 --> 00:15:23,800
they've taken some of the principles and applied it through engineering and created something useful.

179
00:15:24,360 --> 00:15:33,000
But there are scientists that are looking at what's been created, like Hinton's criticism

180
00:15:33,000 --> 00:15:41,800
of back propagation, and are looking for other models that would fit with the principles they see

181
00:15:41,800 --> 00:15:46,920
in cognitive science or in the brain.

182
00:15:46,920 --> 00:15:52,040
And I mentioned this forward-forward algorithm, which you said you hadn't looked at.

183
00:15:52,040 --> 00:16:04,520
But I found it compelling in that it doesn't require signals to be passing back through

184
00:16:04,840 --> 00:16:18,440
the neurons. I mean, they pass back, but then stimulate other neurons as you move forward

185
00:16:18,440 --> 00:16:30,760
in time. But I mean, is there nothing that's been learned in the study of AI or the research

186
00:16:31,720 --> 00:16:32,600
of neural nets?

187
00:16:35,160 --> 00:16:42,920
But if you can find anything, it's great. Nothing against search, but it's just,

188
00:16:43,720 --> 00:16:51,160
but we have to remember what you asked about chatbots. What do we learn from them? Zero.

189
00:16:52,040 --> 00:17:02,280
For the simple reason that the systems work as well for impossible languages as for possible ones.

190
00:17:03,160 --> 00:17:10,840
So it's like the biologist with the new theory that has organisms and impossible ones and can't

191
00:17:10,840 --> 00:17:16,520
tell the difference. Now, maybe by the look at these systems, you'll learn something about

192
00:17:16,520 --> 00:17:24,280
possible organisms. Okay, great. All in favor of learning things. But there's no issues.

193
00:17:25,080 --> 00:17:33,480
It's just that the systems themselves, there are great claims by some of the leading figures in

194
00:17:33,480 --> 00:17:43,720
the field. We've solved the problem of language acquisition, namely zero contribution, because

195
00:17:43,720 --> 00:17:52,280
the systems work as well for impossible languages. Therefore, they can't be telling you anything about

196
00:17:52,280 --> 00:18:00,040
language acquisition. Period. Maybe they're useful for something else. Okay, let's take a look.

197
00:18:01,160 --> 00:18:09,400
Well, maybe for the audience that this is going out to, you know, I understand what you mean by

198
00:18:09,400 --> 00:18:17,000
impossible, impossible, but could you just give a brief synopsis of what you mean by impossible

199
00:18:17,000 --> 00:18:25,720
languages for people that haven't read your work? Well, I mean, there are certain general properties

200
00:18:26,600 --> 00:18:37,320
that every infant knows, already tested down to two years old, no evidence, couldn't have evidence.

201
00:18:38,280 --> 00:18:47,400
So one of the basic properties of language is that the linguistic rules apply to structures,

202
00:18:47,960 --> 00:18:54,280
not linear strings. So if you want to take a sentence like

203
00:18:54,600 --> 00:19:09,080
instinctively, birds that fly swim, it means instinctively they swim, not instinctively they

204
00:19:09,080 --> 00:19:18,920
fly. Well, the adverb instinctively has to find a verb to attach to. It skips the closest verb

205
00:19:19,560 --> 00:19:25,400
and finds the structurally closest ones. That principle turns out to be universal

206
00:19:26,040 --> 00:19:33,240
for all structures, all constructions, and all languages. What it means is that an infant

207
00:19:34,040 --> 00:19:43,560
from birth, as soon as you can test automatically, disregards linear order and disregards 100% of

208
00:19:43,560 --> 00:19:50,840
what it hears, notice, as all we hear is words in linear order, but you disregard that and you

209
00:19:50,840 --> 00:19:59,320
deal only with abstract structures in your mind, which you never hear. Take another simple example,

210
00:19:59,320 --> 00:20:09,240
take the friends of my brothers are in England. Who's in England? The friends of the brothers,

211
00:20:10,200 --> 00:20:16,280
the friends, not the brothers, the one that's adjacent, you just disregard all the linear

212
00:20:16,280 --> 00:20:23,640
information. It means you disregard everything you hear, everything, and you pay attention only

213
00:20:23,640 --> 00:20:31,160
to what your mind constructs. That's the basic, most fundamental property of language. Well,

214
00:20:31,160 --> 00:20:38,040
you can make up impossible languages that work with what you hear. Simple rule, take the first

215
00:20:38,760 --> 00:20:47,000
relevant thing, associate them. Friends of my brothers are here, brothers are the closest things,

216
00:20:47,000 --> 00:20:53,800
and the brothers are here. Trivial rule, much simpler than the rule we use. You can construct

217
00:20:53,800 --> 00:21:00,840
languages that use only those simple rules that will be based on the linear order of what we hear.

218
00:21:01,800 --> 00:21:09,640
Well, maybe children, people could acquire them as a puzzle somehow using non-linguistic

219
00:21:09,640 --> 00:21:16,360
capacities, but they're not what children, infants, reflexively construct with no evidence.

220
00:21:17,320 --> 00:21:24,600
Well, there's many things like this, impossible and impossible languages. Well, nobody's tried

221
00:21:24,600 --> 00:21:30,120
it out because it's too obvious how it's going to turn out. You take a large language model,

222
00:21:30,120 --> 00:21:37,800
apply it to one of these models, systems that use linear order. Of course, it's going to work fine,

223
00:21:38,360 --> 00:21:43,560
trivial rules. Well, that's a refutation of the system.

224
00:21:45,560 --> 00:21:51,000
Meaning that if you trained it on an impossible language, it would produce impossible languages.

225
00:21:51,000 --> 00:21:56,200
How would you mean? Well, you don't even have to train it because the rules are simple.

226
00:21:56,760 --> 00:22:02,840
Yeah. Rules are much simpler than the rules of language. Like taking things that are,

227
00:22:02,840 --> 00:22:11,080
take the example, the friends of my brother are here. The way we actually do it is we don't say,

228
00:22:11,720 --> 00:22:19,240
take the noun phrase that's closest. We don't do that. That would be trivial, but we don't do it.

229
00:22:19,240 --> 00:22:24,520
What we say is first construct the structure in your mind, friends of my brothers,

230
00:22:25,160 --> 00:22:31,080
then figure out that the central element in that structure is friends, not brothers.

231
00:22:31,720 --> 00:22:38,520
And then let's let it be talking about the head of it. It's a pretty complicated computation,

232
00:22:39,160 --> 00:22:45,720
but that's the one we do instantaneously and reflexively. And we ignore, and we never see it,

233
00:22:45,720 --> 00:22:52,520
hear it, remember? We don't hear structures. All we hear is words in linear order. What we hear

234
00:22:52,600 --> 00:22:59,400
is words in linear order. We never use that information. We use only the much more looks

235
00:22:59,400 --> 00:23:05,960
like complex. If you think about it computationally, it's actually simpler, but that's a deeper

236
00:23:05,960 --> 00:23:12,200
question, which is why we do it. To move to a different dimension, there's a reason for this.

237
00:23:13,160 --> 00:23:19,880
The reason has to do with the theory of computation. You're trying to construct

238
00:23:20,840 --> 00:23:28,680
an infinite array of structured expressions. Simplest way to do that, the simplest computational

239
00:23:28,680 --> 00:23:35,560
procedure is binary set formation. But if you use binary set formation, you're just going to get

240
00:23:35,560 --> 00:23:42,040
structures, not order. So what the brain is doing is the simplest computational system,

241
00:23:42,920 --> 00:23:50,920
which happens to be very much harder to use. Nature doesn't care about that. Nature constructs

242
00:23:50,920 --> 00:23:59,240
the simplest system, doesn't care about it, if it's hard to use or not. I mean, you know, nature

243
00:23:59,240 --> 00:24:05,880
could have saved us a lot of trouble if it had developed eight fingers instead of 10.

244
00:24:06,840 --> 00:24:14,040
Then we'd have a much better base for computation. But nature didn't care about that when it developed

245
00:24:14,040 --> 00:24:21,160
10 fingers. If you look at evolution, it pays no attention to function. It just constructs the

246
00:24:21,160 --> 00:24:27,320
best system at each point. There's a lot of misleading talk about that. But if you just think

247
00:24:27,320 --> 00:24:35,160
about the physics of evolution, say a bacterium swallows another organism,

248
00:24:36,760 --> 00:24:43,800
the basis for what became complex cells, and nature doesn't get the new system,

249
00:24:44,440 --> 00:24:50,600
it reconstructs it in the simplest possible way. It doesn't pay any attention to how

250
00:24:50,680 --> 00:24:59,240
complex organisms are going to behave, not what nature can do. And that's the way evolution works

251
00:24:59,240 --> 00:25:07,240
all the way down the line. So not surprisingly, nature constructed language so that it's

252
00:25:07,800 --> 00:25:16,200
computationally elegant, but dysfunctional, hard to use in many ways. Not nature's problem,

253
00:25:16,920 --> 00:25:22,920
just like every other aspect of nature. You can think of a way in which you can do it better,

254
00:25:22,920 --> 00:25:32,920
but it didn't happen stage by stage. Two questions from that. So your view is that

255
00:25:33,960 --> 00:25:38,920
artificial intelligence, as it's being called, and particularly generative AI,

256
00:25:39,800 --> 00:25:47,160
doesn't exhibit true intelligence. Is that right? I wouldn't even say that.

257
00:25:47,880 --> 00:25:56,680
It's irrelevant to the question of intelligence. It's not its problem. A guy who designs a jet plane

258
00:25:57,320 --> 00:26:06,920
is not trying to answer the question, how do eagles fly? So to say, well, it doesn't tell us how

259
00:26:06,920 --> 00:26:16,920
eagles fly is the wrong question to ask. It's not the goal. Except what people are struggling with

260
00:26:16,920 --> 00:26:25,880
right now. You've heard the existential threat argument that these models, if they get large

261
00:26:25,880 --> 00:26:32,600
enough, they'll actually be more intelligent than humans. That's science fiction. I mean,

262
00:26:32,600 --> 00:26:40,280
there is a theoretical possibility. You can give a theoretical argument that, in principle,

263
00:26:42,360 --> 00:26:52,520
a complex system with vast search capacity could conceivably turn into something that would start

264
00:26:52,520 --> 00:27:02,200
to do things that you can't predict, maybe beyond. But that's even more remote than some

265
00:27:03,880 --> 00:27:10,040
distant asteroid, maybe someday hitting the earth, could happen. I mean, if you read a

266
00:27:10,040 --> 00:27:18,120
serious scientist on this, like Max Tagmark, his book on the three levels of intelligence,

267
00:27:19,080 --> 00:27:27,560
does give a sound theoretical argument as to how a massive system could, say,

268
00:27:29,320 --> 00:27:38,280
run through all the scientific discoveries in history, maybe find out some better way of

269
00:27:40,120 --> 00:27:44,920
developing them and use that better way to design something new which would destroy us all.

270
00:27:45,880 --> 00:27:53,080
It's, in theory, possible, but it's so remote from anything that's available that it's a waste of

271
00:27:53,080 --> 00:28:00,040
time to think about it. Yeah, so your view is that whatever threat exists from

272
00:28:01,080 --> 00:28:10,520
generative AI, it's the more mundane threat of disinformation. Disinformation, defamation,

273
00:28:11,480 --> 00:28:20,760
gullibility, Gary Marcus has done a lot of work on this, real cases, those are problems. I mean,

274
00:28:20,760 --> 00:28:29,800
you may have seen that there was a, sort of as a joke, people, somebody developed a defamation of the

275
00:28:29,880 --> 00:28:40,840
pope, put an image of the pope, somebody could do it for you, duplicate your face so it looks more

276
00:28:40,840 --> 00:28:50,120
or less like your face, pretty much duplicate your voice, develop a robot that looks kind of like you,

277
00:28:50,120 --> 00:28:56,360
have you say some insane thing, it would be hard only an expert could tell whether it was you or

278
00:28:56,360 --> 00:29:03,720
none. It's like this was done already several times, but basically is a joke.

279
00:29:04,440 --> 00:29:09,720
When powerful institutions get started on it, it's not going to be a joke.

280
00:29:14,520 --> 00:29:21,640
Another argument that's swirling around these large language models is the question of

281
00:29:22,600 --> 00:29:30,200
a sentence of whether if the model is large enough, and this goes a little bit back to how

282
00:29:30,200 --> 00:29:36,520
there's a lot more going on in the brain than the neural network or the cerebral cortex, but

283
00:29:38,520 --> 00:29:46,280
that there is the potential for some kind of sentence, not necessarily equivalent to human

284
00:29:46,280 --> 00:29:55,720
sentence. These are vacuous questions. It's like asking, does a submarine really swim?

285
00:29:57,000 --> 00:30:03,560
You want to call that swimming? Yeah, it swims. You don't want to call it swimming? It's not a

286
00:30:03,560 --> 00:30:13,080
substantive question. Well, in the sense that it supports the view that there's no separation between

287
00:30:13,320 --> 00:30:22,200
consciousness and the material activities of the brain. There's a separation that hasn't

288
00:30:22,200 --> 00:30:30,840
been believed since the 17th century. John Locke, after Newton's demonstration, said, well leaves

289
00:30:30,840 --> 00:30:39,400
us only with the possibility that thinking is some property of organized matter. That's the 17th

290
00:30:39,400 --> 00:30:51,000
century. Yeah, okay. But the belief in a soul and consciousness is something separate from a

291
00:30:51,000 --> 00:30:59,640
material biology. It persists. The belief in all kinds of things. But within the rational part

292
00:30:59,720 --> 00:31:08,200
of the human species, once Newton demonstrated that the mechanical model doesn't work,

293
00:31:09,160 --> 00:31:17,560
there's no material universe in the only sense that was understood. Locke took the obvious conclusion,

294
00:31:17,560 --> 00:31:25,800
said, well, since matter, as Mr. Newton has demonstrated, has properties that we cannot

295
00:31:25,800 --> 00:31:32,520
conceive of. They're not part of our intuitive picture. Since matter has those properties,

296
00:31:33,800 --> 00:31:39,240
organized matter can also have the property of thought. This was investigated all through the

297
00:31:39,240 --> 00:31:48,600
18th century. Ended up finally with Joseph Priestley, a philosopher in the late 18th century,

298
00:31:48,600 --> 00:31:58,040
gave pretty extensive discussions of how material, organized material objects could have

299
00:31:58,040 --> 00:32:05,080
properties of thought. You can even find it in Darwin's early notebooks. It was kind of forgotten

300
00:32:05,080 --> 00:32:12,280
after that. Rediscovered in the late 20th century as some radical new discovery,

301
00:32:12,840 --> 00:32:20,040
astonishing hypothesis. Matter can think. Of course it can. In fact, we're doing it right now.

302
00:32:20,920 --> 00:32:27,160
But the only problem then is to find out what's involved in what we call thinking,

303
00:32:28,120 --> 00:32:34,440
what we call sentience, what are the properties of whatever matter is. We don't know what matter is,

304
00:32:34,440 --> 00:32:41,560
but whatever it turns out to be, whatever constitutes the world, what physicists don't

305
00:32:41,560 --> 00:32:49,800
know, but whatever it is, there's something organized. Elements of it can have various properties,

306
00:32:50,440 --> 00:32:56,920
like the properties that we are now using, properties that we call sentience. Then the question

307
00:32:56,920 --> 00:33:04,920
whether something else has sentience is as interesting as whether airplanes fly. If you're

308
00:33:04,920 --> 00:33:11,880
talking English, airplanes fly. If you're talking Hebrew, airplanes glide, they don't fly.

309
00:33:13,160 --> 00:33:19,560
It's not a substantive question. What metaphors do we like?

310
00:33:21,800 --> 00:33:30,840
But what you're saying then is that neural net may not be the engineering solution, but

311
00:33:31,800 --> 00:33:41,880
that eventually it may be possible to create a system outside of the human brain that can think

312
00:33:43,960 --> 00:33:51,640
whatever thinking means. And do what we call thinking. But whether it thinks or not is like

313
00:33:51,640 --> 00:34:00,520
asking the airplanes fly, not a substantive question. We shouldn't waste time on questions

314
00:34:00,520 --> 00:34:06,760
that are completely meaningless. Going back to the history then,

315
00:34:09,160 --> 00:34:14,120
you know, Minsky was very interested in the possibility of neural nets as a

316
00:34:17,720 --> 00:34:24,680
computational model. In Minsky's time, it looked as if neural nets were the right place to look.

317
00:34:25,480 --> 00:34:30,840
Now I think it's not so obvious, especially because of Galastal's work,

318
00:34:31,800 --> 00:34:37,720
which is not accepted by most neuroscientists, but seems to me pretty compelling.

319
00:34:38,760 --> 00:34:43,800
Can you talk a little bit about that because I haven't read that and I'm guessing our readers

320
00:34:43,800 --> 00:34:52,040
haven't, our listeners haven't. Galastal is not the only one. Roger Penrose is another

321
00:34:53,000 --> 00:34:58,920
Nobel Prize winning physicist, but a number of people have pointed out Galastal mostly that

322
00:34:59,800 --> 00:35:07,960
have argued, I think plausibly, that the basic component of a computational system,

323
00:35:09,000 --> 00:35:15,560
the basic element of essentially a Turing machine, cannot be constructed from neural nets.

324
00:35:16,520 --> 00:35:23,640
So you have to look somewhere else with a different form of computation. And he's also

325
00:35:23,640 --> 00:35:30,840
pointed out, but in fact, it's true that there's much richer computational capacity in the brain

326
00:35:30,840 --> 00:35:38,680
than neural nets, even internal to a cell. There's massive computational capacity

327
00:35:39,640 --> 00:35:46,040
intercellular. So maybe that's involved in computation. And then there's by now some

328
00:35:46,600 --> 00:35:52,920
experimental work, I think, giving some evidence for this, but it's a problem for neuroscientists

329
00:35:52,920 --> 00:36:00,520
to work on. I'm not an expert in the field. I'm looking at it from the outside,

330
00:36:01,240 --> 00:36:06,040
so don't take my opinion too seriously. But to me, it looks pretty compelling.

331
00:36:06,680 --> 00:36:13,320
But whatever it is, neural nets or something else, yes, some organization of them, of whatever

332
00:36:13,320 --> 00:36:19,800
is there, is giving us the capacity to do what we're doing. So if you're a scientist, what you do is

333
00:36:21,160 --> 00:36:28,200
approach it in two different ways. One is you try to find the properties of the system.

334
00:36:28,840 --> 00:36:35,000
What is the nature of the system? That's first step kind of thing I was talking about before with

335
00:36:35,640 --> 00:36:43,480
structure dependence. What are the properties of the system that an infant automatically

336
00:36:43,480 --> 00:36:49,960
develops in the mind? And there's a lot of work on that. From the other point of view, you can say,

337
00:36:50,760 --> 00:36:57,080
what can we learn about the brain that relates to this? Actually, there is some work. So there is

338
00:36:57,080 --> 00:37:10,920
neurophysiological studies which have shown that for artificial languages that violate the principle

339
00:37:11,640 --> 00:37:18,120
that I mentioned, this structure dependent principle, if you train people on those,

340
00:37:19,080 --> 00:37:25,480
the ordinary language centers don't function. You get diffuse functioning of the brain,

341
00:37:25,480 --> 00:37:33,720
means they're being treated as puzzles basically. So you can find some neurological correlates of

342
00:37:34,440 --> 00:37:40,360
some of the things that are discovered by looking at the nature of the phenotype.

343
00:37:41,640 --> 00:37:48,840
But it's very hard for humans for a number of reasons. We know a lot about human, the physiology

344
00:37:48,920 --> 00:37:57,480
of human vision. But the reason is because of invasive experiments with nonhumans, cats,

345
00:37:58,680 --> 00:38:06,200
monkeys and so on. Can't do that for language. There aren't any other organisms unique to humans.

346
00:38:07,000 --> 00:38:13,480
So there's no comparative studies. You can think of a lot of invasive experiments which

347
00:38:13,560 --> 00:38:21,240
teach you a lot. You can't do them for ethical reasons. So study of the neurophysiology of

348
00:38:22,120 --> 00:38:31,240
human cognition is a uniquely hard problem. In its basic elements like language,

349
00:38:31,240 --> 00:38:38,920
it's just unique to the species. And in fact, a very recent development in evolutionary history,

350
00:38:38,920 --> 00:38:45,880
probably the last couple of hundred thousand years, which is nothing. So you can't do the

351
00:38:45,880 --> 00:38:50,760
invasive experiments for ethical reasons. You can think of them, but you can't do them,

352
00:38:50,760 --> 00:38:58,120
fortunately. And there's no comparative evidence. So it's much harder to do. You have to do things

353
00:38:58,120 --> 00:39:07,160
like, you know, looking at a blood flow in the brain, MRI type things, electrical stimulation,

354
00:39:07,160 --> 00:39:13,560
looking from the outside. It's tough. It's not like doing the kind of experiments you can think of.

355
00:39:14,600 --> 00:39:22,200
So it's very hard to find out the neurophysiological basis for things like use of language. But

356
00:39:22,920 --> 00:39:27,240
it's one way to proceed. And the other way to proceed is learn more about the phenol.

357
00:39:27,800 --> 00:39:38,120
It's like chemistry for hundreds of years. You just postulated the existence of atoms.

358
00:39:39,560 --> 00:39:44,120
Nobody could see them. You know, why are they there? You know, because unless

359
00:39:44,840 --> 00:39:51,640
there are atoms with the Dalton's properties, you don't explain anything. Early genetics,

360
00:39:52,600 --> 00:39:58,120
early genetics work before anybody had any idea what a gene is. You just looked at the

361
00:39:59,800 --> 00:40:04,040
properties of the system, try to figure out what must be going on.

362
00:40:05,400 --> 00:40:12,600
It's the way astrophysics works. You know, most of science works like that. So this does too.

363
00:40:13,320 --> 00:40:21,000
When you talk about invasive exploration, there are tools that are increasingly

364
00:40:21,080 --> 00:40:28,920
sophisticated. I'm thinking of neural link, Elon Musk's startup that has these super fine

365
00:40:31,080 --> 00:40:39,080
electrodes that can be put into the brain without damaging individual neurons.

366
00:40:40,520 --> 00:40:46,280
There's actually, I think, much more advanced than that is work that's being done with

367
00:40:47,240 --> 00:40:54,360
patients under brain surgery. Under brain surgery, with the brain basically exposed,

368
00:40:54,360 --> 00:41:03,160
there are some noninvasive procedures that can be used to study what particular

369
00:41:04,600 --> 00:41:09,480
parts of the brain, even particular neurons are doing. It's very delicate work.

370
00:41:10,440 --> 00:41:18,280
But there is some work going on. One person is working on it is Andrea Moro,

371
00:41:18,280 --> 00:41:23,560
the same person who designed the experiments that I described before about impossible languages.

372
00:41:24,360 --> 00:41:33,320
That seems to me a promising direction. There's other kinds of work. I could mention some of it.

373
00:41:33,320 --> 00:41:43,000
Alec Moran, why you was doing interesting studies that shed some light on the very

374
00:41:43,000 --> 00:41:52,600
elementary function. How do words get stored in the brain? What's going on in the brain that

375
00:41:54,040 --> 00:42:01,400
tells us that blake is a possible word, but the nick isn't for an English speaker. It is for

376
00:42:01,480 --> 00:42:06,840
an Arabic speaker. What's going on in the brain that deals with that?

377
00:42:08,280 --> 00:42:18,040
Hard work. David Peppel, another very good neuroscientist, has found evidence

378
00:42:19,000 --> 00:42:28,920
for things like pharyngeal structure in the brain. But the kinds of invasive experiments

379
00:42:28,920 --> 00:42:36,200
you can dream of, you can think of, he's just not allowed to do. So you have to try it in much

380
00:42:36,200 --> 00:42:44,280
indirect ways. Do you think that understanding cognition has advanced in your lifetime?

381
00:42:45,080 --> 00:42:52,120
And are you hopeful that we'll eventually really understand how the brain thinks?

382
00:42:52,200 --> 00:43:04,360
Well, there's been vast improvement in understanding the phenotype that we know a great deal about

383
00:43:04,360 --> 00:43:13,320
that was not known even a few years ago. There's been some progress in the neuroscience of

384
00:43:14,120 --> 00:43:25,880
the relates to it, but it's much harder. Yeah. I'm just curious about where you are in,

385
00:43:26,840 --> 00:43:34,040
not physically you're in Arizona, but where you are in your thinking. Are you still

386
00:43:34,200 --> 00:43:49,320
pushing forward in trying to understand language in the brain or are you sort of retired, so to speak,

387
00:43:49,320 --> 00:43:56,280
at this point? No, very much involved. I mean, I don't work on the neurophysiology.

388
00:43:56,600 --> 00:44:08,120
A man I mentioned, Andrea Moro, happens to be a good friend. So I follow the work they're doing,

389
00:44:08,120 --> 00:44:15,240
we interact, but my work is just on the phenotype. What's the nature of the system?

390
00:44:15,880 --> 00:44:21,480
And there, I think we're learning a lot. I'm right in the middle of papers at the moment,

391
00:44:22,440 --> 00:44:28,840
looking at more subtle, complex properties. The idea is essentially to find

392
00:44:30,040 --> 00:44:38,760
what I said about binary set formation. How can we show that from the simplest

393
00:44:39,800 --> 00:44:49,080
computational procedures, we can account for the apparently complex and apparently varied

394
00:44:49,880 --> 00:44:56,200
properties of the language systems. There's a fair amount of progress on that,

395
00:44:57,640 --> 00:45:05,480
that was unheard of 20, 30 years ago. So this is all new. Understanding is one thing and then

396
00:45:06,120 --> 00:45:18,520
re-creating it through computation in external hardware is another. Is that a blind ally or do

397
00:45:18,520 --> 00:45:27,320
you think that? Well, at the moment, I don't see any particular point in it. If there is some point,

398
00:45:28,280 --> 00:45:37,240
okay. I mean, the kinds of things that we're learning about the nature of language,

399
00:45:38,360 --> 00:45:43,720
I suppose you could construct some sort of system that would duplicate them,

400
00:45:45,320 --> 00:45:54,520
but it doesn't seem any obvious point to it. It's like taking chemistry in 100 years ago and saying,

401
00:45:55,400 --> 00:45:58,760
can I construct models that will look sort of like,

402
00:45:59,480 --> 00:46:06,840
suppose you took, I was saying, a kick of the diagram for an organic molecule

403
00:46:07,960 --> 00:46:14,680
and study its properties. You could presumably construct a mechanical model

404
00:46:15,480 --> 00:46:21,880
that would do some of those things. Would it be useful? Apparently chemists didn't think so, but

405
00:46:22,520 --> 00:46:27,400
if it would, okay. If it wouldn't, then don't.

406
00:46:30,120 --> 00:46:35,080
Nonetheless, I mean, we are using neural nets even in this call.

407
00:46:37,080 --> 00:46:46,440
Do you see, I mean, setting inside the question of whether or not they help us understand anything

408
00:46:46,440 --> 00:46:54,600
about the brain. Are you excited at all in about the promise that these large

409
00:46:54,600 --> 00:46:58,680
models hold? I mean, because they do something very useful.

410
00:46:59,800 --> 00:47:07,960
They are. Like I said, I'm using it right now. I think it's fine for me, somebody who can't hear

411
00:47:08,040 --> 00:47:16,040
to be able to read what you're saying pretty accurately. It's an achievement, so great.

412
00:47:17,400 --> 00:47:25,160
I have nothing against technology. And who do you think is going to carry on

413
00:47:27,400 --> 00:47:34,840
your work from here? I mean, are there any students of yours who you think we should

414
00:47:34,920 --> 00:47:42,120
be paying attention to? Well, quite a lot. A lot of young people doing fine work.

415
00:47:43,000 --> 00:47:48,600
In fact, I work with a, closely with a small research group

416
00:47:50,920 --> 00:47:58,520
by now, spread all over the world. We meet virtually from Japan and Holland and other places

417
00:47:58,520 --> 00:48:02,200
regularly working on the kinds of problems I was talking about.

418
00:48:03,640 --> 00:48:07,960
But right now, I should say it's a pretty special interest. Most linguists aren't

419
00:48:07,960 --> 00:48:15,880
interested in these foundational questions. But I think that's happened to be my interest.

420
00:48:15,880 --> 00:48:24,360
I want to see if we can show the, ultimately try to show that language is essentially a

421
00:48:24,360 --> 00:48:33,960
natural object. I mean, there was an interesting paper written about the time that I started

422
00:48:33,960 --> 00:48:42,840
working on this by Albert Einstein in 1950. He had an article in Scientific American, which

423
00:48:43,560 --> 00:48:49,320
I read, but didn't appreciate at the time, began to appreciate later, in which he talked

424
00:48:49,320 --> 00:48:56,440
about what he called a miracle creed. He has an interesting history. It goes back to Galileo.

425
00:48:57,320 --> 00:49:07,080
Galileo had a maxim saying, nature is simple. It doesn't do things in a complicated way if it

426
00:49:07,080 --> 00:49:15,720
could do them in a simple way. Galileo's maxim couldn't prove it. But they said, I think that's

427
00:49:15,720 --> 00:49:22,840
the way it is. That's the task of the scientist to prove it. Well, over the centuries, it's been

428
00:49:22,840 --> 00:49:30,840
substantiated case after case. It shows up in Leibniz's principle of optimality. But by then,

429
00:49:30,840 --> 00:49:38,760
there was a lot of evidence for it. By now, it's just a norm for science. It's what Einstein called

430
00:49:38,760 --> 00:49:48,600
the miracle creed. Nature is simple. Our task is to show it. It says, improve it. Skeptic can say,

431
00:49:48,600 --> 00:49:56,920
I don't believe it. Okay. But that's the way science works. Well, this one's worked the same way for

432
00:49:56,920 --> 00:50:04,200
language. But you couldn't have proposed that 50 years ago, 20 years ago. I think now you can

433
00:50:04,520 --> 00:50:13,000
believe that maybe language is just basically a perfect computational system at its base.

434
00:50:14,120 --> 00:50:20,680
You look at the phenomena, it doesn't look like that. But the same was true of biology. Go back to

435
00:50:20,680 --> 00:50:32,200
the 1950s, 1960s, biologists assumed that organisms could vary so widely that each one has to be

436
00:50:32,200 --> 00:50:39,400
studied on its own without bias. By now, that's all forgotten. It's recognized that there,

437
00:50:40,040 --> 00:50:46,360
since the Cambrian explosion, there's virtually no variation in the kinds of organisms,

438
00:50:47,160 --> 00:50:55,800
fundamentally all the same. Deep homologies, and so on. So even been proposed that there's a universal

439
00:50:55,800 --> 00:51:03,880
genome, not totally accepted, but not considered ridiculous. Well, I think we're in the same

440
00:51:03,880 --> 00:51:09,640
direction as the study of language. Now, let me say again, there's not many linguists interested in

441
00:51:09,640 --> 00:51:17,000
this. Most linguists, like most biologists, are studying particular things, which is fine. You

442
00:51:17,000 --> 00:51:27,240
learn a lot that way. But I think it is possible now to formulate a plausible thesis that language is a

443
00:51:27,960 --> 00:51:34,040
natural object like others, which evolved in such a way as to have perfect design,

444
00:51:34,840 --> 00:51:40,520
but to be highly dysfunctional. Because that's true of natural objects, generally. It's part of

445
00:51:40,520 --> 00:51:49,400
the nature of evolution, which doesn't take into account possible functions. I mean, the last stage

446
00:51:49,400 --> 00:51:57,560
of evolution, the reproductive success that does take function into account, natural selection,

447
00:51:58,520 --> 00:52:05,560
that's a fringe of evolution. It's just the peripheral fringe, very important, not denigrated,

448
00:52:05,560 --> 00:52:13,720
but it's the basic part of evolution is constructing the optimal system that meets

449
00:52:13,720 --> 00:52:20,280
the physical conditions established by some disruption in the system. That's the core of

450
00:52:20,280 --> 00:52:30,840
evolution. That's what Turing studied. Darcy Thompson, others by now, I think it's understood.

451
00:52:30,840 --> 00:52:37,160
And I think maybe the study of this particular biology after a language is a biological object.

452
00:52:37,800 --> 00:52:44,440
So why should it be different? Let's see if we can show it. There's been a lot of talk in the news

453
00:52:44,440 --> 00:52:55,720
recently about extraterrestrial craft having been found by the government. I don't put much

454
00:52:55,720 --> 00:53:04,920
talk in it, but imagine that there is extraterrestrial life, advanced forms of life. Do you think that

455
00:53:04,920 --> 00:53:15,800
their language would have developed the same way if it's based on these simple principles? Or is it

456
00:53:16,600 --> 00:53:24,840
could there be other forms of language in other biological organisms that would be quote unquote

457
00:53:24,840 --> 00:53:33,720
impossible in the human context? Back around the 1960s, I guess, Minsky

458
00:53:35,960 --> 00:53:42,680
studied with one of his students, Daniel Belbrom, studied the simplest Turing machines,

459
00:53:43,560 --> 00:53:52,040
few estates, fewest symbols, and asked what happens if you just let them run free?

460
00:53:53,000 --> 00:54:02,680
Well, it turned out that most of them crash, either get into endless loops or just crash

461
00:54:03,480 --> 00:54:09,080
don't proceed. But the ones that didn't crash all produced the successor function.

462
00:54:10,680 --> 00:54:18,280
So he suggested what we're going to find if any kind of intelligence develops is

463
00:54:18,840 --> 00:54:25,000
it'll be based on the successor function. And if we want to try to communicate with some

464
00:54:25,960 --> 00:54:31,160
extraterrestrial intelligence, we should first see if they have the successor function

465
00:54:32,200 --> 00:54:36,600
and then maybe build up from there. Well, turns out the successor

466
00:54:38,200 --> 00:54:47,480
happens to be what you get from the simplest possible language. The language is one symbol

467
00:54:47,480 --> 00:54:53,000
and the simplest form of binary set formation basically gives it a successor function.

468
00:54:53,880 --> 00:54:59,480
Add a little bit more to it, you get something like arithmetic. Add a little bit more to it,

469
00:54:59,480 --> 00:55:07,880
you get something like the poor properties of language. So it's conceivable that if there is any

470
00:55:08,520 --> 00:55:14,920
extraterrestrial intelligence, it would have pursued the same course. Where it goes from there,

471
00:55:14,920 --> 00:55:22,920
we don't know enough to say. And back to the idea that there is no super natural realm,

472
00:55:22,920 --> 00:55:31,240
that the consciousness is an emergent property from the physical attributes of the brain.

473
00:55:33,880 --> 00:55:42,840
Do you believe in a higher intelligence behind the creation or continuation of the universe?

474
00:55:45,640 --> 00:55:55,320
I don't see any point in vacuous hypotheses. If you want to believe it okay, it has no consequences.

475
00:55:57,880 --> 00:56:03,720
But do you believe it? No, I don't see any point in believing things for

476
00:56:05,160 --> 00:56:11,560
which there's no evidence and do no work. And another thing I've always wanted to ask

477
00:56:12,200 --> 00:56:20,840
someone like you, clearly your intelligence surpasses most peoples.

478
00:56:21,960 --> 00:56:27,560
I don't think so. Well, that's a good, that's interesting that you would say that. You think

479
00:56:27,560 --> 00:56:35,160
it's just a matter of applying yourself to study throughout your career.

480
00:56:35,160 --> 00:56:45,880
I have certain talents, I know, like not believing things just cause people believe them.

481
00:56:48,120 --> 00:56:55,160
And keeping an open mind and looking for arguments and evidence, not

482
00:56:56,360 --> 00:57:01,160
anything we've been talking about when meaningless questions are proposed, like

483
00:57:01,160 --> 00:57:09,560
our other organism, sentient or the submarine swim, I say let's discard them and look at

484
00:57:09,560 --> 00:57:19,240
meaningful questions. If you just pursue common sense like that, I think you can make some progress.

485
00:57:19,240 --> 00:57:25,960
Same on the questions we're talking about language. If you think it through, there's every reason why

486
00:57:26,600 --> 00:57:32,920
the organic object language should be an object. If so, it should follow the

487
00:57:33,480 --> 00:57:40,840
general principles of evolution, which satisfy what Einstein called the miracle creed. So why

488
00:57:40,840 --> 00:57:48,680
shouldn't language, so let's pursue that CFR we can do. I think that's just common sense. Many

489
00:57:48,680 --> 00:57:55,960
people think it's superior intelligence. I don't think so. That's it for this episode. I want to

490
00:57:55,960 --> 00:58:02,520
thank Noam for his time. If you'd like a transcript of this conversation, you can find one on our

491
00:58:02,520 --> 00:58:16,040
website, I on AI, that's EYE-ON.AI. In the meantime, remember, the singularity may not be near,

492
00:58:16,040 --> 00:58:26,760
but AI is about to change your world. So pay attention.

