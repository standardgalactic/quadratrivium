Even if you train a system to have a world model that can predict what's going to happen next,
the world is really complicated and there's probably all kinds of situations that the system
hasn't been trained on and need to, you know, fine-tune itself as it goes. The question of how
we organize AI research going forward which is somewhat determined by how afraid people are
of the consequences of AI. So if you have a rather positive view of the impact of AI on society and
you trust humanity and society and democracies to use it in good ways, then the best way to
make progress is to open research. AI might be the most important new computer technology ever.
It's storming every industry and literally billions of dollars are being invested. So buckle up.
The problem is that AI needs a lot of speed and processing power. So how do you compete
without cost spiraling out of control? It's time to upgrade to the next generation of the cloud,
Oracle Cloud Infrastructure or OCI. OCI is a single platform for your infrastructure,
database, application, development, and AI needs. OCI has four to eight times the bandwidth of other
clouds, offers one consistent price instead of variable regional pricing, and of course nobody
does data better than Oracle. So now you can train your AI models at twice the speed and less than
half the cost of other clouds. If you want to do more and spend less like Uber, eight by eight,
and Databricks Mosaic, take a free test drive of OCI at oracle.com slash ion AI. That's E-Y-E-O-N-A-I
all run together oracle.com slash ion AI. Hi, I'm Craig Smith and this is ION AI. In this episode,
I speak again with Yan Lacoon, one of the founders of deep learning and someone who followers of AI
should need no introduction to. Yan talks about his work on developing world models on why he
does not believe AI research poses a threat to humanity and why he thinks open source AI models
are the future. In the course of the conversation, we talk about a new model, Gaia 1, developed by a
company called Wave AI. I'll have an episode with Wave's founder to further explore that world model
which has produced some startling results. I hope you find the conversation with Yan
as enlightening as I did. First, the notion of world model is the idea that the system would get
some idea of the state of the world and be able to predict sort of following states of the world
resulting from just the natural evolution of the world or resulting from an action that the
agent might take. If you have an idea of the state of the world and you imagine an action that
you're going to take and you can predict the resulting state of the world, that means you
can predict what's going to happen as a consequence of a sequence of actions and that means you can
plan a sequence of actions to arrive at a particular goal. That's really what a world model is. At
least that's what the Wave people have understood the word in other contexts, like in the context of
optimal control and robotics and things like that. That's what a world model is. Now, there's several
levels of complexity of those world models, whether they model yourself, the agent, or whether they
model the external world, which is much more complicated. Training a world model basically
consists in just observing the world go by and then learning to predict what's going to happen next,
or observing the world taking an action and then observing the resulting effect, an action that
you take as an agent or an action that you see other agents taking. That establishes
causality essentially. You could think of this as a causal model. Those models don't need to predict
all the details about the world. They don't need to be generative. They don't need to predict exactly
every pixel in a video, for example, because what you need to be able to predict is enough details,
some sort of abstract representation to allow you to plan. You're assembling something out of wood
and you're going to put two planks together and attach them with screws. It doesn't matter the
details of which type of screwdriver you're using or the size of the screw within some limits and
things like that. There are details that in the end don't matter as to what the end result
will be or the precise grain of the wood and things of that type. You need to have some
abstract level of representation within which you can make the prediction without having to
predict every detail. That's why those JPA architectures I've been advocating are useful. Models like
the Gaia 1 model from Wave actually makes prediction in an abstract representation space.
There's been a lot of work in that area for years also at FAIR, but generally the
abstract representation were pre-trained. The encoders that would take images from videos and
then encode them into some representation were trained in some other way. The progress we've
made over the last six months in self-improvised learning for images and video is that now we
can train the entire system to make those predictions simultaneously. We have systems now that can
learn good representations of images and the basic idea is very simple. You take an image,
you run it through an encoder, then you corrupt that image, you mask parts of it, for example,
or you transform it in various ways, you blur it, you change the colors, you change the framing a
little bit, and you run that corrupted image through the same encoder or something very similar.
And then you train the encoder to predict the features of the complete image from the features
of the corrupted one. You're not trying to reconstruct the perfect image,
you're just trying to predict the representation of it. And this is different,
this is not generative in the sense that it does not produce pixels. And that's the secret to getting
self-supervisual into work in the context of images and video. You don't want to be predicting pixels,
it doesn't work. You can't predict pixels as an afterthought, which is what the Gaia system is
doing by sticking a decoder on it and with some diffusion model that will produce a nice image,
but that's kind of a second step. If you train the system by predicting pixels,
you just don't get good representations, you don't get good predictions, you get blurry
predictions most of the time. So that's what makes learning from images and video fundamentally
different from learning from text because in text you don't have that problem. It's easy to predict
words, even if you cannot do a perfect prediction because language is discrete. So language is
simple compared to the real world. And there's a lot written right now about the energy required
in the computational resources GPUs required to train language models. Is it less
in training a world model like using iJAPA architecture? Well, it's hard to tell because
there is no equivalent training procedure, self-supervised training procedure for video,
for example, that does not use JAPA. The ones that are generative don't really work.
Yeah. Well, but this architecture could also be applied to language, couldn't it?
Oh yeah, absolutely. Yeah, so you could very well use a JAPA architecture that makes prediction
in representation space and apply to language. Yeah, definitely. And in that case, would it be
less computationally intense than training a large language model?
It's possible. It's not entirely clear either. I mean, there is some advantage regardless of what
technique you're using to making those models really big. They just seem to work better if you
make them big. So if you make them bigger. So scaling is useful. Contrary to some claims,
I do not believe that scaling is sufficient. So in other words, we're not going to get anywhere
close to human level AI. In fact, not even any more level AI by simply scaling up language models.
Even multimodal language models that we applied to video, we're going to have to find new concepts,
new architectures. And I've written a vision paper about this a while back of a different
type of architecture that would be necessary for this. So scaling is necessary, but not sufficient.
And we're missing some basic ingredients to get to human level AI. We're fooled by the fact that
LLMs are fluent. And so we think that they have human level intelligence because they can manipulate
language. But that's false. And in fact, there's a very good symptom for this, which is that
we have systems that can pass the bar exam, but answering questions from text by basically
regurgitating what they've learned, more or less by road. But we don't have completely autonomous
level five cell driving cars, or at least no system that can learn to do this in about 20 hours
of practice, just like any 17-year-old. And we certainly don't have any domestic robot that can
clear up the dinner table and fill up the dishwasher attest that any 10-year-old can learn
in one shot. So clearly, we're missing something big. And that something is an ability to learn
how the world works and the world is much more complicated than language. And also being able
to plan and reason, basically having a mental world model that allows to plan and predict
consequences of actions. That's what we're missing. It takes a while before we figure this out.
You were on another paper that talked about augmented language models. And
in the embodied touring test, was that the same paper, the embodied touring test?
Can you talk about that? First of all, what is the embodied touring test? I didn't quite
understand that. Well, okay, it's a different concept. But it's basically the idea that you
do, it's based on the Moravec paradox, right? So Moravec many years ago noticed that things that
appeared difficult for humans turned out to sometimes be very easy for computers to do,
like playing chess, much better than humans. Or I don't know, computing integrals or whatever,
certainly doing arithmetic. But then there are things that we take for granted as humans that
we don't even consider them intelligent tasks that we are incapable of reproducing with computers.
And so that's where the embodied touring test comes in. Observe what a cat can do or how fast a cat
can learn new tricks or how a cat can plan to jump on a bunch of different furniture to get to the
top of wherever it wants to go. That's an amazing feat that we can't reproduce with robots today.
So that's kind of the embodied touring test, if you want. Like, can you make a robot that
can behave, have behaviors that are easily wishable from those of animals, first of all,
and can acquire new ones with the same efficiency as animals? Then the augmented
LLM paper is different. It's about how do you sort of minimally change large language models so
that they can use tools so they can, to some extent, plan actions. Like, you know, you need to compute
the product of two numbers, right? You just call a calculator and you know you're going to get the
product of those two numbers. And LLMs are notoriously bad for arithmetic, so they need to do
this kind of stuff or do a search, you know, using a search engine or database lookup or
something like that. So there's a lot of work on this right now and it's somewhat incremental. Like,
you know, how can you sort of minimally change LLM and take advantage of their current capabilities
but still augment them with the ability to use tools? Yeah. And I don't want to get into the
too much into the threat debate. But, you know, you're on one side, your colleagues,
Jeff and Yashor on the other. I recently saw a picture of the three of you. I think you put that
up on social media, saying how, you know, you can disagree but still be friends. This idea of
augmenting language models with stronger reasoning capabilities and the ability,
and agency, the ability to use tools is precisely what Jeff and Yashor are worried about.
Can you just, why are you not worried about that? Okay. So first of all, what you're describing
is not necessarily what they are afraid of. They are alerting people and various governments and
others about various dangers that they perceive. Okay. So one danger, one set of dangers are
relatively short-term. There are things like, you know, bad people will use technology for bad
things. What can bad people use powerful AI systems for? And one concern that, you know,
governments have been worried about and intelligence agencies encounter intelligence and stuff like
that is, you know, could value-intentioned organizations or countries use LLM to help them,
I don't know, design pathogens or chemical weapons or other things or cyber attacks,
you know, things like that, right? Now, those problems are not new. Those problems have been
with us for a long time. And the question is, what incremental help would AI systems bring to the
table? So my opinion is that as of today, AI systems are not sophisticated enough to provide
any significant help for such value-intentioned people because those systems are trained with
public data that is publicly available on the internet. And they can't really invent anything.
They're going to regurgitate with a little bit of interpolation if you want. But
they cannot produce anything that you can't get from a search engine in a few minutes.
So that claim is being tested at the moment. There are people who are actually kind of
trying to figure out, like, is it the case that you can actually do something, you're
unable to do something more dangerous with sort of current AI technology that you can do with a
search engine results are not out yet. But my hunch is that, you know, it's not going to enable
a lot of people to do significantly bad things. Then there is the issue of things like code
generation for cyber attacks and things like this. And those problems have been with us for years.
And the interesting thing that most people should know, like, you know, also for like
disinformation or attempts to corrupt the electoral process and things like this. And what's
very important for everyone to know is that the best countermeasures that we have
against all of those attacks currently use AI massively. Okay. So AI is used as a defense
mechanism against those attacks. It's not actually used to do the attacks yet. And so now it becomes
the question of, you know, who has the better system, like other countermeasures? Is the AI
countermeasures significantly better than the AI is used by the attackers so that, you know,
the problem is satisfactorily mitigated. And that's what we are. Now, the good news is that there are
many more good guys and bad guys. They're usually much more competent. They're usually much more
sophisticated. They're usually much more better funded. And they have a strong incentive to take
down the attackers. So it's a game of cat and mouse, just like every security that's ever existed.
There's nothing new there. Okay. Nothing quite entirely new. Yeah. But then there is the question
of existential risk, right? And this is something that both Jeff and Yosha have been thinking of
fairly recently. So for Jeff, it's only sort of just before last summer that he became,
he started thinking about this because before he thought he was convinced that the kind of
algorithms that we had were significantly inferior to the kind of learning algorithm that the brain
used. And the epiphany he had was that, in fact, no, because looking at the capabilities of
large English models, they can do pretty amazing things with a relatively small number of neurons
and synapses. He said, maybe they're more efficient than the brain. And maybe the learning algorithm
that we use, back propagation, is actually better than whatever it is that the brain uses.
So he started thinking about like, you know, what are the consequences? And
but that's very recent. And in my opinion, he hasn't thought about this enough.
Yosha went to a similar epiphany last winter, where he started thinking about the long-term
consequences. And came to the conclusion also that there was a potential danger.
They're both convinced that AI has enormous potential benefits. They're just worried
about the dangers. And they're both worried about the dangers because they have some doubts
about the ability of our institutions to do the best with technology.
You know, whether they are political, economic, geopolitical, financial institutions,
or industrial, to do the right thing, to be motivated by the right thing. So
you know, if you trust the system, if you trust humanity and democracy,
you might be entitled to believe that society is going to make the best use of
future technology. If you don't believe in the solidity of those institutions,
then you might be scared. Okay. I think I'm more confident in humanity and democracy than they are.
And, and, you know, whatever current systems and they are, I've been thinking about this
problem for much longer, actually, since at least 2014. So when I started fair at Facebook at the
time, it became pretty clear, pretty early on that, you know, deploying AI systems was going to have
big consequences on people in society. And we got confronted to this very early.
And so I started thinking about those problems very early on. Things like, you know, counter
measures against like bias in AI systems, systematic bias, counter measures against attacks,
or, you know, detection of hate speech in every language, things like that. These are things that
people at fair worked on and then were eventually deployed. To just to give you an example, the
proportion of hate speech that was taken down automatically by AI systems five years ago,
you know, in 2017, was about 20 to 25%. Last year, it was 95%. And the difference is entirely due to
progress in natural language understanding, entirely grew to transformers that are pretrained
self-supervised and can essentially detect hate speech in any language. Not perfectly. Nothing
is perfect. It's ever perfect. But AI is massively there. And that's the solution. So I started
thinking about those issues, including existential risk, very early on. In fact, in 2015, early 2016,
actually, I organized a conference hosted at NYU on the future of AI, where a lot of those
questions were discussed. I invited people like, you know, Eric Schmidt and Mark Schreffer, who
was the CTO of Facebook at the time. A lot of people, both from the academic and AI research
side and from the industry side. And there were two days, a public day and kind of a more private
day. What came out of this is the creation of an institution called a partnership on AI.
So this is a discussion I had with Mr. Sabis, which was, you know, would it be useful to have a
forum where we can discuss before they happen, sort of bad things that could happen as a consequence
of deploying AI? Pretty soon, we brought on board Eric Horvitz and a bunch of other people,
and we co-founded this thing called a partnership on AI, which basically has been
funding studies about AI ethics and consequences of AI and publishing guidelines about, you know,
how you do it right to me and my time. So this is not a new thing for me. Like, I've been thinking
about this for 10 years, essentially. Whereas for Yosha and Jeff, it's much more recent.
Yeah. But nonetheless, this augmented AI or augmented language models that have stronger
reasoning and agency raises the threat, regardless of whether or not it can be countered
to a higher level. Right. Okay. So I guess the question there becomes, what is the blueprint
of future AI systems that will be capable of reasoning and planning, will understand how the
world works, will be able to, you know, use tools and have agency and things like that. Right.
And I tell you, they will not be autoregressive LLMs. So the problems that we see at the moment
of autoregressive LLM, the fact that they hallucinate, they sometimes say really stupid
things. They don't really have a good understanding of the world. People claim that they have some
simple word model, but it's very implicit and it's really not good at all. Like, for example,
you know, you can tell an LLM that A is the same as B. And then you ask if B is the same as A,
and it will say, I don't know, or no. Right. I mean, those things don't really understand
logic or anything like that. Right. So the type of system that we're talking about that might be,
that might approach any more level intelligence and let alone human level intelligence have not
been designed. They don't exist. And so discussing their danger and their potential harm is a bit
like, you know, discussing the sex of angels at the moment, or to be a little more
accurate, perhaps, it would be kind of like discussing how we're going to make transatlantic
flight at near the speed of sound safe when we haven't yet invented the turbojet in 1925.
Yeah. Yeah. Like, you know, we can speculate, but you know, how do we, how did we make turbojet
safe? It required decades of really careful engineering to make them incredibly reliable.
And, you know, now we can, you know, run like halfway around the world with the two-engine
turbojet aircraft. I mean, that's an incredible feat. And it's not like people were discussing
sort of philosophical questions about how you make turbojet safe. It's just really careful and
complicated engineering that no one, none of us would understand. So, you know,
how could we ask the AI community now to explain how AI systems are going to be safe? We haven't
invented them yet. No. Okay. That said, I have some idea about how we can design them so that
they have these capabilities. And as a consequence, how they will be safe, I call this objective
driven AI. So what that means is essentially systems that produce their answer by planning
their answer so as to satisfy an objective or a set of objectives. So this is very different
from current LLNs. Current LLNs produce one word after the other or one token, which is,
which has a board unit, doesn't matter, right? They don't really think and plan ahead as we,
as we said before. They just produce one word after the other. That's not controllable.
The only thing we can do is see if what they've produced, like check if what they've produced
satisfies some criterion or set of criteria and then not produce an answer or produce a
non-answer if the answer that was produced isn't appropriate. But we can't really force them to
produce an answer that satisfies a set of objectives. So objective driven AI is the other way,
is the opposite. The only thing that the system can produce are answers that satisfy a certain
number of objectives. So what objective would be? Did you answer the question? Another objective could
be, is your answer understandable by a 13 year old because you're talking to a 13 year old?
Another would be, is this, I don't know, terrorist propaganda or something? You know,
you can have a number of criteria like this, guardrails that would guarantee that the answer
that's produced is satisfy certain criteria, whatever they are. Okay. Same for a robot,
you could guarantee that the sequence of actions that is produced will not hurt anyone.
Like you can have very low level, you know, guardrails of this type that say,
okay, you have, you know, humans nearby and you're cooking, so you have a big knife in your hand,
don't flare your arms. Okay, that would be a very simple guardrails to impose. And you can imagine
having a whole bunch of guardrails like this that will guarantee that the behavior of those systems
would be safe and that their primary goal would be to be basically subservient to us, right? So I
do not believe that we'll have AI systems that can work that will not be subservient to us,
will define their own goals, they will define their own sub goals, but those sub goals would
be sub goals or goals that we set them and will not have all kinds of guardrails that will
guarantee the safety. And we're not going to, it's not like we're going to invent a system
and make a gigantic one that we know will have human level AI and just turning on and then from
the next minute is going to take over the world. That's completely preposterous.
What we're going to do is try with small ones, you know, maybe as smart as a mouse or something,
maybe a dog, maybe a cat, maybe a dog, maybe and work our way up and then, you know,
put some more guardrails. Basically, like we've engineered, you know, more and more powerful
and more reliable turbojets. It's an engineering problem. Yeah, yeah. You were also on a paper,
maybe this is the one that talked about the embodied Turing test on neuro AI.
Can you explain what the neuro AI is? Okay. Well, it's the idea that we should get some
inspiration from neuroscience to build AI systems and that there is something to be learned from
neuroscience and from cognitive science to drive the design of AI systems, some inspiration.
Okay. Something to be learned as well as the other way around. So what's interesting right now is
that the best models that we have of how, for example, the visual cortex works is convolutional
neural networks, which are also the models that we use to recognize images primarily
in artificial systems. So there is kind of information kind of being exchanged both ways.
There's one, you know, one way to make progress in AI is to kind of ignore nature and just,
you know, kind of try to solve problems in a sort of engineering fashion, if you want.
I found interaction with neuroscience always thought provoking. So you don't want to be
copying nature very too closely because there are details in nature that are irrelevant.
And there are principles on which, you know, natural intelligence is based that we haven't
discovered. So, but there is some inspiration to have certainly in your convolutional net for
inspired by the architecture of the visual cortex. The whole idea of neural net and deep learning
came out of the idea that, you know, intelligence can emerge from a large collection of simple
elements that are connected with each other and change the nature of their interactions.
That's the whole idea, right? So, so inspiration from neuroscience certainly has been extremely
beneficial so far. And the idea of neural AI is that you should keep going. You don't want to go
too far. So going too far, for example, is trying to reproduce the some aspect of the functioning
of neurons with electronics. I'm not sure that's a good idea. I'm skeptical about this, for example.
So your research right now, are you, your main focus is on
furthering the JEPA architecture into other modalities or where are you headed?
Yeah. So, I mean, the long term goal is, you know, to get machines to be as intelligent and learn
as efficiently as animals and humans. Okay. And the reason for this is that we need this because
we need to amplify human intelligence. And so intelligence is the most needed commodity that
we want in the world, right? And so we could, you know, possibly bring a new renaissance to humanity
if we could amplify human intelligence using machines, which we are doing already with computers,
right? I mean, that's pretty much what they've been designed to do. But even more, you know,
imagine a future where every one of us has an intelligent assistant with us at all times.
They can be smarter than us. You shouldn't feel threatened by that. We should feel
like we are like, you know, a director of a big lab or a CEO of a company that has a staff working
for them of people who are smarter than themselves. I mean, we're used to this already. I'm used to
this certainly working with people who are smarter than me. So we shouldn't feel threatened by this,
but it's going to empower a lot of us, right, and humanity as a whole. So I think that's a good
thing. That's the overall practical goal, if you want, right? Then there's a scientific
question that's behind this, which is really what is intelligence and how you build it.
And then which is, you know, how can system learn the way animals and humans seem to be
learning so efficiently? And the next thing is, how do we learn how the world works by observation,
by watching the world go by through vision and all the other senses? And animals can do this
without language, right? So it has nothing to do with language, has to do with learning from sensory
perceives and learning mostly without acting, because any action you take can kill you. So
it's better to be able to learn as much as you can without actually acting at all, just observing,
which is what babies do in the first few months of life. They can't hardly do anything, right? So
they mostly observe and learn how the world works by observation. So what kind of learning takes
place there? So that's obviously kind of self-supervised, right? It's learning by prediction. That's an
whole idea from cognitive science. And the thing is, you know, we can learn to predict videos,
but then we notice that predicting videos, predicting pixels in video, is so initially
complicated that it doesn't work. And so then came this idea of JEPA, right? Learn representations
so that you can make predictions in representation space. And that turned out to work really well
for learning image features. And now we're working on getting this to work for video. And
eventually, we'll be able to use this to learn to learn world models, where you show a piece of
video, and then you say, I'm going to take this action, predict what's going to happen next in the
world. And, you know, which is a bit where the Gaia system from Wave is doing at a high level,
but we need this at sort of various levels of abstraction, so that we can build,
you know, systems that are more general than autonomous driving. Okay. That's the...
Yeah. And it's my fault, so I won't go over the hour. But is it conceivable that someday there'll be
a model that you may be embodied in a robot that is ingesting video from its environment
and learning as it's just continuously learning and getting smarter and smarter and smarter?
Yeah. I mean, that's kind of a bit of a necessity. The reason being that, you know,
even if you train a system to have a world model that can predict what's going to happen next,
the world is really complicated. And there's probably all kinds of situations that you,
you know, the system hasn't been trained on and need to, you know, fine tune itself as it goes.
So, you know, animals and humans do this early in life by playing. So play is a way of
learning your world model in situations that basically you won't hurt you.
And, but then during life, of course, you know, when we don't drive, there's all kinds of these
mistakes that we do initially that we don't do after having some experience. And that's because
we're fine tuning our world model to some extent. We're learning a new task. We're basically just
learning a new version of our world model. Right. So, so yeah, I mean, this type of continuous,
continual learning is going to have to be present. But the overall power and the
intelligence of the system will be limited by, you know, how much a co-governor on that is using
and various other constraints, you know, computational constraints, basically.
And, you know, you're still young. And, and this not sure about that.
Well, you're younger than Jeff. Let me put it that way.
But this, the progress you've made on world models is, is fairly rapid from my point of
view, watching it. Are you, are you hopeful that within your career, you'll have
embodied robots that are, are building world models through their interaction in reality,
and, and then being able to, well, I guess the other question on world models,
do you then combine it with a language model to do reasoning or, or is the world model able to,
to do reasoning on its own? But are you hopeful that in your career, you'll, you'll get to the
point where you'll have this continuous learning in a world model? Yeah, I sure hope so. I might have
another, you know, 10, 10 useful years or something like this in research before my brain, you know,
turns into dish and male sauce, but, or something like that, you know, 15 years if I'm lucky.
So, or perhaps less. But yeah, I hope that there's going to be breakthroughs in that direction
during that time. Now, whether that will result in the kind of artifact that you're describing,
you know, robots that can, like, you know, domestic robots, for example, or,
or sort of in cars that are, they can run fairly quickly by themselves.
I don't know, because there might be all kinds of obstacles that we have not envisaged that may
appear on the way. You know, that's, it's a constant in the history of AI that you have some new idea
and a breakthrough, and you think that's going to solve all the world's problems.
And then you're going to hit limitation, and you have to go beyond that limitation. So it's like,
you know, you're climbing a mountain, you find a way to climb the mountain that you're seeing.
And you know that once you get to the top, you will have the problem solved because now it's,
you know, the gentle slope down. And once you get to the top, you realize that there is
another mountain behind it that you hadn't seen. So that's, that's, that's been the history of AI,
right, where people have come up with sort of new concepts, new ideas, new way to approach
AI reasoning, whatever, perception, and then realize that their idea basically was very limited.
And so, so, you know, this, inevitably, we're trying to figure out what's the next
revolution in AI. That's what I'm trying to figure out. So, you know, learning how the world works
from video, having systems that have world model allows systems to reason and plan.
And there's something I want to be very clear about, which is an answer to your question,
which is that you can have systems that reason and plan without manipulating language. Animals are
capable of amazing feats of planning and also to some extent reasoning. They don't have language,
at least most of them don't. And so, many of them don't have culture because they are mostly
solitary animals. So, you know, it's only the animals that have some level of culture. So,
so the idea that a system can plan and reason is not connected with the idea that you can
manipulate language. Those are two different things. It needs to be able to manipulate abstract
notions. But those notions do not necessarily correspond to linguistic entities like words
or things like that. We can have mental images if you want to things. Like you do
ask a physicist or a mathematician, you know, how they reason is very much in terms of sort of
mental models. I have nothing to do with language. Then you can turn things into
language. But that's a different story. That's the second step. So,
so, you know, we're going to have to figure out how to do this reasoning,
hierarchical planning in machines, reproduce this first. And then, of course, you know,
sticking language on top of it will help. Like, we'll make those systems smarter and be able,
you know, we will allow us to communicate with them and teach them things. And they're going to
be able to teach us things and stuff like that. But this is a different question, really.
The question of how we organize AI research going forward, which is somewhat determined by how
afraid people are of the consequences of AI. So, if you have a rather positive view of the impact
of AI on society, and you trust humanity and society and democracies to use it in good ways,
then the best way to make progress is to open research. And for the people who are
afraid of the consequences, whether they are societal or geopolitical,
they're putting pressure on governments around the world to regulate AI in ways that basically limit
access, particularly of open source code and things like that. And it's a big debate at the
moment. I'm very much on the side. So, he's met up very much on the side of open research.
Yeah, actually, that was something I was going to ask you. And now that you've brought it up.
Because there, I've been talking to people about this. And there is a view that aside from the
risks of open source, you know, again, Jeff Hinton saying, you know, would you open source
thermonuclear weapons? Aside from that is the question of as to whether open source can marshal
the resources to compete with proprietary models. And because of the tremendous resources required
for when you're scaling these models. And there's a question as to whether or not
Meta will continue to open source future versions of Lama or not continue to open source, but whether
it'll continue to invest the resources needed to push the open source models.
So what do you think about that? Okay, there's a lot to say about this. Okay, so first thing is,
there's no question that Meta will continue to invest the resources to build
better and better AI systems, because it needs it for its own products. So the resources will
be invested. Now, the next question is, do you, you know, will we continue to open source the
base models? And the answer is, you know, probably yes, because that creates an ecosystem on top of
which an entire industry can be built. And there is no point, you know, having 50 different companies
building proprietary close systems when you can have, you know, one good base open source base
model that everybody can use. It's wasteful. And it's not a good idea. And another reason for
having open source models is that it, it nobody has no entity as powerful as it thinks it is,
as a monopoly on good ideas. And so if you want people who can have good new innovative ideas
to contribute, you need an open source platform. If you want the academic world to contribute,
you need open source platforms. If you want the startup world to be able to build
customized products, you need open source base models, because they don't have the resources to
build to train large models, right? Okay, and then there is the history that shows that for,
for foundational technology, for infrastructure type technology, open source always wins,
right? It's true of the software infrastructure of the internet. In the early 90s and mid 90s,
there was a big battle between sun macro systems and Microsoft to produce the, deliver the
software infrastructure of the internet, you know, operating systems, web servers,
web browsers, and, and, you know, various servers aside and client-side frameworks, right?
They're both lost. Nobody is talking about them anymore. The entire world is,
of the web is using Linux and Apache and MySQL and JavaScript and, and, you know, and even the,
the basic core code for, for web browser is open source. So,
open source won by a huge margin. Why? Because it's safer, gathers more people to contribute.
All the features are unnecessary. It's more reliable.
Venerabilities are fixed faster. And, and it's customizable. So anybody can customize Linux
to run on whatever hardware they want, right? So open source wins. And the same, same for AI.
It's going to be the same thing. It's inevitable. The, the people now who are climbing up,
like open AI, their, their system is based on publications from all of us. Sure. And from
open platforms like, like PyTorch. Yeah.
Judgeability is built using PyTorch. PyTorch was produced originally by Meta. Now it's owned by
the Linux Foundation. It's open source. They've contributed to it, by the way. You know, their
LLM is based on transformer architectures invented at Google. Yeah. All the tricks to kind of train
all those things came out of like various papers from all kinds of different institutions,
including academia, all the fine-tuning techniques, same. So nobody works in a vacuum.
The thing is, nobody can keep their advance and their advantage
for very long if they are secretive. Yeah. Except that with these models, because they're
so compute intensive and they cost so much money to train, you need somebody like Meta that who's,
who's going to be willing to build them and open source them. And that's why I was, when I was
asking whether they'll continue, obviously Meta will continue building, you know, resource-intensive
models. But the question is whether they'll continue to open source. I mean, if- I'm telling you,
I'm telling you the only reason why Meta could stop open sourcing models are legal.
So if there is a law that adds laws, open source AI systems above a certain level of
sophistication, then of course we can do it. If there are laws that in the US or across the world
makes it illegal to use public content to train AI systems, then it's the end of AI
for everybody, not just for the open source. Okay. So, or at least the end of the type of AI
that we are talking about today might have, you know, new AI in the future, but that don't
require as much data. So, and then there is, you know, liability. If you, if you, if you, if you
believe in the kind of that someone doing something bad with an AI system that was open sourced by
by Meta, then Meta is liable, then Meta will have a big incentive not to release it, obviously.
So it's the entire question about this is around legal reasons and political decisions.
But on the idea of open source winning, don't you need more people or more companies like Meta
building the foundation models and open sourcing them? Or could it be, could an open source
ecosystem win based on a single company building the models? No, I mean, you need two or three.
And there are two or three, right? I mean, there is this hugging face. There is Mistral in France,
who's also embracing sort of an open source LLM. They're very good LLM. It's a small one, but it's
very good. There is, you know, academic efforts like Lyon. They don't have all the resources they
need, but they, you know, they collect the data that is used by everyone. So everybody can contribute.
One thing that I think is really important to understand also is that there is a future in
which I described earlier in which every one of us, every one of our interactions with the digital
world will be mediated by an AI assistant. And this is going to be for true for everyone around
the world, right? Everyone who has any kind of smart device. Eventually it's going to be in our,
you know, augmented reality glasses, but, you know, for the time being in our smartphones, right?
And so imagine that future where, you know, you are, I don't know, from
Indonesia or Senegal or France. And your entire digital diet is done through the
mediation of an AI system. Your government is not going to be happy about it. Your government
is going to want the local culture to be present in that system. It doesn't want that system to be
closed sourced and controlled by a company on the west coast of the US. So just for reasons of
preserving the diversity of culture across the world and not having or entire information
diet being biased by whatever it is that some company on the west coast of the US states,
there's going to need to be open source platforms. And they're going to be predominant
in at least outside the US for that reason. Including China, right? There is all those
talks about, oh, what if China puts their hands on our open source code? I mean, China wants control
over its own LLM because they don't want their citizen to, you know, have access to certain type
of information. So they're not going to use our LLMs. They're going to trend theirs that they already
have. And nobody is, you know, particularly ahead of anybody else by more than about a year.
Yeah. And China is pushing open source. I mean, they're very pro open source within their
ecosystems. Some of them, you know, it's there's no like unified opinion there. But
I mean, it's the same in in the West, right? There are some some governments that are too
afraid of the risks. And then or are thinking about it and some others that are all for open
source because they see this as the only way for them to have any influence on the
type of information and culture that would be mediated by those systems. So it's going to have
to be like Wikipedia, right? Wikipedia, you know, is built by millions of people who contribute to
or from all around the world in all kinds of languages. Okay. And it has a system for sort
of vetting the information. The way AI systems of the future will be taught and we'll be fine
tuned will have to be the same way will have to be quite sourced. Because something that matters to
a farmer in southern India is probably not going to be taken into account by the fine
tuning done by, you know, some some company on the west coast of the US. AI might be the most
important new computer technology ever. It's storming every industry and literally billions
of dollars are being invested. So buckle up. The problem is that AI needs a lot of speed and
processing power. So how do you compete without cost spiraling out of control? It's time to upgrade
to the next generation of the cloud oracle cloud infrastructure or OCI. OCI is a single platform
for your infrastructure, database, application, development and AI needs. OCI has four to eight
times the bandwidth of other clouds, offers one consistent price instead of variable regional
pricing. And of course, nobody does data better than oracle. So now you can train your AI models
at twice the speed and less than half the cost of other clouds. If you want to do more and spend
less like Uber, eight by eight and Databricks Mosaic, take a free test drive of OCI at oracle.com
slash I on AI. That's E Y E O N A I all run together oracle.com slash I on AI. That's it for this
episode. I want to thank Yen for his time. If you want to read a transcript of this conversation,
you can find one on our website I on AI. That's E Y E hyphen O N dot AI. And remember the singularity
may not be near, but AI is changing your world. So best pay attention.
