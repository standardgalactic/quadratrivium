start	end	text
0	6000	What this whole open AI saga has shown us is that, I mean, obviously we can't have something like
6000	10720	this being developed by just like a handful of, you know, weird people, unaccountable
10720	14480	billionaires in the Bay Area. This is actually something that I've been telling
14480	18080	saying to people for years now. Obviously, this is not the right governance structure.
18080	22320	So the weirdest thing I would say about this technology should be a tool. It should not be a
22320	29360	goal in and of itself. Hi, I wanted to jump in and give a shout out to our sponsor NetSuite
29360	37120	by Oracle. I'm a journalist and getting a single source of truth is nearly impossible.
37120	42960	If you're a business owner, having a single source of truth is critical to running your
42960	54320	operations. If this is you, you should know these three numbers. 36,025,1. 36,000 because that's the
54320	60720	number of businesses that have upgraded to NetSuite by Oracle. NetSuite is the number one
60720	68080	cloud financial system, streamlining, accounting, financial management, inventory, HR, and more.
68960	77040	25, because NetSuite turns 25 this year. That's 25 years of helping businesses do more with less,
77040	83920	close their books in days, not weeks, and drive down costs. One, because your business is one
83920	91600	of a kind. So you get a customized solution for all of your KPIs in one efficient system
91600	98880	with one source of truth. Manage risk, get reliable, forecast, and improve margins. Everything you need
99440	107280	all in one place. As I said, I'm not the most organized person in the world, and there's real
107280	112000	power to having all of the information in one place to make better decisions.
112960	121440	This is an unprecedented offer by NetSuite to make that possible. Right now, download NetSuite's
121440	129200	popular KPI checklist designed to give you consistently excellent performance, absolutely free
129360	141600	at netsuite.com slash I on AI. That's I on AI, E-Y-E-O-N-A-I, all run together. Go to net suite.com
142320	154880	slash I on AI to get your own KPI checklist. Again, that's net suite.com slash I on AI, E-Y-E-O-N-A-I.
155120	159120	They support us, so let's support them.
162880	170400	Hi, my name is Craig Smith, and this is I on AI. In this episode, I speak again with Connor Lehi.
171360	178080	He's the founder and CEO of a startup called Conjecture that's working on AI alignment.
178960	185360	Before that, he was one of the founders and leaders of a group called Eleuther AI
186320	196560	that built one of the world's first open source large language models. Connor is concerned about
196560	205360	AI safety, about where AI development is going, concerned about the push towards
206320	212640	artificial general intelligence, and has a lot of thoughts about what we should be doing
213280	222240	to control development so that we don't end up creating something that is harmful to humanity.
222240	227840	I talked to him particularly because I wanted to hear his thoughts on the open AI saga,
228800	235520	which highlighted for a lot of people the dangers of having such a small group of people
236720	246000	controlling such a fundamentally powerful technology today. I hope you find the conversation
246000	253280	as interesting as I did. So I'm Connor. I'm currently the CEO of Conjecture, an AI company in
253280	258960	London focused on AI safety and building architectures for AI systems that are understandable
258960	265520	and controllable, and various other things. I also do a bit of work in policy regulation,
265520	272320	public messaging, that kind of stuff. Before this, I was well known as one of the founders of Eleuther
272320	279760	AI, which was kind of one of the first, if not the first kind of like open source large language
279760	286240	models, research, building kind of groups. And technically, even before that, I was someone who
286240	291200	worked on open source GBT2 as I think maybe literally the first person.
292480	300400	Yeah. And so last time we spoke was after the release of chat GBT and GBD4, and
301840	307520	you were very concerned as were a lot of people and a lot of people continue to be about
308480	316320	releasing these kinds of models into the public before having fully explored
317760	326320	the safety issues or without having adequate guidelines. But it struck me during this open AI
326320	334960	saga that we have lived through the last week, that having this kind of powerful technology
334960	345440	in the hands of a handful of people who have different agendas and can't get along is in itself
345440	356640	a security issue. And I would think that having these models open source where everyone can
357440	365040	see the data they were trained on in particular, because even Llama, I've learned, doesn't
365920	374560	make public the training data. But having the data open for all to see and having the weights
375200	384640	of the models open so people can improve them or play with them or whatever. That to me seems
384640	392880	like a much safer path than having proprietary models. But I wanted to hear how your thinking is
392880	401760	involved on that. In the 1940s and in the early 1950s, the Soviet Union built a what's called a
401760	408960	closed city around what would be known as the Mayak facility. The Mayak facility was the largest and
408960	412800	one of the first, well I don't know if it was literally the first but it's one of the largest
412800	417120	nuclear facilities for the Soviet Union in their breakthrough development attempt to build the
417120	423680	nuclear bomb. To give a bit of a flavor for what Mayak and similar facilities that existed throughout
423680	431920	the Soviet Union were like, there was a program where if you got caught by the secret police
431920	436080	and you were being sent to a gulag for the rest of your life, you would give them an option.
436960	441120	Either you go to Siberia, work yourself to death for the rest of your life,
441120	445680	or you get sent to Mayak for only three months. And if you serve your term, you're free.
446560	452560	Sounds great, doesn't it? Well, no one survived the three months. So what happened, of course, is
452560	457040	that one of the first things that the Americans developed while developing the bomb is the HEPA
457040	463200	filter, which is a form of air filter that is powerful enough to be able to filter radioactive
463200	468800	material very sufficiently out of the air, making it safe for the workers. The Soviets didn't
468800	475440	bother developing HEPA filters. Not really. So a lot of people died. To this day,
476160	481600	Lake Karakai, which is near the Mayak facility, is one of the most radioactive places on earth,
482160	487520	so much so that it is said that standing next to it for an hour can kill a man. These are all
487520	491280	just some fun facts that surely have nothing to do with the topic we're talking about today.
491840	500400	It's a story about how some people who were pretty bad people developed something or working
500400	506000	on something pretty dangerous, and then other people got access to it, and also really bad things
506000	513040	out because even worse people got access to it. Now, from this, I don't conclude, oh, so we should
513040	520320	have just had everyone develop plutonium. That would have made it safer. This is not a draw from
520400	526720	this conclusion from the story. Now, nuclear power is obviously very different from AI in many
526720	534480	factors. So why even bring it up? So I can ask the question in the other direction, though.
534480	539920	We're talking about AI. We're talking about AGI. So let's focus on AGI. I'm not really interested
539920	546080	in talking about the risks of current-day models, like chat GPT or something. We can talk about
546080	549920	those, too. They are real. There are real risks from those, but they're not the kind where I
549920	554800	think we have to stop all publication necessarily. But let's talk about AGI systems.
557120	564000	What reference class does AGI fall into? Is it like open source? Is it like nuclear bombs?
564960	572960	Is it like something different? The reference class we choose forms our thinking around
572960	577840	something which is fundamentally none of those things. AGI is in nukes. AGI is in open source.
578480	584400	It's not Linux. It's something very different. So while it has things in common with all of those
584400	590880	things, AGI runs on computers. Linux runs on computers. And it has other things that come
590880	594960	with nuclear bombs. Nuclear bombs can kill everybody. AGI can kill everybody. Those are
594960	599040	things in common. Is it more like nukes? This is more like open source. At some point, we have
599040	604000	to actually drop down from the metaphors and into the actual models of reality. So from my
604000	610240	perspective, I think you're completely correct. What this whole opening AGI has shown us is that
611440	617840	obviously, we can't have something like this being developed by just a handful of weird people,
617840	622480	unaccountable billionaires in the Bay Area. Obviously, they're not acting in humanity's
622480	628880	best interest to no one's surprise. A couple of months ago, Sam Altman interviewed about this,
628880	632720	and he said, oh, yeah, the board can fire me at any time if I go about a mission. I think that's
632720	639200	important. And then they try to fire him, and he's back. And well, that didn't work. So this is
639200	647200	actually something that I've been telling saying to people for years now. I've gotten some disagreements.
647200	651760	He did disagreements, let us call them, with some of the people who were involved with the creation
651760	657040	of the board or in favor of the existence of the board. And the point I always made to them was
658000	666560	this obviously cannot control a charismatic billionaire political mastermind. Why the
666560	672320	hell would you think it would? This is crazy. And this is exactly what we saw play out. And
672320	676320	I'm not even trying to make a comment on Sam Altman, good Sam Altman, bad. I'm just saying,
676320	681680	obviously, he was just not going to just say, oh gosh darn, I guess the board said no more AGI for
681680	687520	me, I guess I'm going to stop. That's not how men like him work. And that's obviously not what was
687520	693840	going to happen. And you think the politically unsavvy nerds could write a document that would
693840	698080	convince someone like him to stop? No, of course not. So obviously, this is not the right governance
698080	703760	structure. I fully agree with this. But there's a great saying, which is that reverse stupidity is
703760	708800	not intelligence. If you take something stupid, and you take the opposite of it, it's probably also
708800	715760	stupid. And so the fact that this governance structure doesn't work for me does not say that
715760	719600	therefore there should be no governance structure. This to me does not follow.
720880	727200	Yeah, but open source, I mean, you are I'm sure very familiar with Yanlacun's argument that,
728000	739920	yeah, that open sourcing can can lead to some abuse by bad actors, but by and large,
741040	750960	the vast majority of people that will be working on an open source model contributing to it or
750960	761440	building products off of it will be doing so with, you know, not without nefarious intent.
762000	769600	And that the larger the open source community, the quicker it would be able to respond to
770560	782560	to bad actors or or misuse or or the more people available to to build guardrails and spot
782960	791280	of weaknesses and that sort of thing. So that that argument makes a lot of sense to me. I mean,
791280	800240	at the beginning, when the, you know, pause letter came out. And around the time that we talked about,
800240	806560	yeah, this stuff is too dangerous to be open source. But,
809920	816880	but I'm changing my mind. And I wanted to hear whether the this this episode has changed your
816880	824720	mind at all. I'll make three points in reaction to that. The first one is a story.
824720	829840	The second one is a heuristic. And the third one is a true observation from my own life.
830560	838000	So first, the story, the story is that the smallpox virus genome is currently online.
838560	843040	You can go download it. It's a small text file. You can just go download it to your computer.
844000	849280	Another fact of the story is that a couple of years ago, Canadian scientists
850240	856480	recreated an extinct version of smallpox called horsepox. They revived it. They may and it was
856480	862720	functional and viable and infectious. And they published how to do it. Do you think either of
862720	868000	those things are good? Now you can argue, well, if we have more eyes on the smallpox virus,
868000	872080	then something, something, you know, good things happen. But this isn't really a model.
872800	876640	So this brings me to the second point. The second point is offense versus defense.
877600	883120	The way technology is work is that some favorite offense, some favorite defense,
883120	890960	very few are symmetric. Most of the time, and most of the time offense wins. It is usually
890960	896320	easier to destroy than it is to protect. There are exceptions to this rule, for example,
896320	901040	cryptography is an interesting exception where defense is easier than offense.
902240	907120	But in most cases, it is easier to build a bomb than it is to build a reactor,
907680	915280	you know, a safe controlled burn. So all things being equal, you should expect that if you have
915280	920480	a technology and you distribute equally, that there will be more destruction. This is the default
920480	925440	case. This is what you should expect by default. Most technologies that are destroyed don't immediately
925440	929840	give you a way to defend against it. Developing vaccines is harder than developing bioweapons.
930480	933840	It is much easier to crank out a bunch of bioweapons and then you have to develop
933840	937600	vaccines in response to that, which is already super hard because, you know,
937600	942800	who knows how far the virus already is. So just because the technology is why the aspect does
942800	948320	not mean it defends wins. Whether offense or defense wins is a property of reality. It is
948320	954640	not a property of your morals or of your ideology. And the third point is an observation from my
954640	959280	own life is that I used to work in open source. I was one of the very first people to work on it.
959280	967040	And I had similar views to Licken and, you know, assuming he holds these views genuinely,
967760	972960	which, you know, I hope he does. I don't know him very well. I talked to him maybe once.
974160	981280	And I think this is just not even wrong. It's just in my experience, what happens when you
981280	985520	build AI models and you release them open source is that the first thing that happens to get
985520	990560	uploaded to Huggingface, and then a guy called the bloke, that's literally his name, uncensored
990560	996160	them, undoes any RLHF training or other security training run have done, trains them all the newest
996160	1002080	data to make them more powerful, more general, more whatever, uploads them again, 4chan downloads it,
1002080	1006160	down, you know, uses them for whatever their applications are, whether it's, you know,
1006160	1013200	pornography mostly, or likes BAM or whatever, et cetera. And now maybe this is fine, right?
1013200	1018400	Like, you know, you know, maybe we say, oh, it's okay. If people want to use their LLMs for porn,
1018400	1025040	so what? That's okay. Sure. What I'm saying is, is the empirical observation is that the amount
1025040	1031040	of effort that gets put into making these things safer or more controllable is absolutely pathetic
1031680	1035280	compared to the amount of effort that the open source community puts into making these things
1035280	1041280	more powerful, more general, and less controllable. This is just an empirical fact. This is just
1041280	1047760	actually, if you go online, you pick the top 1000 LLM repos, how many of them are about controlling
1047760	1052880	the models better versus making them faster, making them more efficient, distilling them,
1052880	1057840	making them more, et cetera. And the fact is that the offense, like the unbalance here is like,
1057840	1062240	it's not even funny. And I understand, right? And this is not to say that the people working on this
1062240	1066720	technology are like morally evil. I think this is an important thing to understand. There's an
1066720	1072000	incentive from people like Lacan and other like big tech, you know, people, like talking heads,
1072000	1078080	to try to focus on it's only the evil people's fault because that absolves them of responsibility.
1079200	1084800	Meta wants open source because it absolves them of responsibility as a corporation.
1084800	1089120	They can't get sued because it owes the user's fault. And this is also what's happening in the
1089120	1095200	EU AI act right now is that people like Lacan are lobbying to remove foundation models from
1095200	1101680	regulation in the EU and saying instead of their uses should be regulated. This is the same thing
1101680	1106720	as when, for example, plastic companies invented recycling. They invented it so that it was the
1106720	1112000	user's fault that there is all this plastic pollution. Like, oh, see, we would have recycled
1112000	1118240	it. But unfortunately, the users just didn't do it. This is a, this is gas lighting. And this is a
1119040	1125760	complete unbalance of power. The externalities of plastic pollution should be on the ones who are
1125760	1130960	most suited to addressing this externality, who are creating this externality. It shouldn't be on
1130960	1136800	the user. And the same thing applies to foundation models is that these systems can do things. They
1136800	1141360	can be used for many things. And we should be taking the big companies building these systems.
1141360	1144880	Is that what these open source models are being built by like, you know, plucky little teenagers
1144880	1150240	in their, in their, you know, rooms as a plucky teenager that did do that. I'm saying most of
1150240	1156480	the ones being built now are being made by like the UAE and meta. Like, these aren't the little
1156480	1160320	guys. These are big guys trying to shirk their responsibility to society.
1161600	1169040	Well, then what, what's the lesson from, from the open AI saga that, that you just need a
1169040	1175040	bigger board or you need a lesson is that none of these structures are correct. This is what we
1175040	1179360	have governments for. This is the same lesson that we've had over and over again. It's like
1180080	1186160	self-regulation does not work. It has never worked. Self-regulate. This is like tobacco
1186160	1190800	companies self-regulating themselves. This does not work. And we as a society have developed a
1190800	1195840	mechanism. I'm not saying it's a perfect mechanism by any means, but we do have a mechanism for
1196400	1201360	intervening in systems that have extreme high externalities that are not self-regulatable.
1201360	1211200	And it's called the government. Yeah. And, and I mean, there has been a lot of work at the government
1211200	1220720	level, not as much in the US as in Europe. But, but how, how do I mean, obviously these
1221680	1226960	models are so commercially, the potential is so commercially
1228560	1235280	exciting that fines aren't going to matter. You're not going to be able to find people
1236480	1242640	to behave in ways that the government wants them to. There's got to be something stronger than
1242640	1248640	that. So, have you thought about, about that? I mean, how do you regulate these things?
1251520	1257520	One of my, one of the most inspiring moments from the history, I think of science and society
1258160	1266480	is many decades ago, biologists and chemists and so on realized that human cloning should be possible.
1266480	1270480	Like it should be possible to do this. They were still far from having the actual technology
1270480	1275440	to perform with human cloning, but they found out it should be possible. And they reasonably
1275440	1280320	understood, wait, that might be really disabilizing. Like that could be, we don't know what the
1280320	1284080	consequence is, but maybe it's great. You know, maybe there's, you know, there's many benefits
1284080	1290160	from human cloning as well. But like, let's chill out. We don't know, like we don't know,
1290160	1295280	and this seems huge. This doesn't, isn't just like another thing. This is not a 10% more effective,
1295280	1302640	you know, cough drop. Like human cloning is a big deal. And so, heroically, long before the
1302640	1308640	technology existed, they came together and banned it and said, let's have a moratorium.
1308640	1314080	Let's not do this until we've had a bit more time to figure out what the hell we as a society
1314080	1319840	want about this. And it wasn't one board. This wasn't one CEO being like, I will, you know,
1320480	1324720	take a moratorium on this. No, it was the scientific community and governments coming
1324720	1330880	together and working very, very hard to create a moratorium. A moratorium is what we do when we
1330880	1336160	are faced with something which we know is huge and we don't know how to deal with. That's what
1336160	1340880	scientists do. You have a moratorium. And we should have a moratorium on AGI. This is what we
1340880	1349280	need to do. And can you enforce a moratorium? Yeah, I mean, it's like technically, like physically,
1349280	1354880	like, yeah, obviously, like, that's not that hard. Whether people will do that, whether people
1354880	1359680	want to do that, whether people can overcome the incredible political power that big tech has,
1359680	1364240	that's the more interesting question. It's not like the government obviously has the ability,
1364320	1368640	like the CIA can track every GPU in the country if it wants to. Like, you know, if you want,
1368640	1372960	if the NSA wants to shut down, just press a button. Like, that's not the problem. You know,
1372960	1377520	if you want to throw a couple CEOs in jail, like, sure, like the FBI can do that. Like,
1377520	1382560	physically, this is not a problem. It's a political problem. This is not a physical problem. This is
1382560	1387120	a political problem. The political problem is, well, if you have legislation around this kind of
1387120	1391840	stuff, well, we just saw what happens if you try to fire Sam Altman, you think he's going to be okay
1391840	1397600	with a huge GPUs away? Well, no, I expect that's going to be a hard fight. I expect, you know,
1397600	1402320	Microsoft lobbyists will fight that tooth and nail. I expect many people will fight this.
1402320	1405760	And this is why, like, you know, I'm not, I'm not here to give point to you,
1405760	1410320	paint you a rosy picture of the future. I'm not optimistic that things are going to go well.
1410320	1416560	We have an unprecedentedly huge political problem here. I think I'd like to say is the thing that's
1416560	1424880	killing us right now, it's not AGI. AGI doesn't exist yet. It's people. It's politics that is
1424880	1432880	killing us. Right, right. But and to that point that AGI doesn't exist, not so much all the other,
1433520	1443280	I mean, yes, no doubt, the political systems are not equipped to deal with the big problems facing
1443280	1451600	humanity. But in this case, AGI doesn't exist. I don't know how you would ban AGI, because
1451600	1459360	no one really knows how and when it might emerge, if it ever does. At the level of the tech now,
1459360	1467680	I mean, what are you, what are you suggesting? And I'm not putting on the spot. I don't expect you
1467680	1473520	to. Oh, I have policy proposals. I have very concrete policy proposals here, here are three.
1475120	1480240	The first one is a compute cap. There should be a limitation that no single training around no
1480240	1484880	single AI system can be built with more than a certain amount of compute. So luckily, we are,
1484880	1489920	so we are very lucky that current frontier AI systems, more and more general purpose systems
1489920	1494400	require more and more computing resources. These computing resources are very easy to track.
1494480	1499440	They're very bulky. They take lots of specialized knowledge, lots of energy. The kinds of supercomputers
1499440	1503920	that can train a GPT-4 or GPT-5 are only built by like three companies in the world,
1503920	1510960	and they're all in the US. So like, this is a solvable problem. And we should put a ban on,
1510960	1515680	you know, there should be like a registration process for, you know, frontier models up to a
1515680	1520800	certain limit. And beyond that, there should be just ban, just a moratorium. Just you are not
1520800	1526320	allowed to perform any experiment that requires more than, I don't know, 10 to the 24, 10 to 25
1526320	1536000	or whatever, FLOPs. FLOP being a unit of measurement for computing power. And this is easily enforceable.
1536000	1540000	This is absolutely something that like technically is enforceable with, it's just a political
1540000	1545680	problem. And this buys you time. Then you're, our scientists figure out, you spend time actually
1545680	1550800	figuring out how far is AGI away, how dangerous is it, how do we control the things, blah, blah,
1550800	1554480	then we can talk about those kinds of things. The first thing is to buy time. The second
1554480	1562880	proposal, or unless you want to comment on that. Well, just on that, you're talking about limiting
1563680	1573040	commercial products. But if, when you say then that gives the research community time to figure
1573040	1578960	these out, things out, they're going to have to experiment with larger models. So there's got to
1578960	1590000	be some. To be clear, these levels are insane. 10 to the 24, 10 to the 25, FLOP is an unimaginably
1590000	1594080	large amount of computing power. There are no academic labs, basically, that need this for
1594080	1601120	research, FD research. This is ridiculous. There is just no, so this is a common propaganda piece
1601120	1605040	that the big labs like to say is like, oh, we need more compute to do safety research.
1605760	1612400	Maybe this is true. I have not seen it. This is just not what has actually happened. Just purely
1612400	1620880	empirically speaking, there is, I have seen basically no safety AGI relevant research that
1620880	1627120	required more than like, you know, a GPT-3 that you couldn't have done with GPT-3 level of compute
1627120	1631760	or less. I have like, maybe it exists, but I sure as hell as I have not seen it.
1633760	1642240	Okay. So limiting compute is one proposal. What are the others you mentioned?
1642880	1648000	Two others I would recommend. The second is strict liability for model developers.
1648800	1654320	So what this means, so strict liability means that the intentions of the developer do not matter.
1654960	1660640	It would matter is that if a harm is caused, the developer is liable. I think this should
1660640	1666560	basically exist for the whole supply chain is that if you create externalities, you have to pay for
1666560	1672400	them. And this aligns the incentives of everyone aligned on the chain. Currently, there are no
1672400	1678240	incentives for developers to develop to minimize the externalities of their systems. Currently,
1678240	1683520	you as an open source developer can be an arbitrarily dangerous thing that causes arbitrarily
1683520	1689200	much damage. And you have no incentive to avoid this. As a concrete example, which not even going
1689200	1695840	to AGI is voice cloning systems. There are right now in GitHub, systems you can just download,
1695840	1700320	which take you 15 seconds of your voice, clone it perfectly. And you know, go call your kids,
1700320	1705200	call your wife, you know, just manipulate them, call in a swat hit on your, on you using your own
1705200	1710560	voice. This is all doable. And the people developing these systems have zero liability.
1710640	1715920	They don't even feel bad about it. Because it's open source, Craig. If it's open source, it must
1715920	1721520	be good. My ideology says so. And you know when your ideology tells you something is morally right,
1721520	1728880	then it's good as we've seen throughout history. So it's, so we have to align incentives here
1728880	1736640	somewhere along the line, you know, if a, it reminds me of cars and seatbelts in the 70s, where
1737520	1745600	car manufacturers fought tooth and nail to not have seatbelts. They fought it viciously with
1745600	1750720	propaganda and with lawsuits and with everything they could throw at it. Because they said, well,
1750720	1754720	it's the driver's fault. If he gets into an accident, it's not our fault. Like, you know,
1754720	1760800	we just build cars. If they drive it poorly and they die, well, it's not our fault. And we, you
1760800	1765920	know, the people rightfully told them to go fuck themselves. Like, no, you have to build a safe
1765920	1771440	product. You can't like, it's, it's not a moral question. It's kind of like the point I want to
1771440	1778480	make. I'm not making an ideological point. I'm not saying my religion says that seatbelts are good.
1778480	1786000	I'm like, I don't care. I care. Do seatbelts mean that less people die? And the answer is, yeah,
1786000	1792880	like they make cars safer. So then I want seatbelts. Cool. And the same thing applies to open
1792880	1799600	source. Does Linux being open source result in more safety? The truth is, yeah, looks pretty
1799600	1805440	obviously like case. So I'm in favor of Linux being open source. Awesome. Great. You know, does,
1805440	1810560	you know, some seven billion parameter model be open source positive or negative? I don't know,
1810560	1815840	probably positive. Like probably so. I'm not sure. Like there's a lot of downsides there as well.
1815840	1822880	But like, seems like it probably is positive. AGI being positive, you know, open source, you know,
1822880	1827680	that does not seem positive to me at all. That does not, that seems like a recipe for disaster.
1827680	1831120	So it's, I'm not trying to make an ideological point is what I'm starting to say. I'm not saying
1831760	1834880	all these things are good. All these things are bad. I'm saying we have to look at things
1834880	1840000	at a case by case basis. This is how proper regulation works. Proper regulation shouldn't
1840000	1846240	be ideological. It shouldn't be everything is regulated as ARB. That would be terrible regulation.
1846880	1854800	Yeah. Well, so that was the capping of the compute on training runs,
1856320	1861280	shifting liability to the model developer. What was the third one?
1861840	1865840	So the third one that I think should be done is that there should be a kill switch.
1865920	1870000	And what I mean by this is it doesn't have to be literally a switch. What I mean is there should
1870000	1878160	be a protocol that any developer of frontier AI systems needs to implement by which at a given
1878160	1886880	notice, any frontier training runs or deployments can be shut down in under a minute. So the reason
1886880	1890800	for this is not per se because I need, I think necessarily that this would be very helpful.
1890800	1895280	The AGI actually happens. If AGI actually happens, this is probably useless. The reason I think this
1895280	1900800	is good is because we should have the institutional capacity to do these kinds of things. There should
1900800	1904720	be every six months, there should be a fire alarm. There should be a fire drill where everyone has
1904720	1909520	to practice. In the next five minutes, all AI companies have to go offline for 60 seconds.
1909520	1913920	If not, you get slapped with a huge fine. These are the kinds of protocols you want to have
1914560	1919280	in worlds where you have tail risks, where things can blow up, where you can have these
1919280	1925600	kind of things. And then there should be a multilateral K of N kind of system around this.
1925600	1931760	Like maybe all major global powers have one of these buttons and if three or five of them push it
1931760	1938880	or seven of 10 or whatever, then the system kicks in. This is the kind of institutional
1938880	1943120	building which doesn't save us, but it's a hell of a lot better than nothing.
1943840	1954880	And how do you see these kinds of proposals moving through the policy making frameworks?
1955680	1966640	There is some advance in the European Union. The White House has come out with its
1967440	1972640	executive order, which as yet doesn't have any real concrete
1975440	1982800	government governance policy in it, but it sort of lays out the things that we should
1982800	1994320	be thinking about. Where do you see these things going? What sort of a timeline do you think that
1994320	2001840	governments are being educated enough that they can deal with this? What government is
2001840	2009120	going to lead? Is it the EU? Will it be the US? Who should it be? And then of course you've got
2009920	2016880	the other side of the world, Russia and China, who have very different agendas and may not want
2016880	2023520	to regulate at all. So when people ask me questions like this and they're like, what's your probability
2023520	2030640	of X happening? And then my follow-up question is usually, is it X conditioned on me and other
2030640	2036880	people doing something about it or not? Because I expect if they're conditioned on me and other
2036880	2041120	people don't do anything about it, then yeah, I just think nothing will happen if big tech wins
2041120	2047520	and then we die. I think it will be very heroic or special. It will just be new products keep
2047520	2051600	happening, AI keep going up, and then just one day, humanity's not in control anymore and we
2051600	2055920	have no idea what's going on. And then it's just over. I don't think it will be dramatic. I think
2055920	2060320	we will just get more and more confused. We won't understand what's going on anymore. Weirder and
2060320	2067600	weirder things will happen, more and more politics, economics, markets, media is controlled by AI,
2067600	2072400	or even just fully generated by AI. There will be no more movies or just AI generated. And then
2072400	2076640	just humanity will not be in control anymore. And then one day we fall over dead for some reason,
2076880	2082000	we don't understand. That's what I expect will happen by default. And along the way to be clear,
2082000	2088080	big tech will pick a lot of money. So go buy that Microsoft stock. You'll get really rich
2088080	2095920	just before you die. So if I could addition on someone actually doing something about this,
2095920	2100640	I do think there is hope. I don't think there's a lot of hope, but there is hope. And the main
2100640	2112320	hope I see from this is that the general public fucking hates AI. It's unfathomable how much
2112320	2119200	normal people hate AI. They use it, of course, but they're freaked out by it, which is just completely
2119200	2125040	the correct reaction. It's just these crazy bizarre weirdo tech people like you and me
2125040	2130560	who are not instantly like, wait, that's actually, let's not do that. If you talk to any normal
2130560	2134480	person, you're like, hey, these people are building systems that are smarter than humans.
2134480	2142560	They don't do that. That seems really dangerous. Don't do that. Well, all the people are like,
2142560	2151760	oh, but actually you see my proposal because we'll make it fine. Or actually universal love
2151760	2157600	means that AI systems will love whatever. I don't even know what these people say anymore.
2157600	2161200	I think they've given up making arguments at this point and they're just vibing.
2162160	2169440	So I don't even know if there's an argument that debunked there. So from my perspective,
2170240	2177760	it's we are building systems. They are going to be built by default unless we do something about it.
2177760	2182960	So the general public wants these systems to not be built, or at least for us to slow down,
2182960	2186160	until we can make them safe and we understand them better and they've been integrated to society,
2186160	2190480	et cetera, et cetera. So now you might ask the question, okay, well, that's true.
2191280	2196320	Why is fuck all happening? And that's a good question. And now we have to talk about models
2196320	2203920	of policy change and like global coordination, which at least how I think about this problem
2203920	2211200	generally is that the general public actually does have power in the West and like in democratic
2211200	2217120	countries. It's very fashionable among elites to sneer and be like, Oh, actually, you see the
2218400	2224000	populace, you know, they don't have true control, you know, we live in a whatever the words are
2224000	2230800	that people like to use. And this is to a large degree true, but it's not fully true.
2231600	2238640	The main problem is that the general public has extremely short attention spans
2238720	2246320	and extremely discoordinated. This is the main problem. The bottleneck on policy action currently
2246320	2254400	is not will of the people. It's not ability to enforce regulation. It's coordination. It's
2254400	2259360	getting people to actually do something about it, you know, to actually write letters to their
2259360	2263600	senators, actually put things on their desks, actually yell at them on the phone, you know,
2263600	2268240	actually like, you know, talk about on social media, et cetera, et cetera. This is the kind
2268240	2272880	of thing that's currently missing basically campaigning. This is the kind of stuff that
2272880	2281440	is missing. And I expect that if you did this well, if you raise this to saliency about people,
2281440	2284400	you wouldn't have to you wouldn't have to convince them. And I'm saying this because
2284400	2288240	empirically, this has been true in my experience, like talking to people and also like doing stuff
2288240	2292320	like focus groups and stuff. I found that you don't really need to convince people very much.
2292320	2297040	You mostly just have to tell them facts just have to, you know, just like present them with,
2297040	2303120	hey, this is what's going on right now. And then mostly they converge to the like a reasonable
2303120	2309600	beliefs around like, hey, that's scary, don't do that. So I think this is currently the best
2309600	2317280	path we have. I'm also, you know, excited to talk to politicians and I talked to many of them,
2317280	2324320	mostly in the UK and the EU, because I'm UK based. But it's hard because, you know, politicians have
2324320	2327840	similar problems. They have very little attention span, because they have so many things they need
2327840	2336720	to do. There's so many things haranguing them. And my model of policymakers is basically that the
2336720	2346320	ultimate goal of a politician is to not get blamed. So it's because the politician you have
2346320	2351120	really like I have so any if there's any policymakers listening or any staffers or so on,
2351200	2357760	I feel you, you're in a shit spot, I get it. Because like, basically the way I see it is like
2357760	2363200	there's like a two by two grid of like what you do as a politician, which you can do. So
2363920	2370560	the idea is that there's a default action is that in a common, in our common, you know, feelings
2370560	2374880	around issue, there's something that is the default thing to do, which is usually nothing.
2375840	2382000	If you do the default action, and it goes wrong, well, you're not blamed, you know,
2382000	2387600	because, you know, you did the sensible thing, not your fault. If you do the default action,
2387600	2393440	and it goes, well, well, great, you're a genius, you know, good job. If you do the non default
2393440	2400720	action, and it goes great, cool, yeah, you're good, great. If you do the non default action,
2400800	2407920	and it goes bad, then you get blamed. That's how you get blamed. So you may notice from
2407920	2412480	this payoff matrix, that it is always better to take the default action rather than non default
2412480	2419280	action. It is always better for the politician to not stray off the path. And this is universally
2419280	2424880	true. So it's easy to yell at politicians and be like they have no spine, they have no courage
2424880	2430880	and whatever. And yeah, that's true for many of them. Many of them are just, yeah, just, you
2430880	2436000	know, just don't care, true. But some do, and they do go off the path and they get burned for it.
2436720	2442400	And that sucks. But it is how the game is. So what we can do as the people is we have to change
2442400	2447760	what the default action is. You have to change the narrative from, I guess we just keep bumbling
2447760	2454800	along until we die to how the fuck dare you keep bumbling, like seize your bumbling immediately.
2455360	2459440	Bumbling is no longer accepted. And that's my biggest hope at the moment.
2460160	2469280	Yeah. When we spoke last time, again, right as GPT four was being released.
2469280	2477600	One of your immediate concerns was that these things can be hooked up to
2479680	2487600	systems that can take action. And I don't remember if we talked about auto GPT that first,
2488480	2496400	I haven't looked at what's happened with that, but that first attempt to create an agent that could
2496400	2504960	use LLMs. But that has developed a pace. And we're now on the cusp of seeing
2506640	2513120	sort of an explosion of AI agents that can leverage the power of large language models or other
2513840	2525440	other tools. I had a guy on earlier from News Guard, a company that builds databases to
2526720	2532320	try and help companies, tech companies identify
2532720	2544160	disinformation and combat it. And we were talking about, once you have these agents building
2547600	2555040	creating disinformation, not only creating the disinformation, but distributing it on a massive
2555040	2564160	scale and maybe on a massively parallel scale. The internet, public discourse, everything is
2564160	2571600	going to get very confusing because you're not going to be able to tell what's real and what's
2571600	2580400	not real and people, which is the majority who are not particularly careful about where they're
2580400	2593520	getting their information will be manipulated. So yeah, the coming AI agent era, how do you
2593520	2606080	deal with that? I mean, I don't know, get your affairs in order. A number of years ago, post
2606080	2614880	GPT-2 was around GPT-3 time. That's how we mark the eras now. Instead of years, we just use GPTs
2614880	2625600	now. I was invited to work kind of like just like a discussion group with some open AI people,
2625600	2631600	policy people, disinformation experts and stuff like this about the potential for misinformation
2631600	2638800	and so on from language models, especially before GPT-4, before chat GPT and so on. And
2640720	2646960	polite lead to all these well-credentialed experts with their triple Stanford professorships or
2646960	2654560	Harvard, whatever, talk about misinformation, bias and whatever. And then when it came my turn
2654880	2663040	to talk, my reaction was like, holy shit, you're all so undressed. You're being so optimistic.
2663040	2669280	It's so much worse than any of you. You're like, oh, it could make it easier for far writers to
2670240	2675120	that's that's that's fucking children's play compared to what you could do with these things.
2675120	2679040	Like you were truly you're not creative. Like if you think that's the worst that can happen,
2679040	2682800	oh, they're going to generate some fake news and some like Russian digital websites. I mean,
2682800	2686880	oh boy, that would be nice. That's the nice timeline. It's going to be much worse than that.
2686880	2691280	It's already getting worse like that. Talk about fully automated cults with fully
2691280	2700720	automated profits. Talk about all sensory, illusionary interactive systems, creating
2700720	2705040	full complex narratives that are completely disconnected from reality. Talk about full
2705040	2712240	epistemic collapse, the semantic apocalypse. Even if AIs don't kill us, they're going to drive us
2712240	2720160	insane. So it's because it will just be harder and harder and harder to survive in a more and more
2720160	2724560	adversarial informational environment. This has already been happening for a very long time.
2724560	2728880	You know, we just had Thanksgiving. And as much as we love her, we all have that one
2728880	2736560	aunt that get way too into QAnon a while back. And imagine so, you know, currently stuff like
2736560	2740880	QAnon or like, I don't even know if QAnon is still a thing, but like whatever the newest thing is,
2740880	2745760	the newest cult is, the newest whatever is, you know, that affects, you know, some percentage of
2745760	2749440	the population, you know, some percentage of the more vulnerable population. I'm going to say
2749440	2753520	stupid, just like, you know, maybe emotionally vulnerable or like epistemically, you know,
2753520	2759120	vulnerable and for some reason, not trying to judge these people here. Now imagine the bar keeps
2759120	2764000	racing. You get systems that become more and more convincing, that become more and more
2764000	2769520	sophisticated, more and more targeted, and slowly, slowly, the number of people who are just
2769520	2777760	functionally schizophrenic keeps going up until at some point, people cannot converge on reality
2777760	2784400	anymore. And people just every person you meet is functionally schizophrenic. You cannot run a society.
2784400	2791200	You cannot organize a system if you and your neighbor cannot come to a conclusion about
2791200	2797920	basic reality. This is like what is possible with these kinds of systems. I'm not saying this is
2797920	2802560	going to happen next year. I mean, maybe, but this is the kinds of things you couldn't do.
2803120	2809920	Like the like epistemics is hard. Like this is the thing that like, there's also things like honesty
2809920	2814640	is hard. This is like some people are like, Oh, just, you know, misinformation is a trivial
2814640	2818240	concept. It's almost become a slur at this point. It's kind of come a joke, you know, like when
2818240	2821840	people use the word answer information, like, at least in my social circles, a lot of people like
2821840	2827200	rolled their eyes to be like, Oh, yeah, anything that isn't big media isn't this information,
2827200	2833520	whatever. But like, it's just not that easy. Like finding out what is true and disseminating and
2833520	2839840	evaluating what is true is hard. This is very hard. It takes energy. It takes effort. It takes
2839840	2845280	mechanisms. It takes like it's hard. And it's going to get harder. It's going to get more expensive.
2845280	2851600	But currently, like, do you really know what's happening in Ukraine right now? Really? I don't.
2852080	2857680	I think I'm at a point where it is like literally impossible for me to actually know what's going
2857680	2863440	on in Ukraine. It's something that affects me, you know, affects family, friends, you know, it is
2863440	2869680	a huge thing. I don't think that there is any way I could actually acquire and verify
2870560	2877600	that the truth of what is actually going on there. And this generalizes. This is even before we get
2877600	2882960	into agents doing worse things than this. I mean, automating all jobs, obviously, you know,
2882960	2888160	anything you can do at a computer, an agent will do better and faster. So there will be complete
2888160	2892880	economic collapse from that. Like, obviously, there will be no more need for human jobs unless
2892880	2897200	until the inference costs, you know, get too high. But you know, you can improve those back down.
2897760	2904960	You'll have systems that can do harm in various ways, you know, by manipulating markets,
2905040	2911200	campaigns, politics, you're going to have systems that are, you know, cybercrime, hacking, you
2911200	2916160	have system like it's like when you ask a question like, what is the worst thing agent based systems
2916160	2920400	are doing? You're asking the question, what are the worst intelligence systems can do?
2920400	2924640	What is the worst that a human can do? The answer is a lot.
2924960	2930560	Yeah. But again,
2933600	2944320	yeah, I mean, you can you can see that that very bleak future. But I'm also a great believer in
2944400	2946880	in how
2950640	2964960	mankind, the worst case scenario generally is not what happens. And people kind of muddle along.
2965920	2972720	But that survivorship bias, there was a man named Stanislav Petrov, who was a Russian soldier
2972720	2977920	stationed in a nuclear bunker. And he had the command that if American missiles appear on the
2977920	2985680	screen, he shoots the missile. And one day, six missiles appeared on his screen. His commands
2985680	2990560	were very clear. The second guy with him there who had, you know, the other key was ready to
2990560	2994720	turn and yelled at him that it's time we have to shoot back. The Americans are attacking.
2996400	3002400	And Stanislav didn't. He disobeyed orders. He could have been, you know, fucking executed for that.
3002400	3007200	And he disobeyed orders that day. And it's because of this one man, one Russian soldier,
3008240	3014160	that you and me weren't nuked. One guy, we got lucky. So when people said, oh, but so far as
3014160	3016800	I was like, what the fuck are you talking about? This is like saying, well, I've played Russian
3016800	3022160	roulette five times so far and it's been great. Let me pull again. That's just not how anything
3022160	3028960	works. This is not how reality works. If you play like this, then eventually you predictably lose.
3029920	3035360	You have to play strategies where you can win in adversarial environments where you can play,
3035360	3040480	where you can win in games where dangers exist. Our ancestors, when they were in the wild,
3040480	3045600	they couldn't be like, well, oh, my forefathers survived. So I don't have to worry about bears.
3046160	3050720	You know, none of my forefathers got killed about bears. No, like that's just, no,
3050720	3055840	this is not how things work. The world isn't nice. There is no arc of history. There is no God
3055840	3061360	that is protecting us. The fact that we are here today is because of the hard work of our ancestors.
3062000	3066560	The fact that I live in this nice, you know, warm apartment, sound like safe that I have enough food
3066560	3073920	to eat and so on is not God that gave me that. It's not some, you know, force of nature. It was
3073920	3081840	the hard scrabble and bloody fight of my ancestors that left me this. And if I let this to rot,
3081840	3089520	if me and other people don't maintain society, then it just dies. Like then entropy wins.
3090160	3097200	Entropy always increases and entropy is death. So if we just sit back and hope things will go well,
3097760	3104960	they will not. So, you know, I was gonna, I was thinking, well, that's a good place to end it,
3104960	3114320	but I don't want to end there because our last conversation got an inordinate number of views.
3115280	3125120	And I have some producers that take these and turn them into shorts and they have these sound bites
3126080	3135200	from that episode that have gotten an enormous number of views because people gravitate towards
3135200	3146240	these doomsday proclamations. And I don't, I mean, whether or not they're true, I want to end
3146960	3154800	something more hopeful. So what, what, what should people do in your view? What should
3154800	3162960	regulators be doing? What should researchers be doing? What should Microsoft be doing now?
3163680	3169280	So the weirdest thing I would tell them is like, to be clear, I don't like being the doomed guy.
3169280	3173680	I absolutely don't like this. I was the pecto optimist throughout my entire life. I was always
3173680	3177760	the person saying, no, we can fix problems. Climate change is solvable. You know, solar
3177760	3181760	powers can be exponentially cheaper. You know, we can do carbon capture with like, there are so
3181760	3186320	many things we can do. I've always been saying like, no, like, you know, see how in the interest
3186320	3190720	improved education, how much people are becoming, you know, better at, you know, and having more
3190720	3196560	access to information. Look at how so many things are like, I was just reading the other day about
3196560	3203360	how slowly over decades, just the flash freezing of frozen food has gotten better. And I've noticed
3203360	3208160	this, just like my frozen broccoli, I'll make it night. It's just a little bit nicer. And you
3208160	3212160	know what, that might sound like a teeny thing compared to all these other things, but I think
3212160	3217920	that's beautiful. I think it's extremely beautiful that life gets better. All things being equal,
3217920	3224080	life has gotten a lot better. I'm very happy to be alive when I am right now. All these small things
3224080	3228640	done by these smart people, mostly done for profit. Sure, the broccoli company, they just want profit,
3228640	3233280	but ultimately, they made my dinner a little bit nicer. It was already fine. Like I was already
3233280	3237680	surviving, but it was a little bit nicer. And you know what, that's awesome. And it's so nice
3237680	3244080	that we can live this way. The truth is, is that we are so lucky that we live in a society full of
3244080	3251040	educated, smart people that for the most part, you know, not all over more angels, they're not heroes,
3251760	3255280	but they want to make, they do want to leave the world better, you know, they want people to be
3255280	3260400	happy, they want people to be safe. Most things being equal, you know, almost everyone, you know,
3260400	3264000	given the option, if they could just help someone else and it didn't cost them anything,
3264000	3269600	they'd do it. And that's really nice. So we have to leverage this. We have to leverage that we,
3269600	3275200	and this is not the case everywhere in the world, I want to say. This is something that even today
3275200	3281200	is not in every country. It is not in every place or in every society. But in the West and, you know,
3281200	3287680	many other countries in the Far East and so on, most people are educated. Most people are decent.
3287760	3293920	Again, I'm saying they're great or heroes, but they're decent. And they want the world to go well.
3293920	3298880	They want their kids to grow up and have a nice life and, you know, eat nice frozen, you know,
3299600	3306320	broccoli, you know, whatever, you know, they want to see art and beauty and, you know, music and so
3306320	3311280	on. And we can have this. This is the important thing to understand. The important thing is,
3311280	3316240	sometimes I'll talk about this, is that like this, this idea of techno optimism, quote unquote,
3316240	3320560	it's just cynicism and disguise. This is a really important thing to understand.
3320560	3325280	These people who put, who's talking about, oh, yeah, actually, we're, we're techno optimists,
3325280	3330720	we're accelerationists or whatever. They're just cynics. They're just libertarian cynics
3330720	3336720	that don't believe that society can be improved, except by just like giving themselves to this
3336720	3342080	abstract process of technology. But technology is not a force of nature. It's not a thing
3342080	3348240	happening to us. It's a thing that we do. It's like, it's about humanity. It's not about technology
3348240	3352640	that like, sure, technology is great, it's helped humans, but I only care about technology because
3352640	3357360	I care about humans, care about people. And we all care about people. We care about our families,
3357360	3362400	we care about our friends. And technology should be a tool. It should not be a goal in and of itself.
3362400	3367760	So when people talk about, well, AGI is inevitable, someone's going to do it. No, no, it is not.
3368320	3373840	It is not inevitable. It is not a force of nature. It's a decision we make. It is a decision we make
3373840	3383120	and we can do better. We can, as people, societies, as civilizations, make choices. We can say, hey,
3383120	3389520	let's be a little more careful. That doesn't mean we'll do not do any AI anymore. We can just say,
3389520	3395280	hey, give our scientists a couple more years, a couple more decades to understand the mathematics
3395280	3399760	of interpretability better, and then maybe we'll give it another shot, you know, like we did with
3399760	3405040	human cloning. These are what's important. I'm not saying that this is easy or that this is what's
3405040	3410320	going to happen. It's because it's not what's going to happen by default. But it's just important
3410320	3417200	that there is this poison in our society that believes that the future is already decided.
3417760	3424240	And it is not. The future is not yet decided. We still have a choice. It is not yet too late
3425280	3432160	for what it will be soon. Hi, I wanted to jump in and give a shout out to our sponsor,
3432160	3440960	NetSuite, by Oracle. I'm a journalist and getting a single source of truth is nearly impossible.
3440960	3446800	If you're a business owner, having a single source of truth is critical to running your
3446800	3457360	operations. If this is you, you should know these three numbers. 36,000, 25,1. 36,000,
3457360	3463920	because that's the number of businesses that have upgraded to NetSuite by Oracle. NetSuite is the
3463920	3470560	number one cloud financial system, streamlining, accounting, financial management, inventory,
3470560	3479600	HR, and more. 25, because NetSuite turns 25 this year. That's 25 years of helping businesses do
3479600	3486880	more with less, close their books in days, not weeks, and drive down costs. One, because your
3486880	3494320	business is one of a kind. So you get a customized solution for all of your KPIs in one efficient
3494320	3501840	system with one source of truth. Manage risk, get reliable, forecast, and improve margins.
3501840	3509120	Everything you need, all in one place. As I said, I'm not the most organized person in the
3509120	3515920	world, and there's real power to having all of the information in one place to make better decisions.
3516880	3522160	This is an unprecedented offer by NetSuite to make that possible.
3523520	3529760	Right now, download NetSuite's popular KPI checklist, designed to give you consistently
3529760	3542000	excellent performance, absolutely free at netsuite.com slash I on AI. That's I on AI, E-Y-E-O-N-A-I,
3542080	3550880	all run together. Go to netsuite.com slash I on AI to get your own KPI checklist.
3550880	3562880	Again, that's netsuite.com slash I on AI, E-Y-E-O-N-A-I. They support us, so let's support them.
3563680	3573200	That's it for this episode. I want to thank Connor for his time. If you want to read a transcript
3573200	3582160	of the conversation, you can find one on our website, I on AI. That's E-Y-E-O-N-A-I.
3583120	3594080	As I always say, the singularity may not be near, but AI is changing your world, changing it rapidly,
3594800	3602000	so pay attention.
