start	end	text
0	4320	There isn't a science around that isn't profoundly influenced by the availability of
4320	8960	massive computing power and just greater regular computing power. It's the story of our age. It's
8960	15680	not just the story of AI. The idea is to leverage computation to make useful things and understand
15680	20320	the mind. These all these things need a lot of computation. It's the fact that computation has
20320	27040	become cheaper exponentially for on the order of 100 years and can be expected to continue going
27520	33680	It looks like doubling every two years now every 18 months and that keeps happening 18 months after
33680	39600	18 months after 18 months and it means you double and you double and things get qualitatively different
39600	44880	every decade and that's happened for a long time for many decades and will happen more so in the
44880	48880	future. So we have that to look forward to. I think it's what we really should mean when we
48880	54000	say the singularity. The singularity is that we have this exploding it's a slow explosion of
54000	59440	computer power and that that is fundamentally changing things. Hi I'm Craig Smith and this is
59440	66560	I on AI. In this episode I speak with Richard Sutton the father of reinforcement learning
66560	72320	and professor at the University of Alberta. We discussed his cooperation with John Carmack
72320	79520	on Keen a startup that vows to reach artificial general intelligence by 2030. Richard also talked
79520	85920	about the Alberta plan his ambitious five-year research agenda focused on building embodied
85920	92560	agents with the capability to learn and plan through interactions with their environment.
92560	98640	Sutton provides insights into the current state of progress new algorithmic developments
98640	105200	and trade-offs between simulated and physical environments in training and the ultimate goal
105200	112640	of creating AGI. I hope you find the conversation as amazing as I did. So why don't you start by
112640	118640	introducing yourself. I assume people know who you are I've had you on the podcast before
120000	128960	but for those new listeners tell us who you are where you are and then we'll talk about the Alberta
128960	138800	plan which I find pretty exciting. Thank you Craig I'm Richard Sutton I'm a scientist I've been
138800	146480	studying artificial intelligence for like 45 years a long time and I'm up in north of the
146480	153280	University of Alberta in Canada and I'm a professor in the computer science computing science department
153280	161040	and also I'm a researcher at Keen Technologies and I got lots of titles and sub-rolls but basically
161040	166560	I'm just trying to figure out how the mind works and I've tried to do it in a very broad and
166560	172880	interdisciplinary way reading all the different thinkers on the subject and addressed from the
172880	180640	point of view of psychology and how the brain might work as well as. Yeah I've read a number of the
180640	188240	recent papers and I can see this thread developing and I don't know whether it's just that you're
188240	195040	writing more and so the thoughts are are more developed in print or whether they're developing
195040	203200	in your mind but from 2019 when you wrote the bitter lesson you talked about the idea that
203200	212000	it's really increasing computation and the striving a lot of things a lot of progress
212720	223600	that kind of coincided with open AI's scaling of the transformer model I talked to Ilya
223600	233280	Sutskover and I asked him whether your essay had triggered their their interest in scaling and he
233280	241600	said no it was coincidental but I kept first can we talk about that about how scale scaling and
241600	250480	and the availability of computational resources and Moore's law has driven a lot of what's happened
250560	258560	in artificial intelligence research almost more than novel algorithms well I think the
258560	263920	first thing to be aware of is it's it's been driving things that are not just artificial
263920	270240	intelligence it's been driving all the sciences and all the engineering developments in the world
271520	276000	there isn't a science around that isn't profoundly influenced by the availability
276720	281440	availability of massive computing power and just greater regular computing power
282480	288480	it's it's it's the story of our age it's not just the story of AI it's not particularly the story
288480	298080	of AI AI has always known that it needs computation the idea is to leverage computation to make useful
298080	306720	things and understand the mind um yeah now it's true that those of us who are interested in
306720	313200	connection to systems or distributed networks nowadays just call neural networks not particularly
313200	318880	good terms so I always shudder a little bit when I use it but those of us that have been doing that
318880	324320	have have those are doing learning I think that learning is important for intelligence
325040	330960	these all these things need a lot of computation and so they're they are limited by the computation
330960	337680	available at the time okay so let's let's be what is this thing what is the so Moore's law what's
337680	344320	called Moore's law it's the fact computation is becoming more plentiful and cheaper exponentially
345040	351280	for on the order of a hundred years and can be expected to continue going that way so
351360	356880	exponentially looks like doubling every two years now each every 18 months and that keeps
356880	362560	happening 18 months after 18 months after 18 months and it means you double and you double
362560	370560	and things get qualitatively different uh every decade and that's happened for a long time for
370560	376080	many decades and will happen it's more so in the future so we have that to look forward to that will
376080	385600	continue having a tremendous influence on everything that's done on the other hand it's just normal
385600	392880	it's just what you would expect and that those of who worked on AI for for a long time have just
392880	401680	you know expect and plan for and um now it's coming but it's an exponential so exponentials
401680	407440	are self-similar so that means they look the same at every point in time every every year it's
407440	412800	you're doubling in a year and a half and so it's it's an explosion as every exponential is an
412800	418560	explosion it's it's sort of I think it's what we really should mean when we say the singularity
418560	425760	the singularity is that we have this exploding it's a slow explosion of computer power and that
426160	432240	has fundamentally changed things yeah and I had a really interesting conversation almost a year
432240	440400	ago with Aidan Gomez who was on the team that that that designed the transformer algorithm at
440400	449120	Google and he now has a startup co-coher he's Canadian and he said an interesting thing that
449840	456160	that he believes it could have been almost any algorithm it didn't have to be the transformer
456800	463760	that the community got behind the transformer poured resources into it continued to scale it
464560	470240	and it was scalable I mean that was important that it that it's a scalable architecture but
471440	477200	that but it didn't have to be the transformer and and that made me think of you because
478000	487840	of so transformers they the way he described it at its core it's a stock of multi-layer
487840	495680	perceptrons with attention you scale it feed it data and it does learns to understand language
495680	502240	or at least seems to understand language but it's got all these obvious limitations
503200	510240	of I've been talking a lot over the last couple of years to Yamakun about world models and that
510240	521200	to me sounded like a much more exciting direction for general intelligence because not all intelligence
521200	531520	is is contained in language or at least most or even less so in human text and then I see you guys
531600	540640	come along with the Alberta plan and that that that sounded even more exciting to me so
542480	549600	how how do you so the Alberta plan you're building the ideas to build an
550640	558480	agent ultimately an embodied agent that that has a world model or can
558720	566720	and create a world model through interactions with its environment how is that different from
567600	578640	Lucune's approach at a very basic level very basic level a good is that they're a very similar
578640	584320	idea it's uh you look at the parts of his architecture and the parts of the architecture
584320	590800	put forth in the Alberta plan they line up one one for one yeah we're trying to do the same thing
593280	597440	we're going about it slightly different and we could talk about that but I think
598080	602160	to just to focus on the differences might even be to distract from the big message the big
602160	608400	message is that you have to have a goal and you have to have a model of the world and
609360	618160	and then everything is driven by using that model to take action and to plan action at various levels
618160	627600	of abstraction in order to to achieve the goal okay so to me this is really what intelligence is
627600	632880	understand the world use your understanding to get to achieve to achieve your your goals
633520	638320	I'd like to formulate the goals as as a reward and I'm super comfortable with that other people
639200	642400	sort of grudgingly accept rewards even though it seems kind of low level
643120	648880	but it's a it's a natural approach I think I think it's something that almost makes more
648880	653520	sense to people who aren't steep and deep learning and to supervise learning and one thing I found
653520	660000	interesting in in the roadmap that you've laid out for the Alberta plan you start with supervised
660000	667680	learning and why is that is it just because it's it's easy yeah I guess we do in a sense
667680	673120	because we want to focus on well continual learning learning continually which is sort of
673120	678800	an obvious thing almost what learning means it has something that goes on at all times but
680000	688800	the first steps getting continual learning with nonlinear networks is still challenging
688800	695360	even for supervised learning and so it's natural to start at the simplest possible case which involves
695360	703280	the fewest other factors and that's a supervised learning case yeah yeah it's funny let me just
703280	708880	say a few words about that because there's sort of been a fight through a struggle throughout the
708880	715280	decades between supervised learning and reinforcement learning you know there's only so much oxygen
715280	721120	for learning methods and all the attention that's paid to supervised learning somewhat
721120	726080	detracts from reinforcement learning so there's a there's a there's a bit of a friendly competition
728160	733200	and supervised learning has always won the competition because supervised learning is so
733200	738960	much more easy to put into practice and for people to use and it's sort of it's sort of
738960	743600	less ambitious but it's really important and really those of us who do reinforcement learning
743600	750000	or try to make whole agent architectures we are consumers of supervised learning outcomes we will
750000	755120	use them as components of our overall architecture so we need them and we can work on them and we
755120	763440	need to structure them for our purposes I saw one of your talks you make a distinction between
764400	773840	AI tools and AI agents and supervised learning falls into the tool category can you sort of
773840	780320	start and and talk about the evolution of the Alberta plan and then present to listeners
780960	788480	what it is in in its simplest form and that'll that'll give me a structure on which to hang
788560	796800	questions the Alberta plan is an attempt to understand intelligence as a as a primarily
796800	804480	a learning phenomenon assists us something that comes to understand its environment and and then
806000	812640	drives the environment to achieve goals so the first step in the Alberta plan is the structure
812640	817280	between the agent the environment and their interaction form the interaction there's the
817280	823200	you're not exchanging states you're exchanging observations like sensors sensors visual touch
823200	831760	auditory it's all abstract to those particulars but it's got to be genuine observations and not
831760	839520	state because state we don't we don't really have access to directly so that you know the
839520	844160	principles number one principle I'm trying to remember them as I speak but number one principle
844160	852080	is this this agent environment interaction is sacrosanct and number two is that learning or
852080	858400	everything is is we could say continual I think we call it we say temporally uniform
859520	864560	temporally symmetric in in the Alberta plan which means that there are no special phases
864560	871280	where you like training and test there's just life goes on and on you get rewards or you don't get
872240	878960	or you don't get the reward you want and you get your observations and there there is no teacher
878960	890160	other than rewards pains and pleasures and maybe I'm not getting the four principles right but
890160	896000	another important point is that you are going to be forming a model and so you're going to plan
896640	903120	both trial and error learning directly from experience and learning a model and then planning
903120	909440	with the model both these are important part of intelligence okay so those are that's the
909440	915440	background then we outline there are 12 steps and the 12 steps really start with let's have
915440	921520	learning that is temporally uniform let's have metal learning and metal learning maybe I should
921520	928240	stop and on that for a moment metal learning means learning to learn not just learning one function
928240	932800	but once you are continually learning you're learning this and you're learning that you get
932800	938080	many many experiences learning and you can get better at learning you can use those repeated
938080	946160	experience with repeatedly learning to make make future learning episodes more efficient so as part
946240	953440	of that you learn representations you learn features you learn step sizes
955680	960800	okay so continual learning and then all the algorithms and once once we add
960800	965840	metal learning and continual learning we have to in supervised learning then we extend that to
967520	972720	reinforcement learning which involves its own set of issues to get more interesting temporal
972720	982080	relationships and I think like the first six steps are crafting the basic algorithms of reinforcement
982080	990080	working through them again to be continual and meta and then we start to bring in the the challenging
990080	997040	issues like learning off policy and learning models of the world and then planning and the
997840	1000320	just to jump to the end the last step is about
1004320	1009520	AI, AI, AI's, AI intelligence augmentation
1011760	1019520	where we combine computers, AI's with our own minds to make make our own minds stronger
1020960	1026640	okay now one of the key steps in there was off policy learning and learning a model of the world
1027840	1033440	off policy learning means you want to be able to learn about things that you're not doing or you're
1033440	1040640	not because you're not doing all the way to completion so even like to recognize an object
1040640	1047760	you look at the object and you say how would you you have to define that in some objective way
1048400	1053280	and the best way to just do that is as a sub problem so
1055520	1063440	yeah maybe maybe I'll just sort of stop there the most interesting strategy
1065280	1070160	distinctive strategy by the Alberta plan is the pose is that the mind works by posing sub
1070160	1075200	problems for itself and then working on them and it's it's not it's sure it's got a main
1075280	1080480	problem which is to get reward but it's also has many thousands of sub problems it's also
1080480	1085840	working on simultaneously and since it's not behaving it cannot behave for all thousand
1085840	1090080	problems at once it has to pick one problem like perhaps the main problem and behave according to
1090080	1094880	that so all the other things have to be able to learn from data that's not exactly on what they
1094880	1102720	would do and this is called off policy learning and it's a key to learning to achieve auxiliary
1102720	1108720	sub problems and also it's a key to efficiently learning a model of the world yeah you you have
1109760	1117360	something called the horde architecture is is that where that comes in when you you break
1117360	1125360	a problem down into multiple sub tasks that that you learn I was one one paper where we
1126480	1129920	we worked on that idea we developed that idea the horde is the horde of sub problems
1130560	1138240	each each demon in the horde which is it could be almost viewed like a single neuron in a neural
1138240	1144640	network as achieving working towards a different task trying to predict a different thing or maybe
1144640	1151840	trying to attain a different thing it's the view of the of the mind as decentralized there is one
1151840	1157520	goal and everything is ultimately driven towards one goal but still it's a useful structure to
1157840	1165920	to have different parts driving towards towards other goals how did you get together with john
1165920	1174000	cormack was that primarily because you need the funding and it gives you a vehicle to raise
1174000	1180560	capital oh seriously I mean you you know Jan Lacoon's got meta behind him well it's just not it's
1180560	1187200	not really comparable uh john's uh john's company is great but it's still like a 20 million dollar
1187200	1195840	company and uh which is which is plenty of money for what we want to do now um john and I got together
1195840	1202800	because we had similar ideas about what was needed um and and also what was not needed
1203520	1213200	um to get to ai or agi um yeah so I read an art newspaper article an interview that john did
1214320	1217920	down in texas and uh I just could see that he was thinking about the way
1218640	1221440	thinking about things the way I was even though our backgrounds were quite different
1222000	1227280	you thought of intelligence you had to there's a few principles that needed to be worked out
1227280	1232320	rather than so this isn't a huge program to write it's a few principles we have to figure those out
1233040	1239840	um not that many maybe uh maybe 10 000 lines instead of 10 million lines of code
1240720	1248240	so it's easy to get it's relatively it's still it's still it's still hard to get basic research
1248240	1254400	funding in the world it's easy to get funding towards uh applications of ai large language models
1254400	1261760	particularly um anyway I'm really enjoying working at keen and being able to focus on the ideas
1262560	1271200	and uh it's a it's a it's a it's a calm company we um um
1272960	1277760	there's a lot of thinking involved a lot of contemplation a lot there is also experiments
1277760	1283200	and we're trying to get um the engineering side of it is really important uh but for me it's
1283200	1288720	been really great just to be able to regroup my thoughts and think about them very carefully and
1289280	1295920	push them forward but keen is is implementing the alberta plan is that right I mean that's
1295920	1301520	that's uh the project well the alberta plan is a research plan it's like a five-year research plan
1302080	1308880	and so research is something you don't implement research is something you conduct and and it
1308880	1316480	doesn't always end up the way you want but um yeah I wouldn't say implement is the right word
1316480	1323360	not yet but but the the work you're doing at keen is is informed by the alberta
1324080	1330240	yeah I'm absolutely I'm working on the alberta plan uh uh and and the end goal at keen is to
1330240	1339600	create the uh the embodied intelligence described by the alberta plan you don't sound very yeah
1339600	1345440	very confident well a plan is just a plan and you know I think there's a good chance that it
1345440	1351680	will work out as planned but you know a five-year plan you make another one after four or three years
1353760	1361520	yeah so I wouldn't I wouldn't uh presume to to know how it's gonna work out but at the same time
1361520	1368800	we have to make you know we have to make our bets we have to think hard about it um just knowing um
1369760	1376800	you know we we may well be right but you know you your work is primarily in reinforcement
1376800	1382400	learning you're you wrote the book on reinforcement learning temporal difference learning and
1383920	1392640	uh lambda and all of that is is this I mean this is this seems a much more ambitious
1393600	1403760	uh project is this was it the the success of the transformer scaling that that said well you know
1404560	1411280	let's do that with rl let's why why are these guys uh uh you know everyone's celebrating what
1411280	1416960	they're doing but but there's much more to be done no no what what you're seeing the alberta plan
1416960	1423120	is is perhaps bigger than the book but this has always been the plan we've always in AI
1423120	1429520	tried to understand all of the mind and reproduce it in computers and so that's an that's that is a
1429520	1438240	big enormous ambition that's what it's always been so the large language models are a bit
1440080	1444160	a bit disappointing in some sense I mean it's really good that people are getting excited
1444240	1452080	and people are wanting to learn about it but um but it's not it's I don't envision that it's the
1452720	1462240	direction um that will be most uh productive to pursue now you know who knows what I do know
1462240	1467920	is it's not the most direction that's useful for me to pursue um I I'm much more interested in
1467920	1473360	actions and goals and how an agent can tell what's true and what's not true all of those
1473360	1482480	things are missing from large language models so uh um no I'm not they're not really what what are
1482480	1488720	they what they are doing that's important is they're showing uh what you can do with uh computation
1488720	1497520	and and networks um and learning that you can get enormously complex um things and you can
1497600	1503440	incorporate a lot of data it just shows the power for those who needed to be shown that
1505360	1515280	and and it could be uh an interface between humans and and whatever you end up creating
1515280	1523840	the agents you end up creating you still need a language interface to communicate yeah but I don't
1524800	1529840	I'm I doubt that what we're doing with large language models today will contribute to that
1529840	1535520	oh is that right yeah I mean in other words the models that you want to build the agents you want
1535520	1542400	to build would learn language uh as as part of the learning process yeah so it's like we say
1543120	1548560	language language last you know language not not language first with large language models
1548560	1554320	are language first we just say large language last just as Jan McCoon says we need to do you
1554320	1559280	know rat level intelligence and then cat level intelligence and we have to get those figured
1559280	1565440	out before we should try to make human level intelligence uh so where are you on the plan I
1565440	1574480	mean you you figured out reinforcement learning you can build agents uh you there are various
1574480	1582240	architectures for creating representations from from various kinds of sensory input
1584160	1592800	and and at that representation level then you can plan efficiently so where in all of that
1593920	1598720	are are you in your research well it's a little hard to explain non-technically
1598800	1605360	but you can say some things certainly you can say that the various steps
1606960	1612480	are not done entirely sequentially you you're always looking for areas of opportunity where
1612480	1618400	you can make an increment of progress and those could be you know on step 10 or they could be on
1618400	1625280	step three but you also I could also try to be very rough and say that we're we're at about step
1625360	1632480	four now we are still doing things where we're changing the basic underlying
1633200	1640640	fundamental reinforcement learning algorithms we are not done with that we need more efficient
1640640	1648000	algorithms and I'm excited about some of the changes new ideas we're developing recently
1648000	1653760	about how that can be done can you talk about those new ideas at all okay well one of the big
1653760	1660960	things is efficient off-policy learning and the use of important sampling important sampling is
1660960	1665760	where you see how likely you're to do things under your target and your behavior policies
1665760	1673920	and you adjust the returns based on those the ratios of those two and for a long time I thought
1673920	1681760	that was the only way to adjust the returns but now the forward correction of the returns I think
1681760	1692800	can be done by by changing your expectations so like if you're expecting a good thing to happen
1692800	1698320	you're expecting a good action to be taken and then a different action was taken a more exploratory
1698320	1706320	action so this is a deviation from your target policy which would be more greedy and one way to
1706320	1710800	take into account the deviation from the target policy is to just say oh okay now I've done something
1711440	1715360	not best so I'm just going to adjust my level now you're going to expect a little a little less
1716080	1718960	and there's a way there's a systematic way of doing that
1721280	1727120	that's gives us a new way to handle the off-policiness of of our returns
1727840	1733520	and so this gives a whole new family of algorithms so that's exciting now
1734320	1742240	exciting maybe mostly for me I think maybe the most accessible direction of of of excitement
1742800	1748080	of novelty is in continual right so there's I'm going to say a bunch of things and to me
1748080	1753360	they're all going to have the same solution continual learning meta learning representation
1753360	1759760	learning learning to learn learning how to generalize state how to construct a state
1759760	1767040	representation feature finding that whole thing is is is coming and it will be a kind of
1768400	1773600	it's just a new kind of a way a new kind of way of doing the learning in deep networks
1775680	1782080	and I call it dynamic learning nets see a dynamic learning nets have learning at three levels
1782080	1787360	whereas usually our neural networks only learn at one level they learn the level of the weights
1788000	1793120	and in addition we also want to learn at the level of step sizes so all of every place you
1793120	1798000	have a weight in your network you're also going to have a step size so a step size is sometimes
1798000	1801440	called a learning rate it's much better to call the step size because the learning rate will be
1801440	1807600	influenced by many other things so if we imagine a whole network all these weights next to each
1807600	1814240	weight is a step size that is adjusted by an adaptive process that's adapted in a meta learning way
1814240	1820000	a meta gradient way towards making the system learn better rather than just perform better
1820000	1825760	an instantaneous moment in time learning rates or step sizes don't affect the function they don't
1825760	1829120	affect some function implemented in a particular point in time they don't affect with the network
1829120	1834800	does they affect what the network learns and so if you can tune the step sizes you also get
1834800	1841840	learning to learn and learning to generalize well and things like that the last three the last
1841840	1848800	element that we wanted to have be adaptive weights step sizes the third one is the connection pattern
1849600	1856240	so who's connected to who and so this will be done by an accretive process
1858480	1864160	like let's say you start with a linear unit and it learns say a value function or a policy
1864160	1870160	and it does the best it can with the features available and and then it needs to induce the
1870160	1875280	creation of new features because you need to learn a nonlinear function of your original
1875280	1882480	signals and so you need to create new features that have become available to that linear unit
1882480	1888720	and in this way you grow in a sort of organic way a system that can learn nonlinear functions
1890960	1898400	and so this is just a different way of ending up with a deep network that was all learned
1898400	1904560	including all the features dynamic learning that's where is the data the input data coming from
1904560	1909680	well the the input data and reinforcement just comes from life from doing things seeing things
1909680	1914080	right there is no labeled data set yeah maybe I should have said this from the very beginning
1914080	1921360	the whole idea of I call it experiential AI is that you know what makes you data you're you you
1921360	1927920	grow up as a baby and you play with things and you see things and you do things and that's the
1927920	1933680	data and the trick of reinforcement learning is how do you turn that kind of data into something
1933680	1939280	you can learn from and grow a mind up from so the the beauty and the limitation of supervised learning
1939280	1944400	is they say well let's not worry about that for now let's assume that somehow we have a data set
1944400	1949920	with labeled things and let's let's work on this sub problem that's a great idea work on a sub problem
1949920	1955760	figure it out and then move on to the next thing but really we have to move on to the next thing
1955760	1961360	we have to worry about how the the data set quote data set is automatically created from
1961360	1967120	the the training information there isn't ever a data set data set is is is such a misleading term
1967120	1972080	it suggests that it's easy to to have this thing and store this thing and curate this thing
1972080	1978000	really life is full of you do things things happen and then there's one you know everything is
1978000	1984880	fleeting you you don't have a record of it and it would be enormously complex and not only valuable
1984880	1990880	to have a record of it the the the feeling is totally different in reinforcement learning and
1990880	1996880	supervised learning and in particularly the way the way I would adjust it you know many people
1997680	2003120	do reinforcement learning by creating a buffer or a record of all the experiences that have been
2003120	2010880	been retained that have been occurred at least for some period of time and I think that's that's
2011520	2017840	uh an appealing but but it's it's not where the action the answer is the answer is
2019040	2026000	embracing the fleeting nature of data and and making most when it happens and then letting it go
2026560	2032560	well that's why you want to make an embodied system so that you have all the the five senses or
2033360	2039600	or more so you need you need as you say an embodied system an interactive system that that
2039920	2047360	influences its its input stream its sensory stream and that you get that interaction and for a long
2047360	2054960	creative time you can do this in simulation or you can do it in robotics there's still I still
2054960	2060320	know what's the best way or if the best ways do both and right or maybe first one and then the other
2061120	2070800	John is interested in uh having um uh learning from video and he likes his his his view of the
2070800	2076880	experience is you have massive numbers of video streams like you're viewing you know 500 channels
2076880	2084400	of television and then you can switch switch to look at one look at another one um uh other people
2084400	2092320	in in in keen my close colleague Joseph Modial he's uh interested in robotics and he thinks the
2092320	2098800	best way to get an appropriate in data stream is to actually build robotic hardware um
2100800	2105520	you know it's important that the world be large and complex because the worlds we want to address
2105520	2112640	are large and complex um and so you want things like video and you want large data streams um
2114400	2121760	now you can use simulations to generate even video streams simulated video but inevitably
2121760	2127680	those simulated worlds are really quite simple they have an underlying simplicity uh they have
2127680	2133920	objects perhaps and three-dimensional straight structure maybe they're rigid objects and the
2133920	2140240	vision is is is a very particular geometric form um they they are generated and they are
2140240	2144880	they are made up worlds and they are generated so they're they're really the worlds are are are
2144880	2150400	less complex than the agent uh their goal would be to have to spend most of the computer power
2150400	2156880	working on the mind and just a little bit to just create the simulated data and and that's that's
2156880	2166000	reversed the way it really is right every person is maybe has a has a complex brain but their world
2166000	2171840	is much more complex not just because the world consists of all these um physics and matter
2171840	2176400	but it also consists of other minds other brains and other minds out there and and what goes on
2176400	2180880	in their minds matters and so the world is inherently vastly more complex than the agent
2182160	2186640	and we we've reversed that when we work on simulated worlds so which is always concerning
2187600	2192880	anyway those are some of the issues in the trade-offs between working with simulations or with
2193600	2198480	physical worlds nonetheless you you need to develop the architecture and the algorithms
2199440	2206080	before you worry about the data data stream i would think yeah but you want to develop the
2206080	2211360	right algorithms and if you're working with the world it's not representative of of your target
2211360	2218880	world in an important way um it can be misleading but you're right and that's what we that's what
2218880	2223120	we strive to do you know i don't know if you know but i think of my own work is almost always
2223680	2228480	i want to focus on some issues so i make a really simple instance of that issue like you know a
2228480	2236320	five-state world and and i study the the hell out of it but i don't like try to take advantage of
2236320	2242320	its smallness you know i study algorithms that are in some sense even simpler than the simple world
2242320	2247520	and i i stress those algorithms and see what their abilities are so we always you know it's always
2247520	2253520	part of research is we we simplify the world understand it fully just like a a physicist might
2253520	2258240	you know make a simplified world with with a ball rolling down a ramp and it's it's a really
2258240	2264480	simple world and you'll try to eliminate the friction and you eliminate other weird effects
2264480	2270880	and just see things in their simplest form yeah have you um paid much attention to um
2271840	2278160	alex kendall's work at at wave ai do you know that company it's an autonomous driving company
2278160	2287440	they have a world model called gaya one um and it's it's it's similar to what yanlacoon's doing
2287440	2297520	it it you know encodes representations from from video from live video and then uh plans
2298480	2309440	uh based on those representations uh and and it can control a car uh from the representation space
2309440	2317600	it's actually pretty remarkable so let's talk about the world model and and what what kind of
2317600	2326560	world model would be appropriate for autonomous driving um so let me say some things that are
2327920	2333920	mistakes they're a natural seeming but mistakes in my opinion uh the mistake would be to make
2333920	2340480	like a physics model of the world or to try to make something that could simulate the world and
2340480	2346480	produce the video frames you don't you don't you don't want the video frames of the future
2346480	2353760	that's not the way you think um instead you think oh i could i could go to the market and
2353760	2360560	maybe there would be strawberries okay you're not creating a visual uh a video you're saying
2360560	2367120	you're like jumping to the market and then your strawberries could be you know different sizes
2367120	2374080	and positions and and and still uh there's not a video there's an idea that will happen if you go
2374080	2386160	to the market um so uh people have realized this like yon lakun used to talk about um generating
2386160	2393840	video of the future and then you realize it would be blurry and and now he realizes that you need
2393840	2399360	to produce outcomes of your model that are not like not at all like video streams and not like
2399360	2407680	observations at all they're like um they're like constructed states um that are the outcome of the
2407680	2415520	action okay so this is this is a very different from from a partial differential equation model
2415520	2422400	of the world and it's so it's very different from what self-driving car companies start with
2422400	2428000	self-driving car companies start with physics and geometry and uh you know things that are
2428000	2433440	calibrated by human understanding engineers understanding of the world and driving but
2434480	2439840	i suspect that's going to be i mean what do i know i'm not into self-driving i don't do
2439840	2448240	self-driving cars but i know that that um like tesla is and elon musk is and um so their goal
2448240	2453120	is to is to make some you know they they started like everyone else with engineering models but i
2453760	2459520	my understanding now is that they're building uh sort of more conceptual models um that are
2459520	2465600	based on the artificial neural networks okay and so rather than starting with geometry and
2465600	2470480	understood things they're just getting massive amounts of data and training it to make a model
2470480	2476080	we need a model that is at the level of high level consequences not at the level of low level
2476080	2481120	things like pixels and video so one way you do that is you're having state features that are
2481120	2487440	at a more advanced level you say oh this is a car uh rather than this is a uh a video frame
2489280	2498480	and um so and then basically it's as simple as you need abstraction in both state and time
2499120	2504880	abstraction in in state is like saying there will be strawberries when i get to the market
2504880	2511040	and abstraction in in time is saying oh i can go to the market and then in 20 minutes i will
2511040	2518640	be there probably and other things will be the same or related in natural ways
2521440	2525360	so we want to be able to think about i could go to the market you also want to think oh i could
2525360	2530240	pick up the coke can i could move a finger and that will have certain consequences these these
2530240	2534960	all these things that we know you think uh are vastly different scales going to the market is like
2534960	2542720	20 minutes um you know taking taking a new job you know might be a year uh deciding to study a
2542720	2548800	topic also might be a period of time we think and we analyze the consequences like you wanted to
2548800	2555360	meet with me today and you know we arranged it we set it up it was your your planning uh took
2555360	2562160	you know place over weeks and some cases months and and and we assembled the the the event of
2562160	2568640	this interview by by planning all that and exchanging mess high-level messages uh it all that
2568640	2573680	you know it's silly to think that that's done at the level of of of imagining videos that we might
2573680	2580640	see with our eyes or our audio is signals that we might hear yeah so we need models that are
2580640	2589200	abstract in time and state and um as a reinforcement learning person um there's a particular set of
2589200	2596640	technologies that i naturally turn towards to do that um the prediction is based on multi-step
2596640	2605520	prediction by temporal difference learning um the planning is done by uh dynamic programming
2605520	2612480	essentially value iteration but where the steps the are not low-level actions but they're called
2612480	2616960	options they're high-level ways of behaving with that that terminate so they're there are things
2616960	2622960	like going to the market and they'll terminate when you're at the market so you know at a certain
2622960	2632400	conceptual level it's clear where we want to go to me um with abstract models in time and state
2633200	2634640	built options and features
2638240	2645920	i don't know you we did write one paper recently put published an AI journal on the the notion of
2645920	2656000	planning using uh sub-problems on the stomp progression stomp means sub-task option model
2656000	2661200	and planning put all those things together and you can do the full progression from from the
2661200	2668560	data stream to abstract planning and that's that's what we're trying to put together yeah yeah and i
2668560	2675680	i sort of misspoke talking about gaya one about that model i mean it they they the input is video
2677200	2685600	it creates a representation and it plans and and and takes action in the representation
2686640	2694480	plans actions in the representation space you can then decode that into video to see what
2695200	2702480	what it's doing but but it's but you're not planning in the video space so the what what's
2702480	2709440	your ambition with with this you'll figure out the refine the algorithms the reinforcement learning
2709440	2719360	algorithms they need to be scalable once you have that uh then you move on and and uh start start
2719360	2727680	scaling them with compute and and uh you know following your roadmap or am i simplifying it too
2727680	2732720	much you know we want to understand how the mind works and then we're going to make a mind or some
2732720	2742400	minds or some mind uh amount of mind uh and this will be useful in all the ways in all sorts of
2742400	2750960	ways economically useful it'll also be useful um to to us to extend the capabilities of our own
2750960	2758560	minds if we can understand how our minds work um we can we can augment them so that they can work
2758560	2766880	better um yeah we're gonna the the key step is understanding and then there would be millions
2766880	2776240	of uses um i don't think it's going to be as simple as making uh workers sort of like slaves for us
2776960	2783760	to direct i don't think it'll be as simple as that um that maybe gives a lower bound on
2783760	2791440	potential utility our sort of our story for etkin is we say that um well if you suppose you
2791440	2798080	could make a virtual worker um this would be enormously useful um much of the work that we
2798080	2803680	all do from day to day is doesn't require a physical presence it doesn't require a robot
2803680	2810480	much of which we do is just shuffling information around we can do most things through through a
2810480	2819840	video interface um so why can't we make workers that are extremely useful by playing the roles that
2820080	2825440	people play in many cases that's that's that's sort of a lower bound and what can be done
2825440	2828800	i think much more can be done and there'll be much more interesting things to be done
2832000	2838960	and then this question of what should be done um yeah those are those are rich
2838960	2845760	philosophical questions and practical questions for the economy yeah uh the the i've seen your
2845760	2850800	third uh well and one thing on reinforcement learning and sort of supervised learning sort
2850800	2858240	of took over for a while now it's transformer based generative uh ai but uh during the supervised
2858240	2867920	learning phase uh the argument was that uh higher knowledge is all supervised learning
2867920	2874880	and and the it's still supervised it's still supervised in general the ai had large language
2874880	2882000	models they the the training information is the next token the next word and that's taken as
2882960	2890880	as the correct action the analogy you gave me was uh you know because the analogy that that's
2890880	2897840	always given is that you know a child sees an elephant the mother says that's an elephant
2898400	2905440	and the child very quickly can generalize and and recognize other elements elephants maybe it
2905440	2912000	makes a mistake and the mother corrects it and says no that's a cow and and that was always given
2912000	2916800	as an example of supervised learning but maybe it's reinforcement learning maybe it's the child's
2917520	2926000	reward from the mother praising him for remembering the label the point is that a child has
2928400	2936480	well-developed concepts classes concepts um before and then and then when it's you know when
2936480	2943040	its mother says that is an elephant uh there's already an extensive understanding on the child's
2943680	2951520	and a part of you know what the space is what the objects are and and this this the the thing
2952080	2959520	that that is being labeled um no the label is the least interesting part of that and the the
2959520	2966400	the child has already learned all the all all the other most interesting parts of of what it means
2966960	2971680	to have animals and moving things and objects in its world the label is the least interesting
2971680	2977760	part well first of all you're talking about agents that that could be virtual workers already
2978320	2988160	uh using reinforcement learning people are building agents and using large language models
2988160	3000720	and knowledge bases to you know carry out tasks knowledge based tasks uh so what you're talking
3000720	3008400	about is is more than uh linguistic tasks or knowledge based tasks you're talking about
3009200	3018080	of physical planning and physical tasks is that right the key thing is having goals and a lot
3019280	3025200	if you have an for example an assistant help you plan your day organize your day or do tasks for
3025200	3033280	you um i'm thinking it's very important that the system is able to have goals and is able to
3033280	3040240	understand your goals i think it's probably the most important part of an assistant is to understand
3040240	3047680	the purposes involved and um large language models don't understand don't really understand
3047680	3055920	purposes involved they will appear to a little bit um but the corner case has always come up and
3055920	3062240	once you spend a bit of time they're always and you're always in a corner case and so an AI system
3062240	3068640	is system that that after a bit does silly things and that don't respect the goals that you have or
3068640	3074560	that have been given to it um that's not going to be a useful assistant so i mean i don't want to be
3074560	3081200	critical of large language models um they're very very useful but it shouldn't be viewed as a criticism
3081200	3086560	to say that they're also at the same time have rather important limitations it's not a competition
3086560	3096000	in that sense are you concerned at all are you ascribed to the threat debate no i think the
3096560	3104400	i don't i don't uh i i think the doomers are they're not just wrong i think i think they're
3104400	3111280	blindingly biased the the bias is blinding them to what's going on basically AI is a broadly
3111280	3116880	applicable technology it's not like it's not like nuclear weapons it's not like it's not like a
3116880	3124640	bio weapons it can be used for all kinds of things and it's not it's not uh it's uh the way we deal
3124640	3132000	with such things is we we uh we try to use them well and there will be people that use them
3132720	3137280	for bad things and then you know this is just normal there's normal technology
3137280	3143120	is it can be used by good people or bad people the the doomers the doomers are just saying oh
3143120	3149760	somehow there's going to be it's going to be it's bad in the same way that nuclear weapons are bad
3149760	3157280	that they and that's just they're just blinded by that metaphor by the thinking that that the AI
3157280	3164880	will be out to kill them that's just it's just silly and i i don't i don't think well they the
3164880	3172560	doomers don't actually give coherent reasons for what they what they believe and so it's hard to
3172560	3179600	argue with them uh so maybe it's fair just to hold that they're they're biased and blind
3179600	3185600	i don't accept i don't accept an argument this is a proper argument so so where you say you're
3185600	3196960	maybe at stage four in the research car max says 2030 uh that's you know it's far enough out there
3196960	3205680	that maybe people won't remember in 2030 that he said 2030 uh it's always uh you know i've 2030
3205680	3212720	has been out there for a long time and it's it's it's uh you can't it doesn't recede it's always
3212720	3224240	been 2030 for the uh computer power reaching human scale um quantities yeah but anyway 2030 is is a
3224240	3230160	reasonable it's a reasonable target for us understanding everything that we need in order
3230160	3236720	to make uh a real mind yeah i'm good with that yeah as you you have to be ambitious
3238560	3249200	i've always said that 2030 is a 25 chance of of of achieving a real intelligence a real human
3249200	3255600	level intelligence 25 chance so probably not but it's it's a big enough chunk of probability that
3256160	3260800	that an ambitious person should work towards it and try to make it true and it does depend upon
3260800	3267760	what we do and not just the uh unfolding of the universe so we should we should try to do that
3267760	3274080	that that is a big the big thing that's happening right now is the the the public is coming to grips
3274080	3279120	with what it means for there to be for us to understand the mind and to have the ability to
3279120	3287680	create uh minded things uh and so that that is a big uh transformation it's a big change in our
3287680	3297280	worldview um and so we absolutely need all kinds of people to uh to help us help us become easy
3297280	3304880	and become have an understanding of what's happening as we uh achieve human level
3305920	3311760	designed intelligence that's it for this week's episode i want to thank richard for his time
3311760	3318080	if you want to read a transcript of today's conversation you can find one on our website
3318160	3326320	i on ai that's e y e hyphen o n dot ai in the meantime remember the singularity may be getting
3326320	3334880	closer but ai is already changing your world so pay attention
