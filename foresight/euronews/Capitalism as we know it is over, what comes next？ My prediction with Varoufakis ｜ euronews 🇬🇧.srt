1
00:00:00,000 --> 00:00:01,000
Are we good?

2
00:00:01,000 --> 00:00:03,000
One, two, three.

3
00:00:03,000 --> 00:00:04,000
Welcome to the show.

4
00:00:04,000 --> 00:00:05,000
Giannis.

5
00:00:05,000 --> 00:00:06,000
Thank you, Tom.

6
00:00:06,000 --> 00:00:07,000
Very good to be here.

7
00:00:07,000 --> 00:00:08,000
Let's start there, shall we?

8
00:00:08,000 --> 00:00:11,000
What is your wildest prediction for the future?

9
00:00:11,000 --> 00:00:14,000
It's not a prediction, it is a diagnosis.

10
00:00:14,000 --> 00:00:23,000
My name is Tom Goodwin and this is my wildest prediction.

11
00:00:23,000 --> 00:00:33,000
His name is shot to fame in 2015 when he was anointed minister of finance in Greece at the height of the European debt crisis.

12
00:00:33,000 --> 00:00:41,000
This is Giannis Varoufakis, an author, academic, perhaps activist, a politician,

13
00:00:41,000 --> 00:00:47,000
and we're here to discuss the future of capitalism and how technology is changing the world.

14
00:00:47,000 --> 00:00:53,000
I want to talk about his latest book, techno feudalism, the end of capitalism.

15
00:00:53,000 --> 00:00:56,000
This book is not about what will happen in the future.

16
00:00:56,000 --> 00:01:03,000
It is a highly controversial notion of what has already gone down.

17
00:01:03,000 --> 00:01:14,000
I'm not writing about what AI would do to the labour market, what will happen to us with big brother and surveillance and any of that.

18
00:01:15,000 --> 00:01:25,000
It is my estimation, and this is a controversial hypothesis, that capitalism has already entered, which is very strange.

19
00:01:25,000 --> 00:01:26,000
Yes.

20
00:01:26,000 --> 00:01:29,000
I don't remember this feeling like something that happened.

21
00:01:29,000 --> 00:01:32,000
Tom, the way I see it is this.

22
00:01:32,000 --> 00:01:39,000
Suppose this was 1776 and we were in London and we were having a discussion about the state of the world.

23
00:01:39,000 --> 00:01:43,000
Now, everywhere we looked in 1776, we would see feudalism.

24
00:01:43,000 --> 00:01:52,000
We would see feudalism in the House of Lords, in the House of Commons, in government, in every local council, around the world, on the land.

25
00:01:52,000 --> 00:01:56,000
We would see peasants, we would see aristocrats.

26
00:01:56,000 --> 00:01:59,000
And yet, we do know that, don't we?

27
00:01:59,000 --> 00:02:08,000
Already, feudalism had died and was being gradually but fast being replaced by something called capitalism.

28
00:02:08,000 --> 00:02:18,000
The magnificent shift of power from the owners of land to the owners of machinery, of steamships, of electrical grids later on.

29
00:02:18,000 --> 00:02:24,000
And the shift of wealth creation from rent accumulation to profit making.

30
00:02:24,000 --> 00:02:28,000
My view is that we are already experiencing a similar transformation.

31
00:02:28,000 --> 00:02:31,000
Wherever we look, we see capital.

32
00:02:31,000 --> 00:02:32,000
We see markets.

33
00:02:32,000 --> 00:02:35,000
We see capitalists doing extremely well.

34
00:02:35,000 --> 00:02:45,000
And yet, I think that already we have undergone a transformation to something like feudalism, but a very technologically advanced version of it.

35
00:02:45,000 --> 00:02:48,000
Markets have been replaced by platforms.

36
00:02:48,000 --> 00:02:51,000
So Amazon.com is not a market.

37
00:02:51,000 --> 00:03:02,000
It looks like a market, but it's more like a digital fiefdom, a cloud fiefdom, belonging to one man whose accumulation of wealth is based not on profit,

38
00:03:02,000 --> 00:03:04,000
but on a form of rent.

39
00:03:04,000 --> 00:03:10,000
Every time you buy something on Amazon, 30-40% of the price goes to Mr Bezos, not to the maker.

40
00:03:10,000 --> 00:03:15,000
And how are you defining assets in this world of technofedalism?

41
00:03:15,000 --> 00:03:17,000
What is it that they are owning?

42
00:03:17,000 --> 00:03:18,000
What is it that they are renting?

43
00:03:18,000 --> 00:03:21,000
Is it our data, our attention, our relationship?

44
00:03:21,000 --> 00:03:24,000
No, all that is part of the story, but it's not the story.

45
00:03:24,000 --> 00:03:30,000
The story is that a new form of capital began to emerge about 10 years ago.

46
00:03:30,000 --> 00:03:34,000
Capital was always a produced means of production.

47
00:03:34,000 --> 00:03:43,000
So whether you have Robinson Cruser's fishing rod, a steam engine, or an industrial, a very advanced industrial robot today.

48
00:03:43,000 --> 00:03:47,000
It's a produced means of production, something we produced in order to produce other stuff.

49
00:03:47,000 --> 00:03:54,000
But this new mutation of capital, which I call cloud capital, it's what lives in your phone.

50
00:03:54,000 --> 00:03:58,000
And by cloud you mean it's sort of a ethereal asset?

51
00:03:58,000 --> 00:03:59,000
No, I'm talking about cloud capital.

52
00:03:59,000 --> 00:04:05,000
So take Alexa or Siri or Google Assistant.

53
00:04:05,000 --> 00:04:11,000
That sits there either on your phone or on your desk and you order it to do things.

54
00:04:11,000 --> 00:04:15,000
Well, yes, but that's only a tiny part of the story.

55
00:04:15,000 --> 00:04:25,000
What it is, it's an interface between you and the whole agglomeration of capital goods, including optic fiber cables that are laid on the ocean floors.

56
00:04:25,000 --> 00:04:35,000
Huge, gigantic server farms that hum like a factory, like a dark satanic mill to quote Edmund Burke, sell towers.

57
00:04:35,000 --> 00:04:39,000
This is capital, it doesn't live in the cloud, but it's what we call the cloud.

58
00:04:39,000 --> 00:04:42,000
When you upload stuff, you know, photographs on the cloud, right?

59
00:04:42,000 --> 00:04:45,000
Or you save stuff on the cloud.

60
00:04:45,000 --> 00:04:52,000
So you interface with this thing, which is a kind of capital, but it's not exactly produced means of production.

61
00:04:52,000 --> 00:04:56,000
What does Amazon Alexa, what does it do?

62
00:04:56,000 --> 00:05:01,000
You are training it essentially through your commands.

63
00:05:01,000 --> 00:05:03,000
It's just speaking in the house.

64
00:05:03,000 --> 00:05:07,000
It gets from you data, but data on what?

65
00:05:07,000 --> 00:05:09,000
On you, on your preferences.

66
00:05:09,000 --> 00:05:15,000
So you are training it to learn how to give you good recommendations.

67
00:05:15,000 --> 00:05:20,000
So I don't know about you, but when Amazon recommends a book, I always want to read it because it knows me.

68
00:05:20,000 --> 00:05:28,000
When Spotify gives me a recommendation for music, invariably I like it, invariably, because it understands me really very well.

69
00:05:28,000 --> 00:05:38,000
So I'm training it to train me, to train it, to train me, to train it, to train me, so that at some point it can actually make a recommendation and then I can say, OK, I want that.

70
00:05:38,000 --> 00:05:49,000
And the fundamental thing, Tom, here is that this is not like standard advertising, where you see a poster, you buy Mercedes Benz, then you go to Mercedes Benz dealership

71
00:05:49,000 --> 00:05:58,000
and you get one. No. Alexa convinces you to buy something, exercise bike, whatever, a pair of binoculars.

72
00:05:58,000 --> 00:06:04,000
And then sells it to you, bypassing every marketplace in the world.

73
00:06:04,000 --> 00:06:07,000
Now that is a fiefdom, that is a market.

74
00:06:07,000 --> 00:06:22,000
And you see, and most income now that is accumulated is accumulating the form of rents that basis charges capitalists for access to this digital fiefdom.

75
00:06:22,000 --> 00:06:28,000
So we're going back to a system where access to the land, only this time it is digital land.

76
00:06:28,000 --> 00:06:43,000
It's what I call cloud capital, is restricted, crucial, outside the marketplace, outside capitalism, and procure a magnificent rent for the new cloud lords.

77
00:06:43,000 --> 00:06:44,000
That's a new system.

78
00:06:44,000 --> 00:06:47,000
And how are you defining these new cloud lords?

79
00:06:47,000 --> 00:06:50,000
Is it a question of the service space that they own?

80
00:06:50,000 --> 00:06:52,000
Is it how many customers they have?

81
00:06:52,000 --> 00:06:55,000
At what point does Walmart become Amazon?

82
00:06:55,000 --> 00:06:57,000
At what point does a taxi company become Amazon?

83
00:06:57,000 --> 00:07:00,000
Walmart has already become, Walmart is already a cloud fief.

84
00:07:00,000 --> 00:07:07,000
Because Walmart, very smartly, has developed its own competitor to Amazon.

85
00:07:07,000 --> 00:07:13,000
It is using its stores in order to build up its cloud fief.

86
00:07:13,000 --> 00:07:25,000
And now increasingly the profits, the net returns of Walmart come from the cloud, not from the analog buildings that it still has,

87
00:07:25,000 --> 00:07:29,000
which it uses in order to lure people effectively into the cloud.

88
00:07:29,000 --> 00:07:32,000
And you talk about this in a very sort of sinister way, almost.

89
00:07:32,000 --> 00:07:34,000
I mean, your book is quite gloomy.

90
00:07:34,000 --> 00:07:40,000
You know, some people would look at this and they would say they have decided to enter in with the relationship with Amazon.

91
00:07:40,000 --> 00:07:43,000
They have decided to upload their pictures to Instagram.

92
00:07:43,000 --> 00:07:46,000
They have decided to use WhatsApp as a way to...

93
00:07:46,000 --> 00:07:48,000
I don't think my book is at all gloomy.

94
00:07:48,000 --> 00:07:52,000
I try to write my book, you know, in a very jovial way.

95
00:07:52,000 --> 00:07:54,000
You could argue that people have...

96
00:07:54,000 --> 00:07:56,000
By the way, as a letter to my dad.

97
00:07:56,000 --> 00:07:57,000
Yes, yes.

98
00:07:57,000 --> 00:07:59,000
So I constantly have a little tiff with my dad.

99
00:07:59,000 --> 00:08:04,000
Because I try to imagine what he would have said to me and I try to answer to what I imagine he would have said to me.

100
00:08:04,000 --> 00:08:06,000
I think my book is a very pleasant read.

101
00:08:06,000 --> 00:08:11,000
It didn't feel that way to me, but I mean, I'm very optimistic about the future and what technology means.

102
00:08:11,000 --> 00:08:13,000
And I think there's something interesting...

103
00:08:13,000 --> 00:08:18,000
But that takes an incredible degree of naivety to be optimistic about the future.

104
00:08:18,000 --> 00:08:19,000
Doesn't it?

105
00:08:19,000 --> 00:08:22,000
There's no empirical evidence to support that anything good will happen.

106
00:08:22,000 --> 00:08:24,000
But where I agree with you...

107
00:08:24,000 --> 00:08:26,000
We have a lot of evidence that things have got better every year.

108
00:08:26,000 --> 00:08:28,000
Oh, no, no, no, no, we don't.

109
00:08:28,000 --> 00:08:32,000
Everything is getting far, far worse for the majority of the people on this earth, including climate catastrophe.

110
00:08:32,000 --> 00:08:33,000
Come on, Tom.

111
00:08:33,000 --> 00:08:35,000
But I'll tell you, I'll give you this.

112
00:08:35,000 --> 00:08:39,000
I'll give you this and let's see if we can converge.

113
00:08:39,000 --> 00:08:41,000
I'm hopeful.

114
00:08:41,000 --> 00:08:43,000
I'm not optimistic.

115
00:08:43,000 --> 00:08:50,000
I make it a very great distinction between optimism, which is the poor cousin of hope and hope.

116
00:08:50,000 --> 00:08:52,000
Hope we need to have.

117
00:08:52,000 --> 00:08:54,000
I love those technologies.

118
00:08:54,000 --> 00:08:56,000
Don't get me wrong.

119
00:08:56,000 --> 00:08:57,000
I'm not a Luddite.

120
00:08:57,000 --> 00:08:59,000
I mind you, Luddites are very misunderstood.

121
00:08:59,000 --> 00:09:02,000
They didn't, like, completely misunderstood.

122
00:09:02,000 --> 00:09:05,000
I love the Luddites, but you know what I mean.

123
00:09:05,000 --> 00:09:07,000
I'm not against machinery.

124
00:09:07,000 --> 00:09:12,000
I am absolutely enthusiastic, completely addicted to all those apps.

125
00:09:12,000 --> 00:09:18,000
For instance, I think the world of AI, I think AI may very well destroy us, but I love it.

126
00:09:18,000 --> 00:09:25,000
I absolutely adore the idea that there is AI today designing antibiotics that can kill superbugs

127
00:09:25,000 --> 00:09:29,000
that human minds cannot design and antibiotic against.

128
00:09:29,000 --> 00:09:30,000
That's brilliant.

129
00:09:30,000 --> 00:09:33,000
That's a triumph of the human spirit.

130
00:09:33,000 --> 00:09:44,000
But not to see that we have exponential concentrations of incomes

131
00:09:44,000 --> 00:09:52,000
in the hands of people who produce nothing except for the right and the opportunity to extract incomes from others

132
00:09:52,000 --> 00:09:57,000
while the world is going to the rocks in terms of the climate catastrophe.

133
00:09:57,000 --> 00:10:00,000
That we need to recognize if we are going to remain hopeful.

134
00:10:00,000 --> 00:10:09,000
I mean, you are a big study of history and you, in your books, talk a lot about technology,

135
00:10:09,000 --> 00:10:15,000
especially when it comes to capital, and you would look at most forms of technology

136
00:10:15,000 --> 00:10:18,000
and see them as levers to human potential.

137
00:10:18,000 --> 00:10:25,000
The loom obviously had big threatening impacts on the Luddites, hence their fight for fairness.

138
00:10:25,000 --> 00:10:30,000
But generally speaking, technology is a lever to allow us to create more wealth.

139
00:10:30,000 --> 00:10:38,000
Its distribution has not always been fair, but over a long period of time, the trend lines are fairly consistent.

140
00:10:38,000 --> 00:10:43,000
And especially since about 1910, global inequality has remained about the same.

141
00:10:43,000 --> 00:10:45,000
Since 1910.

142
00:10:45,000 --> 00:10:48,000
Since around about 1910, 1930 on a global scale.

143
00:10:48,000 --> 00:10:49,000
They remain the same.

144
00:10:49,000 --> 00:10:50,000
That is not true.

145
00:10:50,000 --> 00:10:51,000
That is not true.

146
00:10:51,000 --> 00:10:52,000
Broadly similar.

147
00:10:52,000 --> 00:10:55,000
It depends on whether you look at the top 0.1%.

148
00:10:55,000 --> 00:10:56,000
It depends on what you measure.

149
00:10:56,000 --> 00:10:57,000
I'm sorry.

150
00:10:57,000 --> 00:11:03,000
But my question is, I'm afflicted by an economic mind that refuses to see things.

151
00:11:03,000 --> 00:11:06,000
It's the 2022 World Inequality Report.

152
00:11:06,000 --> 00:11:07,000
It's roasted glasses.

153
00:11:07,000 --> 00:11:10,000
It's the 2022 World Inequality Report.

154
00:11:10,000 --> 00:11:12,000
And it's looked at things globally.

155
00:11:12,000 --> 00:11:14,000
And obviously no one lives in a global world.

156
00:11:14,000 --> 00:11:16,000
We all live in our own reality.

157
00:11:16,000 --> 00:11:21,000
To some extent, we have to wonder if it's about relative income or whether it's about absolute income.

158
00:11:21,000 --> 00:11:27,000
But at what point in the last few years, do you think we switched over to sort of techno feudalism?

159
00:11:27,000 --> 00:11:31,000
Is there a defining moment where you think we sort of reached this tipping point?

160
00:11:31,000 --> 00:11:34,000
Well, it's between 2008 and today.

161
00:11:34,000 --> 00:11:39,000
It's impossible to, it's like saying, you know, when did you become bald?

162
00:11:39,000 --> 00:11:45,000
Which hair did you lose so that you switched from being a person with hair to a bald person?

163
00:11:45,000 --> 00:11:48,000
There's no such hair that defines your transition.

164
00:11:48,000 --> 00:11:51,000
But I can tell you that it was around the night when I was in my forties.

165
00:11:51,000 --> 00:11:54,000
Similarly, the switch happened after 2008.

166
00:11:54,000 --> 00:11:57,000
And it happened because of 2008 to a very large extent.

167
00:11:57,000 --> 00:12:04,000
Because the way in which the G7 government and central banks responded to the great financial catastrophe

168
00:12:04,000 --> 00:12:11,000
by a combination of socialism for the bankers, you know, trillions pumped out of our central banks

169
00:12:11,000 --> 00:12:15,000
to go to the financial sector with huge austerity for everybody else.

170
00:12:15,000 --> 00:12:20,000
That starved, so you know, you create lots of money.

171
00:12:20,000 --> 00:12:27,000
You have liquidity that we never had in the history of the world, which never went into investment

172
00:12:27,000 --> 00:12:29,000
because of low levels of demand.

173
00:12:29,000 --> 00:12:35,000
So the companies that got this money from the central banks bought back their own shares that created asset price inflation.

174
00:12:35,000 --> 00:12:42,000
The only ones who invested were the cloud elists, the people who owned cloud capital, you know, the techno feudal lords.

175
00:12:42,000 --> 00:12:45,000
And, you know, wonderful machinery and all that.

176
00:12:45,000 --> 00:12:53,000
But that investment went into creating the cloud capital, which then replaced markets with platforms

177
00:12:53,000 --> 00:13:02,000
and shifted a very significant percentage of the circular flow of income from profits to rents.

178
00:13:02,000 --> 00:13:05,000
And that is destabilizing for the global system.

179
00:13:05,000 --> 00:13:11,000
What do you think the relationship between the rise of these tech companies and zero interest rates almost has been?

180
00:13:11,000 --> 00:13:12,000
It's what I said.

181
00:13:12,000 --> 00:13:13,000
Yeah.

182
00:13:13,000 --> 00:13:20,000
I mean, zero interest rates is what happens when you're trying to reflow the financial sector by printing huge amounts of money, right?

183
00:13:20,000 --> 00:13:24,000
I mean, the price of money is related to its supply.

184
00:13:24,000 --> 00:13:33,000
So when you boost supply, as if they do now tomorrow, then the price of money, which is related to interest, will go to zero and below zero, which is what happened.

185
00:13:33,000 --> 00:13:34,000
It's just interesting.

186
00:13:34,000 --> 00:13:42,000
When you try and find the sort of defining feature of what a tech company is versus what a tech company isn't, you can look at many things like network effects.

187
00:13:42,000 --> 00:13:44,000
You can look at the use of data.

188
00:13:44,000 --> 00:13:50,000
But one sort of interesting element, I guess, is to look at capital return and the degree to which they need to make a profit.

189
00:13:50,000 --> 00:13:51,000
That's all irrelevant.

190
00:13:51,000 --> 00:13:55,000
That is just mumbo jumbo, just trying to sound as if you're financially intelligent.

191
00:13:55,000 --> 00:13:56,000
I'm sorry.

192
00:13:56,000 --> 00:13:58,000
I'm intelligent about technology.

193
00:13:58,000 --> 00:13:59,000
Yeah, I'm sure.

194
00:13:59,000 --> 00:14:00,000
I'm sure.

195
00:14:00,000 --> 00:14:05,000
But from a socioeconomic point of view, what really matters is none of that.

196
00:14:05,000 --> 00:14:07,000
I don't care how you define a tech company.

197
00:14:07,000 --> 00:14:12,000
You can have a tech company that makes fantastic industrial robots.

198
00:14:12,000 --> 00:14:14,000
That's not cloud capital.

199
00:14:14,000 --> 00:14:15,000
It's beautiful.

200
00:14:15,000 --> 00:14:16,000
I love it.

201
00:14:16,000 --> 00:14:19,000
I see industrial robot assemble cars and microchips.

202
00:14:19,000 --> 00:14:20,000
Beautiful.

203
00:14:20,000 --> 00:14:22,000
It's like poetry in motion.

204
00:14:22,000 --> 00:14:23,000
A little bit of a body.

205
00:14:23,000 --> 00:14:24,000
Now, that's a tech company.

206
00:14:24,000 --> 00:14:25,000
Yeah.

207
00:14:25,000 --> 00:14:26,000
It's not what I'm talking about.

208
00:14:26,000 --> 00:14:37,000
I'm talking about companies that are investing in the creation of my definition of cloud capital, which is a produced means of behavioral modification.

209
00:14:37,000 --> 00:14:40,000
The difference between an industrial robot.

210
00:14:40,000 --> 00:14:43,000
Fantastic, technologically snazzy and so on.

211
00:14:43,000 --> 00:14:54,000
And Amazon, or for that matter, Facebook, is that the latter is not a produced means of production.

212
00:14:54,000 --> 00:14:58,000
It is a produced means of behavioral modification.

213
00:14:58,000 --> 00:15:09,000
So that cloud capital gives the owner an immense exorbitant power and privilege to alter people's behavior.

214
00:15:09,000 --> 00:15:24,000
In order to create alternatives to markets in which we are all caught up as buyers and sellers, but not within a market in which you can choose a partner and choose.

215
00:15:24,000 --> 00:15:26,000
The algorithms does the choice for us.

216
00:15:26,000 --> 00:15:31,000
And the algorithm chooses in a manner that maximizes cloud rents of the owners of that cloud capital.

217
00:15:31,000 --> 00:15:46,000
And you're sort of concerned that this becomes somewhat monopolistic because of the information they have on us making it hard to leave that ecosystem or because of their market share or because that's impossible for other people to enter the market.

218
00:15:46,000 --> 00:15:48,000
Or this isn't a sort of monopoly concern.

219
00:15:48,000 --> 00:15:52,000
Well, you see, I avoid the word monopoly.

220
00:15:52,000 --> 00:15:55,000
And I avoid it because a monopoly is a market.

221
00:15:55,000 --> 00:15:57,000
It's a monopolized market.

222
00:15:57,000 --> 00:16:05,000
My point about Alibaba, so I was not to talk only about, you know, Amazon, is that it is not a market.

223
00:16:05,000 --> 00:16:08,000
You see, and I tried to explain this in the book.

224
00:16:08,000 --> 00:16:20,000
Imagine you and I are entering a town in the United States of America, you know, back in the 19th century, let's make it a bit, you know, of a western movie, right?

225
00:16:20,000 --> 00:16:25,000
And we discovered that every shop in the town belongs to one man.

226
00:16:25,000 --> 00:16:27,000
You've seen westerns like that, right?

227
00:16:27,000 --> 00:16:28,000
And there's a showdown.

228
00:16:28,000 --> 00:16:31,000
It would be more that one person owns the land.

229
00:16:31,000 --> 00:16:32,000
No, no, no.

230
00:16:32,000 --> 00:16:42,000
Suppose that this person owns the bar, the saloon, the, you know, the shops, the hotel, everything, everything, the post office, the sheriff.

231
00:16:42,000 --> 00:16:43,000
Okay.

232
00:16:43,000 --> 00:16:45,000
You've seen these movies with John Wayne and so on.

233
00:16:45,000 --> 00:16:46,000
Right.

234
00:16:46,000 --> 00:16:48,000
Now that's a monopolized market.

235
00:16:48,000 --> 00:16:49,000
A monopolized town.

236
00:16:49,000 --> 00:16:51,000
You and I walked down the store.

237
00:16:51,000 --> 00:16:53,000
We know it belongs to that one person who owns everything.

238
00:16:53,000 --> 00:16:56,000
He has immense monopoly power over everyone in that town.

239
00:16:56,000 --> 00:17:00,000
But in Alibaba, for example, 100% their revenues from third parties.

240
00:17:00,000 --> 00:17:02,000
Wait, wait, wait, wait, wait, wait, wait.

241
00:17:02,000 --> 00:17:04,000
In Amazon, 45% of their revenues.

242
00:17:04,000 --> 00:17:06,000
You're missing the point of my allegory here.

243
00:17:06,000 --> 00:17:08,000
I have a western movie allegory.

244
00:17:08,000 --> 00:17:09,000
Let's not lose it.

245
00:17:09,000 --> 00:17:10,000
Okay.

246
00:17:10,000 --> 00:17:13,000
The point I'm trying to make is this is a monopolized market, right?

247
00:17:13,000 --> 00:17:22,000
But Alibaba and Amazon are not because in that town, in the western movie, you and I, Tom, we're walking down the street, right?

248
00:17:22,000 --> 00:17:24,000
And we look at the shop window.

249
00:17:24,000 --> 00:17:26,000
You and I see the same thing.

250
00:17:26,000 --> 00:17:28,000
We may not buy it.

251
00:17:28,000 --> 00:17:33,000
We may say, you know, let's not give our money to this terrible man who owns everything, right?

252
00:17:33,000 --> 00:17:34,000
But we converse.

253
00:17:34,000 --> 00:17:35,000
We see the same thing.

254
00:17:35,000 --> 00:17:44,000
If you and I had our laptops here and we went to Alibaba or Amazon and we typed extravagant binoculars.

255
00:17:44,000 --> 00:17:47,000
You see different things to what I'm going to see.

256
00:17:47,000 --> 00:17:53,000
The algorithm knows you, knows me and calibrates what we see and it does not select the same thing.

257
00:17:53,000 --> 00:17:55,000
So we don't even see the same things.

258
00:17:55,000 --> 00:17:56,000
That's not a marketplace.

259
00:17:56,000 --> 00:17:57,000
It's not a monopolized market.

260
00:17:57,000 --> 00:17:59,000
It is not a market.

261
00:17:59,000 --> 00:18:15,000
It is an algorithm like a Soviet economic system which decides who does what with whom, without any consultation, without any way that you and I can communicate as buyers or you as a seller and me as a buyer.

262
00:18:15,000 --> 00:18:20,000
No way that we can communicate unless the algorithm chooses for us to communicate.

263
00:18:20,000 --> 00:18:23,000
I mean, some people would call that personalization.

264
00:18:23,000 --> 00:18:24,000
That's all rubbish.

265
00:18:24,000 --> 00:18:26,000
It is not a market.

266
00:18:26,000 --> 00:18:27,000
You can call it whatever.

267
00:18:27,000 --> 00:18:29,000
You can call it Snoopy Do.

268
00:18:29,000 --> 00:18:31,000
It is not the point I'm making.

269
00:18:31,000 --> 00:18:33,000
It's not the market, right?

270
00:18:33,000 --> 00:18:42,000
And it is an algorithm which matches the people who are selling with the people who are buying in the interest, not of the seller even.

271
00:18:42,000 --> 00:18:44,000
That could be monopoly.

272
00:18:44,000 --> 00:18:50,000
But of the rentier or the landlord of that cloud capital.

273
00:18:50,000 --> 00:18:52,000
That's the point I'm making.

274
00:18:52,000 --> 00:18:54,000
What do you think is the solution to all this?

275
00:18:54,000 --> 00:18:56,000
How are things going to progress from here?

276
00:18:56,000 --> 00:18:59,000
Can technology be the fix as well as the problem?

277
00:18:59,000 --> 00:19:01,000
Technology has never been the fix.

278
00:19:01,000 --> 00:19:05,000
To the problems we create with the technology, the problem is political.

279
00:19:05,000 --> 00:19:06,000
It's social.

280
00:19:06,000 --> 00:19:16,000
So, you know, steam engines were not responsible for the awful conditions of the working class in Manchester when the first dark satanic meals were put together, right?

281
00:19:16,000 --> 00:19:18,000
And the solution was not technology.

282
00:19:18,000 --> 00:19:24,000
The solution was, you know, social, political interventions.

283
00:19:24,000 --> 00:19:26,000
That will always be the case.

284
00:19:26,000 --> 00:19:30,000
So, I don't blame the technology and therefore I do not expect the technology to solve the problem.

285
00:19:30,000 --> 00:19:32,000
The question is who owns what?

286
00:19:32,000 --> 00:19:38,000
You see, some people are very worried about surveillance.

287
00:19:38,000 --> 00:19:41,000
That, you know, these companies know so much about us.

288
00:19:41,000 --> 00:19:43,000
I'm not that bothered personally.

289
00:19:43,000 --> 00:19:45,000
I mean, I understand why people are worried.

290
00:19:45,000 --> 00:19:52,000
I'm slightly worried, but I'm far more worried by what they own.

291
00:19:52,000 --> 00:20:07,000
They own this capital, which is a capacity to separate us, to fragment us as markets, as communities, as societies, to influence us in ways we don't understand.

292
00:20:07,000 --> 00:20:10,000
In ways that the people who wrote the algorithms do not understand.

293
00:20:10,000 --> 00:20:13,000
This is even more worrying, right?

294
00:20:13,000 --> 00:20:16,000
You hear that from coders, from AI and so on.

295
00:20:16,000 --> 00:20:18,000
It becomes ever more true, like AI is a good example.

296
00:20:18,000 --> 00:20:19,000
People are surprised.

297
00:20:19,000 --> 00:20:26,000
So, for me, as an old lefty, the answer must always be the socialization of the means of production.

298
00:20:26,000 --> 00:20:32,000
In some ways, technology has been a force to kind of democratize access to wealth creation.

299
00:20:32,000 --> 00:20:33,000
So, now it's...

300
00:20:33,000 --> 00:20:35,000
You live in a different universe, right?

301
00:20:35,000 --> 00:20:37,000
There are plenty of...

302
00:20:37,000 --> 00:20:38,000
Democratization.

303
00:20:38,000 --> 00:20:39,000
What democratization?

304
00:20:39,000 --> 00:20:40,000
We have exactly the opposite.

305
00:20:40,000 --> 00:20:48,000
We live in a world where three companies, BlackRock, State Street and Vanguard, own 90% of all the companies in the New York Stock Exchange.

306
00:20:48,000 --> 00:20:49,000
We're talking about democratization.

307
00:20:49,000 --> 00:20:50,000
They don't own 90%.

308
00:20:50,000 --> 00:20:55,000
They all have a majority shareholding together in 90%.

309
00:20:55,000 --> 00:20:57,000
Again, nitpicking.

310
00:20:57,000 --> 00:20:59,000
This is...

311
00:20:59,000 --> 00:21:03,000
Adam Smith is the patron saint of the factory market.

312
00:21:03,000 --> 00:21:10,000
He would be a gust hearing you talk about the democratization of capitalism.

313
00:21:10,000 --> 00:21:14,000
For him, what we now have would be a nightmare.

314
00:21:14,000 --> 00:21:21,000
But you could argue that a platform like Shopify makes it easy for people without many means to set up a store.

315
00:21:21,000 --> 00:21:28,000
You could argue that YouTube gives an opportunity for anyone in the world to make a world-class documentary and they can make money from advertising.

316
00:21:28,000 --> 00:21:37,000
You could argue that Facebook has democratized access to advertising tools that allow people to buy media at the same rate as bigger companies.

317
00:21:37,000 --> 00:21:40,000
I got my line with almost all of your thinking.

318
00:21:40,000 --> 00:21:48,000
I just think it's a little bit unfair to look at some of the dynamics that provide access to people in a more level way.

319
00:21:48,000 --> 00:22:00,000
And to some extent, the data that allows a marketplace to personalize what they offer actually works in favor in some cases of smaller companies that use these...

320
00:22:00,000 --> 00:22:02,000
That last sentence is absurd.

321
00:22:02,000 --> 00:22:04,000
Everything else you said before was fine.

322
00:22:04,000 --> 00:22:06,000
The conclusion was absolutely absurd.

323
00:22:06,000 --> 00:22:08,000
You will allow me to say, right?

324
00:22:08,000 --> 00:22:18,000
Now, listen, there is no doubt that Amazon gives every day fantastic opportunities to producers to reach customers.

325
00:22:18,000 --> 00:22:20,000
There's no doubt about that.

326
00:22:20,000 --> 00:22:22,000
Shopify does the same thing.

327
00:22:22,000 --> 00:22:30,000
Not this is ancient, but all the paraphernalia of podcasting and so on allows all of us to be broadcasters.

328
00:22:30,000 --> 00:22:33,000
That is all perfectly true.

329
00:22:33,000 --> 00:22:35,000
And it's great.

330
00:22:35,000 --> 00:22:45,000
However, the point I'm making is that the techno feudal forces at work, which are based on the money in which cloud capital operates,

331
00:22:45,000 --> 00:22:56,000
are ensuring that all those people who create businesses and sell stuff through Amazon or Shopify and so on in the end become vassals.

332
00:22:56,000 --> 00:23:02,000
Because landlords under feudalism did allow people to actually do things.

333
00:23:02,000 --> 00:23:04,000
They gave them land, they gave them the opportunity to produce stuff.

334
00:23:04,000 --> 00:23:08,000
They were called vassals in the sense that they were complete, dependent on the landlord,

335
00:23:08,000 --> 00:23:15,000
who actually grabbed rent out of them until he squeezed the living wits out of them.

336
00:23:15,000 --> 00:23:17,000
This is precisely what we're having.

337
00:23:17,000 --> 00:23:23,000
We're having machinery and cloud capital that allows us to do a lot of stuff, right?

338
00:23:23,000 --> 00:23:25,000
Podcasts and so on.

339
00:23:25,000 --> 00:23:32,000
That said, if you look at the concentration of the capacity to influence public opinion,

340
00:23:32,000 --> 00:23:45,000
we've never had less of a free press than we have today within the context of each one of us being able to be a small BBC or Euro news or whatever.

341
00:23:45,000 --> 00:23:51,000
I guess when I hear a lot of what you say, the sentiment that you have is something that I agree with entirely.

342
00:23:51,000 --> 00:24:01,000
I'm personally by no means a fan of Amazon or any of these tech giants and I hate the level of influence they have over our lives.

343
00:24:01,000 --> 00:24:04,000
But I think of it more in terms of algorithmic persuasion.

344
00:24:04,000 --> 00:24:15,000
I think of it more in terms of slightly sociopathic tendencies to monetize our attention in ways that leads us to be more angry with each other than we should be.

345
00:24:15,000 --> 00:24:23,000
So I wonder sometimes if maybe the real brunt of your concern is not more about algorithms and the way that they're used.

346
00:24:23,000 --> 00:24:28,000
I love algorithms. The question is who owns the bloody thing, right?

347
00:24:28,000 --> 00:24:35,000
If one person owns the algorithm that controls billions of people, then we have something worse than 1984.

348
00:24:35,000 --> 00:24:38,000
We're shifting towards Brave New World.

349
00:24:38,000 --> 00:24:40,000
There's sort of opening up access to algorithms.

350
00:24:40,000 --> 00:24:43,000
You see, 1984 was a problem of surveillance.

351
00:24:43,000 --> 00:24:49,000
Brave New World is a problem where we are all happy little slaves who love slavery, right?

352
00:24:49,000 --> 00:24:54,000
And that is a problem. If you are a liberal, if you believe in freedom, if you believe...

353
00:24:54,000 --> 00:24:58,000
But also there's something else that really worries me.

354
00:24:58,000 --> 00:25:06,000
You talked about algorithms that are primed to maximize rage and outrage and intolerance.

355
00:25:06,000 --> 00:25:13,000
We all know that, right? You only need to go on ex-formerly Twitter to five minutes of that.

356
00:25:13,000 --> 00:25:17,000
Who said that? I think it was Stephen Fry who said something brilliant.

357
00:25:17,000 --> 00:25:19,000
He said this quite a long time ago before Musk.

358
00:25:19,000 --> 00:25:27,000
It's a bit like taking everything which is written on the walls of mail toilets around the world and posting it online.

359
00:25:27,000 --> 00:25:30,000
So yeah, I agree with you. But think of this.

360
00:25:30,000 --> 00:25:34,000
If my macroeconomic analysis in the book is right,

361
00:25:34,000 --> 00:25:42,000
we have a situation where, as David Ricardo in 1809 wonders,

362
00:25:42,000 --> 00:25:54,000
if you have an economic system where increasing percentages of income are siphoned off the cycle of investment by renteers.

363
00:25:54,000 --> 00:25:59,000
He was talking about the corn laws back then, due to the Napoleonic Wars.

364
00:25:59,000 --> 00:26:11,000
The war in Europe then, the Napoleonic Wars, were a boon to landlords because they didn't have to compete with imported corn.

365
00:26:11,000 --> 00:26:18,000
And therefore they managed to charge higher and higher rents on the producers of corn who used their land.

366
00:26:18,000 --> 00:26:25,000
But because they just simply got rich in their sleep because it was rent, it was not capitalist profit.

367
00:26:25,000 --> 00:26:31,000
It was as if this money, this economic energy was taken out of the circular flow.

368
00:26:31,000 --> 00:26:35,000
There was less investment and the whole system was becoming degenerate.

369
00:26:35,000 --> 00:26:37,000
So I'm telling a similar story.

370
00:26:37,000 --> 00:26:47,000
If increasing quantities of economic energy are being siphoned off as rents by the owners of those cloud thieves, these platforms,

371
00:26:47,000 --> 00:26:53,000
then that explains to a very large extent why we have inflation.

372
00:26:53,000 --> 00:26:56,000
Central banks continue to print money.

373
00:26:56,000 --> 00:26:57,000
Why?

374
00:26:57,000 --> 00:26:59,000
Even though you have inflation?

375
00:26:59,000 --> 00:27:09,000
Well, because aggregate demand is shrinking as a result of the fact that a lot of wealth is being siphoned off the circular flow of income in the form of cloud rents.

376
00:27:09,000 --> 00:27:18,000
So you have inflation, you have bullshit jobs, as David Greber scientifically put it.

377
00:27:18,000 --> 00:27:26,000
You have discontent building up with our central banks, with our governments, with markets, with inflation.

378
00:27:26,000 --> 00:27:33,000
And then you have those algorithms that make a lot more money out of priming the outrage.

379
00:27:33,000 --> 00:27:39,000
And then the more they prime the outrage, the more they extract money from the circular flow of income and the more outrage there is.

380
00:27:39,000 --> 00:27:41,000
That spins out of control.

381
00:27:41,000 --> 00:27:43,000
And what becomes the endpoint for this?

382
00:27:43,000 --> 00:27:45,000
The end of civilization.

383
00:27:45,000 --> 00:27:46,000
Okay.

384
00:27:46,000 --> 00:27:49,000
This is not going to be good.

385
00:27:49,000 --> 00:27:52,000
I don't think anything good is going to come out of it.

386
00:27:52,000 --> 00:27:59,000
I'm not one of those left-wing revolutionists who think, like Lenin once said, that the worst things get the better it is.

387
00:27:59,000 --> 00:28:00,000
I don't believe that.

388
00:28:00,000 --> 00:28:07,000
I've seen, in this country, in Greece, I've seen the deterioration of living standards year after year after year.

389
00:28:07,000 --> 00:28:09,000
And that only produces Nazis.

390
00:28:09,000 --> 00:28:10,000
Nothing good.

391
00:28:10,000 --> 00:28:20,000
And when you look towards a tool like AI, can you see that as being something to bring us out of that spiral?

392
00:28:20,000 --> 00:28:22,000
No technology will bring us out.

393
00:28:22,000 --> 00:28:26,000
I mean, in a good society, we will be using AI all the time.

394
00:28:26,000 --> 00:28:28,000
All the time.

395
00:28:28,000 --> 00:28:33,000
We no longer need, for instance, teachers training us to do things.

396
00:28:33,000 --> 00:28:35,000
We need teachers to educate us.

397
00:28:35,000 --> 00:28:39,000
But the training can be subcontracted to AI beautifully already.

398
00:28:39,000 --> 00:28:41,000
So I love it.

399
00:28:41,000 --> 00:28:48,000
But it will not solve the problem of the cloudelists, as I call them, exorbitant power of the rest of society.

400
00:28:48,000 --> 00:28:53,000
If anything, it will make it worse because AI makes the algorithms faster and better.

401
00:28:53,000 --> 00:29:08,000
How do you sort of reconcile in your mind these sort of cloud owners with forces for absolute evil versus companies that have elements to what they do, which is beneficial to society?

402
00:29:08,000 --> 00:29:14,000
One could look at YouTube and see how that could educate people across the world who otherwise wouldn't have access to books.

403
00:29:14,000 --> 00:29:20,000
Is it possible in your head to sort of reconcile what's a good use of server-based technology and what's bad?

404
00:29:20,000 --> 00:29:21,000
Of course, absolutely.

405
00:29:21,000 --> 00:29:29,000
There are a lot of fantastic and fully humanist uses of technology today.

406
00:29:29,000 --> 00:29:37,000
The question is, where is humanity as a whole being led by the more powerful forces to work within it?

407
00:29:37,000 --> 00:29:39,000
That is the question.

408
00:29:39,000 --> 00:29:51,000
And we have to constantly be on the lookout for good uses of technology, which are all over us, for ideas of how society should be functioning, designed.

409
00:29:51,000 --> 00:29:54,000
What its architecture should be.

410
00:29:54,000 --> 00:30:02,000
How do you see these conversations progressing in the context of global climate change and move towards net zero?

411
00:30:02,000 --> 00:30:07,000
Do you think of that as being again a sort of catalyst to bring the end closer?

412
00:30:07,000 --> 00:30:15,000
Do you think of it as an environment which changes people's motivations away from consumption in a way that helps decelerate this change?

413
00:30:15,000 --> 00:30:16,000
No, that's the opposite.

414
00:30:16,000 --> 00:30:28,000
Because in the same way that the algorithms are primed to excite intolerance in our souls, they are primed to make us buy things that we neither need nor want,

415
00:30:28,000 --> 00:30:35,000
and to forget about difficult things like the climate crisis.

416
00:30:35,000 --> 00:30:41,000
Of course, these algorithms are essential in fighting the climate crisis.

417
00:30:41,000 --> 00:30:53,000
So if we as a society, as a community, as a League of Nations or societies, if we could agree to stop drilling for oil and, you know,

418
00:30:53,000 --> 00:30:59,000
force influence more generally, if we agree to end the wars, that would be very helpful.

419
00:30:59,000 --> 00:31:03,000
War doesn't help the climate in the slightest.

420
00:31:03,000 --> 00:31:08,000
Then we would design our new green energy grids.

421
00:31:08,000 --> 00:31:18,000
And you can only design it if you have very strong use of fantastic algorithms that are necessary in order to ensure that the peak load is always used properly,

422
00:31:18,000 --> 00:31:26,000
that, you know, wind, solar and other renewables are combined in the optimal manner.

423
00:31:26,000 --> 00:31:37,000
So, like the beginning of time, our technologies are a force for good and for a force of evil.

424
00:31:37,000 --> 00:31:39,000
And if evil prevails, it is our fault.

425
00:31:39,000 --> 00:31:44,000
I think I want you to leave on some action that we can take.

426
00:31:44,000 --> 00:31:50,000
You know, like, I love what you say makes me feel like we're at this sort of liminal point.

427
00:31:50,000 --> 00:31:51,000
Okay, I will.

428
00:31:51,000 --> 00:31:55,000
If things work out well or badly, what can we do to ensure that we get to a better place together?

429
00:31:55,000 --> 00:31:58,000
Well, I think that we should concentrate on two things.

430
00:31:58,000 --> 00:32:04,000
Firstly, we must end free services, because you don't need me to explain that.

431
00:32:04,000 --> 00:32:11,000
When you have free services affected, if you've got the complete tyranny of the cloud capitalist or the cloudless,

432
00:32:11,000 --> 00:32:16,000
it would be fantastic if we had subscript micropayments, a micropayment system.

433
00:32:16,000 --> 00:32:21,000
And if some people can't afford it, they should get social security payments in order to make for these micropayments.

434
00:32:21,000 --> 00:32:23,000
So you're creating an app, right?

435
00:32:23,000 --> 00:32:29,000
You get paid directly by the person who is using the app, not indirectly through advertising,

436
00:32:29,000 --> 00:32:35,000
because that way you do not have this complete takeover of our souls.

437
00:32:35,000 --> 00:32:37,000
That's one thing.

438
00:32:37,000 --> 00:32:44,000
The second thing is fantastic if we started thinking in terms of changing corporate law.

439
00:32:44,000 --> 00:32:46,000
Imagine, just imagine.

440
00:32:46,000 --> 00:32:49,000
I know it sounds like science fiction, but technically it's really very simple.

441
00:32:49,000 --> 00:33:01,000
Imagine that you and I, if we were to form a company or 30 of us, 40 of us, we form a cooperative and we have one share each.

442
00:33:01,000 --> 00:33:12,000
Imagine if every company, especially large companies, had a share structure whereby every employee had one share which could not be traded.

443
00:33:12,000 --> 00:33:20,000
In the same way as a university student gets a library card or a student union card when they enroll,

444
00:33:20,000 --> 00:33:26,000
and then they have to hand it over or it becomes invalid when they leave, when they graduate.

445
00:33:26,000 --> 00:33:31,000
They cannot sell it, they cannot buy it, but they can use it to vote, they can use it to take books out,

446
00:33:31,000 --> 00:33:34,000
they can use it to use the internet and so on.

447
00:33:34,000 --> 00:33:39,000
Imagine if that's how shares were and they gave you one vote in the company and you worked.

448
00:33:39,000 --> 00:33:42,000
It didn't mean equality because we could vote.

449
00:33:42,000 --> 00:33:50,000
The person who actually creates the really good stuff which allows our company to do well, we should give more money to him or her.

450
00:33:50,000 --> 00:33:52,000
Imagine that.

451
00:33:52,000 --> 00:33:55,000
That would be a magnificent revolution.

452
00:33:55,000 --> 00:33:58,000
It would end share markets and labour markets in one go.

453
00:33:58,000 --> 00:34:00,000
And then you would have no state.

454
00:34:00,000 --> 00:34:07,000
You had market-based cooperatives owning the algorithm but in a way that is not predatory.

455
00:34:07,000 --> 00:34:15,000
And if they had to receive micropayments from those who actually use them, the algorithms,

456
00:34:15,000 --> 00:34:22,000
then we would be talking about technology in the interest of a combination of freedom and justice.

457
00:34:22,000 --> 00:34:23,000
Okay.

458
00:34:23,000 --> 00:34:29,000
So something kind of rooted in philosophy almost as like a DAO or something sort of based on blockchain

459
00:34:29,000 --> 00:34:31,000
or does the technology not matter?

460
00:34:31,000 --> 00:34:33,000
The technology doesn't matter.

461
00:34:33,000 --> 00:34:36,000
So what I described, you could do it with pieces of paper really.

462
00:34:36,000 --> 00:34:40,000
It helps to have an algorithm.

463
00:34:40,000 --> 00:34:46,000
Blockchain might be useful but the problem with blockchain is that it has become a religion.

464
00:34:46,000 --> 00:34:52,000
And you have people who religiously hate it and people who religiously adopt it.

465
00:34:52,000 --> 00:34:55,000
And I'm just not a religious person when it comes to these things.

466
00:34:55,000 --> 00:34:57,000
I think horses for courses.

467
00:34:57,000 --> 00:34:59,000
Blockchain can be very useful.

468
00:34:59,000 --> 00:35:01,000
I mean, J.B. Morgan uses it internally.

469
00:35:01,000 --> 00:35:03,000
If they use it internally, we should use it internally.

470
00:35:03,000 --> 00:35:07,000
But we must not think that blockchain is the answer.

471
00:35:07,000 --> 00:35:08,000
Blockchain is a tool.

472
00:35:08,000 --> 00:35:09,000
That makes sense.

473
00:35:09,000 --> 00:35:18,000
So when it does come to your wildest prediction, in some ways you're thinking that perhaps this could be the start of the downfall of civilization

474
00:35:18,000 --> 00:35:21,000
or you're also open-minded to there being other ways.

475
00:35:21,000 --> 00:35:23,000
I maintain hope.

476
00:35:23,000 --> 00:35:28,000
Hope is my duty and I cling on to it against all empirical evidence.

477
00:35:28,000 --> 00:35:30,000
That makes absolutely sense.

478
00:35:30,000 --> 00:35:32,000
Janne is very flacky. Thanks very much.

479
00:35:32,000 --> 00:35:33,000
Thank you, Tom.

480
00:35:33,000 --> 00:35:34,000
Thank you.

481
00:35:34,000 --> 00:35:35,000
Very good.

482
00:35:35,000 --> 00:35:37,000
Have a good time.

483
00:35:37,000 --> 00:35:38,000
That was good.

484
00:35:38,000 --> 00:35:39,000
It was fun.

