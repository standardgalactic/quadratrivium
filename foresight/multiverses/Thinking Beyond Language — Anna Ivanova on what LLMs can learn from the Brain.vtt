WEBVTT

00:00.000 --> 00:04.940
I'm James Robinson, you're listening to Multiverses.

00:04.940 --> 00:10.880
Language can do and express many things, and in fact this was the subject of my last conversation

00:10.880 --> 00:15.360
on this podcast with Nicol Krishnam talking about ordinary language philosophy.

00:15.360 --> 00:21.560
Just because language is so powerful, we might be tempted to think that that's all we need

00:21.560 --> 00:23.780
for understanding and predicting the world.

00:23.780 --> 00:27.720
It's just manipulation of symbols, next word prediction.

00:28.440 --> 00:34.760
However, if we look at the how the human brain at least actually works, it's rather different.

00:34.760 --> 00:37.320
Our guest this week is Anna Ivanova.

00:37.320 --> 00:41.400
She's an assistant professor at Georgia Institute of Technology,

00:41.400 --> 00:45.240
and she tries to understand the relationship between language and thought,

00:45.240 --> 00:51.480
and she does this by looking at brain scans, essentially MRIs, of what's going on when

00:51.480 --> 00:54.200
humans are presented with particular scenarios.

00:54.200 --> 00:58.920
For example, I'm looking at this marvellous view right now from Carlton Hill, Edinburgh.

00:58.920 --> 01:02.920
I'm not thinking about it linguistically, it's going straight into my visual cortex

01:02.920 --> 01:06.440
and processes happening there, and if I want to reason about it,

01:06.440 --> 01:09.080
I'm not going to reason about it linguistically either.

01:10.200 --> 01:15.880
A lot of her work looks at how conceptual knowledge of the world is not tied to the language area

01:15.880 --> 01:21.560
of our brain, so for example subjects with aphantasia, people who have large-scale

01:21.560 --> 01:27.480
damage to the language network, are still able to reason not only logically about

01:27.480 --> 01:31.320
chess problems and things like that, but they can reason socially as well.

01:31.320 --> 01:34.280
They can understand what situations are unusual.

01:35.960 --> 01:41.320
So this is a really insightful and very timely conversation because it plays into a lot of

01:41.320 --> 01:47.720
the enthusiasm about LOMs, which I certainly buy into myself, but it calls into question

01:47.720 --> 01:56.680
some of this, forcing us to think, well, what is necessary on top of simple linguistic abilities

01:56.680 --> 02:00.120
to really be a fully-fledged thinking machine?

02:00.920 --> 02:06.920
I think one question that still exists in my mind is, to what extent just language

02:06.920 --> 02:11.320
manipulation could get us to a fully thinking machine, somewhat in the same way as, you know,

02:11.320 --> 02:21.080
I have a GPU and a CPU on this laptop, and I could use my CPU to play, you know, vector-based

02:21.640 --> 02:26.280
computer games, and I could use my GPU to send emails, but they're not really suited to that task,

02:26.280 --> 02:33.640
but the point is maybe language could be a kind of fully-fledged thinking system.

02:33.640 --> 02:37.960
However, I think it is valuable to learn from, in fact, what the brain does, of course.

02:38.680 --> 02:41.160
So I really enjoyed this conversation. I hope you did too.

02:57.400 --> 03:00.280
Hi, Anna Ivanova, welcome to Multiverses.

03:01.320 --> 03:02.680
Hi, thank you for having me.

03:03.400 --> 03:09.240
So we're speaking and we're thinking, I think, and people who are listening are listening to our

03:09.240 --> 03:15.640
words and they're thinking, and thought seems like something that should be really familiar to us,

03:15.640 --> 03:21.560
because it's one of those kind of few things that we have really direct access to, and yet,

03:21.560 --> 03:27.400
at the same time, it seems so mysterious, so hard to figure out exactly what's going on.

03:27.400 --> 03:31.320
Maybe it's because the piece that's doing the figuring out is the thing that we're trying to

03:31.320 --> 03:38.520
figure out itself, I don't know. But yeah, how can we get some sort of grip on what thought is?

03:39.800 --> 03:40.600
Where do we start?

03:42.280 --> 03:49.880
Well, I think we need to start with definitions, what it is that we mean by thought,

03:49.880 --> 03:56.280
because different people use the word in different senses, and of course, it also depends on the

03:56.280 --> 04:06.600
context. And generally speaking, at least in my area of work, I think there is the broad definition

04:06.600 --> 04:13.880
and the narrow definition. And in the broad definition, thought is synonymous with cognition.

04:13.880 --> 04:24.360
So the mental processes that we use to make sense of the world around us, so that includes reasoning,

04:24.440 --> 04:32.280
that includes accessing memories, that includes various social communication capacities,

04:32.280 --> 04:40.600
so very broadly speaking, something we would call cognition. And then the narrow definition is the

04:40.600 --> 04:48.520
stuff that happens kind of like in between us doing things. So it's not necessarily you get a math

04:48.520 --> 04:55.800
problem, your reason about it, you give the result. But it's more about you lying down in bed at night

04:55.800 --> 05:04.440
and thinking about your day tomorrow, or you're walking somewhere and you're playing out your

05:05.000 --> 05:10.840
conversation that you're going to have with a friend. And so this kind of inner thinking that

05:10.840 --> 05:17.480
happens spontaneously, not in response to any external task is also something where people

05:18.360 --> 05:22.520
that people commonly refer to as thought. And of course, those are very different, right? Like

05:22.520 --> 05:27.080
if we're talking about the broad thing or the narrow thing, this specific kind of mechanisms

05:27.080 --> 05:33.240
that might support them might differ quite a bit. Yeah, I think both of those kind of definitions

05:34.120 --> 05:42.200
capture something of what one intuitively would characterize as thinking. So we might say,

05:43.160 --> 05:48.520
you know, broadly speaking, oh, yeah, of course, when this person solved that math problem,

05:48.520 --> 05:52.840
they had to be thinking. But then you might also say, oh, they did it so quickly, they just did it

05:52.840 --> 05:57.480
without thinking, which would be sort of more of the narrow definition, they did it sort of

05:58.040 --> 06:03.000
without any kind of reflection, which is kind of what the narrower definition requires, I guess.

06:04.680 --> 06:08.040
I'm curious, do you have a kind of preference for either of those definitions, or do you think

06:08.120 --> 06:15.160
they both serve a kind of useful purpose? I've used both. I'm interested in the relationship

06:15.160 --> 06:20.680
between language and thought, the role that language plays in thinking. And so the role

06:20.680 --> 06:27.800
that language plays in thinking broadly defined thinking as cognition, that actually turns out to

06:27.800 --> 06:33.640
be a more tractable question, because we can ask people to do a math problem and look whether

06:33.640 --> 06:40.120
language processing regions in the brain are engaged. But for inner thinking,

06:40.120 --> 06:45.720
stuff that happens spontaneously without an external task, that's much harder to capture.

06:45.720 --> 06:50.120
But that's also where people have very strong intuitions there. Of course, I think in words,

06:50.120 --> 06:56.520
or of course, I think without words. And so that area is harder to study, but also very

06:56.520 --> 06:59.640
interesting. And so that's where I see some of my future work going.

07:00.600 --> 07:05.720
Interesting. Yeah, it's true that kind of more reflexive thinking almost by definition,

07:05.720 --> 07:08.760
people are going to have opinions about it, because they are kind of

07:09.400 --> 07:16.520
cogitacing, turning things over. And part of that process is inward looking. So yeah,

07:16.520 --> 07:20.040
people are going to be like, oh, well, I always do that with words or with images or a mix.

07:21.000 --> 07:25.640
And yet sometimes I can also find it a little bit hard to remember to do that and think about,

07:25.720 --> 07:30.200
because whenever I sort of think about, if I try to think about what thinking is,

07:30.200 --> 07:33.400
I will do it linguistically. But maybe that's just because it's the sort of,

07:34.280 --> 07:37.880
that's the sort of way that I need to think about that thing. Whereas if I think about

07:37.880 --> 07:42.280
something entirely different, like a kind of, you know, spatial reasoning problem,

07:42.280 --> 07:48.040
I'm sure I would do it in a different way. But it maybe wouldn't engage the linguistic part of my

07:48.600 --> 07:56.840
brain. But anyway, I guess, yeah, this is a very, we've got very quickly to a very

07:58.600 --> 08:02.280
key area, which is this, this, this relationship between language and thought, which

08:04.600 --> 08:11.320
my last guest was Nikol Krishnan, an ordinary language philosopher. And we spent some time

08:11.320 --> 08:15.720
just talking about, well, how for a while, people just thought there was no difference

08:15.720 --> 08:24.120
between the two. There was, or rather, in some way, language captured the entirety of everything,

08:24.120 --> 08:28.280
including thought, maybe including some other things. So, you know, Wittgenstein's famous

08:28.280 --> 08:33.160
dictum, the limits of my language, the limits are the limits of, or mean the limits of my world.

08:36.200 --> 08:42.200
But I guess your research is maybe questioning that. Would that be fair to say?

08:42.440 --> 08:50.840
Yeah, I use Wittgenstein's quote as an example of a worldview, a paradigm that I'm pushing against.

08:53.000 --> 09:00.760
And what's wrong with it? I mean, as we said, like, when we try to describe what thought is,

09:00.760 --> 09:11.480
we'll probably reach for language. And yeah, it seems like so many of the ideas,

09:11.480 --> 09:15.880
I don't want to say everything, but so much of the ideas that we have, and we pass on, we do

09:16.840 --> 09:21.480
with language. And maybe there's an argument that the places where we're not doing it with

09:21.480 --> 09:29.480
language are somehow dependent on language behind the scenes. But that's not a very scientific

09:29.480 --> 09:34.440
argument. And I think you have some kind of evidence to the contrary. So, yeah, maybe take us

09:34.440 --> 09:42.200
through some of the things that you've done to probe this. Yeah, let me, there is a lot to say.

09:42.200 --> 09:49.800
So, let me start first by, I guess, acknowledging the last bit that you said, where clearly,

09:49.800 --> 09:57.160
there is a relationship between language and thought. And the most trivial, but also important

09:57.160 --> 10:03.240
one is the fact that we use language to transmit information, to communicate thoughts to one

10:03.240 --> 10:08.840
another. And that's a very powerful mechanism, we can translate knowledge from generation to

10:08.840 --> 10:17.000
generation. So that is a very important role of language in thinking, helping us share information

10:17.000 --> 10:23.160
without having to figure out every single thing individually. But here, what we're talking about

10:23.160 --> 10:29.720
is using language as a medium of thinking. So internally, do we think in words, do we recruit

10:29.800 --> 10:36.200
the mechanisms for language processing when we're thinking? And so that's an important distinction.

10:36.200 --> 10:44.360
So that's the scope. Now, as we said, people have strong intuitions about whether or not

10:44.360 --> 10:52.120
they use language to think. And probably that these intuitions are grounded in people's personal

10:52.120 --> 11:00.760
experiences thinking. And so one important fact to keep in mind is that there is huge

11:00.760 --> 11:09.000
individual variability in how people perceive their own thinking to be. And so my pet theory

11:09.000 --> 11:15.960
is that a lot of philosophers are strong verbal thinkers, they spend a lot of time writing,

11:15.960 --> 11:23.000
they think about abstract topics. And so to them, the link between language and thought and

11:23.000 --> 11:30.360
their experience is very strong. And it just seems that people, not just philosophers,

11:31.080 --> 11:38.120
have this tendency to assume that everybody else thinks in kind of the same way. And so

11:38.760 --> 11:44.120
if you are a strong verbal thinker, you automatically assume that everybody else is as well.

11:44.680 --> 11:51.400
Until you start actually talking to other people and asking about their experiences.

11:51.400 --> 11:57.080
And so I've had these conversations with people at parties or just informally, you ask them,

11:57.080 --> 12:03.720
oh, hey, how often do you think in words? Most of the time, some of the time never. And people

12:03.720 --> 12:08.920
are always surprised to hear that other people's experiences might be completely different. And

12:08.920 --> 12:18.040
so this is just, I think, a very important thing to keep in mind that our intuitions

12:18.040 --> 12:23.400
can lead us because they don't necessarily reflect a universal human experience. It's just,

12:23.400 --> 12:33.320
you know, that's how we think. And so on to the actual evidence that we can use to dissociate

12:33.320 --> 12:45.160
language and thought. There are a few different strands. So one example, very briefly, is the fact

12:45.160 --> 12:51.880
that animals who don't have language might often have pretty sophisticated thought and planning

12:51.880 --> 13:01.960
capabilities, right? We know examples of crows being very smart, alphoctopi, even a squirrel that

13:01.960 --> 13:06.760
is trying to figure out whether to jump from tree to tree or if it's too far and it needs to go on

13:06.760 --> 13:13.640
the ground instead. These are pretty sophisticated capabilities. And so that's just a very basic

13:13.640 --> 13:18.760
example of at least some kinds of thought. You can then argue, you know, oh, but like the kind of

13:18.760 --> 13:22.600
thinking they're doing is not, you know, the kind of thinking that we care about, right? And that's

13:22.600 --> 13:27.400
where the meat of the debate is. But pretty sophisticated cognition is possible in non-human

13:27.400 --> 13:37.560
animals from what we know. For me, I work with humans and adult humans. And so what we can do

13:37.560 --> 13:47.560
is we can identify parts of the brain that are engaged in language processing. So it turns out

13:47.560 --> 13:55.800
that there is a set of brain regions in the brain known as the language network that are responsible

13:55.800 --> 14:01.480
for language comprehension. So whether you're listening to somebody talk or reading, they're

14:01.480 --> 14:05.960
also engaged during language production. So when you're speaking and when you're writing,

14:06.920 --> 14:13.320
they are engaged in response to any language that you might know that also includes sign languages.

14:13.320 --> 14:21.080
So it doesn't even have to be a spoken language. And these regions turns out are pretty selective

14:21.080 --> 14:28.920
for language. So they respond to all kinds of language, but not to music, not to math,

14:29.560 --> 14:39.560
not to general problem solving. And so this is pretty strong evidence that language and many

14:39.560 --> 14:45.000
different kinds of thinking are actually separable in the brain. That language has its own neural

14:45.000 --> 14:54.120
machinery. And that's important because it turns out that if language areas of the brain

14:54.120 --> 15:01.160
are damaged, it will affect your ability to use language, but not necessarily your ability to

15:01.160 --> 15:09.240
think. And so the most common example of that is a condition known as aphasia. So it difficulties

15:09.240 --> 15:15.640
with language production or comprehension. Often, most commonly, it arises as a result of

15:16.280 --> 15:21.640
a person having a stroke. And so if it's stroke effects left hemisphere, which is where the

15:21.640 --> 15:27.160
language network is in most people, they might have really serious difficulties with language

15:27.160 --> 15:33.880
production or comprehension. But if that language is limited to the language network, it turns out

15:33.880 --> 15:42.920
that their ability to use other kinds of thinking remains intact. So these people with really severe

15:42.920 --> 15:50.920
aphasia who really can't understand language or speak, they can solve math problems. They can

15:50.920 --> 15:57.720
arrange pictures so they form a story so they can reason about cause and effect. They can look at

15:57.720 --> 16:02.680
the picture or show in some kind of event, like a fox cheese in the rabbit or the rabbit cheese in

16:02.680 --> 16:08.600
the fox and say which one is more likely to happen in the real world. If it's something like really

16:08.600 --> 16:14.920
weird, like the scuba diver biting a shark, they will laugh because it's just kind of ridiculous.

16:14.920 --> 16:19.160
And so, you know, you can tell that they understand what's going on. And there are really fascinating

16:19.160 --> 16:23.800
cases, you know, some of them like like the plague chest on the weekend. So clearly, very

16:23.800 --> 16:30.520
sophisticated forms of reasoning are preserved even in the face of severe damage to language

16:30.520 --> 16:35.960
processing centres in the brain. Yeah, that's completely fascinating. And well, firstly, by the

16:35.960 --> 16:45.880
way, I love your theory about philosophers and how maybe the sort of minds that they have that

16:45.880 --> 16:54.600
make them good philosophers sort of self select or selecting a very biased or unusual community of

16:54.600 --> 17:02.520
people who think in a particular way. And yeah, so that's really interesting. It'd be great to have a

17:02.520 --> 17:08.920
survey of philosophers, I don't know if this has happened, and how they describe their own thoughts

17:08.920 --> 17:16.360
and compare it to other groups. Yeah, very interesting. Yeah, I think sometimes that maybe I

17:16.360 --> 17:19.720
should, you know, not talk about this theory and actually test it experimentally first,

17:19.800 --> 17:27.000
that we add on biased people in advance. No, as long as they don't listen to this,

17:27.000 --> 17:32.520
or maybe you can do it anonymously or something. I'm actually talking, I think,

17:33.480 --> 17:39.560
soon to someone who from the philosophy of science who surveyed physicists to see if they are

17:39.560 --> 17:43.320
realists in terms of, you know, how they think about the entities of science or not. So I think

17:43.320 --> 17:48.120
it's like it is really interesting to actually just, yeah, try to figure out how it is that

17:50.680 --> 17:56.520
yeah, how it is that people's personal beliefs and their kind of academic disciplines or their own

17:56.520 --> 18:02.840
or the peculiarities of their minds intertwine. But coming back to the kind of experiments that

18:02.840 --> 18:10.040
you describe, so yeah, I think these are just really, yeah, wonderful illustrations of how

18:10.040 --> 18:16.200
thought maybe extends beyond the language network. And I suppose what you're doing is you're asking

18:16.200 --> 18:22.760
people, so for example, the way that we know that music is not within the language network is,

18:23.560 --> 18:27.880
I don't know, the language network is defined as the kind of a bit of the brain that lights up

18:27.880 --> 18:33.800
in MRI scans. You see a lot of activity there when you give people sentences and linguistic

18:34.760 --> 18:40.120
tasks, I don't know, maybe reading or producing language. And then it's a different area of the

18:40.120 --> 18:45.800
brain that lights up when they're listening to music or when they're solving a math or chess

18:45.800 --> 18:55.080
problem. And even when people have quite severe damage at South Asia, and the language part of

18:55.080 --> 19:00.360
the brain is unable to comprehend or produce language or both, they can still do many of

19:00.360 --> 19:05.880
those other things, which is, I mean, that's really interesting for one thing, because I often

19:05.880 --> 19:12.680
think as well, language is maybe being so key to the kind of input output of the brain that,

19:13.400 --> 19:20.120
you know, for example, reading a math problem would kind of go via the language network,

19:20.120 --> 19:26.120
or is it that our brain is sort of able to just kind of directly take those symbols into,

19:26.120 --> 19:33.080
I don't know, a different area of the brain, or perhaps do we have to kind of pose those

19:33.080 --> 19:39.320
problems in a kind of more visual way? I don't know, I'm curious about whether the language

19:39.320 --> 19:46.200
network is kind of a gateway for much of the information going in. So it looks like if you

19:46.200 --> 19:53.640
give people math problems in the form of mathematical symbols, right, like five plus three,

19:53.640 --> 19:59.000
first question mark, it seems like it doesn't need to go through the language network. So even

19:59.000 --> 20:06.680
though it's symbolic, not all symbols get processed by the language network. And perhaps even more

20:06.680 --> 20:16.280
strikingly, one study that I did in graduate school was looking at computer code. And specifically,

20:16.280 --> 20:26.120
we looked at Python, which is very English-like by design, so it uses English words. And on the

20:26.120 --> 20:31.400
other end of the spectrum, we took a graphical programming language for kids called Scratch

20:31.400 --> 20:37.320
Junior. So it has different characters. And so then you have different arrows showing, you know,

20:37.320 --> 20:42.280
the characters going left or jumping. But it has a lot of the same control flow elements that you

20:42.280 --> 20:50.520
would have in text-based code, like if statements and for loops and stuff like that. And so it turns

20:50.520 --> 20:55.800
out that for both of these languages, the main network in the brain that's responsible for

20:55.800 --> 21:02.920
extracting meaning from that code is the so-called multiple demand network. And so that's the

21:02.920 --> 21:09.160
network that's responsible for problem solving and reasoning and planning, and not the language

21:09.160 --> 21:15.480
network. The language network responded a little bit to Python code. But even there, we actually

21:15.480 --> 21:21.400
weren't able to exactly establish its relation, its role and why it would. It might be some kind of

21:21.400 --> 21:25.960
false positive where the language network is like, oh, that's language, oh, wait, no, never mind. And

21:25.960 --> 21:31.720
it kind of goes down. So there are other researchers that are promoting that theory currently. But

21:33.640 --> 21:41.800
even for code, we call programming languages languages because how similar they are

21:41.800 --> 21:47.240
structurally to natural languages. Even there, it looks like it's not the language network that's

21:47.240 --> 21:53.160
doing the majority of the heavy lifting. Yeah, I found that completely, well, surprising actually.

21:53.160 --> 21:58.280
And I think you noted in the paper that people kind of fell on two sides of the fence. Some people

21:58.280 --> 22:02.680
were surprised and some people were, oh, no, that makes complete sense. But I was personally

22:03.480 --> 22:08.600
really surprised because I, as you say, there's so much similarity between the way that natural

22:08.600 --> 22:14.680
language works in terms of being compositional and having these kind of hierarchical features

22:14.680 --> 22:18.360
and the way that programming languages work that I would think, okay, well, you know,

22:21.560 --> 22:26.600
it's right that we call them languages because they're so closely related, they're just sort of,

22:27.640 --> 22:36.440
I guess, a bit stricter, less ambiguous, perhaps. But the kind of, the nature of the rules is not

22:36.440 --> 22:44.920
so different. And yet, yeah, it's almost as if one could imagine there being some sort of animal,

22:44.920 --> 22:49.400
like a crow, like you said, like very intelligent creature, doesn't have language, but maybe it's

22:49.400 --> 22:54.200
got a really good multiple demand network. And perhaps we could, perhaps could be a really good

22:54.200 --> 22:59.240
programmer, because it's not that part of the brain, which is being recruited. But it's rather

22:59.240 --> 23:05.160
this kind of almost clearinghouse from what I understand, the multiple demand network just

23:05.160 --> 23:14.280
picks up so many different jobs. The other thing that really stood out for me in this paper,

23:14.280 --> 23:20.280
which I really enjoyed, was that as a kind of, I guess, control, you could, you presented people

23:20.280 --> 23:29.640
the same problems. So the, if I remember rightly, one of the, one of the code pieces of code that

23:29.640 --> 23:35.240
people had to interpret in Python was a calculation of BMI. And so it's like, here is a variable,

23:35.240 --> 23:40.680
which is your weight, here is one issue is your height, BMI equals height divided by weight squared.

23:42.280 --> 23:46.600
And so the person kind of reads through that. And, and you see it being passed off to the

23:46.600 --> 23:54.120
multiple demand network. But then there's the same problem defined entirely verbally. So instead

23:54.120 --> 23:58.680
of using, you know, symbols with equals, and it clearly being Python code, it's just, I know,

23:58.680 --> 24:03.800
this is what BMI is, here, you know, here's how much you weigh, here's how tall you are, what's

24:03.800 --> 24:11.560
your, what's your BMI. And that went to a different region of the brain, which for me was just like,

24:11.560 --> 24:17.800
okay, well, this is the same problem, but the way that it's presented really changes the way that

24:17.800 --> 24:25.080
we think about it. Which, yeah, that was another huge surprise for me to think just how influential

24:25.080 --> 24:33.560
the kind of presentation or the, the medium, I guess, for a set of concepts, how much that,

24:33.560 --> 24:37.320
that determines how those concepts are handled internally, mentally.

24:39.000 --> 24:46.280
Yeah. And in fact, that's not that uncommon of a situation if you think about it. So let's say

24:46.280 --> 24:52.840
somebody is listening to this podcast, versus reading the transcript, the way information

24:52.840 --> 24:59.000
gets into the brain is different. So in the auditory modality, it goes to the auditory cortex

24:59.000 --> 25:05.240
first. And in the visual modality, it goes through the visual cortex first, and then it gets into,

25:05.240 --> 25:10.200
we have a specialized part of the brain that's responsible for readings of recognizing written

25:10.200 --> 25:16.440
letters. But then they will converge in the language network, because language network

25:17.160 --> 25:24.280
is responsible for processing either a modality. And that means that these initially distinct

25:24.280 --> 25:32.680
representations have to converge in some, in some way. And so for some of the other cases,

25:32.680 --> 25:41.320
like a problem that's written in language versus in code, it looks like that convergence is also

25:41.400 --> 25:46.920
happening, but it's happening later on in the processing, right? So it goes, you know, through

25:46.920 --> 25:53.000
the language network, and I guess the multiple demand. And then you have some shared problem

25:53.000 --> 26:00.040
solving. So in this case, calculating the BMI is doing some math. And so that we think also happens

26:00.040 --> 26:03.960
in the multiple demand network. And in fact, we show in the paper that you can kind of break down

26:03.960 --> 26:08.360
that activity that we capture into the code reading part, an actual problem solving part.

26:08.840 --> 26:16.680
But it's a fascinating endeavor. In general, in cognitive neuroscience, how do we design

26:16.680 --> 26:22.840
an experiment where we have those kinds of different conditions, where they're very similar,

26:22.840 --> 26:29.960
except for something that we've changed. And so at what point that difference, right, auditor versus

26:29.960 --> 26:36.360
visual language versus code, where in the brain does that make a difference? And where doesn't it?

26:37.320 --> 26:44.200
Yeah, yeah. So you're saying that even though it goes the language area lights up when we have

26:44.200 --> 26:49.160
that kind of BMI problem, it's just kind of passing the thing. And then it gets passed off to

26:50.600 --> 26:55.000
you know, to actually run the calculation that happens, like that doesn't happen in the language

26:55.960 --> 27:02.680
area. Yeah, that makes sense. Yeah, okay. That clarifies my, I was very excited. I thought that

27:02.760 --> 27:07.080
maybe that we had some sort of like way of doing the confrontation, just linguistically.

27:09.000 --> 27:14.360
Guess that's what that doesn't work. I think it's possible. And we don't well,

27:15.640 --> 27:18.920
I don't know. Well, maybe not linguistically. But like, you know,

27:19.800 --> 27:26.280
we memorize the multiplication table, or for like some problem that we do very often, we don't need

27:26.280 --> 27:31.240
to actually go through the steps of the calculation, we kind of just retrieve the correct answer.

27:32.040 --> 27:37.160
I don't know if it happens linguistically or not, potentially not, probably not. But it's

27:37.160 --> 27:41.400
still a different mechanism than actually going step by step and doing, you know,

27:41.400 --> 27:45.560
long division in your head, or like summing multidigit numbers or something like that.

27:46.280 --> 27:50.920
Yeah. Yeah, I think that that's a really interesting question, which we can maybe come back to.

27:50.920 --> 27:55.320
But I guess, yeah, so you kind of see both the multiple demand and the language

27:55.320 --> 27:59.720
network lighting up when this problem is presented linguistically. So it's sort of a

27:59.720 --> 28:05.400
fair assumption that what is in fact happening is that, you know, there's probably some linguistic

28:05.400 --> 28:12.200
processing, but then it then gets passed to the same sort of area of the brain, which is,

28:12.200 --> 28:17.800
which handles the pure Python problem. But of course, yeah, I mean, that is really interesting

28:18.600 --> 28:24.200
and kind of useful in some ways, in that, you know, it seems more efficient to be

28:24.200 --> 28:30.120
presented just with the Python code, right? You kind of bypass that. Oh, I turn this into,

28:31.560 --> 28:37.880
you know, it goes straight into the, the system, which can perform the ultimate calculation, I

28:37.880 --> 28:43.560
guess. I don't know if you, if you were able to capture any information on whether it was quicker

28:43.560 --> 28:49.880
for people to kind of solve the problem when presented with the Python code or not.

28:50.680 --> 29:02.040
I don't remember whether we saw a difference in how long it took people.

29:02.760 --> 29:08.360
I think it's possible, but I don't, some of it, of course, depends on how proficient they are in

29:08.360 --> 29:15.080
Python. So there might be individual differences there, also individual differences in how

29:15.080 --> 29:24.440
fast they would read text. So I'm sure there's some variability there. But it's actually

29:25.640 --> 29:31.720
an interesting thought that you bring enough so that this, having this abstract skeleton with

29:32.360 --> 29:38.840
other information stripped away might make the problem solving the calculation easier.

29:38.840 --> 29:46.520
Because in fact, there are cases where researchers have observed the reverse,

29:47.640 --> 29:57.160
though there is this famous Waste and Selection task, which you have, let's see,

29:57.560 --> 30:07.080
a card with like a two and a seven, and then a, and then a card with like

30:07.960 --> 30:21.000
green, red and blue. And you need to test the rule that says if the number is even,

30:21.080 --> 30:27.560
then the other side of the card has to be blue. And so then the question is which cards do you

30:27.560 --> 30:33.240
need to turn over to make sure that that rule is correct. And so then people want to test

30:35.480 --> 30:40.040
the card with the two on it because it's even, and so they want to make sure that their reverse

30:40.040 --> 30:45.240
is blue. But then they often want to turn over the blue to make sure that the other side is odd,

30:46.040 --> 30:51.720
sorry, is even. But that actually is not what you should do because it doesn't matter. Like if you

30:51.720 --> 30:56.360
have blue and odd, that's actually not a violation of the rule. What you need to do is you need to

30:56.360 --> 31:01.560
turn over the red card because of an even number there, then that rule gets violated.

31:02.360 --> 31:10.760
So that problem is hard for people. But if you cast the same problem saying that there are

31:11.640 --> 31:20.200
people at the bar and somebody, you know, is 16 and somebody is 25 and then somebody is drinking

31:20.200 --> 31:27.080
beer and somebody is drinking a Coke, then, you know, how will you verify that only people over

31:27.080 --> 31:32.120
the age of 18 are drinking alcohol? And then of course, you know, that you need to, you know,

31:32.120 --> 31:38.040
check the 16 year old and check the person drinking the beer and not any other way. And so

31:38.040 --> 31:44.280
mathematically, the same exact process, but it's much, much easier for people to ground the rule

31:44.280 --> 31:49.080
and their existing knowledge, not necessarily the bar example, but they're just kind of, you know,

31:49.080 --> 31:55.240
the easiest one and the most common one. And so this phenomenon is known as content effects on

31:55.240 --> 32:03.480
reasoning. And yeah, I think a lot of people, especially like, you know, physicists and mathematicians

32:03.480 --> 32:08.040
and so people trained in like hardcore STEM discipline, they're like, hey, Alex, trip away,

32:08.040 --> 32:12.760
all of the extra information only focus on the abstract symbols. That's the easiest thing. But

32:12.760 --> 32:19.400
actually for a lot of people grounding the problem in some specific content domain tends to help.

32:19.400 --> 32:25.000
And so I know that some people in like math education are very interested in this phenomenon and

32:25.000 --> 32:33.000
how does it how to make it easier for people to, for kids to learn math. Is it by focusing on the

32:33.000 --> 32:37.480
abstract or is it by grounding math problems in real life situations?

32:39.560 --> 32:45.720
And I suppose part of the reason why that grounding might work, well, there could be kind

32:45.720 --> 32:51.080
of two hypotheses. One is just like, it locates it in a different area of the brain, which is somehow

32:51.080 --> 32:56.680
better at processing this thing. So maybe that just the social reasoning part is just better at

32:56.680 --> 33:01.720
doing that kind of problem. But doesn't seem so likely in this case. And another is just that

33:02.200 --> 33:09.000
it gets it, it clicks it in to a place where you're able to recognize a pattern that you've

33:09.000 --> 33:15.160
seen before. And so you don't have to do, you know, you're already on the right track.

33:16.920 --> 33:22.600
And this maybe kind of comes back to your point about, well, maybe when we calculate the BMI for

33:22.600 --> 33:26.760
certain kind of combinations of numbers, you just know the answer. So it's being kind of recalled

33:26.840 --> 33:32.280
from memory, like that pattern is already so established that you don't need to reason through

33:32.280 --> 33:42.920
it in the same way. It's more of a recall operation. And I mean, this is getting us towards one of the

33:42.920 --> 33:47.720
kind of central questions, which is around, well, what are LLMs doing? Because they're kind of

33:48.920 --> 33:53.720
glorified recall machines in a certain way, or just really good pattern matches.

33:54.200 --> 34:01.960
Maybe before we get to that, though, I want to talk about another of your experiments, which I

34:01.960 --> 34:08.360
really enjoyed, which is about where people are looking at images of improbable and probable

34:08.360 --> 34:17.000
things like the shark and the swimmer that you mentioned. And what I found, well, maybe you

34:17.000 --> 34:21.800
should describe the experiment, you'll do a much better job of it than me. Because I think, yeah,

34:21.800 --> 34:24.440
there was just a really interesting piece here that kind of writes this.

34:26.200 --> 34:32.840
Yeah. So here, we use that same idea that the same information might arrive in the brain through

34:32.840 --> 34:41.480
different routes. And so in this case, we were looking at sentences describing basic interactions

34:41.480 --> 34:51.400
between two entities, like the, I guess we can roll with the shark bites the swimmer,

34:51.400 --> 35:01.320
the swimmer bites the shark, and pictures depicting the same kinds of events. And so here,

35:01.320 --> 35:07.000
by switching around, who's doing what to whom, we're manipulating whether the event is plausible.

35:07.000 --> 35:14.600
So likely to occur in the real world or impossible. So unlikely to occur. And the question was,

35:15.560 --> 35:23.640
does the language network respond to language specifically? Or does it respond to meaning

35:23.640 --> 35:29.880
and concepts more generally? And so if it's language, it should only really respond to

35:30.440 --> 35:37.880
sentences and not to pictures. And if it's responsible to meaning, it should respond equally

35:37.880 --> 35:43.240
strongly to sentences and pictures, as long as the person is thinking about the meaning. And so we

35:43.240 --> 35:48.600
had people tell us whether they think the event is plausible or impossible. So you have to be

35:48.600 --> 35:57.160
thinking about the meaning. And so what we found was actually something in between, where the language

35:57.160 --> 36:05.000
network, in accordance with all of the prior work, responds more strongly to sentences than to pictures.

36:05.880 --> 36:13.080
But it still responded to pictures to some extent. And I will say that in another study,

36:13.080 --> 36:21.000
we recorded responses in the language regions to pictures of objects. So is this animal dangerous?

36:21.000 --> 36:26.280
Can this object be found in the kitchen, that kind of stuff? And it did not respond to pictures of

36:26.280 --> 36:33.720
objects. So it was something about events, maybe just something more complex, maybe something just

36:33.720 --> 36:40.840
more fast-paced, that was specifically triggering responses in the language regions. And so this

36:40.840 --> 36:48.760
intermediate result, so preference for sentences over pictures, but also responses to meaning,

36:48.760 --> 36:57.560
even in known sentences, was kind of puzzling. And so one piece of evidence that helped us

36:58.680 --> 37:07.720
make sense of this information was evidence from individuals with global aphasia, from people with

37:07.720 --> 37:13.320
brain damage. And I should say that this is, yeah, so lots of the brain imaging work I'm describing,

37:13.320 --> 37:22.280
I did with my PhD advisor at Fedorenko, and the global aphasia bit is done in collaboration with

37:22.280 --> 37:32.120
Rosemary Varley at UCL, who works with individuals with global aphasia very, very closely. And so

37:32.920 --> 37:40.920
here we had two individuals with global aphasia, really serious issues, looking at pictures of

37:43.480 --> 37:49.640
swimmer by sharks, shark bite swimmer. And so as I mentioned earlier, they were laughing at the

37:49.640 --> 37:55.000
weird ones. And so in general, they were very good at distinguishing plausible and implausible

37:55.000 --> 38:03.480
pictures, suggesting that their ability to extract meaning from pictures was there, it did not

38:03.480 --> 38:10.280
require a functioning language network. And so then the title of the paper, the language network

38:10.280 --> 38:17.720
is recruited but not required for pictorial events and semantics. So we see this activation,

38:18.440 --> 38:23.800
and we, but it's not, it's not necessary to do the task.

38:25.480 --> 38:31.880
Yeah, yeah, I found that, yeah, very insightful. And what struck me was

38:33.480 --> 38:38.360
one of the hypotheses as to why the language network was recruited is that

38:38.360 --> 38:45.080
it's sort of another way of getting evidence or information on whether this event is likely or not.

38:45.880 --> 38:49.560
And of course, we can't be sure what's going on in there. But, you know, perhaps it is somewhat

38:49.560 --> 38:55.640
like a large language model where you, you look at the thing, you're like, part of trying to figure

38:55.640 --> 39:00.200
out whether this picture is likely or not is you kind of read it out to yourself and like, well,

39:00.200 --> 39:10.120
does this, is this a familiar pattern, right? Does it, is, you know, shark bites swimmer,

39:10.120 --> 39:15.800
you know, that's that's that sequence of words feels close to sequences of words that I produced

39:15.800 --> 39:22.680
before or read before, whereas swimmer bites shark is kind of jarring. And maybe behind that is just

39:22.680 --> 39:29.960
the improbability of that, that, that, that sentence being produced according to the language

39:29.960 --> 39:36.920
model in our, in our own minds. And of course, yeah, we, we don't really know that our minds work

39:36.920 --> 39:43.560
like a large language model at all, but it's an attractive hypothesis in as much as it works.

39:45.160 --> 39:51.080
Yeah, so I guess, so generally speaking, right, so we got this result language network recruited

39:51.080 --> 39:55.400
but not required. And the question was, what's going on, right? And so generally speaking,

39:55.400 --> 40:05.080
we consider two broad hypothesis. One is that that activation is not necessary to do the task.

40:05.080 --> 40:11.560
So you see the picture, sharks, swimmer and biting. And so you activate those words kind of

40:11.560 --> 40:18.360
automatically by association, but you're not actually using them to reason about whether

40:18.360 --> 40:24.040
the event makes sense or not. So that's one hypothesis. And the other hypothesis is that

40:24.040 --> 40:30.360
actually, the information in the language network is helpful. But when you're trying to

40:31.240 --> 40:37.160
recast information that you're seeing in linguistic form, you can then compare it with

40:37.160 --> 40:42.120
all of the linguistic information that you received in your lifetime. And maybe that

40:42.120 --> 40:47.720
information ended up distilled in your brain in some general way, just kind of, we know that

40:47.800 --> 40:52.760
people are very sensitive to statistical regularities in language, we know that we're very good at

40:52.760 --> 41:02.600
predicting what word would come next, right? Like there is this information about what patterns

41:02.600 --> 41:09.000
are likely in text is very much what people use during real life language comprehension.

41:09.800 --> 41:18.360
And of course, that information also can help us, in many cases, figure out which events

41:19.160 --> 41:25.560
make sense and which doesn't. And so actually, we try to test that hypothesis. We

41:27.320 --> 41:34.440
didn't necessarily, it's hard to test that in actual human brains, although we now have

41:35.160 --> 41:45.160
ideas for how we might be able to do that. But we started by using language models as proof of

41:45.160 --> 41:54.600
concept, right? So the hypothesis is statistical patterns in language input can help us distinguish

41:54.600 --> 42:01.480
plausible and implausible events. And language models are very good at capturing these statistical

42:01.480 --> 42:08.920
patterns. So if language models can systematically distinguish plausible and implausible events,

42:08.920 --> 42:14.600
that means that there is enough information there where maybe humans might be able to use that

42:14.600 --> 42:18.760
information also to distinguish plausible and implausible events, right? So it's not evident

42:18.760 --> 42:26.040
that humans do, but it's evident that humans can. And so we did that, we use language models and

42:26.040 --> 42:34.360
try to see whether they systematically evaluate plausible event descriptions as more likely than

42:34.360 --> 42:43.640
implausible. And so in that study, we specifically distinguished between two kinds of events. So

42:43.640 --> 42:52.280
one is the teacher bought the laptop versus the laptop bought the teacher. So animate, inanimate

42:52.280 --> 42:59.080
interactions. And so when you swap them around, if you interpret the sense verbatim and the inanimate

42:59.080 --> 43:07.320
object laptop cannot buy anything, buying requires that the subject is animate. And so that's a very

43:07.320 --> 43:13.960
kind of in your face screaming violation. And then the other example is kind of like the fox

43:13.960 --> 43:19.000
chased the rabbit, the rabbit chased the fox or the swimmer by the shark, the shark by the swimmer,

43:19.000 --> 43:24.760
right? The swimmer by the shark is not impossible. It can happen. It's just way less likely.

43:25.400 --> 43:30.120
And so what we found is that when it comes to distinguishing possible and impossible events,

43:30.120 --> 43:37.960
language models were very good, almost at ceiling. So that was actually very easy for them. But when

43:37.960 --> 43:44.120
it came to likely versus unlikely events, there was a gap in performance. So they weren't quite as

43:44.120 --> 43:49.880
good. They were okay. They were above chance. But they definitely weren't perfect. And so we're

43:49.880 --> 43:57.640
not as good as people, I guess, right? Yeah, not not as good as people and not as good as when they

43:57.640 --> 44:03.480
have to deal with animate inanimate sentence, right with impossible events. Yeah. So I think it's

44:03.480 --> 44:08.200
like, to me, it's actually more interesting to compare those two sentence types, like how models

44:08.200 --> 44:15.000
do on them. But also humans humans do well on both because these are easy sentences. So

44:15.000 --> 44:23.240
they're not meant to be challenging. And so, yeah. No, I was gonna say, and do we take that as evidence

44:23.240 --> 44:31.640
that then when humans reason about these things, they're doing it, not just linguistically, or is

44:31.640 --> 44:36.440
it that our language models are kind of like better than the language models that that are out there

44:36.440 --> 44:42.920
in, you know, the computer language, large language models? I think that I think it's

44:42.920 --> 44:48.120
evident that humans are doing it not linguistically. And so the reason why we think there is this

44:48.120 --> 44:56.200
performance gap is because actually, the language input that we receive doesn't faithfully describe

44:56.200 --> 45:04.120
the world around us. So when we talk to each other, we don't just passively describe everything

45:04.120 --> 45:09.880
that we're seeing. So I'm not telling you, you know, I am sitting down, the lights are on,

45:09.880 --> 45:16.520
the room is empty, like it's very boring stuff. I'm telling you about things that are unusual,

45:16.520 --> 45:23.640
novel, interesting, newsworthy in some way. And so this phenomenon is known as reporting bias.

45:24.280 --> 45:33.160
So language tends to undercover on the report information that is kind of trivial, right,

45:33.240 --> 45:39.480
that everybody already knows, or can reasonably infer. And so maybe actually events that are

45:39.480 --> 45:45.560
unlikely are not as unlikely for LLMs, because, you know, we talk about unlikely things all the

45:45.560 --> 45:50.200
time, that's the stuff that's worth talking about. And so if that's true, that's the reason why we

45:50.200 --> 45:55.800
see this performance gap, then even if the human language model is very good, which by the way,

45:55.800 --> 45:59.640
we don't think it is actually, I think large language models now are much better at predicting

45:59.640 --> 46:05.080
the next war that humans are. So actually, they're better. But even if humans were really good,

46:05.080 --> 46:13.320
just the language input is insufficient for us to be able to distinguish plausible and

46:13.320 --> 46:19.000
implausible events. That means that we have to use something else in addition, we have to maybe

46:19.000 --> 46:24.520
have some more sophisticated model of the world, where we can actually correct for this reporting

46:24.520 --> 46:33.000
bias, we have to also bring in information that's about what things are typical, what we

46:33.000 --> 46:39.160
can expect, what we cannot expect. So we're probably drawing on sort of multiple mental

46:39.160 --> 46:49.880
resources or systems. In the case of the clearly kind of impossible, so computer bias teacher,

46:50.840 --> 46:56.840
is it just the language, can you see if it's just the language, or have you seen if it was just

46:56.840 --> 47:01.080
the language network that's recruited there, as one might think, well, if it can just be done

47:01.080 --> 47:07.800
within the kind of the one region, maybe it's more efficient and metabolically, there might be some

47:07.800 --> 47:15.480
kind of preference for doing that if it were possible. By default, we kind of light up various

47:15.640 --> 47:23.880
regions just to make sure I don't know. So that's actually a study that I would love to do next,

47:23.880 --> 47:30.040
so this difference between impossible and unlikely events is something that emerged out of this

47:30.040 --> 47:37.320
language model study. And so now, of course, yeah, I think it would be great to bring it back to

47:37.880 --> 47:44.200
the MRI machine, measure people's brain activity in response to impossible versus

47:44.200 --> 47:49.720
unlikely centers, and see if the language network alone is sufficient for distinguishing

47:50.360 --> 47:55.160
possible and impossible events. That is the prediction that follows from this language

47:55.160 --> 48:00.600
model work. And so I would love to test that. Yeah, yeah, that would be so, yeah, I'd love to

48:00.600 --> 48:07.560
see the results of that. Yeah, so I hope that that happens. But I suppose, you know, coming back to

48:08.200 --> 48:16.280
LLMs, what we're starting to see is that maybe just a large language model in itself,

48:18.200 --> 48:28.520
for various reasons, might not be so effective at thinking or reasoning as the human brain. And

48:28.520 --> 48:36.520
one is, as you kind of mentioned, that the data that comes in is kind of biased toward the salient

48:36.600 --> 48:42.680
and newsworthy as you put it. But then another from the kind of Python example is that, well,

48:42.680 --> 48:51.720
as a matter of fact, we don't use the language part of the brain for code comprehension or for

48:52.280 --> 49:01.480
logical mathematical reasoning, either for that matter. I suppose my question there is, though,

49:02.040 --> 49:07.000
you know, could it be possible for LLMs to kind of just be

49:11.880 --> 49:19.240
be able to take on the functions of the multiple demands network, for instance, which is doing

49:19.240 --> 49:25.000
all this, which is the place which does the mathematical logical Python code interpretation

49:25.560 --> 49:31.720
comprehension? Could it kind of take on all those responsibilities just by having

49:33.400 --> 49:37.960
getting really good at saying, you know, next word prediction for mathematical problems and

49:37.960 --> 49:44.840
next word prediction for code generation and so on? Or is just that kind of, or is that implausible?

49:44.840 --> 49:52.840
I don't really know how we characterize, you know, where the LLMs just could have kind of

49:52.920 --> 49:58.120
emergently develop all those capabilities within a single language model, or if that's just

49:59.720 --> 50:12.200
very, very unlikely. Yeah, so LLMs do a bunch of different things. In general, as you mentioned,

50:12.200 --> 50:17.400
they're very, very good at pattern recognition and pattern completion at different levels of

50:17.400 --> 50:24.360
abstraction. So they do a lot of just direct memorization, right? The larger the model,

50:24.360 --> 50:30.600
the more texts that can just memorize straight up, which is why a lot of those copyright issues

50:30.600 --> 50:35.800
end up arising. But that's not the only thing that these models do, because they definitely

50:35.800 --> 50:42.760
are capable of generating novel texts and mixing and matching previous inputs. And so the patterns

50:42.760 --> 50:49.880
that they can recognize and reproduce, they can be fairly abstract. But then, of course, the question

50:49.880 --> 51:02.360
then is pattern completion all it takes? Is that the only thing that's necessary? And so that's where

51:02.360 --> 51:10.840
it gets tricky, because a lot of logical reasoning is algorithmic reasoning. It's symbolic. It's

51:11.480 --> 51:18.280
very regimented. And so these are the kinds of problems where these models seem to struggle.

51:18.280 --> 51:24.360
So for example, if you ask them to add and multiply two numbers together, if the numbers are small

51:24.360 --> 51:31.800
enough, then the model is doing just fine. But if the number is large, that means it wasn't part of

51:31.800 --> 51:36.920
the training set. It means it couldn't have just memorized the response, which it probably does for

51:36.920 --> 51:43.800
a lot of smaller number combinations. And so then it would actually have to multiply step by step.

51:43.800 --> 51:51.080
And it doesn't seem to be doing that very successfully. In fact, it often gives you a number

51:51.080 --> 51:58.680
that's close, but just a little bit off. And so the kinds of mistake that it's making is different

51:58.680 --> 52:03.400
from the kind of mistake a human would be making, because it's still trying to use pattern matching

52:03.480 --> 52:11.480
to complete. And it's not quite working, it seems. Yeah. But I mean, it's very hard to figure out

52:11.480 --> 52:16.520
exactly how they're doing what they're doing. I mean, they've got so many parameters. And it's

52:16.520 --> 52:26.360
surprising how good they are yet still imperfect at doing those sort of problems. They're kind of

52:26.440 --> 52:34.600
like a broken calculator. So they're much faster at getting to an answer, but it's not quite the

52:34.600 --> 52:41.320
right answer. It's a pretty good estimate often. And yet it's not completely out. So it's really...

52:43.160 --> 52:50.360
Yeah, I don't have a strong opinion, but part of me thinks, well, maybe they'll just kind of,

52:50.360 --> 52:57.080
with enough data going in, they might just crack that. That might come a point at which

52:59.720 --> 53:04.840
that kind of ability emerges. Although you point out in one of your papers, though, well,

53:04.840 --> 53:07.960
perhaps if that ability emerges, it might be that a particular kind of

53:08.760 --> 53:14.440
architecture that models the human brain emerges as well. So it may not be that...

53:14.680 --> 53:22.760
It might be happened in such a way that it becomes less fruitful to think of a large language

53:23.480 --> 53:29.640
model as simply a model of language, but something that has a kind of linguistic language network

53:29.640 --> 53:37.640
part like the human brain and then hands off to a logical part. And as it happens, obviously,

53:37.960 --> 53:44.600
in chat GPT, without that has had that kind of architecture imposed on it, at least in the

53:44.600 --> 53:49.080
version with the Python code interpretation, for instance, because you can say, well, add these

53:49.080 --> 53:52.360
two numbers together, and it will figure out, oh, well, I'm doing a math problem here. So I'm going

53:52.360 --> 53:56.840
to convert this into a Python problem, and then it runs the Python code. So actually,

53:58.120 --> 54:00.760
you know, some of these problems seem to be

54:01.080 --> 54:09.240
are being sort of addressed, I guess, by the developers. But the way they're doing it is,

54:09.240 --> 54:17.480
yeah, offloading. Yeah. Yeah. So I guess let me unpack a little bit. There's a lot there. So

54:17.720 --> 54:30.760
first of all, it is very tempting for people to over ascribe intelligence to a language model.

54:31.400 --> 54:38.200
And presumably that's because in our everyday interactions, we're used that language gets

54:38.200 --> 54:45.160
generated by a thinking feeling being other humans. And now we have a system which is breaking

54:45.240 --> 54:50.520
that relationship where we have something that generates coherence language that's not human.

54:50.520 --> 54:58.680
And so it gets confusing. And that's the reason why that's one of the reasons why

54:58.680 --> 55:03.480
there's so much hyper-intelligent language models, and they're expected to be the general

55:03.480 --> 55:10.280
intelligence models, because of this tight perceived relationship between language and thought.

55:10.280 --> 55:15.400
And so then when they make a math mistake, or they make a factually inaccurate statement,

55:15.400 --> 55:19.240
you're like, oh, no, like how, you know, these models are terrible, they're not

55:19.240 --> 55:23.720
terrible, they're just like not, they're just a totally different capacity you're evaluating.

55:23.720 --> 55:31.800
And so what we argue is that it's very important to distinguish different kinds of

55:31.800 --> 55:38.840
capabilities in these models. And so there is something that we call formal linguistic

55:38.920 --> 55:44.600
competence. And so that's the ability to generate coherent grammatical language.

55:44.600 --> 55:50.440
And that's something that in humans, the language brain network is responsible for.

55:51.560 --> 55:58.360
And then there is all of the other stuff that you need in order to actually use language in

55:58.360 --> 56:04.600
real life situation in interactions, you might want to ask somebody to close the door,

56:04.600 --> 56:11.640
you might want to tell somebody how you feel. And there are all kinds of situations that

56:11.640 --> 56:18.360
where you need to use language. But to do that, you actually need other capabilities,

56:18.360 --> 56:23.720
you need to be able to reason about social situations, you need to be able to know things

56:23.720 --> 56:28.760
about the world in order to generate actually accurate statements, you need to be able to reason

56:28.760 --> 56:33.640
logically and know some math if you want to solve a math problem. So even if that information is

56:33.640 --> 56:39.240
coming in as language, in order to be able to make sense of it, and also generate language that

56:39.240 --> 56:45.160
achieves a particular purpose, you need all of these other capacities, which broadly speaking,

56:45.160 --> 56:53.080
recall functional competence. And so different kinds of capabilities might suffer from different

56:53.080 --> 57:00.120
problems. And so we already touched upon a few. We touched upon the fact that mathematical reasoning

57:00.120 --> 57:06.760
and logical reasoning might require a different kind of algorithm. So instead of pattern matching,

57:06.760 --> 57:12.600
it might need to be more symbolic. And it's not fully clear whether the large language models

57:12.600 --> 57:21.560
today are capable of doing that. Maybe they are. But that's not necessarily in their default,

57:22.360 --> 57:27.480
in the default way they operate. So that's an open debate there. When it comes to world

57:27.480 --> 57:32.440
knowledge and knowing things about the world, distinguish implausible and implausible events,

57:32.440 --> 57:39.400
there a big problem is reporting bias and the fact that the training data that they have is biased.

57:39.400 --> 57:46.040
And so you might need to be able to build up a more general situation model, event model,

57:46.040 --> 57:51.640
that will not just take in the language that you receive, but also fill in some kind of commonly

57:51.640 --> 58:00.440
assumed things. If it's daytime, it slides out, stuff like that. And yeah, so different kinds

58:00.440 --> 58:07.160
of problems might require different kinds of solutions. A more general kind of potential solution

58:07.160 --> 58:18.600
that we advocate or talk about is modularity. So the fact that the brain is modular suggests that

58:18.920 --> 58:25.480
might be an efficient architecture. So a language process in module, the goal of the language

58:25.480 --> 58:33.080
network in the brain is not to reason, it's to get information that's expressed in fuzzy,

58:33.080 --> 58:39.240
imprecise words and extract meaning out of it. And then pass it on to relevant systems that can

58:39.240 --> 58:47.160
solve the math problem that can infer the social goal, all of that stuff. And presumably, for an

58:47.240 --> 58:52.120
artificial intelligence system, you might want to do something similar where language

58:52.120 --> 58:58.760
is not a replacement for thought, but is an interface to thought. And so in your example,

58:58.760 --> 59:05.320
right, you have a math problem, the language model translates it into code. It's very good at

59:05.320 --> 59:10.280
taking this broad like fuzzy natural language and translating into a more precise, symbolic

59:10.280 --> 59:14.600
representation. That's something that we didn't have at all, even a few years back. So it's a

59:14.600 --> 59:20.280
huge achievement. But then instead of trying to have that same language model to run the code,

59:20.280 --> 59:26.680
you're much better off passing it off to a code interpreter that will run the code and give you

59:26.680 --> 59:32.760
the answer. So the same kind of modularity that we see in the brain, that seems to be an effective

59:32.760 --> 59:38.280
way forward in the AI world that indeed some developers have started to adopt.

59:39.240 --> 59:47.240
Yeah. Yeah. And I think there's probably other ways in which the builders of these tools are trying

59:47.240 --> 59:53.320
to modularize. Like another one that comes up a lot is Rang or retrieval augmented generation, where

59:55.080 --> 01:00:00.600
yeah, there's some kind of database or just could just be a whole bunch of, you know,

01:00:00.600 --> 01:00:07.080
documents or whatever. And instead of hallucinating an answer, you want to make sure that you pick up

01:00:07.800 --> 01:00:14.280
something from one of those documents. And there's a whole different kind of machinery for that.

01:00:14.280 --> 01:00:21.160
But again, like in the code interpreter example, it's, I guess the language part is

01:00:22.440 --> 01:00:27.560
key, maybe less key in Rang because it's kind of a vector search. But it's a way, you know,

01:00:27.560 --> 01:00:33.560
it begins with translating language into something a bit more precise, in this case a vector instead

01:00:34.120 --> 01:00:41.640
of some code, I guess. And yeah, one wonders then if, you know, how close the parallels are between

01:00:42.680 --> 01:00:48.600
what is being built here and what's going on in the brain. You mentioned that, yeah,

01:00:49.240 --> 01:00:55.000
perhaps this is a good model for thinking about how we think. Language is this part where,

01:00:55.640 --> 01:01:02.280
this place where things kind of, you know, entry point for concepts, but the places where

01:01:02.360 --> 01:01:08.040
those concepts often get manipulated in terms of reasoning might be in other areas of the

01:01:08.040 --> 01:01:12.680
brain. They sort of become something more abstract than language itself.

01:01:16.200 --> 01:01:22.520
Yeah. Yeah. One thing I actually just slight tangent, but I do sometimes think that

01:01:24.440 --> 01:01:29.800
maybe language is being so associated with thought because it's kind of like

01:01:30.760 --> 01:01:36.760
the easiest thing to do, right? Like, you know, we know thinking is about concepts and some,

01:01:37.320 --> 01:01:43.880
you know, manipulating these things which are representations of the world. And language is

01:01:43.880 --> 01:01:49.160
just such an easy way of visualizing all of that, right, and understanding what's going on. But

01:01:49.160 --> 01:01:56.920
perhaps it's just the surface level of something much deeper that we really don't have an easy way

01:01:57.000 --> 01:02:04.040
of capturing. And, you know, that would map, I think, quite well to this kind of model of

01:02:04.680 --> 01:02:09.960
concepts being passed around, but the concepts themselves being, you know, beyond linguistics

01:02:09.960 --> 01:02:18.760
somehow. Yeah. So, as we mentioned, language is a system designed to communicate thoughts,

01:02:19.320 --> 01:02:25.320
concepts from one mind to another. And so, for this communication to be efficient,

01:02:25.320 --> 01:02:30.760
presumably language needs to parallel the structure of thought, the structure of concepts

01:02:30.760 --> 01:02:37.320
in some way, right? And so, it's much more abstract already than the raw perceptual input,

01:02:37.320 --> 01:02:43.800
than just audio, than just pictures, right? So, it kind of captures the relevant abstractions to

01:02:43.800 --> 01:02:50.200
a large extent. And so, that seems to be helping a lot. And so, that does bring us much closer to

01:02:51.000 --> 01:02:54.920
this more abstract conceptual representation. We're getting rid of a lot of extra details,

01:02:54.920 --> 01:03:01.000
we say cat, we don't care which color, which size is the cat. But, of course, at mapping

01:03:01.000 --> 01:03:05.720
between concepts and languages imprecise, we know that different languages partition the

01:03:05.720 --> 01:03:11.640
conceptual space in different ways, right? So, the words don't necessarily map the concepts one

01:03:11.640 --> 01:03:17.800
and one. Even within the same language, the same word can be used in many different contexts,

01:03:17.880 --> 01:03:25.720
in different ways, with different meanings. And so, that link is pretty fuzzy, can get pretty

01:03:25.720 --> 01:03:31.560
fuzzy. But it's definitely, I think you're right, when it comes to raw surface form,

01:03:31.560 --> 01:03:37.400
it's a very decent proxy, imperfect, but it makes sense why people are tempted to use it.

01:03:38.040 --> 01:03:43.400
Yeah. And, you know, in some ways, that means it makes what LLMs do so much more impressive,

01:03:43.400 --> 01:03:49.720
because they're also somehow capturing that surface form of concepts. Someone,

01:03:51.000 --> 01:03:58.280
a previous guest pointed out this wonderful quote from Ilya Sotskava saying, well, you know, if

01:04:00.920 --> 01:04:09.400
your LLM can predict the, you know, it's not just predicting text, because if your LLM can be fed

01:04:10.120 --> 01:04:17.800
the first part of a mystery novel that it's not read before, and it can tell you who the murderer

01:04:17.800 --> 01:04:24.520
was, it's not just predicting a word, it's somehow kind of understood what's going on in that story.

01:04:24.520 --> 01:04:31.720
Now, one of the difficulties, obviously, with all these things is, well, we don't know how

01:04:31.720 --> 01:04:37.640
open AIs LLMs are trained. So, it's very hard to test them, because you really need someone to write

01:04:37.640 --> 01:04:44.840
a new mystery novel to actually see if Ilya Sotskava's claim cashed us out. So, it's quite a

01:04:45.400 --> 01:04:51.480
high effort test. Unless, yeah, we happen to know of one which is definitely not in the corpus that

01:04:51.480 --> 01:05:02.040
was used. But, yeah, it does seem, you know, the fact that they are so good at mirroring what

01:05:02.040 --> 01:05:11.480
we produce, and that what we produce is somehow a good map onto something somewhat deeper, the world

01:05:11.480 --> 01:05:18.280
or an inner world. Yeah, it's so impressive. And you point out as well that it seems that,

01:05:19.080 --> 01:05:26.360
you know, the way that LLMs operate is very similar structurally to the way that

01:05:27.080 --> 01:05:35.400
our minds operate, in that, you know, it's not working on the raw audio or pixel forms of things.

01:05:36.040 --> 01:05:43.080
Like, the beauty of language is the compositionality at the level of small units, which are

01:05:43.720 --> 01:05:52.120
combinations of symbols or small sounds. And, yeah, the LLMs perfectly match that. So,

01:05:52.840 --> 01:05:59.400
we've built these things which really do capture something quite essential about how at least a

01:05:59.400 --> 01:06:05.400
part of our mind operates, it seems. And, yeah, maybe we've been seduced into thinking. That's

01:06:05.400 --> 01:06:13.720
all there is to thinking. Well, yeah, so in fact, well, that question, I guess I don't want to get

01:06:13.720 --> 01:06:18.760
too technical, but the question of what LLMs are starting with is actually an important one when

01:06:18.760 --> 01:06:27.480
we're trying to compare them with human minds or human brains. So, in fact, what LLMs operate over

01:06:27.480 --> 01:06:36.360
is tokens. So, it's chunks of characters that tend to occur pretty frequently in text. And so,

01:06:36.360 --> 01:06:41.320
oftentimes, they're words, like, though, but they're sometimes not words. If the word is long,

01:06:41.320 --> 01:06:48.440
it gets split up into multiple tokens. Yeah. And so, the problem is that those tokens actually

01:06:48.440 --> 01:06:54.600
don't match linguistic units that the word is actually made of, like morphine. They can be

01:06:54.600 --> 01:07:01.720
pretty arbitrary. And so, that does cause some differences between the way LLMs process them

01:07:01.720 --> 01:07:09.800
and humans do. In fact, people think that one reason why large language models are bad at arithmetic

01:07:09.800 --> 01:07:15.800
is because they tokenize numbers in weird ways, right? So, like, I don't know, 1618 is chunks

01:07:15.880 --> 01:07:20.760
in, like, 161 and then eight. And so, then it gets weird when they have to, like, add up the numbers.

01:07:20.760 --> 01:07:27.480
And so, that's where you get this weird, better-matching errors. And so, this kind of form is

01:07:27.480 --> 01:07:33.240
that, it's very engineering-driven. It's actually not, like, very rigorously scientifically based.

01:07:33.240 --> 01:07:38.680
And so, it's interesting, like, maybe if we change this little thing, it actually will result in much

01:07:38.680 --> 01:07:49.000
better performance. And so, it's funny how a lot of those choices are pretty random engineering-driven

01:07:49.000 --> 01:07:55.800
things. And, you know, they often work very well. But it's possible that with a small few tweaks,

01:07:55.800 --> 01:07:59.880
you can actually make the model much better. Yeah. No, I always thought that there was more

01:08:01.400 --> 01:08:06.600
sort of reasoning behind the n-grams that were used. But maybe, is it just kind of randomly

01:08:07.240 --> 01:08:13.240
chunks? Because I would have thought, well, there's some kind of, it makes sense to split

01:08:13.240 --> 01:08:21.480
words up, because, you know, particles like nus, if I think of, like, redness, right? It's not a

01:08:21.480 --> 01:08:27.000
word in itself, but it does attach to so many different words that it's sort of part of the

01:08:27.000 --> 01:08:34.360
compositional structure, I guess. But if it's getting chunked up is just two s's, right? And not

01:08:34.440 --> 01:08:41.320
nus, then it's kind of odd, yeah. No, that's exactly right, because nus is a morpheme,

01:08:41.320 --> 01:08:46.280
it's a suffix with a particular meaning. And so, if redness is chunked into red and nus,

01:08:46.280 --> 01:08:51.560
that makes a lot of sense, and it's linguistically justified. But oftentimes,

01:08:51.560 --> 01:08:56.920
that's not how the chunking happens. That's where the mismatch arises. So, you can definitely

01:08:56.920 --> 01:09:01.880
have the two s's in principle. Okay, interesting. Yeah, it seems like, yeah,

01:09:02.360 --> 01:09:08.840
one would think that with a bit of curation, maybe they could be even more effective. And yeah, it's

01:09:08.840 --> 01:09:14.440
hard to imagine them being more effective in terms of producing language. But perhaps that's

01:09:14.440 --> 01:09:21.480
just because they've been fed such a, such copious amounts of data that they sort of these,

01:09:22.600 --> 01:09:28.840
you know, they could be more efficient, right? Well, the stalker has an algorithm,

01:09:29.720 --> 01:09:36.120
it's kind of the goal is for it to be universal and that driven, right, without human curation,

01:09:36.120 --> 01:09:43.160
which is why the morphes don't get respected all the time. It causes a lot of issues for languages

01:09:43.160 --> 01:09:51.320
that aren't based on the Roman alphabet. So, let's say Arabic, for example, it ends up getting

01:09:51.320 --> 01:09:56.840
tokenized at the character level, because the tokenizer is just not adapted to deal with it.

01:09:56.840 --> 01:10:00.920
And so that does mean that performance on these languages that are not

01:10:00.920 --> 01:10:07.080
Roman alphabet based is actually worse, often substantially worse. It's generally a problem

01:10:07.080 --> 01:10:13.800
that like the fewer, the less data a language has, the worse the performance in that language.

01:10:13.800 --> 01:10:18.200
Some of the more general information seems to get pulled across different languages,

01:10:18.200 --> 01:10:23.400
which is cool. But a lot of language specific stuff, like grammar, right, of course depends

01:10:23.400 --> 01:10:28.680
on how much data you have in that language. But a particular distinction that tends to

01:10:28.680 --> 01:10:33.400
matter beyond just the amount of data is which alphabet. And so because so many of these morphs

01:10:33.400 --> 01:10:40.600
are English centric, a lot of other languages get left behind. Yeah, interesting. And to one

01:10:40.600 --> 01:10:46.280
extent, I mean, I know there are techniques for doing this. So you spend, you know, a lot of

01:10:46.280 --> 01:10:50.200
experiments looking into the minds or the brains of people.

01:10:52.200 --> 01:10:59.000
There are tools which allow us to do this to an extent with LLMs. But, you know, how effective

01:10:59.000 --> 01:11:05.000
are they? How does it compare to looking at an MRI, trying to understand what's going on inside

01:11:05.000 --> 01:11:11.400
of an LLM, what concepts it has, or what's lighting up as it is given a prompt?

01:11:12.200 --> 01:11:19.080
Yeah. So I am fascinated, honestly, by how many parallels there are between studying biological

01:11:19.080 --> 01:11:26.760
intelligence and humans and artificial intelligence. And for me, the first similarity is really just

01:11:26.760 --> 01:11:33.080
starting at the behavioral level. So developing separate experiments to look at formal competence

01:11:33.080 --> 01:11:37.080
like grammar, functional competence, like reasoning, these are methods from cognitive

01:11:37.480 --> 01:11:42.280
science, how do we design good experiments, how do we disentangle different contributors to

01:11:42.280 --> 01:11:47.560
performance. So even before we start looking inside the model or inside the brain, just looking

01:11:47.560 --> 01:11:54.760
at how humans behave and how models behave can tell us a lot about potentially how they do it,

01:11:54.760 --> 01:11:59.960
what kind of mistakes they make, what does it tell us about the potential mechanism that they're

01:12:00.600 --> 01:12:09.000
using to solve the task. But then, of course, we can get even more insight by looking at the

01:12:09.000 --> 01:12:14.040
actual mechanisms or their neural correlates. So for humans, that means looking inside the brain.

01:12:14.040 --> 01:12:22.600
And for models, that means looking inside the model. And so the movement that is getting seen

01:12:22.600 --> 01:12:28.840
currently the mechanistic interpretability movement in AI is doing that, essentially,

01:12:28.840 --> 01:12:36.440
they're asking which circuits, which units inside the network are responsible for a particular

01:12:36.440 --> 01:12:45.720
behavior. And so they first try to identify those units that get particularly engaged in a task.

01:12:45.720 --> 01:12:50.520
Maybe they respond differently to plausible sentences compared to implausible sentences.

01:12:51.160 --> 01:12:57.720
And then the beauty of having an artificial system is that they can actually go and

01:12:57.720 --> 01:13:03.320
manipulate it directly. So you can knock out that circuit or you can replace activations from one

01:13:03.320 --> 01:13:08.040
sentence with activations from another sentence. So in neuroscience, people sometimes do that as

01:13:08.040 --> 01:13:13.320
well. In animal research, for example, or there are certain kinds of stimulation that you can do

01:13:13.320 --> 01:13:21.960
that aren't harmful, but can maybe do the desired effect. In aphasia studies, these are natural

01:13:21.960 --> 01:13:26.520
causal experiments, right? We didn't cause delusion that destroyed the language network.

01:13:26.520 --> 01:13:34.360
But because we see those cases occur naturally, we can look at those effects. And so the causal

01:13:34.360 --> 01:13:41.560
tools are really powerful because they can really help us to see whether this part of the circuit is

01:13:41.560 --> 01:13:50.360
necessary for the behavior that we observe. And so in AI systems, we can do that quite easily.

01:13:50.360 --> 01:13:57.000
But conceptually, I would say in neuroscience and in AI, what we're trying to find out is very

01:13:57.000 --> 01:14:03.000
similar. Yeah. Yeah. And it's, I mean, it's wonderful, as you say, at least with the behavioral point,

01:14:03.640 --> 01:14:10.440
you can draw on the same kind of experiments that, you know, we finally have a kind of

01:14:10.440 --> 01:14:13.880
artificial intelligence that you can feed the same sort of things that you'd feed a person,

01:14:14.200 --> 01:14:18.760
i.e. sentences. And so it makes it very natural to run those kind of experiments.

01:14:20.200 --> 01:14:26.360
But then on the other hand, you can also go into the thing itself and tinker it with it in a way

01:14:26.360 --> 01:14:31.720
which would be very unethical and, you know, even just impossible with a person. So you could,

01:14:31.720 --> 01:14:38.280
I think there was one example where you had, I don't know, the concept of or Berlin was replaced

01:14:38.680 --> 01:14:45.560
with Paris or, no, what was it? It was Rome. Was it the Eiffel Tower was placed in

01:14:46.600 --> 01:14:49.960
conceptually into Rome or something like this? And you asked, well, how do you get from

01:14:51.400 --> 01:14:56.760
Berlin to the Eiffel Tower? It wasn't me, but it was, yeah, it's a famous kind of editing study.

01:14:57.800 --> 01:15:01.720
Yeah, I think I must have read it in one of your papers referring to it.

01:15:02.360 --> 01:15:14.440
And so the LLM does really, it kind of responds in the way that you would think if what's going

01:15:14.440 --> 01:15:19.480
on is that it has some kind of model of the world. And what all you've done is kind of

01:15:19.480 --> 01:15:23.720
switch around some pieces inside that model. It's not that it gets completely, you know,

01:15:23.720 --> 01:15:29.080
it doesn't throw everything completely out of whack, I suppose. And it even kind of

01:15:29.160 --> 01:15:32.760
infers some things that, you know, the Eiffel Tower will be in the center of Rome and

01:15:32.760 --> 01:15:36.440
it's going to be up with the Coliseum or something like that, which is, yeah,

01:15:37.080 --> 01:15:45.080
yeah, it's so fascinating to have something where we can kind of, you know, plausibly

01:15:45.080 --> 01:15:51.560
peer in into the internal workings. And yet just like the human brain, everything is

01:15:51.560 --> 01:15:57.320
so complicated that actually also it's not a trivial task, I guess.

01:15:57.640 --> 01:16:02.680
No, but that's the benefit, I guess, when neuroscientists have, we're used to dealing

01:16:02.680 --> 01:16:09.480
with this complexity. And, you know, there are ways to zoom out beyond just each individual

01:16:09.480 --> 01:16:16.280
neural unit to try and look at general trends and general patterns. And so I think a lot of

01:16:16.280 --> 01:16:21.320
people are daunted by the task of trying to understand the neural net because it's so big

01:16:21.320 --> 01:16:26.440
and complex. And because it's trained in this way where we don't necessarily know which features

01:16:26.440 --> 01:16:32.200
it sticks up on. But to me as a researcher, I'm just excited. It's like a cool puzzle to solve

01:16:32.200 --> 01:16:36.440
and a cool problem to understand. So generally, I'm pretty optimistic about this endeavor.

01:16:37.240 --> 01:16:43.400
Cool. Yeah. I think you mentioned at the very beginning that your research is now starting

01:16:43.400 --> 01:16:48.280
to look at some of the, you know, possibly trickier question of this kind of reflexive

01:16:49.080 --> 01:16:54.040
thinking, the narrow type of thinking I think you mentioned. So we've been talking a lot maybe

01:16:54.040 --> 01:17:01.400
about the broader definition of cognition of just kind of reasoning, manipulation of concepts,

01:17:01.400 --> 01:17:05.080
which might, one might even do in a very automatic way, as we were saying, like you might just solve

01:17:05.080 --> 01:17:10.440
a mass problem without really, you know, in a way where you'd say, oh, yeah, I didn't think about

01:17:10.440 --> 01:17:19.000
that. I just did it. But yeah, how does one, what kind of things have you, how can you pick,

01:17:19.000 --> 01:17:22.520
how can you look at this other problem of like when, when people kind of cogitate about things

01:17:22.520 --> 01:17:27.560
and turn them over in their, in their minds? Where are you going with that? I'm really curious.

01:17:29.720 --> 01:17:37.160
I think to me, the interesting question here is the question of individual differences. If

01:17:38.040 --> 01:17:43.720
some people report thinking in words most of the time and others say they don't think in words at

01:17:43.720 --> 01:17:48.920
all, presumably we should be able to see that at the brain level. Presumably we should be able to

01:17:48.920 --> 01:17:54.360
see the language network working hard for the first group and not at all for the second group

01:17:55.400 --> 01:18:01.160
while they're thinking right spontaneously in this task-free setting. And so that's really what I

01:18:01.160 --> 01:18:08.680
want to look at. But in order to do that, we need to have a good questionnaire that will capture

01:18:08.680 --> 01:18:15.000
those differences precisely, right? So I think instead of just asking people, although you think

01:18:15.000 --> 01:18:19.640
in words a lot, they're a little, it would be helpful to think, to get more information, right?

01:18:19.640 --> 01:18:30.200
Do they think in like, what does it mean? Like, what if other meta assessments of their own

01:18:30.200 --> 01:18:36.680
thinking style is reliable, right? So like, can we trust those judgments? How can we make them

01:18:36.680 --> 01:18:42.760
more granular? Another question that I'm very interested in, and that's really understood

01:18:42.760 --> 01:18:51.400
currently is, is there a difference between thinking in words and hearing the words, right?

01:18:51.400 --> 01:18:56.760
So if you're using some kind of words and some kind of language to think, does it mean that

01:18:56.760 --> 01:19:03.320
there is a voice or not necessarily? Some people, it turns out, they might see the words written

01:19:03.320 --> 01:19:11.320
in their mind's eyes, so they spell it out. It's a minority, like less than half of the population,

01:19:11.400 --> 01:19:16.040
but it does happen. And the capturing those differences, I think, is fascinating and then

01:19:16.040 --> 01:19:21.400
trying to look at the neural correlates to essentially establish the validity of those

01:19:21.400 --> 01:19:27.480
differences to show that they're really not just something that people perceive and report,

01:19:27.480 --> 01:19:35.000
but actually, that's not necessarily how they actually think. It's an interesting direction

01:19:35.000 --> 01:19:45.800
because psychology has this interesting history of an interesting relationship with phenomenology.

01:19:45.800 --> 01:19:51.400
So the people reporting their own experiences, right? That used to be very common, and then it

01:19:51.400 --> 01:20:01.000
turned out to result in a lot of pseudoscience and discredited a lot of psychology. And so then

01:20:01.000 --> 01:20:06.600
there was this huge turn to behaviorism where all that mattered was the stimulus and the response,

01:20:06.600 --> 01:20:14.600
and people were refusing to talk about any internal operations at all. So people are still

01:20:14.600 --> 01:20:21.160
very suspicious of phenomenology, so self-reporting experiences. And I think for the right reason,

01:20:21.160 --> 01:20:25.320
because often, yeah, we just don't know how we think. We're like, I think it's words or I think

01:20:25.320 --> 01:20:31.320
it's not. Sometimes we'll make a decision, like we were saying very quickly. And then when we have

01:20:31.320 --> 01:20:37.800
to explain what we did and how, we have to rationalize it. And so maybe that's actually not how we

01:20:37.800 --> 01:20:42.920
arrived at the decision, but post-talk, we come up with an explanation that might not correspond

01:20:42.920 --> 01:20:48.760
to the reality. So I think we'll have to be careful when taking people at their words.

01:20:49.880 --> 01:20:54.760
But to me, when people report this strike in differences of like, oh, yeah, I think in words,

01:20:54.840 --> 01:21:00.920
all the time versus like, never, not at all, it seems like there's something there and so I would

01:21:00.920 --> 01:21:05.720
love to use neuroscience to get at that question more deeply. Yeah, it's a tricky one. I mean,

01:21:05.720 --> 01:21:09.160
it strikes me that even the process of asking someone, do you think in words,

01:21:10.200 --> 01:21:16.440
it almost necessarily linguistic to communicate that because as we say, this is the way that we

01:21:16.440 --> 01:21:23.080
pass ideas around. And so maybe maybe there's just like that kind of arrogance in the language

01:21:23.080 --> 01:21:28.440
network, which is going to intercept that question and say, oh, yes, it's me. I do all the thinking.

01:21:29.320 --> 01:21:37.000
But as you say, well, many people do report thinking in many other ways. So yeah, I would,

01:21:37.640 --> 01:21:43.880
yeah, I'm really curious about what that shows. I mean, it's just so, this must surprise you all

01:21:43.880 --> 01:21:49.640
the time just how, you know, outwardly, we sort of walk around and we move around and we breathe

01:21:49.720 --> 01:21:55.480
and we have all our organs are, you know, working in pretty similar ways. And yet,

01:21:55.480 --> 01:22:01.880
internally, it might be, you know, we seem so heterogeneous, I guess. Does that sound about

01:22:01.880 --> 01:22:07.160
right? Or am I overstating the kind of differences in brains that we have?

01:22:09.880 --> 01:22:13.800
I don't know. I think, yeah, it just depends on your intuition about, you know, how much

01:22:13.800 --> 01:22:17.240
similarity and differences you would expect. Of course, our personalities are very different,

01:22:17.240 --> 01:22:21.160
right? Our likes and dislikes are interesting. So at the cognitive level, there are lots of

01:22:21.160 --> 01:22:28.280
differences between people, of course. And so I guess the interesting thing is that we have

01:22:28.280 --> 01:22:34.440
this huge differences in how we perceive our own thinking, but they don't necessarily manifest

01:22:34.440 --> 01:22:41.400
very obviously in differences in this year, right? So in addition to differences in inner

01:22:41.480 --> 01:22:49.000
speech, another common example is differences in mental imagery, right? So it turns out that some

01:22:49.000 --> 01:22:57.480
people never experience visual images in their mind's eye. When they're asked to imagine a red

01:22:57.480 --> 01:23:04.120
apple, they will think about the concept of an apple and like redness, but they will not like

01:23:04.120 --> 01:23:11.000
see a red apple in front of them when they close their eyes. And so that phenomenon has a name,

01:23:11.080 --> 01:23:18.280
aphantasia. And the name got coined in 2015. So very recently, really. And this is the phenomenon

01:23:18.280 --> 01:23:23.640
that kind of got discovered over the centuries at various times and then forgotten again,

01:23:23.640 --> 01:23:28.440
and rediscovered because again, people just tend to assume that everybody else has the same

01:23:28.440 --> 01:23:34.120
roughly inner experience with them. And so those differences just end up getting neglected.

01:23:35.560 --> 01:23:40.120
But it turns out that people with aphantasia, you know, again, you cannot tell them apart very

01:23:40.120 --> 01:23:45.800
easily from people with this visual imagery. So it turns out that lots of things we do in the world,

01:23:46.680 --> 01:23:51.640
you can do whether or not the experience images visually. Similarly, whether you have strong

01:23:51.640 --> 01:23:58.360
inner speech or not, turns out it's you can't really spot these people very easily out in the

01:23:58.360 --> 01:24:03.000
wild because they act very differently. So that's the interesting thing, right? Despite these

01:24:03.000 --> 01:24:08.920
experiences being so different, somehow we can still act in roughly similar ways and do the

01:24:08.920 --> 01:24:13.880
tasks that we need to do in the world. We might be using different strategies, it's very possible,

01:24:13.880 --> 01:24:18.760
but the end result is that actually, those differences are very hard to see.

01:24:19.560 --> 01:24:24.680
Yeah, yeah, that is fascinating. Yeah, actually, I have a friend who is an aphant, I guess.

01:24:25.800 --> 01:24:33.560
And well, he didn't find out until a few years ago. And we, you know, there's no kind of outward

01:24:33.560 --> 01:24:39.960
sign, right? You just seem completely, you know, normal. But then we're like, oh, yeah, I just

01:24:39.960 --> 01:24:44.680
can't visualize triangles, right? I know what a triangle is, I can reason about triangles, I can.

01:24:46.120 --> 01:24:50.440
And actually, often there's, I think there might be some research which shows that that in some ways,

01:24:51.640 --> 01:24:57.080
oftentimes better at reasoning about certain things, where one might think it requires a

01:24:57.960 --> 01:25:04.040
visual element. But yeah, he was, you know, a very good physicist, very good colleague,

01:25:05.800 --> 01:25:08.280
but just thought in a different way, I guess.

01:25:12.120 --> 01:25:17.960
Yeah, I think we'll find some differences, right? Like, now that there's more awareness,

01:25:17.960 --> 01:25:22.200
once we start doing more systematic research, I think we'll, like, I mean, there are already

01:25:22.200 --> 01:25:28.200
attempts, trying to look at the relationship between aphantasia and episodic memory,

01:25:29.000 --> 01:25:35.960
turned out that aphantasia and, yeah, spatial reasoning, geometric reasoning there, the link

01:25:37.080 --> 01:25:43.640
is not as strong and may or maybe not even there, even though people expected it to be.

01:25:43.640 --> 01:25:47.560
And there are variations as to why. But yeah, essentially, like, I think,

01:25:47.560 --> 01:25:51.320
even though those differences aren't apparent, I think we'll find some eventually.

01:25:51.320 --> 01:25:55.160
And probably we'll just find out that different people are using different strategies to do the

01:25:55.160 --> 01:26:00.680
same thing. Some of them might require imagery or thinking in words, you know, speech, and some

01:26:00.680 --> 01:26:10.360
might not. I mean, all this is a reminder that well, LLMs might be, we might end up producing

01:26:10.360 --> 01:26:17.080
artificial intelligences, which outwardly look very similar. But we shouldn't, or yeah, we should

01:26:17.080 --> 01:26:23.000
be careful to think that to be mindful that inwardly, they could be very, very different.

01:26:24.440 --> 01:26:31.080
And I think it's very true, and it's already happening, right? There are all those cases where

01:26:31.960 --> 01:26:39.000
people were screenshotting chat GPT responses, especially right after it came out a year ago,

01:26:39.000 --> 01:26:44.360
and just showing it responding to some very complicated prompt and doing it correctly,

01:26:44.440 --> 01:26:48.120
and people were so impressed being like, oh, you know, if you have, if you put like,

01:26:49.960 --> 01:26:57.240
a mail on a table, like, you know, like on on a chair, is that like a construction

01:26:57.240 --> 01:27:03.000
stable or not? Or, you know, what kind of thing we can put on top? And like, it looks very impressive.

01:27:03.000 --> 01:27:09.080
And then it turns out that if you change the problem just slightly, then it just starts

01:27:09.080 --> 01:27:16.360
spitting out total nonsense. And the same thing happened with a social reasoning problem, kind

01:27:16.360 --> 01:27:20.680
of predicting what the other person would think, which is a classical problem from psychology.

01:27:20.680 --> 01:27:26.440
And so the claim was that, you know, now LLMs can reason about people and what they do. And then,

01:27:26.440 --> 01:27:31.480
again, at third dollar, we change the prompt to be slightly different from the problems that

01:27:31.480 --> 01:27:38.120
were already available on the internet, then model performance drops drastically. So it's very

01:27:38.120 --> 01:27:46.280
easy to fall for this seemingly impressive performance, seemingly seemingly impressive

01:27:46.280 --> 01:27:53.960
understanding. And so luckily for us, in this case, there are ways to design even behavioral

01:27:53.960 --> 01:27:59.960
interventions that can maybe help us figure out what's actually going on and what strategy is

01:27:59.960 --> 01:28:05.560
actually being used. Yeah, yeah, that's a very important problem for sure. I found the social

01:28:05.560 --> 01:28:11.800
reasoning example really, I just loved it. So I think if I remember correctly, one of the ways

01:28:11.800 --> 01:28:18.360
that you fall them is just inserting a few words in the middle. So the classic, the classic one

01:28:18.360 --> 01:28:25.000
is something like, you know, Susie hides Bob's apple. It was in the closet. Where does he think

01:28:25.000 --> 01:28:31.080
the apple is? And no, the thing will think it will say, Oh, not in the closet anymore.

01:28:31.800 --> 01:28:35.800
Correct. But if you change it, say, well, Susie hides Bob's apple, it was in the closet,

01:28:35.800 --> 01:28:41.400
and she tells him that she hid it, right? And then the LLM still says, Oh, he thinks it's not in

01:28:41.400 --> 01:28:50.040
the closet because it and I think, yeah, so clearly they've been either on the training set or in

01:28:50.040 --> 01:28:57.240
the fine tuning, you know, it's gone so far in getting them to kind of give the appearance of

01:28:57.240 --> 01:29:02.040
having some kind of theory of mind and being able to solve those problems. But then with a little

01:29:02.040 --> 01:29:08.040
bit of tweaking, it becomes apparent that they they don't. But it seems to be coming harder and

01:29:08.040 --> 01:29:18.040
harder to, you know, fox these systems. And you point out that one of the real difficulties is

01:29:18.040 --> 01:29:24.200
we just, you know, without knowing what has gone into the training of open AI's models and

01:29:24.520 --> 01:29:32.680
it, it's hard to know how much genuine kind of intelligence has emerged and how much is just

01:29:32.680 --> 01:29:39.160
kind of pan recognition and, and, you know, or simple pan recognition and recall. But of course,

01:29:39.160 --> 01:29:43.480
the paradox here as well, they have some of the most advanced models. So maybe there's something

01:29:43.480 --> 01:29:49.720
genuinely interesting going on, but it's black box. So we can't really say. But I think it's

01:29:49.720 --> 01:29:57.880
encouraging that other folks are, you know, mistrial perhaps are being more open and about

01:29:57.880 --> 01:30:05.160
what's going into that into their models. So, you know, maybe easier to, to test on the very

01:30:05.160 --> 01:30:11.800
latest things and have confidence that I don't know, some problem was not appearing verbatim in

01:30:11.800 --> 01:30:15.960
in the training set. But even then, yeah, it could be some very similar problem.

01:30:16.280 --> 01:30:23.960
Yeah, for sure. And like, it's not always bad if they've been fine tuned on this problem. And if

01:30:23.960 --> 01:30:29.880
they've seen examples before, as humans learn lots of things, we can do math right off the bat,

01:30:29.880 --> 01:30:34.920
we learn, we learn from examples. It's easier for us to the problems that are familiar to us and

01:30:34.920 --> 01:30:41.160
novel ones, like, that's fine. It's just important to know, because that helps us figure out what

01:30:41.160 --> 01:30:46.600
is the mechanism that they're using, or potentially using, and what are the keys is where they might

01:30:46.600 --> 01:30:54.040
break, right? So it's, we don't necessarily need to expect these models to be like amazing,

01:30:54.040 --> 01:30:59.880
zero shot thinkers on total and novel problems all the time. It's just that, yeah, knowing

01:30:59.880 --> 01:31:03.240
what goes in the training data, knowing what problems they've been fine tuned on,

01:31:03.240 --> 01:31:08.840
just helps us assess them more accurately. Yeah, yeah, that's a good point. It's unfair to

01:31:09.800 --> 01:31:13.800
sort of demand development so that they don't learn anything, or they don't benefit from fine

01:31:13.800 --> 01:31:22.680
tuning. But yeah, but we want to know if they are, if the way that they're responding is using,

01:31:23.960 --> 01:31:27.960
is that they've abstracted patterns, or they've just, they're just regurgitating.

01:31:30.360 --> 01:31:36.840
Yeah, well, I, yeah, I found it really interesting to go through your work. I'm usually,

01:31:37.800 --> 01:31:45.160
I don't know, optimistic, I guess, or maybe not, maybe that's the wrong word. I really admire how

01:31:45.160 --> 01:31:53.720
LLMs are, have taken off, and it surprised me how quickly they've advanced. But one thing I've

01:31:53.720 --> 01:31:58.280
enjoyed about your work is kind of reminding me, well, actually, maybe they're not as far along in

01:31:58.280 --> 01:32:03.960
some ways as, as they appear to be at the surface level. Like there still seems to be some,

01:32:04.920 --> 01:32:09.640
some nuts to crack here to make them, to bring them closer to, to human thought.

01:32:11.000 --> 01:32:18.120
What's your take? Do you think, you know, I don't want to ask the typical how far away is AGI,

01:32:18.120 --> 01:32:27.240
because, but, you know, are you of the opinion that just cranking through more data

01:32:27.880 --> 01:32:35.960
is going to continue to produce results, or should more be invested in this kind of modularity

01:32:36.600 --> 01:32:45.960
approach? And if the latter, well, do we have, you know, are the things that are taking place

01:32:45.960 --> 01:32:50.920
on the right track, or do we need to look more closely and learn more from the human mind, perhaps?

01:32:51.000 --> 01:33:02.120
I think we definitely should recognize all the impressive achievements that we observe in LLMs

01:33:02.120 --> 01:33:09.640
today. And one way that my colleagues and I have been thinking about it is through the formal

01:33:09.640 --> 01:33:16.120
functional competence lens. On the formal competence front, learning the rules and patterns of natural

01:33:16.120 --> 01:33:24.120
language. These models have been incredibly impressive. So even a couple of years ago,

01:33:24.120 --> 01:33:28.440
they almost reached the ceiling for English, at least the language where they have the most data.

01:33:29.160 --> 01:33:36.120
And they did it without the need for any fine tuning. So just learning to predict words and

01:33:36.120 --> 01:33:43.880
method corpora of text. Turns out that is something that gives you all the most of the grammar and

01:33:44.520 --> 01:33:51.400
knowledge of idioms and all kinds of patterns that characterize a language. And that wasn't trivial

01:33:51.400 --> 01:33:56.760
at all. That was the subject of debate for linguists for decades. Is it possible to just learn from

01:33:56.760 --> 01:34:03.400
data? Or do you need a rich set of internal rules that can help you figure out what's grammatical

01:34:03.400 --> 01:34:09.480
and what's not? So that is incredibly impressive scientifically. And on the engineering front,

01:34:10.360 --> 01:34:16.280
different language processing systems in the past have struggled so much because you just can't

01:34:16.280 --> 01:34:22.280
encode language with a few simple rules or even not simple rules. Just like so fuzzy and there are

01:34:22.280 --> 01:34:26.280
so many exceptions in the regular forms and this and that. And the fact that these models have

01:34:26.280 --> 01:34:30.120
mastered that is so impressive. And people kind of forget and start talking about AGI right away,

01:34:30.120 --> 01:34:34.840
but that's an impressive achievement. Then being good at language is already very impressive.

01:34:35.800 --> 01:34:40.760
And then we get to functional components and their ability to reason and be factually accurate,

01:34:40.760 --> 01:34:45.320
know what's true and what's false and be actually helpful. And so that's a whole other host of

01:34:45.320 --> 01:34:53.880
problems where they actually seem to be spotty. They have achieved a lot because of pattern

01:34:53.880 --> 01:34:58.520
recognition, but then it turns out that that performance is not robust and it breaks. And

01:34:58.600 --> 01:35:08.440
so that's where it gets more complicated and more controversial. And that's where we argue modularity

01:35:08.440 --> 01:35:15.400
will be helpful. Again, looking at the human brain as an example. And one distinction we make though

01:35:15.400 --> 01:35:24.200
is that the modularity doesn't necessarily have to be built in by design. So this built-in approach

01:35:24.200 --> 01:35:29.720
we call architectural modularity where we have a language model and let's say a path and code

01:35:29.720 --> 01:35:33.640
interpreter and we put them together and they're clearly different and they're doing different

01:35:33.640 --> 01:35:39.160
things. So that can be promising, but then of course you need to know what the right modules are,

01:35:39.160 --> 01:35:45.080
you need to set them up in the right way. An alternative approach that might work for certain

01:35:45.080 --> 01:35:51.240
cases is what we call emergent modularity where you start out with one network, but you don't

01:35:51.320 --> 01:35:56.200
necessarily specify what parts need to be doing, what you let the network figure that out over the

01:35:56.200 --> 01:36:01.000
course of training and you can have different parts self-specialized to do different things.

01:36:01.640 --> 01:36:06.600
That might require some changes to the architecture to be able to promote the kind of

01:36:06.600 --> 01:36:12.360
specialization. It might require changes in the objective function, maybe next work prediction

01:36:12.360 --> 01:36:21.080
alone is not necessarily going to be good. And it might require changes in the training data

01:36:21.240 --> 01:36:28.120
kind of like what's happening with fine tuning today where you are feeding its specific problems

01:36:28.120 --> 01:36:33.000
that you're asking the model to do in a specific way so you might have selectively boost the

01:36:33.000 --> 01:36:37.480
social reasoning and the formal reasoning and the factual knowledge. There might be specific

01:36:37.480 --> 01:36:45.080
things you need to do, but there is a lot of promise in these approaches and the paper where

01:36:45.080 --> 01:36:51.160
we introduced the formal and functional competence, it's something we started working on in 2020

01:36:51.480 --> 01:36:57.720
around the start of the pandemic. Language models were around then, but not nearly as advanced and

01:36:57.720 --> 01:37:03.880
as we were writing the paper and in fact after the initial preprint version came out, that's when

01:37:03.880 --> 01:37:10.280
we started seeing the field, the developers shifting away from this simple scaling up approach,

01:37:10.280 --> 01:37:15.320
that's not the approach that's common anymore. People have started to shift towards specialized

01:37:15.320 --> 01:37:21.320
fine tuning, using very targeted data sets to improve performance on specific domains,

01:37:21.320 --> 01:37:26.440
coupling an LLM with external modules, all of those things that we kind of suggested that might

01:37:26.440 --> 01:37:31.320
be good because that's more brain-like. RAG became big, all of those things are something we've seen

01:37:31.320 --> 01:37:38.440
over the past year that's very encouraging that now the AI field is also recognizing that it's

01:37:38.440 --> 01:37:42.200
not just about scale, that you do benefit from different components working together.

01:37:43.160 --> 01:37:53.480
Yeah, I think very exciting to see what comes next, both in your work and in the field of LLMs,

01:37:53.480 --> 01:37:59.480
which it seems like maybe someone's listening to what you're suggesting because all these things

01:37:59.560 --> 01:38:09.400
are happening. Yeah, I don't know if you have any final comments or predictions,

01:38:11.960 --> 01:38:22.840
warnings of doom often come up in these discussions, but this has been surprisingly positive.

01:38:23.160 --> 01:38:30.520
No, I just think that science is important and we just need to use good methods and not run after

01:38:30.520 --> 01:38:36.840
the hype and be realistic in how we evaluate the strengths and limitations of these models.

01:38:37.720 --> 01:38:41.720
There are strengths, there are limitations, so being too far on just the positive and just

01:38:41.720 --> 01:38:46.360
negative is not necessarily the most productive. We just want to be able to disentangle them

01:38:46.360 --> 01:38:53.720
effectively. Yeah, Rene, thank you so much, Anna. This has been really insightful. Yeah, thank you.

01:39:16.360 --> 01:39:17.740
you

