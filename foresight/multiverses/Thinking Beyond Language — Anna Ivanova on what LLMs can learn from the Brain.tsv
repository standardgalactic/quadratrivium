start	end	text
0	4940	I'm James Robinson, you're listening to Multiverses.
4940	10880	Language can do and express many things, and in fact this was the subject of my last conversation
10880	15360	on this podcast with Nicol Krishnam talking about ordinary language philosophy.
15360	21560	Just because language is so powerful, we might be tempted to think that that's all we need
21560	23780	for understanding and predicting the world.
23780	27720	It's just manipulation of symbols, next word prediction.
28440	34760	However, if we look at the how the human brain at least actually works, it's rather different.
34760	37320	Our guest this week is Anna Ivanova.
37320	41400	She's an assistant professor at Georgia Institute of Technology,
41400	45240	and she tries to understand the relationship between language and thought,
45240	51480	and she does this by looking at brain scans, essentially MRIs, of what's going on when
51480	54200	humans are presented with particular scenarios.
54200	58920	For example, I'm looking at this marvellous view right now from Carlton Hill, Edinburgh.
58920	62920	I'm not thinking about it linguistically, it's going straight into my visual cortex
62920	66440	and processes happening there, and if I want to reason about it,
66440	69080	I'm not going to reason about it linguistically either.
70200	75880	A lot of her work looks at how conceptual knowledge of the world is not tied to the language area
75880	81560	of our brain, so for example subjects with aphantasia, people who have large-scale
81560	87480	damage to the language network, are still able to reason not only logically about
87480	91320	chess problems and things like that, but they can reason socially as well.
91320	94280	They can understand what situations are unusual.
95960	101320	So this is a really insightful and very timely conversation because it plays into a lot of
101320	107720	the enthusiasm about LOMs, which I certainly buy into myself, but it calls into question
107720	116680	some of this, forcing us to think, well, what is necessary on top of simple linguistic abilities
116680	120120	to really be a fully-fledged thinking machine?
120920	126920	I think one question that still exists in my mind is, to what extent just language
126920	131320	manipulation could get us to a fully thinking machine, somewhat in the same way as, you know,
131320	141080	I have a GPU and a CPU on this laptop, and I could use my CPU to play, you know, vector-based
141640	146280	computer games, and I could use my GPU to send emails, but they're not really suited to that task,
146280	153640	but the point is maybe language could be a kind of fully-fledged thinking system.
153640	157960	However, I think it is valuable to learn from, in fact, what the brain does, of course.
158680	161160	So I really enjoyed this conversation. I hope you did too.
177400	180280	Hi, Anna Ivanova, welcome to Multiverses.
181320	182680	Hi, thank you for having me.
183400	189240	So we're speaking and we're thinking, I think, and people who are listening are listening to our
189240	195640	words and they're thinking, and thought seems like something that should be really familiar to us,
195640	201560	because it's one of those kind of few things that we have really direct access to, and yet,
201560	207400	at the same time, it seems so mysterious, so hard to figure out exactly what's going on.
207400	211320	Maybe it's because the piece that's doing the figuring out is the thing that we're trying to
211320	218520	figure out itself, I don't know. But yeah, how can we get some sort of grip on what thought is?
219800	220600	Where do we start?
222280	229880	Well, I think we need to start with definitions, what it is that we mean by thought,
229880	236280	because different people use the word in different senses, and of course, it also depends on the
236280	246600	context. And generally speaking, at least in my area of work, I think there is the broad definition
246600	253880	and the narrow definition. And in the broad definition, thought is synonymous with cognition.
253880	264360	So the mental processes that we use to make sense of the world around us, so that includes reasoning,
264440	272280	that includes accessing memories, that includes various social communication capacities,
272280	280600	so very broadly speaking, something we would call cognition. And then the narrow definition is the
280600	288520	stuff that happens kind of like in between us doing things. So it's not necessarily you get a math
288520	295800	problem, your reason about it, you give the result. But it's more about you lying down in bed at night
295800	304440	and thinking about your day tomorrow, or you're walking somewhere and you're playing out your
305000	310840	conversation that you're going to have with a friend. And so this kind of inner thinking that
310840	317480	happens spontaneously, not in response to any external task is also something where people
318360	322520	that people commonly refer to as thought. And of course, those are very different, right? Like
322520	327080	if we're talking about the broad thing or the narrow thing, this specific kind of mechanisms
327080	333240	that might support them might differ quite a bit. Yeah, I think both of those kind of definitions
334120	342200	capture something of what one intuitively would characterize as thinking. So we might say,
343160	348520	you know, broadly speaking, oh, yeah, of course, when this person solved that math problem,
348520	352840	they had to be thinking. But then you might also say, oh, they did it so quickly, they just did it
352840	357480	without thinking, which would be sort of more of the narrow definition, they did it sort of
358040	363000	without any kind of reflection, which is kind of what the narrower definition requires, I guess.
364680	368040	I'm curious, do you have a kind of preference for either of those definitions, or do you think
368120	375160	they both serve a kind of useful purpose? I've used both. I'm interested in the relationship
375160	380680	between language and thought, the role that language plays in thinking. And so the role
380680	387800	that language plays in thinking broadly defined thinking as cognition, that actually turns out to
387800	393640	be a more tractable question, because we can ask people to do a math problem and look whether
393640	400120	language processing regions in the brain are engaged. But for inner thinking,
400120	405720	stuff that happens spontaneously without an external task, that's much harder to capture.
405720	410120	But that's also where people have very strong intuitions there. Of course, I think in words,
410120	416520	or of course, I think without words. And so that area is harder to study, but also very
416520	419640	interesting. And so that's where I see some of my future work going.
420600	425720	Interesting. Yeah, it's true that kind of more reflexive thinking almost by definition,
425720	428760	people are going to have opinions about it, because they are kind of
429400	436520	cogitacing, turning things over. And part of that process is inward looking. So yeah,
436520	440040	people are going to be like, oh, well, I always do that with words or with images or a mix.
441000	445640	And yet sometimes I can also find it a little bit hard to remember to do that and think about,
445720	450200	because whenever I sort of think about, if I try to think about what thinking is,
450200	453400	I will do it linguistically. But maybe that's just because it's the sort of,
454280	457880	that's the sort of way that I need to think about that thing. Whereas if I think about
457880	462280	something entirely different, like a kind of, you know, spatial reasoning problem,
462280	468040	I'm sure I would do it in a different way. But it maybe wouldn't engage the linguistic part of my
468600	476840	brain. But anyway, I guess, yeah, this is a very, we've got very quickly to a very
478600	482280	key area, which is this, this, this relationship between language and thought, which
484600	491320	my last guest was Nikol Krishnan, an ordinary language philosopher. And we spent some time
491320	495720	just talking about, well, how for a while, people just thought there was no difference
495720	504120	between the two. There was, or rather, in some way, language captured the entirety of everything,
504120	508280	including thought, maybe including some other things. So, you know, Wittgenstein's famous
508280	513160	dictum, the limits of my language, the limits are the limits of, or mean the limits of my world.
516200	522200	But I guess your research is maybe questioning that. Would that be fair to say?
522440	530840	Yeah, I use Wittgenstein's quote as an example of a worldview, a paradigm that I'm pushing against.
533000	540760	And what's wrong with it? I mean, as we said, like, when we try to describe what thought is,
540760	551480	we'll probably reach for language. And yeah, it seems like so many of the ideas,
551480	555880	I don't want to say everything, but so much of the ideas that we have, and we pass on, we do
556840	561480	with language. And maybe there's an argument that the places where we're not doing it with
561480	569480	language are somehow dependent on language behind the scenes. But that's not a very scientific
569480	574440	argument. And I think you have some kind of evidence to the contrary. So, yeah, maybe take us
574440	582200	through some of the things that you've done to probe this. Yeah, let me, there is a lot to say.
582200	589800	So, let me start first by, I guess, acknowledging the last bit that you said, where clearly,
589800	597160	there is a relationship between language and thought. And the most trivial, but also important
597160	603240	one is the fact that we use language to transmit information, to communicate thoughts to one
603240	608840	another. And that's a very powerful mechanism, we can translate knowledge from generation to
608840	617000	generation. So that is a very important role of language in thinking, helping us share information
617000	623160	without having to figure out every single thing individually. But here, what we're talking about
623160	629720	is using language as a medium of thinking. So internally, do we think in words, do we recruit
629800	636200	the mechanisms for language processing when we're thinking? And so that's an important distinction.
636200	644360	So that's the scope. Now, as we said, people have strong intuitions about whether or not
644360	652120	they use language to think. And probably that these intuitions are grounded in people's personal
652120	660760	experiences thinking. And so one important fact to keep in mind is that there is huge
660760	669000	individual variability in how people perceive their own thinking to be. And so my pet theory
669000	675960	is that a lot of philosophers are strong verbal thinkers, they spend a lot of time writing,
675960	683000	they think about abstract topics. And so to them, the link between language and thought and
683000	690360	their experience is very strong. And it just seems that people, not just philosophers,
691080	698120	have this tendency to assume that everybody else thinks in kind of the same way. And so
698760	704120	if you are a strong verbal thinker, you automatically assume that everybody else is as well.
704680	711400	Until you start actually talking to other people and asking about their experiences.
711400	717080	And so I've had these conversations with people at parties or just informally, you ask them,
717080	723720	oh, hey, how often do you think in words? Most of the time, some of the time never. And people
723720	728920	are always surprised to hear that other people's experiences might be completely different. And
728920	738040	so this is just, I think, a very important thing to keep in mind that our intuitions
738040	743400	can lead us because they don't necessarily reflect a universal human experience. It's just,
743400	753320	you know, that's how we think. And so on to the actual evidence that we can use to dissociate
753320	765160	language and thought. There are a few different strands. So one example, very briefly, is the fact
765160	771880	that animals who don't have language might often have pretty sophisticated thought and planning
771880	781960	capabilities, right? We know examples of crows being very smart, alphoctopi, even a squirrel that
781960	786760	is trying to figure out whether to jump from tree to tree or if it's too far and it needs to go on
786760	793640	the ground instead. These are pretty sophisticated capabilities. And so that's just a very basic
793640	798760	example of at least some kinds of thought. You can then argue, you know, oh, but like the kind of
798760	802600	thinking they're doing is not, you know, the kind of thinking that we care about, right? And that's
802600	807400	where the meat of the debate is. But pretty sophisticated cognition is possible in non-human
807400	817560	animals from what we know. For me, I work with humans and adult humans. And so what we can do
817560	827560	is we can identify parts of the brain that are engaged in language processing. So it turns out
827560	835800	that there is a set of brain regions in the brain known as the language network that are responsible
835800	841480	for language comprehension. So whether you're listening to somebody talk or reading, they're
841480	845960	also engaged during language production. So when you're speaking and when you're writing,
846920	853320	they are engaged in response to any language that you might know that also includes sign languages.
853320	861080	So it doesn't even have to be a spoken language. And these regions turns out are pretty selective
861080	868920	for language. So they respond to all kinds of language, but not to music, not to math,
869560	879560	not to general problem solving. And so this is pretty strong evidence that language and many
879560	885000	different kinds of thinking are actually separable in the brain. That language has its own neural
885000	894120	machinery. And that's important because it turns out that if language areas of the brain
894120	901160	are damaged, it will affect your ability to use language, but not necessarily your ability to
901160	909240	think. And so the most common example of that is a condition known as aphasia. So it difficulties
909240	915640	with language production or comprehension. Often, most commonly, it arises as a result of
916280	921640	a person having a stroke. And so if it's stroke effects left hemisphere, which is where the
921640	927160	language network is in most people, they might have really serious difficulties with language
927160	933880	production or comprehension. But if that language is limited to the language network, it turns out
933880	942920	that their ability to use other kinds of thinking remains intact. So these people with really severe
942920	950920	aphasia who really can't understand language or speak, they can solve math problems. They can
950920	957720	arrange pictures so they form a story so they can reason about cause and effect. They can look at
957720	962680	the picture or show in some kind of event, like a fox cheese in the rabbit or the rabbit cheese in
962680	968600	the fox and say which one is more likely to happen in the real world. If it's something like really
968600	974920	weird, like the scuba diver biting a shark, they will laugh because it's just kind of ridiculous.
974920	979160	And so, you know, you can tell that they understand what's going on. And there are really fascinating
979160	983800	cases, you know, some of them like like the plague chest on the weekend. So clearly, very
983800	990520	sophisticated forms of reasoning are preserved even in the face of severe damage to language
990520	995960	processing centres in the brain. Yeah, that's completely fascinating. And well, firstly, by the
995960	1005880	way, I love your theory about philosophers and how maybe the sort of minds that they have that
1005880	1014600	make them good philosophers sort of self select or selecting a very biased or unusual community of
1014600	1022520	people who think in a particular way. And yeah, so that's really interesting. It'd be great to have a
1022520	1028920	survey of philosophers, I don't know if this has happened, and how they describe their own thoughts
1028920	1036360	and compare it to other groups. Yeah, very interesting. Yeah, I think sometimes that maybe I
1036360	1039720	should, you know, not talk about this theory and actually test it experimentally first,
1039800	1047000	that we add on biased people in advance. No, as long as they don't listen to this,
1047000	1052520	or maybe you can do it anonymously or something. I'm actually talking, I think,
1053480	1059560	soon to someone who from the philosophy of science who surveyed physicists to see if they are
1059560	1063320	realists in terms of, you know, how they think about the entities of science or not. So I think
1063320	1068120	it's like it is really interesting to actually just, yeah, try to figure out how it is that
1070680	1076520	yeah, how it is that people's personal beliefs and their kind of academic disciplines or their own
1076520	1082840	or the peculiarities of their minds intertwine. But coming back to the kind of experiments that
1082840	1090040	you describe, so yeah, I think these are just really, yeah, wonderful illustrations of how
1090040	1096200	thought maybe extends beyond the language network. And I suppose what you're doing is you're asking
1096200	1102760	people, so for example, the way that we know that music is not within the language network is,
1103560	1107880	I don't know, the language network is defined as the kind of a bit of the brain that lights up
1107880	1113800	in MRI scans. You see a lot of activity there when you give people sentences and linguistic
1114760	1120120	tasks, I don't know, maybe reading or producing language. And then it's a different area of the
1120120	1125800	brain that lights up when they're listening to music or when they're solving a math or chess
1125800	1135080	problem. And even when people have quite severe damage at South Asia, and the language part of
1135080	1140360	the brain is unable to comprehend or produce language or both, they can still do many of
1140360	1145880	those other things, which is, I mean, that's really interesting for one thing, because I often
1145880	1152680	think as well, language is maybe being so key to the kind of input output of the brain that,
1153400	1160120	you know, for example, reading a math problem would kind of go via the language network,
1160120	1166120	or is it that our brain is sort of able to just kind of directly take those symbols into,
1166120	1173080	I don't know, a different area of the brain, or perhaps do we have to kind of pose those
1173080	1179320	problems in a kind of more visual way? I don't know, I'm curious about whether the language
1179320	1186200	network is kind of a gateway for much of the information going in. So it looks like if you
1186200	1193640	give people math problems in the form of mathematical symbols, right, like five plus three,
1193640	1199000	first question mark, it seems like it doesn't need to go through the language network. So even
1199000	1206680	though it's symbolic, not all symbols get processed by the language network. And perhaps even more
1206680	1216280	strikingly, one study that I did in graduate school was looking at computer code. And specifically,
1216280	1226120	we looked at Python, which is very English-like by design, so it uses English words. And on the
1226120	1231400	other end of the spectrum, we took a graphical programming language for kids called Scratch
1231400	1237320	Junior. So it has different characters. And so then you have different arrows showing, you know,
1237320	1242280	the characters going left or jumping. But it has a lot of the same control flow elements that you
1242280	1250520	would have in text-based code, like if statements and for loops and stuff like that. And so it turns
1250520	1255800	out that for both of these languages, the main network in the brain that's responsible for
1255800	1262920	extracting meaning from that code is the so-called multiple demand network. And so that's the
1262920	1269160	network that's responsible for problem solving and reasoning and planning, and not the language
1269160	1275480	network. The language network responded a little bit to Python code. But even there, we actually
1275480	1281400	weren't able to exactly establish its relation, its role and why it would. It might be some kind of
1281400	1285960	false positive where the language network is like, oh, that's language, oh, wait, no, never mind. And
1285960	1291720	it kind of goes down. So there are other researchers that are promoting that theory currently. But
1293640	1301800	even for code, we call programming languages languages because how similar they are
1301800	1307240	structurally to natural languages. Even there, it looks like it's not the language network that's
1307240	1313160	doing the majority of the heavy lifting. Yeah, I found that completely, well, surprising actually.
1313160	1318280	And I think you noted in the paper that people kind of fell on two sides of the fence. Some people
1318280	1322680	were surprised and some people were, oh, no, that makes complete sense. But I was personally
1323480	1328600	really surprised because I, as you say, there's so much similarity between the way that natural
1328600	1334680	language works in terms of being compositional and having these kind of hierarchical features
1334680	1338360	and the way that programming languages work that I would think, okay, well, you know,
1341560	1346600	it's right that we call them languages because they're so closely related, they're just sort of,
1347640	1356440	I guess, a bit stricter, less ambiguous, perhaps. But the kind of, the nature of the rules is not
1356440	1364920	so different. And yet, yeah, it's almost as if one could imagine there being some sort of animal,
1364920	1369400	like a crow, like you said, like very intelligent creature, doesn't have language, but maybe it's
1369400	1374200	got a really good multiple demand network. And perhaps we could, perhaps could be a really good
1374200	1379240	programmer, because it's not that part of the brain, which is being recruited. But it's rather
1379240	1385160	this kind of almost clearinghouse from what I understand, the multiple demand network just
1385160	1394280	picks up so many different jobs. The other thing that really stood out for me in this paper,
1394280	1400280	which I really enjoyed, was that as a kind of, I guess, control, you could, you presented people
1400280	1409640	the same problems. So the, if I remember rightly, one of the, one of the code pieces of code that
1409640	1415240	people had to interpret in Python was a calculation of BMI. And so it's like, here is a variable,
1415240	1420680	which is your weight, here is one issue is your height, BMI equals height divided by weight squared.
1422280	1426600	And so the person kind of reads through that. And, and you see it being passed off to the
1426600	1434120	multiple demand network. But then there's the same problem defined entirely verbally. So instead
1434120	1438680	of using, you know, symbols with equals, and it clearly being Python code, it's just, I know,
1438680	1443800	this is what BMI is, here, you know, here's how much you weigh, here's how tall you are, what's
1443800	1451560	your, what's your BMI. And that went to a different region of the brain, which for me was just like,
1451560	1457800	okay, well, this is the same problem, but the way that it's presented really changes the way that
1457800	1465080	we think about it. Which, yeah, that was another huge surprise for me to think just how influential
1465080	1473560	the kind of presentation or the, the medium, I guess, for a set of concepts, how much that,
1473560	1477320	that determines how those concepts are handled internally, mentally.
1479000	1486280	Yeah. And in fact, that's not that uncommon of a situation if you think about it. So let's say
1486280	1492840	somebody is listening to this podcast, versus reading the transcript, the way information
1492840	1499000	gets into the brain is different. So in the auditory modality, it goes to the auditory cortex
1499000	1505240	first. And in the visual modality, it goes through the visual cortex first, and then it gets into,
1505240	1510200	we have a specialized part of the brain that's responsible for readings of recognizing written
1510200	1516440	letters. But then they will converge in the language network, because language network
1517160	1524280	is responsible for processing either a modality. And that means that these initially distinct
1524280	1532680	representations have to converge in some, in some way. And so for some of the other cases,
1532680	1541320	like a problem that's written in language versus in code, it looks like that convergence is also
1541400	1546920	happening, but it's happening later on in the processing, right? So it goes, you know, through
1546920	1553000	the language network, and I guess the multiple demand. And then you have some shared problem
1553000	1560040	solving. So in this case, calculating the BMI is doing some math. And so that we think also happens
1560040	1563960	in the multiple demand network. And in fact, we show in the paper that you can kind of break down
1563960	1568360	that activity that we capture into the code reading part, an actual problem solving part.
1568840	1576680	But it's a fascinating endeavor. In general, in cognitive neuroscience, how do we design
1576680	1582840	an experiment where we have those kinds of different conditions, where they're very similar,
1582840	1589960	except for something that we've changed. And so at what point that difference, right, auditor versus
1589960	1596360	visual language versus code, where in the brain does that make a difference? And where doesn't it?
1597320	1604200	Yeah, yeah. So you're saying that even though it goes the language area lights up when we have
1604200	1609160	that kind of BMI problem, it's just kind of passing the thing. And then it gets passed off to
1610600	1615000	you know, to actually run the calculation that happens, like that doesn't happen in the language
1615960	1622680	area. Yeah, that makes sense. Yeah, okay. That clarifies my, I was very excited. I thought that
1622760	1627080	maybe that we had some sort of like way of doing the confrontation, just linguistically.
1629000	1634360	Guess that's what that doesn't work. I think it's possible. And we don't well,
1635640	1638920	I don't know. Well, maybe not linguistically. But like, you know,
1639800	1646280	we memorize the multiplication table, or for like some problem that we do very often, we don't need
1646280	1651240	to actually go through the steps of the calculation, we kind of just retrieve the correct answer.
1652040	1657160	I don't know if it happens linguistically or not, potentially not, probably not. But it's
1657160	1661400	still a different mechanism than actually going step by step and doing, you know,
1661400	1665560	long division in your head, or like summing multidigit numbers or something like that.
1666280	1670920	Yeah. Yeah, I think that that's a really interesting question, which we can maybe come back to.
1670920	1675320	But I guess, yeah, so you kind of see both the multiple demand and the language
1675320	1679720	network lighting up when this problem is presented linguistically. So it's sort of a
1679720	1685400	fair assumption that what is in fact happening is that, you know, there's probably some linguistic
1685400	1692200	processing, but then it then gets passed to the same sort of area of the brain, which is,
1692200	1697800	which handles the pure Python problem. But of course, yeah, I mean, that is really interesting
1698600	1704200	and kind of useful in some ways, in that, you know, it seems more efficient to be
1704200	1710120	presented just with the Python code, right? You kind of bypass that. Oh, I turn this into,
1711560	1717880	you know, it goes straight into the, the system, which can perform the ultimate calculation, I
1717880	1723560	guess. I don't know if you, if you were able to capture any information on whether it was quicker
1723560	1729880	for people to kind of solve the problem when presented with the Python code or not.
1730680	1742040	I don't remember whether we saw a difference in how long it took people.
1742760	1748360	I think it's possible, but I don't, some of it, of course, depends on how proficient they are in
1748360	1755080	Python. So there might be individual differences there, also individual differences in how
1755080	1764440	fast they would read text. So I'm sure there's some variability there. But it's actually
1765640	1771720	an interesting thought that you bring enough so that this, having this abstract skeleton with
1772360	1778840	other information stripped away might make the problem solving the calculation easier.
1778840	1786520	Because in fact, there are cases where researchers have observed the reverse,
1787640	1797160	though there is this famous Waste and Selection task, which you have, let's see,
1797560	1807080	a card with like a two and a seven, and then a, and then a card with like
1807960	1821000	green, red and blue. And you need to test the rule that says if the number is even,
1821080	1827560	then the other side of the card has to be blue. And so then the question is which cards do you
1827560	1833240	need to turn over to make sure that that rule is correct. And so then people want to test
1835480	1840040	the card with the two on it because it's even, and so they want to make sure that their reverse
1840040	1845240	is blue. But then they often want to turn over the blue to make sure that the other side is odd,
1846040	1851720	sorry, is even. But that actually is not what you should do because it doesn't matter. Like if you
1851720	1856360	have blue and odd, that's actually not a violation of the rule. What you need to do is you need to
1856360	1861560	turn over the red card because of an even number there, then that rule gets violated.
1862360	1870760	So that problem is hard for people. But if you cast the same problem saying that there are
1871640	1880200	people at the bar and somebody, you know, is 16 and somebody is 25 and then somebody is drinking
1880200	1887080	beer and somebody is drinking a Coke, then, you know, how will you verify that only people over
1887080	1892120	the age of 18 are drinking alcohol? And then of course, you know, that you need to, you know,
1892120	1898040	check the 16 year old and check the person drinking the beer and not any other way. And so
1898040	1904280	mathematically, the same exact process, but it's much, much easier for people to ground the rule
1904280	1909080	and their existing knowledge, not necessarily the bar example, but they're just kind of, you know,
1909080	1915240	the easiest one and the most common one. And so this phenomenon is known as content effects on
1915240	1923480	reasoning. And yeah, I think a lot of people, especially like, you know, physicists and mathematicians
1923480	1928040	and so people trained in like hardcore STEM discipline, they're like, hey, Alex, trip away,
1928040	1932760	all of the extra information only focus on the abstract symbols. That's the easiest thing. But
1932760	1939400	actually for a lot of people grounding the problem in some specific content domain tends to help.
1939400	1945000	And so I know that some people in like math education are very interested in this phenomenon and
1945000	1953000	how does it how to make it easier for people to, for kids to learn math. Is it by focusing on the
1953000	1957480	abstract or is it by grounding math problems in real life situations?
1959560	1965720	And I suppose part of the reason why that grounding might work, well, there could be kind
1965720	1971080	of two hypotheses. One is just like, it locates it in a different area of the brain, which is somehow
1971080	1976680	better at processing this thing. So maybe that just the social reasoning part is just better at
1976680	1981720	doing that kind of problem. But doesn't seem so likely in this case. And another is just that
1982200	1989000	it gets it, it clicks it in to a place where you're able to recognize a pattern that you've
1989000	1995160	seen before. And so you don't have to do, you know, you're already on the right track.
1996920	2002600	And this maybe kind of comes back to your point about, well, maybe when we calculate the BMI for
2002600	2006760	certain kind of combinations of numbers, you just know the answer. So it's being kind of recalled
2006840	2012280	from memory, like that pattern is already so established that you don't need to reason through
2012280	2022920	it in the same way. It's more of a recall operation. And I mean, this is getting us towards one of the
2022920	2027720	kind of central questions, which is around, well, what are LLMs doing? Because they're kind of
2028920	2033720	glorified recall machines in a certain way, or just really good pattern matches.
2034200	2041960	Maybe before we get to that, though, I want to talk about another of your experiments, which I
2041960	2048360	really enjoyed, which is about where people are looking at images of improbable and probable
2048360	2057000	things like the shark and the swimmer that you mentioned. And what I found, well, maybe you
2057000	2061800	should describe the experiment, you'll do a much better job of it than me. Because I think, yeah,
2061800	2064440	there was just a really interesting piece here that kind of writes this.
2066200	2072840	Yeah. So here, we use that same idea that the same information might arrive in the brain through
2072840	2081480	different routes. And so in this case, we were looking at sentences describing basic interactions
2081480	2091400	between two entities, like the, I guess we can roll with the shark bites the swimmer,
2091400	2101320	the swimmer bites the shark, and pictures depicting the same kinds of events. And so here,
2101320	2107000	by switching around, who's doing what to whom, we're manipulating whether the event is plausible.
2107000	2114600	So likely to occur in the real world or impossible. So unlikely to occur. And the question was,
2115560	2123640	does the language network respond to language specifically? Or does it respond to meaning
2123640	2129880	and concepts more generally? And so if it's language, it should only really respond to
2130440	2137880	sentences and not to pictures. And if it's responsible to meaning, it should respond equally
2137880	2143240	strongly to sentences and pictures, as long as the person is thinking about the meaning. And so we
2143240	2148600	had people tell us whether they think the event is plausible or impossible. So you have to be
2148600	2157160	thinking about the meaning. And so what we found was actually something in between, where the language
2157160	2165000	network, in accordance with all of the prior work, responds more strongly to sentences than to pictures.
2165880	2173080	But it still responded to pictures to some extent. And I will say that in another study,
2173080	2181000	we recorded responses in the language regions to pictures of objects. So is this animal dangerous?
2181000	2186280	Can this object be found in the kitchen, that kind of stuff? And it did not respond to pictures of
2186280	2193720	objects. So it was something about events, maybe just something more complex, maybe something just
2193720	2200840	more fast-paced, that was specifically triggering responses in the language regions. And so this
2200840	2208760	intermediate result, so preference for sentences over pictures, but also responses to meaning,
2208760	2217560	even in known sentences, was kind of puzzling. And so one piece of evidence that helped us
2218680	2227720	make sense of this information was evidence from individuals with global aphasia, from people with
2227720	2233320	brain damage. And I should say that this is, yeah, so lots of the brain imaging work I'm describing,
2233320	2242280	I did with my PhD advisor at Fedorenko, and the global aphasia bit is done in collaboration with
2242280	2252120	Rosemary Varley at UCL, who works with individuals with global aphasia very, very closely. And so
2252920	2260920	here we had two individuals with global aphasia, really serious issues, looking at pictures of
2263480	2269640	swimmer by sharks, shark bite swimmer. And so as I mentioned earlier, they were laughing at the
2269640	2275000	weird ones. And so in general, they were very good at distinguishing plausible and implausible
2275000	2283480	pictures, suggesting that their ability to extract meaning from pictures was there, it did not
2283480	2290280	require a functioning language network. And so then the title of the paper, the language network
2290280	2297720	is recruited but not required for pictorial events and semantics. So we see this activation,
2298440	2303800	and we, but it's not, it's not necessary to do the task.
2305480	2311880	Yeah, yeah, I found that, yeah, very insightful. And what struck me was
2313480	2318360	one of the hypotheses as to why the language network was recruited is that
2318360	2325080	it's sort of another way of getting evidence or information on whether this event is likely or not.
2325880	2329560	And of course, we can't be sure what's going on in there. But, you know, perhaps it is somewhat
2329560	2335640	like a large language model where you, you look at the thing, you're like, part of trying to figure
2335640	2340200	out whether this picture is likely or not is you kind of read it out to yourself and like, well,
2340200	2350120	does this, is this a familiar pattern, right? Does it, is, you know, shark bites swimmer,
2350120	2355800	you know, that's that's that sequence of words feels close to sequences of words that I produced
2355800	2362680	before or read before, whereas swimmer bites shark is kind of jarring. And maybe behind that is just
2362680	2369960	the improbability of that, that, that, that sentence being produced according to the language
2369960	2376920	model in our, in our own minds. And of course, yeah, we, we don't really know that our minds work
2376920	2383560	like a large language model at all, but it's an attractive hypothesis in as much as it works.
2385160	2391080	Yeah, so I guess, so generally speaking, right, so we got this result language network recruited
2391080	2395400	but not required. And the question was, what's going on, right? And so generally speaking,
2395400	2405080	we consider two broad hypothesis. One is that that activation is not necessary to do the task.
2405080	2411560	So you see the picture, sharks, swimmer and biting. And so you activate those words kind of
2411560	2418360	automatically by association, but you're not actually using them to reason about whether
2418360	2424040	the event makes sense or not. So that's one hypothesis. And the other hypothesis is that
2424040	2430360	actually, the information in the language network is helpful. But when you're trying to
2431240	2437160	recast information that you're seeing in linguistic form, you can then compare it with
2437160	2442120	all of the linguistic information that you received in your lifetime. And maybe that
2442120	2447720	information ended up distilled in your brain in some general way, just kind of, we know that
2447800	2452760	people are very sensitive to statistical regularities in language, we know that we're very good at
2452760	2462600	predicting what word would come next, right? Like there is this information about what patterns
2462600	2469000	are likely in text is very much what people use during real life language comprehension.
2469800	2478360	And of course, that information also can help us, in many cases, figure out which events
2479160	2485560	make sense and which doesn't. And so actually, we try to test that hypothesis. We
2487320	2494440	didn't necessarily, it's hard to test that in actual human brains, although we now have
2495160	2505160	ideas for how we might be able to do that. But we started by using language models as proof of
2505160	2514600	concept, right? So the hypothesis is statistical patterns in language input can help us distinguish
2514600	2521480	plausible and implausible events. And language models are very good at capturing these statistical
2521480	2528920	patterns. So if language models can systematically distinguish plausible and implausible events,
2528920	2534600	that means that there is enough information there where maybe humans might be able to use that
2534600	2538760	information also to distinguish plausible and implausible events, right? So it's not evident
2538760	2546040	that humans do, but it's evident that humans can. And so we did that, we use language models and
2546040	2554360	try to see whether they systematically evaluate plausible event descriptions as more likely than
2554360	2563640	implausible. And so in that study, we specifically distinguished between two kinds of events. So
2563640	2572280	one is the teacher bought the laptop versus the laptop bought the teacher. So animate, inanimate
2572280	2579080	interactions. And so when you swap them around, if you interpret the sense verbatim and the inanimate
2579080	2587320	object laptop cannot buy anything, buying requires that the subject is animate. And so that's a very
2587320	2593960	kind of in your face screaming violation. And then the other example is kind of like the fox
2593960	2599000	chased the rabbit, the rabbit chased the fox or the swimmer by the shark, the shark by the swimmer,
2599000	2604760	right? The swimmer by the shark is not impossible. It can happen. It's just way less likely.
2605400	2610120	And so what we found is that when it comes to distinguishing possible and impossible events,
2610120	2617960	language models were very good, almost at ceiling. So that was actually very easy for them. But when
2617960	2624120	it came to likely versus unlikely events, there was a gap in performance. So they weren't quite as
2624120	2629880	good. They were okay. They were above chance. But they definitely weren't perfect. And so we're
2629880	2637640	not as good as people, I guess, right? Yeah, not not as good as people and not as good as when they
2637640	2643480	have to deal with animate inanimate sentence, right with impossible events. Yeah. So I think it's
2643480	2648200	like, to me, it's actually more interesting to compare those two sentence types, like how models
2648200	2655000	do on them. But also humans humans do well on both because these are easy sentences. So
2655000	2663240	they're not meant to be challenging. And so, yeah. No, I was gonna say, and do we take that as evidence
2663240	2671640	that then when humans reason about these things, they're doing it, not just linguistically, or is
2671640	2676440	it that our language models are kind of like better than the language models that that are out there
2676440	2682920	in, you know, the computer language, large language models? I think that I think it's
2682920	2688120	evident that humans are doing it not linguistically. And so the reason why we think there is this
2688120	2696200	performance gap is because actually, the language input that we receive doesn't faithfully describe
2696200	2704120	the world around us. So when we talk to each other, we don't just passively describe everything
2704120	2709880	that we're seeing. So I'm not telling you, you know, I am sitting down, the lights are on,
2709880	2716520	the room is empty, like it's very boring stuff. I'm telling you about things that are unusual,
2716520	2723640	novel, interesting, newsworthy in some way. And so this phenomenon is known as reporting bias.
2724280	2733160	So language tends to undercover on the report information that is kind of trivial, right,
2733240	2739480	that everybody already knows, or can reasonably infer. And so maybe actually events that are
2739480	2745560	unlikely are not as unlikely for LLMs, because, you know, we talk about unlikely things all the
2745560	2750200	time, that's the stuff that's worth talking about. And so if that's true, that's the reason why we
2750200	2755800	see this performance gap, then even if the human language model is very good, which by the way,
2755800	2759640	we don't think it is actually, I think large language models now are much better at predicting
2759640	2765080	the next war that humans are. So actually, they're better. But even if humans were really good,
2765080	2773320	just the language input is insufficient for us to be able to distinguish plausible and
2773320	2779000	implausible events. That means that we have to use something else in addition, we have to maybe
2779000	2784520	have some more sophisticated model of the world, where we can actually correct for this reporting
2784520	2793000	bias, we have to also bring in information that's about what things are typical, what we
2793000	2799160	can expect, what we cannot expect. So we're probably drawing on sort of multiple mental
2799160	2809880	resources or systems. In the case of the clearly kind of impossible, so computer bias teacher,
2810840	2816840	is it just the language, can you see if it's just the language, or have you seen if it was just
2816840	2821080	the language network that's recruited there, as one might think, well, if it can just be done
2821080	2827800	within the kind of the one region, maybe it's more efficient and metabolically, there might be some
2827800	2835480	kind of preference for doing that if it were possible. By default, we kind of light up various
2835640	2843880	regions just to make sure I don't know. So that's actually a study that I would love to do next,
2843880	2850040	so this difference between impossible and unlikely events is something that emerged out of this
2850040	2857320	language model study. And so now, of course, yeah, I think it would be great to bring it back to
2857880	2864200	the MRI machine, measure people's brain activity in response to impossible versus
2864200	2869720	unlikely centers, and see if the language network alone is sufficient for distinguishing
2870360	2875160	possible and impossible events. That is the prediction that follows from this language
2875160	2880600	model work. And so I would love to test that. Yeah, yeah, that would be so, yeah, I'd love to
2880600	2887560	see the results of that. Yeah, so I hope that that happens. But I suppose, you know, coming back to
2888200	2896280	LLMs, what we're starting to see is that maybe just a large language model in itself,
2898200	2908520	for various reasons, might not be so effective at thinking or reasoning as the human brain. And
2908520	2916520	one is, as you kind of mentioned, that the data that comes in is kind of biased toward the salient
2916600	2922680	and newsworthy as you put it. But then another from the kind of Python example is that, well,
2922680	2931720	as a matter of fact, we don't use the language part of the brain for code comprehension or for
2932280	2941480	logical mathematical reasoning, either for that matter. I suppose my question there is, though,
2942040	2947000	you know, could it be possible for LLMs to kind of just be
2951880	2959240	be able to take on the functions of the multiple demands network, for instance, which is doing
2959240	2965000	all this, which is the place which does the mathematical logical Python code interpretation
2965560	2971720	comprehension? Could it kind of take on all those responsibilities just by having
2973400	2977960	getting really good at saying, you know, next word prediction for mathematical problems and
2977960	2984840	next word prediction for code generation and so on? Or is just that kind of, or is that implausible?
2984840	2992840	I don't really know how we characterize, you know, where the LLMs just could have kind of
2992920	2998120	emergently develop all those capabilities within a single language model, or if that's just
2999720	3012200	very, very unlikely. Yeah, so LLMs do a bunch of different things. In general, as you mentioned,
3012200	3017400	they're very, very good at pattern recognition and pattern completion at different levels of
3017400	3024360	abstraction. So they do a lot of just direct memorization, right? The larger the model,
3024360	3030600	the more texts that can just memorize straight up, which is why a lot of those copyright issues
3030600	3035800	end up arising. But that's not the only thing that these models do, because they definitely
3035800	3042760	are capable of generating novel texts and mixing and matching previous inputs. And so the patterns
3042760	3049880	that they can recognize and reproduce, they can be fairly abstract. But then, of course, the question
3049880	3062360	then is pattern completion all it takes? Is that the only thing that's necessary? And so that's where
3062360	3070840	it gets tricky, because a lot of logical reasoning is algorithmic reasoning. It's symbolic. It's
3071480	3078280	very regimented. And so these are the kinds of problems where these models seem to struggle.
3078280	3084360	So for example, if you ask them to add and multiply two numbers together, if the numbers are small
3084360	3091800	enough, then the model is doing just fine. But if the number is large, that means it wasn't part of
3091800	3096920	the training set. It means it couldn't have just memorized the response, which it probably does for
3096920	3103800	a lot of smaller number combinations. And so then it would actually have to multiply step by step.
3103800	3111080	And it doesn't seem to be doing that very successfully. In fact, it often gives you a number
3111080	3118680	that's close, but just a little bit off. And so the kinds of mistake that it's making is different
3118680	3123400	from the kind of mistake a human would be making, because it's still trying to use pattern matching
3123480	3131480	to complete. And it's not quite working, it seems. Yeah. But I mean, it's very hard to figure out
3131480	3136520	exactly how they're doing what they're doing. I mean, they've got so many parameters. And it's
3136520	3146360	surprising how good they are yet still imperfect at doing those sort of problems. They're kind of
3146440	3154600	like a broken calculator. So they're much faster at getting to an answer, but it's not quite the
3154600	3161320	right answer. It's a pretty good estimate often. And yet it's not completely out. So it's really...
3163160	3170360	Yeah, I don't have a strong opinion, but part of me thinks, well, maybe they'll just kind of,
3170360	3177080	with enough data going in, they might just crack that. That might come a point at which
3179720	3184840	that kind of ability emerges. Although you point out in one of your papers, though, well,
3184840	3187960	perhaps if that ability emerges, it might be that a particular kind of
3188760	3194440	architecture that models the human brain emerges as well. So it may not be that...
3194680	3202760	It might be happened in such a way that it becomes less fruitful to think of a large language
3203480	3209640	model as simply a model of language, but something that has a kind of linguistic language network
3209640	3217640	part like the human brain and then hands off to a logical part. And as it happens, obviously,
3217960	3224600	in chat GPT, without that has had that kind of architecture imposed on it, at least in the
3224600	3229080	version with the Python code interpretation, for instance, because you can say, well, add these
3229080	3232360	two numbers together, and it will figure out, oh, well, I'm doing a math problem here. So I'm going
3232360	3236840	to convert this into a Python problem, and then it runs the Python code. So actually,
3238120	3240760	you know, some of these problems seem to be
3241080	3249240	are being sort of addressed, I guess, by the developers. But the way they're doing it is,
3249240	3257480	yeah, offloading. Yeah. Yeah. So I guess let me unpack a little bit. There's a lot there. So
3257720	3270760	first of all, it is very tempting for people to over ascribe intelligence to a language model.
3271400	3278200	And presumably that's because in our everyday interactions, we're used that language gets
3278200	3285160	generated by a thinking feeling being other humans. And now we have a system which is breaking
3285240	3290520	that relationship where we have something that generates coherence language that's not human.
3290520	3298680	And so it gets confusing. And that's the reason why that's one of the reasons why
3298680	3303480	there's so much hyper-intelligent language models, and they're expected to be the general
3303480	3310280	intelligence models, because of this tight perceived relationship between language and thought.
3310280	3315400	And so then when they make a math mistake, or they make a factually inaccurate statement,
3315400	3319240	you're like, oh, no, like how, you know, these models are terrible, they're not
3319240	3323720	terrible, they're just like not, they're just a totally different capacity you're evaluating.
3323720	3331800	And so what we argue is that it's very important to distinguish different kinds of
3331800	3338840	capabilities in these models. And so there is something that we call formal linguistic
3338920	3344600	competence. And so that's the ability to generate coherent grammatical language.
3344600	3350440	And that's something that in humans, the language brain network is responsible for.
3351560	3358360	And then there is all of the other stuff that you need in order to actually use language in
3358360	3364600	real life situation in interactions, you might want to ask somebody to close the door,
3364600	3371640	you might want to tell somebody how you feel. And there are all kinds of situations that
3371640	3378360	where you need to use language. But to do that, you actually need other capabilities,
3378360	3383720	you need to be able to reason about social situations, you need to be able to know things
3383720	3388760	about the world in order to generate actually accurate statements, you need to be able to reason
3388760	3393640	logically and know some math if you want to solve a math problem. So even if that information is
3393640	3399240	coming in as language, in order to be able to make sense of it, and also generate language that
3399240	3405160	achieves a particular purpose, you need all of these other capacities, which broadly speaking,
3405160	3413080	recall functional competence. And so different kinds of capabilities might suffer from different
3413080	3420120	problems. And so we already touched upon a few. We touched upon the fact that mathematical reasoning
3420120	3426760	and logical reasoning might require a different kind of algorithm. So instead of pattern matching,
3426760	3432600	it might need to be more symbolic. And it's not fully clear whether the large language models
3432600	3441560	today are capable of doing that. Maybe they are. But that's not necessarily in their default,
3442360	3447480	in the default way they operate. So that's an open debate there. When it comes to world
3447480	3452440	knowledge and knowing things about the world, distinguish implausible and implausible events,
3452440	3459400	there a big problem is reporting bias and the fact that the training data that they have is biased.
3459400	3466040	And so you might need to be able to build up a more general situation model, event model,
3466040	3471640	that will not just take in the language that you receive, but also fill in some kind of commonly
3471640	3480440	assumed things. If it's daytime, it slides out, stuff like that. And yeah, so different kinds
3480440	3487160	of problems might require different kinds of solutions. A more general kind of potential solution
3487160	3498600	that we advocate or talk about is modularity. So the fact that the brain is modular suggests that
3498920	3505480	might be an efficient architecture. So a language process in module, the goal of the language
3505480	3513080	network in the brain is not to reason, it's to get information that's expressed in fuzzy,
3513080	3519240	imprecise words and extract meaning out of it. And then pass it on to relevant systems that can
3519240	3527160	solve the math problem that can infer the social goal, all of that stuff. And presumably, for an
3527240	3532120	artificial intelligence system, you might want to do something similar where language
3532120	3538760	is not a replacement for thought, but is an interface to thought. And so in your example,
3538760	3545320	right, you have a math problem, the language model translates it into code. It's very good at
3545320	3550280	taking this broad like fuzzy natural language and translating into a more precise, symbolic
3550280	3554600	representation. That's something that we didn't have at all, even a few years back. So it's a
3554600	3560280	huge achievement. But then instead of trying to have that same language model to run the code,
3560280	3566680	you're much better off passing it off to a code interpreter that will run the code and give you
3566680	3572760	the answer. So the same kind of modularity that we see in the brain, that seems to be an effective
3572760	3578280	way forward in the AI world that indeed some developers have started to adopt.
3579240	3587240	Yeah. Yeah. And I think there's probably other ways in which the builders of these tools are trying
3587240	3593320	to modularize. Like another one that comes up a lot is Rang or retrieval augmented generation, where
3595080	3600600	yeah, there's some kind of database or just could just be a whole bunch of, you know,
3600600	3607080	documents or whatever. And instead of hallucinating an answer, you want to make sure that you pick up
3607800	3614280	something from one of those documents. And there's a whole different kind of machinery for that.
3614280	3621160	But again, like in the code interpreter example, it's, I guess the language part is
3622440	3627560	key, maybe less key in Rang because it's kind of a vector search. But it's a way, you know,
3627560	3633560	it begins with translating language into something a bit more precise, in this case a vector instead
3634120	3641640	of some code, I guess. And yeah, one wonders then if, you know, how close the parallels are between
3642680	3648600	what is being built here and what's going on in the brain. You mentioned that, yeah,
3649240	3655000	perhaps this is a good model for thinking about how we think. Language is this part where,
3655640	3662280	this place where things kind of, you know, entry point for concepts, but the places where
3662360	3668040	those concepts often get manipulated in terms of reasoning might be in other areas of the
3668040	3672680	brain. They sort of become something more abstract than language itself.
3676200	3682520	Yeah. Yeah. One thing I actually just slight tangent, but I do sometimes think that
3684440	3689800	maybe language is being so associated with thought because it's kind of like
3690760	3696760	the easiest thing to do, right? Like, you know, we know thinking is about concepts and some,
3697320	3703880	you know, manipulating these things which are representations of the world. And language is
3703880	3709160	just such an easy way of visualizing all of that, right, and understanding what's going on. But
3709160	3716920	perhaps it's just the surface level of something much deeper that we really don't have an easy way
3717000	3724040	of capturing. And, you know, that would map, I think, quite well to this kind of model of
3724680	3729960	concepts being passed around, but the concepts themselves being, you know, beyond linguistics
3729960	3738760	somehow. Yeah. So, as we mentioned, language is a system designed to communicate thoughts,
3739320	3745320	concepts from one mind to another. And so, for this communication to be efficient,
3745320	3750760	presumably language needs to parallel the structure of thought, the structure of concepts
3750760	3757320	in some way, right? And so, it's much more abstract already than the raw perceptual input,
3757320	3763800	than just audio, than just pictures, right? So, it kind of captures the relevant abstractions to
3763800	3770200	a large extent. And so, that seems to be helping a lot. And so, that does bring us much closer to
3771000	3774920	this more abstract conceptual representation. We're getting rid of a lot of extra details,
3774920	3781000	we say cat, we don't care which color, which size is the cat. But, of course, at mapping
3781000	3785720	between concepts and languages imprecise, we know that different languages partition the
3785720	3791640	conceptual space in different ways, right? So, the words don't necessarily map the concepts one
3791640	3797800	and one. Even within the same language, the same word can be used in many different contexts,
3797880	3805720	in different ways, with different meanings. And so, that link is pretty fuzzy, can get pretty
3805720	3811560	fuzzy. But it's definitely, I think you're right, when it comes to raw surface form,
3811560	3817400	it's a very decent proxy, imperfect, but it makes sense why people are tempted to use it.
3818040	3823400	Yeah. And, you know, in some ways, that means it makes what LLMs do so much more impressive,
3823400	3829720	because they're also somehow capturing that surface form of concepts. Someone,
3831000	3838280	a previous guest pointed out this wonderful quote from Ilya Sotskava saying, well, you know, if
3840920	3849400	your LLM can predict the, you know, it's not just predicting text, because if your LLM can be fed
3850120	3857800	the first part of a mystery novel that it's not read before, and it can tell you who the murderer
3857800	3864520	was, it's not just predicting a word, it's somehow kind of understood what's going on in that story.
3864520	3871720	Now, one of the difficulties, obviously, with all these things is, well, we don't know how
3871720	3877640	open AIs LLMs are trained. So, it's very hard to test them, because you really need someone to write
3877640	3884840	a new mystery novel to actually see if Ilya Sotskava's claim cashed us out. So, it's quite a
3885400	3891480	high effort test. Unless, yeah, we happen to know of one which is definitely not in the corpus that
3891480	3902040	was used. But, yeah, it does seem, you know, the fact that they are so good at mirroring what
3902040	3911480	we produce, and that what we produce is somehow a good map onto something somewhat deeper, the world
3911480	3918280	or an inner world. Yeah, it's so impressive. And you point out as well that it seems that,
3919080	3926360	you know, the way that LLMs operate is very similar structurally to the way that
3927080	3935400	our minds operate, in that, you know, it's not working on the raw audio or pixel forms of things.
3936040	3943080	Like, the beauty of language is the compositionality at the level of small units, which are
3943720	3952120	combinations of symbols or small sounds. And, yeah, the LLMs perfectly match that. So,
3952840	3959400	we've built these things which really do capture something quite essential about how at least a
3959400	3965400	part of our mind operates, it seems. And, yeah, maybe we've been seduced into thinking. That's
3965400	3973720	all there is to thinking. Well, yeah, so in fact, well, that question, I guess I don't want to get
3973720	3978760	too technical, but the question of what LLMs are starting with is actually an important one when
3978760	3987480	we're trying to compare them with human minds or human brains. So, in fact, what LLMs operate over
3987480	3996360	is tokens. So, it's chunks of characters that tend to occur pretty frequently in text. And so,
3996360	4001320	oftentimes, they're words, like, though, but they're sometimes not words. If the word is long,
4001320	4008440	it gets split up into multiple tokens. Yeah. And so, the problem is that those tokens actually
4008440	4014600	don't match linguistic units that the word is actually made of, like morphine. They can be
4014600	4021720	pretty arbitrary. And so, that does cause some differences between the way LLMs process them
4021720	4029800	and humans do. In fact, people think that one reason why large language models are bad at arithmetic
4029800	4035800	is because they tokenize numbers in weird ways, right? So, like, I don't know, 1618 is chunks
4035880	4040760	in, like, 161 and then eight. And so, then it gets weird when they have to, like, add up the numbers.
4040760	4047480	And so, that's where you get this weird, better-matching errors. And so, this kind of form is
4047480	4053240	that, it's very engineering-driven. It's actually not, like, very rigorously scientifically based.
4053240	4058680	And so, it's interesting, like, maybe if we change this little thing, it actually will result in much
4058680	4069000	better performance. And so, it's funny how a lot of those choices are pretty random engineering-driven
4069000	4075800	things. And, you know, they often work very well. But it's possible that with a small few tweaks,
4075800	4079880	you can actually make the model much better. Yeah. No, I always thought that there was more
4081400	4086600	sort of reasoning behind the n-grams that were used. But maybe, is it just kind of randomly
4087240	4093240	chunks? Because I would have thought, well, there's some kind of, it makes sense to split
4093240	4101480	words up, because, you know, particles like nus, if I think of, like, redness, right? It's not a
4101480	4107000	word in itself, but it does attach to so many different words that it's sort of part of the
4107000	4114360	compositional structure, I guess. But if it's getting chunked up is just two s's, right? And not
4114440	4121320	nus, then it's kind of odd, yeah. No, that's exactly right, because nus is a morpheme,
4121320	4126280	it's a suffix with a particular meaning. And so, if redness is chunked into red and nus,
4126280	4131560	that makes a lot of sense, and it's linguistically justified. But oftentimes,
4131560	4136920	that's not how the chunking happens. That's where the mismatch arises. So, you can definitely
4136920	4141880	have the two s's in principle. Okay, interesting. Yeah, it seems like, yeah,
4142360	4148840	one would think that with a bit of curation, maybe they could be even more effective. And yeah, it's
4148840	4154440	hard to imagine them being more effective in terms of producing language. But perhaps that's
4154440	4161480	just because they've been fed such a, such copious amounts of data that they sort of these,
4162600	4168840	you know, they could be more efficient, right? Well, the stalker has an algorithm,
4169720	4176120	it's kind of the goal is for it to be universal and that driven, right, without human curation,
4176120	4183160	which is why the morphes don't get respected all the time. It causes a lot of issues for languages
4183160	4191320	that aren't based on the Roman alphabet. So, let's say Arabic, for example, it ends up getting
4191320	4196840	tokenized at the character level, because the tokenizer is just not adapted to deal with it.
4196840	4200920	And so that does mean that performance on these languages that are not
4200920	4207080	Roman alphabet based is actually worse, often substantially worse. It's generally a problem
4207080	4213800	that like the fewer, the less data a language has, the worse the performance in that language.
4213800	4218200	Some of the more general information seems to get pulled across different languages,
4218200	4223400	which is cool. But a lot of language specific stuff, like grammar, right, of course depends
4223400	4228680	on how much data you have in that language. But a particular distinction that tends to
4228680	4233400	matter beyond just the amount of data is which alphabet. And so because so many of these morphs
4233400	4240600	are English centric, a lot of other languages get left behind. Yeah, interesting. And to one
4240600	4246280	extent, I mean, I know there are techniques for doing this. So you spend, you know, a lot of
4246280	4250200	experiments looking into the minds or the brains of people.
4252200	4259000	There are tools which allow us to do this to an extent with LLMs. But, you know, how effective
4259000	4265000	are they? How does it compare to looking at an MRI, trying to understand what's going on inside
4265000	4271400	of an LLM, what concepts it has, or what's lighting up as it is given a prompt?
4272200	4279080	Yeah. So I am fascinated, honestly, by how many parallels there are between studying biological
4279080	4286760	intelligence and humans and artificial intelligence. And for me, the first similarity is really just
4286760	4293080	starting at the behavioral level. So developing separate experiments to look at formal competence
4293080	4297080	like grammar, functional competence, like reasoning, these are methods from cognitive
4297480	4302280	science, how do we design good experiments, how do we disentangle different contributors to
4302280	4307560	performance. So even before we start looking inside the model or inside the brain, just looking
4307560	4314760	at how humans behave and how models behave can tell us a lot about potentially how they do it,
4314760	4319960	what kind of mistakes they make, what does it tell us about the potential mechanism that they're
4320600	4329000	using to solve the task. But then, of course, we can get even more insight by looking at the
4329000	4334040	actual mechanisms or their neural correlates. So for humans, that means looking inside the brain.
4334040	4342600	And for models, that means looking inside the model. And so the movement that is getting seen
4342600	4348840	currently the mechanistic interpretability movement in AI is doing that, essentially,
4348840	4356440	they're asking which circuits, which units inside the network are responsible for a particular
4356440	4365720	behavior. And so they first try to identify those units that get particularly engaged in a task.
4365720	4370520	Maybe they respond differently to plausible sentences compared to implausible sentences.
4371160	4377720	And then the beauty of having an artificial system is that they can actually go and
4377720	4383320	manipulate it directly. So you can knock out that circuit or you can replace activations from one
4383320	4388040	sentence with activations from another sentence. So in neuroscience, people sometimes do that as
4388040	4393320	well. In animal research, for example, or there are certain kinds of stimulation that you can do
4393320	4401960	that aren't harmful, but can maybe do the desired effect. In aphasia studies, these are natural
4401960	4406520	causal experiments, right? We didn't cause delusion that destroyed the language network.
4406520	4414360	But because we see those cases occur naturally, we can look at those effects. And so the causal
4414360	4421560	tools are really powerful because they can really help us to see whether this part of the circuit is
4421560	4430360	necessary for the behavior that we observe. And so in AI systems, we can do that quite easily.
4430360	4437000	But conceptually, I would say in neuroscience and in AI, what we're trying to find out is very
4437000	4443000	similar. Yeah. Yeah. And it's, I mean, it's wonderful, as you say, at least with the behavioral point,
4443640	4450440	you can draw on the same kind of experiments that, you know, we finally have a kind of
4450440	4453880	artificial intelligence that you can feed the same sort of things that you'd feed a person,
4454200	4458760	i.e. sentences. And so it makes it very natural to run those kind of experiments.
4460200	4466360	But then on the other hand, you can also go into the thing itself and tinker it with it in a way
4466360	4471720	which would be very unethical and, you know, even just impossible with a person. So you could,
4471720	4478280	I think there was one example where you had, I don't know, the concept of or Berlin was replaced
4478680	4485560	with Paris or, no, what was it? It was Rome. Was it the Eiffel Tower was placed in
4486600	4489960	conceptually into Rome or something like this? And you asked, well, how do you get from
4491400	4496760	Berlin to the Eiffel Tower? It wasn't me, but it was, yeah, it's a famous kind of editing study.
4497800	4501720	Yeah, I think I must have read it in one of your papers referring to it.
4502360	4514440	And so the LLM does really, it kind of responds in the way that you would think if what's going
4514440	4519480	on is that it has some kind of model of the world. And what all you've done is kind of
4519480	4523720	switch around some pieces inside that model. It's not that it gets completely, you know,
4523720	4529080	it doesn't throw everything completely out of whack, I suppose. And it even kind of
4529160	4532760	infers some things that, you know, the Eiffel Tower will be in the center of Rome and
4532760	4536440	it's going to be up with the Coliseum or something like that, which is, yeah,
4537080	4545080	yeah, it's so fascinating to have something where we can kind of, you know, plausibly
4545080	4551560	peer in into the internal workings. And yet just like the human brain, everything is
4551560	4557320	so complicated that actually also it's not a trivial task, I guess.
4557640	4562680	No, but that's the benefit, I guess, when neuroscientists have, we're used to dealing
4562680	4569480	with this complexity. And, you know, there are ways to zoom out beyond just each individual
4569480	4576280	neural unit to try and look at general trends and general patterns. And so I think a lot of
4576280	4581320	people are daunted by the task of trying to understand the neural net because it's so big
4581320	4586440	and complex. And because it's trained in this way where we don't necessarily know which features
4586440	4592200	it sticks up on. But to me as a researcher, I'm just excited. It's like a cool puzzle to solve
4592200	4596440	and a cool problem to understand. So generally, I'm pretty optimistic about this endeavor.
4597240	4603400	Cool. Yeah. I think you mentioned at the very beginning that your research is now starting
4603400	4608280	to look at some of the, you know, possibly trickier question of this kind of reflexive
4609080	4614040	thinking, the narrow type of thinking I think you mentioned. So we've been talking a lot maybe
4614040	4621400	about the broader definition of cognition of just kind of reasoning, manipulation of concepts,
4621400	4625080	which might, one might even do in a very automatic way, as we were saying, like you might just solve
4625080	4630440	a mass problem without really, you know, in a way where you'd say, oh, yeah, I didn't think about
4630440	4639000	that. I just did it. But yeah, how does one, what kind of things have you, how can you pick,
4639000	4642520	how can you look at this other problem of like when, when people kind of cogitate about things
4642520	4647560	and turn them over in their, in their minds? Where are you going with that? I'm really curious.
4649720	4657160	I think to me, the interesting question here is the question of individual differences. If
4658040	4663720	some people report thinking in words most of the time and others say they don't think in words at
4663720	4668920	all, presumably we should be able to see that at the brain level. Presumably we should be able to
4668920	4674360	see the language network working hard for the first group and not at all for the second group
4675400	4681160	while they're thinking right spontaneously in this task-free setting. And so that's really what I
4681160	4688680	want to look at. But in order to do that, we need to have a good questionnaire that will capture
4688680	4695000	those differences precisely, right? So I think instead of just asking people, although you think
4695000	4699640	in words a lot, they're a little, it would be helpful to think, to get more information, right?
4699640	4710200	Do they think in like, what does it mean? Like, what if other meta assessments of their own
4710200	4716680	thinking style is reliable, right? So like, can we trust those judgments? How can we make them
4716680	4722760	more granular? Another question that I'm very interested in, and that's really understood
4722760	4731400	currently is, is there a difference between thinking in words and hearing the words, right?
4731400	4736760	So if you're using some kind of words and some kind of language to think, does it mean that
4736760	4743320	there is a voice or not necessarily? Some people, it turns out, they might see the words written
4743320	4751320	in their mind's eyes, so they spell it out. It's a minority, like less than half of the population,
4751400	4756040	but it does happen. And the capturing those differences, I think, is fascinating and then
4756040	4761400	trying to look at the neural correlates to essentially establish the validity of those
4761400	4767480	differences to show that they're really not just something that people perceive and report,
4767480	4775000	but actually, that's not necessarily how they actually think. It's an interesting direction
4775000	4785800	because psychology has this interesting history of an interesting relationship with phenomenology.
4785800	4791400	So the people reporting their own experiences, right? That used to be very common, and then it
4791400	4801000	turned out to result in a lot of pseudoscience and discredited a lot of psychology. And so then
4801000	4806600	there was this huge turn to behaviorism where all that mattered was the stimulus and the response,
4806600	4814600	and people were refusing to talk about any internal operations at all. So people are still
4814600	4821160	very suspicious of phenomenology, so self-reporting experiences. And I think for the right reason,
4821160	4825320	because often, yeah, we just don't know how we think. We're like, I think it's words or I think
4825320	4831320	it's not. Sometimes we'll make a decision, like we were saying very quickly. And then when we have
4831320	4837800	to explain what we did and how, we have to rationalize it. And so maybe that's actually not how we
4837800	4842920	arrived at the decision, but post-talk, we come up with an explanation that might not correspond
4842920	4848760	to the reality. So I think we'll have to be careful when taking people at their words.
4849880	4854760	But to me, when people report this strike in differences of like, oh, yeah, I think in words,
4854840	4860920	all the time versus like, never, not at all, it seems like there's something there and so I would
4860920	4865720	love to use neuroscience to get at that question more deeply. Yeah, it's a tricky one. I mean,
4865720	4869160	it strikes me that even the process of asking someone, do you think in words,
4870200	4876440	it almost necessarily linguistic to communicate that because as we say, this is the way that we
4876440	4883080	pass ideas around. And so maybe maybe there's just like that kind of arrogance in the language
4883080	4888440	network, which is going to intercept that question and say, oh, yes, it's me. I do all the thinking.
4889320	4897000	But as you say, well, many people do report thinking in many other ways. So yeah, I would,
4897640	4903880	yeah, I'm really curious about what that shows. I mean, it's just so, this must surprise you all
4903880	4909640	the time just how, you know, outwardly, we sort of walk around and we move around and we breathe
4909720	4915480	and we have all our organs are, you know, working in pretty similar ways. And yet,
4915480	4921880	internally, it might be, you know, we seem so heterogeneous, I guess. Does that sound about
4921880	4927160	right? Or am I overstating the kind of differences in brains that we have?
4929880	4933800	I don't know. I think, yeah, it just depends on your intuition about, you know, how much
4933800	4937240	similarity and differences you would expect. Of course, our personalities are very different,
4937240	4941160	right? Our likes and dislikes are interesting. So at the cognitive level, there are lots of
4941160	4948280	differences between people, of course. And so I guess the interesting thing is that we have
4948280	4954440	this huge differences in how we perceive our own thinking, but they don't necessarily manifest
4954440	4961400	very obviously in differences in this year, right? So in addition to differences in inner
4961480	4969000	speech, another common example is differences in mental imagery, right? So it turns out that some
4969000	4977480	people never experience visual images in their mind's eye. When they're asked to imagine a red
4977480	4984120	apple, they will think about the concept of an apple and like redness, but they will not like
4984120	4991000	see a red apple in front of them when they close their eyes. And so that phenomenon has a name,
4991080	4998280	aphantasia. And the name got coined in 2015. So very recently, really. And this is the phenomenon
4998280	5003640	that kind of got discovered over the centuries at various times and then forgotten again,
5003640	5008440	and rediscovered because again, people just tend to assume that everybody else has the same
5008440	5014120	roughly inner experience with them. And so those differences just end up getting neglected.
5015560	5020120	But it turns out that people with aphantasia, you know, again, you cannot tell them apart very
5020120	5025800	easily from people with this visual imagery. So it turns out that lots of things we do in the world,
5026680	5031640	you can do whether or not the experience images visually. Similarly, whether you have strong
5031640	5038360	inner speech or not, turns out it's you can't really spot these people very easily out in the
5038360	5043000	wild because they act very differently. So that's the interesting thing, right? Despite these
5043000	5048920	experiences being so different, somehow we can still act in roughly similar ways and do the
5048920	5053880	tasks that we need to do in the world. We might be using different strategies, it's very possible,
5053880	5058760	but the end result is that actually, those differences are very hard to see.
5059560	5064680	Yeah, yeah, that is fascinating. Yeah, actually, I have a friend who is an aphant, I guess.
5065800	5073560	And well, he didn't find out until a few years ago. And we, you know, there's no kind of outward
5073560	5079960	sign, right? You just seem completely, you know, normal. But then we're like, oh, yeah, I just
5079960	5084680	can't visualize triangles, right? I know what a triangle is, I can reason about triangles, I can.
5086120	5090440	And actually, often there's, I think there might be some research which shows that that in some ways,
5091640	5097080	oftentimes better at reasoning about certain things, where one might think it requires a
5097960	5104040	visual element. But yeah, he was, you know, a very good physicist, very good colleague,
5105800	5108280	but just thought in a different way, I guess.
5112120	5117960	Yeah, I think we'll find some differences, right? Like, now that there's more awareness,
5117960	5122200	once we start doing more systematic research, I think we'll, like, I mean, there are already
5122200	5128200	attempts, trying to look at the relationship between aphantasia and episodic memory,
5129000	5135960	turned out that aphantasia and, yeah, spatial reasoning, geometric reasoning there, the link
5137080	5143640	is not as strong and may or maybe not even there, even though people expected it to be.
5143640	5147560	And there are variations as to why. But yeah, essentially, like, I think,
5147560	5151320	even though those differences aren't apparent, I think we'll find some eventually.
5151320	5155160	And probably we'll just find out that different people are using different strategies to do the
5155160	5160680	same thing. Some of them might require imagery or thinking in words, you know, speech, and some
5160680	5170360	might not. I mean, all this is a reminder that well, LLMs might be, we might end up producing
5170360	5177080	artificial intelligences, which outwardly look very similar. But we shouldn't, or yeah, we should
5177080	5183000	be careful to think that to be mindful that inwardly, they could be very, very different.
5184440	5191080	And I think it's very true, and it's already happening, right? There are all those cases where
5191960	5199000	people were screenshotting chat GPT responses, especially right after it came out a year ago,
5199000	5204360	and just showing it responding to some very complicated prompt and doing it correctly,
5204440	5208120	and people were so impressed being like, oh, you know, if you have, if you put like,
5209960	5217240	a mail on a table, like, you know, like on on a chair, is that like a construction
5217240	5223000	stable or not? Or, you know, what kind of thing we can put on top? And like, it looks very impressive.
5223000	5229080	And then it turns out that if you change the problem just slightly, then it just starts
5229080	5236360	spitting out total nonsense. And the same thing happened with a social reasoning problem, kind
5236360	5240680	of predicting what the other person would think, which is a classical problem from psychology.
5240680	5246440	And so the claim was that, you know, now LLMs can reason about people and what they do. And then,
5246440	5251480	again, at third dollar, we change the prompt to be slightly different from the problems that
5251480	5258120	were already available on the internet, then model performance drops drastically. So it's very
5258120	5266280	easy to fall for this seemingly impressive performance, seemingly seemingly impressive
5266280	5273960	understanding. And so luckily for us, in this case, there are ways to design even behavioral
5273960	5279960	interventions that can maybe help us figure out what's actually going on and what strategy is
5279960	5285560	actually being used. Yeah, yeah, that's a very important problem for sure. I found the social
5285560	5291800	reasoning example really, I just loved it. So I think if I remember correctly, one of the ways
5291800	5298360	that you fall them is just inserting a few words in the middle. So the classic, the classic one
5298360	5305000	is something like, you know, Susie hides Bob's apple. It was in the closet. Where does he think
5305000	5311080	the apple is? And no, the thing will think it will say, Oh, not in the closet anymore.
5311800	5315800	Correct. But if you change it, say, well, Susie hides Bob's apple, it was in the closet,
5315800	5321400	and she tells him that she hid it, right? And then the LLM still says, Oh, he thinks it's not in
5321400	5330040	the closet because it and I think, yeah, so clearly they've been either on the training set or in
5330040	5337240	the fine tuning, you know, it's gone so far in getting them to kind of give the appearance of
5337240	5342040	having some kind of theory of mind and being able to solve those problems. But then with a little
5342040	5348040	bit of tweaking, it becomes apparent that they they don't. But it seems to be coming harder and
5348040	5358040	harder to, you know, fox these systems. And you point out that one of the real difficulties is
5358040	5364200	we just, you know, without knowing what has gone into the training of open AI's models and
5364520	5372680	it, it's hard to know how much genuine kind of intelligence has emerged and how much is just
5372680	5379160	kind of pan recognition and, and, you know, or simple pan recognition and recall. But of course,
5379160	5383480	the paradox here as well, they have some of the most advanced models. So maybe there's something
5383480	5389720	genuinely interesting going on, but it's black box. So we can't really say. But I think it's
5389720	5397880	encouraging that other folks are, you know, mistrial perhaps are being more open and about
5397880	5405160	what's going into that into their models. So, you know, maybe easier to, to test on the very
5405160	5411800	latest things and have confidence that I don't know, some problem was not appearing verbatim in
5411800	5415960	in the training set. But even then, yeah, it could be some very similar problem.
5416280	5423960	Yeah, for sure. And like, it's not always bad if they've been fine tuned on this problem. And if
5423960	5429880	they've seen examples before, as humans learn lots of things, we can do math right off the bat,
5429880	5434920	we learn, we learn from examples. It's easier for us to the problems that are familiar to us and
5434920	5441160	novel ones, like, that's fine. It's just important to know, because that helps us figure out what
5441160	5446600	is the mechanism that they're using, or potentially using, and what are the keys is where they might
5446600	5454040	break, right? So it's, we don't necessarily need to expect these models to be like amazing,
5454040	5459880	zero shot thinkers on total and novel problems all the time. It's just that, yeah, knowing
5459880	5463240	what goes in the training data, knowing what problems they've been fine tuned on,
5463240	5468840	just helps us assess them more accurately. Yeah, yeah, that's a good point. It's unfair to
5469800	5473800	sort of demand development so that they don't learn anything, or they don't benefit from fine
5473800	5482680	tuning. But yeah, but we want to know if they are, if the way that they're responding is using,
5483960	5487960	is that they've abstracted patterns, or they've just, they're just regurgitating.
5490360	5496840	Yeah, well, I, yeah, I found it really interesting to go through your work. I'm usually,
5497800	5505160	I don't know, optimistic, I guess, or maybe not, maybe that's the wrong word. I really admire how
5505160	5513720	LLMs are, have taken off, and it surprised me how quickly they've advanced. But one thing I've
5513720	5518280	enjoyed about your work is kind of reminding me, well, actually, maybe they're not as far along in
5518280	5523960	some ways as, as they appear to be at the surface level. Like there still seems to be some,
5524920	5529640	some nuts to crack here to make them, to bring them closer to, to human thought.
5531000	5538120	What's your take? Do you think, you know, I don't want to ask the typical how far away is AGI,
5538120	5547240	because, but, you know, are you of the opinion that just cranking through more data
5547880	5555960	is going to continue to produce results, or should more be invested in this kind of modularity
5556600	5565960	approach? And if the latter, well, do we have, you know, are the things that are taking place
5565960	5570920	on the right track, or do we need to look more closely and learn more from the human mind, perhaps?
5571000	5582120	I think we definitely should recognize all the impressive achievements that we observe in LLMs
5582120	5589640	today. And one way that my colleagues and I have been thinking about it is through the formal
5589640	5596120	functional competence lens. On the formal competence front, learning the rules and patterns of natural
5596120	5604120	language. These models have been incredibly impressive. So even a couple of years ago,
5604120	5608440	they almost reached the ceiling for English, at least the language where they have the most data.
5609160	5616120	And they did it without the need for any fine tuning. So just learning to predict words and
5616120	5623880	method corpora of text. Turns out that is something that gives you all the most of the grammar and
5624520	5631400	knowledge of idioms and all kinds of patterns that characterize a language. And that wasn't trivial
5631400	5636760	at all. That was the subject of debate for linguists for decades. Is it possible to just learn from
5636760	5643400	data? Or do you need a rich set of internal rules that can help you figure out what's grammatical
5643400	5649480	and what's not? So that is incredibly impressive scientifically. And on the engineering front,
5650360	5656280	different language processing systems in the past have struggled so much because you just can't
5656280	5662280	encode language with a few simple rules or even not simple rules. Just like so fuzzy and there are
5662280	5666280	so many exceptions in the regular forms and this and that. And the fact that these models have
5666280	5670120	mastered that is so impressive. And people kind of forget and start talking about AGI right away,
5670120	5674840	but that's an impressive achievement. Then being good at language is already very impressive.
5675800	5680760	And then we get to functional components and their ability to reason and be factually accurate,
5680760	5685320	know what's true and what's false and be actually helpful. And so that's a whole other host of
5685320	5693880	problems where they actually seem to be spotty. They have achieved a lot because of pattern
5693880	5698520	recognition, but then it turns out that that performance is not robust and it breaks. And
5698600	5708440	so that's where it gets more complicated and more controversial. And that's where we argue modularity
5708440	5715400	will be helpful. Again, looking at the human brain as an example. And one distinction we make though
5715400	5724200	is that the modularity doesn't necessarily have to be built in by design. So this built-in approach
5724200	5729720	we call architectural modularity where we have a language model and let's say a path and code
5729720	5733640	interpreter and we put them together and they're clearly different and they're doing different
5733640	5739160	things. So that can be promising, but then of course you need to know what the right modules are,
5739160	5745080	you need to set them up in the right way. An alternative approach that might work for certain
5745080	5751240	cases is what we call emergent modularity where you start out with one network, but you don't
5751320	5756200	necessarily specify what parts need to be doing, what you let the network figure that out over the
5756200	5761000	course of training and you can have different parts self-specialized to do different things.
5761640	5766600	That might require some changes to the architecture to be able to promote the kind of
5766600	5772360	specialization. It might require changes in the objective function, maybe next work prediction
5772360	5781080	alone is not necessarily going to be good. And it might require changes in the training data
5781240	5788120	kind of like what's happening with fine tuning today where you are feeding its specific problems
5788120	5793000	that you're asking the model to do in a specific way so you might have selectively boost the
5793000	5797480	social reasoning and the formal reasoning and the factual knowledge. There might be specific
5797480	5805080	things you need to do, but there is a lot of promise in these approaches and the paper where
5805080	5811160	we introduced the formal and functional competence, it's something we started working on in 2020
5811480	5817720	around the start of the pandemic. Language models were around then, but not nearly as advanced and
5817720	5823880	as we were writing the paper and in fact after the initial preprint version came out, that's when
5823880	5830280	we started seeing the field, the developers shifting away from this simple scaling up approach,
5830280	5835320	that's not the approach that's common anymore. People have started to shift towards specialized
5835320	5841320	fine tuning, using very targeted data sets to improve performance on specific domains,
5841320	5846440	coupling an LLM with external modules, all of those things that we kind of suggested that might
5846440	5851320	be good because that's more brain-like. RAG became big, all of those things are something we've seen
5851320	5858440	over the past year that's very encouraging that now the AI field is also recognizing that it's
5858440	5862200	not just about scale, that you do benefit from different components working together.
5863160	5873480	Yeah, I think very exciting to see what comes next, both in your work and in the field of LLMs,
5873480	5879480	which it seems like maybe someone's listening to what you're suggesting because all these things
5879560	5889400	are happening. Yeah, I don't know if you have any final comments or predictions,
5891960	5902840	warnings of doom often come up in these discussions, but this has been surprisingly positive.
5903160	5910520	No, I just think that science is important and we just need to use good methods and not run after
5910520	5916840	the hype and be realistic in how we evaluate the strengths and limitations of these models.
5917720	5921720	There are strengths, there are limitations, so being too far on just the positive and just
5921720	5926360	negative is not necessarily the most productive. We just want to be able to disentangle them
5926360	5933720	effectively. Yeah, Rene, thank you so much, Anna. This has been really insightful. Yeah, thank you.
5956360	5957740	you
