{"text": " I'm James Robinson, you're listening to Multiverses. Language can do and express many things, and in fact this was the subject of my last conversation on this podcast with Nicol Krishnam talking about ordinary language philosophy. Just because language is so powerful, we might be tempted to think that that's all we need for understanding and predicting the world. It's just manipulation of symbols, next word prediction. However, if we look at the how the human brain at least actually works, it's rather different. Our guest this week is Anna Ivanova. She's an assistant professor at Georgia Institute of Technology, and she tries to understand the relationship between language and thought, and she does this by looking at brain scans, essentially MRIs, of what's going on when humans are presented with particular scenarios. For example, I'm looking at this marvellous view right now from Carlton Hill, Edinburgh. I'm not thinking about it linguistically, it's going straight into my visual cortex and processes happening there, and if I want to reason about it, I'm not going to reason about it linguistically either. A lot of her work looks at how conceptual knowledge of the world is not tied to the language area of our brain, so for example subjects with aphantasia, people who have large-scale damage to the language network, are still able to reason not only logically about chess problems and things like that, but they can reason socially as well. They can understand what situations are unusual. So this is a really insightful and very timely conversation because it plays into a lot of the enthusiasm about LOMs, which I certainly buy into myself, but it calls into question some of this, forcing us to think, well, what is necessary on top of simple linguistic abilities to really be a fully-fledged thinking machine? I think one question that still exists in my mind is, to what extent just language manipulation could get us to a fully thinking machine, somewhat in the same way as, you know, I have a GPU and a CPU on this laptop, and I could use my CPU to play, you know, vector-based computer games, and I could use my GPU to send emails, but they're not really suited to that task, but the point is maybe language could be a kind of fully-fledged thinking system. However, I think it is valuable to learn from, in fact, what the brain does, of course. So I really enjoyed this conversation. I hope you did too. Hi, Anna Ivanova, welcome to Multiverses. Hi, thank you for having me. So we're speaking and we're thinking, I think, and people who are listening are listening to our words and they're thinking, and thought seems like something that should be really familiar to us, because it's one of those kind of few things that we have really direct access to, and yet, at the same time, it seems so mysterious, so hard to figure out exactly what's going on. Maybe it's because the piece that's doing the figuring out is the thing that we're trying to figure out itself, I don't know. But yeah, how can we get some sort of grip on what thought is? Where do we start? Well, I think we need to start with definitions, what it is that we mean by thought, because different people use the word in different senses, and of course, it also depends on the context. And generally speaking, at least in my area of work, I think there is the broad definition and the narrow definition. And in the broad definition, thought is synonymous with cognition. So the mental processes that we use to make sense of the world around us, so that includes reasoning, that includes accessing memories, that includes various social communication capacities, so very broadly speaking, something we would call cognition. And then the narrow definition is the stuff that happens kind of like in between us doing things. So it's not necessarily you get a math problem, your reason about it, you give the result. But it's more about you lying down in bed at night and thinking about your day tomorrow, or you're walking somewhere and you're playing out your conversation that you're going to have with a friend. And so this kind of inner thinking that happens spontaneously, not in response to any external task is also something where people that people commonly refer to as thought. And of course, those are very different, right? Like if we're talking about the broad thing or the narrow thing, this specific kind of mechanisms that might support them might differ quite a bit. Yeah, I think both of those kind of definitions capture something of what one intuitively would characterize as thinking. So we might say, you know, broadly speaking, oh, yeah, of course, when this person solved that math problem, they had to be thinking. But then you might also say, oh, they did it so quickly, they just did it without thinking, which would be sort of more of the narrow definition, they did it sort of without any kind of reflection, which is kind of what the narrower definition requires, I guess. I'm curious, do you have a kind of preference for either of those definitions, or do you think they both serve a kind of useful purpose? I've used both. I'm interested in the relationship between language and thought, the role that language plays in thinking. And so the role that language plays in thinking broadly defined thinking as cognition, that actually turns out to be a more tractable question, because we can ask people to do a math problem and look whether language processing regions in the brain are engaged. But for inner thinking, stuff that happens spontaneously without an external task, that's much harder to capture. But that's also where people have very strong intuitions there. Of course, I think in words, or of course, I think without words. And so that area is harder to study, but also very interesting. And so that's where I see some of my future work going. Interesting. Yeah, it's true that kind of more reflexive thinking almost by definition, people are going to have opinions about it, because they are kind of cogitacing, turning things over. And part of that process is inward looking. So yeah, people are going to be like, oh, well, I always do that with words or with images or a mix. And yet sometimes I can also find it a little bit hard to remember to do that and think about, because whenever I sort of think about, if I try to think about what thinking is, I will do it linguistically. But maybe that's just because it's the sort of, that's the sort of way that I need to think about that thing. Whereas if I think about something entirely different, like a kind of, you know, spatial reasoning problem, I'm sure I would do it in a different way. But it maybe wouldn't engage the linguistic part of my brain. But anyway, I guess, yeah, this is a very, we've got very quickly to a very key area, which is this, this, this relationship between language and thought, which my last guest was Nikol Krishnan, an ordinary language philosopher. And we spent some time just talking about, well, how for a while, people just thought there was no difference between the two. There was, or rather, in some way, language captured the entirety of everything, including thought, maybe including some other things. So, you know, Wittgenstein's famous dictum, the limits of my language, the limits are the limits of, or mean the limits of my world. But I guess your research is maybe questioning that. Would that be fair to say? Yeah, I use Wittgenstein's quote as an example of a worldview, a paradigm that I'm pushing against. And what's wrong with it? I mean, as we said, like, when we try to describe what thought is, we'll probably reach for language. And yeah, it seems like so many of the ideas, I don't want to say everything, but so much of the ideas that we have, and we pass on, we do with language. And maybe there's an argument that the places where we're not doing it with language are somehow dependent on language behind the scenes. But that's not a very scientific argument. And I think you have some kind of evidence to the contrary. So, yeah, maybe take us through some of the things that you've done to probe this. Yeah, let me, there is a lot to say. So, let me start first by, I guess, acknowledging the last bit that you said, where clearly, there is a relationship between language and thought. And the most trivial, but also important one is the fact that we use language to transmit information, to communicate thoughts to one another. And that's a very powerful mechanism, we can translate knowledge from generation to generation. So that is a very important role of language in thinking, helping us share information without having to figure out every single thing individually. But here, what we're talking about is using language as a medium of thinking. So internally, do we think in words, do we recruit the mechanisms for language processing when we're thinking? And so that's an important distinction. So that's the scope. Now, as we said, people have strong intuitions about whether or not they use language to think. And probably that these intuitions are grounded in people's personal experiences thinking. And so one important fact to keep in mind is that there is huge individual variability in how people perceive their own thinking to be. And so my pet theory is that a lot of philosophers are strong verbal thinkers, they spend a lot of time writing, they think about abstract topics. And so to them, the link between language and thought and their experience is very strong. And it just seems that people, not just philosophers, have this tendency to assume that everybody else thinks in kind of the same way. And so if you are a strong verbal thinker, you automatically assume that everybody else is as well. Until you start actually talking to other people and asking about their experiences. And so I've had these conversations with people at parties or just informally, you ask them, oh, hey, how often do you think in words? Most of the time, some of the time never. And people are always surprised to hear that other people's experiences might be completely different. And so this is just, I think, a very important thing to keep in mind that our intuitions can lead us because they don't necessarily reflect a universal human experience. It's just, you know, that's how we think. And so on to the actual evidence that we can use to dissociate language and thought. There are a few different strands. So one example, very briefly, is the fact that animals who don't have language might often have pretty sophisticated thought and planning capabilities, right? We know examples of crows being very smart, alphoctopi, even a squirrel that is trying to figure out whether to jump from tree to tree or if it's too far and it needs to go on the ground instead. These are pretty sophisticated capabilities. And so that's just a very basic example of at least some kinds of thought. You can then argue, you know, oh, but like the kind of thinking they're doing is not, you know, the kind of thinking that we care about, right? And that's where the meat of the debate is. But pretty sophisticated cognition is possible in non-human animals from what we know. For me, I work with humans and adult humans. And so what we can do is we can identify parts of the brain that are engaged in language processing. So it turns out that there is a set of brain regions in the brain known as the language network that are responsible for language comprehension. So whether you're listening to somebody talk or reading, they're also engaged during language production. So when you're speaking and when you're writing, they are engaged in response to any language that you might know that also includes sign languages. So it doesn't even have to be a spoken language. And these regions turns out are pretty selective for language. So they respond to all kinds of language, but not to music, not to math, not to general problem solving. And so this is pretty strong evidence that language and many different kinds of thinking are actually separable in the brain. That language has its own neural machinery. And that's important because it turns out that if language areas of the brain are damaged, it will affect your ability to use language, but not necessarily your ability to think. And so the most common example of that is a condition known as aphasia. So it difficulties with language production or comprehension. Often, most commonly, it arises as a result of a person having a stroke. And so if it's stroke effects left hemisphere, which is where the language network is in most people, they might have really serious difficulties with language production or comprehension. But if that language is limited to the language network, it turns out that their ability to use other kinds of thinking remains intact. So these people with really severe aphasia who really can't understand language or speak, they can solve math problems. They can arrange pictures so they form a story so they can reason about cause and effect. They can look at the picture or show in some kind of event, like a fox cheese in the rabbit or the rabbit cheese in the fox and say which one is more likely to happen in the real world. If it's something like really weird, like the scuba diver biting a shark, they will laugh because it's just kind of ridiculous. And so, you know, you can tell that they understand what's going on. And there are really fascinating cases, you know, some of them like like the plague chest on the weekend. So clearly, very sophisticated forms of reasoning are preserved even in the face of severe damage to language processing centres in the brain. Yeah, that's completely fascinating. And well, firstly, by the way, I love your theory about philosophers and how maybe the sort of minds that they have that make them good philosophers sort of self select or selecting a very biased or unusual community of people who think in a particular way. And yeah, so that's really interesting. It'd be great to have a survey of philosophers, I don't know if this has happened, and how they describe their own thoughts and compare it to other groups. Yeah, very interesting. Yeah, I think sometimes that maybe I should, you know, not talk about this theory and actually test it experimentally first, that we add on biased people in advance. No, as long as they don't listen to this, or maybe you can do it anonymously or something. I'm actually talking, I think, soon to someone who from the philosophy of science who surveyed physicists to see if they are realists in terms of, you know, how they think about the entities of science or not. So I think it's like it is really interesting to actually just, yeah, try to figure out how it is that yeah, how it is that people's personal beliefs and their kind of academic disciplines or their own or the peculiarities of their minds intertwine. But coming back to the kind of experiments that you describe, so yeah, I think these are just really, yeah, wonderful illustrations of how thought maybe extends beyond the language network. And I suppose what you're doing is you're asking people, so for example, the way that we know that music is not within the language network is, I don't know, the language network is defined as the kind of a bit of the brain that lights up in MRI scans. You see a lot of activity there when you give people sentences and linguistic tasks, I don't know, maybe reading or producing language. And then it's a different area of the brain that lights up when they're listening to music or when they're solving a math or chess problem. And even when people have quite severe damage at South Asia, and the language part of the brain is unable to comprehend or produce language or both, they can still do many of those other things, which is, I mean, that's really interesting for one thing, because I often think as well, language is maybe being so key to the kind of input output of the brain that, you know, for example, reading a math problem would kind of go via the language network, or is it that our brain is sort of able to just kind of directly take those symbols into, I don't know, a different area of the brain, or perhaps do we have to kind of pose those problems in a kind of more visual way? I don't know, I'm curious about whether the language network is kind of a gateway for much of the information going in. So it looks like if you give people math problems in the form of mathematical symbols, right, like five plus three, first question mark, it seems like it doesn't need to go through the language network. So even though it's symbolic, not all symbols get processed by the language network. And perhaps even more strikingly, one study that I did in graduate school was looking at computer code. And specifically, we looked at Python, which is very English-like by design, so it uses English words. And on the other end of the spectrum, we took a graphical programming language for kids called Scratch Junior. So it has different characters. And so then you have different arrows showing, you know, the characters going left or jumping. But it has a lot of the same control flow elements that you would have in text-based code, like if statements and for loops and stuff like that. And so it turns out that for both of these languages, the main network in the brain that's responsible for extracting meaning from that code is the so-called multiple demand network. And so that's the network that's responsible for problem solving and reasoning and planning, and not the language network. The language network responded a little bit to Python code. But even there, we actually weren't able to exactly establish its relation, its role and why it would. It might be some kind of false positive where the language network is like, oh, that's language, oh, wait, no, never mind. And it kind of goes down. So there are other researchers that are promoting that theory currently. But even for code, we call programming languages languages because how similar they are structurally to natural languages. Even there, it looks like it's not the language network that's doing the majority of the heavy lifting. Yeah, I found that completely, well, surprising actually. And I think you noted in the paper that people kind of fell on two sides of the fence. Some people were surprised and some people were, oh, no, that makes complete sense. But I was personally really surprised because I, as you say, there's so much similarity between the way that natural language works in terms of being compositional and having these kind of hierarchical features and the way that programming languages work that I would think, okay, well, you know, it's right that we call them languages because they're so closely related, they're just sort of, I guess, a bit stricter, less ambiguous, perhaps. But the kind of, the nature of the rules is not so different. And yet, yeah, it's almost as if one could imagine there being some sort of animal, like a crow, like you said, like very intelligent creature, doesn't have language, but maybe it's got a really good multiple demand network. And perhaps we could, perhaps could be a really good programmer, because it's not that part of the brain, which is being recruited. But it's rather this kind of almost clearinghouse from what I understand, the multiple demand network just picks up so many different jobs. The other thing that really stood out for me in this paper, which I really enjoyed, was that as a kind of, I guess, control, you could, you presented people the same problems. So the, if I remember rightly, one of the, one of the code pieces of code that people had to interpret in Python was a calculation of BMI. And so it's like, here is a variable, which is your weight, here is one issue is your height, BMI equals height divided by weight squared. And so the person kind of reads through that. And, and you see it being passed off to the multiple demand network. But then there's the same problem defined entirely verbally. So instead of using, you know, symbols with equals, and it clearly being Python code, it's just, I know, this is what BMI is, here, you know, here's how much you weigh, here's how tall you are, what's your, what's your BMI. And that went to a different region of the brain, which for me was just like, okay, well, this is the same problem, but the way that it's presented really changes the way that we think about it. Which, yeah, that was another huge surprise for me to think just how influential the kind of presentation or the, the medium, I guess, for a set of concepts, how much that, that determines how those concepts are handled internally, mentally. Yeah. And in fact, that's not that uncommon of a situation if you think about it. So let's say somebody is listening to this podcast, versus reading the transcript, the way information gets into the brain is different. So in the auditory modality, it goes to the auditory cortex first. And in the visual modality, it goes through the visual cortex first, and then it gets into, we have a specialized part of the brain that's responsible for readings of recognizing written letters. But then they will converge in the language network, because language network is responsible for processing either a modality. And that means that these initially distinct representations have to converge in some, in some way. And so for some of the other cases, like a problem that's written in language versus in code, it looks like that convergence is also happening, but it's happening later on in the processing, right? So it goes, you know, through the language network, and I guess the multiple demand. And then you have some shared problem solving. So in this case, calculating the BMI is doing some math. And so that we think also happens in the multiple demand network. And in fact, we show in the paper that you can kind of break down that activity that we capture into the code reading part, an actual problem solving part. But it's a fascinating endeavor. In general, in cognitive neuroscience, how do we design an experiment where we have those kinds of different conditions, where they're very similar, except for something that we've changed. And so at what point that difference, right, auditor versus visual language versus code, where in the brain does that make a difference? And where doesn't it? Yeah, yeah. So you're saying that even though it goes the language area lights up when we have that kind of BMI problem, it's just kind of passing the thing. And then it gets passed off to you know, to actually run the calculation that happens, like that doesn't happen in the language area. Yeah, that makes sense. Yeah, okay. That clarifies my, I was very excited. I thought that maybe that we had some sort of like way of doing the confrontation, just linguistically. Guess that's what that doesn't work. I think it's possible. And we don't well, I don't know. Well, maybe not linguistically. But like, you know, we memorize the multiplication table, or for like some problem that we do very often, we don't need to actually go through the steps of the calculation, we kind of just retrieve the correct answer. I don't know if it happens linguistically or not, potentially not, probably not. But it's still a different mechanism than actually going step by step and doing, you know, long division in your head, or like summing multidigit numbers or something like that. Yeah. Yeah, I think that that's a really interesting question, which we can maybe come back to. But I guess, yeah, so you kind of see both the multiple demand and the language network lighting up when this problem is presented linguistically. So it's sort of a fair assumption that what is in fact happening is that, you know, there's probably some linguistic processing, but then it then gets passed to the same sort of area of the brain, which is, which handles the pure Python problem. But of course, yeah, I mean, that is really interesting and kind of useful in some ways, in that, you know, it seems more efficient to be presented just with the Python code, right? You kind of bypass that. Oh, I turn this into, you know, it goes straight into the, the system, which can perform the ultimate calculation, I guess. I don't know if you, if you were able to capture any information on whether it was quicker for people to kind of solve the problem when presented with the Python code or not. I don't remember whether we saw a difference in how long it took people. I think it's possible, but I don't, some of it, of course, depends on how proficient they are in Python. So there might be individual differences there, also individual differences in how fast they would read text. So I'm sure there's some variability there. But it's actually an interesting thought that you bring enough so that this, having this abstract skeleton with other information stripped away might make the problem solving the calculation easier. Because in fact, there are cases where researchers have observed the reverse, though there is this famous Waste and Selection task, which you have, let's see, a card with like a two and a seven, and then a, and then a card with like green, red and blue. And you need to test the rule that says if the number is even, then the other side of the card has to be blue. And so then the question is which cards do you need to turn over to make sure that that rule is correct. And so then people want to test the card with the two on it because it's even, and so they want to make sure that their reverse is blue. But then they often want to turn over the blue to make sure that the other side is odd, sorry, is even. But that actually is not what you should do because it doesn't matter. Like if you have blue and odd, that's actually not a violation of the rule. What you need to do is you need to turn over the red card because of an even number there, then that rule gets violated. So that problem is hard for people. But if you cast the same problem saying that there are people at the bar and somebody, you know, is 16 and somebody is 25 and then somebody is drinking beer and somebody is drinking a Coke, then, you know, how will you verify that only people over the age of 18 are drinking alcohol? And then of course, you know, that you need to, you know, check the 16 year old and check the person drinking the beer and not any other way. And so mathematically, the same exact process, but it's much, much easier for people to ground the rule and their existing knowledge, not necessarily the bar example, but they're just kind of, you know, the easiest one and the most common one. And so this phenomenon is known as content effects on reasoning. And yeah, I think a lot of people, especially like, you know, physicists and mathematicians and so people trained in like hardcore STEM discipline, they're like, hey, Alex, trip away, all of the extra information only focus on the abstract symbols. That's the easiest thing. But actually for a lot of people grounding the problem in some specific content domain tends to help. And so I know that some people in like math education are very interested in this phenomenon and how does it how to make it easier for people to, for kids to learn math. Is it by focusing on the abstract or is it by grounding math problems in real life situations? And I suppose part of the reason why that grounding might work, well, there could be kind of two hypotheses. One is just like, it locates it in a different area of the brain, which is somehow better at processing this thing. So maybe that just the social reasoning part is just better at doing that kind of problem. But doesn't seem so likely in this case. And another is just that it gets it, it clicks it in to a place where you're able to recognize a pattern that you've seen before. And so you don't have to do, you know, you're already on the right track. And this maybe kind of comes back to your point about, well, maybe when we calculate the BMI for certain kind of combinations of numbers, you just know the answer. So it's being kind of recalled from memory, like that pattern is already so established that you don't need to reason through it in the same way. It's more of a recall operation. And I mean, this is getting us towards one of the kind of central questions, which is around, well, what are LLMs doing? Because they're kind of glorified recall machines in a certain way, or just really good pattern matches. Maybe before we get to that, though, I want to talk about another of your experiments, which I really enjoyed, which is about where people are looking at images of improbable and probable things like the shark and the swimmer that you mentioned. And what I found, well, maybe you should describe the experiment, you'll do a much better job of it than me. Because I think, yeah, there was just a really interesting piece here that kind of writes this. Yeah. So here, we use that same idea that the same information might arrive in the brain through different routes. And so in this case, we were looking at sentences describing basic interactions between two entities, like the, I guess we can roll with the shark bites the swimmer, the swimmer bites the shark, and pictures depicting the same kinds of events. And so here, by switching around, who's doing what to whom, we're manipulating whether the event is plausible. So likely to occur in the real world or impossible. So unlikely to occur. And the question was, does the language network respond to language specifically? Or does it respond to meaning and concepts more generally? And so if it's language, it should only really respond to sentences and not to pictures. And if it's responsible to meaning, it should respond equally strongly to sentences and pictures, as long as the person is thinking about the meaning. And so we had people tell us whether they think the event is plausible or impossible. So you have to be thinking about the meaning. And so what we found was actually something in between, where the language network, in accordance with all of the prior work, responds more strongly to sentences than to pictures. But it still responded to pictures to some extent. And I will say that in another study, we recorded responses in the language regions to pictures of objects. So is this animal dangerous? Can this object be found in the kitchen, that kind of stuff? And it did not respond to pictures of objects. So it was something about events, maybe just something more complex, maybe something just more fast-paced, that was specifically triggering responses in the language regions. And so this intermediate result, so preference for sentences over pictures, but also responses to meaning, even in known sentences, was kind of puzzling. And so one piece of evidence that helped us make sense of this information was evidence from individuals with global aphasia, from people with brain damage. And I should say that this is, yeah, so lots of the brain imaging work I'm describing, I did with my PhD advisor at Fedorenko, and the global aphasia bit is done in collaboration with Rosemary Varley at UCL, who works with individuals with global aphasia very, very closely. And so here we had two individuals with global aphasia, really serious issues, looking at pictures of swimmer by sharks, shark bite swimmer. And so as I mentioned earlier, they were laughing at the weird ones. And so in general, they were very good at distinguishing plausible and implausible pictures, suggesting that their ability to extract meaning from pictures was there, it did not require a functioning language network. And so then the title of the paper, the language network is recruited but not required for pictorial events and semantics. So we see this activation, and we, but it's not, it's not necessary to do the task. Yeah, yeah, I found that, yeah, very insightful. And what struck me was one of the hypotheses as to why the language network was recruited is that it's sort of another way of getting evidence or information on whether this event is likely or not. And of course, we can't be sure what's going on in there. But, you know, perhaps it is somewhat like a large language model where you, you look at the thing, you're like, part of trying to figure out whether this picture is likely or not is you kind of read it out to yourself and like, well, does this, is this a familiar pattern, right? Does it, is, you know, shark bites swimmer, you know, that's that's that sequence of words feels close to sequences of words that I produced before or read before, whereas swimmer bites shark is kind of jarring. And maybe behind that is just the improbability of that, that, that, that sentence being produced according to the language model in our, in our own minds. And of course, yeah, we, we don't really know that our minds work like a large language model at all, but it's an attractive hypothesis in as much as it works. Yeah, so I guess, so generally speaking, right, so we got this result language network recruited but not required. And the question was, what's going on, right? And so generally speaking, we consider two broad hypothesis. One is that that activation is not necessary to do the task. So you see the picture, sharks, swimmer and biting. And so you activate those words kind of automatically by association, but you're not actually using them to reason about whether the event makes sense or not. So that's one hypothesis. And the other hypothesis is that actually, the information in the language network is helpful. But when you're trying to recast information that you're seeing in linguistic form, you can then compare it with all of the linguistic information that you received in your lifetime. And maybe that information ended up distilled in your brain in some general way, just kind of, we know that people are very sensitive to statistical regularities in language, we know that we're very good at predicting what word would come next, right? Like there is this information about what patterns are likely in text is very much what people use during real life language comprehension. And of course, that information also can help us, in many cases, figure out which events make sense and which doesn't. And so actually, we try to test that hypothesis. We didn't necessarily, it's hard to test that in actual human brains, although we now have ideas for how we might be able to do that. But we started by using language models as proof of concept, right? So the hypothesis is statistical patterns in language input can help us distinguish plausible and implausible events. And language models are very good at capturing these statistical patterns. So if language models can systematically distinguish plausible and implausible events, that means that there is enough information there where maybe humans might be able to use that information also to distinguish plausible and implausible events, right? So it's not evident that humans do, but it's evident that humans can. And so we did that, we use language models and try to see whether they systematically evaluate plausible event descriptions as more likely than implausible. And so in that study, we specifically distinguished between two kinds of events. So one is the teacher bought the laptop versus the laptop bought the teacher. So animate, inanimate interactions. And so when you swap them around, if you interpret the sense verbatim and the inanimate object laptop cannot buy anything, buying requires that the subject is animate. And so that's a very kind of in your face screaming violation. And then the other example is kind of like the fox chased the rabbit, the rabbit chased the fox or the swimmer by the shark, the shark by the swimmer, right? The swimmer by the shark is not impossible. It can happen. It's just way less likely. And so what we found is that when it comes to distinguishing possible and impossible events, language models were very good, almost at ceiling. So that was actually very easy for them. But when it came to likely versus unlikely events, there was a gap in performance. So they weren't quite as good. They were okay. They were above chance. But they definitely weren't perfect. And so we're not as good as people, I guess, right? Yeah, not not as good as people and not as good as when they have to deal with animate inanimate sentence, right with impossible events. Yeah. So I think it's like, to me, it's actually more interesting to compare those two sentence types, like how models do on them. But also humans humans do well on both because these are easy sentences. So they're not meant to be challenging. And so, yeah. No, I was gonna say, and do we take that as evidence that then when humans reason about these things, they're doing it, not just linguistically, or is it that our language models are kind of like better than the language models that that are out there in, you know, the computer language, large language models? I think that I think it's evident that humans are doing it not linguistically. And so the reason why we think there is this performance gap is because actually, the language input that we receive doesn't faithfully describe the world around us. So when we talk to each other, we don't just passively describe everything that we're seeing. So I'm not telling you, you know, I am sitting down, the lights are on, the room is empty, like it's very boring stuff. I'm telling you about things that are unusual, novel, interesting, newsworthy in some way. And so this phenomenon is known as reporting bias. So language tends to undercover on the report information that is kind of trivial, right, that everybody already knows, or can reasonably infer. And so maybe actually events that are unlikely are not as unlikely for LLMs, because, you know, we talk about unlikely things all the time, that's the stuff that's worth talking about. And so if that's true, that's the reason why we see this performance gap, then even if the human language model is very good, which by the way, we don't think it is actually, I think large language models now are much better at predicting the next war that humans are. So actually, they're better. But even if humans were really good, just the language input is insufficient for us to be able to distinguish plausible and implausible events. That means that we have to use something else in addition, we have to maybe have some more sophisticated model of the world, where we can actually correct for this reporting bias, we have to also bring in information that's about what things are typical, what we can expect, what we cannot expect. So we're probably drawing on sort of multiple mental resources or systems. In the case of the clearly kind of impossible, so computer bias teacher, is it just the language, can you see if it's just the language, or have you seen if it was just the language network that's recruited there, as one might think, well, if it can just be done within the kind of the one region, maybe it's more efficient and metabolically, there might be some kind of preference for doing that if it were possible. By default, we kind of light up various regions just to make sure I don't know. So that's actually a study that I would love to do next, so this difference between impossible and unlikely events is something that emerged out of this language model study. And so now, of course, yeah, I think it would be great to bring it back to the MRI machine, measure people's brain activity in response to impossible versus unlikely centers, and see if the language network alone is sufficient for distinguishing possible and impossible events. That is the prediction that follows from this language model work. And so I would love to test that. Yeah, yeah, that would be so, yeah, I'd love to see the results of that. Yeah, so I hope that that happens. But I suppose, you know, coming back to LLMs, what we're starting to see is that maybe just a large language model in itself, for various reasons, might not be so effective at thinking or reasoning as the human brain. And one is, as you kind of mentioned, that the data that comes in is kind of biased toward the salient and newsworthy as you put it. But then another from the kind of Python example is that, well, as a matter of fact, we don't use the language part of the brain for code comprehension or for logical mathematical reasoning, either for that matter. I suppose my question there is, though, you know, could it be possible for LLMs to kind of just be be able to take on the functions of the multiple demands network, for instance, which is doing all this, which is the place which does the mathematical logical Python code interpretation comprehension? Could it kind of take on all those responsibilities just by having getting really good at saying, you know, next word prediction for mathematical problems and next word prediction for code generation and so on? Or is just that kind of, or is that implausible? I don't really know how we characterize, you know, where the LLMs just could have kind of emergently develop all those capabilities within a single language model, or if that's just very, very unlikely. Yeah, so LLMs do a bunch of different things. In general, as you mentioned, they're very, very good at pattern recognition and pattern completion at different levels of abstraction. So they do a lot of just direct memorization, right? The larger the model, the more texts that can just memorize straight up, which is why a lot of those copyright issues end up arising. But that's not the only thing that these models do, because they definitely are capable of generating novel texts and mixing and matching previous inputs. And so the patterns that they can recognize and reproduce, they can be fairly abstract. But then, of course, the question then is pattern completion all it takes? Is that the only thing that's necessary? And so that's where it gets tricky, because a lot of logical reasoning is algorithmic reasoning. It's symbolic. It's very regimented. And so these are the kinds of problems where these models seem to struggle. So for example, if you ask them to add and multiply two numbers together, if the numbers are small enough, then the model is doing just fine. But if the number is large, that means it wasn't part of the training set. It means it couldn't have just memorized the response, which it probably does for a lot of smaller number combinations. And so then it would actually have to multiply step by step. And it doesn't seem to be doing that very successfully. In fact, it often gives you a number that's close, but just a little bit off. And so the kinds of mistake that it's making is different from the kind of mistake a human would be making, because it's still trying to use pattern matching to complete. And it's not quite working, it seems. Yeah. But I mean, it's very hard to figure out exactly how they're doing what they're doing. I mean, they've got so many parameters. And it's surprising how good they are yet still imperfect at doing those sort of problems. They're kind of like a broken calculator. So they're much faster at getting to an answer, but it's not quite the right answer. It's a pretty good estimate often. And yet it's not completely out. So it's really... Yeah, I don't have a strong opinion, but part of me thinks, well, maybe they'll just kind of, with enough data going in, they might just crack that. That might come a point at which that kind of ability emerges. Although you point out in one of your papers, though, well, perhaps if that ability emerges, it might be that a particular kind of architecture that models the human brain emerges as well. So it may not be that... It might be happened in such a way that it becomes less fruitful to think of a large language model as simply a model of language, but something that has a kind of linguistic language network part like the human brain and then hands off to a logical part. And as it happens, obviously, in chat GPT, without that has had that kind of architecture imposed on it, at least in the version with the Python code interpretation, for instance, because you can say, well, add these two numbers together, and it will figure out, oh, well, I'm doing a math problem here. So I'm going to convert this into a Python problem, and then it runs the Python code. So actually, you know, some of these problems seem to be are being sort of addressed, I guess, by the developers. But the way they're doing it is, yeah, offloading. Yeah. Yeah. So I guess let me unpack a little bit. There's a lot there. So first of all, it is very tempting for people to over ascribe intelligence to a language model. And presumably that's because in our everyday interactions, we're used that language gets generated by a thinking feeling being other humans. And now we have a system which is breaking that relationship where we have something that generates coherence language that's not human. And so it gets confusing. And that's the reason why that's one of the reasons why there's so much hyper-intelligent language models, and they're expected to be the general intelligence models, because of this tight perceived relationship between language and thought. And so then when they make a math mistake, or they make a factually inaccurate statement, you're like, oh, no, like how, you know, these models are terrible, they're not terrible, they're just like not, they're just a totally different capacity you're evaluating. And so what we argue is that it's very important to distinguish different kinds of capabilities in these models. And so there is something that we call formal linguistic competence. And so that's the ability to generate coherent grammatical language. And that's something that in humans, the language brain network is responsible for. And then there is all of the other stuff that you need in order to actually use language in real life situation in interactions, you might want to ask somebody to close the door, you might want to tell somebody how you feel. And there are all kinds of situations that where you need to use language. But to do that, you actually need other capabilities, you need to be able to reason about social situations, you need to be able to know things about the world in order to generate actually accurate statements, you need to be able to reason logically and know some math if you want to solve a math problem. So even if that information is coming in as language, in order to be able to make sense of it, and also generate language that achieves a particular purpose, you need all of these other capacities, which broadly speaking, recall functional competence. And so different kinds of capabilities might suffer from different problems. And so we already touched upon a few. We touched upon the fact that mathematical reasoning and logical reasoning might require a different kind of algorithm. So instead of pattern matching, it might need to be more symbolic. And it's not fully clear whether the large language models today are capable of doing that. Maybe they are. But that's not necessarily in their default, in the default way they operate. So that's an open debate there. When it comes to world knowledge and knowing things about the world, distinguish implausible and implausible events, there a big problem is reporting bias and the fact that the training data that they have is biased. And so you might need to be able to build up a more general situation model, event model, that will not just take in the language that you receive, but also fill in some kind of commonly assumed things. If it's daytime, it slides out, stuff like that. And yeah, so different kinds of problems might require different kinds of solutions. A more general kind of potential solution that we advocate or talk about is modularity. So the fact that the brain is modular suggests that might be an efficient architecture. So a language process in module, the goal of the language network in the brain is not to reason, it's to get information that's expressed in fuzzy, imprecise words and extract meaning out of it. And then pass it on to relevant systems that can solve the math problem that can infer the social goal, all of that stuff. And presumably, for an artificial intelligence system, you might want to do something similar where language is not a replacement for thought, but is an interface to thought. And so in your example, right, you have a math problem, the language model translates it into code. It's very good at taking this broad like fuzzy natural language and translating into a more precise, symbolic representation. That's something that we didn't have at all, even a few years back. So it's a huge achievement. But then instead of trying to have that same language model to run the code, you're much better off passing it off to a code interpreter that will run the code and give you the answer. So the same kind of modularity that we see in the brain, that seems to be an effective way forward in the AI world that indeed some developers have started to adopt. Yeah. Yeah. And I think there's probably other ways in which the builders of these tools are trying to modularize. Like another one that comes up a lot is Rang or retrieval augmented generation, where yeah, there's some kind of database or just could just be a whole bunch of, you know, documents or whatever. And instead of hallucinating an answer, you want to make sure that you pick up something from one of those documents. And there's a whole different kind of machinery for that. But again, like in the code interpreter example, it's, I guess the language part is key, maybe less key in Rang because it's kind of a vector search. But it's a way, you know, it begins with translating language into something a bit more precise, in this case a vector instead of some code, I guess. And yeah, one wonders then if, you know, how close the parallels are between what is being built here and what's going on in the brain. You mentioned that, yeah, perhaps this is a good model for thinking about how we think. Language is this part where, this place where things kind of, you know, entry point for concepts, but the places where those concepts often get manipulated in terms of reasoning might be in other areas of the brain. They sort of become something more abstract than language itself. Yeah. Yeah. One thing I actually just slight tangent, but I do sometimes think that maybe language is being so associated with thought because it's kind of like the easiest thing to do, right? Like, you know, we know thinking is about concepts and some, you know, manipulating these things which are representations of the world. And language is just such an easy way of visualizing all of that, right, and understanding what's going on. But perhaps it's just the surface level of something much deeper that we really don't have an easy way of capturing. And, you know, that would map, I think, quite well to this kind of model of concepts being passed around, but the concepts themselves being, you know, beyond linguistics somehow. Yeah. So, as we mentioned, language is a system designed to communicate thoughts, concepts from one mind to another. And so, for this communication to be efficient, presumably language needs to parallel the structure of thought, the structure of concepts in some way, right? And so, it's much more abstract already than the raw perceptual input, than just audio, than just pictures, right? So, it kind of captures the relevant abstractions to a large extent. And so, that seems to be helping a lot. And so, that does bring us much closer to this more abstract conceptual representation. We're getting rid of a lot of extra details, we say cat, we don't care which color, which size is the cat. But, of course, at mapping between concepts and languages imprecise, we know that different languages partition the conceptual space in different ways, right? So, the words don't necessarily map the concepts one and one. Even within the same language, the same word can be used in many different contexts, in different ways, with different meanings. And so, that link is pretty fuzzy, can get pretty fuzzy. But it's definitely, I think you're right, when it comes to raw surface form, it's a very decent proxy, imperfect, but it makes sense why people are tempted to use it. Yeah. And, you know, in some ways, that means it makes what LLMs do so much more impressive, because they're also somehow capturing that surface form of concepts. Someone, a previous guest pointed out this wonderful quote from Ilya Sotskava saying, well, you know, if your LLM can predict the, you know, it's not just predicting text, because if your LLM can be fed the first part of a mystery novel that it's not read before, and it can tell you who the murderer was, it's not just predicting a word, it's somehow kind of understood what's going on in that story. Now, one of the difficulties, obviously, with all these things is, well, we don't know how open AIs LLMs are trained. So, it's very hard to test them, because you really need someone to write a new mystery novel to actually see if Ilya Sotskava's claim cashed us out. So, it's quite a high effort test. Unless, yeah, we happen to know of one which is definitely not in the corpus that was used. But, yeah, it does seem, you know, the fact that they are so good at mirroring what we produce, and that what we produce is somehow a good map onto something somewhat deeper, the world or an inner world. Yeah, it's so impressive. And you point out as well that it seems that, you know, the way that LLMs operate is very similar structurally to the way that our minds operate, in that, you know, it's not working on the raw audio or pixel forms of things. Like, the beauty of language is the compositionality at the level of small units, which are combinations of symbols or small sounds. And, yeah, the LLMs perfectly match that. So, we've built these things which really do capture something quite essential about how at least a part of our mind operates, it seems. And, yeah, maybe we've been seduced into thinking. That's all there is to thinking. Well, yeah, so in fact, well, that question, I guess I don't want to get too technical, but the question of what LLMs are starting with is actually an important one when we're trying to compare them with human minds or human brains. So, in fact, what LLMs operate over is tokens. So, it's chunks of characters that tend to occur pretty frequently in text. And so, oftentimes, they're words, like, though, but they're sometimes not words. If the word is long, it gets split up into multiple tokens. Yeah. And so, the problem is that those tokens actually don't match linguistic units that the word is actually made of, like morphine. They can be pretty arbitrary. And so, that does cause some differences between the way LLMs process them and humans do. In fact, people think that one reason why large language models are bad at arithmetic is because they tokenize numbers in weird ways, right? So, like, I don't know, 1618 is chunks in, like, 161 and then eight. And so, then it gets weird when they have to, like, add up the numbers. And so, that's where you get this weird, better-matching errors. And so, this kind of form is that, it's very engineering-driven. It's actually not, like, very rigorously scientifically based. And so, it's interesting, like, maybe if we change this little thing, it actually will result in much better performance. And so, it's funny how a lot of those choices are pretty random engineering-driven things. And, you know, they often work very well. But it's possible that with a small few tweaks, you can actually make the model much better. Yeah. No, I always thought that there was more sort of reasoning behind the n-grams that were used. But maybe, is it just kind of randomly chunks? Because I would have thought, well, there's some kind of, it makes sense to split words up, because, you know, particles like nus, if I think of, like, redness, right? It's not a word in itself, but it does attach to so many different words that it's sort of part of the compositional structure, I guess. But if it's getting chunked up is just two s's, right? And not nus, then it's kind of odd, yeah. No, that's exactly right, because nus is a morpheme, it's a suffix with a particular meaning. And so, if redness is chunked into red and nus, that makes a lot of sense, and it's linguistically justified. But oftentimes, that's not how the chunking happens. That's where the mismatch arises. So, you can definitely have the two s's in principle. Okay, interesting. Yeah, it seems like, yeah, one would think that with a bit of curation, maybe they could be even more effective. And yeah, it's hard to imagine them being more effective in terms of producing language. But perhaps that's just because they've been fed such a, such copious amounts of data that they sort of these, you know, they could be more efficient, right? Well, the stalker has an algorithm, it's kind of the goal is for it to be universal and that driven, right, without human curation, which is why the morphes don't get respected all the time. It causes a lot of issues for languages that aren't based on the Roman alphabet. So, let's say Arabic, for example, it ends up getting tokenized at the character level, because the tokenizer is just not adapted to deal with it. And so that does mean that performance on these languages that are not Roman alphabet based is actually worse, often substantially worse. It's generally a problem that like the fewer, the less data a language has, the worse the performance in that language. Some of the more general information seems to get pulled across different languages, which is cool. But a lot of language specific stuff, like grammar, right, of course depends on how much data you have in that language. But a particular distinction that tends to matter beyond just the amount of data is which alphabet. And so because so many of these morphs are English centric, a lot of other languages get left behind. Yeah, interesting. And to one extent, I mean, I know there are techniques for doing this. So you spend, you know, a lot of experiments looking into the minds or the brains of people. There are tools which allow us to do this to an extent with LLMs. But, you know, how effective are they? How does it compare to looking at an MRI, trying to understand what's going on inside of an LLM, what concepts it has, or what's lighting up as it is given a prompt? Yeah. So I am fascinated, honestly, by how many parallels there are between studying biological intelligence and humans and artificial intelligence. And for me, the first similarity is really just starting at the behavioral level. So developing separate experiments to look at formal competence like grammar, functional competence, like reasoning, these are methods from cognitive science, how do we design good experiments, how do we disentangle different contributors to performance. So even before we start looking inside the model or inside the brain, just looking at how humans behave and how models behave can tell us a lot about potentially how they do it, what kind of mistakes they make, what does it tell us about the potential mechanism that they're using to solve the task. But then, of course, we can get even more insight by looking at the actual mechanisms or their neural correlates. So for humans, that means looking inside the brain. And for models, that means looking inside the model. And so the movement that is getting seen currently the mechanistic interpretability movement in AI is doing that, essentially, they're asking which circuits, which units inside the network are responsible for a particular behavior. And so they first try to identify those units that get particularly engaged in a task. Maybe they respond differently to plausible sentences compared to implausible sentences. And then the beauty of having an artificial system is that they can actually go and manipulate it directly. So you can knock out that circuit or you can replace activations from one sentence with activations from another sentence. So in neuroscience, people sometimes do that as well. In animal research, for example, or there are certain kinds of stimulation that you can do that aren't harmful, but can maybe do the desired effect. In aphasia studies, these are natural causal experiments, right? We didn't cause delusion that destroyed the language network. But because we see those cases occur naturally, we can look at those effects. And so the causal tools are really powerful because they can really help us to see whether this part of the circuit is necessary for the behavior that we observe. And so in AI systems, we can do that quite easily. But conceptually, I would say in neuroscience and in AI, what we're trying to find out is very similar. Yeah. Yeah. And it's, I mean, it's wonderful, as you say, at least with the behavioral point, you can draw on the same kind of experiments that, you know, we finally have a kind of artificial intelligence that you can feed the same sort of things that you'd feed a person, i.e. sentences. And so it makes it very natural to run those kind of experiments. But then on the other hand, you can also go into the thing itself and tinker it with it in a way which would be very unethical and, you know, even just impossible with a person. So you could, I think there was one example where you had, I don't know, the concept of or Berlin was replaced with Paris or, no, what was it? It was Rome. Was it the Eiffel Tower was placed in conceptually into Rome or something like this? And you asked, well, how do you get from Berlin to the Eiffel Tower? It wasn't me, but it was, yeah, it's a famous kind of editing study. Yeah, I think I must have read it in one of your papers referring to it. And so the LLM does really, it kind of responds in the way that you would think if what's going on is that it has some kind of model of the world. And what all you've done is kind of switch around some pieces inside that model. It's not that it gets completely, you know, it doesn't throw everything completely out of whack, I suppose. And it even kind of infers some things that, you know, the Eiffel Tower will be in the center of Rome and it's going to be up with the Coliseum or something like that, which is, yeah, yeah, it's so fascinating to have something where we can kind of, you know, plausibly peer in into the internal workings. And yet just like the human brain, everything is so complicated that actually also it's not a trivial task, I guess. No, but that's the benefit, I guess, when neuroscientists have, we're used to dealing with this complexity. And, you know, there are ways to zoom out beyond just each individual neural unit to try and look at general trends and general patterns. And so I think a lot of people are daunted by the task of trying to understand the neural net because it's so big and complex. And because it's trained in this way where we don't necessarily know which features it sticks up on. But to me as a researcher, I'm just excited. It's like a cool puzzle to solve and a cool problem to understand. So generally, I'm pretty optimistic about this endeavor. Cool. Yeah. I think you mentioned at the very beginning that your research is now starting to look at some of the, you know, possibly trickier question of this kind of reflexive thinking, the narrow type of thinking I think you mentioned. So we've been talking a lot maybe about the broader definition of cognition of just kind of reasoning, manipulation of concepts, which might, one might even do in a very automatic way, as we were saying, like you might just solve a mass problem without really, you know, in a way where you'd say, oh, yeah, I didn't think about that. I just did it. But yeah, how does one, what kind of things have you, how can you pick, how can you look at this other problem of like when, when people kind of cogitate about things and turn them over in their, in their minds? Where are you going with that? I'm really curious. I think to me, the interesting question here is the question of individual differences. If some people report thinking in words most of the time and others say they don't think in words at all, presumably we should be able to see that at the brain level. Presumably we should be able to see the language network working hard for the first group and not at all for the second group while they're thinking right spontaneously in this task-free setting. And so that's really what I want to look at. But in order to do that, we need to have a good questionnaire that will capture those differences precisely, right? So I think instead of just asking people, although you think in words a lot, they're a little, it would be helpful to think, to get more information, right? Do they think in like, what does it mean? Like, what if other meta assessments of their own thinking style is reliable, right? So like, can we trust those judgments? How can we make them more granular? Another question that I'm very interested in, and that's really understood currently is, is there a difference between thinking in words and hearing the words, right? So if you're using some kind of words and some kind of language to think, does it mean that there is a voice or not necessarily? Some people, it turns out, they might see the words written in their mind's eyes, so they spell it out. It's a minority, like less than half of the population, but it does happen. And the capturing those differences, I think, is fascinating and then trying to look at the neural correlates to essentially establish the validity of those differences to show that they're really not just something that people perceive and report, but actually, that's not necessarily how they actually think. It's an interesting direction because psychology has this interesting history of an interesting relationship with phenomenology. So the people reporting their own experiences, right? That used to be very common, and then it turned out to result in a lot of pseudoscience and discredited a lot of psychology. And so then there was this huge turn to behaviorism where all that mattered was the stimulus and the response, and people were refusing to talk about any internal operations at all. So people are still very suspicious of phenomenology, so self-reporting experiences. And I think for the right reason, because often, yeah, we just don't know how we think. We're like, I think it's words or I think it's not. Sometimes we'll make a decision, like we were saying very quickly. And then when we have to explain what we did and how, we have to rationalize it. And so maybe that's actually not how we arrived at the decision, but post-talk, we come up with an explanation that might not correspond to the reality. So I think we'll have to be careful when taking people at their words. But to me, when people report this strike in differences of like, oh, yeah, I think in words, all the time versus like, never, not at all, it seems like there's something there and so I would love to use neuroscience to get at that question more deeply. Yeah, it's a tricky one. I mean, it strikes me that even the process of asking someone, do you think in words, it almost necessarily linguistic to communicate that because as we say, this is the way that we pass ideas around. And so maybe maybe there's just like that kind of arrogance in the language network, which is going to intercept that question and say, oh, yes, it's me. I do all the thinking. But as you say, well, many people do report thinking in many other ways. So yeah, I would, yeah, I'm really curious about what that shows. I mean, it's just so, this must surprise you all the time just how, you know, outwardly, we sort of walk around and we move around and we breathe and we have all our organs are, you know, working in pretty similar ways. And yet, internally, it might be, you know, we seem so heterogeneous, I guess. Does that sound about right? Or am I overstating the kind of differences in brains that we have? I don't know. I think, yeah, it just depends on your intuition about, you know, how much similarity and differences you would expect. Of course, our personalities are very different, right? Our likes and dislikes are interesting. So at the cognitive level, there are lots of differences between people, of course. And so I guess the interesting thing is that we have this huge differences in how we perceive our own thinking, but they don't necessarily manifest very obviously in differences in this year, right? So in addition to differences in inner speech, another common example is differences in mental imagery, right? So it turns out that some people never experience visual images in their mind's eye. When they're asked to imagine a red apple, they will think about the concept of an apple and like redness, but they will not like see a red apple in front of them when they close their eyes. And so that phenomenon has a name, aphantasia. And the name got coined in 2015. So very recently, really. And this is the phenomenon that kind of got discovered over the centuries at various times and then forgotten again, and rediscovered because again, people just tend to assume that everybody else has the same roughly inner experience with them. And so those differences just end up getting neglected. But it turns out that people with aphantasia, you know, again, you cannot tell them apart very easily from people with this visual imagery. So it turns out that lots of things we do in the world, you can do whether or not the experience images visually. Similarly, whether you have strong inner speech or not, turns out it's you can't really spot these people very easily out in the wild because they act very differently. So that's the interesting thing, right? Despite these experiences being so different, somehow we can still act in roughly similar ways and do the tasks that we need to do in the world. We might be using different strategies, it's very possible, but the end result is that actually, those differences are very hard to see. Yeah, yeah, that is fascinating. Yeah, actually, I have a friend who is an aphant, I guess. And well, he didn't find out until a few years ago. And we, you know, there's no kind of outward sign, right? You just seem completely, you know, normal. But then we're like, oh, yeah, I just can't visualize triangles, right? I know what a triangle is, I can reason about triangles, I can. And actually, often there's, I think there might be some research which shows that that in some ways, oftentimes better at reasoning about certain things, where one might think it requires a visual element. But yeah, he was, you know, a very good physicist, very good colleague, but just thought in a different way, I guess. Yeah, I think we'll find some differences, right? Like, now that there's more awareness, once we start doing more systematic research, I think we'll, like, I mean, there are already attempts, trying to look at the relationship between aphantasia and episodic memory, turned out that aphantasia and, yeah, spatial reasoning, geometric reasoning there, the link is not as strong and may or maybe not even there, even though people expected it to be. And there are variations as to why. But yeah, essentially, like, I think, even though those differences aren't apparent, I think we'll find some eventually. And probably we'll just find out that different people are using different strategies to do the same thing. Some of them might require imagery or thinking in words, you know, speech, and some might not. I mean, all this is a reminder that well, LLMs might be, we might end up producing artificial intelligences, which outwardly look very similar. But we shouldn't, or yeah, we should be careful to think that to be mindful that inwardly, they could be very, very different. And I think it's very true, and it's already happening, right? There are all those cases where people were screenshotting chat GPT responses, especially right after it came out a year ago, and just showing it responding to some very complicated prompt and doing it correctly, and people were so impressed being like, oh, you know, if you have, if you put like, a mail on a table, like, you know, like on on a chair, is that like a construction stable or not? Or, you know, what kind of thing we can put on top? And like, it looks very impressive. And then it turns out that if you change the problem just slightly, then it just starts spitting out total nonsense. And the same thing happened with a social reasoning problem, kind of predicting what the other person would think, which is a classical problem from psychology. And so the claim was that, you know, now LLMs can reason about people and what they do. And then, again, at third dollar, we change the prompt to be slightly different from the problems that were already available on the internet, then model performance drops drastically. So it's very easy to fall for this seemingly impressive performance, seemingly seemingly impressive understanding. And so luckily for us, in this case, there are ways to design even behavioral interventions that can maybe help us figure out what's actually going on and what strategy is actually being used. Yeah, yeah, that's a very important problem for sure. I found the social reasoning example really, I just loved it. So I think if I remember correctly, one of the ways that you fall them is just inserting a few words in the middle. So the classic, the classic one is something like, you know, Susie hides Bob's apple. It was in the closet. Where does he think the apple is? And no, the thing will think it will say, Oh, not in the closet anymore. Correct. But if you change it, say, well, Susie hides Bob's apple, it was in the closet, and she tells him that she hid it, right? And then the LLM still says, Oh, he thinks it's not in the closet because it and I think, yeah, so clearly they've been either on the training set or in the fine tuning, you know, it's gone so far in getting them to kind of give the appearance of having some kind of theory of mind and being able to solve those problems. But then with a little bit of tweaking, it becomes apparent that they they don't. But it seems to be coming harder and harder to, you know, fox these systems. And you point out that one of the real difficulties is we just, you know, without knowing what has gone into the training of open AI's models and it, it's hard to know how much genuine kind of intelligence has emerged and how much is just kind of pan recognition and, and, you know, or simple pan recognition and recall. But of course, the paradox here as well, they have some of the most advanced models. So maybe there's something genuinely interesting going on, but it's black box. So we can't really say. But I think it's encouraging that other folks are, you know, mistrial perhaps are being more open and about what's going into that into their models. So, you know, maybe easier to, to test on the very latest things and have confidence that I don't know, some problem was not appearing verbatim in in the training set. But even then, yeah, it could be some very similar problem. Yeah, for sure. And like, it's not always bad if they've been fine tuned on this problem. And if they've seen examples before, as humans learn lots of things, we can do math right off the bat, we learn, we learn from examples. It's easier for us to the problems that are familiar to us and novel ones, like, that's fine. It's just important to know, because that helps us figure out what is the mechanism that they're using, or potentially using, and what are the keys is where they might break, right? So it's, we don't necessarily need to expect these models to be like amazing, zero shot thinkers on total and novel problems all the time. It's just that, yeah, knowing what goes in the training data, knowing what problems they've been fine tuned on, just helps us assess them more accurately. Yeah, yeah, that's a good point. It's unfair to sort of demand development so that they don't learn anything, or they don't benefit from fine tuning. But yeah, but we want to know if they are, if the way that they're responding is using, is that they've abstracted patterns, or they've just, they're just regurgitating. Yeah, well, I, yeah, I found it really interesting to go through your work. I'm usually, I don't know, optimistic, I guess, or maybe not, maybe that's the wrong word. I really admire how LLMs are, have taken off, and it surprised me how quickly they've advanced. But one thing I've enjoyed about your work is kind of reminding me, well, actually, maybe they're not as far along in some ways as, as they appear to be at the surface level. Like there still seems to be some, some nuts to crack here to make them, to bring them closer to, to human thought. What's your take? Do you think, you know, I don't want to ask the typical how far away is AGI, because, but, you know, are you of the opinion that just cranking through more data is going to continue to produce results, or should more be invested in this kind of modularity approach? And if the latter, well, do we have, you know, are the things that are taking place on the right track, or do we need to look more closely and learn more from the human mind, perhaps? I think we definitely should recognize all the impressive achievements that we observe in LLMs today. And one way that my colleagues and I have been thinking about it is through the formal functional competence lens. On the formal competence front, learning the rules and patterns of natural language. These models have been incredibly impressive. So even a couple of years ago, they almost reached the ceiling for English, at least the language where they have the most data. And they did it without the need for any fine tuning. So just learning to predict words and method corpora of text. Turns out that is something that gives you all the most of the grammar and knowledge of idioms and all kinds of patterns that characterize a language. And that wasn't trivial at all. That was the subject of debate for linguists for decades. Is it possible to just learn from data? Or do you need a rich set of internal rules that can help you figure out what's grammatical and what's not? So that is incredibly impressive scientifically. And on the engineering front, different language processing systems in the past have struggled so much because you just can't encode language with a few simple rules or even not simple rules. Just like so fuzzy and there are so many exceptions in the regular forms and this and that. And the fact that these models have mastered that is so impressive. And people kind of forget and start talking about AGI right away, but that's an impressive achievement. Then being good at language is already very impressive. And then we get to functional components and their ability to reason and be factually accurate, know what's true and what's false and be actually helpful. And so that's a whole other host of problems where they actually seem to be spotty. They have achieved a lot because of pattern recognition, but then it turns out that that performance is not robust and it breaks. And so that's where it gets more complicated and more controversial. And that's where we argue modularity will be helpful. Again, looking at the human brain as an example. And one distinction we make though is that the modularity doesn't necessarily have to be built in by design. So this built-in approach we call architectural modularity where we have a language model and let's say a path and code interpreter and we put them together and they're clearly different and they're doing different things. So that can be promising, but then of course you need to know what the right modules are, you need to set them up in the right way. An alternative approach that might work for certain cases is what we call emergent modularity where you start out with one network, but you don't necessarily specify what parts need to be doing, what you let the network figure that out over the course of training and you can have different parts self-specialized to do different things. That might require some changes to the architecture to be able to promote the kind of specialization. It might require changes in the objective function, maybe next work prediction alone is not necessarily going to be good. And it might require changes in the training data kind of like what's happening with fine tuning today where you are feeding its specific problems that you're asking the model to do in a specific way so you might have selectively boost the social reasoning and the formal reasoning and the factual knowledge. There might be specific things you need to do, but there is a lot of promise in these approaches and the paper where we introduced the formal and functional competence, it's something we started working on in 2020 around the start of the pandemic. Language models were around then, but not nearly as advanced and as we were writing the paper and in fact after the initial preprint version came out, that's when we started seeing the field, the developers shifting away from this simple scaling up approach, that's not the approach that's common anymore. People have started to shift towards specialized fine tuning, using very targeted data sets to improve performance on specific domains, coupling an LLM with external modules, all of those things that we kind of suggested that might be good because that's more brain-like. RAG became big, all of those things are something we've seen over the past year that's very encouraging that now the AI field is also recognizing that it's not just about scale, that you do benefit from different components working together. Yeah, I think very exciting to see what comes next, both in your work and in the field of LLMs, which it seems like maybe someone's listening to what you're suggesting because all these things are happening. Yeah, I don't know if you have any final comments or predictions, warnings of doom often come up in these discussions, but this has been surprisingly positive. No, I just think that science is important and we just need to use good methods and not run after the hype and be realistic in how we evaluate the strengths and limitations of these models. There are strengths, there are limitations, so being too far on just the positive and just negative is not necessarily the most productive. We just want to be able to disentangle them effectively. Yeah, Rene, thank you so much, Anna. This has been really insightful. Yeah, thank you. you", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.94, "text": " I'm James Robinson, you're listening to Multiverses.", "tokens": [50364, 286, 478, 5678, 25105, 11, 291, 434, 4764, 281, 14665, 1762, 279, 13, 50611], "temperature": 0.0, "avg_logprob": -0.24316454906852877, "compression_ratio": 1.586466165413534, "no_speech_prob": 0.09058920294046402}, {"id": 1, "seek": 0, "start": 4.94, "end": 10.88, "text": " Language can do and express many things, and in fact this was the subject of my last conversation", "tokens": [50611, 24445, 393, 360, 293, 5109, 867, 721, 11, 293, 294, 1186, 341, 390, 264, 3983, 295, 452, 1036, 3761, 50908], "temperature": 0.0, "avg_logprob": -0.24316454906852877, "compression_ratio": 1.586466165413534, "no_speech_prob": 0.09058920294046402}, {"id": 2, "seek": 0, "start": 10.88, "end": 15.36, "text": " on this podcast with Nicol Krishnam talking about ordinary language philosophy.", "tokens": [50908, 322, 341, 7367, 365, 14776, 401, 6332, 742, 5378, 1417, 466, 10547, 2856, 10675, 13, 51132], "temperature": 0.0, "avg_logprob": -0.24316454906852877, "compression_ratio": 1.586466165413534, "no_speech_prob": 0.09058920294046402}, {"id": 3, "seek": 0, "start": 15.36, "end": 21.56, "text": " Just because language is so powerful, we might be tempted to think that that's all we need", "tokens": [51132, 1449, 570, 2856, 307, 370, 4005, 11, 321, 1062, 312, 29941, 281, 519, 300, 300, 311, 439, 321, 643, 51442], "temperature": 0.0, "avg_logprob": -0.24316454906852877, "compression_ratio": 1.586466165413534, "no_speech_prob": 0.09058920294046402}, {"id": 4, "seek": 0, "start": 21.56, "end": 23.78, "text": " for understanding and predicting the world.", "tokens": [51442, 337, 3701, 293, 32884, 264, 1002, 13, 51553], "temperature": 0.0, "avg_logprob": -0.24316454906852877, "compression_ratio": 1.586466165413534, "no_speech_prob": 0.09058920294046402}, {"id": 5, "seek": 0, "start": 23.78, "end": 27.72, "text": " It's just manipulation of symbols, next word prediction.", "tokens": [51553, 467, 311, 445, 26475, 295, 16944, 11, 958, 1349, 17630, 13, 51750], "temperature": 0.0, "avg_logprob": -0.24316454906852877, "compression_ratio": 1.586466165413534, "no_speech_prob": 0.09058920294046402}, {"id": 6, "seek": 2772, "start": 28.439999999999998, "end": 34.76, "text": " However, if we look at the how the human brain at least actually works, it's rather different.", "tokens": [50400, 2908, 11, 498, 321, 574, 412, 264, 577, 264, 1952, 3567, 412, 1935, 767, 1985, 11, 309, 311, 2831, 819, 13, 50716], "temperature": 0.0, "avg_logprob": -0.21352535047029195, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0011209634831175208}, {"id": 7, "seek": 2772, "start": 34.76, "end": 37.32, "text": " Our guest this week is Anna Ivanova.", "tokens": [50716, 2621, 8341, 341, 1243, 307, 12899, 26546, 3730, 2757, 13, 50844], "temperature": 0.0, "avg_logprob": -0.21352535047029195, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0011209634831175208}, {"id": 8, "seek": 2772, "start": 37.32, "end": 41.4, "text": " She's an assistant professor at Georgia Institute of Technology,", "tokens": [50844, 1240, 311, 364, 10994, 8304, 412, 11859, 9446, 295, 15037, 11, 51048], "temperature": 0.0, "avg_logprob": -0.21352535047029195, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0011209634831175208}, {"id": 9, "seek": 2772, "start": 41.4, "end": 45.239999999999995, "text": " and she tries to understand the relationship between language and thought,", "tokens": [51048, 293, 750, 9898, 281, 1223, 264, 2480, 1296, 2856, 293, 1194, 11, 51240], "temperature": 0.0, "avg_logprob": -0.21352535047029195, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0011209634831175208}, {"id": 10, "seek": 2772, "start": 45.239999999999995, "end": 51.480000000000004, "text": " and she does this by looking at brain scans, essentially MRIs, of what's going on when", "tokens": [51240, 293, 750, 775, 341, 538, 1237, 412, 3567, 35116, 11, 4476, 32812, 82, 11, 295, 437, 311, 516, 322, 562, 51552], "temperature": 0.0, "avg_logprob": -0.21352535047029195, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0011209634831175208}, {"id": 11, "seek": 2772, "start": 51.480000000000004, "end": 54.2, "text": " humans are presented with particular scenarios.", "tokens": [51552, 6255, 366, 8212, 365, 1729, 15077, 13, 51688], "temperature": 0.0, "avg_logprob": -0.21352535047029195, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0011209634831175208}, {"id": 12, "seek": 5420, "start": 54.2, "end": 58.92, "text": " For example, I'm looking at this marvellous view right now from Carlton Hill, Edinburgh.", "tokens": [50364, 1171, 1365, 11, 286, 478, 1237, 412, 341, 1849, 48592, 563, 1910, 558, 586, 490, 14256, 1756, 9109, 11, 41215, 13, 50600], "temperature": 0.0, "avg_logprob": -0.16149268191084903, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.0019456082955002785}, {"id": 13, "seek": 5420, "start": 58.92, "end": 62.92, "text": " I'm not thinking about it linguistically, it's going straight into my visual cortex", "tokens": [50600, 286, 478, 406, 1953, 466, 309, 21766, 20458, 11, 309, 311, 516, 2997, 666, 452, 5056, 33312, 50800], "temperature": 0.0, "avg_logprob": -0.16149268191084903, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.0019456082955002785}, {"id": 14, "seek": 5420, "start": 62.92, "end": 66.44, "text": " and processes happening there, and if I want to reason about it,", "tokens": [50800, 293, 7555, 2737, 456, 11, 293, 498, 286, 528, 281, 1778, 466, 309, 11, 50976], "temperature": 0.0, "avg_logprob": -0.16149268191084903, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.0019456082955002785}, {"id": 15, "seek": 5420, "start": 66.44, "end": 69.08, "text": " I'm not going to reason about it linguistically either.", "tokens": [50976, 286, 478, 406, 516, 281, 1778, 466, 309, 21766, 20458, 2139, 13, 51108], "temperature": 0.0, "avg_logprob": -0.16149268191084903, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.0019456082955002785}, {"id": 16, "seek": 5420, "start": 70.2, "end": 75.88, "text": " A lot of her work looks at how conceptual knowledge of the world is not tied to the language area", "tokens": [51164, 316, 688, 295, 720, 589, 1542, 412, 577, 24106, 3601, 295, 264, 1002, 307, 406, 9601, 281, 264, 2856, 1859, 51448], "temperature": 0.0, "avg_logprob": -0.16149268191084903, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.0019456082955002785}, {"id": 17, "seek": 5420, "start": 75.88, "end": 81.56, "text": " of our brain, so for example subjects with aphantasia, people who have large-scale", "tokens": [51448, 295, 527, 3567, 11, 370, 337, 1365, 13066, 365, 257, 15071, 25251, 11, 561, 567, 362, 2416, 12, 20033, 51732], "temperature": 0.0, "avg_logprob": -0.16149268191084903, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.0019456082955002785}, {"id": 18, "seek": 8156, "start": 81.56, "end": 87.48, "text": " damage to the language network, are still able to reason not only logically about", "tokens": [50364, 4344, 281, 264, 2856, 3209, 11, 366, 920, 1075, 281, 1778, 406, 787, 38887, 466, 50660], "temperature": 0.0, "avg_logprob": -0.13903733186943587, "compression_ratio": 1.610878661087866, "no_speech_prob": 0.0013778263237327337}, {"id": 19, "seek": 8156, "start": 87.48, "end": 91.32000000000001, "text": " chess problems and things like that, but they can reason socially as well.", "tokens": [50660, 24122, 2740, 293, 721, 411, 300, 11, 457, 436, 393, 1778, 21397, 382, 731, 13, 50852], "temperature": 0.0, "avg_logprob": -0.13903733186943587, "compression_ratio": 1.610878661087866, "no_speech_prob": 0.0013778263237327337}, {"id": 20, "seek": 8156, "start": 91.32000000000001, "end": 94.28, "text": " They can understand what situations are unusual.", "tokens": [50852, 814, 393, 1223, 437, 6851, 366, 10901, 13, 51000], "temperature": 0.0, "avg_logprob": -0.13903733186943587, "compression_ratio": 1.610878661087866, "no_speech_prob": 0.0013778263237327337}, {"id": 21, "seek": 8156, "start": 95.96000000000001, "end": 101.32000000000001, "text": " So this is a really insightful and very timely conversation because it plays into a lot of", "tokens": [51084, 407, 341, 307, 257, 534, 46401, 293, 588, 25150, 3761, 570, 309, 5749, 666, 257, 688, 295, 51352], "temperature": 0.0, "avg_logprob": -0.13903733186943587, "compression_ratio": 1.610878661087866, "no_speech_prob": 0.0013778263237327337}, {"id": 22, "seek": 8156, "start": 101.32000000000001, "end": 107.72, "text": " the enthusiasm about LOMs, which I certainly buy into myself, but it calls into question", "tokens": [51352, 264, 23417, 466, 441, 5251, 82, 11, 597, 286, 3297, 2256, 666, 2059, 11, 457, 309, 5498, 666, 1168, 51672], "temperature": 0.0, "avg_logprob": -0.13903733186943587, "compression_ratio": 1.610878661087866, "no_speech_prob": 0.0013778263237327337}, {"id": 23, "seek": 10772, "start": 107.72, "end": 116.68, "text": " some of this, forcing us to think, well, what is necessary on top of simple linguistic abilities", "tokens": [50364, 512, 295, 341, 11, 19030, 505, 281, 519, 11, 731, 11, 437, 307, 4818, 322, 1192, 295, 2199, 43002, 11582, 50812], "temperature": 0.0, "avg_logprob": -0.13112108013297938, "compression_ratio": 1.6161616161616161, "no_speech_prob": 0.0009178381878882647}, {"id": 24, "seek": 10772, "start": 116.68, "end": 120.12, "text": " to really be a fully-fledged thinking machine?", "tokens": [50812, 281, 534, 312, 257, 4498, 12, 69, 1493, 3004, 1953, 3479, 30, 50984], "temperature": 0.0, "avg_logprob": -0.13112108013297938, "compression_ratio": 1.6161616161616161, "no_speech_prob": 0.0009178381878882647}, {"id": 25, "seek": 10772, "start": 120.92, "end": 126.92, "text": " I think one question that still exists in my mind is, to what extent just language", "tokens": [51024, 286, 519, 472, 1168, 300, 920, 8198, 294, 452, 1575, 307, 11, 281, 437, 8396, 445, 2856, 51324], "temperature": 0.0, "avg_logprob": -0.13112108013297938, "compression_ratio": 1.6161616161616161, "no_speech_prob": 0.0009178381878882647}, {"id": 26, "seek": 10772, "start": 126.92, "end": 131.32, "text": " manipulation could get us to a fully thinking machine, somewhat in the same way as, you know,", "tokens": [51324, 26475, 727, 483, 505, 281, 257, 4498, 1953, 3479, 11, 8344, 294, 264, 912, 636, 382, 11, 291, 458, 11, 51544], "temperature": 0.0, "avg_logprob": -0.13112108013297938, "compression_ratio": 1.6161616161616161, "no_speech_prob": 0.0009178381878882647}, {"id": 27, "seek": 13132, "start": 131.32, "end": 141.07999999999998, "text": " I have a GPU and a CPU on this laptop, and I could use my CPU to play, you know, vector-based", "tokens": [50364, 286, 362, 257, 18407, 293, 257, 13199, 322, 341, 10732, 11, 293, 286, 727, 764, 452, 13199, 281, 862, 11, 291, 458, 11, 8062, 12, 6032, 50852], "temperature": 0.0, "avg_logprob": -0.08445655822753906, "compression_ratio": 1.5739130434782609, "no_speech_prob": 0.019539743661880493}, {"id": 28, "seek": 13132, "start": 141.64, "end": 146.28, "text": " computer games, and I could use my GPU to send emails, but they're not really suited to that task,", "tokens": [50880, 3820, 2813, 11, 293, 286, 727, 764, 452, 18407, 281, 2845, 12524, 11, 457, 436, 434, 406, 534, 24736, 281, 300, 5633, 11, 51112], "temperature": 0.0, "avg_logprob": -0.08445655822753906, "compression_ratio": 1.5739130434782609, "no_speech_prob": 0.019539743661880493}, {"id": 29, "seek": 13132, "start": 146.28, "end": 153.64, "text": " but the point is maybe language could be a kind of fully-fledged thinking system.", "tokens": [51112, 457, 264, 935, 307, 1310, 2856, 727, 312, 257, 733, 295, 4498, 12, 69, 1493, 3004, 1953, 1185, 13, 51480], "temperature": 0.0, "avg_logprob": -0.08445655822753906, "compression_ratio": 1.5739130434782609, "no_speech_prob": 0.019539743661880493}, {"id": 30, "seek": 13132, "start": 153.64, "end": 157.95999999999998, "text": " However, I think it is valuable to learn from, in fact, what the brain does, of course.", "tokens": [51480, 2908, 11, 286, 519, 309, 307, 8263, 281, 1466, 490, 11, 294, 1186, 11, 437, 264, 3567, 775, 11, 295, 1164, 13, 51696], "temperature": 0.0, "avg_logprob": -0.08445655822753906, "compression_ratio": 1.5739130434782609, "no_speech_prob": 0.019539743661880493}, {"id": 31, "seek": 15796, "start": 158.68, "end": 161.16, "text": " So I really enjoyed this conversation. I hope you did too.", "tokens": [50400, 407, 286, 534, 4626, 341, 3761, 13, 286, 1454, 291, 630, 886, 13, 50524], "temperature": 0.0, "avg_logprob": -0.36070503507341656, "compression_ratio": 1.1727272727272726, "no_speech_prob": 0.01518929097801447}, {"id": 32, "seek": 15796, "start": 177.4, "end": 180.28, "text": " Hi, Anna Ivanova, welcome to Multiverses.", "tokens": [51336, 2421, 11, 12899, 26546, 3730, 2757, 11, 2928, 281, 14665, 1762, 279, 13, 51480], "temperature": 0.0, "avg_logprob": -0.36070503507341656, "compression_ratio": 1.1727272727272726, "no_speech_prob": 0.01518929097801447}, {"id": 33, "seek": 15796, "start": 181.32, "end": 182.68, "text": " Hi, thank you for having me.", "tokens": [51532, 2421, 11, 1309, 291, 337, 1419, 385, 13, 51600], "temperature": 0.0, "avg_logprob": -0.36070503507341656, "compression_ratio": 1.1727272727272726, "no_speech_prob": 0.01518929097801447}, {"id": 34, "seek": 18268, "start": 183.4, "end": 189.24, "text": " So we're speaking and we're thinking, I think, and people who are listening are listening to our", "tokens": [50400, 407, 321, 434, 4124, 293, 321, 434, 1953, 11, 286, 519, 11, 293, 561, 567, 366, 4764, 366, 4764, 281, 527, 50692], "temperature": 0.0, "avg_logprob": -0.11265200899358381, "compression_ratio": 1.83203125, "no_speech_prob": 0.0176442489027977}, {"id": 35, "seek": 18268, "start": 189.24, "end": 195.64000000000001, "text": " words and they're thinking, and thought seems like something that should be really familiar to us,", "tokens": [50692, 2283, 293, 436, 434, 1953, 11, 293, 1194, 2544, 411, 746, 300, 820, 312, 534, 4963, 281, 505, 11, 51012], "temperature": 0.0, "avg_logprob": -0.11265200899358381, "compression_ratio": 1.83203125, "no_speech_prob": 0.0176442489027977}, {"id": 36, "seek": 18268, "start": 195.64000000000001, "end": 201.56, "text": " because it's one of those kind of few things that we have really direct access to, and yet,", "tokens": [51012, 570, 309, 311, 472, 295, 729, 733, 295, 1326, 721, 300, 321, 362, 534, 2047, 2105, 281, 11, 293, 1939, 11, 51308], "temperature": 0.0, "avg_logprob": -0.11265200899358381, "compression_ratio": 1.83203125, "no_speech_prob": 0.0176442489027977}, {"id": 37, "seek": 18268, "start": 201.56, "end": 207.4, "text": " at the same time, it seems so mysterious, so hard to figure out exactly what's going on.", "tokens": [51308, 412, 264, 912, 565, 11, 309, 2544, 370, 13831, 11, 370, 1152, 281, 2573, 484, 2293, 437, 311, 516, 322, 13, 51600], "temperature": 0.0, "avg_logprob": -0.11265200899358381, "compression_ratio": 1.83203125, "no_speech_prob": 0.0176442489027977}, {"id": 38, "seek": 18268, "start": 207.4, "end": 211.32, "text": " Maybe it's because the piece that's doing the figuring out is the thing that we're trying to", "tokens": [51600, 2704, 309, 311, 570, 264, 2522, 300, 311, 884, 264, 15213, 484, 307, 264, 551, 300, 321, 434, 1382, 281, 51796], "temperature": 0.0, "avg_logprob": -0.11265200899358381, "compression_ratio": 1.83203125, "no_speech_prob": 0.0176442489027977}, {"id": 39, "seek": 21132, "start": 211.32, "end": 218.51999999999998, "text": " figure out itself, I don't know. But yeah, how can we get some sort of grip on what thought is?", "tokens": [50364, 2573, 484, 2564, 11, 286, 500, 380, 458, 13, 583, 1338, 11, 577, 393, 321, 483, 512, 1333, 295, 12007, 322, 437, 1194, 307, 30, 50724], "temperature": 0.0, "avg_logprob": -0.1495559547520891, "compression_ratio": 1.5179487179487179, "no_speech_prob": 0.003934304695576429}, {"id": 40, "seek": 21132, "start": 219.79999999999998, "end": 220.6, "text": " Where do we start?", "tokens": [50788, 2305, 360, 321, 722, 30, 50828], "temperature": 0.0, "avg_logprob": -0.1495559547520891, "compression_ratio": 1.5179487179487179, "no_speech_prob": 0.003934304695576429}, {"id": 41, "seek": 21132, "start": 222.28, "end": 229.88, "text": " Well, I think we need to start with definitions, what it is that we mean by thought,", "tokens": [50912, 1042, 11, 286, 519, 321, 643, 281, 722, 365, 21988, 11, 437, 309, 307, 300, 321, 914, 538, 1194, 11, 51292], "temperature": 0.0, "avg_logprob": -0.1495559547520891, "compression_ratio": 1.5179487179487179, "no_speech_prob": 0.003934304695576429}, {"id": 42, "seek": 21132, "start": 229.88, "end": 236.28, "text": " because different people use the word in different senses, and of course, it also depends on the", "tokens": [51292, 570, 819, 561, 764, 264, 1349, 294, 819, 17057, 11, 293, 295, 1164, 11, 309, 611, 5946, 322, 264, 51612], "temperature": 0.0, "avg_logprob": -0.1495559547520891, "compression_ratio": 1.5179487179487179, "no_speech_prob": 0.003934304695576429}, {"id": 43, "seek": 23628, "start": 236.28, "end": 246.6, "text": " context. And generally speaking, at least in my area of work, I think there is the broad definition", "tokens": [50364, 4319, 13, 400, 5101, 4124, 11, 412, 1935, 294, 452, 1859, 295, 589, 11, 286, 519, 456, 307, 264, 4152, 7123, 50880], "temperature": 0.0, "avg_logprob": -0.07171881198883057, "compression_ratio": 1.5691489361702127, "no_speech_prob": 0.004823522642254829}, {"id": 44, "seek": 23628, "start": 246.6, "end": 253.88, "text": " and the narrow definition. And in the broad definition, thought is synonymous with cognition.", "tokens": [50880, 293, 264, 9432, 7123, 13, 400, 294, 264, 4152, 7123, 11, 1194, 307, 5451, 18092, 365, 46905, 13, 51244], "temperature": 0.0, "avg_logprob": -0.07171881198883057, "compression_ratio": 1.5691489361702127, "no_speech_prob": 0.004823522642254829}, {"id": 45, "seek": 23628, "start": 253.88, "end": 264.36, "text": " So the mental processes that we use to make sense of the world around us, so that includes reasoning,", "tokens": [51244, 407, 264, 4973, 7555, 300, 321, 764, 281, 652, 2020, 295, 264, 1002, 926, 505, 11, 370, 300, 5974, 21577, 11, 51768], "temperature": 0.0, "avg_logprob": -0.07171881198883057, "compression_ratio": 1.5691489361702127, "no_speech_prob": 0.004823522642254829}, {"id": 46, "seek": 26436, "start": 264.44, "end": 272.28000000000003, "text": " that includes accessing memories, that includes various social communication capacities,", "tokens": [50368, 300, 5974, 26440, 8495, 11, 300, 5974, 3683, 2093, 6101, 39396, 11, 50760], "temperature": 0.0, "avg_logprob": -0.09941613472114175, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.0012434044620022178}, {"id": 47, "seek": 26436, "start": 272.28000000000003, "end": 280.6, "text": " so very broadly speaking, something we would call cognition. And then the narrow definition is the", "tokens": [50760, 370, 588, 19511, 4124, 11, 746, 321, 576, 818, 46905, 13, 400, 550, 264, 9432, 7123, 307, 264, 51176], "temperature": 0.0, "avg_logprob": -0.09941613472114175, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.0012434044620022178}, {"id": 48, "seek": 26436, "start": 280.6, "end": 288.52000000000004, "text": " stuff that happens kind of like in between us doing things. So it's not necessarily you get a math", "tokens": [51176, 1507, 300, 2314, 733, 295, 411, 294, 1296, 505, 884, 721, 13, 407, 309, 311, 406, 4725, 291, 483, 257, 5221, 51572], "temperature": 0.0, "avg_logprob": -0.09941613472114175, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.0012434044620022178}, {"id": 49, "seek": 28852, "start": 288.52, "end": 295.79999999999995, "text": " problem, your reason about it, you give the result. But it's more about you lying down in bed at night", "tokens": [50364, 1154, 11, 428, 1778, 466, 309, 11, 291, 976, 264, 1874, 13, 583, 309, 311, 544, 466, 291, 8493, 760, 294, 2901, 412, 1818, 50728], "temperature": 0.0, "avg_logprob": -0.13222031260645667, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.073597252368927}, {"id": 50, "seek": 28852, "start": 295.79999999999995, "end": 304.44, "text": " and thinking about your day tomorrow, or you're walking somewhere and you're playing out your", "tokens": [50728, 293, 1953, 466, 428, 786, 4153, 11, 420, 291, 434, 4494, 4079, 293, 291, 434, 2433, 484, 428, 51160], "temperature": 0.0, "avg_logprob": -0.13222031260645667, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.073597252368927}, {"id": 51, "seek": 28852, "start": 305.0, "end": 310.84, "text": " conversation that you're going to have with a friend. And so this kind of inner thinking that", "tokens": [51188, 3761, 300, 291, 434, 516, 281, 362, 365, 257, 1277, 13, 400, 370, 341, 733, 295, 7284, 1953, 300, 51480], "temperature": 0.0, "avg_logprob": -0.13222031260645667, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.073597252368927}, {"id": 52, "seek": 28852, "start": 310.84, "end": 317.47999999999996, "text": " happens spontaneously, not in response to any external task is also something where people", "tokens": [51480, 2314, 47632, 11, 406, 294, 4134, 281, 604, 8320, 5633, 307, 611, 746, 689, 561, 51812], "temperature": 0.0, "avg_logprob": -0.13222031260645667, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.073597252368927}, {"id": 53, "seek": 31748, "start": 318.36, "end": 322.52000000000004, "text": " that people commonly refer to as thought. And of course, those are very different, right? Like", "tokens": [50408, 300, 561, 12719, 2864, 281, 382, 1194, 13, 400, 295, 1164, 11, 729, 366, 588, 819, 11, 558, 30, 1743, 50616], "temperature": 0.0, "avg_logprob": -0.11384313447134835, "compression_ratio": 1.5798319327731092, "no_speech_prob": 0.0008157278643921018}, {"id": 54, "seek": 31748, "start": 322.52000000000004, "end": 327.08000000000004, "text": " if we're talking about the broad thing or the narrow thing, this specific kind of mechanisms", "tokens": [50616, 498, 321, 434, 1417, 466, 264, 4152, 551, 420, 264, 9432, 551, 11, 341, 2685, 733, 295, 15902, 50844], "temperature": 0.0, "avg_logprob": -0.11384313447134835, "compression_ratio": 1.5798319327731092, "no_speech_prob": 0.0008157278643921018}, {"id": 55, "seek": 31748, "start": 327.08000000000004, "end": 333.24, "text": " that might support them might differ quite a bit. Yeah, I think both of those kind of definitions", "tokens": [50844, 300, 1062, 1406, 552, 1062, 743, 1596, 257, 857, 13, 865, 11, 286, 519, 1293, 295, 729, 733, 295, 21988, 51152], "temperature": 0.0, "avg_logprob": -0.11384313447134835, "compression_ratio": 1.5798319327731092, "no_speech_prob": 0.0008157278643921018}, {"id": 56, "seek": 31748, "start": 334.12, "end": 342.20000000000005, "text": " capture something of what one intuitively would characterize as thinking. So we might say,", "tokens": [51196, 7983, 746, 295, 437, 472, 46506, 576, 38463, 382, 1953, 13, 407, 321, 1062, 584, 11, 51600], "temperature": 0.0, "avg_logprob": -0.11384313447134835, "compression_ratio": 1.5798319327731092, "no_speech_prob": 0.0008157278643921018}, {"id": 57, "seek": 34220, "start": 343.15999999999997, "end": 348.52, "text": " you know, broadly speaking, oh, yeah, of course, when this person solved that math problem,", "tokens": [50412, 291, 458, 11, 19511, 4124, 11, 1954, 11, 1338, 11, 295, 1164, 11, 562, 341, 954, 13041, 300, 5221, 1154, 11, 50680], "temperature": 0.0, "avg_logprob": -0.09829482869205312, "compression_ratio": 1.8160919540229885, "no_speech_prob": 0.003168292809277773}, {"id": 58, "seek": 34220, "start": 348.52, "end": 352.84, "text": " they had to be thinking. But then you might also say, oh, they did it so quickly, they just did it", "tokens": [50680, 436, 632, 281, 312, 1953, 13, 583, 550, 291, 1062, 611, 584, 11, 1954, 11, 436, 630, 309, 370, 2661, 11, 436, 445, 630, 309, 50896], "temperature": 0.0, "avg_logprob": -0.09829482869205312, "compression_ratio": 1.8160919540229885, "no_speech_prob": 0.003168292809277773}, {"id": 59, "seek": 34220, "start": 352.84, "end": 357.48, "text": " without thinking, which would be sort of more of the narrow definition, they did it sort of", "tokens": [50896, 1553, 1953, 11, 597, 576, 312, 1333, 295, 544, 295, 264, 9432, 7123, 11, 436, 630, 309, 1333, 295, 51128], "temperature": 0.0, "avg_logprob": -0.09829482869205312, "compression_ratio": 1.8160919540229885, "no_speech_prob": 0.003168292809277773}, {"id": 60, "seek": 34220, "start": 358.03999999999996, "end": 363.0, "text": " without any kind of reflection, which is kind of what the narrower definition requires, I guess.", "tokens": [51156, 1553, 604, 733, 295, 12914, 11, 597, 307, 733, 295, 437, 264, 46751, 7123, 7029, 11, 286, 2041, 13, 51404], "temperature": 0.0, "avg_logprob": -0.09829482869205312, "compression_ratio": 1.8160919540229885, "no_speech_prob": 0.003168292809277773}, {"id": 61, "seek": 34220, "start": 364.68, "end": 368.03999999999996, "text": " I'm curious, do you have a kind of preference for either of those definitions, or do you think", "tokens": [51488, 286, 478, 6369, 11, 360, 291, 362, 257, 733, 295, 17502, 337, 2139, 295, 729, 21988, 11, 420, 360, 291, 519, 51656], "temperature": 0.0, "avg_logprob": -0.09829482869205312, "compression_ratio": 1.8160919540229885, "no_speech_prob": 0.003168292809277773}, {"id": 62, "seek": 36804, "start": 368.12, "end": 375.16, "text": " they both serve a kind of useful purpose? I've used both. I'm interested in the relationship", "tokens": [50368, 436, 1293, 4596, 257, 733, 295, 4420, 4334, 30, 286, 600, 1143, 1293, 13, 286, 478, 3102, 294, 264, 2480, 50720], "temperature": 0.0, "avg_logprob": -0.09675579760448043, "compression_ratio": 1.7302325581395348, "no_speech_prob": 0.0015711648156866431}, {"id": 63, "seek": 36804, "start": 375.16, "end": 380.68, "text": " between language and thought, the role that language plays in thinking. And so the role", "tokens": [50720, 1296, 2856, 293, 1194, 11, 264, 3090, 300, 2856, 5749, 294, 1953, 13, 400, 370, 264, 3090, 50996], "temperature": 0.0, "avg_logprob": -0.09675579760448043, "compression_ratio": 1.7302325581395348, "no_speech_prob": 0.0015711648156866431}, {"id": 64, "seek": 36804, "start": 380.68, "end": 387.8, "text": " that language plays in thinking broadly defined thinking as cognition, that actually turns out to", "tokens": [50996, 300, 2856, 5749, 294, 1953, 19511, 7642, 1953, 382, 46905, 11, 300, 767, 4523, 484, 281, 51352], "temperature": 0.0, "avg_logprob": -0.09675579760448043, "compression_ratio": 1.7302325581395348, "no_speech_prob": 0.0015711648156866431}, {"id": 65, "seek": 36804, "start": 387.8, "end": 393.64000000000004, "text": " be a more tractable question, because we can ask people to do a math problem and look whether", "tokens": [51352, 312, 257, 544, 24207, 712, 1168, 11, 570, 321, 393, 1029, 561, 281, 360, 257, 5221, 1154, 293, 574, 1968, 51644], "temperature": 0.0, "avg_logprob": -0.09675579760448043, "compression_ratio": 1.7302325581395348, "no_speech_prob": 0.0015711648156866431}, {"id": 66, "seek": 39364, "start": 393.64, "end": 400.12, "text": " language processing regions in the brain are engaged. But for inner thinking,", "tokens": [50364, 2856, 9007, 10682, 294, 264, 3567, 366, 8237, 13, 583, 337, 7284, 1953, 11, 50688], "temperature": 0.0, "avg_logprob": -0.09843626022338867, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.007115777116268873}, {"id": 67, "seek": 39364, "start": 400.12, "end": 405.71999999999997, "text": " stuff that happens spontaneously without an external task, that's much harder to capture.", "tokens": [50688, 1507, 300, 2314, 47632, 1553, 364, 8320, 5633, 11, 300, 311, 709, 6081, 281, 7983, 13, 50968], "temperature": 0.0, "avg_logprob": -0.09843626022338867, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.007115777116268873}, {"id": 68, "seek": 39364, "start": 405.71999999999997, "end": 410.12, "text": " But that's also where people have very strong intuitions there. Of course, I think in words,", "tokens": [50968, 583, 300, 311, 611, 689, 561, 362, 588, 2068, 16224, 626, 456, 13, 2720, 1164, 11, 286, 519, 294, 2283, 11, 51188], "temperature": 0.0, "avg_logprob": -0.09843626022338867, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.007115777116268873}, {"id": 69, "seek": 39364, "start": 410.12, "end": 416.52, "text": " or of course, I think without words. And so that area is harder to study, but also very", "tokens": [51188, 420, 295, 1164, 11, 286, 519, 1553, 2283, 13, 400, 370, 300, 1859, 307, 6081, 281, 2979, 11, 457, 611, 588, 51508], "temperature": 0.0, "avg_logprob": -0.09843626022338867, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.007115777116268873}, {"id": 70, "seek": 39364, "start": 416.52, "end": 419.64, "text": " interesting. And so that's where I see some of my future work going.", "tokens": [51508, 1880, 13, 400, 370, 300, 311, 689, 286, 536, 512, 295, 452, 2027, 589, 516, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09843626022338867, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.007115777116268873}, {"id": 71, "seek": 41964, "start": 420.59999999999997, "end": 425.71999999999997, "text": " Interesting. Yeah, it's true that kind of more reflexive thinking almost by definition,", "tokens": [50412, 14711, 13, 865, 11, 309, 311, 2074, 300, 733, 295, 544, 23802, 488, 1953, 1920, 538, 7123, 11, 50668], "temperature": 0.0, "avg_logprob": -0.1590910045378799, "compression_ratio": 1.6823529411764706, "no_speech_prob": 0.0035893148742616177}, {"id": 72, "seek": 41964, "start": 425.71999999999997, "end": 428.76, "text": " people are going to have opinions about it, because they are kind of", "tokens": [50668, 561, 366, 516, 281, 362, 11819, 466, 309, 11, 570, 436, 366, 733, 295, 50820], "temperature": 0.0, "avg_logprob": -0.1590910045378799, "compression_ratio": 1.6823529411764706, "no_speech_prob": 0.0035893148742616177}, {"id": 73, "seek": 41964, "start": 429.4, "end": 436.52, "text": " cogitacing, turning things over. And part of that process is inward looking. So yeah,", "tokens": [50852, 46521, 270, 5615, 11, 6246, 721, 670, 13, 400, 644, 295, 300, 1399, 307, 29876, 1237, 13, 407, 1338, 11, 51208], "temperature": 0.0, "avg_logprob": -0.1590910045378799, "compression_ratio": 1.6823529411764706, "no_speech_prob": 0.0035893148742616177}, {"id": 74, "seek": 41964, "start": 436.52, "end": 440.03999999999996, "text": " people are going to be like, oh, well, I always do that with words or with images or a mix.", "tokens": [51208, 561, 366, 516, 281, 312, 411, 11, 1954, 11, 731, 11, 286, 1009, 360, 300, 365, 2283, 420, 365, 5267, 420, 257, 2890, 13, 51384], "temperature": 0.0, "avg_logprob": -0.1590910045378799, "compression_ratio": 1.6823529411764706, "no_speech_prob": 0.0035893148742616177}, {"id": 75, "seek": 41964, "start": 441.0, "end": 445.64, "text": " And yet sometimes I can also find it a little bit hard to remember to do that and think about,", "tokens": [51432, 400, 1939, 2171, 286, 393, 611, 915, 309, 257, 707, 857, 1152, 281, 1604, 281, 360, 300, 293, 519, 466, 11, 51664], "temperature": 0.0, "avg_logprob": -0.1590910045378799, "compression_ratio": 1.6823529411764706, "no_speech_prob": 0.0035893148742616177}, {"id": 76, "seek": 44564, "start": 445.71999999999997, "end": 450.2, "text": " because whenever I sort of think about, if I try to think about what thinking is,", "tokens": [50368, 570, 5699, 286, 1333, 295, 519, 466, 11, 498, 286, 853, 281, 519, 466, 437, 1953, 307, 11, 50592], "temperature": 0.0, "avg_logprob": -0.05789264484688088, "compression_ratio": 1.844155844155844, "no_speech_prob": 0.0062734573148190975}, {"id": 77, "seek": 44564, "start": 450.2, "end": 453.4, "text": " I will do it linguistically. But maybe that's just because it's the sort of,", "tokens": [50592, 286, 486, 360, 309, 21766, 20458, 13, 583, 1310, 300, 311, 445, 570, 309, 311, 264, 1333, 295, 11, 50752], "temperature": 0.0, "avg_logprob": -0.05789264484688088, "compression_ratio": 1.844155844155844, "no_speech_prob": 0.0062734573148190975}, {"id": 78, "seek": 44564, "start": 454.28, "end": 457.88, "text": " that's the sort of way that I need to think about that thing. Whereas if I think about", "tokens": [50796, 300, 311, 264, 1333, 295, 636, 300, 286, 643, 281, 519, 466, 300, 551, 13, 13813, 498, 286, 519, 466, 50976], "temperature": 0.0, "avg_logprob": -0.05789264484688088, "compression_ratio": 1.844155844155844, "no_speech_prob": 0.0062734573148190975}, {"id": 79, "seek": 44564, "start": 457.88, "end": 462.28, "text": " something entirely different, like a kind of, you know, spatial reasoning problem,", "tokens": [50976, 746, 7696, 819, 11, 411, 257, 733, 295, 11, 291, 458, 11, 23598, 21577, 1154, 11, 51196], "temperature": 0.0, "avg_logprob": -0.05789264484688088, "compression_ratio": 1.844155844155844, "no_speech_prob": 0.0062734573148190975}, {"id": 80, "seek": 44564, "start": 462.28, "end": 468.03999999999996, "text": " I'm sure I would do it in a different way. But it maybe wouldn't engage the linguistic part of my", "tokens": [51196, 286, 478, 988, 286, 576, 360, 309, 294, 257, 819, 636, 13, 583, 309, 1310, 2759, 380, 4683, 264, 43002, 644, 295, 452, 51484], "temperature": 0.0, "avg_logprob": -0.05789264484688088, "compression_ratio": 1.844155844155844, "no_speech_prob": 0.0062734573148190975}, {"id": 81, "seek": 46804, "start": 468.6, "end": 476.84000000000003, "text": " brain. But anyway, I guess, yeah, this is a very, we've got very quickly to a very", "tokens": [50392, 3567, 13, 583, 4033, 11, 286, 2041, 11, 1338, 11, 341, 307, 257, 588, 11, 321, 600, 658, 588, 2661, 281, 257, 588, 50804], "temperature": 0.0, "avg_logprob": -0.12340241128748114, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.08615308254957199}, {"id": 82, "seek": 46804, "start": 478.6, "end": 482.28000000000003, "text": " key area, which is this, this, this relationship between language and thought, which", "tokens": [50892, 2141, 1859, 11, 597, 307, 341, 11, 341, 11, 341, 2480, 1296, 2856, 293, 1194, 11, 597, 51076], "temperature": 0.0, "avg_logprob": -0.12340241128748114, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.08615308254957199}, {"id": 83, "seek": 46804, "start": 484.6, "end": 491.32000000000005, "text": " my last guest was Nikol Krishnan, an ordinary language philosopher. And we spent some time", "tokens": [51192, 452, 1036, 8341, 390, 13969, 401, 6332, 742, 17622, 11, 364, 10547, 2856, 29805, 13, 400, 321, 4418, 512, 565, 51528], "temperature": 0.0, "avg_logprob": -0.12340241128748114, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.08615308254957199}, {"id": 84, "seek": 46804, "start": 491.32000000000005, "end": 495.72, "text": " just talking about, well, how for a while, people just thought there was no difference", "tokens": [51528, 445, 1417, 466, 11, 731, 11, 577, 337, 257, 1339, 11, 561, 445, 1194, 456, 390, 572, 2649, 51748], "temperature": 0.0, "avg_logprob": -0.12340241128748114, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.08615308254957199}, {"id": 85, "seek": 49572, "start": 495.72, "end": 504.12, "text": " between the two. There was, or rather, in some way, language captured the entirety of everything,", "tokens": [50364, 1296, 264, 732, 13, 821, 390, 11, 420, 2831, 11, 294, 512, 636, 11, 2856, 11828, 264, 31557, 295, 1203, 11, 50784], "temperature": 0.0, "avg_logprob": -0.08143390261608621, "compression_ratio": 1.6697247706422018, "no_speech_prob": 0.0018612773856148124}, {"id": 86, "seek": 49572, "start": 504.12, "end": 508.28000000000003, "text": " including thought, maybe including some other things. So, you know, Wittgenstein's famous", "tokens": [50784, 3009, 1194, 11, 1310, 3009, 512, 661, 721, 13, 407, 11, 291, 458, 11, 343, 593, 1766, 9089, 311, 4618, 50992], "temperature": 0.0, "avg_logprob": -0.08143390261608621, "compression_ratio": 1.6697247706422018, "no_speech_prob": 0.0018612773856148124}, {"id": 87, "seek": 49572, "start": 508.28000000000003, "end": 513.1600000000001, "text": " dictum, the limits of my language, the limits are the limits of, or mean the limits of my world.", "tokens": [50992, 12569, 449, 11, 264, 10406, 295, 452, 2856, 11, 264, 10406, 366, 264, 10406, 295, 11, 420, 914, 264, 10406, 295, 452, 1002, 13, 51236], "temperature": 0.0, "avg_logprob": -0.08143390261608621, "compression_ratio": 1.6697247706422018, "no_speech_prob": 0.0018612773856148124}, {"id": 88, "seek": 49572, "start": 516.2, "end": 522.2, "text": " But I guess your research is maybe questioning that. Would that be fair to say?", "tokens": [51388, 583, 286, 2041, 428, 2132, 307, 1310, 21257, 300, 13, 6068, 300, 312, 3143, 281, 584, 30, 51688], "temperature": 0.0, "avg_logprob": -0.08143390261608621, "compression_ratio": 1.6697247706422018, "no_speech_prob": 0.0018612773856148124}, {"id": 89, "seek": 52220, "start": 522.44, "end": 530.84, "text": " Yeah, I use Wittgenstein's quote as an example of a worldview, a paradigm that I'm pushing against.", "tokens": [50376, 865, 11, 286, 764, 343, 593, 1766, 9089, 311, 6513, 382, 364, 1365, 295, 257, 41141, 11, 257, 24709, 300, 286, 478, 7380, 1970, 13, 50796], "temperature": 0.0, "avg_logprob": -0.11160928552800958, "compression_ratio": 1.421875, "no_speech_prob": 0.0011135793756693602}, {"id": 90, "seek": 52220, "start": 533.0, "end": 540.76, "text": " And what's wrong with it? I mean, as we said, like, when we try to describe what thought is,", "tokens": [50904, 400, 437, 311, 2085, 365, 309, 30, 286, 914, 11, 382, 321, 848, 11, 411, 11, 562, 321, 853, 281, 6786, 437, 1194, 307, 11, 51292], "temperature": 0.0, "avg_logprob": -0.11160928552800958, "compression_ratio": 1.421875, "no_speech_prob": 0.0011135793756693602}, {"id": 91, "seek": 52220, "start": 540.76, "end": 551.48, "text": " we'll probably reach for language. And yeah, it seems like so many of the ideas,", "tokens": [51292, 321, 603, 1391, 2524, 337, 2856, 13, 400, 1338, 11, 309, 2544, 411, 370, 867, 295, 264, 3487, 11, 51828], "temperature": 0.0, "avg_logprob": -0.11160928552800958, "compression_ratio": 1.421875, "no_speech_prob": 0.0011135793756693602}, {"id": 92, "seek": 55148, "start": 551.48, "end": 555.88, "text": " I don't want to say everything, but so much of the ideas that we have, and we pass on, we do", "tokens": [50364, 286, 500, 380, 528, 281, 584, 1203, 11, 457, 370, 709, 295, 264, 3487, 300, 321, 362, 11, 293, 321, 1320, 322, 11, 321, 360, 50584], "temperature": 0.0, "avg_logprob": -0.09072261728266234, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0009839378762990236}, {"id": 93, "seek": 55148, "start": 556.84, "end": 561.48, "text": " with language. And maybe there's an argument that the places where we're not doing it with", "tokens": [50632, 365, 2856, 13, 400, 1310, 456, 311, 364, 6770, 300, 264, 3190, 689, 321, 434, 406, 884, 309, 365, 50864], "temperature": 0.0, "avg_logprob": -0.09072261728266234, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0009839378762990236}, {"id": 94, "seek": 55148, "start": 561.48, "end": 569.48, "text": " language are somehow dependent on language behind the scenes. But that's not a very scientific", "tokens": [50864, 2856, 366, 6063, 12334, 322, 2856, 2261, 264, 8026, 13, 583, 300, 311, 406, 257, 588, 8134, 51264], "temperature": 0.0, "avg_logprob": -0.09072261728266234, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0009839378762990236}, {"id": 95, "seek": 55148, "start": 569.48, "end": 574.44, "text": " argument. And I think you have some kind of evidence to the contrary. So, yeah, maybe take us", "tokens": [51264, 6770, 13, 400, 286, 519, 291, 362, 512, 733, 295, 4467, 281, 264, 19506, 13, 407, 11, 1338, 11, 1310, 747, 505, 51512], "temperature": 0.0, "avg_logprob": -0.09072261728266234, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0009839378762990236}, {"id": 96, "seek": 57444, "start": 574.44, "end": 582.2, "text": " through some of the things that you've done to probe this. Yeah, let me, there is a lot to say.", "tokens": [50364, 807, 512, 295, 264, 721, 300, 291, 600, 1096, 281, 22715, 341, 13, 865, 11, 718, 385, 11, 456, 307, 257, 688, 281, 584, 13, 50752], "temperature": 0.0, "avg_logprob": -0.1059490455375923, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.05494506657123566}, {"id": 97, "seek": 57444, "start": 582.2, "end": 589.8000000000001, "text": " So, let me start first by, I guess, acknowledging the last bit that you said, where clearly,", "tokens": [50752, 407, 11, 718, 385, 722, 700, 538, 11, 286, 2041, 11, 30904, 264, 1036, 857, 300, 291, 848, 11, 689, 4448, 11, 51132], "temperature": 0.0, "avg_logprob": -0.1059490455375923, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.05494506657123566}, {"id": 98, "seek": 57444, "start": 589.8000000000001, "end": 597.1600000000001, "text": " there is a relationship between language and thought. And the most trivial, but also important", "tokens": [51132, 456, 307, 257, 2480, 1296, 2856, 293, 1194, 13, 400, 264, 881, 26703, 11, 457, 611, 1021, 51500], "temperature": 0.0, "avg_logprob": -0.1059490455375923, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.05494506657123566}, {"id": 99, "seek": 57444, "start": 597.1600000000001, "end": 603.24, "text": " one is the fact that we use language to transmit information, to communicate thoughts to one", "tokens": [51500, 472, 307, 264, 1186, 300, 321, 764, 2856, 281, 17831, 1589, 11, 281, 7890, 4598, 281, 472, 51804], "temperature": 0.0, "avg_logprob": -0.1059490455375923, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.05494506657123566}, {"id": 100, "seek": 60324, "start": 603.24, "end": 608.84, "text": " another. And that's a very powerful mechanism, we can translate knowledge from generation to", "tokens": [50364, 1071, 13, 400, 300, 311, 257, 588, 4005, 7513, 11, 321, 393, 13799, 3601, 490, 5125, 281, 50644], "temperature": 0.0, "avg_logprob": -0.07921128613608223, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.00118669040966779}, {"id": 101, "seek": 60324, "start": 608.84, "end": 617.0, "text": " generation. So that is a very important role of language in thinking, helping us share information", "tokens": [50644, 5125, 13, 407, 300, 307, 257, 588, 1021, 3090, 295, 2856, 294, 1953, 11, 4315, 505, 2073, 1589, 51052], "temperature": 0.0, "avg_logprob": -0.07921128613608223, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.00118669040966779}, {"id": 102, "seek": 60324, "start": 617.0, "end": 623.16, "text": " without having to figure out every single thing individually. But here, what we're talking about", "tokens": [51052, 1553, 1419, 281, 2573, 484, 633, 2167, 551, 16652, 13, 583, 510, 11, 437, 321, 434, 1417, 466, 51360], "temperature": 0.0, "avg_logprob": -0.07921128613608223, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.00118669040966779}, {"id": 103, "seek": 60324, "start": 623.16, "end": 629.72, "text": " is using language as a medium of thinking. So internally, do we think in words, do we recruit", "tokens": [51360, 307, 1228, 2856, 382, 257, 6399, 295, 1953, 13, 407, 19501, 11, 360, 321, 519, 294, 2283, 11, 360, 321, 15119, 51688], "temperature": 0.0, "avg_logprob": -0.07921128613608223, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.00118669040966779}, {"id": 104, "seek": 62972, "start": 629.8000000000001, "end": 636.2, "text": " the mechanisms for language processing when we're thinking? And so that's an important distinction.", "tokens": [50368, 264, 15902, 337, 2856, 9007, 562, 321, 434, 1953, 30, 400, 370, 300, 311, 364, 1021, 16844, 13, 50688], "temperature": 0.0, "avg_logprob": -0.08720680383535531, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.002250001998618245}, {"id": 105, "seek": 62972, "start": 636.2, "end": 644.36, "text": " So that's the scope. Now, as we said, people have strong intuitions about whether or not", "tokens": [50688, 407, 300, 311, 264, 11923, 13, 823, 11, 382, 321, 848, 11, 561, 362, 2068, 16224, 626, 466, 1968, 420, 406, 51096], "temperature": 0.0, "avg_logprob": -0.08720680383535531, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.002250001998618245}, {"id": 106, "seek": 62972, "start": 644.36, "end": 652.12, "text": " they use language to think. And probably that these intuitions are grounded in people's personal", "tokens": [51096, 436, 764, 2856, 281, 519, 13, 400, 1391, 300, 613, 16224, 626, 366, 23535, 294, 561, 311, 2973, 51484], "temperature": 0.0, "avg_logprob": -0.08720680383535531, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.002250001998618245}, {"id": 107, "seek": 65212, "start": 652.12, "end": 660.76, "text": " experiences thinking. And so one important fact to keep in mind is that there is huge", "tokens": [50364, 5235, 1953, 13, 400, 370, 472, 1021, 1186, 281, 1066, 294, 1575, 307, 300, 456, 307, 2603, 50796], "temperature": 0.0, "avg_logprob": -0.07777710429957656, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.03207928687334061}, {"id": 108, "seek": 65212, "start": 660.76, "end": 669.0, "text": " individual variability in how people perceive their own thinking to be. And so my pet theory", "tokens": [50796, 2609, 35709, 294, 577, 561, 20281, 641, 1065, 1953, 281, 312, 13, 400, 370, 452, 3817, 5261, 51208], "temperature": 0.0, "avg_logprob": -0.07777710429957656, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.03207928687334061}, {"id": 109, "seek": 65212, "start": 669.0, "end": 675.96, "text": " is that a lot of philosophers are strong verbal thinkers, they spend a lot of time writing,", "tokens": [51208, 307, 300, 257, 688, 295, 36839, 366, 2068, 24781, 37895, 11, 436, 3496, 257, 688, 295, 565, 3579, 11, 51556], "temperature": 0.0, "avg_logprob": -0.07777710429957656, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.03207928687334061}, {"id": 110, "seek": 67596, "start": 675.96, "end": 683.0, "text": " they think about abstract topics. And so to them, the link between language and thought and", "tokens": [50364, 436, 519, 466, 12649, 8378, 13, 400, 370, 281, 552, 11, 264, 2113, 1296, 2856, 293, 1194, 293, 50716], "temperature": 0.0, "avg_logprob": -0.08069928680978171, "compression_ratio": 1.7259615384615385, "no_speech_prob": 0.0896819457411766}, {"id": 111, "seek": 67596, "start": 683.0, "end": 690.36, "text": " their experience is very strong. And it just seems that people, not just philosophers,", "tokens": [50716, 641, 1752, 307, 588, 2068, 13, 400, 309, 445, 2544, 300, 561, 11, 406, 445, 36839, 11, 51084], "temperature": 0.0, "avg_logprob": -0.08069928680978171, "compression_ratio": 1.7259615384615385, "no_speech_prob": 0.0896819457411766}, {"id": 112, "seek": 67596, "start": 691.08, "end": 698.12, "text": " have this tendency to assume that everybody else thinks in kind of the same way. And so", "tokens": [51120, 362, 341, 18187, 281, 6552, 300, 2201, 1646, 7309, 294, 733, 295, 264, 912, 636, 13, 400, 370, 51472], "temperature": 0.0, "avg_logprob": -0.08069928680978171, "compression_ratio": 1.7259615384615385, "no_speech_prob": 0.0896819457411766}, {"id": 113, "seek": 67596, "start": 698.76, "end": 704.12, "text": " if you are a strong verbal thinker, you automatically assume that everybody else is as well.", "tokens": [51504, 498, 291, 366, 257, 2068, 24781, 519, 260, 11, 291, 6772, 6552, 300, 2201, 1646, 307, 382, 731, 13, 51772], "temperature": 0.0, "avg_logprob": -0.08069928680978171, "compression_ratio": 1.7259615384615385, "no_speech_prob": 0.0896819457411766}, {"id": 114, "seek": 70412, "start": 704.68, "end": 711.4, "text": " Until you start actually talking to other people and asking about their experiences.", "tokens": [50392, 9088, 291, 722, 767, 1417, 281, 661, 561, 293, 3365, 466, 641, 5235, 13, 50728], "temperature": 0.0, "avg_logprob": -0.13564186096191405, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.003703800495713949}, {"id": 115, "seek": 70412, "start": 711.4, "end": 717.08, "text": " And so I've had these conversations with people at parties or just informally, you ask them,", "tokens": [50728, 400, 370, 286, 600, 632, 613, 7315, 365, 561, 412, 8265, 420, 445, 1356, 379, 11, 291, 1029, 552, 11, 51012], "temperature": 0.0, "avg_logprob": -0.13564186096191405, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.003703800495713949}, {"id": 116, "seek": 70412, "start": 717.08, "end": 723.72, "text": " oh, hey, how often do you think in words? Most of the time, some of the time never. And people", "tokens": [51012, 1954, 11, 4177, 11, 577, 2049, 360, 291, 519, 294, 2283, 30, 4534, 295, 264, 565, 11, 512, 295, 264, 565, 1128, 13, 400, 561, 51344], "temperature": 0.0, "avg_logprob": -0.13564186096191405, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.003703800495713949}, {"id": 117, "seek": 70412, "start": 723.72, "end": 728.92, "text": " are always surprised to hear that other people's experiences might be completely different. And", "tokens": [51344, 366, 1009, 6100, 281, 1568, 300, 661, 561, 311, 5235, 1062, 312, 2584, 819, 13, 400, 51604], "temperature": 0.0, "avg_logprob": -0.13564186096191405, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.003703800495713949}, {"id": 118, "seek": 72892, "start": 728.92, "end": 738.04, "text": " so this is just, I think, a very important thing to keep in mind that our intuitions", "tokens": [50364, 370, 341, 307, 445, 11, 286, 519, 11, 257, 588, 1021, 551, 281, 1066, 294, 1575, 300, 527, 16224, 626, 50820], "temperature": 0.0, "avg_logprob": -0.10718324218971143, "compression_ratio": 1.5, "no_speech_prob": 0.000587643007747829}, {"id": 119, "seek": 72892, "start": 738.04, "end": 743.4, "text": " can lead us because they don't necessarily reflect a universal human experience. It's just,", "tokens": [50820, 393, 1477, 505, 570, 436, 500, 380, 4725, 5031, 257, 11455, 1952, 1752, 13, 467, 311, 445, 11, 51088], "temperature": 0.0, "avg_logprob": -0.10718324218971143, "compression_ratio": 1.5, "no_speech_prob": 0.000587643007747829}, {"id": 120, "seek": 72892, "start": 743.4, "end": 753.3199999999999, "text": " you know, that's how we think. And so on to the actual evidence that we can use to dissociate", "tokens": [51088, 291, 458, 11, 300, 311, 577, 321, 519, 13, 400, 370, 322, 281, 264, 3539, 4467, 300, 321, 393, 764, 281, 44446, 473, 51584], "temperature": 0.0, "avg_logprob": -0.10718324218971143, "compression_ratio": 1.5, "no_speech_prob": 0.000587643007747829}, {"id": 121, "seek": 75332, "start": 753.32, "end": 765.1600000000001, "text": " language and thought. There are a few different strands. So one example, very briefly, is the fact", "tokens": [50364, 2856, 293, 1194, 13, 821, 366, 257, 1326, 819, 29664, 13, 407, 472, 1365, 11, 588, 10515, 11, 307, 264, 1186, 50956], "temperature": 0.0, "avg_logprob": -0.1572913294253142, "compression_ratio": 1.489795918367347, "no_speech_prob": 0.01638936810195446}, {"id": 122, "seek": 75332, "start": 765.1600000000001, "end": 771.88, "text": " that animals who don't have language might often have pretty sophisticated thought and planning", "tokens": [50956, 300, 4882, 567, 500, 380, 362, 2856, 1062, 2049, 362, 1238, 16950, 1194, 293, 5038, 51292], "temperature": 0.0, "avg_logprob": -0.1572913294253142, "compression_ratio": 1.489795918367347, "no_speech_prob": 0.01638936810195446}, {"id": 123, "seek": 75332, "start": 771.88, "end": 781.96, "text": " capabilities, right? We know examples of crows being very smart, alphoctopi, even a squirrel that", "tokens": [51292, 10862, 11, 558, 30, 492, 458, 5110, 295, 941, 1509, 885, 588, 4069, 11, 419, 950, 78, 349, 404, 72, 11, 754, 257, 28565, 300, 51796], "temperature": 0.0, "avg_logprob": -0.1572913294253142, "compression_ratio": 1.489795918367347, "no_speech_prob": 0.01638936810195446}, {"id": 124, "seek": 78196, "start": 781.96, "end": 786.76, "text": " is trying to figure out whether to jump from tree to tree or if it's too far and it needs to go on", "tokens": [50364, 307, 1382, 281, 2573, 484, 1968, 281, 3012, 490, 4230, 281, 4230, 420, 498, 309, 311, 886, 1400, 293, 309, 2203, 281, 352, 322, 50604], "temperature": 0.0, "avg_logprob": -0.08213003476460774, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.0015961817698553205}, {"id": 125, "seek": 78196, "start": 786.76, "end": 793.64, "text": " the ground instead. These are pretty sophisticated capabilities. And so that's just a very basic", "tokens": [50604, 264, 2727, 2602, 13, 1981, 366, 1238, 16950, 10862, 13, 400, 370, 300, 311, 445, 257, 588, 3875, 50948], "temperature": 0.0, "avg_logprob": -0.08213003476460774, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.0015961817698553205}, {"id": 126, "seek": 78196, "start": 793.64, "end": 798.76, "text": " example of at least some kinds of thought. You can then argue, you know, oh, but like the kind of", "tokens": [50948, 1365, 295, 412, 1935, 512, 3685, 295, 1194, 13, 509, 393, 550, 9695, 11, 291, 458, 11, 1954, 11, 457, 411, 264, 733, 295, 51204], "temperature": 0.0, "avg_logprob": -0.08213003476460774, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.0015961817698553205}, {"id": 127, "seek": 78196, "start": 798.76, "end": 802.6, "text": " thinking they're doing is not, you know, the kind of thinking that we care about, right? And that's", "tokens": [51204, 1953, 436, 434, 884, 307, 406, 11, 291, 458, 11, 264, 733, 295, 1953, 300, 321, 1127, 466, 11, 558, 30, 400, 300, 311, 51396], "temperature": 0.0, "avg_logprob": -0.08213003476460774, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.0015961817698553205}, {"id": 128, "seek": 78196, "start": 802.6, "end": 807.4000000000001, "text": " where the meat of the debate is. But pretty sophisticated cognition is possible in non-human", "tokens": [51396, 689, 264, 4615, 295, 264, 7958, 307, 13, 583, 1238, 16950, 46905, 307, 1944, 294, 2107, 12, 18796, 51636], "temperature": 0.0, "avg_logprob": -0.08213003476460774, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.0015961817698553205}, {"id": 129, "seek": 80740, "start": 807.4, "end": 817.56, "text": " animals from what we know. For me, I work with humans and adult humans. And so what we can do", "tokens": [50364, 4882, 490, 437, 321, 458, 13, 1171, 385, 11, 286, 589, 365, 6255, 293, 5075, 6255, 13, 400, 370, 437, 321, 393, 360, 50872], "temperature": 0.0, "avg_logprob": -0.07716971549434938, "compression_ratio": 1.6145251396648044, "no_speech_prob": 0.0015971066895872355}, {"id": 130, "seek": 80740, "start": 817.56, "end": 827.56, "text": " is we can identify parts of the brain that are engaged in language processing. So it turns out", "tokens": [50872, 307, 321, 393, 5876, 3166, 295, 264, 3567, 300, 366, 8237, 294, 2856, 9007, 13, 407, 309, 4523, 484, 51372], "temperature": 0.0, "avg_logprob": -0.07716971549434938, "compression_ratio": 1.6145251396648044, "no_speech_prob": 0.0015971066895872355}, {"id": 131, "seek": 80740, "start": 827.56, "end": 835.8, "text": " that there is a set of brain regions in the brain known as the language network that are responsible", "tokens": [51372, 300, 456, 307, 257, 992, 295, 3567, 10682, 294, 264, 3567, 2570, 382, 264, 2856, 3209, 300, 366, 6250, 51784], "temperature": 0.0, "avg_logprob": -0.07716971549434938, "compression_ratio": 1.6145251396648044, "no_speech_prob": 0.0015971066895872355}, {"id": 132, "seek": 83580, "start": 835.8, "end": 841.4799999999999, "text": " for language comprehension. So whether you're listening to somebody talk or reading, they're", "tokens": [50364, 337, 2856, 44991, 13, 407, 1968, 291, 434, 4764, 281, 2618, 751, 420, 3760, 11, 436, 434, 50648], "temperature": 0.0, "avg_logprob": -0.09683060064548399, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.0023220195434987545}, {"id": 133, "seek": 83580, "start": 841.4799999999999, "end": 845.9599999999999, "text": " also engaged during language production. So when you're speaking and when you're writing,", "tokens": [50648, 611, 8237, 1830, 2856, 4265, 13, 407, 562, 291, 434, 4124, 293, 562, 291, 434, 3579, 11, 50872], "temperature": 0.0, "avg_logprob": -0.09683060064548399, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.0023220195434987545}, {"id": 134, "seek": 83580, "start": 846.92, "end": 853.3199999999999, "text": " they are engaged in response to any language that you might know that also includes sign languages.", "tokens": [50920, 436, 366, 8237, 294, 4134, 281, 604, 2856, 300, 291, 1062, 458, 300, 611, 5974, 1465, 8650, 13, 51240], "temperature": 0.0, "avg_logprob": -0.09683060064548399, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.0023220195434987545}, {"id": 135, "seek": 83580, "start": 853.3199999999999, "end": 861.0799999999999, "text": " So it doesn't even have to be a spoken language. And these regions turns out are pretty selective", "tokens": [51240, 407, 309, 1177, 380, 754, 362, 281, 312, 257, 10759, 2856, 13, 400, 613, 10682, 4523, 484, 366, 1238, 33930, 51628], "temperature": 0.0, "avg_logprob": -0.09683060064548399, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.0023220195434987545}, {"id": 136, "seek": 86108, "start": 861.08, "end": 868.9200000000001, "text": " for language. So they respond to all kinds of language, but not to music, not to math,", "tokens": [50364, 337, 2856, 13, 407, 436, 4196, 281, 439, 3685, 295, 2856, 11, 457, 406, 281, 1318, 11, 406, 281, 5221, 11, 50756], "temperature": 0.0, "avg_logprob": -0.06924267113208771, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.007572764530777931}, {"id": 137, "seek": 86108, "start": 869.5600000000001, "end": 879.5600000000001, "text": " not to general problem solving. And so this is pretty strong evidence that language and many", "tokens": [50788, 406, 281, 2674, 1154, 12606, 13, 400, 370, 341, 307, 1238, 2068, 4467, 300, 2856, 293, 867, 51288], "temperature": 0.0, "avg_logprob": -0.06924267113208771, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.007572764530777931}, {"id": 138, "seek": 86108, "start": 879.5600000000001, "end": 885.0, "text": " different kinds of thinking are actually separable in the brain. That language has its own neural", "tokens": [51288, 819, 3685, 295, 1953, 366, 767, 3128, 712, 294, 264, 3567, 13, 663, 2856, 575, 1080, 1065, 18161, 51560], "temperature": 0.0, "avg_logprob": -0.06924267113208771, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.007572764530777931}, {"id": 139, "seek": 88500, "start": 885.0, "end": 894.12, "text": " machinery. And that's important because it turns out that if language areas of the brain", "tokens": [50364, 27302, 13, 400, 300, 311, 1021, 570, 309, 4523, 484, 300, 498, 2856, 3179, 295, 264, 3567, 50820], "temperature": 0.0, "avg_logprob": -0.08958150423490084, "compression_ratio": 1.5469613259668509, "no_speech_prob": 0.01048615388572216}, {"id": 140, "seek": 88500, "start": 894.12, "end": 901.16, "text": " are damaged, it will affect your ability to use language, but not necessarily your ability to", "tokens": [50820, 366, 14080, 11, 309, 486, 3345, 428, 3485, 281, 764, 2856, 11, 457, 406, 4725, 428, 3485, 281, 51172], "temperature": 0.0, "avg_logprob": -0.08958150423490084, "compression_ratio": 1.5469613259668509, "no_speech_prob": 0.01048615388572216}, {"id": 141, "seek": 88500, "start": 901.16, "end": 909.24, "text": " think. And so the most common example of that is a condition known as aphasia. So it difficulties", "tokens": [51172, 519, 13, 400, 370, 264, 881, 2689, 1365, 295, 300, 307, 257, 4188, 2570, 382, 257, 7485, 654, 13, 407, 309, 14399, 51576], "temperature": 0.0, "avg_logprob": -0.08958150423490084, "compression_ratio": 1.5469613259668509, "no_speech_prob": 0.01048615388572216}, {"id": 142, "seek": 90924, "start": 909.24, "end": 915.64, "text": " with language production or comprehension. Often, most commonly, it arises as a result of", "tokens": [50364, 365, 2856, 4265, 420, 44991, 13, 20043, 11, 881, 12719, 11, 309, 27388, 382, 257, 1874, 295, 50684], "temperature": 0.0, "avg_logprob": -0.13791381120681762, "compression_ratio": 1.7725118483412323, "no_speech_prob": 0.03619328886270523}, {"id": 143, "seek": 90924, "start": 916.28, "end": 921.64, "text": " a person having a stroke. And so if it's stroke effects left hemisphere, which is where the", "tokens": [50716, 257, 954, 1419, 257, 12403, 13, 400, 370, 498, 309, 311, 12403, 5065, 1411, 38453, 11, 597, 307, 689, 264, 50984], "temperature": 0.0, "avg_logprob": -0.13791381120681762, "compression_ratio": 1.7725118483412323, "no_speech_prob": 0.03619328886270523}, {"id": 144, "seek": 90924, "start": 921.64, "end": 927.16, "text": " language network is in most people, they might have really serious difficulties with language", "tokens": [50984, 2856, 3209, 307, 294, 881, 561, 11, 436, 1062, 362, 534, 3156, 14399, 365, 2856, 51260], "temperature": 0.0, "avg_logprob": -0.13791381120681762, "compression_ratio": 1.7725118483412323, "no_speech_prob": 0.03619328886270523}, {"id": 145, "seek": 90924, "start": 927.16, "end": 933.88, "text": " production or comprehension. But if that language is limited to the language network, it turns out", "tokens": [51260, 4265, 420, 44991, 13, 583, 498, 300, 2856, 307, 5567, 281, 264, 2856, 3209, 11, 309, 4523, 484, 51596], "temperature": 0.0, "avg_logprob": -0.13791381120681762, "compression_ratio": 1.7725118483412323, "no_speech_prob": 0.03619328886270523}, {"id": 146, "seek": 93388, "start": 933.88, "end": 942.92, "text": " that their ability to use other kinds of thinking remains intact. So these people with really severe", "tokens": [50364, 300, 641, 3485, 281, 764, 661, 3685, 295, 1953, 7023, 23493, 13, 407, 613, 561, 365, 534, 8922, 50816], "temperature": 0.0, "avg_logprob": -0.1112409167819553, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0029335522558540106}, {"id": 147, "seek": 93388, "start": 942.92, "end": 950.92, "text": " aphasia who really can't understand language or speak, they can solve math problems. They can", "tokens": [50816, 257, 7485, 654, 567, 534, 393, 380, 1223, 2856, 420, 1710, 11, 436, 393, 5039, 5221, 2740, 13, 814, 393, 51216], "temperature": 0.0, "avg_logprob": -0.1112409167819553, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0029335522558540106}, {"id": 148, "seek": 93388, "start": 950.92, "end": 957.72, "text": " arrange pictures so they form a story so they can reason about cause and effect. They can look at", "tokens": [51216, 9424, 5242, 370, 436, 1254, 257, 1657, 370, 436, 393, 1778, 466, 3082, 293, 1802, 13, 814, 393, 574, 412, 51556], "temperature": 0.0, "avg_logprob": -0.1112409167819553, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0029335522558540106}, {"id": 149, "seek": 93388, "start": 957.72, "end": 962.68, "text": " the picture or show in some kind of event, like a fox cheese in the rabbit or the rabbit cheese in", "tokens": [51556, 264, 3036, 420, 855, 294, 512, 733, 295, 2280, 11, 411, 257, 21026, 5399, 294, 264, 19509, 420, 264, 19509, 5399, 294, 51804], "temperature": 0.0, "avg_logprob": -0.1112409167819553, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0029335522558540106}, {"id": 150, "seek": 96268, "start": 962.68, "end": 968.5999999999999, "text": " the fox and say which one is more likely to happen in the real world. If it's something like really", "tokens": [50364, 264, 21026, 293, 584, 597, 472, 307, 544, 3700, 281, 1051, 294, 264, 957, 1002, 13, 759, 309, 311, 746, 411, 534, 50660], "temperature": 0.0, "avg_logprob": -0.1350507568894771, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.004535263404250145}, {"id": 151, "seek": 96268, "start": 968.5999999999999, "end": 974.92, "text": " weird, like the scuba diver biting a shark, they will laugh because it's just kind of ridiculous.", "tokens": [50660, 3657, 11, 411, 264, 795, 12584, 18558, 32912, 257, 13327, 11, 436, 486, 5801, 570, 309, 311, 445, 733, 295, 11083, 13, 50976], "temperature": 0.0, "avg_logprob": -0.1350507568894771, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.004535263404250145}, {"id": 152, "seek": 96268, "start": 974.92, "end": 979.16, "text": " And so, you know, you can tell that they understand what's going on. And there are really fascinating", "tokens": [50976, 400, 370, 11, 291, 458, 11, 291, 393, 980, 300, 436, 1223, 437, 311, 516, 322, 13, 400, 456, 366, 534, 10343, 51188], "temperature": 0.0, "avg_logprob": -0.1350507568894771, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.004535263404250145}, {"id": 153, "seek": 96268, "start": 979.16, "end": 983.8, "text": " cases, you know, some of them like like the plague chest on the weekend. So clearly, very", "tokens": [51188, 3331, 11, 291, 458, 11, 512, 295, 552, 411, 411, 264, 28185, 7443, 322, 264, 6711, 13, 407, 4448, 11, 588, 51420], "temperature": 0.0, "avg_logprob": -0.1350507568894771, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.004535263404250145}, {"id": 154, "seek": 96268, "start": 983.8, "end": 990.52, "text": " sophisticated forms of reasoning are preserved even in the face of severe damage to language", "tokens": [51420, 16950, 6422, 295, 21577, 366, 22242, 754, 294, 264, 1851, 295, 8922, 4344, 281, 2856, 51756], "temperature": 0.0, "avg_logprob": -0.1350507568894771, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.004535263404250145}, {"id": 155, "seek": 99052, "start": 990.52, "end": 995.96, "text": " processing centres in the brain. Yeah, that's completely fascinating. And well, firstly, by the", "tokens": [50364, 9007, 30096, 294, 264, 3567, 13, 865, 11, 300, 311, 2584, 10343, 13, 400, 731, 11, 27376, 11, 538, 264, 50636], "temperature": 0.0, "avg_logprob": -0.11904677003622055, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.001239501521922648}, {"id": 156, "seek": 99052, "start": 995.96, "end": 1005.88, "text": " way, I love your theory about philosophers and how maybe the sort of minds that they have that", "tokens": [50636, 636, 11, 286, 959, 428, 5261, 466, 36839, 293, 577, 1310, 264, 1333, 295, 9634, 300, 436, 362, 300, 51132], "temperature": 0.0, "avg_logprob": -0.11904677003622055, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.001239501521922648}, {"id": 157, "seek": 99052, "start": 1005.88, "end": 1014.6, "text": " make them good philosophers sort of self select or selecting a very biased or unusual community of", "tokens": [51132, 652, 552, 665, 36839, 1333, 295, 2698, 3048, 420, 18182, 257, 588, 28035, 420, 10901, 1768, 295, 51568], "temperature": 0.0, "avg_logprob": -0.11904677003622055, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.001239501521922648}, {"id": 158, "seek": 101460, "start": 1014.6, "end": 1022.52, "text": " people who think in a particular way. And yeah, so that's really interesting. It'd be great to have a", "tokens": [50364, 561, 567, 519, 294, 257, 1729, 636, 13, 400, 1338, 11, 370, 300, 311, 534, 1880, 13, 467, 1116, 312, 869, 281, 362, 257, 50760], "temperature": 0.0, "avg_logprob": -0.14330541569253671, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.03492487221956253}, {"id": 159, "seek": 101460, "start": 1022.52, "end": 1028.92, "text": " survey of philosophers, I don't know if this has happened, and how they describe their own thoughts", "tokens": [50760, 8984, 295, 36839, 11, 286, 500, 380, 458, 498, 341, 575, 2011, 11, 293, 577, 436, 6786, 641, 1065, 4598, 51080], "temperature": 0.0, "avg_logprob": -0.14330541569253671, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.03492487221956253}, {"id": 160, "seek": 101460, "start": 1028.92, "end": 1036.3600000000001, "text": " and compare it to other groups. Yeah, very interesting. Yeah, I think sometimes that maybe I", "tokens": [51080, 293, 6794, 309, 281, 661, 3935, 13, 865, 11, 588, 1880, 13, 865, 11, 286, 519, 2171, 300, 1310, 286, 51452], "temperature": 0.0, "avg_logprob": -0.14330541569253671, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.03492487221956253}, {"id": 161, "seek": 101460, "start": 1036.3600000000001, "end": 1039.72, "text": " should, you know, not talk about this theory and actually test it experimentally first,", "tokens": [51452, 820, 11, 291, 458, 11, 406, 751, 466, 341, 5261, 293, 767, 1500, 309, 5120, 379, 700, 11, 51620], "temperature": 0.0, "avg_logprob": -0.14330541569253671, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.03492487221956253}, {"id": 162, "seek": 103972, "start": 1039.8, "end": 1047.0, "text": " that we add on biased people in advance. No, as long as they don't listen to this,", "tokens": [50368, 300, 321, 909, 322, 28035, 561, 294, 7295, 13, 883, 11, 382, 938, 382, 436, 500, 380, 2140, 281, 341, 11, 50728], "temperature": 0.0, "avg_logprob": -0.15076259174178133, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.0054494114592671394}, {"id": 163, "seek": 103972, "start": 1047.0, "end": 1052.52, "text": " or maybe you can do it anonymously or something. I'm actually talking, I think,", "tokens": [50728, 420, 1310, 291, 393, 360, 309, 37293, 5098, 420, 746, 13, 286, 478, 767, 1417, 11, 286, 519, 11, 51004], "temperature": 0.0, "avg_logprob": -0.15076259174178133, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.0054494114592671394}, {"id": 164, "seek": 103972, "start": 1053.48, "end": 1059.56, "text": " soon to someone who from the philosophy of science who surveyed physicists to see if they are", "tokens": [51052, 2321, 281, 1580, 567, 490, 264, 10675, 295, 3497, 567, 8984, 292, 48716, 281, 536, 498, 436, 366, 51356], "temperature": 0.0, "avg_logprob": -0.15076259174178133, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.0054494114592671394}, {"id": 165, "seek": 103972, "start": 1059.56, "end": 1063.32, "text": " realists in terms of, you know, how they think about the entities of science or not. So I think", "tokens": [51356, 957, 1751, 294, 2115, 295, 11, 291, 458, 11, 577, 436, 519, 466, 264, 16667, 295, 3497, 420, 406, 13, 407, 286, 519, 51544], "temperature": 0.0, "avg_logprob": -0.15076259174178133, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.0054494114592671394}, {"id": 166, "seek": 103972, "start": 1063.32, "end": 1068.1200000000001, "text": " it's like it is really interesting to actually just, yeah, try to figure out how it is that", "tokens": [51544, 309, 311, 411, 309, 307, 534, 1880, 281, 767, 445, 11, 1338, 11, 853, 281, 2573, 484, 577, 309, 307, 300, 51784], "temperature": 0.0, "avg_logprob": -0.15076259174178133, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.0054494114592671394}, {"id": 167, "seek": 106972, "start": 1070.68, "end": 1076.52, "text": " yeah, how it is that people's personal beliefs and their kind of academic disciplines or their own", "tokens": [50412, 1338, 11, 577, 309, 307, 300, 561, 311, 2973, 13585, 293, 641, 733, 295, 7778, 21919, 420, 641, 1065, 50704], "temperature": 0.0, "avg_logprob": -0.13697756168454192, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.0003565159277059138}, {"id": 168, "seek": 106972, "start": 1076.52, "end": 1082.84, "text": " or the peculiarities of their minds intertwine. But coming back to the kind of experiments that", "tokens": [50704, 420, 264, 27149, 1088, 295, 641, 9634, 44400, 533, 13, 583, 1348, 646, 281, 264, 733, 295, 12050, 300, 51020], "temperature": 0.0, "avg_logprob": -0.13697756168454192, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.0003565159277059138}, {"id": 169, "seek": 106972, "start": 1082.84, "end": 1090.04, "text": " you describe, so yeah, I think these are just really, yeah, wonderful illustrations of how", "tokens": [51020, 291, 6786, 11, 370, 1338, 11, 286, 519, 613, 366, 445, 534, 11, 1338, 11, 3715, 34540, 295, 577, 51380], "temperature": 0.0, "avg_logprob": -0.13697756168454192, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.0003565159277059138}, {"id": 170, "seek": 106972, "start": 1090.04, "end": 1096.2, "text": " thought maybe extends beyond the language network. And I suppose what you're doing is you're asking", "tokens": [51380, 1194, 1310, 26448, 4399, 264, 2856, 3209, 13, 400, 286, 7297, 437, 291, 434, 884, 307, 291, 434, 3365, 51688], "temperature": 0.0, "avg_logprob": -0.13697756168454192, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.0003565159277059138}, {"id": 171, "seek": 109620, "start": 1096.2, "end": 1102.76, "text": " people, so for example, the way that we know that music is not within the language network is,", "tokens": [50364, 561, 11, 370, 337, 1365, 11, 264, 636, 300, 321, 458, 300, 1318, 307, 406, 1951, 264, 2856, 3209, 307, 11, 50692], "temperature": 0.0, "avg_logprob": -0.0954616529899731, "compression_ratio": 1.828793774319066, "no_speech_prob": 0.0050396304577589035}, {"id": 172, "seek": 109620, "start": 1103.56, "end": 1107.88, "text": " I don't know, the language network is defined as the kind of a bit of the brain that lights up", "tokens": [50732, 286, 500, 380, 458, 11, 264, 2856, 3209, 307, 7642, 382, 264, 733, 295, 257, 857, 295, 264, 3567, 300, 5811, 493, 50948], "temperature": 0.0, "avg_logprob": -0.0954616529899731, "compression_ratio": 1.828793774319066, "no_speech_prob": 0.0050396304577589035}, {"id": 173, "seek": 109620, "start": 1107.88, "end": 1113.8, "text": " in MRI scans. You see a lot of activity there when you give people sentences and linguistic", "tokens": [50948, 294, 32812, 35116, 13, 509, 536, 257, 688, 295, 5191, 456, 562, 291, 976, 561, 16579, 293, 43002, 51244], "temperature": 0.0, "avg_logprob": -0.0954616529899731, "compression_ratio": 1.828793774319066, "no_speech_prob": 0.0050396304577589035}, {"id": 174, "seek": 109620, "start": 1114.76, "end": 1120.1200000000001, "text": " tasks, I don't know, maybe reading or producing language. And then it's a different area of the", "tokens": [51292, 9608, 11, 286, 500, 380, 458, 11, 1310, 3760, 420, 10501, 2856, 13, 400, 550, 309, 311, 257, 819, 1859, 295, 264, 51560], "temperature": 0.0, "avg_logprob": -0.0954616529899731, "compression_ratio": 1.828793774319066, "no_speech_prob": 0.0050396304577589035}, {"id": 175, "seek": 109620, "start": 1120.1200000000001, "end": 1125.8, "text": " brain that lights up when they're listening to music or when they're solving a math or chess", "tokens": [51560, 3567, 300, 5811, 493, 562, 436, 434, 4764, 281, 1318, 420, 562, 436, 434, 12606, 257, 5221, 420, 24122, 51844], "temperature": 0.0, "avg_logprob": -0.0954616529899731, "compression_ratio": 1.828793774319066, "no_speech_prob": 0.0050396304577589035}, {"id": 176, "seek": 112580, "start": 1125.8, "end": 1135.08, "text": " problem. And even when people have quite severe damage at South Asia, and the language part of", "tokens": [50364, 1154, 13, 400, 754, 562, 561, 362, 1596, 8922, 4344, 412, 4242, 10038, 11, 293, 264, 2856, 644, 295, 50828], "temperature": 0.0, "avg_logprob": -0.12466361013691077, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0005857169744558632}, {"id": 177, "seek": 112580, "start": 1135.08, "end": 1140.36, "text": " the brain is unable to comprehend or produce language or both, they can still do many of", "tokens": [50828, 264, 3567, 307, 11299, 281, 38183, 420, 5258, 2856, 420, 1293, 11, 436, 393, 920, 360, 867, 295, 51092], "temperature": 0.0, "avg_logprob": -0.12466361013691077, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0005857169744558632}, {"id": 178, "seek": 112580, "start": 1140.36, "end": 1145.8799999999999, "text": " those other things, which is, I mean, that's really interesting for one thing, because I often", "tokens": [51092, 729, 661, 721, 11, 597, 307, 11, 286, 914, 11, 300, 311, 534, 1880, 337, 472, 551, 11, 570, 286, 2049, 51368], "temperature": 0.0, "avg_logprob": -0.12466361013691077, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0005857169744558632}, {"id": 179, "seek": 112580, "start": 1145.8799999999999, "end": 1152.68, "text": " think as well, language is maybe being so key to the kind of input output of the brain that,", "tokens": [51368, 519, 382, 731, 11, 2856, 307, 1310, 885, 370, 2141, 281, 264, 733, 295, 4846, 5598, 295, 264, 3567, 300, 11, 51708], "temperature": 0.0, "avg_logprob": -0.12466361013691077, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0005857169744558632}, {"id": 180, "seek": 115268, "start": 1153.4, "end": 1160.1200000000001, "text": " you know, for example, reading a math problem would kind of go via the language network,", "tokens": [50400, 291, 458, 11, 337, 1365, 11, 3760, 257, 5221, 1154, 576, 733, 295, 352, 5766, 264, 2856, 3209, 11, 50736], "temperature": 0.0, "avg_logprob": -0.12639093399047852, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.000999367912299931}, {"id": 181, "seek": 115268, "start": 1160.1200000000001, "end": 1166.1200000000001, "text": " or is it that our brain is sort of able to just kind of directly take those symbols into,", "tokens": [50736, 420, 307, 309, 300, 527, 3567, 307, 1333, 295, 1075, 281, 445, 733, 295, 3838, 747, 729, 16944, 666, 11, 51036], "temperature": 0.0, "avg_logprob": -0.12639093399047852, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.000999367912299931}, {"id": 182, "seek": 115268, "start": 1166.1200000000001, "end": 1173.0800000000002, "text": " I don't know, a different area of the brain, or perhaps do we have to kind of pose those", "tokens": [51036, 286, 500, 380, 458, 11, 257, 819, 1859, 295, 264, 3567, 11, 420, 4317, 360, 321, 362, 281, 733, 295, 10774, 729, 51384], "temperature": 0.0, "avg_logprob": -0.12639093399047852, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.000999367912299931}, {"id": 183, "seek": 115268, "start": 1173.0800000000002, "end": 1179.3200000000002, "text": " problems in a kind of more visual way? I don't know, I'm curious about whether the language", "tokens": [51384, 2740, 294, 257, 733, 295, 544, 5056, 636, 30, 286, 500, 380, 458, 11, 286, 478, 6369, 466, 1968, 264, 2856, 51696], "temperature": 0.0, "avg_logprob": -0.12639093399047852, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.000999367912299931}, {"id": 184, "seek": 117932, "start": 1179.32, "end": 1186.2, "text": " network is kind of a gateway for much of the information going in. So it looks like if you", "tokens": [50364, 3209, 307, 733, 295, 257, 28532, 337, 709, 295, 264, 1589, 516, 294, 13, 407, 309, 1542, 411, 498, 291, 50708], "temperature": 0.0, "avg_logprob": -0.10849447359983948, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.0023576875682920218}, {"id": 185, "seek": 117932, "start": 1186.2, "end": 1193.6399999999999, "text": " give people math problems in the form of mathematical symbols, right, like five plus three,", "tokens": [50708, 976, 561, 5221, 2740, 294, 264, 1254, 295, 18894, 16944, 11, 558, 11, 411, 1732, 1804, 1045, 11, 51080], "temperature": 0.0, "avg_logprob": -0.10849447359983948, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.0023576875682920218}, {"id": 186, "seek": 117932, "start": 1193.6399999999999, "end": 1199.0, "text": " first question mark, it seems like it doesn't need to go through the language network. So even", "tokens": [51080, 700, 1168, 1491, 11, 309, 2544, 411, 309, 1177, 380, 643, 281, 352, 807, 264, 2856, 3209, 13, 407, 754, 51348], "temperature": 0.0, "avg_logprob": -0.10849447359983948, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.0023576875682920218}, {"id": 187, "seek": 117932, "start": 1199.0, "end": 1206.6799999999998, "text": " though it's symbolic, not all symbols get processed by the language network. And perhaps even more", "tokens": [51348, 1673, 309, 311, 25755, 11, 406, 439, 16944, 483, 18846, 538, 264, 2856, 3209, 13, 400, 4317, 754, 544, 51732], "temperature": 0.0, "avg_logprob": -0.10849447359983948, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.0023576875682920218}, {"id": 188, "seek": 120668, "start": 1206.68, "end": 1216.28, "text": " strikingly, one study that I did in graduate school was looking at computer code. And specifically,", "tokens": [50364, 18559, 356, 11, 472, 2979, 300, 286, 630, 294, 8080, 1395, 390, 1237, 412, 3820, 3089, 13, 400, 4682, 11, 50844], "temperature": 0.0, "avg_logprob": -0.0853881415198831, "compression_ratio": 1.4793814432989691, "no_speech_prob": 0.002081186044961214}, {"id": 189, "seek": 120668, "start": 1216.28, "end": 1226.1200000000001, "text": " we looked at Python, which is very English-like by design, so it uses English words. And on the", "tokens": [50844, 321, 2956, 412, 15329, 11, 597, 307, 588, 3669, 12, 4092, 538, 1715, 11, 370, 309, 4960, 3669, 2283, 13, 400, 322, 264, 51336], "temperature": 0.0, "avg_logprob": -0.0853881415198831, "compression_ratio": 1.4793814432989691, "no_speech_prob": 0.002081186044961214}, {"id": 190, "seek": 120668, "start": 1226.1200000000001, "end": 1231.4, "text": " other end of the spectrum, we took a graphical programming language for kids called Scratch", "tokens": [51336, 661, 917, 295, 264, 11143, 11, 321, 1890, 257, 35942, 9410, 2856, 337, 2301, 1219, 34944, 852, 51600], "temperature": 0.0, "avg_logprob": -0.0853881415198831, "compression_ratio": 1.4793814432989691, "no_speech_prob": 0.002081186044961214}, {"id": 191, "seek": 123140, "start": 1231.4, "end": 1237.3200000000002, "text": " Junior. So it has different characters. And so then you have different arrows showing, you know,", "tokens": [50364, 21954, 13, 407, 309, 575, 819, 4342, 13, 400, 370, 550, 291, 362, 819, 19669, 4099, 11, 291, 458, 11, 50660], "temperature": 0.0, "avg_logprob": -0.11728125352125901, "compression_ratio": 1.7004405286343611, "no_speech_prob": 0.020638583227992058}, {"id": 192, "seek": 123140, "start": 1237.3200000000002, "end": 1242.2800000000002, "text": " the characters going left or jumping. But it has a lot of the same control flow elements that you", "tokens": [50660, 264, 4342, 516, 1411, 420, 11233, 13, 583, 309, 575, 257, 688, 295, 264, 912, 1969, 3095, 4959, 300, 291, 50908], "temperature": 0.0, "avg_logprob": -0.11728125352125901, "compression_ratio": 1.7004405286343611, "no_speech_prob": 0.020638583227992058}, {"id": 193, "seek": 123140, "start": 1242.2800000000002, "end": 1250.52, "text": " would have in text-based code, like if statements and for loops and stuff like that. And so it turns", "tokens": [50908, 576, 362, 294, 2487, 12, 6032, 3089, 11, 411, 498, 12363, 293, 337, 16121, 293, 1507, 411, 300, 13, 400, 370, 309, 4523, 51320], "temperature": 0.0, "avg_logprob": -0.11728125352125901, "compression_ratio": 1.7004405286343611, "no_speech_prob": 0.020638583227992058}, {"id": 194, "seek": 123140, "start": 1250.52, "end": 1255.8000000000002, "text": " out that for both of these languages, the main network in the brain that's responsible for", "tokens": [51320, 484, 300, 337, 1293, 295, 613, 8650, 11, 264, 2135, 3209, 294, 264, 3567, 300, 311, 6250, 337, 51584], "temperature": 0.0, "avg_logprob": -0.11728125352125901, "compression_ratio": 1.7004405286343611, "no_speech_prob": 0.020638583227992058}, {"id": 195, "seek": 125580, "start": 1255.8, "end": 1262.9199999999998, "text": " extracting meaning from that code is the so-called multiple demand network. And so that's the", "tokens": [50364, 49844, 3620, 490, 300, 3089, 307, 264, 370, 12, 11880, 3866, 4733, 3209, 13, 400, 370, 300, 311, 264, 50720], "temperature": 0.0, "avg_logprob": -0.10079576802808185, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.0076931193470954895}, {"id": 196, "seek": 125580, "start": 1262.9199999999998, "end": 1269.1599999999999, "text": " network that's responsible for problem solving and reasoning and planning, and not the language", "tokens": [50720, 3209, 300, 311, 6250, 337, 1154, 12606, 293, 21577, 293, 5038, 11, 293, 406, 264, 2856, 51032], "temperature": 0.0, "avg_logprob": -0.10079576802808185, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.0076931193470954895}, {"id": 197, "seek": 125580, "start": 1269.1599999999999, "end": 1275.48, "text": " network. The language network responded a little bit to Python code. But even there, we actually", "tokens": [51032, 3209, 13, 440, 2856, 3209, 15806, 257, 707, 857, 281, 15329, 3089, 13, 583, 754, 456, 11, 321, 767, 51348], "temperature": 0.0, "avg_logprob": -0.10079576802808185, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.0076931193470954895}, {"id": 198, "seek": 125580, "start": 1275.48, "end": 1281.3999999999999, "text": " weren't able to exactly establish its relation, its role and why it would. It might be some kind of", "tokens": [51348, 4999, 380, 1075, 281, 2293, 8327, 1080, 9721, 11, 1080, 3090, 293, 983, 309, 576, 13, 467, 1062, 312, 512, 733, 295, 51644], "temperature": 0.0, "avg_logprob": -0.10079576802808185, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.0076931193470954895}, {"id": 199, "seek": 128140, "start": 1281.4, "end": 1285.96, "text": " false positive where the language network is like, oh, that's language, oh, wait, no, never mind. And", "tokens": [50364, 7908, 3353, 689, 264, 2856, 3209, 307, 411, 11, 1954, 11, 300, 311, 2856, 11, 1954, 11, 1699, 11, 572, 11, 1128, 1575, 13, 400, 50592], "temperature": 0.0, "avg_logprob": -0.16694473136555066, "compression_ratio": 1.7053571428571428, "no_speech_prob": 0.03555610030889511}, {"id": 200, "seek": 128140, "start": 1285.96, "end": 1291.72, "text": " it kind of goes down. So there are other researchers that are promoting that theory currently. But", "tokens": [50592, 309, 733, 295, 1709, 760, 13, 407, 456, 366, 661, 10309, 300, 366, 16383, 300, 5261, 4362, 13, 583, 50880], "temperature": 0.0, "avg_logprob": -0.16694473136555066, "compression_ratio": 1.7053571428571428, "no_speech_prob": 0.03555610030889511}, {"id": 201, "seek": 128140, "start": 1293.64, "end": 1301.8000000000002, "text": " even for code, we call programming languages languages because how similar they are", "tokens": [50976, 754, 337, 3089, 11, 321, 818, 9410, 8650, 8650, 570, 577, 2531, 436, 366, 51384], "temperature": 0.0, "avg_logprob": -0.16694473136555066, "compression_ratio": 1.7053571428571428, "no_speech_prob": 0.03555610030889511}, {"id": 202, "seek": 128140, "start": 1301.8000000000002, "end": 1307.24, "text": " structurally to natural languages. Even there, it looks like it's not the language network that's", "tokens": [51384, 6594, 6512, 281, 3303, 8650, 13, 2754, 456, 11, 309, 1542, 411, 309, 311, 406, 264, 2856, 3209, 300, 311, 51656], "temperature": 0.0, "avg_logprob": -0.16694473136555066, "compression_ratio": 1.7053571428571428, "no_speech_prob": 0.03555610030889511}, {"id": 203, "seek": 130724, "start": 1307.24, "end": 1313.16, "text": " doing the majority of the heavy lifting. Yeah, I found that completely, well, surprising actually.", "tokens": [50364, 884, 264, 6286, 295, 264, 4676, 15798, 13, 865, 11, 286, 1352, 300, 2584, 11, 731, 11, 8830, 767, 13, 50660], "temperature": 0.0, "avg_logprob": -0.12405056034753081, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.0035148023162037134}, {"id": 204, "seek": 130724, "start": 1313.16, "end": 1318.28, "text": " And I think you noted in the paper that people kind of fell on two sides of the fence. Some people", "tokens": [50660, 400, 286, 519, 291, 12964, 294, 264, 3035, 300, 561, 733, 295, 5696, 322, 732, 4881, 295, 264, 15422, 13, 2188, 561, 50916], "temperature": 0.0, "avg_logprob": -0.12405056034753081, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.0035148023162037134}, {"id": 205, "seek": 130724, "start": 1318.28, "end": 1322.68, "text": " were surprised and some people were, oh, no, that makes complete sense. But I was personally", "tokens": [50916, 645, 6100, 293, 512, 561, 645, 11, 1954, 11, 572, 11, 300, 1669, 3566, 2020, 13, 583, 286, 390, 5665, 51136], "temperature": 0.0, "avg_logprob": -0.12405056034753081, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.0035148023162037134}, {"id": 206, "seek": 130724, "start": 1323.48, "end": 1328.6, "text": " really surprised because I, as you say, there's so much similarity between the way that natural", "tokens": [51176, 534, 6100, 570, 286, 11, 382, 291, 584, 11, 456, 311, 370, 709, 32194, 1296, 264, 636, 300, 3303, 51432], "temperature": 0.0, "avg_logprob": -0.12405056034753081, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.0035148023162037134}, {"id": 207, "seek": 130724, "start": 1328.6, "end": 1334.68, "text": " language works in terms of being compositional and having these kind of hierarchical features", "tokens": [51432, 2856, 1985, 294, 2115, 295, 885, 10199, 2628, 293, 1419, 613, 733, 295, 35250, 804, 4122, 51736], "temperature": 0.0, "avg_logprob": -0.12405056034753081, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.0035148023162037134}, {"id": 208, "seek": 133468, "start": 1334.68, "end": 1338.3600000000001, "text": " and the way that programming languages work that I would think, okay, well, you know,", "tokens": [50364, 293, 264, 636, 300, 9410, 8650, 589, 300, 286, 576, 519, 11, 1392, 11, 731, 11, 291, 458, 11, 50548], "temperature": 0.0, "avg_logprob": -0.1368769310616158, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.0005697864689864218}, {"id": 209, "seek": 133468, "start": 1341.5600000000002, "end": 1346.6000000000001, "text": " it's right that we call them languages because they're so closely related, they're just sort of,", "tokens": [50708, 309, 311, 558, 300, 321, 818, 552, 8650, 570, 436, 434, 370, 8185, 4077, 11, 436, 434, 445, 1333, 295, 11, 50960], "temperature": 0.0, "avg_logprob": -0.1368769310616158, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.0005697864689864218}, {"id": 210, "seek": 133468, "start": 1347.64, "end": 1356.44, "text": " I guess, a bit stricter, less ambiguous, perhaps. But the kind of, the nature of the rules is not", "tokens": [51012, 286, 2041, 11, 257, 857, 1056, 299, 391, 11, 1570, 39465, 11, 4317, 13, 583, 264, 733, 295, 11, 264, 3687, 295, 264, 4474, 307, 406, 51452], "temperature": 0.0, "avg_logprob": -0.1368769310616158, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.0005697864689864218}, {"id": 211, "seek": 135644, "start": 1356.44, "end": 1364.92, "text": " so different. And yet, yeah, it's almost as if one could imagine there being some sort of animal,", "tokens": [50364, 370, 819, 13, 400, 1939, 11, 1338, 11, 309, 311, 1920, 382, 498, 472, 727, 3811, 456, 885, 512, 1333, 295, 5496, 11, 50788], "temperature": 0.0, "avg_logprob": -0.14845856523091813, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.06747491657733917}, {"id": 212, "seek": 135644, "start": 1364.92, "end": 1369.4, "text": " like a crow, like you said, like very intelligent creature, doesn't have language, but maybe it's", "tokens": [50788, 411, 257, 6401, 11, 411, 291, 848, 11, 411, 588, 13232, 12797, 11, 1177, 380, 362, 2856, 11, 457, 1310, 309, 311, 51012], "temperature": 0.0, "avg_logprob": -0.14845856523091813, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.06747491657733917}, {"id": 213, "seek": 135644, "start": 1369.4, "end": 1374.2, "text": " got a really good multiple demand network. And perhaps we could, perhaps could be a really good", "tokens": [51012, 658, 257, 534, 665, 3866, 4733, 3209, 13, 400, 4317, 321, 727, 11, 4317, 727, 312, 257, 534, 665, 51252], "temperature": 0.0, "avg_logprob": -0.14845856523091813, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.06747491657733917}, {"id": 214, "seek": 135644, "start": 1374.2, "end": 1379.24, "text": " programmer, because it's not that part of the brain, which is being recruited. But it's rather", "tokens": [51252, 32116, 11, 570, 309, 311, 406, 300, 644, 295, 264, 3567, 11, 597, 307, 885, 33004, 13, 583, 309, 311, 2831, 51504], "temperature": 0.0, "avg_logprob": -0.14845856523091813, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.06747491657733917}, {"id": 215, "seek": 135644, "start": 1379.24, "end": 1385.16, "text": " this kind of almost clearinghouse from what I understand, the multiple demand network just", "tokens": [51504, 341, 733, 295, 1920, 23937, 6410, 490, 437, 286, 1223, 11, 264, 3866, 4733, 3209, 445, 51800], "temperature": 0.0, "avg_logprob": -0.14845856523091813, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.06747491657733917}, {"id": 216, "seek": 138516, "start": 1385.16, "end": 1394.28, "text": " picks up so many different jobs. The other thing that really stood out for me in this paper,", "tokens": [50364, 16137, 493, 370, 867, 819, 4782, 13, 440, 661, 551, 300, 534, 9371, 484, 337, 385, 294, 341, 3035, 11, 50820], "temperature": 0.0, "avg_logprob": -0.09817609151204428, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.0012813603971153498}, {"id": 217, "seek": 138516, "start": 1394.28, "end": 1400.28, "text": " which I really enjoyed, was that as a kind of, I guess, control, you could, you presented people", "tokens": [50820, 597, 286, 534, 4626, 11, 390, 300, 382, 257, 733, 295, 11, 286, 2041, 11, 1969, 11, 291, 727, 11, 291, 8212, 561, 51120], "temperature": 0.0, "avg_logprob": -0.09817609151204428, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.0012813603971153498}, {"id": 218, "seek": 138516, "start": 1400.28, "end": 1409.64, "text": " the same problems. So the, if I remember rightly, one of the, one of the code pieces of code that", "tokens": [51120, 264, 912, 2740, 13, 407, 264, 11, 498, 286, 1604, 32879, 11, 472, 295, 264, 11, 472, 295, 264, 3089, 3755, 295, 3089, 300, 51588], "temperature": 0.0, "avg_logprob": -0.09817609151204428, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.0012813603971153498}, {"id": 219, "seek": 140964, "start": 1409.64, "end": 1415.24, "text": " people had to interpret in Python was a calculation of BMI. And so it's like, here is a variable,", "tokens": [50364, 561, 632, 281, 7302, 294, 15329, 390, 257, 17108, 295, 363, 13808, 13, 400, 370, 309, 311, 411, 11, 510, 307, 257, 7006, 11, 50644], "temperature": 0.0, "avg_logprob": -0.13602725664774576, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.02435975894331932}, {"id": 220, "seek": 140964, "start": 1415.24, "end": 1420.68, "text": " which is your weight, here is one issue is your height, BMI equals height divided by weight squared.", "tokens": [50644, 597, 307, 428, 3364, 11, 510, 307, 472, 2734, 307, 428, 6681, 11, 363, 13808, 6915, 6681, 6666, 538, 3364, 8889, 13, 50916], "temperature": 0.0, "avg_logprob": -0.13602725664774576, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.02435975894331932}, {"id": 221, "seek": 140964, "start": 1422.2800000000002, "end": 1426.6000000000001, "text": " And so the person kind of reads through that. And, and you see it being passed off to the", "tokens": [50996, 400, 370, 264, 954, 733, 295, 15700, 807, 300, 13, 400, 11, 293, 291, 536, 309, 885, 4678, 766, 281, 264, 51212], "temperature": 0.0, "avg_logprob": -0.13602725664774576, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.02435975894331932}, {"id": 222, "seek": 140964, "start": 1426.6000000000001, "end": 1434.1200000000001, "text": " multiple demand network. But then there's the same problem defined entirely verbally. So instead", "tokens": [51212, 3866, 4733, 3209, 13, 583, 550, 456, 311, 264, 912, 1154, 7642, 7696, 48162, 13, 407, 2602, 51588], "temperature": 0.0, "avg_logprob": -0.13602725664774576, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.02435975894331932}, {"id": 223, "seek": 140964, "start": 1434.1200000000001, "end": 1438.68, "text": " of using, you know, symbols with equals, and it clearly being Python code, it's just, I know,", "tokens": [51588, 295, 1228, 11, 291, 458, 11, 16944, 365, 6915, 11, 293, 309, 4448, 885, 15329, 3089, 11, 309, 311, 445, 11, 286, 458, 11, 51816], "temperature": 0.0, "avg_logprob": -0.13602725664774576, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.02435975894331932}, {"id": 224, "seek": 143868, "start": 1438.68, "end": 1443.8, "text": " this is what BMI is, here, you know, here's how much you weigh, here's how tall you are, what's", "tokens": [50364, 341, 307, 437, 363, 13808, 307, 11, 510, 11, 291, 458, 11, 510, 311, 577, 709, 291, 13843, 11, 510, 311, 577, 6764, 291, 366, 11, 437, 311, 50620], "temperature": 0.0, "avg_logprob": -0.10876940789623796, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.0004617357044480741}, {"id": 225, "seek": 143868, "start": 1443.8, "end": 1451.5600000000002, "text": " your, what's your BMI. And that went to a different region of the brain, which for me was just like,", "tokens": [50620, 428, 11, 437, 311, 428, 363, 13808, 13, 400, 300, 1437, 281, 257, 819, 4458, 295, 264, 3567, 11, 597, 337, 385, 390, 445, 411, 11, 51008], "temperature": 0.0, "avg_logprob": -0.10876940789623796, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.0004617357044480741}, {"id": 226, "seek": 143868, "start": 1451.5600000000002, "end": 1457.8, "text": " okay, well, this is the same problem, but the way that it's presented really changes the way that", "tokens": [51008, 1392, 11, 731, 11, 341, 307, 264, 912, 1154, 11, 457, 264, 636, 300, 309, 311, 8212, 534, 2962, 264, 636, 300, 51320], "temperature": 0.0, "avg_logprob": -0.10876940789623796, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.0004617357044480741}, {"id": 227, "seek": 143868, "start": 1457.8, "end": 1465.0800000000002, "text": " we think about it. Which, yeah, that was another huge surprise for me to think just how influential", "tokens": [51320, 321, 519, 466, 309, 13, 3013, 11, 1338, 11, 300, 390, 1071, 2603, 6365, 337, 385, 281, 519, 445, 577, 22215, 51684], "temperature": 0.0, "avg_logprob": -0.10876940789623796, "compression_ratio": 1.720524017467249, "no_speech_prob": 0.0004617357044480741}, {"id": 228, "seek": 146508, "start": 1465.08, "end": 1473.56, "text": " the kind of presentation or the, the medium, I guess, for a set of concepts, how much that,", "tokens": [50364, 264, 733, 295, 5860, 420, 264, 11, 264, 6399, 11, 286, 2041, 11, 337, 257, 992, 295, 10392, 11, 577, 709, 300, 11, 50788], "temperature": 0.0, "avg_logprob": -0.09835942969264755, "compression_ratio": 1.5681818181818181, "no_speech_prob": 0.000984532292932272}, {"id": 229, "seek": 146508, "start": 1473.56, "end": 1477.32, "text": " that determines how those concepts are handled internally, mentally.", "tokens": [50788, 300, 24799, 577, 729, 10392, 366, 18033, 19501, 11, 17072, 13, 50976], "temperature": 0.0, "avg_logprob": -0.09835942969264755, "compression_ratio": 1.5681818181818181, "no_speech_prob": 0.000984532292932272}, {"id": 230, "seek": 146508, "start": 1479.0, "end": 1486.28, "text": " Yeah. And in fact, that's not that uncommon of a situation if you think about it. So let's say", "tokens": [51060, 865, 13, 400, 294, 1186, 11, 300, 311, 406, 300, 29289, 295, 257, 2590, 498, 291, 519, 466, 309, 13, 407, 718, 311, 584, 51424], "temperature": 0.0, "avg_logprob": -0.09835942969264755, "compression_ratio": 1.5681818181818181, "no_speech_prob": 0.000984532292932272}, {"id": 231, "seek": 146508, "start": 1486.28, "end": 1492.84, "text": " somebody is listening to this podcast, versus reading the transcript, the way information", "tokens": [51424, 2618, 307, 4764, 281, 341, 7367, 11, 5717, 3760, 264, 24444, 11, 264, 636, 1589, 51752], "temperature": 0.0, "avg_logprob": -0.09835942969264755, "compression_ratio": 1.5681818181818181, "no_speech_prob": 0.000984532292932272}, {"id": 232, "seek": 149284, "start": 1492.84, "end": 1499.0, "text": " gets into the brain is different. So in the auditory modality, it goes to the auditory cortex", "tokens": [50364, 2170, 666, 264, 3567, 307, 819, 13, 407, 294, 264, 17748, 827, 1072, 1860, 11, 309, 1709, 281, 264, 17748, 827, 33312, 50672], "temperature": 0.0, "avg_logprob": -0.13164827435515647, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.006384577136486769}, {"id": 233, "seek": 149284, "start": 1499.0, "end": 1505.24, "text": " first. And in the visual modality, it goes through the visual cortex first, and then it gets into,", "tokens": [50672, 700, 13, 400, 294, 264, 5056, 1072, 1860, 11, 309, 1709, 807, 264, 5056, 33312, 700, 11, 293, 550, 309, 2170, 666, 11, 50984], "temperature": 0.0, "avg_logprob": -0.13164827435515647, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.006384577136486769}, {"id": 234, "seek": 149284, "start": 1505.24, "end": 1510.1999999999998, "text": " we have a specialized part of the brain that's responsible for readings of recognizing written", "tokens": [50984, 321, 362, 257, 19813, 644, 295, 264, 3567, 300, 311, 6250, 337, 27319, 295, 18538, 3720, 51232], "temperature": 0.0, "avg_logprob": -0.13164827435515647, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.006384577136486769}, {"id": 235, "seek": 149284, "start": 1510.1999999999998, "end": 1516.4399999999998, "text": " letters. But then they will converge in the language network, because language network", "tokens": [51232, 7825, 13, 583, 550, 436, 486, 41881, 294, 264, 2856, 3209, 11, 570, 2856, 3209, 51544], "temperature": 0.0, "avg_logprob": -0.13164827435515647, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.006384577136486769}, {"id": 236, "seek": 151644, "start": 1517.16, "end": 1524.28, "text": " is responsible for processing either a modality. And that means that these initially distinct", "tokens": [50400, 307, 6250, 337, 9007, 2139, 257, 1072, 1860, 13, 400, 300, 1355, 300, 613, 9105, 10644, 50756], "temperature": 0.0, "avg_logprob": -0.10231776464553106, "compression_ratio": 1.5875706214689265, "no_speech_prob": 0.005729484837502241}, {"id": 237, "seek": 151644, "start": 1524.28, "end": 1532.68, "text": " representations have to converge in some, in some way. And so for some of the other cases,", "tokens": [50756, 33358, 362, 281, 41881, 294, 512, 11, 294, 512, 636, 13, 400, 370, 337, 512, 295, 264, 661, 3331, 11, 51176], "temperature": 0.0, "avg_logprob": -0.10231776464553106, "compression_ratio": 1.5875706214689265, "no_speech_prob": 0.005729484837502241}, {"id": 238, "seek": 151644, "start": 1532.68, "end": 1541.3200000000002, "text": " like a problem that's written in language versus in code, it looks like that convergence is also", "tokens": [51176, 411, 257, 1154, 300, 311, 3720, 294, 2856, 5717, 294, 3089, 11, 309, 1542, 411, 300, 32181, 307, 611, 51608], "temperature": 0.0, "avg_logprob": -0.10231776464553106, "compression_ratio": 1.5875706214689265, "no_speech_prob": 0.005729484837502241}, {"id": 239, "seek": 154132, "start": 1541.3999999999999, "end": 1546.9199999999998, "text": " happening, but it's happening later on in the processing, right? So it goes, you know, through", "tokens": [50368, 2737, 11, 457, 309, 311, 2737, 1780, 322, 294, 264, 9007, 11, 558, 30, 407, 309, 1709, 11, 291, 458, 11, 807, 50644], "temperature": 0.0, "avg_logprob": -0.12885524515520064, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.017164036631584167}, {"id": 240, "seek": 154132, "start": 1546.9199999999998, "end": 1553.0, "text": " the language network, and I guess the multiple demand. And then you have some shared problem", "tokens": [50644, 264, 2856, 3209, 11, 293, 286, 2041, 264, 3866, 4733, 13, 400, 550, 291, 362, 512, 5507, 1154, 50948], "temperature": 0.0, "avg_logprob": -0.12885524515520064, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.017164036631584167}, {"id": 241, "seek": 154132, "start": 1553.0, "end": 1560.04, "text": " solving. So in this case, calculating the BMI is doing some math. And so that we think also happens", "tokens": [50948, 12606, 13, 407, 294, 341, 1389, 11, 28258, 264, 363, 13808, 307, 884, 512, 5221, 13, 400, 370, 300, 321, 519, 611, 2314, 51300], "temperature": 0.0, "avg_logprob": -0.12885524515520064, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.017164036631584167}, {"id": 242, "seek": 154132, "start": 1560.04, "end": 1563.96, "text": " in the multiple demand network. And in fact, we show in the paper that you can kind of break down", "tokens": [51300, 294, 264, 3866, 4733, 3209, 13, 400, 294, 1186, 11, 321, 855, 294, 264, 3035, 300, 291, 393, 733, 295, 1821, 760, 51496], "temperature": 0.0, "avg_logprob": -0.12885524515520064, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.017164036631584167}, {"id": 243, "seek": 154132, "start": 1563.96, "end": 1568.36, "text": " that activity that we capture into the code reading part, an actual problem solving part.", "tokens": [51496, 300, 5191, 300, 321, 7983, 666, 264, 3089, 3760, 644, 11, 364, 3539, 1154, 12606, 644, 13, 51716], "temperature": 0.0, "avg_logprob": -0.12885524515520064, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.017164036631584167}, {"id": 244, "seek": 156836, "start": 1568.84, "end": 1576.6799999999998, "text": " But it's a fascinating endeavor. In general, in cognitive neuroscience, how do we design", "tokens": [50388, 583, 309, 311, 257, 10343, 34975, 13, 682, 2674, 11, 294, 15605, 42762, 11, 577, 360, 321, 1715, 50780], "temperature": 0.0, "avg_logprob": -0.1423896634301474, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.001081316266208887}, {"id": 245, "seek": 156836, "start": 1576.6799999999998, "end": 1582.84, "text": " an experiment where we have those kinds of different conditions, where they're very similar,", "tokens": [50780, 364, 5120, 689, 321, 362, 729, 3685, 295, 819, 4487, 11, 689, 436, 434, 588, 2531, 11, 51088], "temperature": 0.0, "avg_logprob": -0.1423896634301474, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.001081316266208887}, {"id": 246, "seek": 156836, "start": 1582.84, "end": 1589.9599999999998, "text": " except for something that we've changed. And so at what point that difference, right, auditor versus", "tokens": [51088, 3993, 337, 746, 300, 321, 600, 3105, 13, 400, 370, 412, 437, 935, 300, 2649, 11, 558, 11, 33970, 5717, 51444], "temperature": 0.0, "avg_logprob": -0.1423896634301474, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.001081316266208887}, {"id": 247, "seek": 156836, "start": 1589.9599999999998, "end": 1596.36, "text": " visual language versus code, where in the brain does that make a difference? And where doesn't it?", "tokens": [51444, 5056, 2856, 5717, 3089, 11, 689, 294, 264, 3567, 775, 300, 652, 257, 2649, 30, 400, 689, 1177, 380, 309, 30, 51764], "temperature": 0.0, "avg_logprob": -0.1423896634301474, "compression_ratio": 1.6075949367088607, "no_speech_prob": 0.001081316266208887}, {"id": 248, "seek": 159636, "start": 1597.32, "end": 1604.1999999999998, "text": " Yeah, yeah. So you're saying that even though it goes the language area lights up when we have", "tokens": [50412, 865, 11, 1338, 13, 407, 291, 434, 1566, 300, 754, 1673, 309, 1709, 264, 2856, 1859, 5811, 493, 562, 321, 362, 50756], "temperature": 0.0, "avg_logprob": -0.175669864732392, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.0016471055569127202}, {"id": 249, "seek": 159636, "start": 1604.1999999999998, "end": 1609.1599999999999, "text": " that kind of BMI problem, it's just kind of passing the thing. And then it gets passed off to", "tokens": [50756, 300, 733, 295, 363, 13808, 1154, 11, 309, 311, 445, 733, 295, 8437, 264, 551, 13, 400, 550, 309, 2170, 4678, 766, 281, 51004], "temperature": 0.0, "avg_logprob": -0.175669864732392, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.0016471055569127202}, {"id": 250, "seek": 159636, "start": 1610.6, "end": 1615.0, "text": " you know, to actually run the calculation that happens, like that doesn't happen in the language", "tokens": [51076, 291, 458, 11, 281, 767, 1190, 264, 17108, 300, 2314, 11, 411, 300, 1177, 380, 1051, 294, 264, 2856, 51296], "temperature": 0.0, "avg_logprob": -0.175669864732392, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.0016471055569127202}, {"id": 251, "seek": 159636, "start": 1615.9599999999998, "end": 1622.6799999999998, "text": " area. Yeah, that makes sense. Yeah, okay. That clarifies my, I was very excited. I thought that", "tokens": [51344, 1859, 13, 865, 11, 300, 1669, 2020, 13, 865, 11, 1392, 13, 663, 6093, 11221, 452, 11, 286, 390, 588, 2919, 13, 286, 1194, 300, 51680], "temperature": 0.0, "avg_logprob": -0.175669864732392, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.0016471055569127202}, {"id": 252, "seek": 162268, "start": 1622.76, "end": 1627.0800000000002, "text": " maybe that we had some sort of like way of doing the confrontation, just linguistically.", "tokens": [50368, 1310, 300, 321, 632, 512, 1333, 295, 411, 636, 295, 884, 264, 35363, 11, 445, 21766, 20458, 13, 50584], "temperature": 0.0, "avg_logprob": -0.16363635631876255, "compression_ratio": 1.68359375, "no_speech_prob": 0.005299240816384554}, {"id": 253, "seek": 162268, "start": 1629.0, "end": 1634.3600000000001, "text": " Guess that's what that doesn't work. I think it's possible. And we don't well,", "tokens": [50680, 17795, 300, 311, 437, 300, 1177, 380, 589, 13, 286, 519, 309, 311, 1944, 13, 400, 321, 500, 380, 731, 11, 50948], "temperature": 0.0, "avg_logprob": -0.16363635631876255, "compression_ratio": 1.68359375, "no_speech_prob": 0.005299240816384554}, {"id": 254, "seek": 162268, "start": 1635.64, "end": 1638.92, "text": " I don't know. Well, maybe not linguistically. But like, you know,", "tokens": [51012, 286, 500, 380, 458, 13, 1042, 11, 1310, 406, 21766, 20458, 13, 583, 411, 11, 291, 458, 11, 51176], "temperature": 0.0, "avg_logprob": -0.16363635631876255, "compression_ratio": 1.68359375, "no_speech_prob": 0.005299240816384554}, {"id": 255, "seek": 162268, "start": 1639.8, "end": 1646.28, "text": " we memorize the multiplication table, or for like some problem that we do very often, we don't need", "tokens": [51220, 321, 27478, 264, 27290, 3199, 11, 420, 337, 411, 512, 1154, 300, 321, 360, 588, 2049, 11, 321, 500, 380, 643, 51544], "temperature": 0.0, "avg_logprob": -0.16363635631876255, "compression_ratio": 1.68359375, "no_speech_prob": 0.005299240816384554}, {"id": 256, "seek": 162268, "start": 1646.28, "end": 1651.24, "text": " to actually go through the steps of the calculation, we kind of just retrieve the correct answer.", "tokens": [51544, 281, 767, 352, 807, 264, 4439, 295, 264, 17108, 11, 321, 733, 295, 445, 30254, 264, 3006, 1867, 13, 51792], "temperature": 0.0, "avg_logprob": -0.16363635631876255, "compression_ratio": 1.68359375, "no_speech_prob": 0.005299240816384554}, {"id": 257, "seek": 165124, "start": 1652.04, "end": 1657.16, "text": " I don't know if it happens linguistically or not, potentially not, probably not. But it's", "tokens": [50404, 286, 500, 380, 458, 498, 309, 2314, 21766, 20458, 420, 406, 11, 7263, 406, 11, 1391, 406, 13, 583, 309, 311, 50660], "temperature": 0.0, "avg_logprob": -0.1155184879899025, "compression_ratio": 1.7242524916943522, "no_speech_prob": 0.00023772283748257905}, {"id": 258, "seek": 165124, "start": 1657.16, "end": 1661.4, "text": " still a different mechanism than actually going step by step and doing, you know,", "tokens": [50660, 920, 257, 819, 7513, 813, 767, 516, 1823, 538, 1823, 293, 884, 11, 291, 458, 11, 50872], "temperature": 0.0, "avg_logprob": -0.1155184879899025, "compression_ratio": 1.7242524916943522, "no_speech_prob": 0.00023772283748257905}, {"id": 259, "seek": 165124, "start": 1661.4, "end": 1665.56, "text": " long division in your head, or like summing multidigit numbers or something like that.", "tokens": [50872, 938, 10044, 294, 428, 1378, 11, 420, 411, 2408, 2810, 2120, 327, 328, 270, 3547, 420, 746, 411, 300, 13, 51080], "temperature": 0.0, "avg_logprob": -0.1155184879899025, "compression_ratio": 1.7242524916943522, "no_speech_prob": 0.00023772283748257905}, {"id": 260, "seek": 165124, "start": 1666.28, "end": 1670.92, "text": " Yeah. Yeah, I think that that's a really interesting question, which we can maybe come back to.", "tokens": [51116, 865, 13, 865, 11, 286, 519, 300, 300, 311, 257, 534, 1880, 1168, 11, 597, 321, 393, 1310, 808, 646, 281, 13, 51348], "temperature": 0.0, "avg_logprob": -0.1155184879899025, "compression_ratio": 1.7242524916943522, "no_speech_prob": 0.00023772283748257905}, {"id": 261, "seek": 165124, "start": 1670.92, "end": 1675.32, "text": " But I guess, yeah, so you kind of see both the multiple demand and the language", "tokens": [51348, 583, 286, 2041, 11, 1338, 11, 370, 291, 733, 295, 536, 1293, 264, 3866, 4733, 293, 264, 2856, 51568], "temperature": 0.0, "avg_logprob": -0.1155184879899025, "compression_ratio": 1.7242524916943522, "no_speech_prob": 0.00023772283748257905}, {"id": 262, "seek": 165124, "start": 1675.32, "end": 1679.72, "text": " network lighting up when this problem is presented linguistically. So it's sort of a", "tokens": [51568, 3209, 9577, 493, 562, 341, 1154, 307, 8212, 21766, 20458, 13, 407, 309, 311, 1333, 295, 257, 51788], "temperature": 0.0, "avg_logprob": -0.1155184879899025, "compression_ratio": 1.7242524916943522, "no_speech_prob": 0.00023772283748257905}, {"id": 263, "seek": 167972, "start": 1679.72, "end": 1685.4, "text": " fair assumption that what is in fact happening is that, you know, there's probably some linguistic", "tokens": [50364, 3143, 15302, 300, 437, 307, 294, 1186, 2737, 307, 300, 11, 291, 458, 11, 456, 311, 1391, 512, 43002, 50648], "temperature": 0.0, "avg_logprob": -0.12046484417385525, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.0002163104945793748}, {"id": 264, "seek": 167972, "start": 1685.4, "end": 1692.2, "text": " processing, but then it then gets passed to the same sort of area of the brain, which is,", "tokens": [50648, 9007, 11, 457, 550, 309, 550, 2170, 4678, 281, 264, 912, 1333, 295, 1859, 295, 264, 3567, 11, 597, 307, 11, 50988], "temperature": 0.0, "avg_logprob": -0.12046484417385525, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.0002163104945793748}, {"id": 265, "seek": 167972, "start": 1692.2, "end": 1697.8, "text": " which handles the pure Python problem. But of course, yeah, I mean, that is really interesting", "tokens": [50988, 597, 18722, 264, 6075, 15329, 1154, 13, 583, 295, 1164, 11, 1338, 11, 286, 914, 11, 300, 307, 534, 1880, 51268], "temperature": 0.0, "avg_logprob": -0.12046484417385525, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.0002163104945793748}, {"id": 266, "seek": 167972, "start": 1698.6000000000001, "end": 1704.2, "text": " and kind of useful in some ways, in that, you know, it seems more efficient to be", "tokens": [51308, 293, 733, 295, 4420, 294, 512, 2098, 11, 294, 300, 11, 291, 458, 11, 309, 2544, 544, 7148, 281, 312, 51588], "temperature": 0.0, "avg_logprob": -0.12046484417385525, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.0002163104945793748}, {"id": 267, "seek": 170420, "start": 1704.2, "end": 1710.1200000000001, "text": " presented just with the Python code, right? You kind of bypass that. Oh, I turn this into,", "tokens": [50364, 8212, 445, 365, 264, 15329, 3089, 11, 558, 30, 509, 733, 295, 24996, 300, 13, 876, 11, 286, 1261, 341, 666, 11, 50660], "temperature": 0.0, "avg_logprob": -0.1315594662676801, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.08605916798114777}, {"id": 268, "seek": 170420, "start": 1711.56, "end": 1717.88, "text": " you know, it goes straight into the, the system, which can perform the ultimate calculation, I", "tokens": [50732, 291, 458, 11, 309, 1709, 2997, 666, 264, 11, 264, 1185, 11, 597, 393, 2042, 264, 9705, 17108, 11, 286, 51048], "temperature": 0.0, "avg_logprob": -0.1315594662676801, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.08605916798114777}, {"id": 269, "seek": 170420, "start": 1717.88, "end": 1723.56, "text": " guess. I don't know if you, if you were able to capture any information on whether it was quicker", "tokens": [51048, 2041, 13, 286, 500, 380, 458, 498, 291, 11, 498, 291, 645, 1075, 281, 7983, 604, 1589, 322, 1968, 309, 390, 16255, 51332], "temperature": 0.0, "avg_logprob": -0.1315594662676801, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.08605916798114777}, {"id": 270, "seek": 170420, "start": 1723.56, "end": 1729.88, "text": " for people to kind of solve the problem when presented with the Python code or not.", "tokens": [51332, 337, 561, 281, 733, 295, 5039, 264, 1154, 562, 8212, 365, 264, 15329, 3089, 420, 406, 13, 51648], "temperature": 0.0, "avg_logprob": -0.1315594662676801, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.08605916798114777}, {"id": 271, "seek": 172988, "start": 1730.68, "end": 1742.0400000000002, "text": " I don't remember whether we saw a difference in how long it took people.", "tokens": [50404, 286, 500, 380, 1604, 1968, 321, 1866, 257, 2649, 294, 577, 938, 309, 1890, 561, 13, 50972], "temperature": 0.0, "avg_logprob": -0.20288816598745493, "compression_ratio": 1.5662650602409638, "no_speech_prob": 0.0014997638063505292}, {"id": 272, "seek": 172988, "start": 1742.7600000000002, "end": 1748.3600000000001, "text": " I think it's possible, but I don't, some of it, of course, depends on how proficient they are in", "tokens": [51008, 286, 519, 309, 311, 1944, 11, 457, 286, 500, 380, 11, 512, 295, 309, 11, 295, 1164, 11, 5946, 322, 577, 1740, 24549, 436, 366, 294, 51288], "temperature": 0.0, "avg_logprob": -0.20288816598745493, "compression_ratio": 1.5662650602409638, "no_speech_prob": 0.0014997638063505292}, {"id": 273, "seek": 172988, "start": 1748.3600000000001, "end": 1755.0800000000002, "text": " Python. So there might be individual differences there, also individual differences in how", "tokens": [51288, 15329, 13, 407, 456, 1062, 312, 2609, 7300, 456, 11, 611, 2609, 7300, 294, 577, 51624], "temperature": 0.0, "avg_logprob": -0.20288816598745493, "compression_ratio": 1.5662650602409638, "no_speech_prob": 0.0014997638063505292}, {"id": 274, "seek": 175508, "start": 1755.08, "end": 1764.4399999999998, "text": " fast they would read text. So I'm sure there's some variability there. But it's actually", "tokens": [50364, 2370, 436, 576, 1401, 2487, 13, 407, 286, 478, 988, 456, 311, 512, 35709, 456, 13, 583, 309, 311, 767, 50832], "temperature": 0.0, "avg_logprob": -0.15563101517526726, "compression_ratio": 1.4944444444444445, "no_speech_prob": 0.0005975090316496789}, {"id": 275, "seek": 175508, "start": 1765.6399999999999, "end": 1771.72, "text": " an interesting thought that you bring enough so that this, having this abstract skeleton with", "tokens": [50892, 364, 1880, 1194, 300, 291, 1565, 1547, 370, 300, 341, 11, 1419, 341, 12649, 25204, 365, 51196], "temperature": 0.0, "avg_logprob": -0.15563101517526726, "compression_ratio": 1.4944444444444445, "no_speech_prob": 0.0005975090316496789}, {"id": 276, "seek": 175508, "start": 1772.36, "end": 1778.84, "text": " other information stripped away might make the problem solving the calculation easier.", "tokens": [51228, 661, 1589, 33221, 1314, 1062, 652, 264, 1154, 12606, 264, 17108, 3571, 13, 51552], "temperature": 0.0, "avg_logprob": -0.15563101517526726, "compression_ratio": 1.4944444444444445, "no_speech_prob": 0.0005975090316496789}, {"id": 277, "seek": 177884, "start": 1778.84, "end": 1786.52, "text": " Because in fact, there are cases where researchers have observed the reverse,", "tokens": [50364, 1436, 294, 1186, 11, 456, 366, 3331, 689, 10309, 362, 13095, 264, 9943, 11, 50748], "temperature": 0.0, "avg_logprob": -0.213387131690979, "compression_ratio": 1.3166666666666667, "no_speech_prob": 0.0014315048465505242}, {"id": 278, "seek": 177884, "start": 1787.6399999999999, "end": 1797.1599999999999, "text": " though there is this famous Waste and Selection task, which you have, let's see,", "tokens": [50804, 1673, 456, 307, 341, 4618, 343, 9079, 293, 1100, 5450, 5633, 11, 597, 291, 362, 11, 718, 311, 536, 11, 51280], "temperature": 0.0, "avg_logprob": -0.213387131690979, "compression_ratio": 1.3166666666666667, "no_speech_prob": 0.0014315048465505242}, {"id": 279, "seek": 179716, "start": 1797.5600000000002, "end": 1807.0800000000002, "text": " a card with like a two and a seven, and then a, and then a card with like", "tokens": [50384, 257, 2920, 365, 411, 257, 732, 293, 257, 3407, 11, 293, 550, 257, 11, 293, 550, 257, 2920, 365, 411, 50860], "temperature": 0.0, "avg_logprob": -0.22868767190486827, "compression_ratio": 1.4952380952380953, "no_speech_prob": 0.11262215673923492}, {"id": 280, "seek": 179716, "start": 1807.96, "end": 1821.0, "text": " green, red and blue. And you need to test the rule that says if the number is even,", "tokens": [50904, 3092, 11, 2182, 293, 3344, 13, 400, 291, 643, 281, 1500, 264, 4978, 300, 1619, 498, 264, 1230, 307, 754, 11, 51556], "temperature": 0.0, "avg_logprob": -0.22868767190486827, "compression_ratio": 1.4952380952380953, "no_speech_prob": 0.11262215673923492}, {"id": 281, "seek": 182100, "start": 1821.08, "end": 1827.56, "text": " then the other side of the card has to be blue. And so then the question is which cards do you", "tokens": [50368, 550, 264, 661, 1252, 295, 264, 2920, 575, 281, 312, 3344, 13, 400, 370, 550, 264, 1168, 307, 597, 5632, 360, 291, 50692], "temperature": 0.0, "avg_logprob": -0.11074789774786566, "compression_ratio": 1.9738219895287958, "no_speech_prob": 0.007928101345896721}, {"id": 282, "seek": 182100, "start": 1827.56, "end": 1833.24, "text": " need to turn over to make sure that that rule is correct. And so then people want to test", "tokens": [50692, 643, 281, 1261, 670, 281, 652, 988, 300, 300, 4978, 307, 3006, 13, 400, 370, 550, 561, 528, 281, 1500, 50976], "temperature": 0.0, "avg_logprob": -0.11074789774786566, "compression_ratio": 1.9738219895287958, "no_speech_prob": 0.007928101345896721}, {"id": 283, "seek": 182100, "start": 1835.48, "end": 1840.04, "text": " the card with the two on it because it's even, and so they want to make sure that their reverse", "tokens": [51088, 264, 2920, 365, 264, 732, 322, 309, 570, 309, 311, 754, 11, 293, 370, 436, 528, 281, 652, 988, 300, 641, 9943, 51316], "temperature": 0.0, "avg_logprob": -0.11074789774786566, "compression_ratio": 1.9738219895287958, "no_speech_prob": 0.007928101345896721}, {"id": 284, "seek": 182100, "start": 1840.04, "end": 1845.24, "text": " is blue. But then they often want to turn over the blue to make sure that the other side is odd,", "tokens": [51316, 307, 3344, 13, 583, 550, 436, 2049, 528, 281, 1261, 670, 264, 3344, 281, 652, 988, 300, 264, 661, 1252, 307, 7401, 11, 51576], "temperature": 0.0, "avg_logprob": -0.11074789774786566, "compression_ratio": 1.9738219895287958, "no_speech_prob": 0.007928101345896721}, {"id": 285, "seek": 184524, "start": 1846.04, "end": 1851.72, "text": " sorry, is even. But that actually is not what you should do because it doesn't matter. Like if you", "tokens": [50404, 2597, 11, 307, 754, 13, 583, 300, 767, 307, 406, 437, 291, 820, 360, 570, 309, 1177, 380, 1871, 13, 1743, 498, 291, 50688], "temperature": 0.0, "avg_logprob": -0.12601244703252265, "compression_ratio": 1.7235023041474655, "no_speech_prob": 0.01081380806863308}, {"id": 286, "seek": 184524, "start": 1851.72, "end": 1856.36, "text": " have blue and odd, that's actually not a violation of the rule. What you need to do is you need to", "tokens": [50688, 362, 3344, 293, 7401, 11, 300, 311, 767, 406, 257, 22840, 295, 264, 4978, 13, 708, 291, 643, 281, 360, 307, 291, 643, 281, 50920], "temperature": 0.0, "avg_logprob": -0.12601244703252265, "compression_ratio": 1.7235023041474655, "no_speech_prob": 0.01081380806863308}, {"id": 287, "seek": 184524, "start": 1856.36, "end": 1861.56, "text": " turn over the red card because of an even number there, then that rule gets violated.", "tokens": [50920, 1261, 670, 264, 2182, 2920, 570, 295, 364, 754, 1230, 456, 11, 550, 300, 4978, 2170, 33239, 13, 51180], "temperature": 0.0, "avg_logprob": -0.12601244703252265, "compression_ratio": 1.7235023041474655, "no_speech_prob": 0.01081380806863308}, {"id": 288, "seek": 184524, "start": 1862.36, "end": 1870.76, "text": " So that problem is hard for people. But if you cast the same problem saying that there are", "tokens": [51220, 407, 300, 1154, 307, 1152, 337, 561, 13, 583, 498, 291, 4193, 264, 912, 1154, 1566, 300, 456, 366, 51640], "temperature": 0.0, "avg_logprob": -0.12601244703252265, "compression_ratio": 1.7235023041474655, "no_speech_prob": 0.01081380806863308}, {"id": 289, "seek": 187076, "start": 1871.64, "end": 1880.2, "text": " people at the bar and somebody, you know, is 16 and somebody is 25 and then somebody is drinking", "tokens": [50408, 561, 412, 264, 2159, 293, 2618, 11, 291, 458, 11, 307, 3165, 293, 2618, 307, 3552, 293, 550, 2618, 307, 7583, 50836], "temperature": 0.0, "avg_logprob": -0.12340734402338664, "compression_ratio": 1.9234693877551021, "no_speech_prob": 0.005907753482460976}, {"id": 290, "seek": 187076, "start": 1880.2, "end": 1887.08, "text": " beer and somebody is drinking a Coke, then, you know, how will you verify that only people over", "tokens": [50836, 8795, 293, 2618, 307, 7583, 257, 32996, 11, 550, 11, 291, 458, 11, 577, 486, 291, 16888, 300, 787, 561, 670, 51180], "temperature": 0.0, "avg_logprob": -0.12340734402338664, "compression_ratio": 1.9234693877551021, "no_speech_prob": 0.005907753482460976}, {"id": 291, "seek": 187076, "start": 1887.08, "end": 1892.12, "text": " the age of 18 are drinking alcohol? And then of course, you know, that you need to, you know,", "tokens": [51180, 264, 3205, 295, 2443, 366, 7583, 7658, 30, 400, 550, 295, 1164, 11, 291, 458, 11, 300, 291, 643, 281, 11, 291, 458, 11, 51432], "temperature": 0.0, "avg_logprob": -0.12340734402338664, "compression_ratio": 1.9234693877551021, "no_speech_prob": 0.005907753482460976}, {"id": 292, "seek": 187076, "start": 1892.12, "end": 1898.04, "text": " check the 16 year old and check the person drinking the beer and not any other way. And so", "tokens": [51432, 1520, 264, 3165, 1064, 1331, 293, 1520, 264, 954, 7583, 264, 8795, 293, 406, 604, 661, 636, 13, 400, 370, 51728], "temperature": 0.0, "avg_logprob": -0.12340734402338664, "compression_ratio": 1.9234693877551021, "no_speech_prob": 0.005907753482460976}, {"id": 293, "seek": 189804, "start": 1898.04, "end": 1904.28, "text": " mathematically, the same exact process, but it's much, much easier for people to ground the rule", "tokens": [50364, 44003, 11, 264, 912, 1900, 1399, 11, 457, 309, 311, 709, 11, 709, 3571, 337, 561, 281, 2727, 264, 4978, 50676], "temperature": 0.0, "avg_logprob": -0.11245631135028342, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.002631101291626692}, {"id": 294, "seek": 189804, "start": 1904.28, "end": 1909.08, "text": " and their existing knowledge, not necessarily the bar example, but they're just kind of, you know,", "tokens": [50676, 293, 641, 6741, 3601, 11, 406, 4725, 264, 2159, 1365, 11, 457, 436, 434, 445, 733, 295, 11, 291, 458, 11, 50916], "temperature": 0.0, "avg_logprob": -0.11245631135028342, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.002631101291626692}, {"id": 295, "seek": 189804, "start": 1909.08, "end": 1915.24, "text": " the easiest one and the most common one. And so this phenomenon is known as content effects on", "tokens": [50916, 264, 12889, 472, 293, 264, 881, 2689, 472, 13, 400, 370, 341, 14029, 307, 2570, 382, 2701, 5065, 322, 51224], "temperature": 0.0, "avg_logprob": -0.11245631135028342, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.002631101291626692}, {"id": 296, "seek": 189804, "start": 1915.24, "end": 1923.48, "text": " reasoning. And yeah, I think a lot of people, especially like, you know, physicists and mathematicians", "tokens": [51224, 21577, 13, 400, 1338, 11, 286, 519, 257, 688, 295, 561, 11, 2318, 411, 11, 291, 458, 11, 48716, 293, 32811, 2567, 51636], "temperature": 0.0, "avg_logprob": -0.11245631135028342, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.002631101291626692}, {"id": 297, "seek": 192348, "start": 1923.48, "end": 1928.04, "text": " and so people trained in like hardcore STEM discipline, they're like, hey, Alex, trip away,", "tokens": [50364, 293, 370, 561, 8895, 294, 411, 28196, 25043, 13635, 11, 436, 434, 411, 11, 4177, 11, 5202, 11, 4931, 1314, 11, 50592], "temperature": 0.0, "avg_logprob": -0.1779895339693342, "compression_ratio": 1.6866197183098592, "no_speech_prob": 0.018826447427272797}, {"id": 298, "seek": 192348, "start": 1928.04, "end": 1932.76, "text": " all of the extra information only focus on the abstract symbols. That's the easiest thing. But", "tokens": [50592, 439, 295, 264, 2857, 1589, 787, 1879, 322, 264, 12649, 16944, 13, 663, 311, 264, 12889, 551, 13, 583, 50828], "temperature": 0.0, "avg_logprob": -0.1779895339693342, "compression_ratio": 1.6866197183098592, "no_speech_prob": 0.018826447427272797}, {"id": 299, "seek": 192348, "start": 1932.76, "end": 1939.4, "text": " actually for a lot of people grounding the problem in some specific content domain tends to help.", "tokens": [50828, 767, 337, 257, 688, 295, 561, 46727, 264, 1154, 294, 512, 2685, 2701, 9274, 12258, 281, 854, 13, 51160], "temperature": 0.0, "avg_logprob": -0.1779895339693342, "compression_ratio": 1.6866197183098592, "no_speech_prob": 0.018826447427272797}, {"id": 300, "seek": 192348, "start": 1939.4, "end": 1945.0, "text": " And so I know that some people in like math education are very interested in this phenomenon and", "tokens": [51160, 400, 370, 286, 458, 300, 512, 561, 294, 411, 5221, 3309, 366, 588, 3102, 294, 341, 14029, 293, 51440], "temperature": 0.0, "avg_logprob": -0.1779895339693342, "compression_ratio": 1.6866197183098592, "no_speech_prob": 0.018826447427272797}, {"id": 301, "seek": 192348, "start": 1945.0, "end": 1953.0, "text": " how does it how to make it easier for people to, for kids to learn math. Is it by focusing on the", "tokens": [51440, 577, 775, 309, 577, 281, 652, 309, 3571, 337, 561, 281, 11, 337, 2301, 281, 1466, 5221, 13, 1119, 309, 538, 8416, 322, 264, 51840], "temperature": 0.0, "avg_logprob": -0.1779895339693342, "compression_ratio": 1.6866197183098592, "no_speech_prob": 0.018826447427272797}, {"id": 302, "seek": 195300, "start": 1953.0, "end": 1957.48, "text": " abstract or is it by grounding math problems in real life situations?", "tokens": [50364, 12649, 420, 307, 309, 538, 46727, 5221, 2740, 294, 957, 993, 6851, 30, 50588], "temperature": 0.0, "avg_logprob": -0.11598025526955863, "compression_ratio": 1.6954887218045114, "no_speech_prob": 0.00028663131524808705}, {"id": 303, "seek": 195300, "start": 1959.56, "end": 1965.72, "text": " And I suppose part of the reason why that grounding might work, well, there could be kind", "tokens": [50692, 400, 286, 7297, 644, 295, 264, 1778, 983, 300, 46727, 1062, 589, 11, 731, 11, 456, 727, 312, 733, 51000], "temperature": 0.0, "avg_logprob": -0.11598025526955863, "compression_ratio": 1.6954887218045114, "no_speech_prob": 0.00028663131524808705}, {"id": 304, "seek": 195300, "start": 1965.72, "end": 1971.08, "text": " of two hypotheses. One is just like, it locates it in a different area of the brain, which is somehow", "tokens": [51000, 295, 732, 49969, 13, 1485, 307, 445, 411, 11, 309, 1628, 1024, 309, 294, 257, 819, 1859, 295, 264, 3567, 11, 597, 307, 6063, 51268], "temperature": 0.0, "avg_logprob": -0.11598025526955863, "compression_ratio": 1.6954887218045114, "no_speech_prob": 0.00028663131524808705}, {"id": 305, "seek": 195300, "start": 1971.08, "end": 1976.68, "text": " better at processing this thing. So maybe that just the social reasoning part is just better at", "tokens": [51268, 1101, 412, 9007, 341, 551, 13, 407, 1310, 300, 445, 264, 2093, 21577, 644, 307, 445, 1101, 412, 51548], "temperature": 0.0, "avg_logprob": -0.11598025526955863, "compression_ratio": 1.6954887218045114, "no_speech_prob": 0.00028663131524808705}, {"id": 306, "seek": 195300, "start": 1976.68, "end": 1981.72, "text": " doing that kind of problem. But doesn't seem so likely in this case. And another is just that", "tokens": [51548, 884, 300, 733, 295, 1154, 13, 583, 1177, 380, 1643, 370, 3700, 294, 341, 1389, 13, 400, 1071, 307, 445, 300, 51800], "temperature": 0.0, "avg_logprob": -0.11598025526955863, "compression_ratio": 1.6954887218045114, "no_speech_prob": 0.00028663131524808705}, {"id": 307, "seek": 198172, "start": 1982.2, "end": 1989.0, "text": " it gets it, it clicks it in to a place where you're able to recognize a pattern that you've", "tokens": [50388, 309, 2170, 309, 11, 309, 18521, 309, 294, 281, 257, 1081, 689, 291, 434, 1075, 281, 5521, 257, 5102, 300, 291, 600, 50728], "temperature": 0.0, "avg_logprob": -0.07892870165638088, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.0031711184419691563}, {"id": 308, "seek": 198172, "start": 1989.0, "end": 1995.16, "text": " seen before. And so you don't have to do, you know, you're already on the right track.", "tokens": [50728, 1612, 949, 13, 400, 370, 291, 500, 380, 362, 281, 360, 11, 291, 458, 11, 291, 434, 1217, 322, 264, 558, 2837, 13, 51036], "temperature": 0.0, "avg_logprob": -0.07892870165638088, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.0031711184419691563}, {"id": 309, "seek": 198172, "start": 1996.92, "end": 2002.6000000000001, "text": " And this maybe kind of comes back to your point about, well, maybe when we calculate the BMI for", "tokens": [51124, 400, 341, 1310, 733, 295, 1487, 646, 281, 428, 935, 466, 11, 731, 11, 1310, 562, 321, 8873, 264, 363, 13808, 337, 51408], "temperature": 0.0, "avg_logprob": -0.07892870165638088, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.0031711184419691563}, {"id": 310, "seek": 198172, "start": 2002.6000000000001, "end": 2006.76, "text": " certain kind of combinations of numbers, you just know the answer. So it's being kind of recalled", "tokens": [51408, 1629, 733, 295, 21267, 295, 3547, 11, 291, 445, 458, 264, 1867, 13, 407, 309, 311, 885, 733, 295, 39301, 51616], "temperature": 0.0, "avg_logprob": -0.07892870165638088, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.0031711184419691563}, {"id": 311, "seek": 200676, "start": 2006.84, "end": 2012.28, "text": " from memory, like that pattern is already so established that you don't need to reason through", "tokens": [50368, 490, 4675, 11, 411, 300, 5102, 307, 1217, 370, 7545, 300, 291, 500, 380, 643, 281, 1778, 807, 50640], "temperature": 0.0, "avg_logprob": -0.08998135597475114, "compression_ratio": 1.5349794238683128, "no_speech_prob": 0.0035900603979825974}, {"id": 312, "seek": 200676, "start": 2012.28, "end": 2022.92, "text": " it in the same way. It's more of a recall operation. And I mean, this is getting us towards one of the", "tokens": [50640, 309, 294, 264, 912, 636, 13, 467, 311, 544, 295, 257, 9901, 6916, 13, 400, 286, 914, 11, 341, 307, 1242, 505, 3030, 472, 295, 264, 51172], "temperature": 0.0, "avg_logprob": -0.08998135597475114, "compression_ratio": 1.5349794238683128, "no_speech_prob": 0.0035900603979825974}, {"id": 313, "seek": 200676, "start": 2022.92, "end": 2027.72, "text": " kind of central questions, which is around, well, what are LLMs doing? Because they're kind of", "tokens": [51172, 733, 295, 5777, 1651, 11, 597, 307, 926, 11, 731, 11, 437, 366, 441, 43, 26386, 884, 30, 1436, 436, 434, 733, 295, 51412], "temperature": 0.0, "avg_logprob": -0.08998135597475114, "compression_ratio": 1.5349794238683128, "no_speech_prob": 0.0035900603979825974}, {"id": 314, "seek": 200676, "start": 2028.92, "end": 2033.72, "text": " glorified recall machines in a certain way, or just really good pattern matches.", "tokens": [51472, 26623, 2587, 9901, 8379, 294, 257, 1629, 636, 11, 420, 445, 534, 665, 5102, 10676, 13, 51712], "temperature": 0.0, "avg_logprob": -0.08998135597475114, "compression_ratio": 1.5349794238683128, "no_speech_prob": 0.0035900603979825974}, {"id": 315, "seek": 203372, "start": 2034.2, "end": 2041.96, "text": " Maybe before we get to that, though, I want to talk about another of your experiments, which I", "tokens": [50388, 2704, 949, 321, 483, 281, 300, 11, 1673, 11, 286, 528, 281, 751, 466, 1071, 295, 428, 12050, 11, 597, 286, 50776], "temperature": 0.0, "avg_logprob": -0.11381723547494539, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0008283515926450491}, {"id": 316, "seek": 203372, "start": 2041.96, "end": 2048.36, "text": " really enjoyed, which is about where people are looking at images of improbable and probable", "tokens": [50776, 534, 4626, 11, 597, 307, 466, 689, 561, 366, 1237, 412, 5267, 295, 2530, 65, 712, 293, 21759, 51096], "temperature": 0.0, "avg_logprob": -0.11381723547494539, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0008283515926450491}, {"id": 317, "seek": 203372, "start": 2048.36, "end": 2057.0, "text": " things like the shark and the swimmer that you mentioned. And what I found, well, maybe you", "tokens": [51096, 721, 411, 264, 13327, 293, 264, 7110, 936, 300, 291, 2835, 13, 400, 437, 286, 1352, 11, 731, 11, 1310, 291, 51528], "temperature": 0.0, "avg_logprob": -0.11381723547494539, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0008283515926450491}, {"id": 318, "seek": 203372, "start": 2057.0, "end": 2061.8, "text": " should describe the experiment, you'll do a much better job of it than me. Because I think, yeah,", "tokens": [51528, 820, 6786, 264, 5120, 11, 291, 603, 360, 257, 709, 1101, 1691, 295, 309, 813, 385, 13, 1436, 286, 519, 11, 1338, 11, 51768], "temperature": 0.0, "avg_logprob": -0.11381723547494539, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0008283515926450491}, {"id": 319, "seek": 206180, "start": 2061.8, "end": 2064.44, "text": " there was just a really interesting piece here that kind of writes this.", "tokens": [50364, 456, 390, 445, 257, 534, 1880, 2522, 510, 300, 733, 295, 13657, 341, 13, 50496], "temperature": 0.0, "avg_logprob": -0.16429956080549854, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.0009981108596548438}, {"id": 320, "seek": 206180, "start": 2066.2000000000003, "end": 2072.84, "text": " Yeah. So here, we use that same idea that the same information might arrive in the brain through", "tokens": [50584, 865, 13, 407, 510, 11, 321, 764, 300, 912, 1558, 300, 264, 912, 1589, 1062, 8881, 294, 264, 3567, 807, 50916], "temperature": 0.0, "avg_logprob": -0.16429956080549854, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.0009981108596548438}, {"id": 321, "seek": 206180, "start": 2072.84, "end": 2081.48, "text": " different routes. And so in this case, we were looking at sentences describing basic interactions", "tokens": [50916, 819, 18242, 13, 400, 370, 294, 341, 1389, 11, 321, 645, 1237, 412, 16579, 16141, 3875, 13280, 51348], "temperature": 0.0, "avg_logprob": -0.16429956080549854, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.0009981108596548438}, {"id": 322, "seek": 208148, "start": 2081.48, "end": 2091.4, "text": " between two entities, like the, I guess we can roll with the shark bites the swimmer,", "tokens": [50364, 1296, 732, 16667, 11, 411, 264, 11, 286, 2041, 321, 393, 3373, 365, 264, 13327, 26030, 264, 7110, 936, 11, 50860], "temperature": 0.0, "avg_logprob": -0.14010204094043677, "compression_ratio": 1.5930232558139534, "no_speech_prob": 0.03673085942864418}, {"id": 323, "seek": 208148, "start": 2091.4, "end": 2101.32, "text": " the swimmer bites the shark, and pictures depicting the same kinds of events. And so here,", "tokens": [50860, 264, 7110, 936, 26030, 264, 13327, 11, 293, 5242, 1367, 21490, 264, 912, 3685, 295, 3931, 13, 400, 370, 510, 11, 51356], "temperature": 0.0, "avg_logprob": -0.14010204094043677, "compression_ratio": 1.5930232558139534, "no_speech_prob": 0.03673085942864418}, {"id": 324, "seek": 208148, "start": 2101.32, "end": 2107.0, "text": " by switching around, who's doing what to whom, we're manipulating whether the event is plausible.", "tokens": [51356, 538, 16493, 926, 11, 567, 311, 884, 437, 281, 7101, 11, 321, 434, 40805, 1968, 264, 2280, 307, 39925, 13, 51640], "temperature": 0.0, "avg_logprob": -0.14010204094043677, "compression_ratio": 1.5930232558139534, "no_speech_prob": 0.03673085942864418}, {"id": 325, "seek": 210700, "start": 2107.0, "end": 2114.6, "text": " So likely to occur in the real world or impossible. So unlikely to occur. And the question was,", "tokens": [50364, 407, 3700, 281, 5160, 294, 264, 957, 1002, 420, 6243, 13, 407, 17518, 281, 5160, 13, 400, 264, 1168, 390, 11, 50744], "temperature": 0.0, "avg_logprob": -0.08534782932650659, "compression_ratio": 1.6094674556213018, "no_speech_prob": 0.005906789097934961}, {"id": 326, "seek": 210700, "start": 2115.56, "end": 2123.64, "text": " does the language network respond to language specifically? Or does it respond to meaning", "tokens": [50792, 775, 264, 2856, 3209, 4196, 281, 2856, 4682, 30, 1610, 775, 309, 4196, 281, 3620, 51196], "temperature": 0.0, "avg_logprob": -0.08534782932650659, "compression_ratio": 1.6094674556213018, "no_speech_prob": 0.005906789097934961}, {"id": 327, "seek": 210700, "start": 2123.64, "end": 2129.88, "text": " and concepts more generally? And so if it's language, it should only really respond to", "tokens": [51196, 293, 10392, 544, 5101, 30, 400, 370, 498, 309, 311, 2856, 11, 309, 820, 787, 534, 4196, 281, 51508], "temperature": 0.0, "avg_logprob": -0.08534782932650659, "compression_ratio": 1.6094674556213018, "no_speech_prob": 0.005906789097934961}, {"id": 328, "seek": 212988, "start": 2130.44, "end": 2137.88, "text": " sentences and not to pictures. And if it's responsible to meaning, it should respond equally", "tokens": [50392, 16579, 293, 406, 281, 5242, 13, 400, 498, 309, 311, 6250, 281, 3620, 11, 309, 820, 4196, 12309, 50764], "temperature": 0.0, "avg_logprob": -0.09074120188868323, "compression_ratio": 1.8046511627906976, "no_speech_prob": 0.012427626177668571}, {"id": 329, "seek": 212988, "start": 2137.88, "end": 2143.2400000000002, "text": " strongly to sentences and pictures, as long as the person is thinking about the meaning. And so we", "tokens": [50764, 10613, 281, 16579, 293, 5242, 11, 382, 938, 382, 264, 954, 307, 1953, 466, 264, 3620, 13, 400, 370, 321, 51032], "temperature": 0.0, "avg_logprob": -0.09074120188868323, "compression_ratio": 1.8046511627906976, "no_speech_prob": 0.012427626177668571}, {"id": 330, "seek": 212988, "start": 2143.2400000000002, "end": 2148.6, "text": " had people tell us whether they think the event is plausible or impossible. So you have to be", "tokens": [51032, 632, 561, 980, 505, 1968, 436, 519, 264, 2280, 307, 39925, 420, 6243, 13, 407, 291, 362, 281, 312, 51300], "temperature": 0.0, "avg_logprob": -0.09074120188868323, "compression_ratio": 1.8046511627906976, "no_speech_prob": 0.012427626177668571}, {"id": 331, "seek": 212988, "start": 2148.6, "end": 2157.1600000000003, "text": " thinking about the meaning. And so what we found was actually something in between, where the language", "tokens": [51300, 1953, 466, 264, 3620, 13, 400, 370, 437, 321, 1352, 390, 767, 746, 294, 1296, 11, 689, 264, 2856, 51728], "temperature": 0.0, "avg_logprob": -0.09074120188868323, "compression_ratio": 1.8046511627906976, "no_speech_prob": 0.012427626177668571}, {"id": 332, "seek": 215716, "start": 2157.16, "end": 2165.0, "text": " network, in accordance with all of the prior work, responds more strongly to sentences than to pictures.", "tokens": [50364, 3209, 11, 294, 31110, 365, 439, 295, 264, 4059, 589, 11, 27331, 544, 10613, 281, 16579, 813, 281, 5242, 13, 50756], "temperature": 0.0, "avg_logprob": -0.08602064647031633, "compression_ratio": 1.7, "no_speech_prob": 0.00910886749625206}, {"id": 333, "seek": 215716, "start": 2165.8799999999997, "end": 2173.08, "text": " But it still responded to pictures to some extent. And I will say that in another study,", "tokens": [50800, 583, 309, 920, 15806, 281, 5242, 281, 512, 8396, 13, 400, 286, 486, 584, 300, 294, 1071, 2979, 11, 51160], "temperature": 0.0, "avg_logprob": -0.08602064647031633, "compression_ratio": 1.7, "no_speech_prob": 0.00910886749625206}, {"id": 334, "seek": 215716, "start": 2173.08, "end": 2181.0, "text": " we recorded responses in the language regions to pictures of objects. So is this animal dangerous?", "tokens": [51160, 321, 8287, 13019, 294, 264, 2856, 10682, 281, 5242, 295, 6565, 13, 407, 307, 341, 5496, 5795, 30, 51556], "temperature": 0.0, "avg_logprob": -0.08602064647031633, "compression_ratio": 1.7, "no_speech_prob": 0.00910886749625206}, {"id": 335, "seek": 215716, "start": 2181.0, "end": 2186.2799999999997, "text": " Can this object be found in the kitchen, that kind of stuff? And it did not respond to pictures of", "tokens": [51556, 1664, 341, 2657, 312, 1352, 294, 264, 6525, 11, 300, 733, 295, 1507, 30, 400, 309, 630, 406, 4196, 281, 5242, 295, 51820], "temperature": 0.0, "avg_logprob": -0.08602064647031633, "compression_ratio": 1.7, "no_speech_prob": 0.00910886749625206}, {"id": 336, "seek": 218628, "start": 2186.28, "end": 2193.7200000000003, "text": " objects. So it was something about events, maybe just something more complex, maybe something just", "tokens": [50364, 6565, 13, 407, 309, 390, 746, 466, 3931, 11, 1310, 445, 746, 544, 3997, 11, 1310, 746, 445, 50736], "temperature": 0.0, "avg_logprob": -0.08868370850880941, "compression_ratio": 1.5846994535519126, "no_speech_prob": 0.0015711067244410515}, {"id": 337, "seek": 218628, "start": 2193.7200000000003, "end": 2200.84, "text": " more fast-paced, that was specifically triggering responses in the language regions. And so this", "tokens": [50736, 544, 2370, 12, 47038, 11, 300, 390, 4682, 40406, 13019, 294, 264, 2856, 10682, 13, 400, 370, 341, 51092], "temperature": 0.0, "avg_logprob": -0.08868370850880941, "compression_ratio": 1.5846994535519126, "no_speech_prob": 0.0015711067244410515}, {"id": 338, "seek": 218628, "start": 2200.84, "end": 2208.76, "text": " intermediate result, so preference for sentences over pictures, but also responses to meaning,", "tokens": [51092, 19376, 1874, 11, 370, 17502, 337, 16579, 670, 5242, 11, 457, 611, 13019, 281, 3620, 11, 51488], "temperature": 0.0, "avg_logprob": -0.08868370850880941, "compression_ratio": 1.5846994535519126, "no_speech_prob": 0.0015711067244410515}, {"id": 339, "seek": 220876, "start": 2208.76, "end": 2217.5600000000004, "text": " even in known sentences, was kind of puzzling. And so one piece of evidence that helped us", "tokens": [50364, 754, 294, 2570, 16579, 11, 390, 733, 295, 18741, 1688, 13, 400, 370, 472, 2522, 295, 4467, 300, 4254, 505, 50804], "temperature": 0.0, "avg_logprob": -0.13680924006870815, "compression_ratio": 1.5425531914893618, "no_speech_prob": 0.23545071482658386}, {"id": 340, "seek": 220876, "start": 2218.6800000000003, "end": 2227.7200000000003, "text": " make sense of this information was evidence from individuals with global aphasia, from people with", "tokens": [50860, 652, 2020, 295, 341, 1589, 390, 4467, 490, 5346, 365, 4338, 257, 7485, 654, 11, 490, 561, 365, 51312], "temperature": 0.0, "avg_logprob": -0.13680924006870815, "compression_ratio": 1.5425531914893618, "no_speech_prob": 0.23545071482658386}, {"id": 341, "seek": 220876, "start": 2227.7200000000003, "end": 2233.32, "text": " brain damage. And I should say that this is, yeah, so lots of the brain imaging work I'm describing,", "tokens": [51312, 3567, 4344, 13, 400, 286, 820, 584, 300, 341, 307, 11, 1338, 11, 370, 3195, 295, 264, 3567, 25036, 589, 286, 478, 16141, 11, 51592], "temperature": 0.0, "avg_logprob": -0.13680924006870815, "compression_ratio": 1.5425531914893618, "no_speech_prob": 0.23545071482658386}, {"id": 342, "seek": 223332, "start": 2233.32, "end": 2242.28, "text": " I did with my PhD advisor at Fedorenko, and the global aphasia bit is done in collaboration with", "tokens": [50364, 286, 630, 365, 452, 14476, 19161, 412, 7772, 10948, 4093, 11, 293, 264, 4338, 257, 7485, 654, 857, 307, 1096, 294, 9363, 365, 50812], "temperature": 0.0, "avg_logprob": -0.12664810598713078, "compression_ratio": 1.5621621621621622, "no_speech_prob": 0.004899857100099325}, {"id": 343, "seek": 223332, "start": 2242.28, "end": 2252.1200000000003, "text": " Rosemary Varley at UCL, who works with individuals with global aphasia very, very closely. And so", "tokens": [50812, 11144, 37529, 14662, 3420, 412, 14079, 43, 11, 567, 1985, 365, 5346, 365, 4338, 257, 7485, 654, 588, 11, 588, 8185, 13, 400, 370, 51304], "temperature": 0.0, "avg_logprob": -0.12664810598713078, "compression_ratio": 1.5621621621621622, "no_speech_prob": 0.004899857100099325}, {"id": 344, "seek": 223332, "start": 2252.92, "end": 2260.92, "text": " here we had two individuals with global aphasia, really serious issues, looking at pictures of", "tokens": [51344, 510, 321, 632, 732, 5346, 365, 4338, 257, 7485, 654, 11, 534, 3156, 2663, 11, 1237, 412, 5242, 295, 51744], "temperature": 0.0, "avg_logprob": -0.12664810598713078, "compression_ratio": 1.5621621621621622, "no_speech_prob": 0.004899857100099325}, {"id": 345, "seek": 226332, "start": 2263.48, "end": 2269.6400000000003, "text": " swimmer by sharks, shark bite swimmer. And so as I mentioned earlier, they were laughing at the", "tokens": [50372, 7110, 936, 538, 26312, 11, 13327, 7988, 7110, 936, 13, 400, 370, 382, 286, 2835, 3071, 11, 436, 645, 5059, 412, 264, 50680], "temperature": 0.0, "avg_logprob": -0.1095542140390681, "compression_ratio": 1.7522935779816513, "no_speech_prob": 0.003757825354114175}, {"id": 346, "seek": 226332, "start": 2269.6400000000003, "end": 2275.0, "text": " weird ones. And so in general, they were very good at distinguishing plausible and implausible", "tokens": [50680, 3657, 2306, 13, 400, 370, 294, 2674, 11, 436, 645, 588, 665, 412, 11365, 3807, 39925, 293, 8484, 8463, 964, 50948], "temperature": 0.0, "avg_logprob": -0.1095542140390681, "compression_ratio": 1.7522935779816513, "no_speech_prob": 0.003757825354114175}, {"id": 347, "seek": 226332, "start": 2275.0, "end": 2283.48, "text": " pictures, suggesting that their ability to extract meaning from pictures was there, it did not", "tokens": [50948, 5242, 11, 18094, 300, 641, 3485, 281, 8947, 3620, 490, 5242, 390, 456, 11, 309, 630, 406, 51372], "temperature": 0.0, "avg_logprob": -0.1095542140390681, "compression_ratio": 1.7522935779816513, "no_speech_prob": 0.003757825354114175}, {"id": 348, "seek": 226332, "start": 2283.48, "end": 2290.28, "text": " require a functioning language network. And so then the title of the paper, the language network", "tokens": [51372, 3651, 257, 18483, 2856, 3209, 13, 400, 370, 550, 264, 4876, 295, 264, 3035, 11, 264, 2856, 3209, 51712], "temperature": 0.0, "avg_logprob": -0.1095542140390681, "compression_ratio": 1.7522935779816513, "no_speech_prob": 0.003757825354114175}, {"id": 349, "seek": 229028, "start": 2290.28, "end": 2297.7200000000003, "text": " is recruited but not required for pictorial events and semantics. So we see this activation,", "tokens": [50364, 307, 33004, 457, 406, 4739, 337, 2317, 5181, 3931, 293, 4361, 45298, 13, 407, 321, 536, 341, 24433, 11, 50736], "temperature": 0.0, "avg_logprob": -0.19471810414240912, "compression_ratio": 1.5336787564766838, "no_speech_prob": 0.0004438467149157077}, {"id": 350, "seek": 229028, "start": 2298.44, "end": 2303.8, "text": " and we, but it's not, it's not necessary to do the task.", "tokens": [50772, 293, 321, 11, 457, 309, 311, 406, 11, 309, 311, 406, 4818, 281, 360, 264, 5633, 13, 51040], "temperature": 0.0, "avg_logprob": -0.19471810414240912, "compression_ratio": 1.5336787564766838, "no_speech_prob": 0.0004438467149157077}, {"id": 351, "seek": 229028, "start": 2305.48, "end": 2311.88, "text": " Yeah, yeah, I found that, yeah, very insightful. And what struck me was", "tokens": [51124, 865, 11, 1338, 11, 286, 1352, 300, 11, 1338, 11, 588, 46401, 13, 400, 437, 13159, 385, 390, 51444], "temperature": 0.0, "avg_logprob": -0.19471810414240912, "compression_ratio": 1.5336787564766838, "no_speech_prob": 0.0004438467149157077}, {"id": 352, "seek": 229028, "start": 2313.48, "end": 2318.36, "text": " one of the hypotheses as to why the language network was recruited is that", "tokens": [51524, 472, 295, 264, 49969, 382, 281, 983, 264, 2856, 3209, 390, 33004, 307, 300, 51768], "temperature": 0.0, "avg_logprob": -0.19471810414240912, "compression_ratio": 1.5336787564766838, "no_speech_prob": 0.0004438467149157077}, {"id": 353, "seek": 231836, "start": 2318.36, "end": 2325.08, "text": " it's sort of another way of getting evidence or information on whether this event is likely or not.", "tokens": [50364, 309, 311, 1333, 295, 1071, 636, 295, 1242, 4467, 420, 1589, 322, 1968, 341, 2280, 307, 3700, 420, 406, 13, 50700], "temperature": 0.0, "avg_logprob": -0.09777837753295898, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.003422711743041873}, {"id": 354, "seek": 231836, "start": 2325.88, "end": 2329.56, "text": " And of course, we can't be sure what's going on in there. But, you know, perhaps it is somewhat", "tokens": [50740, 400, 295, 1164, 11, 321, 393, 380, 312, 988, 437, 311, 516, 322, 294, 456, 13, 583, 11, 291, 458, 11, 4317, 309, 307, 8344, 50924], "temperature": 0.0, "avg_logprob": -0.09777837753295898, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.003422711743041873}, {"id": 355, "seek": 231836, "start": 2329.56, "end": 2335.6400000000003, "text": " like a large language model where you, you look at the thing, you're like, part of trying to figure", "tokens": [50924, 411, 257, 2416, 2856, 2316, 689, 291, 11, 291, 574, 412, 264, 551, 11, 291, 434, 411, 11, 644, 295, 1382, 281, 2573, 51228], "temperature": 0.0, "avg_logprob": -0.09777837753295898, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.003422711743041873}, {"id": 356, "seek": 231836, "start": 2335.6400000000003, "end": 2340.2000000000003, "text": " out whether this picture is likely or not is you kind of read it out to yourself and like, well,", "tokens": [51228, 484, 1968, 341, 3036, 307, 3700, 420, 406, 307, 291, 733, 295, 1401, 309, 484, 281, 1803, 293, 411, 11, 731, 11, 51456], "temperature": 0.0, "avg_logprob": -0.09777837753295898, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.003422711743041873}, {"id": 357, "seek": 234020, "start": 2340.2, "end": 2350.12, "text": " does this, is this a familiar pattern, right? Does it, is, you know, shark bites swimmer,", "tokens": [50364, 775, 341, 11, 307, 341, 257, 4963, 5102, 11, 558, 30, 4402, 309, 11, 307, 11, 291, 458, 11, 13327, 26030, 7110, 936, 11, 50860], "temperature": 0.0, "avg_logprob": -0.17849481427991712, "compression_ratio": 1.6306818181818181, "no_speech_prob": 0.022249765694141388}, {"id": 358, "seek": 234020, "start": 2350.12, "end": 2355.7999999999997, "text": " you know, that's that's that sequence of words feels close to sequences of words that I produced", "tokens": [50860, 291, 458, 11, 300, 311, 300, 311, 300, 8310, 295, 2283, 3417, 1998, 281, 22978, 295, 2283, 300, 286, 7126, 51144], "temperature": 0.0, "avg_logprob": -0.17849481427991712, "compression_ratio": 1.6306818181818181, "no_speech_prob": 0.022249765694141388}, {"id": 359, "seek": 234020, "start": 2355.7999999999997, "end": 2362.68, "text": " before or read before, whereas swimmer bites shark is kind of jarring. And maybe behind that is just", "tokens": [51144, 949, 420, 1401, 949, 11, 9735, 7110, 936, 26030, 13327, 307, 733, 295, 361, 18285, 13, 400, 1310, 2261, 300, 307, 445, 51488], "temperature": 0.0, "avg_logprob": -0.17849481427991712, "compression_ratio": 1.6306818181818181, "no_speech_prob": 0.022249765694141388}, {"id": 360, "seek": 236268, "start": 2362.68, "end": 2369.96, "text": " the improbability of that, that, that, that sentence being produced according to the language", "tokens": [50364, 264, 2530, 65, 2310, 295, 300, 11, 300, 11, 300, 11, 300, 8174, 885, 7126, 4650, 281, 264, 2856, 50728], "temperature": 0.0, "avg_logprob": -0.13325225313504538, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.010312519967556}, {"id": 361, "seek": 236268, "start": 2369.96, "end": 2376.9199999999996, "text": " model in our, in our own minds. And of course, yeah, we, we don't really know that our minds work", "tokens": [50728, 2316, 294, 527, 11, 294, 527, 1065, 9634, 13, 400, 295, 1164, 11, 1338, 11, 321, 11, 321, 500, 380, 534, 458, 300, 527, 9634, 589, 51076], "temperature": 0.0, "avg_logprob": -0.13325225313504538, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.010312519967556}, {"id": 362, "seek": 236268, "start": 2376.9199999999996, "end": 2383.56, "text": " like a large language model at all, but it's an attractive hypothesis in as much as it works.", "tokens": [51076, 411, 257, 2416, 2856, 2316, 412, 439, 11, 457, 309, 311, 364, 12609, 17291, 294, 382, 709, 382, 309, 1985, 13, 51408], "temperature": 0.0, "avg_logprob": -0.13325225313504538, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.010312519967556}, {"id": 363, "seek": 236268, "start": 2385.16, "end": 2391.08, "text": " Yeah, so I guess, so generally speaking, right, so we got this result language network recruited", "tokens": [51488, 865, 11, 370, 286, 2041, 11, 370, 5101, 4124, 11, 558, 11, 370, 321, 658, 341, 1874, 2856, 3209, 33004, 51784], "temperature": 0.0, "avg_logprob": -0.13325225313504538, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.010312519967556}, {"id": 364, "seek": 239108, "start": 2391.08, "end": 2395.4, "text": " but not required. And the question was, what's going on, right? And so generally speaking,", "tokens": [50364, 457, 406, 4739, 13, 400, 264, 1168, 390, 11, 437, 311, 516, 322, 11, 558, 30, 400, 370, 5101, 4124, 11, 50580], "temperature": 0.0, "avg_logprob": -0.12066164682077807, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.0015007253969088197}, {"id": 365, "seek": 239108, "start": 2395.4, "end": 2405.08, "text": " we consider two broad hypothesis. One is that that activation is not necessary to do the task.", "tokens": [50580, 321, 1949, 732, 4152, 17291, 13, 1485, 307, 300, 300, 24433, 307, 406, 4818, 281, 360, 264, 5633, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12066164682077807, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.0015007253969088197}, {"id": 366, "seek": 239108, "start": 2405.08, "end": 2411.56, "text": " So you see the picture, sharks, swimmer and biting. And so you activate those words kind of", "tokens": [51064, 407, 291, 536, 264, 3036, 11, 26312, 11, 7110, 936, 293, 32912, 13, 400, 370, 291, 13615, 729, 2283, 733, 295, 51388], "temperature": 0.0, "avg_logprob": -0.12066164682077807, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.0015007253969088197}, {"id": 367, "seek": 239108, "start": 2411.56, "end": 2418.36, "text": " automatically by association, but you're not actually using them to reason about whether", "tokens": [51388, 6772, 538, 14598, 11, 457, 291, 434, 406, 767, 1228, 552, 281, 1778, 466, 1968, 51728], "temperature": 0.0, "avg_logprob": -0.12066164682077807, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.0015007253969088197}, {"id": 368, "seek": 241836, "start": 2418.36, "end": 2424.04, "text": " the event makes sense or not. So that's one hypothesis. And the other hypothesis is that", "tokens": [50364, 264, 2280, 1669, 2020, 420, 406, 13, 407, 300, 311, 472, 17291, 13, 400, 264, 661, 17291, 307, 300, 50648], "temperature": 0.0, "avg_logprob": -0.09752650260925293, "compression_ratio": 1.7926829268292683, "no_speech_prob": 0.006586733274161816}, {"id": 369, "seek": 241836, "start": 2424.04, "end": 2430.36, "text": " actually, the information in the language network is helpful. But when you're trying to", "tokens": [50648, 767, 11, 264, 1589, 294, 264, 2856, 3209, 307, 4961, 13, 583, 562, 291, 434, 1382, 281, 50964], "temperature": 0.0, "avg_logprob": -0.09752650260925293, "compression_ratio": 1.7926829268292683, "no_speech_prob": 0.006586733274161816}, {"id": 370, "seek": 241836, "start": 2431.2400000000002, "end": 2437.1600000000003, "text": " recast information that you're seeing in linguistic form, you can then compare it with", "tokens": [51008, 850, 525, 1589, 300, 291, 434, 2577, 294, 43002, 1254, 11, 291, 393, 550, 6794, 309, 365, 51304], "temperature": 0.0, "avg_logprob": -0.09752650260925293, "compression_ratio": 1.7926829268292683, "no_speech_prob": 0.006586733274161816}, {"id": 371, "seek": 241836, "start": 2437.1600000000003, "end": 2442.1200000000003, "text": " all of the linguistic information that you received in your lifetime. And maybe that", "tokens": [51304, 439, 295, 264, 43002, 1589, 300, 291, 4613, 294, 428, 11364, 13, 400, 1310, 300, 51552], "temperature": 0.0, "avg_logprob": -0.09752650260925293, "compression_ratio": 1.7926829268292683, "no_speech_prob": 0.006586733274161816}, {"id": 372, "seek": 241836, "start": 2442.1200000000003, "end": 2447.7200000000003, "text": " information ended up distilled in your brain in some general way, just kind of, we know that", "tokens": [51552, 1589, 4590, 493, 1483, 6261, 294, 428, 3567, 294, 512, 2674, 636, 11, 445, 733, 295, 11, 321, 458, 300, 51832], "temperature": 0.0, "avg_logprob": -0.09752650260925293, "compression_ratio": 1.7926829268292683, "no_speech_prob": 0.006586733274161816}, {"id": 373, "seek": 244772, "start": 2447.7999999999997, "end": 2452.7599999999998, "text": " people are very sensitive to statistical regularities in language, we know that we're very good at", "tokens": [50368, 561, 366, 588, 9477, 281, 22820, 3890, 1088, 294, 2856, 11, 321, 458, 300, 321, 434, 588, 665, 412, 50616], "temperature": 0.0, "avg_logprob": -0.13168110847473144, "compression_ratio": 1.5133689839572193, "no_speech_prob": 0.0009989291429519653}, {"id": 374, "seek": 244772, "start": 2452.7599999999998, "end": 2462.6, "text": " predicting what word would come next, right? Like there is this information about what patterns", "tokens": [50616, 32884, 437, 1349, 576, 808, 958, 11, 558, 30, 1743, 456, 307, 341, 1589, 466, 437, 8294, 51108], "temperature": 0.0, "avg_logprob": -0.13168110847473144, "compression_ratio": 1.5133689839572193, "no_speech_prob": 0.0009989291429519653}, {"id": 375, "seek": 244772, "start": 2462.6, "end": 2469.0, "text": " are likely in text is very much what people use during real life language comprehension.", "tokens": [51108, 366, 3700, 294, 2487, 307, 588, 709, 437, 561, 764, 1830, 957, 993, 2856, 44991, 13, 51428], "temperature": 0.0, "avg_logprob": -0.13168110847473144, "compression_ratio": 1.5133689839572193, "no_speech_prob": 0.0009989291429519653}, {"id": 376, "seek": 246900, "start": 2469.8, "end": 2478.36, "text": " And of course, that information also can help us, in many cases, figure out which events", "tokens": [50404, 400, 295, 1164, 11, 300, 1589, 611, 393, 854, 505, 11, 294, 867, 3331, 11, 2573, 484, 597, 3931, 50832], "temperature": 0.0, "avg_logprob": -0.11737092091487004, "compression_ratio": 1.5087719298245614, "no_speech_prob": 0.014274575747549534}, {"id": 377, "seek": 246900, "start": 2479.16, "end": 2485.56, "text": " make sense and which doesn't. And so actually, we try to test that hypothesis. We", "tokens": [50872, 652, 2020, 293, 597, 1177, 380, 13, 400, 370, 767, 11, 321, 853, 281, 1500, 300, 17291, 13, 492, 51192], "temperature": 0.0, "avg_logprob": -0.11737092091487004, "compression_ratio": 1.5087719298245614, "no_speech_prob": 0.014274575747549534}, {"id": 378, "seek": 246900, "start": 2487.32, "end": 2494.44, "text": " didn't necessarily, it's hard to test that in actual human brains, although we now have", "tokens": [51280, 994, 380, 4725, 11, 309, 311, 1152, 281, 1500, 300, 294, 3539, 1952, 15442, 11, 4878, 321, 586, 362, 51636], "temperature": 0.0, "avg_logprob": -0.11737092091487004, "compression_ratio": 1.5087719298245614, "no_speech_prob": 0.014274575747549534}, {"id": 379, "seek": 249444, "start": 2495.16, "end": 2505.16, "text": " ideas for how we might be able to do that. But we started by using language models as proof of", "tokens": [50400, 3487, 337, 577, 321, 1062, 312, 1075, 281, 360, 300, 13, 583, 321, 1409, 538, 1228, 2856, 5245, 382, 8177, 295, 50900], "temperature": 0.0, "avg_logprob": -0.07462157143486871, "compression_ratio": 1.6187845303867403, "no_speech_prob": 0.005136216524988413}, {"id": 380, "seek": 249444, "start": 2505.16, "end": 2514.6, "text": " concept, right? So the hypothesis is statistical patterns in language input can help us distinguish", "tokens": [50900, 3410, 11, 558, 30, 407, 264, 17291, 307, 22820, 8294, 294, 2856, 4846, 393, 854, 505, 20206, 51372], "temperature": 0.0, "avg_logprob": -0.07462157143486871, "compression_ratio": 1.6187845303867403, "no_speech_prob": 0.005136216524988413}, {"id": 381, "seek": 249444, "start": 2514.6, "end": 2521.48, "text": " plausible and implausible events. And language models are very good at capturing these statistical", "tokens": [51372, 39925, 293, 8484, 8463, 964, 3931, 13, 400, 2856, 5245, 366, 588, 665, 412, 23384, 613, 22820, 51716], "temperature": 0.0, "avg_logprob": -0.07462157143486871, "compression_ratio": 1.6187845303867403, "no_speech_prob": 0.005136216524988413}, {"id": 382, "seek": 252148, "start": 2521.48, "end": 2528.92, "text": " patterns. So if language models can systematically distinguish plausible and implausible events,", "tokens": [50364, 8294, 13, 407, 498, 2856, 5245, 393, 39531, 20206, 39925, 293, 8484, 8463, 964, 3931, 11, 50736], "temperature": 0.0, "avg_logprob": -0.09514289242880684, "compression_ratio": 1.9340101522842639, "no_speech_prob": 0.0010979331564158201}, {"id": 383, "seek": 252148, "start": 2528.92, "end": 2534.6, "text": " that means that there is enough information there where maybe humans might be able to use that", "tokens": [50736, 300, 1355, 300, 456, 307, 1547, 1589, 456, 689, 1310, 6255, 1062, 312, 1075, 281, 764, 300, 51020], "temperature": 0.0, "avg_logprob": -0.09514289242880684, "compression_ratio": 1.9340101522842639, "no_speech_prob": 0.0010979331564158201}, {"id": 384, "seek": 252148, "start": 2534.6, "end": 2538.76, "text": " information also to distinguish plausible and implausible events, right? So it's not evident", "tokens": [51020, 1589, 611, 281, 20206, 39925, 293, 8484, 8463, 964, 3931, 11, 558, 30, 407, 309, 311, 406, 16371, 51228], "temperature": 0.0, "avg_logprob": -0.09514289242880684, "compression_ratio": 1.9340101522842639, "no_speech_prob": 0.0010979331564158201}, {"id": 385, "seek": 252148, "start": 2538.76, "end": 2546.04, "text": " that humans do, but it's evident that humans can. And so we did that, we use language models and", "tokens": [51228, 300, 6255, 360, 11, 457, 309, 311, 16371, 300, 6255, 393, 13, 400, 370, 321, 630, 300, 11, 321, 764, 2856, 5245, 293, 51592], "temperature": 0.0, "avg_logprob": -0.09514289242880684, "compression_ratio": 1.9340101522842639, "no_speech_prob": 0.0010979331564158201}, {"id": 386, "seek": 254604, "start": 2546.04, "end": 2554.36, "text": " try to see whether they systematically evaluate plausible event descriptions as more likely than", "tokens": [50364, 853, 281, 536, 1968, 436, 39531, 13059, 39925, 2280, 24406, 382, 544, 3700, 813, 50780], "temperature": 0.0, "avg_logprob": -0.08575742752825627, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.003171674907207489}, {"id": 387, "seek": 254604, "start": 2554.36, "end": 2563.64, "text": " implausible. And so in that study, we specifically distinguished between two kinds of events. So", "tokens": [50780, 8484, 8463, 964, 13, 400, 370, 294, 300, 2979, 11, 321, 4682, 21702, 1296, 732, 3685, 295, 3931, 13, 407, 51244], "temperature": 0.0, "avg_logprob": -0.08575742752825627, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.003171674907207489}, {"id": 388, "seek": 254604, "start": 2563.64, "end": 2572.2799999999997, "text": " one is the teacher bought the laptop versus the laptop bought the teacher. So animate, inanimate", "tokens": [51244, 472, 307, 264, 5027, 4243, 264, 10732, 5717, 264, 10732, 4243, 264, 5027, 13, 407, 36439, 11, 33113, 2905, 51676], "temperature": 0.0, "avg_logprob": -0.08575742752825627, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.003171674907207489}, {"id": 389, "seek": 257228, "start": 2572.28, "end": 2579.0800000000004, "text": " interactions. And so when you swap them around, if you interpret the sense verbatim and the inanimate", "tokens": [50364, 13280, 13, 400, 370, 562, 291, 18135, 552, 926, 11, 498, 291, 7302, 264, 2020, 9595, 267, 332, 293, 264, 33113, 2905, 50704], "temperature": 0.0, "avg_logprob": -0.13862036654823703, "compression_ratio": 1.8036529680365296, "no_speech_prob": 0.07792876660823822}, {"id": 390, "seek": 257228, "start": 2579.0800000000004, "end": 2587.32, "text": " object laptop cannot buy anything, buying requires that the subject is animate. And so that's a very", "tokens": [50704, 2657, 10732, 2644, 2256, 1340, 11, 6382, 7029, 300, 264, 3983, 307, 36439, 13, 400, 370, 300, 311, 257, 588, 51116], "temperature": 0.0, "avg_logprob": -0.13862036654823703, "compression_ratio": 1.8036529680365296, "no_speech_prob": 0.07792876660823822}, {"id": 391, "seek": 257228, "start": 2587.32, "end": 2593.96, "text": " kind of in your face screaming violation. And then the other example is kind of like the fox", "tokens": [51116, 733, 295, 294, 428, 1851, 12636, 22840, 13, 400, 550, 264, 661, 1365, 307, 733, 295, 411, 264, 21026, 51448], "temperature": 0.0, "avg_logprob": -0.13862036654823703, "compression_ratio": 1.8036529680365296, "no_speech_prob": 0.07792876660823822}, {"id": 392, "seek": 257228, "start": 2593.96, "end": 2599.0, "text": " chased the rabbit, the rabbit chased the fox or the swimmer by the shark, the shark by the swimmer,", "tokens": [51448, 33091, 264, 19509, 11, 264, 19509, 33091, 264, 21026, 420, 264, 7110, 936, 538, 264, 13327, 11, 264, 13327, 538, 264, 7110, 936, 11, 51700], "temperature": 0.0, "avg_logprob": -0.13862036654823703, "compression_ratio": 1.8036529680365296, "no_speech_prob": 0.07792876660823822}, {"id": 393, "seek": 259900, "start": 2599.0, "end": 2604.76, "text": " right? The swimmer by the shark is not impossible. It can happen. It's just way less likely.", "tokens": [50364, 558, 30, 440, 7110, 936, 538, 264, 13327, 307, 406, 6243, 13, 467, 393, 1051, 13, 467, 311, 445, 636, 1570, 3700, 13, 50652], "temperature": 0.0, "avg_logprob": -0.12327412636049333, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.0012442123843356967}, {"id": 394, "seek": 259900, "start": 2605.4, "end": 2610.12, "text": " And so what we found is that when it comes to distinguishing possible and impossible events,", "tokens": [50684, 400, 370, 437, 321, 1352, 307, 300, 562, 309, 1487, 281, 11365, 3807, 1944, 293, 6243, 3931, 11, 50920], "temperature": 0.0, "avg_logprob": -0.12327412636049333, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.0012442123843356967}, {"id": 395, "seek": 259900, "start": 2610.12, "end": 2617.96, "text": " language models were very good, almost at ceiling. So that was actually very easy for them. But when", "tokens": [50920, 2856, 5245, 645, 588, 665, 11, 1920, 412, 13655, 13, 407, 300, 390, 767, 588, 1858, 337, 552, 13, 583, 562, 51312], "temperature": 0.0, "avg_logprob": -0.12327412636049333, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.0012442123843356967}, {"id": 396, "seek": 259900, "start": 2617.96, "end": 2624.12, "text": " it came to likely versus unlikely events, there was a gap in performance. So they weren't quite as", "tokens": [51312, 309, 1361, 281, 3700, 5717, 17518, 3931, 11, 456, 390, 257, 7417, 294, 3389, 13, 407, 436, 4999, 380, 1596, 382, 51620], "temperature": 0.0, "avg_logprob": -0.12327412636049333, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.0012442123843356967}, {"id": 397, "seek": 262412, "start": 2624.12, "end": 2629.88, "text": " good. They were okay. They were above chance. But they definitely weren't perfect. And so we're", "tokens": [50364, 665, 13, 814, 645, 1392, 13, 814, 645, 3673, 2931, 13, 583, 436, 2138, 4999, 380, 2176, 13, 400, 370, 321, 434, 50652], "temperature": 0.0, "avg_logprob": -0.17514065058544429, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.02972792088985443}, {"id": 398, "seek": 262412, "start": 2629.88, "end": 2637.64, "text": " not as good as people, I guess, right? Yeah, not not as good as people and not as good as when they", "tokens": [50652, 406, 382, 665, 382, 561, 11, 286, 2041, 11, 558, 30, 865, 11, 406, 406, 382, 665, 382, 561, 293, 406, 382, 665, 382, 562, 436, 51040], "temperature": 0.0, "avg_logprob": -0.17514065058544429, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.02972792088985443}, {"id": 399, "seek": 262412, "start": 2637.64, "end": 2643.48, "text": " have to deal with animate inanimate sentence, right with impossible events. Yeah. So I think it's", "tokens": [51040, 362, 281, 2028, 365, 36439, 33113, 2905, 8174, 11, 558, 365, 6243, 3931, 13, 865, 13, 407, 286, 519, 309, 311, 51332], "temperature": 0.0, "avg_logprob": -0.17514065058544429, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.02972792088985443}, {"id": 400, "seek": 262412, "start": 2643.48, "end": 2648.2, "text": " like, to me, it's actually more interesting to compare those two sentence types, like how models", "tokens": [51332, 411, 11, 281, 385, 11, 309, 311, 767, 544, 1880, 281, 6794, 729, 732, 8174, 3467, 11, 411, 577, 5245, 51568], "temperature": 0.0, "avg_logprob": -0.17514065058544429, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.02972792088985443}, {"id": 401, "seek": 264820, "start": 2648.2, "end": 2655.0, "text": " do on them. But also humans humans do well on both because these are easy sentences. So", "tokens": [50364, 360, 322, 552, 13, 583, 611, 6255, 6255, 360, 731, 322, 1293, 570, 613, 366, 1858, 16579, 13, 407, 50704], "temperature": 0.0, "avg_logprob": -0.15445128083229065, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.009402272291481495}, {"id": 402, "seek": 264820, "start": 2655.0, "end": 2663.24, "text": " they're not meant to be challenging. And so, yeah. No, I was gonna say, and do we take that as evidence", "tokens": [50704, 436, 434, 406, 4140, 281, 312, 7595, 13, 400, 370, 11, 1338, 13, 883, 11, 286, 390, 799, 584, 11, 293, 360, 321, 747, 300, 382, 4467, 51116], "temperature": 0.0, "avg_logprob": -0.15445128083229065, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.009402272291481495}, {"id": 403, "seek": 264820, "start": 2663.24, "end": 2671.64, "text": " that then when humans reason about these things, they're doing it, not just linguistically, or is", "tokens": [51116, 300, 550, 562, 6255, 1778, 466, 613, 721, 11, 436, 434, 884, 309, 11, 406, 445, 21766, 20458, 11, 420, 307, 51536], "temperature": 0.0, "avg_logprob": -0.15445128083229065, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.009402272291481495}, {"id": 404, "seek": 264820, "start": 2671.64, "end": 2676.4399999999996, "text": " it that our language models are kind of like better than the language models that that are out there", "tokens": [51536, 309, 300, 527, 2856, 5245, 366, 733, 295, 411, 1101, 813, 264, 2856, 5245, 300, 300, 366, 484, 456, 51776], "temperature": 0.0, "avg_logprob": -0.15445128083229065, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.009402272291481495}, {"id": 405, "seek": 267644, "start": 2676.44, "end": 2682.92, "text": " in, you know, the computer language, large language models? I think that I think it's", "tokens": [50364, 294, 11, 291, 458, 11, 264, 3820, 2856, 11, 2416, 2856, 5245, 30, 286, 519, 300, 286, 519, 309, 311, 50688], "temperature": 0.0, "avg_logprob": -0.09626097028905695, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0006984866922721267}, {"id": 406, "seek": 267644, "start": 2682.92, "end": 2688.12, "text": " evident that humans are doing it not linguistically. And so the reason why we think there is this", "tokens": [50688, 16371, 300, 6255, 366, 884, 309, 406, 21766, 20458, 13, 400, 370, 264, 1778, 983, 321, 519, 456, 307, 341, 50948], "temperature": 0.0, "avg_logprob": -0.09626097028905695, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0006984866922721267}, {"id": 407, "seek": 267644, "start": 2688.12, "end": 2696.2000000000003, "text": " performance gap is because actually, the language input that we receive doesn't faithfully describe", "tokens": [50948, 3389, 7417, 307, 570, 767, 11, 264, 2856, 4846, 300, 321, 4774, 1177, 380, 4522, 2277, 6786, 51352], "temperature": 0.0, "avg_logprob": -0.09626097028905695, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0006984866922721267}, {"id": 408, "seek": 267644, "start": 2696.2000000000003, "end": 2704.12, "text": " the world around us. So when we talk to each other, we don't just passively describe everything", "tokens": [51352, 264, 1002, 926, 505, 13, 407, 562, 321, 751, 281, 1184, 661, 11, 321, 500, 380, 445, 1320, 3413, 6786, 1203, 51748], "temperature": 0.0, "avg_logprob": -0.09626097028905695, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0006984866922721267}, {"id": 409, "seek": 270412, "start": 2704.12, "end": 2709.88, "text": " that we're seeing. So I'm not telling you, you know, I am sitting down, the lights are on,", "tokens": [50364, 300, 321, 434, 2577, 13, 407, 286, 478, 406, 3585, 291, 11, 291, 458, 11, 286, 669, 3798, 760, 11, 264, 5811, 366, 322, 11, 50652], "temperature": 0.0, "avg_logprob": -0.09023650149081616, "compression_ratio": 1.5811965811965811, "no_speech_prob": 0.021579749882221222}, {"id": 410, "seek": 270412, "start": 2709.88, "end": 2716.52, "text": " the room is empty, like it's very boring stuff. I'm telling you about things that are unusual,", "tokens": [50652, 264, 1808, 307, 6707, 11, 411, 309, 311, 588, 9989, 1507, 13, 286, 478, 3585, 291, 466, 721, 300, 366, 10901, 11, 50984], "temperature": 0.0, "avg_logprob": -0.09023650149081616, "compression_ratio": 1.5811965811965811, "no_speech_prob": 0.021579749882221222}, {"id": 411, "seek": 270412, "start": 2716.52, "end": 2723.64, "text": " novel, interesting, newsworthy in some way. And so this phenomenon is known as reporting bias.", "tokens": [50984, 7613, 11, 1880, 11, 2583, 23727, 294, 512, 636, 13, 400, 370, 341, 14029, 307, 2570, 382, 10031, 12577, 13, 51340], "temperature": 0.0, "avg_logprob": -0.09023650149081616, "compression_ratio": 1.5811965811965811, "no_speech_prob": 0.021579749882221222}, {"id": 412, "seek": 270412, "start": 2724.2799999999997, "end": 2733.16, "text": " So language tends to undercover on the report information that is kind of trivial, right,", "tokens": [51372, 407, 2856, 12258, 281, 48099, 322, 264, 2275, 1589, 300, 307, 733, 295, 26703, 11, 558, 11, 51816], "temperature": 0.0, "avg_logprob": -0.09023650149081616, "compression_ratio": 1.5811965811965811, "no_speech_prob": 0.021579749882221222}, {"id": 413, "seek": 273316, "start": 2733.24, "end": 2739.48, "text": " that everybody already knows, or can reasonably infer. And so maybe actually events that are", "tokens": [50368, 300, 2201, 1217, 3255, 11, 420, 393, 23551, 13596, 13, 400, 370, 1310, 767, 3931, 300, 366, 50680], "temperature": 0.0, "avg_logprob": -0.10563989343314335, "compression_ratio": 1.7902621722846441, "no_speech_prob": 0.010473676025867462}, {"id": 414, "seek": 273316, "start": 2739.48, "end": 2745.56, "text": " unlikely are not as unlikely for LLMs, because, you know, we talk about unlikely things all the", "tokens": [50680, 17518, 366, 406, 382, 17518, 337, 441, 43, 26386, 11, 570, 11, 291, 458, 11, 321, 751, 466, 17518, 721, 439, 264, 50984], "temperature": 0.0, "avg_logprob": -0.10563989343314335, "compression_ratio": 1.7902621722846441, "no_speech_prob": 0.010473676025867462}, {"id": 415, "seek": 273316, "start": 2745.56, "end": 2750.2, "text": " time, that's the stuff that's worth talking about. And so if that's true, that's the reason why we", "tokens": [50984, 565, 11, 300, 311, 264, 1507, 300, 311, 3163, 1417, 466, 13, 400, 370, 498, 300, 311, 2074, 11, 300, 311, 264, 1778, 983, 321, 51216], "temperature": 0.0, "avg_logprob": -0.10563989343314335, "compression_ratio": 1.7902621722846441, "no_speech_prob": 0.010473676025867462}, {"id": 416, "seek": 273316, "start": 2750.2, "end": 2755.7999999999997, "text": " see this performance gap, then even if the human language model is very good, which by the way,", "tokens": [51216, 536, 341, 3389, 7417, 11, 550, 754, 498, 264, 1952, 2856, 2316, 307, 588, 665, 11, 597, 538, 264, 636, 11, 51496], "temperature": 0.0, "avg_logprob": -0.10563989343314335, "compression_ratio": 1.7902621722846441, "no_speech_prob": 0.010473676025867462}, {"id": 417, "seek": 273316, "start": 2755.7999999999997, "end": 2759.64, "text": " we don't think it is actually, I think large language models now are much better at predicting", "tokens": [51496, 321, 500, 380, 519, 309, 307, 767, 11, 286, 519, 2416, 2856, 5245, 586, 366, 709, 1101, 412, 32884, 51688], "temperature": 0.0, "avg_logprob": -0.10563989343314335, "compression_ratio": 1.7902621722846441, "no_speech_prob": 0.010473676025867462}, {"id": 418, "seek": 275964, "start": 2759.64, "end": 2765.08, "text": " the next war that humans are. So actually, they're better. But even if humans were really good,", "tokens": [50364, 264, 958, 1516, 300, 6255, 366, 13, 407, 767, 11, 436, 434, 1101, 13, 583, 754, 498, 6255, 645, 534, 665, 11, 50636], "temperature": 0.0, "avg_logprob": -0.08124863680671243, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.003481079824268818}, {"id": 419, "seek": 275964, "start": 2765.08, "end": 2773.3199999999997, "text": " just the language input is insufficient for us to be able to distinguish plausible and", "tokens": [50636, 445, 264, 2856, 4846, 307, 41709, 337, 505, 281, 312, 1075, 281, 20206, 39925, 293, 51048], "temperature": 0.0, "avg_logprob": -0.08124863680671243, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.003481079824268818}, {"id": 420, "seek": 275964, "start": 2773.3199999999997, "end": 2779.0, "text": " implausible events. That means that we have to use something else in addition, we have to maybe", "tokens": [51048, 8484, 8463, 964, 3931, 13, 663, 1355, 300, 321, 362, 281, 764, 746, 1646, 294, 4500, 11, 321, 362, 281, 1310, 51332], "temperature": 0.0, "avg_logprob": -0.08124863680671243, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.003481079824268818}, {"id": 421, "seek": 275964, "start": 2779.0, "end": 2784.52, "text": " have some more sophisticated model of the world, where we can actually correct for this reporting", "tokens": [51332, 362, 512, 544, 16950, 2316, 295, 264, 1002, 11, 689, 321, 393, 767, 3006, 337, 341, 10031, 51608], "temperature": 0.0, "avg_logprob": -0.08124863680671243, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.003481079824268818}, {"id": 422, "seek": 278452, "start": 2784.52, "end": 2793.0, "text": " bias, we have to also bring in information that's about what things are typical, what we", "tokens": [50364, 12577, 11, 321, 362, 281, 611, 1565, 294, 1589, 300, 311, 466, 437, 721, 366, 7476, 11, 437, 321, 50788], "temperature": 0.0, "avg_logprob": -0.11861666291952133, "compression_ratio": 1.4972375690607735, "no_speech_prob": 0.00743885338306427}, {"id": 423, "seek": 278452, "start": 2793.0, "end": 2799.16, "text": " can expect, what we cannot expect. So we're probably drawing on sort of multiple mental", "tokens": [50788, 393, 2066, 11, 437, 321, 2644, 2066, 13, 407, 321, 434, 1391, 6316, 322, 1333, 295, 3866, 4973, 51096], "temperature": 0.0, "avg_logprob": -0.11861666291952133, "compression_ratio": 1.4972375690607735, "no_speech_prob": 0.00743885338306427}, {"id": 424, "seek": 278452, "start": 2799.16, "end": 2809.88, "text": " resources or systems. In the case of the clearly kind of impossible, so computer bias teacher,", "tokens": [51096, 3593, 420, 3652, 13, 682, 264, 1389, 295, 264, 4448, 733, 295, 6243, 11, 370, 3820, 12577, 5027, 11, 51632], "temperature": 0.0, "avg_logprob": -0.11861666291952133, "compression_ratio": 1.4972375690607735, "no_speech_prob": 0.00743885338306427}, {"id": 425, "seek": 280988, "start": 2810.84, "end": 2816.84, "text": " is it just the language, can you see if it's just the language, or have you seen if it was just", "tokens": [50412, 307, 309, 445, 264, 2856, 11, 393, 291, 536, 498, 309, 311, 445, 264, 2856, 11, 420, 362, 291, 1612, 498, 309, 390, 445, 50712], "temperature": 0.0, "avg_logprob": -0.18414549032847086, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.023543676361441612}, {"id": 426, "seek": 280988, "start": 2816.84, "end": 2821.08, "text": " the language network that's recruited there, as one might think, well, if it can just be done", "tokens": [50712, 264, 2856, 3209, 300, 311, 33004, 456, 11, 382, 472, 1062, 519, 11, 731, 11, 498, 309, 393, 445, 312, 1096, 50924], "temperature": 0.0, "avg_logprob": -0.18414549032847086, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.023543676361441612}, {"id": 427, "seek": 280988, "start": 2821.08, "end": 2827.8, "text": " within the kind of the one region, maybe it's more efficient and metabolically, there might be some", "tokens": [50924, 1951, 264, 733, 295, 264, 472, 4458, 11, 1310, 309, 311, 544, 7148, 293, 19110, 984, 11, 456, 1062, 312, 512, 51260], "temperature": 0.0, "avg_logprob": -0.18414549032847086, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.023543676361441612}, {"id": 428, "seek": 280988, "start": 2827.8, "end": 2835.48, "text": " kind of preference for doing that if it were possible. By default, we kind of light up various", "tokens": [51260, 733, 295, 17502, 337, 884, 300, 498, 309, 645, 1944, 13, 3146, 7576, 11, 321, 733, 295, 1442, 493, 3683, 51644], "temperature": 0.0, "avg_logprob": -0.18414549032847086, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.023543676361441612}, {"id": 429, "seek": 283548, "start": 2835.64, "end": 2843.88, "text": " regions just to make sure I don't know. So that's actually a study that I would love to do next,", "tokens": [50372, 10682, 445, 281, 652, 988, 286, 500, 380, 458, 13, 407, 300, 311, 767, 257, 2979, 300, 286, 576, 959, 281, 360, 958, 11, 50784], "temperature": 0.0, "avg_logprob": -0.12605680268386316, "compression_ratio": 1.5458333333333334, "no_speech_prob": 0.004326784051954746}, {"id": 430, "seek": 283548, "start": 2843.88, "end": 2850.04, "text": " so this difference between impossible and unlikely events is something that emerged out of this", "tokens": [50784, 370, 341, 2649, 1296, 6243, 293, 17518, 3931, 307, 746, 300, 20178, 484, 295, 341, 51092], "temperature": 0.0, "avg_logprob": -0.12605680268386316, "compression_ratio": 1.5458333333333334, "no_speech_prob": 0.004326784051954746}, {"id": 431, "seek": 283548, "start": 2850.04, "end": 2857.32, "text": " language model study. And so now, of course, yeah, I think it would be great to bring it back to", "tokens": [51092, 2856, 2316, 2979, 13, 400, 370, 586, 11, 295, 1164, 11, 1338, 11, 286, 519, 309, 576, 312, 869, 281, 1565, 309, 646, 281, 51456], "temperature": 0.0, "avg_logprob": -0.12605680268386316, "compression_ratio": 1.5458333333333334, "no_speech_prob": 0.004326784051954746}, {"id": 432, "seek": 283548, "start": 2857.88, "end": 2864.2, "text": " the MRI machine, measure people's brain activity in response to impossible versus", "tokens": [51484, 264, 32812, 3479, 11, 3481, 561, 311, 3567, 5191, 294, 4134, 281, 6243, 5717, 51800], "temperature": 0.0, "avg_logprob": -0.12605680268386316, "compression_ratio": 1.5458333333333334, "no_speech_prob": 0.004326784051954746}, {"id": 433, "seek": 286420, "start": 2864.2, "end": 2869.72, "text": " unlikely centers, and see if the language network alone is sufficient for distinguishing", "tokens": [50364, 17518, 10898, 11, 293, 536, 498, 264, 2856, 3209, 3312, 307, 11563, 337, 11365, 3807, 50640], "temperature": 0.0, "avg_logprob": -0.139938228732937, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.0011322617065161467}, {"id": 434, "seek": 286420, "start": 2870.3599999999997, "end": 2875.16, "text": " possible and impossible events. That is the prediction that follows from this language", "tokens": [50672, 1944, 293, 6243, 3931, 13, 663, 307, 264, 17630, 300, 10002, 490, 341, 2856, 50912], "temperature": 0.0, "avg_logprob": -0.139938228732937, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.0011322617065161467}, {"id": 435, "seek": 286420, "start": 2875.16, "end": 2880.6, "text": " model work. And so I would love to test that. Yeah, yeah, that would be so, yeah, I'd love to", "tokens": [50912, 2316, 589, 13, 400, 370, 286, 576, 959, 281, 1500, 300, 13, 865, 11, 1338, 11, 300, 576, 312, 370, 11, 1338, 11, 286, 1116, 959, 281, 51184], "temperature": 0.0, "avg_logprob": -0.139938228732937, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.0011322617065161467}, {"id": 436, "seek": 286420, "start": 2880.6, "end": 2887.56, "text": " see the results of that. Yeah, so I hope that that happens. But I suppose, you know, coming back to", "tokens": [51184, 536, 264, 3542, 295, 300, 13, 865, 11, 370, 286, 1454, 300, 300, 2314, 13, 583, 286, 7297, 11, 291, 458, 11, 1348, 646, 281, 51532], "temperature": 0.0, "avg_logprob": -0.139938228732937, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.0011322617065161467}, {"id": 437, "seek": 288756, "start": 2888.2, "end": 2896.2799999999997, "text": " LLMs, what we're starting to see is that maybe just a large language model in itself,", "tokens": [50396, 441, 43, 26386, 11, 437, 321, 434, 2891, 281, 536, 307, 300, 1310, 445, 257, 2416, 2856, 2316, 294, 2564, 11, 50800], "temperature": 0.0, "avg_logprob": -0.12154275598660322, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.008568448945879936}, {"id": 438, "seek": 288756, "start": 2898.2, "end": 2908.52, "text": " for various reasons, might not be so effective at thinking or reasoning as the human brain. And", "tokens": [50896, 337, 3683, 4112, 11, 1062, 406, 312, 370, 4942, 412, 1953, 420, 21577, 382, 264, 1952, 3567, 13, 400, 51412], "temperature": 0.0, "avg_logprob": -0.12154275598660322, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.008568448945879936}, {"id": 439, "seek": 288756, "start": 2908.52, "end": 2916.52, "text": " one is, as you kind of mentioned, that the data that comes in is kind of biased toward the salient", "tokens": [51412, 472, 307, 11, 382, 291, 733, 295, 2835, 11, 300, 264, 1412, 300, 1487, 294, 307, 733, 295, 28035, 7361, 264, 1845, 1196, 51812], "temperature": 0.0, "avg_logprob": -0.12154275598660322, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.008568448945879936}, {"id": 440, "seek": 291652, "start": 2916.6, "end": 2922.68, "text": " and newsworthy as you put it. But then another from the kind of Python example is that, well,", "tokens": [50368, 293, 2583, 23727, 382, 291, 829, 309, 13, 583, 550, 1071, 490, 264, 733, 295, 15329, 1365, 307, 300, 11, 731, 11, 50672], "temperature": 0.0, "avg_logprob": -0.12874545221743378, "compression_ratio": 1.5106382978723405, "no_speech_prob": 0.0005524681764654815}, {"id": 441, "seek": 291652, "start": 2922.68, "end": 2931.72, "text": " as a matter of fact, we don't use the language part of the brain for code comprehension or for", "tokens": [50672, 382, 257, 1871, 295, 1186, 11, 321, 500, 380, 764, 264, 2856, 644, 295, 264, 3567, 337, 3089, 44991, 420, 337, 51124], "temperature": 0.0, "avg_logprob": -0.12874545221743378, "compression_ratio": 1.5106382978723405, "no_speech_prob": 0.0005524681764654815}, {"id": 442, "seek": 291652, "start": 2932.28, "end": 2941.48, "text": " logical mathematical reasoning, either for that matter. I suppose my question there is, though,", "tokens": [51152, 14978, 18894, 21577, 11, 2139, 337, 300, 1871, 13, 286, 7297, 452, 1168, 456, 307, 11, 1673, 11, 51612], "temperature": 0.0, "avg_logprob": -0.12874545221743378, "compression_ratio": 1.5106382978723405, "no_speech_prob": 0.0005524681764654815}, {"id": 443, "seek": 294148, "start": 2942.04, "end": 2947.0, "text": " you know, could it be possible for LLMs to kind of just be", "tokens": [50392, 291, 458, 11, 727, 309, 312, 1944, 337, 441, 43, 26386, 281, 733, 295, 445, 312, 50640], "temperature": 0.0, "avg_logprob": -0.12656728152571053, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.001500381506048143}, {"id": 444, "seek": 294148, "start": 2951.88, "end": 2959.2400000000002, "text": " be able to take on the functions of the multiple demands network, for instance, which is doing", "tokens": [50884, 312, 1075, 281, 747, 322, 264, 6828, 295, 264, 3866, 15107, 3209, 11, 337, 5197, 11, 597, 307, 884, 51252], "temperature": 0.0, "avg_logprob": -0.12656728152571053, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.001500381506048143}, {"id": 445, "seek": 294148, "start": 2959.2400000000002, "end": 2965.0, "text": " all this, which is the place which does the mathematical logical Python code interpretation", "tokens": [51252, 439, 341, 11, 597, 307, 264, 1081, 597, 775, 264, 18894, 14978, 15329, 3089, 14174, 51540], "temperature": 0.0, "avg_logprob": -0.12656728152571053, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.001500381506048143}, {"id": 446, "seek": 296500, "start": 2965.56, "end": 2971.72, "text": " comprehension? Could it kind of take on all those responsibilities just by having", "tokens": [50392, 44991, 30, 7497, 309, 733, 295, 747, 322, 439, 729, 16190, 445, 538, 1419, 50700], "temperature": 0.0, "avg_logprob": -0.1656107126280319, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.02030598558485508}, {"id": 447, "seek": 296500, "start": 2973.4, "end": 2977.96, "text": " getting really good at saying, you know, next word prediction for mathematical problems and", "tokens": [50784, 1242, 534, 665, 412, 1566, 11, 291, 458, 11, 958, 1349, 17630, 337, 18894, 2740, 293, 51012], "temperature": 0.0, "avg_logprob": -0.1656107126280319, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.02030598558485508}, {"id": 448, "seek": 296500, "start": 2977.96, "end": 2984.84, "text": " next word prediction for code generation and so on? Or is just that kind of, or is that implausible?", "tokens": [51012, 958, 1349, 17630, 337, 3089, 5125, 293, 370, 322, 30, 1610, 307, 445, 300, 733, 295, 11, 420, 307, 300, 8484, 8463, 964, 30, 51356], "temperature": 0.0, "avg_logprob": -0.1656107126280319, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.02030598558485508}, {"id": 449, "seek": 296500, "start": 2984.84, "end": 2992.84, "text": " I don't really know how we characterize, you know, where the LLMs just could have kind of", "tokens": [51356, 286, 500, 380, 534, 458, 577, 321, 38463, 11, 291, 458, 11, 689, 264, 441, 43, 26386, 445, 727, 362, 733, 295, 51756], "temperature": 0.0, "avg_logprob": -0.1656107126280319, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.02030598558485508}, {"id": 450, "seek": 299284, "start": 2992.92, "end": 2998.1200000000003, "text": " emergently develop all those capabilities within a single language model, or if that's just", "tokens": [50368, 4345, 70, 2276, 1499, 439, 729, 10862, 1951, 257, 2167, 2856, 2316, 11, 420, 498, 300, 311, 445, 50628], "temperature": 0.0, "avg_logprob": -0.12663333095721344, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.00035133736673742533}, {"id": 451, "seek": 299284, "start": 2999.7200000000003, "end": 3012.2000000000003, "text": " very, very unlikely. Yeah, so LLMs do a bunch of different things. In general, as you mentioned,", "tokens": [50708, 588, 11, 588, 17518, 13, 865, 11, 370, 441, 43, 26386, 360, 257, 3840, 295, 819, 721, 13, 682, 2674, 11, 382, 291, 2835, 11, 51332], "temperature": 0.0, "avg_logprob": -0.12663333095721344, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.00035133736673742533}, {"id": 452, "seek": 299284, "start": 3012.2000000000003, "end": 3017.4, "text": " they're very, very good at pattern recognition and pattern completion at different levels of", "tokens": [51332, 436, 434, 588, 11, 588, 665, 412, 5102, 11150, 293, 5102, 19372, 412, 819, 4358, 295, 51592], "temperature": 0.0, "avg_logprob": -0.12663333095721344, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.00035133736673742533}, {"id": 453, "seek": 301740, "start": 3017.4, "end": 3024.36, "text": " abstraction. So they do a lot of just direct memorization, right? The larger the model,", "tokens": [50364, 37765, 13, 407, 436, 360, 257, 688, 295, 445, 2047, 10560, 2144, 11, 558, 30, 440, 4833, 264, 2316, 11, 50712], "temperature": 0.0, "avg_logprob": -0.09340190887451172, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.00115127710159868}, {"id": 454, "seek": 301740, "start": 3024.36, "end": 3030.6, "text": " the more texts that can just memorize straight up, which is why a lot of those copyright issues", "tokens": [50712, 264, 544, 15765, 300, 393, 445, 27478, 2997, 493, 11, 597, 307, 983, 257, 688, 295, 729, 17996, 2663, 51024], "temperature": 0.0, "avg_logprob": -0.09340190887451172, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.00115127710159868}, {"id": 455, "seek": 301740, "start": 3030.6, "end": 3035.8, "text": " end up arising. But that's not the only thing that these models do, because they definitely", "tokens": [51024, 917, 493, 44900, 13, 583, 300, 311, 406, 264, 787, 551, 300, 613, 5245, 360, 11, 570, 436, 2138, 51284], "temperature": 0.0, "avg_logprob": -0.09340190887451172, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.00115127710159868}, {"id": 456, "seek": 301740, "start": 3035.8, "end": 3042.76, "text": " are capable of generating novel texts and mixing and matching previous inputs. And so the patterns", "tokens": [51284, 366, 8189, 295, 17746, 7613, 15765, 293, 11983, 293, 14324, 3894, 15743, 13, 400, 370, 264, 8294, 51632], "temperature": 0.0, "avg_logprob": -0.09340190887451172, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.00115127710159868}, {"id": 457, "seek": 304276, "start": 3042.76, "end": 3049.88, "text": " that they can recognize and reproduce, they can be fairly abstract. But then, of course, the question", "tokens": [50364, 300, 436, 393, 5521, 293, 29501, 11, 436, 393, 312, 6457, 12649, 13, 583, 550, 11, 295, 1164, 11, 264, 1168, 50720], "temperature": 0.0, "avg_logprob": -0.10126067532433404, "compression_ratio": 1.5544041450777202, "no_speech_prob": 0.0028004366904497147}, {"id": 458, "seek": 304276, "start": 3049.88, "end": 3062.36, "text": " then is pattern completion all it takes? Is that the only thing that's necessary? And so that's where", "tokens": [50720, 550, 307, 5102, 19372, 439, 309, 2516, 30, 1119, 300, 264, 787, 551, 300, 311, 4818, 30, 400, 370, 300, 311, 689, 51344], "temperature": 0.0, "avg_logprob": -0.10126067532433404, "compression_ratio": 1.5544041450777202, "no_speech_prob": 0.0028004366904497147}, {"id": 459, "seek": 304276, "start": 3062.36, "end": 3070.84, "text": " it gets tricky, because a lot of logical reasoning is algorithmic reasoning. It's symbolic. It's", "tokens": [51344, 309, 2170, 12414, 11, 570, 257, 688, 295, 14978, 21577, 307, 9284, 299, 21577, 13, 467, 311, 25755, 13, 467, 311, 51768], "temperature": 0.0, "avg_logprob": -0.10126067532433404, "compression_ratio": 1.5544041450777202, "no_speech_prob": 0.0028004366904497147}, {"id": 460, "seek": 307084, "start": 3071.48, "end": 3078.28, "text": " very regimented. And so these are the kinds of problems where these models seem to struggle.", "tokens": [50396, 588, 47888, 292, 13, 400, 370, 613, 366, 264, 3685, 295, 2740, 689, 613, 5245, 1643, 281, 7799, 13, 50736], "temperature": 0.0, "avg_logprob": -0.05622151050161808, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.0005357019254006445}, {"id": 461, "seek": 307084, "start": 3078.28, "end": 3084.36, "text": " So for example, if you ask them to add and multiply two numbers together, if the numbers are small", "tokens": [50736, 407, 337, 1365, 11, 498, 291, 1029, 552, 281, 909, 293, 12972, 732, 3547, 1214, 11, 498, 264, 3547, 366, 1359, 51040], "temperature": 0.0, "avg_logprob": -0.05622151050161808, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.0005357019254006445}, {"id": 462, "seek": 307084, "start": 3084.36, "end": 3091.8, "text": " enough, then the model is doing just fine. But if the number is large, that means it wasn't part of", "tokens": [51040, 1547, 11, 550, 264, 2316, 307, 884, 445, 2489, 13, 583, 498, 264, 1230, 307, 2416, 11, 300, 1355, 309, 2067, 380, 644, 295, 51412], "temperature": 0.0, "avg_logprob": -0.05622151050161808, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.0005357019254006445}, {"id": 463, "seek": 307084, "start": 3091.8, "end": 3096.92, "text": " the training set. It means it couldn't have just memorized the response, which it probably does for", "tokens": [51412, 264, 3097, 992, 13, 467, 1355, 309, 2809, 380, 362, 445, 46677, 264, 4134, 11, 597, 309, 1391, 775, 337, 51668], "temperature": 0.0, "avg_logprob": -0.05622151050161808, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.0005357019254006445}, {"id": 464, "seek": 309692, "start": 3096.92, "end": 3103.8, "text": " a lot of smaller number combinations. And so then it would actually have to multiply step by step.", "tokens": [50364, 257, 688, 295, 4356, 1230, 21267, 13, 400, 370, 550, 309, 576, 767, 362, 281, 12972, 1823, 538, 1823, 13, 50708], "temperature": 0.0, "avg_logprob": -0.05619875928188892, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0022496015299111605}, {"id": 465, "seek": 309692, "start": 3103.8, "end": 3111.08, "text": " And it doesn't seem to be doing that very successfully. In fact, it often gives you a number", "tokens": [50708, 400, 309, 1177, 380, 1643, 281, 312, 884, 300, 588, 10727, 13, 682, 1186, 11, 309, 2049, 2709, 291, 257, 1230, 51072], "temperature": 0.0, "avg_logprob": -0.05619875928188892, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0022496015299111605}, {"id": 466, "seek": 309692, "start": 3111.08, "end": 3118.6800000000003, "text": " that's close, but just a little bit off. And so the kinds of mistake that it's making is different", "tokens": [51072, 300, 311, 1998, 11, 457, 445, 257, 707, 857, 766, 13, 400, 370, 264, 3685, 295, 6146, 300, 309, 311, 1455, 307, 819, 51452], "temperature": 0.0, "avg_logprob": -0.05619875928188892, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0022496015299111605}, {"id": 467, "seek": 309692, "start": 3118.6800000000003, "end": 3123.4, "text": " from the kind of mistake a human would be making, because it's still trying to use pattern matching", "tokens": [51452, 490, 264, 733, 295, 6146, 257, 1952, 576, 312, 1455, 11, 570, 309, 311, 920, 1382, 281, 764, 5102, 14324, 51688], "temperature": 0.0, "avg_logprob": -0.05619875928188892, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0022496015299111605}, {"id": 468, "seek": 312340, "start": 3123.48, "end": 3131.48, "text": " to complete. And it's not quite working, it seems. Yeah. But I mean, it's very hard to figure out", "tokens": [50368, 281, 3566, 13, 400, 309, 311, 406, 1596, 1364, 11, 309, 2544, 13, 865, 13, 583, 286, 914, 11, 309, 311, 588, 1152, 281, 2573, 484, 50768], "temperature": 0.0, "avg_logprob": -0.1106703657852976, "compression_ratio": 1.5508021390374331, "no_speech_prob": 0.0051317280158400536}, {"id": 469, "seek": 312340, "start": 3131.48, "end": 3136.52, "text": " exactly how they're doing what they're doing. I mean, they've got so many parameters. And it's", "tokens": [50768, 2293, 577, 436, 434, 884, 437, 436, 434, 884, 13, 286, 914, 11, 436, 600, 658, 370, 867, 9834, 13, 400, 309, 311, 51020], "temperature": 0.0, "avg_logprob": -0.1106703657852976, "compression_ratio": 1.5508021390374331, "no_speech_prob": 0.0051317280158400536}, {"id": 470, "seek": 312340, "start": 3136.52, "end": 3146.36, "text": " surprising how good they are yet still imperfect at doing those sort of problems. They're kind of", "tokens": [51020, 8830, 577, 665, 436, 366, 1939, 920, 26714, 412, 884, 729, 1333, 295, 2740, 13, 814, 434, 733, 295, 51512], "temperature": 0.0, "avg_logprob": -0.1106703657852976, "compression_ratio": 1.5508021390374331, "no_speech_prob": 0.0051317280158400536}, {"id": 471, "seek": 314636, "start": 3146.44, "end": 3154.6, "text": " like a broken calculator. So they're much faster at getting to an answer, but it's not quite the", "tokens": [50368, 411, 257, 5463, 24993, 13, 407, 436, 434, 709, 4663, 412, 1242, 281, 364, 1867, 11, 457, 309, 311, 406, 1596, 264, 50776], "temperature": 0.0, "avg_logprob": -0.13676480401920366, "compression_ratio": 1.4720812182741116, "no_speech_prob": 0.10356719046831131}, {"id": 472, "seek": 314636, "start": 3154.6, "end": 3161.32, "text": " right answer. It's a pretty good estimate often. And yet it's not completely out. So it's really...", "tokens": [50776, 558, 1867, 13, 467, 311, 257, 1238, 665, 12539, 2049, 13, 400, 1939, 309, 311, 406, 2584, 484, 13, 407, 309, 311, 534, 485, 51112], "temperature": 0.0, "avg_logprob": -0.13676480401920366, "compression_ratio": 1.4720812182741116, "no_speech_prob": 0.10356719046831131}, {"id": 473, "seek": 314636, "start": 3163.1600000000003, "end": 3170.36, "text": " Yeah, I don't have a strong opinion, but part of me thinks, well, maybe they'll just kind of,", "tokens": [51204, 865, 11, 286, 500, 380, 362, 257, 2068, 4800, 11, 457, 644, 295, 385, 7309, 11, 731, 11, 1310, 436, 603, 445, 733, 295, 11, 51564], "temperature": 0.0, "avg_logprob": -0.13676480401920366, "compression_ratio": 1.4720812182741116, "no_speech_prob": 0.10356719046831131}, {"id": 474, "seek": 317036, "start": 3170.36, "end": 3177.08, "text": " with enough data going in, they might just crack that. That might come a point at which", "tokens": [50364, 365, 1547, 1412, 516, 294, 11, 436, 1062, 445, 6226, 300, 13, 663, 1062, 808, 257, 935, 412, 597, 50700], "temperature": 0.0, "avg_logprob": -0.12232792377471924, "compression_ratio": 1.6887755102040816, "no_speech_prob": 0.00668310234323144}, {"id": 475, "seek": 317036, "start": 3179.7200000000003, "end": 3184.84, "text": " that kind of ability emerges. Although you point out in one of your papers, though, well,", "tokens": [50832, 300, 733, 295, 3485, 38965, 13, 5780, 291, 935, 484, 294, 472, 295, 428, 10577, 11, 1673, 11, 731, 11, 51088], "temperature": 0.0, "avg_logprob": -0.12232792377471924, "compression_ratio": 1.6887755102040816, "no_speech_prob": 0.00668310234323144}, {"id": 476, "seek": 317036, "start": 3184.84, "end": 3187.96, "text": " perhaps if that ability emerges, it might be that a particular kind of", "tokens": [51088, 4317, 498, 300, 3485, 38965, 11, 309, 1062, 312, 300, 257, 1729, 733, 295, 51244], "temperature": 0.0, "avg_logprob": -0.12232792377471924, "compression_ratio": 1.6887755102040816, "no_speech_prob": 0.00668310234323144}, {"id": 477, "seek": 317036, "start": 3188.76, "end": 3194.44, "text": " architecture that models the human brain emerges as well. So it may not be that...", "tokens": [51284, 9482, 300, 5245, 264, 1952, 3567, 38965, 382, 731, 13, 407, 309, 815, 406, 312, 300, 485, 51568], "temperature": 0.0, "avg_logprob": -0.12232792377471924, "compression_ratio": 1.6887755102040816, "no_speech_prob": 0.00668310234323144}, {"id": 478, "seek": 319444, "start": 3194.68, "end": 3202.76, "text": " It might be happened in such a way that it becomes less fruitful to think of a large language", "tokens": [50376, 467, 1062, 312, 2011, 294, 1270, 257, 636, 300, 309, 3643, 1570, 49795, 281, 519, 295, 257, 2416, 2856, 50780], "temperature": 0.0, "avg_logprob": -0.1411218787684585, "compression_ratio": 1.548913043478261, "no_speech_prob": 0.01097618043422699}, {"id": 479, "seek": 319444, "start": 3203.48, "end": 3209.64, "text": " model as simply a model of language, but something that has a kind of linguistic language network", "tokens": [50816, 2316, 382, 2935, 257, 2316, 295, 2856, 11, 457, 746, 300, 575, 257, 733, 295, 43002, 2856, 3209, 51124], "temperature": 0.0, "avg_logprob": -0.1411218787684585, "compression_ratio": 1.548913043478261, "no_speech_prob": 0.01097618043422699}, {"id": 480, "seek": 319444, "start": 3209.64, "end": 3217.64, "text": " part like the human brain and then hands off to a logical part. And as it happens, obviously,", "tokens": [51124, 644, 411, 264, 1952, 3567, 293, 550, 2377, 766, 281, 257, 14978, 644, 13, 400, 382, 309, 2314, 11, 2745, 11, 51524], "temperature": 0.0, "avg_logprob": -0.1411218787684585, "compression_ratio": 1.548913043478261, "no_speech_prob": 0.01097618043422699}, {"id": 481, "seek": 321764, "start": 3217.96, "end": 3224.6, "text": " in chat GPT, without that has had that kind of architecture imposed on it, at least in the", "tokens": [50380, 294, 5081, 26039, 51, 11, 1553, 300, 575, 632, 300, 733, 295, 9482, 26491, 322, 309, 11, 412, 1935, 294, 264, 50712], "temperature": 0.0, "avg_logprob": -0.16887286890332945, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.18693600594997406}, {"id": 482, "seek": 321764, "start": 3224.6, "end": 3229.08, "text": " version with the Python code interpretation, for instance, because you can say, well, add these", "tokens": [50712, 3037, 365, 264, 15329, 3089, 14174, 11, 337, 5197, 11, 570, 291, 393, 584, 11, 731, 11, 909, 613, 50936], "temperature": 0.0, "avg_logprob": -0.16887286890332945, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.18693600594997406}, {"id": 483, "seek": 321764, "start": 3229.08, "end": 3232.3599999999997, "text": " two numbers together, and it will figure out, oh, well, I'm doing a math problem here. So I'm going", "tokens": [50936, 732, 3547, 1214, 11, 293, 309, 486, 2573, 484, 11, 1954, 11, 731, 11, 286, 478, 884, 257, 5221, 1154, 510, 13, 407, 286, 478, 516, 51100], "temperature": 0.0, "avg_logprob": -0.16887286890332945, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.18693600594997406}, {"id": 484, "seek": 321764, "start": 3232.3599999999997, "end": 3236.8399999999997, "text": " to convert this into a Python problem, and then it runs the Python code. So actually,", "tokens": [51100, 281, 7620, 341, 666, 257, 15329, 1154, 11, 293, 550, 309, 6676, 264, 15329, 3089, 13, 407, 767, 11, 51324], "temperature": 0.0, "avg_logprob": -0.16887286890332945, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.18693600594997406}, {"id": 485, "seek": 321764, "start": 3238.12, "end": 3240.7599999999998, "text": " you know, some of these problems seem to be", "tokens": [51388, 291, 458, 11, 512, 295, 613, 2740, 1643, 281, 312, 51520], "temperature": 0.0, "avg_logprob": -0.16887286890332945, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.18693600594997406}, {"id": 486, "seek": 324076, "start": 3241.0800000000004, "end": 3249.2400000000002, "text": " are being sort of addressed, I guess, by the developers. But the way they're doing it is,", "tokens": [50380, 366, 885, 1333, 295, 13847, 11, 286, 2041, 11, 538, 264, 8849, 13, 583, 264, 636, 436, 434, 884, 309, 307, 11, 50788], "temperature": 0.0, "avg_logprob": -0.26522945057262076, "compression_ratio": 1.3582089552238805, "no_speech_prob": 0.008832691237330437}, {"id": 487, "seek": 324076, "start": 3249.2400000000002, "end": 3257.48, "text": " yeah, offloading. Yeah. Yeah. So I guess let me unpack a little bit. There's a lot there. So", "tokens": [50788, 1338, 11, 766, 2907, 278, 13, 865, 13, 865, 13, 407, 286, 2041, 718, 385, 26699, 257, 707, 857, 13, 821, 311, 257, 688, 456, 13, 407, 51200], "temperature": 0.0, "avg_logprob": -0.26522945057262076, "compression_ratio": 1.3582089552238805, "no_speech_prob": 0.008832691237330437}, {"id": 488, "seek": 325748, "start": 3257.72, "end": 3270.76, "text": " first of all, it is very tempting for people to over ascribe intelligence to a language model.", "tokens": [50376, 700, 295, 439, 11, 309, 307, 588, 37900, 337, 561, 281, 670, 382, 8056, 7599, 281, 257, 2856, 2316, 13, 51028], "temperature": 0.0, "avg_logprob": -0.11142153124655446, "compression_ratio": 1.516304347826087, "no_speech_prob": 0.0021145218051970005}, {"id": 489, "seek": 325748, "start": 3271.4, "end": 3278.2, "text": " And presumably that's because in our everyday interactions, we're used that language gets", "tokens": [51060, 400, 26742, 300, 311, 570, 294, 527, 7429, 13280, 11, 321, 434, 1143, 300, 2856, 2170, 51400], "temperature": 0.0, "avg_logprob": -0.11142153124655446, "compression_ratio": 1.516304347826087, "no_speech_prob": 0.0021145218051970005}, {"id": 490, "seek": 325748, "start": 3278.2, "end": 3285.16, "text": " generated by a thinking feeling being other humans. And now we have a system which is breaking", "tokens": [51400, 10833, 538, 257, 1953, 2633, 885, 661, 6255, 13, 400, 586, 321, 362, 257, 1185, 597, 307, 7697, 51748], "temperature": 0.0, "avg_logprob": -0.11142153124655446, "compression_ratio": 1.516304347826087, "no_speech_prob": 0.0021145218051970005}, {"id": 491, "seek": 328516, "start": 3285.24, "end": 3290.52, "text": " that relationship where we have something that generates coherence language that's not human.", "tokens": [50368, 300, 2480, 689, 321, 362, 746, 300, 23815, 26528, 655, 2856, 300, 311, 406, 1952, 13, 50632], "temperature": 0.0, "avg_logprob": -0.19186431933672, "compression_ratio": 1.814070351758794, "no_speech_prob": 0.005298306699842215}, {"id": 492, "seek": 328516, "start": 3290.52, "end": 3298.68, "text": " And so it gets confusing. And that's the reason why that's one of the reasons why", "tokens": [50632, 400, 370, 309, 2170, 13181, 13, 400, 300, 311, 264, 1778, 983, 300, 311, 472, 295, 264, 4112, 983, 51040], "temperature": 0.0, "avg_logprob": -0.19186431933672, "compression_ratio": 1.814070351758794, "no_speech_prob": 0.005298306699842215}, {"id": 493, "seek": 328516, "start": 3298.68, "end": 3303.48, "text": " there's so much hyper-intelligent language models, and they're expected to be the general", "tokens": [51040, 456, 311, 370, 709, 9848, 12, 20761, 25002, 2856, 5245, 11, 293, 436, 434, 5176, 281, 312, 264, 2674, 51280], "temperature": 0.0, "avg_logprob": -0.19186431933672, "compression_ratio": 1.814070351758794, "no_speech_prob": 0.005298306699842215}, {"id": 494, "seek": 328516, "start": 3303.48, "end": 3310.2799999999997, "text": " intelligence models, because of this tight perceived relationship between language and thought.", "tokens": [51280, 7599, 5245, 11, 570, 295, 341, 4524, 19049, 2480, 1296, 2856, 293, 1194, 13, 51620], "temperature": 0.0, "avg_logprob": -0.19186431933672, "compression_ratio": 1.814070351758794, "no_speech_prob": 0.005298306699842215}, {"id": 495, "seek": 331028, "start": 3310.28, "end": 3315.4, "text": " And so then when they make a math mistake, or they make a factually inaccurate statement,", "tokens": [50364, 400, 370, 550, 562, 436, 652, 257, 5221, 6146, 11, 420, 436, 652, 257, 1186, 671, 46443, 5629, 11, 50620], "temperature": 0.0, "avg_logprob": -0.11720101810196071, "compression_ratio": 1.8347457627118644, "no_speech_prob": 0.0060914261266589165}, {"id": 496, "seek": 331028, "start": 3315.4, "end": 3319.2400000000002, "text": " you're like, oh, no, like how, you know, these models are terrible, they're not", "tokens": [50620, 291, 434, 411, 11, 1954, 11, 572, 11, 411, 577, 11, 291, 458, 11, 613, 5245, 366, 6237, 11, 436, 434, 406, 50812], "temperature": 0.0, "avg_logprob": -0.11720101810196071, "compression_ratio": 1.8347457627118644, "no_speech_prob": 0.0060914261266589165}, {"id": 497, "seek": 331028, "start": 3319.2400000000002, "end": 3323.7200000000003, "text": " terrible, they're just like not, they're just a totally different capacity you're evaluating.", "tokens": [50812, 6237, 11, 436, 434, 445, 411, 406, 11, 436, 434, 445, 257, 3879, 819, 6042, 291, 434, 27479, 13, 51036], "temperature": 0.0, "avg_logprob": -0.11720101810196071, "compression_ratio": 1.8347457627118644, "no_speech_prob": 0.0060914261266589165}, {"id": 498, "seek": 331028, "start": 3323.7200000000003, "end": 3331.8, "text": " And so what we argue is that it's very important to distinguish different kinds of", "tokens": [51036, 400, 370, 437, 321, 9695, 307, 300, 309, 311, 588, 1021, 281, 20206, 819, 3685, 295, 51440], "temperature": 0.0, "avg_logprob": -0.11720101810196071, "compression_ratio": 1.8347457627118644, "no_speech_prob": 0.0060914261266589165}, {"id": 499, "seek": 331028, "start": 3331.8, "end": 3338.84, "text": " capabilities in these models. And so there is something that we call formal linguistic", "tokens": [51440, 10862, 294, 613, 5245, 13, 400, 370, 456, 307, 746, 300, 321, 818, 9860, 43002, 51792], "temperature": 0.0, "avg_logprob": -0.11720101810196071, "compression_ratio": 1.8347457627118644, "no_speech_prob": 0.0060914261266589165}, {"id": 500, "seek": 333884, "start": 3338.92, "end": 3344.6000000000004, "text": " competence. And so that's the ability to generate coherent grammatical language.", "tokens": [50368, 39965, 13, 400, 370, 300, 311, 264, 3485, 281, 8460, 36239, 17570, 267, 804, 2856, 13, 50652], "temperature": 0.0, "avg_logprob": -0.07666619618733723, "compression_ratio": 1.6490384615384615, "no_speech_prob": 0.0007205913425423205}, {"id": 501, "seek": 333884, "start": 3344.6000000000004, "end": 3350.44, "text": " And that's something that in humans, the language brain network is responsible for.", "tokens": [50652, 400, 300, 311, 746, 300, 294, 6255, 11, 264, 2856, 3567, 3209, 307, 6250, 337, 13, 50944], "temperature": 0.0, "avg_logprob": -0.07666619618733723, "compression_ratio": 1.6490384615384615, "no_speech_prob": 0.0007205913425423205}, {"id": 502, "seek": 333884, "start": 3351.56, "end": 3358.36, "text": " And then there is all of the other stuff that you need in order to actually use language in", "tokens": [51000, 400, 550, 456, 307, 439, 295, 264, 661, 1507, 300, 291, 643, 294, 1668, 281, 767, 764, 2856, 294, 51340], "temperature": 0.0, "avg_logprob": -0.07666619618733723, "compression_ratio": 1.6490384615384615, "no_speech_prob": 0.0007205913425423205}, {"id": 503, "seek": 333884, "start": 3358.36, "end": 3364.6000000000004, "text": " real life situation in interactions, you might want to ask somebody to close the door,", "tokens": [51340, 957, 993, 2590, 294, 13280, 11, 291, 1062, 528, 281, 1029, 2618, 281, 1998, 264, 2853, 11, 51652], "temperature": 0.0, "avg_logprob": -0.07666619618733723, "compression_ratio": 1.6490384615384615, "no_speech_prob": 0.0007205913425423205}, {"id": 504, "seek": 336460, "start": 3364.6, "end": 3371.64, "text": " you might want to tell somebody how you feel. And there are all kinds of situations that", "tokens": [50364, 291, 1062, 528, 281, 980, 2618, 577, 291, 841, 13, 400, 456, 366, 439, 3685, 295, 6851, 300, 50716], "temperature": 0.0, "avg_logprob": -0.09500098455519904, "compression_ratio": 1.8925619834710743, "no_speech_prob": 0.005055202636867762}, {"id": 505, "seek": 336460, "start": 3371.64, "end": 3378.36, "text": " where you need to use language. But to do that, you actually need other capabilities,", "tokens": [50716, 689, 291, 643, 281, 764, 2856, 13, 583, 281, 360, 300, 11, 291, 767, 643, 661, 10862, 11, 51052], "temperature": 0.0, "avg_logprob": -0.09500098455519904, "compression_ratio": 1.8925619834710743, "no_speech_prob": 0.005055202636867762}, {"id": 506, "seek": 336460, "start": 3378.36, "end": 3383.72, "text": " you need to be able to reason about social situations, you need to be able to know things", "tokens": [51052, 291, 643, 281, 312, 1075, 281, 1778, 466, 2093, 6851, 11, 291, 643, 281, 312, 1075, 281, 458, 721, 51320], "temperature": 0.0, "avg_logprob": -0.09500098455519904, "compression_ratio": 1.8925619834710743, "no_speech_prob": 0.005055202636867762}, {"id": 507, "seek": 336460, "start": 3383.72, "end": 3388.7599999999998, "text": " about the world in order to generate actually accurate statements, you need to be able to reason", "tokens": [51320, 466, 264, 1002, 294, 1668, 281, 8460, 767, 8559, 12363, 11, 291, 643, 281, 312, 1075, 281, 1778, 51572], "temperature": 0.0, "avg_logprob": -0.09500098455519904, "compression_ratio": 1.8925619834710743, "no_speech_prob": 0.005055202636867762}, {"id": 508, "seek": 336460, "start": 3388.7599999999998, "end": 3393.64, "text": " logically and know some math if you want to solve a math problem. So even if that information is", "tokens": [51572, 38887, 293, 458, 512, 5221, 498, 291, 528, 281, 5039, 257, 5221, 1154, 13, 407, 754, 498, 300, 1589, 307, 51816], "temperature": 0.0, "avg_logprob": -0.09500098455519904, "compression_ratio": 1.8925619834710743, "no_speech_prob": 0.005055202636867762}, {"id": 509, "seek": 339364, "start": 3393.64, "end": 3399.24, "text": " coming in as language, in order to be able to make sense of it, and also generate language that", "tokens": [50364, 1348, 294, 382, 2856, 11, 294, 1668, 281, 312, 1075, 281, 652, 2020, 295, 309, 11, 293, 611, 8460, 2856, 300, 50644], "temperature": 0.0, "avg_logprob": -0.07351867745562297, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.0005273604765534401}, {"id": 510, "seek": 339364, "start": 3399.24, "end": 3405.16, "text": " achieves a particular purpose, you need all of these other capacities, which broadly speaking,", "tokens": [50644, 3538, 977, 257, 1729, 4334, 11, 291, 643, 439, 295, 613, 661, 39396, 11, 597, 19511, 4124, 11, 50940], "temperature": 0.0, "avg_logprob": -0.07351867745562297, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.0005273604765534401}, {"id": 511, "seek": 339364, "start": 3405.16, "end": 3413.08, "text": " recall functional competence. And so different kinds of capabilities might suffer from different", "tokens": [50940, 9901, 11745, 39965, 13, 400, 370, 819, 3685, 295, 10862, 1062, 9753, 490, 819, 51336], "temperature": 0.0, "avg_logprob": -0.07351867745562297, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.0005273604765534401}, {"id": 512, "seek": 339364, "start": 3413.08, "end": 3420.12, "text": " problems. And so we already touched upon a few. We touched upon the fact that mathematical reasoning", "tokens": [51336, 2740, 13, 400, 370, 321, 1217, 9828, 3564, 257, 1326, 13, 492, 9828, 3564, 264, 1186, 300, 18894, 21577, 51688], "temperature": 0.0, "avg_logprob": -0.07351867745562297, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.0005273604765534401}, {"id": 513, "seek": 342012, "start": 3420.12, "end": 3426.7599999999998, "text": " and logical reasoning might require a different kind of algorithm. So instead of pattern matching,", "tokens": [50364, 293, 14978, 21577, 1062, 3651, 257, 819, 733, 295, 9284, 13, 407, 2602, 295, 5102, 14324, 11, 50696], "temperature": 0.0, "avg_logprob": -0.09111755393272222, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0068970052525401115}, {"id": 514, "seek": 342012, "start": 3426.7599999999998, "end": 3432.6, "text": " it might need to be more symbolic. And it's not fully clear whether the large language models", "tokens": [50696, 309, 1062, 643, 281, 312, 544, 25755, 13, 400, 309, 311, 406, 4498, 1850, 1968, 264, 2416, 2856, 5245, 50988], "temperature": 0.0, "avg_logprob": -0.09111755393272222, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0068970052525401115}, {"id": 515, "seek": 342012, "start": 3432.6, "end": 3441.56, "text": " today are capable of doing that. Maybe they are. But that's not necessarily in their default,", "tokens": [50988, 965, 366, 8189, 295, 884, 300, 13, 2704, 436, 366, 13, 583, 300, 311, 406, 4725, 294, 641, 7576, 11, 51436], "temperature": 0.0, "avg_logprob": -0.09111755393272222, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0068970052525401115}, {"id": 516, "seek": 342012, "start": 3442.3599999999997, "end": 3447.48, "text": " in the default way they operate. So that's an open debate there. When it comes to world", "tokens": [51476, 294, 264, 7576, 636, 436, 9651, 13, 407, 300, 311, 364, 1269, 7958, 456, 13, 1133, 309, 1487, 281, 1002, 51732], "temperature": 0.0, "avg_logprob": -0.09111755393272222, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0068970052525401115}, {"id": 517, "seek": 344748, "start": 3447.48, "end": 3452.44, "text": " knowledge and knowing things about the world, distinguish implausible and implausible events,", "tokens": [50364, 3601, 293, 5276, 721, 466, 264, 1002, 11, 20206, 8484, 8463, 964, 293, 8484, 8463, 964, 3931, 11, 50612], "temperature": 0.0, "avg_logprob": -0.08726085316051137, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.003763066604733467}, {"id": 518, "seek": 344748, "start": 3452.44, "end": 3459.4, "text": " there a big problem is reporting bias and the fact that the training data that they have is biased.", "tokens": [50612, 456, 257, 955, 1154, 307, 10031, 12577, 293, 264, 1186, 300, 264, 3097, 1412, 300, 436, 362, 307, 28035, 13, 50960], "temperature": 0.0, "avg_logprob": -0.08726085316051137, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.003763066604733467}, {"id": 519, "seek": 344748, "start": 3459.4, "end": 3466.04, "text": " And so you might need to be able to build up a more general situation model, event model,", "tokens": [50960, 400, 370, 291, 1062, 643, 281, 312, 1075, 281, 1322, 493, 257, 544, 2674, 2590, 2316, 11, 2280, 2316, 11, 51292], "temperature": 0.0, "avg_logprob": -0.08726085316051137, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.003763066604733467}, {"id": 520, "seek": 344748, "start": 3466.04, "end": 3471.64, "text": " that will not just take in the language that you receive, but also fill in some kind of commonly", "tokens": [51292, 300, 486, 406, 445, 747, 294, 264, 2856, 300, 291, 4774, 11, 457, 611, 2836, 294, 512, 733, 295, 12719, 51572], "temperature": 0.0, "avg_logprob": -0.08726085316051137, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.003763066604733467}, {"id": 521, "seek": 347164, "start": 3471.64, "end": 3480.44, "text": " assumed things. If it's daytime, it slides out, stuff like that. And yeah, so different kinds", "tokens": [50364, 15895, 721, 13, 759, 309, 311, 31908, 11, 309, 9788, 484, 11, 1507, 411, 300, 13, 400, 1338, 11, 370, 819, 3685, 50804], "temperature": 0.0, "avg_logprob": -0.15405218528978754, "compression_ratio": 1.5792349726775956, "no_speech_prob": 0.007009205874055624}, {"id": 522, "seek": 347164, "start": 3480.44, "end": 3487.16, "text": " of problems might require different kinds of solutions. A more general kind of potential solution", "tokens": [50804, 295, 2740, 1062, 3651, 819, 3685, 295, 6547, 13, 316, 544, 2674, 733, 295, 3995, 3827, 51140], "temperature": 0.0, "avg_logprob": -0.15405218528978754, "compression_ratio": 1.5792349726775956, "no_speech_prob": 0.007009205874055624}, {"id": 523, "seek": 347164, "start": 3487.16, "end": 3498.6, "text": " that we advocate or talk about is modularity. So the fact that the brain is modular suggests that", "tokens": [51140, 300, 321, 14608, 420, 751, 466, 307, 31111, 507, 13, 407, 264, 1186, 300, 264, 3567, 307, 31111, 13409, 300, 51712], "temperature": 0.0, "avg_logprob": -0.15405218528978754, "compression_ratio": 1.5792349726775956, "no_speech_prob": 0.007009205874055624}, {"id": 524, "seek": 349860, "start": 3498.92, "end": 3505.48, "text": " might be an efficient architecture. So a language process in module, the goal of the language", "tokens": [50380, 1062, 312, 364, 7148, 9482, 13, 407, 257, 2856, 1399, 294, 10088, 11, 264, 3387, 295, 264, 2856, 50708], "temperature": 0.0, "avg_logprob": -0.11660135057237413, "compression_ratio": 1.613733905579399, "no_speech_prob": 0.0031199392396956682}, {"id": 525, "seek": 349860, "start": 3505.48, "end": 3513.08, "text": " network in the brain is not to reason, it's to get information that's expressed in fuzzy,", "tokens": [50708, 3209, 294, 264, 3567, 307, 406, 281, 1778, 11, 309, 311, 281, 483, 1589, 300, 311, 12675, 294, 34710, 11, 51088], "temperature": 0.0, "avg_logprob": -0.11660135057237413, "compression_ratio": 1.613733905579399, "no_speech_prob": 0.0031199392396956682}, {"id": 526, "seek": 349860, "start": 3513.08, "end": 3519.24, "text": " imprecise words and extract meaning out of it. And then pass it on to relevant systems that can", "tokens": [51088, 704, 13867, 908, 2283, 293, 8947, 3620, 484, 295, 309, 13, 400, 550, 1320, 309, 322, 281, 7340, 3652, 300, 393, 51396], "temperature": 0.0, "avg_logprob": -0.11660135057237413, "compression_ratio": 1.613733905579399, "no_speech_prob": 0.0031199392396956682}, {"id": 527, "seek": 349860, "start": 3519.24, "end": 3527.16, "text": " solve the math problem that can infer the social goal, all of that stuff. And presumably, for an", "tokens": [51396, 5039, 264, 5221, 1154, 300, 393, 13596, 264, 2093, 3387, 11, 439, 295, 300, 1507, 13, 400, 26742, 11, 337, 364, 51792], "temperature": 0.0, "avg_logprob": -0.11660135057237413, "compression_ratio": 1.613733905579399, "no_speech_prob": 0.0031199392396956682}, {"id": 528, "seek": 352716, "start": 3527.24, "end": 3532.12, "text": " artificial intelligence system, you might want to do something similar where language", "tokens": [50368, 11677, 7599, 1185, 11, 291, 1062, 528, 281, 360, 746, 2531, 689, 2856, 50612], "temperature": 0.0, "avg_logprob": -0.09540512011601375, "compression_ratio": 1.6366906474820144, "no_speech_prob": 0.0019560998771339655}, {"id": 529, "seek": 352716, "start": 3532.12, "end": 3538.7599999999998, "text": " is not a replacement for thought, but is an interface to thought. And so in your example,", "tokens": [50612, 307, 406, 257, 14419, 337, 1194, 11, 457, 307, 364, 9226, 281, 1194, 13, 400, 370, 294, 428, 1365, 11, 50944], "temperature": 0.0, "avg_logprob": -0.09540512011601375, "compression_ratio": 1.6366906474820144, "no_speech_prob": 0.0019560998771339655}, {"id": 530, "seek": 352716, "start": 3538.7599999999998, "end": 3545.3199999999997, "text": " right, you have a math problem, the language model translates it into code. It's very good at", "tokens": [50944, 558, 11, 291, 362, 257, 5221, 1154, 11, 264, 2856, 2316, 28468, 309, 666, 3089, 13, 467, 311, 588, 665, 412, 51272], "temperature": 0.0, "avg_logprob": -0.09540512011601375, "compression_ratio": 1.6366906474820144, "no_speech_prob": 0.0019560998771339655}, {"id": 531, "seek": 352716, "start": 3545.3199999999997, "end": 3550.2799999999997, "text": " taking this broad like fuzzy natural language and translating into a more precise, symbolic", "tokens": [51272, 1940, 341, 4152, 411, 34710, 3303, 2856, 293, 35030, 666, 257, 544, 13600, 11, 25755, 51520], "temperature": 0.0, "avg_logprob": -0.09540512011601375, "compression_ratio": 1.6366906474820144, "no_speech_prob": 0.0019560998771339655}, {"id": 532, "seek": 352716, "start": 3550.2799999999997, "end": 3554.6, "text": " representation. That's something that we didn't have at all, even a few years back. So it's a", "tokens": [51520, 10290, 13, 663, 311, 746, 300, 321, 994, 380, 362, 412, 439, 11, 754, 257, 1326, 924, 646, 13, 407, 309, 311, 257, 51736], "temperature": 0.0, "avg_logprob": -0.09540512011601375, "compression_ratio": 1.6366906474820144, "no_speech_prob": 0.0019560998771339655}, {"id": 533, "seek": 355460, "start": 3554.6, "end": 3560.2799999999997, "text": " huge achievement. But then instead of trying to have that same language model to run the code,", "tokens": [50364, 2603, 15838, 13, 583, 550, 2602, 295, 1382, 281, 362, 300, 912, 2856, 2316, 281, 1190, 264, 3089, 11, 50648], "temperature": 0.0, "avg_logprob": -0.0888192955104784, "compression_ratio": 1.6576576576576576, "no_speech_prob": 0.0071112350560724735}, {"id": 534, "seek": 355460, "start": 3560.2799999999997, "end": 3566.68, "text": " you're much better off passing it off to a code interpreter that will run the code and give you", "tokens": [50648, 291, 434, 709, 1101, 766, 8437, 309, 766, 281, 257, 3089, 34132, 300, 486, 1190, 264, 3089, 293, 976, 291, 50968], "temperature": 0.0, "avg_logprob": -0.0888192955104784, "compression_ratio": 1.6576576576576576, "no_speech_prob": 0.0071112350560724735}, {"id": 535, "seek": 355460, "start": 3566.68, "end": 3572.7599999999998, "text": " the answer. So the same kind of modularity that we see in the brain, that seems to be an effective", "tokens": [50968, 264, 1867, 13, 407, 264, 912, 733, 295, 31111, 507, 300, 321, 536, 294, 264, 3567, 11, 300, 2544, 281, 312, 364, 4942, 51272], "temperature": 0.0, "avg_logprob": -0.0888192955104784, "compression_ratio": 1.6576576576576576, "no_speech_prob": 0.0071112350560724735}, {"id": 536, "seek": 355460, "start": 3572.7599999999998, "end": 3578.2799999999997, "text": " way forward in the AI world that indeed some developers have started to adopt.", "tokens": [51272, 636, 2128, 294, 264, 7318, 1002, 300, 6451, 512, 8849, 362, 1409, 281, 6878, 13, 51548], "temperature": 0.0, "avg_logprob": -0.0888192955104784, "compression_ratio": 1.6576576576576576, "no_speech_prob": 0.0071112350560724735}, {"id": 537, "seek": 357828, "start": 3579.2400000000002, "end": 3587.2400000000002, "text": " Yeah. Yeah. And I think there's probably other ways in which the builders of these tools are trying", "tokens": [50412, 865, 13, 865, 13, 400, 286, 519, 456, 311, 1391, 661, 2098, 294, 597, 264, 36281, 295, 613, 3873, 366, 1382, 50812], "temperature": 0.0, "avg_logprob": -0.14234457517925062, "compression_ratio": 1.5582329317269077, "no_speech_prob": 0.006898775696754456}, {"id": 538, "seek": 357828, "start": 3587.2400000000002, "end": 3593.32, "text": " to modularize. Like another one that comes up a lot is Rang or retrieval augmented generation, where", "tokens": [50812, 281, 31111, 1125, 13, 1743, 1071, 472, 300, 1487, 493, 257, 688, 307, 497, 656, 420, 19817, 3337, 36155, 5125, 11, 689, 51116], "temperature": 0.0, "avg_logprob": -0.14234457517925062, "compression_ratio": 1.5582329317269077, "no_speech_prob": 0.006898775696754456}, {"id": 539, "seek": 357828, "start": 3595.0800000000004, "end": 3600.6000000000004, "text": " yeah, there's some kind of database or just could just be a whole bunch of, you know,", "tokens": [51204, 1338, 11, 456, 311, 512, 733, 295, 8149, 420, 445, 727, 445, 312, 257, 1379, 3840, 295, 11, 291, 458, 11, 51480], "temperature": 0.0, "avg_logprob": -0.14234457517925062, "compression_ratio": 1.5582329317269077, "no_speech_prob": 0.006898775696754456}, {"id": 540, "seek": 357828, "start": 3600.6000000000004, "end": 3607.0800000000004, "text": " documents or whatever. And instead of hallucinating an answer, you want to make sure that you pick up", "tokens": [51480, 8512, 420, 2035, 13, 400, 2602, 295, 35212, 8205, 364, 1867, 11, 291, 528, 281, 652, 988, 300, 291, 1888, 493, 51804], "temperature": 0.0, "avg_logprob": -0.14234457517925062, "compression_ratio": 1.5582329317269077, "no_speech_prob": 0.006898775696754456}, {"id": 541, "seek": 360708, "start": 3607.7999999999997, "end": 3614.2799999999997, "text": " something from one of those documents. And there's a whole different kind of machinery for that.", "tokens": [50400, 746, 490, 472, 295, 729, 8512, 13, 400, 456, 311, 257, 1379, 819, 733, 295, 27302, 337, 300, 13, 50724], "temperature": 0.0, "avg_logprob": -0.13765382766723633, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.0018095156410709023}, {"id": 542, "seek": 360708, "start": 3614.2799999999997, "end": 3621.16, "text": " But again, like in the code interpreter example, it's, I guess the language part is", "tokens": [50724, 583, 797, 11, 411, 294, 264, 3089, 34132, 1365, 11, 309, 311, 11, 286, 2041, 264, 2856, 644, 307, 51068], "temperature": 0.0, "avg_logprob": -0.13765382766723633, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.0018095156410709023}, {"id": 543, "seek": 360708, "start": 3622.44, "end": 3627.56, "text": " key, maybe less key in Rang because it's kind of a vector search. But it's a way, you know,", "tokens": [51132, 2141, 11, 1310, 1570, 2141, 294, 497, 656, 570, 309, 311, 733, 295, 257, 8062, 3164, 13, 583, 309, 311, 257, 636, 11, 291, 458, 11, 51388], "temperature": 0.0, "avg_logprob": -0.13765382766723633, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.0018095156410709023}, {"id": 544, "seek": 360708, "start": 3627.56, "end": 3633.56, "text": " it begins with translating language into something a bit more precise, in this case a vector instead", "tokens": [51388, 309, 7338, 365, 35030, 2856, 666, 746, 257, 857, 544, 13600, 11, 294, 341, 1389, 257, 8062, 2602, 51688], "temperature": 0.0, "avg_logprob": -0.13765382766723633, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.0018095156410709023}, {"id": 545, "seek": 363356, "start": 3634.12, "end": 3641.64, "text": " of some code, I guess. And yeah, one wonders then if, you know, how close the parallels are between", "tokens": [50392, 295, 512, 3089, 11, 286, 2041, 13, 400, 1338, 11, 472, 27348, 550, 498, 11, 291, 458, 11, 577, 1998, 264, 44223, 366, 1296, 50768], "temperature": 0.0, "avg_logprob": -0.1153640539749809, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.0019843399059027433}, {"id": 546, "seek": 363356, "start": 3642.68, "end": 3648.6, "text": " what is being built here and what's going on in the brain. You mentioned that, yeah,", "tokens": [50820, 437, 307, 885, 3094, 510, 293, 437, 311, 516, 322, 294, 264, 3567, 13, 509, 2835, 300, 11, 1338, 11, 51116], "temperature": 0.0, "avg_logprob": -0.1153640539749809, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.0019843399059027433}, {"id": 547, "seek": 363356, "start": 3649.24, "end": 3655.0, "text": " perhaps this is a good model for thinking about how we think. Language is this part where,", "tokens": [51148, 4317, 341, 307, 257, 665, 2316, 337, 1953, 466, 577, 321, 519, 13, 24445, 307, 341, 644, 689, 11, 51436], "temperature": 0.0, "avg_logprob": -0.1153640539749809, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.0019843399059027433}, {"id": 548, "seek": 363356, "start": 3655.64, "end": 3662.2799999999997, "text": " this place where things kind of, you know, entry point for concepts, but the places where", "tokens": [51468, 341, 1081, 689, 721, 733, 295, 11, 291, 458, 11, 8729, 935, 337, 10392, 11, 457, 264, 3190, 689, 51800], "temperature": 0.0, "avg_logprob": -0.1153640539749809, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.0019843399059027433}, {"id": 549, "seek": 366228, "start": 3662.36, "end": 3668.0400000000004, "text": " those concepts often get manipulated in terms of reasoning might be in other areas of the", "tokens": [50368, 729, 10392, 2049, 483, 37161, 294, 2115, 295, 21577, 1062, 312, 294, 661, 3179, 295, 264, 50652], "temperature": 0.0, "avg_logprob": -0.15119066372723647, "compression_ratio": 1.599009900990099, "no_speech_prob": 0.0006448994390666485}, {"id": 550, "seek": 366228, "start": 3668.0400000000004, "end": 3672.6800000000003, "text": " brain. They sort of become something more abstract than language itself.", "tokens": [50652, 3567, 13, 814, 1333, 295, 1813, 746, 544, 12649, 813, 2856, 2564, 13, 50884], "temperature": 0.0, "avg_logprob": -0.15119066372723647, "compression_ratio": 1.599009900990099, "no_speech_prob": 0.0006448994390666485}, {"id": 551, "seek": 366228, "start": 3676.2000000000003, "end": 3682.52, "text": " Yeah. Yeah. One thing I actually just slight tangent, but I do sometimes think that", "tokens": [51060, 865, 13, 865, 13, 1485, 551, 286, 767, 445, 4036, 27747, 11, 457, 286, 360, 2171, 519, 300, 51376], "temperature": 0.0, "avg_logprob": -0.15119066372723647, "compression_ratio": 1.599009900990099, "no_speech_prob": 0.0006448994390666485}, {"id": 552, "seek": 366228, "start": 3684.44, "end": 3689.8, "text": " maybe language is being so associated with thought because it's kind of like", "tokens": [51472, 1310, 2856, 307, 885, 370, 6615, 365, 1194, 570, 309, 311, 733, 295, 411, 51740], "temperature": 0.0, "avg_logprob": -0.15119066372723647, "compression_ratio": 1.599009900990099, "no_speech_prob": 0.0006448994390666485}, {"id": 553, "seek": 368980, "start": 3690.76, "end": 3696.76, "text": " the easiest thing to do, right? Like, you know, we know thinking is about concepts and some,", "tokens": [50412, 264, 12889, 551, 281, 360, 11, 558, 30, 1743, 11, 291, 458, 11, 321, 458, 1953, 307, 466, 10392, 293, 512, 11, 50712], "temperature": 0.0, "avg_logprob": -0.09813308715820312, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.0003914008557330817}, {"id": 554, "seek": 368980, "start": 3697.32, "end": 3703.88, "text": " you know, manipulating these things which are representations of the world. And language is", "tokens": [50740, 291, 458, 11, 40805, 613, 721, 597, 366, 33358, 295, 264, 1002, 13, 400, 2856, 307, 51068], "temperature": 0.0, "avg_logprob": -0.09813308715820312, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.0003914008557330817}, {"id": 555, "seek": 368980, "start": 3703.88, "end": 3709.1600000000003, "text": " just such an easy way of visualizing all of that, right, and understanding what's going on. But", "tokens": [51068, 445, 1270, 364, 1858, 636, 295, 5056, 3319, 439, 295, 300, 11, 558, 11, 293, 3701, 437, 311, 516, 322, 13, 583, 51332], "temperature": 0.0, "avg_logprob": -0.09813308715820312, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.0003914008557330817}, {"id": 556, "seek": 368980, "start": 3709.1600000000003, "end": 3716.92, "text": " perhaps it's just the surface level of something much deeper that we really don't have an easy way", "tokens": [51332, 4317, 309, 311, 445, 264, 3753, 1496, 295, 746, 709, 7731, 300, 321, 534, 500, 380, 362, 364, 1858, 636, 51720], "temperature": 0.0, "avg_logprob": -0.09813308715820312, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.0003914008557330817}, {"id": 557, "seek": 371692, "start": 3717.0, "end": 3724.04, "text": " of capturing. And, you know, that would map, I think, quite well to this kind of model of", "tokens": [50368, 295, 23384, 13, 400, 11, 291, 458, 11, 300, 576, 4471, 11, 286, 519, 11, 1596, 731, 281, 341, 733, 295, 2316, 295, 50720], "temperature": 0.0, "avg_logprob": -0.13188316655713458, "compression_ratio": 1.600896860986547, "no_speech_prob": 0.0004171555337961763}, {"id": 558, "seek": 371692, "start": 3724.6800000000003, "end": 3729.96, "text": " concepts being passed around, but the concepts themselves being, you know, beyond linguistics", "tokens": [50752, 10392, 885, 4678, 926, 11, 457, 264, 10392, 2969, 885, 11, 291, 458, 11, 4399, 21766, 6006, 51016], "temperature": 0.0, "avg_logprob": -0.13188316655713458, "compression_ratio": 1.600896860986547, "no_speech_prob": 0.0004171555337961763}, {"id": 559, "seek": 371692, "start": 3729.96, "end": 3738.76, "text": " somehow. Yeah. So, as we mentioned, language is a system designed to communicate thoughts,", "tokens": [51016, 6063, 13, 865, 13, 407, 11, 382, 321, 2835, 11, 2856, 307, 257, 1185, 4761, 281, 7890, 4598, 11, 51456], "temperature": 0.0, "avg_logprob": -0.13188316655713458, "compression_ratio": 1.600896860986547, "no_speech_prob": 0.0004171555337961763}, {"id": 560, "seek": 371692, "start": 3739.32, "end": 3745.32, "text": " concepts from one mind to another. And so, for this communication to be efficient,", "tokens": [51484, 10392, 490, 472, 1575, 281, 1071, 13, 400, 370, 11, 337, 341, 6101, 281, 312, 7148, 11, 51784], "temperature": 0.0, "avg_logprob": -0.13188316655713458, "compression_ratio": 1.600896860986547, "no_speech_prob": 0.0004171555337961763}, {"id": 561, "seek": 374532, "start": 3745.32, "end": 3750.76, "text": " presumably language needs to parallel the structure of thought, the structure of concepts", "tokens": [50364, 26742, 2856, 2203, 281, 8952, 264, 3877, 295, 1194, 11, 264, 3877, 295, 10392, 50636], "temperature": 0.0, "avg_logprob": -0.08509312878857862, "compression_ratio": 1.841897233201581, "no_speech_prob": 0.0004170773026999086}, {"id": 562, "seek": 374532, "start": 3750.76, "end": 3757.32, "text": " in some way, right? And so, it's much more abstract already than the raw perceptual input,", "tokens": [50636, 294, 512, 636, 11, 558, 30, 400, 370, 11, 309, 311, 709, 544, 12649, 1217, 813, 264, 8936, 43276, 901, 4846, 11, 50964], "temperature": 0.0, "avg_logprob": -0.08509312878857862, "compression_ratio": 1.841897233201581, "no_speech_prob": 0.0004170773026999086}, {"id": 563, "seek": 374532, "start": 3757.32, "end": 3763.8, "text": " than just audio, than just pictures, right? So, it kind of captures the relevant abstractions to", "tokens": [50964, 813, 445, 6278, 11, 813, 445, 5242, 11, 558, 30, 407, 11, 309, 733, 295, 27986, 264, 7340, 12649, 626, 281, 51288], "temperature": 0.0, "avg_logprob": -0.08509312878857862, "compression_ratio": 1.841897233201581, "no_speech_prob": 0.0004170773026999086}, {"id": 564, "seek": 374532, "start": 3763.8, "end": 3770.2000000000003, "text": " a large extent. And so, that seems to be helping a lot. And so, that does bring us much closer to", "tokens": [51288, 257, 2416, 8396, 13, 400, 370, 11, 300, 2544, 281, 312, 4315, 257, 688, 13, 400, 370, 11, 300, 775, 1565, 505, 709, 4966, 281, 51608], "temperature": 0.0, "avg_logprob": -0.08509312878857862, "compression_ratio": 1.841897233201581, "no_speech_prob": 0.0004170773026999086}, {"id": 565, "seek": 374532, "start": 3771.0, "end": 3774.92, "text": " this more abstract conceptual representation. We're getting rid of a lot of extra details,", "tokens": [51648, 341, 544, 12649, 24106, 10290, 13, 492, 434, 1242, 3973, 295, 257, 688, 295, 2857, 4365, 11, 51844], "temperature": 0.0, "avg_logprob": -0.08509312878857862, "compression_ratio": 1.841897233201581, "no_speech_prob": 0.0004170773026999086}, {"id": 566, "seek": 377492, "start": 3774.92, "end": 3781.0, "text": " we say cat, we don't care which color, which size is the cat. But, of course, at mapping", "tokens": [50364, 321, 584, 3857, 11, 321, 500, 380, 1127, 597, 2017, 11, 597, 2744, 307, 264, 3857, 13, 583, 11, 295, 1164, 11, 412, 18350, 50668], "temperature": 0.0, "avg_logprob": -0.11884260177612305, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.000444012664956972}, {"id": 567, "seek": 377492, "start": 3781.0, "end": 3785.7200000000003, "text": " between concepts and languages imprecise, we know that different languages partition the", "tokens": [50668, 1296, 10392, 293, 8650, 704, 13867, 908, 11, 321, 458, 300, 819, 8650, 24808, 264, 50904], "temperature": 0.0, "avg_logprob": -0.11884260177612305, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.000444012664956972}, {"id": 568, "seek": 377492, "start": 3785.7200000000003, "end": 3791.64, "text": " conceptual space in different ways, right? So, the words don't necessarily map the concepts one", "tokens": [50904, 24106, 1901, 294, 819, 2098, 11, 558, 30, 407, 11, 264, 2283, 500, 380, 4725, 4471, 264, 10392, 472, 51200], "temperature": 0.0, "avg_logprob": -0.11884260177612305, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.000444012664956972}, {"id": 569, "seek": 377492, "start": 3791.64, "end": 3797.8, "text": " and one. Even within the same language, the same word can be used in many different contexts,", "tokens": [51200, 293, 472, 13, 2754, 1951, 264, 912, 2856, 11, 264, 912, 1349, 393, 312, 1143, 294, 867, 819, 30628, 11, 51508], "temperature": 0.0, "avg_logprob": -0.11884260177612305, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.000444012664956972}, {"id": 570, "seek": 379780, "start": 3797.88, "end": 3805.7200000000003, "text": " in different ways, with different meanings. And so, that link is pretty fuzzy, can get pretty", "tokens": [50368, 294, 819, 2098, 11, 365, 819, 28138, 13, 400, 370, 11, 300, 2113, 307, 1238, 34710, 11, 393, 483, 1238, 50760], "temperature": 0.0, "avg_logprob": -0.13278289676941546, "compression_ratio": 1.5560344827586208, "no_speech_prob": 0.010645574890077114}, {"id": 571, "seek": 379780, "start": 3805.7200000000003, "end": 3811.5600000000004, "text": " fuzzy. But it's definitely, I think you're right, when it comes to raw surface form,", "tokens": [50760, 34710, 13, 583, 309, 311, 2138, 11, 286, 519, 291, 434, 558, 11, 562, 309, 1487, 281, 8936, 3753, 1254, 11, 51052], "temperature": 0.0, "avg_logprob": -0.13278289676941546, "compression_ratio": 1.5560344827586208, "no_speech_prob": 0.010645574890077114}, {"id": 572, "seek": 379780, "start": 3811.5600000000004, "end": 3817.4, "text": " it's a very decent proxy, imperfect, but it makes sense why people are tempted to use it.", "tokens": [51052, 309, 311, 257, 588, 8681, 29690, 11, 26714, 11, 457, 309, 1669, 2020, 983, 561, 366, 29941, 281, 764, 309, 13, 51344], "temperature": 0.0, "avg_logprob": -0.13278289676941546, "compression_ratio": 1.5560344827586208, "no_speech_prob": 0.010645574890077114}, {"id": 573, "seek": 379780, "start": 3818.04, "end": 3823.4, "text": " Yeah. And, you know, in some ways, that means it makes what LLMs do so much more impressive,", "tokens": [51376, 865, 13, 400, 11, 291, 458, 11, 294, 512, 2098, 11, 300, 1355, 309, 1669, 437, 441, 43, 26386, 360, 370, 709, 544, 8992, 11, 51644], "temperature": 0.0, "avg_logprob": -0.13278289676941546, "compression_ratio": 1.5560344827586208, "no_speech_prob": 0.010645574890077114}, {"id": 574, "seek": 382340, "start": 3823.4, "end": 3829.7200000000003, "text": " because they're also somehow capturing that surface form of concepts. Someone,", "tokens": [50364, 570, 436, 434, 611, 6063, 23384, 300, 3753, 1254, 295, 10392, 13, 8734, 11, 50680], "temperature": 0.0, "avg_logprob": -0.185846832063463, "compression_ratio": 1.4945054945054945, "no_speech_prob": 0.005052727647125721}, {"id": 575, "seek": 382340, "start": 3831.0, "end": 3838.28, "text": " a previous guest pointed out this wonderful quote from Ilya Sotskava saying, well, you know, if", "tokens": [50744, 257, 3894, 8341, 10932, 484, 341, 3715, 6513, 490, 286, 45106, 318, 1971, 74, 4061, 1566, 11, 731, 11, 291, 458, 11, 498, 51108], "temperature": 0.0, "avg_logprob": -0.185846832063463, "compression_ratio": 1.4945054945054945, "no_speech_prob": 0.005052727647125721}, {"id": 576, "seek": 382340, "start": 3840.92, "end": 3849.4, "text": " your LLM can predict the, you know, it's not just predicting text, because if your LLM can be fed", "tokens": [51240, 428, 441, 43, 44, 393, 6069, 264, 11, 291, 458, 11, 309, 311, 406, 445, 32884, 2487, 11, 570, 498, 428, 441, 43, 44, 393, 312, 4636, 51664], "temperature": 0.0, "avg_logprob": -0.185846832063463, "compression_ratio": 1.4945054945054945, "no_speech_prob": 0.005052727647125721}, {"id": 577, "seek": 384940, "start": 3850.12, "end": 3857.8, "text": " the first part of a mystery novel that it's not read before, and it can tell you who the murderer", "tokens": [50400, 264, 700, 644, 295, 257, 11422, 7613, 300, 309, 311, 406, 1401, 949, 11, 293, 309, 393, 980, 291, 567, 264, 28703, 50784], "temperature": 0.0, "avg_logprob": -0.11683271481440617, "compression_ratio": 1.5662650602409638, "no_speech_prob": 0.010322315618395805}, {"id": 578, "seek": 384940, "start": 3857.8, "end": 3864.52, "text": " was, it's not just predicting a word, it's somehow kind of understood what's going on in that story.", "tokens": [50784, 390, 11, 309, 311, 406, 445, 32884, 257, 1349, 11, 309, 311, 6063, 733, 295, 7320, 437, 311, 516, 322, 294, 300, 1657, 13, 51120], "temperature": 0.0, "avg_logprob": -0.11683271481440617, "compression_ratio": 1.5662650602409638, "no_speech_prob": 0.010322315618395805}, {"id": 579, "seek": 384940, "start": 3864.52, "end": 3871.7200000000003, "text": " Now, one of the difficulties, obviously, with all these things is, well, we don't know how", "tokens": [51120, 823, 11, 472, 295, 264, 14399, 11, 2745, 11, 365, 439, 613, 721, 307, 11, 731, 11, 321, 500, 380, 458, 577, 51480], "temperature": 0.0, "avg_logprob": -0.11683271481440617, "compression_ratio": 1.5662650602409638, "no_speech_prob": 0.010322315618395805}, {"id": 580, "seek": 384940, "start": 3871.7200000000003, "end": 3877.64, "text": " open AIs LLMs are trained. So, it's very hard to test them, because you really need someone to write", "tokens": [51480, 1269, 316, 6802, 441, 43, 26386, 366, 8895, 13, 407, 11, 309, 311, 588, 1152, 281, 1500, 552, 11, 570, 291, 534, 643, 1580, 281, 2464, 51776], "temperature": 0.0, "avg_logprob": -0.11683271481440617, "compression_ratio": 1.5662650602409638, "no_speech_prob": 0.010322315618395805}, {"id": 581, "seek": 387764, "start": 3877.64, "end": 3884.8399999999997, "text": " a new mystery novel to actually see if Ilya Sotskava's claim cashed us out. So, it's quite a", "tokens": [50364, 257, 777, 11422, 7613, 281, 767, 536, 498, 286, 45106, 318, 1971, 74, 4061, 311, 3932, 6388, 292, 505, 484, 13, 407, 11, 309, 311, 1596, 257, 50724], "temperature": 0.0, "avg_logprob": -0.13622404581092926, "compression_ratio": 1.4818652849740932, "no_speech_prob": 0.001725714304484427}, {"id": 582, "seek": 387764, "start": 3885.4, "end": 3891.48, "text": " high effort test. Unless, yeah, we happen to know of one which is definitely not in the corpus that", "tokens": [50752, 1090, 4630, 1500, 13, 16581, 11, 1338, 11, 321, 1051, 281, 458, 295, 472, 597, 307, 2138, 406, 294, 264, 1181, 31624, 300, 51056], "temperature": 0.0, "avg_logprob": -0.13622404581092926, "compression_ratio": 1.4818652849740932, "no_speech_prob": 0.001725714304484427}, {"id": 583, "seek": 387764, "start": 3891.48, "end": 3902.04, "text": " was used. But, yeah, it does seem, you know, the fact that they are so good at mirroring what", "tokens": [51056, 390, 1143, 13, 583, 11, 1338, 11, 309, 775, 1643, 11, 291, 458, 11, 264, 1186, 300, 436, 366, 370, 665, 412, 8013, 278, 437, 51584], "temperature": 0.0, "avg_logprob": -0.13622404581092926, "compression_ratio": 1.4818652849740932, "no_speech_prob": 0.001725714304484427}, {"id": 584, "seek": 390204, "start": 3902.04, "end": 3911.48, "text": " we produce, and that what we produce is somehow a good map onto something somewhat deeper, the world", "tokens": [50364, 321, 5258, 11, 293, 300, 437, 321, 5258, 307, 6063, 257, 665, 4471, 3911, 746, 8344, 7731, 11, 264, 1002, 50836], "temperature": 0.0, "avg_logprob": -0.141497802734375, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.059106454253196716}, {"id": 585, "seek": 390204, "start": 3911.48, "end": 3918.2799999999997, "text": " or an inner world. Yeah, it's so impressive. And you point out as well that it seems that,", "tokens": [50836, 420, 364, 7284, 1002, 13, 865, 11, 309, 311, 370, 8992, 13, 400, 291, 935, 484, 382, 731, 300, 309, 2544, 300, 11, 51176], "temperature": 0.0, "avg_logprob": -0.141497802734375, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.059106454253196716}, {"id": 586, "seek": 390204, "start": 3919.08, "end": 3926.36, "text": " you know, the way that LLMs operate is very similar structurally to the way that", "tokens": [51216, 291, 458, 11, 264, 636, 300, 441, 43, 26386, 9651, 307, 588, 2531, 6594, 6512, 281, 264, 636, 300, 51580], "temperature": 0.0, "avg_logprob": -0.141497802734375, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.059106454253196716}, {"id": 587, "seek": 392636, "start": 3927.08, "end": 3935.4, "text": " our minds operate, in that, you know, it's not working on the raw audio or pixel forms of things.", "tokens": [50400, 527, 9634, 9651, 11, 294, 300, 11, 291, 458, 11, 309, 311, 406, 1364, 322, 264, 8936, 6278, 420, 19261, 6422, 295, 721, 13, 50816], "temperature": 0.0, "avg_logprob": -0.1506834692425198, "compression_ratio": 1.4759358288770053, "no_speech_prob": 0.023632925003767014}, {"id": 588, "seek": 392636, "start": 3936.04, "end": 3943.08, "text": " Like, the beauty of language is the compositionality at the level of small units, which are", "tokens": [50848, 1743, 11, 264, 6643, 295, 2856, 307, 264, 12686, 1860, 412, 264, 1496, 295, 1359, 6815, 11, 597, 366, 51200], "temperature": 0.0, "avg_logprob": -0.1506834692425198, "compression_ratio": 1.4759358288770053, "no_speech_prob": 0.023632925003767014}, {"id": 589, "seek": 392636, "start": 3943.7200000000003, "end": 3952.1200000000003, "text": " combinations of symbols or small sounds. And, yeah, the LLMs perfectly match that. So,", "tokens": [51232, 21267, 295, 16944, 420, 1359, 3263, 13, 400, 11, 1338, 11, 264, 441, 43, 26386, 6239, 2995, 300, 13, 407, 11, 51652], "temperature": 0.0, "avg_logprob": -0.1506834692425198, "compression_ratio": 1.4759358288770053, "no_speech_prob": 0.023632925003767014}, {"id": 590, "seek": 395212, "start": 3952.8399999999997, "end": 3959.4, "text": " we've built these things which really do capture something quite essential about how at least a", "tokens": [50400, 321, 600, 3094, 613, 721, 597, 534, 360, 7983, 746, 1596, 7115, 466, 577, 412, 1935, 257, 50728], "temperature": 0.0, "avg_logprob": -0.14888692388729174, "compression_ratio": 1.5819672131147542, "no_speech_prob": 0.0006655759643763304}, {"id": 591, "seek": 395212, "start": 3959.4, "end": 3965.4, "text": " part of our mind operates, it seems. And, yeah, maybe we've been seduced into thinking. That's", "tokens": [50728, 644, 295, 527, 1575, 22577, 11, 309, 2544, 13, 400, 11, 1338, 11, 1310, 321, 600, 668, 9643, 41209, 666, 1953, 13, 663, 311, 51028], "temperature": 0.0, "avg_logprob": -0.14888692388729174, "compression_ratio": 1.5819672131147542, "no_speech_prob": 0.0006655759643763304}, {"id": 592, "seek": 395212, "start": 3965.4, "end": 3973.72, "text": " all there is to thinking. Well, yeah, so in fact, well, that question, I guess I don't want to get", "tokens": [51028, 439, 456, 307, 281, 1953, 13, 1042, 11, 1338, 11, 370, 294, 1186, 11, 731, 11, 300, 1168, 11, 286, 2041, 286, 500, 380, 528, 281, 483, 51444], "temperature": 0.0, "avg_logprob": -0.14888692388729174, "compression_ratio": 1.5819672131147542, "no_speech_prob": 0.0006655759643763304}, {"id": 593, "seek": 395212, "start": 3973.72, "end": 3978.7599999999998, "text": " too technical, but the question of what LLMs are starting with is actually an important one when", "tokens": [51444, 886, 6191, 11, 457, 264, 1168, 295, 437, 441, 43, 26386, 366, 2891, 365, 307, 767, 364, 1021, 472, 562, 51696], "temperature": 0.0, "avg_logprob": -0.14888692388729174, "compression_ratio": 1.5819672131147542, "no_speech_prob": 0.0006655759643763304}, {"id": 594, "seek": 397876, "start": 3978.76, "end": 3987.48, "text": " we're trying to compare them with human minds or human brains. So, in fact, what LLMs operate over", "tokens": [50364, 321, 434, 1382, 281, 6794, 552, 365, 1952, 9634, 420, 1952, 15442, 13, 407, 11, 294, 1186, 11, 437, 441, 43, 26386, 9651, 670, 50800], "temperature": 0.0, "avg_logprob": -0.13484765062428483, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.00968336220830679}, {"id": 595, "seek": 397876, "start": 3987.48, "end": 3996.36, "text": " is tokens. So, it's chunks of characters that tend to occur pretty frequently in text. And so,", "tokens": [50800, 307, 22667, 13, 407, 11, 309, 311, 24004, 295, 4342, 300, 3928, 281, 5160, 1238, 10374, 294, 2487, 13, 400, 370, 11, 51244], "temperature": 0.0, "avg_logprob": -0.13484765062428483, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.00968336220830679}, {"id": 596, "seek": 397876, "start": 3996.36, "end": 4001.32, "text": " oftentimes, they're words, like, though, but they're sometimes not words. If the word is long,", "tokens": [51244, 18349, 11, 436, 434, 2283, 11, 411, 11, 1673, 11, 457, 436, 434, 2171, 406, 2283, 13, 759, 264, 1349, 307, 938, 11, 51492], "temperature": 0.0, "avg_logprob": -0.13484765062428483, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.00968336220830679}, {"id": 597, "seek": 397876, "start": 4001.32, "end": 4008.44, "text": " it gets split up into multiple tokens. Yeah. And so, the problem is that those tokens actually", "tokens": [51492, 309, 2170, 7472, 493, 666, 3866, 22667, 13, 865, 13, 400, 370, 11, 264, 1154, 307, 300, 729, 22667, 767, 51848], "temperature": 0.0, "avg_logprob": -0.13484765062428483, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.00968336220830679}, {"id": 598, "seek": 400844, "start": 4008.44, "end": 4014.6, "text": " don't match linguistic units that the word is actually made of, like morphine. They can be", "tokens": [50364, 500, 380, 2995, 43002, 6815, 300, 264, 1349, 307, 767, 1027, 295, 11, 411, 25778, 533, 13, 814, 393, 312, 50672], "temperature": 0.0, "avg_logprob": -0.11385448942793176, "compression_ratio": 1.4823529411764707, "no_speech_prob": 0.0010156797943636775}, {"id": 599, "seek": 400844, "start": 4014.6, "end": 4021.7200000000003, "text": " pretty arbitrary. And so, that does cause some differences between the way LLMs process them", "tokens": [50672, 1238, 23211, 13, 400, 370, 11, 300, 775, 3082, 512, 7300, 1296, 264, 636, 441, 43, 26386, 1399, 552, 51028], "temperature": 0.0, "avg_logprob": -0.11385448942793176, "compression_ratio": 1.4823529411764707, "no_speech_prob": 0.0010156797943636775}, {"id": 600, "seek": 400844, "start": 4021.7200000000003, "end": 4029.8, "text": " and humans do. In fact, people think that one reason why large language models are bad at arithmetic", "tokens": [51028, 293, 6255, 360, 13, 682, 1186, 11, 561, 519, 300, 472, 1778, 983, 2416, 2856, 5245, 366, 1578, 412, 42973, 51432], "temperature": 0.0, "avg_logprob": -0.11385448942793176, "compression_ratio": 1.4823529411764707, "no_speech_prob": 0.0010156797943636775}, {"id": 601, "seek": 400844, "start": 4029.8, "end": 4035.8, "text": " is because they tokenize numbers in weird ways, right? So, like, I don't know, 1618 is chunks", "tokens": [51432, 307, 570, 436, 14862, 1125, 3547, 294, 3657, 2098, 11, 558, 30, 407, 11, 411, 11, 286, 500, 380, 458, 11, 3165, 6494, 307, 24004, 51732], "temperature": 0.0, "avg_logprob": -0.11385448942793176, "compression_ratio": 1.4823529411764707, "no_speech_prob": 0.0010156797943636775}, {"id": 602, "seek": 403580, "start": 4035.88, "end": 4040.76, "text": " in, like, 161 and then eight. And so, then it gets weird when they have to, like, add up the numbers.", "tokens": [50368, 294, 11, 411, 11, 3165, 16, 293, 550, 3180, 13, 400, 370, 11, 550, 309, 2170, 3657, 562, 436, 362, 281, 11, 411, 11, 909, 493, 264, 3547, 13, 50612], "temperature": 0.0, "avg_logprob": -0.1923511050163059, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.0032203062437474728}, {"id": 603, "seek": 403580, "start": 4040.76, "end": 4047.48, "text": " And so, that's where you get this weird, better-matching errors. And so, this kind of form is", "tokens": [50612, 400, 370, 11, 300, 311, 689, 291, 483, 341, 3657, 11, 1101, 12, 76, 29569, 13603, 13, 400, 370, 11, 341, 733, 295, 1254, 307, 50948], "temperature": 0.0, "avg_logprob": -0.1923511050163059, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.0032203062437474728}, {"id": 604, "seek": 403580, "start": 4047.48, "end": 4053.2400000000002, "text": " that, it's very engineering-driven. It's actually not, like, very rigorously scientifically based.", "tokens": [50948, 300, 11, 309, 311, 588, 7043, 12, 25456, 13, 467, 311, 767, 406, 11, 411, 11, 588, 42191, 5098, 39719, 2361, 13, 51236], "temperature": 0.0, "avg_logprob": -0.1923511050163059, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.0032203062437474728}, {"id": 605, "seek": 403580, "start": 4053.2400000000002, "end": 4058.6800000000003, "text": " And so, it's interesting, like, maybe if we change this little thing, it actually will result in much", "tokens": [51236, 400, 370, 11, 309, 311, 1880, 11, 411, 11, 1310, 498, 321, 1319, 341, 707, 551, 11, 309, 767, 486, 1874, 294, 709, 51508], "temperature": 0.0, "avg_logprob": -0.1923511050163059, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.0032203062437474728}, {"id": 606, "seek": 405868, "start": 4058.68, "end": 4069.0, "text": " better performance. And so, it's funny how a lot of those choices are pretty random engineering-driven", "tokens": [50364, 1101, 3389, 13, 400, 370, 11, 309, 311, 4074, 577, 257, 688, 295, 729, 7994, 366, 1238, 4974, 7043, 12, 25456, 50880], "temperature": 0.0, "avg_logprob": -0.11588083837450165, "compression_ratio": 1.5546558704453441, "no_speech_prob": 0.033054664731025696}, {"id": 607, "seek": 405868, "start": 4069.0, "end": 4075.7999999999997, "text": " things. And, you know, they often work very well. But it's possible that with a small few tweaks,", "tokens": [50880, 721, 13, 400, 11, 291, 458, 11, 436, 2049, 589, 588, 731, 13, 583, 309, 311, 1944, 300, 365, 257, 1359, 1326, 46664, 11, 51220], "temperature": 0.0, "avg_logprob": -0.11588083837450165, "compression_ratio": 1.5546558704453441, "no_speech_prob": 0.033054664731025696}, {"id": 608, "seek": 405868, "start": 4075.7999999999997, "end": 4079.8799999999997, "text": " you can actually make the model much better. Yeah. No, I always thought that there was more", "tokens": [51220, 291, 393, 767, 652, 264, 2316, 709, 1101, 13, 865, 13, 883, 11, 286, 1009, 1194, 300, 456, 390, 544, 51424], "temperature": 0.0, "avg_logprob": -0.11588083837450165, "compression_ratio": 1.5546558704453441, "no_speech_prob": 0.033054664731025696}, {"id": 609, "seek": 405868, "start": 4081.3999999999996, "end": 4086.6, "text": " sort of reasoning behind the n-grams that were used. But maybe, is it just kind of randomly", "tokens": [51500, 1333, 295, 21577, 2261, 264, 297, 12, 1342, 82, 300, 645, 1143, 13, 583, 1310, 11, 307, 309, 445, 733, 295, 16979, 51760], "temperature": 0.0, "avg_logprob": -0.11588083837450165, "compression_ratio": 1.5546558704453441, "no_speech_prob": 0.033054664731025696}, {"id": 610, "seek": 408660, "start": 4087.24, "end": 4093.24, "text": " chunks? Because I would have thought, well, there's some kind of, it makes sense to split", "tokens": [50396, 24004, 30, 1436, 286, 576, 362, 1194, 11, 731, 11, 456, 311, 512, 733, 295, 11, 309, 1669, 2020, 281, 7472, 50696], "temperature": 0.0, "avg_logprob": -0.1504983812008264, "compression_ratio": 1.5625, "no_speech_prob": 0.0031687472946941853}, {"id": 611, "seek": 408660, "start": 4093.24, "end": 4101.48, "text": " words up, because, you know, particles like nus, if I think of, like, redness, right? It's not a", "tokens": [50696, 2283, 493, 11, 570, 11, 291, 458, 11, 10007, 411, 297, 301, 11, 498, 286, 519, 295, 11, 411, 11, 2182, 1287, 11, 558, 30, 467, 311, 406, 257, 51108], "temperature": 0.0, "avg_logprob": -0.1504983812008264, "compression_ratio": 1.5625, "no_speech_prob": 0.0031687472946941853}, {"id": 612, "seek": 408660, "start": 4101.48, "end": 4107.0, "text": " word in itself, but it does attach to so many different words that it's sort of part of the", "tokens": [51108, 1349, 294, 2564, 11, 457, 309, 775, 5085, 281, 370, 867, 819, 2283, 300, 309, 311, 1333, 295, 644, 295, 264, 51384], "temperature": 0.0, "avg_logprob": -0.1504983812008264, "compression_ratio": 1.5625, "no_speech_prob": 0.0031687472946941853}, {"id": 613, "seek": 408660, "start": 4107.0, "end": 4114.36, "text": " compositional structure, I guess. But if it's getting chunked up is just two s's, right? And not", "tokens": [51384, 10199, 2628, 3877, 11, 286, 2041, 13, 583, 498, 309, 311, 1242, 16635, 292, 493, 307, 445, 732, 262, 311, 11, 558, 30, 400, 406, 51752], "temperature": 0.0, "avg_logprob": -0.1504983812008264, "compression_ratio": 1.5625, "no_speech_prob": 0.0031687472946941853}, {"id": 614, "seek": 411436, "start": 4114.44, "end": 4121.32, "text": " nus, then it's kind of odd, yeah. No, that's exactly right, because nus is a morpheme,", "tokens": [50368, 297, 301, 11, 550, 309, 311, 733, 295, 7401, 11, 1338, 13, 883, 11, 300, 311, 2293, 558, 11, 570, 297, 301, 307, 257, 25778, 5729, 11, 50712], "temperature": 0.0, "avg_logprob": -0.15912674694526485, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.005216160323470831}, {"id": 615, "seek": 411436, "start": 4121.32, "end": 4126.28, "text": " it's a suffix with a particular meaning. And so, if redness is chunked into red and nus,", "tokens": [50712, 309, 311, 257, 3889, 970, 365, 257, 1729, 3620, 13, 400, 370, 11, 498, 2182, 1287, 307, 16635, 292, 666, 2182, 293, 297, 301, 11, 50960], "temperature": 0.0, "avg_logprob": -0.15912674694526485, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.005216160323470831}, {"id": 616, "seek": 411436, "start": 4126.28, "end": 4131.5599999999995, "text": " that makes a lot of sense, and it's linguistically justified. But oftentimes,", "tokens": [50960, 300, 1669, 257, 688, 295, 2020, 11, 293, 309, 311, 21766, 20458, 27808, 13, 583, 18349, 11, 51224], "temperature": 0.0, "avg_logprob": -0.15912674694526485, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.005216160323470831}, {"id": 617, "seek": 411436, "start": 4131.5599999999995, "end": 4136.92, "text": " that's not how the chunking happens. That's where the mismatch arises. So, you can definitely", "tokens": [51224, 300, 311, 406, 577, 264, 16635, 278, 2314, 13, 663, 311, 689, 264, 23220, 852, 27388, 13, 407, 11, 291, 393, 2138, 51492], "temperature": 0.0, "avg_logprob": -0.15912674694526485, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.005216160323470831}, {"id": 618, "seek": 411436, "start": 4136.92, "end": 4141.88, "text": " have the two s's in principle. Okay, interesting. Yeah, it seems like, yeah,", "tokens": [51492, 362, 264, 732, 262, 311, 294, 8665, 13, 1033, 11, 1880, 13, 865, 11, 309, 2544, 411, 11, 1338, 11, 51740], "temperature": 0.0, "avg_logprob": -0.15912674694526485, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.005216160323470831}, {"id": 619, "seek": 414188, "start": 4142.36, "end": 4148.84, "text": " one would think that with a bit of curation, maybe they could be even more effective. And yeah, it's", "tokens": [50388, 472, 576, 519, 300, 365, 257, 857, 295, 1262, 399, 11, 1310, 436, 727, 312, 754, 544, 4942, 13, 400, 1338, 11, 309, 311, 50712], "temperature": 0.0, "avg_logprob": -0.20051784100739853, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.0010315935360267758}, {"id": 620, "seek": 414188, "start": 4148.84, "end": 4154.4400000000005, "text": " hard to imagine them being more effective in terms of producing language. But perhaps that's", "tokens": [50712, 1152, 281, 3811, 552, 885, 544, 4942, 294, 2115, 295, 10501, 2856, 13, 583, 4317, 300, 311, 50992], "temperature": 0.0, "avg_logprob": -0.20051784100739853, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.0010315935360267758}, {"id": 621, "seek": 414188, "start": 4154.4400000000005, "end": 4161.4800000000005, "text": " just because they've been fed such a, such copious amounts of data that they sort of these,", "tokens": [50992, 445, 570, 436, 600, 668, 4636, 1270, 257, 11, 1270, 2971, 851, 11663, 295, 1412, 300, 436, 1333, 295, 613, 11, 51344], "temperature": 0.0, "avg_logprob": -0.20051784100739853, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.0010315935360267758}, {"id": 622, "seek": 414188, "start": 4162.6, "end": 4168.84, "text": " you know, they could be more efficient, right? Well, the stalker has an algorithm,", "tokens": [51400, 291, 458, 11, 436, 727, 312, 544, 7148, 11, 558, 30, 1042, 11, 264, 21789, 260, 575, 364, 9284, 11, 51712], "temperature": 0.0, "avg_logprob": -0.20051784100739853, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.0010315935360267758}, {"id": 623, "seek": 416884, "start": 4169.72, "end": 4176.12, "text": " it's kind of the goal is for it to be universal and that driven, right, without human curation,", "tokens": [50408, 309, 311, 733, 295, 264, 3387, 307, 337, 309, 281, 312, 11455, 293, 300, 9555, 11, 558, 11, 1553, 1952, 1262, 399, 11, 50728], "temperature": 0.0, "avg_logprob": -0.09410391048509248, "compression_ratio": 1.5720164609053497, "no_speech_prob": 0.0006261817179620266}, {"id": 624, "seek": 416884, "start": 4176.12, "end": 4183.16, "text": " which is why the morphes don't get respected all the time. It causes a lot of issues for languages", "tokens": [50728, 597, 307, 983, 264, 25778, 279, 500, 380, 483, 20020, 439, 264, 565, 13, 467, 7700, 257, 688, 295, 2663, 337, 8650, 51080], "temperature": 0.0, "avg_logprob": -0.09410391048509248, "compression_ratio": 1.5720164609053497, "no_speech_prob": 0.0006261817179620266}, {"id": 625, "seek": 416884, "start": 4183.16, "end": 4191.32, "text": " that aren't based on the Roman alphabet. So, let's say Arabic, for example, it ends up getting", "tokens": [51080, 300, 3212, 380, 2361, 322, 264, 8566, 23339, 13, 407, 11, 718, 311, 584, 19938, 11, 337, 1365, 11, 309, 5314, 493, 1242, 51488], "temperature": 0.0, "avg_logprob": -0.09410391048509248, "compression_ratio": 1.5720164609053497, "no_speech_prob": 0.0006261817179620266}, {"id": 626, "seek": 416884, "start": 4191.32, "end": 4196.84, "text": " tokenized at the character level, because the tokenizer is just not adapted to deal with it.", "tokens": [51488, 14862, 1602, 412, 264, 2517, 1496, 11, 570, 264, 14862, 6545, 307, 445, 406, 20871, 281, 2028, 365, 309, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09410391048509248, "compression_ratio": 1.5720164609053497, "no_speech_prob": 0.0006261817179620266}, {"id": 627, "seek": 419684, "start": 4196.84, "end": 4200.92, "text": " And so that does mean that performance on these languages that are not", "tokens": [50364, 400, 370, 300, 775, 914, 300, 3389, 322, 613, 8650, 300, 366, 406, 50568], "temperature": 0.0, "avg_logprob": -0.11978640305368524, "compression_ratio": 1.736, "no_speech_prob": 0.0009394345688633621}, {"id": 628, "seek": 419684, "start": 4200.92, "end": 4207.08, "text": " Roman alphabet based is actually worse, often substantially worse. It's generally a problem", "tokens": [50568, 8566, 23339, 2361, 307, 767, 5324, 11, 2049, 30797, 5324, 13, 467, 311, 5101, 257, 1154, 50876], "temperature": 0.0, "avg_logprob": -0.11978640305368524, "compression_ratio": 1.736, "no_speech_prob": 0.0009394345688633621}, {"id": 629, "seek": 419684, "start": 4207.08, "end": 4213.8, "text": " that like the fewer, the less data a language has, the worse the performance in that language.", "tokens": [50876, 300, 411, 264, 13366, 11, 264, 1570, 1412, 257, 2856, 575, 11, 264, 5324, 264, 3389, 294, 300, 2856, 13, 51212], "temperature": 0.0, "avg_logprob": -0.11978640305368524, "compression_ratio": 1.736, "no_speech_prob": 0.0009394345688633621}, {"id": 630, "seek": 419684, "start": 4213.8, "end": 4218.2, "text": " Some of the more general information seems to get pulled across different languages,", "tokens": [51212, 2188, 295, 264, 544, 2674, 1589, 2544, 281, 483, 7373, 2108, 819, 8650, 11, 51432], "temperature": 0.0, "avg_logprob": -0.11978640305368524, "compression_ratio": 1.736, "no_speech_prob": 0.0009394345688633621}, {"id": 631, "seek": 419684, "start": 4218.2, "end": 4223.400000000001, "text": " which is cool. But a lot of language specific stuff, like grammar, right, of course depends", "tokens": [51432, 597, 307, 1627, 13, 583, 257, 688, 295, 2856, 2685, 1507, 11, 411, 22317, 11, 558, 11, 295, 1164, 5946, 51692], "temperature": 0.0, "avg_logprob": -0.11978640305368524, "compression_ratio": 1.736, "no_speech_prob": 0.0009394345688633621}, {"id": 632, "seek": 422340, "start": 4223.4, "end": 4228.679999999999, "text": " on how much data you have in that language. But a particular distinction that tends to", "tokens": [50364, 322, 577, 709, 1412, 291, 362, 294, 300, 2856, 13, 583, 257, 1729, 16844, 300, 12258, 281, 50628], "temperature": 0.0, "avg_logprob": -0.11635842530623726, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.004259418696165085}, {"id": 633, "seek": 422340, "start": 4228.679999999999, "end": 4233.4, "text": " matter beyond just the amount of data is which alphabet. And so because so many of these morphs", "tokens": [50628, 1871, 4399, 445, 264, 2372, 295, 1412, 307, 597, 23339, 13, 400, 370, 570, 370, 867, 295, 613, 25778, 82, 50864], "temperature": 0.0, "avg_logprob": -0.11635842530623726, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.004259418696165085}, {"id": 634, "seek": 422340, "start": 4233.4, "end": 4240.599999999999, "text": " are English centric, a lot of other languages get left behind. Yeah, interesting. And to one", "tokens": [50864, 366, 3669, 1489, 1341, 11, 257, 688, 295, 661, 8650, 483, 1411, 2261, 13, 865, 11, 1880, 13, 400, 281, 472, 51224], "temperature": 0.0, "avg_logprob": -0.11635842530623726, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.004259418696165085}, {"id": 635, "seek": 422340, "start": 4240.599999999999, "end": 4246.28, "text": " extent, I mean, I know there are techniques for doing this. So you spend, you know, a lot of", "tokens": [51224, 8396, 11, 286, 914, 11, 286, 458, 456, 366, 7512, 337, 884, 341, 13, 407, 291, 3496, 11, 291, 458, 11, 257, 688, 295, 51508], "temperature": 0.0, "avg_logprob": -0.11635842530623726, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.004259418696165085}, {"id": 636, "seek": 424628, "start": 4246.28, "end": 4250.2, "text": " experiments looking into the minds or the brains of people.", "tokens": [50364, 12050, 1237, 666, 264, 9634, 420, 264, 15442, 295, 561, 13, 50560], "temperature": 0.0, "avg_logprob": -0.13523740446969365, "compression_ratio": 1.5137614678899083, "no_speech_prob": 0.10184043645858765}, {"id": 637, "seek": 424628, "start": 4252.2, "end": 4259.0, "text": " There are tools which allow us to do this to an extent with LLMs. But, you know, how effective", "tokens": [50660, 821, 366, 3873, 597, 2089, 505, 281, 360, 341, 281, 364, 8396, 365, 441, 43, 26386, 13, 583, 11, 291, 458, 11, 577, 4942, 51000], "temperature": 0.0, "avg_logprob": -0.13523740446969365, "compression_ratio": 1.5137614678899083, "no_speech_prob": 0.10184043645858765}, {"id": 638, "seek": 424628, "start": 4259.0, "end": 4265.0, "text": " are they? How does it compare to looking at an MRI, trying to understand what's going on inside", "tokens": [51000, 366, 436, 30, 1012, 775, 309, 6794, 281, 1237, 412, 364, 32812, 11, 1382, 281, 1223, 437, 311, 516, 322, 1854, 51300], "temperature": 0.0, "avg_logprob": -0.13523740446969365, "compression_ratio": 1.5137614678899083, "no_speech_prob": 0.10184043645858765}, {"id": 639, "seek": 424628, "start": 4265.0, "end": 4271.4, "text": " of an LLM, what concepts it has, or what's lighting up as it is given a prompt?", "tokens": [51300, 295, 364, 441, 43, 44, 11, 437, 10392, 309, 575, 11, 420, 437, 311, 9577, 493, 382, 309, 307, 2212, 257, 12391, 30, 51620], "temperature": 0.0, "avg_logprob": -0.13523740446969365, "compression_ratio": 1.5137614678899083, "no_speech_prob": 0.10184043645858765}, {"id": 640, "seek": 427140, "start": 4272.2, "end": 4279.08, "text": " Yeah. So I am fascinated, honestly, by how many parallels there are between studying biological", "tokens": [50404, 865, 13, 407, 286, 669, 24597, 11, 6095, 11, 538, 577, 867, 44223, 456, 366, 1296, 7601, 13910, 50748], "temperature": 0.0, "avg_logprob": -0.20843777785430084, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.0011330734705552459}, {"id": 641, "seek": 427140, "start": 4279.08, "end": 4286.759999999999, "text": " intelligence and humans and artificial intelligence. And for me, the first similarity is really just", "tokens": [50748, 7599, 293, 6255, 293, 11677, 7599, 13, 400, 337, 385, 11, 264, 700, 32194, 307, 534, 445, 51132], "temperature": 0.0, "avg_logprob": -0.20843777785430084, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.0011330734705552459}, {"id": 642, "seek": 427140, "start": 4286.759999999999, "end": 4293.08, "text": " starting at the behavioral level. So developing separate experiments to look at formal competence", "tokens": [51132, 2891, 412, 264, 19124, 1496, 13, 407, 6416, 4994, 12050, 281, 574, 412, 9860, 39965, 51448], "temperature": 0.0, "avg_logprob": -0.20843777785430084, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.0011330734705552459}, {"id": 643, "seek": 427140, "start": 4293.08, "end": 4297.08, "text": " like grammar, functional competence, like reasoning, these are methods from cognitive", "tokens": [51448, 411, 22317, 11, 11745, 39965, 11, 411, 21577, 11, 613, 366, 7150, 490, 15605, 51648], "temperature": 0.0, "avg_logprob": -0.20843777785430084, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.0011330734705552459}, {"id": 644, "seek": 429708, "start": 4297.48, "end": 4302.28, "text": " science, how do we design good experiments, how do we disentangle different contributors to", "tokens": [50384, 3497, 11, 577, 360, 321, 1715, 665, 12050, 11, 577, 360, 321, 37313, 7846, 819, 45627, 281, 50624], "temperature": 0.0, "avg_logprob": -0.18983508291698636, "compression_ratio": 1.7546296296296295, "no_speech_prob": 0.002214159118011594}, {"id": 645, "seek": 429708, "start": 4302.28, "end": 4307.5599999999995, "text": " performance. So even before we start looking inside the model or inside the brain, just looking", "tokens": [50624, 3389, 13, 407, 754, 949, 321, 722, 1237, 1854, 264, 2316, 420, 1854, 264, 3567, 11, 445, 1237, 50888], "temperature": 0.0, "avg_logprob": -0.18983508291698636, "compression_ratio": 1.7546296296296295, "no_speech_prob": 0.002214159118011594}, {"id": 646, "seek": 429708, "start": 4307.5599999999995, "end": 4314.76, "text": " at how humans behave and how models behave can tell us a lot about potentially how they do it,", "tokens": [50888, 412, 577, 6255, 15158, 293, 577, 5245, 15158, 393, 980, 505, 257, 688, 466, 7263, 577, 436, 360, 309, 11, 51248], "temperature": 0.0, "avg_logprob": -0.18983508291698636, "compression_ratio": 1.7546296296296295, "no_speech_prob": 0.002214159118011594}, {"id": 647, "seek": 429708, "start": 4314.76, "end": 4319.96, "text": " what kind of mistakes they make, what does it tell us about the potential mechanism that they're", "tokens": [51248, 437, 733, 295, 8038, 436, 652, 11, 437, 775, 309, 980, 505, 466, 264, 3995, 7513, 300, 436, 434, 51508], "temperature": 0.0, "avg_logprob": -0.18983508291698636, "compression_ratio": 1.7546296296296295, "no_speech_prob": 0.002214159118011594}, {"id": 648, "seek": 431996, "start": 4320.6, "end": 4329.0, "text": " using to solve the task. But then, of course, we can get even more insight by looking at the", "tokens": [50396, 1228, 281, 5039, 264, 5633, 13, 583, 550, 11, 295, 1164, 11, 321, 393, 483, 754, 544, 11269, 538, 1237, 412, 264, 50816], "temperature": 0.0, "avg_logprob": -0.15621919070973117, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.017972711473703384}, {"id": 649, "seek": 431996, "start": 4329.0, "end": 4334.04, "text": " actual mechanisms or their neural correlates. So for humans, that means looking inside the brain.", "tokens": [50816, 3539, 15902, 420, 641, 18161, 13983, 1024, 13, 407, 337, 6255, 11, 300, 1355, 1237, 1854, 264, 3567, 13, 51068], "temperature": 0.0, "avg_logprob": -0.15621919070973117, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.017972711473703384}, {"id": 650, "seek": 431996, "start": 4334.04, "end": 4342.6, "text": " And for models, that means looking inside the model. And so the movement that is getting seen", "tokens": [51068, 400, 337, 5245, 11, 300, 1355, 1237, 1854, 264, 2316, 13, 400, 370, 264, 3963, 300, 307, 1242, 1612, 51496], "temperature": 0.0, "avg_logprob": -0.15621919070973117, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.017972711473703384}, {"id": 651, "seek": 431996, "start": 4342.6, "end": 4348.84, "text": " currently the mechanistic interpretability movement in AI is doing that, essentially,", "tokens": [51496, 4362, 264, 4236, 3142, 7302, 2310, 3963, 294, 7318, 307, 884, 300, 11, 4476, 11, 51808], "temperature": 0.0, "avg_logprob": -0.15621919070973117, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.017972711473703384}, {"id": 652, "seek": 434884, "start": 4348.84, "end": 4356.4400000000005, "text": " they're asking which circuits, which units inside the network are responsible for a particular", "tokens": [50364, 436, 434, 3365, 597, 26354, 11, 597, 6815, 1854, 264, 3209, 366, 6250, 337, 257, 1729, 50744], "temperature": 0.0, "avg_logprob": -0.0848179562886556, "compression_ratio": 1.6697247706422018, "no_speech_prob": 0.0027971069794148207}, {"id": 653, "seek": 434884, "start": 4356.4400000000005, "end": 4365.72, "text": " behavior. And so they first try to identify those units that get particularly engaged in a task.", "tokens": [50744, 5223, 13, 400, 370, 436, 700, 853, 281, 5876, 729, 6815, 300, 483, 4098, 8237, 294, 257, 5633, 13, 51208], "temperature": 0.0, "avg_logprob": -0.0848179562886556, "compression_ratio": 1.6697247706422018, "no_speech_prob": 0.0027971069794148207}, {"id": 654, "seek": 434884, "start": 4365.72, "end": 4370.52, "text": " Maybe they respond differently to plausible sentences compared to implausible sentences.", "tokens": [51208, 2704, 436, 4196, 7614, 281, 39925, 16579, 5347, 281, 8484, 8463, 964, 16579, 13, 51448], "temperature": 0.0, "avg_logprob": -0.0848179562886556, "compression_ratio": 1.6697247706422018, "no_speech_prob": 0.0027971069794148207}, {"id": 655, "seek": 434884, "start": 4371.16, "end": 4377.72, "text": " And then the beauty of having an artificial system is that they can actually go and", "tokens": [51480, 400, 550, 264, 6643, 295, 1419, 364, 11677, 1185, 307, 300, 436, 393, 767, 352, 293, 51808], "temperature": 0.0, "avg_logprob": -0.0848179562886556, "compression_ratio": 1.6697247706422018, "no_speech_prob": 0.0027971069794148207}, {"id": 656, "seek": 437772, "start": 4377.72, "end": 4383.320000000001, "text": " manipulate it directly. So you can knock out that circuit or you can replace activations from one", "tokens": [50364, 20459, 309, 3838, 13, 407, 291, 393, 6728, 484, 300, 9048, 420, 291, 393, 7406, 2430, 763, 490, 472, 50644], "temperature": 0.0, "avg_logprob": -0.09973513522994852, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.007924838922917843}, {"id": 657, "seek": 437772, "start": 4383.320000000001, "end": 4388.04, "text": " sentence with activations from another sentence. So in neuroscience, people sometimes do that as", "tokens": [50644, 8174, 365, 2430, 763, 490, 1071, 8174, 13, 407, 294, 42762, 11, 561, 2171, 360, 300, 382, 50880], "temperature": 0.0, "avg_logprob": -0.09973513522994852, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.007924838922917843}, {"id": 658, "seek": 437772, "start": 4388.04, "end": 4393.320000000001, "text": " well. In animal research, for example, or there are certain kinds of stimulation that you can do", "tokens": [50880, 731, 13, 682, 5496, 2132, 11, 337, 1365, 11, 420, 456, 366, 1629, 3685, 295, 37405, 300, 291, 393, 360, 51144], "temperature": 0.0, "avg_logprob": -0.09973513522994852, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.007924838922917843}, {"id": 659, "seek": 437772, "start": 4393.320000000001, "end": 4401.96, "text": " that aren't harmful, but can maybe do the desired effect. In aphasia studies, these are natural", "tokens": [51144, 300, 3212, 380, 19727, 11, 457, 393, 1310, 360, 264, 14721, 1802, 13, 682, 257, 7485, 654, 5313, 11, 613, 366, 3303, 51576], "temperature": 0.0, "avg_logprob": -0.09973513522994852, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.007924838922917843}, {"id": 660, "seek": 437772, "start": 4401.96, "end": 4406.52, "text": " causal experiments, right? We didn't cause delusion that destroyed the language network.", "tokens": [51576, 38755, 12050, 11, 558, 30, 492, 994, 380, 3082, 1103, 5704, 300, 8937, 264, 2856, 3209, 13, 51804], "temperature": 0.0, "avg_logprob": -0.09973513522994852, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.007924838922917843}, {"id": 661, "seek": 440652, "start": 4406.52, "end": 4414.360000000001, "text": " But because we see those cases occur naturally, we can look at those effects. And so the causal", "tokens": [50364, 583, 570, 321, 536, 729, 3331, 5160, 8195, 11, 321, 393, 574, 412, 729, 5065, 13, 400, 370, 264, 38755, 50756], "temperature": 0.0, "avg_logprob": -0.0786149081061868, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.00036241073394194245}, {"id": 662, "seek": 440652, "start": 4414.360000000001, "end": 4421.56, "text": " tools are really powerful because they can really help us to see whether this part of the circuit is", "tokens": [50756, 3873, 366, 534, 4005, 570, 436, 393, 534, 854, 505, 281, 536, 1968, 341, 644, 295, 264, 9048, 307, 51116], "temperature": 0.0, "avg_logprob": -0.0786149081061868, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.00036241073394194245}, {"id": 663, "seek": 440652, "start": 4421.56, "end": 4430.360000000001, "text": " necessary for the behavior that we observe. And so in AI systems, we can do that quite easily.", "tokens": [51116, 4818, 337, 264, 5223, 300, 321, 11441, 13, 400, 370, 294, 7318, 3652, 11, 321, 393, 360, 300, 1596, 3612, 13, 51556], "temperature": 0.0, "avg_logprob": -0.0786149081061868, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.00036241073394194245}, {"id": 664, "seek": 443036, "start": 4430.36, "end": 4437.0, "text": " But conceptually, I would say in neuroscience and in AI, what we're trying to find out is very", "tokens": [50364, 583, 3410, 671, 11, 286, 576, 584, 294, 42762, 293, 294, 7318, 11, 437, 321, 434, 1382, 281, 915, 484, 307, 588, 50696], "temperature": 0.0, "avg_logprob": -0.10699250746746453, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0016968456329777837}, {"id": 665, "seek": 443036, "start": 4437.0, "end": 4443.0, "text": " similar. Yeah. Yeah. And it's, I mean, it's wonderful, as you say, at least with the behavioral point,", "tokens": [50696, 2531, 13, 865, 13, 865, 13, 400, 309, 311, 11, 286, 914, 11, 309, 311, 3715, 11, 382, 291, 584, 11, 412, 1935, 365, 264, 19124, 935, 11, 50996], "temperature": 0.0, "avg_logprob": -0.10699250746746453, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0016968456329777837}, {"id": 666, "seek": 443036, "start": 4443.639999999999, "end": 4450.44, "text": " you can draw on the same kind of experiments that, you know, we finally have a kind of", "tokens": [51028, 291, 393, 2642, 322, 264, 912, 733, 295, 12050, 300, 11, 291, 458, 11, 321, 2721, 362, 257, 733, 295, 51368], "temperature": 0.0, "avg_logprob": -0.10699250746746453, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0016968456329777837}, {"id": 667, "seek": 443036, "start": 4450.44, "end": 4453.88, "text": " artificial intelligence that you can feed the same sort of things that you'd feed a person,", "tokens": [51368, 11677, 7599, 300, 291, 393, 3154, 264, 912, 1333, 295, 721, 300, 291, 1116, 3154, 257, 954, 11, 51540], "temperature": 0.0, "avg_logprob": -0.10699250746746453, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0016968456329777837}, {"id": 668, "seek": 445388, "start": 4454.2, "end": 4458.76, "text": " i.e. sentences. And so it makes it very natural to run those kind of experiments.", "tokens": [50380, 741, 13, 68, 13, 16579, 13, 400, 370, 309, 1669, 309, 588, 3303, 281, 1190, 729, 733, 295, 12050, 13, 50608], "temperature": 0.0, "avg_logprob": -0.1111643531105735, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.22737370431423187}, {"id": 669, "seek": 445388, "start": 4460.2, "end": 4466.36, "text": " But then on the other hand, you can also go into the thing itself and tinker it with it in a way", "tokens": [50680, 583, 550, 322, 264, 661, 1011, 11, 291, 393, 611, 352, 666, 264, 551, 2564, 293, 256, 40467, 309, 365, 309, 294, 257, 636, 50988], "temperature": 0.0, "avg_logprob": -0.1111643531105735, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.22737370431423187}, {"id": 670, "seek": 445388, "start": 4466.36, "end": 4471.72, "text": " which would be very unethical and, you know, even just impossible with a person. So you could,", "tokens": [50988, 597, 576, 312, 588, 517, 3293, 804, 293, 11, 291, 458, 11, 754, 445, 6243, 365, 257, 954, 13, 407, 291, 727, 11, 51256], "temperature": 0.0, "avg_logprob": -0.1111643531105735, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.22737370431423187}, {"id": 671, "seek": 445388, "start": 4471.72, "end": 4478.28, "text": " I think there was one example where you had, I don't know, the concept of or Berlin was replaced", "tokens": [51256, 286, 519, 456, 390, 472, 1365, 689, 291, 632, 11, 286, 500, 380, 458, 11, 264, 3410, 295, 420, 13848, 390, 10772, 51584], "temperature": 0.0, "avg_logprob": -0.1111643531105735, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.22737370431423187}, {"id": 672, "seek": 447828, "start": 4478.679999999999, "end": 4485.5599999999995, "text": " with Paris or, no, what was it? It was Rome. Was it the Eiffel Tower was placed in", "tokens": [50384, 365, 8380, 420, 11, 572, 11, 437, 390, 309, 30, 467, 390, 12043, 13, 3027, 309, 264, 462, 3661, 338, 17877, 390, 7074, 294, 50728], "temperature": 0.0, "avg_logprob": -0.18519964218139648, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.041753094643354416}, {"id": 673, "seek": 447828, "start": 4486.599999999999, "end": 4489.96, "text": " conceptually into Rome or something like this? And you asked, well, how do you get from", "tokens": [50780, 3410, 671, 666, 12043, 420, 746, 411, 341, 30, 400, 291, 2351, 11, 731, 11, 577, 360, 291, 483, 490, 50948], "temperature": 0.0, "avg_logprob": -0.18519964218139648, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.041753094643354416}, {"id": 674, "seek": 447828, "start": 4491.4, "end": 4496.759999999999, "text": " Berlin to the Eiffel Tower? It wasn't me, but it was, yeah, it's a famous kind of editing study.", "tokens": [51020, 13848, 281, 264, 462, 3661, 338, 17877, 30, 467, 2067, 380, 385, 11, 457, 309, 390, 11, 1338, 11, 309, 311, 257, 4618, 733, 295, 10000, 2979, 13, 51288], "temperature": 0.0, "avg_logprob": -0.18519964218139648, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.041753094643354416}, {"id": 675, "seek": 447828, "start": 4497.8, "end": 4501.719999999999, "text": " Yeah, I think I must have read it in one of your papers referring to it.", "tokens": [51340, 865, 11, 286, 519, 286, 1633, 362, 1401, 309, 294, 472, 295, 428, 10577, 13761, 281, 309, 13, 51536], "temperature": 0.0, "avg_logprob": -0.18519964218139648, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.041753094643354416}, {"id": 676, "seek": 450172, "start": 4502.360000000001, "end": 4514.4400000000005, "text": " And so the LLM does really, it kind of responds in the way that you would think if what's going", "tokens": [50396, 400, 370, 264, 441, 43, 44, 775, 534, 11, 309, 733, 295, 27331, 294, 264, 636, 300, 291, 576, 519, 498, 437, 311, 516, 51000], "temperature": 0.0, "avg_logprob": -0.15671781314316616, "compression_ratio": 1.6745283018867925, "no_speech_prob": 0.0073000164702534676}, {"id": 677, "seek": 450172, "start": 4514.4400000000005, "end": 4519.4800000000005, "text": " on is that it has some kind of model of the world. And what all you've done is kind of", "tokens": [51000, 322, 307, 300, 309, 575, 512, 733, 295, 2316, 295, 264, 1002, 13, 400, 437, 439, 291, 600, 1096, 307, 733, 295, 51252], "temperature": 0.0, "avg_logprob": -0.15671781314316616, "compression_ratio": 1.6745283018867925, "no_speech_prob": 0.0073000164702534676}, {"id": 678, "seek": 450172, "start": 4519.4800000000005, "end": 4523.72, "text": " switch around some pieces inside that model. It's not that it gets completely, you know,", "tokens": [51252, 3679, 926, 512, 3755, 1854, 300, 2316, 13, 467, 311, 406, 300, 309, 2170, 2584, 11, 291, 458, 11, 51464], "temperature": 0.0, "avg_logprob": -0.15671781314316616, "compression_ratio": 1.6745283018867925, "no_speech_prob": 0.0073000164702534676}, {"id": 679, "seek": 450172, "start": 4523.72, "end": 4529.08, "text": " it doesn't throw everything completely out of whack, I suppose. And it even kind of", "tokens": [51464, 309, 1177, 380, 3507, 1203, 2584, 484, 295, 42877, 11, 286, 7297, 13, 400, 309, 754, 733, 295, 51732], "temperature": 0.0, "avg_logprob": -0.15671781314316616, "compression_ratio": 1.6745283018867925, "no_speech_prob": 0.0073000164702534676}, {"id": 680, "seek": 452908, "start": 4529.16, "end": 4532.76, "text": " infers some things that, you know, the Eiffel Tower will be in the center of Rome and", "tokens": [50368, 1536, 433, 512, 721, 300, 11, 291, 458, 11, 264, 462, 3661, 338, 17877, 486, 312, 294, 264, 3056, 295, 12043, 293, 50548], "temperature": 0.0, "avg_logprob": -0.12855573053713198, "compression_ratio": 1.6275303643724697, "no_speech_prob": 0.0013642775593325496}, {"id": 681, "seek": 452908, "start": 4532.76, "end": 4536.44, "text": " it's going to be up with the Coliseum or something like that, which is, yeah,", "tokens": [50548, 309, 311, 516, 281, 312, 493, 365, 264, 4004, 908, 449, 420, 746, 411, 300, 11, 597, 307, 11, 1338, 11, 50732], "temperature": 0.0, "avg_logprob": -0.12855573053713198, "compression_ratio": 1.6275303643724697, "no_speech_prob": 0.0013642775593325496}, {"id": 682, "seek": 452908, "start": 4537.08, "end": 4545.08, "text": " yeah, it's so fascinating to have something where we can kind of, you know, plausibly", "tokens": [50764, 1338, 11, 309, 311, 370, 10343, 281, 362, 746, 689, 321, 393, 733, 295, 11, 291, 458, 11, 34946, 3545, 51164], "temperature": 0.0, "avg_logprob": -0.12855573053713198, "compression_ratio": 1.6275303643724697, "no_speech_prob": 0.0013642775593325496}, {"id": 683, "seek": 452908, "start": 4545.08, "end": 4551.5599999999995, "text": " peer in into the internal workings. And yet just like the human brain, everything is", "tokens": [51164, 15108, 294, 666, 264, 6920, 589, 1109, 13, 400, 1939, 445, 411, 264, 1952, 3567, 11, 1203, 307, 51488], "temperature": 0.0, "avg_logprob": -0.12855573053713198, "compression_ratio": 1.6275303643724697, "no_speech_prob": 0.0013642775593325496}, {"id": 684, "seek": 452908, "start": 4551.5599999999995, "end": 4557.32, "text": " so complicated that actually also it's not a trivial task, I guess.", "tokens": [51488, 370, 6179, 300, 767, 611, 309, 311, 406, 257, 26703, 5633, 11, 286, 2041, 13, 51776], "temperature": 0.0, "avg_logprob": -0.12855573053713198, "compression_ratio": 1.6275303643724697, "no_speech_prob": 0.0013642775593325496}, {"id": 685, "seek": 455732, "start": 4557.639999999999, "end": 4562.679999999999, "text": " No, but that's the benefit, I guess, when neuroscientists have, we're used to dealing", "tokens": [50380, 883, 11, 457, 300, 311, 264, 5121, 11, 286, 2041, 11, 562, 28813, 5412, 1751, 362, 11, 321, 434, 1143, 281, 6260, 50632], "temperature": 0.0, "avg_logprob": -0.1261825991106463, "compression_ratio": 1.6581818181818182, "no_speech_prob": 0.0010978843783959746}, {"id": 686, "seek": 455732, "start": 4562.679999999999, "end": 4569.48, "text": " with this complexity. And, you know, there are ways to zoom out beyond just each individual", "tokens": [50632, 365, 341, 14024, 13, 400, 11, 291, 458, 11, 456, 366, 2098, 281, 8863, 484, 4399, 445, 1184, 2609, 50972], "temperature": 0.0, "avg_logprob": -0.1261825991106463, "compression_ratio": 1.6581818181818182, "no_speech_prob": 0.0010978843783959746}, {"id": 687, "seek": 455732, "start": 4569.48, "end": 4576.28, "text": " neural unit to try and look at general trends and general patterns. And so I think a lot of", "tokens": [50972, 18161, 4985, 281, 853, 293, 574, 412, 2674, 13892, 293, 2674, 8294, 13, 400, 370, 286, 519, 257, 688, 295, 51312], "temperature": 0.0, "avg_logprob": -0.1261825991106463, "compression_ratio": 1.6581818181818182, "no_speech_prob": 0.0010978843783959746}, {"id": 688, "seek": 455732, "start": 4576.28, "end": 4581.32, "text": " people are daunted by the task of trying to understand the neural net because it's so big", "tokens": [51312, 561, 366, 1120, 19015, 538, 264, 5633, 295, 1382, 281, 1223, 264, 18161, 2533, 570, 309, 311, 370, 955, 51564], "temperature": 0.0, "avg_logprob": -0.1261825991106463, "compression_ratio": 1.6581818181818182, "no_speech_prob": 0.0010978843783959746}, {"id": 689, "seek": 455732, "start": 4581.32, "end": 4586.44, "text": " and complex. And because it's trained in this way where we don't necessarily know which features", "tokens": [51564, 293, 3997, 13, 400, 570, 309, 311, 8895, 294, 341, 636, 689, 321, 500, 380, 4725, 458, 597, 4122, 51820], "temperature": 0.0, "avg_logprob": -0.1261825991106463, "compression_ratio": 1.6581818181818182, "no_speech_prob": 0.0010978843783959746}, {"id": 690, "seek": 458644, "start": 4586.44, "end": 4592.2, "text": " it sticks up on. But to me as a researcher, I'm just excited. It's like a cool puzzle to solve", "tokens": [50364, 309, 12518, 493, 322, 13, 583, 281, 385, 382, 257, 21751, 11, 286, 478, 445, 2919, 13, 467, 311, 411, 257, 1627, 12805, 281, 5039, 50652], "temperature": 0.0, "avg_logprob": -0.1129501055827183, "compression_ratio": 1.6776556776556777, "no_speech_prob": 0.0007540787919424474}, {"id": 691, "seek": 458644, "start": 4592.2, "end": 4596.44, "text": " and a cool problem to understand. So generally, I'm pretty optimistic about this endeavor.", "tokens": [50652, 293, 257, 1627, 1154, 281, 1223, 13, 407, 5101, 11, 286, 478, 1238, 19397, 466, 341, 34975, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1129501055827183, "compression_ratio": 1.6776556776556777, "no_speech_prob": 0.0007540787919424474}, {"id": 692, "seek": 458644, "start": 4597.24, "end": 4603.4, "text": " Cool. Yeah. I think you mentioned at the very beginning that your research is now starting", "tokens": [50904, 8561, 13, 865, 13, 286, 519, 291, 2835, 412, 264, 588, 2863, 300, 428, 2132, 307, 586, 2891, 51212], "temperature": 0.0, "avg_logprob": -0.1129501055827183, "compression_ratio": 1.6776556776556777, "no_speech_prob": 0.0007540787919424474}, {"id": 693, "seek": 458644, "start": 4603.4, "end": 4608.28, "text": " to look at some of the, you know, possibly trickier question of this kind of reflexive", "tokens": [51212, 281, 574, 412, 512, 295, 264, 11, 291, 458, 11, 6264, 4282, 811, 1168, 295, 341, 733, 295, 23802, 488, 51456], "temperature": 0.0, "avg_logprob": -0.1129501055827183, "compression_ratio": 1.6776556776556777, "no_speech_prob": 0.0007540787919424474}, {"id": 694, "seek": 458644, "start": 4609.08, "end": 4614.04, "text": " thinking, the narrow type of thinking I think you mentioned. So we've been talking a lot maybe", "tokens": [51496, 1953, 11, 264, 9432, 2010, 295, 1953, 286, 519, 291, 2835, 13, 407, 321, 600, 668, 1417, 257, 688, 1310, 51744], "temperature": 0.0, "avg_logprob": -0.1129501055827183, "compression_ratio": 1.6776556776556777, "no_speech_prob": 0.0007540787919424474}, {"id": 695, "seek": 461404, "start": 4614.04, "end": 4621.4, "text": " about the broader definition of cognition of just kind of reasoning, manipulation of concepts,", "tokens": [50364, 466, 264, 13227, 7123, 295, 46905, 295, 445, 733, 295, 21577, 11, 26475, 295, 10392, 11, 50732], "temperature": 0.0, "avg_logprob": -0.15308849986006573, "compression_ratio": 1.801498127340824, "no_speech_prob": 0.001519128680229187}, {"id": 696, "seek": 461404, "start": 4621.4, "end": 4625.08, "text": " which might, one might even do in a very automatic way, as we were saying, like you might just solve", "tokens": [50732, 597, 1062, 11, 472, 1062, 754, 360, 294, 257, 588, 12509, 636, 11, 382, 321, 645, 1566, 11, 411, 291, 1062, 445, 5039, 50916], "temperature": 0.0, "avg_logprob": -0.15308849986006573, "compression_ratio": 1.801498127340824, "no_speech_prob": 0.001519128680229187}, {"id": 697, "seek": 461404, "start": 4625.08, "end": 4630.44, "text": " a mass problem without really, you know, in a way where you'd say, oh, yeah, I didn't think about", "tokens": [50916, 257, 2758, 1154, 1553, 534, 11, 291, 458, 11, 294, 257, 636, 689, 291, 1116, 584, 11, 1954, 11, 1338, 11, 286, 994, 380, 519, 466, 51184], "temperature": 0.0, "avg_logprob": -0.15308849986006573, "compression_ratio": 1.801498127340824, "no_speech_prob": 0.001519128680229187}, {"id": 698, "seek": 461404, "start": 4630.44, "end": 4639.0, "text": " that. I just did it. But yeah, how does one, what kind of things have you, how can you pick,", "tokens": [51184, 300, 13, 286, 445, 630, 309, 13, 583, 1338, 11, 577, 775, 472, 11, 437, 733, 295, 721, 362, 291, 11, 577, 393, 291, 1888, 11, 51612], "temperature": 0.0, "avg_logprob": -0.15308849986006573, "compression_ratio": 1.801498127340824, "no_speech_prob": 0.001519128680229187}, {"id": 699, "seek": 461404, "start": 4639.0, "end": 4642.5199999999995, "text": " how can you look at this other problem of like when, when people kind of cogitate about things", "tokens": [51612, 577, 393, 291, 574, 412, 341, 661, 1154, 295, 411, 562, 11, 562, 561, 733, 295, 46521, 8086, 466, 721, 51788], "temperature": 0.0, "avg_logprob": -0.15308849986006573, "compression_ratio": 1.801498127340824, "no_speech_prob": 0.001519128680229187}, {"id": 700, "seek": 464252, "start": 4642.52, "end": 4647.56, "text": " and turn them over in their, in their minds? Where are you going with that? I'm really curious.", "tokens": [50364, 293, 1261, 552, 670, 294, 641, 11, 294, 641, 9634, 30, 2305, 366, 291, 516, 365, 300, 30, 286, 478, 534, 6369, 13, 50616], "temperature": 0.0, "avg_logprob": -0.10309950510660808, "compression_ratio": 1.7130044843049328, "no_speech_prob": 0.000855315534863621}, {"id": 701, "seek": 464252, "start": 4649.72, "end": 4657.160000000001, "text": " I think to me, the interesting question here is the question of individual differences. If", "tokens": [50724, 286, 519, 281, 385, 11, 264, 1880, 1168, 510, 307, 264, 1168, 295, 2609, 7300, 13, 759, 51096], "temperature": 0.0, "avg_logprob": -0.10309950510660808, "compression_ratio": 1.7130044843049328, "no_speech_prob": 0.000855315534863621}, {"id": 702, "seek": 464252, "start": 4658.040000000001, "end": 4663.72, "text": " some people report thinking in words most of the time and others say they don't think in words at", "tokens": [51140, 512, 561, 2275, 1953, 294, 2283, 881, 295, 264, 565, 293, 2357, 584, 436, 500, 380, 519, 294, 2283, 412, 51424], "temperature": 0.0, "avg_logprob": -0.10309950510660808, "compression_ratio": 1.7130044843049328, "no_speech_prob": 0.000855315534863621}, {"id": 703, "seek": 464252, "start": 4663.72, "end": 4668.92, "text": " all, presumably we should be able to see that at the brain level. Presumably we should be able to", "tokens": [51424, 439, 11, 26742, 321, 820, 312, 1075, 281, 536, 300, 412, 264, 3567, 1496, 13, 2718, 449, 1188, 321, 820, 312, 1075, 281, 51684], "temperature": 0.0, "avg_logprob": -0.10309950510660808, "compression_ratio": 1.7130044843049328, "no_speech_prob": 0.000855315534863621}, {"id": 704, "seek": 466892, "start": 4668.92, "end": 4674.36, "text": " see the language network working hard for the first group and not at all for the second group", "tokens": [50364, 536, 264, 2856, 3209, 1364, 1152, 337, 264, 700, 1594, 293, 406, 412, 439, 337, 264, 1150, 1594, 50636], "temperature": 0.0, "avg_logprob": -0.11442181197079745, "compression_ratio": 1.5843621399176955, "no_speech_prob": 0.005059050861746073}, {"id": 705, "seek": 466892, "start": 4675.4, "end": 4681.16, "text": " while they're thinking right spontaneously in this task-free setting. And so that's really what I", "tokens": [50688, 1339, 436, 434, 1953, 558, 47632, 294, 341, 5633, 12, 10792, 3287, 13, 400, 370, 300, 311, 534, 437, 286, 50976], "temperature": 0.0, "avg_logprob": -0.11442181197079745, "compression_ratio": 1.5843621399176955, "no_speech_prob": 0.005059050861746073}, {"id": 706, "seek": 466892, "start": 4681.16, "end": 4688.68, "text": " want to look at. But in order to do that, we need to have a good questionnaire that will capture", "tokens": [50976, 528, 281, 574, 412, 13, 583, 294, 1668, 281, 360, 300, 11, 321, 643, 281, 362, 257, 665, 44702, 300, 486, 7983, 51352], "temperature": 0.0, "avg_logprob": -0.11442181197079745, "compression_ratio": 1.5843621399176955, "no_speech_prob": 0.005059050861746073}, {"id": 707, "seek": 466892, "start": 4688.68, "end": 4695.0, "text": " those differences precisely, right? So I think instead of just asking people, although you think", "tokens": [51352, 729, 7300, 13402, 11, 558, 30, 407, 286, 519, 2602, 295, 445, 3365, 561, 11, 4878, 291, 519, 51668], "temperature": 0.0, "avg_logprob": -0.11442181197079745, "compression_ratio": 1.5843621399176955, "no_speech_prob": 0.005059050861746073}, {"id": 708, "seek": 469500, "start": 4695.0, "end": 4699.64, "text": " in words a lot, they're a little, it would be helpful to think, to get more information, right?", "tokens": [50364, 294, 2283, 257, 688, 11, 436, 434, 257, 707, 11, 309, 576, 312, 4961, 281, 519, 11, 281, 483, 544, 1589, 11, 558, 30, 50596], "temperature": 0.0, "avg_logprob": -0.15864289191461378, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.008307664655148983}, {"id": 709, "seek": 469500, "start": 4699.64, "end": 4710.2, "text": " Do they think in like, what does it mean? Like, what if other meta assessments of their own", "tokens": [50596, 1144, 436, 519, 294, 411, 11, 437, 775, 309, 914, 30, 1743, 11, 437, 498, 661, 19616, 24338, 295, 641, 1065, 51124], "temperature": 0.0, "avg_logprob": -0.15864289191461378, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.008307664655148983}, {"id": 710, "seek": 469500, "start": 4710.2, "end": 4716.68, "text": " thinking style is reliable, right? So like, can we trust those judgments? How can we make them", "tokens": [51124, 1953, 3758, 307, 12924, 11, 558, 30, 407, 411, 11, 393, 321, 3361, 729, 40337, 30, 1012, 393, 321, 652, 552, 51448], "temperature": 0.0, "avg_logprob": -0.15864289191461378, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.008307664655148983}, {"id": 711, "seek": 469500, "start": 4716.68, "end": 4722.76, "text": " more granular? Another question that I'm very interested in, and that's really understood", "tokens": [51448, 544, 39962, 30, 3996, 1168, 300, 286, 478, 588, 3102, 294, 11, 293, 300, 311, 534, 7320, 51752], "temperature": 0.0, "avg_logprob": -0.15864289191461378, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.008307664655148983}, {"id": 712, "seek": 472276, "start": 4722.76, "end": 4731.400000000001, "text": " currently is, is there a difference between thinking in words and hearing the words, right?", "tokens": [50364, 4362, 307, 11, 307, 456, 257, 2649, 1296, 1953, 294, 2283, 293, 4763, 264, 2283, 11, 558, 30, 50796], "temperature": 0.0, "avg_logprob": -0.12615712818346525, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.003120380686596036}, {"id": 713, "seek": 472276, "start": 4731.400000000001, "end": 4736.76, "text": " So if you're using some kind of words and some kind of language to think, does it mean that", "tokens": [50796, 407, 498, 291, 434, 1228, 512, 733, 295, 2283, 293, 512, 733, 295, 2856, 281, 519, 11, 775, 309, 914, 300, 51064], "temperature": 0.0, "avg_logprob": -0.12615712818346525, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.003120380686596036}, {"id": 714, "seek": 472276, "start": 4736.76, "end": 4743.320000000001, "text": " there is a voice or not necessarily? Some people, it turns out, they might see the words written", "tokens": [51064, 456, 307, 257, 3177, 420, 406, 4725, 30, 2188, 561, 11, 309, 4523, 484, 11, 436, 1062, 536, 264, 2283, 3720, 51392], "temperature": 0.0, "avg_logprob": -0.12615712818346525, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.003120380686596036}, {"id": 715, "seek": 472276, "start": 4743.320000000001, "end": 4751.320000000001, "text": " in their mind's eyes, so they spell it out. It's a minority, like less than half of the population,", "tokens": [51392, 294, 641, 1575, 311, 2575, 11, 370, 436, 9827, 309, 484, 13, 467, 311, 257, 16166, 11, 411, 1570, 813, 1922, 295, 264, 4415, 11, 51792], "temperature": 0.0, "avg_logprob": -0.12615712818346525, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.003120380686596036}, {"id": 716, "seek": 475132, "start": 4751.4, "end": 4756.04, "text": " but it does happen. And the capturing those differences, I think, is fascinating and then", "tokens": [50368, 457, 309, 775, 1051, 13, 400, 264, 23384, 729, 7300, 11, 286, 519, 11, 307, 10343, 293, 550, 50600], "temperature": 0.0, "avg_logprob": -0.083389539467661, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0034823480527848005}, {"id": 717, "seek": 475132, "start": 4756.04, "end": 4761.4, "text": " trying to look at the neural correlates to essentially establish the validity of those", "tokens": [50600, 1382, 281, 574, 412, 264, 18161, 13983, 1024, 281, 4476, 8327, 264, 40943, 295, 729, 50868], "temperature": 0.0, "avg_logprob": -0.083389539467661, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0034823480527848005}, {"id": 718, "seek": 475132, "start": 4761.4, "end": 4767.48, "text": " differences to show that they're really not just something that people perceive and report,", "tokens": [50868, 7300, 281, 855, 300, 436, 434, 534, 406, 445, 746, 300, 561, 20281, 293, 2275, 11, 51172], "temperature": 0.0, "avg_logprob": -0.083389539467661, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0034823480527848005}, {"id": 719, "seek": 475132, "start": 4767.48, "end": 4775.0, "text": " but actually, that's not necessarily how they actually think. It's an interesting direction", "tokens": [51172, 457, 767, 11, 300, 311, 406, 4725, 577, 436, 767, 519, 13, 467, 311, 364, 1880, 3513, 51548], "temperature": 0.0, "avg_logprob": -0.083389539467661, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0034823480527848005}, {"id": 720, "seek": 477500, "start": 4775.0, "end": 4785.8, "text": " because psychology has this interesting history of an interesting relationship with phenomenology.", "tokens": [50364, 570, 15105, 575, 341, 1880, 2503, 295, 364, 1880, 2480, 365, 9388, 1793, 13, 50904], "temperature": 0.0, "avg_logprob": -0.14621351682222805, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.0156555138528347}, {"id": 721, "seek": 477500, "start": 4785.8, "end": 4791.4, "text": " So the people reporting their own experiences, right? That used to be very common, and then it", "tokens": [50904, 407, 264, 561, 10031, 641, 1065, 5235, 11, 558, 30, 663, 1143, 281, 312, 588, 2689, 11, 293, 550, 309, 51184], "temperature": 0.0, "avg_logprob": -0.14621351682222805, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.0156555138528347}, {"id": 722, "seek": 477500, "start": 4791.4, "end": 4801.0, "text": " turned out to result in a lot of pseudoscience and discredited a lot of psychology. And so then", "tokens": [51184, 3574, 484, 281, 1874, 294, 257, 688, 295, 25505, 35063, 6699, 293, 2983, 986, 1226, 257, 688, 295, 15105, 13, 400, 370, 550, 51664], "temperature": 0.0, "avg_logprob": -0.14621351682222805, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.0156555138528347}, {"id": 723, "seek": 480100, "start": 4801.0, "end": 4806.6, "text": " there was this huge turn to behaviorism where all that mattered was the stimulus and the response,", "tokens": [50364, 456, 390, 341, 2603, 1261, 281, 5223, 1434, 689, 439, 300, 44282, 390, 264, 21366, 293, 264, 4134, 11, 50644], "temperature": 0.0, "avg_logprob": -0.09219133093001995, "compression_ratio": 1.5867768595041323, "no_speech_prob": 0.006000848952680826}, {"id": 724, "seek": 480100, "start": 4806.6, "end": 4814.6, "text": " and people were refusing to talk about any internal operations at all. So people are still", "tokens": [50644, 293, 561, 645, 37289, 281, 751, 466, 604, 6920, 7705, 412, 439, 13, 407, 561, 366, 920, 51044], "temperature": 0.0, "avg_logprob": -0.09219133093001995, "compression_ratio": 1.5867768595041323, "no_speech_prob": 0.006000848952680826}, {"id": 725, "seek": 480100, "start": 4814.6, "end": 4821.16, "text": " very suspicious of phenomenology, so self-reporting experiences. And I think for the right reason,", "tokens": [51044, 588, 17931, 295, 9388, 1793, 11, 370, 2698, 12, 265, 2707, 278, 5235, 13, 400, 286, 519, 337, 264, 558, 1778, 11, 51372], "temperature": 0.0, "avg_logprob": -0.09219133093001995, "compression_ratio": 1.5867768595041323, "no_speech_prob": 0.006000848952680826}, {"id": 726, "seek": 480100, "start": 4821.16, "end": 4825.32, "text": " because often, yeah, we just don't know how we think. We're like, I think it's words or I think", "tokens": [51372, 570, 2049, 11, 1338, 11, 321, 445, 500, 380, 458, 577, 321, 519, 13, 492, 434, 411, 11, 286, 519, 309, 311, 2283, 420, 286, 519, 51580], "temperature": 0.0, "avg_logprob": -0.09219133093001995, "compression_ratio": 1.5867768595041323, "no_speech_prob": 0.006000848952680826}, {"id": 727, "seek": 482532, "start": 4825.32, "end": 4831.32, "text": " it's not. Sometimes we'll make a decision, like we were saying very quickly. And then when we have", "tokens": [50364, 309, 311, 406, 13, 4803, 321, 603, 652, 257, 3537, 11, 411, 321, 645, 1566, 588, 2661, 13, 400, 550, 562, 321, 362, 50664], "temperature": 0.0, "avg_logprob": -0.16055677757888545, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.020636459812521935}, {"id": 728, "seek": 482532, "start": 4831.32, "end": 4837.799999999999, "text": " to explain what we did and how, we have to rationalize it. And so maybe that's actually not how we", "tokens": [50664, 281, 2903, 437, 321, 630, 293, 577, 11, 321, 362, 281, 15090, 1125, 309, 13, 400, 370, 1310, 300, 311, 767, 406, 577, 321, 50988], "temperature": 0.0, "avg_logprob": -0.16055677757888545, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.020636459812521935}, {"id": 729, "seek": 482532, "start": 4837.799999999999, "end": 4842.92, "text": " arrived at the decision, but post-talk, we come up with an explanation that might not correspond", "tokens": [50988, 6678, 412, 264, 3537, 11, 457, 2183, 12, 29302, 11, 321, 808, 493, 365, 364, 10835, 300, 1062, 406, 6805, 51244], "temperature": 0.0, "avg_logprob": -0.16055677757888545, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.020636459812521935}, {"id": 730, "seek": 482532, "start": 4842.92, "end": 4848.759999999999, "text": " to the reality. So I think we'll have to be careful when taking people at their words.", "tokens": [51244, 281, 264, 4103, 13, 407, 286, 519, 321, 603, 362, 281, 312, 5026, 562, 1940, 561, 412, 641, 2283, 13, 51536], "temperature": 0.0, "avg_logprob": -0.16055677757888545, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.020636459812521935}, {"id": 731, "seek": 482532, "start": 4849.88, "end": 4854.759999999999, "text": " But to me, when people report this strike in differences of like, oh, yeah, I think in words,", "tokens": [51592, 583, 281, 385, 11, 562, 561, 2275, 341, 9302, 294, 7300, 295, 411, 11, 1954, 11, 1338, 11, 286, 519, 294, 2283, 11, 51836], "temperature": 0.0, "avg_logprob": -0.16055677757888545, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.020636459812521935}, {"id": 732, "seek": 485476, "start": 4854.84, "end": 4860.92, "text": " all the time versus like, never, not at all, it seems like there's something there and so I would", "tokens": [50368, 439, 264, 565, 5717, 411, 11, 1128, 11, 406, 412, 439, 11, 309, 2544, 411, 456, 311, 746, 456, 293, 370, 286, 576, 50672], "temperature": 0.0, "avg_logprob": -0.12278245176587786, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.00043682067189365625}, {"id": 733, "seek": 485476, "start": 4860.92, "end": 4865.72, "text": " love to use neuroscience to get at that question more deeply. Yeah, it's a tricky one. I mean,", "tokens": [50672, 959, 281, 764, 42762, 281, 483, 412, 300, 1168, 544, 8760, 13, 865, 11, 309, 311, 257, 12414, 472, 13, 286, 914, 11, 50912], "temperature": 0.0, "avg_logprob": -0.12278245176587786, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.00043682067189365625}, {"id": 734, "seek": 485476, "start": 4865.72, "end": 4869.16, "text": " it strikes me that even the process of asking someone, do you think in words,", "tokens": [50912, 309, 16750, 385, 300, 754, 264, 1399, 295, 3365, 1580, 11, 360, 291, 519, 294, 2283, 11, 51084], "temperature": 0.0, "avg_logprob": -0.12278245176587786, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.00043682067189365625}, {"id": 735, "seek": 485476, "start": 4870.2, "end": 4876.4400000000005, "text": " it almost necessarily linguistic to communicate that because as we say, this is the way that we", "tokens": [51136, 309, 1920, 4725, 43002, 281, 7890, 300, 570, 382, 321, 584, 11, 341, 307, 264, 636, 300, 321, 51448], "temperature": 0.0, "avg_logprob": -0.12278245176587786, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.00043682067189365625}, {"id": 736, "seek": 485476, "start": 4876.4400000000005, "end": 4883.08, "text": " pass ideas around. And so maybe maybe there's just like that kind of arrogance in the language", "tokens": [51448, 1320, 3487, 926, 13, 400, 370, 1310, 1310, 456, 311, 445, 411, 300, 733, 295, 46444, 294, 264, 2856, 51780], "temperature": 0.0, "avg_logprob": -0.12278245176587786, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.00043682067189365625}, {"id": 737, "seek": 488308, "start": 4883.08, "end": 4888.44, "text": " network, which is going to intercept that question and say, oh, yes, it's me. I do all the thinking.", "tokens": [50364, 3209, 11, 597, 307, 516, 281, 24700, 300, 1168, 293, 584, 11, 1954, 11, 2086, 11, 309, 311, 385, 13, 286, 360, 439, 264, 1953, 13, 50632], "temperature": 0.0, "avg_logprob": -0.11316120399619048, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.0062829297967255116}, {"id": 738, "seek": 488308, "start": 4889.32, "end": 4897.0, "text": " But as you say, well, many people do report thinking in many other ways. So yeah, I would,", "tokens": [50676, 583, 382, 291, 584, 11, 731, 11, 867, 561, 360, 2275, 1953, 294, 867, 661, 2098, 13, 407, 1338, 11, 286, 576, 11, 51060], "temperature": 0.0, "avg_logprob": -0.11316120399619048, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.0062829297967255116}, {"id": 739, "seek": 488308, "start": 4897.64, "end": 4903.88, "text": " yeah, I'm really curious about what that shows. I mean, it's just so, this must surprise you all", "tokens": [51092, 1338, 11, 286, 478, 534, 6369, 466, 437, 300, 3110, 13, 286, 914, 11, 309, 311, 445, 370, 11, 341, 1633, 6365, 291, 439, 51404], "temperature": 0.0, "avg_logprob": -0.11316120399619048, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.0062829297967255116}, {"id": 740, "seek": 488308, "start": 4903.88, "end": 4909.64, "text": " the time just how, you know, outwardly, we sort of walk around and we move around and we breathe", "tokens": [51404, 264, 565, 445, 577, 11, 291, 458, 11, 26914, 356, 11, 321, 1333, 295, 1792, 926, 293, 321, 1286, 926, 293, 321, 10192, 51692], "temperature": 0.0, "avg_logprob": -0.11316120399619048, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.0062829297967255116}, {"id": 741, "seek": 490964, "start": 4909.72, "end": 4915.4800000000005, "text": " and we have all our organs are, you know, working in pretty similar ways. And yet,", "tokens": [50368, 293, 321, 362, 439, 527, 20659, 366, 11, 291, 458, 11, 1364, 294, 1238, 2531, 2098, 13, 400, 1939, 11, 50656], "temperature": 0.0, "avg_logprob": -0.10541145151311701, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0019866761285811663}, {"id": 742, "seek": 490964, "start": 4915.4800000000005, "end": 4921.88, "text": " internally, it might be, you know, we seem so heterogeneous, I guess. Does that sound about", "tokens": [50656, 19501, 11, 309, 1062, 312, 11, 291, 458, 11, 321, 1643, 370, 20789, 31112, 11, 286, 2041, 13, 4402, 300, 1626, 466, 50976], "temperature": 0.0, "avg_logprob": -0.10541145151311701, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0019866761285811663}, {"id": 743, "seek": 490964, "start": 4921.88, "end": 4927.160000000001, "text": " right? Or am I overstating the kind of differences in brains that we have?", "tokens": [50976, 558, 30, 1610, 669, 286, 48834, 990, 264, 733, 295, 7300, 294, 15442, 300, 321, 362, 30, 51240], "temperature": 0.0, "avg_logprob": -0.10541145151311701, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0019866761285811663}, {"id": 744, "seek": 490964, "start": 4929.88, "end": 4933.8, "text": " I don't know. I think, yeah, it just depends on your intuition about, you know, how much", "tokens": [51376, 286, 500, 380, 458, 13, 286, 519, 11, 1338, 11, 309, 445, 5946, 322, 428, 24002, 466, 11, 291, 458, 11, 577, 709, 51572], "temperature": 0.0, "avg_logprob": -0.10541145151311701, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0019866761285811663}, {"id": 745, "seek": 490964, "start": 4933.8, "end": 4937.240000000001, "text": " similarity and differences you would expect. Of course, our personalities are very different,", "tokens": [51572, 32194, 293, 7300, 291, 576, 2066, 13, 2720, 1164, 11, 527, 25308, 366, 588, 819, 11, 51744], "temperature": 0.0, "avg_logprob": -0.10541145151311701, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0019866761285811663}, {"id": 746, "seek": 493724, "start": 4937.24, "end": 4941.16, "text": " right? Our likes and dislikes are interesting. So at the cognitive level, there are lots of", "tokens": [50364, 558, 30, 2621, 5902, 293, 43186, 8916, 366, 1880, 13, 407, 412, 264, 15605, 1496, 11, 456, 366, 3195, 295, 50560], "temperature": 0.0, "avg_logprob": -0.15175423971036586, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.0010646141599863768}, {"id": 747, "seek": 493724, "start": 4941.16, "end": 4948.28, "text": " differences between people, of course. And so I guess the interesting thing is that we have", "tokens": [50560, 7300, 1296, 561, 11, 295, 1164, 13, 400, 370, 286, 2041, 264, 1880, 551, 307, 300, 321, 362, 50916], "temperature": 0.0, "avg_logprob": -0.15175423971036586, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.0010646141599863768}, {"id": 748, "seek": 493724, "start": 4948.28, "end": 4954.44, "text": " this huge differences in how we perceive our own thinking, but they don't necessarily manifest", "tokens": [50916, 341, 2603, 7300, 294, 577, 321, 20281, 527, 1065, 1953, 11, 457, 436, 500, 380, 4725, 10067, 51224], "temperature": 0.0, "avg_logprob": -0.15175423971036586, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.0010646141599863768}, {"id": 749, "seek": 493724, "start": 4954.44, "end": 4961.4, "text": " very obviously in differences in this year, right? So in addition to differences in inner", "tokens": [51224, 588, 2745, 294, 7300, 294, 341, 1064, 11, 558, 30, 407, 294, 4500, 281, 7300, 294, 7284, 51572], "temperature": 0.0, "avg_logprob": -0.15175423971036586, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.0010646141599863768}, {"id": 750, "seek": 496140, "start": 4961.48, "end": 4969.0, "text": " speech, another common example is differences in mental imagery, right? So it turns out that some", "tokens": [50368, 6218, 11, 1071, 2689, 1365, 307, 7300, 294, 4973, 24340, 11, 558, 30, 407, 309, 4523, 484, 300, 512, 50744], "temperature": 0.0, "avg_logprob": -0.10093810532119248, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.041436560451984406}, {"id": 751, "seek": 496140, "start": 4969.0, "end": 4977.48, "text": " people never experience visual images in their mind's eye. When they're asked to imagine a red", "tokens": [50744, 561, 1128, 1752, 5056, 5267, 294, 641, 1575, 311, 3313, 13, 1133, 436, 434, 2351, 281, 3811, 257, 2182, 51168], "temperature": 0.0, "avg_logprob": -0.10093810532119248, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.041436560451984406}, {"id": 752, "seek": 496140, "start": 4977.48, "end": 4984.12, "text": " apple, they will think about the concept of an apple and like redness, but they will not like", "tokens": [51168, 10606, 11, 436, 486, 519, 466, 264, 3410, 295, 364, 10606, 293, 411, 2182, 1287, 11, 457, 436, 486, 406, 411, 51500], "temperature": 0.0, "avg_logprob": -0.10093810532119248, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.041436560451984406}, {"id": 753, "seek": 496140, "start": 4984.12, "end": 4991.0, "text": " see a red apple in front of them when they close their eyes. And so that phenomenon has a name,", "tokens": [51500, 536, 257, 2182, 10606, 294, 1868, 295, 552, 562, 436, 1998, 641, 2575, 13, 400, 370, 300, 14029, 575, 257, 1315, 11, 51844], "temperature": 0.0, "avg_logprob": -0.10093810532119248, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.041436560451984406}, {"id": 754, "seek": 499100, "start": 4991.08, "end": 4998.28, "text": " aphantasia. And the name got coined in 2015. So very recently, really. And this is the phenomenon", "tokens": [50368, 257, 15071, 25251, 13, 400, 264, 1315, 658, 45222, 294, 7546, 13, 407, 588, 3938, 11, 534, 13, 400, 341, 307, 264, 14029, 50728], "temperature": 0.0, "avg_logprob": -0.13545148617753358, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.0012836899841204286}, {"id": 755, "seek": 499100, "start": 4998.28, "end": 5003.64, "text": " that kind of got discovered over the centuries at various times and then forgotten again,", "tokens": [50728, 300, 733, 295, 658, 6941, 670, 264, 13926, 412, 3683, 1413, 293, 550, 11832, 797, 11, 50996], "temperature": 0.0, "avg_logprob": -0.13545148617753358, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.0012836899841204286}, {"id": 756, "seek": 499100, "start": 5003.64, "end": 5008.44, "text": " and rediscovered because again, people just tend to assume that everybody else has the same", "tokens": [50996, 293, 2182, 40080, 292, 570, 797, 11, 561, 445, 3928, 281, 6552, 300, 2201, 1646, 575, 264, 912, 51236], "temperature": 0.0, "avg_logprob": -0.13545148617753358, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.0012836899841204286}, {"id": 757, "seek": 499100, "start": 5008.44, "end": 5014.12, "text": " roughly inner experience with them. And so those differences just end up getting neglected.", "tokens": [51236, 9810, 7284, 1752, 365, 552, 13, 400, 370, 729, 7300, 445, 917, 493, 1242, 32701, 13, 51520], "temperature": 0.0, "avg_logprob": -0.13545148617753358, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.0012836899841204286}, {"id": 758, "seek": 499100, "start": 5015.56, "end": 5020.12, "text": " But it turns out that people with aphantasia, you know, again, you cannot tell them apart very", "tokens": [51592, 583, 309, 4523, 484, 300, 561, 365, 257, 15071, 25251, 11, 291, 458, 11, 797, 11, 291, 2644, 980, 552, 4936, 588, 51820], "temperature": 0.0, "avg_logprob": -0.13545148617753358, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.0012836899841204286}, {"id": 759, "seek": 502012, "start": 5020.12, "end": 5025.8, "text": " easily from people with this visual imagery. So it turns out that lots of things we do in the world,", "tokens": [50364, 3612, 490, 561, 365, 341, 5056, 24340, 13, 407, 309, 4523, 484, 300, 3195, 295, 721, 321, 360, 294, 264, 1002, 11, 50648], "temperature": 0.0, "avg_logprob": -0.11610721873345776, "compression_ratio": 1.8053435114503817, "no_speech_prob": 0.0035352734848856926}, {"id": 760, "seek": 502012, "start": 5026.68, "end": 5031.64, "text": " you can do whether or not the experience images visually. Similarly, whether you have strong", "tokens": [50692, 291, 393, 360, 1968, 420, 406, 264, 1752, 5267, 19622, 13, 13157, 11, 1968, 291, 362, 2068, 50940], "temperature": 0.0, "avg_logprob": -0.11610721873345776, "compression_ratio": 1.8053435114503817, "no_speech_prob": 0.0035352734848856926}, {"id": 761, "seek": 502012, "start": 5031.64, "end": 5038.36, "text": " inner speech or not, turns out it's you can't really spot these people very easily out in the", "tokens": [50940, 7284, 6218, 420, 406, 11, 4523, 484, 309, 311, 291, 393, 380, 534, 4008, 613, 561, 588, 3612, 484, 294, 264, 51276], "temperature": 0.0, "avg_logprob": -0.11610721873345776, "compression_ratio": 1.8053435114503817, "no_speech_prob": 0.0035352734848856926}, {"id": 762, "seek": 502012, "start": 5038.36, "end": 5043.0, "text": " wild because they act very differently. So that's the interesting thing, right? Despite these", "tokens": [51276, 4868, 570, 436, 605, 588, 7614, 13, 407, 300, 311, 264, 1880, 551, 11, 558, 30, 11334, 613, 51508], "temperature": 0.0, "avg_logprob": -0.11610721873345776, "compression_ratio": 1.8053435114503817, "no_speech_prob": 0.0035352734848856926}, {"id": 763, "seek": 502012, "start": 5043.0, "end": 5048.92, "text": " experiences being so different, somehow we can still act in roughly similar ways and do the", "tokens": [51508, 5235, 885, 370, 819, 11, 6063, 321, 393, 920, 605, 294, 9810, 2531, 2098, 293, 360, 264, 51804], "temperature": 0.0, "avg_logprob": -0.11610721873345776, "compression_ratio": 1.8053435114503817, "no_speech_prob": 0.0035352734848856926}, {"id": 764, "seek": 504892, "start": 5048.92, "end": 5053.88, "text": " tasks that we need to do in the world. We might be using different strategies, it's very possible,", "tokens": [50364, 9608, 300, 321, 643, 281, 360, 294, 264, 1002, 13, 492, 1062, 312, 1228, 819, 9029, 11, 309, 311, 588, 1944, 11, 50612], "temperature": 0.0, "avg_logprob": -0.1382636831264303, "compression_ratio": 1.6177777777777778, "no_speech_prob": 0.0015234868042171001}, {"id": 765, "seek": 504892, "start": 5053.88, "end": 5058.76, "text": " but the end result is that actually, those differences are very hard to see.", "tokens": [50612, 457, 264, 917, 1874, 307, 300, 767, 11, 729, 7300, 366, 588, 1152, 281, 536, 13, 50856], "temperature": 0.0, "avg_logprob": -0.1382636831264303, "compression_ratio": 1.6177777777777778, "no_speech_prob": 0.0015234868042171001}, {"id": 766, "seek": 504892, "start": 5059.56, "end": 5064.68, "text": " Yeah, yeah, that is fascinating. Yeah, actually, I have a friend who is an aphant, I guess.", "tokens": [50896, 865, 11, 1338, 11, 300, 307, 10343, 13, 865, 11, 767, 11, 286, 362, 257, 1277, 567, 307, 364, 257, 15071, 11, 286, 2041, 13, 51152], "temperature": 0.0, "avg_logprob": -0.1382636831264303, "compression_ratio": 1.6177777777777778, "no_speech_prob": 0.0015234868042171001}, {"id": 767, "seek": 504892, "start": 5065.8, "end": 5073.56, "text": " And well, he didn't find out until a few years ago. And we, you know, there's no kind of outward", "tokens": [51208, 400, 731, 11, 415, 994, 380, 915, 484, 1826, 257, 1326, 924, 2057, 13, 400, 321, 11, 291, 458, 11, 456, 311, 572, 733, 295, 26914, 51596], "temperature": 0.0, "avg_logprob": -0.1382636831264303, "compression_ratio": 1.6177777777777778, "no_speech_prob": 0.0015234868042171001}, {"id": 768, "seek": 507356, "start": 5073.56, "end": 5079.96, "text": " sign, right? You just seem completely, you know, normal. But then we're like, oh, yeah, I just", "tokens": [50364, 1465, 11, 558, 30, 509, 445, 1643, 2584, 11, 291, 458, 11, 2710, 13, 583, 550, 321, 434, 411, 11, 1954, 11, 1338, 11, 286, 445, 50684], "temperature": 0.0, "avg_logprob": -0.1620541214942932, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.15790216624736786}, {"id": 769, "seek": 507356, "start": 5079.96, "end": 5084.68, "text": " can't visualize triangles, right? I know what a triangle is, I can reason about triangles, I can.", "tokens": [50684, 393, 380, 23273, 29896, 11, 558, 30, 286, 458, 437, 257, 13369, 307, 11, 286, 393, 1778, 466, 29896, 11, 286, 393, 13, 50920], "temperature": 0.0, "avg_logprob": -0.1620541214942932, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.15790216624736786}, {"id": 770, "seek": 507356, "start": 5086.120000000001, "end": 5090.4400000000005, "text": " And actually, often there's, I think there might be some research which shows that that in some ways,", "tokens": [50992, 400, 767, 11, 2049, 456, 311, 11, 286, 519, 456, 1062, 312, 512, 2132, 597, 3110, 300, 300, 294, 512, 2098, 11, 51208], "temperature": 0.0, "avg_logprob": -0.1620541214942932, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.15790216624736786}, {"id": 771, "seek": 507356, "start": 5091.64, "end": 5097.080000000001, "text": " oftentimes better at reasoning about certain things, where one might think it requires a", "tokens": [51268, 18349, 1101, 412, 21577, 466, 1629, 721, 11, 689, 472, 1062, 519, 309, 7029, 257, 51540], "temperature": 0.0, "avg_logprob": -0.1620541214942932, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.15790216624736786}, {"id": 772, "seek": 509708, "start": 5097.96, "end": 5104.04, "text": " visual element. But yeah, he was, you know, a very good physicist, very good colleague,", "tokens": [50408, 5056, 4478, 13, 583, 1338, 11, 415, 390, 11, 291, 458, 11, 257, 588, 665, 42466, 11, 588, 665, 13532, 11, 50712], "temperature": 0.0, "avg_logprob": -0.17596978232974098, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.0046040392480790615}, {"id": 773, "seek": 509708, "start": 5105.8, "end": 5108.28, "text": " but just thought in a different way, I guess.", "tokens": [50800, 457, 445, 1194, 294, 257, 819, 636, 11, 286, 2041, 13, 50924], "temperature": 0.0, "avg_logprob": -0.17596978232974098, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.0046040392480790615}, {"id": 774, "seek": 509708, "start": 5112.12, "end": 5117.96, "text": " Yeah, I think we'll find some differences, right? Like, now that there's more awareness,", "tokens": [51116, 865, 11, 286, 519, 321, 603, 915, 512, 7300, 11, 558, 30, 1743, 11, 586, 300, 456, 311, 544, 8888, 11, 51408], "temperature": 0.0, "avg_logprob": -0.17596978232974098, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.0046040392480790615}, {"id": 775, "seek": 509708, "start": 5117.96, "end": 5122.2, "text": " once we start doing more systematic research, I think we'll, like, I mean, there are already", "tokens": [51408, 1564, 321, 722, 884, 544, 27249, 2132, 11, 286, 519, 321, 603, 11, 411, 11, 286, 914, 11, 456, 366, 1217, 51620], "temperature": 0.0, "avg_logprob": -0.17596978232974098, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.0046040392480790615}, {"id": 776, "seek": 512220, "start": 5122.2, "end": 5128.2, "text": " attempts, trying to look at the relationship between aphantasia and episodic memory,", "tokens": [50364, 15257, 11, 1382, 281, 574, 412, 264, 2480, 1296, 257, 15071, 25251, 293, 39200, 299, 4675, 11, 50664], "temperature": 0.0, "avg_logprob": -0.18906658770991305, "compression_ratio": 1.708502024291498, "no_speech_prob": 0.030186796560883522}, {"id": 777, "seek": 512220, "start": 5129.0, "end": 5135.96, "text": " turned out that aphantasia and, yeah, spatial reasoning, geometric reasoning there, the link", "tokens": [50704, 3574, 484, 300, 257, 15071, 25251, 293, 11, 1338, 11, 23598, 21577, 11, 33246, 21577, 456, 11, 264, 2113, 51052], "temperature": 0.0, "avg_logprob": -0.18906658770991305, "compression_ratio": 1.708502024291498, "no_speech_prob": 0.030186796560883522}, {"id": 778, "seek": 512220, "start": 5137.08, "end": 5143.639999999999, "text": " is not as strong and may or maybe not even there, even though people expected it to be.", "tokens": [51108, 307, 406, 382, 2068, 293, 815, 420, 1310, 406, 754, 456, 11, 754, 1673, 561, 5176, 309, 281, 312, 13, 51436], "temperature": 0.0, "avg_logprob": -0.18906658770991305, "compression_ratio": 1.708502024291498, "no_speech_prob": 0.030186796560883522}, {"id": 779, "seek": 512220, "start": 5143.639999999999, "end": 5147.5599999999995, "text": " And there are variations as to why. But yeah, essentially, like, I think,", "tokens": [51436, 400, 456, 366, 17840, 382, 281, 983, 13, 583, 1338, 11, 4476, 11, 411, 11, 286, 519, 11, 51632], "temperature": 0.0, "avg_logprob": -0.18906658770991305, "compression_ratio": 1.708502024291498, "no_speech_prob": 0.030186796560883522}, {"id": 780, "seek": 512220, "start": 5147.5599999999995, "end": 5151.32, "text": " even though those differences aren't apparent, I think we'll find some eventually.", "tokens": [51632, 754, 1673, 729, 7300, 3212, 380, 18335, 11, 286, 519, 321, 603, 915, 512, 4728, 13, 51820], "temperature": 0.0, "avg_logprob": -0.18906658770991305, "compression_ratio": 1.708502024291498, "no_speech_prob": 0.030186796560883522}, {"id": 781, "seek": 515132, "start": 5151.32, "end": 5155.16, "text": " And probably we'll just find out that different people are using different strategies to do the", "tokens": [50364, 400, 1391, 321, 603, 445, 915, 484, 300, 819, 561, 366, 1228, 819, 9029, 281, 360, 264, 50556], "temperature": 0.0, "avg_logprob": -0.13264416633768283, "compression_ratio": 1.5506072874493928, "no_speech_prob": 0.00014864880358800292}, {"id": 782, "seek": 515132, "start": 5155.16, "end": 5160.679999999999, "text": " same thing. Some of them might require imagery or thinking in words, you know, speech, and some", "tokens": [50556, 912, 551, 13, 2188, 295, 552, 1062, 3651, 24340, 420, 1953, 294, 2283, 11, 291, 458, 11, 6218, 11, 293, 512, 50832], "temperature": 0.0, "avg_logprob": -0.13264416633768283, "compression_ratio": 1.5506072874493928, "no_speech_prob": 0.00014864880358800292}, {"id": 783, "seek": 515132, "start": 5160.679999999999, "end": 5170.36, "text": " might not. I mean, all this is a reminder that well, LLMs might be, we might end up producing", "tokens": [50832, 1062, 406, 13, 286, 914, 11, 439, 341, 307, 257, 13548, 300, 731, 11, 441, 43, 26386, 1062, 312, 11, 321, 1062, 917, 493, 10501, 51316], "temperature": 0.0, "avg_logprob": -0.13264416633768283, "compression_ratio": 1.5506072874493928, "no_speech_prob": 0.00014864880358800292}, {"id": 784, "seek": 515132, "start": 5170.36, "end": 5177.08, "text": " artificial intelligences, which outwardly look very similar. But we shouldn't, or yeah, we should", "tokens": [51316, 11677, 5613, 2667, 11, 597, 26914, 356, 574, 588, 2531, 13, 583, 321, 4659, 380, 11, 420, 1338, 11, 321, 820, 51652], "temperature": 0.0, "avg_logprob": -0.13264416633768283, "compression_ratio": 1.5506072874493928, "no_speech_prob": 0.00014864880358800292}, {"id": 785, "seek": 517708, "start": 5177.08, "end": 5183.0, "text": " be careful to think that to be mindful that inwardly, they could be very, very different.", "tokens": [50364, 312, 5026, 281, 519, 300, 281, 312, 14618, 300, 29876, 356, 11, 436, 727, 312, 588, 11, 588, 819, 13, 50660], "temperature": 0.0, "avg_logprob": -0.13963571814603584, "compression_ratio": 1.5938864628820961, "no_speech_prob": 0.004059553612023592}, {"id": 786, "seek": 517708, "start": 5184.44, "end": 5191.08, "text": " And I think it's very true, and it's already happening, right? There are all those cases where", "tokens": [50732, 400, 286, 519, 309, 311, 588, 2074, 11, 293, 309, 311, 1217, 2737, 11, 558, 30, 821, 366, 439, 729, 3331, 689, 51064], "temperature": 0.0, "avg_logprob": -0.13963571814603584, "compression_ratio": 1.5938864628820961, "no_speech_prob": 0.004059553612023592}, {"id": 787, "seek": 517708, "start": 5191.96, "end": 5199.0, "text": " people were screenshotting chat GPT responses, especially right after it came out a year ago,", "tokens": [51108, 561, 645, 27712, 783, 5081, 26039, 51, 13019, 11, 2318, 558, 934, 309, 1361, 484, 257, 1064, 2057, 11, 51460], "temperature": 0.0, "avg_logprob": -0.13963571814603584, "compression_ratio": 1.5938864628820961, "no_speech_prob": 0.004059553612023592}, {"id": 788, "seek": 517708, "start": 5199.0, "end": 5204.36, "text": " and just showing it responding to some very complicated prompt and doing it correctly,", "tokens": [51460, 293, 445, 4099, 309, 16670, 281, 512, 588, 6179, 12391, 293, 884, 309, 8944, 11, 51728], "temperature": 0.0, "avg_logprob": -0.13963571814603584, "compression_ratio": 1.5938864628820961, "no_speech_prob": 0.004059553612023592}, {"id": 789, "seek": 520436, "start": 5204.44, "end": 5208.12, "text": " and people were so impressed being like, oh, you know, if you have, if you put like,", "tokens": [50368, 293, 561, 645, 370, 11679, 885, 411, 11, 1954, 11, 291, 458, 11, 498, 291, 362, 11, 498, 291, 829, 411, 11, 50552], "temperature": 0.0, "avg_logprob": -0.1777103501136857, "compression_ratio": 1.7378640776699028, "no_speech_prob": 0.02226896956562996}, {"id": 790, "seek": 520436, "start": 5209.96, "end": 5217.24, "text": " a mail on a table, like, you know, like on on a chair, is that like a construction", "tokens": [50644, 257, 10071, 322, 257, 3199, 11, 411, 11, 291, 458, 11, 411, 322, 322, 257, 6090, 11, 307, 300, 411, 257, 6435, 51008], "temperature": 0.0, "avg_logprob": -0.1777103501136857, "compression_ratio": 1.7378640776699028, "no_speech_prob": 0.02226896956562996}, {"id": 791, "seek": 520436, "start": 5217.24, "end": 5223.0, "text": " stable or not? Or, you know, what kind of thing we can put on top? And like, it looks very impressive.", "tokens": [51008, 8351, 420, 406, 30, 1610, 11, 291, 458, 11, 437, 733, 295, 551, 321, 393, 829, 322, 1192, 30, 400, 411, 11, 309, 1542, 588, 8992, 13, 51296], "temperature": 0.0, "avg_logprob": -0.1777103501136857, "compression_ratio": 1.7378640776699028, "no_speech_prob": 0.02226896956562996}, {"id": 792, "seek": 520436, "start": 5223.0, "end": 5229.08, "text": " And then it turns out that if you change the problem just slightly, then it just starts", "tokens": [51296, 400, 550, 309, 4523, 484, 300, 498, 291, 1319, 264, 1154, 445, 4748, 11, 550, 309, 445, 3719, 51600], "temperature": 0.0, "avg_logprob": -0.1777103501136857, "compression_ratio": 1.7378640776699028, "no_speech_prob": 0.02226896956562996}, {"id": 793, "seek": 522908, "start": 5229.08, "end": 5236.36, "text": " spitting out total nonsense. And the same thing happened with a social reasoning problem, kind", "tokens": [50364, 637, 2414, 484, 3217, 14925, 13, 400, 264, 912, 551, 2011, 365, 257, 2093, 21577, 1154, 11, 733, 50728], "temperature": 0.0, "avg_logprob": -0.10344749909860117, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.04143721982836723}, {"id": 794, "seek": 522908, "start": 5236.36, "end": 5240.68, "text": " of predicting what the other person would think, which is a classical problem from psychology.", "tokens": [50728, 295, 32884, 437, 264, 661, 954, 576, 519, 11, 597, 307, 257, 13735, 1154, 490, 15105, 13, 50944], "temperature": 0.0, "avg_logprob": -0.10344749909860117, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.04143721982836723}, {"id": 795, "seek": 522908, "start": 5240.68, "end": 5246.44, "text": " And so the claim was that, you know, now LLMs can reason about people and what they do. And then,", "tokens": [50944, 400, 370, 264, 3932, 390, 300, 11, 291, 458, 11, 586, 441, 43, 26386, 393, 1778, 466, 561, 293, 437, 436, 360, 13, 400, 550, 11, 51232], "temperature": 0.0, "avg_logprob": -0.10344749909860117, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.04143721982836723}, {"id": 796, "seek": 522908, "start": 5246.44, "end": 5251.48, "text": " again, at third dollar, we change the prompt to be slightly different from the problems that", "tokens": [51232, 797, 11, 412, 2636, 7241, 11, 321, 1319, 264, 12391, 281, 312, 4748, 819, 490, 264, 2740, 300, 51484], "temperature": 0.0, "avg_logprob": -0.10344749909860117, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.04143721982836723}, {"id": 797, "seek": 522908, "start": 5251.48, "end": 5258.12, "text": " were already available on the internet, then model performance drops drastically. So it's very", "tokens": [51484, 645, 1217, 2435, 322, 264, 4705, 11, 550, 2316, 3389, 11438, 29673, 13, 407, 309, 311, 588, 51816], "temperature": 0.0, "avg_logprob": -0.10344749909860117, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.04143721982836723}, {"id": 798, "seek": 525812, "start": 5258.12, "end": 5266.28, "text": " easy to fall for this seemingly impressive performance, seemingly seemingly impressive", "tokens": [50364, 1858, 281, 2100, 337, 341, 18709, 8992, 3389, 11, 18709, 18709, 8992, 50772], "temperature": 0.0, "avg_logprob": -0.12043193623989443, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.0031706688459962606}, {"id": 799, "seek": 525812, "start": 5266.28, "end": 5273.96, "text": " understanding. And so luckily for us, in this case, there are ways to design even behavioral", "tokens": [50772, 3701, 13, 400, 370, 22880, 337, 505, 11, 294, 341, 1389, 11, 456, 366, 2098, 281, 1715, 754, 19124, 51156], "temperature": 0.0, "avg_logprob": -0.12043193623989443, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.0031706688459962606}, {"id": 800, "seek": 525812, "start": 5273.96, "end": 5279.96, "text": " interventions that can maybe help us figure out what's actually going on and what strategy is", "tokens": [51156, 20924, 300, 393, 1310, 854, 505, 2573, 484, 437, 311, 767, 516, 322, 293, 437, 5206, 307, 51456], "temperature": 0.0, "avg_logprob": -0.12043193623989443, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.0031706688459962606}, {"id": 801, "seek": 525812, "start": 5279.96, "end": 5285.5599999999995, "text": " actually being used. Yeah, yeah, that's a very important problem for sure. I found the social", "tokens": [51456, 767, 885, 1143, 13, 865, 11, 1338, 11, 300, 311, 257, 588, 1021, 1154, 337, 988, 13, 286, 1352, 264, 2093, 51736], "temperature": 0.0, "avg_logprob": -0.12043193623989443, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.0031706688459962606}, {"id": 802, "seek": 528556, "start": 5285.56, "end": 5291.8, "text": " reasoning example really, I just loved it. So I think if I remember correctly, one of the ways", "tokens": [50364, 21577, 1365, 534, 11, 286, 445, 4333, 309, 13, 407, 286, 519, 498, 286, 1604, 8944, 11, 472, 295, 264, 2098, 50676], "temperature": 0.0, "avg_logprob": -0.14934658281730884, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.06936100125312805}, {"id": 803, "seek": 528556, "start": 5291.8, "end": 5298.360000000001, "text": " that you fall them is just inserting a few words in the middle. So the classic, the classic one", "tokens": [50676, 300, 291, 2100, 552, 307, 445, 46567, 257, 1326, 2283, 294, 264, 2808, 13, 407, 264, 7230, 11, 264, 7230, 472, 51004], "temperature": 0.0, "avg_logprob": -0.14934658281730884, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.06936100125312805}, {"id": 804, "seek": 528556, "start": 5298.360000000001, "end": 5305.0, "text": " is something like, you know, Susie hides Bob's apple. It was in the closet. Where does he think", "tokens": [51004, 307, 746, 411, 11, 291, 458, 11, 9545, 414, 35953, 6085, 311, 10606, 13, 467, 390, 294, 264, 16669, 13, 2305, 775, 415, 519, 51336], "temperature": 0.0, "avg_logprob": -0.14934658281730884, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.06936100125312805}, {"id": 805, "seek": 528556, "start": 5305.0, "end": 5311.080000000001, "text": " the apple is? And no, the thing will think it will say, Oh, not in the closet anymore.", "tokens": [51336, 264, 10606, 307, 30, 400, 572, 11, 264, 551, 486, 519, 309, 486, 584, 11, 876, 11, 406, 294, 264, 16669, 3602, 13, 51640], "temperature": 0.0, "avg_logprob": -0.14934658281730884, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.06936100125312805}, {"id": 806, "seek": 531108, "start": 5311.8, "end": 5315.8, "text": " Correct. But if you change it, say, well, Susie hides Bob's apple, it was in the closet,", "tokens": [50400, 12753, 13, 583, 498, 291, 1319, 309, 11, 584, 11, 731, 11, 9545, 414, 35953, 6085, 311, 10606, 11, 309, 390, 294, 264, 16669, 11, 50600], "temperature": 0.0, "avg_logprob": -0.13016123192332615, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.014495576731860638}, {"id": 807, "seek": 531108, "start": 5315.8, "end": 5321.4, "text": " and she tells him that she hid it, right? And then the LLM still says, Oh, he thinks it's not in", "tokens": [50600, 293, 750, 5112, 796, 300, 750, 16253, 309, 11, 558, 30, 400, 550, 264, 441, 43, 44, 920, 1619, 11, 876, 11, 415, 7309, 309, 311, 406, 294, 50880], "temperature": 0.0, "avg_logprob": -0.13016123192332615, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.014495576731860638}, {"id": 808, "seek": 531108, "start": 5321.4, "end": 5330.04, "text": " the closet because it and I think, yeah, so clearly they've been either on the training set or in", "tokens": [50880, 264, 16669, 570, 309, 293, 286, 519, 11, 1338, 11, 370, 4448, 436, 600, 668, 2139, 322, 264, 3097, 992, 420, 294, 51312], "temperature": 0.0, "avg_logprob": -0.13016123192332615, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.014495576731860638}, {"id": 809, "seek": 531108, "start": 5330.04, "end": 5337.24, "text": " the fine tuning, you know, it's gone so far in getting them to kind of give the appearance of", "tokens": [51312, 264, 2489, 15164, 11, 291, 458, 11, 309, 311, 2780, 370, 1400, 294, 1242, 552, 281, 733, 295, 976, 264, 8967, 295, 51672], "temperature": 0.0, "avg_logprob": -0.13016123192332615, "compression_ratio": 1.597457627118644, "no_speech_prob": 0.014495576731860638}, {"id": 810, "seek": 533724, "start": 5337.24, "end": 5342.04, "text": " having some kind of theory of mind and being able to solve those problems. But then with a little", "tokens": [50364, 1419, 512, 733, 295, 5261, 295, 1575, 293, 885, 1075, 281, 5039, 729, 2740, 13, 583, 550, 365, 257, 707, 50604], "temperature": 0.0, "avg_logprob": -0.09820884786626344, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.020934756845235825}, {"id": 811, "seek": 533724, "start": 5342.04, "end": 5348.04, "text": " bit of tweaking, it becomes apparent that they they don't. But it seems to be coming harder and", "tokens": [50604, 857, 295, 6986, 2456, 11, 309, 3643, 18335, 300, 436, 436, 500, 380, 13, 583, 309, 2544, 281, 312, 1348, 6081, 293, 50904], "temperature": 0.0, "avg_logprob": -0.09820884786626344, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.020934756845235825}, {"id": 812, "seek": 533724, "start": 5348.04, "end": 5358.04, "text": " harder to, you know, fox these systems. And you point out that one of the real difficulties is", "tokens": [50904, 6081, 281, 11, 291, 458, 11, 21026, 613, 3652, 13, 400, 291, 935, 484, 300, 472, 295, 264, 957, 14399, 307, 51404], "temperature": 0.0, "avg_logprob": -0.09820884786626344, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.020934756845235825}, {"id": 813, "seek": 533724, "start": 5358.04, "end": 5364.2, "text": " we just, you know, without knowing what has gone into the training of open AI's models and", "tokens": [51404, 321, 445, 11, 291, 458, 11, 1553, 5276, 437, 575, 2780, 666, 264, 3097, 295, 1269, 7318, 311, 5245, 293, 51712], "temperature": 0.0, "avg_logprob": -0.09820884786626344, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.020934756845235825}, {"id": 814, "seek": 536420, "start": 5364.5199999999995, "end": 5372.679999999999, "text": " it, it's hard to know how much genuine kind of intelligence has emerged and how much is just", "tokens": [50380, 309, 11, 309, 311, 1152, 281, 458, 577, 709, 16699, 733, 295, 7599, 575, 20178, 293, 577, 709, 307, 445, 50788], "temperature": 0.0, "avg_logprob": -0.1541163494712428, "compression_ratio": 1.669603524229075, "no_speech_prob": 0.0010000303154811263}, {"id": 815, "seek": 536420, "start": 5372.679999999999, "end": 5379.16, "text": " kind of pan recognition and, and, you know, or simple pan recognition and recall. But of course,", "tokens": [50788, 733, 295, 2462, 11150, 293, 11, 293, 11, 291, 458, 11, 420, 2199, 2462, 11150, 293, 9901, 13, 583, 295, 1164, 11, 51112], "temperature": 0.0, "avg_logprob": -0.1541163494712428, "compression_ratio": 1.669603524229075, "no_speech_prob": 0.0010000303154811263}, {"id": 816, "seek": 536420, "start": 5379.16, "end": 5383.48, "text": " the paradox here as well, they have some of the most advanced models. So maybe there's something", "tokens": [51112, 264, 26221, 510, 382, 731, 11, 436, 362, 512, 295, 264, 881, 7339, 5245, 13, 407, 1310, 456, 311, 746, 51328], "temperature": 0.0, "avg_logprob": -0.1541163494712428, "compression_ratio": 1.669603524229075, "no_speech_prob": 0.0010000303154811263}, {"id": 817, "seek": 536420, "start": 5383.48, "end": 5389.72, "text": " genuinely interesting going on, but it's black box. So we can't really say. But I think it's", "tokens": [51328, 17839, 1880, 516, 322, 11, 457, 309, 311, 2211, 2424, 13, 407, 321, 393, 380, 534, 584, 13, 583, 286, 519, 309, 311, 51640], "temperature": 0.0, "avg_logprob": -0.1541163494712428, "compression_ratio": 1.669603524229075, "no_speech_prob": 0.0010000303154811263}, {"id": 818, "seek": 538972, "start": 5389.72, "end": 5397.88, "text": " encouraging that other folks are, you know, mistrial perhaps are being more open and about", "tokens": [50364, 14580, 300, 661, 4024, 366, 11, 291, 458, 11, 3544, 7111, 4317, 366, 885, 544, 1269, 293, 466, 50772], "temperature": 0.0, "avg_logprob": -0.15371021694607204, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.004966561682522297}, {"id": 819, "seek": 538972, "start": 5397.88, "end": 5405.16, "text": " what's going into that into their models. So, you know, maybe easier to, to test on the very", "tokens": [50772, 437, 311, 516, 666, 300, 666, 641, 5245, 13, 407, 11, 291, 458, 11, 1310, 3571, 281, 11, 281, 1500, 322, 264, 588, 51136], "temperature": 0.0, "avg_logprob": -0.15371021694607204, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.004966561682522297}, {"id": 820, "seek": 538972, "start": 5405.16, "end": 5411.8, "text": " latest things and have confidence that I don't know, some problem was not appearing verbatim in", "tokens": [51136, 6792, 721, 293, 362, 6687, 300, 286, 500, 380, 458, 11, 512, 1154, 390, 406, 19870, 9595, 267, 332, 294, 51468], "temperature": 0.0, "avg_logprob": -0.15371021694607204, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.004966561682522297}, {"id": 821, "seek": 538972, "start": 5411.8, "end": 5415.96, "text": " in the training set. But even then, yeah, it could be some very similar problem.", "tokens": [51468, 294, 264, 3097, 992, 13, 583, 754, 550, 11, 1338, 11, 309, 727, 312, 512, 588, 2531, 1154, 13, 51676], "temperature": 0.0, "avg_logprob": -0.15371021694607204, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.004966561682522297}, {"id": 822, "seek": 541596, "start": 5416.28, "end": 5423.96, "text": " Yeah, for sure. And like, it's not always bad if they've been fine tuned on this problem. And if", "tokens": [50380, 865, 11, 337, 988, 13, 400, 411, 11, 309, 311, 406, 1009, 1578, 498, 436, 600, 668, 2489, 10870, 322, 341, 1154, 13, 400, 498, 50764], "temperature": 0.0, "avg_logprob": -0.1806624543433096, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.0016225617146119475}, {"id": 823, "seek": 541596, "start": 5423.96, "end": 5429.88, "text": " they've seen examples before, as humans learn lots of things, we can do math right off the bat,", "tokens": [50764, 436, 600, 1612, 5110, 949, 11, 382, 6255, 1466, 3195, 295, 721, 11, 321, 393, 360, 5221, 558, 766, 264, 7362, 11, 51060], "temperature": 0.0, "avg_logprob": -0.1806624543433096, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.0016225617146119475}, {"id": 824, "seek": 541596, "start": 5429.88, "end": 5434.92, "text": " we learn, we learn from examples. It's easier for us to the problems that are familiar to us and", "tokens": [51060, 321, 1466, 11, 321, 1466, 490, 5110, 13, 467, 311, 3571, 337, 505, 281, 264, 2740, 300, 366, 4963, 281, 505, 293, 51312], "temperature": 0.0, "avg_logprob": -0.1806624543433096, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.0016225617146119475}, {"id": 825, "seek": 541596, "start": 5434.92, "end": 5441.16, "text": " novel ones, like, that's fine. It's just important to know, because that helps us figure out what", "tokens": [51312, 7613, 2306, 11, 411, 11, 300, 311, 2489, 13, 467, 311, 445, 1021, 281, 458, 11, 570, 300, 3665, 505, 2573, 484, 437, 51624], "temperature": 0.0, "avg_logprob": -0.1806624543433096, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.0016225617146119475}, {"id": 826, "seek": 544116, "start": 5441.16, "end": 5446.599999999999, "text": " is the mechanism that they're using, or potentially using, and what are the keys is where they might", "tokens": [50364, 307, 264, 7513, 300, 436, 434, 1228, 11, 420, 7263, 1228, 11, 293, 437, 366, 264, 9317, 307, 689, 436, 1062, 50636], "temperature": 0.0, "avg_logprob": -0.16191278424179345, "compression_ratio": 1.6826568265682658, "no_speech_prob": 0.0054568517953157425}, {"id": 827, "seek": 544116, "start": 5446.599999999999, "end": 5454.04, "text": " break, right? So it's, we don't necessarily need to expect these models to be like amazing,", "tokens": [50636, 1821, 11, 558, 30, 407, 309, 311, 11, 321, 500, 380, 4725, 643, 281, 2066, 613, 5245, 281, 312, 411, 2243, 11, 51008], "temperature": 0.0, "avg_logprob": -0.16191278424179345, "compression_ratio": 1.6826568265682658, "no_speech_prob": 0.0054568517953157425}, {"id": 828, "seek": 544116, "start": 5454.04, "end": 5459.88, "text": " zero shot thinkers on total and novel problems all the time. It's just that, yeah, knowing", "tokens": [51008, 4018, 3347, 37895, 322, 3217, 293, 7613, 2740, 439, 264, 565, 13, 467, 311, 445, 300, 11, 1338, 11, 5276, 51300], "temperature": 0.0, "avg_logprob": -0.16191278424179345, "compression_ratio": 1.6826568265682658, "no_speech_prob": 0.0054568517953157425}, {"id": 829, "seek": 544116, "start": 5459.88, "end": 5463.24, "text": " what goes in the training data, knowing what problems they've been fine tuned on,", "tokens": [51300, 437, 1709, 294, 264, 3097, 1412, 11, 5276, 437, 2740, 436, 600, 668, 2489, 10870, 322, 11, 51468], "temperature": 0.0, "avg_logprob": -0.16191278424179345, "compression_ratio": 1.6826568265682658, "no_speech_prob": 0.0054568517953157425}, {"id": 830, "seek": 544116, "start": 5463.24, "end": 5468.84, "text": " just helps us assess them more accurately. Yeah, yeah, that's a good point. It's unfair to", "tokens": [51468, 445, 3665, 505, 5877, 552, 544, 20095, 13, 865, 11, 1338, 11, 300, 311, 257, 665, 935, 13, 467, 311, 17019, 281, 51748], "temperature": 0.0, "avg_logprob": -0.16191278424179345, "compression_ratio": 1.6826568265682658, "no_speech_prob": 0.0054568517953157425}, {"id": 831, "seek": 546884, "start": 5469.8, "end": 5473.8, "text": " sort of demand development so that they don't learn anything, or they don't benefit from fine", "tokens": [50412, 1333, 295, 4733, 3250, 370, 300, 436, 500, 380, 1466, 1340, 11, 420, 436, 500, 380, 5121, 490, 2489, 50612], "temperature": 0.0, "avg_logprob": -0.18767633040746054, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.002047773916274309}, {"id": 832, "seek": 546884, "start": 5473.8, "end": 5482.68, "text": " tuning. But yeah, but we want to know if they are, if the way that they're responding is using,", "tokens": [50612, 15164, 13, 583, 1338, 11, 457, 321, 528, 281, 458, 498, 436, 366, 11, 498, 264, 636, 300, 436, 434, 16670, 307, 1228, 11, 51056], "temperature": 0.0, "avg_logprob": -0.18767633040746054, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.002047773916274309}, {"id": 833, "seek": 546884, "start": 5483.96, "end": 5487.96, "text": " is that they've abstracted patterns, or they've just, they're just regurgitating.", "tokens": [51120, 307, 300, 436, 600, 12649, 292, 8294, 11, 420, 436, 600, 445, 11, 436, 434, 445, 1121, 5476, 16350, 13, 51320], "temperature": 0.0, "avg_logprob": -0.18767633040746054, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.002047773916274309}, {"id": 834, "seek": 546884, "start": 5490.360000000001, "end": 5496.84, "text": " Yeah, well, I, yeah, I found it really interesting to go through your work. I'm usually,", "tokens": [51440, 865, 11, 731, 11, 286, 11, 1338, 11, 286, 1352, 309, 534, 1880, 281, 352, 807, 428, 589, 13, 286, 478, 2673, 11, 51764], "temperature": 0.0, "avg_logprob": -0.18767633040746054, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.002047773916274309}, {"id": 835, "seek": 549684, "start": 5497.8, "end": 5505.16, "text": " I don't know, optimistic, I guess, or maybe not, maybe that's the wrong word. I really admire how", "tokens": [50412, 286, 500, 380, 458, 11, 19397, 11, 286, 2041, 11, 420, 1310, 406, 11, 1310, 300, 311, 264, 2085, 1349, 13, 286, 534, 21951, 577, 50780], "temperature": 0.0, "avg_logprob": -0.11898862042473358, "compression_ratio": 1.556910569105691, "no_speech_prob": 0.0015464435564354062}, {"id": 836, "seek": 549684, "start": 5505.16, "end": 5513.72, "text": " LLMs are, have taken off, and it surprised me how quickly they've advanced. But one thing I've", "tokens": [50780, 441, 43, 26386, 366, 11, 362, 2726, 766, 11, 293, 309, 6100, 385, 577, 2661, 436, 600, 7339, 13, 583, 472, 551, 286, 600, 51208], "temperature": 0.0, "avg_logprob": -0.11898862042473358, "compression_ratio": 1.556910569105691, "no_speech_prob": 0.0015464435564354062}, {"id": 837, "seek": 549684, "start": 5513.72, "end": 5518.28, "text": " enjoyed about your work is kind of reminding me, well, actually, maybe they're not as far along in", "tokens": [51208, 4626, 466, 428, 589, 307, 733, 295, 27639, 385, 11, 731, 11, 767, 11, 1310, 436, 434, 406, 382, 1400, 2051, 294, 51436], "temperature": 0.0, "avg_logprob": -0.11898862042473358, "compression_ratio": 1.556910569105691, "no_speech_prob": 0.0015464435564354062}, {"id": 838, "seek": 549684, "start": 5518.28, "end": 5523.96, "text": " some ways as, as they appear to be at the surface level. Like there still seems to be some,", "tokens": [51436, 512, 2098, 382, 11, 382, 436, 4204, 281, 312, 412, 264, 3753, 1496, 13, 1743, 456, 920, 2544, 281, 312, 512, 11, 51720], "temperature": 0.0, "avg_logprob": -0.11898862042473358, "compression_ratio": 1.556910569105691, "no_speech_prob": 0.0015464435564354062}, {"id": 839, "seek": 552396, "start": 5524.92, "end": 5529.64, "text": " some nuts to crack here to make them, to bring them closer to, to human thought.", "tokens": [50412, 512, 10483, 281, 6226, 510, 281, 652, 552, 11, 281, 1565, 552, 4966, 281, 11, 281, 1952, 1194, 13, 50648], "temperature": 0.0, "avg_logprob": -0.11425324008889394, "compression_ratio": 1.48, "no_speech_prob": 0.0021778063382953405}, {"id": 840, "seek": 552396, "start": 5531.0, "end": 5538.12, "text": " What's your take? Do you think, you know, I don't want to ask the typical how far away is AGI,", "tokens": [50716, 708, 311, 428, 747, 30, 1144, 291, 519, 11, 291, 458, 11, 286, 500, 380, 528, 281, 1029, 264, 7476, 577, 1400, 1314, 307, 316, 26252, 11, 51072], "temperature": 0.0, "avg_logprob": -0.11425324008889394, "compression_ratio": 1.48, "no_speech_prob": 0.0021778063382953405}, {"id": 841, "seek": 552396, "start": 5538.12, "end": 5547.24, "text": " because, but, you know, are you of the opinion that just cranking through more data", "tokens": [51072, 570, 11, 457, 11, 291, 458, 11, 366, 291, 295, 264, 4800, 300, 445, 21263, 278, 807, 544, 1412, 51528], "temperature": 0.0, "avg_logprob": -0.11425324008889394, "compression_ratio": 1.48, "no_speech_prob": 0.0021778063382953405}, {"id": 842, "seek": 554724, "start": 5547.88, "end": 5555.96, "text": " is going to continue to produce results, or should more be invested in this kind of modularity", "tokens": [50396, 307, 516, 281, 2354, 281, 5258, 3542, 11, 420, 820, 544, 312, 13104, 294, 341, 733, 295, 31111, 507, 50800], "temperature": 0.0, "avg_logprob": -0.08862006174374933, "compression_ratio": 1.5401069518716577, "no_speech_prob": 0.014041083864867687}, {"id": 843, "seek": 554724, "start": 5556.599999999999, "end": 5565.96, "text": " approach? And if the latter, well, do we have, you know, are the things that are taking place", "tokens": [50832, 3109, 30, 400, 498, 264, 18481, 11, 731, 11, 360, 321, 362, 11, 291, 458, 11, 366, 264, 721, 300, 366, 1940, 1081, 51300], "temperature": 0.0, "avg_logprob": -0.08862006174374933, "compression_ratio": 1.5401069518716577, "no_speech_prob": 0.014041083864867687}, {"id": 844, "seek": 554724, "start": 5565.96, "end": 5570.92, "text": " on the right track, or do we need to look more closely and learn more from the human mind, perhaps?", "tokens": [51300, 322, 264, 558, 2837, 11, 420, 360, 321, 643, 281, 574, 544, 8185, 293, 1466, 544, 490, 264, 1952, 1575, 11, 4317, 30, 51548], "temperature": 0.0, "avg_logprob": -0.08862006174374933, "compression_ratio": 1.5401069518716577, "no_speech_prob": 0.014041083864867687}, {"id": 845, "seek": 557092, "start": 5571.0, "end": 5582.12, "text": " I think we definitely should recognize all the impressive achievements that we observe in LLMs", "tokens": [50368, 286, 519, 321, 2138, 820, 5521, 439, 264, 8992, 21420, 300, 321, 11441, 294, 441, 43, 26386, 50924], "temperature": 0.0, "avg_logprob": -0.08478197504262455, "compression_ratio": 1.515625, "no_speech_prob": 0.001064607873558998}, {"id": 846, "seek": 557092, "start": 5582.12, "end": 5589.64, "text": " today. And one way that my colleagues and I have been thinking about it is through the formal", "tokens": [50924, 965, 13, 400, 472, 636, 300, 452, 7734, 293, 286, 362, 668, 1953, 466, 309, 307, 807, 264, 9860, 51300], "temperature": 0.0, "avg_logprob": -0.08478197504262455, "compression_ratio": 1.515625, "no_speech_prob": 0.001064607873558998}, {"id": 847, "seek": 557092, "start": 5589.64, "end": 5596.12, "text": " functional competence lens. On the formal competence front, learning the rules and patterns of natural", "tokens": [51300, 11745, 39965, 6765, 13, 1282, 264, 9860, 39965, 1868, 11, 2539, 264, 4474, 293, 8294, 295, 3303, 51624], "temperature": 0.0, "avg_logprob": -0.08478197504262455, "compression_ratio": 1.515625, "no_speech_prob": 0.001064607873558998}, {"id": 848, "seek": 559612, "start": 5596.12, "end": 5604.12, "text": " language. These models have been incredibly impressive. So even a couple of years ago,", "tokens": [50364, 2856, 13, 1981, 5245, 362, 668, 6252, 8992, 13, 407, 754, 257, 1916, 295, 924, 2057, 11, 50764], "temperature": 0.0, "avg_logprob": -0.16923327778660974, "compression_ratio": 1.609442060085837, "no_speech_prob": 0.0028877181466668844}, {"id": 849, "seek": 559612, "start": 5604.12, "end": 5608.44, "text": " they almost reached the ceiling for English, at least the language where they have the most data.", "tokens": [50764, 436, 1920, 6488, 264, 13655, 337, 3669, 11, 412, 1935, 264, 2856, 689, 436, 362, 264, 881, 1412, 13, 50980], "temperature": 0.0, "avg_logprob": -0.16923327778660974, "compression_ratio": 1.609442060085837, "no_speech_prob": 0.0028877181466668844}, {"id": 850, "seek": 559612, "start": 5609.16, "end": 5616.12, "text": " And they did it without the need for any fine tuning. So just learning to predict words and", "tokens": [51016, 400, 436, 630, 309, 1553, 264, 643, 337, 604, 2489, 15164, 13, 407, 445, 2539, 281, 6069, 2283, 293, 51364], "temperature": 0.0, "avg_logprob": -0.16923327778660974, "compression_ratio": 1.609442060085837, "no_speech_prob": 0.0028877181466668844}, {"id": 851, "seek": 559612, "start": 5616.12, "end": 5623.88, "text": " method corpora of text. Turns out that is something that gives you all the most of the grammar and", "tokens": [51364, 3170, 6804, 64, 295, 2487, 13, 29524, 484, 300, 307, 746, 300, 2709, 291, 439, 264, 881, 295, 264, 22317, 293, 51752], "temperature": 0.0, "avg_logprob": -0.16923327778660974, "compression_ratio": 1.609442060085837, "no_speech_prob": 0.0028877181466668844}, {"id": 852, "seek": 562388, "start": 5624.52, "end": 5631.400000000001, "text": " knowledge of idioms and all kinds of patterns that characterize a language. And that wasn't trivial", "tokens": [50396, 3601, 295, 18014, 4785, 293, 439, 3685, 295, 8294, 300, 38463, 257, 2856, 13, 400, 300, 2067, 380, 26703, 50740], "temperature": 0.0, "avg_logprob": -0.08855951350668202, "compression_ratio": 1.5617529880478087, "no_speech_prob": 0.0022861668840050697}, {"id": 853, "seek": 562388, "start": 5631.400000000001, "end": 5636.76, "text": " at all. That was the subject of debate for linguists for decades. Is it possible to just learn from", "tokens": [50740, 412, 439, 13, 663, 390, 264, 3983, 295, 7958, 337, 21766, 1751, 337, 7878, 13, 1119, 309, 1944, 281, 445, 1466, 490, 51008], "temperature": 0.0, "avg_logprob": -0.08855951350668202, "compression_ratio": 1.5617529880478087, "no_speech_prob": 0.0022861668840050697}, {"id": 854, "seek": 562388, "start": 5636.76, "end": 5643.400000000001, "text": " data? Or do you need a rich set of internal rules that can help you figure out what's grammatical", "tokens": [51008, 1412, 30, 1610, 360, 291, 643, 257, 4593, 992, 295, 6920, 4474, 300, 393, 854, 291, 2573, 484, 437, 311, 17570, 267, 804, 51340], "temperature": 0.0, "avg_logprob": -0.08855951350668202, "compression_ratio": 1.5617529880478087, "no_speech_prob": 0.0022861668840050697}, {"id": 855, "seek": 562388, "start": 5643.400000000001, "end": 5649.4800000000005, "text": " and what's not? So that is incredibly impressive scientifically. And on the engineering front,", "tokens": [51340, 293, 437, 311, 406, 30, 407, 300, 307, 6252, 8992, 39719, 13, 400, 322, 264, 7043, 1868, 11, 51644], "temperature": 0.0, "avg_logprob": -0.08855951350668202, "compression_ratio": 1.5617529880478087, "no_speech_prob": 0.0022861668840050697}, {"id": 856, "seek": 564948, "start": 5650.36, "end": 5656.28, "text": " different language processing systems in the past have struggled so much because you just can't", "tokens": [50408, 819, 2856, 9007, 3652, 294, 264, 1791, 362, 19023, 370, 709, 570, 291, 445, 393, 380, 50704], "temperature": 0.0, "avg_logprob": -0.15152330852690196, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.014947350136935711}, {"id": 857, "seek": 564948, "start": 5656.28, "end": 5662.28, "text": " encode language with a few simple rules or even not simple rules. Just like so fuzzy and there are", "tokens": [50704, 2058, 1429, 2856, 365, 257, 1326, 2199, 4474, 420, 754, 406, 2199, 4474, 13, 1449, 411, 370, 34710, 293, 456, 366, 51004], "temperature": 0.0, "avg_logprob": -0.15152330852690196, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.014947350136935711}, {"id": 858, "seek": 564948, "start": 5662.28, "end": 5666.28, "text": " so many exceptions in the regular forms and this and that. And the fact that these models have", "tokens": [51004, 370, 867, 22847, 294, 264, 3890, 6422, 293, 341, 293, 300, 13, 400, 264, 1186, 300, 613, 5245, 362, 51204], "temperature": 0.0, "avg_logprob": -0.15152330852690196, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.014947350136935711}, {"id": 859, "seek": 564948, "start": 5666.28, "end": 5670.12, "text": " mastered that is so impressive. And people kind of forget and start talking about AGI right away,", "tokens": [51204, 38686, 300, 307, 370, 8992, 13, 400, 561, 733, 295, 2870, 293, 722, 1417, 466, 316, 26252, 558, 1314, 11, 51396], "temperature": 0.0, "avg_logprob": -0.15152330852690196, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.014947350136935711}, {"id": 860, "seek": 564948, "start": 5670.12, "end": 5674.839999999999, "text": " but that's an impressive achievement. Then being good at language is already very impressive.", "tokens": [51396, 457, 300, 311, 364, 8992, 15838, 13, 1396, 885, 665, 412, 2856, 307, 1217, 588, 8992, 13, 51632], "temperature": 0.0, "avg_logprob": -0.15152330852690196, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.014947350136935711}, {"id": 861, "seek": 567484, "start": 5675.8, "end": 5680.76, "text": " And then we get to functional components and their ability to reason and be factually accurate,", "tokens": [50412, 400, 550, 321, 483, 281, 11745, 6677, 293, 641, 3485, 281, 1778, 293, 312, 1186, 671, 8559, 11, 50660], "temperature": 0.0, "avg_logprob": -0.16001541669978653, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.0010319724678993225}, {"id": 862, "seek": 567484, "start": 5680.76, "end": 5685.32, "text": " know what's true and what's false and be actually helpful. And so that's a whole other host of", "tokens": [50660, 458, 437, 311, 2074, 293, 437, 311, 7908, 293, 312, 767, 4961, 13, 400, 370, 300, 311, 257, 1379, 661, 3975, 295, 50888], "temperature": 0.0, "avg_logprob": -0.16001541669978653, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.0010319724678993225}, {"id": 863, "seek": 567484, "start": 5685.32, "end": 5693.88, "text": " problems where they actually seem to be spotty. They have achieved a lot because of pattern", "tokens": [50888, 2740, 689, 436, 767, 1643, 281, 312, 4008, 874, 13, 814, 362, 11042, 257, 688, 570, 295, 5102, 51316], "temperature": 0.0, "avg_logprob": -0.16001541669978653, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.0010319724678993225}, {"id": 864, "seek": 567484, "start": 5693.88, "end": 5698.52, "text": " recognition, but then it turns out that that performance is not robust and it breaks. And", "tokens": [51316, 11150, 11, 457, 550, 309, 4523, 484, 300, 300, 3389, 307, 406, 13956, 293, 309, 9857, 13, 400, 51548], "temperature": 0.0, "avg_logprob": -0.16001541669978653, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.0010319724678993225}, {"id": 865, "seek": 569852, "start": 5698.6, "end": 5708.4400000000005, "text": " so that's where it gets more complicated and more controversial. And that's where we argue modularity", "tokens": [50368, 370, 300, 311, 689, 309, 2170, 544, 6179, 293, 544, 17323, 13, 400, 300, 311, 689, 321, 9695, 31111, 507, 50860], "temperature": 0.0, "avg_logprob": -0.08173831080047178, "compression_ratio": 1.5487179487179488, "no_speech_prob": 0.004129295237362385}, {"id": 866, "seek": 569852, "start": 5708.4400000000005, "end": 5715.400000000001, "text": " will be helpful. Again, looking at the human brain as an example. And one distinction we make though", "tokens": [50860, 486, 312, 4961, 13, 3764, 11, 1237, 412, 264, 1952, 3567, 382, 364, 1365, 13, 400, 472, 16844, 321, 652, 1673, 51208], "temperature": 0.0, "avg_logprob": -0.08173831080047178, "compression_ratio": 1.5487179487179488, "no_speech_prob": 0.004129295237362385}, {"id": 867, "seek": 569852, "start": 5715.400000000001, "end": 5724.200000000001, "text": " is that the modularity doesn't necessarily have to be built in by design. So this built-in approach", "tokens": [51208, 307, 300, 264, 31111, 507, 1177, 380, 4725, 362, 281, 312, 3094, 294, 538, 1715, 13, 407, 341, 3094, 12, 259, 3109, 51648], "temperature": 0.0, "avg_logprob": -0.08173831080047178, "compression_ratio": 1.5487179487179488, "no_speech_prob": 0.004129295237362385}, {"id": 868, "seek": 572420, "start": 5724.2, "end": 5729.72, "text": " we call architectural modularity where we have a language model and let's say a path and code", "tokens": [50364, 321, 818, 26621, 31111, 507, 689, 321, 362, 257, 2856, 2316, 293, 718, 311, 584, 257, 3100, 293, 3089, 50640], "temperature": 0.0, "avg_logprob": -0.12029341601450509, "compression_ratio": 1.7954545454545454, "no_speech_prob": 0.01405331026762724}, {"id": 869, "seek": 572420, "start": 5729.72, "end": 5733.639999999999, "text": " interpreter and we put them together and they're clearly different and they're doing different", "tokens": [50640, 34132, 293, 321, 829, 552, 1214, 293, 436, 434, 4448, 819, 293, 436, 434, 884, 819, 50836], "temperature": 0.0, "avg_logprob": -0.12029341601450509, "compression_ratio": 1.7954545454545454, "no_speech_prob": 0.01405331026762724}, {"id": 870, "seek": 572420, "start": 5733.639999999999, "end": 5739.16, "text": " things. So that can be promising, but then of course you need to know what the right modules are,", "tokens": [50836, 721, 13, 407, 300, 393, 312, 20257, 11, 457, 550, 295, 1164, 291, 643, 281, 458, 437, 264, 558, 16679, 366, 11, 51112], "temperature": 0.0, "avg_logprob": -0.12029341601450509, "compression_ratio": 1.7954545454545454, "no_speech_prob": 0.01405331026762724}, {"id": 871, "seek": 572420, "start": 5739.16, "end": 5745.08, "text": " you need to set them up in the right way. An alternative approach that might work for certain", "tokens": [51112, 291, 643, 281, 992, 552, 493, 294, 264, 558, 636, 13, 1107, 8535, 3109, 300, 1062, 589, 337, 1629, 51408], "temperature": 0.0, "avg_logprob": -0.12029341601450509, "compression_ratio": 1.7954545454545454, "no_speech_prob": 0.01405331026762724}, {"id": 872, "seek": 572420, "start": 5745.08, "end": 5751.24, "text": " cases is what we call emergent modularity where you start out with one network, but you don't", "tokens": [51408, 3331, 307, 437, 321, 818, 4345, 6930, 31111, 507, 689, 291, 722, 484, 365, 472, 3209, 11, 457, 291, 500, 380, 51716], "temperature": 0.0, "avg_logprob": -0.12029341601450509, "compression_ratio": 1.7954545454545454, "no_speech_prob": 0.01405331026762724}, {"id": 873, "seek": 575124, "start": 5751.32, "end": 5756.2, "text": " necessarily specify what parts need to be doing, what you let the network figure that out over the", "tokens": [50368, 4725, 16500, 437, 3166, 643, 281, 312, 884, 11, 437, 291, 718, 264, 3209, 2573, 300, 484, 670, 264, 50612], "temperature": 0.0, "avg_logprob": -0.10860511510059087, "compression_ratio": 1.830708661417323, "no_speech_prob": 0.0015718735521659255}, {"id": 874, "seek": 575124, "start": 5756.2, "end": 5761.0, "text": " course of training and you can have different parts self-specialized to do different things.", "tokens": [50612, 1164, 295, 3097, 293, 291, 393, 362, 819, 3166, 2698, 12, 7053, 1013, 1602, 281, 360, 819, 721, 13, 50852], "temperature": 0.0, "avg_logprob": -0.10860511510059087, "compression_ratio": 1.830708661417323, "no_speech_prob": 0.0015718735521659255}, {"id": 875, "seek": 575124, "start": 5761.639999999999, "end": 5766.599999999999, "text": " That might require some changes to the architecture to be able to promote the kind of", "tokens": [50884, 663, 1062, 3651, 512, 2962, 281, 264, 9482, 281, 312, 1075, 281, 9773, 264, 733, 295, 51132], "temperature": 0.0, "avg_logprob": -0.10860511510059087, "compression_ratio": 1.830708661417323, "no_speech_prob": 0.0015718735521659255}, {"id": 876, "seek": 575124, "start": 5766.599999999999, "end": 5772.36, "text": " specialization. It might require changes in the objective function, maybe next work prediction", "tokens": [51132, 2121, 2144, 13, 467, 1062, 3651, 2962, 294, 264, 10024, 2445, 11, 1310, 958, 589, 17630, 51420], "temperature": 0.0, "avg_logprob": -0.10860511510059087, "compression_ratio": 1.830708661417323, "no_speech_prob": 0.0015718735521659255}, {"id": 877, "seek": 575124, "start": 5772.36, "end": 5781.08, "text": " alone is not necessarily going to be good. And it might require changes in the training data", "tokens": [51420, 3312, 307, 406, 4725, 516, 281, 312, 665, 13, 400, 309, 1062, 3651, 2962, 294, 264, 3097, 1412, 51856], "temperature": 0.0, "avg_logprob": -0.10860511510059087, "compression_ratio": 1.830708661417323, "no_speech_prob": 0.0015718735521659255}, {"id": 878, "seek": 578124, "start": 5781.24, "end": 5788.12, "text": " kind of like what's happening with fine tuning today where you are feeding its specific problems", "tokens": [50364, 733, 295, 411, 437, 311, 2737, 365, 2489, 15164, 965, 689, 291, 366, 12919, 1080, 2685, 2740, 50708], "temperature": 0.0, "avg_logprob": -0.1348540712111067, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.0006981384940445423}, {"id": 879, "seek": 578124, "start": 5788.12, "end": 5793.0, "text": " that you're asking the model to do in a specific way so you might have selectively boost the", "tokens": [50708, 300, 291, 434, 3365, 264, 2316, 281, 360, 294, 257, 2685, 636, 370, 291, 1062, 362, 3048, 3413, 9194, 264, 50952], "temperature": 0.0, "avg_logprob": -0.1348540712111067, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.0006981384940445423}, {"id": 880, "seek": 578124, "start": 5793.0, "end": 5797.48, "text": " social reasoning and the formal reasoning and the factual knowledge. There might be specific", "tokens": [50952, 2093, 21577, 293, 264, 9860, 21577, 293, 264, 48029, 3601, 13, 821, 1062, 312, 2685, 51176], "temperature": 0.0, "avg_logprob": -0.1348540712111067, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.0006981384940445423}, {"id": 881, "seek": 578124, "start": 5797.48, "end": 5805.08, "text": " things you need to do, but there is a lot of promise in these approaches and the paper where", "tokens": [51176, 721, 291, 643, 281, 360, 11, 457, 456, 307, 257, 688, 295, 6228, 294, 613, 11587, 293, 264, 3035, 689, 51556], "temperature": 0.0, "avg_logprob": -0.1348540712111067, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.0006981384940445423}, {"id": 882, "seek": 578124, "start": 5805.08, "end": 5811.16, "text": " we introduced the formal and functional competence, it's something we started working on in 2020", "tokens": [51556, 321, 7268, 264, 9860, 293, 11745, 39965, 11, 309, 311, 746, 321, 1409, 1364, 322, 294, 4808, 51860], "temperature": 0.0, "avg_logprob": -0.1348540712111067, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.0006981384940445423}, {"id": 883, "seek": 581116, "start": 5811.48, "end": 5817.72, "text": " around the start of the pandemic. Language models were around then, but not nearly as advanced and", "tokens": [50380, 926, 264, 722, 295, 264, 5388, 13, 24445, 5245, 645, 926, 550, 11, 457, 406, 6217, 382, 7339, 293, 50692], "temperature": 0.0, "avg_logprob": -0.1495122236364028, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.005291280336678028}, {"id": 884, "seek": 581116, "start": 5817.72, "end": 5823.88, "text": " as we were writing the paper and in fact after the initial preprint version came out, that's when", "tokens": [50692, 382, 321, 645, 3579, 264, 3035, 293, 294, 1186, 934, 264, 5883, 659, 14030, 3037, 1361, 484, 11, 300, 311, 562, 51000], "temperature": 0.0, "avg_logprob": -0.1495122236364028, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.005291280336678028}, {"id": 885, "seek": 581116, "start": 5823.88, "end": 5830.28, "text": " we started seeing the field, the developers shifting away from this simple scaling up approach,", "tokens": [51000, 321, 1409, 2577, 264, 2519, 11, 264, 8849, 17573, 1314, 490, 341, 2199, 21589, 493, 3109, 11, 51320], "temperature": 0.0, "avg_logprob": -0.1495122236364028, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.005291280336678028}, {"id": 886, "seek": 581116, "start": 5830.28, "end": 5835.32, "text": " that's not the approach that's common anymore. People have started to shift towards specialized", "tokens": [51320, 300, 311, 406, 264, 3109, 300, 311, 2689, 3602, 13, 3432, 362, 1409, 281, 5513, 3030, 2121, 1602, 51572], "temperature": 0.0, "avg_logprob": -0.1495122236364028, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.005291280336678028}, {"id": 887, "seek": 583532, "start": 5835.32, "end": 5841.32, "text": " fine tuning, using very targeted data sets to improve performance on specific domains,", "tokens": [50364, 2489, 15164, 11, 1228, 588, 15045, 1412, 6352, 281, 3470, 3389, 322, 2685, 25514, 11, 50664], "temperature": 0.0, "avg_logprob": -0.16456075395856584, "compression_ratio": 1.6280701754385964, "no_speech_prob": 0.13457231223583221}, {"id": 888, "seek": 583532, "start": 5841.32, "end": 5846.44, "text": " coupling an LLM with external modules, all of those things that we kind of suggested that might", "tokens": [50664, 37447, 364, 441, 43, 44, 365, 8320, 16679, 11, 439, 295, 729, 721, 300, 321, 733, 295, 10945, 300, 1062, 50920], "temperature": 0.0, "avg_logprob": -0.16456075395856584, "compression_ratio": 1.6280701754385964, "no_speech_prob": 0.13457231223583221}, {"id": 889, "seek": 583532, "start": 5846.44, "end": 5851.32, "text": " be good because that's more brain-like. RAG became big, all of those things are something we've seen", "tokens": [50920, 312, 665, 570, 300, 311, 544, 3567, 12, 4092, 13, 14626, 38, 3062, 955, 11, 439, 295, 729, 721, 366, 746, 321, 600, 1612, 51164], "temperature": 0.0, "avg_logprob": -0.16456075395856584, "compression_ratio": 1.6280701754385964, "no_speech_prob": 0.13457231223583221}, {"id": 890, "seek": 583532, "start": 5851.32, "end": 5858.44, "text": " over the past year that's very encouraging that now the AI field is also recognizing that it's", "tokens": [51164, 670, 264, 1791, 1064, 300, 311, 588, 14580, 300, 586, 264, 7318, 2519, 307, 611, 18538, 300, 309, 311, 51520], "temperature": 0.0, "avg_logprob": -0.16456075395856584, "compression_ratio": 1.6280701754385964, "no_speech_prob": 0.13457231223583221}, {"id": 891, "seek": 583532, "start": 5858.44, "end": 5862.2, "text": " not just about scale, that you do benefit from different components working together.", "tokens": [51520, 406, 445, 466, 4373, 11, 300, 291, 360, 5121, 490, 819, 6677, 1364, 1214, 13, 51708], "temperature": 0.0, "avg_logprob": -0.16456075395856584, "compression_ratio": 1.6280701754385964, "no_speech_prob": 0.13457231223583221}, {"id": 892, "seek": 586220, "start": 5863.16, "end": 5873.48, "text": " Yeah, I think very exciting to see what comes next, both in your work and in the field of LLMs,", "tokens": [50412, 865, 11, 286, 519, 588, 4670, 281, 536, 437, 1487, 958, 11, 1293, 294, 428, 589, 293, 294, 264, 2519, 295, 441, 43, 26386, 11, 50928], "temperature": 0.0, "avg_logprob": -0.1898047924041748, "compression_ratio": 1.352112676056338, "no_speech_prob": 0.0027072555385529995}, {"id": 893, "seek": 586220, "start": 5873.48, "end": 5879.48, "text": " which it seems like maybe someone's listening to what you're suggesting because all these things", "tokens": [50928, 597, 309, 2544, 411, 1310, 1580, 311, 4764, 281, 437, 291, 434, 18094, 570, 439, 613, 721, 51228], "temperature": 0.0, "avg_logprob": -0.1898047924041748, "compression_ratio": 1.352112676056338, "no_speech_prob": 0.0027072555385529995}, {"id": 894, "seek": 587948, "start": 5879.5599999999995, "end": 5889.4, "text": " are happening. Yeah, I don't know if you have any final comments or predictions,", "tokens": [50368, 366, 2737, 13, 865, 11, 286, 500, 380, 458, 498, 291, 362, 604, 2572, 3053, 420, 21264, 11, 50860], "temperature": 0.0, "avg_logprob": -0.12973378344279965, "compression_ratio": 1.3082706766917294, "no_speech_prob": 0.21372370421886444}, {"id": 895, "seek": 587948, "start": 5891.959999999999, "end": 5902.839999999999, "text": " warnings of doom often come up in these discussions, but this has been surprisingly positive.", "tokens": [50988, 30009, 295, 37131, 2049, 808, 493, 294, 613, 11088, 11, 457, 341, 575, 668, 17600, 3353, 13, 51532], "temperature": 0.0, "avg_logprob": -0.12973378344279965, "compression_ratio": 1.3082706766917294, "no_speech_prob": 0.21372370421886444}, {"id": 896, "seek": 590284, "start": 5903.16, "end": 5910.52, "text": " No, I just think that science is important and we just need to use good methods and not run after", "tokens": [50380, 883, 11, 286, 445, 519, 300, 3497, 307, 1021, 293, 321, 445, 643, 281, 764, 665, 7150, 293, 406, 1190, 934, 50748], "temperature": 0.0, "avg_logprob": -0.13439364660353886, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.0239943228662014}, {"id": 897, "seek": 590284, "start": 5910.52, "end": 5916.84, "text": " the hype and be realistic in how we evaluate the strengths and limitations of these models.", "tokens": [50748, 264, 24144, 293, 312, 12465, 294, 577, 321, 13059, 264, 16986, 293, 15705, 295, 613, 5245, 13, 51064], "temperature": 0.0, "avg_logprob": -0.13439364660353886, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.0239943228662014}, {"id": 898, "seek": 590284, "start": 5917.72, "end": 5921.72, "text": " There are strengths, there are limitations, so being too far on just the positive and just", "tokens": [51108, 821, 366, 16986, 11, 456, 366, 15705, 11, 370, 885, 886, 1400, 322, 445, 264, 3353, 293, 445, 51308], "temperature": 0.0, "avg_logprob": -0.13439364660353886, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.0239943228662014}, {"id": 899, "seek": 590284, "start": 5921.72, "end": 5926.360000000001, "text": " negative is not necessarily the most productive. We just want to be able to disentangle them", "tokens": [51308, 3671, 307, 406, 4725, 264, 881, 13304, 13, 492, 445, 528, 281, 312, 1075, 281, 37313, 7846, 552, 51540], "temperature": 0.0, "avg_logprob": -0.13439364660353886, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.0239943228662014}, {"id": 900, "seek": 592636, "start": 5926.36, "end": 5933.719999999999, "text": " effectively. Yeah, Rene, thank you so much, Anna. This has been really insightful. Yeah, thank you.", "tokens": [50364, 8659, 13, 865, 11, 497, 1450, 11, 1309, 291, 370, 709, 11, 12899, 13, 639, 575, 668, 534, 46401, 13, 865, 11, 1309, 291, 13, 50732], "temperature": 0.0, "avg_logprob": -0.35610192162649973, "compression_ratio": 1.125, "no_speech_prob": 0.0767873227596283}, {"id": 901, "seek": 595636, "start": 5956.36, "end": 5957.74, "text": " you", "tokens": [50404, 291, 50433], "temperature": 0.0, "avg_logprob": -0.951136589050293, "compression_ratio": 0.2727272727272727, "no_speech_prob": 0.9285953044891357}], "language": "en"}