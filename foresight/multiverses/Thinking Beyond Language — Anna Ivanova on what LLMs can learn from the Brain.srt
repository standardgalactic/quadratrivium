1
00:00:00,000 --> 00:00:04,940
I'm James Robinson, you're listening to Multiverses.

2
00:00:04,940 --> 00:00:10,880
Language can do and express many things, and in fact this was the subject of my last conversation

3
00:00:10,880 --> 00:00:15,360
on this podcast with Nicol Krishnam talking about ordinary language philosophy.

4
00:00:15,360 --> 00:00:21,560
Just because language is so powerful, we might be tempted to think that that's all we need

5
00:00:21,560 --> 00:00:23,780
for understanding and predicting the world.

6
00:00:23,780 --> 00:00:27,720
It's just manipulation of symbols, next word prediction.

7
00:00:28,440 --> 00:00:34,760
However, if we look at the how the human brain at least actually works, it's rather different.

8
00:00:34,760 --> 00:00:37,320
Our guest this week is Anna Ivanova.

9
00:00:37,320 --> 00:00:41,400
She's an assistant professor at Georgia Institute of Technology,

10
00:00:41,400 --> 00:00:45,240
and she tries to understand the relationship between language and thought,

11
00:00:45,240 --> 00:00:51,480
and she does this by looking at brain scans, essentially MRIs, of what's going on when

12
00:00:51,480 --> 00:00:54,200
humans are presented with particular scenarios.

13
00:00:54,200 --> 00:00:58,920
For example, I'm looking at this marvellous view right now from Carlton Hill, Edinburgh.

14
00:00:58,920 --> 00:01:02,920
I'm not thinking about it linguistically, it's going straight into my visual cortex

15
00:01:02,920 --> 00:01:06,440
and processes happening there, and if I want to reason about it,

16
00:01:06,440 --> 00:01:09,080
I'm not going to reason about it linguistically either.

17
00:01:10,200 --> 00:01:15,880
A lot of her work looks at how conceptual knowledge of the world is not tied to the language area

18
00:01:15,880 --> 00:01:21,560
of our brain, so for example subjects with aphantasia, people who have large-scale

19
00:01:21,560 --> 00:01:27,480
damage to the language network, are still able to reason not only logically about

20
00:01:27,480 --> 00:01:31,320
chess problems and things like that, but they can reason socially as well.

21
00:01:31,320 --> 00:01:34,280
They can understand what situations are unusual.

22
00:01:35,960 --> 00:01:41,320
So this is a really insightful and very timely conversation because it plays into a lot of

23
00:01:41,320 --> 00:01:47,720
the enthusiasm about LOMs, which I certainly buy into myself, but it calls into question

24
00:01:47,720 --> 00:01:56,680
some of this, forcing us to think, well, what is necessary on top of simple linguistic abilities

25
00:01:56,680 --> 00:02:00,120
to really be a fully-fledged thinking machine?

26
00:02:00,920 --> 00:02:06,920
I think one question that still exists in my mind is, to what extent just language

27
00:02:06,920 --> 00:02:11,320
manipulation could get us to a fully thinking machine, somewhat in the same way as, you know,

28
00:02:11,320 --> 00:02:21,080
I have a GPU and a CPU on this laptop, and I could use my CPU to play, you know, vector-based

29
00:02:21,640 --> 00:02:26,280
computer games, and I could use my GPU to send emails, but they're not really suited to that task,

30
00:02:26,280 --> 00:02:33,640
but the point is maybe language could be a kind of fully-fledged thinking system.

31
00:02:33,640 --> 00:02:37,960
However, I think it is valuable to learn from, in fact, what the brain does, of course.

32
00:02:38,680 --> 00:02:41,160
So I really enjoyed this conversation. I hope you did too.

33
00:02:57,400 --> 00:03:00,280
Hi, Anna Ivanova, welcome to Multiverses.

34
00:03:01,320 --> 00:03:02,680
Hi, thank you for having me.

35
00:03:03,400 --> 00:03:09,240
So we're speaking and we're thinking, I think, and people who are listening are listening to our

36
00:03:09,240 --> 00:03:15,640
words and they're thinking, and thought seems like something that should be really familiar to us,

37
00:03:15,640 --> 00:03:21,560
because it's one of those kind of few things that we have really direct access to, and yet,

38
00:03:21,560 --> 00:03:27,400
at the same time, it seems so mysterious, so hard to figure out exactly what's going on.

39
00:03:27,400 --> 00:03:31,320
Maybe it's because the piece that's doing the figuring out is the thing that we're trying to

40
00:03:31,320 --> 00:03:38,520
figure out itself, I don't know. But yeah, how can we get some sort of grip on what thought is?

41
00:03:39,800 --> 00:03:40,600
Where do we start?

42
00:03:42,280 --> 00:03:49,880
Well, I think we need to start with definitions, what it is that we mean by thought,

43
00:03:49,880 --> 00:03:56,280
because different people use the word in different senses, and of course, it also depends on the

44
00:03:56,280 --> 00:04:06,600
context. And generally speaking, at least in my area of work, I think there is the broad definition

45
00:04:06,600 --> 00:04:13,880
and the narrow definition. And in the broad definition, thought is synonymous with cognition.

46
00:04:13,880 --> 00:04:24,360
So the mental processes that we use to make sense of the world around us, so that includes reasoning,

47
00:04:24,440 --> 00:04:32,280
that includes accessing memories, that includes various social communication capacities,

48
00:04:32,280 --> 00:04:40,600
so very broadly speaking, something we would call cognition. And then the narrow definition is the

49
00:04:40,600 --> 00:04:48,520
stuff that happens kind of like in between us doing things. So it's not necessarily you get a math

50
00:04:48,520 --> 00:04:55,800
problem, your reason about it, you give the result. But it's more about you lying down in bed at night

51
00:04:55,800 --> 00:05:04,440
and thinking about your day tomorrow, or you're walking somewhere and you're playing out your

52
00:05:05,000 --> 00:05:10,840
conversation that you're going to have with a friend. And so this kind of inner thinking that

53
00:05:10,840 --> 00:05:17,480
happens spontaneously, not in response to any external task is also something where people

54
00:05:18,360 --> 00:05:22,520
that people commonly refer to as thought. And of course, those are very different, right? Like

55
00:05:22,520 --> 00:05:27,080
if we're talking about the broad thing or the narrow thing, this specific kind of mechanisms

56
00:05:27,080 --> 00:05:33,240
that might support them might differ quite a bit. Yeah, I think both of those kind of definitions

57
00:05:34,120 --> 00:05:42,200
capture something of what one intuitively would characterize as thinking. So we might say,

58
00:05:43,160 --> 00:05:48,520
you know, broadly speaking, oh, yeah, of course, when this person solved that math problem,

59
00:05:48,520 --> 00:05:52,840
they had to be thinking. But then you might also say, oh, they did it so quickly, they just did it

60
00:05:52,840 --> 00:05:57,480
without thinking, which would be sort of more of the narrow definition, they did it sort of

61
00:05:58,040 --> 00:06:03,000
without any kind of reflection, which is kind of what the narrower definition requires, I guess.

62
00:06:04,680 --> 00:06:08,040
I'm curious, do you have a kind of preference for either of those definitions, or do you think

63
00:06:08,120 --> 00:06:15,160
they both serve a kind of useful purpose? I've used both. I'm interested in the relationship

64
00:06:15,160 --> 00:06:20,680
between language and thought, the role that language plays in thinking. And so the role

65
00:06:20,680 --> 00:06:27,800
that language plays in thinking broadly defined thinking as cognition, that actually turns out to

66
00:06:27,800 --> 00:06:33,640
be a more tractable question, because we can ask people to do a math problem and look whether

67
00:06:33,640 --> 00:06:40,120
language processing regions in the brain are engaged. But for inner thinking,

68
00:06:40,120 --> 00:06:45,720
stuff that happens spontaneously without an external task, that's much harder to capture.

69
00:06:45,720 --> 00:06:50,120
But that's also where people have very strong intuitions there. Of course, I think in words,

70
00:06:50,120 --> 00:06:56,520
or of course, I think without words. And so that area is harder to study, but also very

71
00:06:56,520 --> 00:06:59,640
interesting. And so that's where I see some of my future work going.

72
00:07:00,600 --> 00:07:05,720
Interesting. Yeah, it's true that kind of more reflexive thinking almost by definition,

73
00:07:05,720 --> 00:07:08,760
people are going to have opinions about it, because they are kind of

74
00:07:09,400 --> 00:07:16,520
cogitacing, turning things over. And part of that process is inward looking. So yeah,

75
00:07:16,520 --> 00:07:20,040
people are going to be like, oh, well, I always do that with words or with images or a mix.

76
00:07:21,000 --> 00:07:25,640
And yet sometimes I can also find it a little bit hard to remember to do that and think about,

77
00:07:25,720 --> 00:07:30,200
because whenever I sort of think about, if I try to think about what thinking is,

78
00:07:30,200 --> 00:07:33,400
I will do it linguistically. But maybe that's just because it's the sort of,

79
00:07:34,280 --> 00:07:37,880
that's the sort of way that I need to think about that thing. Whereas if I think about

80
00:07:37,880 --> 00:07:42,280
something entirely different, like a kind of, you know, spatial reasoning problem,

81
00:07:42,280 --> 00:07:48,040
I'm sure I would do it in a different way. But it maybe wouldn't engage the linguistic part of my

82
00:07:48,600 --> 00:07:56,840
brain. But anyway, I guess, yeah, this is a very, we've got very quickly to a very

83
00:07:58,600 --> 00:08:02,280
key area, which is this, this, this relationship between language and thought, which

84
00:08:04,600 --> 00:08:11,320
my last guest was Nikol Krishnan, an ordinary language philosopher. And we spent some time

85
00:08:11,320 --> 00:08:15,720
just talking about, well, how for a while, people just thought there was no difference

86
00:08:15,720 --> 00:08:24,120
between the two. There was, or rather, in some way, language captured the entirety of everything,

87
00:08:24,120 --> 00:08:28,280
including thought, maybe including some other things. So, you know, Wittgenstein's famous

88
00:08:28,280 --> 00:08:33,160
dictum, the limits of my language, the limits are the limits of, or mean the limits of my world.

89
00:08:36,200 --> 00:08:42,200
But I guess your research is maybe questioning that. Would that be fair to say?

90
00:08:42,440 --> 00:08:50,840
Yeah, I use Wittgenstein's quote as an example of a worldview, a paradigm that I'm pushing against.

91
00:08:53,000 --> 00:09:00,760
And what's wrong with it? I mean, as we said, like, when we try to describe what thought is,

92
00:09:00,760 --> 00:09:11,480
we'll probably reach for language. And yeah, it seems like so many of the ideas,

93
00:09:11,480 --> 00:09:15,880
I don't want to say everything, but so much of the ideas that we have, and we pass on, we do

94
00:09:16,840 --> 00:09:21,480
with language. And maybe there's an argument that the places where we're not doing it with

95
00:09:21,480 --> 00:09:29,480
language are somehow dependent on language behind the scenes. But that's not a very scientific

96
00:09:29,480 --> 00:09:34,440
argument. And I think you have some kind of evidence to the contrary. So, yeah, maybe take us

97
00:09:34,440 --> 00:09:42,200
through some of the things that you've done to probe this. Yeah, let me, there is a lot to say.

98
00:09:42,200 --> 00:09:49,800
So, let me start first by, I guess, acknowledging the last bit that you said, where clearly,

99
00:09:49,800 --> 00:09:57,160
there is a relationship between language and thought. And the most trivial, but also important

100
00:09:57,160 --> 00:10:03,240
one is the fact that we use language to transmit information, to communicate thoughts to one

101
00:10:03,240 --> 00:10:08,840
another. And that's a very powerful mechanism, we can translate knowledge from generation to

102
00:10:08,840 --> 00:10:17,000
generation. So that is a very important role of language in thinking, helping us share information

103
00:10:17,000 --> 00:10:23,160
without having to figure out every single thing individually. But here, what we're talking about

104
00:10:23,160 --> 00:10:29,720
is using language as a medium of thinking. So internally, do we think in words, do we recruit

105
00:10:29,800 --> 00:10:36,200
the mechanisms for language processing when we're thinking? And so that's an important distinction.

106
00:10:36,200 --> 00:10:44,360
So that's the scope. Now, as we said, people have strong intuitions about whether or not

107
00:10:44,360 --> 00:10:52,120
they use language to think. And probably that these intuitions are grounded in people's personal

108
00:10:52,120 --> 00:11:00,760
experiences thinking. And so one important fact to keep in mind is that there is huge

109
00:11:00,760 --> 00:11:09,000
individual variability in how people perceive their own thinking to be. And so my pet theory

110
00:11:09,000 --> 00:11:15,960
is that a lot of philosophers are strong verbal thinkers, they spend a lot of time writing,

111
00:11:15,960 --> 00:11:23,000
they think about abstract topics. And so to them, the link between language and thought and

112
00:11:23,000 --> 00:11:30,360
their experience is very strong. And it just seems that people, not just philosophers,

113
00:11:31,080 --> 00:11:38,120
have this tendency to assume that everybody else thinks in kind of the same way. And so

114
00:11:38,760 --> 00:11:44,120
if you are a strong verbal thinker, you automatically assume that everybody else is as well.

115
00:11:44,680 --> 00:11:51,400
Until you start actually talking to other people and asking about their experiences.

116
00:11:51,400 --> 00:11:57,080
And so I've had these conversations with people at parties or just informally, you ask them,

117
00:11:57,080 --> 00:12:03,720
oh, hey, how often do you think in words? Most of the time, some of the time never. And people

118
00:12:03,720 --> 00:12:08,920
are always surprised to hear that other people's experiences might be completely different. And

119
00:12:08,920 --> 00:12:18,040
so this is just, I think, a very important thing to keep in mind that our intuitions

120
00:12:18,040 --> 00:12:23,400
can lead us because they don't necessarily reflect a universal human experience. It's just,

121
00:12:23,400 --> 00:12:33,320
you know, that's how we think. And so on to the actual evidence that we can use to dissociate

122
00:12:33,320 --> 00:12:45,160
language and thought. There are a few different strands. So one example, very briefly, is the fact

123
00:12:45,160 --> 00:12:51,880
that animals who don't have language might often have pretty sophisticated thought and planning

124
00:12:51,880 --> 00:13:01,960
capabilities, right? We know examples of crows being very smart, alphoctopi, even a squirrel that

125
00:13:01,960 --> 00:13:06,760
is trying to figure out whether to jump from tree to tree or if it's too far and it needs to go on

126
00:13:06,760 --> 00:13:13,640
the ground instead. These are pretty sophisticated capabilities. And so that's just a very basic

127
00:13:13,640 --> 00:13:18,760
example of at least some kinds of thought. You can then argue, you know, oh, but like the kind of

128
00:13:18,760 --> 00:13:22,600
thinking they're doing is not, you know, the kind of thinking that we care about, right? And that's

129
00:13:22,600 --> 00:13:27,400
where the meat of the debate is. But pretty sophisticated cognition is possible in non-human

130
00:13:27,400 --> 00:13:37,560
animals from what we know. For me, I work with humans and adult humans. And so what we can do

131
00:13:37,560 --> 00:13:47,560
is we can identify parts of the brain that are engaged in language processing. So it turns out

132
00:13:47,560 --> 00:13:55,800
that there is a set of brain regions in the brain known as the language network that are responsible

133
00:13:55,800 --> 00:14:01,480
for language comprehension. So whether you're listening to somebody talk or reading, they're

134
00:14:01,480 --> 00:14:05,960
also engaged during language production. So when you're speaking and when you're writing,

135
00:14:06,920 --> 00:14:13,320
they are engaged in response to any language that you might know that also includes sign languages.

136
00:14:13,320 --> 00:14:21,080
So it doesn't even have to be a spoken language. And these regions turns out are pretty selective

137
00:14:21,080 --> 00:14:28,920
for language. So they respond to all kinds of language, but not to music, not to math,

138
00:14:29,560 --> 00:14:39,560
not to general problem solving. And so this is pretty strong evidence that language and many

139
00:14:39,560 --> 00:14:45,000
different kinds of thinking are actually separable in the brain. That language has its own neural

140
00:14:45,000 --> 00:14:54,120
machinery. And that's important because it turns out that if language areas of the brain

141
00:14:54,120 --> 00:15:01,160
are damaged, it will affect your ability to use language, but not necessarily your ability to

142
00:15:01,160 --> 00:15:09,240
think. And so the most common example of that is a condition known as aphasia. So it difficulties

143
00:15:09,240 --> 00:15:15,640
with language production or comprehension. Often, most commonly, it arises as a result of

144
00:15:16,280 --> 00:15:21,640
a person having a stroke. And so if it's stroke effects left hemisphere, which is where the

145
00:15:21,640 --> 00:15:27,160
language network is in most people, they might have really serious difficulties with language

146
00:15:27,160 --> 00:15:33,880
production or comprehension. But if that language is limited to the language network, it turns out

147
00:15:33,880 --> 00:15:42,920
that their ability to use other kinds of thinking remains intact. So these people with really severe

148
00:15:42,920 --> 00:15:50,920
aphasia who really can't understand language or speak, they can solve math problems. They can

149
00:15:50,920 --> 00:15:57,720
arrange pictures so they form a story so they can reason about cause and effect. They can look at

150
00:15:57,720 --> 00:16:02,680
the picture or show in some kind of event, like a fox cheese in the rabbit or the rabbit cheese in

151
00:16:02,680 --> 00:16:08,600
the fox and say which one is more likely to happen in the real world. If it's something like really

152
00:16:08,600 --> 00:16:14,920
weird, like the scuba diver biting a shark, they will laugh because it's just kind of ridiculous.

153
00:16:14,920 --> 00:16:19,160
And so, you know, you can tell that they understand what's going on. And there are really fascinating

154
00:16:19,160 --> 00:16:23,800
cases, you know, some of them like like the plague chest on the weekend. So clearly, very

155
00:16:23,800 --> 00:16:30,520
sophisticated forms of reasoning are preserved even in the face of severe damage to language

156
00:16:30,520 --> 00:16:35,960
processing centres in the brain. Yeah, that's completely fascinating. And well, firstly, by the

157
00:16:35,960 --> 00:16:45,880
way, I love your theory about philosophers and how maybe the sort of minds that they have that

158
00:16:45,880 --> 00:16:54,600
make them good philosophers sort of self select or selecting a very biased or unusual community of

159
00:16:54,600 --> 00:17:02,520
people who think in a particular way. And yeah, so that's really interesting. It'd be great to have a

160
00:17:02,520 --> 00:17:08,920
survey of philosophers, I don't know if this has happened, and how they describe their own thoughts

161
00:17:08,920 --> 00:17:16,360
and compare it to other groups. Yeah, very interesting. Yeah, I think sometimes that maybe I

162
00:17:16,360 --> 00:17:19,720
should, you know, not talk about this theory and actually test it experimentally first,

163
00:17:19,800 --> 00:17:27,000
that we add on biased people in advance. No, as long as they don't listen to this,

164
00:17:27,000 --> 00:17:32,520
or maybe you can do it anonymously or something. I'm actually talking, I think,

165
00:17:33,480 --> 00:17:39,560
soon to someone who from the philosophy of science who surveyed physicists to see if they are

166
00:17:39,560 --> 00:17:43,320
realists in terms of, you know, how they think about the entities of science or not. So I think

167
00:17:43,320 --> 00:17:48,120
it's like it is really interesting to actually just, yeah, try to figure out how it is that

168
00:17:50,680 --> 00:17:56,520
yeah, how it is that people's personal beliefs and their kind of academic disciplines or their own

169
00:17:56,520 --> 00:18:02,840
or the peculiarities of their minds intertwine. But coming back to the kind of experiments that

170
00:18:02,840 --> 00:18:10,040
you describe, so yeah, I think these are just really, yeah, wonderful illustrations of how

171
00:18:10,040 --> 00:18:16,200
thought maybe extends beyond the language network. And I suppose what you're doing is you're asking

172
00:18:16,200 --> 00:18:22,760
people, so for example, the way that we know that music is not within the language network is,

173
00:18:23,560 --> 00:18:27,880
I don't know, the language network is defined as the kind of a bit of the brain that lights up

174
00:18:27,880 --> 00:18:33,800
in MRI scans. You see a lot of activity there when you give people sentences and linguistic

175
00:18:34,760 --> 00:18:40,120
tasks, I don't know, maybe reading or producing language. And then it's a different area of the

176
00:18:40,120 --> 00:18:45,800
brain that lights up when they're listening to music or when they're solving a math or chess

177
00:18:45,800 --> 00:18:55,080
problem. And even when people have quite severe damage at South Asia, and the language part of

178
00:18:55,080 --> 00:19:00,360
the brain is unable to comprehend or produce language or both, they can still do many of

179
00:19:00,360 --> 00:19:05,880
those other things, which is, I mean, that's really interesting for one thing, because I often

180
00:19:05,880 --> 00:19:12,680
think as well, language is maybe being so key to the kind of input output of the brain that,

181
00:19:13,400 --> 00:19:20,120
you know, for example, reading a math problem would kind of go via the language network,

182
00:19:20,120 --> 00:19:26,120
or is it that our brain is sort of able to just kind of directly take those symbols into,

183
00:19:26,120 --> 00:19:33,080
I don't know, a different area of the brain, or perhaps do we have to kind of pose those

184
00:19:33,080 --> 00:19:39,320
problems in a kind of more visual way? I don't know, I'm curious about whether the language

185
00:19:39,320 --> 00:19:46,200
network is kind of a gateway for much of the information going in. So it looks like if you

186
00:19:46,200 --> 00:19:53,640
give people math problems in the form of mathematical symbols, right, like five plus three,

187
00:19:53,640 --> 00:19:59,000
first question mark, it seems like it doesn't need to go through the language network. So even

188
00:19:59,000 --> 00:20:06,680
though it's symbolic, not all symbols get processed by the language network. And perhaps even more

189
00:20:06,680 --> 00:20:16,280
strikingly, one study that I did in graduate school was looking at computer code. And specifically,

190
00:20:16,280 --> 00:20:26,120
we looked at Python, which is very English-like by design, so it uses English words. And on the

191
00:20:26,120 --> 00:20:31,400
other end of the spectrum, we took a graphical programming language for kids called Scratch

192
00:20:31,400 --> 00:20:37,320
Junior. So it has different characters. And so then you have different arrows showing, you know,

193
00:20:37,320 --> 00:20:42,280
the characters going left or jumping. But it has a lot of the same control flow elements that you

194
00:20:42,280 --> 00:20:50,520
would have in text-based code, like if statements and for loops and stuff like that. And so it turns

195
00:20:50,520 --> 00:20:55,800
out that for both of these languages, the main network in the brain that's responsible for

196
00:20:55,800 --> 00:21:02,920
extracting meaning from that code is the so-called multiple demand network. And so that's the

197
00:21:02,920 --> 00:21:09,160
network that's responsible for problem solving and reasoning and planning, and not the language

198
00:21:09,160 --> 00:21:15,480
network. The language network responded a little bit to Python code. But even there, we actually

199
00:21:15,480 --> 00:21:21,400
weren't able to exactly establish its relation, its role and why it would. It might be some kind of

200
00:21:21,400 --> 00:21:25,960
false positive where the language network is like, oh, that's language, oh, wait, no, never mind. And

201
00:21:25,960 --> 00:21:31,720
it kind of goes down. So there are other researchers that are promoting that theory currently. But

202
00:21:33,640 --> 00:21:41,800
even for code, we call programming languages languages because how similar they are

203
00:21:41,800 --> 00:21:47,240
structurally to natural languages. Even there, it looks like it's not the language network that's

204
00:21:47,240 --> 00:21:53,160
doing the majority of the heavy lifting. Yeah, I found that completely, well, surprising actually.

205
00:21:53,160 --> 00:21:58,280
And I think you noted in the paper that people kind of fell on two sides of the fence. Some people

206
00:21:58,280 --> 00:22:02,680
were surprised and some people were, oh, no, that makes complete sense. But I was personally

207
00:22:03,480 --> 00:22:08,600
really surprised because I, as you say, there's so much similarity between the way that natural

208
00:22:08,600 --> 00:22:14,680
language works in terms of being compositional and having these kind of hierarchical features

209
00:22:14,680 --> 00:22:18,360
and the way that programming languages work that I would think, okay, well, you know,

210
00:22:21,560 --> 00:22:26,600
it's right that we call them languages because they're so closely related, they're just sort of,

211
00:22:27,640 --> 00:22:36,440
I guess, a bit stricter, less ambiguous, perhaps. But the kind of, the nature of the rules is not

212
00:22:36,440 --> 00:22:44,920
so different. And yet, yeah, it's almost as if one could imagine there being some sort of animal,

213
00:22:44,920 --> 00:22:49,400
like a crow, like you said, like very intelligent creature, doesn't have language, but maybe it's

214
00:22:49,400 --> 00:22:54,200
got a really good multiple demand network. And perhaps we could, perhaps could be a really good

215
00:22:54,200 --> 00:22:59,240
programmer, because it's not that part of the brain, which is being recruited. But it's rather

216
00:22:59,240 --> 00:23:05,160
this kind of almost clearinghouse from what I understand, the multiple demand network just

217
00:23:05,160 --> 00:23:14,280
picks up so many different jobs. The other thing that really stood out for me in this paper,

218
00:23:14,280 --> 00:23:20,280
which I really enjoyed, was that as a kind of, I guess, control, you could, you presented people

219
00:23:20,280 --> 00:23:29,640
the same problems. So the, if I remember rightly, one of the, one of the code pieces of code that

220
00:23:29,640 --> 00:23:35,240
people had to interpret in Python was a calculation of BMI. And so it's like, here is a variable,

221
00:23:35,240 --> 00:23:40,680
which is your weight, here is one issue is your height, BMI equals height divided by weight squared.

222
00:23:42,280 --> 00:23:46,600
And so the person kind of reads through that. And, and you see it being passed off to the

223
00:23:46,600 --> 00:23:54,120
multiple demand network. But then there's the same problem defined entirely verbally. So instead

224
00:23:54,120 --> 00:23:58,680
of using, you know, symbols with equals, and it clearly being Python code, it's just, I know,

225
00:23:58,680 --> 00:24:03,800
this is what BMI is, here, you know, here's how much you weigh, here's how tall you are, what's

226
00:24:03,800 --> 00:24:11,560
your, what's your BMI. And that went to a different region of the brain, which for me was just like,

227
00:24:11,560 --> 00:24:17,800
okay, well, this is the same problem, but the way that it's presented really changes the way that

228
00:24:17,800 --> 00:24:25,080
we think about it. Which, yeah, that was another huge surprise for me to think just how influential

229
00:24:25,080 --> 00:24:33,560
the kind of presentation or the, the medium, I guess, for a set of concepts, how much that,

230
00:24:33,560 --> 00:24:37,320
that determines how those concepts are handled internally, mentally.

231
00:24:39,000 --> 00:24:46,280
Yeah. And in fact, that's not that uncommon of a situation if you think about it. So let's say

232
00:24:46,280 --> 00:24:52,840
somebody is listening to this podcast, versus reading the transcript, the way information

233
00:24:52,840 --> 00:24:59,000
gets into the brain is different. So in the auditory modality, it goes to the auditory cortex

234
00:24:59,000 --> 00:25:05,240
first. And in the visual modality, it goes through the visual cortex first, and then it gets into,

235
00:25:05,240 --> 00:25:10,200
we have a specialized part of the brain that's responsible for readings of recognizing written

236
00:25:10,200 --> 00:25:16,440
letters. But then they will converge in the language network, because language network

237
00:25:17,160 --> 00:25:24,280
is responsible for processing either a modality. And that means that these initially distinct

238
00:25:24,280 --> 00:25:32,680
representations have to converge in some, in some way. And so for some of the other cases,

239
00:25:32,680 --> 00:25:41,320
like a problem that's written in language versus in code, it looks like that convergence is also

240
00:25:41,400 --> 00:25:46,920
happening, but it's happening later on in the processing, right? So it goes, you know, through

241
00:25:46,920 --> 00:25:53,000
the language network, and I guess the multiple demand. And then you have some shared problem

242
00:25:53,000 --> 00:26:00,040
solving. So in this case, calculating the BMI is doing some math. And so that we think also happens

243
00:26:00,040 --> 00:26:03,960
in the multiple demand network. And in fact, we show in the paper that you can kind of break down

244
00:26:03,960 --> 00:26:08,360
that activity that we capture into the code reading part, an actual problem solving part.

245
00:26:08,840 --> 00:26:16,680
But it's a fascinating endeavor. In general, in cognitive neuroscience, how do we design

246
00:26:16,680 --> 00:26:22,840
an experiment where we have those kinds of different conditions, where they're very similar,

247
00:26:22,840 --> 00:26:29,960
except for something that we've changed. And so at what point that difference, right, auditor versus

248
00:26:29,960 --> 00:26:36,360
visual language versus code, where in the brain does that make a difference? And where doesn't it?

249
00:26:37,320 --> 00:26:44,200
Yeah, yeah. So you're saying that even though it goes the language area lights up when we have

250
00:26:44,200 --> 00:26:49,160
that kind of BMI problem, it's just kind of passing the thing. And then it gets passed off to

251
00:26:50,600 --> 00:26:55,000
you know, to actually run the calculation that happens, like that doesn't happen in the language

252
00:26:55,960 --> 00:27:02,680
area. Yeah, that makes sense. Yeah, okay. That clarifies my, I was very excited. I thought that

253
00:27:02,760 --> 00:27:07,080
maybe that we had some sort of like way of doing the confrontation, just linguistically.

254
00:27:09,000 --> 00:27:14,360
Guess that's what that doesn't work. I think it's possible. And we don't well,

255
00:27:15,640 --> 00:27:18,920
I don't know. Well, maybe not linguistically. But like, you know,

256
00:27:19,800 --> 00:27:26,280
we memorize the multiplication table, or for like some problem that we do very often, we don't need

257
00:27:26,280 --> 00:27:31,240
to actually go through the steps of the calculation, we kind of just retrieve the correct answer.

258
00:27:32,040 --> 00:27:37,160
I don't know if it happens linguistically or not, potentially not, probably not. But it's

259
00:27:37,160 --> 00:27:41,400
still a different mechanism than actually going step by step and doing, you know,

260
00:27:41,400 --> 00:27:45,560
long division in your head, or like summing multidigit numbers or something like that.

261
00:27:46,280 --> 00:27:50,920
Yeah. Yeah, I think that that's a really interesting question, which we can maybe come back to.

262
00:27:50,920 --> 00:27:55,320
But I guess, yeah, so you kind of see both the multiple demand and the language

263
00:27:55,320 --> 00:27:59,720
network lighting up when this problem is presented linguistically. So it's sort of a

264
00:27:59,720 --> 00:28:05,400
fair assumption that what is in fact happening is that, you know, there's probably some linguistic

265
00:28:05,400 --> 00:28:12,200
processing, but then it then gets passed to the same sort of area of the brain, which is,

266
00:28:12,200 --> 00:28:17,800
which handles the pure Python problem. But of course, yeah, I mean, that is really interesting

267
00:28:18,600 --> 00:28:24,200
and kind of useful in some ways, in that, you know, it seems more efficient to be

268
00:28:24,200 --> 00:28:30,120
presented just with the Python code, right? You kind of bypass that. Oh, I turn this into,

269
00:28:31,560 --> 00:28:37,880
you know, it goes straight into the, the system, which can perform the ultimate calculation, I

270
00:28:37,880 --> 00:28:43,560
guess. I don't know if you, if you were able to capture any information on whether it was quicker

271
00:28:43,560 --> 00:28:49,880
for people to kind of solve the problem when presented with the Python code or not.

272
00:28:50,680 --> 00:29:02,040
I don't remember whether we saw a difference in how long it took people.

273
00:29:02,760 --> 00:29:08,360
I think it's possible, but I don't, some of it, of course, depends on how proficient they are in

274
00:29:08,360 --> 00:29:15,080
Python. So there might be individual differences there, also individual differences in how

275
00:29:15,080 --> 00:29:24,440
fast they would read text. So I'm sure there's some variability there. But it's actually

276
00:29:25,640 --> 00:29:31,720
an interesting thought that you bring enough so that this, having this abstract skeleton with

277
00:29:32,360 --> 00:29:38,840
other information stripped away might make the problem solving the calculation easier.

278
00:29:38,840 --> 00:29:46,520
Because in fact, there are cases where researchers have observed the reverse,

279
00:29:47,640 --> 00:29:57,160
though there is this famous Waste and Selection task, which you have, let's see,

280
00:29:57,560 --> 00:30:07,080
a card with like a two and a seven, and then a, and then a card with like

281
00:30:07,960 --> 00:30:21,000
green, red and blue. And you need to test the rule that says if the number is even,

282
00:30:21,080 --> 00:30:27,560
then the other side of the card has to be blue. And so then the question is which cards do you

283
00:30:27,560 --> 00:30:33,240
need to turn over to make sure that that rule is correct. And so then people want to test

284
00:30:35,480 --> 00:30:40,040
the card with the two on it because it's even, and so they want to make sure that their reverse

285
00:30:40,040 --> 00:30:45,240
is blue. But then they often want to turn over the blue to make sure that the other side is odd,

286
00:30:46,040 --> 00:30:51,720
sorry, is even. But that actually is not what you should do because it doesn't matter. Like if you

287
00:30:51,720 --> 00:30:56,360
have blue and odd, that's actually not a violation of the rule. What you need to do is you need to

288
00:30:56,360 --> 00:31:01,560
turn over the red card because of an even number there, then that rule gets violated.

289
00:31:02,360 --> 00:31:10,760
So that problem is hard for people. But if you cast the same problem saying that there are

290
00:31:11,640 --> 00:31:20,200
people at the bar and somebody, you know, is 16 and somebody is 25 and then somebody is drinking

291
00:31:20,200 --> 00:31:27,080
beer and somebody is drinking a Coke, then, you know, how will you verify that only people over

292
00:31:27,080 --> 00:31:32,120
the age of 18 are drinking alcohol? And then of course, you know, that you need to, you know,

293
00:31:32,120 --> 00:31:38,040
check the 16 year old and check the person drinking the beer and not any other way. And so

294
00:31:38,040 --> 00:31:44,280
mathematically, the same exact process, but it's much, much easier for people to ground the rule

295
00:31:44,280 --> 00:31:49,080
and their existing knowledge, not necessarily the bar example, but they're just kind of, you know,

296
00:31:49,080 --> 00:31:55,240
the easiest one and the most common one. And so this phenomenon is known as content effects on

297
00:31:55,240 --> 00:32:03,480
reasoning. And yeah, I think a lot of people, especially like, you know, physicists and mathematicians

298
00:32:03,480 --> 00:32:08,040
and so people trained in like hardcore STEM discipline, they're like, hey, Alex, trip away,

299
00:32:08,040 --> 00:32:12,760
all of the extra information only focus on the abstract symbols. That's the easiest thing. But

300
00:32:12,760 --> 00:32:19,400
actually for a lot of people grounding the problem in some specific content domain tends to help.

301
00:32:19,400 --> 00:32:25,000
And so I know that some people in like math education are very interested in this phenomenon and

302
00:32:25,000 --> 00:32:33,000
how does it how to make it easier for people to, for kids to learn math. Is it by focusing on the

303
00:32:33,000 --> 00:32:37,480
abstract or is it by grounding math problems in real life situations?

304
00:32:39,560 --> 00:32:45,720
And I suppose part of the reason why that grounding might work, well, there could be kind

305
00:32:45,720 --> 00:32:51,080
of two hypotheses. One is just like, it locates it in a different area of the brain, which is somehow

306
00:32:51,080 --> 00:32:56,680
better at processing this thing. So maybe that just the social reasoning part is just better at

307
00:32:56,680 --> 00:33:01,720
doing that kind of problem. But doesn't seem so likely in this case. And another is just that

308
00:33:02,200 --> 00:33:09,000
it gets it, it clicks it in to a place where you're able to recognize a pattern that you've

309
00:33:09,000 --> 00:33:15,160
seen before. And so you don't have to do, you know, you're already on the right track.

310
00:33:16,920 --> 00:33:22,600
And this maybe kind of comes back to your point about, well, maybe when we calculate the BMI for

311
00:33:22,600 --> 00:33:26,760
certain kind of combinations of numbers, you just know the answer. So it's being kind of recalled

312
00:33:26,840 --> 00:33:32,280
from memory, like that pattern is already so established that you don't need to reason through

313
00:33:32,280 --> 00:33:42,920
it in the same way. It's more of a recall operation. And I mean, this is getting us towards one of the

314
00:33:42,920 --> 00:33:47,720
kind of central questions, which is around, well, what are LLMs doing? Because they're kind of

315
00:33:48,920 --> 00:33:53,720
glorified recall machines in a certain way, or just really good pattern matches.

316
00:33:54,200 --> 00:34:01,960
Maybe before we get to that, though, I want to talk about another of your experiments, which I

317
00:34:01,960 --> 00:34:08,360
really enjoyed, which is about where people are looking at images of improbable and probable

318
00:34:08,360 --> 00:34:17,000
things like the shark and the swimmer that you mentioned. And what I found, well, maybe you

319
00:34:17,000 --> 00:34:21,800
should describe the experiment, you'll do a much better job of it than me. Because I think, yeah,

320
00:34:21,800 --> 00:34:24,440
there was just a really interesting piece here that kind of writes this.

321
00:34:26,200 --> 00:34:32,840
Yeah. So here, we use that same idea that the same information might arrive in the brain through

322
00:34:32,840 --> 00:34:41,480
different routes. And so in this case, we were looking at sentences describing basic interactions

323
00:34:41,480 --> 00:34:51,400
between two entities, like the, I guess we can roll with the shark bites the swimmer,

324
00:34:51,400 --> 00:35:01,320
the swimmer bites the shark, and pictures depicting the same kinds of events. And so here,

325
00:35:01,320 --> 00:35:07,000
by switching around, who's doing what to whom, we're manipulating whether the event is plausible.

326
00:35:07,000 --> 00:35:14,600
So likely to occur in the real world or impossible. So unlikely to occur. And the question was,

327
00:35:15,560 --> 00:35:23,640
does the language network respond to language specifically? Or does it respond to meaning

328
00:35:23,640 --> 00:35:29,880
and concepts more generally? And so if it's language, it should only really respond to

329
00:35:30,440 --> 00:35:37,880
sentences and not to pictures. And if it's responsible to meaning, it should respond equally

330
00:35:37,880 --> 00:35:43,240
strongly to sentences and pictures, as long as the person is thinking about the meaning. And so we

331
00:35:43,240 --> 00:35:48,600
had people tell us whether they think the event is plausible or impossible. So you have to be

332
00:35:48,600 --> 00:35:57,160
thinking about the meaning. And so what we found was actually something in between, where the language

333
00:35:57,160 --> 00:36:05,000
network, in accordance with all of the prior work, responds more strongly to sentences than to pictures.

334
00:36:05,880 --> 00:36:13,080
But it still responded to pictures to some extent. And I will say that in another study,

335
00:36:13,080 --> 00:36:21,000
we recorded responses in the language regions to pictures of objects. So is this animal dangerous?

336
00:36:21,000 --> 00:36:26,280
Can this object be found in the kitchen, that kind of stuff? And it did not respond to pictures of

337
00:36:26,280 --> 00:36:33,720
objects. So it was something about events, maybe just something more complex, maybe something just

338
00:36:33,720 --> 00:36:40,840
more fast-paced, that was specifically triggering responses in the language regions. And so this

339
00:36:40,840 --> 00:36:48,760
intermediate result, so preference for sentences over pictures, but also responses to meaning,

340
00:36:48,760 --> 00:36:57,560
even in known sentences, was kind of puzzling. And so one piece of evidence that helped us

341
00:36:58,680 --> 00:37:07,720
make sense of this information was evidence from individuals with global aphasia, from people with

342
00:37:07,720 --> 00:37:13,320
brain damage. And I should say that this is, yeah, so lots of the brain imaging work I'm describing,

343
00:37:13,320 --> 00:37:22,280
I did with my PhD advisor at Fedorenko, and the global aphasia bit is done in collaboration with

344
00:37:22,280 --> 00:37:32,120
Rosemary Varley at UCL, who works with individuals with global aphasia very, very closely. And so

345
00:37:32,920 --> 00:37:40,920
here we had two individuals with global aphasia, really serious issues, looking at pictures of

346
00:37:43,480 --> 00:37:49,640
swimmer by sharks, shark bite swimmer. And so as I mentioned earlier, they were laughing at the

347
00:37:49,640 --> 00:37:55,000
weird ones. And so in general, they were very good at distinguishing plausible and implausible

348
00:37:55,000 --> 00:38:03,480
pictures, suggesting that their ability to extract meaning from pictures was there, it did not

349
00:38:03,480 --> 00:38:10,280
require a functioning language network. And so then the title of the paper, the language network

350
00:38:10,280 --> 00:38:17,720
is recruited but not required for pictorial events and semantics. So we see this activation,

351
00:38:18,440 --> 00:38:23,800
and we, but it's not, it's not necessary to do the task.

352
00:38:25,480 --> 00:38:31,880
Yeah, yeah, I found that, yeah, very insightful. And what struck me was

353
00:38:33,480 --> 00:38:38,360
one of the hypotheses as to why the language network was recruited is that

354
00:38:38,360 --> 00:38:45,080
it's sort of another way of getting evidence or information on whether this event is likely or not.

355
00:38:45,880 --> 00:38:49,560
And of course, we can't be sure what's going on in there. But, you know, perhaps it is somewhat

356
00:38:49,560 --> 00:38:55,640
like a large language model where you, you look at the thing, you're like, part of trying to figure

357
00:38:55,640 --> 00:39:00,200
out whether this picture is likely or not is you kind of read it out to yourself and like, well,

358
00:39:00,200 --> 00:39:10,120
does this, is this a familiar pattern, right? Does it, is, you know, shark bites swimmer,

359
00:39:10,120 --> 00:39:15,800
you know, that's that's that sequence of words feels close to sequences of words that I produced

360
00:39:15,800 --> 00:39:22,680
before or read before, whereas swimmer bites shark is kind of jarring. And maybe behind that is just

361
00:39:22,680 --> 00:39:29,960
the improbability of that, that, that, that sentence being produced according to the language

362
00:39:29,960 --> 00:39:36,920
model in our, in our own minds. And of course, yeah, we, we don't really know that our minds work

363
00:39:36,920 --> 00:39:43,560
like a large language model at all, but it's an attractive hypothesis in as much as it works.

364
00:39:45,160 --> 00:39:51,080
Yeah, so I guess, so generally speaking, right, so we got this result language network recruited

365
00:39:51,080 --> 00:39:55,400
but not required. And the question was, what's going on, right? And so generally speaking,

366
00:39:55,400 --> 00:40:05,080
we consider two broad hypothesis. One is that that activation is not necessary to do the task.

367
00:40:05,080 --> 00:40:11,560
So you see the picture, sharks, swimmer and biting. And so you activate those words kind of

368
00:40:11,560 --> 00:40:18,360
automatically by association, but you're not actually using them to reason about whether

369
00:40:18,360 --> 00:40:24,040
the event makes sense or not. So that's one hypothesis. And the other hypothesis is that

370
00:40:24,040 --> 00:40:30,360
actually, the information in the language network is helpful. But when you're trying to

371
00:40:31,240 --> 00:40:37,160
recast information that you're seeing in linguistic form, you can then compare it with

372
00:40:37,160 --> 00:40:42,120
all of the linguistic information that you received in your lifetime. And maybe that

373
00:40:42,120 --> 00:40:47,720
information ended up distilled in your brain in some general way, just kind of, we know that

374
00:40:47,800 --> 00:40:52,760
people are very sensitive to statistical regularities in language, we know that we're very good at

375
00:40:52,760 --> 00:41:02,600
predicting what word would come next, right? Like there is this information about what patterns

376
00:41:02,600 --> 00:41:09,000
are likely in text is very much what people use during real life language comprehension.

377
00:41:09,800 --> 00:41:18,360
And of course, that information also can help us, in many cases, figure out which events

378
00:41:19,160 --> 00:41:25,560
make sense and which doesn't. And so actually, we try to test that hypothesis. We

379
00:41:27,320 --> 00:41:34,440
didn't necessarily, it's hard to test that in actual human brains, although we now have

380
00:41:35,160 --> 00:41:45,160
ideas for how we might be able to do that. But we started by using language models as proof of

381
00:41:45,160 --> 00:41:54,600
concept, right? So the hypothesis is statistical patterns in language input can help us distinguish

382
00:41:54,600 --> 00:42:01,480
plausible and implausible events. And language models are very good at capturing these statistical

383
00:42:01,480 --> 00:42:08,920
patterns. So if language models can systematically distinguish plausible and implausible events,

384
00:42:08,920 --> 00:42:14,600
that means that there is enough information there where maybe humans might be able to use that

385
00:42:14,600 --> 00:42:18,760
information also to distinguish plausible and implausible events, right? So it's not evident

386
00:42:18,760 --> 00:42:26,040
that humans do, but it's evident that humans can. And so we did that, we use language models and

387
00:42:26,040 --> 00:42:34,360
try to see whether they systematically evaluate plausible event descriptions as more likely than

388
00:42:34,360 --> 00:42:43,640
implausible. And so in that study, we specifically distinguished between two kinds of events. So

389
00:42:43,640 --> 00:42:52,280
one is the teacher bought the laptop versus the laptop bought the teacher. So animate, inanimate

390
00:42:52,280 --> 00:42:59,080
interactions. And so when you swap them around, if you interpret the sense verbatim and the inanimate

391
00:42:59,080 --> 00:43:07,320
object laptop cannot buy anything, buying requires that the subject is animate. And so that's a very

392
00:43:07,320 --> 00:43:13,960
kind of in your face screaming violation. And then the other example is kind of like the fox

393
00:43:13,960 --> 00:43:19,000
chased the rabbit, the rabbit chased the fox or the swimmer by the shark, the shark by the swimmer,

394
00:43:19,000 --> 00:43:24,760
right? The swimmer by the shark is not impossible. It can happen. It's just way less likely.

395
00:43:25,400 --> 00:43:30,120
And so what we found is that when it comes to distinguishing possible and impossible events,

396
00:43:30,120 --> 00:43:37,960
language models were very good, almost at ceiling. So that was actually very easy for them. But when

397
00:43:37,960 --> 00:43:44,120
it came to likely versus unlikely events, there was a gap in performance. So they weren't quite as

398
00:43:44,120 --> 00:43:49,880
good. They were okay. They were above chance. But they definitely weren't perfect. And so we're

399
00:43:49,880 --> 00:43:57,640
not as good as people, I guess, right? Yeah, not not as good as people and not as good as when they

400
00:43:57,640 --> 00:44:03,480
have to deal with animate inanimate sentence, right with impossible events. Yeah. So I think it's

401
00:44:03,480 --> 00:44:08,200
like, to me, it's actually more interesting to compare those two sentence types, like how models

402
00:44:08,200 --> 00:44:15,000
do on them. But also humans humans do well on both because these are easy sentences. So

403
00:44:15,000 --> 00:44:23,240
they're not meant to be challenging. And so, yeah. No, I was gonna say, and do we take that as evidence

404
00:44:23,240 --> 00:44:31,640
that then when humans reason about these things, they're doing it, not just linguistically, or is

405
00:44:31,640 --> 00:44:36,440
it that our language models are kind of like better than the language models that that are out there

406
00:44:36,440 --> 00:44:42,920
in, you know, the computer language, large language models? I think that I think it's

407
00:44:42,920 --> 00:44:48,120
evident that humans are doing it not linguistically. And so the reason why we think there is this

408
00:44:48,120 --> 00:44:56,200
performance gap is because actually, the language input that we receive doesn't faithfully describe

409
00:44:56,200 --> 00:45:04,120
the world around us. So when we talk to each other, we don't just passively describe everything

410
00:45:04,120 --> 00:45:09,880
that we're seeing. So I'm not telling you, you know, I am sitting down, the lights are on,

411
00:45:09,880 --> 00:45:16,520
the room is empty, like it's very boring stuff. I'm telling you about things that are unusual,

412
00:45:16,520 --> 00:45:23,640
novel, interesting, newsworthy in some way. And so this phenomenon is known as reporting bias.

413
00:45:24,280 --> 00:45:33,160
So language tends to undercover on the report information that is kind of trivial, right,

414
00:45:33,240 --> 00:45:39,480
that everybody already knows, or can reasonably infer. And so maybe actually events that are

415
00:45:39,480 --> 00:45:45,560
unlikely are not as unlikely for LLMs, because, you know, we talk about unlikely things all the

416
00:45:45,560 --> 00:45:50,200
time, that's the stuff that's worth talking about. And so if that's true, that's the reason why we

417
00:45:50,200 --> 00:45:55,800
see this performance gap, then even if the human language model is very good, which by the way,

418
00:45:55,800 --> 00:45:59,640
we don't think it is actually, I think large language models now are much better at predicting

419
00:45:59,640 --> 00:46:05,080
the next war that humans are. So actually, they're better. But even if humans were really good,

420
00:46:05,080 --> 00:46:13,320
just the language input is insufficient for us to be able to distinguish plausible and

421
00:46:13,320 --> 00:46:19,000
implausible events. That means that we have to use something else in addition, we have to maybe

422
00:46:19,000 --> 00:46:24,520
have some more sophisticated model of the world, where we can actually correct for this reporting

423
00:46:24,520 --> 00:46:33,000
bias, we have to also bring in information that's about what things are typical, what we

424
00:46:33,000 --> 00:46:39,160
can expect, what we cannot expect. So we're probably drawing on sort of multiple mental

425
00:46:39,160 --> 00:46:49,880
resources or systems. In the case of the clearly kind of impossible, so computer bias teacher,

426
00:46:50,840 --> 00:46:56,840
is it just the language, can you see if it's just the language, or have you seen if it was just

427
00:46:56,840 --> 00:47:01,080
the language network that's recruited there, as one might think, well, if it can just be done

428
00:47:01,080 --> 00:47:07,800
within the kind of the one region, maybe it's more efficient and metabolically, there might be some

429
00:47:07,800 --> 00:47:15,480
kind of preference for doing that if it were possible. By default, we kind of light up various

430
00:47:15,640 --> 00:47:23,880
regions just to make sure I don't know. So that's actually a study that I would love to do next,

431
00:47:23,880 --> 00:47:30,040
so this difference between impossible and unlikely events is something that emerged out of this

432
00:47:30,040 --> 00:47:37,320
language model study. And so now, of course, yeah, I think it would be great to bring it back to

433
00:47:37,880 --> 00:47:44,200
the MRI machine, measure people's brain activity in response to impossible versus

434
00:47:44,200 --> 00:47:49,720
unlikely centers, and see if the language network alone is sufficient for distinguishing

435
00:47:50,360 --> 00:47:55,160
possible and impossible events. That is the prediction that follows from this language

436
00:47:55,160 --> 00:48:00,600
model work. And so I would love to test that. Yeah, yeah, that would be so, yeah, I'd love to

437
00:48:00,600 --> 00:48:07,560
see the results of that. Yeah, so I hope that that happens. But I suppose, you know, coming back to

438
00:48:08,200 --> 00:48:16,280
LLMs, what we're starting to see is that maybe just a large language model in itself,

439
00:48:18,200 --> 00:48:28,520
for various reasons, might not be so effective at thinking or reasoning as the human brain. And

440
00:48:28,520 --> 00:48:36,520
one is, as you kind of mentioned, that the data that comes in is kind of biased toward the salient

441
00:48:36,600 --> 00:48:42,680
and newsworthy as you put it. But then another from the kind of Python example is that, well,

442
00:48:42,680 --> 00:48:51,720
as a matter of fact, we don't use the language part of the brain for code comprehension or for

443
00:48:52,280 --> 00:49:01,480
logical mathematical reasoning, either for that matter. I suppose my question there is, though,

444
00:49:02,040 --> 00:49:07,000
you know, could it be possible for LLMs to kind of just be

445
00:49:11,880 --> 00:49:19,240
be able to take on the functions of the multiple demands network, for instance, which is doing

446
00:49:19,240 --> 00:49:25,000
all this, which is the place which does the mathematical logical Python code interpretation

447
00:49:25,560 --> 00:49:31,720
comprehension? Could it kind of take on all those responsibilities just by having

448
00:49:33,400 --> 00:49:37,960
getting really good at saying, you know, next word prediction for mathematical problems and

449
00:49:37,960 --> 00:49:44,840
next word prediction for code generation and so on? Or is just that kind of, or is that implausible?

450
00:49:44,840 --> 00:49:52,840
I don't really know how we characterize, you know, where the LLMs just could have kind of

451
00:49:52,920 --> 00:49:58,120
emergently develop all those capabilities within a single language model, or if that's just

452
00:49:59,720 --> 00:50:12,200
very, very unlikely. Yeah, so LLMs do a bunch of different things. In general, as you mentioned,

453
00:50:12,200 --> 00:50:17,400
they're very, very good at pattern recognition and pattern completion at different levels of

454
00:50:17,400 --> 00:50:24,360
abstraction. So they do a lot of just direct memorization, right? The larger the model,

455
00:50:24,360 --> 00:50:30,600
the more texts that can just memorize straight up, which is why a lot of those copyright issues

456
00:50:30,600 --> 00:50:35,800
end up arising. But that's not the only thing that these models do, because they definitely

457
00:50:35,800 --> 00:50:42,760
are capable of generating novel texts and mixing and matching previous inputs. And so the patterns

458
00:50:42,760 --> 00:50:49,880
that they can recognize and reproduce, they can be fairly abstract. But then, of course, the question

459
00:50:49,880 --> 00:51:02,360
then is pattern completion all it takes? Is that the only thing that's necessary? And so that's where

460
00:51:02,360 --> 00:51:10,840
it gets tricky, because a lot of logical reasoning is algorithmic reasoning. It's symbolic. It's

461
00:51:11,480 --> 00:51:18,280
very regimented. And so these are the kinds of problems where these models seem to struggle.

462
00:51:18,280 --> 00:51:24,360
So for example, if you ask them to add and multiply two numbers together, if the numbers are small

463
00:51:24,360 --> 00:51:31,800
enough, then the model is doing just fine. But if the number is large, that means it wasn't part of

464
00:51:31,800 --> 00:51:36,920
the training set. It means it couldn't have just memorized the response, which it probably does for

465
00:51:36,920 --> 00:51:43,800
a lot of smaller number combinations. And so then it would actually have to multiply step by step.

466
00:51:43,800 --> 00:51:51,080
And it doesn't seem to be doing that very successfully. In fact, it often gives you a number

467
00:51:51,080 --> 00:51:58,680
that's close, but just a little bit off. And so the kinds of mistake that it's making is different

468
00:51:58,680 --> 00:52:03,400
from the kind of mistake a human would be making, because it's still trying to use pattern matching

469
00:52:03,480 --> 00:52:11,480
to complete. And it's not quite working, it seems. Yeah. But I mean, it's very hard to figure out

470
00:52:11,480 --> 00:52:16,520
exactly how they're doing what they're doing. I mean, they've got so many parameters. And it's

471
00:52:16,520 --> 00:52:26,360
surprising how good they are yet still imperfect at doing those sort of problems. They're kind of

472
00:52:26,440 --> 00:52:34,600
like a broken calculator. So they're much faster at getting to an answer, but it's not quite the

473
00:52:34,600 --> 00:52:41,320
right answer. It's a pretty good estimate often. And yet it's not completely out. So it's really...

474
00:52:43,160 --> 00:52:50,360
Yeah, I don't have a strong opinion, but part of me thinks, well, maybe they'll just kind of,

475
00:52:50,360 --> 00:52:57,080
with enough data going in, they might just crack that. That might come a point at which

476
00:52:59,720 --> 00:53:04,840
that kind of ability emerges. Although you point out in one of your papers, though, well,

477
00:53:04,840 --> 00:53:07,960
perhaps if that ability emerges, it might be that a particular kind of

478
00:53:08,760 --> 00:53:14,440
architecture that models the human brain emerges as well. So it may not be that...

479
00:53:14,680 --> 00:53:22,760
It might be happened in such a way that it becomes less fruitful to think of a large language

480
00:53:23,480 --> 00:53:29,640
model as simply a model of language, but something that has a kind of linguistic language network

481
00:53:29,640 --> 00:53:37,640
part like the human brain and then hands off to a logical part. And as it happens, obviously,

482
00:53:37,960 --> 00:53:44,600
in chat GPT, without that has had that kind of architecture imposed on it, at least in the

483
00:53:44,600 --> 00:53:49,080
version with the Python code interpretation, for instance, because you can say, well, add these

484
00:53:49,080 --> 00:53:52,360
two numbers together, and it will figure out, oh, well, I'm doing a math problem here. So I'm going

485
00:53:52,360 --> 00:53:56,840
to convert this into a Python problem, and then it runs the Python code. So actually,

486
00:53:58,120 --> 00:54:00,760
you know, some of these problems seem to be

487
00:54:01,080 --> 00:54:09,240
are being sort of addressed, I guess, by the developers. But the way they're doing it is,

488
00:54:09,240 --> 00:54:17,480
yeah, offloading. Yeah. Yeah. So I guess let me unpack a little bit. There's a lot there. So

489
00:54:17,720 --> 00:54:30,760
first of all, it is very tempting for people to over ascribe intelligence to a language model.

490
00:54:31,400 --> 00:54:38,200
And presumably that's because in our everyday interactions, we're used that language gets

491
00:54:38,200 --> 00:54:45,160
generated by a thinking feeling being other humans. And now we have a system which is breaking

492
00:54:45,240 --> 00:54:50,520
that relationship where we have something that generates coherence language that's not human.

493
00:54:50,520 --> 00:54:58,680
And so it gets confusing. And that's the reason why that's one of the reasons why

494
00:54:58,680 --> 00:55:03,480
there's so much hyper-intelligent language models, and they're expected to be the general

495
00:55:03,480 --> 00:55:10,280
intelligence models, because of this tight perceived relationship between language and thought.

496
00:55:10,280 --> 00:55:15,400
And so then when they make a math mistake, or they make a factually inaccurate statement,

497
00:55:15,400 --> 00:55:19,240
you're like, oh, no, like how, you know, these models are terrible, they're not

498
00:55:19,240 --> 00:55:23,720
terrible, they're just like not, they're just a totally different capacity you're evaluating.

499
00:55:23,720 --> 00:55:31,800
And so what we argue is that it's very important to distinguish different kinds of

500
00:55:31,800 --> 00:55:38,840
capabilities in these models. And so there is something that we call formal linguistic

501
00:55:38,920 --> 00:55:44,600
competence. And so that's the ability to generate coherent grammatical language.

502
00:55:44,600 --> 00:55:50,440
And that's something that in humans, the language brain network is responsible for.

503
00:55:51,560 --> 00:55:58,360
And then there is all of the other stuff that you need in order to actually use language in

504
00:55:58,360 --> 00:56:04,600
real life situation in interactions, you might want to ask somebody to close the door,

505
00:56:04,600 --> 00:56:11,640
you might want to tell somebody how you feel. And there are all kinds of situations that

506
00:56:11,640 --> 00:56:18,360
where you need to use language. But to do that, you actually need other capabilities,

507
00:56:18,360 --> 00:56:23,720
you need to be able to reason about social situations, you need to be able to know things

508
00:56:23,720 --> 00:56:28,760
about the world in order to generate actually accurate statements, you need to be able to reason

509
00:56:28,760 --> 00:56:33,640
logically and know some math if you want to solve a math problem. So even if that information is

510
00:56:33,640 --> 00:56:39,240
coming in as language, in order to be able to make sense of it, and also generate language that

511
00:56:39,240 --> 00:56:45,160
achieves a particular purpose, you need all of these other capacities, which broadly speaking,

512
00:56:45,160 --> 00:56:53,080
recall functional competence. And so different kinds of capabilities might suffer from different

513
00:56:53,080 --> 00:57:00,120
problems. And so we already touched upon a few. We touched upon the fact that mathematical reasoning

514
00:57:00,120 --> 00:57:06,760
and logical reasoning might require a different kind of algorithm. So instead of pattern matching,

515
00:57:06,760 --> 00:57:12,600
it might need to be more symbolic. And it's not fully clear whether the large language models

516
00:57:12,600 --> 00:57:21,560
today are capable of doing that. Maybe they are. But that's not necessarily in their default,

517
00:57:22,360 --> 00:57:27,480
in the default way they operate. So that's an open debate there. When it comes to world

518
00:57:27,480 --> 00:57:32,440
knowledge and knowing things about the world, distinguish implausible and implausible events,

519
00:57:32,440 --> 00:57:39,400
there a big problem is reporting bias and the fact that the training data that they have is biased.

520
00:57:39,400 --> 00:57:46,040
And so you might need to be able to build up a more general situation model, event model,

521
00:57:46,040 --> 00:57:51,640
that will not just take in the language that you receive, but also fill in some kind of commonly

522
00:57:51,640 --> 00:58:00,440
assumed things. If it's daytime, it slides out, stuff like that. And yeah, so different kinds

523
00:58:00,440 --> 00:58:07,160
of problems might require different kinds of solutions. A more general kind of potential solution

524
00:58:07,160 --> 00:58:18,600
that we advocate or talk about is modularity. So the fact that the brain is modular suggests that

525
00:58:18,920 --> 00:58:25,480
might be an efficient architecture. So a language process in module, the goal of the language

526
00:58:25,480 --> 00:58:33,080
network in the brain is not to reason, it's to get information that's expressed in fuzzy,

527
00:58:33,080 --> 00:58:39,240
imprecise words and extract meaning out of it. And then pass it on to relevant systems that can

528
00:58:39,240 --> 00:58:47,160
solve the math problem that can infer the social goal, all of that stuff. And presumably, for an

529
00:58:47,240 --> 00:58:52,120
artificial intelligence system, you might want to do something similar where language

530
00:58:52,120 --> 00:58:58,760
is not a replacement for thought, but is an interface to thought. And so in your example,

531
00:58:58,760 --> 00:59:05,320
right, you have a math problem, the language model translates it into code. It's very good at

532
00:59:05,320 --> 00:59:10,280
taking this broad like fuzzy natural language and translating into a more precise, symbolic

533
00:59:10,280 --> 00:59:14,600
representation. That's something that we didn't have at all, even a few years back. So it's a

534
00:59:14,600 --> 00:59:20,280
huge achievement. But then instead of trying to have that same language model to run the code,

535
00:59:20,280 --> 00:59:26,680
you're much better off passing it off to a code interpreter that will run the code and give you

536
00:59:26,680 --> 00:59:32,760
the answer. So the same kind of modularity that we see in the brain, that seems to be an effective

537
00:59:32,760 --> 00:59:38,280
way forward in the AI world that indeed some developers have started to adopt.

538
00:59:39,240 --> 00:59:47,240
Yeah. Yeah. And I think there's probably other ways in which the builders of these tools are trying

539
00:59:47,240 --> 00:59:53,320
to modularize. Like another one that comes up a lot is Rang or retrieval augmented generation, where

540
00:59:55,080 --> 01:00:00,600
yeah, there's some kind of database or just could just be a whole bunch of, you know,

541
01:00:00,600 --> 01:00:07,080
documents or whatever. And instead of hallucinating an answer, you want to make sure that you pick up

542
01:00:07,800 --> 01:00:14,280
something from one of those documents. And there's a whole different kind of machinery for that.

543
01:00:14,280 --> 01:00:21,160
But again, like in the code interpreter example, it's, I guess the language part is

544
01:00:22,440 --> 01:00:27,560
key, maybe less key in Rang because it's kind of a vector search. But it's a way, you know,

545
01:00:27,560 --> 01:00:33,560
it begins with translating language into something a bit more precise, in this case a vector instead

546
01:00:34,120 --> 01:00:41,640
of some code, I guess. And yeah, one wonders then if, you know, how close the parallels are between

547
01:00:42,680 --> 01:00:48,600
what is being built here and what's going on in the brain. You mentioned that, yeah,

548
01:00:49,240 --> 01:00:55,000
perhaps this is a good model for thinking about how we think. Language is this part where,

549
01:00:55,640 --> 01:01:02,280
this place where things kind of, you know, entry point for concepts, but the places where

550
01:01:02,360 --> 01:01:08,040
those concepts often get manipulated in terms of reasoning might be in other areas of the

551
01:01:08,040 --> 01:01:12,680
brain. They sort of become something more abstract than language itself.

552
01:01:16,200 --> 01:01:22,520
Yeah. Yeah. One thing I actually just slight tangent, but I do sometimes think that

553
01:01:24,440 --> 01:01:29,800
maybe language is being so associated with thought because it's kind of like

554
01:01:30,760 --> 01:01:36,760
the easiest thing to do, right? Like, you know, we know thinking is about concepts and some,

555
01:01:37,320 --> 01:01:43,880
you know, manipulating these things which are representations of the world. And language is

556
01:01:43,880 --> 01:01:49,160
just such an easy way of visualizing all of that, right, and understanding what's going on. But

557
01:01:49,160 --> 01:01:56,920
perhaps it's just the surface level of something much deeper that we really don't have an easy way

558
01:01:57,000 --> 01:02:04,040
of capturing. And, you know, that would map, I think, quite well to this kind of model of

559
01:02:04,680 --> 01:02:09,960
concepts being passed around, but the concepts themselves being, you know, beyond linguistics

560
01:02:09,960 --> 01:02:18,760
somehow. Yeah. So, as we mentioned, language is a system designed to communicate thoughts,

561
01:02:19,320 --> 01:02:25,320
concepts from one mind to another. And so, for this communication to be efficient,

562
01:02:25,320 --> 01:02:30,760
presumably language needs to parallel the structure of thought, the structure of concepts

563
01:02:30,760 --> 01:02:37,320
in some way, right? And so, it's much more abstract already than the raw perceptual input,

564
01:02:37,320 --> 01:02:43,800
than just audio, than just pictures, right? So, it kind of captures the relevant abstractions to

565
01:02:43,800 --> 01:02:50,200
a large extent. And so, that seems to be helping a lot. And so, that does bring us much closer to

566
01:02:51,000 --> 01:02:54,920
this more abstract conceptual representation. We're getting rid of a lot of extra details,

567
01:02:54,920 --> 01:03:01,000
we say cat, we don't care which color, which size is the cat. But, of course, at mapping

568
01:03:01,000 --> 01:03:05,720
between concepts and languages imprecise, we know that different languages partition the

569
01:03:05,720 --> 01:03:11,640
conceptual space in different ways, right? So, the words don't necessarily map the concepts one

570
01:03:11,640 --> 01:03:17,800
and one. Even within the same language, the same word can be used in many different contexts,

571
01:03:17,880 --> 01:03:25,720
in different ways, with different meanings. And so, that link is pretty fuzzy, can get pretty

572
01:03:25,720 --> 01:03:31,560
fuzzy. But it's definitely, I think you're right, when it comes to raw surface form,

573
01:03:31,560 --> 01:03:37,400
it's a very decent proxy, imperfect, but it makes sense why people are tempted to use it.

574
01:03:38,040 --> 01:03:43,400
Yeah. And, you know, in some ways, that means it makes what LLMs do so much more impressive,

575
01:03:43,400 --> 01:03:49,720
because they're also somehow capturing that surface form of concepts. Someone,

576
01:03:51,000 --> 01:03:58,280
a previous guest pointed out this wonderful quote from Ilya Sotskava saying, well, you know, if

577
01:04:00,920 --> 01:04:09,400
your LLM can predict the, you know, it's not just predicting text, because if your LLM can be fed

578
01:04:10,120 --> 01:04:17,800
the first part of a mystery novel that it's not read before, and it can tell you who the murderer

579
01:04:17,800 --> 01:04:24,520
was, it's not just predicting a word, it's somehow kind of understood what's going on in that story.

580
01:04:24,520 --> 01:04:31,720
Now, one of the difficulties, obviously, with all these things is, well, we don't know how

581
01:04:31,720 --> 01:04:37,640
open AIs LLMs are trained. So, it's very hard to test them, because you really need someone to write

582
01:04:37,640 --> 01:04:44,840
a new mystery novel to actually see if Ilya Sotskava's claim cashed us out. So, it's quite a

583
01:04:45,400 --> 01:04:51,480
high effort test. Unless, yeah, we happen to know of one which is definitely not in the corpus that

584
01:04:51,480 --> 01:05:02,040
was used. But, yeah, it does seem, you know, the fact that they are so good at mirroring what

585
01:05:02,040 --> 01:05:11,480
we produce, and that what we produce is somehow a good map onto something somewhat deeper, the world

586
01:05:11,480 --> 01:05:18,280
or an inner world. Yeah, it's so impressive. And you point out as well that it seems that,

587
01:05:19,080 --> 01:05:26,360
you know, the way that LLMs operate is very similar structurally to the way that

588
01:05:27,080 --> 01:05:35,400
our minds operate, in that, you know, it's not working on the raw audio or pixel forms of things.

589
01:05:36,040 --> 01:05:43,080
Like, the beauty of language is the compositionality at the level of small units, which are

590
01:05:43,720 --> 01:05:52,120
combinations of symbols or small sounds. And, yeah, the LLMs perfectly match that. So,

591
01:05:52,840 --> 01:05:59,400
we've built these things which really do capture something quite essential about how at least a

592
01:05:59,400 --> 01:06:05,400
part of our mind operates, it seems. And, yeah, maybe we've been seduced into thinking. That's

593
01:06:05,400 --> 01:06:13,720
all there is to thinking. Well, yeah, so in fact, well, that question, I guess I don't want to get

594
01:06:13,720 --> 01:06:18,760
too technical, but the question of what LLMs are starting with is actually an important one when

595
01:06:18,760 --> 01:06:27,480
we're trying to compare them with human minds or human brains. So, in fact, what LLMs operate over

596
01:06:27,480 --> 01:06:36,360
is tokens. So, it's chunks of characters that tend to occur pretty frequently in text. And so,

597
01:06:36,360 --> 01:06:41,320
oftentimes, they're words, like, though, but they're sometimes not words. If the word is long,

598
01:06:41,320 --> 01:06:48,440
it gets split up into multiple tokens. Yeah. And so, the problem is that those tokens actually

599
01:06:48,440 --> 01:06:54,600
don't match linguistic units that the word is actually made of, like morphine. They can be

600
01:06:54,600 --> 01:07:01,720
pretty arbitrary. And so, that does cause some differences between the way LLMs process them

601
01:07:01,720 --> 01:07:09,800
and humans do. In fact, people think that one reason why large language models are bad at arithmetic

602
01:07:09,800 --> 01:07:15,800
is because they tokenize numbers in weird ways, right? So, like, I don't know, 1618 is chunks

603
01:07:15,880 --> 01:07:20,760
in, like, 161 and then eight. And so, then it gets weird when they have to, like, add up the numbers.

604
01:07:20,760 --> 01:07:27,480
And so, that's where you get this weird, better-matching errors. And so, this kind of form is

605
01:07:27,480 --> 01:07:33,240
that, it's very engineering-driven. It's actually not, like, very rigorously scientifically based.

606
01:07:33,240 --> 01:07:38,680
And so, it's interesting, like, maybe if we change this little thing, it actually will result in much

607
01:07:38,680 --> 01:07:49,000
better performance. And so, it's funny how a lot of those choices are pretty random engineering-driven

608
01:07:49,000 --> 01:07:55,800
things. And, you know, they often work very well. But it's possible that with a small few tweaks,

609
01:07:55,800 --> 01:07:59,880
you can actually make the model much better. Yeah. No, I always thought that there was more

610
01:08:01,400 --> 01:08:06,600
sort of reasoning behind the n-grams that were used. But maybe, is it just kind of randomly

611
01:08:07,240 --> 01:08:13,240
chunks? Because I would have thought, well, there's some kind of, it makes sense to split

612
01:08:13,240 --> 01:08:21,480
words up, because, you know, particles like nus, if I think of, like, redness, right? It's not a

613
01:08:21,480 --> 01:08:27,000
word in itself, but it does attach to so many different words that it's sort of part of the

614
01:08:27,000 --> 01:08:34,360
compositional structure, I guess. But if it's getting chunked up is just two s's, right? And not

615
01:08:34,440 --> 01:08:41,320
nus, then it's kind of odd, yeah. No, that's exactly right, because nus is a morpheme,

616
01:08:41,320 --> 01:08:46,280
it's a suffix with a particular meaning. And so, if redness is chunked into red and nus,

617
01:08:46,280 --> 01:08:51,560
that makes a lot of sense, and it's linguistically justified. But oftentimes,

618
01:08:51,560 --> 01:08:56,920
that's not how the chunking happens. That's where the mismatch arises. So, you can definitely

619
01:08:56,920 --> 01:09:01,880
have the two s's in principle. Okay, interesting. Yeah, it seems like, yeah,

620
01:09:02,360 --> 01:09:08,840
one would think that with a bit of curation, maybe they could be even more effective. And yeah, it's

621
01:09:08,840 --> 01:09:14,440
hard to imagine them being more effective in terms of producing language. But perhaps that's

622
01:09:14,440 --> 01:09:21,480
just because they've been fed such a, such copious amounts of data that they sort of these,

623
01:09:22,600 --> 01:09:28,840
you know, they could be more efficient, right? Well, the stalker has an algorithm,

624
01:09:29,720 --> 01:09:36,120
it's kind of the goal is for it to be universal and that driven, right, without human curation,

625
01:09:36,120 --> 01:09:43,160
which is why the morphes don't get respected all the time. It causes a lot of issues for languages

626
01:09:43,160 --> 01:09:51,320
that aren't based on the Roman alphabet. So, let's say Arabic, for example, it ends up getting

627
01:09:51,320 --> 01:09:56,840
tokenized at the character level, because the tokenizer is just not adapted to deal with it.

628
01:09:56,840 --> 01:10:00,920
And so that does mean that performance on these languages that are not

629
01:10:00,920 --> 01:10:07,080
Roman alphabet based is actually worse, often substantially worse. It's generally a problem

630
01:10:07,080 --> 01:10:13,800
that like the fewer, the less data a language has, the worse the performance in that language.

631
01:10:13,800 --> 01:10:18,200
Some of the more general information seems to get pulled across different languages,

632
01:10:18,200 --> 01:10:23,400
which is cool. But a lot of language specific stuff, like grammar, right, of course depends

633
01:10:23,400 --> 01:10:28,680
on how much data you have in that language. But a particular distinction that tends to

634
01:10:28,680 --> 01:10:33,400
matter beyond just the amount of data is which alphabet. And so because so many of these morphs

635
01:10:33,400 --> 01:10:40,600
are English centric, a lot of other languages get left behind. Yeah, interesting. And to one

636
01:10:40,600 --> 01:10:46,280
extent, I mean, I know there are techniques for doing this. So you spend, you know, a lot of

637
01:10:46,280 --> 01:10:50,200
experiments looking into the minds or the brains of people.

638
01:10:52,200 --> 01:10:59,000
There are tools which allow us to do this to an extent with LLMs. But, you know, how effective

639
01:10:59,000 --> 01:11:05,000
are they? How does it compare to looking at an MRI, trying to understand what's going on inside

640
01:11:05,000 --> 01:11:11,400
of an LLM, what concepts it has, or what's lighting up as it is given a prompt?

641
01:11:12,200 --> 01:11:19,080
Yeah. So I am fascinated, honestly, by how many parallels there are between studying biological

642
01:11:19,080 --> 01:11:26,760
intelligence and humans and artificial intelligence. And for me, the first similarity is really just

643
01:11:26,760 --> 01:11:33,080
starting at the behavioral level. So developing separate experiments to look at formal competence

644
01:11:33,080 --> 01:11:37,080
like grammar, functional competence, like reasoning, these are methods from cognitive

645
01:11:37,480 --> 01:11:42,280
science, how do we design good experiments, how do we disentangle different contributors to

646
01:11:42,280 --> 01:11:47,560
performance. So even before we start looking inside the model or inside the brain, just looking

647
01:11:47,560 --> 01:11:54,760
at how humans behave and how models behave can tell us a lot about potentially how they do it,

648
01:11:54,760 --> 01:11:59,960
what kind of mistakes they make, what does it tell us about the potential mechanism that they're

649
01:12:00,600 --> 01:12:09,000
using to solve the task. But then, of course, we can get even more insight by looking at the

650
01:12:09,000 --> 01:12:14,040
actual mechanisms or their neural correlates. So for humans, that means looking inside the brain.

651
01:12:14,040 --> 01:12:22,600
And for models, that means looking inside the model. And so the movement that is getting seen

652
01:12:22,600 --> 01:12:28,840
currently the mechanistic interpretability movement in AI is doing that, essentially,

653
01:12:28,840 --> 01:12:36,440
they're asking which circuits, which units inside the network are responsible for a particular

654
01:12:36,440 --> 01:12:45,720
behavior. And so they first try to identify those units that get particularly engaged in a task.

655
01:12:45,720 --> 01:12:50,520
Maybe they respond differently to plausible sentences compared to implausible sentences.

656
01:12:51,160 --> 01:12:57,720
And then the beauty of having an artificial system is that they can actually go and

657
01:12:57,720 --> 01:13:03,320
manipulate it directly. So you can knock out that circuit or you can replace activations from one

658
01:13:03,320 --> 01:13:08,040
sentence with activations from another sentence. So in neuroscience, people sometimes do that as

659
01:13:08,040 --> 01:13:13,320
well. In animal research, for example, or there are certain kinds of stimulation that you can do

660
01:13:13,320 --> 01:13:21,960
that aren't harmful, but can maybe do the desired effect. In aphasia studies, these are natural

661
01:13:21,960 --> 01:13:26,520
causal experiments, right? We didn't cause delusion that destroyed the language network.

662
01:13:26,520 --> 01:13:34,360
But because we see those cases occur naturally, we can look at those effects. And so the causal

663
01:13:34,360 --> 01:13:41,560
tools are really powerful because they can really help us to see whether this part of the circuit is

664
01:13:41,560 --> 01:13:50,360
necessary for the behavior that we observe. And so in AI systems, we can do that quite easily.

665
01:13:50,360 --> 01:13:57,000
But conceptually, I would say in neuroscience and in AI, what we're trying to find out is very

666
01:13:57,000 --> 01:14:03,000
similar. Yeah. Yeah. And it's, I mean, it's wonderful, as you say, at least with the behavioral point,

667
01:14:03,640 --> 01:14:10,440
you can draw on the same kind of experiments that, you know, we finally have a kind of

668
01:14:10,440 --> 01:14:13,880
artificial intelligence that you can feed the same sort of things that you'd feed a person,

669
01:14:14,200 --> 01:14:18,760
i.e. sentences. And so it makes it very natural to run those kind of experiments.

670
01:14:20,200 --> 01:14:26,360
But then on the other hand, you can also go into the thing itself and tinker it with it in a way

671
01:14:26,360 --> 01:14:31,720
which would be very unethical and, you know, even just impossible with a person. So you could,

672
01:14:31,720 --> 01:14:38,280
I think there was one example where you had, I don't know, the concept of or Berlin was replaced

673
01:14:38,680 --> 01:14:45,560
with Paris or, no, what was it? It was Rome. Was it the Eiffel Tower was placed in

674
01:14:46,600 --> 01:14:49,960
conceptually into Rome or something like this? And you asked, well, how do you get from

675
01:14:51,400 --> 01:14:56,760
Berlin to the Eiffel Tower? It wasn't me, but it was, yeah, it's a famous kind of editing study.

676
01:14:57,800 --> 01:15:01,720
Yeah, I think I must have read it in one of your papers referring to it.

677
01:15:02,360 --> 01:15:14,440
And so the LLM does really, it kind of responds in the way that you would think if what's going

678
01:15:14,440 --> 01:15:19,480
on is that it has some kind of model of the world. And what all you've done is kind of

679
01:15:19,480 --> 01:15:23,720
switch around some pieces inside that model. It's not that it gets completely, you know,

680
01:15:23,720 --> 01:15:29,080
it doesn't throw everything completely out of whack, I suppose. And it even kind of

681
01:15:29,160 --> 01:15:32,760
infers some things that, you know, the Eiffel Tower will be in the center of Rome and

682
01:15:32,760 --> 01:15:36,440
it's going to be up with the Coliseum or something like that, which is, yeah,

683
01:15:37,080 --> 01:15:45,080
yeah, it's so fascinating to have something where we can kind of, you know, plausibly

684
01:15:45,080 --> 01:15:51,560
peer in into the internal workings. And yet just like the human brain, everything is

685
01:15:51,560 --> 01:15:57,320
so complicated that actually also it's not a trivial task, I guess.

686
01:15:57,640 --> 01:16:02,680
No, but that's the benefit, I guess, when neuroscientists have, we're used to dealing

687
01:16:02,680 --> 01:16:09,480
with this complexity. And, you know, there are ways to zoom out beyond just each individual

688
01:16:09,480 --> 01:16:16,280
neural unit to try and look at general trends and general patterns. And so I think a lot of

689
01:16:16,280 --> 01:16:21,320
people are daunted by the task of trying to understand the neural net because it's so big

690
01:16:21,320 --> 01:16:26,440
and complex. And because it's trained in this way where we don't necessarily know which features

691
01:16:26,440 --> 01:16:32,200
it sticks up on. But to me as a researcher, I'm just excited. It's like a cool puzzle to solve

692
01:16:32,200 --> 01:16:36,440
and a cool problem to understand. So generally, I'm pretty optimistic about this endeavor.

693
01:16:37,240 --> 01:16:43,400
Cool. Yeah. I think you mentioned at the very beginning that your research is now starting

694
01:16:43,400 --> 01:16:48,280
to look at some of the, you know, possibly trickier question of this kind of reflexive

695
01:16:49,080 --> 01:16:54,040
thinking, the narrow type of thinking I think you mentioned. So we've been talking a lot maybe

696
01:16:54,040 --> 01:17:01,400
about the broader definition of cognition of just kind of reasoning, manipulation of concepts,

697
01:17:01,400 --> 01:17:05,080
which might, one might even do in a very automatic way, as we were saying, like you might just solve

698
01:17:05,080 --> 01:17:10,440
a mass problem without really, you know, in a way where you'd say, oh, yeah, I didn't think about

699
01:17:10,440 --> 01:17:19,000
that. I just did it. But yeah, how does one, what kind of things have you, how can you pick,

700
01:17:19,000 --> 01:17:22,520
how can you look at this other problem of like when, when people kind of cogitate about things

701
01:17:22,520 --> 01:17:27,560
and turn them over in their, in their minds? Where are you going with that? I'm really curious.

702
01:17:29,720 --> 01:17:37,160
I think to me, the interesting question here is the question of individual differences. If

703
01:17:38,040 --> 01:17:43,720
some people report thinking in words most of the time and others say they don't think in words at

704
01:17:43,720 --> 01:17:48,920
all, presumably we should be able to see that at the brain level. Presumably we should be able to

705
01:17:48,920 --> 01:17:54,360
see the language network working hard for the first group and not at all for the second group

706
01:17:55,400 --> 01:18:01,160
while they're thinking right spontaneously in this task-free setting. And so that's really what I

707
01:18:01,160 --> 01:18:08,680
want to look at. But in order to do that, we need to have a good questionnaire that will capture

708
01:18:08,680 --> 01:18:15,000
those differences precisely, right? So I think instead of just asking people, although you think

709
01:18:15,000 --> 01:18:19,640
in words a lot, they're a little, it would be helpful to think, to get more information, right?

710
01:18:19,640 --> 01:18:30,200
Do they think in like, what does it mean? Like, what if other meta assessments of their own

711
01:18:30,200 --> 01:18:36,680
thinking style is reliable, right? So like, can we trust those judgments? How can we make them

712
01:18:36,680 --> 01:18:42,760
more granular? Another question that I'm very interested in, and that's really understood

713
01:18:42,760 --> 01:18:51,400
currently is, is there a difference between thinking in words and hearing the words, right?

714
01:18:51,400 --> 01:18:56,760
So if you're using some kind of words and some kind of language to think, does it mean that

715
01:18:56,760 --> 01:19:03,320
there is a voice or not necessarily? Some people, it turns out, they might see the words written

716
01:19:03,320 --> 01:19:11,320
in their mind's eyes, so they spell it out. It's a minority, like less than half of the population,

717
01:19:11,400 --> 01:19:16,040
but it does happen. And the capturing those differences, I think, is fascinating and then

718
01:19:16,040 --> 01:19:21,400
trying to look at the neural correlates to essentially establish the validity of those

719
01:19:21,400 --> 01:19:27,480
differences to show that they're really not just something that people perceive and report,

720
01:19:27,480 --> 01:19:35,000
but actually, that's not necessarily how they actually think. It's an interesting direction

721
01:19:35,000 --> 01:19:45,800
because psychology has this interesting history of an interesting relationship with phenomenology.

722
01:19:45,800 --> 01:19:51,400
So the people reporting their own experiences, right? That used to be very common, and then it

723
01:19:51,400 --> 01:20:01,000
turned out to result in a lot of pseudoscience and discredited a lot of psychology. And so then

724
01:20:01,000 --> 01:20:06,600
there was this huge turn to behaviorism where all that mattered was the stimulus and the response,

725
01:20:06,600 --> 01:20:14,600
and people were refusing to talk about any internal operations at all. So people are still

726
01:20:14,600 --> 01:20:21,160
very suspicious of phenomenology, so self-reporting experiences. And I think for the right reason,

727
01:20:21,160 --> 01:20:25,320
because often, yeah, we just don't know how we think. We're like, I think it's words or I think

728
01:20:25,320 --> 01:20:31,320
it's not. Sometimes we'll make a decision, like we were saying very quickly. And then when we have

729
01:20:31,320 --> 01:20:37,800
to explain what we did and how, we have to rationalize it. And so maybe that's actually not how we

730
01:20:37,800 --> 01:20:42,920
arrived at the decision, but post-talk, we come up with an explanation that might not correspond

731
01:20:42,920 --> 01:20:48,760
to the reality. So I think we'll have to be careful when taking people at their words.

732
01:20:49,880 --> 01:20:54,760
But to me, when people report this strike in differences of like, oh, yeah, I think in words,

733
01:20:54,840 --> 01:21:00,920
all the time versus like, never, not at all, it seems like there's something there and so I would

734
01:21:00,920 --> 01:21:05,720
love to use neuroscience to get at that question more deeply. Yeah, it's a tricky one. I mean,

735
01:21:05,720 --> 01:21:09,160
it strikes me that even the process of asking someone, do you think in words,

736
01:21:10,200 --> 01:21:16,440
it almost necessarily linguistic to communicate that because as we say, this is the way that we

737
01:21:16,440 --> 01:21:23,080
pass ideas around. And so maybe maybe there's just like that kind of arrogance in the language

738
01:21:23,080 --> 01:21:28,440
network, which is going to intercept that question and say, oh, yes, it's me. I do all the thinking.

739
01:21:29,320 --> 01:21:37,000
But as you say, well, many people do report thinking in many other ways. So yeah, I would,

740
01:21:37,640 --> 01:21:43,880
yeah, I'm really curious about what that shows. I mean, it's just so, this must surprise you all

741
01:21:43,880 --> 01:21:49,640
the time just how, you know, outwardly, we sort of walk around and we move around and we breathe

742
01:21:49,720 --> 01:21:55,480
and we have all our organs are, you know, working in pretty similar ways. And yet,

743
01:21:55,480 --> 01:22:01,880
internally, it might be, you know, we seem so heterogeneous, I guess. Does that sound about

744
01:22:01,880 --> 01:22:07,160
right? Or am I overstating the kind of differences in brains that we have?

745
01:22:09,880 --> 01:22:13,800
I don't know. I think, yeah, it just depends on your intuition about, you know, how much

746
01:22:13,800 --> 01:22:17,240
similarity and differences you would expect. Of course, our personalities are very different,

747
01:22:17,240 --> 01:22:21,160
right? Our likes and dislikes are interesting. So at the cognitive level, there are lots of

748
01:22:21,160 --> 01:22:28,280
differences between people, of course. And so I guess the interesting thing is that we have

749
01:22:28,280 --> 01:22:34,440
this huge differences in how we perceive our own thinking, but they don't necessarily manifest

750
01:22:34,440 --> 01:22:41,400
very obviously in differences in this year, right? So in addition to differences in inner

751
01:22:41,480 --> 01:22:49,000
speech, another common example is differences in mental imagery, right? So it turns out that some

752
01:22:49,000 --> 01:22:57,480
people never experience visual images in their mind's eye. When they're asked to imagine a red

753
01:22:57,480 --> 01:23:04,120
apple, they will think about the concept of an apple and like redness, but they will not like

754
01:23:04,120 --> 01:23:11,000
see a red apple in front of them when they close their eyes. And so that phenomenon has a name,

755
01:23:11,080 --> 01:23:18,280
aphantasia. And the name got coined in 2015. So very recently, really. And this is the phenomenon

756
01:23:18,280 --> 01:23:23,640
that kind of got discovered over the centuries at various times and then forgotten again,

757
01:23:23,640 --> 01:23:28,440
and rediscovered because again, people just tend to assume that everybody else has the same

758
01:23:28,440 --> 01:23:34,120
roughly inner experience with them. And so those differences just end up getting neglected.

759
01:23:35,560 --> 01:23:40,120
But it turns out that people with aphantasia, you know, again, you cannot tell them apart very

760
01:23:40,120 --> 01:23:45,800
easily from people with this visual imagery. So it turns out that lots of things we do in the world,

761
01:23:46,680 --> 01:23:51,640
you can do whether or not the experience images visually. Similarly, whether you have strong

762
01:23:51,640 --> 01:23:58,360
inner speech or not, turns out it's you can't really spot these people very easily out in the

763
01:23:58,360 --> 01:24:03,000
wild because they act very differently. So that's the interesting thing, right? Despite these

764
01:24:03,000 --> 01:24:08,920
experiences being so different, somehow we can still act in roughly similar ways and do the

765
01:24:08,920 --> 01:24:13,880
tasks that we need to do in the world. We might be using different strategies, it's very possible,

766
01:24:13,880 --> 01:24:18,760
but the end result is that actually, those differences are very hard to see.

767
01:24:19,560 --> 01:24:24,680
Yeah, yeah, that is fascinating. Yeah, actually, I have a friend who is an aphant, I guess.

768
01:24:25,800 --> 01:24:33,560
And well, he didn't find out until a few years ago. And we, you know, there's no kind of outward

769
01:24:33,560 --> 01:24:39,960
sign, right? You just seem completely, you know, normal. But then we're like, oh, yeah, I just

770
01:24:39,960 --> 01:24:44,680
can't visualize triangles, right? I know what a triangle is, I can reason about triangles, I can.

771
01:24:46,120 --> 01:24:50,440
And actually, often there's, I think there might be some research which shows that that in some ways,

772
01:24:51,640 --> 01:24:57,080
oftentimes better at reasoning about certain things, where one might think it requires a

773
01:24:57,960 --> 01:25:04,040
visual element. But yeah, he was, you know, a very good physicist, very good colleague,

774
01:25:05,800 --> 01:25:08,280
but just thought in a different way, I guess.

775
01:25:12,120 --> 01:25:17,960
Yeah, I think we'll find some differences, right? Like, now that there's more awareness,

776
01:25:17,960 --> 01:25:22,200
once we start doing more systematic research, I think we'll, like, I mean, there are already

777
01:25:22,200 --> 01:25:28,200
attempts, trying to look at the relationship between aphantasia and episodic memory,

778
01:25:29,000 --> 01:25:35,960
turned out that aphantasia and, yeah, spatial reasoning, geometric reasoning there, the link

779
01:25:37,080 --> 01:25:43,640
is not as strong and may or maybe not even there, even though people expected it to be.

780
01:25:43,640 --> 01:25:47,560
And there are variations as to why. But yeah, essentially, like, I think,

781
01:25:47,560 --> 01:25:51,320
even though those differences aren't apparent, I think we'll find some eventually.

782
01:25:51,320 --> 01:25:55,160
And probably we'll just find out that different people are using different strategies to do the

783
01:25:55,160 --> 01:26:00,680
same thing. Some of them might require imagery or thinking in words, you know, speech, and some

784
01:26:00,680 --> 01:26:10,360
might not. I mean, all this is a reminder that well, LLMs might be, we might end up producing

785
01:26:10,360 --> 01:26:17,080
artificial intelligences, which outwardly look very similar. But we shouldn't, or yeah, we should

786
01:26:17,080 --> 01:26:23,000
be careful to think that to be mindful that inwardly, they could be very, very different.

787
01:26:24,440 --> 01:26:31,080
And I think it's very true, and it's already happening, right? There are all those cases where

788
01:26:31,960 --> 01:26:39,000
people were screenshotting chat GPT responses, especially right after it came out a year ago,

789
01:26:39,000 --> 01:26:44,360
and just showing it responding to some very complicated prompt and doing it correctly,

790
01:26:44,440 --> 01:26:48,120
and people were so impressed being like, oh, you know, if you have, if you put like,

791
01:26:49,960 --> 01:26:57,240
a mail on a table, like, you know, like on on a chair, is that like a construction

792
01:26:57,240 --> 01:27:03,000
stable or not? Or, you know, what kind of thing we can put on top? And like, it looks very impressive.

793
01:27:03,000 --> 01:27:09,080
And then it turns out that if you change the problem just slightly, then it just starts

794
01:27:09,080 --> 01:27:16,360
spitting out total nonsense. And the same thing happened with a social reasoning problem, kind

795
01:27:16,360 --> 01:27:20,680
of predicting what the other person would think, which is a classical problem from psychology.

796
01:27:20,680 --> 01:27:26,440
And so the claim was that, you know, now LLMs can reason about people and what they do. And then,

797
01:27:26,440 --> 01:27:31,480
again, at third dollar, we change the prompt to be slightly different from the problems that

798
01:27:31,480 --> 01:27:38,120
were already available on the internet, then model performance drops drastically. So it's very

799
01:27:38,120 --> 01:27:46,280
easy to fall for this seemingly impressive performance, seemingly seemingly impressive

800
01:27:46,280 --> 01:27:53,960
understanding. And so luckily for us, in this case, there are ways to design even behavioral

801
01:27:53,960 --> 01:27:59,960
interventions that can maybe help us figure out what's actually going on and what strategy is

802
01:27:59,960 --> 01:28:05,560
actually being used. Yeah, yeah, that's a very important problem for sure. I found the social

803
01:28:05,560 --> 01:28:11,800
reasoning example really, I just loved it. So I think if I remember correctly, one of the ways

804
01:28:11,800 --> 01:28:18,360
that you fall them is just inserting a few words in the middle. So the classic, the classic one

805
01:28:18,360 --> 01:28:25,000
is something like, you know, Susie hides Bob's apple. It was in the closet. Where does he think

806
01:28:25,000 --> 01:28:31,080
the apple is? And no, the thing will think it will say, Oh, not in the closet anymore.

807
01:28:31,800 --> 01:28:35,800
Correct. But if you change it, say, well, Susie hides Bob's apple, it was in the closet,

808
01:28:35,800 --> 01:28:41,400
and she tells him that she hid it, right? And then the LLM still says, Oh, he thinks it's not in

809
01:28:41,400 --> 01:28:50,040
the closet because it and I think, yeah, so clearly they've been either on the training set or in

810
01:28:50,040 --> 01:28:57,240
the fine tuning, you know, it's gone so far in getting them to kind of give the appearance of

811
01:28:57,240 --> 01:29:02,040
having some kind of theory of mind and being able to solve those problems. But then with a little

812
01:29:02,040 --> 01:29:08,040
bit of tweaking, it becomes apparent that they they don't. But it seems to be coming harder and

813
01:29:08,040 --> 01:29:18,040
harder to, you know, fox these systems. And you point out that one of the real difficulties is

814
01:29:18,040 --> 01:29:24,200
we just, you know, without knowing what has gone into the training of open AI's models and

815
01:29:24,520 --> 01:29:32,680
it, it's hard to know how much genuine kind of intelligence has emerged and how much is just

816
01:29:32,680 --> 01:29:39,160
kind of pan recognition and, and, you know, or simple pan recognition and recall. But of course,

817
01:29:39,160 --> 01:29:43,480
the paradox here as well, they have some of the most advanced models. So maybe there's something

818
01:29:43,480 --> 01:29:49,720
genuinely interesting going on, but it's black box. So we can't really say. But I think it's

819
01:29:49,720 --> 01:29:57,880
encouraging that other folks are, you know, mistrial perhaps are being more open and about

820
01:29:57,880 --> 01:30:05,160
what's going into that into their models. So, you know, maybe easier to, to test on the very

821
01:30:05,160 --> 01:30:11,800
latest things and have confidence that I don't know, some problem was not appearing verbatim in

822
01:30:11,800 --> 01:30:15,960
in the training set. But even then, yeah, it could be some very similar problem.

823
01:30:16,280 --> 01:30:23,960
Yeah, for sure. And like, it's not always bad if they've been fine tuned on this problem. And if

824
01:30:23,960 --> 01:30:29,880
they've seen examples before, as humans learn lots of things, we can do math right off the bat,

825
01:30:29,880 --> 01:30:34,920
we learn, we learn from examples. It's easier for us to the problems that are familiar to us and

826
01:30:34,920 --> 01:30:41,160
novel ones, like, that's fine. It's just important to know, because that helps us figure out what

827
01:30:41,160 --> 01:30:46,600
is the mechanism that they're using, or potentially using, and what are the keys is where they might

828
01:30:46,600 --> 01:30:54,040
break, right? So it's, we don't necessarily need to expect these models to be like amazing,

829
01:30:54,040 --> 01:30:59,880
zero shot thinkers on total and novel problems all the time. It's just that, yeah, knowing

830
01:30:59,880 --> 01:31:03,240
what goes in the training data, knowing what problems they've been fine tuned on,

831
01:31:03,240 --> 01:31:08,840
just helps us assess them more accurately. Yeah, yeah, that's a good point. It's unfair to

832
01:31:09,800 --> 01:31:13,800
sort of demand development so that they don't learn anything, or they don't benefit from fine

833
01:31:13,800 --> 01:31:22,680
tuning. But yeah, but we want to know if they are, if the way that they're responding is using,

834
01:31:23,960 --> 01:31:27,960
is that they've abstracted patterns, or they've just, they're just regurgitating.

835
01:31:30,360 --> 01:31:36,840
Yeah, well, I, yeah, I found it really interesting to go through your work. I'm usually,

836
01:31:37,800 --> 01:31:45,160
I don't know, optimistic, I guess, or maybe not, maybe that's the wrong word. I really admire how

837
01:31:45,160 --> 01:31:53,720
LLMs are, have taken off, and it surprised me how quickly they've advanced. But one thing I've

838
01:31:53,720 --> 01:31:58,280
enjoyed about your work is kind of reminding me, well, actually, maybe they're not as far along in

839
01:31:58,280 --> 01:32:03,960
some ways as, as they appear to be at the surface level. Like there still seems to be some,

840
01:32:04,920 --> 01:32:09,640
some nuts to crack here to make them, to bring them closer to, to human thought.

841
01:32:11,000 --> 01:32:18,120
What's your take? Do you think, you know, I don't want to ask the typical how far away is AGI,

842
01:32:18,120 --> 01:32:27,240
because, but, you know, are you of the opinion that just cranking through more data

843
01:32:27,880 --> 01:32:35,960
is going to continue to produce results, or should more be invested in this kind of modularity

844
01:32:36,600 --> 01:32:45,960
approach? And if the latter, well, do we have, you know, are the things that are taking place

845
01:32:45,960 --> 01:32:50,920
on the right track, or do we need to look more closely and learn more from the human mind, perhaps?

846
01:32:51,000 --> 01:33:02,120
I think we definitely should recognize all the impressive achievements that we observe in LLMs

847
01:33:02,120 --> 01:33:09,640
today. And one way that my colleagues and I have been thinking about it is through the formal

848
01:33:09,640 --> 01:33:16,120
functional competence lens. On the formal competence front, learning the rules and patterns of natural

849
01:33:16,120 --> 01:33:24,120
language. These models have been incredibly impressive. So even a couple of years ago,

850
01:33:24,120 --> 01:33:28,440
they almost reached the ceiling for English, at least the language where they have the most data.

851
01:33:29,160 --> 01:33:36,120
And they did it without the need for any fine tuning. So just learning to predict words and

852
01:33:36,120 --> 01:33:43,880
method corpora of text. Turns out that is something that gives you all the most of the grammar and

853
01:33:44,520 --> 01:33:51,400
knowledge of idioms and all kinds of patterns that characterize a language. And that wasn't trivial

854
01:33:51,400 --> 01:33:56,760
at all. That was the subject of debate for linguists for decades. Is it possible to just learn from

855
01:33:56,760 --> 01:34:03,400
data? Or do you need a rich set of internal rules that can help you figure out what's grammatical

856
01:34:03,400 --> 01:34:09,480
and what's not? So that is incredibly impressive scientifically. And on the engineering front,

857
01:34:10,360 --> 01:34:16,280
different language processing systems in the past have struggled so much because you just can't

858
01:34:16,280 --> 01:34:22,280
encode language with a few simple rules or even not simple rules. Just like so fuzzy and there are

859
01:34:22,280 --> 01:34:26,280
so many exceptions in the regular forms and this and that. And the fact that these models have

860
01:34:26,280 --> 01:34:30,120
mastered that is so impressive. And people kind of forget and start talking about AGI right away,

861
01:34:30,120 --> 01:34:34,840
but that's an impressive achievement. Then being good at language is already very impressive.

862
01:34:35,800 --> 01:34:40,760
And then we get to functional components and their ability to reason and be factually accurate,

863
01:34:40,760 --> 01:34:45,320
know what's true and what's false and be actually helpful. And so that's a whole other host of

864
01:34:45,320 --> 01:34:53,880
problems where they actually seem to be spotty. They have achieved a lot because of pattern

865
01:34:53,880 --> 01:34:58,520
recognition, but then it turns out that that performance is not robust and it breaks. And

866
01:34:58,600 --> 01:35:08,440
so that's where it gets more complicated and more controversial. And that's where we argue modularity

867
01:35:08,440 --> 01:35:15,400
will be helpful. Again, looking at the human brain as an example. And one distinction we make though

868
01:35:15,400 --> 01:35:24,200
is that the modularity doesn't necessarily have to be built in by design. So this built-in approach

869
01:35:24,200 --> 01:35:29,720
we call architectural modularity where we have a language model and let's say a path and code

870
01:35:29,720 --> 01:35:33,640
interpreter and we put them together and they're clearly different and they're doing different

871
01:35:33,640 --> 01:35:39,160
things. So that can be promising, but then of course you need to know what the right modules are,

872
01:35:39,160 --> 01:35:45,080
you need to set them up in the right way. An alternative approach that might work for certain

873
01:35:45,080 --> 01:35:51,240
cases is what we call emergent modularity where you start out with one network, but you don't

874
01:35:51,320 --> 01:35:56,200
necessarily specify what parts need to be doing, what you let the network figure that out over the

875
01:35:56,200 --> 01:36:01,000
course of training and you can have different parts self-specialized to do different things.

876
01:36:01,640 --> 01:36:06,600
That might require some changes to the architecture to be able to promote the kind of

877
01:36:06,600 --> 01:36:12,360
specialization. It might require changes in the objective function, maybe next work prediction

878
01:36:12,360 --> 01:36:21,080
alone is not necessarily going to be good. And it might require changes in the training data

879
01:36:21,240 --> 01:36:28,120
kind of like what's happening with fine tuning today where you are feeding its specific problems

880
01:36:28,120 --> 01:36:33,000
that you're asking the model to do in a specific way so you might have selectively boost the

881
01:36:33,000 --> 01:36:37,480
social reasoning and the formal reasoning and the factual knowledge. There might be specific

882
01:36:37,480 --> 01:36:45,080
things you need to do, but there is a lot of promise in these approaches and the paper where

883
01:36:45,080 --> 01:36:51,160
we introduced the formal and functional competence, it's something we started working on in 2020

884
01:36:51,480 --> 01:36:57,720
around the start of the pandemic. Language models were around then, but not nearly as advanced and

885
01:36:57,720 --> 01:37:03,880
as we were writing the paper and in fact after the initial preprint version came out, that's when

886
01:37:03,880 --> 01:37:10,280
we started seeing the field, the developers shifting away from this simple scaling up approach,

887
01:37:10,280 --> 01:37:15,320
that's not the approach that's common anymore. People have started to shift towards specialized

888
01:37:15,320 --> 01:37:21,320
fine tuning, using very targeted data sets to improve performance on specific domains,

889
01:37:21,320 --> 01:37:26,440
coupling an LLM with external modules, all of those things that we kind of suggested that might

890
01:37:26,440 --> 01:37:31,320
be good because that's more brain-like. RAG became big, all of those things are something we've seen

891
01:37:31,320 --> 01:37:38,440
over the past year that's very encouraging that now the AI field is also recognizing that it's

892
01:37:38,440 --> 01:37:42,200
not just about scale, that you do benefit from different components working together.

893
01:37:43,160 --> 01:37:53,480
Yeah, I think very exciting to see what comes next, both in your work and in the field of LLMs,

894
01:37:53,480 --> 01:37:59,480
which it seems like maybe someone's listening to what you're suggesting because all these things

895
01:37:59,560 --> 01:38:09,400
are happening. Yeah, I don't know if you have any final comments or predictions,

896
01:38:11,960 --> 01:38:22,840
warnings of doom often come up in these discussions, but this has been surprisingly positive.

897
01:38:23,160 --> 01:38:30,520
No, I just think that science is important and we just need to use good methods and not run after

898
01:38:30,520 --> 01:38:36,840
the hype and be realistic in how we evaluate the strengths and limitations of these models.

899
01:38:37,720 --> 01:38:41,720
There are strengths, there are limitations, so being too far on just the positive and just

900
01:38:41,720 --> 01:38:46,360
negative is not necessarily the most productive. We just want to be able to disentangle them

901
01:38:46,360 --> 01:38:53,720
effectively. Yeah, Rene, thank you so much, Anna. This has been really insightful. Yeah, thank you.

902
01:39:16,360 --> 01:39:17,740
you

