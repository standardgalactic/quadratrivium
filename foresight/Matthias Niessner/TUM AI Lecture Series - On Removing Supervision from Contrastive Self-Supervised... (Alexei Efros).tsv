start	end	text
0	12800	All right, welcome everybody today to my lecture.
12800	14960	It's a real pleasure to have Alyosha Efros today.
14960	19920	Alyosha is a professor at UC Berkeley, where he's part of the Berkeley Artificial Intelligence
19920	22200	Research Lab there.
22200	26240	His work is at the intersection of graphics and computer vision, and I'm sure pretty much
26240	29240	everybody in the community has heard of him.
29240	33400	He's a pioneer at the intersection in these fields.
33400	38880	He has countless of exciting papers, starting from texture synthesis to conditional GANs
38880	41720	like Pics to Pics and Cycle GAN.
41720	46480	He's particularly known for his creativity and thought-provoking work, which is an inspiration
46480	48280	to many young researchers.
48280	51320	His students have also had really great success.
51320	54960	You can see many of his students are now professors themselves.
54960	60120	And I think it's also, yeah, it's fair to say that he has also a very great social
60120	63840	engagement, in particular, contributing to the research community.
63840	68280	If you haven't met him in person, I can only recommend reach out to him at the conferences
68280	69280	once we have them again.
69280	71040	It's really great to have him around.
71040	75320	It's really great hanging around with him at one of the poster sessions and chat about
75320	76760	some really exciting research.
76760	81000	He's particularly known for his, yeah, really cool attitude, and it's really great to have
81000	82800	him as part of the community.
82800	84880	So it's a real pleasure to have you here.
84880	86800	And I'm really looking forward to the talk.
86800	89640	And you also promised some philosophical components.
89640	94120	So I'm really excited what that's going to be.
94120	97960	Thank you so much for such a gracious introduction.
97960	111680	Yeah, I'm sad that we have to do this virtually because I would love to have hung out with
111680	116400	you guys and gone for some wonderful Barbarian beers.
116400	121880	But next time, yes, thank you very much for inviting me.
121880	131040	And it's such a star spangled roster of speakers that you have there.
131040	136520	I hope I will not disappoint.
136520	142960	And so I'm going to talk about aspects of self-supervision.
142960	148360	Self-supervision is something that my lab has been working for a number of years.
148360	153520	And just to make sure everyone's on the same page, self-supervised learning is when we,
153520	161280	this is my definition, you know, hopefully there is no one set definition, but my definition
161280	169960	is that it's when we use the tools of supervised learning, but where the labels are the raw
169960	174600	data instead of being human-provided.
174600	181640	And so the question that often students ask is why use self-supervised learning?
181640	183480	What's the point?
183480	189120	And the classic answer, the common answer is because labels are expensive.
189120	196400	Instead of having humans provide labels and annotating them using lots of hours of work
196400	205960	or high cost, here we get used the data itself as our labels.
205960	210320	So this is the common answer, but that's not really my main answer.
210320	214480	This is, it's nice, but it's not the main reason for me.
214480	218320	For me, I actually have a couple of answers.
218320	226360	The first one answer is that self-supervised learning allows us to get away from the tyranny
226360	233240	of this top-down semantic categorization that goes all the way to Plato and Socrates.
233240	237960	And I'll tell you why I think this is a good thing.
237960	246280	And the second reason is that self-supervised learning will hopefully enable us to move away
246520	256160	from this idea of a fixed training set to a more continuous lifelong learning where you
256160	260120	have data streaming in and then you learn on the go.
260120	264840	You learn as you live, rather than having this kind of training set, testing set, split
264840	270120	that is kind of the classic thing in machine learning.
270120	274440	So I will start, and most of the talk is going to be about the first one, and then hopefully
274440	278000	we'll do a little bit of number two, okay?
278000	282520	So what's the problem with semantic categories?
282520	288320	Well, let's look at, from the visual point of view, let's look at a couple of categories
288320	291440	from a standard visual data set.
291440	296360	So the first is what's called a chair, okay?
296360	306280	But you can look at all the different chairs that you could have in the wild, and you realize
306280	316360	that this is very hard to find what is in common between all of these chairs, right?
316360	321360	Visually, actually, there is pretty much nothing in common because between something
321360	323240	like this and something like this, right?
323240	327760	This chair is really more of a functional category, right?
327760	329520	And think about it this way.
329520	335840	We can see them all being chairs because we have seen, you know, butts being squeezed
335840	338300	into these places.
338300	344240	But if you are a computer, if all you've seen in your life is ImageNet data set, for example,
344240	350120	if you've never seen any videos, you've never seen any people sitting on anything, there
350120	355920	is basically no way for you to realize that all of these are somehow related, right?
355920	360600	So the relationship here is not visual.
360600	362440	The relationship is functional.
362440	366040	The relationship is that of affordances.
366040	372200	And it's not really fair for a computer to try really, really hard to find a way to kind
372200	378680	of somehow bring them all into some kind of a connection where maybe there isn't that
378680	382400	much of a connection, okay?
382400	386760	The second example I like is this one called City, okay?
386760	392320	And here what I'm showing here is a picture of downtown Pittsburgh in a picture of the
392320	394440	center of Paris, okay?
394440	401960	And frankly, you know, the fact that both of these by some fluke of the English language
401960	408600	are termed City, the same noun, this is just kind of a coincidence because there is really
408600	412720	nothing visually in common between those two, right?
412720	416320	And people can argue, well, wait a minute, you know, both contain buildings, but look
416320	417320	at the buildings.
417320	421760	The buildings are so different visually, there is nothing in common between those buildings,
421760	422760	right?
422760	423760	They're so visually different.
423760	427280	And then you can say, well, but they both have windows, but again, look at the windows.
427280	429240	The windows don't look anything the same.
429240	434960	Look, there is really not a single pixel in common between these two things.
434960	440920	And so again, we are forcing the computer to do something, to somehow try to generalize
440920	446880	across these two things that are very, very different, that might not have anything in
446880	447960	common.
447960	453960	And so, in a way, what we're forcing the computer to do is basically to cheat, right?
453960	462040	It's kind of like you haven't attended classes and then now it's your final exam and you're
462040	467240	trying to see what, you know, you're trying to cram for the final exam.
467240	470360	And so what the computer is going to do is like, anything that looks like this, it will
470360	473920	be called the city, anything that looks like this will also be called the city.
473920	477880	But you're not going to basically get the concept of a city.
477880	484720	You're just going to basically just remember the, you know, the nearest neighbors, right?
484720	492080	So basically with labels like these, we're, I worry that we're setting ourselves up for
492080	493080	failure, right?
493080	502200	We're setting our algorithms up for just memorizing examples and not really even having building
502200	504120	connections between them, okay?
504120	508560	Which is of course very bad if we want our models to generalize, right?
508560	512720	If we just wanted to, you know, train an image net and then test an image net, that's fine.
512720	517200	But if we wanted to train an image net and test on, you know, the real world out there,
517200	518200	then that's not fine.
518200	522840	We need to somehow induce generalization, okay?
522840	532440	And so this is where my promised bit of philosophy comes in, which is that I argue, I've been
532440	541440	arguing functionally for many years now, that we should step away from the stop down categorization
541440	548000	paradigm and try to think more of it as a bottom up association.
548000	555600	And there is some movement towards that in, especially in the 20th century.
555600	560520	The philosophers have really kind of pushed away plateaus and Socrates notions of these
560520	568840	rigid categories and really started to think about categorization in a much more bottom
568840	570240	up way.
570240	578320	So Plato, of course, he argued that categories were a list of, you know, there were abstract
578320	581180	definitions with a list of shared properties.
581180	586400	And then in the mid 20th century, the philosopher Wittgenstein came out said, no, no, no, no,
586400	588560	this is that people don't do this.
588560	594280	In fact, people are much more fluid about categories.
594280	599160	And, you know, for example, you know, if you ask people, you know, our curtains furniture,
599200	603560	olives, fruit, different people will give you different answers.
603560	607760	In fact, the same person might give you different answers different times of different times
607760	608760	you ask them.
608760	620280	Wittgenstein's classic puzzle was to ask people to name all what is in common across all games.
620280	625480	What are the common properties shared by all games and not shared by non games?
625480	627600	And that you cannot do it.
627600	632840	It's just impossible that there's such a variety of games out there that you cannot think of
632840	635640	any single thing that defines a game.
635640	638680	It's not a definitional thing.
638680	643000	It's much more kind of data driven, much more bottom up kind of, well, it's a game like
643000	644000	like football.
644000	647000	It's a game like chess, right?
647000	654520	And so in the second half of the 20th century, psychologists in particular, Eleanor Rush
654520	663120	have tried to kind of update the notion of categorization and tied to think about categories
663120	669600	forming from the bottom up and her famous prototype theory of categorization has really
669600	676560	started this trend where the idea was that you basically cluster, you do bottom up clustering
676560	682480	of similar instances into these prototypes and the prototypes then get clustered again
682480	688240	and then you have this kind of a bottom up hierarchy where the categories emerge directly
688240	692800	from the data rather than being kind of this top down, okay?
692800	699880	And later folks have, psychologists have gone even further and argued for what they call
699880	708360	the exemplar based theory of categorization where you basically, you don't really have
708360	717640	categories when you kind, you basically just store instances, store exemplars of everything
717640	723080	that you see and then you basically learn associations between those examples and the
723080	727120	things that are closer together, essentially can you can think of it as like this kind
727120	733360	of a soft clustering of your exemplars into chunks and to groups and those groups then
733360	736400	emerge to be categories.
737280	744120	My favorite slogan is provided by a neuroscientist Moshe Barker who says, ask not what is it,
744120	750000	ask what is it like to this kind of a, this idea that association bottom up association
750000	755080	trumps this top down labeling, okay?
755080	763400	And so this work has been very inspiring to me and my group over the years and we have
763400	770240	been kind of chugging away at in this direction for a number of years and maybe one of the
770240	775720	earlier works that we did was with my former student Tamash Milosevich where we basically
775720	783720	try to kind of instantiate this exemplar based way of thinking about categorization
783720	792680	and here the example, the idea is that we basically try to find per-exampler distances
792680	794440	given a set of data.
794440	802480	So you have a set of labeled data, you know, cars, people, pedestrians, trees, etc.
802480	807440	And here instead of trying to separate all cars from all pedestrians, for example, we
807440	814560	wanted to basically learn a way to group every single instance of say car.
814560	823200	So for this particular focal example of a car, we wanted to find what other things are
823200	824560	close to it, right?
824560	830560	And so those other things should be also labeled car, but there shouldn't be all cars because,
830560	833480	you know, this car, for example, looks nothing like this car.
833480	837240	So should they really be in the same cluster in the same category?
837240	838240	Maybe not.
838240	844320	And so the idea that Tamash came up with was to basically kind of treat this as a kind
844320	849920	of a classification problem where you basically learn a decision boundary between things that
849920	856680	are close to your focal example and things that are far, but here we actually have instead
856680	862520	of two classes, the ones that are inside of a category and one's out, we have three classes.
862520	865840	There are the things that are close and these are kind of these kind of cars.
865840	870000	There are things that are not cars and they're on the other side.
870000	872960	And then there is a third class which is don't care.
872960	879840	And these are basically other cars that might not actually be that close to the focal car.
879840	888680	And then you basically optimize this, optimize this decision boundary given some set of constraints.
888680	895400	And as a result, what you get is you get something where you learn these distances that are, that
895400	906040	produce much more visually meaningful relationships rather than if you were just doing a standard
906040	910440	set of distances without this, okay?
910440	915720	And so this was kind of a, we were very excited about this, but still we're here, we're still
915720	920920	using the label car, the labels are still being used in this computation.
920920	924400	And we really wanted to go away from labels entirely.
924400	929360	And this is where Tamash's next paper came in.
929360	936560	And this work, we call it exemplar SVM, was basically kind of pushing this farther and
936560	945960	really thinking about it in terms of classifiers and basically switching from the standard
945960	952440	classifier where you have class A and class B instead to take every single instance, every
952440	961120	single data point and train a separate classifier for that one instance against everyone else,
961120	964000	whether it's your own class or a different class.
964000	966760	So it's one against all classifier.
966760	970040	So this is kind of an interesting way to think about it because it's really basically you're
970040	976560	defining yourself, not by who is in your category, who is in your class, but you're
976560	980560	defining yourself by what you are not, right?
980560	987360	So what makes you different from everyone else in your data, okay?
987360	996960	And then the cool result was that we were able to do one classifier for every instance
996960	998840	and then assemble them together.
998840	1008160	And the result was that this assemble actually worked no worse in many cases than your standard
1008160	1012400	two-class classifier like an SVM, okay?
1012400	1018560	And this was a little bit of a, it was kind of like a bit of a trolling paper, which basically
1018560	1024400	kind of tried to push the community to say, look, maybe you're not getting as much juice
1024400	1030560	as you think you are from basically trying to group all those things into one class because
1030560	1037960	it seems like if you don't do it, you get basically as much of performance as not, okay?
1037960	1042920	And so we also use the same idea for retrieval.
1042920	1051360	So basically the idea is you take, at runtime, you take your retrieval query image and then
1051360	1058280	you have a big data set and you're basically training at runtime an SVM classifier to separate
1058280	1062120	your query from everything else, okay?
1062120	1071360	And then you order all of your data based on that, on the coefficients of that decision
1071360	1072360	boundary, okay?
1072360	1079480	You basically find who are the closest things, who are your support vectors inside of your
1079480	1085360	data set and those tend to be the retrieval examples, the kind of the closest ones on
1085360	1091920	the other side will be the retrieval examples and that also worked surprisingly well.
1091920	1096560	And so we were very excited about this and we were very hopeful and then of course deep
1096560	1105280	learning revolution hit and all of this became irrelevant because much better classifiers
1105280	1107640	came on the scene.
1107640	1117880	We tried to update it for the deep learning age and we didn't really succeed but Alexey
1117880	1121280	DeSavitsky and colleagues did, okay?
1121280	1129040	So one of the kind of very early influential papers was called exemplar CNN, which basically
1129040	1139840	adopted the same idea of one against all classification on using neural networks, okay?
1139840	1147360	And the main difference that we didn't really think of was that whereas we used a single
1147360	1156160	exemplar, one image against everything else, DeSavitsky and colleagues they used what's
1156160	1158360	called data augmentation.
1158360	1164000	So they basically, they took one example and then they created a whole bunch of similar
1164000	1173200	examples by basically applying various different transformations to it, you know, changing
1173200	1180680	lighting, changing contrast, changing shapes, you know, various geometric transformation,
1180680	1181680	etc.
1181840	1188800	In the end, even though all of these things came from a single example, they all were
1188800	1193880	a little bit different and so this became the positive class and then everything else
1193880	1196880	became the negative class and that worked really, really well, okay?
1196880	1205280	And this work really was an inspiration for a lot of the current contrast of self-supervised
1205280	1209480	learning that we are familiar with right now.
1209480	1215600	So we thought we will be very pure and just use a single image, but this of course worked
1215600	1216840	much, much better.
1216840	1221840	And of course, now in kind of modern day, the self-supervised learning representations
1221840	1230920	that seem to work the best, they're all based on this idea of similarity learning, of instead
1230920	1238960	of learning which class you are, which category you are, the idea is to learn instances that
1238960	1241760	are either close or far from each other.
1241760	1249360	So learning the distances between the instances in your training data, okay?
1249360	1255440	So things like metric learning, SiameseNet and the new contrastive learning are all based
1255440	1256920	on that same principle.
1256920	1262560	So you basically, you have some sort of an embedding space and your goal is to say, okay,
1262560	1268240	for a given positive like this particular instance of a dog here, you create a bunch
1268240	1272120	of different positive example by data augmentation.
1272120	1278240	And then you basically try to push these ones, all of them to be close to each other in this
1278240	1288160	embedding space and far away from other things which are dogs or other cats, et cetera.
1288160	1294920	And this learning of the similarity is really what a lot of the contemporary self-supervised
1294920	1298920	learning methods are doing, okay?
1298920	1306840	And so, you know, the reason why this, maybe like a year ago, this area really took off,
1306840	1312120	one of the reasons is, of course, you know, the improvements in the representation learning.
1312120	1318040	The contrastive formulation is actually just works much better as shown by papers like
1318040	1320480	Simplier and stuff.
1320480	1326640	But another reason I think that's maybe being a little bit underappreciated is that we are
1326640	1331520	just much better at doing this data augmentation.
1331520	1338400	So we have learned to do data augmentation in a better way than the Seitzky and his exemplar
1338400	1339560	Sienna, okay?
1339560	1345280	For example, now cropping is a very standard trick for data augmentation, which wasn't a
1345280	1349560	standard trick before, and that gives us a lot of boost.
1349560	1355280	So again, what data augmentation is, you get yourself an input image, a single instance,
1355280	1360160	and then, you know, you just randomly create a whole bunch of different versions of that
1360160	1368240	image by applying a whole bunch of different parameter transformation, whole transformation,
1368240	1371880	cropping, flipping, blurring, et cetera, et cetera, et cetera, okay?
1371880	1379720	And then once you do that, then you set up your kind of a distance function.
1379720	1387360	So you basically say that I want these two images that all came from the same image really,
1387360	1392800	I want those two images to be similar in our embedding space.
1392800	1399080	So I'm going to try to bring them close together and farther away from the other images in
1399080	1400080	my data set.
1400200	1405160	That's really the whole story of contrastive learning, okay?
1405160	1413440	Now the thing is that the choice of data augmentation itself turns out to be very, very critical.
1413440	1421760	And in fact, I want to argue that this data augmentation is itself a little bit of supervised
1421760	1429960	learning, because the way you choose your augmentation can make a huge difference in
1429960	1432800	your final performance, okay?
1432800	1439600	So here is an example from our recent paper in iClear 21, where you can think of, let's
1439600	1445320	say that you have different types of data augmentation, like color augmentation, maybe
1445320	1448760	rotation, and maybe texture, okay?
1448760	1456680	So now we can look at different tasks, for example, if we want something like ImageNet,
1456680	1464200	course-level categorization, then data augmentation with color makes a lot of sense, with texture
1464200	1468560	also makes a lot of sense, but rotation is actually going to hurt you, because an upside-down
1468560	1472600	elephant is not going to be recognized as an elephant, okay?
1472600	1478720	Whereas if your task is, for example, fine-grain recognition, well, then it gets even more
1478720	1483040	complicated, because if you're fine-grain the different species of birds, then actually
1483040	1487240	you don't want any of those data augmentations, because they're all meaningful, like changing
1487240	1493320	texture may change the species, changing the color definitely will change the species.
1493320	1499200	Whereas maybe if you're classifying different types of flowers, then rotation is fine, because
1499200	1506760	rotation augmentation, you know, because flowers are usually rotationally symmetric, okay?
1506760	1513360	And so you can see that it really becomes very, very task-dependent.
1513360	1516800	And another example is the cropping and image classification.
1516800	1522760	So cropping for something like ImageNet classification makes a lot of sense, because you have one
1522760	1526160	big object in the center of the image.
1526160	1531280	But the same cropping for object detection actually doesn't make that much sense, because
1531280	1537280	you crop it and you might lose, you know, where your object is, okay?
1537280	1543120	And so this is something that we started to worry about, because we feel like a lot of
1543120	1551880	the advances in the modern self-supervised learning might actually be due to us being
1551880	1558200	very good at overfitting the right kind of data augmentation for a particular problem
1558200	1562080	rather than the actual methods themselves, okay?
1562080	1570120	So what we wanted to do is to try to do contrastive self-supervised learning without data augmentation,
1570120	1578240	to really try to get it to, to figure it out on its own without this kind of help, okay?
1578240	1582880	And the way we wanted to do this is to make these, these augmentations, these what they're
1582880	1588840	called views, to make them latent, to make the computer come up with its own data augmentation
1588840	1591520	in this, in a sense, okay?
1591520	1597360	And of course, the big question here is, this is all great, but where do you get the supervisory
1597360	1598360	signal, right?
1598360	1599360	There is no free light.
1599360	1603280	You need to get some sort of supervisory signal from somewhere in your data.
1603280	1605760	So where is it going to come from?
1605760	1609840	And here we're going to, I'm going to talk about a couple of papers where we answer
1609840	1611520	this question differently.
1611520	1619640	The first paper, the answer we have is that we want to use time as our self-supervisor
1619640	1621680	signal, okay?
1621680	1627160	And here I have a wonderful quote from one of my favorite writers, Jorge Luis Borges,
1627160	1634760	in his short story about fumes, who is this kind of a man on the spectrum.
1634760	1640840	He writes, it irritated him that the dog at 3.14 in the afternoon seen in profile should
1640840	1646640	be indicated by the same down as dog at 3.15 seen in front of it, okay?
1646640	1657040	So basically what he's talking about is that two different instances of time makes most
1657040	1666000	of us assume that there is some continuity of what we are perceiving, that the dog here
1666000	1670280	and the dog here, it's almost certainly the same dog.
1670280	1674800	But nothing happened to this dog while it was jumping in the water, right?
1674800	1682280	But fumes, of course, couldn't figure this out and neither can our computers, right?
1682280	1691560	And so the idea is that this temporal correspondence, basically time as a way to align things together,
1691560	1698120	to bring things into correspondence, is a very powerful supervisory signal that we should
1698120	1699960	be using, okay?
1699960	1706600	And we have evidence that biological algorithms use it very strongly.
1706600	1721440	There is plenty of psychology data for human infants that shows that temporal cues are
1721440	1724160	very important to learning vision.
1724160	1730760	And there is this wonderful line of work by Wood who basically did this kind of this
1730760	1733820	experiments with newly born chicks.
1733820	1739640	So basically what he says he has is this kind of VR cave for chickens, for little chicks.
1739640	1744520	So you put an egg and then the chicken is born and the chicken is born in this VR cave
1744520	1747920	where he's basically projected things on all sides.
1747920	1754720	So everything that the chick knows from birth is being controlled by the researcher, okay?
1754720	1761360	And what he showed that some of the chicks were shown videos that were not temporally
1761360	1765200	coherent, that basically broke this temporal continuity.
1765200	1772880	As you can see, it was not kind of physically correct and he compared them to chicks that
1772880	1777040	were shown normal, normal continuous things.
1777040	1785800	And the chicks who saw these continuous patterns, they were not able to function in the world
1785800	1786800	as well.
1786800	1791920	They lacked some of the visual perception skills.
1791920	1797920	So that showed that this is extremely important, that temporal continuity is extremely important.
1797920	1806240	And so what we want to do in this work is to use video as data augmentation, as a way
1806240	1813680	to create these data augmented views ourselves, okay?
1813680	1821880	And basically the main thing, of course, is that this can provide correspondences across
1821880	1830200	different instances and allows the computer to learn how something looks across time change,
1830200	1831360	okay?
1831360	1839160	But it can give us a bit more because we can also think about contextual relationships
1839160	1844600	and notice things that are moving in the same way, what Bernheimer called common fate.
1844600	1852920	We also use that as a way to group little points, little trajectories into groups and
1852920	1857280	maybe get to the notion of objects from the notion of points, okay?
1857280	1861680	Again something that you can use temporal information for, okay?
1861680	1864720	And so this is basically the story.
1864720	1870200	And then the question is how do we harness this information without any sort of supervisory
1870200	1871200	signal, okay?
1871200	1879120	And in the past, people have used things like slow feature learning where they basically
1879120	1885840	kind of looked at connecting nearby frames together, basically look at nearby frames
1885840	1890520	as the positives and far away frames as the negatives, but you just collapse the entire
1890520	1896160	frame and so that's not really something that can give you these point tracks, it's much
1896160	1898160	more coarse signal.
1898600	1905000	Alternatively people use things like optical flow or tracking to create correspondences
1905000	1910120	using some off the shelf methods and then use learning to connect things that are supposed
1910120	1911280	to be in correspondence.
1911280	1916800	But here you're using two different methods and so what we wanted to do is do something
1916800	1921360	like this but kind of in one go, in one pack.
1921360	1930640	And this is our paper that tries to do this, this was published in Europe's past year
1930640	1933720	and here is the idea, okay?
1933720	1941080	And so for a warm up, let's consider the case where you actually do have labels.
1941080	1946400	Let's say that somebody went ahead and labeled that this patch corresponds to this patch,
1946400	1947400	okay?
1947400	1955280	And your goal is to basically learn a representation that brings things that are the same into
1955280	1959520	correspondence and away from things that are different, right?
1959520	1963800	So in this case of course it's very simple, you just say, okay, these two things are my
1963800	1968480	two positives, I want them to be close together and all the other patches are my negatives,
1968480	1972760	I want them to be pushed far away and then you have your standard self-supervised learning,
1972760	1975800	contrastive learning problem and off you go, right?
1975800	1977080	Nothing very exciting.
1977080	1983600	So things get a little bit more exciting if you have maybe another frame in between, okay?
1983600	1986800	Because now you have, these two guys are your two positives.
1986800	1996600	We know this by labeling but also because this is a video, we know that from here somehow
1996600	2005520	it needed to go to be in this final place and so there must have been a path, the most
2006440	2011040	likely path from this guy to go from here to here and the most likely path is going
2011040	2018040	to go through this patch so it's reasonable to assume that this patch should also be in
2018040	2022840	our positive category, it should be in the same category as this guy and this guy.
2022840	2027920	So now these triplets should be the positives and everything else should be the negatives.
2027920	2033240	So now you can think of a little bit of kind of automatic data documentation that this guy
2033320	2040680	just by virtue of being tracked in the video becomes a data augmented positive for our kids,
2040680	2042040	okay?
2042040	2047000	So this is okay but this is still requiring us to have this supervision.
2047000	2048360	How could we get rid of supervision?
2048360	2055400	Well, we are going to use our old trick which we've used before called cycle consistency
2055400	2060040	and what we're going to do is we're going to make this video into a palindrome.
2060040	2065160	Palindrome if you remember in language is a word that you read it forward and backwards
2065160	2066520	and it reads the same, right?
2066520	2069000	So how can we make a palindrome out of a video?
2069000	2074760	Well, what we can do is we can take this video and flip it around and put it back
2076440	2078280	in the reverse order, okay?
2078280	2082680	So now what we have is we have something like frame one, frame two, frame three
2082680	2086040	and now we're going back to frame two and then frame one, okay?
2086040	2090680	So now we have this new video that's a palindrome and look what's happening.
2090680	2099800	Now the final place, the destination is now exactly the same as the origin by construction.
2100520	2103560	So now we don't need supervision anymore.
2103560	2104920	We got rid of supervision.
2104920	2108760	All we need to do is to get from the blue guy to the green guy
2109080	2117720	and the way we do it is we're basically trying to do a track through this video
2118280	2123800	and everything that's on this track should be in our positive category
2123800	2125960	and everything else should be in the next, okay?
2125960	2132600	And so now you can see the setup where we basically are getting something out of nothing.
2132600	2140360	We're getting some supervisory signal just from the mere video information, okay?
2140360	2143640	So basically the story is that we are going to take a video.
2143640	2150280	We're going to make it a palindrome by going from t to t plus k and then minus to back to t.
2150280	2153320	We're going to make it into a graph.
2153320	2156760	We're going to turn the video into a graph, okay?
2157480	2166440	And then we're going to walk along this graph until we get to the end, okay?
2166440	2170040	And we're going to do basically a random walk on this graph
2170840	2173560	and then we're going to steer that random walk
2174200	2177960	such that if you start from this blue point right here,
2178600	2182520	we want to steer it to get us to this green point right here, okay?
2183400	2186520	And this is going to be our only supervision.
2186520	2189880	The supervision is going to be at the last frame where we're going to say that the green,
2190520	2194280	the positive is going to be anything that lands on the green dot
2194280	2200120	and negative is going to be anything that lands anywhere else, anywhere on this red dot, okay?
2200120	2203000	And that's basically going to be the signal that we're going to use, okay?
2205080	2211640	So, and also notice that we don't have to have a single path through this graph.
2211640	2219480	We kind of naturally, we can incorporate probabilistic information
2219480	2222680	by tracing many paths through this graph, okay?
2224040	2227720	So, you know, how do you turn the video into a graph?
2227720	2229240	Well, it's kind of a standard thing.
2229240	2232600	You know, create nodes and then, you know, you're basically,
2232600	2236280	your nodes are some representation and some using some encoder, phi.
2236280	2239240	And really, this phi is really the only thing that you're learning.
2239240	2244600	What you're learning is you're learning the representation of each patch
2244600	2245720	in your feature space.
2245720	2251720	And you're basically trying to figure out how to arrange those features
2252360	2255720	in your representation, who's going to be close, who's going to be far, okay?
2256280	2261080	And so, now your video is going to be a graph.
2261080	2267800	And then from frame T to frame T plus one, you're just going to have a transition matrix
2267800	2273720	that's just going to say, you know, where did all the points from T go in T plus one?
2273720	2278120	And then this is just basically like a dot product in the feature space.
2278120	2284280	So, the closest things are going to be to get the higher dot product, okay?
2284280	2286600	And then how are we going to do it around the work?
2286600	2291240	Well, just going to compose all of these transition matrices A,
2291240	2295000	just like, you know, standard Markov chain, you know, you're just multiplying
2295000	2302280	all of those transition matrices and you get your full work on this graph, right?
2302280	2308520	So, now kind of the nice thing is that the task of learning this representation phi
2308520	2313640	is essentially the same as fitting these transition probabilities, okay?
2313640	2318440	You find the right transition probabilities and it gives you your representation phi, okay?
2319400	2325640	So, again, in kind of a, if we do have the target somewhere, if we do have the supervision,
2325640	2329640	then this is just a standard contrast of learning problem.
2329640	2337480	You basically, you want to find a representation where this query goes directly to the target.
2337480	2339640	It doesn't go to the red ones, right?
2339640	2342840	And this is basically just, this is your positive, this is your negative,
2342840	2345800	this is your standard kind of static learning problem.
2345800	2351720	If you have multiple frames in between, then in a sense, you have some latent views
2351720	2353880	that you can also use, right?
2353880	2360280	So, now we have, these are all the positives and these are the negatives if you,
2360280	2361880	if it's an obvious path.
2361880	2366840	But if you have multiple, multiple hyperability paths and these will be late, like,
2368520	2372680	awaited positives and these will be weighted negatives and you can still do it.
2372680	2378040	And so, again, from a single point of supervision, you get all of this data-augmented information,
2378040	2378280	okay?
2380120	2387720	And of course, what we can do then is we can do the palindrome trick and now we get all of these
2387720	2394600	latent data-augmented positives without even providing any supervision, okay?
2394600	2399640	Because this is basically by construction, the target is the same as the quick, okay?
2400520	2406280	And so, where we can just set this up at training time and you can see that it's,
2407080	2412760	you know, if you pick a point in the query image, in the first image, you can see that
2412760	2417080	over time, it kind of gives you a little probability distribution of where that image
2417080	2423560	might have gone and you can say, well, maybe we can even do this by trying to get grouping
2423560	2428280	happening and find out a group which corresponds to a single object.
2428840	2433640	And to do that, we have a little extra thing that we can do which is we can do a dropout.
2433640	2441880	We can cut some of the engines away and force the correspondences to go through nearby paths
2441880	2446120	and that basically allows us to get a little bit more of this kind of grouping
2447320	2451880	happening where you basically kind of, you go through the paths that are also on the same
2451880	2459400	object, okay? And, you know, we basically violated it at runtime by essentially nearest
2459400	2466040	neighbor in the phi space and here are some examples. This is the kind of the state-of-the-art
2467240	2472520	label propagation results and this are the results of our methods and you can see that it's
2472520	2479080	it's basically behaving much better in terms of occlusion handling and just seems to do quite
2479080	2484920	a bit better. Here is again state-of-the-art self-supervised method and this is ours, right?
2486200	2491480	Even against supervised methods actually does pretty well even though it doesn't get any
2491480	2500280	sort of supervision. So here is kind of an example of how we do compared to some of the
2500280	2505160	self-supervised comparators and interestingly even for methods that are trained on image,
2505320	2511800	using image net representation we are actually doing better than that, okay? And here are some
2511800	2517800	examples of kind of propagating various things like skeletons or labels or things like that, okay?
2520680	2531080	So this is one way to use this contrastive learning as without data augmentation but of
2531080	2536520	course in my lab we also like to make pretty pictures and so I'll briefly show you another
2537160	2546120	way of using the same kind of an idea of kind of creating your own latent views for an image-to-image
2546120	2554680	translation setup, okay? And here the idea is of course image unfair translation. The classic
2554680	2559320	thing that we've been doing for a while we want to translate horses into zebras but we don't have
2559320	2564680	a correspondence between horses and zebras, okay? So we want to go from here to here but we don't
2564680	2574200	have a correspondence, okay? And of course the one powerful signal here is we can use a GAN loss
2574200	2581640	which basically says make this thing into a zebra by basically forcing it to look like other zebras
2581640	2587080	that I have seen, okay? Using a GAN loss but that GAN loss is not enough because it can make it look
2587160	2593560	like a zebra many ways, right? But we wanted to kind of be in correspondence and so this is where
2593560	2600440	we also want to have another constraint and in the past in works like cycle GAN we use the cycle
2600440	2605640	consistency constraint which says okay make it a zebra but also make it so that when you translate
2605640	2612440	it back you'll get back to the original horse and that kind of forces this to be the right answer,
2612440	2619160	not these, okay? But there is a problem with this cycle consistency constraint because the problem
2619160	2624520	is that it forces it to be a bijection and forces it to be one-to-one because yes it's not going to,
2625800	2633640	it's not going to go back to here but it's also going to constrain us to have to go back to this
2633640	2641160	particular horse whereas these other horses might have been just good enough, right? So this is where
2641160	2648600	kind of a bijection is not always desirable because sometimes these cycles are not a bijection.
2648600	2657240	So how could we kind of address this problem? And here is the approach that we came up with
2657240	2666040	which is we are going to have an image-to-image translation framework. We're starting with our
2666520	2671560	with our horse. We want to get a zebra so we have a GAN loss that says okay make this a zebra,
2671560	2678360	okay? And then what we're going to do in addition is we want to make this zebra to be
2679240	2686440	similar in structure to this horse but not in texture and what we're going to do way to do this
2686440	2694520	is we're going to enforce the structures to be the same by taking pairs of patches across the
2694520	2702600	input and the output and say that these two patches need to be close to each other in features play
2702600	2712840	the space and farther away than other patches from the horse image, okay? So you can see that again
2712840	2719160	we are getting this whole similarity learning story here where we are basically bringing these
2719160	2728600	two things to be closer and farther from the other patches of horse and again just unlike other
2728600	2735640	methods where the positives are somehow automatic created by data augmentation here the the the
2735640	2743720	positives are basically our input and our output so the output becomes our data augmentation, okay?
2744200	2751240	And now we are back into our contrastive learning land we just basically formulate this and we
2751240	2756520	basically say learn a representation such that these two guys are close so basically it kind
2756520	2762840	of ignores the texture and focuses on the structure and these things are fine, okay? And
2762840	2769480	and and of course what we do this we don't do this on just on the pixels we do it at different
2770120	2777560	levels of of representation at at basically different menu multi-scale patch representation
2777560	2784840	and we do this contrastive learning basically everywhere here in our decoder, okay? And of
2784840	2794280	course we also have our gap losses as usual, okay? One kind of interesting cute note for for those who
2795000	2799800	who are in this might appreciate this well we thought okay you know the positives that's
2799800	2804680	everything is clear with negatives we just take all the patches from the same image and then we
2804680	2810520	thought you know maybe it will be better if we take the negatives to be not just patches from
2810520	2816280	the same image but also just add other patches from other images other negatives, right? Even
2816280	2824760	should be even better even more negative data, right? And guess what? It turned out that this
2824760	2831560	did not work as well these external patches actually made performance worse than if we just
2831560	2838360	kept the eternal patch, okay? And this kind of goes back to some old work that that I have been
2838360	2844440	doing on textures this is where we also seen that that patches from the same image actually provide
2844440	2849880	much more information than if you start mixing them up with patches from other image and in fact
2849880	2857240	Michala Rani has this wonderful example story of doing super resolution using a single image
2857240	2863320	where she shows that you can do super resolution by learning from a single image you basically
2863320	2872280	take an image down sample it train a cnn to up sample that one image, right? So you basically
2872360	2877160	train a single image network and then just reuse that network for the original image.
2877160	2883720	So that works better than if you're if you're training a standard thing with many with a large
2883720	2890600	data set, okay? And basically we're seeing the same thing happening here that it's actually the
2890600	2896760	patches that are in the same image that have the same illumination the same you know camera
2896760	2901800	parameters the same setting they actually much more powerful information than if you just put
2901800	2908840	a whole data set. So this is kind of a cute little story and you can see here how using the
2908840	2914040	internal patches we get much better translation than if you we use external patches where you
2914040	2920920	can see that there is a lot of mode collapse happening, okay? So yeah, so basically that's
2920920	2930040	the story and this is our method and compared to compared to something like CycleGAN and other
2930040	2937000	methods as well and basically we can we see that we're basically getting as well performance as good
2937000	2944600	as CycleGAN in most cases but it's much faster and it's it's it's one sided you don't need to
2944600	2951880	train a two thing two-way thing and and it's basically a we think it's kind of a much better
2952680	2956840	a much better story and so here are some of the transformations that we have
2957560	2963000	and one cute thing that we can also do is we can basically apply this to instances so for
2963000	2967880	example let's say that we have a single image Claude Monet's painting we want to make it into
2969480	2975720	photograph and maybe what we have also is a single image instead of data set we have a
2975720	2982280	single photograph that is also let's say of water lilies, okay? Well we can basically use the same kind
2982280	2991880	of contrastive learning basically just between a single reference photo and our output, okay?
2993720	3001720	and have have have one have the same thing here for the for the positives and have a instead of
3001720	3007080	instead of again have basically just a single discriminator here and we can get something that
3007080	3014600	actually works quite a bit better than a lot of these kind of a stylization methods, okay?
3014600	3020280	So this is this is competitors and this is ours and I think that ours actually looks quite a bit
3020280	3032120	more natural and also better than CycleGAN. There are some other examples, okay? Let's see what
3032120	3042840	timing is. Well you know I don't think I have time to go over the second point of why you sell
3042840	3047560	supervision maybe I'll just give you a little bit of a hint of what I mean and then you can
3047560	3054520	you can read the paper if you're interested. So basically the idea is that it's a little bit weird
3055560	3061320	that we are in most of machine learning we are using a fixed training set. It's not very natural
3061400	3069080	biologically because biological agents they never see the same data twice, right? So you live your
3069080	3074600	life you never see the same thing twice. You see something first you you know you you deal with it
3076040	3081080	you hopefully learn from it if you you know if you didn't deal from it you're dead if you deal
3081080	3087400	with it you learn from it and then and then you you can recover some information from it but then
3087400	3093640	you never see that again you see maybe something similar, okay? So every new piece of data is
3093640	3102760	basically first in your test set and then in your training set, okay? And it seems like using a fixed
3102760	3108120	data set it kind of encourages memorization because you see the same exact thing over and over and over
3108120	3112840	again. In fact maybe this is actually another reason why data augmentation works because
3112920	3118600	data augmentation is kind of random you create a random thing every time so you kind of get away
3118600	3125400	a little bit from this memorization. So in fact this this might be kind of a subtle way in which
3125400	3130120	data augmentation helps that actually has nothing to do with the data augmentation just basically
3130120	3136680	randomization of your data, okay? But the point is that if you're using self supervised learning
3136680	3141400	like the whole point of having a fixed training set was because it was expensive to do all these
3141400	3147960	labels, you know? ImageNet, poor Fei Fei spent all of her startup money in Stanford labeling this
3147960	3153720	huge data set, right? So it kind of makes sense that it's it's fixed because it took so much money
3153720	3159000	to label it. But if you're using self supervised learning if you don't need the labels what's the
3159000	3164680	point of having a fixed data set? Why can't we just keep downloading images from whatever the
3164680	3170920	internet the TV whatever and just keep doing it all the time because we can generate our own
3170920	3178120	labels. Seems kind of natural and so this is where kind of I've been pushing on this idea of kind of
3178120	3183320	this online continual learning. So you can you can think of it in terms of of the standard
3183880	3188120	a train valve separation. So you know you have your training set and kind of the standard thing
3188120	3192520	in machine learning is you can separate it into a training set and a validation set, right?
3192520	3198520	And we know that if you just train on a training set and then use the validation set to tune your
3198520	3204760	high parameters you usually get better performance on the eventual test data than if you just train
3204760	3210360	on all the training set all at once. Even though you're kind of you you think that it's less data
3210360	3215400	that you're using for training but actually this is effect turns out to be more effective. Well we
3215400	3221400	can think of the same thing in a continual way. So we can think of it as you train on the data
3221400	3227400	that you have seen and then you're validating on the next data that comes along, okay? And then
3227400	3231960	once you do that you just incorporate it into your training center you can keep going and they
3231960	3237000	can keep going on forever. You don't ever need to stop, okay? And this I think is a kind of a very
3237000	3245800	powerful trick that is made that we can now do because we can use self-supervision to do this
3245800	3251160	kind of this evaluation, this testing, okay? And so this is the idea of test time training which is
3251160	3257320	our attempt to operationalize this on an infinite smoothly changing stream and the idea is to basically
3257640	3267000	use self-supervision to continuously adapt to new data, okay? And we did this already in the
3268440	3276040	case of reinforcement learning with our curiosity work and this new work is basically trying to do
3276040	3286360	it for images and this is the paper, test time training and it was in ICML 2000. Maybe
3286920	3293000	just give me, I'll give you one slide of intuition of what we're doing. Basically,
3293000	3301880	the idea is that we're, let's say we have a training set of object detection, right? And at
3301880	3308680	training time we have our standard thing, we have our image and we have our label so nothing new
3308680	3318360	here, we have input in, label out, we are training and then at the same time we also have a self-supervised
3318360	3325080	head that basically given your image it does some self-supervised task. In this case we are basically
3325080	3330760	our task here is rotation prediction. Given the rotated version of the image we want to predict
3330760	3334920	which rotation it is. It doesn't really matter, it could be any task at all, okay? So at training
3334920	3341480	time we do both of those tasks together but then at test time of course we don't have the labels
3341480	3348760	but we still have this task, okay? And so we can basically around this, we can evaluate this task
3349400	3358120	and if the result is not good, if it failed this self-supervised task we can do a little bit of
3358120	3364680	fine-tuning, a little bit of fine-tuning training for this other task but as we're doing the fine
3364680	3372760	tuning it's going to get changed representation in a way that will also impact the real task that
3372760	3381880	we care about and that allows us to do better as we are changing, as we're going for the dataset.
3381880	3389480	And so here is an example where you know given this image at test time basically the right label is
3389480	3396760	elephant but initially it basically thinks it's a dog but then as we do this fine-tuning on our
3399240	3405160	self-supervised task it figures out that it's actually less of a dog and more of an elephant
3405160	3410120	and gives us the right answer and that's basically the story of the paper, sorry I had to rush but
3410680	3419000	you can look at the paper online. And to conclude, why use self-supervision?
3421320	3427400	One reason that I like is that it allows us to get away from this top-down semantic categorization
3427400	3434760	and gets us more into this bottom-up association story and learn things from the bottom-up
3434760	3441240	but we must be careful that the supervision doesn't leak in through things like data augmentation
3441240	3447720	right and we need to be careful about this and second is that eventually self-supervision should
3447720	3454200	enable us to check the datasets, forget about all these fixed datasets and and learn things continuously
3455080	3460360	and it's you know we're still we're only starting on this direction I think it's very exciting direction
3460440	3466120	very exciting problem so I'm hoping people will get excited about it okay thank you very much
3467640	3470920	yeah awesome fantastic talk thanks a lot for all the amazing works
3472120	3477240	yeah self-supervision is cool and does anybody have some some some question on Zoom maybe let's
3477240	3480920	start with this we have a lot of questions on YouTube but I'm going to start to assume
3480920	3485800	I have a lot of questions too but if somebody wants to ask question on Zoom just turn on your video
3485800	3487720	and and just pick up probably
3493720	3500600	I can start with one with a maybe a higher level question first so I mean the challenge
3500600	3505480	in self-supervision is right you basically have visual data on you let's say correlate patches
3505480	3510600	with whatever contrastive loss or whatever whatever people do now um I mean what do you
3510600	3514360	think about if you're thinking about the 3d world right you have obviously a third dimension
3514920	3519560	is it a smart idea to do this actually all on on images and videos and not think about
3520280	3526680	I don't know like kind of project a 3d representation maybe first and then think about how to kind of
3526680	3532200	get similarities in some 3d space learn a 3d representation and then you know try to
3532200	3539000	channelize with the onscreen tasks later on right right no this is absolutely and and as you know
3539000	3545560	you know I've been I've been angling for for going into 3d you know since since since a long
3545560	3551400	time ago since our work with Derry Coyne on qualitative 3d I'm a I'm a big fan of 3d in
3551400	3557560	my heart and it's kind of a little bit sad that once you know once we went to neural networks
3557560	3562840	the kind of things dropped back to 2d plane for a while and now of course they're they're coming
3562840	3571000	back again um okay so there's there's two answers to this question one the final you know the the
3571000	3582120	ultimate answer is that 3d should emerge from our 2d of observation that the representation
3582120	3592680	should figure out 3d on its own okay uh just like it's done with humans right humans are only seeing
3593480	3599320	2d projections of the 3d world okay if you have stereo maybe you have a little bit of 3d but
3599320	3603640	you know I don't have stereo for example 10 percent of people in the world don't have stereo
3603640	3611320	and we are perfectly fine seeing 3d okay so we learn 3d from uh from from a series of 2d
3611320	3618840	representations uh and I think if we if we go from you know collections of images like ImageNet
3618840	3628840	to videos for example hopefully and I'm very hoping that like it will encourage 3d to automatically
3628840	3637960	emerge as as the you know inside of the representation okay so that's kind of a the the the the the
3637960	3644520	glorious answer at the end of the rainbow okay uh but of course this is this is very hard this is
3644520	3650040	kind of a a very tall order uh you know we are seeing a little bit of this happening we are seeing
3650040	3655880	a little bit of kind of a maybe two and a half two or two point one d kind of occlusion occlusion
3655880	3663720	reasoning you know figure ground reasoning uh a little bit of of that but but but it we're
3663720	3669640	they're definitely far away from that right and so the second direction is okay can we kind of help
3669640	3678760	it out a little bit how can we can we provide features that are more amenable to to to three
3678760	3687240	dimensional manipulation and there I think uh things like like like holo-gan or or pie-gan this
3687240	3694520	kind of directions are I think very exciting in in that it's kind of you you can inject some things
3694520	3701640	that you know are physically true like rotation for example and and uh and so I think I think in the
3701640	3709080	short in the short uh uh uh short term all of those things are I think going to be extremely
3709080	3715640	helpful in the long term I'm still kind of hoping that I can learn 3d from scratch okay but who knows
3715640	3721160	maybe it's too much to ask but I'm still kind of hoping that one day I will wake up in the morning
3721160	3727560	and boom my computer learned 3d but we'll see do it with two cameras right we have stereo that's I
3727560	3735560	mean that's the thing but I don't have stereo for example right like 10 percent of people don't
3735560	3745080	have stereo stereo is actually not as important as as as as we we we we think stereo is only really
3745960	3750440	important for like the you know half a meter in front of you it's like it's you know what
3750440	3755480	what is it that I cannot do that everybody else can do okay I cannot put you know thread through
3755480	3761320	the needle and I have trouble you know pouring wine right other than that I'm fine so really it
3761320	3767560	stereo is kind of over overemphasized I think it's really parallax is much more important
3767560	3773720	and parallax you can get from from video no good point I have another follow-up question
3773720	3779560	um so in the similar spirit right like one argument is you can do contrastive learning
3779560	3783720	and mostly it's about comparing things right you're saying one versus all classifiers like
3783720	3788440	how similar are these things I mean what about going back to the original things when people
3788440	3792760	using like auto encoders for pre-training and so on for like basically using generative tasks
3792760	3798760	let's say oh I train my favorite game how good of a representation can I learn from learning
3798760	3803800	the distribution basically right like how well like it's like this famous thing like you have to
3803800	3809640	be able to create in order to understand and where to see that competing or maybe going
3809640	3813160	going along the same line so what's your take channel is speaking on the lines there
3815080	3822360	um I mean yeah I mean yeah we have definitely been also using out encoders as well I think
3822360	3831640	with an outer encoder it's a little bit of a it's a little bit of a of a magic box like if you
3831640	3836920	know if you get really lucky your outer encoder is going to capture exactly the right things
3836920	3840840	and if you get unlikely it will capture all exactly the wrong things right it's it's kind of
3840840	3847720	it's a compression mechanism it somehow compresses your data and and it it really depends on what you
3847720	3853880	care about like sometimes it will compress the away the stuff that you care about or sometimes it
3853880	3859400	will retain the stuff that you care about and it's it's a little bit hard to control what it's going
3859400	3868200	to do so I think I think this kind of a similarity learning is it allows you to get a little bit
3868200	3874520	more control and a little bit of more of kind of a intuition about what is it what is it being
3875160	3879240	what is it that's being learned it's also kind of a has a very nice connection to kind of to
3879240	3883800	graphs and graph theory that kind of think you you think about it like you have different
3883800	3888360	entities and then you have kind of you can think of it like as a as like an as a like a network
3888360	3895000	right like a like a uh a social network for example where you you can think of different
3895000	3899640	people being connected in different ways and you can think about yeah we call them
3899640	3906040	senses of similarity so there's many different senses of similarity between two instances and
3906040	3909960	you know something like an out encoder is probably going to collapse them all together and here you
3909960	3914440	can actually separate them you can have a similarity in color similarity in texture maybe
3914440	3921720	similarity in 3d and they're all can be kind of exposed hopefully separately now that's interesting
3921720	3926200	I mean our experiences so we've done a lot of stuff on like shape completion in 3d so whenever we
3926200	3931000	had the ability to take stuff away and predict it then we got great features this was always amazing
3931000	3936280	in terms of using these features to help semantics and whenever we're trying to classify it to help
3936280	3940760	the completion this is this is always a total disaster it never worked we tried really hard
3940760	3948360	actually I mean I think that that's that's been our our experience as well but but I think have
3948360	3955160	you have you tried the latest uh contrastive learning because it's really it's to me the way
3955160	3962040	I think about contrastive learning is it's really just old school triplet loss you know uh Siamese
3962040	3970920	network learning except you're switching from from from from kind of a regression to to a
3970920	3977480	classification but it's a classification with like huge amounts of data and it's very very fine
3977480	3983720	grain classification so it's almost it's it's really not like your your grandma's classification
3983800	3990200	it's uh I mean we we did something like this for for uh for when we did colorization so we we first
3990200	3995960	we tried to do colorization with the regression and then we we we we got better results by doing
3995960	4002920	classification but the classification was across like you know 500 classes of different colors
4002920	4009480	in the in the in the color gamut right so it's it's a much more kind of narrow thing and that
4009480	4014280	seemed to work for us but yeah like if you have a few classes then then then it's very hard to
4014280	4019240	make it work but if you do something like either have lots of classes or do something like like
4019240	4025080	contrastive learning where it's basically just really kind of push it with data yet it seems to
4025080	4029240	to work for us now we've actually tried that so we had we had one one student project actually
4029240	4032920	in collaboration with fair so Chihu one of my students they've been working on basically
4034680	4038040	basically doing contrastive learning for pre-training 3d structures and in a similar
4038040	4045000	way than you would do it in 2d it does help but the completion still seems to work a bit better
4045880	4053960	it's very interesting I think so in general yeah I mean I think if if completions if if actually
4053960	4062200	predicting you know pixels or predicting voxels whatever um it it has it has more data it has
4062200	4071240	more information that and and we know that 3d world is actually much you know it's much more
4071240	4078520	informative right so and it's also I think much more um uni model so the one thing that was hard
4078520	4083000	for us for example when we did core colorization is what we're you know we're trying to colorize a
4083000	4088840	bird and the birds could be yellow or the bird could be green right and so you have multi model
4088840	4094120	you have two modes and if you're doing kind of a just sort of like a regression completion
4094120	4098760	what's it going to do it's going to do the average right so it's going to be neither here nor there
4098760	4105320	right but if you have a single mode it works really well so it might be that in 3d you're really
4105320	4112120	in a world that's much more uni model in which you don't have like you're not trying to have an
4112120	4116440	average between two different completions and they get something that doesn't look like either
4116440	4121880	but you're actually really focusing on a single mode so in that case maybe this is why you're
4121880	4128360	getting better results but if I suspect that if you had multi model like if you have a hole that's
4128360	4135800	big enough that you could have many different plausible completions happen to it I suspect that
4135800	4141880	that that then you're kind of uh the kind of the the prediction route might have more problems
4141880	4147160	no I fully agree with you no that one is definitely true and and but most of the case right you're
4147160	4151560	thinking about it's more like a dropout in a sense right so you're leaving out some stuff right and
4151560	4154600	then you're trying to figure out what's missing in this case I think we've experienced that it
4154600	4158760	works remarkably well if it's too large then you need probabilistic models again and stuff like
4158760	4164040	that then it's a lot more difficult um yeah I agree yeah I think I think it's kind of a
4164040	4169880	if it's a level of dropout and it should just work yeah I I I agree yeah yeah I think I think
4169880	4174920	if it works you should definitely use it absolutely um actually any other questions maybe
4174920	4178200	maybe somebody else can ask questions I don't want to dominate the discussion too much
4182040	4190760	hi yeah thanks for the talk um I have a similarly high level question so speaking along the lines of
4190760	4195960	like multi modality and and stuff like this it seems like you have a lot of inspiration in terms of
4196920	4205080	how to learn perception based on how people perform perception and it seems like people do have
4205080	4211400	naturally some kind of estimate of uncertainty multi modality and the ability to generate
4211400	4215480	also for like these video tracking kind of applications that you showed like multiple
4215480	4223560	hypotheses for where um the prediction should go and how far do you think you can get without this
4223640	4226760	explicitly modeled or do you think it needs to be explicitly modeled
4229000	4239000	good question I think I think I would go with I don't know so um yes humans are very good at
4239000	4245800	at modeling uncertainty but they I don't think they're doing it in the way that was the decisions
4245800	4253320	to it I don't think humans are actually probabilistic I think I think they might be doing it
4254040	4258840	almost like if you remember from a long time ago like all this particle filtering where you
4258840	4263720	can keep a whole bunch of hypotheses and then you kind of keep all of them going for a while and then
4263720	4269800	you kind of uh drop one like you know there's illusion of like young lady old woman visual
4269800	4275480	illusion where you know one day once time you see like an old lady one time you see a young woman
4275480	4282680	right and you never see both of them so it seems some there's some very interesting mechanism going
4282760	4292200	on but I think it's not it's not like a standard probabilistic mechanism and so yeah so I don't
4292200	4298840	know how to deal with it and in the in the vision in the in this video paper that I showed you know
4298840	4304520	we are really just keeping a whole bunch of hypotheses as we're going through the through
4304520	4312840	the video at training time at test time we don't and but whether that's the right thing to do or
4312840	4318360	not I don't know I think it's a very important question I don't I don't have an answer but
4318360	4327960	frankly I think that nobody else does either sounds good I think one other somewhat unrelated
4327960	4333400	thing I think there's a bit of a tension between people who think that we should be able to learn
4333400	4338920	everything from scratch like you mentioned in terms of being able to learn 3d and whether this
4338920	4344920	actually possible because it's unclear I guess how many how much supervision certainly for
4344920	4352120	like some rantic perception people get direct supervision and so yeah okay now we're back to
4352200	4359400	philosophy I think it must be possible because because it already happened right
4362040	4369640	supervised learning is something that happens in nature but it's it's very very rare like like
4370520	4376600	parents teaching their children things I know that a lot of modern parents they feel like it's
4376600	4382120	super super important but sorry you know a developmental psychologist disagree they say
4382120	4388120	that it doesn't really matter that much most of the things that a kid picks up they pick up
4388120	4395160	without supervision they keep pick up on their own and and you could think about it you know
4396440	4400600	from in the very beginning right in the beginning if you're you know as long as you believe in
4400600	4405640	evolution you must believe in in unsupervised or self-supervised learning because in the beginning
4405640	4410200	there was nothing there was no there was no teacher there was no supervision there was only
4410200	4415720	data right and kind of the organism and its environment were co-involving and learning
4415720	4423960	from each other and and and and and and develop so I think there is there is to me there is no
4423960	4431800	question that it should be possible in theory I think that the kind of the the the interesting
4431880	4437080	question is is it does it make sense to do in practice right and like you could also say well
4437080	4442760	why don't we just simulate evolution for for a gazillion years and then we'll get everything
4442760	4448120	right and that's of course not feasible with the current technology so I don't think that there is
4448120	4453400	that much or maybe there shouldn't be that much tension because I think I think there are people
4453400	4458040	like me who really want to try to learn things from first principles and I think this is very
4458040	4463720	interesting if if if anything from you know from the biological plausibility point of view
4463720	4469880	okay and there are people who just want to get stuff done and and and get to a good result too
4469880	4475880	fast and those people should definitely just use whatever works best at the time so I'm not sure
4475880	4481640	that it's either or I think both directions are are useful and I think we're learning from each
4481640	4486200	other I think those two direction directions are informing each other so for example for a very
4486200	4493640	long time self-supervised approaches worked worse than supervised approaches and so you know
4494360	4498600	if you know we could have all quit because oh my god you know our stuff doesn't work as well as
4498600	4503320	supervision but we persevered because we thought that you know there's something interesting that
4503320	4509960	that we could learn anyway and now what we are seeing is that for some tasks self-supervision
4509960	4514840	actually works better than supervised learning not for all not for many but for some there's
4514840	4521880	definitely some cases when it actually the the the learning from the data actually gives you
4521880	4527880	better results than learning from uh from from from from labels and so I think I think I think
4527880	4534520	you know let all the flowers bloom it I think both directions are useful and I think it's
4534520	4541640	it's great that people are are pushing in in in in both of them and I think we'll we'll get to a
4541720	4548040	better point eventually and we'll learn more so I'm actually optimistic on on on all fronts it's
4548040	4553240	yeah it's not a competition well it is a competition but it's not like it's not one is right another
4553240	4560280	is wrong I think both are right cool all right I think that's a that's a very good um I guess
4560280	4564280	ending of the live stream I think thanks a really lot for the amazing talk um we're a little bit
4564280	4569000	over I have to apologize to a lot of questions on YouTube we couldn't unfortunately go into all of
4569000	4575480	them um but um it was really great to have you and um I hope also for everybody who is with
4575480	4581080	with here right now next week we'll have another great lecture with rock help and yeah we'll see
4581080	4584200	so thanks a lot again for the for the great year I need great research
4588680	4589800	okay um
