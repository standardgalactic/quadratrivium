All right, welcome everybody today to my lecture.
It's a real pleasure to have Alyosha Efros today.
Alyosha is a professor at UC Berkeley, where he's part of the Berkeley Artificial Intelligence
Research Lab there.
His work is at the intersection of graphics and computer vision, and I'm sure pretty much
everybody in the community has heard of him.
He's a pioneer at the intersection in these fields.
He has countless of exciting papers, starting from texture synthesis to conditional GANs
like Pics to Pics and Cycle GAN.
He's particularly known for his creativity and thought-provoking work, which is an inspiration
to many young researchers.
His students have also had really great success.
You can see many of his students are now professors themselves.
And I think it's also, yeah, it's fair to say that he has also a very great social
engagement, in particular, contributing to the research community.
If you haven't met him in person, I can only recommend reach out to him at the conferences
once we have them again.
It's really great to have him around.
It's really great hanging around with him at one of the poster sessions and chat about
some really exciting research.
He's particularly known for his, yeah, really cool attitude, and it's really great to have
him as part of the community.
So it's a real pleasure to have you here.
And I'm really looking forward to the talk.
And you also promised some philosophical components.
So I'm really excited what that's going to be.
Thank you so much for such a gracious introduction.
Yeah, I'm sad that we have to do this virtually because I would love to have hung out with
you guys and gone for some wonderful Barbarian beers.
But next time, yes, thank you very much for inviting me.
And it's such a star spangled roster of speakers that you have there.
I hope I will not disappoint.
And so I'm going to talk about aspects of self-supervision.
Self-supervision is something that my lab has been working for a number of years.
And just to make sure everyone's on the same page, self-supervised learning is when we,
this is my definition, you know, hopefully there is no one set definition, but my definition
is that it's when we use the tools of supervised learning, but where the labels are the raw
data instead of being human-provided.
And so the question that often students ask is why use self-supervised learning?
What's the point?
And the classic answer, the common answer is because labels are expensive.
Instead of having humans provide labels and annotating them using lots of hours of work
or high cost, here we get used the data itself as our labels.
So this is the common answer, but that's not really my main answer.
This is, it's nice, but it's not the main reason for me.
For me, I actually have a couple of answers.
The first one answer is that self-supervised learning allows us to get away from the tyranny
of this top-down semantic categorization that goes all the way to Plato and Socrates.
And I'll tell you why I think this is a good thing.
And the second reason is that self-supervised learning will hopefully enable us to move away
from this idea of a fixed training set to a more continuous lifelong learning where you
have data streaming in and then you learn on the go.
You learn as you live, rather than having this kind of training set, testing set, split
that is kind of the classic thing in machine learning.
So I will start, and most of the talk is going to be about the first one, and then hopefully
we'll do a little bit of number two, okay?
So what's the problem with semantic categories?
Well, let's look at, from the visual point of view, let's look at a couple of categories
from a standard visual data set.
So the first is what's called a chair, okay?
But you can look at all the different chairs that you could have in the wild, and you realize
that this is very hard to find what is in common between all of these chairs, right?
Visually, actually, there is pretty much nothing in common because between something
like this and something like this, right?
This chair is really more of a functional category, right?
And think about it this way.
We can see them all being chairs because we have seen, you know, butts being squeezed
into these places.
But if you are a computer, if all you've seen in your life is ImageNet data set, for example,
if you've never seen any videos, you've never seen any people sitting on anything, there
is basically no way for you to realize that all of these are somehow related, right?
So the relationship here is not visual.
The relationship is functional.
The relationship is that of affordances.
And it's not really fair for a computer to try really, really hard to find a way to kind
of somehow bring them all into some kind of a connection where maybe there isn't that
much of a connection, okay?
The second example I like is this one called City, okay?
And here what I'm showing here is a picture of downtown Pittsburgh in a picture of the
center of Paris, okay?
And frankly, you know, the fact that both of these by some fluke of the English language
are termed City, the same noun, this is just kind of a coincidence because there is really
nothing visually in common between those two, right?
And people can argue, well, wait a minute, you know, both contain buildings, but look
at the buildings.
The buildings are so different visually, there is nothing in common between those buildings,
right?
They're so visually different.
And then you can say, well, but they both have windows, but again, look at the windows.
The windows don't look anything the same.
Look, there is really not a single pixel in common between these two things.
And so again, we are forcing the computer to do something, to somehow try to generalize
across these two things that are very, very different, that might not have anything in
common.
And so, in a way, what we're forcing the computer to do is basically to cheat, right?
It's kind of like you haven't attended classes and then now it's your final exam and you're
trying to see what, you know, you're trying to cram for the final exam.
And so what the computer is going to do is like, anything that looks like this, it will
be called the city, anything that looks like this will also be called the city.
But you're not going to basically get the concept of a city.
You're just going to basically just remember the, you know, the nearest neighbors, right?
So basically with labels like these, we're, I worry that we're setting ourselves up for
failure, right?
We're setting our algorithms up for just memorizing examples and not really even having building
connections between them, okay?
Which is of course very bad if we want our models to generalize, right?
If we just wanted to, you know, train an image net and then test an image net, that's fine.
But if we wanted to train an image net and test on, you know, the real world out there,
then that's not fine.
We need to somehow induce generalization, okay?
And so this is where my promised bit of philosophy comes in, which is that I argue, I've been
arguing functionally for many years now, that we should step away from the stop down categorization
paradigm and try to think more of it as a bottom up association.
And there is some movement towards that in, especially in the 20th century.
The philosophers have really kind of pushed away plateaus and Socrates notions of these
rigid categories and really started to think about categorization in a much more bottom
up way.
So Plato, of course, he argued that categories were a list of, you know, there were abstract
definitions with a list of shared properties.
And then in the mid 20th century, the philosopher Wittgenstein came out said, no, no, no, no,
this is that people don't do this.
In fact, people are much more fluid about categories.
And, you know, for example, you know, if you ask people, you know, our curtains furniture,
olives, fruit, different people will give you different answers.
In fact, the same person might give you different answers different times of different times
you ask them.
Wittgenstein's classic puzzle was to ask people to name all what is in common across all games.
What are the common properties shared by all games and not shared by non games?
And that you cannot do it.
It's just impossible that there's such a variety of games out there that you cannot think of
any single thing that defines a game.
It's not a definitional thing.
It's much more kind of data driven, much more bottom up kind of, well, it's a game like
like football.
It's a game like chess, right?
And so in the second half of the 20th century, psychologists in particular, Eleanor Rush
have tried to kind of update the notion of categorization and tied to think about categories
forming from the bottom up and her famous prototype theory of categorization has really
started this trend where the idea was that you basically cluster, you do bottom up clustering
of similar instances into these prototypes and the prototypes then get clustered again
and then you have this kind of a bottom up hierarchy where the categories emerge directly
from the data rather than being kind of this top down, okay?
And later folks have, psychologists have gone even further and argued for what they call
the exemplar based theory of categorization where you basically, you don't really have
categories when you kind, you basically just store instances, store exemplars of everything
that you see and then you basically learn associations between those examples and the
things that are closer together, essentially can you can think of it as like this kind
of a soft clustering of your exemplars into chunks and to groups and those groups then
emerge to be categories.
My favorite slogan is provided by a neuroscientist Moshe Barker who says, ask not what is it,
ask what is it like to this kind of a, this idea that association bottom up association
trumps this top down labeling, okay?
And so this work has been very inspiring to me and my group over the years and we have
been kind of chugging away at in this direction for a number of years and maybe one of the
earlier works that we did was with my former student Tamash Milosevich where we basically
try to kind of instantiate this exemplar based way of thinking about categorization
and here the example, the idea is that we basically try to find per-exampler distances
given a set of data.
So you have a set of labeled data, you know, cars, people, pedestrians, trees, etc.
And here instead of trying to separate all cars from all pedestrians, for example, we
wanted to basically learn a way to group every single instance of say car.
So for this particular focal example of a car, we wanted to find what other things are
close to it, right?
And so those other things should be also labeled car, but there shouldn't be all cars because,
you know, this car, for example, looks nothing like this car.
So should they really be in the same cluster in the same category?
Maybe not.
And so the idea that Tamash came up with was to basically kind of treat this as a kind
of a classification problem where you basically learn a decision boundary between things that
are close to your focal example and things that are far, but here we actually have instead
of two classes, the ones that are inside of a category and one's out, we have three classes.
There are the things that are close and these are kind of these kind of cars.
There are things that are not cars and they're on the other side.
And then there is a third class which is don't care.
And these are basically other cars that might not actually be that close to the focal car.
And then you basically optimize this, optimize this decision boundary given some set of constraints.
And as a result, what you get is you get something where you learn these distances that are, that
produce much more visually meaningful relationships rather than if you were just doing a standard
set of distances without this, okay?
And so this was kind of a, we were very excited about this, but still we're here, we're still
using the label car, the labels are still being used in this computation.
And we really wanted to go away from labels entirely.
And this is where Tamash's next paper came in.
And this work, we call it exemplar SVM, was basically kind of pushing this farther and
really thinking about it in terms of classifiers and basically switching from the standard
classifier where you have class A and class B instead to take every single instance, every
single data point and train a separate classifier for that one instance against everyone else,
whether it's your own class or a different class.
So it's one against all classifier.
So this is kind of an interesting way to think about it because it's really basically you're
defining yourself, not by who is in your category, who is in your class, but you're
defining yourself by what you are not, right?
So what makes you different from everyone else in your data, okay?
And then the cool result was that we were able to do one classifier for every instance
and then assemble them together.
And the result was that this assemble actually worked no worse in many cases than your standard
two-class classifier like an SVM, okay?
And this was a little bit of a, it was kind of like a bit of a trolling paper, which basically
kind of tried to push the community to say, look, maybe you're not getting as much juice
as you think you are from basically trying to group all those things into one class because
it seems like if you don't do it, you get basically as much of performance as not, okay?
And so we also use the same idea for retrieval.
So basically the idea is you take, at runtime, you take your retrieval query image and then
you have a big data set and you're basically training at runtime an SVM classifier to separate
your query from everything else, okay?
And then you order all of your data based on that, on the coefficients of that decision
boundary, okay?
You basically find who are the closest things, who are your support vectors inside of your
data set and those tend to be the retrieval examples, the kind of the closest ones on
the other side will be the retrieval examples and that also worked surprisingly well.
And so we were very excited about this and we were very hopeful and then of course deep
learning revolution hit and all of this became irrelevant because much better classifiers
came on the scene.
We tried to update it for the deep learning age and we didn't really succeed but Alexey
DeSavitsky and colleagues did, okay?
So one of the kind of very early influential papers was called exemplar CNN, which basically
adopted the same idea of one against all classification on using neural networks, okay?
And the main difference that we didn't really think of was that whereas we used a single
exemplar, one image against everything else, DeSavitsky and colleagues they used what's
called data augmentation.
So they basically, they took one example and then they created a whole bunch of similar
examples by basically applying various different transformations to it, you know, changing
lighting, changing contrast, changing shapes, you know, various geometric transformation,
etc.
In the end, even though all of these things came from a single example, they all were
a little bit different and so this became the positive class and then everything else
became the negative class and that worked really, really well, okay?
And this work really was an inspiration for a lot of the current contrast of self-supervised
learning that we are familiar with right now.
So we thought we will be very pure and just use a single image, but this of course worked
much, much better.
And of course, now in kind of modern day, the self-supervised learning representations
that seem to work the best, they're all based on this idea of similarity learning, of instead
of learning which class you are, which category you are, the idea is to learn instances that
are either close or far from each other.
So learning the distances between the instances in your training data, okay?
So things like metric learning, SiameseNet and the new contrastive learning are all based
on that same principle.
So you basically, you have some sort of an embedding space and your goal is to say, okay,
for a given positive like this particular instance of a dog here, you create a bunch
of different positive example by data augmentation.
And then you basically try to push these ones, all of them to be close to each other in this
embedding space and far away from other things which are dogs or other cats, et cetera.
And this learning of the similarity is really what a lot of the contemporary self-supervised
learning methods are doing, okay?
And so, you know, the reason why this, maybe like a year ago, this area really took off,
one of the reasons is, of course, you know, the improvements in the representation learning.
The contrastive formulation is actually just works much better as shown by papers like
Simplier and stuff.
But another reason I think that's maybe being a little bit underappreciated is that we are
just much better at doing this data augmentation.
So we have learned to do data augmentation in a better way than the Seitzky and his exemplar
Sienna, okay?
For example, now cropping is a very standard trick for data augmentation, which wasn't a
standard trick before, and that gives us a lot of boost.
So again, what data augmentation is, you get yourself an input image, a single instance,
and then, you know, you just randomly create a whole bunch of different versions of that
image by applying a whole bunch of different parameter transformation, whole transformation,
cropping, flipping, blurring, et cetera, et cetera, et cetera, okay?
And then once you do that, then you set up your kind of a distance function.
So you basically say that I want these two images that all came from the same image really,
I want those two images to be similar in our embedding space.
So I'm going to try to bring them close together and farther away from the other images in
my data set.
That's really the whole story of contrastive learning, okay?
Now the thing is that the choice of data augmentation itself turns out to be very, very critical.
And in fact, I want to argue that this data augmentation is itself a little bit of supervised
learning, because the way you choose your augmentation can make a huge difference in
your final performance, okay?
So here is an example from our recent paper in iClear 21, where you can think of, let's
say that you have different types of data augmentation, like color augmentation, maybe
rotation, and maybe texture, okay?
So now we can look at different tasks, for example, if we want something like ImageNet,
course-level categorization, then data augmentation with color makes a lot of sense, with texture
also makes a lot of sense, but rotation is actually going to hurt you, because an upside-down
elephant is not going to be recognized as an elephant, okay?
Whereas if your task is, for example, fine-grain recognition, well, then it gets even more
complicated, because if you're fine-grain the different species of birds, then actually
you don't want any of those data augmentations, because they're all meaningful, like changing
texture may change the species, changing the color definitely will change the species.
Whereas maybe if you're classifying different types of flowers, then rotation is fine, because
rotation augmentation, you know, because flowers are usually rotationally symmetric, okay?
And so you can see that it really becomes very, very task-dependent.
And another example is the cropping and image classification.
So cropping for something like ImageNet classification makes a lot of sense, because you have one
big object in the center of the image.
But the same cropping for object detection actually doesn't make that much sense, because
you crop it and you might lose, you know, where your object is, okay?
And so this is something that we started to worry about, because we feel like a lot of
the advances in the modern self-supervised learning might actually be due to us being
very good at overfitting the right kind of data augmentation for a particular problem
rather than the actual methods themselves, okay?
So what we wanted to do is to try to do contrastive self-supervised learning without data augmentation,
to really try to get it to, to figure it out on its own without this kind of help, okay?
And the way we wanted to do this is to make these, these augmentations, these what they're
called views, to make them latent, to make the computer come up with its own data augmentation
in this, in a sense, okay?
And of course, the big question here is, this is all great, but where do you get the supervisory
signal, right?
There is no free light.
You need to get some sort of supervisory signal from somewhere in your data.
So where is it going to come from?
And here we're going to, I'm going to talk about a couple of papers where we answer
this question differently.
The first paper, the answer we have is that we want to use time as our self-supervisor
signal, okay?
And here I have a wonderful quote from one of my favorite writers, Jorge Luis Borges,
in his short story about fumes, who is this kind of a man on the spectrum.
He writes, it irritated him that the dog at 3.14 in the afternoon seen in profile should
be indicated by the same down as dog at 3.15 seen in front of it, okay?
So basically what he's talking about is that two different instances of time makes most
of us assume that there is some continuity of what we are perceiving, that the dog here
and the dog here, it's almost certainly the same dog.
But nothing happened to this dog while it was jumping in the water, right?
But fumes, of course, couldn't figure this out and neither can our computers, right?
And so the idea is that this temporal correspondence, basically time as a way to align things together,
to bring things into correspondence, is a very powerful supervisory signal that we should
be using, okay?
And we have evidence that biological algorithms use it very strongly.
There is plenty of psychology data for human infants that shows that temporal cues are
very important to learning vision.
And there is this wonderful line of work by Wood who basically did this kind of this
experiments with newly born chicks.
So basically what he says he has is this kind of VR cave for chickens, for little chicks.
So you put an egg and then the chicken is born and the chicken is born in this VR cave
where he's basically projected things on all sides.
So everything that the chick knows from birth is being controlled by the researcher, okay?
And what he showed that some of the chicks were shown videos that were not temporally
coherent, that basically broke this temporal continuity.
As you can see, it was not kind of physically correct and he compared them to chicks that
were shown normal, normal continuous things.
And the chicks who saw these continuous patterns, they were not able to function in the world
as well.
They lacked some of the visual perception skills.
So that showed that this is extremely important, that temporal continuity is extremely important.
And so what we want to do in this work is to use video as data augmentation, as a way
to create these data augmented views ourselves, okay?
And basically the main thing, of course, is that this can provide correspondences across
different instances and allows the computer to learn how something looks across time change,
okay?
But it can give us a bit more because we can also think about contextual relationships
and notice things that are moving in the same way, what Bernheimer called common fate.
We also use that as a way to group little points, little trajectories into groups and
maybe get to the notion of objects from the notion of points, okay?
Again something that you can use temporal information for, okay?
And so this is basically the story.
And then the question is how do we harness this information without any sort of supervisory
signal, okay?
And in the past, people have used things like slow feature learning where they basically
kind of looked at connecting nearby frames together, basically look at nearby frames
as the positives and far away frames as the negatives, but you just collapse the entire
frame and so that's not really something that can give you these point tracks, it's much
more coarse signal.
Alternatively people use things like optical flow or tracking to create correspondences
using some off the shelf methods and then use learning to connect things that are supposed
to be in correspondence.
But here you're using two different methods and so what we wanted to do is do something
like this but kind of in one go, in one pack.
And this is our paper that tries to do this, this was published in Europe's past year
and here is the idea, okay?
And so for a warm up, let's consider the case where you actually do have labels.
Let's say that somebody went ahead and labeled that this patch corresponds to this patch,
okay?
And your goal is to basically learn a representation that brings things that are the same into
correspondence and away from things that are different, right?
So in this case of course it's very simple, you just say, okay, these two things are my
two positives, I want them to be close together and all the other patches are my negatives,
I want them to be pushed far away and then you have your standard self-supervised learning,
contrastive learning problem and off you go, right?
Nothing very exciting.
So things get a little bit more exciting if you have maybe another frame in between, okay?
Because now you have, these two guys are your two positives.
We know this by labeling but also because this is a video, we know that from here somehow
it needed to go to be in this final place and so there must have been a path, the most
likely path from this guy to go from here to here and the most likely path is going
to go through this patch so it's reasonable to assume that this patch should also be in
our positive category, it should be in the same category as this guy and this guy.
So now these triplets should be the positives and everything else should be the negatives.
So now you can think of a little bit of kind of automatic data documentation that this guy
just by virtue of being tracked in the video becomes a data augmented positive for our kids,
okay?
So this is okay but this is still requiring us to have this supervision.
How could we get rid of supervision?
Well, we are going to use our old trick which we've used before called cycle consistency
and what we're going to do is we're going to make this video into a palindrome.
Palindrome if you remember in language is a word that you read it forward and backwards
and it reads the same, right?
So how can we make a palindrome out of a video?
Well, what we can do is we can take this video and flip it around and put it back
in the reverse order, okay?
So now what we have is we have something like frame one, frame two, frame three
and now we're going back to frame two and then frame one, okay?
So now we have this new video that's a palindrome and look what's happening.
Now the final place, the destination is now exactly the same as the origin by construction.
So now we don't need supervision anymore.
We got rid of supervision.
All we need to do is to get from the blue guy to the green guy
and the way we do it is we're basically trying to do a track through this video
and everything that's on this track should be in our positive category
and everything else should be in the next, okay?
And so now you can see the setup where we basically are getting something out of nothing.
We're getting some supervisory signal just from the mere video information, okay?
So basically the story is that we are going to take a video.
We're going to make it a palindrome by going from t to t plus k and then minus to back to t.
We're going to make it into a graph.
We're going to turn the video into a graph, okay?
And then we're going to walk along this graph until we get to the end, okay?
And we're going to do basically a random walk on this graph
and then we're going to steer that random walk
such that if you start from this blue point right here,
we want to steer it to get us to this green point right here, okay?
And this is going to be our only supervision.
The supervision is going to be at the last frame where we're going to say that the green,
the positive is going to be anything that lands on the green dot
and negative is going to be anything that lands anywhere else, anywhere on this red dot, okay?
And that's basically going to be the signal that we're going to use, okay?
So, and also notice that we don't have to have a single path through this graph.
We kind of naturally, we can incorporate probabilistic information
by tracing many paths through this graph, okay?
So, you know, how do you turn the video into a graph?
Well, it's kind of a standard thing.
You know, create nodes and then, you know, you're basically,
your nodes are some representation and some using some encoder, phi.
And really, this phi is really the only thing that you're learning.
What you're learning is you're learning the representation of each patch
in your feature space.
And you're basically trying to figure out how to arrange those features
in your representation, who's going to be close, who's going to be far, okay?
And so, now your video is going to be a graph.
And then from frame T to frame T plus one, you're just going to have a transition matrix
that's just going to say, you know, where did all the points from T go in T plus one?
And then this is just basically like a dot product in the feature space.
So, the closest things are going to be to get the higher dot product, okay?
And then how are we going to do it around the work?
Well, just going to compose all of these transition matrices A,
just like, you know, standard Markov chain, you know, you're just multiplying
all of those transition matrices and you get your full work on this graph, right?
So, now kind of the nice thing is that the task of learning this representation phi
is essentially the same as fitting these transition probabilities, okay?
You find the right transition probabilities and it gives you your representation phi, okay?
So, again, in kind of a, if we do have the target somewhere, if we do have the supervision,
then this is just a standard contrast of learning problem.
You basically, you want to find a representation where this query goes directly to the target.
It doesn't go to the red ones, right?
And this is basically just, this is your positive, this is your negative,
this is your standard kind of static learning problem.
If you have multiple frames in between, then in a sense, you have some latent views
that you can also use, right?
So, now we have, these are all the positives and these are the negatives if you,
if it's an obvious path.
But if you have multiple, multiple hyperability paths and these will be late, like,
awaited positives and these will be weighted negatives and you can still do it.
And so, again, from a single point of supervision, you get all of this data-augmented information,
okay?
And of course, what we can do then is we can do the palindrome trick and now we get all of these
latent data-augmented positives without even providing any supervision, okay?
Because this is basically by construction, the target is the same as the quick, okay?
And so, where we can just set this up at training time and you can see that it's,
you know, if you pick a point in the query image, in the first image, you can see that
over time, it kind of gives you a little probability distribution of where that image
might have gone and you can say, well, maybe we can even do this by trying to get grouping
happening and find out a group which corresponds to a single object.
And to do that, we have a little extra thing that we can do which is we can do a dropout.
We can cut some of the engines away and force the correspondences to go through nearby paths
and that basically allows us to get a little bit more of this kind of grouping
happening where you basically kind of, you go through the paths that are also on the same
object, okay? And, you know, we basically violated it at runtime by essentially nearest
neighbor in the phi space and here are some examples. This is the kind of the state-of-the-art
label propagation results and this are the results of our methods and you can see that it's
it's basically behaving much better in terms of occlusion handling and just seems to do quite
a bit better. Here is again state-of-the-art self-supervised method and this is ours, right?
Even against supervised methods actually does pretty well even though it doesn't get any
sort of supervision. So here is kind of an example of how we do compared to some of the
self-supervised comparators and interestingly even for methods that are trained on image,
using image net representation we are actually doing better than that, okay? And here are some
examples of kind of propagating various things like skeletons or labels or things like that, okay?
So this is one way to use this contrastive learning as without data augmentation but of
course in my lab we also like to make pretty pictures and so I'll briefly show you another
way of using the same kind of an idea of kind of creating your own latent views for an image-to-image
translation setup, okay? And here the idea is of course image unfair translation. The classic
thing that we've been doing for a while we want to translate horses into zebras but we don't have
a correspondence between horses and zebras, okay? So we want to go from here to here but we don't
have a correspondence, okay? And of course the one powerful signal here is we can use a GAN loss
which basically says make this thing into a zebra by basically forcing it to look like other zebras
that I have seen, okay? Using a GAN loss but that GAN loss is not enough because it can make it look
like a zebra many ways, right? But we wanted to kind of be in correspondence and so this is where
we also want to have another constraint and in the past in works like cycle GAN we use the cycle
consistency constraint which says okay make it a zebra but also make it so that when you translate
it back you'll get back to the original horse and that kind of forces this to be the right answer,
not these, okay? But there is a problem with this cycle consistency constraint because the problem
is that it forces it to be a bijection and forces it to be one-to-one because yes it's not going to,
it's not going to go back to here but it's also going to constrain us to have to go back to this
particular horse whereas these other horses might have been just good enough, right? So this is where
kind of a bijection is not always desirable because sometimes these cycles are not a bijection.
So how could we kind of address this problem? And here is the approach that we came up with
which is we are going to have an image-to-image translation framework. We're starting with our
with our horse. We want to get a zebra so we have a GAN loss that says okay make this a zebra,
okay? And then what we're going to do in addition is we want to make this zebra to be
similar in structure to this horse but not in texture and what we're going to do way to do this
is we're going to enforce the structures to be the same by taking pairs of patches across the
input and the output and say that these two patches need to be close to each other in features play
the space and farther away than other patches from the horse image, okay? So you can see that again
we are getting this whole similarity learning story here where we are basically bringing these
two things to be closer and farther from the other patches of horse and again just unlike other
methods where the positives are somehow automatic created by data augmentation here the the the
positives are basically our input and our output so the output becomes our data augmentation, okay?
And now we are back into our contrastive learning land we just basically formulate this and we
basically say learn a representation such that these two guys are close so basically it kind
of ignores the texture and focuses on the structure and these things are fine, okay? And
and and of course what we do this we don't do this on just on the pixels we do it at different
levels of of representation at at basically different menu multi-scale patch representation
and we do this contrastive learning basically everywhere here in our decoder, okay? And of
course we also have our gap losses as usual, okay? One kind of interesting cute note for for those who
who are in this might appreciate this well we thought okay you know the positives that's
everything is clear with negatives we just take all the patches from the same image and then we
thought you know maybe it will be better if we take the negatives to be not just patches from
the same image but also just add other patches from other images other negatives, right? Even
should be even better even more negative data, right? And guess what? It turned out that this
did not work as well these external patches actually made performance worse than if we just
kept the eternal patch, okay? And this kind of goes back to some old work that that I have been
doing on textures this is where we also seen that that patches from the same image actually provide
much more information than if you start mixing them up with patches from other image and in fact
Michala Rani has this wonderful example story of doing super resolution using a single image
where she shows that you can do super resolution by learning from a single image you basically
take an image down sample it train a cnn to up sample that one image, right? So you basically
train a single image network and then just reuse that network for the original image.
So that works better than if you're if you're training a standard thing with many with a large
data set, okay? And basically we're seeing the same thing happening here that it's actually the
patches that are in the same image that have the same illumination the same you know camera
parameters the same setting they actually much more powerful information than if you just put
a whole data set. So this is kind of a cute little story and you can see here how using the
internal patches we get much better translation than if you we use external patches where you
can see that there is a lot of mode collapse happening, okay? So yeah, so basically that's
the story and this is our method and compared to compared to something like CycleGAN and other
methods as well and basically we can we see that we're basically getting as well performance as good
as CycleGAN in most cases but it's much faster and it's it's it's one sided you don't need to
train a two thing two-way thing and and it's basically a we think it's kind of a much better
a much better story and so here are some of the transformations that we have
and one cute thing that we can also do is we can basically apply this to instances so for
example let's say that we have a single image Claude Monet's painting we want to make it into
photograph and maybe what we have also is a single image instead of data set we have a
single photograph that is also let's say of water lilies, okay? Well we can basically use the same kind
of contrastive learning basically just between a single reference photo and our output, okay?
and have have have one have the same thing here for the for the positives and have a instead of
instead of again have basically just a single discriminator here and we can get something that
actually works quite a bit better than a lot of these kind of a stylization methods, okay?
So this is this is competitors and this is ours and I think that ours actually looks quite a bit
more natural and also better than CycleGAN. There are some other examples, okay? Let's see what
timing is. Well you know I don't think I have time to go over the second point of why you sell
supervision maybe I'll just give you a little bit of a hint of what I mean and then you can
you can read the paper if you're interested. So basically the idea is that it's a little bit weird
that we are in most of machine learning we are using a fixed training set. It's not very natural
biologically because biological agents they never see the same data twice, right? So you live your
life you never see the same thing twice. You see something first you you know you you deal with it
you hopefully learn from it if you you know if you didn't deal from it you're dead if you deal
with it you learn from it and then and then you you can recover some information from it but then
you never see that again you see maybe something similar, okay? So every new piece of data is
basically first in your test set and then in your training set, okay? And it seems like using a fixed
data set it kind of encourages memorization because you see the same exact thing over and over and over
again. In fact maybe this is actually another reason why data augmentation works because
data augmentation is kind of random you create a random thing every time so you kind of get away
a little bit from this memorization. So in fact this this might be kind of a subtle way in which
data augmentation helps that actually has nothing to do with the data augmentation just basically
randomization of your data, okay? But the point is that if you're using self supervised learning
like the whole point of having a fixed training set was because it was expensive to do all these
labels, you know? ImageNet, poor Fei Fei spent all of her startup money in Stanford labeling this
huge data set, right? So it kind of makes sense that it's it's fixed because it took so much money
to label it. But if you're using self supervised learning if you don't need the labels what's the
point of having a fixed data set? Why can't we just keep downloading images from whatever the
internet the TV whatever and just keep doing it all the time because we can generate our own
labels. Seems kind of natural and so this is where kind of I've been pushing on this idea of kind of
this online continual learning. So you can you can think of it in terms of of the standard
a train valve separation. So you know you have your training set and kind of the standard thing
in machine learning is you can separate it into a training set and a validation set, right?
And we know that if you just train on a training set and then use the validation set to tune your
high parameters you usually get better performance on the eventual test data than if you just train
on all the training set all at once. Even though you're kind of you you think that it's less data
that you're using for training but actually this is effect turns out to be more effective. Well we
can think of the same thing in a continual way. So we can think of it as you train on the data
that you have seen and then you're validating on the next data that comes along, okay? And then
once you do that you just incorporate it into your training center you can keep going and they
can keep going on forever. You don't ever need to stop, okay? And this I think is a kind of a very
powerful trick that is made that we can now do because we can use self-supervision to do this
kind of this evaluation, this testing, okay? And so this is the idea of test time training which is
our attempt to operationalize this on an infinite smoothly changing stream and the idea is to basically
use self-supervision to continuously adapt to new data, okay? And we did this already in the
case of reinforcement learning with our curiosity work and this new work is basically trying to do
it for images and this is the paper, test time training and it was in ICML 2000. Maybe
just give me, I'll give you one slide of intuition of what we're doing. Basically,
the idea is that we're, let's say we have a training set of object detection, right? And at
training time we have our standard thing, we have our image and we have our label so nothing new
here, we have input in, label out, we are training and then at the same time we also have a self-supervised
head that basically given your image it does some self-supervised task. In this case we are basically
our task here is rotation prediction. Given the rotated version of the image we want to predict
which rotation it is. It doesn't really matter, it could be any task at all, okay? So at training
time we do both of those tasks together but then at test time of course we don't have the labels
but we still have this task, okay? And so we can basically around this, we can evaluate this task
and if the result is not good, if it failed this self-supervised task we can do a little bit of
fine-tuning, a little bit of fine-tuning training for this other task but as we're doing the fine
tuning it's going to get changed representation in a way that will also impact the real task that
we care about and that allows us to do better as we are changing, as we're going for the dataset.
And so here is an example where you know given this image at test time basically the right label is
elephant but initially it basically thinks it's a dog but then as we do this fine-tuning on our
self-supervised task it figures out that it's actually less of a dog and more of an elephant
and gives us the right answer and that's basically the story of the paper, sorry I had to rush but
you can look at the paper online. And to conclude, why use self-supervision?
One reason that I like is that it allows us to get away from this top-down semantic categorization
and gets us more into this bottom-up association story and learn things from the bottom-up
but we must be careful that the supervision doesn't leak in through things like data augmentation
right and we need to be careful about this and second is that eventually self-supervision should
enable us to check the datasets, forget about all these fixed datasets and and learn things continuously
and it's you know we're still we're only starting on this direction I think it's very exciting direction
very exciting problem so I'm hoping people will get excited about it okay thank you very much
yeah awesome fantastic talk thanks a lot for all the amazing works
yeah self-supervision is cool and does anybody have some some some question on Zoom maybe let's
start with this we have a lot of questions on YouTube but I'm going to start to assume
I have a lot of questions too but if somebody wants to ask question on Zoom just turn on your video
and and just pick up probably
I can start with one with a maybe a higher level question first so I mean the challenge
in self-supervision is right you basically have visual data on you let's say correlate patches
with whatever contrastive loss or whatever whatever people do now um I mean what do you
think about if you're thinking about the 3d world right you have obviously a third dimension
is it a smart idea to do this actually all on on images and videos and not think about
I don't know like kind of project a 3d representation maybe first and then think about how to kind of
get similarities in some 3d space learn a 3d representation and then you know try to
channelize with the onscreen tasks later on right right no this is absolutely and and as you know
you know I've been I've been angling for for going into 3d you know since since since a long
time ago since our work with Derry Coyne on qualitative 3d I'm a I'm a big fan of 3d in
my heart and it's kind of a little bit sad that once you know once we went to neural networks
the kind of things dropped back to 2d plane for a while and now of course they're they're coming
back again um okay so there's there's two answers to this question one the final you know the the
ultimate answer is that 3d should emerge from our 2d of observation that the representation
should figure out 3d on its own okay uh just like it's done with humans right humans are only seeing
2d projections of the 3d world okay if you have stereo maybe you have a little bit of 3d but
you know I don't have stereo for example 10 percent of people in the world don't have stereo
and we are perfectly fine seeing 3d okay so we learn 3d from uh from from a series of 2d
representations uh and I think if we if we go from you know collections of images like ImageNet
to videos for example hopefully and I'm very hoping that like it will encourage 3d to automatically
emerge as as the you know inside of the representation okay so that's kind of a the the the the the
glorious answer at the end of the rainbow okay uh but of course this is this is very hard this is
kind of a a very tall order uh you know we are seeing a little bit of this happening we are seeing
a little bit of kind of a maybe two and a half two or two point one d kind of occlusion occlusion
reasoning you know figure ground reasoning uh a little bit of of that but but but it we're
they're definitely far away from that right and so the second direction is okay can we kind of help
it out a little bit how can we can we provide features that are more amenable to to to three
dimensional manipulation and there I think uh things like like like holo-gan or or pie-gan this
kind of directions are I think very exciting in in that it's kind of you you can inject some things
that you know are physically true like rotation for example and and uh and so I think I think in the
short in the short uh uh uh short term all of those things are I think going to be extremely
helpful in the long term I'm still kind of hoping that I can learn 3d from scratch okay but who knows
maybe it's too much to ask but I'm still kind of hoping that one day I will wake up in the morning
and boom my computer learned 3d but we'll see do it with two cameras right we have stereo that's I
mean that's the thing but I don't have stereo for example right like 10 percent of people don't
have stereo stereo is actually not as important as as as as we we we we think stereo is only really
important for like the you know half a meter in front of you it's like it's you know what
what is it that I cannot do that everybody else can do okay I cannot put you know thread through
the needle and I have trouble you know pouring wine right other than that I'm fine so really it
stereo is kind of over overemphasized I think it's really parallax is much more important
and parallax you can get from from video no good point I have another follow-up question
um so in the similar spirit right like one argument is you can do contrastive learning
and mostly it's about comparing things right you're saying one versus all classifiers like
how similar are these things I mean what about going back to the original things when people
using like auto encoders for pre-training and so on for like basically using generative tasks
let's say oh I train my favorite game how good of a representation can I learn from learning
the distribution basically right like how well like it's like this famous thing like you have to
be able to create in order to understand and where to see that competing or maybe going
going along the same line so what's your take channel is speaking on the lines there
um I mean yeah I mean yeah we have definitely been also using out encoders as well I think
with an outer encoder it's a little bit of a it's a little bit of a of a magic box like if you
know if you get really lucky your outer encoder is going to capture exactly the right things
and if you get unlikely it will capture all exactly the wrong things right it's it's kind of
it's a compression mechanism it somehow compresses your data and and it it really depends on what you
care about like sometimes it will compress the away the stuff that you care about or sometimes it
will retain the stuff that you care about and it's it's a little bit hard to control what it's going
to do so I think I think this kind of a similarity learning is it allows you to get a little bit
more control and a little bit of more of kind of a intuition about what is it what is it being
what is it that's being learned it's also kind of a has a very nice connection to kind of to
graphs and graph theory that kind of think you you think about it like you have different
entities and then you have kind of you can think of it like as a as like an as a like a network
right like a like a uh a social network for example where you you can think of different
people being connected in different ways and you can think about yeah we call them
senses of similarity so there's many different senses of similarity between two instances and
you know something like an out encoder is probably going to collapse them all together and here you
can actually separate them you can have a similarity in color similarity in texture maybe
similarity in 3d and they're all can be kind of exposed hopefully separately now that's interesting
I mean our experiences so we've done a lot of stuff on like shape completion in 3d so whenever we
had the ability to take stuff away and predict it then we got great features this was always amazing
in terms of using these features to help semantics and whenever we're trying to classify it to help
the completion this is this is always a total disaster it never worked we tried really hard
actually I mean I think that that's that's been our our experience as well but but I think have
you have you tried the latest uh contrastive learning because it's really it's to me the way
I think about contrastive learning is it's really just old school triplet loss you know uh Siamese
network learning except you're switching from from from from kind of a regression to to a
classification but it's a classification with like huge amounts of data and it's very very fine
grain classification so it's almost it's it's really not like your your grandma's classification
it's uh I mean we we did something like this for for uh for when we did colorization so we we first
we tried to do colorization with the regression and then we we we we got better results by doing
classification but the classification was across like you know 500 classes of different colors
in the in the in the color gamut right so it's it's a much more kind of narrow thing and that
seemed to work for us but yeah like if you have a few classes then then then it's very hard to
make it work but if you do something like either have lots of classes or do something like like
contrastive learning where it's basically just really kind of push it with data yet it seems to
to work for us now we've actually tried that so we had we had one one student project actually
in collaboration with fair so Chihu one of my students they've been working on basically
basically doing contrastive learning for pre-training 3d structures and in a similar
way than you would do it in 2d it does help but the completion still seems to work a bit better
it's very interesting I think so in general yeah I mean I think if if completions if if actually
predicting you know pixels or predicting voxels whatever um it it has it has more data it has
more information that and and we know that 3d world is actually much you know it's much more
informative right so and it's also I think much more um uni model so the one thing that was hard
for us for example when we did core colorization is what we're you know we're trying to colorize a
bird and the birds could be yellow or the bird could be green right and so you have multi model
you have two modes and if you're doing kind of a just sort of like a regression completion
what's it going to do it's going to do the average right so it's going to be neither here nor there
right but if you have a single mode it works really well so it might be that in 3d you're really
in a world that's much more uni model in which you don't have like you're not trying to have an
average between two different completions and they get something that doesn't look like either
but you're actually really focusing on a single mode so in that case maybe this is why you're
getting better results but if I suspect that if you had multi model like if you have a hole that's
big enough that you could have many different plausible completions happen to it I suspect that
that that then you're kind of uh the kind of the the prediction route might have more problems
no I fully agree with you no that one is definitely true and and but most of the case right you're
thinking about it's more like a dropout in a sense right so you're leaving out some stuff right and
then you're trying to figure out what's missing in this case I think we've experienced that it
works remarkably well if it's too large then you need probabilistic models again and stuff like
that then it's a lot more difficult um yeah I agree yeah I think I think it's kind of a
if it's a level of dropout and it should just work yeah I I I agree yeah yeah I think I think
if it works you should definitely use it absolutely um actually any other questions maybe
maybe somebody else can ask questions I don't want to dominate the discussion too much
hi yeah thanks for the talk um I have a similarly high level question so speaking along the lines of
like multi modality and and stuff like this it seems like you have a lot of inspiration in terms of
how to learn perception based on how people perform perception and it seems like people do have
naturally some kind of estimate of uncertainty multi modality and the ability to generate
also for like these video tracking kind of applications that you showed like multiple
hypotheses for where um the prediction should go and how far do you think you can get without this
explicitly modeled or do you think it needs to be explicitly modeled
good question I think I think I would go with I don't know so um yes humans are very good at
at modeling uncertainty but they I don't think they're doing it in the way that was the decisions
to it I don't think humans are actually probabilistic I think I think they might be doing it
almost like if you remember from a long time ago like all this particle filtering where you
can keep a whole bunch of hypotheses and then you kind of keep all of them going for a while and then
you kind of uh drop one like you know there's illusion of like young lady old woman visual
illusion where you know one day once time you see like an old lady one time you see a young woman
right and you never see both of them so it seems some there's some very interesting mechanism going
on but I think it's not it's not like a standard probabilistic mechanism and so yeah so I don't
know how to deal with it and in the in the vision in the in this video paper that I showed you know
we are really just keeping a whole bunch of hypotheses as we're going through the through
the video at training time at test time we don't and but whether that's the right thing to do or
not I don't know I think it's a very important question I don't I don't have an answer but
frankly I think that nobody else does either sounds good I think one other somewhat unrelated
thing I think there's a bit of a tension between people who think that we should be able to learn
everything from scratch like you mentioned in terms of being able to learn 3d and whether this
actually possible because it's unclear I guess how many how much supervision certainly for
like some rantic perception people get direct supervision and so yeah okay now we're back to
philosophy I think it must be possible because because it already happened right
supervised learning is something that happens in nature but it's it's very very rare like like
parents teaching their children things I know that a lot of modern parents they feel like it's
super super important but sorry you know a developmental psychologist disagree they say
that it doesn't really matter that much most of the things that a kid picks up they pick up
without supervision they keep pick up on their own and and you could think about it you know
from in the very beginning right in the beginning if you're you know as long as you believe in
evolution you must believe in in unsupervised or self-supervised learning because in the beginning
there was nothing there was no there was no teacher there was no supervision there was only
data right and kind of the organism and its environment were co-involving and learning
from each other and and and and and and develop so I think there is there is to me there is no
question that it should be possible in theory I think that the kind of the the the interesting
question is is it does it make sense to do in practice right and like you could also say well
why don't we just simulate evolution for for a gazillion years and then we'll get everything
right and that's of course not feasible with the current technology so I don't think that there is
that much or maybe there shouldn't be that much tension because I think I think there are people
like me who really want to try to learn things from first principles and I think this is very
interesting if if if anything from you know from the biological plausibility point of view
okay and there are people who just want to get stuff done and and and get to a good result too
fast and those people should definitely just use whatever works best at the time so I'm not sure
that it's either or I think both directions are are useful and I think we're learning from each
other I think those two direction directions are informing each other so for example for a very
long time self-supervised approaches worked worse than supervised approaches and so you know
if you know we could have all quit because oh my god you know our stuff doesn't work as well as
supervision but we persevered because we thought that you know there's something interesting that
that we could learn anyway and now what we are seeing is that for some tasks self-supervision
actually works better than supervised learning not for all not for many but for some there's
definitely some cases when it actually the the the learning from the data actually gives you
better results than learning from uh from from from from labels and so I think I think I think
you know let all the flowers bloom it I think both directions are useful and I think it's
it's great that people are are pushing in in in in both of them and I think we'll we'll get to a
better point eventually and we'll learn more so I'm actually optimistic on on on all fronts it's
yeah it's not a competition well it is a competition but it's not like it's not one is right another
is wrong I think both are right cool all right I think that's a that's a very good um I guess
ending of the live stream I think thanks a really lot for the amazing talk um we're a little bit
over I have to apologize to a lot of questions on YouTube we couldn't unfortunately go into all of
them um but um it was really great to have you and um I hope also for everybody who is with
with here right now next week we'll have another great lecture with rock help and yeah we'll see
so thanks a lot again for the for the great year I need great research
okay um
