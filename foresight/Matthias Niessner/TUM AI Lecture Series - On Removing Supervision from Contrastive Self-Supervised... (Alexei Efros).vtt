WEBVTT

00:00.000 --> 00:12.800
All right, welcome everybody today to my lecture.

00:12.800 --> 00:14.960
It's a real pleasure to have Alyosha Efros today.

00:14.960 --> 00:19.920
Alyosha is a professor at UC Berkeley, where he's part of the Berkeley Artificial Intelligence

00:19.920 --> 00:22.200
Research Lab there.

00:22.200 --> 00:26.240
His work is at the intersection of graphics and computer vision, and I'm sure pretty much

00:26.240 --> 00:29.240
everybody in the community has heard of him.

00:29.240 --> 00:33.400
He's a pioneer at the intersection in these fields.

00:33.400 --> 00:38.880
He has countless of exciting papers, starting from texture synthesis to conditional GANs

00:38.880 --> 00:41.720
like Pics to Pics and Cycle GAN.

00:41.720 --> 00:46.480
He's particularly known for his creativity and thought-provoking work, which is an inspiration

00:46.480 --> 00:48.280
to many young researchers.

00:48.280 --> 00:51.320
His students have also had really great success.

00:51.320 --> 00:54.960
You can see many of his students are now professors themselves.

00:54.960 --> 01:00.120
And I think it's also, yeah, it's fair to say that he has also a very great social

01:00.120 --> 01:03.840
engagement, in particular, contributing to the research community.

01:03.840 --> 01:08.280
If you haven't met him in person, I can only recommend reach out to him at the conferences

01:08.280 --> 01:09.280
once we have them again.

01:09.280 --> 01:11.040
It's really great to have him around.

01:11.040 --> 01:15.320
It's really great hanging around with him at one of the poster sessions and chat about

01:15.320 --> 01:16.760
some really exciting research.

01:16.760 --> 01:21.000
He's particularly known for his, yeah, really cool attitude, and it's really great to have

01:21.000 --> 01:22.800
him as part of the community.

01:22.800 --> 01:24.880
So it's a real pleasure to have you here.

01:24.880 --> 01:26.800
And I'm really looking forward to the talk.

01:26.800 --> 01:29.640
And you also promised some philosophical components.

01:29.640 --> 01:34.120
So I'm really excited what that's going to be.

01:34.120 --> 01:37.960
Thank you so much for such a gracious introduction.

01:37.960 --> 01:51.680
Yeah, I'm sad that we have to do this virtually because I would love to have hung out with

01:51.680 --> 01:56.400
you guys and gone for some wonderful Barbarian beers.

01:56.400 --> 02:01.880
But next time, yes, thank you very much for inviting me.

02:01.880 --> 02:11.040
And it's such a star spangled roster of speakers that you have there.

02:11.040 --> 02:16.520
I hope I will not disappoint.

02:16.520 --> 02:22.960
And so I'm going to talk about aspects of self-supervision.

02:22.960 --> 02:28.360
Self-supervision is something that my lab has been working for a number of years.

02:28.360 --> 02:33.520
And just to make sure everyone's on the same page, self-supervised learning is when we,

02:33.520 --> 02:41.280
this is my definition, you know, hopefully there is no one set definition, but my definition

02:41.280 --> 02:49.960
is that it's when we use the tools of supervised learning, but where the labels are the raw

02:49.960 --> 02:54.600
data instead of being human-provided.

02:54.600 --> 03:01.640
And so the question that often students ask is why use self-supervised learning?

03:01.640 --> 03:03.480
What's the point?

03:03.480 --> 03:09.120
And the classic answer, the common answer is because labels are expensive.

03:09.120 --> 03:16.400
Instead of having humans provide labels and annotating them using lots of hours of work

03:16.400 --> 03:25.960
or high cost, here we get used the data itself as our labels.

03:25.960 --> 03:30.320
So this is the common answer, but that's not really my main answer.

03:30.320 --> 03:34.480
This is, it's nice, but it's not the main reason for me.

03:34.480 --> 03:38.320
For me, I actually have a couple of answers.

03:38.320 --> 03:46.360
The first one answer is that self-supervised learning allows us to get away from the tyranny

03:46.360 --> 03:53.240
of this top-down semantic categorization that goes all the way to Plato and Socrates.

03:53.240 --> 03:57.960
And I'll tell you why I think this is a good thing.

03:57.960 --> 04:06.280
And the second reason is that self-supervised learning will hopefully enable us to move away

04:06.520 --> 04:16.160
from this idea of a fixed training set to a more continuous lifelong learning where you

04:16.160 --> 04:20.120
have data streaming in and then you learn on the go.

04:20.120 --> 04:24.840
You learn as you live, rather than having this kind of training set, testing set, split

04:24.840 --> 04:30.120
that is kind of the classic thing in machine learning.

04:30.120 --> 04:34.440
So I will start, and most of the talk is going to be about the first one, and then hopefully

04:34.440 --> 04:38.000
we'll do a little bit of number two, okay?

04:38.000 --> 04:42.520
So what's the problem with semantic categories?

04:42.520 --> 04:48.320
Well, let's look at, from the visual point of view, let's look at a couple of categories

04:48.320 --> 04:51.440
from a standard visual data set.

04:51.440 --> 04:56.360
So the first is what's called a chair, okay?

04:56.360 --> 05:06.280
But you can look at all the different chairs that you could have in the wild, and you realize

05:06.280 --> 05:16.360
that this is very hard to find what is in common between all of these chairs, right?

05:16.360 --> 05:21.360
Visually, actually, there is pretty much nothing in common because between something

05:21.360 --> 05:23.240
like this and something like this, right?

05:23.240 --> 05:27.760
This chair is really more of a functional category, right?

05:27.760 --> 05:29.520
And think about it this way.

05:29.520 --> 05:35.840
We can see them all being chairs because we have seen, you know, butts being squeezed

05:35.840 --> 05:38.300
into these places.

05:38.300 --> 05:44.240
But if you are a computer, if all you've seen in your life is ImageNet data set, for example,

05:44.240 --> 05:50.120
if you've never seen any videos, you've never seen any people sitting on anything, there

05:50.120 --> 05:55.920
is basically no way for you to realize that all of these are somehow related, right?

05:55.920 --> 06:00.600
So the relationship here is not visual.

06:00.600 --> 06:02.440
The relationship is functional.

06:02.440 --> 06:06.040
The relationship is that of affordances.

06:06.040 --> 06:12.200
And it's not really fair for a computer to try really, really hard to find a way to kind

06:12.200 --> 06:18.680
of somehow bring them all into some kind of a connection where maybe there isn't that

06:18.680 --> 06:22.400
much of a connection, okay?

06:22.400 --> 06:26.760
The second example I like is this one called City, okay?

06:26.760 --> 06:32.320
And here what I'm showing here is a picture of downtown Pittsburgh in a picture of the

06:32.320 --> 06:34.440
center of Paris, okay?

06:34.440 --> 06:41.960
And frankly, you know, the fact that both of these by some fluke of the English language

06:41.960 --> 06:48.600
are termed City, the same noun, this is just kind of a coincidence because there is really

06:48.600 --> 06:52.720
nothing visually in common between those two, right?

06:52.720 --> 06:56.320
And people can argue, well, wait a minute, you know, both contain buildings, but look

06:56.320 --> 06:57.320
at the buildings.

06:57.320 --> 07:01.760
The buildings are so different visually, there is nothing in common between those buildings,

07:01.760 --> 07:02.760
right?

07:02.760 --> 07:03.760
They're so visually different.

07:03.760 --> 07:07.280
And then you can say, well, but they both have windows, but again, look at the windows.

07:07.280 --> 07:09.240
The windows don't look anything the same.

07:09.240 --> 07:14.960
Look, there is really not a single pixel in common between these two things.

07:14.960 --> 07:20.920
And so again, we are forcing the computer to do something, to somehow try to generalize

07:20.920 --> 07:26.880
across these two things that are very, very different, that might not have anything in

07:26.880 --> 07:27.960
common.

07:27.960 --> 07:33.960
And so, in a way, what we're forcing the computer to do is basically to cheat, right?

07:33.960 --> 07:42.040
It's kind of like you haven't attended classes and then now it's your final exam and you're

07:42.040 --> 07:47.240
trying to see what, you know, you're trying to cram for the final exam.

07:47.240 --> 07:50.360
And so what the computer is going to do is like, anything that looks like this, it will

07:50.360 --> 07:53.920
be called the city, anything that looks like this will also be called the city.

07:53.920 --> 07:57.880
But you're not going to basically get the concept of a city.

07:57.880 --> 08:04.720
You're just going to basically just remember the, you know, the nearest neighbors, right?

08:04.720 --> 08:12.080
So basically with labels like these, we're, I worry that we're setting ourselves up for

08:12.080 --> 08:13.080
failure, right?

08:13.080 --> 08:22.200
We're setting our algorithms up for just memorizing examples and not really even having building

08:22.200 --> 08:24.120
connections between them, okay?

08:24.120 --> 08:28.560
Which is of course very bad if we want our models to generalize, right?

08:28.560 --> 08:32.720
If we just wanted to, you know, train an image net and then test an image net, that's fine.

08:32.720 --> 08:37.200
But if we wanted to train an image net and test on, you know, the real world out there,

08:37.200 --> 08:38.200
then that's not fine.

08:38.200 --> 08:42.840
We need to somehow induce generalization, okay?

08:42.840 --> 08:52.440
And so this is where my promised bit of philosophy comes in, which is that I argue, I've been

08:52.440 --> 09:01.440
arguing functionally for many years now, that we should step away from the stop down categorization

09:01.440 --> 09:08.000
paradigm and try to think more of it as a bottom up association.

09:08.000 --> 09:15.600
And there is some movement towards that in, especially in the 20th century.

09:15.600 --> 09:20.520
The philosophers have really kind of pushed away plateaus and Socrates notions of these

09:20.520 --> 09:28.840
rigid categories and really started to think about categorization in a much more bottom

09:28.840 --> 09:30.240
up way.

09:30.240 --> 09:38.320
So Plato, of course, he argued that categories were a list of, you know, there were abstract

09:38.320 --> 09:41.180
definitions with a list of shared properties.

09:41.180 --> 09:46.400
And then in the mid 20th century, the philosopher Wittgenstein came out said, no, no, no, no,

09:46.400 --> 09:48.560
this is that people don't do this.

09:48.560 --> 09:54.280
In fact, people are much more fluid about categories.

09:54.280 --> 09:59.160
And, you know, for example, you know, if you ask people, you know, our curtains furniture,

09:59.200 --> 10:03.560
olives, fruit, different people will give you different answers.

10:03.560 --> 10:07.760
In fact, the same person might give you different answers different times of different times

10:07.760 --> 10:08.760
you ask them.

10:08.760 --> 10:20.280
Wittgenstein's classic puzzle was to ask people to name all what is in common across all games.

10:20.280 --> 10:25.480
What are the common properties shared by all games and not shared by non games?

10:25.480 --> 10:27.600
And that you cannot do it.

10:27.600 --> 10:32.840
It's just impossible that there's such a variety of games out there that you cannot think of

10:32.840 --> 10:35.640
any single thing that defines a game.

10:35.640 --> 10:38.680
It's not a definitional thing.

10:38.680 --> 10:43.000
It's much more kind of data driven, much more bottom up kind of, well, it's a game like

10:43.000 --> 10:44.000
like football.

10:44.000 --> 10:47.000
It's a game like chess, right?

10:47.000 --> 10:54.520
And so in the second half of the 20th century, psychologists in particular, Eleanor Rush

10:54.520 --> 11:03.120
have tried to kind of update the notion of categorization and tied to think about categories

11:03.120 --> 11:09.600
forming from the bottom up and her famous prototype theory of categorization has really

11:09.600 --> 11:16.560
started this trend where the idea was that you basically cluster, you do bottom up clustering

11:16.560 --> 11:22.480
of similar instances into these prototypes and the prototypes then get clustered again

11:22.480 --> 11:28.240
and then you have this kind of a bottom up hierarchy where the categories emerge directly

11:28.240 --> 11:32.800
from the data rather than being kind of this top down, okay?

11:32.800 --> 11:39.880
And later folks have, psychologists have gone even further and argued for what they call

11:39.880 --> 11:48.360
the exemplar based theory of categorization where you basically, you don't really have

11:48.360 --> 11:57.640
categories when you kind, you basically just store instances, store exemplars of everything

11:57.640 --> 12:03.080
that you see and then you basically learn associations between those examples and the

12:03.080 --> 12:07.120
things that are closer together, essentially can you can think of it as like this kind

12:07.120 --> 12:13.360
of a soft clustering of your exemplars into chunks and to groups and those groups then

12:13.360 --> 12:16.400
emerge to be categories.

12:17.280 --> 12:24.120
My favorite slogan is provided by a neuroscientist Moshe Barker who says, ask not what is it,

12:24.120 --> 12:30.000
ask what is it like to this kind of a, this idea that association bottom up association

12:30.000 --> 12:35.080
trumps this top down labeling, okay?

12:35.080 --> 12:43.400
And so this work has been very inspiring to me and my group over the years and we have

12:43.400 --> 12:50.240
been kind of chugging away at in this direction for a number of years and maybe one of the

12:50.240 --> 12:55.720
earlier works that we did was with my former student Tamash Milosevich where we basically

12:55.720 --> 13:03.720
try to kind of instantiate this exemplar based way of thinking about categorization

13:03.720 --> 13:12.680
and here the example, the idea is that we basically try to find per-exampler distances

13:12.680 --> 13:14.440
given a set of data.

13:14.440 --> 13:22.480
So you have a set of labeled data, you know, cars, people, pedestrians, trees, etc.

13:22.480 --> 13:27.440
And here instead of trying to separate all cars from all pedestrians, for example, we

13:27.440 --> 13:34.560
wanted to basically learn a way to group every single instance of say car.

13:34.560 --> 13:43.200
So for this particular focal example of a car, we wanted to find what other things are

13:43.200 --> 13:44.560
close to it, right?

13:44.560 --> 13:50.560
And so those other things should be also labeled car, but there shouldn't be all cars because,

13:50.560 --> 13:53.480
you know, this car, for example, looks nothing like this car.

13:53.480 --> 13:57.240
So should they really be in the same cluster in the same category?

13:57.240 --> 13:58.240
Maybe not.

13:58.240 --> 14:04.320
And so the idea that Tamash came up with was to basically kind of treat this as a kind

14:04.320 --> 14:09.920
of a classification problem where you basically learn a decision boundary between things that

14:09.920 --> 14:16.680
are close to your focal example and things that are far, but here we actually have instead

14:16.680 --> 14:22.520
of two classes, the ones that are inside of a category and one's out, we have three classes.

14:22.520 --> 14:25.840
There are the things that are close and these are kind of these kind of cars.

14:25.840 --> 14:30.000
There are things that are not cars and they're on the other side.

14:30.000 --> 14:32.960
And then there is a third class which is don't care.

14:32.960 --> 14:39.840
And these are basically other cars that might not actually be that close to the focal car.

14:39.840 --> 14:48.680
And then you basically optimize this, optimize this decision boundary given some set of constraints.

14:48.680 --> 14:55.400
And as a result, what you get is you get something where you learn these distances that are, that

14:55.400 --> 15:06.040
produce much more visually meaningful relationships rather than if you were just doing a standard

15:06.040 --> 15:10.440
set of distances without this, okay?

15:10.440 --> 15:15.720
And so this was kind of a, we were very excited about this, but still we're here, we're still

15:15.720 --> 15:20.920
using the label car, the labels are still being used in this computation.

15:20.920 --> 15:24.400
And we really wanted to go away from labels entirely.

15:24.400 --> 15:29.360
And this is where Tamash's next paper came in.

15:29.360 --> 15:36.560
And this work, we call it exemplar SVM, was basically kind of pushing this farther and

15:36.560 --> 15:45.960
really thinking about it in terms of classifiers and basically switching from the standard

15:45.960 --> 15:52.440
classifier where you have class A and class B instead to take every single instance, every

15:52.440 --> 16:01.120
single data point and train a separate classifier for that one instance against everyone else,

16:01.120 --> 16:04.000
whether it's your own class or a different class.

16:04.000 --> 16:06.760
So it's one against all classifier.

16:06.760 --> 16:10.040
So this is kind of an interesting way to think about it because it's really basically you're

16:10.040 --> 16:16.560
defining yourself, not by who is in your category, who is in your class, but you're

16:16.560 --> 16:20.560
defining yourself by what you are not, right?

16:20.560 --> 16:27.360
So what makes you different from everyone else in your data, okay?

16:27.360 --> 16:36.960
And then the cool result was that we were able to do one classifier for every instance

16:36.960 --> 16:38.840
and then assemble them together.

16:38.840 --> 16:48.160
And the result was that this assemble actually worked no worse in many cases than your standard

16:48.160 --> 16:52.400
two-class classifier like an SVM, okay?

16:52.400 --> 16:58.560
And this was a little bit of a, it was kind of like a bit of a trolling paper, which basically

16:58.560 --> 17:04.400
kind of tried to push the community to say, look, maybe you're not getting as much juice

17:04.400 --> 17:10.560
as you think you are from basically trying to group all those things into one class because

17:10.560 --> 17:17.960
it seems like if you don't do it, you get basically as much of performance as not, okay?

17:17.960 --> 17:22.920
And so we also use the same idea for retrieval.

17:22.920 --> 17:31.360
So basically the idea is you take, at runtime, you take your retrieval query image and then

17:31.360 --> 17:38.280
you have a big data set and you're basically training at runtime an SVM classifier to separate

17:38.280 --> 17:42.120
your query from everything else, okay?

17:42.120 --> 17:51.360
And then you order all of your data based on that, on the coefficients of that decision

17:51.360 --> 17:52.360
boundary, okay?

17:52.360 --> 17:59.480
You basically find who are the closest things, who are your support vectors inside of your

17:59.480 --> 18:05.360
data set and those tend to be the retrieval examples, the kind of the closest ones on

18:05.360 --> 18:11.920
the other side will be the retrieval examples and that also worked surprisingly well.

18:11.920 --> 18:16.560
And so we were very excited about this and we were very hopeful and then of course deep

18:16.560 --> 18:25.280
learning revolution hit and all of this became irrelevant because much better classifiers

18:25.280 --> 18:27.640
came on the scene.

18:27.640 --> 18:37.880
We tried to update it for the deep learning age and we didn't really succeed but Alexey

18:37.880 --> 18:41.280
DeSavitsky and colleagues did, okay?

18:41.280 --> 18:49.040
So one of the kind of very early influential papers was called exemplar CNN, which basically

18:49.040 --> 18:59.840
adopted the same idea of one against all classification on using neural networks, okay?

18:59.840 --> 19:07.360
And the main difference that we didn't really think of was that whereas we used a single

19:07.360 --> 19:16.160
exemplar, one image against everything else, DeSavitsky and colleagues they used what's

19:16.160 --> 19:18.360
called data augmentation.

19:18.360 --> 19:24.000
So they basically, they took one example and then they created a whole bunch of similar

19:24.000 --> 19:33.200
examples by basically applying various different transformations to it, you know, changing

19:33.200 --> 19:40.680
lighting, changing contrast, changing shapes, you know, various geometric transformation,

19:40.680 --> 19:41.680
etc.

19:41.840 --> 19:48.800
In the end, even though all of these things came from a single example, they all were

19:48.800 --> 19:53.880
a little bit different and so this became the positive class and then everything else

19:53.880 --> 19:56.880
became the negative class and that worked really, really well, okay?

19:56.880 --> 20:05.280
And this work really was an inspiration for a lot of the current contrast of self-supervised

20:05.280 --> 20:09.480
learning that we are familiar with right now.

20:09.480 --> 20:15.600
So we thought we will be very pure and just use a single image, but this of course worked

20:15.600 --> 20:16.840
much, much better.

20:16.840 --> 20:21.840
And of course, now in kind of modern day, the self-supervised learning representations

20:21.840 --> 20:30.920
that seem to work the best, they're all based on this idea of similarity learning, of instead

20:30.920 --> 20:38.960
of learning which class you are, which category you are, the idea is to learn instances that

20:38.960 --> 20:41.760
are either close or far from each other.

20:41.760 --> 20:49.360
So learning the distances between the instances in your training data, okay?

20:49.360 --> 20:55.440
So things like metric learning, SiameseNet and the new contrastive learning are all based

20:55.440 --> 20:56.920
on that same principle.

20:56.920 --> 21:02.560
So you basically, you have some sort of an embedding space and your goal is to say, okay,

21:02.560 --> 21:08.240
for a given positive like this particular instance of a dog here, you create a bunch

21:08.240 --> 21:12.120
of different positive example by data augmentation.

21:12.120 --> 21:18.240
And then you basically try to push these ones, all of them to be close to each other in this

21:18.240 --> 21:28.160
embedding space and far away from other things which are dogs or other cats, et cetera.

21:28.160 --> 21:34.920
And this learning of the similarity is really what a lot of the contemporary self-supervised

21:34.920 --> 21:38.920
learning methods are doing, okay?

21:38.920 --> 21:46.840
And so, you know, the reason why this, maybe like a year ago, this area really took off,

21:46.840 --> 21:52.120
one of the reasons is, of course, you know, the improvements in the representation learning.

21:52.120 --> 21:58.040
The contrastive formulation is actually just works much better as shown by papers like

21:58.040 --> 22:00.480
Simplier and stuff.

22:00.480 --> 22:06.640
But another reason I think that's maybe being a little bit underappreciated is that we are

22:06.640 --> 22:11.520
just much better at doing this data augmentation.

22:11.520 --> 22:18.400
So we have learned to do data augmentation in a better way than the Seitzky and his exemplar

22:18.400 --> 22:19.560
Sienna, okay?

22:19.560 --> 22:25.280
For example, now cropping is a very standard trick for data augmentation, which wasn't a

22:25.280 --> 22:29.560
standard trick before, and that gives us a lot of boost.

22:29.560 --> 22:35.280
So again, what data augmentation is, you get yourself an input image, a single instance,

22:35.280 --> 22:40.160
and then, you know, you just randomly create a whole bunch of different versions of that

22:40.160 --> 22:48.240
image by applying a whole bunch of different parameter transformation, whole transformation,

22:48.240 --> 22:51.880
cropping, flipping, blurring, et cetera, et cetera, et cetera, okay?

22:51.880 --> 22:59.720
And then once you do that, then you set up your kind of a distance function.

22:59.720 --> 23:07.360
So you basically say that I want these two images that all came from the same image really,

23:07.360 --> 23:12.800
I want those two images to be similar in our embedding space.

23:12.800 --> 23:19.080
So I'm going to try to bring them close together and farther away from the other images in

23:19.080 --> 23:20.080
my data set.

23:20.200 --> 23:25.160
That's really the whole story of contrastive learning, okay?

23:25.160 --> 23:33.440
Now the thing is that the choice of data augmentation itself turns out to be very, very critical.

23:33.440 --> 23:41.760
And in fact, I want to argue that this data augmentation is itself a little bit of supervised

23:41.760 --> 23:49.960
learning, because the way you choose your augmentation can make a huge difference in

23:49.960 --> 23:52.800
your final performance, okay?

23:52.800 --> 23:59.600
So here is an example from our recent paper in iClear 21, where you can think of, let's

23:59.600 --> 24:05.320
say that you have different types of data augmentation, like color augmentation, maybe

24:05.320 --> 24:08.760
rotation, and maybe texture, okay?

24:08.760 --> 24:16.680
So now we can look at different tasks, for example, if we want something like ImageNet,

24:16.680 --> 24:24.200
course-level categorization, then data augmentation with color makes a lot of sense, with texture

24:24.200 --> 24:28.560
also makes a lot of sense, but rotation is actually going to hurt you, because an upside-down

24:28.560 --> 24:32.600
elephant is not going to be recognized as an elephant, okay?

24:32.600 --> 24:38.720
Whereas if your task is, for example, fine-grain recognition, well, then it gets even more

24:38.720 --> 24:43.040
complicated, because if you're fine-grain the different species of birds, then actually

24:43.040 --> 24:47.240
you don't want any of those data augmentations, because they're all meaningful, like changing

24:47.240 --> 24:53.320
texture may change the species, changing the color definitely will change the species.

24:53.320 --> 24:59.200
Whereas maybe if you're classifying different types of flowers, then rotation is fine, because

24:59.200 --> 25:06.760
rotation augmentation, you know, because flowers are usually rotationally symmetric, okay?

25:06.760 --> 25:13.360
And so you can see that it really becomes very, very task-dependent.

25:13.360 --> 25:16.800
And another example is the cropping and image classification.

25:16.800 --> 25:22.760
So cropping for something like ImageNet classification makes a lot of sense, because you have one

25:22.760 --> 25:26.160
big object in the center of the image.

25:26.160 --> 25:31.280
But the same cropping for object detection actually doesn't make that much sense, because

25:31.280 --> 25:37.280
you crop it and you might lose, you know, where your object is, okay?

25:37.280 --> 25:43.120
And so this is something that we started to worry about, because we feel like a lot of

25:43.120 --> 25:51.880
the advances in the modern self-supervised learning might actually be due to us being

25:51.880 --> 25:58.200
very good at overfitting the right kind of data augmentation for a particular problem

25:58.200 --> 26:02.080
rather than the actual methods themselves, okay?

26:02.080 --> 26:10.120
So what we wanted to do is to try to do contrastive self-supervised learning without data augmentation,

26:10.120 --> 26:18.240
to really try to get it to, to figure it out on its own without this kind of help, okay?

26:18.240 --> 26:22.880
And the way we wanted to do this is to make these, these augmentations, these what they're

26:22.880 --> 26:28.840
called views, to make them latent, to make the computer come up with its own data augmentation

26:28.840 --> 26:31.520
in this, in a sense, okay?

26:31.520 --> 26:37.360
And of course, the big question here is, this is all great, but where do you get the supervisory

26:37.360 --> 26:38.360
signal, right?

26:38.360 --> 26:39.360
There is no free light.

26:39.360 --> 26:43.280
You need to get some sort of supervisory signal from somewhere in your data.

26:43.280 --> 26:45.760
So where is it going to come from?

26:45.760 --> 26:49.840
And here we're going to, I'm going to talk about a couple of papers where we answer

26:49.840 --> 26:51.520
this question differently.

26:51.520 --> 26:59.640
The first paper, the answer we have is that we want to use time as our self-supervisor

26:59.640 --> 27:01.680
signal, okay?

27:01.680 --> 27:07.160
And here I have a wonderful quote from one of my favorite writers, Jorge Luis Borges,

27:07.160 --> 27:14.760
in his short story about fumes, who is this kind of a man on the spectrum.

27:14.760 --> 27:20.840
He writes, it irritated him that the dog at 3.14 in the afternoon seen in profile should

27:20.840 --> 27:26.640
be indicated by the same down as dog at 3.15 seen in front of it, okay?

27:26.640 --> 27:37.040
So basically what he's talking about is that two different instances of time makes most

27:37.040 --> 27:46.000
of us assume that there is some continuity of what we are perceiving, that the dog here

27:46.000 --> 27:50.280
and the dog here, it's almost certainly the same dog.

27:50.280 --> 27:54.800
But nothing happened to this dog while it was jumping in the water, right?

27:54.800 --> 28:02.280
But fumes, of course, couldn't figure this out and neither can our computers, right?

28:02.280 --> 28:11.560
And so the idea is that this temporal correspondence, basically time as a way to align things together,

28:11.560 --> 28:18.120
to bring things into correspondence, is a very powerful supervisory signal that we should

28:18.120 --> 28:19.960
be using, okay?

28:19.960 --> 28:26.600
And we have evidence that biological algorithms use it very strongly.

28:26.600 --> 28:41.440
There is plenty of psychology data for human infants that shows that temporal cues are

28:41.440 --> 28:44.160
very important to learning vision.

28:44.160 --> 28:50.760
And there is this wonderful line of work by Wood who basically did this kind of this

28:50.760 --> 28:53.820
experiments with newly born chicks.

28:53.820 --> 28:59.640
So basically what he says he has is this kind of VR cave for chickens, for little chicks.

28:59.640 --> 29:04.520
So you put an egg and then the chicken is born and the chicken is born in this VR cave

29:04.520 --> 29:07.920
where he's basically projected things on all sides.

29:07.920 --> 29:14.720
So everything that the chick knows from birth is being controlled by the researcher, okay?

29:14.720 --> 29:21.360
And what he showed that some of the chicks were shown videos that were not temporally

29:21.360 --> 29:25.200
coherent, that basically broke this temporal continuity.

29:25.200 --> 29:32.880
As you can see, it was not kind of physically correct and he compared them to chicks that

29:32.880 --> 29:37.040
were shown normal, normal continuous things.

29:37.040 --> 29:45.800
And the chicks who saw these continuous patterns, they were not able to function in the world

29:45.800 --> 29:46.800
as well.

29:46.800 --> 29:51.920
They lacked some of the visual perception skills.

29:51.920 --> 29:57.920
So that showed that this is extremely important, that temporal continuity is extremely important.

29:57.920 --> 30:06.240
And so what we want to do in this work is to use video as data augmentation, as a way

30:06.240 --> 30:13.680
to create these data augmented views ourselves, okay?

30:13.680 --> 30:21.880
And basically the main thing, of course, is that this can provide correspondences across

30:21.880 --> 30:30.200
different instances and allows the computer to learn how something looks across time change,

30:30.200 --> 30:31.360
okay?

30:31.360 --> 30:39.160
But it can give us a bit more because we can also think about contextual relationships

30:39.160 --> 30:44.600
and notice things that are moving in the same way, what Bernheimer called common fate.

30:44.600 --> 30:52.920
We also use that as a way to group little points, little trajectories into groups and

30:52.920 --> 30:57.280
maybe get to the notion of objects from the notion of points, okay?

30:57.280 --> 31:01.680
Again something that you can use temporal information for, okay?

31:01.680 --> 31:04.720
And so this is basically the story.

31:04.720 --> 31:10.200
And then the question is how do we harness this information without any sort of supervisory

31:10.200 --> 31:11.200
signal, okay?

31:11.200 --> 31:19.120
And in the past, people have used things like slow feature learning where they basically

31:19.120 --> 31:25.840
kind of looked at connecting nearby frames together, basically look at nearby frames

31:25.840 --> 31:30.520
as the positives and far away frames as the negatives, but you just collapse the entire

31:30.520 --> 31:36.160
frame and so that's not really something that can give you these point tracks, it's much

31:36.160 --> 31:38.160
more coarse signal.

31:38.600 --> 31:45.000
Alternatively people use things like optical flow or tracking to create correspondences

31:45.000 --> 31:50.120
using some off the shelf methods and then use learning to connect things that are supposed

31:50.120 --> 31:51.280
to be in correspondence.

31:51.280 --> 31:56.800
But here you're using two different methods and so what we wanted to do is do something

31:56.800 --> 32:01.360
like this but kind of in one go, in one pack.

32:01.360 --> 32:10.640
And this is our paper that tries to do this, this was published in Europe's past year

32:10.640 --> 32:13.720
and here is the idea, okay?

32:13.720 --> 32:21.080
And so for a warm up, let's consider the case where you actually do have labels.

32:21.080 --> 32:26.400
Let's say that somebody went ahead and labeled that this patch corresponds to this patch,

32:26.400 --> 32:27.400
okay?

32:27.400 --> 32:35.280
And your goal is to basically learn a representation that brings things that are the same into

32:35.280 --> 32:39.520
correspondence and away from things that are different, right?

32:39.520 --> 32:43.800
So in this case of course it's very simple, you just say, okay, these two things are my

32:43.800 --> 32:48.480
two positives, I want them to be close together and all the other patches are my negatives,

32:48.480 --> 32:52.760
I want them to be pushed far away and then you have your standard self-supervised learning,

32:52.760 --> 32:55.800
contrastive learning problem and off you go, right?

32:55.800 --> 32:57.080
Nothing very exciting.

32:57.080 --> 33:03.600
So things get a little bit more exciting if you have maybe another frame in between, okay?

33:03.600 --> 33:06.800
Because now you have, these two guys are your two positives.

33:06.800 --> 33:16.600
We know this by labeling but also because this is a video, we know that from here somehow

33:16.600 --> 33:25.520
it needed to go to be in this final place and so there must have been a path, the most

33:26.440 --> 33:31.040
likely path from this guy to go from here to here and the most likely path is going

33:31.040 --> 33:38.040
to go through this patch so it's reasonable to assume that this patch should also be in

33:38.040 --> 33:42.840
our positive category, it should be in the same category as this guy and this guy.

33:42.840 --> 33:47.920
So now these triplets should be the positives and everything else should be the negatives.

33:47.920 --> 33:53.240
So now you can think of a little bit of kind of automatic data documentation that this guy

33:53.320 --> 34:00.680
just by virtue of being tracked in the video becomes a data augmented positive for our kids,

34:00.680 --> 34:02.040
okay?

34:02.040 --> 34:07.000
So this is okay but this is still requiring us to have this supervision.

34:07.000 --> 34:08.360
How could we get rid of supervision?

34:08.360 --> 34:15.400
Well, we are going to use our old trick which we've used before called cycle consistency

34:15.400 --> 34:20.040
and what we're going to do is we're going to make this video into a palindrome.

34:20.040 --> 34:25.160
Palindrome if you remember in language is a word that you read it forward and backwards

34:25.160 --> 34:26.520
and it reads the same, right?

34:26.520 --> 34:29.000
So how can we make a palindrome out of a video?

34:29.000 --> 34:34.760
Well, what we can do is we can take this video and flip it around and put it back

34:36.440 --> 34:38.280
in the reverse order, okay?

34:38.280 --> 34:42.680
So now what we have is we have something like frame one, frame two, frame three

34:42.680 --> 34:46.040
and now we're going back to frame two and then frame one, okay?

34:46.040 --> 34:50.680
So now we have this new video that's a palindrome and look what's happening.

34:50.680 --> 34:59.800
Now the final place, the destination is now exactly the same as the origin by construction.

35:00.520 --> 35:03.560
So now we don't need supervision anymore.

35:03.560 --> 35:04.920
We got rid of supervision.

35:04.920 --> 35:08.760
All we need to do is to get from the blue guy to the green guy

35:09.080 --> 35:17.720
and the way we do it is we're basically trying to do a track through this video

35:18.280 --> 35:23.800
and everything that's on this track should be in our positive category

35:23.800 --> 35:25.960
and everything else should be in the next, okay?

35:25.960 --> 35:32.600
And so now you can see the setup where we basically are getting something out of nothing.

35:32.600 --> 35:40.360
We're getting some supervisory signal just from the mere video information, okay?

35:40.360 --> 35:43.640
So basically the story is that we are going to take a video.

35:43.640 --> 35:50.280
We're going to make it a palindrome by going from t to t plus k and then minus to back to t.

35:50.280 --> 35:53.320
We're going to make it into a graph.

35:53.320 --> 35:56.760
We're going to turn the video into a graph, okay?

35:57.480 --> 36:06.440
And then we're going to walk along this graph until we get to the end, okay?

36:06.440 --> 36:10.040
And we're going to do basically a random walk on this graph

36:10.840 --> 36:13.560
and then we're going to steer that random walk

36:14.200 --> 36:17.960
such that if you start from this blue point right here,

36:18.600 --> 36:22.520
we want to steer it to get us to this green point right here, okay?

36:23.400 --> 36:26.520
And this is going to be our only supervision.

36:26.520 --> 36:29.880
The supervision is going to be at the last frame where we're going to say that the green,

36:30.520 --> 36:34.280
the positive is going to be anything that lands on the green dot

36:34.280 --> 36:40.120
and negative is going to be anything that lands anywhere else, anywhere on this red dot, okay?

36:40.120 --> 36:43.000
And that's basically going to be the signal that we're going to use, okay?

36:45.080 --> 36:51.640
So, and also notice that we don't have to have a single path through this graph.

36:51.640 --> 36:59.480
We kind of naturally, we can incorporate probabilistic information

36:59.480 --> 37:02.680
by tracing many paths through this graph, okay?

37:04.040 --> 37:07.720
So, you know, how do you turn the video into a graph?

37:07.720 --> 37:09.240
Well, it's kind of a standard thing.

37:09.240 --> 37:12.600
You know, create nodes and then, you know, you're basically,

37:12.600 --> 37:16.280
your nodes are some representation and some using some encoder, phi.

37:16.280 --> 37:19.240
And really, this phi is really the only thing that you're learning.

37:19.240 --> 37:24.600
What you're learning is you're learning the representation of each patch

37:24.600 --> 37:25.720
in your feature space.

37:25.720 --> 37:31.720
And you're basically trying to figure out how to arrange those features

37:32.360 --> 37:35.720
in your representation, who's going to be close, who's going to be far, okay?

37:36.280 --> 37:41.080
And so, now your video is going to be a graph.

37:41.080 --> 37:47.800
And then from frame T to frame T plus one, you're just going to have a transition matrix

37:47.800 --> 37:53.720
that's just going to say, you know, where did all the points from T go in T plus one?

37:53.720 --> 37:58.120
And then this is just basically like a dot product in the feature space.

37:58.120 --> 38:04.280
So, the closest things are going to be to get the higher dot product, okay?

38:04.280 --> 38:06.600
And then how are we going to do it around the work?

38:06.600 --> 38:11.240
Well, just going to compose all of these transition matrices A,

38:11.240 --> 38:15.000
just like, you know, standard Markov chain, you know, you're just multiplying

38:15.000 --> 38:22.280
all of those transition matrices and you get your full work on this graph, right?

38:22.280 --> 38:28.520
So, now kind of the nice thing is that the task of learning this representation phi

38:28.520 --> 38:33.640
is essentially the same as fitting these transition probabilities, okay?

38:33.640 --> 38:38.440
You find the right transition probabilities and it gives you your representation phi, okay?

38:39.400 --> 38:45.640
So, again, in kind of a, if we do have the target somewhere, if we do have the supervision,

38:45.640 --> 38:49.640
then this is just a standard contrast of learning problem.

38:49.640 --> 38:57.480
You basically, you want to find a representation where this query goes directly to the target.

38:57.480 --> 38:59.640
It doesn't go to the red ones, right?

38:59.640 --> 39:02.840
And this is basically just, this is your positive, this is your negative,

39:02.840 --> 39:05.800
this is your standard kind of static learning problem.

39:05.800 --> 39:11.720
If you have multiple frames in between, then in a sense, you have some latent views

39:11.720 --> 39:13.880
that you can also use, right?

39:13.880 --> 39:20.280
So, now we have, these are all the positives and these are the negatives if you,

39:20.280 --> 39:21.880
if it's an obvious path.

39:21.880 --> 39:26.840
But if you have multiple, multiple hyperability paths and these will be late, like,

39:28.520 --> 39:32.680
awaited positives and these will be weighted negatives and you can still do it.

39:32.680 --> 39:38.040
And so, again, from a single point of supervision, you get all of this data-augmented information,

39:38.040 --> 39:38.280
okay?

39:40.120 --> 39:47.720
And of course, what we can do then is we can do the palindrome trick and now we get all of these

39:47.720 --> 39:54.600
latent data-augmented positives without even providing any supervision, okay?

39:54.600 --> 39:59.640
Because this is basically by construction, the target is the same as the quick, okay?

40:00.520 --> 40:06.280
And so, where we can just set this up at training time and you can see that it's,

40:07.080 --> 40:12.760
you know, if you pick a point in the query image, in the first image, you can see that

40:12.760 --> 40:17.080
over time, it kind of gives you a little probability distribution of where that image

40:17.080 --> 40:23.560
might have gone and you can say, well, maybe we can even do this by trying to get grouping

40:23.560 --> 40:28.280
happening and find out a group which corresponds to a single object.

40:28.840 --> 40:33.640
And to do that, we have a little extra thing that we can do which is we can do a dropout.

40:33.640 --> 40:41.880
We can cut some of the engines away and force the correspondences to go through nearby paths

40:41.880 --> 40:46.120
and that basically allows us to get a little bit more of this kind of grouping

40:47.320 --> 40:51.880
happening where you basically kind of, you go through the paths that are also on the same

40:51.880 --> 40:59.400
object, okay? And, you know, we basically violated it at runtime by essentially nearest

40:59.400 --> 41:06.040
neighbor in the phi space and here are some examples. This is the kind of the state-of-the-art

41:07.240 --> 41:12.520
label propagation results and this are the results of our methods and you can see that it's

41:12.520 --> 41:19.080
it's basically behaving much better in terms of occlusion handling and just seems to do quite

41:19.080 --> 41:24.920
a bit better. Here is again state-of-the-art self-supervised method and this is ours, right?

41:26.200 --> 41:31.480
Even against supervised methods actually does pretty well even though it doesn't get any

41:31.480 --> 41:40.280
sort of supervision. So here is kind of an example of how we do compared to some of the

41:40.280 --> 41:45.160
self-supervised comparators and interestingly even for methods that are trained on image,

41:45.320 --> 41:51.800
using image net representation we are actually doing better than that, okay? And here are some

41:51.800 --> 41:57.800
examples of kind of propagating various things like skeletons or labels or things like that, okay?

42:00.680 --> 42:11.080
So this is one way to use this contrastive learning as without data augmentation but of

42:11.080 --> 42:16.520
course in my lab we also like to make pretty pictures and so I'll briefly show you another

42:17.160 --> 42:26.120
way of using the same kind of an idea of kind of creating your own latent views for an image-to-image

42:26.120 --> 42:34.680
translation setup, okay? And here the idea is of course image unfair translation. The classic

42:34.680 --> 42:39.320
thing that we've been doing for a while we want to translate horses into zebras but we don't have

42:39.320 --> 42:44.680
a correspondence between horses and zebras, okay? So we want to go from here to here but we don't

42:44.680 --> 42:54.200
have a correspondence, okay? And of course the one powerful signal here is we can use a GAN loss

42:54.200 --> 43:01.640
which basically says make this thing into a zebra by basically forcing it to look like other zebras

43:01.640 --> 43:07.080
that I have seen, okay? Using a GAN loss but that GAN loss is not enough because it can make it look

43:07.160 --> 43:13.560
like a zebra many ways, right? But we wanted to kind of be in correspondence and so this is where

43:13.560 --> 43:20.440
we also want to have another constraint and in the past in works like cycle GAN we use the cycle

43:20.440 --> 43:25.640
consistency constraint which says okay make it a zebra but also make it so that when you translate

43:25.640 --> 43:32.440
it back you'll get back to the original horse and that kind of forces this to be the right answer,

43:32.440 --> 43:39.160
not these, okay? But there is a problem with this cycle consistency constraint because the problem

43:39.160 --> 43:44.520
is that it forces it to be a bijection and forces it to be one-to-one because yes it's not going to,

43:45.800 --> 43:53.640
it's not going to go back to here but it's also going to constrain us to have to go back to this

43:53.640 --> 44:01.160
particular horse whereas these other horses might have been just good enough, right? So this is where

44:01.160 --> 44:08.600
kind of a bijection is not always desirable because sometimes these cycles are not a bijection.

44:08.600 --> 44:17.240
So how could we kind of address this problem? And here is the approach that we came up with

44:17.240 --> 44:26.040
which is we are going to have an image-to-image translation framework. We're starting with our

44:26.520 --> 44:31.560
with our horse. We want to get a zebra so we have a GAN loss that says okay make this a zebra,

44:31.560 --> 44:38.360
okay? And then what we're going to do in addition is we want to make this zebra to be

44:39.240 --> 44:46.440
similar in structure to this horse but not in texture and what we're going to do way to do this

44:46.440 --> 44:54.520
is we're going to enforce the structures to be the same by taking pairs of patches across the

44:54.520 --> 45:02.600
input and the output and say that these two patches need to be close to each other in features play

45:02.600 --> 45:12.840
the space and farther away than other patches from the horse image, okay? So you can see that again

45:12.840 --> 45:19.160
we are getting this whole similarity learning story here where we are basically bringing these

45:19.160 --> 45:28.600
two things to be closer and farther from the other patches of horse and again just unlike other

45:28.600 --> 45:35.640
methods where the positives are somehow automatic created by data augmentation here the the the

45:35.640 --> 45:43.720
positives are basically our input and our output so the output becomes our data augmentation, okay?

45:44.200 --> 45:51.240
And now we are back into our contrastive learning land we just basically formulate this and we

45:51.240 --> 45:56.520
basically say learn a representation such that these two guys are close so basically it kind

45:56.520 --> 46:02.840
of ignores the texture and focuses on the structure and these things are fine, okay? And

46:02.840 --> 46:09.480
and and of course what we do this we don't do this on just on the pixels we do it at different

46:10.120 --> 46:17.560
levels of of representation at at basically different menu multi-scale patch representation

46:17.560 --> 46:24.840
and we do this contrastive learning basically everywhere here in our decoder, okay? And of

46:24.840 --> 46:34.280
course we also have our gap losses as usual, okay? One kind of interesting cute note for for those who

46:35.000 --> 46:39.800
who are in this might appreciate this well we thought okay you know the positives that's

46:39.800 --> 46:44.680
everything is clear with negatives we just take all the patches from the same image and then we

46:44.680 --> 46:50.520
thought you know maybe it will be better if we take the negatives to be not just patches from

46:50.520 --> 46:56.280
the same image but also just add other patches from other images other negatives, right? Even

46:56.280 --> 47:04.760
should be even better even more negative data, right? And guess what? It turned out that this

47:04.760 --> 47:11.560
did not work as well these external patches actually made performance worse than if we just

47:11.560 --> 47:18.360
kept the eternal patch, okay? And this kind of goes back to some old work that that I have been

47:18.360 --> 47:24.440
doing on textures this is where we also seen that that patches from the same image actually provide

47:24.440 --> 47:29.880
much more information than if you start mixing them up with patches from other image and in fact

47:29.880 --> 47:37.240
Michala Rani has this wonderful example story of doing super resolution using a single image

47:37.240 --> 47:43.320
where she shows that you can do super resolution by learning from a single image you basically

47:43.320 --> 47:52.280
take an image down sample it train a cnn to up sample that one image, right? So you basically

47:52.360 --> 47:57.160
train a single image network and then just reuse that network for the original image.

47:57.160 --> 48:03.720
So that works better than if you're if you're training a standard thing with many with a large

48:03.720 --> 48:10.600
data set, okay? And basically we're seeing the same thing happening here that it's actually the

48:10.600 --> 48:16.760
patches that are in the same image that have the same illumination the same you know camera

48:16.760 --> 48:21.800
parameters the same setting they actually much more powerful information than if you just put

48:21.800 --> 48:28.840
a whole data set. So this is kind of a cute little story and you can see here how using the

48:28.840 --> 48:34.040
internal patches we get much better translation than if you we use external patches where you

48:34.040 --> 48:40.920
can see that there is a lot of mode collapse happening, okay? So yeah, so basically that's

48:40.920 --> 48:50.040
the story and this is our method and compared to compared to something like CycleGAN and other

48:50.040 --> 48:57.000
methods as well and basically we can we see that we're basically getting as well performance as good

48:57.000 --> 49:04.600
as CycleGAN in most cases but it's much faster and it's it's it's one sided you don't need to

49:04.600 --> 49:11.880
train a two thing two-way thing and and it's basically a we think it's kind of a much better

49:12.680 --> 49:16.840
a much better story and so here are some of the transformations that we have

49:17.560 --> 49:23.000
and one cute thing that we can also do is we can basically apply this to instances so for

49:23.000 --> 49:27.880
example let's say that we have a single image Claude Monet's painting we want to make it into

49:29.480 --> 49:35.720
photograph and maybe what we have also is a single image instead of data set we have a

49:35.720 --> 49:42.280
single photograph that is also let's say of water lilies, okay? Well we can basically use the same kind

49:42.280 --> 49:51.880
of contrastive learning basically just between a single reference photo and our output, okay?

49:53.720 --> 50:01.720
and have have have one have the same thing here for the for the positives and have a instead of

50:01.720 --> 50:07.080
instead of again have basically just a single discriminator here and we can get something that

50:07.080 --> 50:14.600
actually works quite a bit better than a lot of these kind of a stylization methods, okay?

50:14.600 --> 50:20.280
So this is this is competitors and this is ours and I think that ours actually looks quite a bit

50:20.280 --> 50:32.120
more natural and also better than CycleGAN. There are some other examples, okay? Let's see what

50:32.120 --> 50:42.840
timing is. Well you know I don't think I have time to go over the second point of why you sell

50:42.840 --> 50:47.560
supervision maybe I'll just give you a little bit of a hint of what I mean and then you can

50:47.560 --> 50:54.520
you can read the paper if you're interested. So basically the idea is that it's a little bit weird

50:55.560 --> 51:01.320
that we are in most of machine learning we are using a fixed training set. It's not very natural

51:01.400 --> 51:09.080
biologically because biological agents they never see the same data twice, right? So you live your

51:09.080 --> 51:14.600
life you never see the same thing twice. You see something first you you know you you deal with it

51:16.040 --> 51:21.080
you hopefully learn from it if you you know if you didn't deal from it you're dead if you deal

51:21.080 --> 51:27.400
with it you learn from it and then and then you you can recover some information from it but then

51:27.400 --> 51:33.640
you never see that again you see maybe something similar, okay? So every new piece of data is

51:33.640 --> 51:42.760
basically first in your test set and then in your training set, okay? And it seems like using a fixed

51:42.760 --> 51:48.120
data set it kind of encourages memorization because you see the same exact thing over and over and over

51:48.120 --> 51:52.840
again. In fact maybe this is actually another reason why data augmentation works because

51:52.920 --> 51:58.600
data augmentation is kind of random you create a random thing every time so you kind of get away

51:58.600 --> 52:05.400
a little bit from this memorization. So in fact this this might be kind of a subtle way in which

52:05.400 --> 52:10.120
data augmentation helps that actually has nothing to do with the data augmentation just basically

52:10.120 --> 52:16.680
randomization of your data, okay? But the point is that if you're using self supervised learning

52:16.680 --> 52:21.400
like the whole point of having a fixed training set was because it was expensive to do all these

52:21.400 --> 52:27.960
labels, you know? ImageNet, poor Fei Fei spent all of her startup money in Stanford labeling this

52:27.960 --> 52:33.720
huge data set, right? So it kind of makes sense that it's it's fixed because it took so much money

52:33.720 --> 52:39.000
to label it. But if you're using self supervised learning if you don't need the labels what's the

52:39.000 --> 52:44.680
point of having a fixed data set? Why can't we just keep downloading images from whatever the

52:44.680 --> 52:50.920
internet the TV whatever and just keep doing it all the time because we can generate our own

52:50.920 --> 52:58.120
labels. Seems kind of natural and so this is where kind of I've been pushing on this idea of kind of

52:58.120 --> 53:03.320
this online continual learning. So you can you can think of it in terms of of the standard

53:03.880 --> 53:08.120
a train valve separation. So you know you have your training set and kind of the standard thing

53:08.120 --> 53:12.520
in machine learning is you can separate it into a training set and a validation set, right?

53:12.520 --> 53:18.520
And we know that if you just train on a training set and then use the validation set to tune your

53:18.520 --> 53:24.760
high parameters you usually get better performance on the eventual test data than if you just train

53:24.760 --> 53:30.360
on all the training set all at once. Even though you're kind of you you think that it's less data

53:30.360 --> 53:35.400
that you're using for training but actually this is effect turns out to be more effective. Well we

53:35.400 --> 53:41.400
can think of the same thing in a continual way. So we can think of it as you train on the data

53:41.400 --> 53:47.400
that you have seen and then you're validating on the next data that comes along, okay? And then

53:47.400 --> 53:51.960
once you do that you just incorporate it into your training center you can keep going and they

53:51.960 --> 53:57.000
can keep going on forever. You don't ever need to stop, okay? And this I think is a kind of a very

53:57.000 --> 54:05.800
powerful trick that is made that we can now do because we can use self-supervision to do this

54:05.800 --> 54:11.160
kind of this evaluation, this testing, okay? And so this is the idea of test time training which is

54:11.160 --> 54:17.320
our attempt to operationalize this on an infinite smoothly changing stream and the idea is to basically

54:17.640 --> 54:27.000
use self-supervision to continuously adapt to new data, okay? And we did this already in the

54:28.440 --> 54:36.040
case of reinforcement learning with our curiosity work and this new work is basically trying to do

54:36.040 --> 54:46.360
it for images and this is the paper, test time training and it was in ICML 2000. Maybe

54:46.920 --> 54:53.000
just give me, I'll give you one slide of intuition of what we're doing. Basically,

54:53.000 --> 55:01.880
the idea is that we're, let's say we have a training set of object detection, right? And at

55:01.880 --> 55:08.680
training time we have our standard thing, we have our image and we have our label so nothing new

55:08.680 --> 55:18.360
here, we have input in, label out, we are training and then at the same time we also have a self-supervised

55:18.360 --> 55:25.080
head that basically given your image it does some self-supervised task. In this case we are basically

55:25.080 --> 55:30.760
our task here is rotation prediction. Given the rotated version of the image we want to predict

55:30.760 --> 55:34.920
which rotation it is. It doesn't really matter, it could be any task at all, okay? So at training

55:34.920 --> 55:41.480
time we do both of those tasks together but then at test time of course we don't have the labels

55:41.480 --> 55:48.760
but we still have this task, okay? And so we can basically around this, we can evaluate this task

55:49.400 --> 55:58.120
and if the result is not good, if it failed this self-supervised task we can do a little bit of

55:58.120 --> 56:04.680
fine-tuning, a little bit of fine-tuning training for this other task but as we're doing the fine

56:04.680 --> 56:12.760
tuning it's going to get changed representation in a way that will also impact the real task that

56:12.760 --> 56:21.880
we care about and that allows us to do better as we are changing, as we're going for the dataset.

56:21.880 --> 56:29.480
And so here is an example where you know given this image at test time basically the right label is

56:29.480 --> 56:36.760
elephant but initially it basically thinks it's a dog but then as we do this fine-tuning on our

56:39.240 --> 56:45.160
self-supervised task it figures out that it's actually less of a dog and more of an elephant

56:45.160 --> 56:50.120
and gives us the right answer and that's basically the story of the paper, sorry I had to rush but

56:50.680 --> 56:59.000
you can look at the paper online. And to conclude, why use self-supervision?

57:01.320 --> 57:07.400
One reason that I like is that it allows us to get away from this top-down semantic categorization

57:07.400 --> 57:14.760
and gets us more into this bottom-up association story and learn things from the bottom-up

57:14.760 --> 57:21.240
but we must be careful that the supervision doesn't leak in through things like data augmentation

57:21.240 --> 57:27.720
right and we need to be careful about this and second is that eventually self-supervision should

57:27.720 --> 57:34.200
enable us to check the datasets, forget about all these fixed datasets and and learn things continuously

57:35.080 --> 57:40.360
and it's you know we're still we're only starting on this direction I think it's very exciting direction

57:40.440 --> 57:46.120
very exciting problem so I'm hoping people will get excited about it okay thank you very much

57:47.640 --> 57:50.920
yeah awesome fantastic talk thanks a lot for all the amazing works

57:52.120 --> 57:57.240
yeah self-supervision is cool and does anybody have some some some question on Zoom maybe let's

57:57.240 --> 58:00.920
start with this we have a lot of questions on YouTube but I'm going to start to assume

58:00.920 --> 58:05.800
I have a lot of questions too but if somebody wants to ask question on Zoom just turn on your video

58:05.800 --> 58:07.720
and and just pick up probably

58:13.720 --> 58:20.600
I can start with one with a maybe a higher level question first so I mean the challenge

58:20.600 --> 58:25.480
in self-supervision is right you basically have visual data on you let's say correlate patches

58:25.480 --> 58:30.600
with whatever contrastive loss or whatever whatever people do now um I mean what do you

58:30.600 --> 58:34.360
think about if you're thinking about the 3d world right you have obviously a third dimension

58:34.920 --> 58:39.560
is it a smart idea to do this actually all on on images and videos and not think about

58:40.280 --> 58:46.680
I don't know like kind of project a 3d representation maybe first and then think about how to kind of

58:46.680 --> 58:52.200
get similarities in some 3d space learn a 3d representation and then you know try to

58:52.200 --> 58:59.000
channelize with the onscreen tasks later on right right no this is absolutely and and as you know

58:59.000 --> 59:05.560
you know I've been I've been angling for for going into 3d you know since since since a long

59:05.560 --> 59:11.400
time ago since our work with Derry Coyne on qualitative 3d I'm a I'm a big fan of 3d in

59:11.400 --> 59:17.560
my heart and it's kind of a little bit sad that once you know once we went to neural networks

59:17.560 --> 59:22.840
the kind of things dropped back to 2d plane for a while and now of course they're they're coming

59:22.840 --> 59:31.000
back again um okay so there's there's two answers to this question one the final you know the the

59:31.000 --> 59:42.120
ultimate answer is that 3d should emerge from our 2d of observation that the representation

59:42.120 --> 59:52.680
should figure out 3d on its own okay uh just like it's done with humans right humans are only seeing

59:53.480 --> 59:59.320
2d projections of the 3d world okay if you have stereo maybe you have a little bit of 3d but

59:59.320 --> 01:00:03.640
you know I don't have stereo for example 10 percent of people in the world don't have stereo

01:00:03.640 --> 01:00:11.320
and we are perfectly fine seeing 3d okay so we learn 3d from uh from from a series of 2d

01:00:11.320 --> 01:00:18.840
representations uh and I think if we if we go from you know collections of images like ImageNet

01:00:18.840 --> 01:00:28.840
to videos for example hopefully and I'm very hoping that like it will encourage 3d to automatically

01:00:28.840 --> 01:00:37.960
emerge as as the you know inside of the representation okay so that's kind of a the the the the the

01:00:37.960 --> 01:00:44.520
glorious answer at the end of the rainbow okay uh but of course this is this is very hard this is

01:00:44.520 --> 01:00:50.040
kind of a a very tall order uh you know we are seeing a little bit of this happening we are seeing

01:00:50.040 --> 01:00:55.880
a little bit of kind of a maybe two and a half two or two point one d kind of occlusion occlusion

01:00:55.880 --> 01:01:03.720
reasoning you know figure ground reasoning uh a little bit of of that but but but it we're

01:01:03.720 --> 01:01:09.640
they're definitely far away from that right and so the second direction is okay can we kind of help

01:01:09.640 --> 01:01:18.760
it out a little bit how can we can we provide features that are more amenable to to to three

01:01:18.760 --> 01:01:27.240
dimensional manipulation and there I think uh things like like like holo-gan or or pie-gan this

01:01:27.240 --> 01:01:34.520
kind of directions are I think very exciting in in that it's kind of you you can inject some things

01:01:34.520 --> 01:01:41.640
that you know are physically true like rotation for example and and uh and so I think I think in the

01:01:41.640 --> 01:01:49.080
short in the short uh uh uh short term all of those things are I think going to be extremely

01:01:49.080 --> 01:01:55.640
helpful in the long term I'm still kind of hoping that I can learn 3d from scratch okay but who knows

01:01:55.640 --> 01:02:01.160
maybe it's too much to ask but I'm still kind of hoping that one day I will wake up in the morning

01:02:01.160 --> 01:02:07.560
and boom my computer learned 3d but we'll see do it with two cameras right we have stereo that's I

01:02:07.560 --> 01:02:15.560
mean that's the thing but I don't have stereo for example right like 10 percent of people don't

01:02:15.560 --> 01:02:25.080
have stereo stereo is actually not as important as as as as we we we we think stereo is only really

01:02:25.960 --> 01:02:30.440
important for like the you know half a meter in front of you it's like it's you know what

01:02:30.440 --> 01:02:35.480
what is it that I cannot do that everybody else can do okay I cannot put you know thread through

01:02:35.480 --> 01:02:41.320
the needle and I have trouble you know pouring wine right other than that I'm fine so really it

01:02:41.320 --> 01:02:47.560
stereo is kind of over overemphasized I think it's really parallax is much more important

01:02:47.560 --> 01:02:53.720
and parallax you can get from from video no good point I have another follow-up question

01:02:53.720 --> 01:02:59.560
um so in the similar spirit right like one argument is you can do contrastive learning

01:02:59.560 --> 01:03:03.720
and mostly it's about comparing things right you're saying one versus all classifiers like

01:03:03.720 --> 01:03:08.440
how similar are these things I mean what about going back to the original things when people

01:03:08.440 --> 01:03:12.760
using like auto encoders for pre-training and so on for like basically using generative tasks

01:03:12.760 --> 01:03:18.760
let's say oh I train my favorite game how good of a representation can I learn from learning

01:03:18.760 --> 01:03:23.800
the distribution basically right like how well like it's like this famous thing like you have to

01:03:23.800 --> 01:03:29.640
be able to create in order to understand and where to see that competing or maybe going

01:03:29.640 --> 01:03:33.160
going along the same line so what's your take channel is speaking on the lines there

01:03:35.080 --> 01:03:42.360
um I mean yeah I mean yeah we have definitely been also using out encoders as well I think

01:03:42.360 --> 01:03:51.640
with an outer encoder it's a little bit of a it's a little bit of a of a magic box like if you

01:03:51.640 --> 01:03:56.920
know if you get really lucky your outer encoder is going to capture exactly the right things

01:03:56.920 --> 01:04:00.840
and if you get unlikely it will capture all exactly the wrong things right it's it's kind of

01:04:00.840 --> 01:04:07.720
it's a compression mechanism it somehow compresses your data and and it it really depends on what you

01:04:07.720 --> 01:04:13.880
care about like sometimes it will compress the away the stuff that you care about or sometimes it

01:04:13.880 --> 01:04:19.400
will retain the stuff that you care about and it's it's a little bit hard to control what it's going

01:04:19.400 --> 01:04:28.200
to do so I think I think this kind of a similarity learning is it allows you to get a little bit

01:04:28.200 --> 01:04:34.520
more control and a little bit of more of kind of a intuition about what is it what is it being

01:04:35.160 --> 01:04:39.240
what is it that's being learned it's also kind of a has a very nice connection to kind of to

01:04:39.240 --> 01:04:43.800
graphs and graph theory that kind of think you you think about it like you have different

01:04:43.800 --> 01:04:48.360
entities and then you have kind of you can think of it like as a as like an as a like a network

01:04:48.360 --> 01:04:55.000
right like a like a uh a social network for example where you you can think of different

01:04:55.000 --> 01:04:59.640
people being connected in different ways and you can think about yeah we call them

01:04:59.640 --> 01:05:06.040
senses of similarity so there's many different senses of similarity between two instances and

01:05:06.040 --> 01:05:09.960
you know something like an out encoder is probably going to collapse them all together and here you

01:05:09.960 --> 01:05:14.440
can actually separate them you can have a similarity in color similarity in texture maybe

01:05:14.440 --> 01:05:21.720
similarity in 3d and they're all can be kind of exposed hopefully separately now that's interesting

01:05:21.720 --> 01:05:26.200
I mean our experiences so we've done a lot of stuff on like shape completion in 3d so whenever we

01:05:26.200 --> 01:05:31.000
had the ability to take stuff away and predict it then we got great features this was always amazing

01:05:31.000 --> 01:05:36.280
in terms of using these features to help semantics and whenever we're trying to classify it to help

01:05:36.280 --> 01:05:40.760
the completion this is this is always a total disaster it never worked we tried really hard

01:05:40.760 --> 01:05:48.360
actually I mean I think that that's that's been our our experience as well but but I think have

01:05:48.360 --> 01:05:55.160
you have you tried the latest uh contrastive learning because it's really it's to me the way

01:05:55.160 --> 01:06:02.040
I think about contrastive learning is it's really just old school triplet loss you know uh Siamese

01:06:02.040 --> 01:06:10.920
network learning except you're switching from from from from kind of a regression to to a

01:06:10.920 --> 01:06:17.480
classification but it's a classification with like huge amounts of data and it's very very fine

01:06:17.480 --> 01:06:23.720
grain classification so it's almost it's it's really not like your your grandma's classification

01:06:23.800 --> 01:06:30.200
it's uh I mean we we did something like this for for uh for when we did colorization so we we first

01:06:30.200 --> 01:06:35.960
we tried to do colorization with the regression and then we we we we got better results by doing

01:06:35.960 --> 01:06:42.920
classification but the classification was across like you know 500 classes of different colors

01:06:42.920 --> 01:06:49.480
in the in the in the color gamut right so it's it's a much more kind of narrow thing and that

01:06:49.480 --> 01:06:54.280
seemed to work for us but yeah like if you have a few classes then then then it's very hard to

01:06:54.280 --> 01:06:59.240
make it work but if you do something like either have lots of classes or do something like like

01:06:59.240 --> 01:07:05.080
contrastive learning where it's basically just really kind of push it with data yet it seems to

01:07:05.080 --> 01:07:09.240
to work for us now we've actually tried that so we had we had one one student project actually

01:07:09.240 --> 01:07:12.920
in collaboration with fair so Chihu one of my students they've been working on basically

01:07:14.680 --> 01:07:18.040
basically doing contrastive learning for pre-training 3d structures and in a similar

01:07:18.040 --> 01:07:25.000
way than you would do it in 2d it does help but the completion still seems to work a bit better

01:07:25.880 --> 01:07:33.960
it's very interesting I think so in general yeah I mean I think if if completions if if actually

01:07:33.960 --> 01:07:42.200
predicting you know pixels or predicting voxels whatever um it it has it has more data it has

01:07:42.200 --> 01:07:51.240
more information that and and we know that 3d world is actually much you know it's much more

01:07:51.240 --> 01:07:58.520
informative right so and it's also I think much more um uni model so the one thing that was hard

01:07:58.520 --> 01:08:03.000
for us for example when we did core colorization is what we're you know we're trying to colorize a

01:08:03.000 --> 01:08:08.840
bird and the birds could be yellow or the bird could be green right and so you have multi model

01:08:08.840 --> 01:08:14.120
you have two modes and if you're doing kind of a just sort of like a regression completion

01:08:14.120 --> 01:08:18.760
what's it going to do it's going to do the average right so it's going to be neither here nor there

01:08:18.760 --> 01:08:25.320
right but if you have a single mode it works really well so it might be that in 3d you're really

01:08:25.320 --> 01:08:32.120
in a world that's much more uni model in which you don't have like you're not trying to have an

01:08:32.120 --> 01:08:36.440
average between two different completions and they get something that doesn't look like either

01:08:36.440 --> 01:08:41.880
but you're actually really focusing on a single mode so in that case maybe this is why you're

01:08:41.880 --> 01:08:48.360
getting better results but if I suspect that if you had multi model like if you have a hole that's

01:08:48.360 --> 01:08:55.800
big enough that you could have many different plausible completions happen to it I suspect that

01:08:55.800 --> 01:09:01.880
that that then you're kind of uh the kind of the the prediction route might have more problems

01:09:01.880 --> 01:09:07.160
no I fully agree with you no that one is definitely true and and but most of the case right you're

01:09:07.160 --> 01:09:11.560
thinking about it's more like a dropout in a sense right so you're leaving out some stuff right and

01:09:11.560 --> 01:09:14.600
then you're trying to figure out what's missing in this case I think we've experienced that it

01:09:14.600 --> 01:09:18.760
works remarkably well if it's too large then you need probabilistic models again and stuff like

01:09:18.760 --> 01:09:24.040
that then it's a lot more difficult um yeah I agree yeah I think I think it's kind of a

01:09:24.040 --> 01:09:29.880
if it's a level of dropout and it should just work yeah I I I agree yeah yeah I think I think

01:09:29.880 --> 01:09:34.920
if it works you should definitely use it absolutely um actually any other questions maybe

01:09:34.920 --> 01:09:38.200
maybe somebody else can ask questions I don't want to dominate the discussion too much

01:09:42.040 --> 01:09:50.760
hi yeah thanks for the talk um I have a similarly high level question so speaking along the lines of

01:09:50.760 --> 01:09:55.960
like multi modality and and stuff like this it seems like you have a lot of inspiration in terms of

01:09:56.920 --> 01:10:05.080
how to learn perception based on how people perform perception and it seems like people do have

01:10:05.080 --> 01:10:11.400
naturally some kind of estimate of uncertainty multi modality and the ability to generate

01:10:11.400 --> 01:10:15.480
also for like these video tracking kind of applications that you showed like multiple

01:10:15.480 --> 01:10:23.560
hypotheses for where um the prediction should go and how far do you think you can get without this

01:10:23.640 --> 01:10:26.760
explicitly modeled or do you think it needs to be explicitly modeled

01:10:29.000 --> 01:10:39.000
good question I think I think I would go with I don't know so um yes humans are very good at

01:10:39.000 --> 01:10:45.800
at modeling uncertainty but they I don't think they're doing it in the way that was the decisions

01:10:45.800 --> 01:10:53.320
to it I don't think humans are actually probabilistic I think I think they might be doing it

01:10:54.040 --> 01:10:58.840
almost like if you remember from a long time ago like all this particle filtering where you

01:10:58.840 --> 01:11:03.720
can keep a whole bunch of hypotheses and then you kind of keep all of them going for a while and then

01:11:03.720 --> 01:11:09.800
you kind of uh drop one like you know there's illusion of like young lady old woman visual

01:11:09.800 --> 01:11:15.480
illusion where you know one day once time you see like an old lady one time you see a young woman

01:11:15.480 --> 01:11:22.680
right and you never see both of them so it seems some there's some very interesting mechanism going

01:11:22.760 --> 01:11:32.200
on but I think it's not it's not like a standard probabilistic mechanism and so yeah so I don't

01:11:32.200 --> 01:11:38.840
know how to deal with it and in the in the vision in the in this video paper that I showed you know

01:11:38.840 --> 01:11:44.520
we are really just keeping a whole bunch of hypotheses as we're going through the through

01:11:44.520 --> 01:11:52.840
the video at training time at test time we don't and but whether that's the right thing to do or

01:11:52.840 --> 01:11:58.360
not I don't know I think it's a very important question I don't I don't have an answer but

01:11:58.360 --> 01:12:07.960
frankly I think that nobody else does either sounds good I think one other somewhat unrelated

01:12:07.960 --> 01:12:13.400
thing I think there's a bit of a tension between people who think that we should be able to learn

01:12:13.400 --> 01:12:18.920
everything from scratch like you mentioned in terms of being able to learn 3d and whether this

01:12:18.920 --> 01:12:24.920
actually possible because it's unclear I guess how many how much supervision certainly for

01:12:24.920 --> 01:12:32.120
like some rantic perception people get direct supervision and so yeah okay now we're back to

01:12:32.200 --> 01:12:39.400
philosophy I think it must be possible because because it already happened right

01:12:42.040 --> 01:12:49.640
supervised learning is something that happens in nature but it's it's very very rare like like

01:12:50.520 --> 01:12:56.600
parents teaching their children things I know that a lot of modern parents they feel like it's

01:12:56.600 --> 01:13:02.120
super super important but sorry you know a developmental psychologist disagree they say

01:13:02.120 --> 01:13:08.120
that it doesn't really matter that much most of the things that a kid picks up they pick up

01:13:08.120 --> 01:13:15.160
without supervision they keep pick up on their own and and you could think about it you know

01:13:16.440 --> 01:13:20.600
from in the very beginning right in the beginning if you're you know as long as you believe in

01:13:20.600 --> 01:13:25.640
evolution you must believe in in unsupervised or self-supervised learning because in the beginning

01:13:25.640 --> 01:13:30.200
there was nothing there was no there was no teacher there was no supervision there was only

01:13:30.200 --> 01:13:35.720
data right and kind of the organism and its environment were co-involving and learning

01:13:35.720 --> 01:13:43.960
from each other and and and and and and develop so I think there is there is to me there is no

01:13:43.960 --> 01:13:51.800
question that it should be possible in theory I think that the kind of the the the interesting

01:13:51.880 --> 01:13:57.080
question is is it does it make sense to do in practice right and like you could also say well

01:13:57.080 --> 01:14:02.760
why don't we just simulate evolution for for a gazillion years and then we'll get everything

01:14:02.760 --> 01:14:08.120
right and that's of course not feasible with the current technology so I don't think that there is

01:14:08.120 --> 01:14:13.400
that much or maybe there shouldn't be that much tension because I think I think there are people

01:14:13.400 --> 01:14:18.040
like me who really want to try to learn things from first principles and I think this is very

01:14:18.040 --> 01:14:23.720
interesting if if if anything from you know from the biological plausibility point of view

01:14:23.720 --> 01:14:29.880
okay and there are people who just want to get stuff done and and and get to a good result too

01:14:29.880 --> 01:14:35.880
fast and those people should definitely just use whatever works best at the time so I'm not sure

01:14:35.880 --> 01:14:41.640
that it's either or I think both directions are are useful and I think we're learning from each

01:14:41.640 --> 01:14:46.200
other I think those two direction directions are informing each other so for example for a very

01:14:46.200 --> 01:14:53.640
long time self-supervised approaches worked worse than supervised approaches and so you know

01:14:54.360 --> 01:14:58.600
if you know we could have all quit because oh my god you know our stuff doesn't work as well as

01:14:58.600 --> 01:15:03.320
supervision but we persevered because we thought that you know there's something interesting that

01:15:03.320 --> 01:15:09.960
that we could learn anyway and now what we are seeing is that for some tasks self-supervision

01:15:09.960 --> 01:15:14.840
actually works better than supervised learning not for all not for many but for some there's

01:15:14.840 --> 01:15:21.880
definitely some cases when it actually the the the learning from the data actually gives you

01:15:21.880 --> 01:15:27.880
better results than learning from uh from from from from labels and so I think I think I think

01:15:27.880 --> 01:15:34.520
you know let all the flowers bloom it I think both directions are useful and I think it's

01:15:34.520 --> 01:15:41.640
it's great that people are are pushing in in in in both of them and I think we'll we'll get to a

01:15:41.720 --> 01:15:48.040
better point eventually and we'll learn more so I'm actually optimistic on on on all fronts it's

01:15:48.040 --> 01:15:53.240
yeah it's not a competition well it is a competition but it's not like it's not one is right another

01:15:53.240 --> 01:16:00.280
is wrong I think both are right cool all right I think that's a that's a very good um I guess

01:16:00.280 --> 01:16:04.280
ending of the live stream I think thanks a really lot for the amazing talk um we're a little bit

01:16:04.280 --> 01:16:09.000
over I have to apologize to a lot of questions on YouTube we couldn't unfortunately go into all of

01:16:09.000 --> 01:16:15.480
them um but um it was really great to have you and um I hope also for everybody who is with

01:16:15.480 --> 01:16:21.080
with here right now next week we'll have another great lecture with rock help and yeah we'll see

01:16:21.080 --> 01:16:24.200
so thanks a lot again for the for the great year I need great research

01:16:28.680 --> 01:16:29.800
okay um

