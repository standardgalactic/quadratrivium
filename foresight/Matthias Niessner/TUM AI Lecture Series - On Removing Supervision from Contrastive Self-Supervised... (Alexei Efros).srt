1
00:00:00,000 --> 00:00:12,800
All right, welcome everybody today to my lecture.

2
00:00:12,800 --> 00:00:14,960
It's a real pleasure to have Alyosha Efros today.

3
00:00:14,960 --> 00:00:19,920
Alyosha is a professor at UC Berkeley, where he's part of the Berkeley Artificial Intelligence

4
00:00:19,920 --> 00:00:22,200
Research Lab there.

5
00:00:22,200 --> 00:00:26,240
His work is at the intersection of graphics and computer vision, and I'm sure pretty much

6
00:00:26,240 --> 00:00:29,240
everybody in the community has heard of him.

7
00:00:29,240 --> 00:00:33,400
He's a pioneer at the intersection in these fields.

8
00:00:33,400 --> 00:00:38,880
He has countless of exciting papers, starting from texture synthesis to conditional GANs

9
00:00:38,880 --> 00:00:41,720
like Pics to Pics and Cycle GAN.

10
00:00:41,720 --> 00:00:46,480
He's particularly known for his creativity and thought-provoking work, which is an inspiration

11
00:00:46,480 --> 00:00:48,280
to many young researchers.

12
00:00:48,280 --> 00:00:51,320
His students have also had really great success.

13
00:00:51,320 --> 00:00:54,960
You can see many of his students are now professors themselves.

14
00:00:54,960 --> 00:01:00,120
And I think it's also, yeah, it's fair to say that he has also a very great social

15
00:01:00,120 --> 00:01:03,840
engagement, in particular, contributing to the research community.

16
00:01:03,840 --> 00:01:08,280
If you haven't met him in person, I can only recommend reach out to him at the conferences

17
00:01:08,280 --> 00:01:09,280
once we have them again.

18
00:01:09,280 --> 00:01:11,040
It's really great to have him around.

19
00:01:11,040 --> 00:01:15,320
It's really great hanging around with him at one of the poster sessions and chat about

20
00:01:15,320 --> 00:01:16,760
some really exciting research.

21
00:01:16,760 --> 00:01:21,000
He's particularly known for his, yeah, really cool attitude, and it's really great to have

22
00:01:21,000 --> 00:01:22,800
him as part of the community.

23
00:01:22,800 --> 00:01:24,880
So it's a real pleasure to have you here.

24
00:01:24,880 --> 00:01:26,800
And I'm really looking forward to the talk.

25
00:01:26,800 --> 00:01:29,640
And you also promised some philosophical components.

26
00:01:29,640 --> 00:01:34,120
So I'm really excited what that's going to be.

27
00:01:34,120 --> 00:01:37,960
Thank you so much for such a gracious introduction.

28
00:01:37,960 --> 00:01:51,680
Yeah, I'm sad that we have to do this virtually because I would love to have hung out with

29
00:01:51,680 --> 00:01:56,400
you guys and gone for some wonderful Barbarian beers.

30
00:01:56,400 --> 00:02:01,880
But next time, yes, thank you very much for inviting me.

31
00:02:01,880 --> 00:02:11,040
And it's such a star spangled roster of speakers that you have there.

32
00:02:11,040 --> 00:02:16,520
I hope I will not disappoint.

33
00:02:16,520 --> 00:02:22,960
And so I'm going to talk about aspects of self-supervision.

34
00:02:22,960 --> 00:02:28,360
Self-supervision is something that my lab has been working for a number of years.

35
00:02:28,360 --> 00:02:33,520
And just to make sure everyone's on the same page, self-supervised learning is when we,

36
00:02:33,520 --> 00:02:41,280
this is my definition, you know, hopefully there is no one set definition, but my definition

37
00:02:41,280 --> 00:02:49,960
is that it's when we use the tools of supervised learning, but where the labels are the raw

38
00:02:49,960 --> 00:02:54,600
data instead of being human-provided.

39
00:02:54,600 --> 00:03:01,640
And so the question that often students ask is why use self-supervised learning?

40
00:03:01,640 --> 00:03:03,480
What's the point?

41
00:03:03,480 --> 00:03:09,120
And the classic answer, the common answer is because labels are expensive.

42
00:03:09,120 --> 00:03:16,400
Instead of having humans provide labels and annotating them using lots of hours of work

43
00:03:16,400 --> 00:03:25,960
or high cost, here we get used the data itself as our labels.

44
00:03:25,960 --> 00:03:30,320
So this is the common answer, but that's not really my main answer.

45
00:03:30,320 --> 00:03:34,480
This is, it's nice, but it's not the main reason for me.

46
00:03:34,480 --> 00:03:38,320
For me, I actually have a couple of answers.

47
00:03:38,320 --> 00:03:46,360
The first one answer is that self-supervised learning allows us to get away from the tyranny

48
00:03:46,360 --> 00:03:53,240
of this top-down semantic categorization that goes all the way to Plato and Socrates.

49
00:03:53,240 --> 00:03:57,960
And I'll tell you why I think this is a good thing.

50
00:03:57,960 --> 00:04:06,280
And the second reason is that self-supervised learning will hopefully enable us to move away

51
00:04:06,520 --> 00:04:16,160
from this idea of a fixed training set to a more continuous lifelong learning where you

52
00:04:16,160 --> 00:04:20,120
have data streaming in and then you learn on the go.

53
00:04:20,120 --> 00:04:24,840
You learn as you live, rather than having this kind of training set, testing set, split

54
00:04:24,840 --> 00:04:30,120
that is kind of the classic thing in machine learning.

55
00:04:30,120 --> 00:04:34,440
So I will start, and most of the talk is going to be about the first one, and then hopefully

56
00:04:34,440 --> 00:04:38,000
we'll do a little bit of number two, okay?

57
00:04:38,000 --> 00:04:42,520
So what's the problem with semantic categories?

58
00:04:42,520 --> 00:04:48,320
Well, let's look at, from the visual point of view, let's look at a couple of categories

59
00:04:48,320 --> 00:04:51,440
from a standard visual data set.

60
00:04:51,440 --> 00:04:56,360
So the first is what's called a chair, okay?

61
00:04:56,360 --> 00:05:06,280
But you can look at all the different chairs that you could have in the wild, and you realize

62
00:05:06,280 --> 00:05:16,360
that this is very hard to find what is in common between all of these chairs, right?

63
00:05:16,360 --> 00:05:21,360
Visually, actually, there is pretty much nothing in common because between something

64
00:05:21,360 --> 00:05:23,240
like this and something like this, right?

65
00:05:23,240 --> 00:05:27,760
This chair is really more of a functional category, right?

66
00:05:27,760 --> 00:05:29,520
And think about it this way.

67
00:05:29,520 --> 00:05:35,840
We can see them all being chairs because we have seen, you know, butts being squeezed

68
00:05:35,840 --> 00:05:38,300
into these places.

69
00:05:38,300 --> 00:05:44,240
But if you are a computer, if all you've seen in your life is ImageNet data set, for example,

70
00:05:44,240 --> 00:05:50,120
if you've never seen any videos, you've never seen any people sitting on anything, there

71
00:05:50,120 --> 00:05:55,920
is basically no way for you to realize that all of these are somehow related, right?

72
00:05:55,920 --> 00:06:00,600
So the relationship here is not visual.

73
00:06:00,600 --> 00:06:02,440
The relationship is functional.

74
00:06:02,440 --> 00:06:06,040
The relationship is that of affordances.

75
00:06:06,040 --> 00:06:12,200
And it's not really fair for a computer to try really, really hard to find a way to kind

76
00:06:12,200 --> 00:06:18,680
of somehow bring them all into some kind of a connection where maybe there isn't that

77
00:06:18,680 --> 00:06:22,400
much of a connection, okay?

78
00:06:22,400 --> 00:06:26,760
The second example I like is this one called City, okay?

79
00:06:26,760 --> 00:06:32,320
And here what I'm showing here is a picture of downtown Pittsburgh in a picture of the

80
00:06:32,320 --> 00:06:34,440
center of Paris, okay?

81
00:06:34,440 --> 00:06:41,960
And frankly, you know, the fact that both of these by some fluke of the English language

82
00:06:41,960 --> 00:06:48,600
are termed City, the same noun, this is just kind of a coincidence because there is really

83
00:06:48,600 --> 00:06:52,720
nothing visually in common between those two, right?

84
00:06:52,720 --> 00:06:56,320
And people can argue, well, wait a minute, you know, both contain buildings, but look

85
00:06:56,320 --> 00:06:57,320
at the buildings.

86
00:06:57,320 --> 00:07:01,760
The buildings are so different visually, there is nothing in common between those buildings,

87
00:07:01,760 --> 00:07:02,760
right?

88
00:07:02,760 --> 00:07:03,760
They're so visually different.

89
00:07:03,760 --> 00:07:07,280
And then you can say, well, but they both have windows, but again, look at the windows.

90
00:07:07,280 --> 00:07:09,240
The windows don't look anything the same.

91
00:07:09,240 --> 00:07:14,960
Look, there is really not a single pixel in common between these two things.

92
00:07:14,960 --> 00:07:20,920
And so again, we are forcing the computer to do something, to somehow try to generalize

93
00:07:20,920 --> 00:07:26,880
across these two things that are very, very different, that might not have anything in

94
00:07:26,880 --> 00:07:27,960
common.

95
00:07:27,960 --> 00:07:33,960
And so, in a way, what we're forcing the computer to do is basically to cheat, right?

96
00:07:33,960 --> 00:07:42,040
It's kind of like you haven't attended classes and then now it's your final exam and you're

97
00:07:42,040 --> 00:07:47,240
trying to see what, you know, you're trying to cram for the final exam.

98
00:07:47,240 --> 00:07:50,360
And so what the computer is going to do is like, anything that looks like this, it will

99
00:07:50,360 --> 00:07:53,920
be called the city, anything that looks like this will also be called the city.

100
00:07:53,920 --> 00:07:57,880
But you're not going to basically get the concept of a city.

101
00:07:57,880 --> 00:08:04,720
You're just going to basically just remember the, you know, the nearest neighbors, right?

102
00:08:04,720 --> 00:08:12,080
So basically with labels like these, we're, I worry that we're setting ourselves up for

103
00:08:12,080 --> 00:08:13,080
failure, right?

104
00:08:13,080 --> 00:08:22,200
We're setting our algorithms up for just memorizing examples and not really even having building

105
00:08:22,200 --> 00:08:24,120
connections between them, okay?

106
00:08:24,120 --> 00:08:28,560
Which is of course very bad if we want our models to generalize, right?

107
00:08:28,560 --> 00:08:32,720
If we just wanted to, you know, train an image net and then test an image net, that's fine.

108
00:08:32,720 --> 00:08:37,200
But if we wanted to train an image net and test on, you know, the real world out there,

109
00:08:37,200 --> 00:08:38,200
then that's not fine.

110
00:08:38,200 --> 00:08:42,840
We need to somehow induce generalization, okay?

111
00:08:42,840 --> 00:08:52,440
And so this is where my promised bit of philosophy comes in, which is that I argue, I've been

112
00:08:52,440 --> 00:09:01,440
arguing functionally for many years now, that we should step away from the stop down categorization

113
00:09:01,440 --> 00:09:08,000
paradigm and try to think more of it as a bottom up association.

114
00:09:08,000 --> 00:09:15,600
And there is some movement towards that in, especially in the 20th century.

115
00:09:15,600 --> 00:09:20,520
The philosophers have really kind of pushed away plateaus and Socrates notions of these

116
00:09:20,520 --> 00:09:28,840
rigid categories and really started to think about categorization in a much more bottom

117
00:09:28,840 --> 00:09:30,240
up way.

118
00:09:30,240 --> 00:09:38,320
So Plato, of course, he argued that categories were a list of, you know, there were abstract

119
00:09:38,320 --> 00:09:41,180
definitions with a list of shared properties.

120
00:09:41,180 --> 00:09:46,400
And then in the mid 20th century, the philosopher Wittgenstein came out said, no, no, no, no,

121
00:09:46,400 --> 00:09:48,560
this is that people don't do this.

122
00:09:48,560 --> 00:09:54,280
In fact, people are much more fluid about categories.

123
00:09:54,280 --> 00:09:59,160
And, you know, for example, you know, if you ask people, you know, our curtains furniture,

124
00:09:59,200 --> 00:10:03,560
olives, fruit, different people will give you different answers.

125
00:10:03,560 --> 00:10:07,760
In fact, the same person might give you different answers different times of different times

126
00:10:07,760 --> 00:10:08,760
you ask them.

127
00:10:08,760 --> 00:10:20,280
Wittgenstein's classic puzzle was to ask people to name all what is in common across all games.

128
00:10:20,280 --> 00:10:25,480
What are the common properties shared by all games and not shared by non games?

129
00:10:25,480 --> 00:10:27,600
And that you cannot do it.

130
00:10:27,600 --> 00:10:32,840
It's just impossible that there's such a variety of games out there that you cannot think of

131
00:10:32,840 --> 00:10:35,640
any single thing that defines a game.

132
00:10:35,640 --> 00:10:38,680
It's not a definitional thing.

133
00:10:38,680 --> 00:10:43,000
It's much more kind of data driven, much more bottom up kind of, well, it's a game like

134
00:10:43,000 --> 00:10:44,000
like football.

135
00:10:44,000 --> 00:10:47,000
It's a game like chess, right?

136
00:10:47,000 --> 00:10:54,520
And so in the second half of the 20th century, psychologists in particular, Eleanor Rush

137
00:10:54,520 --> 00:11:03,120
have tried to kind of update the notion of categorization and tied to think about categories

138
00:11:03,120 --> 00:11:09,600
forming from the bottom up and her famous prototype theory of categorization has really

139
00:11:09,600 --> 00:11:16,560
started this trend where the idea was that you basically cluster, you do bottom up clustering

140
00:11:16,560 --> 00:11:22,480
of similar instances into these prototypes and the prototypes then get clustered again

141
00:11:22,480 --> 00:11:28,240
and then you have this kind of a bottom up hierarchy where the categories emerge directly

142
00:11:28,240 --> 00:11:32,800
from the data rather than being kind of this top down, okay?

143
00:11:32,800 --> 00:11:39,880
And later folks have, psychologists have gone even further and argued for what they call

144
00:11:39,880 --> 00:11:48,360
the exemplar based theory of categorization where you basically, you don't really have

145
00:11:48,360 --> 00:11:57,640
categories when you kind, you basically just store instances, store exemplars of everything

146
00:11:57,640 --> 00:12:03,080
that you see and then you basically learn associations between those examples and the

147
00:12:03,080 --> 00:12:07,120
things that are closer together, essentially can you can think of it as like this kind

148
00:12:07,120 --> 00:12:13,360
of a soft clustering of your exemplars into chunks and to groups and those groups then

149
00:12:13,360 --> 00:12:16,400
emerge to be categories.

150
00:12:17,280 --> 00:12:24,120
My favorite slogan is provided by a neuroscientist Moshe Barker who says, ask not what is it,

151
00:12:24,120 --> 00:12:30,000
ask what is it like to this kind of a, this idea that association bottom up association

152
00:12:30,000 --> 00:12:35,080
trumps this top down labeling, okay?

153
00:12:35,080 --> 00:12:43,400
And so this work has been very inspiring to me and my group over the years and we have

154
00:12:43,400 --> 00:12:50,240
been kind of chugging away at in this direction for a number of years and maybe one of the

155
00:12:50,240 --> 00:12:55,720
earlier works that we did was with my former student Tamash Milosevich where we basically

156
00:12:55,720 --> 00:13:03,720
try to kind of instantiate this exemplar based way of thinking about categorization

157
00:13:03,720 --> 00:13:12,680
and here the example, the idea is that we basically try to find per-exampler distances

158
00:13:12,680 --> 00:13:14,440
given a set of data.

159
00:13:14,440 --> 00:13:22,480
So you have a set of labeled data, you know, cars, people, pedestrians, trees, etc.

160
00:13:22,480 --> 00:13:27,440
And here instead of trying to separate all cars from all pedestrians, for example, we

161
00:13:27,440 --> 00:13:34,560
wanted to basically learn a way to group every single instance of say car.

162
00:13:34,560 --> 00:13:43,200
So for this particular focal example of a car, we wanted to find what other things are

163
00:13:43,200 --> 00:13:44,560
close to it, right?

164
00:13:44,560 --> 00:13:50,560
And so those other things should be also labeled car, but there shouldn't be all cars because,

165
00:13:50,560 --> 00:13:53,480
you know, this car, for example, looks nothing like this car.

166
00:13:53,480 --> 00:13:57,240
So should they really be in the same cluster in the same category?

167
00:13:57,240 --> 00:13:58,240
Maybe not.

168
00:13:58,240 --> 00:14:04,320
And so the idea that Tamash came up with was to basically kind of treat this as a kind

169
00:14:04,320 --> 00:14:09,920
of a classification problem where you basically learn a decision boundary between things that

170
00:14:09,920 --> 00:14:16,680
are close to your focal example and things that are far, but here we actually have instead

171
00:14:16,680 --> 00:14:22,520
of two classes, the ones that are inside of a category and one's out, we have three classes.

172
00:14:22,520 --> 00:14:25,840
There are the things that are close and these are kind of these kind of cars.

173
00:14:25,840 --> 00:14:30,000
There are things that are not cars and they're on the other side.

174
00:14:30,000 --> 00:14:32,960
And then there is a third class which is don't care.

175
00:14:32,960 --> 00:14:39,840
And these are basically other cars that might not actually be that close to the focal car.

176
00:14:39,840 --> 00:14:48,680
And then you basically optimize this, optimize this decision boundary given some set of constraints.

177
00:14:48,680 --> 00:14:55,400
And as a result, what you get is you get something where you learn these distances that are, that

178
00:14:55,400 --> 00:15:06,040
produce much more visually meaningful relationships rather than if you were just doing a standard

179
00:15:06,040 --> 00:15:10,440
set of distances without this, okay?

180
00:15:10,440 --> 00:15:15,720
And so this was kind of a, we were very excited about this, but still we're here, we're still

181
00:15:15,720 --> 00:15:20,920
using the label car, the labels are still being used in this computation.

182
00:15:20,920 --> 00:15:24,400
And we really wanted to go away from labels entirely.

183
00:15:24,400 --> 00:15:29,360
And this is where Tamash's next paper came in.

184
00:15:29,360 --> 00:15:36,560
And this work, we call it exemplar SVM, was basically kind of pushing this farther and

185
00:15:36,560 --> 00:15:45,960
really thinking about it in terms of classifiers and basically switching from the standard

186
00:15:45,960 --> 00:15:52,440
classifier where you have class A and class B instead to take every single instance, every

187
00:15:52,440 --> 00:16:01,120
single data point and train a separate classifier for that one instance against everyone else,

188
00:16:01,120 --> 00:16:04,000
whether it's your own class or a different class.

189
00:16:04,000 --> 00:16:06,760
So it's one against all classifier.

190
00:16:06,760 --> 00:16:10,040
So this is kind of an interesting way to think about it because it's really basically you're

191
00:16:10,040 --> 00:16:16,560
defining yourself, not by who is in your category, who is in your class, but you're

192
00:16:16,560 --> 00:16:20,560
defining yourself by what you are not, right?

193
00:16:20,560 --> 00:16:27,360
So what makes you different from everyone else in your data, okay?

194
00:16:27,360 --> 00:16:36,960
And then the cool result was that we were able to do one classifier for every instance

195
00:16:36,960 --> 00:16:38,840
and then assemble them together.

196
00:16:38,840 --> 00:16:48,160
And the result was that this assemble actually worked no worse in many cases than your standard

197
00:16:48,160 --> 00:16:52,400
two-class classifier like an SVM, okay?

198
00:16:52,400 --> 00:16:58,560
And this was a little bit of a, it was kind of like a bit of a trolling paper, which basically

199
00:16:58,560 --> 00:17:04,400
kind of tried to push the community to say, look, maybe you're not getting as much juice

200
00:17:04,400 --> 00:17:10,560
as you think you are from basically trying to group all those things into one class because

201
00:17:10,560 --> 00:17:17,960
it seems like if you don't do it, you get basically as much of performance as not, okay?

202
00:17:17,960 --> 00:17:22,920
And so we also use the same idea for retrieval.

203
00:17:22,920 --> 00:17:31,360
So basically the idea is you take, at runtime, you take your retrieval query image and then

204
00:17:31,360 --> 00:17:38,280
you have a big data set and you're basically training at runtime an SVM classifier to separate

205
00:17:38,280 --> 00:17:42,120
your query from everything else, okay?

206
00:17:42,120 --> 00:17:51,360
And then you order all of your data based on that, on the coefficients of that decision

207
00:17:51,360 --> 00:17:52,360
boundary, okay?

208
00:17:52,360 --> 00:17:59,480
You basically find who are the closest things, who are your support vectors inside of your

209
00:17:59,480 --> 00:18:05,360
data set and those tend to be the retrieval examples, the kind of the closest ones on

210
00:18:05,360 --> 00:18:11,920
the other side will be the retrieval examples and that also worked surprisingly well.

211
00:18:11,920 --> 00:18:16,560
And so we were very excited about this and we were very hopeful and then of course deep

212
00:18:16,560 --> 00:18:25,280
learning revolution hit and all of this became irrelevant because much better classifiers

213
00:18:25,280 --> 00:18:27,640
came on the scene.

214
00:18:27,640 --> 00:18:37,880
We tried to update it for the deep learning age and we didn't really succeed but Alexey

215
00:18:37,880 --> 00:18:41,280
DeSavitsky and colleagues did, okay?

216
00:18:41,280 --> 00:18:49,040
So one of the kind of very early influential papers was called exemplar CNN, which basically

217
00:18:49,040 --> 00:18:59,840
adopted the same idea of one against all classification on using neural networks, okay?

218
00:18:59,840 --> 00:19:07,360
And the main difference that we didn't really think of was that whereas we used a single

219
00:19:07,360 --> 00:19:16,160
exemplar, one image against everything else, DeSavitsky and colleagues they used what's

220
00:19:16,160 --> 00:19:18,360
called data augmentation.

221
00:19:18,360 --> 00:19:24,000
So they basically, they took one example and then they created a whole bunch of similar

222
00:19:24,000 --> 00:19:33,200
examples by basically applying various different transformations to it, you know, changing

223
00:19:33,200 --> 00:19:40,680
lighting, changing contrast, changing shapes, you know, various geometric transformation,

224
00:19:40,680 --> 00:19:41,680
etc.

225
00:19:41,840 --> 00:19:48,800
In the end, even though all of these things came from a single example, they all were

226
00:19:48,800 --> 00:19:53,880
a little bit different and so this became the positive class and then everything else

227
00:19:53,880 --> 00:19:56,880
became the negative class and that worked really, really well, okay?

228
00:19:56,880 --> 00:20:05,280
And this work really was an inspiration for a lot of the current contrast of self-supervised

229
00:20:05,280 --> 00:20:09,480
learning that we are familiar with right now.

230
00:20:09,480 --> 00:20:15,600
So we thought we will be very pure and just use a single image, but this of course worked

231
00:20:15,600 --> 00:20:16,840
much, much better.

232
00:20:16,840 --> 00:20:21,840
And of course, now in kind of modern day, the self-supervised learning representations

233
00:20:21,840 --> 00:20:30,920
that seem to work the best, they're all based on this idea of similarity learning, of instead

234
00:20:30,920 --> 00:20:38,960
of learning which class you are, which category you are, the idea is to learn instances that

235
00:20:38,960 --> 00:20:41,760
are either close or far from each other.

236
00:20:41,760 --> 00:20:49,360
So learning the distances between the instances in your training data, okay?

237
00:20:49,360 --> 00:20:55,440
So things like metric learning, SiameseNet and the new contrastive learning are all based

238
00:20:55,440 --> 00:20:56,920
on that same principle.

239
00:20:56,920 --> 00:21:02,560
So you basically, you have some sort of an embedding space and your goal is to say, okay,

240
00:21:02,560 --> 00:21:08,240
for a given positive like this particular instance of a dog here, you create a bunch

241
00:21:08,240 --> 00:21:12,120
of different positive example by data augmentation.

242
00:21:12,120 --> 00:21:18,240
And then you basically try to push these ones, all of them to be close to each other in this

243
00:21:18,240 --> 00:21:28,160
embedding space and far away from other things which are dogs or other cats, et cetera.

244
00:21:28,160 --> 00:21:34,920
And this learning of the similarity is really what a lot of the contemporary self-supervised

245
00:21:34,920 --> 00:21:38,920
learning methods are doing, okay?

246
00:21:38,920 --> 00:21:46,840
And so, you know, the reason why this, maybe like a year ago, this area really took off,

247
00:21:46,840 --> 00:21:52,120
one of the reasons is, of course, you know, the improvements in the representation learning.

248
00:21:52,120 --> 00:21:58,040
The contrastive formulation is actually just works much better as shown by papers like

249
00:21:58,040 --> 00:22:00,480
Simplier and stuff.

250
00:22:00,480 --> 00:22:06,640
But another reason I think that's maybe being a little bit underappreciated is that we are

251
00:22:06,640 --> 00:22:11,520
just much better at doing this data augmentation.

252
00:22:11,520 --> 00:22:18,400
So we have learned to do data augmentation in a better way than the Seitzky and his exemplar

253
00:22:18,400 --> 00:22:19,560
Sienna, okay?

254
00:22:19,560 --> 00:22:25,280
For example, now cropping is a very standard trick for data augmentation, which wasn't a

255
00:22:25,280 --> 00:22:29,560
standard trick before, and that gives us a lot of boost.

256
00:22:29,560 --> 00:22:35,280
So again, what data augmentation is, you get yourself an input image, a single instance,

257
00:22:35,280 --> 00:22:40,160
and then, you know, you just randomly create a whole bunch of different versions of that

258
00:22:40,160 --> 00:22:48,240
image by applying a whole bunch of different parameter transformation, whole transformation,

259
00:22:48,240 --> 00:22:51,880
cropping, flipping, blurring, et cetera, et cetera, et cetera, okay?

260
00:22:51,880 --> 00:22:59,720
And then once you do that, then you set up your kind of a distance function.

261
00:22:59,720 --> 00:23:07,360
So you basically say that I want these two images that all came from the same image really,

262
00:23:07,360 --> 00:23:12,800
I want those two images to be similar in our embedding space.

263
00:23:12,800 --> 00:23:19,080
So I'm going to try to bring them close together and farther away from the other images in

264
00:23:19,080 --> 00:23:20,080
my data set.

265
00:23:20,200 --> 00:23:25,160
That's really the whole story of contrastive learning, okay?

266
00:23:25,160 --> 00:23:33,440
Now the thing is that the choice of data augmentation itself turns out to be very, very critical.

267
00:23:33,440 --> 00:23:41,760
And in fact, I want to argue that this data augmentation is itself a little bit of supervised

268
00:23:41,760 --> 00:23:49,960
learning, because the way you choose your augmentation can make a huge difference in

269
00:23:49,960 --> 00:23:52,800
your final performance, okay?

270
00:23:52,800 --> 00:23:59,600
So here is an example from our recent paper in iClear 21, where you can think of, let's

271
00:23:59,600 --> 00:24:05,320
say that you have different types of data augmentation, like color augmentation, maybe

272
00:24:05,320 --> 00:24:08,760
rotation, and maybe texture, okay?

273
00:24:08,760 --> 00:24:16,680
So now we can look at different tasks, for example, if we want something like ImageNet,

274
00:24:16,680 --> 00:24:24,200
course-level categorization, then data augmentation with color makes a lot of sense, with texture

275
00:24:24,200 --> 00:24:28,560
also makes a lot of sense, but rotation is actually going to hurt you, because an upside-down

276
00:24:28,560 --> 00:24:32,600
elephant is not going to be recognized as an elephant, okay?

277
00:24:32,600 --> 00:24:38,720
Whereas if your task is, for example, fine-grain recognition, well, then it gets even more

278
00:24:38,720 --> 00:24:43,040
complicated, because if you're fine-grain the different species of birds, then actually

279
00:24:43,040 --> 00:24:47,240
you don't want any of those data augmentations, because they're all meaningful, like changing

280
00:24:47,240 --> 00:24:53,320
texture may change the species, changing the color definitely will change the species.

281
00:24:53,320 --> 00:24:59,200
Whereas maybe if you're classifying different types of flowers, then rotation is fine, because

282
00:24:59,200 --> 00:25:06,760
rotation augmentation, you know, because flowers are usually rotationally symmetric, okay?

283
00:25:06,760 --> 00:25:13,360
And so you can see that it really becomes very, very task-dependent.

284
00:25:13,360 --> 00:25:16,800
And another example is the cropping and image classification.

285
00:25:16,800 --> 00:25:22,760
So cropping for something like ImageNet classification makes a lot of sense, because you have one

286
00:25:22,760 --> 00:25:26,160
big object in the center of the image.

287
00:25:26,160 --> 00:25:31,280
But the same cropping for object detection actually doesn't make that much sense, because

288
00:25:31,280 --> 00:25:37,280
you crop it and you might lose, you know, where your object is, okay?

289
00:25:37,280 --> 00:25:43,120
And so this is something that we started to worry about, because we feel like a lot of

290
00:25:43,120 --> 00:25:51,880
the advances in the modern self-supervised learning might actually be due to us being

291
00:25:51,880 --> 00:25:58,200
very good at overfitting the right kind of data augmentation for a particular problem

292
00:25:58,200 --> 00:26:02,080
rather than the actual methods themselves, okay?

293
00:26:02,080 --> 00:26:10,120
So what we wanted to do is to try to do contrastive self-supervised learning without data augmentation,

294
00:26:10,120 --> 00:26:18,240
to really try to get it to, to figure it out on its own without this kind of help, okay?

295
00:26:18,240 --> 00:26:22,880
And the way we wanted to do this is to make these, these augmentations, these what they're

296
00:26:22,880 --> 00:26:28,840
called views, to make them latent, to make the computer come up with its own data augmentation

297
00:26:28,840 --> 00:26:31,520
in this, in a sense, okay?

298
00:26:31,520 --> 00:26:37,360
And of course, the big question here is, this is all great, but where do you get the supervisory

299
00:26:37,360 --> 00:26:38,360
signal, right?

300
00:26:38,360 --> 00:26:39,360
There is no free light.

301
00:26:39,360 --> 00:26:43,280
You need to get some sort of supervisory signal from somewhere in your data.

302
00:26:43,280 --> 00:26:45,760
So where is it going to come from?

303
00:26:45,760 --> 00:26:49,840
And here we're going to, I'm going to talk about a couple of papers where we answer

304
00:26:49,840 --> 00:26:51,520
this question differently.

305
00:26:51,520 --> 00:26:59,640
The first paper, the answer we have is that we want to use time as our self-supervisor

306
00:26:59,640 --> 00:27:01,680
signal, okay?

307
00:27:01,680 --> 00:27:07,160
And here I have a wonderful quote from one of my favorite writers, Jorge Luis Borges,

308
00:27:07,160 --> 00:27:14,760
in his short story about fumes, who is this kind of a man on the spectrum.

309
00:27:14,760 --> 00:27:20,840
He writes, it irritated him that the dog at 3.14 in the afternoon seen in profile should

310
00:27:20,840 --> 00:27:26,640
be indicated by the same down as dog at 3.15 seen in front of it, okay?

311
00:27:26,640 --> 00:27:37,040
So basically what he's talking about is that two different instances of time makes most

312
00:27:37,040 --> 00:27:46,000
of us assume that there is some continuity of what we are perceiving, that the dog here

313
00:27:46,000 --> 00:27:50,280
and the dog here, it's almost certainly the same dog.

314
00:27:50,280 --> 00:27:54,800
But nothing happened to this dog while it was jumping in the water, right?

315
00:27:54,800 --> 00:28:02,280
But fumes, of course, couldn't figure this out and neither can our computers, right?

316
00:28:02,280 --> 00:28:11,560
And so the idea is that this temporal correspondence, basically time as a way to align things together,

317
00:28:11,560 --> 00:28:18,120
to bring things into correspondence, is a very powerful supervisory signal that we should

318
00:28:18,120 --> 00:28:19,960
be using, okay?

319
00:28:19,960 --> 00:28:26,600
And we have evidence that biological algorithms use it very strongly.

320
00:28:26,600 --> 00:28:41,440
There is plenty of psychology data for human infants that shows that temporal cues are

321
00:28:41,440 --> 00:28:44,160
very important to learning vision.

322
00:28:44,160 --> 00:28:50,760
And there is this wonderful line of work by Wood who basically did this kind of this

323
00:28:50,760 --> 00:28:53,820
experiments with newly born chicks.

324
00:28:53,820 --> 00:28:59,640
So basically what he says he has is this kind of VR cave for chickens, for little chicks.

325
00:28:59,640 --> 00:29:04,520
So you put an egg and then the chicken is born and the chicken is born in this VR cave

326
00:29:04,520 --> 00:29:07,920
where he's basically projected things on all sides.

327
00:29:07,920 --> 00:29:14,720
So everything that the chick knows from birth is being controlled by the researcher, okay?

328
00:29:14,720 --> 00:29:21,360
And what he showed that some of the chicks were shown videos that were not temporally

329
00:29:21,360 --> 00:29:25,200
coherent, that basically broke this temporal continuity.

330
00:29:25,200 --> 00:29:32,880
As you can see, it was not kind of physically correct and he compared them to chicks that

331
00:29:32,880 --> 00:29:37,040
were shown normal, normal continuous things.

332
00:29:37,040 --> 00:29:45,800
And the chicks who saw these continuous patterns, they were not able to function in the world

333
00:29:45,800 --> 00:29:46,800
as well.

334
00:29:46,800 --> 00:29:51,920
They lacked some of the visual perception skills.

335
00:29:51,920 --> 00:29:57,920
So that showed that this is extremely important, that temporal continuity is extremely important.

336
00:29:57,920 --> 00:30:06,240
And so what we want to do in this work is to use video as data augmentation, as a way

337
00:30:06,240 --> 00:30:13,680
to create these data augmented views ourselves, okay?

338
00:30:13,680 --> 00:30:21,880
And basically the main thing, of course, is that this can provide correspondences across

339
00:30:21,880 --> 00:30:30,200
different instances and allows the computer to learn how something looks across time change,

340
00:30:30,200 --> 00:30:31,360
okay?

341
00:30:31,360 --> 00:30:39,160
But it can give us a bit more because we can also think about contextual relationships

342
00:30:39,160 --> 00:30:44,600
and notice things that are moving in the same way, what Bernheimer called common fate.

343
00:30:44,600 --> 00:30:52,920
We also use that as a way to group little points, little trajectories into groups and

344
00:30:52,920 --> 00:30:57,280
maybe get to the notion of objects from the notion of points, okay?

345
00:30:57,280 --> 00:31:01,680
Again something that you can use temporal information for, okay?

346
00:31:01,680 --> 00:31:04,720
And so this is basically the story.

347
00:31:04,720 --> 00:31:10,200
And then the question is how do we harness this information without any sort of supervisory

348
00:31:10,200 --> 00:31:11,200
signal, okay?

349
00:31:11,200 --> 00:31:19,120
And in the past, people have used things like slow feature learning where they basically

350
00:31:19,120 --> 00:31:25,840
kind of looked at connecting nearby frames together, basically look at nearby frames

351
00:31:25,840 --> 00:31:30,520
as the positives and far away frames as the negatives, but you just collapse the entire

352
00:31:30,520 --> 00:31:36,160
frame and so that's not really something that can give you these point tracks, it's much

353
00:31:36,160 --> 00:31:38,160
more coarse signal.

354
00:31:38,600 --> 00:31:45,000
Alternatively people use things like optical flow or tracking to create correspondences

355
00:31:45,000 --> 00:31:50,120
using some off the shelf methods and then use learning to connect things that are supposed

356
00:31:50,120 --> 00:31:51,280
to be in correspondence.

357
00:31:51,280 --> 00:31:56,800
But here you're using two different methods and so what we wanted to do is do something

358
00:31:56,800 --> 00:32:01,360
like this but kind of in one go, in one pack.

359
00:32:01,360 --> 00:32:10,640
And this is our paper that tries to do this, this was published in Europe's past year

360
00:32:10,640 --> 00:32:13,720
and here is the idea, okay?

361
00:32:13,720 --> 00:32:21,080
And so for a warm up, let's consider the case where you actually do have labels.

362
00:32:21,080 --> 00:32:26,400
Let's say that somebody went ahead and labeled that this patch corresponds to this patch,

363
00:32:26,400 --> 00:32:27,400
okay?

364
00:32:27,400 --> 00:32:35,280
And your goal is to basically learn a representation that brings things that are the same into

365
00:32:35,280 --> 00:32:39,520
correspondence and away from things that are different, right?

366
00:32:39,520 --> 00:32:43,800
So in this case of course it's very simple, you just say, okay, these two things are my

367
00:32:43,800 --> 00:32:48,480
two positives, I want them to be close together and all the other patches are my negatives,

368
00:32:48,480 --> 00:32:52,760
I want them to be pushed far away and then you have your standard self-supervised learning,

369
00:32:52,760 --> 00:32:55,800
contrastive learning problem and off you go, right?

370
00:32:55,800 --> 00:32:57,080
Nothing very exciting.

371
00:32:57,080 --> 00:33:03,600
So things get a little bit more exciting if you have maybe another frame in between, okay?

372
00:33:03,600 --> 00:33:06,800
Because now you have, these two guys are your two positives.

373
00:33:06,800 --> 00:33:16,600
We know this by labeling but also because this is a video, we know that from here somehow

374
00:33:16,600 --> 00:33:25,520
it needed to go to be in this final place and so there must have been a path, the most

375
00:33:26,440 --> 00:33:31,040
likely path from this guy to go from here to here and the most likely path is going

376
00:33:31,040 --> 00:33:38,040
to go through this patch so it's reasonable to assume that this patch should also be in

377
00:33:38,040 --> 00:33:42,840
our positive category, it should be in the same category as this guy and this guy.

378
00:33:42,840 --> 00:33:47,920
So now these triplets should be the positives and everything else should be the negatives.

379
00:33:47,920 --> 00:33:53,240
So now you can think of a little bit of kind of automatic data documentation that this guy

380
00:33:53,320 --> 00:34:00,680
just by virtue of being tracked in the video becomes a data augmented positive for our kids,

381
00:34:00,680 --> 00:34:02,040
okay?

382
00:34:02,040 --> 00:34:07,000
So this is okay but this is still requiring us to have this supervision.

383
00:34:07,000 --> 00:34:08,360
How could we get rid of supervision?

384
00:34:08,360 --> 00:34:15,400
Well, we are going to use our old trick which we've used before called cycle consistency

385
00:34:15,400 --> 00:34:20,040
and what we're going to do is we're going to make this video into a palindrome.

386
00:34:20,040 --> 00:34:25,160
Palindrome if you remember in language is a word that you read it forward and backwards

387
00:34:25,160 --> 00:34:26,520
and it reads the same, right?

388
00:34:26,520 --> 00:34:29,000
So how can we make a palindrome out of a video?

389
00:34:29,000 --> 00:34:34,760
Well, what we can do is we can take this video and flip it around and put it back

390
00:34:36,440 --> 00:34:38,280
in the reverse order, okay?

391
00:34:38,280 --> 00:34:42,680
So now what we have is we have something like frame one, frame two, frame three

392
00:34:42,680 --> 00:34:46,040
and now we're going back to frame two and then frame one, okay?

393
00:34:46,040 --> 00:34:50,680
So now we have this new video that's a palindrome and look what's happening.

394
00:34:50,680 --> 00:34:59,800
Now the final place, the destination is now exactly the same as the origin by construction.

395
00:35:00,520 --> 00:35:03,560
So now we don't need supervision anymore.

396
00:35:03,560 --> 00:35:04,920
We got rid of supervision.

397
00:35:04,920 --> 00:35:08,760
All we need to do is to get from the blue guy to the green guy

398
00:35:09,080 --> 00:35:17,720
and the way we do it is we're basically trying to do a track through this video

399
00:35:18,280 --> 00:35:23,800
and everything that's on this track should be in our positive category

400
00:35:23,800 --> 00:35:25,960
and everything else should be in the next, okay?

401
00:35:25,960 --> 00:35:32,600
And so now you can see the setup where we basically are getting something out of nothing.

402
00:35:32,600 --> 00:35:40,360
We're getting some supervisory signal just from the mere video information, okay?

403
00:35:40,360 --> 00:35:43,640
So basically the story is that we are going to take a video.

404
00:35:43,640 --> 00:35:50,280
We're going to make it a palindrome by going from t to t plus k and then minus to back to t.

405
00:35:50,280 --> 00:35:53,320
We're going to make it into a graph.

406
00:35:53,320 --> 00:35:56,760
We're going to turn the video into a graph, okay?

407
00:35:57,480 --> 00:36:06,440
And then we're going to walk along this graph until we get to the end, okay?

408
00:36:06,440 --> 00:36:10,040
And we're going to do basically a random walk on this graph

409
00:36:10,840 --> 00:36:13,560
and then we're going to steer that random walk

410
00:36:14,200 --> 00:36:17,960
such that if you start from this blue point right here,

411
00:36:18,600 --> 00:36:22,520
we want to steer it to get us to this green point right here, okay?

412
00:36:23,400 --> 00:36:26,520
And this is going to be our only supervision.

413
00:36:26,520 --> 00:36:29,880
The supervision is going to be at the last frame where we're going to say that the green,

414
00:36:30,520 --> 00:36:34,280
the positive is going to be anything that lands on the green dot

415
00:36:34,280 --> 00:36:40,120
and negative is going to be anything that lands anywhere else, anywhere on this red dot, okay?

416
00:36:40,120 --> 00:36:43,000
And that's basically going to be the signal that we're going to use, okay?

417
00:36:45,080 --> 00:36:51,640
So, and also notice that we don't have to have a single path through this graph.

418
00:36:51,640 --> 00:36:59,480
We kind of naturally, we can incorporate probabilistic information

419
00:36:59,480 --> 00:37:02,680
by tracing many paths through this graph, okay?

420
00:37:04,040 --> 00:37:07,720
So, you know, how do you turn the video into a graph?

421
00:37:07,720 --> 00:37:09,240
Well, it's kind of a standard thing.

422
00:37:09,240 --> 00:37:12,600
You know, create nodes and then, you know, you're basically,

423
00:37:12,600 --> 00:37:16,280
your nodes are some representation and some using some encoder, phi.

424
00:37:16,280 --> 00:37:19,240
And really, this phi is really the only thing that you're learning.

425
00:37:19,240 --> 00:37:24,600
What you're learning is you're learning the representation of each patch

426
00:37:24,600 --> 00:37:25,720
in your feature space.

427
00:37:25,720 --> 00:37:31,720
And you're basically trying to figure out how to arrange those features

428
00:37:32,360 --> 00:37:35,720
in your representation, who's going to be close, who's going to be far, okay?

429
00:37:36,280 --> 00:37:41,080
And so, now your video is going to be a graph.

430
00:37:41,080 --> 00:37:47,800
And then from frame T to frame T plus one, you're just going to have a transition matrix

431
00:37:47,800 --> 00:37:53,720
that's just going to say, you know, where did all the points from T go in T plus one?

432
00:37:53,720 --> 00:37:58,120
And then this is just basically like a dot product in the feature space.

433
00:37:58,120 --> 00:38:04,280
So, the closest things are going to be to get the higher dot product, okay?

434
00:38:04,280 --> 00:38:06,600
And then how are we going to do it around the work?

435
00:38:06,600 --> 00:38:11,240
Well, just going to compose all of these transition matrices A,

436
00:38:11,240 --> 00:38:15,000
just like, you know, standard Markov chain, you know, you're just multiplying

437
00:38:15,000 --> 00:38:22,280
all of those transition matrices and you get your full work on this graph, right?

438
00:38:22,280 --> 00:38:28,520
So, now kind of the nice thing is that the task of learning this representation phi

439
00:38:28,520 --> 00:38:33,640
is essentially the same as fitting these transition probabilities, okay?

440
00:38:33,640 --> 00:38:38,440
You find the right transition probabilities and it gives you your representation phi, okay?

441
00:38:39,400 --> 00:38:45,640
So, again, in kind of a, if we do have the target somewhere, if we do have the supervision,

442
00:38:45,640 --> 00:38:49,640
then this is just a standard contrast of learning problem.

443
00:38:49,640 --> 00:38:57,480
You basically, you want to find a representation where this query goes directly to the target.

444
00:38:57,480 --> 00:38:59,640
It doesn't go to the red ones, right?

445
00:38:59,640 --> 00:39:02,840
And this is basically just, this is your positive, this is your negative,

446
00:39:02,840 --> 00:39:05,800
this is your standard kind of static learning problem.

447
00:39:05,800 --> 00:39:11,720
If you have multiple frames in between, then in a sense, you have some latent views

448
00:39:11,720 --> 00:39:13,880
that you can also use, right?

449
00:39:13,880 --> 00:39:20,280
So, now we have, these are all the positives and these are the negatives if you,

450
00:39:20,280 --> 00:39:21,880
if it's an obvious path.

451
00:39:21,880 --> 00:39:26,840
But if you have multiple, multiple hyperability paths and these will be late, like,

452
00:39:28,520 --> 00:39:32,680
awaited positives and these will be weighted negatives and you can still do it.

453
00:39:32,680 --> 00:39:38,040
And so, again, from a single point of supervision, you get all of this data-augmented information,

454
00:39:38,040 --> 00:39:38,280
okay?

455
00:39:40,120 --> 00:39:47,720
And of course, what we can do then is we can do the palindrome trick and now we get all of these

456
00:39:47,720 --> 00:39:54,600
latent data-augmented positives without even providing any supervision, okay?

457
00:39:54,600 --> 00:39:59,640
Because this is basically by construction, the target is the same as the quick, okay?

458
00:40:00,520 --> 00:40:06,280
And so, where we can just set this up at training time and you can see that it's,

459
00:40:07,080 --> 00:40:12,760
you know, if you pick a point in the query image, in the first image, you can see that

460
00:40:12,760 --> 00:40:17,080
over time, it kind of gives you a little probability distribution of where that image

461
00:40:17,080 --> 00:40:23,560
might have gone and you can say, well, maybe we can even do this by trying to get grouping

462
00:40:23,560 --> 00:40:28,280
happening and find out a group which corresponds to a single object.

463
00:40:28,840 --> 00:40:33,640
And to do that, we have a little extra thing that we can do which is we can do a dropout.

464
00:40:33,640 --> 00:40:41,880
We can cut some of the engines away and force the correspondences to go through nearby paths

465
00:40:41,880 --> 00:40:46,120
and that basically allows us to get a little bit more of this kind of grouping

466
00:40:47,320 --> 00:40:51,880
happening where you basically kind of, you go through the paths that are also on the same

467
00:40:51,880 --> 00:40:59,400
object, okay? And, you know, we basically violated it at runtime by essentially nearest

468
00:40:59,400 --> 00:41:06,040
neighbor in the phi space and here are some examples. This is the kind of the state-of-the-art

469
00:41:07,240 --> 00:41:12,520
label propagation results and this are the results of our methods and you can see that it's

470
00:41:12,520 --> 00:41:19,080
it's basically behaving much better in terms of occlusion handling and just seems to do quite

471
00:41:19,080 --> 00:41:24,920
a bit better. Here is again state-of-the-art self-supervised method and this is ours, right?

472
00:41:26,200 --> 00:41:31,480
Even against supervised methods actually does pretty well even though it doesn't get any

473
00:41:31,480 --> 00:41:40,280
sort of supervision. So here is kind of an example of how we do compared to some of the

474
00:41:40,280 --> 00:41:45,160
self-supervised comparators and interestingly even for methods that are trained on image,

475
00:41:45,320 --> 00:41:51,800
using image net representation we are actually doing better than that, okay? And here are some

476
00:41:51,800 --> 00:41:57,800
examples of kind of propagating various things like skeletons or labels or things like that, okay?

477
00:42:00,680 --> 00:42:11,080
So this is one way to use this contrastive learning as without data augmentation but of

478
00:42:11,080 --> 00:42:16,520
course in my lab we also like to make pretty pictures and so I'll briefly show you another

479
00:42:17,160 --> 00:42:26,120
way of using the same kind of an idea of kind of creating your own latent views for an image-to-image

480
00:42:26,120 --> 00:42:34,680
translation setup, okay? And here the idea is of course image unfair translation. The classic

481
00:42:34,680 --> 00:42:39,320
thing that we've been doing for a while we want to translate horses into zebras but we don't have

482
00:42:39,320 --> 00:42:44,680
a correspondence between horses and zebras, okay? So we want to go from here to here but we don't

483
00:42:44,680 --> 00:42:54,200
have a correspondence, okay? And of course the one powerful signal here is we can use a GAN loss

484
00:42:54,200 --> 00:43:01,640
which basically says make this thing into a zebra by basically forcing it to look like other zebras

485
00:43:01,640 --> 00:43:07,080
that I have seen, okay? Using a GAN loss but that GAN loss is not enough because it can make it look

486
00:43:07,160 --> 00:43:13,560
like a zebra many ways, right? But we wanted to kind of be in correspondence and so this is where

487
00:43:13,560 --> 00:43:20,440
we also want to have another constraint and in the past in works like cycle GAN we use the cycle

488
00:43:20,440 --> 00:43:25,640
consistency constraint which says okay make it a zebra but also make it so that when you translate

489
00:43:25,640 --> 00:43:32,440
it back you'll get back to the original horse and that kind of forces this to be the right answer,

490
00:43:32,440 --> 00:43:39,160
not these, okay? But there is a problem with this cycle consistency constraint because the problem

491
00:43:39,160 --> 00:43:44,520
is that it forces it to be a bijection and forces it to be one-to-one because yes it's not going to,

492
00:43:45,800 --> 00:43:53,640
it's not going to go back to here but it's also going to constrain us to have to go back to this

493
00:43:53,640 --> 00:44:01,160
particular horse whereas these other horses might have been just good enough, right? So this is where

494
00:44:01,160 --> 00:44:08,600
kind of a bijection is not always desirable because sometimes these cycles are not a bijection.

495
00:44:08,600 --> 00:44:17,240
So how could we kind of address this problem? And here is the approach that we came up with

496
00:44:17,240 --> 00:44:26,040
which is we are going to have an image-to-image translation framework. We're starting with our

497
00:44:26,520 --> 00:44:31,560
with our horse. We want to get a zebra so we have a GAN loss that says okay make this a zebra,

498
00:44:31,560 --> 00:44:38,360
okay? And then what we're going to do in addition is we want to make this zebra to be

499
00:44:39,240 --> 00:44:46,440
similar in structure to this horse but not in texture and what we're going to do way to do this

500
00:44:46,440 --> 00:44:54,520
is we're going to enforce the structures to be the same by taking pairs of patches across the

501
00:44:54,520 --> 00:45:02,600
input and the output and say that these two patches need to be close to each other in features play

502
00:45:02,600 --> 00:45:12,840
the space and farther away than other patches from the horse image, okay? So you can see that again

503
00:45:12,840 --> 00:45:19,160
we are getting this whole similarity learning story here where we are basically bringing these

504
00:45:19,160 --> 00:45:28,600
two things to be closer and farther from the other patches of horse and again just unlike other

505
00:45:28,600 --> 00:45:35,640
methods where the positives are somehow automatic created by data augmentation here the the the

506
00:45:35,640 --> 00:45:43,720
positives are basically our input and our output so the output becomes our data augmentation, okay?

507
00:45:44,200 --> 00:45:51,240
And now we are back into our contrastive learning land we just basically formulate this and we

508
00:45:51,240 --> 00:45:56,520
basically say learn a representation such that these two guys are close so basically it kind

509
00:45:56,520 --> 00:46:02,840
of ignores the texture and focuses on the structure and these things are fine, okay? And

510
00:46:02,840 --> 00:46:09,480
and and of course what we do this we don't do this on just on the pixels we do it at different

511
00:46:10,120 --> 00:46:17,560
levels of of representation at at basically different menu multi-scale patch representation

512
00:46:17,560 --> 00:46:24,840
and we do this contrastive learning basically everywhere here in our decoder, okay? And of

513
00:46:24,840 --> 00:46:34,280
course we also have our gap losses as usual, okay? One kind of interesting cute note for for those who

514
00:46:35,000 --> 00:46:39,800
who are in this might appreciate this well we thought okay you know the positives that's

515
00:46:39,800 --> 00:46:44,680
everything is clear with negatives we just take all the patches from the same image and then we

516
00:46:44,680 --> 00:46:50,520
thought you know maybe it will be better if we take the negatives to be not just patches from

517
00:46:50,520 --> 00:46:56,280
the same image but also just add other patches from other images other negatives, right? Even

518
00:46:56,280 --> 00:47:04,760
should be even better even more negative data, right? And guess what? It turned out that this

519
00:47:04,760 --> 00:47:11,560
did not work as well these external patches actually made performance worse than if we just

520
00:47:11,560 --> 00:47:18,360
kept the eternal patch, okay? And this kind of goes back to some old work that that I have been

521
00:47:18,360 --> 00:47:24,440
doing on textures this is where we also seen that that patches from the same image actually provide

522
00:47:24,440 --> 00:47:29,880
much more information than if you start mixing them up with patches from other image and in fact

523
00:47:29,880 --> 00:47:37,240
Michala Rani has this wonderful example story of doing super resolution using a single image

524
00:47:37,240 --> 00:47:43,320
where she shows that you can do super resolution by learning from a single image you basically

525
00:47:43,320 --> 00:47:52,280
take an image down sample it train a cnn to up sample that one image, right? So you basically

526
00:47:52,360 --> 00:47:57,160
train a single image network and then just reuse that network for the original image.

527
00:47:57,160 --> 00:48:03,720
So that works better than if you're if you're training a standard thing with many with a large

528
00:48:03,720 --> 00:48:10,600
data set, okay? And basically we're seeing the same thing happening here that it's actually the

529
00:48:10,600 --> 00:48:16,760
patches that are in the same image that have the same illumination the same you know camera

530
00:48:16,760 --> 00:48:21,800
parameters the same setting they actually much more powerful information than if you just put

531
00:48:21,800 --> 00:48:28,840
a whole data set. So this is kind of a cute little story and you can see here how using the

532
00:48:28,840 --> 00:48:34,040
internal patches we get much better translation than if you we use external patches where you

533
00:48:34,040 --> 00:48:40,920
can see that there is a lot of mode collapse happening, okay? So yeah, so basically that's

534
00:48:40,920 --> 00:48:50,040
the story and this is our method and compared to compared to something like CycleGAN and other

535
00:48:50,040 --> 00:48:57,000
methods as well and basically we can we see that we're basically getting as well performance as good

536
00:48:57,000 --> 00:49:04,600
as CycleGAN in most cases but it's much faster and it's it's it's one sided you don't need to

537
00:49:04,600 --> 00:49:11,880
train a two thing two-way thing and and it's basically a we think it's kind of a much better

538
00:49:12,680 --> 00:49:16,840
a much better story and so here are some of the transformations that we have

539
00:49:17,560 --> 00:49:23,000
and one cute thing that we can also do is we can basically apply this to instances so for

540
00:49:23,000 --> 00:49:27,880
example let's say that we have a single image Claude Monet's painting we want to make it into

541
00:49:29,480 --> 00:49:35,720
photograph and maybe what we have also is a single image instead of data set we have a

542
00:49:35,720 --> 00:49:42,280
single photograph that is also let's say of water lilies, okay? Well we can basically use the same kind

543
00:49:42,280 --> 00:49:51,880
of contrastive learning basically just between a single reference photo and our output, okay?

544
00:49:53,720 --> 00:50:01,720
and have have have one have the same thing here for the for the positives and have a instead of

545
00:50:01,720 --> 00:50:07,080
instead of again have basically just a single discriminator here and we can get something that

546
00:50:07,080 --> 00:50:14,600
actually works quite a bit better than a lot of these kind of a stylization methods, okay?

547
00:50:14,600 --> 00:50:20,280
So this is this is competitors and this is ours and I think that ours actually looks quite a bit

548
00:50:20,280 --> 00:50:32,120
more natural and also better than CycleGAN. There are some other examples, okay? Let's see what

549
00:50:32,120 --> 00:50:42,840
timing is. Well you know I don't think I have time to go over the second point of why you sell

550
00:50:42,840 --> 00:50:47,560
supervision maybe I'll just give you a little bit of a hint of what I mean and then you can

551
00:50:47,560 --> 00:50:54,520
you can read the paper if you're interested. So basically the idea is that it's a little bit weird

552
00:50:55,560 --> 00:51:01,320
that we are in most of machine learning we are using a fixed training set. It's not very natural

553
00:51:01,400 --> 00:51:09,080
biologically because biological agents they never see the same data twice, right? So you live your

554
00:51:09,080 --> 00:51:14,600
life you never see the same thing twice. You see something first you you know you you deal with it

555
00:51:16,040 --> 00:51:21,080
you hopefully learn from it if you you know if you didn't deal from it you're dead if you deal

556
00:51:21,080 --> 00:51:27,400
with it you learn from it and then and then you you can recover some information from it but then

557
00:51:27,400 --> 00:51:33,640
you never see that again you see maybe something similar, okay? So every new piece of data is

558
00:51:33,640 --> 00:51:42,760
basically first in your test set and then in your training set, okay? And it seems like using a fixed

559
00:51:42,760 --> 00:51:48,120
data set it kind of encourages memorization because you see the same exact thing over and over and over

560
00:51:48,120 --> 00:51:52,840
again. In fact maybe this is actually another reason why data augmentation works because

561
00:51:52,920 --> 00:51:58,600
data augmentation is kind of random you create a random thing every time so you kind of get away

562
00:51:58,600 --> 00:52:05,400
a little bit from this memorization. So in fact this this might be kind of a subtle way in which

563
00:52:05,400 --> 00:52:10,120
data augmentation helps that actually has nothing to do with the data augmentation just basically

564
00:52:10,120 --> 00:52:16,680
randomization of your data, okay? But the point is that if you're using self supervised learning

565
00:52:16,680 --> 00:52:21,400
like the whole point of having a fixed training set was because it was expensive to do all these

566
00:52:21,400 --> 00:52:27,960
labels, you know? ImageNet, poor Fei Fei spent all of her startup money in Stanford labeling this

567
00:52:27,960 --> 00:52:33,720
huge data set, right? So it kind of makes sense that it's it's fixed because it took so much money

568
00:52:33,720 --> 00:52:39,000
to label it. But if you're using self supervised learning if you don't need the labels what's the

569
00:52:39,000 --> 00:52:44,680
point of having a fixed data set? Why can't we just keep downloading images from whatever the

570
00:52:44,680 --> 00:52:50,920
internet the TV whatever and just keep doing it all the time because we can generate our own

571
00:52:50,920 --> 00:52:58,120
labels. Seems kind of natural and so this is where kind of I've been pushing on this idea of kind of

572
00:52:58,120 --> 00:53:03,320
this online continual learning. So you can you can think of it in terms of of the standard

573
00:53:03,880 --> 00:53:08,120
a train valve separation. So you know you have your training set and kind of the standard thing

574
00:53:08,120 --> 00:53:12,520
in machine learning is you can separate it into a training set and a validation set, right?

575
00:53:12,520 --> 00:53:18,520
And we know that if you just train on a training set and then use the validation set to tune your

576
00:53:18,520 --> 00:53:24,760
high parameters you usually get better performance on the eventual test data than if you just train

577
00:53:24,760 --> 00:53:30,360
on all the training set all at once. Even though you're kind of you you think that it's less data

578
00:53:30,360 --> 00:53:35,400
that you're using for training but actually this is effect turns out to be more effective. Well we

579
00:53:35,400 --> 00:53:41,400
can think of the same thing in a continual way. So we can think of it as you train on the data

580
00:53:41,400 --> 00:53:47,400
that you have seen and then you're validating on the next data that comes along, okay? And then

581
00:53:47,400 --> 00:53:51,960
once you do that you just incorporate it into your training center you can keep going and they

582
00:53:51,960 --> 00:53:57,000
can keep going on forever. You don't ever need to stop, okay? And this I think is a kind of a very

583
00:53:57,000 --> 00:54:05,800
powerful trick that is made that we can now do because we can use self-supervision to do this

584
00:54:05,800 --> 00:54:11,160
kind of this evaluation, this testing, okay? And so this is the idea of test time training which is

585
00:54:11,160 --> 00:54:17,320
our attempt to operationalize this on an infinite smoothly changing stream and the idea is to basically

586
00:54:17,640 --> 00:54:27,000
use self-supervision to continuously adapt to new data, okay? And we did this already in the

587
00:54:28,440 --> 00:54:36,040
case of reinforcement learning with our curiosity work and this new work is basically trying to do

588
00:54:36,040 --> 00:54:46,360
it for images and this is the paper, test time training and it was in ICML 2000. Maybe

589
00:54:46,920 --> 00:54:53,000
just give me, I'll give you one slide of intuition of what we're doing. Basically,

590
00:54:53,000 --> 00:55:01,880
the idea is that we're, let's say we have a training set of object detection, right? And at

591
00:55:01,880 --> 00:55:08,680
training time we have our standard thing, we have our image and we have our label so nothing new

592
00:55:08,680 --> 00:55:18,360
here, we have input in, label out, we are training and then at the same time we also have a self-supervised

593
00:55:18,360 --> 00:55:25,080
head that basically given your image it does some self-supervised task. In this case we are basically

594
00:55:25,080 --> 00:55:30,760
our task here is rotation prediction. Given the rotated version of the image we want to predict

595
00:55:30,760 --> 00:55:34,920
which rotation it is. It doesn't really matter, it could be any task at all, okay? So at training

596
00:55:34,920 --> 00:55:41,480
time we do both of those tasks together but then at test time of course we don't have the labels

597
00:55:41,480 --> 00:55:48,760
but we still have this task, okay? And so we can basically around this, we can evaluate this task

598
00:55:49,400 --> 00:55:58,120
and if the result is not good, if it failed this self-supervised task we can do a little bit of

599
00:55:58,120 --> 00:56:04,680
fine-tuning, a little bit of fine-tuning training for this other task but as we're doing the fine

600
00:56:04,680 --> 00:56:12,760
tuning it's going to get changed representation in a way that will also impact the real task that

601
00:56:12,760 --> 00:56:21,880
we care about and that allows us to do better as we are changing, as we're going for the dataset.

602
00:56:21,880 --> 00:56:29,480
And so here is an example where you know given this image at test time basically the right label is

603
00:56:29,480 --> 00:56:36,760
elephant but initially it basically thinks it's a dog but then as we do this fine-tuning on our

604
00:56:39,240 --> 00:56:45,160
self-supervised task it figures out that it's actually less of a dog and more of an elephant

605
00:56:45,160 --> 00:56:50,120
and gives us the right answer and that's basically the story of the paper, sorry I had to rush but

606
00:56:50,680 --> 00:56:59,000
you can look at the paper online. And to conclude, why use self-supervision?

607
00:57:01,320 --> 00:57:07,400
One reason that I like is that it allows us to get away from this top-down semantic categorization

608
00:57:07,400 --> 00:57:14,760
and gets us more into this bottom-up association story and learn things from the bottom-up

609
00:57:14,760 --> 00:57:21,240
but we must be careful that the supervision doesn't leak in through things like data augmentation

610
00:57:21,240 --> 00:57:27,720
right and we need to be careful about this and second is that eventually self-supervision should

611
00:57:27,720 --> 00:57:34,200
enable us to check the datasets, forget about all these fixed datasets and and learn things continuously

612
00:57:35,080 --> 00:57:40,360
and it's you know we're still we're only starting on this direction I think it's very exciting direction

613
00:57:40,440 --> 00:57:46,120
very exciting problem so I'm hoping people will get excited about it okay thank you very much

614
00:57:47,640 --> 00:57:50,920
yeah awesome fantastic talk thanks a lot for all the amazing works

615
00:57:52,120 --> 00:57:57,240
yeah self-supervision is cool and does anybody have some some some question on Zoom maybe let's

616
00:57:57,240 --> 00:58:00,920
start with this we have a lot of questions on YouTube but I'm going to start to assume

617
00:58:00,920 --> 00:58:05,800
I have a lot of questions too but if somebody wants to ask question on Zoom just turn on your video

618
00:58:05,800 --> 00:58:07,720
and and just pick up probably

619
00:58:13,720 --> 00:58:20,600
I can start with one with a maybe a higher level question first so I mean the challenge

620
00:58:20,600 --> 00:58:25,480
in self-supervision is right you basically have visual data on you let's say correlate patches

621
00:58:25,480 --> 00:58:30,600
with whatever contrastive loss or whatever whatever people do now um I mean what do you

622
00:58:30,600 --> 00:58:34,360
think about if you're thinking about the 3d world right you have obviously a third dimension

623
00:58:34,920 --> 00:58:39,560
is it a smart idea to do this actually all on on images and videos and not think about

624
00:58:40,280 --> 00:58:46,680
I don't know like kind of project a 3d representation maybe first and then think about how to kind of

625
00:58:46,680 --> 00:58:52,200
get similarities in some 3d space learn a 3d representation and then you know try to

626
00:58:52,200 --> 00:58:59,000
channelize with the onscreen tasks later on right right no this is absolutely and and as you know

627
00:58:59,000 --> 00:59:05,560
you know I've been I've been angling for for going into 3d you know since since since a long

628
00:59:05,560 --> 00:59:11,400
time ago since our work with Derry Coyne on qualitative 3d I'm a I'm a big fan of 3d in

629
00:59:11,400 --> 00:59:17,560
my heart and it's kind of a little bit sad that once you know once we went to neural networks

630
00:59:17,560 --> 00:59:22,840
the kind of things dropped back to 2d plane for a while and now of course they're they're coming

631
00:59:22,840 --> 00:59:31,000
back again um okay so there's there's two answers to this question one the final you know the the

632
00:59:31,000 --> 00:59:42,120
ultimate answer is that 3d should emerge from our 2d of observation that the representation

633
00:59:42,120 --> 00:59:52,680
should figure out 3d on its own okay uh just like it's done with humans right humans are only seeing

634
00:59:53,480 --> 00:59:59,320
2d projections of the 3d world okay if you have stereo maybe you have a little bit of 3d but

635
00:59:59,320 --> 01:00:03,640
you know I don't have stereo for example 10 percent of people in the world don't have stereo

636
01:00:03,640 --> 01:00:11,320
and we are perfectly fine seeing 3d okay so we learn 3d from uh from from a series of 2d

637
01:00:11,320 --> 01:00:18,840
representations uh and I think if we if we go from you know collections of images like ImageNet

638
01:00:18,840 --> 01:00:28,840
to videos for example hopefully and I'm very hoping that like it will encourage 3d to automatically

639
01:00:28,840 --> 01:00:37,960
emerge as as the you know inside of the representation okay so that's kind of a the the the the the

640
01:00:37,960 --> 01:00:44,520
glorious answer at the end of the rainbow okay uh but of course this is this is very hard this is

641
01:00:44,520 --> 01:00:50,040
kind of a a very tall order uh you know we are seeing a little bit of this happening we are seeing

642
01:00:50,040 --> 01:00:55,880
a little bit of kind of a maybe two and a half two or two point one d kind of occlusion occlusion

643
01:00:55,880 --> 01:01:03,720
reasoning you know figure ground reasoning uh a little bit of of that but but but it we're

644
01:01:03,720 --> 01:01:09,640
they're definitely far away from that right and so the second direction is okay can we kind of help

645
01:01:09,640 --> 01:01:18,760
it out a little bit how can we can we provide features that are more amenable to to to three

646
01:01:18,760 --> 01:01:27,240
dimensional manipulation and there I think uh things like like like holo-gan or or pie-gan this

647
01:01:27,240 --> 01:01:34,520
kind of directions are I think very exciting in in that it's kind of you you can inject some things

648
01:01:34,520 --> 01:01:41,640
that you know are physically true like rotation for example and and uh and so I think I think in the

649
01:01:41,640 --> 01:01:49,080
short in the short uh uh uh short term all of those things are I think going to be extremely

650
01:01:49,080 --> 01:01:55,640
helpful in the long term I'm still kind of hoping that I can learn 3d from scratch okay but who knows

651
01:01:55,640 --> 01:02:01,160
maybe it's too much to ask but I'm still kind of hoping that one day I will wake up in the morning

652
01:02:01,160 --> 01:02:07,560
and boom my computer learned 3d but we'll see do it with two cameras right we have stereo that's I

653
01:02:07,560 --> 01:02:15,560
mean that's the thing but I don't have stereo for example right like 10 percent of people don't

654
01:02:15,560 --> 01:02:25,080
have stereo stereo is actually not as important as as as as we we we we think stereo is only really

655
01:02:25,960 --> 01:02:30,440
important for like the you know half a meter in front of you it's like it's you know what

656
01:02:30,440 --> 01:02:35,480
what is it that I cannot do that everybody else can do okay I cannot put you know thread through

657
01:02:35,480 --> 01:02:41,320
the needle and I have trouble you know pouring wine right other than that I'm fine so really it

658
01:02:41,320 --> 01:02:47,560
stereo is kind of over overemphasized I think it's really parallax is much more important

659
01:02:47,560 --> 01:02:53,720
and parallax you can get from from video no good point I have another follow-up question

660
01:02:53,720 --> 01:02:59,560
um so in the similar spirit right like one argument is you can do contrastive learning

661
01:02:59,560 --> 01:03:03,720
and mostly it's about comparing things right you're saying one versus all classifiers like

662
01:03:03,720 --> 01:03:08,440
how similar are these things I mean what about going back to the original things when people

663
01:03:08,440 --> 01:03:12,760
using like auto encoders for pre-training and so on for like basically using generative tasks

664
01:03:12,760 --> 01:03:18,760
let's say oh I train my favorite game how good of a representation can I learn from learning

665
01:03:18,760 --> 01:03:23,800
the distribution basically right like how well like it's like this famous thing like you have to

666
01:03:23,800 --> 01:03:29,640
be able to create in order to understand and where to see that competing or maybe going

667
01:03:29,640 --> 01:03:33,160
going along the same line so what's your take channel is speaking on the lines there

668
01:03:35,080 --> 01:03:42,360
um I mean yeah I mean yeah we have definitely been also using out encoders as well I think

669
01:03:42,360 --> 01:03:51,640
with an outer encoder it's a little bit of a it's a little bit of a of a magic box like if you

670
01:03:51,640 --> 01:03:56,920
know if you get really lucky your outer encoder is going to capture exactly the right things

671
01:03:56,920 --> 01:04:00,840
and if you get unlikely it will capture all exactly the wrong things right it's it's kind of

672
01:04:00,840 --> 01:04:07,720
it's a compression mechanism it somehow compresses your data and and it it really depends on what you

673
01:04:07,720 --> 01:04:13,880
care about like sometimes it will compress the away the stuff that you care about or sometimes it

674
01:04:13,880 --> 01:04:19,400
will retain the stuff that you care about and it's it's a little bit hard to control what it's going

675
01:04:19,400 --> 01:04:28,200
to do so I think I think this kind of a similarity learning is it allows you to get a little bit

676
01:04:28,200 --> 01:04:34,520
more control and a little bit of more of kind of a intuition about what is it what is it being

677
01:04:35,160 --> 01:04:39,240
what is it that's being learned it's also kind of a has a very nice connection to kind of to

678
01:04:39,240 --> 01:04:43,800
graphs and graph theory that kind of think you you think about it like you have different

679
01:04:43,800 --> 01:04:48,360
entities and then you have kind of you can think of it like as a as like an as a like a network

680
01:04:48,360 --> 01:04:55,000
right like a like a uh a social network for example where you you can think of different

681
01:04:55,000 --> 01:04:59,640
people being connected in different ways and you can think about yeah we call them

682
01:04:59,640 --> 01:05:06,040
senses of similarity so there's many different senses of similarity between two instances and

683
01:05:06,040 --> 01:05:09,960
you know something like an out encoder is probably going to collapse them all together and here you

684
01:05:09,960 --> 01:05:14,440
can actually separate them you can have a similarity in color similarity in texture maybe

685
01:05:14,440 --> 01:05:21,720
similarity in 3d and they're all can be kind of exposed hopefully separately now that's interesting

686
01:05:21,720 --> 01:05:26,200
I mean our experiences so we've done a lot of stuff on like shape completion in 3d so whenever we

687
01:05:26,200 --> 01:05:31,000
had the ability to take stuff away and predict it then we got great features this was always amazing

688
01:05:31,000 --> 01:05:36,280
in terms of using these features to help semantics and whenever we're trying to classify it to help

689
01:05:36,280 --> 01:05:40,760
the completion this is this is always a total disaster it never worked we tried really hard

690
01:05:40,760 --> 01:05:48,360
actually I mean I think that that's that's been our our experience as well but but I think have

691
01:05:48,360 --> 01:05:55,160
you have you tried the latest uh contrastive learning because it's really it's to me the way

692
01:05:55,160 --> 01:06:02,040
I think about contrastive learning is it's really just old school triplet loss you know uh Siamese

693
01:06:02,040 --> 01:06:10,920
network learning except you're switching from from from from kind of a regression to to a

694
01:06:10,920 --> 01:06:17,480
classification but it's a classification with like huge amounts of data and it's very very fine

695
01:06:17,480 --> 01:06:23,720
grain classification so it's almost it's it's really not like your your grandma's classification

696
01:06:23,800 --> 01:06:30,200
it's uh I mean we we did something like this for for uh for when we did colorization so we we first

697
01:06:30,200 --> 01:06:35,960
we tried to do colorization with the regression and then we we we we got better results by doing

698
01:06:35,960 --> 01:06:42,920
classification but the classification was across like you know 500 classes of different colors

699
01:06:42,920 --> 01:06:49,480
in the in the in the color gamut right so it's it's a much more kind of narrow thing and that

700
01:06:49,480 --> 01:06:54,280
seemed to work for us but yeah like if you have a few classes then then then it's very hard to

701
01:06:54,280 --> 01:06:59,240
make it work but if you do something like either have lots of classes or do something like like

702
01:06:59,240 --> 01:07:05,080
contrastive learning where it's basically just really kind of push it with data yet it seems to

703
01:07:05,080 --> 01:07:09,240
to work for us now we've actually tried that so we had we had one one student project actually

704
01:07:09,240 --> 01:07:12,920
in collaboration with fair so Chihu one of my students they've been working on basically

705
01:07:14,680 --> 01:07:18,040
basically doing contrastive learning for pre-training 3d structures and in a similar

706
01:07:18,040 --> 01:07:25,000
way than you would do it in 2d it does help but the completion still seems to work a bit better

707
01:07:25,880 --> 01:07:33,960
it's very interesting I think so in general yeah I mean I think if if completions if if actually

708
01:07:33,960 --> 01:07:42,200
predicting you know pixels or predicting voxels whatever um it it has it has more data it has

709
01:07:42,200 --> 01:07:51,240
more information that and and we know that 3d world is actually much you know it's much more

710
01:07:51,240 --> 01:07:58,520
informative right so and it's also I think much more um uni model so the one thing that was hard

711
01:07:58,520 --> 01:08:03,000
for us for example when we did core colorization is what we're you know we're trying to colorize a

712
01:08:03,000 --> 01:08:08,840
bird and the birds could be yellow or the bird could be green right and so you have multi model

713
01:08:08,840 --> 01:08:14,120
you have two modes and if you're doing kind of a just sort of like a regression completion

714
01:08:14,120 --> 01:08:18,760
what's it going to do it's going to do the average right so it's going to be neither here nor there

715
01:08:18,760 --> 01:08:25,320
right but if you have a single mode it works really well so it might be that in 3d you're really

716
01:08:25,320 --> 01:08:32,120
in a world that's much more uni model in which you don't have like you're not trying to have an

717
01:08:32,120 --> 01:08:36,440
average between two different completions and they get something that doesn't look like either

718
01:08:36,440 --> 01:08:41,880
but you're actually really focusing on a single mode so in that case maybe this is why you're

719
01:08:41,880 --> 01:08:48,360
getting better results but if I suspect that if you had multi model like if you have a hole that's

720
01:08:48,360 --> 01:08:55,800
big enough that you could have many different plausible completions happen to it I suspect that

721
01:08:55,800 --> 01:09:01,880
that that then you're kind of uh the kind of the the prediction route might have more problems

722
01:09:01,880 --> 01:09:07,160
no I fully agree with you no that one is definitely true and and but most of the case right you're

723
01:09:07,160 --> 01:09:11,560
thinking about it's more like a dropout in a sense right so you're leaving out some stuff right and

724
01:09:11,560 --> 01:09:14,600
then you're trying to figure out what's missing in this case I think we've experienced that it

725
01:09:14,600 --> 01:09:18,760
works remarkably well if it's too large then you need probabilistic models again and stuff like

726
01:09:18,760 --> 01:09:24,040
that then it's a lot more difficult um yeah I agree yeah I think I think it's kind of a

727
01:09:24,040 --> 01:09:29,880
if it's a level of dropout and it should just work yeah I I I agree yeah yeah I think I think

728
01:09:29,880 --> 01:09:34,920
if it works you should definitely use it absolutely um actually any other questions maybe

729
01:09:34,920 --> 01:09:38,200
maybe somebody else can ask questions I don't want to dominate the discussion too much

730
01:09:42,040 --> 01:09:50,760
hi yeah thanks for the talk um I have a similarly high level question so speaking along the lines of

731
01:09:50,760 --> 01:09:55,960
like multi modality and and stuff like this it seems like you have a lot of inspiration in terms of

732
01:09:56,920 --> 01:10:05,080
how to learn perception based on how people perform perception and it seems like people do have

733
01:10:05,080 --> 01:10:11,400
naturally some kind of estimate of uncertainty multi modality and the ability to generate

734
01:10:11,400 --> 01:10:15,480
also for like these video tracking kind of applications that you showed like multiple

735
01:10:15,480 --> 01:10:23,560
hypotheses for where um the prediction should go and how far do you think you can get without this

736
01:10:23,640 --> 01:10:26,760
explicitly modeled or do you think it needs to be explicitly modeled

737
01:10:29,000 --> 01:10:39,000
good question I think I think I would go with I don't know so um yes humans are very good at

738
01:10:39,000 --> 01:10:45,800
at modeling uncertainty but they I don't think they're doing it in the way that was the decisions

739
01:10:45,800 --> 01:10:53,320
to it I don't think humans are actually probabilistic I think I think they might be doing it

740
01:10:54,040 --> 01:10:58,840
almost like if you remember from a long time ago like all this particle filtering where you

741
01:10:58,840 --> 01:11:03,720
can keep a whole bunch of hypotheses and then you kind of keep all of them going for a while and then

742
01:11:03,720 --> 01:11:09,800
you kind of uh drop one like you know there's illusion of like young lady old woman visual

743
01:11:09,800 --> 01:11:15,480
illusion where you know one day once time you see like an old lady one time you see a young woman

744
01:11:15,480 --> 01:11:22,680
right and you never see both of them so it seems some there's some very interesting mechanism going

745
01:11:22,760 --> 01:11:32,200
on but I think it's not it's not like a standard probabilistic mechanism and so yeah so I don't

746
01:11:32,200 --> 01:11:38,840
know how to deal with it and in the in the vision in the in this video paper that I showed you know

747
01:11:38,840 --> 01:11:44,520
we are really just keeping a whole bunch of hypotheses as we're going through the through

748
01:11:44,520 --> 01:11:52,840
the video at training time at test time we don't and but whether that's the right thing to do or

749
01:11:52,840 --> 01:11:58,360
not I don't know I think it's a very important question I don't I don't have an answer but

750
01:11:58,360 --> 01:12:07,960
frankly I think that nobody else does either sounds good I think one other somewhat unrelated

751
01:12:07,960 --> 01:12:13,400
thing I think there's a bit of a tension between people who think that we should be able to learn

752
01:12:13,400 --> 01:12:18,920
everything from scratch like you mentioned in terms of being able to learn 3d and whether this

753
01:12:18,920 --> 01:12:24,920
actually possible because it's unclear I guess how many how much supervision certainly for

754
01:12:24,920 --> 01:12:32,120
like some rantic perception people get direct supervision and so yeah okay now we're back to

755
01:12:32,200 --> 01:12:39,400
philosophy I think it must be possible because because it already happened right

756
01:12:42,040 --> 01:12:49,640
supervised learning is something that happens in nature but it's it's very very rare like like

757
01:12:50,520 --> 01:12:56,600
parents teaching their children things I know that a lot of modern parents they feel like it's

758
01:12:56,600 --> 01:13:02,120
super super important but sorry you know a developmental psychologist disagree they say

759
01:13:02,120 --> 01:13:08,120
that it doesn't really matter that much most of the things that a kid picks up they pick up

760
01:13:08,120 --> 01:13:15,160
without supervision they keep pick up on their own and and you could think about it you know

761
01:13:16,440 --> 01:13:20,600
from in the very beginning right in the beginning if you're you know as long as you believe in

762
01:13:20,600 --> 01:13:25,640
evolution you must believe in in unsupervised or self-supervised learning because in the beginning

763
01:13:25,640 --> 01:13:30,200
there was nothing there was no there was no teacher there was no supervision there was only

764
01:13:30,200 --> 01:13:35,720
data right and kind of the organism and its environment were co-involving and learning

765
01:13:35,720 --> 01:13:43,960
from each other and and and and and and develop so I think there is there is to me there is no

766
01:13:43,960 --> 01:13:51,800
question that it should be possible in theory I think that the kind of the the the interesting

767
01:13:51,880 --> 01:13:57,080
question is is it does it make sense to do in practice right and like you could also say well

768
01:13:57,080 --> 01:14:02,760
why don't we just simulate evolution for for a gazillion years and then we'll get everything

769
01:14:02,760 --> 01:14:08,120
right and that's of course not feasible with the current technology so I don't think that there is

770
01:14:08,120 --> 01:14:13,400
that much or maybe there shouldn't be that much tension because I think I think there are people

771
01:14:13,400 --> 01:14:18,040
like me who really want to try to learn things from first principles and I think this is very

772
01:14:18,040 --> 01:14:23,720
interesting if if if anything from you know from the biological plausibility point of view

773
01:14:23,720 --> 01:14:29,880
okay and there are people who just want to get stuff done and and and get to a good result too

774
01:14:29,880 --> 01:14:35,880
fast and those people should definitely just use whatever works best at the time so I'm not sure

775
01:14:35,880 --> 01:14:41,640
that it's either or I think both directions are are useful and I think we're learning from each

776
01:14:41,640 --> 01:14:46,200
other I think those two direction directions are informing each other so for example for a very

777
01:14:46,200 --> 01:14:53,640
long time self-supervised approaches worked worse than supervised approaches and so you know

778
01:14:54,360 --> 01:14:58,600
if you know we could have all quit because oh my god you know our stuff doesn't work as well as

779
01:14:58,600 --> 01:15:03,320
supervision but we persevered because we thought that you know there's something interesting that

780
01:15:03,320 --> 01:15:09,960
that we could learn anyway and now what we are seeing is that for some tasks self-supervision

781
01:15:09,960 --> 01:15:14,840
actually works better than supervised learning not for all not for many but for some there's

782
01:15:14,840 --> 01:15:21,880
definitely some cases when it actually the the the learning from the data actually gives you

783
01:15:21,880 --> 01:15:27,880
better results than learning from uh from from from from labels and so I think I think I think

784
01:15:27,880 --> 01:15:34,520
you know let all the flowers bloom it I think both directions are useful and I think it's

785
01:15:34,520 --> 01:15:41,640
it's great that people are are pushing in in in in both of them and I think we'll we'll get to a

786
01:15:41,720 --> 01:15:48,040
better point eventually and we'll learn more so I'm actually optimistic on on on all fronts it's

787
01:15:48,040 --> 01:15:53,240
yeah it's not a competition well it is a competition but it's not like it's not one is right another

788
01:15:53,240 --> 01:16:00,280
is wrong I think both are right cool all right I think that's a that's a very good um I guess

789
01:16:00,280 --> 01:16:04,280
ending of the live stream I think thanks a really lot for the amazing talk um we're a little bit

790
01:16:04,280 --> 01:16:09,000
over I have to apologize to a lot of questions on YouTube we couldn't unfortunately go into all of

791
01:16:09,000 --> 01:16:15,480
them um but um it was really great to have you and um I hope also for everybody who is with

792
01:16:15,480 --> 01:16:21,080
with here right now next week we'll have another great lecture with rock help and yeah we'll see

793
01:16:21,080 --> 01:16:24,200
so thanks a lot again for the for the great year I need great research

794
01:16:28,680 --> 01:16:29,800
okay um

