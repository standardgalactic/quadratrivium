start	end	text
0	8000	Hi, I am Jeremy Howard from fast.ai, and this is a Hackers Guide to Language Models.
8000	19000	When I say a Hackers Guide, what we're going to be looking at is a code-first approach to understanding how to use language models in practice.
19000	24000	So before we get started, we should probably talk about what is a language model.
25000	34000	I would say that this is going to make more sense if you know the kind of basics of deep learning.
34000	40000	If you don't, I think you'll still get plenty out of it, and there'll be plenty of things you can do.
40000	47000	But if you do have a chance, I would recommend checking out course.fast.ai, which is a free course.
47000	57000	And specifically, if you could at least kind of watch, if not work through the first five lessons,
57000	69000	that would get you to a point where you understand all the basic fundamentals of deep learning that will make this lesson tutorial make even more sense.
69000	73000	Maybe I shouldn't call this a tutorial, it's more of a quick run-through.
73000	82000	So I'm going to try to run through all the basic ideas of language models, how to use them, both open source ones and open AI based ones.
82000	87000	And it's all going to be based using code as much as possible.
87000	92000	So let's start by talking about what a language model is.
92000	101000	And so as you might have heard before, a language model is something that knows how to predict the next word of a sentence or knows how to fill in the missing words of a sentence.
101000	108000	And we can look at an example of one. Open AI has a language model, text of enchi 003.
108000	116000	And we can play with it by passing in some words and ask it to predict what the next words might be.
116000	125000	So if we pass in, when I arrived back at the Panda breeding facility after the extraordinary reign of live frogs, I couldn't believe what I saw.
125000	129000	Or I just came up with that yesterday and I thought what might happen next.
129000	132000	So kind of fun for creative brainstorming.
132000	135000	There's a nice site called nat.dev.
135000	139000	Nat.dev lets us play with a variety of language models.
139000	144000	And here I've selected text of enchi 003 and I'll hit submit.
144000	147000	And it starts printing stuff out.
147000	151000	The pandas were happily playing and eating the frogs that had fallen in the sky.
151000	156000	There's an amazing site to see these animals taking advantage of such a unique opportunity.
156000	160000	The staff took quick measures to ensure the safety of the pandas and the frogs.
160000	165000	So there you go. That's what happened after the extraordinary reign of live frogs at the Panda breeding facility.
165000	172000	You'll see here that I've enabled show probabilities, which is a thing in nat.dev where it shows.
172000	174000	Well, let's take a look.
174000	177000	It's pretty likely the next word here is going to be there.
177000	182000	And after that, since we're talking about a panda breeding facility, it's going to be pandas were.
182000	183000	And what were they doing?
183000	185000	Well, they could have been doing a few things.
185000	190000	They could have been doing something happily or the pandas were having the pandas were out.
190000	191000	The pandas were playing.
191000	194000	So it picked the most likely.
194000	197000	It thought it was 20% likely it's going to be happily.
197000	199000	And what were they happily doing?
199000	206000	Could have been playing, hopping, eating and so forth.
206000	211000	So they're eating the frogs that and then had almost certainly.
211000	217000	So you can see what it's doing at each point is it's predicting the probability of a variety of possible next words.
217000	225000	And depending on how you set it up, it will either pick the most likely one every time or you can change,
225000	233000	muck around with things like p values and temperatures to change what comes up.
233000	239000	So at each time, then it will give us a different result.
239000	243000	And this is kind of fun.
243000	245000	Frogs perched on the heads of some of the pandas.
245000	249000	It was an amazing sight, et cetera, et cetera.
249000	250000	Okay.
250000	258000	So that's what a language model does.
258000	263000	Now you might notice here it hasn't predicted pandas.
263000	270000	It's predicted panned and then separately us.
270000	271000	Okay.
271000	272000	After panned, it's going to be us.
272000	274000	So it's not always a whole word.
274000	278000	Here it's un and then harmed.
278000	281000	Oh, actually it's un ha mood.
281000	284000	So you can see that it's not always predicting words,
284000	287000	specifically what it's doing is predicting tokens.
287000	294000	Tokens are either whole words or sub word units, pieces of a word,
294000	300000	or it could even be punctuation or numbers or so forth.
300000	302000	So let's have a look at how that works.
302000	310000	So for example, we can use the actual, it's called tokenization to create tokens from a string.
310000	315000	We can use the same tokenizer that GPT users by using tick token.
315000	322000	And we can specifically say we want to use the same tokenizer that that model text eventually 003 users.
322000	328000	And so, for example, when I earlier tried this, it talked about the frog splashing.
328000	332000	And so I thought, well, we'll encode they are splashing.
332000	335000	And the result is a bunch of numbers.
335000	342000	And what those numbers are, they basically just look ups into a vocabulary that open AI in this case created.
342000	346000	And if you train your own models, you'll be automatically creating or your code will create.
346000	357000	And if I then decode those, it says, oh, these numbers are they space are space spool ashing.
357000	359000	And so put that all together.
359000	360000	They are splashing.
360000	372000	So you can see that the start of a word is the space before it is also being encoded here.
372000	380000	So these language models are quite neat that they can work at all.
380000	388000	But they're not of themselves really designed to do anything.
388000	390000	Let me explain.
390000	407000	The basic idea of what chat GPT, GPT4, etc. are doing comes from a paper which describes an algorithm that I created back in 2017 called ULM fit.
407000	416000	And Sebastian Ruder and I wrote a paper up describing the ULM fit approach, which was the one that basically laid out what everybody's doing, how this system works.
416000	419000	And the system has three steps.
419000	423000	Step one is language model training.
423000	425000	But you'll see this is actually from the paper.
425000	427000	We actually described it as pre-training.
427000	434000	Now what language model pre-training does is this is the thing which predicts the next word of a sentence.
434000	444000	And so in the original ULM fit paper, so the algorithm I developed in 2017, then Sebastian Ruder and I wrote it up in 2018, early 2018.
444000	449000	What I originally did was I trained this language model on Wikipedia.
449000	456000	Now what that meant is I took a neural network and a neural network is just a function.
456000	461000	If you don't know what it is, it's just a mathematical function that's extremely flexible and it's got lots and lots of parameters.
461000	470000	And initially it can't do anything, but using stochastic gradient descent or SGD, you can teach it to do almost anything if you give it examples.
470000	477000	And so I gave it lots of examples of sentences from Wikipedia, so for example from the Wikipedia article for the birds.
477000	486000	The birds is a 1963 American natural horror thriller film produced and directed by Alfred, and then it would stop.
486000	490000	And so then the model would have to guess what the next word is.
490000	494000	And if it guessed Hitchcock, it would be rewarded.
494000	497000	And if it guessed something else, it would be penalized.
497000	500000	And effectively, basically it's trying to maximize those rewards.
500000	506000	It's trying to find a set of weights for this function that makes it more likely that it would predict Hitchcock.
506000	518000	And then later on in this article, it reads, from Wikipedia, any previously dated Mitch but ended it due to Mitch's cold, overbearing mother Lydia, who dislikes any woman in Mitch's.
519000	528000	They can see that filling this in actually requires being pretty thoughtful because there's a bunch of things that kind of logically could go there.
528000	537000	Like a woman could be in Mitch's closet, could be in Mitch's house.
537000	545000	And so you could probably guess in the Wikipedia article describing the plot of the birds as actually any woman in Mitch's life.
545000	555000	Now, to do a good job of solving this problem as well as possible of guessing the next word of sentences,
555000	563000	the neural network is going to have to learn a lot of stuff about the world.
563000	573000	It's going to learn that there are things called objects, that there's a thing called time, that objects react to each other over time,
573000	579000	that there are things called movies, that movies have directors, that there are people, that people have names and so forth,
579000	587000	and that a movie director is Alfred Hitchcock and he directed horror films and so on and so forth.
587000	594000	It's going to have to learn an extraordinary amount if it's going to do a really good job of predicting the next word of sentences.
594000	599000	Now, these neural networks specifically are deep neural networks.
599000	600000	So this is deep learning.
600000	607000	And in these deep neural networks, which have, when I created this, I think it had like 100 million parameters.
607000	613000	Nowadays, they have billions of parameters.
613000	623000	It's got the ability to create a rich hierarchy of abstractions and representations which it can build on.
623000	631000	And so this is really the key idea behind neural networks and language models,
631000	638000	is that if it's going to do a good job of being able to predict the next word of any sentence in any situation,
638000	640000	it's going to have to know an awful lot about the world.
640000	652000	It's going to have to know about how to solve math questions or figure out the next move in a chess game or recognize poetry and so on and so forth.
652000	656000	Now, nobody says it's going to do a good job of that.
656000	662000	So it's a lot of work to find to create and train a model that is good at that.
662000	672000	But if you can create one that's good at that, it's going to have a lot of capabilities internally that it would have to be drawing on to be able to do this effectively.
672000	681000	So the key idea here for me is that this is a form of compression and this idea of the relationship between compression
681000	685000	and intelligence goes back many, many decades.
685000	693000	And the basic idea is that, yeah, if you can guess what words are coming up next,
693000	699000	then effectively you're compressing all that information down into a neural network.
699000	703000	Now, I said this is not useful of itself.
703000	705000	Well, why do we do it?
705000	709000	Well, we do it because we want to pull out those capabilities.
709000	713000	And the way we pull out those capabilities is we take two more steps.
713000	717000	The second step is we do something called language model fine tuning.
717000	726000	And in language model fine tuning, we are no longer just giving it all of Wikipedia or nowadays we don't just give it all of Wikipedia.
726000	732000	But in fact, a large chunk of the internet is fed to pre-training these models.
732000	742000	In the fine tuning stage, we feed it a set of documents a lot closer to the final task that we want the model to do.
742000	744000	But it's still the same basic idea.
744000	749000	It's still trying to predict the next word of a sentence.
749000	753000	After that, we then do a final classifier fine tuning.
753000	759000	And then the classifier fine tuning, this is the kind of end task we're trying to get it to do.
759000	765000	Nowadays, these two steps are very specific approaches are taken.
765000	772000	For the step two, the step B, the language model fine tuning, people nowadays do a particular kind called instruction tuning.
772000	780000	The idea is that the task we want most of the time to achieve is solve problems, answer questions.
780000	786000	And so in the instruction tuning phase, we use data sets like this one.
786000	792000	This is a great data set called OpenOrca created by a fantastic open source group.
792000	798000	And it's built on top of something called the Flan collection.
798000	804000	And you can see that basically there's all kinds of different questions in here.
804000	811000	So there's four gigabytes of questions and context and so forth.
811000	820000	And each one generally has a question or an instruction or a request and then a response.
820000	824000	Here are some examples of instructions.
824000	827000	I think this is from the Flan data set if I remember correctly.
827000	832000	So for instance, it could be does the sentence in the iron age answer the question,
832000	839000	the period of time from 1200 to 1000 BCE is known as what choice is one yes or no.
839000	847000	And then the language model is meant to write one or two as appropriate for yes or no.
847000	850000	Or it could be things about I think this is from a music video.
850000	857000	Who is the girl in more than you know answer and then it would have to write the correct name of the
857000	863000	remember model or dancer or whatever from from that music video and so forth.
863000	866000	So it's still doing language modeling.
866000	869000	So fine tuning and pre training are kind of the same thing.
869000	878000	But this is more targeted now, not just to be able to fill in the missing parts of any document from the Internet.
878000	886000	But to fill in the words necessary to answer questions to do useful things.
886000	889000	Okay, so that's instruction tuning.
889000	893000	And then step three, which is the classifier fine tuning.
893000	899000	Nowadays, there's generally various approaches such as reinforcement learning from human feedback and others,
899000	909000	which are basically giving humans or sometimes more advanced models,
909000	916000	multiple answers to a question such as here are some from a reinforcement learning from human feedback paper.
916000	918000	I can't remember which one I got it from.
918000	921000	Five ideas for how to regain enthusiasm for my career.
921000	928000	And so the model will spit out two possible answers or it will have a less good model and more good model.
928000	933000	And then a human or a better model will pick, which is best.
933000	937000	And so that's used for the the final fine tuning stage.
937000	947000	So all of that is to say, although you can download pure language models from the Internet.
948000	954000	They're not generally that useful of their on their own until you've fine tuned them.
954000	957000	Now, you don't necessarily need step C nowadays.
957000	960000	Actually, people are discovering that maybe just step B might be enough.
960000	962000	It's still a bit controversial.
962000	971000	Okay, so when we talk about a language model, where we could be talking about something that's just been pre-trained,
971000	975000	something that's been fine tuned or something that's gone through something like RLHF.
975000	980000	All of those things are generally described nowadays as language models.
983000	990000	So my view is that if you are going to be good at language modeling in any way,
990000	995000	then you need to start by being a really effective user of language models.
995000	999000	And to be a really effective user of language models, you've got to use the best one that there is.
999000	1004000	And currently, so what are we up to September 2023?
1004000	1008000	The best one is by far GPT-4.
1008000	1013000	This might change sometime in the not too distant future, but this is right now.
1013000	1016000	GPT-4 is the recommendation, strong, strong recommendation.
1016000	1025000	Now, you can use GPT-4 by paying 20 bucks a month to open AI, and then you can use it a whole lot.
1025000	1030000	It's very hard to run out of credits, I find.
1030000	1033000	Now, what can GPT-2?
1033000	1041000	It's interesting and instructive, in my opinion, to start with the very common views you see on the internet
1041000	1043000	or even in academia about what it can't do.
1043000	1049000	So, for example, there was this paper you might have seen, GPT-4 can't reason,
1049000	1057000	which describes a number of empirical analysis done of 25 diverse reasoning problems
1057000	1064000	and found that it was not able to solve them, it's utterly incapable of reasoning.
1064000	1070000	So, I always find you've got to be a bit careful about reading stuff like this
1070000	1078000	because I just took the first three that I came across in that paper and I gave them to GPT-4
1079000	1088000	and, by the way, something very useful in GPT-4 is you can click on the share button
1088000	1092000	and you'll get something that looks like this, and this is really handy.
1092000	1098000	So, here's an example of something from the paper that said GPT-4 can't do this.
1098000	1102000	Mabel's heart rate at 9 a.m. was 75 beats per minute.
1102000	1105000	Her blood pressure at 7 p.m. was 120 over 80.
1105000	1108000	She died at 11 p.m. while she alive at noon.
1108000	1112000	So, of course, we're human, we know obviously she must be.
1112000	1119000	And GPT-4 says, hmm, this appears to be a riddle, not a real inquiry into medical conditions.
1119000	1127000	Here's a summary of the information and, yeah, it sounds like Mabel was alive at noon.
1127000	1128000	So, that's correct.
1128000	1133000	This was the second one I tried from the paper that says GPT-4 can't do this
1133000	1136000	and I found actually GPT-4 can do this.
1137000	1142000	And it said that GPT-4 can't do this and I found GPT-4 can do this.
1142000	1149000	Now, I mentioned this to say GPT-4 is probably a lot better than you would expect
1149000	1155000	if you've read all this stuff on the internet about all the dumb things that it does.
1157000	1162000	Almost every time I see on the internet saying something that GPT-4 can't do,
1162000	1164000	I check it and it turns out it does.
1164000	1166000	This one was just last week.
1166000	1168000	Sally, a girl, has three brothers.
1168000	1170000	Each brother has two sisters.
1170000	1173000	How many sisters does Sally have?
1173000	1175000	So, I have to think about it.
1177000	1183000	And so, GPT-4 says, OK, Sally's counted as one sister by each of her brothers.
1183000	1188000	If each brother has two sisters, that means there's another sister in the picture apart from Sally.
1188000	1190000	So, Sally has one sister.
1190000	1191000	Correct.
1197000	1200000	And then this one I got sort of like three or four days ago.
1201000	1207000	This is a common view that language models can't track things like this.
1207000	1208000	Here's the riddle.
1208000	1209000	I'm in my house.
1209000	1211000	On top of my chair in the living room is a coffee cup.
1211000	1213000	Inside the coffee cup is a thimble.
1213000	1215000	Inside the thimble is a diamond.
1215000	1217000	I move the chair to the bedroom.
1217000	1218000	I put the coffee cup in the bed.
1218000	1219000	I turn the cup upside down.
1219000	1221000	Then I return it upside up.
1221000	1223000	Place the coffee cup on the counter in the kitchen.
1223000	1225000	Where's my diamond?
1225000	1228000	And so GPT-4 says, yeah, OK.
1228000	1230000	You turned it upside down.
1230000	1232000	So, probably the diamond fell out.
1232000	1235000	So, therefore, the diamond in the bedroom fell out.
1235000	1236000	Again, correct.
1238000	1245000	Why is it that people are claiming that GPT-4 can't do these things when it can?
1245000	1251000	Well, the reason is because I think on the whole they are not aware of how GPT-4 was trained.
1251000	1259000	GPT-4 was not trained at any point to give correct answers.
1259000	1265000	GPT-4 was trained initially to give most likely next words.
1265000	1272000	And there's an awful lot of stuff on the internet where documents are not describing things that are true.
1272000	1273000	There could be fiction.
1273000	1275000	There could be jokes.
1275000	1278000	There could be just stupid people saying dumb stuff.
1278000	1283000	So, this first stage does not necessarily give you correct answers.
1283000	1291000	The second stage with the instruction tuning, also, like, it's trying to give correct answers.
1291000	1298000	But part of the problem is that then in the stage where you start asking people which answer do they like better,
1299000	1307000	people tended to say in these things that they prefer more confident answers
1307000	1313000	and they often were not people who were trained well enough to recognize wrong answers.
1313000	1321000	So, there's lots of reasons that the SGD weight updates from this process for stuff like GPT-4
1321000	1327000	don't particularly or don't entirely reward correct answers.
1327000	1331000	But you can help it want to give you correct answers.
1331000	1339000	If you think about the LM pre-training, what are the kinds of things in a document that would suggest,
1339000	1342000	oh, this is going to be high quality information.
1342000	1353000	And so, you can actually prime GPT-4 to give you high quality information by giving it custom instructions.
1353000	1360000	And what this does is this is basically text that is prepended to all of your queries.
1360000	1364000	And so, you say like, oh, you're brilliant at reasoning.
1364000	1370000	So, like, okay, that's obviously to prime it to give good answers.
1370000	1380000	And then try to work against the fact that the RLHF folks preferred confidence.
1380000	1385000	Just tell it. No, tell me if there might not be a correct answer.
1385000	1392000	Also, the way that the text is generated is it literally generates the next word.
1392000	1397000	And then it puts all that whole lot back into the model and generates the next next word,
1397000	1402000	puts that all back in the model, generates the next next next word, and so forth.
1402000	1406000	That means the more words it generates, the more computation it can do.
1406000	1409000	And so, I literally, I tell it that, right?
1409000	1415000	So, I say, first, bend a few sentences explaining background context, et cetera.
1415000	1428000	So, this custom instruction allows it to solve more challenging problems.
1428000	1432000	And you can see the difference.
1432000	1438000	Here's what it looks like, for example, if I say, how do I get a count of rows grouped by value in pandas?
1438000	1443000	And it just gives me a whole lot of information, which is actually it thinking.
1443000	1446000	So, I just skip over it, and then it gives me the answer.
1446000	1455000	And actually, in my custom instructions, I actually say, if the request begins with VV,
1455000	1458000	actually make it as concise as possible.
1458000	1461000	And so, it kind of goes into brief mode.
1461000	1463000	And here is brief mode.
1463000	1464000	How do I get the grid?
1464000	1466000	This is the same thing, but with VV at the start.
1466000	1468000	And it just spits it out.
1468000	1472000	Now, in this case, it's a really simple question, so I didn't need time to think.
1472000	1480000	So, hopefully, that gives you a sense of how to get language models to give good answers.
1480000	1482000	You have to help them.
1482000	1487000	And if it's not working, it might be user error, basically.
1487000	1493000	But having said that, there's plenty of stuff that language models like GPT-4 can't do.
1494000	1499000	One thing to think carefully about is, does it know about itself?
1499000	1500000	Can you ask it?
1500000	1502000	What is your context length?
1502000	1504000	How were you trained?
1504000	1509000	What transformer architecture are you based on?
1509000	1515000	At any one of these stages, did it have the opportunity to learn any of those things?
1515000	1517000	Well, obviously, not at the pre-training stage.
1517000	1525000	Nothing on the internet existed during GPT-4's training saying how GPT-4 was trained.
1525000	1529000	Probably ditto in the instruction tuning, probably ditto in the RLHF.
1529000	1535000	So, in general, you can't ask, for example, a language model about itself.
1535000	1541000	Now, again, because of the RLHF, it'll want to make you happy by giving you opinionated answers.
1541000	1547000	So, it'll just spit out the most likely thing it thinks with great confidence.
1547000	1550000	This is just a general kind of hallucination, right?
1550000	1556000	So, hallucinations is just this idea that the language model wants to complete the sentence,
1556000	1560000	and it wants to do it in an opinionated way that's likely to make people happy.
1562000	1564000	It doesn't know anything about URLs.
1564000	1567000	It really hasn't seen many at all.
1567000	1570000	I think a lot of them, if not all of them, pretty much were stripped out.
1571000	1578000	So, if you ask it anything about, like, what's at this web page, again, it'll generally just make it up.
1578000	1583000	And it doesn't know, at least GPT-4 doesn't know anything after September 2021,
1583000	1593000	because the information it was pre-trained on was from that time period, September 2021 and before,
1593000	1595000	called the Knowledge Cutoff.
1595000	1598000	So, here's some things it can't do.
1599000	1602000	Steve Newman sent me this good example of something that it can't do.
1605000	1607000	Here is a logic puzzle.
1607000	1611000	I need to carry a cabbage, a goat, and a wolf across a river.
1611000	1613000	I can only carry one item at a time.
1613000	1616000	I can't leave the goat with a cabbage.
1616000	1618000	I can't leave the cabbage with a wolf.
1618000	1621000	How do I get everything across to the other side?
1621000	1627000	Now, the problem is, this looks a lot like something called the classic river crossing puzzle.
1629000	1635000	So, classic, in fact, that it has a whole Wikipedia page about it.
1635000	1644000	And in the classic puzzle, the wolf would eat the goat, or the goat would eat the cabbage.
1645000	1652000	Now, in Steve's version, he changed it.
1655000	1660000	The goat would eat the cabbage, and the wolf would eat the cabbage, but the wolf won't eat the goat.
1662000	1663000	So, what happens?
1663000	1669000	Well, very interestingly, GPT-4 here is entirely overwhelmed by the language model training.
1669000	1673000	It's seen this puzzle so many times, it knows what word comes next.
1673000	1678000	So, it says, oh, yeah, I take the goat across the road, across the river, and leave it on the other side,
1678000	1684000	leaving the wolf with a cabbage, but we're just told you can't leave the wolf with a cabbage.
1684000	1686000	So, it gets it wrong.
1686000	1693000	Now, the thing is, though, you can encourage GPT-4 or any of these language models to try again.
1693000	1699000	So, during the instruction tuning in RLHF, they're actually fine-tuned with multi-stage conversations.
1699000	1702000	So, you can give it a multi-stage conversation.
1702000	1704000	Repeat back to me the constraints I listed.
1704000	1708000	What happened after step one is a constraint violated.
1708000	1711000	Oh, yeah, yeah, yeah, I made a mistake.
1711000	1717000	Okay, my new attempt, instead of taking the goat across the river and leaving it on the other side,
1717000	1721000	is, oh, I'll take the goat across the river and leave it on the other side.
1721000	1723000	It's done the same thing.
1723000	1727000	Oh, yeah, I did do the same thing.
1727000	1729000	Okay, I'll take the wolf across.
1729000	1732000	Well, now the goat's with a cabbage. That still doesn't work.
1732000	1736000	Oh, yeah, that didn't work either.
1736000	1738000	Sorry about that.
1738000	1741000	Instead of taking the goat across the other side, I'll take the goat across the other side.
1741000	1743000	Okay, what's going on here?
1743000	1745000	Right, this is terrible.
1745000	1756000	Well, one of the problems here is that not only is on the internet it's so common to see this particular goat puzzle
1756000	1759000	that it's so confident it knows what the next word is.
1759000	1765000	Also, on the internet, when you see stuff which is stupid on a web page,
1765000	1770000	it's really likely to be followed up with more stuff that is stupid.
1770000	1778000	Once GPT-4 starts being wrong, it tends to be more and more wrong.
1778000	1784000	It's very hard to turn it around to start it making it be right.
1784000	1797000	So you actually have to go back and there's actually an edit button on these chats.
1797000	1800000	And so what you generally want to do is if it's made a mistake,
1800000	1803000	is don't say, oh, here's more information to help you fix it.
1803000	1816000	But instead, go back and click the edit and change it here.
1816000	1821000	And so this time it's not going to get confused.
1821000	1826000	So in this case, actually fixing Steve's example takes quite a lot of effort,
1826000	1829000	but I think I managed to get it to work eventually.
1829000	1832000	And I actually said, oh, sometimes people read things too quickly.
1832000	1835000	They don't notice things, it can trick them up.
1835000	1838000	Then they apply some pattern, get the wrong answer.
1838000	1840000	You do the same thing, by the way.
1840000	1842000	So I'm going to trick you.
1842000	1846000	So before you're about to get tricked, make sure you don't get tricked.
1846000	1848000	Here's the tricky puzzle.
1848000	1854000	And then also with my custom instructions, it takes time discussing it.
1854000	1856000	And this time it gets it correct.
1856000	1858000	It takes the cabbage across first.
1858000	1863000	So it took a lot of effort to get to a point where it could actually solve this
1863000	1871000	because for things where it's been primed to answer a certain way
1871000	1876000	again and again and again, it's very hard for it to not do that.
1876000	1883000	OK, now something else super helpful that you can use
1883000	1886000	is what they call advanced data analysis.
1886000	1891000	In advanced data analysis, you can ask it to basically write code for you.
1891000	1895000	And we're going to look at how to implement this from scratch ourselves quite soon.
1895000	1897000	But first of all, let's learn how to use it.
1897000	1903000	So I was trying to build something that split into markdown headings,
1903000	1906000	a document on third level markdown headings.
1906000	1909000	So that's three hashes at the start of a line.
1909000	1912000	And I was doing it on the whole of Wikipedia.
1912000	1915000	So using regular expressions was really slow.
1915000	1917000	I said, oh, I want to speed this up.
1917000	1921000	And it said, OK, here's some code, which is great,
1921000	1925000	because then I can say, OK, test it and include edge cases.
1925000	1934000	And so it then puts in the code, creates extra cases, tests it,
1934000	1939000	and says, yep, it's working.
1939000	1941000	However, I discovered it's not.
1941000	1944000	I noticed it's actually removing the carriage return at the end of each sentence.
1944000	1948000	So I said, oh, fix that and update your tests.
1948000	1954000	So it said, OK, so now it's changed the test, updated the test cases,
1954000	1958000	it's run them and it's not working.
1958000	1963000	So it says, oh, yeah, fix the issue in the test cases.
1963000	1965000	Nope, it didn't work.
1965000	1972000	And you can see it's quite clever the way it's trying to fix it by looking at the results.
1973000	1976000	But as you can see, it's not.
1976000	1980000	Every one of these is another attempt, another attempt, another attempt,
1980000	1982000	until eventually I gave up waiting.
1982000	1983000	It's so funny each time.
1983000	1985000	It's like, de-banging again.
1985000	1988000	OK, this time I got to handle it properly.
1988000	1992000	And I gave up at the point where it's like, oh, one more attempt.
1992000	1995000	So it didn't solve it, interestingly enough.
1995000	2005000	And again, there's some limits to the amount of logic that it can do.
2005000	2007000	This is really a very simple question.
2007000	2009000	I asked it to do for me.
2009000	2015000	And so hopefully you can see you can't expect even GPT for code interpreter
2015000	2022000	or advanced data analysis that is now called to make it so you don't have to write code anymore.
2022000	2027000	So there's a lot of substitute for having programmers.
2031000	2034000	But it can often do a lot, as I'll show you in a moment.
2034000	2041000	So for example, actually OCR, like this is something I thought was really cool.
2041000	2044000	You can just paste, sorry, paste or upload.
2044000	2051000	So GPT for you can upload an image, advanced data analysis.
2051000	2055000	Yeah, you can upload an image here.
2055000	2060000	And then I wanted to basically grab some text out of an image.
2060000	2064000	Somebody had got a screenshot with their screen, which was something saying,
2064000	2067000	oh, this language model can't do this.
2067000	2068000	And I wanted to try it as well.
2068000	2072000	So rather than retyping it, I just uploaded that image, my screenshot,
2072000	2074000	and said, can you extract the text from this image?
2074000	2076000	And it said, oh, yeah, I could do that.
2076000	2078000	I could use OCR.
2078000	2082000	And like so it literally wrote an OCR script.
2082000	2085000	And there it is.
2085000	2087000	Just took a few seconds.
2087000	2093000	So the difference here is it didn't really require it to think of much logic.
2093000	2099000	It could just use a very, very familiar pattern that it would have seen many times.
2099000	2106000	So this is generally where I find language models excel is where it doesn't have to think too far outside the box.
2106000	2109000	I mean, it's great on kind of creativity tasks,
2109000	2114000	but for like reasoning and logic tasks that are outside the box, I find it not great.
2114000	2121000	But yeah, it's great at doing code for a whole wide variety of different libraries and languages.
2121000	2128000	Having said that, by the way, Google also has a language model called Bard.
2128000	2131000	It's way less good than GPT for most of the time.
2131000	2137000	But there is a nice thing that you can literally paste an image straight into the prompt.
2137000	2139000	And I just typed OCR this.
2139000	2142000	And it didn't even have to go through code interpreter or whatever.
2142000	2144000	It just said, oh, sure, I've done it.
2144000	2147000	And there's the result of the OCR.
2147000	2151000	And even commented on what it just OCRed, which I thought was cute.
2151000	2160000	And oh, even more interestingly, it even figured out where the OCR text came from and gave me a link to it.
2160000	2163000	So I thought that was pretty cool.
2163000	2168000	OK, so there's an example of it doing well.
2168000	2171000	I'll show you one for this talk I found really helpful.
2171000	2178000	I wanted to show you guys how much it costs to use the OpenAI API.
2178000	2184000	But unfortunately, when I went to the OpenAI web page, it was like all over the place.
2184000	2189000	The pricing information was on all separate tables and it was kind of a bit of a mess.
2189000	2197000	So I wanted to create a table with all of the information combined like this.
2197000	2202000	And here's how I did it.
2202000	2205000	I went to the OpenAI page.
2205000	2208000	I hit Apple A to select all.
2208000	2215000	And then I said in chat GPT, create a table with the pricing information rows, no summarization,
2215000	2217000	no information, not in this page.
2217000	2219000	Every row should appear as a separate row in your output.
2219000	2221000	And I hit Paste.
2221000	2225000	Now that was not very helpful to it because hitting Paste, it's got the nav bar.
2225000	2231000	It's got lots of extra information at the bottom.
2231000	2236000	It's got all of its footer, et cetera.
2236000	2239000	But it's really good at this stuff.
2239000	2241000	It did it first time.
2241000	2243000	So there was the markdown table.
2243000	2246000	And then pasted that into Jupiter.
2246000	2248000	And I got my markdown table.
2248000	2254000	And so now you can see at a glance the cost of GPT-4, 3.5, et cetera.
2254000	2258000	But then what I really wanted to do was show you that as a picture.
2258000	2262000	So I just said, oh, chart the input row from this table.
2262000	2266000	Just pasted the table back.
2266000	2268000	And it did.
2268000	2270000	So that's pretty amazing.
2270000	2273000	So let's talk about this pricing.
2273000	2277000	So far, we've used chat GPT, which costs $20 a month.
2277000	2280000	And there's no per token cost or anything.
2280000	2284000	But if you want to use the API from Python or whatever, you have to pay per token.
2284000	2287000	Which is approximately per word.
2287000	2292000	Maybe it's about one and a third tokens per word on average.
2292000	2297000	Unfortunately, in the chart, it did not include these headers, GPT-4, GPT-3.5.
2297000	2299000	So these first two ones are GPT-4.
2299000	2301000	And these two are GPT-3.5.
2301000	2307000	So you can see the GPT-3.5 is way, way cheaper.
2307000	2309000	And you can see it here.
2309000	2314000	It's 0.03 versus 0.0015.
2314000	2320000	So it's so cheap, you can really play around with it and not worry.
2320000	2323000	And I want to give you a sense of what that looks like.
2324000	2331000	So why would you use the OpenAI API rather than chat GPT?
2331000	2333000	Because you can do it programmatically.
2333000	2342000	So you can analyze data sets, you can do repetitive stuff.
2342000	2345000	It's kind of like a different way of programming.
2345000	2349000	It's things that you can think of describing.
2349000	2352000	But let's just look at the most simple example of what that looks like.
2352000	2358000	So if you pip install OpenAI, then you can import chat completion.
2358000	2364000	And then you can say, OK, chat completion.create using GPT-3.5 turbo.
2364000	2367000	And then you can pass in a system message.
2367000	2370000	This is basically the same as custom instructions.
2370000	2375000	So, OK, you're an Aussie LLM that uses Aussie slang and analogies wherever possible.
2375000	2379000	OK, and so you can see I'm passing in an array here of messages.
2379000	2385000	So the first is the system message and then the user message, which is what is money?
2385000	2392000	OK, so GPT-3.5 returns a big embedded dictionary.
2392000	2402000	And the message content is, well, money is like the oil that keeps the machinery of our economy running smoothly.
2402000	2403000	There you go.
2403000	2408000	Just like Aquila loves its eucalyptus leaves, we humans can't survive without this stuff.
2408000	2412000	So there's the Aussie LLM's view of what is money.
2412000	2422000	So the main ones I pretty much always use are GPT-4 and GPT-3.5.
2422000	2431000	GPT-4 is just so, so much better at anything remotely challenging, but obviously it's much more expensive.
2431000	2436000	So rule of thumb, maybe try 3.5 turbo first, see how it goes.
2436000	2438000	If you're happy with the results, then great.
2438000	2442000	If you're not, pony out for the more expensive one.
2442000	2452000	OK, so I just created a little function here called response that will print out this nested thing.
2452000	2460000	And so now, oh, and so then the other thing to point out here is that the result of this also has a usage field,
2460000	2464000	which contains how many tokens was it?
2464000	2466000	So it's about 150 tokens.
2466000	2486000	So at $0.002 per thousand tokens for 150 tokens means we just paid 0.03 cents, 0.0003 dollars to get that done.
2486000	2489000	So as you can see, the cost is insignificant.
2489000	2495000	If we were using GPT-4, it would be 0.03 per thousand.
2495000	2499000	So it would be half a cent.
2499000	2511000	So unless you're doing many thousands of GPT-4, you're not going to be even up into the dollars and GPT-3.5 even more than that.
2511000	2512000	But you know, keep an eye on it.
2512000	2516000	OpenAI has a usage page and you can track your usage.
2516000	2527000	Now, what happens when we are this is really important to understand when we have a follow up in the same conversation?
2527000	2531000	How does that work?
2531000	2534000	So we just asked what goat means.
2534000	2545000	So for example, Michael Jordan is often referred to as the goat for his exceptional skills and accomplishments.
2545000	2550000	And Elvis and the Beatles referred to as goat due to their profound influence and achievement.
2550000	2565000	So I could say, what profound influence and achievements are you referring to?
2565000	2569000	Okay, well, I meant Elvis Presley and the Beatles did all these things.
2569000	2570000	Now, how does that work?
2570000	2572000	How does this follow up work?
2572000	2577000	What happens is the entire conversation is passed back.
2577000	2580000	And so we can actually do that here.
2580000	2584000	So here is the same system prompt.
2584000	2587000	Here is the same question, right?
2587000	2590000	And then the answer comes back with role assistant.
2590000	2592000	And I'm going to do something pretty cheeky.
2592000	2597000	I'm going to pretend that it didn't say money is like oil.
2597000	2602000	I'm going to say, oh, you actually said money is like kangaroos.
2602000	2604000	I thought, what is it going to do?
2604000	2610000	Okay, so you can like literally invent a conversation in which the language model said something different.
2610000	2614000	Because this is actually how it's done in a multi-stage conversation.
2614000	2616000	There's no state, right?
2616000	2618000	There's nothing stored on the server.
2618000	2625000	You're passing back the entire conversation again and telling it what it told you, right?
2625000	2626000	So I'm going to tell it.
2626000	2628000	It told me that money is like kangaroos.
2628000	2631000	And then I'll ask the user, oh, really?
2631000	2632000	In what way?
2632000	2638000	And it's kind of cool because you can see how it convinces you of something I just invented.
2638000	2641000	Oh, let me break it down for you, Cuba.
2641000	2644000	Just like kangaroos hop around and carry their joeys in their pouch,
2644000	2646000	money is a means of carrying value around.
2646000	2647000	So there you go.
2647000	2650000	It's make your own analogy.
2650000	2651000	Cool.
2651000	2657000	So I'll create a little function here that just puts these things together for us.
2657000	2662000	System message, if there is one, the user message and returns the completion.
2662000	2665000	And so now we can ask it, what's the meaning of life?
2665000	2668000	Passing in the Aussie system prompt.
2668000	2673000	The meaning of life is like trying to catch a wave on a sunny day at Bondo Beach.
2673000	2674000	Okay, there you go.
2674000	2677000	So what do you need to be aware of?
2677000	2680000	Well, as I said, one thing is keep an eye on your usage.
2680000	2684000	If you're doing it, you know, hundreds or thousands of times in a loop,
2684000	2687000	keep an eye on not spending too much money.
2687000	2691000	But also if you do it too fast, particularly the first day or two,
2691000	2697000	you've got an account, you're likely to hit the limits for the API.
2697000	2702000	And so the limits initially are pretty low.
2702000	2709000	As you can see, three requests per minute.
2710000	2713000	So that's for free users, page users, first 48 hours.
2713000	2717000	And after that, it starts going up and you can always ask for more.
2717000	2723000	I just mentioned this because you're going to want to have a function that keeps an eye on that.
2723000	2726000	And so what I did is I actually just went to Bing,
2726000	2730000	which has a somewhat crappy version of GPT-4 nowadays,
2730000	2732000	but it can still do basic stuff for free.
2732000	2741000	And I said, please show me Python code to call the OpenAI API and handle rate limits.
2741000	2743000	And it wrote this code.
2743000	2750000	It's got a try, checks your rate limit errors, grabs the retry after,
2750000	2754000	sleeps for that long and calls itself.
2754000	2757000	And so now we can use that to ask, for example,
2757000	2760000	what's the world's funniest joke?
2760000	2764000	And there we go.
2764000	2768000	Here's the world's funniest joke.
2768000	2778000	So that's like the basic stuff you need to get started using the OpenAI LLMs.
2778000	2784000	And yeah, it's definitely suggest spending plenty of time with that
2784000	2793000	so that you feel like you're really a LLM using expert.
2793000	2795000	So what else can we do?
2795000	2800000	Well, let's create our own code interpreter that runs inside GPT-4.
2800000	2808000	And so to do this, we're going to take advantage of a really nifty thing called function calling,
2808000	2811000	which is provided by the OpenAI API.
2811000	2816000	And in function calling, when we call our askGPT function,
2816000	2822000	which is this little one here, we had room to pass in some keyword arguments
2822000	2825000	that would be just passed along to checkCompletion.create.
2825000	2832000	And one of those keyword arguments you can pass is functions.
2832000	2834000	What on earth is that?
2834000	2843000	Functions tells OpenAI about tools that you have, about functions that you have.
2843000	2853000	So for example, I created a really simple function called sums and it adds two things.
2853000	2857000	In fact, it adds two ints.
2857000	2865000	And I am going to pass that function to checkCompletion.create.
2865000	2869000	Now you can't pass a Python function directly.
2869000	2873000	You actually have to pass what's called the JSON schema.
2873000	2876000	So you have to pass the schema for the function.
2876000	2881000	So I created this nifty little function that you're welcome to borrow,
2881000	2888000	which uses pydantic and also Python's inspect module
2888000	2895000	to automatically take a Python function and return the schema for it.
2895000	2898000	And so this is actually what's going to get passed to OpenAI.
2898000	2900000	So it's going to know that there's a function called sums.
2900000	2905000	It's going to know what it does and it's going to know what parameters it takes,
2905000	2909000	what the defaults are and what's required.
2909000	2914000	So this is like, when I first heard about this, I found this a bit mind-bending
2914000	2917000	because this is so different to how we normally program computers.
2917000	2922000	The key thing for programming the computer here actually is the doc string.
2922000	2928000	This is the thing that GPT-4 will look at and say, oh, what does this function do?
2928000	2932000	So it's critical that this describes exactly what the function does.
2932000	2940000	And so if I then say, what is 6 plus 3?
2940000	2944000	And I just, I really wanted to make sure it actually did it here.
2944000	2948000	So I gave it lots of prompts to say, because obviously it knows how to do it itself
2948000	2950000	without calling sums.
2950000	2956000	So it'll only use your functions if it feels it needs to, which is a weird concept.
2956000	2958000	I mean, I guess fields is not a great word to use,
2958000	2962000	but you kind of have to anthropomorphize these things a little bit
2962000	2965000	because they don't behave like normal computer programs.
2965000	2973000	So if I ask GPT, what is 6 plus 3, and tell it that there's a function called sums,
2973000	2977000	then it does not actually return the number 9.
2977000	2981000	Instead, it returns something saying, please call a function.
2981000	2986000	Call this function and pass it these arguments.
2986000	2990000	So if I print it out, there's the arguments.
2990000	2993000	So I created a little function called core function,
2993000	3000000	and it goes into the result of OpenAI, grabs the function call,
3000000	3004000	checks that the name is something that it's allowed to do,
3004000	3011000	grabs it from the global system table, and calls it, passing in the parameters.
3011000	3022000	So if I now say, OK, call the function that we got back, we finally get 9.
3022000	3025000	So this is a very simple example.
3025000	3027000	It's not really doing anything that useful,
3027000	3035000	but what we could do now is we can create a much more powerful function called Python.
3035000	3046000	And the Python function executes code using Python and returns the result.
3046000	3052000	Now, of course, I didn't want my computer to run arbitrary Python code
3052000	3055000	that GPT4 told it to without checking.
3055000	3057000	So I just got it to check first.
3057000	3061000	So say, oh, are you sure you want to do this?
3061000	3069000	So now I can say, ask GPT, what is 12 factorial?
3069000	3070000	System prompt.
3070000	3075000	You can use Python for any required computations and say, OK, here's a function you've got available.
3075000	3078000	It's the Python function.
3078000	3086000	So if I now call this, it will pass me back again a completion object.
3086000	3093000	And here it's going to say, OK, I want you to call Python passing in this argument.
3093000	3099000	And when I do, it's going to go import math, result equals blah, and then return result.
3099000	3100000	Do I want to do that?
3100000	3103000	Yes, I do.
3103000	3105000	And there it is.
3105000	3109000	Now, there's one more step which we can optionally do.
3109000	3114000	I mean, we've got the answer we wanted, but often we want the answer in more of a chat format.
3114000	3122000	And so the way to do that is to, again, repeat everything that you've passed into so far.
3122000	3130000	But then instead of adding an assistant role response, we have to provide a function role response
3130000	3136000	and simply put in here the result we got back from the function.
3136000	3150000	And if we do that, we now get the pros response 12 factorial is equal to 470 million 1,600.
3150000	3160000	Now, functions like Python, you can still ask it about non Python things.
3160000	3164000	And it just ignores it if you don't need it, right?
3164000	3177000	So you can have a whole bunch of functions available that you've built to do whatever you need for the stuff which the language model isn't familiar with.
3177000	3189000	And it'll still solve whatever it can on its own and use your tools, use your functions where possible.
3189000	3195000	So we have built our own code interpreter from scratch.
3195000	3202000	I think that's pretty amazing.
3202000	3212000	So that is what you can do with or some of the stuff you can do with open AI.
3212000	3216000	What about stuff that you can do on your own computer?
3216000	3225000	Well, to use a language model on your own computer, you're going to need to use a GPU.
3225000	3230000	So I guess the first thing to think about is like, do you want this?
3230000	3235000	Does it make sense to do stuff on your own computer?
3235000	3238000	What are the benefits?
3238000	3246000	There are not any open source models that are as good yet as GPT-4.
3246000	3251000	And I would have to say also like actually open AI pricing is really pretty good.
3251000	3259000	So it's not immediately obvious that you definitely want to kind of go in-house, but there's lots of reasons you might want to.
3259000	3264000	And we'll look at some examples of them today.
3265000	3274000	One example you might want to go in-house is that you want to be able to ask questions about your proprietary documents
3274000	3279000	or about information after September 2021, the knowledge cutoff.
3279000	3286000	Or you might want to create your own model that's particularly good at solving the kinds of problems that you need to solve using fine tuning.
3286000	3297000	And these are all things that you absolutely can get better than GPT-4 performance at work or at home without too much money or travel.
3297000	3300000	So these are the situations in which you might want to go down this path.
3300000	3304000	And so you don't necessarily have to buy a GPU.
3304000	3312000	On Kaggle, they will give you a notebook with two quite old GPUs attached and very little RAM.
3312000	3314000	But it's something.
3314000	3316000	Or you can use Colab.
3316000	3328000	And on Colab, you can get much better GPUs than Kaggle has and more RAM, particularly if you pay a monthly subscription fee.
3328000	3333000	So those are some options for free or low cost.
3333000	3349000	You can also, of course, go to one of the many GPU server providers and they change all the time as to what's good or what's not.
3349000	3352000	RunPod is one example.
3352000	3362000	And you can see if you want the biggest and best machine, you're talking $34 an hour, so it gets pretty expensive.
3362000	3367000	But you can certainly get things a lot cheaper, $0.80 an hour.
3367000	3374000	Lambda Labs is often pretty good.
3374000	3382000	You know, it's really hard at the moment to actually find.
3382000	3385000	Let's see, pricing to actually find people that have them available.
3385000	3392000	So they've got lots listed here, but they often have none or very few available.
3392000	3406000	There's also something pretty interesting called Vast AI, which basically lets you use other people's computers when they're not using them.
3406000	3415000	And as you can see, you know, they tend to be much cheaper than other folks.
3415000	3418000	And then they tend to have better availability as well.
3418000	3422000	But of course, for sensitive stuff, you don't want to be running it on some Rando's computer.
3422000	3425000	So anyway, so there's a few options for renting stuff.
3425000	3433000	You know, I think it's, if you can, it's worth buying something and definitely the one to buy at the moment is the GTX 3090 used.
3433000	3439000	You can generally get them from eBay for like 700 bucks or so.
3439000	3445000	A 4090 isn't really better for language models, even though it's a newer GPU.
3445000	3450000	The reason for that is that language models are all about memory speed.
3450000	3454000	How quickly can you get in and stuff in and out of memory rather than how fast is the processor?
3454000	3457000	And that hasn't really improved a whole lot.
3457000	3460000	So the 2000 bucks.
3460000	3464000	The other thing as well as memory speed is memory size 24 gigs.
3464000	3467000	It doesn't quite cut it for a lot of things.
3467000	3469000	So you'd probably want to get two of these GPUs.
3469000	3474000	So you're talking like $1,500 or so.
3474000	3477000	Or you can get a 48 gig gram GPU.
3477000	3479000	It's called an A6000.
3479000	3483000	But this is going to cost you more like five grand.
3483000	3488000	So again, getting two of these is going to be a better deal.
3488000	3493000	And this is not going to be faster than these either.
3493000	3500000	Or funnily enough, you could just get a Mac with a lot of RAM, particularly if you get an M2 Ultra.
3500000	3506000	Macs have, particularly the M2 Ultra has pretty fast memory.
3506000	3509000	It's still going to be way slower than using an Nvidia card.
3509000	3517000	But it's going to be like you're going to be able to get, you know, like I think 192 gig or something.
3517000	3523000	So it's not a terrible option, particularly if you're not training models.
3523000	3531000	You just want to use other existing trained models.
3531000	3538000	So anyway, most people who do this stuff seriously, almost everybody has Nvidia cards.
3539000	3544000	So then what we're going to be using is a library called Transformers from Hugging Face.
3544000	3554000	And the reason for that is that basically people upload lots of pre-trained models or five trained models up to the Hugging Face Hub.
3554000	3560000	And in fact, there's even a leaderboard where you can see which are the best models.
3560000	3567000	Now, this is a really fraught area.
3567000	3570000	So at the moment, this one is meant to be the best model.
3570000	3572000	It has the highest average score.
3572000	3574000	And maybe it is good.
3574000	3576000	I haven't actually used this particular model.
3576000	3578000	Or maybe it's not.
3578000	3589000	I actually have no idea because the problem is these metrics are not particularly well aligned with real life usage.
3590000	3591000	For all kinds of reasons.
3591000	3601000	And also sometimes you get something called leakage, which means that sometimes some of the questions from these things actually leaks through to some of the training sets.
3601000	3609000	So you can get as a rule of thumb what to use from here, but you should always try things.
3609000	3614000	And you can also say, you know, these ones are all the 70B here that tells you how big it is.
3614000	3618000	So this is a 70 billion parameter model.
3619000	3630000	So generally speaking for the kinds of GPUs we're talking about, you'll be wanting no bigger than 13B and quite often 7B.
3630000	3637000	So let's see if we can find here's a 13B model, for example.
3637000	3638000	All right.
3638000	3644000	So you can find models to try out from things like this leaderboard.
3644000	3658000	And there's also a really great leaderboard called FastEval, which I like a lot because it focuses on some more sophisticated evaluation methods such as this chain of thought evaluation method.
3658000	3661000	So I kind of trust these a little bit more.
3661000	3670000	And these are also, you know, GSM 8K is a difficult math benchmark, big bench hard, so forth.
3670000	3671000	So, yeah.
3671000	3677000	So, you know, StableBlog2, WizardMeth, 13B, Dolphin, Lama, 13B, et cetera.
3677000	3682000	These would all be good options.
3682000	3683000	Yeah.
3683000	3684000	So you need to pick a model.
3684000	3691000	And at the moment, nearly all the good models are based on Meta's Lama 2.
3691000	3694000	So when I say based on, what does that mean?
3694000	3700000	Well, what that means is this model here, Lama 2, 7B.
3700000	3702000	So it's a Lama model.
3702000	3704000	That's just the name Meta called it.
3704000	3706000	This is their version 2 of Lama.
3706000	3708000	This is their 7 billion size one.
3708000	3710000	It's the smallest one that they make.
3710000	3713000	And specifically, these weights have been created for hugging face.
3713000	3716000	So you can load it with the hugging face transformers.
3716000	3720000	And this model has only got as far as here.
3720000	3722000	It's done the language model for pre-training.
3722000	3729000	It's done none of the instruction tuning and none of the RLHF.
3729000	3735000	So we would need to fine tune it to really get it to do much useful.
3735000	3743000	So we can just say, OK, create a, automatically create the appropriate model for language model.
3743000	3750000	So causalLM is basically refers to that ULM fifth stage one process or stage two, in fact.
3750000	3753000	Create the pre-trained model from this name.
3753000	3755000	Meta Lama Lama 2, blah, blah, blah.
3755000	3756000	OK.
3756000	3767000	Now, generally speaking, we use 16 bit floating point numbers nowadays.
3767000	3772000	But if you think about it, 16 bit is two bytes.
3772000	3778000	So 7B times 2, it's going to be 14 gigabytes.
3778000	3781000	Just to load in the weights.
3781000	3786000	So you've got to have a decent model to be able to do that.
3786000	3790000	Perhaps surprisingly, you can actually just cast it to 8 bit.
3790000	3794000	And it still works pretty well, thanks to some encode discretization.
3794000	3797000	So let's try that.
3797000	3799000	So remember, this is just a language model.
3799000	3800000	It can only complete sentences.
3800000	3803000	We can't ask it a question and expect a great answer.
3803000	3807000	So let's just give it the start of a sentence, Jeremy, how it is R.
3807000	3809000	So we need the right tokenizer.
3809000	3812000	So this will automatically create the right kind of tokenizer for this model.
3812000	3816000	We can grab the tokens as PyTorch.
3816000	3821000	Here they are.
3821000	3825000	And just to confirm, if we decode them back again,
3825000	3830000	we get the original plus a special token to say this is the start of a document.
3830000	3833000	And so we can now call generate.
3833000	3839000	So generate will auto regressively.
3839000	3841000	So call the model again and again,
3841000	3848000	passing its previous result back as the next input.
3848000	3851000	And I'm just going to do that 15 times.
3851000	3854000	So you can write this for loop yourself.
3854000	3855000	This isn't doing anything fancy.
3855000	3858000	In fact, I would recommend writing this yourself
3858000	3862000	to make sure that you know how, that it all works OK.
3863000	3866000	We have to put those tokens on the GPU.
3866000	3869000	And at the end, I recommend putting them back onto the CPU, the result.
3869000	3871000	And here are the tokens.
3871000	3872000	Not very interesting.
3872000	3875000	So we have to decode them using the tokenizer.
3875000	3879000	And so the first 25, so first 15 tokens are Jeremy, how it is R.
3879000	3883000	28 year old Australian AI researcher and entrepreneur.
3883000	3887000	OK, well, 28 years old is not exactly correct, but we'll call it close enough.
3887000	3888000	I like that.
3888000	3890000	Thank you very much.
3890000	3892000	Lama 7B.
3892000	3897000	So OK, so we've got a language model completing sentences.
3897000	3902000	It took one and a third seconds.
3902000	3906000	And that's a bit slower than it could be because we used 8 bit.
3906000	3910000	If we use 16 bit, there's a special thing called B float 16,
3910000	3914000	which is a really great 16 bit floating point format
3914000	3919000	that's usable on any somewhat recent GPU, Nvidia GPU.
3919000	3924000	If we use it, it's going to take twice as much RAM as we discussed.
3924000	3926000	But look at the time.
3926000	3931000	It's come down to 390 milliseconds.
3931000	3935000	Now, there is a better option still than even that.
3935000	3939000	There's a different kind of discretization called GPTQ,
3939000	3946000	where a model is carefully optimized to work with 4 or 8
3946000	3951000	or other lower precision data automatically.
3951000	3957000	And this particular person known as the bloke
3957000	3962000	is fantastic at taking popular models, running that optimization process,
3962000	3967000	and then uploading the results back to Hackingface.
3967000	3971000	So we can use this GPTQ version.
3971000	3974000	And internally, this is actually going to use,
3974000	3976000	I'm not sure exactly how many bits this particular one is.
3976000	3978000	I think it's probably going to be four bits,
3978000	3981000	but it's going to be much more optimized.
3981000	3984000	And so look at this, 270 milliseconds.
3984000	3989000	It's actually faster than 16 bit.
3989000	3994000	Even though internally, it's actually casting it up to 16 bit each layer to do it.
3994000	3998000	And that's because there's a lot less memory moving around.
3998000	4003000	And to confirm, in fact, what we could even do now is we could go up to 13D.
4003000	4004000	Easy.
4004000	4007000	And in fact, it's still faster than the 7B,
4007000	4009000	now that we're using the GPTQ version.
4009000	4012000	So this is a really helpful tip.
4012000	4015000	So let's put all those things together, the tokenizer,
4015000	4018000	the generate, the batch decode, we'll call this gen for generate.
4018000	4023000	And so we can now use the 13B GPTQ model.
4023000	4028000	And let's try this Jeremy Howard is a, so it's got to 50 tokens so fast.
4028000	4031000	16 year veteran of Silicon Valley, co-founder of Kaggle,
4031000	4033000	a Marketplace predictive model.
4033000	4036000	His company Kaggle.com has become to data science competitions.
4036000	4039000	What I don't know what I was going to say, but anyway, it's on the right track.
4039000	4045000	I was actually there for 10 years, not 16, but that's all right.
4045000	4046000	Okay.
4046000	4049000	So this is looking good.
4049000	4054000	But probably a lot of the time we're going to be interested in, you know,
4054000	4056000	asking questions or using instructions.
4056000	4060000	So stability AI has this nice series called stable beluga,
4060000	4064000	including a small 7B one and other bigger ones.
4064000	4068000	And these are all based on Lama two, but these have been instruction tuned.
4068000	4070000	They might even have been RLHDF.
4070000	4072000	I can't remember now.
4072000	4076000	So we can create a stable beluga model.
4076000	4084000	And now something really important that I keep forgetting everybody keeps forgetting is
4084000	4092000	during the instruction tuning process,
4092000	4104000	during the instruction tuning process, the instructions that are passed in actually are,
4104000	4106000	they don't just appear like this.
4106000	4109000	They actually always are in a particular format.
4109000	4115000	And the format, believe it or not, changes quite a bit from fine-tuned to fine-tuned.
4115000	4127000	And so you have to go to the webpage for the model and scroll down to find out what the prompt format is.
4127000	4129000	So here's the prompt format.
4129000	4138000	So I generally just copy it and then I paste it into Python, which I did here.
4138000	4148000	And created a function called make prompt that used the exact same format that it said to you use.
4148000	4153000	And so now if I want to say who is Jeremy Howard, I can call Jen again.
4153000	4160000	That was that function I created up here and make the correct prompt from that question.
4160000	4162000	And then it returns back.
4162000	4165000	Okay, so you can see here all this prefix.
4165000	4167000	This is a system instruction.
4167000	4169000	This is my question.
4169000	4177000	And then the assistant says, Jeremy Howard's an Australian entrepreneur, computer scientist, co-founder of machine learning and deep learning company, faster than AI.
4177000	4179000	Okay, so this one's actually all correct.
4179000	4186000	So it's getting better by using an actual instruction tune model.
4186000	4188000	And so we could then start to scale up.
4188000	4190000	So we could use the 13b.
4190000	4195000	And in fact, we looked briefly at this open orca data set earlier.
4195000	4204000	So llama two has been fine tuned on open orca and then also fine tuned on another really great data set called platypus.
4204000	4209000	And so the whole thing together is the open orca platypus.
4209000	4211000	And then this is going to be the bigger 13b.
4211000	4215000	GPTQ means it's going to be quantized.
4215000	4218000	So that's got a different format.
4218000	4220000	Okay, a different prompt format.
4220000	4224000	So again, we can scroll down and see what the prompt format is.
4224000	4226000	There it is.
4226000	4227000	Okay.
4227000	4237000	And so we can create a function called make open orca prompt that has that prompt format.
4237000	4240000	And so now we can say, okay, who is Jeremy Howard?
4240000	4242000	And now I've become British, which is kind of true.
4242000	4245000	I was born in England, but I moved to Australia.
4245000	4247000	Professional poker player.
4247000	4248000	Definitely not that.
4248000	4253000	Co-founding several companies, including faster AI, also Kaggle.
4253000	4254000	Okay.
4254000	4255000	So not bad.
4255000	4259000	It was acquired by Google with 2017, probably something around there.
4259000	4260000	Okay.
4260000	4269000	So you can see we've got our own models giving us some pretty good information.
4269000	4271000	How do we make it even better?
4271000	4282000	You know, because it's still hallucinating, you know, and, you know, llama two, I think,
4282000	4290000	has been trained with more up to date information than GPT for it doesn't have the September 2021 cutoff.
4290000	4293000	But it, you know, it's still got a knowledge cutoff.
4293000	4296000	You know, we would like to be able to use the most up to date information.
4296000	4301000	We want to use the right information to answer these questions as well as possible.
4301000	4306000	So to do this, we can use something called retrieval augmented generation.
4306000	4317000	So what happens with retrieval augmented generation is when we take the question we've been asked, like, who is Jeremy Houd?
4317000	4327000	And then we say, okay, let's try and search for documents that may help us answer that question.
4327000	4332000	So obviously we would expect, for example, Wikipedia to be useful.
4332000	4349000	And then what we do is we say, okay, with that information, let's now see if we can tell the language model about what we found and then have it answer the question.
4349000	4350000	So let me show you.
4350000	4356000	So let's actually grab a Wikipedia Python package.
4356000	4362000	We will scrape Wikipedia, grabbing the Jeremy Howard webpage.
4362000	4368000	And so here's the start of the Jeremy Howard Wikipedia page.
4368000	4371000	It has 613 words.
4371000	4377000	Now, generally speaking, these open source models will have a context length of about 2000 or 4000.
4377000	4380000	So the context length is how many tokens can it handle.
4380000	4381000	So that's fine.
4381000	4383000	It'll be able to handle this webpage.
4383000	4386000	And what we're going to do is we're going to ask it the question.
4386000	4389000	So we're going to have here question and with a question.
4389000	4392000	But before that, we're going to say, answer the question with the help of the context.
4392000	4394000	We're going to provide this to the language model.
4394000	4397000	And we're going to say context and they're going to have the whole webpage.
4397000	4400000	So suddenly now our question is going to be a lot bigger.
4400000	4403000	They're prompt.
4403000	4404000	Right.
4404000	4414000	So our prompt now contains the entire webpage, the whole Wikipedia page, followed by a question.
4414000	4420000	And so now it says Jeremy Howard is an Australian data scientist,
4420000	4423000	entrepreneur, an educator, known for his work in deep learning,
4423000	4428000	co-founder of FastAI, teaches courses, develops software, conducts research.
4428000	4431000	Used to be, yeah, okay, it's perfect.
4431000	4434000	So it's actually done a really good job.
4434000	4440000	Like if somebody asked me to send them a, you know, 100 word bio,
4440000	4444000	that would actually probably be better than I would have written myself.
4444000	4451000	And just so even though I asked for 300 tokens, it actually got sent back the end of stream token.
4451000	4457000	And so it knows to stop at this point.
4457000	4459000	Well, that's all very well.
4459000	4463000	But how do we know to pass in the Jeremy Howard Wikipedia page?
4463000	4473000	Well, the way we know which Wikipedia page to pass in is that we can use another model to tell us which web page
4473000	4481000	or which document is the most useful for answering a question.
4481000	4486000	And the way we do that is we can use something called Sentence Transformer.
4486000	4492000	And we can use a special kind of model that's specifically designed to take a document
4492000	4501000	and turn it into a bunch of activations where two documents that are similar will have similar activations.
4501000	4503000	So let me just, let me show you what I mean.
4503000	4509000	What I'm going to do is I'm going to grab just the first paragraph of my Wikipedia page.
4509000	4514000	And I'm going to grab the first paragraph of Tony Blair's Wikipedia page.
4514000	4517000	Okay, so we're pretty different people, right?
4517000	4520000	This is just like a really simple, small example.
4520000	4525000	And I'm going to then call this model, I'm going to say encode,
4525000	4529000	and I'm going to encode my Wikipedia first paragraph, Tony Blair's first paragraph,
4529000	4537000	and the question, which was, who is Jeremy Howard?
4537000	4549000	And it's going to pass back a 384 long vector of embeddings for the question for me and for Tony Blair.
4549000	4558000	And what I can now do is I can calculate the similarity between the question and the Jeremy Howard Wikipedia page.
4558000	4563000	And I can also do it for the question versus the Tony Blair Wikipedia page.
4563000	4566000	And as you can see, it's higher for me.
4566000	4573000	And so that tells you that if you're trying to figure out what document to use to help you answer this question,
4573000	4580000	better off using the Jeremy Howard Wikipedia page than the Tony Blair Wikipedia page.
4580000	4589000	So if you had a few hundred documents you were thinking of using to give back to the model as context to help it answer a question,
4589000	4597000	you could literally just pass them all through to encode, go through each one at a time and see which is closest.
4597000	4604000	When you've got thousands or millions of documents, you can use something called a vector database,
4604000	4611000	where basically as a one-off thing, you go through and you encode all of your documents.
4612000	4617000	And so in fact, there's lots of pre-built systems for this.
4617000	4622000	Here's an example of one called H2O GPT.
4622000	4632000	And this is just something that I've got running here on my computer.
4632000	4638000	It's just an open source thing written in Python and sitting here running on port 7860.
4638000	4642000	And so I've just gone to localhost 7860.
4642000	4650000	And what I did was I just uploaded, I just clicked upload and uploaded a bunch of papers.
4650000	4652000	In fact, I might be able to see it better.
4652000	4655000	Yeah, here we go, a bunch of papers.
4655000	4662000	And so we could look at, can we search?
4662000	4663000	Yeah, I can.
4663000	4668000	So for example, we can look at the ULM fit paper that Sir Bruder and I did.
4668000	4675000	And you can see it's taken the PDF and turned it into slightly crappily, a text format.
4675000	4682000	And then it's created an embedding for each section.
4682000	4691000	So I could then ask it, you know, what is ULM fit?
4691000	4695000	And I'll hit enter.
4695000	4698000	And you can see here it's now actually saying based on the information provided in the context.
4698000	4701000	So it's showing us it's been given some context.
4701000	4702000	What context did it get?
4702000	4707000	So here are the things that it found, right?
4707000	4711000	So it's being sent this context.
4711000	4716000	So this is kind of citations.
4716000	4726000	A goal of ULM fit proves a performance by leveraging the knowledge and adapting it to the specific task at hand.
4726000	4730000	Now, what techniques be more specific?
4730000	4735000	Does ULM fit?
4735000	4740000	Let's see how it goes.
4740000	4742000	Okay, there we go.
4742000	4745000	So here's the three steps, pre-trained, fine-tuned, fine-tuned.
4745000	4746000	Cool.
4746000	4749000	So you can see it's not bad, right?
4749000	4752000	It's not amazing.
4752000	4757000	Like, you know, the context in this particular case is pretty small.
4757000	4765000	And in particular, if you think about how that embedding thing worked, you can't really use like the normal kind of follow-up.
4765000	4776000	So for example, if I say it says fine-tuning a classifier, so I could say what classifier is used.
4776000	4781000	Now, the problem is that there's no context here being sent to the embedding model.
4781000	4784000	So it's actually going to have no idea I'm talking about ULM fit.
4784000	4788000	So generally speaking, it's going to do a terrible job.
4788000	4791000	Yeah, see, it says used as a Roberta model, but it's not.
4791000	4796000	But if I look at the sources, it's no longer actually referring to Howard and Ruder.
4796000	4799000	So anyway, you can see the basic idea.
4799000	4805000	This is called retrieval augmented generation, R-A-G.
4805000	4812000	And it's a nifty approach, but you have to do it with some care.
4812000	4818000	And so there are lots of these private GPT things out there.
4818000	4829000	And actually the H2O GPT web page is a fantastic job of listing lots of them and comparing.
4829000	4839000	So as you can see, if you want to run a private GPT, there's no shortage of options.
4839000	4842000	And you can have your retrieval augmented generation.
4842000	4845000	I haven't tried, I've only tried this one H2O GPT.
4845000	4849000	I don't love it. It's all right.
4849000	4858000	So finally, I want to talk about what's perhaps the most interesting option we have, which is to do our own fine tuning.
4858000	4863000	And fine tuning is cool because rather than just retrieving documents which might have useful context,
4863000	4870000	we can actually change our model to behave based on the documents that we have available.
4870000	4874000	I'm going to show you a really interesting example of fine tuning here.
4874000	4881000	What we're going to do is we're going to fine tune using this no SQL data set.
4881000	4892000	And it's got examples of like a schema for a table in a database, a question,
4892000	4904000	and then the answer is the correct SQL to solve that question using that database schema.
4904000	4910000	And so I'm hoping we could use this to create a, you know,
4910000	4916000	it could be a handy tool for business users where they type some English question
4916000	4921000	and SQL generated for them automatically.
4921000	4926000	Don't know if it actually work in practice or not, but this is just a little fun idea.
4926000	4928000	I thought we'd try out.
4928000	4934000	I know there's lots of startups and stuff out there trying to do this more seriously.
4934000	4940000	But this is, this is quite cool because it actually got it working today in just a couple of hours.
4940000	4947000	So what we do is we use the hugging face data sets library.
4947000	4952000	And what that does, just like the hugging face hub has lots of models stored on it,
4952000	4956000	hugging face data sets has lots of data sets stored on it.
4956000	4960000	And so instead of using transformers, which is what we use to grab models,
4960000	4968000	we use data sets and we just pass in the name of the person and the name of their repo and it grabs the data set.
4968000	4975000	And so we can take a look at it and it just has a training set with features.
4975000	4982000	And so then I can have a look at the training set.
4982000	4987000	So here's an example, which looks a bit like what we've just seen.
4987000	4992000	So what we do now is we want to fine tune a model.
4992000	4997000	So we can do that in in a notebook from scratch takes, I don't know,
4997000	5000000	a hundred or so lines of code is not too much.
5000000	5004000	But given the time constraints here and also like I thought, why not?
5004000	5007000	Why don't we just use something that's ready to go?
5007000	5014000	So for example, there's something called axolotl, which is quite nice in my opinion.
5014000	5015000	Here it is here.
5015000	5018000	Another very nice open source piece of software.
5018000	5021000	And again, you can just pip install it.
5021000	5027000	And it's got things like GPTQ and 16 bit and so forth ready to go.
5027000	5038000	And so what I did was I it basically has a whole bunch of examples of things that it already knows how to do.
5038000	5040000	It's got llama to example.
5040000	5045000	So I copied the llama to example and I created a SQL example.
5045000	5049000	So basically just told it, this is the path to the data set that I want.
5049000	5056000	This is the type and everything else pretty much I left the same.
5056000	5060000	And then I just ran this command, which is from there.
5060000	5064000	Read me accelerate launch axolotl passed in my YAML.
5064000	5068000	And that took about an hour on my GPU.
5068000	5075000	And at the end of the hour, it had created a QLaura out directory.
5075000	5077000	Q stands for quantize.
5077000	5079000	It's because I was creating a smaller quantized model.
5079000	5093000	Laura, I'm not going to talk about today, but Laura is a very cool thing that basically another thing that makes your models smaller and also handles can use bigger models on smaller GPUs for training.
5094000	5098000	So I trained it.
5098000	5104000	And then I thought, okay, let's create our own one.
5104000	5118000	So we're going to have this context and this question get the count of competition hosts by theme.
5118000	5120000	And I'm not going to pass it an answer.
5120000	5122000	So I'll just ignore that.
5122000	5131000	So again, I found out what prompt they were using and created a SQL prompt function.
5131000	5133000	And so here's what I'm going to do.
5133000	5138000	Use the following contextual information to answer the question context create table.
5138000	5144000	So there's the context question list or competition hosts sorted in ascending order.
5144000	5152000	And then I tokenized that called generate.
5152000	5160000	And the answer was select count hosts comma theme from farm competition group by theme.
5160000	5162000	That is correct.
5162000	5165000	So I think that's pretty remarkable.
5165000	5175000	We have just built, you know, so it took me like an hour to figure out how to do it and then an hour to actually do the training.
5175000	5185000	And at the end of that, we've actually got something which, which is converting pros into SQL based on a schema.
5185000	5189000	So I think that's, that's a really exciting idea.
5189000	5196000	The only other thing I do want to briefly mention is, is doing stuff on max.
5196000	5202000	If you've got a Mac, you, there's a couple of really good options.
5202000	5208000	The options are MLC and llama dot CPP currently MLC in particular.
5208000	5210000	I think it's kind of underappreciated.
5210000	5228000	It's a really nice project where you can run language models on literally iPhone, Android, web browsers, everything.
5228000	5230000	It's really cool.
5230000	5237000	And, and so I'm now actually on my Mac here.
5237000	5244000	And I've got a tiny little Python program called chat.
5244000	5255000	And it's going to import chat module and it's going to import a discretized seven B.
5255000	5260000	And that's going to ask the question, what is the meaning of life?
5260000	5262000	So let's try it.
5262000	5264000	Python chat.py.
5264000	5268000	And again, I just installed this earlier today.
5268000	5278000	I haven't done that much stuff on max before, but I was pretty impressed to see that it is doing a good job here.
5278000	5283000	What is the meaning of life is complex and philosophical.
5283000	5287000	Some people might find meaning in their relationships with others.
5287000	5290000	They're impacting the world, et cetera, et cetera.
5290000	5291000	Okay.
5291000	5295000	And it's doing 9.6 tokens per second.
5295000	5296000	So there you go.
5296000	5299000	So there is running a model on a Mac.
5299000	5304000	And then another option that you've probably heard about is llama dot CPP.
5304000	5312000	Llama dot CPP runs on lots of different things as well, including Max and also on CUDA.
5312000	5316000	It uses a different format called gguf.
5316000	5318000	And again, you can use it from Python.
5318000	5321000	Even though that was a CPP thing, it's got a Python wrapper.
5321000	5329000	So you can just download, again, from Huggingface, a gguf file.
5329000	5332000	So you can just go through and there's lots of different ones.
5332000	5334000	They're all documented as to what's what.
5334000	5336000	You can pick how big a file you want.
5336000	5338000	You can download it.
5338000	5343000	And then you just say, okay, llama model path equals passing that gguf file.
5343000	5347000	It spits out lots and lots and lots of gunk.
5347000	5350000	And then you can say, okay.
5350000	5353000	So if I called that LLM, you can then say LLM question.
5353000	5357000	Name the planets of the solar system, 32 tokens.
5357000	5361000	And there we go.
5361000	5362000	Run Pluto.
5362000	5363000	No longer considered a planet.
5363000	5366000	Two, Mercury, three, Venus, four, Earth, Mars, six.
5366000	5368000	Oh, no, right out of tokens.
5368000	5372000	So again, you know, it's just to show you here.
5372000	5377000	There are all these different options.
5377000	5382000	You know, I would say, you know, if you've got a NVIDIA graphics card
5382000	5385000	and you're a reasonably capable Python programmer,
5385000	5392000	you probably be one of you use PyTorch and the Huggingface ecosystem.
5392000	5395000	But, you know, I think, you know, these things might change over time as well.
5395000	5398000	And certainly a lot of stuff is coming into Llama pretty quickly now
5398000	5400000	and it's developing very fast.
5400000	5404000	As you can see, there's a lot of stuff that you can do right now
5404000	5409000	with language models, particularly if you feel pretty comfortable
5409000	5412000	as a Python programmer.
5412000	5414000	I think it's a really exciting time to get involved.
5414000	5417000	In some ways, it's a frustrating time to get involved
5417000	5423000	because, you know, it's very early
5423000	5427000	and a lot of stuff has weird little edge cases
5427000	5432000	and it's tricky to install and stuff like that.
5432000	5434000	There's a lot of great Discord channels.
5434000	5436000	However, FastAI have our own Discord channel,
5436000	5440000	so feel free to just Google for FastAI Discord and drop in.
5440000	5442000	We've got a channel called Generative.
5442000	5448000	You feel free to ask any questions or tell us about what you're finding.
5448000	5450000	Yeah, it's definitely something where you want to be getting help
5450000	5454000	from other people on this journey because it is very early days.
5454000	5458000	And, you know, people are still figuring things out as we go.
5458000	5461000	But I think it's an exciting time to be doing this stuff
5461000	5463000	and I'm really enjoying it.
5463000	5467000	And I hope that this has given some of you a useful starting point
5467000	5469000	on your own journey.
5469000	5470000	So I hope you found this useful.
5470000	5471000	Thanks for listening.
5471000	5472000	Bye.
