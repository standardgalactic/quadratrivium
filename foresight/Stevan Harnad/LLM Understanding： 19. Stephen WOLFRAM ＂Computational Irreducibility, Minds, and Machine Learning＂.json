{"text": " Even Wolfram is a mathematician, computer scientist, physicist, and businessman. Have I lied so far, Steve? I don't know. So some people would not put mathematician first, but that's okay. He's known for his work in computer science, mathematics, and theoretical physics. The fellow of the American Mathematical Society, founder and CEO of the software company Wolfram Research, where he works as chief designer of Mathematica, at the Wolfram Alpha NSERP engine. Now I'm going to move that away, and I'm going to give it, you have also, besides the people in the the attendees, there's also one, two, three, four, five panelists that are here so far, and there might be some more. Carl Friston, Ronnie Katzier, Sammy Benjiro. I can see the names, yeah. Hi, Carl. The rest of you, I don't know, so nice to meet you. Sammy, it's the other Benjiro. We'll find out more about that. He's the brother of Yoshua, who is the co-sponsor of this event. Okay, it's all yours. Wing it. Okay, so, well, let's see. So I think you guys want to talk about language, and computation, and AI, and all those good kinds of things, and I was thinking I could talk about things about LLMs and so on. You know, I wrote this little book last February about ChatGBT. You can find a version of it online. I can put it in the chat. It's, let me not talk about that, but if people want to ask about it, I'm happy to chat about it. I thought what I would try to do in 45 minutes give you kind of the, a very rough tour of my last four and a half decades of development of a worldview, and see how that relates to things about AI, and language, and so on. So to begin, you know, I think a thing that, sort of the, there's kind of this progression of paradigms to do with how we formalize the world. So there's sort of this question of what, what can we do? We start, when we, when we see the world, we're interested in finding ways to have sort of formal descriptions of it that we can build on. So historically, kind of the first of those, the big one for our species, was the invention of human language, and the idea that you didn't have to just point at each individual rock, but you could have this kind of symbolic name, rock, wasn't rock originally, obviously, for that concept, and you could communicate abstractly about that thing using human language. Now then, we've had sort of a stack of other ideas, another big idea is logic, being able to, as a way of sort of abstracting things about the world, formalizing things about the world. Another big direction is mathematics, being able to, something that sort of became big in the late 1600s, of being able to describe our world by mathematical concepts and constructs. In this century, and the end of the last one, the big new thing has been using computation as a way to formalize and describe the world. Being able to specify kind of if you, the way I think about computation, it's a way of setting up rules, and then saying, let these rules run. And the question is, can we set up rules that describe the way the world is, is, or the aspects of the world at least that we care about? And my kind of day job for the last four decades has been building our computational language, Wolfram Language, Mathematica, and so on, as a way to kind of take the things that we humans care about, whether they're molecules or cities or algorithms or whatever, and create a systematic computational language to let one describe those things, and not only describe them in a way that humans can read, but also something where computers can help humans to execute those things. So that's been kind of the way I see sort of that effort, is take the things that we humans care about, and find a way to formalize them computationally, so that we can provide sort of the raw material that we need to work with the world computationally. It's kind of the effort, is a bit like the effort that happened maybe 500 years ago, with the development of mathematical notation, where people went from kind of talking about math in terms of words, to having sort of a streamlined notation with plus signs and equal signs and things like that. And that idea of notation, that sort of streamlined notation for mathematics, is what ended up launching algebra and then calculus, and basically the modern mathematical sciences. Kind of my day job mission has been to create a computational language that lets one kind of launch computational x for all fields x. So okay, that's, so kind of this idea is computational language as a way to sort of formalize things that happen in the world, things that we care about in the world. Now the next question is what is the intrinsic description of the world, so to speak? How should we describe the world in general? And the thing that sort of had been the tradition of exact science for about 300 years, was use mathematical equations, write down an equation that describes this or that aspect of the world. The thing that I got interested in in the early 1980s is how does one generalize that idea? How does one, how does one, what kinds of raw material can you find to talk about the world? And the thing that I started studying a lot was using computation as kind of the raw material for describing the world. And so the question, the first question is, well okay, what kinds of, how do you set up kind of computational systems to do that? And for example, let's see, let's actually do something here. Okay, let's say we are just kind of, we want to see what do programs that might be computational descriptions of the world, what, let's just look at some program that, just say what do the typical programs out there do? So this is a very simple example, it's just you imagine a line of black and white cells and you have some simple rule that says, given the colors of cells on one row, these are the, this is the color, this is how you determine the color of the cells on the next row. So if you run this and just run this starting, let's say, from one black cell, let's run it for like 40 steps. According to that rule, you start off from, you end up with something where you have this very simple rule, very simple computational rule, you run it, you get this very simple pattern. Now the question is, what happens if we look at other kinds of rules? What happens if we kind of turn our computational telescope out into the computational universe and just look at what's out there? So we can do that, let's do this, let's just make a, let's just make a table of all possible rules, let's say the first 63 of these rules, 64 of these rules. Okay, so each one of these rules corresponds, each one of these pictures corresponds to a different rule for how the colors of cells are determined by colors of cells above them. What we see is many of these patterns are very simple, sometimes we'll get slightly more complicated patterns, we might get a nested pattern, for example, here, but the thing that is kind of my all-time favorite science discovery that I made almost exactly 40 years ago, it was 40 years ago on June 1st, is this thing that I call rule 30. It's specified by this set of cases here, and if we just run, just run this, started off from single black cell, let's run it for let's say 200 steps, this is what we get. And to me, this is something very surprising and kind of intuition breaking. We have a very simple rule, and yet when we run that rule, we're generating something that looks at least to us very complicated. And actually, you can go and you can sort of work out what's the center column of cells here, and for all practical purposes, it seems completely random. But so what this is telling us is out in the computational universe, even very simple rules can easily give one very complicated behavior. And there are a lot of consequences of this. One thing that this led me to is this thing I call the principle of computational equivalence. So the thing that is sort of a big question is, how do you characterize what's going on in a system like this? Well, you can think about the system as performing a computation. It starts from its initial conditions at the top, and then it's going crunch, crunch, crunch, and and executing a computation. The question then is sort of how sophisticated is that computation? And we might have thought, well, it's just a simple rule, it's doing what it does. If we think about the kinds of computations that, for example, we do in our brains, well, those are going to be much more sophisticated than this. But what the principle of computational equivalence says is that actually that's not true. Above some very low threshold, essentially, all of these kinds of systems, regardless of how simple their rules are, are equivalent in the sophistication of the computations that they can do. So that means that in a sense, this little rule 30 thing is doing a computation that's just as sophisticated as the computations that go on, for example, in our brains. Well, what consequences does that have? One consequence that has is this phenomenon I call computational irreducibility. So let's say you want to know what this, well, how this pattern is going to work out a billion steps later from now. Well, how do you how do you figure that out? One way you can figure that out is just to follow those billion steps and see what happens. Another thing you can do is to say, wait a minute, I'm much smarter than this system. I'm just going to jump ahead and I'm going to say, I know what the answer is after a billion steps. That's the thing we've become used to in doing, for example, mathematical science. You imagine an idealized planet orbiting a star. You say, do you have to work out where it's going to be a million years from now? Do you have to follow those million orbits? Or can you just use a formula and kind of fill in the number of million and jump ahead and see what the answer is? That kind of what we can call computational reducibility is what we've become used to from kind of what happens in mathematical science. But the principle of computational equivalence tells us that will not generally be what one can do. In general, the systems that we're studying will be just as computationally sophisticated as anything that we can muster in studying them. And so that means we won't be able to do that kind of jumping ahead. We won't be able to do that kind of computational outrunning of the system and will be reduced to something where to work out what the system does, we basically have to follow every step and see what the outcome is. So this is something which or kind of in a sense for science, it's telling when there's a major limitation on science. And by the way, this idea is something, things like girl's theorem, a sort of a special case of this idea and lots of other kinds of things that one knows about universal computation and so on is also related to this. But this is kind of a tighter version of those kinds of ideas and one which I think sort of shows one kind of the relationship of these things to science. And sort of the big consequences, there's lots of stuff that you won't be able to have a theory for, work out, jump ahead, know what's going to happen. You'll have to just follow every step and see what happens. And so in a sense, that's a limitation on science. From within science, one is seeing kind of a fundamental limitation of science. It's actually something which for many purposes might, one might not think of being a such bad thing, because in a sense, it's the thing that makes, for example, the passage of time meaningful. If it wasn't for computational irreducibility, then if you live for 50 years, then in a sense, that would not be, nothing would be achieved by that. One would be able to say, oh, I know what's going to happen in the end. I can jump ahead and say what the outcome is going to be. But because of computational irreducibility, there is something sort of really happening in the passage of time. It is a sort of an irreducible computation that's going on. There are many other consequences of computational irreducibility. For example, when it comes to things like AI, we can ask the question, what in so far as AI is doing computation? And we'll talk about the sense maybe later in which typical modern neural nets are doing only very weak levels of computation. But let's imagine that we have a system that is doing computation as computation can be done. Well, we sort of have a choice. Either we can say that system is we're going to make that system computationally reducible. So we know what the outcome is going to be. And so, for example, we can say we're absolutely sure this system will never do the wrong thing because we know its outcomes and we can constrain it to say that, to set it up so we can sort of prove that we'll never do the wrong thing. It's reducible enough that we can know enough about what it's going to do that we can know it isn't going to do the wrong thing. So that's plan A. But the problem with plan A is that means that the system can't do irreducible computations. The system can only do computations where we can jump ahead and foresee the outcome. So in a sense, that means we're crippling the system. We're preventing it from doing what it could do as a computational system. We're saying it's only going to do those things which are kind of reducible. So in a sense, I think it's going to end up being sort of a big societal choice is, do we want the AIs, computational systems, to be able to do all the powerful things that computational systems can do, or do we want to insist that they'll only do things where we can foresee what they'll do? And in a sense, it's kind of like we have a, you could say, well, I'm going to set up all these rules for the AIs that make sure they only do the right things. Well, to make that work, you have to have the AIs be sort of computationally reducible. If they're computationally irreducible, well, maybe you can constrain it in all sorts of ways, but there'll always be surprises. There'll always be things where you can't foresee that particular outcome. By the way, computational irreducibility has many, many consequences. But another consequence it has is that sort of science will never be finished. There will always be, if we think about kind of, there'll always be things where we can't foresee the next thing that will happen. There will always be surprises in mathematics. There will always be new theorems that can be proved and so on. The thing that is an issue there in terms of things like will science be finished and so on is, well, okay, there might be things that were surprises, but are they surprises we care about? If we were exploring all of mathematics, we would prove more and more and more theorems. But it could be that we get to the point where we know all the theorems we care about, and anything else is something we're not going to care about. So in a sense, it's a, there's sort of this, this connection to sort of human issues in what, but the point is that there is ultimately an infinite and unlimited frontier of what's possible to discover in science and so on. By the way, that also relates, maybe we can talk about, to things like, well, okay, let's, let's maybe talk about, so, so kind of this, this idea of computational irreducibility, that you can't know the outcome of a computational process in general, except by running it and seeing what happens. Limitation on science, thing that makes the passage of time meaningful, kind of dichotomy for thinking about AI and so on. It's the, so that, so let's see, one of the things that's sort of interesting about this is we can just sort of, in this computational universe, we'll find all sorts of, of things that go on. The question becomes, sort of, are those things that we find out there, things that we care about or not? In other words, we can go and we can, oh, I don't know, we can, you know, that's a, an example of just a simple rule and what it does and we can get the lots of other, lots of other examples we can, we can, we can go and do this ourselves if we want to, let's see, and just go find very simple rules that do very complicated things. It's easy to kind of launch out into the computational universe and find these things. The question ends up being, so how, what do we humans care about these things? Well, it could be that this particular thing, we will be able to use it for technology in some way. It could be that we'll think this is something very important for art, but it's something where out there in the computational universe, there's kind of an infinite supply of original things. The question is, which ones do we humans choose to care about? And, and for example, if we imagine kind of the, the, the future of AIs, you can say, okay AI, go out into the computational universe, you can go and create things that have never been seen before, all kinds of things. The question is, are those things that are of, of kind of human relevance to us now? Well, one thing you might do, you can actually do a little experiment here, let me show you something. Where is it? So for example, we could say we could take some image generation AI, and this is just a diffusion image generator. And we could say, let's, let's look, let's ask the thing to make a picture of a cat in a party hat. Okay, but inside the AI, it's got some, you know, embedding vector, it's got some, some set of numbers that describe that is its version of what that concept is. But one thing we could do is something very simple to sort of explore the universe of possibilities. We could say we're going to take this AI that's very aligned with human interest because it's been trained on billions of human images. But nevertheless, we could say, let's take this AI and let's sort of move around in this space of possibilities. And so for some set of numbers, we've got the cat and the party hat. But as we change those numbers, we're moving out from that. And we have this kind of in the middle, we have this thing we might sort of describe as kind of cat island, that is things that to us kind of look like cats. But then we go further away, and we'll get into things which aren't like cats. If we go far enough, you know, we'll, we'll be able to go, I don't know, as an example, we'd be able to go from, what is that going to? That's, well, okay, here's one that goes from a cat to a dog, we're going through through this kind of meaning space from a cat to a dog. But in general, what we'll find is that we in this sort of space of possibilities, there's this region that corresponds to this concept that we have of a cat and a party hat. But as we go away from that, eventually, we move far enough, we'll get to a picture of a, you know, a dog wearing a sweater or something. But we go through a large volume of inter concept space of things which are images, which were generated by this AI using, you know, computationally generated out there in the computational universe, even set up to be quite aligned with kind of the pictures that we humans have put on the web. But nevertheless, they're not things which are normally described by a word like a cat or a dog or whatever else. So you might ask the question, you know, in an image generation AI, what volume of the space of possibilities is covered by concepts that we have already defined? The answer is maybe one part and 10 to the 600. So in other words, there's this vast kind of inter concept space of possible images, only tiny corners of which are described by words that we have in human languages. So in a sense, as we look at this kind of inter concept space, we could say, you know, we don't necessarily have a word to describe some of these patterns, but we might say, oh, that's kind of a cool pattern. And maybe we decide at some point that that's a particular style of art. And eventually we get a word for it. And then we develop this whole kind of human interest in that particular piece of what was inter concept space. And now that becomes a concept in our languages and so on. So this idea is sort of this core idea that there's this huge space, this huge kind of computational universe of possibilities, even reduced here by ones that are sort of images aligned with images that we put on the web. Even if you reduce it in that way, the part of that space that we have so far explored, that we have so far come up with words for and described with concepts, is a tiny part of the space. And there's vastly more that is kind of be found in the sort of inter concept space. Now, what, you know, can we describe kind of the way that kind of we, we think about sort of our progression in kind of the progression of human civilization and so on. In some sense, you can think about us as progressively colonizing inter concept space. We're progressively coming up with things coming up with, we're coming up with sort of this social construct of language that that different ones of us sort of collectively understand, that corresponds to the, these different points in the space of possibilities. And sort of the progression of civilization, we can think of as being this progressive kind of progressive exploration of inter concept space. And, you know, as we invent new paradigms for things, we get to kind of, or new ways of describing things, we get to kind of move outwards in the space. Now, for example, in my day job of creating computational language to describe things, my, my mission in a sense is to find those, those places in the space of possibilities that we humans care about, and that we can use as kind of building blocks to construct kind of in a computational way, a description of what we want. But there's kind of a broader science of what's in principle out there, which is broader than the things that we humans have so far chosen to, to come up with words for and so on and have languages for. Well, just to kind of fill out a little bit, kind of the, a little bit more of kind of the, the world view that develops from all of this, we can ask questions about, okay, what about our physical world? How is that constructed? What is the, what's kind of the, the underlying structure there? And one of the things that's been very exciting to me in the last few years, something I really did not expect sort of to, to, to happen is that it's, it's turned out that we've been able to work out that how this kind of computational ideas provide sort of an ultimate infrastructure, an ultimate kind of machine code for the physical universe. And what, what, let me describe that a little bit because we're going to come back to this question of concepts and into concept space and so on, but we're going to come at it now from a different direction from understanding the structure of the physical world. So, sort of big picture, back in antiquity, people were arguing, you know, is the world discreet or is it continuous? Is it made of atoms or is it just things that are sort of flowing? And one didn't know. End of the 19th century, it became clear, yes, there are molecules, matter is discreet. A little bit later, became clear, there are photons like can be thought of as being discreet. At that time, people mostly assumed that space would turn out to be discreet as well. But for various reasons, nobody technically managed to make that work. And so physics kind of went on with the space is continuous, you can kind of put things anywhere you want in space. Well, if you're thinking about things in kind of computational terms, you're immediately led to say, wait a minute, you know, perhaps space is actually fundamentally a computational construct, fundamentally a discreet kind of thing. And the big surprise of four years ago now was that, yes, we actually managed to figure out how to make that work and managed to figure out how that connects to the big theories of current 20th century physics. And actually, the really remarkable thing that maybe I'll have a chance to describe is that the big theories of 20th century physics, essentially general relativity, the theory of gravity and spacetime, quantum mechanics, and statistical mechanics for the second law of thermodynamics, those are sort of three big theories of 20th century physics. It turns out that all three of those theories are not just things that we can kind of say, oh, that's what's true. There are actually things that we can in some sense derive from fundamental considerations. I had not expected any such thing to be the case that we could derive the laws of physics, so to speak. But we can and I'll explain how that works. And that's kind of loop back to questions about language and concepts and so on. But okay, so what's the universe made of? Well, in our models, the universe consists of a bunch of sort of discrete atoms of space, we tend to call them eames, kind of atoms of existence. They're things where the only thing you can say about them is they exist, and they have an identity, and they're distinct from each other. And then there's one more thing, which is you can say how these eames, how these atoms of space are related to each other. You can say this one is related to these two other ones. It's kind of like what atom of space is friends with what other atoms of space? And you define this whole collection of relations between atoms of space, and you can represent that by a graph, a network, or actually more formally in our models, a hypergraph. But the essentially one's just dealing with this big network of relations between the atoms of space. And so everything in the universe in our models is just made of the relations between atoms of space. So for example, if something like a black hole, for example, is just a structure in the, I might even be able to show you a picture of one, let me see if I can pull this up. This is actually in kind of the fabric of space. This is two little tiny black holes. And we'll see in this video kind of space, most of the activity of the universe actually is knitting together the structure of space. But there are two black holes there, and you can kind of see they eventually merge. They produce gravitational radiation. Actually, what we get from this model, where we're looking at kind of the discrete structure of space, we can successfully reproduce the actual things that are observed in black hole mergers and so on. But in any case, the basic point is what the universe is made of, everything in the universe is just a feature of the structure of space. And when it comes to time, time is the progressive rewriting of the structure of that network that represents space. So time is actually a very different kind of thing in these models from space. Things like relativity emerge as a feature of the model. They're not things that are put in from the underlying structure of the model. Okay, so we've got sort of the notion of space, notion of time. It turns out quantum mechanics is a thing that inevitably emerges from the fact that when we are updating this network, there isn't just one possible path of history. There isn't just one possible way that the network can be updated. There are many possible paths of history that branch and merge. And essentially, the structure of those things is what leads to quantum mechanics. Well, one of the issues is when we're looking at the system, and we're seeing all these rewrites and the structure of space and so on, the question is, how do we experience that? There are all these things microscopically happening, but we have a certain experience of that. And it turns out that sort of a critical feature of what's going on is that we are observers of a certain kind. So let's take the case of, let's look at, for example, let's see, let's look at something like statistical mechanics. We've got a bunch of molecules bouncing around in a box. And one of the kind of big principles is the second law of thermodynamics that says when you start those molecules off in an orderly way, their motion will tend to eventually look disordered and random. It will look as if it has higher entropy. And the question is sort of what's really going on there? And it turns out that what's actually happening, something I finally understood, I've been thinking about this for like 50 years, actually, is that what's ultimately going on, you can look at different kinds of versions of this, what's ultimately going on is that these molecules are bouncing around in a certain determined way according to some rule. And in fact, that rule can be reversed. So you can take this pattern of molecules you get at the end and you can say, I can figure out, oh, yes, that pattern of molecules came from the simple initial state. Well, in principle, you can do that, but it's a computationally irreducible process. And the difficulty is that we human observers of things computationally bounded, we can't do that, that, that all the computation that's needed to reverse what happens in the molecules, we're just stuck saying that we can, we can get this impression of what's going on. And with that impression of what's going on with that computationally bounded impression of what's going on, all we can say is, oh, it looks random to us. And that's kind of the ultimate origin of the second law of thermodynamics is something which is to do with the relationship between underlying computational irreducibility and our computational boundedness as observers. Well, it turns out that both general relativity and quantum mechanics come from the exact same thing. They both come from this idea that there is computational irreducibility underneath. But we are, well, actually, there are two attributes that we have to have as observers, that we are computationally bounded, and that we believe we are persistent in time. So in this model, for example, we are at every moment in time, we're made of different atoms of space, yet we all have the impression that we are experiencing things through that it's still us a second later, so to speak, and that we experience things we are persistent, we have a continuous thread of experience through time. Well, okay, so the really ultimately big concept here is this thing we call the Ruliad. And so here's how this works. When we look at these, this underlying hypergraph and its rewrite rules and all those kinds of things, we can, we say, okay, there are these underlying rules. And if we run those enough times, we'll eventually get something that seems like our universe that satisfies Einstein's equations of general relativity, that shows the Feynman path integral for quantum mechanics, all those kinds of good things. But we still might be asking the question, well, why did our universe get one particular rule and not another? And that had me very confused for quite a while, until I realized that actually we can think of the universe as running all possible rules. So what we imagine is that there are these possible computational rules that can be used to update this hypergraph and so on. But let's just imagine that we use all possible rules. What we get are all these different parts of history that branched and merge and so on, corresponding to the application of all these different rules. And this whole object that is the entangled limit of all possible computational processes, we call the Ruliad. And the Ruliad is a completely unique thing. It is, it is you take every possible Turing machine, every possible computational system, you run all of them, and you run them in such a way that they are producing kind of, that they don't just have one possible outcome, they have all possible outcomes. You might say, what an incredible mess, how could you ever conclude anything from this Ruliad object? It is the case that this Ruliad object is a unique thing, there's not, it's not like there's seven different Ruliad's, there's just this thing that is the entangled limit of all possible computations. And so then the question is, well, how can you conclude anything about, about this Ruliad object? Well, what you have to realize is the Ruliad object represents everything that's possible, everything. And so, for example, we, as observers of what's going on, we must be embedded within this Ruliad. And so what we can think of is that this, this was I sharing the screen or did I stop sharing? Well, anyway, the, this, so the issue is we are observers embedded within this Ruliad, observing the Ruliad. And the question is, what do we conclude about the Ruliad? And the Ruliad is a necessary thing, there's no choice about it. But the nature of us as observers is contingent, so to speak. And so what turns out to be the case is that observers like us, observers that have certain attributes necessarily conclude that necessarily describe the Ruliad in certain ways. So in a sense, by being an observer who is computationally bounded, who believes they're persistent in time, those two attributes alone are sufficient to tell us that the slice of the Ruliad, the way that we parse the Ruliad is exactly the way that corresponds to the laws of physics that we know. So in other words, what we're saying is you can derive the laws of physics, the laws of physics are derived by starting with this Ruliad, which is a necessary unique object, and then saying what, for observers like us, which happen to have the properties that we have of being computationally bounded and believing we're persistent in time, any observer with those very coarse properties will necessarily conclude that the universe operates according to Einstein's equations and the path integral and so on. So that's a rather interesting philosophical conclusion. Now you can ask, well, what would observers not like us conclude? Well, we don't know. You can kind of, and that's sort of a question of how do we think about observers not like us? Well, one thing to realize is we can think of in the Ruliad, we can think of different possible observers as being sort of at different points in the Ruliad. There are different places in Rulial space. Just like in physical space, we could be here on this planet, we could be on a galaxy on the other side of the universe, we can be at different places in physical space, and each different place in physical space will give us a different point of view about how the universe works. Well, so it is in Rulial space, each different place in Rulial space will give us a different point of view about how the universe works, how things work. So here's a way to think about that. We can think of essentially different minds as being at different places in Rulial space. It's as if, and these different minds are kind of experiencing possibilities in a different way. So if we think about that in terms of, you know, the LLMs and so on, it's kind of like we could imagine just having a differently trained LLM and that differently trained LLM basically exists at a different place in Rulial space. So for example, minds that are sort of similar and sort of similarly trained will be fairly close in Rulial space. Minds that are different, like, you know, let's say cats and dogs, further away in Rulial space. Minds, I tend to, I think that one of the consequences of the principle of computational equivalence that I mentioned earlier is that one could sort of attribute mind like things to lots of systems in the world and lots of abstract systems. And so for example, when one says the weather has a mind of its own, in the principle of computational equivalence says, yes, that's a meaningful thing to say. But in a sense, the mind that corresponds to the weather is pretty far away from us in Rulial space. Also now there's a question, how do you communicate across Rulial space? How do you, what is it what's involved in doing that? Well, at some computational level, one point in Rulial space corresponds to sort of computing, according to let's say one Turing machine, another point in Rulial space computing, according to another Turing machine, another computer. We know that in principle, we can make a translation from one place in Rulial space to another place in Rulial space takes effort. We have to actually create that interpreter that's going to interpret the instructions of one machine as the instructions of another machine. It takes effort in the same way as it takes effort to move in physical space. In a sense, when we move in physical space in our models, we're reconstructing ourselves at a different point in physical space. And by the way, you can understand things like time dilation and relativity. There's a nice kind of mechanical explanation of that. If you're always in one place, you're spending your kind of computation budget figuring out what the next behave what the what the next stage you'll be in is. But if you're moving, then you're using some of your computation budget to kind of recreate yourself at a different place in space. And so that's used up having used up some of your computation budget, you necessarily sort of moves go through time more slowly time time goes more slowly because you used up some of your computation budget in moving in space. But in any case, you can you can think of so so by the way, in our models, the possibility of motion is non trivial. It's not obvious that you can you know pick up a glass and move it somewhere, and it'll still be the same glass. That's something that we generally assume about the world that pure motion is possible. But it's something in our models that you have to prove that pure motion is possible. And even in traditional physics, if you're sufficiently near a space time singularity, for example, no no material object will maintain its identity as you move it around that that singularity. But in our models, the the possibility that that thing can just move, and that it's still the same thing is non trivial. And actually, in a sense, the the particles of motion are exactly the kinds of particles that we know about like electrons and quarks and so on. What is an electron an electron in some sense in an abstract level is a lump that is capable of pure motion. It's something where you can have an electron in one place, and you can move it and it'll still just be that electron. So it's it's a particles are kind of the carriers of pure motion and physical space. So here's a here's a thing in rural space, we can ask sort of what is motion in rural space about. Well, in a sense, what what it means to have motion in rural space is you're effectively transporting something from one mind to another if different points in rural space correspond to the positions of different minds, you're asking the question, what does it take to kind of transport things around rural space. And I think this is one of the very bizarre kinds of things that one realizes is it seems to be the case that concepts are the analog of particles. So what in physical space and an electron that doesn't change as you move it from here to there, in rural space, it's the concept of a cat, for example, that can be moved from one mind to another without change. I mean, the particular details of the neural firings that exist in my brain, when I think of the concept of cat, in any of your brains, the particular neural firings will be different. But yet, we can package up the concept of a cat, and I can say the word cat, I can transport it to you, and then you can unpack it. And in your place in rural space, you can end up with the same thing, so to speak. So it's kind of a way of understanding that that's sort of the fundamental thing that's going on. And we can think of kind of concepts as being the particles of rural space. Well, there are lots of lots of things I see I'm running out of time here. But there are lots of things we can talk about about what, well, let me just say a couple of other things about, I'll talk a little bit about AI. And I mean, the AI has had many different meanings over the course of time. And many things where people have said, if we can only have that, then we have AI are things that I've built as kind of pure computational systems. And then people say, well, it's just a computational system, it's not really AI. And in more than seconds, just one second, I want to, you do have more time, because for some reason, I can't explain, Caillou, who has been extremely conscientious in everything, is not here, which may mean that he had misunderstood being a discussant for being a member of the panel, which means he won't be here until the panel starts, in which case you have more time, if you wish. I just compressed four and a half decades into a remarkably short time, I hope people could follow it. Nothing, you could have taken place at this time. Well, okay, so let me let me finish what I was saying here, and then maybe we can turn this over to discussion, which is more fun for me. So talking about kind of modern AIs, and you know, to many people, modern AI is neural networks. And the, there's sort of a question of, well, what can, how do neural networks relate to all of the things I've been talking about? And one of the questions we can ask is, okay, we have, we have our friendly neural net here, let's see, oops, share the screen and then, okay, we have some typical trained neural net, let's say we're trying to train it, let's say we're trying to train it to reproduce the sine wave. So what we're doing is we're going to feed in the x value at the top there, and we're going to have set up these neural net weights, and it's going to compute the y value down here. And actually, we'll do a pretty crummy job of that typically. And you can, you can change the neural net, you'll get different kinds of behavior, it's usually not particularly good at computing something like this. Well, so one thing you can ask is, you know, neural nets, let's say if we have a big enough neural net, maybe we can break computational irreducibility, maybe we can just predict what's going to happen in any kind of system. That is not going to work. I mean, the way that a neural net of this type works, it's just having kind of numbers ripple through the sequence of layers. And we're ending up with something where you can, this is something trained, I used a modern transformer architecture and trained it to try and recognize what was going to happen in a cellular automaton. And it has certain, it says, well, there's certain probability of what's going to happen. But when the behavior is pretty simple, it'll nail it. When the behavior is more complicated, it's like, I'm sorry, I can't, you know, I can't figure that out. This is, this is different levels of training of one of those neural nets. So in a sense, it's not surprisingly, the kind of very finite computation of these layers of a neural net can't do the unboundedly large computation required to kind of solve a computationally irreducible problem. And you can see that again. See, where do I have an example here? This is, these are examples of the three body problem in celestial mechanics, Earth, Moon, Sun, all idealized, all with interacting through gravity. You can ask the question, if you train a neural net, can it correctly reproduce the behavior? The answer is the neural net is the kind of solid line here, that's its prediction. When the behavior is fairly simple, yes, it can do it. When the behavior is kind of computationally irreducible, no, it can't do it. None of this is really very surprising. But there's kind of the question, for example, when we look at something like chat GPT, and we say, oh my gosh, it actually worked, it produced something that is like human language. How did that work? What I think is the main thing going on is something which tells us a lot more about human language, probably, than it does about neural nets. Because what it's telling us is, if we think about how does chat GPT work, it's basically just saying, I'm going to predict the next word by figuring out certain probabilities. And it's going to do that by, at the very simplest level, it might just do it, let's see if we've got one here, might just do it by knowing the frequencies of different letters. And then, if you just use the frequencies of different letters, you get pretty much nonsense. If you use blocks of letters, you'll start getting more sensible kinds of things. If you use kind of whole words occurring with the probability that they occur in English, you'll get things that don't make much sense, but they're kind of things that can construct. Now, the big thing that's interesting and surprising is that when you kind of train a neural net from kind of all of the text, you know, a trillion words of text or something, that the extrapolations it makes about what make meaningful sentences tend to agree with the extrapolations that we humans would make about that. It's very similar to the fact that if we train a neural net to recognize cats from dogs and images, that the distinctions it will make seem to be similar to the distinctions we will make. At a theoretical level, if we say, where's the dividing line between cat pictures and dog pictures? There isn't a good mathematical characterization of where that dividing line is. It's really a question of where do we humans say is a dividing line between cats and dogs? And the thing that's interesting about neural nets is they tend to make the same kinds of decisions about that that we tend to make. Probably the reason is that ultimately their architecture is similar to the architecture of our brains. But the main point is that those kinds of distinctions, there's not a theorem, there's no theorem that says the neural net will reproduce the distinction between cats and dogs, because you don't know what the target is. The target is what do humans think is going on there, and it does a pretty good job at that. So now the question is, in the case of language, what's going on? And I think what's happened is that the thing that allows an LLM to produce reasonable language is something that is a regularity of language that we could have recognized a long time ago, but we didn't. And so we know certain regularities in language. We know that, for example, in English, you tend to have sentences that go noun verb noun. But there are plenty of sentences of the form noun verb noun that are total nonsense. So the question is, you have this kind of syntactic grammar of language that says that you go things like noun verb noun. But now you have the question of, well, what noun verb nouns actually make sense? And so what I think chat, you know, chat GBT and LLMs and so on are kind of showing us is that there is also a semantic grammar of language. There's also a construction kit, not only of what the parts of speech might be, but also what kinds of words they might be to have them make sense. And that's something that eventually kind of sort of expands up to write a whole essay and have these puzzle pieces fit together in a way so that the whole thing makes sense. So, you know, in a sense, what one's seeing and one can kind of look at, let's see if I have some pictures here. Maybe I have some pictures. Yeah, these are from, these are very ancient, actually, there's better ones now for GBT-4. What extent can you kind of imagine semantic laws of motion where you're kind of moving around in meaning space, and where, just like Newton's laws tell you in physical space, how you move from one, you know, how motion happens when in the absence of a force you just keep moving in the same direction and so on. So you can ask questions about rural space, and you can ask questions about kind of the structure of rural space and how that works. And I think we're kind of learning some scientific things from the operation of LLMs about how that works. Now, another question would be, so in other words, I think the reason LLMs work as well as they do is because there are a bunch of regularities in human language that we kind of didn't know were there and that we've never really codified. People started codifying these things back in the 1600s, for example, people tried to invent these so-called philosophical languages that would be kind of not specific to any particular language, but they would be things that sort of represent the meaning of things without the specificity of particular languages. Well, actually, I've had a project for a while now much more energetic to make what I call a symbolic discourse language, a language where just like in Wolfman language, we have this computational language that describes many aspects of the world. I mean, we might have all sorts of different sort of categories of thing that we describe in our language. And the question is, can we kind of describe all, can we describe sort of things that come up in everyday language? Can we describe those kinds of things in a sort of precise symbolic way? And I have to say that I can't say I've got the full answer to that, but it's going really well. And it's become clear, and by the way, LLMs are quite helpful in this, to having a way to take something not the level of language where we're actually putting words together, but the representation of the core meaning of what's going on. Just like in our computational language, we have that representation of sort of the core meaning of what's going on in a way that can be read by humans, but also executed by a computer. So in any case, that's sort of one direction about things with LLMs and so on. Another question that I was curious about is, okay, why does machine learning work at all? Why is it the case that you can train one of these neural nets to do something like, I don't know, recognize digits or recognize cats and dogs or generate language or whatever else? Why does that work? When I played around with neural nets back in 1981, and I couldn't get them to do anything interesting. And I kind of thought at the time, oh, if I've got a simple enough problem, I'll be able to get a simple neural net to do things, didn't really work very well, wasn't very interesting. The big thing that got sort of accidentally discovered basically in 2011 was that if you have a big neural net and you bash it really hard, you show it enough training examples, it'll learn, well, lots of different kinds of things, it'll learn almost anything. And the kind of the big meta discovery of modern machine learning is that if you bash a neural net hard enough, it'll learn almost anything. We don't know quite what the almost is. We can't really characterize what kind of thing it can learn. For example, as I said, it can't break out of computational irreducibility. So there's limitations to what it can learn, what it can do. But nevertheless, there's a broad class of things that seem to correspond a lot to kinds of things that we humans can do easily that the neural net can successfully do. And so that's sort of the meta discovery. The question is, why does that work? Why is it the case that this neural net can be successfully sort of bashed into learning things? Why doesn't it get stuck? Why doesn't it get to the point where you just can't get there from here? You can't arrange it. Why is it the case that it's possible to do it and then why is it the case that you can iteratively do it by sort of adaptively training it? I got interested in this very recently, actually. And I don't know whether I can show you pictures. Let me see. I can show you some things that I did recently. And then maybe I'll be able to pull up some pictures just from the last few days that let me see here. Right. So actually, I decided to look at a simpler problem, which is the problem of biological evolution, which is sort of another case of adaptation that is a little simpler than neural nets. But let me explain what I figured out about biological evolution. For a long time, I wondered what sort of the minimal model of biological evolution was always very unsatisfied because models, you know, natural selection seems like a simple principle. But when you actually try and make explicit models for it, you end up with all kinds of hair about, you know, how many sub, you know, suboptimal organisms do you keep and all this kind of thing. So I was interested in sort of a minimal version of that. So here's a version of that. This is actually one of these cellular automata. It's got these rules here, starts off from one red cell here. And with these particular rules, you get a pattern that lives for this amount of time and then dies out. Okay, so let's imagine that you're interested in doing something where you just keep on tweaking the rules, you keep on resetting the rules, you keep on making single point mutations in the rules to try and get it to live longer and longer. This is what happens. You start off from something that that is just a blank rule. For example, it dies immediately, you keep tweaking the rule, you have to go through many different tweaks and so on. But eventually you'll get to the point where it lives there for 50 something steps. Well, and you can see that the sequence of mutations that got made there. And if you look at how the fitness of this organism, the length of time it lived, varies as you go through all these different sort of steps of adaptive evolution, you'll see there are, you know, it's going along and there are many things that don't work out. But, you know, it'll kind of cruise along here at a certain fitness, and then it makes a discovery. And then it can go to higher fitness. And actually, you can end up with all kinds of discoveries that it makes. These are different sort of paths of evolution. And you'll see that, for example, here, it's kind of going along and eventually it manages to discover a lot. It manages to live a long time. You could sort of imagine in the fossil record, you might find, you know, a critter from the Cambrian period that looks like this. And then it uses that idea to extend further. And by the time it's in the Silurian period, it's looking like this. And maybe it makes it to this in the Triassic period or something. But what's happening here is that there are sort of, it's having progressively more ideas, in a sense, about how to live longer in this particular case. And actually, you can even go ahead, this is a simple enough system, that you can actually work out. Let's see, that's an example of a better example. There we go. This is, this is the path of all possible paths of evolution for a simple system like this. So every different picture here is a possible organism. And the arrows show the possible adaptation paths. And what you see is something that's very much like what happens in biological evolution. There are different branches in the tree of life. There are, you know, one set of ideas leads to long life over here. In this way, a different set of ideas leads to kind of long life over here in a different way. Okay, what does this have to do with machine learning? Well, you can, you can ask the question, let me see if I can pull this up. I am going to have to pull up something that I just made. So I'm not sure whether I can find it here. Hold on, you can get hot off the press or not really off the press at all. Where is it? Let me see. Maybe. Maybe this will have it. Oh yeah, this is, this might be it. This is a very minimal model for, let's see if I can get this bigger. It's a very minimal model for a neural net where it's actually a cellular automaton as well. But instead of having a fixed rule that it keeps on applying kind of like a recurrent neural network, it has something more like a feed forward neural network where you have a discrete choice of one of let's say two different possible rules. And at every point in space time, so to speak, you're picking a different rule. And so then the learning consists of, well, what's the pattern of rules you should pick to get a particular outcome. In this particular case, we're trying to learn to live as long as possible. And what's interesting here, and again, this is just raw off the literally raw material that from a couple of days ago, this is kind of showing in a sense how the thing does what it does. So in a standard neural net, it's just much more complicated to display what's going on. You've got these neurons with continuous weights and you've got connectivity all over the place and so on. This is a much simpler case. So you can kind of see more about what's going on. What's non trivial is that training actually works in this case. And it does, you can find this arrangement of bits that will cause the thing to do, I don't know whether I have it in this example here, but that will cause it to learn, see if I have one here. Now those activation levels, well, it doesn't matter, but that will basically cause it to learn something like, you know, to tell whether the number of bits at the beginning is even or odd or something like this. And we actually even tried training this on the MNIST training set, and it doesn't do too badly. So the point here, the thing that's interesting here, these are all different solutions that this kind of very idealized neural net found to living for this exact number of steps. What's interesting about these is they're very bizarre. They're not sort of engineered solutions. They're not solutions where we can say, oh, yeah, let me look inside and see how this works. Let me show you another example of that. And this is more back to the biological evolution case. This is kind of all the different ways that a certain class of systems manages to live a long time. And some of them, it's kind of pretty structured. You can imagine sort of this was an engineered thing, but some of them, it's like it just seems to sort of happen to live that long, and then it dies out. So in other words, there's a lot. And by doing this sort of adaptive evolution, you're ending up finding these things which are very not, they're not mechanical, they're not engineered kind of ways that things work. They're things where kind of this is sort of what's happening inside. This is the thing that's going on, but it's not something where you can say, oh, I've got a mechanism. By the way, if you're interested in neuroscience, this is something you should pay attention to, because in a sense, if you're trying to explain what's happening in the brain, and you say, oh, I'm going to figure out how this works. Well, how this works is an attempt to have kind of a human understandable narrative for what's going on. But if I were to look at these pictures in the background here, if this was something going on in a brain, I might be able to say, okay, I can have some human narrative about what's happening here. If this is what's going on in a brain, it's just, well, it happens to work that way, and it happens to give this result. It's kind of a computational irreducible story. It's something where there's no sort of narrative mechanistic explanation. It's something which just works that way, and it's computationally irreducible, but it just comes out in that fashion. And I think there's sort of an interesting question for in machine learning. Why does machine learning work? Okay, so let's look at, was a nice picture of that. By the way, this is in biological evolution. People often talk about fitness landscapes. This is an actual fitness landscape correctly drawn, so to speak. And you can start seeing all kinds of things about things evolving on fitness landscapes. But the thing I really wanted to show you, here it is. This is kind of the local behavior at a particular point in rule space at various steps in the adaptive evolution. So what's happening here is at this step, for example, in the adaptive evolution, here are different possible directions in rule space that you might go. And the ones inside the circle are ones that are losers relative to where you've already got. They're ones that would be live less long than what we have here, but there are some that would make progress. And in fact, this is the one we happened to choose in this particular random sequence of adaptive evolution steps. And that was the thing that made progress. So the thing that is not obvious is in this sort of high dimensional space of possible ways you could go, the question is, will you always be able to make progress? Will there be a direction that makes progress? Or will you get stuck? Well, I think that this is again a computational irreducibility story that basically what would make you get stuck? Well, if the structure of this rule space was very orderly, very reducible and easy to predict, you might end up in a box with very precisely defined walls and you just can't escape from that. But the presence of computational irreducibility kind of implies a certain degree of unpredictability, a certain degree of intrinsic randomness effectively in the structure of rule space. And that's what means that in these high dimensional spaces, there's always a kind of a path to success. So in a sense, I think computational irreducibility, which is a limitation on what one can do with, for example, a neural net, what kinds of things computations one can expect to do, is also the reason that training of neural nets, for example, can work. And so I think that's a, anyway, this is still an in progress kind of investigation. But I sort of think it's an interesting connection between a lot of different things I've talked about. All right, I've gone on longer than I intended to. So let me wrap up there and I'm happy to have a discussion, questions, whatever else. I just fed you an awful lot of material. Yes, good. Let's first applaud this president. The problem was that Kaiyou had an emergency during your talk, so he couldn't hear it. Kaiyou, do you think that you have, from the background material you might have looked at, you have a basis for saying something? Sorry, I missed most part of the time, so probably. If you haven't seen, this is a large amount of material. I would be surprised if we could have a useful conversation without having some anchor to this. Could you please explain to Kaiyou and to me and to us how computational irreducibility differs from ABC, the commagor of complexity, the church-touring thesis, and NP completeness. Okay. All right, let's start off with Chetan Komogorov complexity. So when we look at a picture like this, the algorithmic complexity of this picture is tiny. That's the program that's needed to produce it, just a few bits. The thing that is remarkable is that even things with very low algorithmic complexity are very complicated. In fact, they're complicated enough that to us humans, we wouldn't even be able to distinguish them from things that have high algorithmic complexity. One of the things I've had a long-running discussion with my friend Greg Chetan, where the question is, is the universe like Pi or like Omega? Omega is this thing that Greg invented 50 years ago, actually, that is the halting probability for a universal Turing machine. It's a fundamentally non-computable object. It's an object with infinite algorithmic complexity. It is a thing where there is no small program that you can't specify it by a small program. Pi, on the other hand, is a thing that is specified by a quite a small program. Once you generate its digits, it looks for all practical purposes random. So the question that one can ask is, in our universe, is there anything of high algorithmic complexity? Or is the whole universe actually a thing that is like Pi generated from something which is a very simple underlying sort of program? And in our model of physics, the answer is the universe is like Pi. The universe is something that is generated from a thing of very tiny algorithmic complexity. So that's kind of the distinction between everything I'm talking about is things of incredibly low algorithmic complexity. The remarkable fact that is not obvious, it kind of breaks one's intuition, is that things, very simple programs, things of very low algorithmic complexity can produce what seems to us like great complexity. And the seems to us becomes much harder when we start talking about our computational boundedness. It's not the case that it's just, oh, it seems complicated. It's that for a computationally bounded observer like us, there is no way to compress it. So in algorithmic complexity, algorithmic information, when saying this is the program and there is no shorter program, one can say for a computationally bounded observer, this is the set of bits and there is no way to make it shorter. So that was algorithmic information theory, algorithmic complexity. I think the second one you had was, what was it? Church Turing. Okay. So, okay. The thing that, if you go back to the beginning of the 20th century, and you'd wanted to get an adding machine, you might go to a store, you buy an adding machine. You want to get a square root machine. Okay, you go to a different store, perhaps, and you buy a different machine that is the square root machine. The big discovery that actually originally got made by Moses Schoenfinkel with combinators in 1920, but nobody understood it then or since, basically, but then kind of got clarified by Turing in 1936 is that there exist kind of, there exist systems that are universal in the sense that you can have a single piece of hardware that by feeding it different initial conditions, by feeding it different inputs, you can make it compute different kinds of things. So, for example, you can have a Turing machine that has some, where just by feeding it different initial conditions, it will emulate any other Turing machine. So, for example, if we go to, in terms of Turing machines, there's some, I'm going to show you something. There we go. Well, so, the big point is there exist machines that are universal in the sense that they can at least emulate all other machines of their type. The thing that then became clear, starting in the 1930s, is that Turing machines, you can have a Turing machine that emulates every other Turing machine. It also, by the way, can emulate every register machine, every piece of lambda calculus, every combinator and so on. So, there's this notion that in the class of computational devices, there's a certain degree of universality. People had not thought that that extended to physics. That was the thing that basically was my effort in the 1980s was to kind of imagine that this notion that what is computationally computable would also be what is what can happen in physics. People had sort of assumed that physics kind of breaks out of kind of this computational paradigm. It has real numbers, precise real numbers, has other kinds of things like that. So, the first thing is the realization in the principle of computational equivalence. The first thing is kind of the claim that, well, first part of it is sort of the physics part that, yes, actually, in the physical universe, this is all we've got. We can't say, oh, we're going to make an analog computer that jumps beyond kind of the church Turing level. Second point is this. People imagined that to make a universal machine was a complicated matter. It was something that would be kind of, you have to build this whole microprocessor. It might have a billion gates in it. It has all these instructions. It's got if statements. It's got all this kind of structure. And the question is, well, what's, you know, is that really necessary? Or is universal computation actually something much more naturally occurring? Is universal computation a special thing? Have to go to a lot of trouble to get? Or is it something that's just sort of lying around the computational universe? One of the big points to the principle of computational equivalence is, yes, it's just lying around the computational universe. So, for example, if we look at these different possible rules here, some of them behave in such simple ways that we can readily see what they're going to do. They're computationally reducible. There's nothing more to say. But some of them behave in a complicated enough way that we're kind of not really sure what they do. Let me show you an example of one of those. So, this is, let me show you rule 110. This thing actually only grows on one side here, but just show that. I shall make it, just show just the part where it's growing. So that's, after 200 steps, let's run it for 1,000 steps. Okay, there it is. It's a little bit unclear what it's going to do. This is kind of computational irreducibility in action or undecidability, ultimately in action. What's it going to do? Is it going to have all those little things, structures? Are they going to survive or are they eventually going to die out? After, I think it's about 4,500 steps, they do eventually all die out and they just get left with this one single structure here. But this kind of computational irreducibility in action, you can't tell what's going on. This particular rule turns out, if you just look at it, let's start it off from random initial conditions. Let's say 600 across. No, let's say 1,000 across. And let's say 800 down. Okay, oh boy. It's a little bit on the screen here. Let me make it a bit bigger. Okay, there we go. So what you see there is that's the initial condition. This is what happens. What you see is a bunch of little structures here. And you might imagine as you look at these structures, oh, they're kind of interacting and maybe that's like a logic gate and maybe we can make an OR gate out of this and so on. But it turns out with considerable effort, you can do that. And you can show that rule 110, which is kind of just the 110th rule in this very simple enumeration of possible rules. It's universal. The first one that you might imagine could be universal is actually universal. So in a sense, you know, the church touring thesis is saying it is possible to have a universal machine, at least universal within the class of computational devices that we're talking about. The principle of computational equivalence says not only is it possible, it's also generically the case it is ubiquitous. And in fact, it goes on to talk more about individual computations rather than programmability, but that's kind of a bonus. By the way, in terms of touring machines, I was very curious what is, you know, if you just look out in the space of possible touring machines, just start enumerating touring machines. The first one whose behavior is not obviously simple as this one here that I found sometime in the 1990s. And so then I was really curious, is this in fact a universal machine in 2007? I put up this little prize and a chap called Alex Smith managed to show that yes, this particular touring machine, the first conceivably universal touring machine actually is universal, which is a nice piece of evidence for the principle of computational equivalence. So that's kind of the relationship between church touring and principle of computational equivalence. And computational irreducibility is something that is sort of, it's made tougher by the fact that this property of universality is ubiquitous in the computational universe. Okay, thank you. MP completeness. What's that? We have a question here already, so. Wait a second. You get me. My software is not empty yet. So you asked about NP completeness. So let me try and address that. So normally, in a touring machine, for example, you have this touring machine, it has a rule, you started off from some initial condition, it just evolves in some specific way. It has a specific history. But you can also have, let's see if I have a picture of this. I have a bunch of these multiway systems. Well, here, let me show you a tag. Find some multiway touring machines. There we go. Multiway touring machines. Okay. So that's a typical touring machine. It has a rule, it evolves in a particular way. But you can also have a multiway touring machine in which there isn't just a single possible path of evolution, but there are many paths. So you can end up with this kind of branching structure. This turns out to be closely related to what happens in quantum mechanics. That's a separate issue. But so this idea of NP completeness, NP problems versus P problems and so on, it's this question. If we have a touring machine and it computes something and it takes a certain amount of number of steps to compute it, an ordinary touring machine might take n squared steps to compute a size n version of some problem. But we can also have a non-deterministic touring machine. We can have a multiway touring machine that follows many different possible paths. And we say, if we have a path that gets to the answer, then it's a winner, so to speak. And that's the story of NP problems, non-deterministic polynomial time problems, ones where there exists a path in this multiway touring machine, which gets you to that answer. So in a sense, this question of NP, sort of the big question, is P equal to NP? Is the class of problems that you can solve with in polynomial time with an ordinary touring machine, the same class or different class, than the ones that you can solve with a non-deterministic with a multiway touring machine? And actually, that question, so, well, let's see. I mean, we can talk about computational irreducibility and its relationship to computational complexity theory in general. But NP completeness in particular, there's perhaps a more interesting thing to say, which is, if I can find this one. Okay, so we can look at all possible touring machines. This is in a sense in rural space. Where is this? Nice picture somewhere here. Do I? Yes, here we go. Okay, so this is a picture of kind of the behavior of all possible multiway touring machines. So in a sense, all possible programs. And this is showing sort of all possible non-deterministic programs. The red part is the part that's showing deterministic programs only. It's not allowing the possibility of the rules changing, so to speak, as you go through the system. So the P equals NP problem, one of the things that's pretty interesting that comes out of our physics project is essentially a geometrization of the P equals NP problem. That is a question of the structure of these objects in rural space, that P equals NP becomes the question of whether essentially the red bit here eventually fills out the gray part of this picture. So you can kind of have a geometrical version of this ball in rural space that corresponds to the P problems and the NP problems. So that's a little bit of an indication of that. But you can, I mean, this whole question about non-determinism and so on, it's a, oh gosh, there's much to say about that. I've studied this a lot because it ends up being sort of a proxy for quantum mechanics. What happens in quantum mechanics is that you are following many paths of history and the observer in quantum mechanics is effectively a sort of an interesting situation. The observer is branching in the same way that these actual paths of history in the universe are branching. So quantum mechanics becomes this question of how does a branching mind perceive a branching universe? And so it's interesting to kind of see a bunch of different examples of multi-way systems as a way to get sort of more intuition about that. Okay, another question, apparently. Well, let's bring it back to our universe just for now. I want to, if possible, in a few minutes left, because I don't want to say anything. I want to make a connection between what you said and what Kyle, what Kai you said before. His program was related to computer aided proof and transformation of verbally stated truths and conjectures into formal form called auto formalization. And my question to you is what does the irreducibility principle say about all the possible theorems and all the possible paths to their solution? And Kyle, you can say a couple of words in response, but you have to say it in your own words first because you missed your talk. Well, let's see, I wrote a book recently about the physicalization of metamathematics, which I think is pretty relevant to this. And so, you know, we can imagine some, let's see, where's a good example? This is that might be some axiom in a mathematical system. And we can ask the question, what are the consequences of that axiom? That axiom is saying x dot y is equivalent to y dot x dot y. And now we can say, well, what things are also equivalent based on that axiom, we can start figuring out. So every path here is a theorem, that that's equivalent to that. We can start just following, we can start making this network of all possible equivalent things. And actually, and so a proof becomes a path in this whole network. And actually, it's a little bit trickier than that when you start looking at, there's a good example here. The way one actually does, let's see where I've got a good example. Okay, so this is an example of what more is actually what's happening in mathematics. You have basically, let's say two axioms here, and you are combining them to get a new theorem. And so you can kind of build up this kind of this structure you get with those two green axioms, you're, you're deriving all those theorems, you get this big network that represents all possible theorems derived from a particular set of axioms. So you can go on, you can get pretty complicated versions of this, you can derive all sorts of theorems that are true based on certain axioms. And what's happening is in this, in this graph, the every blue dot is a theorem. Okay. So then you can ask the question, if you look at, do I have an example of this? If you look at actual axiom systems in present day mathematics, so for example, you can look at, there's the axiom for semi-groups. You can start proving theorems about semi-groups. Okay, so we've got this whole network of theorems about semi-groups. And so one big question is, if you do that, well, okay, so here's, here's an example based on the axioms of Boolean algebra. So this is proving theorems based on axioms in Boolean algebra. And you can go and you can build up this giant network of theorems of Boolean algebra. And this is, this is kind of the, the enumeration of all possibilities. Okay. So now the question is, is we enumerate all those possibilities? Where are the theorems we care about? We've got gazillions of theorems that we can just build out eventually. And this is kind of spoiled. If you didn't know, you don't haven't followed this, but this really odd object that I talked about in the talk I gave, that is the ultimate limit of all possible mathematics is this really odd object. And the question of what theorems are, so all these theorems here are true. All these theorems can be constructed from the axioms. But these are the only two theorems that people give names to in textbooks of logic out of this collection. We can keep going. We can find, we can go to, to lots of other theorems. If we, if we use a theorem proving system, we can, in that big giant explosion of possible theorems, we can go and say, this is the theorem we're searching for, and we can find a path to it using, using theorem proving. And those are the paths in that, in that structure of, of, of possible theorems. Okay, so one question then is, well, out of all these possible, this, this complicated network of all possible theorems, where are the ones that we humans care about? And so I looked a little bit at that. And so, for example, you can, you can look, well, that's, that's, for example, that's Euclid. So Euclid has 465 theorems. And you can start off from the axioms at the top. And you can see what are the connections between those theorems, according to the proofs in Euclid. Perhaps more interestingly, you can take a proof of existence system. I looked at lean. I looked a little bit simpler to look at the system called metamath, which is a formalized math system. And you can ask questions like, well, that's the Pythagorean theorem, proved from the axioms in metamath. And you can see they're a different, it's a pretty complicated thing. This somewhere, I think at the bottom here is the Pythagorean theorem. And you start from the axioms there, and you can kind of count up of the various axioms, you know, how many times did you use the axiom of equality? Five times 10 to the 31 times. This is a, you know, this, this is kind of a, a, this is what happens if you start from sort of the axiomatic foundation, and you build up to something like the Pythagorean theorem. So, okay. So one interesting point here is this is the axiomatic sort of structure of the Pythagorean theorem. The question is, do mathematicians care about this? So, you know, a decade ago, I was very interested in formalization of mathematics. I organized this conference. We invited all these formalization of mathematics people, all these people interested in mathematics itself. The formalizers all showed up. The mathematicians didn't show up. And so the question is, what does a working mathematician actually do? You know, working mathematician who's, who's thinking about the, you know, the Pythagorean theorem, are they thinking about it in this kind of axiomatic way? Are they drilling down to kind of this, this low level axiomatic structure? Or are they just saying it's the Pythagorean theorem and I'm going to do things at that level? The thing that's pretty interesting and relates to a lot of what I was talking about is at this kind of axiomatic level of mathematics, it's kind of like molecular dynamics in a, in a fluid. You've got all these molecules bouncing around. They're doing all these complicated things. But then at the higher level, at the more human level, what we get to see is fluid dynamics. And we can ask the question, can we make conclusions at the fluid dynamics level? Or do we get dragged down to the molecular dynamics level and have to address things at that level? So in the question of, you know, using the Pythagorean theorem, for example, do you need to go down to the level of these axioms and worry about how you define the real numbers and so on? Or are you actually operating in practical mathematics at a higher level, at this level, where you're, you're just operating in terms of, of, of these kind of sort of fluid dynamics concepts? So I have to say, I was curious in, you know, in the, in sort of the world of LLMs and so on. So I will say that the mission of taking kind of a piece of informal mathematics and formalizing it seems like a fairly, fairly promising use case for things like LLMs. I don't know whether the formalization in terms of, you know, existing proof assistance and so on, I don't know how useful that will really end up being. I was curious whether you could use LLMs as a way to, as a way to, to kind of guide theorem proving. So I looked here at, here we go. May I make another suggestion instead of looking at, let Kai answer, just a second, because there's so little time now we can continue in the back. Please. Am I, let me just ask a very practical question. I have another thing in 30 minutes. Am I, I could push it back, but am I going to make that or not? Yes. The other thing in 30 minutes is the plan, is the panel where you, where you plan, Oh really? The panel, are you going to do something else? Well, I don't know. It really depends, but go ahead. Danielle, Danielle said that you were going to do both. Okay. All right. I think she, if you got the other thing from Danielle, it is in fact the panel. No, no, no. It's a completely different thing, but that's okay. We'll, we'll, we'll tell you a panel. That's great. Say this. Yeah. I think the short answer is yes. Yes. People are using informal mathematics and auto formalization, which means translating informal to formal to guide theorem proving. For example, given a proof, you can use language model like GPT-4 to, you just ask it to generate informal proof or even a sketch, some ideas of, could be high level idea of how this proof might go. And then conditioned on this informal sketch, you, there will be a second step to generate the formal proof. And, but I think a caveat is if you rely on auto formalization to give you the proof, it only works if human already discovered this proof. Because then there's no, if not, if it's completely alien to mathematics, then there's nothing for you to auto formalize. I think another direction related to what Stephen mentioned is, how can we even take one step further? Can we use language models to generate conjecture? Like generate the huge graph Stephen was mentioning. But of course the graph, I think it might be infinite or it may be simply too big. So a really interesting question I want to maybe learn from Stephen is say we want to generate this graph, but how do we tell if a node is worthy? Like if a math statement is interesting, because I imagine in this infinite graph, most of the nodes will be just garbage, like two greater than one, three greater than two. But we really want to focus on this interesting nodes. Yes, it's an interesting question. So I've looked at this a bit and I can tell you that in the case of Boolean algebra, there is a criterion. So maybe I can pull up a picture of that. If you order, here we go. Hold on. Let's see. If you order the theorems of Boolean algebra in lexicographic order, then you can ask which are the theorems of all possible theorems? Which ones are given names in logic textbooks? And sort of a surprise to me is the theorems that are given names in logic textbooks are the theorems that have, in this case, no back links. So there's a backlink from this result here, which might be one of your boring results. This result is derivable from something earlier in this lexicographic list. So in a sense, it gives you no new information. It turns out the ones that get given names are precisely the ones that do not have back links. They are not derivable from lexicographically simpler theorems. So in other words, I was surprised that I discovered this sometime in the 90s. I was surprised by this, that there was actually a criterion for what would be given a name in a logic textbook. Now, the general case of is this theorem interesting? Can we learn enough about the humans to know what they'll think is interesting? It's a good question. I mean, by the way, in this connection between formal and informal, obviously, Wolfram language gets connected to LLMs. And we've done lots of work in kind of tool calling from LLMs to Wolfram language. And there's this whole question of, can you take, to what extent can you get the LLM to crisp things up to the point where you can, I don't know, I mean, if I say something like draw a pentagon and a hexagon, for example, let's see what it does. I don't know if it'll figure it out or not. It might be able to, we could, the question is, can we generate a, can we turn that informal statement into a piece of formal Wolfram language code? Okay, not bad to manage to do that one. And if we look here, I'm sure we can get it to, there we go. So that showed us the actual code that did that, wouldn't have been the way I would have done it, but it's okay. That's a reasonable way to do it. But so this is a case where we're going from an informal description to this computational language, which we can then compute from. And that's a very powerful thing to do. And in fact, we even have a product that's coming out soon that is based precisely on that idea. But so I think this question of whether you can sort of, can you guide the proof this way? I suspect you can. Now this question of what is a human proof, what's, okay, this is an example. So in automated theorem proving, one of the shocking things about automated theorem proving, I believe you might correct me and tell me, one day somebody is going to tell me I'm wrong about this. But so far as I know, essentially all the theorems that have been proved by automated theorem proving were theorems that somebody already believed were true. In other words, there is no newly discovered thing that came from automated theorem proving with one counter example. The one counter example is something I found 24 years ago now, which is this is the simplest axiom system for Boolean algebra. So you can think of that as a NAND operator. This is of all possible from that one axiom, you can derive all the true statements of Boolean algebra. The proof of that is this long 100 step automated proof. Let's see if I have a picture of it. Yeah, I mean, that's sort of some kind of visual representation of that proof. It has various popular lemmas in it and so on. In the last 24 years, despite quite a bit of effort, actually, nobody has ever understood this proof. But it is interesting because it is a proof of something surprising, potentially interesting, depending on whether you care about simplest axiom systems for things. But it was found by automated theorem proving without already knowing what you were searching for, so to speak. And that's a case where now you ask the question, if you're out in the wilds of, I mean, we can look, I have a nice picture of this, we can look at, oh, here, is that one? Yeah, this is, these are axiom systems down the left. Those are theorems across the top. And there's a dot, a blue square, whenever that theorem is true in that axiom system. So we can ask the question, given an axiom system that we decide is exciting, and how we decide that is an interesting question, it's unright. But given an axiom system, we can say, here are the theorems that are true. Now, which are the theorems here that we care about? And that's essentially a model for humans. And I think it's an interesting question. We've done some experiments kind of grinding up archive and so on, and trying to figure out, can we deduce what, in a space of possible theorems, what theorems are likely to be interesting? I don't know if you've looked at that. Is that, I think that's an interesting thing to look at. Have you looked at that? Yeah, I think the way I'm looking at it is more, so I kind of am not considering its relationship with other theorems. I'm taking the theorem statement itself and try to have some, for example, have the language model telling me whether it's interesting. I believe the language model can look at some kind of superficial cues, like how long the theorem is and what are the variables, how they are arranged, how messy it is, and which can already give us some way of judging how interesting it is. But I agree, like maybe ultimately what an interesting theorem is, it can help you prove a lot of other theorems. Is that your definition of an interesting theorem? I'm not sure that's right. I mean, in other words, that is one possible way, I guess. There are many different criteria you can imagine for interestingness. That particular one would be saying that if that was the correct criterion, then what you could do, like I was just showing that picture, actually, in the Boolean algebra case, wherever it is, then I would deduce from your statement that the theorems that are big here are the ones that have many, those are the high-out-degree theorems. In other words, those theorems, that theorem there should be the one I should care about. I don't understand these theorems, honestly. What's that? I mean, in that sense, yes, because they are special in this graph. That's right. But the question of whether those are human useful is, I think, a different question. I mean, in other words, what is, you know, there's this question. It could be the case that two, okay, first question is, if you look at many different possible things you might prove, you can ask the question, are there repeated theorems that often, are there repeated lemmas that often come up in those proofs? Okay. So I looked at that. And the answer is there are. And so, for example, that axiom system of mine for Boolean algebra, if you use it to prove theorems in Boolean algebra, you can just look at what intermediate lemmas does a theorem prove are typically proved to make progress. And the answer is, for example, it takes it 100 steps to prove the commutativity of NAND, and it often does that and then goes on and does other things. So in that sense, you know, you can, I mean, it is an interesting experimental question. To what extent are there repeated lemmas that show up? And that might be a criterion, but that's a criterion that has nothing to do with LLMs and so on. That's a criterion that just has to do with the mathematical graph. I think this question of, you know, if we look at images, for example, you didn't see what I was showing earlier, but here I'll pull up a, I was just showing something like this. This is in, you know, embedding space of a generative AI system with in the middle is the cat in the party hat, then there's sort of a cat island of cat-like things, and then you're out in sort of inter-concept space that we have not yet explored. And so you can imagine the same kind of thing for mathematics. You say, here's a theorem that somebody wrote down. Let's sort of change the embedding in some sense and say here are nearby theorems that weren't necessarily, you know, where is the island? How far out does the island of interestingness go? What happens in this kind of inter-concept space between this theorem that we thought was interesting and this other one we thought was interesting? So I mean, I think it's a, I mean, let's take an example. Let's say, I don't know, let's take, well here, we've got, you know, these are random pictures generated in inter-concept space that maybe are of things that we care about. I don't know. I mean, that one on the right, we might kind of think it's, I don't know what it is, but you know, it was just generated by a generative AI. And similarly, imagine that was a theorem. The question is, is this a theorem that we care about? You know, it's just like, is this a picture we somehow seems relevant to us? And I think, you know, this question of whether, I mean, if we look, one of the things that's sort of interesting that one can do is to kind of look at this whole space of, let's see, we can kind of look at metamathematical space and we can kind of ask, let's take a look here. I think I had a nice picture and find it. I mean, we can ask all sorts of questions about different possible proof structures, which are, that's a meta mathematical thing, but let's see if I can find a picture here. Yeah, that's a picture of, I think this is from metamath. This is empirical metamathematics. It's asking, in the space of all 200,000 theorems, discussed in, you know, presented in the metamath corpus, where do these famous theorems of mathematics lie in that space? So it's kind of asking this question that this isn't all possible theorems. This one is reduced to just the ones that are in the, I think it's set dot mm corpus for metamath. But, you know, this is again related to this question of where are the ones, where are the ones one cares about, so to speak. And you can kind of, well, you can kind of see, this is kind of how the different theorems and different areas of mathematics kind of get related to each other. But I think it's a really interesting question. What, you know, and you mentioned that, I mean, I suspect LLM is a really good at picking up on cues from humans. And so I'm sure there's ways that people will write their master theorem. You know, they'll make, there'll be more trumpets blaring when they present the master theorem in their paper than when they present a little lemma. But here's a good question. Here's a question. If you try and make this, this is an easy thing to test. Okay. Can an LLM classify, given a statement, can it decide whether the, whether the author of the paper will have called it a theorem or a lemma? Well, I would guess yes, because there are different subtle cues, but I didn't try. What I tried, what I did try is I gave LLM some inequalities, like very simple elementary inequalities. Some were written by humans from the problem sets, from MO problem sets, for example. And others are just generated randomly by machines, which typically look very messy. And LLMs can do a reasonably good job at that task. Although I would say that task may not be very difficult. Well, okay, so we've tried to do things like this, because we've been interested in automated testing of Mathematica and Moulton language. So we're interested in generating tests that are plausible input, so to speak. So we've indeed tried doing things like that. Not been particularly successful. I mean, in other words, you can, you can generate an expression at random just by some Markov process, for example, and you can generate an expression by using some LLM like device. And you can ask the question, you know, given, given that you've seen, I don't know, we've got billions of sort of human related Wolfram language expressions. And then the question is, can we generate others that are like those? That's one question. Another question of great practical interest for us is, can we guess whether something that somebody entered is likely to be what they meant? Or is it something, in other words, it's like asking the question, is this a, a plausible sentence, or is this a sentence nobody would ever write, which is something which LLMs in a sense implicitly are capable of doing? I must make an executive decision now. And it's a calculated risk. We may lose Steven Wolfram, if it turns out he has something else of higher priority. We may use lose other people for the panel, in which case I will have called closure on this needlessly. And there may be nothing in the panel, but I have to call closure because we have to stop this session and then restart for the next session. I want to thank you very much, Steven, for your, for your talk. And I hope we'll be seeing you again in 10 minutes, but we'll see. Okay. Are we using the same link for the panel? Are we using the same Zoom link for the panel? I will redo it on, I'll restart it for the panel and I hope you'll be there. Steven's there too, but I have to break it now.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.84, "text": " Even Wolfram is a mathematician, computer scientist,", "tokens": [50364, 2754, 16634, 2356, 307, 257, 48281, 11, 3820, 12662, 11, 50556], "temperature": 0.0, "avg_logprob": -0.170817051293715, "compression_ratio": 1.6360153256704981, "no_speech_prob": 0.05611582100391388}, {"id": 1, "seek": 0, "start": 3.84, "end": 9.200000000000001, "text": " physicist, and businessman. Have I lied so far, Steve?", "tokens": [50556, 42466, 11, 293, 35317, 13, 3560, 286, 20101, 370, 1400, 11, 7466, 30, 50824], "temperature": 0.0, "avg_logprob": -0.170817051293715, "compression_ratio": 1.6360153256704981, "no_speech_prob": 0.05611582100391388}, {"id": 2, "seek": 0, "start": 9.200000000000001, "end": 13.68, "text": " I don't know. So some people would not put mathematician first, but that's okay.", "tokens": [50824, 286, 500, 380, 458, 13, 407, 512, 561, 576, 406, 829, 48281, 700, 11, 457, 300, 311, 1392, 13, 51048], "temperature": 0.0, "avg_logprob": -0.170817051293715, "compression_ratio": 1.6360153256704981, "no_speech_prob": 0.05611582100391388}, {"id": 3, "seek": 0, "start": 13.68, "end": 17.68, "text": " He's known for his work in computer science, mathematics, and theoretical", "tokens": [51048, 634, 311, 2570, 337, 702, 589, 294, 3820, 3497, 11, 18666, 11, 293, 20864, 51248], "temperature": 0.0, "avg_logprob": -0.170817051293715, "compression_ratio": 1.6360153256704981, "no_speech_prob": 0.05611582100391388}, {"id": 4, "seek": 0, "start": 17.68, "end": 19.92, "text": " physics. The fellow of the American Mathematical", "tokens": [51248, 10649, 13, 440, 7177, 295, 264, 2665, 15776, 8615, 804, 51360], "temperature": 0.0, "avg_logprob": -0.170817051293715, "compression_ratio": 1.6360153256704981, "no_speech_prob": 0.05611582100391388}, {"id": 5, "seek": 0, "start": 19.92, "end": 23.92, "text": " Society, founder and CEO of the software company", "tokens": [51360, 13742, 11, 14917, 293, 9282, 295, 264, 4722, 2237, 51560], "temperature": 0.0, "avg_logprob": -0.170817051293715, "compression_ratio": 1.6360153256704981, "no_speech_prob": 0.05611582100391388}, {"id": 6, "seek": 0, "start": 23.92, "end": 27.84, "text": " Wolfram Research, where he works as chief designer of Mathematica,", "tokens": [51560, 16634, 2356, 10303, 11, 689, 415, 1985, 382, 9588, 11795, 295, 15776, 8615, 2262, 11, 51756], "temperature": 0.0, "avg_logprob": -0.170817051293715, "compression_ratio": 1.6360153256704981, "no_speech_prob": 0.05611582100391388}, {"id": 7, "seek": 2784, "start": 27.84, "end": 34.56, "text": " at the Wolfram Alpha NSERP engine. Now I'm going to move that away,", "tokens": [50364, 412, 264, 16634, 2356, 20588, 15943, 1598, 47, 2848, 13, 823, 286, 478, 516, 281, 1286, 300, 1314, 11, 50700], "temperature": 0.0, "avg_logprob": -0.22505683128279869, "compression_ratio": 1.4930875576036866, "no_speech_prob": 0.0058713010512292385}, {"id": 8, "seek": 2784, "start": 34.56, "end": 39.92, "text": " and I'm going to give it, you have also, besides the people in the", "tokens": [50700, 293, 286, 478, 516, 281, 976, 309, 11, 291, 362, 611, 11, 11868, 264, 561, 294, 264, 50968], "temperature": 0.0, "avg_logprob": -0.22505683128279869, "compression_ratio": 1.4930875576036866, "no_speech_prob": 0.0058713010512292385}, {"id": 9, "seek": 2784, "start": 39.92, "end": 45.6, "text": " the attendees, there's also one, two, three, four, five", "tokens": [50968, 264, 34826, 11, 456, 311, 611, 472, 11, 732, 11, 1045, 11, 1451, 11, 1732, 51252], "temperature": 0.0, "avg_logprob": -0.22505683128279869, "compression_ratio": 1.4930875576036866, "no_speech_prob": 0.0058713010512292385}, {"id": 10, "seek": 2784, "start": 45.6, "end": 48.32, "text": " panelists that are here so far, and there might be some more.", "tokens": [51252, 20162, 300, 366, 510, 370, 1400, 11, 293, 456, 1062, 312, 512, 544, 13, 51388], "temperature": 0.0, "avg_logprob": -0.22505683128279869, "compression_ratio": 1.4930875576036866, "no_speech_prob": 0.0058713010512292385}, {"id": 11, "seek": 2784, "start": 48.32, "end": 54.08, "text": " Carl Friston, Ronnie Katzier, Sammy Benjiro. I can see the names, yeah.", "tokens": [51388, 14256, 1526, 47345, 11, 46131, 8365, 89, 811, 11, 44316, 3964, 4013, 340, 13, 286, 393, 536, 264, 5288, 11, 1338, 13, 51676], "temperature": 0.0, "avg_logprob": -0.22505683128279869, "compression_ratio": 1.4930875576036866, "no_speech_prob": 0.0058713010512292385}, {"id": 12, "seek": 5408, "start": 54.08, "end": 57.839999999999996, "text": " Hi, Carl. The rest of you, I don't know, so nice to meet you.", "tokens": [50364, 2421, 11, 14256, 13, 440, 1472, 295, 291, 11, 286, 500, 380, 458, 11, 370, 1481, 281, 1677, 291, 13, 50552], "temperature": 0.0, "avg_logprob": -0.1634139855702718, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.011619901284575462}, {"id": 13, "seek": 5408, "start": 57.839999999999996, "end": 62.08, "text": " Sammy, it's the other Benjiro. We'll find out more about that. He's the", "tokens": [50552, 44316, 11, 309, 311, 264, 661, 3964, 4013, 340, 13, 492, 603, 915, 484, 544, 466, 300, 13, 634, 311, 264, 50764], "temperature": 0.0, "avg_logprob": -0.1634139855702718, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.011619901284575462}, {"id": 14, "seek": 5408, "start": 62.08, "end": 67.92, "text": " brother of Yoshua, who is the co-sponsor of this event.", "tokens": [50764, 3708, 295, 38949, 4398, 11, 567, 307, 264, 598, 12, 4952, 892, 284, 295, 341, 2280, 13, 51056], "temperature": 0.0, "avg_logprob": -0.1634139855702718, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.011619901284575462}, {"id": 15, "seek": 5408, "start": 67.92, "end": 74.32, "text": " Okay, it's all yours. Wing it. Okay, so, well, let's see.", "tokens": [51056, 1033, 11, 309, 311, 439, 6342, 13, 28785, 309, 13, 1033, 11, 370, 11, 731, 11, 718, 311, 536, 13, 51376], "temperature": 0.0, "avg_logprob": -0.1634139855702718, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.011619901284575462}, {"id": 16, "seek": 5408, "start": 74.32, "end": 78.96, "text": " So I think you guys want to talk about language,", "tokens": [51376, 407, 286, 519, 291, 1074, 528, 281, 751, 466, 2856, 11, 51608], "temperature": 0.0, "avg_logprob": -0.1634139855702718, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.011619901284575462}, {"id": 17, "seek": 5408, "start": 78.96, "end": 83.12, "text": " and computation, and AI, and all those good kinds of things, and I was", "tokens": [51608, 293, 24903, 11, 293, 7318, 11, 293, 439, 729, 665, 3685, 295, 721, 11, 293, 286, 390, 51816], "temperature": 0.0, "avg_logprob": -0.1634139855702718, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.011619901284575462}, {"id": 18, "seek": 8312, "start": 83.12, "end": 87.60000000000001, "text": " thinking I could talk about things about LLMs and so on.", "tokens": [50364, 1953, 286, 727, 751, 466, 721, 466, 441, 43, 26386, 293, 370, 322, 13, 50588], "temperature": 0.0, "avg_logprob": -0.11943072631579488, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.004242513328790665}, {"id": 19, "seek": 8312, "start": 87.60000000000001, "end": 91.92, "text": " You know, I wrote this little book last February about", "tokens": [50588, 509, 458, 11, 286, 4114, 341, 707, 1446, 1036, 8711, 466, 50804], "temperature": 0.0, "avg_logprob": -0.11943072631579488, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.004242513328790665}, {"id": 20, "seek": 8312, "start": 91.92, "end": 95.92, "text": " ChatGBT. You can find a version of it online. I can put it in the", "tokens": [50804, 27503, 8769, 51, 13, 509, 393, 915, 257, 3037, 295, 309, 2950, 13, 286, 393, 829, 309, 294, 264, 51004], "temperature": 0.0, "avg_logprob": -0.11943072631579488, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.004242513328790665}, {"id": 21, "seek": 8312, "start": 95.92, "end": 99.2, "text": " chat. It's, let me not talk about that, but if", "tokens": [51004, 5081, 13, 467, 311, 11, 718, 385, 406, 751, 466, 300, 11, 457, 498, 51168], "temperature": 0.0, "avg_logprob": -0.11943072631579488, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.004242513328790665}, {"id": 22, "seek": 8312, "start": 99.2, "end": 102.0, "text": " people want to ask about it, I'm happy to chat about it.", "tokens": [51168, 561, 528, 281, 1029, 466, 309, 11, 286, 478, 2055, 281, 5081, 466, 309, 13, 51308], "temperature": 0.0, "avg_logprob": -0.11943072631579488, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.004242513328790665}, {"id": 23, "seek": 8312, "start": 102.0, "end": 105.44, "text": " I thought what I would try to do in 45 minutes give you", "tokens": [51308, 286, 1194, 437, 286, 576, 853, 281, 360, 294, 6905, 2077, 976, 291, 51480], "temperature": 0.0, "avg_logprob": -0.11943072631579488, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.004242513328790665}, {"id": 24, "seek": 8312, "start": 105.44, "end": 110.56, "text": " kind of the, a very rough tour of my last four and a half decades of", "tokens": [51480, 733, 295, 264, 11, 257, 588, 5903, 3512, 295, 452, 1036, 1451, 293, 257, 1922, 7878, 295, 51736], "temperature": 0.0, "avg_logprob": -0.11943072631579488, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.004242513328790665}, {"id": 25, "seek": 11056, "start": 110.56, "end": 114.8, "text": " development of a worldview, and see how that relates to", "tokens": [50364, 3250, 295, 257, 41141, 11, 293, 536, 577, 300, 16155, 281, 50576], "temperature": 0.0, "avg_logprob": -0.162407974144081, "compression_ratio": 1.543859649122807, "no_speech_prob": 0.003272932954132557}, {"id": 26, "seek": 11056, "start": 114.8, "end": 118.8, "text": " things about AI, and language, and so on.", "tokens": [50576, 721, 466, 7318, 11, 293, 2856, 11, 293, 370, 322, 13, 50776], "temperature": 0.0, "avg_logprob": -0.162407974144081, "compression_ratio": 1.543859649122807, "no_speech_prob": 0.003272932954132557}, {"id": 27, "seek": 11056, "start": 118.8, "end": 128.0, "text": " So to begin, you know, I think a thing that,", "tokens": [50776, 407, 281, 1841, 11, 291, 458, 11, 286, 519, 257, 551, 300, 11, 51236], "temperature": 0.0, "avg_logprob": -0.162407974144081, "compression_ratio": 1.543859649122807, "no_speech_prob": 0.003272932954132557}, {"id": 28, "seek": 11056, "start": 128.0, "end": 131.92000000000002, "text": " sort of the, there's kind of this progression of paradigms", "tokens": [51236, 1333, 295, 264, 11, 456, 311, 733, 295, 341, 18733, 295, 13480, 328, 2592, 51432], "temperature": 0.0, "avg_logprob": -0.162407974144081, "compression_ratio": 1.543859649122807, "no_speech_prob": 0.003272932954132557}, {"id": 29, "seek": 11056, "start": 131.92000000000002, "end": 136.0, "text": " to do with how we formalize the world. So there's sort of this", "tokens": [51432, 281, 360, 365, 577, 321, 9860, 1125, 264, 1002, 13, 407, 456, 311, 1333, 295, 341, 51636], "temperature": 0.0, "avg_logprob": -0.162407974144081, "compression_ratio": 1.543859649122807, "no_speech_prob": 0.003272932954132557}, {"id": 30, "seek": 13600, "start": 136.08, "end": 141.92, "text": " question of what, what can we do? We start, when we, when we see the", "tokens": [50368, 1168, 295, 437, 11, 437, 393, 321, 360, 30, 492, 722, 11, 562, 321, 11, 562, 321, 536, 264, 50660], "temperature": 0.0, "avg_logprob": -0.13408746010015818, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.06998347491025925}, {"id": 31, "seek": 13600, "start": 141.92, "end": 146.08, "text": " world, we're interested in finding ways to have sort of formal", "tokens": [50660, 1002, 11, 321, 434, 3102, 294, 5006, 2098, 281, 362, 1333, 295, 9860, 50868], "temperature": 0.0, "avg_logprob": -0.13408746010015818, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.06998347491025925}, {"id": 32, "seek": 13600, "start": 146.08, "end": 149.84, "text": " descriptions of it that we can build on. So historically, kind of the", "tokens": [50868, 24406, 295, 309, 300, 321, 393, 1322, 322, 13, 407, 16180, 11, 733, 295, 264, 51056], "temperature": 0.0, "avg_logprob": -0.13408746010015818, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.06998347491025925}, {"id": 33, "seek": 13600, "start": 149.84, "end": 153.28, "text": " first of those, the big one for our species, was the invention of", "tokens": [51056, 700, 295, 729, 11, 264, 955, 472, 337, 527, 6172, 11, 390, 264, 22265, 295, 51228], "temperature": 0.0, "avg_logprob": -0.13408746010015818, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.06998347491025925}, {"id": 34, "seek": 13600, "start": 153.28, "end": 156.72, "text": " human language, and the idea that you didn't have to just point at", "tokens": [51228, 1952, 2856, 11, 293, 264, 1558, 300, 291, 994, 380, 362, 281, 445, 935, 412, 51400], "temperature": 0.0, "avg_logprob": -0.13408746010015818, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.06998347491025925}, {"id": 35, "seek": 13600, "start": 156.72, "end": 159.84, "text": " each individual rock, but you could have this kind of symbolic", "tokens": [51400, 1184, 2609, 3727, 11, 457, 291, 727, 362, 341, 733, 295, 25755, 51556], "temperature": 0.0, "avg_logprob": -0.13408746010015818, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.06998347491025925}, {"id": 36, "seek": 13600, "start": 159.84, "end": 165.04, "text": " name, rock, wasn't rock originally, obviously, for that concept, and", "tokens": [51556, 1315, 11, 3727, 11, 2067, 380, 3727, 7993, 11, 2745, 11, 337, 300, 3410, 11, 293, 51816], "temperature": 0.0, "avg_logprob": -0.13408746010015818, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.06998347491025925}, {"id": 37, "seek": 16504, "start": 165.12, "end": 170.0, "text": " you could communicate abstractly about that thing using human language.", "tokens": [50368, 291, 727, 7890, 12649, 356, 466, 300, 551, 1228, 1952, 2856, 13, 50612], "temperature": 0.0, "avg_logprob": -0.11630618053933849, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.0009132204577326775}, {"id": 38, "seek": 16504, "start": 170.0, "end": 173.04, "text": " Now then, we've had sort of a stack of other ideas,", "tokens": [50612, 823, 550, 11, 321, 600, 632, 1333, 295, 257, 8630, 295, 661, 3487, 11, 50764], "temperature": 0.0, "avg_logprob": -0.11630618053933849, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.0009132204577326775}, {"id": 39, "seek": 16504, "start": 173.04, "end": 178.0, "text": " another big idea is logic, being able to, as a way of sort of", "tokens": [50764, 1071, 955, 1558, 307, 9952, 11, 885, 1075, 281, 11, 382, 257, 636, 295, 1333, 295, 51012], "temperature": 0.0, "avg_logprob": -0.11630618053933849, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.0009132204577326775}, {"id": 40, "seek": 16504, "start": 178.0, "end": 182.39999999999998, "text": " abstracting things about the world, formalizing things about the world.", "tokens": [51012, 12649, 278, 721, 466, 264, 1002, 11, 9860, 3319, 721, 466, 264, 1002, 13, 51232], "temperature": 0.0, "avg_logprob": -0.11630618053933849, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.0009132204577326775}, {"id": 41, "seek": 16504, "start": 182.39999999999998, "end": 186.64, "text": " Another big direction is mathematics, being able to,", "tokens": [51232, 3996, 955, 3513, 307, 18666, 11, 885, 1075, 281, 11, 51444], "temperature": 0.0, "avg_logprob": -0.11630618053933849, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.0009132204577326775}, {"id": 42, "seek": 16504, "start": 186.64, "end": 189.76, "text": " something that sort of became big in the late 1600s,", "tokens": [51444, 746, 300, 1333, 295, 3062, 955, 294, 264, 3469, 36885, 82, 11, 51600], "temperature": 0.0, "avg_logprob": -0.11630618053933849, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.0009132204577326775}, {"id": 43, "seek": 18976, "start": 189.84, "end": 194.48, "text": " of being able to describe our world by mathematical concepts and constructs.", "tokens": [50368, 295, 885, 1075, 281, 6786, 527, 1002, 538, 18894, 10392, 293, 7690, 82, 13, 50600], "temperature": 0.0, "avg_logprob": -0.10905040489448296, "compression_ratio": 1.6952380952380952, "no_speech_prob": 0.01355218980461359}, {"id": 44, "seek": 18976, "start": 195.51999999999998, "end": 200.64, "text": " In this century, and the end of the last one, the big new thing has been", "tokens": [50652, 682, 341, 4901, 11, 293, 264, 917, 295, 264, 1036, 472, 11, 264, 955, 777, 551, 575, 668, 50908], "temperature": 0.0, "avg_logprob": -0.10905040489448296, "compression_ratio": 1.6952380952380952, "no_speech_prob": 0.01355218980461359}, {"id": 45, "seek": 18976, "start": 200.64, "end": 204.23999999999998, "text": " using computation as a way to formalize and describe the world.", "tokens": [50908, 1228, 24903, 382, 257, 636, 281, 9860, 1125, 293, 6786, 264, 1002, 13, 51088], "temperature": 0.0, "avg_logprob": -0.10905040489448296, "compression_ratio": 1.6952380952380952, "no_speech_prob": 0.01355218980461359}, {"id": 46, "seek": 18976, "start": 204.88, "end": 210.32, "text": " Being able to specify kind of if you, the way I think about computation,", "tokens": [51120, 8891, 1075, 281, 16500, 733, 295, 498, 291, 11, 264, 636, 286, 519, 466, 24903, 11, 51392], "temperature": 0.0, "avg_logprob": -0.10905040489448296, "compression_ratio": 1.6952380952380952, "no_speech_prob": 0.01355218980461359}, {"id": 47, "seek": 18976, "start": 210.32, "end": 216.48, "text": " it's a way of setting up rules, and then saying, let these rules run.", "tokens": [51392, 309, 311, 257, 636, 295, 3287, 493, 4474, 11, 293, 550, 1566, 11, 718, 613, 4474, 1190, 13, 51700], "temperature": 0.0, "avg_logprob": -0.10905040489448296, "compression_ratio": 1.6952380952380952, "no_speech_prob": 0.01355218980461359}, {"id": 48, "seek": 21648, "start": 216.48, "end": 221.04, "text": " And the question is, can we set up rules that describe the way the world is,", "tokens": [50364, 400, 264, 1168, 307, 11, 393, 321, 992, 493, 4474, 300, 6786, 264, 636, 264, 1002, 307, 11, 50592], "temperature": 0.0, "avg_logprob": -0.11751842498779297, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.008313911035656929}, {"id": 49, "seek": 21648, "start": 221.04, "end": 224.16, "text": " is, or the aspects of the world at least that we care about?", "tokens": [50592, 307, 11, 420, 264, 7270, 295, 264, 1002, 412, 1935, 300, 321, 1127, 466, 30, 50748], "temperature": 0.0, "avg_logprob": -0.11751842498779297, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.008313911035656929}, {"id": 50, "seek": 21648, "start": 224.79999999999998, "end": 229.76, "text": " And my kind of day job for the last four decades has been building our", "tokens": [50780, 400, 452, 733, 295, 786, 1691, 337, 264, 1036, 1451, 7878, 575, 668, 2390, 527, 51028], "temperature": 0.0, "avg_logprob": -0.11751842498779297, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.008313911035656929}, {"id": 51, "seek": 21648, "start": 229.76, "end": 232.56, "text": " computational language, Wolfram Language, Mathematica, and so on,", "tokens": [51028, 28270, 2856, 11, 16634, 2356, 24445, 11, 15776, 8615, 2262, 11, 293, 370, 322, 11, 51168], "temperature": 0.0, "avg_logprob": -0.11751842498779297, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.008313911035656929}, {"id": 52, "seek": 21648, "start": 233.12, "end": 237.35999999999999, "text": " as a way to kind of take the things that we humans care about,", "tokens": [51196, 382, 257, 636, 281, 733, 295, 747, 264, 721, 300, 321, 6255, 1127, 466, 11, 51408], "temperature": 0.0, "avg_logprob": -0.11751842498779297, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.008313911035656929}, {"id": 53, "seek": 21648, "start": 237.35999999999999, "end": 242.48, "text": " whether they're molecules or cities or algorithms or whatever,", "tokens": [51408, 1968, 436, 434, 13093, 420, 6486, 420, 14642, 420, 2035, 11, 51664], "temperature": 0.0, "avg_logprob": -0.11751842498779297, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.008313911035656929}, {"id": 54, "seek": 24248, "start": 242.48, "end": 249.04, "text": " and create a systematic computational language to let one describe those things,", "tokens": [50364, 293, 1884, 257, 27249, 28270, 2856, 281, 718, 472, 6786, 729, 721, 11, 50692], "temperature": 0.0, "avg_logprob": -0.07483498255411784, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.004443557001650333}, {"id": 55, "seek": 24248, "start": 249.04, "end": 252.72, "text": " and not only describe them in a way that humans can read,", "tokens": [50692, 293, 406, 787, 6786, 552, 294, 257, 636, 300, 6255, 393, 1401, 11, 50876], "temperature": 0.0, "avg_logprob": -0.07483498255411784, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.004443557001650333}, {"id": 56, "seek": 24248, "start": 253.44, "end": 259.52, "text": " but also something where computers can help humans to execute those things.", "tokens": [50912, 457, 611, 746, 689, 10807, 393, 854, 6255, 281, 14483, 729, 721, 13, 51216], "temperature": 0.0, "avg_logprob": -0.07483498255411784, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.004443557001650333}, {"id": 57, "seek": 24248, "start": 260.08, "end": 263.03999999999996, "text": " So that's been kind of the way I see sort of that effort,", "tokens": [51244, 407, 300, 311, 668, 733, 295, 264, 636, 286, 536, 1333, 295, 300, 4630, 11, 51392], "temperature": 0.0, "avg_logprob": -0.07483498255411784, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.004443557001650333}, {"id": 58, "seek": 24248, "start": 263.03999999999996, "end": 268.08, "text": " is take the things that we humans care about, and find a way to formalize them", "tokens": [51392, 307, 747, 264, 721, 300, 321, 6255, 1127, 466, 11, 293, 915, 257, 636, 281, 9860, 1125, 552, 51644], "temperature": 0.0, "avg_logprob": -0.07483498255411784, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.004443557001650333}, {"id": 59, "seek": 26808, "start": 268.15999999999997, "end": 272.96, "text": " computationally, so that we can provide sort of the raw material that we need", "tokens": [50368, 24903, 379, 11, 370, 300, 321, 393, 2893, 1333, 295, 264, 8936, 2527, 300, 321, 643, 50608], "temperature": 0.0, "avg_logprob": -0.1118751985055429, "compression_ratio": 1.8661417322834646, "no_speech_prob": 0.025383733212947845}, {"id": 60, "seek": 26808, "start": 272.96, "end": 276.08, "text": " to work with the world computationally. It's kind of the effort,", "tokens": [50608, 281, 589, 365, 264, 1002, 24903, 379, 13, 467, 311, 733, 295, 264, 4630, 11, 50764], "temperature": 0.0, "avg_logprob": -0.1118751985055429, "compression_ratio": 1.8661417322834646, "no_speech_prob": 0.025383733212947845}, {"id": 61, "seek": 26808, "start": 276.08, "end": 279.12, "text": " is a bit like the effort that happened maybe 500 years ago,", "tokens": [50764, 307, 257, 857, 411, 264, 4630, 300, 2011, 1310, 5923, 924, 2057, 11, 50916], "temperature": 0.0, "avg_logprob": -0.1118751985055429, "compression_ratio": 1.8661417322834646, "no_speech_prob": 0.025383733212947845}, {"id": 62, "seek": 26808, "start": 279.12, "end": 283.91999999999996, "text": " with the development of mathematical notation, where people went from kind of talking about", "tokens": [50916, 365, 264, 3250, 295, 18894, 24657, 11, 689, 561, 1437, 490, 733, 295, 1417, 466, 51156], "temperature": 0.0, "avg_logprob": -0.1118751985055429, "compression_ratio": 1.8661417322834646, "no_speech_prob": 0.025383733212947845}, {"id": 63, "seek": 26808, "start": 283.91999999999996, "end": 288.32, "text": " math in terms of words, to having sort of a streamlined notation with plus signs and equal", "tokens": [51156, 5221, 294, 2115, 295, 2283, 11, 281, 1419, 1333, 295, 257, 48155, 24657, 365, 1804, 7880, 293, 2681, 51376], "temperature": 0.0, "avg_logprob": -0.1118751985055429, "compression_ratio": 1.8661417322834646, "no_speech_prob": 0.025383733212947845}, {"id": 64, "seek": 26808, "start": 288.32, "end": 293.68, "text": " signs and things like that. And that idea of notation, that sort of streamlined notation", "tokens": [51376, 7880, 293, 721, 411, 300, 13, 400, 300, 1558, 295, 24657, 11, 300, 1333, 295, 48155, 24657, 51644], "temperature": 0.0, "avg_logprob": -0.1118751985055429, "compression_ratio": 1.8661417322834646, "no_speech_prob": 0.025383733212947845}, {"id": 65, "seek": 29368, "start": 293.68, "end": 298.32, "text": " for mathematics, is what ended up launching algebra and then calculus, and basically the", "tokens": [50364, 337, 18666, 11, 307, 437, 4590, 493, 18354, 21989, 293, 550, 33400, 11, 293, 1936, 264, 50596], "temperature": 0.0, "avg_logprob": -0.14452311571906595, "compression_ratio": 1.669603524229075, "no_speech_prob": 0.03406558558344841}, {"id": 66, "seek": 29368, "start": 298.32, "end": 305.12, "text": " modern mathematical sciences. Kind of my day job mission has been to create a computational", "tokens": [50596, 4363, 18894, 17677, 13, 9242, 295, 452, 786, 1691, 4447, 575, 668, 281, 1884, 257, 28270, 50936], "temperature": 0.0, "avg_logprob": -0.14452311571906595, "compression_ratio": 1.669603524229075, "no_speech_prob": 0.03406558558344841}, {"id": 67, "seek": 29368, "start": 305.12, "end": 314.08, "text": " language that lets one kind of launch computational x for all fields x. So okay, that's, so kind of", "tokens": [50936, 2856, 300, 6653, 472, 733, 295, 4025, 28270, 2031, 337, 439, 7909, 2031, 13, 407, 1392, 11, 300, 311, 11, 370, 733, 295, 51384], "temperature": 0.0, "avg_logprob": -0.14452311571906595, "compression_ratio": 1.669603524229075, "no_speech_prob": 0.03406558558344841}, {"id": 68, "seek": 29368, "start": 314.08, "end": 320.32, "text": " this idea is computational language as a way to sort of formalize things that happen in the world,", "tokens": [51384, 341, 1558, 307, 28270, 2856, 382, 257, 636, 281, 1333, 295, 9860, 1125, 721, 300, 1051, 294, 264, 1002, 11, 51696], "temperature": 0.0, "avg_logprob": -0.14452311571906595, "compression_ratio": 1.669603524229075, "no_speech_prob": 0.03406558558344841}, {"id": 69, "seek": 32032, "start": 320.32, "end": 326.08, "text": " things that we care about in the world. Now the next question is what is the intrinsic description", "tokens": [50364, 721, 300, 321, 1127, 466, 294, 264, 1002, 13, 823, 264, 958, 1168, 307, 437, 307, 264, 35698, 3855, 50652], "temperature": 0.0, "avg_logprob": -0.11184065992181952, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.02536182291805744}, {"id": 70, "seek": 32032, "start": 326.08, "end": 333.28, "text": " of the world, so to speak? How should we describe the world in general? And the thing that sort of", "tokens": [50652, 295, 264, 1002, 11, 370, 281, 1710, 30, 1012, 820, 321, 6786, 264, 1002, 294, 2674, 30, 400, 264, 551, 300, 1333, 295, 51012], "temperature": 0.0, "avg_logprob": -0.11184065992181952, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.02536182291805744}, {"id": 71, "seek": 32032, "start": 333.28, "end": 338.32, "text": " had been the tradition of exact science for about 300 years, was use mathematical equations,", "tokens": [51012, 632, 668, 264, 6994, 295, 1900, 3497, 337, 466, 6641, 924, 11, 390, 764, 18894, 11787, 11, 51264], "temperature": 0.0, "avg_logprob": -0.11184065992181952, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.02536182291805744}, {"id": 72, "seek": 32032, "start": 338.32, "end": 343.6, "text": " write down an equation that describes this or that aspect of the world. The thing that I got", "tokens": [51264, 2464, 760, 364, 5367, 300, 15626, 341, 420, 300, 4171, 295, 264, 1002, 13, 440, 551, 300, 286, 658, 51528], "temperature": 0.0, "avg_logprob": -0.11184065992181952, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.02536182291805744}, {"id": 73, "seek": 34360, "start": 343.6, "end": 349.76000000000005, "text": " interested in in the early 1980s is how does one generalize that idea? How does one, how does one,", "tokens": [50364, 3102, 294, 294, 264, 2440, 13626, 82, 307, 577, 775, 472, 2674, 1125, 300, 1558, 30, 1012, 775, 472, 11, 577, 775, 472, 11, 50672], "temperature": 0.0, "avg_logprob": -0.09757123441777678, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.12220771610736847}, {"id": 74, "seek": 34360, "start": 349.76000000000005, "end": 355.6, "text": " what kinds of raw material can you find to talk about the world? And the thing that I started", "tokens": [50672, 437, 3685, 295, 8936, 2527, 393, 291, 915, 281, 751, 466, 264, 1002, 30, 400, 264, 551, 300, 286, 1409, 50964], "temperature": 0.0, "avg_logprob": -0.09757123441777678, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.12220771610736847}, {"id": 75, "seek": 34360, "start": 355.6, "end": 360.64000000000004, "text": " studying a lot was using computation as kind of the raw material for describing the world.", "tokens": [50964, 7601, 257, 688, 390, 1228, 24903, 382, 733, 295, 264, 8936, 2527, 337, 16141, 264, 1002, 13, 51216], "temperature": 0.0, "avg_logprob": -0.09757123441777678, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.12220771610736847}, {"id": 76, "seek": 34360, "start": 361.36, "end": 367.28000000000003, "text": " And so the question, the first question is, well okay, what kinds of, how do you set up kind of", "tokens": [51252, 400, 370, 264, 1168, 11, 264, 700, 1168, 307, 11, 731, 1392, 11, 437, 3685, 295, 11, 577, 360, 291, 992, 493, 733, 295, 51548], "temperature": 0.0, "avg_logprob": -0.09757123441777678, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.12220771610736847}, {"id": 77, "seek": 34360, "start": 367.28000000000003, "end": 372.48, "text": " computational systems to do that? And for example, let's see, let's actually do something here.", "tokens": [51548, 28270, 3652, 281, 360, 300, 30, 400, 337, 1365, 11, 718, 311, 536, 11, 718, 311, 767, 360, 746, 510, 13, 51808], "temperature": 0.0, "avg_logprob": -0.09757123441777678, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.12220771610736847}, {"id": 78, "seek": 37360, "start": 374.56, "end": 386.64000000000004, "text": " Okay, let's say we are just kind of, we want to see what do", "tokens": [50412, 1033, 11, 718, 311, 584, 321, 366, 445, 733, 295, 11, 321, 528, 281, 536, 437, 360, 51016], "temperature": 0.0, "avg_logprob": -0.21016901456392728, "compression_ratio": 1.5521472392638036, "no_speech_prob": 0.0045014345087111}, {"id": 79, "seek": 37360, "start": 387.6, "end": 394.16, "text": " programs that might be computational descriptions of the world, what, let's just look at some", "tokens": [51064, 4268, 300, 1062, 312, 28270, 24406, 295, 264, 1002, 11, 437, 11, 718, 311, 445, 574, 412, 512, 51392], "temperature": 0.0, "avg_logprob": -0.21016901456392728, "compression_ratio": 1.5521472392638036, "no_speech_prob": 0.0045014345087111}, {"id": 80, "seek": 37360, "start": 394.16, "end": 402.96000000000004, "text": " program that, just say what do the typical programs out there do? So this is a very simple example,", "tokens": [51392, 1461, 300, 11, 445, 584, 437, 360, 264, 7476, 4268, 484, 456, 360, 30, 407, 341, 307, 257, 588, 2199, 1365, 11, 51832], "temperature": 0.0, "avg_logprob": -0.21016901456392728, "compression_ratio": 1.5521472392638036, "no_speech_prob": 0.0045014345087111}, {"id": 81, "seek": 40296, "start": 403.03999999999996, "end": 408.08, "text": " it's just you imagine a line of black and white cells and you have some simple rule that says,", "tokens": [50368, 309, 311, 445, 291, 3811, 257, 1622, 295, 2211, 293, 2418, 5438, 293, 291, 362, 512, 2199, 4978, 300, 1619, 11, 50620], "temperature": 0.0, "avg_logprob": -0.0921622879651128, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.0030929800122976303}, {"id": 82, "seek": 40296, "start": 408.08, "end": 412.96, "text": " given the colors of cells on one row, these are the, this is the color, this is how you determine", "tokens": [50620, 2212, 264, 4577, 295, 5438, 322, 472, 5386, 11, 613, 366, 264, 11, 341, 307, 264, 2017, 11, 341, 307, 577, 291, 6997, 50864], "temperature": 0.0, "avg_logprob": -0.0921622879651128, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.0030929800122976303}, {"id": 83, "seek": 40296, "start": 412.96, "end": 422.08, "text": " the color of the cells on the next row. So if you run this and just run this starting,", "tokens": [50864, 264, 2017, 295, 264, 5438, 322, 264, 958, 5386, 13, 407, 498, 291, 1190, 341, 293, 445, 1190, 341, 2891, 11, 51320], "temperature": 0.0, "avg_logprob": -0.0921622879651128, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.0030929800122976303}, {"id": 84, "seek": 40296, "start": 422.08, "end": 428.47999999999996, "text": " let's say, from one black cell, let's run it for like 40 steps. According to that rule,", "tokens": [51320, 718, 311, 584, 11, 490, 472, 2211, 2815, 11, 718, 311, 1190, 309, 337, 411, 3356, 4439, 13, 7328, 281, 300, 4978, 11, 51640], "temperature": 0.0, "avg_logprob": -0.0921622879651128, "compression_ratio": 1.7311320754716981, "no_speech_prob": 0.0030929800122976303}, {"id": 85, "seek": 42848, "start": 429.04, "end": 433.68, "text": " you start off from, you end up with something where you have this very simple rule, very", "tokens": [50392, 291, 722, 766, 490, 11, 291, 917, 493, 365, 746, 689, 291, 362, 341, 588, 2199, 4978, 11, 588, 50624], "temperature": 0.0, "avg_logprob": -0.09200058850375088, "compression_ratio": 1.9051724137931034, "no_speech_prob": 0.015975240617990494}, {"id": 86, "seek": 42848, "start": 433.68, "end": 438.24, "text": " simple computational rule, you run it, you get this very simple pattern. Now the question is,", "tokens": [50624, 2199, 28270, 4978, 11, 291, 1190, 309, 11, 291, 483, 341, 588, 2199, 5102, 13, 823, 264, 1168, 307, 11, 50852], "temperature": 0.0, "avg_logprob": -0.09200058850375088, "compression_ratio": 1.9051724137931034, "no_speech_prob": 0.015975240617990494}, {"id": 87, "seek": 42848, "start": 438.24, "end": 442.56, "text": " what happens if we look at other kinds of rules? What happens if we kind of turn our", "tokens": [50852, 437, 2314, 498, 321, 574, 412, 661, 3685, 295, 4474, 30, 708, 2314, 498, 321, 733, 295, 1261, 527, 51068], "temperature": 0.0, "avg_logprob": -0.09200058850375088, "compression_ratio": 1.9051724137931034, "no_speech_prob": 0.015975240617990494}, {"id": 88, "seek": 42848, "start": 442.56, "end": 447.20000000000005, "text": " computational telescope out into the computational universe and just look at what's out there?", "tokens": [51068, 28270, 26114, 484, 666, 264, 28270, 6445, 293, 445, 574, 412, 437, 311, 484, 456, 30, 51300], "temperature": 0.0, "avg_logprob": -0.09200058850375088, "compression_ratio": 1.9051724137931034, "no_speech_prob": 0.015975240617990494}, {"id": 89, "seek": 42848, "start": 447.84000000000003, "end": 453.84000000000003, "text": " So we can do that, let's do this, let's just make a, let's just make a table of", "tokens": [51332, 407, 321, 393, 360, 300, 11, 718, 311, 360, 341, 11, 718, 311, 445, 652, 257, 11, 718, 311, 445, 652, 257, 3199, 295, 51632], "temperature": 0.0, "avg_logprob": -0.09200058850375088, "compression_ratio": 1.9051724137931034, "no_speech_prob": 0.015975240617990494}, {"id": 90, "seek": 45384, "start": 453.91999999999996, "end": 465.35999999999996, "text": " all possible rules, let's say the first 63 of these rules, 64 of these rules.", "tokens": [50368, 439, 1944, 4474, 11, 718, 311, 584, 264, 700, 25082, 295, 613, 4474, 11, 12145, 295, 613, 4474, 13, 50940], "temperature": 0.0, "avg_logprob": -0.1052507996559143, "compression_ratio": 1.7891891891891891, "no_speech_prob": 0.002576693193987012}, {"id": 91, "seek": 45384, "start": 466.15999999999997, "end": 469.91999999999996, "text": " Okay, so each one of these rules corresponds, each one of these pictures", "tokens": [50980, 1033, 11, 370, 1184, 472, 295, 613, 4474, 23249, 11, 1184, 472, 295, 613, 5242, 51168], "temperature": 0.0, "avg_logprob": -0.1052507996559143, "compression_ratio": 1.7891891891891891, "no_speech_prob": 0.002576693193987012}, {"id": 92, "seek": 45384, "start": 470.64, "end": 476.32, "text": " corresponds to a different rule for how the colors of cells are determined by colors of", "tokens": [51204, 23249, 281, 257, 819, 4978, 337, 577, 264, 4577, 295, 5438, 366, 9540, 538, 4577, 295, 51488], "temperature": 0.0, "avg_logprob": -0.1052507996559143, "compression_ratio": 1.7891891891891891, "no_speech_prob": 0.002576693193987012}, {"id": 93, "seek": 45384, "start": 476.32, "end": 482.47999999999996, "text": " cells above them. What we see is many of these patterns are very simple, sometimes we'll get", "tokens": [51488, 5438, 3673, 552, 13, 708, 321, 536, 307, 867, 295, 613, 8294, 366, 588, 2199, 11, 2171, 321, 603, 483, 51796], "temperature": 0.0, "avg_logprob": -0.1052507996559143, "compression_ratio": 1.7891891891891891, "no_speech_prob": 0.002576693193987012}, {"id": 94, "seek": 48248, "start": 482.48, "end": 487.12, "text": " slightly more complicated patterns, we might get a nested pattern, for example, here,", "tokens": [50364, 4748, 544, 6179, 8294, 11, 321, 1062, 483, 257, 15646, 292, 5102, 11, 337, 1365, 11, 510, 11, 50596], "temperature": 0.0, "avg_logprob": -0.131613809008931, "compression_ratio": 1.555023923444976, "no_speech_prob": 0.004229947924613953}, {"id": 95, "seek": 48248, "start": 487.76, "end": 491.68, "text": " but the thing that is kind of my all-time favorite science discovery that I made almost", "tokens": [50628, 457, 264, 551, 300, 307, 733, 295, 452, 439, 12, 3766, 2954, 3497, 12114, 300, 286, 1027, 1920, 50824], "temperature": 0.0, "avg_logprob": -0.131613809008931, "compression_ratio": 1.555023923444976, "no_speech_prob": 0.004229947924613953}, {"id": 96, "seek": 48248, "start": 491.68, "end": 497.84000000000003, "text": " exactly 40 years ago, it was 40 years ago on June 1st, is this thing that I call rule 30.", "tokens": [50824, 2293, 3356, 924, 2057, 11, 309, 390, 3356, 924, 2057, 322, 6928, 502, 372, 11, 307, 341, 551, 300, 286, 818, 4978, 2217, 13, 51132], "temperature": 0.0, "avg_logprob": -0.131613809008931, "compression_ratio": 1.555023923444976, "no_speech_prob": 0.004229947924613953}, {"id": 97, "seek": 48248, "start": 498.48, "end": 506.16, "text": " It's specified by this set of cases here, and if we just run,", "tokens": [51164, 467, 311, 22206, 538, 341, 992, 295, 3331, 510, 11, 293, 498, 321, 445, 1190, 11, 51548], "temperature": 0.0, "avg_logprob": -0.131613809008931, "compression_ratio": 1.555023923444976, "no_speech_prob": 0.004229947924613953}, {"id": 98, "seek": 50616, "start": 506.48, "end": 517.6, "text": " just run this, started off from single black cell, let's run it for let's say 200 steps,", "tokens": [50380, 445, 1190, 341, 11, 1409, 766, 490, 2167, 2211, 2815, 11, 718, 311, 1190, 309, 337, 718, 311, 584, 2331, 4439, 11, 50936], "temperature": 0.0, "avg_logprob": -0.14789844883812797, "compression_ratio": 1.554945054945055, "no_speech_prob": 0.005031410604715347}, {"id": 99, "seek": 50616, "start": 519.9200000000001, "end": 526.24, "text": " this is what we get. And to me, this is something very surprising and kind of intuition breaking.", "tokens": [51052, 341, 307, 437, 321, 483, 13, 400, 281, 385, 11, 341, 307, 746, 588, 8830, 293, 733, 295, 24002, 7697, 13, 51368], "temperature": 0.0, "avg_logprob": -0.14789844883812797, "compression_ratio": 1.554945054945055, "no_speech_prob": 0.005031410604715347}, {"id": 100, "seek": 50616, "start": 526.24, "end": 533.2, "text": " We have a very simple rule, and yet when we run that rule, we're generating something that looks", "tokens": [51368, 492, 362, 257, 588, 2199, 4978, 11, 293, 1939, 562, 321, 1190, 300, 4978, 11, 321, 434, 17746, 746, 300, 1542, 51716], "temperature": 0.0, "avg_logprob": -0.14789844883812797, "compression_ratio": 1.554945054945055, "no_speech_prob": 0.005031410604715347}, {"id": 101, "seek": 53320, "start": 533.2, "end": 538.6400000000001, "text": " at least to us very complicated. And actually, you can go and you can sort of work out what's the", "tokens": [50364, 412, 1935, 281, 505, 588, 6179, 13, 400, 767, 11, 291, 393, 352, 293, 291, 393, 1333, 295, 589, 484, 437, 311, 264, 50636], "temperature": 0.0, "avg_logprob": -0.06300561768668038, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.013439428992569447}, {"id": 102, "seek": 53320, "start": 538.6400000000001, "end": 543.76, "text": " center column of cells here, and for all practical purposes, it seems completely random. But so what", "tokens": [50636, 3056, 7738, 295, 5438, 510, 11, 293, 337, 439, 8496, 9932, 11, 309, 2544, 2584, 4974, 13, 583, 370, 437, 50892], "temperature": 0.0, "avg_logprob": -0.06300561768668038, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.013439428992569447}, {"id": 103, "seek": 53320, "start": 543.76, "end": 549.6, "text": " this is telling us is out in the computational universe, even very simple rules can easily give", "tokens": [50892, 341, 307, 3585, 505, 307, 484, 294, 264, 28270, 6445, 11, 754, 588, 2199, 4474, 393, 3612, 976, 51184], "temperature": 0.0, "avg_logprob": -0.06300561768668038, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.013439428992569447}, {"id": 104, "seek": 53320, "start": 549.6, "end": 555.0400000000001, "text": " one very complicated behavior. And there are a lot of consequences of this. One thing that this led", "tokens": [51184, 472, 588, 6179, 5223, 13, 400, 456, 366, 257, 688, 295, 10098, 295, 341, 13, 1485, 551, 300, 341, 4684, 51456], "temperature": 0.0, "avg_logprob": -0.06300561768668038, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.013439428992569447}, {"id": 105, "seek": 53320, "start": 555.0400000000001, "end": 560.5600000000001, "text": " me to is this thing I call the principle of computational equivalence. So the thing that is", "tokens": [51456, 385, 281, 307, 341, 551, 286, 818, 264, 8665, 295, 28270, 9052, 655, 13, 407, 264, 551, 300, 307, 51732], "temperature": 0.0, "avg_logprob": -0.06300561768668038, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.013439428992569447}, {"id": 106, "seek": 56056, "start": 560.56, "end": 566.0, "text": " sort of a big question is, how do you characterize what's going on in a system like this? Well,", "tokens": [50364, 1333, 295, 257, 955, 1168, 307, 11, 577, 360, 291, 38463, 437, 311, 516, 322, 294, 257, 1185, 411, 341, 30, 1042, 11, 50636], "temperature": 0.0, "avg_logprob": -0.09051729460894051, "compression_ratio": 1.8365019011406845, "no_speech_prob": 0.01665044017136097}, {"id": 107, "seek": 56056, "start": 566.0, "end": 572.0, "text": " you can think about the system as performing a computation. It starts from its initial conditions", "tokens": [50636, 291, 393, 519, 466, 264, 1185, 382, 10205, 257, 24903, 13, 467, 3719, 490, 1080, 5883, 4487, 50936], "temperature": 0.0, "avg_logprob": -0.09051729460894051, "compression_ratio": 1.8365019011406845, "no_speech_prob": 0.01665044017136097}, {"id": 108, "seek": 56056, "start": 572.0, "end": 578.3199999999999, "text": " at the top, and then it's going crunch, crunch, crunch, and and executing a computation. The", "tokens": [50936, 412, 264, 1192, 11, 293, 550, 309, 311, 516, 13386, 11, 13386, 11, 13386, 11, 293, 293, 32368, 257, 24903, 13, 440, 51252], "temperature": 0.0, "avg_logprob": -0.09051729460894051, "compression_ratio": 1.8365019011406845, "no_speech_prob": 0.01665044017136097}, {"id": 109, "seek": 56056, "start": 578.3199999999999, "end": 583.1999999999999, "text": " question then is sort of how sophisticated is that computation? And we might have thought, well,", "tokens": [51252, 1168, 550, 307, 1333, 295, 577, 16950, 307, 300, 24903, 30, 400, 321, 1062, 362, 1194, 11, 731, 11, 51496], "temperature": 0.0, "avg_logprob": -0.09051729460894051, "compression_ratio": 1.8365019011406845, "no_speech_prob": 0.01665044017136097}, {"id": 110, "seek": 56056, "start": 583.1999999999999, "end": 587.5999999999999, "text": " it's just a simple rule, it's doing what it does. If we think about the kinds of computations that,", "tokens": [51496, 309, 311, 445, 257, 2199, 4978, 11, 309, 311, 884, 437, 309, 775, 13, 759, 321, 519, 466, 264, 3685, 295, 2807, 763, 300, 11, 51716], "temperature": 0.0, "avg_logprob": -0.09051729460894051, "compression_ratio": 1.8365019011406845, "no_speech_prob": 0.01665044017136097}, {"id": 111, "seek": 58760, "start": 587.6800000000001, "end": 591.76, "text": " for example, we do in our brains, well, those are going to be much more sophisticated than this.", "tokens": [50368, 337, 1365, 11, 321, 360, 294, 527, 15442, 11, 731, 11, 729, 366, 516, 281, 312, 709, 544, 16950, 813, 341, 13, 50572], "temperature": 0.0, "avg_logprob": -0.08212411725843274, "compression_ratio": 1.7491039426523298, "no_speech_prob": 0.0025948670227080584}, {"id": 112, "seek": 58760, "start": 592.5600000000001, "end": 597.6800000000001, "text": " But what the principle of computational equivalence says is that actually that's not true.", "tokens": [50612, 583, 437, 264, 8665, 295, 28270, 9052, 655, 1619, 307, 300, 767, 300, 311, 406, 2074, 13, 50868], "temperature": 0.0, "avg_logprob": -0.08212411725843274, "compression_ratio": 1.7491039426523298, "no_speech_prob": 0.0025948670227080584}, {"id": 113, "seek": 58760, "start": 597.6800000000001, "end": 603.9200000000001, "text": " Above some very low threshold, essentially, all of these kinds of systems, regardless of how simple", "tokens": [50868, 32691, 512, 588, 2295, 14678, 11, 4476, 11, 439, 295, 613, 3685, 295, 3652, 11, 10060, 295, 577, 2199, 51180], "temperature": 0.0, "avg_logprob": -0.08212411725843274, "compression_ratio": 1.7491039426523298, "no_speech_prob": 0.0025948670227080584}, {"id": 114, "seek": 58760, "start": 603.9200000000001, "end": 609.9200000000001, "text": " their rules are, are equivalent in the sophistication of the computations that they can do. So that", "tokens": [51180, 641, 4474, 366, 11, 366, 10344, 294, 264, 15572, 399, 295, 264, 2807, 763, 300, 436, 393, 360, 13, 407, 300, 51480], "temperature": 0.0, "avg_logprob": -0.08212411725843274, "compression_ratio": 1.7491039426523298, "no_speech_prob": 0.0025948670227080584}, {"id": 115, "seek": 58760, "start": 609.9200000000001, "end": 615.12, "text": " means that in a sense, this little rule 30 thing is doing a computation that's just as sophisticated", "tokens": [51480, 1355, 300, 294, 257, 2020, 11, 341, 707, 4978, 2217, 551, 307, 884, 257, 24903, 300, 311, 445, 382, 16950, 51740], "temperature": 0.0, "avg_logprob": -0.08212411725843274, "compression_ratio": 1.7491039426523298, "no_speech_prob": 0.0025948670227080584}, {"id": 116, "seek": 61512, "start": 615.2, "end": 620.72, "text": " as the computations that go on, for example, in our brains. Well, what consequences does that have?", "tokens": [50368, 382, 264, 2807, 763, 300, 352, 322, 11, 337, 1365, 11, 294, 527, 15442, 13, 1042, 11, 437, 10098, 775, 300, 362, 30, 50644], "temperature": 0.0, "avg_logprob": -0.0725139711723953, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.008406521752476692}, {"id": 117, "seek": 61512, "start": 621.6, "end": 627.12, "text": " One consequence that has is this phenomenon I call computational irreducibility. So let's say", "tokens": [50688, 1485, 18326, 300, 575, 307, 341, 14029, 286, 818, 28270, 16014, 769, 537, 39802, 13, 407, 718, 311, 584, 50964], "temperature": 0.0, "avg_logprob": -0.0725139711723953, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.008406521752476692}, {"id": 118, "seek": 61512, "start": 627.12, "end": 632.4, "text": " you want to know what this, well, how this pattern is going to work out a billion steps", "tokens": [50964, 291, 528, 281, 458, 437, 341, 11, 731, 11, 577, 341, 5102, 307, 516, 281, 589, 484, 257, 5218, 4439, 51228], "temperature": 0.0, "avg_logprob": -0.0725139711723953, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.008406521752476692}, {"id": 119, "seek": 61512, "start": 632.4, "end": 637.6, "text": " later from now. Well, how do you how do you figure that out? One way you can figure that out is just", "tokens": [51228, 1780, 490, 586, 13, 1042, 11, 577, 360, 291, 577, 360, 291, 2573, 300, 484, 30, 1485, 636, 291, 393, 2573, 300, 484, 307, 445, 51488], "temperature": 0.0, "avg_logprob": -0.0725139711723953, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.008406521752476692}, {"id": 120, "seek": 61512, "start": 637.6, "end": 642.32, "text": " to follow those billion steps and see what happens. Another thing you can do is to say, wait a minute,", "tokens": [51488, 281, 1524, 729, 5218, 4439, 293, 536, 437, 2314, 13, 3996, 551, 291, 393, 360, 307, 281, 584, 11, 1699, 257, 3456, 11, 51724], "temperature": 0.0, "avg_logprob": -0.0725139711723953, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.008406521752476692}, {"id": 121, "seek": 64232, "start": 642.4000000000001, "end": 646.08, "text": " I'm much smarter than this system. I'm just going to jump ahead and I'm going to say,", "tokens": [50368, 286, 478, 709, 20294, 813, 341, 1185, 13, 286, 478, 445, 516, 281, 3012, 2286, 293, 286, 478, 516, 281, 584, 11, 50552], "temperature": 0.0, "avg_logprob": -0.08525119754050275, "compression_ratio": 1.7717041800643087, "no_speech_prob": 0.016284829005599022}, {"id": 122, "seek": 64232, "start": 646.08, "end": 651.0400000000001, "text": " I know what the answer is after a billion steps. That's the thing we've become used to", "tokens": [50552, 286, 458, 437, 264, 1867, 307, 934, 257, 5218, 4439, 13, 663, 311, 264, 551, 321, 600, 1813, 1143, 281, 50800], "temperature": 0.0, "avg_logprob": -0.08525119754050275, "compression_ratio": 1.7717041800643087, "no_speech_prob": 0.016284829005599022}, {"id": 123, "seek": 64232, "start": 651.0400000000001, "end": 657.7600000000001, "text": " in doing, for example, mathematical science. You imagine an idealized planet orbiting a star.", "tokens": [50800, 294, 884, 11, 337, 1365, 11, 18894, 3497, 13, 509, 3811, 364, 7157, 1602, 5054, 48985, 257, 3543, 13, 51136], "temperature": 0.0, "avg_logprob": -0.08525119754050275, "compression_ratio": 1.7717041800643087, "no_speech_prob": 0.016284829005599022}, {"id": 124, "seek": 64232, "start": 657.7600000000001, "end": 662.1600000000001, "text": " You say, do you have to work out where it's going to be a million years from now? Do you have to", "tokens": [51136, 509, 584, 11, 360, 291, 362, 281, 589, 484, 689, 309, 311, 516, 281, 312, 257, 2459, 924, 490, 586, 30, 1144, 291, 362, 281, 51356], "temperature": 0.0, "avg_logprob": -0.08525119754050275, "compression_ratio": 1.7717041800643087, "no_speech_prob": 0.016284829005599022}, {"id": 125, "seek": 64232, "start": 662.1600000000001, "end": 667.6, "text": " follow those million orbits? Or can you just use a formula and kind of fill in the number of million", "tokens": [51356, 1524, 729, 2459, 43522, 30, 1610, 393, 291, 445, 764, 257, 8513, 293, 733, 295, 2836, 294, 264, 1230, 295, 2459, 51628], "temperature": 0.0, "avg_logprob": -0.08525119754050275, "compression_ratio": 1.7717041800643087, "no_speech_prob": 0.016284829005599022}, {"id": 126, "seek": 64232, "start": 667.6, "end": 672.0, "text": " and jump ahead and see what the answer is? That kind of what we can call computational", "tokens": [51628, 293, 3012, 2286, 293, 536, 437, 264, 1867, 307, 30, 663, 733, 295, 437, 321, 393, 818, 28270, 51848], "temperature": 0.0, "avg_logprob": -0.08525119754050275, "compression_ratio": 1.7717041800643087, "no_speech_prob": 0.016284829005599022}, {"id": 127, "seek": 67200, "start": 672.0, "end": 677.12, "text": " reducibility is what we've become used to from kind of what happens in mathematical science.", "tokens": [50364, 2783, 537, 39802, 307, 437, 321, 600, 1813, 1143, 281, 490, 733, 295, 437, 2314, 294, 18894, 3497, 13, 50620], "temperature": 0.0, "avg_logprob": -0.04743377512151545, "compression_ratio": 1.85546875, "no_speech_prob": 0.0012490631779655814}, {"id": 128, "seek": 67200, "start": 677.84, "end": 682.56, "text": " But the principle of computational equivalence tells us that will not generally be what one", "tokens": [50656, 583, 264, 8665, 295, 28270, 9052, 655, 5112, 505, 300, 486, 406, 5101, 312, 437, 472, 50892], "temperature": 0.0, "avg_logprob": -0.04743377512151545, "compression_ratio": 1.85546875, "no_speech_prob": 0.0012490631779655814}, {"id": 129, "seek": 67200, "start": 682.56, "end": 688.32, "text": " can do. In general, the systems that we're studying will be just as computationally sophisticated", "tokens": [50892, 393, 360, 13, 682, 2674, 11, 264, 3652, 300, 321, 434, 7601, 486, 312, 445, 382, 24903, 379, 16950, 51180], "temperature": 0.0, "avg_logprob": -0.04743377512151545, "compression_ratio": 1.85546875, "no_speech_prob": 0.0012490631779655814}, {"id": 130, "seek": 67200, "start": 688.32, "end": 692.96, "text": " as anything that we can muster in studying them. And so that means we won't be able to do that kind", "tokens": [51180, 382, 1340, 300, 321, 393, 1633, 260, 294, 7601, 552, 13, 400, 370, 300, 1355, 321, 1582, 380, 312, 1075, 281, 360, 300, 733, 51412], "temperature": 0.0, "avg_logprob": -0.04743377512151545, "compression_ratio": 1.85546875, "no_speech_prob": 0.0012490631779655814}, {"id": 131, "seek": 67200, "start": 692.96, "end": 697.68, "text": " of jumping ahead. We won't be able to do that kind of computational outrunning of the system", "tokens": [51412, 295, 11233, 2286, 13, 492, 1582, 380, 312, 1075, 281, 360, 300, 733, 295, 28270, 484, 45482, 295, 264, 1185, 51648], "temperature": 0.0, "avg_logprob": -0.04743377512151545, "compression_ratio": 1.85546875, "no_speech_prob": 0.0012490631779655814}, {"id": 132, "seek": 69768, "start": 697.68, "end": 701.52, "text": " and will be reduced to something where to work out what the system does,", "tokens": [50364, 293, 486, 312, 9212, 281, 746, 689, 281, 589, 484, 437, 264, 1185, 775, 11, 50556], "temperature": 0.0, "avg_logprob": -0.10184345421967683, "compression_ratio": 1.765625, "no_speech_prob": 0.017292896285653114}, {"id": 133, "seek": 69768, "start": 701.52, "end": 707.04, "text": " we basically have to follow every step and see what the outcome is. So this is something which", "tokens": [50556, 321, 1936, 362, 281, 1524, 633, 1823, 293, 536, 437, 264, 9700, 307, 13, 407, 341, 307, 746, 597, 50832], "temperature": 0.0, "avg_logprob": -0.10184345421967683, "compression_ratio": 1.765625, "no_speech_prob": 0.017292896285653114}, {"id": 134, "seek": 69768, "start": 707.5999999999999, "end": 713.1999999999999, "text": " or kind of in a sense for science, it's telling when there's a major limitation on science. And", "tokens": [50860, 420, 733, 295, 294, 257, 2020, 337, 3497, 11, 309, 311, 3585, 562, 456, 311, 257, 2563, 27432, 322, 3497, 13, 400, 51140], "temperature": 0.0, "avg_logprob": -0.10184345421967683, "compression_ratio": 1.765625, "no_speech_prob": 0.017292896285653114}, {"id": 135, "seek": 69768, "start": 713.1999999999999, "end": 718.64, "text": " by the way, this idea is something, things like girl's theorem, a sort of a special case of this", "tokens": [51140, 538, 264, 636, 11, 341, 1558, 307, 746, 11, 721, 411, 2013, 311, 20904, 11, 257, 1333, 295, 257, 2121, 1389, 295, 341, 51412], "temperature": 0.0, "avg_logprob": -0.10184345421967683, "compression_ratio": 1.765625, "no_speech_prob": 0.017292896285653114}, {"id": 136, "seek": 69768, "start": 718.64, "end": 723.5999999999999, "text": " idea and lots of other kinds of things that one knows about universal computation and so on", "tokens": [51412, 1558, 293, 3195, 295, 661, 3685, 295, 721, 300, 472, 3255, 466, 11455, 24903, 293, 370, 322, 51660], "temperature": 0.0, "avg_logprob": -0.10184345421967683, "compression_ratio": 1.765625, "no_speech_prob": 0.017292896285653114}, {"id": 137, "seek": 72360, "start": 723.6, "end": 730.24, "text": " is also related to this. But this is kind of a tighter version of those kinds of ideas", "tokens": [50364, 307, 611, 4077, 281, 341, 13, 583, 341, 307, 733, 295, 257, 30443, 3037, 295, 729, 3685, 295, 3487, 50696], "temperature": 0.0, "avg_logprob": -0.10286056602394188, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.025082053616642952}, {"id": 138, "seek": 72360, "start": 731.2, "end": 736.8000000000001, "text": " and one which I think sort of shows one kind of the relationship of these things to science.", "tokens": [50744, 293, 472, 597, 286, 519, 1333, 295, 3110, 472, 733, 295, 264, 2480, 295, 613, 721, 281, 3497, 13, 51024], "temperature": 0.0, "avg_logprob": -0.10286056602394188, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.025082053616642952}, {"id": 139, "seek": 72360, "start": 736.8000000000001, "end": 742.32, "text": " And sort of the big consequences, there's lots of stuff that you won't be able to", "tokens": [51024, 400, 1333, 295, 264, 955, 10098, 11, 456, 311, 3195, 295, 1507, 300, 291, 1582, 380, 312, 1075, 281, 51300], "temperature": 0.0, "avg_logprob": -0.10286056602394188, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.025082053616642952}, {"id": 140, "seek": 72360, "start": 742.32, "end": 747.6, "text": " have a theory for, work out, jump ahead, know what's going to happen. You'll have to just follow", "tokens": [51300, 362, 257, 5261, 337, 11, 589, 484, 11, 3012, 2286, 11, 458, 437, 311, 516, 281, 1051, 13, 509, 603, 362, 281, 445, 1524, 51564], "temperature": 0.0, "avg_logprob": -0.10286056602394188, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.025082053616642952}, {"id": 141, "seek": 74760, "start": 747.6, "end": 754.32, "text": " every step and see what happens. And so in a sense, that's a limitation on science. From within", "tokens": [50364, 633, 1823, 293, 536, 437, 2314, 13, 400, 370, 294, 257, 2020, 11, 300, 311, 257, 27432, 322, 3497, 13, 3358, 1951, 50700], "temperature": 0.0, "avg_logprob": -0.11283943964087445, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.10854272544384003}, {"id": 142, "seek": 74760, "start": 754.32, "end": 759.6800000000001, "text": " science, one is seeing kind of a fundamental limitation of science. It's actually something", "tokens": [50700, 3497, 11, 472, 307, 2577, 733, 295, 257, 8088, 27432, 295, 3497, 13, 467, 311, 767, 746, 50968], "temperature": 0.0, "avg_logprob": -0.11283943964087445, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.10854272544384003}, {"id": 143, "seek": 74760, "start": 759.6800000000001, "end": 764.48, "text": " which for many purposes might, one might not think of being a such bad thing, because in a sense,", "tokens": [50968, 597, 337, 867, 9932, 1062, 11, 472, 1062, 406, 519, 295, 885, 257, 1270, 1578, 551, 11, 570, 294, 257, 2020, 11, 51208], "temperature": 0.0, "avg_logprob": -0.11283943964087445, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.10854272544384003}, {"id": 144, "seek": 74760, "start": 764.48, "end": 770.24, "text": " it's the thing that makes, for example, the passage of time meaningful. If it wasn't for", "tokens": [51208, 309, 311, 264, 551, 300, 1669, 11, 337, 1365, 11, 264, 11497, 295, 565, 10995, 13, 759, 309, 2067, 380, 337, 51496], "temperature": 0.0, "avg_logprob": -0.11283943964087445, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.10854272544384003}, {"id": 145, "seek": 77024, "start": 770.24, "end": 778.16, "text": " computational irreducibility, then if you live for 50 years, then in a sense, that would not be,", "tokens": [50364, 28270, 16014, 769, 537, 39802, 11, 550, 498, 291, 1621, 337, 2625, 924, 11, 550, 294, 257, 2020, 11, 300, 576, 406, 312, 11, 50760], "temperature": 0.0, "avg_logprob": -0.07419069936452818, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.2392548769712448}, {"id": 146, "seek": 77024, "start": 778.16, "end": 782.72, "text": " nothing would be achieved by that. One would be able to say, oh, I know what's going to happen in", "tokens": [50760, 1825, 576, 312, 11042, 538, 300, 13, 1485, 576, 312, 1075, 281, 584, 11, 1954, 11, 286, 458, 437, 311, 516, 281, 1051, 294, 50988], "temperature": 0.0, "avg_logprob": -0.07419069936452818, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.2392548769712448}, {"id": 147, "seek": 77024, "start": 782.72, "end": 787.2, "text": " the end. I can jump ahead and say what the outcome is going to be. But because of computational", "tokens": [50988, 264, 917, 13, 286, 393, 3012, 2286, 293, 584, 437, 264, 9700, 307, 516, 281, 312, 13, 583, 570, 295, 28270, 51212], "temperature": 0.0, "avg_logprob": -0.07419069936452818, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.2392548769712448}, {"id": 148, "seek": 77024, "start": 787.2, "end": 792.72, "text": " irreducibility, there is something sort of really happening in the passage of time. It is a sort of", "tokens": [51212, 16014, 769, 537, 39802, 11, 456, 307, 746, 1333, 295, 534, 2737, 294, 264, 11497, 295, 565, 13, 467, 307, 257, 1333, 295, 51488], "temperature": 0.0, "avg_logprob": -0.07419069936452818, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.2392548769712448}, {"id": 149, "seek": 77024, "start": 792.72, "end": 797.2, "text": " an irreducible computation that's going on. There are many other consequences of computational", "tokens": [51488, 364, 16014, 769, 32128, 24903, 300, 311, 516, 322, 13, 821, 366, 867, 661, 10098, 295, 28270, 51712], "temperature": 0.0, "avg_logprob": -0.07419069936452818, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.2392548769712448}, {"id": 150, "seek": 79720, "start": 797.2800000000001, "end": 803.0400000000001, "text": " irreducibility. For example, when it comes to things like AI, we can ask the question,", "tokens": [50368, 16014, 769, 537, 39802, 13, 1171, 1365, 11, 562, 309, 1487, 281, 721, 411, 7318, 11, 321, 393, 1029, 264, 1168, 11, 50656], "temperature": 0.0, "avg_logprob": -0.07979071561027976, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.015407007187604904}, {"id": 151, "seek": 79720, "start": 804.1600000000001, "end": 809.84, "text": " what in so far as AI is doing computation? And we'll talk about the sense maybe later", "tokens": [50712, 437, 294, 370, 1400, 382, 7318, 307, 884, 24903, 30, 400, 321, 603, 751, 466, 264, 2020, 1310, 1780, 50996], "temperature": 0.0, "avg_logprob": -0.07979071561027976, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.015407007187604904}, {"id": 152, "seek": 79720, "start": 809.84, "end": 815.44, "text": " in which typical modern neural nets are doing only very weak levels of computation.", "tokens": [50996, 294, 597, 7476, 4363, 18161, 36170, 366, 884, 787, 588, 5336, 4358, 295, 24903, 13, 51276], "temperature": 0.0, "avg_logprob": -0.07979071561027976, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.015407007187604904}, {"id": 153, "seek": 79720, "start": 815.44, "end": 821.0400000000001, "text": " But let's imagine that we have a system that is doing computation as computation can be done.", "tokens": [51276, 583, 718, 311, 3811, 300, 321, 362, 257, 1185, 300, 307, 884, 24903, 382, 24903, 393, 312, 1096, 13, 51556], "temperature": 0.0, "avg_logprob": -0.07979071561027976, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.015407007187604904}, {"id": 154, "seek": 82104, "start": 821.68, "end": 827.76, "text": " Well, we sort of have a choice. Either we can say that system is we're going to make that system", "tokens": [50396, 1042, 11, 321, 1333, 295, 362, 257, 3922, 13, 13746, 321, 393, 584, 300, 1185, 307, 321, 434, 516, 281, 652, 300, 1185, 50700], "temperature": 0.0, "avg_logprob": -0.09601544189453125, "compression_ratio": 1.9246031746031746, "no_speech_prob": 0.055213041603565216}, {"id": 155, "seek": 82104, "start": 827.76, "end": 832.8, "text": " computationally reducible. So we know what the outcome is going to be. And so, for example,", "tokens": [50700, 24903, 379, 2783, 32128, 13, 407, 321, 458, 437, 264, 9700, 307, 516, 281, 312, 13, 400, 370, 11, 337, 1365, 11, 50952], "temperature": 0.0, "avg_logprob": -0.09601544189453125, "compression_ratio": 1.9246031746031746, "no_speech_prob": 0.055213041603565216}, {"id": 156, "seek": 82104, "start": 832.8, "end": 838.0, "text": " we can say we're absolutely sure this system will never do the wrong thing because we know its outcomes", "tokens": [50952, 321, 393, 584, 321, 434, 3122, 988, 341, 1185, 486, 1128, 360, 264, 2085, 551, 570, 321, 458, 1080, 10070, 51212], "temperature": 0.0, "avg_logprob": -0.09601544189453125, "compression_ratio": 1.9246031746031746, "no_speech_prob": 0.055213041603565216}, {"id": 157, "seek": 82104, "start": 838.0, "end": 843.36, "text": " and we can constrain it to say that, to set it up so we can sort of prove that we'll never do the", "tokens": [51212, 293, 321, 393, 1817, 7146, 309, 281, 584, 300, 11, 281, 992, 309, 493, 370, 321, 393, 1333, 295, 7081, 300, 321, 603, 1128, 360, 264, 51480], "temperature": 0.0, "avg_logprob": -0.09601544189453125, "compression_ratio": 1.9246031746031746, "no_speech_prob": 0.055213041603565216}, {"id": 158, "seek": 82104, "start": 843.36, "end": 847.68, "text": " wrong thing. It's reducible enough that we can know enough about what it's going to do that we", "tokens": [51480, 2085, 551, 13, 467, 311, 2783, 32128, 1547, 300, 321, 393, 458, 1547, 466, 437, 309, 311, 516, 281, 360, 300, 321, 51696], "temperature": 0.0, "avg_logprob": -0.09601544189453125, "compression_ratio": 1.9246031746031746, "no_speech_prob": 0.055213041603565216}, {"id": 159, "seek": 84768, "start": 847.68, "end": 854.0799999999999, "text": " can know it isn't going to do the wrong thing. So that's plan A. But the problem with plan A", "tokens": [50364, 393, 458, 309, 1943, 380, 516, 281, 360, 264, 2085, 551, 13, 407, 300, 311, 1393, 316, 13, 583, 264, 1154, 365, 1393, 316, 50684], "temperature": 0.0, "avg_logprob": -0.051753929138183594, "compression_ratio": 1.9203187250996017, "no_speech_prob": 0.006853064987808466}, {"id": 160, "seek": 84768, "start": 854.0799999999999, "end": 859.8399999999999, "text": " is that means that the system can't do irreducible computations. The system can only do computations", "tokens": [50684, 307, 300, 1355, 300, 264, 1185, 393, 380, 360, 16014, 769, 32128, 2807, 763, 13, 440, 1185, 393, 787, 360, 2807, 763, 50972], "temperature": 0.0, "avg_logprob": -0.051753929138183594, "compression_ratio": 1.9203187250996017, "no_speech_prob": 0.006853064987808466}, {"id": 161, "seek": 84768, "start": 859.8399999999999, "end": 864.7199999999999, "text": " where we can jump ahead and foresee the outcome. So in a sense, that means we're crippling the", "tokens": [50972, 689, 321, 393, 3012, 2286, 293, 38736, 264, 9700, 13, 407, 294, 257, 2020, 11, 300, 1355, 321, 434, 37667, 1688, 264, 51216], "temperature": 0.0, "avg_logprob": -0.051753929138183594, "compression_ratio": 1.9203187250996017, "no_speech_prob": 0.006853064987808466}, {"id": 162, "seek": 84768, "start": 864.7199999999999, "end": 869.52, "text": " system. We're preventing it from doing what it could do as a computational system. We're saying", "tokens": [51216, 1185, 13, 492, 434, 19965, 309, 490, 884, 437, 309, 727, 360, 382, 257, 28270, 1185, 13, 492, 434, 1566, 51456], "temperature": 0.0, "avg_logprob": -0.051753929138183594, "compression_ratio": 1.9203187250996017, "no_speech_prob": 0.006853064987808466}, {"id": 163, "seek": 84768, "start": 869.52, "end": 874.64, "text": " it's only going to do those things which are kind of reducible. So in a sense, I think it's going", "tokens": [51456, 309, 311, 787, 516, 281, 360, 729, 721, 597, 366, 733, 295, 2783, 32128, 13, 407, 294, 257, 2020, 11, 286, 519, 309, 311, 516, 51712], "temperature": 0.0, "avg_logprob": -0.051753929138183594, "compression_ratio": 1.9203187250996017, "no_speech_prob": 0.006853064987808466}, {"id": 164, "seek": 87464, "start": 874.72, "end": 881.36, "text": " to end up being sort of a big societal choice is, do we want the AIs, computational systems,", "tokens": [50368, 281, 917, 493, 885, 1333, 295, 257, 955, 33472, 3922, 307, 11, 360, 321, 528, 264, 316, 6802, 11, 28270, 3652, 11, 50700], "temperature": 0.0, "avg_logprob": -0.10860948761304219, "compression_ratio": 1.6777251184834123, "no_speech_prob": 0.024195140227675438}, {"id": 165, "seek": 87464, "start": 881.36, "end": 885.1999999999999, "text": " to be able to do all the powerful things that computational systems can do,", "tokens": [50700, 281, 312, 1075, 281, 360, 439, 264, 4005, 721, 300, 28270, 3652, 393, 360, 11, 50892], "temperature": 0.0, "avg_logprob": -0.10860948761304219, "compression_ratio": 1.6777251184834123, "no_speech_prob": 0.024195140227675438}, {"id": 166, "seek": 87464, "start": 885.1999999999999, "end": 889.28, "text": " or do we want to insist that they'll only do things where we can foresee what they'll do?", "tokens": [50892, 420, 360, 321, 528, 281, 13466, 300, 436, 603, 787, 360, 721, 689, 321, 393, 38736, 437, 436, 603, 360, 30, 51096], "temperature": 0.0, "avg_logprob": -0.10860948761304219, "compression_ratio": 1.6777251184834123, "no_speech_prob": 0.024195140227675438}, {"id": 167, "seek": 87464, "start": 889.92, "end": 902.0, "text": " And in a sense, it's kind of like we have a, you could say, well, I'm going to set up all these", "tokens": [51128, 400, 294, 257, 2020, 11, 309, 311, 733, 295, 411, 321, 362, 257, 11, 291, 727, 584, 11, 731, 11, 286, 478, 516, 281, 992, 493, 439, 613, 51732], "temperature": 0.0, "avg_logprob": -0.10860948761304219, "compression_ratio": 1.6777251184834123, "no_speech_prob": 0.024195140227675438}, {"id": 168, "seek": 90200, "start": 902.0, "end": 908.24, "text": " rules for the AIs that make sure they only do the right things. Well, to make that work,", "tokens": [50364, 4474, 337, 264, 316, 6802, 300, 652, 988, 436, 787, 360, 264, 558, 721, 13, 1042, 11, 281, 652, 300, 589, 11, 50676], "temperature": 0.0, "avg_logprob": -0.06512464253248366, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.09752245992422104}, {"id": 169, "seek": 90200, "start": 908.24, "end": 913.44, "text": " you have to have the AIs be sort of computationally reducible. If they're computationally", "tokens": [50676, 291, 362, 281, 362, 264, 316, 6802, 312, 1333, 295, 24903, 379, 2783, 32128, 13, 759, 436, 434, 24903, 379, 50936], "temperature": 0.0, "avg_logprob": -0.06512464253248366, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.09752245992422104}, {"id": 170, "seek": 90200, "start": 913.44, "end": 918.08, "text": " irreducible, well, maybe you can constrain it in all sorts of ways, but there'll always be surprises.", "tokens": [50936, 16014, 769, 32128, 11, 731, 11, 1310, 291, 393, 1817, 7146, 309, 294, 439, 7527, 295, 2098, 11, 457, 456, 603, 1009, 312, 22655, 13, 51168], "temperature": 0.0, "avg_logprob": -0.06512464253248366, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.09752245992422104}, {"id": 171, "seek": 90200, "start": 918.08, "end": 922.16, "text": " There'll always be things where you can't foresee that particular outcome. By the way,", "tokens": [51168, 821, 603, 1009, 312, 721, 689, 291, 393, 380, 38736, 300, 1729, 9700, 13, 3146, 264, 636, 11, 51372], "temperature": 0.0, "avg_logprob": -0.06512464253248366, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.09752245992422104}, {"id": 172, "seek": 90200, "start": 922.16, "end": 927.36, "text": " computational irreducibility has many, many consequences. But another consequence it has", "tokens": [51372, 28270, 16014, 769, 537, 39802, 575, 867, 11, 867, 10098, 13, 583, 1071, 18326, 309, 575, 51632], "temperature": 0.0, "avg_logprob": -0.06512464253248366, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.09752245992422104}, {"id": 173, "seek": 92736, "start": 927.36, "end": 933.28, "text": " is that sort of science will never be finished. There will always be, if we think about kind of,", "tokens": [50364, 307, 300, 1333, 295, 3497, 486, 1128, 312, 4335, 13, 821, 486, 1009, 312, 11, 498, 321, 519, 466, 733, 295, 11, 50660], "temperature": 0.0, "avg_logprob": -0.09118686710392032, "compression_ratio": 2.039647577092511, "no_speech_prob": 0.030065564438700676}, {"id": 174, "seek": 92736, "start": 933.28, "end": 938.0, "text": " there'll always be things where we can't foresee the next thing that will happen.", "tokens": [50660, 456, 603, 1009, 312, 721, 689, 321, 393, 380, 38736, 264, 958, 551, 300, 486, 1051, 13, 50896], "temperature": 0.0, "avg_logprob": -0.09118686710392032, "compression_ratio": 2.039647577092511, "no_speech_prob": 0.030065564438700676}, {"id": 175, "seek": 92736, "start": 938.0, "end": 942.5600000000001, "text": " There will always be surprises in mathematics. There will always be new theorems that can be", "tokens": [50896, 821, 486, 1009, 312, 22655, 294, 18666, 13, 821, 486, 1009, 312, 777, 10299, 2592, 300, 393, 312, 51124], "temperature": 0.0, "avg_logprob": -0.09118686710392032, "compression_ratio": 2.039647577092511, "no_speech_prob": 0.030065564438700676}, {"id": 176, "seek": 92736, "start": 942.5600000000001, "end": 947.6, "text": " proved and so on. The thing that is an issue there in terms of things like will science be", "tokens": [51124, 14617, 293, 370, 322, 13, 440, 551, 300, 307, 364, 2734, 456, 294, 2115, 295, 721, 411, 486, 3497, 312, 51376], "temperature": 0.0, "avg_logprob": -0.09118686710392032, "compression_ratio": 2.039647577092511, "no_speech_prob": 0.030065564438700676}, {"id": 177, "seek": 92736, "start": 947.6, "end": 954.96, "text": " finished and so on is, well, okay, there might be things that were surprises, but are they surprises", "tokens": [51376, 4335, 293, 370, 322, 307, 11, 731, 11, 1392, 11, 456, 1062, 312, 721, 300, 645, 22655, 11, 457, 366, 436, 22655, 51744], "temperature": 0.0, "avg_logprob": -0.09118686710392032, "compression_ratio": 2.039647577092511, "no_speech_prob": 0.030065564438700676}, {"id": 178, "seek": 95496, "start": 954.96, "end": 959.44, "text": " we care about? If we were exploring all of mathematics, we would prove more and more and more", "tokens": [50364, 321, 1127, 466, 30, 759, 321, 645, 12736, 439, 295, 18666, 11, 321, 576, 7081, 544, 293, 544, 293, 544, 50588], "temperature": 0.0, "avg_logprob": -0.09570169034211531, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.01727488450706005}, {"id": 179, "seek": 95496, "start": 959.44, "end": 964.24, "text": " theorems. But it could be that we get to the point where we know all the theorems we care about,", "tokens": [50588, 10299, 2592, 13, 583, 309, 727, 312, 300, 321, 483, 281, 264, 935, 689, 321, 458, 439, 264, 10299, 2592, 321, 1127, 466, 11, 50828], "temperature": 0.0, "avg_logprob": -0.09570169034211531, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.01727488450706005}, {"id": 180, "seek": 95496, "start": 964.24, "end": 968.0, "text": " and anything else is something we're not going to care about. So in a sense, it's a,", "tokens": [50828, 293, 1340, 1646, 307, 746, 321, 434, 406, 516, 281, 1127, 466, 13, 407, 294, 257, 2020, 11, 309, 311, 257, 11, 51016], "temperature": 0.0, "avg_logprob": -0.09570169034211531, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.01727488450706005}, {"id": 181, "seek": 95496, "start": 968.0, "end": 975.12, "text": " there's sort of this, this connection to sort of human issues in what, but the point is that", "tokens": [51016, 456, 311, 1333, 295, 341, 11, 341, 4984, 281, 1333, 295, 1952, 2663, 294, 437, 11, 457, 264, 935, 307, 300, 51372], "temperature": 0.0, "avg_logprob": -0.09570169034211531, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.01727488450706005}, {"id": 182, "seek": 95496, "start": 975.12, "end": 981.12, "text": " there is ultimately an infinite and unlimited frontier of what's possible to discover in science", "tokens": [51372, 456, 307, 6284, 364, 13785, 293, 21950, 35853, 295, 437, 311, 1944, 281, 4411, 294, 3497, 51672], "temperature": 0.0, "avg_logprob": -0.09570169034211531, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.01727488450706005}, {"id": 183, "seek": 98112, "start": 981.2, "end": 985.76, "text": " and so on. By the way, that also relates, maybe we can talk about, to things like,", "tokens": [50368, 293, 370, 322, 13, 3146, 264, 636, 11, 300, 611, 16155, 11, 1310, 321, 393, 751, 466, 11, 281, 721, 411, 11, 50596], "temperature": 0.0, "avg_logprob": -0.10646443565686543, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.015201554633677006}, {"id": 184, "seek": 98112, "start": 987.44, "end": 993.92, "text": " well, okay, let's, let's maybe talk about, so, so kind of this, this idea of computational", "tokens": [50680, 731, 11, 1392, 11, 718, 311, 11, 718, 311, 1310, 751, 466, 11, 370, 11, 370, 733, 295, 341, 11, 341, 1558, 295, 28270, 51004], "temperature": 0.0, "avg_logprob": -0.10646443565686543, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.015201554633677006}, {"id": 185, "seek": 98112, "start": 993.92, "end": 999.84, "text": " irreducibility, that you can't know the outcome of a computational process in general, except by", "tokens": [51004, 16014, 769, 537, 39802, 11, 300, 291, 393, 380, 458, 264, 9700, 295, 257, 28270, 1399, 294, 2674, 11, 3993, 538, 51300], "temperature": 0.0, "avg_logprob": -0.10646443565686543, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.015201554633677006}, {"id": 186, "seek": 98112, "start": 999.84, "end": 1005.04, "text": " running it and seeing what happens. Limitation on science, thing that makes the passage of time", "tokens": [51300, 2614, 309, 293, 2577, 437, 2314, 13, 16406, 4614, 322, 3497, 11, 551, 300, 1669, 264, 11497, 295, 565, 51560], "temperature": 0.0, "avg_logprob": -0.10646443565686543, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.015201554633677006}, {"id": 187, "seek": 100504, "start": 1005.04, "end": 1020.7199999999999, "text": " meaningful, kind of dichotomy for thinking about AI and so on. It's the, so that, so let's see,", "tokens": [50364, 10995, 11, 733, 295, 10390, 310, 8488, 337, 1953, 466, 7318, 293, 370, 322, 13, 467, 311, 264, 11, 370, 300, 11, 370, 718, 311, 536, 11, 51148], "temperature": 0.0, "avg_logprob": -0.1594330589726286, "compression_ratio": 1.4523809523809523, "no_speech_prob": 0.007381839212030172}, {"id": 188, "seek": 100504, "start": 1021.68, "end": 1028.32, "text": " one of the things that's sort of interesting about this is we can just sort of, in this", "tokens": [51196, 472, 295, 264, 721, 300, 311, 1333, 295, 1880, 466, 341, 307, 321, 393, 445, 1333, 295, 11, 294, 341, 51528], "temperature": 0.0, "avg_logprob": -0.1594330589726286, "compression_ratio": 1.4523809523809523, "no_speech_prob": 0.007381839212030172}, {"id": 189, "seek": 102832, "start": 1028.32, "end": 1035.04, "text": " computational universe, we'll find all sorts of, of things that go on. The question becomes,", "tokens": [50364, 28270, 6445, 11, 321, 603, 915, 439, 7527, 295, 11, 295, 721, 300, 352, 322, 13, 440, 1168, 3643, 11, 50700], "temperature": 0.0, "avg_logprob": -0.09886459951047544, "compression_ratio": 1.8413461538461537, "no_speech_prob": 0.059031303972005844}, {"id": 190, "seek": 102832, "start": 1035.04, "end": 1040.48, "text": " sort of, are those things that we find out there, things that we care about or not? In other words,", "tokens": [50700, 1333, 295, 11, 366, 729, 721, 300, 321, 915, 484, 456, 11, 721, 300, 321, 1127, 466, 420, 406, 30, 682, 661, 2283, 11, 50972], "temperature": 0.0, "avg_logprob": -0.09886459951047544, "compression_ratio": 1.8413461538461537, "no_speech_prob": 0.059031303972005844}, {"id": 191, "seek": 102832, "start": 1040.48, "end": 1046.32, "text": " we can go and we can, oh, I don't know, we can, you know, that's a, an example of just a simple", "tokens": [50972, 321, 393, 352, 293, 321, 393, 11, 1954, 11, 286, 500, 380, 458, 11, 321, 393, 11, 291, 458, 11, 300, 311, 257, 11, 364, 1365, 295, 445, 257, 2199, 51264], "temperature": 0.0, "avg_logprob": -0.09886459951047544, "compression_ratio": 1.8413461538461537, "no_speech_prob": 0.059031303972005844}, {"id": 192, "seek": 102832, "start": 1046.32, "end": 1051.12, "text": " rule and what it does and we can get the lots of other, lots of other examples we can, we can,", "tokens": [51264, 4978, 293, 437, 309, 775, 293, 321, 393, 483, 264, 3195, 295, 661, 11, 3195, 295, 661, 5110, 321, 393, 11, 321, 393, 11, 51504], "temperature": 0.0, "avg_logprob": -0.09886459951047544, "compression_ratio": 1.8413461538461537, "no_speech_prob": 0.059031303972005844}, {"id": 193, "seek": 105112, "start": 1051.84, "end": 1061.04, "text": " we can go and do this ourselves if we want to, let's see, and just go find very simple rules", "tokens": [50400, 321, 393, 352, 293, 360, 341, 4175, 498, 321, 528, 281, 11, 718, 311, 536, 11, 293, 445, 352, 915, 588, 2199, 4474, 50860], "temperature": 0.0, "avg_logprob": -0.08900494160859482, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.026214415207505226}, {"id": 194, "seek": 105112, "start": 1061.04, "end": 1065.28, "text": " that do very complicated things. It's easy to kind of launch out into the computational", "tokens": [50860, 300, 360, 588, 6179, 721, 13, 467, 311, 1858, 281, 733, 295, 4025, 484, 666, 264, 28270, 51072], "temperature": 0.0, "avg_logprob": -0.08900494160859482, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.026214415207505226}, {"id": 195, "seek": 105112, "start": 1065.28, "end": 1073.36, "text": " universe and find these things. The question ends up being, so how, what do we humans care about", "tokens": [51072, 6445, 293, 915, 613, 721, 13, 440, 1168, 5314, 493, 885, 11, 370, 577, 11, 437, 360, 321, 6255, 1127, 466, 51476], "temperature": 0.0, "avg_logprob": -0.08900494160859482, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.026214415207505226}, {"id": 196, "seek": 105112, "start": 1073.36, "end": 1079.28, "text": " these things? Well, it could be that this particular thing, we will be able to use it for", "tokens": [51476, 613, 721, 30, 1042, 11, 309, 727, 312, 300, 341, 1729, 551, 11, 321, 486, 312, 1075, 281, 764, 309, 337, 51772], "temperature": 0.0, "avg_logprob": -0.08900494160859482, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.026214415207505226}, {"id": 197, "seek": 107928, "start": 1079.28, "end": 1083.84, "text": " technology in some way. It could be that we'll think this is something very important for art,", "tokens": [50364, 2899, 294, 512, 636, 13, 467, 727, 312, 300, 321, 603, 519, 341, 307, 746, 588, 1021, 337, 1523, 11, 50592], "temperature": 0.0, "avg_logprob": -0.09146110700524372, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.008211614564061165}, {"id": 198, "seek": 107928, "start": 1084.3999999999999, "end": 1088.08, "text": " but it's something where out there in the computational universe, there's kind of an", "tokens": [50620, 457, 309, 311, 746, 689, 484, 456, 294, 264, 28270, 6445, 11, 456, 311, 733, 295, 364, 50804], "temperature": 0.0, "avg_logprob": -0.09146110700524372, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.008211614564061165}, {"id": 199, "seek": 107928, "start": 1088.08, "end": 1095.44, "text": " infinite supply of original things. The question is, which ones do we humans choose to care about?", "tokens": [50804, 13785, 5847, 295, 3380, 721, 13, 440, 1168, 307, 11, 597, 2306, 360, 321, 6255, 2826, 281, 1127, 466, 30, 51172], "temperature": 0.0, "avg_logprob": -0.09146110700524372, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.008211614564061165}, {"id": 200, "seek": 107928, "start": 1096.24, "end": 1103.6, "text": " And, and for example, if we imagine kind of the, the, the future of AIs, you can say, okay AI,", "tokens": [51212, 400, 11, 293, 337, 1365, 11, 498, 321, 3811, 733, 295, 264, 11, 264, 11, 264, 2027, 295, 316, 6802, 11, 291, 393, 584, 11, 1392, 7318, 11, 51580], "temperature": 0.0, "avg_logprob": -0.09146110700524372, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.008211614564061165}, {"id": 201, "seek": 107928, "start": 1103.6, "end": 1108.24, "text": " go out into the computational universe, you can go and create things that have never been seen", "tokens": [51580, 352, 484, 666, 264, 28270, 6445, 11, 291, 393, 352, 293, 1884, 721, 300, 362, 1128, 668, 1612, 51812], "temperature": 0.0, "avg_logprob": -0.09146110700524372, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.008211614564061165}, {"id": 202, "seek": 110824, "start": 1108.24, "end": 1113.76, "text": " before, all kinds of things. The question is, are those things that are of, of kind of human", "tokens": [50364, 949, 11, 439, 3685, 295, 721, 13, 440, 1168, 307, 11, 366, 729, 721, 300, 366, 295, 11, 295, 733, 295, 1952, 50640], "temperature": 0.0, "avg_logprob": -0.09361892777520257, "compression_ratio": 1.5297297297297296, "no_speech_prob": 0.0027402627747505903}, {"id": 203, "seek": 110824, "start": 1113.76, "end": 1120.08, "text": " relevance to us now? Well, one thing you might do, you can actually do a little experiment here,", "tokens": [50640, 32684, 281, 505, 586, 30, 1042, 11, 472, 551, 291, 1062, 360, 11, 291, 393, 767, 360, 257, 707, 5120, 510, 11, 50956], "temperature": 0.0, "avg_logprob": -0.09361892777520257, "compression_ratio": 1.5297297297297296, "no_speech_prob": 0.0027402627747505903}, {"id": 204, "seek": 110824, "start": 1120.08, "end": 1128.64, "text": " let me show you something. Where is it? So for example, we could say we could take some image", "tokens": [50956, 718, 385, 855, 291, 746, 13, 2305, 307, 309, 30, 407, 337, 1365, 11, 321, 727, 584, 321, 727, 747, 512, 3256, 51384], "temperature": 0.0, "avg_logprob": -0.09361892777520257, "compression_ratio": 1.5297297297297296, "no_speech_prob": 0.0027402627747505903}, {"id": 205, "seek": 112864, "start": 1128.64, "end": 1138.88, "text": " generation AI, and this is just a diffusion image generator. And we could say, let's, let's look,", "tokens": [50364, 5125, 7318, 11, 293, 341, 307, 445, 257, 25242, 3256, 19265, 13, 400, 321, 727, 584, 11, 718, 311, 11, 718, 311, 574, 11, 50876], "temperature": 0.0, "avg_logprob": -0.0871532931186185, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.1572573482990265}, {"id": 206, "seek": 112864, "start": 1138.88, "end": 1145.44, "text": " let's ask the thing to make a picture of a cat in a party hat. Okay, but inside the AI,", "tokens": [50876, 718, 311, 1029, 264, 551, 281, 652, 257, 3036, 295, 257, 3857, 294, 257, 3595, 2385, 13, 1033, 11, 457, 1854, 264, 7318, 11, 51204], "temperature": 0.0, "avg_logprob": -0.0871532931186185, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.1572573482990265}, {"id": 207, "seek": 112864, "start": 1146.0800000000002, "end": 1151.92, "text": " it's got some, you know, embedding vector, it's got some, some set of numbers that describe", "tokens": [51236, 309, 311, 658, 512, 11, 291, 458, 11, 12240, 3584, 8062, 11, 309, 311, 658, 512, 11, 512, 992, 295, 3547, 300, 6786, 51528], "temperature": 0.0, "avg_logprob": -0.0871532931186185, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.1572573482990265}, {"id": 208, "seek": 112864, "start": 1151.92, "end": 1157.2800000000002, "text": " that is its version of what that concept is. But one thing we could do is something very simple", "tokens": [51528, 300, 307, 1080, 3037, 295, 437, 300, 3410, 307, 13, 583, 472, 551, 321, 727, 360, 307, 746, 588, 2199, 51796], "temperature": 0.0, "avg_logprob": -0.0871532931186185, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.1572573482990265}, {"id": 209, "seek": 115728, "start": 1158.08, "end": 1163.52, "text": " to sort of explore the universe of possibilities. We could say we're going to take this AI that's", "tokens": [50404, 281, 1333, 295, 6839, 264, 6445, 295, 12178, 13, 492, 727, 584, 321, 434, 516, 281, 747, 341, 7318, 300, 311, 50676], "temperature": 0.0, "avg_logprob": -0.0823599387859476, "compression_ratio": 1.7756653992395437, "no_speech_prob": 0.006827753037214279}, {"id": 210, "seek": 115728, "start": 1163.52, "end": 1168.08, "text": " very aligned with human interest because it's been trained on billions of human images.", "tokens": [50676, 588, 17962, 365, 1952, 1179, 570, 309, 311, 668, 8895, 322, 17375, 295, 1952, 5267, 13, 50904], "temperature": 0.0, "avg_logprob": -0.0823599387859476, "compression_ratio": 1.7756653992395437, "no_speech_prob": 0.006827753037214279}, {"id": 211, "seek": 115728, "start": 1168.08, "end": 1173.84, "text": " But nevertheless, we could say, let's take this AI and let's sort of move around in this space of", "tokens": [50904, 583, 26924, 11, 321, 727, 584, 11, 718, 311, 747, 341, 7318, 293, 718, 311, 1333, 295, 1286, 926, 294, 341, 1901, 295, 51192], "temperature": 0.0, "avg_logprob": -0.0823599387859476, "compression_ratio": 1.7756653992395437, "no_speech_prob": 0.006827753037214279}, {"id": 212, "seek": 115728, "start": 1173.84, "end": 1179.44, "text": " possibilities. And so for some set of numbers, we've got the cat and the party hat. But as we", "tokens": [51192, 12178, 13, 400, 370, 337, 512, 992, 295, 3547, 11, 321, 600, 658, 264, 3857, 293, 264, 3595, 2385, 13, 583, 382, 321, 51472], "temperature": 0.0, "avg_logprob": -0.0823599387859476, "compression_ratio": 1.7756653992395437, "no_speech_prob": 0.006827753037214279}, {"id": 213, "seek": 115728, "start": 1179.44, "end": 1184.24, "text": " change those numbers, we're moving out from that. And we have this kind of in the middle,", "tokens": [51472, 1319, 729, 3547, 11, 321, 434, 2684, 484, 490, 300, 13, 400, 321, 362, 341, 733, 295, 294, 264, 2808, 11, 51712], "temperature": 0.0, "avg_logprob": -0.0823599387859476, "compression_ratio": 1.7756653992395437, "no_speech_prob": 0.006827753037214279}, {"id": 214, "seek": 118424, "start": 1184.24, "end": 1189.92, "text": " we have this thing we might sort of describe as kind of cat island, that is things that to us kind", "tokens": [50364, 321, 362, 341, 551, 321, 1062, 1333, 295, 6786, 382, 733, 295, 3857, 6077, 11, 300, 307, 721, 300, 281, 505, 733, 50648], "temperature": 0.0, "avg_logprob": -0.11485088893345424, "compression_ratio": 1.8984375, "no_speech_prob": 0.003788398578763008}, {"id": 215, "seek": 118424, "start": 1189.92, "end": 1196.08, "text": " of look like cats. But then we go further away, and we'll get into things which aren't like cats.", "tokens": [50648, 295, 574, 411, 11111, 13, 583, 550, 321, 352, 3052, 1314, 11, 293, 321, 603, 483, 666, 721, 597, 3212, 380, 411, 11111, 13, 50956], "temperature": 0.0, "avg_logprob": -0.11485088893345424, "compression_ratio": 1.8984375, "no_speech_prob": 0.003788398578763008}, {"id": 216, "seek": 118424, "start": 1196.08, "end": 1200.72, "text": " If we go far enough, you know, we'll, we'll be able to go, I don't know, as an example, we'd be", "tokens": [50956, 759, 321, 352, 1400, 1547, 11, 291, 458, 11, 321, 603, 11, 321, 603, 312, 1075, 281, 352, 11, 286, 500, 380, 458, 11, 382, 364, 1365, 11, 321, 1116, 312, 51188], "temperature": 0.0, "avg_logprob": -0.11485088893345424, "compression_ratio": 1.8984375, "no_speech_prob": 0.003788398578763008}, {"id": 217, "seek": 118424, "start": 1200.72, "end": 1206.08, "text": " able to go from, what is that going to? That's, well, okay, here's one that goes from a cat to", "tokens": [51188, 1075, 281, 352, 490, 11, 437, 307, 300, 516, 281, 30, 663, 311, 11, 731, 11, 1392, 11, 510, 311, 472, 300, 1709, 490, 257, 3857, 281, 51456], "temperature": 0.0, "avg_logprob": -0.11485088893345424, "compression_ratio": 1.8984375, "no_speech_prob": 0.003788398578763008}, {"id": 218, "seek": 118424, "start": 1206.08, "end": 1213.6, "text": " a dog, we're going through through this kind of meaning space from a cat to a dog. But in general,", "tokens": [51456, 257, 3000, 11, 321, 434, 516, 807, 807, 341, 733, 295, 3620, 1901, 490, 257, 3857, 281, 257, 3000, 13, 583, 294, 2674, 11, 51832], "temperature": 0.0, "avg_logprob": -0.11485088893345424, "compression_ratio": 1.8984375, "no_speech_prob": 0.003788398578763008}, {"id": 219, "seek": 121360, "start": 1213.6, "end": 1221.28, "text": " what we'll find is that we in this sort of space of possibilities, there's this region that corresponds", "tokens": [50364, 437, 321, 603, 915, 307, 300, 321, 294, 341, 1333, 295, 1901, 295, 12178, 11, 456, 311, 341, 4458, 300, 23249, 50748], "temperature": 0.0, "avg_logprob": -0.07086219001062137, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0025479269679635763}, {"id": 220, "seek": 121360, "start": 1221.28, "end": 1227.36, "text": " to this concept that we have of a cat and a party hat. But as we go away from that, eventually,", "tokens": [50748, 281, 341, 3410, 300, 321, 362, 295, 257, 3857, 293, 257, 3595, 2385, 13, 583, 382, 321, 352, 1314, 490, 300, 11, 4728, 11, 51052], "temperature": 0.0, "avg_logprob": -0.07086219001062137, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0025479269679635763}, {"id": 221, "seek": 121360, "start": 1227.36, "end": 1232.0, "text": " we move far enough, we'll get to a picture of a, you know, a dog wearing a sweater or something.", "tokens": [51052, 321, 1286, 1400, 1547, 11, 321, 603, 483, 281, 257, 3036, 295, 257, 11, 291, 458, 11, 257, 3000, 4769, 257, 26550, 420, 746, 13, 51284], "temperature": 0.0, "avg_logprob": -0.07086219001062137, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0025479269679635763}, {"id": 222, "seek": 121360, "start": 1232.9599999999998, "end": 1239.36, "text": " But we go through a large volume of inter concept space of things which are images,", "tokens": [51332, 583, 321, 352, 807, 257, 2416, 5523, 295, 728, 3410, 1901, 295, 721, 597, 366, 5267, 11, 51652], "temperature": 0.0, "avg_logprob": -0.07086219001062137, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0025479269679635763}, {"id": 223, "seek": 123936, "start": 1239.36, "end": 1245.12, "text": " which were generated by this AI using, you know, computationally generated out there in the", "tokens": [50364, 597, 645, 10833, 538, 341, 7318, 1228, 11, 291, 458, 11, 24903, 379, 10833, 484, 456, 294, 264, 50652], "temperature": 0.0, "avg_logprob": -0.07777283570476781, "compression_ratio": 1.7185185185185186, "no_speech_prob": 0.012361656874418259}, {"id": 224, "seek": 123936, "start": 1245.12, "end": 1250.8799999999999, "text": " computational universe, even set up to be quite aligned with kind of the pictures that we humans", "tokens": [50652, 28270, 6445, 11, 754, 992, 493, 281, 312, 1596, 17962, 365, 733, 295, 264, 5242, 300, 321, 6255, 50940], "temperature": 0.0, "avg_logprob": -0.07777283570476781, "compression_ratio": 1.7185185185185186, "no_speech_prob": 0.012361656874418259}, {"id": 225, "seek": 123936, "start": 1250.8799999999999, "end": 1255.12, "text": " have put on the web. But nevertheless, they're not things which are normally described by a", "tokens": [50940, 362, 829, 322, 264, 3670, 13, 583, 26924, 11, 436, 434, 406, 721, 597, 366, 5646, 7619, 538, 257, 51152], "temperature": 0.0, "avg_logprob": -0.07777283570476781, "compression_ratio": 1.7185185185185186, "no_speech_prob": 0.012361656874418259}, {"id": 226, "seek": 123936, "start": 1255.12, "end": 1260.0, "text": " word like a cat or a dog or whatever else. So you might ask the question, you know, in an image", "tokens": [51152, 1349, 411, 257, 3857, 420, 257, 3000, 420, 2035, 1646, 13, 407, 291, 1062, 1029, 264, 1168, 11, 291, 458, 11, 294, 364, 3256, 51396], "temperature": 0.0, "avg_logprob": -0.07777283570476781, "compression_ratio": 1.7185185185185186, "no_speech_prob": 0.012361656874418259}, {"id": 227, "seek": 123936, "start": 1260.0, "end": 1266.56, "text": " generation AI, what volume of the space of possibilities is covered by concepts that we", "tokens": [51396, 5125, 7318, 11, 437, 5523, 295, 264, 1901, 295, 12178, 307, 5343, 538, 10392, 300, 321, 51724], "temperature": 0.0, "avg_logprob": -0.07777283570476781, "compression_ratio": 1.7185185185185186, "no_speech_prob": 0.012361656874418259}, {"id": 228, "seek": 126656, "start": 1266.56, "end": 1273.6799999999998, "text": " have already defined? The answer is maybe one part and 10 to the 600. So in other words, there's", "tokens": [50364, 362, 1217, 7642, 30, 440, 1867, 307, 1310, 472, 644, 293, 1266, 281, 264, 11849, 13, 407, 294, 661, 2283, 11, 456, 311, 50720], "temperature": 0.0, "avg_logprob": -0.061427681716447025, "compression_ratio": 1.6512605042016806, "no_speech_prob": 0.005088651552796364}, {"id": 229, "seek": 126656, "start": 1273.6799999999998, "end": 1281.6, "text": " this vast kind of inter concept space of possible images, only tiny corners of which are described", "tokens": [50720, 341, 8369, 733, 295, 728, 3410, 1901, 295, 1944, 5267, 11, 787, 5870, 12413, 295, 597, 366, 7619, 51116], "temperature": 0.0, "avg_logprob": -0.061427681716447025, "compression_ratio": 1.6512605042016806, "no_speech_prob": 0.005088651552796364}, {"id": 230, "seek": 126656, "start": 1281.6, "end": 1288.72, "text": " by words that we have in human languages. So in a sense, as we look at this kind of inter concept", "tokens": [51116, 538, 2283, 300, 321, 362, 294, 1952, 8650, 13, 407, 294, 257, 2020, 11, 382, 321, 574, 412, 341, 733, 295, 728, 3410, 51472], "temperature": 0.0, "avg_logprob": -0.061427681716447025, "compression_ratio": 1.6512605042016806, "no_speech_prob": 0.005088651552796364}, {"id": 231, "seek": 126656, "start": 1288.72, "end": 1294.32, "text": " space, we could say, you know, we don't necessarily have a word to describe some of these patterns,", "tokens": [51472, 1901, 11, 321, 727, 584, 11, 291, 458, 11, 321, 500, 380, 4725, 362, 257, 1349, 281, 6786, 512, 295, 613, 8294, 11, 51752], "temperature": 0.0, "avg_logprob": -0.061427681716447025, "compression_ratio": 1.6512605042016806, "no_speech_prob": 0.005088651552796364}, {"id": 232, "seek": 129432, "start": 1294.3999999999999, "end": 1299.28, "text": " but we might say, oh, that's kind of a cool pattern. And maybe we decide at some point that", "tokens": [50368, 457, 321, 1062, 584, 11, 1954, 11, 300, 311, 733, 295, 257, 1627, 5102, 13, 400, 1310, 321, 4536, 412, 512, 935, 300, 50612], "temperature": 0.0, "avg_logprob": -0.08943700360822247, "compression_ratio": 1.807843137254902, "no_speech_prob": 0.0013280390994623303}, {"id": 233, "seek": 129432, "start": 1299.28, "end": 1303.6, "text": " that's a particular style of art. And eventually we get a word for it. And then we develop this", "tokens": [50612, 300, 311, 257, 1729, 3758, 295, 1523, 13, 400, 4728, 321, 483, 257, 1349, 337, 309, 13, 400, 550, 321, 1499, 341, 50828], "temperature": 0.0, "avg_logprob": -0.08943700360822247, "compression_ratio": 1.807843137254902, "no_speech_prob": 0.0013280390994623303}, {"id": 234, "seek": 129432, "start": 1303.6, "end": 1309.12, "text": " whole kind of human interest in that particular piece of what was inter concept space. And now", "tokens": [50828, 1379, 733, 295, 1952, 1179, 294, 300, 1729, 2522, 295, 437, 390, 728, 3410, 1901, 13, 400, 586, 51104], "temperature": 0.0, "avg_logprob": -0.08943700360822247, "compression_ratio": 1.807843137254902, "no_speech_prob": 0.0013280390994623303}, {"id": 235, "seek": 129432, "start": 1309.12, "end": 1318.32, "text": " that becomes a concept in our languages and so on. So this idea is sort of this core idea that", "tokens": [51104, 300, 3643, 257, 3410, 294, 527, 8650, 293, 370, 322, 13, 407, 341, 1558, 307, 1333, 295, 341, 4965, 1558, 300, 51564], "temperature": 0.0, "avg_logprob": -0.08943700360822247, "compression_ratio": 1.807843137254902, "no_speech_prob": 0.0013280390994623303}, {"id": 236, "seek": 129432, "start": 1318.32, "end": 1322.72, "text": " there's this huge space, this huge kind of computational universe of possibilities,", "tokens": [51564, 456, 311, 341, 2603, 1901, 11, 341, 2603, 733, 295, 28270, 6445, 295, 12178, 11, 51784], "temperature": 0.0, "avg_logprob": -0.08943700360822247, "compression_ratio": 1.807843137254902, "no_speech_prob": 0.0013280390994623303}, {"id": 237, "seek": 132272, "start": 1322.72, "end": 1328.24, "text": " even reduced here by ones that are sort of images aligned with images that we put on the web.", "tokens": [50364, 754, 9212, 510, 538, 2306, 300, 366, 1333, 295, 5267, 17962, 365, 5267, 300, 321, 829, 322, 264, 3670, 13, 50640], "temperature": 0.0, "avg_logprob": -0.06351103321198494, "compression_ratio": 1.8078817733990147, "no_speech_prob": 0.0029303862247616053}, {"id": 238, "seek": 132272, "start": 1328.24, "end": 1333.68, "text": " Even if you reduce it in that way, the part of that space that we have so far explored,", "tokens": [50640, 2754, 498, 291, 5407, 309, 294, 300, 636, 11, 264, 644, 295, 300, 1901, 300, 321, 362, 370, 1400, 24016, 11, 50912], "temperature": 0.0, "avg_logprob": -0.06351103321198494, "compression_ratio": 1.8078817733990147, "no_speech_prob": 0.0029303862247616053}, {"id": 239, "seek": 132272, "start": 1333.68, "end": 1340.24, "text": " that we have so far come up with words for and described with concepts, is a tiny part of the", "tokens": [50912, 300, 321, 362, 370, 1400, 808, 493, 365, 2283, 337, 293, 7619, 365, 10392, 11, 307, 257, 5870, 644, 295, 264, 51240], "temperature": 0.0, "avg_logprob": -0.06351103321198494, "compression_ratio": 1.8078817733990147, "no_speech_prob": 0.0029303862247616053}, {"id": 240, "seek": 132272, "start": 1340.24, "end": 1346.32, "text": " space. And there's vastly more that is kind of be found in the sort of inter concept space.", "tokens": [51240, 1901, 13, 400, 456, 311, 41426, 544, 300, 307, 733, 295, 312, 1352, 294, 264, 1333, 295, 728, 3410, 1901, 13, 51544], "temperature": 0.0, "avg_logprob": -0.06351103321198494, "compression_ratio": 1.8078817733990147, "no_speech_prob": 0.0029303862247616053}, {"id": 241, "seek": 134632, "start": 1346.3999999999999, "end": 1351.9199999999998, "text": " Now, what, you know, can we describe kind of the way that kind of we, we think about", "tokens": [50368, 823, 11, 437, 11, 291, 458, 11, 393, 321, 6786, 733, 295, 264, 636, 300, 733, 295, 321, 11, 321, 519, 466, 50644], "temperature": 0.0, "avg_logprob": -0.12149764523647799, "compression_ratio": 1.8803418803418803, "no_speech_prob": 0.006825888995081186}, {"id": 242, "seek": 134632, "start": 1351.9199999999998, "end": 1358.96, "text": " sort of our progression in kind of the progression of human civilization and so on.", "tokens": [50644, 1333, 295, 527, 18733, 294, 733, 295, 264, 18733, 295, 1952, 18036, 293, 370, 322, 13, 50996], "temperature": 0.0, "avg_logprob": -0.12149764523647799, "compression_ratio": 1.8803418803418803, "no_speech_prob": 0.006825888995081186}, {"id": 243, "seek": 134632, "start": 1358.96, "end": 1364.6399999999999, "text": " In some sense, you can think about us as progressively colonizing inter concept space.", "tokens": [50996, 682, 512, 2020, 11, 291, 393, 519, 466, 505, 382, 46667, 8255, 3319, 728, 3410, 1901, 13, 51280], "temperature": 0.0, "avg_logprob": -0.12149764523647799, "compression_ratio": 1.8803418803418803, "no_speech_prob": 0.006825888995081186}, {"id": 244, "seek": 134632, "start": 1364.6399999999999, "end": 1369.28, "text": " We're progressively coming up with things coming up with, we're coming up with sort of this", "tokens": [51280, 492, 434, 46667, 1348, 493, 365, 721, 1348, 493, 365, 11, 321, 434, 1348, 493, 365, 1333, 295, 341, 51512], "temperature": 0.0, "avg_logprob": -0.12149764523647799, "compression_ratio": 1.8803418803418803, "no_speech_prob": 0.006825888995081186}, {"id": 245, "seek": 134632, "start": 1369.28, "end": 1375.84, "text": " social construct of language that that different ones of us sort of collectively understand,", "tokens": [51512, 2093, 7690, 295, 2856, 300, 300, 819, 2306, 295, 505, 1333, 295, 24341, 1223, 11, 51840], "temperature": 0.0, "avg_logprob": -0.12149764523647799, "compression_ratio": 1.8803418803418803, "no_speech_prob": 0.006825888995081186}, {"id": 246, "seek": 137584, "start": 1375.84, "end": 1380.72, "text": " that corresponds to the, these different points in the space of possibilities.", "tokens": [50364, 300, 23249, 281, 264, 11, 613, 819, 2793, 294, 264, 1901, 295, 12178, 13, 50608], "temperature": 0.0, "avg_logprob": -0.11475746518089658, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0018882463918998837}, {"id": 247, "seek": 137584, "start": 1381.28, "end": 1386.1599999999999, "text": " And sort of the progression of civilization, we can think of as being this progressive kind of", "tokens": [50636, 400, 1333, 295, 264, 18733, 295, 18036, 11, 321, 393, 519, 295, 382, 885, 341, 16131, 733, 295, 50880], "temperature": 0.0, "avg_logprob": -0.11475746518089658, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0018882463918998837}, {"id": 248, "seek": 137584, "start": 1387.12, "end": 1392.8799999999999, "text": " progressive exploration of inter concept space. And, you know, as we invent new paradigms for", "tokens": [50928, 16131, 16197, 295, 728, 3410, 1901, 13, 400, 11, 291, 458, 11, 382, 321, 7962, 777, 13480, 328, 2592, 337, 51216], "temperature": 0.0, "avg_logprob": -0.11475746518089658, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0018882463918998837}, {"id": 249, "seek": 137584, "start": 1392.8799999999999, "end": 1396.72, "text": " things, we get to kind of, or new ways of describing things, we get to kind of move", "tokens": [51216, 721, 11, 321, 483, 281, 733, 295, 11, 420, 777, 2098, 295, 16141, 721, 11, 321, 483, 281, 733, 295, 1286, 51408], "temperature": 0.0, "avg_logprob": -0.11475746518089658, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0018882463918998837}, {"id": 250, "seek": 137584, "start": 1396.72, "end": 1403.4399999999998, "text": " outwards in the space. Now, for example, in my day job of creating computational language to", "tokens": [51408, 484, 2015, 294, 264, 1901, 13, 823, 11, 337, 1365, 11, 294, 452, 786, 1691, 295, 4084, 28270, 2856, 281, 51744], "temperature": 0.0, "avg_logprob": -0.11475746518089658, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0018882463918998837}, {"id": 251, "seek": 140344, "start": 1403.44, "end": 1411.76, "text": " describe things, my, my mission in a sense is to find those, those places in the space of", "tokens": [50364, 6786, 721, 11, 452, 11, 452, 4447, 294, 257, 2020, 307, 281, 915, 729, 11, 729, 3190, 294, 264, 1901, 295, 50780], "temperature": 0.0, "avg_logprob": -0.07515542073683305, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0054161506704986095}, {"id": 252, "seek": 140344, "start": 1411.76, "end": 1417.76, "text": " possibilities that we humans care about, and that we can use as kind of building blocks", "tokens": [50780, 12178, 300, 321, 6255, 1127, 466, 11, 293, 300, 321, 393, 764, 382, 733, 295, 2390, 8474, 51080], "temperature": 0.0, "avg_logprob": -0.07515542073683305, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0054161506704986095}, {"id": 253, "seek": 140344, "start": 1417.76, "end": 1424.88, "text": " to construct kind of in a computational way, a description of what we want. But there's kind", "tokens": [51080, 281, 7690, 733, 295, 294, 257, 28270, 636, 11, 257, 3855, 295, 437, 321, 528, 13, 583, 456, 311, 733, 51436], "temperature": 0.0, "avg_logprob": -0.07515542073683305, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0054161506704986095}, {"id": 254, "seek": 140344, "start": 1424.88, "end": 1430.8, "text": " of a broader science of what's in principle out there, which is broader than the things that we", "tokens": [51436, 295, 257, 13227, 3497, 295, 437, 311, 294, 8665, 484, 456, 11, 597, 307, 13227, 813, 264, 721, 300, 321, 51732], "temperature": 0.0, "avg_logprob": -0.07515542073683305, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0054161506704986095}, {"id": 255, "seek": 143080, "start": 1430.8, "end": 1435.6, "text": " humans have so far chosen to, to come up with words for and so on and have languages for.", "tokens": [50364, 6255, 362, 370, 1400, 8614, 281, 11, 281, 808, 493, 365, 2283, 337, 293, 370, 322, 293, 362, 8650, 337, 13, 50604], "temperature": 0.0, "avg_logprob": -0.0989997148513794, "compression_ratio": 1.7293233082706767, "no_speech_prob": 0.0030490956269204617}, {"id": 256, "seek": 143080, "start": 1436.48, "end": 1441.76, "text": " Well, just to kind of fill out a little bit, kind of the, a little bit more of kind of the,", "tokens": [50648, 1042, 11, 445, 281, 733, 295, 2836, 484, 257, 707, 857, 11, 733, 295, 264, 11, 257, 707, 857, 544, 295, 733, 295, 264, 11, 50912], "temperature": 0.0, "avg_logprob": -0.0989997148513794, "compression_ratio": 1.7293233082706767, "no_speech_prob": 0.0030490956269204617}, {"id": 257, "seek": 143080, "start": 1442.32, "end": 1449.52, "text": " the world view that develops from all of this, we can ask questions about, okay, what about", "tokens": [50940, 264, 1002, 1910, 300, 25453, 490, 439, 295, 341, 11, 321, 393, 1029, 1651, 466, 11, 1392, 11, 437, 466, 51300], "temperature": 0.0, "avg_logprob": -0.0989997148513794, "compression_ratio": 1.7293233082706767, "no_speech_prob": 0.0030490956269204617}, {"id": 258, "seek": 143080, "start": 1449.52, "end": 1454.6399999999999, "text": " our physical world? How is that constructed? What is the, what's kind of the, the underlying", "tokens": [51300, 527, 4001, 1002, 30, 1012, 307, 300, 17083, 30, 708, 307, 264, 11, 437, 311, 733, 295, 264, 11, 264, 14217, 51556], "temperature": 0.0, "avg_logprob": -0.0989997148513794, "compression_ratio": 1.7293233082706767, "no_speech_prob": 0.0030490956269204617}, {"id": 259, "seek": 143080, "start": 1454.6399999999999, "end": 1458.6399999999999, "text": " structure there? And one of the things that's been very exciting to me in the last few years,", "tokens": [51556, 3877, 456, 30, 400, 472, 295, 264, 721, 300, 311, 668, 588, 4670, 281, 385, 294, 264, 1036, 1326, 924, 11, 51756], "temperature": 0.0, "avg_logprob": -0.0989997148513794, "compression_ratio": 1.7293233082706767, "no_speech_prob": 0.0030490956269204617}, {"id": 260, "seek": 145864, "start": 1458.72, "end": 1465.92, "text": " something I really did not expect sort of to, to, to happen is that it's, it's turned out that we've", "tokens": [50368, 746, 286, 534, 630, 406, 2066, 1333, 295, 281, 11, 281, 11, 281, 1051, 307, 300, 309, 311, 11, 309, 311, 3574, 484, 300, 321, 600, 50728], "temperature": 0.0, "avg_logprob": -0.0953091197543674, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.003101303242146969}, {"id": 261, "seek": 145864, "start": 1465.92, "end": 1473.1200000000001, "text": " been able to work out that how this kind of computational ideas provide sort of an ultimate", "tokens": [50728, 668, 1075, 281, 589, 484, 300, 577, 341, 733, 295, 28270, 3487, 2893, 1333, 295, 364, 9705, 51088], "temperature": 0.0, "avg_logprob": -0.0953091197543674, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.003101303242146969}, {"id": 262, "seek": 145864, "start": 1474.24, "end": 1480.0, "text": " infrastructure, an ultimate kind of machine code for the physical universe. And what, what,", "tokens": [51144, 6896, 11, 364, 9705, 733, 295, 3479, 3089, 337, 264, 4001, 6445, 13, 400, 437, 11, 437, 11, 51432], "temperature": 0.0, "avg_logprob": -0.0953091197543674, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.003101303242146969}, {"id": 263, "seek": 145864, "start": 1480.0, "end": 1484.5600000000002, "text": " let me describe that a little bit because we're going to come back to this question of concepts", "tokens": [51432, 718, 385, 6786, 300, 257, 707, 857, 570, 321, 434, 516, 281, 808, 646, 281, 341, 1168, 295, 10392, 51660], "temperature": 0.0, "avg_logprob": -0.0953091197543674, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.003101303242146969}, {"id": 264, "seek": 148456, "start": 1484.56, "end": 1489.44, "text": " and into concept space and so on, but we're going to come at it now from a different direction", "tokens": [50364, 293, 666, 3410, 1901, 293, 370, 322, 11, 457, 321, 434, 516, 281, 808, 412, 309, 586, 490, 257, 819, 3513, 50608], "temperature": 0.0, "avg_logprob": -0.11042411268250016, "compression_ratio": 1.6468531468531469, "no_speech_prob": 0.011586250737309456}, {"id": 265, "seek": 148456, "start": 1489.44, "end": 1497.12, "text": " from understanding the structure of the physical world. So, sort of big picture, back in antiquity,", "tokens": [50608, 490, 3701, 264, 3877, 295, 264, 4001, 1002, 13, 407, 11, 1333, 295, 955, 3036, 11, 646, 294, 41036, 507, 11, 50992], "temperature": 0.0, "avg_logprob": -0.11042411268250016, "compression_ratio": 1.6468531468531469, "no_speech_prob": 0.011586250737309456}, {"id": 266, "seek": 148456, "start": 1497.6799999999998, "end": 1501.12, "text": " people were arguing, you know, is the world discreet or is it continuous? Is it made of", "tokens": [51020, 561, 645, 19697, 11, 291, 458, 11, 307, 264, 1002, 2983, 4751, 420, 307, 309, 10957, 30, 1119, 309, 1027, 295, 51192], "temperature": 0.0, "avg_logprob": -0.11042411268250016, "compression_ratio": 1.6468531468531469, "no_speech_prob": 0.011586250737309456}, {"id": 267, "seek": 148456, "start": 1501.12, "end": 1507.44, "text": " atoms or is it just things that are sort of flowing? And one didn't know. End of the 19th", "tokens": [51192, 16871, 420, 307, 309, 445, 721, 300, 366, 1333, 295, 13974, 30, 400, 472, 994, 380, 458, 13, 6967, 295, 264, 1294, 392, 51508], "temperature": 0.0, "avg_logprob": -0.11042411268250016, "compression_ratio": 1.6468531468531469, "no_speech_prob": 0.011586250737309456}, {"id": 268, "seek": 148456, "start": 1507.44, "end": 1514.24, "text": " century, it became clear, yes, there are molecules, matter is discreet. A little bit later, became", "tokens": [51508, 4901, 11, 309, 3062, 1850, 11, 2086, 11, 456, 366, 13093, 11, 1871, 307, 2983, 4751, 13, 316, 707, 857, 1780, 11, 3062, 51848], "temperature": 0.0, "avg_logprob": -0.11042411268250016, "compression_ratio": 1.6468531468531469, "no_speech_prob": 0.011586250737309456}, {"id": 269, "seek": 151424, "start": 1514.24, "end": 1519.44, "text": " clear, there are photons like can be thought of as being discreet. At that time, people mostly", "tokens": [50364, 1850, 11, 456, 366, 40209, 411, 393, 312, 1194, 295, 382, 885, 2983, 4751, 13, 1711, 300, 565, 11, 561, 5240, 50624], "temperature": 0.0, "avg_logprob": -0.06608618860659392, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.002587120281532407}, {"id": 270, "seek": 151424, "start": 1519.44, "end": 1524.48, "text": " assumed that space would turn out to be discreet as well. But for various reasons, nobody technically", "tokens": [50624, 15895, 300, 1901, 576, 1261, 484, 281, 312, 2983, 4751, 382, 731, 13, 583, 337, 3683, 4112, 11, 5079, 12120, 50876], "temperature": 0.0, "avg_logprob": -0.06608618860659392, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.002587120281532407}, {"id": 271, "seek": 151424, "start": 1524.48, "end": 1529.92, "text": " managed to make that work. And so physics kind of went on with the space is continuous, you can", "tokens": [50876, 6453, 281, 652, 300, 589, 13, 400, 370, 10649, 733, 295, 1437, 322, 365, 264, 1901, 307, 10957, 11, 291, 393, 51148], "temperature": 0.0, "avg_logprob": -0.06608618860659392, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.002587120281532407}, {"id": 272, "seek": 151424, "start": 1529.92, "end": 1535.68, "text": " kind of put things anywhere you want in space. Well, if you're thinking about things in kind", "tokens": [51148, 733, 295, 829, 721, 4992, 291, 528, 294, 1901, 13, 1042, 11, 498, 291, 434, 1953, 466, 721, 294, 733, 51436], "temperature": 0.0, "avg_logprob": -0.06608618860659392, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.002587120281532407}, {"id": 273, "seek": 151424, "start": 1535.68, "end": 1540.8, "text": " of computational terms, you're immediately led to say, wait a minute, you know, perhaps space is", "tokens": [51436, 295, 28270, 2115, 11, 291, 434, 4258, 4684, 281, 584, 11, 1699, 257, 3456, 11, 291, 458, 11, 4317, 1901, 307, 51692], "temperature": 0.0, "avg_logprob": -0.06608618860659392, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.002587120281532407}, {"id": 274, "seek": 154080, "start": 1540.8, "end": 1545.68, "text": " actually fundamentally a computational construct, fundamentally a discreet kind of thing. And the", "tokens": [50364, 767, 17879, 257, 28270, 7690, 11, 17879, 257, 2983, 4751, 733, 295, 551, 13, 400, 264, 50608], "temperature": 0.0, "avg_logprob": -0.09216076594132644, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.009536770172417164}, {"id": 275, "seek": 154080, "start": 1545.68, "end": 1550.96, "text": " big surprise of four years ago now was that, yes, we actually managed to figure out how to make that", "tokens": [50608, 955, 6365, 295, 1451, 924, 2057, 586, 390, 300, 11, 2086, 11, 321, 767, 6453, 281, 2573, 484, 577, 281, 652, 300, 50872], "temperature": 0.0, "avg_logprob": -0.09216076594132644, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.009536770172417164}, {"id": 276, "seek": 154080, "start": 1550.96, "end": 1558.08, "text": " work and managed to figure out how that connects to the big theories of current 20th century physics.", "tokens": [50872, 589, 293, 6453, 281, 2573, 484, 577, 300, 16967, 281, 264, 955, 13667, 295, 2190, 945, 392, 4901, 10649, 13, 51228], "temperature": 0.0, "avg_logprob": -0.09216076594132644, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.009536770172417164}, {"id": 277, "seek": 154080, "start": 1558.08, "end": 1561.84, "text": " And actually, the really remarkable thing that maybe I'll have a chance to describe", "tokens": [51228, 400, 767, 11, 264, 534, 12802, 551, 300, 1310, 286, 603, 362, 257, 2931, 281, 6786, 51416], "temperature": 0.0, "avg_logprob": -0.09216076594132644, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.009536770172417164}, {"id": 278, "seek": 154080, "start": 1561.84, "end": 1566.48, "text": " is that the big theories of 20th century physics, essentially general relativity, the theory of", "tokens": [51416, 307, 300, 264, 955, 13667, 295, 945, 392, 4901, 10649, 11, 4476, 2674, 45675, 11, 264, 5261, 295, 51648], "temperature": 0.0, "avg_logprob": -0.09216076594132644, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.009536770172417164}, {"id": 279, "seek": 156648, "start": 1566.48, "end": 1571.6, "text": " gravity and spacetime, quantum mechanics, and statistical mechanics for the second law of", "tokens": [50364, 12110, 293, 39404, 9764, 11, 13018, 12939, 11, 293, 22820, 12939, 337, 264, 1150, 2101, 295, 50620], "temperature": 0.0, "avg_logprob": -0.10248651417023545, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.010641541332006454}, {"id": 280, "seek": 156648, "start": 1571.6, "end": 1577.3600000000001, "text": " thermodynamics, those are sort of three big theories of 20th century physics. It turns out that all", "tokens": [50620, 8810, 35483, 11, 729, 366, 1333, 295, 1045, 955, 13667, 295, 945, 392, 4901, 10649, 13, 467, 4523, 484, 300, 439, 50908], "temperature": 0.0, "avg_logprob": -0.10248651417023545, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.010641541332006454}, {"id": 281, "seek": 156648, "start": 1577.3600000000001, "end": 1583.04, "text": " three of those theories are not just things that we can kind of say, oh, that's what's true.", "tokens": [50908, 1045, 295, 729, 13667, 366, 406, 445, 721, 300, 321, 393, 733, 295, 584, 11, 1954, 11, 300, 311, 437, 311, 2074, 13, 51192], "temperature": 0.0, "avg_logprob": -0.10248651417023545, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.010641541332006454}, {"id": 282, "seek": 156648, "start": 1583.04, "end": 1588.8, "text": " There are actually things that we can in some sense derive from fundamental considerations.", "tokens": [51192, 821, 366, 767, 721, 300, 321, 393, 294, 512, 2020, 28446, 490, 8088, 24070, 13, 51480], "temperature": 0.0, "avg_logprob": -0.10248651417023545, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.010641541332006454}, {"id": 283, "seek": 156648, "start": 1588.8, "end": 1594.0, "text": " I had not expected any such thing to be the case that we could derive the laws of physics, so to", "tokens": [51480, 286, 632, 406, 5176, 604, 1270, 551, 281, 312, 264, 1389, 300, 321, 727, 28446, 264, 6064, 295, 10649, 11, 370, 281, 51740], "temperature": 0.0, "avg_logprob": -0.10248651417023545, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.010641541332006454}, {"id": 284, "seek": 159400, "start": 1594.0, "end": 1599.44, "text": " speak. But we can and I'll explain how that works. And that's kind of loop back to questions about", "tokens": [50364, 1710, 13, 583, 321, 393, 293, 286, 603, 2903, 577, 300, 1985, 13, 400, 300, 311, 733, 295, 6367, 646, 281, 1651, 466, 50636], "temperature": 0.0, "avg_logprob": -0.0955235570923895, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.007049816194921732}, {"id": 285, "seek": 159400, "start": 1599.44, "end": 1607.36, "text": " language and concepts and so on. But okay, so what's the universe made of? Well, in our models,", "tokens": [50636, 2856, 293, 10392, 293, 370, 322, 13, 583, 1392, 11, 370, 437, 311, 264, 6445, 1027, 295, 30, 1042, 11, 294, 527, 5245, 11, 51032], "temperature": 0.0, "avg_logprob": -0.0955235570923895, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.007049816194921732}, {"id": 286, "seek": 159400, "start": 1607.36, "end": 1612.56, "text": " the universe consists of a bunch of sort of discrete atoms of space, we tend to call them", "tokens": [51032, 264, 6445, 14689, 295, 257, 3840, 295, 1333, 295, 27706, 16871, 295, 1901, 11, 321, 3928, 281, 818, 552, 51292], "temperature": 0.0, "avg_logprob": -0.0955235570923895, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.007049816194921732}, {"id": 287, "seek": 159400, "start": 1612.56, "end": 1618.32, "text": " eames, kind of atoms of existence. They're things where the only thing you can say about them is", "tokens": [51292, 308, 1632, 11, 733, 295, 16871, 295, 9123, 13, 814, 434, 721, 689, 264, 787, 551, 291, 393, 584, 466, 552, 307, 51580], "temperature": 0.0, "avg_logprob": -0.0955235570923895, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.007049816194921732}, {"id": 288, "seek": 159400, "start": 1618.32, "end": 1622.0, "text": " they exist, and they have an identity, and they're distinct from each other.", "tokens": [51580, 436, 2514, 11, 293, 436, 362, 364, 6575, 11, 293, 436, 434, 10644, 490, 1184, 661, 13, 51764], "temperature": 0.0, "avg_logprob": -0.0955235570923895, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.007049816194921732}, {"id": 289, "seek": 162200, "start": 1622.96, "end": 1628.16, "text": " And then there's one more thing, which is you can say how these eames, how these atoms of space", "tokens": [50412, 400, 550, 456, 311, 472, 544, 551, 11, 597, 307, 291, 393, 584, 577, 613, 308, 1632, 11, 577, 613, 16871, 295, 1901, 50672], "temperature": 0.0, "avg_logprob": -0.0894173342606117, "compression_ratio": 1.8068181818181819, "no_speech_prob": 0.00092318159295246}, {"id": 290, "seek": 162200, "start": 1628.16, "end": 1632.8, "text": " are related to each other. You can say this one is related to these two other ones. It's kind of", "tokens": [50672, 366, 4077, 281, 1184, 661, 13, 509, 393, 584, 341, 472, 307, 4077, 281, 613, 732, 661, 2306, 13, 467, 311, 733, 295, 50904], "temperature": 0.0, "avg_logprob": -0.0894173342606117, "compression_ratio": 1.8068181818181819, "no_speech_prob": 0.00092318159295246}, {"id": 291, "seek": 162200, "start": 1632.8, "end": 1638.08, "text": " like what atom of space is friends with what other atoms of space? And you define this whole", "tokens": [50904, 411, 437, 12018, 295, 1901, 307, 1855, 365, 437, 661, 16871, 295, 1901, 30, 400, 291, 6964, 341, 1379, 51168], "temperature": 0.0, "avg_logprob": -0.0894173342606117, "compression_ratio": 1.8068181818181819, "no_speech_prob": 0.00092318159295246}, {"id": 292, "seek": 162200, "start": 1638.08, "end": 1642.8, "text": " collection of relations between atoms of space, and you can represent that by a graph, a network,", "tokens": [51168, 5765, 295, 2299, 1296, 16871, 295, 1901, 11, 293, 291, 393, 2906, 300, 538, 257, 4295, 11, 257, 3209, 11, 51404], "temperature": 0.0, "avg_logprob": -0.0894173342606117, "compression_ratio": 1.8068181818181819, "no_speech_prob": 0.00092318159295246}, {"id": 293, "seek": 162200, "start": 1642.8, "end": 1649.2, "text": " or actually more formally in our models, a hypergraph. But the essentially one's just dealing", "tokens": [51404, 420, 767, 544, 25983, 294, 527, 5245, 11, 257, 9848, 34091, 13, 583, 264, 4476, 472, 311, 445, 6260, 51724], "temperature": 0.0, "avg_logprob": -0.0894173342606117, "compression_ratio": 1.8068181818181819, "no_speech_prob": 0.00092318159295246}, {"id": 294, "seek": 164920, "start": 1649.2, "end": 1655.44, "text": " with this big network of relations between the atoms of space. And so everything in the universe", "tokens": [50364, 365, 341, 955, 3209, 295, 2299, 1296, 264, 16871, 295, 1901, 13, 400, 370, 1203, 294, 264, 6445, 50676], "temperature": 0.0, "avg_logprob": -0.1045592181822833, "compression_ratio": 1.688622754491018, "no_speech_prob": 0.007769513875246048}, {"id": 295, "seek": 164920, "start": 1655.44, "end": 1660.64, "text": " in our models is just made of the relations between atoms of space. So for example, if", "tokens": [50676, 294, 527, 5245, 307, 445, 1027, 295, 264, 2299, 1296, 16871, 295, 1901, 13, 407, 337, 1365, 11, 498, 50936], "temperature": 0.0, "avg_logprob": -0.1045592181822833, "compression_ratio": 1.688622754491018, "no_speech_prob": 0.007769513875246048}, {"id": 296, "seek": 164920, "start": 1661.3600000000001, "end": 1670.16, "text": " something like a black hole, for example, is just a structure in the, I might even be able to show", "tokens": [50972, 746, 411, 257, 2211, 5458, 11, 337, 1365, 11, 307, 445, 257, 3877, 294, 264, 11, 286, 1062, 754, 312, 1075, 281, 855, 51412], "temperature": 0.0, "avg_logprob": -0.1045592181822833, "compression_ratio": 1.688622754491018, "no_speech_prob": 0.007769513875246048}, {"id": 297, "seek": 167016, "start": 1670.16, "end": 1687.1200000000001, "text": " you a picture of one, let me see if I can pull this up. This is actually in kind of the fabric", "tokens": [50364, 291, 257, 3036, 295, 472, 11, 718, 385, 536, 498, 286, 393, 2235, 341, 493, 13, 639, 307, 767, 294, 733, 295, 264, 7253, 51212], "temperature": 0.0, "avg_logprob": -0.11834367987227766, "compression_ratio": 1.6235955056179776, "no_speech_prob": 0.010058696381747723}, {"id": 298, "seek": 167016, "start": 1687.1200000000001, "end": 1694.0, "text": " of space. This is two little tiny black holes. And we'll see in this video kind of space, most of", "tokens": [51212, 295, 1901, 13, 639, 307, 732, 707, 5870, 2211, 8118, 13, 400, 321, 603, 536, 294, 341, 960, 733, 295, 1901, 11, 881, 295, 51556], "temperature": 0.0, "avg_logprob": -0.11834367987227766, "compression_ratio": 1.6235955056179776, "no_speech_prob": 0.010058696381747723}, {"id": 299, "seek": 167016, "start": 1694.0, "end": 1698.0800000000002, "text": " the activity of the universe actually is knitting together the structure of space. But there are", "tokens": [51556, 264, 5191, 295, 264, 6445, 767, 307, 25498, 1214, 264, 3877, 295, 1901, 13, 583, 456, 366, 51760], "temperature": 0.0, "avg_logprob": -0.11834367987227766, "compression_ratio": 1.6235955056179776, "no_speech_prob": 0.010058696381747723}, {"id": 300, "seek": 169808, "start": 1698.08, "end": 1702.3999999999999, "text": " two black holes there, and you can kind of see they eventually merge. They produce gravitational", "tokens": [50364, 732, 2211, 8118, 456, 11, 293, 291, 393, 733, 295, 536, 436, 4728, 22183, 13, 814, 5258, 28538, 50580], "temperature": 0.0, "avg_logprob": -0.09136952486905185, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.005686797201633453}, {"id": 301, "seek": 169808, "start": 1702.3999999999999, "end": 1708.56, "text": " radiation. Actually, what we get from this model, where we're looking at kind of the", "tokens": [50580, 12420, 13, 5135, 11, 437, 321, 483, 490, 341, 2316, 11, 689, 321, 434, 1237, 412, 733, 295, 264, 50888], "temperature": 0.0, "avg_logprob": -0.09136952486905185, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.005686797201633453}, {"id": 302, "seek": 169808, "start": 1708.56, "end": 1713.6, "text": " discrete structure of space, we can successfully reproduce the actual things that are observed", "tokens": [50888, 27706, 3877, 295, 1901, 11, 321, 393, 10727, 29501, 264, 3539, 721, 300, 366, 13095, 51140], "temperature": 0.0, "avg_logprob": -0.09136952486905185, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.005686797201633453}, {"id": 303, "seek": 169808, "start": 1713.6, "end": 1720.0, "text": " in black hole mergers and so on. But in any case, the basic point is what the universe is made of,", "tokens": [51140, 294, 2211, 5458, 3551, 9458, 293, 370, 322, 13, 583, 294, 604, 1389, 11, 264, 3875, 935, 307, 437, 264, 6445, 307, 1027, 295, 11, 51460], "temperature": 0.0, "avg_logprob": -0.09136952486905185, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.005686797201633453}, {"id": 304, "seek": 169808, "start": 1720.0, "end": 1725.52, "text": " everything in the universe is just a feature of the structure of space. And when it comes to time,", "tokens": [51460, 1203, 294, 264, 6445, 307, 445, 257, 4111, 295, 264, 3877, 295, 1901, 13, 400, 562, 309, 1487, 281, 565, 11, 51736], "temperature": 0.0, "avg_logprob": -0.09136952486905185, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.005686797201633453}, {"id": 305, "seek": 172552, "start": 1726.16, "end": 1732.4, "text": " time is the progressive rewriting of the structure of that network that represents space.", "tokens": [50396, 565, 307, 264, 16131, 319, 19868, 295, 264, 3877, 295, 300, 3209, 300, 8855, 1901, 13, 50708], "temperature": 0.0, "avg_logprob": -0.05620448566177516, "compression_ratio": 1.834008097165992, "no_speech_prob": 0.004400185775011778}, {"id": 306, "seek": 172552, "start": 1732.4, "end": 1736.24, "text": " So time is actually a very different kind of thing in these models from space.", "tokens": [50708, 407, 565, 307, 767, 257, 588, 819, 733, 295, 551, 294, 613, 5245, 490, 1901, 13, 50900], "temperature": 0.0, "avg_logprob": -0.05620448566177516, "compression_ratio": 1.834008097165992, "no_speech_prob": 0.004400185775011778}, {"id": 307, "seek": 172552, "start": 1736.24, "end": 1741.92, "text": " Things like relativity emerge as a feature of the model. They're not things that are put in", "tokens": [50900, 9514, 411, 45675, 21511, 382, 257, 4111, 295, 264, 2316, 13, 814, 434, 406, 721, 300, 366, 829, 294, 51184], "temperature": 0.0, "avg_logprob": -0.05620448566177516, "compression_ratio": 1.834008097165992, "no_speech_prob": 0.004400185775011778}, {"id": 308, "seek": 172552, "start": 1741.92, "end": 1747.68, "text": " from the underlying structure of the model. Okay, so we've got sort of the notion of space,", "tokens": [51184, 490, 264, 14217, 3877, 295, 264, 2316, 13, 1033, 11, 370, 321, 600, 658, 1333, 295, 264, 10710, 295, 1901, 11, 51472], "temperature": 0.0, "avg_logprob": -0.05620448566177516, "compression_ratio": 1.834008097165992, "no_speech_prob": 0.004400185775011778}, {"id": 309, "seek": 172552, "start": 1747.68, "end": 1755.2, "text": " notion of time. It turns out quantum mechanics is a thing that inevitably emerges from the fact that", "tokens": [51472, 10710, 295, 565, 13, 467, 4523, 484, 13018, 12939, 307, 257, 551, 300, 28171, 38965, 490, 264, 1186, 300, 51848], "temperature": 0.0, "avg_logprob": -0.05620448566177516, "compression_ratio": 1.834008097165992, "no_speech_prob": 0.004400185775011778}, {"id": 310, "seek": 175520, "start": 1755.28, "end": 1759.68, "text": " when we are updating this network, there isn't just one possible path of history. There isn't", "tokens": [50368, 562, 321, 366, 25113, 341, 3209, 11, 456, 1943, 380, 445, 472, 1944, 3100, 295, 2503, 13, 821, 1943, 380, 50588], "temperature": 0.0, "avg_logprob": -0.06806519654420046, "compression_ratio": 1.8425196850393701, "no_speech_prob": 0.0008615091792307794}, {"id": 311, "seek": 175520, "start": 1759.68, "end": 1764.8, "text": " just one possible way that the network can be updated. There are many possible paths of history", "tokens": [50588, 445, 472, 1944, 636, 300, 264, 3209, 393, 312, 10588, 13, 821, 366, 867, 1944, 14518, 295, 2503, 50844], "temperature": 0.0, "avg_logprob": -0.06806519654420046, "compression_ratio": 1.8425196850393701, "no_speech_prob": 0.0008615091792307794}, {"id": 312, "seek": 175520, "start": 1764.8, "end": 1770.0, "text": " that branch and merge. And essentially, the structure of those things is what leads to", "tokens": [50844, 300, 9819, 293, 22183, 13, 400, 4476, 11, 264, 3877, 295, 729, 721, 307, 437, 6689, 281, 51104], "temperature": 0.0, "avg_logprob": -0.06806519654420046, "compression_ratio": 1.8425196850393701, "no_speech_prob": 0.0008615091792307794}, {"id": 313, "seek": 175520, "start": 1770.0, "end": 1775.92, "text": " quantum mechanics. Well, one of the issues is when we're looking at the system, and we're", "tokens": [51104, 13018, 12939, 13, 1042, 11, 472, 295, 264, 2663, 307, 562, 321, 434, 1237, 412, 264, 1185, 11, 293, 321, 434, 51400], "temperature": 0.0, "avg_logprob": -0.06806519654420046, "compression_ratio": 1.8425196850393701, "no_speech_prob": 0.0008615091792307794}, {"id": 314, "seek": 175520, "start": 1775.92, "end": 1781.92, "text": " seeing all these rewrites and the structure of space and so on, the question is, how do we experience", "tokens": [51400, 2577, 439, 613, 319, 86, 30931, 293, 264, 3877, 295, 1901, 293, 370, 322, 11, 264, 1168, 307, 11, 577, 360, 321, 1752, 51700], "temperature": 0.0, "avg_logprob": -0.06806519654420046, "compression_ratio": 1.8425196850393701, "no_speech_prob": 0.0008615091792307794}, {"id": 315, "seek": 178192, "start": 1781.92, "end": 1788.48, "text": " that? There are all these things microscopically happening, but we have a certain experience of", "tokens": [50364, 300, 30, 821, 366, 439, 613, 721, 30483, 984, 2737, 11, 457, 321, 362, 257, 1629, 1752, 295, 50692], "temperature": 0.0, "avg_logprob": -0.06521862679785424, "compression_ratio": 1.6306306306306306, "no_speech_prob": 0.0016717342659831047}, {"id": 316, "seek": 178192, "start": 1788.48, "end": 1794.24, "text": " that. And it turns out that sort of a critical feature of what's going on is that we are observers", "tokens": [50692, 300, 13, 400, 309, 4523, 484, 300, 1333, 295, 257, 4924, 4111, 295, 437, 311, 516, 322, 307, 300, 321, 366, 48090, 50980], "temperature": 0.0, "avg_logprob": -0.06521862679785424, "compression_ratio": 1.6306306306306306, "no_speech_prob": 0.0016717342659831047}, {"id": 317, "seek": 178192, "start": 1794.24, "end": 1802.64, "text": " of a certain kind. So let's take the case of, let's look at, for example, let's see,", "tokens": [50980, 295, 257, 1629, 733, 13, 407, 718, 311, 747, 264, 1389, 295, 11, 718, 311, 574, 412, 11, 337, 1365, 11, 718, 311, 536, 11, 51400], "temperature": 0.0, "avg_logprob": -0.06521862679785424, "compression_ratio": 1.6306306306306306, "no_speech_prob": 0.0016717342659831047}, {"id": 318, "seek": 178192, "start": 1806.88, "end": 1810.64, "text": " let's look at something like statistical mechanics. We've got a bunch of molecules", "tokens": [51612, 718, 311, 574, 412, 746, 411, 22820, 12939, 13, 492, 600, 658, 257, 3840, 295, 13093, 51800], "temperature": 0.0, "avg_logprob": -0.06521862679785424, "compression_ratio": 1.6306306306306306, "no_speech_prob": 0.0016717342659831047}, {"id": 319, "seek": 181064, "start": 1810.64, "end": 1815.92, "text": " bouncing around in a box. And one of the kind of big principles is the second law of thermodynamics", "tokens": [50364, 27380, 926, 294, 257, 2424, 13, 400, 472, 295, 264, 733, 295, 955, 9156, 307, 264, 1150, 2101, 295, 8810, 35483, 50628], "temperature": 0.0, "avg_logprob": -0.08092226180355105, "compression_ratio": 1.6819787985865724, "no_speech_prob": 0.003976143430918455}, {"id": 320, "seek": 181064, "start": 1815.92, "end": 1820.96, "text": " that says when you start those molecules off in an orderly way, their motion will tend to", "tokens": [50628, 300, 1619, 562, 291, 722, 729, 13093, 766, 294, 364, 1668, 356, 636, 11, 641, 5394, 486, 3928, 281, 50880], "temperature": 0.0, "avg_logprob": -0.08092226180355105, "compression_ratio": 1.6819787985865724, "no_speech_prob": 0.003976143430918455}, {"id": 321, "seek": 181064, "start": 1820.96, "end": 1826.24, "text": " eventually look disordered and random. It will look as if it has higher entropy. And the question", "tokens": [50880, 4728, 574, 717, 765, 4073, 293, 4974, 13, 467, 486, 574, 382, 498, 309, 575, 2946, 30867, 13, 400, 264, 1168, 51144], "temperature": 0.0, "avg_logprob": -0.08092226180355105, "compression_ratio": 1.6819787985865724, "no_speech_prob": 0.003976143430918455}, {"id": 322, "seek": 181064, "start": 1826.24, "end": 1833.0400000000002, "text": " is sort of what's really going on there? And it turns out that what's actually happening,", "tokens": [51144, 307, 1333, 295, 437, 311, 534, 516, 322, 456, 30, 400, 309, 4523, 484, 300, 437, 311, 767, 2737, 11, 51484], "temperature": 0.0, "avg_logprob": -0.08092226180355105, "compression_ratio": 1.6819787985865724, "no_speech_prob": 0.003976143430918455}, {"id": 323, "seek": 181064, "start": 1833.0400000000002, "end": 1838.96, "text": " something I finally understood, I've been thinking about this for like 50 years, actually, is that", "tokens": [51484, 746, 286, 2721, 7320, 11, 286, 600, 668, 1953, 466, 341, 337, 411, 2625, 924, 11, 767, 11, 307, 300, 51780], "temperature": 0.0, "avg_logprob": -0.08092226180355105, "compression_ratio": 1.6819787985865724, "no_speech_prob": 0.003976143430918455}, {"id": 324, "seek": 183896, "start": 1839.76, "end": 1844.16, "text": " what's ultimately going on, you can look at different kinds of versions of this, what's", "tokens": [50404, 437, 311, 6284, 516, 322, 11, 291, 393, 574, 412, 819, 3685, 295, 9606, 295, 341, 11, 437, 311, 50624], "temperature": 0.0, "avg_logprob": -0.13987574370011038, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.0014643030008301139}, {"id": 325, "seek": 183896, "start": 1844.16, "end": 1850.16, "text": " ultimately going on is that these molecules are bouncing around in a certain determined way", "tokens": [50624, 6284, 516, 322, 307, 300, 613, 13093, 366, 27380, 926, 294, 257, 1629, 9540, 636, 50924], "temperature": 0.0, "avg_logprob": -0.13987574370011038, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.0014643030008301139}, {"id": 326, "seek": 183896, "start": 1850.16, "end": 1855.04, "text": " according to some rule. And in fact, that rule can be reversed. So you can take this pattern of", "tokens": [50924, 4650, 281, 512, 4978, 13, 400, 294, 1186, 11, 300, 4978, 393, 312, 30563, 13, 407, 291, 393, 747, 341, 5102, 295, 51168], "temperature": 0.0, "avg_logprob": -0.13987574370011038, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.0014643030008301139}, {"id": 327, "seek": 183896, "start": 1855.04, "end": 1859.28, "text": " molecules you get at the end and you can say, I can figure out, oh, yes, that pattern of molecules", "tokens": [51168, 13093, 291, 483, 412, 264, 917, 293, 291, 393, 584, 11, 286, 393, 2573, 484, 11, 1954, 11, 2086, 11, 300, 5102, 295, 13093, 51380], "temperature": 0.0, "avg_logprob": -0.13987574370011038, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.0014643030008301139}, {"id": 328, "seek": 183896, "start": 1859.28, "end": 1865.1200000000001, "text": " came from the simple initial state. Well, in principle, you can do that, but it's a computationally", "tokens": [51380, 1361, 490, 264, 2199, 5883, 1785, 13, 1042, 11, 294, 8665, 11, 291, 393, 360, 300, 11, 457, 309, 311, 257, 24903, 379, 51672], "temperature": 0.0, "avg_logprob": -0.13987574370011038, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.0014643030008301139}, {"id": 329, "seek": 186512, "start": 1865.6799999999998, "end": 1870.9599999999998, "text": " irreducible process. And the difficulty is that we human observers of things", "tokens": [50392, 16014, 769, 32128, 1399, 13, 400, 264, 10360, 307, 300, 321, 1952, 48090, 295, 721, 50656], "temperature": 0.0, "avg_logprob": -0.08076090472085136, "compression_ratio": 2.017699115044248, "no_speech_prob": 0.035273388028144836}, {"id": 330, "seek": 186512, "start": 1871.6799999999998, "end": 1878.08, "text": " computationally bounded, we can't do that, that, that all the computation that's needed to reverse", "tokens": [50692, 24903, 379, 37498, 11, 321, 393, 380, 360, 300, 11, 300, 11, 300, 439, 264, 24903, 300, 311, 2978, 281, 9943, 51012], "temperature": 0.0, "avg_logprob": -0.08076090472085136, "compression_ratio": 2.017699115044248, "no_speech_prob": 0.035273388028144836}, {"id": 331, "seek": 186512, "start": 1878.08, "end": 1883.6, "text": " what happens in the molecules, we're just stuck saying that we can, we can get this", "tokens": [51012, 437, 2314, 294, 264, 13093, 11, 321, 434, 445, 5541, 1566, 300, 321, 393, 11, 321, 393, 483, 341, 51288], "temperature": 0.0, "avg_logprob": -0.08076090472085136, "compression_ratio": 2.017699115044248, "no_speech_prob": 0.035273388028144836}, {"id": 332, "seek": 186512, "start": 1883.6, "end": 1888.0, "text": " impression of what's going on. And with that impression of what's going on with that computationally", "tokens": [51288, 9995, 295, 437, 311, 516, 322, 13, 400, 365, 300, 9995, 295, 437, 311, 516, 322, 365, 300, 24903, 379, 51508], "temperature": 0.0, "avg_logprob": -0.08076090472085136, "compression_ratio": 2.017699115044248, "no_speech_prob": 0.035273388028144836}, {"id": 333, "seek": 186512, "start": 1888.0, "end": 1893.28, "text": " bounded impression of what's going on, all we can say is, oh, it looks random to us. And that's", "tokens": [51508, 37498, 9995, 295, 437, 311, 516, 322, 11, 439, 321, 393, 584, 307, 11, 1954, 11, 309, 1542, 4974, 281, 505, 13, 400, 300, 311, 51772], "temperature": 0.0, "avg_logprob": -0.08076090472085136, "compression_ratio": 2.017699115044248, "no_speech_prob": 0.035273388028144836}, {"id": 334, "seek": 189328, "start": 1893.28, "end": 1897.68, "text": " kind of the ultimate origin of the second law of thermodynamics is something which is to do with", "tokens": [50364, 733, 295, 264, 9705, 4957, 295, 264, 1150, 2101, 295, 8810, 35483, 307, 746, 597, 307, 281, 360, 365, 50584], "temperature": 0.0, "avg_logprob": -0.0839280011702557, "compression_ratio": 1.8046875, "no_speech_prob": 0.003846576903015375}, {"id": 335, "seek": 189328, "start": 1897.68, "end": 1903.44, "text": " the relationship between underlying computational irreducibility and our computational boundedness", "tokens": [50584, 264, 2480, 1296, 14217, 28270, 16014, 769, 537, 39802, 293, 527, 28270, 37498, 1287, 50872], "temperature": 0.0, "avg_logprob": -0.0839280011702557, "compression_ratio": 1.8046875, "no_speech_prob": 0.003846576903015375}, {"id": 336, "seek": 189328, "start": 1903.44, "end": 1910.8799999999999, "text": " as observers. Well, it turns out that both general relativity and quantum mechanics", "tokens": [50872, 382, 48090, 13, 1042, 11, 309, 4523, 484, 300, 1293, 2674, 45675, 293, 13018, 12939, 51244], "temperature": 0.0, "avg_logprob": -0.0839280011702557, "compression_ratio": 1.8046875, "no_speech_prob": 0.003846576903015375}, {"id": 337, "seek": 189328, "start": 1910.8799999999999, "end": 1917.44, "text": " come from the exact same thing. They both come from this idea that there is computational", "tokens": [51244, 808, 490, 264, 1900, 912, 551, 13, 814, 1293, 808, 490, 341, 1558, 300, 456, 307, 28270, 51572], "temperature": 0.0, "avg_logprob": -0.0839280011702557, "compression_ratio": 1.8046875, "no_speech_prob": 0.003846576903015375}, {"id": 338, "seek": 189328, "start": 1917.44, "end": 1922.32, "text": " irreducibility underneath. But we are, well, actually, there are two attributes that we have", "tokens": [51572, 16014, 769, 537, 39802, 7223, 13, 583, 321, 366, 11, 731, 11, 767, 11, 456, 366, 732, 17212, 300, 321, 362, 51816], "temperature": 0.0, "avg_logprob": -0.0839280011702557, "compression_ratio": 1.8046875, "no_speech_prob": 0.003846576903015375}, {"id": 339, "seek": 192232, "start": 1922.32, "end": 1927.6799999999998, "text": " to have as observers, that we are computationally bounded, and that we believe we are persistent", "tokens": [50364, 281, 362, 382, 48090, 11, 300, 321, 366, 24903, 379, 37498, 11, 293, 300, 321, 1697, 321, 366, 24315, 50632], "temperature": 0.0, "avg_logprob": -0.09840646703192528, "compression_ratio": 1.7953488372093023, "no_speech_prob": 0.003859310643747449}, {"id": 340, "seek": 192232, "start": 1927.6799999999998, "end": 1933.28, "text": " in time. So in this model, for example, we are at every moment in time, we're made of different", "tokens": [50632, 294, 565, 13, 407, 294, 341, 2316, 11, 337, 1365, 11, 321, 366, 412, 633, 1623, 294, 565, 11, 321, 434, 1027, 295, 819, 50912], "temperature": 0.0, "avg_logprob": -0.09840646703192528, "compression_ratio": 1.7953488372093023, "no_speech_prob": 0.003859310643747449}, {"id": 341, "seek": 192232, "start": 1933.28, "end": 1940.24, "text": " atoms of space, yet we all have the impression that we are experiencing things through that it's", "tokens": [50912, 16871, 295, 1901, 11, 1939, 321, 439, 362, 264, 9995, 300, 321, 366, 11139, 721, 807, 300, 309, 311, 51260], "temperature": 0.0, "avg_logprob": -0.09840646703192528, "compression_ratio": 1.7953488372093023, "no_speech_prob": 0.003859310643747449}, {"id": 342, "seek": 192232, "start": 1940.24, "end": 1946.1599999999999, "text": " still us a second later, so to speak, and that we experience things we are persistent, we have a", "tokens": [51260, 920, 505, 257, 1150, 1780, 11, 370, 281, 1710, 11, 293, 300, 321, 1752, 721, 321, 366, 24315, 11, 321, 362, 257, 51556], "temperature": 0.0, "avg_logprob": -0.09840646703192528, "compression_ratio": 1.7953488372093023, "no_speech_prob": 0.003859310643747449}, {"id": 343, "seek": 194616, "start": 1946.16, "end": 1952.5600000000002, "text": " continuous thread of experience through time. Well, okay, so the really ultimately big concept", "tokens": [50364, 10957, 7207, 295, 1752, 807, 565, 13, 1042, 11, 1392, 11, 370, 264, 534, 6284, 955, 3410, 50684], "temperature": 0.0, "avg_logprob": -0.14187761371055346, "compression_ratio": 1.6952380952380952, "no_speech_prob": 0.010813099332153797}, {"id": 344, "seek": 194616, "start": 1952.5600000000002, "end": 1959.2, "text": " here is this thing we call the Ruliad. And so here's how this works. When we look at", "tokens": [50684, 510, 307, 341, 551, 321, 818, 264, 497, 425, 38069, 13, 400, 370, 510, 311, 577, 341, 1985, 13, 1133, 321, 574, 412, 51016], "temperature": 0.0, "avg_logprob": -0.14187761371055346, "compression_ratio": 1.6952380952380952, "no_speech_prob": 0.010813099332153797}, {"id": 345, "seek": 194616, "start": 1960.8000000000002, "end": 1964.96, "text": " these, this underlying hypergraph and its rewrite rules and all those kinds of things,", "tokens": [51096, 613, 11, 341, 14217, 9848, 34091, 293, 1080, 28132, 4474, 293, 439, 729, 3685, 295, 721, 11, 51304], "temperature": 0.0, "avg_logprob": -0.14187761371055346, "compression_ratio": 1.6952380952380952, "no_speech_prob": 0.010813099332153797}, {"id": 346, "seek": 194616, "start": 1965.76, "end": 1973.76, "text": " we can, we say, okay, there are these underlying rules. And if we run those enough times,", "tokens": [51344, 321, 393, 11, 321, 584, 11, 1392, 11, 456, 366, 613, 14217, 4474, 13, 400, 498, 321, 1190, 729, 1547, 1413, 11, 51744], "temperature": 0.0, "avg_logprob": -0.14187761371055346, "compression_ratio": 1.6952380952380952, "no_speech_prob": 0.010813099332153797}, {"id": 347, "seek": 197376, "start": 1973.76, "end": 1978.56, "text": " we'll eventually get something that seems like our universe that satisfies Einstein's equations", "tokens": [50364, 321, 603, 4728, 483, 746, 300, 2544, 411, 527, 6445, 300, 44271, 23486, 311, 11787, 50604], "temperature": 0.0, "avg_logprob": -0.08846705814577499, "compression_ratio": 1.654109589041096, "no_speech_prob": 0.01567896269261837}, {"id": 348, "seek": 197376, "start": 1978.56, "end": 1983.04, "text": " of general relativity, that shows the Feynman path integral for quantum mechanics, all those kinds of", "tokens": [50604, 295, 2674, 45675, 11, 300, 3110, 264, 46530, 77, 1601, 3100, 11573, 337, 13018, 12939, 11, 439, 729, 3685, 295, 50828], "temperature": 0.0, "avg_logprob": -0.08846705814577499, "compression_ratio": 1.654109589041096, "no_speech_prob": 0.01567896269261837}, {"id": 349, "seek": 197376, "start": 1983.04, "end": 1989.76, "text": " good things. But we still might be asking the question, well, why did our universe get one", "tokens": [50828, 665, 721, 13, 583, 321, 920, 1062, 312, 3365, 264, 1168, 11, 731, 11, 983, 630, 527, 6445, 483, 472, 51164], "temperature": 0.0, "avg_logprob": -0.08846705814577499, "compression_ratio": 1.654109589041096, "no_speech_prob": 0.01567896269261837}, {"id": 350, "seek": 197376, "start": 1989.76, "end": 1996.48, "text": " particular rule and not another? And that had me very confused for quite a while, until I realized", "tokens": [51164, 1729, 4978, 293, 406, 1071, 30, 400, 300, 632, 385, 588, 9019, 337, 1596, 257, 1339, 11, 1826, 286, 5334, 51500], "temperature": 0.0, "avg_logprob": -0.08846705814577499, "compression_ratio": 1.654109589041096, "no_speech_prob": 0.01567896269261837}, {"id": 351, "seek": 197376, "start": 1996.48, "end": 2003.44, "text": " that actually we can think of the universe as running all possible rules. So what we imagine is", "tokens": [51500, 300, 767, 321, 393, 519, 295, 264, 6445, 382, 2614, 439, 1944, 4474, 13, 407, 437, 321, 3811, 307, 51848], "temperature": 0.0, "avg_logprob": -0.08846705814577499, "compression_ratio": 1.654109589041096, "no_speech_prob": 0.01567896269261837}, {"id": 352, "seek": 200344, "start": 2003.44, "end": 2008.0800000000002, "text": " that there are these possible computational rules that can be used to update this hypergraph and so", "tokens": [50364, 300, 456, 366, 613, 1944, 28270, 4474, 300, 393, 312, 1143, 281, 5623, 341, 9848, 34091, 293, 370, 50596], "temperature": 0.0, "avg_logprob": -0.08155754156279982, "compression_ratio": 1.8377358490566038, "no_speech_prob": 0.0008664001943543553}, {"id": 353, "seek": 200344, "start": 2008.0800000000002, "end": 2013.92, "text": " on. But let's just imagine that we use all possible rules. What we get are all these different parts", "tokens": [50596, 322, 13, 583, 718, 311, 445, 3811, 300, 321, 764, 439, 1944, 4474, 13, 708, 321, 483, 366, 439, 613, 819, 3166, 50888], "temperature": 0.0, "avg_logprob": -0.08155754156279982, "compression_ratio": 1.8377358490566038, "no_speech_prob": 0.0008664001943543553}, {"id": 354, "seek": 200344, "start": 2013.92, "end": 2018.8, "text": " of history that branched and merge and so on, corresponding to the application of all these", "tokens": [50888, 295, 2503, 300, 9819, 292, 293, 22183, 293, 370, 322, 11, 11760, 281, 264, 3861, 295, 439, 613, 51132], "temperature": 0.0, "avg_logprob": -0.08155754156279982, "compression_ratio": 1.8377358490566038, "no_speech_prob": 0.0008664001943543553}, {"id": 355, "seek": 200344, "start": 2018.8, "end": 2025.1200000000001, "text": " different rules. And this whole object that is the entangled limit of all possible computational", "tokens": [51132, 819, 4474, 13, 400, 341, 1379, 2657, 300, 307, 264, 948, 39101, 4948, 295, 439, 1944, 28270, 51448], "temperature": 0.0, "avg_logprob": -0.08155754156279982, "compression_ratio": 1.8377358490566038, "no_speech_prob": 0.0008664001943543553}, {"id": 356, "seek": 200344, "start": 2025.1200000000001, "end": 2032.0, "text": " processes, we call the Ruliad. And the Ruliad is a completely unique thing. It is, it is you take", "tokens": [51448, 7555, 11, 321, 818, 264, 497, 425, 38069, 13, 400, 264, 497, 425, 38069, 307, 257, 2584, 3845, 551, 13, 467, 307, 11, 309, 307, 291, 747, 51792], "temperature": 0.0, "avg_logprob": -0.08155754156279982, "compression_ratio": 1.8377358490566038, "no_speech_prob": 0.0008664001943543553}, {"id": 357, "seek": 203200, "start": 2032.0, "end": 2037.36, "text": " every possible Turing machine, every possible computational system, you run all of them, and", "tokens": [50364, 633, 1944, 314, 1345, 3479, 11, 633, 1944, 28270, 1185, 11, 291, 1190, 439, 295, 552, 11, 293, 50632], "temperature": 0.0, "avg_logprob": -0.08581520759896057, "compression_ratio": 1.922077922077922, "no_speech_prob": 0.00983534287661314}, {"id": 358, "seek": 203200, "start": 2037.36, "end": 2042.48, "text": " you run them in such a way that they are producing kind of, that they don't just have one possible", "tokens": [50632, 291, 1190, 552, 294, 1270, 257, 636, 300, 436, 366, 10501, 733, 295, 11, 300, 436, 500, 380, 445, 362, 472, 1944, 50888], "temperature": 0.0, "avg_logprob": -0.08581520759896057, "compression_ratio": 1.922077922077922, "no_speech_prob": 0.00983534287661314}, {"id": 359, "seek": 203200, "start": 2042.48, "end": 2047.2, "text": " outcome, they have all possible outcomes. You might say, what an incredible mess, how could you ever", "tokens": [50888, 9700, 11, 436, 362, 439, 1944, 10070, 13, 509, 1062, 584, 11, 437, 364, 4651, 2082, 11, 577, 727, 291, 1562, 51124], "temperature": 0.0, "avg_logprob": -0.08581520759896057, "compression_ratio": 1.922077922077922, "no_speech_prob": 0.00983534287661314}, {"id": 360, "seek": 203200, "start": 2047.2, "end": 2051.92, "text": " conclude anything from this Ruliad object? It is the case that this Ruliad object is a unique thing,", "tokens": [51124, 16886, 1340, 490, 341, 497, 425, 38069, 2657, 30, 467, 307, 264, 1389, 300, 341, 497, 425, 38069, 2657, 307, 257, 3845, 551, 11, 51360], "temperature": 0.0, "avg_logprob": -0.08581520759896057, "compression_ratio": 1.922077922077922, "no_speech_prob": 0.00983534287661314}, {"id": 361, "seek": 203200, "start": 2051.92, "end": 2056.88, "text": " there's not, it's not like there's seven different Ruliad's, there's just this thing that is the", "tokens": [51360, 456, 311, 406, 11, 309, 311, 406, 411, 456, 311, 3407, 819, 497, 425, 38069, 311, 11, 456, 311, 445, 341, 551, 300, 307, 264, 51608], "temperature": 0.0, "avg_logprob": -0.08581520759896057, "compression_ratio": 1.922077922077922, "no_speech_prob": 0.00983534287661314}, {"id": 362, "seek": 203200, "start": 2056.88, "end": 2061.44, "text": " entangled limit of all possible computations. And so then the question is, well, how can you conclude", "tokens": [51608, 948, 39101, 4948, 295, 439, 1944, 2807, 763, 13, 400, 370, 550, 264, 1168, 307, 11, 731, 11, 577, 393, 291, 16886, 51836], "temperature": 0.0, "avg_logprob": -0.08581520759896057, "compression_ratio": 1.922077922077922, "no_speech_prob": 0.00983534287661314}, {"id": 363, "seek": 206144, "start": 2061.44, "end": 2066.56, "text": " anything about, about this Ruliad object? Well, what you have to realize is the Ruliad object", "tokens": [50364, 1340, 466, 11, 466, 341, 497, 425, 38069, 2657, 30, 1042, 11, 437, 291, 362, 281, 4325, 307, 264, 497, 425, 38069, 2657, 50620], "temperature": 0.0, "avg_logprob": -0.12155752058153028, "compression_ratio": 1.5966850828729282, "no_speech_prob": 0.002606369322165847}, {"id": 364, "seek": 206144, "start": 2066.56, "end": 2074.16, "text": " represents everything that's possible, everything. And so, for example, we, as observers of what's", "tokens": [50620, 8855, 1203, 300, 311, 1944, 11, 1203, 13, 400, 370, 11, 337, 1365, 11, 321, 11, 382, 48090, 295, 437, 311, 51000], "temperature": 0.0, "avg_logprob": -0.12155752058153028, "compression_ratio": 1.5966850828729282, "no_speech_prob": 0.002606369322165847}, {"id": 365, "seek": 206144, "start": 2074.16, "end": 2081.12, "text": " going on, we must be embedded within this Ruliad. And so what we can think of is that this, this", "tokens": [51000, 516, 322, 11, 321, 1633, 312, 16741, 1951, 341, 497, 425, 38069, 13, 400, 370, 437, 321, 393, 519, 295, 307, 300, 341, 11, 341, 51348], "temperature": 0.0, "avg_logprob": -0.12155752058153028, "compression_ratio": 1.5966850828729282, "no_speech_prob": 0.002606369322165847}, {"id": 366, "seek": 208112, "start": 2081.68, "end": 2093.68, "text": " was I sharing the screen or did I stop sharing? Well, anyway, the, this, so the issue is we are", "tokens": [50392, 390, 286, 5414, 264, 2568, 420, 630, 286, 1590, 5414, 30, 1042, 11, 4033, 11, 264, 11, 341, 11, 370, 264, 2734, 307, 321, 366, 50992], "temperature": 0.0, "avg_logprob": -0.1476886362969121, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.025264127179980278}, {"id": 367, "seek": 208112, "start": 2093.68, "end": 2099.68, "text": " observers embedded within this Ruliad, observing the Ruliad. And the question is, what do we", "tokens": [50992, 48090, 16741, 1951, 341, 497, 425, 38069, 11, 22107, 264, 497, 425, 38069, 13, 400, 264, 1168, 307, 11, 437, 360, 321, 51292], "temperature": 0.0, "avg_logprob": -0.1476886362969121, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.025264127179980278}, {"id": 368, "seek": 208112, "start": 2099.68, "end": 2104.72, "text": " conclude about the Ruliad? And the Ruliad is a necessary thing, there's no choice about it.", "tokens": [51292, 16886, 466, 264, 497, 425, 38069, 30, 400, 264, 497, 425, 38069, 307, 257, 4818, 551, 11, 456, 311, 572, 3922, 466, 309, 13, 51544], "temperature": 0.0, "avg_logprob": -0.1476886362969121, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.025264127179980278}, {"id": 369, "seek": 210472, "start": 2105.2799999999997, "end": 2112.0, "text": " But the nature of us as observers is contingent, so to speak. And so what turns out to be the case", "tokens": [50392, 583, 264, 3687, 295, 505, 382, 48090, 307, 27820, 317, 11, 370, 281, 1710, 13, 400, 370, 437, 4523, 484, 281, 312, 264, 1389, 50728], "temperature": 0.0, "avg_logprob": -0.05083100935992073, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.12761986255645752}, {"id": 370, "seek": 210472, "start": 2112.0, "end": 2119.68, "text": " is that observers like us, observers that have certain attributes necessarily conclude that", "tokens": [50728, 307, 300, 48090, 411, 505, 11, 48090, 300, 362, 1629, 17212, 4725, 16886, 300, 51112], "temperature": 0.0, "avg_logprob": -0.05083100935992073, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.12761986255645752}, {"id": 371, "seek": 210472, "start": 2119.68, "end": 2126.16, "text": " necessarily describe the Ruliad in certain ways. So in a sense, by being an observer who is", "tokens": [51112, 4725, 6786, 264, 497, 425, 38069, 294, 1629, 2098, 13, 407, 294, 257, 2020, 11, 538, 885, 364, 27878, 567, 307, 51436], "temperature": 0.0, "avg_logprob": -0.05083100935992073, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.12761986255645752}, {"id": 372, "seek": 210472, "start": 2126.16, "end": 2131.68, "text": " computationally bounded, who believes they're persistent in time, those two attributes alone", "tokens": [51436, 24903, 379, 37498, 11, 567, 12307, 436, 434, 24315, 294, 565, 11, 729, 732, 17212, 3312, 51712], "temperature": 0.0, "avg_logprob": -0.05083100935992073, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.12761986255645752}, {"id": 373, "seek": 213168, "start": 2131.7599999999998, "end": 2136.3199999999997, "text": " are sufficient to tell us that the slice of the Ruliad, the way that we parse the Ruliad", "tokens": [50368, 366, 11563, 281, 980, 505, 300, 264, 13153, 295, 264, 497, 425, 38069, 11, 264, 636, 300, 321, 48377, 264, 497, 425, 38069, 50596], "temperature": 0.0, "avg_logprob": -0.09000769683292933, "compression_ratio": 1.860655737704918, "no_speech_prob": 0.015593399293720722}, {"id": 374, "seek": 213168, "start": 2136.8799999999997, "end": 2140.08, "text": " is exactly the way that corresponds to the laws of physics that we know.", "tokens": [50624, 307, 2293, 264, 636, 300, 23249, 281, 264, 6064, 295, 10649, 300, 321, 458, 13, 50784], "temperature": 0.0, "avg_logprob": -0.09000769683292933, "compression_ratio": 1.860655737704918, "no_speech_prob": 0.015593399293720722}, {"id": 375, "seek": 213168, "start": 2140.7999999999997, "end": 2145.8399999999997, "text": " So in other words, what we're saying is you can derive the laws of physics, the laws of physics", "tokens": [50820, 407, 294, 661, 2283, 11, 437, 321, 434, 1566, 307, 291, 393, 28446, 264, 6064, 295, 10649, 11, 264, 6064, 295, 10649, 51072], "temperature": 0.0, "avg_logprob": -0.09000769683292933, "compression_ratio": 1.860655737704918, "no_speech_prob": 0.015593399293720722}, {"id": 376, "seek": 213168, "start": 2145.8399999999997, "end": 2151.44, "text": " are derived by starting with this Ruliad, which is a necessary unique object, and then saying what,", "tokens": [51072, 366, 18949, 538, 2891, 365, 341, 497, 425, 38069, 11, 597, 307, 257, 4818, 3845, 2657, 11, 293, 550, 1566, 437, 11, 51352], "temperature": 0.0, "avg_logprob": -0.09000769683292933, "compression_ratio": 1.860655737704918, "no_speech_prob": 0.015593399293720722}, {"id": 377, "seek": 213168, "start": 2152.24, "end": 2156.96, "text": " for observers like us, which happen to have the properties that we have of being computationally", "tokens": [51392, 337, 48090, 411, 505, 11, 597, 1051, 281, 362, 264, 7221, 300, 321, 362, 295, 885, 24903, 379, 51628], "temperature": 0.0, "avg_logprob": -0.09000769683292933, "compression_ratio": 1.860655737704918, "no_speech_prob": 0.015593399293720722}, {"id": 378, "seek": 215696, "start": 2156.96, "end": 2162.7200000000003, "text": " bounded and believing we're persistent in time, any observer with those very coarse properties", "tokens": [50364, 37498, 293, 16594, 321, 434, 24315, 294, 565, 11, 604, 27878, 365, 729, 588, 39312, 7221, 50652], "temperature": 0.0, "avg_logprob": -0.0917803352954341, "compression_ratio": 1.696629213483146, "no_speech_prob": 0.013862897641956806}, {"id": 379, "seek": 215696, "start": 2162.7200000000003, "end": 2168.0, "text": " will necessarily conclude that the universe operates according to Einstein's equations and", "tokens": [50652, 486, 4725, 16886, 300, 264, 6445, 22577, 4650, 281, 23486, 311, 11787, 293, 50916], "temperature": 0.0, "avg_logprob": -0.0917803352954341, "compression_ratio": 1.696629213483146, "no_speech_prob": 0.013862897641956806}, {"id": 380, "seek": 215696, "start": 2168.0, "end": 2172.64, "text": " the path integral and so on. So that's a rather interesting philosophical conclusion.", "tokens": [50916, 264, 3100, 11573, 293, 370, 322, 13, 407, 300, 311, 257, 2831, 1880, 25066, 10063, 13, 51148], "temperature": 0.0, "avg_logprob": -0.0917803352954341, "compression_ratio": 1.696629213483146, "no_speech_prob": 0.013862897641956806}, {"id": 381, "seek": 215696, "start": 2173.2, "end": 2177.6, "text": " Now you can ask, well, what would observers not like us conclude? Well, we don't know.", "tokens": [51176, 823, 291, 393, 1029, 11, 731, 11, 437, 576, 48090, 406, 411, 505, 16886, 30, 1042, 11, 321, 500, 380, 458, 13, 51396], "temperature": 0.0, "avg_logprob": -0.0917803352954341, "compression_ratio": 1.696629213483146, "no_speech_prob": 0.013862897641956806}, {"id": 382, "seek": 215696, "start": 2178.4, "end": 2185.68, "text": " You can kind of, and that's sort of a question of how do we think about observers not like us?", "tokens": [51436, 509, 393, 733, 295, 11, 293, 300, 311, 1333, 295, 257, 1168, 295, 577, 360, 321, 519, 466, 48090, 406, 411, 505, 30, 51800], "temperature": 0.0, "avg_logprob": -0.0917803352954341, "compression_ratio": 1.696629213483146, "no_speech_prob": 0.013862897641956806}, {"id": 383, "seek": 218568, "start": 2186.56, "end": 2192.3999999999996, "text": " Well, one thing to realize is we can think of in the Ruliad, we can think of different possible", "tokens": [50408, 1042, 11, 472, 551, 281, 4325, 307, 321, 393, 519, 295, 294, 264, 497, 425, 38069, 11, 321, 393, 519, 295, 819, 1944, 50700], "temperature": 0.0, "avg_logprob": -0.07456427607043035, "compression_ratio": 2.1538461538461537, "no_speech_prob": 0.0014058873057365417}, {"id": 384, "seek": 218568, "start": 2192.3999999999996, "end": 2198.08, "text": " observers as being sort of at different points in the Ruliad. There are different places in Rulial", "tokens": [50700, 48090, 382, 885, 1333, 295, 412, 819, 2793, 294, 264, 497, 425, 38069, 13, 821, 366, 819, 3190, 294, 497, 425, 831, 50984], "temperature": 0.0, "avg_logprob": -0.07456427607043035, "compression_ratio": 2.1538461538461537, "no_speech_prob": 0.0014058873057365417}, {"id": 385, "seek": 218568, "start": 2198.08, "end": 2203.6, "text": " space. Just like in physical space, we could be here on this planet, we could be on a galaxy on", "tokens": [50984, 1901, 13, 1449, 411, 294, 4001, 1901, 11, 321, 727, 312, 510, 322, 341, 5054, 11, 321, 727, 312, 322, 257, 17639, 322, 51260], "temperature": 0.0, "avg_logprob": -0.07456427607043035, "compression_ratio": 2.1538461538461537, "no_speech_prob": 0.0014058873057365417}, {"id": 386, "seek": 218568, "start": 2203.6, "end": 2208.0, "text": " the other side of the universe, we can be at different places in physical space, and each", "tokens": [51260, 264, 661, 1252, 295, 264, 6445, 11, 321, 393, 312, 412, 819, 3190, 294, 4001, 1901, 11, 293, 1184, 51480], "temperature": 0.0, "avg_logprob": -0.07456427607043035, "compression_ratio": 2.1538461538461537, "no_speech_prob": 0.0014058873057365417}, {"id": 387, "seek": 218568, "start": 2208.0, "end": 2212.16, "text": " different place in physical space will give us a different point of view about how the universe", "tokens": [51480, 819, 1081, 294, 4001, 1901, 486, 976, 505, 257, 819, 935, 295, 1910, 466, 577, 264, 6445, 51688], "temperature": 0.0, "avg_logprob": -0.07456427607043035, "compression_ratio": 2.1538461538461537, "no_speech_prob": 0.0014058873057365417}, {"id": 388, "seek": 221216, "start": 2212.16, "end": 2218.24, "text": " works. Well, so it is in Rulial space, each different place in Rulial space will give us", "tokens": [50364, 1985, 13, 1042, 11, 370, 309, 307, 294, 497, 425, 831, 1901, 11, 1184, 819, 1081, 294, 497, 425, 831, 1901, 486, 976, 505, 50668], "temperature": 0.0, "avg_logprob": -0.06259451630294964, "compression_ratio": 1.8407960199004976, "no_speech_prob": 0.0027909548953175545}, {"id": 389, "seek": 221216, "start": 2218.24, "end": 2225.2799999999997, "text": " a different point of view about how the universe works, how things work. So here's a way to think", "tokens": [50668, 257, 819, 935, 295, 1910, 466, 577, 264, 6445, 1985, 11, 577, 721, 589, 13, 407, 510, 311, 257, 636, 281, 519, 51020], "temperature": 0.0, "avg_logprob": -0.06259451630294964, "compression_ratio": 1.8407960199004976, "no_speech_prob": 0.0027909548953175545}, {"id": 390, "seek": 221216, "start": 2225.2799999999997, "end": 2231.2799999999997, "text": " about that. We can think of essentially different minds as being at different places in Rulial", "tokens": [51020, 466, 300, 13, 492, 393, 519, 295, 4476, 819, 9634, 382, 885, 412, 819, 3190, 294, 497, 425, 831, 51320], "temperature": 0.0, "avg_logprob": -0.06259451630294964, "compression_ratio": 1.8407960199004976, "no_speech_prob": 0.0027909548953175545}, {"id": 391, "seek": 221216, "start": 2231.2799999999997, "end": 2240.7999999999997, "text": " space. It's as if, and these different minds are kind of experiencing possibilities in a", "tokens": [51320, 1901, 13, 467, 311, 382, 498, 11, 293, 613, 819, 9634, 366, 733, 295, 11139, 12178, 294, 257, 51796], "temperature": 0.0, "avg_logprob": -0.06259451630294964, "compression_ratio": 1.8407960199004976, "no_speech_prob": 0.0027909548953175545}, {"id": 392, "seek": 224080, "start": 2240.8, "end": 2246.0, "text": " different way. So if we think about that in terms of, you know, the LLMs and so on, it's kind of", "tokens": [50364, 819, 636, 13, 407, 498, 321, 519, 466, 300, 294, 2115, 295, 11, 291, 458, 11, 264, 441, 43, 26386, 293, 370, 322, 11, 309, 311, 733, 295, 50624], "temperature": 0.0, "avg_logprob": -0.08854658749638772, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.006487448234111071}, {"id": 393, "seek": 224080, "start": 2246.0, "end": 2251.52, "text": " like we could imagine just having a differently trained LLM and that differently trained LLM", "tokens": [50624, 411, 321, 727, 3811, 445, 1419, 257, 7614, 8895, 441, 43, 44, 293, 300, 7614, 8895, 441, 43, 44, 50900], "temperature": 0.0, "avg_logprob": -0.08854658749638772, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.006487448234111071}, {"id": 394, "seek": 224080, "start": 2251.52, "end": 2258.8, "text": " basically exists at a different place in Rulial space. So for example, minds that are sort of", "tokens": [50900, 1936, 8198, 412, 257, 819, 1081, 294, 497, 425, 831, 1901, 13, 407, 337, 1365, 11, 9634, 300, 366, 1333, 295, 51264], "temperature": 0.0, "avg_logprob": -0.08854658749638772, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.006487448234111071}, {"id": 395, "seek": 224080, "start": 2258.8, "end": 2263.6000000000004, "text": " similar and sort of similarly trained will be fairly close in Rulial space. Minds that are", "tokens": [51264, 2531, 293, 1333, 295, 14138, 8895, 486, 312, 6457, 1998, 294, 497, 425, 831, 1901, 13, 13719, 82, 300, 366, 51504], "temperature": 0.0, "avg_logprob": -0.08854658749638772, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.006487448234111071}, {"id": 396, "seek": 226360, "start": 2263.6, "end": 2268.0, "text": " different, like, you know, let's say cats and dogs, further away in Rulial space.", "tokens": [50364, 819, 11, 411, 11, 291, 458, 11, 718, 311, 584, 11111, 293, 7197, 11, 3052, 1314, 294, 497, 425, 831, 1901, 13, 50584], "temperature": 0.0, "avg_logprob": -0.10640620457307073, "compression_ratio": 1.8310344827586207, "no_speech_prob": 0.13900016248226166}, {"id": 397, "seek": 226360, "start": 2268.7999999999997, "end": 2273.52, "text": " Minds, I tend to, I think that one of the consequences of the principle of computational", "tokens": [50624, 13719, 82, 11, 286, 3928, 281, 11, 286, 519, 300, 472, 295, 264, 10098, 295, 264, 8665, 295, 28270, 50860], "temperature": 0.0, "avg_logprob": -0.10640620457307073, "compression_ratio": 1.8310344827586207, "no_speech_prob": 0.13900016248226166}, {"id": 398, "seek": 226360, "start": 2273.52, "end": 2278.24, "text": " equivalence that I mentioned earlier is that one could sort of attribute mind like things", "tokens": [50860, 9052, 655, 300, 286, 2835, 3071, 307, 300, 472, 727, 1333, 295, 19667, 1575, 411, 721, 51096], "temperature": 0.0, "avg_logprob": -0.10640620457307073, "compression_ratio": 1.8310344827586207, "no_speech_prob": 0.13900016248226166}, {"id": 399, "seek": 226360, "start": 2278.24, "end": 2283.04, "text": " to lots of systems in the world and lots of abstract systems. And so for example,", "tokens": [51096, 281, 3195, 295, 3652, 294, 264, 1002, 293, 3195, 295, 12649, 3652, 13, 400, 370, 337, 1365, 11, 51336], "temperature": 0.0, "avg_logprob": -0.10640620457307073, "compression_ratio": 1.8310344827586207, "no_speech_prob": 0.13900016248226166}, {"id": 400, "seek": 226360, "start": 2283.04, "end": 2288.72, "text": " when one says the weather has a mind of its own, in the principle of computational equivalence says,", "tokens": [51336, 562, 472, 1619, 264, 5503, 575, 257, 1575, 295, 1080, 1065, 11, 294, 264, 8665, 295, 28270, 9052, 655, 1619, 11, 51620], "temperature": 0.0, "avg_logprob": -0.10640620457307073, "compression_ratio": 1.8310344827586207, "no_speech_prob": 0.13900016248226166}, {"id": 401, "seek": 226360, "start": 2288.72, "end": 2293.44, "text": " yes, that's a meaningful thing to say. But in a sense, the mind that corresponds to the", "tokens": [51620, 2086, 11, 300, 311, 257, 10995, 551, 281, 584, 13, 583, 294, 257, 2020, 11, 264, 1575, 300, 23249, 281, 264, 51856], "temperature": 0.0, "avg_logprob": -0.10640620457307073, "compression_ratio": 1.8310344827586207, "no_speech_prob": 0.13900016248226166}, {"id": 402, "seek": 229344, "start": 2293.44, "end": 2299.28, "text": " weather is pretty far away from us in Rulial space. Also now there's a question, how do you", "tokens": [50364, 5503, 307, 1238, 1400, 1314, 490, 505, 294, 497, 425, 831, 1901, 13, 2743, 586, 456, 311, 257, 1168, 11, 577, 360, 291, 50656], "temperature": 0.0, "avg_logprob": -0.08305969944706669, "compression_ratio": 2.049808429118774, "no_speech_prob": 0.0014572403160855174}, {"id": 403, "seek": 229344, "start": 2299.28, "end": 2304.32, "text": " communicate across Rulial space? How do you, what is it what's involved in doing that? Well,", "tokens": [50656, 7890, 2108, 497, 425, 831, 1901, 30, 1012, 360, 291, 11, 437, 307, 309, 437, 311, 3288, 294, 884, 300, 30, 1042, 11, 50908], "temperature": 0.0, "avg_logprob": -0.08305969944706669, "compression_ratio": 2.049808429118774, "no_speech_prob": 0.0014572403160855174}, {"id": 404, "seek": 229344, "start": 2304.32, "end": 2309.52, "text": " at some computational level, one point in Rulial space corresponds to sort of computing,", "tokens": [50908, 412, 512, 28270, 1496, 11, 472, 935, 294, 497, 425, 831, 1901, 23249, 281, 1333, 295, 15866, 11, 51168], "temperature": 0.0, "avg_logprob": -0.08305969944706669, "compression_ratio": 2.049808429118774, "no_speech_prob": 0.0014572403160855174}, {"id": 405, "seek": 229344, "start": 2309.52, "end": 2313.44, "text": " according to let's say one Turing machine, another point in Rulial space computing,", "tokens": [51168, 4650, 281, 718, 311, 584, 472, 314, 1345, 3479, 11, 1071, 935, 294, 497, 425, 831, 1901, 15866, 11, 51364], "temperature": 0.0, "avg_logprob": -0.08305969944706669, "compression_ratio": 2.049808429118774, "no_speech_prob": 0.0014572403160855174}, {"id": 406, "seek": 229344, "start": 2313.44, "end": 2317.52, "text": " according to another Turing machine, another computer. We know that in principle, we can", "tokens": [51364, 4650, 281, 1071, 314, 1345, 3479, 11, 1071, 3820, 13, 492, 458, 300, 294, 8665, 11, 321, 393, 51568], "temperature": 0.0, "avg_logprob": -0.08305969944706669, "compression_ratio": 2.049808429118774, "no_speech_prob": 0.0014572403160855174}, {"id": 407, "seek": 229344, "start": 2317.52, "end": 2321.44, "text": " make a translation from one place in Rulial space to another place in Rulial space takes", "tokens": [51568, 652, 257, 12853, 490, 472, 1081, 294, 497, 425, 831, 1901, 281, 1071, 1081, 294, 497, 425, 831, 1901, 2516, 51764], "temperature": 0.0, "avg_logprob": -0.08305969944706669, "compression_ratio": 2.049808429118774, "no_speech_prob": 0.0014572403160855174}, {"id": 408, "seek": 232144, "start": 2321.44, "end": 2325.36, "text": " effort. We have to actually create that interpreter that's going to interpret the", "tokens": [50364, 4630, 13, 492, 362, 281, 767, 1884, 300, 34132, 300, 311, 516, 281, 7302, 264, 50560], "temperature": 0.0, "avg_logprob": -0.057684905945308625, "compression_ratio": 1.8791946308724832, "no_speech_prob": 0.010472942143678665}, {"id": 409, "seek": 232144, "start": 2325.36, "end": 2329.76, "text": " instructions of one machine as the instructions of another machine. It takes effort in the same", "tokens": [50560, 9415, 295, 472, 3479, 382, 264, 9415, 295, 1071, 3479, 13, 467, 2516, 4630, 294, 264, 912, 50780], "temperature": 0.0, "avg_logprob": -0.057684905945308625, "compression_ratio": 1.8791946308724832, "no_speech_prob": 0.010472942143678665}, {"id": 410, "seek": 232144, "start": 2329.76, "end": 2335.28, "text": " way as it takes effort to move in physical space. In a sense, when we move in physical space in our", "tokens": [50780, 636, 382, 309, 2516, 4630, 281, 1286, 294, 4001, 1901, 13, 682, 257, 2020, 11, 562, 321, 1286, 294, 4001, 1901, 294, 527, 51056], "temperature": 0.0, "avg_logprob": -0.057684905945308625, "compression_ratio": 1.8791946308724832, "no_speech_prob": 0.010472942143678665}, {"id": 411, "seek": 232144, "start": 2335.28, "end": 2340.56, "text": " models, we're reconstructing ourselves at a different point in physical space. And by the way,", "tokens": [51056, 5245, 11, 321, 434, 31499, 278, 4175, 412, 257, 819, 935, 294, 4001, 1901, 13, 400, 538, 264, 636, 11, 51320], "temperature": 0.0, "avg_logprob": -0.057684905945308625, "compression_ratio": 1.8791946308724832, "no_speech_prob": 0.010472942143678665}, {"id": 412, "seek": 232144, "start": 2340.56, "end": 2344.7200000000003, "text": " you can understand things like time dilation and relativity. There's a nice kind of mechanical", "tokens": [51320, 291, 393, 1223, 721, 411, 565, 11504, 399, 293, 45675, 13, 821, 311, 257, 1481, 733, 295, 12070, 51528], "temperature": 0.0, "avg_logprob": -0.057684905945308625, "compression_ratio": 1.8791946308724832, "no_speech_prob": 0.010472942143678665}, {"id": 413, "seek": 232144, "start": 2344.7200000000003, "end": 2349.52, "text": " explanation of that. If you're always in one place, you're spending your kind of computation", "tokens": [51528, 10835, 295, 300, 13, 759, 291, 434, 1009, 294, 472, 1081, 11, 291, 434, 6434, 428, 733, 295, 24903, 51768], "temperature": 0.0, "avg_logprob": -0.057684905945308625, "compression_ratio": 1.8791946308724832, "no_speech_prob": 0.010472942143678665}, {"id": 414, "seek": 234952, "start": 2349.52, "end": 2355.52, "text": " budget figuring out what the next behave what the what the next stage you'll be in is. But if", "tokens": [50364, 4706, 15213, 484, 437, 264, 958, 15158, 437, 264, 437, 264, 958, 3233, 291, 603, 312, 294, 307, 13, 583, 498, 50664], "temperature": 0.0, "avg_logprob": -0.09734670465642756, "compression_ratio": 2.0170212765957447, "no_speech_prob": 0.005658965557813644}, {"id": 415, "seek": 234952, "start": 2355.52, "end": 2360.72, "text": " you're moving, then you're using some of your computation budget to kind of recreate yourself", "tokens": [50664, 291, 434, 2684, 11, 550, 291, 434, 1228, 512, 295, 428, 24903, 4706, 281, 733, 295, 25833, 1803, 50924], "temperature": 0.0, "avg_logprob": -0.09734670465642756, "compression_ratio": 2.0170212765957447, "no_speech_prob": 0.005658965557813644}, {"id": 416, "seek": 234952, "start": 2360.72, "end": 2366.56, "text": " at a different place in space. And so that's used up having used up some of your computation budget,", "tokens": [50924, 412, 257, 819, 1081, 294, 1901, 13, 400, 370, 300, 311, 1143, 493, 1419, 1143, 493, 512, 295, 428, 24903, 4706, 11, 51216], "temperature": 0.0, "avg_logprob": -0.09734670465642756, "compression_ratio": 2.0170212765957447, "no_speech_prob": 0.005658965557813644}, {"id": 417, "seek": 234952, "start": 2366.56, "end": 2372.96, "text": " you necessarily sort of moves go through time more slowly time time goes more slowly because", "tokens": [51216, 291, 4725, 1333, 295, 6067, 352, 807, 565, 544, 5692, 565, 565, 1709, 544, 5692, 570, 51536], "temperature": 0.0, "avg_logprob": -0.09734670465642756, "compression_ratio": 2.0170212765957447, "no_speech_prob": 0.005658965557813644}, {"id": 418, "seek": 234952, "start": 2372.96, "end": 2378.96, "text": " you used up some of your computation budget in moving in space. But in any case, you can you", "tokens": [51536, 291, 1143, 493, 512, 295, 428, 24903, 4706, 294, 2684, 294, 1901, 13, 583, 294, 604, 1389, 11, 291, 393, 291, 51836], "temperature": 0.0, "avg_logprob": -0.09734670465642756, "compression_ratio": 2.0170212765957447, "no_speech_prob": 0.005658965557813644}, {"id": 419, "seek": 237896, "start": 2378.96, "end": 2385.2, "text": " can think of so so by the way, in our models, the possibility of motion is non trivial. It's not", "tokens": [50364, 393, 519, 295, 370, 370, 538, 264, 636, 11, 294, 527, 5245, 11, 264, 7959, 295, 5394, 307, 2107, 26703, 13, 467, 311, 406, 50676], "temperature": 0.0, "avg_logprob": -0.09905242083365456, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.002504888456314802}, {"id": 420, "seek": 237896, "start": 2385.2, "end": 2390.2400000000002, "text": " obvious that you can you know pick up a glass and move it somewhere, and it'll still be the same", "tokens": [50676, 6322, 300, 291, 393, 291, 458, 1888, 493, 257, 4276, 293, 1286, 309, 4079, 11, 293, 309, 603, 920, 312, 264, 912, 50928], "temperature": 0.0, "avg_logprob": -0.09905242083365456, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.002504888456314802}, {"id": 421, "seek": 237896, "start": 2390.2400000000002, "end": 2395.2, "text": " glass. That's something that we generally assume about the world that pure motion is possible.", "tokens": [50928, 4276, 13, 663, 311, 746, 300, 321, 5101, 6552, 466, 264, 1002, 300, 6075, 5394, 307, 1944, 13, 51176], "temperature": 0.0, "avg_logprob": -0.09905242083365456, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.002504888456314802}, {"id": 422, "seek": 237896, "start": 2395.2, "end": 2399.52, "text": " But it's something in our models that you have to prove that pure motion is possible. And even", "tokens": [51176, 583, 309, 311, 746, 294, 527, 5245, 300, 291, 362, 281, 7081, 300, 6075, 5394, 307, 1944, 13, 400, 754, 51392], "temperature": 0.0, "avg_logprob": -0.09905242083365456, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.002504888456314802}, {"id": 423, "seek": 237896, "start": 2399.52, "end": 2405.04, "text": " in traditional physics, if you're sufficiently near a space time singularity, for example,", "tokens": [51392, 294, 5164, 10649, 11, 498, 291, 434, 31868, 2651, 257, 1901, 565, 20010, 507, 11, 337, 1365, 11, 51668], "temperature": 0.0, "avg_logprob": -0.09905242083365456, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.002504888456314802}, {"id": 424, "seek": 240504, "start": 2405.04, "end": 2411.2799999999997, "text": " no no material object will maintain its identity as you move it around that that singularity.", "tokens": [50364, 572, 572, 2527, 2657, 486, 6909, 1080, 6575, 382, 291, 1286, 309, 926, 300, 300, 20010, 507, 13, 50676], "temperature": 0.0, "avg_logprob": -0.1073266581485146, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.00296171847730875}, {"id": 425, "seek": 240504, "start": 2411.2799999999997, "end": 2416.56, "text": " But in our models, the the possibility that that thing can just move, and that it's still the same", "tokens": [50676, 583, 294, 527, 5245, 11, 264, 264, 7959, 300, 300, 551, 393, 445, 1286, 11, 293, 300, 309, 311, 920, 264, 912, 50940], "temperature": 0.0, "avg_logprob": -0.1073266581485146, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.00296171847730875}, {"id": 426, "seek": 240504, "start": 2416.56, "end": 2423.2, "text": " thing is non trivial. And actually, in a sense, the the particles of motion are exactly the kinds", "tokens": [50940, 551, 307, 2107, 26703, 13, 400, 767, 11, 294, 257, 2020, 11, 264, 264, 10007, 295, 5394, 366, 2293, 264, 3685, 51272], "temperature": 0.0, "avg_logprob": -0.1073266581485146, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.00296171847730875}, {"id": 427, "seek": 240504, "start": 2423.2, "end": 2427.6, "text": " of particles that we know about like electrons and quarks and so on. What is an electron an", "tokens": [51272, 295, 10007, 300, 321, 458, 466, 411, 14265, 293, 421, 20851, 293, 370, 322, 13, 708, 307, 364, 6084, 364, 51492], "temperature": 0.0, "avg_logprob": -0.1073266581485146, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.00296171847730875}, {"id": 428, "seek": 240504, "start": 2427.6, "end": 2433.84, "text": " electron in some sense in an abstract level is a lump that is capable of pure motion. It's something", "tokens": [51492, 6084, 294, 512, 2020, 294, 364, 12649, 1496, 307, 257, 25551, 300, 307, 8189, 295, 6075, 5394, 13, 467, 311, 746, 51804], "temperature": 0.0, "avg_logprob": -0.1073266581485146, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.00296171847730875}, {"id": 429, "seek": 243384, "start": 2433.84, "end": 2439.44, "text": " where you can have an electron in one place, and you can move it and it'll still just be that electron.", "tokens": [50364, 689, 291, 393, 362, 364, 6084, 294, 472, 1081, 11, 293, 291, 393, 1286, 309, 293, 309, 603, 920, 445, 312, 300, 6084, 13, 50644], "temperature": 0.0, "avg_logprob": -0.10082318297529642, "compression_ratio": 1.8582995951417005, "no_speech_prob": 0.010877556167542934}, {"id": 430, "seek": 243384, "start": 2439.44, "end": 2443.44, "text": " So it's it's a particles are kind of the carriers of pure motion and physical space.", "tokens": [50644, 407, 309, 311, 309, 311, 257, 10007, 366, 733, 295, 264, 28541, 295, 6075, 5394, 293, 4001, 1901, 13, 50844], "temperature": 0.0, "avg_logprob": -0.10082318297529642, "compression_ratio": 1.8582995951417005, "no_speech_prob": 0.010877556167542934}, {"id": 431, "seek": 243384, "start": 2444.32, "end": 2450.4, "text": " So here's a here's a thing in rural space, we can ask sort of what is motion in rural space about.", "tokens": [50888, 407, 510, 311, 257, 510, 311, 257, 551, 294, 11165, 1901, 11, 321, 393, 1029, 1333, 295, 437, 307, 5394, 294, 11165, 1901, 466, 13, 51192], "temperature": 0.0, "avg_logprob": -0.10082318297529642, "compression_ratio": 1.8582995951417005, "no_speech_prob": 0.010877556167542934}, {"id": 432, "seek": 243384, "start": 2451.1200000000003, "end": 2456.0, "text": " Well, in a sense, what what it means to have motion in rural space is you're effectively", "tokens": [51228, 1042, 11, 294, 257, 2020, 11, 437, 437, 309, 1355, 281, 362, 5394, 294, 11165, 1901, 307, 291, 434, 8659, 51472], "temperature": 0.0, "avg_logprob": -0.10082318297529642, "compression_ratio": 1.8582995951417005, "no_speech_prob": 0.010877556167542934}, {"id": 433, "seek": 243384, "start": 2456.0, "end": 2461.2000000000003, "text": " transporting something from one mind to another if different points in rural space", "tokens": [51472, 49302, 746, 490, 472, 1575, 281, 1071, 498, 819, 2793, 294, 11165, 1901, 51732], "temperature": 0.0, "avg_logprob": -0.10082318297529642, "compression_ratio": 1.8582995951417005, "no_speech_prob": 0.010877556167542934}, {"id": 434, "seek": 246120, "start": 2461.2799999999997, "end": 2465.8399999999997, "text": " correspond to the positions of different minds, you're asking the question, what does it take", "tokens": [50368, 6805, 281, 264, 8432, 295, 819, 9634, 11, 291, 434, 3365, 264, 1168, 11, 437, 775, 309, 747, 50596], "temperature": 0.0, "avg_logprob": -0.10082439013889857, "compression_ratio": 1.7306273062730628, "no_speech_prob": 0.005102047696709633}, {"id": 435, "seek": 246120, "start": 2465.8399999999997, "end": 2471.6, "text": " to kind of transport things around rural space. And I think this is one of the very bizarre", "tokens": [50596, 281, 733, 295, 5495, 721, 926, 11165, 1901, 13, 400, 286, 519, 341, 307, 472, 295, 264, 588, 18265, 50884], "temperature": 0.0, "avg_logprob": -0.10082439013889857, "compression_ratio": 1.7306273062730628, "no_speech_prob": 0.005102047696709633}, {"id": 436, "seek": 246120, "start": 2471.6, "end": 2477.04, "text": " kinds of things that one realizes is it seems to be the case that concepts are the analog of", "tokens": [50884, 3685, 295, 721, 300, 472, 29316, 307, 309, 2544, 281, 312, 264, 1389, 300, 10392, 366, 264, 16660, 295, 51156], "temperature": 0.0, "avg_logprob": -0.10082439013889857, "compression_ratio": 1.7306273062730628, "no_speech_prob": 0.005102047696709633}, {"id": 437, "seek": 246120, "start": 2477.04, "end": 2482.0, "text": " particles. So what in physical space and an electron that doesn't change as you move it from", "tokens": [51156, 10007, 13, 407, 437, 294, 4001, 1901, 293, 364, 6084, 300, 1177, 380, 1319, 382, 291, 1286, 309, 490, 51404], "temperature": 0.0, "avg_logprob": -0.10082439013889857, "compression_ratio": 1.7306273062730628, "no_speech_prob": 0.005102047696709633}, {"id": 438, "seek": 246120, "start": 2482.0, "end": 2489.2, "text": " here to there, in rural space, it's the concept of a cat, for example, that can be moved from one", "tokens": [51404, 510, 281, 456, 11, 294, 11165, 1901, 11, 309, 311, 264, 3410, 295, 257, 3857, 11, 337, 1365, 11, 300, 393, 312, 4259, 490, 472, 51764], "temperature": 0.0, "avg_logprob": -0.10082439013889857, "compression_ratio": 1.7306273062730628, "no_speech_prob": 0.005102047696709633}, {"id": 439, "seek": 248920, "start": 2489.2, "end": 2494.56, "text": " mind to another without change. I mean, the particular details of the neural firings that", "tokens": [50364, 1575, 281, 1071, 1553, 1319, 13, 286, 914, 11, 264, 1729, 4365, 295, 264, 18161, 12159, 1109, 300, 50632], "temperature": 0.0, "avg_logprob": -0.07153111457824707, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.013396723195910454}, {"id": 440, "seek": 248920, "start": 2494.56, "end": 2500.3199999999997, "text": " exist in my brain, when I think of the concept of cat, in any of your brains, the particular", "tokens": [50632, 2514, 294, 452, 3567, 11, 562, 286, 519, 295, 264, 3410, 295, 3857, 11, 294, 604, 295, 428, 15442, 11, 264, 1729, 50920], "temperature": 0.0, "avg_logprob": -0.07153111457824707, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.013396723195910454}, {"id": 441, "seek": 248920, "start": 2500.3199999999997, "end": 2506.7999999999997, "text": " neural firings will be different. But yet, we can package up the concept of a cat, and I can say", "tokens": [50920, 18161, 12159, 1109, 486, 312, 819, 13, 583, 1939, 11, 321, 393, 7372, 493, 264, 3410, 295, 257, 3857, 11, 293, 286, 393, 584, 51244], "temperature": 0.0, "avg_logprob": -0.07153111457824707, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.013396723195910454}, {"id": 442, "seek": 248920, "start": 2506.7999999999997, "end": 2514.0, "text": " the word cat, I can transport it to you, and then you can unpack it. And in your place in rural space,", "tokens": [51244, 264, 1349, 3857, 11, 286, 393, 5495, 309, 281, 291, 11, 293, 550, 291, 393, 26699, 309, 13, 400, 294, 428, 1081, 294, 11165, 1901, 11, 51604], "temperature": 0.0, "avg_logprob": -0.07153111457824707, "compression_ratio": 1.7850467289719627, "no_speech_prob": 0.013396723195910454}, {"id": 443, "seek": 251400, "start": 2514.08, "end": 2519.52, "text": " you can end up with the same thing, so to speak. So it's kind of a way of understanding that that's", "tokens": [50368, 291, 393, 917, 493, 365, 264, 912, 551, 11, 370, 281, 1710, 13, 407, 309, 311, 733, 295, 257, 636, 295, 3701, 300, 300, 311, 50640], "temperature": 0.0, "avg_logprob": -0.10218450338533609, "compression_ratio": 1.7276785714285714, "no_speech_prob": 0.03478798270225525}, {"id": 444, "seek": 251400, "start": 2519.52, "end": 2525.92, "text": " sort of the fundamental thing that's going on. And we can think of kind of concepts as being", "tokens": [50640, 1333, 295, 264, 8088, 551, 300, 311, 516, 322, 13, 400, 321, 393, 519, 295, 733, 295, 10392, 382, 885, 50960], "temperature": 0.0, "avg_logprob": -0.10218450338533609, "compression_ratio": 1.7276785714285714, "no_speech_prob": 0.03478798270225525}, {"id": 445, "seek": 251400, "start": 2525.92, "end": 2532.96, "text": " the particles of rural space. Well, there are lots of lots of things I see I'm running out of time", "tokens": [50960, 264, 10007, 295, 11165, 1901, 13, 1042, 11, 456, 366, 3195, 295, 3195, 295, 721, 286, 536, 286, 478, 2614, 484, 295, 565, 51312], "temperature": 0.0, "avg_logprob": -0.10218450338533609, "compression_ratio": 1.7276785714285714, "no_speech_prob": 0.03478798270225525}, {"id": 446, "seek": 251400, "start": 2532.96, "end": 2540.32, "text": " here. But there are lots of things we can talk about about what, well, let me just say a couple", "tokens": [51312, 510, 13, 583, 456, 366, 3195, 295, 721, 321, 393, 751, 466, 466, 437, 11, 731, 11, 718, 385, 445, 584, 257, 1916, 51680], "temperature": 0.0, "avg_logprob": -0.10218450338533609, "compression_ratio": 1.7276785714285714, "no_speech_prob": 0.03478798270225525}, {"id": 447, "seek": 254032, "start": 2540.32, "end": 2549.28, "text": " of other things about, I'll talk a little bit about AI. And I mean, the AI has had many different", "tokens": [50364, 295, 661, 721, 466, 11, 286, 603, 751, 257, 707, 857, 466, 7318, 13, 400, 286, 914, 11, 264, 7318, 575, 632, 867, 819, 50812], "temperature": 0.0, "avg_logprob": -0.15643566786640822, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.023428749293088913}, {"id": 448, "seek": 254032, "start": 2549.28, "end": 2553.6800000000003, "text": " meanings over the course of time. And many things where people have said, if we can only have that,", "tokens": [50812, 28138, 670, 264, 1164, 295, 565, 13, 400, 867, 721, 689, 561, 362, 848, 11, 498, 321, 393, 787, 362, 300, 11, 51032], "temperature": 0.0, "avg_logprob": -0.15643566786640822, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.023428749293088913}, {"id": 449, "seek": 254032, "start": 2553.6800000000003, "end": 2559.28, "text": " then we have AI are things that I've built as kind of pure computational systems. And then", "tokens": [51032, 550, 321, 362, 7318, 366, 721, 300, 286, 600, 3094, 382, 733, 295, 6075, 28270, 3652, 13, 400, 550, 51312], "temperature": 0.0, "avg_logprob": -0.15643566786640822, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.023428749293088913}, {"id": 450, "seek": 254032, "start": 2559.28, "end": 2563.28, "text": " people say, well, it's just a computational system, it's not really AI. And in more than seconds,", "tokens": [51312, 561, 584, 11, 731, 11, 309, 311, 445, 257, 28270, 1185, 11, 309, 311, 406, 534, 7318, 13, 400, 294, 544, 813, 3949, 11, 51512], "temperature": 0.0, "avg_logprob": -0.15643566786640822, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.023428749293088913}, {"id": 451, "seek": 256328, "start": 2563.84, "end": 2569.0400000000004, "text": " just one second, I want to, you do have more time, because for some reason, I can't explain,", "tokens": [50392, 445, 472, 1150, 11, 286, 528, 281, 11, 291, 360, 362, 544, 565, 11, 570, 337, 512, 1778, 11, 286, 393, 380, 2903, 11, 50652], "temperature": 0.0, "avg_logprob": -0.18899275332081075, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.04005957394838333}, {"id": 452, "seek": 256328, "start": 2570.1600000000003, "end": 2575.84, "text": " Caillou, who has been extremely conscientious in everything, is not here, which may mean that he", "tokens": [50708, 7544, 373, 263, 11, 567, 575, 668, 4664, 44507, 851, 294, 1203, 11, 307, 406, 510, 11, 597, 815, 914, 300, 415, 50992], "temperature": 0.0, "avg_logprob": -0.18899275332081075, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.04005957394838333}, {"id": 453, "seek": 256328, "start": 2575.84, "end": 2580.96, "text": " had misunderstood being a discussant for being a member of the panel, which means he won't be here", "tokens": [50992, 632, 33870, 885, 257, 2248, 394, 337, 885, 257, 4006, 295, 264, 4831, 11, 597, 1355, 415, 1582, 380, 312, 510, 51248], "temperature": 0.0, "avg_logprob": -0.18899275332081075, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.04005957394838333}, {"id": 454, "seek": 256328, "start": 2580.96, "end": 2588.32, "text": " until the panel starts, in which case you have more time, if you wish. I just compressed four", "tokens": [51248, 1826, 264, 4831, 3719, 11, 294, 597, 1389, 291, 362, 544, 565, 11, 498, 291, 3172, 13, 286, 445, 30353, 1451, 51616], "temperature": 0.0, "avg_logprob": -0.18899275332081075, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.04005957394838333}, {"id": 455, "seek": 258832, "start": 2588.4, "end": 2592.2400000000002, "text": " and a half decades into a remarkably short time, I hope people could follow it.", "tokens": [50368, 293, 257, 1922, 7878, 666, 257, 37381, 2099, 565, 11, 286, 1454, 561, 727, 1524, 309, 13, 50560], "temperature": 0.0, "avg_logprob": -0.1543587293380346, "compression_ratio": 1.4801980198019802, "no_speech_prob": 0.012504576705396175}, {"id": 456, "seek": 258832, "start": 2592.8, "end": 2596.1600000000003, "text": " Nothing, you could have taken place at this time.", "tokens": [50588, 6693, 11, 291, 727, 362, 2726, 1081, 412, 341, 565, 13, 50756], "temperature": 0.0, "avg_logprob": -0.1543587293380346, "compression_ratio": 1.4801980198019802, "no_speech_prob": 0.012504576705396175}, {"id": 457, "seek": 258832, "start": 2599.28, "end": 2602.7200000000003, "text": " Well, okay, so let me let me finish what I was saying here, and then maybe we can", "tokens": [50912, 1042, 11, 1392, 11, 370, 718, 385, 718, 385, 2413, 437, 286, 390, 1566, 510, 11, 293, 550, 1310, 321, 393, 51084], "temperature": 0.0, "avg_logprob": -0.1543587293380346, "compression_ratio": 1.4801980198019802, "no_speech_prob": 0.012504576705396175}, {"id": 458, "seek": 258832, "start": 2602.7200000000003, "end": 2609.84, "text": " turn this over to discussion, which is more fun for me. So talking about kind of modern", "tokens": [51084, 1261, 341, 670, 281, 5017, 11, 597, 307, 544, 1019, 337, 385, 13, 407, 1417, 466, 733, 295, 4363, 51440], "temperature": 0.0, "avg_logprob": -0.1543587293380346, "compression_ratio": 1.4801980198019802, "no_speech_prob": 0.012504576705396175}, {"id": 459, "seek": 260984, "start": 2609.84, "end": 2619.52, "text": " AIs, and you know, to many people, modern AI is neural networks. And the, there's sort of a", "tokens": [50364, 316, 6802, 11, 293, 291, 458, 11, 281, 867, 561, 11, 4363, 7318, 307, 18161, 9590, 13, 400, 264, 11, 456, 311, 1333, 295, 257, 50848], "temperature": 0.0, "avg_logprob": -0.13745896021525064, "compression_ratio": 1.5810055865921788, "no_speech_prob": 0.069085031747818}, {"id": 460, "seek": 260984, "start": 2619.52, "end": 2630.2400000000002, "text": " question of, well, what can, how do neural networks relate to all of the things I've been talking", "tokens": [50848, 1168, 295, 11, 731, 11, 437, 393, 11, 577, 360, 18161, 9590, 10961, 281, 439, 295, 264, 721, 286, 600, 668, 1417, 51384], "temperature": 0.0, "avg_logprob": -0.13745896021525064, "compression_ratio": 1.5810055865921788, "no_speech_prob": 0.069085031747818}, {"id": 461, "seek": 260984, "start": 2630.2400000000002, "end": 2636.08, "text": " about? And one of the questions we can ask is, okay, we have, we have our friendly neural net", "tokens": [51384, 466, 30, 400, 472, 295, 264, 1651, 321, 393, 1029, 307, 11, 1392, 11, 321, 362, 11, 321, 362, 527, 9208, 18161, 2533, 51676], "temperature": 0.0, "avg_logprob": -0.13745896021525064, "compression_ratio": 1.5810055865921788, "no_speech_prob": 0.069085031747818}, {"id": 462, "seek": 263608, "start": 2636.16, "end": 2639.12, "text": " here, let's see, oops, share the screen and then,", "tokens": [50368, 510, 11, 718, 311, 536, 11, 34166, 11, 2073, 264, 2568, 293, 550, 11, 50516], "temperature": 0.0, "avg_logprob": -0.13672217078830884, "compression_ratio": 1.8742857142857143, "no_speech_prob": 0.0025116298347711563}, {"id": 463, "seek": 263608, "start": 2646.24, "end": 2650.24, "text": " okay, we have some typical trained neural net, let's say we're trying to train it,", "tokens": [50872, 1392, 11, 321, 362, 512, 7476, 8895, 18161, 2533, 11, 718, 311, 584, 321, 434, 1382, 281, 3847, 309, 11, 51072], "temperature": 0.0, "avg_logprob": -0.13672217078830884, "compression_ratio": 1.8742857142857143, "no_speech_prob": 0.0025116298347711563}, {"id": 464, "seek": 263608, "start": 2650.24, "end": 2655.68, "text": " let's say we're trying to train it to reproduce the sine wave. So what we're doing is we're going", "tokens": [51072, 718, 311, 584, 321, 434, 1382, 281, 3847, 309, 281, 29501, 264, 18609, 5772, 13, 407, 437, 321, 434, 884, 307, 321, 434, 516, 51344], "temperature": 0.0, "avg_logprob": -0.13672217078830884, "compression_ratio": 1.8742857142857143, "no_speech_prob": 0.0025116298347711563}, {"id": 465, "seek": 263608, "start": 2655.68, "end": 2662.4, "text": " to feed in the x value at the top there, and we're going to have set up these neural net weights,", "tokens": [51344, 281, 3154, 294, 264, 2031, 2158, 412, 264, 1192, 456, 11, 293, 321, 434, 516, 281, 362, 992, 493, 613, 18161, 2533, 17443, 11, 51680], "temperature": 0.0, "avg_logprob": -0.13672217078830884, "compression_ratio": 1.8742857142857143, "no_speech_prob": 0.0025116298347711563}, {"id": 466, "seek": 266240, "start": 2662.4, "end": 2667.36, "text": " and it's going to compute the y value down here. And actually, we'll do a pretty crummy job of that", "tokens": [50364, 293, 309, 311, 516, 281, 14722, 264, 288, 2158, 760, 510, 13, 400, 767, 11, 321, 603, 360, 257, 1238, 941, 8620, 1691, 295, 300, 50612], "temperature": 0.0, "avg_logprob": -0.09193489628453408, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.006769353058189154}, {"id": 467, "seek": 266240, "start": 2667.36, "end": 2673.04, "text": " typically. And you can, you can change the neural net, you'll get different kinds of behavior,", "tokens": [50612, 5850, 13, 400, 291, 393, 11, 291, 393, 1319, 264, 18161, 2533, 11, 291, 603, 483, 819, 3685, 295, 5223, 11, 50896], "temperature": 0.0, "avg_logprob": -0.09193489628453408, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.006769353058189154}, {"id": 468, "seek": 266240, "start": 2673.04, "end": 2677.52, "text": " it's usually not particularly good at computing something like this. Well, so one thing you can", "tokens": [50896, 309, 311, 2673, 406, 4098, 665, 412, 15866, 746, 411, 341, 13, 1042, 11, 370, 472, 551, 291, 393, 51120], "temperature": 0.0, "avg_logprob": -0.09193489628453408, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.006769353058189154}, {"id": 469, "seek": 266240, "start": 2677.52, "end": 2683.44, "text": " ask is, you know, neural nets, let's say if we have a big enough neural net, maybe we can break", "tokens": [51120, 1029, 307, 11, 291, 458, 11, 18161, 36170, 11, 718, 311, 584, 498, 321, 362, 257, 955, 1547, 18161, 2533, 11, 1310, 321, 393, 1821, 51416], "temperature": 0.0, "avg_logprob": -0.09193489628453408, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.006769353058189154}, {"id": 470, "seek": 266240, "start": 2683.44, "end": 2687.44, "text": " computational irreducibility, maybe we can just predict what's going to happen in any kind of system.", "tokens": [51416, 28270, 16014, 769, 537, 39802, 11, 1310, 321, 393, 445, 6069, 437, 311, 516, 281, 1051, 294, 604, 733, 295, 1185, 13, 51616], "temperature": 0.0, "avg_logprob": -0.09193489628453408, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.006769353058189154}, {"id": 471, "seek": 268744, "start": 2688.4, "end": 2696.32, "text": " That is not going to work. I mean, the way that a neural net of this type works, it's just having", "tokens": [50412, 663, 307, 406, 516, 281, 589, 13, 286, 914, 11, 264, 636, 300, 257, 18161, 2533, 295, 341, 2010, 1985, 11, 309, 311, 445, 1419, 50808], "temperature": 0.0, "avg_logprob": -0.06708325809902616, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.005398089997470379}, {"id": 472, "seek": 268744, "start": 2696.32, "end": 2701.6, "text": " kind of numbers ripple through the sequence of layers. And we're ending up with something where", "tokens": [50808, 733, 295, 3547, 40688, 807, 264, 8310, 295, 7914, 13, 400, 321, 434, 8121, 493, 365, 746, 689, 51072], "temperature": 0.0, "avg_logprob": -0.06708325809902616, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.005398089997470379}, {"id": 473, "seek": 268744, "start": 2701.6, "end": 2710.32, "text": " you can, this is something trained, I used a modern transformer architecture and trained it", "tokens": [51072, 291, 393, 11, 341, 307, 746, 8895, 11, 286, 1143, 257, 4363, 31782, 9482, 293, 8895, 309, 51508], "temperature": 0.0, "avg_logprob": -0.06708325809902616, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.005398089997470379}, {"id": 474, "seek": 268744, "start": 2710.32, "end": 2714.96, "text": " to try and recognize what was going to happen in a cellular automaton. And it has certain,", "tokens": [51508, 281, 853, 293, 5521, 437, 390, 516, 281, 1051, 294, 257, 29267, 3553, 25781, 13, 400, 309, 575, 1629, 11, 51740], "temperature": 0.0, "avg_logprob": -0.06708325809902616, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.005398089997470379}, {"id": 475, "seek": 271496, "start": 2714.96, "end": 2718.96, "text": " it says, well, there's certain probability of what's going to happen. But when the behavior is", "tokens": [50364, 309, 1619, 11, 731, 11, 456, 311, 1629, 8482, 295, 437, 311, 516, 281, 1051, 13, 583, 562, 264, 5223, 307, 50564], "temperature": 0.0, "avg_logprob": -0.09485552046034071, "compression_ratio": 1.743859649122807, "no_speech_prob": 0.002135509392246604}, {"id": 476, "seek": 271496, "start": 2718.96, "end": 2724.32, "text": " pretty simple, it'll nail it. When the behavior is more complicated, it's like, I'm sorry, I can't,", "tokens": [50564, 1238, 2199, 11, 309, 603, 10173, 309, 13, 1133, 264, 5223, 307, 544, 6179, 11, 309, 311, 411, 11, 286, 478, 2597, 11, 286, 393, 380, 11, 50832], "temperature": 0.0, "avg_logprob": -0.09485552046034071, "compression_ratio": 1.743859649122807, "no_speech_prob": 0.002135509392246604}, {"id": 477, "seek": 271496, "start": 2725.68, "end": 2730.08, "text": " you know, I can't figure that out. This is, this is different levels of training of one of those", "tokens": [50900, 291, 458, 11, 286, 393, 380, 2573, 300, 484, 13, 639, 307, 11, 341, 307, 819, 4358, 295, 3097, 295, 472, 295, 729, 51120], "temperature": 0.0, "avg_logprob": -0.09485552046034071, "compression_ratio": 1.743859649122807, "no_speech_prob": 0.002135509392246604}, {"id": 478, "seek": 271496, "start": 2730.08, "end": 2736.7200000000003, "text": " neural nets. So in a sense, it's not surprisingly, the kind of very finite computation of these layers", "tokens": [51120, 18161, 36170, 13, 407, 294, 257, 2020, 11, 309, 311, 406, 17600, 11, 264, 733, 295, 588, 19362, 24903, 295, 613, 7914, 51452], "temperature": 0.0, "avg_logprob": -0.09485552046034071, "compression_ratio": 1.743859649122807, "no_speech_prob": 0.002135509392246604}, {"id": 479, "seek": 271496, "start": 2736.7200000000003, "end": 2743.36, "text": " of a neural net can't do the unboundedly large computation required to kind of solve a computationally", "tokens": [51452, 295, 257, 18161, 2533, 393, 380, 360, 264, 517, 18767, 13516, 2416, 24903, 4739, 281, 733, 295, 5039, 257, 24903, 379, 51784], "temperature": 0.0, "avg_logprob": -0.09485552046034071, "compression_ratio": 1.743859649122807, "no_speech_prob": 0.002135509392246604}, {"id": 480, "seek": 274336, "start": 2743.44, "end": 2749.44, "text": " irreducible problem. And you can see that again. See, where do I have an example here? This is,", "tokens": [50368, 16014, 769, 32128, 1154, 13, 400, 291, 393, 536, 300, 797, 13, 3008, 11, 689, 360, 286, 362, 364, 1365, 510, 30, 639, 307, 11, 50668], "temperature": 0.0, "avg_logprob": -0.11836692034187964, "compression_ratio": 1.625, "no_speech_prob": 0.005565493833273649}, {"id": 481, "seek": 274336, "start": 2749.44, "end": 2756.96, "text": " these are examples of the three body problem in celestial mechanics, Earth, Moon, Sun, all idealized,", "tokens": [50668, 613, 366, 5110, 295, 264, 1045, 1772, 1154, 294, 41003, 12939, 11, 4755, 11, 10714, 11, 6163, 11, 439, 7157, 1602, 11, 51044], "temperature": 0.0, "avg_logprob": -0.11836692034187964, "compression_ratio": 1.625, "no_speech_prob": 0.005565493833273649}, {"id": 482, "seek": 274336, "start": 2756.96, "end": 2762.08, "text": " all with interacting through gravity. You can ask the question, if you train a neural net,", "tokens": [51044, 439, 365, 18017, 807, 12110, 13, 509, 393, 1029, 264, 1168, 11, 498, 291, 3847, 257, 18161, 2533, 11, 51300], "temperature": 0.0, "avg_logprob": -0.11836692034187964, "compression_ratio": 1.625, "no_speech_prob": 0.005565493833273649}, {"id": 483, "seek": 274336, "start": 2762.08, "end": 2767.2000000000003, "text": " can it correctly reproduce the behavior? The answer is the neural net is the kind of solid", "tokens": [51300, 393, 309, 8944, 29501, 264, 5223, 30, 440, 1867, 307, 264, 18161, 2533, 307, 264, 733, 295, 5100, 51556], "temperature": 0.0, "avg_logprob": -0.11836692034187964, "compression_ratio": 1.625, "no_speech_prob": 0.005565493833273649}, {"id": 484, "seek": 274336, "start": 2767.2000000000003, "end": 2771.76, "text": " line here, that's its prediction. When the behavior is fairly simple, yes, it can do it.", "tokens": [51556, 1622, 510, 11, 300, 311, 1080, 17630, 13, 1133, 264, 5223, 307, 6457, 2199, 11, 2086, 11, 309, 393, 360, 309, 13, 51784], "temperature": 0.0, "avg_logprob": -0.11836692034187964, "compression_ratio": 1.625, "no_speech_prob": 0.005565493833273649}, {"id": 485, "seek": 277176, "start": 2771.76, "end": 2774.96, "text": " When the behavior is kind of computationally irreducible, no, it can't do it.", "tokens": [50364, 1133, 264, 5223, 307, 733, 295, 24903, 379, 16014, 769, 32128, 11, 572, 11, 309, 393, 380, 360, 309, 13, 50524], "temperature": 0.0, "avg_logprob": -0.07986316346285637, "compression_ratio": 1.6386861313868613, "no_speech_prob": 0.0024504861794412136}, {"id": 486, "seek": 277176, "start": 2775.5200000000004, "end": 2782.1600000000003, "text": " None of this is really very surprising. But there's kind of the question, for example,", "tokens": [50552, 14492, 295, 341, 307, 534, 588, 8830, 13, 583, 456, 311, 733, 295, 264, 1168, 11, 337, 1365, 11, 50884], "temperature": 0.0, "avg_logprob": -0.07986316346285637, "compression_ratio": 1.6386861313868613, "no_speech_prob": 0.0024504861794412136}, {"id": 487, "seek": 277176, "start": 2782.1600000000003, "end": 2786.88, "text": " when we look at something like chat GPT, and we say, oh my gosh, it actually worked, it produced", "tokens": [50884, 562, 321, 574, 412, 746, 411, 5081, 26039, 51, 11, 293, 321, 584, 11, 1954, 452, 6502, 11, 309, 767, 2732, 11, 309, 7126, 51120], "temperature": 0.0, "avg_logprob": -0.07986316346285637, "compression_ratio": 1.6386861313868613, "no_speech_prob": 0.0024504861794412136}, {"id": 488, "seek": 277176, "start": 2786.88, "end": 2792.8, "text": " something that is like human language. How did that work? What I think is the main thing going on", "tokens": [51120, 746, 300, 307, 411, 1952, 2856, 13, 1012, 630, 300, 589, 30, 708, 286, 519, 307, 264, 2135, 551, 516, 322, 51416], "temperature": 0.0, "avg_logprob": -0.07986316346285637, "compression_ratio": 1.6386861313868613, "no_speech_prob": 0.0024504861794412136}, {"id": 489, "seek": 277176, "start": 2792.8, "end": 2796.7200000000003, "text": " is something which tells us a lot more about human language, probably, than it does about", "tokens": [51416, 307, 746, 597, 5112, 505, 257, 688, 544, 466, 1952, 2856, 11, 1391, 11, 813, 309, 775, 466, 51612], "temperature": 0.0, "avg_logprob": -0.07986316346285637, "compression_ratio": 1.6386861313868613, "no_speech_prob": 0.0024504861794412136}, {"id": 490, "seek": 279672, "start": 2796.72, "end": 2805.3599999999997, "text": " neural nets. Because what it's telling us is, if we think about how does chat GPT work,", "tokens": [50364, 18161, 36170, 13, 1436, 437, 309, 311, 3585, 505, 307, 11, 498, 321, 519, 466, 577, 775, 5081, 26039, 51, 589, 11, 50796], "temperature": 0.0, "avg_logprob": -0.11558034502226731, "compression_ratio": 1.752851711026616, "no_speech_prob": 0.01911829225718975}, {"id": 491, "seek": 279672, "start": 2805.9199999999996, "end": 2812.3999999999996, "text": " it's basically just saying, I'm going to predict the next word by figuring out certain probabilities.", "tokens": [50824, 309, 311, 1936, 445, 1566, 11, 286, 478, 516, 281, 6069, 264, 958, 1349, 538, 15213, 484, 1629, 33783, 13, 51148], "temperature": 0.0, "avg_logprob": -0.11558034502226731, "compression_ratio": 1.752851711026616, "no_speech_prob": 0.01911829225718975}, {"id": 492, "seek": 279672, "start": 2812.3999999999996, "end": 2817.68, "text": " And it's going to do that by, at the very simplest level, it might just do it, let's see if we've got", "tokens": [51148, 400, 309, 311, 516, 281, 360, 300, 538, 11, 412, 264, 588, 22811, 1496, 11, 309, 1062, 445, 360, 309, 11, 718, 311, 536, 498, 321, 600, 658, 51412], "temperature": 0.0, "avg_logprob": -0.11558034502226731, "compression_ratio": 1.752851711026616, "no_speech_prob": 0.01911829225718975}, {"id": 493, "seek": 279672, "start": 2817.68, "end": 2823.04, "text": " one here, might just do it by knowing the frequencies of different letters. And then,", "tokens": [51412, 472, 510, 11, 1062, 445, 360, 309, 538, 5276, 264, 20250, 295, 819, 7825, 13, 400, 550, 11, 51680], "temperature": 0.0, "avg_logprob": -0.11558034502226731, "compression_ratio": 1.752851711026616, "no_speech_prob": 0.01911829225718975}, {"id": 494, "seek": 279672, "start": 2823.04, "end": 2826.3199999999997, "text": " if you just use the frequencies of different letters, you get pretty much nonsense.", "tokens": [51680, 498, 291, 445, 764, 264, 20250, 295, 819, 7825, 11, 291, 483, 1238, 709, 14925, 13, 51844], "temperature": 0.0, "avg_logprob": -0.11558034502226731, "compression_ratio": 1.752851711026616, "no_speech_prob": 0.01911829225718975}, {"id": 495, "seek": 282632, "start": 2826.32, "end": 2830.0800000000004, "text": " If you use blocks of letters, you'll start getting more sensible kinds of things.", "tokens": [50364, 759, 291, 764, 8474, 295, 7825, 11, 291, 603, 722, 1242, 544, 25380, 3685, 295, 721, 13, 50552], "temperature": 0.0, "avg_logprob": -0.07052029632940525, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0017130024498328567}, {"id": 496, "seek": 282632, "start": 2830.0800000000004, "end": 2836.88, "text": " If you use kind of whole words occurring with the probability that they occur in English,", "tokens": [50552, 759, 291, 764, 733, 295, 1379, 2283, 18386, 365, 264, 8482, 300, 436, 5160, 294, 3669, 11, 50892], "temperature": 0.0, "avg_logprob": -0.07052029632940525, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0017130024498328567}, {"id": 497, "seek": 282632, "start": 2836.88, "end": 2843.36, "text": " you'll get things that don't make much sense, but they're kind of things that can construct.", "tokens": [50892, 291, 603, 483, 721, 300, 500, 380, 652, 709, 2020, 11, 457, 436, 434, 733, 295, 721, 300, 393, 7690, 13, 51216], "temperature": 0.0, "avg_logprob": -0.07052029632940525, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0017130024498328567}, {"id": 498, "seek": 282632, "start": 2843.36, "end": 2849.44, "text": " Now, the big thing that's interesting and surprising is that when you kind of train", "tokens": [51216, 823, 11, 264, 955, 551, 300, 311, 1880, 293, 8830, 307, 300, 562, 291, 733, 295, 3847, 51520], "temperature": 0.0, "avg_logprob": -0.07052029632940525, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0017130024498328567}, {"id": 499, "seek": 284944, "start": 2849.44, "end": 2856.8, "text": " a neural net from kind of all of the text, you know, a trillion words of text or something,", "tokens": [50364, 257, 18161, 2533, 490, 733, 295, 439, 295, 264, 2487, 11, 291, 458, 11, 257, 18723, 2283, 295, 2487, 420, 746, 11, 50732], "temperature": 0.0, "avg_logprob": -0.08270835876464844, "compression_ratio": 1.84, "no_speech_prob": 0.04918640851974487}, {"id": 500, "seek": 284944, "start": 2857.44, "end": 2863.76, "text": " that the extrapolations it makes about what make meaningful sentences tend to agree with", "tokens": [50764, 300, 264, 48224, 763, 309, 1669, 466, 437, 652, 10995, 16579, 3928, 281, 3986, 365, 51080], "temperature": 0.0, "avg_logprob": -0.08270835876464844, "compression_ratio": 1.84, "no_speech_prob": 0.04918640851974487}, {"id": 501, "seek": 284944, "start": 2863.76, "end": 2868.7200000000003, "text": " the extrapolations that we humans would make about that. It's very similar to the fact that if we", "tokens": [51080, 264, 48224, 763, 300, 321, 6255, 576, 652, 466, 300, 13, 467, 311, 588, 2531, 281, 264, 1186, 300, 498, 321, 51328], "temperature": 0.0, "avg_logprob": -0.08270835876464844, "compression_ratio": 1.84, "no_speech_prob": 0.04918640851974487}, {"id": 502, "seek": 284944, "start": 2868.7200000000003, "end": 2874.7200000000003, "text": " train a neural net to recognize cats from dogs and images, that the distinctions it will make", "tokens": [51328, 3847, 257, 18161, 2533, 281, 5521, 11111, 490, 7197, 293, 5267, 11, 300, 264, 1483, 49798, 309, 486, 652, 51628], "temperature": 0.0, "avg_logprob": -0.08270835876464844, "compression_ratio": 1.84, "no_speech_prob": 0.04918640851974487}, {"id": 503, "seek": 284944, "start": 2874.7200000000003, "end": 2879.28, "text": " seem to be similar to the distinctions we will make. At a theoretical level, if we say,", "tokens": [51628, 1643, 281, 312, 2531, 281, 264, 1483, 49798, 321, 486, 652, 13, 1711, 257, 20864, 1496, 11, 498, 321, 584, 11, 51856], "temperature": 0.0, "avg_logprob": -0.08270835876464844, "compression_ratio": 1.84, "no_speech_prob": 0.04918640851974487}, {"id": 504, "seek": 287928, "start": 2879.28, "end": 2884.8, "text": " where's the dividing line between cat pictures and dog pictures? There isn't a good mathematical", "tokens": [50364, 689, 311, 264, 26764, 1622, 1296, 3857, 5242, 293, 3000, 5242, 30, 821, 1943, 380, 257, 665, 18894, 50640], "temperature": 0.0, "avg_logprob": -0.08150256266359424, "compression_ratio": 1.8904109589041096, "no_speech_prob": 0.0009680709918029606}, {"id": 505, "seek": 287928, "start": 2884.8, "end": 2890.2400000000002, "text": " characterization of where that dividing line is. It's really a question of where do we humans say", "tokens": [50640, 49246, 295, 689, 300, 26764, 1622, 307, 13, 467, 311, 534, 257, 1168, 295, 689, 360, 321, 6255, 584, 50912], "temperature": 0.0, "avg_logprob": -0.08150256266359424, "compression_ratio": 1.8904109589041096, "no_speech_prob": 0.0009680709918029606}, {"id": 506, "seek": 287928, "start": 2890.2400000000002, "end": 2894.7200000000003, "text": " is a dividing line between cats and dogs? And the thing that's interesting about neural nets", "tokens": [50912, 307, 257, 26764, 1622, 1296, 11111, 293, 7197, 30, 400, 264, 551, 300, 311, 1880, 466, 18161, 36170, 51136], "temperature": 0.0, "avg_logprob": -0.08150256266359424, "compression_ratio": 1.8904109589041096, "no_speech_prob": 0.0009680709918029606}, {"id": 507, "seek": 287928, "start": 2894.7200000000003, "end": 2899.0400000000004, "text": " is they tend to make the same kinds of decisions about that that we tend to make.", "tokens": [51136, 307, 436, 3928, 281, 652, 264, 912, 3685, 295, 5327, 466, 300, 300, 321, 3928, 281, 652, 13, 51352], "temperature": 0.0, "avg_logprob": -0.08150256266359424, "compression_ratio": 1.8904109589041096, "no_speech_prob": 0.0009680709918029606}, {"id": 508, "seek": 287928, "start": 2899.6000000000004, "end": 2903.6000000000004, "text": " Probably the reason is that ultimately their architecture is similar to the architecture", "tokens": [51380, 9210, 264, 1778, 307, 300, 6284, 641, 9482, 307, 2531, 281, 264, 9482, 51580], "temperature": 0.0, "avg_logprob": -0.08150256266359424, "compression_ratio": 1.8904109589041096, "no_speech_prob": 0.0009680709918029606}, {"id": 509, "seek": 287928, "start": 2903.6000000000004, "end": 2908.32, "text": " of our brains. But the main point is that those kinds of distinctions, there's not a theorem,", "tokens": [51580, 295, 527, 15442, 13, 583, 264, 2135, 935, 307, 300, 729, 3685, 295, 1483, 49798, 11, 456, 311, 406, 257, 20904, 11, 51816], "temperature": 0.0, "avg_logprob": -0.08150256266359424, "compression_ratio": 1.8904109589041096, "no_speech_prob": 0.0009680709918029606}, {"id": 510, "seek": 290832, "start": 2908.4, "end": 2913.28, "text": " there's no theorem that says the neural net will reproduce the distinction between cats and dogs,", "tokens": [50368, 456, 311, 572, 20904, 300, 1619, 264, 18161, 2533, 486, 29501, 264, 16844, 1296, 11111, 293, 7197, 11, 50612], "temperature": 0.0, "avg_logprob": -0.07092441061268683, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.0017149458872154355}, {"id": 511, "seek": 290832, "start": 2913.28, "end": 2917.44, "text": " because you don't know what the target is. The target is what do humans think is going on there,", "tokens": [50612, 570, 291, 500, 380, 458, 437, 264, 3779, 307, 13, 440, 3779, 307, 437, 360, 6255, 519, 307, 516, 322, 456, 11, 50820], "temperature": 0.0, "avg_logprob": -0.07092441061268683, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.0017149458872154355}, {"id": 512, "seek": 290832, "start": 2917.44, "end": 2922.6400000000003, "text": " and it does a pretty good job at that. So now the question is, in the case of language, what's", "tokens": [50820, 293, 309, 775, 257, 1238, 665, 1691, 412, 300, 13, 407, 586, 264, 1168, 307, 11, 294, 264, 1389, 295, 2856, 11, 437, 311, 51080], "temperature": 0.0, "avg_logprob": -0.07092441061268683, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.0017149458872154355}, {"id": 513, "seek": 290832, "start": 2922.6400000000003, "end": 2932.7200000000003, "text": " going on? And I think what's happened is that the thing that allows an LLM to produce reasonable", "tokens": [51080, 516, 322, 30, 400, 286, 519, 437, 311, 2011, 307, 300, 264, 551, 300, 4045, 364, 441, 43, 44, 281, 5258, 10585, 51584], "temperature": 0.0, "avg_logprob": -0.07092441061268683, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.0017149458872154355}, {"id": 514, "seek": 290832, "start": 2932.7200000000003, "end": 2937.92, "text": " language is something that is a regularity of language that we could have recognized a long", "tokens": [51584, 2856, 307, 746, 300, 307, 257, 3890, 507, 295, 2856, 300, 321, 727, 362, 9823, 257, 938, 51844], "temperature": 0.0, "avg_logprob": -0.07092441061268683, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.0017149458872154355}, {"id": 515, "seek": 293792, "start": 2937.92, "end": 2944.0, "text": " time ago, but we didn't. And so we know certain regularities in language. We know that, for example,", "tokens": [50364, 565, 2057, 11, 457, 321, 994, 380, 13, 400, 370, 321, 458, 1629, 3890, 1088, 294, 2856, 13, 492, 458, 300, 11, 337, 1365, 11, 50668], "temperature": 0.0, "avg_logprob": -0.1132121483484904, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.0031877292785793543}, {"id": 516, "seek": 293792, "start": 2944.0, "end": 2950.7200000000003, "text": " in English, you tend to have sentences that go noun verb noun. But there are plenty of sentences", "tokens": [50668, 294, 3669, 11, 291, 3928, 281, 362, 16579, 300, 352, 23307, 9595, 23307, 13, 583, 456, 366, 7140, 295, 16579, 51004], "temperature": 0.0, "avg_logprob": -0.1132121483484904, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.0031877292785793543}, {"id": 517, "seek": 293792, "start": 2950.7200000000003, "end": 2956.64, "text": " of the form noun verb noun that are total nonsense. So the question is, you have this kind of", "tokens": [51004, 295, 264, 1254, 23307, 9595, 23307, 300, 366, 3217, 14925, 13, 407, 264, 1168, 307, 11, 291, 362, 341, 733, 295, 51300], "temperature": 0.0, "avg_logprob": -0.1132121483484904, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.0031877292785793543}, {"id": 518, "seek": 293792, "start": 2956.64, "end": 2961.76, "text": " syntactic grammar of language that says that you go things like noun verb noun. But now you have", "tokens": [51300, 23980, 19892, 22317, 295, 2856, 300, 1619, 300, 291, 352, 721, 411, 23307, 9595, 23307, 13, 583, 586, 291, 362, 51556], "temperature": 0.0, "avg_logprob": -0.1132121483484904, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.0031877292785793543}, {"id": 519, "seek": 296176, "start": 2961.76, "end": 2967.6000000000004, "text": " the question of, well, what noun verb nouns actually make sense? And so what I think chat,", "tokens": [50364, 264, 1168, 295, 11, 731, 11, 437, 23307, 9595, 48184, 767, 652, 2020, 30, 400, 370, 437, 286, 519, 5081, 11, 50656], "temperature": 0.0, "avg_logprob": -0.07843587358119124, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.26771941781044006}, {"id": 520, "seek": 296176, "start": 2967.6000000000004, "end": 2972.7200000000003, "text": " you know, chat GBT and LLMs and so on are kind of showing us is that there is also a semantic", "tokens": [50656, 291, 458, 11, 5081, 26809, 51, 293, 441, 43, 26386, 293, 370, 322, 366, 733, 295, 4099, 505, 307, 300, 456, 307, 611, 257, 47982, 50912], "temperature": 0.0, "avg_logprob": -0.07843587358119124, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.26771941781044006}, {"id": 521, "seek": 296176, "start": 2972.7200000000003, "end": 2979.44, "text": " grammar of language. There's also a construction kit, not only of what the parts of speech might be,", "tokens": [50912, 22317, 295, 2856, 13, 821, 311, 611, 257, 6435, 8260, 11, 406, 787, 295, 437, 264, 3166, 295, 6218, 1062, 312, 11, 51248], "temperature": 0.0, "avg_logprob": -0.07843587358119124, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.26771941781044006}, {"id": 522, "seek": 296176, "start": 2979.44, "end": 2984.6400000000003, "text": " but also what kinds of words they might be to have them make sense. And that's something that", "tokens": [51248, 457, 611, 437, 3685, 295, 2283, 436, 1062, 312, 281, 362, 552, 652, 2020, 13, 400, 300, 311, 746, 300, 51508], "temperature": 0.0, "avg_logprob": -0.07843587358119124, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.26771941781044006}, {"id": 523, "seek": 296176, "start": 2984.6400000000003, "end": 2989.76, "text": " eventually kind of sort of expands up to write a whole essay and have these puzzle pieces fit", "tokens": [51508, 4728, 733, 295, 1333, 295, 33706, 493, 281, 2464, 257, 1379, 16238, 293, 362, 613, 12805, 3755, 3318, 51764], "temperature": 0.0, "avg_logprob": -0.07843587358119124, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.26771941781044006}, {"id": 524, "seek": 298976, "start": 2989.76, "end": 2995.1200000000003, "text": " together in a way so that the whole thing makes sense. So, you know, in a sense, what one's", "tokens": [50364, 1214, 294, 257, 636, 370, 300, 264, 1379, 551, 1669, 2020, 13, 407, 11, 291, 458, 11, 294, 257, 2020, 11, 437, 472, 311, 50632], "temperature": 0.0, "avg_logprob": -0.15844051510679955, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.006931372452527285}, {"id": 525, "seek": 298976, "start": 2995.1200000000003, "end": 3002.5600000000004, "text": " seeing and one can kind of look at, let's see if I have some pictures here. Maybe I have some pictures.", "tokens": [50632, 2577, 293, 472, 393, 733, 295, 574, 412, 11, 718, 311, 536, 498, 286, 362, 512, 5242, 510, 13, 2704, 286, 362, 512, 5242, 13, 51004], "temperature": 0.0, "avg_logprob": -0.15844051510679955, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.006931372452527285}, {"id": 526, "seek": 298976, "start": 3004.96, "end": 3009.92, "text": " Yeah, these are from, these are very ancient, actually, there's better ones now for GBT-4.", "tokens": [51124, 865, 11, 613, 366, 490, 11, 613, 366, 588, 7832, 11, 767, 11, 456, 311, 1101, 2306, 586, 337, 26809, 51, 12, 19, 13, 51372], "temperature": 0.0, "avg_logprob": -0.15844051510679955, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.006931372452527285}, {"id": 527, "seek": 298976, "start": 3012.2400000000002, "end": 3018.2400000000002, "text": " What extent can you kind of imagine semantic laws of motion where you're kind of moving around in", "tokens": [51488, 708, 8396, 393, 291, 733, 295, 3811, 47982, 6064, 295, 5394, 689, 291, 434, 733, 295, 2684, 926, 294, 51788], "temperature": 0.0, "avg_logprob": -0.15844051510679955, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.006931372452527285}, {"id": 528, "seek": 301824, "start": 3018.24, "end": 3024.4799999999996, "text": " meaning space, and where, just like Newton's laws tell you in physical space, how you move", "tokens": [50364, 3620, 1901, 11, 293, 689, 11, 445, 411, 19541, 311, 6064, 980, 291, 294, 4001, 1901, 11, 577, 291, 1286, 50676], "temperature": 0.0, "avg_logprob": -0.1004003871570934, "compression_ratio": 1.8380566801619433, "no_speech_prob": 0.009259797632694244}, {"id": 529, "seek": 301824, "start": 3024.4799999999996, "end": 3030.24, "text": " from one, you know, how motion happens when in the absence of a force you just keep moving in", "tokens": [50676, 490, 472, 11, 291, 458, 11, 577, 5394, 2314, 562, 294, 264, 17145, 295, 257, 3464, 291, 445, 1066, 2684, 294, 50964], "temperature": 0.0, "avg_logprob": -0.1004003871570934, "compression_ratio": 1.8380566801619433, "no_speech_prob": 0.009259797632694244}, {"id": 530, "seek": 301824, "start": 3030.24, "end": 3035.12, "text": " the same direction and so on. So you can ask questions about rural space, and you can ask", "tokens": [50964, 264, 912, 3513, 293, 370, 322, 13, 407, 291, 393, 1029, 1651, 466, 11165, 1901, 11, 293, 291, 393, 1029, 51208], "temperature": 0.0, "avg_logprob": -0.1004003871570934, "compression_ratio": 1.8380566801619433, "no_speech_prob": 0.009259797632694244}, {"id": 531, "seek": 301824, "start": 3035.12, "end": 3042.0, "text": " questions about kind of the structure of rural space and how that works. And I think we're kind", "tokens": [51208, 1651, 466, 733, 295, 264, 3877, 295, 11165, 1901, 293, 577, 300, 1985, 13, 400, 286, 519, 321, 434, 733, 51552], "temperature": 0.0, "avg_logprob": -0.1004003871570934, "compression_ratio": 1.8380566801619433, "no_speech_prob": 0.009259797632694244}, {"id": 532, "seek": 301824, "start": 3042.0, "end": 3046.56, "text": " of learning some scientific things from the operation of LLMs about how that works.", "tokens": [51552, 295, 2539, 512, 8134, 721, 490, 264, 6916, 295, 441, 43, 26386, 466, 577, 300, 1985, 13, 51780], "temperature": 0.0, "avg_logprob": -0.1004003871570934, "compression_ratio": 1.8380566801619433, "no_speech_prob": 0.009259797632694244}, {"id": 533, "seek": 304656, "start": 3047.2799999999997, "end": 3054.7999999999997, "text": " Now, another question would be, so in other words, I think the reason LLMs work as well as they do", "tokens": [50400, 823, 11, 1071, 1168, 576, 312, 11, 370, 294, 661, 2283, 11, 286, 519, 264, 1778, 441, 43, 26386, 589, 382, 731, 382, 436, 360, 50776], "temperature": 0.0, "avg_logprob": -0.08371159904881527, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.002238634740933776}, {"id": 534, "seek": 304656, "start": 3054.7999999999997, "end": 3058.56, "text": " is because there are a bunch of regularities in human language that we kind of didn't know were", "tokens": [50776, 307, 570, 456, 366, 257, 3840, 295, 3890, 1088, 294, 1952, 2856, 300, 321, 733, 295, 994, 380, 458, 645, 50964], "temperature": 0.0, "avg_logprob": -0.08371159904881527, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.002238634740933776}, {"id": 535, "seek": 304656, "start": 3058.56, "end": 3064.32, "text": " there and that we've never really codified. People started codifying these things back in the 1600s,", "tokens": [50964, 456, 293, 300, 321, 600, 1128, 534, 17656, 2587, 13, 3432, 1409, 17656, 5489, 613, 721, 646, 294, 264, 36885, 82, 11, 51252], "temperature": 0.0, "avg_logprob": -0.08371159904881527, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.002238634740933776}, {"id": 536, "seek": 304656, "start": 3064.32, "end": 3069.2799999999997, "text": " for example, people tried to invent these so-called philosophical languages that would be kind of", "tokens": [51252, 337, 1365, 11, 561, 3031, 281, 7962, 613, 370, 12, 11880, 25066, 8650, 300, 576, 312, 733, 295, 51500], "temperature": 0.0, "avg_logprob": -0.08371159904881527, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.002238634740933776}, {"id": 537, "seek": 304656, "start": 3069.92, "end": 3073.84, "text": " not specific to any particular language, but they would be things that sort of represent the", "tokens": [51532, 406, 2685, 281, 604, 1729, 2856, 11, 457, 436, 576, 312, 721, 300, 1333, 295, 2906, 264, 51728], "temperature": 0.0, "avg_logprob": -0.08371159904881527, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.002238634740933776}, {"id": 538, "seek": 307384, "start": 3073.84, "end": 3079.6000000000004, "text": " meaning of things without the specificity of particular languages. Well, actually,", "tokens": [50364, 3620, 295, 721, 1553, 264, 2685, 507, 295, 1729, 8650, 13, 1042, 11, 767, 11, 50652], "temperature": 0.0, "avg_logprob": -0.09678762555122375, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.0027612631674855947}, {"id": 539, "seek": 307384, "start": 3079.6000000000004, "end": 3085.1200000000003, "text": " I've had a project for a while now much more energetic to make what I call a symbolic discourse", "tokens": [50652, 286, 600, 632, 257, 1716, 337, 257, 1339, 586, 709, 544, 24935, 281, 652, 437, 286, 818, 257, 25755, 23938, 50928], "temperature": 0.0, "avg_logprob": -0.09678762555122375, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.0027612631674855947}, {"id": 540, "seek": 307384, "start": 3085.1200000000003, "end": 3091.1200000000003, "text": " language, a language where just like in Wolfman language, we have this computational language", "tokens": [50928, 2856, 11, 257, 2856, 689, 445, 411, 294, 16634, 1601, 2856, 11, 321, 362, 341, 28270, 2856, 51228], "temperature": 0.0, "avg_logprob": -0.09678762555122375, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.0027612631674855947}, {"id": 541, "seek": 307384, "start": 3091.1200000000003, "end": 3098.1600000000003, "text": " that describes many aspects of the world. I mean, we might have all sorts of different", "tokens": [51228, 300, 15626, 867, 7270, 295, 264, 1002, 13, 286, 914, 11, 321, 1062, 362, 439, 7527, 295, 819, 51580], "temperature": 0.0, "avg_logprob": -0.09678762555122375, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.0027612631674855947}, {"id": 542, "seek": 309816, "start": 3099.12, "end": 3108.8799999999997, "text": " sort of categories of thing that we describe in our language. And the question is, can we kind", "tokens": [50412, 1333, 295, 10479, 295, 551, 300, 321, 6786, 294, 527, 2856, 13, 400, 264, 1168, 307, 11, 393, 321, 733, 50900], "temperature": 0.0, "avg_logprob": -0.12421086716325316, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.002299657789990306}, {"id": 543, "seek": 309816, "start": 3108.8799999999997, "end": 3120.0, "text": " of describe all, can we describe sort of things that come up in everyday language? Can we describe", "tokens": [50900, 295, 6786, 439, 11, 393, 321, 6786, 1333, 295, 721, 300, 808, 493, 294, 7429, 2856, 30, 1664, 321, 6786, 51456], "temperature": 0.0, "avg_logprob": -0.12421086716325316, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.002299657789990306}, {"id": 544, "seek": 309816, "start": 3120.0, "end": 3125.2, "text": " those kinds of things in a sort of precise symbolic way? And I have to say that I can't say I've got", "tokens": [51456, 729, 3685, 295, 721, 294, 257, 1333, 295, 13600, 25755, 636, 30, 400, 286, 362, 281, 584, 300, 286, 393, 380, 584, 286, 600, 658, 51716], "temperature": 0.0, "avg_logprob": -0.12421086716325316, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.002299657789990306}, {"id": 545, "seek": 312520, "start": 3125.2, "end": 3129.12, "text": " the full answer to that, but it's going really well. And it's become clear, and by the way,", "tokens": [50364, 264, 1577, 1867, 281, 300, 11, 457, 309, 311, 516, 534, 731, 13, 400, 309, 311, 1813, 1850, 11, 293, 538, 264, 636, 11, 50560], "temperature": 0.0, "avg_logprob": -0.05538073156633948, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.02275773510336876}, {"id": 546, "seek": 312520, "start": 3129.12, "end": 3136.16, "text": " LLMs are quite helpful in this, to having a way to take something not the level of language where", "tokens": [50560, 441, 43, 26386, 366, 1596, 4961, 294, 341, 11, 281, 1419, 257, 636, 281, 747, 746, 406, 264, 1496, 295, 2856, 689, 50912], "temperature": 0.0, "avg_logprob": -0.05538073156633948, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.02275773510336876}, {"id": 547, "seek": 312520, "start": 3136.16, "end": 3140.64, "text": " we're actually putting words together, but the representation of the core meaning of what's", "tokens": [50912, 321, 434, 767, 3372, 2283, 1214, 11, 457, 264, 10290, 295, 264, 4965, 3620, 295, 437, 311, 51136], "temperature": 0.0, "avg_logprob": -0.05538073156633948, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.02275773510336876}, {"id": 548, "seek": 312520, "start": 3140.64, "end": 3146.72, "text": " going on. Just like in our computational language, we have that representation of sort of the core", "tokens": [51136, 516, 322, 13, 1449, 411, 294, 527, 28270, 2856, 11, 321, 362, 300, 10290, 295, 1333, 295, 264, 4965, 51440], "temperature": 0.0, "avg_logprob": -0.05538073156633948, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.02275773510336876}, {"id": 549, "seek": 312520, "start": 3146.72, "end": 3151.52, "text": " meaning of what's going on in a way that can be read by humans, but also executed by a computer.", "tokens": [51440, 3620, 295, 437, 311, 516, 322, 294, 257, 636, 300, 393, 312, 1401, 538, 6255, 11, 457, 611, 17577, 538, 257, 3820, 13, 51680], "temperature": 0.0, "avg_logprob": -0.05538073156633948, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.02275773510336876}, {"id": 550, "seek": 315152, "start": 3152.32, "end": 3157.6, "text": " So in any case, that's sort of one direction about things with LLMs and so on. Another question", "tokens": [50404, 407, 294, 604, 1389, 11, 300, 311, 1333, 295, 472, 3513, 466, 721, 365, 441, 43, 26386, 293, 370, 322, 13, 3996, 1168, 50668], "temperature": 0.0, "avg_logprob": -0.08272496449578669, "compression_ratio": 1.5870445344129556, "no_speech_prob": 0.0045793987810611725}, {"id": 551, "seek": 315152, "start": 3157.6, "end": 3164.4, "text": " that I was curious about is, okay, why does machine learning work at all? Why is it the case that you", "tokens": [50668, 300, 286, 390, 6369, 466, 307, 11, 1392, 11, 983, 775, 3479, 2539, 589, 412, 439, 30, 1545, 307, 309, 264, 1389, 300, 291, 51008], "temperature": 0.0, "avg_logprob": -0.08272496449578669, "compression_ratio": 1.5870445344129556, "no_speech_prob": 0.0045793987810611725}, {"id": 552, "seek": 315152, "start": 3164.4, "end": 3171.36, "text": " can train one of these neural nets to do something like, I don't know, recognize digits or recognize", "tokens": [51008, 393, 3847, 472, 295, 613, 18161, 36170, 281, 360, 746, 411, 11, 286, 500, 380, 458, 11, 5521, 27011, 420, 5521, 51356], "temperature": 0.0, "avg_logprob": -0.08272496449578669, "compression_ratio": 1.5870445344129556, "no_speech_prob": 0.0045793987810611725}, {"id": 553, "seek": 315152, "start": 3171.36, "end": 3177.68, "text": " cats and dogs or generate language or whatever else? Why does that work? When I played around", "tokens": [51356, 11111, 293, 7197, 420, 8460, 2856, 420, 2035, 1646, 30, 1545, 775, 300, 589, 30, 1133, 286, 3737, 926, 51672], "temperature": 0.0, "avg_logprob": -0.08272496449578669, "compression_ratio": 1.5870445344129556, "no_speech_prob": 0.0045793987810611725}, {"id": 554, "seek": 317768, "start": 3177.68, "end": 3183.44, "text": " with neural nets back in 1981, and I couldn't get them to do anything interesting. And I kind of", "tokens": [50364, 365, 18161, 36170, 646, 294, 33117, 11, 293, 286, 2809, 380, 483, 552, 281, 360, 1340, 1880, 13, 400, 286, 733, 295, 50652], "temperature": 0.0, "avg_logprob": -0.07420971393585205, "compression_ratio": 1.7214285714285715, "no_speech_prob": 0.021151812747120857}, {"id": 555, "seek": 317768, "start": 3183.44, "end": 3186.96, "text": " thought at the time, oh, if I've got a simple enough problem, I'll be able to get a simple neural", "tokens": [50652, 1194, 412, 264, 565, 11, 1954, 11, 498, 286, 600, 658, 257, 2199, 1547, 1154, 11, 286, 603, 312, 1075, 281, 483, 257, 2199, 18161, 50828], "temperature": 0.0, "avg_logprob": -0.07420971393585205, "compression_ratio": 1.7214285714285715, "no_speech_prob": 0.021151812747120857}, {"id": 556, "seek": 317768, "start": 3186.96, "end": 3193.2, "text": " net to do things, didn't really work very well, wasn't very interesting. The big thing that got", "tokens": [50828, 2533, 281, 360, 721, 11, 994, 380, 534, 589, 588, 731, 11, 2067, 380, 588, 1880, 13, 440, 955, 551, 300, 658, 51140], "temperature": 0.0, "avg_logprob": -0.07420971393585205, "compression_ratio": 1.7214285714285715, "no_speech_prob": 0.021151812747120857}, {"id": 557, "seek": 317768, "start": 3193.2, "end": 3199.6, "text": " sort of accidentally discovered basically in 2011 was that if you have a big neural net and you", "tokens": [51140, 1333, 295, 15715, 6941, 1936, 294, 10154, 390, 300, 498, 291, 362, 257, 955, 18161, 2533, 293, 291, 51460], "temperature": 0.0, "avg_logprob": -0.07420971393585205, "compression_ratio": 1.7214285714285715, "no_speech_prob": 0.021151812747120857}, {"id": 558, "seek": 317768, "start": 3199.6, "end": 3205.2, "text": " bash it really hard, you show it enough training examples, it'll learn, well, lots of different", "tokens": [51460, 46183, 309, 534, 1152, 11, 291, 855, 309, 1547, 3097, 5110, 11, 309, 603, 1466, 11, 731, 11, 3195, 295, 819, 51740], "temperature": 0.0, "avg_logprob": -0.07420971393585205, "compression_ratio": 1.7214285714285715, "no_speech_prob": 0.021151812747120857}, {"id": 559, "seek": 320520, "start": 3205.2799999999997, "end": 3210.3199999999997, "text": " kinds of things, it'll learn almost anything. And the kind of the big meta discovery of modern", "tokens": [50368, 3685, 295, 721, 11, 309, 603, 1466, 1920, 1340, 13, 400, 264, 733, 295, 264, 955, 19616, 12114, 295, 4363, 50620], "temperature": 0.0, "avg_logprob": -0.06539397544049202, "compression_ratio": 1.9042904290429044, "no_speech_prob": 0.027294429019093513}, {"id": 560, "seek": 320520, "start": 3210.3199999999997, "end": 3215.2799999999997, "text": " machine learning is that if you bash a neural net hard enough, it'll learn almost anything. We don't", "tokens": [50620, 3479, 2539, 307, 300, 498, 291, 46183, 257, 18161, 2533, 1152, 1547, 11, 309, 603, 1466, 1920, 1340, 13, 492, 500, 380, 50868], "temperature": 0.0, "avg_logprob": -0.06539397544049202, "compression_ratio": 1.9042904290429044, "no_speech_prob": 0.027294429019093513}, {"id": 561, "seek": 320520, "start": 3215.2799999999997, "end": 3219.9199999999996, "text": " know quite what the almost is. We can't really characterize what kind of thing it can learn.", "tokens": [50868, 458, 1596, 437, 264, 1920, 307, 13, 492, 393, 380, 534, 38463, 437, 733, 295, 551, 309, 393, 1466, 13, 51100], "temperature": 0.0, "avg_logprob": -0.06539397544049202, "compression_ratio": 1.9042904290429044, "no_speech_prob": 0.027294429019093513}, {"id": 562, "seek": 320520, "start": 3219.9199999999996, "end": 3225.2799999999997, "text": " For example, as I said, it can't break out of computational irreducibility. So there's limitations", "tokens": [51100, 1171, 1365, 11, 382, 286, 848, 11, 309, 393, 380, 1821, 484, 295, 28270, 16014, 769, 537, 39802, 13, 407, 456, 311, 15705, 51368], "temperature": 0.0, "avg_logprob": -0.06539397544049202, "compression_ratio": 1.9042904290429044, "no_speech_prob": 0.027294429019093513}, {"id": 563, "seek": 320520, "start": 3225.2799999999997, "end": 3229.68, "text": " to what it can learn, what it can do. But nevertheless, there's a broad class of things that seem to", "tokens": [51368, 281, 437, 309, 393, 1466, 11, 437, 309, 393, 360, 13, 583, 26924, 11, 456, 311, 257, 4152, 1508, 295, 721, 300, 1643, 281, 51588], "temperature": 0.0, "avg_logprob": -0.06539397544049202, "compression_ratio": 1.9042904290429044, "no_speech_prob": 0.027294429019093513}, {"id": 564, "seek": 320520, "start": 3229.68, "end": 3234.48, "text": " correspond a lot to kinds of things that we humans can do easily that the neural net can", "tokens": [51588, 6805, 257, 688, 281, 3685, 295, 721, 300, 321, 6255, 393, 360, 3612, 300, 264, 18161, 2533, 393, 51828], "temperature": 0.0, "avg_logprob": -0.06539397544049202, "compression_ratio": 1.9042904290429044, "no_speech_prob": 0.027294429019093513}, {"id": 565, "seek": 323448, "start": 3234.48, "end": 3243.52, "text": " successfully do. And so that's sort of the meta discovery. The question is, why does that work?", "tokens": [50364, 10727, 360, 13, 400, 370, 300, 311, 1333, 295, 264, 19616, 12114, 13, 440, 1168, 307, 11, 983, 775, 300, 589, 30, 50816], "temperature": 0.0, "avg_logprob": -0.07791056156158448, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0038546761497855186}, {"id": 566, "seek": 323448, "start": 3244.2400000000002, "end": 3249.6, "text": " Why is it the case that this neural net can be successfully sort of bashed into learning", "tokens": [50852, 1545, 307, 309, 264, 1389, 300, 341, 18161, 2533, 393, 312, 10727, 1333, 295, 987, 27096, 666, 2539, 51120], "temperature": 0.0, "avg_logprob": -0.07791056156158448, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0038546761497855186}, {"id": 567, "seek": 323448, "start": 3249.6, "end": 3254.88, "text": " things? Why doesn't it get stuck? Why doesn't it get to the point where you just can't get there", "tokens": [51120, 721, 30, 1545, 1177, 380, 309, 483, 5541, 30, 1545, 1177, 380, 309, 483, 281, 264, 935, 689, 291, 445, 393, 380, 483, 456, 51384], "temperature": 0.0, "avg_logprob": -0.07791056156158448, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0038546761497855186}, {"id": 568, "seek": 323448, "start": 3254.88, "end": 3260.08, "text": " from here? You can't arrange it. Why is it the case that it's possible to do it and then why is it", "tokens": [51384, 490, 510, 30, 509, 393, 380, 9424, 309, 13, 1545, 307, 309, 264, 1389, 300, 309, 311, 1944, 281, 360, 309, 293, 550, 983, 307, 309, 51644], "temperature": 0.0, "avg_logprob": -0.07791056156158448, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0038546761497855186}, {"id": 569, "seek": 326008, "start": 3260.08, "end": 3267.2799999999997, "text": " the case that you can iteratively do it by sort of adaptively training it? I got interested in", "tokens": [50364, 264, 1389, 300, 291, 393, 17138, 19020, 360, 309, 538, 1333, 295, 6231, 3413, 3097, 309, 30, 286, 658, 3102, 294, 50724], "temperature": 0.0, "avg_logprob": -0.0866861629486084, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.005365453194826841}, {"id": 570, "seek": 326008, "start": 3267.2799999999997, "end": 3272.4, "text": " this very recently, actually. And I don't know whether I can show you pictures. Let me see. I can", "tokens": [50724, 341, 588, 3938, 11, 767, 13, 400, 286, 500, 380, 458, 1968, 286, 393, 855, 291, 5242, 13, 961, 385, 536, 13, 286, 393, 50980], "temperature": 0.0, "avg_logprob": -0.0866861629486084, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.005365453194826841}, {"id": 571, "seek": 326008, "start": 3272.4, "end": 3276.56, "text": " show you some things that I did recently. And then maybe I'll be able to pull up some pictures", "tokens": [50980, 855, 291, 512, 721, 300, 286, 630, 3938, 13, 400, 550, 1310, 286, 603, 312, 1075, 281, 2235, 493, 512, 5242, 51188], "temperature": 0.0, "avg_logprob": -0.0866861629486084, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.005365453194826841}, {"id": 572, "seek": 326008, "start": 3276.56, "end": 3286.56, "text": " just from the last few days that let me see here. Right. So actually, I decided to look at a simpler", "tokens": [51188, 445, 490, 264, 1036, 1326, 1708, 300, 718, 385, 536, 510, 13, 1779, 13, 407, 767, 11, 286, 3047, 281, 574, 412, 257, 18587, 51688], "temperature": 0.0, "avg_logprob": -0.0866861629486084, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.005365453194826841}, {"id": 573, "seek": 328656, "start": 3286.56, "end": 3292.0, "text": " problem, which is the problem of biological evolution, which is sort of another case of adaptation", "tokens": [50364, 1154, 11, 597, 307, 264, 1154, 295, 13910, 9303, 11, 597, 307, 1333, 295, 1071, 1389, 295, 21549, 50636], "temperature": 0.0, "avg_logprob": -0.1086327080587739, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.012509681284427643}, {"id": 574, "seek": 328656, "start": 3292.7999999999997, "end": 3297.92, "text": " that is a little simpler than neural nets. But let me explain what I figured out about", "tokens": [50676, 300, 307, 257, 707, 18587, 813, 18161, 36170, 13, 583, 718, 385, 2903, 437, 286, 8932, 484, 466, 50932], "temperature": 0.0, "avg_logprob": -0.1086327080587739, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.012509681284427643}, {"id": 575, "seek": 328656, "start": 3297.92, "end": 3302.08, "text": " biological evolution. For a long time, I wondered what sort of the minimal model of biological", "tokens": [50932, 13910, 9303, 13, 1171, 257, 938, 565, 11, 286, 17055, 437, 1333, 295, 264, 13206, 2316, 295, 13910, 51140], "temperature": 0.0, "avg_logprob": -0.1086327080587739, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.012509681284427643}, {"id": 576, "seek": 328656, "start": 3302.08, "end": 3306.7999999999997, "text": " evolution was always very unsatisfied because models, you know, natural selection seems like a", "tokens": [51140, 9303, 390, 1009, 588, 2693, 38502, 570, 5245, 11, 291, 458, 11, 3303, 9450, 2544, 411, 257, 51376], "temperature": 0.0, "avg_logprob": -0.1086327080587739, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.012509681284427643}, {"id": 577, "seek": 328656, "start": 3306.7999999999997, "end": 3311.12, "text": " simple principle. But when you actually try and make explicit models for it, you end up with all", "tokens": [51376, 2199, 8665, 13, 583, 562, 291, 767, 853, 293, 652, 13691, 5245, 337, 309, 11, 291, 917, 493, 365, 439, 51592], "temperature": 0.0, "avg_logprob": -0.1086327080587739, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.012509681284427643}, {"id": 578, "seek": 331112, "start": 3311.12, "end": 3316.72, "text": " kinds of hair about, you know, how many sub, you know, suboptimal organisms do you keep and all", "tokens": [50364, 3685, 295, 2578, 466, 11, 291, 458, 11, 577, 867, 1422, 11, 291, 458, 11, 1422, 5747, 10650, 22110, 360, 291, 1066, 293, 439, 50644], "temperature": 0.0, "avg_logprob": -0.08886908709518308, "compression_ratio": 1.7717391304347827, "no_speech_prob": 0.02252117358148098}, {"id": 579, "seek": 331112, "start": 3316.72, "end": 3320.7999999999997, "text": " this kind of thing. So I was interested in sort of a minimal version of that. So here's a version", "tokens": [50644, 341, 733, 295, 551, 13, 407, 286, 390, 3102, 294, 1333, 295, 257, 13206, 3037, 295, 300, 13, 407, 510, 311, 257, 3037, 50848], "temperature": 0.0, "avg_logprob": -0.08886908709518308, "compression_ratio": 1.7717391304347827, "no_speech_prob": 0.02252117358148098}, {"id": 580, "seek": 331112, "start": 3320.7999999999997, "end": 3325.44, "text": " of that. This is actually one of these cellular automata. It's got these rules here, starts off", "tokens": [50848, 295, 300, 13, 639, 307, 767, 472, 295, 613, 29267, 3553, 3274, 13, 467, 311, 658, 613, 4474, 510, 11, 3719, 766, 51080], "temperature": 0.0, "avg_logprob": -0.08886908709518308, "compression_ratio": 1.7717391304347827, "no_speech_prob": 0.02252117358148098}, {"id": 581, "seek": 331112, "start": 3325.44, "end": 3331.04, "text": " from one red cell here. And with these particular rules, you get a pattern that lives for this amount", "tokens": [51080, 490, 472, 2182, 2815, 510, 13, 400, 365, 613, 1729, 4474, 11, 291, 483, 257, 5102, 300, 2909, 337, 341, 2372, 51360], "temperature": 0.0, "avg_logprob": -0.08886908709518308, "compression_ratio": 1.7717391304347827, "no_speech_prob": 0.02252117358148098}, {"id": 582, "seek": 331112, "start": 3331.04, "end": 3339.52, "text": " of time and then dies out. Okay, so let's imagine that you're interested in doing something where", "tokens": [51360, 295, 565, 293, 550, 2714, 484, 13, 1033, 11, 370, 718, 311, 3811, 300, 291, 434, 3102, 294, 884, 746, 689, 51784], "temperature": 0.0, "avg_logprob": -0.08886908709518308, "compression_ratio": 1.7717391304347827, "no_speech_prob": 0.02252117358148098}, {"id": 583, "seek": 333952, "start": 3339.6, "end": 3344.96, "text": " you just keep on tweaking the rules, you keep on resetting the rules, you keep on making", "tokens": [50368, 291, 445, 1066, 322, 6986, 2456, 264, 4474, 11, 291, 1066, 322, 14322, 783, 264, 4474, 11, 291, 1066, 322, 1455, 50636], "temperature": 0.0, "avg_logprob": -0.0872096585797834, "compression_ratio": 1.7944664031620554, "no_speech_prob": 0.005860323552042246}, {"id": 584, "seek": 333952, "start": 3344.96, "end": 3350.88, "text": " single point mutations in the rules to try and get it to live longer and longer. This is what", "tokens": [50636, 2167, 935, 29243, 294, 264, 4474, 281, 853, 293, 483, 309, 281, 1621, 2854, 293, 2854, 13, 639, 307, 437, 50932], "temperature": 0.0, "avg_logprob": -0.0872096585797834, "compression_ratio": 1.7944664031620554, "no_speech_prob": 0.005860323552042246}, {"id": 585, "seek": 333952, "start": 3350.88, "end": 3356.56, "text": " happens. You start off from something that that is just a blank rule. For example, it dies immediately,", "tokens": [50932, 2314, 13, 509, 722, 766, 490, 746, 300, 300, 307, 445, 257, 8247, 4978, 13, 1171, 1365, 11, 309, 2714, 4258, 11, 51216], "temperature": 0.0, "avg_logprob": -0.0872096585797834, "compression_ratio": 1.7944664031620554, "no_speech_prob": 0.005860323552042246}, {"id": 586, "seek": 333952, "start": 3356.56, "end": 3360.72, "text": " you keep tweaking the rule, you have to go through many different tweaks and so on. But", "tokens": [51216, 291, 1066, 6986, 2456, 264, 4978, 11, 291, 362, 281, 352, 807, 867, 819, 46664, 293, 370, 322, 13, 583, 51424], "temperature": 0.0, "avg_logprob": -0.0872096585797834, "compression_ratio": 1.7944664031620554, "no_speech_prob": 0.005860323552042246}, {"id": 587, "seek": 333952, "start": 3360.72, "end": 3364.16, "text": " eventually you'll get to the point where it lives there for 50 something steps.", "tokens": [51424, 4728, 291, 603, 483, 281, 264, 935, 689, 309, 2909, 456, 337, 2625, 746, 4439, 13, 51596], "temperature": 0.0, "avg_logprob": -0.0872096585797834, "compression_ratio": 1.7944664031620554, "no_speech_prob": 0.005860323552042246}, {"id": 588, "seek": 336416, "start": 3364.72, "end": 3368.96, "text": " Well, and you can see that the sequence of mutations that got made there. And if you look at how the", "tokens": [50392, 1042, 11, 293, 291, 393, 536, 300, 264, 8310, 295, 29243, 300, 658, 1027, 456, 13, 400, 498, 291, 574, 412, 577, 264, 50604], "temperature": 0.0, "avg_logprob": -0.17461454672891585, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.00956484954804182}, {"id": 589, "seek": 336416, "start": 3368.96, "end": 3374.7999999999997, "text": " fitness of this organism, the length of time it lived, varies as you go through all these", "tokens": [50604, 15303, 295, 341, 24128, 11, 264, 4641, 295, 565, 309, 5152, 11, 21716, 382, 291, 352, 807, 439, 613, 50896], "temperature": 0.0, "avg_logprob": -0.17461454672891585, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.00956484954804182}, {"id": 590, "seek": 336416, "start": 3374.7999999999997, "end": 3380.72, "text": " different sort of steps of adaptive evolution, you'll see there are, you know, it's going along", "tokens": [50896, 819, 1333, 295, 4439, 295, 27912, 9303, 11, 291, 603, 536, 456, 366, 11, 291, 458, 11, 309, 311, 516, 2051, 51192], "temperature": 0.0, "avg_logprob": -0.17461454672891585, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.00956484954804182}, {"id": 591, "seek": 336416, "start": 3380.72, "end": 3386.3999999999996, "text": " and there are many things that don't work out. But, you know, it'll kind of cruise along here at a", "tokens": [51192, 293, 456, 366, 867, 721, 300, 500, 380, 589, 484, 13, 583, 11, 291, 458, 11, 309, 603, 733, 295, 17754, 2051, 510, 412, 257, 51476], "temperature": 0.0, "avg_logprob": -0.17461454672891585, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.00956484954804182}, {"id": 592, "seek": 336416, "start": 3386.3999999999996, "end": 3391.6, "text": " certain fitness, and then it makes a discovery. And then it can go to higher fitness. And actually,", "tokens": [51476, 1629, 15303, 11, 293, 550, 309, 1669, 257, 12114, 13, 400, 550, 309, 393, 352, 281, 2946, 15303, 13, 400, 767, 11, 51736], "temperature": 0.0, "avg_logprob": -0.17461454672891585, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.00956484954804182}, {"id": 593, "seek": 339160, "start": 3392.56, "end": 3396.48, "text": " you can end up with all kinds of discoveries that it makes. These are different sort of", "tokens": [50412, 291, 393, 917, 493, 365, 439, 3685, 295, 28400, 300, 309, 1669, 13, 1981, 366, 819, 1333, 295, 50608], "temperature": 0.0, "avg_logprob": -0.11152210556158498, "compression_ratio": 1.6714801444043321, "no_speech_prob": 0.0030158048029989004}, {"id": 594, "seek": 339160, "start": 3396.48, "end": 3401.2, "text": " paths of evolution. And you'll see that, for example, here, it's kind of going along and", "tokens": [50608, 14518, 295, 9303, 13, 400, 291, 603, 536, 300, 11, 337, 1365, 11, 510, 11, 309, 311, 733, 295, 516, 2051, 293, 50844], "temperature": 0.0, "avg_logprob": -0.11152210556158498, "compression_ratio": 1.6714801444043321, "no_speech_prob": 0.0030158048029989004}, {"id": 595, "seek": 339160, "start": 3401.2, "end": 3405.92, "text": " eventually it manages to discover a lot. It manages to live a long time. You could sort of imagine", "tokens": [50844, 4728, 309, 22489, 281, 4411, 257, 688, 13, 467, 22489, 281, 1621, 257, 938, 565, 13, 509, 727, 1333, 295, 3811, 51080], "temperature": 0.0, "avg_logprob": -0.11152210556158498, "compression_ratio": 1.6714801444043321, "no_speech_prob": 0.0030158048029989004}, {"id": 596, "seek": 339160, "start": 3405.92, "end": 3411.7599999999998, "text": " in the fossil record, you might find, you know, a critter from the Cambrian period that looks", "tokens": [51080, 294, 264, 18737, 2136, 11, 291, 1062, 915, 11, 291, 458, 11, 257, 3113, 391, 490, 264, 29287, 5501, 2896, 300, 1542, 51372], "temperature": 0.0, "avg_logprob": -0.11152210556158498, "compression_ratio": 1.6714801444043321, "no_speech_prob": 0.0030158048029989004}, {"id": 597, "seek": 339160, "start": 3411.7599999999998, "end": 3417.2799999999997, "text": " like this. And then it uses that idea to extend further. And by the time it's in the Silurian", "tokens": [51372, 411, 341, 13, 400, 550, 309, 4960, 300, 1558, 281, 10101, 3052, 13, 400, 538, 264, 565, 309, 311, 294, 264, 6943, 374, 952, 51648], "temperature": 0.0, "avg_logprob": -0.11152210556158498, "compression_ratio": 1.6714801444043321, "no_speech_prob": 0.0030158048029989004}, {"id": 598, "seek": 341728, "start": 3417.36, "end": 3422.5600000000004, "text": " period, it's looking like this. And maybe it makes it to this in the Triassic period or something.", "tokens": [50368, 2896, 11, 309, 311, 1237, 411, 341, 13, 400, 1310, 309, 1669, 309, 281, 341, 294, 264, 10931, 35685, 2896, 420, 746, 13, 50628], "temperature": 0.0, "avg_logprob": -0.13472723960876465, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.0272367000579834}, {"id": 599, "seek": 341728, "start": 3422.5600000000004, "end": 3427.84, "text": " But what's happening here is that there are sort of, it's having progressively more ideas,", "tokens": [50628, 583, 437, 311, 2737, 510, 307, 300, 456, 366, 1333, 295, 11, 309, 311, 1419, 46667, 544, 3487, 11, 50892], "temperature": 0.0, "avg_logprob": -0.13472723960876465, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.0272367000579834}, {"id": 600, "seek": 341728, "start": 3427.84, "end": 3433.76, "text": " in a sense, about how to live longer in this particular case. And actually, you can even", "tokens": [50892, 294, 257, 2020, 11, 466, 577, 281, 1621, 2854, 294, 341, 1729, 1389, 13, 400, 767, 11, 291, 393, 754, 51188], "temperature": 0.0, "avg_logprob": -0.13472723960876465, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.0272367000579834}, {"id": 601, "seek": 341728, "start": 3433.76, "end": 3438.96, "text": " go ahead, this is a simple enough system, that you can actually work out. Let's see, that's an", "tokens": [51188, 352, 2286, 11, 341, 307, 257, 2199, 1547, 1185, 11, 300, 291, 393, 767, 589, 484, 13, 961, 311, 536, 11, 300, 311, 364, 51448], "temperature": 0.0, "avg_logprob": -0.13472723960876465, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.0272367000579834}, {"id": 602, "seek": 341728, "start": 3438.96, "end": 3444.96, "text": " example of a better example. There we go. This is, this is the path of all possible paths of", "tokens": [51448, 1365, 295, 257, 1101, 1365, 13, 821, 321, 352, 13, 639, 307, 11, 341, 307, 264, 3100, 295, 439, 1944, 14518, 295, 51748], "temperature": 0.0, "avg_logprob": -0.13472723960876465, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.0272367000579834}, {"id": 603, "seek": 344496, "start": 3444.96, "end": 3451.76, "text": " evolution for a simple system like this. So every different picture here is a possible organism.", "tokens": [50364, 9303, 337, 257, 2199, 1185, 411, 341, 13, 407, 633, 819, 3036, 510, 307, 257, 1944, 24128, 13, 50704], "temperature": 0.0, "avg_logprob": -0.07615647145680018, "compression_ratio": 1.8282442748091603, "no_speech_prob": 0.019217435270547867}, {"id": 604, "seek": 344496, "start": 3451.76, "end": 3456.48, "text": " And the arrows show the possible adaptation paths. And what you see is something that's very much", "tokens": [50704, 400, 264, 19669, 855, 264, 1944, 21549, 14518, 13, 400, 437, 291, 536, 307, 746, 300, 311, 588, 709, 50940], "temperature": 0.0, "avg_logprob": -0.07615647145680018, "compression_ratio": 1.8282442748091603, "no_speech_prob": 0.019217435270547867}, {"id": 605, "seek": 344496, "start": 3456.48, "end": 3461.12, "text": " like what happens in biological evolution. There are different branches in the tree of life.", "tokens": [50940, 411, 437, 2314, 294, 13910, 9303, 13, 821, 366, 819, 14770, 294, 264, 4230, 295, 993, 13, 51172], "temperature": 0.0, "avg_logprob": -0.07615647145680018, "compression_ratio": 1.8282442748091603, "no_speech_prob": 0.019217435270547867}, {"id": 606, "seek": 344496, "start": 3461.12, "end": 3466.32, "text": " There are, you know, one set of ideas leads to long life over here. In this way, a different", "tokens": [51172, 821, 366, 11, 291, 458, 11, 472, 992, 295, 3487, 6689, 281, 938, 993, 670, 510, 13, 682, 341, 636, 11, 257, 819, 51432], "temperature": 0.0, "avg_logprob": -0.07615647145680018, "compression_ratio": 1.8282442748091603, "no_speech_prob": 0.019217435270547867}, {"id": 607, "seek": 344496, "start": 3466.32, "end": 3471.44, "text": " set of ideas leads to kind of long life over here in a different way. Okay, what does this have to", "tokens": [51432, 992, 295, 3487, 6689, 281, 733, 295, 938, 993, 670, 510, 294, 257, 819, 636, 13, 1033, 11, 437, 775, 341, 362, 281, 51688], "temperature": 0.0, "avg_logprob": -0.07615647145680018, "compression_ratio": 1.8282442748091603, "no_speech_prob": 0.019217435270547867}, {"id": 608, "seek": 347144, "start": 3471.52, "end": 3478.0, "text": " do with machine learning? Well, you can, you can ask the question, let me see if I can pull this up.", "tokens": [50368, 360, 365, 3479, 2539, 30, 1042, 11, 291, 393, 11, 291, 393, 1029, 264, 1168, 11, 718, 385, 536, 498, 286, 393, 2235, 341, 493, 13, 50692], "temperature": 0.0, "avg_logprob": -0.1379936006334093, "compression_ratio": 1.5685279187817258, "no_speech_prob": 0.012947507202625275}, {"id": 609, "seek": 347144, "start": 3478.7200000000003, "end": 3484.16, "text": " I am going to have to pull up something that I just made. So I'm not sure whether I can find it here.", "tokens": [50728, 286, 669, 516, 281, 362, 281, 2235, 493, 746, 300, 286, 445, 1027, 13, 407, 286, 478, 406, 988, 1968, 286, 393, 915, 309, 510, 13, 51000], "temperature": 0.0, "avg_logprob": -0.1379936006334093, "compression_ratio": 1.5685279187817258, "no_speech_prob": 0.012947507202625275}, {"id": 610, "seek": 347144, "start": 3485.04, "end": 3490.64, "text": " Hold on, you can get hot off the press or not really off the press at all. Where is it?", "tokens": [51044, 6962, 322, 11, 291, 393, 483, 2368, 766, 264, 1886, 420, 406, 534, 766, 264, 1886, 412, 439, 13, 2305, 307, 309, 30, 51324], "temperature": 0.0, "avg_logprob": -0.1379936006334093, "compression_ratio": 1.5685279187817258, "no_speech_prob": 0.012947507202625275}, {"id": 611, "seek": 347144, "start": 3493.52, "end": 3495.52, "text": " Let me see. Maybe.", "tokens": [51468, 961, 385, 536, 13, 2704, 13, 51568], "temperature": 0.0, "avg_logprob": -0.1379936006334093, "compression_ratio": 1.5685279187817258, "no_speech_prob": 0.012947507202625275}, {"id": 612, "seek": 350144, "start": 3502.4, "end": 3506.08, "text": " Maybe this will have it. Oh yeah, this is, this might be it.", "tokens": [50412, 2704, 341, 486, 362, 309, 13, 876, 1338, 11, 341, 307, 11, 341, 1062, 312, 309, 13, 50596], "temperature": 0.0, "avg_logprob": -0.18070882326596743, "compression_ratio": 1.5625, "no_speech_prob": 0.002753891283646226}, {"id": 613, "seek": 350144, "start": 3509.84, "end": 3512.56, "text": " This is a very minimal model for,", "tokens": [50784, 639, 307, 257, 588, 13206, 2316, 337, 11, 50920], "temperature": 0.0, "avg_logprob": -0.18070882326596743, "compression_ratio": 1.5625, "no_speech_prob": 0.002753891283646226}, {"id": 614, "seek": 350144, "start": 3516.2400000000002, "end": 3524.2400000000002, "text": " let's see if I can get this bigger. It's a very minimal model for a neural net where", "tokens": [51104, 718, 311, 536, 498, 286, 393, 483, 341, 3801, 13, 467, 311, 257, 588, 13206, 2316, 337, 257, 18161, 2533, 689, 51504], "temperature": 0.0, "avg_logprob": -0.18070882326596743, "compression_ratio": 1.5625, "no_speech_prob": 0.002753891283646226}, {"id": 615, "seek": 350144, "start": 3524.2400000000002, "end": 3530.48, "text": " it's actually a cellular automaton as well. But instead of having a fixed rule that it keeps on", "tokens": [51504, 309, 311, 767, 257, 29267, 3553, 25781, 382, 731, 13, 583, 2602, 295, 1419, 257, 6806, 4978, 300, 309, 5965, 322, 51816], "temperature": 0.0, "avg_logprob": -0.18070882326596743, "compression_ratio": 1.5625, "no_speech_prob": 0.002753891283646226}, {"id": 616, "seek": 353048, "start": 3530.48, "end": 3535.6, "text": " applying kind of like a recurrent neural network, it has something more like a feed forward neural", "tokens": [50364, 9275, 733, 295, 411, 257, 18680, 1753, 18161, 3209, 11, 309, 575, 746, 544, 411, 257, 3154, 2128, 18161, 50620], "temperature": 0.0, "avg_logprob": -0.09348794033652857, "compression_ratio": 1.7765567765567765, "no_speech_prob": 0.01658003404736519}, {"id": 617, "seek": 353048, "start": 3535.6, "end": 3541.36, "text": " network where you have a discrete choice of one of let's say two different possible rules. And at", "tokens": [50620, 3209, 689, 291, 362, 257, 27706, 3922, 295, 472, 295, 718, 311, 584, 732, 819, 1944, 4474, 13, 400, 412, 50908], "temperature": 0.0, "avg_logprob": -0.09348794033652857, "compression_ratio": 1.7765567765567765, "no_speech_prob": 0.01658003404736519}, {"id": 618, "seek": 353048, "start": 3541.36, "end": 3546.2400000000002, "text": " every point in space time, so to speak, you're picking a different rule. And so then the learning", "tokens": [50908, 633, 935, 294, 1901, 565, 11, 370, 281, 1710, 11, 291, 434, 8867, 257, 819, 4978, 13, 400, 370, 550, 264, 2539, 51152], "temperature": 0.0, "avg_logprob": -0.09348794033652857, "compression_ratio": 1.7765567765567765, "no_speech_prob": 0.01658003404736519}, {"id": 619, "seek": 353048, "start": 3546.2400000000002, "end": 3550.72, "text": " consists of, well, what's the pattern of rules you should pick to get a particular outcome. In", "tokens": [51152, 14689, 295, 11, 731, 11, 437, 311, 264, 5102, 295, 4474, 291, 820, 1888, 281, 483, 257, 1729, 9700, 13, 682, 51376], "temperature": 0.0, "avg_logprob": -0.09348794033652857, "compression_ratio": 1.7765567765567765, "no_speech_prob": 0.01658003404736519}, {"id": 620, "seek": 353048, "start": 3550.72, "end": 3555.68, "text": " this particular case, we're trying to learn to live as long as possible. And what's interesting", "tokens": [51376, 341, 1729, 1389, 11, 321, 434, 1382, 281, 1466, 281, 1621, 382, 938, 382, 1944, 13, 400, 437, 311, 1880, 51624], "temperature": 0.0, "avg_logprob": -0.09348794033652857, "compression_ratio": 1.7765567765567765, "no_speech_prob": 0.01658003404736519}, {"id": 621, "seek": 355568, "start": 3555.7599999999998, "end": 3563.7599999999998, "text": " here, and again, this is just raw off the literally raw material that from a couple of days ago,", "tokens": [50368, 510, 11, 293, 797, 11, 341, 307, 445, 8936, 766, 264, 3736, 8936, 2527, 300, 490, 257, 1916, 295, 1708, 2057, 11, 50768], "temperature": 0.0, "avg_logprob": -0.11356238101391082, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.09605434536933899}, {"id": 622, "seek": 355568, "start": 3564.96, "end": 3571.8399999999997, "text": " this is kind of showing in a sense how the thing does what it does. So in a standard neural net,", "tokens": [50828, 341, 307, 733, 295, 4099, 294, 257, 2020, 577, 264, 551, 775, 437, 309, 775, 13, 407, 294, 257, 3832, 18161, 2533, 11, 51172], "temperature": 0.0, "avg_logprob": -0.11356238101391082, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.09605434536933899}, {"id": 623, "seek": 355568, "start": 3571.8399999999997, "end": 3576.08, "text": " it's just much more complicated to display what's going on. You've got these neurons with", "tokens": [51172, 309, 311, 445, 709, 544, 6179, 281, 4674, 437, 311, 516, 322, 13, 509, 600, 658, 613, 22027, 365, 51384], "temperature": 0.0, "avg_logprob": -0.11356238101391082, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.09605434536933899}, {"id": 624, "seek": 355568, "start": 3576.08, "end": 3581.44, "text": " continuous weights and you've got connectivity all over the place and so on. This is a much simpler", "tokens": [51384, 10957, 17443, 293, 291, 600, 658, 21095, 439, 670, 264, 1081, 293, 370, 322, 13, 639, 307, 257, 709, 18587, 51652], "temperature": 0.0, "avg_logprob": -0.11356238101391082, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.09605434536933899}, {"id": 625, "seek": 358144, "start": 3581.44, "end": 3585.68, "text": " case. So you can kind of see more about what's going on. What's non trivial is that training", "tokens": [50364, 1389, 13, 407, 291, 393, 733, 295, 536, 544, 466, 437, 311, 516, 322, 13, 708, 311, 2107, 26703, 307, 300, 3097, 50576], "temperature": 0.0, "avg_logprob": -0.12253347772066711, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.04754247888922691}, {"id": 626, "seek": 358144, "start": 3585.68, "end": 3591.12, "text": " actually works in this case. And it does, you can find this arrangement of bits that will cause", "tokens": [50576, 767, 1985, 294, 341, 1389, 13, 400, 309, 775, 11, 291, 393, 915, 341, 17620, 295, 9239, 300, 486, 3082, 50848], "temperature": 0.0, "avg_logprob": -0.12253347772066711, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.04754247888922691}, {"id": 627, "seek": 358144, "start": 3591.12, "end": 3596.16, "text": " the thing to do, I don't know whether I have it in this example here, but that will cause it to", "tokens": [50848, 264, 551, 281, 360, 11, 286, 500, 380, 458, 1968, 286, 362, 309, 294, 341, 1365, 510, 11, 457, 300, 486, 3082, 309, 281, 51100], "temperature": 0.0, "avg_logprob": -0.12253347772066711, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.04754247888922691}, {"id": 628, "seek": 358144, "start": 3596.16, "end": 3604.32, "text": " learn, see if I have one here. Now those activation levels, well, it doesn't matter, but that will", "tokens": [51100, 1466, 11, 536, 498, 286, 362, 472, 510, 13, 823, 729, 24433, 4358, 11, 731, 11, 309, 1177, 380, 1871, 11, 457, 300, 486, 51508], "temperature": 0.0, "avg_logprob": -0.12253347772066711, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.04754247888922691}, {"id": 629, "seek": 358144, "start": 3604.32, "end": 3610.88, "text": " basically cause it to learn something like, you know, to tell whether the number of bits at the", "tokens": [51508, 1936, 3082, 309, 281, 1466, 746, 411, 11, 291, 458, 11, 281, 980, 1968, 264, 1230, 295, 9239, 412, 264, 51836], "temperature": 0.0, "avg_logprob": -0.12253347772066711, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.04754247888922691}, {"id": 630, "seek": 361088, "start": 3610.88, "end": 3615.04, "text": " beginning is even or odd or something like this. And we actually even tried training this on the", "tokens": [50364, 2863, 307, 754, 420, 7401, 420, 746, 411, 341, 13, 400, 321, 767, 754, 3031, 3097, 341, 322, 264, 50572], "temperature": 0.0, "avg_logprob": -0.1024184544881185, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.0007881268975324929}, {"id": 631, "seek": 361088, "start": 3615.04, "end": 3623.2000000000003, "text": " MNIST training set, and it doesn't do too badly. So the point here, the thing that's interesting", "tokens": [50572, 376, 45, 19756, 3097, 992, 11, 293, 309, 1177, 380, 360, 886, 13425, 13, 407, 264, 935, 510, 11, 264, 551, 300, 311, 1880, 50980], "temperature": 0.0, "avg_logprob": -0.1024184544881185, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.0007881268975324929}, {"id": 632, "seek": 361088, "start": 3623.2000000000003, "end": 3629.84, "text": " here, these are all different solutions that this kind of very idealized neural net found", "tokens": [50980, 510, 11, 613, 366, 439, 819, 6547, 300, 341, 733, 295, 588, 7157, 1602, 18161, 2533, 1352, 51312], "temperature": 0.0, "avg_logprob": -0.1024184544881185, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.0007881268975324929}, {"id": 633, "seek": 361088, "start": 3630.4, "end": 3637.12, "text": " to living for this exact number of steps. What's interesting about these is they're very bizarre.", "tokens": [51340, 281, 2647, 337, 341, 1900, 1230, 295, 4439, 13, 708, 311, 1880, 466, 613, 307, 436, 434, 588, 18265, 13, 51676], "temperature": 0.0, "avg_logprob": -0.1024184544881185, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.0007881268975324929}, {"id": 634, "seek": 363712, "start": 3637.12, "end": 3642.08, "text": " They're not sort of engineered solutions. They're not solutions where we can say, oh, yeah, let me", "tokens": [50364, 814, 434, 406, 1333, 295, 38648, 6547, 13, 814, 434, 406, 6547, 689, 321, 393, 584, 11, 1954, 11, 1338, 11, 718, 385, 50612], "temperature": 0.0, "avg_logprob": -0.09955655551347577, "compression_ratio": 1.7678571428571428, "no_speech_prob": 0.005509658250957727}, {"id": 635, "seek": 363712, "start": 3642.08, "end": 3646.7999999999997, "text": " look inside and see how this works. Let me show you another example of that. And this is more back", "tokens": [50612, 574, 1854, 293, 536, 577, 341, 1985, 13, 961, 385, 855, 291, 1071, 1365, 295, 300, 13, 400, 341, 307, 544, 646, 50848], "temperature": 0.0, "avg_logprob": -0.09955655551347577, "compression_ratio": 1.7678571428571428, "no_speech_prob": 0.005509658250957727}, {"id": 636, "seek": 363712, "start": 3646.7999999999997, "end": 3653.04, "text": " to the biological evolution case. This is kind of all the different ways that a certain class of", "tokens": [50848, 281, 264, 13910, 9303, 1389, 13, 639, 307, 733, 295, 439, 264, 819, 2098, 300, 257, 1629, 1508, 295, 51160], "temperature": 0.0, "avg_logprob": -0.09955655551347577, "compression_ratio": 1.7678571428571428, "no_speech_prob": 0.005509658250957727}, {"id": 637, "seek": 363712, "start": 3653.04, "end": 3658.24, "text": " systems manages to live a long time. And some of them, it's kind of pretty structured. You can imagine", "tokens": [51160, 3652, 22489, 281, 1621, 257, 938, 565, 13, 400, 512, 295, 552, 11, 309, 311, 733, 295, 1238, 18519, 13, 509, 393, 3811, 51420], "temperature": 0.0, "avg_logprob": -0.09955655551347577, "compression_ratio": 1.7678571428571428, "no_speech_prob": 0.005509658250957727}, {"id": 638, "seek": 363712, "start": 3658.24, "end": 3662.64, "text": " sort of this was an engineered thing, but some of them, it's like it just seems to sort of happen", "tokens": [51420, 1333, 295, 341, 390, 364, 38648, 551, 11, 457, 512, 295, 552, 11, 309, 311, 411, 309, 445, 2544, 281, 1333, 295, 1051, 51640], "temperature": 0.0, "avg_logprob": -0.09955655551347577, "compression_ratio": 1.7678571428571428, "no_speech_prob": 0.005509658250957727}, {"id": 639, "seek": 366264, "start": 3662.64, "end": 3669.12, "text": " to live that long, and then it dies out. So in other words, there's a lot. And by doing this", "tokens": [50364, 281, 1621, 300, 938, 11, 293, 550, 309, 2714, 484, 13, 407, 294, 661, 2283, 11, 456, 311, 257, 688, 13, 400, 538, 884, 341, 50688], "temperature": 0.0, "avg_logprob": -0.11757014108740765, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.02240610495209694}, {"id": 640, "seek": 366264, "start": 3669.12, "end": 3674.8799999999997, "text": " sort of adaptive evolution, you're ending up finding these things which are very not,", "tokens": [50688, 1333, 295, 27912, 9303, 11, 291, 434, 8121, 493, 5006, 613, 721, 597, 366, 588, 406, 11, 50976], "temperature": 0.0, "avg_logprob": -0.11757014108740765, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.02240610495209694}, {"id": 641, "seek": 366264, "start": 3674.8799999999997, "end": 3680.0, "text": " they're not mechanical, they're not engineered kind of ways that things work. They're things", "tokens": [50976, 436, 434, 406, 12070, 11, 436, 434, 406, 38648, 733, 295, 2098, 300, 721, 589, 13, 814, 434, 721, 51232], "temperature": 0.0, "avg_logprob": -0.11757014108740765, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.02240610495209694}, {"id": 642, "seek": 366264, "start": 3680.0, "end": 3688.64, "text": " where kind of this is sort of what's happening inside. This is the thing that's going on,", "tokens": [51232, 689, 733, 295, 341, 307, 1333, 295, 437, 311, 2737, 1854, 13, 639, 307, 264, 551, 300, 311, 516, 322, 11, 51664], "temperature": 0.0, "avg_logprob": -0.11757014108740765, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.02240610495209694}, {"id": 643, "seek": 368864, "start": 3688.64, "end": 3693.44, "text": " but it's not something where you can say, oh, I've got a mechanism. By the way, if you're interested", "tokens": [50364, 457, 309, 311, 406, 746, 689, 291, 393, 584, 11, 1954, 11, 286, 600, 658, 257, 7513, 13, 3146, 264, 636, 11, 498, 291, 434, 3102, 50604], "temperature": 0.0, "avg_logprob": -0.06530592627559148, "compression_ratio": 1.794701986754967, "no_speech_prob": 0.2507452964782715}, {"id": 644, "seek": 368864, "start": 3693.44, "end": 3698.72, "text": " in neuroscience, this is something you should pay attention to, because in a sense, if you're", "tokens": [50604, 294, 42762, 11, 341, 307, 746, 291, 820, 1689, 3202, 281, 11, 570, 294, 257, 2020, 11, 498, 291, 434, 50868], "temperature": 0.0, "avg_logprob": -0.06530592627559148, "compression_ratio": 1.794701986754967, "no_speech_prob": 0.2507452964782715}, {"id": 645, "seek": 368864, "start": 3698.72, "end": 3703.12, "text": " trying to explain what's happening in the brain, and you say, oh, I'm going to figure out how this", "tokens": [50868, 1382, 281, 2903, 437, 311, 2737, 294, 264, 3567, 11, 293, 291, 584, 11, 1954, 11, 286, 478, 516, 281, 2573, 484, 577, 341, 51088], "temperature": 0.0, "avg_logprob": -0.06530592627559148, "compression_ratio": 1.794701986754967, "no_speech_prob": 0.2507452964782715}, {"id": 646, "seek": 368864, "start": 3703.12, "end": 3709.2799999999997, "text": " works. Well, how this works is an attempt to have kind of a human understandable narrative", "tokens": [51088, 1985, 13, 1042, 11, 577, 341, 1985, 307, 364, 5217, 281, 362, 733, 295, 257, 1952, 25648, 9977, 51396], "temperature": 0.0, "avg_logprob": -0.06530592627559148, "compression_ratio": 1.794701986754967, "no_speech_prob": 0.2507452964782715}, {"id": 647, "seek": 368864, "start": 3709.2799999999997, "end": 3714.4, "text": " for what's going on. But if I were to look at these pictures in the background here,", "tokens": [51396, 337, 437, 311, 516, 322, 13, 583, 498, 286, 645, 281, 574, 412, 613, 5242, 294, 264, 3678, 510, 11, 51652], "temperature": 0.0, "avg_logprob": -0.06530592627559148, "compression_ratio": 1.794701986754967, "no_speech_prob": 0.2507452964782715}, {"id": 648, "seek": 368864, "start": 3714.4, "end": 3718.3199999999997, "text": " if this was something going on in a brain, I might be able to say, okay,", "tokens": [51652, 498, 341, 390, 746, 516, 322, 294, 257, 3567, 11, 286, 1062, 312, 1075, 281, 584, 11, 1392, 11, 51848], "temperature": 0.0, "avg_logprob": -0.06530592627559148, "compression_ratio": 1.794701986754967, "no_speech_prob": 0.2507452964782715}, {"id": 649, "seek": 371832, "start": 3718.4, "end": 3722.1600000000003, "text": " I can have some human narrative about what's happening here. If this is what's going on in a", "tokens": [50368, 286, 393, 362, 512, 1952, 9977, 466, 437, 311, 2737, 510, 13, 759, 341, 307, 437, 311, 516, 322, 294, 257, 50556], "temperature": 0.0, "avg_logprob": -0.08174472744182004, "compression_ratio": 1.8924302788844622, "no_speech_prob": 0.00444013299420476}, {"id": 650, "seek": 371832, "start": 3722.1600000000003, "end": 3726.7200000000003, "text": " brain, it's just, well, it happens to work that way, and it happens to give this result. It's", "tokens": [50556, 3567, 11, 309, 311, 445, 11, 731, 11, 309, 2314, 281, 589, 300, 636, 11, 293, 309, 2314, 281, 976, 341, 1874, 13, 467, 311, 50784], "temperature": 0.0, "avg_logprob": -0.08174472744182004, "compression_ratio": 1.8924302788844622, "no_speech_prob": 0.00444013299420476}, {"id": 651, "seek": 371832, "start": 3726.7200000000003, "end": 3732.6400000000003, "text": " kind of a computational irreducible story. It's something where there's no sort of narrative", "tokens": [50784, 733, 295, 257, 28270, 16014, 769, 32128, 1657, 13, 467, 311, 746, 689, 456, 311, 572, 1333, 295, 9977, 51080], "temperature": 0.0, "avg_logprob": -0.08174472744182004, "compression_ratio": 1.8924302788844622, "no_speech_prob": 0.00444013299420476}, {"id": 652, "seek": 371832, "start": 3732.6400000000003, "end": 3738.0800000000004, "text": " mechanistic explanation. It's something which just works that way, and it's computationally", "tokens": [51080, 4236, 3142, 10835, 13, 467, 311, 746, 597, 445, 1985, 300, 636, 11, 293, 309, 311, 24903, 379, 51352], "temperature": 0.0, "avg_logprob": -0.08174472744182004, "compression_ratio": 1.8924302788844622, "no_speech_prob": 0.00444013299420476}, {"id": 653, "seek": 371832, "start": 3738.0800000000004, "end": 3742.6400000000003, "text": " irreducible, but it just comes out in that fashion. And I think there's sort of an interesting question", "tokens": [51352, 16014, 769, 32128, 11, 457, 309, 445, 1487, 484, 294, 300, 6700, 13, 400, 286, 519, 456, 311, 1333, 295, 364, 1880, 1168, 51580], "temperature": 0.0, "avg_logprob": -0.08174472744182004, "compression_ratio": 1.8924302788844622, "no_speech_prob": 0.00444013299420476}, {"id": 654, "seek": 374264, "start": 3743.2, "end": 3750.72, "text": " for in machine learning. Why does machine learning work? Okay, so let's look at,", "tokens": [50392, 337, 294, 3479, 2539, 13, 1545, 775, 3479, 2539, 589, 30, 1033, 11, 370, 718, 311, 574, 412, 11, 50768], "temperature": 0.0, "avg_logprob": -0.13221779102232398, "compression_ratio": 1.6729857819905214, "no_speech_prob": 0.02331872470676899}, {"id": 655, "seek": 374264, "start": 3752.7999999999997, "end": 3760.0, "text": " was a nice picture of that. By the way, this is in biological evolution. People often talk", "tokens": [50872, 390, 257, 1481, 3036, 295, 300, 13, 3146, 264, 636, 11, 341, 307, 294, 13910, 9303, 13, 3432, 2049, 751, 51232], "temperature": 0.0, "avg_logprob": -0.13221779102232398, "compression_ratio": 1.6729857819905214, "no_speech_prob": 0.02331872470676899}, {"id": 656, "seek": 374264, "start": 3760.0, "end": 3764.7999999999997, "text": " about fitness landscapes. This is an actual fitness landscape correctly drawn, so to speak.", "tokens": [51232, 466, 15303, 29822, 13, 639, 307, 364, 3539, 15303, 9661, 8944, 10117, 11, 370, 281, 1710, 13, 51472], "temperature": 0.0, "avg_logprob": -0.13221779102232398, "compression_ratio": 1.6729857819905214, "no_speech_prob": 0.02331872470676899}, {"id": 657, "seek": 374264, "start": 3764.7999999999997, "end": 3769.92, "text": " And you can start seeing all kinds of things about things evolving on fitness landscapes.", "tokens": [51472, 400, 291, 393, 722, 2577, 439, 3685, 295, 721, 466, 721, 21085, 322, 15303, 29822, 13, 51728], "temperature": 0.0, "avg_logprob": -0.13221779102232398, "compression_ratio": 1.6729857819905214, "no_speech_prob": 0.02331872470676899}, {"id": 658, "seek": 376992, "start": 3769.92, "end": 3776.88, "text": " But the thing I really wanted to show you, here it is. This is kind of the local behavior", "tokens": [50364, 583, 264, 551, 286, 534, 1415, 281, 855, 291, 11, 510, 309, 307, 13, 639, 307, 733, 295, 264, 2654, 5223, 50712], "temperature": 0.0, "avg_logprob": -0.10512020037724422, "compression_ratio": 1.727626459143969, "no_speech_prob": 0.0020767797250300646}, {"id": 659, "seek": 376992, "start": 3776.88, "end": 3784.32, "text": " at a particular point in rule space at various steps in the adaptive evolution.", "tokens": [50712, 412, 257, 1729, 935, 294, 4978, 1901, 412, 3683, 4439, 294, 264, 27912, 9303, 13, 51084], "temperature": 0.0, "avg_logprob": -0.10512020037724422, "compression_ratio": 1.727626459143969, "no_speech_prob": 0.0020767797250300646}, {"id": 660, "seek": 376992, "start": 3784.32, "end": 3789.28, "text": " So what's happening here is at this step, for example, in the adaptive evolution,", "tokens": [51084, 407, 437, 311, 2737, 510, 307, 412, 341, 1823, 11, 337, 1365, 11, 294, 264, 27912, 9303, 11, 51332], "temperature": 0.0, "avg_logprob": -0.10512020037724422, "compression_ratio": 1.727626459143969, "no_speech_prob": 0.0020767797250300646}, {"id": 661, "seek": 376992, "start": 3789.28, "end": 3794.96, "text": " here are different possible directions in rule space that you might go. And the ones inside", "tokens": [51332, 510, 366, 819, 1944, 11095, 294, 4978, 1901, 300, 291, 1062, 352, 13, 400, 264, 2306, 1854, 51616], "temperature": 0.0, "avg_logprob": -0.10512020037724422, "compression_ratio": 1.727626459143969, "no_speech_prob": 0.0020767797250300646}, {"id": 662, "seek": 376992, "start": 3794.96, "end": 3798.8, "text": " the circle are ones that are losers relative to where you've already got. They're ones that would be", "tokens": [51616, 264, 6329, 366, 2306, 300, 366, 37713, 4972, 281, 689, 291, 600, 1217, 658, 13, 814, 434, 2306, 300, 576, 312, 51808], "temperature": 0.0, "avg_logprob": -0.10512020037724422, "compression_ratio": 1.727626459143969, "no_speech_prob": 0.0020767797250300646}, {"id": 663, "seek": 379880, "start": 3798.88, "end": 3803.44, "text": " live less long than what we have here, but there are some that would make progress.", "tokens": [50368, 1621, 1570, 938, 813, 437, 321, 362, 510, 11, 457, 456, 366, 512, 300, 576, 652, 4205, 13, 50596], "temperature": 0.0, "avg_logprob": -0.09677656685433737, "compression_ratio": 1.8229166666666667, "no_speech_prob": 0.007108576130121946}, {"id": 664, "seek": 379880, "start": 3804.2400000000002, "end": 3808.32, "text": " And in fact, this is the one we happened to choose in this particular random", "tokens": [50636, 400, 294, 1186, 11, 341, 307, 264, 472, 321, 2011, 281, 2826, 294, 341, 1729, 4974, 50840], "temperature": 0.0, "avg_logprob": -0.09677656685433737, "compression_ratio": 1.8229166666666667, "no_speech_prob": 0.007108576130121946}, {"id": 665, "seek": 379880, "start": 3808.32, "end": 3812.8, "text": " sequence of adaptive evolution steps. And that was the thing that made progress. So", "tokens": [50840, 8310, 295, 27912, 9303, 4439, 13, 400, 300, 390, 264, 551, 300, 1027, 4205, 13, 407, 51064], "temperature": 0.0, "avg_logprob": -0.09677656685433737, "compression_ratio": 1.8229166666666667, "no_speech_prob": 0.007108576130121946}, {"id": 666, "seek": 379880, "start": 3812.8, "end": 3818.1600000000003, "text": " the thing that is not obvious is in this sort of high dimensional space of possible ways you could", "tokens": [51064, 264, 551, 300, 307, 406, 6322, 307, 294, 341, 1333, 295, 1090, 18795, 1901, 295, 1944, 2098, 291, 727, 51332], "temperature": 0.0, "avg_logprob": -0.09677656685433737, "compression_ratio": 1.8229166666666667, "no_speech_prob": 0.007108576130121946}, {"id": 667, "seek": 379880, "start": 3818.1600000000003, "end": 3822.7200000000003, "text": " go, the question is, will you always be able to make progress? Will there be a direction that", "tokens": [51332, 352, 11, 264, 1168, 307, 11, 486, 291, 1009, 312, 1075, 281, 652, 4205, 30, 3099, 456, 312, 257, 3513, 300, 51560], "temperature": 0.0, "avg_logprob": -0.09677656685433737, "compression_ratio": 1.8229166666666667, "no_speech_prob": 0.007108576130121946}, {"id": 668, "seek": 379880, "start": 3822.7200000000003, "end": 3828.1600000000003, "text": " makes progress? Or will you get stuck? Well, I think that this is again a computational", "tokens": [51560, 1669, 4205, 30, 1610, 486, 291, 483, 5541, 30, 1042, 11, 286, 519, 300, 341, 307, 797, 257, 28270, 51832], "temperature": 0.0, "avg_logprob": -0.09677656685433737, "compression_ratio": 1.8229166666666667, "no_speech_prob": 0.007108576130121946}, {"id": 669, "seek": 382816, "start": 3828.16, "end": 3834.3199999999997, "text": " irreducibility story that basically what would make you get stuck? Well, if the structure of", "tokens": [50364, 16014, 769, 537, 39802, 1657, 300, 1936, 437, 576, 652, 291, 483, 5541, 30, 1042, 11, 498, 264, 3877, 295, 50672], "temperature": 0.0, "avg_logprob": -0.07399029152415623, "compression_ratio": 1.7471698113207548, "no_speech_prob": 0.00241789105348289}, {"id": 670, "seek": 382816, "start": 3834.3199999999997, "end": 3841.2, "text": " this rule space was very orderly, very reducible and easy to predict, you might end up in a box", "tokens": [50672, 341, 4978, 1901, 390, 588, 1668, 356, 11, 588, 2783, 32128, 293, 1858, 281, 6069, 11, 291, 1062, 917, 493, 294, 257, 2424, 51016], "temperature": 0.0, "avg_logprob": -0.07399029152415623, "compression_ratio": 1.7471698113207548, "no_speech_prob": 0.00241789105348289}, {"id": 671, "seek": 382816, "start": 3841.2, "end": 3847.04, "text": " with very precisely defined walls and you just can't escape from that. But the presence of", "tokens": [51016, 365, 588, 13402, 7642, 7920, 293, 291, 445, 393, 380, 7615, 490, 300, 13, 583, 264, 6814, 295, 51308], "temperature": 0.0, "avg_logprob": -0.07399029152415623, "compression_ratio": 1.7471698113207548, "no_speech_prob": 0.00241789105348289}, {"id": 672, "seek": 382816, "start": 3847.04, "end": 3851.92, "text": " computational irreducibility kind of implies a certain degree of unpredictability, a certain", "tokens": [51308, 28270, 16014, 769, 537, 39802, 733, 295, 18779, 257, 1629, 4314, 295, 28341, 2310, 11, 257, 1629, 51552], "temperature": 0.0, "avg_logprob": -0.07399029152415623, "compression_ratio": 1.7471698113207548, "no_speech_prob": 0.00241789105348289}, {"id": 673, "seek": 382816, "start": 3851.92, "end": 3857.68, "text": " degree of intrinsic randomness effectively in the structure of rule space. And that's what", "tokens": [51552, 4314, 295, 35698, 4974, 1287, 8659, 294, 264, 3877, 295, 4978, 1901, 13, 400, 300, 311, 437, 51840], "temperature": 0.0, "avg_logprob": -0.07399029152415623, "compression_ratio": 1.7471698113207548, "no_speech_prob": 0.00241789105348289}, {"id": 674, "seek": 385768, "start": 3857.68, "end": 3862.72, "text": " means that in these high dimensional spaces, there's always a kind of a path to success.", "tokens": [50364, 1355, 300, 294, 613, 1090, 18795, 7673, 11, 456, 311, 1009, 257, 733, 295, 257, 3100, 281, 2245, 13, 50616], "temperature": 0.0, "avg_logprob": -0.08727788925170898, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.0018237332114949822}, {"id": 675, "seek": 385768, "start": 3862.72, "end": 3870.3199999999997, "text": " So in a sense, I think computational irreducibility, which is a limitation on what one can do with,", "tokens": [50616, 407, 294, 257, 2020, 11, 286, 519, 28270, 16014, 769, 537, 39802, 11, 597, 307, 257, 27432, 322, 437, 472, 393, 360, 365, 11, 50996], "temperature": 0.0, "avg_logprob": -0.08727788925170898, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.0018237332114949822}, {"id": 676, "seek": 385768, "start": 3870.3199999999997, "end": 3874.48, "text": " for example, a neural net, what kinds of things computations one can expect to do,", "tokens": [50996, 337, 1365, 11, 257, 18161, 2533, 11, 437, 3685, 295, 721, 2807, 763, 472, 393, 2066, 281, 360, 11, 51204], "temperature": 0.0, "avg_logprob": -0.08727788925170898, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.0018237332114949822}, {"id": 677, "seek": 385768, "start": 3874.48, "end": 3881.44, "text": " is also the reason that training of neural nets, for example, can work. And so I think that's a,", "tokens": [51204, 307, 611, 264, 1778, 300, 3097, 295, 18161, 36170, 11, 337, 1365, 11, 393, 589, 13, 400, 370, 286, 519, 300, 311, 257, 11, 51552], "temperature": 0.0, "avg_logprob": -0.08727788925170898, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.0018237332114949822}, {"id": 678, "seek": 388144, "start": 3881.52, "end": 3889.92, "text": " anyway, this is still an in progress kind of investigation. But I sort of think it's", "tokens": [50368, 4033, 11, 341, 307, 920, 364, 294, 4205, 733, 295, 9627, 13, 583, 286, 1333, 295, 519, 309, 311, 50788], "temperature": 0.0, "avg_logprob": -0.11252477557160133, "compression_ratio": 1.5155555555555555, "no_speech_prob": 0.04799221083521843}, {"id": 679, "seek": 388144, "start": 3889.92, "end": 3893.52, "text": " an interesting connection between a lot of different things I've talked about.", "tokens": [50788, 364, 1880, 4984, 1296, 257, 688, 295, 819, 721, 286, 600, 2825, 466, 13, 50968], "temperature": 0.0, "avg_logprob": -0.11252477557160133, "compression_ratio": 1.5155555555555555, "no_speech_prob": 0.04799221083521843}, {"id": 680, "seek": 388144, "start": 3893.52, "end": 3901.2000000000003, "text": " All right, I've gone on longer than I intended to. So let me wrap up there and I'm happy to", "tokens": [50968, 1057, 558, 11, 286, 600, 2780, 322, 2854, 813, 286, 10226, 281, 13, 407, 718, 385, 7019, 493, 456, 293, 286, 478, 2055, 281, 51352], "temperature": 0.0, "avg_logprob": -0.11252477557160133, "compression_ratio": 1.5155555555555555, "no_speech_prob": 0.04799221083521843}, {"id": 681, "seek": 388144, "start": 3901.2000000000003, "end": 3906.96, "text": " have a discussion, questions, whatever else. I just fed you an awful lot of material.", "tokens": [51352, 362, 257, 5017, 11, 1651, 11, 2035, 1646, 13, 286, 445, 4636, 291, 364, 11232, 688, 295, 2527, 13, 51640], "temperature": 0.0, "avg_logprob": -0.11252477557160133, "compression_ratio": 1.5155555555555555, "no_speech_prob": 0.04799221083521843}, {"id": 682, "seek": 390696, "start": 3907.44, "end": 3911.76, "text": " Yes, good. Let's first applaud this president.", "tokens": [50388, 1079, 11, 665, 13, 961, 311, 700, 9644, 341, 3868, 13, 50604], "temperature": 0.0, "avg_logprob": -0.2479879797958746, "compression_ratio": 1.4928909952606635, "no_speech_prob": 0.0050543202087283134}, {"id": 683, "seek": 390696, "start": 3916.64, "end": 3924.16, "text": " The problem was that Kaiyou had an emergency during your talk, so he couldn't hear it.", "tokens": [50848, 440, 1154, 390, 300, 20753, 5616, 632, 364, 7473, 1830, 428, 751, 11, 370, 415, 2809, 380, 1568, 309, 13, 51224], "temperature": 0.0, "avg_logprob": -0.2479879797958746, "compression_ratio": 1.4928909952606635, "no_speech_prob": 0.0050543202087283134}, {"id": 684, "seek": 390696, "start": 3924.16, "end": 3928.88, "text": " Kaiyou, do you think that you have, from the background material you might have looked at,", "tokens": [51224, 20753, 5616, 11, 360, 291, 519, 300, 291, 362, 11, 490, 264, 3678, 2527, 291, 1062, 362, 2956, 412, 11, 51460], "temperature": 0.0, "avg_logprob": -0.2479879797958746, "compression_ratio": 1.4928909952606635, "no_speech_prob": 0.0050543202087283134}, {"id": 685, "seek": 390696, "start": 3928.88, "end": 3935.52, "text": " you have a basis for saying something? Sorry, I missed most part of the time, so probably.", "tokens": [51460, 291, 362, 257, 5143, 337, 1566, 746, 30, 4919, 11, 286, 6721, 881, 644, 295, 264, 565, 11, 370, 1391, 13, 51792], "temperature": 0.0, "avg_logprob": -0.2479879797958746, "compression_ratio": 1.4928909952606635, "no_speech_prob": 0.0050543202087283134}, {"id": 686, "seek": 393552, "start": 3936.24, "end": 3943.12, "text": " If you haven't seen, this is a large amount of material. I would be surprised if we could", "tokens": [50400, 759, 291, 2378, 380, 1612, 11, 341, 307, 257, 2416, 2372, 295, 2527, 13, 286, 576, 312, 6100, 498, 321, 727, 50744], "temperature": 0.0, "avg_logprob": -0.1665188925606864, "compression_ratio": 1.4659090909090908, "no_speech_prob": 0.0045619625598192215}, {"id": 687, "seek": 393552, "start": 3943.12, "end": 3948.96, "text": " have a useful conversation without having some anchor to this. Could you please explain to Kaiyou", "tokens": [50744, 362, 257, 4420, 3761, 1553, 1419, 512, 18487, 281, 341, 13, 7497, 291, 1767, 2903, 281, 20753, 5616, 51036], "temperature": 0.0, "avg_logprob": -0.1665188925606864, "compression_ratio": 1.4659090909090908, "no_speech_prob": 0.0045619625598192215}, {"id": 688, "seek": 393552, "start": 3948.96, "end": 3957.28, "text": " and to me and to us how computational irreducibility differs from ABC,", "tokens": [51036, 293, 281, 385, 293, 281, 505, 577, 28270, 16014, 769, 537, 39802, 37761, 490, 22342, 11, 51452], "temperature": 0.0, "avg_logprob": -0.1665188925606864, "compression_ratio": 1.4659090909090908, "no_speech_prob": 0.0045619625598192215}, {"id": 689, "seek": 395728, "start": 3957.6000000000004, "end": 3966.0800000000004, "text": " the commagor of complexity, the church-touring thesis, and NP completeness.", "tokens": [50380, 264, 800, 559, 284, 295, 14024, 11, 264, 4128, 12, 83, 40510, 22288, 11, 293, 38611, 1557, 15264, 13, 50804], "temperature": 0.0, "avg_logprob": -0.3385596181832108, "compression_ratio": 1.2666666666666666, "no_speech_prob": 0.006444618571549654}, {"id": 690, "seek": 395728, "start": 3967.1200000000003, "end": 3977.2000000000003, "text": " Okay. All right, let's start off with Chetan Komogorov complexity. So when we look at a picture", "tokens": [50856, 1033, 13, 1057, 558, 11, 718, 311, 722, 766, 365, 761, 302, 282, 14286, 664, 284, 5179, 14024, 13, 407, 562, 321, 574, 412, 257, 3036, 51360], "temperature": 0.0, "avg_logprob": -0.3385596181832108, "compression_ratio": 1.2666666666666666, "no_speech_prob": 0.006444618571549654}, {"id": 691, "seek": 397720, "start": 3977.52, "end": 3987.52, "text": " like this, the algorithmic complexity of this picture is tiny. That's the program that's needed", "tokens": [50380, 411, 341, 11, 264, 9284, 299, 14024, 295, 341, 3036, 307, 5870, 13, 663, 311, 264, 1461, 300, 311, 2978, 50880], "temperature": 0.0, "avg_logprob": -0.09417019107125023, "compression_ratio": 1.7625570776255708, "no_speech_prob": 0.049271900206804276}, {"id": 692, "seek": 397720, "start": 3987.52, "end": 3994.3199999999997, "text": " to produce it, just a few bits. The thing that is remarkable is that even things with very low", "tokens": [50880, 281, 5258, 309, 11, 445, 257, 1326, 9239, 13, 440, 551, 300, 307, 12802, 307, 300, 754, 721, 365, 588, 2295, 51220], "temperature": 0.0, "avg_logprob": -0.09417019107125023, "compression_ratio": 1.7625570776255708, "no_speech_prob": 0.049271900206804276}, {"id": 693, "seek": 397720, "start": 3994.3199999999997, "end": 4000.8799999999997, "text": " algorithmic complexity are very complicated. In fact, they're complicated enough that to us humans,", "tokens": [51220, 9284, 299, 14024, 366, 588, 6179, 13, 682, 1186, 11, 436, 434, 6179, 1547, 300, 281, 505, 6255, 11, 51548], "temperature": 0.0, "avg_logprob": -0.09417019107125023, "compression_ratio": 1.7625570776255708, "no_speech_prob": 0.049271900206804276}, {"id": 694, "seek": 397720, "start": 4000.8799999999997, "end": 4005.8399999999997, "text": " we wouldn't even be able to distinguish them from things that have high algorithmic complexity.", "tokens": [51548, 321, 2759, 380, 754, 312, 1075, 281, 20206, 552, 490, 721, 300, 362, 1090, 9284, 299, 14024, 13, 51796], "temperature": 0.0, "avg_logprob": -0.09417019107125023, "compression_ratio": 1.7625570776255708, "no_speech_prob": 0.049271900206804276}, {"id": 695, "seek": 400584, "start": 4005.84, "end": 4009.92, "text": " One of the things I've had a long-running discussion with my friend Greg Chetan,", "tokens": [50364, 1485, 295, 264, 721, 286, 600, 632, 257, 938, 12, 45482, 5017, 365, 452, 1277, 11490, 761, 302, 282, 11, 50568], "temperature": 0.0, "avg_logprob": -0.09602344429099953, "compression_ratio": 1.5574468085106383, "no_speech_prob": 0.0022704952862113714}, {"id": 696, "seek": 400584, "start": 4009.92, "end": 4016.48, "text": " where the question is, is the universe like Pi or like Omega? Omega is this thing that Greg", "tokens": [50568, 689, 264, 1168, 307, 11, 307, 264, 6445, 411, 17741, 420, 411, 27645, 30, 27645, 307, 341, 551, 300, 11490, 50896], "temperature": 0.0, "avg_logprob": -0.09602344429099953, "compression_ratio": 1.5574468085106383, "no_speech_prob": 0.0022704952862113714}, {"id": 697, "seek": 400584, "start": 4016.48, "end": 4022.48, "text": " invented 50 years ago, actually, that is the halting probability for a universal Turing machine.", "tokens": [50896, 14479, 2625, 924, 2057, 11, 767, 11, 300, 307, 264, 7523, 783, 8482, 337, 257, 11455, 314, 1345, 3479, 13, 51196], "temperature": 0.0, "avg_logprob": -0.09602344429099953, "compression_ratio": 1.5574468085106383, "no_speech_prob": 0.0022704952862113714}, {"id": 698, "seek": 400584, "start": 4022.48, "end": 4029.6000000000004, "text": " It's a fundamentally non-computable object. It's an object with infinite algorithmic complexity.", "tokens": [51196, 467, 311, 257, 17879, 2107, 12, 1112, 2582, 712, 2657, 13, 467, 311, 364, 2657, 365, 13785, 9284, 299, 14024, 13, 51552], "temperature": 0.0, "avg_logprob": -0.09602344429099953, "compression_ratio": 1.5574468085106383, "no_speech_prob": 0.0022704952862113714}, {"id": 699, "seek": 402960, "start": 4029.68, "end": 4037.52, "text": " It is a thing where there is no small program that you can't specify it by a small program.", "tokens": [50368, 467, 307, 257, 551, 689, 456, 307, 572, 1359, 1461, 300, 291, 393, 380, 16500, 309, 538, 257, 1359, 1461, 13, 50760], "temperature": 0.0, "avg_logprob": -0.08749104668112362, "compression_ratio": 1.6766169154228856, "no_speech_prob": 0.07594402134418488}, {"id": 700, "seek": 402960, "start": 4037.52, "end": 4044.56, "text": " Pi, on the other hand, is a thing that is specified by a quite a small program.", "tokens": [50760, 17741, 11, 322, 264, 661, 1011, 11, 307, 257, 551, 300, 307, 22206, 538, 257, 1596, 257, 1359, 1461, 13, 51112], "temperature": 0.0, "avg_logprob": -0.08749104668112362, "compression_ratio": 1.6766169154228856, "no_speech_prob": 0.07594402134418488}, {"id": 701, "seek": 402960, "start": 4046.4, "end": 4050.08, "text": " Once you generate its digits, it looks for all practical purposes random.", "tokens": [51204, 3443, 291, 8460, 1080, 27011, 11, 309, 1542, 337, 439, 8496, 9932, 4974, 13, 51388], "temperature": 0.0, "avg_logprob": -0.08749104668112362, "compression_ratio": 1.6766169154228856, "no_speech_prob": 0.07594402134418488}, {"id": 702, "seek": 402960, "start": 4050.08, "end": 4055.68, "text": " So the question that one can ask is, in our universe, is there anything of high algorithmic", "tokens": [51388, 407, 264, 1168, 300, 472, 393, 1029, 307, 11, 294, 527, 6445, 11, 307, 456, 1340, 295, 1090, 9284, 299, 51668], "temperature": 0.0, "avg_logprob": -0.08749104668112362, "compression_ratio": 1.6766169154228856, "no_speech_prob": 0.07594402134418488}, {"id": 703, "seek": 405568, "start": 4055.68, "end": 4062.3999999999996, "text": " complexity? Or is the whole universe actually a thing that is like Pi generated from something", "tokens": [50364, 14024, 30, 1610, 307, 264, 1379, 6445, 767, 257, 551, 300, 307, 411, 17741, 10833, 490, 746, 50700], "temperature": 0.0, "avg_logprob": -0.0811776827616864, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.011994522996246815}, {"id": 704, "seek": 405568, "start": 4062.3999999999996, "end": 4069.44, "text": " which is a very simple underlying sort of program? And in our model of physics, the answer is the", "tokens": [50700, 597, 307, 257, 588, 2199, 14217, 1333, 295, 1461, 30, 400, 294, 527, 2316, 295, 10649, 11, 264, 1867, 307, 264, 51052], "temperature": 0.0, "avg_logprob": -0.0811776827616864, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.011994522996246815}, {"id": 705, "seek": 405568, "start": 4069.44, "end": 4076.3999999999996, "text": " universe is like Pi. The universe is something that is generated from a thing of very tiny", "tokens": [51052, 6445, 307, 411, 17741, 13, 440, 6445, 307, 746, 300, 307, 10833, 490, 257, 551, 295, 588, 5870, 51400], "temperature": 0.0, "avg_logprob": -0.0811776827616864, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.011994522996246815}, {"id": 706, "seek": 405568, "start": 4076.3999999999996, "end": 4082.08, "text": " algorithmic complexity. So that's kind of the distinction between everything I'm talking about", "tokens": [51400, 9284, 299, 14024, 13, 407, 300, 311, 733, 295, 264, 16844, 1296, 1203, 286, 478, 1417, 466, 51684], "temperature": 0.0, "avg_logprob": -0.0811776827616864, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.011994522996246815}, {"id": 707, "seek": 408208, "start": 4082.16, "end": 4087.12, "text": " is things of incredibly low algorithmic complexity. The remarkable fact that is not obvious,", "tokens": [50368, 307, 721, 295, 6252, 2295, 9284, 299, 14024, 13, 440, 12802, 1186, 300, 307, 406, 6322, 11, 50616], "temperature": 0.0, "avg_logprob": -0.09446395948095229, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.029982587322592735}, {"id": 708, "seek": 408208, "start": 4087.12, "end": 4091.7599999999998, "text": " it kind of breaks one's intuition, is that things, very simple programs, things of very", "tokens": [50616, 309, 733, 295, 9857, 472, 311, 24002, 11, 307, 300, 721, 11, 588, 2199, 4268, 11, 721, 295, 588, 50848], "temperature": 0.0, "avg_logprob": -0.09446395948095229, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.029982587322592735}, {"id": 709, "seek": 408208, "start": 4091.7599999999998, "end": 4097.6, "text": " low algorithmic complexity can produce what seems to us like great complexity. And the", "tokens": [50848, 2295, 9284, 299, 14024, 393, 5258, 437, 2544, 281, 505, 411, 869, 14024, 13, 400, 264, 51140], "temperature": 0.0, "avg_logprob": -0.09446395948095229, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.029982587322592735}, {"id": 710, "seek": 408208, "start": 4097.6, "end": 4104.32, "text": " seems to us becomes much harder when we start talking about our computational boundedness.", "tokens": [51140, 2544, 281, 505, 3643, 709, 6081, 562, 321, 722, 1417, 466, 527, 28270, 37498, 1287, 13, 51476], "temperature": 0.0, "avg_logprob": -0.09446395948095229, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.029982587322592735}, {"id": 711, "seek": 408208, "start": 4104.32, "end": 4109.68, "text": " It's not the case that it's just, oh, it seems complicated. It's that for a computationally", "tokens": [51476, 467, 311, 406, 264, 1389, 300, 309, 311, 445, 11, 1954, 11, 309, 2544, 6179, 13, 467, 311, 300, 337, 257, 24903, 379, 51744], "temperature": 0.0, "avg_logprob": -0.09446395948095229, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.029982587322592735}, {"id": 712, "seek": 410968, "start": 4109.68, "end": 4117.6, "text": " bounded observer like us, there is no way to compress it. So in algorithmic complexity,", "tokens": [50364, 37498, 27878, 411, 505, 11, 456, 307, 572, 636, 281, 14778, 309, 13, 407, 294, 9284, 299, 14024, 11, 50760], "temperature": 0.0, "avg_logprob": -0.08425434430440266, "compression_ratio": 1.9944751381215469, "no_speech_prob": 0.007981055416166782}, {"id": 713, "seek": 410968, "start": 4117.6, "end": 4122.56, "text": " algorithmic information, when saying this is the program and there is no shorter program,", "tokens": [50760, 9284, 299, 1589, 11, 562, 1566, 341, 307, 264, 1461, 293, 456, 307, 572, 11639, 1461, 11, 51008], "temperature": 0.0, "avg_logprob": -0.08425434430440266, "compression_ratio": 1.9944751381215469, "no_speech_prob": 0.007981055416166782}, {"id": 714, "seek": 410968, "start": 4122.56, "end": 4128.0, "text": " one can say for a computationally bounded observer, this is the set of bits and there is no way to", "tokens": [51008, 472, 393, 584, 337, 257, 24903, 379, 37498, 27878, 11, 341, 307, 264, 992, 295, 9239, 293, 456, 307, 572, 636, 281, 51280], "temperature": 0.0, "avg_logprob": -0.08425434430440266, "compression_ratio": 1.9944751381215469, "no_speech_prob": 0.007981055416166782}, {"id": 715, "seek": 410968, "start": 4128.0, "end": 4135.04, "text": " make it shorter. So that was algorithmic information theory, algorithmic complexity.", "tokens": [51280, 652, 309, 11639, 13, 407, 300, 390, 9284, 299, 1589, 5261, 11, 9284, 299, 14024, 13, 51632], "temperature": 0.0, "avg_logprob": -0.08425434430440266, "compression_ratio": 1.9944751381215469, "no_speech_prob": 0.007981055416166782}, {"id": 716, "seek": 413504, "start": 4135.04, "end": 4144.48, "text": " I think the second one you had was, what was it? Church Turing. Okay. So, okay. The thing that,", "tokens": [50364, 286, 519, 264, 1150, 472, 291, 632, 390, 11, 437, 390, 309, 30, 7882, 314, 1345, 13, 1033, 13, 407, 11, 1392, 13, 440, 551, 300, 11, 50836], "temperature": 0.0, "avg_logprob": -0.16404815673828124, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.06442182511091232}, {"id": 717, "seek": 413504, "start": 4148.24, "end": 4152.96, "text": " if you go back to the beginning of the 20th century, and you'd wanted to get an adding", "tokens": [51024, 498, 291, 352, 646, 281, 264, 2863, 295, 264, 945, 392, 4901, 11, 293, 291, 1116, 1415, 281, 483, 364, 5127, 51260], "temperature": 0.0, "avg_logprob": -0.16404815673828124, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.06442182511091232}, {"id": 718, "seek": 413504, "start": 4152.96, "end": 4157.92, "text": " machine, you might go to a store, you buy an adding machine. You want to get a square root", "tokens": [51260, 3479, 11, 291, 1062, 352, 281, 257, 3531, 11, 291, 2256, 364, 5127, 3479, 13, 509, 528, 281, 483, 257, 3732, 5593, 51508], "temperature": 0.0, "avg_logprob": -0.16404815673828124, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.06442182511091232}, {"id": 719, "seek": 413504, "start": 4157.92, "end": 4162.24, "text": " machine. Okay, you go to a different store, perhaps, and you buy a different machine that", "tokens": [51508, 3479, 13, 1033, 11, 291, 352, 281, 257, 819, 3531, 11, 4317, 11, 293, 291, 2256, 257, 819, 3479, 300, 51724], "temperature": 0.0, "avg_logprob": -0.16404815673828124, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.06442182511091232}, {"id": 720, "seek": 416224, "start": 4162.24, "end": 4167.12, "text": " is the square root machine. The big discovery that actually originally got made by Moses", "tokens": [50364, 307, 264, 3732, 5593, 3479, 13, 440, 955, 12114, 300, 767, 7993, 658, 1027, 538, 17580, 50608], "temperature": 0.0, "avg_logprob": -0.09107645034790039, "compression_ratio": 1.6953125, "no_speech_prob": 0.0014889725716784596}, {"id": 721, "seek": 416224, "start": 4167.12, "end": 4172.32, "text": " Schoenfinkel with combinators in 1920, but nobody understood it then or since, basically,", "tokens": [50608, 2065, 78, 268, 5194, 7124, 365, 38514, 3391, 294, 22003, 11, 457, 5079, 7320, 309, 550, 420, 1670, 11, 1936, 11, 50868], "temperature": 0.0, "avg_logprob": -0.09107645034790039, "compression_ratio": 1.6953125, "no_speech_prob": 0.0014889725716784596}, {"id": 722, "seek": 416224, "start": 4172.32, "end": 4180.639999999999, "text": " but then kind of got clarified by Turing in 1936 is that there exist kind of,", "tokens": [50868, 457, 550, 733, 295, 658, 47605, 538, 314, 1345, 294, 1294, 11309, 307, 300, 456, 2514, 733, 295, 11, 51284], "temperature": 0.0, "avg_logprob": -0.09107645034790039, "compression_ratio": 1.6953125, "no_speech_prob": 0.0014889725716784596}, {"id": 723, "seek": 416224, "start": 4182.5599999999995, "end": 4187.5199999999995, "text": " there exist systems that are universal in the sense that you can have a single piece of hardware", "tokens": [51380, 456, 2514, 3652, 300, 366, 11455, 294, 264, 2020, 300, 291, 393, 362, 257, 2167, 2522, 295, 8837, 51628], "temperature": 0.0, "avg_logprob": -0.09107645034790039, "compression_ratio": 1.6953125, "no_speech_prob": 0.0014889725716784596}, {"id": 724, "seek": 416224, "start": 4187.5199999999995, "end": 4191.44, "text": " that by feeding it different initial conditions, by feeding it different inputs,", "tokens": [51628, 300, 538, 12919, 309, 819, 5883, 4487, 11, 538, 12919, 309, 819, 15743, 11, 51824], "temperature": 0.0, "avg_logprob": -0.09107645034790039, "compression_ratio": 1.6953125, "no_speech_prob": 0.0014889725716784596}, {"id": 725, "seek": 419144, "start": 4191.5199999999995, "end": 4196.639999999999, "text": " you can make it compute different kinds of things. So, for example, you can have a Turing machine", "tokens": [50368, 291, 393, 652, 309, 14722, 819, 3685, 295, 721, 13, 407, 11, 337, 1365, 11, 291, 393, 362, 257, 314, 1345, 3479, 50624], "temperature": 0.0, "avg_logprob": -0.08937759148447137, "compression_ratio": 1.6685714285714286, "no_speech_prob": 0.0015672650188207626}, {"id": 726, "seek": 419144, "start": 4196.639999999999, "end": 4202.96, "text": " that has some, where just by feeding it different initial conditions, it will emulate any other", "tokens": [50624, 300, 575, 512, 11, 689, 445, 538, 12919, 309, 819, 5883, 4487, 11, 309, 486, 45497, 604, 661, 50940], "temperature": 0.0, "avg_logprob": -0.08937759148447137, "compression_ratio": 1.6685714285714286, "no_speech_prob": 0.0015672650188207626}, {"id": 727, "seek": 419144, "start": 4202.96, "end": 4210.4, "text": " Turing machine. So, for example, if we go to, in terms of Turing machines, there's some, I'm going", "tokens": [50940, 314, 1345, 3479, 13, 407, 11, 337, 1365, 11, 498, 321, 352, 281, 11, 294, 2115, 295, 314, 1345, 8379, 11, 456, 311, 512, 11, 286, 478, 516, 51312], "temperature": 0.0, "avg_logprob": -0.08937759148447137, "compression_ratio": 1.6685714285714286, "no_speech_prob": 0.0015672650188207626}, {"id": 728, "seek": 421040, "start": 4210.48, "end": 4222.879999999999, "text": " to show you something. There we go. Well, so, the big point is there exist machines that are", "tokens": [50368, 281, 855, 291, 746, 13, 821, 321, 352, 13, 1042, 11, 370, 11, 264, 955, 935, 307, 456, 2514, 8379, 300, 366, 50988], "temperature": 0.0, "avg_logprob": -0.10764522159222475, "compression_ratio": 1.8018433179723503, "no_speech_prob": 0.02123774029314518}, {"id": 729, "seek": 421040, "start": 4222.879999999999, "end": 4227.679999999999, "text": " universal in the sense that they can at least emulate all other machines of their type. The thing", "tokens": [50988, 11455, 294, 264, 2020, 300, 436, 393, 412, 1935, 45497, 439, 661, 8379, 295, 641, 2010, 13, 440, 551, 51228], "temperature": 0.0, "avg_logprob": -0.10764522159222475, "compression_ratio": 1.8018433179723503, "no_speech_prob": 0.02123774029314518}, {"id": 730, "seek": 421040, "start": 4227.679999999999, "end": 4232.879999999999, "text": " that then became clear, starting in the 1930s, is that Turing machines, you can have a Turing machine", "tokens": [51228, 300, 550, 3062, 1850, 11, 2891, 294, 264, 22350, 82, 11, 307, 300, 314, 1345, 8379, 11, 291, 393, 362, 257, 314, 1345, 3479, 51488], "temperature": 0.0, "avg_logprob": -0.10764522159222475, "compression_ratio": 1.8018433179723503, "no_speech_prob": 0.02123774029314518}, {"id": 731, "seek": 421040, "start": 4232.879999999999, "end": 4237.28, "text": " that emulates every other Turing machine. It also, by the way, can emulate every register machine,", "tokens": [51488, 300, 846, 26192, 633, 661, 314, 1345, 3479, 13, 467, 611, 11, 538, 264, 636, 11, 393, 45497, 633, 7280, 3479, 11, 51708], "temperature": 0.0, "avg_logprob": -0.10764522159222475, "compression_ratio": 1.8018433179723503, "no_speech_prob": 0.02123774029314518}, {"id": 732, "seek": 423728, "start": 4237.28, "end": 4244.48, "text": " every piece of lambda calculus, every combinator and so on. So, there's this notion that in the", "tokens": [50364, 633, 2522, 295, 13607, 33400, 11, 633, 2512, 31927, 293, 370, 322, 13, 407, 11, 456, 311, 341, 10710, 300, 294, 264, 50724], "temperature": 0.0, "avg_logprob": -0.09630450335415927, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.002858793828636408}, {"id": 733, "seek": 423728, "start": 4244.48, "end": 4250.32, "text": " class of computational devices, there's a certain degree of universality. People had not thought", "tokens": [50724, 1508, 295, 28270, 5759, 11, 456, 311, 257, 1629, 4314, 295, 5950, 1860, 13, 3432, 632, 406, 1194, 51016], "temperature": 0.0, "avg_logprob": -0.09630450335415927, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.002858793828636408}, {"id": 734, "seek": 423728, "start": 4250.32, "end": 4257.679999999999, "text": " that that extended to physics. That was the thing that basically was my effort in the 1980s was to", "tokens": [51016, 300, 300, 10913, 281, 10649, 13, 663, 390, 264, 551, 300, 1936, 390, 452, 4630, 294, 264, 13626, 82, 390, 281, 51384], "temperature": 0.0, "avg_logprob": -0.09630450335415927, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.002858793828636408}, {"id": 735, "seek": 423728, "start": 4257.679999999999, "end": 4265.04, "text": " kind of imagine that this notion that what is computationally computable would also be what", "tokens": [51384, 733, 295, 3811, 300, 341, 10710, 300, 437, 307, 24903, 379, 2807, 712, 576, 611, 312, 437, 51752], "temperature": 0.0, "avg_logprob": -0.09630450335415927, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.002858793828636408}, {"id": 736, "seek": 426504, "start": 4265.04, "end": 4270.16, "text": " is what can happen in physics. People had sort of assumed that physics kind of breaks out of", "tokens": [50364, 307, 437, 393, 1051, 294, 10649, 13, 3432, 632, 1333, 295, 15895, 300, 10649, 733, 295, 9857, 484, 295, 50620], "temperature": 0.0, "avg_logprob": -0.10293951449186906, "compression_ratio": 1.8262548262548262, "no_speech_prob": 0.011567498557269573}, {"id": 737, "seek": 426504, "start": 4270.16, "end": 4274.48, "text": " kind of this computational paradigm. It has real numbers, precise real numbers, has other kinds", "tokens": [50620, 733, 295, 341, 28270, 24709, 13, 467, 575, 957, 3547, 11, 13600, 957, 3547, 11, 575, 661, 3685, 50836], "temperature": 0.0, "avg_logprob": -0.10293951449186906, "compression_ratio": 1.8262548262548262, "no_speech_prob": 0.011567498557269573}, {"id": 738, "seek": 426504, "start": 4274.48, "end": 4280.0, "text": " of things like that. So, the first thing is the realization in the principle of computational", "tokens": [50836, 295, 721, 411, 300, 13, 407, 11, 264, 700, 551, 307, 264, 25138, 294, 264, 8665, 295, 28270, 51112], "temperature": 0.0, "avg_logprob": -0.10293951449186906, "compression_ratio": 1.8262548262548262, "no_speech_prob": 0.011567498557269573}, {"id": 739, "seek": 426504, "start": 4280.0, "end": 4285.84, "text": " equivalence. The first thing is kind of the claim that, well, first part of it is sort of the", "tokens": [51112, 9052, 655, 13, 440, 700, 551, 307, 733, 295, 264, 3932, 300, 11, 731, 11, 700, 644, 295, 309, 307, 1333, 295, 264, 51404], "temperature": 0.0, "avg_logprob": -0.10293951449186906, "compression_ratio": 1.8262548262548262, "no_speech_prob": 0.011567498557269573}, {"id": 740, "seek": 426504, "start": 4285.84, "end": 4291.44, "text": " physics part that, yes, actually, in the physical universe, this is all we've got. We can't say,", "tokens": [51404, 10649, 644, 300, 11, 2086, 11, 767, 11, 294, 264, 4001, 6445, 11, 341, 307, 439, 321, 600, 658, 13, 492, 393, 380, 584, 11, 51684], "temperature": 0.0, "avg_logprob": -0.10293951449186906, "compression_ratio": 1.8262548262548262, "no_speech_prob": 0.011567498557269573}, {"id": 741, "seek": 429144, "start": 4291.5199999999995, "end": 4299.2, "text": " oh, we're going to make an analog computer that jumps beyond kind of the church Turing level.", "tokens": [50368, 1954, 11, 321, 434, 516, 281, 652, 364, 16660, 3820, 300, 16704, 4399, 733, 295, 264, 4128, 314, 1345, 1496, 13, 50752], "temperature": 0.0, "avg_logprob": -0.09787902426212391, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.013575478456914425}, {"id": 742, "seek": 429144, "start": 4299.2, "end": 4305.839999999999, "text": " Second point is this. People imagined that to make a universal machine was a complicated", "tokens": [50752, 5736, 935, 307, 341, 13, 3432, 16590, 300, 281, 652, 257, 11455, 3479, 390, 257, 6179, 51084], "temperature": 0.0, "avg_logprob": -0.09787902426212391, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.013575478456914425}, {"id": 743, "seek": 429144, "start": 4305.839999999999, "end": 4311.679999999999, "text": " matter. It was something that would be kind of, you have to build this whole microprocessor. It", "tokens": [51084, 1871, 13, 467, 390, 746, 300, 576, 312, 733, 295, 11, 291, 362, 281, 1322, 341, 1379, 3123, 1513, 340, 25432, 13, 467, 51376], "temperature": 0.0, "avg_logprob": -0.09787902426212391, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.013575478456914425}, {"id": 744, "seek": 429144, "start": 4311.679999999999, "end": 4316.4, "text": " might have a billion gates in it. It has all these instructions. It's got if statements. It's got all", "tokens": [51376, 1062, 362, 257, 5218, 19792, 294, 309, 13, 467, 575, 439, 613, 9415, 13, 467, 311, 658, 498, 12363, 13, 467, 311, 658, 439, 51612], "temperature": 0.0, "avg_logprob": -0.09787902426212391, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.013575478456914425}, {"id": 745, "seek": 431640, "start": 4316.4, "end": 4321.12, "text": " this kind of structure. And the question is, well, what's, you know, is that really necessary?", "tokens": [50364, 341, 733, 295, 3877, 13, 400, 264, 1168, 307, 11, 731, 11, 437, 311, 11, 291, 458, 11, 307, 300, 534, 4818, 30, 50600], "temperature": 0.0, "avg_logprob": -0.07943594644940088, "compression_ratio": 1.8969072164948453, "no_speech_prob": 0.07527143508195877}, {"id": 746, "seek": 431640, "start": 4321.12, "end": 4326.24, "text": " Or is universal computation actually something much more naturally occurring? Is universal", "tokens": [50600, 1610, 307, 11455, 24903, 767, 746, 709, 544, 8195, 18386, 30, 1119, 11455, 50856], "temperature": 0.0, "avg_logprob": -0.07943594644940088, "compression_ratio": 1.8969072164948453, "no_speech_prob": 0.07527143508195877}, {"id": 747, "seek": 431640, "start": 4326.24, "end": 4330.32, "text": " computation a special thing? Have to go to a lot of trouble to get? Or is it something that's just", "tokens": [50856, 24903, 257, 2121, 551, 30, 3560, 281, 352, 281, 257, 688, 295, 5253, 281, 483, 30, 1610, 307, 309, 746, 300, 311, 445, 51060], "temperature": 0.0, "avg_logprob": -0.07943594644940088, "compression_ratio": 1.8969072164948453, "no_speech_prob": 0.07527143508195877}, {"id": 748, "seek": 431640, "start": 4330.32, "end": 4334.5599999999995, "text": " sort of lying around the computational universe? One of the big points to the principle of", "tokens": [51060, 1333, 295, 8493, 926, 264, 28270, 6445, 30, 1485, 295, 264, 955, 2793, 281, 264, 8665, 295, 51272], "temperature": 0.0, "avg_logprob": -0.07943594644940088, "compression_ratio": 1.8969072164948453, "no_speech_prob": 0.07527143508195877}, {"id": 749, "seek": 431640, "start": 4334.5599999999995, "end": 4339.12, "text": " computational equivalence is, yes, it's just lying around the computational universe. So,", "tokens": [51272, 28270, 9052, 655, 307, 11, 2086, 11, 309, 311, 445, 8493, 926, 264, 28270, 6445, 13, 407, 11, 51500], "temperature": 0.0, "avg_logprob": -0.07943594644940088, "compression_ratio": 1.8969072164948453, "no_speech_prob": 0.07527143508195877}, {"id": 750, "seek": 431640, "start": 4339.12, "end": 4345.36, "text": " for example, if we look at these different possible rules here, some of them behave in", "tokens": [51500, 337, 1365, 11, 498, 321, 574, 412, 613, 819, 1944, 4474, 510, 11, 512, 295, 552, 15158, 294, 51812], "temperature": 0.0, "avg_logprob": -0.07943594644940088, "compression_ratio": 1.8969072164948453, "no_speech_prob": 0.07527143508195877}, {"id": 751, "seek": 434536, "start": 4345.44, "end": 4348.48, "text": " such simple ways that we can readily see what they're going to do. They're computationally", "tokens": [50368, 1270, 2199, 2098, 300, 321, 393, 26336, 536, 437, 436, 434, 516, 281, 360, 13, 814, 434, 24903, 379, 50520], "temperature": 0.0, "avg_logprob": -0.11284881432851156, "compression_ratio": 1.7041198501872659, "no_speech_prob": 0.0039125001057982445}, {"id": 752, "seek": 434536, "start": 4348.48, "end": 4354.08, "text": " reducible. There's nothing more to say. But some of them behave in a complicated enough way", "tokens": [50520, 2783, 32128, 13, 821, 311, 1825, 544, 281, 584, 13, 583, 512, 295, 552, 15158, 294, 257, 6179, 1547, 636, 50800], "temperature": 0.0, "avg_logprob": -0.11284881432851156, "compression_ratio": 1.7041198501872659, "no_speech_prob": 0.0039125001057982445}, {"id": 753, "seek": 434536, "start": 4354.08, "end": 4358.4, "text": " that we're kind of not really sure what they do. Let me show you an example of one of those. So,", "tokens": [50800, 300, 321, 434, 733, 295, 406, 534, 988, 437, 436, 360, 13, 961, 385, 855, 291, 364, 1365, 295, 472, 295, 729, 13, 407, 11, 51016], "temperature": 0.0, "avg_logprob": -0.11284881432851156, "compression_ratio": 1.7041198501872659, "no_speech_prob": 0.0039125001057982445}, {"id": 754, "seek": 434536, "start": 4358.4, "end": 4367.599999999999, "text": " this is, let me show you rule 110. This thing actually only grows on one side here, but", "tokens": [51016, 341, 307, 11, 718, 385, 855, 291, 4978, 20154, 13, 639, 551, 767, 787, 13156, 322, 472, 1252, 510, 11, 457, 51476], "temperature": 0.0, "avg_logprob": -0.11284881432851156, "compression_ratio": 1.7041198501872659, "no_speech_prob": 0.0039125001057982445}, {"id": 755, "seek": 434536, "start": 4368.88, "end": 4373.28, "text": " just show that. I shall make it, just show just the part where it's growing. So that's,", "tokens": [51540, 445, 855, 300, 13, 286, 4393, 652, 309, 11, 445, 855, 445, 264, 644, 689, 309, 311, 4194, 13, 407, 300, 311, 11, 51760], "temperature": 0.0, "avg_logprob": -0.11284881432851156, "compression_ratio": 1.7041198501872659, "no_speech_prob": 0.0039125001057982445}, {"id": 756, "seek": 437328, "start": 4373.759999999999, "end": 4379.04, "text": " after 200 steps, let's run it for 1,000 steps. Okay, there it is. It's a little bit unclear what", "tokens": [50388, 934, 2331, 4439, 11, 718, 311, 1190, 309, 337, 502, 11, 1360, 4439, 13, 1033, 11, 456, 309, 307, 13, 467, 311, 257, 707, 857, 25636, 437, 50652], "temperature": 0.0, "avg_logprob": -0.09150625775744031, "compression_ratio": 1.9222614840989398, "no_speech_prob": 0.004365275148302317}, {"id": 757, "seek": 437328, "start": 4379.04, "end": 4383.599999999999, "text": " it's going to do. This is kind of computational irreducibility in action or undecidability,", "tokens": [50652, 309, 311, 516, 281, 360, 13, 639, 307, 733, 295, 28270, 16014, 769, 537, 39802, 294, 3069, 420, 674, 3045, 327, 2310, 11, 50880], "temperature": 0.0, "avg_logprob": -0.09150625775744031, "compression_ratio": 1.9222614840989398, "no_speech_prob": 0.004365275148302317}, {"id": 758, "seek": 437328, "start": 4383.599999999999, "end": 4387.759999999999, "text": " ultimately in action. What's it going to do? Is it going to have all those little things,", "tokens": [50880, 6284, 294, 3069, 13, 708, 311, 309, 516, 281, 360, 30, 1119, 309, 516, 281, 362, 439, 729, 707, 721, 11, 51088], "temperature": 0.0, "avg_logprob": -0.09150625775744031, "compression_ratio": 1.9222614840989398, "no_speech_prob": 0.004365275148302317}, {"id": 759, "seek": 437328, "start": 4387.759999999999, "end": 4390.48, "text": " structures? Are they going to survive or are they eventually going to die out?", "tokens": [51088, 9227, 30, 2014, 436, 516, 281, 7867, 420, 366, 436, 4728, 516, 281, 978, 484, 30, 51224], "temperature": 0.0, "avg_logprob": -0.09150625775744031, "compression_ratio": 1.9222614840989398, "no_speech_prob": 0.004365275148302317}, {"id": 760, "seek": 437328, "start": 4391.12, "end": 4397.2, "text": " After, I think it's about 4,500 steps, they do eventually all die out and they just get left", "tokens": [51256, 2381, 11, 286, 519, 309, 311, 466, 1017, 11, 7526, 4439, 11, 436, 360, 4728, 439, 978, 484, 293, 436, 445, 483, 1411, 51560], "temperature": 0.0, "avg_logprob": -0.09150625775744031, "compression_ratio": 1.9222614840989398, "no_speech_prob": 0.004365275148302317}, {"id": 761, "seek": 437328, "start": 4397.2, "end": 4401.679999999999, "text": " with this one single structure here. But this kind of computational irreducibility in action,", "tokens": [51560, 365, 341, 472, 2167, 3877, 510, 13, 583, 341, 733, 295, 28270, 16014, 769, 537, 39802, 294, 3069, 11, 51784], "temperature": 0.0, "avg_logprob": -0.09150625775744031, "compression_ratio": 1.9222614840989398, "no_speech_prob": 0.004365275148302317}, {"id": 762, "seek": 440168, "start": 4401.68, "end": 4407.04, "text": " you can't tell what's going on. This particular rule turns out, if you just look at it, let's", "tokens": [50364, 291, 393, 380, 980, 437, 311, 516, 322, 13, 639, 1729, 4978, 4523, 484, 11, 498, 291, 445, 574, 412, 309, 11, 718, 311, 50632], "temperature": 0.0, "avg_logprob": -0.14462708234786986, "compression_ratio": 1.4414893617021276, "no_speech_prob": 0.0033027648460119963}, {"id": 763, "seek": 440168, "start": 4407.04, "end": 4415.6, "text": " start it off from random initial conditions. Let's say 600 across. No, let's say 1,000 across.", "tokens": [50632, 722, 309, 766, 490, 4974, 5883, 4487, 13, 961, 311, 584, 11849, 2108, 13, 883, 11, 718, 311, 584, 502, 11, 1360, 2108, 13, 51060], "temperature": 0.0, "avg_logprob": -0.14462708234786986, "compression_ratio": 1.4414893617021276, "no_speech_prob": 0.0033027648460119963}, {"id": 764, "seek": 440168, "start": 4416.8, "end": 4425.280000000001, "text": " And let's say 800 down. Okay, oh boy. It's a little bit on the screen here. Let me", "tokens": [51120, 400, 718, 311, 584, 13083, 760, 13, 1033, 11, 1954, 3237, 13, 467, 311, 257, 707, 857, 322, 264, 2568, 510, 13, 961, 385, 51544], "temperature": 0.0, "avg_logprob": -0.14462708234786986, "compression_ratio": 1.4414893617021276, "no_speech_prob": 0.0033027648460119963}, {"id": 765, "seek": 442528, "start": 4426.24, "end": 4433.04, "text": " make it a bit bigger. Okay, there we go. So what you see there is that's the initial condition.", "tokens": [50412, 652, 309, 257, 857, 3801, 13, 1033, 11, 456, 321, 352, 13, 407, 437, 291, 536, 456, 307, 300, 311, 264, 5883, 4188, 13, 50752], "temperature": 0.0, "avg_logprob": -0.12313964502598213, "compression_ratio": 1.7148014440433212, "no_speech_prob": 0.013179724104702473}, {"id": 766, "seek": 442528, "start": 4433.679999999999, "end": 4438.639999999999, "text": " This is what happens. What you see is a bunch of little structures here. And you might imagine as", "tokens": [50784, 639, 307, 437, 2314, 13, 708, 291, 536, 307, 257, 3840, 295, 707, 9227, 510, 13, 400, 291, 1062, 3811, 382, 51032], "temperature": 0.0, "avg_logprob": -0.12313964502598213, "compression_ratio": 1.7148014440433212, "no_speech_prob": 0.013179724104702473}, {"id": 767, "seek": 442528, "start": 4438.639999999999, "end": 4442.32, "text": " you look at these structures, oh, they're kind of interacting and maybe that's like a logic gate", "tokens": [51032, 291, 574, 412, 613, 9227, 11, 1954, 11, 436, 434, 733, 295, 18017, 293, 1310, 300, 311, 411, 257, 9952, 8539, 51216], "temperature": 0.0, "avg_logprob": -0.12313964502598213, "compression_ratio": 1.7148014440433212, "no_speech_prob": 0.013179724104702473}, {"id": 768, "seek": 442528, "start": 4442.32, "end": 4446.96, "text": " and maybe we can make an OR gate out of this and so on. But it turns out with considerable", "tokens": [51216, 293, 1310, 321, 393, 652, 364, 19654, 8539, 484, 295, 341, 293, 370, 322, 13, 583, 309, 4523, 484, 365, 24167, 51448], "temperature": 0.0, "avg_logprob": -0.12313964502598213, "compression_ratio": 1.7148014440433212, "no_speech_prob": 0.013179724104702473}, {"id": 769, "seek": 442528, "start": 4446.96, "end": 4452.4, "text": " effort, you can do that. And you can show that rule 110, which is kind of just the 110th rule", "tokens": [51448, 4630, 11, 291, 393, 360, 300, 13, 400, 291, 393, 855, 300, 4978, 20154, 11, 597, 307, 733, 295, 445, 264, 20154, 392, 4978, 51720], "temperature": 0.0, "avg_logprob": -0.12313964502598213, "compression_ratio": 1.7148014440433212, "no_speech_prob": 0.013179724104702473}, {"id": 770, "seek": 445240, "start": 4452.4, "end": 4457.12, "text": " in this very simple enumeration of possible rules. It's universal. The first one that you", "tokens": [50364, 294, 341, 588, 2199, 465, 449, 5053, 295, 1944, 4474, 13, 467, 311, 11455, 13, 440, 700, 472, 300, 291, 50600], "temperature": 0.0, "avg_logprob": -0.08963843642688188, "compression_ratio": 1.787375415282392, "no_speech_prob": 0.00879730749875307}, {"id": 771, "seek": 445240, "start": 4457.12, "end": 4462.24, "text": " might imagine could be universal is actually universal. So in a sense, you know, the church", "tokens": [50600, 1062, 3811, 727, 312, 11455, 307, 767, 11455, 13, 407, 294, 257, 2020, 11, 291, 458, 11, 264, 4128, 50856], "temperature": 0.0, "avg_logprob": -0.08963843642688188, "compression_ratio": 1.787375415282392, "no_speech_prob": 0.00879730749875307}, {"id": 772, "seek": 445240, "start": 4462.24, "end": 4466.639999999999, "text": " touring thesis is saying it is possible to have a universal machine, at least universal within the", "tokens": [50856, 32487, 22288, 307, 1566, 309, 307, 1944, 281, 362, 257, 11455, 3479, 11, 412, 1935, 11455, 1951, 264, 51076], "temperature": 0.0, "avg_logprob": -0.08963843642688188, "compression_ratio": 1.787375415282392, "no_speech_prob": 0.00879730749875307}, {"id": 773, "seek": 445240, "start": 4466.639999999999, "end": 4470.4, "text": " class of computational devices that we're talking about. The principle of computational", "tokens": [51076, 1508, 295, 28270, 5759, 300, 321, 434, 1417, 466, 13, 440, 8665, 295, 28270, 51264], "temperature": 0.0, "avg_logprob": -0.08963843642688188, "compression_ratio": 1.787375415282392, "no_speech_prob": 0.00879730749875307}, {"id": 774, "seek": 445240, "start": 4470.4, "end": 4475.36, "text": " equivalence says not only is it possible, it's also generically the case it is ubiquitous.", "tokens": [51264, 9052, 655, 1619, 406, 787, 307, 309, 1944, 11, 309, 311, 611, 1337, 984, 264, 1389, 309, 307, 43868, 39831, 13, 51512], "temperature": 0.0, "avg_logprob": -0.08963843642688188, "compression_ratio": 1.787375415282392, "no_speech_prob": 0.00879730749875307}, {"id": 775, "seek": 445240, "start": 4475.92, "end": 4480.799999999999, "text": " And in fact, it goes on to talk more about individual computations rather than", "tokens": [51540, 400, 294, 1186, 11, 309, 1709, 322, 281, 751, 544, 466, 2609, 2807, 763, 2831, 813, 51784], "temperature": 0.0, "avg_logprob": -0.08963843642688188, "compression_ratio": 1.787375415282392, "no_speech_prob": 0.00879730749875307}, {"id": 776, "seek": 448080, "start": 4480.8, "end": 4485.92, "text": " programmability, but that's kind of a bonus. By the way, in terms of touring machines,", "tokens": [50364, 37648, 2310, 11, 457, 300, 311, 733, 295, 257, 10882, 13, 3146, 264, 636, 11, 294, 2115, 295, 32487, 8379, 11, 50620], "temperature": 0.0, "avg_logprob": -0.09982615186457049, "compression_ratio": 1.6690909090909092, "no_speech_prob": 0.0038569425232708454}, {"id": 777, "seek": 448080, "start": 4485.92, "end": 4490.08, "text": " I was very curious what is, you know, if you just look out in the space of possible touring", "tokens": [50620, 286, 390, 588, 6369, 437, 307, 11, 291, 458, 11, 498, 291, 445, 574, 484, 294, 264, 1901, 295, 1944, 32487, 50828], "temperature": 0.0, "avg_logprob": -0.09982615186457049, "compression_ratio": 1.6690909090909092, "no_speech_prob": 0.0038569425232708454}, {"id": 778, "seek": 448080, "start": 4490.08, "end": 4495.28, "text": " machines, just start enumerating touring machines. The first one whose behavior is not", "tokens": [50828, 8379, 11, 445, 722, 465, 15583, 990, 32487, 8379, 13, 440, 700, 472, 6104, 5223, 307, 406, 51088], "temperature": 0.0, "avg_logprob": -0.09982615186457049, "compression_ratio": 1.6690909090909092, "no_speech_prob": 0.0038569425232708454}, {"id": 779, "seek": 448080, "start": 4495.28, "end": 4499.92, "text": " obviously simple as this one here that I found sometime in the 1990s. And so then I was really", "tokens": [51088, 2745, 2199, 382, 341, 472, 510, 300, 286, 1352, 15053, 294, 264, 13384, 82, 13, 400, 370, 550, 286, 390, 534, 51320], "temperature": 0.0, "avg_logprob": -0.09982615186457049, "compression_ratio": 1.6690909090909092, "no_speech_prob": 0.0038569425232708454}, {"id": 780, "seek": 448080, "start": 4499.92, "end": 4507.2, "text": " curious, is this in fact a universal machine in 2007? I put up this little prize and a chap called", "tokens": [51320, 6369, 11, 307, 341, 294, 1186, 257, 11455, 3479, 294, 12656, 30, 286, 829, 493, 341, 707, 12818, 293, 257, 13223, 1219, 51684], "temperature": 0.0, "avg_logprob": -0.09982615186457049, "compression_ratio": 1.6690909090909092, "no_speech_prob": 0.0038569425232708454}, {"id": 781, "seek": 450720, "start": 4507.2, "end": 4513.92, "text": " Alex Smith managed to show that yes, this particular touring machine, the first conceivably", "tokens": [50364, 5202, 8538, 6453, 281, 855, 300, 2086, 11, 341, 1729, 32487, 3479, 11, 264, 700, 10413, 592, 1188, 50700], "temperature": 0.0, "avg_logprob": -0.12387537956237793, "compression_ratio": 1.7772511848341233, "no_speech_prob": 0.0024858457036316395}, {"id": 782, "seek": 450720, "start": 4513.92, "end": 4518.5599999999995, "text": " universal touring machine actually is universal, which is a nice piece of evidence for the principle", "tokens": [50700, 11455, 32487, 3479, 767, 307, 11455, 11, 597, 307, 257, 1481, 2522, 295, 4467, 337, 264, 8665, 50932], "temperature": 0.0, "avg_logprob": -0.12387537956237793, "compression_ratio": 1.7772511848341233, "no_speech_prob": 0.0024858457036316395}, {"id": 783, "seek": 450720, "start": 4518.5599999999995, "end": 4524.16, "text": " of computational equivalence. So that's kind of the relationship between church touring and", "tokens": [50932, 295, 28270, 9052, 655, 13, 407, 300, 311, 733, 295, 264, 2480, 1296, 4128, 32487, 293, 51212], "temperature": 0.0, "avg_logprob": -0.12387537956237793, "compression_ratio": 1.7772511848341233, "no_speech_prob": 0.0024858457036316395}, {"id": 784, "seek": 450720, "start": 4525.28, "end": 4530.4, "text": " principle of computational equivalence. And computational irreducibility is something that", "tokens": [51268, 8665, 295, 28270, 9052, 655, 13, 400, 28270, 16014, 769, 537, 39802, 307, 746, 300, 51524], "temperature": 0.0, "avg_logprob": -0.12387537956237793, "compression_ratio": 1.7772511848341233, "no_speech_prob": 0.0024858457036316395}, {"id": 785, "seek": 453040, "start": 4531.04, "end": 4538.719999999999, "text": " is sort of, it's made tougher by the fact that this property of universality", "tokens": [50396, 307, 1333, 295, 11, 309, 311, 1027, 30298, 538, 264, 1186, 300, 341, 4707, 295, 5950, 1860, 50780], "temperature": 0.0, "avg_logprob": -0.2808795383998326, "compression_ratio": 1.423913043478261, "no_speech_prob": 0.037325579673051834}, {"id": 786, "seek": 453040, "start": 4538.719999999999, "end": 4544.16, "text": " is ubiquitous in the computational universe. Okay, thank you. MP completeness. What's that?", "tokens": [50780, 307, 43868, 39831, 294, 264, 28270, 6445, 13, 1033, 11, 1309, 291, 13, 14146, 1557, 15264, 13, 708, 311, 300, 30, 51052], "temperature": 0.0, "avg_logprob": -0.2808795383998326, "compression_ratio": 1.423913043478261, "no_speech_prob": 0.037325579673051834}, {"id": 787, "seek": 453040, "start": 4544.719999999999, "end": 4551.599999999999, "text": " We have a question here already, so. Wait a second. You get me. My software is not empty yet.", "tokens": [51080, 492, 362, 257, 1168, 510, 1217, 11, 370, 13, 3802, 257, 1150, 13, 509, 483, 385, 13, 1222, 4722, 307, 406, 6707, 1939, 13, 51424], "temperature": 0.0, "avg_logprob": -0.2808795383998326, "compression_ratio": 1.423913043478261, "no_speech_prob": 0.037325579673051834}, {"id": 788, "seek": 455160, "start": 4552.0, "end": 4558.64, "text": " So you asked about NP completeness. So let me try and address that. So", "tokens": [50384, 407, 291, 2351, 466, 38611, 1557, 15264, 13, 407, 718, 385, 853, 293, 2985, 300, 13, 407, 50716], "temperature": 0.0, "avg_logprob": -0.17687183437925397, "compression_ratio": 1.5411764705882354, "no_speech_prob": 0.016861509531736374}, {"id": 789, "seek": 455160, "start": 4560.88, "end": 4567.76, "text": " normally, in a touring machine, for example, you have this touring machine, it has a rule,", "tokens": [50828, 5646, 11, 294, 257, 32487, 3479, 11, 337, 1365, 11, 291, 362, 341, 32487, 3479, 11, 309, 575, 257, 4978, 11, 51172], "temperature": 0.0, "avg_logprob": -0.17687183437925397, "compression_ratio": 1.5411764705882354, "no_speech_prob": 0.016861509531736374}, {"id": 790, "seek": 455160, "start": 4567.76, "end": 4574.240000000001, "text": " you started off from some initial condition, it just evolves in some specific way. It has a specific", "tokens": [51172, 291, 1409, 766, 490, 512, 5883, 4188, 11, 309, 445, 43737, 294, 512, 2685, 636, 13, 467, 575, 257, 2685, 51496], "temperature": 0.0, "avg_logprob": -0.17687183437925397, "compression_ratio": 1.5411764705882354, "no_speech_prob": 0.016861509531736374}, {"id": 791, "seek": 457424, "start": 4574.24, "end": 4578.88, "text": " history. But you can also have, let's see if I have a picture of this.", "tokens": [50364, 2503, 13, 583, 291, 393, 611, 362, 11, 718, 311, 536, 498, 286, 362, 257, 3036, 295, 341, 13, 50596], "temperature": 0.0, "avg_logprob": -0.2366245360601516, "compression_ratio": 1.2727272727272727, "no_speech_prob": 0.025927158072590828}, {"id": 792, "seek": 457424, "start": 4595.44, "end": 4598.8, "text": " I have a bunch of these multiway systems. Well, here, let me show you", "tokens": [51424, 286, 362, 257, 3840, 295, 613, 4825, 676, 3652, 13, 1042, 11, 510, 11, 718, 385, 855, 291, 51592], "temperature": 0.0, "avg_logprob": -0.2366245360601516, "compression_ratio": 1.2727272727272727, "no_speech_prob": 0.025927158072590828}, {"id": 793, "seek": 460424, "start": 4605.2, "end": 4607.2, "text": " a tag.", "tokens": [50412, 257, 6162, 13, 50512], "temperature": 0.0, "avg_logprob": -0.3384554337482063, "compression_ratio": 1.4297520661157024, "no_speech_prob": 0.0011413348838686943}, {"id": 794, "seek": 460424, "start": 4618.4, "end": 4624.88, "text": " Find some multiway touring machines. There we go. Multiway touring machines. Okay. So", "tokens": [51072, 11809, 512, 4825, 676, 32487, 8379, 13, 821, 321, 352, 13, 29238, 676, 32487, 8379, 13, 1033, 13, 407, 51396], "temperature": 0.0, "avg_logprob": -0.3384554337482063, "compression_ratio": 1.4297520661157024, "no_speech_prob": 0.0011413348838686943}, {"id": 795, "seek": 460424, "start": 4627.5199999999995, "end": 4631.5199999999995, "text": " that's a typical touring machine. It has a rule, it evolves in a particular way.", "tokens": [51528, 300, 311, 257, 7476, 32487, 3479, 13, 467, 575, 257, 4978, 11, 309, 43737, 294, 257, 1729, 636, 13, 51728], "temperature": 0.0, "avg_logprob": -0.3384554337482063, "compression_ratio": 1.4297520661157024, "no_speech_prob": 0.0011413348838686943}, {"id": 796, "seek": 463152, "start": 4632.240000000001, "end": 4636.96, "text": " But you can also have a multiway touring machine in which there isn't just a single", "tokens": [50400, 583, 291, 393, 611, 362, 257, 4825, 676, 32487, 3479, 294, 597, 456, 1943, 380, 445, 257, 2167, 50636], "temperature": 0.0, "avg_logprob": -0.07916858461168078, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0024025135207921267}, {"id": 797, "seek": 463152, "start": 4636.96, "end": 4640.64, "text": " possible path of evolution, but there are many paths. So you can end up with this", "tokens": [50636, 1944, 3100, 295, 9303, 11, 457, 456, 366, 867, 14518, 13, 407, 291, 393, 917, 493, 365, 341, 50820], "temperature": 0.0, "avg_logprob": -0.07916858461168078, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0024025135207921267}, {"id": 798, "seek": 463152, "start": 4640.64, "end": 4644.8, "text": " kind of branching structure. This turns out to be closely related to what happens in quantum", "tokens": [50820, 733, 295, 9819, 278, 3877, 13, 639, 4523, 484, 281, 312, 8185, 4077, 281, 437, 2314, 294, 13018, 51028], "temperature": 0.0, "avg_logprob": -0.07916858461168078, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0024025135207921267}, {"id": 799, "seek": 463152, "start": 4644.8, "end": 4652.080000000001, "text": " mechanics. That's a separate issue. But so this idea of NP completeness, NP problems versus", "tokens": [51028, 12939, 13, 663, 311, 257, 4994, 2734, 13, 583, 370, 341, 1558, 295, 38611, 1557, 15264, 11, 38611, 2740, 5717, 51392], "temperature": 0.0, "avg_logprob": -0.07916858461168078, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0024025135207921267}, {"id": 800, "seek": 463152, "start": 4652.080000000001, "end": 4658.0, "text": " P problems and so on, it's this question. If we have a touring machine and it computes something", "tokens": [51392, 430, 2740, 293, 370, 322, 11, 309, 311, 341, 1168, 13, 759, 321, 362, 257, 32487, 3479, 293, 309, 715, 1819, 746, 51688], "temperature": 0.0, "avg_logprob": -0.07916858461168078, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0024025135207921267}, {"id": 801, "seek": 465800, "start": 4658.08, "end": 4662.56, "text": " and it takes a certain amount of number of steps to compute it, an ordinary touring machine", "tokens": [50368, 293, 309, 2516, 257, 1629, 2372, 295, 1230, 295, 4439, 281, 14722, 309, 11, 364, 10547, 32487, 3479, 50592], "temperature": 0.0, "avg_logprob": -0.07118163964687249, "compression_ratio": 1.7984790874524714, "no_speech_prob": 0.02348758466541767}, {"id": 802, "seek": 465800, "start": 4662.56, "end": 4668.16, "text": " might take n squared steps to compute a size n version of some problem. But we can also have", "tokens": [50592, 1062, 747, 297, 8889, 4439, 281, 14722, 257, 2744, 297, 3037, 295, 512, 1154, 13, 583, 321, 393, 611, 362, 50872], "temperature": 0.0, "avg_logprob": -0.07118163964687249, "compression_ratio": 1.7984790874524714, "no_speech_prob": 0.02348758466541767}, {"id": 803, "seek": 465800, "start": 4668.16, "end": 4673.2, "text": " a non-deterministic touring machine. We can have a multiway touring machine that follows many", "tokens": [50872, 257, 2107, 12, 49136, 259, 3142, 32487, 3479, 13, 492, 393, 362, 257, 4825, 676, 32487, 3479, 300, 10002, 867, 51124], "temperature": 0.0, "avg_logprob": -0.07118163964687249, "compression_ratio": 1.7984790874524714, "no_speech_prob": 0.02348758466541767}, {"id": 804, "seek": 465800, "start": 4673.2, "end": 4679.04, "text": " different possible paths. And we say, if we have a path that gets to the answer, then it's a winner,", "tokens": [51124, 819, 1944, 14518, 13, 400, 321, 584, 11, 498, 321, 362, 257, 3100, 300, 2170, 281, 264, 1867, 11, 550, 309, 311, 257, 8507, 11, 51416], "temperature": 0.0, "avg_logprob": -0.07118163964687249, "compression_ratio": 1.7984790874524714, "no_speech_prob": 0.02348758466541767}, {"id": 805, "seek": 465800, "start": 4679.04, "end": 4684.32, "text": " so to speak. And that's the story of NP problems, non-deterministic polynomial time problems,", "tokens": [51416, 370, 281, 1710, 13, 400, 300, 311, 264, 1657, 295, 38611, 2740, 11, 2107, 12, 49136, 259, 3142, 26110, 565, 2740, 11, 51680], "temperature": 0.0, "avg_logprob": -0.07118163964687249, "compression_ratio": 1.7984790874524714, "no_speech_prob": 0.02348758466541767}, {"id": 806, "seek": 468432, "start": 4684.48, "end": 4690.719999999999, "text": " ones where there exists a path in this multiway touring machine, which gets you to that answer.", "tokens": [50372, 2306, 689, 456, 8198, 257, 3100, 294, 341, 4825, 676, 32487, 3479, 11, 597, 2170, 291, 281, 300, 1867, 13, 50684], "temperature": 0.0, "avg_logprob": -0.12976311600726584, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.0034770916681736708}, {"id": 807, "seek": 468432, "start": 4691.5199999999995, "end": 4700.48, "text": " So in a sense, this question of NP, sort of the big question, is P equal to NP? Is the class of", "tokens": [50724, 407, 294, 257, 2020, 11, 341, 1168, 295, 38611, 11, 1333, 295, 264, 955, 1168, 11, 307, 430, 2681, 281, 38611, 30, 1119, 264, 1508, 295, 51172], "temperature": 0.0, "avg_logprob": -0.12976311600726584, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.0034770916681736708}, {"id": 808, "seek": 468432, "start": 4700.48, "end": 4708.639999999999, "text": " problems that you can solve with in polynomial time with an ordinary touring machine, the same", "tokens": [51172, 2740, 300, 291, 393, 5039, 365, 294, 26110, 565, 365, 364, 10547, 32487, 3479, 11, 264, 912, 51580], "temperature": 0.0, "avg_logprob": -0.12976311600726584, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.0034770916681736708}, {"id": 809, "seek": 468432, "start": 4708.639999999999, "end": 4713.44, "text": " class or different class, than the ones that you can solve with a non-deterministic", "tokens": [51580, 1508, 420, 819, 1508, 11, 813, 264, 2306, 300, 291, 393, 5039, 365, 257, 2107, 12, 49136, 259, 3142, 51820], "temperature": 0.0, "avg_logprob": -0.12976311600726584, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.0034770916681736708}, {"id": 810, "seek": 471344, "start": 4714.08, "end": 4722.24, "text": " with a multiway touring machine? And actually, that question, so, well, let's see. I mean,", "tokens": [50396, 365, 257, 4825, 676, 32487, 3479, 30, 400, 767, 11, 300, 1168, 11, 370, 11, 731, 11, 718, 311, 536, 13, 286, 914, 11, 50804], "temperature": 0.0, "avg_logprob": -0.11508512496948242, "compression_ratio": 1.4945652173913044, "no_speech_prob": 0.002565073547884822}, {"id": 811, "seek": 471344, "start": 4722.24, "end": 4726.08, "text": " we can talk about computational irreducibility and its relationship to computational complexity", "tokens": [50804, 321, 393, 751, 466, 28270, 16014, 769, 537, 39802, 293, 1080, 2480, 281, 28270, 14024, 50996], "temperature": 0.0, "avg_logprob": -0.11508512496948242, "compression_ratio": 1.4945652173913044, "no_speech_prob": 0.002565073547884822}, {"id": 812, "seek": 471344, "start": 4726.08, "end": 4731.5199999999995, "text": " theory in general. But NP completeness in particular, there's perhaps a more interesting", "tokens": [50996, 5261, 294, 2674, 13, 583, 38611, 1557, 15264, 294, 1729, 11, 456, 311, 4317, 257, 544, 1880, 51268], "temperature": 0.0, "avg_logprob": -0.11508512496948242, "compression_ratio": 1.4945652173913044, "no_speech_prob": 0.002565073547884822}, {"id": 813, "seek": 473152, "start": 4731.52, "end": 4748.160000000001, "text": " thing to say, which is, if I can find this one. Okay, so we can look at all possible touring machines.", "tokens": [50364, 551, 281, 584, 11, 597, 307, 11, 498, 286, 393, 915, 341, 472, 13, 1033, 11, 370, 321, 393, 574, 412, 439, 1944, 32487, 8379, 13, 51196], "temperature": 0.0, "avg_logprob": -0.16748935893430547, "compression_ratio": 1.3509933774834437, "no_speech_prob": 0.051567599177360535}, {"id": 814, "seek": 473152, "start": 4748.160000000001, "end": 4756.8, "text": " This is in a sense in rural space. Where is this? Nice picture somewhere here. Do I? Yes, here we go.", "tokens": [51196, 639, 307, 294, 257, 2020, 294, 11165, 1901, 13, 2305, 307, 341, 30, 5490, 3036, 4079, 510, 13, 1144, 286, 30, 1079, 11, 510, 321, 352, 13, 51628], "temperature": 0.0, "avg_logprob": -0.16748935893430547, "compression_ratio": 1.3509933774834437, "no_speech_prob": 0.051567599177360535}, {"id": 815, "seek": 475680, "start": 4757.360000000001, "end": 4761.68, "text": " Okay, so this is a picture of kind of the behavior of all possible multiway touring", "tokens": [50392, 1033, 11, 370, 341, 307, 257, 3036, 295, 733, 295, 264, 5223, 295, 439, 1944, 4825, 676, 32487, 50608], "temperature": 0.0, "avg_logprob": -0.07432242556735202, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0024253318551927805}, {"id": 816, "seek": 475680, "start": 4761.68, "end": 4768.320000000001, "text": " machines. So in a sense, all possible programs. And this is showing sort of all possible", "tokens": [50608, 8379, 13, 407, 294, 257, 2020, 11, 439, 1944, 4268, 13, 400, 341, 307, 4099, 1333, 295, 439, 1944, 50940], "temperature": 0.0, "avg_logprob": -0.07432242556735202, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0024253318551927805}, {"id": 817, "seek": 475680, "start": 4768.320000000001, "end": 4775.6, "text": " non-deterministic programs. The red part is the part that's showing deterministic programs only.", "tokens": [50940, 2107, 12, 49136, 259, 3142, 4268, 13, 440, 2182, 644, 307, 264, 644, 300, 311, 4099, 15957, 3142, 4268, 787, 13, 51304], "temperature": 0.0, "avg_logprob": -0.07432242556735202, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0024253318551927805}, {"id": 818, "seek": 475680, "start": 4775.6, "end": 4780.24, "text": " It's not allowing the possibility of the rules changing, so to speak, as you go through the system.", "tokens": [51304, 467, 311, 406, 8293, 264, 7959, 295, 264, 4474, 4473, 11, 370, 281, 1710, 11, 382, 291, 352, 807, 264, 1185, 13, 51536], "temperature": 0.0, "avg_logprob": -0.07432242556735202, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0024253318551927805}, {"id": 819, "seek": 475680, "start": 4781.04, "end": 4785.28, "text": " So the P equals NP problem, one of the things that's pretty interesting that comes out of", "tokens": [51576, 407, 264, 430, 6915, 38611, 1154, 11, 472, 295, 264, 721, 300, 311, 1238, 1880, 300, 1487, 484, 295, 51788], "temperature": 0.0, "avg_logprob": -0.07432242556735202, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0024253318551927805}, {"id": 820, "seek": 478528, "start": 4785.28, "end": 4791.679999999999, "text": " our physics project is essentially a geometrization of the P equals NP problem. That is a question", "tokens": [50364, 527, 10649, 1716, 307, 4476, 257, 12956, 24959, 399, 295, 264, 430, 6915, 38611, 1154, 13, 663, 307, 257, 1168, 50684], "temperature": 0.0, "avg_logprob": -0.10471269062587193, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.007261024322360754}, {"id": 821, "seek": 478528, "start": 4791.679999999999, "end": 4800.0, "text": " of the structure of these objects in rural space, that P equals NP becomes the question of", "tokens": [50684, 295, 264, 3877, 295, 613, 6565, 294, 11165, 1901, 11, 300, 430, 6915, 38611, 3643, 264, 1168, 295, 51100], "temperature": 0.0, "avg_logprob": -0.10471269062587193, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.007261024322360754}, {"id": 822, "seek": 478528, "start": 4800.0, "end": 4805.28, "text": " whether essentially the red bit here eventually fills out the gray part of this picture. So you", "tokens": [51100, 1968, 4476, 264, 2182, 857, 510, 4728, 22498, 484, 264, 10855, 644, 295, 341, 3036, 13, 407, 291, 51364], "temperature": 0.0, "avg_logprob": -0.10471269062587193, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.007261024322360754}, {"id": 823, "seek": 478528, "start": 4805.28, "end": 4810.8, "text": " can kind of have a geometrical version of this ball in rural space that corresponds to the", "tokens": [51364, 393, 733, 295, 362, 257, 12956, 15888, 3037, 295, 341, 2594, 294, 11165, 1901, 300, 23249, 281, 264, 51640], "temperature": 0.0, "avg_logprob": -0.10471269062587193, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.007261024322360754}, {"id": 824, "seek": 481080, "start": 4811.76, "end": 4817.2, "text": " P problems and the NP problems. So that's a little bit of an indication of that. But you can,", "tokens": [50412, 430, 2740, 293, 264, 38611, 2740, 13, 407, 300, 311, 257, 707, 857, 295, 364, 18877, 295, 300, 13, 583, 291, 393, 11, 50684], "temperature": 0.0, "avg_logprob": -0.0978106862490939, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.005347105674445629}, {"id": 825, "seek": 481080, "start": 4817.2, "end": 4824.88, "text": " I mean, this whole question about non-determinism and so on, it's a, oh gosh, there's much to", "tokens": [50684, 286, 914, 11, 341, 1379, 1168, 466, 2107, 12, 49136, 259, 1434, 293, 370, 322, 11, 309, 311, 257, 11, 1954, 6502, 11, 456, 311, 709, 281, 51068], "temperature": 0.0, "avg_logprob": -0.0978106862490939, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.005347105674445629}, {"id": 826, "seek": 481080, "start": 4824.88, "end": 4834.400000000001, "text": " say about that. I've studied this a lot because it ends up being sort of a proxy for quantum", "tokens": [51068, 584, 466, 300, 13, 286, 600, 9454, 341, 257, 688, 570, 309, 5314, 493, 885, 1333, 295, 257, 29690, 337, 13018, 51544], "temperature": 0.0, "avg_logprob": -0.0978106862490939, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.005347105674445629}, {"id": 827, "seek": 481080, "start": 4834.400000000001, "end": 4840.0, "text": " mechanics. What happens in quantum mechanics is that you are following many paths of history", "tokens": [51544, 12939, 13, 708, 2314, 294, 13018, 12939, 307, 300, 291, 366, 3480, 867, 14518, 295, 2503, 51824], "temperature": 0.0, "avg_logprob": -0.0978106862490939, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.005347105674445629}, {"id": 828, "seek": 484000, "start": 4840.0, "end": 4845.36, "text": " and the observer in quantum mechanics is effectively a sort of an interesting situation.", "tokens": [50364, 293, 264, 27878, 294, 13018, 12939, 307, 8659, 257, 1333, 295, 364, 1880, 2590, 13, 50632], "temperature": 0.0, "avg_logprob": -0.06880575237852155, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.000726365193258971}, {"id": 829, "seek": 484000, "start": 4845.36, "end": 4850.0, "text": " The observer is branching in the same way that these actual paths of history in the universe", "tokens": [50632, 440, 27878, 307, 9819, 278, 294, 264, 912, 636, 300, 613, 3539, 14518, 295, 2503, 294, 264, 6445, 50864], "temperature": 0.0, "avg_logprob": -0.06880575237852155, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.000726365193258971}, {"id": 830, "seek": 484000, "start": 4850.0, "end": 4854.72, "text": " are branching. So quantum mechanics becomes this question of how does a branching mind", "tokens": [50864, 366, 9819, 278, 13, 407, 13018, 12939, 3643, 341, 1168, 295, 577, 775, 257, 9819, 278, 1575, 51100], "temperature": 0.0, "avg_logprob": -0.06880575237852155, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.000726365193258971}, {"id": 831, "seek": 484000, "start": 4854.72, "end": 4859.92, "text": " perceive a branching universe? And so it's interesting to kind of see a bunch of different", "tokens": [51100, 20281, 257, 9819, 278, 6445, 30, 400, 370, 309, 311, 1880, 281, 733, 295, 536, 257, 3840, 295, 819, 51360], "temperature": 0.0, "avg_logprob": -0.06880575237852155, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.000726365193258971}, {"id": 832, "seek": 484000, "start": 4859.92, "end": 4864.64, "text": " examples of multi-way systems as a way to get sort of more intuition about that.", "tokens": [51360, 5110, 295, 4825, 12, 676, 3652, 382, 257, 636, 281, 483, 1333, 295, 544, 24002, 466, 300, 13, 51596], "temperature": 0.0, "avg_logprob": -0.06880575237852155, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.000726365193258971}, {"id": 833, "seek": 486464, "start": 4865.4400000000005, "end": 4867.12, "text": " Okay, another question, apparently.", "tokens": [50404, 1033, 11, 1071, 1168, 11, 7970, 13, 50488], "temperature": 0.0, "avg_logprob": -0.1834764713194312, "compression_ratio": 1.5144230769230769, "no_speech_prob": 0.012404785491526127}, {"id": 834, "seek": 486464, "start": 4868.56, "end": 4875.04, "text": " Well, let's bring it back to our universe just for now. I want to, if possible, in a few minutes", "tokens": [50560, 1042, 11, 718, 311, 1565, 309, 646, 281, 527, 6445, 445, 337, 586, 13, 286, 528, 281, 11, 498, 1944, 11, 294, 257, 1326, 2077, 50884], "temperature": 0.0, "avg_logprob": -0.1834764713194312, "compression_ratio": 1.5144230769230769, "no_speech_prob": 0.012404785491526127}, {"id": 835, "seek": 486464, "start": 4875.04, "end": 4880.56, "text": " left, because I don't want to say anything. I want to make a connection between what you said", "tokens": [50884, 1411, 11, 570, 286, 500, 380, 528, 281, 584, 1340, 13, 286, 528, 281, 652, 257, 4984, 1296, 437, 291, 848, 51160], "temperature": 0.0, "avg_logprob": -0.1834764713194312, "compression_ratio": 1.5144230769230769, "no_speech_prob": 0.012404785491526127}, {"id": 836, "seek": 486464, "start": 4880.56, "end": 4890.8, "text": " and what Kyle, what Kai you said before. His program was related to computer aided proof", "tokens": [51160, 293, 437, 18023, 11, 437, 20753, 291, 848, 949, 13, 2812, 1461, 390, 4077, 281, 3820, 257, 2112, 8177, 51672], "temperature": 0.0, "avg_logprob": -0.1834764713194312, "compression_ratio": 1.5144230769230769, "no_speech_prob": 0.012404785491526127}, {"id": 837, "seek": 489080, "start": 4891.52, "end": 4900.16, "text": " and transformation of verbally stated truths and conjectures into", "tokens": [50400, 293, 9887, 295, 48162, 11323, 30079, 293, 416, 1020, 1303, 666, 50832], "temperature": 0.0, "avg_logprob": -0.15120060103280203, "compression_ratio": 1.56875, "no_speech_prob": 0.0032646977342665195}, {"id": 838, "seek": 489080, "start": 4901.2, "end": 4908.0, "text": " formal form called auto formalization. And my question to you is what does the irreducibility", "tokens": [50884, 9860, 1254, 1219, 8399, 9860, 2144, 13, 400, 452, 1168, 281, 291, 307, 437, 775, 264, 16014, 769, 537, 39802, 51224], "temperature": 0.0, "avg_logprob": -0.15120060103280203, "compression_ratio": 1.56875, "no_speech_prob": 0.0032646977342665195}, {"id": 839, "seek": 489080, "start": 4908.0, "end": 4914.320000000001, "text": " principle say about all the possible theorems and all the possible paths to their solution?", "tokens": [51224, 8665, 584, 466, 439, 264, 1944, 10299, 2592, 293, 439, 264, 1944, 14518, 281, 641, 3827, 30, 51540], "temperature": 0.0, "avg_logprob": -0.15120060103280203, "compression_ratio": 1.56875, "no_speech_prob": 0.0032646977342665195}, {"id": 840, "seek": 491432, "start": 4914.32, "end": 4920.96, "text": " And Kyle, you can say a couple of words in response, but you have to say it in your own", "tokens": [50364, 400, 18023, 11, 291, 393, 584, 257, 1916, 295, 2283, 294, 4134, 11, 457, 291, 362, 281, 584, 309, 294, 428, 1065, 50696], "temperature": 0.0, "avg_logprob": -0.14333667942121917, "compression_ratio": 1.5481171548117154, "no_speech_prob": 0.0007081726216711104}, {"id": 841, "seek": 491432, "start": 4920.96, "end": 4926.4, "text": " words first because you missed your talk. Well, let's see, I wrote a book recently about the", "tokens": [50696, 2283, 700, 570, 291, 6721, 428, 751, 13, 1042, 11, 718, 311, 536, 11, 286, 4114, 257, 1446, 3938, 466, 264, 50968], "temperature": 0.0, "avg_logprob": -0.14333667942121917, "compression_ratio": 1.5481171548117154, "no_speech_prob": 0.0007081726216711104}, {"id": 842, "seek": 491432, "start": 4926.4, "end": 4934.08, "text": " physicalization of metamathematics, which I think is pretty relevant to this. And so, you know,", "tokens": [50968, 4001, 2144, 295, 1131, 335, 998, 37541, 11, 597, 286, 519, 307, 1238, 7340, 281, 341, 13, 400, 370, 11, 291, 458, 11, 51352], "temperature": 0.0, "avg_logprob": -0.14333667942121917, "compression_ratio": 1.5481171548117154, "no_speech_prob": 0.0007081726216711104}, {"id": 843, "seek": 491432, "start": 4934.08, "end": 4940.4, "text": " we can imagine some, let's see, where's a good example? This is that might be some axiom in a", "tokens": [51352, 321, 393, 3811, 512, 11, 718, 311, 536, 11, 689, 311, 257, 665, 1365, 30, 639, 307, 300, 1062, 312, 512, 6360, 72, 298, 294, 257, 51668], "temperature": 0.0, "avg_logprob": -0.14333667942121917, "compression_ratio": 1.5481171548117154, "no_speech_prob": 0.0007081726216711104}, {"id": 844, "seek": 494040, "start": 4940.4, "end": 4945.5199999999995, "text": " mathematical system. And we can ask the question, what are the consequences of that axiom? That", "tokens": [50364, 18894, 1185, 13, 400, 321, 393, 1029, 264, 1168, 11, 437, 366, 264, 10098, 295, 300, 6360, 72, 298, 30, 663, 50620], "temperature": 0.0, "avg_logprob": -0.08834489501348816, "compression_ratio": 1.794392523364486, "no_speech_prob": 0.05155697837471962}, {"id": 845, "seek": 494040, "start": 4945.5199999999995, "end": 4952.16, "text": " axiom is saying x dot y is equivalent to y dot x dot y. And now we can say, well, what things are", "tokens": [50620, 6360, 72, 298, 307, 1566, 2031, 5893, 288, 307, 10344, 281, 288, 5893, 2031, 5893, 288, 13, 400, 586, 321, 393, 584, 11, 731, 11, 437, 721, 366, 50952], "temperature": 0.0, "avg_logprob": -0.08834489501348816, "compression_ratio": 1.794392523364486, "no_speech_prob": 0.05155697837471962}, {"id": 846, "seek": 494040, "start": 4952.16, "end": 4958.639999999999, "text": " also equivalent based on that axiom, we can start figuring out. So every path here is a theorem,", "tokens": [50952, 611, 10344, 2361, 322, 300, 6360, 72, 298, 11, 321, 393, 722, 15213, 484, 13, 407, 633, 3100, 510, 307, 257, 20904, 11, 51276], "temperature": 0.0, "avg_logprob": -0.08834489501348816, "compression_ratio": 1.794392523364486, "no_speech_prob": 0.05155697837471962}, {"id": 847, "seek": 494040, "start": 4958.639999999999, "end": 4964.24, "text": " that that's equivalent to that. We can start just following, we can start making this network", "tokens": [51276, 300, 300, 311, 10344, 281, 300, 13, 492, 393, 722, 445, 3480, 11, 321, 393, 722, 1455, 341, 3209, 51556], "temperature": 0.0, "avg_logprob": -0.08834489501348816, "compression_ratio": 1.794392523364486, "no_speech_prob": 0.05155697837471962}, {"id": 848, "seek": 496424, "start": 4964.32, "end": 4974.48, "text": " of all possible equivalent things. And actually, and so a proof becomes a path in this whole network.", "tokens": [50368, 295, 439, 1944, 10344, 721, 13, 400, 767, 11, 293, 370, 257, 8177, 3643, 257, 3100, 294, 341, 1379, 3209, 13, 50876], "temperature": 0.0, "avg_logprob": -0.14295822061518187, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.014535226859152317}, {"id": 849, "seek": 496424, "start": 4974.48, "end": 4977.599999999999, "text": " And actually, it's a little bit trickier than that when you start looking at,", "tokens": [50876, 400, 767, 11, 309, 311, 257, 707, 857, 4282, 811, 813, 300, 562, 291, 722, 1237, 412, 11, 51032], "temperature": 0.0, "avg_logprob": -0.14295822061518187, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.014535226859152317}, {"id": 850, "seek": 496424, "start": 4979.28, "end": 4987.04, "text": " there's a good example here. The way one actually does, let's see where I've got a good example.", "tokens": [51116, 456, 311, 257, 665, 1365, 510, 13, 440, 636, 472, 767, 775, 11, 718, 311, 536, 689, 286, 600, 658, 257, 665, 1365, 13, 51504], "temperature": 0.0, "avg_logprob": -0.14295822061518187, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.014535226859152317}, {"id": 851, "seek": 496424, "start": 4987.04, "end": 4993.28, "text": " Okay, so this is an example of what more is actually what's happening in mathematics. You have", "tokens": [51504, 1033, 11, 370, 341, 307, 364, 1365, 295, 437, 544, 307, 767, 437, 311, 2737, 294, 18666, 13, 509, 362, 51816], "temperature": 0.0, "avg_logprob": -0.14295822061518187, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.014535226859152317}, {"id": 852, "seek": 499328, "start": 4993.28, "end": 4999.84, "text": " basically, let's say two axioms here, and you are combining them to get a new theorem. And so you", "tokens": [50364, 1936, 11, 718, 311, 584, 732, 6360, 72, 4785, 510, 11, 293, 291, 366, 21928, 552, 281, 483, 257, 777, 20904, 13, 400, 370, 291, 50692], "temperature": 0.0, "avg_logprob": -0.05615167307659862, "compression_ratio": 1.8443579766536966, "no_speech_prob": 0.0028917663730680943}, {"id": 853, "seek": 499328, "start": 4999.84, "end": 5005.5199999999995, "text": " can kind of build up this kind of this structure you get with those two green axioms, you're,", "tokens": [50692, 393, 733, 295, 1322, 493, 341, 733, 295, 341, 3877, 291, 483, 365, 729, 732, 3092, 6360, 72, 4785, 11, 291, 434, 11, 50976], "temperature": 0.0, "avg_logprob": -0.05615167307659862, "compression_ratio": 1.8443579766536966, "no_speech_prob": 0.0028917663730680943}, {"id": 854, "seek": 499328, "start": 5005.5199999999995, "end": 5011.2, "text": " you're deriving all those theorems, you get this big network that represents all possible", "tokens": [50976, 291, 434, 1163, 2123, 439, 729, 10299, 2592, 11, 291, 483, 341, 955, 3209, 300, 8855, 439, 1944, 51260], "temperature": 0.0, "avg_logprob": -0.05615167307659862, "compression_ratio": 1.8443579766536966, "no_speech_prob": 0.0028917663730680943}, {"id": 855, "seek": 499328, "start": 5011.2, "end": 5016.5599999999995, "text": " theorems derived from a particular set of axioms. So you can go on, you can get pretty complicated", "tokens": [51260, 10299, 2592, 18949, 490, 257, 1729, 992, 295, 6360, 72, 4785, 13, 407, 291, 393, 352, 322, 11, 291, 393, 483, 1238, 6179, 51528], "temperature": 0.0, "avg_logprob": -0.05615167307659862, "compression_ratio": 1.8443579766536966, "no_speech_prob": 0.0028917663730680943}, {"id": 856, "seek": 499328, "start": 5016.5599999999995, "end": 5022.0, "text": " versions of this, you can derive all sorts of theorems that are true based on certain axioms.", "tokens": [51528, 9606, 295, 341, 11, 291, 393, 28446, 439, 7527, 295, 10299, 2592, 300, 366, 2074, 2361, 322, 1629, 6360, 72, 4785, 13, 51800], "temperature": 0.0, "avg_logprob": -0.05615167307659862, "compression_ratio": 1.8443579766536966, "no_speech_prob": 0.0028917663730680943}, {"id": 857, "seek": 502200, "start": 5022.0, "end": 5028.96, "text": " And what's happening is in this, in this graph, the every blue dot is a theorem. Okay. So then you", "tokens": [50364, 400, 437, 311, 2737, 307, 294, 341, 11, 294, 341, 4295, 11, 264, 633, 3344, 5893, 307, 257, 20904, 13, 1033, 13, 407, 550, 291, 50712], "temperature": 0.0, "avg_logprob": -0.11204426629202706, "compression_ratio": 1.6864406779661016, "no_speech_prob": 0.001192631316371262}, {"id": 858, "seek": 502200, "start": 5028.96, "end": 5038.08, "text": " can ask the question, if you look at, do I have an example of this? If you look at actual axiom", "tokens": [50712, 393, 1029, 264, 1168, 11, 498, 291, 574, 412, 11, 360, 286, 362, 364, 1365, 295, 341, 30, 759, 291, 574, 412, 3539, 6360, 72, 298, 51168], "temperature": 0.0, "avg_logprob": -0.11204426629202706, "compression_ratio": 1.6864406779661016, "no_speech_prob": 0.001192631316371262}, {"id": 859, "seek": 502200, "start": 5038.08, "end": 5043.36, "text": " systems in present day mathematics, so for example, you can look at, there's the axiom for semi-groups.", "tokens": [51168, 3652, 294, 1974, 786, 18666, 11, 370, 337, 1365, 11, 291, 393, 574, 412, 11, 456, 311, 264, 6360, 72, 298, 337, 12909, 12, 17377, 82, 13, 51432], "temperature": 0.0, "avg_logprob": -0.11204426629202706, "compression_ratio": 1.6864406779661016, "no_speech_prob": 0.001192631316371262}, {"id": 860, "seek": 502200, "start": 5044.08, "end": 5048.4, "text": " You can start proving theorems about semi-groups. Okay, so we've got this whole network of theorems", "tokens": [51468, 509, 393, 722, 27221, 10299, 2592, 466, 12909, 12, 17377, 82, 13, 1033, 11, 370, 321, 600, 658, 341, 1379, 3209, 295, 10299, 2592, 51684], "temperature": 0.0, "avg_logprob": -0.11204426629202706, "compression_ratio": 1.6864406779661016, "no_speech_prob": 0.001192631316371262}, {"id": 861, "seek": 504840, "start": 5048.4, "end": 5054.799999999999, "text": " about semi-groups. And so one big question is, if you do that, well, okay, so here's, here's an", "tokens": [50364, 466, 12909, 12, 17377, 82, 13, 400, 370, 472, 955, 1168, 307, 11, 498, 291, 360, 300, 11, 731, 11, 1392, 11, 370, 510, 311, 11, 510, 311, 364, 50684], "temperature": 0.0, "avg_logprob": -0.07453263260936009, "compression_ratio": 1.8795180722891567, "no_speech_prob": 0.005929792765527964}, {"id": 862, "seek": 504840, "start": 5054.799999999999, "end": 5060.719999999999, "text": " example based on the axioms of Boolean algebra. So this is proving theorems based on axioms in", "tokens": [50684, 1365, 2361, 322, 264, 6360, 72, 4785, 295, 23351, 28499, 21989, 13, 407, 341, 307, 27221, 10299, 2592, 2361, 322, 6360, 72, 4785, 294, 50980], "temperature": 0.0, "avg_logprob": -0.07453263260936009, "compression_ratio": 1.8795180722891567, "no_speech_prob": 0.005929792765527964}, {"id": 863, "seek": 504840, "start": 5060.719999999999, "end": 5065.92, "text": " Boolean algebra. And you can go and you can build up this giant network of theorems of Boolean", "tokens": [50980, 23351, 28499, 21989, 13, 400, 291, 393, 352, 293, 291, 393, 1322, 493, 341, 7410, 3209, 295, 10299, 2592, 295, 23351, 28499, 51240], "temperature": 0.0, "avg_logprob": -0.07453263260936009, "compression_ratio": 1.8795180722891567, "no_speech_prob": 0.005929792765527964}, {"id": 864, "seek": 504840, "start": 5065.92, "end": 5071.599999999999, "text": " algebra. And this is, this is kind of the, the enumeration of all possibilities. Okay. So now", "tokens": [51240, 21989, 13, 400, 341, 307, 11, 341, 307, 733, 295, 264, 11, 264, 465, 449, 5053, 295, 439, 12178, 13, 1033, 13, 407, 586, 51524], "temperature": 0.0, "avg_logprob": -0.07453263260936009, "compression_ratio": 1.8795180722891567, "no_speech_prob": 0.005929792765527964}, {"id": 865, "seek": 504840, "start": 5071.599999999999, "end": 5076.16, "text": " the question is, is we enumerate all those possibilities? Where are the theorems we care", "tokens": [51524, 264, 1168, 307, 11, 307, 321, 465, 15583, 473, 439, 729, 12178, 30, 2305, 366, 264, 10299, 2592, 321, 1127, 51752], "temperature": 0.0, "avg_logprob": -0.07453263260936009, "compression_ratio": 1.8795180722891567, "no_speech_prob": 0.005929792765527964}, {"id": 866, "seek": 507616, "start": 5076.16, "end": 5081.44, "text": " about? We've got gazillions of theorems that we can just build out eventually. And this is kind", "tokens": [50364, 466, 30, 492, 600, 658, 26232, 46279, 295, 10299, 2592, 300, 321, 393, 445, 1322, 484, 4728, 13, 400, 341, 307, 733, 50628], "temperature": 0.0, "avg_logprob": -0.09577509982526795, "compression_ratio": 1.7814814814814814, "no_speech_prob": 0.01334148645401001}, {"id": 867, "seek": 507616, "start": 5081.44, "end": 5086.48, "text": " of spoiled. If you didn't know, you don't haven't followed this, but this really odd object that", "tokens": [50628, 295, 32439, 13, 759, 291, 994, 380, 458, 11, 291, 500, 380, 2378, 380, 6263, 341, 11, 457, 341, 534, 7401, 2657, 300, 50880], "temperature": 0.0, "avg_logprob": -0.09577509982526795, "compression_ratio": 1.7814814814814814, "no_speech_prob": 0.01334148645401001}, {"id": 868, "seek": 507616, "start": 5086.48, "end": 5092.32, "text": " I talked about in the talk I gave, that is the ultimate limit of all possible mathematics is", "tokens": [50880, 286, 2825, 466, 294, 264, 751, 286, 2729, 11, 300, 307, 264, 9705, 4948, 295, 439, 1944, 18666, 307, 51172], "temperature": 0.0, "avg_logprob": -0.09577509982526795, "compression_ratio": 1.7814814814814814, "no_speech_prob": 0.01334148645401001}, {"id": 869, "seek": 507616, "start": 5092.32, "end": 5099.76, "text": " this really odd object. And the question of what theorems are, so all these theorems here are true.", "tokens": [51172, 341, 534, 7401, 2657, 13, 400, 264, 1168, 295, 437, 10299, 2592, 366, 11, 370, 439, 613, 10299, 2592, 510, 366, 2074, 13, 51544], "temperature": 0.0, "avg_logprob": -0.09577509982526795, "compression_ratio": 1.7814814814814814, "no_speech_prob": 0.01334148645401001}, {"id": 870, "seek": 507616, "start": 5099.76, "end": 5104.16, "text": " All these theorems can be constructed from the axioms. But these are the only two theorems that", "tokens": [51544, 1057, 613, 10299, 2592, 393, 312, 17083, 490, 264, 6360, 72, 4785, 13, 583, 613, 366, 264, 787, 732, 10299, 2592, 300, 51764], "temperature": 0.0, "avg_logprob": -0.09577509982526795, "compression_ratio": 1.7814814814814814, "no_speech_prob": 0.01334148645401001}, {"id": 871, "seek": 510416, "start": 5104.16, "end": 5110.72, "text": " people give names to in textbooks of logic out of this collection. We can keep going. We can find,", "tokens": [50364, 561, 976, 5288, 281, 294, 33587, 295, 9952, 484, 295, 341, 5765, 13, 492, 393, 1066, 516, 13, 492, 393, 915, 11, 50692], "temperature": 0.0, "avg_logprob": -0.10079338526961827, "compression_ratio": 1.737327188940092, "no_speech_prob": 0.005850072950124741}, {"id": 872, "seek": 510416, "start": 5110.72, "end": 5115.28, "text": " we can go to, to lots of other theorems. If we, if we use a theorem proving system,", "tokens": [50692, 321, 393, 352, 281, 11, 281, 3195, 295, 661, 10299, 2592, 13, 759, 321, 11, 498, 321, 764, 257, 20904, 27221, 1185, 11, 50920], "temperature": 0.0, "avg_logprob": -0.10079338526961827, "compression_ratio": 1.737327188940092, "no_speech_prob": 0.005850072950124741}, {"id": 873, "seek": 510416, "start": 5115.28, "end": 5120.08, "text": " we can, in that big giant explosion of possible theorems, we can go and say, this is the theorem", "tokens": [50920, 321, 393, 11, 294, 300, 955, 7410, 15673, 295, 1944, 10299, 2592, 11, 321, 393, 352, 293, 584, 11, 341, 307, 264, 20904, 51160], "temperature": 0.0, "avg_logprob": -0.10079338526961827, "compression_ratio": 1.737327188940092, "no_speech_prob": 0.005850072950124741}, {"id": 874, "seek": 510416, "start": 5120.08, "end": 5125.28, "text": " we're searching for, and we can find a path to it using, using theorem proving. And those are the", "tokens": [51160, 321, 434, 10808, 337, 11, 293, 321, 393, 915, 257, 3100, 281, 309, 1228, 11, 1228, 20904, 27221, 13, 400, 729, 366, 264, 51420], "temperature": 0.0, "avg_logprob": -0.10079338526961827, "compression_ratio": 1.737327188940092, "no_speech_prob": 0.005850072950124741}, {"id": 875, "seek": 512528, "start": 5125.28, "end": 5133.92, "text": " paths in that, in that structure of, of, of possible theorems. Okay, so one question then is,", "tokens": [50364, 14518, 294, 300, 11, 294, 300, 3877, 295, 11, 295, 11, 295, 1944, 10299, 2592, 13, 1033, 11, 370, 472, 1168, 550, 307, 11, 50796], "temperature": 0.0, "avg_logprob": -0.07256098406030498, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.07981936633586884}, {"id": 876, "seek": 512528, "start": 5134.8, "end": 5140.8, "text": " well, out of all these possible, this, this complicated network of all possible theorems,", "tokens": [50840, 731, 11, 484, 295, 439, 613, 1944, 11, 341, 11, 341, 6179, 3209, 295, 439, 1944, 10299, 2592, 11, 51140], "temperature": 0.0, "avg_logprob": -0.07256098406030498, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.07981936633586884}, {"id": 877, "seek": 512528, "start": 5140.8, "end": 5146.719999999999, "text": " where are the ones that we humans care about? And so I looked a little bit at that. And so,", "tokens": [51140, 689, 366, 264, 2306, 300, 321, 6255, 1127, 466, 30, 400, 370, 286, 2956, 257, 707, 857, 412, 300, 13, 400, 370, 11, 51436], "temperature": 0.0, "avg_logprob": -0.07256098406030498, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.07981936633586884}, {"id": 878, "seek": 512528, "start": 5146.719999999999, "end": 5152.4, "text": " for example, you can, you can look, well, that's, that's, for example, that's Euclid. So Euclid", "tokens": [51436, 337, 1365, 11, 291, 393, 11, 291, 393, 574, 11, 731, 11, 300, 311, 11, 300, 311, 11, 337, 1365, 11, 300, 311, 462, 1311, 75, 327, 13, 407, 462, 1311, 75, 327, 51720], "temperature": 0.0, "avg_logprob": -0.07256098406030498, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.07981936633586884}, {"id": 879, "seek": 515240, "start": 5152.4, "end": 5158.16, "text": " has 465 theorems. And you can start off from the axioms at the top. And you can see what are the", "tokens": [50364, 575, 1017, 16824, 10299, 2592, 13, 400, 291, 393, 722, 766, 490, 264, 6360, 72, 4785, 412, 264, 1192, 13, 400, 291, 393, 536, 437, 366, 264, 50652], "temperature": 0.0, "avg_logprob": -0.10536331705527731, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.005423387046903372}, {"id": 880, "seek": 515240, "start": 5158.16, "end": 5163.599999999999, "text": " connections between those theorems, according to the proofs in Euclid. Perhaps more interestingly,", "tokens": [50652, 9271, 1296, 729, 10299, 2592, 11, 4650, 281, 264, 8177, 82, 294, 462, 1311, 75, 327, 13, 10517, 544, 25873, 11, 50924], "temperature": 0.0, "avg_logprob": -0.10536331705527731, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.005423387046903372}, {"id": 881, "seek": 515240, "start": 5163.599999999999, "end": 5171.04, "text": " you can take a proof of existence system. I looked at lean. I looked a little bit simpler", "tokens": [50924, 291, 393, 747, 257, 8177, 295, 9123, 1185, 13, 286, 2956, 412, 11659, 13, 286, 2956, 257, 707, 857, 18587, 51296], "temperature": 0.0, "avg_logprob": -0.10536331705527731, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.005423387046903372}, {"id": 882, "seek": 515240, "start": 5171.04, "end": 5177.36, "text": " to look at the system called metamath, which is a formalized math system. And you can ask questions", "tokens": [51296, 281, 574, 412, 264, 1185, 1219, 1131, 335, 998, 11, 597, 307, 257, 9860, 1602, 5221, 1185, 13, 400, 291, 393, 1029, 1651, 51612], "temperature": 0.0, "avg_logprob": -0.10536331705527731, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.005423387046903372}, {"id": 883, "seek": 517736, "start": 5177.36, "end": 5183.92, "text": " like, well, that's the Pythagorean theorem, proved from the axioms in metamath. And you can see", "tokens": [50364, 411, 11, 731, 11, 300, 311, 264, 9953, 392, 559, 25885, 20904, 11, 14617, 490, 264, 6360, 72, 4785, 294, 1131, 335, 998, 13, 400, 291, 393, 536, 50692], "temperature": 0.0, "avg_logprob": -0.10397718944688783, "compression_ratio": 1.8188976377952757, "no_speech_prob": 0.01681692898273468}, {"id": 884, "seek": 517736, "start": 5183.92, "end": 5188.08, "text": " they're a different, it's a pretty complicated thing. This somewhere, I think at the bottom here", "tokens": [50692, 436, 434, 257, 819, 11, 309, 311, 257, 1238, 6179, 551, 13, 639, 4079, 11, 286, 519, 412, 264, 2767, 510, 50900], "temperature": 0.0, "avg_logprob": -0.10397718944688783, "compression_ratio": 1.8188976377952757, "no_speech_prob": 0.01681692898273468}, {"id": 885, "seek": 517736, "start": 5188.08, "end": 5192.4, "text": " is the Pythagorean theorem. And you start from the axioms there, and you can kind of count up", "tokens": [50900, 307, 264, 9953, 392, 559, 25885, 20904, 13, 400, 291, 722, 490, 264, 6360, 72, 4785, 456, 11, 293, 291, 393, 733, 295, 1207, 493, 51116], "temperature": 0.0, "avg_logprob": -0.10397718944688783, "compression_ratio": 1.8188976377952757, "no_speech_prob": 0.01681692898273468}, {"id": 886, "seek": 517736, "start": 5193.12, "end": 5197.839999999999, "text": " of the various axioms, you know, how many times did you use the axiom of equality?", "tokens": [51152, 295, 264, 3683, 6360, 72, 4785, 11, 291, 458, 11, 577, 867, 1413, 630, 291, 764, 264, 6360, 72, 298, 295, 14949, 30, 51388], "temperature": 0.0, "avg_logprob": -0.10397718944688783, "compression_ratio": 1.8188976377952757, "no_speech_prob": 0.01681692898273468}, {"id": 887, "seek": 517736, "start": 5197.839999999999, "end": 5204.08, "text": " Five times 10 to the 31 times. This is a, you know, this, this is kind of a, a, this is what", "tokens": [51388, 9436, 1413, 1266, 281, 264, 10353, 1413, 13, 639, 307, 257, 11, 291, 458, 11, 341, 11, 341, 307, 733, 295, 257, 11, 257, 11, 341, 307, 437, 51700], "temperature": 0.0, "avg_logprob": -0.10397718944688783, "compression_ratio": 1.8188976377952757, "no_speech_prob": 0.01681692898273468}, {"id": 888, "seek": 520408, "start": 5204.08, "end": 5209.2, "text": " happens if you start from sort of the axiomatic foundation, and you build up to something like", "tokens": [50364, 2314, 498, 291, 722, 490, 1333, 295, 264, 6360, 72, 13143, 7030, 11, 293, 291, 1322, 493, 281, 746, 411, 50620], "temperature": 0.0, "avg_logprob": -0.06459002702132516, "compression_ratio": 1.8604651162790697, "no_speech_prob": 0.0021008108742535114}, {"id": 889, "seek": 520408, "start": 5209.2, "end": 5216.0, "text": " the Pythagorean theorem. So, okay. So one interesting point here is this is the axiomatic", "tokens": [50620, 264, 9953, 392, 559, 25885, 20904, 13, 407, 11, 1392, 13, 407, 472, 1880, 935, 510, 307, 341, 307, 264, 6360, 72, 13143, 50960], "temperature": 0.0, "avg_logprob": -0.06459002702132516, "compression_ratio": 1.8604651162790697, "no_speech_prob": 0.0021008108742535114}, {"id": 890, "seek": 520408, "start": 5216.0, "end": 5220.64, "text": " sort of structure of the Pythagorean theorem. The question is, do mathematicians care about this?", "tokens": [50960, 1333, 295, 3877, 295, 264, 9953, 392, 559, 25885, 20904, 13, 440, 1168, 307, 11, 360, 32811, 2567, 1127, 466, 341, 30, 51192], "temperature": 0.0, "avg_logprob": -0.06459002702132516, "compression_ratio": 1.8604651162790697, "no_speech_prob": 0.0021008108742535114}, {"id": 891, "seek": 520408, "start": 5221.36, "end": 5225.6, "text": " So, you know, a decade ago, I was very interested in formalization of mathematics. I organized", "tokens": [51228, 407, 11, 291, 458, 11, 257, 10378, 2057, 11, 286, 390, 588, 3102, 294, 9860, 2144, 295, 18666, 13, 286, 9983, 51440], "temperature": 0.0, "avg_logprob": -0.06459002702132516, "compression_ratio": 1.8604651162790697, "no_speech_prob": 0.0021008108742535114}, {"id": 892, "seek": 520408, "start": 5225.6, "end": 5230.72, "text": " this conference. We invited all these formalization of mathematics people, all these people interested", "tokens": [51440, 341, 7586, 13, 492, 9185, 439, 613, 9860, 2144, 295, 18666, 561, 11, 439, 613, 561, 3102, 51696], "temperature": 0.0, "avg_logprob": -0.06459002702132516, "compression_ratio": 1.8604651162790697, "no_speech_prob": 0.0021008108742535114}, {"id": 893, "seek": 523072, "start": 5230.72, "end": 5235.6, "text": " in mathematics itself. The formalizers all showed up. The mathematicians didn't show up.", "tokens": [50364, 294, 18666, 2564, 13, 440, 9860, 22525, 439, 4712, 493, 13, 440, 32811, 2567, 994, 380, 855, 493, 13, 50608], "temperature": 0.0, "avg_logprob": -0.0956123636123982, "compression_ratio": 1.8949152542372882, "no_speech_prob": 0.0035597379319369793}, {"id": 894, "seek": 523072, "start": 5237.6, "end": 5241.4400000000005, "text": " And so the question is, what does a working mathematician actually do? You know, working", "tokens": [50708, 400, 370, 264, 1168, 307, 11, 437, 775, 257, 1364, 48281, 767, 360, 30, 509, 458, 11, 1364, 50900], "temperature": 0.0, "avg_logprob": -0.0956123636123982, "compression_ratio": 1.8949152542372882, "no_speech_prob": 0.0035597379319369793}, {"id": 895, "seek": 523072, "start": 5241.4400000000005, "end": 5247.68, "text": " mathematician who's, who's thinking about the, you know, the Pythagorean theorem, are they", "tokens": [50900, 48281, 567, 311, 11, 567, 311, 1953, 466, 264, 11, 291, 458, 11, 264, 9953, 392, 559, 25885, 20904, 11, 366, 436, 51212], "temperature": 0.0, "avg_logprob": -0.0956123636123982, "compression_ratio": 1.8949152542372882, "no_speech_prob": 0.0035597379319369793}, {"id": 896, "seek": 523072, "start": 5247.68, "end": 5252.240000000001, "text": " thinking about it in this kind of axiomatic way? Are they drilling down to kind of this, this low", "tokens": [51212, 1953, 466, 309, 294, 341, 733, 295, 6360, 72, 13143, 636, 30, 2014, 436, 26290, 760, 281, 733, 295, 341, 11, 341, 2295, 51440], "temperature": 0.0, "avg_logprob": -0.0956123636123982, "compression_ratio": 1.8949152542372882, "no_speech_prob": 0.0035597379319369793}, {"id": 897, "seek": 523072, "start": 5252.240000000001, "end": 5256.16, "text": " level axiomatic structure? Or are they just saying it's the Pythagorean theorem and I'm going to do", "tokens": [51440, 1496, 6360, 72, 13143, 3877, 30, 1610, 366, 436, 445, 1566, 309, 311, 264, 9953, 392, 559, 25885, 20904, 293, 286, 478, 516, 281, 360, 51636], "temperature": 0.0, "avg_logprob": -0.0956123636123982, "compression_ratio": 1.8949152542372882, "no_speech_prob": 0.0035597379319369793}, {"id": 898, "seek": 523072, "start": 5256.16, "end": 5259.84, "text": " things at that level? The thing that's pretty interesting and relates to a lot of what I was", "tokens": [51636, 721, 412, 300, 1496, 30, 440, 551, 300, 311, 1238, 1880, 293, 16155, 281, 257, 688, 295, 437, 286, 390, 51820], "temperature": 0.0, "avg_logprob": -0.0956123636123982, "compression_ratio": 1.8949152542372882, "no_speech_prob": 0.0035597379319369793}, {"id": 899, "seek": 525984, "start": 5259.84, "end": 5266.0, "text": " talking about is at this kind of axiomatic level of mathematics, it's kind of like molecular", "tokens": [50364, 1417, 466, 307, 412, 341, 733, 295, 6360, 72, 13143, 1496, 295, 18666, 11, 309, 311, 733, 295, 411, 19046, 50672], "temperature": 0.0, "avg_logprob": -0.07419891192995269, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0023227406200021505}, {"id": 900, "seek": 525984, "start": 5266.0, "end": 5270.56, "text": " dynamics in a, in a fluid. You've got all these molecules bouncing around. They're doing all these", "tokens": [50672, 15679, 294, 257, 11, 294, 257, 9113, 13, 509, 600, 658, 439, 613, 13093, 27380, 926, 13, 814, 434, 884, 439, 613, 50900], "temperature": 0.0, "avg_logprob": -0.07419891192995269, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0023227406200021505}, {"id": 901, "seek": 525984, "start": 5270.56, "end": 5275.92, "text": " complicated things. But then at the higher level, at the more human level, what we get to see is", "tokens": [50900, 6179, 721, 13, 583, 550, 412, 264, 2946, 1496, 11, 412, 264, 544, 1952, 1496, 11, 437, 321, 483, 281, 536, 307, 51168], "temperature": 0.0, "avg_logprob": -0.07419891192995269, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0023227406200021505}, {"id": 902, "seek": 525984, "start": 5275.92, "end": 5281.76, "text": " fluid dynamics. And we can ask the question, can we make conclusions at the fluid dynamics level?", "tokens": [51168, 9113, 15679, 13, 400, 321, 393, 1029, 264, 1168, 11, 393, 321, 652, 22865, 412, 264, 9113, 15679, 1496, 30, 51460], "temperature": 0.0, "avg_logprob": -0.07419891192995269, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0023227406200021505}, {"id": 903, "seek": 525984, "start": 5281.76, "end": 5286.64, "text": " Or do we get dragged down to the molecular dynamics level and have to address things at that level?", "tokens": [51460, 1610, 360, 321, 483, 25717, 760, 281, 264, 19046, 15679, 1496, 293, 362, 281, 2985, 721, 412, 300, 1496, 30, 51704], "temperature": 0.0, "avg_logprob": -0.07419891192995269, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0023227406200021505}, {"id": 904, "seek": 528664, "start": 5286.64, "end": 5290.8, "text": " So in the question of, you know, using the Pythagorean theorem, for example, do you need", "tokens": [50364, 407, 294, 264, 1168, 295, 11, 291, 458, 11, 1228, 264, 9953, 392, 559, 25885, 20904, 11, 337, 1365, 11, 360, 291, 643, 50572], "temperature": 0.0, "avg_logprob": -0.0889362891515096, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.0012870896607637405}, {"id": 905, "seek": 528664, "start": 5290.8, "end": 5295.52, "text": " to go down to the level of these axioms and worry about how you define the real numbers and so on?", "tokens": [50572, 281, 352, 760, 281, 264, 1496, 295, 613, 6360, 72, 4785, 293, 3292, 466, 577, 291, 6964, 264, 957, 3547, 293, 370, 322, 30, 50808], "temperature": 0.0, "avg_logprob": -0.0889362891515096, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.0012870896607637405}, {"id": 906, "seek": 528664, "start": 5295.52, "end": 5300.160000000001, "text": " Or are you actually operating in practical mathematics at a higher level, at this level,", "tokens": [50808, 1610, 366, 291, 767, 7447, 294, 8496, 18666, 412, 257, 2946, 1496, 11, 412, 341, 1496, 11, 51040], "temperature": 0.0, "avg_logprob": -0.0889362891515096, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.0012870896607637405}, {"id": 907, "seek": 528664, "start": 5300.160000000001, "end": 5309.76, "text": " where you're, you're just operating in terms of, of, of these kind of sort of fluid dynamics", "tokens": [51040, 689, 291, 434, 11, 291, 434, 445, 7447, 294, 2115, 295, 11, 295, 11, 295, 613, 733, 295, 1333, 295, 9113, 15679, 51520], "temperature": 0.0, "avg_logprob": -0.0889362891515096, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.0012870896607637405}, {"id": 908, "seek": 530976, "start": 5310.0, "end": 5316.24, "text": " concepts? So I have to say, I was curious in, you know, in the, in sort of the world of LLMs and", "tokens": [50376, 10392, 30, 407, 286, 362, 281, 584, 11, 286, 390, 6369, 294, 11, 291, 458, 11, 294, 264, 11, 294, 1333, 295, 264, 1002, 295, 441, 43, 26386, 293, 50688], "temperature": 0.0, "avg_logprob": -0.11359280586242676, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.3734481930732727}, {"id": 909, "seek": 530976, "start": 5316.24, "end": 5322.88, "text": " so on. So I will say that the mission of taking kind of a piece of informal mathematics and", "tokens": [50688, 370, 322, 13, 407, 286, 486, 584, 300, 264, 4447, 295, 1940, 733, 295, 257, 2522, 295, 24342, 18666, 293, 51020], "temperature": 0.0, "avg_logprob": -0.11359280586242676, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.3734481930732727}, {"id": 910, "seek": 530976, "start": 5322.88, "end": 5328.4800000000005, "text": " formalizing it seems like a fairly, fairly promising use case for things like LLMs.", "tokens": [51020, 9860, 3319, 309, 2544, 411, 257, 6457, 11, 6457, 20257, 764, 1389, 337, 721, 411, 441, 43, 26386, 13, 51300], "temperature": 0.0, "avg_logprob": -0.11359280586242676, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.3734481930732727}, {"id": 911, "seek": 530976, "start": 5329.6, "end": 5334.96, "text": " I don't know whether the formalization in terms of, you know, existing proof assistance and so on,", "tokens": [51356, 286, 500, 380, 458, 1968, 264, 9860, 2144, 294, 2115, 295, 11, 291, 458, 11, 6741, 8177, 9683, 293, 370, 322, 11, 51624], "temperature": 0.0, "avg_logprob": -0.11359280586242676, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.3734481930732727}, {"id": 912, "seek": 533496, "start": 5334.96, "end": 5340.4, "text": " I don't know how useful that will really end up being. I was curious whether you could use", "tokens": [50364, 286, 500, 380, 458, 577, 4420, 300, 486, 534, 917, 493, 885, 13, 286, 390, 6369, 1968, 291, 727, 764, 50636], "temperature": 0.0, "avg_logprob": -0.19295180362203848, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.006704905070364475}, {"id": 913, "seek": 533496, "start": 5340.4, "end": 5355.92, "text": " LLMs as a way to, as a way to, to kind of guide theorem proving. So I looked here at, here we go.", "tokens": [50636, 441, 43, 26386, 382, 257, 636, 281, 11, 382, 257, 636, 281, 11, 281, 733, 295, 5934, 20904, 27221, 13, 407, 286, 2956, 510, 412, 11, 510, 321, 352, 13, 51412], "temperature": 0.0, "avg_logprob": -0.19295180362203848, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.006704905070364475}, {"id": 914, "seek": 533496, "start": 5356.4800000000005, "end": 5361.52, "text": " May I make another suggestion instead of looking at, let Kai answer, just a second,", "tokens": [51440, 1891, 286, 652, 1071, 16541, 2602, 295, 1237, 412, 11, 718, 20753, 1867, 11, 445, 257, 1150, 11, 51692], "temperature": 0.0, "avg_logprob": -0.19295180362203848, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.006704905070364475}, {"id": 915, "seek": 533496, "start": 5361.52, "end": 5363.92, "text": " because there's so little time now we can continue in the back.", "tokens": [51692, 570, 456, 311, 370, 707, 565, 586, 321, 393, 2354, 294, 264, 646, 13, 51812], "temperature": 0.0, "avg_logprob": -0.19295180362203848, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.006704905070364475}, {"id": 916, "seek": 536392, "start": 5363.92, "end": 5368.72, "text": " Please. Am I, let me just ask a very practical question. I have another thing in 30 minutes.", "tokens": [50364, 2555, 13, 2012, 286, 11, 718, 385, 445, 1029, 257, 588, 8496, 1168, 13, 286, 362, 1071, 551, 294, 2217, 2077, 13, 50604], "temperature": 0.0, "avg_logprob": -0.2041659387166068, "compression_ratio": 1.7824561403508772, "no_speech_prob": 0.014376809820532799}, {"id": 917, "seek": 536392, "start": 5368.72, "end": 5372.8, "text": " Am I, I could push it back, but am I going to make that or not?", "tokens": [50604, 2012, 286, 11, 286, 727, 2944, 309, 646, 11, 457, 669, 286, 516, 281, 652, 300, 420, 406, 30, 50808], "temperature": 0.0, "avg_logprob": -0.2041659387166068, "compression_ratio": 1.7824561403508772, "no_speech_prob": 0.014376809820532799}, {"id": 918, "seek": 536392, "start": 5372.8, "end": 5378.32, "text": " Yes. The other thing in 30 minutes is the plan, is the panel where you, where you plan,", "tokens": [50808, 1079, 13, 440, 661, 551, 294, 2217, 2077, 307, 264, 1393, 11, 307, 264, 4831, 689, 291, 11, 689, 291, 1393, 11, 51084], "temperature": 0.0, "avg_logprob": -0.2041659387166068, "compression_ratio": 1.7824561403508772, "no_speech_prob": 0.014376809820532799}, {"id": 919, "seek": 536392, "start": 5378.32, "end": 5378.8, "text": " Oh really?", "tokens": [51084, 876, 534, 30, 51108], "temperature": 0.0, "avg_logprob": -0.2041659387166068, "compression_ratio": 1.7824561403508772, "no_speech_prob": 0.014376809820532799}, {"id": 920, "seek": 536392, "start": 5378.8, "end": 5380.4, "text": " The panel, are you going to do something else?", "tokens": [51108, 440, 4831, 11, 366, 291, 516, 281, 360, 746, 1646, 30, 51188], "temperature": 0.0, "avg_logprob": -0.2041659387166068, "compression_ratio": 1.7824561403508772, "no_speech_prob": 0.014376809820532799}, {"id": 921, "seek": 536392, "start": 5381.76, "end": 5384.64, "text": " Well, I don't know. It really depends, but go ahead.", "tokens": [51256, 1042, 11, 286, 500, 380, 458, 13, 467, 534, 5946, 11, 457, 352, 2286, 13, 51400], "temperature": 0.0, "avg_logprob": -0.2041659387166068, "compression_ratio": 1.7824561403508772, "no_speech_prob": 0.014376809820532799}, {"id": 922, "seek": 536392, "start": 5384.64, "end": 5387.12, "text": " Danielle, Danielle said that you were going to do both.", "tokens": [51400, 21182, 11, 21182, 848, 300, 291, 645, 516, 281, 360, 1293, 13, 51524], "temperature": 0.0, "avg_logprob": -0.2041659387166068, "compression_ratio": 1.7824561403508772, "no_speech_prob": 0.014376809820532799}, {"id": 923, "seek": 536392, "start": 5387.68, "end": 5388.4800000000005, "text": " Okay. All right.", "tokens": [51552, 1033, 13, 1057, 558, 13, 51592], "temperature": 0.0, "avg_logprob": -0.2041659387166068, "compression_ratio": 1.7824561403508772, "no_speech_prob": 0.014376809820532799}, {"id": 924, "seek": 536392, "start": 5388.4800000000005, "end": 5393.04, "text": " I think she, if you got the other thing from Danielle, it is in fact the panel.", "tokens": [51592, 286, 519, 750, 11, 498, 291, 658, 264, 661, 551, 490, 21182, 11, 309, 307, 294, 1186, 264, 4831, 13, 51820], "temperature": 0.0, "avg_logprob": -0.2041659387166068, "compression_ratio": 1.7824561403508772, "no_speech_prob": 0.014376809820532799}, {"id": 925, "seek": 539304, "start": 5393.12, "end": 5398.0, "text": " No, no, no. It's a completely different thing, but that's okay. We'll, we'll, we'll tell you a panel.", "tokens": [50368, 883, 11, 572, 11, 572, 13, 467, 311, 257, 2584, 819, 551, 11, 457, 300, 311, 1392, 13, 492, 603, 11, 321, 603, 11, 321, 603, 980, 291, 257, 4831, 13, 50612], "temperature": 0.0, "avg_logprob": -0.21690069369184292, "compression_ratio": 1.6277372262773722, "no_speech_prob": 0.0008732027490623295}, {"id": 926, "seek": 539304, "start": 5398.0, "end": 5398.48, "text": " That's great.", "tokens": [50612, 663, 311, 869, 13, 50636], "temperature": 0.0, "avg_logprob": -0.21690069369184292, "compression_ratio": 1.6277372262773722, "no_speech_prob": 0.0008732027490623295}, {"id": 927, "seek": 539304, "start": 5399.2, "end": 5399.68, "text": " Say this.", "tokens": [50672, 6463, 341, 13, 50696], "temperature": 0.0, "avg_logprob": -0.21690069369184292, "compression_ratio": 1.6277372262773722, "no_speech_prob": 0.0008732027490623295}, {"id": 928, "seek": 539304, "start": 5400.4, "end": 5406.24, "text": " Yeah. I think the short answer is yes. Yes. People are using informal mathematics and", "tokens": [50732, 865, 13, 286, 519, 264, 2099, 1867, 307, 2086, 13, 1079, 13, 3432, 366, 1228, 24342, 18666, 293, 51024], "temperature": 0.0, "avg_logprob": -0.21690069369184292, "compression_ratio": 1.6277372262773722, "no_speech_prob": 0.0008732027490623295}, {"id": 929, "seek": 539304, "start": 5406.8, "end": 5411.28, "text": " auto formalization, which means translating informal to formal to guide theorem proving.", "tokens": [51052, 8399, 9860, 2144, 11, 597, 1355, 35030, 24342, 281, 9860, 281, 5934, 20904, 27221, 13, 51276], "temperature": 0.0, "avg_logprob": -0.21690069369184292, "compression_ratio": 1.6277372262773722, "no_speech_prob": 0.0008732027490623295}, {"id": 930, "seek": 539304, "start": 5411.28, "end": 5415.68, "text": " For example, given a proof, you can use language model like GPT-4 to,", "tokens": [51276, 1171, 1365, 11, 2212, 257, 8177, 11, 291, 393, 764, 2856, 2316, 411, 26039, 51, 12, 19, 281, 11, 51496], "temperature": 0.0, "avg_logprob": -0.21690069369184292, "compression_ratio": 1.6277372262773722, "no_speech_prob": 0.0008732027490623295}, {"id": 931, "seek": 539304, "start": 5415.68, "end": 5420.0, "text": " you just ask it to generate informal proof or even a sketch, some ideas of,", "tokens": [51496, 291, 445, 1029, 309, 281, 8460, 24342, 8177, 420, 754, 257, 12325, 11, 512, 3487, 295, 11, 51712], "temperature": 0.0, "avg_logprob": -0.21690069369184292, "compression_ratio": 1.6277372262773722, "no_speech_prob": 0.0008732027490623295}, {"id": 932, "seek": 542000, "start": 5420.96, "end": 5423.6, "text": " could be high level idea of how this proof might go.", "tokens": [50412, 727, 312, 1090, 1496, 1558, 295, 577, 341, 8177, 1062, 352, 13, 50544], "temperature": 0.0, "avg_logprob": -0.14778382579485574, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.0007093279273249209}, {"id": 933, "seek": 542000, "start": 5423.6, "end": 5426.32, "text": " And then conditioned on this informal sketch,", "tokens": [50544, 400, 550, 35833, 322, 341, 24342, 12325, 11, 50680], "temperature": 0.0, "avg_logprob": -0.14778382579485574, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.0007093279273249209}, {"id": 934, "seek": 542000, "start": 5427.04, "end": 5430.64, "text": " you, there will be a second step to generate the formal proof.", "tokens": [50716, 291, 11, 456, 486, 312, 257, 1150, 1823, 281, 8460, 264, 9860, 8177, 13, 50896], "temperature": 0.0, "avg_logprob": -0.14778382579485574, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.0007093279273249209}, {"id": 935, "seek": 542000, "start": 5430.64, "end": 5436.4, "text": " And, but I think a caveat is if you rely on auto formalization to give you the proof,", "tokens": [50896, 400, 11, 457, 286, 519, 257, 43012, 307, 498, 291, 10687, 322, 8399, 9860, 2144, 281, 976, 291, 264, 8177, 11, 51184], "temperature": 0.0, "avg_logprob": -0.14778382579485574, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.0007093279273249209}, {"id": 936, "seek": 542000, "start": 5437.36, "end": 5440.64, "text": " it only works if human already discovered this proof.", "tokens": [51232, 309, 787, 1985, 498, 1952, 1217, 6941, 341, 8177, 13, 51396], "temperature": 0.0, "avg_logprob": -0.14778382579485574, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.0007093279273249209}, {"id": 937, "seek": 542000, "start": 5442.32, "end": 5447.44, "text": " Because then there's no, if not, if it's completely alien to mathematics,", "tokens": [51480, 1436, 550, 456, 311, 572, 11, 498, 406, 11, 498, 309, 311, 2584, 12319, 281, 18666, 11, 51736], "temperature": 0.0, "avg_logprob": -0.14778382579485574, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.0007093279273249209}, {"id": 938, "seek": 544744, "start": 5447.44, "end": 5449.2, "text": " then there's nothing for you to auto formalize.", "tokens": [50364, 550, 456, 311, 1825, 337, 291, 281, 8399, 9860, 1125, 13, 50452], "temperature": 0.0, "avg_logprob": -0.15366283728151905, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.0002912849886342883}, {"id": 939, "seek": 544744, "start": 5450.24, "end": 5454.719999999999, "text": " I think another direction related to what Stephen mentioned is,", "tokens": [50504, 286, 519, 1071, 3513, 4077, 281, 437, 13391, 2835, 307, 11, 50728], "temperature": 0.0, "avg_logprob": -0.15366283728151905, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.0002912849886342883}, {"id": 940, "seek": 544744, "start": 5455.679999999999, "end": 5461.04, "text": " how can we even take one step further? Can we use language models to generate conjecture?", "tokens": [50776, 577, 393, 321, 754, 747, 472, 1823, 3052, 30, 1664, 321, 764, 2856, 5245, 281, 8460, 416, 1020, 540, 30, 51044], "temperature": 0.0, "avg_logprob": -0.15366283728151905, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.0002912849886342883}, {"id": 941, "seek": 544744, "start": 5461.04, "end": 5464.5599999999995, "text": " Like generate the huge graph Stephen was mentioning.", "tokens": [51044, 1743, 8460, 264, 2603, 4295, 13391, 390, 18315, 13, 51220], "temperature": 0.0, "avg_logprob": -0.15366283728151905, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.0002912849886342883}, {"id": 942, "seek": 544744, "start": 5465.2, "end": 5470.879999999999, "text": " But of course the graph, I think it might be infinite or it may be simply too big.", "tokens": [51252, 583, 295, 1164, 264, 4295, 11, 286, 519, 309, 1062, 312, 13785, 420, 309, 815, 312, 2935, 886, 955, 13, 51536], "temperature": 0.0, "avg_logprob": -0.15366283728151905, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.0002912849886342883}, {"id": 943, "seek": 544744, "start": 5470.879999999999, "end": 5476.0, "text": " So a really interesting question I want to maybe learn from Stephen is", "tokens": [51536, 407, 257, 534, 1880, 1168, 286, 528, 281, 1310, 1466, 490, 13391, 307, 51792], "temperature": 0.0, "avg_logprob": -0.15366283728151905, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.0002912849886342883}, {"id": 944, "seek": 547600, "start": 5476.72, "end": 5483.04, "text": " say we want to generate this graph, but how do we tell if a node is worthy?", "tokens": [50400, 584, 321, 528, 281, 8460, 341, 4295, 11, 457, 577, 360, 321, 980, 498, 257, 9984, 307, 14829, 30, 50716], "temperature": 0.0, "avg_logprob": -0.15415864777796476, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.002013880992308259}, {"id": 945, "seek": 547600, "start": 5483.04, "end": 5488.88, "text": " Like if a math statement is interesting, because I imagine in this infinite graph,", "tokens": [50716, 1743, 498, 257, 5221, 5629, 307, 1880, 11, 570, 286, 3811, 294, 341, 13785, 4295, 11, 51008], "temperature": 0.0, "avg_logprob": -0.15415864777796476, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.002013880992308259}, {"id": 946, "seek": 547600, "start": 5488.88, "end": 5493.52, "text": " most of the nodes will be just garbage, like two greater than one, three greater than two.", "tokens": [51008, 881, 295, 264, 13891, 486, 312, 445, 14150, 11, 411, 732, 5044, 813, 472, 11, 1045, 5044, 813, 732, 13, 51240], "temperature": 0.0, "avg_logprob": -0.15415864777796476, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.002013880992308259}, {"id": 947, "seek": 547600, "start": 5493.52, "end": 5496.4, "text": " But we really want to focus on this interesting nodes.", "tokens": [51240, 583, 321, 534, 528, 281, 1879, 322, 341, 1880, 13891, 13, 51384], "temperature": 0.0, "avg_logprob": -0.15415864777796476, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.002013880992308259}, {"id": 948, "seek": 547600, "start": 5497.04, "end": 5502.08, "text": " Yes, it's an interesting question. So I've looked at this a bit and I can tell you that in the case", "tokens": [51416, 1079, 11, 309, 311, 364, 1880, 1168, 13, 407, 286, 600, 2956, 412, 341, 257, 857, 293, 286, 393, 980, 291, 300, 294, 264, 1389, 51668], "temperature": 0.0, "avg_logprob": -0.15415864777796476, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.002013880992308259}, {"id": 949, "seek": 550208, "start": 5502.08, "end": 5507.44, "text": " of Boolean algebra, there is a criterion. So maybe I can pull up a picture of that.", "tokens": [50364, 295, 23351, 28499, 21989, 11, 456, 307, 257, 46691, 13, 407, 1310, 286, 393, 2235, 493, 257, 3036, 295, 300, 13, 50632], "temperature": 0.0, "avg_logprob": -0.1541296151968149, "compression_ratio": 1.403225806451613, "no_speech_prob": 0.005900433752685785}, {"id": 950, "seek": 550208, "start": 5508.64, "end": 5524.64, "text": " If you order, here we go. Hold on. Let's see. If you order the theorems of Boolean algebra", "tokens": [50692, 759, 291, 1668, 11, 510, 321, 352, 13, 6962, 322, 13, 961, 311, 536, 13, 759, 291, 1668, 264, 10299, 2592, 295, 23351, 28499, 21989, 51492], "temperature": 0.0, "avg_logprob": -0.1541296151968149, "compression_ratio": 1.403225806451613, "no_speech_prob": 0.005900433752685785}, {"id": 951, "seek": 552464, "start": 5525.360000000001, "end": 5533.52, "text": " in lexicographic order, then you can ask which are the theorems of all possible theorems? Which", "tokens": [50400, 294, 476, 47228, 12295, 1668, 11, 550, 291, 393, 1029, 597, 366, 264, 10299, 2592, 295, 439, 1944, 10299, 2592, 30, 3013, 50808], "temperature": 0.0, "avg_logprob": -0.12762299404349378, "compression_ratio": 1.7853658536585366, "no_speech_prob": 0.023179812356829643}, {"id": 952, "seek": 552464, "start": 5533.52, "end": 5539.92, "text": " ones are given names in logic textbooks? And sort of a surprise to me is the theorems that", "tokens": [50808, 2306, 366, 2212, 5288, 294, 9952, 33587, 30, 400, 1333, 295, 257, 6365, 281, 385, 307, 264, 10299, 2592, 300, 51128], "temperature": 0.0, "avg_logprob": -0.12762299404349378, "compression_ratio": 1.7853658536585366, "no_speech_prob": 0.023179812356829643}, {"id": 953, "seek": 552464, "start": 5539.92, "end": 5544.400000000001, "text": " are given names in logic textbooks are the theorems that have, in this case, no back links.", "tokens": [51128, 366, 2212, 5288, 294, 9952, 33587, 366, 264, 10299, 2592, 300, 362, 11, 294, 341, 1389, 11, 572, 646, 6123, 13, 51352], "temperature": 0.0, "avg_logprob": -0.12762299404349378, "compression_ratio": 1.7853658536585366, "no_speech_prob": 0.023179812356829643}, {"id": 954, "seek": 552464, "start": 5544.400000000001, "end": 5548.72, "text": " So there's a backlink from this result here, which might be one of your boring results.", "tokens": [51352, 407, 456, 311, 257, 646, 22473, 490, 341, 1874, 510, 11, 597, 1062, 312, 472, 295, 428, 9989, 3542, 13, 51568], "temperature": 0.0, "avg_logprob": -0.12762299404349378, "compression_ratio": 1.7853658536585366, "no_speech_prob": 0.023179812356829643}, {"id": 955, "seek": 554872, "start": 5549.4400000000005, "end": 5554.240000000001, "text": " This result is derivable from something earlier in this lexicographic list.", "tokens": [50400, 639, 1874, 307, 10151, 712, 490, 746, 3071, 294, 341, 476, 47228, 12295, 1329, 13, 50640], "temperature": 0.0, "avg_logprob": -0.09181943911950564, "compression_ratio": 1.7510373443983402, "no_speech_prob": 0.018522722646594048}, {"id": 956, "seek": 554872, "start": 5555.12, "end": 5560.16, "text": " So in a sense, it gives you no new information. It turns out the ones that get given names", "tokens": [50684, 407, 294, 257, 2020, 11, 309, 2709, 291, 572, 777, 1589, 13, 467, 4523, 484, 264, 2306, 300, 483, 2212, 5288, 50936], "temperature": 0.0, "avg_logprob": -0.09181943911950564, "compression_ratio": 1.7510373443983402, "no_speech_prob": 0.018522722646594048}, {"id": 957, "seek": 554872, "start": 5560.16, "end": 5564.64, "text": " are precisely the ones that do not have back links. They are not derivable", "tokens": [50936, 366, 13402, 264, 2306, 300, 360, 406, 362, 646, 6123, 13, 814, 366, 406, 10151, 712, 51160], "temperature": 0.0, "avg_logprob": -0.09181943911950564, "compression_ratio": 1.7510373443983402, "no_speech_prob": 0.018522722646594048}, {"id": 958, "seek": 554872, "start": 5564.64, "end": 5570.8, "text": " from lexicographically simpler theorems. So in other words, I was surprised that I discovered", "tokens": [51160, 490, 476, 47228, 3108, 984, 18587, 10299, 2592, 13, 407, 294, 661, 2283, 11, 286, 390, 6100, 300, 286, 6941, 51468], "temperature": 0.0, "avg_logprob": -0.09181943911950564, "compression_ratio": 1.7510373443983402, "no_speech_prob": 0.018522722646594048}, {"id": 959, "seek": 554872, "start": 5570.8, "end": 5575.360000000001, "text": " this sometime in the 90s. I was surprised by this, that there was actually a criterion", "tokens": [51468, 341, 15053, 294, 264, 4289, 82, 13, 286, 390, 6100, 538, 341, 11, 300, 456, 390, 767, 257, 46691, 51696], "temperature": 0.0, "avg_logprob": -0.09181943911950564, "compression_ratio": 1.7510373443983402, "no_speech_prob": 0.018522722646594048}, {"id": 960, "seek": 557536, "start": 5575.44, "end": 5580.08, "text": " for what would be given a name in a logic textbook. Now, the general case of is this", "tokens": [50368, 337, 437, 576, 312, 2212, 257, 1315, 294, 257, 9952, 25591, 13, 823, 11, 264, 2674, 1389, 295, 307, 341, 50600], "temperature": 0.0, "avg_logprob": -0.12168333265516493, "compression_ratio": 1.660649819494585, "no_speech_prob": 0.028163732960820198}, {"id": 961, "seek": 557536, "start": 5580.08, "end": 5585.28, "text": " theorem interesting? Can we learn enough about the humans to know what they'll think is interesting?", "tokens": [50600, 20904, 1880, 30, 1664, 321, 1466, 1547, 466, 264, 6255, 281, 458, 437, 436, 603, 519, 307, 1880, 30, 50860], "temperature": 0.0, "avg_logprob": -0.12168333265516493, "compression_ratio": 1.660649819494585, "no_speech_prob": 0.028163732960820198}, {"id": 962, "seek": 557536, "start": 5585.28, "end": 5589.44, "text": " It's a good question. I mean, by the way, in this connection between formal and informal,", "tokens": [50860, 467, 311, 257, 665, 1168, 13, 286, 914, 11, 538, 264, 636, 11, 294, 341, 4984, 1296, 9860, 293, 24342, 11, 51068], "temperature": 0.0, "avg_logprob": -0.12168333265516493, "compression_ratio": 1.660649819494585, "no_speech_prob": 0.028163732960820198}, {"id": 963, "seek": 557536, "start": 5589.44, "end": 5595.2, "text": " obviously, Wolfram language gets connected to LLMs. And we've done lots of work in kind of", "tokens": [51068, 2745, 11, 16634, 2356, 2856, 2170, 4582, 281, 441, 43, 26386, 13, 400, 321, 600, 1096, 3195, 295, 589, 294, 733, 295, 51356], "temperature": 0.0, "avg_logprob": -0.12168333265516493, "compression_ratio": 1.660649819494585, "no_speech_prob": 0.028163732960820198}, {"id": 964, "seek": 557536, "start": 5595.2, "end": 5600.799999999999, "text": " tool calling from LLMs to Wolfram language. And there's this whole question of, can you take,", "tokens": [51356, 2290, 5141, 490, 441, 43, 26386, 281, 16634, 2356, 2856, 13, 400, 456, 311, 341, 1379, 1168, 295, 11, 393, 291, 747, 11, 51636], "temperature": 0.0, "avg_logprob": -0.12168333265516493, "compression_ratio": 1.660649819494585, "no_speech_prob": 0.028163732960820198}, {"id": 965, "seek": 560080, "start": 5600.96, "end": 5605.84, "text": " to what extent can you get the LLM to crisp things up to the point where you can, I don't know,", "tokens": [50372, 281, 437, 8396, 393, 291, 483, 264, 441, 43, 44, 281, 22952, 721, 493, 281, 264, 935, 689, 291, 393, 11, 286, 500, 380, 458, 11, 50616], "temperature": 0.0, "avg_logprob": -0.15316421985626222, "compression_ratio": 1.4857142857142858, "no_speech_prob": 0.02003362402319908}, {"id": 966, "seek": 560080, "start": 5605.84, "end": 5614.16, "text": " I mean, if I say something like draw a pentagon and a hexagon, for example,", "tokens": [50616, 286, 914, 11, 498, 286, 584, 746, 411, 2642, 257, 16834, 6709, 293, 257, 23291, 6709, 11, 337, 1365, 11, 51032], "temperature": 0.0, "avg_logprob": -0.15316421985626222, "compression_ratio": 1.4857142857142858, "no_speech_prob": 0.02003362402319908}, {"id": 967, "seek": 560080, "start": 5615.2, "end": 5620.16, "text": " let's see what it does. I don't know if it'll figure it out or not. It might be able to,", "tokens": [51084, 718, 311, 536, 437, 309, 775, 13, 286, 500, 380, 458, 498, 309, 603, 2573, 309, 484, 420, 406, 13, 467, 1062, 312, 1075, 281, 11, 51332], "temperature": 0.0, "avg_logprob": -0.15316421985626222, "compression_ratio": 1.4857142857142858, "no_speech_prob": 0.02003362402319908}, {"id": 968, "seek": 562016, "start": 5621.12, "end": 5632.08, "text": " we could, the question is, can we generate a, can we turn that informal statement into a piece", "tokens": [50412, 321, 727, 11, 264, 1168, 307, 11, 393, 321, 8460, 257, 11, 393, 321, 1261, 300, 24342, 5629, 666, 257, 2522, 50960], "temperature": 0.0, "avg_logprob": -0.13157427156126345, "compression_ratio": 1.538888888888889, "no_speech_prob": 0.1004159078001976}, {"id": 969, "seek": 562016, "start": 5632.08, "end": 5640.0, "text": " of formal Wolfram language code? Okay, not bad to manage to do that one. And if we look here,", "tokens": [50960, 295, 9860, 16634, 2356, 2856, 3089, 30, 1033, 11, 406, 1578, 281, 3067, 281, 360, 300, 472, 13, 400, 498, 321, 574, 510, 11, 51356], "temperature": 0.0, "avg_logprob": -0.13157427156126345, "compression_ratio": 1.538888888888889, "no_speech_prob": 0.1004159078001976}, {"id": 970, "seek": 562016, "start": 5640.0, "end": 5646.639999999999, "text": " I'm sure we can get it to, there we go. So that showed us the actual code that did that,", "tokens": [51356, 286, 478, 988, 321, 393, 483, 309, 281, 11, 456, 321, 352, 13, 407, 300, 4712, 505, 264, 3539, 3089, 300, 630, 300, 11, 51688], "temperature": 0.0, "avg_logprob": -0.13157427156126345, "compression_ratio": 1.538888888888889, "no_speech_prob": 0.1004159078001976}, {"id": 971, "seek": 564664, "start": 5646.64, "end": 5651.4400000000005, "text": " wouldn't have been the way I would have done it, but it's okay. That's a reasonable way to do it.", "tokens": [50364, 2759, 380, 362, 668, 264, 636, 286, 576, 362, 1096, 309, 11, 457, 309, 311, 1392, 13, 663, 311, 257, 10585, 636, 281, 360, 309, 13, 50604], "temperature": 0.0, "avg_logprob": -0.07962095635568994, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.06654085963964462}, {"id": 972, "seek": 564664, "start": 5651.4400000000005, "end": 5655.76, "text": " But so this is a case where we're going from an informal description to this computational", "tokens": [50604, 583, 370, 341, 307, 257, 1389, 689, 321, 434, 516, 490, 364, 24342, 3855, 281, 341, 28270, 50820], "temperature": 0.0, "avg_logprob": -0.07962095635568994, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.06654085963964462}, {"id": 973, "seek": 564664, "start": 5655.76, "end": 5659.92, "text": " language, which we can then compute from. And that's a very powerful thing to do. And in fact,", "tokens": [50820, 2856, 11, 597, 321, 393, 550, 14722, 490, 13, 400, 300, 311, 257, 588, 4005, 551, 281, 360, 13, 400, 294, 1186, 11, 51028], "temperature": 0.0, "avg_logprob": -0.07962095635568994, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.06654085963964462}, {"id": 974, "seek": 564664, "start": 5659.92, "end": 5664.160000000001, "text": " we even have a product that's coming out soon that is based precisely on that idea.", "tokens": [51028, 321, 754, 362, 257, 1674, 300, 311, 1348, 484, 2321, 300, 307, 2361, 13402, 322, 300, 1558, 13, 51240], "temperature": 0.0, "avg_logprob": -0.07962095635568994, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.06654085963964462}, {"id": 975, "seek": 564664, "start": 5665.280000000001, "end": 5671.92, "text": " But so I think this question of whether you can sort of, can you guide the proof this way?", "tokens": [51296, 583, 370, 286, 519, 341, 1168, 295, 1968, 291, 393, 1333, 295, 11, 393, 291, 5934, 264, 8177, 341, 636, 30, 51628], "temperature": 0.0, "avg_logprob": -0.07962095635568994, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.06654085963964462}, {"id": 976, "seek": 567192, "start": 5671.92, "end": 5677.84, "text": " I suspect you can. Now this question of what is a human proof, what's, okay, this is an example.", "tokens": [50364, 286, 9091, 291, 393, 13, 823, 341, 1168, 295, 437, 307, 257, 1952, 8177, 11, 437, 311, 11, 1392, 11, 341, 307, 364, 1365, 13, 50660], "temperature": 0.0, "avg_logprob": -0.10149508123999243, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.05321258679032326}, {"id": 977, "seek": 567192, "start": 5677.84, "end": 5683.2, "text": " So in automated theorem proving, one of the shocking things about automated theorem proving,", "tokens": [50660, 407, 294, 18473, 20904, 27221, 11, 472, 295, 264, 18776, 721, 466, 18473, 20904, 27221, 11, 50928], "temperature": 0.0, "avg_logprob": -0.10149508123999243, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.05321258679032326}, {"id": 978, "seek": 567192, "start": 5683.2, "end": 5688.0, "text": " I believe you might correct me and tell me, one day somebody is going to tell me I'm wrong about", "tokens": [50928, 286, 1697, 291, 1062, 3006, 385, 293, 980, 385, 11, 472, 786, 2618, 307, 516, 281, 980, 385, 286, 478, 2085, 466, 51168], "temperature": 0.0, "avg_logprob": -0.10149508123999243, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.05321258679032326}, {"id": 979, "seek": 567192, "start": 5688.0, "end": 5694.56, "text": " this. But so far as I know, essentially all the theorems that have been proved by automated", "tokens": [51168, 341, 13, 583, 370, 1400, 382, 286, 458, 11, 4476, 439, 264, 10299, 2592, 300, 362, 668, 14617, 538, 18473, 51496], "temperature": 0.0, "avg_logprob": -0.10149508123999243, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.05321258679032326}, {"id": 980, "seek": 567192, "start": 5694.56, "end": 5699.28, "text": " theorem proving were theorems that somebody already believed were true. In other words,", "tokens": [51496, 20904, 27221, 645, 10299, 2592, 300, 2618, 1217, 7847, 645, 2074, 13, 682, 661, 2283, 11, 51732], "temperature": 0.0, "avg_logprob": -0.10149508123999243, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.05321258679032326}, {"id": 981, "seek": 569928, "start": 5699.28, "end": 5705.2, "text": " there is no newly discovered thing that came from automated theorem proving with one counter", "tokens": [50364, 456, 307, 572, 15109, 6941, 551, 300, 1361, 490, 18473, 20904, 27221, 365, 472, 5682, 50660], "temperature": 0.0, "avg_logprob": -0.10183395428603954, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.002839485416188836}, {"id": 982, "seek": 569928, "start": 5705.2, "end": 5710.4, "text": " example. The one counter example is something I found 24 years ago now, which is this is the", "tokens": [50660, 1365, 13, 440, 472, 5682, 1365, 307, 746, 286, 1352, 4022, 924, 2057, 586, 11, 597, 307, 341, 307, 264, 50920], "temperature": 0.0, "avg_logprob": -0.10183395428603954, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.002839485416188836}, {"id": 983, "seek": 569928, "start": 5710.4, "end": 5717.12, "text": " simplest axiom system for Boolean algebra. So you can think of that as a NAND operator.", "tokens": [50920, 22811, 6360, 72, 298, 1185, 337, 23351, 28499, 21989, 13, 407, 291, 393, 519, 295, 300, 382, 257, 426, 8070, 12973, 13, 51256], "temperature": 0.0, "avg_logprob": -0.10183395428603954, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.002839485416188836}, {"id": 984, "seek": 569928, "start": 5717.12, "end": 5723.759999999999, "text": " This is of all possible from that one axiom, you can derive all the true statements of Boolean", "tokens": [51256, 639, 307, 295, 439, 1944, 490, 300, 472, 6360, 72, 298, 11, 291, 393, 28446, 439, 264, 2074, 12363, 295, 23351, 28499, 51588], "temperature": 0.0, "avg_logprob": -0.10183395428603954, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.002839485416188836}, {"id": 985, "seek": 572376, "start": 5723.76, "end": 5732.400000000001, "text": " algebra. The proof of that is this long 100 step automated proof. Let's see if I have a picture", "tokens": [50364, 21989, 13, 440, 8177, 295, 300, 307, 341, 938, 2319, 1823, 18473, 8177, 13, 961, 311, 536, 498, 286, 362, 257, 3036, 50796], "temperature": 0.0, "avg_logprob": -0.08214795220758497, "compression_ratio": 1.5708333333333333, "no_speech_prob": 0.008037141524255276}, {"id": 986, "seek": 572376, "start": 5732.400000000001, "end": 5736.8, "text": " of it. Yeah, I mean, that's sort of some kind of visual representation of that proof. It has", "tokens": [50796, 295, 309, 13, 865, 11, 286, 914, 11, 300, 311, 1333, 295, 512, 733, 295, 5056, 10290, 295, 300, 8177, 13, 467, 575, 51016], "temperature": 0.0, "avg_logprob": -0.08214795220758497, "compression_ratio": 1.5708333333333333, "no_speech_prob": 0.008037141524255276}, {"id": 987, "seek": 572376, "start": 5736.8, "end": 5742.56, "text": " various popular lemmas in it and so on. In the last 24 years, despite quite a bit of effort,", "tokens": [51016, 3683, 3743, 7495, 3799, 294, 309, 293, 370, 322, 13, 682, 264, 1036, 4022, 924, 11, 7228, 1596, 257, 857, 295, 4630, 11, 51304], "temperature": 0.0, "avg_logprob": -0.08214795220758497, "compression_ratio": 1.5708333333333333, "no_speech_prob": 0.008037141524255276}, {"id": 988, "seek": 572376, "start": 5742.56, "end": 5747.84, "text": " actually, nobody has ever understood this proof. But it is interesting because it is a proof of", "tokens": [51304, 767, 11, 5079, 575, 1562, 7320, 341, 8177, 13, 583, 309, 307, 1880, 570, 309, 307, 257, 8177, 295, 51568], "temperature": 0.0, "avg_logprob": -0.08214795220758497, "compression_ratio": 1.5708333333333333, "no_speech_prob": 0.008037141524255276}, {"id": 989, "seek": 574784, "start": 5747.84, "end": 5753.6, "text": " something surprising, potentially interesting, depending on whether you care about simplest", "tokens": [50364, 746, 8830, 11, 7263, 1880, 11, 5413, 322, 1968, 291, 1127, 466, 22811, 50652], "temperature": 0.0, "avg_logprob": -0.1311255476691506, "compression_ratio": 1.5947136563876652, "no_speech_prob": 0.01392311044037342}, {"id": 990, "seek": 574784, "start": 5753.6, "end": 5760.4800000000005, "text": " axiom systems for things. But it was found by automated theorem proving without already knowing", "tokens": [50652, 6360, 72, 298, 3652, 337, 721, 13, 583, 309, 390, 1352, 538, 18473, 20904, 27221, 1553, 1217, 5276, 50996], "temperature": 0.0, "avg_logprob": -0.1311255476691506, "compression_ratio": 1.5947136563876652, "no_speech_prob": 0.01392311044037342}, {"id": 991, "seek": 574784, "start": 5760.4800000000005, "end": 5767.28, "text": " what you were searching for, so to speak. And that's a case where now you ask the question,", "tokens": [50996, 437, 291, 645, 10808, 337, 11, 370, 281, 1710, 13, 400, 300, 311, 257, 1389, 689, 586, 291, 1029, 264, 1168, 11, 51336], "temperature": 0.0, "avg_logprob": -0.1311255476691506, "compression_ratio": 1.5947136563876652, "no_speech_prob": 0.01392311044037342}, {"id": 992, "seek": 574784, "start": 5767.28, "end": 5774.56, "text": " if you're out in the wilds of, I mean, we can look, I have a nice picture of this,", "tokens": [51336, 498, 291, 434, 484, 294, 264, 4868, 82, 295, 11, 286, 914, 11, 321, 393, 574, 11, 286, 362, 257, 1481, 3036, 295, 341, 11, 51700], "temperature": 0.0, "avg_logprob": -0.1311255476691506, "compression_ratio": 1.5947136563876652, "no_speech_prob": 0.01392311044037342}, {"id": 993, "seek": 577456, "start": 5774.56, "end": 5785.120000000001, "text": " we can look at, oh, here, is that one? Yeah, this is, these are axiom systems down the left.", "tokens": [50364, 321, 393, 574, 412, 11, 1954, 11, 510, 11, 307, 300, 472, 30, 865, 11, 341, 307, 11, 613, 366, 6360, 72, 298, 3652, 760, 264, 1411, 13, 50892], "temperature": 0.0, "avg_logprob": -0.14768182247056874, "compression_ratio": 1.7568807339449541, "no_speech_prob": 0.0012021885486319661}, {"id": 994, "seek": 577456, "start": 5785.84, "end": 5793.120000000001, "text": " Those are theorems across the top. And there's a dot, a blue square, whenever that theorem", "tokens": [50928, 3950, 366, 10299, 2592, 2108, 264, 1192, 13, 400, 456, 311, 257, 5893, 11, 257, 3344, 3732, 11, 5699, 300, 20904, 51292], "temperature": 0.0, "avg_logprob": -0.14768182247056874, "compression_ratio": 1.7568807339449541, "no_speech_prob": 0.0012021885486319661}, {"id": 995, "seek": 577456, "start": 5793.120000000001, "end": 5799.280000000001, "text": " is true in that axiom system. So we can ask the question, given an axiom system that we decide", "tokens": [51292, 307, 2074, 294, 300, 6360, 72, 298, 1185, 13, 407, 321, 393, 1029, 264, 1168, 11, 2212, 364, 6360, 72, 298, 1185, 300, 321, 4536, 51600], "temperature": 0.0, "avg_logprob": -0.14768182247056874, "compression_ratio": 1.7568807339449541, "no_speech_prob": 0.0012021885486319661}, {"id": 996, "seek": 577456, "start": 5799.280000000001, "end": 5804.240000000001, "text": " is exciting, and how we decide that is an interesting question, it's unright. But given an axiom system,", "tokens": [51600, 307, 4670, 11, 293, 577, 321, 4536, 300, 307, 364, 1880, 1168, 11, 309, 311, 517, 1938, 13, 583, 2212, 364, 6360, 72, 298, 1185, 11, 51848], "temperature": 0.0, "avg_logprob": -0.14768182247056874, "compression_ratio": 1.7568807339449541, "no_speech_prob": 0.0012021885486319661}, {"id": 997, "seek": 580456, "start": 5804.64, "end": 5810.240000000001, "text": " we can say, here are the theorems that are true. Now, which are the theorems here that we care about?", "tokens": [50368, 321, 393, 584, 11, 510, 366, 264, 10299, 2592, 300, 366, 2074, 13, 823, 11, 597, 366, 264, 10299, 2592, 510, 300, 321, 1127, 466, 30, 50648], "temperature": 0.0, "avg_logprob": -0.10931164120870923, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.0017230487428605556}, {"id": 998, "seek": 580456, "start": 5810.88, "end": 5816.240000000001, "text": " And that's essentially a model for humans. And I think it's an interesting question. We've done", "tokens": [50680, 400, 300, 311, 4476, 257, 2316, 337, 6255, 13, 400, 286, 519, 309, 311, 364, 1880, 1168, 13, 492, 600, 1096, 50948], "temperature": 0.0, "avg_logprob": -0.10931164120870923, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.0017230487428605556}, {"id": 999, "seek": 580456, "start": 5816.240000000001, "end": 5821.6, "text": " some experiments kind of grinding up archive and so on, and trying to figure out, can we deduce", "tokens": [50948, 512, 12050, 733, 295, 25300, 493, 23507, 293, 370, 322, 11, 293, 1382, 281, 2573, 484, 11, 393, 321, 4172, 4176, 51216], "temperature": 0.0, "avg_logprob": -0.10931164120870923, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.0017230487428605556}, {"id": 1000, "seek": 580456, "start": 5822.240000000001, "end": 5828.160000000001, "text": " what, in a space of possible theorems, what theorems are likely to be interesting? I don't", "tokens": [51248, 437, 11, 294, 257, 1901, 295, 1944, 10299, 2592, 11, 437, 10299, 2592, 366, 3700, 281, 312, 1880, 30, 286, 500, 380, 51544], "temperature": 0.0, "avg_logprob": -0.10931164120870923, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.0017230487428605556}, {"id": 1001, "seek": 580456, "start": 5828.160000000001, "end": 5833.04, "text": " know if you've looked at that. Is that, I think that's an interesting thing to look at. Have you", "tokens": [51544, 458, 498, 291, 600, 2956, 412, 300, 13, 1119, 300, 11, 286, 519, 300, 311, 364, 1880, 551, 281, 574, 412, 13, 3560, 291, 51788], "temperature": 0.0, "avg_logprob": -0.10931164120870923, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.0017230487428605556}, {"id": 1002, "seek": 583304, "start": 5833.04, "end": 5845.6, "text": " looked at that? Yeah, I think the way I'm looking at it is more, so I kind of am not considering", "tokens": [50364, 2956, 412, 300, 30, 865, 11, 286, 519, 264, 636, 286, 478, 1237, 412, 309, 307, 544, 11, 370, 286, 733, 295, 669, 406, 8079, 50992], "temperature": 0.0, "avg_logprob": -0.1238607084247428, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.0020457804203033447}, {"id": 1003, "seek": 583304, "start": 5845.6, "end": 5853.36, "text": " its relationship with other theorems. I'm taking the theorem statement itself and try to have some,", "tokens": [50992, 1080, 2480, 365, 661, 10299, 2592, 13, 286, 478, 1940, 264, 20904, 5629, 2564, 293, 853, 281, 362, 512, 11, 51380], "temperature": 0.0, "avg_logprob": -0.1238607084247428, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.0020457804203033447}, {"id": 1004, "seek": 583304, "start": 5854.08, "end": 5858.24, "text": " for example, have the language model telling me whether it's interesting. I believe the language", "tokens": [51416, 337, 1365, 11, 362, 264, 2856, 2316, 3585, 385, 1968, 309, 311, 1880, 13, 286, 1697, 264, 2856, 51624], "temperature": 0.0, "avg_logprob": -0.1238607084247428, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.0020457804203033447}, {"id": 1005, "seek": 585824, "start": 5858.24, "end": 5865.76, "text": " model can look at some kind of superficial cues, like how long the theorem is and what are the", "tokens": [50364, 2316, 393, 574, 412, 512, 733, 295, 34622, 32192, 11, 411, 577, 938, 264, 20904, 307, 293, 437, 366, 264, 50740], "temperature": 0.0, "avg_logprob": -0.12435925661862551, "compression_ratio": 1.740909090909091, "no_speech_prob": 0.0009997760644182563}, {"id": 1006, "seek": 585824, "start": 5865.76, "end": 5872.0, "text": " variables, how they are arranged, how messy it is, and which can already give us some way of", "tokens": [50740, 9102, 11, 577, 436, 366, 18721, 11, 577, 16191, 309, 307, 11, 293, 597, 393, 1217, 976, 505, 512, 636, 295, 51052], "temperature": 0.0, "avg_logprob": -0.12435925661862551, "compression_ratio": 1.740909090909091, "no_speech_prob": 0.0009997760644182563}, {"id": 1007, "seek": 585824, "start": 5873.36, "end": 5879.2, "text": " judging how interesting it is. But I agree, like maybe ultimately what an interesting theorem is,", "tokens": [51120, 23587, 577, 1880, 309, 307, 13, 583, 286, 3986, 11, 411, 1310, 6284, 437, 364, 1880, 20904, 307, 11, 51412], "temperature": 0.0, "avg_logprob": -0.12435925661862551, "compression_ratio": 1.740909090909091, "no_speech_prob": 0.0009997760644182563}, {"id": 1008, "seek": 585824, "start": 5880.48, "end": 5886.4, "text": " it can help you prove a lot of other theorems. Is that your definition of an interesting theorem?", "tokens": [51476, 309, 393, 854, 291, 7081, 257, 688, 295, 661, 10299, 2592, 13, 1119, 300, 428, 7123, 295, 364, 1880, 20904, 30, 51772], "temperature": 0.0, "avg_logprob": -0.12435925661862551, "compression_ratio": 1.740909090909091, "no_speech_prob": 0.0009997760644182563}, {"id": 1009, "seek": 588640, "start": 5886.4, "end": 5893.92, "text": " I'm not sure that's right. I mean, in other words, that is one possible way, I guess.", "tokens": [50364, 286, 478, 406, 988, 300, 311, 558, 13, 286, 914, 11, 294, 661, 2283, 11, 300, 307, 472, 1944, 636, 11, 286, 2041, 13, 50740], "temperature": 0.0, "avg_logprob": -0.158707837024367, "compression_ratio": 1.5560747663551402, "no_speech_prob": 0.012782973237335682}, {"id": 1010, "seek": 588640, "start": 5895.28, "end": 5899.599999999999, "text": " There are many different criteria you can imagine for interestingness. That particular one", "tokens": [50808, 821, 366, 867, 819, 11101, 291, 393, 3811, 337, 1880, 1287, 13, 663, 1729, 472, 51024], "temperature": 0.0, "avg_logprob": -0.158707837024367, "compression_ratio": 1.5560747663551402, "no_speech_prob": 0.012782973237335682}, {"id": 1011, "seek": 588640, "start": 5899.599999999999, "end": 5905.12, "text": " would be saying that if that was the correct criterion, then what you could do,", "tokens": [51024, 576, 312, 1566, 300, 498, 300, 390, 264, 3006, 46691, 11, 550, 437, 291, 727, 360, 11, 51300], "temperature": 0.0, "avg_logprob": -0.158707837024367, "compression_ratio": 1.5560747663551402, "no_speech_prob": 0.012782973237335682}, {"id": 1012, "seek": 588640, "start": 5906.24, "end": 5910.08, "text": " like I was just showing that picture, actually, in the Boolean algebra case,", "tokens": [51356, 411, 286, 390, 445, 4099, 300, 3036, 11, 767, 11, 294, 264, 23351, 28499, 21989, 1389, 11, 51548], "temperature": 0.0, "avg_logprob": -0.158707837024367, "compression_ratio": 1.5560747663551402, "no_speech_prob": 0.012782973237335682}, {"id": 1013, "seek": 591008, "start": 5910.64, "end": 5919.84, "text": " wherever it is, then I would deduce from your statement that the theorems that are big here are", "tokens": [50392, 8660, 309, 307, 11, 550, 286, 576, 4172, 4176, 490, 428, 5629, 300, 264, 10299, 2592, 300, 366, 955, 510, 366, 50852], "temperature": 0.0, "avg_logprob": -0.13485974013203322, "compression_ratio": 1.7819905213270142, "no_speech_prob": 0.019970819354057312}, {"id": 1014, "seek": 591008, "start": 5919.84, "end": 5926.32, "text": " the ones that have many, those are the high-out-degree theorems. In other words, those theorems,", "tokens": [50852, 264, 2306, 300, 362, 867, 11, 729, 366, 264, 1090, 12, 346, 12, 34368, 10299, 2592, 13, 682, 661, 2283, 11, 729, 10299, 2592, 11, 51176], "temperature": 0.0, "avg_logprob": -0.13485974013203322, "compression_ratio": 1.7819905213270142, "no_speech_prob": 0.019970819354057312}, {"id": 1015, "seek": 591008, "start": 5926.32, "end": 5931.5199999999995, "text": " that theorem there should be the one I should care about. I don't understand these theorems,", "tokens": [51176, 300, 20904, 456, 820, 312, 264, 472, 286, 820, 1127, 466, 13, 286, 500, 380, 1223, 613, 10299, 2592, 11, 51436], "temperature": 0.0, "avg_logprob": -0.13485974013203322, "compression_ratio": 1.7819905213270142, "no_speech_prob": 0.019970819354057312}, {"id": 1016, "seek": 591008, "start": 5931.5199999999995, "end": 5938.96, "text": " honestly. What's that? I mean, in that sense, yes, because they are special in this graph.", "tokens": [51436, 6095, 13, 708, 311, 300, 30, 286, 914, 11, 294, 300, 2020, 11, 2086, 11, 570, 436, 366, 2121, 294, 341, 4295, 13, 51808], "temperature": 0.0, "avg_logprob": -0.13485974013203322, "compression_ratio": 1.7819905213270142, "no_speech_prob": 0.019970819354057312}, {"id": 1017, "seek": 593896, "start": 5939.68, "end": 5947.2, "text": " That's right. But the question of whether those are human useful is, I think, a different question.", "tokens": [50400, 663, 311, 558, 13, 583, 264, 1168, 295, 1968, 729, 366, 1952, 4420, 307, 11, 286, 519, 11, 257, 819, 1168, 13, 50776], "temperature": 0.0, "avg_logprob": -0.12935473793431332, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.0007086468394845724}, {"id": 1018, "seek": 593896, "start": 5947.2, "end": 5953.68, "text": " I mean, in other words, what is, you know, there's this question. It could be the case that two,", "tokens": [50776, 286, 914, 11, 294, 661, 2283, 11, 437, 307, 11, 291, 458, 11, 456, 311, 341, 1168, 13, 467, 727, 312, 264, 1389, 300, 732, 11, 51100], "temperature": 0.0, "avg_logprob": -0.12935473793431332, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.0007086468394845724}, {"id": 1019, "seek": 593896, "start": 5953.68, "end": 5959.28, "text": " okay, first question is, if you look at many different possible things you might prove,", "tokens": [51100, 1392, 11, 700, 1168, 307, 11, 498, 291, 574, 412, 867, 819, 1944, 721, 291, 1062, 7081, 11, 51380], "temperature": 0.0, "avg_logprob": -0.12935473793431332, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.0007086468394845724}, {"id": 1020, "seek": 593896, "start": 5959.28, "end": 5965.2, "text": " you can ask the question, are there repeated theorems that often, are there repeated lemmas", "tokens": [51380, 291, 393, 1029, 264, 1168, 11, 366, 456, 10477, 10299, 2592, 300, 2049, 11, 366, 456, 10477, 7495, 3799, 51676], "temperature": 0.0, "avg_logprob": -0.12935473793431332, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.0007086468394845724}, {"id": 1021, "seek": 596520, "start": 5965.2, "end": 5971.679999999999, "text": " that often come up in those proofs? Okay. So I looked at that. And the answer is there are.", "tokens": [50364, 300, 2049, 808, 493, 294, 729, 8177, 82, 30, 1033, 13, 407, 286, 2956, 412, 300, 13, 400, 264, 1867, 307, 456, 366, 13, 50688], "temperature": 0.0, "avg_logprob": -0.1079579664736378, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.004312791861593723}, {"id": 1022, "seek": 596520, "start": 5972.32, "end": 5979.92, "text": " And so, for example, that axiom system of mine for Boolean algebra, if you use it to prove", "tokens": [50720, 400, 370, 11, 337, 1365, 11, 300, 6360, 72, 298, 1185, 295, 3892, 337, 23351, 28499, 21989, 11, 498, 291, 764, 309, 281, 7081, 51100], "temperature": 0.0, "avg_logprob": -0.1079579664736378, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.004312791861593723}, {"id": 1023, "seek": 596520, "start": 5979.92, "end": 5985.2, "text": " theorems in Boolean algebra, you can just look at what intermediate lemmas does a theorem prove", "tokens": [51100, 10299, 2592, 294, 23351, 28499, 21989, 11, 291, 393, 445, 574, 412, 437, 19376, 7495, 3799, 775, 257, 20904, 7081, 51364], "temperature": 0.0, "avg_logprob": -0.1079579664736378, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.004312791861593723}, {"id": 1024, "seek": 596520, "start": 5985.2, "end": 5990.08, "text": " are typically proved to make progress. And the answer is, for example, it takes it 100 steps", "tokens": [51364, 366, 5850, 14617, 281, 652, 4205, 13, 400, 264, 1867, 307, 11, 337, 1365, 11, 309, 2516, 309, 2319, 4439, 51608], "temperature": 0.0, "avg_logprob": -0.1079579664736378, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.004312791861593723}, {"id": 1025, "seek": 599008, "start": 5990.08, "end": 5995.04, "text": " to prove the commutativity of NAND, and it often does that and then goes on and does other things.", "tokens": [50364, 281, 7081, 264, 800, 325, 30142, 295, 426, 8070, 11, 293, 309, 2049, 775, 300, 293, 550, 1709, 322, 293, 775, 661, 721, 13, 50612], "temperature": 0.0, "avg_logprob": -0.11155497543210906, "compression_ratio": 1.7518796992481203, "no_speech_prob": 0.01740092970430851}, {"id": 1026, "seek": 599008, "start": 5995.68, "end": 6000.24, "text": " So in that sense, you know, you can, I mean, it is an interesting experimental question.", "tokens": [50644, 407, 294, 300, 2020, 11, 291, 458, 11, 291, 393, 11, 286, 914, 11, 309, 307, 364, 1880, 17069, 1168, 13, 50872], "temperature": 0.0, "avg_logprob": -0.11155497543210906, "compression_ratio": 1.7518796992481203, "no_speech_prob": 0.01740092970430851}, {"id": 1027, "seek": 599008, "start": 6000.24, "end": 6004.72, "text": " To what extent are there repeated lemmas that show up? And that might be a criterion,", "tokens": [50872, 1407, 437, 8396, 366, 456, 10477, 7495, 3799, 300, 855, 493, 30, 400, 300, 1062, 312, 257, 46691, 11, 51096], "temperature": 0.0, "avg_logprob": -0.11155497543210906, "compression_ratio": 1.7518796992481203, "no_speech_prob": 0.01740092970430851}, {"id": 1028, "seek": 599008, "start": 6004.72, "end": 6008.8, "text": " but that's a criterion that has nothing to do with LLMs and so on. That's a criterion that just", "tokens": [51096, 457, 300, 311, 257, 46691, 300, 575, 1825, 281, 360, 365, 441, 43, 26386, 293, 370, 322, 13, 663, 311, 257, 46691, 300, 445, 51300], "temperature": 0.0, "avg_logprob": -0.11155497543210906, "compression_ratio": 1.7518796992481203, "no_speech_prob": 0.01740092970430851}, {"id": 1029, "seek": 599008, "start": 6008.8, "end": 6017.28, "text": " has to do with the mathematical graph. I think this question of, you know, if we look at images,", "tokens": [51300, 575, 281, 360, 365, 264, 18894, 4295, 13, 286, 519, 341, 1168, 295, 11, 291, 458, 11, 498, 321, 574, 412, 5267, 11, 51724], "temperature": 0.0, "avg_logprob": -0.11155497543210906, "compression_ratio": 1.7518796992481203, "no_speech_prob": 0.01740092970430851}, {"id": 1030, "seek": 601728, "start": 6017.28, "end": 6022.96, "text": " for example, you didn't see what I was showing earlier, but here I'll pull up a,", "tokens": [50364, 337, 1365, 11, 291, 994, 380, 536, 437, 286, 390, 4099, 3071, 11, 457, 510, 286, 603, 2235, 493, 257, 11, 50648], "temperature": 0.0, "avg_logprob": -0.15204648297242443, "compression_ratio": 1.634703196347032, "no_speech_prob": 0.018259642645716667}, {"id": 1031, "seek": 601728, "start": 6024.719999999999, "end": 6030.88, "text": " I was just showing something like this. This is in, you know, embedding space of a", "tokens": [50736, 286, 390, 445, 4099, 746, 411, 341, 13, 639, 307, 294, 11, 291, 458, 11, 12240, 3584, 1901, 295, 257, 51044], "temperature": 0.0, "avg_logprob": -0.15204648297242443, "compression_ratio": 1.634703196347032, "no_speech_prob": 0.018259642645716667}, {"id": 1032, "seek": 601728, "start": 6030.88, "end": 6036.639999999999, "text": " generative AI system with in the middle is the cat in the party hat, then there's sort of a cat", "tokens": [51044, 1337, 1166, 7318, 1185, 365, 294, 264, 2808, 307, 264, 3857, 294, 264, 3595, 2385, 11, 550, 456, 311, 1333, 295, 257, 3857, 51332], "temperature": 0.0, "avg_logprob": -0.15204648297242443, "compression_ratio": 1.634703196347032, "no_speech_prob": 0.018259642645716667}, {"id": 1033, "seek": 601728, "start": 6036.639999999999, "end": 6041.92, "text": " island of cat-like things, and then you're out in sort of inter-concept space that we have not yet", "tokens": [51332, 6077, 295, 3857, 12, 4092, 721, 11, 293, 550, 291, 434, 484, 294, 1333, 295, 728, 12, 1671, 1336, 1901, 300, 321, 362, 406, 1939, 51596], "temperature": 0.0, "avg_logprob": -0.15204648297242443, "compression_ratio": 1.634703196347032, "no_speech_prob": 0.018259642645716667}, {"id": 1034, "seek": 604192, "start": 6041.92, "end": 6047.6, "text": " explored. And so you can imagine the same kind of thing for mathematics. You say, here's a theorem", "tokens": [50364, 24016, 13, 400, 370, 291, 393, 3811, 264, 912, 733, 295, 551, 337, 18666, 13, 509, 584, 11, 510, 311, 257, 20904, 50648], "temperature": 0.0, "avg_logprob": -0.08201828832211702, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.15876080095767975}, {"id": 1035, "seek": 604192, "start": 6047.6, "end": 6053.92, "text": " that somebody wrote down. Let's sort of change the embedding in some sense and say here are nearby", "tokens": [50648, 300, 2618, 4114, 760, 13, 961, 311, 1333, 295, 1319, 264, 12240, 3584, 294, 512, 2020, 293, 584, 510, 366, 11184, 50964], "temperature": 0.0, "avg_logprob": -0.08201828832211702, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.15876080095767975}, {"id": 1036, "seek": 604192, "start": 6053.92, "end": 6061.4400000000005, "text": " theorems that weren't necessarily, you know, where is the island? How far out does the island of", "tokens": [50964, 10299, 2592, 300, 4999, 380, 4725, 11, 291, 458, 11, 689, 307, 264, 6077, 30, 1012, 1400, 484, 775, 264, 6077, 295, 51340], "temperature": 0.0, "avg_logprob": -0.08201828832211702, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.15876080095767975}, {"id": 1037, "seek": 604192, "start": 6061.4400000000005, "end": 6067.6, "text": " interestingness go? What happens in this kind of inter-concept space between this theorem", "tokens": [51340, 1880, 1287, 352, 30, 708, 2314, 294, 341, 733, 295, 728, 12, 1671, 1336, 1901, 1296, 341, 20904, 51648], "temperature": 0.0, "avg_logprob": -0.08201828832211702, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.15876080095767975}, {"id": 1038, "seek": 604192, "start": 6067.6, "end": 6071.36, "text": " that we thought was interesting and this other one we thought was interesting? So I mean, I think", "tokens": [51648, 300, 321, 1194, 390, 1880, 293, 341, 661, 472, 321, 1194, 390, 1880, 30, 407, 286, 914, 11, 286, 519, 51836], "temperature": 0.0, "avg_logprob": -0.08201828832211702, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.15876080095767975}, {"id": 1039, "seek": 607136, "start": 6072.16, "end": 6080.88, "text": " it's a, I mean, let's take an example. Let's say, I don't know, let's take, well here, we've got,", "tokens": [50404, 309, 311, 257, 11, 286, 914, 11, 718, 311, 747, 364, 1365, 13, 961, 311, 584, 11, 286, 500, 380, 458, 11, 718, 311, 747, 11, 731, 510, 11, 321, 600, 658, 11, 50840], "temperature": 0.0, "avg_logprob": -0.09517005637839988, "compression_ratio": 1.8412698412698412, "no_speech_prob": 0.0006864713504910469}, {"id": 1040, "seek": 607136, "start": 6080.88, "end": 6085.759999999999, "text": " you know, these are random pictures generated in inter-concept space that maybe are of things", "tokens": [50840, 291, 458, 11, 613, 366, 4974, 5242, 10833, 294, 728, 12, 1671, 1336, 1901, 300, 1310, 366, 295, 721, 51084], "temperature": 0.0, "avg_logprob": -0.09517005637839988, "compression_ratio": 1.8412698412698412, "no_speech_prob": 0.0006864713504910469}, {"id": 1041, "seek": 607136, "start": 6085.759999999999, "end": 6090.16, "text": " that we care about. I don't know. I mean, that one on the right, we might kind of think it's,", "tokens": [51084, 300, 321, 1127, 466, 13, 286, 500, 380, 458, 13, 286, 914, 11, 300, 472, 322, 264, 558, 11, 321, 1062, 733, 295, 519, 309, 311, 11, 51304], "temperature": 0.0, "avg_logprob": -0.09517005637839988, "compression_ratio": 1.8412698412698412, "no_speech_prob": 0.0006864713504910469}, {"id": 1042, "seek": 607136, "start": 6090.16, "end": 6094.639999999999, "text": " I don't know what it is, but you know, it was just generated by a generative AI.", "tokens": [51304, 286, 500, 380, 458, 437, 309, 307, 11, 457, 291, 458, 11, 309, 390, 445, 10833, 538, 257, 1337, 1166, 7318, 13, 51528], "temperature": 0.0, "avg_logprob": -0.09517005637839988, "compression_ratio": 1.8412698412698412, "no_speech_prob": 0.0006864713504910469}, {"id": 1043, "seek": 607136, "start": 6094.639999999999, "end": 6099.2, "text": " And similarly, imagine that was a theorem. The question is, is this a theorem that we care about?", "tokens": [51528, 400, 14138, 11, 3811, 300, 390, 257, 20904, 13, 440, 1168, 307, 11, 307, 341, 257, 20904, 300, 321, 1127, 466, 30, 51756], "temperature": 0.0, "avg_logprob": -0.09517005637839988, "compression_ratio": 1.8412698412698412, "no_speech_prob": 0.0006864713504910469}, {"id": 1044, "seek": 609920, "start": 6100.16, "end": 6102.72, "text": " You know, it's just like, is this a picture we somehow", "tokens": [50412, 509, 458, 11, 309, 311, 445, 411, 11, 307, 341, 257, 3036, 321, 6063, 50540], "temperature": 0.0, "avg_logprob": -0.09109676762631065, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0020394036546349525}, {"id": 1045, "seek": 609920, "start": 6103.84, "end": 6110.639999999999, "text": " seems relevant to us? And I think, you know, this question of whether, I mean, if we look,", "tokens": [50596, 2544, 7340, 281, 505, 30, 400, 286, 519, 11, 291, 458, 11, 341, 1168, 295, 1968, 11, 286, 914, 11, 498, 321, 574, 11, 50936], "temperature": 0.0, "avg_logprob": -0.09109676762631065, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0020394036546349525}, {"id": 1046, "seek": 609920, "start": 6111.28, "end": 6115.76, "text": " one of the things that's sort of interesting that one can do is to kind of look at this whole space", "tokens": [50968, 472, 295, 264, 721, 300, 311, 1333, 295, 1880, 300, 472, 393, 360, 307, 281, 733, 295, 574, 412, 341, 1379, 1901, 51192], "temperature": 0.0, "avg_logprob": -0.09109676762631065, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0020394036546349525}, {"id": 1047, "seek": 609920, "start": 6116.639999999999, "end": 6124.24, "text": " of, let's see, we can kind of look at metamathematical space and we can kind of ask,", "tokens": [51236, 295, 11, 718, 311, 536, 11, 321, 393, 733, 295, 574, 412, 1131, 335, 998, 8615, 804, 1901, 293, 321, 393, 733, 295, 1029, 11, 51616], "temperature": 0.0, "avg_logprob": -0.09109676762631065, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0020394036546349525}, {"id": 1048, "seek": 612424, "start": 6125.2, "end": 6131.12, "text": " let's take a look here. I think I had a nice picture and find it. I mean, we can ask all sorts", "tokens": [50412, 718, 311, 747, 257, 574, 510, 13, 286, 519, 286, 632, 257, 1481, 3036, 293, 915, 309, 13, 286, 914, 11, 321, 393, 1029, 439, 7527, 50708], "temperature": 0.0, "avg_logprob": -0.11782087470000645, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.0048881336115300655}, {"id": 1049, "seek": 612424, "start": 6131.12, "end": 6138.4, "text": " of questions about different possible proof structures, which are, that's a meta mathematical", "tokens": [50708, 295, 1651, 466, 819, 1944, 8177, 9227, 11, 597, 366, 11, 300, 311, 257, 19616, 18894, 51072], "temperature": 0.0, "avg_logprob": -0.11782087470000645, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.0048881336115300655}, {"id": 1050, "seek": 612424, "start": 6138.4, "end": 6144.719999999999, "text": " thing, but let's see if I can find a picture here. Yeah, that's a picture of, I think this is", "tokens": [51072, 551, 11, 457, 718, 311, 536, 498, 286, 393, 915, 257, 3036, 510, 13, 865, 11, 300, 311, 257, 3036, 295, 11, 286, 519, 341, 307, 51388], "temperature": 0.0, "avg_logprob": -0.11782087470000645, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.0048881336115300655}, {"id": 1051, "seek": 612424, "start": 6144.719999999999, "end": 6153.44, "text": " from metamath. This is empirical metamathematics. It's asking, in the space of all 200,000 theorems,", "tokens": [51388, 490, 1131, 335, 998, 13, 639, 307, 31886, 1131, 335, 998, 37541, 13, 467, 311, 3365, 11, 294, 264, 1901, 295, 439, 2331, 11, 1360, 10299, 2592, 11, 51824], "temperature": 0.0, "avg_logprob": -0.11782087470000645, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.0048881336115300655}, {"id": 1052, "seek": 615344, "start": 6153.5199999999995, "end": 6160.799999999999, "text": " discussed in, you know, presented in the metamath corpus, where do these famous theorems of mathematics", "tokens": [50368, 7152, 294, 11, 291, 458, 11, 8212, 294, 264, 1131, 335, 998, 1181, 31624, 11, 689, 360, 613, 4618, 10299, 2592, 295, 18666, 50732], "temperature": 0.0, "avg_logprob": -0.12852723121643067, "compression_ratio": 1.7235023041474655, "no_speech_prob": 0.001707271789200604}, {"id": 1053, "seek": 615344, "start": 6160.799999999999, "end": 6166.48, "text": " lie in that space? So it's kind of asking this question that this isn't all possible theorems.", "tokens": [50732, 4544, 294, 300, 1901, 30, 407, 309, 311, 733, 295, 3365, 341, 1168, 300, 341, 1943, 380, 439, 1944, 10299, 2592, 13, 51016], "temperature": 0.0, "avg_logprob": -0.12852723121643067, "compression_ratio": 1.7235023041474655, "no_speech_prob": 0.001707271789200604}, {"id": 1054, "seek": 615344, "start": 6166.48, "end": 6174.16, "text": " This one is reduced to just the ones that are in the, I think it's set dot mm corpus for metamath.", "tokens": [51016, 639, 472, 307, 9212, 281, 445, 264, 2306, 300, 366, 294, 264, 11, 286, 519, 309, 311, 992, 5893, 11169, 1181, 31624, 337, 1131, 335, 998, 13, 51400], "temperature": 0.0, "avg_logprob": -0.12852723121643067, "compression_ratio": 1.7235023041474655, "no_speech_prob": 0.001707271789200604}, {"id": 1055, "seek": 615344, "start": 6176.5599999999995, "end": 6180.24, "text": " But, you know, this is again related to this question of where are the ones,", "tokens": [51520, 583, 11, 291, 458, 11, 341, 307, 797, 4077, 281, 341, 1168, 295, 689, 366, 264, 2306, 11, 51704], "temperature": 0.0, "avg_logprob": -0.12852723121643067, "compression_ratio": 1.7235023041474655, "no_speech_prob": 0.001707271789200604}, {"id": 1056, "seek": 618024, "start": 6180.24, "end": 6184.5599999999995, "text": " where are the ones one cares about, so to speak. And you can kind of, well, you can kind of see,", "tokens": [50364, 689, 366, 264, 2306, 472, 12310, 466, 11, 370, 281, 1710, 13, 400, 291, 393, 733, 295, 11, 731, 11, 291, 393, 733, 295, 536, 11, 50580], "temperature": 0.0, "avg_logprob": -0.11012121726726663, "compression_ratio": 1.7115384615384615, "no_speech_prob": 0.005027971230447292}, {"id": 1057, "seek": 618024, "start": 6184.5599999999995, "end": 6188.5599999999995, "text": " this is kind of how the different theorems and different areas of mathematics kind of", "tokens": [50580, 341, 307, 733, 295, 577, 264, 819, 10299, 2592, 293, 819, 3179, 295, 18666, 733, 295, 50780], "temperature": 0.0, "avg_logprob": -0.11012121726726663, "compression_ratio": 1.7115384615384615, "no_speech_prob": 0.005027971230447292}, {"id": 1058, "seek": 618024, "start": 6189.36, "end": 6192.5599999999995, "text": " get related to each other. But I think it's a really interesting question. What,", "tokens": [50820, 483, 4077, 281, 1184, 661, 13, 583, 286, 519, 309, 311, 257, 534, 1880, 1168, 13, 708, 11, 50980], "temperature": 0.0, "avg_logprob": -0.11012121726726663, "compression_ratio": 1.7115384615384615, "no_speech_prob": 0.005027971230447292}, {"id": 1059, "seek": 618024, "start": 6193.84, "end": 6200.5599999999995, "text": " you know, and you mentioned that, I mean, I suspect LLM is a really good at picking up on", "tokens": [51044, 291, 458, 11, 293, 291, 2835, 300, 11, 286, 914, 11, 286, 9091, 441, 43, 44, 307, 257, 534, 665, 412, 8867, 493, 322, 51380], "temperature": 0.0, "avg_logprob": -0.11012121726726663, "compression_ratio": 1.7115384615384615, "no_speech_prob": 0.005027971230447292}, {"id": 1060, "seek": 618024, "start": 6200.5599999999995, "end": 6206.96, "text": " cues from humans. And so I'm sure there's ways that people will write their master theorem.", "tokens": [51380, 32192, 490, 6255, 13, 400, 370, 286, 478, 988, 456, 311, 2098, 300, 561, 486, 2464, 641, 4505, 20904, 13, 51700], "temperature": 0.0, "avg_logprob": -0.11012121726726663, "compression_ratio": 1.7115384615384615, "no_speech_prob": 0.005027971230447292}, {"id": 1061, "seek": 620696, "start": 6206.96, "end": 6211.2, "text": " You know, they'll make, there'll be more trumpets blaring when they present the master theorem in", "tokens": [50364, 509, 458, 11, 436, 603, 652, 11, 456, 603, 312, 544, 21779, 1385, 888, 1921, 562, 436, 1974, 264, 4505, 20904, 294, 50576], "temperature": 0.0, "avg_logprob": -0.10820234065153161, "compression_ratio": 1.6475770925110131, "no_speech_prob": 0.010138148441910744}, {"id": 1062, "seek": 620696, "start": 6211.2, "end": 6216.32, "text": " their paper than when they present a little lemma. But here's a good question. Here's a question.", "tokens": [50576, 641, 3035, 813, 562, 436, 1974, 257, 707, 7495, 1696, 13, 583, 510, 311, 257, 665, 1168, 13, 1692, 311, 257, 1168, 13, 50832], "temperature": 0.0, "avg_logprob": -0.10820234065153161, "compression_ratio": 1.6475770925110131, "no_speech_prob": 0.010138148441910744}, {"id": 1063, "seek": 620696, "start": 6216.32, "end": 6223.68, "text": " If you try and make this, this is an easy thing to test. Okay. Can an LLM classify,", "tokens": [50832, 759, 291, 853, 293, 652, 341, 11, 341, 307, 364, 1858, 551, 281, 1500, 13, 1033, 13, 1664, 364, 441, 43, 44, 33872, 11, 51200], "temperature": 0.0, "avg_logprob": -0.10820234065153161, "compression_ratio": 1.6475770925110131, "no_speech_prob": 0.010138148441910744}, {"id": 1064, "seek": 620696, "start": 6223.68, "end": 6229.12, "text": " given a statement, can it decide whether the, whether the author of the paper will have called", "tokens": [51200, 2212, 257, 5629, 11, 393, 309, 4536, 1968, 264, 11, 1968, 264, 3793, 295, 264, 3035, 486, 362, 1219, 51472], "temperature": 0.0, "avg_logprob": -0.10820234065153161, "compression_ratio": 1.6475770925110131, "no_speech_prob": 0.010138148441910744}, {"id": 1065, "seek": 622912, "start": 6229.2, "end": 6237.5199999999995, "text": " it a theorem or a lemma? Well, I would guess yes, because there are different subtle cues,", "tokens": [50368, 309, 257, 20904, 420, 257, 7495, 1696, 30, 1042, 11, 286, 576, 2041, 2086, 11, 570, 456, 366, 819, 13743, 32192, 11, 50784], "temperature": 0.0, "avg_logprob": -0.12967342915742294, "compression_ratio": 1.5601659751037344, "no_speech_prob": 0.009122221730649471}, {"id": 1066, "seek": 622912, "start": 6237.5199999999995, "end": 6245.68, "text": " but I didn't try. What I tried, what I did try is I gave LLM some inequalities, like very simple", "tokens": [50784, 457, 286, 994, 380, 853, 13, 708, 286, 3031, 11, 437, 286, 630, 853, 307, 286, 2729, 441, 43, 44, 512, 41874, 11, 411, 588, 2199, 51192], "temperature": 0.0, "avg_logprob": -0.12967342915742294, "compression_ratio": 1.5601659751037344, "no_speech_prob": 0.009122221730649471}, {"id": 1067, "seek": 622912, "start": 6245.68, "end": 6250.88, "text": " elementary inequalities. Some were written by humans from the problem sets, from MO problem", "tokens": [51192, 16429, 41874, 13, 2188, 645, 3720, 538, 6255, 490, 264, 1154, 6352, 11, 490, 19290, 1154, 51452], "temperature": 0.0, "avg_logprob": -0.12967342915742294, "compression_ratio": 1.5601659751037344, "no_speech_prob": 0.009122221730649471}, {"id": 1068, "seek": 622912, "start": 6250.88, "end": 6256.16, "text": " sets, for example. And others are just generated randomly by machines, which typically look very", "tokens": [51452, 6352, 11, 337, 1365, 13, 400, 2357, 366, 445, 10833, 16979, 538, 8379, 11, 597, 5850, 574, 588, 51716], "temperature": 0.0, "avg_logprob": -0.12967342915742294, "compression_ratio": 1.5601659751037344, "no_speech_prob": 0.009122221730649471}, {"id": 1069, "seek": 625616, "start": 6256.16, "end": 6263.12, "text": " messy. And LLMs can do a reasonably good job at that task. Although I would say that task may not", "tokens": [50364, 16191, 13, 400, 441, 43, 26386, 393, 360, 257, 23551, 665, 1691, 412, 300, 5633, 13, 5780, 286, 576, 584, 300, 5633, 815, 406, 50712], "temperature": 0.0, "avg_logprob": -0.11230278830242972, "compression_ratio": 1.6798561151079137, "no_speech_prob": 0.0033203959465026855}, {"id": 1070, "seek": 625616, "start": 6263.12, "end": 6267.5199999999995, "text": " be very difficult. Well, okay, so we've tried to do things like this, because we've been interested", "tokens": [50712, 312, 588, 2252, 13, 1042, 11, 1392, 11, 370, 321, 600, 3031, 281, 360, 721, 411, 341, 11, 570, 321, 600, 668, 3102, 50932], "temperature": 0.0, "avg_logprob": -0.11230278830242972, "compression_ratio": 1.6798561151079137, "no_speech_prob": 0.0033203959465026855}, {"id": 1071, "seek": 625616, "start": 6267.5199999999995, "end": 6272.32, "text": " in automated testing of Mathematica and Moulton language. So we're interested in generating", "tokens": [50932, 294, 18473, 4997, 295, 15776, 8615, 2262, 293, 376, 3298, 1756, 2856, 13, 407, 321, 434, 3102, 294, 17746, 51172], "temperature": 0.0, "avg_logprob": -0.11230278830242972, "compression_ratio": 1.6798561151079137, "no_speech_prob": 0.0033203959465026855}, {"id": 1072, "seek": 625616, "start": 6272.32, "end": 6278.24, "text": " tests that are plausible input, so to speak. So we've indeed tried doing things like that.", "tokens": [51172, 6921, 300, 366, 39925, 4846, 11, 370, 281, 1710, 13, 407, 321, 600, 6451, 3031, 884, 721, 411, 300, 13, 51468], "temperature": 0.0, "avg_logprob": -0.11230278830242972, "compression_ratio": 1.6798561151079137, "no_speech_prob": 0.0033203959465026855}, {"id": 1073, "seek": 625616, "start": 6279.2, "end": 6283.68, "text": " Not been particularly successful. I mean, in other words, you can, you can generate an", "tokens": [51516, 1726, 668, 4098, 4406, 13, 286, 914, 11, 294, 661, 2283, 11, 291, 393, 11, 291, 393, 8460, 364, 51740], "temperature": 0.0, "avg_logprob": -0.11230278830242972, "compression_ratio": 1.6798561151079137, "no_speech_prob": 0.0033203959465026855}, {"id": 1074, "seek": 628368, "start": 6283.68, "end": 6288.8, "text": " expression at random just by some Markov process, for example, and you can generate an expression", "tokens": [50364, 6114, 412, 4974, 445, 538, 512, 3934, 5179, 1399, 11, 337, 1365, 11, 293, 291, 393, 8460, 364, 6114, 50620], "temperature": 0.0, "avg_logprob": -0.08627924711807915, "compression_ratio": 1.7773722627737227, "no_speech_prob": 0.008910556323826313}, {"id": 1075, "seek": 628368, "start": 6288.8, "end": 6295.52, "text": " by using some LLM like device. And you can ask the question, you know, given, given that you've seen,", "tokens": [50620, 538, 1228, 512, 441, 43, 44, 411, 4302, 13, 400, 291, 393, 1029, 264, 1168, 11, 291, 458, 11, 2212, 11, 2212, 300, 291, 600, 1612, 11, 50956], "temperature": 0.0, "avg_logprob": -0.08627924711807915, "compression_ratio": 1.7773722627737227, "no_speech_prob": 0.008910556323826313}, {"id": 1076, "seek": 628368, "start": 6296.240000000001, "end": 6301.92, "text": " I don't know, we've got billions of sort of human related Wolfram language expressions.", "tokens": [50992, 286, 500, 380, 458, 11, 321, 600, 658, 17375, 295, 1333, 295, 1952, 4077, 16634, 2356, 2856, 15277, 13, 51276], "temperature": 0.0, "avg_logprob": -0.08627924711807915, "compression_ratio": 1.7773722627737227, "no_speech_prob": 0.008910556323826313}, {"id": 1077, "seek": 628368, "start": 6301.92, "end": 6307.52, "text": " And then the question is, can we generate others that are like those? That's one question. Another", "tokens": [51276, 400, 550, 264, 1168, 307, 11, 393, 321, 8460, 2357, 300, 366, 411, 729, 30, 663, 311, 472, 1168, 13, 3996, 51556], "temperature": 0.0, "avg_logprob": -0.08627924711807915, "compression_ratio": 1.7773722627737227, "no_speech_prob": 0.008910556323826313}, {"id": 1078, "seek": 628368, "start": 6307.52, "end": 6313.12, "text": " question of great practical interest for us is, can we guess whether something that somebody entered", "tokens": [51556, 1168, 295, 869, 8496, 1179, 337, 505, 307, 11, 393, 321, 2041, 1968, 746, 300, 2618, 9065, 51836], "temperature": 0.0, "avg_logprob": -0.08627924711807915, "compression_ratio": 1.7773722627737227, "no_speech_prob": 0.008910556323826313}, {"id": 1079, "seek": 631312, "start": 6313.2, "end": 6319.44, "text": " is likely to be what they meant? Or is it something, in other words, it's like asking the question,", "tokens": [50368, 307, 3700, 281, 312, 437, 436, 4140, 30, 1610, 307, 309, 746, 11, 294, 661, 2283, 11, 309, 311, 411, 3365, 264, 1168, 11, 50680], "temperature": 0.0, "avg_logprob": -0.13667457753961737, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.0018424863228574395}, {"id": 1080, "seek": 631312, "start": 6319.44, "end": 6323.92, "text": " is this a, a plausible sentence, or is this a sentence nobody would ever write,", "tokens": [50680, 307, 341, 257, 11, 257, 39925, 8174, 11, 420, 307, 341, 257, 8174, 5079, 576, 1562, 2464, 11, 50904], "temperature": 0.0, "avg_logprob": -0.13667457753961737, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.0018424863228574395}, {"id": 1081, "seek": 631312, "start": 6324.64, "end": 6328.32, "text": " which is something which LLMs in a sense implicitly are capable of doing?", "tokens": [50940, 597, 307, 746, 597, 441, 43, 26386, 294, 257, 2020, 26947, 356, 366, 8189, 295, 884, 30, 51124], "temperature": 0.0, "avg_logprob": -0.13667457753961737, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.0018424863228574395}, {"id": 1082, "seek": 631312, "start": 6329.36, "end": 6334.72, "text": " I must make an executive decision now. And it's a calculated risk. We may lose", "tokens": [51176, 286, 1633, 652, 364, 10140, 3537, 586, 13, 400, 309, 311, 257, 15598, 3148, 13, 492, 815, 3624, 51444], "temperature": 0.0, "avg_logprob": -0.13667457753961737, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.0018424863228574395}, {"id": 1083, "seek": 631312, "start": 6335.44, "end": 6340.88, "text": " Steven Wolfram, if it turns out he has something else of higher priority. We may use lose other", "tokens": [51480, 12754, 16634, 2356, 11, 498, 309, 4523, 484, 415, 575, 746, 1646, 295, 2946, 9365, 13, 492, 815, 764, 3624, 661, 51752], "temperature": 0.0, "avg_logprob": -0.13667457753961737, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.0018424863228574395}, {"id": 1084, "seek": 634088, "start": 6340.88, "end": 6347.04, "text": " people for the panel, in which case I will have called closure on this needlessly. And there may", "tokens": [50364, 561, 337, 264, 4831, 11, 294, 597, 1389, 286, 486, 362, 1219, 24653, 322, 341, 643, 12048, 13, 400, 456, 815, 50672], "temperature": 0.0, "avg_logprob": -0.13723138508043792, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.008688332512974739}, {"id": 1085, "seek": 634088, "start": 6347.04, "end": 6351.6, "text": " be nothing in the panel, but I have to call closure because we have to stop this session", "tokens": [50672, 312, 1825, 294, 264, 4831, 11, 457, 286, 362, 281, 818, 24653, 570, 321, 362, 281, 1590, 341, 5481, 50900], "temperature": 0.0, "avg_logprob": -0.13723138508043792, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.008688332512974739}, {"id": 1086, "seek": 634088, "start": 6351.6, "end": 6357.6, "text": " and then restart for the next session. I want to thank you very much, Steven, for your, for", "tokens": [50900, 293, 550, 21022, 337, 264, 958, 5481, 13, 286, 528, 281, 1309, 291, 588, 709, 11, 12754, 11, 337, 428, 11, 337, 51200], "temperature": 0.0, "avg_logprob": -0.13723138508043792, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.008688332512974739}, {"id": 1087, "seek": 634088, "start": 6357.6, "end": 6363.4400000000005, "text": " your talk. And I hope we'll be seeing you again in 10 minutes, but we'll see. Okay.", "tokens": [51200, 428, 751, 13, 400, 286, 1454, 321, 603, 312, 2577, 291, 797, 294, 1266, 2077, 11, 457, 321, 603, 536, 13, 1033, 13, 51492], "temperature": 0.0, "avg_logprob": -0.13723138508043792, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.008688332512974739}, {"id": 1088, "seek": 636344, "start": 6364.0, "end": 6366.719999999999, "text": " Are we using the same link for the panel?", "tokens": [50392, 2014, 321, 1228, 264, 912, 2113, 337, 264, 4831, 30, 50528], "temperature": 0.0, "avg_logprob": -0.27049118280410767, "compression_ratio": 1.6384615384615384, "no_speech_prob": 0.06624659895896912}, {"id": 1089, "seek": 636344, "start": 6368.719999999999, "end": 6371.04, "text": " Are we using the same Zoom link for the panel?", "tokens": [50628, 2014, 321, 1228, 264, 912, 13453, 2113, 337, 264, 4831, 30, 50744], "temperature": 0.0, "avg_logprob": -0.27049118280410767, "compression_ratio": 1.6384615384615384, "no_speech_prob": 0.06624659895896912}, {"id": 1090, "seek": 636344, "start": 6371.04, "end": 6375.2, "text": " I will redo it on, I'll restart it for the panel and I hope you'll be there.", "tokens": [50744, 286, 486, 29956, 309, 322, 11, 286, 603, 21022, 309, 337, 264, 4831, 293, 286, 1454, 291, 603, 312, 456, 13, 50952], "temperature": 0.0, "avg_logprob": -0.27049118280410767, "compression_ratio": 1.6384615384615384, "no_speech_prob": 0.06624659895896912}, {"id": 1091, "seek": 636344, "start": 6376.24, "end": 6379.44, "text": " Steven's there too, but I have to break it now.", "tokens": [51004, 12754, 311, 456, 886, 11, 457, 286, 362, 281, 1821, 309, 586, 13, 51164], "temperature": 0.0, "avg_logprob": -0.27049118280410767, "compression_ratio": 1.6384615384615384, "no_speech_prob": 0.06624659895896912}], "language": "en"}