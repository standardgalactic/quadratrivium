1
00:00:00,000 --> 00:00:03,600
I forget now whether you do have a discussant or not. Yeah, Virginia.

2
00:00:04,160 --> 00:00:06,160
Oh, it's Virginia's gonna. Okay, great. So

3
00:00:07,040 --> 00:00:10,160
She's here. Yeah, she's there. Hi, Jenny. I've been Virginia. I mean

4
00:00:24,720 --> 00:00:25,920
Have you

5
00:00:26,000 --> 00:00:33,040
Josh, you haven't seen any sessions at all yet, right? Um, I've seen I haven't been able to join live, but I watched video from the first two days

6
00:00:33,680 --> 00:00:35,680
Did you watch belkin?

7
00:00:35,680 --> 00:00:41,520
I did. Yeah. Okay, great. Okay. I was even gonna refer to it briefly. Yeah. Okay. That's why I don't want this to explain. Yeah

8
00:00:41,840 --> 00:00:43,840
Um

9
00:00:54,720 --> 00:00:56,960
I'm I'm also trying to see if there's some time

10
00:00:57,200 --> 00:01:02,240
I know you said that there might I might be able to join I can't join the discuss or there is no discussion today, but um

11
00:01:02,800 --> 00:01:07,120
I'm trying to see if there's any one of the other panels that my schedule would be great. I can add

12
00:01:08,240 --> 00:01:10,240
Yeah, I'll I'll uh

13
00:01:10,480 --> 00:01:12,960
I'll I'll be in touch about that. Okay. Let me just

14
00:01:14,960 --> 00:01:16,960
Solid duty here

15
00:01:33,200 --> 00:01:37,680
Welcome everybody to the afternoon session everybody out in in the

16
00:01:38,640 --> 00:01:42,640
Distributed land and everybody here. I'd like to um

17
00:01:43,200 --> 00:01:50,560
Introduce Josh Stendenbaum who's professor of computational cognitive science in the department of brain and cognitive science

18
00:01:51,280 --> 00:01:52,880
at mit

19
00:01:52,880 --> 00:02:00,720
He's a principal investigator at the computer science and artificial intelligence lab called sea sail. Is that former sail?

20
00:02:02,000 --> 00:02:04,880
No, uh sail is a stem from sail

21
00:02:05,840 --> 00:02:10,800
But it's formerly the AI lab and the lcs laboratory for computer science

22
00:02:11,200 --> 00:02:16,640
Great. Yeah, and a thrust leader in the center for brains minds and machines cbn

23
00:02:16,640 --> 00:02:24,240
And his papers are on perception learning common sensory reasoning in humans and machines with the twin goals of better

24
00:02:24,640 --> 00:02:31,360
Understanding human intelligence in computational terms and building more human like intelligence in machines

25
00:02:32,000 --> 00:02:34,640
In other words, he's cut out perfectly for this

26
00:02:36,080 --> 00:02:39,200
Summer school. I hand it over now to Josh Stendenbaum

27
00:02:40,160 --> 00:02:43,520
Okay, great. Thank you so much, Steven for organizing for inviting me

28
00:02:45,120 --> 00:02:51,600
You know, I I got to watch much of the first two days and it was really interesting to see a back and forth between people who are

29
00:02:52,160 --> 00:02:54,640
Extremely impressed with large language models

30
00:02:55,120 --> 00:03:00,240
Both on their language abilities and maybe some of their general thinking abilities and other people who are much more skeptical

31
00:03:00,880 --> 00:03:04,080
That they have really anything to do with intelligence at least of the human form

32
00:03:04,720 --> 00:03:09,200
The work I'm going to talk about here is a is an interesting I think mix of those two perspectives

33
00:03:10,720 --> 00:03:12,240
And I hope that will

34
00:03:12,240 --> 00:03:14,240
Be useful stimulating

35
00:03:14,560 --> 00:03:18,800
Engage in some interesting discussion both now and going forward over the next two weeks

36
00:03:20,000 --> 00:03:24,000
The the heart of the talk, which will really be more like the second half is based on this paper

37
00:03:24,000 --> 00:03:27,200
Which you can find on archive from word models to world models

38
00:03:27,600 --> 00:03:31,200
Understanding natural language by translating into a probabilistic language of thought

39
00:03:32,080 --> 00:03:40,080
But I'm going to spend the first half setting some context on just how we think about thinking and then it's built on that for the relationship between language and thought

40
00:03:41,280 --> 00:03:45,120
This is you know, I would say this is both an AI talk and a cognitive science talk

41
00:03:45,200 --> 00:03:48,480
Although most fundamentally to me. I'm really interested in the computational structure

42
00:03:49,280 --> 00:03:51,200
of the human mind

43
00:03:51,200 --> 00:03:52,640
So just in that

44
00:03:52,640 --> 00:03:54,640
hear it in that lens

45
00:03:54,640 --> 00:03:57,280
But I think also just sort of very generally

46
00:03:58,000 --> 00:04:01,040
You know, nobody can fail to be surprised and impressed

47
00:04:01,600 --> 00:04:04,640
At what has happened with the most recent machine learning models

48
00:04:05,680 --> 00:04:10,000
And at the same time they're they're very puzzling and confounding in certain ways

49
00:04:10,000 --> 00:04:12,560
So I hope to try to be able to resolve some of that or at least

50
00:04:12,960 --> 00:04:14,960
Point the way towards some of that

51
00:04:15,360 --> 00:04:18,320
This paper the work i'm going to talk about is joint with a number of people

52
00:04:18,640 --> 00:04:24,480
I want to single out two people Lionel long and Gabe grand who are the joint first authors of that paper

53
00:04:24,640 --> 00:04:26,640
I'm putting Lionel a little bit bigger

54
00:04:27,200 --> 00:04:29,840
In part because a lot more of my slides are taken from lines

55
00:04:30,240 --> 00:04:35,920
So I would say all of the credit for the good stuff both on the research and in the slides to Lionel and Gabe

56
00:04:37,360 --> 00:04:42,320
And you know anything that doesn't quite work or make sense is probably just me garbling things

57
00:04:42,960 --> 00:04:44,000
Okay

58
00:04:44,000 --> 00:04:47,120
But a lot of other people contribute to the work that i'm going to talk about here

59
00:04:48,000 --> 00:04:52,400
So again, you know, we can't who cannot be impressed with the advances of AI

60
00:04:52,640 --> 00:04:56,320
Whether it's in perception and and you know robots deployed in the real world

61
00:04:56,320 --> 00:05:02,640
Like the self-driving car systems of tesla and wemo or most recently conversational AI systems like chat gpt

62
00:05:03,840 --> 00:05:06,240
Clearly there's something really interesting and important happening

63
00:05:06,400 --> 00:05:08,560
But at the same time something quite puzzling now

64
00:05:08,560 --> 00:05:11,120
There's a number of puzzles that people here have talked about just one

65
00:05:11,120 --> 00:05:13,920
I'll point to is what you might call the puzzle of confabulation

66
00:05:15,280 --> 00:05:17,520
Which points to what ways I think that the kinds of

67
00:05:18,480 --> 00:05:25,280
What intelligence or unquote intelligence that we see in these systems is different in nature and origins from that in our own minds

68
00:05:25,760 --> 00:05:29,520
So think about for example in the context of driving. This is a video from tesla

69
00:05:30,160 --> 00:05:36,800
A self-driving system where it's it's faced with something a little bit out of its training set a horse drawn carriage and and

70
00:05:37,840 --> 00:05:42,320
Buggy and it interprets it as a truck first facing one way the other then it's

71
00:05:43,440 --> 00:05:45,680
SUV then it's a truck with the person behind it

72
00:05:46,640 --> 00:05:48,640
Now the person's gone away

73
00:05:48,720 --> 00:05:50,720
Shortly the person will come back

74
00:05:51,520 --> 00:05:53,680
You know, they can't figure out which way the truck is facing

75
00:05:54,320 --> 00:05:59,280
If a person were having this experience on the road, you would think they should stop driving and get off the road as soon as possible

76
00:05:59,920 --> 00:06:01,920
Right. There's something fundamentally

77
00:06:02,560 --> 00:06:04,560
Detached about their mind from reality here

78
00:06:05,600 --> 00:06:11,200
And when this happens in systems where lives are at stake, it can be deadly. So this is a serious worry

79
00:06:11,760 --> 00:06:13,040
technologically and

80
00:06:13,120 --> 00:06:19,760
Sightly now when it comes to computer vision and tesla self-driving a lot of progress has been made

81
00:06:21,680 --> 00:06:24,880
What I showed you what i'm showing you here is from a few years ago

82
00:06:25,680 --> 00:06:29,360
Um, but you still see similar kinds of things and also on the language side

83
00:06:29,360 --> 00:06:34,560
I'm just going to show one example of analogous kinds of things that really make you scratch your head

84
00:06:34,960 --> 00:06:40,960
On the you you could find these in language models, but i'm going to talk about multimodal language and vision models because this is where

85
00:06:41,280 --> 00:06:44,400
You know, there's a lot of really interesting stuff going on

86
00:06:45,200 --> 00:06:48,080
Where machine learning and especially like grounded language

87
00:06:48,480 --> 00:06:51,360
Potentially might meet up with what goes on in cognition

88
00:06:51,920 --> 00:06:55,200
um, so this is a i'm just going to show you a couple of little demos from

89
00:06:56,800 --> 00:07:02,880
Google's large language model multimodal language model. Um, gemini both earlier versions and then the most recent version

90
00:07:03,520 --> 00:07:09,360
This is the figure i'm showing here, which i'm giving to gemini and I should say you can see similar things with chat gpt

91
00:07:09,360 --> 00:07:12,480
But it's a little bit funnier with the google ones. I'm going to focus on that

92
00:07:12,960 --> 00:07:17,680
Um, this is an image from my colleague vikash mansinga whose work will also figure in the talk

93
00:07:18,240 --> 00:07:21,120
Um, vikash is one of the world's experts in probabilistic programming

94
00:07:21,280 --> 00:07:25,360
He and I are very interested in how our minds and our machines could model uncertainty

95
00:07:25,920 --> 00:07:28,880
So here's an image that that I think all of us when we look at this

96
00:07:28,960 --> 00:07:32,480
It's it's it's pretty clear that though. It's a little hard to read kind of messy

97
00:07:32,640 --> 00:07:35,920
It's really clear that there's a word here and that the word is uncertainty

98
00:07:36,480 --> 00:07:40,240
Um, but what is uh an lm a multimodal lmc from this?

99
00:07:40,640 --> 00:07:45,040
Well, it might see this the image you sent me shows a black and white image of a text box

100
00:07:45,120 --> 00:07:51,520
The text book has a dark gray border rounded corners. Good inside is the word dream written in a bold sans serif font

101
00:07:52,240 --> 00:07:55,440
Is there anything else I can help you with? I ask how many letters are shown in the text box?

102
00:07:55,680 --> 00:07:59,680
There are five letters shown in the text box the letters are d r e a and m

103
00:08:01,120 --> 00:08:03,120
What is going on with this?

104
00:08:03,120 --> 00:08:05,840
Now we could say all right. Well, at least it's pretty good

105
00:08:05,920 --> 00:08:10,560
I mean at least it's recognizing that there's text and it's you know, it could be worse

106
00:08:11,200 --> 00:08:16,400
But it seems to be strangely self-confident strangely confident about an incorrect answer

107
00:08:16,640 --> 00:08:20,720
But you could say at least it's not like seeing a person or person walking down the street

108
00:08:21,760 --> 00:08:28,400
But consider this small change to the input image now our lm says this shows a black and white pixelated image of a person

109
00:08:28,640 --> 00:08:33,120
The person is facing forward and appears to be standing still. Here are some details about the person

110
00:08:33,440 --> 00:08:37,520
They have short hair. They're wearing a shirt with a collar. Their legs are slightly wider than their torso

111
00:08:38,080 --> 00:08:39,360
Since the image is pixelated

112
00:08:39,360 --> 00:08:43,200
It's difficult to discern other details such as facial features or specific articles of clothing

113
00:08:43,600 --> 00:08:47,120
Now again, the thing that I think should make us puzzle here is

114
00:08:47,920 --> 00:08:53,360
Is not just that the system gets this wrong, but that it seems to confidently assert things that are so

115
00:08:54,400 --> 00:08:58,000
detached from our sense of of the reality of what we're seeing

116
00:08:58,640 --> 00:09:01,440
And where just a small change can make such a huge difference

117
00:09:02,080 --> 00:09:08,720
Just to update things this was this was from the first version of google's gemini the most recent version of gemini 1.5 pro

118
00:09:09,120 --> 00:09:11,120
Has gotten better in some sense

119
00:09:11,520 --> 00:09:14,880
Now given uncertainty. It doesn't say dream. It says optical illusion

120
00:09:15,840 --> 00:09:20,960
But it's still wrong and it's still over confident. Um, it rates itself as fairly confident

121
00:09:21,680 --> 00:09:28,160
That it says optical illusion. It's interesting. It makes you think is there some semantic association between dreams optical illusions and

122
00:09:28,400 --> 00:09:32,480
This particular way of presenting the notion of uncertainty. I don't know. Um,

123
00:09:33,120 --> 00:09:38,080
The the very most recent thing this was well, it's not that recent but the last time I gave this talk

124
00:09:38,800 --> 00:09:45,200
Um, I tried this on the the newest updated version of gemini 1.5 pro just to see if perhaps it had been improved

125
00:09:45,600 --> 00:09:47,600
And now it gave a very different answer

126
00:09:47,600 --> 00:09:50,160
It didn't say it didn't read it as optical illusion. It gave this

127
00:09:51,040 --> 00:09:56,400
Extended digit number six one two nine eight one five three two four two and I asked it to judge its confidence and it said

128
00:09:56,560 --> 00:09:58,560
Well 70 to 80 percent

129
00:09:58,640 --> 00:10:03,840
Again, what what happened? Just to check again consistency and coherence of this

130
00:10:04,320 --> 00:10:08,720
Weird interpretation. I asked it just 10 seconds later just in an out of context again

131
00:10:08,800 --> 00:10:13,360
Just gave it the same thing and the next 10 seconds later. It said geocache

132
00:10:14,000 --> 00:10:18,080
Um, again now 75 confident and then one more time

133
00:10:19,200 --> 00:10:23,280
Um, a distorted text appears to read northeast blackout

134
00:10:24,240 --> 00:10:25,600
However, it's difficult to be certain

135
00:10:25,600 --> 00:10:31,040
But it's moderately confident that the text reads northeast blackout because the first and last words are relatively clear

136
00:10:31,040 --> 00:10:34,880
While the middle word is slightly less discernible, but still suggestive of blackout

137
00:10:35,600 --> 00:10:38,080
Okay, so what's going on here?

138
00:10:39,120 --> 00:10:40,160
That's one of the mysteries now

139
00:10:40,160 --> 00:10:44,640
I'm not I'm not I'm going to gesture at what I think some of the mysteries or some of the answers might be

140
00:10:44,960 --> 00:10:50,400
But mostly to point the way towards the difference with human intelligence and what we're trying to understand in in artwork

141
00:10:51,200 --> 00:10:56,240
Okay, I think fundamentally what's going on is whether you're building a computer vision system or

142
00:10:56,720 --> 00:10:59,760
An llm or a multimodal language text system

143
00:10:59,760 --> 00:11:04,640
You're building a system that takes the inputs and outputs that our brains do

144
00:11:04,960 --> 00:11:11,360
perception of the eternal world sense data of some form and it produces actions or something like actions that can be

145
00:11:11,920 --> 00:11:17,280
Grounded back in the external world, but the inside of the system doesn't have any notion of a world

146
00:11:17,360 --> 00:11:18,800
It's a function approximator

147
00:11:18,800 --> 00:11:22,800
It's learning to approximate the input output functions that our minds produce

148
00:11:23,200 --> 00:11:29,360
And it's learning to do that from various data sources including objective data sources as well as human reinforcement

149
00:11:30,160 --> 00:11:34,080
Now how why does this possibly work? Well, again, there there may be some

150
00:11:34,560 --> 00:11:40,000
laws of physics things that are like physics like for example the famous scaling laws of neural language models

151
00:11:40,400 --> 00:11:42,400
Where you can show in some form

152
00:11:42,880 --> 00:11:45,360
Especially when you're trying to predict like you know

153
00:11:45,760 --> 00:11:51,920
And this is very much following some of the information theoretic ideas that richard has been talking about and others in the group here

154
00:11:52,960 --> 00:11:55,920
If i'm just trying to predict the next token from the previous ones

155
00:11:56,720 --> 00:12:02,320
There are certain fundamental power laws of language and distributions in language that these systems seem to incorporate

156
00:12:02,800 --> 00:12:06,720
And build on such that if you increase by an order of magnitude the amount of compute

157
00:12:07,040 --> 00:12:11,520
You can in a predictable way lower the test loss in predicting the next token. That's on the left

158
00:12:12,320 --> 00:12:14,320
In gpt4's technical report

159
00:12:14,560 --> 00:12:19,120
They suggested that you could see similar kinds of scaling laws for problem solving not just text prediction

160
00:12:19,120 --> 00:12:20,800
Although it's a lot if you're there

161
00:12:20,800 --> 00:12:25,840
And I think fundamentally the problem is the power laws are beautiful laws and they're predictable in a certain sense

162
00:12:26,160 --> 00:12:28,960
The thing about a power law is while it approaches asymptote

163
00:12:29,360 --> 00:12:32,320
Or it rather approaches zero error in its asymptote

164
00:12:32,800 --> 00:12:37,600
It in some important sense never gets there compared to like an exponential decay

165
00:12:37,840 --> 00:12:42,400
Where there's a predictable timescale on which it will get to zero a power law keeps slowing down

166
00:12:42,800 --> 00:12:46,960
and if you have any uncertainty in the power laws coefficient or

167
00:12:48,080 --> 00:12:53,760
You know in its applicability to the not just to predicting the data stream, but actually solving a problem

168
00:12:54,480 --> 00:13:00,400
Then it's it's basically impossible to know how much data and compute you're going to need to actually get to the asymptote

169
00:13:00,400 --> 00:13:03,280
Which we want to call, you know full adult human intelligence

170
00:13:03,680 --> 00:13:05,680
In contrast

171
00:13:06,320 --> 00:13:12,400
Humans don't seem to be built this way. We our minds seem to be built as world modelers from the start

172
00:13:12,560 --> 00:13:14,720
And I'll say more about this, but from the very beginning

173
00:13:15,440 --> 00:13:18,720
We have our minds and our brains and this is something that

174
00:13:19,360 --> 00:13:25,360
Inherits shared with other animals and inherits from our evolutionary legacy seem to be built to model the world

175
00:13:25,840 --> 00:13:28,800
And to deal with all kinds of incompleteness and uncertainty

176
00:13:29,520 --> 00:13:33,120
And that means both in how in the the structure of the world

177
00:13:33,840 --> 00:13:38,560
And our uncertainty over what's out there the current state as well as the the fundamental deeper laws

178
00:13:38,720 --> 00:13:41,760
The causal laws of physics or how agents plan and so on

179
00:13:42,000 --> 00:13:45,280
But we're built to engage these kinds of mental representations

180
00:13:45,840 --> 00:13:48,080
As i'll as i'll show in a little bit the way in our group

181
00:13:48,080 --> 00:13:53,760
We've modeled this for a long time is what we call the game engine in the head by analogy to say video game engines

182
00:13:54,080 --> 00:13:58,640
That our brains and minds seem to be built with these kinds of resources for world modeling

183
00:13:59,360 --> 00:14:03,760
You can see that this the the notion of world models that are used

184
00:14:04,160 --> 00:14:10,400
Even not just in human adults, but in young children to coherently understand the world and deal with situations that are

185
00:14:11,040 --> 00:14:13,040
Importantly out of distribution

186
00:14:13,040 --> 00:14:15,040
Right and that's really key

187
00:14:15,040 --> 00:14:20,720
The ways in which we perceive and learn about the world are not nearly as time to the distributions of our experience as

188
00:14:21,120 --> 00:14:23,120
machine learning or function approximation

189
00:14:23,760 --> 00:14:27,520
So here's some examples of different kind of self-driving system. These are

190
00:14:28,400 --> 00:14:31,040
four-year-olds for the most part three four five-year-olds

191
00:14:31,920 --> 00:14:36,240
In a genre of youtube videos, you can see one of them actually the video on the left

192
00:14:37,200 --> 00:14:40,800
It's it's sort of the the video genre on youtube is like, you know

193
00:14:40,880 --> 00:14:48,000
My four-year-old driving for the first time and parents have put their kids behind the wheels of golf carts or tractors or cars or trucks

194
00:14:48,960 --> 00:14:53,040
And just go at it and they video from the side or from next to them

195
00:14:53,680 --> 00:14:54,480
and

196
00:14:54,480 --> 00:15:00,160
You know, you could question the rationality of the adults who are putting their kids in the situation and then putting the videos online

197
00:15:00,480 --> 00:15:03,680
But there's a certain basic rationality that the four-year-old has

198
00:15:04,080 --> 00:15:10,640
Even though they haven't been in the situation before the systems that their mind has built through evolution and the last couple of years

199
00:15:10,640 --> 00:15:12,640
The first few years of their experience

200
00:15:12,880 --> 00:15:15,760
Allow them to handle this totally new kind of perceptual information

201
00:15:15,760 --> 00:15:19,280
The world's going by them at a high speed and they're in control in some way

202
00:15:19,840 --> 00:15:25,360
But they can generalize from their from from their experience because the the nature of their generalization

203
00:15:25,600 --> 00:15:29,520
Is their mind's models of the world and those transfer to these new situations?

204
00:15:29,520 --> 00:15:32,480
Of course, their new things they have to learn about exactly how the steering wheel works

205
00:15:32,560 --> 00:15:37,280
But they can learn those very quickly too because the learning is grounded in their world model

206
00:15:38,240 --> 00:15:42,560
So fundamentally, I think what we have is the is a contrast between the scaling thesis

207
00:15:43,040 --> 00:15:45,040
of deep learning and today's ai

208
00:15:45,360 --> 00:15:49,520
And what you could call the original scaling route that human intelligence follows what we might call

209
00:15:49,920 --> 00:15:53,040
Growing up as opposed to scaling up and I want to contrast

210
00:15:53,520 --> 00:15:56,800
Three points, which I think are really important for understanding

211
00:15:57,280 --> 00:16:03,200
In general our intelligence and the contrast between ai and machine learning and especially the role that language and language models play in

212
00:16:03,840 --> 00:16:09,200
So three points that are that are fundamental to the way today's ai is working based on deep learning one

213
00:16:09,840 --> 00:16:12,960
That intelligence is seen as the end result of learning

214
00:16:13,440 --> 00:16:17,760
We're learning starts with some simple very general and in some ways dumb mechanisms

215
00:16:18,240 --> 00:16:20,720
associative learning prediction, you know in some way

216
00:16:21,520 --> 00:16:27,600
But the idea is if you scale that up enough you have the surprising emergent phenomenon that now we start to call intelligence

217
00:16:28,160 --> 00:16:29,360
um

218
00:16:29,360 --> 00:16:34,080
The the problem is though, I mean either you call it a problem or you can call it just the way things work

219
00:16:34,160 --> 00:16:36,480
But there's some remarkable kind of generalization

220
00:16:36,480 --> 00:16:39,680
But it still is going to depend on similarity to training data

221
00:16:40,160 --> 00:16:46,640
And its ability to go beyond the training distribution is is weak and unpredictable compared to when you're in distribution

222
00:16:47,600 --> 00:16:51,920
Um, this is true whether you're talking about perception or other forms of higher level cognition

223
00:16:51,920 --> 00:16:58,080
But especially thinking things that we recognize as forms of reasoning planning problem solving and so on

224
00:16:58,720 --> 00:17:01,680
The the sort of thing that we've never seen in any computer vision system

225
00:17:01,680 --> 00:17:05,840
But we're starting to see some interesting kind of approximation to in language models

226
00:17:06,480 --> 00:17:10,320
Crucially that in machine learning thinking derives from language

227
00:17:10,640 --> 00:17:16,640
Because it's only the data of language not the data of pixels that conveys information really about human thoughts

228
00:17:16,720 --> 00:17:21,360
Which makes sense again humans have always used language as our main medium to express our thoughts to others

229
00:17:21,440 --> 00:17:24,640
Just like what we're doing right now in giving talks like this, right?

230
00:17:25,120 --> 00:17:25,680
um

231
00:17:25,680 --> 00:17:30,880
But crucially thinking requires language data and even carefully cleaned and curated language data

232
00:17:30,880 --> 00:17:34,400
Our colleagues who are at the big ai companies can tell us a lot more about this or rather

233
00:17:34,480 --> 00:17:38,000
Maybe they can't tell us about this but they can tell us that they can't tell us about

234
00:17:38,560 --> 00:17:44,320
And those of us who've worked with this know the importance of having the right kinds of language data as well as the right reinforcement data

235
00:17:45,120 --> 00:17:48,640
Okay, so contrast this with human minds and especially human children

236
00:17:49,680 --> 00:17:51,760
Our intelligence if for our intelligence

237
00:17:52,000 --> 00:17:55,920
It's not just the thing that is at the end result of all of our learning although we do

238
00:17:56,240 --> 00:18:00,560
You know babies are not in some important way not as intelligent as human adults we would like to say

239
00:18:01,120 --> 00:18:07,200
But human intelligence is is built in in some form from the start. It's the foundation of learning. It's not just the end state

240
00:18:07,600 --> 00:18:12,800
It's what's there and the the learning mechanisms that let you learn so much from so little

241
00:18:13,520 --> 00:18:17,040
Okay, and we saw some of that in virginia's talk and you know

242
00:18:17,040 --> 00:18:21,600
I think the the field of human cognitive development is a testament to this or what I was just showing from the four-year-olds

243
00:18:21,600 --> 00:18:24,400
Is another example but in other work that i'm not going to talk about here

244
00:18:24,400 --> 00:18:28,080
We've done a lot of work trying to model the core knowledge of even you know

245
00:18:28,080 --> 00:18:33,520
12-month-old or 10-month-old babies and there's important ways in which they have a common sense understanding of the world

246
00:18:34,000 --> 00:18:38,720
That is in in in significant ways. It's not all built in but in significant ways

247
00:18:38,720 --> 00:18:42,080
It seems to be present even even in two and three month olds. Okay

248
00:18:42,880 --> 00:18:43,600
um

249
00:18:43,600 --> 00:18:50,800
As a result of our built-in capacity for modeling the world our generalization is based on the fit of our mental models

250
00:18:50,800 --> 00:18:55,760
It's not about the training data. Okay data is important. We improve our models. We grow models

251
00:18:55,760 --> 00:19:02,080
We can make new models from data, but what accounts for generalization and what drives generalization is not similarities to the data

252
00:19:02,240 --> 00:19:08,240
But the fit of our world models and their ability to be flexible and themselves to be generalized

253
00:19:09,360 --> 00:19:15,360
And crucially thinking isn't the thing that comes at the end state as an emergent property of modeling language

254
00:19:15,360 --> 00:19:19,360
But rather thinking is there from the start and it's the basis for language

255
00:19:19,440 --> 00:19:25,440
It's the basis for why human children construct language so robustly and resiliently and again there

256
00:19:25,520 --> 00:19:27,200
I'm referring to

257
00:19:27,200 --> 00:19:33,360
By using that word resilience to some of the ideas that virginia talked about in her talk and susan golden meadow has talked about

258
00:19:33,680 --> 00:19:37,360
I urge everyone to watch her rumelhardt prize talk where she talks about this as well

259
00:19:37,920 --> 00:19:43,280
This idea that as we saw in susan's work and annie sangus's work with nicaraguan sign language

260
00:19:43,680 --> 00:19:46,240
That you know children who grow up who grow up without

261
00:19:47,120 --> 00:19:48,480
Any language input?

262
00:19:48,480 --> 00:19:51,680
It's not just about the poverty the stimulus in the traditional linguistic sense

263
00:19:51,680 --> 00:19:57,840
But you grow up with deaf without sign language input and you create in some form your own personal proto language

264
00:19:57,840 --> 00:20:01,040
Or at least a way of communicating that has hierarchical symbolic structure

265
00:20:01,280 --> 00:20:04,800
And then you bring a few such children together and within the span of a couple of generations

266
00:20:04,800 --> 00:20:10,000
They've created a whole new language from scratch. Okay, so it's very clear from data like that

267
00:20:10,800 --> 00:20:13,840
That humans are built to think and we're built with an

268
00:20:14,400 --> 00:20:18,320
A desire to understand and to under to be understood to express our thoughts

269
00:20:18,640 --> 00:20:22,320
In some form and share them with others in our as our social partners

270
00:20:22,960 --> 00:20:26,080
And that's you know, there's nothing I think more fundamental to

271
00:20:26,720 --> 00:20:29,600
To understand if you want to understand where our intelligence comes from

272
00:20:30,080 --> 00:20:36,960
So what we've been trying to do in our work is to try to capture the human growing up scaling root in computational terms

273
00:20:37,520 --> 00:20:40,480
And it starts with this idea of probabilistic inference and

274
00:20:41,040 --> 00:20:44,800
Expected value decision-making on top of world models, you know informally

275
00:20:44,960 --> 00:20:49,200
I would say and like many others that thinking not just in human brains

276
00:20:49,280 --> 00:20:54,560
But in brains and in many other animals is about making good guesses and bets not about the next

277
00:20:55,040 --> 00:20:59,760
Data thing you're going to see not about the next token or the next pixel or set of pixel values

278
00:20:59,920 --> 00:21:02,720
But about the world what's going to happen in the world and how

279
00:21:03,280 --> 00:21:08,400
It might depend on your actions or the actions of others or how you might be able to change the world to change those

280
00:21:08,960 --> 00:21:14,160
dependencies causal and counterfactual reasoning and then having some sense of what you'd like to see happen

281
00:21:14,560 --> 00:21:18,160
Or and what you'd not like to see happen or what could really cost you your life or

282
00:21:18,800 --> 00:21:24,080
Be hugely valuable and making good bets about how to act and what to think about next

283
00:21:24,960 --> 00:21:26,960
fundamentally this classic idea of

284
00:21:27,360 --> 00:21:29,360
effectively rationality that

285
00:21:30,000 --> 00:21:34,640
You know, you could say our minds and brains are the original sources of this idea through evolution

286
00:21:35,520 --> 00:21:38,640
Being constructed to do this kind of computation

287
00:21:39,280 --> 00:21:43,520
In our work and this is the stuff we've been doing with language models builds on this

288
00:21:43,840 --> 00:21:46,960
We've been using the idea of probabilistic programs, which is a

289
00:21:47,840 --> 00:21:52,080
family of mathematical languages and actual programming languages and platforms

290
00:21:52,560 --> 00:21:58,720
To embody this idea basically take this conceptual idea of rational world modeling inference of decision

291
00:21:59,120 --> 00:22:04,960
And turn it into practical engineering terms that can be models of human minds as well as more human like ai

292
00:22:05,520 --> 00:22:08,240
And I don't have time to give a whole introduction to probabilistic programs

293
00:22:08,800 --> 00:22:15,040
But you know, you can think of it as a kind of catch-all phrase or a complicated suitcase phrase just like neural network

294
00:22:16,080 --> 00:22:20,400
Packages a number of different things together, but probabilistic programs are formalisms

295
00:22:20,880 --> 00:22:26,400
For combining what I think are several of the best ideas about intelligence that have come up through through a number of

296
00:22:27,840 --> 00:22:32,480
You know decades in the field that includes neural networks modern probabilistic programming languages

297
00:22:32,800 --> 00:22:37,920
Like gen from vikash man singha's group who I mentioned before at mit or pyro

298
00:22:38,480 --> 00:22:42,240
That was developed by originally a group at uber ai that noah goodman

299
00:22:42,720 --> 00:22:45,920
Another collaborator of ours helped to start along with a number of others

300
00:22:47,120 --> 00:22:48,480
you know

301
00:22:48,480 --> 00:22:52,080
In many ways build on languages like pie torch

302
00:22:52,720 --> 00:22:59,680
Or tensor flow the languages that support modern deep learning and that allow you to construct really complex but end to end differentiable

303
00:23:00,640 --> 00:23:02,640
Functions for approximation or other purposes

304
00:23:03,280 --> 00:23:09,840
But that's not the most important part the most important part are building on the idea of symbolic languages for expressing abstract knowledge

305
00:23:10,080 --> 00:23:11,920
for modeling the world

306
00:23:11,920 --> 00:23:13,280
that is

307
00:23:13,280 --> 00:23:15,520
You know across many areas of science and engineering

308
00:23:16,000 --> 00:23:21,040
consistently the strongest most powerful toolkit we've had for building coherent models to

309
00:23:21,680 --> 00:23:24,720
To understand the world are various forms of symbols

310
00:23:25,440 --> 00:23:32,720
And then the third idea is the probabilistic one using those symbolic languages to express probabilistic models where you can be uncertain about

311
00:23:33,120 --> 00:23:35,840
Everything and in a in a turing computational sense

312
00:23:35,840 --> 00:23:40,480
You can be uncertain about the state of the world right now or more abstractly how the world works

313
00:23:41,200 --> 00:23:46,880
How the different kinds of data that you're getting perceptual or others are connected to the underlying state of the world

314
00:23:47,600 --> 00:23:52,400
And then to be able to do joint inference about all those different sources of uncertainty as the basis for perception

315
00:23:52,800 --> 00:23:54,800
reasoning planning learning and so on

316
00:23:55,040 --> 00:23:57,040
And probabilistic programs bring those things together

317
00:23:57,600 --> 00:24:03,600
I'll say a little bit about how that toolkit works as well as then how once you learn language

318
00:24:04,720 --> 00:24:11,200
The ability to externalize and internalize thoughts that are produced by these kinds of probabilistic programs basically

319
00:24:11,920 --> 00:24:17,360
That transforms things in fundamental ways and here, you know, less people think that i'm just a

320
00:24:18,080 --> 00:24:20,400
Deep learning or a llm skeptic or something

321
00:24:20,720 --> 00:24:27,040
I you know, again, I find neural language models and the the long tradition of distributional statistical learning

322
00:24:27,440 --> 00:24:31,600
In language that they build on to be really important and in the work that we've been doing we've been using

323
00:24:32,400 --> 00:24:35,120
large language models or in many by today's standards

324
00:24:35,120 --> 00:24:38,240
I might even say small language models to capture this actually

325
00:24:38,960 --> 00:24:44,880
So I think this is a place where the you know, I would not say that you want to think about human minds

326
00:24:45,040 --> 00:24:48,320
It's llms or or transformers all the way down nothing like that

327
00:24:48,640 --> 00:24:52,160
But the kinds of things that are going on in distributional sequence learning

328
00:24:52,560 --> 00:24:59,120
Could be a way to capture some aspects of how language grounds in these tools for mental modeling and and also

329
00:24:59,600 --> 00:25:01,600
enriches and extends it

330
00:25:02,000 --> 00:25:05,920
So, I mean just very briefly for people who aren't familiar with this idea

331
00:25:06,400 --> 00:25:09,840
There, you know, for a couple of decades now I and and many colleagues

332
00:25:10,240 --> 00:25:15,360
Students former students a number of others who are very interested in what you could call the Bayesian approach to cognition

333
00:25:16,080 --> 00:25:20,800
Have been using this general toolkit of probabilistic inference over structured symbolic models

334
00:25:21,200 --> 00:25:26,240
To capture many aspects of mental models and I just this is a plug for a book that's coming

335
00:25:26,800 --> 00:25:31,520
forthcoming edited by tom griffith's nick shader and myself and with many many other contributors

336
00:25:32,000 --> 00:25:34,400
And it'll be coming later this year from mit press

337
00:25:34,720 --> 00:25:39,600
But it's it's part textbook part research monograph and just showing how this toolkit can be used to

338
00:25:40,160 --> 00:25:45,440
Capture to both explain in a principled and you know, actually understandable way

339
00:25:45,760 --> 00:25:52,160
But also to quantitatively predict and model behavioral data across so many different ways in which our minds model the world

340
00:25:52,640 --> 00:25:55,840
Just to illustrate one which i'll come back to in the context of language in a minute

341
00:25:57,360 --> 00:26:00,080
An area that we've done a lot of work on is intuitive physics

342
00:26:01,040 --> 00:26:04,800
As as a kind of probabilistic inference and the particular kinds of intuitive physics

343
00:26:04,800 --> 00:26:10,000
We're thinking about are in these complex seed understanding cases, which again not coincidentally

344
00:26:10,080 --> 00:26:13,840
I think are also really interesting in classic settings for studying grounded language

345
00:26:14,400 --> 00:26:16,400
So for example, if I show you a

346
00:26:17,120 --> 00:26:20,880
Scene of a bunch of blocks think like jenga blocks stacked up in various ways

347
00:26:21,200 --> 00:26:26,800
Some of these images might look very stable others might look unstable and I can ask you a question

348
00:26:26,880 --> 00:26:30,080
How likely do you think this stack of blocks is to fall under gravity?

349
00:26:30,800 --> 00:26:37,280
And we can model that by having a a structured world model which consists of basically 3d object models

350
00:26:37,680 --> 00:26:43,040
And then causal models of how though of how those underlying 3d scenes give rise to images

351
00:26:43,120 --> 00:26:48,960
That's like a graphics program basically the computer graphics is a way of writing programs that generate images

352
00:26:49,280 --> 00:26:51,280
From those underlying 3d world models

353
00:26:51,680 --> 00:26:55,440
But then there's also these physics programs and again here I mentioned game engines

354
00:26:55,680 --> 00:27:01,040
So game engine style physics simulators capture a lot of aspects of effectively real world

355
00:27:01,120 --> 00:27:04,960
But common sense physics in ways that hack hack Newtonian or actual

356
00:27:05,520 --> 00:27:07,920
True scientific physics in all sorts of ways to be efficient

357
00:27:08,400 --> 00:27:10,400
But to a pretty good job of capturing

358
00:27:10,560 --> 00:27:15,120
What we expect to happen in the world which makes sense because we're the ones playing the video games and they're designed for us

359
00:27:15,520 --> 00:27:21,040
So by doing probabilistic inference to infer the input to a graphics program given the output

360
00:27:21,120 --> 00:27:26,960
Which is the image you can do a lot of 3d perception and then by doing probabilistic forward simulation

361
00:27:27,120 --> 00:27:30,720
You can imagine what might happen next. It could also be conditional on your action

362
00:27:31,200 --> 00:27:34,800
So the same toolkit could be applied to a much less familiar sort of judgment

363
00:27:35,360 --> 00:27:38,400
Like imagine I have these scenes of red and yellow blocks on a table

364
00:27:38,480 --> 00:27:41,760
And what if I bump the table hard enough to knock some of the blocks onto the floor?

365
00:27:42,080 --> 00:27:44,400
Will you knock off more red blocks or yellow blocks?

366
00:27:45,040 --> 00:27:48,160
So the first judgment the one about how likely the stack of blocks to fall

367
00:27:48,160 --> 00:27:52,000
I mean anyone who's played jenga or anybody who's ever been a kid probably has built things

368
00:27:52,240 --> 00:27:54,720
If you're a professional builder, you have a lot of other intuitions

369
00:27:55,040 --> 00:27:59,520
But you know, that's very familiar the question on the right is one that unless you've seen me talk about this

370
00:27:59,760 --> 00:28:05,040
You probably never thought about it's not something you have direct experience about you can't learn it from feedback

371
00:28:05,520 --> 00:28:06,960
How to answer that question?

372
00:28:07,280 --> 00:28:13,040
But I can use my language to give you that question as well as the other relevant world knowledge

373
00:28:13,040 --> 00:28:18,880
And then you can reason about it and the models that we build can capture both the familiar judgments of like

374
00:28:18,960 --> 00:28:21,680
How likely the stack is to fall as well as these novel judgments

375
00:28:22,480 --> 00:28:23,840
Relatively equally well

376
00:28:23,840 --> 00:28:28,400
So these scatter plots are examples of the kind of data and modeling that we've done in our lab for a long time

377
00:28:28,880 --> 00:28:36,080
On the y-axis we're plotting the average human judgments of let's say on one to seven scale how stable or unstable the blocks are

378
00:28:36,800 --> 00:28:38,800
and on the x-axis the average

379
00:28:39,760 --> 00:28:44,160
Result of doing a small number of probabilistic simulations where we imagine running forward

380
00:28:45,120 --> 00:28:51,520
Game-style physics a few time steps with uncertainty about exactly where the blocks might be and how the physics works because we don't

381
00:28:51,840 --> 00:28:53,840
There's are all things that our minds don't fully know

382
00:28:54,240 --> 00:28:58,480
The same kind of model can be used to answer these red and yellow questions. So just to illustrate this

383
00:28:59,440 --> 00:29:00,960
If we take one of these

384
00:29:00,960 --> 00:29:05,840
Scenes and we reconstruct it in a game-style physics engine and we simulate a bump of the table

385
00:29:06,160 --> 00:29:10,480
So there's one simulation on the right. I'll show you another simulation with a harder bump

386
00:29:10,800 --> 00:29:14,640
Okay, and you can see watch it again that different things happen

387
00:29:15,440 --> 00:29:17,440
in these two simulations

388
00:29:17,600 --> 00:29:21,280
But it doesn't really matter which one of those you ran to answer the question, right?

389
00:29:21,280 --> 00:29:25,200
I mean you look at the scene also and it's very clear from the beginning that if I bump the table

390
00:29:25,680 --> 00:29:29,680
Relatively hard. It's going to probably be mostly yellow blocks on the floor. How do you do that?

391
00:29:29,760 --> 00:29:31,760
Well in our model you only need to run

392
00:29:32,160 --> 00:29:36,400
One or a small number of these simulations to answer the question at the grain of intuitive physics

393
00:29:36,560 --> 00:29:39,760
And you don't need to run it very long. You could stop it now and you already know the answer

394
00:29:40,160 --> 00:29:40,640
Okay

395
00:29:40,640 --> 00:29:45,040
So a small number of short incomplete simulations is the basis for these pretty

396
00:29:45,360 --> 00:29:51,040
Quantitative models and they've also been used in robotics to actually predict and and be able to get for example a robot to

397
00:29:51,600 --> 00:29:56,000
Learn to play the game genka, but learn from a very small amount of realistic human experience

398
00:29:56,640 --> 00:29:58,640
The same kind of idea has been very

399
00:29:58,960 --> 00:29:59,920
impactful

400
00:29:59,920 --> 00:30:03,040
I think even more impactful in intuitive psychology

401
00:30:03,760 --> 00:30:09,840
Where we and many others this there's work that I've done with a number of students and my colleague at mit rebecca sacks

402
00:30:09,920 --> 00:30:15,840
But especially I would highlight work from chris baker and julia har edinger who did quantitative modeling here

403
00:30:16,160 --> 00:30:18,160
Going back years with us

404
00:30:18,480 --> 00:30:21,840
julian continued chris is now working on self-driving cars and industry julian

405
00:30:22,560 --> 00:30:24,560
is now working on

406
00:30:25,280 --> 00:30:32,480
It's now a professor at Yale where he's he's extended on this kind of approach in many areas of social and communicate with social cognition and communication

407
00:30:33,280 --> 00:30:34,800
in really interesting ways

408
00:30:34,800 --> 00:30:36,800
And many other colleagues here

409
00:30:37,440 --> 00:30:39,440
I and I should mention also though I didn't

410
00:30:39,920 --> 00:30:43,040
I forgot to mention this before the intuitive physics work started in our group

411
00:30:43,360 --> 00:30:46,560
More than 10 years ago with pete patalia and jess hamrick and others

412
00:30:47,040 --> 00:30:53,360
And they are now actually both working at deep mind google deep mind working on various interesting kinds of deep learning approaches

413
00:30:53,600 --> 00:31:00,000
For both intuitive physics and all sorts of real-world physics that matter like climate modeling. That's where pete's current focuses

414
00:31:00,720 --> 00:31:03,760
I won't go into the details on the bayesian theory of mind

415
00:31:04,400 --> 00:31:08,160
But the basic idea is again, there's a program that now describes

416
00:31:08,560 --> 00:31:15,040
Not necessarily how actual minds and brains works, but our mental models of other minds how the actions we see agents take

417
00:31:15,600 --> 00:31:19,520
We interpret them as the effect of planning programs that take as input

418
00:31:20,240 --> 00:31:23,680
The our minds representations of their beliefs and desires

419
00:31:24,080 --> 00:31:29,040
And by seeing how agents act and change the state of the world and modeling also their perception process

420
00:31:29,040 --> 00:31:34,400
Which leads to belief formation and updating we can model many aspects of how people understand

421
00:31:35,520 --> 00:31:41,600
Other minds especially in what you might call these sort of physically grounded and perceptual scenarios the kinds that you can also

422
00:31:41,600 --> 00:31:47,040
Study in young babies like in the experiments. I'm showing here from gargay chibra. Kylie hamlin paul bloom and others

423
00:31:47,760 --> 00:31:51,040
Um the the intuitive psychology case is especially interesting

424
00:31:51,600 --> 00:31:58,720
Because in work that we've done like in this paper with uh, brendan lake tomer olemann sam gershman that helped to set some of the

425
00:31:58,720 --> 00:32:02,960
Current stage of debates between deep learning and more cognitive approaches to ai

426
00:32:03,920 --> 00:32:07,520
Where we put about a challenge back in 2016 and 2017

427
00:32:09,040 --> 00:32:14,880
For the the deep learning ai world about how to think about the kinds of inductive biases mental model

428
00:32:15,600 --> 00:32:19,600
Materials that seem to be built into human minds and the learning mechanisms that build on those

429
00:32:20,400 --> 00:32:27,200
We highlighted both intuitive physics and intuitive psychology in the famous sparks of agi paper showing again some of the most

430
00:32:28,400 --> 00:32:32,880
earliest glimpse into some of the surprising things that gpd4 seemed to be able to do

431
00:32:33,680 --> 00:32:40,080
bubeka and colleagues also highlighted intuitive physics and intuitive psychology and then in a recent paper from eric schultz's group

432
00:32:40,640 --> 00:32:47,520
Which is kind of forms a a three-part story here and it continues as as i know bushoff and and schultz and colleagues

433
00:32:47,760 --> 00:32:50,000
Continue to work on this area and we also do

434
00:32:50,560 --> 00:32:55,440
Is to now in the era of these multimodal language and vision models like i showed you at the beginning

435
00:32:55,760 --> 00:33:02,640
We can go and take on all these tasks like whether it's block tower intuitive physics things or agents moving around in the world and interpreting

436
00:33:03,280 --> 00:33:07,760
As julia harr edinger called it the naive utility calculus of their rewards and costs

437
00:33:08,400 --> 00:33:13,760
And give language models or multimodal models the same kinds of stimuli and questions that we give people

438
00:33:14,720 --> 00:33:16,720
And as schultz and colleagues showed

439
00:33:17,840 --> 00:33:21,520
In some of the intuitive physics settings, they're kind of okay. They're not great

440
00:33:22,800 --> 00:33:26,560
In the intuitive theory of mind ones they fail altogether. That's a quote from their paper

441
00:33:26,720 --> 00:33:33,600
So as an example, this is this is work that julia harr edinger did as part of his phd thesis with myself and laura schultz

442
00:33:34,480 --> 00:33:40,640
Um where we call these the astronaut studies where people would see an agent an astronaut on some planet

443
00:33:40,880 --> 00:33:48,000
Who would start at a certain point and have a home base that they had to get to and they would follow some path along

444
00:33:48,720 --> 00:33:50,720
This the surface of the planet

445
00:33:50,720 --> 00:33:54,080
They could just go straight to their home base or they could take a different a not straight path

446
00:33:54,640 --> 00:34:00,880
Um crucially there were various objects that they could pick up that could either be positively valuable or aversive to them

447
00:34:01,440 --> 00:34:03,440
And we asked people based on the path

448
00:34:03,440 --> 00:34:07,200
How likely do you think that the agent likes or doesn't like one of these objects?

449
00:34:07,600 --> 00:34:11,040
And also there are different terrains and the terrains could be more or less costly

450
00:34:11,360 --> 00:34:16,560
And by showing people different maps different configurations of objects based terrain and different paths

451
00:34:16,880 --> 00:34:22,880
You could get very interesting rich inferences about what about what the agent both wants and

452
00:34:23,360 --> 00:34:27,600
What the rewards assigned to the different objects are as well as the costs for moving around on the terrain

453
00:34:27,840 --> 00:34:30,560
So here's an example of some of the stimuli from one experiment

454
00:34:30,800 --> 00:34:33,920
Just to show you the kind of variation and then in each of these cases

455
00:34:33,920 --> 00:34:38,480
We can ask people to make four judgments or three or four depending on how many kinds of terrain

456
00:34:38,720 --> 00:34:42,480
And that's what's shown here. These are these are the actually the predictions of the model

457
00:34:43,200 --> 00:34:48,960
Z scored for the relative cost for the different kinds of terrain and the relative value for the different kinds of objects

458
00:34:49,200 --> 00:34:51,520
And we're assuming that the agent basically takes in a

459
00:34:52,160 --> 00:34:55,520
of rational efficient plan trying to maximize reward

460
00:34:56,000 --> 00:35:01,280
Minus cost where there's a small cost for each step, but especially costly when you travel over certain kinds of terrain

461
00:35:01,840 --> 00:35:06,560
So seeing the path you can make inferences about how the agent has rewards and costs

462
00:35:06,880 --> 00:35:10,560
And then when you ask people to make the same judgments, they line up almost perfectly

463
00:35:10,960 --> 00:35:16,080
This is just one of many experiments that julian did showing what is really a remarkable quantitative

464
00:35:16,560 --> 00:35:20,160
Match and but but by a model that isn't just fit to data

465
00:35:20,400 --> 00:35:22,720
There's a little bit of fitting but it's mostly based on

466
00:35:23,120 --> 00:35:28,480
Thinking about what are the core concepts of theory of mind that people like I mentioned before like gargay and chibra

467
00:35:28,640 --> 00:35:33,120
Have studied even in very young infants pre-verbal infants infants who can't even walk

468
00:35:34,560 --> 00:35:37,840
And barely just able to reach for things themselves yet

469
00:35:37,840 --> 00:35:43,280
They still have these ideas these intuitions about efficient inference and and use those it seems

470
00:35:44,160 --> 00:35:46,160
I mentioned sherry lou's work who was a

471
00:35:46,480 --> 00:35:50,720
PhD student at harvard a few years ago with this velki who worked with tomer omen also in me

472
00:35:51,040 --> 00:35:56,640
To show those same kinds of things work in babies sherry's now doing amazing stuff extending that in a new lab at johns hopkins

473
00:35:57,280 --> 00:36:02,720
Um, so these these are cases where this kind of probabilistic program model works really well, but just a pure language model

474
00:36:03,280 --> 00:36:09,280
Uh is basically at chance zero correlation with human judgments, although quite good at telling you things like the background color of the scene

475
00:36:10,000 --> 00:36:14,800
Okay, so in the last part of the talk having sort of set the stage for how are our

476
00:36:15,680 --> 00:36:20,080
Human thinking seems to work and how we can model this in ways that are explanatory

477
00:36:20,720 --> 00:36:26,720
Understandable and quite quantitatively predictive using these probabilistic programs. Let's understand where language comes into picture

478
00:36:27,200 --> 00:36:31,840
I don't think that a pure machine learning transformer approach like the ones we've been talking about

479
00:36:32,400 --> 00:36:37,200
Are on track to give a human level or certainly a human like account of how it works

480
00:36:37,440 --> 00:36:43,680
But I do think the ideas of sequence modeling statistical distribution learning that you see having

481
00:36:44,240 --> 00:36:46,240
such great success in llms

482
00:36:46,560 --> 00:36:51,280
Even perhaps on a smaller scale could say something important about how language comes into the picture

483
00:36:51,920 --> 00:36:54,160
So this here is the at the beginning

484
00:36:54,160 --> 00:36:57,440
I talked about this archive paper the word models to world models paper

485
00:36:57,840 --> 00:37:04,080
From line along Gabe grand and colleagues and that's what i'm going to be talking about here is how we've been using bringing these tools together

486
00:37:04,560 --> 00:37:09,280
But a key ingredient is this I would say the modern return of the language of thought hypothesis

487
00:37:09,280 --> 00:37:14,320
You know made famous by jerry fodor, but obviously with a history that goes back hundreds if not thousands of years

488
00:37:14,640 --> 00:37:16,640
But in the number of recent papers

489
00:37:17,360 --> 00:37:21,680
And proposals from various groups the idea that again that there's some kind of

490
00:37:22,480 --> 00:37:27,840
Not necessarily single language, but but abstract symbolic languages, which could be general or also

491
00:37:28,320 --> 00:37:30,820
Created and constructed domain specific languages

492
00:37:31,200 --> 00:37:37,600
Some kind of abstract symbolic language is seems to be a powerful way to think about human thinking abstraction

493
00:37:37,840 --> 00:37:39,520
concept learning and so on

494
00:37:39,520 --> 00:37:42,480
And the particular kind of languages of thought that we've been thinking about are

495
00:37:43,040 --> 00:37:47,200
What noah goodman and toby gersenberg and I called the probabilistic language of thought

496
00:37:47,520 --> 00:37:51,440
So this is the idea of using probabilistic programming languages the technical tool

497
00:37:51,520 --> 00:37:52,960
I talked about before

498
00:37:52,960 --> 00:37:58,160
To to formalize in a cognitive setting a certain kind of hypothesis of a language of thought

499
00:37:58,400 --> 00:38:00,720
But one that is focused on modeling the world

500
00:38:01,120 --> 00:38:07,440
Not just possible worlds, but probable worlds and where the the symbolic language can also express ways of conditioning and query

501
00:38:07,760 --> 00:38:11,360
So that we can ask and answer the kinds of questions that our minds do and that

502
00:38:11,680 --> 00:38:15,200
We might we might want for example any kind of general ai system to do

503
00:38:15,520 --> 00:38:20,400
So if you want to learn more about the probabilistic language of thought check out our chapter in the conceptual mind

504
00:38:20,800 --> 00:38:23,280
This is one of the margiles and laurence

505
00:38:24,160 --> 00:38:30,080
Readers or the web book probabilistic models of cognition, which which has examples of the kinds of models

506
00:38:30,080 --> 00:38:31,520
I'm going to be talking about

507
00:38:31,520 --> 00:38:36,960
Not with language models though the new thing is to take advantage of llms and specifically the fact that

508
00:38:37,200 --> 00:38:40,240
Most llms these days are trained not only on natural language

509
00:38:40,480 --> 00:38:43,200
But on on programming languages and source code

510
00:38:43,600 --> 00:38:49,360
Namely programming languages like all the ones we're probably familiar with that are designed to be read and written by humans

511
00:38:49,440 --> 00:38:52,320
And not just machines and so they're written in a very english like way

512
00:38:52,880 --> 00:38:56,720
Or natural language like way, you know linguists have long pointed out

513
00:38:57,200 --> 00:39:02,640
Sometimes the differences between natural languages and programming languages, but from a certain perspective

514
00:39:02,880 --> 00:39:04,880
They're a lot more similar than they are different

515
00:39:05,120 --> 00:39:10,320
A hierarchical structure syntax and even just much of the lexicon, you know natural languages

516
00:39:10,400 --> 00:39:14,080
They're not just commented or programming languages are not just commented in natural language

517
00:39:14,080 --> 00:39:18,880
But functions variables data structures are named using english for the most part

518
00:39:18,960 --> 00:39:22,960
Okay, so that's a very powerful data source that allows

519
00:39:23,600 --> 00:39:31,680
Some kind of statistical sequence to sequence model for predicting and translating between streams sequential streams to effectively learn to translate

520
00:39:32,000 --> 00:39:37,760
From english or any other natural language into programming languages of thought and that includes probabilistic programming

521
00:39:38,160 --> 00:39:42,400
So the idea of this paper what we call rational meaning construction. That's the name of this

522
00:39:43,120 --> 00:39:49,280
Is is a particular thesis on how language is understood and perhaps also how it might be produced and learned

523
00:39:49,360 --> 00:39:51,680
Although our focus here is on language understanding

524
00:39:52,400 --> 00:39:56,400
And thinking about the relation between language of language and thought in that context

525
00:39:57,040 --> 00:40:03,280
By thinking what we think of as like the core original notion of thinking is what I've been talking about for the for the first part of the talk

526
00:40:03,520 --> 00:40:05,520
Which is the idea of having a

527
00:40:06,080 --> 00:40:10,240
Structured probabilistic model of the world conditioning it on observations and then

528
00:40:10,800 --> 00:40:14,960
And then drawing samples of underlying latent states and future states

529
00:40:15,440 --> 00:40:17,440
Okay, that's thinking from this standpoint

530
00:40:17,680 --> 00:40:21,760
And then understanding language is effectively translating from natural language

531
00:40:22,080 --> 00:40:27,600
Into a probabilistic language of thought that's used to define and condition and query the probabilistic world model

532
00:40:28,000 --> 00:40:30,800
So we're going to exploit the the property of these

533
00:40:31,280 --> 00:40:36,720
LLMs to translate from natural language to code in a way that might be familiar if you've tried using them to code

534
00:40:36,720 --> 00:40:39,040
But it's different in some key ways in particular

535
00:40:39,120 --> 00:40:43,520
We're not going to be just asking our LLM to write a whole bunch of code at least not to start

536
00:40:43,680 --> 00:40:47,520
But we're going to be focusing on the sentence level and what is meaning at the sentence level

537
00:40:47,840 --> 00:40:52,480
It's something like trying to infer a line of code in a mental programming language

538
00:40:52,800 --> 00:40:54,800
that is your best understanding of

539
00:40:55,280 --> 00:41:01,040
The meaning by which we just mean the thought that the person who's uttering that sentence is trying to convey

540
00:41:01,840 --> 00:41:06,960
Okay, and the the LLMs in this case can represent that meaning construction function

541
00:41:07,280 --> 00:41:09,440
Okay in ways that have some very interesting properties

542
00:41:09,680 --> 00:41:15,760
So i'll just illustrate this with a classic example from the probabilistic language of thought literature the so-called Bayesian tug-of-war

543
00:41:16,160 --> 00:41:19,360
So just to ground this imagine that you're reasoning about a situation

544
00:41:20,160 --> 00:41:21,680
Like for example, uh

545
00:41:21,680 --> 00:41:26,960
Various games of tug-of-war various people symbolized by colored shirts here are facing off against each other

546
00:41:27,200 --> 00:41:29,760
And let's just take one person. Let's just say this is a

547
00:41:30,640 --> 00:41:34,880
Guy named jack and you might say well, how strong do you think jack is I haven't shown you any information?

548
00:41:34,960 --> 00:41:36,960
So your best guess might be just average. Okay

549
00:41:38,240 --> 00:41:40,960
Now, uh, suppose I give you some other information

550
00:41:42,320 --> 00:41:45,680
Like um, or I should say here. So here are a few samples, you know, I might say average

551
00:41:45,680 --> 00:41:47,520
Maybe it's a little less than average

552
00:41:47,520 --> 00:41:50,320
If we're talking about MIT students, you may just pick the MIT average

553
00:41:50,480 --> 00:41:54,960
But now I could give more information like suppose, you know that jack beat leo in a game of tug-of-war

554
00:41:55,200 --> 00:41:58,320
So that might move your arrow up from the MIT average because well, you know

555
00:41:59,040 --> 00:42:03,440
People who are who are stronger might be more likely to win than people who are weaker somehow. Okay

556
00:42:04,000 --> 00:42:06,000
I could give you some more information

557
00:42:06,000 --> 00:42:11,280
Like leo had just won 10 previous matches as well. So that means you might think leo's pretty strong and jack must be even stronger

558
00:42:11,280 --> 00:42:12,800
So your arrow goes way up

559
00:42:12,800 --> 00:42:16,080
But suppose I told you well leo sometimes it doesn't pull as hard as they really could

560
00:42:16,320 --> 00:42:21,760
So maybe maybe leo was just getting a little lazy when leo faced jack. So it might go down a little bit

561
00:42:22,400 --> 00:42:27,200
But then leo single-handedly beat a team with jack and tom on it. Okay. So now you think okay

562
00:42:27,200 --> 00:42:31,360
Well leo was probably lazy before jack. Maybe isn't that strong

563
00:42:32,240 --> 00:42:35,440
Since leo when they wanted to could beat both jack and tom. Okay

564
00:42:35,440 --> 00:42:42,000
So the point is in a classic example of non-monotonic reasoning your inference about this one aspect of the world jack strength

565
00:42:42,000 --> 00:42:48,400
It's going up and down as you get various information. So we'd like to understand how that thinking process works how

566
00:42:49,280 --> 00:42:56,320
Updating your beliefs based on linguistic evidence linguistically expressed evidence works. Okay, that's the starting point of this paper

567
00:42:56,880 --> 00:42:58,800
And the idea is to use in this case

568
00:42:58,800 --> 00:43:04,400
We're using the the probabilistic programming language church which is based on a dialect of lisp or scheme

569
00:43:04,640 --> 00:43:07,360
So there's a lot of parentheses and i'm not going to be able to unpack all the language

570
00:43:07,440 --> 00:43:12,480
But hopefully you can get the basic idea that we write we define functions that describe

571
00:43:13,040 --> 00:43:15,040
probabilistic distributions on

572
00:43:15,520 --> 00:43:21,600
Strength laziness and so on all work through this and that's you know, that's been that's well-attested work

573
00:43:21,600 --> 00:43:27,920
That's been very good. You know, it's basically the the general toolkit for doing the kind of probabilistic intuitive

574
00:43:28,560 --> 00:43:34,000
Mental modeling that I showed you with two to physics and intuitive psychology, but here we're doing it for this kind of novel domain

575
00:43:34,000 --> 00:43:37,520
It's not there's no core domain that infants are born with for tug of war

576
00:43:37,680 --> 00:43:41,840
But we can write a model like this to capture what adults in our culture at least might think

577
00:43:42,240 --> 00:43:48,400
And then the key new thing here is to understand how to to ground language in these mental models

578
00:43:48,880 --> 00:43:55,440
So this is where we're going to model the translation of a statement in english like jack one against leo or a question

579
00:43:55,520 --> 00:44:02,560
Like how strong is jack into lines of code that support probabilistic updating and querying in this language

580
00:44:03,280 --> 00:44:07,360
And this these the meaning functions, which will be implemented by the neural network here

581
00:44:08,400 --> 00:44:09,760
other the

582
00:44:09,760 --> 00:44:11,760
Large or even small language model

583
00:44:12,160 --> 00:44:15,360
Um capture various interesting ideas that have been proposed

584
00:44:15,600 --> 00:44:20,320
You know, I don't have time to review this history but proposed in different eras of linguistics and thinking about

585
00:44:21,120 --> 00:44:26,480
How to think about meaning how to think about concepts and you know, I it's it's a it's a very rich literature

586
00:44:26,480 --> 00:44:29,680
That I'd be happy to try to discuss if we have time during the question period

587
00:44:30,400 --> 00:44:37,200
Okay, um, but the hypothesis here is that maybe these distributional language code models can implement some

588
00:44:37,840 --> 00:44:41,440
human like perhaps approximation to this meaning function

589
00:44:42,400 --> 00:44:43,440
Um

590
00:44:43,440 --> 00:44:50,480
And again, crucially, it's it's we're not trying to now capture like to learn patterns in data in the in in the world

591
00:44:50,480 --> 00:44:54,080
We're trying to learn patterns in our thinking and how thought is expressed in language

592
00:44:54,240 --> 00:44:56,240
So it's a more modular problem

593
00:44:56,240 --> 00:45:00,560
It which I think is better suited to the way language actually works in the human brain, right?

594
00:45:00,560 --> 00:45:04,800
It's there's a delimited part of our brain. That is a language processing network

595
00:45:05,200 --> 00:45:11,360
Strokes or other lesions there can really impact your language ability without impacting your general thinking ability

596
00:45:12,640 --> 00:45:16,640
And you know, it's a relatively late evolutionary addition to us to a brain structure

597
00:45:16,720 --> 00:45:19,760
Which shares a lot with other non-human primates, for example

598
00:45:20,240 --> 00:45:23,760
And so it's it's that that modularity. I think is also really important here

599
00:45:24,480 --> 00:45:26,400
So again, just to unpack how this works

600
00:45:26,480 --> 00:45:29,600
You have a probabilistic program that describes these concepts

601
00:45:30,080 --> 00:45:33,120
That I can just I can describe to you in language in this way

602
00:45:33,280 --> 00:45:37,360
But to the model right now, they're just given in code and I'm not and they just describe for example that

603
00:45:37,760 --> 00:45:41,840
You know players strength is drawn from a gaussian distribution players can occasion be lazy

604
00:45:42,480 --> 00:45:45,520
When you when you're lazy it cuts your effective strength in half

605
00:45:45,760 --> 00:45:51,120
And the strength of the team is that some of the total strength that they pull that the members pull with at any one time

606
00:45:51,440 --> 00:45:54,960
And the stronger pulling team wins. That's basically what this is now for now

607
00:45:55,040 --> 00:45:57,920
We'll just assume that that's you have that kind of mental

608
00:45:58,640 --> 00:46:00,640
Model of a tug of war game

609
00:46:00,640 --> 00:46:02,960
And we'll just talk. How do you update your release language?

610
00:46:03,280 --> 00:46:08,240
And the basic way of doing it is that we're using what I would call now a medium language model

611
00:46:08,240 --> 00:46:16,480
We used open ai's codex, which was the first widely used code llm pretty small by today's standards much smaller than gpt4

612
00:46:17,120 --> 00:46:19,520
Kind of like an early version of gpt 3.5

613
00:46:20,000 --> 00:46:26,000
To translate from a sentence in english like this into what in church the probabilistic programming language is called a condition statement

614
00:46:26,160 --> 00:46:30,320
It just expresses the constraint on possible worlds that while the underlying

615
00:46:30,720 --> 00:46:34,960
Define statements define stochastic or probabilistic functions, which give you distributions

616
00:46:35,360 --> 00:46:39,680
Probability distributions on possible worlds. This says we're going to restrict our probability distribution

617
00:46:39,840 --> 00:46:46,160
Which is like our prior to a posterior just those worlds that are consistent with jack beating in one match leo

618
00:46:46,560 --> 00:46:47,760
okay

619
00:46:47,840 --> 00:46:52,960
And then similarly a quest to a query statement like how strong is jack turns into this worry

620
00:46:53,680 --> 00:46:57,040
Which is which the probabilistic programming language then basically

621
00:46:57,600 --> 00:47:03,520
Evaluates it draws samples from possible worlds consistent with the condition and generated from that prior on the left

622
00:47:04,000 --> 00:47:07,840
And and checks what the strength is and then just kind of counts up those distributions

623
00:47:07,840 --> 00:47:12,720
So it's a kind of probabilistic mental simulation the same effectively the same or a generalization of what we were doing

624
00:47:12,720 --> 00:47:17,440
You like those intuitive physics examples and from this one piece of data jack beating leo

625
00:47:17,520 --> 00:47:21,760
You can see the posterior updates from the prior so jack is stronger than average

626
00:47:22,080 --> 00:47:27,200
If I say oh jack also beat alex or proceeded to claim victory from alex that turns into another condition

627
00:47:27,520 --> 00:47:31,200
And that updates the posterior even more so now while jack's a lot stronger than average

628
00:47:32,080 --> 00:47:37,600
Even working as a team leo and alex still could not be jack. Wow now jack's even stronger when you add in that conditioning statement

629
00:47:37,920 --> 00:47:40,640
So each again the where the llm is coming in here

630
00:47:41,520 --> 00:47:49,120
Is it's just it's just adding in the the statements into the language of thought and then we're running inference in our probabilistic programming language to give these answers

631
00:47:50,000 --> 00:47:53,840
It's worth and this is again, especially if we want to understand what I think is

632
00:47:55,280 --> 00:48:00,160
The way forward to thinking about how meaning and language works in humans and where l lm's can come into the mix

633
00:48:00,480 --> 00:48:02,480
What's what's really powerful about?

634
00:48:02,880 --> 00:48:04,800
Neuro language models here

635
00:48:04,800 --> 00:48:11,600
As ways to parameterize a meaning function is the ways that they can pick up on statistics context pragmatics

636
00:48:12,080 --> 00:48:13,120
metaphor

637
00:48:13,120 --> 00:48:19,200
Um semantic associations all the things that in many ways were most appealing in connectionism like distributed content addressable

638
00:48:19,760 --> 00:48:24,720
Uh associate of memory and distributed representations of graded semantic associations

639
00:48:25,040 --> 00:48:28,800
So for example, the llm will translate jack won against leo

640
00:48:29,200 --> 00:48:34,400
Which into this statement which looks like a fairly transparent semantic parse of that natural language

641
00:48:34,800 --> 00:48:39,840
But it'll make basically the same semantic parse of a sentence which on its surface

642
00:48:40,560 --> 00:48:46,480
And and in traditional syntax, you know language structuralities looks rather different, right? The syntax here is more complicated

643
00:48:46,480 --> 00:48:48,880
I'm not actually using the word win, but

644
00:48:49,600 --> 00:48:56,240
In the context the relevant aspect of meaning for thought here is the same namely. This is just another way. It may be more poetic way

645
00:48:57,200 --> 00:49:01,760
Or dramatic way to say that jack won against alex and the model knows that automatically

646
00:49:01,760 --> 00:49:06,160
It doesn't have to be specially prompted or trained for that using its associative memory properties

647
00:49:06,480 --> 00:49:10,240
It's also distributional. These are probabilistic models not on worlds

648
00:49:10,400 --> 00:49:15,120
But on strings and in this case they're probabilistic models on strings in our mental programming language

649
00:49:15,280 --> 00:49:16,800
so they can

650
00:49:16,800 --> 00:49:21,120
Bring into you know bring in classic notions of vagueness if I say jack is strong or very strong

651
00:49:21,280 --> 00:49:25,680
I'm not telling you exactly how strong jack is but you might interpret that as a distribution

652
00:49:26,000 --> 00:49:29,760
On different condition statements saying well jack strength is is greater than some threshold

653
00:49:29,760 --> 00:49:33,200
But I don't know what that threshold is but it's probably pretty big remember this in this case

654
00:49:33,280 --> 00:49:36,960
The mean is 50 and the standard deviation 20 so 80 is you know

655
00:49:37,760 --> 00:49:42,400
One and a half standard deviations above the mean and the idea is this is again like in scalar

656
00:49:44,000 --> 00:49:47,600
Adjectives it's basically saying I'm conditioning on the idea that jack is greater than some threshold

657
00:49:47,600 --> 00:49:53,920
But I but I could be uncertain about what that threshold is and even metaphorical things like if I say jack is pretty strong

658
00:49:54,000 --> 00:49:56,400
And ben is a beast right in this context

659
00:49:56,720 --> 00:50:01,360
It's reasonable to interpret ben is a beast as as saying saying ben is really really strong

660
00:50:01,520 --> 00:50:07,280
You know 80 where jack was maybe 60 or greater in a different context if i'm talking about you know

661
00:50:08,000 --> 00:50:13,440
Whether you should date someone and I say ben is a beast, you know, then i'm probably saying stay away

662
00:50:14,160 --> 00:50:15,280
right

663
00:50:15,280 --> 00:50:23,600
So that that both metaphor sort of context sensitive metaphorical interpretation is the kind of thing that this meaning function is very good at

664
00:50:24,560 --> 00:50:28,800
um, I'll mostly just skip over this but in work with ben lytkin and and

665
00:50:29,840 --> 00:50:36,000
Leo and gape and others we've we've done and this is really all ben's work some very nice quantitative studies

666
00:50:36,880 --> 00:50:40,960
Of showing that these contextual aspects of understanding of strength

667
00:50:42,000 --> 00:50:48,720
In in the kind of tug-of-war context can match very nicely not always but in a lot of cases very nicely with human judgment

668
00:50:48,800 --> 00:50:52,160
so it's suggesting that the distributional aspects of

669
00:50:53,120 --> 00:50:55,520
um meaning that we're capturing here are at least somewhat

670
00:50:56,000 --> 00:50:59,280
Not psychologically real at least consistent with the judgments that people make

671
00:50:59,680 --> 00:51:05,600
Okay, so so but mostly what i've done with this example is just to try to show you how this kind of framework can work

672
00:51:06,000 --> 00:51:10,560
To be a way of implementing what is in some sense a classical idea that language isn't

673
00:51:11,360 --> 00:51:14,240
That directly the medium of thought at least the only one

674
00:51:14,640 --> 00:51:18,560
But it's a way of expressing and communicating thoughts internalizing and externalizing them

675
00:51:18,720 --> 00:51:24,160
Okay, and that statistical distributional mechanisms can be a powerful way of learning that

676
00:51:24,560 --> 00:51:27,600
The mappings from the signs and symbols that we externalize

677
00:51:28,240 --> 00:51:31,600
To the to an internal compositional structured language of our thought

678
00:51:32,160 --> 00:51:35,440
Now a lot of what goes on in this paper and i'll just show you one or two examples

679
00:51:36,320 --> 00:51:43,040
Such as work with sed zhong in an intuitive physics domain or some intuitive psychology work with lan xing and shen or

680
00:51:44,000 --> 00:51:45,520
Tan shi shen

681
00:51:45,520 --> 00:51:49,120
Is showing how we can take the same kinds of things that i showed you in the first part of the talk

682
00:51:49,520 --> 00:51:55,520
And effectively reconstruct them, but now in a world's worlds that are just described by language

683
00:51:56,080 --> 00:52:01,440
So to manage it take that task of the red and yellow thing where before i was showing you the image of a scene

684
00:52:01,840 --> 00:52:04,160
And then i was using language to describe a question

685
00:52:04,480 --> 00:52:07,680
But what if i just use language to describe the whole world, right?

686
00:52:07,760 --> 00:52:09,760
So if i didn't show you an image i just described

687
00:52:10,240 --> 00:52:17,520
What's there, you know in many ways, but but but we're still going to do a a mental simulation a probabilistic mental simulation in a physics engine

688
00:52:17,680 --> 00:52:23,040
There's many cases, you know in our daily life where we use language to describe the physical world as we experience it

689
00:52:23,280 --> 00:52:28,640
It can be extremely expressive very complimentary often to you know images or photos

690
00:52:29,120 --> 00:52:31,840
Um, it has vagueness and uncertainty and that's interesting

691
00:52:32,080 --> 00:52:35,680
So we might describe a scene that like imagine a table and there's some blocks on it

692
00:52:35,920 --> 00:52:37,280
There's some red blocks in the center

693
00:52:37,280 --> 00:52:41,200
There are many tall stacks of yellow blocks on the side of the table if the table is bumped hard enough

694
00:52:41,280 --> 00:52:44,240
So we ask the same question but for scenes that are described in language

695
00:52:44,240 --> 00:52:50,080
And then people make a graded judgment the same one to seven graded judgment of is it more likely to be red or yellow blocks?

696
00:52:50,800 --> 00:52:54,480
And so when we can test this again the same kind of quantitative study with

697
00:52:55,040 --> 00:52:58,080
large number of participants all online all just reading stuff

698
00:52:58,880 --> 00:53:02,720
With different kinds of language expressing exact as well as approximate number

699
00:53:03,440 --> 00:53:08,400
Approximate quantifiers logical quantifiers vagueness like the stacks could be tall or very tall

700
00:53:08,800 --> 00:53:13,200
And and different kinds of spatial relations and across many different stimuli

701
00:53:13,520 --> 00:53:18,000
We we mix up more or less complex sentences using these different kinds of language

702
00:53:18,640 --> 00:53:22,240
And in each case we're using again a relatively small

703
00:53:22,640 --> 00:53:25,840
LLM to translate sentence by sentence into

704
00:53:26,400 --> 00:53:29,600
statements for conditioning and querying in our probabilistic language of thought

705
00:53:30,000 --> 00:53:33,840
Then we run a small number of mental simulations in this case in a 2d physics engine

706
00:53:34,320 --> 00:53:39,360
Compute the outputs and compare those with people and what i'm showing you here is again the same kind of scatterplot

707
00:53:39,360 --> 00:53:46,000
I showed before but on the vertical axis are again our human judgments and on the x-axis now are the model

708
00:53:46,160 --> 00:53:48,960
Or the predictions of this language informed thinking model

709
00:53:49,440 --> 00:53:55,360
So it looks a lot like what I showed you before these are the judgments from the battalion all work on the red and yellow task

710
00:53:55,760 --> 00:53:59,600
We're not we're here the model was given a visual scene people were given visual scenes

711
00:53:59,920 --> 00:54:04,000
And the model did that same kind of probabilistic mental simulation in the physics engine

712
00:54:04,480 --> 00:54:06,480
But in the in the current work on the left

713
00:54:07,040 --> 00:54:13,920
Again, the scene description is constructed from language using the tools that i've shown you by conditioning a prior on scenes

714
00:54:14,000 --> 00:54:19,840
Okay, and the main point is just that the models fit human judgments pretty well in both cases and about equally well

715
00:54:20,080 --> 00:54:22,080
All right, which is which is interesting

716
00:54:22,400 --> 00:54:26,960
We can also compare with just a sort of zero shot or a few shot baseline llm

717
00:54:26,960 --> 00:54:32,800
So these are language models, which don't have an explicit mental model of physics or any doing explicit simulation

718
00:54:32,800 --> 00:54:35,840
And they are are much worse fits to people

719
00:54:36,080 --> 00:54:39,520
Okay, this is this on the what the high up here on the y-axis is

720
00:54:39,840 --> 00:54:44,080
Distance and distribution from human distribution of responses to the various models

721
00:54:44,080 --> 00:54:49,600
The blue one is the rational meeting construction model and you can do a similar thing in the intuitive psychology domain

722
00:54:50,400 --> 00:54:54,960
Like for example in the settings that I showed you with julien harenger's work

723
00:54:55,520 --> 00:55:00,560
This is work that uh lans ying and shen did i won't go into the details, but again

724
00:55:00,560 --> 00:55:03,280
We can describe worlds with various

725
00:55:04,000 --> 00:55:08,160
Goal objects that an agent might have and constraints like you have to go through doors the doors could be locked

726
00:55:08,160 --> 00:55:10,960
You have to use keys that could be a red key that could unlock the red door

727
00:55:11,360 --> 00:55:13,760
And so on people could be told either that you know

728
00:55:13,920 --> 00:55:16,880
You need a key of the right color to open each door of the same color

729
00:55:17,280 --> 00:55:19,280
Or you could be told weird things like

730
00:55:19,360 --> 00:55:21,360
Keys only unlock doors of other colors

731
00:55:21,360 --> 00:55:25,440
Okay, um, you know in these kinds of uh worlds again

732
00:55:25,440 --> 00:55:29,120
The rational meeting construction model does a very good job of capturing people's judgments

733
00:55:29,520 --> 00:55:35,120
For some kinds of judgments easy cases even gpt 3.5 does reasonably well gpt 4 does better

734
00:55:35,440 --> 00:55:40,560
But as the situation gets more complex or unusual things change and break down like especially if

735
00:55:40,880 --> 00:55:44,240
We say well in this world keys only unlock doors of different colors

736
00:55:44,320 --> 00:55:47,280
That's an easy thing to say to somebody and you have to change your mental model

737
00:55:47,680 --> 00:55:55,120
But when you do something like that gpt 3.5 becomes anti correlated with people and gpt 4 just go drops from being highly correlated to just being a chance

738
00:55:55,840 --> 00:56:00,960
So these are examples of the ways in which you know going out of distribution from our training experience

739
00:56:00,960 --> 00:56:06,640
If we're just using a machine learning function approximation approach, which is remarkable as those systems like gpt 4r

740
00:56:06,720 --> 00:56:08,160
That's what they're doing

741
00:56:08,160 --> 00:56:13,600
But ways in which if the function approximation is just approximating a much more modular translation function

742
00:56:14,240 --> 00:56:17,120
And building on our mental model tools. We can just do much better

743
00:56:18,080 --> 00:56:21,760
The last thing i'll just talk about very briefly is you know what is

744
00:56:22,400 --> 00:56:24,800
Any work like this has to raise more interesting questions

745
00:56:24,880 --> 00:56:28,800
If if anything like this is right on the right track, then it's it's not answering

746
00:56:29,440 --> 00:56:33,600
At this point the most interesting questions, but just raising them and perhaps highlighting ways you might get at them

747
00:56:34,160 --> 00:56:39,840
So in all the work i've been doing in in for a couple of decades now in probabilistic inference in mental models

748
00:56:40,160 --> 00:56:44,640
You start by showing how you can do inference with a mental model, but then you have to ask where do you get it from

749
00:56:45,120 --> 00:56:49,680
Now learning from experience is one thing and i've done a lot of work and others in our group and many other

750
00:56:49,760 --> 00:56:57,840
Colleagues a lot of that book i mentioned with tom griffiths and nick shader is about how we can learn using hierarchical bays and probabilistic programs learn abstract

751
00:56:58,400 --> 00:57:02,720
Programs by doing inference over the space of programs to make sense of our data

752
00:57:02,800 --> 00:57:04,640
Maybe even small amounts of data

753
00:57:04,640 --> 00:57:09,600
But much of our learning probably the most powerful form of human learning comes through language more abstract

754
00:57:09,920 --> 00:57:14,080
Generic language as many folks in cognitive science cognitive development

755
00:57:14,640 --> 00:57:15,760
Have shown

756
00:57:15,760 --> 00:57:21,360
So the power of language not only to update your beliefs, but actually to give you new world models is really

757
00:57:21,920 --> 00:57:27,520
Incredible and that's probably the most that that's the real human singularity there right is the ability of language to let us

758
00:57:28,160 --> 00:57:33,360
Learn and think about situations that we haven't directly experienced. I mean think about the tug of war for example

759
00:57:33,440 --> 00:57:35,280
I mean, I don't know if you're like me

760
00:57:35,360 --> 00:57:37,680
I've maybe done one or two tug of war games at my

761
00:57:38,320 --> 00:57:42,720
Most of my knowledge about tug of war or my beliefs at least don't come from my direct experience

762
00:57:43,040 --> 00:57:46,320
They come from things maybe people told me about analogous situations

763
00:57:46,720 --> 00:57:52,240
And and more generally many of our mental models and our intuitive theories come from what people tell us

764
00:57:52,400 --> 00:57:52,880
All right

765
00:57:52,880 --> 00:57:59,600
So the same approach that we talked about for updating beliefs from language can also be used to acquire new mental models

766
00:58:00,240 --> 00:58:03,920
But here what's going on is we're we're modeling how somebody might explain to you

767
00:58:04,320 --> 00:58:11,120
The way this tug of war works and in fact in our experiments with humans and in the in the toby gerstenberg and noah goodman and

768
00:58:12,160 --> 00:58:13,680
colleagues did

769
00:58:13,680 --> 00:58:15,680
This is exactly what we do would tell people

770
00:58:16,240 --> 00:58:20,000
Um about how this work people have various strengths it can vary from person to person

771
00:58:20,480 --> 00:58:22,800
And then now we're using the llm to translate

772
00:58:23,440 --> 00:58:28,560
Those say sentences in english that are generic sentences about the domain describing the world model

773
00:58:28,640 --> 00:58:30,640
We want our participants to use

774
00:58:31,040 --> 00:58:35,120
Into the same kind of probabilistic program code, but now these are defined statements

775
00:58:35,200 --> 00:58:40,960
So these are not these don't condition on a specific a specific world, but they define the general distribution on worlds

776
00:58:41,200 --> 00:58:43,200
But again, they're contextual they're distributional

777
00:58:43,440 --> 00:58:46,400
There could be different ways of understanding the vagueness in language

778
00:58:46,720 --> 00:58:51,680
But the basic idea is that we can describe a world model in english and then the code llm

779
00:58:52,480 --> 00:58:56,400
Can construct that world model. So all the defined statements now are constructed

780
00:58:56,480 --> 00:58:58,640
They're not exactly the ones that we use in the original paper

781
00:58:58,640 --> 00:59:02,160
But they have the same functional role and they can support the same kinds of inferences

782
00:59:02,560 --> 00:59:08,800
So it's just just showing you the way we can I think go towards one of the most interesting ways that language informs our thinking

783
00:59:09,120 --> 00:59:12,560
Not just as a way to convey specific beliefs about situations

784
00:59:12,560 --> 00:59:18,320
But new but new world models and in some recent most recent work that's still in progress with tyler brook wilson

785
00:59:19,040 --> 00:59:25,600
Katie collins and a number of the others I mentioned here tyler is a is a brilliant philosopher who recently graduated from MIT

786
00:59:26,240 --> 00:59:32,720
And he's actually just just accepted a faculty job at Yale. So he'll be at Yale in a in a year or so

787
00:59:34,080 --> 00:59:36,080
But together with with tyler and others

788
00:59:36,560 --> 00:59:39,200
And tyler's thesis goes into some early stages of this

789
00:59:39,680 --> 00:59:44,000
We've talked about we've been exploring the ways that the same approach can support

790
00:59:44,640 --> 00:59:48,480
Constructing new models, even if I don't explicitly tell you in language how the world works

791
00:59:48,480 --> 00:59:54,880
But just by again using your associative memory and marshaling implicit knowledge to construct a model of a new situation

792
00:59:55,280 --> 00:59:58,960
So I could tell you about a relay race and I'm not tell the relay race is like a new domain

793
00:59:58,960 --> 01:00:01,920
I'm not really telling you how races work. I'm just giving you some information

794
01:00:02,480 --> 01:00:04,880
But we're exploring ways in which the LLM can

795
01:00:05,520 --> 01:00:08,640
Can be queried to construct possible background knowledge

796
01:00:09,120 --> 01:00:13,040
And write probabilistic program code that can be suitable for reasoning about this domain

797
01:00:13,360 --> 01:00:14,800
Just on its own the LLM isn't enough

798
01:00:14,800 --> 01:00:18,240
But you have to do some reasoning about the models that it suggests

799
01:00:18,800 --> 01:00:20,880
And then those models when suitably

800
01:00:21,680 --> 01:00:27,280
Reason to put upon can support novel reasoning in this domain and even sensible updating

801
01:00:27,280 --> 01:00:31,600
So we're really getting you know, at least demos of steps towards

802
01:00:32,240 --> 01:00:37,040
Computational models that can capture the richness of how we are able to think about new situations

803
01:00:37,040 --> 01:00:42,960
Even ones that we haven't really thought about very much before or that we haven't been explicitly told how to think about

804
01:00:43,440 --> 01:00:45,440
but using the combination of

805
01:00:46,080 --> 01:00:52,640
Language the associative knowledge that's in it and an underlying ability for constructing probabilistic models of the world and updating

806
01:00:53,360 --> 01:00:56,480
I'm the last thing I just want to leave you with is a set of thoughts about

807
01:00:57,440 --> 01:00:59,440
You know, ridiculously

808
01:01:00,560 --> 01:01:02,400
It is it is the last thing

809
01:01:02,400 --> 01:01:06,160
But I think this will set up some of the discussion with with virginia and others too

810
01:01:06,880 --> 01:01:12,560
Which is many people in cognitive science, whether in linguistics or other areas

811
01:01:12,960 --> 01:01:19,520
Engaged in meaning have been interested in what you could call, you know a unified account of meaning and what we're trying to at least

812
01:01:19,920 --> 01:01:23,280
Point towards steps towards this with the framework. I talked about

813
01:01:24,480 --> 01:01:26,480
just to just to again

814
01:01:27,200 --> 01:01:29,360
Raise controversial points for discussion if you like

815
01:01:29,840 --> 01:01:35,840
um, but the idea that we can capture the meaning of a word in context as well as more generally as

816
01:01:36,400 --> 01:01:40,000
Effectively, it's like you might think of it as a form of dynamic semantics if you're familiar with this

817
01:01:40,080 --> 01:01:46,720
But the meaning being in in context in a discourse the incremental contribution to the probability distribution over

818
01:01:47,200 --> 01:01:51,360
PLOT expressions in the problem that we're thinking about with a problem in discussion

819
01:01:51,760 --> 01:01:55,600
And the meaning of a word or phrase or sentence or other unit of language in general

820
01:01:55,840 --> 01:02:02,400
Is a is a higher order stochastic function that can take as input a discourse context and return as output a meaning in context

821
01:02:03,040 --> 01:02:05,600
And the idea is that if we think about the different approaches

822
01:02:05,680 --> 01:02:06,880
These are just four

823
01:02:06,880 --> 01:02:13,280
You know traditional ways of thinking about meaning and language which all have great value and have often been seen as being competitors

824
01:02:13,760 --> 01:02:18,000
Ways in which we can really bring them together ways in which the PLOT ideas that we've talked about

825
01:02:18,320 --> 01:02:25,040
Can integrate the compositional logical aspects of meaning that formal semantics and other areas in the language of thought tradition have emphasized

826
01:02:25,520 --> 01:02:31,600
As well as in the context of probabilistic language of thought and mental models of the world can give a powerful form of grounding

827
01:02:31,680 --> 01:02:33,680
That's not grounded in sense data

828
01:02:33,760 --> 01:02:40,320
But in our models of the world that's what the PLOT does and the LLM or more generally a statistical distributional

829
01:02:40,720 --> 01:02:46,640
Sequence models can capture both the distributional statistical aspects of meaning. It's the base that the

830
01:02:47,680 --> 01:02:53,440
Both the sort of distributional usage approach, but also more general semantic association needed to make sense of language

831
01:02:53,440 --> 01:02:58,080
So flexibly as well as some of the very flexible pragmatic communicative ways we use language

832
01:02:58,800 --> 01:03:04,400
So I'll just leave it at that. Okay. Okay. Could you turn off your share, please? Yes

833
01:03:05,920 --> 01:03:07,920
First of all, thank you very much

834
01:03:12,240 --> 01:03:16,880
There's not a chance in the world that I'm going to forget Virginia this time and I'll tell you why

835
01:03:17,600 --> 01:03:24,800
Because I'm gobsmacked at how many different areas you're an expert in and so anyway, here's another one

836
01:03:25,200 --> 01:03:26,320
uh

837
01:03:26,320 --> 01:03:28,320
Virginia valium from

838
01:03:29,040 --> 01:03:30,960
CUNY hunter

839
01:03:30,960 --> 01:03:32,960
It's all yours. Thanks

840
01:03:33,520 --> 01:03:36,640
Josh that was such a great talk and so rich

841
01:03:37,520 --> 01:03:44,160
In 10 minutes, which is how much time you and I have will only scratch the surface

842
01:03:46,320 --> 01:03:49,360
So departing from the comments I sent you

843
01:03:50,640 --> 01:03:52,640
with respect to the

844
01:03:53,520 --> 01:03:56,320
Unified theory of meaning

845
01:03:58,400 --> 01:04:00,400
Many years ago

846
01:04:01,200 --> 01:04:03,200
Jerry Katz suggested that

847
01:04:04,000 --> 01:04:06,640
The question what is meaning could be

848
01:04:07,600 --> 01:04:15,600
Separated into questions like what is sameness of meaning? What is contradiction? What is anomaly?

849
01:04:16,800 --> 01:04:19,040
What is entailment and so on?

850
01:04:19,760 --> 01:04:25,200
And it occurs to me that it would be interesting to try this

851
01:04:26,080 --> 01:04:30,160
To try your model to see just how well

852
01:04:31,680 --> 01:04:33,680
It can detect

853
01:04:35,280 --> 01:04:39,440
Synonomy contradiction anomaly and so on

854
01:04:41,360 --> 01:04:46,640
Yeah, I think that's um, that's a great connection. I mean I I know a little bit about that

855
01:04:46,720 --> 01:04:50,400
But I you're inspiring me to go back and reread and learn much more about it

856
01:04:50,400 --> 01:04:52,960
I mean, I think just in the context of the last thing I said

857
01:04:53,440 --> 01:04:56,960
The idea exactly that in a sense in this framework at least

858
01:04:57,440 --> 01:05:03,520
What the statistical language model is doing is capturing the notion of sameness effectively because there the distribution on

859
01:05:03,920 --> 01:05:07,600
Code in the probabilistic language of thought if that distribution is similar

860
01:05:08,000 --> 01:05:10,400
And that can be measured in different ways then you might say well things

861
01:05:10,800 --> 01:05:17,600
Have the same meaning either in context or in general because and that that distribution function can be contextualized or it can be

862
01:05:18,000 --> 01:05:22,400
Made higher order. Okay, so that's really interesting, but it doesn't compute entailment or other

863
01:05:22,720 --> 01:05:28,320
You know a conceptual or inferential relations those come from reasoning in the probabilistic language of thought

864
01:05:28,880 --> 01:05:29,840
um

865
01:05:29,840 --> 01:05:34,400
And yeah, I mean I think that's that would be great to explore that more see if that can account for

866
01:05:34,960 --> 01:05:38,800
Unify it both account for the different set of phenomena and unify in that sense

867
01:05:39,680 --> 01:05:42,960
Yeah, I think that would be really interesting to explore

868
01:05:43,600 --> 01:05:45,760
um, so going back to

869
01:05:47,520 --> 01:05:49,520
Some of the more mundane

870
01:05:50,960 --> 01:05:52,960
Which I guess is upon

871
01:05:54,720 --> 01:05:59,200
Aspects um the way that I'm understanding what you've

872
01:06:00,320 --> 01:06:03,040
Said about how the theories

873
01:06:03,760 --> 01:06:09,120
Intersect that is how llm's intersect with basian models

874
01:06:09,760 --> 01:06:14,480
Is basically you're using the llm as a kind of tool

875
01:06:15,120 --> 01:06:20,640
To translate from one vocabulary to another vocabulary. Is that accurate?

876
01:06:21,520 --> 01:06:26,080
Yeah, I mean and that is at the most practical level. That's what we're doing. Um,

877
01:06:26,960 --> 01:06:28,480
yeah

878
01:06:28,480 --> 01:06:32,320
Okay, more about that, but let's let's go with that for now. Yes. Okay. Um

879
01:06:33,360 --> 01:06:35,200
so

880
01:06:35,200 --> 01:06:43,040
The critical difference that I see in what you're doing and what llm's are doing is the

881
01:06:43,600 --> 01:06:49,200
Well, I guess there are two critical differences, but for me the most important one is the use of symbols

882
01:06:49,920 --> 01:06:53,200
um, the other part is the basian

883
01:06:54,880 --> 01:06:58,320
Mechanism, but let's separate them. So

884
01:06:59,520 --> 01:07:01,520
When you put symbols in

885
01:07:02,240 --> 01:07:07,600
To me you're putting a lot of content into the mechanism. Do you agree with that?

886
01:07:09,200 --> 01:07:14,160
um, I agree that adding symbols adds a lot of content, but I think

887
01:07:15,280 --> 01:07:17,760
And I again, I didn't have time to unpack all of this very well

888
01:07:17,840 --> 01:07:24,080
I think but when you say you putting in symbols the in different different things that I'm talking about here

889
01:07:24,400 --> 01:07:29,600
The you and the putting in are different, but yeah, but whenever you add in symbols that adds a lot of

890
01:07:30,080 --> 01:07:36,960
Content and structure here. Yeah. Yeah content. Right. So I'm thinking about the implications of that for nativism

891
01:07:37,520 --> 01:07:41,200
So it seems to me. This is a nativist theory

892
01:07:44,480 --> 01:07:49,200
Um, I would say it is compatible with some forms of nativism that I find plausible

893
01:07:49,280 --> 01:07:51,600
although none I don't have a horse in that game, but I

894
01:07:52,480 --> 01:07:58,000
Through a lot of interactions with friends and colleagues like lisbelki susan carry many others

895
01:07:58,400 --> 01:07:59,680
Come to find

896
01:07:59,680 --> 01:08:04,160
Certain kinds certain aspects of nativism and conceptual nativism plausible especially in

897
01:08:04,480 --> 01:08:06,880
Certain core domains that are shared with other animals

898
01:08:07,360 --> 01:08:12,560
Like intuitive physics about objects that the world is three dimensional objects have some that there's some kind of

899
01:08:13,120 --> 01:08:14,640
physical interactions

900
01:08:14,640 --> 01:08:21,360
That are you know our bodies engage with and also some forms of intuitive psychology not necessarily higher order belief reasoning

901
01:08:21,680 --> 01:08:26,960
But the idea of efficient action and that agents have goals and they pursue actions in the physical world

902
01:08:27,040 --> 01:08:31,200
Grounded in physics to achieve their goals efficiently. Honestly, there's evidence

903
01:08:31,920 --> 01:08:37,200
Again, sherry lou who did this work in spelki's lab and continue to build on it has really, you know

904
01:08:37,600 --> 01:08:40,400
I mean, I was a striking evidence in three month olds

905
01:08:40,800 --> 01:08:46,080
But it's every experiment with three month olds is very small and you know a lot needs to be built on

906
01:08:46,400 --> 01:08:48,400
But as striking as it gets in three month olds

907
01:08:49,360 --> 01:08:50,400
science

908
01:08:50,400 --> 01:08:56,160
I'm showing that that aspects certain aspects of not only physics, but efficient goal directed action

909
01:08:56,240 --> 01:08:58,880
Understanding seem to be present that doesn't mean they're innate

910
01:08:59,120 --> 01:09:03,360
But at least they're not they're present way before language and probably build on some innate stuff

911
01:09:03,760 --> 01:09:05,760
It's compatible with that notion

912
01:09:14,560 --> 01:09:19,600
I'm not sure who that is but could you mute that was an accident. I think they just

913
01:09:20,000 --> 01:09:25,360
Should have turned off their okay, but so so yeah, there's this idea that certain kinds of symbols

914
01:09:25,840 --> 01:09:29,760
Are used in our framework to describe those some of those core systems

915
01:09:30,080 --> 01:09:35,040
But we are in contrast to like a fedoria nativism which says all concepts are innate

916
01:09:35,680 --> 01:09:40,640
In this in this framework here. I mean it or you could say in contrast, although some of uh,

917
01:09:41,200 --> 01:09:45,680
I don't know what jerry would have actually said about this but people like paul patrowski have suggested

918
01:09:46,080 --> 01:09:52,640
You know, maybe he would love it. I don't know but in in a rather different version of like some forms of radical

919
01:09:53,280 --> 01:09:57,840
Conceptual nativism most of the concepts in this framework are not innate

920
01:09:58,160 --> 01:10:00,160
They're written in a language of thought

921
01:10:00,240 --> 01:10:04,400
That it that could be somehow innate or somehow bootstrapped through natural language

922
01:10:04,480 --> 01:10:07,360
I find some of the ideas that that uh,

923
01:10:07,440 --> 01:10:13,760
Susan carrey and lis spelti in their different ways along with jesse snettaker have developed their ways in which language acquisition

924
01:10:14,240 --> 01:10:15,280
and

925
01:10:15,280 --> 01:10:22,000
Mental languages of thought might bootstrap each other and many things in the kind of glitman tradition also I think are reflect that idea

926
01:10:22,640 --> 01:10:25,520
But I so I I think we we are

927
01:10:26,080 --> 01:10:29,040
We still need to show this this is like the most interesting thing to do

928
01:10:29,360 --> 01:10:34,320
But to show ways in which the things I was showing at the very end could be could be used

929
01:10:34,800 --> 01:10:39,520
To explain how and model how natural language can start off

930
01:10:40,000 --> 01:10:48,320
Being grounded semantically and logically in a limited symbolic vocabulary of probable worlds that reflect core knowledge

931
01:10:48,560 --> 01:10:53,120
But then that can support bootstrapping and introducing new concepts via those mechanisms

932
01:10:53,200 --> 01:10:54,560
I was talking about at the end

933
01:10:54,560 --> 01:11:01,040
Including new concepts, but also new domain theories that we get explicitly or implicitly through our linguistic interaction with

934
01:11:01,360 --> 01:11:03,360
Other people who we think know more than us

935
01:11:04,480 --> 01:11:07,600
Okay, um, that sounds great. Um

936
01:11:08,320 --> 01:11:13,120
It also suggests to me that it is a highly modular

937
01:11:13,760 --> 01:11:16,800
System even though it's also probabilistic

938
01:11:17,360 --> 01:11:22,160
um, so it's probabilistic within each of these different modules

939
01:11:22,800 --> 01:11:29,280
And depending on what you think the symbols are that you start with you can

940
01:11:30,480 --> 01:11:34,080
Iterate what you think the modules of the mind are

941
01:11:35,760 --> 01:11:36,960
Yeah, no, I think that's right

942
01:11:36,960 --> 01:11:39,920
And I think you know, I think it's this framework is not a

943
01:11:40,480 --> 01:11:43,200
On its own a proposal for how our minds start

944
01:11:43,280 --> 01:11:47,040
But it can be used to instantiate and build and test some of them

945
01:11:47,680 --> 01:11:51,760
And so that's I think that's right. It's it does suggest that you could have, you know, different

946
01:11:52,800 --> 01:11:54,560
Sub-languages of thought for different domains

947
01:11:54,560 --> 01:12:00,000
But crucially the picture that you get with adults from this is both in some ways like strikingly

948
01:12:00,720 --> 01:12:03,360
Supermodular and in some other ways completely holistic

949
01:12:03,680 --> 01:12:10,160
So the striking super modularity is that the actual reasoning that you do in is in a discourse when I'm thinking about a situation

950
01:12:10,240 --> 01:12:13,840
And we're talking in a conversation is very modular if it might even be just

951
01:12:14,480 --> 01:12:18,880
Specific to this context this what Tyler Brook Wilson calls bespoke model construction

952
01:12:19,200 --> 01:12:23,920
This idea that we might construct a model on the fly to think about a particular situation

953
01:12:25,040 --> 01:12:27,600
That that we're that's the last thing that we've been working on there

954
01:12:27,920 --> 01:12:32,960
Is in some sense super modular because that model is is relatively small and all the inference. I'm doing is just here

955
01:12:32,960 --> 01:12:34,240
so I so I

956
01:12:34,240 --> 01:12:41,120
Side step or avoid the classic problems of what has made Bayesian inference intractable the idea that if I'm going to actually have

957
01:12:41,440 --> 01:12:44,400
A distribution over all possible worlds. I could think about it update that

958
01:12:44,880 --> 01:12:49,920
You know, that's completely intractable many people in the nativist tradition dan osherson and others have written about that

959
01:12:50,320 --> 01:12:54,480
And this this is a a way around that or as Tyler puts it in his thesis

960
01:12:54,480 --> 01:12:58,800
It's in a sense a kind of way to think about a solution certain kind of solution to the frame problem

961
01:12:59,120 --> 01:13:06,080
So it's super modular, but it's also very holistic in the sense that the world knowledge that's used to construct that

962
01:13:06,560 --> 01:13:10,080
Comes from like all the code you've ever written and all the semantic associations

963
01:13:10,160 --> 01:13:15,520
And it's like a gigantic holistic almost quinian web of language and code

964
01:13:15,680 --> 01:13:18,160
What we've sometimes called the github in the mind view

965
01:13:18,880 --> 01:13:21,920
There's the game engine in the head the github in the mind like github is this

966
01:13:22,240 --> 01:13:24,640
You know thing on the web which was crucial for training

967
01:13:25,200 --> 01:13:29,680
Language code models and you can imagine your own mind has lots of chunks of code

968
01:13:30,080 --> 01:13:34,720
Some of which are maybe innate many of which are not and natural language interweave with it

969
01:13:35,120 --> 01:13:42,080
And and having a content addressable associative memory that can use that and and marshal out from that

970
01:13:42,960 --> 01:13:44,240
relevant

971
01:13:44,240 --> 01:13:49,280
symbolic probabilistic models that can be used to reason about a particular situation in some sense that's extremely holistic

972
01:13:49,840 --> 01:13:56,560
And maybe is necessary to grapple with what is in you know very clearly some of the wholism of human cognition

973
01:13:59,360 --> 01:14:01,360
So going back a few steps

974
01:14:02,720 --> 01:14:05,280
The intuitive physics part

975
01:14:06,960 --> 01:14:08,960
In principle

976
01:14:09,520 --> 01:14:15,360
Animals could do that as well, right? Yes. So a chimp could have intuitive physics

977
01:14:16,320 --> 01:14:19,520
Yeah, and joseph call and others have studied that a number of other

978
01:14:20,080 --> 01:14:24,320
Amanda ced and other brothers had studied other non-human primates and we're actually collaborating

979
01:14:24,720 --> 01:14:26,880
With erica cart mill and some others studying

980
01:14:27,520 --> 01:14:31,360
non-human primates on intuitive physics and I know colleagues

981
01:14:31,840 --> 01:14:34,880
Who are studying analogous kinds of intuitive physics in rats?

982
01:14:36,080 --> 01:14:38,880
And I think even simpler organisms

983
01:14:39,920 --> 01:14:44,160
Okay, so the difference between humans and

984
01:14:45,120 --> 01:14:50,560
Other animals is going to be that language allows you to go further

985
01:14:51,200 --> 01:14:54,240
Language allows you to go beyond intuitive physics

986
01:14:54,640 --> 01:14:59,680
Whereas nothing is going to help these other animals go beyond intuitive physics

987
01:15:00,640 --> 01:15:02,400
Yeah, I I think that's right. I mean again

988
01:15:02,400 --> 01:15:06,160
I wouldn't say that all the all the mental models of other animals is just intuitive physics

989
01:15:06,160 --> 01:15:11,440
They also have models of their social world like you know the chainy and save farce baboon metaphysics is mostly a social theory

990
01:15:11,920 --> 01:15:13,920
But yeah, but I would say

991
01:15:14,480 --> 01:15:18,880
Definitely that the key thing here and it's the real human singularity. I would say

992
01:15:19,680 --> 01:15:25,040
Is that language allows us to construct to both enrich our intuitive physics to think about

993
01:15:25,440 --> 01:15:29,280
Aspects of the physical world that are not initially intuitive to us and maybe still aren't

994
01:15:29,600 --> 01:15:34,160
But also to construct things that just go totally beyond any any core domain that evolution gave us

995
01:15:34,880 --> 01:15:35,920
Right

996
01:15:35,920 --> 01:15:40,480
Okay, so one question that this brought up to me

997
01:15:41,280 --> 01:15:47,520
Because what you're talking about are all the ways that humans are so good at what they do

998
01:15:48,240 --> 01:15:49,120
um

999
01:15:49,120 --> 01:15:53,120
But then there are ways that they're not so good at what they do

1000
01:15:53,680 --> 01:15:57,760
And far transfer is one of those examples

1001
01:15:58,480 --> 01:15:59,760
um

1002
01:15:59,760 --> 01:16:05,280
And there there are some things that are hard for people to learn

1003
01:16:06,000 --> 01:16:10,080
Or some places where it's hard to get from

1004
01:16:11,200 --> 01:16:13,200
A to z

1005
01:16:13,840 --> 01:16:16,400
Even though you think you've learned a

1006
01:16:17,120 --> 01:16:21,680
So there's a famous little story about someone going to their logic teacher

1007
01:16:22,480 --> 01:16:24,480
asking about

1008
01:16:24,560 --> 01:16:29,600
Understanding if a then b and the logic teacher spends a lot of time on that

1009
01:16:30,160 --> 01:16:32,160
And the student says

1010
01:16:32,480 --> 01:16:34,480
Okay, I think I get it

1011
01:16:34,880 --> 01:16:37,440
But we try it with r and s now

1012
01:16:38,240 --> 01:16:39,600
um

1013
01:16:39,600 --> 01:16:45,040
So that's something that just shouldn't occur, but it does occur

1014
01:16:45,760 --> 01:16:48,880
Uh, even with smart people

1015
01:16:49,840 --> 01:16:54,400
So like when you try to teach people about experimental design

1016
01:16:55,040 --> 01:16:58,480
It's hard for them to see confounds often

1017
01:16:59,040 --> 01:17:01,440
um, it's hard for them to

1018
01:17:02,400 --> 01:17:06,720
Get at what's wrong with some experimental design

1019
01:17:07,520 --> 01:17:13,280
And it's not that they don't have a general intelligence. It's not that they don't have some principles

1020
01:17:13,840 --> 01:17:15,840
but it's as if

1021
01:17:15,840 --> 01:17:18,080
What they've learned is just

1022
01:17:18,800 --> 01:17:27,040
Too far away as far as the the string of examples goes for them to get to to the next one

1023
01:17:27,760 --> 01:17:30,320
So I'm wondering how

1024
01:17:31,200 --> 01:17:32,960
On your system

1025
01:17:32,960 --> 01:17:36,720
those kinds of limitations would be modeled

1026
01:17:37,520 --> 01:17:43,600
Yeah, no, that's a that's a great point a great question and a great pointer. I think to work that

1027
01:17:44,240 --> 01:17:50,240
We could and should do more of so the the stuff I talked about at the very end, which is again this the work with

1028
01:17:50,880 --> 01:17:53,040
Tyler and uh, katie collins and

1029
01:17:53,760 --> 01:17:56,560
Lansing and and and very much with leo wang also

1030
01:17:57,280 --> 01:17:58,720
um

1031
01:17:58,720 --> 01:17:59,840
is

1032
01:17:59,840 --> 01:18:02,720
What we're what we're getting out there like is is again how we can

1033
01:18:03,600 --> 01:18:09,600
Describe a situation in language, maybe even very implicitly just start talking and then see if

1034
01:18:10,320 --> 01:18:16,000
See if this architecture can be used to construct a mental model needed to reason about that situation

1035
01:18:16,000 --> 01:18:18,000
And you could also make it better

1036
01:18:18,160 --> 01:18:23,920
More robust if I explicitly give you instructions as in the educational context you're talking about if I try to explain to someone

1037
01:18:24,240 --> 01:18:30,640
Logic or experimental design actually another collaborator on that project said zhong is actually very interested for his thesis

1038
01:18:31,040 --> 01:18:33,840
In how we learn like logic and learn to reason through language

1039
01:18:34,720 --> 01:18:38,880
But so we have you know, our framework is providing possibly a way to do that

1040
01:18:39,440 --> 01:18:40,400
but

1041
01:18:40,400 --> 01:18:42,960
In order for it to work at least in the current system

1042
01:18:43,520 --> 01:18:48,880
We do the kind of thing that lm folks are generally doing these days, which is some kind of few shot prompting

1043
01:18:49,200 --> 01:18:55,360
We prompt the system with examples of other mental models and language for describing them in

1044
01:18:55,920 --> 01:19:01,040
Related domains they could be similar or they could be further and what we already can see right is i mean

1045
01:19:01,600 --> 01:19:04,080
The interesting thing is can you generalize to new domains?

1046
01:19:04,240 --> 01:19:06,240
So our system is somewhat able to do that

1047
01:19:06,640 --> 01:19:11,280
But you know, this is a place where near or far transfer would be relevant at least so far

1048
01:19:11,280 --> 01:19:15,360
you know as you might expect you have to have some domain that's at least reasonably close

1049
01:19:16,080 --> 01:19:21,440
And for example, we're trying we could transfer from like a tug of war to a relay race or to some other sport setting

1050
01:19:22,160 --> 01:19:24,800
Or from a couple of sport settings to yet some new sport

1051
01:19:25,280 --> 01:19:25,680
um

1052
01:19:25,680 --> 01:19:30,560
Or you might transfer from a sport setting to like a math competition or to some other kind of thing

1053
01:19:30,560 --> 01:19:34,080
So if it really interesting ideas about abstraction and metaphor

1054
01:19:34,960 --> 01:19:39,520
Analogy some of the kinds of things that the the transfer literature has studied are going to be relevant here

1055
01:19:39,520 --> 01:19:44,320
We're just beginning but what you're pointing to is actually some set of phenomena and things we should really

1056
01:19:44,800 --> 01:19:47,760
Engage with and I expect you know at least based on our current system

1057
01:19:48,320 --> 01:19:50,720
It will definitely struggle sometimes with far transfer

1058
01:19:51,280 --> 01:19:56,480
Um, which be which might be because at that level, you know, I I I drew this contrast at the very beginning

1059
01:19:57,200 --> 01:19:59,440
between the pattern recognition approach

1060
01:19:59,920 --> 01:20:02,240
Data driven learning and the mental models approach

1061
01:20:02,480 --> 01:20:06,720
But at that level we are doing a kind of pattern recognition. It's just not patterns in the world

1062
01:20:06,800 --> 01:20:10,560
It's patterns and thoughts and thought structures and ways of expressing them

1063
01:20:11,040 --> 01:20:14,640
And you know at that point if if if that kind of really interesting

1064
01:20:14,960 --> 01:20:20,160
But much more abstract kind of pattern recognition is what's driving your ability to construct new models of new situations

1065
01:20:20,640 --> 01:20:25,360
Or at least initially before you've had formal instruction or when you're just at the beginning of formal instruction

1066
01:20:25,760 --> 01:20:32,320
Then we should expect that it to have it to have the same kinds of fragility with far transfer that any machine learning approach does

1067
01:20:32,400 --> 01:20:35,920
Okay, sorry to be a gilljoy here. We have about 10 minutes left

1068
01:20:35,920 --> 01:20:39,840
But we have at least four people that want to ask questions. So what I want to suggest

1069
01:20:40,400 --> 01:20:43,360
to alina and julia and

1070
01:20:44,320 --> 01:20:45,520
also

1071
01:20:45,520 --> 01:20:52,400
Uh leijin and stephan carlson is to raise your hand again, and I'll recognize you in the order that you raise your hand

1072
01:20:53,440 --> 01:20:55,440
Okay, julia go ahead

1073
01:20:55,600 --> 01:20:57,600
well, um, so I was wondering

1074
01:20:57,600 --> 01:21:08,320
I was thinking like maybe you can use the um the LLM translator plus the probabilistic models that system to um

1075
01:21:09,280 --> 01:21:16,720
Model how the beliefs of a reader would change over the course of being told a story or like reading a book

1076
01:21:17,840 --> 01:21:20,320
And then you could look across

1077
01:21:21,280 --> 01:21:25,520
The genres or books or whatever to kind of get a sense of

1078
01:21:26,000 --> 01:21:32,320
What the kind of model of those stories being told since the stories are themselves a world model what that would be

1079
01:21:32,960 --> 01:21:34,960
and that you could incorporate

1080
01:21:36,160 --> 01:21:40,560
Because if you have a guess because something that you didn't like touch on in

1081
01:21:41,200 --> 01:21:44,640
You know in your act talk, which I'm sure you didn't have time

1082
01:21:44,720 --> 01:21:51,120
But was that like if you have a guess about where you are in the trajectory of the story you expect

1083
01:21:51,200 --> 01:21:53,040
Then you have a guess about

1084
01:21:53,040 --> 01:21:55,040
About what you think's going to happen next

1085
01:21:55,600 --> 01:22:00,240
Wrap it up because we um, so I was just wondering like do you do yeah, that's a great question

1086
01:22:00,800 --> 01:22:02,480
Um, let me just try to answer it really quickly yet

1087
01:22:02,880 --> 01:22:06,880
Leo wong would love that question because in addition to being a great cognitive scientist

1088
01:22:06,880 --> 01:22:09,280
They are also a writer they write stories and even novels

1089
01:22:09,680 --> 01:22:15,600
And one of the things that we've been working on is little mini like three act structures and things like that that

1090
01:22:16,400 --> 01:22:18,400
Follow either classic narrative structures or other

1091
01:22:19,040 --> 01:22:23,920
Things and and you know, maybe not surprisingly those are places where we see a big gap between

1092
01:22:24,640 --> 01:22:26,640
human story understanding and

1093
01:22:27,120 --> 01:22:33,040
LLMs even the state of the art ones, but exactly we're trying to use these models to capture how that kind of

1094
01:22:33,840 --> 01:22:36,960
You know journey of understanding might unfold and also even how

1095
01:22:37,680 --> 01:22:41,840
A creator might create it. So another student that we've worked with Karthik Chandra has

1096
01:22:42,640 --> 01:22:48,640
Had had some work at last year's cog side conference on storytelling as inverse inverse planning and the idea that like if somebody's

1097
01:22:49,120 --> 01:22:53,360
Understanding another character's journey by doing some inverse planning as in those theory of my models

1098
01:22:53,600 --> 01:22:57,520
Then a storyteller can try to invert that inverse planner to to convey

1099
01:22:58,000 --> 01:22:59,280
the emotional

1100
01:22:59,280 --> 01:23:05,520
Or mental journey that the character has and that's a way to use this toolkit for both story creation as well as story understanding

1101
01:23:05,520 --> 01:23:10,720
So again, it's those are mostly promissory notes, but great question and future research

1102
01:23:10,720 --> 01:23:15,200
I think from Karthik and leo and others will address it. Thank you. Thanks

1103
01:23:16,000 --> 01:23:19,360
Hi, I'm gonna ask a question in person josh. This is ever

1104
01:23:19,920 --> 01:23:24,240
Hi, I yeah go for it. I I really liked your talk. By the way, I watched it on video. Yeah

1105
01:23:24,720 --> 01:23:26,240
Cool, great. Okay. So yeah

1106
01:23:26,240 --> 01:23:32,160
I was gonna so following the work that you were showing where you were basically based also on what you were saying and then in the

1107
01:23:32,960 --> 01:23:38,720
Question period where you were, you know, few shot prompting these language models to produce probabilistic language

1108
01:23:41,280 --> 01:23:44,480
World models, I guess I have my I have sort of a more medic question

1109
01:23:44,480 --> 01:23:50,560
Which is do you see that as just a way to sort of help step by step reasoning in these models?

1110
01:23:50,640 --> 01:23:53,040
Or do you think this is sort of evidence for

1111
01:23:53,760 --> 01:23:57,600
Early sort of possible world modeling being learned in these models

1112
01:24:00,080 --> 01:24:02,080
Um

1113
01:24:03,120 --> 01:24:08,640
I'm not sure if I see it as either of those. Um, I I mean, uh, uh, so

1114
01:24:10,720 --> 01:24:14,640
I don't think of it as either doing step step by step reasoning in language models or

1115
01:24:15,760 --> 01:24:21,440
Evidence that they do possible world modeling. It might be relate. I mean, there is you know, I've obviously as you know

1116
01:24:21,760 --> 01:24:24,080
You talked about some and others have, you know, there's certainly

1117
01:24:24,880 --> 01:24:29,440
Um, a lot of evidence that llms if you try to just use them as end-to-end reasoning systems

1118
01:24:29,440 --> 01:24:34,000
They can benefit from step by step by reasoning and there might be some emergent world modeling capacities

1119
01:24:34,000 --> 01:24:39,680
But you know, again, I look at that and I see a really interesting and mixed pattern of successes and failures

1120
01:24:40,000 --> 01:24:45,520
And depending on who's writing the paper and what their agendas are you can highlight the successes or you can highlight the failures and the gaps

1121
01:24:46,000 --> 01:24:47,120
um

1122
01:24:47,120 --> 01:24:52,800
The to me that an objective perspective is that it's patchy and fragile although extremely impressive and really interesting

1123
01:24:53,040 --> 01:24:56,560
So the way I think of what we're trying to do is to say, yeah, there's there's you know

1124
01:24:57,280 --> 01:25:03,200
Various kinds of interesting approximate implicit knowledge that those models have that can be used sometimes to succeed

1125
01:25:03,680 --> 01:25:06,880
In reasoning complex sequences of reasoning or world modeling

1126
01:25:07,200 --> 01:25:12,560
But I think a more robust way to use it is the way we're using it a more both a more human like and a more robust for ai

1127
01:25:12,720 --> 01:25:14,720
way to use it which is to

1128
01:25:14,880 --> 01:25:18,400
Which is to condition and construct these bespoke world models

1129
01:25:19,120 --> 01:25:21,120
that you know where the

1130
01:25:21,680 --> 01:25:24,960
long chains of sequential reasoning or actual

1131
01:25:25,920 --> 01:25:29,200
You know coherent world modeling are there

1132
01:25:30,240 --> 01:25:31,840
By construction

1133
01:25:31,840 --> 01:25:36,960
But you know, there are other limits like again as as I was saying, you know Bayesian inference in really complex models

1134
01:25:37,760 --> 01:25:40,320
Is very difficult and I don't think people do it. There's a lot of evidence

1135
01:25:40,320 --> 01:25:41,120
They don't do it

1136
01:25:41,120 --> 01:25:44,560
But they seem to do it very generally in in just the right small model

1137
01:25:44,640 --> 01:25:49,440
At least in cases where they have the relevant world knowledge to construct those models and when they don't they don't

1138
01:25:50,000 --> 01:25:51,600
Right, so that's all you know

1139
01:25:51,600 --> 01:25:54,480
Our minds have mixed patterns of successes and failures there too

1140
01:25:54,720 --> 01:25:59,760
But I think this toolkit is better matched to the mixed patterns of successes and failures, which is

1141
01:26:00,480 --> 01:26:05,680
What what i'm trying to get out if that makes sense the the the one that we see in humans as opposed to the the weird head scratching

1142
01:26:05,760 --> 01:26:12,080
Like super intelligent cases in some places and then super dumb Jesus and others that you just see in a pure sequence model

1143
01:26:13,440 --> 01:26:17,040
I'm told Alina can go next. Thanks for your answer. Yeah, thanks

1144
01:26:18,000 --> 01:26:20,000
Good question

1145
01:26:21,360 --> 01:26:23,360
Alina pick it up or you lose your turn

1146
01:26:25,680 --> 01:26:32,720
Yes, um, hello, uh, thank you for insightful talk. Um, Josh so

1147
01:26:33,680 --> 01:26:42,880
You said that um, and correct me if I'm misheard that language allows us to construct meaning that it's totally out of bounds of evolutionary gain

1148
01:26:43,520 --> 01:26:45,200
um

1149
01:26:45,200 --> 01:26:50,640
Is if uh, if this is what you said, I think this is uh, super profound. So

1150
01:26:54,560 --> 01:27:01,840
Yeah, I I'm sure I'm not the only person who said that many people sure. Okay. It's just a reminder. I guess profound truth. Yes

1151
01:27:02,640 --> 01:27:04,640
Okay, uh, so the actual question

1152
01:27:04,720 --> 01:27:10,000
So, um, you mentioned that the meaning on the world is constructed contextually and incrementally

1153
01:27:10,400 --> 01:27:16,000
So could you please elaborate on how this process occurs and how it impacts our understanding of complex concepts?

1154
01:27:16,080 --> 01:27:22,480
So so we transition between the um from incremental construction of the meaning to generalizations

1155
01:27:24,160 --> 01:27:26,160
Right. So the way it works in our

1156
01:27:26,400 --> 01:27:29,280
I can just I can tell you how it works in the models that we've built so far

1157
01:27:29,360 --> 01:27:32,160
And again, this is not to say that it's exactly like this in the mind

1158
01:27:32,240 --> 01:27:35,600
But I think it might be something like this and a lot more work needs to be done

1159
01:27:35,920 --> 01:27:40,080
But the way it works in our models is something kind of like what you're familiar in a chat

1160
01:27:40,160 --> 01:27:42,640
Like if you've used a chat gbt or other

1161
01:27:43,280 --> 01:27:45,040
conversational ai systems

1162
01:27:45,040 --> 01:27:47,440
The way it's working is that basically there

1163
01:27:48,000 --> 01:27:52,240
You know, if you're used to using chat gbt you type something it types something back and in the middle

1164
01:27:52,480 --> 01:27:56,880
Some wheels turn and then you type something more in it wheels turn in the black box and it types more stuff

1165
01:27:56,960 --> 01:27:58,960
Okay, so it's it's basically like that

1166
01:27:59,360 --> 01:28:04,480
It in that you at each each sentence is translated into some

1167
01:28:05,120 --> 01:28:12,240
Expression in the language of thought roughly at the sentence level. I think sentences are real units of meaning like I I am very

1168
01:28:13,200 --> 01:28:15,920
respectful and admiring of many insights from language

1169
01:28:16,160 --> 01:28:20,880
Including that words are real and sentences are real and there's real syntactic structure to be understood there

1170
01:28:21,120 --> 01:28:24,320
That is only being approximated in some ways by these models

1171
01:28:24,320 --> 01:28:29,680
But the key is that yes, there's a process the contextual process is like at the discourse level

1172
01:28:30,000 --> 01:28:32,800
You go sentence code sentence code sentence code

1173
01:28:33,200 --> 01:28:37,120
And each sentence to code translation is conditioned on the previous

1174
01:28:37,820 --> 01:28:42,000
Conversational history or the discourse that that you have been interpreting. Okay

1175
01:28:42,640 --> 01:28:44,240
That's just a first approximation

1176
01:28:44,240 --> 01:28:49,600
Just another thing you might want to do and surely you'll have to do is go back and edit previous code because if you realize

1177
01:28:49,680 --> 01:28:55,040
Oh, I misunderstood something. I can't just add new code. I have to go back and edit the code that I wrote before

1178
01:28:55,120 --> 01:28:57,120
Which is also something that code llms can do

1179
01:28:57,680 --> 01:28:59,360
I'm not saying they'll do it right

1180
01:28:59,360 --> 01:29:03,600
But there's other processes that involve like checking and refining and fixing models

1181
01:29:04,160 --> 01:29:05,120
That are written in this way

1182
01:29:05,120 --> 01:29:09,760
But maybe that gives you some sense of the contextual dynamics of how language is understood in this model

1183
01:29:11,440 --> 01:29:13,440
Last question lay jim

1184
01:29:15,600 --> 01:29:17,600
Hi

1185
01:29:18,640 --> 01:29:22,080
Joshua, thank you for for the presentation. It's very interesting

1186
01:29:22,640 --> 01:29:27,520
Um, I have an engineering background, but I'm not an expert in

1187
01:29:28,400 --> 01:29:30,400
artificial intelligence

1188
01:29:30,400 --> 01:29:32,400
Uh, I have a few questions

1189
01:29:32,880 --> 01:29:33,920
um

1190
01:29:33,920 --> 01:29:36,640
From your presentation, I understand that

1191
01:29:38,240 --> 01:29:40,240
actually llms

1192
01:29:40,240 --> 01:29:42,240
the

1193
01:29:42,320 --> 01:29:47,360
The the it's it's kind of a gold mine of human knowledge

1194
01:29:47,760 --> 01:29:54,480
And uh, you seem to say that they have a modelized the real world facts and the logics are quite correctly

1195
01:29:56,400 --> 01:30:04,080
If you ask the question directly in human language, uh, currently they don't seem to to give the good answer

1196
01:30:04,720 --> 01:30:07,600
because some with some lack of

1197
01:30:08,480 --> 01:30:11,600
algorithm, but if you first translate the

1198
01:30:12,480 --> 01:30:14,480
human questions into

1199
01:30:15,200 --> 01:30:17,200
functional code and like

1200
01:30:17,600 --> 01:30:20,800
The work you are doing and then they are capable of um

1201
01:30:21,680 --> 01:30:26,080
I know we execute the code then we we are capable of having

1202
01:30:26,640 --> 01:30:32,800
Result results that are quite close to human behavior. Uh, so is that the correct?

1203
01:30:34,400 --> 01:30:36,560
Yeah, that's I mean, that's more or less correct

1204
01:30:36,640 --> 01:30:40,560
But I don't want to I mean and that's that's what we've been doing in these examples

1205
01:30:40,560 --> 01:30:42,560
And what I've been showing but I don't want to

1206
01:30:43,040 --> 01:30:44,560
Claim I don't and I don't think it's true

1207
01:30:44,640 --> 01:30:48,800
But certainly I don't have the evidence for it being true or how true it is that

1208
01:30:50,400 --> 01:30:53,120
You know llms like always do this, right? There's a lot of

1209
01:30:54,320 --> 01:30:58,640
Ways in which language is only an imperfect reflection of the ways that we think

1210
01:30:58,880 --> 01:31:05,200
There's certain things that again are partly based on evolutionarily ancient core systems. The language is not very good at expressing

1211
01:31:06,240 --> 01:31:10,000
And that includes also spatial reasoning and you can see ways in which

1212
01:31:10,640 --> 01:31:14,720
Even the best language models break down here and language multimodal language vision models

1213
01:31:15,680 --> 01:31:18,560
Have have had persistent problems always from the beginning there

1214
01:31:19,200 --> 01:31:23,520
So I think you know, I don't want to suggest like I think yes language is a treasure trove

1215
01:31:23,920 --> 01:31:30,400
Of knowledge explicit and implicit about the world, but it's and for us it's it's such a valuable resource as human beings

1216
01:31:30,480 --> 01:31:38,400
Okay, that's why it's no accident that language models when they're trained to capture patterns on all the language that humanity's basically ever produced and put out there on the web

1217
01:31:39,120 --> 01:31:42,960
Uh, you know start to have remarkable properties at the same time

1218
01:31:43,600 --> 01:31:49,440
It's only some parts of our knowledge about the world is key parts to actually understanding and being in the world

1219
01:31:50,000 --> 01:31:52,240
That basically nobody ever talks about

1220
01:31:54,480 --> 01:32:01,360
And even if they do talk about it they talk about it very incompletely and imperfectly and yet our brains are designed to understand in those terms

1221
01:32:01,760 --> 01:32:08,320
So I I don't want to convey the idea that somehow it's all there in language and it's and that's all you need

1222
01:32:08,400 --> 01:32:10,400
by not by any means

1223
01:32:11,280 --> 01:32:14,880
Wonderful before we, uh, let people applaud

1224
01:32:15,680 --> 01:32:21,840
You know, there's some several panels coming. Are there any of them that you can join? They're not today, unfortunately

1225
01:32:21,920 --> 01:32:28,640
Yeah, um, I I need to check my calendar and coordinate on some family things and I will I'll I'll try very hard to join one of them

1226
01:32:28,800 --> 01:32:30,800
Okay, now we'll applaud you for

1227
01:32:31,760 --> 01:32:33,760
Thank you

1228
01:32:33,920 --> 01:32:44,080
Thanks, and and thanks so much for the discussion Virginia and all the all the questions. There were great questions. So I hope to engage more

1229
01:32:44,320 --> 01:32:45,360
um

1230
01:32:45,360 --> 01:32:48,960
in one of the panels and um in person if if anyone hears

