start	end	text
0	3600	I forget now whether you do have a discussant or not. Yeah, Virginia.
4160	6160	Oh, it's Virginia's gonna. Okay, great. So
7040	10160	She's here. Yeah, she's there. Hi, Jenny. I've been Virginia. I mean
24720	25920	Have you
26000	33040	Josh, you haven't seen any sessions at all yet, right? Um, I've seen I haven't been able to join live, but I watched video from the first two days
33680	35680	Did you watch belkin?
35680	41520	I did. Yeah. Okay, great. Okay. I was even gonna refer to it briefly. Yeah. Okay. That's why I don't want this to explain. Yeah
41840	43840	Um
54720	56960	I'm I'm also trying to see if there's some time
57200	62240	I know you said that there might I might be able to join I can't join the discuss or there is no discussion today, but um
62800	67120	I'm trying to see if there's any one of the other panels that my schedule would be great. I can add
68240	70240	Yeah, I'll I'll uh
70480	72960	I'll I'll be in touch about that. Okay. Let me just
74960	76960	Solid duty here
93200	97680	Welcome everybody to the afternoon session everybody out in in the
98640	102640	Distributed land and everybody here. I'd like to um
103200	110560	Introduce Josh Stendenbaum who's professor of computational cognitive science in the department of brain and cognitive science
111280	112880	at mit
112880	120720	He's a principal investigator at the computer science and artificial intelligence lab called sea sail. Is that former sail?
122000	124880	No, uh sail is a stem from sail
125840	130800	But it's formerly the AI lab and the lcs laboratory for computer science
131200	136640	Great. Yeah, and a thrust leader in the center for brains minds and machines cbn
136640	144240	And his papers are on perception learning common sensory reasoning in humans and machines with the twin goals of better
144640	151360	Understanding human intelligence in computational terms and building more human like intelligence in machines
152000	154640	In other words, he's cut out perfectly for this
156080	159200	Summer school. I hand it over now to Josh Stendenbaum
160160	163520	Okay, great. Thank you so much, Steven for organizing for inviting me
165120	171600	You know, I I got to watch much of the first two days and it was really interesting to see a back and forth between people who are
172160	174640	Extremely impressed with large language models
175120	180240	Both on their language abilities and maybe some of their general thinking abilities and other people who are much more skeptical
180880	184080	That they have really anything to do with intelligence at least of the human form
184720	189200	The work I'm going to talk about here is a is an interesting I think mix of those two perspectives
190720	192240	And I hope that will
192240	194240	Be useful stimulating
194560	198800	Engage in some interesting discussion both now and going forward over the next two weeks
200000	204000	The the heart of the talk, which will really be more like the second half is based on this paper
204000	207200	Which you can find on archive from word models to world models
207600	211200	Understanding natural language by translating into a probabilistic language of thought
212080	220080	But I'm going to spend the first half setting some context on just how we think about thinking and then it's built on that for the relationship between language and thought
221280	225120	This is you know, I would say this is both an AI talk and a cognitive science talk
225200	228480	Although most fundamentally to me. I'm really interested in the computational structure
229280	231200	of the human mind
231200	232640	So just in that
232640	234640	hear it in that lens
234640	237280	But I think also just sort of very generally
238000	241040	You know, nobody can fail to be surprised and impressed
241600	244640	At what has happened with the most recent machine learning models
245680	250000	And at the same time they're they're very puzzling and confounding in certain ways
250000	252560	So I hope to try to be able to resolve some of that or at least
252960	254960	Point the way towards some of that
255360	258320	This paper the work i'm going to talk about is joint with a number of people
258640	264480	I want to single out two people Lionel long and Gabe grand who are the joint first authors of that paper
264640	266640	I'm putting Lionel a little bit bigger
267200	269840	In part because a lot more of my slides are taken from lines
270240	275920	So I would say all of the credit for the good stuff both on the research and in the slides to Lionel and Gabe
277360	282320	And you know anything that doesn't quite work or make sense is probably just me garbling things
282960	284000	Okay
284000	287120	But a lot of other people contribute to the work that i'm going to talk about here
288000	292400	So again, you know, we can't who cannot be impressed with the advances of AI
292640	296320	Whether it's in perception and and you know robots deployed in the real world
296320	302640	Like the self-driving car systems of tesla and wemo or most recently conversational AI systems like chat gpt
303840	306240	Clearly there's something really interesting and important happening
306400	308560	But at the same time something quite puzzling now
308560	311120	There's a number of puzzles that people here have talked about just one
311120	313920	I'll point to is what you might call the puzzle of confabulation
315280	317520	Which points to what ways I think that the kinds of
318480	325280	What intelligence or unquote intelligence that we see in these systems is different in nature and origins from that in our own minds
325760	329520	So think about for example in the context of driving. This is a video from tesla
330160	336800	A self-driving system where it's it's faced with something a little bit out of its training set a horse drawn carriage and and
337840	342320	Buggy and it interprets it as a truck first facing one way the other then it's
343440	345680	SUV then it's a truck with the person behind it
346640	348640	Now the person's gone away
348720	350720	Shortly the person will come back
351520	353680	You know, they can't figure out which way the truck is facing
354320	359280	If a person were having this experience on the road, you would think they should stop driving and get off the road as soon as possible
359920	361920	Right. There's something fundamentally
362560	364560	Detached about their mind from reality here
365600	371200	And when this happens in systems where lives are at stake, it can be deadly. So this is a serious worry
371760	373040	technologically and
373120	379760	Sightly now when it comes to computer vision and tesla self-driving a lot of progress has been made
381680	384880	What I showed you what i'm showing you here is from a few years ago
385680	389360	Um, but you still see similar kinds of things and also on the language side
389360	394560	I'm just going to show one example of analogous kinds of things that really make you scratch your head
394960	400960	On the you you could find these in language models, but i'm going to talk about multimodal language and vision models because this is where
401280	404400	You know, there's a lot of really interesting stuff going on
405200	408080	Where machine learning and especially like grounded language
408480	411360	Potentially might meet up with what goes on in cognition
411920	415200	um, so this is a i'm just going to show you a couple of little demos from
416800	422880	Google's large language model multimodal language model. Um, gemini both earlier versions and then the most recent version
423520	429360	This is the figure i'm showing here, which i'm giving to gemini and I should say you can see similar things with chat gpt
429360	432480	But it's a little bit funnier with the google ones. I'm going to focus on that
432960	437680	Um, this is an image from my colleague vikash mansinga whose work will also figure in the talk
438240	441120	Um, vikash is one of the world's experts in probabilistic programming
441280	445360	He and I are very interested in how our minds and our machines could model uncertainty
445920	448880	So here's an image that that I think all of us when we look at this
448960	452480	It's it's it's pretty clear that though. It's a little hard to read kind of messy
452640	455920	It's really clear that there's a word here and that the word is uncertainty
456480	460240	Um, but what is uh an lm a multimodal lmc from this?
460640	465040	Well, it might see this the image you sent me shows a black and white image of a text box
465120	471520	The text book has a dark gray border rounded corners. Good inside is the word dream written in a bold sans serif font
472240	475440	Is there anything else I can help you with? I ask how many letters are shown in the text box?
475680	479680	There are five letters shown in the text box the letters are d r e a and m
481120	483120	What is going on with this?
483120	485840	Now we could say all right. Well, at least it's pretty good
485920	490560	I mean at least it's recognizing that there's text and it's you know, it could be worse
491200	496400	But it seems to be strangely self-confident strangely confident about an incorrect answer
496640	500720	But you could say at least it's not like seeing a person or person walking down the street
501760	508400	But consider this small change to the input image now our lm says this shows a black and white pixelated image of a person
508640	513120	The person is facing forward and appears to be standing still. Here are some details about the person
513440	517520	They have short hair. They're wearing a shirt with a collar. Their legs are slightly wider than their torso
518080	519360	Since the image is pixelated
519360	523200	It's difficult to discern other details such as facial features or specific articles of clothing
523600	527120	Now again, the thing that I think should make us puzzle here is
527920	533360	Is not just that the system gets this wrong, but that it seems to confidently assert things that are so
534400	538000	detached from our sense of of the reality of what we're seeing
538640	541440	And where just a small change can make such a huge difference
542080	548720	Just to update things this was this was from the first version of google's gemini the most recent version of gemini 1.5 pro
549120	551120	Has gotten better in some sense
551520	554880	Now given uncertainty. It doesn't say dream. It says optical illusion
555840	560960	But it's still wrong and it's still over confident. Um, it rates itself as fairly confident
561680	568160	That it says optical illusion. It's interesting. It makes you think is there some semantic association between dreams optical illusions and
568400	572480	This particular way of presenting the notion of uncertainty. I don't know. Um,
573120	578080	The the very most recent thing this was well, it's not that recent but the last time I gave this talk
578800	585200	Um, I tried this on the the newest updated version of gemini 1.5 pro just to see if perhaps it had been improved
585600	587600	And now it gave a very different answer
587600	590160	It didn't say it didn't read it as optical illusion. It gave this
591040	596400	Extended digit number six one two nine eight one five three two four two and I asked it to judge its confidence and it said
596560	598560	Well 70 to 80 percent
598640	603840	Again, what what happened? Just to check again consistency and coherence of this
604320	608720	Weird interpretation. I asked it just 10 seconds later just in an out of context again
608800	613360	Just gave it the same thing and the next 10 seconds later. It said geocache
614000	618080	Um, again now 75 confident and then one more time
619200	623280	Um, a distorted text appears to read northeast blackout
624240	625600	However, it's difficult to be certain
625600	631040	But it's moderately confident that the text reads northeast blackout because the first and last words are relatively clear
631040	634880	While the middle word is slightly less discernible, but still suggestive of blackout
635600	638080	Okay, so what's going on here?
639120	640160	That's one of the mysteries now
640160	644640	I'm not I'm not I'm going to gesture at what I think some of the mysteries or some of the answers might be
644960	650400	But mostly to point the way towards the difference with human intelligence and what we're trying to understand in in artwork
651200	656240	Okay, I think fundamentally what's going on is whether you're building a computer vision system or
656720	659760	An llm or a multimodal language text system
659760	664640	You're building a system that takes the inputs and outputs that our brains do
664960	671360	perception of the eternal world sense data of some form and it produces actions or something like actions that can be
671920	677280	Grounded back in the external world, but the inside of the system doesn't have any notion of a world
677360	678800	It's a function approximator
678800	682800	It's learning to approximate the input output functions that our minds produce
683200	689360	And it's learning to do that from various data sources including objective data sources as well as human reinforcement
690160	694080	Now how why does this possibly work? Well, again, there there may be some
694560	700000	laws of physics things that are like physics like for example the famous scaling laws of neural language models
700400	702400	Where you can show in some form
702880	705360	Especially when you're trying to predict like you know
705760	711920	And this is very much following some of the information theoretic ideas that richard has been talking about and others in the group here
712960	715920	If i'm just trying to predict the next token from the previous ones
716720	722320	There are certain fundamental power laws of language and distributions in language that these systems seem to incorporate
722800	726720	And build on such that if you increase by an order of magnitude the amount of compute
727040	731520	You can in a predictable way lower the test loss in predicting the next token. That's on the left
732320	734320	In gpt4's technical report
734560	739120	They suggested that you could see similar kinds of scaling laws for problem solving not just text prediction
739120	740800	Although it's a lot if you're there
740800	745840	And I think fundamentally the problem is the power laws are beautiful laws and they're predictable in a certain sense
746160	748960	The thing about a power law is while it approaches asymptote
749360	752320	Or it rather approaches zero error in its asymptote
752800	757600	It in some important sense never gets there compared to like an exponential decay
757840	762400	Where there's a predictable timescale on which it will get to zero a power law keeps slowing down
762800	766960	and if you have any uncertainty in the power laws coefficient or
768080	773760	You know in its applicability to the not just to predicting the data stream, but actually solving a problem
774480	780400	Then it's it's basically impossible to know how much data and compute you're going to need to actually get to the asymptote
780400	783280	Which we want to call, you know full adult human intelligence
783680	785680	In contrast
786320	792400	Humans don't seem to be built this way. We our minds seem to be built as world modelers from the start
792560	794720	And I'll say more about this, but from the very beginning
795440	798720	We have our minds and our brains and this is something that
799360	805360	Inherits shared with other animals and inherits from our evolutionary legacy seem to be built to model the world
805840	808800	And to deal with all kinds of incompleteness and uncertainty
809520	813120	And that means both in how in the the structure of the world
813840	818560	And our uncertainty over what's out there the current state as well as the the fundamental deeper laws
818720	821760	The causal laws of physics or how agents plan and so on
822000	825280	But we're built to engage these kinds of mental representations
825840	828080	As i'll as i'll show in a little bit the way in our group
828080	833760	We've modeled this for a long time is what we call the game engine in the head by analogy to say video game engines
834080	838640	That our brains and minds seem to be built with these kinds of resources for world modeling
839360	843760	You can see that this the the notion of world models that are used
844160	850400	Even not just in human adults, but in young children to coherently understand the world and deal with situations that are
851040	853040	Importantly out of distribution
853040	855040	Right and that's really key
855040	860720	The ways in which we perceive and learn about the world are not nearly as time to the distributions of our experience as
861120	863120	machine learning or function approximation
863760	867520	So here's some examples of different kind of self-driving system. These are
868400	871040	four-year-olds for the most part three four five-year-olds
871920	876240	In a genre of youtube videos, you can see one of them actually the video on the left
877200	880800	It's it's sort of the the video genre on youtube is like, you know
880880	888000	My four-year-old driving for the first time and parents have put their kids behind the wheels of golf carts or tractors or cars or trucks
888960	893040	And just go at it and they video from the side or from next to them
893680	894480	and
894480	900160	You know, you could question the rationality of the adults who are putting their kids in the situation and then putting the videos online
900480	903680	But there's a certain basic rationality that the four-year-old has
904080	910640	Even though they haven't been in the situation before the systems that their mind has built through evolution and the last couple of years
910640	912640	The first few years of their experience
912880	915760	Allow them to handle this totally new kind of perceptual information
915760	919280	The world's going by them at a high speed and they're in control in some way
919840	925360	But they can generalize from their from from their experience because the the nature of their generalization
925600	929520	Is their mind's models of the world and those transfer to these new situations?
929520	932480	Of course, their new things they have to learn about exactly how the steering wheel works
932560	937280	But they can learn those very quickly too because the learning is grounded in their world model
938240	942560	So fundamentally, I think what we have is the is a contrast between the scaling thesis
943040	945040	of deep learning and today's ai
945360	949520	And what you could call the original scaling route that human intelligence follows what we might call
949920	953040	Growing up as opposed to scaling up and I want to contrast
953520	956800	Three points, which I think are really important for understanding
957280	963200	In general our intelligence and the contrast between ai and machine learning and especially the role that language and language models play in
963840	969200	So three points that are that are fundamental to the way today's ai is working based on deep learning one
969840	972960	That intelligence is seen as the end result of learning
973440	977760	We're learning starts with some simple very general and in some ways dumb mechanisms
978240	980720	associative learning prediction, you know in some way
981520	987600	But the idea is if you scale that up enough you have the surprising emergent phenomenon that now we start to call intelligence
988160	989360	um
989360	994080	The the problem is though, I mean either you call it a problem or you can call it just the way things work
994160	996480	But there's some remarkable kind of generalization
996480	999680	But it still is going to depend on similarity to training data
1000160	1006640	And its ability to go beyond the training distribution is is weak and unpredictable compared to when you're in distribution
1007600	1011920	Um, this is true whether you're talking about perception or other forms of higher level cognition
1011920	1018080	But especially thinking things that we recognize as forms of reasoning planning problem solving and so on
1018720	1021680	The the sort of thing that we've never seen in any computer vision system
1021680	1025840	But we're starting to see some interesting kind of approximation to in language models
1026480	1030320	Crucially that in machine learning thinking derives from language
1030640	1036640	Because it's only the data of language not the data of pixels that conveys information really about human thoughts
1036720	1041360	Which makes sense again humans have always used language as our main medium to express our thoughts to others
1041440	1044640	Just like what we're doing right now in giving talks like this, right?
1045120	1045680	um
1045680	1050880	But crucially thinking requires language data and even carefully cleaned and curated language data
1050880	1054400	Our colleagues who are at the big ai companies can tell us a lot more about this or rather
1054480	1058000	Maybe they can't tell us about this but they can tell us that they can't tell us about
1058560	1064320	And those of us who've worked with this know the importance of having the right kinds of language data as well as the right reinforcement data
1065120	1068640	Okay, so contrast this with human minds and especially human children
1069680	1071760	Our intelligence if for our intelligence
1072000	1075920	It's not just the thing that is at the end result of all of our learning although we do
1076240	1080560	You know babies are not in some important way not as intelligent as human adults we would like to say
1081120	1087200	But human intelligence is is built in in some form from the start. It's the foundation of learning. It's not just the end state
1087600	1092800	It's what's there and the the learning mechanisms that let you learn so much from so little
1093520	1097040	Okay, and we saw some of that in virginia's talk and you know
1097040	1101600	I think the the field of human cognitive development is a testament to this or what I was just showing from the four-year-olds
1101600	1104400	Is another example but in other work that i'm not going to talk about here
1104400	1108080	We've done a lot of work trying to model the core knowledge of even you know
1108080	1113520	12-month-old or 10-month-old babies and there's important ways in which they have a common sense understanding of the world
1114000	1118720	That is in in in significant ways. It's not all built in but in significant ways
1118720	1122080	It seems to be present even even in two and three month olds. Okay
1122880	1123600	um
1123600	1130800	As a result of our built-in capacity for modeling the world our generalization is based on the fit of our mental models
1130800	1135760	It's not about the training data. Okay data is important. We improve our models. We grow models
1135760	1142080	We can make new models from data, but what accounts for generalization and what drives generalization is not similarities to the data
1142240	1148240	But the fit of our world models and their ability to be flexible and themselves to be generalized
1149360	1155360	And crucially thinking isn't the thing that comes at the end state as an emergent property of modeling language
1155360	1159360	But rather thinking is there from the start and it's the basis for language
1159440	1165440	It's the basis for why human children construct language so robustly and resiliently and again there
1165520	1167200	I'm referring to
1167200	1173360	By using that word resilience to some of the ideas that virginia talked about in her talk and susan golden meadow has talked about
1173680	1177360	I urge everyone to watch her rumelhardt prize talk where she talks about this as well
1177920	1183280	This idea that as we saw in susan's work and annie sangus's work with nicaraguan sign language
1183680	1186240	That you know children who grow up who grow up without
1187120	1188480	Any language input?
1188480	1191680	It's not just about the poverty the stimulus in the traditional linguistic sense
1191680	1197840	But you grow up with deaf without sign language input and you create in some form your own personal proto language
1197840	1201040	Or at least a way of communicating that has hierarchical symbolic structure
1201280	1204800	And then you bring a few such children together and within the span of a couple of generations
1204800	1210000	They've created a whole new language from scratch. Okay, so it's very clear from data like that
1210800	1213840	That humans are built to think and we're built with an
1214400	1218320	A desire to understand and to under to be understood to express our thoughts
1218640	1222320	In some form and share them with others in our as our social partners
1222960	1226080	And that's you know, there's nothing I think more fundamental to
1226720	1229600	To understand if you want to understand where our intelligence comes from
1230080	1236960	So what we've been trying to do in our work is to try to capture the human growing up scaling root in computational terms
1237520	1240480	And it starts with this idea of probabilistic inference and
1241040	1244800	Expected value decision-making on top of world models, you know informally
1244960	1249200	I would say and like many others that thinking not just in human brains
1249280	1254560	But in brains and in many other animals is about making good guesses and bets not about the next
1255040	1259760	Data thing you're going to see not about the next token or the next pixel or set of pixel values
1259920	1262720	But about the world what's going to happen in the world and how
1263280	1268400	It might depend on your actions or the actions of others or how you might be able to change the world to change those
1268960	1274160	dependencies causal and counterfactual reasoning and then having some sense of what you'd like to see happen
1274560	1278160	Or and what you'd not like to see happen or what could really cost you your life or
1278800	1284080	Be hugely valuable and making good bets about how to act and what to think about next
1284960	1286960	fundamentally this classic idea of
1287360	1289360	effectively rationality that
1290000	1294640	You know, you could say our minds and brains are the original sources of this idea through evolution
1295520	1298640	Being constructed to do this kind of computation
1299280	1303520	In our work and this is the stuff we've been doing with language models builds on this
1303840	1306960	We've been using the idea of probabilistic programs, which is a
1307840	1312080	family of mathematical languages and actual programming languages and platforms
1312560	1318720	To embody this idea basically take this conceptual idea of rational world modeling inference of decision
1319120	1324960	And turn it into practical engineering terms that can be models of human minds as well as more human like ai
1325520	1328240	And I don't have time to give a whole introduction to probabilistic programs
1328800	1335040	But you know, you can think of it as a kind of catch-all phrase or a complicated suitcase phrase just like neural network
1336080	1340400	Packages a number of different things together, but probabilistic programs are formalisms
1340880	1346400	For combining what I think are several of the best ideas about intelligence that have come up through through a number of
1347840	1352480	You know decades in the field that includes neural networks modern probabilistic programming languages
1352800	1357920	Like gen from vikash man singha's group who I mentioned before at mit or pyro
1358480	1362240	That was developed by originally a group at uber ai that noah goodman
1362720	1365920	Another collaborator of ours helped to start along with a number of others
1367120	1368480	you know
1368480	1372080	In many ways build on languages like pie torch
1372720	1379680	Or tensor flow the languages that support modern deep learning and that allow you to construct really complex but end to end differentiable
1380640	1382640	Functions for approximation or other purposes
1383280	1389840	But that's not the most important part the most important part are building on the idea of symbolic languages for expressing abstract knowledge
1390080	1391920	for modeling the world
1391920	1393280	that is
1393280	1395520	You know across many areas of science and engineering
1396000	1401040	consistently the strongest most powerful toolkit we've had for building coherent models to
1401680	1404720	To understand the world are various forms of symbols
1405440	1412720	And then the third idea is the probabilistic one using those symbolic languages to express probabilistic models where you can be uncertain about
1413120	1415840	Everything and in a in a turing computational sense
1415840	1420480	You can be uncertain about the state of the world right now or more abstractly how the world works
1421200	1426880	How the different kinds of data that you're getting perceptual or others are connected to the underlying state of the world
1427600	1432400	And then to be able to do joint inference about all those different sources of uncertainty as the basis for perception
1432800	1434800	reasoning planning learning and so on
1435040	1437040	And probabilistic programs bring those things together
1437600	1443600	I'll say a little bit about how that toolkit works as well as then how once you learn language
1444720	1451200	The ability to externalize and internalize thoughts that are produced by these kinds of probabilistic programs basically
1451920	1457360	That transforms things in fundamental ways and here, you know, less people think that i'm just a
1458080	1460400	Deep learning or a llm skeptic or something
1460720	1467040	I you know, again, I find neural language models and the the long tradition of distributional statistical learning
1467440	1471600	In language that they build on to be really important and in the work that we've been doing we've been using
1472400	1475120	large language models or in many by today's standards
1475120	1478240	I might even say small language models to capture this actually
1478960	1484880	So I think this is a place where the you know, I would not say that you want to think about human minds
1485040	1488320	It's llms or or transformers all the way down nothing like that
1488640	1492160	But the kinds of things that are going on in distributional sequence learning
1492560	1499120	Could be a way to capture some aspects of how language grounds in these tools for mental modeling and and also
1499600	1501600	enriches and extends it
1502000	1505920	So, I mean just very briefly for people who aren't familiar with this idea
1506400	1509840	There, you know, for a couple of decades now I and and many colleagues
1510240	1515360	Students former students a number of others who are very interested in what you could call the Bayesian approach to cognition
1516080	1520800	Have been using this general toolkit of probabilistic inference over structured symbolic models
1521200	1526240	To capture many aspects of mental models and I just this is a plug for a book that's coming
1526800	1531520	forthcoming edited by tom griffith's nick shader and myself and with many many other contributors
1532000	1534400	And it'll be coming later this year from mit press
1534720	1539600	But it's it's part textbook part research monograph and just showing how this toolkit can be used to
1540160	1545440	Capture to both explain in a principled and you know, actually understandable way
1545760	1552160	But also to quantitatively predict and model behavioral data across so many different ways in which our minds model the world
1552640	1555840	Just to illustrate one which i'll come back to in the context of language in a minute
1557360	1560080	An area that we've done a lot of work on is intuitive physics
1561040	1564800	As as a kind of probabilistic inference and the particular kinds of intuitive physics
1564800	1570000	We're thinking about are in these complex seed understanding cases, which again not coincidentally
1570080	1573840	I think are also really interesting in classic settings for studying grounded language
1574400	1576400	So for example, if I show you a
1577120	1580880	Scene of a bunch of blocks think like jenga blocks stacked up in various ways
1581200	1586800	Some of these images might look very stable others might look unstable and I can ask you a question
1586880	1590080	How likely do you think this stack of blocks is to fall under gravity?
1590800	1597280	And we can model that by having a a structured world model which consists of basically 3d object models
1597680	1603040	And then causal models of how though of how those underlying 3d scenes give rise to images
1603120	1608960	That's like a graphics program basically the computer graphics is a way of writing programs that generate images
1609280	1611280	From those underlying 3d world models
1611680	1615440	But then there's also these physics programs and again here I mentioned game engines
1615680	1621040	So game engine style physics simulators capture a lot of aspects of effectively real world
1621120	1624960	But common sense physics in ways that hack hack Newtonian or actual
1625520	1627920	True scientific physics in all sorts of ways to be efficient
1628400	1630400	But to a pretty good job of capturing
1630560	1635120	What we expect to happen in the world which makes sense because we're the ones playing the video games and they're designed for us
1635520	1641040	So by doing probabilistic inference to infer the input to a graphics program given the output
1641120	1646960	Which is the image you can do a lot of 3d perception and then by doing probabilistic forward simulation
1647120	1650720	You can imagine what might happen next. It could also be conditional on your action
1651200	1654800	So the same toolkit could be applied to a much less familiar sort of judgment
1655360	1658400	Like imagine I have these scenes of red and yellow blocks on a table
1658480	1661760	And what if I bump the table hard enough to knock some of the blocks onto the floor?
1662080	1664400	Will you knock off more red blocks or yellow blocks?
1665040	1668160	So the first judgment the one about how likely the stack of blocks to fall
1668160	1672000	I mean anyone who's played jenga or anybody who's ever been a kid probably has built things
1672240	1674720	If you're a professional builder, you have a lot of other intuitions
1675040	1679520	But you know, that's very familiar the question on the right is one that unless you've seen me talk about this
1679760	1685040	You probably never thought about it's not something you have direct experience about you can't learn it from feedback
1685520	1686960	How to answer that question?
1687280	1693040	But I can use my language to give you that question as well as the other relevant world knowledge
1693040	1698880	And then you can reason about it and the models that we build can capture both the familiar judgments of like
1698960	1701680	How likely the stack is to fall as well as these novel judgments
1702480	1703840	Relatively equally well
1703840	1708400	So these scatter plots are examples of the kind of data and modeling that we've done in our lab for a long time
1708880	1716080	On the y-axis we're plotting the average human judgments of let's say on one to seven scale how stable or unstable the blocks are
1716800	1718800	and on the x-axis the average
1719760	1724160	Result of doing a small number of probabilistic simulations where we imagine running forward
1725120	1731520	Game-style physics a few time steps with uncertainty about exactly where the blocks might be and how the physics works because we don't
1731840	1733840	There's are all things that our minds don't fully know
1734240	1738480	The same kind of model can be used to answer these red and yellow questions. So just to illustrate this
1739440	1740960	If we take one of these
1740960	1745840	Scenes and we reconstruct it in a game-style physics engine and we simulate a bump of the table
1746160	1750480	So there's one simulation on the right. I'll show you another simulation with a harder bump
1750800	1754640	Okay, and you can see watch it again that different things happen
1755440	1757440	in these two simulations
1757600	1761280	But it doesn't really matter which one of those you ran to answer the question, right?
1761280	1765200	I mean you look at the scene also and it's very clear from the beginning that if I bump the table
1765680	1769680	Relatively hard. It's going to probably be mostly yellow blocks on the floor. How do you do that?
1769760	1771760	Well in our model you only need to run
1772160	1776400	One or a small number of these simulations to answer the question at the grain of intuitive physics
1776560	1779760	And you don't need to run it very long. You could stop it now and you already know the answer
1780160	1780640	Okay
1780640	1785040	So a small number of short incomplete simulations is the basis for these pretty
1785360	1791040	Quantitative models and they've also been used in robotics to actually predict and and be able to get for example a robot to
1791600	1796000	Learn to play the game genka, but learn from a very small amount of realistic human experience
1796640	1798640	The same kind of idea has been very
1798960	1799920	impactful
1799920	1803040	I think even more impactful in intuitive psychology
1803760	1809840	Where we and many others this there's work that I've done with a number of students and my colleague at mit rebecca sacks
1809920	1815840	But especially I would highlight work from chris baker and julia har edinger who did quantitative modeling here
1816160	1818160	Going back years with us
1818480	1821840	julian continued chris is now working on self-driving cars and industry julian
1822560	1824560	is now working on
1825280	1832480	It's now a professor at Yale where he's he's extended on this kind of approach in many areas of social and communicate with social cognition and communication
1833280	1834800	in really interesting ways
1834800	1836800	And many other colleagues here
1837440	1839440	I and I should mention also though I didn't
1839920	1843040	I forgot to mention this before the intuitive physics work started in our group
1843360	1846560	More than 10 years ago with pete patalia and jess hamrick and others
1847040	1853360	And they are now actually both working at deep mind google deep mind working on various interesting kinds of deep learning approaches
1853600	1860000	For both intuitive physics and all sorts of real-world physics that matter like climate modeling. That's where pete's current focuses
1860720	1863760	I won't go into the details on the bayesian theory of mind
1864400	1868160	But the basic idea is again, there's a program that now describes
1868560	1875040	Not necessarily how actual minds and brains works, but our mental models of other minds how the actions we see agents take
1875600	1879520	We interpret them as the effect of planning programs that take as input
1880240	1883680	The our minds representations of their beliefs and desires
1884080	1889040	And by seeing how agents act and change the state of the world and modeling also their perception process
1889040	1894400	Which leads to belief formation and updating we can model many aspects of how people understand
1895520	1901600	Other minds especially in what you might call these sort of physically grounded and perceptual scenarios the kinds that you can also
1901600	1907040	Study in young babies like in the experiments. I'm showing here from gargay chibra. Kylie hamlin paul bloom and others
1907760	1911040	Um the the intuitive psychology case is especially interesting
1911600	1918720	Because in work that we've done like in this paper with uh, brendan lake tomer olemann sam gershman that helped to set some of the
1918720	1922960	Current stage of debates between deep learning and more cognitive approaches to ai
1923920	1927520	Where we put about a challenge back in 2016 and 2017
1929040	1934880	For the the deep learning ai world about how to think about the kinds of inductive biases mental model
1935600	1939600	Materials that seem to be built into human minds and the learning mechanisms that build on those
1940400	1947200	We highlighted both intuitive physics and intuitive psychology in the famous sparks of agi paper showing again some of the most
1948400	1952880	earliest glimpse into some of the surprising things that gpd4 seemed to be able to do
1953680	1960080	bubeka and colleagues also highlighted intuitive physics and intuitive psychology and then in a recent paper from eric schultz's group
1960640	1967520	Which is kind of forms a a three-part story here and it continues as as i know bushoff and and schultz and colleagues
1967760	1970000	Continue to work on this area and we also do
1970560	1975440	Is to now in the era of these multimodal language and vision models like i showed you at the beginning
1975760	1982640	We can go and take on all these tasks like whether it's block tower intuitive physics things or agents moving around in the world and interpreting
1983280	1987760	As julia harr edinger called it the naive utility calculus of their rewards and costs
1988400	1993760	And give language models or multimodal models the same kinds of stimuli and questions that we give people
1994720	1996720	And as schultz and colleagues showed
1997840	2001520	In some of the intuitive physics settings, they're kind of okay. They're not great
2002800	2006560	In the intuitive theory of mind ones they fail altogether. That's a quote from their paper
2006720	2013600	So as an example, this is this is work that julia harr edinger did as part of his phd thesis with myself and laura schultz
2014480	2020640	Um where we call these the astronaut studies where people would see an agent an astronaut on some planet
2020880	2028000	Who would start at a certain point and have a home base that they had to get to and they would follow some path along
2028720	2030720	This the surface of the planet
2030720	2034080	They could just go straight to their home base or they could take a different a not straight path
2034640	2040880	Um crucially there were various objects that they could pick up that could either be positively valuable or aversive to them
2041440	2043440	And we asked people based on the path
2043440	2047200	How likely do you think that the agent likes or doesn't like one of these objects?
2047600	2051040	And also there are different terrains and the terrains could be more or less costly
2051360	2056560	And by showing people different maps different configurations of objects based terrain and different paths
2056880	2062880	You could get very interesting rich inferences about what about what the agent both wants and
2063360	2067600	What the rewards assigned to the different objects are as well as the costs for moving around on the terrain
2067840	2070560	So here's an example of some of the stimuli from one experiment
2070800	2073920	Just to show you the kind of variation and then in each of these cases
2073920	2078480	We can ask people to make four judgments or three or four depending on how many kinds of terrain
2078720	2082480	And that's what's shown here. These are these are the actually the predictions of the model
2083200	2088960	Z scored for the relative cost for the different kinds of terrain and the relative value for the different kinds of objects
2089200	2091520	And we're assuming that the agent basically takes in a
2092160	2095520	of rational efficient plan trying to maximize reward
2096000	2101280	Minus cost where there's a small cost for each step, but especially costly when you travel over certain kinds of terrain
2101840	2106560	So seeing the path you can make inferences about how the agent has rewards and costs
2106880	2110560	And then when you ask people to make the same judgments, they line up almost perfectly
2110960	2116080	This is just one of many experiments that julian did showing what is really a remarkable quantitative
2116560	2120160	Match and but but by a model that isn't just fit to data
2120400	2122720	There's a little bit of fitting but it's mostly based on
2123120	2128480	Thinking about what are the core concepts of theory of mind that people like I mentioned before like gargay and chibra
2128640	2133120	Have studied even in very young infants pre-verbal infants infants who can't even walk
2134560	2137840	And barely just able to reach for things themselves yet
2137840	2143280	They still have these ideas these intuitions about efficient inference and and use those it seems
2144160	2146160	I mentioned sherry lou's work who was a
2146480	2150720	PhD student at harvard a few years ago with this velki who worked with tomer omen also in me
2151040	2156640	To show those same kinds of things work in babies sherry's now doing amazing stuff extending that in a new lab at johns hopkins
2157280	2162720	Um, so these these are cases where this kind of probabilistic program model works really well, but just a pure language model
2163280	2169280	Uh is basically at chance zero correlation with human judgments, although quite good at telling you things like the background color of the scene
2170000	2174800	Okay, so in the last part of the talk having sort of set the stage for how are our
2175680	2180080	Human thinking seems to work and how we can model this in ways that are explanatory
2180720	2186720	Understandable and quite quantitatively predictive using these probabilistic programs. Let's understand where language comes into picture
2187200	2191840	I don't think that a pure machine learning transformer approach like the ones we've been talking about
2192400	2197200	Are on track to give a human level or certainly a human like account of how it works
2197440	2203680	But I do think the ideas of sequence modeling statistical distribution learning that you see having
2204240	2206240	such great success in llms
2206560	2211280	Even perhaps on a smaller scale could say something important about how language comes into the picture
2211920	2214160	So this here is the at the beginning
2214160	2217440	I talked about this archive paper the word models to world models paper
2217840	2224080	From line along Gabe grand and colleagues and that's what i'm going to be talking about here is how we've been using bringing these tools together
2224560	2229280	But a key ingredient is this I would say the modern return of the language of thought hypothesis
2229280	2234320	You know made famous by jerry fodor, but obviously with a history that goes back hundreds if not thousands of years
2234640	2236640	But in the number of recent papers
2237360	2241680	And proposals from various groups the idea that again that there's some kind of
2242480	2247840	Not necessarily single language, but but abstract symbolic languages, which could be general or also
2248320	2250820	Created and constructed domain specific languages
2251200	2257600	Some kind of abstract symbolic language is seems to be a powerful way to think about human thinking abstraction
2257840	2259520	concept learning and so on
2259520	2262480	And the particular kind of languages of thought that we've been thinking about are
2263040	2267200	What noah goodman and toby gersenberg and I called the probabilistic language of thought
2267520	2271440	So this is the idea of using probabilistic programming languages the technical tool
2271520	2272960	I talked about before
2272960	2278160	To to formalize in a cognitive setting a certain kind of hypothesis of a language of thought
2278400	2280720	But one that is focused on modeling the world
2281120	2287440	Not just possible worlds, but probable worlds and where the the symbolic language can also express ways of conditioning and query
2287760	2291360	So that we can ask and answer the kinds of questions that our minds do and that
2291680	2295200	We might we might want for example any kind of general ai system to do
2295520	2300400	So if you want to learn more about the probabilistic language of thought check out our chapter in the conceptual mind
2300800	2303280	This is one of the margiles and laurence
2304160	2310080	Readers or the web book probabilistic models of cognition, which which has examples of the kinds of models
2310080	2311520	I'm going to be talking about
2311520	2316960	Not with language models though the new thing is to take advantage of llms and specifically the fact that
2317200	2320240	Most llms these days are trained not only on natural language
2320480	2323200	But on on programming languages and source code
2323600	2329360	Namely programming languages like all the ones we're probably familiar with that are designed to be read and written by humans
2329440	2332320	And not just machines and so they're written in a very english like way
2332880	2336720	Or natural language like way, you know linguists have long pointed out
2337200	2342640	Sometimes the differences between natural languages and programming languages, but from a certain perspective
2342880	2344880	They're a lot more similar than they are different
2345120	2350320	A hierarchical structure syntax and even just much of the lexicon, you know natural languages
2350400	2354080	They're not just commented or programming languages are not just commented in natural language
2354080	2358880	But functions variables data structures are named using english for the most part
2358960	2362960	Okay, so that's a very powerful data source that allows
2363600	2371680	Some kind of statistical sequence to sequence model for predicting and translating between streams sequential streams to effectively learn to translate
2372000	2377760	From english or any other natural language into programming languages of thought and that includes probabilistic programming
2378160	2382400	So the idea of this paper what we call rational meaning construction. That's the name of this
2383120	2389280	Is is a particular thesis on how language is understood and perhaps also how it might be produced and learned
2389360	2391680	Although our focus here is on language understanding
2392400	2396400	And thinking about the relation between language of language and thought in that context
2397040	2403280	By thinking what we think of as like the core original notion of thinking is what I've been talking about for the for the first part of the talk
2403520	2405520	Which is the idea of having a
2406080	2410240	Structured probabilistic model of the world conditioning it on observations and then
2410800	2414960	And then drawing samples of underlying latent states and future states
2415440	2417440	Okay, that's thinking from this standpoint
2417680	2421760	And then understanding language is effectively translating from natural language
2422080	2427600	Into a probabilistic language of thought that's used to define and condition and query the probabilistic world model
2428000	2430800	So we're going to exploit the the property of these
2431280	2436720	LLMs to translate from natural language to code in a way that might be familiar if you've tried using them to code
2436720	2439040	But it's different in some key ways in particular
2439120	2443520	We're not going to be just asking our LLM to write a whole bunch of code at least not to start
2443680	2447520	But we're going to be focusing on the sentence level and what is meaning at the sentence level
2447840	2452480	It's something like trying to infer a line of code in a mental programming language
2452800	2454800	that is your best understanding of
2455280	2461040	The meaning by which we just mean the thought that the person who's uttering that sentence is trying to convey
2461840	2466960	Okay, and the the LLMs in this case can represent that meaning construction function
2467280	2469440	Okay in ways that have some very interesting properties
2469680	2475760	So i'll just illustrate this with a classic example from the probabilistic language of thought literature the so-called Bayesian tug-of-war
2476160	2479360	So just to ground this imagine that you're reasoning about a situation
2480160	2481680	Like for example, uh
2481680	2486960	Various games of tug-of-war various people symbolized by colored shirts here are facing off against each other
2487200	2489760	And let's just take one person. Let's just say this is a
2490640	2494880	Guy named jack and you might say well, how strong do you think jack is I haven't shown you any information?
2494960	2496960	So your best guess might be just average. Okay
2498240	2500960	Now, uh, suppose I give you some other information
2502320	2505680	Like um, or I should say here. So here are a few samples, you know, I might say average
2505680	2507520	Maybe it's a little less than average
2507520	2510320	If we're talking about MIT students, you may just pick the MIT average
2510480	2514960	But now I could give more information like suppose, you know that jack beat leo in a game of tug-of-war
2515200	2518320	So that might move your arrow up from the MIT average because well, you know
2519040	2523440	People who are who are stronger might be more likely to win than people who are weaker somehow. Okay
2524000	2526000	I could give you some more information
2526000	2531280	Like leo had just won 10 previous matches as well. So that means you might think leo's pretty strong and jack must be even stronger
2531280	2532800	So your arrow goes way up
2532800	2536080	But suppose I told you well leo sometimes it doesn't pull as hard as they really could
2536320	2541760	So maybe maybe leo was just getting a little lazy when leo faced jack. So it might go down a little bit
2542400	2547200	But then leo single-handedly beat a team with jack and tom on it. Okay. So now you think okay
2547200	2551360	Well leo was probably lazy before jack. Maybe isn't that strong
2552240	2555440	Since leo when they wanted to could beat both jack and tom. Okay
2555440	2562000	So the point is in a classic example of non-monotonic reasoning your inference about this one aspect of the world jack strength
2562000	2568400	It's going up and down as you get various information. So we'd like to understand how that thinking process works how
2569280	2576320	Updating your beliefs based on linguistic evidence linguistically expressed evidence works. Okay, that's the starting point of this paper
2576880	2578800	And the idea is to use in this case
2578800	2584400	We're using the the probabilistic programming language church which is based on a dialect of lisp or scheme
2584640	2587360	So there's a lot of parentheses and i'm not going to be able to unpack all the language
2587440	2592480	But hopefully you can get the basic idea that we write we define functions that describe
2593040	2595040	probabilistic distributions on
2595520	2601600	Strength laziness and so on all work through this and that's you know, that's been that's well-attested work
2601600	2607920	That's been very good. You know, it's basically the the general toolkit for doing the kind of probabilistic intuitive
2608560	2614000	Mental modeling that I showed you with two to physics and intuitive psychology, but here we're doing it for this kind of novel domain
2614000	2617520	It's not there's no core domain that infants are born with for tug of war
2617680	2621840	But we can write a model like this to capture what adults in our culture at least might think
2622240	2628400	And then the key new thing here is to understand how to to ground language in these mental models
2628880	2635440	So this is where we're going to model the translation of a statement in english like jack one against leo or a question
2635520	2642560	Like how strong is jack into lines of code that support probabilistic updating and querying in this language
2643280	2647360	And this these the meaning functions, which will be implemented by the neural network here
2648400	2649760	other the
2649760	2651760	Large or even small language model
2652160	2655360	Um capture various interesting ideas that have been proposed
2655600	2660320	You know, I don't have time to review this history but proposed in different eras of linguistics and thinking about
2661120	2666480	How to think about meaning how to think about concepts and you know, I it's it's a it's a very rich literature
2666480	2669680	That I'd be happy to try to discuss if we have time during the question period
2670400	2677200	Okay, um, but the hypothesis here is that maybe these distributional language code models can implement some
2677840	2681440	human like perhaps approximation to this meaning function
2682400	2683440	Um
2683440	2690480	And again, crucially, it's it's we're not trying to now capture like to learn patterns in data in the in in the world
2690480	2694080	We're trying to learn patterns in our thinking and how thought is expressed in language
2694240	2696240	So it's a more modular problem
2696240	2700560	It which I think is better suited to the way language actually works in the human brain, right?
2700560	2704800	It's there's a delimited part of our brain. That is a language processing network
2705200	2711360	Strokes or other lesions there can really impact your language ability without impacting your general thinking ability
2712640	2716640	And you know, it's a relatively late evolutionary addition to us to a brain structure
2716720	2719760	Which shares a lot with other non-human primates, for example
2720240	2723760	And so it's it's that that modularity. I think is also really important here
2724480	2726400	So again, just to unpack how this works
2726480	2729600	You have a probabilistic program that describes these concepts
2730080	2733120	That I can just I can describe to you in language in this way
2733280	2737360	But to the model right now, they're just given in code and I'm not and they just describe for example that
2737760	2741840	You know players strength is drawn from a gaussian distribution players can occasion be lazy
2742480	2745520	When you when you're lazy it cuts your effective strength in half
2745760	2751120	And the strength of the team is that some of the total strength that they pull that the members pull with at any one time
2751440	2754960	And the stronger pulling team wins. That's basically what this is now for now
2755040	2757920	We'll just assume that that's you have that kind of mental
2758640	2760640	Model of a tug of war game
2760640	2762960	And we'll just talk. How do you update your release language?
2763280	2768240	And the basic way of doing it is that we're using what I would call now a medium language model
2768240	2776480	We used open ai's codex, which was the first widely used code llm pretty small by today's standards much smaller than gpt4
2777120	2779520	Kind of like an early version of gpt 3.5
2780000	2786000	To translate from a sentence in english like this into what in church the probabilistic programming language is called a condition statement
2786160	2790320	It just expresses the constraint on possible worlds that while the underlying
2790720	2794960	Define statements define stochastic or probabilistic functions, which give you distributions
2795360	2799680	Probability distributions on possible worlds. This says we're going to restrict our probability distribution
2799840	2806160	Which is like our prior to a posterior just those worlds that are consistent with jack beating in one match leo
2806560	2807760	okay
2807840	2812960	And then similarly a quest to a query statement like how strong is jack turns into this worry
2813680	2817040	Which is which the probabilistic programming language then basically
2817600	2823520	Evaluates it draws samples from possible worlds consistent with the condition and generated from that prior on the left
2824000	2827840	And and checks what the strength is and then just kind of counts up those distributions
2827840	2832720	So it's a kind of probabilistic mental simulation the same effectively the same or a generalization of what we were doing
2832720	2837440	You like those intuitive physics examples and from this one piece of data jack beating leo
2837520	2841760	You can see the posterior updates from the prior so jack is stronger than average
2842080	2847200	If I say oh jack also beat alex or proceeded to claim victory from alex that turns into another condition
2847520	2851200	And that updates the posterior even more so now while jack's a lot stronger than average
2852080	2857600	Even working as a team leo and alex still could not be jack. Wow now jack's even stronger when you add in that conditioning statement
2857920	2860640	So each again the where the llm is coming in here
2861520	2869120	Is it's just it's just adding in the the statements into the language of thought and then we're running inference in our probabilistic programming language to give these answers
2870000	2873840	It's worth and this is again, especially if we want to understand what I think is
2875280	2880160	The way forward to thinking about how meaning and language works in humans and where l lm's can come into the mix
2880480	2882480	What's what's really powerful about?
2882880	2884800	Neuro language models here
2884800	2891600	As ways to parameterize a meaning function is the ways that they can pick up on statistics context pragmatics
2892080	2893120	metaphor
2893120	2899200	Um semantic associations all the things that in many ways were most appealing in connectionism like distributed content addressable
2899760	2904720	Uh associate of memory and distributed representations of graded semantic associations
2905040	2908800	So for example, the llm will translate jack won against leo
2909200	2914400	Which into this statement which looks like a fairly transparent semantic parse of that natural language
2914800	2919840	But it'll make basically the same semantic parse of a sentence which on its surface
2920560	2926480	And and in traditional syntax, you know language structuralities looks rather different, right? The syntax here is more complicated
2926480	2928880	I'm not actually using the word win, but
2929600	2936240	In the context the relevant aspect of meaning for thought here is the same namely. This is just another way. It may be more poetic way
2937200	2941760	Or dramatic way to say that jack won against alex and the model knows that automatically
2941760	2946160	It doesn't have to be specially prompted or trained for that using its associative memory properties
2946480	2950240	It's also distributional. These are probabilistic models not on worlds
2950400	2955120	But on strings and in this case they're probabilistic models on strings in our mental programming language
2955280	2956800	so they can
2956800	2961120	Bring into you know bring in classic notions of vagueness if I say jack is strong or very strong
2961280	2965680	I'm not telling you exactly how strong jack is but you might interpret that as a distribution
2966000	2969760	On different condition statements saying well jack strength is is greater than some threshold
2969760	2973200	But I don't know what that threshold is but it's probably pretty big remember this in this case
2973280	2976960	The mean is 50 and the standard deviation 20 so 80 is you know
2977760	2982400	One and a half standard deviations above the mean and the idea is this is again like in scalar
2984000	2987600	Adjectives it's basically saying I'm conditioning on the idea that jack is greater than some threshold
2987600	2993920	But I but I could be uncertain about what that threshold is and even metaphorical things like if I say jack is pretty strong
2994000	2996400	And ben is a beast right in this context
2996720	3001360	It's reasonable to interpret ben is a beast as as saying saying ben is really really strong
3001520	3007280	You know 80 where jack was maybe 60 or greater in a different context if i'm talking about you know
3008000	3013440	Whether you should date someone and I say ben is a beast, you know, then i'm probably saying stay away
3014160	3015280	right
3015280	3023600	So that that both metaphor sort of context sensitive metaphorical interpretation is the kind of thing that this meaning function is very good at
3024560	3028800	um, I'll mostly just skip over this but in work with ben lytkin and and
3029840	3036000	Leo and gape and others we've we've done and this is really all ben's work some very nice quantitative studies
3036880	3040960	Of showing that these contextual aspects of understanding of strength
3042000	3048720	In in the kind of tug-of-war context can match very nicely not always but in a lot of cases very nicely with human judgment
3048800	3052160	so it's suggesting that the distributional aspects of
3053120	3055520	um meaning that we're capturing here are at least somewhat
3056000	3059280	Not psychologically real at least consistent with the judgments that people make
3059680	3065600	Okay, so so but mostly what i've done with this example is just to try to show you how this kind of framework can work
3066000	3070560	To be a way of implementing what is in some sense a classical idea that language isn't
3071360	3074240	That directly the medium of thought at least the only one
3074640	3078560	But it's a way of expressing and communicating thoughts internalizing and externalizing them
3078720	3084160	Okay, and that statistical distributional mechanisms can be a powerful way of learning that
3084560	3087600	The mappings from the signs and symbols that we externalize
3088240	3091600	To the to an internal compositional structured language of our thought
3092160	3095440	Now a lot of what goes on in this paper and i'll just show you one or two examples
3096320	3103040	Such as work with sed zhong in an intuitive physics domain or some intuitive psychology work with lan xing and shen or
3104000	3105520	Tan shi shen
3105520	3109120	Is showing how we can take the same kinds of things that i showed you in the first part of the talk
3109520	3115520	And effectively reconstruct them, but now in a world's worlds that are just described by language
3116080	3121440	So to manage it take that task of the red and yellow thing where before i was showing you the image of a scene
3121840	3124160	And then i was using language to describe a question
3124480	3127680	But what if i just use language to describe the whole world, right?
3127760	3129760	So if i didn't show you an image i just described
3130240	3137520	What's there, you know in many ways, but but but we're still going to do a a mental simulation a probabilistic mental simulation in a physics engine
3137680	3143040	There's many cases, you know in our daily life where we use language to describe the physical world as we experience it
3143280	3148640	It can be extremely expressive very complimentary often to you know images or photos
3149120	3151840	Um, it has vagueness and uncertainty and that's interesting
3152080	3155680	So we might describe a scene that like imagine a table and there's some blocks on it
3155920	3157280	There's some red blocks in the center
3157280	3161200	There are many tall stacks of yellow blocks on the side of the table if the table is bumped hard enough
3161280	3164240	So we ask the same question but for scenes that are described in language
3164240	3170080	And then people make a graded judgment the same one to seven graded judgment of is it more likely to be red or yellow blocks?
3170800	3174480	And so when we can test this again the same kind of quantitative study with
3175040	3178080	large number of participants all online all just reading stuff
3178880	3182720	With different kinds of language expressing exact as well as approximate number
3183440	3188400	Approximate quantifiers logical quantifiers vagueness like the stacks could be tall or very tall
3188800	3193200	And and different kinds of spatial relations and across many different stimuli
3193520	3198000	We we mix up more or less complex sentences using these different kinds of language
3198640	3202240	And in each case we're using again a relatively small
3202640	3205840	LLM to translate sentence by sentence into
3206400	3209600	statements for conditioning and querying in our probabilistic language of thought
3210000	3213840	Then we run a small number of mental simulations in this case in a 2d physics engine
3214320	3219360	Compute the outputs and compare those with people and what i'm showing you here is again the same kind of scatterplot
3219360	3226000	I showed before but on the vertical axis are again our human judgments and on the x-axis now are the model
3226160	3228960	Or the predictions of this language informed thinking model
3229440	3235360	So it looks a lot like what I showed you before these are the judgments from the battalion all work on the red and yellow task
3235760	3239600	We're not we're here the model was given a visual scene people were given visual scenes
3239920	3244000	And the model did that same kind of probabilistic mental simulation in the physics engine
3244480	3246480	But in the in the current work on the left
3247040	3253920	Again, the scene description is constructed from language using the tools that i've shown you by conditioning a prior on scenes
3254000	3259840	Okay, and the main point is just that the models fit human judgments pretty well in both cases and about equally well
3260080	3262080	All right, which is which is interesting
3262400	3266960	We can also compare with just a sort of zero shot or a few shot baseline llm
3266960	3272800	So these are language models, which don't have an explicit mental model of physics or any doing explicit simulation
3272800	3275840	And they are are much worse fits to people
3276080	3279520	Okay, this is this on the what the high up here on the y-axis is
3279840	3284080	Distance and distribution from human distribution of responses to the various models
3284080	3289600	The blue one is the rational meeting construction model and you can do a similar thing in the intuitive psychology domain
3290400	3294960	Like for example in the settings that I showed you with julien harenger's work
3295520	3300560	This is work that uh lans ying and shen did i won't go into the details, but again
3300560	3303280	We can describe worlds with various
3304000	3308160	Goal objects that an agent might have and constraints like you have to go through doors the doors could be locked
3308160	3310960	You have to use keys that could be a red key that could unlock the red door
3311360	3313760	And so on people could be told either that you know
3313920	3316880	You need a key of the right color to open each door of the same color
3317280	3319280	Or you could be told weird things like
3319360	3321360	Keys only unlock doors of other colors
3321360	3325440	Okay, um, you know in these kinds of uh worlds again
3325440	3329120	The rational meeting construction model does a very good job of capturing people's judgments
3329520	3335120	For some kinds of judgments easy cases even gpt 3.5 does reasonably well gpt 4 does better
3335440	3340560	But as the situation gets more complex or unusual things change and break down like especially if
3340880	3344240	We say well in this world keys only unlock doors of different colors
3344320	3347280	That's an easy thing to say to somebody and you have to change your mental model
3347680	3355120	But when you do something like that gpt 3.5 becomes anti correlated with people and gpt 4 just go drops from being highly correlated to just being a chance
3355840	3360960	So these are examples of the ways in which you know going out of distribution from our training experience
3360960	3366640	If we're just using a machine learning function approximation approach, which is remarkable as those systems like gpt 4r
3366720	3368160	That's what they're doing
3368160	3373600	But ways in which if the function approximation is just approximating a much more modular translation function
3374240	3377120	And building on our mental model tools. We can just do much better
3378080	3381760	The last thing i'll just talk about very briefly is you know what is
3382400	3384800	Any work like this has to raise more interesting questions
3384880	3388800	If if anything like this is right on the right track, then it's it's not answering
3389440	3393600	At this point the most interesting questions, but just raising them and perhaps highlighting ways you might get at them
3394160	3399840	So in all the work i've been doing in in for a couple of decades now in probabilistic inference in mental models
3400160	3404640	You start by showing how you can do inference with a mental model, but then you have to ask where do you get it from
3405120	3409680	Now learning from experience is one thing and i've done a lot of work and others in our group and many other
3409760	3417840	Colleagues a lot of that book i mentioned with tom griffiths and nick shader is about how we can learn using hierarchical bays and probabilistic programs learn abstract
3418400	3422720	Programs by doing inference over the space of programs to make sense of our data
3422800	3424640	Maybe even small amounts of data
3424640	3429600	But much of our learning probably the most powerful form of human learning comes through language more abstract
3429920	3434080	Generic language as many folks in cognitive science cognitive development
3434640	3435760	Have shown
3435760	3441360	So the power of language not only to update your beliefs, but actually to give you new world models is really
3441920	3447520	Incredible and that's probably the most that that's the real human singularity there right is the ability of language to let us
3448160	3453360	Learn and think about situations that we haven't directly experienced. I mean think about the tug of war for example
3453440	3455280	I mean, I don't know if you're like me
3455360	3457680	I've maybe done one or two tug of war games at my
3458320	3462720	Most of my knowledge about tug of war or my beliefs at least don't come from my direct experience
3463040	3466320	They come from things maybe people told me about analogous situations
3466720	3472240	And and more generally many of our mental models and our intuitive theories come from what people tell us
3472400	3472880	All right
3472880	3479600	So the same approach that we talked about for updating beliefs from language can also be used to acquire new mental models
3480240	3483920	But here what's going on is we're we're modeling how somebody might explain to you
3484320	3491120	The way this tug of war works and in fact in our experiments with humans and in the in the toby gerstenberg and noah goodman and
3492160	3493680	colleagues did
3493680	3495680	This is exactly what we do would tell people
3496240	3500000	Um about how this work people have various strengths it can vary from person to person
3500480	3502800	And then now we're using the llm to translate
3503440	3508560	Those say sentences in english that are generic sentences about the domain describing the world model
3508640	3510640	We want our participants to use
3511040	3515120	Into the same kind of probabilistic program code, but now these are defined statements
3515200	3520960	So these are not these don't condition on a specific a specific world, but they define the general distribution on worlds
3521200	3523200	But again, they're contextual they're distributional
3523440	3526400	There could be different ways of understanding the vagueness in language
3526720	3531680	But the basic idea is that we can describe a world model in english and then the code llm
3532480	3536400	Can construct that world model. So all the defined statements now are constructed
3536480	3538640	They're not exactly the ones that we use in the original paper
3538640	3542160	But they have the same functional role and they can support the same kinds of inferences
3542560	3548800	So it's just just showing you the way we can I think go towards one of the most interesting ways that language informs our thinking
3549120	3552560	Not just as a way to convey specific beliefs about situations
3552560	3558320	But new but new world models and in some recent most recent work that's still in progress with tyler brook wilson
3559040	3565600	Katie collins and a number of the others I mentioned here tyler is a is a brilliant philosopher who recently graduated from MIT
3566240	3572720	And he's actually just just accepted a faculty job at Yale. So he'll be at Yale in a in a year or so
3574080	3576080	But together with with tyler and others
3576560	3579200	And tyler's thesis goes into some early stages of this
3579680	3584000	We've talked about we've been exploring the ways that the same approach can support
3584640	3588480	Constructing new models, even if I don't explicitly tell you in language how the world works
3588480	3594880	But just by again using your associative memory and marshaling implicit knowledge to construct a model of a new situation
3595280	3598960	So I could tell you about a relay race and I'm not tell the relay race is like a new domain
3598960	3601920	I'm not really telling you how races work. I'm just giving you some information
3602480	3604880	But we're exploring ways in which the LLM can
3605520	3608640	Can be queried to construct possible background knowledge
3609120	3613040	And write probabilistic program code that can be suitable for reasoning about this domain
3613360	3614800	Just on its own the LLM isn't enough
3614800	3618240	But you have to do some reasoning about the models that it suggests
3618800	3620880	And then those models when suitably
3621680	3627280	Reason to put upon can support novel reasoning in this domain and even sensible updating
3627280	3631600	So we're really getting you know, at least demos of steps towards
3632240	3637040	Computational models that can capture the richness of how we are able to think about new situations
3637040	3642960	Even ones that we haven't really thought about very much before or that we haven't been explicitly told how to think about
3643440	3645440	but using the combination of
3646080	3652640	Language the associative knowledge that's in it and an underlying ability for constructing probabilistic models of the world and updating
3653360	3656480	I'm the last thing I just want to leave you with is a set of thoughts about
3657440	3659440	You know, ridiculously
3660560	3662400	It is it is the last thing
3662400	3666160	But I think this will set up some of the discussion with with virginia and others too
3666880	3672560	Which is many people in cognitive science, whether in linguistics or other areas
3672960	3679520	Engaged in meaning have been interested in what you could call, you know a unified account of meaning and what we're trying to at least
3679920	3683280	Point towards steps towards this with the framework. I talked about
3684480	3686480	just to just to again
3687200	3689360	Raise controversial points for discussion if you like
3689840	3695840	um, but the idea that we can capture the meaning of a word in context as well as more generally as
3696400	3700000	Effectively, it's like you might think of it as a form of dynamic semantics if you're familiar with this
3700080	3706720	But the meaning being in in context in a discourse the incremental contribution to the probability distribution over
3707200	3711360	PLOT expressions in the problem that we're thinking about with a problem in discussion
3711760	3715600	And the meaning of a word or phrase or sentence or other unit of language in general
3715840	3722400	Is a is a higher order stochastic function that can take as input a discourse context and return as output a meaning in context
3723040	3725600	And the idea is that if we think about the different approaches
3725680	3726880	These are just four
3726880	3733280	You know traditional ways of thinking about meaning and language which all have great value and have often been seen as being competitors
3733760	3738000	Ways in which we can really bring them together ways in which the PLOT ideas that we've talked about
3738320	3745040	Can integrate the compositional logical aspects of meaning that formal semantics and other areas in the language of thought tradition have emphasized
3745520	3751600	As well as in the context of probabilistic language of thought and mental models of the world can give a powerful form of grounding
3751680	3753680	That's not grounded in sense data
3753760	3760320	But in our models of the world that's what the PLOT does and the LLM or more generally a statistical distributional
3760720	3766640	Sequence models can capture both the distributional statistical aspects of meaning. It's the base that the
3767680	3773440	Both the sort of distributional usage approach, but also more general semantic association needed to make sense of language
3773440	3778080	So flexibly as well as some of the very flexible pragmatic communicative ways we use language
3778800	3784400	So I'll just leave it at that. Okay. Okay. Could you turn off your share, please? Yes
3785920	3787920	First of all, thank you very much
3792240	3796880	There's not a chance in the world that I'm going to forget Virginia this time and I'll tell you why
3797600	3804800	Because I'm gobsmacked at how many different areas you're an expert in and so anyway, here's another one
3805200	3806320	uh
3806320	3808320	Virginia valium from
3809040	3810960	CUNY hunter
3810960	3812960	It's all yours. Thanks
3813520	3816640	Josh that was such a great talk and so rich
3817520	3824160	In 10 minutes, which is how much time you and I have will only scratch the surface
3826320	3829360	So departing from the comments I sent you
3830640	3832640	with respect to the
3833520	3836320	Unified theory of meaning
3838400	3840400	Many years ago
3841200	3843200	Jerry Katz suggested that
3844000	3846640	The question what is meaning could be
3847600	3855600	Separated into questions like what is sameness of meaning? What is contradiction? What is anomaly?
3856800	3859040	What is entailment and so on?
3859760	3865200	And it occurs to me that it would be interesting to try this
3866080	3870160	To try your model to see just how well
3871680	3873680	It can detect
3875280	3879440	Synonomy contradiction anomaly and so on
3881360	3886640	Yeah, I think that's um, that's a great connection. I mean I I know a little bit about that
3886720	3890400	But I you're inspiring me to go back and reread and learn much more about it
3890400	3892960	I mean, I think just in the context of the last thing I said
3893440	3896960	The idea exactly that in a sense in this framework at least
3897440	3903520	What the statistical language model is doing is capturing the notion of sameness effectively because there the distribution on
3903920	3907600	Code in the probabilistic language of thought if that distribution is similar
3908000	3910400	And that can be measured in different ways then you might say well things
3910800	3917600	Have the same meaning either in context or in general because and that that distribution function can be contextualized or it can be
3918000	3922400	Made higher order. Okay, so that's really interesting, but it doesn't compute entailment or other
3922720	3928320	You know a conceptual or inferential relations those come from reasoning in the probabilistic language of thought
3928880	3929840	um
3929840	3934400	And yeah, I mean I think that's that would be great to explore that more see if that can account for
3934960	3938800	Unify it both account for the different set of phenomena and unify in that sense
3939680	3942960	Yeah, I think that would be really interesting to explore
3943600	3945760	um, so going back to
3947520	3949520	Some of the more mundane
3950960	3952960	Which I guess is upon
3954720	3959200	Aspects um the way that I'm understanding what you've
3960320	3963040	Said about how the theories
3963760	3969120	Intersect that is how llm's intersect with basian models
3969760	3974480	Is basically you're using the llm as a kind of tool
3975120	3980640	To translate from one vocabulary to another vocabulary. Is that accurate?
3981520	3986080	Yeah, I mean and that is at the most practical level. That's what we're doing. Um,
3986960	3988480	yeah
3988480	3992320	Okay, more about that, but let's let's go with that for now. Yes. Okay. Um
3993360	3995200	so
3995200	4003040	The critical difference that I see in what you're doing and what llm's are doing is the
4003600	4009200	Well, I guess there are two critical differences, but for me the most important one is the use of symbols
4009920	4013200	um, the other part is the basian
4014880	4018320	Mechanism, but let's separate them. So
4019520	4021520	When you put symbols in
4022240	4027600	To me you're putting a lot of content into the mechanism. Do you agree with that?
4029200	4034160	um, I agree that adding symbols adds a lot of content, but I think
4035280	4037760	And I again, I didn't have time to unpack all of this very well
4037840	4044080	I think but when you say you putting in symbols the in different different things that I'm talking about here
4044400	4049600	The you and the putting in are different, but yeah, but whenever you add in symbols that adds a lot of
4050080	4056960	Content and structure here. Yeah. Yeah content. Right. So I'm thinking about the implications of that for nativism
4057520	4061200	So it seems to me. This is a nativist theory
4064480	4069200	Um, I would say it is compatible with some forms of nativism that I find plausible
4069280	4071600	although none I don't have a horse in that game, but I
4072480	4078000	Through a lot of interactions with friends and colleagues like lisbelki susan carry many others
4078400	4079680	Come to find
4079680	4084160	Certain kinds certain aspects of nativism and conceptual nativism plausible especially in
4084480	4086880	Certain core domains that are shared with other animals
4087360	4092560	Like intuitive physics about objects that the world is three dimensional objects have some that there's some kind of
4093120	4094640	physical interactions
4094640	4101360	That are you know our bodies engage with and also some forms of intuitive psychology not necessarily higher order belief reasoning
4101680	4106960	But the idea of efficient action and that agents have goals and they pursue actions in the physical world
4107040	4111200	Grounded in physics to achieve their goals efficiently. Honestly, there's evidence
4111920	4117200	Again, sherry lou who did this work in spelki's lab and continue to build on it has really, you know
4117600	4120400	I mean, I was a striking evidence in three month olds
4120800	4126080	But it's every experiment with three month olds is very small and you know a lot needs to be built on
4126400	4128400	But as striking as it gets in three month olds
4129360	4130400	science
4130400	4136160	I'm showing that that aspects certain aspects of not only physics, but efficient goal directed action
4136240	4138880	Understanding seem to be present that doesn't mean they're innate
4139120	4143360	But at least they're not they're present way before language and probably build on some innate stuff
4143760	4145760	It's compatible with that notion
4154560	4159600	I'm not sure who that is but could you mute that was an accident. I think they just
4160000	4165360	Should have turned off their okay, but so so yeah, there's this idea that certain kinds of symbols
4165840	4169760	Are used in our framework to describe those some of those core systems
4170080	4175040	But we are in contrast to like a fedoria nativism which says all concepts are innate
4175680	4180640	In this in this framework here. I mean it or you could say in contrast, although some of uh,
4181200	4185680	I don't know what jerry would have actually said about this but people like paul patrowski have suggested
4186080	4192640	You know, maybe he would love it. I don't know but in in a rather different version of like some forms of radical
4193280	4197840	Conceptual nativism most of the concepts in this framework are not innate
4198160	4200160	They're written in a language of thought
4200240	4204400	That it that could be somehow innate or somehow bootstrapped through natural language
4204480	4207360	I find some of the ideas that that uh,
4207440	4213760	Susan carrey and lis spelti in their different ways along with jesse snettaker have developed their ways in which language acquisition
4214240	4215280	and
4215280	4222000	Mental languages of thought might bootstrap each other and many things in the kind of glitman tradition also I think are reflect that idea
4222640	4225520	But I so I I think we we are
4226080	4229040	We still need to show this this is like the most interesting thing to do
4229360	4234320	But to show ways in which the things I was showing at the very end could be could be used
4234800	4239520	To explain how and model how natural language can start off
4240000	4248320	Being grounded semantically and logically in a limited symbolic vocabulary of probable worlds that reflect core knowledge
4248560	4253120	But then that can support bootstrapping and introducing new concepts via those mechanisms
4253200	4254560	I was talking about at the end
4254560	4261040	Including new concepts, but also new domain theories that we get explicitly or implicitly through our linguistic interaction with
4261360	4263360	Other people who we think know more than us
4264480	4267600	Okay, um, that sounds great. Um
4268320	4273120	It also suggests to me that it is a highly modular
4273760	4276800	System even though it's also probabilistic
4277360	4282160	um, so it's probabilistic within each of these different modules
4282800	4289280	And depending on what you think the symbols are that you start with you can
4290480	4294080	Iterate what you think the modules of the mind are
4295760	4296960	Yeah, no, I think that's right
4296960	4299920	And I think you know, I think it's this framework is not a
4300480	4303200	On its own a proposal for how our minds start
4303280	4307040	But it can be used to instantiate and build and test some of them
4307680	4311760	And so that's I think that's right. It's it does suggest that you could have, you know, different
4312800	4314560	Sub-languages of thought for different domains
4314560	4320000	But crucially the picture that you get with adults from this is both in some ways like strikingly
4320720	4323360	Supermodular and in some other ways completely holistic
4323680	4330160	So the striking super modularity is that the actual reasoning that you do in is in a discourse when I'm thinking about a situation
4330240	4333840	And we're talking in a conversation is very modular if it might even be just
4334480	4338880	Specific to this context this what Tyler Brook Wilson calls bespoke model construction
4339200	4343920	This idea that we might construct a model on the fly to think about a particular situation
4345040	4347600	That that we're that's the last thing that we've been working on there
4347920	4352960	Is in some sense super modular because that model is is relatively small and all the inference. I'm doing is just here
4352960	4354240	so I so I
4354240	4361120	Side step or avoid the classic problems of what has made Bayesian inference intractable the idea that if I'm going to actually have
4361440	4364400	A distribution over all possible worlds. I could think about it update that
4364880	4369920	You know, that's completely intractable many people in the nativist tradition dan osherson and others have written about that
4370320	4374480	And this this is a a way around that or as Tyler puts it in his thesis
4374480	4378800	It's in a sense a kind of way to think about a solution certain kind of solution to the frame problem
4379120	4386080	So it's super modular, but it's also very holistic in the sense that the world knowledge that's used to construct that
4386560	4390080	Comes from like all the code you've ever written and all the semantic associations
4390160	4395520	And it's like a gigantic holistic almost quinian web of language and code
4395680	4398160	What we've sometimes called the github in the mind view
4398880	4401920	There's the game engine in the head the github in the mind like github is this
4402240	4404640	You know thing on the web which was crucial for training
4405200	4409680	Language code models and you can imagine your own mind has lots of chunks of code
4410080	4414720	Some of which are maybe innate many of which are not and natural language interweave with it
4415120	4422080	And and having a content addressable associative memory that can use that and and marshal out from that
4422960	4424240	relevant
4424240	4429280	symbolic probabilistic models that can be used to reason about a particular situation in some sense that's extremely holistic
4429840	4436560	And maybe is necessary to grapple with what is in you know very clearly some of the wholism of human cognition
4439360	4441360	So going back a few steps
4442720	4445280	The intuitive physics part
4446960	4448960	In principle
4449520	4455360	Animals could do that as well, right? Yes. So a chimp could have intuitive physics
4456320	4459520	Yeah, and joseph call and others have studied that a number of other
4460080	4464320	Amanda ced and other brothers had studied other non-human primates and we're actually collaborating
4464720	4466880	With erica cart mill and some others studying
4467520	4471360	non-human primates on intuitive physics and I know colleagues
4471840	4474880	Who are studying analogous kinds of intuitive physics in rats?
4476080	4478880	And I think even simpler organisms
4479920	4484160	Okay, so the difference between humans and
4485120	4490560	Other animals is going to be that language allows you to go further
4491200	4494240	Language allows you to go beyond intuitive physics
4494640	4499680	Whereas nothing is going to help these other animals go beyond intuitive physics
4500640	4502400	Yeah, I I think that's right. I mean again
4502400	4506160	I wouldn't say that all the all the mental models of other animals is just intuitive physics
4506160	4511440	They also have models of their social world like you know the chainy and save farce baboon metaphysics is mostly a social theory
4511920	4513920	But yeah, but I would say
4514480	4518880	Definitely that the key thing here and it's the real human singularity. I would say
4519680	4525040	Is that language allows us to construct to both enrich our intuitive physics to think about
4525440	4529280	Aspects of the physical world that are not initially intuitive to us and maybe still aren't
4529600	4534160	But also to construct things that just go totally beyond any any core domain that evolution gave us
4534880	4535920	Right
4535920	4540480	Okay, so one question that this brought up to me
4541280	4547520	Because what you're talking about are all the ways that humans are so good at what they do
4548240	4549120	um
4549120	4553120	But then there are ways that they're not so good at what they do
4553680	4557760	And far transfer is one of those examples
4558480	4559760	um
4559760	4565280	And there there are some things that are hard for people to learn
4566000	4570080	Or some places where it's hard to get from
4571200	4573200	A to z
4573840	4576400	Even though you think you've learned a
4577120	4581680	So there's a famous little story about someone going to their logic teacher
4582480	4584480	asking about
4584560	4589600	Understanding if a then b and the logic teacher spends a lot of time on that
4590160	4592160	And the student says
4592480	4594480	Okay, I think I get it
4594880	4597440	But we try it with r and s now
4598240	4599600	um
4599600	4605040	So that's something that just shouldn't occur, but it does occur
4605760	4608880	Uh, even with smart people
4609840	4614400	So like when you try to teach people about experimental design
4615040	4618480	It's hard for them to see confounds often
4619040	4621440	um, it's hard for them to
4622400	4626720	Get at what's wrong with some experimental design
4627520	4633280	And it's not that they don't have a general intelligence. It's not that they don't have some principles
4633840	4635840	but it's as if
4635840	4638080	What they've learned is just
4638800	4647040	Too far away as far as the the string of examples goes for them to get to to the next one
4647760	4650320	So I'm wondering how
4651200	4652960	On your system
4652960	4656720	those kinds of limitations would be modeled
4657520	4663600	Yeah, no, that's a that's a great point a great question and a great pointer. I think to work that
4664240	4670240	We could and should do more of so the the stuff I talked about at the very end, which is again this the work with
4670880	4673040	Tyler and uh, katie collins and
4673760	4676560	Lansing and and and very much with leo wang also
4677280	4678720	um
4678720	4679840	is
4679840	4682720	What we're what we're getting out there like is is again how we can
4683600	4689600	Describe a situation in language, maybe even very implicitly just start talking and then see if
4690320	4696000	See if this architecture can be used to construct a mental model needed to reason about that situation
4696000	4698000	And you could also make it better
4698160	4703920	More robust if I explicitly give you instructions as in the educational context you're talking about if I try to explain to someone
4704240	4710640	Logic or experimental design actually another collaborator on that project said zhong is actually very interested for his thesis
4711040	4713840	In how we learn like logic and learn to reason through language
4714720	4718880	But so we have you know, our framework is providing possibly a way to do that
4719440	4720400	but
4720400	4722960	In order for it to work at least in the current system
4723520	4728880	We do the kind of thing that lm folks are generally doing these days, which is some kind of few shot prompting
4729200	4735360	We prompt the system with examples of other mental models and language for describing them in
4735920	4741040	Related domains they could be similar or they could be further and what we already can see right is i mean
4741600	4744080	The interesting thing is can you generalize to new domains?
4744240	4746240	So our system is somewhat able to do that
4746640	4751280	But you know, this is a place where near or far transfer would be relevant at least so far
4751280	4755360	you know as you might expect you have to have some domain that's at least reasonably close
4756080	4761440	And for example, we're trying we could transfer from like a tug of war to a relay race or to some other sport setting
4762160	4764800	Or from a couple of sport settings to yet some new sport
4765280	4765680	um
4765680	4770560	Or you might transfer from a sport setting to like a math competition or to some other kind of thing
4770560	4774080	So if it really interesting ideas about abstraction and metaphor
4774960	4779520	Analogy some of the kinds of things that the the transfer literature has studied are going to be relevant here
4779520	4784320	We're just beginning but what you're pointing to is actually some set of phenomena and things we should really
4784800	4787760	Engage with and I expect you know at least based on our current system
4788320	4790720	It will definitely struggle sometimes with far transfer
4791280	4796480	Um, which be which might be because at that level, you know, I I I drew this contrast at the very beginning
4797200	4799440	between the pattern recognition approach
4799920	4802240	Data driven learning and the mental models approach
4802480	4806720	But at that level we are doing a kind of pattern recognition. It's just not patterns in the world
4806800	4810560	It's patterns and thoughts and thought structures and ways of expressing them
4811040	4814640	And you know at that point if if if that kind of really interesting
4814960	4820160	But much more abstract kind of pattern recognition is what's driving your ability to construct new models of new situations
4820640	4825360	Or at least initially before you've had formal instruction or when you're just at the beginning of formal instruction
4825760	4832320	Then we should expect that it to have it to have the same kinds of fragility with far transfer that any machine learning approach does
4832400	4835920	Okay, sorry to be a gilljoy here. We have about 10 minutes left
4835920	4839840	But we have at least four people that want to ask questions. So what I want to suggest
4840400	4843360	to alina and julia and
4844320	4845520	also
4845520	4852400	Uh leijin and stephan carlson is to raise your hand again, and I'll recognize you in the order that you raise your hand
4853440	4855440	Okay, julia go ahead
4855600	4857600	well, um, so I was wondering
4857600	4868320	I was thinking like maybe you can use the um the LLM translator plus the probabilistic models that system to um
4869280	4876720	Model how the beliefs of a reader would change over the course of being told a story or like reading a book
4877840	4880320	And then you could look across
4881280	4885520	The genres or books or whatever to kind of get a sense of
4886000	4892320	What the kind of model of those stories being told since the stories are themselves a world model what that would be
4892960	4894960	and that you could incorporate
4896160	4900560	Because if you have a guess because something that you didn't like touch on in
4901200	4904640	You know in your act talk, which I'm sure you didn't have time
4904720	4911120	But was that like if you have a guess about where you are in the trajectory of the story you expect
4911200	4913040	Then you have a guess about
4913040	4915040	About what you think's going to happen next
4915600	4920240	Wrap it up because we um, so I was just wondering like do you do yeah, that's a great question
4920800	4922480	Um, let me just try to answer it really quickly yet
4922880	4926880	Leo wong would love that question because in addition to being a great cognitive scientist
4926880	4929280	They are also a writer they write stories and even novels
4929680	4935600	And one of the things that we've been working on is little mini like three act structures and things like that that
4936400	4938400	Follow either classic narrative structures or other
4939040	4943920	Things and and you know, maybe not surprisingly those are places where we see a big gap between
4944640	4946640	human story understanding and
4947120	4953040	LLMs even the state of the art ones, but exactly we're trying to use these models to capture how that kind of
4953840	4956960	You know journey of understanding might unfold and also even how
4957680	4961840	A creator might create it. So another student that we've worked with Karthik Chandra has
4962640	4968640	Had had some work at last year's cog side conference on storytelling as inverse inverse planning and the idea that like if somebody's
4969120	4973360	Understanding another character's journey by doing some inverse planning as in those theory of my models
4973600	4977520	Then a storyteller can try to invert that inverse planner to to convey
4978000	4979280	the emotional
4979280	4985520	Or mental journey that the character has and that's a way to use this toolkit for both story creation as well as story understanding
4985520	4990720	So again, it's those are mostly promissory notes, but great question and future research
4990720	4995200	I think from Karthik and leo and others will address it. Thank you. Thanks
4996000	4999360	Hi, I'm gonna ask a question in person josh. This is ever
4999920	5004240	Hi, I yeah go for it. I I really liked your talk. By the way, I watched it on video. Yeah
5004720	5006240	Cool, great. Okay. So yeah
5006240	5012160	I was gonna so following the work that you were showing where you were basically based also on what you were saying and then in the
5012960	5018720	Question period where you were, you know, few shot prompting these language models to produce probabilistic language
5021280	5024480	World models, I guess I have my I have sort of a more medic question
5024480	5030560	Which is do you see that as just a way to sort of help step by step reasoning in these models?
5030640	5033040	Or do you think this is sort of evidence for
5033760	5037600	Early sort of possible world modeling being learned in these models
5040080	5042080	Um
5043120	5048640	I'm not sure if I see it as either of those. Um, I I mean, uh, uh, so
5050720	5054640	I don't think of it as either doing step step by step reasoning in language models or
5055760	5061440	Evidence that they do possible world modeling. It might be relate. I mean, there is you know, I've obviously as you know
5061760	5064080	You talked about some and others have, you know, there's certainly
5064880	5069440	Um, a lot of evidence that llms if you try to just use them as end-to-end reasoning systems
5069440	5074000	They can benefit from step by step by reasoning and there might be some emergent world modeling capacities
5074000	5079680	But you know, again, I look at that and I see a really interesting and mixed pattern of successes and failures
5080000	5085520	And depending on who's writing the paper and what their agendas are you can highlight the successes or you can highlight the failures and the gaps
5086000	5087120	um
5087120	5092800	The to me that an objective perspective is that it's patchy and fragile although extremely impressive and really interesting
5093040	5096560	So the way I think of what we're trying to do is to say, yeah, there's there's you know
5097280	5103200	Various kinds of interesting approximate implicit knowledge that those models have that can be used sometimes to succeed
5103680	5106880	In reasoning complex sequences of reasoning or world modeling
5107200	5112560	But I think a more robust way to use it is the way we're using it a more both a more human like and a more robust for ai
5112720	5114720	way to use it which is to
5114880	5118400	Which is to condition and construct these bespoke world models
5119120	5121120	that you know where the
5121680	5124960	long chains of sequential reasoning or actual
5125920	5129200	You know coherent world modeling are there
5130240	5131840	By construction
5131840	5136960	But you know, there are other limits like again as as I was saying, you know Bayesian inference in really complex models
5137760	5140320	Is very difficult and I don't think people do it. There's a lot of evidence
5140320	5141120	They don't do it
5141120	5144560	But they seem to do it very generally in in just the right small model
5144640	5149440	At least in cases where they have the relevant world knowledge to construct those models and when they don't they don't
5150000	5151600	Right, so that's all you know
5151600	5154480	Our minds have mixed patterns of successes and failures there too
5154720	5159760	But I think this toolkit is better matched to the mixed patterns of successes and failures, which is
5160480	5165680	What what i'm trying to get out if that makes sense the the the one that we see in humans as opposed to the the weird head scratching
5165760	5172080	Like super intelligent cases in some places and then super dumb Jesus and others that you just see in a pure sequence model
5173440	5177040	I'm told Alina can go next. Thanks for your answer. Yeah, thanks
5178000	5180000	Good question
5181360	5183360	Alina pick it up or you lose your turn
5185680	5192720	Yes, um, hello, uh, thank you for insightful talk. Um, Josh so
5193680	5202880	You said that um, and correct me if I'm misheard that language allows us to construct meaning that it's totally out of bounds of evolutionary gain
5203520	5205200	um
5205200	5210640	Is if uh, if this is what you said, I think this is uh, super profound. So
5214560	5221840	Yeah, I I'm sure I'm not the only person who said that many people sure. Okay. It's just a reminder. I guess profound truth. Yes
5222640	5224640	Okay, uh, so the actual question
5224720	5230000	So, um, you mentioned that the meaning on the world is constructed contextually and incrementally
5230400	5236000	So could you please elaborate on how this process occurs and how it impacts our understanding of complex concepts?
5236080	5242480	So so we transition between the um from incremental construction of the meaning to generalizations
5244160	5246160	Right. So the way it works in our
5246400	5249280	I can just I can tell you how it works in the models that we've built so far
5249360	5252160	And again, this is not to say that it's exactly like this in the mind
5252240	5255600	But I think it might be something like this and a lot more work needs to be done
5255920	5260080	But the way it works in our models is something kind of like what you're familiar in a chat
5260160	5262640	Like if you've used a chat gbt or other
5263280	5265040	conversational ai systems
5265040	5267440	The way it's working is that basically there
5268000	5272240	You know, if you're used to using chat gbt you type something it types something back and in the middle
5272480	5276880	Some wheels turn and then you type something more in it wheels turn in the black box and it types more stuff
5276960	5278960	Okay, so it's it's basically like that
5279360	5284480	It in that you at each each sentence is translated into some
5285120	5292240	Expression in the language of thought roughly at the sentence level. I think sentences are real units of meaning like I I am very
5293200	5295920	respectful and admiring of many insights from language
5296160	5300880	Including that words are real and sentences are real and there's real syntactic structure to be understood there
5301120	5304320	That is only being approximated in some ways by these models
5304320	5309680	But the key is that yes, there's a process the contextual process is like at the discourse level
5310000	5312800	You go sentence code sentence code sentence code
5313200	5317120	And each sentence to code translation is conditioned on the previous
5317820	5322000	Conversational history or the discourse that that you have been interpreting. Okay
5322640	5324240	That's just a first approximation
5324240	5329600	Just another thing you might want to do and surely you'll have to do is go back and edit previous code because if you realize
5329680	5335040	Oh, I misunderstood something. I can't just add new code. I have to go back and edit the code that I wrote before
5335120	5337120	Which is also something that code llms can do
5337680	5339360	I'm not saying they'll do it right
5339360	5343600	But there's other processes that involve like checking and refining and fixing models
5344160	5345120	That are written in this way
5345120	5349760	But maybe that gives you some sense of the contextual dynamics of how language is understood in this model
5351440	5353440	Last question lay jim
5355600	5357600	Hi
5358640	5362080	Joshua, thank you for for the presentation. It's very interesting
5362640	5367520	Um, I have an engineering background, but I'm not an expert in
5368400	5370400	artificial intelligence
5370400	5372400	Uh, I have a few questions
5372880	5373920	um
5373920	5376640	From your presentation, I understand that
5378240	5380240	actually llms
5380240	5382240	the
5382320	5387360	The the it's it's kind of a gold mine of human knowledge
5387760	5394480	And uh, you seem to say that they have a modelized the real world facts and the logics are quite correctly
5396400	5404080	If you ask the question directly in human language, uh, currently they don't seem to to give the good answer
5404720	5407600	because some with some lack of
5408480	5411600	algorithm, but if you first translate the
5412480	5414480	human questions into
5415200	5417200	functional code and like
5417600	5420800	The work you are doing and then they are capable of um
5421680	5426080	I know we execute the code then we we are capable of having
5426640	5432800	Result results that are quite close to human behavior. Uh, so is that the correct?
5434400	5436560	Yeah, that's I mean, that's more or less correct
5436640	5440560	But I don't want to I mean and that's that's what we've been doing in these examples
5440560	5442560	And what I've been showing but I don't want to
5443040	5444560	Claim I don't and I don't think it's true
5444640	5448800	But certainly I don't have the evidence for it being true or how true it is that
5450400	5453120	You know llms like always do this, right? There's a lot of
5454320	5458640	Ways in which language is only an imperfect reflection of the ways that we think
5458880	5465200	There's certain things that again are partly based on evolutionarily ancient core systems. The language is not very good at expressing
5466240	5470000	And that includes also spatial reasoning and you can see ways in which
5470640	5474720	Even the best language models break down here and language multimodal language vision models
5475680	5478560	Have have had persistent problems always from the beginning there
5479200	5483520	So I think you know, I don't want to suggest like I think yes language is a treasure trove
5483920	5490400	Of knowledge explicit and implicit about the world, but it's and for us it's it's such a valuable resource as human beings
5490480	5498400	Okay, that's why it's no accident that language models when they're trained to capture patterns on all the language that humanity's basically ever produced and put out there on the web
5499120	5502960	Uh, you know start to have remarkable properties at the same time
5503600	5509440	It's only some parts of our knowledge about the world is key parts to actually understanding and being in the world
5510000	5512240	That basically nobody ever talks about
5514480	5521360	And even if they do talk about it they talk about it very incompletely and imperfectly and yet our brains are designed to understand in those terms
5521760	5528320	So I I don't want to convey the idea that somehow it's all there in language and it's and that's all you need
5528400	5530400	by not by any means
5531280	5534880	Wonderful before we, uh, let people applaud
5535680	5541840	You know, there's some several panels coming. Are there any of them that you can join? They're not today, unfortunately
5541920	5548640	Yeah, um, I I need to check my calendar and coordinate on some family things and I will I'll I'll try very hard to join one of them
5548800	5550800	Okay, now we'll applaud you for
5551760	5553760	Thank you
5553920	5564080	Thanks, and and thanks so much for the discussion Virginia and all the all the questions. There were great questions. So I hope to engage more
5564320	5565360	um
5565360	5568960	in one of the panels and um in person if if anyone hears
