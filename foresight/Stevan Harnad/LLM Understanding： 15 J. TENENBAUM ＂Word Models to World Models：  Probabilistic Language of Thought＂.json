{"text": " I forget now whether you do have a discussant or not. Yeah, Virginia. Oh, it's Virginia's gonna. Okay, great. So She's here. Yeah, she's there. Hi, Jenny. I've been Virginia. I mean Have you Josh, you haven't seen any sessions at all yet, right? Um, I've seen I haven't been able to join live, but I watched video from the first two days Did you watch belkin? I did. Yeah. Okay, great. Okay. I was even gonna refer to it briefly. Yeah. Okay. That's why I don't want this to explain. Yeah Um I'm I'm also trying to see if there's some time I know you said that there might I might be able to join I can't join the discuss or there is no discussion today, but um I'm trying to see if there's any one of the other panels that my schedule would be great. I can add Yeah, I'll I'll uh I'll I'll be in touch about that. Okay. Let me just Solid duty here Welcome everybody to the afternoon session everybody out in in the Distributed land and everybody here. I'd like to um Introduce Josh Stendenbaum who's professor of computational cognitive science in the department of brain and cognitive science at mit He's a principal investigator at the computer science and artificial intelligence lab called sea sail. Is that former sail? No, uh sail is a stem from sail But it's formerly the AI lab and the lcs laboratory for computer science Great. Yeah, and a thrust leader in the center for brains minds and machines cbn And his papers are on perception learning common sensory reasoning in humans and machines with the twin goals of better Understanding human intelligence in computational terms and building more human like intelligence in machines In other words, he's cut out perfectly for this Summer school. I hand it over now to Josh Stendenbaum Okay, great. Thank you so much, Steven for organizing for inviting me You know, I I got to watch much of the first two days and it was really interesting to see a back and forth between people who are Extremely impressed with large language models Both on their language abilities and maybe some of their general thinking abilities and other people who are much more skeptical That they have really anything to do with intelligence at least of the human form The work I'm going to talk about here is a is an interesting I think mix of those two perspectives And I hope that will Be useful stimulating Engage in some interesting discussion both now and going forward over the next two weeks The the heart of the talk, which will really be more like the second half is based on this paper Which you can find on archive from word models to world models Understanding natural language by translating into a probabilistic language of thought But I'm going to spend the first half setting some context on just how we think about thinking and then it's built on that for the relationship between language and thought This is you know, I would say this is both an AI talk and a cognitive science talk Although most fundamentally to me. I'm really interested in the computational structure of the human mind So just in that hear it in that lens But I think also just sort of very generally You know, nobody can fail to be surprised and impressed At what has happened with the most recent machine learning models And at the same time they're they're very puzzling and confounding in certain ways So I hope to try to be able to resolve some of that or at least Point the way towards some of that This paper the work i'm going to talk about is joint with a number of people I want to single out two people Lionel long and Gabe grand who are the joint first authors of that paper I'm putting Lionel a little bit bigger In part because a lot more of my slides are taken from lines So I would say all of the credit for the good stuff both on the research and in the slides to Lionel and Gabe And you know anything that doesn't quite work or make sense is probably just me garbling things Okay But a lot of other people contribute to the work that i'm going to talk about here So again, you know, we can't who cannot be impressed with the advances of AI Whether it's in perception and and you know robots deployed in the real world Like the self-driving car systems of tesla and wemo or most recently conversational AI systems like chat gpt Clearly there's something really interesting and important happening But at the same time something quite puzzling now There's a number of puzzles that people here have talked about just one I'll point to is what you might call the puzzle of confabulation Which points to what ways I think that the kinds of What intelligence or unquote intelligence that we see in these systems is different in nature and origins from that in our own minds So think about for example in the context of driving. This is a video from tesla A self-driving system where it's it's faced with something a little bit out of its training set a horse drawn carriage and and Buggy and it interprets it as a truck first facing one way the other then it's SUV then it's a truck with the person behind it Now the person's gone away Shortly the person will come back You know, they can't figure out which way the truck is facing If a person were having this experience on the road, you would think they should stop driving and get off the road as soon as possible Right. There's something fundamentally Detached about their mind from reality here And when this happens in systems where lives are at stake, it can be deadly. So this is a serious worry technologically and Sightly now when it comes to computer vision and tesla self-driving a lot of progress has been made What I showed you what i'm showing you here is from a few years ago Um, but you still see similar kinds of things and also on the language side I'm just going to show one example of analogous kinds of things that really make you scratch your head On the you you could find these in language models, but i'm going to talk about multimodal language and vision models because this is where You know, there's a lot of really interesting stuff going on Where machine learning and especially like grounded language Potentially might meet up with what goes on in cognition um, so this is a i'm just going to show you a couple of little demos from Google's large language model multimodal language model. Um, gemini both earlier versions and then the most recent version This is the figure i'm showing here, which i'm giving to gemini and I should say you can see similar things with chat gpt But it's a little bit funnier with the google ones. I'm going to focus on that Um, this is an image from my colleague vikash mansinga whose work will also figure in the talk Um, vikash is one of the world's experts in probabilistic programming He and I are very interested in how our minds and our machines could model uncertainty So here's an image that that I think all of us when we look at this It's it's it's pretty clear that though. It's a little hard to read kind of messy It's really clear that there's a word here and that the word is uncertainty Um, but what is uh an lm a multimodal lmc from this? Well, it might see this the image you sent me shows a black and white image of a text box The text book has a dark gray border rounded corners. Good inside is the word dream written in a bold sans serif font Is there anything else I can help you with? I ask how many letters are shown in the text box? There are five letters shown in the text box the letters are d r e a and m What is going on with this? Now we could say all right. Well, at least it's pretty good I mean at least it's recognizing that there's text and it's you know, it could be worse But it seems to be strangely self-confident strangely confident about an incorrect answer But you could say at least it's not like seeing a person or person walking down the street But consider this small change to the input image now our lm says this shows a black and white pixelated image of a person The person is facing forward and appears to be standing still. Here are some details about the person They have short hair. They're wearing a shirt with a collar. Their legs are slightly wider than their torso Since the image is pixelated It's difficult to discern other details such as facial features or specific articles of clothing Now again, the thing that I think should make us puzzle here is Is not just that the system gets this wrong, but that it seems to confidently assert things that are so detached from our sense of of the reality of what we're seeing And where just a small change can make such a huge difference Just to update things this was this was from the first version of google's gemini the most recent version of gemini 1.5 pro Has gotten better in some sense Now given uncertainty. It doesn't say dream. It says optical illusion But it's still wrong and it's still over confident. Um, it rates itself as fairly confident That it says optical illusion. It's interesting. It makes you think is there some semantic association between dreams optical illusions and This particular way of presenting the notion of uncertainty. I don't know. Um, The the very most recent thing this was well, it's not that recent but the last time I gave this talk Um, I tried this on the the newest updated version of gemini 1.5 pro just to see if perhaps it had been improved And now it gave a very different answer It didn't say it didn't read it as optical illusion. It gave this Extended digit number six one two nine eight one five three two four two and I asked it to judge its confidence and it said Well 70 to 80 percent Again, what what happened? Just to check again consistency and coherence of this Weird interpretation. I asked it just 10 seconds later just in an out of context again Just gave it the same thing and the next 10 seconds later. It said geocache Um, again now 75 confident and then one more time Um, a distorted text appears to read northeast blackout However, it's difficult to be certain But it's moderately confident that the text reads northeast blackout because the first and last words are relatively clear While the middle word is slightly less discernible, but still suggestive of blackout Okay, so what's going on here? That's one of the mysteries now I'm not I'm not I'm going to gesture at what I think some of the mysteries or some of the answers might be But mostly to point the way towards the difference with human intelligence and what we're trying to understand in in artwork Okay, I think fundamentally what's going on is whether you're building a computer vision system or An llm or a multimodal language text system You're building a system that takes the inputs and outputs that our brains do perception of the eternal world sense data of some form and it produces actions or something like actions that can be Grounded back in the external world, but the inside of the system doesn't have any notion of a world It's a function approximator It's learning to approximate the input output functions that our minds produce And it's learning to do that from various data sources including objective data sources as well as human reinforcement Now how why does this possibly work? Well, again, there there may be some laws of physics things that are like physics like for example the famous scaling laws of neural language models Where you can show in some form Especially when you're trying to predict like you know And this is very much following some of the information theoretic ideas that richard has been talking about and others in the group here If i'm just trying to predict the next token from the previous ones There are certain fundamental power laws of language and distributions in language that these systems seem to incorporate And build on such that if you increase by an order of magnitude the amount of compute You can in a predictable way lower the test loss in predicting the next token. That's on the left In gpt4's technical report They suggested that you could see similar kinds of scaling laws for problem solving not just text prediction Although it's a lot if you're there And I think fundamentally the problem is the power laws are beautiful laws and they're predictable in a certain sense The thing about a power law is while it approaches asymptote Or it rather approaches zero error in its asymptote It in some important sense never gets there compared to like an exponential decay Where there's a predictable timescale on which it will get to zero a power law keeps slowing down and if you have any uncertainty in the power laws coefficient or You know in its applicability to the not just to predicting the data stream, but actually solving a problem Then it's it's basically impossible to know how much data and compute you're going to need to actually get to the asymptote Which we want to call, you know full adult human intelligence In contrast Humans don't seem to be built this way. We our minds seem to be built as world modelers from the start And I'll say more about this, but from the very beginning We have our minds and our brains and this is something that Inherits shared with other animals and inherits from our evolutionary legacy seem to be built to model the world And to deal with all kinds of incompleteness and uncertainty And that means both in how in the the structure of the world And our uncertainty over what's out there the current state as well as the the fundamental deeper laws The causal laws of physics or how agents plan and so on But we're built to engage these kinds of mental representations As i'll as i'll show in a little bit the way in our group We've modeled this for a long time is what we call the game engine in the head by analogy to say video game engines That our brains and minds seem to be built with these kinds of resources for world modeling You can see that this the the notion of world models that are used Even not just in human adults, but in young children to coherently understand the world and deal with situations that are Importantly out of distribution Right and that's really key The ways in which we perceive and learn about the world are not nearly as time to the distributions of our experience as machine learning or function approximation So here's some examples of different kind of self-driving system. These are four-year-olds for the most part three four five-year-olds In a genre of youtube videos, you can see one of them actually the video on the left It's it's sort of the the video genre on youtube is like, you know My four-year-old driving for the first time and parents have put their kids behind the wheels of golf carts or tractors or cars or trucks And just go at it and they video from the side or from next to them and You know, you could question the rationality of the adults who are putting their kids in the situation and then putting the videos online But there's a certain basic rationality that the four-year-old has Even though they haven't been in the situation before the systems that their mind has built through evolution and the last couple of years The first few years of their experience Allow them to handle this totally new kind of perceptual information The world's going by them at a high speed and they're in control in some way But they can generalize from their from from their experience because the the nature of their generalization Is their mind's models of the world and those transfer to these new situations? Of course, their new things they have to learn about exactly how the steering wheel works But they can learn those very quickly too because the learning is grounded in their world model So fundamentally, I think what we have is the is a contrast between the scaling thesis of deep learning and today's ai And what you could call the original scaling route that human intelligence follows what we might call Growing up as opposed to scaling up and I want to contrast Three points, which I think are really important for understanding In general our intelligence and the contrast between ai and machine learning and especially the role that language and language models play in So three points that are that are fundamental to the way today's ai is working based on deep learning one That intelligence is seen as the end result of learning We're learning starts with some simple very general and in some ways dumb mechanisms associative learning prediction, you know in some way But the idea is if you scale that up enough you have the surprising emergent phenomenon that now we start to call intelligence um The the problem is though, I mean either you call it a problem or you can call it just the way things work But there's some remarkable kind of generalization But it still is going to depend on similarity to training data And its ability to go beyond the training distribution is is weak and unpredictable compared to when you're in distribution Um, this is true whether you're talking about perception or other forms of higher level cognition But especially thinking things that we recognize as forms of reasoning planning problem solving and so on The the sort of thing that we've never seen in any computer vision system But we're starting to see some interesting kind of approximation to in language models Crucially that in machine learning thinking derives from language Because it's only the data of language not the data of pixels that conveys information really about human thoughts Which makes sense again humans have always used language as our main medium to express our thoughts to others Just like what we're doing right now in giving talks like this, right? um But crucially thinking requires language data and even carefully cleaned and curated language data Our colleagues who are at the big ai companies can tell us a lot more about this or rather Maybe they can't tell us about this but they can tell us that they can't tell us about And those of us who've worked with this know the importance of having the right kinds of language data as well as the right reinforcement data Okay, so contrast this with human minds and especially human children Our intelligence if for our intelligence It's not just the thing that is at the end result of all of our learning although we do You know babies are not in some important way not as intelligent as human adults we would like to say But human intelligence is is built in in some form from the start. It's the foundation of learning. It's not just the end state It's what's there and the the learning mechanisms that let you learn so much from so little Okay, and we saw some of that in virginia's talk and you know I think the the field of human cognitive development is a testament to this or what I was just showing from the four-year-olds Is another example but in other work that i'm not going to talk about here We've done a lot of work trying to model the core knowledge of even you know 12-month-old or 10-month-old babies and there's important ways in which they have a common sense understanding of the world That is in in in significant ways. It's not all built in but in significant ways It seems to be present even even in two and three month olds. Okay um As a result of our built-in capacity for modeling the world our generalization is based on the fit of our mental models It's not about the training data. Okay data is important. We improve our models. We grow models We can make new models from data, but what accounts for generalization and what drives generalization is not similarities to the data But the fit of our world models and their ability to be flexible and themselves to be generalized And crucially thinking isn't the thing that comes at the end state as an emergent property of modeling language But rather thinking is there from the start and it's the basis for language It's the basis for why human children construct language so robustly and resiliently and again there I'm referring to By using that word resilience to some of the ideas that virginia talked about in her talk and susan golden meadow has talked about I urge everyone to watch her rumelhardt prize talk where she talks about this as well This idea that as we saw in susan's work and annie sangus's work with nicaraguan sign language That you know children who grow up who grow up without Any language input? It's not just about the poverty the stimulus in the traditional linguistic sense But you grow up with deaf without sign language input and you create in some form your own personal proto language Or at least a way of communicating that has hierarchical symbolic structure And then you bring a few such children together and within the span of a couple of generations They've created a whole new language from scratch. Okay, so it's very clear from data like that That humans are built to think and we're built with an A desire to understand and to under to be understood to express our thoughts In some form and share them with others in our as our social partners And that's you know, there's nothing I think more fundamental to To understand if you want to understand where our intelligence comes from So what we've been trying to do in our work is to try to capture the human growing up scaling root in computational terms And it starts with this idea of probabilistic inference and Expected value decision-making on top of world models, you know informally I would say and like many others that thinking not just in human brains But in brains and in many other animals is about making good guesses and bets not about the next Data thing you're going to see not about the next token or the next pixel or set of pixel values But about the world what's going to happen in the world and how It might depend on your actions or the actions of others or how you might be able to change the world to change those dependencies causal and counterfactual reasoning and then having some sense of what you'd like to see happen Or and what you'd not like to see happen or what could really cost you your life or Be hugely valuable and making good bets about how to act and what to think about next fundamentally this classic idea of effectively rationality that You know, you could say our minds and brains are the original sources of this idea through evolution Being constructed to do this kind of computation In our work and this is the stuff we've been doing with language models builds on this We've been using the idea of probabilistic programs, which is a family of mathematical languages and actual programming languages and platforms To embody this idea basically take this conceptual idea of rational world modeling inference of decision And turn it into practical engineering terms that can be models of human minds as well as more human like ai And I don't have time to give a whole introduction to probabilistic programs But you know, you can think of it as a kind of catch-all phrase or a complicated suitcase phrase just like neural network Packages a number of different things together, but probabilistic programs are formalisms For combining what I think are several of the best ideas about intelligence that have come up through through a number of You know decades in the field that includes neural networks modern probabilistic programming languages Like gen from vikash man singha's group who I mentioned before at mit or pyro That was developed by originally a group at uber ai that noah goodman Another collaborator of ours helped to start along with a number of others you know In many ways build on languages like pie torch Or tensor flow the languages that support modern deep learning and that allow you to construct really complex but end to end differentiable Functions for approximation or other purposes But that's not the most important part the most important part are building on the idea of symbolic languages for expressing abstract knowledge for modeling the world that is You know across many areas of science and engineering consistently the strongest most powerful toolkit we've had for building coherent models to To understand the world are various forms of symbols And then the third idea is the probabilistic one using those symbolic languages to express probabilistic models where you can be uncertain about Everything and in a in a turing computational sense You can be uncertain about the state of the world right now or more abstractly how the world works How the different kinds of data that you're getting perceptual or others are connected to the underlying state of the world And then to be able to do joint inference about all those different sources of uncertainty as the basis for perception reasoning planning learning and so on And probabilistic programs bring those things together I'll say a little bit about how that toolkit works as well as then how once you learn language The ability to externalize and internalize thoughts that are produced by these kinds of probabilistic programs basically That transforms things in fundamental ways and here, you know, less people think that i'm just a Deep learning or a llm skeptic or something I you know, again, I find neural language models and the the long tradition of distributional statistical learning In language that they build on to be really important and in the work that we've been doing we've been using large language models or in many by today's standards I might even say small language models to capture this actually So I think this is a place where the you know, I would not say that you want to think about human minds It's llms or or transformers all the way down nothing like that But the kinds of things that are going on in distributional sequence learning Could be a way to capture some aspects of how language grounds in these tools for mental modeling and and also enriches and extends it So, I mean just very briefly for people who aren't familiar with this idea There, you know, for a couple of decades now I and and many colleagues Students former students a number of others who are very interested in what you could call the Bayesian approach to cognition Have been using this general toolkit of probabilistic inference over structured symbolic models To capture many aspects of mental models and I just this is a plug for a book that's coming forthcoming edited by tom griffith's nick shader and myself and with many many other contributors And it'll be coming later this year from mit press But it's it's part textbook part research monograph and just showing how this toolkit can be used to Capture to both explain in a principled and you know, actually understandable way But also to quantitatively predict and model behavioral data across so many different ways in which our minds model the world Just to illustrate one which i'll come back to in the context of language in a minute An area that we've done a lot of work on is intuitive physics As as a kind of probabilistic inference and the particular kinds of intuitive physics We're thinking about are in these complex seed understanding cases, which again not coincidentally I think are also really interesting in classic settings for studying grounded language So for example, if I show you a Scene of a bunch of blocks think like jenga blocks stacked up in various ways Some of these images might look very stable others might look unstable and I can ask you a question How likely do you think this stack of blocks is to fall under gravity? And we can model that by having a a structured world model which consists of basically 3d object models And then causal models of how though of how those underlying 3d scenes give rise to images That's like a graphics program basically the computer graphics is a way of writing programs that generate images From those underlying 3d world models But then there's also these physics programs and again here I mentioned game engines So game engine style physics simulators capture a lot of aspects of effectively real world But common sense physics in ways that hack hack Newtonian or actual True scientific physics in all sorts of ways to be efficient But to a pretty good job of capturing What we expect to happen in the world which makes sense because we're the ones playing the video games and they're designed for us So by doing probabilistic inference to infer the input to a graphics program given the output Which is the image you can do a lot of 3d perception and then by doing probabilistic forward simulation You can imagine what might happen next. It could also be conditional on your action So the same toolkit could be applied to a much less familiar sort of judgment Like imagine I have these scenes of red and yellow blocks on a table And what if I bump the table hard enough to knock some of the blocks onto the floor? Will you knock off more red blocks or yellow blocks? So the first judgment the one about how likely the stack of blocks to fall I mean anyone who's played jenga or anybody who's ever been a kid probably has built things If you're a professional builder, you have a lot of other intuitions But you know, that's very familiar the question on the right is one that unless you've seen me talk about this You probably never thought about it's not something you have direct experience about you can't learn it from feedback How to answer that question? But I can use my language to give you that question as well as the other relevant world knowledge And then you can reason about it and the models that we build can capture both the familiar judgments of like How likely the stack is to fall as well as these novel judgments Relatively equally well So these scatter plots are examples of the kind of data and modeling that we've done in our lab for a long time On the y-axis we're plotting the average human judgments of let's say on one to seven scale how stable or unstable the blocks are and on the x-axis the average Result of doing a small number of probabilistic simulations where we imagine running forward Game-style physics a few time steps with uncertainty about exactly where the blocks might be and how the physics works because we don't There's are all things that our minds don't fully know The same kind of model can be used to answer these red and yellow questions. So just to illustrate this If we take one of these Scenes and we reconstruct it in a game-style physics engine and we simulate a bump of the table So there's one simulation on the right. I'll show you another simulation with a harder bump Okay, and you can see watch it again that different things happen in these two simulations But it doesn't really matter which one of those you ran to answer the question, right? I mean you look at the scene also and it's very clear from the beginning that if I bump the table Relatively hard. It's going to probably be mostly yellow blocks on the floor. How do you do that? Well in our model you only need to run One or a small number of these simulations to answer the question at the grain of intuitive physics And you don't need to run it very long. You could stop it now and you already know the answer Okay So a small number of short incomplete simulations is the basis for these pretty Quantitative models and they've also been used in robotics to actually predict and and be able to get for example a robot to Learn to play the game genka, but learn from a very small amount of realistic human experience The same kind of idea has been very impactful I think even more impactful in intuitive psychology Where we and many others this there's work that I've done with a number of students and my colleague at mit rebecca sacks But especially I would highlight work from chris baker and julia har edinger who did quantitative modeling here Going back years with us julian continued chris is now working on self-driving cars and industry julian is now working on It's now a professor at Yale where he's he's extended on this kind of approach in many areas of social and communicate with social cognition and communication in really interesting ways And many other colleagues here I and I should mention also though I didn't I forgot to mention this before the intuitive physics work started in our group More than 10 years ago with pete patalia and jess hamrick and others And they are now actually both working at deep mind google deep mind working on various interesting kinds of deep learning approaches For both intuitive physics and all sorts of real-world physics that matter like climate modeling. That's where pete's current focuses I won't go into the details on the bayesian theory of mind But the basic idea is again, there's a program that now describes Not necessarily how actual minds and brains works, but our mental models of other minds how the actions we see agents take We interpret them as the effect of planning programs that take as input The our minds representations of their beliefs and desires And by seeing how agents act and change the state of the world and modeling also their perception process Which leads to belief formation and updating we can model many aspects of how people understand Other minds especially in what you might call these sort of physically grounded and perceptual scenarios the kinds that you can also Study in young babies like in the experiments. I'm showing here from gargay chibra. Kylie hamlin paul bloom and others Um the the intuitive psychology case is especially interesting Because in work that we've done like in this paper with uh, brendan lake tomer olemann sam gershman that helped to set some of the Current stage of debates between deep learning and more cognitive approaches to ai Where we put about a challenge back in 2016 and 2017 For the the deep learning ai world about how to think about the kinds of inductive biases mental model Materials that seem to be built into human minds and the learning mechanisms that build on those We highlighted both intuitive physics and intuitive psychology in the famous sparks of agi paper showing again some of the most earliest glimpse into some of the surprising things that gpd4 seemed to be able to do bubeka and colleagues also highlighted intuitive physics and intuitive psychology and then in a recent paper from eric schultz's group Which is kind of forms a a three-part story here and it continues as as i know bushoff and and schultz and colleagues Continue to work on this area and we also do Is to now in the era of these multimodal language and vision models like i showed you at the beginning We can go and take on all these tasks like whether it's block tower intuitive physics things or agents moving around in the world and interpreting As julia harr edinger called it the naive utility calculus of their rewards and costs And give language models or multimodal models the same kinds of stimuli and questions that we give people And as schultz and colleagues showed In some of the intuitive physics settings, they're kind of okay. They're not great In the intuitive theory of mind ones they fail altogether. That's a quote from their paper So as an example, this is this is work that julia harr edinger did as part of his phd thesis with myself and laura schultz Um where we call these the astronaut studies where people would see an agent an astronaut on some planet Who would start at a certain point and have a home base that they had to get to and they would follow some path along This the surface of the planet They could just go straight to their home base or they could take a different a not straight path Um crucially there were various objects that they could pick up that could either be positively valuable or aversive to them And we asked people based on the path How likely do you think that the agent likes or doesn't like one of these objects? And also there are different terrains and the terrains could be more or less costly And by showing people different maps different configurations of objects based terrain and different paths You could get very interesting rich inferences about what about what the agent both wants and What the rewards assigned to the different objects are as well as the costs for moving around on the terrain So here's an example of some of the stimuli from one experiment Just to show you the kind of variation and then in each of these cases We can ask people to make four judgments or three or four depending on how many kinds of terrain And that's what's shown here. These are these are the actually the predictions of the model Z scored for the relative cost for the different kinds of terrain and the relative value for the different kinds of objects And we're assuming that the agent basically takes in a of rational efficient plan trying to maximize reward Minus cost where there's a small cost for each step, but especially costly when you travel over certain kinds of terrain So seeing the path you can make inferences about how the agent has rewards and costs And then when you ask people to make the same judgments, they line up almost perfectly This is just one of many experiments that julian did showing what is really a remarkable quantitative Match and but but by a model that isn't just fit to data There's a little bit of fitting but it's mostly based on Thinking about what are the core concepts of theory of mind that people like I mentioned before like gargay and chibra Have studied even in very young infants pre-verbal infants infants who can't even walk And barely just able to reach for things themselves yet They still have these ideas these intuitions about efficient inference and and use those it seems I mentioned sherry lou's work who was a PhD student at harvard a few years ago with this velki who worked with tomer omen also in me To show those same kinds of things work in babies sherry's now doing amazing stuff extending that in a new lab at johns hopkins Um, so these these are cases where this kind of probabilistic program model works really well, but just a pure language model Uh is basically at chance zero correlation with human judgments, although quite good at telling you things like the background color of the scene Okay, so in the last part of the talk having sort of set the stage for how are our Human thinking seems to work and how we can model this in ways that are explanatory Understandable and quite quantitatively predictive using these probabilistic programs. Let's understand where language comes into picture I don't think that a pure machine learning transformer approach like the ones we've been talking about Are on track to give a human level or certainly a human like account of how it works But I do think the ideas of sequence modeling statistical distribution learning that you see having such great success in llms Even perhaps on a smaller scale could say something important about how language comes into the picture So this here is the at the beginning I talked about this archive paper the word models to world models paper From line along Gabe grand and colleagues and that's what i'm going to be talking about here is how we've been using bringing these tools together But a key ingredient is this I would say the modern return of the language of thought hypothesis You know made famous by jerry fodor, but obviously with a history that goes back hundreds if not thousands of years But in the number of recent papers And proposals from various groups the idea that again that there's some kind of Not necessarily single language, but but abstract symbolic languages, which could be general or also Created and constructed domain specific languages Some kind of abstract symbolic language is seems to be a powerful way to think about human thinking abstraction concept learning and so on And the particular kind of languages of thought that we've been thinking about are What noah goodman and toby gersenberg and I called the probabilistic language of thought So this is the idea of using probabilistic programming languages the technical tool I talked about before To to formalize in a cognitive setting a certain kind of hypothesis of a language of thought But one that is focused on modeling the world Not just possible worlds, but probable worlds and where the the symbolic language can also express ways of conditioning and query So that we can ask and answer the kinds of questions that our minds do and that We might we might want for example any kind of general ai system to do So if you want to learn more about the probabilistic language of thought check out our chapter in the conceptual mind This is one of the margiles and laurence Readers or the web book probabilistic models of cognition, which which has examples of the kinds of models I'm going to be talking about Not with language models though the new thing is to take advantage of llms and specifically the fact that Most llms these days are trained not only on natural language But on on programming languages and source code Namely programming languages like all the ones we're probably familiar with that are designed to be read and written by humans And not just machines and so they're written in a very english like way Or natural language like way, you know linguists have long pointed out Sometimes the differences between natural languages and programming languages, but from a certain perspective They're a lot more similar than they are different A hierarchical structure syntax and even just much of the lexicon, you know natural languages They're not just commented or programming languages are not just commented in natural language But functions variables data structures are named using english for the most part Okay, so that's a very powerful data source that allows Some kind of statistical sequence to sequence model for predicting and translating between streams sequential streams to effectively learn to translate From english or any other natural language into programming languages of thought and that includes probabilistic programming So the idea of this paper what we call rational meaning construction. That's the name of this Is is a particular thesis on how language is understood and perhaps also how it might be produced and learned Although our focus here is on language understanding And thinking about the relation between language of language and thought in that context By thinking what we think of as like the core original notion of thinking is what I've been talking about for the for the first part of the talk Which is the idea of having a Structured probabilistic model of the world conditioning it on observations and then And then drawing samples of underlying latent states and future states Okay, that's thinking from this standpoint And then understanding language is effectively translating from natural language Into a probabilistic language of thought that's used to define and condition and query the probabilistic world model So we're going to exploit the the property of these LLMs to translate from natural language to code in a way that might be familiar if you've tried using them to code But it's different in some key ways in particular We're not going to be just asking our LLM to write a whole bunch of code at least not to start But we're going to be focusing on the sentence level and what is meaning at the sentence level It's something like trying to infer a line of code in a mental programming language that is your best understanding of The meaning by which we just mean the thought that the person who's uttering that sentence is trying to convey Okay, and the the LLMs in this case can represent that meaning construction function Okay in ways that have some very interesting properties So i'll just illustrate this with a classic example from the probabilistic language of thought literature the so-called Bayesian tug-of-war So just to ground this imagine that you're reasoning about a situation Like for example, uh Various games of tug-of-war various people symbolized by colored shirts here are facing off against each other And let's just take one person. Let's just say this is a Guy named jack and you might say well, how strong do you think jack is I haven't shown you any information? So your best guess might be just average. Okay Now, uh, suppose I give you some other information Like um, or I should say here. So here are a few samples, you know, I might say average Maybe it's a little less than average If we're talking about MIT students, you may just pick the MIT average But now I could give more information like suppose, you know that jack beat leo in a game of tug-of-war So that might move your arrow up from the MIT average because well, you know People who are who are stronger might be more likely to win than people who are weaker somehow. Okay I could give you some more information Like leo had just won 10 previous matches as well. So that means you might think leo's pretty strong and jack must be even stronger So your arrow goes way up But suppose I told you well leo sometimes it doesn't pull as hard as they really could So maybe maybe leo was just getting a little lazy when leo faced jack. So it might go down a little bit But then leo single-handedly beat a team with jack and tom on it. Okay. So now you think okay Well leo was probably lazy before jack. Maybe isn't that strong Since leo when they wanted to could beat both jack and tom. Okay So the point is in a classic example of non-monotonic reasoning your inference about this one aspect of the world jack strength It's going up and down as you get various information. So we'd like to understand how that thinking process works how Updating your beliefs based on linguistic evidence linguistically expressed evidence works. Okay, that's the starting point of this paper And the idea is to use in this case We're using the the probabilistic programming language church which is based on a dialect of lisp or scheme So there's a lot of parentheses and i'm not going to be able to unpack all the language But hopefully you can get the basic idea that we write we define functions that describe probabilistic distributions on Strength laziness and so on all work through this and that's you know, that's been that's well-attested work That's been very good. You know, it's basically the the general toolkit for doing the kind of probabilistic intuitive Mental modeling that I showed you with two to physics and intuitive psychology, but here we're doing it for this kind of novel domain It's not there's no core domain that infants are born with for tug of war But we can write a model like this to capture what adults in our culture at least might think And then the key new thing here is to understand how to to ground language in these mental models So this is where we're going to model the translation of a statement in english like jack one against leo or a question Like how strong is jack into lines of code that support probabilistic updating and querying in this language And this these the meaning functions, which will be implemented by the neural network here other the Large or even small language model Um capture various interesting ideas that have been proposed You know, I don't have time to review this history but proposed in different eras of linguistics and thinking about How to think about meaning how to think about concepts and you know, I it's it's a it's a very rich literature That I'd be happy to try to discuss if we have time during the question period Okay, um, but the hypothesis here is that maybe these distributional language code models can implement some human like perhaps approximation to this meaning function Um And again, crucially, it's it's we're not trying to now capture like to learn patterns in data in the in in the world We're trying to learn patterns in our thinking and how thought is expressed in language So it's a more modular problem It which I think is better suited to the way language actually works in the human brain, right? It's there's a delimited part of our brain. That is a language processing network Strokes or other lesions there can really impact your language ability without impacting your general thinking ability And you know, it's a relatively late evolutionary addition to us to a brain structure Which shares a lot with other non-human primates, for example And so it's it's that that modularity. I think is also really important here So again, just to unpack how this works You have a probabilistic program that describes these concepts That I can just I can describe to you in language in this way But to the model right now, they're just given in code and I'm not and they just describe for example that You know players strength is drawn from a gaussian distribution players can occasion be lazy When you when you're lazy it cuts your effective strength in half And the strength of the team is that some of the total strength that they pull that the members pull with at any one time And the stronger pulling team wins. That's basically what this is now for now We'll just assume that that's you have that kind of mental Model of a tug of war game And we'll just talk. How do you update your release language? And the basic way of doing it is that we're using what I would call now a medium language model We used open ai's codex, which was the first widely used code llm pretty small by today's standards much smaller than gpt4 Kind of like an early version of gpt 3.5 To translate from a sentence in english like this into what in church the probabilistic programming language is called a condition statement It just expresses the constraint on possible worlds that while the underlying Define statements define stochastic or probabilistic functions, which give you distributions Probability distributions on possible worlds. This says we're going to restrict our probability distribution Which is like our prior to a posterior just those worlds that are consistent with jack beating in one match leo okay And then similarly a quest to a query statement like how strong is jack turns into this worry Which is which the probabilistic programming language then basically Evaluates it draws samples from possible worlds consistent with the condition and generated from that prior on the left And and checks what the strength is and then just kind of counts up those distributions So it's a kind of probabilistic mental simulation the same effectively the same or a generalization of what we were doing You like those intuitive physics examples and from this one piece of data jack beating leo You can see the posterior updates from the prior so jack is stronger than average If I say oh jack also beat alex or proceeded to claim victory from alex that turns into another condition And that updates the posterior even more so now while jack's a lot stronger than average Even working as a team leo and alex still could not be jack. Wow now jack's even stronger when you add in that conditioning statement So each again the where the llm is coming in here Is it's just it's just adding in the the statements into the language of thought and then we're running inference in our probabilistic programming language to give these answers It's worth and this is again, especially if we want to understand what I think is The way forward to thinking about how meaning and language works in humans and where l lm's can come into the mix What's what's really powerful about? Neuro language models here As ways to parameterize a meaning function is the ways that they can pick up on statistics context pragmatics metaphor Um semantic associations all the things that in many ways were most appealing in connectionism like distributed content addressable Uh associate of memory and distributed representations of graded semantic associations So for example, the llm will translate jack won against leo Which into this statement which looks like a fairly transparent semantic parse of that natural language But it'll make basically the same semantic parse of a sentence which on its surface And and in traditional syntax, you know language structuralities looks rather different, right? The syntax here is more complicated I'm not actually using the word win, but In the context the relevant aspect of meaning for thought here is the same namely. This is just another way. It may be more poetic way Or dramatic way to say that jack won against alex and the model knows that automatically It doesn't have to be specially prompted or trained for that using its associative memory properties It's also distributional. These are probabilistic models not on worlds But on strings and in this case they're probabilistic models on strings in our mental programming language so they can Bring into you know bring in classic notions of vagueness if I say jack is strong or very strong I'm not telling you exactly how strong jack is but you might interpret that as a distribution On different condition statements saying well jack strength is is greater than some threshold But I don't know what that threshold is but it's probably pretty big remember this in this case The mean is 50 and the standard deviation 20 so 80 is you know One and a half standard deviations above the mean and the idea is this is again like in scalar Adjectives it's basically saying I'm conditioning on the idea that jack is greater than some threshold But I but I could be uncertain about what that threshold is and even metaphorical things like if I say jack is pretty strong And ben is a beast right in this context It's reasonable to interpret ben is a beast as as saying saying ben is really really strong You know 80 where jack was maybe 60 or greater in a different context if i'm talking about you know Whether you should date someone and I say ben is a beast, you know, then i'm probably saying stay away right So that that both metaphor sort of context sensitive metaphorical interpretation is the kind of thing that this meaning function is very good at um, I'll mostly just skip over this but in work with ben lytkin and and Leo and gape and others we've we've done and this is really all ben's work some very nice quantitative studies Of showing that these contextual aspects of understanding of strength In in the kind of tug-of-war context can match very nicely not always but in a lot of cases very nicely with human judgment so it's suggesting that the distributional aspects of um meaning that we're capturing here are at least somewhat Not psychologically real at least consistent with the judgments that people make Okay, so so but mostly what i've done with this example is just to try to show you how this kind of framework can work To be a way of implementing what is in some sense a classical idea that language isn't That directly the medium of thought at least the only one But it's a way of expressing and communicating thoughts internalizing and externalizing them Okay, and that statistical distributional mechanisms can be a powerful way of learning that The mappings from the signs and symbols that we externalize To the to an internal compositional structured language of our thought Now a lot of what goes on in this paper and i'll just show you one or two examples Such as work with sed zhong in an intuitive physics domain or some intuitive psychology work with lan xing and shen or Tan shi shen Is showing how we can take the same kinds of things that i showed you in the first part of the talk And effectively reconstruct them, but now in a world's worlds that are just described by language So to manage it take that task of the red and yellow thing where before i was showing you the image of a scene And then i was using language to describe a question But what if i just use language to describe the whole world, right? So if i didn't show you an image i just described What's there, you know in many ways, but but but we're still going to do a a mental simulation a probabilistic mental simulation in a physics engine There's many cases, you know in our daily life where we use language to describe the physical world as we experience it It can be extremely expressive very complimentary often to you know images or photos Um, it has vagueness and uncertainty and that's interesting So we might describe a scene that like imagine a table and there's some blocks on it There's some red blocks in the center There are many tall stacks of yellow blocks on the side of the table if the table is bumped hard enough So we ask the same question but for scenes that are described in language And then people make a graded judgment the same one to seven graded judgment of is it more likely to be red or yellow blocks? And so when we can test this again the same kind of quantitative study with large number of participants all online all just reading stuff With different kinds of language expressing exact as well as approximate number Approximate quantifiers logical quantifiers vagueness like the stacks could be tall or very tall And and different kinds of spatial relations and across many different stimuli We we mix up more or less complex sentences using these different kinds of language And in each case we're using again a relatively small LLM to translate sentence by sentence into statements for conditioning and querying in our probabilistic language of thought Then we run a small number of mental simulations in this case in a 2d physics engine Compute the outputs and compare those with people and what i'm showing you here is again the same kind of scatterplot I showed before but on the vertical axis are again our human judgments and on the x-axis now are the model Or the predictions of this language informed thinking model So it looks a lot like what I showed you before these are the judgments from the battalion all work on the red and yellow task We're not we're here the model was given a visual scene people were given visual scenes And the model did that same kind of probabilistic mental simulation in the physics engine But in the in the current work on the left Again, the scene description is constructed from language using the tools that i've shown you by conditioning a prior on scenes Okay, and the main point is just that the models fit human judgments pretty well in both cases and about equally well All right, which is which is interesting We can also compare with just a sort of zero shot or a few shot baseline llm So these are language models, which don't have an explicit mental model of physics or any doing explicit simulation And they are are much worse fits to people Okay, this is this on the what the high up here on the y-axis is Distance and distribution from human distribution of responses to the various models The blue one is the rational meeting construction model and you can do a similar thing in the intuitive psychology domain Like for example in the settings that I showed you with julien harenger's work This is work that uh lans ying and shen did i won't go into the details, but again We can describe worlds with various Goal objects that an agent might have and constraints like you have to go through doors the doors could be locked You have to use keys that could be a red key that could unlock the red door And so on people could be told either that you know You need a key of the right color to open each door of the same color Or you could be told weird things like Keys only unlock doors of other colors Okay, um, you know in these kinds of uh worlds again The rational meeting construction model does a very good job of capturing people's judgments For some kinds of judgments easy cases even gpt 3.5 does reasonably well gpt 4 does better But as the situation gets more complex or unusual things change and break down like especially if We say well in this world keys only unlock doors of different colors That's an easy thing to say to somebody and you have to change your mental model But when you do something like that gpt 3.5 becomes anti correlated with people and gpt 4 just go drops from being highly correlated to just being a chance So these are examples of the ways in which you know going out of distribution from our training experience If we're just using a machine learning function approximation approach, which is remarkable as those systems like gpt 4r That's what they're doing But ways in which if the function approximation is just approximating a much more modular translation function And building on our mental model tools. We can just do much better The last thing i'll just talk about very briefly is you know what is Any work like this has to raise more interesting questions If if anything like this is right on the right track, then it's it's not answering At this point the most interesting questions, but just raising them and perhaps highlighting ways you might get at them So in all the work i've been doing in in for a couple of decades now in probabilistic inference in mental models You start by showing how you can do inference with a mental model, but then you have to ask where do you get it from Now learning from experience is one thing and i've done a lot of work and others in our group and many other Colleagues a lot of that book i mentioned with tom griffiths and nick shader is about how we can learn using hierarchical bays and probabilistic programs learn abstract Programs by doing inference over the space of programs to make sense of our data Maybe even small amounts of data But much of our learning probably the most powerful form of human learning comes through language more abstract Generic language as many folks in cognitive science cognitive development Have shown So the power of language not only to update your beliefs, but actually to give you new world models is really Incredible and that's probably the most that that's the real human singularity there right is the ability of language to let us Learn and think about situations that we haven't directly experienced. I mean think about the tug of war for example I mean, I don't know if you're like me I've maybe done one or two tug of war games at my Most of my knowledge about tug of war or my beliefs at least don't come from my direct experience They come from things maybe people told me about analogous situations And and more generally many of our mental models and our intuitive theories come from what people tell us All right So the same approach that we talked about for updating beliefs from language can also be used to acquire new mental models But here what's going on is we're we're modeling how somebody might explain to you The way this tug of war works and in fact in our experiments with humans and in the in the toby gerstenberg and noah goodman and colleagues did This is exactly what we do would tell people Um about how this work people have various strengths it can vary from person to person And then now we're using the llm to translate Those say sentences in english that are generic sentences about the domain describing the world model We want our participants to use Into the same kind of probabilistic program code, but now these are defined statements So these are not these don't condition on a specific a specific world, but they define the general distribution on worlds But again, they're contextual they're distributional There could be different ways of understanding the vagueness in language But the basic idea is that we can describe a world model in english and then the code llm Can construct that world model. So all the defined statements now are constructed They're not exactly the ones that we use in the original paper But they have the same functional role and they can support the same kinds of inferences So it's just just showing you the way we can I think go towards one of the most interesting ways that language informs our thinking Not just as a way to convey specific beliefs about situations But new but new world models and in some recent most recent work that's still in progress with tyler brook wilson Katie collins and a number of the others I mentioned here tyler is a is a brilliant philosopher who recently graduated from MIT And he's actually just just accepted a faculty job at Yale. So he'll be at Yale in a in a year or so But together with with tyler and others And tyler's thesis goes into some early stages of this We've talked about we've been exploring the ways that the same approach can support Constructing new models, even if I don't explicitly tell you in language how the world works But just by again using your associative memory and marshaling implicit knowledge to construct a model of a new situation So I could tell you about a relay race and I'm not tell the relay race is like a new domain I'm not really telling you how races work. I'm just giving you some information But we're exploring ways in which the LLM can Can be queried to construct possible background knowledge And write probabilistic program code that can be suitable for reasoning about this domain Just on its own the LLM isn't enough But you have to do some reasoning about the models that it suggests And then those models when suitably Reason to put upon can support novel reasoning in this domain and even sensible updating So we're really getting you know, at least demos of steps towards Computational models that can capture the richness of how we are able to think about new situations Even ones that we haven't really thought about very much before or that we haven't been explicitly told how to think about but using the combination of Language the associative knowledge that's in it and an underlying ability for constructing probabilistic models of the world and updating I'm the last thing I just want to leave you with is a set of thoughts about You know, ridiculously It is it is the last thing But I think this will set up some of the discussion with with virginia and others too Which is many people in cognitive science, whether in linguistics or other areas Engaged in meaning have been interested in what you could call, you know a unified account of meaning and what we're trying to at least Point towards steps towards this with the framework. I talked about just to just to again Raise controversial points for discussion if you like um, but the idea that we can capture the meaning of a word in context as well as more generally as Effectively, it's like you might think of it as a form of dynamic semantics if you're familiar with this But the meaning being in in context in a discourse the incremental contribution to the probability distribution over PLOT expressions in the problem that we're thinking about with a problem in discussion And the meaning of a word or phrase or sentence or other unit of language in general Is a is a higher order stochastic function that can take as input a discourse context and return as output a meaning in context And the idea is that if we think about the different approaches These are just four You know traditional ways of thinking about meaning and language which all have great value and have often been seen as being competitors Ways in which we can really bring them together ways in which the PLOT ideas that we've talked about Can integrate the compositional logical aspects of meaning that formal semantics and other areas in the language of thought tradition have emphasized As well as in the context of probabilistic language of thought and mental models of the world can give a powerful form of grounding That's not grounded in sense data But in our models of the world that's what the PLOT does and the LLM or more generally a statistical distributional Sequence models can capture both the distributional statistical aspects of meaning. It's the base that the Both the sort of distributional usage approach, but also more general semantic association needed to make sense of language So flexibly as well as some of the very flexible pragmatic communicative ways we use language So I'll just leave it at that. Okay. Okay. Could you turn off your share, please? Yes First of all, thank you very much There's not a chance in the world that I'm going to forget Virginia this time and I'll tell you why Because I'm gobsmacked at how many different areas you're an expert in and so anyway, here's another one uh Virginia valium from CUNY hunter It's all yours. Thanks Josh that was such a great talk and so rich In 10 minutes, which is how much time you and I have will only scratch the surface So departing from the comments I sent you with respect to the Unified theory of meaning Many years ago Jerry Katz suggested that The question what is meaning could be Separated into questions like what is sameness of meaning? What is contradiction? What is anomaly? What is entailment and so on? And it occurs to me that it would be interesting to try this To try your model to see just how well It can detect Synonomy contradiction anomaly and so on Yeah, I think that's um, that's a great connection. I mean I I know a little bit about that But I you're inspiring me to go back and reread and learn much more about it I mean, I think just in the context of the last thing I said The idea exactly that in a sense in this framework at least What the statistical language model is doing is capturing the notion of sameness effectively because there the distribution on Code in the probabilistic language of thought if that distribution is similar And that can be measured in different ways then you might say well things Have the same meaning either in context or in general because and that that distribution function can be contextualized or it can be Made higher order. Okay, so that's really interesting, but it doesn't compute entailment or other You know a conceptual or inferential relations those come from reasoning in the probabilistic language of thought um And yeah, I mean I think that's that would be great to explore that more see if that can account for Unify it both account for the different set of phenomena and unify in that sense Yeah, I think that would be really interesting to explore um, so going back to Some of the more mundane Which I guess is upon Aspects um the way that I'm understanding what you've Said about how the theories Intersect that is how llm's intersect with basian models Is basically you're using the llm as a kind of tool To translate from one vocabulary to another vocabulary. Is that accurate? Yeah, I mean and that is at the most practical level. That's what we're doing. Um, yeah Okay, more about that, but let's let's go with that for now. Yes. Okay. Um so The critical difference that I see in what you're doing and what llm's are doing is the Well, I guess there are two critical differences, but for me the most important one is the use of symbols um, the other part is the basian Mechanism, but let's separate them. So When you put symbols in To me you're putting a lot of content into the mechanism. Do you agree with that? um, I agree that adding symbols adds a lot of content, but I think And I again, I didn't have time to unpack all of this very well I think but when you say you putting in symbols the in different different things that I'm talking about here The you and the putting in are different, but yeah, but whenever you add in symbols that adds a lot of Content and structure here. Yeah. Yeah content. Right. So I'm thinking about the implications of that for nativism So it seems to me. This is a nativist theory Um, I would say it is compatible with some forms of nativism that I find plausible although none I don't have a horse in that game, but I Through a lot of interactions with friends and colleagues like lisbelki susan carry many others Come to find Certain kinds certain aspects of nativism and conceptual nativism plausible especially in Certain core domains that are shared with other animals Like intuitive physics about objects that the world is three dimensional objects have some that there's some kind of physical interactions That are you know our bodies engage with and also some forms of intuitive psychology not necessarily higher order belief reasoning But the idea of efficient action and that agents have goals and they pursue actions in the physical world Grounded in physics to achieve their goals efficiently. Honestly, there's evidence Again, sherry lou who did this work in spelki's lab and continue to build on it has really, you know I mean, I was a striking evidence in three month olds But it's every experiment with three month olds is very small and you know a lot needs to be built on But as striking as it gets in three month olds science I'm showing that that aspects certain aspects of not only physics, but efficient goal directed action Understanding seem to be present that doesn't mean they're innate But at least they're not they're present way before language and probably build on some innate stuff It's compatible with that notion I'm not sure who that is but could you mute that was an accident. I think they just Should have turned off their okay, but so so yeah, there's this idea that certain kinds of symbols Are used in our framework to describe those some of those core systems But we are in contrast to like a fedoria nativism which says all concepts are innate In this in this framework here. I mean it or you could say in contrast, although some of uh, I don't know what jerry would have actually said about this but people like paul patrowski have suggested You know, maybe he would love it. I don't know but in in a rather different version of like some forms of radical Conceptual nativism most of the concepts in this framework are not innate They're written in a language of thought That it that could be somehow innate or somehow bootstrapped through natural language I find some of the ideas that that uh, Susan carrey and lis spelti in their different ways along with jesse snettaker have developed their ways in which language acquisition and Mental languages of thought might bootstrap each other and many things in the kind of glitman tradition also I think are reflect that idea But I so I I think we we are We still need to show this this is like the most interesting thing to do But to show ways in which the things I was showing at the very end could be could be used To explain how and model how natural language can start off Being grounded semantically and logically in a limited symbolic vocabulary of probable worlds that reflect core knowledge But then that can support bootstrapping and introducing new concepts via those mechanisms I was talking about at the end Including new concepts, but also new domain theories that we get explicitly or implicitly through our linguistic interaction with Other people who we think know more than us Okay, um, that sounds great. Um It also suggests to me that it is a highly modular System even though it's also probabilistic um, so it's probabilistic within each of these different modules And depending on what you think the symbols are that you start with you can Iterate what you think the modules of the mind are Yeah, no, I think that's right And I think you know, I think it's this framework is not a On its own a proposal for how our minds start But it can be used to instantiate and build and test some of them And so that's I think that's right. It's it does suggest that you could have, you know, different Sub-languages of thought for different domains But crucially the picture that you get with adults from this is both in some ways like strikingly Supermodular and in some other ways completely holistic So the striking super modularity is that the actual reasoning that you do in is in a discourse when I'm thinking about a situation And we're talking in a conversation is very modular if it might even be just Specific to this context this what Tyler Brook Wilson calls bespoke model construction This idea that we might construct a model on the fly to think about a particular situation That that we're that's the last thing that we've been working on there Is in some sense super modular because that model is is relatively small and all the inference. I'm doing is just here so I so I Side step or avoid the classic problems of what has made Bayesian inference intractable the idea that if I'm going to actually have A distribution over all possible worlds. I could think about it update that You know, that's completely intractable many people in the nativist tradition dan osherson and others have written about that And this this is a a way around that or as Tyler puts it in his thesis It's in a sense a kind of way to think about a solution certain kind of solution to the frame problem So it's super modular, but it's also very holistic in the sense that the world knowledge that's used to construct that Comes from like all the code you've ever written and all the semantic associations And it's like a gigantic holistic almost quinian web of language and code What we've sometimes called the github in the mind view There's the game engine in the head the github in the mind like github is this You know thing on the web which was crucial for training Language code models and you can imagine your own mind has lots of chunks of code Some of which are maybe innate many of which are not and natural language interweave with it And and having a content addressable associative memory that can use that and and marshal out from that relevant symbolic probabilistic models that can be used to reason about a particular situation in some sense that's extremely holistic And maybe is necessary to grapple with what is in you know very clearly some of the wholism of human cognition So going back a few steps The intuitive physics part In principle Animals could do that as well, right? Yes. So a chimp could have intuitive physics Yeah, and joseph call and others have studied that a number of other Amanda ced and other brothers had studied other non-human primates and we're actually collaborating With erica cart mill and some others studying non-human primates on intuitive physics and I know colleagues Who are studying analogous kinds of intuitive physics in rats? And I think even simpler organisms Okay, so the difference between humans and Other animals is going to be that language allows you to go further Language allows you to go beyond intuitive physics Whereas nothing is going to help these other animals go beyond intuitive physics Yeah, I I think that's right. I mean again I wouldn't say that all the all the mental models of other animals is just intuitive physics They also have models of their social world like you know the chainy and save farce baboon metaphysics is mostly a social theory But yeah, but I would say Definitely that the key thing here and it's the real human singularity. I would say Is that language allows us to construct to both enrich our intuitive physics to think about Aspects of the physical world that are not initially intuitive to us and maybe still aren't But also to construct things that just go totally beyond any any core domain that evolution gave us Right Okay, so one question that this brought up to me Because what you're talking about are all the ways that humans are so good at what they do um But then there are ways that they're not so good at what they do And far transfer is one of those examples um And there there are some things that are hard for people to learn Or some places where it's hard to get from A to z Even though you think you've learned a So there's a famous little story about someone going to their logic teacher asking about Understanding if a then b and the logic teacher spends a lot of time on that And the student says Okay, I think I get it But we try it with r and s now um So that's something that just shouldn't occur, but it does occur Uh, even with smart people So like when you try to teach people about experimental design It's hard for them to see confounds often um, it's hard for them to Get at what's wrong with some experimental design And it's not that they don't have a general intelligence. It's not that they don't have some principles but it's as if What they've learned is just Too far away as far as the the string of examples goes for them to get to to the next one So I'm wondering how On your system those kinds of limitations would be modeled Yeah, no, that's a that's a great point a great question and a great pointer. I think to work that We could and should do more of so the the stuff I talked about at the very end, which is again this the work with Tyler and uh, katie collins and Lansing and and and very much with leo wang also um is What we're what we're getting out there like is is again how we can Describe a situation in language, maybe even very implicitly just start talking and then see if See if this architecture can be used to construct a mental model needed to reason about that situation And you could also make it better More robust if I explicitly give you instructions as in the educational context you're talking about if I try to explain to someone Logic or experimental design actually another collaborator on that project said zhong is actually very interested for his thesis In how we learn like logic and learn to reason through language But so we have you know, our framework is providing possibly a way to do that but In order for it to work at least in the current system We do the kind of thing that lm folks are generally doing these days, which is some kind of few shot prompting We prompt the system with examples of other mental models and language for describing them in Related domains they could be similar or they could be further and what we already can see right is i mean The interesting thing is can you generalize to new domains? So our system is somewhat able to do that But you know, this is a place where near or far transfer would be relevant at least so far you know as you might expect you have to have some domain that's at least reasonably close And for example, we're trying we could transfer from like a tug of war to a relay race or to some other sport setting Or from a couple of sport settings to yet some new sport um Or you might transfer from a sport setting to like a math competition or to some other kind of thing So if it really interesting ideas about abstraction and metaphor Analogy some of the kinds of things that the the transfer literature has studied are going to be relevant here We're just beginning but what you're pointing to is actually some set of phenomena and things we should really Engage with and I expect you know at least based on our current system It will definitely struggle sometimes with far transfer Um, which be which might be because at that level, you know, I I I drew this contrast at the very beginning between the pattern recognition approach Data driven learning and the mental models approach But at that level we are doing a kind of pattern recognition. It's just not patterns in the world It's patterns and thoughts and thought structures and ways of expressing them And you know at that point if if if that kind of really interesting But much more abstract kind of pattern recognition is what's driving your ability to construct new models of new situations Or at least initially before you've had formal instruction or when you're just at the beginning of formal instruction Then we should expect that it to have it to have the same kinds of fragility with far transfer that any machine learning approach does Okay, sorry to be a gilljoy here. We have about 10 minutes left But we have at least four people that want to ask questions. So what I want to suggest to alina and julia and also Uh leijin and stephan carlson is to raise your hand again, and I'll recognize you in the order that you raise your hand Okay, julia go ahead well, um, so I was wondering I was thinking like maybe you can use the um the LLM translator plus the probabilistic models that system to um Model how the beliefs of a reader would change over the course of being told a story or like reading a book And then you could look across The genres or books or whatever to kind of get a sense of What the kind of model of those stories being told since the stories are themselves a world model what that would be and that you could incorporate Because if you have a guess because something that you didn't like touch on in You know in your act talk, which I'm sure you didn't have time But was that like if you have a guess about where you are in the trajectory of the story you expect Then you have a guess about About what you think's going to happen next Wrap it up because we um, so I was just wondering like do you do yeah, that's a great question Um, let me just try to answer it really quickly yet Leo wong would love that question because in addition to being a great cognitive scientist They are also a writer they write stories and even novels And one of the things that we've been working on is little mini like three act structures and things like that that Follow either classic narrative structures or other Things and and you know, maybe not surprisingly those are places where we see a big gap between human story understanding and LLMs even the state of the art ones, but exactly we're trying to use these models to capture how that kind of You know journey of understanding might unfold and also even how A creator might create it. So another student that we've worked with Karthik Chandra has Had had some work at last year's cog side conference on storytelling as inverse inverse planning and the idea that like if somebody's Understanding another character's journey by doing some inverse planning as in those theory of my models Then a storyteller can try to invert that inverse planner to to convey the emotional Or mental journey that the character has and that's a way to use this toolkit for both story creation as well as story understanding So again, it's those are mostly promissory notes, but great question and future research I think from Karthik and leo and others will address it. Thank you. Thanks Hi, I'm gonna ask a question in person josh. This is ever Hi, I yeah go for it. I I really liked your talk. By the way, I watched it on video. Yeah Cool, great. Okay. So yeah I was gonna so following the work that you were showing where you were basically based also on what you were saying and then in the Question period where you were, you know, few shot prompting these language models to produce probabilistic language World models, I guess I have my I have sort of a more medic question Which is do you see that as just a way to sort of help step by step reasoning in these models? Or do you think this is sort of evidence for Early sort of possible world modeling being learned in these models Um I'm not sure if I see it as either of those. Um, I I mean, uh, uh, so I don't think of it as either doing step step by step reasoning in language models or Evidence that they do possible world modeling. It might be relate. I mean, there is you know, I've obviously as you know You talked about some and others have, you know, there's certainly Um, a lot of evidence that llms if you try to just use them as end-to-end reasoning systems They can benefit from step by step by reasoning and there might be some emergent world modeling capacities But you know, again, I look at that and I see a really interesting and mixed pattern of successes and failures And depending on who's writing the paper and what their agendas are you can highlight the successes or you can highlight the failures and the gaps um The to me that an objective perspective is that it's patchy and fragile although extremely impressive and really interesting So the way I think of what we're trying to do is to say, yeah, there's there's you know Various kinds of interesting approximate implicit knowledge that those models have that can be used sometimes to succeed In reasoning complex sequences of reasoning or world modeling But I think a more robust way to use it is the way we're using it a more both a more human like and a more robust for ai way to use it which is to Which is to condition and construct these bespoke world models that you know where the long chains of sequential reasoning or actual You know coherent world modeling are there By construction But you know, there are other limits like again as as I was saying, you know Bayesian inference in really complex models Is very difficult and I don't think people do it. There's a lot of evidence They don't do it But they seem to do it very generally in in just the right small model At least in cases where they have the relevant world knowledge to construct those models and when they don't they don't Right, so that's all you know Our minds have mixed patterns of successes and failures there too But I think this toolkit is better matched to the mixed patterns of successes and failures, which is What what i'm trying to get out if that makes sense the the the one that we see in humans as opposed to the the weird head scratching Like super intelligent cases in some places and then super dumb Jesus and others that you just see in a pure sequence model I'm told Alina can go next. Thanks for your answer. Yeah, thanks Good question Alina pick it up or you lose your turn Yes, um, hello, uh, thank you for insightful talk. Um, Josh so You said that um, and correct me if I'm misheard that language allows us to construct meaning that it's totally out of bounds of evolutionary gain um Is if uh, if this is what you said, I think this is uh, super profound. So Yeah, I I'm sure I'm not the only person who said that many people sure. Okay. It's just a reminder. I guess profound truth. Yes Okay, uh, so the actual question So, um, you mentioned that the meaning on the world is constructed contextually and incrementally So could you please elaborate on how this process occurs and how it impacts our understanding of complex concepts? So so we transition between the um from incremental construction of the meaning to generalizations Right. So the way it works in our I can just I can tell you how it works in the models that we've built so far And again, this is not to say that it's exactly like this in the mind But I think it might be something like this and a lot more work needs to be done But the way it works in our models is something kind of like what you're familiar in a chat Like if you've used a chat gbt or other conversational ai systems The way it's working is that basically there You know, if you're used to using chat gbt you type something it types something back and in the middle Some wheels turn and then you type something more in it wheels turn in the black box and it types more stuff Okay, so it's it's basically like that It in that you at each each sentence is translated into some Expression in the language of thought roughly at the sentence level. I think sentences are real units of meaning like I I am very respectful and admiring of many insights from language Including that words are real and sentences are real and there's real syntactic structure to be understood there That is only being approximated in some ways by these models But the key is that yes, there's a process the contextual process is like at the discourse level You go sentence code sentence code sentence code And each sentence to code translation is conditioned on the previous Conversational history or the discourse that that you have been interpreting. Okay That's just a first approximation Just another thing you might want to do and surely you'll have to do is go back and edit previous code because if you realize Oh, I misunderstood something. I can't just add new code. I have to go back and edit the code that I wrote before Which is also something that code llms can do I'm not saying they'll do it right But there's other processes that involve like checking and refining and fixing models That are written in this way But maybe that gives you some sense of the contextual dynamics of how language is understood in this model Last question lay jim Hi Joshua, thank you for for the presentation. It's very interesting Um, I have an engineering background, but I'm not an expert in artificial intelligence Uh, I have a few questions um From your presentation, I understand that actually llms the The the it's it's kind of a gold mine of human knowledge And uh, you seem to say that they have a modelized the real world facts and the logics are quite correctly If you ask the question directly in human language, uh, currently they don't seem to to give the good answer because some with some lack of algorithm, but if you first translate the human questions into functional code and like The work you are doing and then they are capable of um I know we execute the code then we we are capable of having Result results that are quite close to human behavior. Uh, so is that the correct? Yeah, that's I mean, that's more or less correct But I don't want to I mean and that's that's what we've been doing in these examples And what I've been showing but I don't want to Claim I don't and I don't think it's true But certainly I don't have the evidence for it being true or how true it is that You know llms like always do this, right? There's a lot of Ways in which language is only an imperfect reflection of the ways that we think There's certain things that again are partly based on evolutionarily ancient core systems. The language is not very good at expressing And that includes also spatial reasoning and you can see ways in which Even the best language models break down here and language multimodal language vision models Have have had persistent problems always from the beginning there So I think you know, I don't want to suggest like I think yes language is a treasure trove Of knowledge explicit and implicit about the world, but it's and for us it's it's such a valuable resource as human beings Okay, that's why it's no accident that language models when they're trained to capture patterns on all the language that humanity's basically ever produced and put out there on the web Uh, you know start to have remarkable properties at the same time It's only some parts of our knowledge about the world is key parts to actually understanding and being in the world That basically nobody ever talks about And even if they do talk about it they talk about it very incompletely and imperfectly and yet our brains are designed to understand in those terms So I I don't want to convey the idea that somehow it's all there in language and it's and that's all you need by not by any means Wonderful before we, uh, let people applaud You know, there's some several panels coming. Are there any of them that you can join? They're not today, unfortunately Yeah, um, I I need to check my calendar and coordinate on some family things and I will I'll I'll try very hard to join one of them Okay, now we'll applaud you for Thank you Thanks, and and thanks so much for the discussion Virginia and all the all the questions. There were great questions. So I hope to engage more um in one of the panels and um in person if if anyone hears", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.6, "text": " I forget now whether you do have a discussant or not. Yeah, Virginia.", "tokens": [50364, 286, 2870, 586, 1968, 291, 360, 362, 257, 2248, 394, 420, 406, 13, 865, 11, 10956, 13, 50544], "temperature": 0.0, "avg_logprob": -0.39407984415690106, "compression_ratio": 1.3768115942028984, "no_speech_prob": 0.1268453299999237}, {"id": 1, "seek": 0, "start": 4.16, "end": 6.16, "text": " Oh, it's Virginia's gonna. Okay, great. So", "tokens": [50572, 876, 11, 309, 311, 10956, 311, 799, 13, 1033, 11, 869, 13, 407, 50672], "temperature": 0.0, "avg_logprob": -0.39407984415690106, "compression_ratio": 1.3768115942028984, "no_speech_prob": 0.1268453299999237}, {"id": 2, "seek": 0, "start": 7.04, "end": 10.16, "text": " She's here. Yeah, she's there. Hi, Jenny. I've been Virginia. I mean", "tokens": [50716, 1240, 311, 510, 13, 865, 11, 750, 311, 456, 13, 2421, 11, 20580, 13, 286, 600, 668, 10956, 13, 286, 914, 50872], "temperature": 0.0, "avg_logprob": -0.39407984415690106, "compression_ratio": 1.3768115942028984, "no_speech_prob": 0.1268453299999237}, {"id": 3, "seek": 0, "start": 24.72, "end": 25.92, "text": " Have you", "tokens": [51600, 3560, 291, 51660], "temperature": 0.0, "avg_logprob": -0.39407984415690106, "compression_ratio": 1.3768115942028984, "no_speech_prob": 0.1268453299999237}, {"id": 4, "seek": 2592, "start": 26.0, "end": 33.04, "text": " Josh, you haven't seen any sessions at all yet, right? Um, I've seen I haven't been able to join live, but I watched video from the first two days", "tokens": [50368, 9785, 11, 291, 2378, 380, 1612, 604, 11081, 412, 439, 1939, 11, 558, 30, 3301, 11, 286, 600, 1612, 286, 2378, 380, 668, 1075, 281, 3917, 1621, 11, 457, 286, 6337, 960, 490, 264, 700, 732, 1708, 50720], "temperature": 0.0, "avg_logprob": -0.22995019233089753, "compression_ratio": 1.4874371859296482, "no_speech_prob": 0.025520537048578262}, {"id": 5, "seek": 2592, "start": 33.68, "end": 35.68, "text": " Did you watch belkin?", "tokens": [50752, 2589, 291, 1159, 989, 5843, 30, 50852], "temperature": 0.0, "avg_logprob": -0.22995019233089753, "compression_ratio": 1.4874371859296482, "no_speech_prob": 0.025520537048578262}, {"id": 6, "seek": 2592, "start": 35.68, "end": 41.52, "text": " I did. Yeah. Okay, great. Okay. I was even gonna refer to it briefly. Yeah. Okay. That's why I don't want this to explain. Yeah", "tokens": [50852, 286, 630, 13, 865, 13, 1033, 11, 869, 13, 1033, 13, 286, 390, 754, 799, 2864, 281, 309, 10515, 13, 865, 13, 1033, 13, 663, 311, 983, 286, 500, 380, 528, 341, 281, 2903, 13, 865, 51144], "temperature": 0.0, "avg_logprob": -0.22995019233089753, "compression_ratio": 1.4874371859296482, "no_speech_prob": 0.025520537048578262}, {"id": 7, "seek": 4152, "start": 41.84, "end": 43.84, "text": " Um", "tokens": [50380, 3301, 50480], "temperature": 0.0, "avg_logprob": -0.15626988130457262, "compression_ratio": 1.691860465116279, "no_speech_prob": 0.04461773484945297}, {"id": 8, "seek": 4152, "start": 54.720000000000006, "end": 56.96, "text": " I'm I'm also trying to see if there's some time", "tokens": [51024, 286, 478, 286, 478, 611, 1382, 281, 536, 498, 456, 311, 512, 565, 51136], "temperature": 0.0, "avg_logprob": -0.15626988130457262, "compression_ratio": 1.691860465116279, "no_speech_prob": 0.04461773484945297}, {"id": 9, "seek": 4152, "start": 57.2, "end": 62.24, "text": " I know you said that there might I might be able to join I can't join the discuss or there is no discussion today, but um", "tokens": [51148, 286, 458, 291, 848, 300, 456, 1062, 286, 1062, 312, 1075, 281, 3917, 286, 393, 380, 3917, 264, 2248, 420, 456, 307, 572, 5017, 965, 11, 457, 1105, 51400], "temperature": 0.0, "avg_logprob": -0.15626988130457262, "compression_ratio": 1.691860465116279, "no_speech_prob": 0.04461773484945297}, {"id": 10, "seek": 4152, "start": 62.800000000000004, "end": 67.12, "text": " I'm trying to see if there's any one of the other panels that my schedule would be great. I can add", "tokens": [51428, 286, 478, 1382, 281, 536, 498, 456, 311, 604, 472, 295, 264, 661, 13419, 300, 452, 7567, 576, 312, 869, 13, 286, 393, 909, 51644], "temperature": 0.0, "avg_logprob": -0.15626988130457262, "compression_ratio": 1.691860465116279, "no_speech_prob": 0.04461773484945297}, {"id": 11, "seek": 4152, "start": 68.24000000000001, "end": 70.24000000000001, "text": " Yeah, I'll I'll uh", "tokens": [51700, 865, 11, 286, 603, 286, 603, 2232, 51800], "temperature": 0.0, "avg_logprob": -0.15626988130457262, "compression_ratio": 1.691860465116279, "no_speech_prob": 0.04461773484945297}, {"id": 12, "seek": 7024, "start": 70.47999999999999, "end": 72.96, "text": " I'll I'll be in touch about that. Okay. Let me just", "tokens": [50376, 286, 603, 286, 603, 312, 294, 2557, 466, 300, 13, 1033, 13, 961, 385, 445, 50500], "temperature": 0.0, "avg_logprob": -0.33419464729927684, "compression_ratio": 1.2181818181818183, "no_speech_prob": 0.004461382981389761}, {"id": 13, "seek": 7024, "start": 74.96, "end": 76.96, "text": " Solid duty here", "tokens": [50600, 26664, 9776, 510, 50700], "temperature": 0.0, "avg_logprob": -0.33419464729927684, "compression_ratio": 1.2181818181818183, "no_speech_prob": 0.004461382981389761}, {"id": 14, "seek": 7024, "start": 93.19999999999999, "end": 97.67999999999999, "text": " Welcome everybody to the afternoon session everybody out in in the", "tokens": [51512, 4027, 2201, 281, 264, 6499, 5481, 2201, 484, 294, 294, 264, 51736], "temperature": 0.0, "avg_logprob": -0.33419464729927684, "compression_ratio": 1.2181818181818183, "no_speech_prob": 0.004461382981389761}, {"id": 15, "seek": 9768, "start": 98.64, "end": 102.64, "text": " Distributed land and everybody here. I'd like to um", "tokens": [50412, 9840, 2024, 4866, 2117, 293, 2201, 510, 13, 286, 1116, 411, 281, 1105, 50612], "temperature": 0.0, "avg_logprob": -0.3764097237888771, "compression_ratio": 1.5570776255707763, "no_speech_prob": 0.004004278685897589}, {"id": 16, "seek": 9768, "start": 103.2, "end": 110.56, "text": " Introduce Josh Stendenbaum who's professor of computational cognitive science in the department of brain and cognitive science", "tokens": [50640, 27193, 384, 9785, 745, 8896, 46641, 567, 311, 8304, 295, 28270, 15605, 3497, 294, 264, 5882, 295, 3567, 293, 15605, 3497, 51008], "temperature": 0.0, "avg_logprob": -0.3764097237888771, "compression_ratio": 1.5570776255707763, "no_speech_prob": 0.004004278685897589}, {"id": 17, "seek": 9768, "start": 111.28, "end": 112.88000000000001, "text": " at mit", "tokens": [51044, 412, 2194, 51124], "temperature": 0.0, "avg_logprob": -0.3764097237888771, "compression_ratio": 1.5570776255707763, "no_speech_prob": 0.004004278685897589}, {"id": 18, "seek": 9768, "start": 112.88000000000001, "end": 120.72, "text": " He's a principal investigator at the computer science and artificial intelligence lab called sea sail. Is that former sail?", "tokens": [51124, 634, 311, 257, 9716, 38330, 412, 264, 3820, 3497, 293, 11677, 7599, 2715, 1219, 4158, 15758, 13, 1119, 300, 5819, 15758, 30, 51516], "temperature": 0.0, "avg_logprob": -0.3764097237888771, "compression_ratio": 1.5570776255707763, "no_speech_prob": 0.004004278685897589}, {"id": 19, "seek": 9768, "start": 122.0, "end": 124.88000000000001, "text": " No, uh sail is a stem from sail", "tokens": [51580, 883, 11, 2232, 15758, 307, 257, 12312, 490, 15758, 51724], "temperature": 0.0, "avg_logprob": -0.3764097237888771, "compression_ratio": 1.5570776255707763, "no_speech_prob": 0.004004278685897589}, {"id": 20, "seek": 12488, "start": 125.83999999999999, "end": 130.79999999999998, "text": " But it's formerly the AI lab and the lcs laboratory for computer science", "tokens": [50412, 583, 309, 311, 34777, 264, 7318, 2715, 293, 264, 287, 14368, 16523, 337, 3820, 3497, 50660], "temperature": 0.0, "avg_logprob": -0.2718553419237013, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.002471150830388069}, {"id": 21, "seek": 12488, "start": 131.2, "end": 136.64, "text": " Great. Yeah, and a thrust leader in the center for brains minds and machines cbn", "tokens": [50680, 3769, 13, 865, 11, 293, 257, 24030, 5263, 294, 264, 3056, 337, 15442, 9634, 293, 8379, 269, 19404, 50952], "temperature": 0.0, "avg_logprob": -0.2718553419237013, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.002471150830388069}, {"id": 22, "seek": 12488, "start": 136.64, "end": 144.24, "text": " And his papers are on perception learning common sensory reasoning in humans and machines with the twin goals of better", "tokens": [50952, 400, 702, 10577, 366, 322, 12860, 2539, 2689, 27233, 21577, 294, 6255, 293, 8379, 365, 264, 18397, 5493, 295, 1101, 51332], "temperature": 0.0, "avg_logprob": -0.2718553419237013, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.002471150830388069}, {"id": 23, "seek": 12488, "start": 144.64, "end": 151.35999999999999, "text": " Understanding human intelligence in computational terms and building more human like intelligence in machines", "tokens": [51352, 36858, 1952, 7599, 294, 28270, 2115, 293, 2390, 544, 1952, 411, 7599, 294, 8379, 51688], "temperature": 0.0, "avg_logprob": -0.2718553419237013, "compression_ratio": 1.7174887892376682, "no_speech_prob": 0.002471150830388069}, {"id": 24, "seek": 15136, "start": 152.0, "end": 154.64000000000001, "text": " In other words, he's cut out perfectly for this", "tokens": [50396, 682, 661, 2283, 11, 415, 311, 1723, 484, 6239, 337, 341, 50528], "temperature": 0.0, "avg_logprob": -0.1595796064897017, "compression_ratio": 1.6258503401360545, "no_speech_prob": 0.002215706743299961}, {"id": 25, "seek": 15136, "start": 156.08, "end": 159.20000000000002, "text": " Summer school. I hand it over now to Josh Stendenbaum", "tokens": [50600, 16161, 1395, 13, 286, 1011, 309, 670, 586, 281, 9785, 745, 8896, 46641, 50756], "temperature": 0.0, "avg_logprob": -0.1595796064897017, "compression_ratio": 1.6258503401360545, "no_speech_prob": 0.002215706743299961}, {"id": 26, "seek": 15136, "start": 160.16000000000003, "end": 163.52, "text": " Okay, great. Thank you so much, Steven for organizing for inviting me", "tokens": [50804, 1033, 11, 869, 13, 1044, 291, 370, 709, 11, 12754, 337, 17608, 337, 18202, 385, 50972], "temperature": 0.0, "avg_logprob": -0.1595796064897017, "compression_ratio": 1.6258503401360545, "no_speech_prob": 0.002215706743299961}, {"id": 27, "seek": 15136, "start": 165.12, "end": 171.60000000000002, "text": " You know, I I got to watch much of the first two days and it was really interesting to see a back and forth between people who are", "tokens": [51052, 509, 458, 11, 286, 286, 658, 281, 1159, 709, 295, 264, 700, 732, 1708, 293, 309, 390, 534, 1880, 281, 536, 257, 646, 293, 5220, 1296, 561, 567, 366, 51376], "temperature": 0.0, "avg_logprob": -0.1595796064897017, "compression_ratio": 1.6258503401360545, "no_speech_prob": 0.002215706743299961}, {"id": 28, "seek": 15136, "start": 172.16000000000003, "end": 174.64000000000001, "text": " Extremely impressed with large language models", "tokens": [51404, 24921, 736, 11679, 365, 2416, 2856, 5245, 51528], "temperature": 0.0, "avg_logprob": -0.1595796064897017, "compression_ratio": 1.6258503401360545, "no_speech_prob": 0.002215706743299961}, {"id": 29, "seek": 15136, "start": 175.12, "end": 180.24, "text": " Both on their language abilities and maybe some of their general thinking abilities and other people who are much more skeptical", "tokens": [51552, 6767, 322, 641, 2856, 11582, 293, 1310, 512, 295, 641, 2674, 1953, 11582, 293, 661, 561, 567, 366, 709, 544, 28601, 51808], "temperature": 0.0, "avg_logprob": -0.1595796064897017, "compression_ratio": 1.6258503401360545, "no_speech_prob": 0.002215706743299961}, {"id": 30, "seek": 18024, "start": 180.88, "end": 184.08, "text": " That they have really anything to do with intelligence at least of the human form", "tokens": [50396, 663, 436, 362, 534, 1340, 281, 360, 365, 7599, 412, 1935, 295, 264, 1952, 1254, 50556], "temperature": 0.0, "avg_logprob": -0.1303997214781035, "compression_ratio": 1.6797153024911031, "no_speech_prob": 0.0015005035093054175}, {"id": 31, "seek": 18024, "start": 184.72, "end": 189.20000000000002, "text": " The work I'm going to talk about here is a is an interesting I think mix of those two perspectives", "tokens": [50588, 440, 589, 286, 478, 516, 281, 751, 466, 510, 307, 257, 307, 364, 1880, 286, 519, 2890, 295, 729, 732, 16766, 50812], "temperature": 0.0, "avg_logprob": -0.1303997214781035, "compression_ratio": 1.6797153024911031, "no_speech_prob": 0.0015005035093054175}, {"id": 32, "seek": 18024, "start": 190.72, "end": 192.24, "text": " And I hope that will", "tokens": [50888, 400, 286, 1454, 300, 486, 50964], "temperature": 0.0, "avg_logprob": -0.1303997214781035, "compression_ratio": 1.6797153024911031, "no_speech_prob": 0.0015005035093054175}, {"id": 33, "seek": 18024, "start": 192.24, "end": 194.24, "text": " Be useful stimulating", "tokens": [50964, 879, 4420, 43671, 51064], "temperature": 0.0, "avg_logprob": -0.1303997214781035, "compression_ratio": 1.6797153024911031, "no_speech_prob": 0.0015005035093054175}, {"id": 34, "seek": 18024, "start": 194.56, "end": 198.8, "text": " Engage in some interesting discussion both now and going forward over the next two weeks", "tokens": [51080, 2469, 609, 294, 512, 1880, 5017, 1293, 586, 293, 516, 2128, 670, 264, 958, 732, 3259, 51292], "temperature": 0.0, "avg_logprob": -0.1303997214781035, "compression_ratio": 1.6797153024911031, "no_speech_prob": 0.0015005035093054175}, {"id": 35, "seek": 18024, "start": 200.0, "end": 204.0, "text": " The the heart of the talk, which will really be more like the second half is based on this paper", "tokens": [51352, 440, 264, 1917, 295, 264, 751, 11, 597, 486, 534, 312, 544, 411, 264, 1150, 1922, 307, 2361, 322, 341, 3035, 51552], "temperature": 0.0, "avg_logprob": -0.1303997214781035, "compression_ratio": 1.6797153024911031, "no_speech_prob": 0.0015005035093054175}, {"id": 36, "seek": 18024, "start": 204.0, "end": 207.20000000000002, "text": " Which you can find on archive from word models to world models", "tokens": [51552, 3013, 291, 393, 915, 322, 23507, 490, 1349, 5245, 281, 1002, 5245, 51712], "temperature": 0.0, "avg_logprob": -0.1303997214781035, "compression_ratio": 1.6797153024911031, "no_speech_prob": 0.0015005035093054175}, {"id": 37, "seek": 20720, "start": 207.6, "end": 211.2, "text": " Understanding natural language by translating into a probabilistic language of thought", "tokens": [50384, 36858, 3303, 2856, 538, 35030, 666, 257, 31959, 3142, 2856, 295, 1194, 50564], "temperature": 0.0, "avg_logprob": -0.13128909886440385, "compression_ratio": 1.6958041958041958, "no_speech_prob": 0.007455979939550161}, {"id": 38, "seek": 20720, "start": 212.07999999999998, "end": 220.07999999999998, "text": " But I'm going to spend the first half setting some context on just how we think about thinking and then it's built on that for the relationship between language and thought", "tokens": [50608, 583, 286, 478, 516, 281, 3496, 264, 700, 1922, 3287, 512, 4319, 322, 445, 577, 321, 519, 466, 1953, 293, 550, 309, 311, 3094, 322, 300, 337, 264, 2480, 1296, 2856, 293, 1194, 51008], "temperature": 0.0, "avg_logprob": -0.13128909886440385, "compression_ratio": 1.6958041958041958, "no_speech_prob": 0.007455979939550161}, {"id": 39, "seek": 20720, "start": 221.28, "end": 225.12, "text": " This is you know, I would say this is both an AI talk and a cognitive science talk", "tokens": [51068, 639, 307, 291, 458, 11, 286, 576, 584, 341, 307, 1293, 364, 7318, 751, 293, 257, 15605, 3497, 751, 51260], "temperature": 0.0, "avg_logprob": -0.13128909886440385, "compression_ratio": 1.6958041958041958, "no_speech_prob": 0.007455979939550161}, {"id": 40, "seek": 20720, "start": 225.2, "end": 228.48, "text": " Although most fundamentally to me. I'm really interested in the computational structure", "tokens": [51264, 5780, 881, 17879, 281, 385, 13, 286, 478, 534, 3102, 294, 264, 28270, 3877, 51428], "temperature": 0.0, "avg_logprob": -0.13128909886440385, "compression_ratio": 1.6958041958041958, "no_speech_prob": 0.007455979939550161}, {"id": 41, "seek": 20720, "start": 229.28, "end": 231.2, "text": " of the human mind", "tokens": [51468, 295, 264, 1952, 1575, 51564], "temperature": 0.0, "avg_logprob": -0.13128909886440385, "compression_ratio": 1.6958041958041958, "no_speech_prob": 0.007455979939550161}, {"id": 42, "seek": 20720, "start": 231.2, "end": 232.64, "text": " So just in that", "tokens": [51564, 407, 445, 294, 300, 51636], "temperature": 0.0, "avg_logprob": -0.13128909886440385, "compression_ratio": 1.6958041958041958, "no_speech_prob": 0.007455979939550161}, {"id": 43, "seek": 20720, "start": 232.64, "end": 234.64, "text": " hear it in that lens", "tokens": [51636, 1568, 309, 294, 300, 6765, 51736], "temperature": 0.0, "avg_logprob": -0.13128909886440385, "compression_ratio": 1.6958041958041958, "no_speech_prob": 0.007455979939550161}, {"id": 44, "seek": 23464, "start": 234.64, "end": 237.27999999999997, "text": " But I think also just sort of very generally", "tokens": [50364, 583, 286, 519, 611, 445, 1333, 295, 588, 5101, 50496], "temperature": 0.0, "avg_logprob": -0.1321801394224167, "compression_ratio": 1.6772151898734178, "no_speech_prob": 0.0007912940345704556}, {"id": 45, "seek": 23464, "start": 238.0, "end": 241.04, "text": " You know, nobody can fail to be surprised and impressed", "tokens": [50532, 509, 458, 11, 5079, 393, 3061, 281, 312, 6100, 293, 11679, 50684], "temperature": 0.0, "avg_logprob": -0.1321801394224167, "compression_ratio": 1.6772151898734178, "no_speech_prob": 0.0007912940345704556}, {"id": 46, "seek": 23464, "start": 241.6, "end": 244.64, "text": " At what has happened with the most recent machine learning models", "tokens": [50712, 1711, 437, 575, 2011, 365, 264, 881, 5162, 3479, 2539, 5245, 50864], "temperature": 0.0, "avg_logprob": -0.1321801394224167, "compression_ratio": 1.6772151898734178, "no_speech_prob": 0.0007912940345704556}, {"id": 47, "seek": 23464, "start": 245.67999999999998, "end": 250.0, "text": " And at the same time they're they're very puzzling and confounding in certain ways", "tokens": [50916, 400, 412, 264, 912, 565, 436, 434, 436, 434, 588, 18741, 1688, 293, 1497, 24625, 294, 1629, 2098, 51132], "temperature": 0.0, "avg_logprob": -0.1321801394224167, "compression_ratio": 1.6772151898734178, "no_speech_prob": 0.0007912940345704556}, {"id": 48, "seek": 23464, "start": 250.0, "end": 252.56, "text": " So I hope to try to be able to resolve some of that or at least", "tokens": [51132, 407, 286, 1454, 281, 853, 281, 312, 1075, 281, 14151, 512, 295, 300, 420, 412, 1935, 51260], "temperature": 0.0, "avg_logprob": -0.1321801394224167, "compression_ratio": 1.6772151898734178, "no_speech_prob": 0.0007912940345704556}, {"id": 49, "seek": 23464, "start": 252.95999999999998, "end": 254.95999999999998, "text": " Point the way towards some of that", "tokens": [51280, 12387, 264, 636, 3030, 512, 295, 300, 51380], "temperature": 0.0, "avg_logprob": -0.1321801394224167, "compression_ratio": 1.6772151898734178, "no_speech_prob": 0.0007912940345704556}, {"id": 50, "seek": 23464, "start": 255.35999999999999, "end": 258.32, "text": " This paper the work i'm going to talk about is joint with a number of people", "tokens": [51400, 639, 3035, 264, 589, 741, 478, 516, 281, 751, 466, 307, 7225, 365, 257, 1230, 295, 561, 51548], "temperature": 0.0, "avg_logprob": -0.1321801394224167, "compression_ratio": 1.6772151898734178, "no_speech_prob": 0.0007912940345704556}, {"id": 51, "seek": 23464, "start": 258.64, "end": 264.47999999999996, "text": " I want to single out two people Lionel long and Gabe grand who are the joint first authors of that paper", "tokens": [51564, 286, 528, 281, 2167, 484, 732, 561, 21704, 338, 938, 293, 39524, 2697, 567, 366, 264, 7225, 700, 16552, 295, 300, 3035, 51856], "temperature": 0.0, "avg_logprob": -0.1321801394224167, "compression_ratio": 1.6772151898734178, "no_speech_prob": 0.0007912940345704556}, {"id": 52, "seek": 26464, "start": 264.64, "end": 266.64, "text": " I'm putting Lionel a little bit bigger", "tokens": [50364, 286, 478, 3372, 21704, 338, 257, 707, 857, 3801, 50464], "temperature": 0.0, "avg_logprob": -0.10060218201965845, "compression_ratio": 1.6151202749140894, "no_speech_prob": 0.0007316218107007444}, {"id": 53, "seek": 26464, "start": 267.2, "end": 269.84, "text": " In part because a lot more of my slides are taken from lines", "tokens": [50492, 682, 644, 570, 257, 688, 544, 295, 452, 9788, 366, 2726, 490, 3876, 50624], "temperature": 0.0, "avg_logprob": -0.10060218201965845, "compression_ratio": 1.6151202749140894, "no_speech_prob": 0.0007316218107007444}, {"id": 54, "seek": 26464, "start": 270.24, "end": 275.91999999999996, "text": " So I would say all of the credit for the good stuff both on the research and in the slides to Lionel and Gabe", "tokens": [50644, 407, 286, 576, 584, 439, 295, 264, 5397, 337, 264, 665, 1507, 1293, 322, 264, 2132, 293, 294, 264, 9788, 281, 21704, 338, 293, 39524, 50928], "temperature": 0.0, "avg_logprob": -0.10060218201965845, "compression_ratio": 1.6151202749140894, "no_speech_prob": 0.0007316218107007444}, {"id": 55, "seek": 26464, "start": 277.36, "end": 282.32, "text": " And you know anything that doesn't quite work or make sense is probably just me garbling things", "tokens": [51000, 400, 291, 458, 1340, 300, 1177, 380, 1596, 589, 420, 652, 2020, 307, 1391, 445, 385, 3691, 18262, 721, 51248], "temperature": 0.0, "avg_logprob": -0.10060218201965845, "compression_ratio": 1.6151202749140894, "no_speech_prob": 0.0007316218107007444}, {"id": 56, "seek": 26464, "start": 282.96, "end": 284.0, "text": " Okay", "tokens": [51280, 1033, 51332], "temperature": 0.0, "avg_logprob": -0.10060218201965845, "compression_ratio": 1.6151202749140894, "no_speech_prob": 0.0007316218107007444}, {"id": 57, "seek": 26464, "start": 284.0, "end": 287.12, "text": " But a lot of other people contribute to the work that i'm going to talk about here", "tokens": [51332, 583, 257, 688, 295, 661, 561, 10586, 281, 264, 589, 300, 741, 478, 516, 281, 751, 466, 510, 51488], "temperature": 0.0, "avg_logprob": -0.10060218201965845, "compression_ratio": 1.6151202749140894, "no_speech_prob": 0.0007316218107007444}, {"id": 58, "seek": 26464, "start": 288.0, "end": 292.4, "text": " So again, you know, we can't who cannot be impressed with the advances of AI", "tokens": [51532, 407, 797, 11, 291, 458, 11, 321, 393, 380, 567, 2644, 312, 11679, 365, 264, 25297, 295, 7318, 51752], "temperature": 0.0, "avg_logprob": -0.10060218201965845, "compression_ratio": 1.6151202749140894, "no_speech_prob": 0.0007316218107007444}, {"id": 59, "seek": 29240, "start": 292.64, "end": 296.32, "text": " Whether it's in perception and and you know robots deployed in the real world", "tokens": [50376, 8503, 309, 311, 294, 12860, 293, 293, 291, 458, 14733, 17826, 294, 264, 957, 1002, 50560], "temperature": 0.0, "avg_logprob": -0.11779276529947917, "compression_ratio": 1.657718120805369, "no_speech_prob": 0.006190334912389517}, {"id": 60, "seek": 29240, "start": 296.32, "end": 302.64, "text": " Like the self-driving car systems of tesla and wemo or most recently conversational AI systems like chat gpt", "tokens": [50560, 1743, 264, 2698, 12, 47094, 1032, 3652, 295, 20018, 875, 293, 321, 3280, 420, 881, 3938, 2615, 1478, 7318, 3652, 411, 5081, 290, 662, 50876], "temperature": 0.0, "avg_logprob": -0.11779276529947917, "compression_ratio": 1.657718120805369, "no_speech_prob": 0.006190334912389517}, {"id": 61, "seek": 29240, "start": 303.84, "end": 306.23999999999995, "text": " Clearly there's something really interesting and important happening", "tokens": [50936, 24120, 456, 311, 746, 534, 1880, 293, 1021, 2737, 51056], "temperature": 0.0, "avg_logprob": -0.11779276529947917, "compression_ratio": 1.657718120805369, "no_speech_prob": 0.006190334912389517}, {"id": 62, "seek": 29240, "start": 306.4, "end": 308.56, "text": " But at the same time something quite puzzling now", "tokens": [51064, 583, 412, 264, 912, 565, 746, 1596, 18741, 1688, 586, 51172], "temperature": 0.0, "avg_logprob": -0.11779276529947917, "compression_ratio": 1.657718120805369, "no_speech_prob": 0.006190334912389517}, {"id": 63, "seek": 29240, "start": 308.56, "end": 311.12, "text": " There's a number of puzzles that people here have talked about just one", "tokens": [51172, 821, 311, 257, 1230, 295, 24138, 300, 561, 510, 362, 2825, 466, 445, 472, 51300], "temperature": 0.0, "avg_logprob": -0.11779276529947917, "compression_ratio": 1.657718120805369, "no_speech_prob": 0.006190334912389517}, {"id": 64, "seek": 29240, "start": 311.12, "end": 313.91999999999996, "text": " I'll point to is what you might call the puzzle of confabulation", "tokens": [51300, 286, 603, 935, 281, 307, 437, 291, 1062, 818, 264, 12805, 295, 1497, 455, 2776, 51440], "temperature": 0.0, "avg_logprob": -0.11779276529947917, "compression_ratio": 1.657718120805369, "no_speech_prob": 0.006190334912389517}, {"id": 65, "seek": 29240, "start": 315.28, "end": 317.52, "text": " Which points to what ways I think that the kinds of", "tokens": [51508, 3013, 2793, 281, 437, 2098, 286, 519, 300, 264, 3685, 295, 51620], "temperature": 0.0, "avg_logprob": -0.11779276529947917, "compression_ratio": 1.657718120805369, "no_speech_prob": 0.006190334912389517}, {"id": 66, "seek": 31752, "start": 318.47999999999996, "end": 325.28, "text": " What intelligence or unquote intelligence that we see in these systems is different in nature and origins from that in our own minds", "tokens": [50412, 708, 7599, 420, 37557, 7599, 300, 321, 536, 294, 613, 3652, 307, 819, 294, 3687, 293, 22721, 490, 300, 294, 527, 1065, 9634, 50752], "temperature": 0.0, "avg_logprob": -0.17159614562988282, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.010813700966536999}, {"id": 67, "seek": 31752, "start": 325.76, "end": 329.52, "text": " So think about for example in the context of driving. This is a video from tesla", "tokens": [50776, 407, 519, 466, 337, 1365, 294, 264, 4319, 295, 4840, 13, 639, 307, 257, 960, 490, 20018, 875, 50964], "temperature": 0.0, "avg_logprob": -0.17159614562988282, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.010813700966536999}, {"id": 68, "seek": 31752, "start": 330.15999999999997, "end": 336.79999999999995, "text": " A self-driving system where it's it's faced with something a little bit out of its training set a horse drawn carriage and and", "tokens": [50996, 316, 2698, 12, 47094, 1185, 689, 309, 311, 309, 311, 11446, 365, 746, 257, 707, 857, 484, 295, 1080, 3097, 992, 257, 6832, 10117, 31811, 293, 293, 51328], "temperature": 0.0, "avg_logprob": -0.17159614562988282, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.010813700966536999}, {"id": 69, "seek": 31752, "start": 337.84, "end": 342.32, "text": " Buggy and it interprets it as a truck first facing one way the other then it's", "tokens": [51380, 23821, 1480, 293, 309, 17489, 1373, 309, 382, 257, 5898, 700, 7170, 472, 636, 264, 661, 550, 309, 311, 51604], "temperature": 0.0, "avg_logprob": -0.17159614562988282, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.010813700966536999}, {"id": 70, "seek": 31752, "start": 343.44, "end": 345.68, "text": " SUV then it's a truck with the person behind it", "tokens": [51660, 28452, 550, 309, 311, 257, 5898, 365, 264, 954, 2261, 309, 51772], "temperature": 0.0, "avg_logprob": -0.17159614562988282, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.010813700966536999}, {"id": 71, "seek": 34568, "start": 346.64, "end": 348.64, "text": " Now the person's gone away", "tokens": [50412, 823, 264, 954, 311, 2780, 1314, 50512], "temperature": 0.0, "avg_logprob": -0.16278615728154913, "compression_ratio": 1.6, "no_speech_prob": 0.0009397333487868309}, {"id": 72, "seek": 34568, "start": 348.72, "end": 350.72, "text": " Shortly the person will come back", "tokens": [50516, 40109, 264, 954, 486, 808, 646, 50616], "temperature": 0.0, "avg_logprob": -0.16278615728154913, "compression_ratio": 1.6, "no_speech_prob": 0.0009397333487868309}, {"id": 73, "seek": 34568, "start": 351.52, "end": 353.68, "text": " You know, they can't figure out which way the truck is facing", "tokens": [50656, 509, 458, 11, 436, 393, 380, 2573, 484, 597, 636, 264, 5898, 307, 7170, 50764], "temperature": 0.0, "avg_logprob": -0.16278615728154913, "compression_ratio": 1.6, "no_speech_prob": 0.0009397333487868309}, {"id": 74, "seek": 34568, "start": 354.32, "end": 359.28000000000003, "text": " If a person were having this experience on the road, you would think they should stop driving and get off the road as soon as possible", "tokens": [50796, 759, 257, 954, 645, 1419, 341, 1752, 322, 264, 3060, 11, 291, 576, 519, 436, 820, 1590, 4840, 293, 483, 766, 264, 3060, 382, 2321, 382, 1944, 51044], "temperature": 0.0, "avg_logprob": -0.16278615728154913, "compression_ratio": 1.6, "no_speech_prob": 0.0009397333487868309}, {"id": 75, "seek": 34568, "start": 359.92, "end": 361.92, "text": " Right. There's something fundamentally", "tokens": [51076, 1779, 13, 821, 311, 746, 17879, 51176], "temperature": 0.0, "avg_logprob": -0.16278615728154913, "compression_ratio": 1.6, "no_speech_prob": 0.0009397333487868309}, {"id": 76, "seek": 34568, "start": 362.56, "end": 364.56, "text": " Detached about their mind from reality here", "tokens": [51208, 4237, 15095, 466, 641, 1575, 490, 4103, 510, 51308], "temperature": 0.0, "avg_logprob": -0.16278615728154913, "compression_ratio": 1.6, "no_speech_prob": 0.0009397333487868309}, {"id": 77, "seek": 34568, "start": 365.6, "end": 371.2, "text": " And when this happens in systems where lives are at stake, it can be deadly. So this is a serious worry", "tokens": [51360, 400, 562, 341, 2314, 294, 3652, 689, 2909, 366, 412, 10407, 11, 309, 393, 312, 18232, 13, 407, 341, 307, 257, 3156, 3292, 51640], "temperature": 0.0, "avg_logprob": -0.16278615728154913, "compression_ratio": 1.6, "no_speech_prob": 0.0009397333487868309}, {"id": 78, "seek": 34568, "start": 371.76, "end": 373.04, "text": " technologically and", "tokens": [51668, 1537, 17157, 293, 51732], "temperature": 0.0, "avg_logprob": -0.16278615728154913, "compression_ratio": 1.6, "no_speech_prob": 0.0009397333487868309}, {"id": 79, "seek": 37304, "start": 373.12, "end": 379.76000000000005, "text": " Sightly now when it comes to computer vision and tesla self-driving a lot of progress has been made", "tokens": [50368, 318, 397, 356, 586, 562, 309, 1487, 281, 3820, 5201, 293, 20018, 875, 2698, 12, 47094, 257, 688, 295, 4205, 575, 668, 1027, 50700], "temperature": 0.0, "avg_logprob": -0.13852657415927985, "compression_ratio": 1.7052631578947368, "no_speech_prob": 0.0017539116088300943}, {"id": 80, "seek": 37304, "start": 381.68, "end": 384.88, "text": " What I showed you what i'm showing you here is from a few years ago", "tokens": [50796, 708, 286, 4712, 291, 437, 741, 478, 4099, 291, 510, 307, 490, 257, 1326, 924, 2057, 50956], "temperature": 0.0, "avg_logprob": -0.13852657415927985, "compression_ratio": 1.7052631578947368, "no_speech_prob": 0.0017539116088300943}, {"id": 81, "seek": 37304, "start": 385.68, "end": 389.36, "text": " Um, but you still see similar kinds of things and also on the language side", "tokens": [50996, 3301, 11, 457, 291, 920, 536, 2531, 3685, 295, 721, 293, 611, 322, 264, 2856, 1252, 51180], "temperature": 0.0, "avg_logprob": -0.13852657415927985, "compression_ratio": 1.7052631578947368, "no_speech_prob": 0.0017539116088300943}, {"id": 82, "seek": 37304, "start": 389.36, "end": 394.56, "text": " I'm just going to show one example of analogous kinds of things that really make you scratch your head", "tokens": [51180, 286, 478, 445, 516, 281, 855, 472, 1365, 295, 16660, 563, 3685, 295, 721, 300, 534, 652, 291, 8459, 428, 1378, 51440], "temperature": 0.0, "avg_logprob": -0.13852657415927985, "compression_ratio": 1.7052631578947368, "no_speech_prob": 0.0017539116088300943}, {"id": 83, "seek": 37304, "start": 394.96000000000004, "end": 400.96000000000004, "text": " On the you you could find these in language models, but i'm going to talk about multimodal language and vision models because this is where", "tokens": [51460, 1282, 264, 291, 291, 727, 915, 613, 294, 2856, 5245, 11, 457, 741, 478, 516, 281, 751, 466, 32972, 378, 304, 2856, 293, 5201, 5245, 570, 341, 307, 689, 51760], "temperature": 0.0, "avg_logprob": -0.13852657415927985, "compression_ratio": 1.7052631578947368, "no_speech_prob": 0.0017539116088300943}, {"id": 84, "seek": 40096, "start": 401.28, "end": 404.4, "text": " You know, there's a lot of really interesting stuff going on", "tokens": [50380, 509, 458, 11, 456, 311, 257, 688, 295, 534, 1880, 1507, 516, 322, 50536], "temperature": 0.0, "avg_logprob": -0.1418592808610302, "compression_ratio": 1.6962457337883958, "no_speech_prob": 0.001572624547407031}, {"id": 85, "seek": 40096, "start": 405.2, "end": 408.08, "text": " Where machine learning and especially like grounded language", "tokens": [50576, 2305, 3479, 2539, 293, 2318, 411, 23535, 2856, 50720], "temperature": 0.0, "avg_logprob": -0.1418592808610302, "compression_ratio": 1.6962457337883958, "no_speech_prob": 0.001572624547407031}, {"id": 86, "seek": 40096, "start": 408.47999999999996, "end": 411.35999999999996, "text": " Potentially might meet up with what goes on in cognition", "tokens": [50740, 9145, 3137, 1062, 1677, 493, 365, 437, 1709, 322, 294, 46905, 50884], "temperature": 0.0, "avg_logprob": -0.1418592808610302, "compression_ratio": 1.6962457337883958, "no_speech_prob": 0.001572624547407031}, {"id": 87, "seek": 40096, "start": 411.91999999999996, "end": 415.2, "text": " um, so this is a i'm just going to show you a couple of little demos from", "tokens": [50912, 1105, 11, 370, 341, 307, 257, 741, 478, 445, 516, 281, 855, 291, 257, 1916, 295, 707, 33788, 490, 51076], "temperature": 0.0, "avg_logprob": -0.1418592808610302, "compression_ratio": 1.6962457337883958, "no_speech_prob": 0.001572624547407031}, {"id": 88, "seek": 40096, "start": 416.79999999999995, "end": 422.88, "text": " Google's large language model multimodal language model. Um, gemini both earlier versions and then the most recent version", "tokens": [51156, 3329, 311, 2416, 2856, 2316, 32972, 378, 304, 2856, 2316, 13, 3301, 11, 7173, 3812, 1293, 3071, 9606, 293, 550, 264, 881, 5162, 3037, 51460], "temperature": 0.0, "avg_logprob": -0.1418592808610302, "compression_ratio": 1.6962457337883958, "no_speech_prob": 0.001572624547407031}, {"id": 89, "seek": 40096, "start": 423.52, "end": 429.35999999999996, "text": " This is the figure i'm showing here, which i'm giving to gemini and I should say you can see similar things with chat gpt", "tokens": [51492, 639, 307, 264, 2573, 741, 478, 4099, 510, 11, 597, 741, 478, 2902, 281, 7173, 3812, 293, 286, 820, 584, 291, 393, 536, 2531, 721, 365, 5081, 290, 662, 51784], "temperature": 0.0, "avg_logprob": -0.1418592808610302, "compression_ratio": 1.6962457337883958, "no_speech_prob": 0.001572624547407031}, {"id": 90, "seek": 42936, "start": 429.36, "end": 432.48, "text": " But it's a little bit funnier with the google ones. I'm going to focus on that", "tokens": [50364, 583, 309, 311, 257, 707, 857, 1019, 19165, 365, 264, 20742, 2306, 13, 286, 478, 516, 281, 1879, 322, 300, 50520], "temperature": 0.0, "avg_logprob": -0.13095557689666748, "compression_ratio": 1.7707006369426752, "no_speech_prob": 0.0009249490685760975}, {"id": 91, "seek": 42936, "start": 432.96000000000004, "end": 437.68, "text": " Um, this is an image from my colleague vikash mansinga whose work will also figure in the talk", "tokens": [50544, 3301, 11, 341, 307, 364, 3256, 490, 452, 13532, 371, 1035, 1299, 18868, 41705, 6104, 589, 486, 611, 2573, 294, 264, 751, 50780], "temperature": 0.0, "avg_logprob": -0.13095557689666748, "compression_ratio": 1.7707006369426752, "no_speech_prob": 0.0009249490685760975}, {"id": 92, "seek": 42936, "start": 438.24, "end": 441.12, "text": " Um, vikash is one of the world's experts in probabilistic programming", "tokens": [50808, 3301, 11, 371, 1035, 1299, 307, 472, 295, 264, 1002, 311, 8572, 294, 31959, 3142, 9410, 50952], "temperature": 0.0, "avg_logprob": -0.13095557689666748, "compression_ratio": 1.7707006369426752, "no_speech_prob": 0.0009249490685760975}, {"id": 93, "seek": 42936, "start": 441.28000000000003, "end": 445.36, "text": " He and I are very interested in how our minds and our machines could model uncertainty", "tokens": [50960, 634, 293, 286, 366, 588, 3102, 294, 577, 527, 9634, 293, 527, 8379, 727, 2316, 15697, 51164], "temperature": 0.0, "avg_logprob": -0.13095557689666748, "compression_ratio": 1.7707006369426752, "no_speech_prob": 0.0009249490685760975}, {"id": 94, "seek": 42936, "start": 445.92, "end": 448.88, "text": " So here's an image that that I think all of us when we look at this", "tokens": [51192, 407, 510, 311, 364, 3256, 300, 300, 286, 519, 439, 295, 505, 562, 321, 574, 412, 341, 51340], "temperature": 0.0, "avg_logprob": -0.13095557689666748, "compression_ratio": 1.7707006369426752, "no_speech_prob": 0.0009249490685760975}, {"id": 95, "seek": 42936, "start": 448.96000000000004, "end": 452.48, "text": " It's it's it's pretty clear that though. It's a little hard to read kind of messy", "tokens": [51344, 467, 311, 309, 311, 309, 311, 1238, 1850, 300, 1673, 13, 467, 311, 257, 707, 1152, 281, 1401, 733, 295, 16191, 51520], "temperature": 0.0, "avg_logprob": -0.13095557689666748, "compression_ratio": 1.7707006369426752, "no_speech_prob": 0.0009249490685760975}, {"id": 96, "seek": 42936, "start": 452.64, "end": 455.92, "text": " It's really clear that there's a word here and that the word is uncertainty", "tokens": [51528, 467, 311, 534, 1850, 300, 456, 311, 257, 1349, 510, 293, 300, 264, 1349, 307, 15697, 51692], "temperature": 0.0, "avg_logprob": -0.13095557689666748, "compression_ratio": 1.7707006369426752, "no_speech_prob": 0.0009249490685760975}, {"id": 97, "seek": 45592, "start": 456.48, "end": 460.24, "text": " Um, but what is uh an lm a multimodal lmc from this?", "tokens": [50392, 3301, 11, 457, 437, 307, 2232, 364, 287, 76, 257, 32972, 378, 304, 287, 76, 66, 490, 341, 30, 50580], "temperature": 0.0, "avg_logprob": -0.1586591327001178, "compression_ratio": 1.731060606060606, "no_speech_prob": 0.012236340902745724}, {"id": 98, "seek": 45592, "start": 460.64000000000004, "end": 465.04, "text": " Well, it might see this the image you sent me shows a black and white image of a text box", "tokens": [50600, 1042, 11, 309, 1062, 536, 341, 264, 3256, 291, 2279, 385, 3110, 257, 2211, 293, 2418, 3256, 295, 257, 2487, 2424, 50820], "temperature": 0.0, "avg_logprob": -0.1586591327001178, "compression_ratio": 1.731060606060606, "no_speech_prob": 0.012236340902745724}, {"id": 99, "seek": 45592, "start": 465.12, "end": 471.52000000000004, "text": " The text book has a dark gray border rounded corners. Good inside is the word dream written in a bold sans serif font", "tokens": [50824, 440, 2487, 1446, 575, 257, 2877, 10855, 7838, 23382, 12413, 13, 2205, 1854, 307, 264, 1349, 3055, 3720, 294, 257, 11928, 12177, 816, 351, 10703, 51144], "temperature": 0.0, "avg_logprob": -0.1586591327001178, "compression_ratio": 1.731060606060606, "no_speech_prob": 0.012236340902745724}, {"id": 100, "seek": 45592, "start": 472.24, "end": 475.44, "text": " Is there anything else I can help you with? I ask how many letters are shown in the text box?", "tokens": [51180, 1119, 456, 1340, 1646, 286, 393, 854, 291, 365, 30, 286, 1029, 577, 867, 7825, 366, 4898, 294, 264, 2487, 2424, 30, 51340], "temperature": 0.0, "avg_logprob": -0.1586591327001178, "compression_ratio": 1.731060606060606, "no_speech_prob": 0.012236340902745724}, {"id": 101, "seek": 45592, "start": 475.68, "end": 479.68, "text": " There are five letters shown in the text box the letters are d r e a and m", "tokens": [51352, 821, 366, 1732, 7825, 4898, 294, 264, 2487, 2424, 264, 7825, 366, 274, 367, 308, 257, 293, 275, 51552], "temperature": 0.0, "avg_logprob": -0.1586591327001178, "compression_ratio": 1.731060606060606, "no_speech_prob": 0.012236340902745724}, {"id": 102, "seek": 45592, "start": 481.12, "end": 483.12, "text": " What is going on with this?", "tokens": [51624, 708, 307, 516, 322, 365, 341, 30, 51724], "temperature": 0.0, "avg_logprob": -0.1586591327001178, "compression_ratio": 1.731060606060606, "no_speech_prob": 0.012236340902745724}, {"id": 103, "seek": 48312, "start": 483.12, "end": 485.84000000000003, "text": " Now we could say all right. Well, at least it's pretty good", "tokens": [50364, 823, 321, 727, 584, 439, 558, 13, 1042, 11, 412, 1935, 309, 311, 1238, 665, 50500], "temperature": 0.0, "avg_logprob": -0.11287654211761755, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0016740147257223725}, {"id": 104, "seek": 48312, "start": 485.92, "end": 490.56, "text": " I mean at least it's recognizing that there's text and it's you know, it could be worse", "tokens": [50504, 286, 914, 412, 1935, 309, 311, 18538, 300, 456, 311, 2487, 293, 309, 311, 291, 458, 11, 309, 727, 312, 5324, 50736], "temperature": 0.0, "avg_logprob": -0.11287654211761755, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0016740147257223725}, {"id": 105, "seek": 48312, "start": 491.2, "end": 496.4, "text": " But it seems to be strangely self-confident strangely confident about an incorrect answer", "tokens": [50768, 583, 309, 2544, 281, 312, 39851, 2698, 12, 24697, 1078, 39851, 6679, 466, 364, 18424, 1867, 51028], "temperature": 0.0, "avg_logprob": -0.11287654211761755, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0016740147257223725}, {"id": 106, "seek": 48312, "start": 496.64, "end": 500.72, "text": " But you could say at least it's not like seeing a person or person walking down the street", "tokens": [51040, 583, 291, 727, 584, 412, 1935, 309, 311, 406, 411, 2577, 257, 954, 420, 954, 4494, 760, 264, 4838, 51244], "temperature": 0.0, "avg_logprob": -0.11287654211761755, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0016740147257223725}, {"id": 107, "seek": 48312, "start": 501.76, "end": 508.4, "text": " But consider this small change to the input image now our lm says this shows a black and white pixelated image of a person", "tokens": [51296, 583, 1949, 341, 1359, 1319, 281, 264, 4846, 3256, 586, 527, 287, 76, 1619, 341, 3110, 257, 2211, 293, 2418, 19261, 770, 3256, 295, 257, 954, 51628], "temperature": 0.0, "avg_logprob": -0.11287654211761755, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0016740147257223725}, {"id": 108, "seek": 50840, "start": 508.64, "end": 513.12, "text": " The person is facing forward and appears to be standing still. Here are some details about the person", "tokens": [50376, 440, 954, 307, 7170, 2128, 293, 7038, 281, 312, 4877, 920, 13, 1692, 366, 512, 4365, 466, 264, 954, 50600], "temperature": 0.0, "avg_logprob": -0.11313968991476392, "compression_ratio": 1.6996996996996998, "no_speech_prob": 0.032564982771873474}, {"id": 109, "seek": 50840, "start": 513.4399999999999, "end": 517.52, "text": " They have short hair. They're wearing a shirt with a collar. Their legs are slightly wider than their torso", "tokens": [50616, 814, 362, 2099, 2578, 13, 814, 434, 4769, 257, 8336, 365, 257, 20672, 13, 6710, 5668, 366, 4748, 11842, 813, 641, 34917, 50820], "temperature": 0.0, "avg_logprob": -0.11313968991476392, "compression_ratio": 1.6996996996996998, "no_speech_prob": 0.032564982771873474}, {"id": 110, "seek": 50840, "start": 518.0799999999999, "end": 519.36, "text": " Since the image is pixelated", "tokens": [50848, 4162, 264, 3256, 307, 19261, 770, 50912], "temperature": 0.0, "avg_logprob": -0.11313968991476392, "compression_ratio": 1.6996996996996998, "no_speech_prob": 0.032564982771873474}, {"id": 111, "seek": 50840, "start": 519.36, "end": 523.1999999999999, "text": " It's difficult to discern other details such as facial features or specific articles of clothing", "tokens": [50912, 467, 311, 2252, 281, 30868, 661, 4365, 1270, 382, 15642, 4122, 420, 2685, 11290, 295, 11502, 51104], "temperature": 0.0, "avg_logprob": -0.11313968991476392, "compression_ratio": 1.6996996996996998, "no_speech_prob": 0.032564982771873474}, {"id": 112, "seek": 50840, "start": 523.6, "end": 527.12, "text": " Now again, the thing that I think should make us puzzle here is", "tokens": [51124, 823, 797, 11, 264, 551, 300, 286, 519, 820, 652, 505, 12805, 510, 307, 51300], "temperature": 0.0, "avg_logprob": -0.11313968991476392, "compression_ratio": 1.6996996996996998, "no_speech_prob": 0.032564982771873474}, {"id": 113, "seek": 50840, "start": 527.92, "end": 533.36, "text": " Is not just that the system gets this wrong, but that it seems to confidently assert things that are so", "tokens": [51340, 1119, 406, 445, 300, 264, 1185, 2170, 341, 2085, 11, 457, 300, 309, 2544, 281, 41956, 19810, 721, 300, 366, 370, 51612], "temperature": 0.0, "avg_logprob": -0.11313968991476392, "compression_ratio": 1.6996996996996998, "no_speech_prob": 0.032564982771873474}, {"id": 114, "seek": 50840, "start": 534.4, "end": 538.0, "text": " detached from our sense of of the reality of what we're seeing", "tokens": [51664, 42050, 490, 527, 2020, 295, 295, 264, 4103, 295, 437, 321, 434, 2577, 51844], "temperature": 0.0, "avg_logprob": -0.11313968991476392, "compression_ratio": 1.6996996996996998, "no_speech_prob": 0.032564982771873474}, {"id": 115, "seek": 53840, "start": 538.64, "end": 541.4399999999999, "text": " And where just a small change can make such a huge difference", "tokens": [50376, 400, 689, 445, 257, 1359, 1319, 393, 652, 1270, 257, 2603, 2649, 50516], "temperature": 0.0, "avg_logprob": -0.13999392843654013, "compression_ratio": 1.759322033898305, "no_speech_prob": 0.000404362945118919}, {"id": 116, "seek": 53840, "start": 542.0799999999999, "end": 548.72, "text": " Just to update things this was this was from the first version of google's gemini the most recent version of gemini 1.5 pro", "tokens": [50548, 1449, 281, 5623, 721, 341, 390, 341, 390, 490, 264, 700, 3037, 295, 20742, 311, 7173, 3812, 264, 881, 5162, 3037, 295, 7173, 3812, 502, 13, 20, 447, 50880], "temperature": 0.0, "avg_logprob": -0.13999392843654013, "compression_ratio": 1.759322033898305, "no_speech_prob": 0.000404362945118919}, {"id": 117, "seek": 53840, "start": 549.12, "end": 551.12, "text": " Has gotten better in some sense", "tokens": [50900, 8646, 5768, 1101, 294, 512, 2020, 51000], "temperature": 0.0, "avg_logprob": -0.13999392843654013, "compression_ratio": 1.759322033898305, "no_speech_prob": 0.000404362945118919}, {"id": 118, "seek": 53840, "start": 551.52, "end": 554.88, "text": " Now given uncertainty. It doesn't say dream. It says optical illusion", "tokens": [51020, 823, 2212, 15697, 13, 467, 1177, 380, 584, 3055, 13, 467, 1619, 20674, 18854, 51188], "temperature": 0.0, "avg_logprob": -0.13999392843654013, "compression_ratio": 1.759322033898305, "no_speech_prob": 0.000404362945118919}, {"id": 119, "seek": 53840, "start": 555.84, "end": 560.9599999999999, "text": " But it's still wrong and it's still over confident. Um, it rates itself as fairly confident", "tokens": [51236, 583, 309, 311, 920, 2085, 293, 309, 311, 920, 670, 6679, 13, 3301, 11, 309, 6846, 2564, 382, 6457, 6679, 51492], "temperature": 0.0, "avg_logprob": -0.13999392843654013, "compression_ratio": 1.759322033898305, "no_speech_prob": 0.000404362945118919}, {"id": 120, "seek": 53840, "start": 561.68, "end": 568.16, "text": " That it says optical illusion. It's interesting. It makes you think is there some semantic association between dreams optical illusions and", "tokens": [51528, 663, 309, 1619, 20674, 18854, 13, 467, 311, 1880, 13, 467, 1669, 291, 519, 307, 456, 512, 47982, 14598, 1296, 7505, 20674, 49836, 293, 51852], "temperature": 0.0, "avg_logprob": -0.13999392843654013, "compression_ratio": 1.759322033898305, "no_speech_prob": 0.000404362945118919}, {"id": 121, "seek": 56840, "start": 568.4, "end": 572.48, "text": " This particular way of presenting the notion of uncertainty. I don't know. Um,", "tokens": [50364, 639, 1729, 636, 295, 15578, 264, 10710, 295, 15697, 13, 286, 500, 380, 458, 13, 3301, 11, 50568], "temperature": 0.0, "avg_logprob": -0.1286667539873196, "compression_ratio": 1.6870967741935483, "no_speech_prob": 0.0005882293917238712}, {"id": 122, "seek": 56840, "start": 573.12, "end": 578.0799999999999, "text": " The the very most recent thing this was well, it's not that recent but the last time I gave this talk", "tokens": [50600, 440, 264, 588, 881, 5162, 551, 341, 390, 731, 11, 309, 311, 406, 300, 5162, 457, 264, 1036, 565, 286, 2729, 341, 751, 50848], "temperature": 0.0, "avg_logprob": -0.1286667539873196, "compression_ratio": 1.6870967741935483, "no_speech_prob": 0.0005882293917238712}, {"id": 123, "seek": 56840, "start": 578.8, "end": 585.1999999999999, "text": " Um, I tried this on the the newest updated version of gemini 1.5 pro just to see if perhaps it had been improved", "tokens": [50884, 3301, 11, 286, 3031, 341, 322, 264, 264, 17569, 10588, 3037, 295, 7173, 3812, 502, 13, 20, 447, 445, 281, 536, 498, 4317, 309, 632, 668, 9689, 51204], "temperature": 0.0, "avg_logprob": -0.1286667539873196, "compression_ratio": 1.6870967741935483, "no_speech_prob": 0.0005882293917238712}, {"id": 124, "seek": 56840, "start": 585.6, "end": 587.6, "text": " And now it gave a very different answer", "tokens": [51224, 400, 586, 309, 2729, 257, 588, 819, 1867, 51324], "temperature": 0.0, "avg_logprob": -0.1286667539873196, "compression_ratio": 1.6870967741935483, "no_speech_prob": 0.0005882293917238712}, {"id": 125, "seek": 56840, "start": 587.6, "end": 590.16, "text": " It didn't say it didn't read it as optical illusion. It gave this", "tokens": [51324, 467, 994, 380, 584, 309, 994, 380, 1401, 309, 382, 20674, 18854, 13, 467, 2729, 341, 51452], "temperature": 0.0, "avg_logprob": -0.1286667539873196, "compression_ratio": 1.6870967741935483, "no_speech_prob": 0.0005882293917238712}, {"id": 126, "seek": 56840, "start": 591.04, "end": 596.4, "text": " Extended digit number six one two nine eight one five three two four two and I asked it to judge its confidence and it said", "tokens": [51496, 9881, 3502, 14293, 1230, 2309, 472, 732, 4949, 3180, 472, 1732, 1045, 732, 1451, 732, 293, 286, 2351, 309, 281, 6995, 1080, 6687, 293, 309, 848, 51764], "temperature": 0.0, "avg_logprob": -0.1286667539873196, "compression_ratio": 1.6870967741935483, "no_speech_prob": 0.0005882293917238712}, {"id": 127, "seek": 59640, "start": 596.56, "end": 598.56, "text": " Well 70 to 80 percent", "tokens": [50372, 1042, 5285, 281, 4688, 3043, 50472], "temperature": 0.0, "avg_logprob": -0.19801610449086066, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.003649105317890644}, {"id": 128, "seek": 59640, "start": 598.64, "end": 603.84, "text": " Again, what what happened? Just to check again consistency and coherence of this", "tokens": [50476, 3764, 11, 437, 437, 2011, 30, 1449, 281, 1520, 797, 14416, 293, 26528, 655, 295, 341, 50736], "temperature": 0.0, "avg_logprob": -0.19801610449086066, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.003649105317890644}, {"id": 129, "seek": 59640, "start": 604.3199999999999, "end": 608.72, "text": " Weird interpretation. I asked it just 10 seconds later just in an out of context again", "tokens": [50760, 32033, 14174, 13, 286, 2351, 309, 445, 1266, 3949, 1780, 445, 294, 364, 484, 295, 4319, 797, 50980], "temperature": 0.0, "avg_logprob": -0.19801610449086066, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.003649105317890644}, {"id": 130, "seek": 59640, "start": 608.8, "end": 613.36, "text": " Just gave it the same thing and the next 10 seconds later. It said geocache", "tokens": [50984, 1449, 2729, 309, 264, 912, 551, 293, 264, 958, 1266, 3949, 1780, 13, 467, 848, 1519, 905, 6000, 51212], "temperature": 0.0, "avg_logprob": -0.19801610449086066, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.003649105317890644}, {"id": 131, "seek": 59640, "start": 614.0, "end": 618.0799999999999, "text": " Um, again now 75 confident and then one more time", "tokens": [51244, 3301, 11, 797, 586, 9562, 6679, 293, 550, 472, 544, 565, 51448], "temperature": 0.0, "avg_logprob": -0.19801610449086066, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.003649105317890644}, {"id": 132, "seek": 59640, "start": 619.1999999999999, "end": 623.28, "text": " Um, a distorted text appears to read northeast blackout", "tokens": [51504, 3301, 11, 257, 33431, 2487, 7038, 281, 1401, 40984, 2211, 346, 51708], "temperature": 0.0, "avg_logprob": -0.19801610449086066, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.003649105317890644}, {"id": 133, "seek": 62328, "start": 624.24, "end": 625.6, "text": " However, it's difficult to be certain", "tokens": [50412, 2908, 11, 309, 311, 2252, 281, 312, 1629, 50480], "temperature": 0.0, "avg_logprob": -0.07408042216864158, "compression_ratio": 1.7532467532467533, "no_speech_prob": 0.006094321142882109}, {"id": 134, "seek": 62328, "start": 625.6, "end": 631.04, "text": " But it's moderately confident that the text reads northeast blackout because the first and last words are relatively clear", "tokens": [50480, 583, 309, 311, 10494, 1592, 6679, 300, 264, 2487, 15700, 40984, 2211, 346, 570, 264, 700, 293, 1036, 2283, 366, 7226, 1850, 50752], "temperature": 0.0, "avg_logprob": -0.07408042216864158, "compression_ratio": 1.7532467532467533, "no_speech_prob": 0.006094321142882109}, {"id": 135, "seek": 62328, "start": 631.04, "end": 634.88, "text": " While the middle word is slightly less discernible, but still suggestive of blackout", "tokens": [50752, 3987, 264, 2808, 1349, 307, 4748, 1570, 30868, 964, 11, 457, 920, 3402, 488, 295, 2211, 346, 50944], "temperature": 0.0, "avg_logprob": -0.07408042216864158, "compression_ratio": 1.7532467532467533, "no_speech_prob": 0.006094321142882109}, {"id": 136, "seek": 62328, "start": 635.6, "end": 638.0799999999999, "text": " Okay, so what's going on here?", "tokens": [50980, 1033, 11, 370, 437, 311, 516, 322, 510, 30, 51104], "temperature": 0.0, "avg_logprob": -0.07408042216864158, "compression_ratio": 1.7532467532467533, "no_speech_prob": 0.006094321142882109}, {"id": 137, "seek": 62328, "start": 639.12, "end": 640.16, "text": " That's one of the mysteries now", "tokens": [51156, 663, 311, 472, 295, 264, 30785, 586, 51208], "temperature": 0.0, "avg_logprob": -0.07408042216864158, "compression_ratio": 1.7532467532467533, "no_speech_prob": 0.006094321142882109}, {"id": 138, "seek": 62328, "start": 640.16, "end": 644.64, "text": " I'm not I'm not I'm going to gesture at what I think some of the mysteries or some of the answers might be", "tokens": [51208, 286, 478, 406, 286, 478, 406, 286, 478, 516, 281, 22252, 412, 437, 286, 519, 512, 295, 264, 30785, 420, 512, 295, 264, 6338, 1062, 312, 51432], "temperature": 0.0, "avg_logprob": -0.07408042216864158, "compression_ratio": 1.7532467532467533, "no_speech_prob": 0.006094321142882109}, {"id": 139, "seek": 62328, "start": 644.9599999999999, "end": 650.4, "text": " But mostly to point the way towards the difference with human intelligence and what we're trying to understand in in artwork", "tokens": [51448, 583, 5240, 281, 935, 264, 636, 3030, 264, 2649, 365, 1952, 7599, 293, 437, 321, 434, 1382, 281, 1223, 294, 294, 15829, 51720], "temperature": 0.0, "avg_logprob": -0.07408042216864158, "compression_ratio": 1.7532467532467533, "no_speech_prob": 0.006094321142882109}, {"id": 140, "seek": 65040, "start": 651.1999999999999, "end": 656.24, "text": " Okay, I think fundamentally what's going on is whether you're building a computer vision system or", "tokens": [50404, 1033, 11, 286, 519, 17879, 437, 311, 516, 322, 307, 1968, 291, 434, 2390, 257, 3820, 5201, 1185, 420, 50656], "temperature": 0.0, "avg_logprob": -0.1416313058078879, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.0019259718246757984}, {"id": 141, "seek": 65040, "start": 656.72, "end": 659.76, "text": " An llm or a multimodal language text system", "tokens": [50680, 1107, 287, 75, 76, 420, 257, 32972, 378, 304, 2856, 2487, 1185, 50832], "temperature": 0.0, "avg_logprob": -0.1416313058078879, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.0019259718246757984}, {"id": 142, "seek": 65040, "start": 659.76, "end": 664.64, "text": " You're building a system that takes the inputs and outputs that our brains do", "tokens": [50832, 509, 434, 2390, 257, 1185, 300, 2516, 264, 15743, 293, 23930, 300, 527, 15442, 360, 51076], "temperature": 0.0, "avg_logprob": -0.1416313058078879, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.0019259718246757984}, {"id": 143, "seek": 65040, "start": 664.9599999999999, "end": 671.36, "text": " perception of the eternal world sense data of some form and it produces actions or something like actions that can be", "tokens": [51092, 12860, 295, 264, 14503, 1002, 2020, 1412, 295, 512, 1254, 293, 309, 14725, 5909, 420, 746, 411, 5909, 300, 393, 312, 51412], "temperature": 0.0, "avg_logprob": -0.1416313058078879, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.0019259718246757984}, {"id": 144, "seek": 65040, "start": 671.92, "end": 677.28, "text": " Grounded back in the external world, but the inside of the system doesn't have any notion of a world", "tokens": [51440, 28371, 292, 646, 294, 264, 8320, 1002, 11, 457, 264, 1854, 295, 264, 1185, 1177, 380, 362, 604, 10710, 295, 257, 1002, 51708], "temperature": 0.0, "avg_logprob": -0.1416313058078879, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.0019259718246757984}, {"id": 145, "seek": 67728, "start": 677.36, "end": 678.8, "text": " It's a function approximator", "tokens": [50368, 467, 311, 257, 2445, 8542, 1639, 50440], "temperature": 0.0, "avg_logprob": -0.11523483196894328, "compression_ratio": 1.734375, "no_speech_prob": 0.009262070059776306}, {"id": 146, "seek": 67728, "start": 678.8, "end": 682.8, "text": " It's learning to approximate the input output functions that our minds produce", "tokens": [50440, 467, 311, 2539, 281, 30874, 264, 4846, 5598, 6828, 300, 527, 9634, 5258, 50640], "temperature": 0.0, "avg_logprob": -0.11523483196894328, "compression_ratio": 1.734375, "no_speech_prob": 0.009262070059776306}, {"id": 147, "seek": 67728, "start": 683.1999999999999, "end": 689.36, "text": " And it's learning to do that from various data sources including objective data sources as well as human reinforcement", "tokens": [50660, 400, 309, 311, 2539, 281, 360, 300, 490, 3683, 1412, 7139, 3009, 10024, 1412, 7139, 382, 731, 382, 1952, 29280, 50968], "temperature": 0.0, "avg_logprob": -0.11523483196894328, "compression_ratio": 1.734375, "no_speech_prob": 0.009262070059776306}, {"id": 148, "seek": 67728, "start": 690.16, "end": 694.0799999999999, "text": " Now how why does this possibly work? Well, again, there there may be some", "tokens": [51008, 823, 577, 983, 775, 341, 6264, 589, 30, 1042, 11, 797, 11, 456, 456, 815, 312, 512, 51204], "temperature": 0.0, "avg_logprob": -0.11523483196894328, "compression_ratio": 1.734375, "no_speech_prob": 0.009262070059776306}, {"id": 149, "seek": 67728, "start": 694.56, "end": 700.0, "text": " laws of physics things that are like physics like for example the famous scaling laws of neural language models", "tokens": [51228, 6064, 295, 10649, 721, 300, 366, 411, 10649, 411, 337, 1365, 264, 4618, 21589, 6064, 295, 18161, 2856, 5245, 51500], "temperature": 0.0, "avg_logprob": -0.11523483196894328, "compression_ratio": 1.734375, "no_speech_prob": 0.009262070059776306}, {"id": 150, "seek": 67728, "start": 700.4, "end": 702.4, "text": " Where you can show in some form", "tokens": [51520, 2305, 291, 393, 855, 294, 512, 1254, 51620], "temperature": 0.0, "avg_logprob": -0.11523483196894328, "compression_ratio": 1.734375, "no_speech_prob": 0.009262070059776306}, {"id": 151, "seek": 70240, "start": 702.88, "end": 705.36, "text": " Especially when you're trying to predict like you know", "tokens": [50388, 8545, 562, 291, 434, 1382, 281, 6069, 411, 291, 458, 50512], "temperature": 0.0, "avg_logprob": -0.09445646603902182, "compression_ratio": 1.72782874617737, "no_speech_prob": 0.0034277592785656452}, {"id": 152, "seek": 70240, "start": 705.76, "end": 711.92, "text": " And this is very much following some of the information theoretic ideas that richard has been talking about and others in the group here", "tokens": [50532, 400, 341, 307, 588, 709, 3480, 512, 295, 264, 1589, 14308, 299, 3487, 300, 4593, 515, 575, 668, 1417, 466, 293, 2357, 294, 264, 1594, 510, 50840], "temperature": 0.0, "avg_logprob": -0.09445646603902182, "compression_ratio": 1.72782874617737, "no_speech_prob": 0.0034277592785656452}, {"id": 153, "seek": 70240, "start": 712.9599999999999, "end": 715.92, "text": " If i'm just trying to predict the next token from the previous ones", "tokens": [50892, 759, 741, 478, 445, 1382, 281, 6069, 264, 958, 14862, 490, 264, 3894, 2306, 51040], "temperature": 0.0, "avg_logprob": -0.09445646603902182, "compression_ratio": 1.72782874617737, "no_speech_prob": 0.0034277592785656452}, {"id": 154, "seek": 70240, "start": 716.72, "end": 722.3199999999999, "text": " There are certain fundamental power laws of language and distributions in language that these systems seem to incorporate", "tokens": [51080, 821, 366, 1629, 8088, 1347, 6064, 295, 2856, 293, 37870, 294, 2856, 300, 613, 3652, 1643, 281, 16091, 51360], "temperature": 0.0, "avg_logprob": -0.09445646603902182, "compression_ratio": 1.72782874617737, "no_speech_prob": 0.0034277592785656452}, {"id": 155, "seek": 70240, "start": 722.8, "end": 726.72, "text": " And build on such that if you increase by an order of magnitude the amount of compute", "tokens": [51384, 400, 1322, 322, 1270, 300, 498, 291, 3488, 538, 364, 1668, 295, 15668, 264, 2372, 295, 14722, 51580], "temperature": 0.0, "avg_logprob": -0.09445646603902182, "compression_ratio": 1.72782874617737, "no_speech_prob": 0.0034277592785656452}, {"id": 156, "seek": 70240, "start": 727.04, "end": 731.52, "text": " You can in a predictable way lower the test loss in predicting the next token. That's on the left", "tokens": [51596, 509, 393, 294, 257, 27737, 636, 3126, 264, 1500, 4470, 294, 32884, 264, 958, 14862, 13, 663, 311, 322, 264, 1411, 51820], "temperature": 0.0, "avg_logprob": -0.09445646603902182, "compression_ratio": 1.72782874617737, "no_speech_prob": 0.0034277592785656452}, {"id": 157, "seek": 73152, "start": 732.3199999999999, "end": 734.3199999999999, "text": " In gpt4's technical report", "tokens": [50404, 682, 290, 662, 19, 311, 6191, 2275, 50504], "temperature": 0.0, "avg_logprob": -0.11382889970440731, "compression_ratio": 1.6864111498257839, "no_speech_prob": 0.003428713884204626}, {"id": 158, "seek": 73152, "start": 734.56, "end": 739.12, "text": " They suggested that you could see similar kinds of scaling laws for problem solving not just text prediction", "tokens": [50516, 814, 10945, 300, 291, 727, 536, 2531, 3685, 295, 21589, 6064, 337, 1154, 12606, 406, 445, 2487, 17630, 50744], "temperature": 0.0, "avg_logprob": -0.11382889970440731, "compression_ratio": 1.6864111498257839, "no_speech_prob": 0.003428713884204626}, {"id": 159, "seek": 73152, "start": 739.12, "end": 740.8, "text": " Although it's a lot if you're there", "tokens": [50744, 5780, 309, 311, 257, 688, 498, 291, 434, 456, 50828], "temperature": 0.0, "avg_logprob": -0.11382889970440731, "compression_ratio": 1.6864111498257839, "no_speech_prob": 0.003428713884204626}, {"id": 160, "seek": 73152, "start": 740.8, "end": 745.84, "text": " And I think fundamentally the problem is the power laws are beautiful laws and they're predictable in a certain sense", "tokens": [50828, 400, 286, 519, 17879, 264, 1154, 307, 264, 1347, 6064, 366, 2238, 6064, 293, 436, 434, 27737, 294, 257, 1629, 2020, 51080], "temperature": 0.0, "avg_logprob": -0.11382889970440731, "compression_ratio": 1.6864111498257839, "no_speech_prob": 0.003428713884204626}, {"id": 161, "seek": 73152, "start": 746.16, "end": 748.96, "text": " The thing about a power law is while it approaches asymptote", "tokens": [51096, 440, 551, 466, 257, 1347, 2101, 307, 1339, 309, 11587, 35114, 1370, 51236], "temperature": 0.0, "avg_logprob": -0.11382889970440731, "compression_ratio": 1.6864111498257839, "no_speech_prob": 0.003428713884204626}, {"id": 162, "seek": 73152, "start": 749.36, "end": 752.3199999999999, "text": " Or it rather approaches zero error in its asymptote", "tokens": [51256, 1610, 309, 2831, 11587, 4018, 6713, 294, 1080, 35114, 1370, 51404], "temperature": 0.0, "avg_logprob": -0.11382889970440731, "compression_ratio": 1.6864111498257839, "no_speech_prob": 0.003428713884204626}, {"id": 163, "seek": 73152, "start": 752.8, "end": 757.6, "text": " It in some important sense never gets there compared to like an exponential decay", "tokens": [51428, 467, 294, 512, 1021, 2020, 1128, 2170, 456, 5347, 281, 411, 364, 21510, 21039, 51668], "temperature": 0.0, "avg_logprob": -0.11382889970440731, "compression_ratio": 1.6864111498257839, "no_speech_prob": 0.003428713884204626}, {"id": 164, "seek": 75760, "start": 757.84, "end": 762.4, "text": " Where there's a predictable timescale on which it will get to zero a power law keeps slowing down", "tokens": [50376, 2305, 456, 311, 257, 27737, 1413, 37088, 322, 597, 309, 486, 483, 281, 4018, 257, 1347, 2101, 5965, 26958, 760, 50604], "temperature": 0.0, "avg_logprob": -0.10720736309162622, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.004680367186665535}, {"id": 165, "seek": 75760, "start": 762.8000000000001, "end": 766.96, "text": " and if you have any uncertainty in the power laws coefficient or", "tokens": [50624, 293, 498, 291, 362, 604, 15697, 294, 264, 1347, 6064, 17619, 420, 50832], "temperature": 0.0, "avg_logprob": -0.10720736309162622, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.004680367186665535}, {"id": 166, "seek": 75760, "start": 768.08, "end": 773.76, "text": " You know in its applicability to the not just to predicting the data stream, but actually solving a problem", "tokens": [50888, 509, 458, 294, 1080, 2580, 2310, 281, 264, 406, 445, 281, 32884, 264, 1412, 4309, 11, 457, 767, 12606, 257, 1154, 51172], "temperature": 0.0, "avg_logprob": -0.10720736309162622, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.004680367186665535}, {"id": 167, "seek": 75760, "start": 774.48, "end": 780.4, "text": " Then it's it's basically impossible to know how much data and compute you're going to need to actually get to the asymptote", "tokens": [51208, 1396, 309, 311, 309, 311, 1936, 6243, 281, 458, 577, 709, 1412, 293, 14722, 291, 434, 516, 281, 643, 281, 767, 483, 281, 264, 35114, 1370, 51504], "temperature": 0.0, "avg_logprob": -0.10720736309162622, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.004680367186665535}, {"id": 168, "seek": 75760, "start": 780.4, "end": 783.28, "text": " Which we want to call, you know full adult human intelligence", "tokens": [51504, 3013, 321, 528, 281, 818, 11, 291, 458, 1577, 5075, 1952, 7599, 51648], "temperature": 0.0, "avg_logprob": -0.10720736309162622, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.004680367186665535}, {"id": 169, "seek": 78328, "start": 783.68, "end": 785.68, "text": " In contrast", "tokens": [50384, 682, 8712, 50484], "temperature": 0.0, "avg_logprob": -0.17896952773585464, "compression_ratio": 1.780701754385965, "no_speech_prob": 0.002589965471997857}, {"id": 170, "seek": 78328, "start": 786.3199999999999, "end": 792.4, "text": " Humans don't seem to be built this way. We our minds seem to be built as world modelers from the start", "tokens": [50516, 35809, 500, 380, 1643, 281, 312, 3094, 341, 636, 13, 492, 527, 9634, 1643, 281, 312, 3094, 382, 1002, 2316, 433, 490, 264, 722, 50820], "temperature": 0.0, "avg_logprob": -0.17896952773585464, "compression_ratio": 1.780701754385965, "no_speech_prob": 0.002589965471997857}, {"id": 171, "seek": 78328, "start": 792.56, "end": 794.72, "text": " And I'll say more about this, but from the very beginning", "tokens": [50828, 400, 286, 603, 584, 544, 466, 341, 11, 457, 490, 264, 588, 2863, 50936], "temperature": 0.0, "avg_logprob": -0.17896952773585464, "compression_ratio": 1.780701754385965, "no_speech_prob": 0.002589965471997857}, {"id": 172, "seek": 78328, "start": 795.4399999999999, "end": 798.72, "text": " We have our minds and our brains and this is something that", "tokens": [50972, 492, 362, 527, 9634, 293, 527, 15442, 293, 341, 307, 746, 300, 51136], "temperature": 0.0, "avg_logprob": -0.17896952773585464, "compression_ratio": 1.780701754385965, "no_speech_prob": 0.002589965471997857}, {"id": 173, "seek": 78328, "start": 799.36, "end": 805.36, "text": " Inherits shared with other animals and inherits from our evolutionary legacy seem to be built to model the world", "tokens": [51168, 682, 511, 1208, 5507, 365, 661, 4882, 293, 9484, 1208, 490, 527, 27567, 11711, 1643, 281, 312, 3094, 281, 2316, 264, 1002, 51468], "temperature": 0.0, "avg_logprob": -0.17896952773585464, "compression_ratio": 1.780701754385965, "no_speech_prob": 0.002589965471997857}, {"id": 174, "seek": 78328, "start": 805.8399999999999, "end": 808.8, "text": " And to deal with all kinds of incompleteness and uncertainty", "tokens": [51492, 400, 281, 2028, 365, 439, 3685, 295, 14036, 14657, 15264, 293, 15697, 51640], "temperature": 0.0, "avg_logprob": -0.17896952773585464, "compression_ratio": 1.780701754385965, "no_speech_prob": 0.002589965471997857}, {"id": 175, "seek": 80880, "start": 809.52, "end": 813.12, "text": " And that means both in how in the the structure of the world", "tokens": [50400, 400, 300, 1355, 1293, 294, 577, 294, 264, 264, 3877, 295, 264, 1002, 50580], "temperature": 0.0, "avg_logprob": -0.09959260767156428, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.004396570380777121}, {"id": 176, "seek": 80880, "start": 813.8399999999999, "end": 818.56, "text": " And our uncertainty over what's out there the current state as well as the the fundamental deeper laws", "tokens": [50616, 400, 527, 15697, 670, 437, 311, 484, 456, 264, 2190, 1785, 382, 731, 382, 264, 264, 8088, 7731, 6064, 50852], "temperature": 0.0, "avg_logprob": -0.09959260767156428, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.004396570380777121}, {"id": 177, "seek": 80880, "start": 818.7199999999999, "end": 821.76, "text": " The causal laws of physics or how agents plan and so on", "tokens": [50860, 440, 38755, 6064, 295, 10649, 420, 577, 12554, 1393, 293, 370, 322, 51012], "temperature": 0.0, "avg_logprob": -0.09959260767156428, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.004396570380777121}, {"id": 178, "seek": 80880, "start": 822.0, "end": 825.28, "text": " But we're built to engage these kinds of mental representations", "tokens": [51024, 583, 321, 434, 3094, 281, 4683, 613, 3685, 295, 4973, 33358, 51188], "temperature": 0.0, "avg_logprob": -0.09959260767156428, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.004396570380777121}, {"id": 179, "seek": 80880, "start": 825.8399999999999, "end": 828.0799999999999, "text": " As i'll as i'll show in a little bit the way in our group", "tokens": [51216, 1018, 741, 603, 382, 741, 603, 855, 294, 257, 707, 857, 264, 636, 294, 527, 1594, 51328], "temperature": 0.0, "avg_logprob": -0.09959260767156428, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.004396570380777121}, {"id": 180, "seek": 80880, "start": 828.0799999999999, "end": 833.76, "text": " We've modeled this for a long time is what we call the game engine in the head by analogy to say video game engines", "tokens": [51328, 492, 600, 37140, 341, 337, 257, 938, 565, 307, 437, 321, 818, 264, 1216, 2848, 294, 264, 1378, 538, 21663, 281, 584, 960, 1216, 12982, 51612], "temperature": 0.0, "avg_logprob": -0.09959260767156428, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.004396570380777121}, {"id": 181, "seek": 83376, "start": 834.08, "end": 838.64, "text": " That our brains and minds seem to be built with these kinds of resources for world modeling", "tokens": [50380, 663, 527, 15442, 293, 9634, 1643, 281, 312, 3094, 365, 613, 3685, 295, 3593, 337, 1002, 15983, 50608], "temperature": 0.0, "avg_logprob": -0.13665599293178982, "compression_ratio": 1.768421052631579, "no_speech_prob": 0.017432501539587975}, {"id": 182, "seek": 83376, "start": 839.36, "end": 843.76, "text": " You can see that this the the notion of world models that are used", "tokens": [50644, 509, 393, 536, 300, 341, 264, 264, 10710, 295, 1002, 5245, 300, 366, 1143, 50864], "temperature": 0.0, "avg_logprob": -0.13665599293178982, "compression_ratio": 1.768421052631579, "no_speech_prob": 0.017432501539587975}, {"id": 183, "seek": 83376, "start": 844.16, "end": 850.4, "text": " Even not just in human adults, but in young children to coherently understand the world and deal with situations that are", "tokens": [50884, 2754, 406, 445, 294, 1952, 8865, 11, 457, 294, 2037, 2227, 281, 26528, 2276, 1223, 264, 1002, 293, 2028, 365, 6851, 300, 366, 51196], "temperature": 0.0, "avg_logprob": -0.13665599293178982, "compression_ratio": 1.768421052631579, "no_speech_prob": 0.017432501539587975}, {"id": 184, "seek": 83376, "start": 851.04, "end": 853.04, "text": " Importantly out of distribution", "tokens": [51228, 26391, 3627, 484, 295, 7316, 51328], "temperature": 0.0, "avg_logprob": -0.13665599293178982, "compression_ratio": 1.768421052631579, "no_speech_prob": 0.017432501539587975}, {"id": 185, "seek": 83376, "start": 853.04, "end": 855.04, "text": " Right and that's really key", "tokens": [51328, 1779, 293, 300, 311, 534, 2141, 51428], "temperature": 0.0, "avg_logprob": -0.13665599293178982, "compression_ratio": 1.768421052631579, "no_speech_prob": 0.017432501539587975}, {"id": 186, "seek": 83376, "start": 855.04, "end": 860.72, "text": " The ways in which we perceive and learn about the world are not nearly as time to the distributions of our experience as", "tokens": [51428, 440, 2098, 294, 597, 321, 20281, 293, 1466, 466, 264, 1002, 366, 406, 6217, 382, 565, 281, 264, 37870, 295, 527, 1752, 382, 51712], "temperature": 0.0, "avg_logprob": -0.13665599293178982, "compression_ratio": 1.768421052631579, "no_speech_prob": 0.017432501539587975}, {"id": 187, "seek": 83376, "start": 861.12, "end": 863.12, "text": " machine learning or function approximation", "tokens": [51732, 3479, 2539, 420, 2445, 28023, 51832], "temperature": 0.0, "avg_logprob": -0.13665599293178982, "compression_ratio": 1.768421052631579, "no_speech_prob": 0.017432501539587975}, {"id": 188, "seek": 86376, "start": 863.76, "end": 867.52, "text": " So here's some examples of different kind of self-driving system. These are", "tokens": [50364, 407, 510, 311, 512, 5110, 295, 819, 733, 295, 2698, 12, 47094, 1185, 13, 1981, 366, 50552], "temperature": 0.0, "avg_logprob": -0.12326025529341264, "compression_ratio": 1.7377049180327868, "no_speech_prob": 0.0010479505872353911}, {"id": 189, "seek": 86376, "start": 868.4, "end": 871.04, "text": " four-year-olds for the most part three four five-year-olds", "tokens": [50596, 1451, 12, 5294, 12, 31518, 337, 264, 881, 644, 1045, 1451, 1732, 12, 5294, 12, 31518, 50728], "temperature": 0.0, "avg_logprob": -0.12326025529341264, "compression_ratio": 1.7377049180327868, "no_speech_prob": 0.0010479505872353911}, {"id": 190, "seek": 86376, "start": 871.92, "end": 876.24, "text": " In a genre of youtube videos, you can see one of them actually the video on the left", "tokens": [50772, 682, 257, 11022, 295, 12487, 2145, 11, 291, 393, 536, 472, 295, 552, 767, 264, 960, 322, 264, 1411, 50988], "temperature": 0.0, "avg_logprob": -0.12326025529341264, "compression_ratio": 1.7377049180327868, "no_speech_prob": 0.0010479505872353911}, {"id": 191, "seek": 86376, "start": 877.2, "end": 880.8, "text": " It's it's sort of the the video genre on youtube is like, you know", "tokens": [51036, 467, 311, 309, 311, 1333, 295, 264, 264, 960, 11022, 322, 12487, 307, 411, 11, 291, 458, 51216], "temperature": 0.0, "avg_logprob": -0.12326025529341264, "compression_ratio": 1.7377049180327868, "no_speech_prob": 0.0010479505872353911}, {"id": 192, "seek": 86376, "start": 880.88, "end": 888.0, "text": " My four-year-old driving for the first time and parents have put their kids behind the wheels of golf carts or tractors or cars or trucks", "tokens": [51220, 1222, 1451, 12, 5294, 12, 2641, 4840, 337, 264, 700, 565, 293, 3152, 362, 829, 641, 2301, 2261, 264, 10046, 295, 12880, 48128, 420, 24207, 830, 420, 5163, 420, 16156, 51576], "temperature": 0.0, "avg_logprob": -0.12326025529341264, "compression_ratio": 1.7377049180327868, "no_speech_prob": 0.0010479505872353911}, {"id": 193, "seek": 88800, "start": 888.96, "end": 893.04, "text": " And just go at it and they video from the side or from next to them", "tokens": [50412, 400, 445, 352, 412, 309, 293, 436, 960, 490, 264, 1252, 420, 490, 958, 281, 552, 50616], "temperature": 0.0, "avg_logprob": -0.09493363003770844, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.024415619671344757}, {"id": 194, "seek": 88800, "start": 893.68, "end": 894.48, "text": " and", "tokens": [50648, 293, 50688], "temperature": 0.0, "avg_logprob": -0.09493363003770844, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.024415619671344757}, {"id": 195, "seek": 88800, "start": 894.48, "end": 900.16, "text": " You know, you could question the rationality of the adults who are putting their kids in the situation and then putting the videos online", "tokens": [50688, 509, 458, 11, 291, 727, 1168, 264, 15090, 507, 295, 264, 8865, 567, 366, 3372, 641, 2301, 294, 264, 2590, 293, 550, 3372, 264, 2145, 2950, 50972], "temperature": 0.0, "avg_logprob": -0.09493363003770844, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.024415619671344757}, {"id": 196, "seek": 88800, "start": 900.48, "end": 903.68, "text": " But there's a certain basic rationality that the four-year-old has", "tokens": [50988, 583, 456, 311, 257, 1629, 3875, 15090, 507, 300, 264, 1451, 12, 5294, 12, 2641, 575, 51148], "temperature": 0.0, "avg_logprob": -0.09493363003770844, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.024415619671344757}, {"id": 197, "seek": 88800, "start": 904.08, "end": 910.64, "text": " Even though they haven't been in the situation before the systems that their mind has built through evolution and the last couple of years", "tokens": [51168, 2754, 1673, 436, 2378, 380, 668, 294, 264, 2590, 949, 264, 3652, 300, 641, 1575, 575, 3094, 807, 9303, 293, 264, 1036, 1916, 295, 924, 51496], "temperature": 0.0, "avg_logprob": -0.09493363003770844, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.024415619671344757}, {"id": 198, "seek": 88800, "start": 910.64, "end": 912.64, "text": " The first few years of their experience", "tokens": [51496, 440, 700, 1326, 924, 295, 641, 1752, 51596], "temperature": 0.0, "avg_logprob": -0.09493363003770844, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.024415619671344757}, {"id": 199, "seek": 88800, "start": 912.88, "end": 915.76, "text": " Allow them to handle this totally new kind of perceptual information", "tokens": [51608, 32225, 552, 281, 4813, 341, 3879, 777, 733, 295, 43276, 901, 1589, 51752], "temperature": 0.0, "avg_logprob": -0.09493363003770844, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.024415619671344757}, {"id": 200, "seek": 91576, "start": 915.76, "end": 919.28, "text": " The world's going by them at a high speed and they're in control in some way", "tokens": [50364, 440, 1002, 311, 516, 538, 552, 412, 257, 1090, 3073, 293, 436, 434, 294, 1969, 294, 512, 636, 50540], "temperature": 0.0, "avg_logprob": -0.1090067165104423, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.000732004176825285}, {"id": 201, "seek": 91576, "start": 919.84, "end": 925.36, "text": " But they can generalize from their from from their experience because the the nature of their generalization", "tokens": [50568, 583, 436, 393, 2674, 1125, 490, 641, 490, 490, 641, 1752, 570, 264, 264, 3687, 295, 641, 2674, 2144, 50844], "temperature": 0.0, "avg_logprob": -0.1090067165104423, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.000732004176825285}, {"id": 202, "seek": 91576, "start": 925.6, "end": 929.52, "text": " Is their mind's models of the world and those transfer to these new situations?", "tokens": [50856, 1119, 641, 1575, 311, 5245, 295, 264, 1002, 293, 729, 5003, 281, 613, 777, 6851, 30, 51052], "temperature": 0.0, "avg_logprob": -0.1090067165104423, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.000732004176825285}, {"id": 203, "seek": 91576, "start": 929.52, "end": 932.48, "text": " Of course, their new things they have to learn about exactly how the steering wheel works", "tokens": [51052, 2720, 1164, 11, 641, 777, 721, 436, 362, 281, 1466, 466, 2293, 577, 264, 14823, 5589, 1985, 51200], "temperature": 0.0, "avg_logprob": -0.1090067165104423, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.000732004176825285}, {"id": 204, "seek": 91576, "start": 932.56, "end": 937.28, "text": " But they can learn those very quickly too because the learning is grounded in their world model", "tokens": [51204, 583, 436, 393, 1466, 729, 588, 2661, 886, 570, 264, 2539, 307, 23535, 294, 641, 1002, 2316, 51440], "temperature": 0.0, "avg_logprob": -0.1090067165104423, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.000732004176825285}, {"id": 205, "seek": 91576, "start": 938.24, "end": 942.56, "text": " So fundamentally, I think what we have is the is a contrast between the scaling thesis", "tokens": [51488, 407, 17879, 11, 286, 519, 437, 321, 362, 307, 264, 307, 257, 8712, 1296, 264, 21589, 22288, 51704], "temperature": 0.0, "avg_logprob": -0.1090067165104423, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.000732004176825285}, {"id": 206, "seek": 91576, "start": 943.04, "end": 945.04, "text": " of deep learning and today's ai", "tokens": [51728, 295, 2452, 2539, 293, 965, 311, 9783, 51828], "temperature": 0.0, "avg_logprob": -0.1090067165104423, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.000732004176825285}, {"id": 207, "seek": 94504, "start": 945.36, "end": 949.52, "text": " And what you could call the original scaling route that human intelligence follows what we might call", "tokens": [50380, 400, 437, 291, 727, 818, 264, 3380, 21589, 7955, 300, 1952, 7599, 10002, 437, 321, 1062, 818, 50588], "temperature": 0.0, "avg_logprob": -0.11535171226218895, "compression_ratio": 1.8408304498269896, "no_speech_prob": 0.0015479506691917777}, {"id": 208, "seek": 94504, "start": 949.92, "end": 953.04, "text": " Growing up as opposed to scaling up and I want to contrast", "tokens": [50608, 32569, 493, 382, 8851, 281, 21589, 493, 293, 286, 528, 281, 8712, 50764], "temperature": 0.0, "avg_logprob": -0.11535171226218895, "compression_ratio": 1.8408304498269896, "no_speech_prob": 0.0015479506691917777}, {"id": 209, "seek": 94504, "start": 953.52, "end": 956.8, "text": " Three points, which I think are really important for understanding", "tokens": [50788, 6244, 2793, 11, 597, 286, 519, 366, 534, 1021, 337, 3701, 50952], "temperature": 0.0, "avg_logprob": -0.11535171226218895, "compression_ratio": 1.8408304498269896, "no_speech_prob": 0.0015479506691917777}, {"id": 210, "seek": 94504, "start": 957.28, "end": 963.1999999999999, "text": " In general our intelligence and the contrast between ai and machine learning and especially the role that language and language models play in", "tokens": [50976, 682, 2674, 527, 7599, 293, 264, 8712, 1296, 9783, 293, 3479, 2539, 293, 2318, 264, 3090, 300, 2856, 293, 2856, 5245, 862, 294, 51272], "temperature": 0.0, "avg_logprob": -0.11535171226218895, "compression_ratio": 1.8408304498269896, "no_speech_prob": 0.0015479506691917777}, {"id": 211, "seek": 94504, "start": 963.8399999999999, "end": 969.1999999999999, "text": " So three points that are that are fundamental to the way today's ai is working based on deep learning one", "tokens": [51304, 407, 1045, 2793, 300, 366, 300, 366, 8088, 281, 264, 636, 965, 311, 9783, 307, 1364, 2361, 322, 2452, 2539, 472, 51572], "temperature": 0.0, "avg_logprob": -0.11535171226218895, "compression_ratio": 1.8408304498269896, "no_speech_prob": 0.0015479506691917777}, {"id": 212, "seek": 94504, "start": 969.8399999999999, "end": 972.9599999999999, "text": " That intelligence is seen as the end result of learning", "tokens": [51604, 663, 7599, 307, 1612, 382, 264, 917, 1874, 295, 2539, 51760], "temperature": 0.0, "avg_logprob": -0.11535171226218895, "compression_ratio": 1.8408304498269896, "no_speech_prob": 0.0015479506691917777}, {"id": 213, "seek": 97296, "start": 973.44, "end": 977.76, "text": " We're learning starts with some simple very general and in some ways dumb mechanisms", "tokens": [50388, 492, 434, 2539, 3719, 365, 512, 2199, 588, 2674, 293, 294, 512, 2098, 10316, 15902, 50604], "temperature": 0.0, "avg_logprob": -0.1286930565361504, "compression_ratio": 1.778181818181818, "no_speech_prob": 0.001000166404992342}, {"id": 214, "seek": 97296, "start": 978.24, "end": 980.72, "text": " associative learning prediction, you know in some way", "tokens": [50628, 4180, 1166, 2539, 17630, 11, 291, 458, 294, 512, 636, 50752], "temperature": 0.0, "avg_logprob": -0.1286930565361504, "compression_ratio": 1.778181818181818, "no_speech_prob": 0.001000166404992342}, {"id": 215, "seek": 97296, "start": 981.52, "end": 987.6, "text": " But the idea is if you scale that up enough you have the surprising emergent phenomenon that now we start to call intelligence", "tokens": [50792, 583, 264, 1558, 307, 498, 291, 4373, 300, 493, 1547, 291, 362, 264, 8830, 4345, 6930, 14029, 300, 586, 321, 722, 281, 818, 7599, 51096], "temperature": 0.0, "avg_logprob": -0.1286930565361504, "compression_ratio": 1.778181818181818, "no_speech_prob": 0.001000166404992342}, {"id": 216, "seek": 97296, "start": 988.1600000000001, "end": 989.36, "text": " um", "tokens": [51124, 1105, 51184], "temperature": 0.0, "avg_logprob": -0.1286930565361504, "compression_ratio": 1.778181818181818, "no_speech_prob": 0.001000166404992342}, {"id": 217, "seek": 97296, "start": 989.36, "end": 994.08, "text": " The the problem is though, I mean either you call it a problem or you can call it just the way things work", "tokens": [51184, 440, 264, 1154, 307, 1673, 11, 286, 914, 2139, 291, 818, 309, 257, 1154, 420, 291, 393, 818, 309, 445, 264, 636, 721, 589, 51420], "temperature": 0.0, "avg_logprob": -0.1286930565361504, "compression_ratio": 1.778181818181818, "no_speech_prob": 0.001000166404992342}, {"id": 218, "seek": 97296, "start": 994.1600000000001, "end": 996.48, "text": " But there's some remarkable kind of generalization", "tokens": [51424, 583, 456, 311, 512, 12802, 733, 295, 2674, 2144, 51540], "temperature": 0.0, "avg_logprob": -0.1286930565361504, "compression_ratio": 1.778181818181818, "no_speech_prob": 0.001000166404992342}, {"id": 219, "seek": 97296, "start": 996.48, "end": 999.6800000000001, "text": " But it still is going to depend on similarity to training data", "tokens": [51540, 583, 309, 920, 307, 516, 281, 5672, 322, 32194, 281, 3097, 1412, 51700], "temperature": 0.0, "avg_logprob": -0.1286930565361504, "compression_ratio": 1.778181818181818, "no_speech_prob": 0.001000166404992342}, {"id": 220, "seek": 99968, "start": 1000.16, "end": 1006.64, "text": " And its ability to go beyond the training distribution is is weak and unpredictable compared to when you're in distribution", "tokens": [50388, 400, 1080, 3485, 281, 352, 4399, 264, 3097, 7316, 307, 307, 5336, 293, 31160, 5347, 281, 562, 291, 434, 294, 7316, 50712], "temperature": 0.0, "avg_logprob": -0.11482762317268216, "compression_ratio": 1.7553956834532374, "no_speech_prob": 0.00232220278121531}, {"id": 221, "seek": 99968, "start": 1007.5999999999999, "end": 1011.92, "text": " Um, this is true whether you're talking about perception or other forms of higher level cognition", "tokens": [50760, 3301, 11, 341, 307, 2074, 1968, 291, 434, 1417, 466, 12860, 420, 661, 6422, 295, 2946, 1496, 46905, 50976], "temperature": 0.0, "avg_logprob": -0.11482762317268216, "compression_ratio": 1.7553956834532374, "no_speech_prob": 0.00232220278121531}, {"id": 222, "seek": 99968, "start": 1011.92, "end": 1018.0799999999999, "text": " But especially thinking things that we recognize as forms of reasoning planning problem solving and so on", "tokens": [50976, 583, 2318, 1953, 721, 300, 321, 5521, 382, 6422, 295, 21577, 5038, 1154, 12606, 293, 370, 322, 51284], "temperature": 0.0, "avg_logprob": -0.11482762317268216, "compression_ratio": 1.7553956834532374, "no_speech_prob": 0.00232220278121531}, {"id": 223, "seek": 99968, "start": 1018.7199999999999, "end": 1021.68, "text": " The the sort of thing that we've never seen in any computer vision system", "tokens": [51316, 440, 264, 1333, 295, 551, 300, 321, 600, 1128, 1612, 294, 604, 3820, 5201, 1185, 51464], "temperature": 0.0, "avg_logprob": -0.11482762317268216, "compression_ratio": 1.7553956834532374, "no_speech_prob": 0.00232220278121531}, {"id": 224, "seek": 99968, "start": 1021.68, "end": 1025.84, "text": " But we're starting to see some interesting kind of approximation to in language models", "tokens": [51464, 583, 321, 434, 2891, 281, 536, 512, 1880, 733, 295, 28023, 281, 294, 2856, 5245, 51672], "temperature": 0.0, "avg_logprob": -0.11482762317268216, "compression_ratio": 1.7553956834532374, "no_speech_prob": 0.00232220278121531}, {"id": 225, "seek": 102584, "start": 1026.48, "end": 1030.32, "text": " Crucially that in machine learning thinking derives from language", "tokens": [50396, 13586, 1909, 300, 294, 3479, 2539, 1953, 1163, 1539, 490, 2856, 50588], "temperature": 0.0, "avg_logprob": -0.0986257245985128, "compression_ratio": 1.7756410256410255, "no_speech_prob": 0.007575447205454111}, {"id": 226, "seek": 102584, "start": 1030.6399999999999, "end": 1036.6399999999999, "text": " Because it's only the data of language not the data of pixels that conveys information really about human thoughts", "tokens": [50604, 1436, 309, 311, 787, 264, 1412, 295, 2856, 406, 264, 1412, 295, 18668, 300, 18053, 749, 1589, 534, 466, 1952, 4598, 50904], "temperature": 0.0, "avg_logprob": -0.0986257245985128, "compression_ratio": 1.7756410256410255, "no_speech_prob": 0.007575447205454111}, {"id": 227, "seek": 102584, "start": 1036.72, "end": 1041.36, "text": " Which makes sense again humans have always used language as our main medium to express our thoughts to others", "tokens": [50908, 3013, 1669, 2020, 797, 6255, 362, 1009, 1143, 2856, 382, 527, 2135, 6399, 281, 5109, 527, 4598, 281, 2357, 51140], "temperature": 0.0, "avg_logprob": -0.0986257245985128, "compression_ratio": 1.7756410256410255, "no_speech_prob": 0.007575447205454111}, {"id": 228, "seek": 102584, "start": 1041.4399999999998, "end": 1044.6399999999999, "text": " Just like what we're doing right now in giving talks like this, right?", "tokens": [51144, 1449, 411, 437, 321, 434, 884, 558, 586, 294, 2902, 6686, 411, 341, 11, 558, 30, 51304], "temperature": 0.0, "avg_logprob": -0.0986257245985128, "compression_ratio": 1.7756410256410255, "no_speech_prob": 0.007575447205454111}, {"id": 229, "seek": 102584, "start": 1045.12, "end": 1045.6799999999998, "text": " um", "tokens": [51328, 1105, 51356], "temperature": 0.0, "avg_logprob": -0.0986257245985128, "compression_ratio": 1.7756410256410255, "no_speech_prob": 0.007575447205454111}, {"id": 230, "seek": 102584, "start": 1045.6799999999998, "end": 1050.8799999999999, "text": " But crucially thinking requires language data and even carefully cleaned and curated language data", "tokens": [51356, 583, 5140, 1909, 1953, 7029, 2856, 1412, 293, 754, 7500, 16146, 293, 47851, 2856, 1412, 51616], "temperature": 0.0, "avg_logprob": -0.0986257245985128, "compression_ratio": 1.7756410256410255, "no_speech_prob": 0.007575447205454111}, {"id": 231, "seek": 102584, "start": 1050.8799999999999, "end": 1054.3999999999999, "text": " Our colleagues who are at the big ai companies can tell us a lot more about this or rather", "tokens": [51616, 2621, 7734, 567, 366, 412, 264, 955, 9783, 3431, 393, 980, 505, 257, 688, 544, 466, 341, 420, 2831, 51792], "temperature": 0.0, "avg_logprob": -0.0986257245985128, "compression_ratio": 1.7756410256410255, "no_speech_prob": 0.007575447205454111}, {"id": 232, "seek": 105440, "start": 1054.48, "end": 1058.0, "text": " Maybe they can't tell us about this but they can tell us that they can't tell us about", "tokens": [50368, 2704, 436, 393, 380, 980, 505, 466, 341, 457, 436, 393, 980, 505, 300, 436, 393, 380, 980, 505, 466, 50544], "temperature": 0.0, "avg_logprob": -0.09802599514231962, "compression_ratio": 1.8596491228070176, "no_speech_prob": 0.0008556176326237619}, {"id": 233, "seek": 105440, "start": 1058.5600000000002, "end": 1064.3200000000002, "text": " And those of us who've worked with this know the importance of having the right kinds of language data as well as the right reinforcement data", "tokens": [50572, 400, 729, 295, 505, 567, 600, 2732, 365, 341, 458, 264, 7379, 295, 1419, 264, 558, 3685, 295, 2856, 1412, 382, 731, 382, 264, 558, 29280, 1412, 50860], "temperature": 0.0, "avg_logprob": -0.09802599514231962, "compression_ratio": 1.8596491228070176, "no_speech_prob": 0.0008556176326237619}, {"id": 234, "seek": 105440, "start": 1065.1200000000001, "end": 1068.64, "text": " Okay, so contrast this with human minds and especially human children", "tokens": [50900, 1033, 11, 370, 8712, 341, 365, 1952, 9634, 293, 2318, 1952, 2227, 51076], "temperature": 0.0, "avg_logprob": -0.09802599514231962, "compression_ratio": 1.8596491228070176, "no_speech_prob": 0.0008556176326237619}, {"id": 235, "seek": 105440, "start": 1069.68, "end": 1071.76, "text": " Our intelligence if for our intelligence", "tokens": [51128, 2621, 7599, 498, 337, 527, 7599, 51232], "temperature": 0.0, "avg_logprob": -0.09802599514231962, "compression_ratio": 1.8596491228070176, "no_speech_prob": 0.0008556176326237619}, {"id": 236, "seek": 105440, "start": 1072.0, "end": 1075.92, "text": " It's not just the thing that is at the end result of all of our learning although we do", "tokens": [51244, 467, 311, 406, 445, 264, 551, 300, 307, 412, 264, 917, 1874, 295, 439, 295, 527, 2539, 4878, 321, 360, 51440], "temperature": 0.0, "avg_logprob": -0.09802599514231962, "compression_ratio": 1.8596491228070176, "no_speech_prob": 0.0008556176326237619}, {"id": 237, "seek": 105440, "start": 1076.24, "end": 1080.5600000000002, "text": " You know babies are not in some important way not as intelligent as human adults we would like to say", "tokens": [51456, 509, 458, 10917, 366, 406, 294, 512, 1021, 636, 406, 382, 13232, 382, 1952, 8865, 321, 576, 411, 281, 584, 51672], "temperature": 0.0, "avg_logprob": -0.09802599514231962, "compression_ratio": 1.8596491228070176, "no_speech_prob": 0.0008556176326237619}, {"id": 238, "seek": 108056, "start": 1081.12, "end": 1087.2, "text": " But human intelligence is is built in in some form from the start. It's the foundation of learning. It's not just the end state", "tokens": [50392, 583, 1952, 7599, 307, 307, 3094, 294, 294, 512, 1254, 490, 264, 722, 13, 467, 311, 264, 7030, 295, 2539, 13, 467, 311, 406, 445, 264, 917, 1785, 50696], "temperature": 0.0, "avg_logprob": -0.07506073304336436, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.002630532020702958}, {"id": 239, "seek": 108056, "start": 1087.6, "end": 1092.8, "text": " It's what's there and the the learning mechanisms that let you learn so much from so little", "tokens": [50716, 467, 311, 437, 311, 456, 293, 264, 264, 2539, 15902, 300, 718, 291, 1466, 370, 709, 490, 370, 707, 50976], "temperature": 0.0, "avg_logprob": -0.07506073304336436, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.002630532020702958}, {"id": 240, "seek": 108056, "start": 1093.52, "end": 1097.04, "text": " Okay, and we saw some of that in virginia's talk and you know", "tokens": [51012, 1033, 11, 293, 321, 1866, 512, 295, 300, 294, 26404, 654, 311, 751, 293, 291, 458, 51188], "temperature": 0.0, "avg_logprob": -0.07506073304336436, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.002630532020702958}, {"id": 241, "seek": 108056, "start": 1097.04, "end": 1101.6, "text": " I think the the field of human cognitive development is a testament to this or what I was just showing from the four-year-olds", "tokens": [51188, 286, 519, 264, 264, 2519, 295, 1952, 15605, 3250, 307, 257, 35499, 281, 341, 420, 437, 286, 390, 445, 4099, 490, 264, 1451, 12, 5294, 12, 31518, 51416], "temperature": 0.0, "avg_logprob": -0.07506073304336436, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.002630532020702958}, {"id": 242, "seek": 108056, "start": 1101.6, "end": 1104.3999999999999, "text": " Is another example but in other work that i'm not going to talk about here", "tokens": [51416, 1119, 1071, 1365, 457, 294, 661, 589, 300, 741, 478, 406, 516, 281, 751, 466, 510, 51556], "temperature": 0.0, "avg_logprob": -0.07506073304336436, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.002630532020702958}, {"id": 243, "seek": 108056, "start": 1104.3999999999999, "end": 1108.08, "text": " We've done a lot of work trying to model the core knowledge of even you know", "tokens": [51556, 492, 600, 1096, 257, 688, 295, 589, 1382, 281, 2316, 264, 4965, 3601, 295, 754, 291, 458, 51740], "temperature": 0.0, "avg_logprob": -0.07506073304336436, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.002630532020702958}, {"id": 244, "seek": 110808, "start": 1108.08, "end": 1113.52, "text": " 12-month-old or 10-month-old babies and there's important ways in which they have a common sense understanding of the world", "tokens": [50364, 2272, 12, 23534, 12, 2641, 420, 1266, 12, 23534, 12, 2641, 10917, 293, 456, 311, 1021, 2098, 294, 597, 436, 362, 257, 2689, 2020, 3701, 295, 264, 1002, 50636], "temperature": 0.0, "avg_logprob": -0.10383273734421027, "compression_ratio": 1.7689530685920578, "no_speech_prob": 0.0011332943104207516}, {"id": 245, "seek": 110808, "start": 1114.0, "end": 1118.72, "text": " That is in in in significant ways. It's not all built in but in significant ways", "tokens": [50660, 663, 307, 294, 294, 294, 4776, 2098, 13, 467, 311, 406, 439, 3094, 294, 457, 294, 4776, 2098, 50896], "temperature": 0.0, "avg_logprob": -0.10383273734421027, "compression_ratio": 1.7689530685920578, "no_speech_prob": 0.0011332943104207516}, {"id": 246, "seek": 110808, "start": 1118.72, "end": 1122.08, "text": " It seems to be present even even in two and three month olds. Okay", "tokens": [50896, 467, 2544, 281, 312, 1974, 754, 754, 294, 732, 293, 1045, 1618, 41972, 13, 1033, 51064], "temperature": 0.0, "avg_logprob": -0.10383273734421027, "compression_ratio": 1.7689530685920578, "no_speech_prob": 0.0011332943104207516}, {"id": 247, "seek": 110808, "start": 1122.8799999999999, "end": 1123.6, "text": " um", "tokens": [51104, 1105, 51140], "temperature": 0.0, "avg_logprob": -0.10383273734421027, "compression_ratio": 1.7689530685920578, "no_speech_prob": 0.0011332943104207516}, {"id": 248, "seek": 110808, "start": 1123.6, "end": 1130.8, "text": " As a result of our built-in capacity for modeling the world our generalization is based on the fit of our mental models", "tokens": [51140, 1018, 257, 1874, 295, 527, 3094, 12, 259, 6042, 337, 15983, 264, 1002, 527, 2674, 2144, 307, 2361, 322, 264, 3318, 295, 527, 4973, 5245, 51500], "temperature": 0.0, "avg_logprob": -0.10383273734421027, "compression_ratio": 1.7689530685920578, "no_speech_prob": 0.0011332943104207516}, {"id": 249, "seek": 110808, "start": 1130.8, "end": 1135.76, "text": " It's not about the training data. Okay data is important. We improve our models. We grow models", "tokens": [51500, 467, 311, 406, 466, 264, 3097, 1412, 13, 1033, 1412, 307, 1021, 13, 492, 3470, 527, 5245, 13, 492, 1852, 5245, 51748], "temperature": 0.0, "avg_logprob": -0.10383273734421027, "compression_ratio": 1.7689530685920578, "no_speech_prob": 0.0011332943104207516}, {"id": 250, "seek": 113576, "start": 1135.76, "end": 1142.08, "text": " We can make new models from data, but what accounts for generalization and what drives generalization is not similarities to the data", "tokens": [50364, 492, 393, 652, 777, 5245, 490, 1412, 11, 457, 437, 9402, 337, 2674, 2144, 293, 437, 11754, 2674, 2144, 307, 406, 24197, 281, 264, 1412, 50680], "temperature": 0.0, "avg_logprob": -0.08067378482303104, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.003592466004192829}, {"id": 251, "seek": 113576, "start": 1142.24, "end": 1148.24, "text": " But the fit of our world models and their ability to be flexible and themselves to be generalized", "tokens": [50688, 583, 264, 3318, 295, 527, 1002, 5245, 293, 641, 3485, 281, 312, 11358, 293, 2969, 281, 312, 44498, 50988], "temperature": 0.0, "avg_logprob": -0.08067378482303104, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.003592466004192829}, {"id": 252, "seek": 113576, "start": 1149.36, "end": 1155.36, "text": " And crucially thinking isn't the thing that comes at the end state as an emergent property of modeling language", "tokens": [51044, 400, 5140, 1909, 1953, 1943, 380, 264, 551, 300, 1487, 412, 264, 917, 1785, 382, 364, 4345, 6930, 4707, 295, 15983, 2856, 51344], "temperature": 0.0, "avg_logprob": -0.08067378482303104, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.003592466004192829}, {"id": 253, "seek": 113576, "start": 1155.36, "end": 1159.36, "text": " But rather thinking is there from the start and it's the basis for language", "tokens": [51344, 583, 2831, 1953, 307, 456, 490, 264, 722, 293, 309, 311, 264, 5143, 337, 2856, 51544], "temperature": 0.0, "avg_logprob": -0.08067378482303104, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.003592466004192829}, {"id": 254, "seek": 113576, "start": 1159.44, "end": 1165.44, "text": " It's the basis for why human children construct language so robustly and resiliently and again there", "tokens": [51548, 467, 311, 264, 5143, 337, 983, 1952, 2227, 7690, 2856, 370, 13956, 356, 293, 23699, 356, 293, 797, 456, 51848], "temperature": 0.0, "avg_logprob": -0.08067378482303104, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.003592466004192829}, {"id": 255, "seek": 116544, "start": 1165.52, "end": 1167.2, "text": " I'm referring to", "tokens": [50368, 286, 478, 13761, 281, 50452], "temperature": 0.0, "avg_logprob": -0.17283358573913574, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.0039427014999091625}, {"id": 256, "seek": 116544, "start": 1167.2, "end": 1173.3600000000001, "text": " By using that word resilience to some of the ideas that virginia talked about in her talk and susan golden meadow has talked about", "tokens": [50452, 3146, 1228, 300, 1349, 19980, 281, 512, 295, 264, 3487, 300, 26404, 654, 2825, 466, 294, 720, 751, 293, 3291, 282, 9729, 385, 11528, 575, 2825, 466, 50760], "temperature": 0.0, "avg_logprob": -0.17283358573913574, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.0039427014999091625}, {"id": 257, "seek": 116544, "start": 1173.68, "end": 1177.3600000000001, "text": " I urge everyone to watch her rumelhardt prize talk where she talks about this as well", "tokens": [50776, 286, 19029, 1518, 281, 1159, 720, 8347, 338, 21491, 83, 12818, 751, 689, 750, 6686, 466, 341, 382, 731, 50960], "temperature": 0.0, "avg_logprob": -0.17283358573913574, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.0039427014999091625}, {"id": 258, "seek": 116544, "start": 1177.92, "end": 1183.28, "text": " This idea that as we saw in susan's work and annie sangus's work with nicaraguan sign language", "tokens": [50988, 639, 1558, 300, 382, 321, 1866, 294, 3291, 282, 311, 589, 293, 2324, 414, 9980, 301, 311, 589, 365, 297, 7953, 559, 6139, 1465, 2856, 51256], "temperature": 0.0, "avg_logprob": -0.17283358573913574, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.0039427014999091625}, {"id": 259, "seek": 116544, "start": 1183.68, "end": 1186.24, "text": " That you know children who grow up who grow up without", "tokens": [51276, 663, 291, 458, 2227, 567, 1852, 493, 567, 1852, 493, 1553, 51404], "temperature": 0.0, "avg_logprob": -0.17283358573913574, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.0039427014999091625}, {"id": 260, "seek": 116544, "start": 1187.1200000000001, "end": 1188.48, "text": " Any language input?", "tokens": [51448, 2639, 2856, 4846, 30, 51516], "temperature": 0.0, "avg_logprob": -0.17283358573913574, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.0039427014999091625}, {"id": 261, "seek": 116544, "start": 1188.48, "end": 1191.68, "text": " It's not just about the poverty the stimulus in the traditional linguistic sense", "tokens": [51516, 467, 311, 406, 445, 466, 264, 10958, 264, 21366, 294, 264, 5164, 43002, 2020, 51676], "temperature": 0.0, "avg_logprob": -0.17283358573913574, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.0039427014999091625}, {"id": 262, "seek": 119168, "start": 1191.68, "end": 1197.8400000000001, "text": " But you grow up with deaf without sign language input and you create in some form your own personal proto language", "tokens": [50364, 583, 291, 1852, 493, 365, 15559, 1553, 1465, 2856, 4846, 293, 291, 1884, 294, 512, 1254, 428, 1065, 2973, 47896, 2856, 50672], "temperature": 0.0, "avg_logprob": -0.0999615255710298, "compression_ratio": 1.7043189368770764, "no_speech_prob": 0.00460647651925683}, {"id": 263, "seek": 119168, "start": 1197.8400000000001, "end": 1201.04, "text": " Or at least a way of communicating that has hierarchical symbolic structure", "tokens": [50672, 1610, 412, 1935, 257, 636, 295, 17559, 300, 575, 35250, 804, 25755, 3877, 50832], "temperature": 0.0, "avg_logprob": -0.0999615255710298, "compression_ratio": 1.7043189368770764, "no_speech_prob": 0.00460647651925683}, {"id": 264, "seek": 119168, "start": 1201.28, "end": 1204.8, "text": " And then you bring a few such children together and within the span of a couple of generations", "tokens": [50844, 400, 550, 291, 1565, 257, 1326, 1270, 2227, 1214, 293, 1951, 264, 16174, 295, 257, 1916, 295, 10593, 51020], "temperature": 0.0, "avg_logprob": -0.0999615255710298, "compression_ratio": 1.7043189368770764, "no_speech_prob": 0.00460647651925683}, {"id": 265, "seek": 119168, "start": 1204.8, "end": 1210.0, "text": " They've created a whole new language from scratch. Okay, so it's very clear from data like that", "tokens": [51020, 814, 600, 2942, 257, 1379, 777, 2856, 490, 8459, 13, 1033, 11, 370, 309, 311, 588, 1850, 490, 1412, 411, 300, 51280], "temperature": 0.0, "avg_logprob": -0.0999615255710298, "compression_ratio": 1.7043189368770764, "no_speech_prob": 0.00460647651925683}, {"id": 266, "seek": 119168, "start": 1210.8, "end": 1213.8400000000001, "text": " That humans are built to think and we're built with an", "tokens": [51320, 663, 6255, 366, 3094, 281, 519, 293, 321, 434, 3094, 365, 364, 51472], "temperature": 0.0, "avg_logprob": -0.0999615255710298, "compression_ratio": 1.7043189368770764, "no_speech_prob": 0.00460647651925683}, {"id": 267, "seek": 119168, "start": 1214.4, "end": 1218.3200000000002, "text": " A desire to understand and to under to be understood to express our thoughts", "tokens": [51500, 316, 7516, 281, 1223, 293, 281, 833, 281, 312, 7320, 281, 5109, 527, 4598, 51696], "temperature": 0.0, "avg_logprob": -0.0999615255710298, "compression_ratio": 1.7043189368770764, "no_speech_prob": 0.00460647651925683}, {"id": 268, "seek": 121832, "start": 1218.6399999999999, "end": 1222.32, "text": " In some form and share them with others in our as our social partners", "tokens": [50380, 682, 512, 1254, 293, 2073, 552, 365, 2357, 294, 527, 382, 527, 2093, 4462, 50564], "temperature": 0.0, "avg_logprob": -0.11773663646769973, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.0038208041805773973}, {"id": 269, "seek": 121832, "start": 1222.96, "end": 1226.08, "text": " And that's you know, there's nothing I think more fundamental to", "tokens": [50596, 400, 300, 311, 291, 458, 11, 456, 311, 1825, 286, 519, 544, 8088, 281, 50752], "temperature": 0.0, "avg_logprob": -0.11773663646769973, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.0038208041805773973}, {"id": 270, "seek": 121832, "start": 1226.72, "end": 1229.6, "text": " To understand if you want to understand where our intelligence comes from", "tokens": [50784, 1407, 1223, 498, 291, 528, 281, 1223, 689, 527, 7599, 1487, 490, 50928], "temperature": 0.0, "avg_logprob": -0.11773663646769973, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.0038208041805773973}, {"id": 271, "seek": 121832, "start": 1230.08, "end": 1236.96, "text": " So what we've been trying to do in our work is to try to capture the human growing up scaling root in computational terms", "tokens": [50952, 407, 437, 321, 600, 668, 1382, 281, 360, 294, 527, 589, 307, 281, 853, 281, 7983, 264, 1952, 4194, 493, 21589, 5593, 294, 28270, 2115, 51296], "temperature": 0.0, "avg_logprob": -0.11773663646769973, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.0038208041805773973}, {"id": 272, "seek": 121832, "start": 1237.52, "end": 1240.48, "text": " And it starts with this idea of probabilistic inference and", "tokens": [51324, 400, 309, 3719, 365, 341, 1558, 295, 31959, 3142, 38253, 293, 51472], "temperature": 0.0, "avg_logprob": -0.11773663646769973, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.0038208041805773973}, {"id": 273, "seek": 121832, "start": 1241.04, "end": 1244.8, "text": " Expected value decision-making on top of world models, you know informally", "tokens": [51500, 2111, 10729, 2158, 3537, 12, 12402, 322, 1192, 295, 1002, 5245, 11, 291, 458, 1356, 379, 51688], "temperature": 0.0, "avg_logprob": -0.11773663646769973, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.0038208041805773973}, {"id": 274, "seek": 124480, "start": 1244.96, "end": 1249.2, "text": " I would say and like many others that thinking not just in human brains", "tokens": [50372, 286, 576, 584, 293, 411, 867, 2357, 300, 1953, 406, 445, 294, 1952, 15442, 50584], "temperature": 0.0, "avg_logprob": -0.12908018203008742, "compression_ratio": 1.9786476868327403, "no_speech_prob": 0.01797568053007126}, {"id": 275, "seek": 124480, "start": 1249.28, "end": 1254.56, "text": " But in brains and in many other animals is about making good guesses and bets not about the next", "tokens": [50588, 583, 294, 15442, 293, 294, 867, 661, 4882, 307, 466, 1455, 665, 42703, 293, 39922, 406, 466, 264, 958, 50852], "temperature": 0.0, "avg_logprob": -0.12908018203008742, "compression_ratio": 1.9786476868327403, "no_speech_prob": 0.01797568053007126}, {"id": 276, "seek": 124480, "start": 1255.04, "end": 1259.76, "text": " Data thing you're going to see not about the next token or the next pixel or set of pixel values", "tokens": [50876, 11888, 551, 291, 434, 516, 281, 536, 406, 466, 264, 958, 14862, 420, 264, 958, 19261, 420, 992, 295, 19261, 4190, 51112], "temperature": 0.0, "avg_logprob": -0.12908018203008742, "compression_ratio": 1.9786476868327403, "no_speech_prob": 0.01797568053007126}, {"id": 277, "seek": 124480, "start": 1259.9199999999998, "end": 1262.72, "text": " But about the world what's going to happen in the world and how", "tokens": [51120, 583, 466, 264, 1002, 437, 311, 516, 281, 1051, 294, 264, 1002, 293, 577, 51260], "temperature": 0.0, "avg_logprob": -0.12908018203008742, "compression_ratio": 1.9786476868327403, "no_speech_prob": 0.01797568053007126}, {"id": 278, "seek": 124480, "start": 1263.28, "end": 1268.3999999999999, "text": " It might depend on your actions or the actions of others or how you might be able to change the world to change those", "tokens": [51288, 467, 1062, 5672, 322, 428, 5909, 420, 264, 5909, 295, 2357, 420, 577, 291, 1062, 312, 1075, 281, 1319, 264, 1002, 281, 1319, 729, 51544], "temperature": 0.0, "avg_logprob": -0.12908018203008742, "compression_ratio": 1.9786476868327403, "no_speech_prob": 0.01797568053007126}, {"id": 279, "seek": 124480, "start": 1268.96, "end": 1274.1599999999999, "text": " dependencies causal and counterfactual reasoning and then having some sense of what you'd like to see happen", "tokens": [51572, 36606, 38755, 293, 5682, 44919, 901, 21577, 293, 550, 1419, 512, 2020, 295, 437, 291, 1116, 411, 281, 536, 1051, 51832], "temperature": 0.0, "avg_logprob": -0.12908018203008742, "compression_ratio": 1.9786476868327403, "no_speech_prob": 0.01797568053007126}, {"id": 280, "seek": 127416, "start": 1274.5600000000002, "end": 1278.16, "text": " Or and what you'd not like to see happen or what could really cost you your life or", "tokens": [50384, 1610, 293, 437, 291, 1116, 406, 411, 281, 536, 1051, 420, 437, 727, 534, 2063, 291, 428, 993, 420, 50564], "temperature": 0.0, "avg_logprob": -0.1389754223373701, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0007913426961749792}, {"id": 281, "seek": 127416, "start": 1278.8000000000002, "end": 1284.0800000000002, "text": " Be hugely valuable and making good bets about how to act and what to think about next", "tokens": [50596, 879, 27417, 8263, 293, 1455, 665, 39922, 466, 577, 281, 605, 293, 437, 281, 519, 466, 958, 50860], "temperature": 0.0, "avg_logprob": -0.1389754223373701, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0007913426961749792}, {"id": 282, "seek": 127416, "start": 1284.96, "end": 1286.96, "text": " fundamentally this classic idea of", "tokens": [50904, 17879, 341, 7230, 1558, 295, 51004], "temperature": 0.0, "avg_logprob": -0.1389754223373701, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0007913426961749792}, {"id": 283, "seek": 127416, "start": 1287.3600000000001, "end": 1289.3600000000001, "text": " effectively rationality that", "tokens": [51024, 8659, 15090, 507, 300, 51124], "temperature": 0.0, "avg_logprob": -0.1389754223373701, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0007913426961749792}, {"id": 284, "seek": 127416, "start": 1290.0, "end": 1294.64, "text": " You know, you could say our minds and brains are the original sources of this idea through evolution", "tokens": [51156, 509, 458, 11, 291, 727, 584, 527, 9634, 293, 15442, 366, 264, 3380, 7139, 295, 341, 1558, 807, 9303, 51388], "temperature": 0.0, "avg_logprob": -0.1389754223373701, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0007913426961749792}, {"id": 285, "seek": 127416, "start": 1295.52, "end": 1298.64, "text": " Being constructed to do this kind of computation", "tokens": [51432, 8891, 17083, 281, 360, 341, 733, 295, 24903, 51588], "temperature": 0.0, "avg_logprob": -0.1389754223373701, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0007913426961749792}, {"id": 286, "seek": 127416, "start": 1299.28, "end": 1303.52, "text": " In our work and this is the stuff we've been doing with language models builds on this", "tokens": [51620, 682, 527, 589, 293, 341, 307, 264, 1507, 321, 600, 668, 884, 365, 2856, 5245, 15182, 322, 341, 51832], "temperature": 0.0, "avg_logprob": -0.1389754223373701, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0007913426961749792}, {"id": 287, "seek": 130352, "start": 1303.84, "end": 1306.96, "text": " We've been using the idea of probabilistic programs, which is a", "tokens": [50380, 492, 600, 668, 1228, 264, 1558, 295, 31959, 3142, 4268, 11, 597, 307, 257, 50536], "temperature": 0.0, "avg_logprob": -0.1381824775175615, "compression_ratio": 1.715415019762846, "no_speech_prob": 0.0011325916275382042}, {"id": 288, "seek": 130352, "start": 1307.84, "end": 1312.08, "text": " family of mathematical languages and actual programming languages and platforms", "tokens": [50580, 1605, 295, 18894, 8650, 293, 3539, 9410, 8650, 293, 9473, 50792], "temperature": 0.0, "avg_logprob": -0.1381824775175615, "compression_ratio": 1.715415019762846, "no_speech_prob": 0.0011325916275382042}, {"id": 289, "seek": 130352, "start": 1312.56, "end": 1318.72, "text": " To embody this idea basically take this conceptual idea of rational world modeling inference of decision", "tokens": [50816, 1407, 42575, 341, 1558, 1936, 747, 341, 24106, 1558, 295, 15090, 1002, 15983, 38253, 295, 3537, 51124], "temperature": 0.0, "avg_logprob": -0.1381824775175615, "compression_ratio": 1.715415019762846, "no_speech_prob": 0.0011325916275382042}, {"id": 290, "seek": 130352, "start": 1319.12, "end": 1324.96, "text": " And turn it into practical engineering terms that can be models of human minds as well as more human like ai", "tokens": [51144, 400, 1261, 309, 666, 8496, 7043, 2115, 300, 393, 312, 5245, 295, 1952, 9634, 382, 731, 382, 544, 1952, 411, 9783, 51436], "temperature": 0.0, "avg_logprob": -0.1381824775175615, "compression_ratio": 1.715415019762846, "no_speech_prob": 0.0011325916275382042}, {"id": 291, "seek": 130352, "start": 1325.52, "end": 1328.24, "text": " And I don't have time to give a whole introduction to probabilistic programs", "tokens": [51464, 400, 286, 500, 380, 362, 565, 281, 976, 257, 1379, 9339, 281, 31959, 3142, 4268, 51600], "temperature": 0.0, "avg_logprob": -0.1381824775175615, "compression_ratio": 1.715415019762846, "no_speech_prob": 0.0011325916275382042}, {"id": 292, "seek": 132824, "start": 1328.8, "end": 1335.04, "text": " But you know, you can think of it as a kind of catch-all phrase or a complicated suitcase phrase just like neural network", "tokens": [50392, 583, 291, 458, 11, 291, 393, 519, 295, 309, 382, 257, 733, 295, 3745, 12, 336, 9535, 420, 257, 6179, 34545, 9535, 445, 411, 18161, 3209, 50704], "temperature": 0.0, "avg_logprob": -0.11892305867055829, "compression_ratio": 1.7233201581027668, "no_speech_prob": 0.027157416567206383}, {"id": 293, "seek": 132824, "start": 1336.08, "end": 1340.4, "text": " Packages a number of different things together, but probabilistic programs are formalisms", "tokens": [50756, 18466, 1660, 257, 1230, 295, 819, 721, 1214, 11, 457, 31959, 3142, 4268, 366, 9860, 13539, 50972], "temperature": 0.0, "avg_logprob": -0.11892305867055829, "compression_ratio": 1.7233201581027668, "no_speech_prob": 0.027157416567206383}, {"id": 294, "seek": 132824, "start": 1340.88, "end": 1346.4, "text": " For combining what I think are several of the best ideas about intelligence that have come up through through a number of", "tokens": [50996, 1171, 21928, 437, 286, 519, 366, 2940, 295, 264, 1151, 3487, 466, 7599, 300, 362, 808, 493, 807, 807, 257, 1230, 295, 51272], "temperature": 0.0, "avg_logprob": -0.11892305867055829, "compression_ratio": 1.7233201581027668, "no_speech_prob": 0.027157416567206383}, {"id": 295, "seek": 132824, "start": 1347.84, "end": 1352.48, "text": " You know decades in the field that includes neural networks modern probabilistic programming languages", "tokens": [51344, 509, 458, 7878, 294, 264, 2519, 300, 5974, 18161, 9590, 4363, 31959, 3142, 9410, 8650, 51576], "temperature": 0.0, "avg_logprob": -0.11892305867055829, "compression_ratio": 1.7233201581027668, "no_speech_prob": 0.027157416567206383}, {"id": 296, "seek": 135248, "start": 1352.8, "end": 1357.92, "text": " Like gen from vikash man singha's group who I mentioned before at mit or pyro", "tokens": [50380, 1743, 1049, 490, 371, 1035, 1299, 587, 1522, 1641, 311, 1594, 567, 286, 2835, 949, 412, 2194, 420, 10664, 340, 50636], "temperature": 0.0, "avg_logprob": -0.20443085949830334, "compression_ratio": 1.5893536121673004, "no_speech_prob": 0.039613962173461914}, {"id": 297, "seek": 135248, "start": 1358.48, "end": 1362.24, "text": " That was developed by originally a group at uber ai that noah goodman", "tokens": [50664, 663, 390, 4743, 538, 7993, 257, 1594, 412, 344, 607, 9783, 300, 572, 545, 665, 1601, 50852], "temperature": 0.0, "avg_logprob": -0.20443085949830334, "compression_ratio": 1.5893536121673004, "no_speech_prob": 0.039613962173461914}, {"id": 298, "seek": 135248, "start": 1362.72, "end": 1365.92, "text": " Another collaborator of ours helped to start along with a number of others", "tokens": [50876, 3996, 5091, 1639, 295, 11896, 4254, 281, 722, 2051, 365, 257, 1230, 295, 2357, 51036], "temperature": 0.0, "avg_logprob": -0.20443085949830334, "compression_ratio": 1.5893536121673004, "no_speech_prob": 0.039613962173461914}, {"id": 299, "seek": 135248, "start": 1367.1200000000001, "end": 1368.48, "text": " you know", "tokens": [51096, 291, 458, 51164], "temperature": 0.0, "avg_logprob": -0.20443085949830334, "compression_ratio": 1.5893536121673004, "no_speech_prob": 0.039613962173461914}, {"id": 300, "seek": 135248, "start": 1368.48, "end": 1372.08, "text": " In many ways build on languages like pie torch", "tokens": [51164, 682, 867, 2098, 1322, 322, 8650, 411, 1730, 27822, 51344], "temperature": 0.0, "avg_logprob": -0.20443085949830334, "compression_ratio": 1.5893536121673004, "no_speech_prob": 0.039613962173461914}, {"id": 301, "seek": 135248, "start": 1372.72, "end": 1379.68, "text": " Or tensor flow the languages that support modern deep learning and that allow you to construct really complex but end to end differentiable", "tokens": [51376, 1610, 40863, 3095, 264, 8650, 300, 1406, 4363, 2452, 2539, 293, 300, 2089, 291, 281, 7690, 534, 3997, 457, 917, 281, 917, 819, 9364, 51724], "temperature": 0.0, "avg_logprob": -0.20443085949830334, "compression_ratio": 1.5893536121673004, "no_speech_prob": 0.039613962173461914}, {"id": 302, "seek": 137968, "start": 1380.64, "end": 1382.64, "text": " Functions for approximation or other purposes", "tokens": [50412, 11166, 3916, 337, 28023, 420, 661, 9932, 50512], "temperature": 0.0, "avg_logprob": -0.13561226339901195, "compression_ratio": 1.7344398340248963, "no_speech_prob": 0.0033232674468308687}, {"id": 303, "seek": 137968, "start": 1383.28, "end": 1389.8400000000001, "text": " But that's not the most important part the most important part are building on the idea of symbolic languages for expressing abstract knowledge", "tokens": [50544, 583, 300, 311, 406, 264, 881, 1021, 644, 264, 881, 1021, 644, 366, 2390, 322, 264, 1558, 295, 25755, 8650, 337, 22171, 12649, 3601, 50872], "temperature": 0.0, "avg_logprob": -0.13561226339901195, "compression_ratio": 1.7344398340248963, "no_speech_prob": 0.0033232674468308687}, {"id": 304, "seek": 137968, "start": 1390.0800000000002, "end": 1391.92, "text": " for modeling the world", "tokens": [50884, 337, 15983, 264, 1002, 50976], "temperature": 0.0, "avg_logprob": -0.13561226339901195, "compression_ratio": 1.7344398340248963, "no_speech_prob": 0.0033232674468308687}, {"id": 305, "seek": 137968, "start": 1391.92, "end": 1393.28, "text": " that is", "tokens": [50976, 300, 307, 51044], "temperature": 0.0, "avg_logprob": -0.13561226339901195, "compression_ratio": 1.7344398340248963, "no_speech_prob": 0.0033232674468308687}, {"id": 306, "seek": 137968, "start": 1393.28, "end": 1395.52, "text": " You know across many areas of science and engineering", "tokens": [51044, 509, 458, 2108, 867, 3179, 295, 3497, 293, 7043, 51156], "temperature": 0.0, "avg_logprob": -0.13561226339901195, "compression_ratio": 1.7344398340248963, "no_speech_prob": 0.0033232674468308687}, {"id": 307, "seek": 137968, "start": 1396.0, "end": 1401.04, "text": " consistently the strongest most powerful toolkit we've had for building coherent models to", "tokens": [51180, 14961, 264, 16595, 881, 4005, 40167, 321, 600, 632, 337, 2390, 36239, 5245, 281, 51432], "temperature": 0.0, "avg_logprob": -0.13561226339901195, "compression_ratio": 1.7344398340248963, "no_speech_prob": 0.0033232674468308687}, {"id": 308, "seek": 137968, "start": 1401.68, "end": 1404.72, "text": " To understand the world are various forms of symbols", "tokens": [51464, 1407, 1223, 264, 1002, 366, 3683, 6422, 295, 16944, 51616], "temperature": 0.0, "avg_logprob": -0.13561226339901195, "compression_ratio": 1.7344398340248963, "no_speech_prob": 0.0033232674468308687}, {"id": 309, "seek": 140472, "start": 1405.44, "end": 1412.72, "text": " And then the third idea is the probabilistic one using those symbolic languages to express probabilistic models where you can be uncertain about", "tokens": [50400, 400, 550, 264, 2636, 1558, 307, 264, 31959, 3142, 472, 1228, 729, 25755, 8650, 281, 5109, 31959, 3142, 5245, 689, 291, 393, 312, 11308, 466, 50764], "temperature": 0.0, "avg_logprob": -0.09699729541400531, "compression_ratio": 1.8680555555555556, "no_speech_prob": 0.007007773034274578}, {"id": 310, "seek": 140472, "start": 1413.1200000000001, "end": 1415.84, "text": " Everything and in a in a turing computational sense", "tokens": [50784, 5471, 293, 294, 257, 294, 257, 256, 1345, 28270, 2020, 50920], "temperature": 0.0, "avg_logprob": -0.09699729541400531, "compression_ratio": 1.8680555555555556, "no_speech_prob": 0.007007773034274578}, {"id": 311, "seek": 140472, "start": 1415.84, "end": 1420.48, "text": " You can be uncertain about the state of the world right now or more abstractly how the world works", "tokens": [50920, 509, 393, 312, 11308, 466, 264, 1785, 295, 264, 1002, 558, 586, 420, 544, 12649, 356, 577, 264, 1002, 1985, 51152], "temperature": 0.0, "avg_logprob": -0.09699729541400531, "compression_ratio": 1.8680555555555556, "no_speech_prob": 0.007007773034274578}, {"id": 312, "seek": 140472, "start": 1421.2, "end": 1426.88, "text": " How the different kinds of data that you're getting perceptual or others are connected to the underlying state of the world", "tokens": [51188, 1012, 264, 819, 3685, 295, 1412, 300, 291, 434, 1242, 43276, 901, 420, 2357, 366, 4582, 281, 264, 14217, 1785, 295, 264, 1002, 51472], "temperature": 0.0, "avg_logprob": -0.09699729541400531, "compression_ratio": 1.8680555555555556, "no_speech_prob": 0.007007773034274578}, {"id": 313, "seek": 140472, "start": 1427.6000000000001, "end": 1432.4, "text": " And then to be able to do joint inference about all those different sources of uncertainty as the basis for perception", "tokens": [51508, 400, 550, 281, 312, 1075, 281, 360, 7225, 38253, 466, 439, 729, 819, 7139, 295, 15697, 382, 264, 5143, 337, 12860, 51748], "temperature": 0.0, "avg_logprob": -0.09699729541400531, "compression_ratio": 1.8680555555555556, "no_speech_prob": 0.007007773034274578}, {"id": 314, "seek": 143240, "start": 1432.8000000000002, "end": 1434.8000000000002, "text": " reasoning planning learning and so on", "tokens": [50384, 21577, 5038, 2539, 293, 370, 322, 50484], "temperature": 0.0, "avg_logprob": -0.1253915498423022, "compression_ratio": 1.6945606694560669, "no_speech_prob": 0.006189764477312565}, {"id": 315, "seek": 143240, "start": 1435.0400000000002, "end": 1437.0400000000002, "text": " And probabilistic programs bring those things together", "tokens": [50496, 400, 31959, 3142, 4268, 1565, 729, 721, 1214, 50596], "temperature": 0.0, "avg_logprob": -0.1253915498423022, "compression_ratio": 1.6945606694560669, "no_speech_prob": 0.006189764477312565}, {"id": 316, "seek": 143240, "start": 1437.6000000000001, "end": 1443.6000000000001, "text": " I'll say a little bit about how that toolkit works as well as then how once you learn language", "tokens": [50624, 286, 603, 584, 257, 707, 857, 466, 577, 300, 40167, 1985, 382, 731, 382, 550, 577, 1564, 291, 1466, 2856, 50924], "temperature": 0.0, "avg_logprob": -0.1253915498423022, "compression_ratio": 1.6945606694560669, "no_speech_prob": 0.006189764477312565}, {"id": 317, "seek": 143240, "start": 1444.72, "end": 1451.2, "text": " The ability to externalize and internalize thoughts that are produced by these kinds of probabilistic programs basically", "tokens": [50980, 440, 3485, 281, 8320, 1125, 293, 6920, 1125, 4598, 300, 366, 7126, 538, 613, 3685, 295, 31959, 3142, 4268, 1936, 51304], "temperature": 0.0, "avg_logprob": -0.1253915498423022, "compression_ratio": 1.6945606694560669, "no_speech_prob": 0.006189764477312565}, {"id": 318, "seek": 143240, "start": 1451.92, "end": 1457.3600000000001, "text": " That transforms things in fundamental ways and here, you know, less people think that i'm just a", "tokens": [51340, 663, 35592, 721, 294, 8088, 2098, 293, 510, 11, 291, 458, 11, 1570, 561, 519, 300, 741, 478, 445, 257, 51612], "temperature": 0.0, "avg_logprob": -0.1253915498423022, "compression_ratio": 1.6945606694560669, "no_speech_prob": 0.006189764477312565}, {"id": 319, "seek": 145736, "start": 1458.08, "end": 1460.3999999999999, "text": " Deep learning or a llm skeptic or something", "tokens": [50400, 14895, 2539, 420, 257, 287, 75, 76, 19128, 299, 420, 746, 50516], "temperature": 0.0, "avg_logprob": -0.14212024421022648, "compression_ratio": 1.7846715328467153, "no_speech_prob": 0.03901660814881325}, {"id": 320, "seek": 145736, "start": 1460.7199999999998, "end": 1467.04, "text": " I you know, again, I find neural language models and the the long tradition of distributional statistical learning", "tokens": [50532, 286, 291, 458, 11, 797, 11, 286, 915, 18161, 2856, 5245, 293, 264, 264, 938, 6994, 295, 7316, 304, 22820, 2539, 50848], "temperature": 0.0, "avg_logprob": -0.14212024421022648, "compression_ratio": 1.7846715328467153, "no_speech_prob": 0.03901660814881325}, {"id": 321, "seek": 145736, "start": 1467.4399999999998, "end": 1471.6, "text": " In language that they build on to be really important and in the work that we've been doing we've been using", "tokens": [50868, 682, 2856, 300, 436, 1322, 322, 281, 312, 534, 1021, 293, 294, 264, 589, 300, 321, 600, 668, 884, 321, 600, 668, 1228, 51076], "temperature": 0.0, "avg_logprob": -0.14212024421022648, "compression_ratio": 1.7846715328467153, "no_speech_prob": 0.03901660814881325}, {"id": 322, "seek": 145736, "start": 1472.3999999999999, "end": 1475.12, "text": " large language models or in many by today's standards", "tokens": [51116, 2416, 2856, 5245, 420, 294, 867, 538, 965, 311, 7787, 51252], "temperature": 0.0, "avg_logprob": -0.14212024421022648, "compression_ratio": 1.7846715328467153, "no_speech_prob": 0.03901660814881325}, {"id": 323, "seek": 145736, "start": 1475.12, "end": 1478.24, "text": " I might even say small language models to capture this actually", "tokens": [51252, 286, 1062, 754, 584, 1359, 2856, 5245, 281, 7983, 341, 767, 51408], "temperature": 0.0, "avg_logprob": -0.14212024421022648, "compression_ratio": 1.7846715328467153, "no_speech_prob": 0.03901660814881325}, {"id": 324, "seek": 145736, "start": 1478.9599999999998, "end": 1484.8799999999999, "text": " So I think this is a place where the you know, I would not say that you want to think about human minds", "tokens": [51444, 407, 286, 519, 341, 307, 257, 1081, 689, 264, 291, 458, 11, 286, 576, 406, 584, 300, 291, 528, 281, 519, 466, 1952, 9634, 51740], "temperature": 0.0, "avg_logprob": -0.14212024421022648, "compression_ratio": 1.7846715328467153, "no_speech_prob": 0.03901660814881325}, {"id": 325, "seek": 148488, "start": 1485.0400000000002, "end": 1488.3200000000002, "text": " It's llms or or transformers all the way down nothing like that", "tokens": [50372, 467, 311, 287, 75, 2592, 420, 420, 4088, 433, 439, 264, 636, 760, 1825, 411, 300, 50536], "temperature": 0.0, "avg_logprob": -0.12731333339915557, "compression_ratio": 1.5984848484848484, "no_speech_prob": 0.0006665780674666166}, {"id": 326, "seek": 148488, "start": 1488.64, "end": 1492.16, "text": " But the kinds of things that are going on in distributional sequence learning", "tokens": [50552, 583, 264, 3685, 295, 721, 300, 366, 516, 322, 294, 7316, 304, 8310, 2539, 50728], "temperature": 0.0, "avg_logprob": -0.12731333339915557, "compression_ratio": 1.5984848484848484, "no_speech_prob": 0.0006665780674666166}, {"id": 327, "seek": 148488, "start": 1492.5600000000002, "end": 1499.1200000000001, "text": " Could be a way to capture some aspects of how language grounds in these tools for mental modeling and and also", "tokens": [50748, 7497, 312, 257, 636, 281, 7983, 512, 7270, 295, 577, 2856, 19196, 294, 613, 3873, 337, 4973, 15983, 293, 293, 611, 51076], "temperature": 0.0, "avg_logprob": -0.12731333339915557, "compression_ratio": 1.5984848484848484, "no_speech_prob": 0.0006665780674666166}, {"id": 328, "seek": 148488, "start": 1499.6000000000001, "end": 1501.6000000000001, "text": " enriches and extends it", "tokens": [51100, 18849, 279, 293, 26448, 309, 51200], "temperature": 0.0, "avg_logprob": -0.12731333339915557, "compression_ratio": 1.5984848484848484, "no_speech_prob": 0.0006665780674666166}, {"id": 329, "seek": 148488, "start": 1502.0, "end": 1505.92, "text": " So, I mean just very briefly for people who aren't familiar with this idea", "tokens": [51220, 407, 11, 286, 914, 445, 588, 10515, 337, 561, 567, 3212, 380, 4963, 365, 341, 1558, 51416], "temperature": 0.0, "avg_logprob": -0.12731333339915557, "compression_ratio": 1.5984848484848484, "no_speech_prob": 0.0006665780674666166}, {"id": 330, "seek": 148488, "start": 1506.4, "end": 1509.8400000000001, "text": " There, you know, for a couple of decades now I and and many colleagues", "tokens": [51440, 821, 11, 291, 458, 11, 337, 257, 1916, 295, 7878, 586, 286, 293, 293, 867, 7734, 51612], "temperature": 0.0, "avg_logprob": -0.12731333339915557, "compression_ratio": 1.5984848484848484, "no_speech_prob": 0.0006665780674666166}, {"id": 331, "seek": 150984, "start": 1510.24, "end": 1515.36, "text": " Students former students a number of others who are very interested in what you could call the Bayesian approach to cognition", "tokens": [50384, 17244, 5819, 1731, 257, 1230, 295, 2357, 567, 366, 588, 3102, 294, 437, 291, 727, 818, 264, 7840, 42434, 3109, 281, 46905, 50640], "temperature": 0.0, "avg_logprob": -0.11199814820092571, "compression_ratio": 1.706060606060606, "no_speech_prob": 0.05497157573699951}, {"id": 332, "seek": 150984, "start": 1516.08, "end": 1520.8, "text": " Have been using this general toolkit of probabilistic inference over structured symbolic models", "tokens": [50676, 3560, 668, 1228, 341, 2674, 40167, 295, 31959, 3142, 38253, 670, 18519, 25755, 5245, 50912], "temperature": 0.0, "avg_logprob": -0.11199814820092571, "compression_ratio": 1.706060606060606, "no_speech_prob": 0.05497157573699951}, {"id": 333, "seek": 150984, "start": 1521.1999999999998, "end": 1526.24, "text": " To capture many aspects of mental models and I just this is a plug for a book that's coming", "tokens": [50932, 1407, 7983, 867, 7270, 295, 4973, 5245, 293, 286, 445, 341, 307, 257, 5452, 337, 257, 1446, 300, 311, 1348, 51184], "temperature": 0.0, "avg_logprob": -0.11199814820092571, "compression_ratio": 1.706060606060606, "no_speech_prob": 0.05497157573699951}, {"id": 334, "seek": 150984, "start": 1526.8, "end": 1531.52, "text": " forthcoming edited by tom griffith's nick shader and myself and with many many other contributors", "tokens": [51212, 5220, 6590, 23016, 538, 2916, 677, 3661, 355, 311, 15416, 5744, 260, 293, 2059, 293, 365, 867, 867, 661, 45627, 51448], "temperature": 0.0, "avg_logprob": -0.11199814820092571, "compression_ratio": 1.706060606060606, "no_speech_prob": 0.05497157573699951}, {"id": 335, "seek": 150984, "start": 1532.0, "end": 1534.3999999999999, "text": " And it'll be coming later this year from mit press", "tokens": [51472, 400, 309, 603, 312, 1348, 1780, 341, 1064, 490, 2194, 1886, 51592], "temperature": 0.0, "avg_logprob": -0.11199814820092571, "compression_ratio": 1.706060606060606, "no_speech_prob": 0.05497157573699951}, {"id": 336, "seek": 150984, "start": 1534.72, "end": 1539.6, "text": " But it's it's part textbook part research monograph and just showing how this toolkit can be used to", "tokens": [51608, 583, 309, 311, 309, 311, 644, 25591, 644, 2132, 1108, 3108, 293, 445, 4099, 577, 341, 40167, 393, 312, 1143, 281, 51852], "temperature": 0.0, "avg_logprob": -0.11199814820092571, "compression_ratio": 1.706060606060606, "no_speech_prob": 0.05497157573699951}, {"id": 337, "seek": 153984, "start": 1540.1599999999999, "end": 1545.4399999999998, "text": " Capture to both explain in a principled and you know, actually understandable way", "tokens": [50380, 9480, 540, 281, 1293, 2903, 294, 257, 3681, 15551, 293, 291, 458, 11, 767, 25648, 636, 50644], "temperature": 0.0, "avg_logprob": -0.10504462055324279, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.00033525816979818046}, {"id": 338, "seek": 153984, "start": 1545.76, "end": 1552.1599999999999, "text": " But also to quantitatively predict and model behavioral data across so many different ways in which our minds model the world", "tokens": [50660, 583, 611, 281, 27778, 356, 6069, 293, 2316, 19124, 1412, 2108, 370, 867, 819, 2098, 294, 597, 527, 9634, 2316, 264, 1002, 50980], "temperature": 0.0, "avg_logprob": -0.10504462055324279, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.00033525816979818046}, {"id": 339, "seek": 153984, "start": 1552.6399999999999, "end": 1555.84, "text": " Just to illustrate one which i'll come back to in the context of language in a minute", "tokens": [51004, 1449, 281, 23221, 472, 597, 741, 603, 808, 646, 281, 294, 264, 4319, 295, 2856, 294, 257, 3456, 51164], "temperature": 0.0, "avg_logprob": -0.10504462055324279, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.00033525816979818046}, {"id": 340, "seek": 153984, "start": 1557.36, "end": 1560.08, "text": " An area that we've done a lot of work on is intuitive physics", "tokens": [51240, 1107, 1859, 300, 321, 600, 1096, 257, 688, 295, 589, 322, 307, 21769, 10649, 51376], "temperature": 0.0, "avg_logprob": -0.10504462055324279, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.00033525816979818046}, {"id": 341, "seek": 153984, "start": 1561.04, "end": 1564.8, "text": " As as a kind of probabilistic inference and the particular kinds of intuitive physics", "tokens": [51424, 1018, 382, 257, 733, 295, 31959, 3142, 38253, 293, 264, 1729, 3685, 295, 21769, 10649, 51612], "temperature": 0.0, "avg_logprob": -0.10504462055324279, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.00033525816979818046}, {"id": 342, "seek": 156480, "start": 1564.8, "end": 1570.0, "text": " We're thinking about are in these complex seed understanding cases, which again not coincidentally", "tokens": [50364, 492, 434, 1953, 466, 366, 294, 613, 3997, 8871, 3701, 3331, 11, 597, 797, 406, 13001, 36578, 50624], "temperature": 0.0, "avg_logprob": -0.15799427958368098, "compression_ratio": 1.6945454545454546, "no_speech_prob": 0.12925641238689423}, {"id": 343, "seek": 156480, "start": 1570.08, "end": 1573.84, "text": " I think are also really interesting in classic settings for studying grounded language", "tokens": [50628, 286, 519, 366, 611, 534, 1880, 294, 7230, 6257, 337, 7601, 23535, 2856, 50816], "temperature": 0.0, "avg_logprob": -0.15799427958368098, "compression_ratio": 1.6945454545454546, "no_speech_prob": 0.12925641238689423}, {"id": 344, "seek": 156480, "start": 1574.3999999999999, "end": 1576.3999999999999, "text": " So for example, if I show you a", "tokens": [50844, 407, 337, 1365, 11, 498, 286, 855, 291, 257, 50944], "temperature": 0.0, "avg_logprob": -0.15799427958368098, "compression_ratio": 1.6945454545454546, "no_speech_prob": 0.12925641238689423}, {"id": 345, "seek": 156480, "start": 1577.12, "end": 1580.8799999999999, "text": " Scene of a bunch of blocks think like jenga blocks stacked up in various ways", "tokens": [50980, 46297, 295, 257, 3840, 295, 8474, 519, 411, 361, 31494, 8474, 28867, 493, 294, 3683, 2098, 51168], "temperature": 0.0, "avg_logprob": -0.15799427958368098, "compression_ratio": 1.6945454545454546, "no_speech_prob": 0.12925641238689423}, {"id": 346, "seek": 156480, "start": 1581.2, "end": 1586.8, "text": " Some of these images might look very stable others might look unstable and I can ask you a question", "tokens": [51184, 2188, 295, 613, 5267, 1062, 574, 588, 8351, 2357, 1062, 574, 23742, 293, 286, 393, 1029, 291, 257, 1168, 51464], "temperature": 0.0, "avg_logprob": -0.15799427958368098, "compression_ratio": 1.6945454545454546, "no_speech_prob": 0.12925641238689423}, {"id": 347, "seek": 156480, "start": 1586.8799999999999, "end": 1590.08, "text": " How likely do you think this stack of blocks is to fall under gravity?", "tokens": [51468, 1012, 3700, 360, 291, 519, 341, 8630, 295, 8474, 307, 281, 2100, 833, 12110, 30, 51628], "temperature": 0.0, "avg_logprob": -0.15799427958368098, "compression_ratio": 1.6945454545454546, "no_speech_prob": 0.12925641238689423}, {"id": 348, "seek": 159008, "start": 1590.8, "end": 1597.28, "text": " And we can model that by having a a structured world model which consists of basically 3d object models", "tokens": [50400, 400, 321, 393, 2316, 300, 538, 1419, 257, 257, 18519, 1002, 2316, 597, 14689, 295, 1936, 805, 67, 2657, 5245, 50724], "temperature": 0.0, "avg_logprob": -0.11751451597108946, "compression_ratio": 1.7622950819672132, "no_speech_prob": 0.003943570423871279}, {"id": 349, "seek": 159008, "start": 1597.6799999999998, "end": 1603.04, "text": " And then causal models of how though of how those underlying 3d scenes give rise to images", "tokens": [50744, 400, 550, 38755, 5245, 295, 577, 1673, 295, 577, 729, 14217, 805, 67, 8026, 976, 6272, 281, 5267, 51012], "temperature": 0.0, "avg_logprob": -0.11751451597108946, "compression_ratio": 1.7622950819672132, "no_speech_prob": 0.003943570423871279}, {"id": 350, "seek": 159008, "start": 1603.12, "end": 1608.96, "text": " That's like a graphics program basically the computer graphics is a way of writing programs that generate images", "tokens": [51016, 663, 311, 411, 257, 11837, 1461, 1936, 264, 3820, 11837, 307, 257, 636, 295, 3579, 4268, 300, 8460, 5267, 51308], "temperature": 0.0, "avg_logprob": -0.11751451597108946, "compression_ratio": 1.7622950819672132, "no_speech_prob": 0.003943570423871279}, {"id": 351, "seek": 159008, "start": 1609.28, "end": 1611.28, "text": " From those underlying 3d world models", "tokens": [51324, 3358, 729, 14217, 805, 67, 1002, 5245, 51424], "temperature": 0.0, "avg_logprob": -0.11751451597108946, "compression_ratio": 1.7622950819672132, "no_speech_prob": 0.003943570423871279}, {"id": 352, "seek": 159008, "start": 1611.6799999999998, "end": 1615.4399999999998, "text": " But then there's also these physics programs and again here I mentioned game engines", "tokens": [51444, 583, 550, 456, 311, 611, 613, 10649, 4268, 293, 797, 510, 286, 2835, 1216, 12982, 51632], "temperature": 0.0, "avg_logprob": -0.11751451597108946, "compression_ratio": 1.7622950819672132, "no_speech_prob": 0.003943570423871279}, {"id": 353, "seek": 161544, "start": 1615.68, "end": 1621.04, "text": " So game engine style physics simulators capture a lot of aspects of effectively real world", "tokens": [50376, 407, 1216, 2848, 3758, 10649, 1034, 39265, 7983, 257, 688, 295, 7270, 295, 8659, 957, 1002, 50644], "temperature": 0.0, "avg_logprob": -0.13072198232014973, "compression_ratio": 1.6971830985915493, "no_speech_prob": 0.01797792688012123}, {"id": 354, "seek": 161544, "start": 1621.1200000000001, "end": 1624.96, "text": " But common sense physics in ways that hack hack Newtonian or actual", "tokens": [50648, 583, 2689, 2020, 10649, 294, 2098, 300, 10339, 10339, 19541, 952, 420, 3539, 50840], "temperature": 0.0, "avg_logprob": -0.13072198232014973, "compression_ratio": 1.6971830985915493, "no_speech_prob": 0.01797792688012123}, {"id": 355, "seek": 161544, "start": 1625.52, "end": 1627.92, "text": " True scientific physics in all sorts of ways to be efficient", "tokens": [50868, 13587, 8134, 10649, 294, 439, 7527, 295, 2098, 281, 312, 7148, 50988], "temperature": 0.0, "avg_logprob": -0.13072198232014973, "compression_ratio": 1.6971830985915493, "no_speech_prob": 0.01797792688012123}, {"id": 356, "seek": 161544, "start": 1628.4, "end": 1630.4, "text": " But to a pretty good job of capturing", "tokens": [51012, 583, 281, 257, 1238, 665, 1691, 295, 23384, 51112], "temperature": 0.0, "avg_logprob": -0.13072198232014973, "compression_ratio": 1.6971830985915493, "no_speech_prob": 0.01797792688012123}, {"id": 357, "seek": 161544, "start": 1630.56, "end": 1635.1200000000001, "text": " What we expect to happen in the world which makes sense because we're the ones playing the video games and they're designed for us", "tokens": [51120, 708, 321, 2066, 281, 1051, 294, 264, 1002, 597, 1669, 2020, 570, 321, 434, 264, 2306, 2433, 264, 960, 2813, 293, 436, 434, 4761, 337, 505, 51348], "temperature": 0.0, "avg_logprob": -0.13072198232014973, "compression_ratio": 1.6971830985915493, "no_speech_prob": 0.01797792688012123}, {"id": 358, "seek": 161544, "start": 1635.52, "end": 1641.04, "text": " So by doing probabilistic inference to infer the input to a graphics program given the output", "tokens": [51368, 407, 538, 884, 31959, 3142, 38253, 281, 13596, 264, 4846, 281, 257, 11837, 1461, 2212, 264, 5598, 51644], "temperature": 0.0, "avg_logprob": -0.13072198232014973, "compression_ratio": 1.6971830985915493, "no_speech_prob": 0.01797792688012123}, {"id": 359, "seek": 164104, "start": 1641.12, "end": 1646.96, "text": " Which is the image you can do a lot of 3d perception and then by doing probabilistic forward simulation", "tokens": [50368, 3013, 307, 264, 3256, 291, 393, 360, 257, 688, 295, 805, 67, 12860, 293, 550, 538, 884, 31959, 3142, 2128, 16575, 50660], "temperature": 0.0, "avg_logprob": -0.0756224534642978, "compression_ratio": 1.7588424437299035, "no_speech_prob": 0.032086241990327835}, {"id": 360, "seek": 164104, "start": 1647.12, "end": 1650.72, "text": " You can imagine what might happen next. It could also be conditional on your action", "tokens": [50668, 509, 393, 3811, 437, 1062, 1051, 958, 13, 467, 727, 611, 312, 27708, 322, 428, 3069, 50848], "temperature": 0.0, "avg_logprob": -0.0756224534642978, "compression_ratio": 1.7588424437299035, "no_speech_prob": 0.032086241990327835}, {"id": 361, "seek": 164104, "start": 1651.2, "end": 1654.8, "text": " So the same toolkit could be applied to a much less familiar sort of judgment", "tokens": [50872, 407, 264, 912, 40167, 727, 312, 6456, 281, 257, 709, 1570, 4963, 1333, 295, 12216, 51052], "temperature": 0.0, "avg_logprob": -0.0756224534642978, "compression_ratio": 1.7588424437299035, "no_speech_prob": 0.032086241990327835}, {"id": 362, "seek": 164104, "start": 1655.36, "end": 1658.3999999999999, "text": " Like imagine I have these scenes of red and yellow blocks on a table", "tokens": [51080, 1743, 3811, 286, 362, 613, 8026, 295, 2182, 293, 5566, 8474, 322, 257, 3199, 51232], "temperature": 0.0, "avg_logprob": -0.0756224534642978, "compression_ratio": 1.7588424437299035, "no_speech_prob": 0.032086241990327835}, {"id": 363, "seek": 164104, "start": 1658.48, "end": 1661.76, "text": " And what if I bump the table hard enough to knock some of the blocks onto the floor?", "tokens": [51236, 400, 437, 498, 286, 9961, 264, 3199, 1152, 1547, 281, 6728, 512, 295, 264, 8474, 3911, 264, 4123, 30, 51400], "temperature": 0.0, "avg_logprob": -0.0756224534642978, "compression_ratio": 1.7588424437299035, "no_speech_prob": 0.032086241990327835}, {"id": 364, "seek": 164104, "start": 1662.08, "end": 1664.3999999999999, "text": " Will you knock off more red blocks or yellow blocks?", "tokens": [51416, 3099, 291, 6728, 766, 544, 2182, 8474, 420, 5566, 8474, 30, 51532], "temperature": 0.0, "avg_logprob": -0.0756224534642978, "compression_ratio": 1.7588424437299035, "no_speech_prob": 0.032086241990327835}, {"id": 365, "seek": 164104, "start": 1665.04, "end": 1668.1599999999999, "text": " So the first judgment the one about how likely the stack of blocks to fall", "tokens": [51564, 407, 264, 700, 12216, 264, 472, 466, 577, 3700, 264, 8630, 295, 8474, 281, 2100, 51720], "temperature": 0.0, "avg_logprob": -0.0756224534642978, "compression_ratio": 1.7588424437299035, "no_speech_prob": 0.032086241990327835}, {"id": 366, "seek": 166816, "start": 1668.16, "end": 1672.0, "text": " I mean anyone who's played jenga or anybody who's ever been a kid probably has built things", "tokens": [50364, 286, 914, 2878, 567, 311, 3737, 361, 31494, 420, 4472, 567, 311, 1562, 668, 257, 1636, 1391, 575, 3094, 721, 50556], "temperature": 0.0, "avg_logprob": -0.1019868129441718, "compression_ratio": 1.72, "no_speech_prob": 0.01565505377948284}, {"id": 367, "seek": 166816, "start": 1672.24, "end": 1674.72, "text": " If you're a professional builder, you have a lot of other intuitions", "tokens": [50568, 759, 291, 434, 257, 4843, 27377, 11, 291, 362, 257, 688, 295, 661, 16224, 626, 50692], "temperature": 0.0, "avg_logprob": -0.1019868129441718, "compression_ratio": 1.72, "no_speech_prob": 0.01565505377948284}, {"id": 368, "seek": 166816, "start": 1675.0400000000002, "end": 1679.52, "text": " But you know, that's very familiar the question on the right is one that unless you've seen me talk about this", "tokens": [50708, 583, 291, 458, 11, 300, 311, 588, 4963, 264, 1168, 322, 264, 558, 307, 472, 300, 5969, 291, 600, 1612, 385, 751, 466, 341, 50932], "temperature": 0.0, "avg_logprob": -0.1019868129441718, "compression_ratio": 1.72, "no_speech_prob": 0.01565505377948284}, {"id": 369, "seek": 166816, "start": 1679.76, "end": 1685.0400000000002, "text": " You probably never thought about it's not something you have direct experience about you can't learn it from feedback", "tokens": [50944, 509, 1391, 1128, 1194, 466, 309, 311, 406, 746, 291, 362, 2047, 1752, 466, 291, 393, 380, 1466, 309, 490, 5824, 51208], "temperature": 0.0, "avg_logprob": -0.1019868129441718, "compression_ratio": 1.72, "no_speech_prob": 0.01565505377948284}, {"id": 370, "seek": 166816, "start": 1685.52, "end": 1686.96, "text": " How to answer that question?", "tokens": [51232, 1012, 281, 1867, 300, 1168, 30, 51304], "temperature": 0.0, "avg_logprob": -0.1019868129441718, "compression_ratio": 1.72, "no_speech_prob": 0.01565505377948284}, {"id": 371, "seek": 166816, "start": 1687.28, "end": 1693.0400000000002, "text": " But I can use my language to give you that question as well as the other relevant world knowledge", "tokens": [51320, 583, 286, 393, 764, 452, 2856, 281, 976, 291, 300, 1168, 382, 731, 382, 264, 661, 7340, 1002, 3601, 51608], "temperature": 0.0, "avg_logprob": -0.1019868129441718, "compression_ratio": 1.72, "no_speech_prob": 0.01565505377948284}, {"id": 372, "seek": 169304, "start": 1693.04, "end": 1698.8799999999999, "text": " And then you can reason about it and the models that we build can capture both the familiar judgments of like", "tokens": [50364, 400, 550, 291, 393, 1778, 466, 309, 293, 264, 5245, 300, 321, 1322, 393, 7983, 1293, 264, 4963, 40337, 295, 411, 50656], "temperature": 0.0, "avg_logprob": -0.11378695283617292, "compression_ratio": 1.7669172932330828, "no_speech_prob": 0.06183004379272461}, {"id": 373, "seek": 169304, "start": 1698.96, "end": 1701.68, "text": " How likely the stack is to fall as well as these novel judgments", "tokens": [50660, 1012, 3700, 264, 8630, 307, 281, 2100, 382, 731, 382, 613, 7613, 40337, 50796], "temperature": 0.0, "avg_logprob": -0.11378695283617292, "compression_ratio": 1.7669172932330828, "no_speech_prob": 0.06183004379272461}, {"id": 374, "seek": 169304, "start": 1702.48, "end": 1703.84, "text": " Relatively equally well", "tokens": [50836, 8738, 19020, 12309, 731, 50904], "temperature": 0.0, "avg_logprob": -0.11378695283617292, "compression_ratio": 1.7669172932330828, "no_speech_prob": 0.06183004379272461}, {"id": 375, "seek": 169304, "start": 1703.84, "end": 1708.3999999999999, "text": " So these scatter plots are examples of the kind of data and modeling that we've done in our lab for a long time", "tokens": [50904, 407, 613, 34951, 28609, 366, 5110, 295, 264, 733, 295, 1412, 293, 15983, 300, 321, 600, 1096, 294, 527, 2715, 337, 257, 938, 565, 51132], "temperature": 0.0, "avg_logprob": -0.11378695283617292, "compression_ratio": 1.7669172932330828, "no_speech_prob": 0.06183004379272461}, {"id": 376, "seek": 169304, "start": 1708.8799999999999, "end": 1716.08, "text": " On the y-axis we're plotting the average human judgments of let's say on one to seven scale how stable or unstable the blocks are", "tokens": [51156, 1282, 264, 288, 12, 24633, 321, 434, 41178, 264, 4274, 1952, 40337, 295, 718, 311, 584, 322, 472, 281, 3407, 4373, 577, 8351, 420, 23742, 264, 8474, 366, 51516], "temperature": 0.0, "avg_logprob": -0.11378695283617292, "compression_ratio": 1.7669172932330828, "no_speech_prob": 0.06183004379272461}, {"id": 377, "seek": 169304, "start": 1716.8, "end": 1718.8, "text": " and on the x-axis the average", "tokens": [51552, 293, 322, 264, 2031, 12, 24633, 264, 4274, 51652], "temperature": 0.0, "avg_logprob": -0.11378695283617292, "compression_ratio": 1.7669172932330828, "no_speech_prob": 0.06183004379272461}, {"id": 378, "seek": 171880, "start": 1719.76, "end": 1724.1599999999999, "text": " Result of doing a small number of probabilistic simulations where we imagine running forward", "tokens": [50412, 5015, 723, 295, 884, 257, 1359, 1230, 295, 31959, 3142, 35138, 689, 321, 3811, 2614, 2128, 50632], "temperature": 0.0, "avg_logprob": -0.12176397751117575, "compression_ratio": 1.69, "no_speech_prob": 0.007690361235290766}, {"id": 379, "seek": 171880, "start": 1725.12, "end": 1731.52, "text": " Game-style physics a few time steps with uncertainty about exactly where the blocks might be and how the physics works because we don't", "tokens": [50680, 7522, 12, 15014, 10649, 257, 1326, 565, 4439, 365, 15697, 466, 2293, 689, 264, 8474, 1062, 312, 293, 577, 264, 10649, 1985, 570, 321, 500, 380, 51000], "temperature": 0.0, "avg_logprob": -0.12176397751117575, "compression_ratio": 1.69, "no_speech_prob": 0.007690361235290766}, {"id": 380, "seek": 171880, "start": 1731.84, "end": 1733.84, "text": " There's are all things that our minds don't fully know", "tokens": [51016, 821, 311, 366, 439, 721, 300, 527, 9634, 500, 380, 4498, 458, 51116], "temperature": 0.0, "avg_logprob": -0.12176397751117575, "compression_ratio": 1.69, "no_speech_prob": 0.007690361235290766}, {"id": 381, "seek": 171880, "start": 1734.24, "end": 1738.48, "text": " The same kind of model can be used to answer these red and yellow questions. So just to illustrate this", "tokens": [51136, 440, 912, 733, 295, 2316, 393, 312, 1143, 281, 1867, 613, 2182, 293, 5566, 1651, 13, 407, 445, 281, 23221, 341, 51348], "temperature": 0.0, "avg_logprob": -0.12176397751117575, "compression_ratio": 1.69, "no_speech_prob": 0.007690361235290766}, {"id": 382, "seek": 171880, "start": 1739.44, "end": 1740.96, "text": " If we take one of these", "tokens": [51396, 759, 321, 747, 472, 295, 613, 51472], "temperature": 0.0, "avg_logprob": -0.12176397751117575, "compression_ratio": 1.69, "no_speech_prob": 0.007690361235290766}, {"id": 383, "seek": 171880, "start": 1740.96, "end": 1745.84, "text": " Scenes and we reconstruct it in a game-style physics engine and we simulate a bump of the table", "tokens": [51472, 2747, 25973, 293, 321, 31499, 309, 294, 257, 1216, 12, 15014, 10649, 2848, 293, 321, 27817, 257, 9961, 295, 264, 3199, 51716], "temperature": 0.0, "avg_logprob": -0.12176397751117575, "compression_ratio": 1.69, "no_speech_prob": 0.007690361235290766}, {"id": 384, "seek": 174584, "start": 1746.1599999999999, "end": 1750.48, "text": " So there's one simulation on the right. I'll show you another simulation with a harder bump", "tokens": [50380, 407, 456, 311, 472, 16575, 322, 264, 558, 13, 286, 603, 855, 291, 1071, 16575, 365, 257, 6081, 9961, 50596], "temperature": 0.0, "avg_logprob": -0.09520370081851356, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0026312307454645634}, {"id": 385, "seek": 174584, "start": 1750.8, "end": 1754.6399999999999, "text": " Okay, and you can see watch it again that different things happen", "tokens": [50612, 1033, 11, 293, 291, 393, 536, 1159, 309, 797, 300, 819, 721, 1051, 50804], "temperature": 0.0, "avg_logprob": -0.09520370081851356, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0026312307454645634}, {"id": 386, "seek": 174584, "start": 1755.4399999999998, "end": 1757.4399999999998, "text": " in these two simulations", "tokens": [50844, 294, 613, 732, 35138, 50944], "temperature": 0.0, "avg_logprob": -0.09520370081851356, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0026312307454645634}, {"id": 387, "seek": 174584, "start": 1757.6, "end": 1761.28, "text": " But it doesn't really matter which one of those you ran to answer the question, right?", "tokens": [50952, 583, 309, 1177, 380, 534, 1871, 597, 472, 295, 729, 291, 5872, 281, 1867, 264, 1168, 11, 558, 30, 51136], "temperature": 0.0, "avg_logprob": -0.09520370081851356, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0026312307454645634}, {"id": 388, "seek": 174584, "start": 1761.28, "end": 1765.1999999999998, "text": " I mean you look at the scene also and it's very clear from the beginning that if I bump the table", "tokens": [51136, 286, 914, 291, 574, 412, 264, 4145, 611, 293, 309, 311, 588, 1850, 490, 264, 2863, 300, 498, 286, 9961, 264, 3199, 51332], "temperature": 0.0, "avg_logprob": -0.09520370081851356, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0026312307454645634}, {"id": 389, "seek": 174584, "start": 1765.6799999999998, "end": 1769.6799999999998, "text": " Relatively hard. It's going to probably be mostly yellow blocks on the floor. How do you do that?", "tokens": [51356, 8738, 19020, 1152, 13, 467, 311, 516, 281, 1391, 312, 5240, 5566, 8474, 322, 264, 4123, 13, 1012, 360, 291, 360, 300, 30, 51556], "temperature": 0.0, "avg_logprob": -0.09520370081851356, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0026312307454645634}, {"id": 390, "seek": 176968, "start": 1769.76, "end": 1771.76, "text": " Well in our model you only need to run", "tokens": [50368, 1042, 294, 527, 2316, 291, 787, 643, 281, 1190, 50468], "temperature": 0.0, "avg_logprob": -0.13609590838032384, "compression_ratio": 1.749185667752443, "no_speech_prob": 0.03066260740160942}, {"id": 391, "seek": 176968, "start": 1772.16, "end": 1776.4, "text": " One or a small number of these simulations to answer the question at the grain of intuitive physics", "tokens": [50488, 1485, 420, 257, 1359, 1230, 295, 613, 35138, 281, 1867, 264, 1168, 412, 264, 12837, 295, 21769, 10649, 50700], "temperature": 0.0, "avg_logprob": -0.13609590838032384, "compression_ratio": 1.749185667752443, "no_speech_prob": 0.03066260740160942}, {"id": 392, "seek": 176968, "start": 1776.5600000000002, "end": 1779.76, "text": " And you don't need to run it very long. You could stop it now and you already know the answer", "tokens": [50708, 400, 291, 500, 380, 643, 281, 1190, 309, 588, 938, 13, 509, 727, 1590, 309, 586, 293, 291, 1217, 458, 264, 1867, 50868], "temperature": 0.0, "avg_logprob": -0.13609590838032384, "compression_ratio": 1.749185667752443, "no_speech_prob": 0.03066260740160942}, {"id": 393, "seek": 176968, "start": 1780.16, "end": 1780.64, "text": " Okay", "tokens": [50888, 1033, 50912], "temperature": 0.0, "avg_logprob": -0.13609590838032384, "compression_ratio": 1.749185667752443, "no_speech_prob": 0.03066260740160942}, {"id": 394, "seek": 176968, "start": 1780.64, "end": 1785.04, "text": " So a small number of short incomplete simulations is the basis for these pretty", "tokens": [50912, 407, 257, 1359, 1230, 295, 2099, 31709, 35138, 307, 264, 5143, 337, 613, 1238, 51132], "temperature": 0.0, "avg_logprob": -0.13609590838032384, "compression_ratio": 1.749185667752443, "no_speech_prob": 0.03066260740160942}, {"id": 395, "seek": 176968, "start": 1785.3600000000001, "end": 1791.04, "text": " Quantitative models and they've also been used in robotics to actually predict and and be able to get for example a robot to", "tokens": [51148, 26968, 14275, 5245, 293, 436, 600, 611, 668, 1143, 294, 34145, 281, 767, 6069, 293, 293, 312, 1075, 281, 483, 337, 1365, 257, 7881, 281, 51432], "temperature": 0.0, "avg_logprob": -0.13609590838032384, "compression_ratio": 1.749185667752443, "no_speech_prob": 0.03066260740160942}, {"id": 396, "seek": 176968, "start": 1791.6000000000001, "end": 1796.0, "text": " Learn to play the game genka, but learn from a very small amount of realistic human experience", "tokens": [51460, 17216, 281, 862, 264, 1216, 1049, 2330, 11, 457, 1466, 490, 257, 588, 1359, 2372, 295, 12465, 1952, 1752, 51680], "temperature": 0.0, "avg_logprob": -0.13609590838032384, "compression_ratio": 1.749185667752443, "no_speech_prob": 0.03066260740160942}, {"id": 397, "seek": 179600, "start": 1796.64, "end": 1798.64, "text": " The same kind of idea has been very", "tokens": [50396, 440, 912, 733, 295, 1558, 575, 668, 588, 50496], "temperature": 0.0, "avg_logprob": -0.17207213935502078, "compression_ratio": 1.6593406593406594, "no_speech_prob": 0.00042381734238006175}, {"id": 398, "seek": 179600, "start": 1798.96, "end": 1799.92, "text": " impactful", "tokens": [50512, 30842, 50560], "temperature": 0.0, "avg_logprob": -0.17207213935502078, "compression_ratio": 1.6593406593406594, "no_speech_prob": 0.00042381734238006175}, {"id": 399, "seek": 179600, "start": 1799.92, "end": 1803.04, "text": " I think even more impactful in intuitive psychology", "tokens": [50560, 286, 519, 754, 544, 30842, 294, 21769, 15105, 50716], "temperature": 0.0, "avg_logprob": -0.17207213935502078, "compression_ratio": 1.6593406593406594, "no_speech_prob": 0.00042381734238006175}, {"id": 400, "seek": 179600, "start": 1803.76, "end": 1809.84, "text": " Where we and many others this there's work that I've done with a number of students and my colleague at mit rebecca sacks", "tokens": [50752, 2305, 321, 293, 867, 2357, 341, 456, 311, 589, 300, 286, 600, 1096, 365, 257, 1230, 295, 1731, 293, 452, 13532, 412, 2194, 319, 17073, 262, 7424, 51056], "temperature": 0.0, "avg_logprob": -0.17207213935502078, "compression_ratio": 1.6593406593406594, "no_speech_prob": 0.00042381734238006175}, {"id": 401, "seek": 179600, "start": 1809.92, "end": 1815.84, "text": " But especially I would highlight work from chris baker and julia har edinger who did quantitative modeling here", "tokens": [51060, 583, 2318, 286, 576, 5078, 589, 490, 417, 5714, 48148, 293, 30764, 654, 2233, 1257, 6911, 567, 630, 27778, 15983, 510, 51356], "temperature": 0.0, "avg_logprob": -0.17207213935502078, "compression_ratio": 1.6593406593406594, "no_speech_prob": 0.00042381734238006175}, {"id": 402, "seek": 179600, "start": 1816.16, "end": 1818.16, "text": " Going back years with us", "tokens": [51372, 10963, 646, 924, 365, 505, 51472], "temperature": 0.0, "avg_logprob": -0.17207213935502078, "compression_ratio": 1.6593406593406594, "no_speech_prob": 0.00042381734238006175}, {"id": 403, "seek": 179600, "start": 1818.48, "end": 1821.84, "text": " julian continued chris is now working on self-driving cars and industry julian", "tokens": [51488, 30764, 952, 7014, 417, 5714, 307, 586, 1364, 322, 2698, 12, 47094, 5163, 293, 3518, 30764, 952, 51656], "temperature": 0.0, "avg_logprob": -0.17207213935502078, "compression_ratio": 1.6593406593406594, "no_speech_prob": 0.00042381734238006175}, {"id": 404, "seek": 179600, "start": 1822.56, "end": 1824.56, "text": " is now working on", "tokens": [51692, 307, 586, 1364, 322, 51792], "temperature": 0.0, "avg_logprob": -0.17207213935502078, "compression_ratio": 1.6593406593406594, "no_speech_prob": 0.00042381734238006175}, {"id": 405, "seek": 182456, "start": 1825.28, "end": 1832.48, "text": " It's now a professor at Yale where he's he's extended on this kind of approach in many areas of social and communicate with social cognition and communication", "tokens": [50400, 467, 311, 586, 257, 8304, 412, 26711, 689, 415, 311, 415, 311, 10913, 322, 341, 733, 295, 3109, 294, 867, 3179, 295, 2093, 293, 7890, 365, 2093, 46905, 293, 6101, 50760], "temperature": 0.0, "avg_logprob": -0.18349403446003543, "compression_ratio": 1.7861842105263157, "no_speech_prob": 0.001366492360830307}, {"id": 406, "seek": 182456, "start": 1833.28, "end": 1834.8, "text": " in really interesting ways", "tokens": [50800, 294, 534, 1880, 2098, 50876], "temperature": 0.0, "avg_logprob": -0.18349403446003543, "compression_ratio": 1.7861842105263157, "no_speech_prob": 0.001366492360830307}, {"id": 407, "seek": 182456, "start": 1834.8, "end": 1836.8, "text": " And many other colleagues here", "tokens": [50876, 400, 867, 661, 7734, 510, 50976], "temperature": 0.0, "avg_logprob": -0.18349403446003543, "compression_ratio": 1.7861842105263157, "no_speech_prob": 0.001366492360830307}, {"id": 408, "seek": 182456, "start": 1837.44, "end": 1839.44, "text": " I and I should mention also though I didn't", "tokens": [51008, 286, 293, 286, 820, 2152, 611, 1673, 286, 994, 380, 51108], "temperature": 0.0, "avg_logprob": -0.18349403446003543, "compression_ratio": 1.7861842105263157, "no_speech_prob": 0.001366492360830307}, {"id": 409, "seek": 182456, "start": 1839.9199999999998, "end": 1843.04, "text": " I forgot to mention this before the intuitive physics work started in our group", "tokens": [51132, 286, 5298, 281, 2152, 341, 949, 264, 21769, 10649, 589, 1409, 294, 527, 1594, 51288], "temperature": 0.0, "avg_logprob": -0.18349403446003543, "compression_ratio": 1.7861842105263157, "no_speech_prob": 0.001366492360830307}, {"id": 410, "seek": 182456, "start": 1843.36, "end": 1846.56, "text": " More than 10 years ago with pete patalia and jess hamrick and others", "tokens": [51304, 5048, 813, 1266, 924, 2057, 365, 280, 3498, 1947, 15579, 293, 361, 442, 7852, 9323, 293, 2357, 51464], "temperature": 0.0, "avg_logprob": -0.18349403446003543, "compression_ratio": 1.7861842105263157, "no_speech_prob": 0.001366492360830307}, {"id": 411, "seek": 182456, "start": 1847.04, "end": 1853.36, "text": " And they are now actually both working at deep mind google deep mind working on various interesting kinds of deep learning approaches", "tokens": [51488, 400, 436, 366, 586, 767, 1293, 1364, 412, 2452, 1575, 20742, 2452, 1575, 1364, 322, 3683, 1880, 3685, 295, 2452, 2539, 11587, 51804], "temperature": 0.0, "avg_logprob": -0.18349403446003543, "compression_ratio": 1.7861842105263157, "no_speech_prob": 0.001366492360830307}, {"id": 412, "seek": 185336, "start": 1853.6, "end": 1860.0, "text": " For both intuitive physics and all sorts of real-world physics that matter like climate modeling. That's where pete's current focuses", "tokens": [50376, 1171, 1293, 21769, 10649, 293, 439, 7527, 295, 957, 12, 13217, 10649, 300, 1871, 411, 5659, 15983, 13, 663, 311, 689, 280, 3498, 311, 2190, 16109, 50696], "temperature": 0.0, "avg_logprob": -0.1082765635322122, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.0019865515641868114}, {"id": 413, "seek": 185336, "start": 1860.7199999999998, "end": 1863.76, "text": " I won't go into the details on the bayesian theory of mind", "tokens": [50732, 286, 1582, 380, 352, 666, 264, 4365, 322, 264, 13642, 42434, 5261, 295, 1575, 50884], "temperature": 0.0, "avg_logprob": -0.1082765635322122, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.0019865515641868114}, {"id": 414, "seek": 185336, "start": 1864.3999999999999, "end": 1868.1599999999999, "text": " But the basic idea is again, there's a program that now describes", "tokens": [50916, 583, 264, 3875, 1558, 307, 797, 11, 456, 311, 257, 1461, 300, 586, 15626, 51104], "temperature": 0.0, "avg_logprob": -0.1082765635322122, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.0019865515641868114}, {"id": 415, "seek": 185336, "start": 1868.56, "end": 1875.04, "text": " Not necessarily how actual minds and brains works, but our mental models of other minds how the actions we see agents take", "tokens": [51124, 1726, 4725, 577, 3539, 9634, 293, 15442, 1985, 11, 457, 527, 4973, 5245, 295, 661, 9634, 577, 264, 5909, 321, 536, 12554, 747, 51448], "temperature": 0.0, "avg_logprob": -0.1082765635322122, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.0019865515641868114}, {"id": 416, "seek": 185336, "start": 1875.6, "end": 1879.52, "text": " We interpret them as the effect of planning programs that take as input", "tokens": [51476, 492, 7302, 552, 382, 264, 1802, 295, 5038, 4268, 300, 747, 382, 4846, 51672], "temperature": 0.0, "avg_logprob": -0.1082765635322122, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.0019865515641868114}, {"id": 417, "seek": 187952, "start": 1880.24, "end": 1883.68, "text": " The our minds representations of their beliefs and desires", "tokens": [50400, 440, 527, 9634, 33358, 295, 641, 13585, 293, 18005, 50572], "temperature": 0.0, "avg_logprob": -0.22080613948680736, "compression_ratio": 1.7009966777408638, "no_speech_prob": 0.0019259543623775244}, {"id": 418, "seek": 187952, "start": 1884.08, "end": 1889.04, "text": " And by seeing how agents act and change the state of the world and modeling also their perception process", "tokens": [50592, 400, 538, 2577, 577, 12554, 605, 293, 1319, 264, 1785, 295, 264, 1002, 293, 15983, 611, 641, 12860, 1399, 50840], "temperature": 0.0, "avg_logprob": -0.22080613948680736, "compression_ratio": 1.7009966777408638, "no_speech_prob": 0.0019259543623775244}, {"id": 419, "seek": 187952, "start": 1889.04, "end": 1894.4, "text": " Which leads to belief formation and updating we can model many aspects of how people understand", "tokens": [50840, 3013, 6689, 281, 7107, 11723, 293, 25113, 321, 393, 2316, 867, 7270, 295, 577, 561, 1223, 51108], "temperature": 0.0, "avg_logprob": -0.22080613948680736, "compression_ratio": 1.7009966777408638, "no_speech_prob": 0.0019259543623775244}, {"id": 420, "seek": 187952, "start": 1895.52, "end": 1901.6, "text": " Other minds especially in what you might call these sort of physically grounded and perceptual scenarios the kinds that you can also", "tokens": [51164, 5358, 9634, 2318, 294, 437, 291, 1062, 818, 613, 1333, 295, 9762, 23535, 293, 43276, 901, 15077, 264, 3685, 300, 291, 393, 611, 51468], "temperature": 0.0, "avg_logprob": -0.22080613948680736, "compression_ratio": 1.7009966777408638, "no_speech_prob": 0.0019259543623775244}, {"id": 421, "seek": 187952, "start": 1901.6, "end": 1907.04, "text": " Study in young babies like in the experiments. I'm showing here from gargay chibra. Kylie hamlin paul bloom and others", "tokens": [51468, 27039, 294, 2037, 10917, 411, 294, 264, 12050, 13, 286, 478, 4099, 510, 490, 3691, 70, 320, 417, 897, 424, 13, 39424, 7852, 5045, 2502, 425, 26899, 293, 2357, 51740], "temperature": 0.0, "avg_logprob": -0.22080613948680736, "compression_ratio": 1.7009966777408638, "no_speech_prob": 0.0019259543623775244}, {"id": 422, "seek": 190704, "start": 1907.76, "end": 1911.04, "text": " Um the the intuitive psychology case is especially interesting", "tokens": [50400, 3301, 264, 264, 21769, 15105, 1389, 307, 2318, 1880, 50564], "temperature": 0.0, "avg_logprob": -0.23213340759277343, "compression_ratio": 1.6240601503759398, "no_speech_prob": 0.002549834782257676}, {"id": 423, "seek": 190704, "start": 1911.6, "end": 1918.72, "text": " Because in work that we've done like in this paper with uh, brendan lake tomer olemann sam gershman that helped to set some of the", "tokens": [50592, 1436, 294, 589, 300, 321, 600, 1096, 411, 294, 341, 3035, 365, 2232, 11, 1403, 273, 282, 11001, 281, 936, 277, 10386, 969, 3247, 290, 433, 71, 1601, 300, 4254, 281, 992, 512, 295, 264, 50948], "temperature": 0.0, "avg_logprob": -0.23213340759277343, "compression_ratio": 1.6240601503759398, "no_speech_prob": 0.002549834782257676}, {"id": 424, "seek": 190704, "start": 1918.72, "end": 1922.96, "text": " Current stage of debates between deep learning and more cognitive approaches to ai", "tokens": [50948, 15629, 3233, 295, 24203, 1296, 2452, 2539, 293, 544, 15605, 11587, 281, 9783, 51160], "temperature": 0.0, "avg_logprob": -0.23213340759277343, "compression_ratio": 1.6240601503759398, "no_speech_prob": 0.002549834782257676}, {"id": 425, "seek": 190704, "start": 1923.92, "end": 1927.52, "text": " Where we put about a challenge back in 2016 and 2017", "tokens": [51208, 2305, 321, 829, 466, 257, 3430, 646, 294, 6549, 293, 6591, 51388], "temperature": 0.0, "avg_logprob": -0.23213340759277343, "compression_ratio": 1.6240601503759398, "no_speech_prob": 0.002549834782257676}, {"id": 426, "seek": 190704, "start": 1929.04, "end": 1934.8799999999999, "text": " For the the deep learning ai world about how to think about the kinds of inductive biases mental model", "tokens": [51464, 1171, 264, 264, 2452, 2539, 9783, 1002, 466, 577, 281, 519, 466, 264, 3685, 295, 31612, 488, 32152, 4973, 2316, 51756], "temperature": 0.0, "avg_logprob": -0.23213340759277343, "compression_ratio": 1.6240601503759398, "no_speech_prob": 0.002549834782257676}, {"id": 427, "seek": 193488, "start": 1935.6000000000001, "end": 1939.6000000000001, "text": " Materials that seem to be built into human minds and the learning mechanisms that build on those", "tokens": [50400, 19188, 12356, 300, 1643, 281, 312, 3094, 666, 1952, 9634, 293, 264, 2539, 15902, 300, 1322, 322, 729, 50600], "temperature": 0.0, "avg_logprob": -0.13522312766627262, "compression_ratio": 1.8541666666666667, "no_speech_prob": 0.005380843300372362}, {"id": 428, "seek": 193488, "start": 1940.4, "end": 1947.2, "text": " We highlighted both intuitive physics and intuitive psychology in the famous sparks of agi paper showing again some of the most", "tokens": [50640, 492, 17173, 1293, 21769, 10649, 293, 21769, 15105, 294, 264, 4618, 44102, 295, 623, 72, 3035, 4099, 797, 512, 295, 264, 881, 50980], "temperature": 0.0, "avg_logprob": -0.13522312766627262, "compression_ratio": 1.8541666666666667, "no_speech_prob": 0.005380843300372362}, {"id": 429, "seek": 193488, "start": 1948.4, "end": 1952.88, "text": " earliest glimpse into some of the surprising things that gpd4 seemed to be able to do", "tokens": [51040, 20573, 25838, 666, 512, 295, 264, 8830, 721, 300, 290, 79, 67, 19, 6576, 281, 312, 1075, 281, 360, 51264], "temperature": 0.0, "avg_logprob": -0.13522312766627262, "compression_ratio": 1.8541666666666667, "no_speech_prob": 0.005380843300372362}, {"id": 430, "seek": 193488, "start": 1953.68, "end": 1960.0800000000002, "text": " bubeka and colleagues also highlighted intuitive physics and intuitive psychology and then in a recent paper from eric schultz's group", "tokens": [51304, 758, 650, 2330, 293, 7734, 611, 17173, 21769, 10649, 293, 21769, 15105, 293, 550, 294, 257, 5162, 3035, 490, 1189, 299, 956, 723, 89, 311, 1594, 51624], "temperature": 0.0, "avg_logprob": -0.13522312766627262, "compression_ratio": 1.8541666666666667, "no_speech_prob": 0.005380843300372362}, {"id": 431, "seek": 196008, "start": 1960.6399999999999, "end": 1967.52, "text": " Which is kind of forms a a three-part story here and it continues as as i know bushoff and and schultz and colleagues", "tokens": [50392, 3013, 307, 733, 295, 6422, 257, 257, 1045, 12, 6971, 1657, 510, 293, 309, 6515, 382, 382, 741, 458, 1255, 1289, 602, 293, 293, 956, 723, 89, 293, 7734, 50736], "temperature": 0.0, "avg_logprob": -0.1304576598991782, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.010811819694936275}, {"id": 432, "seek": 196008, "start": 1967.76, "end": 1970.0, "text": " Continue to work on this area and we also do", "tokens": [50748, 24472, 281, 589, 322, 341, 1859, 293, 321, 611, 360, 50860], "temperature": 0.0, "avg_logprob": -0.1304576598991782, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.010811819694936275}, {"id": 433, "seek": 196008, "start": 1970.56, "end": 1975.4399999999998, "text": " Is to now in the era of these multimodal language and vision models like i showed you at the beginning", "tokens": [50888, 1119, 281, 586, 294, 264, 4249, 295, 613, 32972, 378, 304, 2856, 293, 5201, 5245, 411, 741, 4712, 291, 412, 264, 2863, 51132], "temperature": 0.0, "avg_logprob": -0.1304576598991782, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.010811819694936275}, {"id": 434, "seek": 196008, "start": 1975.76, "end": 1982.6399999999999, "text": " We can go and take on all these tasks like whether it's block tower intuitive physics things or agents moving around in the world and interpreting", "tokens": [51148, 492, 393, 352, 293, 747, 322, 439, 613, 9608, 411, 1968, 309, 311, 3461, 10567, 21769, 10649, 721, 420, 12554, 2684, 926, 294, 264, 1002, 293, 37395, 51492], "temperature": 0.0, "avg_logprob": -0.1304576598991782, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.010811819694936275}, {"id": 435, "seek": 196008, "start": 1983.28, "end": 1987.76, "text": " As julia harr edinger called it the naive utility calculus of their rewards and costs", "tokens": [51524, 1018, 30764, 654, 2233, 81, 1257, 6911, 1219, 309, 264, 29052, 14877, 33400, 295, 641, 17203, 293, 5497, 51748], "temperature": 0.0, "avg_logprob": -0.1304576598991782, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.010811819694936275}, {"id": 436, "seek": 198776, "start": 1988.4, "end": 1993.76, "text": " And give language models or multimodal models the same kinds of stimuli and questions that we give people", "tokens": [50396, 400, 976, 2856, 5245, 420, 32972, 378, 304, 5245, 264, 912, 3685, 295, 47752, 293, 1651, 300, 321, 976, 561, 50664], "temperature": 0.0, "avg_logprob": -0.10713154679044672, "compression_ratio": 1.6884615384615385, "no_speech_prob": 0.0021815472282469273}, {"id": 437, "seek": 198776, "start": 1994.72, "end": 1996.72, "text": " And as schultz and colleagues showed", "tokens": [50712, 400, 382, 956, 723, 89, 293, 7734, 4712, 50812], "temperature": 0.0, "avg_logprob": -0.10713154679044672, "compression_ratio": 1.6884615384615385, "no_speech_prob": 0.0021815472282469273}, {"id": 438, "seek": 198776, "start": 1997.84, "end": 2001.52, "text": " In some of the intuitive physics settings, they're kind of okay. They're not great", "tokens": [50868, 682, 512, 295, 264, 21769, 10649, 6257, 11, 436, 434, 733, 295, 1392, 13, 814, 434, 406, 869, 51052], "temperature": 0.0, "avg_logprob": -0.10713154679044672, "compression_ratio": 1.6884615384615385, "no_speech_prob": 0.0021815472282469273}, {"id": 439, "seek": 198776, "start": 2002.8, "end": 2006.56, "text": " In the intuitive theory of mind ones they fail altogether. That's a quote from their paper", "tokens": [51116, 682, 264, 21769, 5261, 295, 1575, 2306, 436, 3061, 19051, 13, 663, 311, 257, 6513, 490, 641, 3035, 51304], "temperature": 0.0, "avg_logprob": -0.10713154679044672, "compression_ratio": 1.6884615384615385, "no_speech_prob": 0.0021815472282469273}, {"id": 440, "seek": 198776, "start": 2006.72, "end": 2013.6, "text": " So as an example, this is this is work that julia harr edinger did as part of his phd thesis with myself and laura schultz", "tokens": [51312, 407, 382, 364, 1365, 11, 341, 307, 341, 307, 589, 300, 30764, 654, 2233, 81, 1257, 6911, 630, 382, 644, 295, 702, 903, 67, 22288, 365, 2059, 293, 635, 2991, 956, 723, 89, 51656], "temperature": 0.0, "avg_logprob": -0.10713154679044672, "compression_ratio": 1.6884615384615385, "no_speech_prob": 0.0021815472282469273}, {"id": 441, "seek": 201360, "start": 2014.48, "end": 2020.6399999999999, "text": " Um where we call these the astronaut studies where people would see an agent an astronaut on some planet", "tokens": [50408, 3301, 689, 321, 818, 613, 264, 18516, 5313, 689, 561, 576, 536, 364, 9461, 364, 18516, 322, 512, 5054, 50716], "temperature": 0.0, "avg_logprob": -0.11810221899123419, "compression_ratio": 1.859375, "no_speech_prob": 0.007813681848347187}, {"id": 442, "seek": 201360, "start": 2020.8799999999999, "end": 2028.0, "text": " Who would start at a certain point and have a home base that they had to get to and they would follow some path along", "tokens": [50728, 2102, 576, 722, 412, 257, 1629, 935, 293, 362, 257, 1280, 3096, 300, 436, 632, 281, 483, 281, 293, 436, 576, 1524, 512, 3100, 2051, 51084], "temperature": 0.0, "avg_logprob": -0.11810221899123419, "compression_ratio": 1.859375, "no_speech_prob": 0.007813681848347187}, {"id": 443, "seek": 201360, "start": 2028.7199999999998, "end": 2030.7199999999998, "text": " This the surface of the planet", "tokens": [51120, 639, 264, 3753, 295, 264, 5054, 51220], "temperature": 0.0, "avg_logprob": -0.11810221899123419, "compression_ratio": 1.859375, "no_speech_prob": 0.007813681848347187}, {"id": 444, "seek": 201360, "start": 2030.7199999999998, "end": 2034.08, "text": " They could just go straight to their home base or they could take a different a not straight path", "tokens": [51220, 814, 727, 445, 352, 2997, 281, 641, 1280, 3096, 420, 436, 727, 747, 257, 819, 257, 406, 2997, 3100, 51388], "temperature": 0.0, "avg_logprob": -0.11810221899123419, "compression_ratio": 1.859375, "no_speech_prob": 0.007813681848347187}, {"id": 445, "seek": 201360, "start": 2034.6399999999999, "end": 2040.8799999999999, "text": " Um crucially there were various objects that they could pick up that could either be positively valuable or aversive to them", "tokens": [51416, 3301, 5140, 1909, 456, 645, 3683, 6565, 300, 436, 727, 1888, 493, 300, 727, 2139, 312, 25795, 8263, 420, 257, 840, 488, 281, 552, 51728], "temperature": 0.0, "avg_logprob": -0.11810221899123419, "compression_ratio": 1.859375, "no_speech_prob": 0.007813681848347187}, {"id": 446, "seek": 204088, "start": 2041.44, "end": 2043.44, "text": " And we asked people based on the path", "tokens": [50392, 400, 321, 2351, 561, 2361, 322, 264, 3100, 50492], "temperature": 0.0, "avg_logprob": -0.07234540554361606, "compression_ratio": 1.896678966789668, "no_speech_prob": 0.0038237699773162603}, {"id": 447, "seek": 204088, "start": 2043.44, "end": 2047.2, "text": " How likely do you think that the agent likes or doesn't like one of these objects?", "tokens": [50492, 1012, 3700, 360, 291, 519, 300, 264, 9461, 5902, 420, 1177, 380, 411, 472, 295, 613, 6565, 30, 50680], "temperature": 0.0, "avg_logprob": -0.07234540554361606, "compression_ratio": 1.896678966789668, "no_speech_prob": 0.0038237699773162603}, {"id": 448, "seek": 204088, "start": 2047.6000000000001, "end": 2051.04, "text": " And also there are different terrains and the terrains could be more or less costly", "tokens": [50700, 400, 611, 456, 366, 819, 26298, 1292, 293, 264, 26298, 1292, 727, 312, 544, 420, 1570, 28328, 50872], "temperature": 0.0, "avg_logprob": -0.07234540554361606, "compression_ratio": 1.896678966789668, "no_speech_prob": 0.0038237699773162603}, {"id": 449, "seek": 204088, "start": 2051.36, "end": 2056.56, "text": " And by showing people different maps different configurations of objects based terrain and different paths", "tokens": [50888, 400, 538, 4099, 561, 819, 11317, 819, 31493, 295, 6565, 2361, 17674, 293, 819, 14518, 51148], "temperature": 0.0, "avg_logprob": -0.07234540554361606, "compression_ratio": 1.896678966789668, "no_speech_prob": 0.0038237699773162603}, {"id": 450, "seek": 204088, "start": 2056.88, "end": 2062.88, "text": " You could get very interesting rich inferences about what about what the agent both wants and", "tokens": [51164, 509, 727, 483, 588, 1880, 4593, 13596, 2667, 466, 437, 466, 437, 264, 9461, 1293, 2738, 293, 51464], "temperature": 0.0, "avg_logprob": -0.07234540554361606, "compression_ratio": 1.896678966789668, "no_speech_prob": 0.0038237699773162603}, {"id": 451, "seek": 204088, "start": 2063.36, "end": 2067.6, "text": " What the rewards assigned to the different objects are as well as the costs for moving around on the terrain", "tokens": [51488, 708, 264, 17203, 13279, 281, 264, 819, 6565, 366, 382, 731, 382, 264, 5497, 337, 2684, 926, 322, 264, 17674, 51700], "temperature": 0.0, "avg_logprob": -0.07234540554361606, "compression_ratio": 1.896678966789668, "no_speech_prob": 0.0038237699773162603}, {"id": 452, "seek": 206760, "start": 2067.8399999999997, "end": 2070.56, "text": " So here's an example of some of the stimuli from one experiment", "tokens": [50376, 407, 510, 311, 364, 1365, 295, 512, 295, 264, 47752, 490, 472, 5120, 50512], "temperature": 0.0, "avg_logprob": -0.09043779626356817, "compression_ratio": 1.832116788321168, "no_speech_prob": 0.002251524245366454}, {"id": 453, "seek": 206760, "start": 2070.7999999999997, "end": 2073.92, "text": " Just to show you the kind of variation and then in each of these cases", "tokens": [50524, 1449, 281, 855, 291, 264, 733, 295, 12990, 293, 550, 294, 1184, 295, 613, 3331, 50680], "temperature": 0.0, "avg_logprob": -0.09043779626356817, "compression_ratio": 1.832116788321168, "no_speech_prob": 0.002251524245366454}, {"id": 454, "seek": 206760, "start": 2073.92, "end": 2078.48, "text": " We can ask people to make four judgments or three or four depending on how many kinds of terrain", "tokens": [50680, 492, 393, 1029, 561, 281, 652, 1451, 40337, 420, 1045, 420, 1451, 5413, 322, 577, 867, 3685, 295, 17674, 50908], "temperature": 0.0, "avg_logprob": -0.09043779626356817, "compression_ratio": 1.832116788321168, "no_speech_prob": 0.002251524245366454}, {"id": 455, "seek": 206760, "start": 2078.72, "end": 2082.48, "text": " And that's what's shown here. These are these are the actually the predictions of the model", "tokens": [50920, 400, 300, 311, 437, 311, 4898, 510, 13, 1981, 366, 613, 366, 264, 767, 264, 21264, 295, 264, 2316, 51108], "temperature": 0.0, "avg_logprob": -0.09043779626356817, "compression_ratio": 1.832116788321168, "no_speech_prob": 0.002251524245366454}, {"id": 456, "seek": 206760, "start": 2083.2, "end": 2088.96, "text": " Z scored for the relative cost for the different kinds of terrain and the relative value for the different kinds of objects", "tokens": [51144, 1176, 18139, 337, 264, 4972, 2063, 337, 264, 819, 3685, 295, 17674, 293, 264, 4972, 2158, 337, 264, 819, 3685, 295, 6565, 51432], "temperature": 0.0, "avg_logprob": -0.09043779626356817, "compression_ratio": 1.832116788321168, "no_speech_prob": 0.002251524245366454}, {"id": 457, "seek": 206760, "start": 2089.2, "end": 2091.52, "text": " And we're assuming that the agent basically takes in a", "tokens": [51444, 400, 321, 434, 11926, 300, 264, 9461, 1936, 2516, 294, 257, 51560], "temperature": 0.0, "avg_logprob": -0.09043779626356817, "compression_ratio": 1.832116788321168, "no_speech_prob": 0.002251524245366454}, {"id": 458, "seek": 209152, "start": 2092.16, "end": 2095.52, "text": " of rational efficient plan trying to maximize reward", "tokens": [50396, 295, 15090, 7148, 1393, 1382, 281, 19874, 7782, 50564], "temperature": 0.0, "avg_logprob": -0.1008029580116272, "compression_ratio": 1.6137184115523466, "no_speech_prob": 0.027578286826610565}, {"id": 459, "seek": 209152, "start": 2096.0, "end": 2101.28, "text": " Minus cost where there's a small cost for each step, but especially costly when you travel over certain kinds of terrain", "tokens": [50588, 2829, 301, 2063, 689, 456, 311, 257, 1359, 2063, 337, 1184, 1823, 11, 457, 2318, 28328, 562, 291, 3147, 670, 1629, 3685, 295, 17674, 50852], "temperature": 0.0, "avg_logprob": -0.1008029580116272, "compression_ratio": 1.6137184115523466, "no_speech_prob": 0.027578286826610565}, {"id": 460, "seek": 209152, "start": 2101.84, "end": 2106.56, "text": " So seeing the path you can make inferences about how the agent has rewards and costs", "tokens": [50880, 407, 2577, 264, 3100, 291, 393, 652, 13596, 2667, 466, 577, 264, 9461, 575, 17203, 293, 5497, 51116], "temperature": 0.0, "avg_logprob": -0.1008029580116272, "compression_ratio": 1.6137184115523466, "no_speech_prob": 0.027578286826610565}, {"id": 461, "seek": 209152, "start": 2106.88, "end": 2110.56, "text": " And then when you ask people to make the same judgments, they line up almost perfectly", "tokens": [51132, 400, 550, 562, 291, 1029, 561, 281, 652, 264, 912, 40337, 11, 436, 1622, 493, 1920, 6239, 51316], "temperature": 0.0, "avg_logprob": -0.1008029580116272, "compression_ratio": 1.6137184115523466, "no_speech_prob": 0.027578286826610565}, {"id": 462, "seek": 209152, "start": 2110.96, "end": 2116.08, "text": " This is just one of many experiments that julian did showing what is really a remarkable quantitative", "tokens": [51336, 639, 307, 445, 472, 295, 867, 12050, 300, 30764, 952, 630, 4099, 437, 307, 534, 257, 12802, 27778, 51592], "temperature": 0.0, "avg_logprob": -0.1008029580116272, "compression_ratio": 1.6137184115523466, "no_speech_prob": 0.027578286826610565}, {"id": 463, "seek": 211608, "start": 2116.56, "end": 2120.16, "text": " Match and but but by a model that isn't just fit to data", "tokens": [50388, 26178, 293, 457, 457, 538, 257, 2316, 300, 1943, 380, 445, 3318, 281, 1412, 50568], "temperature": 0.0, "avg_logprob": -0.15025748862876548, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.03160024434328079}, {"id": 464, "seek": 211608, "start": 2120.4, "end": 2122.72, "text": " There's a little bit of fitting but it's mostly based on", "tokens": [50580, 821, 311, 257, 707, 857, 295, 15669, 457, 309, 311, 5240, 2361, 322, 50696], "temperature": 0.0, "avg_logprob": -0.15025748862876548, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.03160024434328079}, {"id": 465, "seek": 211608, "start": 2123.12, "end": 2128.48, "text": " Thinking about what are the core concepts of theory of mind that people like I mentioned before like gargay and chibra", "tokens": [50716, 24460, 466, 437, 366, 264, 4965, 10392, 295, 5261, 295, 1575, 300, 561, 411, 286, 2835, 949, 411, 3691, 70, 320, 293, 417, 897, 424, 50984], "temperature": 0.0, "avg_logprob": -0.15025748862876548, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.03160024434328079}, {"id": 466, "seek": 211608, "start": 2128.64, "end": 2133.12, "text": " Have studied even in very young infants pre-verbal infants infants who can't even walk", "tokens": [50992, 3560, 9454, 754, 294, 588, 2037, 38829, 659, 12, 331, 2645, 38829, 38829, 567, 393, 380, 754, 1792, 51216], "temperature": 0.0, "avg_logprob": -0.15025748862876548, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.03160024434328079}, {"id": 467, "seek": 211608, "start": 2134.56, "end": 2137.84, "text": " And barely just able to reach for things themselves yet", "tokens": [51288, 400, 10268, 445, 1075, 281, 2524, 337, 721, 2969, 1939, 51452], "temperature": 0.0, "avg_logprob": -0.15025748862876548, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.03160024434328079}, {"id": 468, "seek": 211608, "start": 2137.84, "end": 2143.2799999999997, "text": " They still have these ideas these intuitions about efficient inference and and use those it seems", "tokens": [51452, 814, 920, 362, 613, 3487, 613, 16224, 626, 466, 7148, 38253, 293, 293, 764, 729, 309, 2544, 51724], "temperature": 0.0, "avg_logprob": -0.15025748862876548, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.03160024434328079}, {"id": 469, "seek": 214328, "start": 2144.1600000000003, "end": 2146.1600000000003, "text": " I mentioned sherry lou's work who was a", "tokens": [50408, 286, 2835, 402, 5318, 15185, 311, 589, 567, 390, 257, 50508], "temperature": 0.0, "avg_logprob": -0.15388037518757144, "compression_ratio": 1.6625, "no_speech_prob": 0.0015482819871976972}, {"id": 470, "seek": 214328, "start": 2146.48, "end": 2150.7200000000003, "text": " PhD student at harvard a few years ago with this velki who worked with tomer omen also in me", "tokens": [50524, 14476, 3107, 412, 2233, 11303, 257, 1326, 924, 2057, 365, 341, 14610, 2984, 567, 2732, 365, 281, 936, 277, 2558, 611, 294, 385, 50736], "temperature": 0.0, "avg_logprob": -0.15388037518757144, "compression_ratio": 1.6625, "no_speech_prob": 0.0015482819871976972}, {"id": 471, "seek": 214328, "start": 2151.0400000000004, "end": 2156.6400000000003, "text": " To show those same kinds of things work in babies sherry's now doing amazing stuff extending that in a new lab at johns hopkins", "tokens": [50752, 1407, 855, 729, 912, 3685, 295, 721, 589, 294, 10917, 402, 5318, 311, 586, 884, 2243, 1507, 24360, 300, 294, 257, 777, 2715, 412, 35097, 82, 3818, 10277, 51032], "temperature": 0.0, "avg_logprob": -0.15388037518757144, "compression_ratio": 1.6625, "no_speech_prob": 0.0015482819871976972}, {"id": 472, "seek": 214328, "start": 2157.28, "end": 2162.7200000000003, "text": " Um, so these these are cases where this kind of probabilistic program model works really well, but just a pure language model", "tokens": [51064, 3301, 11, 370, 613, 613, 366, 3331, 689, 341, 733, 295, 31959, 3142, 1461, 2316, 1985, 534, 731, 11, 457, 445, 257, 6075, 2856, 2316, 51336], "temperature": 0.0, "avg_logprob": -0.15388037518757144, "compression_ratio": 1.6625, "no_speech_prob": 0.0015482819871976972}, {"id": 473, "seek": 214328, "start": 2163.28, "end": 2169.28, "text": " Uh is basically at chance zero correlation with human judgments, although quite good at telling you things like the background color of the scene", "tokens": [51364, 4019, 307, 1936, 412, 2931, 4018, 20009, 365, 1952, 40337, 11, 4878, 1596, 665, 412, 3585, 291, 721, 411, 264, 3678, 2017, 295, 264, 4145, 51664], "temperature": 0.0, "avg_logprob": -0.15388037518757144, "compression_ratio": 1.6625, "no_speech_prob": 0.0015482819871976972}, {"id": 474, "seek": 216928, "start": 2170.0, "end": 2174.8, "text": " Okay, so in the last part of the talk having sort of set the stage for how are our", "tokens": [50400, 1033, 11, 370, 294, 264, 1036, 644, 295, 264, 751, 1419, 1333, 295, 992, 264, 3233, 337, 577, 366, 527, 50640], "temperature": 0.0, "avg_logprob": -0.13879105755101853, "compression_ratio": 1.6677966101694914, "no_speech_prob": 0.0008828922873362899}, {"id": 475, "seek": 216928, "start": 2175.6800000000003, "end": 2180.0800000000004, "text": " Human thinking seems to work and how we can model this in ways that are explanatory", "tokens": [50684, 10294, 1953, 2544, 281, 589, 293, 577, 321, 393, 2316, 341, 294, 2098, 300, 366, 9045, 4745, 50904], "temperature": 0.0, "avg_logprob": -0.13879105755101853, "compression_ratio": 1.6677966101694914, "no_speech_prob": 0.0008828922873362899}, {"id": 476, "seek": 216928, "start": 2180.7200000000003, "end": 2186.7200000000003, "text": " Understandable and quite quantitatively predictive using these probabilistic programs. Let's understand where language comes into picture", "tokens": [50936, 26093, 712, 293, 1596, 27778, 356, 35521, 1228, 613, 31959, 3142, 4268, 13, 961, 311, 1223, 689, 2856, 1487, 666, 3036, 51236], "temperature": 0.0, "avg_logprob": -0.13879105755101853, "compression_ratio": 1.6677966101694914, "no_speech_prob": 0.0008828922873362899}, {"id": 477, "seek": 216928, "start": 2187.2000000000003, "end": 2191.84, "text": " I don't think that a pure machine learning transformer approach like the ones we've been talking about", "tokens": [51260, 286, 500, 380, 519, 300, 257, 6075, 3479, 2539, 31782, 3109, 411, 264, 2306, 321, 600, 668, 1417, 466, 51492], "temperature": 0.0, "avg_logprob": -0.13879105755101853, "compression_ratio": 1.6677966101694914, "no_speech_prob": 0.0008828922873362899}, {"id": 478, "seek": 216928, "start": 2192.4, "end": 2197.2000000000003, "text": " Are on track to give a human level or certainly a human like account of how it works", "tokens": [51520, 2014, 322, 2837, 281, 976, 257, 1952, 1496, 420, 3297, 257, 1952, 411, 2696, 295, 577, 309, 1985, 51760], "temperature": 0.0, "avg_logprob": -0.13879105755101853, "compression_ratio": 1.6677966101694914, "no_speech_prob": 0.0008828922873362899}, {"id": 479, "seek": 219720, "start": 2197.4399999999996, "end": 2203.68, "text": " But I do think the ideas of sequence modeling statistical distribution learning that you see having", "tokens": [50376, 583, 286, 360, 519, 264, 3487, 295, 8310, 15983, 22820, 7316, 2539, 300, 291, 536, 1419, 50688], "temperature": 0.0, "avg_logprob": -0.1476966142654419, "compression_ratio": 1.7052631578947368, "no_speech_prob": 0.001133171608671546}, {"id": 480, "seek": 219720, "start": 2204.24, "end": 2206.24, "text": " such great success in llms", "tokens": [50716, 1270, 869, 2245, 294, 287, 75, 2592, 50816], "temperature": 0.0, "avg_logprob": -0.1476966142654419, "compression_ratio": 1.7052631578947368, "no_speech_prob": 0.001133171608671546}, {"id": 481, "seek": 219720, "start": 2206.56, "end": 2211.2799999999997, "text": " Even perhaps on a smaller scale could say something important about how language comes into the picture", "tokens": [50832, 2754, 4317, 322, 257, 4356, 4373, 727, 584, 746, 1021, 466, 577, 2856, 1487, 666, 264, 3036, 51068], "temperature": 0.0, "avg_logprob": -0.1476966142654419, "compression_ratio": 1.7052631578947368, "no_speech_prob": 0.001133171608671546}, {"id": 482, "seek": 219720, "start": 2211.9199999999996, "end": 2214.16, "text": " So this here is the at the beginning", "tokens": [51100, 407, 341, 510, 307, 264, 412, 264, 2863, 51212], "temperature": 0.0, "avg_logprob": -0.1476966142654419, "compression_ratio": 1.7052631578947368, "no_speech_prob": 0.001133171608671546}, {"id": 483, "seek": 219720, "start": 2214.16, "end": 2217.4399999999996, "text": " I talked about this archive paper the word models to world models paper", "tokens": [51212, 286, 2825, 466, 341, 23507, 3035, 264, 1349, 5245, 281, 1002, 5245, 3035, 51376], "temperature": 0.0, "avg_logprob": -0.1476966142654419, "compression_ratio": 1.7052631578947368, "no_speech_prob": 0.001133171608671546}, {"id": 484, "seek": 219720, "start": 2217.8399999999997, "end": 2224.08, "text": " From line along Gabe grand and colleagues and that's what i'm going to be talking about here is how we've been using bringing these tools together", "tokens": [51396, 3358, 1622, 2051, 39524, 2697, 293, 7734, 293, 300, 311, 437, 741, 478, 516, 281, 312, 1417, 466, 510, 307, 577, 321, 600, 668, 1228, 5062, 613, 3873, 1214, 51708], "temperature": 0.0, "avg_logprob": -0.1476966142654419, "compression_ratio": 1.7052631578947368, "no_speech_prob": 0.001133171608671546}, {"id": 485, "seek": 222408, "start": 2224.56, "end": 2229.2799999999997, "text": " But a key ingredient is this I would say the modern return of the language of thought hypothesis", "tokens": [50388, 583, 257, 2141, 14751, 307, 341, 286, 576, 584, 264, 4363, 2736, 295, 264, 2856, 295, 1194, 17291, 50624], "temperature": 0.0, "avg_logprob": -0.1374927541261078, "compression_ratio": 1.6090225563909775, "no_speech_prob": 0.002887960523366928}, {"id": 486, "seek": 222408, "start": 2229.2799999999997, "end": 2234.3199999999997, "text": " You know made famous by jerry fodor, but obviously with a history that goes back hundreds if not thousands of years", "tokens": [50624, 509, 458, 1027, 4618, 538, 361, 5318, 283, 34024, 11, 457, 2745, 365, 257, 2503, 300, 1709, 646, 6779, 498, 406, 5383, 295, 924, 50876], "temperature": 0.0, "avg_logprob": -0.1374927541261078, "compression_ratio": 1.6090225563909775, "no_speech_prob": 0.002887960523366928}, {"id": 487, "seek": 222408, "start": 2234.64, "end": 2236.64, "text": " But in the number of recent papers", "tokens": [50892, 583, 294, 264, 1230, 295, 5162, 10577, 50992], "temperature": 0.0, "avg_logprob": -0.1374927541261078, "compression_ratio": 1.6090225563909775, "no_speech_prob": 0.002887960523366928}, {"id": 488, "seek": 222408, "start": 2237.36, "end": 2241.68, "text": " And proposals from various groups the idea that again that there's some kind of", "tokens": [51028, 400, 20198, 490, 3683, 3935, 264, 1558, 300, 797, 300, 456, 311, 512, 733, 295, 51244], "temperature": 0.0, "avg_logprob": -0.1374927541261078, "compression_ratio": 1.6090225563909775, "no_speech_prob": 0.002887960523366928}, {"id": 489, "seek": 222408, "start": 2242.48, "end": 2247.84, "text": " Not necessarily single language, but but abstract symbolic languages, which could be general or also", "tokens": [51284, 1726, 4725, 2167, 2856, 11, 457, 457, 12649, 25755, 8650, 11, 597, 727, 312, 2674, 420, 611, 51552], "temperature": 0.0, "avg_logprob": -0.1374927541261078, "compression_ratio": 1.6090225563909775, "no_speech_prob": 0.002887960523366928}, {"id": 490, "seek": 224784, "start": 2248.32, "end": 2250.82, "text": " Created and constructed domain specific languages", "tokens": [50388, 11972, 292, 293, 17083, 9274, 2685, 8650, 50513], "temperature": 0.0, "avg_logprob": -0.12101625688005202, "compression_ratio": 1.8346456692913387, "no_speech_prob": 0.020324930548667908}, {"id": 491, "seek": 224784, "start": 2251.2000000000003, "end": 2257.6000000000004, "text": " Some kind of abstract symbolic language is seems to be a powerful way to think about human thinking abstraction", "tokens": [50532, 2188, 733, 295, 12649, 25755, 2856, 307, 2544, 281, 312, 257, 4005, 636, 281, 519, 466, 1952, 1953, 37765, 50852], "temperature": 0.0, "avg_logprob": -0.12101625688005202, "compression_ratio": 1.8346456692913387, "no_speech_prob": 0.020324930548667908}, {"id": 492, "seek": 224784, "start": 2257.84, "end": 2259.52, "text": " concept learning and so on", "tokens": [50864, 3410, 2539, 293, 370, 322, 50948], "temperature": 0.0, "avg_logprob": -0.12101625688005202, "compression_ratio": 1.8346456692913387, "no_speech_prob": 0.020324930548667908}, {"id": 493, "seek": 224784, "start": 2259.52, "end": 2262.48, "text": " And the particular kind of languages of thought that we've been thinking about are", "tokens": [50948, 400, 264, 1729, 733, 295, 8650, 295, 1194, 300, 321, 600, 668, 1953, 466, 366, 51096], "temperature": 0.0, "avg_logprob": -0.12101625688005202, "compression_ratio": 1.8346456692913387, "no_speech_prob": 0.020324930548667908}, {"id": 494, "seek": 224784, "start": 2263.04, "end": 2267.2000000000003, "text": " What noah goodman and toby gersenberg and I called the probabilistic language of thought", "tokens": [51124, 708, 572, 545, 665, 1601, 293, 281, 2322, 290, 433, 268, 6873, 293, 286, 1219, 264, 31959, 3142, 2856, 295, 1194, 51332], "temperature": 0.0, "avg_logprob": -0.12101625688005202, "compression_ratio": 1.8346456692913387, "no_speech_prob": 0.020324930548667908}, {"id": 495, "seek": 224784, "start": 2267.52, "end": 2271.44, "text": " So this is the idea of using probabilistic programming languages the technical tool", "tokens": [51348, 407, 341, 307, 264, 1558, 295, 1228, 31959, 3142, 9410, 8650, 264, 6191, 2290, 51544], "temperature": 0.0, "avg_logprob": -0.12101625688005202, "compression_ratio": 1.8346456692913387, "no_speech_prob": 0.020324930548667908}, {"id": 496, "seek": 224784, "start": 2271.52, "end": 2272.96, "text": " I talked about before", "tokens": [51548, 286, 2825, 466, 949, 51620], "temperature": 0.0, "avg_logprob": -0.12101625688005202, "compression_ratio": 1.8346456692913387, "no_speech_prob": 0.020324930548667908}, {"id": 497, "seek": 227296, "start": 2272.96, "end": 2278.16, "text": " To to formalize in a cognitive setting a certain kind of hypothesis of a language of thought", "tokens": [50364, 1407, 281, 9860, 1125, 294, 257, 15605, 3287, 257, 1629, 733, 295, 17291, 295, 257, 2856, 295, 1194, 50624], "temperature": 0.0, "avg_logprob": -0.08436350537161542, "compression_ratio": 1.79, "no_speech_prob": 0.009408427402377129}, {"id": 498, "seek": 227296, "start": 2278.4, "end": 2280.7200000000003, "text": " But one that is focused on modeling the world", "tokens": [50636, 583, 472, 300, 307, 5178, 322, 15983, 264, 1002, 50752], "temperature": 0.0, "avg_logprob": -0.08436350537161542, "compression_ratio": 1.79, "no_speech_prob": 0.009408427402377129}, {"id": 499, "seek": 227296, "start": 2281.12, "end": 2287.44, "text": " Not just possible worlds, but probable worlds and where the the symbolic language can also express ways of conditioning and query", "tokens": [50772, 1726, 445, 1944, 13401, 11, 457, 21759, 13401, 293, 689, 264, 264, 25755, 2856, 393, 611, 5109, 2098, 295, 21901, 293, 14581, 51088], "temperature": 0.0, "avg_logprob": -0.08436350537161542, "compression_ratio": 1.79, "no_speech_prob": 0.009408427402377129}, {"id": 500, "seek": 227296, "start": 2287.76, "end": 2291.36, "text": " So that we can ask and answer the kinds of questions that our minds do and that", "tokens": [51104, 407, 300, 321, 393, 1029, 293, 1867, 264, 3685, 295, 1651, 300, 527, 9634, 360, 293, 300, 51284], "temperature": 0.0, "avg_logprob": -0.08436350537161542, "compression_ratio": 1.79, "no_speech_prob": 0.009408427402377129}, {"id": 501, "seek": 227296, "start": 2291.68, "end": 2295.2, "text": " We might we might want for example any kind of general ai system to do", "tokens": [51300, 492, 1062, 321, 1062, 528, 337, 1365, 604, 733, 295, 2674, 9783, 1185, 281, 360, 51476], "temperature": 0.0, "avg_logprob": -0.08436350537161542, "compression_ratio": 1.79, "no_speech_prob": 0.009408427402377129}, {"id": 502, "seek": 227296, "start": 2295.52, "end": 2300.4, "text": " So if you want to learn more about the probabilistic language of thought check out our chapter in the conceptual mind", "tokens": [51492, 407, 498, 291, 528, 281, 1466, 544, 466, 264, 31959, 3142, 2856, 295, 1194, 1520, 484, 527, 7187, 294, 264, 24106, 1575, 51736], "temperature": 0.0, "avg_logprob": -0.08436350537161542, "compression_ratio": 1.79, "no_speech_prob": 0.009408427402377129}, {"id": 503, "seek": 230040, "start": 2300.8, "end": 2303.28, "text": " This is one of the margiles and laurence", "tokens": [50384, 639, 307, 472, 295, 264, 1849, 70, 4680, 293, 635, 9873, 384, 50508], "temperature": 0.0, "avg_logprob": -0.14683994576950704, "compression_ratio": 1.7508417508417509, "no_speech_prob": 0.012044643983244896}, {"id": 504, "seek": 230040, "start": 2304.1600000000003, "end": 2310.08, "text": " Readers or the web book probabilistic models of cognition, which which has examples of the kinds of models", "tokens": [50552, 1300, 15221, 420, 264, 3670, 1446, 31959, 3142, 5245, 295, 46905, 11, 597, 597, 575, 5110, 295, 264, 3685, 295, 5245, 50848], "temperature": 0.0, "avg_logprob": -0.14683994576950704, "compression_ratio": 1.7508417508417509, "no_speech_prob": 0.012044643983244896}, {"id": 505, "seek": 230040, "start": 2310.08, "end": 2311.52, "text": " I'm going to be talking about", "tokens": [50848, 286, 478, 516, 281, 312, 1417, 466, 50920], "temperature": 0.0, "avg_logprob": -0.14683994576950704, "compression_ratio": 1.7508417508417509, "no_speech_prob": 0.012044643983244896}, {"id": 506, "seek": 230040, "start": 2311.52, "end": 2316.96, "text": " Not with language models though the new thing is to take advantage of llms and specifically the fact that", "tokens": [50920, 1726, 365, 2856, 5245, 1673, 264, 777, 551, 307, 281, 747, 5002, 295, 287, 75, 2592, 293, 4682, 264, 1186, 300, 51192], "temperature": 0.0, "avg_logprob": -0.14683994576950704, "compression_ratio": 1.7508417508417509, "no_speech_prob": 0.012044643983244896}, {"id": 507, "seek": 230040, "start": 2317.2000000000003, "end": 2320.2400000000002, "text": " Most llms these days are trained not only on natural language", "tokens": [51204, 4534, 287, 75, 2592, 613, 1708, 366, 8895, 406, 787, 322, 3303, 2856, 51356], "temperature": 0.0, "avg_logprob": -0.14683994576950704, "compression_ratio": 1.7508417508417509, "no_speech_prob": 0.012044643983244896}, {"id": 508, "seek": 230040, "start": 2320.48, "end": 2323.2000000000003, "text": " But on on programming languages and source code", "tokens": [51368, 583, 322, 322, 9410, 8650, 293, 4009, 3089, 51504], "temperature": 0.0, "avg_logprob": -0.14683994576950704, "compression_ratio": 1.7508417508417509, "no_speech_prob": 0.012044643983244896}, {"id": 509, "seek": 230040, "start": 2323.6, "end": 2329.36, "text": " Namely programming languages like all the ones we're probably familiar with that are designed to be read and written by humans", "tokens": [51524, 10684, 736, 9410, 8650, 411, 439, 264, 2306, 321, 434, 1391, 4963, 365, 300, 366, 4761, 281, 312, 1401, 293, 3720, 538, 6255, 51812], "temperature": 0.0, "avg_logprob": -0.14683994576950704, "compression_ratio": 1.7508417508417509, "no_speech_prob": 0.012044643983244896}, {"id": 510, "seek": 232936, "start": 2329.44, "end": 2332.32, "text": " And not just machines and so they're written in a very english like way", "tokens": [50368, 400, 406, 445, 8379, 293, 370, 436, 434, 3720, 294, 257, 588, 32169, 411, 636, 50512], "temperature": 0.0, "avg_logprob": -0.133083885791255, "compression_ratio": 1.9446640316205535, "no_speech_prob": 0.010482427664101124}, {"id": 511, "seek": 232936, "start": 2332.88, "end": 2336.7200000000003, "text": " Or natural language like way, you know linguists have long pointed out", "tokens": [50540, 1610, 3303, 2856, 411, 636, 11, 291, 458, 21766, 1751, 362, 938, 10932, 484, 50732], "temperature": 0.0, "avg_logprob": -0.133083885791255, "compression_ratio": 1.9446640316205535, "no_speech_prob": 0.010482427664101124}, {"id": 512, "seek": 232936, "start": 2337.2000000000003, "end": 2342.6400000000003, "text": " Sometimes the differences between natural languages and programming languages, but from a certain perspective", "tokens": [50756, 4803, 264, 7300, 1296, 3303, 8650, 293, 9410, 8650, 11, 457, 490, 257, 1629, 4585, 51028], "temperature": 0.0, "avg_logprob": -0.133083885791255, "compression_ratio": 1.9446640316205535, "no_speech_prob": 0.010482427664101124}, {"id": 513, "seek": 232936, "start": 2342.88, "end": 2344.88, "text": " They're a lot more similar than they are different", "tokens": [51040, 814, 434, 257, 688, 544, 2531, 813, 436, 366, 819, 51140], "temperature": 0.0, "avg_logprob": -0.133083885791255, "compression_ratio": 1.9446640316205535, "no_speech_prob": 0.010482427664101124}, {"id": 514, "seek": 232936, "start": 2345.1200000000003, "end": 2350.32, "text": " A hierarchical structure syntax and even just much of the lexicon, you know natural languages", "tokens": [51152, 316, 35250, 804, 3877, 28431, 293, 754, 445, 709, 295, 264, 476, 87, 11911, 11, 291, 458, 3303, 8650, 51412], "temperature": 0.0, "avg_logprob": -0.133083885791255, "compression_ratio": 1.9446640316205535, "no_speech_prob": 0.010482427664101124}, {"id": 515, "seek": 232936, "start": 2350.4, "end": 2354.08, "text": " They're not just commented or programming languages are not just commented in natural language", "tokens": [51416, 814, 434, 406, 445, 26940, 420, 9410, 8650, 366, 406, 445, 26940, 294, 3303, 2856, 51600], "temperature": 0.0, "avg_logprob": -0.133083885791255, "compression_ratio": 1.9446640316205535, "no_speech_prob": 0.010482427664101124}, {"id": 516, "seek": 235408, "start": 2354.08, "end": 2358.88, "text": " But functions variables data structures are named using english for the most part", "tokens": [50364, 583, 6828, 9102, 1412, 9227, 366, 4926, 1228, 32169, 337, 264, 881, 644, 50604], "temperature": 0.0, "avg_logprob": -0.12156506588584498, "compression_ratio": 1.7397260273972603, "no_speech_prob": 0.007229967042803764}, {"id": 517, "seek": 235408, "start": 2358.96, "end": 2362.96, "text": " Okay, so that's a very powerful data source that allows", "tokens": [50608, 1033, 11, 370, 300, 311, 257, 588, 4005, 1412, 4009, 300, 4045, 50808], "temperature": 0.0, "avg_logprob": -0.12156506588584498, "compression_ratio": 1.7397260273972603, "no_speech_prob": 0.007229967042803764}, {"id": 518, "seek": 235408, "start": 2363.6, "end": 2371.68, "text": " Some kind of statistical sequence to sequence model for predicting and translating between streams sequential streams to effectively learn to translate", "tokens": [50840, 2188, 733, 295, 22820, 8310, 281, 8310, 2316, 337, 32884, 293, 35030, 1296, 15842, 42881, 15842, 281, 8659, 1466, 281, 13799, 51244], "temperature": 0.0, "avg_logprob": -0.12156506588584498, "compression_ratio": 1.7397260273972603, "no_speech_prob": 0.007229967042803764}, {"id": 519, "seek": 235408, "start": 2372.0, "end": 2377.7599999999998, "text": " From english or any other natural language into programming languages of thought and that includes probabilistic programming", "tokens": [51260, 3358, 32169, 420, 604, 661, 3303, 2856, 666, 9410, 8650, 295, 1194, 293, 300, 5974, 31959, 3142, 9410, 51548], "temperature": 0.0, "avg_logprob": -0.12156506588584498, "compression_ratio": 1.7397260273972603, "no_speech_prob": 0.007229967042803764}, {"id": 520, "seek": 235408, "start": 2378.16, "end": 2382.4, "text": " So the idea of this paper what we call rational meaning construction. That's the name of this", "tokens": [51568, 407, 264, 1558, 295, 341, 3035, 437, 321, 818, 15090, 3620, 6435, 13, 663, 311, 264, 1315, 295, 341, 51780], "temperature": 0.0, "avg_logprob": -0.12156506588584498, "compression_ratio": 1.7397260273972603, "no_speech_prob": 0.007229967042803764}, {"id": 521, "seek": 238240, "start": 2383.12, "end": 2389.28, "text": " Is is a particular thesis on how language is understood and perhaps also how it might be produced and learned", "tokens": [50400, 1119, 307, 257, 1729, 22288, 322, 577, 2856, 307, 7320, 293, 4317, 611, 577, 309, 1062, 312, 7126, 293, 3264, 50708], "temperature": 0.0, "avg_logprob": -0.12489821532062281, "compression_ratio": 1.8581818181818182, "no_speech_prob": 0.006384408567100763}, {"id": 522, "seek": 238240, "start": 2389.36, "end": 2391.6800000000003, "text": " Although our focus here is on language understanding", "tokens": [50712, 5780, 527, 1879, 510, 307, 322, 2856, 3701, 50828], "temperature": 0.0, "avg_logprob": -0.12489821532062281, "compression_ratio": 1.8581818181818182, "no_speech_prob": 0.006384408567100763}, {"id": 523, "seek": 238240, "start": 2392.4, "end": 2396.4, "text": " And thinking about the relation between language of language and thought in that context", "tokens": [50864, 400, 1953, 466, 264, 9721, 1296, 2856, 295, 2856, 293, 1194, 294, 300, 4319, 51064], "temperature": 0.0, "avg_logprob": -0.12489821532062281, "compression_ratio": 1.8581818181818182, "no_speech_prob": 0.006384408567100763}, {"id": 524, "seek": 238240, "start": 2397.04, "end": 2403.28, "text": " By thinking what we think of as like the core original notion of thinking is what I've been talking about for the for the first part of the talk", "tokens": [51096, 3146, 1953, 437, 321, 519, 295, 382, 411, 264, 4965, 3380, 10710, 295, 1953, 307, 437, 286, 600, 668, 1417, 466, 337, 264, 337, 264, 700, 644, 295, 264, 751, 51408], "temperature": 0.0, "avg_logprob": -0.12489821532062281, "compression_ratio": 1.8581818181818182, "no_speech_prob": 0.006384408567100763}, {"id": 525, "seek": 238240, "start": 2403.52, "end": 2405.52, "text": " Which is the idea of having a", "tokens": [51420, 3013, 307, 264, 1558, 295, 1419, 257, 51520], "temperature": 0.0, "avg_logprob": -0.12489821532062281, "compression_ratio": 1.8581818181818182, "no_speech_prob": 0.006384408567100763}, {"id": 526, "seek": 238240, "start": 2406.08, "end": 2410.2400000000002, "text": " Structured probabilistic model of the world conditioning it on observations and then", "tokens": [51548, 745, 46847, 31959, 3142, 2316, 295, 264, 1002, 21901, 309, 322, 18163, 293, 550, 51756], "temperature": 0.0, "avg_logprob": -0.12489821532062281, "compression_ratio": 1.8581818181818182, "no_speech_prob": 0.006384408567100763}, {"id": 527, "seek": 241024, "start": 2410.7999999999997, "end": 2414.9599999999996, "text": " And then drawing samples of underlying latent states and future states", "tokens": [50392, 400, 550, 6316, 10938, 295, 14217, 48994, 4368, 293, 2027, 4368, 50600], "temperature": 0.0, "avg_logprob": -0.11383237838745117, "compression_ratio": 1.8106060606060606, "no_speech_prob": 0.0009397965623065829}, {"id": 528, "seek": 241024, "start": 2415.4399999999996, "end": 2417.4399999999996, "text": " Okay, that's thinking from this standpoint", "tokens": [50624, 1033, 11, 300, 311, 1953, 490, 341, 15827, 50724], "temperature": 0.0, "avg_logprob": -0.11383237838745117, "compression_ratio": 1.8106060606060606, "no_speech_prob": 0.0009397965623065829}, {"id": 529, "seek": 241024, "start": 2417.68, "end": 2421.7599999999998, "text": " And then understanding language is effectively translating from natural language", "tokens": [50736, 400, 550, 3701, 2856, 307, 8659, 35030, 490, 3303, 2856, 50940], "temperature": 0.0, "avg_logprob": -0.11383237838745117, "compression_ratio": 1.8106060606060606, "no_speech_prob": 0.0009397965623065829}, {"id": 530, "seek": 241024, "start": 2422.08, "end": 2427.6, "text": " Into a probabilistic language of thought that's used to define and condition and query the probabilistic world model", "tokens": [50956, 23373, 257, 31959, 3142, 2856, 295, 1194, 300, 311, 1143, 281, 6964, 293, 4188, 293, 14581, 264, 31959, 3142, 1002, 2316, 51232], "temperature": 0.0, "avg_logprob": -0.11383237838745117, "compression_ratio": 1.8106060606060606, "no_speech_prob": 0.0009397965623065829}, {"id": 531, "seek": 241024, "start": 2428.0, "end": 2430.7999999999997, "text": " So we're going to exploit the the property of these", "tokens": [51252, 407, 321, 434, 516, 281, 25924, 264, 264, 4707, 295, 613, 51392], "temperature": 0.0, "avg_logprob": -0.11383237838745117, "compression_ratio": 1.8106060606060606, "no_speech_prob": 0.0009397965623065829}, {"id": 532, "seek": 241024, "start": 2431.2799999999997, "end": 2436.72, "text": " LLMs to translate from natural language to code in a way that might be familiar if you've tried using them to code", "tokens": [51416, 441, 43, 26386, 281, 13799, 490, 3303, 2856, 281, 3089, 294, 257, 636, 300, 1062, 312, 4963, 498, 291, 600, 3031, 1228, 552, 281, 3089, 51688], "temperature": 0.0, "avg_logprob": -0.11383237838745117, "compression_ratio": 1.8106060606060606, "no_speech_prob": 0.0009397965623065829}, {"id": 533, "seek": 243672, "start": 2436.72, "end": 2439.04, "text": " But it's different in some key ways in particular", "tokens": [50364, 583, 309, 311, 819, 294, 512, 2141, 2098, 294, 1729, 50480], "temperature": 0.0, "avg_logprob": -0.079811886624173, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.006485401187092066}, {"id": 534, "seek": 243672, "start": 2439.12, "end": 2443.52, "text": " We're not going to be just asking our LLM to write a whole bunch of code at least not to start", "tokens": [50484, 492, 434, 406, 516, 281, 312, 445, 3365, 527, 441, 43, 44, 281, 2464, 257, 1379, 3840, 295, 3089, 412, 1935, 406, 281, 722, 50704], "temperature": 0.0, "avg_logprob": -0.079811886624173, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.006485401187092066}, {"id": 535, "seek": 243672, "start": 2443.68, "end": 2447.52, "text": " But we're going to be focusing on the sentence level and what is meaning at the sentence level", "tokens": [50712, 583, 321, 434, 516, 281, 312, 8416, 322, 264, 8174, 1496, 293, 437, 307, 3620, 412, 264, 8174, 1496, 50904], "temperature": 0.0, "avg_logprob": -0.079811886624173, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.006485401187092066}, {"id": 536, "seek": 243672, "start": 2447.8399999999997, "end": 2452.48, "text": " It's something like trying to infer a line of code in a mental programming language", "tokens": [50920, 467, 311, 746, 411, 1382, 281, 13596, 257, 1622, 295, 3089, 294, 257, 4973, 9410, 2856, 51152], "temperature": 0.0, "avg_logprob": -0.079811886624173, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.006485401187092066}, {"id": 537, "seek": 243672, "start": 2452.7999999999997, "end": 2454.7999999999997, "text": " that is your best understanding of", "tokens": [51168, 300, 307, 428, 1151, 3701, 295, 51268], "temperature": 0.0, "avg_logprob": -0.079811886624173, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.006485401187092066}, {"id": 538, "seek": 243672, "start": 2455.2799999999997, "end": 2461.04, "text": " The meaning by which we just mean the thought that the person who's uttering that sentence is trying to convey", "tokens": [51292, 440, 3620, 538, 597, 321, 445, 914, 264, 1194, 300, 264, 954, 567, 311, 17567, 278, 300, 8174, 307, 1382, 281, 16965, 51580], "temperature": 0.0, "avg_logprob": -0.079811886624173, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.006485401187092066}, {"id": 539, "seek": 246104, "start": 2461.84, "end": 2466.96, "text": " Okay, and the the LLMs in this case can represent that meaning construction function", "tokens": [50404, 1033, 11, 293, 264, 264, 441, 43, 26386, 294, 341, 1389, 393, 2906, 300, 3620, 6435, 2445, 50660], "temperature": 0.0, "avg_logprob": -0.13362159128264178, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.005640178918838501}, {"id": 540, "seek": 246104, "start": 2467.2799999999997, "end": 2469.44, "text": " Okay in ways that have some very interesting properties", "tokens": [50676, 1033, 294, 2098, 300, 362, 512, 588, 1880, 7221, 50784], "temperature": 0.0, "avg_logprob": -0.13362159128264178, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.005640178918838501}, {"id": 541, "seek": 246104, "start": 2469.68, "end": 2475.7599999999998, "text": " So i'll just illustrate this with a classic example from the probabilistic language of thought literature the so-called Bayesian tug-of-war", "tokens": [50796, 407, 741, 603, 445, 23221, 341, 365, 257, 7230, 1365, 490, 264, 31959, 3142, 2856, 295, 1194, 10394, 264, 370, 12, 11880, 7840, 42434, 33543, 12, 2670, 12, 6925, 51100], "temperature": 0.0, "avg_logprob": -0.13362159128264178, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.005640178918838501}, {"id": 542, "seek": 246104, "start": 2476.16, "end": 2479.36, "text": " So just to ground this imagine that you're reasoning about a situation", "tokens": [51120, 407, 445, 281, 2727, 341, 3811, 300, 291, 434, 21577, 466, 257, 2590, 51280], "temperature": 0.0, "avg_logprob": -0.13362159128264178, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.005640178918838501}, {"id": 543, "seek": 246104, "start": 2480.16, "end": 2481.68, "text": " Like for example, uh", "tokens": [51320, 1743, 337, 1365, 11, 2232, 51396], "temperature": 0.0, "avg_logprob": -0.13362159128264178, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.005640178918838501}, {"id": 544, "seek": 246104, "start": 2481.68, "end": 2486.96, "text": " Various games of tug-of-war various people symbolized by colored shirts here are facing off against each other", "tokens": [51396, 14662, 851, 2813, 295, 33543, 12, 2670, 12, 6925, 3683, 561, 5986, 1602, 538, 14332, 20832, 510, 366, 7170, 766, 1970, 1184, 661, 51660], "temperature": 0.0, "avg_logprob": -0.13362159128264178, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.005640178918838501}, {"id": 545, "seek": 246104, "start": 2487.2, "end": 2489.7599999999998, "text": " And let's just take one person. Let's just say this is a", "tokens": [51672, 400, 718, 311, 445, 747, 472, 954, 13, 961, 311, 445, 584, 341, 307, 257, 51800], "temperature": 0.0, "avg_logprob": -0.13362159128264178, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.005640178918838501}, {"id": 546, "seek": 248976, "start": 2490.6400000000003, "end": 2494.88, "text": " Guy named jack and you might say well, how strong do you think jack is I haven't shown you any information?", "tokens": [50408, 14690, 4926, 7109, 293, 291, 1062, 584, 731, 11, 577, 2068, 360, 291, 519, 7109, 307, 286, 2378, 380, 4898, 291, 604, 1589, 30, 50620], "temperature": 0.0, "avg_logprob": -0.12807443126147947, "compression_ratio": 1.7938461538461539, "no_speech_prob": 0.002980301156640053}, {"id": 547, "seek": 248976, "start": 2494.96, "end": 2496.96, "text": " So your best guess might be just average. Okay", "tokens": [50624, 407, 428, 1151, 2041, 1062, 312, 445, 4274, 13, 1033, 50724], "temperature": 0.0, "avg_logprob": -0.12807443126147947, "compression_ratio": 1.7938461538461539, "no_speech_prob": 0.002980301156640053}, {"id": 548, "seek": 248976, "start": 2498.2400000000002, "end": 2500.96, "text": " Now, uh, suppose I give you some other information", "tokens": [50788, 823, 11, 2232, 11, 7297, 286, 976, 291, 512, 661, 1589, 50924], "temperature": 0.0, "avg_logprob": -0.12807443126147947, "compression_ratio": 1.7938461538461539, "no_speech_prob": 0.002980301156640053}, {"id": 549, "seek": 248976, "start": 2502.32, "end": 2505.6800000000003, "text": " Like um, or I should say here. So here are a few samples, you know, I might say average", "tokens": [50992, 1743, 1105, 11, 420, 286, 820, 584, 510, 13, 407, 510, 366, 257, 1326, 10938, 11, 291, 458, 11, 286, 1062, 584, 4274, 51160], "temperature": 0.0, "avg_logprob": -0.12807443126147947, "compression_ratio": 1.7938461538461539, "no_speech_prob": 0.002980301156640053}, {"id": 550, "seek": 248976, "start": 2505.6800000000003, "end": 2507.5200000000004, "text": " Maybe it's a little less than average", "tokens": [51160, 2704, 309, 311, 257, 707, 1570, 813, 4274, 51252], "temperature": 0.0, "avg_logprob": -0.12807443126147947, "compression_ratio": 1.7938461538461539, "no_speech_prob": 0.002980301156640053}, {"id": 551, "seek": 248976, "start": 2507.5200000000004, "end": 2510.32, "text": " If we're talking about MIT students, you may just pick the MIT average", "tokens": [51252, 759, 321, 434, 1417, 466, 13100, 1731, 11, 291, 815, 445, 1888, 264, 13100, 4274, 51392], "temperature": 0.0, "avg_logprob": -0.12807443126147947, "compression_ratio": 1.7938461538461539, "no_speech_prob": 0.002980301156640053}, {"id": 552, "seek": 248976, "start": 2510.48, "end": 2514.96, "text": " But now I could give more information like suppose, you know that jack beat leo in a game of tug-of-war", "tokens": [51400, 583, 586, 286, 727, 976, 544, 1589, 411, 7297, 11, 291, 458, 300, 7109, 4224, 476, 78, 294, 257, 1216, 295, 33543, 12, 2670, 12, 6925, 51624], "temperature": 0.0, "avg_logprob": -0.12807443126147947, "compression_ratio": 1.7938461538461539, "no_speech_prob": 0.002980301156640053}, {"id": 553, "seek": 248976, "start": 2515.2000000000003, "end": 2518.32, "text": " So that might move your arrow up from the MIT average because well, you know", "tokens": [51636, 407, 300, 1062, 1286, 428, 11610, 493, 490, 264, 13100, 4274, 570, 731, 11, 291, 458, 51792], "temperature": 0.0, "avg_logprob": -0.12807443126147947, "compression_ratio": 1.7938461538461539, "no_speech_prob": 0.002980301156640053}, {"id": 554, "seek": 251832, "start": 2519.04, "end": 2523.44, "text": " People who are who are stronger might be more likely to win than people who are weaker somehow. Okay", "tokens": [50400, 3432, 567, 366, 567, 366, 7249, 1062, 312, 544, 3700, 281, 1942, 813, 561, 567, 366, 24286, 6063, 13, 1033, 50620], "temperature": 0.0, "avg_logprob": -0.09466526135295426, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0012841533171012998}, {"id": 555, "seek": 251832, "start": 2524.0, "end": 2526.0, "text": " I could give you some more information", "tokens": [50648, 286, 727, 976, 291, 512, 544, 1589, 50748], "temperature": 0.0, "avg_logprob": -0.09466526135295426, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0012841533171012998}, {"id": 556, "seek": 251832, "start": 2526.0, "end": 2531.28, "text": " Like leo had just won 10 previous matches as well. So that means you might think leo's pretty strong and jack must be even stronger", "tokens": [50748, 1743, 476, 78, 632, 445, 1582, 1266, 3894, 10676, 382, 731, 13, 407, 300, 1355, 291, 1062, 519, 476, 78, 311, 1238, 2068, 293, 7109, 1633, 312, 754, 7249, 51012], "temperature": 0.0, "avg_logprob": -0.09466526135295426, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0012841533171012998}, {"id": 557, "seek": 251832, "start": 2531.28, "end": 2532.8, "text": " So your arrow goes way up", "tokens": [51012, 407, 428, 11610, 1709, 636, 493, 51088], "temperature": 0.0, "avg_logprob": -0.09466526135295426, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0012841533171012998}, {"id": 558, "seek": 251832, "start": 2532.8, "end": 2536.0800000000004, "text": " But suppose I told you well leo sometimes it doesn't pull as hard as they really could", "tokens": [51088, 583, 7297, 286, 1907, 291, 731, 476, 78, 2171, 309, 1177, 380, 2235, 382, 1152, 382, 436, 534, 727, 51252], "temperature": 0.0, "avg_logprob": -0.09466526135295426, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0012841533171012998}, {"id": 559, "seek": 251832, "start": 2536.32, "end": 2541.76, "text": " So maybe maybe leo was just getting a little lazy when leo faced jack. So it might go down a little bit", "tokens": [51264, 407, 1310, 1310, 476, 78, 390, 445, 1242, 257, 707, 14847, 562, 476, 78, 11446, 7109, 13, 407, 309, 1062, 352, 760, 257, 707, 857, 51536], "temperature": 0.0, "avg_logprob": -0.09466526135295426, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0012841533171012998}, {"id": 560, "seek": 251832, "start": 2542.4, "end": 2547.2000000000003, "text": " But then leo single-handedly beat a team with jack and tom on it. Okay. So now you think okay", "tokens": [51568, 583, 550, 476, 78, 2167, 12, 5543, 13516, 4224, 257, 1469, 365, 7109, 293, 2916, 322, 309, 13, 1033, 13, 407, 586, 291, 519, 1392, 51808], "temperature": 0.0, "avg_logprob": -0.09466526135295426, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0012841533171012998}, {"id": 561, "seek": 254720, "start": 2547.2, "end": 2551.3599999999997, "text": " Well leo was probably lazy before jack. Maybe isn't that strong", "tokens": [50364, 1042, 476, 78, 390, 1391, 14847, 949, 7109, 13, 2704, 1943, 380, 300, 2068, 50572], "temperature": 0.0, "avg_logprob": -0.12281759031887712, "compression_ratio": 1.6677524429967427, "no_speech_prob": 0.0007553163450211287}, {"id": 562, "seek": 254720, "start": 2552.24, "end": 2555.4399999999996, "text": " Since leo when they wanted to could beat both jack and tom. Okay", "tokens": [50616, 4162, 476, 78, 562, 436, 1415, 281, 727, 4224, 1293, 7109, 293, 2916, 13, 1033, 50776], "temperature": 0.0, "avg_logprob": -0.12281759031887712, "compression_ratio": 1.6677524429967427, "no_speech_prob": 0.0007553163450211287}, {"id": 563, "seek": 254720, "start": 2555.4399999999996, "end": 2562.0, "text": " So the point is in a classic example of non-monotonic reasoning your inference about this one aspect of the world jack strength", "tokens": [50776, 407, 264, 935, 307, 294, 257, 7230, 1365, 295, 2107, 12, 3317, 310, 11630, 21577, 428, 38253, 466, 341, 472, 4171, 295, 264, 1002, 7109, 3800, 51104], "temperature": 0.0, "avg_logprob": -0.12281759031887712, "compression_ratio": 1.6677524429967427, "no_speech_prob": 0.0007553163450211287}, {"id": 564, "seek": 254720, "start": 2562.0, "end": 2568.3999999999996, "text": " It's going up and down as you get various information. So we'd like to understand how that thinking process works how", "tokens": [51104, 467, 311, 516, 493, 293, 760, 382, 291, 483, 3683, 1589, 13, 407, 321, 1116, 411, 281, 1223, 577, 300, 1953, 1399, 1985, 577, 51424], "temperature": 0.0, "avg_logprob": -0.12281759031887712, "compression_ratio": 1.6677524429967427, "no_speech_prob": 0.0007553163450211287}, {"id": 565, "seek": 254720, "start": 2569.2799999999997, "end": 2576.3199999999997, "text": " Updating your beliefs based on linguistic evidence linguistically expressed evidence works. Okay, that's the starting point of this paper", "tokens": [51468, 5858, 67, 990, 428, 13585, 2361, 322, 43002, 4467, 21766, 20458, 12675, 4467, 1985, 13, 1033, 11, 300, 311, 264, 2891, 935, 295, 341, 3035, 51820], "temperature": 0.0, "avg_logprob": -0.12281759031887712, "compression_ratio": 1.6677524429967427, "no_speech_prob": 0.0007553163450211287}, {"id": 566, "seek": 257632, "start": 2576.88, "end": 2578.8, "text": " And the idea is to use in this case", "tokens": [50392, 400, 264, 1558, 307, 281, 764, 294, 341, 1389, 50488], "temperature": 0.0, "avg_logprob": -0.15294198122891514, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.0009108266094699502}, {"id": 567, "seek": 257632, "start": 2578.8, "end": 2584.4, "text": " We're using the the probabilistic programming language church which is based on a dialect of lisp or scheme", "tokens": [50488, 492, 434, 1228, 264, 264, 31959, 3142, 9410, 2856, 4128, 597, 307, 2361, 322, 257, 24652, 295, 287, 7631, 420, 12232, 50768], "temperature": 0.0, "avg_logprob": -0.15294198122891514, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.0009108266094699502}, {"id": 568, "seek": 257632, "start": 2584.6400000000003, "end": 2587.36, "text": " So there's a lot of parentheses and i'm not going to be able to unpack all the language", "tokens": [50780, 407, 456, 311, 257, 688, 295, 34153, 293, 741, 478, 406, 516, 281, 312, 1075, 281, 26699, 439, 264, 2856, 50916], "temperature": 0.0, "avg_logprob": -0.15294198122891514, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.0009108266094699502}, {"id": 569, "seek": 257632, "start": 2587.44, "end": 2592.48, "text": " But hopefully you can get the basic idea that we write we define functions that describe", "tokens": [50920, 583, 4696, 291, 393, 483, 264, 3875, 1558, 300, 321, 2464, 321, 6964, 6828, 300, 6786, 51172], "temperature": 0.0, "avg_logprob": -0.15294198122891514, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.0009108266094699502}, {"id": 570, "seek": 257632, "start": 2593.04, "end": 2595.04, "text": " probabilistic distributions on", "tokens": [51200, 31959, 3142, 37870, 322, 51300], "temperature": 0.0, "avg_logprob": -0.15294198122891514, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.0009108266094699502}, {"id": 571, "seek": 257632, "start": 2595.52, "end": 2601.6000000000004, "text": " Strength laziness and so on all work through this and that's you know, that's been that's well-attested work", "tokens": [51324, 39251, 19320, 1324, 293, 370, 322, 439, 589, 807, 341, 293, 300, 311, 291, 458, 11, 300, 311, 668, 300, 311, 731, 12, 1591, 21885, 589, 51628], "temperature": 0.0, "avg_logprob": -0.15294198122891514, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.0009108266094699502}, {"id": 572, "seek": 260160, "start": 2601.6, "end": 2607.92, "text": " That's been very good. You know, it's basically the the general toolkit for doing the kind of probabilistic intuitive", "tokens": [50364, 663, 311, 668, 588, 665, 13, 509, 458, 11, 309, 311, 1936, 264, 264, 2674, 40167, 337, 884, 264, 733, 295, 31959, 3142, 21769, 50680], "temperature": 0.0, "avg_logprob": -0.12519151736528444, "compression_ratio": 1.7119205298013245, "no_speech_prob": 0.015658345073461533}, {"id": 573, "seek": 260160, "start": 2608.56, "end": 2614.0, "text": " Mental modeling that I showed you with two to physics and intuitive psychology, but here we're doing it for this kind of novel domain", "tokens": [50712, 30294, 15983, 300, 286, 4712, 291, 365, 732, 281, 10649, 293, 21769, 15105, 11, 457, 510, 321, 434, 884, 309, 337, 341, 733, 295, 7613, 9274, 50984], "temperature": 0.0, "avg_logprob": -0.12519151736528444, "compression_ratio": 1.7119205298013245, "no_speech_prob": 0.015658345073461533}, {"id": 574, "seek": 260160, "start": 2614.0, "end": 2617.52, "text": " It's not there's no core domain that infants are born with for tug of war", "tokens": [50984, 467, 311, 406, 456, 311, 572, 4965, 9274, 300, 38829, 366, 4232, 365, 337, 33543, 295, 1516, 51160], "temperature": 0.0, "avg_logprob": -0.12519151736528444, "compression_ratio": 1.7119205298013245, "no_speech_prob": 0.015658345073461533}, {"id": 575, "seek": 260160, "start": 2617.68, "end": 2621.8399999999997, "text": " But we can write a model like this to capture what adults in our culture at least might think", "tokens": [51168, 583, 321, 393, 2464, 257, 2316, 411, 341, 281, 7983, 437, 8865, 294, 527, 3713, 412, 1935, 1062, 519, 51376], "temperature": 0.0, "avg_logprob": -0.12519151736528444, "compression_ratio": 1.7119205298013245, "no_speech_prob": 0.015658345073461533}, {"id": 576, "seek": 260160, "start": 2622.24, "end": 2628.4, "text": " And then the key new thing here is to understand how to to ground language in these mental models", "tokens": [51396, 400, 550, 264, 2141, 777, 551, 510, 307, 281, 1223, 577, 281, 281, 2727, 2856, 294, 613, 4973, 5245, 51704], "temperature": 0.0, "avg_logprob": -0.12519151736528444, "compression_ratio": 1.7119205298013245, "no_speech_prob": 0.015658345073461533}, {"id": 577, "seek": 262840, "start": 2628.88, "end": 2635.44, "text": " So this is where we're going to model the translation of a statement in english like jack one against leo or a question", "tokens": [50388, 407, 341, 307, 689, 321, 434, 516, 281, 2316, 264, 12853, 295, 257, 5629, 294, 32169, 411, 7109, 472, 1970, 476, 78, 420, 257, 1168, 50716], "temperature": 0.0, "avg_logprob": -0.16442136142564856, "compression_ratio": 1.6221374045801527, "no_speech_prob": 0.001926146331243217}, {"id": 578, "seek": 262840, "start": 2635.52, "end": 2642.56, "text": " Like how strong is jack into lines of code that support probabilistic updating and querying in this language", "tokens": [50720, 1743, 577, 2068, 307, 7109, 666, 3876, 295, 3089, 300, 1406, 31959, 3142, 25113, 293, 7083, 1840, 294, 341, 2856, 51072], "temperature": 0.0, "avg_logprob": -0.16442136142564856, "compression_ratio": 1.6221374045801527, "no_speech_prob": 0.001926146331243217}, {"id": 579, "seek": 262840, "start": 2643.28, "end": 2647.36, "text": " And this these the meaning functions, which will be implemented by the neural network here", "tokens": [51108, 400, 341, 613, 264, 3620, 6828, 11, 597, 486, 312, 12270, 538, 264, 18161, 3209, 510, 51312], "temperature": 0.0, "avg_logprob": -0.16442136142564856, "compression_ratio": 1.6221374045801527, "no_speech_prob": 0.001926146331243217}, {"id": 580, "seek": 262840, "start": 2648.4, "end": 2649.76, "text": " other the", "tokens": [51364, 661, 264, 51432], "temperature": 0.0, "avg_logprob": -0.16442136142564856, "compression_ratio": 1.6221374045801527, "no_speech_prob": 0.001926146331243217}, {"id": 581, "seek": 262840, "start": 2649.76, "end": 2651.76, "text": " Large or even small language model", "tokens": [51432, 33092, 420, 754, 1359, 2856, 2316, 51532], "temperature": 0.0, "avg_logprob": -0.16442136142564856, "compression_ratio": 1.6221374045801527, "no_speech_prob": 0.001926146331243217}, {"id": 582, "seek": 262840, "start": 2652.1600000000003, "end": 2655.36, "text": " Um capture various interesting ideas that have been proposed", "tokens": [51552, 3301, 7983, 3683, 1880, 3487, 300, 362, 668, 10348, 51712], "temperature": 0.0, "avg_logprob": -0.16442136142564856, "compression_ratio": 1.6221374045801527, "no_speech_prob": 0.001926146331243217}, {"id": 583, "seek": 265536, "start": 2655.6, "end": 2660.32, "text": " You know, I don't have time to review this history but proposed in different eras of linguistics and thinking about", "tokens": [50376, 509, 458, 11, 286, 500, 380, 362, 565, 281, 3131, 341, 2503, 457, 10348, 294, 819, 1189, 296, 295, 21766, 6006, 293, 1953, 466, 50612], "temperature": 0.0, "avg_logprob": -0.13760544429315585, "compression_ratio": 1.7226277372262773, "no_speech_prob": 0.0017817300977185369}, {"id": 584, "seek": 265536, "start": 2661.1200000000003, "end": 2666.48, "text": " How to think about meaning how to think about concepts and you know, I it's it's a it's a very rich literature", "tokens": [50652, 1012, 281, 519, 466, 3620, 577, 281, 519, 466, 10392, 293, 291, 458, 11, 286, 309, 311, 309, 311, 257, 309, 311, 257, 588, 4593, 10394, 50920], "temperature": 0.0, "avg_logprob": -0.13760544429315585, "compression_ratio": 1.7226277372262773, "no_speech_prob": 0.0017817300977185369}, {"id": 585, "seek": 265536, "start": 2666.48, "end": 2669.6800000000003, "text": " That I'd be happy to try to discuss if we have time during the question period", "tokens": [50920, 663, 286, 1116, 312, 2055, 281, 853, 281, 2248, 498, 321, 362, 565, 1830, 264, 1168, 2896, 51080], "temperature": 0.0, "avg_logprob": -0.13760544429315585, "compression_ratio": 1.7226277372262773, "no_speech_prob": 0.0017817300977185369}, {"id": 586, "seek": 265536, "start": 2670.4, "end": 2677.2000000000003, "text": " Okay, um, but the hypothesis here is that maybe these distributional language code models can implement some", "tokens": [51116, 1033, 11, 1105, 11, 457, 264, 17291, 510, 307, 300, 1310, 613, 7316, 304, 2856, 3089, 5245, 393, 4445, 512, 51456], "temperature": 0.0, "avg_logprob": -0.13760544429315585, "compression_ratio": 1.7226277372262773, "no_speech_prob": 0.0017817300977185369}, {"id": 587, "seek": 265536, "start": 2677.84, "end": 2681.44, "text": " human like perhaps approximation to this meaning function", "tokens": [51488, 1952, 411, 4317, 28023, 281, 341, 3620, 2445, 51668], "temperature": 0.0, "avg_logprob": -0.13760544429315585, "compression_ratio": 1.7226277372262773, "no_speech_prob": 0.0017817300977185369}, {"id": 588, "seek": 268144, "start": 2682.4, "end": 2683.44, "text": " Um", "tokens": [50412, 3301, 50464], "temperature": 0.0, "avg_logprob": -0.13325542773840562, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.003221177961677313}, {"id": 589, "seek": 268144, "start": 2683.44, "end": 2690.48, "text": " And again, crucially, it's it's we're not trying to now capture like to learn patterns in data in the in in the world", "tokens": [50464, 400, 797, 11, 5140, 1909, 11, 309, 311, 309, 311, 321, 434, 406, 1382, 281, 586, 7983, 411, 281, 1466, 8294, 294, 1412, 294, 264, 294, 294, 264, 1002, 50816], "temperature": 0.0, "avg_logprob": -0.13325542773840562, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.003221177961677313}, {"id": 590, "seek": 268144, "start": 2690.48, "end": 2694.08, "text": " We're trying to learn patterns in our thinking and how thought is expressed in language", "tokens": [50816, 492, 434, 1382, 281, 1466, 8294, 294, 527, 1953, 293, 577, 1194, 307, 12675, 294, 2856, 50996], "temperature": 0.0, "avg_logprob": -0.13325542773840562, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.003221177961677313}, {"id": 591, "seek": 268144, "start": 2694.2400000000002, "end": 2696.2400000000002, "text": " So it's a more modular problem", "tokens": [51004, 407, 309, 311, 257, 544, 31111, 1154, 51104], "temperature": 0.0, "avg_logprob": -0.13325542773840562, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.003221177961677313}, {"id": 592, "seek": 268144, "start": 2696.2400000000002, "end": 2700.56, "text": " It which I think is better suited to the way language actually works in the human brain, right?", "tokens": [51104, 467, 597, 286, 519, 307, 1101, 24736, 281, 264, 636, 2856, 767, 1985, 294, 264, 1952, 3567, 11, 558, 30, 51320], "temperature": 0.0, "avg_logprob": -0.13325542773840562, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.003221177961677313}, {"id": 593, "seek": 268144, "start": 2700.56, "end": 2704.8, "text": " It's there's a delimited part of our brain. That is a language processing network", "tokens": [51320, 467, 311, 456, 311, 257, 1103, 332, 1226, 644, 295, 527, 3567, 13, 663, 307, 257, 2856, 9007, 3209, 51532], "temperature": 0.0, "avg_logprob": -0.13325542773840562, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.003221177961677313}, {"id": 594, "seek": 270480, "start": 2705.2000000000003, "end": 2711.36, "text": " Strokes or other lesions there can really impact your language ability without impacting your general thinking ability", "tokens": [50384, 42196, 5993, 420, 661, 1512, 626, 456, 393, 534, 2712, 428, 2856, 3485, 1553, 29963, 428, 2674, 1953, 3485, 50692], "temperature": 0.0, "avg_logprob": -0.07975675564001103, "compression_ratio": 1.6397058823529411, "no_speech_prob": 0.04811776801943779}, {"id": 595, "seek": 270480, "start": 2712.6400000000003, "end": 2716.6400000000003, "text": " And you know, it's a relatively late evolutionary addition to us to a brain structure", "tokens": [50756, 400, 291, 458, 11, 309, 311, 257, 7226, 3469, 27567, 4500, 281, 505, 281, 257, 3567, 3877, 50956], "temperature": 0.0, "avg_logprob": -0.07975675564001103, "compression_ratio": 1.6397058823529411, "no_speech_prob": 0.04811776801943779}, {"id": 596, "seek": 270480, "start": 2716.7200000000003, "end": 2719.76, "text": " Which shares a lot with other non-human primates, for example", "tokens": [50960, 3013, 12182, 257, 688, 365, 661, 2107, 12, 18796, 2886, 1024, 11, 337, 1365, 51112], "temperature": 0.0, "avg_logprob": -0.07975675564001103, "compression_ratio": 1.6397058823529411, "no_speech_prob": 0.04811776801943779}, {"id": 597, "seek": 270480, "start": 2720.2400000000002, "end": 2723.76, "text": " And so it's it's that that modularity. I think is also really important here", "tokens": [51136, 400, 370, 309, 311, 309, 311, 300, 300, 31111, 507, 13, 286, 519, 307, 611, 534, 1021, 510, 51312], "temperature": 0.0, "avg_logprob": -0.07975675564001103, "compression_ratio": 1.6397058823529411, "no_speech_prob": 0.04811776801943779}, {"id": 598, "seek": 270480, "start": 2724.48, "end": 2726.4, "text": " So again, just to unpack how this works", "tokens": [51348, 407, 797, 11, 445, 281, 26699, 577, 341, 1985, 51444], "temperature": 0.0, "avg_logprob": -0.07975675564001103, "compression_ratio": 1.6397058823529411, "no_speech_prob": 0.04811776801943779}, {"id": 599, "seek": 270480, "start": 2726.48, "end": 2729.6000000000004, "text": " You have a probabilistic program that describes these concepts", "tokens": [51448, 509, 362, 257, 31959, 3142, 1461, 300, 15626, 613, 10392, 51604], "temperature": 0.0, "avg_logprob": -0.07975675564001103, "compression_ratio": 1.6397058823529411, "no_speech_prob": 0.04811776801943779}, {"id": 600, "seek": 272960, "start": 2730.08, "end": 2733.12, "text": " That I can just I can describe to you in language in this way", "tokens": [50388, 663, 286, 393, 445, 286, 393, 6786, 281, 291, 294, 2856, 294, 341, 636, 50540], "temperature": 0.0, "avg_logprob": -0.13167723671334688, "compression_ratio": 1.8172413793103448, "no_speech_prob": 0.008845127187669277}, {"id": 601, "seek": 272960, "start": 2733.2799999999997, "end": 2737.36, "text": " But to the model right now, they're just given in code and I'm not and they just describe for example that", "tokens": [50548, 583, 281, 264, 2316, 558, 586, 11, 436, 434, 445, 2212, 294, 3089, 293, 286, 478, 406, 293, 436, 445, 6786, 337, 1365, 300, 50752], "temperature": 0.0, "avg_logprob": -0.13167723671334688, "compression_ratio": 1.8172413793103448, "no_speech_prob": 0.008845127187669277}, {"id": 602, "seek": 272960, "start": 2737.7599999999998, "end": 2741.8399999999997, "text": " You know players strength is drawn from a gaussian distribution players can occasion be lazy", "tokens": [50772, 509, 458, 4150, 3800, 307, 10117, 490, 257, 5959, 21948, 7316, 4150, 393, 9674, 312, 14847, 50976], "temperature": 0.0, "avg_logprob": -0.13167723671334688, "compression_ratio": 1.8172413793103448, "no_speech_prob": 0.008845127187669277}, {"id": 603, "seek": 272960, "start": 2742.48, "end": 2745.52, "text": " When you when you're lazy it cuts your effective strength in half", "tokens": [51008, 1133, 291, 562, 291, 434, 14847, 309, 9992, 428, 4942, 3800, 294, 1922, 51160], "temperature": 0.0, "avg_logprob": -0.13167723671334688, "compression_ratio": 1.8172413793103448, "no_speech_prob": 0.008845127187669277}, {"id": 604, "seek": 272960, "start": 2745.7599999999998, "end": 2751.12, "text": " And the strength of the team is that some of the total strength that they pull that the members pull with at any one time", "tokens": [51172, 400, 264, 3800, 295, 264, 1469, 307, 300, 512, 295, 264, 3217, 3800, 300, 436, 2235, 300, 264, 2679, 2235, 365, 412, 604, 472, 565, 51440], "temperature": 0.0, "avg_logprob": -0.13167723671334688, "compression_ratio": 1.8172413793103448, "no_speech_prob": 0.008845127187669277}, {"id": 605, "seek": 272960, "start": 2751.44, "end": 2754.96, "text": " And the stronger pulling team wins. That's basically what this is now for now", "tokens": [51456, 400, 264, 7249, 8407, 1469, 10641, 13, 663, 311, 1936, 437, 341, 307, 586, 337, 586, 51632], "temperature": 0.0, "avg_logprob": -0.13167723671334688, "compression_ratio": 1.8172413793103448, "no_speech_prob": 0.008845127187669277}, {"id": 606, "seek": 275496, "start": 2755.04, "end": 2757.92, "text": " We'll just assume that that's you have that kind of mental", "tokens": [50368, 492, 603, 445, 6552, 300, 300, 311, 291, 362, 300, 733, 295, 4973, 50512], "temperature": 0.0, "avg_logprob": -0.1440304507006396, "compression_ratio": 1.6023622047244095, "no_speech_prob": 0.024411184713244438}, {"id": 607, "seek": 275496, "start": 2758.64, "end": 2760.64, "text": " Model of a tug of war game", "tokens": [50548, 17105, 295, 257, 33543, 295, 1516, 1216, 50648], "temperature": 0.0, "avg_logprob": -0.1440304507006396, "compression_ratio": 1.6023622047244095, "no_speech_prob": 0.024411184713244438}, {"id": 608, "seek": 275496, "start": 2760.64, "end": 2762.96, "text": " And we'll just talk. How do you update your release language?", "tokens": [50648, 400, 321, 603, 445, 751, 13, 1012, 360, 291, 5623, 428, 4374, 2856, 30, 50764], "temperature": 0.0, "avg_logprob": -0.1440304507006396, "compression_ratio": 1.6023622047244095, "no_speech_prob": 0.024411184713244438}, {"id": 609, "seek": 275496, "start": 2763.28, "end": 2768.2400000000002, "text": " And the basic way of doing it is that we're using what I would call now a medium language model", "tokens": [50780, 400, 264, 3875, 636, 295, 884, 309, 307, 300, 321, 434, 1228, 437, 286, 576, 818, 586, 257, 6399, 2856, 2316, 51028], "temperature": 0.0, "avg_logprob": -0.1440304507006396, "compression_ratio": 1.6023622047244095, "no_speech_prob": 0.024411184713244438}, {"id": 610, "seek": 275496, "start": 2768.2400000000002, "end": 2776.48, "text": " We used open ai's codex, which was the first widely used code llm pretty small by today's standards much smaller than gpt4", "tokens": [51028, 492, 1143, 1269, 9783, 311, 3089, 87, 11, 597, 390, 264, 700, 13371, 1143, 3089, 287, 75, 76, 1238, 1359, 538, 965, 311, 7787, 709, 4356, 813, 290, 662, 19, 51440], "temperature": 0.0, "avg_logprob": -0.1440304507006396, "compression_ratio": 1.6023622047244095, "no_speech_prob": 0.024411184713244438}, {"id": 611, "seek": 275496, "start": 2777.12, "end": 2779.52, "text": " Kind of like an early version of gpt 3.5", "tokens": [51472, 9242, 295, 411, 364, 2440, 3037, 295, 290, 662, 805, 13, 20, 51592], "temperature": 0.0, "avg_logprob": -0.1440304507006396, "compression_ratio": 1.6023622047244095, "no_speech_prob": 0.024411184713244438}, {"id": 612, "seek": 277952, "start": 2780.0, "end": 2786.0, "text": " To translate from a sentence in english like this into what in church the probabilistic programming language is called a condition statement", "tokens": [50388, 1407, 13799, 490, 257, 8174, 294, 32169, 411, 341, 666, 437, 294, 4128, 264, 31959, 3142, 9410, 2856, 307, 1219, 257, 4188, 5629, 50688], "temperature": 0.0, "avg_logprob": -0.12687803904215494, "compression_ratio": 1.8517241379310345, "no_speech_prob": 0.007572449743747711}, {"id": 613, "seek": 277952, "start": 2786.16, "end": 2790.32, "text": " It just expresses the constraint on possible worlds that while the underlying", "tokens": [50696, 467, 445, 39204, 264, 25534, 322, 1944, 13401, 300, 1339, 264, 14217, 50904], "temperature": 0.0, "avg_logprob": -0.12687803904215494, "compression_ratio": 1.8517241379310345, "no_speech_prob": 0.007572449743747711}, {"id": 614, "seek": 277952, "start": 2790.72, "end": 2794.96, "text": " Define statements define stochastic or probabilistic functions, which give you distributions", "tokens": [50924, 9548, 533, 12363, 6964, 342, 8997, 2750, 420, 31959, 3142, 6828, 11, 597, 976, 291, 37870, 51136], "temperature": 0.0, "avg_logprob": -0.12687803904215494, "compression_ratio": 1.8517241379310345, "no_speech_prob": 0.007572449743747711}, {"id": 615, "seek": 277952, "start": 2795.36, "end": 2799.68, "text": " Probability distributions on possible worlds. This says we're going to restrict our probability distribution", "tokens": [51156, 8736, 2310, 37870, 322, 1944, 13401, 13, 639, 1619, 321, 434, 516, 281, 7694, 527, 8482, 7316, 51372], "temperature": 0.0, "avg_logprob": -0.12687803904215494, "compression_ratio": 1.8517241379310345, "no_speech_prob": 0.007572449743747711}, {"id": 616, "seek": 277952, "start": 2799.84, "end": 2806.16, "text": " Which is like our prior to a posterior just those worlds that are consistent with jack beating in one match leo", "tokens": [51380, 3013, 307, 411, 527, 4059, 281, 257, 33529, 445, 729, 13401, 300, 366, 8398, 365, 7109, 13497, 294, 472, 2995, 476, 78, 51696], "temperature": 0.0, "avg_logprob": -0.12687803904215494, "compression_ratio": 1.8517241379310345, "no_speech_prob": 0.007572449743747711}, {"id": 617, "seek": 277952, "start": 2806.56, "end": 2807.7599999999998, "text": " okay", "tokens": [51716, 1392, 51776], "temperature": 0.0, "avg_logprob": -0.12687803904215494, "compression_ratio": 1.8517241379310345, "no_speech_prob": 0.007572449743747711}, {"id": 618, "seek": 280776, "start": 2807.84, "end": 2812.96, "text": " And then similarly a quest to a query statement like how strong is jack turns into this worry", "tokens": [50368, 400, 550, 14138, 257, 866, 281, 257, 14581, 5629, 411, 577, 2068, 307, 7109, 4523, 666, 341, 3292, 50624], "temperature": 0.0, "avg_logprob": -0.11497458845081895, "compression_ratio": 1.7202797202797202, "no_speech_prob": 0.0014545866288244724}, {"id": 619, "seek": 280776, "start": 2813.6800000000003, "end": 2817.0400000000004, "text": " Which is which the probabilistic programming language then basically", "tokens": [50660, 3013, 307, 597, 264, 31959, 3142, 9410, 2856, 550, 1936, 50828], "temperature": 0.0, "avg_logprob": -0.11497458845081895, "compression_ratio": 1.7202797202797202, "no_speech_prob": 0.0014545866288244724}, {"id": 620, "seek": 280776, "start": 2817.6000000000004, "end": 2823.5200000000004, "text": " Evaluates it draws samples from possible worlds consistent with the condition and generated from that prior on the left", "tokens": [50856, 462, 3337, 27710, 309, 20045, 10938, 490, 1944, 13401, 8398, 365, 264, 4188, 293, 10833, 490, 300, 4059, 322, 264, 1411, 51152], "temperature": 0.0, "avg_logprob": -0.11497458845081895, "compression_ratio": 1.7202797202797202, "no_speech_prob": 0.0014545866288244724}, {"id": 621, "seek": 280776, "start": 2824.0, "end": 2827.84, "text": " And and checks what the strength is and then just kind of counts up those distributions", "tokens": [51176, 400, 293, 13834, 437, 264, 3800, 307, 293, 550, 445, 733, 295, 14893, 493, 729, 37870, 51368], "temperature": 0.0, "avg_logprob": -0.11497458845081895, "compression_ratio": 1.7202797202797202, "no_speech_prob": 0.0014545866288244724}, {"id": 622, "seek": 280776, "start": 2827.84, "end": 2832.7200000000003, "text": " So it's a kind of probabilistic mental simulation the same effectively the same or a generalization of what we were doing", "tokens": [51368, 407, 309, 311, 257, 733, 295, 31959, 3142, 4973, 16575, 264, 912, 8659, 264, 912, 420, 257, 2674, 2144, 295, 437, 321, 645, 884, 51612], "temperature": 0.0, "avg_logprob": -0.11497458845081895, "compression_ratio": 1.7202797202797202, "no_speech_prob": 0.0014545866288244724}, {"id": 623, "seek": 283272, "start": 2832.72, "end": 2837.4399999999996, "text": " You like those intuitive physics examples and from this one piece of data jack beating leo", "tokens": [50364, 509, 411, 729, 21769, 10649, 5110, 293, 490, 341, 472, 2522, 295, 1412, 7109, 13497, 476, 78, 50600], "temperature": 0.0, "avg_logprob": -0.138803588019477, "compression_ratio": 1.8006535947712419, "no_speech_prob": 0.10084347426891327}, {"id": 624, "seek": 283272, "start": 2837.52, "end": 2841.7599999999998, "text": " You can see the posterior updates from the prior so jack is stronger than average", "tokens": [50604, 509, 393, 536, 264, 33529, 9205, 490, 264, 4059, 370, 7109, 307, 7249, 813, 4274, 50816], "temperature": 0.0, "avg_logprob": -0.138803588019477, "compression_ratio": 1.8006535947712419, "no_speech_prob": 0.10084347426891327}, {"id": 625, "seek": 283272, "start": 2842.08, "end": 2847.2, "text": " If I say oh jack also beat alex or proceeded to claim victory from alex that turns into another condition", "tokens": [50832, 759, 286, 584, 1954, 7109, 611, 4224, 257, 2021, 420, 39053, 281, 3932, 9812, 490, 257, 2021, 300, 4523, 666, 1071, 4188, 51088], "temperature": 0.0, "avg_logprob": -0.138803588019477, "compression_ratio": 1.8006535947712419, "no_speech_prob": 0.10084347426891327}, {"id": 626, "seek": 283272, "start": 2847.52, "end": 2851.2, "text": " And that updates the posterior even more so now while jack's a lot stronger than average", "tokens": [51104, 400, 300, 9205, 264, 33529, 754, 544, 370, 586, 1339, 7109, 311, 257, 688, 7249, 813, 4274, 51288], "temperature": 0.0, "avg_logprob": -0.138803588019477, "compression_ratio": 1.8006535947712419, "no_speech_prob": 0.10084347426891327}, {"id": 627, "seek": 283272, "start": 2852.08, "end": 2857.6, "text": " Even working as a team leo and alex still could not be jack. Wow now jack's even stronger when you add in that conditioning statement", "tokens": [51332, 2754, 1364, 382, 257, 1469, 476, 78, 293, 257, 2021, 920, 727, 406, 312, 7109, 13, 3153, 586, 7109, 311, 754, 7249, 562, 291, 909, 294, 300, 21901, 5629, 51608], "temperature": 0.0, "avg_logprob": -0.138803588019477, "compression_ratio": 1.8006535947712419, "no_speech_prob": 0.10084347426891327}, {"id": 628, "seek": 283272, "start": 2857.9199999999996, "end": 2860.64, "text": " So each again the where the llm is coming in here", "tokens": [51624, 407, 1184, 797, 264, 689, 264, 4849, 76, 307, 1348, 294, 510, 51760], "temperature": 0.0, "avg_logprob": -0.138803588019477, "compression_ratio": 1.8006535947712419, "no_speech_prob": 0.10084347426891327}, {"id": 629, "seek": 286064, "start": 2861.52, "end": 2869.12, "text": " Is it's just it's just adding in the the statements into the language of thought and then we're running inference in our probabilistic programming language to give these answers", "tokens": [50408, 1119, 309, 311, 445, 309, 311, 445, 5127, 294, 264, 264, 12363, 666, 264, 2856, 295, 1194, 293, 550, 321, 434, 2614, 38253, 294, 527, 31959, 3142, 9410, 2856, 281, 976, 613, 6338, 50788], "temperature": 0.0, "avg_logprob": -0.13638872146606446, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.003648885525763035}, {"id": 630, "seek": 286064, "start": 2870.0, "end": 2873.8399999999997, "text": " It's worth and this is again, especially if we want to understand what I think is", "tokens": [50832, 467, 311, 3163, 293, 341, 307, 797, 11, 2318, 498, 321, 528, 281, 1223, 437, 286, 519, 307, 51024], "temperature": 0.0, "avg_logprob": -0.13638872146606446, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.003648885525763035}, {"id": 631, "seek": 286064, "start": 2875.2799999999997, "end": 2880.16, "text": " The way forward to thinking about how meaning and language works in humans and where l lm's can come into the mix", "tokens": [51096, 440, 636, 2128, 281, 1953, 466, 577, 3620, 293, 2856, 1985, 294, 6255, 293, 689, 287, 287, 76, 311, 393, 808, 666, 264, 2890, 51340], "temperature": 0.0, "avg_logprob": -0.13638872146606446, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.003648885525763035}, {"id": 632, "seek": 286064, "start": 2880.48, "end": 2882.48, "text": " What's what's really powerful about?", "tokens": [51356, 708, 311, 437, 311, 534, 4005, 466, 30, 51456], "temperature": 0.0, "avg_logprob": -0.13638872146606446, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.003648885525763035}, {"id": 633, "seek": 286064, "start": 2882.8799999999997, "end": 2884.7999999999997, "text": " Neuro language models here", "tokens": [51476, 1734, 7052, 2856, 5245, 510, 51572], "temperature": 0.0, "avg_logprob": -0.13638872146606446, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.003648885525763035}, {"id": 634, "seek": 288480, "start": 2884.8, "end": 2891.6000000000004, "text": " As ways to parameterize a meaning function is the ways that they can pick up on statistics context pragmatics", "tokens": [50364, 1018, 2098, 281, 13075, 1125, 257, 3620, 2445, 307, 264, 2098, 300, 436, 393, 1888, 493, 322, 12523, 4319, 33394, 15677, 1167, 50704], "temperature": 0.0, "avg_logprob": -0.18721818923950195, "compression_ratio": 1.782918149466192, "no_speech_prob": 0.06005270034074783}, {"id": 635, "seek": 288480, "start": 2892.0800000000004, "end": 2893.1200000000003, "text": " metaphor", "tokens": [50728, 19157, 50780], "temperature": 0.0, "avg_logprob": -0.18721818923950195, "compression_ratio": 1.782918149466192, "no_speech_prob": 0.06005270034074783}, {"id": 636, "seek": 288480, "start": 2893.1200000000003, "end": 2899.2000000000003, "text": " Um semantic associations all the things that in many ways were most appealing in connectionism like distributed content addressable", "tokens": [50780, 3301, 47982, 26597, 439, 264, 721, 300, 294, 867, 2098, 645, 881, 23842, 294, 4984, 1434, 411, 12631, 2701, 2985, 712, 51084], "temperature": 0.0, "avg_logprob": -0.18721818923950195, "compression_ratio": 1.782918149466192, "no_speech_prob": 0.06005270034074783}, {"id": 637, "seek": 288480, "start": 2899.76, "end": 2904.7200000000003, "text": " Uh associate of memory and distributed representations of graded semantic associations", "tokens": [51112, 4019, 14644, 295, 4675, 293, 12631, 33358, 295, 2771, 292, 47982, 26597, 51360], "temperature": 0.0, "avg_logprob": -0.18721818923950195, "compression_ratio": 1.782918149466192, "no_speech_prob": 0.06005270034074783}, {"id": 638, "seek": 288480, "start": 2905.04, "end": 2908.8, "text": " So for example, the llm will translate jack won against leo", "tokens": [51376, 407, 337, 1365, 11, 264, 287, 75, 76, 486, 13799, 7109, 1582, 1970, 476, 78, 51564], "temperature": 0.0, "avg_logprob": -0.18721818923950195, "compression_ratio": 1.782918149466192, "no_speech_prob": 0.06005270034074783}, {"id": 639, "seek": 288480, "start": 2909.2000000000003, "end": 2914.4, "text": " Which into this statement which looks like a fairly transparent semantic parse of that natural language", "tokens": [51584, 3013, 666, 341, 5629, 597, 1542, 411, 257, 6457, 12737, 47982, 48377, 295, 300, 3303, 2856, 51844], "temperature": 0.0, "avg_logprob": -0.18721818923950195, "compression_ratio": 1.782918149466192, "no_speech_prob": 0.06005270034074783}, {"id": 640, "seek": 291480, "start": 2914.8, "end": 2919.84, "text": " But it'll make basically the same semantic parse of a sentence which on its surface", "tokens": [50364, 583, 309, 603, 652, 1936, 264, 912, 47982, 48377, 295, 257, 8174, 597, 322, 1080, 3753, 50616], "temperature": 0.0, "avg_logprob": -0.16052573581911483, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.00036823179107159376}, {"id": 641, "seek": 291480, "start": 2920.5600000000004, "end": 2926.48, "text": " And and in traditional syntax, you know language structuralities looks rather different, right? The syntax here is more complicated", "tokens": [50652, 400, 293, 294, 5164, 28431, 11, 291, 458, 2856, 15067, 1088, 1542, 2831, 819, 11, 558, 30, 440, 28431, 510, 307, 544, 6179, 50948], "temperature": 0.0, "avg_logprob": -0.16052573581911483, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.00036823179107159376}, {"id": 642, "seek": 291480, "start": 2926.48, "end": 2928.88, "text": " I'm not actually using the word win, but", "tokens": [50948, 286, 478, 406, 767, 1228, 264, 1349, 1942, 11, 457, 51068], "temperature": 0.0, "avg_logprob": -0.16052573581911483, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.00036823179107159376}, {"id": 643, "seek": 291480, "start": 2929.6000000000004, "end": 2936.2400000000002, "text": " In the context the relevant aspect of meaning for thought here is the same namely. This is just another way. It may be more poetic way", "tokens": [51104, 682, 264, 4319, 264, 7340, 4171, 295, 3620, 337, 1194, 510, 307, 264, 912, 20926, 13, 639, 307, 445, 1071, 636, 13, 467, 815, 312, 544, 41080, 636, 51436], "temperature": 0.0, "avg_logprob": -0.16052573581911483, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.00036823179107159376}, {"id": 644, "seek": 291480, "start": 2937.2000000000003, "end": 2941.76, "text": " Or dramatic way to say that jack won against alex and the model knows that automatically", "tokens": [51484, 1610, 12023, 636, 281, 584, 300, 7109, 1582, 1970, 257, 2021, 293, 264, 2316, 3255, 300, 6772, 51712], "temperature": 0.0, "avg_logprob": -0.16052573581911483, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.00036823179107159376}, {"id": 645, "seek": 294176, "start": 2941.76, "end": 2946.1600000000003, "text": " It doesn't have to be specially prompted or trained for that using its associative memory properties", "tokens": [50364, 467, 1177, 380, 362, 281, 312, 22549, 31042, 420, 8895, 337, 300, 1228, 1080, 4180, 1166, 4675, 7221, 50584], "temperature": 0.0, "avg_logprob": -0.11300345164973562, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.005138806067407131}, {"id": 646, "seek": 294176, "start": 2946.48, "end": 2950.2400000000002, "text": " It's also distributional. These are probabilistic models not on worlds", "tokens": [50600, 467, 311, 611, 7316, 304, 13, 1981, 366, 31959, 3142, 5245, 406, 322, 13401, 50788], "temperature": 0.0, "avg_logprob": -0.11300345164973562, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.005138806067407131}, {"id": 647, "seek": 294176, "start": 2950.4, "end": 2955.1200000000003, "text": " But on strings and in this case they're probabilistic models on strings in our mental programming language", "tokens": [50796, 583, 322, 13985, 293, 294, 341, 1389, 436, 434, 31959, 3142, 5245, 322, 13985, 294, 527, 4973, 9410, 2856, 51032], "temperature": 0.0, "avg_logprob": -0.11300345164973562, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.005138806067407131}, {"id": 648, "seek": 294176, "start": 2955.28, "end": 2956.8, "text": " so they can", "tokens": [51040, 370, 436, 393, 51116], "temperature": 0.0, "avg_logprob": -0.11300345164973562, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.005138806067407131}, {"id": 649, "seek": 294176, "start": 2956.8, "end": 2961.1200000000003, "text": " Bring into you know bring in classic notions of vagueness if I say jack is strong or very strong", "tokens": [51116, 12842, 666, 291, 458, 1565, 294, 7230, 35799, 295, 13501, 7801, 442, 498, 286, 584, 7109, 307, 2068, 420, 588, 2068, 51332], "temperature": 0.0, "avg_logprob": -0.11300345164973562, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.005138806067407131}, {"id": 650, "seek": 294176, "start": 2961.28, "end": 2965.6800000000003, "text": " I'm not telling you exactly how strong jack is but you might interpret that as a distribution", "tokens": [51340, 286, 478, 406, 3585, 291, 2293, 577, 2068, 7109, 307, 457, 291, 1062, 7302, 300, 382, 257, 7316, 51560], "temperature": 0.0, "avg_logprob": -0.11300345164973562, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.005138806067407131}, {"id": 651, "seek": 294176, "start": 2966.0, "end": 2969.76, "text": " On different condition statements saying well jack strength is is greater than some threshold", "tokens": [51576, 1282, 819, 4188, 12363, 1566, 731, 7109, 3800, 307, 307, 5044, 813, 512, 14678, 51764], "temperature": 0.0, "avg_logprob": -0.11300345164973562, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.005138806067407131}, {"id": 652, "seek": 296976, "start": 2969.76, "end": 2973.2000000000003, "text": " But I don't know what that threshold is but it's probably pretty big remember this in this case", "tokens": [50364, 583, 286, 500, 380, 458, 437, 300, 14678, 307, 457, 309, 311, 1391, 1238, 955, 1604, 341, 294, 341, 1389, 50536], "temperature": 0.0, "avg_logprob": -0.09959706322091524, "compression_ratio": 1.8125, "no_speech_prob": 0.0012063974281772971}, {"id": 653, "seek": 296976, "start": 2973.28, "end": 2976.96, "text": " The mean is 50 and the standard deviation 20 so 80 is you know", "tokens": [50540, 440, 914, 307, 2625, 293, 264, 3832, 25163, 945, 370, 4688, 307, 291, 458, 50724], "temperature": 0.0, "avg_logprob": -0.09959706322091524, "compression_ratio": 1.8125, "no_speech_prob": 0.0012063974281772971}, {"id": 654, "seek": 296976, "start": 2977.76, "end": 2982.4, "text": " One and a half standard deviations above the mean and the idea is this is again like in scalar", "tokens": [50764, 1485, 293, 257, 1922, 3832, 31219, 763, 3673, 264, 914, 293, 264, 1558, 307, 341, 307, 797, 411, 294, 39684, 50996], "temperature": 0.0, "avg_logprob": -0.09959706322091524, "compression_ratio": 1.8125, "no_speech_prob": 0.0012063974281772971}, {"id": 655, "seek": 296976, "start": 2984.0, "end": 2987.6000000000004, "text": " Adjectives it's basically saying I'm conditioning on the idea that jack is greater than some threshold", "tokens": [51076, 1999, 1020, 1539, 309, 311, 1936, 1566, 286, 478, 21901, 322, 264, 1558, 300, 7109, 307, 5044, 813, 512, 14678, 51256], "temperature": 0.0, "avg_logprob": -0.09959706322091524, "compression_ratio": 1.8125, "no_speech_prob": 0.0012063974281772971}, {"id": 656, "seek": 296976, "start": 2987.6000000000004, "end": 2993.92, "text": " But I but I could be uncertain about what that threshold is and even metaphorical things like if I say jack is pretty strong", "tokens": [51256, 583, 286, 457, 286, 727, 312, 11308, 466, 437, 300, 14678, 307, 293, 754, 19157, 804, 721, 411, 498, 286, 584, 7109, 307, 1238, 2068, 51572], "temperature": 0.0, "avg_logprob": -0.09959706322091524, "compression_ratio": 1.8125, "no_speech_prob": 0.0012063974281772971}, {"id": 657, "seek": 296976, "start": 2994.0, "end": 2996.4, "text": " And ben is a beast right in this context", "tokens": [51576, 400, 3271, 307, 257, 13464, 558, 294, 341, 4319, 51696], "temperature": 0.0, "avg_logprob": -0.09959706322091524, "compression_ratio": 1.8125, "no_speech_prob": 0.0012063974281772971}, {"id": 658, "seek": 299640, "start": 2996.7200000000003, "end": 3001.36, "text": " It's reasonable to interpret ben is a beast as as saying saying ben is really really strong", "tokens": [50380, 467, 311, 10585, 281, 7302, 3271, 307, 257, 13464, 382, 382, 1566, 1566, 3271, 307, 534, 534, 2068, 50612], "temperature": 0.0, "avg_logprob": -0.15502253532409668, "compression_ratio": 1.73828125, "no_speech_prob": 0.006096093915402889}, {"id": 659, "seek": 299640, "start": 3001.52, "end": 3007.28, "text": " You know 80 where jack was maybe 60 or greater in a different context if i'm talking about you know", "tokens": [50620, 509, 458, 4688, 689, 7109, 390, 1310, 4060, 420, 5044, 294, 257, 819, 4319, 498, 741, 478, 1417, 466, 291, 458, 50908], "temperature": 0.0, "avg_logprob": -0.15502253532409668, "compression_ratio": 1.73828125, "no_speech_prob": 0.006096093915402889}, {"id": 660, "seek": 299640, "start": 3008.0, "end": 3013.44, "text": " Whether you should date someone and I say ben is a beast, you know, then i'm probably saying stay away", "tokens": [50944, 8503, 291, 820, 4002, 1580, 293, 286, 584, 3271, 307, 257, 13464, 11, 291, 458, 11, 550, 741, 478, 1391, 1566, 1754, 1314, 51216], "temperature": 0.0, "avg_logprob": -0.15502253532409668, "compression_ratio": 1.73828125, "no_speech_prob": 0.006096093915402889}, {"id": 661, "seek": 299640, "start": 3014.1600000000003, "end": 3015.28, "text": " right", "tokens": [51252, 558, 51308], "temperature": 0.0, "avg_logprob": -0.15502253532409668, "compression_ratio": 1.73828125, "no_speech_prob": 0.006096093915402889}, {"id": 662, "seek": 299640, "start": 3015.28, "end": 3023.6, "text": " So that that both metaphor sort of context sensitive metaphorical interpretation is the kind of thing that this meaning function is very good at", "tokens": [51308, 407, 300, 300, 1293, 19157, 1333, 295, 4319, 9477, 19157, 804, 14174, 307, 264, 733, 295, 551, 300, 341, 3620, 2445, 307, 588, 665, 412, 51724], "temperature": 0.0, "avg_logprob": -0.15502253532409668, "compression_ratio": 1.73828125, "no_speech_prob": 0.006096093915402889}, {"id": 663, "seek": 302360, "start": 3024.56, "end": 3028.7999999999997, "text": " um, I'll mostly just skip over this but in work with ben lytkin and and", "tokens": [50412, 1105, 11, 286, 603, 5240, 445, 10023, 670, 341, 457, 294, 589, 365, 3271, 287, 4328, 5843, 293, 293, 50624], "temperature": 0.0, "avg_logprob": -0.1940479463743932, "compression_ratio": 1.699604743083004, "no_speech_prob": 0.0017816803883761168}, {"id": 664, "seek": 302360, "start": 3029.8399999999997, "end": 3036.0, "text": " Leo and gape and others we've we've done and this is really all ben's work some very nice quantitative studies", "tokens": [50676, 19344, 293, 5959, 494, 293, 2357, 321, 600, 321, 600, 1096, 293, 341, 307, 534, 439, 3271, 311, 589, 512, 588, 1481, 27778, 5313, 50984], "temperature": 0.0, "avg_logprob": -0.1940479463743932, "compression_ratio": 1.699604743083004, "no_speech_prob": 0.0017816803883761168}, {"id": 665, "seek": 302360, "start": 3036.88, "end": 3040.96, "text": " Of showing that these contextual aspects of understanding of strength", "tokens": [51028, 2720, 4099, 300, 613, 35526, 7270, 295, 3701, 295, 3800, 51232], "temperature": 0.0, "avg_logprob": -0.1940479463743932, "compression_ratio": 1.699604743083004, "no_speech_prob": 0.0017816803883761168}, {"id": 666, "seek": 302360, "start": 3042.0, "end": 3048.72, "text": " In in the kind of tug-of-war context can match very nicely not always but in a lot of cases very nicely with human judgment", "tokens": [51284, 682, 294, 264, 733, 295, 33543, 12, 2670, 12, 6925, 4319, 393, 2995, 588, 9594, 406, 1009, 457, 294, 257, 688, 295, 3331, 588, 9594, 365, 1952, 12216, 51620], "temperature": 0.0, "avg_logprob": -0.1940479463743932, "compression_ratio": 1.699604743083004, "no_speech_prob": 0.0017816803883761168}, {"id": 667, "seek": 302360, "start": 3048.7999999999997, "end": 3052.16, "text": " so it's suggesting that the distributional aspects of", "tokens": [51624, 370, 309, 311, 18094, 300, 264, 7316, 304, 7270, 295, 51792], "temperature": 0.0, "avg_logprob": -0.1940479463743932, "compression_ratio": 1.699604743083004, "no_speech_prob": 0.0017816803883761168}, {"id": 668, "seek": 305216, "start": 3053.12, "end": 3055.52, "text": " um meaning that we're capturing here are at least somewhat", "tokens": [50412, 1105, 3620, 300, 321, 434, 23384, 510, 366, 412, 1935, 8344, 50532], "temperature": 0.0, "avg_logprob": -0.11335064949245628, "compression_ratio": 1.7526501766784452, "no_speech_prob": 0.0012640014756470919}, {"id": 669, "seek": 305216, "start": 3056.0, "end": 3059.2799999999997, "text": " Not psychologically real at least consistent with the judgments that people make", "tokens": [50556, 1726, 41387, 957, 412, 1935, 8398, 365, 264, 40337, 300, 561, 652, 50720], "temperature": 0.0, "avg_logprob": -0.11335064949245628, "compression_ratio": 1.7526501766784452, "no_speech_prob": 0.0012640014756470919}, {"id": 670, "seek": 305216, "start": 3059.68, "end": 3065.6, "text": " Okay, so so but mostly what i've done with this example is just to try to show you how this kind of framework can work", "tokens": [50740, 1033, 11, 370, 370, 457, 5240, 437, 741, 600, 1096, 365, 341, 1365, 307, 445, 281, 853, 281, 855, 291, 577, 341, 733, 295, 8388, 393, 589, 51036], "temperature": 0.0, "avg_logprob": -0.11335064949245628, "compression_ratio": 1.7526501766784452, "no_speech_prob": 0.0012640014756470919}, {"id": 671, "seek": 305216, "start": 3066.0, "end": 3070.56, "text": " To be a way of implementing what is in some sense a classical idea that language isn't", "tokens": [51056, 1407, 312, 257, 636, 295, 18114, 437, 307, 294, 512, 2020, 257, 13735, 1558, 300, 2856, 1943, 380, 51284], "temperature": 0.0, "avg_logprob": -0.11335064949245628, "compression_ratio": 1.7526501766784452, "no_speech_prob": 0.0012640014756470919}, {"id": 672, "seek": 305216, "start": 3071.3599999999997, "end": 3074.24, "text": " That directly the medium of thought at least the only one", "tokens": [51324, 663, 3838, 264, 6399, 295, 1194, 412, 1935, 264, 787, 472, 51468], "temperature": 0.0, "avg_logprob": -0.11335064949245628, "compression_ratio": 1.7526501766784452, "no_speech_prob": 0.0012640014756470919}, {"id": 673, "seek": 305216, "start": 3074.64, "end": 3078.56, "text": " But it's a way of expressing and communicating thoughts internalizing and externalizing them", "tokens": [51488, 583, 309, 311, 257, 636, 295, 22171, 293, 17559, 4598, 6920, 3319, 293, 8320, 3319, 552, 51684], "temperature": 0.0, "avg_logprob": -0.11335064949245628, "compression_ratio": 1.7526501766784452, "no_speech_prob": 0.0012640014756470919}, {"id": 674, "seek": 307856, "start": 3078.72, "end": 3084.16, "text": " Okay, and that statistical distributional mechanisms can be a powerful way of learning that", "tokens": [50372, 1033, 11, 293, 300, 22820, 7316, 304, 15902, 393, 312, 257, 4005, 636, 295, 2539, 300, 50644], "temperature": 0.0, "avg_logprob": -0.19439085734259223, "compression_ratio": 1.6434108527131783, "no_speech_prob": 0.0027534973341971636}, {"id": 675, "seek": 307856, "start": 3084.56, "end": 3087.6, "text": " The mappings from the signs and symbols that we externalize", "tokens": [50664, 440, 463, 28968, 490, 264, 7880, 293, 16944, 300, 321, 8320, 1125, 50816], "temperature": 0.0, "avg_logprob": -0.19439085734259223, "compression_ratio": 1.6434108527131783, "no_speech_prob": 0.0027534973341971636}, {"id": 676, "seek": 307856, "start": 3088.24, "end": 3091.6, "text": " To the to an internal compositional structured language of our thought", "tokens": [50848, 1407, 264, 281, 364, 6920, 10199, 2628, 18519, 2856, 295, 527, 1194, 51016], "temperature": 0.0, "avg_logprob": -0.19439085734259223, "compression_ratio": 1.6434108527131783, "no_speech_prob": 0.0027534973341971636}, {"id": 677, "seek": 307856, "start": 3092.16, "end": 3095.44, "text": " Now a lot of what goes on in this paper and i'll just show you one or two examples", "tokens": [51044, 823, 257, 688, 295, 437, 1709, 322, 294, 341, 3035, 293, 741, 603, 445, 855, 291, 472, 420, 732, 5110, 51208], "temperature": 0.0, "avg_logprob": -0.19439085734259223, "compression_ratio": 1.6434108527131783, "no_speech_prob": 0.0027534973341971636}, {"id": 678, "seek": 307856, "start": 3096.32, "end": 3103.04, "text": " Such as work with sed zhong in an intuitive physics domain or some intuitive psychology work with lan xing and shen or", "tokens": [51252, 9653, 382, 589, 365, 9643, 710, 46507, 294, 364, 21769, 10649, 9274, 420, 512, 21769, 15105, 589, 365, 9326, 2031, 278, 293, 402, 268, 420, 51588], "temperature": 0.0, "avg_logprob": -0.19439085734259223, "compression_ratio": 1.6434108527131783, "no_speech_prob": 0.0027534973341971636}, {"id": 679, "seek": 310304, "start": 3104.0, "end": 3105.52, "text": " Tan shi shen", "tokens": [50412, 17046, 402, 72, 402, 268, 50488], "temperature": 0.0, "avg_logprob": -0.16072298268802832, "compression_ratio": 1.828996282527881, "no_speech_prob": 0.02370097115635872}, {"id": 680, "seek": 310304, "start": 3105.52, "end": 3109.12, "text": " Is showing how we can take the same kinds of things that i showed you in the first part of the talk", "tokens": [50488, 1119, 4099, 577, 321, 393, 747, 264, 912, 3685, 295, 721, 300, 741, 4712, 291, 294, 264, 700, 644, 295, 264, 751, 50668], "temperature": 0.0, "avg_logprob": -0.16072298268802832, "compression_ratio": 1.828996282527881, "no_speech_prob": 0.02370097115635872}, {"id": 681, "seek": 310304, "start": 3109.52, "end": 3115.52, "text": " And effectively reconstruct them, but now in a world's worlds that are just described by language", "tokens": [50688, 400, 8659, 31499, 552, 11, 457, 586, 294, 257, 1002, 311, 13401, 300, 366, 445, 7619, 538, 2856, 50988], "temperature": 0.0, "avg_logprob": -0.16072298268802832, "compression_ratio": 1.828996282527881, "no_speech_prob": 0.02370097115635872}, {"id": 682, "seek": 310304, "start": 3116.08, "end": 3121.44, "text": " So to manage it take that task of the red and yellow thing where before i was showing you the image of a scene", "tokens": [51016, 407, 281, 3067, 309, 747, 300, 5633, 295, 264, 2182, 293, 5566, 551, 689, 949, 741, 390, 4099, 291, 264, 3256, 295, 257, 4145, 51284], "temperature": 0.0, "avg_logprob": -0.16072298268802832, "compression_ratio": 1.828996282527881, "no_speech_prob": 0.02370097115635872}, {"id": 683, "seek": 310304, "start": 3121.84, "end": 3124.16, "text": " And then i was using language to describe a question", "tokens": [51304, 400, 550, 741, 390, 1228, 2856, 281, 6786, 257, 1168, 51420], "temperature": 0.0, "avg_logprob": -0.16072298268802832, "compression_ratio": 1.828996282527881, "no_speech_prob": 0.02370097115635872}, {"id": 684, "seek": 310304, "start": 3124.48, "end": 3127.68, "text": " But what if i just use language to describe the whole world, right?", "tokens": [51436, 583, 437, 498, 741, 445, 764, 2856, 281, 6786, 264, 1379, 1002, 11, 558, 30, 51596], "temperature": 0.0, "avg_logprob": -0.16072298268802832, "compression_ratio": 1.828996282527881, "no_speech_prob": 0.02370097115635872}, {"id": 685, "seek": 310304, "start": 3127.7599999999998, "end": 3129.7599999999998, "text": " So if i didn't show you an image i just described", "tokens": [51600, 407, 498, 741, 994, 380, 855, 291, 364, 3256, 741, 445, 7619, 51700], "temperature": 0.0, "avg_logprob": -0.16072298268802832, "compression_ratio": 1.828996282527881, "no_speech_prob": 0.02370097115635872}, {"id": 686, "seek": 312976, "start": 3130.2400000000002, "end": 3137.5200000000004, "text": " What's there, you know in many ways, but but but we're still going to do a a mental simulation a probabilistic mental simulation in a physics engine", "tokens": [50388, 708, 311, 456, 11, 291, 458, 294, 867, 2098, 11, 457, 457, 457, 321, 434, 920, 516, 281, 360, 257, 257, 4973, 16575, 257, 31959, 3142, 4973, 16575, 294, 257, 10649, 2848, 50752], "temperature": 0.0, "avg_logprob": -0.13343989954585522, "compression_ratio": 1.7473684210526317, "no_speech_prob": 0.011668190360069275}, {"id": 687, "seek": 312976, "start": 3137.6800000000003, "end": 3143.0400000000004, "text": " There's many cases, you know in our daily life where we use language to describe the physical world as we experience it", "tokens": [50760, 821, 311, 867, 3331, 11, 291, 458, 294, 527, 5212, 993, 689, 321, 764, 2856, 281, 6786, 264, 4001, 1002, 382, 321, 1752, 309, 51028], "temperature": 0.0, "avg_logprob": -0.13343989954585522, "compression_ratio": 1.7473684210526317, "no_speech_prob": 0.011668190360069275}, {"id": 688, "seek": 312976, "start": 3143.28, "end": 3148.6400000000003, "text": " It can be extremely expressive very complimentary often to you know images or photos", "tokens": [51040, 467, 393, 312, 4664, 40189, 588, 47162, 2049, 281, 291, 458, 5267, 420, 5787, 51308], "temperature": 0.0, "avg_logprob": -0.13343989954585522, "compression_ratio": 1.7473684210526317, "no_speech_prob": 0.011668190360069275}, {"id": 689, "seek": 312976, "start": 3149.1200000000003, "end": 3151.84, "text": " Um, it has vagueness and uncertainty and that's interesting", "tokens": [51332, 3301, 11, 309, 575, 13501, 7801, 442, 293, 15697, 293, 300, 311, 1880, 51468], "temperature": 0.0, "avg_logprob": -0.13343989954585522, "compression_ratio": 1.7473684210526317, "no_speech_prob": 0.011668190360069275}, {"id": 690, "seek": 312976, "start": 3152.0800000000004, "end": 3155.6800000000003, "text": " So we might describe a scene that like imagine a table and there's some blocks on it", "tokens": [51480, 407, 321, 1062, 6786, 257, 4145, 300, 411, 3811, 257, 3199, 293, 456, 311, 512, 8474, 322, 309, 51660], "temperature": 0.0, "avg_logprob": -0.13343989954585522, "compression_ratio": 1.7473684210526317, "no_speech_prob": 0.011668190360069275}, {"id": 691, "seek": 315568, "start": 3155.9199999999996, "end": 3157.2799999999997, "text": " There's some red blocks in the center", "tokens": [50376, 821, 311, 512, 2182, 8474, 294, 264, 3056, 50444], "temperature": 0.0, "avg_logprob": -0.12144527127665858, "compression_ratio": 1.7721518987341771, "no_speech_prob": 0.00297964783385396}, {"id": 692, "seek": 315568, "start": 3157.2799999999997, "end": 3161.2, "text": " There are many tall stacks of yellow blocks on the side of the table if the table is bumped hard enough", "tokens": [50444, 821, 366, 867, 6764, 30792, 295, 5566, 8474, 322, 264, 1252, 295, 264, 3199, 498, 264, 3199, 307, 42696, 1152, 1547, 50640], "temperature": 0.0, "avg_logprob": -0.12144527127665858, "compression_ratio": 1.7721518987341771, "no_speech_prob": 0.00297964783385396}, {"id": 693, "seek": 315568, "start": 3161.2799999999997, "end": 3164.24, "text": " So we ask the same question but for scenes that are described in language", "tokens": [50644, 407, 321, 1029, 264, 912, 1168, 457, 337, 8026, 300, 366, 7619, 294, 2856, 50792], "temperature": 0.0, "avg_logprob": -0.12144527127665858, "compression_ratio": 1.7721518987341771, "no_speech_prob": 0.00297964783385396}, {"id": 694, "seek": 315568, "start": 3164.24, "end": 3170.08, "text": " And then people make a graded judgment the same one to seven graded judgment of is it more likely to be red or yellow blocks?", "tokens": [50792, 400, 550, 561, 652, 257, 2771, 292, 12216, 264, 912, 472, 281, 3407, 2771, 292, 12216, 295, 307, 309, 544, 3700, 281, 312, 2182, 420, 5566, 8474, 30, 51084], "temperature": 0.0, "avg_logprob": -0.12144527127665858, "compression_ratio": 1.7721518987341771, "no_speech_prob": 0.00297964783385396}, {"id": 695, "seek": 315568, "start": 3170.7999999999997, "end": 3174.48, "text": " And so when we can test this again the same kind of quantitative study with", "tokens": [51120, 400, 370, 562, 321, 393, 1500, 341, 797, 264, 912, 733, 295, 27778, 2979, 365, 51304], "temperature": 0.0, "avg_logprob": -0.12144527127665858, "compression_ratio": 1.7721518987341771, "no_speech_prob": 0.00297964783385396}, {"id": 696, "seek": 315568, "start": 3175.04, "end": 3178.08, "text": " large number of participants all online all just reading stuff", "tokens": [51332, 2416, 1230, 295, 10503, 439, 2950, 439, 445, 3760, 1507, 51484], "temperature": 0.0, "avg_logprob": -0.12144527127665858, "compression_ratio": 1.7721518987341771, "no_speech_prob": 0.00297964783385396}, {"id": 697, "seek": 315568, "start": 3178.8799999999997, "end": 3182.72, "text": " With different kinds of language expressing exact as well as approximate number", "tokens": [51524, 2022, 819, 3685, 295, 2856, 22171, 1900, 382, 731, 382, 30874, 1230, 51716], "temperature": 0.0, "avg_logprob": -0.12144527127665858, "compression_ratio": 1.7721518987341771, "no_speech_prob": 0.00297964783385396}, {"id": 698, "seek": 318272, "start": 3183.4399999999996, "end": 3188.3999999999996, "text": " Approximate quantifiers logical quantifiers vagueness like the stacks could be tall or very tall", "tokens": [50400, 29551, 87, 2905, 4426, 23463, 14978, 4426, 23463, 13501, 7801, 442, 411, 264, 30792, 727, 312, 6764, 420, 588, 6764, 50648], "temperature": 0.0, "avg_logprob": -0.1889407381098321, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.0017002900131046772}, {"id": 699, "seek": 318272, "start": 3188.7999999999997, "end": 3193.2, "text": " And and different kinds of spatial relations and across many different stimuli", "tokens": [50668, 400, 293, 819, 3685, 295, 23598, 2299, 293, 2108, 867, 819, 47752, 50888], "temperature": 0.0, "avg_logprob": -0.1889407381098321, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.0017002900131046772}, {"id": 700, "seek": 318272, "start": 3193.52, "end": 3198.0, "text": " We we mix up more or less complex sentences using these different kinds of language", "tokens": [50904, 492, 321, 2890, 493, 544, 420, 1570, 3997, 16579, 1228, 613, 819, 3685, 295, 2856, 51128], "temperature": 0.0, "avg_logprob": -0.1889407381098321, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.0017002900131046772}, {"id": 701, "seek": 318272, "start": 3198.64, "end": 3202.24, "text": " And in each case we're using again a relatively small", "tokens": [51160, 400, 294, 1184, 1389, 321, 434, 1228, 797, 257, 7226, 1359, 51340], "temperature": 0.0, "avg_logprob": -0.1889407381098321, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.0017002900131046772}, {"id": 702, "seek": 318272, "start": 3202.64, "end": 3205.8399999999997, "text": " LLM to translate sentence by sentence into", "tokens": [51360, 441, 43, 44, 281, 13799, 8174, 538, 8174, 666, 51520], "temperature": 0.0, "avg_logprob": -0.1889407381098321, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.0017002900131046772}, {"id": 703, "seek": 318272, "start": 3206.3999999999996, "end": 3209.6, "text": " statements for conditioning and querying in our probabilistic language of thought", "tokens": [51548, 12363, 337, 21901, 293, 7083, 1840, 294, 527, 31959, 3142, 2856, 295, 1194, 51708], "temperature": 0.0, "avg_logprob": -0.1889407381098321, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.0017002900131046772}, {"id": 704, "seek": 320960, "start": 3210.0, "end": 3213.8399999999997, "text": " Then we run a small number of mental simulations in this case in a 2d physics engine", "tokens": [50384, 1396, 321, 1190, 257, 1359, 1230, 295, 4973, 35138, 294, 341, 1389, 294, 257, 568, 67, 10649, 2848, 50576], "temperature": 0.0, "avg_logprob": -0.11736026563142475, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0023223895113915205}, {"id": 705, "seek": 320960, "start": 3214.3199999999997, "end": 3219.36, "text": " Compute the outputs and compare those with people and what i'm showing you here is again the same kind of scatterplot", "tokens": [50600, 6620, 1169, 264, 23930, 293, 6794, 729, 365, 561, 293, 437, 741, 478, 4099, 291, 510, 307, 797, 264, 912, 733, 295, 34951, 564, 310, 50852], "temperature": 0.0, "avg_logprob": -0.11736026563142475, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0023223895113915205}, {"id": 706, "seek": 320960, "start": 3219.36, "end": 3226.0, "text": " I showed before but on the vertical axis are again our human judgments and on the x-axis now are the model", "tokens": [50852, 286, 4712, 949, 457, 322, 264, 9429, 10298, 366, 797, 527, 1952, 40337, 293, 322, 264, 2031, 12, 24633, 586, 366, 264, 2316, 51184], "temperature": 0.0, "avg_logprob": -0.11736026563142475, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0023223895113915205}, {"id": 707, "seek": 320960, "start": 3226.16, "end": 3228.96, "text": " Or the predictions of this language informed thinking model", "tokens": [51192, 1610, 264, 21264, 295, 341, 2856, 11740, 1953, 2316, 51332], "temperature": 0.0, "avg_logprob": -0.11736026563142475, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0023223895113915205}, {"id": 708, "seek": 320960, "start": 3229.44, "end": 3235.36, "text": " So it looks a lot like what I showed you before these are the judgments from the battalion all work on the red and yellow task", "tokens": [51356, 407, 309, 1542, 257, 688, 411, 437, 286, 4712, 291, 949, 613, 366, 264, 40337, 490, 264, 9591, 35791, 439, 589, 322, 264, 2182, 293, 5566, 5633, 51652], "temperature": 0.0, "avg_logprob": -0.11736026563142475, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0023223895113915205}, {"id": 709, "seek": 323536, "start": 3235.76, "end": 3239.6, "text": " We're not we're here the model was given a visual scene people were given visual scenes", "tokens": [50384, 492, 434, 406, 321, 434, 510, 264, 2316, 390, 2212, 257, 5056, 4145, 561, 645, 2212, 5056, 8026, 50576], "temperature": 0.0, "avg_logprob": -0.09302605901445661, "compression_ratio": 1.7482758620689656, "no_speech_prob": 0.007574367802590132}, {"id": 710, "seek": 323536, "start": 3239.92, "end": 3244.0, "text": " And the model did that same kind of probabilistic mental simulation in the physics engine", "tokens": [50592, 400, 264, 2316, 630, 300, 912, 733, 295, 31959, 3142, 4973, 16575, 294, 264, 10649, 2848, 50796], "temperature": 0.0, "avg_logprob": -0.09302605901445661, "compression_ratio": 1.7482758620689656, "no_speech_prob": 0.007574367802590132}, {"id": 711, "seek": 323536, "start": 3244.48, "end": 3246.48, "text": " But in the in the current work on the left", "tokens": [50820, 583, 294, 264, 294, 264, 2190, 589, 322, 264, 1411, 50920], "temperature": 0.0, "avg_logprob": -0.09302605901445661, "compression_ratio": 1.7482758620689656, "no_speech_prob": 0.007574367802590132}, {"id": 712, "seek": 323536, "start": 3247.04, "end": 3253.92, "text": " Again, the scene description is constructed from language using the tools that i've shown you by conditioning a prior on scenes", "tokens": [50948, 3764, 11, 264, 4145, 3855, 307, 17083, 490, 2856, 1228, 264, 3873, 300, 741, 600, 4898, 291, 538, 21901, 257, 4059, 322, 8026, 51292], "temperature": 0.0, "avg_logprob": -0.09302605901445661, "compression_ratio": 1.7482758620689656, "no_speech_prob": 0.007574367802590132}, {"id": 713, "seek": 323536, "start": 3254.0, "end": 3259.84, "text": " Okay, and the main point is just that the models fit human judgments pretty well in both cases and about equally well", "tokens": [51296, 1033, 11, 293, 264, 2135, 935, 307, 445, 300, 264, 5245, 3318, 1952, 40337, 1238, 731, 294, 1293, 3331, 293, 466, 12309, 731, 51588], "temperature": 0.0, "avg_logprob": -0.09302605901445661, "compression_ratio": 1.7482758620689656, "no_speech_prob": 0.007574367802590132}, {"id": 714, "seek": 323536, "start": 3260.08, "end": 3262.08, "text": " All right, which is which is interesting", "tokens": [51600, 1057, 558, 11, 597, 307, 597, 307, 1880, 51700], "temperature": 0.0, "avg_logprob": -0.09302605901445661, "compression_ratio": 1.7482758620689656, "no_speech_prob": 0.007574367802590132}, {"id": 715, "seek": 326208, "start": 3262.4, "end": 3266.96, "text": " We can also compare with just a sort of zero shot or a few shot baseline llm", "tokens": [50380, 492, 393, 611, 6794, 365, 445, 257, 1333, 295, 4018, 3347, 420, 257, 1326, 3347, 20518, 287, 75, 76, 50608], "temperature": 0.0, "avg_logprob": -0.1311766582986583, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0012062236201018095}, {"id": 716, "seek": 326208, "start": 3266.96, "end": 3272.7999999999997, "text": " So these are language models, which don't have an explicit mental model of physics or any doing explicit simulation", "tokens": [50608, 407, 613, 366, 2856, 5245, 11, 597, 500, 380, 362, 364, 13691, 4973, 2316, 295, 10649, 420, 604, 884, 13691, 16575, 50900], "temperature": 0.0, "avg_logprob": -0.1311766582986583, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0012062236201018095}, {"id": 717, "seek": 326208, "start": 3272.7999999999997, "end": 3275.84, "text": " And they are are much worse fits to people", "tokens": [50900, 400, 436, 366, 366, 709, 5324, 9001, 281, 561, 51052], "temperature": 0.0, "avg_logprob": -0.1311766582986583, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0012062236201018095}, {"id": 718, "seek": 326208, "start": 3276.08, "end": 3279.52, "text": " Okay, this is this on the what the high up here on the y-axis is", "tokens": [51064, 1033, 11, 341, 307, 341, 322, 264, 437, 264, 1090, 493, 510, 322, 264, 288, 12, 24633, 307, 51236], "temperature": 0.0, "avg_logprob": -0.1311766582986583, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0012062236201018095}, {"id": 719, "seek": 326208, "start": 3279.84, "end": 3284.08, "text": " Distance and distribution from human distribution of responses to the various models", "tokens": [51252, 9840, 719, 293, 7316, 490, 1952, 7316, 295, 13019, 281, 264, 3683, 5245, 51464], "temperature": 0.0, "avg_logprob": -0.1311766582986583, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0012062236201018095}, {"id": 720, "seek": 326208, "start": 3284.08, "end": 3289.6, "text": " The blue one is the rational meeting construction model and you can do a similar thing in the intuitive psychology domain", "tokens": [51464, 440, 3344, 472, 307, 264, 15090, 3440, 6435, 2316, 293, 291, 393, 360, 257, 2531, 551, 294, 264, 21769, 15105, 9274, 51740], "temperature": 0.0, "avg_logprob": -0.1311766582986583, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0012062236201018095}, {"id": 721, "seek": 328960, "start": 3290.4, "end": 3294.96, "text": " Like for example in the settings that I showed you with julien harenger's work", "tokens": [50404, 1743, 337, 1365, 294, 264, 6257, 300, 286, 4712, 291, 365, 30764, 1053, 324, 1095, 1321, 311, 589, 50632], "temperature": 0.0, "avg_logprob": -0.1887215311213057, "compression_ratio": 1.7673611111111112, "no_speech_prob": 0.014949491247534752}, {"id": 722, "seek": 328960, "start": 3295.52, "end": 3300.56, "text": " This is work that uh lans ying and shen did i won't go into the details, but again", "tokens": [50660, 639, 307, 589, 300, 2232, 287, 599, 288, 278, 293, 402, 268, 630, 741, 1582, 380, 352, 666, 264, 4365, 11, 457, 797, 50912], "temperature": 0.0, "avg_logprob": -0.1887215311213057, "compression_ratio": 1.7673611111111112, "no_speech_prob": 0.014949491247534752}, {"id": 723, "seek": 328960, "start": 3300.56, "end": 3303.2799999999997, "text": " We can describe worlds with various", "tokens": [50912, 492, 393, 6786, 13401, 365, 3683, 51048], "temperature": 0.0, "avg_logprob": -0.1887215311213057, "compression_ratio": 1.7673611111111112, "no_speech_prob": 0.014949491247534752}, {"id": 724, "seek": 328960, "start": 3304.0, "end": 3308.16, "text": " Goal objects that an agent might have and constraints like you have to go through doors the doors could be locked", "tokens": [51084, 1037, 304, 6565, 300, 364, 9461, 1062, 362, 293, 18491, 411, 291, 362, 281, 352, 807, 8077, 264, 8077, 727, 312, 9376, 51292], "temperature": 0.0, "avg_logprob": -0.1887215311213057, "compression_ratio": 1.7673611111111112, "no_speech_prob": 0.014949491247534752}, {"id": 725, "seek": 328960, "start": 3308.16, "end": 3310.96, "text": " You have to use keys that could be a red key that could unlock the red door", "tokens": [51292, 509, 362, 281, 764, 9317, 300, 727, 312, 257, 2182, 2141, 300, 727, 11634, 264, 2182, 2853, 51432], "temperature": 0.0, "avg_logprob": -0.1887215311213057, "compression_ratio": 1.7673611111111112, "no_speech_prob": 0.014949491247534752}, {"id": 726, "seek": 328960, "start": 3311.36, "end": 3313.7599999999998, "text": " And so on people could be told either that you know", "tokens": [51452, 400, 370, 322, 561, 727, 312, 1907, 2139, 300, 291, 458, 51572], "temperature": 0.0, "avg_logprob": -0.1887215311213057, "compression_ratio": 1.7673611111111112, "no_speech_prob": 0.014949491247534752}, {"id": 727, "seek": 328960, "start": 3313.92, "end": 3316.88, "text": " You need a key of the right color to open each door of the same color", "tokens": [51580, 509, 643, 257, 2141, 295, 264, 558, 2017, 281, 1269, 1184, 2853, 295, 264, 912, 2017, 51728], "temperature": 0.0, "avg_logprob": -0.1887215311213057, "compression_ratio": 1.7673611111111112, "no_speech_prob": 0.014949491247534752}, {"id": 728, "seek": 331688, "start": 3317.28, "end": 3319.28, "text": " Or you could be told weird things like", "tokens": [50384, 1610, 291, 727, 312, 1907, 3657, 721, 411, 50484], "temperature": 0.0, "avg_logprob": -0.13361322229558772, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.005639917682856321}, {"id": 729, "seek": 331688, "start": 3319.36, "end": 3321.36, "text": " Keys only unlock doors of other colors", "tokens": [50488, 43733, 787, 11634, 8077, 295, 661, 4577, 50588], "temperature": 0.0, "avg_logprob": -0.13361322229558772, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.005639917682856321}, {"id": 730, "seek": 331688, "start": 3321.36, "end": 3325.44, "text": " Okay, um, you know in these kinds of uh worlds again", "tokens": [50588, 1033, 11, 1105, 11, 291, 458, 294, 613, 3685, 295, 2232, 13401, 797, 50792], "temperature": 0.0, "avg_logprob": -0.13361322229558772, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.005639917682856321}, {"id": 731, "seek": 331688, "start": 3325.44, "end": 3329.12, "text": " The rational meeting construction model does a very good job of capturing people's judgments", "tokens": [50792, 440, 15090, 3440, 6435, 2316, 775, 257, 588, 665, 1691, 295, 23384, 561, 311, 40337, 50976], "temperature": 0.0, "avg_logprob": -0.13361322229558772, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.005639917682856321}, {"id": 732, "seek": 331688, "start": 3329.52, "end": 3335.12, "text": " For some kinds of judgments easy cases even gpt 3.5 does reasonably well gpt 4 does better", "tokens": [50996, 1171, 512, 3685, 295, 40337, 1858, 3331, 754, 290, 662, 805, 13, 20, 775, 23551, 731, 290, 662, 1017, 775, 1101, 51276], "temperature": 0.0, "avg_logprob": -0.13361322229558772, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.005639917682856321}, {"id": 733, "seek": 331688, "start": 3335.44, "end": 3340.56, "text": " But as the situation gets more complex or unusual things change and break down like especially if", "tokens": [51292, 583, 382, 264, 2590, 2170, 544, 3997, 420, 10901, 721, 1319, 293, 1821, 760, 411, 2318, 498, 51548], "temperature": 0.0, "avg_logprob": -0.13361322229558772, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.005639917682856321}, {"id": 734, "seek": 331688, "start": 3340.88, "end": 3344.2400000000002, "text": " We say well in this world keys only unlock doors of different colors", "tokens": [51564, 492, 584, 731, 294, 341, 1002, 9317, 787, 11634, 8077, 295, 819, 4577, 51732], "temperature": 0.0, "avg_logprob": -0.13361322229558772, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.005639917682856321}, {"id": 735, "seek": 334424, "start": 3344.3199999999997, "end": 3347.2799999999997, "text": " That's an easy thing to say to somebody and you have to change your mental model", "tokens": [50368, 663, 311, 364, 1858, 551, 281, 584, 281, 2618, 293, 291, 362, 281, 1319, 428, 4973, 2316, 50516], "temperature": 0.0, "avg_logprob": -0.09687670626381571, "compression_ratio": 1.8664596273291925, "no_speech_prob": 0.004069302696734667}, {"id": 736, "seek": 334424, "start": 3347.68, "end": 3355.12, "text": " But when you do something like that gpt 3.5 becomes anti correlated with people and gpt 4 just go drops from being highly correlated to just being a chance", "tokens": [50536, 583, 562, 291, 360, 746, 411, 300, 290, 662, 805, 13, 20, 3643, 6061, 38574, 365, 561, 293, 290, 662, 1017, 445, 352, 11438, 490, 885, 5405, 38574, 281, 445, 885, 257, 2931, 50908], "temperature": 0.0, "avg_logprob": -0.09687670626381571, "compression_ratio": 1.8664596273291925, "no_speech_prob": 0.004069302696734667}, {"id": 737, "seek": 334424, "start": 3355.8399999999997, "end": 3360.9599999999996, "text": " So these are examples of the ways in which you know going out of distribution from our training experience", "tokens": [50944, 407, 613, 366, 5110, 295, 264, 2098, 294, 597, 291, 458, 516, 484, 295, 7316, 490, 527, 3097, 1752, 51200], "temperature": 0.0, "avg_logprob": -0.09687670626381571, "compression_ratio": 1.8664596273291925, "no_speech_prob": 0.004069302696734667}, {"id": 738, "seek": 334424, "start": 3360.9599999999996, "end": 3366.64, "text": " If we're just using a machine learning function approximation approach, which is remarkable as those systems like gpt 4r", "tokens": [51200, 759, 321, 434, 445, 1228, 257, 3479, 2539, 2445, 28023, 3109, 11, 597, 307, 12802, 382, 729, 3652, 411, 290, 662, 1017, 81, 51484], "temperature": 0.0, "avg_logprob": -0.09687670626381571, "compression_ratio": 1.8664596273291925, "no_speech_prob": 0.004069302696734667}, {"id": 739, "seek": 334424, "start": 3366.72, "end": 3368.16, "text": " That's what they're doing", "tokens": [51488, 663, 311, 437, 436, 434, 884, 51560], "temperature": 0.0, "avg_logprob": -0.09687670626381571, "compression_ratio": 1.8664596273291925, "no_speech_prob": 0.004069302696734667}, {"id": 740, "seek": 334424, "start": 3368.16, "end": 3373.6, "text": " But ways in which if the function approximation is just approximating a much more modular translation function", "tokens": [51560, 583, 2098, 294, 597, 498, 264, 2445, 28023, 307, 445, 8542, 990, 257, 709, 544, 31111, 12853, 2445, 51832], "temperature": 0.0, "avg_logprob": -0.09687670626381571, "compression_ratio": 1.8664596273291925, "no_speech_prob": 0.004069302696734667}, {"id": 741, "seek": 337424, "start": 3374.24, "end": 3377.12, "text": " And building on our mental model tools. We can just do much better", "tokens": [50364, 400, 2390, 322, 527, 4973, 2316, 3873, 13, 492, 393, 445, 360, 709, 1101, 50508], "temperature": 0.0, "avg_logprob": -0.12486449673644498, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.00010228878090856597}, {"id": 742, "seek": 337424, "start": 3378.08, "end": 3381.7599999999998, "text": " The last thing i'll just talk about very briefly is you know what is", "tokens": [50556, 440, 1036, 551, 741, 603, 445, 751, 466, 588, 10515, 307, 291, 458, 437, 307, 50740], "temperature": 0.0, "avg_logprob": -0.12486449673644498, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.00010228878090856597}, {"id": 743, "seek": 337424, "start": 3382.3999999999996, "end": 3384.7999999999997, "text": " Any work like this has to raise more interesting questions", "tokens": [50772, 2639, 589, 411, 341, 575, 281, 5300, 544, 1880, 1651, 50892], "temperature": 0.0, "avg_logprob": -0.12486449673644498, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.00010228878090856597}, {"id": 744, "seek": 337424, "start": 3384.8799999999997, "end": 3388.7999999999997, "text": " If if anything like this is right on the right track, then it's it's not answering", "tokens": [50896, 759, 498, 1340, 411, 341, 307, 558, 322, 264, 558, 2837, 11, 550, 309, 311, 309, 311, 406, 13430, 51092], "temperature": 0.0, "avg_logprob": -0.12486449673644498, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.00010228878090856597}, {"id": 745, "seek": 337424, "start": 3389.4399999999996, "end": 3393.6, "text": " At this point the most interesting questions, but just raising them and perhaps highlighting ways you might get at them", "tokens": [51124, 1711, 341, 935, 264, 881, 1880, 1651, 11, 457, 445, 11225, 552, 293, 4317, 26551, 2098, 291, 1062, 483, 412, 552, 51332], "temperature": 0.0, "avg_logprob": -0.12486449673644498, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.00010228878090856597}, {"id": 746, "seek": 337424, "start": 3394.16, "end": 3399.8399999999997, "text": " So in all the work i've been doing in in for a couple of decades now in probabilistic inference in mental models", "tokens": [51360, 407, 294, 439, 264, 589, 741, 600, 668, 884, 294, 294, 337, 257, 1916, 295, 7878, 586, 294, 31959, 3142, 38253, 294, 4973, 5245, 51644], "temperature": 0.0, "avg_logprob": -0.12486449673644498, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.00010228878090856597}, {"id": 747, "seek": 339984, "start": 3400.1600000000003, "end": 3404.6400000000003, "text": " You start by showing how you can do inference with a mental model, but then you have to ask where do you get it from", "tokens": [50380, 509, 722, 538, 4099, 577, 291, 393, 360, 38253, 365, 257, 4973, 2316, 11, 457, 550, 291, 362, 281, 1029, 689, 360, 291, 483, 309, 490, 50604], "temperature": 0.0, "avg_logprob": -0.12502345485963684, "compression_ratio": 1.7971014492753623, "no_speech_prob": 0.008185501210391521}, {"id": 748, "seek": 339984, "start": 3405.1200000000003, "end": 3409.6800000000003, "text": " Now learning from experience is one thing and i've done a lot of work and others in our group and many other", "tokens": [50628, 823, 2539, 490, 1752, 307, 472, 551, 293, 741, 600, 1096, 257, 688, 295, 589, 293, 2357, 294, 527, 1594, 293, 867, 661, 50856], "temperature": 0.0, "avg_logprob": -0.12502345485963684, "compression_ratio": 1.7971014492753623, "no_speech_prob": 0.008185501210391521}, {"id": 749, "seek": 339984, "start": 3409.76, "end": 3417.84, "text": " Colleagues a lot of that book i mentioned with tom griffiths and nick shader is about how we can learn using hierarchical bays and probabilistic programs learn abstract", "tokens": [50860, 4586, 68, 7063, 257, 688, 295, 300, 1446, 741, 2835, 365, 2916, 677, 3661, 355, 82, 293, 15416, 5744, 260, 307, 466, 577, 321, 393, 1466, 1228, 35250, 804, 272, 3772, 293, 31959, 3142, 4268, 1466, 12649, 51264], "temperature": 0.0, "avg_logprob": -0.12502345485963684, "compression_ratio": 1.7971014492753623, "no_speech_prob": 0.008185501210391521}, {"id": 750, "seek": 339984, "start": 3418.4, "end": 3422.7200000000003, "text": " Programs by doing inference over the space of programs to make sense of our data", "tokens": [51292, 44762, 538, 884, 38253, 670, 264, 1901, 295, 4268, 281, 652, 2020, 295, 527, 1412, 51508], "temperature": 0.0, "avg_logprob": -0.12502345485963684, "compression_ratio": 1.7971014492753623, "no_speech_prob": 0.008185501210391521}, {"id": 751, "seek": 339984, "start": 3422.8, "end": 3424.6400000000003, "text": " Maybe even small amounts of data", "tokens": [51512, 2704, 754, 1359, 11663, 295, 1412, 51604], "temperature": 0.0, "avg_logprob": -0.12502345485963684, "compression_ratio": 1.7971014492753623, "no_speech_prob": 0.008185501210391521}, {"id": 752, "seek": 339984, "start": 3424.6400000000003, "end": 3429.6000000000004, "text": " But much of our learning probably the most powerful form of human learning comes through language more abstract", "tokens": [51604, 583, 709, 295, 527, 2539, 1391, 264, 881, 4005, 1254, 295, 1952, 2539, 1487, 807, 2856, 544, 12649, 51852], "temperature": 0.0, "avg_logprob": -0.12502345485963684, "compression_ratio": 1.7971014492753623, "no_speech_prob": 0.008185501210391521}, {"id": 753, "seek": 342984, "start": 3429.92, "end": 3434.08, "text": " Generic language as many folks in cognitive science cognitive development", "tokens": [50368, 15409, 299, 2856, 382, 867, 4024, 294, 15605, 3497, 15605, 3250, 50576], "temperature": 0.0, "avg_logprob": -0.10250294650042499, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.0025085306260734797}, {"id": 754, "seek": 342984, "start": 3434.6400000000003, "end": 3435.76, "text": " Have shown", "tokens": [50604, 3560, 4898, 50660], "temperature": 0.0, "avg_logprob": -0.10250294650042499, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.0025085306260734797}, {"id": 755, "seek": 342984, "start": 3435.76, "end": 3441.36, "text": " So the power of language not only to update your beliefs, but actually to give you new world models is really", "tokens": [50660, 407, 264, 1347, 295, 2856, 406, 787, 281, 5623, 428, 13585, 11, 457, 767, 281, 976, 291, 777, 1002, 5245, 307, 534, 50940], "temperature": 0.0, "avg_logprob": -0.10250294650042499, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.0025085306260734797}, {"id": 756, "seek": 342984, "start": 3441.92, "end": 3447.52, "text": " Incredible and that's probably the most that that's the real human singularity there right is the ability of language to let us", "tokens": [50968, 35261, 293, 300, 311, 1391, 264, 881, 300, 300, 311, 264, 957, 1952, 20010, 507, 456, 558, 307, 264, 3485, 295, 2856, 281, 718, 505, 51248], "temperature": 0.0, "avg_logprob": -0.10250294650042499, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.0025085306260734797}, {"id": 757, "seek": 342984, "start": 3448.1600000000003, "end": 3453.36, "text": " Learn and think about situations that we haven't directly experienced. I mean think about the tug of war for example", "tokens": [51280, 17216, 293, 519, 466, 6851, 300, 321, 2378, 380, 3838, 6751, 13, 286, 914, 519, 466, 264, 33543, 295, 1516, 337, 1365, 51540], "temperature": 0.0, "avg_logprob": -0.10250294650042499, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.0025085306260734797}, {"id": 758, "seek": 342984, "start": 3453.44, "end": 3455.28, "text": " I mean, I don't know if you're like me", "tokens": [51544, 286, 914, 11, 286, 500, 380, 458, 498, 291, 434, 411, 385, 51636], "temperature": 0.0, "avg_logprob": -0.10250294650042499, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.0025085306260734797}, {"id": 759, "seek": 345528, "start": 3455.36, "end": 3457.6800000000003, "text": " I've maybe done one or two tug of war games at my", "tokens": [50368, 286, 600, 1310, 1096, 472, 420, 732, 33543, 295, 1516, 2813, 412, 452, 50484], "temperature": 0.0, "avg_logprob": -0.10132969402876056, "compression_ratio": 1.761437908496732, "no_speech_prob": 0.02929205261170864}, {"id": 760, "seek": 345528, "start": 3458.32, "end": 3462.7200000000003, "text": " Most of my knowledge about tug of war or my beliefs at least don't come from my direct experience", "tokens": [50516, 4534, 295, 452, 3601, 466, 33543, 295, 1516, 420, 452, 13585, 412, 1935, 500, 380, 808, 490, 452, 2047, 1752, 50736], "temperature": 0.0, "avg_logprob": -0.10132969402876056, "compression_ratio": 1.761437908496732, "no_speech_prob": 0.02929205261170864}, {"id": 761, "seek": 345528, "start": 3463.0400000000004, "end": 3466.32, "text": " They come from things maybe people told me about analogous situations", "tokens": [50752, 814, 808, 490, 721, 1310, 561, 1907, 385, 466, 16660, 563, 6851, 50916], "temperature": 0.0, "avg_logprob": -0.10132969402876056, "compression_ratio": 1.761437908496732, "no_speech_prob": 0.02929205261170864}, {"id": 762, "seek": 345528, "start": 3466.7200000000003, "end": 3472.2400000000002, "text": " And and more generally many of our mental models and our intuitive theories come from what people tell us", "tokens": [50936, 400, 293, 544, 5101, 867, 295, 527, 4973, 5245, 293, 527, 21769, 13667, 808, 490, 437, 561, 980, 505, 51212], "temperature": 0.0, "avg_logprob": -0.10132969402876056, "compression_ratio": 1.761437908496732, "no_speech_prob": 0.02929205261170864}, {"id": 763, "seek": 345528, "start": 3472.4, "end": 3472.88, "text": " All right", "tokens": [51220, 1057, 558, 51244], "temperature": 0.0, "avg_logprob": -0.10132969402876056, "compression_ratio": 1.761437908496732, "no_speech_prob": 0.02929205261170864}, {"id": 764, "seek": 345528, "start": 3472.88, "end": 3479.6000000000004, "text": " So the same approach that we talked about for updating beliefs from language can also be used to acquire new mental models", "tokens": [51244, 407, 264, 912, 3109, 300, 321, 2825, 466, 337, 25113, 13585, 490, 2856, 393, 611, 312, 1143, 281, 20001, 777, 4973, 5245, 51580], "temperature": 0.0, "avg_logprob": -0.10132969402876056, "compression_ratio": 1.761437908496732, "no_speech_prob": 0.02929205261170864}, {"id": 765, "seek": 345528, "start": 3480.2400000000002, "end": 3483.92, "text": " But here what's going on is we're we're modeling how somebody might explain to you", "tokens": [51612, 583, 510, 437, 311, 516, 322, 307, 321, 434, 321, 434, 15983, 577, 2618, 1062, 2903, 281, 291, 51796], "temperature": 0.0, "avg_logprob": -0.10132969402876056, "compression_ratio": 1.761437908496732, "no_speech_prob": 0.02929205261170864}, {"id": 766, "seek": 348392, "start": 3484.32, "end": 3491.12, "text": " The way this tug of war works and in fact in our experiments with humans and in the in the toby gerstenberg and noah goodman and", "tokens": [50384, 440, 636, 341, 33543, 295, 1516, 1985, 293, 294, 1186, 294, 527, 12050, 365, 6255, 293, 294, 264, 294, 264, 281, 2322, 5713, 6266, 6873, 293, 572, 545, 665, 1601, 293, 50724], "temperature": 0.0, "avg_logprob": -0.20329955772117334, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.0025893589481711388}, {"id": 767, "seek": 348392, "start": 3492.16, "end": 3493.6800000000003, "text": " colleagues did", "tokens": [50776, 7734, 630, 50852], "temperature": 0.0, "avg_logprob": -0.20329955772117334, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.0025893589481711388}, {"id": 768, "seek": 348392, "start": 3493.6800000000003, "end": 3495.6800000000003, "text": " This is exactly what we do would tell people", "tokens": [50852, 639, 307, 2293, 437, 321, 360, 576, 980, 561, 50952], "temperature": 0.0, "avg_logprob": -0.20329955772117334, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.0025893589481711388}, {"id": 769, "seek": 348392, "start": 3496.2400000000002, "end": 3500.0, "text": " Um about how this work people have various strengths it can vary from person to person", "tokens": [50980, 3301, 466, 577, 341, 589, 561, 362, 3683, 16986, 309, 393, 10559, 490, 954, 281, 954, 51168], "temperature": 0.0, "avg_logprob": -0.20329955772117334, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.0025893589481711388}, {"id": 770, "seek": 348392, "start": 3500.48, "end": 3502.8, "text": " And then now we're using the llm to translate", "tokens": [51192, 400, 550, 586, 321, 434, 1228, 264, 287, 75, 76, 281, 13799, 51308], "temperature": 0.0, "avg_logprob": -0.20329955772117334, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.0025893589481711388}, {"id": 771, "seek": 348392, "start": 3503.44, "end": 3508.56, "text": " Those say sentences in english that are generic sentences about the domain describing the world model", "tokens": [51340, 3950, 584, 16579, 294, 32169, 300, 366, 19577, 16579, 466, 264, 9274, 16141, 264, 1002, 2316, 51596], "temperature": 0.0, "avg_logprob": -0.20329955772117334, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.0025893589481711388}, {"id": 772, "seek": 348392, "start": 3508.64, "end": 3510.64, "text": " We want our participants to use", "tokens": [51600, 492, 528, 527, 10503, 281, 764, 51700], "temperature": 0.0, "avg_logprob": -0.20329955772117334, "compression_ratio": 1.6605839416058394, "no_speech_prob": 0.0025893589481711388}, {"id": 773, "seek": 351064, "start": 3511.04, "end": 3515.12, "text": " Into the same kind of probabilistic program code, but now these are defined statements", "tokens": [50384, 23373, 264, 912, 733, 295, 31959, 3142, 1461, 3089, 11, 457, 586, 613, 366, 7642, 12363, 50588], "temperature": 0.0, "avg_logprob": -0.09941394909008129, "compression_ratio": 1.8467153284671534, "no_speech_prob": 0.005382840055972338}, {"id": 774, "seek": 351064, "start": 3515.2, "end": 3520.96, "text": " So these are not these don't condition on a specific a specific world, but they define the general distribution on worlds", "tokens": [50592, 407, 613, 366, 406, 613, 500, 380, 4188, 322, 257, 2685, 257, 2685, 1002, 11, 457, 436, 6964, 264, 2674, 7316, 322, 13401, 50880], "temperature": 0.0, "avg_logprob": -0.09941394909008129, "compression_ratio": 1.8467153284671534, "no_speech_prob": 0.005382840055972338}, {"id": 775, "seek": 351064, "start": 3521.2, "end": 3523.2, "text": " But again, they're contextual they're distributional", "tokens": [50892, 583, 797, 11, 436, 434, 35526, 436, 434, 7316, 304, 50992], "temperature": 0.0, "avg_logprob": -0.09941394909008129, "compression_ratio": 1.8467153284671534, "no_speech_prob": 0.005382840055972338}, {"id": 776, "seek": 351064, "start": 3523.44, "end": 3526.4, "text": " There could be different ways of understanding the vagueness in language", "tokens": [51004, 821, 727, 312, 819, 2098, 295, 3701, 264, 13501, 7801, 442, 294, 2856, 51152], "temperature": 0.0, "avg_logprob": -0.09941394909008129, "compression_ratio": 1.8467153284671534, "no_speech_prob": 0.005382840055972338}, {"id": 777, "seek": 351064, "start": 3526.72, "end": 3531.68, "text": " But the basic idea is that we can describe a world model in english and then the code llm", "tokens": [51168, 583, 264, 3875, 1558, 307, 300, 321, 393, 6786, 257, 1002, 2316, 294, 32169, 293, 550, 264, 3089, 287, 75, 76, 51416], "temperature": 0.0, "avg_logprob": -0.09941394909008129, "compression_ratio": 1.8467153284671534, "no_speech_prob": 0.005382840055972338}, {"id": 778, "seek": 351064, "start": 3532.48, "end": 3536.4, "text": " Can construct that world model. So all the defined statements now are constructed", "tokens": [51456, 1664, 7690, 300, 1002, 2316, 13, 407, 439, 264, 7642, 12363, 586, 366, 17083, 51652], "temperature": 0.0, "avg_logprob": -0.09941394909008129, "compression_ratio": 1.8467153284671534, "no_speech_prob": 0.005382840055972338}, {"id": 779, "seek": 353640, "start": 3536.48, "end": 3538.64, "text": " They're not exactly the ones that we use in the original paper", "tokens": [50368, 814, 434, 406, 2293, 264, 2306, 300, 321, 764, 294, 264, 3380, 3035, 50476], "temperature": 0.0, "avg_logprob": -0.12729523928110836, "compression_ratio": 1.7470238095238095, "no_speech_prob": 0.04666190966963768}, {"id": 780, "seek": 353640, "start": 3538.64, "end": 3542.1600000000003, "text": " But they have the same functional role and they can support the same kinds of inferences", "tokens": [50476, 583, 436, 362, 264, 912, 11745, 3090, 293, 436, 393, 1406, 264, 912, 3685, 295, 13596, 2667, 50652], "temperature": 0.0, "avg_logprob": -0.12729523928110836, "compression_ratio": 1.7470238095238095, "no_speech_prob": 0.04666190966963768}, {"id": 781, "seek": 353640, "start": 3542.56, "end": 3548.8, "text": " So it's just just showing you the way we can I think go towards one of the most interesting ways that language informs our thinking", "tokens": [50672, 407, 309, 311, 445, 445, 4099, 291, 264, 636, 321, 393, 286, 519, 352, 3030, 472, 295, 264, 881, 1880, 2098, 300, 2856, 45320, 527, 1953, 50984], "temperature": 0.0, "avg_logprob": -0.12729523928110836, "compression_ratio": 1.7470238095238095, "no_speech_prob": 0.04666190966963768}, {"id": 782, "seek": 353640, "start": 3549.12, "end": 3552.56, "text": " Not just as a way to convey specific beliefs about situations", "tokens": [51000, 1726, 445, 382, 257, 636, 281, 16965, 2685, 13585, 466, 6851, 51172], "temperature": 0.0, "avg_logprob": -0.12729523928110836, "compression_ratio": 1.7470238095238095, "no_speech_prob": 0.04666190966963768}, {"id": 783, "seek": 353640, "start": 3552.56, "end": 3558.32, "text": " But new but new world models and in some recent most recent work that's still in progress with tyler brook wilson", "tokens": [51172, 583, 777, 457, 777, 1002, 5245, 293, 294, 512, 5162, 881, 5162, 589, 300, 311, 920, 294, 4205, 365, 1104, 1918, 2006, 453, 20501, 3015, 51460], "temperature": 0.0, "avg_logprob": -0.12729523928110836, "compression_ratio": 1.7470238095238095, "no_speech_prob": 0.04666190966963768}, {"id": 784, "seek": 353640, "start": 3559.04, "end": 3565.6, "text": " Katie collins and a number of the others I mentioned here tyler is a is a brilliant philosopher who recently graduated from MIT", "tokens": [51496, 19602, 1263, 1292, 293, 257, 1230, 295, 264, 2357, 286, 2835, 510, 1104, 1918, 307, 257, 307, 257, 10248, 29805, 567, 3938, 13693, 490, 13100, 51824], "temperature": 0.0, "avg_logprob": -0.12729523928110836, "compression_ratio": 1.7470238095238095, "no_speech_prob": 0.04666190966963768}, {"id": 785, "seek": 356560, "start": 3566.24, "end": 3572.72, "text": " And he's actually just just accepted a faculty job at Yale. So he'll be at Yale in a in a year or so", "tokens": [50396, 400, 415, 311, 767, 445, 445, 9035, 257, 6389, 1691, 412, 26711, 13, 407, 415, 603, 312, 412, 26711, 294, 257, 294, 257, 1064, 420, 370, 50720], "temperature": 0.0, "avg_logprob": -0.11379113439786231, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0020177511032670736}, {"id": 786, "seek": 356560, "start": 3574.08, "end": 3576.08, "text": " But together with with tyler and others", "tokens": [50788, 583, 1214, 365, 365, 1104, 1918, 293, 2357, 50888], "temperature": 0.0, "avg_logprob": -0.11379113439786231, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0020177511032670736}, {"id": 787, "seek": 356560, "start": 3576.56, "end": 3579.2, "text": " And tyler's thesis goes into some early stages of this", "tokens": [50912, 400, 1104, 1918, 311, 22288, 1709, 666, 512, 2440, 10232, 295, 341, 51044], "temperature": 0.0, "avg_logprob": -0.11379113439786231, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0020177511032670736}, {"id": 788, "seek": 356560, "start": 3579.68, "end": 3584.0, "text": " We've talked about we've been exploring the ways that the same approach can support", "tokens": [51068, 492, 600, 2825, 466, 321, 600, 668, 12736, 264, 2098, 300, 264, 912, 3109, 393, 1406, 51284], "temperature": 0.0, "avg_logprob": -0.11379113439786231, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0020177511032670736}, {"id": 789, "seek": 356560, "start": 3584.64, "end": 3588.48, "text": " Constructing new models, even if I don't explicitly tell you in language how the world works", "tokens": [51316, 8574, 1757, 278, 777, 5245, 11, 754, 498, 286, 500, 380, 20803, 980, 291, 294, 2856, 577, 264, 1002, 1985, 51508], "temperature": 0.0, "avg_logprob": -0.11379113439786231, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0020177511032670736}, {"id": 790, "seek": 356560, "start": 3588.48, "end": 3594.88, "text": " But just by again using your associative memory and marshaling implicit knowledge to construct a model of a new situation", "tokens": [51508, 583, 445, 538, 797, 1228, 428, 4180, 1166, 4675, 293, 21653, 4270, 26947, 3601, 281, 7690, 257, 2316, 295, 257, 777, 2590, 51828], "temperature": 0.0, "avg_logprob": -0.11379113439786231, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0020177511032670736}, {"id": 791, "seek": 359488, "start": 3595.28, "end": 3598.96, "text": " So I could tell you about a relay race and I'm not tell the relay race is like a new domain", "tokens": [50384, 407, 286, 727, 980, 291, 466, 257, 24214, 4569, 293, 286, 478, 406, 980, 264, 24214, 4569, 307, 411, 257, 777, 9274, 50568], "temperature": 0.0, "avg_logprob": -0.10094395760566957, "compression_ratio": 1.738831615120275, "no_speech_prob": 0.0014099833788350224}, {"id": 792, "seek": 359488, "start": 3598.96, "end": 3601.92, "text": " I'm not really telling you how races work. I'm just giving you some information", "tokens": [50568, 286, 478, 406, 534, 3585, 291, 577, 15484, 589, 13, 286, 478, 445, 2902, 291, 512, 1589, 50716], "temperature": 0.0, "avg_logprob": -0.10094395760566957, "compression_ratio": 1.738831615120275, "no_speech_prob": 0.0014099833788350224}, {"id": 793, "seek": 359488, "start": 3602.48, "end": 3604.88, "text": " But we're exploring ways in which the LLM can", "tokens": [50744, 583, 321, 434, 12736, 2098, 294, 597, 264, 441, 43, 44, 393, 50864], "temperature": 0.0, "avg_logprob": -0.10094395760566957, "compression_ratio": 1.738831615120275, "no_speech_prob": 0.0014099833788350224}, {"id": 794, "seek": 359488, "start": 3605.52, "end": 3608.6400000000003, "text": " Can be queried to construct possible background knowledge", "tokens": [50896, 1664, 312, 7083, 1091, 281, 7690, 1944, 3678, 3601, 51052], "temperature": 0.0, "avg_logprob": -0.10094395760566957, "compression_ratio": 1.738831615120275, "no_speech_prob": 0.0014099833788350224}, {"id": 795, "seek": 359488, "start": 3609.12, "end": 3613.04, "text": " And write probabilistic program code that can be suitable for reasoning about this domain", "tokens": [51076, 400, 2464, 31959, 3142, 1461, 3089, 300, 393, 312, 12873, 337, 21577, 466, 341, 9274, 51272], "temperature": 0.0, "avg_logprob": -0.10094395760566957, "compression_ratio": 1.738831615120275, "no_speech_prob": 0.0014099833788350224}, {"id": 796, "seek": 359488, "start": 3613.36, "end": 3614.8, "text": " Just on its own the LLM isn't enough", "tokens": [51288, 1449, 322, 1080, 1065, 264, 441, 43, 44, 1943, 380, 1547, 51360], "temperature": 0.0, "avg_logprob": -0.10094395760566957, "compression_ratio": 1.738831615120275, "no_speech_prob": 0.0014099833788350224}, {"id": 797, "seek": 359488, "start": 3614.8, "end": 3618.2400000000002, "text": " But you have to do some reasoning about the models that it suggests", "tokens": [51360, 583, 291, 362, 281, 360, 512, 21577, 466, 264, 5245, 300, 309, 13409, 51532], "temperature": 0.0, "avg_logprob": -0.10094395760566957, "compression_ratio": 1.738831615120275, "no_speech_prob": 0.0014099833788350224}, {"id": 798, "seek": 359488, "start": 3618.8, "end": 3620.88, "text": " And then those models when suitably", "tokens": [51560, 400, 550, 729, 5245, 562, 5722, 1188, 51664], "temperature": 0.0, "avg_logprob": -0.10094395760566957, "compression_ratio": 1.738831615120275, "no_speech_prob": 0.0014099833788350224}, {"id": 799, "seek": 362088, "start": 3621.6800000000003, "end": 3627.28, "text": " Reason to put upon can support novel reasoning in this domain and even sensible updating", "tokens": [50404, 39693, 281, 829, 3564, 393, 1406, 7613, 21577, 294, 341, 9274, 293, 754, 25380, 25113, 50684], "temperature": 0.0, "avg_logprob": -0.12980592250823975, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.013215070590376854}, {"id": 800, "seek": 362088, "start": 3627.28, "end": 3631.6, "text": " So we're really getting you know, at least demos of steps towards", "tokens": [50684, 407, 321, 434, 534, 1242, 291, 458, 11, 412, 1935, 33788, 295, 4439, 3030, 50900], "temperature": 0.0, "avg_logprob": -0.12980592250823975, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.013215070590376854}, {"id": 801, "seek": 362088, "start": 3632.2400000000002, "end": 3637.04, "text": " Computational models that can capture the richness of how we are able to think about new situations", "tokens": [50932, 37804, 1478, 5245, 300, 393, 7983, 264, 44506, 295, 577, 321, 366, 1075, 281, 519, 466, 777, 6851, 51172], "temperature": 0.0, "avg_logprob": -0.12980592250823975, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.013215070590376854}, {"id": 802, "seek": 362088, "start": 3637.04, "end": 3642.96, "text": " Even ones that we haven't really thought about very much before or that we haven't been explicitly told how to think about", "tokens": [51172, 2754, 2306, 300, 321, 2378, 380, 534, 1194, 466, 588, 709, 949, 420, 300, 321, 2378, 380, 668, 20803, 1907, 577, 281, 519, 466, 51468], "temperature": 0.0, "avg_logprob": -0.12980592250823975, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.013215070590376854}, {"id": 803, "seek": 362088, "start": 3643.44, "end": 3645.44, "text": " but using the combination of", "tokens": [51492, 457, 1228, 264, 6562, 295, 51592], "temperature": 0.0, "avg_logprob": -0.12980592250823975, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.013215070590376854}, {"id": 804, "seek": 364544, "start": 3646.08, "end": 3652.64, "text": " Language the associative knowledge that's in it and an underlying ability for constructing probabilistic models of the world and updating", "tokens": [50396, 24445, 264, 4180, 1166, 3601, 300, 311, 294, 309, 293, 364, 14217, 3485, 337, 39969, 31959, 3142, 5245, 295, 264, 1002, 293, 25113, 50724], "temperature": 0.0, "avg_logprob": -0.1679553600272747, "compression_ratio": 1.6929133858267718, "no_speech_prob": 0.01032334379851818}, {"id": 805, "seek": 364544, "start": 3653.36, "end": 3656.48, "text": " I'm the last thing I just want to leave you with is a set of thoughts about", "tokens": [50760, 286, 478, 264, 1036, 551, 286, 445, 528, 281, 1856, 291, 365, 307, 257, 992, 295, 4598, 466, 50916], "temperature": 0.0, "avg_logprob": -0.1679553600272747, "compression_ratio": 1.6929133858267718, "no_speech_prob": 0.01032334379851818}, {"id": 806, "seek": 364544, "start": 3657.44, "end": 3659.44, "text": " You know, ridiculously", "tokens": [50964, 509, 458, 11, 41358, 51064], "temperature": 0.0, "avg_logprob": -0.1679553600272747, "compression_ratio": 1.6929133858267718, "no_speech_prob": 0.01032334379851818}, {"id": 807, "seek": 364544, "start": 3660.56, "end": 3662.4, "text": " It is it is the last thing", "tokens": [51120, 467, 307, 309, 307, 264, 1036, 551, 51212], "temperature": 0.0, "avg_logprob": -0.1679553600272747, "compression_ratio": 1.6929133858267718, "no_speech_prob": 0.01032334379851818}, {"id": 808, "seek": 364544, "start": 3662.4, "end": 3666.16, "text": " But I think this will set up some of the discussion with with virginia and others too", "tokens": [51212, 583, 286, 519, 341, 486, 992, 493, 512, 295, 264, 5017, 365, 365, 26404, 654, 293, 2357, 886, 51400], "temperature": 0.0, "avg_logprob": -0.1679553600272747, "compression_ratio": 1.6929133858267718, "no_speech_prob": 0.01032334379851818}, {"id": 809, "seek": 364544, "start": 3666.88, "end": 3672.56, "text": " Which is many people in cognitive science, whether in linguistics or other areas", "tokens": [51436, 3013, 307, 867, 561, 294, 15605, 3497, 11, 1968, 294, 21766, 6006, 420, 661, 3179, 51720], "temperature": 0.0, "avg_logprob": -0.1679553600272747, "compression_ratio": 1.6929133858267718, "no_speech_prob": 0.01032334379851818}, {"id": 810, "seek": 367256, "start": 3672.96, "end": 3679.52, "text": " Engaged in meaning have been interested in what you could call, you know a unified account of meaning and what we're trying to at least", "tokens": [50384, 2469, 2980, 294, 3620, 362, 668, 3102, 294, 437, 291, 727, 818, 11, 291, 458, 257, 26787, 2696, 295, 3620, 293, 437, 321, 434, 1382, 281, 412, 1935, 50712], "temperature": 0.0, "avg_logprob": -0.1684476801779418, "compression_ratio": 1.7007042253521127, "no_speech_prob": 0.010980639606714249}, {"id": 811, "seek": 367256, "start": 3679.92, "end": 3683.2799999999997, "text": " Point towards steps towards this with the framework. I talked about", "tokens": [50732, 12387, 3030, 4439, 3030, 341, 365, 264, 8388, 13, 286, 2825, 466, 50900], "temperature": 0.0, "avg_logprob": -0.1684476801779418, "compression_ratio": 1.7007042253521127, "no_speech_prob": 0.010980639606714249}, {"id": 812, "seek": 367256, "start": 3684.48, "end": 3686.48, "text": " just to just to again", "tokens": [50960, 445, 281, 445, 281, 797, 51060], "temperature": 0.0, "avg_logprob": -0.1684476801779418, "compression_ratio": 1.7007042253521127, "no_speech_prob": 0.010980639606714249}, {"id": 813, "seek": 367256, "start": 3687.2, "end": 3689.36, "text": " Raise controversial points for discussion if you like", "tokens": [51096, 30062, 17323, 2793, 337, 5017, 498, 291, 411, 51204], "temperature": 0.0, "avg_logprob": -0.1684476801779418, "compression_ratio": 1.7007042253521127, "no_speech_prob": 0.010980639606714249}, {"id": 814, "seek": 367256, "start": 3689.84, "end": 3695.84, "text": " um, but the idea that we can capture the meaning of a word in context as well as more generally as", "tokens": [51228, 1105, 11, 457, 264, 1558, 300, 321, 393, 7983, 264, 3620, 295, 257, 1349, 294, 4319, 382, 731, 382, 544, 5101, 382, 51528], "temperature": 0.0, "avg_logprob": -0.1684476801779418, "compression_ratio": 1.7007042253521127, "no_speech_prob": 0.010980639606714249}, {"id": 815, "seek": 367256, "start": 3696.4, "end": 3700.0, "text": " Effectively, it's like you might think of it as a form of dynamic semantics if you're familiar with this", "tokens": [51556, 17764, 3413, 11, 309, 311, 411, 291, 1062, 519, 295, 309, 382, 257, 1254, 295, 8546, 4361, 45298, 498, 291, 434, 4963, 365, 341, 51736], "temperature": 0.0, "avg_logprob": -0.1684476801779418, "compression_ratio": 1.7007042253521127, "no_speech_prob": 0.010980639606714249}, {"id": 816, "seek": 370000, "start": 3700.08, "end": 3706.72, "text": " But the meaning being in in context in a discourse the incremental contribution to the probability distribution over", "tokens": [50368, 583, 264, 3620, 885, 294, 294, 4319, 294, 257, 23938, 264, 35759, 13150, 281, 264, 8482, 7316, 670, 50700], "temperature": 0.0, "avg_logprob": -0.1372978946950176, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.0037633725441992283}, {"id": 817, "seek": 370000, "start": 3707.2, "end": 3711.36, "text": " PLOT expressions in the problem that we're thinking about with a problem in discussion", "tokens": [50724, 6999, 5068, 15277, 294, 264, 1154, 300, 321, 434, 1953, 466, 365, 257, 1154, 294, 5017, 50932], "temperature": 0.0, "avg_logprob": -0.1372978946950176, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.0037633725441992283}, {"id": 818, "seek": 370000, "start": 3711.76, "end": 3715.6, "text": " And the meaning of a word or phrase or sentence or other unit of language in general", "tokens": [50952, 400, 264, 3620, 295, 257, 1349, 420, 9535, 420, 8174, 420, 661, 4985, 295, 2856, 294, 2674, 51144], "temperature": 0.0, "avg_logprob": -0.1372978946950176, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.0037633725441992283}, {"id": 819, "seek": 370000, "start": 3715.84, "end": 3722.4, "text": " Is a is a higher order stochastic function that can take as input a discourse context and return as output a meaning in context", "tokens": [51156, 1119, 257, 307, 257, 2946, 1668, 342, 8997, 2750, 2445, 300, 393, 747, 382, 4846, 257, 23938, 4319, 293, 2736, 382, 5598, 257, 3620, 294, 4319, 51484], "temperature": 0.0, "avg_logprob": -0.1372978946950176, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.0037633725441992283}, {"id": 820, "seek": 370000, "start": 3723.04, "end": 3725.6, "text": " And the idea is that if we think about the different approaches", "tokens": [51516, 400, 264, 1558, 307, 300, 498, 321, 519, 466, 264, 819, 11587, 51644], "temperature": 0.0, "avg_logprob": -0.1372978946950176, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.0037633725441992283}, {"id": 821, "seek": 372560, "start": 3725.68, "end": 3726.88, "text": " These are just four", "tokens": [50368, 1981, 366, 445, 1451, 50428], "temperature": 0.0, "avg_logprob": -0.10514249721495043, "compression_ratio": 1.7881619937694704, "no_speech_prob": 0.03208416700363159}, {"id": 822, "seek": 372560, "start": 3726.88, "end": 3733.2799999999997, "text": " You know traditional ways of thinking about meaning and language which all have great value and have often been seen as being competitors", "tokens": [50428, 509, 458, 5164, 2098, 295, 1953, 466, 3620, 293, 2856, 597, 439, 362, 869, 2158, 293, 362, 2049, 668, 1612, 382, 885, 18333, 50748], "temperature": 0.0, "avg_logprob": -0.10514249721495043, "compression_ratio": 1.7881619937694704, "no_speech_prob": 0.03208416700363159}, {"id": 823, "seek": 372560, "start": 3733.7599999999998, "end": 3738.0, "text": " Ways in which we can really bring them together ways in which the PLOT ideas that we've talked about", "tokens": [50772, 343, 3772, 294, 597, 321, 393, 534, 1565, 552, 1214, 2098, 294, 597, 264, 6999, 5068, 3487, 300, 321, 600, 2825, 466, 50984], "temperature": 0.0, "avg_logprob": -0.10514249721495043, "compression_ratio": 1.7881619937694704, "no_speech_prob": 0.03208416700363159}, {"id": 824, "seek": 372560, "start": 3738.3199999999997, "end": 3745.04, "text": " Can integrate the compositional logical aspects of meaning that formal semantics and other areas in the language of thought tradition have emphasized", "tokens": [51000, 1664, 13365, 264, 10199, 2628, 14978, 7270, 295, 3620, 300, 9860, 4361, 45298, 293, 661, 3179, 294, 264, 2856, 295, 1194, 6994, 362, 34068, 51336], "temperature": 0.0, "avg_logprob": -0.10514249721495043, "compression_ratio": 1.7881619937694704, "no_speech_prob": 0.03208416700363159}, {"id": 825, "seek": 372560, "start": 3745.52, "end": 3751.6, "text": " As well as in the context of probabilistic language of thought and mental models of the world can give a powerful form of grounding", "tokens": [51360, 1018, 731, 382, 294, 264, 4319, 295, 31959, 3142, 2856, 295, 1194, 293, 4973, 5245, 295, 264, 1002, 393, 976, 257, 4005, 1254, 295, 46727, 51664], "temperature": 0.0, "avg_logprob": -0.10514249721495043, "compression_ratio": 1.7881619937694704, "no_speech_prob": 0.03208416700363159}, {"id": 826, "seek": 372560, "start": 3751.68, "end": 3753.68, "text": " That's not grounded in sense data", "tokens": [51668, 663, 311, 406, 23535, 294, 2020, 1412, 51768], "temperature": 0.0, "avg_logprob": -0.10514249721495043, "compression_ratio": 1.7881619937694704, "no_speech_prob": 0.03208416700363159}, {"id": 827, "seek": 375368, "start": 3753.7599999999998, "end": 3760.3199999999997, "text": " But in our models of the world that's what the PLOT does and the LLM or more generally a statistical distributional", "tokens": [50368, 583, 294, 527, 5245, 295, 264, 1002, 300, 311, 437, 264, 6999, 5068, 775, 293, 264, 441, 43, 44, 420, 544, 5101, 257, 22820, 7316, 304, 50696], "temperature": 0.0, "avg_logprob": -0.12346318897448089, "compression_ratio": 1.746031746031746, "no_speech_prob": 0.0012632958823814988}, {"id": 828, "seek": 375368, "start": 3760.72, "end": 3766.64, "text": " Sequence models can capture both the distributional statistical aspects of meaning. It's the base that the", "tokens": [50716, 46859, 655, 5245, 393, 7983, 1293, 264, 7316, 304, 22820, 7270, 295, 3620, 13, 467, 311, 264, 3096, 300, 264, 51012], "temperature": 0.0, "avg_logprob": -0.12346318897448089, "compression_ratio": 1.746031746031746, "no_speech_prob": 0.0012632958823814988}, {"id": 829, "seek": 375368, "start": 3767.68, "end": 3773.44, "text": " Both the sort of distributional usage approach, but also more general semantic association needed to make sense of language", "tokens": [51064, 6767, 264, 1333, 295, 7316, 304, 14924, 3109, 11, 457, 611, 544, 2674, 47982, 14598, 2978, 281, 652, 2020, 295, 2856, 51352], "temperature": 0.0, "avg_logprob": -0.12346318897448089, "compression_ratio": 1.746031746031746, "no_speech_prob": 0.0012632958823814988}, {"id": 830, "seek": 375368, "start": 3773.44, "end": 3778.08, "text": " So flexibly as well as some of the very flexible pragmatic communicative ways we use language", "tokens": [51352, 407, 5896, 3545, 382, 731, 382, 512, 295, 264, 588, 11358, 46904, 3363, 1166, 2098, 321, 764, 2856, 51584], "temperature": 0.0, "avg_logprob": -0.12346318897448089, "compression_ratio": 1.746031746031746, "no_speech_prob": 0.0012632958823814988}, {"id": 831, "seek": 377808, "start": 3778.7999999999997, "end": 3784.4, "text": " So I'll just leave it at that. Okay. Okay. Could you turn off your share, please? Yes", "tokens": [50400, 407, 286, 603, 445, 1856, 309, 412, 300, 13, 1033, 13, 1033, 13, 7497, 291, 1261, 766, 428, 2073, 11, 1767, 30, 1079, 50680], "temperature": 0.0, "avg_logprob": -0.15093085471163975, "compression_ratio": 1.4794520547945205, "no_speech_prob": 0.005622558761388063}, {"id": 832, "seek": 377808, "start": 3785.92, "end": 3787.92, "text": " First of all, thank you very much", "tokens": [50756, 2386, 295, 439, 11, 1309, 291, 588, 709, 50856], "temperature": 0.0, "avg_logprob": -0.15093085471163975, "compression_ratio": 1.4794520547945205, "no_speech_prob": 0.005622558761388063}, {"id": 833, "seek": 377808, "start": 3792.24, "end": 3796.88, "text": " There's not a chance in the world that I'm going to forget Virginia this time and I'll tell you why", "tokens": [51072, 821, 311, 406, 257, 2931, 294, 264, 1002, 300, 286, 478, 516, 281, 2870, 10956, 341, 565, 293, 286, 603, 980, 291, 983, 51304], "temperature": 0.0, "avg_logprob": -0.15093085471163975, "compression_ratio": 1.4794520547945205, "no_speech_prob": 0.005622558761388063}, {"id": 834, "seek": 377808, "start": 3797.6, "end": 3804.7999999999997, "text": " Because I'm gobsmacked at how many different areas you're an expert in and so anyway, here's another one", "tokens": [51340, 1436, 286, 478, 352, 929, 76, 25949, 412, 577, 867, 819, 3179, 291, 434, 364, 5844, 294, 293, 370, 4033, 11, 510, 311, 1071, 472, 51700], "temperature": 0.0, "avg_logprob": -0.15093085471163975, "compression_ratio": 1.4794520547945205, "no_speech_prob": 0.005622558761388063}, {"id": 835, "seek": 380480, "start": 3805.2000000000003, "end": 3806.32, "text": " uh", "tokens": [50384, 2232, 50440], "temperature": 0.0, "avg_logprob": -0.22067954089190508, "compression_ratio": 1.3954802259887005, "no_speech_prob": 0.0006456825649365783}, {"id": 836, "seek": 380480, "start": 3806.32, "end": 3808.32, "text": " Virginia valium from", "tokens": [50440, 10956, 1323, 2197, 490, 50540], "temperature": 0.0, "avg_logprob": -0.22067954089190508, "compression_ratio": 1.3954802259887005, "no_speech_prob": 0.0006456825649365783}, {"id": 837, "seek": 380480, "start": 3809.04, "end": 3810.96, "text": " CUNY hunter", "tokens": [50576, 383, 3979, 56, 22970, 50672], "temperature": 0.0, "avg_logprob": -0.22067954089190508, "compression_ratio": 1.3954802259887005, "no_speech_prob": 0.0006456825649365783}, {"id": 838, "seek": 380480, "start": 3810.96, "end": 3812.96, "text": " It's all yours. Thanks", "tokens": [50672, 467, 311, 439, 6342, 13, 2561, 50772], "temperature": 0.0, "avg_logprob": -0.22067954089190508, "compression_ratio": 1.3954802259887005, "no_speech_prob": 0.0006456825649365783}, {"id": 839, "seek": 380480, "start": 3813.52, "end": 3816.6400000000003, "text": " Josh that was such a great talk and so rich", "tokens": [50800, 9785, 300, 390, 1270, 257, 869, 751, 293, 370, 4593, 50956], "temperature": 0.0, "avg_logprob": -0.22067954089190508, "compression_ratio": 1.3954802259887005, "no_speech_prob": 0.0006456825649365783}, {"id": 840, "seek": 380480, "start": 3817.52, "end": 3824.1600000000003, "text": " In 10 minutes, which is how much time you and I have will only scratch the surface", "tokens": [51000, 682, 1266, 2077, 11, 597, 307, 577, 709, 565, 291, 293, 286, 362, 486, 787, 8459, 264, 3753, 51332], "temperature": 0.0, "avg_logprob": -0.22067954089190508, "compression_ratio": 1.3954802259887005, "no_speech_prob": 0.0006456825649365783}, {"id": 841, "seek": 380480, "start": 3826.32, "end": 3829.36, "text": " So departing from the comments I sent you", "tokens": [51440, 407, 9110, 278, 490, 264, 3053, 286, 2279, 291, 51592], "temperature": 0.0, "avg_logprob": -0.22067954089190508, "compression_ratio": 1.3954802259887005, "no_speech_prob": 0.0006456825649365783}, {"id": 842, "seek": 380480, "start": 3830.6400000000003, "end": 3832.6400000000003, "text": " with respect to the", "tokens": [51656, 365, 3104, 281, 264, 51756], "temperature": 0.0, "avg_logprob": -0.22067954089190508, "compression_ratio": 1.3954802259887005, "no_speech_prob": 0.0006456825649365783}, {"id": 843, "seek": 383264, "start": 3833.52, "end": 3836.3199999999997, "text": " Unified theory of meaning", "tokens": [50408, 1156, 2587, 5261, 295, 3620, 50548], "temperature": 0.0, "avg_logprob": -0.23348364754328652, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.0021811018232256174}, {"id": 844, "seek": 383264, "start": 3838.4, "end": 3840.4, "text": " Many years ago", "tokens": [50652, 5126, 924, 2057, 50752], "temperature": 0.0, "avg_logprob": -0.23348364754328652, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.0021811018232256174}, {"id": 845, "seek": 383264, "start": 3841.2, "end": 3843.2, "text": " Jerry Katz suggested that", "tokens": [50792, 17454, 8365, 89, 10945, 300, 50892], "temperature": 0.0, "avg_logprob": -0.23348364754328652, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.0021811018232256174}, {"id": 846, "seek": 383264, "start": 3844.0, "end": 3846.64, "text": " The question what is meaning could be", "tokens": [50932, 440, 1168, 437, 307, 3620, 727, 312, 51064], "temperature": 0.0, "avg_logprob": -0.23348364754328652, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.0021811018232256174}, {"id": 847, "seek": 383264, "start": 3847.6, "end": 3855.6, "text": " Separated into questions like what is sameness of meaning? What is contradiction? What is anomaly?", "tokens": [51112, 43480, 770, 666, 1651, 411, 437, 307, 3247, 15264, 295, 3620, 30, 708, 307, 34937, 30, 708, 307, 42737, 30, 51512], "temperature": 0.0, "avg_logprob": -0.23348364754328652, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.0021811018232256174}, {"id": 848, "seek": 383264, "start": 3856.7999999999997, "end": 3859.04, "text": " What is entailment and so on?", "tokens": [51572, 708, 307, 948, 864, 518, 293, 370, 322, 30, 51684], "temperature": 0.0, "avg_logprob": -0.23348364754328652, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.0021811018232256174}, {"id": 849, "seek": 385904, "start": 3859.7599999999998, "end": 3865.2, "text": " And it occurs to me that it would be interesting to try this", "tokens": [50400, 400, 309, 11843, 281, 385, 300, 309, 576, 312, 1880, 281, 853, 341, 50672], "temperature": 0.0, "avg_logprob": -0.1560215108534869, "compression_ratio": 1.455621301775148, "no_speech_prob": 0.0020500249229371548}, {"id": 850, "seek": 385904, "start": 3866.08, "end": 3870.16, "text": " To try your model to see just how well", "tokens": [50716, 1407, 853, 428, 2316, 281, 536, 445, 577, 731, 50920], "temperature": 0.0, "avg_logprob": -0.1560215108534869, "compression_ratio": 1.455621301775148, "no_speech_prob": 0.0020500249229371548}, {"id": 851, "seek": 385904, "start": 3871.68, "end": 3873.68, "text": " It can detect", "tokens": [50996, 467, 393, 5531, 51096], "temperature": 0.0, "avg_logprob": -0.1560215108534869, "compression_ratio": 1.455621301775148, "no_speech_prob": 0.0020500249229371548}, {"id": 852, "seek": 385904, "start": 3875.2799999999997, "end": 3879.44, "text": " Synonomy contradiction anomaly and so on", "tokens": [51176, 26155, 23423, 34937, 42737, 293, 370, 322, 51384], "temperature": 0.0, "avg_logprob": -0.1560215108534869, "compression_ratio": 1.455621301775148, "no_speech_prob": 0.0020500249229371548}, {"id": 853, "seek": 385904, "start": 3881.36, "end": 3886.64, "text": " Yeah, I think that's um, that's a great connection. I mean I I know a little bit about that", "tokens": [51480, 865, 11, 286, 519, 300, 311, 1105, 11, 300, 311, 257, 869, 4984, 13, 286, 914, 286, 286, 458, 257, 707, 857, 466, 300, 51744], "temperature": 0.0, "avg_logprob": -0.1560215108534869, "compression_ratio": 1.455621301775148, "no_speech_prob": 0.0020500249229371548}, {"id": 854, "seek": 388664, "start": 3886.72, "end": 3890.4, "text": " But I you're inspiring me to go back and reread and learn much more about it", "tokens": [50368, 583, 286, 291, 434, 15883, 385, 281, 352, 646, 293, 46453, 345, 293, 1466, 709, 544, 466, 309, 50552], "temperature": 0.0, "avg_logprob": -0.09765650191397038, "compression_ratio": 1.7060931899641576, "no_speech_prob": 0.014945635572075844}, {"id": 855, "seek": 388664, "start": 3890.4, "end": 3892.96, "text": " I mean, I think just in the context of the last thing I said", "tokens": [50552, 286, 914, 11, 286, 519, 445, 294, 264, 4319, 295, 264, 1036, 551, 286, 848, 50680], "temperature": 0.0, "avg_logprob": -0.09765650191397038, "compression_ratio": 1.7060931899641576, "no_speech_prob": 0.014945635572075844}, {"id": 856, "seek": 388664, "start": 3893.44, "end": 3896.96, "text": " The idea exactly that in a sense in this framework at least", "tokens": [50704, 440, 1558, 2293, 300, 294, 257, 2020, 294, 341, 8388, 412, 1935, 50880], "temperature": 0.0, "avg_logprob": -0.09765650191397038, "compression_ratio": 1.7060931899641576, "no_speech_prob": 0.014945635572075844}, {"id": 857, "seek": 388664, "start": 3897.44, "end": 3903.52, "text": " What the statistical language model is doing is capturing the notion of sameness effectively because there the distribution on", "tokens": [50904, 708, 264, 22820, 2856, 2316, 307, 884, 307, 23384, 264, 10710, 295, 3247, 15264, 8659, 570, 456, 264, 7316, 322, 51208], "temperature": 0.0, "avg_logprob": -0.09765650191397038, "compression_ratio": 1.7060931899641576, "no_speech_prob": 0.014945635572075844}, {"id": 858, "seek": 388664, "start": 3903.92, "end": 3907.6, "text": " Code in the probabilistic language of thought if that distribution is similar", "tokens": [51228, 15549, 294, 264, 31959, 3142, 2856, 295, 1194, 498, 300, 7316, 307, 2531, 51412], "temperature": 0.0, "avg_logprob": -0.09765650191397038, "compression_ratio": 1.7060931899641576, "no_speech_prob": 0.014945635572075844}, {"id": 859, "seek": 388664, "start": 3908.0, "end": 3910.4, "text": " And that can be measured in different ways then you might say well things", "tokens": [51432, 400, 300, 393, 312, 12690, 294, 819, 2098, 550, 291, 1062, 584, 731, 721, 51552], "temperature": 0.0, "avg_logprob": -0.09765650191397038, "compression_ratio": 1.7060931899641576, "no_speech_prob": 0.014945635572075844}, {"id": 860, "seek": 391040, "start": 3910.8, "end": 3917.6, "text": " Have the same meaning either in context or in general because and that that distribution function can be contextualized or it can be", "tokens": [50384, 3560, 264, 912, 3620, 2139, 294, 4319, 420, 294, 2674, 570, 293, 300, 300, 7316, 2445, 393, 312, 35526, 1602, 420, 309, 393, 312, 50724], "temperature": 0.0, "avg_logprob": -0.12123306592305501, "compression_ratio": 1.706451612903226, "no_speech_prob": 0.013631490059196949}, {"id": 861, "seek": 391040, "start": 3918.0, "end": 3922.4, "text": " Made higher order. Okay, so that's really interesting, but it doesn't compute entailment or other", "tokens": [50744, 18330, 2946, 1668, 13, 1033, 11, 370, 300, 311, 534, 1880, 11, 457, 309, 1177, 380, 14722, 948, 864, 518, 420, 661, 50964], "temperature": 0.0, "avg_logprob": -0.12123306592305501, "compression_ratio": 1.706451612903226, "no_speech_prob": 0.013631490059196949}, {"id": 862, "seek": 391040, "start": 3922.7200000000003, "end": 3928.32, "text": " You know a conceptual or inferential relations those come from reasoning in the probabilistic language of thought", "tokens": [50980, 509, 458, 257, 24106, 420, 13596, 2549, 2299, 729, 808, 490, 21577, 294, 264, 31959, 3142, 2856, 295, 1194, 51260], "temperature": 0.0, "avg_logprob": -0.12123306592305501, "compression_ratio": 1.706451612903226, "no_speech_prob": 0.013631490059196949}, {"id": 863, "seek": 391040, "start": 3928.88, "end": 3929.84, "text": " um", "tokens": [51288, 1105, 51336], "temperature": 0.0, "avg_logprob": -0.12123306592305501, "compression_ratio": 1.706451612903226, "no_speech_prob": 0.013631490059196949}, {"id": 864, "seek": 391040, "start": 3929.84, "end": 3934.4, "text": " And yeah, I mean I think that's that would be great to explore that more see if that can account for", "tokens": [51336, 400, 1338, 11, 286, 914, 286, 519, 300, 311, 300, 576, 312, 869, 281, 6839, 300, 544, 536, 498, 300, 393, 2696, 337, 51564], "temperature": 0.0, "avg_logprob": -0.12123306592305501, "compression_ratio": 1.706451612903226, "no_speech_prob": 0.013631490059196949}, {"id": 865, "seek": 391040, "start": 3934.96, "end": 3938.8, "text": " Unify it both account for the different set of phenomena and unify in that sense", "tokens": [51592, 1156, 2505, 309, 1293, 2696, 337, 264, 819, 992, 295, 22004, 293, 517, 2505, 294, 300, 2020, 51784], "temperature": 0.0, "avg_logprob": -0.12123306592305501, "compression_ratio": 1.706451612903226, "no_speech_prob": 0.013631490059196949}, {"id": 866, "seek": 393880, "start": 3939.6800000000003, "end": 3942.96, "text": " Yeah, I think that would be really interesting to explore", "tokens": [50408, 865, 11, 286, 519, 300, 576, 312, 534, 1880, 281, 6839, 50572], "temperature": 0.0, "avg_logprob": -0.22435391959497483, "compression_ratio": 1.344155844155844, "no_speech_prob": 0.000209800447919406}, {"id": 867, "seek": 393880, "start": 3943.6000000000004, "end": 3945.76, "text": " um, so going back to", "tokens": [50604, 1105, 11, 370, 516, 646, 281, 50712], "temperature": 0.0, "avg_logprob": -0.22435391959497483, "compression_ratio": 1.344155844155844, "no_speech_prob": 0.000209800447919406}, {"id": 868, "seek": 393880, "start": 3947.52, "end": 3949.52, "text": " Some of the more mundane", "tokens": [50800, 2188, 295, 264, 544, 43497, 50900], "temperature": 0.0, "avg_logprob": -0.22435391959497483, "compression_ratio": 1.344155844155844, "no_speech_prob": 0.000209800447919406}, {"id": 869, "seek": 393880, "start": 3950.96, "end": 3952.96, "text": " Which I guess is upon", "tokens": [50972, 3013, 286, 2041, 307, 3564, 51072], "temperature": 0.0, "avg_logprob": -0.22435391959497483, "compression_ratio": 1.344155844155844, "no_speech_prob": 0.000209800447919406}, {"id": 870, "seek": 393880, "start": 3954.7200000000003, "end": 3959.2000000000003, "text": " Aspects um the way that I'm understanding what you've", "tokens": [51160, 1018, 1043, 82, 1105, 264, 636, 300, 286, 478, 3701, 437, 291, 600, 51384], "temperature": 0.0, "avg_logprob": -0.22435391959497483, "compression_ratio": 1.344155844155844, "no_speech_prob": 0.000209800447919406}, {"id": 871, "seek": 393880, "start": 3960.32, "end": 3963.04, "text": " Said about how the theories", "tokens": [51440, 26490, 466, 577, 264, 13667, 51576], "temperature": 0.0, "avg_logprob": -0.22435391959497483, "compression_ratio": 1.344155844155844, "no_speech_prob": 0.000209800447919406}, {"id": 872, "seek": 396304, "start": 3963.7599999999998, "end": 3969.12, "text": " Intersect that is how llm's intersect with basian models", "tokens": [50400, 5751, 9632, 300, 307, 577, 287, 75, 76, 311, 27815, 365, 987, 952, 5245, 50668], "temperature": 0.0, "avg_logprob": -0.17196021417174676, "compression_ratio": 1.5610859728506787, "no_speech_prob": 0.0022865391802042723}, {"id": 873, "seek": 396304, "start": 3969.7599999999998, "end": 3974.48, "text": " Is basically you're using the llm as a kind of tool", "tokens": [50700, 1119, 1936, 291, 434, 1228, 264, 287, 75, 76, 382, 257, 733, 295, 2290, 50936], "temperature": 0.0, "avg_logprob": -0.17196021417174676, "compression_ratio": 1.5610859728506787, "no_speech_prob": 0.0022865391802042723}, {"id": 874, "seek": 396304, "start": 3975.12, "end": 3980.64, "text": " To translate from one vocabulary to another vocabulary. Is that accurate?", "tokens": [50968, 1407, 13799, 490, 472, 19864, 281, 1071, 19864, 13, 1119, 300, 8559, 30, 51244], "temperature": 0.0, "avg_logprob": -0.17196021417174676, "compression_ratio": 1.5610859728506787, "no_speech_prob": 0.0022865391802042723}, {"id": 875, "seek": 396304, "start": 3981.52, "end": 3986.08, "text": " Yeah, I mean and that is at the most practical level. That's what we're doing. Um,", "tokens": [51288, 865, 11, 286, 914, 293, 300, 307, 412, 264, 881, 8496, 1496, 13, 663, 311, 437, 321, 434, 884, 13, 3301, 11, 51516], "temperature": 0.0, "avg_logprob": -0.17196021417174676, "compression_ratio": 1.5610859728506787, "no_speech_prob": 0.0022865391802042723}, {"id": 876, "seek": 396304, "start": 3986.96, "end": 3988.48, "text": " yeah", "tokens": [51560, 1338, 51636], "temperature": 0.0, "avg_logprob": -0.17196021417174676, "compression_ratio": 1.5610859728506787, "no_speech_prob": 0.0022865391802042723}, {"id": 877, "seek": 396304, "start": 3988.48, "end": 3992.32, "text": " Okay, more about that, but let's let's go with that for now. Yes. Okay. Um", "tokens": [51636, 1033, 11, 544, 466, 300, 11, 457, 718, 311, 718, 311, 352, 365, 300, 337, 586, 13, 1079, 13, 1033, 13, 3301, 51828], "temperature": 0.0, "avg_logprob": -0.17196021417174676, "compression_ratio": 1.5610859728506787, "no_speech_prob": 0.0022865391802042723}, {"id": 878, "seek": 399304, "start": 3993.36, "end": 3995.2, "text": " so", "tokens": [50380, 370, 50472], "temperature": 0.0, "avg_logprob": -0.16665354007627906, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.00031483013299293816}, {"id": 879, "seek": 399304, "start": 3995.2, "end": 4003.04, "text": " The critical difference that I see in what you're doing and what llm's are doing is the", "tokens": [50472, 440, 4924, 2649, 300, 286, 536, 294, 437, 291, 434, 884, 293, 437, 287, 75, 76, 311, 366, 884, 307, 264, 50864], "temperature": 0.0, "avg_logprob": -0.16665354007627906, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.00031483013299293816}, {"id": 880, "seek": 399304, "start": 4003.6, "end": 4009.2, "text": " Well, I guess there are two critical differences, but for me the most important one is the use of symbols", "tokens": [50892, 1042, 11, 286, 2041, 456, 366, 732, 4924, 7300, 11, 457, 337, 385, 264, 881, 1021, 472, 307, 264, 764, 295, 16944, 51172], "temperature": 0.0, "avg_logprob": -0.16665354007627906, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.00031483013299293816}, {"id": 881, "seek": 399304, "start": 4009.92, "end": 4013.2, "text": " um, the other part is the basian", "tokens": [51208, 1105, 11, 264, 661, 644, 307, 264, 987, 952, 51372], "temperature": 0.0, "avg_logprob": -0.16665354007627906, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.00031483013299293816}, {"id": 882, "seek": 399304, "start": 4014.88, "end": 4018.32, "text": " Mechanism, but let's separate them. So", "tokens": [51456, 30175, 1434, 11, 457, 718, 311, 4994, 552, 13, 407, 51628], "temperature": 0.0, "avg_logprob": -0.16665354007627906, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.00031483013299293816}, {"id": 883, "seek": 399304, "start": 4019.52, "end": 4021.52, "text": " When you put symbols in", "tokens": [51688, 1133, 291, 829, 16944, 294, 51788], "temperature": 0.0, "avg_logprob": -0.16665354007627906, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.00031483013299293816}, {"id": 884, "seek": 402152, "start": 4022.24, "end": 4027.6, "text": " To me you're putting a lot of content into the mechanism. Do you agree with that?", "tokens": [50400, 1407, 385, 291, 434, 3372, 257, 688, 295, 2701, 666, 264, 7513, 13, 1144, 291, 3986, 365, 300, 30, 50668], "temperature": 0.0, "avg_logprob": -0.1150034164713922, "compression_ratio": 1.831896551724138, "no_speech_prob": 0.0033748093992471695}, {"id": 885, "seek": 402152, "start": 4029.2, "end": 4034.16, "text": " um, I agree that adding symbols adds a lot of content, but I think", "tokens": [50748, 1105, 11, 286, 3986, 300, 5127, 16944, 10860, 257, 688, 295, 2701, 11, 457, 286, 519, 50996], "temperature": 0.0, "avg_logprob": -0.1150034164713922, "compression_ratio": 1.831896551724138, "no_speech_prob": 0.0033748093992471695}, {"id": 886, "seek": 402152, "start": 4035.28, "end": 4037.7599999999998, "text": " And I again, I didn't have time to unpack all of this very well", "tokens": [51052, 400, 286, 797, 11, 286, 994, 380, 362, 565, 281, 26699, 439, 295, 341, 588, 731, 51176], "temperature": 0.0, "avg_logprob": -0.1150034164713922, "compression_ratio": 1.831896551724138, "no_speech_prob": 0.0033748093992471695}, {"id": 887, "seek": 402152, "start": 4037.84, "end": 4044.08, "text": " I think but when you say you putting in symbols the in different different things that I'm talking about here", "tokens": [51180, 286, 519, 457, 562, 291, 584, 291, 3372, 294, 16944, 264, 294, 819, 819, 721, 300, 286, 478, 1417, 466, 510, 51492], "temperature": 0.0, "avg_logprob": -0.1150034164713922, "compression_ratio": 1.831896551724138, "no_speech_prob": 0.0033748093992471695}, {"id": 888, "seek": 402152, "start": 4044.4, "end": 4049.6, "text": " The you and the putting in are different, but yeah, but whenever you add in symbols that adds a lot of", "tokens": [51508, 440, 291, 293, 264, 3372, 294, 366, 819, 11, 457, 1338, 11, 457, 5699, 291, 909, 294, 16944, 300, 10860, 257, 688, 295, 51768], "temperature": 0.0, "avg_logprob": -0.1150034164713922, "compression_ratio": 1.831896551724138, "no_speech_prob": 0.0033748093992471695}, {"id": 889, "seek": 404960, "start": 4050.08, "end": 4056.96, "text": " Content and structure here. Yeah. Yeah content. Right. So I'm thinking about the implications of that for nativism", "tokens": [50388, 30078, 293, 3877, 510, 13, 865, 13, 865, 2701, 13, 1779, 13, 407, 286, 478, 1953, 466, 264, 16602, 295, 300, 337, 2249, 592, 1434, 50732], "temperature": 0.0, "avg_logprob": -0.22058458611516668, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.00157229567412287}, {"id": 890, "seek": 404960, "start": 4057.52, "end": 4061.2, "text": " So it seems to me. This is a nativist theory", "tokens": [50760, 407, 309, 2544, 281, 385, 13, 639, 307, 257, 2249, 592, 468, 5261, 50944], "temperature": 0.0, "avg_logprob": -0.22058458611516668, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.00157229567412287}, {"id": 891, "seek": 404960, "start": 4064.48, "end": 4069.2, "text": " Um, I would say it is compatible with some forms of nativism that I find plausible", "tokens": [51108, 3301, 11, 286, 576, 584, 309, 307, 18218, 365, 512, 6422, 295, 2249, 592, 1434, 300, 286, 915, 39925, 51344], "temperature": 0.0, "avg_logprob": -0.22058458611516668, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.00157229567412287}, {"id": 892, "seek": 404960, "start": 4069.2799999999997, "end": 4071.6, "text": " although none I don't have a horse in that game, but I", "tokens": [51348, 4878, 6022, 286, 500, 380, 362, 257, 6832, 294, 300, 1216, 11, 457, 286, 51464], "temperature": 0.0, "avg_logprob": -0.22058458611516668, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.00157229567412287}, {"id": 893, "seek": 404960, "start": 4072.48, "end": 4078.0, "text": " Through a lot of interactions with friends and colleagues like lisbelki susan carry many others", "tokens": [51508, 8927, 257, 688, 295, 13280, 365, 1855, 293, 7734, 411, 32670, 5390, 2984, 3291, 282, 3985, 867, 2357, 51784], "temperature": 0.0, "avg_logprob": -0.22058458611516668, "compression_ratio": 1.591093117408907, "no_speech_prob": 0.00157229567412287}, {"id": 894, "seek": 407800, "start": 4078.4, "end": 4079.68, "text": " Come to find", "tokens": [50384, 2492, 281, 915, 50448], "temperature": 0.0, "avg_logprob": -0.14988664411148936, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.005815754644572735}, {"id": 895, "seek": 407800, "start": 4079.68, "end": 4084.16, "text": " Certain kinds certain aspects of nativism and conceptual nativism plausible especially in", "tokens": [50448, 13407, 3685, 1629, 7270, 295, 2249, 592, 1434, 293, 24106, 2249, 592, 1434, 39925, 2318, 294, 50672], "temperature": 0.0, "avg_logprob": -0.14988664411148936, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.005815754644572735}, {"id": 896, "seek": 407800, "start": 4084.48, "end": 4086.88, "text": " Certain core domains that are shared with other animals", "tokens": [50688, 13407, 4965, 25514, 300, 366, 5507, 365, 661, 4882, 50808], "temperature": 0.0, "avg_logprob": -0.14988664411148936, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.005815754644572735}, {"id": 897, "seek": 407800, "start": 4087.36, "end": 4092.56, "text": " Like intuitive physics about objects that the world is three dimensional objects have some that there's some kind of", "tokens": [50832, 1743, 21769, 10649, 466, 6565, 300, 264, 1002, 307, 1045, 18795, 6565, 362, 512, 300, 456, 311, 512, 733, 295, 51092], "temperature": 0.0, "avg_logprob": -0.14988664411148936, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.005815754644572735}, {"id": 898, "seek": 407800, "start": 4093.12, "end": 4094.64, "text": " physical interactions", "tokens": [51120, 4001, 13280, 51196], "temperature": 0.0, "avg_logprob": -0.14988664411148936, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.005815754644572735}, {"id": 899, "seek": 407800, "start": 4094.64, "end": 4101.36, "text": " That are you know our bodies engage with and also some forms of intuitive psychology not necessarily higher order belief reasoning", "tokens": [51196, 663, 366, 291, 458, 527, 7510, 4683, 365, 293, 611, 512, 6422, 295, 21769, 15105, 406, 4725, 2946, 1668, 7107, 21577, 51532], "temperature": 0.0, "avg_logprob": -0.14988664411148936, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.005815754644572735}, {"id": 900, "seek": 407800, "start": 4101.68, "end": 4106.96, "text": " But the idea of efficient action and that agents have goals and they pursue actions in the physical world", "tokens": [51548, 583, 264, 1558, 295, 7148, 3069, 293, 300, 12554, 362, 5493, 293, 436, 12392, 5909, 294, 264, 4001, 1002, 51812], "temperature": 0.0, "avg_logprob": -0.14988664411148936, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.005815754644572735}, {"id": 901, "seek": 410696, "start": 4107.04, "end": 4111.2, "text": " Grounded in physics to achieve their goals efficiently. Honestly, there's evidence", "tokens": [50368, 28371, 292, 294, 10649, 281, 4584, 641, 5493, 19621, 13, 12348, 11, 456, 311, 4467, 50576], "temperature": 0.0, "avg_logprob": -0.15522538513696493, "compression_ratio": 1.7651245551601424, "no_speech_prob": 0.0013452069833874702}, {"id": 902, "seek": 410696, "start": 4111.92, "end": 4117.2, "text": " Again, sherry lou who did this work in spelki's lab and continue to build on it has really, you know", "tokens": [50612, 3764, 11, 402, 5318, 15185, 567, 630, 341, 589, 294, 46486, 2984, 311, 2715, 293, 2354, 281, 1322, 322, 309, 575, 534, 11, 291, 458, 50876], "temperature": 0.0, "avg_logprob": -0.15522538513696493, "compression_ratio": 1.7651245551601424, "no_speech_prob": 0.0013452069833874702}, {"id": 903, "seek": 410696, "start": 4117.6, "end": 4120.4, "text": " I mean, I was a striking evidence in three month olds", "tokens": [50896, 286, 914, 11, 286, 390, 257, 18559, 4467, 294, 1045, 1618, 41972, 51036], "temperature": 0.0, "avg_logprob": -0.15522538513696493, "compression_ratio": 1.7651245551601424, "no_speech_prob": 0.0013452069833874702}, {"id": 904, "seek": 410696, "start": 4120.8, "end": 4126.08, "text": " But it's every experiment with three month olds is very small and you know a lot needs to be built on", "tokens": [51056, 583, 309, 311, 633, 5120, 365, 1045, 1618, 41972, 307, 588, 1359, 293, 291, 458, 257, 688, 2203, 281, 312, 3094, 322, 51320], "temperature": 0.0, "avg_logprob": -0.15522538513696493, "compression_ratio": 1.7651245551601424, "no_speech_prob": 0.0013452069833874702}, {"id": 905, "seek": 410696, "start": 4126.4, "end": 4128.4, "text": " But as striking as it gets in three month olds", "tokens": [51336, 583, 382, 18559, 382, 309, 2170, 294, 1045, 1618, 41972, 51436], "temperature": 0.0, "avg_logprob": -0.15522538513696493, "compression_ratio": 1.7651245551601424, "no_speech_prob": 0.0013452069833874702}, {"id": 906, "seek": 410696, "start": 4129.36, "end": 4130.4, "text": " science", "tokens": [51484, 3497, 51536], "temperature": 0.0, "avg_logprob": -0.15522538513696493, "compression_ratio": 1.7651245551601424, "no_speech_prob": 0.0013452069833874702}, {"id": 907, "seek": 410696, "start": 4130.4, "end": 4136.16, "text": " I'm showing that that aspects certain aspects of not only physics, but efficient goal directed action", "tokens": [51536, 286, 478, 4099, 300, 300, 7270, 1629, 7270, 295, 406, 787, 10649, 11, 457, 7148, 3387, 12898, 3069, 51824], "temperature": 0.0, "avg_logprob": -0.15522538513696493, "compression_ratio": 1.7651245551601424, "no_speech_prob": 0.0013452069833874702}, {"id": 908, "seek": 413616, "start": 4136.24, "end": 4138.88, "text": " Understanding seem to be present that doesn't mean they're innate", "tokens": [50368, 36858, 1643, 281, 312, 1974, 300, 1177, 380, 914, 436, 434, 41766, 50500], "temperature": 0.0, "avg_logprob": -0.19573018815782334, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.0052083395421504974}, {"id": 909, "seek": 413616, "start": 4139.12, "end": 4143.36, "text": " But at least they're not they're present way before language and probably build on some innate stuff", "tokens": [50512, 583, 412, 1935, 436, 434, 406, 436, 434, 1974, 636, 949, 2856, 293, 1391, 1322, 322, 512, 41766, 1507, 50724], "temperature": 0.0, "avg_logprob": -0.19573018815782334, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.0052083395421504974}, {"id": 910, "seek": 413616, "start": 4143.76, "end": 4145.76, "text": " It's compatible with that notion", "tokens": [50744, 467, 311, 18218, 365, 300, 10710, 50844], "temperature": 0.0, "avg_logprob": -0.19573018815782334, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.0052083395421504974}, {"id": 911, "seek": 413616, "start": 4154.5599999999995, "end": 4159.599999999999, "text": " I'm not sure who that is but could you mute that was an accident. I think they just", "tokens": [51284, 286, 478, 406, 988, 567, 300, 307, 457, 727, 291, 24523, 300, 390, 364, 6398, 13, 286, 519, 436, 445, 51536], "temperature": 0.0, "avg_logprob": -0.19573018815782334, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.0052083395421504974}, {"id": 912, "seek": 413616, "start": 4160.0, "end": 4165.36, "text": " Should have turned off their okay, but so so yeah, there's this idea that certain kinds of symbols", "tokens": [51556, 6454, 362, 3574, 766, 641, 1392, 11, 457, 370, 370, 1338, 11, 456, 311, 341, 1558, 300, 1629, 3685, 295, 16944, 51824], "temperature": 0.0, "avg_logprob": -0.19573018815782334, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.0052083395421504974}, {"id": 913, "seek": 416536, "start": 4165.839999999999, "end": 4169.759999999999, "text": " Are used in our framework to describe those some of those core systems", "tokens": [50388, 2014, 1143, 294, 527, 8388, 281, 6786, 729, 512, 295, 729, 4965, 3652, 50584], "temperature": 0.0, "avg_logprob": -0.1594560540240744, "compression_ratio": 1.683453237410072, "no_speech_prob": 0.00376435206271708}, {"id": 914, "seek": 416536, "start": 4170.08, "end": 4175.04, "text": " But we are in contrast to like a fedoria nativism which says all concepts are innate", "tokens": [50600, 583, 321, 366, 294, 8712, 281, 411, 257, 4636, 8172, 2249, 592, 1434, 597, 1619, 439, 10392, 366, 41766, 50848], "temperature": 0.0, "avg_logprob": -0.1594560540240744, "compression_ratio": 1.683453237410072, "no_speech_prob": 0.00376435206271708}, {"id": 915, "seek": 416536, "start": 4175.679999999999, "end": 4180.639999999999, "text": " In this in this framework here. I mean it or you could say in contrast, although some of uh,", "tokens": [50880, 682, 341, 294, 341, 8388, 510, 13, 286, 914, 309, 420, 291, 727, 584, 294, 8712, 11, 4878, 512, 295, 2232, 11, 51128], "temperature": 0.0, "avg_logprob": -0.1594560540240744, "compression_ratio": 1.683453237410072, "no_speech_prob": 0.00376435206271708}, {"id": 916, "seek": 416536, "start": 4181.2, "end": 4185.679999999999, "text": " I don't know what jerry would have actually said about this but people like paul patrowski have suggested", "tokens": [51156, 286, 500, 380, 458, 437, 361, 5318, 576, 362, 767, 848, 466, 341, 457, 561, 411, 2502, 425, 1947, 81, 21866, 362, 10945, 51380], "temperature": 0.0, "avg_logprob": -0.1594560540240744, "compression_ratio": 1.683453237410072, "no_speech_prob": 0.00376435206271708}, {"id": 917, "seek": 416536, "start": 4186.08, "end": 4192.639999999999, "text": " You know, maybe he would love it. I don't know but in in a rather different version of like some forms of radical", "tokens": [51400, 509, 458, 11, 1310, 415, 576, 959, 309, 13, 286, 500, 380, 458, 457, 294, 294, 257, 2831, 819, 3037, 295, 411, 512, 6422, 295, 12001, 51728], "temperature": 0.0, "avg_logprob": -0.1594560540240744, "compression_ratio": 1.683453237410072, "no_speech_prob": 0.00376435206271708}, {"id": 918, "seek": 419264, "start": 4193.280000000001, "end": 4197.84, "text": " Conceptual nativism most of the concepts in this framework are not innate", "tokens": [50396, 47482, 901, 2249, 592, 1434, 881, 295, 264, 10392, 294, 341, 8388, 366, 406, 41766, 50624], "temperature": 0.0, "avg_logprob": -0.20654567082722983, "compression_ratio": 1.8140350877192983, "no_speech_prob": 0.010323981754481792}, {"id": 919, "seek": 419264, "start": 4198.160000000001, "end": 4200.160000000001, "text": " They're written in a language of thought", "tokens": [50640, 814, 434, 3720, 294, 257, 2856, 295, 1194, 50740], "temperature": 0.0, "avg_logprob": -0.20654567082722983, "compression_ratio": 1.8140350877192983, "no_speech_prob": 0.010323981754481792}, {"id": 920, "seek": 419264, "start": 4200.240000000001, "end": 4204.400000000001, "text": " That it that could be somehow innate or somehow bootstrapped through natural language", "tokens": [50744, 663, 309, 300, 727, 312, 6063, 41766, 420, 6063, 11450, 19639, 3320, 807, 3303, 2856, 50952], "temperature": 0.0, "avg_logprob": -0.20654567082722983, "compression_ratio": 1.8140350877192983, "no_speech_prob": 0.010323981754481792}, {"id": 921, "seek": 419264, "start": 4204.4800000000005, "end": 4207.360000000001, "text": " I find some of the ideas that that uh,", "tokens": [50956, 286, 915, 512, 295, 264, 3487, 300, 300, 2232, 11, 51100], "temperature": 0.0, "avg_logprob": -0.20654567082722983, "compression_ratio": 1.8140350877192983, "no_speech_prob": 0.010323981754481792}, {"id": 922, "seek": 419264, "start": 4207.4400000000005, "end": 4213.76, "text": " Susan carrey and lis spelti in their different ways along with jesse snettaker have developed their ways in which language acquisition", "tokens": [51104, 15160, 1032, 7950, 293, 32670, 637, 2018, 72, 294, 641, 819, 2098, 2051, 365, 361, 7357, 2406, 3093, 4003, 362, 4743, 641, 2098, 294, 597, 2856, 21668, 51420], "temperature": 0.0, "avg_logprob": -0.20654567082722983, "compression_ratio": 1.8140350877192983, "no_speech_prob": 0.010323981754481792}, {"id": 923, "seek": 419264, "start": 4214.240000000001, "end": 4215.280000000001, "text": " and", "tokens": [51444, 293, 51496], "temperature": 0.0, "avg_logprob": -0.20654567082722983, "compression_ratio": 1.8140350877192983, "no_speech_prob": 0.010323981754481792}, {"id": 924, "seek": 419264, "start": 4215.280000000001, "end": 4222.0, "text": " Mental languages of thought might bootstrap each other and many things in the kind of glitman tradition also I think are reflect that idea", "tokens": [51496, 30294, 8650, 295, 1194, 1062, 11450, 372, 4007, 1184, 661, 293, 867, 721, 294, 264, 733, 295, 1563, 270, 1601, 6994, 611, 286, 519, 366, 5031, 300, 1558, 51832], "temperature": 0.0, "avg_logprob": -0.20654567082722983, "compression_ratio": 1.8140350877192983, "no_speech_prob": 0.010323981754481792}, {"id": 925, "seek": 422200, "start": 4222.64, "end": 4225.52, "text": " But I so I I think we we are", "tokens": [50396, 583, 286, 370, 286, 286, 519, 321, 321, 366, 50540], "temperature": 0.0, "avg_logprob": -0.14609550027286305, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.0007551239687018096}, {"id": 926, "seek": 422200, "start": 4226.08, "end": 4229.04, "text": " We still need to show this this is like the most interesting thing to do", "tokens": [50568, 492, 920, 643, 281, 855, 341, 341, 307, 411, 264, 881, 1880, 551, 281, 360, 50716], "temperature": 0.0, "avg_logprob": -0.14609550027286305, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.0007551239687018096}, {"id": 927, "seek": 422200, "start": 4229.36, "end": 4234.32, "text": " But to show ways in which the things I was showing at the very end could be could be used", "tokens": [50732, 583, 281, 855, 2098, 294, 597, 264, 721, 286, 390, 4099, 412, 264, 588, 917, 727, 312, 727, 312, 1143, 50980], "temperature": 0.0, "avg_logprob": -0.14609550027286305, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.0007551239687018096}, {"id": 928, "seek": 422200, "start": 4234.8, "end": 4239.52, "text": " To explain how and model how natural language can start off", "tokens": [51004, 1407, 2903, 577, 293, 2316, 577, 3303, 2856, 393, 722, 766, 51240], "temperature": 0.0, "avg_logprob": -0.14609550027286305, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.0007551239687018096}, {"id": 929, "seek": 422200, "start": 4240.0, "end": 4248.32, "text": " Being grounded semantically and logically in a limited symbolic vocabulary of probable worlds that reflect core knowledge", "tokens": [51264, 8891, 23535, 4361, 49505, 293, 38887, 294, 257, 5567, 25755, 19864, 295, 21759, 13401, 300, 5031, 4965, 3601, 51680], "temperature": 0.0, "avg_logprob": -0.14609550027286305, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.0007551239687018096}, {"id": 930, "seek": 424832, "start": 4248.5599999999995, "end": 4253.12, "text": " But then that can support bootstrapping and introducing new concepts via those mechanisms", "tokens": [50376, 583, 550, 300, 393, 1406, 11450, 19639, 3759, 293, 15424, 777, 10392, 5766, 729, 15902, 50604], "temperature": 0.0, "avg_logprob": -0.14209385712941489, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0017267202492803335}, {"id": 931, "seek": 424832, "start": 4253.2, "end": 4254.5599999999995, "text": " I was talking about at the end", "tokens": [50608, 286, 390, 1417, 466, 412, 264, 917, 50676], "temperature": 0.0, "avg_logprob": -0.14209385712941489, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0017267202492803335}, {"id": 932, "seek": 424832, "start": 4254.5599999999995, "end": 4261.04, "text": " Including new concepts, but also new domain theories that we get explicitly or implicitly through our linguistic interaction with", "tokens": [50676, 27137, 777, 10392, 11, 457, 611, 777, 9274, 13667, 300, 321, 483, 20803, 420, 26947, 356, 807, 527, 43002, 9285, 365, 51000], "temperature": 0.0, "avg_logprob": -0.14209385712941489, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0017267202492803335}, {"id": 933, "seek": 424832, "start": 4261.36, "end": 4263.36, "text": " Other people who we think know more than us", "tokens": [51016, 5358, 561, 567, 321, 519, 458, 544, 813, 505, 51116], "temperature": 0.0, "avg_logprob": -0.14209385712941489, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0017267202492803335}, {"id": 934, "seek": 424832, "start": 4264.48, "end": 4267.599999999999, "text": " Okay, um, that sounds great. Um", "tokens": [51172, 1033, 11, 1105, 11, 300, 3263, 869, 13, 3301, 51328], "temperature": 0.0, "avg_logprob": -0.14209385712941489, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0017267202492803335}, {"id": 935, "seek": 424832, "start": 4268.32, "end": 4273.12, "text": " It also suggests to me that it is a highly modular", "tokens": [51364, 467, 611, 13409, 281, 385, 300, 309, 307, 257, 5405, 31111, 51604], "temperature": 0.0, "avg_logprob": -0.14209385712941489, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0017267202492803335}, {"id": 936, "seek": 424832, "start": 4273.759999999999, "end": 4276.799999999999, "text": " System even though it's also probabilistic", "tokens": [51636, 8910, 754, 1673, 309, 311, 611, 31959, 3142, 51788], "temperature": 0.0, "avg_logprob": -0.14209385712941489, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0017267202492803335}, {"id": 937, "seek": 427680, "start": 4277.360000000001, "end": 4282.16, "text": " um, so it's probabilistic within each of these different modules", "tokens": [50392, 1105, 11, 370, 309, 311, 31959, 3142, 1951, 1184, 295, 613, 819, 16679, 50632], "temperature": 0.0, "avg_logprob": -0.15803174539045853, "compression_ratio": 1.6855670103092784, "no_speech_prob": 0.0007205577567219734}, {"id": 938, "seek": 427680, "start": 4282.8, "end": 4289.28, "text": " And depending on what you think the symbols are that you start with you can", "tokens": [50664, 400, 5413, 322, 437, 291, 519, 264, 16944, 366, 300, 291, 722, 365, 291, 393, 50988], "temperature": 0.0, "avg_logprob": -0.15803174539045853, "compression_ratio": 1.6855670103092784, "no_speech_prob": 0.0007205577567219734}, {"id": 939, "seek": 427680, "start": 4290.4800000000005, "end": 4294.08, "text": " Iterate what you think the modules of the mind are", "tokens": [51048, 286, 391, 473, 437, 291, 519, 264, 16679, 295, 264, 1575, 366, 51228], "temperature": 0.0, "avg_logprob": -0.15803174539045853, "compression_ratio": 1.6855670103092784, "no_speech_prob": 0.0007205577567219734}, {"id": 940, "seek": 427680, "start": 4295.76, "end": 4296.96, "text": " Yeah, no, I think that's right", "tokens": [51312, 865, 11, 572, 11, 286, 519, 300, 311, 558, 51372], "temperature": 0.0, "avg_logprob": -0.15803174539045853, "compression_ratio": 1.6855670103092784, "no_speech_prob": 0.0007205577567219734}, {"id": 941, "seek": 427680, "start": 4296.96, "end": 4299.92, "text": " And I think you know, I think it's this framework is not a", "tokens": [51372, 400, 286, 519, 291, 458, 11, 286, 519, 309, 311, 341, 8388, 307, 406, 257, 51520], "temperature": 0.0, "avg_logprob": -0.15803174539045853, "compression_ratio": 1.6855670103092784, "no_speech_prob": 0.0007205577567219734}, {"id": 942, "seek": 427680, "start": 4300.4800000000005, "end": 4303.2, "text": " On its own a proposal for how our minds start", "tokens": [51548, 1282, 1080, 1065, 257, 11494, 337, 577, 527, 9634, 722, 51684], "temperature": 0.0, "avg_logprob": -0.15803174539045853, "compression_ratio": 1.6855670103092784, "no_speech_prob": 0.0007205577567219734}, {"id": 943, "seek": 430320, "start": 4303.28, "end": 4307.04, "text": " But it can be used to instantiate and build and test some of them", "tokens": [50368, 583, 309, 393, 312, 1143, 281, 9836, 13024, 293, 1322, 293, 1500, 512, 295, 552, 50556], "temperature": 0.0, "avg_logprob": -0.14765763483127625, "compression_ratio": 1.71875, "no_speech_prob": 0.010162858292460442}, {"id": 944, "seek": 430320, "start": 4307.679999999999, "end": 4311.76, "text": " And so that's I think that's right. It's it does suggest that you could have, you know, different", "tokens": [50588, 400, 370, 300, 311, 286, 519, 300, 311, 558, 13, 467, 311, 309, 775, 3402, 300, 291, 727, 362, 11, 291, 458, 11, 819, 50792], "temperature": 0.0, "avg_logprob": -0.14765763483127625, "compression_ratio": 1.71875, "no_speech_prob": 0.010162858292460442}, {"id": 945, "seek": 430320, "start": 4312.8, "end": 4314.5599999999995, "text": " Sub-languages of thought for different domains", "tokens": [50844, 8511, 12, 25241, 84, 1660, 295, 1194, 337, 819, 25514, 50932], "temperature": 0.0, "avg_logprob": -0.14765763483127625, "compression_ratio": 1.71875, "no_speech_prob": 0.010162858292460442}, {"id": 946, "seek": 430320, "start": 4314.5599999999995, "end": 4320.0, "text": " But crucially the picture that you get with adults from this is both in some ways like strikingly", "tokens": [50932, 583, 5140, 1909, 264, 3036, 300, 291, 483, 365, 8865, 490, 341, 307, 1293, 294, 512, 2098, 411, 18559, 356, 51204], "temperature": 0.0, "avg_logprob": -0.14765763483127625, "compression_ratio": 1.71875, "no_speech_prob": 0.010162858292460442}, {"id": 947, "seek": 430320, "start": 4320.72, "end": 4323.36, "text": " Supermodular and in some other ways completely holistic", "tokens": [51240, 4548, 8014, 1040, 293, 294, 512, 661, 2098, 2584, 30334, 51372], "temperature": 0.0, "avg_logprob": -0.14765763483127625, "compression_ratio": 1.71875, "no_speech_prob": 0.010162858292460442}, {"id": 948, "seek": 430320, "start": 4323.679999999999, "end": 4330.16, "text": " So the striking super modularity is that the actual reasoning that you do in is in a discourse when I'm thinking about a situation", "tokens": [51388, 407, 264, 18559, 1687, 31111, 507, 307, 300, 264, 3539, 21577, 300, 291, 360, 294, 307, 294, 257, 23938, 562, 286, 478, 1953, 466, 257, 2590, 51712], "temperature": 0.0, "avg_logprob": -0.14765763483127625, "compression_ratio": 1.71875, "no_speech_prob": 0.010162858292460442}, {"id": 949, "seek": 433016, "start": 4330.24, "end": 4333.84, "text": " And we're talking in a conversation is very modular if it might even be just", "tokens": [50368, 400, 321, 434, 1417, 294, 257, 3761, 307, 588, 31111, 498, 309, 1062, 754, 312, 445, 50548], "temperature": 0.0, "avg_logprob": -0.15249034593690117, "compression_ratio": 1.7196969696969697, "no_speech_prob": 0.04398155212402344}, {"id": 950, "seek": 433016, "start": 4334.48, "end": 4338.88, "text": " Specific to this context this what Tyler Brook Wilson calls bespoke model construction", "tokens": [50580, 20484, 1089, 281, 341, 4319, 341, 437, 16869, 13945, 15388, 5498, 4097, 48776, 2316, 6435, 50800], "temperature": 0.0, "avg_logprob": -0.15249034593690117, "compression_ratio": 1.7196969696969697, "no_speech_prob": 0.04398155212402344}, {"id": 951, "seek": 433016, "start": 4339.2, "end": 4343.92, "text": " This idea that we might construct a model on the fly to think about a particular situation", "tokens": [50816, 639, 1558, 300, 321, 1062, 7690, 257, 2316, 322, 264, 3603, 281, 519, 466, 257, 1729, 2590, 51052], "temperature": 0.0, "avg_logprob": -0.15249034593690117, "compression_ratio": 1.7196969696969697, "no_speech_prob": 0.04398155212402344}, {"id": 952, "seek": 433016, "start": 4345.04, "end": 4347.599999999999, "text": " That that we're that's the last thing that we've been working on there", "tokens": [51108, 663, 300, 321, 434, 300, 311, 264, 1036, 551, 300, 321, 600, 668, 1364, 322, 456, 51236], "temperature": 0.0, "avg_logprob": -0.15249034593690117, "compression_ratio": 1.7196969696969697, "no_speech_prob": 0.04398155212402344}, {"id": 953, "seek": 433016, "start": 4347.92, "end": 4352.96, "text": " Is in some sense super modular because that model is is relatively small and all the inference. I'm doing is just here", "tokens": [51252, 1119, 294, 512, 2020, 1687, 31111, 570, 300, 2316, 307, 307, 7226, 1359, 293, 439, 264, 38253, 13, 286, 478, 884, 307, 445, 510, 51504], "temperature": 0.0, "avg_logprob": -0.15249034593690117, "compression_ratio": 1.7196969696969697, "no_speech_prob": 0.04398155212402344}, {"id": 954, "seek": 433016, "start": 4352.96, "end": 4354.24, "text": " so I so I", "tokens": [51504, 370, 286, 370, 286, 51568], "temperature": 0.0, "avg_logprob": -0.15249034593690117, "compression_ratio": 1.7196969696969697, "no_speech_prob": 0.04398155212402344}, {"id": 955, "seek": 435424, "start": 4354.24, "end": 4361.12, "text": " Side step or avoid the classic problems of what has made Bayesian inference intractable the idea that if I'm going to actually have", "tokens": [50364, 19026, 1823, 420, 5042, 264, 7230, 2740, 295, 437, 575, 1027, 7840, 42434, 38253, 560, 1897, 712, 264, 1558, 300, 498, 286, 478, 516, 281, 767, 362, 50708], "temperature": 0.0, "avg_logprob": -0.13897031896254597, "compression_ratio": 1.726962457337884, "no_speech_prob": 0.018826186656951904}, {"id": 956, "seek": 435424, "start": 4361.44, "end": 4364.4, "text": " A distribution over all possible worlds. I could think about it update that", "tokens": [50724, 316, 7316, 670, 439, 1944, 13401, 13, 286, 727, 519, 466, 309, 5623, 300, 50872], "temperature": 0.0, "avg_logprob": -0.13897031896254597, "compression_ratio": 1.726962457337884, "no_speech_prob": 0.018826186656951904}, {"id": 957, "seek": 435424, "start": 4364.88, "end": 4369.92, "text": " You know, that's completely intractable many people in the nativist tradition dan osherson and others have written about that", "tokens": [50896, 509, 458, 11, 300, 311, 2584, 560, 1897, 712, 867, 561, 294, 264, 2249, 592, 468, 6994, 3277, 3003, 511, 3015, 293, 2357, 362, 3720, 466, 300, 51148], "temperature": 0.0, "avg_logprob": -0.13897031896254597, "compression_ratio": 1.726962457337884, "no_speech_prob": 0.018826186656951904}, {"id": 958, "seek": 435424, "start": 4370.32, "end": 4374.48, "text": " And this this is a a way around that or as Tyler puts it in his thesis", "tokens": [51168, 400, 341, 341, 307, 257, 257, 636, 926, 300, 420, 382, 16869, 8137, 309, 294, 702, 22288, 51376], "temperature": 0.0, "avg_logprob": -0.13897031896254597, "compression_ratio": 1.726962457337884, "no_speech_prob": 0.018826186656951904}, {"id": 959, "seek": 435424, "start": 4374.48, "end": 4378.8, "text": " It's in a sense a kind of way to think about a solution certain kind of solution to the frame problem", "tokens": [51376, 467, 311, 294, 257, 2020, 257, 733, 295, 636, 281, 519, 466, 257, 3827, 1629, 733, 295, 3827, 281, 264, 3920, 1154, 51592], "temperature": 0.0, "avg_logprob": -0.13897031896254597, "compression_ratio": 1.726962457337884, "no_speech_prob": 0.018826186656951904}, {"id": 960, "seek": 437880, "start": 4379.12, "end": 4386.08, "text": " So it's super modular, but it's also very holistic in the sense that the world knowledge that's used to construct that", "tokens": [50380, 407, 309, 311, 1687, 31111, 11, 457, 309, 311, 611, 588, 30334, 294, 264, 2020, 300, 264, 1002, 3601, 300, 311, 1143, 281, 7690, 300, 50728], "temperature": 0.0, "avg_logprob": -0.10499044886806555, "compression_ratio": 1.7756653992395437, "no_speech_prob": 0.012049813754856586}, {"id": 961, "seek": 437880, "start": 4386.56, "end": 4390.08, "text": " Comes from like all the code you've ever written and all the semantic associations", "tokens": [50752, 47290, 490, 411, 439, 264, 3089, 291, 600, 1562, 3720, 293, 439, 264, 47982, 26597, 50928], "temperature": 0.0, "avg_logprob": -0.10499044886806555, "compression_ratio": 1.7756653992395437, "no_speech_prob": 0.012049813754856586}, {"id": 962, "seek": 437880, "start": 4390.16, "end": 4395.52, "text": " And it's like a gigantic holistic almost quinian web of language and code", "tokens": [50932, 400, 309, 311, 411, 257, 26800, 30334, 1920, 42215, 952, 3670, 295, 2856, 293, 3089, 51200], "temperature": 0.0, "avg_logprob": -0.10499044886806555, "compression_ratio": 1.7756653992395437, "no_speech_prob": 0.012049813754856586}, {"id": 963, "seek": 437880, "start": 4395.68, "end": 4398.16, "text": " What we've sometimes called the github in the mind view", "tokens": [51208, 708, 321, 600, 2171, 1219, 264, 290, 355, 836, 294, 264, 1575, 1910, 51332], "temperature": 0.0, "avg_logprob": -0.10499044886806555, "compression_ratio": 1.7756653992395437, "no_speech_prob": 0.012049813754856586}, {"id": 964, "seek": 437880, "start": 4398.88, "end": 4401.92, "text": " There's the game engine in the head the github in the mind like github is this", "tokens": [51368, 821, 311, 264, 1216, 2848, 294, 264, 1378, 264, 290, 355, 836, 294, 264, 1575, 411, 290, 355, 836, 307, 341, 51520], "temperature": 0.0, "avg_logprob": -0.10499044886806555, "compression_ratio": 1.7756653992395437, "no_speech_prob": 0.012049813754856586}, {"id": 965, "seek": 437880, "start": 4402.24, "end": 4404.64, "text": " You know thing on the web which was crucial for training", "tokens": [51536, 509, 458, 551, 322, 264, 3670, 597, 390, 11462, 337, 3097, 51656], "temperature": 0.0, "avg_logprob": -0.10499044886806555, "compression_ratio": 1.7756653992395437, "no_speech_prob": 0.012049813754856586}, {"id": 966, "seek": 440464, "start": 4405.200000000001, "end": 4409.68, "text": " Language code models and you can imagine your own mind has lots of chunks of code", "tokens": [50392, 24445, 3089, 5245, 293, 291, 393, 3811, 428, 1065, 1575, 575, 3195, 295, 24004, 295, 3089, 50616], "temperature": 0.0, "avg_logprob": -0.15069887373182508, "compression_ratio": 1.7136929460580912, "no_speech_prob": 0.03787257894873619}, {"id": 967, "seek": 440464, "start": 4410.08, "end": 4414.72, "text": " Some of which are maybe innate many of which are not and natural language interweave with it", "tokens": [50636, 2188, 295, 597, 366, 1310, 41766, 867, 295, 597, 366, 406, 293, 3303, 2856, 728, 826, 946, 365, 309, 50868], "temperature": 0.0, "avg_logprob": -0.15069887373182508, "compression_ratio": 1.7136929460580912, "no_speech_prob": 0.03787257894873619}, {"id": 968, "seek": 440464, "start": 4415.12, "end": 4422.08, "text": " And and having a content addressable associative memory that can use that and and marshal out from that", "tokens": [50888, 400, 293, 1419, 257, 2701, 2985, 712, 4180, 1166, 4675, 300, 393, 764, 300, 293, 293, 30517, 4947, 484, 490, 300, 51236], "temperature": 0.0, "avg_logprob": -0.15069887373182508, "compression_ratio": 1.7136929460580912, "no_speech_prob": 0.03787257894873619}, {"id": 969, "seek": 440464, "start": 4422.96, "end": 4424.240000000001, "text": " relevant", "tokens": [51280, 7340, 51344], "temperature": 0.0, "avg_logprob": -0.15069887373182508, "compression_ratio": 1.7136929460580912, "no_speech_prob": 0.03787257894873619}, {"id": 970, "seek": 440464, "start": 4424.240000000001, "end": 4429.280000000001, "text": " symbolic probabilistic models that can be used to reason about a particular situation in some sense that's extremely holistic", "tokens": [51344, 25755, 31959, 3142, 5245, 300, 393, 312, 1143, 281, 1778, 466, 257, 1729, 2590, 294, 512, 2020, 300, 311, 4664, 30334, 51596], "temperature": 0.0, "avg_logprob": -0.15069887373182508, "compression_ratio": 1.7136929460580912, "no_speech_prob": 0.03787257894873619}, {"id": 971, "seek": 442928, "start": 4429.84, "end": 4436.5599999999995, "text": " And maybe is necessary to grapple with what is in you know very clearly some of the wholism of human cognition", "tokens": [50392, 400, 1310, 307, 4818, 281, 27165, 306, 365, 437, 307, 294, 291, 458, 588, 4448, 512, 295, 264, 315, 401, 1434, 295, 1952, 46905, 50728], "temperature": 0.0, "avg_logprob": -0.16592788696289062, "compression_ratio": 1.4550561797752808, "no_speech_prob": 0.004607006907463074}, {"id": 972, "seek": 442928, "start": 4439.36, "end": 4441.36, "text": " So going back a few steps", "tokens": [50868, 407, 516, 646, 257, 1326, 4439, 50968], "temperature": 0.0, "avg_logprob": -0.16592788696289062, "compression_ratio": 1.4550561797752808, "no_speech_prob": 0.004607006907463074}, {"id": 973, "seek": 442928, "start": 4442.719999999999, "end": 4445.28, "text": " The intuitive physics part", "tokens": [51036, 440, 21769, 10649, 644, 51164], "temperature": 0.0, "avg_logprob": -0.16592788696289062, "compression_ratio": 1.4550561797752808, "no_speech_prob": 0.004607006907463074}, {"id": 974, "seek": 442928, "start": 4446.96, "end": 4448.96, "text": " In principle", "tokens": [51248, 682, 8665, 51348], "temperature": 0.0, "avg_logprob": -0.16592788696289062, "compression_ratio": 1.4550561797752808, "no_speech_prob": 0.004607006907463074}, {"id": 975, "seek": 442928, "start": 4449.5199999999995, "end": 4455.36, "text": " Animals could do that as well, right? Yes. So a chimp could have intuitive physics", "tokens": [51376, 47164, 727, 360, 300, 382, 731, 11, 558, 30, 1079, 13, 407, 257, 417, 8814, 727, 362, 21769, 10649, 51668], "temperature": 0.0, "avg_logprob": -0.16592788696289062, "compression_ratio": 1.4550561797752808, "no_speech_prob": 0.004607006907463074}, {"id": 976, "seek": 445536, "start": 4456.32, "end": 4459.5199999999995, "text": " Yeah, and joseph call and others have studied that a number of other", "tokens": [50412, 865, 11, 293, 361, 541, 950, 818, 293, 2357, 362, 9454, 300, 257, 1230, 295, 661, 50572], "temperature": 0.0, "avg_logprob": -0.18514384055624203, "compression_ratio": 1.7521008403361344, "no_speech_prob": 0.001283331774175167}, {"id": 977, "seek": 445536, "start": 4460.08, "end": 4464.32, "text": " Amanda ced and other brothers had studied other non-human primates and we're actually collaborating", "tokens": [50600, 20431, 269, 292, 293, 661, 8452, 632, 9454, 661, 2107, 12, 18796, 2886, 1024, 293, 321, 434, 767, 30188, 50812], "temperature": 0.0, "avg_logprob": -0.18514384055624203, "compression_ratio": 1.7521008403361344, "no_speech_prob": 0.001283331774175167}, {"id": 978, "seek": 445536, "start": 4464.719999999999, "end": 4466.88, "text": " With erica cart mill and some others studying", "tokens": [50832, 2022, 1189, 2262, 5467, 1728, 293, 512, 2357, 7601, 50940], "temperature": 0.0, "avg_logprob": -0.18514384055624203, "compression_ratio": 1.7521008403361344, "no_speech_prob": 0.001283331774175167}, {"id": 979, "seek": 445536, "start": 4467.5199999999995, "end": 4471.36, "text": " non-human primates on intuitive physics and I know colleagues", "tokens": [50972, 2107, 12, 18796, 2886, 1024, 322, 21769, 10649, 293, 286, 458, 7734, 51164], "temperature": 0.0, "avg_logprob": -0.18514384055624203, "compression_ratio": 1.7521008403361344, "no_speech_prob": 0.001283331774175167}, {"id": 980, "seek": 445536, "start": 4471.839999999999, "end": 4474.88, "text": " Who are studying analogous kinds of intuitive physics in rats?", "tokens": [51188, 2102, 366, 7601, 16660, 563, 3685, 295, 21769, 10649, 294, 25691, 30, 51340], "temperature": 0.0, "avg_logprob": -0.18514384055624203, "compression_ratio": 1.7521008403361344, "no_speech_prob": 0.001283331774175167}, {"id": 981, "seek": 445536, "start": 4476.08, "end": 4478.88, "text": " And I think even simpler organisms", "tokens": [51400, 400, 286, 519, 754, 18587, 22110, 51540], "temperature": 0.0, "avg_logprob": -0.18514384055624203, "compression_ratio": 1.7521008403361344, "no_speech_prob": 0.001283331774175167}, {"id": 982, "seek": 445536, "start": 4479.92, "end": 4484.16, "text": " Okay, so the difference between humans and", "tokens": [51592, 1033, 11, 370, 264, 2649, 1296, 6255, 293, 51804], "temperature": 0.0, "avg_logprob": -0.18514384055624203, "compression_ratio": 1.7521008403361344, "no_speech_prob": 0.001283331774175167}, {"id": 983, "seek": 448416, "start": 4485.12, "end": 4490.5599999999995, "text": " Other animals is going to be that language allows you to go further", "tokens": [50412, 5358, 4882, 307, 516, 281, 312, 300, 2856, 4045, 291, 281, 352, 3052, 50684], "temperature": 0.0, "avg_logprob": -0.17069556826636906, "compression_ratio": 1.9094650205761317, "no_speech_prob": 0.001596798188984394}, {"id": 984, "seek": 448416, "start": 4491.2, "end": 4494.24, "text": " Language allows you to go beyond intuitive physics", "tokens": [50716, 24445, 4045, 291, 281, 352, 4399, 21769, 10649, 50868], "temperature": 0.0, "avg_logprob": -0.17069556826636906, "compression_ratio": 1.9094650205761317, "no_speech_prob": 0.001596798188984394}, {"id": 985, "seek": 448416, "start": 4494.639999999999, "end": 4499.68, "text": " Whereas nothing is going to help these other animals go beyond intuitive physics", "tokens": [50888, 13813, 1825, 307, 516, 281, 854, 613, 661, 4882, 352, 4399, 21769, 10649, 51140], "temperature": 0.0, "avg_logprob": -0.17069556826636906, "compression_ratio": 1.9094650205761317, "no_speech_prob": 0.001596798188984394}, {"id": 986, "seek": 448416, "start": 4500.639999999999, "end": 4502.4, "text": " Yeah, I I think that's right. I mean again", "tokens": [51188, 865, 11, 286, 286, 519, 300, 311, 558, 13, 286, 914, 797, 51276], "temperature": 0.0, "avg_logprob": -0.17069556826636906, "compression_ratio": 1.9094650205761317, "no_speech_prob": 0.001596798188984394}, {"id": 987, "seek": 448416, "start": 4502.4, "end": 4506.16, "text": " I wouldn't say that all the all the mental models of other animals is just intuitive physics", "tokens": [51276, 286, 2759, 380, 584, 300, 439, 264, 439, 264, 4973, 5245, 295, 661, 4882, 307, 445, 21769, 10649, 51464], "temperature": 0.0, "avg_logprob": -0.17069556826636906, "compression_ratio": 1.9094650205761317, "no_speech_prob": 0.001596798188984394}, {"id": 988, "seek": 448416, "start": 4506.16, "end": 4511.44, "text": " They also have models of their social world like you know the chainy and save farce baboon metaphysics is mostly a social theory", "tokens": [51464, 814, 611, 362, 5245, 295, 641, 2093, 1002, 411, 291, 458, 264, 5021, 88, 293, 3155, 1400, 384, 7564, 4106, 30946, 41732, 307, 5240, 257, 2093, 5261, 51728], "temperature": 0.0, "avg_logprob": -0.17069556826636906, "compression_ratio": 1.9094650205761317, "no_speech_prob": 0.001596798188984394}, {"id": 989, "seek": 451144, "start": 4511.919999999999, "end": 4513.919999999999, "text": " But yeah, but I would say", "tokens": [50388, 583, 1338, 11, 457, 286, 576, 584, 50488], "temperature": 0.0, "avg_logprob": -0.11712185541788737, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.0008291784906759858}, {"id": 990, "seek": 451144, "start": 4514.48, "end": 4518.879999999999, "text": " Definitely that the key thing here and it's the real human singularity. I would say", "tokens": [50516, 12151, 300, 264, 2141, 551, 510, 293, 309, 311, 264, 957, 1952, 20010, 507, 13, 286, 576, 584, 50736], "temperature": 0.0, "avg_logprob": -0.11712185541788737, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.0008291784906759858}, {"id": 991, "seek": 451144, "start": 4519.679999999999, "end": 4525.04, "text": " Is that language allows us to construct to both enrich our intuitive physics to think about", "tokens": [50776, 1119, 300, 2856, 4045, 505, 281, 7690, 281, 1293, 18849, 527, 21769, 10649, 281, 519, 466, 51044], "temperature": 0.0, "avg_logprob": -0.11712185541788737, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.0008291784906759858}, {"id": 992, "seek": 451144, "start": 4525.44, "end": 4529.28, "text": " Aspects of the physical world that are not initially intuitive to us and maybe still aren't", "tokens": [51064, 1018, 1043, 82, 295, 264, 4001, 1002, 300, 366, 406, 9105, 21769, 281, 505, 293, 1310, 920, 3212, 380, 51256], "temperature": 0.0, "avg_logprob": -0.11712185541788737, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.0008291784906759858}, {"id": 993, "seek": 451144, "start": 4529.599999999999, "end": 4534.16, "text": " But also to construct things that just go totally beyond any any core domain that evolution gave us", "tokens": [51272, 583, 611, 281, 7690, 721, 300, 445, 352, 3879, 4399, 604, 604, 4965, 9274, 300, 9303, 2729, 505, 51500], "temperature": 0.0, "avg_logprob": -0.11712185541788737, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.0008291784906759858}, {"id": 994, "seek": 453416, "start": 4534.88, "end": 4535.92, "text": " Right", "tokens": [50400, 1779, 50452], "temperature": 0.0, "avg_logprob": -0.1663215160369873, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.003121482441201806}, {"id": 995, "seek": 453416, "start": 4535.92, "end": 4540.48, "text": " Okay, so one question that this brought up to me", "tokens": [50452, 1033, 11, 370, 472, 1168, 300, 341, 3038, 493, 281, 385, 50680], "temperature": 0.0, "avg_logprob": -0.1663215160369873, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.003121482441201806}, {"id": 996, "seek": 453416, "start": 4541.28, "end": 4547.5199999999995, "text": " Because what you're talking about are all the ways that humans are so good at what they do", "tokens": [50720, 1436, 437, 291, 434, 1417, 466, 366, 439, 264, 2098, 300, 6255, 366, 370, 665, 412, 437, 436, 360, 51032], "temperature": 0.0, "avg_logprob": -0.1663215160369873, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.003121482441201806}, {"id": 997, "seek": 453416, "start": 4548.24, "end": 4549.12, "text": " um", "tokens": [51068, 1105, 51112], "temperature": 0.0, "avg_logprob": -0.1663215160369873, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.003121482441201806}, {"id": 998, "seek": 453416, "start": 4549.12, "end": 4553.12, "text": " But then there are ways that they're not so good at what they do", "tokens": [51112, 583, 550, 456, 366, 2098, 300, 436, 434, 406, 370, 665, 412, 437, 436, 360, 51312], "temperature": 0.0, "avg_logprob": -0.1663215160369873, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.003121482441201806}, {"id": 999, "seek": 453416, "start": 4553.68, "end": 4557.76, "text": " And far transfer is one of those examples", "tokens": [51340, 400, 1400, 5003, 307, 472, 295, 729, 5110, 51544], "temperature": 0.0, "avg_logprob": -0.1663215160369873, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.003121482441201806}, {"id": 1000, "seek": 453416, "start": 4558.48, "end": 4559.76, "text": " um", "tokens": [51580, 1105, 51644], "temperature": 0.0, "avg_logprob": -0.1663215160369873, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.003121482441201806}, {"id": 1001, "seek": 455976, "start": 4559.76, "end": 4565.280000000001, "text": " And there there are some things that are hard for people to learn", "tokens": [50364, 400, 456, 456, 366, 512, 721, 300, 366, 1152, 337, 561, 281, 1466, 50640], "temperature": 0.0, "avg_logprob": -0.1492712703751929, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.00015111347602214664}, {"id": 1002, "seek": 455976, "start": 4566.0, "end": 4570.08, "text": " Or some places where it's hard to get from", "tokens": [50676, 1610, 512, 3190, 689, 309, 311, 1152, 281, 483, 490, 50880], "temperature": 0.0, "avg_logprob": -0.1492712703751929, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.00015111347602214664}, {"id": 1003, "seek": 455976, "start": 4571.2, "end": 4573.2, "text": " A to z", "tokens": [50936, 316, 281, 710, 51036], "temperature": 0.0, "avg_logprob": -0.1492712703751929, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.00015111347602214664}, {"id": 1004, "seek": 455976, "start": 4573.84, "end": 4576.400000000001, "text": " Even though you think you've learned a", "tokens": [51068, 2754, 1673, 291, 519, 291, 600, 3264, 257, 51196], "temperature": 0.0, "avg_logprob": -0.1492712703751929, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.00015111347602214664}, {"id": 1005, "seek": 455976, "start": 4577.12, "end": 4581.68, "text": " So there's a famous little story about someone going to their logic teacher", "tokens": [51232, 407, 456, 311, 257, 4618, 707, 1657, 466, 1580, 516, 281, 641, 9952, 5027, 51460], "temperature": 0.0, "avg_logprob": -0.1492712703751929, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.00015111347602214664}, {"id": 1006, "seek": 455976, "start": 4582.4800000000005, "end": 4584.4800000000005, "text": " asking about", "tokens": [51500, 3365, 466, 51600], "temperature": 0.0, "avg_logprob": -0.1492712703751929, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.00015111347602214664}, {"id": 1007, "seek": 455976, "start": 4584.56, "end": 4589.6, "text": " Understanding if a then b and the logic teacher spends a lot of time on that", "tokens": [51604, 36858, 498, 257, 550, 272, 293, 264, 9952, 5027, 25620, 257, 688, 295, 565, 322, 300, 51856], "temperature": 0.0, "avg_logprob": -0.1492712703751929, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.00015111347602214664}, {"id": 1008, "seek": 458976, "start": 4590.16, "end": 4592.16, "text": " And the student says", "tokens": [50384, 400, 264, 3107, 1619, 50484], "temperature": 0.0, "avg_logprob": -0.1649155616760254, "compression_ratio": 1.4497354497354498, "no_speech_prob": 0.0001851354754762724}, {"id": 1009, "seek": 458976, "start": 4592.4800000000005, "end": 4594.4800000000005, "text": " Okay, I think I get it", "tokens": [50500, 1033, 11, 286, 519, 286, 483, 309, 50600], "temperature": 0.0, "avg_logprob": -0.1649155616760254, "compression_ratio": 1.4497354497354498, "no_speech_prob": 0.0001851354754762724}, {"id": 1010, "seek": 458976, "start": 4594.88, "end": 4597.4400000000005, "text": " But we try it with r and s now", "tokens": [50620, 583, 321, 853, 309, 365, 367, 293, 262, 586, 50748], "temperature": 0.0, "avg_logprob": -0.1649155616760254, "compression_ratio": 1.4497354497354498, "no_speech_prob": 0.0001851354754762724}, {"id": 1011, "seek": 458976, "start": 4598.24, "end": 4599.6, "text": " um", "tokens": [50788, 1105, 50856], "temperature": 0.0, "avg_logprob": -0.1649155616760254, "compression_ratio": 1.4497354497354498, "no_speech_prob": 0.0001851354754762724}, {"id": 1012, "seek": 458976, "start": 4599.6, "end": 4605.04, "text": " So that's something that just shouldn't occur, but it does occur", "tokens": [50856, 407, 300, 311, 746, 300, 445, 4659, 380, 5160, 11, 457, 309, 775, 5160, 51128], "temperature": 0.0, "avg_logprob": -0.1649155616760254, "compression_ratio": 1.4497354497354498, "no_speech_prob": 0.0001851354754762724}, {"id": 1013, "seek": 458976, "start": 4605.76, "end": 4608.88, "text": " Uh, even with smart people", "tokens": [51164, 4019, 11, 754, 365, 4069, 561, 51320], "temperature": 0.0, "avg_logprob": -0.1649155616760254, "compression_ratio": 1.4497354497354498, "no_speech_prob": 0.0001851354754762724}, {"id": 1014, "seek": 458976, "start": 4609.84, "end": 4614.400000000001, "text": " So like when you try to teach people about experimental design", "tokens": [51368, 407, 411, 562, 291, 853, 281, 2924, 561, 466, 17069, 1715, 51596], "temperature": 0.0, "avg_logprob": -0.1649155616760254, "compression_ratio": 1.4497354497354498, "no_speech_prob": 0.0001851354754762724}, {"id": 1015, "seek": 458976, "start": 4615.04, "end": 4618.4800000000005, "text": " It's hard for them to see confounds often", "tokens": [51628, 467, 311, 1152, 337, 552, 281, 536, 1497, 4432, 2049, 51800], "temperature": 0.0, "avg_logprob": -0.1649155616760254, "compression_ratio": 1.4497354497354498, "no_speech_prob": 0.0001851354754762724}, {"id": 1016, "seek": 461848, "start": 4619.04, "end": 4621.44, "text": " um, it's hard for them to", "tokens": [50392, 1105, 11, 309, 311, 1152, 337, 552, 281, 50512], "temperature": 0.0, "avg_logprob": -0.14221995375877203, "compression_ratio": 1.6302083333333333, "no_speech_prob": 0.0006980600883252919}, {"id": 1017, "seek": 461848, "start": 4622.4, "end": 4626.719999999999, "text": " Get at what's wrong with some experimental design", "tokens": [50560, 3240, 412, 437, 311, 2085, 365, 512, 17069, 1715, 50776], "temperature": 0.0, "avg_logprob": -0.14221995375877203, "compression_ratio": 1.6302083333333333, "no_speech_prob": 0.0006980600883252919}, {"id": 1018, "seek": 461848, "start": 4627.5199999999995, "end": 4633.28, "text": " And it's not that they don't have a general intelligence. It's not that they don't have some principles", "tokens": [50816, 400, 309, 311, 406, 300, 436, 500, 380, 362, 257, 2674, 7599, 13, 467, 311, 406, 300, 436, 500, 380, 362, 512, 9156, 51104], "temperature": 0.0, "avg_logprob": -0.14221995375877203, "compression_ratio": 1.6302083333333333, "no_speech_prob": 0.0006980600883252919}, {"id": 1019, "seek": 461848, "start": 4633.839999999999, "end": 4635.839999999999, "text": " but it's as if", "tokens": [51132, 457, 309, 311, 382, 498, 51232], "temperature": 0.0, "avg_logprob": -0.14221995375877203, "compression_ratio": 1.6302083333333333, "no_speech_prob": 0.0006980600883252919}, {"id": 1020, "seek": 461848, "start": 4635.839999999999, "end": 4638.08, "text": " What they've learned is just", "tokens": [51232, 708, 436, 600, 3264, 307, 445, 51344], "temperature": 0.0, "avg_logprob": -0.14221995375877203, "compression_ratio": 1.6302083333333333, "no_speech_prob": 0.0006980600883252919}, {"id": 1021, "seek": 461848, "start": 4638.799999999999, "end": 4647.04, "text": " Too far away as far as the the string of examples goes for them to get to to the next one", "tokens": [51380, 11395, 1400, 1314, 382, 1400, 382, 264, 264, 6798, 295, 5110, 1709, 337, 552, 281, 483, 281, 281, 264, 958, 472, 51792], "temperature": 0.0, "avg_logprob": -0.14221995375877203, "compression_ratio": 1.6302083333333333, "no_speech_prob": 0.0006980600883252919}, {"id": 1022, "seek": 464704, "start": 4647.76, "end": 4650.32, "text": " So I'm wondering how", "tokens": [50400, 407, 286, 478, 6359, 577, 50528], "temperature": 0.0, "avg_logprob": -0.177054640952121, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0016733916709199548}, {"id": 1023, "seek": 464704, "start": 4651.2, "end": 4652.96, "text": " On your system", "tokens": [50572, 1282, 428, 1185, 50660], "temperature": 0.0, "avg_logprob": -0.177054640952121, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0016733916709199548}, {"id": 1024, "seek": 464704, "start": 4652.96, "end": 4656.72, "text": " those kinds of limitations would be modeled", "tokens": [50660, 729, 3685, 295, 15705, 576, 312, 37140, 50848], "temperature": 0.0, "avg_logprob": -0.177054640952121, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0016733916709199548}, {"id": 1025, "seek": 464704, "start": 4657.5199999999995, "end": 4663.6, "text": " Yeah, no, that's a that's a great point a great question and a great pointer. I think to work that", "tokens": [50888, 865, 11, 572, 11, 300, 311, 257, 300, 311, 257, 869, 935, 257, 869, 1168, 293, 257, 869, 23918, 13, 286, 519, 281, 589, 300, 51192], "temperature": 0.0, "avg_logprob": -0.177054640952121, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0016733916709199548}, {"id": 1026, "seek": 464704, "start": 4664.24, "end": 4670.24, "text": " We could and should do more of so the the stuff I talked about at the very end, which is again this the work with", "tokens": [51224, 492, 727, 293, 820, 360, 544, 295, 370, 264, 264, 1507, 286, 2825, 466, 412, 264, 588, 917, 11, 597, 307, 797, 341, 264, 589, 365, 51524], "temperature": 0.0, "avg_logprob": -0.177054640952121, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0016733916709199548}, {"id": 1027, "seek": 464704, "start": 4670.88, "end": 4673.04, "text": " Tyler and uh, katie collins and", "tokens": [51556, 16869, 293, 2232, 11, 16536, 414, 1263, 1292, 293, 51664], "temperature": 0.0, "avg_logprob": -0.177054640952121, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0016733916709199548}, {"id": 1028, "seek": 467304, "start": 4673.76, "end": 4676.56, "text": " Lansing and and and very much with leo wang also", "tokens": [50400, 441, 44829, 293, 293, 293, 588, 709, 365, 476, 78, 261, 656, 611, 50540], "temperature": 0.0, "avg_logprob": -0.17006712693434495, "compression_ratio": 1.628440366972477, "no_speech_prob": 0.016137097030878067}, {"id": 1029, "seek": 467304, "start": 4677.28, "end": 4678.72, "text": " um", "tokens": [50576, 1105, 50648], "temperature": 0.0, "avg_logprob": -0.17006712693434495, "compression_ratio": 1.628440366972477, "no_speech_prob": 0.016137097030878067}, {"id": 1030, "seek": 467304, "start": 4678.72, "end": 4679.84, "text": " is", "tokens": [50648, 307, 50704], "temperature": 0.0, "avg_logprob": -0.17006712693434495, "compression_ratio": 1.628440366972477, "no_speech_prob": 0.016137097030878067}, {"id": 1031, "seek": 467304, "start": 4679.84, "end": 4682.72, "text": " What we're what we're getting out there like is is again how we can", "tokens": [50704, 708, 321, 434, 437, 321, 434, 1242, 484, 456, 411, 307, 307, 797, 577, 321, 393, 50848], "temperature": 0.0, "avg_logprob": -0.17006712693434495, "compression_ratio": 1.628440366972477, "no_speech_prob": 0.016137097030878067}, {"id": 1032, "seek": 467304, "start": 4683.6, "end": 4689.6, "text": " Describe a situation in language, maybe even very implicitly just start talking and then see if", "tokens": [50892, 3885, 8056, 257, 2590, 294, 2856, 11, 1310, 754, 588, 26947, 356, 445, 722, 1417, 293, 550, 536, 498, 51192], "temperature": 0.0, "avg_logprob": -0.17006712693434495, "compression_ratio": 1.628440366972477, "no_speech_prob": 0.016137097030878067}, {"id": 1033, "seek": 467304, "start": 4690.32, "end": 4696.0, "text": " See if this architecture can be used to construct a mental model needed to reason about that situation", "tokens": [51228, 3008, 498, 341, 9482, 393, 312, 1143, 281, 7690, 257, 4973, 2316, 2978, 281, 1778, 466, 300, 2590, 51512], "temperature": 0.0, "avg_logprob": -0.17006712693434495, "compression_ratio": 1.628440366972477, "no_speech_prob": 0.016137097030878067}, {"id": 1034, "seek": 467304, "start": 4696.0, "end": 4698.0, "text": " And you could also make it better", "tokens": [51512, 400, 291, 727, 611, 652, 309, 1101, 51612], "temperature": 0.0, "avg_logprob": -0.17006712693434495, "compression_ratio": 1.628440366972477, "no_speech_prob": 0.016137097030878067}, {"id": 1035, "seek": 469800, "start": 4698.16, "end": 4703.92, "text": " More robust if I explicitly give you instructions as in the educational context you're talking about if I try to explain to someone", "tokens": [50372, 5048, 13956, 498, 286, 20803, 976, 291, 9415, 382, 294, 264, 10189, 4319, 291, 434, 1417, 466, 498, 286, 853, 281, 2903, 281, 1580, 50660], "temperature": 0.0, "avg_logprob": -0.1270158031199238, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.059945013374090195}, {"id": 1036, "seek": 469800, "start": 4704.24, "end": 4710.64, "text": " Logic or experimental design actually another collaborator on that project said zhong is actually very interested for his thesis", "tokens": [50676, 49898, 420, 17069, 1715, 767, 1071, 5091, 1639, 322, 300, 1716, 848, 710, 46507, 307, 767, 588, 3102, 337, 702, 22288, 50996], "temperature": 0.0, "avg_logprob": -0.1270158031199238, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.059945013374090195}, {"id": 1037, "seek": 469800, "start": 4711.04, "end": 4713.84, "text": " In how we learn like logic and learn to reason through language", "tokens": [51016, 682, 577, 321, 1466, 411, 9952, 293, 1466, 281, 1778, 807, 2856, 51156], "temperature": 0.0, "avg_logprob": -0.1270158031199238, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.059945013374090195}, {"id": 1038, "seek": 469800, "start": 4714.72, "end": 4718.88, "text": " But so we have you know, our framework is providing possibly a way to do that", "tokens": [51200, 583, 370, 321, 362, 291, 458, 11, 527, 8388, 307, 6530, 6264, 257, 636, 281, 360, 300, 51408], "temperature": 0.0, "avg_logprob": -0.1270158031199238, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.059945013374090195}, {"id": 1039, "seek": 469800, "start": 4719.44, "end": 4720.4, "text": " but", "tokens": [51436, 457, 51484], "temperature": 0.0, "avg_logprob": -0.1270158031199238, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.059945013374090195}, {"id": 1040, "seek": 469800, "start": 4720.4, "end": 4722.96, "text": " In order for it to work at least in the current system", "tokens": [51484, 682, 1668, 337, 309, 281, 589, 412, 1935, 294, 264, 2190, 1185, 51612], "temperature": 0.0, "avg_logprob": -0.1270158031199238, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.059945013374090195}, {"id": 1041, "seek": 472296, "start": 4723.52, "end": 4728.88, "text": " We do the kind of thing that lm folks are generally doing these days, which is some kind of few shot prompting", "tokens": [50392, 492, 360, 264, 733, 295, 551, 300, 287, 76, 4024, 366, 5101, 884, 613, 1708, 11, 597, 307, 512, 733, 295, 1326, 3347, 12391, 278, 50660], "temperature": 0.0, "avg_logprob": -0.1135154174546064, "compression_ratio": 1.75, "no_speech_prob": 0.025939326733350754}, {"id": 1042, "seek": 472296, "start": 4729.2, "end": 4735.36, "text": " We prompt the system with examples of other mental models and language for describing them in", "tokens": [50676, 492, 12391, 264, 1185, 365, 5110, 295, 661, 4973, 5245, 293, 2856, 337, 16141, 552, 294, 50984], "temperature": 0.0, "avg_logprob": -0.1135154174546064, "compression_ratio": 1.75, "no_speech_prob": 0.025939326733350754}, {"id": 1043, "seek": 472296, "start": 4735.92, "end": 4741.04, "text": " Related domains they could be similar or they could be further and what we already can see right is i mean", "tokens": [51012, 8738, 770, 25514, 436, 727, 312, 2531, 420, 436, 727, 312, 3052, 293, 437, 321, 1217, 393, 536, 558, 307, 741, 914, 51268], "temperature": 0.0, "avg_logprob": -0.1135154174546064, "compression_ratio": 1.75, "no_speech_prob": 0.025939326733350754}, {"id": 1044, "seek": 472296, "start": 4741.6, "end": 4744.08, "text": " The interesting thing is can you generalize to new domains?", "tokens": [51296, 440, 1880, 551, 307, 393, 291, 2674, 1125, 281, 777, 25514, 30, 51420], "temperature": 0.0, "avg_logprob": -0.1135154174546064, "compression_ratio": 1.75, "no_speech_prob": 0.025939326733350754}, {"id": 1045, "seek": 472296, "start": 4744.24, "end": 4746.24, "text": " So our system is somewhat able to do that", "tokens": [51428, 407, 527, 1185, 307, 8344, 1075, 281, 360, 300, 51528], "temperature": 0.0, "avg_logprob": -0.1135154174546064, "compression_ratio": 1.75, "no_speech_prob": 0.025939326733350754}, {"id": 1046, "seek": 472296, "start": 4746.64, "end": 4751.28, "text": " But you know, this is a place where near or far transfer would be relevant at least so far", "tokens": [51548, 583, 291, 458, 11, 341, 307, 257, 1081, 689, 2651, 420, 1400, 5003, 576, 312, 7340, 412, 1935, 370, 1400, 51780], "temperature": 0.0, "avg_logprob": -0.1135154174546064, "compression_ratio": 1.75, "no_speech_prob": 0.025939326733350754}, {"id": 1047, "seek": 475128, "start": 4751.28, "end": 4755.36, "text": " you know as you might expect you have to have some domain that's at least reasonably close", "tokens": [50364, 291, 458, 382, 291, 1062, 2066, 291, 362, 281, 362, 512, 9274, 300, 311, 412, 1935, 23551, 1998, 50568], "temperature": 0.0, "avg_logprob": -0.14072513580322266, "compression_ratio": 1.8858131487889274, "no_speech_prob": 0.0035923318937420845}, {"id": 1048, "seek": 475128, "start": 4756.08, "end": 4761.44, "text": " And for example, we're trying we could transfer from like a tug of war to a relay race or to some other sport setting", "tokens": [50604, 400, 337, 1365, 11, 321, 434, 1382, 321, 727, 5003, 490, 411, 257, 33543, 295, 1516, 281, 257, 24214, 4569, 420, 281, 512, 661, 7282, 3287, 50872], "temperature": 0.0, "avg_logprob": -0.14072513580322266, "compression_ratio": 1.8858131487889274, "no_speech_prob": 0.0035923318937420845}, {"id": 1049, "seek": 475128, "start": 4762.16, "end": 4764.8, "text": " Or from a couple of sport settings to yet some new sport", "tokens": [50908, 1610, 490, 257, 1916, 295, 7282, 6257, 281, 1939, 512, 777, 7282, 51040], "temperature": 0.0, "avg_logprob": -0.14072513580322266, "compression_ratio": 1.8858131487889274, "no_speech_prob": 0.0035923318937420845}, {"id": 1050, "seek": 475128, "start": 4765.28, "end": 4765.679999999999, "text": " um", "tokens": [51064, 1105, 51084], "temperature": 0.0, "avg_logprob": -0.14072513580322266, "compression_ratio": 1.8858131487889274, "no_speech_prob": 0.0035923318937420845}, {"id": 1051, "seek": 475128, "start": 4765.679999999999, "end": 4770.5599999999995, "text": " Or you might transfer from a sport setting to like a math competition or to some other kind of thing", "tokens": [51084, 1610, 291, 1062, 5003, 490, 257, 7282, 3287, 281, 411, 257, 5221, 6211, 420, 281, 512, 661, 733, 295, 551, 51328], "temperature": 0.0, "avg_logprob": -0.14072513580322266, "compression_ratio": 1.8858131487889274, "no_speech_prob": 0.0035923318937420845}, {"id": 1052, "seek": 475128, "start": 4770.5599999999995, "end": 4774.08, "text": " So if it really interesting ideas about abstraction and metaphor", "tokens": [51328, 407, 498, 309, 534, 1880, 3487, 466, 37765, 293, 19157, 51504], "temperature": 0.0, "avg_logprob": -0.14072513580322266, "compression_ratio": 1.8858131487889274, "no_speech_prob": 0.0035923318937420845}, {"id": 1053, "seek": 475128, "start": 4774.96, "end": 4779.5199999999995, "text": " Analogy some of the kinds of things that the the transfer literature has studied are going to be relevant here", "tokens": [51548, 16128, 7794, 512, 295, 264, 3685, 295, 721, 300, 264, 264, 5003, 10394, 575, 9454, 366, 516, 281, 312, 7340, 510, 51776], "temperature": 0.0, "avg_logprob": -0.14072513580322266, "compression_ratio": 1.8858131487889274, "no_speech_prob": 0.0035923318937420845}, {"id": 1054, "seek": 477952, "start": 4779.52, "end": 4784.320000000001, "text": " We're just beginning but what you're pointing to is actually some set of phenomena and things we should really", "tokens": [50364, 492, 434, 445, 2863, 457, 437, 291, 434, 12166, 281, 307, 767, 512, 992, 295, 22004, 293, 721, 321, 820, 534, 50604], "temperature": 0.0, "avg_logprob": -0.12195115930893842, "compression_ratio": 1.7516339869281046, "no_speech_prob": 0.0007792627438902855}, {"id": 1055, "seek": 477952, "start": 4784.8, "end": 4787.76, "text": " Engage with and I expect you know at least based on our current system", "tokens": [50628, 2469, 609, 365, 293, 286, 2066, 291, 458, 412, 1935, 2361, 322, 527, 2190, 1185, 50776], "temperature": 0.0, "avg_logprob": -0.12195115930893842, "compression_ratio": 1.7516339869281046, "no_speech_prob": 0.0007792627438902855}, {"id": 1056, "seek": 477952, "start": 4788.320000000001, "end": 4790.72, "text": " It will definitely struggle sometimes with far transfer", "tokens": [50804, 467, 486, 2138, 7799, 2171, 365, 1400, 5003, 50924], "temperature": 0.0, "avg_logprob": -0.12195115930893842, "compression_ratio": 1.7516339869281046, "no_speech_prob": 0.0007792627438902855}, {"id": 1057, "seek": 477952, "start": 4791.280000000001, "end": 4796.4800000000005, "text": " Um, which be which might be because at that level, you know, I I I drew this contrast at the very beginning", "tokens": [50952, 3301, 11, 597, 312, 597, 1062, 312, 570, 412, 300, 1496, 11, 291, 458, 11, 286, 286, 286, 12804, 341, 8712, 412, 264, 588, 2863, 51212], "temperature": 0.0, "avg_logprob": -0.12195115930893842, "compression_ratio": 1.7516339869281046, "no_speech_prob": 0.0007792627438902855}, {"id": 1058, "seek": 477952, "start": 4797.200000000001, "end": 4799.4400000000005, "text": " between the pattern recognition approach", "tokens": [51248, 1296, 264, 5102, 11150, 3109, 51360], "temperature": 0.0, "avg_logprob": -0.12195115930893842, "compression_ratio": 1.7516339869281046, "no_speech_prob": 0.0007792627438902855}, {"id": 1059, "seek": 477952, "start": 4799.92, "end": 4802.240000000001, "text": " Data driven learning and the mental models approach", "tokens": [51384, 11888, 9555, 2539, 293, 264, 4973, 5245, 3109, 51500], "temperature": 0.0, "avg_logprob": -0.12195115930893842, "compression_ratio": 1.7516339869281046, "no_speech_prob": 0.0007792627438902855}, {"id": 1060, "seek": 477952, "start": 4802.4800000000005, "end": 4806.72, "text": " But at that level we are doing a kind of pattern recognition. It's just not patterns in the world", "tokens": [51512, 583, 412, 300, 1496, 321, 366, 884, 257, 733, 295, 5102, 11150, 13, 467, 311, 445, 406, 8294, 294, 264, 1002, 51724], "temperature": 0.0, "avg_logprob": -0.12195115930893842, "compression_ratio": 1.7516339869281046, "no_speech_prob": 0.0007792627438902855}, {"id": 1061, "seek": 480672, "start": 4806.8, "end": 4810.56, "text": " It's patterns and thoughts and thought structures and ways of expressing them", "tokens": [50368, 467, 311, 8294, 293, 4598, 293, 1194, 9227, 293, 2098, 295, 22171, 552, 50556], "temperature": 0.0, "avg_logprob": -0.09023047830456886, "compression_ratio": 1.8, "no_speech_prob": 0.004131224937736988}, {"id": 1062, "seek": 480672, "start": 4811.04, "end": 4814.64, "text": " And you know at that point if if if that kind of really interesting", "tokens": [50580, 400, 291, 458, 412, 300, 935, 498, 498, 498, 300, 733, 295, 534, 1880, 50760], "temperature": 0.0, "avg_logprob": -0.09023047830456886, "compression_ratio": 1.8, "no_speech_prob": 0.004131224937736988}, {"id": 1063, "seek": 480672, "start": 4814.96, "end": 4820.16, "text": " But much more abstract kind of pattern recognition is what's driving your ability to construct new models of new situations", "tokens": [50776, 583, 709, 544, 12649, 733, 295, 5102, 11150, 307, 437, 311, 4840, 428, 3485, 281, 7690, 777, 5245, 295, 777, 6851, 51036], "temperature": 0.0, "avg_logprob": -0.09023047830456886, "compression_ratio": 1.8, "no_speech_prob": 0.004131224937736988}, {"id": 1064, "seek": 480672, "start": 4820.64, "end": 4825.360000000001, "text": " Or at least initially before you've had formal instruction or when you're just at the beginning of formal instruction", "tokens": [51060, 1610, 412, 1935, 9105, 949, 291, 600, 632, 9860, 10951, 420, 562, 291, 434, 445, 412, 264, 2863, 295, 9860, 10951, 51296], "temperature": 0.0, "avg_logprob": -0.09023047830456886, "compression_ratio": 1.8, "no_speech_prob": 0.004131224937736988}, {"id": 1065, "seek": 480672, "start": 4825.76, "end": 4832.320000000001, "text": " Then we should expect that it to have it to have the same kinds of fragility with far transfer that any machine learning approach does", "tokens": [51316, 1396, 321, 820, 2066, 300, 309, 281, 362, 309, 281, 362, 264, 912, 3685, 295, 9241, 1140, 365, 1400, 5003, 300, 604, 3479, 2539, 3109, 775, 51644], "temperature": 0.0, "avg_logprob": -0.09023047830456886, "compression_ratio": 1.8, "no_speech_prob": 0.004131224937736988}, {"id": 1066, "seek": 483232, "start": 4832.4, "end": 4835.92, "text": " Okay, sorry to be a gilljoy here. We have about 10 minutes left", "tokens": [50368, 1033, 11, 2597, 281, 312, 257, 290, 373, 1994, 510, 13, 492, 362, 466, 1266, 2077, 1411, 50544], "temperature": 0.0, "avg_logprob": -0.1921510513012226, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.0026668747887015343}, {"id": 1067, "seek": 483232, "start": 4835.92, "end": 4839.84, "text": " But we have at least four people that want to ask questions. So what I want to suggest", "tokens": [50544, 583, 321, 362, 412, 1935, 1451, 561, 300, 528, 281, 1029, 1651, 13, 407, 437, 286, 528, 281, 3402, 50740], "temperature": 0.0, "avg_logprob": -0.1921510513012226, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.0026668747887015343}, {"id": 1068, "seek": 483232, "start": 4840.4, "end": 4843.36, "text": " to alina and julia and", "tokens": [50768, 281, 419, 1426, 293, 30764, 654, 293, 50916], "temperature": 0.0, "avg_logprob": -0.1921510513012226, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.0026668747887015343}, {"id": 1069, "seek": 483232, "start": 4844.32, "end": 4845.5199999999995, "text": " also", "tokens": [50964, 611, 51024], "temperature": 0.0, "avg_logprob": -0.1921510513012226, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.0026668747887015343}, {"id": 1070, "seek": 483232, "start": 4845.5199999999995, "end": 4852.4, "text": " Uh leijin and stephan carlson is to raise your hand again, and I'll recognize you in the order that you raise your hand", "tokens": [51024, 4019, 476, 1718, 259, 293, 1823, 3451, 1032, 75, 3015, 307, 281, 5300, 428, 1011, 797, 11, 293, 286, 603, 5521, 291, 294, 264, 1668, 300, 291, 5300, 428, 1011, 51368], "temperature": 0.0, "avg_logprob": -0.1921510513012226, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.0026668747887015343}, {"id": 1071, "seek": 483232, "start": 4853.44, "end": 4855.44, "text": " Okay, julia go ahead", "tokens": [51420, 1033, 11, 30764, 654, 352, 2286, 51520], "temperature": 0.0, "avg_logprob": -0.1921510513012226, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.0026668747887015343}, {"id": 1072, "seek": 483232, "start": 4855.599999999999, "end": 4857.599999999999, "text": " well, um, so I was wondering", "tokens": [51528, 731, 11, 1105, 11, 370, 286, 390, 6359, 51628], "temperature": 0.0, "avg_logprob": -0.1921510513012226, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.0026668747887015343}, {"id": 1073, "seek": 485760, "start": 4857.6, "end": 4868.320000000001, "text": " I was thinking like maybe you can use the um the LLM translator plus the probabilistic models that system to um", "tokens": [50364, 286, 390, 1953, 411, 1310, 291, 393, 764, 264, 1105, 264, 441, 43, 44, 35223, 1804, 264, 31959, 3142, 5245, 300, 1185, 281, 1105, 50900], "temperature": 0.0, "avg_logprob": -0.22329904238382975, "compression_ratio": 1.524390243902439, "no_speech_prob": 0.0013878908939659595}, {"id": 1074, "seek": 485760, "start": 4869.280000000001, "end": 4876.72, "text": " Model how the beliefs of a reader would change over the course of being told a story or like reading a book", "tokens": [50948, 17105, 577, 264, 13585, 295, 257, 15149, 576, 1319, 670, 264, 1164, 295, 885, 1907, 257, 1657, 420, 411, 3760, 257, 1446, 51320], "temperature": 0.0, "avg_logprob": -0.22329904238382975, "compression_ratio": 1.524390243902439, "no_speech_prob": 0.0013878908939659595}, {"id": 1075, "seek": 485760, "start": 4877.84, "end": 4880.320000000001, "text": " And then you could look across", "tokens": [51376, 400, 550, 291, 727, 574, 2108, 51500], "temperature": 0.0, "avg_logprob": -0.22329904238382975, "compression_ratio": 1.524390243902439, "no_speech_prob": 0.0013878908939659595}, {"id": 1076, "seek": 488032, "start": 4881.28, "end": 4885.5199999999995, "text": " The genres or books or whatever to kind of get a sense of", "tokens": [50412, 440, 30057, 420, 3642, 420, 2035, 281, 733, 295, 483, 257, 2020, 295, 50624], "temperature": 0.0, "avg_logprob": -0.13270672162373862, "compression_ratio": 1.6682692307692308, "no_speech_prob": 0.025942683219909668}, {"id": 1077, "seek": 488032, "start": 4886.0, "end": 4892.32, "text": " What the kind of model of those stories being told since the stories are themselves a world model what that would be", "tokens": [50648, 708, 264, 733, 295, 2316, 295, 729, 3676, 885, 1907, 1670, 264, 3676, 366, 2969, 257, 1002, 2316, 437, 300, 576, 312, 50964], "temperature": 0.0, "avg_logprob": -0.13270672162373862, "compression_ratio": 1.6682692307692308, "no_speech_prob": 0.025942683219909668}, {"id": 1078, "seek": 488032, "start": 4892.96, "end": 4894.96, "text": " and that you could incorporate", "tokens": [50996, 293, 300, 291, 727, 16091, 51096], "temperature": 0.0, "avg_logprob": -0.13270672162373862, "compression_ratio": 1.6682692307692308, "no_speech_prob": 0.025942683219909668}, {"id": 1079, "seek": 488032, "start": 4896.16, "end": 4900.5599999999995, "text": " Because if you have a guess because something that you didn't like touch on in", "tokens": [51156, 1436, 498, 291, 362, 257, 2041, 570, 746, 300, 291, 994, 380, 411, 2557, 322, 294, 51376], "temperature": 0.0, "avg_logprob": -0.13270672162373862, "compression_ratio": 1.6682692307692308, "no_speech_prob": 0.025942683219909668}, {"id": 1080, "seek": 488032, "start": 4901.2, "end": 4904.639999999999, "text": " You know in your act talk, which I'm sure you didn't have time", "tokens": [51408, 509, 458, 294, 428, 605, 751, 11, 597, 286, 478, 988, 291, 994, 380, 362, 565, 51580], "temperature": 0.0, "avg_logprob": -0.13270672162373862, "compression_ratio": 1.6682692307692308, "no_speech_prob": 0.025942683219909668}, {"id": 1081, "seek": 490464, "start": 4904.72, "end": 4911.12, "text": " But was that like if you have a guess about where you are in the trajectory of the story you expect", "tokens": [50368, 583, 390, 300, 411, 498, 291, 362, 257, 2041, 466, 689, 291, 366, 294, 264, 21512, 295, 264, 1657, 291, 2066, 50688], "temperature": 0.0, "avg_logprob": -0.17695030413175883, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.02515995502471924}, {"id": 1082, "seek": 490464, "start": 4911.200000000001, "end": 4913.04, "text": " Then you have a guess about", "tokens": [50692, 1396, 291, 362, 257, 2041, 466, 50784], "temperature": 0.0, "avg_logprob": -0.17695030413175883, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.02515995502471924}, {"id": 1083, "seek": 490464, "start": 4913.04, "end": 4915.04, "text": " About what you think's going to happen next", "tokens": [50784, 7769, 437, 291, 519, 311, 516, 281, 1051, 958, 50884], "temperature": 0.0, "avg_logprob": -0.17695030413175883, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.02515995502471924}, {"id": 1084, "seek": 490464, "start": 4915.6, "end": 4920.240000000001, "text": " Wrap it up because we um, so I was just wondering like do you do yeah, that's a great question", "tokens": [50912, 41291, 309, 493, 570, 321, 1105, 11, 370, 286, 390, 445, 6359, 411, 360, 291, 360, 1338, 11, 300, 311, 257, 869, 1168, 51144], "temperature": 0.0, "avg_logprob": -0.17695030413175883, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.02515995502471924}, {"id": 1085, "seek": 490464, "start": 4920.8, "end": 4922.4800000000005, "text": " Um, let me just try to answer it really quickly yet", "tokens": [51172, 3301, 11, 718, 385, 445, 853, 281, 1867, 309, 534, 2661, 1939, 51256], "temperature": 0.0, "avg_logprob": -0.17695030413175883, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.02515995502471924}, {"id": 1086, "seek": 490464, "start": 4922.88, "end": 4926.88, "text": " Leo wong would love that question because in addition to being a great cognitive scientist", "tokens": [51276, 19344, 261, 556, 576, 959, 300, 1168, 570, 294, 4500, 281, 885, 257, 869, 15605, 12662, 51476], "temperature": 0.0, "avg_logprob": -0.17695030413175883, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.02515995502471924}, {"id": 1087, "seek": 490464, "start": 4926.88, "end": 4929.280000000001, "text": " They are also a writer they write stories and even novels", "tokens": [51476, 814, 366, 611, 257, 9936, 436, 2464, 3676, 293, 754, 24574, 51596], "temperature": 0.0, "avg_logprob": -0.17695030413175883, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.02515995502471924}, {"id": 1088, "seek": 492928, "start": 4929.679999999999, "end": 4935.599999999999, "text": " And one of the things that we've been working on is little mini like three act structures and things like that that", "tokens": [50384, 400, 472, 295, 264, 721, 300, 321, 600, 668, 1364, 322, 307, 707, 8382, 411, 1045, 605, 9227, 293, 721, 411, 300, 300, 50680], "temperature": 0.0, "avg_logprob": -0.1194846721795889, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.03730705752968788}, {"id": 1089, "seek": 492928, "start": 4936.4, "end": 4938.4, "text": " Follow either classic narrative structures or other", "tokens": [50720, 9876, 2139, 7230, 9977, 9227, 420, 661, 50820], "temperature": 0.0, "avg_logprob": -0.1194846721795889, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.03730705752968788}, {"id": 1090, "seek": 492928, "start": 4939.04, "end": 4943.92, "text": " Things and and you know, maybe not surprisingly those are places where we see a big gap between", "tokens": [50852, 9514, 293, 293, 291, 458, 11, 1310, 406, 17600, 729, 366, 3190, 689, 321, 536, 257, 955, 7417, 1296, 51096], "temperature": 0.0, "avg_logprob": -0.1194846721795889, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.03730705752968788}, {"id": 1091, "seek": 492928, "start": 4944.639999999999, "end": 4946.639999999999, "text": " human story understanding and", "tokens": [51132, 1952, 1657, 3701, 293, 51232], "temperature": 0.0, "avg_logprob": -0.1194846721795889, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.03730705752968788}, {"id": 1092, "seek": 492928, "start": 4947.12, "end": 4953.04, "text": " LLMs even the state of the art ones, but exactly we're trying to use these models to capture how that kind of", "tokens": [51256, 441, 43, 26386, 754, 264, 1785, 295, 264, 1523, 2306, 11, 457, 2293, 321, 434, 1382, 281, 764, 613, 5245, 281, 7983, 577, 300, 733, 295, 51552], "temperature": 0.0, "avg_logprob": -0.1194846721795889, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.03730705752968788}, {"id": 1093, "seek": 492928, "start": 4953.84, "end": 4956.96, "text": " You know journey of understanding might unfold and also even how", "tokens": [51592, 509, 458, 4671, 295, 3701, 1062, 17980, 293, 611, 754, 577, 51748], "temperature": 0.0, "avg_logprob": -0.1194846721795889, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.03730705752968788}, {"id": 1094, "seek": 495696, "start": 4957.68, "end": 4961.84, "text": " A creator might create it. So another student that we've worked with Karthik Chandra has", "tokens": [50400, 316, 14181, 1062, 1884, 309, 13, 407, 1071, 3107, 300, 321, 600, 2732, 365, 8009, 392, 1035, 761, 18401, 575, 50608], "temperature": 0.0, "avg_logprob": -0.18155345753726795, "compression_ratio": 1.835016835016835, "no_speech_prob": 0.0014100766275078058}, {"id": 1095, "seek": 495696, "start": 4962.64, "end": 4968.64, "text": " Had had some work at last year's cog side conference on storytelling as inverse inverse planning and the idea that like if somebody's", "tokens": [50648, 12298, 632, 512, 589, 412, 1036, 1064, 311, 46521, 1252, 7586, 322, 21479, 382, 17340, 17340, 5038, 293, 264, 1558, 300, 411, 498, 2618, 311, 50948], "temperature": 0.0, "avg_logprob": -0.18155345753726795, "compression_ratio": 1.835016835016835, "no_speech_prob": 0.0014100766275078058}, {"id": 1096, "seek": 495696, "start": 4969.12, "end": 4973.36, "text": " Understanding another character's journey by doing some inverse planning as in those theory of my models", "tokens": [50972, 36858, 1071, 2517, 311, 4671, 538, 884, 512, 17340, 5038, 382, 294, 729, 5261, 295, 452, 5245, 51184], "temperature": 0.0, "avg_logprob": -0.18155345753726795, "compression_ratio": 1.835016835016835, "no_speech_prob": 0.0014100766275078058}, {"id": 1097, "seek": 495696, "start": 4973.6, "end": 4977.52, "text": " Then a storyteller can try to invert that inverse planner to to convey", "tokens": [51196, 1396, 257, 17541, 14983, 393, 853, 281, 33966, 300, 17340, 31268, 281, 281, 16965, 51392], "temperature": 0.0, "avg_logprob": -0.18155345753726795, "compression_ratio": 1.835016835016835, "no_speech_prob": 0.0014100766275078058}, {"id": 1098, "seek": 495696, "start": 4978.0, "end": 4979.28, "text": " the emotional", "tokens": [51416, 264, 6863, 51480], "temperature": 0.0, "avg_logprob": -0.18155345753726795, "compression_ratio": 1.835016835016835, "no_speech_prob": 0.0014100766275078058}, {"id": 1099, "seek": 495696, "start": 4979.28, "end": 4985.52, "text": " Or mental journey that the character has and that's a way to use this toolkit for both story creation as well as story understanding", "tokens": [51480, 1610, 4973, 4671, 300, 264, 2517, 575, 293, 300, 311, 257, 636, 281, 764, 341, 40167, 337, 1293, 1657, 8016, 382, 731, 382, 1657, 3701, 51792], "temperature": 0.0, "avg_logprob": -0.18155345753726795, "compression_ratio": 1.835016835016835, "no_speech_prob": 0.0014100766275078058}, {"id": 1100, "seek": 498552, "start": 4985.52, "end": 4990.72, "text": " So again, it's those are mostly promissory notes, but great question and future research", "tokens": [50364, 407, 797, 11, 309, 311, 729, 366, 5240, 2234, 891, 827, 5570, 11, 457, 869, 1168, 293, 2027, 2132, 50624], "temperature": 0.0, "avg_logprob": -0.16453664004802704, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.001244324492290616}, {"id": 1101, "seek": 498552, "start": 4990.72, "end": 4995.200000000001, "text": " I think from Karthik and leo and others will address it. Thank you. Thanks", "tokens": [50624, 286, 519, 490, 8009, 392, 1035, 293, 476, 78, 293, 2357, 486, 2985, 309, 13, 1044, 291, 13, 2561, 50848], "temperature": 0.0, "avg_logprob": -0.16453664004802704, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.001244324492290616}, {"id": 1102, "seek": 498552, "start": 4996.0, "end": 4999.360000000001, "text": " Hi, I'm gonna ask a question in person josh. This is ever", "tokens": [50888, 2421, 11, 286, 478, 799, 1029, 257, 1168, 294, 954, 361, 3019, 13, 639, 307, 1562, 51056], "temperature": 0.0, "avg_logprob": -0.16453664004802704, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.001244324492290616}, {"id": 1103, "seek": 498552, "start": 4999.92, "end": 5004.240000000001, "text": " Hi, I yeah go for it. I I really liked your talk. By the way, I watched it on video. Yeah", "tokens": [51084, 2421, 11, 286, 1338, 352, 337, 309, 13, 286, 286, 534, 4501, 428, 751, 13, 3146, 264, 636, 11, 286, 6337, 309, 322, 960, 13, 865, 51300], "temperature": 0.0, "avg_logprob": -0.16453664004802704, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.001244324492290616}, {"id": 1104, "seek": 498552, "start": 5004.72, "end": 5006.240000000001, "text": " Cool, great. Okay. So yeah", "tokens": [51324, 8561, 11, 869, 13, 1033, 13, 407, 1338, 51400], "temperature": 0.0, "avg_logprob": -0.16453664004802704, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.001244324492290616}, {"id": 1105, "seek": 498552, "start": 5006.240000000001, "end": 5012.160000000001, "text": " I was gonna so following the work that you were showing where you were basically based also on what you were saying and then in the", "tokens": [51400, 286, 390, 799, 370, 3480, 264, 589, 300, 291, 645, 4099, 689, 291, 645, 1936, 2361, 611, 322, 437, 291, 645, 1566, 293, 550, 294, 264, 51696], "temperature": 0.0, "avg_logprob": -0.16453664004802704, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.001244324492290616}, {"id": 1106, "seek": 501216, "start": 5012.96, "end": 5018.72, "text": " Question period where you were, you know, few shot prompting these language models to produce probabilistic language", "tokens": [50404, 14464, 2896, 689, 291, 645, 11, 291, 458, 11, 1326, 3347, 12391, 278, 613, 2856, 5245, 281, 5258, 31959, 3142, 2856, 50692], "temperature": 0.0, "avg_logprob": -0.13758932275975005, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.0016223570564761758}, {"id": 1107, "seek": 501216, "start": 5021.28, "end": 5024.48, "text": " World models, I guess I have my I have sort of a more medic question", "tokens": [50820, 3937, 5245, 11, 286, 2041, 286, 362, 452, 286, 362, 1333, 295, 257, 544, 4355, 1168, 50980], "temperature": 0.0, "avg_logprob": -0.13758932275975005, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.0016223570564761758}, {"id": 1108, "seek": 501216, "start": 5024.48, "end": 5030.5599999999995, "text": " Which is do you see that as just a way to sort of help step by step reasoning in these models?", "tokens": [50980, 3013, 307, 360, 291, 536, 300, 382, 445, 257, 636, 281, 1333, 295, 854, 1823, 538, 1823, 21577, 294, 613, 5245, 30, 51284], "temperature": 0.0, "avg_logprob": -0.13758932275975005, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.0016223570564761758}, {"id": 1109, "seek": 501216, "start": 5030.639999999999, "end": 5033.04, "text": " Or do you think this is sort of evidence for", "tokens": [51288, 1610, 360, 291, 519, 341, 307, 1333, 295, 4467, 337, 51408], "temperature": 0.0, "avg_logprob": -0.13758932275975005, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.0016223570564761758}, {"id": 1110, "seek": 501216, "start": 5033.76, "end": 5037.599999999999, "text": " Early sort of possible world modeling being learned in these models", "tokens": [51444, 18344, 1333, 295, 1944, 1002, 15983, 885, 3264, 294, 613, 5245, 51636], "temperature": 0.0, "avg_logprob": -0.13758932275975005, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.0016223570564761758}, {"id": 1111, "seek": 501216, "start": 5040.08, "end": 5042.08, "text": " Um", "tokens": [51760, 3301, 51860], "temperature": 0.0, "avg_logprob": -0.13758932275975005, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.0016223570564761758}, {"id": 1112, "seek": 504216, "start": 5043.12, "end": 5048.639999999999, "text": " I'm not sure if I see it as either of those. Um, I I mean, uh, uh, so", "tokens": [50412, 286, 478, 406, 988, 498, 286, 536, 309, 382, 2139, 295, 729, 13, 3301, 11, 286, 286, 914, 11, 2232, 11, 2232, 11, 370, 50688], "temperature": 0.0, "avg_logprob": -0.13004365243202398, "compression_ratio": 1.69921875, "no_speech_prob": 0.00023778050672262907}, {"id": 1113, "seek": 504216, "start": 5050.72, "end": 5054.639999999999, "text": " I don't think of it as either doing step step by step reasoning in language models or", "tokens": [50792, 286, 500, 380, 519, 295, 309, 382, 2139, 884, 1823, 1823, 538, 1823, 21577, 294, 2856, 5245, 420, 50988], "temperature": 0.0, "avg_logprob": -0.13004365243202398, "compression_ratio": 1.69921875, "no_speech_prob": 0.00023778050672262907}, {"id": 1114, "seek": 504216, "start": 5055.76, "end": 5061.44, "text": " Evidence that they do possible world modeling. It might be relate. I mean, there is you know, I've obviously as you know", "tokens": [51044, 5689, 2778, 300, 436, 360, 1944, 1002, 15983, 13, 467, 1062, 312, 10961, 13, 286, 914, 11, 456, 307, 291, 458, 11, 286, 600, 2745, 382, 291, 458, 51328], "temperature": 0.0, "avg_logprob": -0.13004365243202398, "compression_ratio": 1.69921875, "no_speech_prob": 0.00023778050672262907}, {"id": 1115, "seek": 504216, "start": 5061.76, "end": 5064.08, "text": " You talked about some and others have, you know, there's certainly", "tokens": [51344, 509, 2825, 466, 512, 293, 2357, 362, 11, 291, 458, 11, 456, 311, 3297, 51460], "temperature": 0.0, "avg_logprob": -0.13004365243202398, "compression_ratio": 1.69921875, "no_speech_prob": 0.00023778050672262907}, {"id": 1116, "seek": 504216, "start": 5064.88, "end": 5069.44, "text": " Um, a lot of evidence that llms if you try to just use them as end-to-end reasoning systems", "tokens": [51500, 3301, 11, 257, 688, 295, 4467, 300, 287, 75, 2592, 498, 291, 853, 281, 445, 764, 552, 382, 917, 12, 1353, 12, 521, 21577, 3652, 51728], "temperature": 0.0, "avg_logprob": -0.13004365243202398, "compression_ratio": 1.69921875, "no_speech_prob": 0.00023778050672262907}, {"id": 1117, "seek": 506944, "start": 5069.44, "end": 5074.0, "text": " They can benefit from step by step by reasoning and there might be some emergent world modeling capacities", "tokens": [50364, 814, 393, 5121, 490, 1823, 538, 1823, 538, 21577, 293, 456, 1062, 312, 512, 4345, 6930, 1002, 15983, 39396, 50592], "temperature": 0.0, "avg_logprob": -0.08538952607374925, "compression_ratio": 1.8471337579617835, "no_speech_prob": 0.003944565542042255}, {"id": 1118, "seek": 506944, "start": 5074.0, "end": 5079.679999999999, "text": " But you know, again, I look at that and I see a really interesting and mixed pattern of successes and failures", "tokens": [50592, 583, 291, 458, 11, 797, 11, 286, 574, 412, 300, 293, 286, 536, 257, 534, 1880, 293, 7467, 5102, 295, 26101, 293, 20774, 50876], "temperature": 0.0, "avg_logprob": -0.08538952607374925, "compression_ratio": 1.8471337579617835, "no_speech_prob": 0.003944565542042255}, {"id": 1119, "seek": 506944, "start": 5080.0, "end": 5085.5199999999995, "text": " And depending on who's writing the paper and what their agendas are you can highlight the successes or you can highlight the failures and the gaps", "tokens": [50892, 400, 5413, 322, 567, 311, 3579, 264, 3035, 293, 437, 641, 623, 45252, 366, 291, 393, 5078, 264, 26101, 420, 291, 393, 5078, 264, 20774, 293, 264, 15031, 51168], "temperature": 0.0, "avg_logprob": -0.08538952607374925, "compression_ratio": 1.8471337579617835, "no_speech_prob": 0.003944565542042255}, {"id": 1120, "seek": 506944, "start": 5086.0, "end": 5087.12, "text": " um", "tokens": [51192, 1105, 51248], "temperature": 0.0, "avg_logprob": -0.08538952607374925, "compression_ratio": 1.8471337579617835, "no_speech_prob": 0.003944565542042255}, {"id": 1121, "seek": 506944, "start": 5087.12, "end": 5092.799999999999, "text": " The to me that an objective perspective is that it's patchy and fragile although extremely impressive and really interesting", "tokens": [51248, 440, 281, 385, 300, 364, 10024, 4585, 307, 300, 309, 311, 9972, 88, 293, 23847, 4878, 4664, 8992, 293, 534, 1880, 51532], "temperature": 0.0, "avg_logprob": -0.08538952607374925, "compression_ratio": 1.8471337579617835, "no_speech_prob": 0.003944565542042255}, {"id": 1122, "seek": 506944, "start": 5093.04, "end": 5096.5599999999995, "text": " So the way I think of what we're trying to do is to say, yeah, there's there's you know", "tokens": [51544, 407, 264, 636, 286, 519, 295, 437, 321, 434, 1382, 281, 360, 307, 281, 584, 11, 1338, 11, 456, 311, 456, 311, 291, 458, 51720], "temperature": 0.0, "avg_logprob": -0.08538952607374925, "compression_ratio": 1.8471337579617835, "no_speech_prob": 0.003944565542042255}, {"id": 1123, "seek": 509656, "start": 5097.280000000001, "end": 5103.200000000001, "text": " Various kinds of interesting approximate implicit knowledge that those models have that can be used sometimes to succeed", "tokens": [50400, 14662, 851, 3685, 295, 1880, 30874, 26947, 3601, 300, 729, 5245, 362, 300, 393, 312, 1143, 2171, 281, 7754, 50696], "temperature": 0.0, "avg_logprob": -0.12809933148897612, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.001500809215940535}, {"id": 1124, "seek": 509656, "start": 5103.68, "end": 5106.88, "text": " In reasoning complex sequences of reasoning or world modeling", "tokens": [50720, 682, 21577, 3997, 22978, 295, 21577, 420, 1002, 15983, 50880], "temperature": 0.0, "avg_logprob": -0.12809933148897612, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.001500809215940535}, {"id": 1125, "seek": 509656, "start": 5107.200000000001, "end": 5112.56, "text": " But I think a more robust way to use it is the way we're using it a more both a more human like and a more robust for ai", "tokens": [50896, 583, 286, 519, 257, 544, 13956, 636, 281, 764, 309, 307, 264, 636, 321, 434, 1228, 309, 257, 544, 1293, 257, 544, 1952, 411, 293, 257, 544, 13956, 337, 9783, 51164], "temperature": 0.0, "avg_logprob": -0.12809933148897612, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.001500809215940535}, {"id": 1126, "seek": 509656, "start": 5112.72, "end": 5114.72, "text": " way to use it which is to", "tokens": [51172, 636, 281, 764, 309, 597, 307, 281, 51272], "temperature": 0.0, "avg_logprob": -0.12809933148897612, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.001500809215940535}, {"id": 1127, "seek": 509656, "start": 5114.88, "end": 5118.400000000001, "text": " Which is to condition and construct these bespoke world models", "tokens": [51280, 3013, 307, 281, 4188, 293, 7690, 613, 4097, 48776, 1002, 5245, 51456], "temperature": 0.0, "avg_logprob": -0.12809933148897612, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.001500809215940535}, {"id": 1128, "seek": 509656, "start": 5119.120000000001, "end": 5121.120000000001, "text": " that you know where the", "tokens": [51492, 300, 291, 458, 689, 264, 51592], "temperature": 0.0, "avg_logprob": -0.12809933148897612, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.001500809215940535}, {"id": 1129, "seek": 509656, "start": 5121.68, "end": 5124.96, "text": " long chains of sequential reasoning or actual", "tokens": [51620, 938, 12626, 295, 42881, 21577, 420, 3539, 51784], "temperature": 0.0, "avg_logprob": -0.12809933148897612, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.001500809215940535}, {"id": 1130, "seek": 512496, "start": 5125.92, "end": 5129.2, "text": " You know coherent world modeling are there", "tokens": [50412, 509, 458, 36239, 1002, 15983, 366, 456, 50576], "temperature": 0.0, "avg_logprob": -0.1319420960572389, "compression_ratio": 1.7945736434108528, "no_speech_prob": 0.0009396874811500311}, {"id": 1131, "seek": 512496, "start": 5130.24, "end": 5131.84, "text": " By construction", "tokens": [50628, 3146, 6435, 50708], "temperature": 0.0, "avg_logprob": -0.1319420960572389, "compression_ratio": 1.7945736434108528, "no_speech_prob": 0.0009396874811500311}, {"id": 1132, "seek": 512496, "start": 5131.84, "end": 5136.96, "text": " But you know, there are other limits like again as as I was saying, you know Bayesian inference in really complex models", "tokens": [50708, 583, 291, 458, 11, 456, 366, 661, 10406, 411, 797, 382, 382, 286, 390, 1566, 11, 291, 458, 7840, 42434, 38253, 294, 534, 3997, 5245, 50964], "temperature": 0.0, "avg_logprob": -0.1319420960572389, "compression_ratio": 1.7945736434108528, "no_speech_prob": 0.0009396874811500311}, {"id": 1133, "seek": 512496, "start": 5137.76, "end": 5140.32, "text": " Is very difficult and I don't think people do it. There's a lot of evidence", "tokens": [51004, 1119, 588, 2252, 293, 286, 500, 380, 519, 561, 360, 309, 13, 821, 311, 257, 688, 295, 4467, 51132], "temperature": 0.0, "avg_logprob": -0.1319420960572389, "compression_ratio": 1.7945736434108528, "no_speech_prob": 0.0009396874811500311}, {"id": 1134, "seek": 512496, "start": 5140.32, "end": 5141.12, "text": " They don't do it", "tokens": [51132, 814, 500, 380, 360, 309, 51172], "temperature": 0.0, "avg_logprob": -0.1319420960572389, "compression_ratio": 1.7945736434108528, "no_speech_prob": 0.0009396874811500311}, {"id": 1135, "seek": 512496, "start": 5141.12, "end": 5144.56, "text": " But they seem to do it very generally in in just the right small model", "tokens": [51172, 583, 436, 1643, 281, 360, 309, 588, 5101, 294, 294, 445, 264, 558, 1359, 2316, 51344], "temperature": 0.0, "avg_logprob": -0.1319420960572389, "compression_ratio": 1.7945736434108528, "no_speech_prob": 0.0009396874811500311}, {"id": 1136, "seek": 512496, "start": 5144.64, "end": 5149.44, "text": " At least in cases where they have the relevant world knowledge to construct those models and when they don't they don't", "tokens": [51348, 1711, 1935, 294, 3331, 689, 436, 362, 264, 7340, 1002, 3601, 281, 7690, 729, 5245, 293, 562, 436, 500, 380, 436, 500, 380, 51588], "temperature": 0.0, "avg_logprob": -0.1319420960572389, "compression_ratio": 1.7945736434108528, "no_speech_prob": 0.0009396874811500311}, {"id": 1137, "seek": 514944, "start": 5150.0, "end": 5151.599999999999, "text": " Right, so that's all you know", "tokens": [50392, 1779, 11, 370, 300, 311, 439, 291, 458, 50472], "temperature": 0.0, "avg_logprob": -0.15276967198395533, "compression_ratio": 1.73, "no_speech_prob": 0.01565554179251194}, {"id": 1138, "seek": 514944, "start": 5151.599999999999, "end": 5154.48, "text": " Our minds have mixed patterns of successes and failures there too", "tokens": [50472, 2621, 9634, 362, 7467, 8294, 295, 26101, 293, 20774, 456, 886, 50616], "temperature": 0.0, "avg_logprob": -0.15276967198395533, "compression_ratio": 1.73, "no_speech_prob": 0.01565554179251194}, {"id": 1139, "seek": 514944, "start": 5154.719999999999, "end": 5159.759999999999, "text": " But I think this toolkit is better matched to the mixed patterns of successes and failures, which is", "tokens": [50628, 583, 286, 519, 341, 40167, 307, 1101, 21447, 281, 264, 7467, 8294, 295, 26101, 293, 20774, 11, 597, 307, 50880], "temperature": 0.0, "avg_logprob": -0.15276967198395533, "compression_ratio": 1.73, "no_speech_prob": 0.01565554179251194}, {"id": 1140, "seek": 514944, "start": 5160.48, "end": 5165.679999999999, "text": " What what i'm trying to get out if that makes sense the the the one that we see in humans as opposed to the the weird head scratching", "tokens": [50916, 708, 437, 741, 478, 1382, 281, 483, 484, 498, 300, 1669, 2020, 264, 264, 264, 472, 300, 321, 536, 294, 6255, 382, 8851, 281, 264, 264, 3657, 1378, 29699, 51176], "temperature": 0.0, "avg_logprob": -0.15276967198395533, "compression_ratio": 1.73, "no_speech_prob": 0.01565554179251194}, {"id": 1141, "seek": 514944, "start": 5165.759999999999, "end": 5172.08, "text": " Like super intelligent cases in some places and then super dumb Jesus and others that you just see in a pure sequence model", "tokens": [51180, 1743, 1687, 13232, 3331, 294, 512, 3190, 293, 550, 1687, 10316, 2705, 293, 2357, 300, 291, 445, 536, 294, 257, 6075, 8310, 2316, 51496], "temperature": 0.0, "avg_logprob": -0.15276967198395533, "compression_ratio": 1.73, "no_speech_prob": 0.01565554179251194}, {"id": 1142, "seek": 514944, "start": 5173.44, "end": 5177.04, "text": " I'm told Alina can go next. Thanks for your answer. Yeah, thanks", "tokens": [51564, 286, 478, 1907, 967, 1426, 393, 352, 958, 13, 2561, 337, 428, 1867, 13, 865, 11, 3231, 51744], "temperature": 0.0, "avg_logprob": -0.15276967198395533, "compression_ratio": 1.73, "no_speech_prob": 0.01565554179251194}, {"id": 1143, "seek": 517704, "start": 5178.0, "end": 5180.0, "text": " Good question", "tokens": [50412, 2205, 1168, 50512], "temperature": 0.0, "avg_logprob": -0.2844205098609402, "compression_ratio": 1.440217391304348, "no_speech_prob": 0.00220933323726058}, {"id": 1144, "seek": 517704, "start": 5181.36, "end": 5183.36, "text": " Alina pick it up or you lose your turn", "tokens": [50580, 967, 1426, 1888, 309, 493, 420, 291, 3624, 428, 1261, 50680], "temperature": 0.0, "avg_logprob": -0.2844205098609402, "compression_ratio": 1.440217391304348, "no_speech_prob": 0.00220933323726058}, {"id": 1145, "seek": 517704, "start": 5185.68, "end": 5192.72, "text": " Yes, um, hello, uh, thank you for insightful talk. Um, Josh so", "tokens": [50796, 1079, 11, 1105, 11, 7751, 11, 2232, 11, 1309, 291, 337, 46401, 751, 13, 3301, 11, 9785, 370, 51148], "temperature": 0.0, "avg_logprob": -0.2844205098609402, "compression_ratio": 1.440217391304348, "no_speech_prob": 0.00220933323726058}, {"id": 1146, "seek": 517704, "start": 5193.68, "end": 5202.88, "text": " You said that um, and correct me if I'm misheard that language allows us to construct meaning that it's totally out of bounds of evolutionary gain", "tokens": [51196, 509, 848, 300, 1105, 11, 293, 3006, 385, 498, 286, 478, 3346, 42915, 300, 2856, 4045, 505, 281, 7690, 3620, 300, 309, 311, 3879, 484, 295, 29905, 295, 27567, 6052, 51656], "temperature": 0.0, "avg_logprob": -0.2844205098609402, "compression_ratio": 1.440217391304348, "no_speech_prob": 0.00220933323726058}, {"id": 1147, "seek": 517704, "start": 5203.5199999999995, "end": 5205.2, "text": " um", "tokens": [51688, 1105, 51772], "temperature": 0.0, "avg_logprob": -0.2844205098609402, "compression_ratio": 1.440217391304348, "no_speech_prob": 0.00220933323726058}, {"id": 1148, "seek": 520520, "start": 5205.2, "end": 5210.639999999999, "text": " Is if uh, if this is what you said, I think this is uh, super profound. So", "tokens": [50364, 1119, 498, 2232, 11, 498, 341, 307, 437, 291, 848, 11, 286, 519, 341, 307, 2232, 11, 1687, 14382, 13, 407, 50636], "temperature": 0.0, "avg_logprob": -0.19611406850290822, "compression_ratio": 1.6213592233009708, "no_speech_prob": 0.000906116736587137}, {"id": 1149, "seek": 520520, "start": 5214.5599999999995, "end": 5221.84, "text": " Yeah, I I'm sure I'm not the only person who said that many people sure. Okay. It's just a reminder. I guess profound truth. Yes", "tokens": [50832, 865, 11, 286, 286, 478, 988, 286, 478, 406, 264, 787, 954, 567, 848, 300, 867, 561, 988, 13, 1033, 13, 467, 311, 445, 257, 13548, 13, 286, 2041, 14382, 3494, 13, 1079, 51196], "temperature": 0.0, "avg_logprob": -0.19611406850290822, "compression_ratio": 1.6213592233009708, "no_speech_prob": 0.000906116736587137}, {"id": 1150, "seek": 520520, "start": 5222.639999999999, "end": 5224.639999999999, "text": " Okay, uh, so the actual question", "tokens": [51236, 1033, 11, 2232, 11, 370, 264, 3539, 1168, 51336], "temperature": 0.0, "avg_logprob": -0.19611406850290822, "compression_ratio": 1.6213592233009708, "no_speech_prob": 0.000906116736587137}, {"id": 1151, "seek": 520520, "start": 5224.72, "end": 5230.0, "text": " So, um, you mentioned that the meaning on the world is constructed contextually and incrementally", "tokens": [51340, 407, 11, 1105, 11, 291, 2835, 300, 264, 3620, 322, 264, 1002, 307, 17083, 4319, 671, 293, 26200, 379, 51604], "temperature": 0.0, "avg_logprob": -0.19611406850290822, "compression_ratio": 1.6213592233009708, "no_speech_prob": 0.000906116736587137}, {"id": 1152, "seek": 523000, "start": 5230.4, "end": 5236.0, "text": " So could you please elaborate on how this process occurs and how it impacts our understanding of complex concepts?", "tokens": [50384, 407, 727, 291, 1767, 20945, 322, 577, 341, 1399, 11843, 293, 577, 309, 11606, 527, 3701, 295, 3997, 10392, 30, 50664], "temperature": 0.0, "avg_logprob": -0.1177409096101744, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.004605371039360762}, {"id": 1153, "seek": 523000, "start": 5236.08, "end": 5242.48, "text": " So so we transition between the um from incremental construction of the meaning to generalizations", "tokens": [50668, 407, 370, 321, 6034, 1296, 264, 1105, 490, 35759, 6435, 295, 264, 3620, 281, 2674, 14455, 50988], "temperature": 0.0, "avg_logprob": -0.1177409096101744, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.004605371039360762}, {"id": 1154, "seek": 523000, "start": 5244.16, "end": 5246.16, "text": " Right. So the way it works in our", "tokens": [51072, 1779, 13, 407, 264, 636, 309, 1985, 294, 527, 51172], "temperature": 0.0, "avg_logprob": -0.1177409096101744, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.004605371039360762}, {"id": 1155, "seek": 523000, "start": 5246.4, "end": 5249.28, "text": " I can just I can tell you how it works in the models that we've built so far", "tokens": [51184, 286, 393, 445, 286, 393, 980, 291, 577, 309, 1985, 294, 264, 5245, 300, 321, 600, 3094, 370, 1400, 51328], "temperature": 0.0, "avg_logprob": -0.1177409096101744, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.004605371039360762}, {"id": 1156, "seek": 523000, "start": 5249.36, "end": 5252.16, "text": " And again, this is not to say that it's exactly like this in the mind", "tokens": [51332, 400, 797, 11, 341, 307, 406, 281, 584, 300, 309, 311, 2293, 411, 341, 294, 264, 1575, 51472], "temperature": 0.0, "avg_logprob": -0.1177409096101744, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.004605371039360762}, {"id": 1157, "seek": 523000, "start": 5252.24, "end": 5255.6, "text": " But I think it might be something like this and a lot more work needs to be done", "tokens": [51476, 583, 286, 519, 309, 1062, 312, 746, 411, 341, 293, 257, 688, 544, 589, 2203, 281, 312, 1096, 51644], "temperature": 0.0, "avg_logprob": -0.1177409096101744, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.004605371039360762}, {"id": 1158, "seek": 525560, "start": 5255.92, "end": 5260.08, "text": " But the way it works in our models is something kind of like what you're familiar in a chat", "tokens": [50380, 583, 264, 636, 309, 1985, 294, 527, 5245, 307, 746, 733, 295, 411, 437, 291, 434, 4963, 294, 257, 5081, 50588], "temperature": 0.0, "avg_logprob": -0.17375864723856135, "compression_ratio": 1.8795620437956204, "no_speech_prob": 0.006901457440108061}, {"id": 1159, "seek": 525560, "start": 5260.160000000001, "end": 5262.64, "text": " Like if you've used a chat gbt or other", "tokens": [50592, 1743, 498, 291, 600, 1143, 257, 5081, 290, 4517, 420, 661, 50716], "temperature": 0.0, "avg_logprob": -0.17375864723856135, "compression_ratio": 1.8795620437956204, "no_speech_prob": 0.006901457440108061}, {"id": 1160, "seek": 525560, "start": 5263.280000000001, "end": 5265.04, "text": " conversational ai systems", "tokens": [50748, 2615, 1478, 9783, 3652, 50836], "temperature": 0.0, "avg_logprob": -0.17375864723856135, "compression_ratio": 1.8795620437956204, "no_speech_prob": 0.006901457440108061}, {"id": 1161, "seek": 525560, "start": 5265.04, "end": 5267.4400000000005, "text": " The way it's working is that basically there", "tokens": [50836, 440, 636, 309, 311, 1364, 307, 300, 1936, 456, 50956], "temperature": 0.0, "avg_logprob": -0.17375864723856135, "compression_ratio": 1.8795620437956204, "no_speech_prob": 0.006901457440108061}, {"id": 1162, "seek": 525560, "start": 5268.0, "end": 5272.240000000001, "text": " You know, if you're used to using chat gbt you type something it types something back and in the middle", "tokens": [50984, 509, 458, 11, 498, 291, 434, 1143, 281, 1228, 5081, 290, 4517, 291, 2010, 746, 309, 3467, 746, 646, 293, 294, 264, 2808, 51196], "temperature": 0.0, "avg_logprob": -0.17375864723856135, "compression_ratio": 1.8795620437956204, "no_speech_prob": 0.006901457440108061}, {"id": 1163, "seek": 525560, "start": 5272.4800000000005, "end": 5276.88, "text": " Some wheels turn and then you type something more in it wheels turn in the black box and it types more stuff", "tokens": [51208, 2188, 10046, 1261, 293, 550, 291, 2010, 746, 544, 294, 309, 10046, 1261, 294, 264, 2211, 2424, 293, 309, 3467, 544, 1507, 51428], "temperature": 0.0, "avg_logprob": -0.17375864723856135, "compression_ratio": 1.8795620437956204, "no_speech_prob": 0.006901457440108061}, {"id": 1164, "seek": 525560, "start": 5276.96, "end": 5278.96, "text": " Okay, so it's it's basically like that", "tokens": [51432, 1033, 11, 370, 309, 311, 309, 311, 1936, 411, 300, 51532], "temperature": 0.0, "avg_logprob": -0.17375864723856135, "compression_ratio": 1.8795620437956204, "no_speech_prob": 0.006901457440108061}, {"id": 1165, "seek": 525560, "start": 5279.360000000001, "end": 5284.4800000000005, "text": " It in that you at each each sentence is translated into some", "tokens": [51552, 467, 294, 300, 291, 412, 1184, 1184, 8174, 307, 16805, 666, 512, 51808], "temperature": 0.0, "avg_logprob": -0.17375864723856135, "compression_ratio": 1.8795620437956204, "no_speech_prob": 0.006901457440108061}, {"id": 1166, "seek": 528448, "start": 5285.12, "end": 5292.24, "text": " Expression in the language of thought roughly at the sentence level. I think sentences are real units of meaning like I I am very", "tokens": [50396, 21391, 2775, 294, 264, 2856, 295, 1194, 9810, 412, 264, 8174, 1496, 13, 286, 519, 16579, 366, 957, 6815, 295, 3620, 411, 286, 286, 669, 588, 50752], "temperature": 0.0, "avg_logprob": -0.14390301945233586, "compression_ratio": 1.7432950191570882, "no_speech_prob": 0.004196178168058395}, {"id": 1167, "seek": 528448, "start": 5293.2, "end": 5295.919999999999, "text": " respectful and admiring of many insights from language", "tokens": [50800, 26205, 293, 5910, 5057, 295, 867, 14310, 490, 2856, 50936], "temperature": 0.0, "avg_logprob": -0.14390301945233586, "compression_ratio": 1.7432950191570882, "no_speech_prob": 0.004196178168058395}, {"id": 1168, "seek": 528448, "start": 5296.16, "end": 5300.879999999999, "text": " Including that words are real and sentences are real and there's real syntactic structure to be understood there", "tokens": [50948, 27137, 300, 2283, 366, 957, 293, 16579, 366, 957, 293, 456, 311, 957, 23980, 19892, 3877, 281, 312, 7320, 456, 51184], "temperature": 0.0, "avg_logprob": -0.14390301945233586, "compression_ratio": 1.7432950191570882, "no_speech_prob": 0.004196178168058395}, {"id": 1169, "seek": 528448, "start": 5301.12, "end": 5304.32, "text": " That is only being approximated in some ways by these models", "tokens": [51196, 663, 307, 787, 885, 8542, 770, 294, 512, 2098, 538, 613, 5245, 51356], "temperature": 0.0, "avg_logprob": -0.14390301945233586, "compression_ratio": 1.7432950191570882, "no_speech_prob": 0.004196178168058395}, {"id": 1170, "seek": 528448, "start": 5304.32, "end": 5309.679999999999, "text": " But the key is that yes, there's a process the contextual process is like at the discourse level", "tokens": [51356, 583, 264, 2141, 307, 300, 2086, 11, 456, 311, 257, 1399, 264, 35526, 1399, 307, 411, 412, 264, 23938, 1496, 51624], "temperature": 0.0, "avg_logprob": -0.14390301945233586, "compression_ratio": 1.7432950191570882, "no_speech_prob": 0.004196178168058395}, {"id": 1171, "seek": 530968, "start": 5310.0, "end": 5312.8, "text": " You go sentence code sentence code sentence code", "tokens": [50380, 509, 352, 8174, 3089, 8174, 3089, 8174, 3089, 50520], "temperature": 0.0, "avg_logprob": -0.14042937655408844, "compression_ratio": 1.8245614035087718, "no_speech_prob": 0.02842479571700096}, {"id": 1172, "seek": 530968, "start": 5313.200000000001, "end": 5317.12, "text": " And each sentence to code translation is conditioned on the previous", "tokens": [50540, 400, 1184, 8174, 281, 3089, 12853, 307, 35833, 322, 264, 3894, 50736], "temperature": 0.0, "avg_logprob": -0.14042937655408844, "compression_ratio": 1.8245614035087718, "no_speech_prob": 0.02842479571700096}, {"id": 1173, "seek": 530968, "start": 5317.820000000001, "end": 5322.0, "text": " Conversational history or the discourse that that you have been interpreting. Okay", "tokens": [50771, 33247, 1478, 2503, 420, 264, 23938, 300, 300, 291, 362, 668, 37395, 13, 1033, 50980], "temperature": 0.0, "avg_logprob": -0.14042937655408844, "compression_ratio": 1.8245614035087718, "no_speech_prob": 0.02842479571700096}, {"id": 1174, "seek": 530968, "start": 5322.64, "end": 5324.240000000001, "text": " That's just a first approximation", "tokens": [51012, 663, 311, 445, 257, 700, 28023, 51092], "temperature": 0.0, "avg_logprob": -0.14042937655408844, "compression_ratio": 1.8245614035087718, "no_speech_prob": 0.02842479571700096}, {"id": 1175, "seek": 530968, "start": 5324.240000000001, "end": 5329.6, "text": " Just another thing you might want to do and surely you'll have to do is go back and edit previous code because if you realize", "tokens": [51092, 1449, 1071, 551, 291, 1062, 528, 281, 360, 293, 11468, 291, 603, 362, 281, 360, 307, 352, 646, 293, 8129, 3894, 3089, 570, 498, 291, 4325, 51360], "temperature": 0.0, "avg_logprob": -0.14042937655408844, "compression_ratio": 1.8245614035087718, "no_speech_prob": 0.02842479571700096}, {"id": 1176, "seek": 530968, "start": 5329.68, "end": 5335.04, "text": " Oh, I misunderstood something. I can't just add new code. I have to go back and edit the code that I wrote before", "tokens": [51364, 876, 11, 286, 33870, 746, 13, 286, 393, 380, 445, 909, 777, 3089, 13, 286, 362, 281, 352, 646, 293, 8129, 264, 3089, 300, 286, 4114, 949, 51632], "temperature": 0.0, "avg_logprob": -0.14042937655408844, "compression_ratio": 1.8245614035087718, "no_speech_prob": 0.02842479571700096}, {"id": 1177, "seek": 530968, "start": 5335.12, "end": 5337.12, "text": " Which is also something that code llms can do", "tokens": [51636, 3013, 307, 611, 746, 300, 3089, 287, 75, 2592, 393, 360, 51736], "temperature": 0.0, "avg_logprob": -0.14042937655408844, "compression_ratio": 1.8245614035087718, "no_speech_prob": 0.02842479571700096}, {"id": 1178, "seek": 533712, "start": 5337.68, "end": 5339.36, "text": " I'm not saying they'll do it right", "tokens": [50392, 286, 478, 406, 1566, 436, 603, 360, 309, 558, 50476], "temperature": 0.0, "avg_logprob": -0.16251273382277714, "compression_ratio": 1.52863436123348, "no_speech_prob": 0.0014973231591284275}, {"id": 1179, "seek": 533712, "start": 5339.36, "end": 5343.599999999999, "text": " But there's other processes that involve like checking and refining and fixing models", "tokens": [50476, 583, 456, 311, 661, 7555, 300, 9494, 411, 8568, 293, 1895, 1760, 293, 19442, 5245, 50688], "temperature": 0.0, "avg_logprob": -0.16251273382277714, "compression_ratio": 1.52863436123348, "no_speech_prob": 0.0014973231591284275}, {"id": 1180, "seek": 533712, "start": 5344.16, "end": 5345.12, "text": " That are written in this way", "tokens": [50716, 663, 366, 3720, 294, 341, 636, 50764], "temperature": 0.0, "avg_logprob": -0.16251273382277714, "compression_ratio": 1.52863436123348, "no_speech_prob": 0.0014973231591284275}, {"id": 1181, "seek": 533712, "start": 5345.12, "end": 5349.76, "text": " But maybe that gives you some sense of the contextual dynamics of how language is understood in this model", "tokens": [50764, 583, 1310, 300, 2709, 291, 512, 2020, 295, 264, 35526, 15679, 295, 577, 2856, 307, 7320, 294, 341, 2316, 50996], "temperature": 0.0, "avg_logprob": -0.16251273382277714, "compression_ratio": 1.52863436123348, "no_speech_prob": 0.0014973231591284275}, {"id": 1182, "seek": 533712, "start": 5351.44, "end": 5353.44, "text": " Last question lay jim", "tokens": [51080, 5264, 1168, 2360, 361, 332, 51180], "temperature": 0.0, "avg_logprob": -0.16251273382277714, "compression_ratio": 1.52863436123348, "no_speech_prob": 0.0014973231591284275}, {"id": 1183, "seek": 533712, "start": 5355.599999999999, "end": 5357.599999999999, "text": " Hi", "tokens": [51288, 2421, 51388], "temperature": 0.0, "avg_logprob": -0.16251273382277714, "compression_ratio": 1.52863436123348, "no_speech_prob": 0.0014973231591284275}, {"id": 1184, "seek": 533712, "start": 5358.64, "end": 5362.08, "text": " Joshua, thank you for for the presentation. It's very interesting", "tokens": [51440, 24005, 11, 1309, 291, 337, 337, 264, 5860, 13, 467, 311, 588, 1880, 51612], "temperature": 0.0, "avg_logprob": -0.16251273382277714, "compression_ratio": 1.52863436123348, "no_speech_prob": 0.0014973231591284275}, {"id": 1185, "seek": 536208, "start": 5362.64, "end": 5367.5199999999995, "text": " Um, I have an engineering background, but I'm not an expert in", "tokens": [50392, 3301, 11, 286, 362, 364, 7043, 3678, 11, 457, 286, 478, 406, 364, 5844, 294, 50636], "temperature": 0.0, "avg_logprob": -0.2936861618705418, "compression_ratio": 1.412121212121212, "no_speech_prob": 0.004307181108742952}, {"id": 1186, "seek": 536208, "start": 5368.4, "end": 5370.4, "text": " artificial intelligence", "tokens": [50680, 11677, 7599, 50780], "temperature": 0.0, "avg_logprob": -0.2936861618705418, "compression_ratio": 1.412121212121212, "no_speech_prob": 0.004307181108742952}, {"id": 1187, "seek": 536208, "start": 5370.4, "end": 5372.4, "text": " Uh, I have a few questions", "tokens": [50780, 4019, 11, 286, 362, 257, 1326, 1651, 50880], "temperature": 0.0, "avg_logprob": -0.2936861618705418, "compression_ratio": 1.412121212121212, "no_speech_prob": 0.004307181108742952}, {"id": 1188, "seek": 536208, "start": 5372.88, "end": 5373.92, "text": " um", "tokens": [50904, 1105, 50956], "temperature": 0.0, "avg_logprob": -0.2936861618705418, "compression_ratio": 1.412121212121212, "no_speech_prob": 0.004307181108742952}, {"id": 1189, "seek": 536208, "start": 5373.92, "end": 5376.64, "text": " From your presentation, I understand that", "tokens": [50956, 3358, 428, 5860, 11, 286, 1223, 300, 51092], "temperature": 0.0, "avg_logprob": -0.2936861618705418, "compression_ratio": 1.412121212121212, "no_speech_prob": 0.004307181108742952}, {"id": 1190, "seek": 536208, "start": 5378.24, "end": 5380.24, "text": " actually llms", "tokens": [51172, 767, 287, 75, 2592, 51272], "temperature": 0.0, "avg_logprob": -0.2936861618705418, "compression_ratio": 1.412121212121212, "no_speech_prob": 0.004307181108742952}, {"id": 1191, "seek": 536208, "start": 5380.24, "end": 5382.24, "text": " the", "tokens": [51272, 264, 51372], "temperature": 0.0, "avg_logprob": -0.2936861618705418, "compression_ratio": 1.412121212121212, "no_speech_prob": 0.004307181108742952}, {"id": 1192, "seek": 536208, "start": 5382.32, "end": 5387.36, "text": " The the it's it's kind of a gold mine of human knowledge", "tokens": [51376, 440, 264, 309, 311, 309, 311, 733, 295, 257, 3821, 3892, 295, 1952, 3601, 51628], "temperature": 0.0, "avg_logprob": -0.2936861618705418, "compression_ratio": 1.412121212121212, "no_speech_prob": 0.004307181108742952}, {"id": 1193, "seek": 538736, "start": 5387.759999999999, "end": 5394.48, "text": " And uh, you seem to say that they have a modelized the real world facts and the logics are quite correctly", "tokens": [50384, 400, 2232, 11, 291, 1643, 281, 584, 300, 436, 362, 257, 2316, 1602, 264, 957, 1002, 9130, 293, 264, 3565, 1167, 366, 1596, 8944, 50720], "temperature": 0.0, "avg_logprob": -0.22267016658076533, "compression_ratio": 1.5980861244019138, "no_speech_prob": 0.004114839248359203}, {"id": 1194, "seek": 538736, "start": 5396.4, "end": 5404.08, "text": " If you ask the question directly in human language, uh, currently they don't seem to to give the good answer", "tokens": [50816, 759, 291, 1029, 264, 1168, 3838, 294, 1952, 2856, 11, 2232, 11, 4362, 436, 500, 380, 1643, 281, 281, 976, 264, 665, 1867, 51200], "temperature": 0.0, "avg_logprob": -0.22267016658076533, "compression_ratio": 1.5980861244019138, "no_speech_prob": 0.004114839248359203}, {"id": 1195, "seek": 538736, "start": 5404.719999999999, "end": 5407.599999999999, "text": " because some with some lack of", "tokens": [51232, 570, 512, 365, 512, 5011, 295, 51376], "temperature": 0.0, "avg_logprob": -0.22267016658076533, "compression_ratio": 1.5980861244019138, "no_speech_prob": 0.004114839248359203}, {"id": 1196, "seek": 538736, "start": 5408.48, "end": 5411.599999999999, "text": " algorithm, but if you first translate the", "tokens": [51420, 9284, 11, 457, 498, 291, 700, 13799, 264, 51576], "temperature": 0.0, "avg_logprob": -0.22267016658076533, "compression_ratio": 1.5980861244019138, "no_speech_prob": 0.004114839248359203}, {"id": 1197, "seek": 538736, "start": 5412.48, "end": 5414.48, "text": " human questions into", "tokens": [51620, 1952, 1651, 666, 51720], "temperature": 0.0, "avg_logprob": -0.22267016658076533, "compression_ratio": 1.5980861244019138, "no_speech_prob": 0.004114839248359203}, {"id": 1198, "seek": 538736, "start": 5415.2, "end": 5417.2, "text": " functional code and like", "tokens": [51756, 11745, 3089, 293, 411, 51856], "temperature": 0.0, "avg_logprob": -0.22267016658076533, "compression_ratio": 1.5980861244019138, "no_speech_prob": 0.004114839248359203}, {"id": 1199, "seek": 541736, "start": 5417.599999999999, "end": 5420.799999999999, "text": " The work you are doing and then they are capable of um", "tokens": [50376, 440, 589, 291, 366, 884, 293, 550, 436, 366, 8189, 295, 1105, 50536], "temperature": 0.0, "avg_logprob": -0.1384663621918494, "compression_ratio": 1.8025751072961373, "no_speech_prob": 0.0007091182051226497}, {"id": 1200, "seek": 541736, "start": 5421.679999999999, "end": 5426.08, "text": " I know we execute the code then we we are capable of having", "tokens": [50580, 286, 458, 321, 14483, 264, 3089, 550, 321, 321, 366, 8189, 295, 1419, 50800], "temperature": 0.0, "avg_logprob": -0.1384663621918494, "compression_ratio": 1.8025751072961373, "no_speech_prob": 0.0007091182051226497}, {"id": 1201, "seek": 541736, "start": 5426.639999999999, "end": 5432.799999999999, "text": " Result results that are quite close to human behavior. Uh, so is that the correct?", "tokens": [50828, 5015, 723, 3542, 300, 366, 1596, 1998, 281, 1952, 5223, 13, 4019, 11, 370, 307, 300, 264, 3006, 30, 51136], "temperature": 0.0, "avg_logprob": -0.1384663621918494, "compression_ratio": 1.8025751072961373, "no_speech_prob": 0.0007091182051226497}, {"id": 1202, "seek": 541736, "start": 5434.4, "end": 5436.5599999999995, "text": " Yeah, that's I mean, that's more or less correct", "tokens": [51216, 865, 11, 300, 311, 286, 914, 11, 300, 311, 544, 420, 1570, 3006, 51324], "temperature": 0.0, "avg_logprob": -0.1384663621918494, "compression_ratio": 1.8025751072961373, "no_speech_prob": 0.0007091182051226497}, {"id": 1203, "seek": 541736, "start": 5436.639999999999, "end": 5440.5599999999995, "text": " But I don't want to I mean and that's that's what we've been doing in these examples", "tokens": [51328, 583, 286, 500, 380, 528, 281, 286, 914, 293, 300, 311, 300, 311, 437, 321, 600, 668, 884, 294, 613, 5110, 51524], "temperature": 0.0, "avg_logprob": -0.1384663621918494, "compression_ratio": 1.8025751072961373, "no_speech_prob": 0.0007091182051226497}, {"id": 1204, "seek": 541736, "start": 5440.5599999999995, "end": 5442.5599999999995, "text": " And what I've been showing but I don't want to", "tokens": [51524, 400, 437, 286, 600, 668, 4099, 457, 286, 500, 380, 528, 281, 51624], "temperature": 0.0, "avg_logprob": -0.1384663621918494, "compression_ratio": 1.8025751072961373, "no_speech_prob": 0.0007091182051226497}, {"id": 1205, "seek": 541736, "start": 5443.04, "end": 5444.5599999999995, "text": " Claim I don't and I don't think it's true", "tokens": [51648, 383, 10970, 286, 500, 380, 293, 286, 500, 380, 519, 309, 311, 2074, 51724], "temperature": 0.0, "avg_logprob": -0.1384663621918494, "compression_ratio": 1.8025751072961373, "no_speech_prob": 0.0007091182051226497}, {"id": 1206, "seek": 544456, "start": 5444.64, "end": 5448.8, "text": " But certainly I don't have the evidence for it being true or how true it is that", "tokens": [50368, 583, 3297, 286, 500, 380, 362, 264, 4467, 337, 309, 885, 2074, 420, 577, 2074, 309, 307, 300, 50576], "temperature": 0.0, "avg_logprob": -0.10537018775939941, "compression_ratio": 1.6447876447876448, "no_speech_prob": 0.008978732861578465}, {"id": 1207, "seek": 544456, "start": 5450.400000000001, "end": 5453.120000000001, "text": " You know llms like always do this, right? There's a lot of", "tokens": [50656, 509, 458, 287, 75, 2592, 411, 1009, 360, 341, 11, 558, 30, 821, 311, 257, 688, 295, 50792], "temperature": 0.0, "avg_logprob": -0.10537018775939941, "compression_ratio": 1.6447876447876448, "no_speech_prob": 0.008978732861578465}, {"id": 1208, "seek": 544456, "start": 5454.320000000001, "end": 5458.64, "text": " Ways in which language is only an imperfect reflection of the ways that we think", "tokens": [50852, 343, 3772, 294, 597, 2856, 307, 787, 364, 26714, 12914, 295, 264, 2098, 300, 321, 519, 51068], "temperature": 0.0, "avg_logprob": -0.10537018775939941, "compression_ratio": 1.6447876447876448, "no_speech_prob": 0.008978732861578465}, {"id": 1209, "seek": 544456, "start": 5458.88, "end": 5465.200000000001, "text": " There's certain things that again are partly based on evolutionarily ancient core systems. The language is not very good at expressing", "tokens": [51080, 821, 311, 1629, 721, 300, 797, 366, 17031, 2361, 322, 9303, 3289, 7832, 4965, 3652, 13, 440, 2856, 307, 406, 588, 665, 412, 22171, 51396], "temperature": 0.0, "avg_logprob": -0.10537018775939941, "compression_ratio": 1.6447876447876448, "no_speech_prob": 0.008978732861578465}, {"id": 1210, "seek": 544456, "start": 5466.240000000001, "end": 5470.0, "text": " And that includes also spatial reasoning and you can see ways in which", "tokens": [51448, 400, 300, 5974, 611, 23598, 21577, 293, 291, 393, 536, 2098, 294, 597, 51636], "temperature": 0.0, "avg_logprob": -0.10537018775939941, "compression_ratio": 1.6447876447876448, "no_speech_prob": 0.008978732861578465}, {"id": 1211, "seek": 547000, "start": 5470.64, "end": 5474.72, "text": " Even the best language models break down here and language multimodal language vision models", "tokens": [50396, 2754, 264, 1151, 2856, 5245, 1821, 760, 510, 293, 2856, 32972, 378, 304, 2856, 5201, 5245, 50600], "temperature": 0.0, "avg_logprob": -0.10139976564000865, "compression_ratio": 1.7852564102564104, "no_speech_prob": 0.013214938342571259}, {"id": 1212, "seek": 547000, "start": 5475.68, "end": 5478.56, "text": " Have have had persistent problems always from the beginning there", "tokens": [50648, 3560, 362, 632, 24315, 2740, 1009, 490, 264, 2863, 456, 50792], "temperature": 0.0, "avg_logprob": -0.10139976564000865, "compression_ratio": 1.7852564102564104, "no_speech_prob": 0.013214938342571259}, {"id": 1213, "seek": 547000, "start": 5479.2, "end": 5483.52, "text": " So I think you know, I don't want to suggest like I think yes language is a treasure trove", "tokens": [50824, 407, 286, 519, 291, 458, 11, 286, 500, 380, 528, 281, 3402, 411, 286, 519, 2086, 2856, 307, 257, 12985, 4495, 303, 51040], "temperature": 0.0, "avg_logprob": -0.10139976564000865, "compression_ratio": 1.7852564102564104, "no_speech_prob": 0.013214938342571259}, {"id": 1214, "seek": 547000, "start": 5483.92, "end": 5490.4, "text": " Of knowledge explicit and implicit about the world, but it's and for us it's it's such a valuable resource as human beings", "tokens": [51060, 2720, 3601, 13691, 293, 26947, 466, 264, 1002, 11, 457, 309, 311, 293, 337, 505, 309, 311, 309, 311, 1270, 257, 8263, 7684, 382, 1952, 8958, 51384], "temperature": 0.0, "avg_logprob": -0.10139976564000865, "compression_ratio": 1.7852564102564104, "no_speech_prob": 0.013214938342571259}, {"id": 1215, "seek": 547000, "start": 5490.48, "end": 5498.4, "text": " Okay, that's why it's no accident that language models when they're trained to capture patterns on all the language that humanity's basically ever produced and put out there on the web", "tokens": [51388, 1033, 11, 300, 311, 983, 309, 311, 572, 6398, 300, 2856, 5245, 562, 436, 434, 8895, 281, 7983, 8294, 322, 439, 264, 2856, 300, 10243, 311, 1936, 1562, 7126, 293, 829, 484, 456, 322, 264, 3670, 51784], "temperature": 0.0, "avg_logprob": -0.10139976564000865, "compression_ratio": 1.7852564102564104, "no_speech_prob": 0.013214938342571259}, {"id": 1216, "seek": 549840, "start": 5499.12, "end": 5502.96, "text": " Uh, you know start to have remarkable properties at the same time", "tokens": [50400, 4019, 11, 291, 458, 722, 281, 362, 12802, 7221, 412, 264, 912, 565, 50592], "temperature": 0.0, "avg_logprob": -0.14269299463394586, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.004607816692441702}, {"id": 1217, "seek": 549840, "start": 5503.599999999999, "end": 5509.44, "text": " It's only some parts of our knowledge about the world is key parts to actually understanding and being in the world", "tokens": [50624, 467, 311, 787, 512, 3166, 295, 527, 3601, 466, 264, 1002, 307, 2141, 3166, 281, 767, 3701, 293, 885, 294, 264, 1002, 50916], "temperature": 0.0, "avg_logprob": -0.14269299463394586, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.004607816692441702}, {"id": 1218, "seek": 549840, "start": 5510.0, "end": 5512.24, "text": " That basically nobody ever talks about", "tokens": [50944, 663, 1936, 5079, 1562, 6686, 466, 51056], "temperature": 0.0, "avg_logprob": -0.14269299463394586, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.004607816692441702}, {"id": 1219, "seek": 549840, "start": 5514.48, "end": 5521.36, "text": " And even if they do talk about it they talk about it very incompletely and imperfectly and yet our brains are designed to understand in those terms", "tokens": [51168, 400, 754, 498, 436, 360, 751, 466, 309, 436, 751, 466, 309, 588, 14036, 14657, 736, 293, 26714, 356, 293, 1939, 527, 15442, 366, 4761, 281, 1223, 294, 729, 2115, 51512], "temperature": 0.0, "avg_logprob": -0.14269299463394586, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.004607816692441702}, {"id": 1220, "seek": 549840, "start": 5521.759999999999, "end": 5528.32, "text": " So I I don't want to convey the idea that somehow it's all there in language and it's and that's all you need", "tokens": [51532, 407, 286, 286, 500, 380, 528, 281, 16965, 264, 1558, 300, 6063, 309, 311, 439, 456, 294, 2856, 293, 309, 311, 293, 300, 311, 439, 291, 643, 51860], "temperature": 0.0, "avg_logprob": -0.14269299463394586, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.004607816692441702}, {"id": 1221, "seek": 552840, "start": 5528.4, "end": 5530.4, "text": " by not by any means", "tokens": [50364, 538, 406, 538, 604, 1355, 50464], "temperature": 0.0, "avg_logprob": -0.16338674227396646, "compression_ratio": 1.5387931034482758, "no_speech_prob": 0.001596437068656087}, {"id": 1222, "seek": 552840, "start": 5531.28, "end": 5534.879999999999, "text": " Wonderful before we, uh, let people applaud", "tokens": [50508, 22768, 949, 321, 11, 2232, 11, 718, 561, 9644, 50688], "temperature": 0.0, "avg_logprob": -0.16338674227396646, "compression_ratio": 1.5387931034482758, "no_speech_prob": 0.001596437068656087}, {"id": 1223, "seek": 552840, "start": 5535.679999999999, "end": 5541.839999999999, "text": " You know, there's some several panels coming. Are there any of them that you can join? They're not today, unfortunately", "tokens": [50728, 509, 458, 11, 456, 311, 512, 2940, 13419, 1348, 13, 2014, 456, 604, 295, 552, 300, 291, 393, 3917, 30, 814, 434, 406, 965, 11, 7015, 51036], "temperature": 0.0, "avg_logprob": -0.16338674227396646, "compression_ratio": 1.5387931034482758, "no_speech_prob": 0.001596437068656087}, {"id": 1224, "seek": 552840, "start": 5541.92, "end": 5548.639999999999, "text": " Yeah, um, I I need to check my calendar and coordinate on some family things and I will I'll I'll try very hard to join one of them", "tokens": [51040, 865, 11, 1105, 11, 286, 286, 643, 281, 1520, 452, 12183, 293, 15670, 322, 512, 1605, 721, 293, 286, 486, 286, 603, 286, 603, 853, 588, 1152, 281, 3917, 472, 295, 552, 51376], "temperature": 0.0, "avg_logprob": -0.16338674227396646, "compression_ratio": 1.5387931034482758, "no_speech_prob": 0.001596437068656087}, {"id": 1225, "seek": 552840, "start": 5548.799999999999, "end": 5550.799999999999, "text": " Okay, now we'll applaud you for", "tokens": [51384, 1033, 11, 586, 321, 603, 9644, 291, 337, 51484], "temperature": 0.0, "avg_logprob": -0.16338674227396646, "compression_ratio": 1.5387931034482758, "no_speech_prob": 0.001596437068656087}, {"id": 1226, "seek": 552840, "start": 5551.759999999999, "end": 5553.759999999999, "text": " Thank you", "tokens": [51532, 1044, 291, 51632], "temperature": 0.0, "avg_logprob": -0.16338674227396646, "compression_ratio": 1.5387931034482758, "no_speech_prob": 0.001596437068656087}, {"id": 1227, "seek": 555376, "start": 5553.92, "end": 5564.08, "text": " Thanks, and and thanks so much for the discussion Virginia and all the all the questions. There were great questions. So I hope to engage more", "tokens": [50372, 2561, 11, 293, 293, 3231, 370, 709, 337, 264, 5017, 10956, 293, 439, 264, 439, 264, 1651, 13, 821, 645, 869, 1651, 13, 407, 286, 1454, 281, 4683, 544, 50880], "temperature": 0.0, "avg_logprob": -0.2739813232421875, "compression_ratio": 1.4962962962962962, "no_speech_prob": 0.0035324678756296635}, {"id": 1228, "seek": 555376, "start": 5564.320000000001, "end": 5565.360000000001, "text": " um", "tokens": [50892, 1105, 50944], "temperature": 0.0, "avg_logprob": -0.2739813232421875, "compression_ratio": 1.4962962962962962, "no_speech_prob": 0.0035324678756296635}, {"id": 1229, "seek": 555376, "start": 5565.360000000001, "end": 5568.96, "text": " in one of the panels and um in person if if anyone hears", "tokens": [50944, 294, 472, 295, 264, 13419, 293, 1105, 294, 954, 498, 498, 2878, 25688, 51124], "temperature": 0.0, "avg_logprob": -0.2739813232421875, "compression_ratio": 1.4962962962962962, "no_speech_prob": 0.0035324678756296635}], "language": "en"}