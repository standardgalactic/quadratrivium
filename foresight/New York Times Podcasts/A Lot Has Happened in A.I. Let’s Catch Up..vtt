WEBVTT

00:00.000 --> 00:23.200
From New York Times Opinion, this is the Ezra Klein Show.

00:23.200 --> 00:28.080
Before we get into the episode today, we are getting ready to do our end of the year Ask

00:28.080 --> 00:29.080
Me Anything.

00:29.080 --> 00:33.080
If you have questions you want to hear me answer on the show, I suspect a lot of them

00:33.080 --> 00:36.720
are going to be about Israel Palestine and AI, but they don't have to be about Israel

00:36.720 --> 00:38.600
Palestine and AI.

00:38.600 --> 00:43.000
Send them to Ezra Klein Show at nytimes.com with AMA in the headline.

00:43.000 --> 00:54.120
Again, to Ezra Klein Show at nytimes.com with AMA in the headline.

00:54.120 --> 00:59.960
If you follow business or tech or artificial intelligence news at all, in recent weeks

00:59.960 --> 01:06.360
you certainly were following Sam Altman being unexpectedly fired as CEO of OpenAI and then

01:06.360 --> 01:11.680
a huge staff revolt at OpenAI where more than 95% of the company said it would resign if

01:11.680 --> 01:16.160
he was not reinstated and then he was reinstated.

01:16.160 --> 01:20.080
And so this whole thing seemed to have happened for nothing.

01:20.080 --> 01:25.120
I spent a lot of time reporting on this and I talked to people on the Altman side of things,

01:25.120 --> 01:28.960
I talked to people on the board side of things, and the thing I am now convinced of, truly

01:28.960 --> 01:33.440
convinced of is that there was less to it than met the eye.

01:33.440 --> 01:40.080
People saw, I saw, Altman fired by this nonprofit board meant to ensure that AI is built to

01:40.080 --> 01:42.080
serve humanity.

01:42.080 --> 01:48.800
And I assumed, and I think many assumed, there was some disagreement here over what OpenAI

01:48.800 --> 01:55.440
was doing, over how much safety was building into the systems, over the pace of commercialization,

01:55.440 --> 02:00.880
over the contracts it was signing, over what it was going to be building next year, over

02:00.880 --> 02:01.880
something.

02:01.880 --> 02:06.120
And that I think I can say conclusively and has been corroborated by other reporting,

02:06.120 --> 02:08.200
that was not what this was about.

02:08.200 --> 02:12.560
The OpenAI board did not trust and did not feel it could control Sam Altman, and that

02:12.560 --> 02:14.360
is why they fired Altman.

02:14.360 --> 02:17.400
It's not that they felt they couldn't trust him on one thing, that they were trying to

02:17.400 --> 02:21.200
control him on X, but he was beating them on X.

02:21.200 --> 02:23.840
It's that a lot of little things added up.

02:23.840 --> 02:28.040
They felt their job was to control the company, that they did not feel they could control

02:28.040 --> 02:31.560
him, and so to do their job, they had to get rid of him.

02:31.560 --> 02:34.720
They did not have, obviously, the support inside the company to do that.

02:34.720 --> 02:37.560
They were not ultimately willing to let OpenAI completely collapse.

02:37.560 --> 02:41.240
And so they largely, although I think in their view, not totally back down.

02:41.240 --> 02:43.240
One of the members is still on the board.

02:43.240 --> 02:47.880
Altman and the president of OpenAI, Greg Brockman, are off the board.

02:47.880 --> 02:51.720
Some new board members are coming in who they think are going to be stronger and more willing

02:51.720 --> 02:52.720
to stand up to them.

02:52.720 --> 02:56.440
There's an investigation that is going to be done of Altman's behavior that will be

02:56.440 --> 03:01.280
at least released to the board, so they'll, I guess, know what to think of him.

03:01.280 --> 03:02.280
It's a very strange story.

03:02.280 --> 03:06.680
I wouldn't be surprised if there's things yet to come out, but I am pretty convinced

03:06.680 --> 03:11.600
that this was truly a struggle for control, not a struggle about X.

03:11.600 --> 03:16.240
But it has been a year since ChatGPT was released, so weird way to mark the year, but it has

03:16.240 --> 03:17.240
been a year.

03:17.240 --> 03:22.880
A year since OpenAI kicked off the whole modern era in artificial intelligence.

03:22.880 --> 03:27.520
A year since a lot of people's estimations of what humanity's future looked like began

03:27.520 --> 03:31.320
to shift and cloud and darken and shimmer.

03:31.320 --> 03:35.240
And so I wanted to have the conversation that many of us thought was a conversation happening

03:35.240 --> 03:40.120
here about what AI was becoming, how it was being used, how it was being commercialized,

03:40.440 --> 03:44.160
the path we're on is going to benefit humanity.

03:44.160 --> 03:48.080
And so I asked my friends over at Hard Fork, another great New York Times podcast, to come

03:48.080 --> 03:49.080
on the show.

03:49.080 --> 03:51.000
Kevin Roos is my colleague at The Times.

03:51.000 --> 03:53.240
He writes a tech column called The Shift.

03:53.240 --> 03:58.120
Casey Newton is the editor of Platformer, an absolutely must read newsletter about the

03:58.120 --> 04:01.800
intersection of technology and democracy.

04:01.800 --> 04:05.440
And they have been following this in and out, but they've been closely following AI for

04:05.440 --> 04:06.440
the past year.

04:06.440 --> 04:08.840
So I wanted to have this broader conversation with them.

04:08.840 --> 04:11.760
As always, my email is reclinedshowatnytimes.com.

04:18.240 --> 04:19.960
Kevin Roos, Casey Newton.

04:19.960 --> 04:20.960
Welcome to the show, my friends.

04:20.960 --> 04:21.960
Hey, Ezra.

04:21.960 --> 04:23.960
Thanks for having us.

04:23.960 --> 04:24.960
All right.

04:24.960 --> 04:28.320
So we're talking on Monday, November 27th.

04:28.320 --> 04:33.840
JetGPT, which kicked off this era in AI, was released on November 30th, 2022.

04:33.840 --> 04:38.600
So the big anniversary party was at Sam Altman got temporarily fired and the company almost

04:38.600 --> 04:42.920
collapsed and was rebuilt over at Microsoft, which I don't think is how people expected

04:42.920 --> 04:44.160
to mark the anniversary.

04:44.160 --> 04:51.720
But it has been now a year, roughly, in this sort of whole new AI world that we're in.

04:51.720 --> 04:55.080
And so I want to talk about what's changed in that year.

04:55.080 --> 04:59.800
And the place I want to begin is with the capabilities of the AI systems we're seeing,

04:59.800 --> 05:04.200
not the ones we're hearing about, but that we know are actually being used by someone

05:04.200 --> 05:07.200
in semi-real world conditions.

05:07.200 --> 05:11.800
What can AI systems do today that they couldn't do a year ago, Kevin?

05:11.800 --> 05:18.160
Well, the first most obvious capabilities improvement is that these models have become

05:18.160 --> 05:19.800
what's called multimodal.

05:19.800 --> 05:26.680
So a year ago, we had ChatGPT, which could take in text input and output other texts

05:26.680 --> 05:28.800
as the response to your prompt.

05:28.800 --> 05:35.520
But now we have models that can take in text and output images, take in text and output

05:35.520 --> 05:40.760
video, take in voice data, and output other voice data.

05:40.760 --> 05:45.960
So these models are now working with many more types of inputs and outputs than they

05:45.960 --> 05:47.760
were a year ago.

05:47.760 --> 05:53.360
And that's sort of the most obvious difference if you just woke up from a year-long nap and

05:53.360 --> 05:56.820
took a look at the AI capabilities on the market, that's the thing that you would probably

05:56.820 --> 05:57.820
notice first.

05:57.820 --> 05:59.320
I want to pull something out about that.

05:59.320 --> 06:04.480
Is that it almost sounds like they're developing what you might call senses.

06:04.480 --> 06:07.920
And I recognize that there's a real danger of anthropomorphizing AI systems.

06:07.920 --> 06:09.480
I'm not trying to do that.

06:09.480 --> 06:13.440
But one thing about having different senses is that we get some information that helps

06:13.440 --> 06:16.360
us learn about the world from our eyes, other information that helps us learn about the

06:16.360 --> 06:18.600
world from our ears, et cetera.

06:18.600 --> 06:22.420
One of the constraints on the models is how much training data they can have.

06:22.420 --> 06:27.400
As they become multimodal, it would seem that would radically expand the amount of training

06:27.400 --> 06:28.400
data.

06:28.400 --> 06:35.600
Not just all of the text on the internet, but all of the audio on YouTube, or all podcast

06:35.600 --> 06:38.640
audio on Spotify or something, or Apple Podcasts.

06:38.640 --> 06:43.120
That's a lot of data to learn about the world from, that in theory will make the models

06:43.120 --> 06:44.440
smarter and more and more capable.

06:44.440 --> 06:46.920
Does it have that kind of recursive quality?

06:46.920 --> 06:47.920
Absolutely.

06:47.920 --> 06:52.800
I mean, part of the backdrop for these capabilities improvements is this race for high-quality

06:52.800 --> 06:53.800
data.

06:53.800 --> 07:00.120
AI labs are obsessed with finding new undiscovered, high-quality data sources that they can use

07:00.120 --> 07:01.200
to train their models.

07:01.200 --> 07:05.640
And so if you run out of text because you've scraped the entire internet, then you've got

07:05.640 --> 07:11.160
to go to podcasts or YouTube videos or some other source of data to keep improving your

07:11.160 --> 07:12.160
models.

07:12.160 --> 07:15.880
For what it's worth, though, I don't think the availability of more training data is

07:15.880 --> 07:17.600
what is interesting about the past year.

07:17.600 --> 07:21.560
I think what was interesting about ChatGBT was that it gave average people a way to interact

07:21.560 --> 07:23.080
with AI for the first time.

07:23.080 --> 07:27.800
It was just a box that you could type in and ask it anything and often get something pretty

07:27.800 --> 07:29.520
good in response.

07:29.520 --> 07:33.640
And even a year into folks using this now, I don't think we fully discovered everything

07:33.640 --> 07:35.360
that it can be used for.

07:35.360 --> 07:39.000
And I think more people are experiencing vertigo every day as they think about what this could

07:39.000 --> 07:40.800
mean for their own jobs and careers.

07:40.800 --> 07:44.200
So to me, the important thing was actually just the box that you type in and get questions

07:44.200 --> 07:45.200
from.

07:45.200 --> 07:46.200
Yeah, I agree with that.

07:46.200 --> 07:51.520
I think if you had just paused there and there was no new development in AI, I think it would

07:51.520 --> 07:58.280
still probably take the next five or 10 years for society to adjust to the new capabilities

07:58.280 --> 07:59.960
in our midst.

07:59.960 --> 08:03.960
So you've made this point in other places, Casey, that a lot of the advances to come

08:03.960 --> 08:09.320
are going to be in user interfaces and in how we interact with these systems.

08:09.320 --> 08:11.520
In a way, that was a big advance of ChatGBT.

08:11.520 --> 08:15.840
The system behind it had been around for a while, but the ability to speak to it, I guess,

08:15.840 --> 08:21.300
write to it in natural language, it created this huge cultural moment around AI.

08:21.300 --> 08:25.180
But what can these AI products actually do that they couldn't do a year ago?

08:25.180 --> 08:30.260
Not just how we interface with them, but their underlying capacity or power.

08:30.260 --> 08:36.260
I mean, as of the developer update that OpenAI had a few weeks back, the world knowledge

08:36.260 --> 08:40.040
of the system has been updated to April of this year.

08:40.040 --> 08:46.060
And so you're able to get something closer to real-time knowledge of world events.

08:46.060 --> 08:52.300
It has now integrated with Microsoft Bing, and so you can get truly real-time information

08:52.300 --> 08:56.300
in a way that was impossible when ChatGBT launched.

08:56.300 --> 08:59.940
And these might sound like relatively minor things, Azura, but you start chaining them

08:59.940 --> 09:05.540
together, you start building the right interfaces, and you actually start to see beyond the internet

09:05.540 --> 09:06.540
as we know it today.

09:06.540 --> 09:11.340
You see a world-worthy web where Google is not our starting point for doing everything

09:11.340 --> 09:12.340
online.

09:12.340 --> 09:15.700
It is just a little box on your computer that you type in and you get the answer without

09:15.700 --> 09:16.900
ever visiting a web page.

09:16.900 --> 09:21.060
So that's all going to take many years to unfold, but the beginnings of it are easy

09:21.060 --> 09:22.060
to see now.

09:22.060 --> 09:27.180
One other capability that didn't exist a year ago, at least in any public products, is the

09:27.180 --> 09:29.980
ability to bring your own data into these models.

09:29.980 --> 09:34.420
So Claude was the first language model that I used that had the ability to, say, upload

09:34.420 --> 09:35.420
a PDF.

09:35.420 --> 09:41.000
So you could say, here's a research paper, it's 100 pages long, help me summarize and

09:41.000 --> 09:42.000
analyze this.

09:42.000 --> 09:43.000
And it could do that.

09:43.020 --> 09:47.920
Now ChatGPT can do the same thing, and I know a bunch of other systems are moving in that

09:47.920 --> 09:48.920
direction too.

09:48.920 --> 09:53.080
There are also companies that have tried to spin up their own language models that are

09:53.080 --> 09:55.120
trained on their own internal data.

09:55.120 --> 10:02.480
So if you are Coca-Cola or BCG or some other business and you want an internal ChatGPT that

10:02.480 --> 10:07.080
you can use for your own employees to ask, say, questions about your HR documents, that

10:07.080 --> 10:08.960
is a thing that companies have been building.

10:08.960 --> 10:12.920
So that's not the sexiest, most consumer-facing application, but that is something that there's

10:12.920 --> 10:14.960
enormous demand for out there.

10:14.960 --> 10:18.840
So one thing it seems to me to be getting better at from what I can tell from others

10:18.840 --> 10:19.840
is coding.

10:19.840 --> 10:25.520
I have to ask people whether they're using AI bots very often, and if so, for what.

10:25.520 --> 10:29.280
And basically nobody says yes unless they are coder.

10:29.280 --> 10:32.520
Everybody says, oh yeah, I played around with it, I thought it was really cool, I sometimes

10:32.520 --> 10:38.600
use Dolly or Mid Journey to make pictures for my kids or for my email newsletter.

10:38.600 --> 10:42.280
But it is the coders who say, I'm using it all the time, it has become completely essential

10:42.280 --> 10:43.280
to me.

10:43.280 --> 10:47.200
I'm curious to hear a bit about that capability increase.

10:47.200 --> 10:52.160
I think where it has sort of become part of the daily habit of programmers is through

10:52.160 --> 10:58.760
tools like GitHub Copilot, which is a basically ChatGPT for coders that finishes whatever line

10:58.760 --> 11:03.360
of code you're working on or helps you debug some code that's broken.

11:03.360 --> 11:05.640
And there have been some studies and tests.

11:05.640 --> 11:11.040
I think there was one test that GitHub itself ran where they gave two groups of coders the

11:11.040 --> 11:12.040
same task.

11:12.040 --> 11:16.560
And one group was allowed to use GitHub Copilot and one group wasn't.

11:16.560 --> 11:21.600
And the group with GitHub Copilot finished the task 55% faster than the group without

11:21.600 --> 11:22.600
it.

11:22.600 --> 11:25.080
Now that is like a radical productivity increase.

11:25.080 --> 11:29.960
And if you tell a programmer, here's a tool that can make you 55% faster, they're going

11:29.960 --> 11:32.400
to want to use that every day.

11:32.400 --> 11:39.040
So when I see function chat bots in the wild, what I see is different versions of what people

11:39.040 --> 11:44.120
used to somewhat derisively call like the fancy autocomplete, right?

11:44.120 --> 11:47.360
Help you finish a line of code, help you finish this email.

11:47.360 --> 11:51.640
You ask a question that you might ask a search engine, like why do I have spots all over

11:51.640 --> 11:52.640
my elbow?

11:52.640 --> 11:55.520
And it gives you an answer that hopefully is right, but maybe is not right.

11:55.520 --> 11:59.240
I do think some of the search implications are interesting, but at the same time, it

11:59.240 --> 12:02.360
is not the case that Bing has made great strides on Google.

12:02.360 --> 12:05.480
People have not moved to asking the kind of Bing chat bot.

12:05.480 --> 12:10.080
Next question is as opposed to asking Google, everybody feels like they need AI in their

12:10.080 --> 12:11.080
thing now, right?

12:11.080 --> 12:14.400
There's a, I don't think you can raise money in Silicon Valley at the moment if you don't

12:14.400 --> 12:19.360
have a generative AI play built into your product or built into your business strategy.

12:19.360 --> 12:21.680
But that was true for a minute for crypto too.

12:21.680 --> 12:25.200
And I'm not one of the people who makes a crypto AI analogy.

12:25.200 --> 12:29.600
I think crypto is largely vaporware and AI is largely real.

12:29.600 --> 12:32.680
But Silicon Valley is faddish and people don't know how to use things.

12:32.680 --> 12:36.200
And so everybody tries to put things in all at once.

12:36.200 --> 12:38.040
What product has actually gotten way better?

12:38.040 --> 12:40.120
I'll just use one example.

12:40.120 --> 12:43.480
There's an app you might be familiar with called Notion.

12:43.480 --> 12:46.440
It's productivity sort of collaborative software.

12:46.440 --> 12:47.440
I write a newsletter.

12:47.440 --> 12:50.640
I save every link that I put in my newsletter into Notion.

12:50.640 --> 12:54.320
And now that there is AI inside Notion, Notion can do a couple of things.

12:54.320 --> 12:57.480
One, it can just look at every link I save and just write a two sentence summary for

12:57.480 --> 13:00.720
me, which is just sort of nice to see at a glance what that story is about.

13:00.760 --> 13:04.760
And most recently, it added a feature where you can just do Q&A with a database and say

13:04.760 --> 13:10.760
like, hey, what are some of the big stories about Meta over the past few weeks?

13:10.760 --> 13:15.120
And it'll just start pulling those up, essentially querying the database that I have built.

13:15.120 --> 13:20.280
And so while we're very early in this, you're beginning to see a world where AI is taking

13:20.280 --> 13:25.360
data that you have stored somewhere and it's turning it into your personal research assistant.

13:25.360 --> 13:26.800
So is it great right now?

13:26.800 --> 13:28.960
No, I would give it like a C.

13:29.200 --> 13:31.200
For one point now, I think it's not bad.

13:31.200 --> 13:35.320
And I'll share another example that is not from my own use, but I was talking a few

13:35.320 --> 13:39.920
weeks ago with a doctor, who's a friend of a friend, and doctors, you get tons of messages

13:39.920 --> 13:40.920
from patients.

13:40.920 --> 13:42.280
You know, what's this rash?

13:42.280 --> 13:44.120
Can you renew this prescription?

13:44.120 --> 13:45.760
Do I need to come in for a blood test?

13:45.760 --> 13:46.760
Like that kind of stuff.

13:46.760 --> 13:52.200
And doctors and nurses spend a ton of time just opening up their message portal, replying

13:52.200 --> 13:53.200
to all these messages.

13:53.200 --> 13:56.240
It's a huge part of being a doctor and it's a part that they don't like.

13:56.240 --> 14:00.720
And so this doctor was telling me that they have this software now that essentially uses

14:00.720 --> 14:01.720
a language model.

14:01.720 --> 14:06.720
I assume it's open AIs or someone very similar to that, that goes in and pre fills the responses

14:06.720 --> 14:08.760
to patient queries.

14:08.760 --> 14:13.080
And the doctor still has to look it over, make sure everything's right and press send.

14:13.080 --> 14:18.200
But just that act of pre-populating the field, this person was saying that saves them a ton

14:18.200 --> 14:21.640
of time, like on the order of several hours a day.

14:21.640 --> 14:28.400
But if you have that and you sort of extrapolate to what if every doctor in America was saving

14:28.400 --> 14:31.800
themselves an hour or two a day of responding to patient messages?

14:31.800 --> 14:34.760
I mean, that's a radical productivity enhancement.

14:34.760 --> 14:38.120
And so you can say that that's just fancy autocomplete and I guess on some level it

14:38.120 --> 14:43.240
is, but just having fancy autocomplete in these paperwork heavy professions could be

14:43.240 --> 14:44.240
very important.

14:44.240 --> 14:51.080
Well, let me push out in two directions because one direction is that I am not super thrilled

14:51.320 --> 14:56.800
about the idea that my doctor theoretically here is glancing over things and clicking

14:56.800 --> 15:02.280
submit as opposed to reading my message themselves and having to do the act of writing, which

15:02.280 --> 15:05.440
helps you think about things and thinking about what I actually emailed them and like

15:05.440 --> 15:07.200
what kind of answer they need to give me.

15:07.200 --> 15:12.680
I mean, I know personally the difference in thought between scanning things and editing

15:12.680 --> 15:14.640
and thinking through things.

15:14.640 --> 15:19.680
So that's like my diminishing response, but the flip of it is the thing I'm not hearing

15:19.720 --> 15:24.480
anybody say here and the thing I keep waiting for and being interested in is the things

15:24.480 --> 15:26.560
that I might be able to do better than my doctor.

15:26.560 --> 15:30.720
I was reading Jack Clark's import AI newsletter today, which I super recommend to people who

15:30.720 --> 15:35.600
want to follow advancements in the field and he was talking about a, I mean, it was a system

15:35.600 --> 15:39.160
being tested, not a system that is in deployment, but it was better at picking up pancreatic

15:39.160 --> 15:43.480
cancer from certain kinds of information than doctors are.

15:43.480 --> 15:48.200
And I keep waiting to hear something like this going out into the field, right?

15:48.200 --> 15:51.120
Something that doesn't just save people a bit of time around the edges.

15:51.120 --> 15:52.800
I agree that's a productivity improvement.

15:52.800 --> 15:55.280
It's fine. You can build a business around that.

15:55.280 --> 15:59.440
But the promise of AI when Sam Altman sat with you all a few weeks ago or however long

15:59.440 --> 16:02.520
it was and said, we're moving to the best world ever.

16:02.520 --> 16:05.200
He didn't mean that our paperwork is going to get a little bit easier to complete.

16:05.200 --> 16:07.600
Like he meant we'd have cures for new diseases.

16:07.600 --> 16:11.160
He meant that we would have new kinds of energy possibilities.

16:11.160 --> 16:16.200
I'm interested in the programs and the models that can create things that don't exist.

16:17.200 --> 16:20.160
Well, to get there, you need systems that can reason.

16:20.160 --> 16:24.000
And right now the systems that we have just aren't very good at reasoning.

16:24.000 --> 16:28.760
I think that over the past year, we have seen them move a little away from the way that

16:28.760 --> 16:32.480
I was thinking of them a year ago, which was a sort of fancy autocomplete, right?

16:32.480 --> 16:35.360
It's sort of making it, making a prediction about what the next word will be.

16:35.360 --> 16:40.120
This is true that they do it that way, but it is able to create a kind of facsimile of

16:40.120 --> 16:42.040
thought that can be interesting in some ways.

16:42.040 --> 16:45.840
But you just can't get to where you're going, Ezra, with like a facsimile of thought.

16:45.840 --> 16:48.880
You need something that has improved reasoning capabilities.

16:48.880 --> 16:51.880
So maybe that comes with the next generation frontier models.

16:51.880 --> 16:54.360
But until then, I think you'll be disappointed.

16:54.360 --> 16:56.000
But do you need a different kind of model?

16:56.000 --> 16:58.200
This is something that lingers in the back of my head.

16:58.200 --> 17:02.560
So I did an interview on the show with Demis Isabis, who's the co-founder of DeepMind.

17:02.560 --> 17:06.040
Now we're going to see integrated DeepMind Google AI program.

17:06.040 --> 17:11.000
And DeepMind had built this system while back called AlphaFold, which treated how

17:11.000 --> 17:14.760
proteins are constructed in 3D space, which is to say, in reality,

17:14.760 --> 17:17.720
we live in 3D space, it treated it as a game.

17:17.720 --> 17:21.640
And it fed itself a bunch of information and it became very good at predicting the

17:21.640 --> 17:25.320
structure of proteins and that solved this really big scientific problem.

17:25.320 --> 17:29.240
And they then created a subsidiary of Alphabet called Isomorphic Labs to try

17:29.240 --> 17:33.280
to build drug discovery on similar foundations.

17:33.280 --> 17:37.680
But my understanding is that Google during this period became terrified of Microsoft

17:37.680 --> 17:40.400
and open AI beating it up in search and office.

17:40.440 --> 17:44.280
And so they pulled a lot of resources, not least Hasabis himself,

17:44.280 --> 17:47.680
into this integrated structure to try to win the chatbot wars,

17:47.680 --> 17:50.680
which is now what their system barred us trying to do.

17:50.680 --> 17:53.840
And so when you said, Casey, that we need things that can reason, I mean, maybe,

17:53.840 --> 17:57.560
but also you could say we need things that are tailored to solve problems we care

17:57.560 --> 17:58.760
about more.

17:58.760 --> 18:00.880
And I think this is one of the things that worries me a bit,

18:00.880 --> 18:08.680
that we've backed ourselves into business models that are not that important for humanity.

18:08.760 --> 18:10.320
Is there some chance of that?

18:10.320 --> 18:16.160
I mean, are we going too hard after language-based general intelligent AI

18:16.160 --> 18:20.720
that, by the way, integrates very nicely into a suite of enterprise software

18:20.720 --> 18:24.080
as opposed to building things that actually create scientific breakthroughs,

18:24.080 --> 18:28.880
but don't have the same kind of high-scalability profit structure behind them?

18:30.800 --> 18:34.920
I would stick up for the people who are working on the sort of what you could call

18:34.920 --> 18:37.760
like the non-language problems in AI right now.

18:37.760 --> 18:39.480
This stuff is going on.

18:39.480 --> 18:43.360
It maybe doesn't get as much attention from people like three of us as it should.

18:43.360 --> 18:48.680
But if you talk to folks in fields like pharmaceuticals and biotech,

18:48.680 --> 18:52.800
there are new AI biotech companies spinning up every day,

18:52.800 --> 18:58.120
getting funding to go after drug discovery or some more narrow application.

18:58.120 --> 19:01.600
Like we talked to a researcher the other day, formerly of Google,

19:01.600 --> 19:04.200
who is teaching AI to smell, right?

19:04.200 --> 19:08.560
Taking the same techniques that go into these transformer-based neural networks

19:08.560 --> 19:13.640
like chat GPT and applying them to the molecular structures of different chemicals

19:13.640 --> 19:17.000
and using that to be able to predict what these things will smell like.

19:17.000 --> 19:19.240
And you might say, well, what's the big deal with that?

19:19.240 --> 19:23.640
And the answer is that some diseases have smells associated with them

19:23.640 --> 19:29.480
that we can't pick up on because our noses aren't as sensitive as, say, dogs or other animals.

19:29.480 --> 19:32.880
But if you could train an AI to be able to recognize scent molecules

19:32.880 --> 19:36.080
and predict odors from just chemical structures,

19:36.080 --> 19:38.040
that could actually be useful in all kinds of ways.

19:38.040 --> 19:40.240
So I think this kind of thing is happening.

19:40.240 --> 19:43.880
It's just not sort of dominating the coverage the way that chat GPT is.

20:03.000 --> 20:11.400
Let me ask you, Kevin, about, I think, an interesting, maybe promising,

20:11.400 --> 20:15.960
maybe scary avenue for AI that you possibly personally foreclosed,

20:15.960 --> 20:24.880
which is at some point during the year, Microsoft gave you access to a open AI-powered chatbot

20:24.880 --> 20:27.560
that had this dual personality of Sydney.

20:27.560 --> 20:30.240
And Sydney tried to convince you you didn't love your wife

20:30.240 --> 20:32.000
and that you wanted to run away with Sydney.

20:32.000 --> 20:35.760
And my understanding is immediately after that happened,

20:35.760 --> 20:38.400
everybody with enough money to have a real business model in AI

20:38.400 --> 20:40.800
lobotomized the personalities of their AI.

20:40.800 --> 20:42.600
It's like, that was the end of Sydney.

20:42.600 --> 20:46.640
But there are a lot of startups out there trying to do AI friends, AI therapists,

20:46.640 --> 20:51.640
AI sex bots, AI, you know, boyfriends and girlfriends and non-binary partners.

20:51.640 --> 20:56.320
Just every kind of AI companion you can imagine.

20:56.360 --> 21:01.480
I've always thought this is a pretty obvious way this will affect society.

21:01.480 --> 21:05.880
And the Sydney thing convinced me that the technology for it already exists.

21:05.880 --> 21:09.680
So where is that?

21:09.680 --> 21:11.880
And how are those companies doing?

21:11.880 --> 21:19.360
Yeah, I mean, I'm sorry, A, if I did foreclose the possibility of AI personalities.

21:19.360 --> 21:23.720
I think what's happening is it's just a little too controversial

21:23.720 --> 21:27.560
and sort of fraught force any of the big companies to wait into.

21:27.560 --> 21:33.240
Like Microsoft doesn't want its AI assistants and co-pilots to have strong personalities.

21:33.240 --> 21:34.760
Like that much is clear.

21:34.760 --> 21:39.160
And I don't think their enterprise customers want them to have strong personalities,

21:39.160 --> 21:42.560
especially those personalities are adversarial or confrontational

21:42.560 --> 21:44.360
or creepy or unpredictable in some way.

21:44.360 --> 21:48.280
They want like, they want clippy, but like with real brain power.

21:48.280 --> 21:52.640
But there are companies that are going after this more social AI market.

21:52.680 --> 21:55.000
One of them is this company Character AI,

21:55.000 --> 22:00.720
which was started by one of the original people at Google who made the transformer breakthrough.

22:00.720 --> 22:04.000
And that company is growing pretty rapidly.

22:04.000 --> 22:07.160
They've got a lot of users, especially young users.

22:07.160 --> 22:09.840
And they are doing essentially AI personas.

22:09.840 --> 22:12.160
You can make your own AI persona and chat with it

22:12.160 --> 22:15.400
or you can pick from ones that others have created.

22:15.400 --> 22:17.840
Meta is also going a little bit in this direction.

22:17.840 --> 22:21.160
They have these sort of persona driven AI chatbots.

22:21.160 --> 22:25.320
And all of these companies have put sort of guardrails around

22:25.320 --> 22:31.160
like no one really wants to do the erotic, what they call erotic sort of role play

22:31.160 --> 22:36.560
in part because they don't want to run afoul of things like the Apple app store terms of service.

22:36.560 --> 22:40.200
But I expect that that will also be a big market for young people.

22:40.200 --> 22:44.320
And anecdotally, I mean, I have just heard from a lot of young people

22:44.320 --> 22:49.480
who already say like my friends have AI chatbot friends that they talk to all the time.

22:49.480 --> 22:53.520
And it does seem to be making inroads into high schools.

22:53.520 --> 22:56.600
And that's just an area that I'll be fascinated to track.

22:56.600 --> 22:58.320
I mean, this is going to be huge.

22:58.320 --> 22:59.320
A couple of thoughts coming to mind.

22:59.320 --> 23:02.880
One, I talked to somebody who works at one of the leading AI companies

23:02.880 --> 23:06.120
and they told me that 99% of people whose accounts they remove,

23:06.120 --> 23:09.040
they remove for trying to get it to write tech space erotica.

23:09.040 --> 23:12.280
OK, so that I think speaks to the market demand for this sort of thing.

23:12.280 --> 23:14.920
I've also talked to people who have used the models of this

23:14.920 --> 23:17.560
that are not constrained by any sort of safety guidelines.

23:17.560 --> 23:20.040
And I've been told these things are actually incredible at writing erotica.

23:20.040 --> 23:22.600
So what I'm telling you is there is a $10 billion.

23:22.600 --> 23:24.360
You've got really a lot of reporting on this case.

23:24.360 --> 23:26.600
You say maybe a personal interest.

23:26.600 --> 23:28.880
Look, I write about content moderation.

23:28.880 --> 23:32.000
And like porn is the content moderation frontier.

23:32.000 --> 23:34.440
And it's just very interesting to me that it's so clear

23:34.440 --> 23:37.360
that there are billions of dollars to be made here and no company will touch it.

23:37.360 --> 23:39.680
And I asked one person involved, I said, why don't you just let people do this?

23:39.680 --> 23:42.760
And they basically said, look, if you do this, you become a porn company overnight.

23:42.760 --> 23:44.400
It like overwhelms the usage.

23:44.400 --> 23:46.120
Like this is what people wind up using your thing for.

23:46.160 --> 23:48.280
And you're you're working in a different company then.

23:48.280 --> 23:49.120
So I sort of get it.

23:49.120 --> 23:52.560
But, you know, even setting aside the explicitly erotic stuff, you know,

23:52.560 --> 23:56.000
as you well know and have talked and written about just like the lowliness epidemic

23:56.000 --> 23:59.600
that we have in this country, there's a lot of isolated people in this world.

23:59.720 --> 24:02.680
And I think there is a very real possibility that a lot of those people

24:02.680 --> 24:07.720
will find comfort and joy and delight with talking to these AI based companions.

24:07.880 --> 24:11.320
I also think that when that happens, there will be a culture war over it.

24:11.440 --> 24:14.920
And we will see lengthy segments on Fox News about how the Silicon Valley

24:14.920 --> 24:17.600
technologists created a generation of shut-ins who wants to do nothing

24:17.600 --> 24:19.280
but talk to their fake friends on their phones.

24:19.280 --> 24:21.760
So I do think this is like the cultural war yet to come.

24:21.920 --> 24:25.120
And the question is just sort of when do the enabling technologies get good enough?

24:25.120 --> 24:28.640
And when do companies decide that they're willing to deal with the blowback?

24:28.760 --> 24:30.840
I also think this is going to be a generational thing.

24:30.840 --> 24:34.000
I mean, I'm very interested in this and have been for for a bit, in part

24:34.000 --> 24:38.040
because I suspect if I had to make a prediction here, my five year old

24:38.040 --> 24:42.560
is going to grow up with AI friends and my sort of pat line is that today

24:42.600 --> 24:45.600
we worry that 12 year olds don't see their friends enough in person.

24:46.000 --> 24:49.760
And tomorrow we'll worry that not enough of our 12 year old's friends or persons

24:50.320 --> 24:52.160
because it's going to become normal.

24:52.280 --> 24:55.080
And my sense is that the systems are really good.

24:55.080 --> 24:58.400
If you unleashed them, you are already good enough to

24:58.400 --> 25:00.600
functionally master this particular application.

25:00.600 --> 25:02.720
And the big players simply haven't unleashed them.

25:02.720 --> 25:07.280
I've heard from people at the big companies here who are like, oh, yeah,

25:07.280 --> 25:10.360
if we wanted to do this, we could dominate it.

25:10.800 --> 25:13.960
But that does bring me to a question, which is META kind of does want to do this.

25:13.960 --> 25:16.760
META, which owns Facebook, which is a social media company,

25:17.280 --> 25:21.760
they seem to want to do it in terms of these lame, seeming celebrity avatars.

25:21.760 --> 25:23.360
Like you can talk to AI Snoop Dogg.

25:23.360 --> 25:24.160
So bad.

25:24.160 --> 25:28.840
But that is interesting to me because their AI division is run by Yanlacun,

25:28.920 --> 25:31.920
who is one of the most important AI researchers in the field.

25:32.320 --> 25:36.960
And they seem to have very different cultural dynamics in their AI shop

25:37.400 --> 25:40.680
than Google DeepMind or OpenAI.

25:41.600 --> 25:45.720
Tell me a bit about META's strategy here and what makes them culturally different.

25:46.680 --> 25:51.200
Well, Casey, you cover META and have for a long time and may have some insight here.

25:51.200 --> 25:55.520
My sense is that they are sort of up against a couple problems,

25:55.520 --> 26:02.080
one of which is they have arrived to AI late and to generative AI specifically.

26:02.080 --> 26:06.200
You know, Facebook was for many years considered one of the top two labs

26:06.200 --> 26:09.640
along with Google when it came to recruiting AI talent,

26:09.640 --> 26:13.560
to putting out cutting edge research, to presenting papers at the big AI conferences.

26:13.560 --> 26:15.160
They were one of the big dogs.

26:15.160 --> 26:17.600
And then they sort of had this funny thing happen

26:17.600 --> 26:23.280
where they released a model called Galactica just right before ChatGPT was released last year.

26:23.640 --> 26:28.480
And it was supposed to be this sort of like LLM for science and for research papers.

26:28.840 --> 26:31.280
And it was out for, I think, three days.

26:31.560 --> 26:35.000
And people started noticing that it was making up fake citations.

26:35.000 --> 26:35.720
It was hallucinating.

26:35.720 --> 26:39.320
It was doing what all the AI models do, but it was from META.

26:39.320 --> 26:40.320
And so it felt different.

26:40.320 --> 26:45.200
It had sort of this tarnish on it because people already worried about fake news on Facebook.

26:45.520 --> 26:50.040
And so it got pulled down and then ChatGPT just shortly thereafter

26:50.200 --> 26:52.320
launched and became this global sensation.

26:52.320 --> 26:57.000
So they're sort of grappling for what to do with this technology that they've built now.

26:57.200 --> 27:03.800
There's not a real obvious business case for shoving AI chatbots into products

27:03.800 --> 27:09.200
like Facebook and Instagram, and they don't sell enterprise software like Microsoft does.

27:09.400 --> 27:12.440
So they can't really shove it into paid subscription products.

27:12.440 --> 27:14.640
So my sense from talking with folks over there

27:14.640 --> 27:18.000
is that they're just kind of not sure what to do with this technology that they've built.

27:18.240 --> 27:21.040
And so they're just flinging it open to the masses.

27:21.480 --> 27:22.920
What do you think?

27:22.920 --> 27:23.600
That tracks with me.

27:23.600 --> 27:25.720
I sort of basically don't get it either.

27:25.720 --> 27:29.400
Like basically what you've just said has been explained to me.

27:29.600 --> 27:35.400
They are investing a ton with no obvious return on investment in the near term future.

27:35.680 --> 27:39.400
I will say that these celebrity AI chatbots they've made are quite bad.

27:39.400 --> 27:40.720
Like it's truly baffling.

27:40.720 --> 27:44.560
And the thing is they've taken celebrities, but the celebrities are not playing themselves in the AI.

27:44.560 --> 27:50.600
They've given all of the celebrities silly names and you can just sort of like follow their Instagram

27:50.600 --> 27:55.360
and like send them messages and say like, hey, like character that Snoop Dogg is portraying.

27:55.400 --> 27:56.520
Like, what do you think about it?

27:56.520 --> 27:57.680
So it's all very silly.

27:57.680 --> 28:00.560
And I expect it'll die a rapid death sometime in the next year.

28:00.560 --> 28:02.120
And then we'll see if they have a better idea.

28:02.120 --> 28:05.600
What I will say is like, if you're somebody who wakes up from AI nightmares

28:05.760 --> 28:09.920
some mornings, as a lot of folks in San Francisco do, go listen to Jan LeCun talk about it.

28:10.000 --> 28:12.560
No one has ever been more relaxed about AI than Jan LeCun.

28:12.560 --> 28:15.960
You know, it's just sort of like an army of superhuman assistants

28:15.960 --> 28:17.280
are about to live inside your computer.

28:17.280 --> 28:21.000
They're going to do anything you want to do and there's no risk of them harming you ever.

28:21.160 --> 28:23.520
So if you're, you know, you're feeling anxious, go listen to Jan.

28:24.040 --> 28:25.560
Do you think he's right?

28:25.560 --> 28:27.440
Because it also has led to policy difference.

28:27.440 --> 28:32.960
Meta has been much more open source in their approach, which open AI

28:32.960 --> 28:37.040
and Google seem to think is irresponsible.

28:37.080 --> 28:40.440
But there's something happening there that I think is also built around a different view of safety.

28:40.440 --> 28:41.560
Like, what is their view of safety?

28:41.560 --> 28:44.600
Why does Jan LeCun, who is like an important figure in this whole world,

28:44.880 --> 28:48.520
why is he so much more chill than, you know, name your other founder?

28:49.480 --> 28:52.360
I mean, part of it is I just think these these are deeply held convictions

28:52.360 --> 28:55.320
from someone who is an expert on this space and who has been a pioneer

28:55.320 --> 28:58.080
and who understands the technology certainly far better than I do.

28:58.240 --> 29:01.000
And he can just sort of not see from here to kill a robot.

29:01.000 --> 29:05.760
So I I respect his viewpoint in that respect, given his credentials in the space.

29:06.080 --> 29:10.080
I think on the question of is open source AI safer?

29:10.320 --> 29:13.280
This is still an open question, not to pun.

29:13.520 --> 29:16.240
The argument for it being safer is well, if it's open source,

29:16.240 --> 29:20.520
that means that average people can go in and look at the code and identify flaws

29:20.720 --> 29:23.720
and kind of see how the machine works and they can point those out in public

29:23.720 --> 29:25.160
and then they can be fixed in public.

29:25.160 --> 29:28.920
Whereas if you have something like open AI, which is building very powerful systems

29:28.920 --> 29:31.960
behind closed doors, we don't have the same kind of access.

29:31.960 --> 29:34.520
And so you might not need to rely on a government regulator

29:34.520 --> 29:36.640
to see how safe their systems were.

29:36.640 --> 29:38.560
So that is the argument in favor of open source.

29:38.560 --> 29:42.680
Of course, the flip side of that is like, well, if you take a very powerful open source model

29:42.880 --> 29:46.520
and you put it out on the open web, even if it's true that anyone can poke holes

29:46.520 --> 29:49.920
and identify flaws, it's also true that a bad actor could take that model

29:49.920 --> 29:51.920
and then use it to do something really, really bad.

29:51.920 --> 29:55.240
So that hasn't happened yet, but it certainly seems like it's

29:55.240 --> 29:57.520
an obvious possibility at some time in the near future.

29:58.120 --> 30:00.600
Let me use that as a bridge to safety more generally.

30:00.600 --> 30:04.040
So we've talked a bit about where these systems have gone over the past year,

30:04.040 --> 30:05.440
where they seem to be going.

30:05.440 --> 30:10.280
But there's been a lot of concern that they are unsafe and fundamentally

30:10.280 --> 30:14.160
that they become misaligned or that we don't understand them or what they're doing.

30:15.000 --> 30:17.600
What kind of breakthroughs have there been with all this investment

30:17.600 --> 30:19.840
and all this attention on safety, Kevin?

30:20.960 --> 30:26.760
So a lot of work has gone into what is called fine tuning of these models.

30:26.960 --> 30:31.720
So basically, if you're making a large language model like GPT-4,

30:31.920 --> 30:33.880
you have several phases of that.

30:33.880 --> 30:38.160
Phase one is what's called pre-training, which is sort of just the basic process.

30:38.160 --> 30:40.800
You take all of this data, you shove it into this neural network,

30:40.800 --> 30:44.080
and it learns to make predictions about the next word in a sequence.

30:44.600 --> 30:46.720
Then from there, you do what's called fine tuning.

30:47.000 --> 30:51.000
And that is basically where you are trying to turn the model into something

30:51.000 --> 30:54.160
that's actually useful or tailored, turned it into a chatbot,

30:54.160 --> 30:58.960
turned it into a tool for doctors, turned it into something for social AIs.

30:58.960 --> 31:02.960
That's the process that includes things like reinforcement learning from human feedback,

31:02.960 --> 31:06.080
which is how a lot of the leading models are fine tuned.

31:06.400 --> 31:08.840
And that work has continued to progress.

31:09.160 --> 31:14.400
The models they say today are sort of safer and less likely to generate harmful outputs

31:14.680 --> 31:16.720
than previous generations of models.

31:17.000 --> 31:19.480
There's also this field of interpretability,

31:19.480 --> 31:22.720
which is where I've been doing a lot of reporting over the past few months,

31:23.080 --> 31:26.040
which is this sort of tiny subfield of AI

31:26.040 --> 31:30.840
that is trying to figure out what the guts of a language model look like

31:30.840 --> 31:34.040
and what is actually happening inside one of these models

31:34.320 --> 31:38.080
when you ask it a question or give it some prompt and it produces an output.

31:38.320 --> 31:42.880
And this is a huge deal, not only because I think people want to know how these things work,

31:42.880 --> 31:46.080
they're not satisfied by just saying these are like mystical black boxes,

31:46.520 --> 31:49.480
but also because if you understand what's going on inside a model,

31:49.480 --> 31:53.280
then you can understand if, for example, the model starts lying to you

31:53.280 --> 31:57.720
or starts becoming deceptive, which is a thing that AI safety researchers worry about.

31:57.720 --> 32:01.360
So that process of interpretability research, I think, is really important.

32:01.360 --> 32:05.720
There have been a few sort of minor breakthroughs in that field over the past year,

32:05.880 --> 32:09.720
but it is still slow going and it's still a very hard problem to crack.

32:10.360 --> 32:13.360
And I think it's worth just pausing to underscore what Kevin said,

32:13.360 --> 32:17.240
which is the people building these systems do not know how they work.

32:17.240 --> 32:20.720
They know at a high level, but there is a lot within that,

32:20.720 --> 32:23.360
where if you show them an individual output from the AI,

32:23.360 --> 32:26.760
they will not be able to tell you exactly why it said what it said.

32:26.800 --> 32:30.120
Also, if you run the same query multiple times, you'll get slightly different answers.

32:30.120 --> 32:32.840
Why is that? Again, the researchers can't tell you.

32:32.960 --> 32:36.240
So as we have these endless debates over AI safety,

32:36.360 --> 32:39.960
one reason why I do tend to lean on the side of the folks who are scared

32:39.960 --> 32:42.120
is this exact point at the end of the day.

32:42.120 --> 32:44.040
We still don't know how the systems work.

32:44.040 --> 32:45.840
Tell me if this tracks for you.

32:45.840 --> 32:49.960
I think compared to a year ago when I talked to the AI safety people,

32:49.960 --> 32:54.120
the people who worry about AIs that become misaligned

32:54.120 --> 32:59.920
and do terrible civilizational level damage, AIs that could be really badly misused.

33:00.520 --> 33:03.960
They seem to think it has been actually a pretty good year, most of them.

33:04.080 --> 33:06.760
They think they've been able to keep big models like GPT-4,

33:06.760 --> 33:10.920
which of course are much less powerful than what they one day expect to invent.

33:10.920 --> 33:13.000
But they think they've been pretty good at keeping them aligned.

33:13.560 --> 33:17.600
They have made some progress on interpretability, which wasn't totally clear.

33:17.600 --> 33:21.640
You know, a year ago, many people said that was potentially not a problem we could solve.

33:22.240 --> 33:24.120
You know, at least we're making some breakthroughs there.

33:24.800 --> 33:26.600
They're not relaxed.

33:26.600 --> 33:30.560
The people who worry about this and they, you know,

33:30.560 --> 33:33.560
will often say like we would need a long time to fully understand

33:33.560 --> 33:36.280
even the things we have now and we may not have that long.

33:36.760 --> 33:42.640
But nevertheless, I get the sense that the safety people seem a little more confident

33:42.640 --> 33:46.000
that the work, the technical work they've been doing is paying off.

33:46.280 --> 33:49.920
Then, you know, at least with the impression I got from the reporting prior.

33:50.600 --> 33:51.200
I think that's right.

33:51.200 --> 33:55.800
I mean, Sam Altman in particular, this has been his strategy is like,

33:55.800 --> 33:59.440
we are we are going to release this stuff that is in our our labs.

33:59.760 --> 34:03.240
And we're going to kind of wait and see how society reacts to it.

34:03.240 --> 34:05.640
And then we'll sort of give it some time to let society address.

34:05.880 --> 34:07.760
And then we will release the next thing.

34:07.760 --> 34:12.200
That's what he thinks is the best way to slowly integrate AI into our lives.

34:12.480 --> 34:17.240
And if you'd asked me maybe 11 months ago, like a month into using chat GPT,

34:17.600 --> 34:20.400
what are the odds of something really, really bad happening

34:20.400 --> 34:22.600
because of the availability of chat GPT?

34:22.920 --> 34:25.680
I would have put them much higher than they turned out to be.

34:25.680 --> 34:27.560
Right. And when you talk to folks at Open AI,

34:27.560 --> 34:31.720
like they will tell you that that company really has taken AI safety

34:31.720 --> 34:35.120
really seriously, you can see this yourself when you use the product.

34:35.120 --> 34:36.360
Ask it a question about sex.

34:36.360 --> 34:37.880
It basically calls the police.

34:37.880 --> 34:41.360
So there is a lot to be said for how these systems have been built so far.

34:42.000 --> 34:46.280
And I would say the other thing that I've heard from AI safety researchers

34:46.280 --> 34:50.520
is that they are feeling relief, not just that the world has not ended,

34:50.760 --> 34:54.120
but that more people are now worried about AI.

34:54.520 --> 34:57.800
It was a very lonely thing for many years

34:58.000 --> 35:01.840
to be someone who worried about AI safety because

35:02.440 --> 35:05.400
there was no apparent reason to be worried about AI safety, right?

35:05.400 --> 35:09.600
That the chatbots that were outward, it was like Siri and Alexa and they were terrible.

35:09.800 --> 35:13.520
And no one could imagine that these things could become dangerous or harmful

35:13.520 --> 35:16.240
because the technology itself was just not that advanced.

35:16.600 --> 35:18.720
Now you have congressional hearings.

35:18.840 --> 35:22.120
You have regulations coming from multiple countries.

35:22.320 --> 35:26.000
You have people like Jeff Hinton and Yashua Bengios,

35:26.000 --> 35:28.760
two of the sort of so-called godfathers of deep learning,

35:29.040 --> 35:31.680
proclaiming that they are worried about where this technology is headed.

35:31.680 --> 35:35.520
So I think for the people who have been working on this stuff for a long time,

35:35.760 --> 35:37.800
there is some just palpable relief at like,

35:37.800 --> 35:40.520
oh, I don't have to carry this all on my shoulders anymore.

35:40.520 --> 35:43.920
The world is now aware of these systems and what risks they could pose.

35:44.400 --> 35:49.360
One irony of it is that my read from talking to people is that AI safety

35:49.360 --> 35:53.040
is going better as a technical matter than was expected.

35:53.720 --> 35:59.320
And I think worse as a matter of governance and inter-corporate

35:59.840 --> 36:03.640
competition and regulatory arbitrage than they had hoped.

36:04.120 --> 36:07.880
There's a fear, as I understand it, that we could make the technical breakthroughs

36:07.880 --> 36:13.680
needed, but that the kind of coordination necessary to go slow enough to make them.

36:14.360 --> 36:16.000
Like that's where a lot of the fear is.

36:16.000 --> 36:18.560
I think they feel like that's actually going worse, not better.

36:19.280 --> 36:24.560
So one of the big narratives coming out of Sam Altman's firing was that it must

36:24.560 --> 36:26.640
have had something to do with AI safety.

36:26.960 --> 36:31.280
And, you know, based on my reporting and reporting shared by many others,

36:31.520 --> 36:35.760
this was not an AI safety issue, but it is very much the story

36:35.760 --> 36:39.240
that is being discussed about the whole affair.

36:39.520 --> 36:44.320
And the folks who are on the board who are associated with these AI safety ideas,

36:44.560 --> 36:47.200
they've taken a huge hit to their public reputation because of the way

36:47.200 --> 36:48.760
they handle the firing and sort of all of that.

36:49.120 --> 36:53.880
And so I think a really bad outcome of this firing is that the AI safety

36:53.880 --> 36:58.800
community loses its credibility, even though AI safety, as far as we can tell,

36:58.920 --> 37:01.320
really didn't have a lot to do with what happened to Sam Altman.

37:01.800 --> 37:05.560
Yeah, I first agree that clearly, AI safety was not behind

37:05.560 --> 37:07.320
whatever disagreements Altman and the board had.

37:07.320 --> 37:11.640
I heard that from both sides of this and I didn't believe it and I didn't believe

37:11.640 --> 37:13.000
it and I finally was convinced of it.

37:13.000 --> 37:14.960
I was like, you guys had to have had some disagreement here.

37:14.960 --> 37:16.000
It seems so fundamental.

37:16.520 --> 37:19.240
But this is sort of what I mean, the governance is going worse.

37:20.120 --> 37:24.040
All the open AI people thought it was very important and Sam Altman himself

37:24.040 --> 37:27.440
talked about its importance all the time, that they had this non-profit board

37:27.440 --> 37:30.840
connected to this non-financial mission, right?

37:30.840 --> 37:34.320
The values of building AI that served humanity that could fire Sam

37:34.320 --> 37:37.560
Altman at any time or even shut down the company fundamentally

37:37.960 --> 37:40.880
if they thought it was going awry in some way or another.

37:41.480 --> 37:45.440
And the moment that board tried to do that, now I think they did not try

37:45.480 --> 37:46.760
to do that on very strong grounds.

37:46.760 --> 37:49.800
But the moment they tried to do that, it turned out they couldn't.

37:50.280 --> 37:55.000
That the company could fundamentally reconstitute itself at Microsoft

37:55.440 --> 37:59.080
or that the board itself couldn't withstand the pressure coming back.

37:59.560 --> 38:04.120
I think the argument from the board's side, the now mostly defunct board,

38:04.880 --> 38:09.040
is that this didn't go as badly for them as the press is reporting,

38:09.040 --> 38:13.680
that they brought in some other board members who are not cronies of Sam

38:13.680 --> 38:15.000
Altman and Greg Brockman.

38:15.600 --> 38:18.480
Sam Altman and Greg Brockman are not on the board now.

38:18.480 --> 38:20.200
There's going to be investigation into Altman.

38:20.600 --> 38:24.720
So maybe they have a stronger board that is better able to stand up to Altman.

38:24.760 --> 38:26.600
That is one argument I have heard.

38:27.000 --> 38:31.480
On the other hand, those stronger board members do not hold the views on AI

38:31.480 --> 38:36.160
safety that the board members who left like Helen Toner of Georgetown

38:36.160 --> 38:38.760
and Tasha Macaulay from Rand held.

38:39.280 --> 38:42.000
I mean, these are people who are going to be very interested in whether or not

38:42.000 --> 38:43.160
open is making money.

38:43.440 --> 38:46.640
I'm not saying they don't care about other things too, but these are people

38:46.640 --> 38:47.600
who know how to run companies.

38:47.600 --> 38:52.040
They serve on corporate boards in a normal way where like the output of the

38:52.040 --> 38:54.800
corporate board is supposed to be shareholder value and that's going to

38:54.800 --> 38:57.800
influence them even if they understand themselves to have a different mission here.

38:58.240 --> 39:00.080
Am I getting that story wrong to you?

39:00.360 --> 39:01.480
No, I think that's right.

39:01.480 --> 39:06.200
And it speaks to one of the most interesting and sort of strangest things

39:06.200 --> 39:11.560
about this whole industry is that the people who started these companies

39:12.040 --> 39:13.400
were weird.

39:13.400 --> 39:16.920
And I say that with no sort of like normative judgment, but they made

39:16.920 --> 39:18.400
very weird decisions.

39:18.600 --> 39:21.120
Like they thought AI was exciting and amazing.

39:21.120 --> 39:25.400
They wanted to build AGI, but they were also terrified of it to the point

39:25.400 --> 39:27.960
that they developed these elaborate safeguards.

39:28.120 --> 39:32.720
I mean, not in open AI's case, they put this nonprofit board in charge

39:32.720 --> 39:36.640
of the for-profit subsidiary and gave essentially the nonprofit board the

39:36.640 --> 39:40.320
power to push a button and shut down the whole thing if they wanted to.

39:40.600 --> 39:44.240
At Anthropic, one of these other AI companies, they are structured as a

39:44.240 --> 39:47.800
public benefit corporation and they have kind of this their own version

39:47.800 --> 39:51.840
of a nonprofit board that is capable of essentially pushing the big red

39:51.840 --> 39:54.800
shut it all down button if things get too crazy.

39:55.200 --> 39:59.360
This is not how Silicon Valley typically structures itself.

39:59.360 --> 40:03.320
Like Mark Zuckerberg was not in his Harvard dorm room building Facebook

40:03.520 --> 40:06.920
thinking like if this thing becomes the most powerful communication

40:06.920 --> 40:11.120
platform in the history of technology, like I will need to put in place these

40:11.120 --> 40:14.160
checks and balances to keep myself from becoming too powerful.

40:14.600 --> 40:17.200
But that was the kind of thing that the people who started open AI

40:17.200 --> 40:18.560
in Anthropic were thinking about.

40:18.560 --> 40:23.080
And so I think what we're seeing is that that kind of structure is sort

40:23.080 --> 40:27.920
of bowing to the requirements of shareholder capitalism, which says

40:27.920 --> 40:31.280
that, you know, if you do need all this money to run these companies to train

40:31.280 --> 40:35.080
these models, you are going to have to make some concessions to the sort

40:35.080 --> 40:38.080
of powers of the shareholder and of the money.

40:38.080 --> 40:41.320
And so I think that one of the big pieces of fallout from this open

40:41.320 --> 40:45.480
AI drama is just that open AI is going to be structured and run much more

40:45.480 --> 40:49.600
like a traditional tech company than this kind of holdover from this nonprofit board.

40:50.600 --> 40:51.920
And that is just a sad story.

40:52.080 --> 40:55.400
I truly wish that it had not worked out that way.

40:55.760 --> 40:58.360
I think one of the reasons why these companies were built in this way was

40:58.360 --> 41:00.320
because it just helped them attract better talent.

41:00.680 --> 41:04.760
I think that so many people working in AI are idealistic and civic

41:04.760 --> 41:07.040
minded and do not want to create harmful things.

41:07.040 --> 41:10.200
And they're also really optimistic about the power that good technology has.

41:10.520 --> 41:13.440
And so when those people say that as powerful and good as these things could

41:13.440 --> 41:16.160
be, it could also be really dangerous, I take them really seriously.

41:16.400 --> 41:17.600
And I want them to be empowered.

41:17.600 --> 41:19.240
I want them to be on company boards.

41:19.560 --> 41:23.880
And those folks have just lost so much ground over the past couple of weeks.

41:23.880 --> 41:27.520
And it is a truly tragic development, I think, in the development of this industry.

41:34.760 --> 41:55.640
One thing you could just say with that is, yeah, it was always going to be up to governments

41:55.640 --> 42:00.640
here, not up to strange nonprofit corporate, semi-corporate structures.

42:01.520 --> 42:06.640
And so we actually have seen a huge amount of government activity in recent weeks.

42:07.080 --> 42:08.840
And so I want to start here in the US.

42:09.120 --> 42:14.080
Biden announced a big package of a big executive order.

42:14.080 --> 42:15.120
You could call them regulations.

42:15.120 --> 42:16.800
I sort of call them pre-regulations.

42:17.120 --> 42:21.720
But Casey, how would you describe in some what they did?

42:21.720 --> 42:26.520
Like, what is a Biden administration's approach that it is signaling to regulating AI?

42:27.320 --> 42:35.160
The big headline was, if you are going to train a new model, so a sort of successor to a GPT-4,

42:35.480 --> 42:40.520
and it uses a certain amount of energy, and the energy there is just sort of a proxy for how

42:40.520 --> 42:44.840
powerful and capable this model might be, you have to tell the federal government that you have

42:44.840 --> 42:50.840
done this, and you have to inform them what safety testing you did on this model before

42:50.920 --> 42:52.600
releasing it to the public.

42:52.920 --> 42:58.200
So that is the one kind of break that they attempted to put on the development of this

42:58.200 --> 43:01.240
industry. It does not say you can't train these models.

43:01.240 --> 43:04.120
It doesn't specify what safety tests you have to do.

43:04.120 --> 43:08.920
It just says, if you're going to go down this road, you have to be in touch with us.

43:08.920 --> 43:13.640
And that will, I think, slightly decelerate the development of these models.

43:13.640 --> 43:19.880
I think critics would say it also pushes us a little bit away from a more open source version

43:19.880 --> 43:24.760
of AI, that open source development is sort of chaotic by its nature.

43:24.760 --> 43:28.840
And if you want to do some sort of giant open source project that would compete

43:28.840 --> 43:32.600
with the GPTs of the world, that would just sort of be harder to do.

43:32.600 --> 43:34.680
But to me, those are sort of the big takeaways.

43:35.320 --> 43:40.360
One of the things that struck me looking at the order was, go back a year, go back two years.

43:41.080 --> 43:45.800
I think the thing that people have said is that the government doesn't understand this at all.

43:46.600 --> 43:49.720
It can barely be conversant in technology.

43:49.720 --> 43:53.240
People remember Senator Orrin Hatch asking Mark Zuckerberg, well,

43:53.240 --> 43:55.480
if you're not making people pay, then how do you make money?

43:56.680 --> 44:02.520
When I read the order and looked at it, this actually struck me as pretty seriously engaged.

44:03.160 --> 44:07.720
Like, for instance, there's a big debate in the AI world about whether or not you're going to

44:07.720 --> 44:13.240
regulate based on the complexity and power of the model or the use of the model.

44:13.960 --> 44:18.040
You have a fear about what happens if you're using the model for medical decisions.

44:18.040 --> 44:20.200
But if you're just using it as your personal assistant, who cares?

44:20.760 --> 44:24.360
Whereas the AI safety people have the view that, no, the personal assistant model might

44:24.360 --> 44:27.720
actually be the really dangerous one because that's one that knows how to act in the real world.

44:28.360 --> 44:32.040
The Biden administration takes a view of the AI safety people.

44:32.040 --> 44:35.480
If you have a model over a certain level of computing complexity,

44:35.480 --> 44:39.080
they want this higher level of scrutiny, higher level of disclosure on it.

44:39.080 --> 44:42.520
They want everything that comes from an AI to be watermarked in some way,

44:42.520 --> 44:45.080
so you can see that it is AI generated.

44:45.960 --> 44:49.720
This struck me as a Biden administration actually clearly having taken this seriously

44:49.720 --> 44:54.040
and having convened some set of group of stakeholders and experts that knew what they

44:54.040 --> 44:57.800
were doing. I mean, I don't necessarily agree with literally every decision and a lot of it

44:57.800 --> 45:01.560
is just asking for reports. But when you think about it as a framework for regulation,

45:02.280 --> 45:06.360
it didn't read to me as a framework coming from people who had not thought about this for 10 minutes.

45:07.320 --> 45:11.400
Absolutely. I was quite impressed. You know, I had a chance to meet with Ben Buchanan at the

45:11.400 --> 45:15.480
White House who worked on this, talked to him about this stuff, and it is clear that they have been

45:15.480 --> 45:19.400
reading everything. They've been talking to as many people as they can, and they did arrive

45:19.400 --> 45:25.400
in a really nuanced place. And I think when you look at the reaction from the AI developers in

45:25.400 --> 45:29.640
general, it was mostly like neutral to lightly positive, right? There was not a lot of blowback,

45:29.640 --> 45:34.200
but at the same time, folks in civic society, I think, were also excited that the government did

45:34.200 --> 45:39.480
have a point of view here and had done its own work. Yeah, it struck me as a very deft set of...

45:40.440 --> 45:44.760
I think I would agree that they're more like pre-regulations than regulations. And to me,

45:44.760 --> 45:48.600
it sounded like what the Biden White House was trying to do was throw a few bones to everyone.

45:48.600 --> 45:53.000
What was like, we're going to throw a few bones to the AI safety community who worries about

45:53.000 --> 45:57.560
foundation models becoming too powerful. We're going to throw some bones to the AI

45:57.560 --> 46:02.280
harms community that is worried about things like bias and inaccuracy. And we're going to throw

46:02.280 --> 46:09.080
some bones to the people who worry about foreign use of AI. So I saw it as a very sort of deliberate

46:09.080 --> 46:13.720
attempt to give every sort of camp in this debate a little to feel happy about.

46:15.240 --> 46:23.560
One of the things it raised for me as a question, though, was, did it point to a world where you

46:23.560 --> 46:27.720
think that regulators are going to be empowered to actually act? This was the thing I was thinking

46:27.720 --> 46:33.800
about after the board collapse. You imagine a world sometime in the future where you have open AI,

46:34.280 --> 46:41.160
with GPT-6 or META or whomever, right? And they are releasing something that the regulator is

46:41.720 --> 46:47.160
looking at the safety data, looking at what's there. They're just itchy about it. It's not

46:47.160 --> 46:51.720
obviously going to do a ton of harm, but they're not convinced it's safe. They've seen some things

46:51.720 --> 47:00.120
that worry them. Are they really going to have the power to say, no, we don't think your safety

47:00.120 --> 47:04.440
testing was good enough. When this is a powerful company, when they won't be able to release a

47:04.440 --> 47:08.360
lot of the proprietary data, right? The thing where the board could not really explain why

47:08.360 --> 47:14.440
they were firing Sam Altman struck me as almost going to be the situation of virtually every regulator

47:14.440 --> 47:18.200
trying to think about the future harms of a model. If you're regulating in time to stop a thing from

47:18.200 --> 47:21.880
doing harm, it's going to be a judgment call. And if it's a judgment call, it's going to be a very

47:21.880 --> 47:26.760
hard one to make. And so if we ever got to the point where somebody needed to flip the switch

47:26.760 --> 47:33.800
and say, no, does anybody actually have the credibility to do it? Or is what we've seen that

47:34.360 --> 47:40.200
in fact, like these very lauded successful companies run by smart people who have huge

47:40.200 --> 47:44.600
Twitter followings or threads followings, whatever they end up being on, that they actually have

47:44.600 --> 47:48.920
so much public power that they'll always be able to make the case for themselves. And like the

47:48.920 --> 47:52.920
political economy of this is actually that we better just hope the AI companies get it right

47:53.560 --> 47:57.480
because nobody's really going to have the capability stand in front of them.

47:58.280 --> 48:02.680
When you talk to folks who are really worried about AI safety, they think that there is a high

48:02.680 --> 48:07.560
possibility that at some point in let's say the next five years, AI triggers some sort of event

48:07.560 --> 48:12.040
that kills multiple thousands of people. What that event could be, we could speculate, but

48:12.040 --> 48:16.120
assume that that is true. I think that changes the political debate a lot, right? Like that's just

48:16.120 --> 48:20.760
all of a sudden you start to see jets get scrambled. Hopefully that never happens, but I think that

48:21.240 --> 48:24.280
the inciting moment. And this is the thing that just frustrates me as somebody who writes about

48:24.280 --> 48:27.800
tech policy is we just live in a country that doesn't pass laws. There are endless hearings,

48:27.800 --> 48:31.320
endless debates, and then it gets time to regulate something. And it's like, well, yeah,

48:31.320 --> 48:35.800
they can regulate AI, but it's going to be based on this one regulation that was passed to deal with

48:35.800 --> 48:40.440
like the oat farming crisis of 1906. And we're just going to hope that it applies. It's like,

48:40.440 --> 48:43.320
we should pass new laws in this country. I don't know that there's a law that needs to be passed

48:43.320 --> 48:48.600
today to ensure that all of this goes well, but certainly Congress is going to need to do something

48:48.600 --> 48:53.160
at some point as the stuff evolves. I mean, one thing I was thinking about as this whole situation

48:53.160 --> 48:59.080
at OpenAI was playing out was actually the financial crisis in 2008 and the scenes that were

48:59.080 --> 49:04.760
captured in books and movies where you have the heads of all the investment banks and they're

49:04.760 --> 49:11.000
scrambling to avoid going under and they're meeting in these boardrooms with people like

49:11.000 --> 49:16.600
Ben Bernanke, the chair of the Federal Reserve, and the government actually had a critical role

49:16.600 --> 49:21.800
there in patching together the financial system because they were sort of interested,

49:21.800 --> 49:27.400
not in which banks survived and which failed, but in making sure that there was a banking system

49:27.400 --> 49:32.520
when the markets opened the next Monday. And so I think we just need a new regulatory framework

49:32.520 --> 49:38.360
that does have some kind of the sort of cliched word would be stakeholder, but someone who is

49:38.360 --> 49:42.920
in there as a representative of the government who's saying, what is the resolution to this

49:43.000 --> 49:46.680
conflict that makes sense for most Americans or most people around the world?

49:47.640 --> 49:52.840
When you looked at who the government gave power to in this document, when you think about who

49:52.840 --> 49:58.040
might play a role like that, when you need to call the government on AI, the way I read it is it

49:58.040 --> 50:03.480
spread power out across a lot of different agencies. And there were places where I invested

50:03.480 --> 50:08.760
more rather than less, but one thing that different people have called for that I didn't see it do,

50:08.760 --> 50:12.760
in part because you would actually need to pass a law to do this, was actually create

50:12.760 --> 50:19.480
the AI department, something that is funded and structured and built to do this exact thing,

50:19.480 --> 50:23.240
to be the central clearinghouse inside the government, to be led by somebody who would

50:23.240 --> 50:29.560
be the most credible on these issues. And would maybe have then the size and strength to do this

50:29.560 --> 50:34.200
kind of research, right? The thing that is in my head here, because I find your analogy really

50:34.200 --> 50:40.200
compelling, Kevin, is a federal reserve. The federal reserve is a big institution. And it has

50:40.760 --> 50:44.600
significant power of its own in terms of setting interest rates. It also does a huge amount of

50:44.600 --> 50:48.280
research. Like when you think about where would a public option for AI come from,

50:48.280 --> 50:51.640
you would need something like that that has the money to be doing its own research and

50:51.640 --> 50:56.600
hiring the really excellent people, in that case, economists, in this case, AI researchers.

50:57.160 --> 51:01.400
And there was nothing like that here. It was sort of an assertion that we more or less have

51:01.400 --> 51:05.640
the structure we need. We more or less have the laws we need. We can apply all those things

51:05.640 --> 51:11.320
creatively. But it did not say like, this is such a big deal that we need a new institution

51:11.960 --> 51:18.680
to be our point person on it. Yeah, I mean, I think that's correct. I think there are some

51:18.680 --> 51:26.440
reasons for that. But I think you do want a government that has its own technological capacity

51:26.440 --> 51:32.440
when it comes to AI, previous waves of innovation, certainly nuclear power during the Manhattan

51:32.440 --> 51:37.480
project, but also things like the internet came out of DARPA. These are areas where the government

51:37.480 --> 51:43.800
did have significant technical expertise and was building its own technology in sort of competition

51:43.800 --> 51:49.160
with the private sector. There is no public sector equivalent of chat GPT. The government has not

51:49.160 --> 51:54.680
built anything even remotely close to that. And I think it's worth asking why that is and what would

51:54.680 --> 52:00.040
need to happen for the government to have its own capacity, not just to evaluate and regulate

52:00.040 --> 52:06.040
these systems, but to actually build some of their own. I think it is genuinely strange on some level

52:06.840 --> 52:14.040
that given how important this is, there is not a bill gathering steam. Look, the private sector

52:14.040 --> 52:22.120
thinks it is worth pumping 50 or $100 billion into these companies so they can help you make

52:22.120 --> 52:29.000
better enterprise software. It seems weird to imagine that there are not public problems that

52:29.000 --> 52:36.200
have an economic value that is equal to that or significantly larger. And we may just not want

52:36.200 --> 52:41.800
to pay that money fine. But we do that for infrastructure. We just passed a gigantic

52:41.800 --> 52:47.960
infrastructure bill. And if we thought of AI like infrastructure, we actually also spend a lot of

52:47.960 --> 52:52.440
money on broadband now. It seems to me you want to think about it that way. And I think it is a

52:52.440 --> 52:59.480
kind of fecklessness and cowardice on the part of like the political culture that it no longer

52:59.480 --> 53:03.640
thinks itself capable of doing things like that. Like at the very least, and I've said this I think

53:03.640 --> 53:08.680
on your show probably, I think they should have prize systems where they say a bunch of things

53:08.680 --> 53:12.280
they want to see solved. And if you can build an AI system that will solve them, they'll give you a

53:12.280 --> 53:18.440
billion dollars. But the one thing is the government does not like to do things that spend money for

53:18.440 --> 53:24.120
an uncertain return. And building a giant AI system is spending a lot of money for an uncertain return.

53:24.920 --> 53:28.200
And so the only part of the government that is probably doing something like it is a defense

53:28.200 --> 53:32.200
department in areas that we don't know. And that does not make me feel better. That makes me feel

53:32.200 --> 53:37.400
worse. That's my take on that. Yeah, I mean, I think there's also a piece of this that has to do

53:37.400 --> 53:44.440
with labor and talent. You know, there are probably on the order of several thousand people in the

53:44.440 --> 53:53.080
world who can oversee the building, training, fine tuning deployment of large language models.

53:53.080 --> 53:59.560
It is a very specific skill set. And the people who have it can make gobs of money in the private

53:59.560 --> 54:06.040
sector working wherever they want to the numbers that you hear coming out of places like open AI

54:06.040 --> 54:12.360
for what engineers are being paid there. I mean, it's like NFL level football compensation packages

54:12.360 --> 54:18.120
for some of their people. And the government simply can't or won't pay that much money to

54:18.120 --> 54:22.040
someone to do equivalent work for the public sector. Now, I'm not saying they should be paying

54:22.040 --> 54:25.960
engineers millions of dollars of taxpayer money to build these things, but that's that's what you

54:25.960 --> 54:31.640
would need to do if you wanted to compete in an open market for the top AI talent. I am saying

54:31.640 --> 54:36.120
they should. I am saying this is like the factlessness and cowardice point. This is stupid.

54:36.120 --> 54:40.040
You think there should be AI engineers working for the federal government making five million

54:40.040 --> 54:44.680
dollars a year? Like maybe not five million dollars a year. But it would this is a this thing

54:44.680 --> 54:49.880
that we don't think civil servants should make as much as people in the private sector.

54:50.600 --> 54:54.200
Because I don't know somebody at a congressional hearing is going to stand and be like that person's

54:54.200 --> 55:01.880
making a lot of money. That is a way we rob the public of value. If Google's not wrong,

55:01.880 --> 55:08.440
Microsoft is not wrong that you can create things that are of social value through AI.

55:08.440 --> 55:12.840
And if you believe that, then leaving it to them, I mean, they intend to make a profit.

55:14.040 --> 55:18.360
Why shouldn't the public get great gains from this? It won't necessarily be through profit.

55:18.920 --> 55:23.000
But, you know, if we could cure different diseases or, you know, make big advances on energy,

55:23.000 --> 55:27.880
I just this way of thinking is actually to me the really significant problem. I'm not sure you

55:27.880 --> 55:31.320
would need to pay people as much as you're saying because I actually do think a lot of,

55:31.320 --> 55:35.400
I mean, we both know the culture of the AI people and at least up until a year or so ago.

55:36.280 --> 55:40.920
It was weird. And a lot of them would do weird things and are not living very lush lives.

55:41.480 --> 55:45.720
You know, they're in group houses with each other, taking psychedelics and working on AI

55:45.720 --> 55:51.480
on the weekdays. But I think you can get people in to do important work and you should.

55:51.480 --> 55:55.080
Now, look, you don't have the votes to do stuff like this. I think that's the real answer.

55:55.800 --> 56:01.880
But in other countries, they will and do. Like when Saudi Arabia decides that it needs an AI

56:02.680 --> 56:07.400
to be geostrategically competitive, it will take the money it makes from selling oil to the world.

56:08.040 --> 56:11.720
And in the same way that it's currently using that money to hire sports stars,

56:11.720 --> 56:16.520
it will hire AI engineers for a bazillion dollars and it will get some of them. And then it will

56:16.520 --> 56:22.360
have a decent AI system one day. I don't know why we're waiting on other people to do that. We're

56:22.360 --> 56:27.000
rich. It's stupid. I agree with you, Ezra. And I'm sorry that Kevin is so resistant to your ideas

56:27.000 --> 56:30.360
because I think paying public servants well would do a lot of good for this country.

56:30.920 --> 56:34.840
Look, I think public service should be paid well. I'm just saying when Jim Jordan

56:34.840 --> 56:41.960
gets up and grills the former deep mind engineer about why the Labor Department is paying them

56:41.960 --> 56:47.000
2.6 million dollars to fine tune language models, I'm not sure what the answer is going to be.

56:47.640 --> 56:51.080
No, I agree with you. I think we're all saying in a way the same thing. It's like,

56:51.080 --> 56:55.320
this is a problem. Government by dumb things Jim Jordan says is not going to be a great government

56:55.320 --> 57:01.240
that takes advantage of opportunities from the public. Good. And that sucks. It would be better

57:01.240 --> 57:07.640
if we were doing this differently and if we thought about it differently. Let me ask about China

57:08.520 --> 57:12.920
because China is where on the one hand, at least on paper, the regulations look much tougher.

57:13.720 --> 57:17.960
So one version is maybe the regulating AI much more strictly than we are. Another view that

57:17.960 --> 57:22.920
I've heard is that in fact, that's true for companies, but the Chinese government is making

57:22.920 --> 57:27.000
sure that it's building very, very strong. Do you know to the extent you all have looked at it,

57:27.000 --> 57:31.960
how do you understand the Chinese regulatory approach and how it differs from our own?

57:32.920 --> 57:38.520
I mean, I've looked at it mostly from the standpoint of what are the consumer facing systems

57:38.520 --> 57:45.000
look like. It has only been I think a couple of months since China approved the first consumer

57:45.000 --> 57:52.600
usable chat GPT equivalent. As you might imagine, they have very strict requirements as far as like

57:52.600 --> 57:57.720
what the chatbot can say about Tiananmen Square. So they wind up being more limited maybe than

57:57.720 --> 58:04.200
what you can use in the United States. As far as what is happening behind closed doors and for

58:04.200 --> 58:09.960
their defense systems and that sort of thing, I'm in the dark. So four or five years ago when I

58:09.960 --> 58:15.320
started reporting a book about AI, the conventional wisdom among AI researchers was that China was

58:15.320 --> 58:21.160
ahead and they were going to make all of the big breakthroughs and beat the U.S. technology

58:21.160 --> 58:26.360
companies when it came to AI. So it's been very surprising to me that in the past year since chat

58:26.360 --> 58:32.760
GPT has come out, we have not seen anything even sort of remotely close to that level of performance

58:32.760 --> 58:37.880
coming out of a Chinese company. Now, I do think they are working on this stuff, but it's been

58:37.880 --> 58:43.560
surprising to me that China has been mostly absent from the frontier AI conversation over the past

58:43.560 --> 58:50.200
year. And do you think those things are related? Do you think that the Chinese government's

58:50.920 --> 58:57.880
risk aversion and the underperformance of at least the products and systems we've seen

58:58.440 --> 59:02.680
in China? I mean, there might be things we don't know about. Do you think those things are connected?

59:03.240 --> 59:08.680
Absolutely. I think you do need a risk appetite to be able to build and govern these systems because

59:08.760 --> 59:14.760
they are unpredictable. We don't know exactly how they work. And what we saw, for example,

59:14.760 --> 59:22.280
with Microsoft was that they put out this Bing Sydney chatbot and it got a lot of attention and

59:22.280 --> 59:27.880
blowback and people reported all these crazy experiences. And in China, if something like that

59:27.880 --> 59:32.280
had happened, they might have shut the company down or they might have been deemed such an

59:32.280 --> 59:36.600
embarrassment that they would have radically scaled back the model. And instead, what Microsoft

59:36.600 --> 59:40.920
did was just say, we're going to make some changes to try to prevent that kind of thing from

59:40.920 --> 59:44.520
happening, but we're keeping this model out there. We're going to let the public use it and

59:44.520 --> 59:48.440
they'll probably discover other crazy things and that's just part of the learning process.

59:48.440 --> 59:53.400
That's something that I've been convinced of over the past year, talking with AI executives and

59:53.400 --> 59:59.400
people at these companies, is that you really do need some contact with the public before you start

59:59.400 --> 01:00:04.040
learning everything that these models are capable of and all the ways that they might misbehave.

01:00:05.000 --> 01:00:09.960
What is the European Union trying to do? They've had draft regulations that were

01:00:09.960 --> 01:00:14.360
seemed very expansive. What has been the difference in how they're trying to regulate this versus

01:00:14.360 --> 01:00:18.280
how we are and what in your understanding is the status of their effort?

01:00:19.160 --> 01:00:25.560
Europe was quite ahead with developing its AI Act, but it was written in a pre-chat GPT world.

01:00:25.560 --> 01:00:31.080
It was written in a pre-generative AI world. And so over the past year, they've been trying to

01:00:31.960 --> 01:00:40.040
retrofit it so that it reflects our new reality and is caught up in debate in the meantime.

01:00:40.040 --> 01:00:45.240
But my understanding is the AI Act is not particularly restrictive on what these companies

01:00:45.240 --> 01:00:48.360
can do. So to my understanding, there's nothing in the AI Act that is going to

01:00:48.360 --> 01:00:53.160
prevent these next generation technologies from being built. It's more about companies being

01:00:53.160 --> 01:00:58.840
transparent. Let me add a little bit of flavor to that because I was in Europe just recently

01:00:58.840 --> 01:01:04.280
talking with some lawmakers. And one of the things that people will say about the AI Act

01:01:04.280 --> 01:01:11.080
is that it has this risk-based framework where different AI products are evaluated and regulated

01:01:11.080 --> 01:01:16.680
based on these classifications of this is a low-risk system or this is a high-risk system

01:01:16.680 --> 01:01:22.040
or this is a medium-risk system. And so different rules apply based on which of those buckets

01:01:22.040 --> 01:01:27.240
a new tool falls into. And so right now, what a lot of regulators and politicians and companies

01:01:27.320 --> 01:01:31.720
and lobbyists in Europe are arguing about is what level of risk should something like a

01:01:31.720 --> 01:01:39.160
foundation model, a GPT-4, a Bard, a Claude, are those low-risk systems because they're just chat

01:01:39.160 --> 01:01:45.160
bots or are they high-risk systems because you can build so many other things once you have

01:01:45.160 --> 01:01:51.960
that basic technology? And so that's what my understanding is of the current battle in Europe

01:01:51.960 --> 01:01:56.440
is over whether foundation models, frontier models, whatever you want to call them,

01:01:56.440 --> 01:02:01.000
whether those should be assigned to one risk bucket or another.

01:02:01.640 --> 01:02:06.440
I think that's a good survey of the waterfront. And so I guess I'll end on this question, which is

01:02:06.440 --> 01:02:09.400
all right, we're talking here at the one-year anniversary roughly of chat GPT.

01:02:10.280 --> 01:02:14.200
If you were to guess, if we were having another conversation a year from now on the

01:02:14.200 --> 01:02:19.160
two-year anniversary, what do you think would have changed? What are one or two things each

01:02:19.160 --> 01:02:23.160
of you think is likely to happen over the next year that did not happen this year?

01:02:23.960 --> 01:02:31.560
I think all communication-based work will start to have an element of AI in it. All email, all

01:02:31.560 --> 01:02:36.680
presentations, office work essentially. AI will be built into all the applications

01:02:36.680 --> 01:02:40.120
that we use for that stuff. And so it'll just be sort of part of the background,

01:02:40.120 --> 01:02:43.400
just like autocomplete is today when you're typing something on your phone.

01:02:44.760 --> 01:02:51.080
I would say that AI is going to continue to hollow out the media industry. I think you're

01:02:51.080 --> 01:02:57.080
going to see more publishers turning to these really bad services that just automate the generation

01:02:57.080 --> 01:03:02.920
of copy. You'll see more sort of content farms springing up on the web. It'll reduce publisher

01:03:02.920 --> 01:03:08.440
revenue and we'll just see more digital media businesses either get sold or sort of quietly

01:03:08.440 --> 01:03:13.400
go out of business. And that's going to go hand in hand with the decline of the web in general.

01:03:13.400 --> 01:03:17.560
A year from now, more and more people are going to be using chat GPT and other tools

01:03:17.560 --> 01:03:22.520
as their kind of front door to internet knowledge. And that's just going to sap a lot of life out

01:03:22.520 --> 01:03:27.720
of the web as we know it. So we don't need one more technological breakthrough for any of that

01:03:27.720 --> 01:03:33.080
to happen. That's just a case of consumer preferences taking a while to change. And I

01:03:33.080 --> 01:03:36.920
think it's well underway. So do you think then that next year we're going to see something that

01:03:36.920 --> 01:03:41.880
has been long predicted, which is significant AI related job losses? Is that sort of the argument

01:03:41.880 --> 01:03:46.680
you're making here? I think that to some degree, it already happened this year in digital media.

01:03:46.680 --> 01:03:52.280
And yes, I do think it will start to pick up. Just keep in mind, 12 months is not a lot of

01:03:52.280 --> 01:03:58.440
time for every single industry to ask itself, could I get away with five or 10 or 15% fewer

01:03:58.440 --> 01:04:02.520
employees? And as the end of this year comes around, I have to believe that in lots and lots

01:04:02.520 --> 01:04:06.440
of industries, people are going to be asking that question. Yeah, I agree. I don't know whether

01:04:06.440 --> 01:04:11.080
that there will be sort of one year where all the jobs that are going to vanish, vanish. I think

01:04:11.080 --> 01:04:17.720
it's more likely to be a slow trickle over time. And it's less likely to be mass layoffs than just

01:04:17.720 --> 01:04:23.320
new entrants that can do the same work as incumbents with many fewer people. The software

01:04:23.320 --> 01:04:29.400
development firm that only needs five coders because they have all their coders are using AI

01:04:29.400 --> 01:04:34.920
and they have software that is sort of building itself competing with companies that have 10,000

01:04:34.920 --> 01:04:40.120
engineers and doing so much more capably. So I don't think it's going to necessarily look like

01:04:40.120 --> 01:04:45.080
all the layoffs hit on one day or in one quarter or even in one year, but I do think we're already

01:04:45.080 --> 01:04:49.880
seeing a displacement of jobs through AI. Those are kind of dark predictions. I mean,

01:04:49.880 --> 01:04:54.040
we'll have a little bit better sort of integration of AI into office tools and also we'll begin to

01:04:54.040 --> 01:05:00.200
see really the productivity improvements, create job losses. Is there anything that you think is

01:05:00.200 --> 01:05:05.480
coming down the pike technologically that would be really deeply to the good things that are

01:05:05.480 --> 01:05:09.400
not too far from fruition that you think will make life a lot better for people?

01:05:10.280 --> 01:05:17.160
I mean, I love the idea of universal translators. It's already pretty good using AI to speak in

01:05:17.160 --> 01:05:21.480
one language and get output in another, but I do think that's going to enable a lot of cross-cultural

01:05:21.480 --> 01:05:25.240
communications and there are a lot of products remaining to be built that will essentially

01:05:25.240 --> 01:05:31.240
just drop the latency so that you can talk and hear in real time and have it be quite good.

01:05:31.240 --> 01:05:32.360
So that's something that makes me happy.

01:05:33.000 --> 01:05:37.240
And I'm hopeful that we will use AI, not we as in me and Casey.

01:05:37.240 --> 01:05:37.800
But we might.

01:05:39.000 --> 01:05:44.680
This would be sort of a career change for us, but we as in society, I have some hope that we will

01:05:44.680 --> 01:05:53.000
use AI to cure one of the sort of top deadliest diseases, cancer, heart disease, Alzheimer's,

01:05:53.000 --> 01:05:57.640
things like that that really affect massive numbers of people. I don't have any inside

01:05:57.720 --> 01:06:02.200
reporting that we are on the cusp of a breakthrough, but I know that a lot of energy and research and

01:06:02.200 --> 01:06:07.800
funding is going into using AI to discover new drugs and therapies for some of the leading

01:06:07.800 --> 01:06:12.600
killer diseases and conditions in the world. And so when I want to feel more optimistic,

01:06:12.600 --> 01:06:16.840
I just think about the possibility that all of that bears fruit sometime in the next few years,

01:06:16.840 --> 01:06:17.800
and that's pretty exciting.

01:06:18.520 --> 01:06:21.800
All right. And then also final question, what are a few books you'd each recommend to the

01:06:21.800 --> 01:06:25.720
audience released recommend the audience ask chat GPT to summarize for them?

01:06:26.280 --> 01:06:28.760
Kevin, you want to go first?

01:06:28.760 --> 01:06:34.120
Sure. I actually have two books in a YouTube video. The two books, one of them is called

01:06:34.120 --> 01:06:41.160
electrifying America by David E. Nye. It is a 30 year old history book about the process by which

01:06:41.160 --> 01:06:46.600
America got electricity. And it has been very interesting to read. I read it first a few years

01:06:46.600 --> 01:06:51.320
ago and have been rereading it just to sort of sketch out what would it look like if AI really

01:06:51.320 --> 01:06:56.280
is the new electricity? What happened the last time society was transformed by technology like

01:06:56.280 --> 01:07:01.640
this? The other book I'll recommend is your face belongs to us by my colleague, our colleague at

01:07:01.640 --> 01:07:07.080
the Times, Cashmere Hill, which is about the facial recognition AI company, Clearview AI,

01:07:07.080 --> 01:07:12.920
and is one of the most compelling tech books I've read in a few years. And then the YouTube

01:07:12.920 --> 01:07:18.120
video I'll recommend was just posted a few days ago. It's called Intro to Large Language Models.

01:07:18.200 --> 01:07:25.640
It's made by Andre Karpathy, who is an AI researcher actually at Open AI. And it's his one hour

01:07:25.640 --> 01:07:29.720
introduction to what is a large language model and how does it work? And I've just found it

01:07:29.720 --> 01:07:35.320
very helpful for my own understanding. Casey? Well, as we're with permission, and given that

01:07:35.320 --> 01:07:39.320
Kevin has just given your listeners two great books and a YouTube video to read, I would actually

01:07:39.320 --> 01:07:43.400
like to recommend three newsletters if I could. And the reason is because the books that were

01:07:43.400 --> 01:07:47.960
published this year did not help me really understand the future of the AI industry. And to

01:07:47.960 --> 01:07:51.720
understand what's happening in real time, I really am leaning on newsletters more than I'm

01:07:51.720 --> 01:07:55.880
leaning on books. So is that okay? Yeah, go for it. All right. So the first one,

01:07:55.880 --> 01:08:00.120
cruelly, you already mentioned earlier in this podcast, it's import AI from Jack Clark. Jack

01:08:00.120 --> 01:08:05.480
co-founded Amthropic, one of the big AI developers. And it is fascinating to know which papers he's

01:08:05.480 --> 01:08:09.080
reading every week that are helping him understand this world. And I think that they're arguably

01:08:09.080 --> 01:08:13.400
having an effect on how Amthropic is being created because he is sitting in all of those rooms. So

01:08:13.400 --> 01:08:18.360
that is just an incredible weekly read. I would also recommend AI Snake Oil from the Princeton

01:08:18.360 --> 01:08:24.200
Professor, Arvind Narayanan and a PhD student at Princeton, Syash Kapoor. They're very skeptical

01:08:24.200 --> 01:08:28.280
of AI hype and doomsday scenarios, but they also take the technology really seriously and have

01:08:28.280 --> 01:08:32.040
a lot of smart thoughts about policy and regulation. And then the final one is Pragmatic

01:08:32.040 --> 01:08:37.320
Engineer by this guy, Gurgely Arose. He's this former Uber engineering manager. And he writes

01:08:37.320 --> 01:08:41.560
about a lot of companies, but he writes about them as workplaces. And I love when he writes about

01:08:41.560 --> 01:08:45.800
open AI as a workplace. He interviews people there about culture and management and process. And he

01:08:45.800 --> 01:08:49.240
just constantly reminds you, they're just human beings showing up to the office every day and

01:08:49.240 --> 01:08:53.080
building this stuff. And it's just a really unique viewpoint on that world. So read those

01:08:53.080 --> 01:08:56.280
three newsletters. You'll have a little better sense of what's coming for us in the future.

01:08:56.280 --> 01:09:00.600
What Casey didn't say is that he actually hasn't read a book in 10 years. So it was a bit of a

01:09:00.600 --> 01:09:04.680
trick question. You know what I will say? I did read Your Face Belongs to Us by Cashion,

01:09:04.680 --> 01:09:08.440
incredible book. Definitely read that one. Sure you did. There you go. Casey Newton,

01:09:08.440 --> 01:09:12.200
Kevin Ruse at your podcast, which requires very little reading. It's hard for it.

01:09:12.920 --> 01:09:16.120
Thank you all for being on the show. It's the first illiterate podcast, actually,

01:09:16.120 --> 01:09:24.360
put out by the New York Times. Thank you for having us. Thanks, Ezra. Thanks, guys.

01:09:24.360 --> 01:09:39.800
This episode of the Ezra Klein Show is produced by Roland Hoof, fact-checking by Michelle Harris

01:09:39.800 --> 01:09:44.200
with Kate St. Clair and Mary Marge Locker. Our senior engineer is Jeff Geld. Our senior editor

01:09:44.200 --> 01:09:48.520
is Claire Gordon. The show's production team also includes Emma Vagabou and Kristen Lin,

01:09:48.520 --> 01:09:53.480
original music by Isaac Jones, audience strategy by Christina Samaluski, and Shannon Busta.

01:09:53.480 --> 01:09:57.480
The executive producer of New York Times' opinion audio is Anero Strasser. And special

01:09:57.480 --> 01:10:07.480
thanks to Sonia Herrero.

