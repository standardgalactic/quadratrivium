WEBVTT

00:00.000 --> 00:23.000
From New York Times Opinion, this is the Ezra Klein Show.

00:23.000 --> 00:29.000
So I think you can date this era in artificial intelligence back to the launch of chat GPT.

00:29.000 --> 00:35.000
And what is weird if you talk to artificial intelligence people about that is they'll tell you chat GPT.

00:35.000 --> 00:38.000
It was just a wrapper, an interface system.

00:38.000 --> 00:42.000
The underlying system GPT-3 had been around for a while.

00:42.000 --> 00:46.000
I mean, I'd had access to GPT-3 for quite a while before chat GPT came around.

00:46.000 --> 00:53.000
What chat GPT did was it allowed you to talk to GPT-3 like you were a human and it was a human.

00:53.000 --> 00:56.000
So it made AI more human.

00:56.000 --> 01:03.000
It made it more able to communicate back and forth with us by doing a better job mimicking us and understanding us,

01:03.000 --> 01:04.000
which is amazing.

01:04.000 --> 01:11.000
I don't mean to take anything away from it, but it created this huge land rush for AIs that functionally mimic human beings.

01:11.000 --> 01:16.000
AIs that relate as if they are human beings and try to fool us into thinking that they're human.

01:16.000 --> 01:20.000
But I've always been more interested in more inhuman AI systems.

01:20.000 --> 01:26.000
When you ask somebody who's working on artificial intelligence, including people who believe it could do terrible harm to the world,

01:26.000 --> 01:29.000
why are you doing it? What's the point of this?

01:29.000 --> 01:36.000
They don't say, oh, you know, we should risk these terrible consequences because it's fun to chat with chat GPT.

01:36.000 --> 01:42.000
They say, oh, AI, it's going to solve all these terrible scientific problems we have, clean energy and drug discovery.

01:42.000 --> 01:48.000
And, you know, it's going to create an era of innovation like nothing humanity's ever experienced.

01:48.000 --> 01:52.000
There aren't that many examples, though, of AI doing that yet.

01:52.000 --> 01:55.000
But there is one, which you may have heard me mention before.

01:55.000 --> 02:01.000
And that's AlphaFold, the system built by DeepMind that solved the protein folding problem.

02:01.000 --> 02:05.000
And the protein folding problem is that there are hundreds of millions of proteins.

02:05.000 --> 02:08.000
The way they function has to do with their 3D structure.

02:08.000 --> 02:12.000
But even though it's fairly straightforward to figure out their amino acid sequence,

02:12.000 --> 02:16.000
it's very hard to predict how they will be structured based on that.

02:16.000 --> 02:17.000
We were never able to do it.

02:17.000 --> 02:22.000
We were doing it one by one, studying each one for years to try to figure out and basically map it.

02:22.000 --> 02:27.000
And then they build the system, AlphaFold, which solves a problem,

02:27.000 --> 02:32.000
is able to predict the structure of hundreds of millions of proteins, a huge scientific advance.

02:32.000 --> 02:34.000
So how did they build that?

02:34.000 --> 02:37.000
And what could other systems like that look like?

02:37.000 --> 02:44.000
What is this other path for AI, this more scientific path where you're tuning these systems to solve scientific problems,

02:44.000 --> 02:48.000
not to communicate with us, but to do what we truly cannot do?

02:48.000 --> 02:51.000
Demis Asabis is the founder of DeepMind.

02:51.000 --> 02:56.000
DeepMind is owned by Google and recently Asabis was put in charge of all Google AI.

02:56.000 --> 03:00.000
So now it's called Google DeepMind and he runs all of it.

03:00.000 --> 03:02.000
That makes him one of the most important people in the world,

03:02.000 --> 03:05.000
charting the future of artificial intelligence.

03:05.000 --> 03:10.000
So I asked him to come on the show to talk me through the development of AlphaFold,

03:10.000 --> 03:15.000
how it was built, what came before it, what could come after it,

03:15.000 --> 03:21.000
and what that says about the different ways and the different pathways forward for artificial intelligence.

03:21.000 --> 03:24.000
As always, my email is reclineshow at nytimes.com.

03:29.000 --> 03:31.000
Demis Asabis, welcome to the show.

03:31.000 --> 03:32.000
Thanks for having me.

03:32.000 --> 03:35.000
So tell me about the founding of DeepMind.

03:35.000 --> 03:36.000
You were pretty young at that point.

03:36.000 --> 03:38.000
How did you decide to start it?

03:38.000 --> 03:41.000
What was the vision for it?

03:41.000 --> 03:45.000
Well, actually, my background in AI starts from way before DeepMind.

03:45.000 --> 03:50.000
So I started actually in the games industry, writing computer games and some best selling games,

03:50.000 --> 03:56.000
actually games like Theme Park and Black and White and other games that I worked on in my teenage years.

03:56.000 --> 03:59.000
And they all had AI as a core component of the game.

03:59.000 --> 04:00.000
So let's take Theme Park.

04:00.000 --> 04:03.000
It was a simulation game, came out in 1994.

04:03.000 --> 04:10.000
And you basically created a theme park and lots of little people came in, played on the rides and bought stuff from your stalls.

04:10.000 --> 04:12.000
So there's a whole kind of simulation underlying it.

04:12.000 --> 04:15.000
And AI was the core part of the gameplay.

04:15.000 --> 04:20.000
So I've been working on AI for nearly 30 years now in various guises.

04:20.000 --> 04:22.000
So if you look at my career, I've done lots of different things,

04:22.000 --> 04:27.000
but they were all doing something like an effort like DeepMind working on AGI.

04:27.000 --> 04:33.000
And so all the things I did, the neuroscience, the computer science undergrad, PhD in neuroscience,

04:33.000 --> 04:39.000
was all for gathering information and inspiration for eventually what would become DeepMind.

04:39.000 --> 04:41.000
So I played Theme Park back in the day.

04:41.000 --> 04:47.000
And if you ask me now and you say, hey, Theme Park, that game you played in the 90s, was that AI?

04:47.000 --> 04:49.000
I would say no.

04:49.000 --> 04:55.000
There's a classic line to paraphrase that AI is anything the computer can't do yet.

04:55.000 --> 04:58.000
But now you look back, you think, oh, no, that's just a little video game.

04:58.000 --> 05:02.000
So when you say that was AI, what are you saying by that?

05:02.000 --> 05:04.000
And to you, what is artificial intelligence?

05:04.000 --> 05:12.000
And what's just machine learning or statistics or some less impressive function?

05:12.000 --> 05:19.000
Yeah, so back in the 90s and with games like Theme Park at the time, that was pretty cutting edge.

05:19.000 --> 05:25.000
We were using relatively simple techniques, cellular automata and relatively narrow AI systems,

05:25.000 --> 05:29.000
sort of logic systems really, which was in vogue back in the 80s and 90s.

05:29.000 --> 05:35.000
But it was AI in terms of making a machine do something smart and actually adapt to,

05:35.000 --> 05:39.000
automatically adapt to the way the gamer in that case was playing the game.

05:39.000 --> 05:44.000
So the cool thing about Theme Park was that and why it was so successful and sold millions of copies around the world

05:44.000 --> 05:48.000
was that everybody who played it got a unique experience.

05:48.000 --> 05:51.000
Because the game adapted to how you were playing.

05:51.000 --> 05:55.000
It's very primitive by today's standards of learning systems.

05:55.000 --> 05:57.000
But back then it was pretty groundbreaking.

05:57.000 --> 06:05.000
It was one of the first games along with SimCity to do these kinds of complicated simulations under the hood powered by AI.

06:05.000 --> 06:09.000
As to your question about what is AI, I think AI is the science of making machines smart.

06:09.000 --> 06:15.000
And then a sub branch of AI is machine learning, which is the kinds of AI systems that learn for themselves

06:15.000 --> 06:18.000
and learn directly from data and experience.

06:18.000 --> 06:23.000
And that's really what, of course, has powered the renaissance of AI in the last 10, 15 years.

06:23.000 --> 06:25.000
And that's the sort of AI that we work on today.

06:25.000 --> 06:27.000
Can you just spend another second on that?

06:27.000 --> 06:31.000
You mentioned a minute ago that what was happening in Theme Park was logic based systems,

06:31.000 --> 06:33.000
which I think is at least one of the other branches.

06:33.000 --> 06:39.000
Can you describe the difference between some of the more logic based or rules based or knowledge encoded systems

06:39.000 --> 06:44.000
and deep learning and some of the other things that are more dominant now?

06:44.000 --> 06:45.000
Sure.

06:45.000 --> 06:50.000
So if you think of AI as being this overarching field of making machines smart,

06:50.000 --> 06:53.000
then they're broadly speaking two approaches to doing that.

06:53.000 --> 06:59.000
One is the classical approach and the sort of approach that was done for the first few decades of AI research

06:59.000 --> 07:02.000
since the 1950s with logic systems.

07:02.000 --> 07:06.000
So this is the idea of the programmers or the creators of the system.

07:06.000 --> 07:12.000
They effectively solve the problem, be that playing chess or controlling little characters in a game.

07:12.000 --> 07:15.000
And then they would program up these routines and heuristics,

07:15.000 --> 07:21.000
and then effectively the system would then deal with new inputs and execute those heuristics.

07:21.000 --> 07:23.000
And you can be pretty powerful systems.

07:23.000 --> 07:25.000
They're sometimes called expert systems.

07:25.000 --> 07:28.000
And the most famous of that is probably Deep Blue IBM's chess program.

07:28.000 --> 07:32.000
They beat Gary Kasparov, the world chess champion at the time in the 90s very famously.

07:32.000 --> 07:35.000
And that was probably the pinnacle of expert systems.

07:35.000 --> 07:39.000
But the problem with them is that they're very brittle and they can't deal with the unexpected, of course,

07:39.000 --> 07:44.000
because they can only do what the programmers have already figured out and the heuristics they've already been given.

07:44.000 --> 07:46.000
They can't learn anything new.

07:46.000 --> 07:50.000
So machine learning is the other approach to solving AI.

07:50.000 --> 07:53.000
And it's turned out to be a lot more powerful and a lot more scalable,

07:53.000 --> 07:56.000
which is what we bet on as well as when we started DeepMind.

07:56.000 --> 08:00.000
And that's the idea of machines, the systems learning for themselves,

08:00.000 --> 08:05.000
learning the structure, learning of heuristics and rules that they should do for themselves,

08:05.000 --> 08:07.000
directly from data or directly from experience.

08:07.000 --> 08:11.000
The big contrast for our first sort of big famous program was AlphaGo,

08:11.000 --> 08:15.000
which learned how to play the complex game of go for itself

08:15.000 --> 08:21.000
and actually came up with better strategies and heuristics than we could have ever thought of as the human designers.

08:21.000 --> 08:25.000
We're going to get to AlphaGo, but I want to hold for a minute in the founding of DeepMind.

08:25.000 --> 08:27.000
So you've been a game designer.

08:27.000 --> 08:31.000
You become a neuroscientist and you do actually some important peer-reviewed research

08:31.000 --> 08:34.000
and highly cited research in that field.

08:34.000 --> 08:38.000
That's a big jump. You got to work pretty hard to become a neuroscientist is my understanding.

08:38.000 --> 08:40.000
So you're an academia, you're doing this research.

08:40.000 --> 08:42.000
It's going well as best I can tell.

08:42.000 --> 08:45.000
And you start deciding you're going to found this AI company.

08:45.000 --> 08:48.000
So take me in that moment of decision making.

08:48.000 --> 08:54.000
Yeah. So again, if you have in mind that I was planning from, I guess I was around 15, 16,

08:54.000 --> 08:59.000
when I was doing this programming on these computer games and theme park and sort of 17 years old,

08:59.000 --> 09:04.000
that's already when I decided my life was going to be and my career was going to be about AI and making AI happen.

09:04.000 --> 09:08.000
So all the other things I chose were in service of that, including the PhD.

09:08.000 --> 09:10.000
So I did my undergrad in computer science.

09:10.000 --> 09:15.000
I got trained in coding, engineering and mathematical side of computer science,

09:15.000 --> 09:20.000
which I love the theoretical side, Turing machines, all of these types of things, computation theory.

09:20.000 --> 09:25.000
And then I decided for my PhD after I ran my own games company for a while,

09:25.000 --> 09:28.000
I went back to academia to do a PhD in neuroscience.

09:28.000 --> 09:32.000
And I chose cognitive neuroscience because first of all, I've always been fascinated by the brain.

09:32.000 --> 09:38.000
The brain is the only existence proof we have in the universe that general intelligence is possible.

09:38.000 --> 09:41.000
So it seems worthy of studying that specific data point.

09:41.000 --> 09:44.000
It's probably not the only way that intelligence could come about,

09:44.000 --> 09:47.000
but it's certainly the only way that we're aware of and that we can study.

09:47.000 --> 09:49.000
And of course, it's fascinating subjects in itself.

09:49.000 --> 09:53.000
But the reason I did the PhD is I wanted to learn about the brain,

09:53.000 --> 09:56.000
maybe get inspiration for some algorithmic ideas, architectural ideas.

09:56.000 --> 10:01.000
And indeed, that's what did happen in things like memory replay, reinforcement learning and things like this,

10:01.000 --> 10:03.000
that we then used in our AI systems.

10:03.000 --> 10:07.000
And also learn how to do cutting edge research too,

10:07.000 --> 10:11.000
and actually learn how to use the scientific method properly and things like control studies and so on.

10:11.000 --> 10:15.000
Really, you learn all of those practical skills by doing a PhD.

10:15.000 --> 10:18.000
You said something about founding DeepMind that I've always found striking,

10:18.000 --> 10:22.000
which is, quote, I want to understand the big questions, the really big ones,

10:22.000 --> 10:26.000
that you normally go into philosophy or physics if you're interested in.

10:26.000 --> 10:31.000
I thought building AI would be the fastest route to answer some of those questions.

10:31.000 --> 10:34.000
And so a lot of people might want to know about the nature of the universe.

10:34.000 --> 10:36.000
The idea is I'll go work on it directly.

10:36.000 --> 10:40.000
I'll get a physics PhD, a mathematics PhD, a PhD in chemistry or neuroscience,

10:40.000 --> 10:42.000
and I'll go solve those problems.

10:43.000 --> 10:51.000
And your theory of the case was I will get this training and build something else to solve those problems.

10:51.000 --> 10:54.000
Why do you think that intermediary intelligence is necessary?

10:54.000 --> 10:56.000
Why not just do it yourself?

10:56.000 --> 10:57.000
It's actually a great question.

10:57.000 --> 11:00.000
I mean, I've always been fascinated by the biggest questions.

11:00.000 --> 11:02.000
I'm not quite sure why that is,

11:02.000 --> 11:07.000
but I've always been interested in the nature of the universe, nature of reality, consciousness,

11:07.000 --> 11:09.000
the meaning of life, all of these big questions.

11:09.000 --> 11:11.000
That's what I wanted to spend my life working on.

11:11.000 --> 11:13.000
And indeed physics was my favorite subject at school.

11:13.000 --> 11:14.000
I love physics.

11:14.000 --> 11:15.000
I still love physics.

11:15.000 --> 11:21.000
I still sort of try to keep tabs on interesting areas of physics like quantum mechanics and so on.

11:21.000 --> 11:26.000
But what happened is a lot of my scientific heroes in my early years were physicists,

11:26.000 --> 11:29.000
so Richard Feynman and Stephen Weinberg, these kinds of people.

11:29.000 --> 11:30.000
But I actually read this book.

11:30.000 --> 11:35.000
It must have been in high school called Dreams of a Final Theory by Stephen Weinberg.

11:35.000 --> 11:39.000
And it's his book, and you know, Nobel Prize winner, amazing physicist of his work,

11:39.000 --> 11:43.000
on trying to come up with a unified theory of physics, you know, to unify everything together.

11:43.000 --> 11:46.000
And I remember reading this book and I was very inspiring book,

11:46.000 --> 11:51.000
but I remember concluding, wow, they hadn't actually made that much progress.

11:51.000 --> 11:54.000
And these incredible people, you know, that I had really admired, you know,

11:54.000 --> 11:57.000
you can think of all the physicists since post-World War II, right?

11:57.000 --> 12:00.000
These incredible people like Feynman and Weinberg and so on.

12:00.000 --> 12:04.000
And I just remember thinking, gosh, I wasn't that convinced they'd got that far.

12:04.000 --> 12:09.000
And then I was thinking, even in the best-case scenario that you might be able to follow their footsteps,

12:09.000 --> 12:12.000
which is a big if, given how brilliant they were,

12:12.000 --> 12:16.000
you still might not be able to make much progress on it in the whole lifetime of, you know,

12:16.000 --> 12:18.000
an amazing career like they had.

12:18.000 --> 12:20.000
And so then I started thinking, well, perhaps the problem is,

12:20.000 --> 12:25.000
is that we need a little bit of help and a little bit of extra intellectual horsepower.

12:25.000 --> 12:27.000
And where can we get that from?

12:27.000 --> 12:31.000
Well, of course, I was also simultaneously programming and doing games

12:31.000 --> 12:33.000
and falling in love with sort of computing.

12:33.000 --> 12:35.000
And that was my real passion.

12:35.000 --> 12:39.000
And I just realized that working on AI, I could satisfy both things at once.

12:39.000 --> 12:44.000
So first of all, it would open up a door to insights into what intelligence is

12:44.000 --> 12:48.000
and how the brain works and, you know, as a comparator and so on,

12:48.000 --> 12:50.000
when those are some of the biggest questions already.

12:50.000 --> 12:55.000
But it could also help with science and physics and help experts in those areas

12:55.000 --> 12:57.000
crack the problems and the questions they care about.

12:57.000 --> 13:01.000
So it seemed to be like the perfect sort of meta solution in a way.

13:01.000 --> 13:06.000
And when I made this realization sometime in high school, that's when I decided, you know,

13:06.000 --> 13:10.000
I was going to work on AI and not go directly, say, for physics

13:10.000 --> 13:13.000
and try and build the ultimate scientific tool.

13:13.000 --> 13:16.000
So let's talk about how some of this experience comes together for you

13:16.000 --> 13:19.000
because one of the early contributions of DeepMind

13:19.000 --> 13:21.000
is you're trying to build this new kind of intelligence

13:21.000 --> 13:28.000
and the way you are trying to build it, train it, test it is on games.

13:28.000 --> 13:31.000
And now it feels like a weird thing to hear,

13:31.000 --> 13:34.000
but it takes you a very long time to build a system

13:34.000 --> 13:37.000
that can score even a single point in Pong.

13:37.000 --> 13:41.000
So tell me first about the decision to begin training AIs on video games

13:41.000 --> 13:43.000
because that doesn't seem totally intuitive.

13:43.000 --> 13:46.000
You want to answer the fundamental questions of physics

13:46.000 --> 13:48.000
and you're building something to play Pong?

13:48.000 --> 13:50.000
It seems ridiculous. Why?

13:50.000 --> 13:54.000
Yes. That was one of the early decisions I think that served us very well

13:54.000 --> 13:58.000
when we started DeepMind was a lot of people who were working on AI at that time

13:58.000 --> 14:01.000
were mostly working on things like robotics

14:01.000 --> 14:03.000
and sort of embodied intelligence

14:03.000 --> 14:06.000
and that's an important branch of AI, of course,

14:06.000 --> 14:09.000
and places like MIT where I was doing my postdoc.

14:09.000 --> 14:12.000
That was a real bastion of that type of work.

14:12.000 --> 14:14.000
But what I realized was that the researchers

14:14.000 --> 14:17.000
ended up spending most of their time fiddling around with the hardware,

14:17.000 --> 14:20.000
you know, in the server motors and they'd always break

14:20.000 --> 14:23.000
and the robots are expensive and they're complicated and they're slow.

14:23.000 --> 14:27.000
And I sort of realized that it would be better to work in simulation

14:27.000 --> 14:31.000
and we could, you know, run millions of experiments in the cloud all at once

14:31.000 --> 14:35.000
and get much faster learning rates relatively cheaply and quickly

14:35.000 --> 14:37.000
and simply if we were to do that.

14:37.000 --> 14:39.000
The other advantage of games, of course,

14:39.000 --> 14:42.000
is that they've been built to be challenging to humans

14:42.000 --> 14:45.000
and you can use top human players as a great benchmark.

14:45.000 --> 14:47.000
The other thing is they have clear objectives, right?

14:47.000 --> 14:50.000
You to win the game or to maximize the score

14:50.000 --> 14:53.000
and those kinds of objectives are very useful

14:53.000 --> 14:55.000
if you want to train reinforcement learning systems

14:55.000 --> 14:59.000
which we specialize in that are reward-seeking and goal-directed,

14:59.000 --> 15:01.000
you know, so they need an objective to solve.

15:01.000 --> 15:04.000
So games are fantastic for all of those reasons.

15:04.000 --> 15:06.000
And the other cool thing about games is, of course,

15:06.000 --> 15:08.000
you can go up the ladder of complexity of games

15:08.000 --> 15:10.000
by just going through the different eras of games.

15:10.000 --> 15:13.000
We started with the 1970s with the simplest Atari games

15:13.000 --> 15:15.000
like you mentioned, like Pong,

15:15.000 --> 15:18.000
and then eventually we went all the way up over, you know,

15:18.000 --> 15:21.000
almost a decade of work to the most complex modern computer games

15:21.000 --> 15:24.000
like StarCraft, and so you can keep on increasing

15:24.000 --> 15:27.000
the difficulty of the test for your AI systems

15:27.000 --> 15:30.000
as your AI systems are getting more sophisticated.

15:30.000 --> 15:33.000
So, you know, it turned out to be a really efficient way

15:33.000 --> 15:37.000
to proving ground, really, to test out these algorithmic ideas.

15:37.000 --> 15:40.000
So I don't want to skip over too quickly what's happening here.

15:40.000 --> 15:42.000
I think when I say the sentence,

15:42.000 --> 15:44.000
oh, you built an AI that can play the game Pong,

15:44.000 --> 15:46.000
that sounds simple.

15:46.000 --> 15:48.000
But it takes you a long time to do that.

15:48.000 --> 15:50.000
It takes you a long time to score any points in Pong,

15:50.000 --> 15:53.000
and then you begin to see exponential dominance of the Pong game,

15:53.000 --> 15:57.000
which is an interesting dynamic here that I want to talk about too.

15:57.000 --> 16:00.000
But tell me what you're actually doing there.

16:00.000 --> 16:03.000
Sure. So the key thing here is,

16:03.000 --> 16:06.000
and this is the difference between what I was doing in the 90s

16:06.000 --> 16:08.000
with my early games career,

16:08.000 --> 16:10.000
where we were directly programming these expert systems.

16:10.000 --> 16:13.000
Of course, you could do that easily for something like Pong.

16:13.000 --> 16:15.000
But what the key breakthrough that we had

16:15.000 --> 16:18.000
that sort of underpinned this whole new field, really,

16:18.000 --> 16:20.000
which we call deep reinforcement learning,

16:20.000 --> 16:23.000
combining neural networks with the reward-seeking algorithms,

16:23.000 --> 16:27.000
is that we played these games directly from the pixels.

16:27.000 --> 16:30.000
So all we gave is the input to our systems.

16:30.000 --> 16:34.000
The first system we built was called DQN to play these Atari games.

16:34.000 --> 16:36.000
What did DQN stand for?

16:36.000 --> 16:38.000
DQ network. So QQ...

16:38.000 --> 16:40.000
I was hoping that was going to be more fun.

16:40.000 --> 16:42.000
No, no, it was a very technical name.

16:42.000 --> 16:45.000
We had one at the beginning before we got better at naming things.

16:45.000 --> 16:47.000
It just refers to the technique that we used.

16:47.000 --> 16:50.000
And so the big innovation and the big breakthrough

16:50.000 --> 16:52.000
was to use just the raw inputs,

16:52.000 --> 16:54.000
in this case the pixels on the screen,

16:54.000 --> 16:56.000
30,000 pixels roughly on the screen,

16:56.000 --> 17:00.000
and not tell the system anything about the game.

17:00.000 --> 17:03.000
Not what it was controlling, not what would get score,

17:03.000 --> 17:05.000
not how it loses points or loses a life.

17:05.000 --> 17:08.000
Any of those things, it had to figure out for itself

17:08.000 --> 17:10.000
from first principles by playing the game,

17:10.000 --> 17:12.000
just like a human would learn.

17:12.000 --> 17:14.000
That was the key thing.

17:14.000 --> 17:16.000
So it learned for itself, and the second thing that was key

17:16.000 --> 17:18.000
was building these general systems.

17:18.000 --> 17:20.000
And that's what the general is in AGI,

17:20.000 --> 17:22.000
and I'm sure we'll come back to that,

17:22.000 --> 17:26.000
is that one single system that can play any of the Atari games,

17:26.000 --> 17:28.000
and the same system out of the box

17:28.000 --> 17:31.000
can play all of them to sort of superhuman level,

17:31.000 --> 17:33.000
like world record scores.

17:33.000 --> 17:36.000
And so those two elements, the generality and the learning,

17:36.000 --> 17:38.000
are the key differences.

17:38.000 --> 17:40.000
So I want to spend just a moment here

17:40.000 --> 17:42.000
on this expert system versus deep learning.

17:42.000 --> 17:45.000
So if you've been building an expert system, a logic system,

17:45.000 --> 17:49.000
you're trying to tell the system how Pong works.

17:49.000 --> 17:51.000
It's like, okay, there are these two paddles,

17:51.000 --> 17:54.000
and there's a ball, and what you want to do is get points,

17:54.000 --> 17:57.000
and you're basically trying to break the game of Pong down

17:57.000 --> 18:00.000
into rules, encode the rules into the system,

18:00.000 --> 18:03.000
and you figure out if you've found all the rules

18:03.000 --> 18:05.000
by how well the system does.

18:05.000 --> 18:08.000
Versus deep learning, where are there any rules there?

18:08.000 --> 18:10.000
Are you just telling the system,

18:10.000 --> 18:12.000
if you get a point that's good,

18:12.000 --> 18:14.000
experiment until you do,

18:14.000 --> 18:18.000
and it gets basically the digital equivalent of a doggy treat

18:18.000 --> 18:20.000
every time it does well,

18:20.000 --> 18:22.000
and then tries to do more of that well.

18:22.000 --> 18:23.000
That's right.

18:23.000 --> 18:24.000
So that's exactly the difference.

18:24.000 --> 18:27.000
So expert systems, you as the human programmers

18:27.000 --> 18:31.000
and designers are trying to break down this complex problem,

18:31.000 --> 18:34.000
be that playing chess or playing an Atari game,

18:34.000 --> 18:36.000
into the set of heuristics and rules.

18:36.000 --> 18:39.000
So sometimes there are rules, but they could be probabilistic,

18:39.000 --> 18:41.000
so there could just be heuristics,

18:41.000 --> 18:44.000
and that's what the system then uses to try and make decisions.

18:44.000 --> 18:47.000
So it's effectively using what it's been given.

18:47.000 --> 18:50.000
But then with machine learning, using techniques like deep learning,

18:50.000 --> 18:52.000
type of machine learning or reinforcement learning,

18:52.000 --> 18:54.000
which is the doggy treat thing that you mentioned.

18:54.000 --> 18:56.000
So you get a point, so it gets a reward,

18:56.000 --> 18:59.000
and then it's more likely to do those actions again.

18:59.000 --> 19:02.000
Those things learn for themselves.

19:02.000 --> 19:04.000
So you just give it the high-level objective.

19:04.000 --> 19:07.000
You say win a game, get a certain number of points,

19:07.000 --> 19:10.000
and then it has to figure it out effectively

19:10.000 --> 19:12.000
what those heuristics are for itself.

19:12.000 --> 19:14.000
And so we're going to talk more about this,

19:14.000 --> 19:18.000
but it's in this divergence where, as I understand it,

19:18.000 --> 19:20.000
the question of alignment problems really begins

19:20.000 --> 19:23.000
to creep into the industry.

19:23.000 --> 19:25.000
Because if I'm encoding all these rules,

19:25.000 --> 19:27.000
if I'm encoding my understanding into a computer,

19:27.000 --> 19:29.000
then I might miss things,

19:29.000 --> 19:31.000
but the computer is basically running off

19:31.000 --> 19:33.000
of what I've told it to do.

19:33.000 --> 19:35.000
Whereas if I've told it to get points,

19:35.000 --> 19:37.000
and it doesn't even know what it's doing

19:37.000 --> 19:38.000
except for getting points.

19:38.000 --> 19:40.000
I mean, as you say, it's learning from pixels.

19:40.000 --> 19:42.000
It doesn't have an understanding of the game Pong.

19:42.000 --> 19:44.000
And ultimately, it's winning points,

19:44.000 --> 19:46.000
but it still may not know it's playing Pong, right?

19:46.000 --> 19:48.000
I've never told it it's playing Pong.

19:48.000 --> 19:53.000
It's not working with that kind of generalized sense of the situation.

19:53.000 --> 19:54.000
It might get points,

19:54.000 --> 19:56.000
but if it can figure out another way to get points,

19:56.000 --> 19:57.000
it'll do that too.

19:57.000 --> 20:01.000
Let's talk a bit about what it means in terms of

20:01.000 --> 20:05.000
what the system is doing versus what we often think it's doing

20:05.000 --> 20:08.000
that comes from when it is doing the learning itself.

20:08.000 --> 20:11.000
You know, this is a very interesting question of

20:11.000 --> 20:13.000
when you do the learning itself,

20:13.000 --> 20:15.000
you can guide that learning by giving it

20:15.000 --> 20:17.000
certain types of data to learn from.

20:17.000 --> 20:19.000
You can set the high level objectives.

20:19.000 --> 20:21.000
You could even set sub-objectives as well

20:21.000 --> 20:23.000
to kind of guide it on the way.

20:23.000 --> 20:24.000
How do you win a game?

20:24.000 --> 20:27.000
Well, you've got to get a certain number of points

20:27.000 --> 20:28.000
or something like that.

20:28.000 --> 20:31.000
So you can sort of give it some sub-objectives

20:31.000 --> 20:33.000
or it can discover that for itself.

20:33.000 --> 20:36.000
But really, you're sort of more coaxing the system

20:36.000 --> 20:38.000
rather than with the logic systems

20:38.000 --> 20:40.000
where you're directly programming that in.

20:40.000 --> 20:43.000
So you have a little bit less direct control over that.

20:43.000 --> 20:45.000
And that, of course, is then linked to

20:45.000 --> 20:46.000
what you mentioned, the alignment problem,

20:46.000 --> 20:48.000
which is sometimes you may care

20:48.000 --> 20:50.000
not just about the overall outcome,

20:50.000 --> 20:52.000
you may care about how it got there, right?

20:52.000 --> 20:55.000
And if you do, then it matters the way that it solves it.

20:55.000 --> 20:58.000
And so then you're kind of leaving the solution

20:58.000 --> 21:00.000
or the type of solution or the approach

21:00.000 --> 21:02.000
up to the system itself.

21:02.000 --> 21:04.000
And what you're caring about is the objective at the end.

21:04.000 --> 21:06.000
So if you care about the approach too,

21:06.000 --> 21:09.000
then you have to add extra constraints into the system

21:09.000 --> 21:12.000
or give it some feedback about the approach too,

21:12.000 --> 21:14.000
which is this sort of reinforcement learning

21:14.000 --> 21:16.000
with human feedback that was now in vogue,

21:16.000 --> 21:20.000
where you can get that feedback directly from human ratings

21:20.000 --> 21:22.000
or you can do it another way.

21:22.000 --> 21:24.000
You can give it sort of further sub-objectives

21:24.000 --> 21:28.000
that help it guide it as to what type of solution you want.

21:28.000 --> 21:30.000
Well, we've talked a bit here about deep learning,

21:30.000 --> 21:32.000
but can you describe reinforcement learning?

21:32.000 --> 21:34.000
What is it? How does it differ from deep learning?

21:34.000 --> 21:36.000
How do they work together?

21:36.000 --> 21:38.000
So both deep learning and reinforcement learning

21:38.000 --> 21:40.000
are types of machine learning,

21:40.000 --> 21:41.000
and they're very complementary.

21:41.000 --> 21:42.000
And we specialize in both

21:42.000 --> 21:44.000
and have done from the start of DeepMind.

21:44.000 --> 21:47.000
So deep learning is on your hierarchical neural networks,

21:47.000 --> 21:50.000
really complex stacks of neural networks,

21:50.000 --> 21:53.000
very loosely modeled on brain neural networks.

21:53.000 --> 21:55.000
And the objective of those neural networks

21:55.000 --> 21:58.000
is to learn the statistics of the environment

21:58.000 --> 22:00.000
that they find themselves in

22:00.000 --> 22:02.000
or the data stream that they're given.

22:02.000 --> 22:03.000
So you can think of the neural network,

22:03.000 --> 22:06.000
the deep learning as building a model of the situation.

22:06.000 --> 22:08.000
And then the reinforcement learning part

22:08.000 --> 22:12.000
is the bit which does the planning and the reward learning.

22:12.000 --> 22:16.000
So effectively, it's kind of like a reward-seeking system

22:16.000 --> 22:19.000
that is trying to solve an objective that you give it.

22:19.000 --> 22:23.000
So for example, we'll be in a game trying to maximize the score.

22:23.000 --> 22:26.000
So for every point it gets, it's sort of like a little reward.

22:26.000 --> 22:29.000
Animals and including humans learn with reinforcement learning.

22:29.000 --> 22:31.000
It's one of the types of learnings we do.

22:31.000 --> 22:33.000
And you see that really well with, as you mentioned earlier,

22:33.000 --> 22:35.000
with dogs and you give them a treat

22:35.000 --> 22:37.000
if they do something that you like or they're well-behaved.

22:37.000 --> 22:40.000
And then they're more likely to do that again in future.

22:40.000 --> 22:43.000
And that's exactly very similar in a digital form

22:43.000 --> 22:46.000
with these reinforcement learning systems that we build.

22:46.000 --> 22:48.000
Effectively, they get treats, a reward,

22:48.000 --> 22:51.000
when they achieve a certain sub-objective.

22:51.000 --> 22:53.000
And so the cool thing is, is that you can combine

22:53.000 --> 22:55.000
these two types of systems together,

22:55.000 --> 22:57.000
and that's called deep reinforcement learning,

22:57.000 --> 23:00.000
where you have the model, and then you also have this

23:00.000 --> 23:03.000
goal-seeking or reward-seeking planning system on top

23:03.000 --> 23:07.000
that uses that model to reach its objectives.

23:07.000 --> 23:09.000
So I want to give ahead a bit now to the system

23:09.000 --> 23:12.000
that most people at least used to know DeepMind for.

23:12.000 --> 23:14.000
And that's AlphaGo.

23:14.000 --> 23:16.000
So tell me what AlphaGo is,

23:16.000 --> 23:19.000
how it differs from the Pong system we're talking about here,

23:19.000 --> 23:23.000
why Go was an important benchmark or milestone

23:23.000 --> 23:25.000
to try to topple?

23:25.000 --> 23:28.000
Yeah, so in effect, AlphaGo was the extension

23:28.000 --> 23:31.000
of the work we'd done in Atari,

23:31.000 --> 23:33.000
but sort of the pinnacle, really,

23:33.000 --> 23:35.000
of what you could achieve in games AI.

23:35.000 --> 23:38.000
So Deep Blue, as I said earlier, beat Gary Kaspar

23:38.000 --> 23:40.000
for chess in the 90s.

23:40.000 --> 23:43.000
He won big first pinnacle in games AI,

23:43.000 --> 23:45.000
but the next sort of Mount Everest, if you like,

23:45.000 --> 23:47.000
was beating the world champion at Go.

23:47.000 --> 23:49.000
And you couldn't use expert systems,

23:49.000 --> 23:51.000
you had to use these learning systems,

23:51.000 --> 23:54.000
because we as even as human, the best human Go players,

23:54.000 --> 23:56.000
they don't understand the game well enough

23:56.000 --> 23:59.000
to break it down into these heuristics and sub-problems.

23:59.000 --> 24:01.000
It's also partly to do with the nature of Go

24:01.000 --> 24:04.000
as a very aesthetic game, it's a very intuitive game

24:04.000 --> 24:06.000
and with a lot of patterns.

24:06.000 --> 24:08.000
So it's quite different from chess,

24:08.000 --> 24:10.000
even top Go players will tell you

24:10.000 --> 24:12.000
why did they play a certain move,

24:12.000 --> 24:14.000
they'll say it just felt right.

24:14.000 --> 24:16.000
So they're kind of using their intuition,

24:16.000 --> 24:18.000
which is not something that we often associate

24:18.000 --> 24:20.000
with computer programs, right?

24:20.000 --> 24:23.000
Being able to do something that mimics intuition.

24:23.000 --> 24:26.000
And so with AlphaGo, what we did is we built a neural network

24:26.000 --> 24:28.000
that modeled the game of Go,

24:28.000 --> 24:31.000
figured out what were good moves in certain positions,

24:31.000 --> 24:33.000
who was likely to win from a certain position,

24:33.000 --> 24:35.000
the probability of either side winning,

24:35.000 --> 24:37.000
and that's what the neural network was predicting.

24:37.000 --> 24:40.000
And then on top of that, we overlaid this reinforcement learning system

24:40.000 --> 24:44.000
that would make plans, do Monte Carlo tree search,

24:44.000 --> 24:46.000
using the model to guide its search

24:46.000 --> 24:49.000
so that it didn't have to search millions and millions of moves

24:49.000 --> 24:51.000
and it would be intractable because it goes too complicated

24:51.000 --> 24:54.000
to search everything, you have to narrow down your search.

24:54.000 --> 24:57.000
And so it used the model to search the most fruitful paths

24:57.000 --> 24:59.000
and then try and find the best move

24:59.000 --> 25:02.000
that would most likely get it to a winning position.

25:02.000 --> 25:05.000
And so it was a kind of culmination

25:05.000 --> 25:07.000
of five, six years worth of working,

25:07.000 --> 25:09.000
our work in deep reinforcement learning.

25:09.000 --> 25:12.000
What I always found striking about that story

25:12.000 --> 25:15.000
is so you beat one of the Go World champions,

25:15.000 --> 25:17.000
it's this big moment,

25:17.000 --> 25:20.000
and then shortly thereafter you beat yourself,

25:20.000 --> 25:22.000
which is to say you have AlphaGo,

25:22.000 --> 25:25.000
which is the system that beats Elisa Dole,

25:25.000 --> 25:27.000
and then you create AlphaZero,

25:27.000 --> 25:30.000
a system that just stomps AlphaGo.

25:30.000 --> 25:33.000
So what was the difference between AlphaGo and AlphaZero

25:33.000 --> 25:36.000
and what was to be learned there?

25:36.000 --> 25:39.000
So AlphaGo was a pretty general system

25:39.000 --> 25:42.000
in that it learned for itself the motifs

25:42.000 --> 25:44.000
and the strategies around Go,

25:44.000 --> 25:46.000
and in fact it created new strategies

25:46.000 --> 25:48.000
very famously in that in the World Championship match

25:48.000 --> 25:50.000
that had never been seen before,

25:50.000 --> 25:53.000
even though we've played Go for thousands of years now

25:53.000 --> 25:55.000
to a couple of thousand years.

25:55.000 --> 25:57.000
So that was pretty incredible to see.

25:57.000 --> 26:00.000
The first version of AlphaGo actually was bootstrapped

26:00.000 --> 26:02.000
by looking at all human games

26:02.000 --> 26:04.000
that had been played on the Internet,

26:04.000 --> 26:06.000
and there's a lot of Internet Go servers

26:06.000 --> 26:09.000
in a very popular career in Japan and China.

26:09.000 --> 26:11.000
So it's using human games as training data?

26:11.000 --> 26:13.000
It used human data as training data.

26:13.000 --> 26:15.000
It also had specific things,

26:15.000 --> 26:18.000
knowledge about Go encoded in it

26:18.000 --> 26:20.000
to do with the symmetry of the board

26:20.000 --> 26:22.000
and some other things that were specific to Go.

26:22.000 --> 26:24.000
So it was a pretty general system,

26:24.000 --> 26:26.000
but it was specialized around Go data,

26:26.000 --> 26:28.000
the human data that it learned from,

26:28.000 --> 26:30.000
and there were some specific things about Go.

26:30.000 --> 26:32.000
Now, once we beat the World Champion, Lisa Doll,

26:32.000 --> 26:34.000
we then, this is quite common for us,

26:34.000 --> 26:36.000
is once we do that we always try

26:36.000 --> 26:38.000
and look back at our systems, in this case AlphaGo,

26:38.000 --> 26:40.000
and we try and remove anything

26:40.000 --> 26:42.000
that was specific to that task

26:42.000 --> 26:44.000
to make it more and more general.

26:44.000 --> 26:46.000
And so that's what AlphaZero was.

26:46.000 --> 26:48.000
AlphaZero was the next version,

26:48.000 --> 26:51.000
and that is able to play any two-player game,

26:51.000 --> 26:53.000
so it doesn't matter whether it's Go or Chess

26:53.000 --> 26:56.000
or Backgammon, any game you can put it in there,

26:56.000 --> 26:58.000
or Japanese chess to be called Shogi,

26:58.000 --> 27:00.000
any two-player game.

27:00.000 --> 27:02.000
And it doesn't need any human data either,

27:02.000 --> 27:05.000
because what it does, it starts off completely random.

27:05.000 --> 27:08.000
So imagine a blank slate neural network

27:08.000 --> 27:11.000
doesn't really know anything about the game or strategies,

27:11.000 --> 27:13.000
and what it does is it plays itself

27:13.000 --> 27:15.000
millions and millions of times,

27:15.000 --> 27:17.000
different versions of itself,

27:17.000 --> 27:20.000
and it learns from its own data and its own experience,

27:20.000 --> 27:23.000
and then eventually it becomes actually stronger

27:23.000 --> 27:26.000
than these individual programs like AlphaGo

27:26.000 --> 27:28.000
that were trained on human data,

27:28.000 --> 27:30.000
and it doesn't require any human data.

27:30.000 --> 27:32.000
It starts literally from random

27:32.000 --> 27:35.000
and explores the space of Go or Chess for itself.

27:35.000 --> 27:37.000
And of course it means it also comes up

27:37.000 --> 27:39.000
with incredible new strategies,

27:39.000 --> 27:43.000
not constrained by what humans have played in the past,

27:43.000 --> 27:45.000
because it doesn't have any knowledge of that.

27:45.000 --> 27:47.000
One of the things I think about,

27:47.000 --> 27:49.000
and that we will talk about later in this conversation,

27:49.000 --> 27:52.000
is the question of how smart something

27:52.000 --> 27:54.000
that is working with our data

27:54.000 --> 27:56.000
and working with the world as we understand it

27:56.000 --> 27:59.000
can really become, if that tops out somewhere.

27:59.000 --> 28:01.000
And one version of saying,

28:01.000 --> 28:03.000
not that smart might be to say,

28:03.000 --> 28:05.000
well, it's kind of constrained by what we know.

28:05.000 --> 28:07.000
It's got to work with what we know,

28:07.000 --> 28:09.000
and if we haven't done that much on something,

28:09.000 --> 28:11.000
well, it can't do that much on something in most cases.

28:11.000 --> 28:14.000
But this is a case where, with some very basic rules,

28:14.000 --> 28:16.000
it actually turned out that it was being held back

28:16.000 --> 28:18.000
by what we knew.

28:18.000 --> 28:21.000
That because human beings are prone to fattishness,

28:21.000 --> 28:24.000
because we follow in the footsteps of those who came before us,

28:24.000 --> 28:26.000
because we're taught by others,

28:26.000 --> 28:28.000
and they tell us, no, that would be a crazy move.

28:28.000 --> 28:31.000
You know, everybody who's done it before did it this way.

28:31.000 --> 28:33.000
I mean, that does help us get better, right?

28:33.000 --> 28:37.000
Cultural learning and evolution is the core of our species advancement.

28:37.000 --> 28:41.000
But it also turns out that that means huge swaths

28:41.000 --> 28:44.000
of useful strategy, information, ideas

28:44.000 --> 28:48.000
have been cut off the board because we just don't do that.

28:48.000 --> 28:52.000
And so in terms of reasons to think these systems

28:52.000 --> 28:56.000
could really be remarkable from our perspective,

28:56.000 --> 28:59.000
the fact that it was being encumbered

28:59.000 --> 29:01.000
by everything we knew about Go,

29:01.000 --> 29:05.000
as opposed to launched forward by everything we knew about Go,

29:05.000 --> 29:09.000
to maybe put less sentiment on the word here,

29:09.000 --> 29:11.000
just strikes me as profound.

29:11.000 --> 29:13.000
Yeah, look, it's an interesting take.

29:13.000 --> 29:17.000
I mean, of course, this is what happens with our cultural civilization,

29:17.000 --> 29:20.000
is that we do get into local maximas, one could say,

29:20.000 --> 29:23.000
in something relatively prescribed like a game.

29:23.000 --> 29:26.000
In fact, I talked to the Go experts after AlphaGo

29:26.000 --> 29:29.000
made these new strategies, most famously Move 37

29:29.000 --> 29:31.000
on Game 2 of the World Championship match,

29:31.000 --> 29:33.000
with this astounding new move,

29:33.000 --> 29:36.000
and I asked all the Go experts afterwards about that move.

29:36.000 --> 29:39.000
And they said, we just told that that's a bad move to make

29:39.000 --> 29:41.000
in that early in the game,

29:41.000 --> 29:43.000
but they can't really explain why.

29:43.000 --> 29:46.000
They just said their teachers would just basically shout at them.

29:46.000 --> 29:49.000
It's interesting that that is a cultural norm then

29:49.000 --> 29:52.000
that actually limits our creativity and exploration.

29:52.000 --> 29:55.000
So what I'm hoping is these systems will expand our own minds,

29:55.000 --> 29:58.000
and I think this is actually what's happening Go and in chess,

29:58.000 --> 30:00.000
again, in very prescribed areas,

30:00.000 --> 30:03.000
but where people have started being more creative themselves.

30:03.000 --> 30:04.000
Oh, actually, we don't.

30:04.000 --> 30:06.000
Maybe we should question some of those cultural norms

30:06.000 --> 30:08.000
and perhaps we would get further.

30:08.000 --> 30:11.000
I believe that's what has happened in Go and other things.

30:11.000 --> 30:15.000
Now, the cool thing is coming back to using AI systems for science.

30:15.000 --> 30:18.000
If you think about that, then science is the pursuit of new knowledge

30:18.000 --> 30:20.000
and understanding.

30:20.000 --> 30:23.000
And so you can now see, I think, of course, in games,

30:23.000 --> 30:25.000
it's just games, and they're fun,

30:25.000 --> 30:28.000
and I love games to death and my huge passion of mine,

30:28.000 --> 30:30.000
but in the end, it's just a game.

30:30.000 --> 30:33.000
But for science, you're actually discovering a medicine,

30:33.000 --> 30:35.000
you're discovering important new knowledge.

30:35.000 --> 30:37.000
There's no reason to assume that isn't going on

30:37.000 --> 30:40.000
in another cultural activity, in this case, science.

30:40.000 --> 30:43.000
And I think these tools could in themselves

30:43.000 --> 30:46.000
help us discover new area regions of knowledge,

30:46.000 --> 30:51.000
but also inspire the human experts to explore more as well in tandem.

31:14.000 --> 31:17.000
Let's shift into science,

31:17.000 --> 31:22.000
because this is where DeepMind begins creating something

31:22.000 --> 31:26.000
that isn't just winning games, but is actually creating an advance.

31:26.000 --> 31:28.000
And I've said before on this show many times

31:28.000 --> 31:31.000
that of all the AI systems that have been released,

31:31.000 --> 31:34.000
the one I've always been most impressed by and interested in is AlphaFold,

31:34.000 --> 31:37.000
which is a DeepMind system.

31:37.000 --> 31:40.000
So tell me what AlphaFold is,

31:40.000 --> 31:43.000
what the problem is, how you come to decide that that's something

31:43.000 --> 31:45.000
that your systems can take on.

31:45.000 --> 31:48.000
I mean, you're doing games, and then you move to this.

31:48.000 --> 31:49.000
What is AlphaFold?

31:49.000 --> 31:52.000
So we were doing games as the testing ground,

31:52.000 --> 31:55.000
but we always had in mind, and I always had in mind

31:55.000 --> 31:58.000
the very thing I was thinking about as a teenager,

31:58.000 --> 32:00.000
of using AI as a tool for science.

32:00.000 --> 32:02.000
And once we'd mastered a lot of games,

32:02.000 --> 32:05.000
the idea was that these systems would be powerful enough

32:05.000 --> 32:07.000
and sophisticated enough we could turn them onto

32:07.000 --> 32:10.000
very important real-world problems and real-world challenges,

32:10.000 --> 32:12.000
especially in the sciences.

32:12.000 --> 32:15.000
So AlphaFold, we pretty much started the day after we got back

32:15.000 --> 32:18.000
from Korea in 2016 and the Lisa Dolmatch,

32:18.000 --> 32:21.000
and that was our next big grand challenge.

32:21.000 --> 32:24.000
And AlphaFold is our program to try and solve the problem

32:24.000 --> 32:26.000
of protein folding, as it's known.

32:26.000 --> 32:29.000
Proteins are the workhorses of biology.

32:29.000 --> 32:32.000
Basically, every biological function in the body

32:32.000 --> 32:34.000
is mediated by proteins.

32:34.000 --> 32:37.000
Proteins are described by their amino acid sequence,

32:37.000 --> 32:40.000
so you can think of it loosely as the genetic sequence

32:40.000 --> 32:44.000
for a protein, and that's a kind of one-dimensional string

32:44.000 --> 32:46.000
of letters you can think of.

32:46.000 --> 32:49.000
But in the body, they scrunch up into a 3D shape

32:49.000 --> 32:53.000
very, very quickly, and it's the 3D shape of the protein

32:53.000 --> 32:56.000
that governs its function, what it does in the body.

32:56.000 --> 32:58.000
And so the protein folding problem, in essence,

32:58.000 --> 33:01.000
is can you predict the 3D shape of the protein

33:01.000 --> 33:05.000
directly from the amino acid sequence?

33:05.000 --> 33:08.000
The reason it's so important is that a lot of disease

33:08.000 --> 33:11.000
is caused by proteins misfolding or folding wrong,

33:11.000 --> 33:14.000
and also if you want to design drugs to combat diseases,

33:14.000 --> 33:17.000
you need to know what the surface of the protein,

33:17.000 --> 33:19.000
therefore the shape of the protein is,

33:19.000 --> 33:21.000
so you know which parts of the protein to target

33:21.000 --> 33:23.000
with your drug compound.

33:23.000 --> 33:27.000
So it's hugely important for many, many biological research questions.

33:27.000 --> 33:30.000
To just give an example of this, people may have heard over time

33:30.000 --> 33:33.000
that coronavirus is a spike protein,

33:33.000 --> 33:36.000
and that's not just an aesthetic point.

33:36.000 --> 33:41.000
The fact that it has this somewhat spiked folding structure

33:41.000 --> 33:43.000
is crucial to the way it actually works.

33:43.000 --> 33:45.000
Do you want to just maybe use the coronavirus protein

33:45.000 --> 33:47.000
as an example of what you're saying?

33:47.000 --> 33:49.000
Yeah, so that's a great example,

33:49.000 --> 33:52.000
and we worked on that as well with the alpha-fold system.

33:52.000 --> 33:54.000
So yes, the spike protein is a thing,

33:54.000 --> 33:57.000
in a sense, that sticks out of the virus.

33:57.000 --> 34:00.000
So that's what you want to latch on to

34:00.000 --> 34:03.000
with a vaccine or a drug to kind of block its function,

34:03.000 --> 34:06.000
so it doesn't attach to the body or the body's cells

34:06.000 --> 34:08.000
in a certain sort of way.

34:08.000 --> 34:11.000
So it's the protein structures that do all the mechanics of that.

34:11.000 --> 34:14.000
So if you understand what the protein structure looks like,

34:14.000 --> 34:16.000
that spike looks like, the shape of it,

34:16.000 --> 34:19.000
you can design something that fits like a glove around it,

34:19.000 --> 34:21.000
right, to block its action.

34:21.000 --> 34:23.000
So that's a great example of, you know,

34:23.000 --> 34:25.000
the criticality of protein structure.

34:25.000 --> 34:32.000
And what made you think that protein folding is like games?

34:32.000 --> 34:34.000
What is the analogy you're drawing here?

34:34.000 --> 34:36.000
When you say, I come back from doing Go,

34:36.000 --> 34:39.000
and I decided to work on protein folding,

34:39.000 --> 34:41.000
what are you seeing here?

34:41.000 --> 34:43.000
Because I would not naturally see a connection

34:43.000 --> 34:45.000
between those two questions.

34:45.000 --> 34:47.000
No, it seems quite far apart,

34:47.000 --> 34:49.000
but actually it depends on if you step back

34:49.000 --> 34:51.000
and look at it sort of meta-level,

34:51.000 --> 34:53.000
they have a lot of things in common.

34:53.000 --> 34:56.000
By the way, protein folding is one of a number of scientific problems,

34:56.000 --> 34:58.000
big sort of grand challenges,

34:58.000 --> 35:00.000
that I came across in my career,

35:00.000 --> 35:03.000
actually protein folding I came across in the 90s in my undergrad,

35:03.000 --> 35:05.000
because I had a lot of biologist friends

35:05.000 --> 35:07.000
who were obsessed about this at Cambridge

35:07.000 --> 35:10.000
and actually went on to do their whole careers on protein structures.

35:10.000 --> 35:12.000
And they explained to me the problem

35:12.000 --> 35:14.000
and I thought it was fascinating,

35:14.000 --> 35:16.000
and I also thought it was a perfect problem

35:16.000 --> 35:18.000
for AI one day to help with.

35:18.000 --> 35:20.000
So I kind of filed it away,

35:20.000 --> 35:23.000
and I always later bought it out of the filing system in a sense

35:23.000 --> 35:26.000
and decided that that was the first big grand challenge

35:26.000 --> 35:29.000
we would apply our learning systems to.

35:29.000 --> 35:31.000
The reason I think it's similar,

35:31.000 --> 35:33.000
that sort of later on, actually while I was doing my postdoc,

35:33.000 --> 35:35.000
the second time I came across protein folding was,

35:35.000 --> 35:37.000
you know, in the late 2000s,

35:37.000 --> 35:39.000
where there was this game called Fold It,

35:39.000 --> 35:41.000
it's called a citizen science game,

35:41.000 --> 35:43.000
you may have come across it,

35:43.000 --> 35:45.000
so basically a lab had created a game, a puzzle game,

35:45.000 --> 35:48.000
where it involved people folding proteins

35:48.000 --> 35:50.000
in a three-dimensional interface.

35:50.000 --> 35:52.000
I don't think it was a very fun game,

35:52.000 --> 35:55.000
but they made it into this sort of quite user-friendly interface,

35:55.000 --> 35:58.000
and a few tens of thousands of amateur games players

35:58.000 --> 36:00.000
got quite obsessed with it.

36:00.000 --> 36:02.000
It got released, I think, in 2008, 2009,

36:02.000 --> 36:04.000
and I remember looking into this and thinking,

36:04.000 --> 36:06.000
wow, this is pretty fascinating

36:06.000 --> 36:09.000
if you can get people to do science by playing a game.

36:09.000 --> 36:11.000
That seems like a great idea,

36:11.000 --> 36:14.000
so my game's designer part of me was fascinated by it.

36:14.000 --> 36:16.000
And so what happened when I looked into it

36:16.000 --> 36:18.000
was some of these gamers, who, by the way,

36:18.000 --> 36:20.000
a lot of them knew nothing about biology, right,

36:20.000 --> 36:22.000
they were just gamers, they'd figured out,

36:22.000 --> 36:24.000
presumably, with their pattern matching of their brain,

36:24.000 --> 36:27.000
their intuition, that certain counter-intuitive folds

36:27.000 --> 36:29.000
of this string of amino acid sequences,

36:29.000 --> 36:31.000
you know, the backbone of the protein,

36:31.000 --> 36:34.000
led it to the right kind of 3D structure.

36:34.000 --> 36:36.000
And they're counter-intuitive in that

36:36.000 --> 36:38.000
if you just do the fold that gets you

36:38.000 --> 36:40.000
locally to the lowest energy state,

36:40.000 --> 36:43.000
which is a kind of greedy search strategy,

36:43.000 --> 36:45.000
you end up with the wrong protein fold,

36:45.000 --> 36:46.000
you have to do that.

36:46.000 --> 36:48.000
So sometimes you have to do local moves,

36:48.000 --> 36:50.000
local bends of the protein

36:50.000 --> 36:52.000
that actually make the energy landscape worse,

36:52.000 --> 36:55.000
effectively the efficiency of the protein structure worse.

36:55.000 --> 36:57.000
And then eventually you resolve that.

36:57.000 --> 37:00.000
And I remember thinking, combining that

37:00.000 --> 37:02.000
with what we then did with AlphaGo,

37:02.000 --> 37:04.000
where in AlphaGo, what had we done?

37:04.000 --> 37:06.000
Well, what we'd managed with AlphaGo and achieved with AlphaGo

37:06.000 --> 37:09.000
is we'd managed to mimic the intuition

37:09.000 --> 37:11.000
of these incredible Go masters.

37:11.000 --> 37:13.000
So I was thinking, wow, if that was the case

37:13.000 --> 37:15.000
for professionals, you spend their whole life on it.

37:15.000 --> 37:17.000
And then these amateur gamers,

37:17.000 --> 37:19.000
who didn't know anything about biology,

37:19.000 --> 37:21.000
were able to, in a couple of cases,

37:21.000 --> 37:23.000
fold a couple of proteins correctly.

37:23.000 --> 37:25.000
Then why wouldn't we be able to mimic

37:25.000 --> 37:26.000
whatever was going on in that?

37:26.000 --> 37:28.000
Those amateur gamers' intuition.

37:28.000 --> 37:30.000
So that gave me some hope, additional hope,

37:30.000 --> 37:32.000
that this would be possible somehow.

37:32.000 --> 37:34.000
And of course, this idea of a game,

37:34.000 --> 37:36.000
protein folding being a puzzle game,

37:36.000 --> 37:38.000
was pretty interesting as an analogy as well.

37:38.000 --> 37:40.000
So for several reasons,

37:40.000 --> 37:42.000
and also the fact it was a grand challenge

37:42.000 --> 37:45.000
and had so many downstream implications

37:45.000 --> 37:47.000
and impact if we were to solve it,

37:47.000 --> 37:49.000
all those factors sort of came together

37:49.000 --> 37:51.000
for me to choose that as the next project.

37:51.000 --> 37:54.000
That brings up, I think, another important part of all this.

37:54.000 --> 37:57.000
So when you think about how are you rewarding the system,

37:57.000 --> 38:00.000
how are you reinforcing the system in Go,

38:00.000 --> 38:03.000
well, Go has rules and you know how you score points

38:03.000 --> 38:05.000
and you know how you win a game.

38:05.000 --> 38:08.000
But when you're predicting the structure

38:08.000 --> 38:13.000
of a heretofore-unpredicted protein,

38:13.000 --> 38:15.000
how do you know if you're right?

38:15.000 --> 38:17.000
How did the gamers know if they were right?

38:17.000 --> 38:19.000
How does the system know if it is right?

38:19.000 --> 38:21.000
What are you reinforcing against?

38:21.000 --> 38:23.000
That was one of the hardest things

38:23.000 --> 38:25.000
and often that's one of the hardest things

38:25.000 --> 38:27.000
with learning systems and machine learning systems

38:27.000 --> 38:29.000
is actually formulating the right objectives.

38:29.000 --> 38:32.000
You can think of it as asking the system the right question.

38:32.000 --> 38:34.000
And how do you formulate what you want

38:34.000 --> 38:38.000
in terms of a simple to optimize objective function?

38:38.000 --> 38:40.000
And you're absolutely right, in the real world

38:40.000 --> 38:42.000
you don't have simple things like scores

38:42.000 --> 38:44.000
or winning conditions, right?

38:44.000 --> 38:45.000
That's obviously in games.

38:45.000 --> 38:47.000
But with proteins and biology,

38:47.000 --> 38:49.000
a lot of the cases there are good proxies for it,

38:49.000 --> 38:52.000
like minimizing the energy in the system.

38:52.000 --> 38:55.000
Most natural systems try to be energy efficient,

38:55.000 --> 38:59.000
so you can sort of follow a gradient of the energy gradient

38:59.000 --> 39:02.000
or the free energy in the system and try to minimize that.

39:02.000 --> 39:03.000
So that's one thing.

39:03.000 --> 39:05.000
The other thing is protein folding.

39:05.000 --> 39:07.000
There is a whole history, 50-year history,

39:07.000 --> 39:11.000
or more actually, of painstaking experimental work.

39:11.000 --> 39:14.000
The rule of thumb is it takes one whole PhD,

39:14.000 --> 39:18.000
the whole PhD time, like one PhD student and their entire PhD,

39:18.000 --> 39:21.000
for five years to crystallize one protein

39:21.000 --> 39:25.000
and then using x-ray crystallography or electron microscopes,

39:25.000 --> 39:27.000
complicated pieces of very expensive,

39:27.000 --> 39:28.000
complicated pieces of equipment

39:28.000 --> 39:32.000
to basically image these incredibly small complex structures.

39:32.000 --> 39:35.000
It's unbelievably painstaking difficult work.

39:35.000 --> 39:37.000
And so over 50 years of human endeavor

39:37.000 --> 39:39.000
from all the labs around the world,

39:39.000 --> 39:41.000
structural biologists managed to find the structure

39:41.000 --> 39:45.000
around 100,000 to 150,000 proteins.

39:45.000 --> 39:48.000
And they're all deposited in this database called the PDB,

39:48.000 --> 39:51.000
and that's what we can use as a training corpus,

39:51.000 --> 39:53.000
but also we can test our predictions.

39:53.000 --> 39:56.000
So you can also do mutagenesis on these systems,

39:56.000 --> 39:58.000
so you can do some genetic experiments

39:58.000 --> 40:01.000
where you change one of the sequences,

40:01.000 --> 40:03.000
one of the residues, one of the amino acids,

40:03.000 --> 40:07.000
and then if it's on the surface from the predictor 3D structure,

40:07.000 --> 40:10.000
it should change the behavior of the protein.

40:10.000 --> 40:13.000
So you can sort of check if your 3D structure prediction

40:13.000 --> 40:16.000
saying that this residue is on the surface of the protein,

40:16.000 --> 40:20.000
you can sort of flip that out with a genetic mutation study

40:20.000 --> 40:23.000
and then see if that's affected the functioning of the protein.

40:23.000 --> 40:25.000
So there are sort of various ways after the fact

40:25.000 --> 40:28.000
to sort of check whether that's right.

40:29.000 --> 40:31.000
Okay, so alpha-fold then.

40:31.000 --> 40:33.000
The training data you're using there

40:33.000 --> 40:37.000
is the 100 to 150,000 proteins that have been figured out.

40:37.000 --> 40:39.000
What you have, as I understand it then,

40:39.000 --> 40:42.000
is their amino acid structures.

40:42.000 --> 40:47.000
You have the final protein 3D structure.

40:47.000 --> 40:51.000
And in the same way that you're setting your original system

40:51.000 --> 40:54.000
on PONG, pixel by pixel,

40:54.000 --> 40:57.000
try to get to an outcome where you're winning points,

40:57.000 --> 41:02.000
you are basically setting alpha-fold loose on this data

41:02.000 --> 41:08.000
and saying, try to figure out how to use the amino acid structure

41:08.000 --> 41:12.000
to predict the protein 3D structure,

41:12.000 --> 41:14.000
and when you do it correctly, you get points.

41:14.000 --> 41:16.000
Is that right?

41:16.000 --> 41:18.000
Yeah, that's basically how the system works.

41:18.000 --> 41:21.000
So you effectively have this amino acid sequence

41:21.000 --> 41:23.000
and you're telling it to predict the 3D structure

41:23.000 --> 41:26.000
and then you compare it against the real structure.

41:26.000 --> 41:28.000
There's various different ways you can compare it,

41:28.000 --> 41:30.000
but basically think about comparing

41:30.000 --> 41:33.000
where all the atoms end up being in 3D coordinate space

41:33.000 --> 41:36.000
and you sort of measure how far it's measured in angstroms,

41:36.000 --> 41:38.000
which is a tiny measure, right?

41:38.000 --> 41:40.000
Basically the width of an atom.

41:40.000 --> 41:44.000
How far away are you from the real 3D position of that atom?

41:44.000 --> 41:46.000
And for it to be useful for biologists,

41:46.000 --> 41:48.000
you've got to get the accuracy of that.

41:48.000 --> 41:50.000
All the atoms in the protein,

41:50.000 --> 41:52.000
and there are many, many thousands,

41:52.000 --> 41:55.000
within one atom width of the correct position.

41:55.000 --> 41:57.000
That's how accurate you have to get it for it

41:57.000 --> 42:00.000
to be useful for downstream biology purposes

42:00.000 --> 42:02.000
like drug discovery or disease understanding.

42:02.000 --> 42:04.000
So effectively the system gets a score

42:04.000 --> 42:06.000
from the average error it's making

42:06.000 --> 42:08.000
across all the atoms in the structure,

42:08.000 --> 42:11.000
and you're trying to get that to less than one angstrom,

42:11.000 --> 42:13.000
less than the width of an atom on average.

42:13.000 --> 42:16.000
So there's 100 to 150,000 of these,

42:16.000 --> 42:20.000
and then there's 100 million, 200 million proteins

42:20.000 --> 42:22.000
that we know of?

42:22.000 --> 42:23.000
That's right, yeah.

42:23.000 --> 42:26.000
So that's not a lot of training data, actually.

42:26.000 --> 42:30.000
You all then do something that I understand to be pretty dangerous

42:30.000 --> 42:33.000
and usually quite frowned upon,

42:33.000 --> 42:36.000
which is the system begins training itself

42:36.000 --> 42:38.000
on the predictions it is making.

42:38.000 --> 42:40.000
It is generating its own data then,

42:40.000 --> 42:43.000
and training itself on that data.

42:43.000 --> 42:45.000
And there's just a new paper that came out

42:45.000 --> 42:47.000
called The Curse of Recursion

42:47.000 --> 42:49.000
about how when AI systems begin training themselves

42:49.000 --> 42:51.000
on AI-generated data,

42:51.000 --> 42:52.000
the models often collapse.

42:52.000 --> 42:54.000
You're basically inbreeding the AI.

42:54.000 --> 42:57.000
So how do you do that

42:57.000 --> 43:00.000
in a way that does not inbreed your AI?

43:00.000 --> 43:02.000
Yeah, you're absolutely right.

43:02.000 --> 43:04.000
You have to be extremely careful

43:04.000 --> 43:06.000
when you start introducing its own, you know,

43:06.000 --> 43:08.000
an AI system's own predictions

43:08.000 --> 43:10.000
back into its training data.

43:10.000 --> 43:13.000
The reason we had to do it, and this is a very interesting,

43:13.000 --> 43:15.000
I think this is the best measure

43:15.000 --> 43:17.000
of how difficult this problem was.

43:17.000 --> 43:18.000
So as you point out,

43:18.000 --> 43:20.000
150,000 data points is tiny

43:20.000 --> 43:22.000
for machine learning systems.

43:22.000 --> 43:24.000
Usually you need millions and millions of data points, right?

43:24.000 --> 43:27.000
Like, for example, with AlphaGo, AlphaZero,

43:27.000 --> 43:29.000
we need a 10 million game,

43:29.000 --> 43:31.000
something like that, that it played itself.

43:31.000 --> 43:33.000
And of course, a game is far simpler

43:33.000 --> 43:35.000
than something like a, you know, protein structure in nature.

43:35.000 --> 43:38.000
So 150,000 is very, very minimal.

43:38.000 --> 43:40.000
I think most people assumed

43:40.000 --> 43:42.000
that there was not enough data,

43:42.000 --> 43:44.000
nowhere near enough data.

43:44.000 --> 43:46.000
And it turned out we had to throw the kitchen sink hat

43:46.000 --> 43:48.000
for Fold to make it work, everything we knew.

43:48.000 --> 43:50.000
So it's by far the most complicated system

43:50.000 --> 43:51.000
we ever worked on,

43:51.000 --> 43:53.000
and it's still the most complicated system we've worked on,

43:53.000 --> 43:56.000
and it took, you know, five years of work

43:56.000 --> 43:58.000
and many difficult wrong turns.

43:58.000 --> 44:01.000
And one of the things we had to do was augment the real data,

44:01.000 --> 44:03.000
which we didn't really have enough of,

44:03.000 --> 44:06.000
use it to build a first version of AlphaFold,

44:06.000 --> 44:08.000
and then that was just about good enough.

44:08.000 --> 44:11.000
I think we got it to do about a million predictions

44:11.000 --> 44:13.000
of new proteins.

44:13.000 --> 44:15.000
And when we got it to assess itself,

44:15.000 --> 44:18.000
how confident it was on those predictions,

44:18.000 --> 44:20.000
and then we sort of triaged it

44:20.000 --> 44:22.000
and cut the top sort of 30, 35%

44:22.000 --> 44:24.000
so around 300,000 predictions

44:24.000 --> 44:26.000
and put them back in the training set,

44:26.000 --> 44:29.000
along with the real data, the 150,000 real data.

44:29.000 --> 44:32.000
So then we had about half a million structures,

44:32.000 --> 44:34.000
obviously including its own predicted ones,

44:34.000 --> 44:36.000
to train the final system.

44:36.000 --> 44:38.000
And that system then was good enough

44:38.000 --> 44:40.000
to reach this atomic accuracy threshold.

44:40.000 --> 44:43.000
What that means is we only just about,

44:43.000 --> 44:45.000
as a scientific community, had enough data

44:45.000 --> 44:49.000
to bootstrap it to do this self-distillation process.

44:49.000 --> 44:51.000
And we think the reason that was okay

44:51.000 --> 44:53.000
was that we were very careful,

44:53.000 --> 44:55.000
first of all, we had enough real data,

44:55.000 --> 44:58.000
so there was a mixture of real and generated data,

44:58.000 --> 45:00.000
and that was enough to keep it on track.

45:00.000 --> 45:03.000
And also, there were lots of very good tests,

45:03.000 --> 45:06.000
independent tests of how good these predictions were

45:06.000 --> 45:08.000
of the overall system.

45:08.000 --> 45:10.000
Because of course, over time,

45:10.000 --> 45:12.000
experimenters were depositing new structures

45:12.000 --> 45:15.000
onto the database that were past

45:15.000 --> 45:18.000
the cut-off training date of when we trained the system.

45:18.000 --> 45:20.000
And so we could compare how accurate the system

45:20.000 --> 45:23.000
was against those new experimental data.

45:23.000 --> 45:24.000
So one thing you're saying here

45:24.000 --> 45:27.000
is the system is actually spitting out two things.

45:27.000 --> 45:30.000
It's spitting out a protein structure prediction

45:30.000 --> 45:33.000
and an uncertainty score.

45:33.000 --> 45:35.000
And that's a really actually cool thing

45:35.000 --> 45:37.000
about the AlphaFold system too,

45:37.000 --> 45:39.000
is that in many machine learning systems,

45:39.000 --> 45:41.000
we would love the system

45:41.000 --> 45:44.000
to not only produce the output, the prediction,

45:44.000 --> 45:48.000
but also measure of its uncertainty about that prediction,

45:48.000 --> 45:50.000
a confidence measure about it.

45:50.000 --> 45:53.000
And that's actually very important in all modern AI systems,

45:53.000 --> 45:55.000
that it would be nice if they, well, it would be very good

45:55.000 --> 45:56.000
if they all did that.

45:56.000 --> 45:58.000
But actually, very few systems do currently,

45:58.000 --> 46:01.000
and it's not known how to do that in general, right?

46:01.000 --> 46:03.000
It's an active area of research.

46:03.000 --> 46:05.000
But we managed to do that with AlphaFold,

46:05.000 --> 46:07.000
and the reason we put so much effort into that

46:07.000 --> 46:10.000
is that ultimately, I wanted this to be useful

46:10.000 --> 46:13.000
to biologists and scientists and medical researchers,

46:13.000 --> 46:17.000
amazing experts, domain experts in their area,

46:17.000 --> 46:19.000
but most of them would not know anything

46:19.000 --> 46:22.000
about machine learning or care, actually, frankly, right?

46:22.000 --> 46:24.000
They're just interested in the structure of the protein

46:24.000 --> 46:26.000
so they can go and cure a disease.

46:26.000 --> 46:30.000
And so it was really important for this particular task

46:30.000 --> 46:32.000
that if these outputs were going to be useful

46:32.000 --> 46:34.000
to anyone, researchers down the line,

46:34.000 --> 46:36.000
it would have to output its confidence level

46:36.000 --> 46:39.000
on the basis of each amino acid.

46:39.000 --> 46:41.000
So it color-coded it in really simple ways

46:41.000 --> 46:44.000
so that anybody, non-expert of machine learning

46:44.000 --> 46:47.000
can understand which parts of the prediction

46:47.000 --> 46:49.000
could they trust as an experimentalist

46:49.000 --> 46:51.000
and which other parts should they basically

46:51.000 --> 46:53.000
probably continue to do experiments on

46:53.000 --> 46:55.000
if they wanted to know the real structure,

46:55.000 --> 46:57.000
or at least tread with caution.

46:57.000 --> 47:01.000
And we wanted the system to really clearly output that

47:01.000 --> 47:04.000
and those confidence levels to be really accurate.

47:04.000 --> 47:06.000
So in effect, we needed to do that anyway

47:06.000 --> 47:08.000
for this tool to be useful downstream

47:08.000 --> 47:10.000
to biologists and medical researchers,

47:10.000 --> 47:13.000
but we ended up using that same confidence level

47:13.000 --> 47:16.000
to allow us to triage our own generated data

47:16.000 --> 47:19.000
and put back the more confident, better ones

47:19.000 --> 47:21.000
into the training data.

47:21.000 --> 47:23.000
So I want to unite this with something

47:23.000 --> 47:25.000
that people who've been following AI

47:25.000 --> 47:27.000
are probably more familiar with,

47:27.000 --> 47:29.000
which is the hallucination problem.

47:29.000 --> 47:33.000
So when I use chat GPT or at least different versions of it,

47:33.000 --> 47:37.000
it is common that you can ask it a kind of question,

47:37.000 --> 47:39.000
like tell me all about this one chemist

47:39.000 --> 47:41.000
from so-and-so who did such-and-such,

47:41.000 --> 47:43.000
and you can make it up and it can make it up,

47:43.000 --> 47:45.000
or maybe you ask it a real question

47:45.000 --> 47:47.000
and it just makes up a citation for you.

47:47.000 --> 47:50.000
This has been a big problem with the large language models,

47:50.000 --> 47:52.000
still a big problem with the large language models.

47:52.000 --> 47:55.000
And the theory is that the reason it's a problem

47:55.000 --> 47:58.000
is that they're just making predictions, right?

47:58.000 --> 48:00.000
They don't know what they're doing

48:00.000 --> 48:02.000
in the same way that your system didn't know it was playing pong.

48:02.000 --> 48:04.000
They just know that on the internet,

48:04.000 --> 48:06.000
on the training data they've been given,

48:06.000 --> 48:09.000
this is the word that would be most likely

48:09.000 --> 48:11.000
to come next in a sentence.

48:11.000 --> 48:15.000
So how come AlphaFold doesn't have

48:15.000 --> 48:18.000
this kind of hallucination problem?

48:18.000 --> 48:21.000
Yeah, so the current chatbots today

48:21.000 --> 48:24.000
have this problem, and partly it's because

48:24.000 --> 48:27.000
if they were better able to understand the confidence

48:27.000 --> 48:31.000
and the likelihood that what they're putting out putting

48:31.000 --> 48:33.000
is correct, they could at some point say,

48:33.000 --> 48:36.000
I don't know, that would be better than making something up,

48:36.000 --> 48:39.000
or they could sort of caveat it by it might be this,

48:39.000 --> 48:41.000
but perhaps you should double check.

48:41.000 --> 48:43.000
And in fact, if they were able to do that,

48:43.000 --> 48:45.000
they could cross-check their references.

48:45.000 --> 48:47.000
That's what they should be doing,

48:47.000 --> 48:49.000
using tools perhaps even like search to sort of go,

48:49.000 --> 48:52.000
oh, actually, that paper doesn't really exist.

48:52.000 --> 48:54.000
Just go and look it up on PubMed.

48:54.000 --> 48:57.000
It's not there, even though it's very plausible sounding.

48:57.000 --> 48:59.000
And so in fact, the human users,

48:59.000 --> 49:02.000
I'm sure you've had experiences, you have to go and look it up.

49:02.000 --> 49:04.000
Just a very funny idea, right, to make up a paper,

49:04.000 --> 49:06.000
then go search to see if the paper you've made up

49:06.000 --> 49:09.000
is actually there, and like, oh, it's actually not.

49:09.000 --> 49:11.000
It shows me. Yeah, exactly.

49:11.000 --> 49:13.000
But it would be better if it did that internally

49:13.000 --> 49:15.000
before we output that to the user.

49:15.000 --> 49:17.000
So you never saw its hallucination.

49:17.000 --> 49:20.000
And in a way, that's what is missing from the current systems

49:20.000 --> 49:22.000
is this actually what AlphaGo does,

49:22.000 --> 49:24.000
and at the systems we build,

49:24.000 --> 49:26.000
where there's a little bit of thinking time or search

49:26.000 --> 49:28.000
or planning that's going on

49:28.000 --> 49:30.000
before they output their prediction.

49:30.000 --> 49:34.000
Right now, they're kind of almost like idiot savants, right?

49:34.000 --> 49:38.000
They just output the immediate thing that just first comes to mind.

49:38.000 --> 49:40.000
And it may or may not be plausible,

49:40.000 --> 49:42.000
and it may or may not be correct.

49:42.000 --> 49:44.000
And we need a bit of sort of, I would say,

49:44.000 --> 49:47.000
deliberation and planning and reasoning

49:47.000 --> 49:49.000
to kind of almost sanity check

49:49.000 --> 49:51.000
what is that the prediction is telling you,

49:51.000 --> 49:54.000
not just output the first thing that comes to mind.

49:54.000 --> 49:56.000
And the kind of deep reinforcement learning systems

49:56.000 --> 49:58.000
we're known for do do that, right?

49:58.000 --> 50:00.000
Effectively, the reinforcement learning part, the planning part,

50:00.000 --> 50:02.000
calibrates what the model is telling it.

50:02.000 --> 50:04.000
It's not just the first most likely move

50:04.000 --> 50:06.000
that you play in every positioning go.

50:06.000 --> 50:08.000
It's the best move that you want.

50:08.000 --> 50:12.000
Is this the difference between building a system

50:12.000 --> 50:15.000
that is ultimately working with a structured data set

50:15.000 --> 50:17.000
where, at least for some amount of training data,

50:17.000 --> 50:19.000
it knows if it has the right answer

50:19.000 --> 50:22.000
and something that is using a truly unstructured,

50:22.000 --> 50:24.000
and I don't mean that in the technical way,

50:24.000 --> 50:26.000
but in the sort of colloquial way,

50:26.000 --> 50:30.000
the unstructured data set of life where people just talk

50:30.000 --> 50:34.000
and conversations don't have a right answer or wrong answer.

50:34.000 --> 50:37.000
If you're inhaling the entire corpus of Reddit,

50:37.000 --> 50:40.000
Reddit is not about did you get the right answer,

50:40.000 --> 50:42.000
people just talking.

50:42.000 --> 50:44.000
And so is what's going on here that because AlphaFold

50:44.000 --> 50:47.000
has this 100,000 proteins in there

50:47.000 --> 50:49.000
that it knows if they're right or not,

50:49.000 --> 50:52.000
and so it knows what it looks like to get something right,

50:52.000 --> 50:54.000
that you can then build a system where the point is

50:54.000 --> 50:56.000
to get something right, but when you're building

50:56.000 --> 50:59.000
these much more generalized language-oriented systems,

50:59.000 --> 51:01.000
that that isn't the structure of language.

51:01.000 --> 51:04.000
Language doesn't, in some internal way,

51:04.000 --> 51:06.000
have like an input and an output

51:06.000 --> 51:08.000
where you can see that some outputs were correct

51:08.000 --> 51:10.000
and some outputs weren't.

51:10.000 --> 51:12.000
Yeah, I think that's the right intuition.

51:12.000 --> 51:14.000
I mean, I think language and the way, obviously,

51:14.000 --> 51:16.000
on the internet encapsulates a huge slice

51:16.000 --> 51:19.000
of human civilization knowledge.

51:19.000 --> 51:21.000
It's far more complex than a game

51:21.000 --> 51:24.000
and perhaps even proteins in a kind of general sense.

51:24.000 --> 51:27.000
I think the difference is actually that in games,

51:27.000 --> 51:30.000
especially but also even with protein structures,

51:30.000 --> 51:33.000
you can automate the correction process.

51:33.000 --> 51:35.000
Like if you don't win the game, then obviously the things

51:35.000 --> 51:38.000
you were planning or the moves that you tried to make

51:38.000 --> 51:41.000
or the reasoning you did wasn't very good to a certain extent.

51:41.000 --> 51:44.000
So you can immediately update on that in an automated way.

51:44.000 --> 51:46.000
The same with protein structures.

51:46.000 --> 51:48.000
If the final structures that you're predicting

51:48.000 --> 51:51.000
have very large errors compared to the known structures,

51:51.000 --> 51:53.000
you've obviously done something wrong.

51:53.000 --> 51:55.000
There's no subjective decision there, neither.

51:55.000 --> 51:57.000
You can just automate that.

51:57.000 --> 51:59.000
Of course, with language and knowledge, human knowledge,

51:59.000 --> 52:01.000
it's much more nuanced than that.

52:01.000 --> 52:03.000
But as we talked earlier, if you hallucinate

52:03.000 --> 52:06.000
a new reference to a paper that doesn't exist,

52:06.000 --> 52:08.000
that's pretty black and white, right?

52:08.000 --> 52:10.000
Like, you know that that's wrong.

52:10.000 --> 52:12.000
So there are a lot of things, I think,

52:12.000 --> 52:14.000
that could be done far better than we're doing today

52:14.000 --> 52:16.000
and we're working really hard on increasing

52:16.000 --> 52:18.000
the magnitude, the factuality, and the reliability

52:18.000 --> 52:20.000
of these systems.

52:20.000 --> 52:23.000
And I don't see any reason why that cannot be improved.

52:23.000 --> 52:26.000
But also, there are some things which are a bit more subjective,

52:26.000 --> 52:28.000
and then you need human feedback on it,

52:28.000 --> 52:30.000
which is why everyone's using reinforcement learning

52:30.000 --> 52:33.000
with human feedback to sort of train these systems

52:33.000 --> 52:35.000
better to behave in ways we would like.

52:35.000 --> 52:37.000
But of course, if you're relying on human feedback,

52:37.000 --> 52:39.000
that itself is quite a nosy process

52:39.000 --> 52:42.000
and very time-consuming and takes a large effort.

52:42.000 --> 52:45.000
And so it's not as quick or as automated

52:45.000 --> 52:47.000
as when you have some objective measure

52:47.000 --> 52:49.000
that you can just optimize against.

52:49.000 --> 52:52.000
So we talked about the exponential curves

52:52.000 --> 52:54.000
that you all have seen again and again

52:54.000 --> 52:56.000
in the gameplay systems.

52:56.000 --> 52:58.000
And you also mentioned that the rule of thumb

52:58.000 --> 53:01.000
in proteins is it takes one PhD researcher,

53:01.000 --> 53:04.000
their whole PhD, to figure out the structure of one protein.

53:04.000 --> 53:08.000
So tell me the timeline then of building off a fold

53:08.000 --> 53:11.000
and then of beginning to find the proteins.

53:11.000 --> 53:14.000
My understanding is that there's this kind of slow takeoff

53:14.000 --> 53:16.000
and then a real takeoff. So what happens here?

53:16.000 --> 53:18.000
We worked on Alpha Fold.

53:18.000 --> 53:20.000
A couple of versions of Alpha Fold, actually.

53:20.000 --> 53:22.000
We had to go to the drawing board at one point

53:22.000 --> 53:24.000
when we hit an asymptote over around a four-year,

53:24.000 --> 53:27.000
four-and-a-half-year period from about 2016 to 2020.

53:27.000 --> 53:32.000
And then we entered it into this competition called CASP,

53:32.000 --> 53:34.000
which is you can think of it as like the Olympics

53:34.000 --> 53:35.000
for protein folding.

53:35.000 --> 53:38.000
So every two years, all of the people working on this

53:38.000 --> 53:41.000
from all the labs around the world enter this competition.

53:41.000 --> 53:43.000
It's an amazing competition because what they do

53:43.000 --> 53:46.000
is they over the last sort of few months, experimentalists

53:46.000 --> 53:48.000
give them their protein structures.

53:48.000 --> 53:50.000
They've just found literally hot off the press then,

53:50.000 --> 53:52.000
but not published yet.

53:52.000 --> 53:54.000
So they're unknown to anyone other than the experimental lab

53:54.000 --> 53:56.000
that produced it.

53:56.000 --> 53:58.000
And they give it to the competition organizers.

53:58.000 --> 54:00.000
The competition organizers give it to the competing

54:00.000 --> 54:01.000
computational teams.

54:01.000 --> 54:04.000
We have to submit within a week our predictions.

54:04.000 --> 54:06.000
And then later on at the end of the summer,

54:06.000 --> 54:07.000
this happens all over the summer.

54:07.000 --> 54:10.000
There's like 100 proteins you get in the competition.

54:10.000 --> 54:12.000
And then they reveal the true structures.

54:12.000 --> 54:13.000
You know, they get published.

54:13.000 --> 54:16.000
And then you compare, obviously you have this double blind

54:16.000 --> 54:19.000
scoring system where nobody knew who the competing teams were

54:19.000 --> 54:21.000
and nobody knew what the real structures were

54:21.000 --> 54:22.000
until the end of the competition.

54:22.000 --> 54:25.000
So it's a beautifully designed competition.

54:25.000 --> 54:27.000
And the organizers have been running it for 30 years,

54:27.000 --> 54:29.000
incredible dedication to do that.

54:29.000 --> 54:31.000
And that was another reason we picked this problem to work on

54:31.000 --> 54:33.000
because it had this amazing competition.

54:33.000 --> 54:35.000
So there was actually a game you could win.

54:35.000 --> 54:37.000
There was a game we could win and a leaderboard

54:37.000 --> 54:39.000
we could optimize against.

54:39.000 --> 54:41.000
And then when those revealed got revealed at the end of 2020,

54:42.000 --> 54:44.000
then it was announced in a big conference

54:44.000 --> 54:48.000
and the organizers sort of proclaimed that the structure

54:48.000 --> 54:50.000
prediction problem or the protein folding problem

54:50.000 --> 54:54.000
had been solved because we got to within atomic accuracy

54:54.000 --> 54:57.000
on these predictions of these 100 new proteins.

54:57.000 --> 55:00.000
And so that was the moment where we knew we had a system

55:00.000 --> 55:03.000
that was going to be really useful for experimentalists

55:03.000 --> 55:05.000
and drug design and so on.

55:05.000 --> 55:08.000
And then the next question was how do we gift this to the world

55:08.000 --> 55:11.000
and in a way that all these world researchers and biologists

55:11.000 --> 55:14.000
and medical researchers could make the fastest use of

55:14.000 --> 55:17.000
to have the biggest impact downstream.

55:17.000 --> 55:20.000
And what we realized is not only was the system really accurate

55:20.000 --> 55:23.000
alpha fold, it was also extremely fast.

55:23.000 --> 55:26.000
So we could fold an average length protein

55:26.000 --> 55:28.000
in a matter of seconds.

55:28.000 --> 55:30.000
And then when we did the calculation, it was like,

55:30.000 --> 55:33.000
well, there are roughly 200 million protein sequences,

55:33.000 --> 55:36.000
genetic sequences known to science.

55:36.000 --> 55:39.000
We could probably fold all of them in a year.

55:39.000 --> 55:41.000
So that's what we set out to do.

55:41.000 --> 55:43.000
We started off with the most important organisms,

55:43.000 --> 55:45.000
obviously the human proteome.

55:45.000 --> 55:47.000
So it's equivalent to the human genome, but in protein space.

55:47.000 --> 55:50.000
And then we went to all the important research organisms,

55:50.000 --> 55:53.000
you know, the mouse, the fly, the zebrafish and so on.

55:53.000 --> 55:56.000
And then some important crops like wheat and rice and so on.

55:56.000 --> 55:58.000
They're very important, obviously to humanity.

55:58.000 --> 56:00.000
And so then we put all of those 20 out first

56:00.000 --> 56:03.000
and then eventually over 2021, we did all of them.

56:03.000 --> 56:06.000
And then we put that out as a database, free access database

56:06.000 --> 56:09.000
to the world, the research community in collaboration

56:09.000 --> 56:13.000
with the European Bioinformatics Institute based in Cambridge.

56:13.000 --> 56:16.000
Before we move on to some other work you all are doing,

56:16.000 --> 56:19.000
one thing that as I understand it, alpha fold has spun out into now

56:19.000 --> 56:23.000
is a group under Alphabet, the Google parent company

56:23.000 --> 56:25.000
called Isomorphic.

56:25.000 --> 56:29.000
Tell me a bit about Isomorphic and both the sort of scientific

56:29.000 --> 56:32.000
theory there, but also it's a very different theory

56:32.000 --> 56:34.000
of how AI could make money.

56:34.000 --> 56:37.000
Then, you know, we're going to add a chat bot into a search engine.

56:37.000 --> 56:40.000
So what's the business theory there?

56:40.000 --> 56:44.000
Yeah, so alpha fold is a grand challenge in biology

56:44.000 --> 56:46.000
of understanding the structure of proteins.

56:46.000 --> 56:49.000
The reason I thought that was so important was

56:49.000 --> 56:52.000
because I think it can hugely accelerate,

56:52.000 --> 56:56.000
be part of accelerating drug discovery and therefore curing diseases.

56:56.000 --> 56:58.000
But it's only one part of the puzzle.

56:58.000 --> 57:00.000
So, you know, knowing the structure of proteins,

57:00.000 --> 57:04.000
only one small bit of the whole drug discovery process.

57:04.000 --> 57:07.000
So there are many other really important things from identifying

57:07.000 --> 57:09.000
which proteins you should target, so, you know,

57:09.000 --> 57:12.000
maybe through genetic analysis and genomics,

57:12.000 --> 57:16.000
all the way to, like, can we design a small molecule,

57:16.000 --> 57:19.000
a drug compound, a chemical compound that will correctly

57:19.000 --> 57:22.000
bind to that protein and the bit of the protein you want

57:22.000 --> 57:25.000
blocked or bind to and not anything else in your body

57:25.000 --> 57:27.000
because that's side effects, right?

57:27.000 --> 57:29.000
Effectively, that's what makes things toxic is

57:29.000 --> 57:31.000
they don't just bind to the thing you want,

57:31.000 --> 57:33.000
but they bind to all sorts of other proteins

57:33.000 --> 57:34.000
that you didn't want them to.

57:34.000 --> 57:37.000
So, in my view, AI is the perfect tool

57:37.000 --> 57:40.000
to accelerate the time scales of doing that

57:40.000 --> 57:43.000
because that's done right now in Big Pharma

57:43.000 --> 57:47.000
in a painstakingly experimental way that takes many, many years.

57:47.000 --> 57:50.000
You know, I think the average time of going from a target

57:50.000 --> 57:53.000
to a compound you can start testing in clinical trials

57:53.000 --> 57:57.000
is, like, five, six years and many, many hundreds of millions of dollars

57:57.000 --> 58:00.000
per drug, which is incredibly slow and incredibly expensive,

58:00.000 --> 58:03.000
and I think that could be brought down by an order of magnitude

58:03.000 --> 58:06.000
using AI and computational techniques

58:06.000 --> 58:09.000
to do the exploration part, do that in silico,

58:09.000 --> 58:11.000
using AI and computational techniques,

58:11.000 --> 58:14.000
and then only at the last step saving experimental work,

58:14.000 --> 58:17.000
of course, very important experimental work and wet lab work

58:17.000 --> 58:19.000
for the validation step.

58:19.000 --> 58:22.000
So, instead of doing all the search, which is the expensive slow part,

58:22.000 --> 58:24.000
you just do it for validating the compounds

58:24.000 --> 58:26.000
that your AI system has come up with.

58:26.000 --> 58:30.000
And so isomorphic is our spin-out, sort of sister company to deep mind,

58:30.000 --> 58:35.000
that is tasked with building more alpha folds, breakthroughs,

58:35.000 --> 58:39.000
but in adjacent spaces, so more going into chemistry,

58:39.000 --> 58:43.000
so designing small molecules, predicting the different properties

58:43.000 --> 58:45.000
of those small molecules called admi properties

58:45.000 --> 58:48.000
and making sure, like, we minimize things like toxicity

58:48.000 --> 58:50.000
and side effects and so on

58:50.000 --> 58:54.000
and maximize its potential at binding to the protein that we want.

58:56.000 --> 58:59.000
â

59:21.000 --> 59:25.000
So there are games, or things that are structured a bit like games-

59:25.000 --> 59:29.520
that I'm very excited about the possibility of AI winning or getting unbelievably good

59:29.520 --> 59:34.120
at. So drug discovery being one of them. And then there are ones where I'm a little more

59:34.120 --> 59:37.800
nervous. I've heard you say, for instance, that from a certain perspective, the stock

59:37.800 --> 59:43.440
market very much has the structure of a game. And if I am a very rich hedge fund and a lot

59:43.440 --> 59:47.360
of them do algorithmic trading, I mean, if theme park, the game was AI, then definitely

59:47.360 --> 59:51.720
what a lot of these hedge funds are doing is AI, they've got a lot of money. If you were

59:51.760 --> 59:57.320
thinking about a system to win the stock market, what does that look like? I mean, there's a

59:57.320 --> 59:59.520
lot of training data out there. Like, what do you do?

01:00:00.600 --> 01:00:04.680
Yeah, I think almost certainly some of the top hedge funds must be using, I would have

01:00:04.680 --> 01:00:08.360
thought, some of these techniques that we and others have invented to trade on the stock

01:00:08.360 --> 01:00:12.680
market. It has some of the same properties, as you say, I mean, finance friends of mine

01:00:12.680 --> 01:00:16.680
talk about it as being the biggest game in some ways, right? That's sometimes how it's

01:00:16.680 --> 01:00:21.560
talked about for better or for worse. And I'm sure a lot of these techniques would work.

01:00:21.760 --> 01:00:27.480
Now, the interesting thing is whether you just treat the stock market, say, as a series of

01:00:27.480 --> 01:00:31.600
numbers. And that's one theory that you just treat it as a big sequence of numbers, you

01:00:31.600 --> 01:00:35.280
know, time series of numbers. And you're just trying to predict the next numbers in the

01:00:35.280 --> 01:00:38.840
sequence. You know, you could imagine that's analogous to predicting the next word, you

01:00:38.840 --> 01:00:42.880
know, with the chatbots. So that's possible. My view is it's probably a bit more complex

01:00:42.880 --> 01:00:46.520
than that, because those numbers actually describe real things, you know, profits and

01:00:46.520 --> 01:00:51.160
losses, real companies, and then real people running those companies and having ideas and

01:00:51.320 --> 01:00:56.480
so on. And there's also macroeconomic forces like geopolitical forces and interest rates

01:00:56.480 --> 01:01:02.040
set by governments and so on. And so my thinking is that probably to understand the full

01:01:02.040 --> 01:01:07.680
context required to predict the next number in that sequence would require you to understand

01:01:07.720 --> 01:01:12.480
a lot more about the world than just the stock prices. So you'd somehow have to

01:01:12.520 --> 01:01:17.200
encapsulate all of that knowledge in a way that the machine could ingest and understand.

01:01:18.240 --> 01:01:22.760
Well, you need to do that to fully win the game. But to come up with local strategies, it

01:01:22.760 --> 01:01:26.840
could be very profitable and or very destructive. I mean, one, we already know that all kinds

01:01:26.840 --> 01:01:30.560
of firms have done that with high speed algorithmic trading. Yes. And two, you could just

01:01:30.560 --> 01:01:35.120
imagine all kinds of, again, if you're willing to run through a very large search space and

01:01:35.120 --> 01:01:38.960
try strategies other people don't try, I mean, you know, you could short out this company

01:01:38.960 --> 01:01:42.600
destroying this competitor such as this other one, you could predict would go up immediately

01:01:42.600 --> 01:01:49.280
if that happened. And you can imagine very weird strategies being deployed by a system

01:01:49.560 --> 01:01:55.240
that has the power to move money around and, you know, a lot of data in it and is just

01:01:55.240 --> 01:01:57.280
getting reinforcement learning for making money.

01:01:57.840 --> 01:02:01.600
Yeah, you could imagine that. I think that as I understand it, that world, I mean, there's

01:02:01.600 --> 01:02:05.720
lots of very, very smart people working in that world with algorithms, not necessarily

01:02:05.880 --> 01:02:09.760
the latest machine learning ones, or the latest statistical algorithms. And they're very

01:02:09.760 --> 01:02:15.040
sophisticated because obviously they're very incented to do that, given that it's literally

01:02:15.040 --> 01:02:21.200
money at stake. So I imagine the efficiency is pretty high already in hedge funds and

01:02:21.240 --> 01:02:26.000
high frequency trading and other things you mentioned, where, you know, there's already

01:02:26.160 --> 01:02:32.320
ways to slightly game the system, perhaps, that are already quite profitable. And it's not

01:02:32.320 --> 01:02:37.720
clear to me that sort of more general learning system would be better than that. It may be

01:02:37.720 --> 01:02:42.240
different from that, but it may already be easier. There may be easier ways, which hedge

01:02:42.240 --> 01:02:45.040
funds are probably already doing, I assume, to make that money.

01:02:45.520 --> 01:02:51.560
Well, this actually brings up, I think, a pretty big question for me, which is one of the

01:02:51.560 --> 01:02:56.320
reasons I wanted to have this conversation with you about Alpha Fold is I think people are now

01:02:56.320 --> 01:03:00.760
used to thinking about these very generalized large language models, inhale a tremendous amount

01:03:00.760 --> 01:03:06.920
of data, come up with these patterns, and then, you know, they're able to answer a lot of kinds

01:03:06.920 --> 01:03:11.800
of questions at a certain level, but not a very high level of rigor. And then there's this other

01:03:11.800 --> 01:03:16.760
approach, which is building much more bespoke systems. I mean, Alpha Fold can do something

01:03:16.760 --> 01:03:21.320
amazing in the protein space, and it cannot help me write a college essay, as best as I

01:03:21.320 --> 01:03:28.600
understand it. And I don't want to put too much of a binary choice here, because I think I know

01:03:28.600 --> 01:03:33.640
that there is overlap, but I do think there have been sort of two theories of how AI is going to

01:03:33.640 --> 01:03:38.680
roll out over time. And one is we're going to have lots of different specialized systems that

01:03:38.680 --> 01:03:43.240
are tuned to do different things, a system to do legal contracts, and a system to do proteins,

01:03:43.240 --> 01:03:47.560
and a system to check out radiology results, and a system to help kids with their homework.

01:03:48.200 --> 01:03:55.080
And another is that we are eventually going to pump enough data into GPT-12 or whatever it might be,

01:03:55.880 --> 01:04:01.240
such it it attains a kind of general intelligence, that it becomes a system that can do everything,

01:04:01.880 --> 01:04:06.040
that the general system eventually will emerge, and that system will be able to do all these

01:04:06.040 --> 01:04:09.720
things. And so what you should really be working on is that, can you talk a bit about, because I

01:04:09.720 --> 01:04:13.320
know you're interested in building a general intelligence system, can you tell me a bit about

01:04:13.320 --> 01:04:18.200
what you understand to be the path to that now? Do we want a lot of little systems or not little,

01:04:18.200 --> 01:04:24.280
but specialized, or is the theory here that no, this is right, you want it all in one system

01:04:24.280 --> 01:04:27.560
that is going to be able to span across functionally every domain?

01:04:28.520 --> 01:04:33.720
Yeah, that's a fascinating question. And actually, DeepMind was founded in our still

01:04:33.720 --> 01:04:38.760
our mission is to create that big general system. That's of course, the way the brain works, right?

01:04:38.760 --> 01:04:43.720
We have one system and it can do, we can do many things with our minds, including science and

01:04:43.720 --> 01:04:49.160
playing chess and all with the same brain, right? So that's the ultimate goal. Now, interestingly,

01:04:49.160 --> 01:04:55.320
on the way to that goal, I always believed that we don't have to wait to get into general

01:04:55.320 --> 01:05:01.560
intelligence or AGI before we can get incredible use out of these systems by using the same

01:05:01.560 --> 01:05:07.080
sorts of techniques, but maybe specializing them around a particular domain. And AlphaFold

01:05:07.080 --> 01:05:11.880
is a great example of that, perhaps the best example so far in AI of that. And AlphaGo,

01:05:11.880 --> 01:05:16.520
obviously, all our game systems were like that too. And so what I think is going to happen in the

01:05:16.520 --> 01:05:21.560
next era of systems, and we're working on our own systems called Gemini, is that I think there's

01:05:21.560 --> 01:05:25.640
going to be a kind of combination of the two things. So we'll have this increasingly more

01:05:25.640 --> 01:05:30.600
powerful general system that you basically interact with through language, but has other

01:05:30.600 --> 01:05:35.320
capabilities, general capabilities like math and coding, and perhaps some reasoning and

01:05:35.320 --> 01:05:39.880
planning eventually in the next generations of these systems. One of the things these systems

01:05:39.960 --> 01:05:46.920
can do is use tools. So tool use is a big part of the research area now of these language models

01:05:46.920 --> 01:05:52.600
or chatbots. In order to achieve what they want, they need to do, they can actually call a tool

01:05:52.600 --> 01:05:57.880
and make use of a tool. And those tools can be of different types. They could be existing

01:05:57.880 --> 01:06:03.480
pieces of software, special case software, like a calculator or maybe like Adobe Photoshop or

01:06:03.480 --> 01:06:08.040
something like that. So big pieces of software that they can learn how to use using reinforcement

01:06:08.040 --> 01:06:14.360
learning and learn how to use the interface and interact with. But they can also be other AI systems,

01:06:14.360 --> 01:06:20.520
other learned systems. I'll give you an example. So if you challenge one of these chatbots to a

01:06:20.520 --> 01:06:25.000
game, you want to play a game of chess or a game of Go against it, they're actually all pretty bad

01:06:25.000 --> 01:06:30.520
at it currently, which is one of the tests I give these chatbots is, can they play a good game and

01:06:30.520 --> 01:06:34.440
hold the board state in mind? And they can't really at the moment, they're not very good.

01:06:35.160 --> 01:06:38.920
But actually, maybe there's something to say, well, these general systems

01:06:38.920 --> 01:06:45.080
shouldn't learn how to play chess or Go or fold proteins, there should be specialized AI systems

01:06:45.080 --> 01:06:50.600
that learn how to do those things, AlphaGo, AlphaZero, AlphaFold. And actually, the general

01:06:50.600 --> 01:06:58.120
system can call those specialized AIs as tools. So I don't think it makes much sense for the language

01:06:58.120 --> 01:07:03.560
model to know how to fold proteins. That would seem like an over-specialization in its data

01:07:03.560 --> 01:07:07.640
corpus relative to language and all the other things that general things that it needs to learn.

01:07:07.640 --> 01:07:12.920
I think it will be more efficient for it to call this other AI system and make use of

01:07:12.920 --> 01:07:17.320
something like AlphaFold if it needed to fold proteins. But it's interesting because at some

01:07:17.320 --> 01:07:23.160
point, more of those capabilities will be forwarded back into the general system over time. But I

01:07:23.160 --> 01:07:27.400
think at least the next era, we'll see a general system making use of these specialized systems.

01:07:28.280 --> 01:07:35.240
So when you think about the road from what we have now to these generally intelligent systems,

01:07:36.440 --> 01:07:41.560
do you think it's simply more training data and more compute, right? Like more processors,

01:07:41.560 --> 01:07:47.960
more stuff we feed into the training set? Or do you think that there are other innovations,

01:07:47.960 --> 01:07:51.560
other technologies that we're going to need to figure out first? What's between here and there?

01:07:52.280 --> 01:07:58.840
I'm in the camp that both needed. I think that large multimodal models of the site we have now

01:07:58.840 --> 01:08:04.840
have a lot more improvement to go. So I think more data, more compute, and better

01:08:04.840 --> 01:08:11.560
techniques will result in a lot of gains and more interesting performance. But I do think there are

01:08:11.560 --> 01:08:16.920
probably one or two innovations, a handful of innovations missing from the current systems

01:08:17.000 --> 01:08:20.920
that will deal with things that we talked about like factuality, robustness,

01:08:20.920 --> 01:08:26.040
in the realm of planning and reasoning and memory that the current systems don't have.

01:08:26.040 --> 01:08:30.360
And that's why they fall short still of a lot of interesting things we would like them to do.

01:08:30.360 --> 01:08:35.640
So I think some new innovations are going to be needed there, as well as pushing the existing

01:08:35.640 --> 01:08:42.520
techniques much further. So it's clear to me in terms of building an AGI system or general AI system,

01:08:42.520 --> 01:08:47.960
these large multimodal models are going to be a core component. So they're definitely necessary,

01:08:47.960 --> 01:08:52.840
but I'm not sure they'll be sufficient in of themselves. You've talked about in interviews

01:08:52.840 --> 01:08:59.720
I've heard you give before that you don't want to see this pursuit develop into a move fast and

01:08:59.720 --> 01:09:04.440
break things kind of race. At the same time, you're part of Google. They just aligned actually all

01:09:04.440 --> 01:09:09.000
of Google AI under you. There used to be two groups, DeepMind and Google Brain. Now it's all

01:09:09.080 --> 01:09:15.000
under your empire. Open AI is aligned with Microsoft. Meta is doing a lot more AI. They're

01:09:15.000 --> 01:09:19.960
working under Yanlacun, who's like one of the founders in all this. China obviously has a number

01:09:19.960 --> 01:09:23.720
of systems that seem to be getting better fairly quickly compared to even what we were seeing six

01:09:23.720 --> 01:09:30.440
months ago. It does feel like a race dynamic has developed. And I'm curious how you think about that.

01:09:31.480 --> 01:09:35.880
I don't think it's ideal. That's for sure. I think it's just the way that technology is panned

01:09:35.880 --> 01:09:40.760
out. It's become more of an engineering kind of technology, or at least the phase we're in now

01:09:40.760 --> 01:09:45.720
versus scientific research and innovation, which was perhaps done over the last decade.

01:09:45.720 --> 01:09:50.040
And Google and DeepMind were responsible for a lot of those breakthroughs that we've discussed,

01:09:50.040 --> 01:09:54.840
you know, reinforcement learning, obviously, transformers, which Google research invented

01:09:54.840 --> 01:10:01.640
that underpin all of the modern systems, very critical breakthrough. And so give Google credit,

01:10:01.640 --> 01:10:07.000
they just released publicly. Yes, exactly. So they released publicly, published it available,

01:10:07.000 --> 01:10:13.160
and everyone uses that now, including Open AI. And so that underpins the kind of technologies

01:10:13.160 --> 01:10:19.640
and systems that we see today. And I would prefer it if we took a scientific approach to this as

01:10:19.640 --> 01:10:25.320
a field and as a community and an industry where we were optimistic about what's coming down the

01:10:25.320 --> 01:10:29.960
line. Obviously, I worked on AI my whole career because I think it's going to be the most beneficial

01:10:29.960 --> 01:10:36.520
technology for humanity ever, cure all diseases and help us with energy and also sustainability,

01:10:36.520 --> 01:10:41.400
all sorts of things. I think that AI can be an incredibly useful tool for, but it has risks.

01:10:41.400 --> 01:10:45.480
It's a dual use technology. And like any new transformative technology, and I think AI will

01:10:45.480 --> 01:10:50.920
be one of the most transformative in human history, it can be used for bad too. And so we have to

01:10:50.920 --> 01:10:55.880
think all of that through. And I would like us to have not move fast and break things like you say,

01:10:55.880 --> 01:11:01.480
and actually be more thoughtful and try and have foresight rather than hindsight about these things.

01:11:01.480 --> 01:11:05.160
We're not going to get everything right with a fast moving cutting edge technology like AI

01:11:05.160 --> 01:11:10.520
first time, but we should try and minimize and think very carefully about the risks at each stage

01:11:10.520 --> 01:11:16.280
and try and mitigate those as far as possible while making sure we're bold and brave with the

01:11:16.280 --> 01:11:21.640
benefits. And so we have this mantra of being bold and responsible. And I think there's a creative

01:11:21.720 --> 01:11:26.120
tension there, but it's sort of intentional between those two things. When you look back at the

01:11:26.120 --> 01:11:31.480
technology, perhaps one of the last big technologies of the last decade or two has been social media,

01:11:31.480 --> 01:11:36.760
I think that embodies this view of like move fast and break things. And I feel like that has,

01:11:36.760 --> 01:11:41.880
of course, had huge benefits and huge growth for certain companies. And it's been very beneficial

01:11:41.880 --> 01:11:46.920
in many ways, but it also had some unintended consequences that we only as a society started

01:11:46.920 --> 01:11:52.760
realizing many, many years later once it had reached huge scale. I would like us to avoid that,

01:11:52.760 --> 01:11:57.560
if possible, with AI to the extent that that's possible. There is also commercial realities

01:11:57.560 --> 01:12:03.080
and geopolitical issues. And we are in this sort of race dynamic. And what I hope is that

01:12:03.080 --> 01:12:08.040
there will be a sort of cooperation actually at the international scale on the safety and

01:12:08.040 --> 01:12:13.640
technical risks as these systems become more and more powerful. I want to talk about one benefit

01:12:13.720 --> 01:12:17.080
that you mentioned in passing there. And then I want to talk through some of the risks more

01:12:17.080 --> 01:12:22.600
specifically. You mentioned help us work on energy. And we've talked a lot here about protein

01:12:22.600 --> 01:12:27.320
folding. We've talked about the applicability to drug discovery. I think the idea that AI could

01:12:27.320 --> 01:12:34.200
help us with clean energy is something people often hear said, but don't get a lot of details on.

01:12:34.200 --> 01:12:39.000
But one of the systems you're building or projects you're working on is around nuclear fusion

01:12:39.000 --> 01:12:43.400
and stabilizing nuclear fusion. So I don't want to spend a ton of time here, but just

01:12:43.400 --> 01:12:48.520
put some meat on the bones of that idea. Can you just talk about what you're doing here and why AI

01:12:48.520 --> 01:12:54.200
might be well suited to it? Yeah, I think AI can actually help with climate and sustainability

01:12:54.200 --> 01:12:59.480
in a number of ways, at least three different ways I think of. One is optimizing our existing

01:12:59.480 --> 01:13:03.720
infrastructure. So we get more out of the same infrastructure. We have a really good example

01:13:03.720 --> 01:13:09.720
of that. Actually, we used a similar system to AlphaGo to control the cooling systems in a data

01:13:09.720 --> 01:13:15.000
centers, in these massive data centers that run all of our compute. They use a huge amount of energy

01:13:15.000 --> 01:13:19.720
and we actually managed to save 30% of the energy the cooling systems used by more efficiently

01:13:19.720 --> 01:13:25.480
controlling all the parameters. Secondly, we can monitor the environment better automatically,

01:13:25.480 --> 01:13:30.360
you know, deforestation and other things, forest fires, all of these types of things using AI.

01:13:30.360 --> 01:13:34.760
So that's helpful for NGOs and governmental organizations to keep track of things. And

01:13:34.760 --> 01:13:39.960
then finally, we can use AI to accelerate breakthrough, new breakthrough technologies. And

01:13:39.960 --> 01:13:44.600
our fusion work is a good example of that, controlling the plasma incredibly hot, hotter than

01:13:44.600 --> 01:13:49.400
the surface of the sun. So it can't touch the sides of the magnets and so on in these big machines

01:13:49.400 --> 01:13:54.600
called Tokamaks that control this plasma, super hot plasma that is generating the electricity.

01:13:54.600 --> 01:13:59.560
And we use AI, our sort of reinforcement learning systems to predict effectively what the shape of

01:13:59.560 --> 01:14:04.520
the plasma is going to be. So in milliseconds, we can change the magnetic field by changing the

01:14:04.520 --> 01:14:10.760
current going in the magnets to keep hold of the plasma in place so it doesn't go out of control.

01:14:10.760 --> 01:14:16.040
So that's a huge problem in fusion and one of the big issues with getting fusion working. But

01:14:16.040 --> 01:14:19.640
there are also other ways I could imagine I could help in things like material design,

01:14:19.640 --> 01:14:24.920
designing better batteries, better solar panel technologies, superconductors and so on, which

01:14:24.920 --> 01:14:29.880
I think AI will be able to help with down the line. So I want to hold that there. And then I want

01:14:29.880 --> 01:14:37.160
to talk about a risk here, which is one of the things I see happening is ever since GPT-3 was

01:14:37.160 --> 01:14:42.840
hooked up to chat GPT, and people could begin interfacing with it in natural language, there's

01:14:42.840 --> 01:14:50.600
been a huge rush towards chat systems, towards chat bots. And this is, I know, an oversimplification,

01:14:50.680 --> 01:14:57.080
but I do think there's an idea here about, are we making systems that are designed to do what

01:14:57.080 --> 01:15:01.800
humans can do, but a little bit better? Are we making systems that what we have built here is

01:15:01.800 --> 01:15:07.400
something meant to seem human to humans? Or are we making systems that are actually very inhuman,

01:15:07.400 --> 01:15:11.720
that are doing what humans cannot do because they can think in a way humans cannot think,

01:15:11.720 --> 01:15:15.800
or more to the point calculate in a way humans cannot calculate. And AlphaFold,

01:15:15.800 --> 01:15:19.400
the nuclear fusion system you're talking about, those strike me as more in that area.

01:15:20.040 --> 01:15:23.640
And I don't want to say there's necessarily a sharp choice between the two because we've talked

01:15:23.640 --> 01:15:29.720
about the possibility of general intelligence systems too. But there is where investment goes.

01:15:29.720 --> 01:15:35.080
There is where the corporate priorities are. There is where the best engineers are working.

01:15:35.880 --> 01:15:41.960
And now you have these very big companies that are basically in a battle for search and enterprise

01:15:41.960 --> 01:15:46.760
software funding, right? They want to get subscriptions to Microsoft Office 365 up,

01:15:46.760 --> 01:15:51.400
and Google doesn't want Bing to take its market share. And one thing that I worry about a bit

01:15:51.400 --> 01:15:56.120
is that I see a lot more possible benefit for humanity from these more scientific inhuman

01:15:56.120 --> 01:16:02.200
systems, but that the hype and the investment and the energy is going towards these more human,

01:16:02.200 --> 01:16:08.120
more kind of familiar systems that I worry are not going to be as beneficial. And so one risk

01:16:08.120 --> 01:16:14.120
I see is simply that the business models are not going to be well hooked to public benefit.

01:16:14.120 --> 01:16:18.120
And you said a minute ago, we sort of were leaving the scientific research period of this

01:16:18.120 --> 01:16:22.200
and entering into the competitive period of this. I think of something that kind of kept

01:16:22.200 --> 01:16:26.360
deep mind a little bit apart from it. Always you're in London and you guys always seemed a

01:16:26.360 --> 01:16:30.280
little bit more like you're on the scientific path and now you have to be on top of all Google.

01:16:30.280 --> 01:16:34.040
How do you think about this tension? Yeah, it's a very interesting question. I think about this

01:16:34.040 --> 01:16:38.920
all the time. And you're right, there is that tension. And I think all of, let's say the venture

01:16:38.920 --> 01:16:43.720
capitalist world and so on has almost sort of lost their minds over chatbots, right?

01:16:43.720 --> 01:16:48.120
And all the money's going into there. I think even in our new guys as Google deep mind,

01:16:48.120 --> 01:16:54.120
we're going to keep pushing really hard on both frontiers. Advancing science and medicine is

01:16:54.120 --> 01:16:59.320
always going to be at the heart of what we do and our overall mission for the benefit of humanity.

01:16:59.320 --> 01:17:05.880
But we are also going to push hard on next generation products with incredible new experiences for

01:17:05.880 --> 01:17:10.280
billions of users that help them in their everyday lives. I'm kind of equally excited about potential

01:17:10.360 --> 01:17:16.520
of both types of things. And so that involves us continuing to invest and work on scientific

01:17:16.520 --> 01:17:21.400
problems like AlphaFold or isomorphic labs is doing drug discovery, fusion, except for quantum

01:17:21.400 --> 01:17:28.280
chemistry, mathematics, many, many of our nature and science papers, as well as doubling down on

01:17:28.280 --> 01:17:33.560
these new types of chatbot interfaces and so on. I don't see them as sort of human and inhuman. It's

01:17:33.560 --> 01:17:39.160
more like the AlphaFold things are scientific tools for experts to use and enhance their work

01:17:39.160 --> 01:17:43.560
so they can accelerate their very important research work. And then the other hand, at the

01:17:43.560 --> 01:17:47.880
moment, I think chatbots are more of a fun entertainment thing. I mean, of course, you

01:17:47.880 --> 01:17:52.120
can do your homework on them and you can do amusing things and it's quite helpful. But I think there's

01:17:52.120 --> 01:17:56.680
so much more to come in that space. And I think where I see them joining together is what we

01:17:56.680 --> 01:18:00.440
discussed earlier about these general systems, perhaps that you interact with in language.

01:18:00.440 --> 01:18:05.240
There's nothing wrong with that because language is the mode that we all can use rather than coding

01:18:05.240 --> 01:18:11.080
or mathematics. Language is the simplest thing for everybody to use to interface with these systems,

01:18:11.080 --> 01:18:17.320
but they could call a bunch of specialized systems and specialized tools and make use of them. And

01:18:17.320 --> 01:18:21.880
so I actually think there's quite an interesting combination to come by pushing the frontiers

01:18:21.880 --> 01:18:25.560
of both of those things. And that's what we're planning to do going forward.

01:18:26.280 --> 01:18:33.560
You recently signed a letter. It was alongside Sam Altman, who leads OpenAI and Dario Amade,

01:18:33.560 --> 01:18:37.720
who is a top OpenAI person, now leads Anthropic. And letter simply says,

01:18:38.360 --> 01:18:43.160
mitigating the risk of extinction from AI should be a global priority alongside other

01:18:43.160 --> 01:18:49.320
societal scale risks, such as pandemics and nuclear war. Why do you believe there is any

01:18:49.320 --> 01:18:56.200
risk of extinction from AI at all? Well, that letter was a kind of compromise thing. I think

01:18:56.200 --> 01:19:01.320
it's not 30 words long. So of course, all the nuances are missing from a letter such as that.

01:19:01.400 --> 01:19:05.960
And at some point soon, we'll put out a fuller statement about our position.

01:19:05.960 --> 01:19:10.040
The only thing I was agreeing with there is that this technology has such potential

01:19:10.040 --> 01:19:16.280
for enormous, enormous good, but it's a dual-use technology. So if bad actors get hold of it,

01:19:16.280 --> 01:19:20.120
it could be used for bad things. There are near-term harms we have to be careful of,

01:19:20.120 --> 01:19:24.520
like, deep fakes. And we need to address with things like watermarking, and we're working on

01:19:24.520 --> 01:19:28.120
that. And I think a lot of that will come out later this year. And then there are technical

01:19:28.120 --> 01:19:33.160
risks. This alignment problem that we discussed earlier on how to make sure these systems do

01:19:33.160 --> 01:19:37.560
what we want. And we set them the right objectives and the right values, and they can be contained

01:19:37.560 --> 01:19:42.840
and controlled as they get more powerful. So there's a whole series of at least three buckets of

01:19:42.840 --> 01:19:48.360
worry. And I think they're all equally important, actually, but they require different solutions.

01:19:48.360 --> 01:19:54.280
And one of them is this longer term people think of as maybe a science fiction scenario of inherent

01:19:54.360 --> 01:19:58.760
technical risk from these systems, where if we don't build them in the right way,

01:19:58.760 --> 01:20:02.680
in the limit when they're decades time, when they're very, very powerful,

01:20:02.680 --> 01:20:06.200
and they're capable of planning and all the things I discussed earlier today, but to the

01:20:06.200 --> 01:20:11.480
nth degree, we have to be careful with those sorts of systems. I don't think it's likely,

01:20:11.480 --> 01:20:15.720
I wouldn't even put a probability on it, but there's uncertainty over it. And it's certainly

01:20:15.720 --> 01:20:20.920
non-zero. I think the possibility that that could go wrong if we're not thoughtful and careful and

01:20:20.920 --> 01:20:26.440
use exceptional care with these technologies. So I think that's the part I was trying to indicate

01:20:26.440 --> 01:20:31.960
by signing that was that it's important to have that debate now. You don't want to have that debate

01:20:31.960 --> 01:20:38.040
on the eve of some kind of technology like that arriving, right? Ten years sounds like a long time,

01:20:38.040 --> 01:20:42.440
but it's not that long, given the amount of research that would be required and is required,

01:20:42.440 --> 01:20:48.040
and I think needs to be done to understand the systems better so that we can mitigate any risks

01:20:48.040 --> 01:20:53.400
that may come about. Well, I think right now when people hear about extinction risk from AI,

01:20:53.400 --> 01:20:58.760
one scenario that they've now been told to think about and more people do is the AI itself getting

01:20:58.760 --> 01:21:03.080
out of control or turning the whole world into paperclips or whatever it might be. But I want

01:21:03.080 --> 01:21:08.520
to talk about another here, which is more along the lines of our conversation. So you build Alpha

01:21:08.520 --> 01:21:13.000
Fold now through isomorphic, you're building a whole suite of tools to search through the

01:21:13.000 --> 01:21:20.600
molecular space, the protein space to better understand how to predict the functions of and

01:21:20.600 --> 01:21:30.280
then eventually create bespoke proteins, molecules, etc. And I think one slightly less sci-fi version

01:21:30.280 --> 01:21:36.120
of extinction or at least mass harm that you can imagine here is through synthetic biology,

01:21:36.120 --> 01:21:42.760
through as it becomes very cheap to figure out how to create an unbelievably lethal virus and print

01:21:42.840 --> 01:21:48.600
an unbelievably lethal virus that in the future, it's actually not that hard for some terrorist

01:21:48.600 --> 01:21:55.480
organization to use tools like this to make something far beyond if you think back in America,

01:21:55.480 --> 01:22:00.360
you know, however many years now, when somebody was mailing anthrax around, if it had been very

01:22:00.360 --> 01:22:06.200
easy for that person to create super smallpox, then you get into something really, really horrifying.

01:22:06.760 --> 01:22:10.200
And how do you think about that suite of risks? Because you're doing more work in that space

01:22:10.200 --> 01:22:13.880
really than anyone else, I think. And that's one of the ones that seems actually much near

01:22:13.880 --> 01:22:19.000
at hand to me. Yeah, we think a lot about that. And we talk with a lot of experts in government

01:22:19.000 --> 01:22:24.360
and academia about this. And actually, before we released Alpha Fold, we spent several months

01:22:24.360 --> 01:22:31.320
talking to over 30 experts in biosecurity, biorethics, also Nobel Prize winning biologists

01:22:31.320 --> 01:22:36.120
and chemists about what we were going to release with the database and what they thought of it.

01:22:36.200 --> 01:22:40.120
And all of them unanimously actually came back with in that case, the benefits far outweighed

01:22:40.120 --> 01:22:44.120
the risks. And I think we're seeing all the benefits of that today with millions of biologists

01:22:44.120 --> 01:22:48.680
around the world using it. But look, going forward, as you get more into chemistry space,

01:22:48.680 --> 01:22:53.400
one has to think about these things. But of course, we want to cure many terrible diseases too,

01:22:53.400 --> 01:22:58.520
right? So we have to weigh up that enormous benefit there to society with these inherent

01:22:58.520 --> 01:23:04.040
risks that you're talking about. And I think one of these issues is access to these technologies

01:23:04.040 --> 01:23:08.760
by bad actors, not scientists and people, you know, medical practitioners who are trying to

01:23:08.760 --> 01:23:13.080
do good with it. But as you say, terrorists, other things like that. And I think that's where

01:23:13.080 --> 01:23:18.520
actually becomes a question of things like open sourcing, or do you publish these results? Or

01:23:18.520 --> 01:23:23.320
do you sort of, how secure is your cybersecurity? So you can't be hacked? All of these questions

01:23:23.320 --> 01:23:28.120
come into play. And I think that's where we're going to have to think a lot more carefully

01:23:28.120 --> 01:23:33.160
in the next few years, as these systems become more sophisticated about who should get access

01:23:33.160 --> 01:23:38.360
to those things? How should that be monitored? Can bad actors be shut down if they're using APIs

01:23:38.360 --> 01:23:42.760
very quickly before they do any harm? Maybe we can use AI there actually to detect what are they

01:23:42.760 --> 01:23:47.080
trying to design with these systems as well. This can also happen with chatbots too. What are they

01:23:47.080 --> 01:23:52.440
asking the chatbots, right? So I think there's a role for AI to play there actually as well on the

01:23:52.440 --> 01:23:57.800
monitoring side. And so the other question though to ask is, and when I've discussed this with experts

01:23:57.800 --> 01:24:03.480
in biosecurity is, there are known toxins today, like you mentioned anthrax, you can probably find

01:24:03.480 --> 01:24:08.520
the recipe for that somewhere on the internet or, and people could do that, but you still need a wet

01:24:08.520 --> 01:24:14.760
lab, and you still need some scientific capability. And so those are areas which are also usually beyond

01:24:14.760 --> 01:24:20.040
naive bad actor individual, their capabilities, right? It's not just the recipe, how do you actually

01:24:20.040 --> 01:24:24.360
make it and then distribute it, right? It's actually pretty difficult. And I would argue that's

01:24:24.360 --> 01:24:29.160
already available today if you want it. It's not that there's no bad toxins that are known.

01:24:29.160 --> 01:24:33.480
There are some that are quite simple. It's just not that easy to make them to the uninitiated,

01:24:33.480 --> 01:24:37.320
right? You need a lab and you need access to it and labs can be monitored and so on.

01:24:37.320 --> 01:24:41.400
There are still a lot of barriers. It's not just a question of understanding the design,

01:24:41.400 --> 01:24:45.560
but we do need to think about that as well and figure out how we control that information.

01:24:46.200 --> 01:24:53.000
Right now, the systems and the kind of labs that could create the systems that could become

01:24:53.000 --> 01:24:56.680
something like general intelligence. I mean, you could count them on two hands, right? Across

01:24:57.480 --> 01:25:01.800
the United States, across Europe, across China. And over time, there'll be even more than that,

01:25:01.800 --> 01:25:06.440
but that's I think where we are now. As we get closer, I mean, you were talking about how much

01:25:06.440 --> 01:25:12.760
can happen here in 10 years. If we're getting to a point where somebody is getting near something

01:25:12.760 --> 01:25:17.960
like a general intelligence system, is that too powerful technology to be in private hands? Should

01:25:18.040 --> 01:25:23.560
this be something that whichever corporate entity gets their first controls or do we need something

01:25:23.560 --> 01:25:29.880
else to govern it? My personal view is that this is such a big thing in this fullness of time.

01:25:29.880 --> 01:25:35.560
I think it's sort of bigger than any one corporation or even one nation. I think it needs

01:25:35.560 --> 01:25:40.520
sort of international cooperation. I've often talked in the past about a kind of CERN-like

01:25:40.520 --> 01:25:46.680
effort for AGI. And I quite like to see something like that as we get closer, maybe in many years

01:25:46.680 --> 01:25:52.920
from now to an AGI system where really careful research is done on the safety side of things,

01:25:53.480 --> 01:25:58.440
understanding what these systems can do, and maybe testing them in controlled conditions

01:25:58.440 --> 01:26:05.880
like simulations or games first, like sandboxes, very robust sandboxes with lots of cyber security

01:26:05.880 --> 01:26:10.840
protection around them. I think that will be a good way forward as we get closer towards human

01:26:10.840 --> 01:26:15.480
level AI systems. I think it's a good place to end. So always our final question then.

01:26:15.480 --> 01:26:17.720
What are three books you would recommend to the audience?

01:26:18.920 --> 01:26:24.040
Well, I've chosen three books that are quite meaningful to me. So I would say, first of all,

01:26:24.040 --> 01:26:30.520
Fabric of Reality by David Deutsch. I think that poses all the big questions in physics that I would

01:26:30.520 --> 01:26:38.600
love one day to tackle with our AI tools. The second book I would say is Permutation City by Greg Egan.

01:26:38.600 --> 01:26:44.600
I think it's an amazing story, actually, wild story of how interesting and strange I think the

01:26:44.600 --> 01:26:51.560
world can get in the context of AI and simulations and hyper-realistic simulations. And then finally,

01:26:51.560 --> 01:26:56.840
I would recommend Consider Fleebers by Ian Banks, which is part of the culture series of novels,

01:26:56.840 --> 01:27:01.320
very formative for me. And I read that while I was writing Theme Park. And I still think it's

01:27:01.320 --> 01:27:07.160
the best depiction of a post-AGI future, an optimistic post-AGI future, where we're traveling

01:27:07.160 --> 01:27:10.840
the stars and humanity sort of reached its full flourishing.

01:27:10.920 --> 01:27:13.080
Ladies and gentlemen, thank you very much.

01:27:13.080 --> 01:27:13.880
Thanks very much.

01:27:24.840 --> 01:27:27.560
This episode of The Israel Clown Show was produced by Roger Karma,

01:27:27.560 --> 01:27:31.880
fact-checking by Michelle Harris. Our senior engineer is a great Jeff Gelb. The show's

01:27:31.880 --> 01:27:36.760
production team also includes Emma Flaugau, Annie Galvant, and Kristen Lin. Our music is by Isaac

01:27:36.760 --> 01:27:41.240
Jones. Audience strategy this week by Christina St. Maluski and Shannon Busta.

01:27:41.240 --> 01:27:44.840
The executive producer of New York Times' opinion audio is Annie Rose Strasser,

01:27:44.840 --> 01:27:46.920
and special thanks to Sonia Herrero.

