start	end	text
0	23000	From New York Times Opinion, this is the Ezra Klein Show.
23000	29000	So I think you can date this era in artificial intelligence back to the launch of chat GPT.
29000	35000	And what is weird if you talk to artificial intelligence people about that is they'll tell you chat GPT.
35000	38000	It was just a wrapper, an interface system.
38000	42000	The underlying system GPT-3 had been around for a while.
42000	46000	I mean, I'd had access to GPT-3 for quite a while before chat GPT came around.
46000	53000	What chat GPT did was it allowed you to talk to GPT-3 like you were a human and it was a human.
53000	56000	So it made AI more human.
56000	63000	It made it more able to communicate back and forth with us by doing a better job mimicking us and understanding us,
63000	64000	which is amazing.
64000	71000	I don't mean to take anything away from it, but it created this huge land rush for AIs that functionally mimic human beings.
71000	76000	AIs that relate as if they are human beings and try to fool us into thinking that they're human.
76000	80000	But I've always been more interested in more inhuman AI systems.
80000	86000	When you ask somebody who's working on artificial intelligence, including people who believe it could do terrible harm to the world,
86000	89000	why are you doing it? What's the point of this?
89000	96000	They don't say, oh, you know, we should risk these terrible consequences because it's fun to chat with chat GPT.
96000	102000	They say, oh, AI, it's going to solve all these terrible scientific problems we have, clean energy and drug discovery.
102000	108000	And, you know, it's going to create an era of innovation like nothing humanity's ever experienced.
108000	112000	There aren't that many examples, though, of AI doing that yet.
112000	115000	But there is one, which you may have heard me mention before.
115000	121000	And that's AlphaFold, the system built by DeepMind that solved the protein folding problem.
121000	125000	And the protein folding problem is that there are hundreds of millions of proteins.
125000	128000	The way they function has to do with their 3D structure.
128000	132000	But even though it's fairly straightforward to figure out their amino acid sequence,
132000	136000	it's very hard to predict how they will be structured based on that.
136000	137000	We were never able to do it.
137000	142000	We were doing it one by one, studying each one for years to try to figure out and basically map it.
142000	147000	And then they build the system, AlphaFold, which solves a problem,
147000	152000	is able to predict the structure of hundreds of millions of proteins, a huge scientific advance.
152000	154000	So how did they build that?
154000	157000	And what could other systems like that look like?
157000	164000	What is this other path for AI, this more scientific path where you're tuning these systems to solve scientific problems,
164000	168000	not to communicate with us, but to do what we truly cannot do?
168000	171000	Demis Asabis is the founder of DeepMind.
171000	176000	DeepMind is owned by Google and recently Asabis was put in charge of all Google AI.
176000	180000	So now it's called Google DeepMind and he runs all of it.
180000	182000	That makes him one of the most important people in the world,
182000	185000	charting the future of artificial intelligence.
185000	190000	So I asked him to come on the show to talk me through the development of AlphaFold,
190000	195000	how it was built, what came before it, what could come after it,
195000	201000	and what that says about the different ways and the different pathways forward for artificial intelligence.
201000	204000	As always, my email is reclineshow at nytimes.com.
209000	211000	Demis Asabis, welcome to the show.
211000	212000	Thanks for having me.
212000	215000	So tell me about the founding of DeepMind.
215000	216000	You were pretty young at that point.
216000	218000	How did you decide to start it?
218000	221000	What was the vision for it?
221000	225000	Well, actually, my background in AI starts from way before DeepMind.
225000	230000	So I started actually in the games industry, writing computer games and some best selling games,
230000	236000	actually games like Theme Park and Black and White and other games that I worked on in my teenage years.
236000	239000	And they all had AI as a core component of the game.
239000	240000	So let's take Theme Park.
240000	243000	It was a simulation game, came out in 1994.
243000	250000	And you basically created a theme park and lots of little people came in, played on the rides and bought stuff from your stalls.
250000	252000	So there's a whole kind of simulation underlying it.
252000	255000	And AI was the core part of the gameplay.
255000	260000	So I've been working on AI for nearly 30 years now in various guises.
260000	262000	So if you look at my career, I've done lots of different things,
262000	267000	but they were all doing something like an effort like DeepMind working on AGI.
267000	273000	And so all the things I did, the neuroscience, the computer science undergrad, PhD in neuroscience,
273000	279000	was all for gathering information and inspiration for eventually what would become DeepMind.
279000	281000	So I played Theme Park back in the day.
281000	287000	And if you ask me now and you say, hey, Theme Park, that game you played in the 90s, was that AI?
287000	289000	I would say no.
289000	295000	There's a classic line to paraphrase that AI is anything the computer can't do yet.
295000	298000	But now you look back, you think, oh, no, that's just a little video game.
298000	302000	So when you say that was AI, what are you saying by that?
302000	304000	And to you, what is artificial intelligence?
304000	312000	And what's just machine learning or statistics or some less impressive function?
312000	319000	Yeah, so back in the 90s and with games like Theme Park at the time, that was pretty cutting edge.
319000	325000	We were using relatively simple techniques, cellular automata and relatively narrow AI systems,
325000	329000	sort of logic systems really, which was in vogue back in the 80s and 90s.
329000	335000	But it was AI in terms of making a machine do something smart and actually adapt to,
335000	339000	automatically adapt to the way the gamer in that case was playing the game.
339000	344000	So the cool thing about Theme Park was that and why it was so successful and sold millions of copies around the world
344000	348000	was that everybody who played it got a unique experience.
348000	351000	Because the game adapted to how you were playing.
351000	355000	It's very primitive by today's standards of learning systems.
355000	357000	But back then it was pretty groundbreaking.
357000	365000	It was one of the first games along with SimCity to do these kinds of complicated simulations under the hood powered by AI.
365000	369000	As to your question about what is AI, I think AI is the science of making machines smart.
369000	375000	And then a sub branch of AI is machine learning, which is the kinds of AI systems that learn for themselves
375000	378000	and learn directly from data and experience.
378000	383000	And that's really what, of course, has powered the renaissance of AI in the last 10, 15 years.
383000	385000	And that's the sort of AI that we work on today.
385000	387000	Can you just spend another second on that?
387000	391000	You mentioned a minute ago that what was happening in Theme Park was logic based systems,
391000	393000	which I think is at least one of the other branches.
393000	399000	Can you describe the difference between some of the more logic based or rules based or knowledge encoded systems
399000	404000	and deep learning and some of the other things that are more dominant now?
404000	405000	Sure.
405000	410000	So if you think of AI as being this overarching field of making machines smart,
410000	413000	then they're broadly speaking two approaches to doing that.
413000	419000	One is the classical approach and the sort of approach that was done for the first few decades of AI research
419000	422000	since the 1950s with logic systems.
422000	426000	So this is the idea of the programmers or the creators of the system.
426000	432000	They effectively solve the problem, be that playing chess or controlling little characters in a game.
432000	435000	And then they would program up these routines and heuristics,
435000	441000	and then effectively the system would then deal with new inputs and execute those heuristics.
441000	443000	And you can be pretty powerful systems.
443000	445000	They're sometimes called expert systems.
445000	448000	And the most famous of that is probably Deep Blue IBM's chess program.
448000	452000	They beat Gary Kasparov, the world chess champion at the time in the 90s very famously.
452000	455000	And that was probably the pinnacle of expert systems.
455000	459000	But the problem with them is that they're very brittle and they can't deal with the unexpected, of course,
459000	464000	because they can only do what the programmers have already figured out and the heuristics they've already been given.
464000	466000	They can't learn anything new.
466000	470000	So machine learning is the other approach to solving AI.
470000	473000	And it's turned out to be a lot more powerful and a lot more scalable,
473000	476000	which is what we bet on as well as when we started DeepMind.
476000	480000	And that's the idea of machines, the systems learning for themselves,
480000	485000	learning the structure, learning of heuristics and rules that they should do for themselves,
485000	487000	directly from data or directly from experience.
487000	491000	The big contrast for our first sort of big famous program was AlphaGo,
491000	495000	which learned how to play the complex game of go for itself
495000	501000	and actually came up with better strategies and heuristics than we could have ever thought of as the human designers.
501000	505000	We're going to get to AlphaGo, but I want to hold for a minute in the founding of DeepMind.
505000	507000	So you've been a game designer.
507000	511000	You become a neuroscientist and you do actually some important peer-reviewed research
511000	514000	and highly cited research in that field.
514000	518000	That's a big jump. You got to work pretty hard to become a neuroscientist is my understanding.
518000	520000	So you're an academia, you're doing this research.
520000	522000	It's going well as best I can tell.
522000	525000	And you start deciding you're going to found this AI company.
525000	528000	So take me in that moment of decision making.
528000	534000	Yeah. So again, if you have in mind that I was planning from, I guess I was around 15, 16,
534000	539000	when I was doing this programming on these computer games and theme park and sort of 17 years old,
539000	544000	that's already when I decided my life was going to be and my career was going to be about AI and making AI happen.
544000	548000	So all the other things I chose were in service of that, including the PhD.
548000	550000	So I did my undergrad in computer science.
550000	555000	I got trained in coding, engineering and mathematical side of computer science,
555000	560000	which I love the theoretical side, Turing machines, all of these types of things, computation theory.
560000	565000	And then I decided for my PhD after I ran my own games company for a while,
565000	568000	I went back to academia to do a PhD in neuroscience.
568000	572000	And I chose cognitive neuroscience because first of all, I've always been fascinated by the brain.
572000	578000	The brain is the only existence proof we have in the universe that general intelligence is possible.
578000	581000	So it seems worthy of studying that specific data point.
581000	584000	It's probably not the only way that intelligence could come about,
584000	587000	but it's certainly the only way that we're aware of and that we can study.
587000	589000	And of course, it's fascinating subjects in itself.
589000	593000	But the reason I did the PhD is I wanted to learn about the brain,
593000	596000	maybe get inspiration for some algorithmic ideas, architectural ideas.
596000	601000	And indeed, that's what did happen in things like memory replay, reinforcement learning and things like this,
601000	603000	that we then used in our AI systems.
603000	607000	And also learn how to do cutting edge research too,
607000	611000	and actually learn how to use the scientific method properly and things like control studies and so on.
611000	615000	Really, you learn all of those practical skills by doing a PhD.
615000	618000	You said something about founding DeepMind that I've always found striking,
618000	622000	which is, quote, I want to understand the big questions, the really big ones,
622000	626000	that you normally go into philosophy or physics if you're interested in.
626000	631000	I thought building AI would be the fastest route to answer some of those questions.
631000	634000	And so a lot of people might want to know about the nature of the universe.
634000	636000	The idea is I'll go work on it directly.
636000	640000	I'll get a physics PhD, a mathematics PhD, a PhD in chemistry or neuroscience,
640000	642000	and I'll go solve those problems.
643000	651000	And your theory of the case was I will get this training and build something else to solve those problems.
651000	654000	Why do you think that intermediary intelligence is necessary?
654000	656000	Why not just do it yourself?
656000	657000	It's actually a great question.
657000	660000	I mean, I've always been fascinated by the biggest questions.
660000	662000	I'm not quite sure why that is,
662000	667000	but I've always been interested in the nature of the universe, nature of reality, consciousness,
667000	669000	the meaning of life, all of these big questions.
669000	671000	That's what I wanted to spend my life working on.
671000	673000	And indeed physics was my favorite subject at school.
673000	674000	I love physics.
674000	675000	I still love physics.
675000	681000	I still sort of try to keep tabs on interesting areas of physics like quantum mechanics and so on.
681000	686000	But what happened is a lot of my scientific heroes in my early years were physicists,
686000	689000	so Richard Feynman and Stephen Weinberg, these kinds of people.
689000	690000	But I actually read this book.
690000	695000	It must have been in high school called Dreams of a Final Theory by Stephen Weinberg.
695000	699000	And it's his book, and you know, Nobel Prize winner, amazing physicist of his work,
699000	703000	on trying to come up with a unified theory of physics, you know, to unify everything together.
703000	706000	And I remember reading this book and I was very inspiring book,
706000	711000	but I remember concluding, wow, they hadn't actually made that much progress.
711000	714000	And these incredible people, you know, that I had really admired, you know,
714000	717000	you can think of all the physicists since post-World War II, right?
717000	720000	These incredible people like Feynman and Weinberg and so on.
720000	724000	And I just remember thinking, gosh, I wasn't that convinced they'd got that far.
724000	729000	And then I was thinking, even in the best-case scenario that you might be able to follow their footsteps,
729000	732000	which is a big if, given how brilliant they were,
732000	736000	you still might not be able to make much progress on it in the whole lifetime of, you know,
736000	738000	an amazing career like they had.
738000	740000	And so then I started thinking, well, perhaps the problem is,
740000	745000	is that we need a little bit of help and a little bit of extra intellectual horsepower.
745000	747000	And where can we get that from?
747000	751000	Well, of course, I was also simultaneously programming and doing games
751000	753000	and falling in love with sort of computing.
753000	755000	And that was my real passion.
755000	759000	And I just realized that working on AI, I could satisfy both things at once.
759000	764000	So first of all, it would open up a door to insights into what intelligence is
764000	768000	and how the brain works and, you know, as a comparator and so on,
768000	770000	when those are some of the biggest questions already.
770000	775000	But it could also help with science and physics and help experts in those areas
775000	777000	crack the problems and the questions they care about.
777000	781000	So it seemed to be like the perfect sort of meta solution in a way.
781000	786000	And when I made this realization sometime in high school, that's when I decided, you know,
786000	790000	I was going to work on AI and not go directly, say, for physics
790000	793000	and try and build the ultimate scientific tool.
793000	796000	So let's talk about how some of this experience comes together for you
796000	799000	because one of the early contributions of DeepMind
799000	801000	is you're trying to build this new kind of intelligence
801000	808000	and the way you are trying to build it, train it, test it is on games.
808000	811000	And now it feels like a weird thing to hear,
811000	814000	but it takes you a very long time to build a system
814000	817000	that can score even a single point in Pong.
817000	821000	So tell me first about the decision to begin training AIs on video games
821000	823000	because that doesn't seem totally intuitive.
823000	826000	You want to answer the fundamental questions of physics
826000	828000	and you're building something to play Pong?
828000	830000	It seems ridiculous. Why?
830000	834000	Yes. That was one of the early decisions I think that served us very well
834000	838000	when we started DeepMind was a lot of people who were working on AI at that time
838000	841000	were mostly working on things like robotics
841000	843000	and sort of embodied intelligence
843000	846000	and that's an important branch of AI, of course,
846000	849000	and places like MIT where I was doing my postdoc.
849000	852000	That was a real bastion of that type of work.
852000	854000	But what I realized was that the researchers
854000	857000	ended up spending most of their time fiddling around with the hardware,
857000	860000	you know, in the server motors and they'd always break
860000	863000	and the robots are expensive and they're complicated and they're slow.
863000	867000	And I sort of realized that it would be better to work in simulation
867000	871000	and we could, you know, run millions of experiments in the cloud all at once
871000	875000	and get much faster learning rates relatively cheaply and quickly
875000	877000	and simply if we were to do that.
877000	879000	The other advantage of games, of course,
879000	882000	is that they've been built to be challenging to humans
882000	885000	and you can use top human players as a great benchmark.
885000	887000	The other thing is they have clear objectives, right?
887000	890000	You to win the game or to maximize the score
890000	893000	and those kinds of objectives are very useful
893000	895000	if you want to train reinforcement learning systems
895000	899000	which we specialize in that are reward-seeking and goal-directed,
899000	901000	you know, so they need an objective to solve.
901000	904000	So games are fantastic for all of those reasons.
904000	906000	And the other cool thing about games is, of course,
906000	908000	you can go up the ladder of complexity of games
908000	910000	by just going through the different eras of games.
910000	913000	We started with the 1970s with the simplest Atari games
913000	915000	like you mentioned, like Pong,
915000	918000	and then eventually we went all the way up over, you know,
918000	921000	almost a decade of work to the most complex modern computer games
921000	924000	like StarCraft, and so you can keep on increasing
924000	927000	the difficulty of the test for your AI systems
927000	930000	as your AI systems are getting more sophisticated.
930000	933000	So, you know, it turned out to be a really efficient way
933000	937000	to proving ground, really, to test out these algorithmic ideas.
937000	940000	So I don't want to skip over too quickly what's happening here.
940000	942000	I think when I say the sentence,
942000	944000	oh, you built an AI that can play the game Pong,
944000	946000	that sounds simple.
946000	948000	But it takes you a long time to do that.
948000	950000	It takes you a long time to score any points in Pong,
950000	953000	and then you begin to see exponential dominance of the Pong game,
953000	957000	which is an interesting dynamic here that I want to talk about too.
957000	960000	But tell me what you're actually doing there.
960000	963000	Sure. So the key thing here is,
963000	966000	and this is the difference between what I was doing in the 90s
966000	968000	with my early games career,
968000	970000	where we were directly programming these expert systems.
970000	973000	Of course, you could do that easily for something like Pong.
973000	975000	But what the key breakthrough that we had
975000	978000	that sort of underpinned this whole new field, really,
978000	980000	which we call deep reinforcement learning,
980000	983000	combining neural networks with the reward-seeking algorithms,
983000	987000	is that we played these games directly from the pixels.
987000	990000	So all we gave is the input to our systems.
990000	994000	The first system we built was called DQN to play these Atari games.
994000	996000	What did DQN stand for?
996000	998000	DQ network. So QQ...
998000	1000000	I was hoping that was going to be more fun.
1000000	1002000	No, no, it was a very technical name.
1002000	1005000	We had one at the beginning before we got better at naming things.
1005000	1007000	It just refers to the technique that we used.
1007000	1010000	And so the big innovation and the big breakthrough
1010000	1012000	was to use just the raw inputs,
1012000	1014000	in this case the pixels on the screen,
1014000	1016000	30,000 pixels roughly on the screen,
1016000	1020000	and not tell the system anything about the game.
1020000	1023000	Not what it was controlling, not what would get score,
1023000	1025000	not how it loses points or loses a life.
1025000	1028000	Any of those things, it had to figure out for itself
1028000	1030000	from first principles by playing the game,
1030000	1032000	just like a human would learn.
1032000	1034000	That was the key thing.
1034000	1036000	So it learned for itself, and the second thing that was key
1036000	1038000	was building these general systems.
1038000	1040000	And that's what the general is in AGI,
1040000	1042000	and I'm sure we'll come back to that,
1042000	1046000	is that one single system that can play any of the Atari games,
1046000	1048000	and the same system out of the box
1048000	1051000	can play all of them to sort of superhuman level,
1051000	1053000	like world record scores.
1053000	1056000	And so those two elements, the generality and the learning,
1056000	1058000	are the key differences.
1058000	1060000	So I want to spend just a moment here
1060000	1062000	on this expert system versus deep learning.
1062000	1065000	So if you've been building an expert system, a logic system,
1065000	1069000	you're trying to tell the system how Pong works.
1069000	1071000	It's like, okay, there are these two paddles,
1071000	1074000	and there's a ball, and what you want to do is get points,
1074000	1077000	and you're basically trying to break the game of Pong down
1077000	1080000	into rules, encode the rules into the system,
1080000	1083000	and you figure out if you've found all the rules
1083000	1085000	by how well the system does.
1085000	1088000	Versus deep learning, where are there any rules there?
1088000	1090000	Are you just telling the system,
1090000	1092000	if you get a point that's good,
1092000	1094000	experiment until you do,
1094000	1098000	and it gets basically the digital equivalent of a doggy treat
1098000	1100000	every time it does well,
1100000	1102000	and then tries to do more of that well.
1102000	1103000	That's right.
1103000	1104000	So that's exactly the difference.
1104000	1107000	So expert systems, you as the human programmers
1107000	1111000	and designers are trying to break down this complex problem,
1111000	1114000	be that playing chess or playing an Atari game,
1114000	1116000	into the set of heuristics and rules.
1116000	1119000	So sometimes there are rules, but they could be probabilistic,
1119000	1121000	so there could just be heuristics,
1121000	1124000	and that's what the system then uses to try and make decisions.
1124000	1127000	So it's effectively using what it's been given.
1127000	1130000	But then with machine learning, using techniques like deep learning,
1130000	1132000	type of machine learning or reinforcement learning,
1132000	1134000	which is the doggy treat thing that you mentioned.
1134000	1136000	So you get a point, so it gets a reward,
1136000	1139000	and then it's more likely to do those actions again.
1139000	1142000	Those things learn for themselves.
1142000	1144000	So you just give it the high-level objective.
1144000	1147000	You say win a game, get a certain number of points,
1147000	1150000	and then it has to figure it out effectively
1150000	1152000	what those heuristics are for itself.
1152000	1154000	And so we're going to talk more about this,
1154000	1158000	but it's in this divergence where, as I understand it,
1158000	1160000	the question of alignment problems really begins
1160000	1163000	to creep into the industry.
1163000	1165000	Because if I'm encoding all these rules,
1165000	1167000	if I'm encoding my understanding into a computer,
1167000	1169000	then I might miss things,
1169000	1171000	but the computer is basically running off
1171000	1173000	of what I've told it to do.
1173000	1175000	Whereas if I've told it to get points,
1175000	1177000	and it doesn't even know what it's doing
1177000	1178000	except for getting points.
1178000	1180000	I mean, as you say, it's learning from pixels.
1180000	1182000	It doesn't have an understanding of the game Pong.
1182000	1184000	And ultimately, it's winning points,
1184000	1186000	but it still may not know it's playing Pong, right?
1186000	1188000	I've never told it it's playing Pong.
1188000	1193000	It's not working with that kind of generalized sense of the situation.
1193000	1194000	It might get points,
1194000	1196000	but if it can figure out another way to get points,
1196000	1197000	it'll do that too.
1197000	1201000	Let's talk a bit about what it means in terms of
1201000	1205000	what the system is doing versus what we often think it's doing
1205000	1208000	that comes from when it is doing the learning itself.
1208000	1211000	You know, this is a very interesting question of
1211000	1213000	when you do the learning itself,
1213000	1215000	you can guide that learning by giving it
1215000	1217000	certain types of data to learn from.
1217000	1219000	You can set the high level objectives.
1219000	1221000	You could even set sub-objectives as well
1221000	1223000	to kind of guide it on the way.
1223000	1224000	How do you win a game?
1224000	1227000	Well, you've got to get a certain number of points
1227000	1228000	or something like that.
1228000	1231000	So you can sort of give it some sub-objectives
1231000	1233000	or it can discover that for itself.
1233000	1236000	But really, you're sort of more coaxing the system
1236000	1238000	rather than with the logic systems
1238000	1240000	where you're directly programming that in.
1240000	1243000	So you have a little bit less direct control over that.
1243000	1245000	And that, of course, is then linked to
1245000	1246000	what you mentioned, the alignment problem,
1246000	1248000	which is sometimes you may care
1248000	1250000	not just about the overall outcome,
1250000	1252000	you may care about how it got there, right?
1252000	1255000	And if you do, then it matters the way that it solves it.
1255000	1258000	And so then you're kind of leaving the solution
1258000	1260000	or the type of solution or the approach
1260000	1262000	up to the system itself.
1262000	1264000	And what you're caring about is the objective at the end.
1264000	1266000	So if you care about the approach too,
1266000	1269000	then you have to add extra constraints into the system
1269000	1272000	or give it some feedback about the approach too,
1272000	1274000	which is this sort of reinforcement learning
1274000	1276000	with human feedback that was now in vogue,
1276000	1280000	where you can get that feedback directly from human ratings
1280000	1282000	or you can do it another way.
1282000	1284000	You can give it sort of further sub-objectives
1284000	1288000	that help it guide it as to what type of solution you want.
1288000	1290000	Well, we've talked a bit here about deep learning,
1290000	1292000	but can you describe reinforcement learning?
1292000	1294000	What is it? How does it differ from deep learning?
1294000	1296000	How do they work together?
1296000	1298000	So both deep learning and reinforcement learning
1298000	1300000	are types of machine learning,
1300000	1301000	and they're very complementary.
1301000	1302000	And we specialize in both
1302000	1304000	and have done from the start of DeepMind.
1304000	1307000	So deep learning is on your hierarchical neural networks,
1307000	1310000	really complex stacks of neural networks,
1310000	1313000	very loosely modeled on brain neural networks.
1313000	1315000	And the objective of those neural networks
1315000	1318000	is to learn the statistics of the environment
1318000	1320000	that they find themselves in
1320000	1322000	or the data stream that they're given.
1322000	1323000	So you can think of the neural network,
1323000	1326000	the deep learning as building a model of the situation.
1326000	1328000	And then the reinforcement learning part
1328000	1332000	is the bit which does the planning and the reward learning.
1332000	1336000	So effectively, it's kind of like a reward-seeking system
1336000	1339000	that is trying to solve an objective that you give it.
1339000	1343000	So for example, we'll be in a game trying to maximize the score.
1343000	1346000	So for every point it gets, it's sort of like a little reward.
1346000	1349000	Animals and including humans learn with reinforcement learning.
1349000	1351000	It's one of the types of learnings we do.
1351000	1353000	And you see that really well with, as you mentioned earlier,
1353000	1355000	with dogs and you give them a treat
1355000	1357000	if they do something that you like or they're well-behaved.
1357000	1360000	And then they're more likely to do that again in future.
1360000	1363000	And that's exactly very similar in a digital form
1363000	1366000	with these reinforcement learning systems that we build.
1366000	1368000	Effectively, they get treats, a reward,
1368000	1371000	when they achieve a certain sub-objective.
1371000	1373000	And so the cool thing is, is that you can combine
1373000	1375000	these two types of systems together,
1375000	1377000	and that's called deep reinforcement learning,
1377000	1380000	where you have the model, and then you also have this
1380000	1383000	goal-seeking or reward-seeking planning system on top
1383000	1387000	that uses that model to reach its objectives.
1387000	1389000	So I want to give ahead a bit now to the system
1389000	1392000	that most people at least used to know DeepMind for.
1392000	1394000	And that's AlphaGo.
1394000	1396000	So tell me what AlphaGo is,
1396000	1399000	how it differs from the Pong system we're talking about here,
1399000	1403000	why Go was an important benchmark or milestone
1403000	1405000	to try to topple?
1405000	1408000	Yeah, so in effect, AlphaGo was the extension
1408000	1411000	of the work we'd done in Atari,
1411000	1413000	but sort of the pinnacle, really,
1413000	1415000	of what you could achieve in games AI.
1415000	1418000	So Deep Blue, as I said earlier, beat Gary Kaspar
1418000	1420000	for chess in the 90s.
1420000	1423000	He won big first pinnacle in games AI,
1423000	1425000	but the next sort of Mount Everest, if you like,
1425000	1427000	was beating the world champion at Go.
1427000	1429000	And you couldn't use expert systems,
1429000	1431000	you had to use these learning systems,
1431000	1434000	because we as even as human, the best human Go players,
1434000	1436000	they don't understand the game well enough
1436000	1439000	to break it down into these heuristics and sub-problems.
1439000	1441000	It's also partly to do with the nature of Go
1441000	1444000	as a very aesthetic game, it's a very intuitive game
1444000	1446000	and with a lot of patterns.
1446000	1448000	So it's quite different from chess,
1448000	1450000	even top Go players will tell you
1450000	1452000	why did they play a certain move,
1452000	1454000	they'll say it just felt right.
1454000	1456000	So they're kind of using their intuition,
1456000	1458000	which is not something that we often associate
1458000	1460000	with computer programs, right?
1460000	1463000	Being able to do something that mimics intuition.
1463000	1466000	And so with AlphaGo, what we did is we built a neural network
1466000	1468000	that modeled the game of Go,
1468000	1471000	figured out what were good moves in certain positions,
1471000	1473000	who was likely to win from a certain position,
1473000	1475000	the probability of either side winning,
1475000	1477000	and that's what the neural network was predicting.
1477000	1480000	And then on top of that, we overlaid this reinforcement learning system
1480000	1484000	that would make plans, do Monte Carlo tree search,
1484000	1486000	using the model to guide its search
1486000	1489000	so that it didn't have to search millions and millions of moves
1489000	1491000	and it would be intractable because it goes too complicated
1491000	1494000	to search everything, you have to narrow down your search.
1494000	1497000	And so it used the model to search the most fruitful paths
1497000	1499000	and then try and find the best move
1499000	1502000	that would most likely get it to a winning position.
1502000	1505000	And so it was a kind of culmination
1505000	1507000	of five, six years worth of working,
1507000	1509000	our work in deep reinforcement learning.
1509000	1512000	What I always found striking about that story
1512000	1515000	is so you beat one of the Go World champions,
1515000	1517000	it's this big moment,
1517000	1520000	and then shortly thereafter you beat yourself,
1520000	1522000	which is to say you have AlphaGo,
1522000	1525000	which is the system that beats Elisa Dole,
1525000	1527000	and then you create AlphaZero,
1527000	1530000	a system that just stomps AlphaGo.
1530000	1533000	So what was the difference between AlphaGo and AlphaZero
1533000	1536000	and what was to be learned there?
1536000	1539000	So AlphaGo was a pretty general system
1539000	1542000	in that it learned for itself the motifs
1542000	1544000	and the strategies around Go,
1544000	1546000	and in fact it created new strategies
1546000	1548000	very famously in that in the World Championship match
1548000	1550000	that had never been seen before,
1550000	1553000	even though we've played Go for thousands of years now
1553000	1555000	to a couple of thousand years.
1555000	1557000	So that was pretty incredible to see.
1557000	1560000	The first version of AlphaGo actually was bootstrapped
1560000	1562000	by looking at all human games
1562000	1564000	that had been played on the Internet,
1564000	1566000	and there's a lot of Internet Go servers
1566000	1569000	in a very popular career in Japan and China.
1569000	1571000	So it's using human games as training data?
1571000	1573000	It used human data as training data.
1573000	1575000	It also had specific things,
1575000	1578000	knowledge about Go encoded in it
1578000	1580000	to do with the symmetry of the board
1580000	1582000	and some other things that were specific to Go.
1582000	1584000	So it was a pretty general system,
1584000	1586000	but it was specialized around Go data,
1586000	1588000	the human data that it learned from,
1588000	1590000	and there were some specific things about Go.
1590000	1592000	Now, once we beat the World Champion, Lisa Doll,
1592000	1594000	we then, this is quite common for us,
1594000	1596000	is once we do that we always try
1596000	1598000	and look back at our systems, in this case AlphaGo,
1598000	1600000	and we try and remove anything
1600000	1602000	that was specific to that task
1602000	1604000	to make it more and more general.
1604000	1606000	And so that's what AlphaZero was.
1606000	1608000	AlphaZero was the next version,
1608000	1611000	and that is able to play any two-player game,
1611000	1613000	so it doesn't matter whether it's Go or Chess
1613000	1616000	or Backgammon, any game you can put it in there,
1616000	1618000	or Japanese chess to be called Shogi,
1618000	1620000	any two-player game.
1620000	1622000	And it doesn't need any human data either,
1622000	1625000	because what it does, it starts off completely random.
1625000	1628000	So imagine a blank slate neural network
1628000	1631000	doesn't really know anything about the game or strategies,
1631000	1633000	and what it does is it plays itself
1633000	1635000	millions and millions of times,
1635000	1637000	different versions of itself,
1637000	1640000	and it learns from its own data and its own experience,
1640000	1643000	and then eventually it becomes actually stronger
1643000	1646000	than these individual programs like AlphaGo
1646000	1648000	that were trained on human data,
1648000	1650000	and it doesn't require any human data.
1650000	1652000	It starts literally from random
1652000	1655000	and explores the space of Go or Chess for itself.
1655000	1657000	And of course it means it also comes up
1657000	1659000	with incredible new strategies,
1659000	1663000	not constrained by what humans have played in the past,
1663000	1665000	because it doesn't have any knowledge of that.
1665000	1667000	One of the things I think about,
1667000	1669000	and that we will talk about later in this conversation,
1669000	1672000	is the question of how smart something
1672000	1674000	that is working with our data
1674000	1676000	and working with the world as we understand it
1676000	1679000	can really become, if that tops out somewhere.
1679000	1681000	And one version of saying,
1681000	1683000	not that smart might be to say,
1683000	1685000	well, it's kind of constrained by what we know.
1685000	1687000	It's got to work with what we know,
1687000	1689000	and if we haven't done that much on something,
1689000	1691000	well, it can't do that much on something in most cases.
1691000	1694000	But this is a case where, with some very basic rules,
1694000	1696000	it actually turned out that it was being held back
1696000	1698000	by what we knew.
1698000	1701000	That because human beings are prone to fattishness,
1701000	1704000	because we follow in the footsteps of those who came before us,
1704000	1706000	because we're taught by others,
1706000	1708000	and they tell us, no, that would be a crazy move.
1708000	1711000	You know, everybody who's done it before did it this way.
1711000	1713000	I mean, that does help us get better, right?
1713000	1717000	Cultural learning and evolution is the core of our species advancement.
1717000	1721000	But it also turns out that that means huge swaths
1721000	1724000	of useful strategy, information, ideas
1724000	1728000	have been cut off the board because we just don't do that.
1728000	1732000	And so in terms of reasons to think these systems
1732000	1736000	could really be remarkable from our perspective,
1736000	1739000	the fact that it was being encumbered
1739000	1741000	by everything we knew about Go,
1741000	1745000	as opposed to launched forward by everything we knew about Go,
1745000	1749000	to maybe put less sentiment on the word here,
1749000	1751000	just strikes me as profound.
1751000	1753000	Yeah, look, it's an interesting take.
1753000	1757000	I mean, of course, this is what happens with our cultural civilization,
1757000	1760000	is that we do get into local maximas, one could say,
1760000	1763000	in something relatively prescribed like a game.
1763000	1766000	In fact, I talked to the Go experts after AlphaGo
1766000	1769000	made these new strategies, most famously Move 37
1769000	1771000	on Game 2 of the World Championship match,
1771000	1773000	with this astounding new move,
1773000	1776000	and I asked all the Go experts afterwards about that move.
1776000	1779000	And they said, we just told that that's a bad move to make
1779000	1781000	in that early in the game,
1781000	1783000	but they can't really explain why.
1783000	1786000	They just said their teachers would just basically shout at them.
1786000	1789000	It's interesting that that is a cultural norm then
1789000	1792000	that actually limits our creativity and exploration.
1792000	1795000	So what I'm hoping is these systems will expand our own minds,
1795000	1798000	and I think this is actually what's happening Go and in chess,
1798000	1800000	again, in very prescribed areas,
1800000	1803000	but where people have started being more creative themselves.
1803000	1804000	Oh, actually, we don't.
1804000	1806000	Maybe we should question some of those cultural norms
1806000	1808000	and perhaps we would get further.
1808000	1811000	I believe that's what has happened in Go and other things.
1811000	1815000	Now, the cool thing is coming back to using AI systems for science.
1815000	1818000	If you think about that, then science is the pursuit of new knowledge
1818000	1820000	and understanding.
1820000	1823000	And so you can now see, I think, of course, in games,
1823000	1825000	it's just games, and they're fun,
1825000	1828000	and I love games to death and my huge passion of mine,
1828000	1830000	but in the end, it's just a game.
1830000	1833000	But for science, you're actually discovering a medicine,
1833000	1835000	you're discovering important new knowledge.
1835000	1837000	There's no reason to assume that isn't going on
1837000	1840000	in another cultural activity, in this case, science.
1840000	1843000	And I think these tools could in themselves
1843000	1846000	help us discover new area regions of knowledge,
1846000	1851000	but also inspire the human experts to explore more as well in tandem.
1874000	1877000	Let's shift into science,
1877000	1882000	because this is where DeepMind begins creating something
1882000	1886000	that isn't just winning games, but is actually creating an advance.
1886000	1888000	And I've said before on this show many times
1888000	1891000	that of all the AI systems that have been released,
1891000	1894000	the one I've always been most impressed by and interested in is AlphaFold,
1894000	1897000	which is a DeepMind system.
1897000	1900000	So tell me what AlphaFold is,
1900000	1903000	what the problem is, how you come to decide that that's something
1903000	1905000	that your systems can take on.
1905000	1908000	I mean, you're doing games, and then you move to this.
1908000	1909000	What is AlphaFold?
1909000	1912000	So we were doing games as the testing ground,
1912000	1915000	but we always had in mind, and I always had in mind
1915000	1918000	the very thing I was thinking about as a teenager,
1918000	1920000	of using AI as a tool for science.
1920000	1922000	And once we'd mastered a lot of games,
1922000	1925000	the idea was that these systems would be powerful enough
1925000	1927000	and sophisticated enough we could turn them onto
1927000	1930000	very important real-world problems and real-world challenges,
1930000	1932000	especially in the sciences.
1932000	1935000	So AlphaFold, we pretty much started the day after we got back
1935000	1938000	from Korea in 2016 and the Lisa Dolmatch,
1938000	1941000	and that was our next big grand challenge.
1941000	1944000	And AlphaFold is our program to try and solve the problem
1944000	1946000	of protein folding, as it's known.
1946000	1949000	Proteins are the workhorses of biology.
1949000	1952000	Basically, every biological function in the body
1952000	1954000	is mediated by proteins.
1954000	1957000	Proteins are described by their amino acid sequence,
1957000	1960000	so you can think of it loosely as the genetic sequence
1960000	1964000	for a protein, and that's a kind of one-dimensional string
1964000	1966000	of letters you can think of.
1966000	1969000	But in the body, they scrunch up into a 3D shape
1969000	1973000	very, very quickly, and it's the 3D shape of the protein
1973000	1976000	that governs its function, what it does in the body.
1976000	1978000	And so the protein folding problem, in essence,
1978000	1981000	is can you predict the 3D shape of the protein
1981000	1985000	directly from the amino acid sequence?
1985000	1988000	The reason it's so important is that a lot of disease
1988000	1991000	is caused by proteins misfolding or folding wrong,
1991000	1994000	and also if you want to design drugs to combat diseases,
1994000	1997000	you need to know what the surface of the protein,
1997000	1999000	therefore the shape of the protein is,
1999000	2001000	so you know which parts of the protein to target
2001000	2003000	with your drug compound.
2003000	2007000	So it's hugely important for many, many biological research questions.
2007000	2010000	To just give an example of this, people may have heard over time
2010000	2013000	that coronavirus is a spike protein,
2013000	2016000	and that's not just an aesthetic point.
2016000	2021000	The fact that it has this somewhat spiked folding structure
2021000	2023000	is crucial to the way it actually works.
2023000	2025000	Do you want to just maybe use the coronavirus protein
2025000	2027000	as an example of what you're saying?
2027000	2029000	Yeah, so that's a great example,
2029000	2032000	and we worked on that as well with the alpha-fold system.
2032000	2034000	So yes, the spike protein is a thing,
2034000	2037000	in a sense, that sticks out of the virus.
2037000	2040000	So that's what you want to latch on to
2040000	2043000	with a vaccine or a drug to kind of block its function,
2043000	2046000	so it doesn't attach to the body or the body's cells
2046000	2048000	in a certain sort of way.
2048000	2051000	So it's the protein structures that do all the mechanics of that.
2051000	2054000	So if you understand what the protein structure looks like,
2054000	2056000	that spike looks like, the shape of it,
2056000	2059000	you can design something that fits like a glove around it,
2059000	2061000	right, to block its action.
2061000	2063000	So that's a great example of, you know,
2063000	2065000	the criticality of protein structure.
2065000	2072000	And what made you think that protein folding is like games?
2072000	2074000	What is the analogy you're drawing here?
2074000	2076000	When you say, I come back from doing Go,
2076000	2079000	and I decided to work on protein folding,
2079000	2081000	what are you seeing here?
2081000	2083000	Because I would not naturally see a connection
2083000	2085000	between those two questions.
2085000	2087000	No, it seems quite far apart,
2087000	2089000	but actually it depends on if you step back
2089000	2091000	and look at it sort of meta-level,
2091000	2093000	they have a lot of things in common.
2093000	2096000	By the way, protein folding is one of a number of scientific problems,
2096000	2098000	big sort of grand challenges,
2098000	2100000	that I came across in my career,
2100000	2103000	actually protein folding I came across in the 90s in my undergrad,
2103000	2105000	because I had a lot of biologist friends
2105000	2107000	who were obsessed about this at Cambridge
2107000	2110000	and actually went on to do their whole careers on protein structures.
2110000	2112000	And they explained to me the problem
2112000	2114000	and I thought it was fascinating,
2114000	2116000	and I also thought it was a perfect problem
2116000	2118000	for AI one day to help with.
2118000	2120000	So I kind of filed it away,
2120000	2123000	and I always later bought it out of the filing system in a sense
2123000	2126000	and decided that that was the first big grand challenge
2126000	2129000	we would apply our learning systems to.
2129000	2131000	The reason I think it's similar,
2131000	2133000	that sort of later on, actually while I was doing my postdoc,
2133000	2135000	the second time I came across protein folding was,
2135000	2137000	you know, in the late 2000s,
2137000	2139000	where there was this game called Fold It,
2139000	2141000	it's called a citizen science game,
2141000	2143000	you may have come across it,
2143000	2145000	so basically a lab had created a game, a puzzle game,
2145000	2148000	where it involved people folding proteins
2148000	2150000	in a three-dimensional interface.
2150000	2152000	I don't think it was a very fun game,
2152000	2155000	but they made it into this sort of quite user-friendly interface,
2155000	2158000	and a few tens of thousands of amateur games players
2158000	2160000	got quite obsessed with it.
2160000	2162000	It got released, I think, in 2008, 2009,
2162000	2164000	and I remember looking into this and thinking,
2164000	2166000	wow, this is pretty fascinating
2166000	2169000	if you can get people to do science by playing a game.
2169000	2171000	That seems like a great idea,
2171000	2174000	so my game's designer part of me was fascinated by it.
2174000	2176000	And so what happened when I looked into it
2176000	2178000	was some of these gamers, who, by the way,
2178000	2180000	a lot of them knew nothing about biology, right,
2180000	2182000	they were just gamers, they'd figured out,
2182000	2184000	presumably, with their pattern matching of their brain,
2184000	2187000	their intuition, that certain counter-intuitive folds
2187000	2189000	of this string of amino acid sequences,
2189000	2191000	you know, the backbone of the protein,
2191000	2194000	led it to the right kind of 3D structure.
2194000	2196000	And they're counter-intuitive in that
2196000	2198000	if you just do the fold that gets you
2198000	2200000	locally to the lowest energy state,
2200000	2203000	which is a kind of greedy search strategy,
2203000	2205000	you end up with the wrong protein fold,
2205000	2206000	you have to do that.
2206000	2208000	So sometimes you have to do local moves,
2208000	2210000	local bends of the protein
2210000	2212000	that actually make the energy landscape worse,
2212000	2215000	effectively the efficiency of the protein structure worse.
2215000	2217000	And then eventually you resolve that.
2217000	2220000	And I remember thinking, combining that
2220000	2222000	with what we then did with AlphaGo,
2222000	2224000	where in AlphaGo, what had we done?
2224000	2226000	Well, what we'd managed with AlphaGo and achieved with AlphaGo
2226000	2229000	is we'd managed to mimic the intuition
2229000	2231000	of these incredible Go masters.
2231000	2233000	So I was thinking, wow, if that was the case
2233000	2235000	for professionals, you spend their whole life on it.
2235000	2237000	And then these amateur gamers,
2237000	2239000	who didn't know anything about biology,
2239000	2241000	were able to, in a couple of cases,
2241000	2243000	fold a couple of proteins correctly.
2243000	2245000	Then why wouldn't we be able to mimic
2245000	2246000	whatever was going on in that?
2246000	2248000	Those amateur gamers' intuition.
2248000	2250000	So that gave me some hope, additional hope,
2250000	2252000	that this would be possible somehow.
2252000	2254000	And of course, this idea of a game,
2254000	2256000	protein folding being a puzzle game,
2256000	2258000	was pretty interesting as an analogy as well.
2258000	2260000	So for several reasons,
2260000	2262000	and also the fact it was a grand challenge
2262000	2265000	and had so many downstream implications
2265000	2267000	and impact if we were to solve it,
2267000	2269000	all those factors sort of came together
2269000	2271000	for me to choose that as the next project.
2271000	2274000	That brings up, I think, another important part of all this.
2274000	2277000	So when you think about how are you rewarding the system,
2277000	2280000	how are you reinforcing the system in Go,
2280000	2283000	well, Go has rules and you know how you score points
2283000	2285000	and you know how you win a game.
2285000	2288000	But when you're predicting the structure
2288000	2293000	of a heretofore-unpredicted protein,
2293000	2295000	how do you know if you're right?
2295000	2297000	How did the gamers know if they were right?
2297000	2299000	How does the system know if it is right?
2299000	2301000	What are you reinforcing against?
2301000	2303000	That was one of the hardest things
2303000	2305000	and often that's one of the hardest things
2305000	2307000	with learning systems and machine learning systems
2307000	2309000	is actually formulating the right objectives.
2309000	2312000	You can think of it as asking the system the right question.
2312000	2314000	And how do you formulate what you want
2314000	2318000	in terms of a simple to optimize objective function?
2318000	2320000	And you're absolutely right, in the real world
2320000	2322000	you don't have simple things like scores
2322000	2324000	or winning conditions, right?
2324000	2325000	That's obviously in games.
2325000	2327000	But with proteins and biology,
2327000	2329000	a lot of the cases there are good proxies for it,
2329000	2332000	like minimizing the energy in the system.
2332000	2335000	Most natural systems try to be energy efficient,
2335000	2339000	so you can sort of follow a gradient of the energy gradient
2339000	2342000	or the free energy in the system and try to minimize that.
2342000	2343000	So that's one thing.
2343000	2345000	The other thing is protein folding.
2345000	2347000	There is a whole history, 50-year history,
2347000	2351000	or more actually, of painstaking experimental work.
2351000	2354000	The rule of thumb is it takes one whole PhD,
2354000	2358000	the whole PhD time, like one PhD student and their entire PhD,
2358000	2361000	for five years to crystallize one protein
2361000	2365000	and then using x-ray crystallography or electron microscopes,
2365000	2367000	complicated pieces of very expensive,
2367000	2368000	complicated pieces of equipment
2368000	2372000	to basically image these incredibly small complex structures.
2372000	2375000	It's unbelievably painstaking difficult work.
2375000	2377000	And so over 50 years of human endeavor
2377000	2379000	from all the labs around the world,
2379000	2381000	structural biologists managed to find the structure
2381000	2385000	around 100,000 to 150,000 proteins.
2385000	2388000	And they're all deposited in this database called the PDB,
2388000	2391000	and that's what we can use as a training corpus,
2391000	2393000	but also we can test our predictions.
2393000	2396000	So you can also do mutagenesis on these systems,
2396000	2398000	so you can do some genetic experiments
2398000	2401000	where you change one of the sequences,
2401000	2403000	one of the residues, one of the amino acids,
2403000	2407000	and then if it's on the surface from the predictor 3D structure,
2407000	2410000	it should change the behavior of the protein.
2410000	2413000	So you can sort of check if your 3D structure prediction
2413000	2416000	saying that this residue is on the surface of the protein,
2416000	2420000	you can sort of flip that out with a genetic mutation study
2420000	2423000	and then see if that's affected the functioning of the protein.
2423000	2425000	So there are sort of various ways after the fact
2425000	2428000	to sort of check whether that's right.
2429000	2431000	Okay, so alpha-fold then.
2431000	2433000	The training data you're using there
2433000	2437000	is the 100 to 150,000 proteins that have been figured out.
2437000	2439000	What you have, as I understand it then,
2439000	2442000	is their amino acid structures.
2442000	2447000	You have the final protein 3D structure.
2447000	2451000	And in the same way that you're setting your original system
2451000	2454000	on PONG, pixel by pixel,
2454000	2457000	try to get to an outcome where you're winning points,
2457000	2462000	you are basically setting alpha-fold loose on this data
2462000	2468000	and saying, try to figure out how to use the amino acid structure
2468000	2472000	to predict the protein 3D structure,
2472000	2474000	and when you do it correctly, you get points.
2474000	2476000	Is that right?
2476000	2478000	Yeah, that's basically how the system works.
2478000	2481000	So you effectively have this amino acid sequence
2481000	2483000	and you're telling it to predict the 3D structure
2483000	2486000	and then you compare it against the real structure.
2486000	2488000	There's various different ways you can compare it,
2488000	2490000	but basically think about comparing
2490000	2493000	where all the atoms end up being in 3D coordinate space
2493000	2496000	and you sort of measure how far it's measured in angstroms,
2496000	2498000	which is a tiny measure, right?
2498000	2500000	Basically the width of an atom.
2500000	2504000	How far away are you from the real 3D position of that atom?
2504000	2506000	And for it to be useful for biologists,
2506000	2508000	you've got to get the accuracy of that.
2508000	2510000	All the atoms in the protein,
2510000	2512000	and there are many, many thousands,
2512000	2515000	within one atom width of the correct position.
2515000	2517000	That's how accurate you have to get it for it
2517000	2520000	to be useful for downstream biology purposes
2520000	2522000	like drug discovery or disease understanding.
2522000	2524000	So effectively the system gets a score
2524000	2526000	from the average error it's making
2526000	2528000	across all the atoms in the structure,
2528000	2531000	and you're trying to get that to less than one angstrom,
2531000	2533000	less than the width of an atom on average.
2533000	2536000	So there's 100 to 150,000 of these,
2536000	2540000	and then there's 100 million, 200 million proteins
2540000	2542000	that we know of?
2542000	2543000	That's right, yeah.
2543000	2546000	So that's not a lot of training data, actually.
2546000	2550000	You all then do something that I understand to be pretty dangerous
2550000	2553000	and usually quite frowned upon,
2553000	2556000	which is the system begins training itself
2556000	2558000	on the predictions it is making.
2558000	2560000	It is generating its own data then,
2560000	2563000	and training itself on that data.
2563000	2565000	And there's just a new paper that came out
2565000	2567000	called The Curse of Recursion
2567000	2569000	about how when AI systems begin training themselves
2569000	2571000	on AI-generated data,
2571000	2572000	the models often collapse.
2572000	2574000	You're basically inbreeding the AI.
2574000	2577000	So how do you do that
2577000	2580000	in a way that does not inbreed your AI?
2580000	2582000	Yeah, you're absolutely right.
2582000	2584000	You have to be extremely careful
2584000	2586000	when you start introducing its own, you know,
2586000	2588000	an AI system's own predictions
2588000	2590000	back into its training data.
2590000	2593000	The reason we had to do it, and this is a very interesting,
2593000	2595000	I think this is the best measure
2595000	2597000	of how difficult this problem was.
2597000	2598000	So as you point out,
2598000	2600000	150,000 data points is tiny
2600000	2602000	for machine learning systems.
2602000	2604000	Usually you need millions and millions of data points, right?
2604000	2607000	Like, for example, with AlphaGo, AlphaZero,
2607000	2609000	we need a 10 million game,
2609000	2611000	something like that, that it played itself.
2611000	2613000	And of course, a game is far simpler
2613000	2615000	than something like a, you know, protein structure in nature.
2615000	2618000	So 150,000 is very, very minimal.
2618000	2620000	I think most people assumed
2620000	2622000	that there was not enough data,
2622000	2624000	nowhere near enough data.
2624000	2626000	And it turned out we had to throw the kitchen sink hat
2626000	2628000	for Fold to make it work, everything we knew.
2628000	2630000	So it's by far the most complicated system
2630000	2631000	we ever worked on,
2631000	2633000	and it's still the most complicated system we've worked on,
2633000	2636000	and it took, you know, five years of work
2636000	2638000	and many difficult wrong turns.
2638000	2641000	And one of the things we had to do was augment the real data,
2641000	2643000	which we didn't really have enough of,
2643000	2646000	use it to build a first version of AlphaFold,
2646000	2648000	and then that was just about good enough.
2648000	2651000	I think we got it to do about a million predictions
2651000	2653000	of new proteins.
2653000	2655000	And when we got it to assess itself,
2655000	2658000	how confident it was on those predictions,
2658000	2660000	and then we sort of triaged it
2660000	2662000	and cut the top sort of 30, 35%
2662000	2664000	so around 300,000 predictions
2664000	2666000	and put them back in the training set,
2666000	2669000	along with the real data, the 150,000 real data.
2669000	2672000	So then we had about half a million structures,
2672000	2674000	obviously including its own predicted ones,
2674000	2676000	to train the final system.
2676000	2678000	And that system then was good enough
2678000	2680000	to reach this atomic accuracy threshold.
2680000	2683000	What that means is we only just about,
2683000	2685000	as a scientific community, had enough data
2685000	2689000	to bootstrap it to do this self-distillation process.
2689000	2691000	And we think the reason that was okay
2691000	2693000	was that we were very careful,
2693000	2695000	first of all, we had enough real data,
2695000	2698000	so there was a mixture of real and generated data,
2698000	2700000	and that was enough to keep it on track.
2700000	2703000	And also, there were lots of very good tests,
2703000	2706000	independent tests of how good these predictions were
2706000	2708000	of the overall system.
2708000	2710000	Because of course, over time,
2710000	2712000	experimenters were depositing new structures
2712000	2715000	onto the database that were past
2715000	2718000	the cut-off training date of when we trained the system.
2718000	2720000	And so we could compare how accurate the system
2720000	2723000	was against those new experimental data.
2723000	2724000	So one thing you're saying here
2724000	2727000	is the system is actually spitting out two things.
2727000	2730000	It's spitting out a protein structure prediction
2730000	2733000	and an uncertainty score.
2733000	2735000	And that's a really actually cool thing
2735000	2737000	about the AlphaFold system too,
2737000	2739000	is that in many machine learning systems,
2739000	2741000	we would love the system
2741000	2744000	to not only produce the output, the prediction,
2744000	2748000	but also measure of its uncertainty about that prediction,
2748000	2750000	a confidence measure about it.
2750000	2753000	And that's actually very important in all modern AI systems,
2753000	2755000	that it would be nice if they, well, it would be very good
2755000	2756000	if they all did that.
2756000	2758000	But actually, very few systems do currently,
2758000	2761000	and it's not known how to do that in general, right?
2761000	2763000	It's an active area of research.
2763000	2765000	But we managed to do that with AlphaFold,
2765000	2767000	and the reason we put so much effort into that
2767000	2770000	is that ultimately, I wanted this to be useful
2770000	2773000	to biologists and scientists and medical researchers,
2773000	2777000	amazing experts, domain experts in their area,
2777000	2779000	but most of them would not know anything
2779000	2782000	about machine learning or care, actually, frankly, right?
2782000	2784000	They're just interested in the structure of the protein
2784000	2786000	so they can go and cure a disease.
2786000	2790000	And so it was really important for this particular task
2790000	2792000	that if these outputs were going to be useful
2792000	2794000	to anyone, researchers down the line,
2794000	2796000	it would have to output its confidence level
2796000	2799000	on the basis of each amino acid.
2799000	2801000	So it color-coded it in really simple ways
2801000	2804000	so that anybody, non-expert of machine learning
2804000	2807000	can understand which parts of the prediction
2807000	2809000	could they trust as an experimentalist
2809000	2811000	and which other parts should they basically
2811000	2813000	probably continue to do experiments on
2813000	2815000	if they wanted to know the real structure,
2815000	2817000	or at least tread with caution.
2817000	2821000	And we wanted the system to really clearly output that
2821000	2824000	and those confidence levels to be really accurate.
2824000	2826000	So in effect, we needed to do that anyway
2826000	2828000	for this tool to be useful downstream
2828000	2830000	to biologists and medical researchers,
2830000	2833000	but we ended up using that same confidence level
2833000	2836000	to allow us to triage our own generated data
2836000	2839000	and put back the more confident, better ones
2839000	2841000	into the training data.
2841000	2843000	So I want to unite this with something
2843000	2845000	that people who've been following AI
2845000	2847000	are probably more familiar with,
2847000	2849000	which is the hallucination problem.
2849000	2853000	So when I use chat GPT or at least different versions of it,
2853000	2857000	it is common that you can ask it a kind of question,
2857000	2859000	like tell me all about this one chemist
2859000	2861000	from so-and-so who did such-and-such,
2861000	2863000	and you can make it up and it can make it up,
2863000	2865000	or maybe you ask it a real question
2865000	2867000	and it just makes up a citation for you.
2867000	2870000	This has been a big problem with the large language models,
2870000	2872000	still a big problem with the large language models.
2872000	2875000	And the theory is that the reason it's a problem
2875000	2878000	is that they're just making predictions, right?
2878000	2880000	They don't know what they're doing
2880000	2882000	in the same way that your system didn't know it was playing pong.
2882000	2884000	They just know that on the internet,
2884000	2886000	on the training data they've been given,
2886000	2889000	this is the word that would be most likely
2889000	2891000	to come next in a sentence.
2891000	2895000	So how come AlphaFold doesn't have
2895000	2898000	this kind of hallucination problem?
2898000	2901000	Yeah, so the current chatbots today
2901000	2904000	have this problem, and partly it's because
2904000	2907000	if they were better able to understand the confidence
2907000	2911000	and the likelihood that what they're putting out putting
2911000	2913000	is correct, they could at some point say,
2913000	2916000	I don't know, that would be better than making something up,
2916000	2919000	or they could sort of caveat it by it might be this,
2919000	2921000	but perhaps you should double check.
2921000	2923000	And in fact, if they were able to do that,
2923000	2925000	they could cross-check their references.
2925000	2927000	That's what they should be doing,
2927000	2929000	using tools perhaps even like search to sort of go,
2929000	2932000	oh, actually, that paper doesn't really exist.
2932000	2934000	Just go and look it up on PubMed.
2934000	2937000	It's not there, even though it's very plausible sounding.
2937000	2939000	And so in fact, the human users,
2939000	2942000	I'm sure you've had experiences, you have to go and look it up.
2942000	2944000	Just a very funny idea, right, to make up a paper,
2944000	2946000	then go search to see if the paper you've made up
2946000	2949000	is actually there, and like, oh, it's actually not.
2949000	2951000	It shows me. Yeah, exactly.
2951000	2953000	But it would be better if it did that internally
2953000	2955000	before we output that to the user.
2955000	2957000	So you never saw its hallucination.
2957000	2960000	And in a way, that's what is missing from the current systems
2960000	2962000	is this actually what AlphaGo does,
2962000	2964000	and at the systems we build,
2964000	2966000	where there's a little bit of thinking time or search
2966000	2968000	or planning that's going on
2968000	2970000	before they output their prediction.
2970000	2974000	Right now, they're kind of almost like idiot savants, right?
2974000	2978000	They just output the immediate thing that just first comes to mind.
2978000	2980000	And it may or may not be plausible,
2980000	2982000	and it may or may not be correct.
2982000	2984000	And we need a bit of sort of, I would say,
2984000	2987000	deliberation and planning and reasoning
2987000	2989000	to kind of almost sanity check
2989000	2991000	what is that the prediction is telling you,
2991000	2994000	not just output the first thing that comes to mind.
2994000	2996000	And the kind of deep reinforcement learning systems
2996000	2998000	we're known for do do that, right?
2998000	3000000	Effectively, the reinforcement learning part, the planning part,
3000000	3002000	calibrates what the model is telling it.
3002000	3004000	It's not just the first most likely move
3004000	3006000	that you play in every positioning go.
3006000	3008000	It's the best move that you want.
3008000	3012000	Is this the difference between building a system
3012000	3015000	that is ultimately working with a structured data set
3015000	3017000	where, at least for some amount of training data,
3017000	3019000	it knows if it has the right answer
3019000	3022000	and something that is using a truly unstructured,
3022000	3024000	and I don't mean that in the technical way,
3024000	3026000	but in the sort of colloquial way,
3026000	3030000	the unstructured data set of life where people just talk
3030000	3034000	and conversations don't have a right answer or wrong answer.
3034000	3037000	If you're inhaling the entire corpus of Reddit,
3037000	3040000	Reddit is not about did you get the right answer,
3040000	3042000	people just talking.
3042000	3044000	And so is what's going on here that because AlphaFold
3044000	3047000	has this 100,000 proteins in there
3047000	3049000	that it knows if they're right or not,
3049000	3052000	and so it knows what it looks like to get something right,
3052000	3054000	that you can then build a system where the point is
3054000	3056000	to get something right, but when you're building
3056000	3059000	these much more generalized language-oriented systems,
3059000	3061000	that that isn't the structure of language.
3061000	3064000	Language doesn't, in some internal way,
3064000	3066000	have like an input and an output
3066000	3068000	where you can see that some outputs were correct
3068000	3070000	and some outputs weren't.
3070000	3072000	Yeah, I think that's the right intuition.
3072000	3074000	I mean, I think language and the way, obviously,
3074000	3076000	on the internet encapsulates a huge slice
3076000	3079000	of human civilization knowledge.
3079000	3081000	It's far more complex than a game
3081000	3084000	and perhaps even proteins in a kind of general sense.
3084000	3087000	I think the difference is actually that in games,
3087000	3090000	especially but also even with protein structures,
3090000	3093000	you can automate the correction process.
3093000	3095000	Like if you don't win the game, then obviously the things
3095000	3098000	you were planning or the moves that you tried to make
3098000	3101000	or the reasoning you did wasn't very good to a certain extent.
3101000	3104000	So you can immediately update on that in an automated way.
3104000	3106000	The same with protein structures.
3106000	3108000	If the final structures that you're predicting
3108000	3111000	have very large errors compared to the known structures,
3111000	3113000	you've obviously done something wrong.
3113000	3115000	There's no subjective decision there, neither.
3115000	3117000	You can just automate that.
3117000	3119000	Of course, with language and knowledge, human knowledge,
3119000	3121000	it's much more nuanced than that.
3121000	3123000	But as we talked earlier, if you hallucinate
3123000	3126000	a new reference to a paper that doesn't exist,
3126000	3128000	that's pretty black and white, right?
3128000	3130000	Like, you know that that's wrong.
3130000	3132000	So there are a lot of things, I think,
3132000	3134000	that could be done far better than we're doing today
3134000	3136000	and we're working really hard on increasing
3136000	3138000	the magnitude, the factuality, and the reliability
3138000	3140000	of these systems.
3140000	3143000	And I don't see any reason why that cannot be improved.
3143000	3146000	But also, there are some things which are a bit more subjective,
3146000	3148000	and then you need human feedback on it,
3148000	3150000	which is why everyone's using reinforcement learning
3150000	3153000	with human feedback to sort of train these systems
3153000	3155000	better to behave in ways we would like.
3155000	3157000	But of course, if you're relying on human feedback,
3157000	3159000	that itself is quite a nosy process
3159000	3162000	and very time-consuming and takes a large effort.
3162000	3165000	And so it's not as quick or as automated
3165000	3167000	as when you have some objective measure
3167000	3169000	that you can just optimize against.
3169000	3172000	So we talked about the exponential curves
3172000	3174000	that you all have seen again and again
3174000	3176000	in the gameplay systems.
3176000	3178000	And you also mentioned that the rule of thumb
3178000	3181000	in proteins is it takes one PhD researcher,
3181000	3184000	their whole PhD, to figure out the structure of one protein.
3184000	3188000	So tell me the timeline then of building off a fold
3188000	3191000	and then of beginning to find the proteins.
3191000	3194000	My understanding is that there's this kind of slow takeoff
3194000	3196000	and then a real takeoff. So what happens here?
3196000	3198000	We worked on Alpha Fold.
3198000	3200000	A couple of versions of Alpha Fold, actually.
3200000	3202000	We had to go to the drawing board at one point
3202000	3204000	when we hit an asymptote over around a four-year,
3204000	3207000	four-and-a-half-year period from about 2016 to 2020.
3207000	3212000	And then we entered it into this competition called CASP,
3212000	3214000	which is you can think of it as like the Olympics
3214000	3215000	for protein folding.
3215000	3218000	So every two years, all of the people working on this
3218000	3221000	from all the labs around the world enter this competition.
3221000	3223000	It's an amazing competition because what they do
3223000	3226000	is they over the last sort of few months, experimentalists
3226000	3228000	give them their protein structures.
3228000	3230000	They've just found literally hot off the press then,
3230000	3232000	but not published yet.
3232000	3234000	So they're unknown to anyone other than the experimental lab
3234000	3236000	that produced it.
3236000	3238000	And they give it to the competition organizers.
3238000	3240000	The competition organizers give it to the competing
3240000	3241000	computational teams.
3241000	3244000	We have to submit within a week our predictions.
3244000	3246000	And then later on at the end of the summer,
3246000	3247000	this happens all over the summer.
3247000	3250000	There's like 100 proteins you get in the competition.
3250000	3252000	And then they reveal the true structures.
3252000	3253000	You know, they get published.
3253000	3256000	And then you compare, obviously you have this double blind
3256000	3259000	scoring system where nobody knew who the competing teams were
3259000	3261000	and nobody knew what the real structures were
3261000	3262000	until the end of the competition.
3262000	3265000	So it's a beautifully designed competition.
3265000	3267000	And the organizers have been running it for 30 years,
3267000	3269000	incredible dedication to do that.
3269000	3271000	And that was another reason we picked this problem to work on
3271000	3273000	because it had this amazing competition.
3273000	3275000	So there was actually a game you could win.
3275000	3277000	There was a game we could win and a leaderboard
3277000	3279000	we could optimize against.
3279000	3281000	And then when those revealed got revealed at the end of 2020,
3282000	3284000	then it was announced in a big conference
3284000	3288000	and the organizers sort of proclaimed that the structure
3288000	3290000	prediction problem or the protein folding problem
3290000	3294000	had been solved because we got to within atomic accuracy
3294000	3297000	on these predictions of these 100 new proteins.
3297000	3300000	And so that was the moment where we knew we had a system
3300000	3303000	that was going to be really useful for experimentalists
3303000	3305000	and drug design and so on.
3305000	3308000	And then the next question was how do we gift this to the world
3308000	3311000	and in a way that all these world researchers and biologists
3311000	3314000	and medical researchers could make the fastest use of
3314000	3317000	to have the biggest impact downstream.
3317000	3320000	And what we realized is not only was the system really accurate
3320000	3323000	alpha fold, it was also extremely fast.
3323000	3326000	So we could fold an average length protein
3326000	3328000	in a matter of seconds.
3328000	3330000	And then when we did the calculation, it was like,
3330000	3333000	well, there are roughly 200 million protein sequences,
3333000	3336000	genetic sequences known to science.
3336000	3339000	We could probably fold all of them in a year.
3339000	3341000	So that's what we set out to do.
3341000	3343000	We started off with the most important organisms,
3343000	3345000	obviously the human proteome.
3345000	3347000	So it's equivalent to the human genome, but in protein space.
3347000	3350000	And then we went to all the important research organisms,
3350000	3353000	you know, the mouse, the fly, the zebrafish and so on.
3353000	3356000	And then some important crops like wheat and rice and so on.
3356000	3358000	They're very important, obviously to humanity.
3358000	3360000	And so then we put all of those 20 out first
3360000	3363000	and then eventually over 2021, we did all of them.
3363000	3366000	And then we put that out as a database, free access database
3366000	3369000	to the world, the research community in collaboration
3369000	3373000	with the European Bioinformatics Institute based in Cambridge.
3373000	3376000	Before we move on to some other work you all are doing,
3376000	3379000	one thing that as I understand it, alpha fold has spun out into now
3379000	3383000	is a group under Alphabet, the Google parent company
3383000	3385000	called Isomorphic.
3385000	3389000	Tell me a bit about Isomorphic and both the sort of scientific
3389000	3392000	theory there, but also it's a very different theory
3392000	3394000	of how AI could make money.
3394000	3397000	Then, you know, we're going to add a chat bot into a search engine.
3397000	3400000	So what's the business theory there?
3400000	3404000	Yeah, so alpha fold is a grand challenge in biology
3404000	3406000	of understanding the structure of proteins.
3406000	3409000	The reason I thought that was so important was
3409000	3412000	because I think it can hugely accelerate,
3412000	3416000	be part of accelerating drug discovery and therefore curing diseases.
3416000	3418000	But it's only one part of the puzzle.
3418000	3420000	So, you know, knowing the structure of proteins,
3420000	3424000	only one small bit of the whole drug discovery process.
3424000	3427000	So there are many other really important things from identifying
3427000	3429000	which proteins you should target, so, you know,
3429000	3432000	maybe through genetic analysis and genomics,
3432000	3436000	all the way to, like, can we design a small molecule,
3436000	3439000	a drug compound, a chemical compound that will correctly
3439000	3442000	bind to that protein and the bit of the protein you want
3442000	3445000	blocked or bind to and not anything else in your body
3445000	3447000	because that's side effects, right?
3447000	3449000	Effectively, that's what makes things toxic is
3449000	3451000	they don't just bind to the thing you want,
3451000	3453000	but they bind to all sorts of other proteins
3453000	3454000	that you didn't want them to.
3454000	3457000	So, in my view, AI is the perfect tool
3457000	3460000	to accelerate the time scales of doing that
3460000	3463000	because that's done right now in Big Pharma
3463000	3467000	in a painstakingly experimental way that takes many, many years.
3467000	3470000	You know, I think the average time of going from a target
3470000	3473000	to a compound you can start testing in clinical trials
3473000	3477000	is, like, five, six years and many, many hundreds of millions of dollars
3477000	3480000	per drug, which is incredibly slow and incredibly expensive,
3480000	3483000	and I think that could be brought down by an order of magnitude
3483000	3486000	using AI and computational techniques
3486000	3489000	to do the exploration part, do that in silico,
3489000	3491000	using AI and computational techniques,
3491000	3494000	and then only at the last step saving experimental work,
3494000	3497000	of course, very important experimental work and wet lab work
3497000	3499000	for the validation step.
3499000	3502000	So, instead of doing all the search, which is the expensive slow part,
3502000	3504000	you just do it for validating the compounds
3504000	3506000	that your AI system has come up with.
3506000	3510000	And so isomorphic is our spin-out, sort of sister company to deep mind,
3510000	3515000	that is tasked with building more alpha folds, breakthroughs,
3515000	3519000	but in adjacent spaces, so more going into chemistry,
3519000	3523000	so designing small molecules, predicting the different properties
3523000	3525000	of those small molecules called admi properties
3525000	3528000	and making sure, like, we minimize things like toxicity
3528000	3530000	and side effects and so on
3530000	3534000	and maximize its potential at binding to the protein that we want.
3536000	3539000	â
3561000	3565000	So there are games, or things that are structured a bit like games-
3565000	3569520	that I'm very excited about the possibility of AI winning or getting unbelievably good
3569520	3574120	at. So drug discovery being one of them. And then there are ones where I'm a little more
3574120	3577800	nervous. I've heard you say, for instance, that from a certain perspective, the stock
3577800	3583440	market very much has the structure of a game. And if I am a very rich hedge fund and a lot
3583440	3587360	of them do algorithmic trading, I mean, if theme park, the game was AI, then definitely
3587360	3591720	what a lot of these hedge funds are doing is AI, they've got a lot of money. If you were
3591760	3597320	thinking about a system to win the stock market, what does that look like? I mean, there's a
3597320	3599520	lot of training data out there. Like, what do you do?
3600600	3604680	Yeah, I think almost certainly some of the top hedge funds must be using, I would have
3604680	3608360	thought, some of these techniques that we and others have invented to trade on the stock
3608360	3612680	market. It has some of the same properties, as you say, I mean, finance friends of mine
3612680	3616680	talk about it as being the biggest game in some ways, right? That's sometimes how it's
3616680	3621560	talked about for better or for worse. And I'm sure a lot of these techniques would work.
3621760	3627480	Now, the interesting thing is whether you just treat the stock market, say, as a series of
3627480	3631600	numbers. And that's one theory that you just treat it as a big sequence of numbers, you
3631600	3635280	know, time series of numbers. And you're just trying to predict the next numbers in the
3635280	3638840	sequence. You know, you could imagine that's analogous to predicting the next word, you
3638840	3642880	know, with the chatbots. So that's possible. My view is it's probably a bit more complex
3642880	3646520	than that, because those numbers actually describe real things, you know, profits and
3646520	3651160	losses, real companies, and then real people running those companies and having ideas and
3651320	3656480	so on. And there's also macroeconomic forces like geopolitical forces and interest rates
3656480	3662040	set by governments and so on. And so my thinking is that probably to understand the full
3662040	3667680	context required to predict the next number in that sequence would require you to understand
3667720	3672480	a lot more about the world than just the stock prices. So you'd somehow have to
3672520	3677200	encapsulate all of that knowledge in a way that the machine could ingest and understand.
3678240	3682760	Well, you need to do that to fully win the game. But to come up with local strategies, it
3682760	3686840	could be very profitable and or very destructive. I mean, one, we already know that all kinds
3686840	3690560	of firms have done that with high speed algorithmic trading. Yes. And two, you could just
3690560	3695120	imagine all kinds of, again, if you're willing to run through a very large search space and
3695120	3698960	try strategies other people don't try, I mean, you know, you could short out this company
3698960	3702600	destroying this competitor such as this other one, you could predict would go up immediately
3702600	3709280	if that happened. And you can imagine very weird strategies being deployed by a system
3709560	3715240	that has the power to move money around and, you know, a lot of data in it and is just
3715240	3717280	getting reinforcement learning for making money.
3717840	3721600	Yeah, you could imagine that. I think that as I understand it, that world, I mean, there's
3721600	3725720	lots of very, very smart people working in that world with algorithms, not necessarily
3725880	3729760	the latest machine learning ones, or the latest statistical algorithms. And they're very
3729760	3735040	sophisticated because obviously they're very incented to do that, given that it's literally
3735040	3741200	money at stake. So I imagine the efficiency is pretty high already in hedge funds and
3741240	3746000	high frequency trading and other things you mentioned, where, you know, there's already
3746160	3752320	ways to slightly game the system, perhaps, that are already quite profitable. And it's not
3752320	3757720	clear to me that sort of more general learning system would be better than that. It may be
3757720	3762240	different from that, but it may already be easier. There may be easier ways, which hedge
3762240	3765040	funds are probably already doing, I assume, to make that money.
3765520	3771560	Well, this actually brings up, I think, a pretty big question for me, which is one of the
3771560	3776320	reasons I wanted to have this conversation with you about Alpha Fold is I think people are now
3776320	3780760	used to thinking about these very generalized large language models, inhale a tremendous amount
3780760	3786920	of data, come up with these patterns, and then, you know, they're able to answer a lot of kinds
3786920	3791800	of questions at a certain level, but not a very high level of rigor. And then there's this other
3791800	3796760	approach, which is building much more bespoke systems. I mean, Alpha Fold can do something
3796760	3801320	amazing in the protein space, and it cannot help me write a college essay, as best as I
3801320	3808600	understand it. And I don't want to put too much of a binary choice here, because I think I know
3808600	3813640	that there is overlap, but I do think there have been sort of two theories of how AI is going to
3813640	3818680	roll out over time. And one is we're going to have lots of different specialized systems that
3818680	3823240	are tuned to do different things, a system to do legal contracts, and a system to do proteins,
3823240	3827560	and a system to check out radiology results, and a system to help kids with their homework.
3828200	3835080	And another is that we are eventually going to pump enough data into GPT-12 or whatever it might be,
3835880	3841240	such it it attains a kind of general intelligence, that it becomes a system that can do everything,
3841880	3846040	that the general system eventually will emerge, and that system will be able to do all these
3846040	3849720	things. And so what you should really be working on is that, can you talk a bit about, because I
3849720	3853320	know you're interested in building a general intelligence system, can you tell me a bit about
3853320	3858200	what you understand to be the path to that now? Do we want a lot of little systems or not little,
3858200	3864280	but specialized, or is the theory here that no, this is right, you want it all in one system
3864280	3867560	that is going to be able to span across functionally every domain?
3868520	3873720	Yeah, that's a fascinating question. And actually, DeepMind was founded in our still
3873720	3878760	our mission is to create that big general system. That's of course, the way the brain works, right?
3878760	3883720	We have one system and it can do, we can do many things with our minds, including science and
3883720	3889160	playing chess and all with the same brain, right? So that's the ultimate goal. Now, interestingly,
3889160	3895320	on the way to that goal, I always believed that we don't have to wait to get into general
3895320	3901560	intelligence or AGI before we can get incredible use out of these systems by using the same
3901560	3907080	sorts of techniques, but maybe specializing them around a particular domain. And AlphaFold
3907080	3911880	is a great example of that, perhaps the best example so far in AI of that. And AlphaGo,
3911880	3916520	obviously, all our game systems were like that too. And so what I think is going to happen in the
3916520	3921560	next era of systems, and we're working on our own systems called Gemini, is that I think there's
3921560	3925640	going to be a kind of combination of the two things. So we'll have this increasingly more
3925640	3930600	powerful general system that you basically interact with through language, but has other
3930600	3935320	capabilities, general capabilities like math and coding, and perhaps some reasoning and
3935320	3939880	planning eventually in the next generations of these systems. One of the things these systems
3939960	3946920	can do is use tools. So tool use is a big part of the research area now of these language models
3946920	3952600	or chatbots. In order to achieve what they want, they need to do, they can actually call a tool
3952600	3957880	and make use of a tool. And those tools can be of different types. They could be existing
3957880	3963480	pieces of software, special case software, like a calculator or maybe like Adobe Photoshop or
3963480	3968040	something like that. So big pieces of software that they can learn how to use using reinforcement
3968040	3974360	learning and learn how to use the interface and interact with. But they can also be other AI systems,
3974360	3980520	other learned systems. I'll give you an example. So if you challenge one of these chatbots to a
3980520	3985000	game, you want to play a game of chess or a game of Go against it, they're actually all pretty bad
3985000	3990520	at it currently, which is one of the tests I give these chatbots is, can they play a good game and
3990520	3994440	hold the board state in mind? And they can't really at the moment, they're not very good.
3995160	3998920	But actually, maybe there's something to say, well, these general systems
3998920	4005080	shouldn't learn how to play chess or Go or fold proteins, there should be specialized AI systems
4005080	4010600	that learn how to do those things, AlphaGo, AlphaZero, AlphaFold. And actually, the general
4010600	4018120	system can call those specialized AIs as tools. So I don't think it makes much sense for the language
4018120	4023560	model to know how to fold proteins. That would seem like an over-specialization in its data
4023560	4027640	corpus relative to language and all the other things that general things that it needs to learn.
4027640	4032920	I think it will be more efficient for it to call this other AI system and make use of
4032920	4037320	something like AlphaFold if it needed to fold proteins. But it's interesting because at some
4037320	4043160	point, more of those capabilities will be forwarded back into the general system over time. But I
4043160	4047400	think at least the next era, we'll see a general system making use of these specialized systems.
4048280	4055240	So when you think about the road from what we have now to these generally intelligent systems,
4056440	4061560	do you think it's simply more training data and more compute, right? Like more processors,
4061560	4067960	more stuff we feed into the training set? Or do you think that there are other innovations,
4067960	4071560	other technologies that we're going to need to figure out first? What's between here and there?
4072280	4078840	I'm in the camp that both needed. I think that large multimodal models of the site we have now
4078840	4084840	have a lot more improvement to go. So I think more data, more compute, and better
4084840	4091560	techniques will result in a lot of gains and more interesting performance. But I do think there are
4091560	4096920	probably one or two innovations, a handful of innovations missing from the current systems
4097000	4100920	that will deal with things that we talked about like factuality, robustness,
4100920	4106040	in the realm of planning and reasoning and memory that the current systems don't have.
4106040	4110360	And that's why they fall short still of a lot of interesting things we would like them to do.
4110360	4115640	So I think some new innovations are going to be needed there, as well as pushing the existing
4115640	4122520	techniques much further. So it's clear to me in terms of building an AGI system or general AI system,
4122520	4127960	these large multimodal models are going to be a core component. So they're definitely necessary,
4127960	4132840	but I'm not sure they'll be sufficient in of themselves. You've talked about in interviews
4132840	4139720	I've heard you give before that you don't want to see this pursuit develop into a move fast and
4139720	4144440	break things kind of race. At the same time, you're part of Google. They just aligned actually all
4144440	4149000	of Google AI under you. There used to be two groups, DeepMind and Google Brain. Now it's all
4149080	4155000	under your empire. Open AI is aligned with Microsoft. Meta is doing a lot more AI. They're
4155000	4159960	working under Yanlacun, who's like one of the founders in all this. China obviously has a number
4159960	4163720	of systems that seem to be getting better fairly quickly compared to even what we were seeing six
4163720	4170440	months ago. It does feel like a race dynamic has developed. And I'm curious how you think about that.
4171480	4175880	I don't think it's ideal. That's for sure. I think it's just the way that technology is panned
4175880	4180760	out. It's become more of an engineering kind of technology, or at least the phase we're in now
4180760	4185720	versus scientific research and innovation, which was perhaps done over the last decade.
4185720	4190040	And Google and DeepMind were responsible for a lot of those breakthroughs that we've discussed,
4190040	4194840	you know, reinforcement learning, obviously, transformers, which Google research invented
4194840	4201640	that underpin all of the modern systems, very critical breakthrough. And so give Google credit,
4201640	4207000	they just released publicly. Yes, exactly. So they released publicly, published it available,
4207000	4213160	and everyone uses that now, including Open AI. And so that underpins the kind of technologies
4213160	4219640	and systems that we see today. And I would prefer it if we took a scientific approach to this as
4219640	4225320	a field and as a community and an industry where we were optimistic about what's coming down the
4225320	4229960	line. Obviously, I worked on AI my whole career because I think it's going to be the most beneficial
4229960	4236520	technology for humanity ever, cure all diseases and help us with energy and also sustainability,
4236520	4241400	all sorts of things. I think that AI can be an incredibly useful tool for, but it has risks.
4241400	4245480	It's a dual use technology. And like any new transformative technology, and I think AI will
4245480	4250920	be one of the most transformative in human history, it can be used for bad too. And so we have to
4250920	4255880	think all of that through. And I would like us to have not move fast and break things like you say,
4255880	4261480	and actually be more thoughtful and try and have foresight rather than hindsight about these things.
4261480	4265160	We're not going to get everything right with a fast moving cutting edge technology like AI
4265160	4270520	first time, but we should try and minimize and think very carefully about the risks at each stage
4270520	4276280	and try and mitigate those as far as possible while making sure we're bold and brave with the
4276280	4281640	benefits. And so we have this mantra of being bold and responsible. And I think there's a creative
4281720	4286120	tension there, but it's sort of intentional between those two things. When you look back at the
4286120	4291480	technology, perhaps one of the last big technologies of the last decade or two has been social media,
4291480	4296760	I think that embodies this view of like move fast and break things. And I feel like that has,
4296760	4301880	of course, had huge benefits and huge growth for certain companies. And it's been very beneficial
4301880	4306920	in many ways, but it also had some unintended consequences that we only as a society started
4306920	4312760	realizing many, many years later once it had reached huge scale. I would like us to avoid that,
4312760	4317560	if possible, with AI to the extent that that's possible. There is also commercial realities
4317560	4323080	and geopolitical issues. And we are in this sort of race dynamic. And what I hope is that
4323080	4328040	there will be a sort of cooperation actually at the international scale on the safety and
4328040	4333640	technical risks as these systems become more and more powerful. I want to talk about one benefit
4333720	4337080	that you mentioned in passing there. And then I want to talk through some of the risks more
4337080	4342600	specifically. You mentioned help us work on energy. And we've talked a lot here about protein
4342600	4347320	folding. We've talked about the applicability to drug discovery. I think the idea that AI could
4347320	4354200	help us with clean energy is something people often hear said, but don't get a lot of details on.
4354200	4359000	But one of the systems you're building or projects you're working on is around nuclear fusion
4359000	4363400	and stabilizing nuclear fusion. So I don't want to spend a ton of time here, but just
4363400	4368520	put some meat on the bones of that idea. Can you just talk about what you're doing here and why AI
4368520	4374200	might be well suited to it? Yeah, I think AI can actually help with climate and sustainability
4374200	4379480	in a number of ways, at least three different ways I think of. One is optimizing our existing
4379480	4383720	infrastructure. So we get more out of the same infrastructure. We have a really good example
4383720	4389720	of that. Actually, we used a similar system to AlphaGo to control the cooling systems in a data
4389720	4395000	centers, in these massive data centers that run all of our compute. They use a huge amount of energy
4395000	4399720	and we actually managed to save 30% of the energy the cooling systems used by more efficiently
4399720	4405480	controlling all the parameters. Secondly, we can monitor the environment better automatically,
4405480	4410360	you know, deforestation and other things, forest fires, all of these types of things using AI.
4410360	4414760	So that's helpful for NGOs and governmental organizations to keep track of things. And
4414760	4419960	then finally, we can use AI to accelerate breakthrough, new breakthrough technologies. And
4419960	4424600	our fusion work is a good example of that, controlling the plasma incredibly hot, hotter than
4424600	4429400	the surface of the sun. So it can't touch the sides of the magnets and so on in these big machines
4429400	4434600	called Tokamaks that control this plasma, super hot plasma that is generating the electricity.
4434600	4439560	And we use AI, our sort of reinforcement learning systems to predict effectively what the shape of
4439560	4444520	the plasma is going to be. So in milliseconds, we can change the magnetic field by changing the
4444520	4450760	current going in the magnets to keep hold of the plasma in place so it doesn't go out of control.
4450760	4456040	So that's a huge problem in fusion and one of the big issues with getting fusion working. But
4456040	4459640	there are also other ways I could imagine I could help in things like material design,
4459640	4464920	designing better batteries, better solar panel technologies, superconductors and so on, which
4464920	4469880	I think AI will be able to help with down the line. So I want to hold that there. And then I want
4469880	4477160	to talk about a risk here, which is one of the things I see happening is ever since GPT-3 was
4477160	4482840	hooked up to chat GPT, and people could begin interfacing with it in natural language, there's
4482840	4490600	been a huge rush towards chat systems, towards chat bots. And this is, I know, an oversimplification,
4490680	4497080	but I do think there's an idea here about, are we making systems that are designed to do what
4497080	4501800	humans can do, but a little bit better? Are we making systems that what we have built here is
4501800	4507400	something meant to seem human to humans? Or are we making systems that are actually very inhuman,
4507400	4511720	that are doing what humans cannot do because they can think in a way humans cannot think,
4511720	4515800	or more to the point calculate in a way humans cannot calculate. And AlphaFold,
4515800	4519400	the nuclear fusion system you're talking about, those strike me as more in that area.
4520040	4523640	And I don't want to say there's necessarily a sharp choice between the two because we've talked
4523640	4529720	about the possibility of general intelligence systems too. But there is where investment goes.
4529720	4535080	There is where the corporate priorities are. There is where the best engineers are working.
4535880	4541960	And now you have these very big companies that are basically in a battle for search and enterprise
4541960	4546760	software funding, right? They want to get subscriptions to Microsoft Office 365 up,
4546760	4551400	and Google doesn't want Bing to take its market share. And one thing that I worry about a bit
4551400	4556120	is that I see a lot more possible benefit for humanity from these more scientific inhuman
4556120	4562200	systems, but that the hype and the investment and the energy is going towards these more human,
4562200	4568120	more kind of familiar systems that I worry are not going to be as beneficial. And so one risk
4568120	4574120	I see is simply that the business models are not going to be well hooked to public benefit.
4574120	4578120	And you said a minute ago, we sort of were leaving the scientific research period of this
4578120	4582200	and entering into the competitive period of this. I think of something that kind of kept
4582200	4586360	deep mind a little bit apart from it. Always you're in London and you guys always seemed a
4586360	4590280	little bit more like you're on the scientific path and now you have to be on top of all Google.
4590280	4594040	How do you think about this tension? Yeah, it's a very interesting question. I think about this
4594040	4598920	all the time. And you're right, there is that tension. And I think all of, let's say the venture
4598920	4603720	capitalist world and so on has almost sort of lost their minds over chatbots, right?
4603720	4608120	And all the money's going into there. I think even in our new guys as Google deep mind,
4608120	4614120	we're going to keep pushing really hard on both frontiers. Advancing science and medicine is
4614120	4619320	always going to be at the heart of what we do and our overall mission for the benefit of humanity.
4619320	4625880	But we are also going to push hard on next generation products with incredible new experiences for
4625880	4630280	billions of users that help them in their everyday lives. I'm kind of equally excited about potential
4630360	4636520	of both types of things. And so that involves us continuing to invest and work on scientific
4636520	4641400	problems like AlphaFold or isomorphic labs is doing drug discovery, fusion, except for quantum
4641400	4648280	chemistry, mathematics, many, many of our nature and science papers, as well as doubling down on
4648280	4653560	these new types of chatbot interfaces and so on. I don't see them as sort of human and inhuman. It's
4653560	4659160	more like the AlphaFold things are scientific tools for experts to use and enhance their work
4659160	4663560	so they can accelerate their very important research work. And then the other hand, at the
4663560	4667880	moment, I think chatbots are more of a fun entertainment thing. I mean, of course, you
4667880	4672120	can do your homework on them and you can do amusing things and it's quite helpful. But I think there's
4672120	4676680	so much more to come in that space. And I think where I see them joining together is what we
4676680	4680440	discussed earlier about these general systems, perhaps that you interact with in language.
4680440	4685240	There's nothing wrong with that because language is the mode that we all can use rather than coding
4685240	4691080	or mathematics. Language is the simplest thing for everybody to use to interface with these systems,
4691080	4697320	but they could call a bunch of specialized systems and specialized tools and make use of them. And
4697320	4701880	so I actually think there's quite an interesting combination to come by pushing the frontiers
4701880	4705560	of both of those things. And that's what we're planning to do going forward.
4706280	4713560	You recently signed a letter. It was alongside Sam Altman, who leads OpenAI and Dario Amade,
4713560	4717720	who is a top OpenAI person, now leads Anthropic. And letter simply says,
4718360	4723160	mitigating the risk of extinction from AI should be a global priority alongside other
4723160	4729320	societal scale risks, such as pandemics and nuclear war. Why do you believe there is any
4729320	4736200	risk of extinction from AI at all? Well, that letter was a kind of compromise thing. I think
4736200	4741320	it's not 30 words long. So of course, all the nuances are missing from a letter such as that.
4741400	4745960	And at some point soon, we'll put out a fuller statement about our position.
4745960	4750040	The only thing I was agreeing with there is that this technology has such potential
4750040	4756280	for enormous, enormous good, but it's a dual-use technology. So if bad actors get hold of it,
4756280	4760120	it could be used for bad things. There are near-term harms we have to be careful of,
4760120	4764520	like, deep fakes. And we need to address with things like watermarking, and we're working on
4764520	4768120	that. And I think a lot of that will come out later this year. And then there are technical
4768120	4773160	risks. This alignment problem that we discussed earlier on how to make sure these systems do
4773160	4777560	what we want. And we set them the right objectives and the right values, and they can be contained
4777560	4782840	and controlled as they get more powerful. So there's a whole series of at least three buckets of
4782840	4788360	worry. And I think they're all equally important, actually, but they require different solutions.
4788360	4794280	And one of them is this longer term people think of as maybe a science fiction scenario of inherent
4794360	4798760	technical risk from these systems, where if we don't build them in the right way,
4798760	4802680	in the limit when they're decades time, when they're very, very powerful,
4802680	4806200	and they're capable of planning and all the things I discussed earlier today, but to the
4806200	4811480	nth degree, we have to be careful with those sorts of systems. I don't think it's likely,
4811480	4815720	I wouldn't even put a probability on it, but there's uncertainty over it. And it's certainly
4815720	4820920	non-zero. I think the possibility that that could go wrong if we're not thoughtful and careful and
4820920	4826440	use exceptional care with these technologies. So I think that's the part I was trying to indicate
4826440	4831960	by signing that was that it's important to have that debate now. You don't want to have that debate
4831960	4838040	on the eve of some kind of technology like that arriving, right? Ten years sounds like a long time,
4838040	4842440	but it's not that long, given the amount of research that would be required and is required,
4842440	4848040	and I think needs to be done to understand the systems better so that we can mitigate any risks
4848040	4853400	that may come about. Well, I think right now when people hear about extinction risk from AI,
4853400	4858760	one scenario that they've now been told to think about and more people do is the AI itself getting
4858760	4863080	out of control or turning the whole world into paperclips or whatever it might be. But I want
4863080	4868520	to talk about another here, which is more along the lines of our conversation. So you build Alpha
4868520	4873000	Fold now through isomorphic, you're building a whole suite of tools to search through the
4873000	4880600	molecular space, the protein space to better understand how to predict the functions of and
4880600	4890280	then eventually create bespoke proteins, molecules, etc. And I think one slightly less sci-fi version
4890280	4896120	of extinction or at least mass harm that you can imagine here is through synthetic biology,
4896120	4902760	through as it becomes very cheap to figure out how to create an unbelievably lethal virus and print
4902840	4908600	an unbelievably lethal virus that in the future, it's actually not that hard for some terrorist
4908600	4915480	organization to use tools like this to make something far beyond if you think back in America,
4915480	4920360	you know, however many years now, when somebody was mailing anthrax around, if it had been very
4920360	4926200	easy for that person to create super smallpox, then you get into something really, really horrifying.
4926760	4930200	And how do you think about that suite of risks? Because you're doing more work in that space
4930200	4933880	really than anyone else, I think. And that's one of the ones that seems actually much near
4933880	4939000	at hand to me. Yeah, we think a lot about that. And we talk with a lot of experts in government
4939000	4944360	and academia about this. And actually, before we released Alpha Fold, we spent several months
4944360	4951320	talking to over 30 experts in biosecurity, biorethics, also Nobel Prize winning biologists
4951320	4956120	and chemists about what we were going to release with the database and what they thought of it.
4956200	4960120	And all of them unanimously actually came back with in that case, the benefits far outweighed
4960120	4964120	the risks. And I think we're seeing all the benefits of that today with millions of biologists
4964120	4968680	around the world using it. But look, going forward, as you get more into chemistry space,
4968680	4973400	one has to think about these things. But of course, we want to cure many terrible diseases too,
4973400	4978520	right? So we have to weigh up that enormous benefit there to society with these inherent
4978520	4984040	risks that you're talking about. And I think one of these issues is access to these technologies
4984040	4988760	by bad actors, not scientists and people, you know, medical practitioners who are trying to
4988760	4993080	do good with it. But as you say, terrorists, other things like that. And I think that's where
4993080	4998520	actually becomes a question of things like open sourcing, or do you publish these results? Or
4998520	5003320	do you sort of, how secure is your cybersecurity? So you can't be hacked? All of these questions
5003320	5008120	come into play. And I think that's where we're going to have to think a lot more carefully
5008120	5013160	in the next few years, as these systems become more sophisticated about who should get access
5013160	5018360	to those things? How should that be monitored? Can bad actors be shut down if they're using APIs
5018360	5022760	very quickly before they do any harm? Maybe we can use AI there actually to detect what are they
5022760	5027080	trying to design with these systems as well. This can also happen with chatbots too. What are they
5027080	5032440	asking the chatbots, right? So I think there's a role for AI to play there actually as well on the
5032440	5037800	monitoring side. And so the other question though to ask is, and when I've discussed this with experts
5037800	5043480	in biosecurity is, there are known toxins today, like you mentioned anthrax, you can probably find
5043480	5048520	the recipe for that somewhere on the internet or, and people could do that, but you still need a wet
5048520	5054760	lab, and you still need some scientific capability. And so those are areas which are also usually beyond
5054760	5060040	naive bad actor individual, their capabilities, right? It's not just the recipe, how do you actually
5060040	5064360	make it and then distribute it, right? It's actually pretty difficult. And I would argue that's
5064360	5069160	already available today if you want it. It's not that there's no bad toxins that are known.
5069160	5073480	There are some that are quite simple. It's just not that easy to make them to the uninitiated,
5073480	5077320	right? You need a lab and you need access to it and labs can be monitored and so on.
5077320	5081400	There are still a lot of barriers. It's not just a question of understanding the design,
5081400	5085560	but we do need to think about that as well and figure out how we control that information.
5086200	5093000	Right now, the systems and the kind of labs that could create the systems that could become
5093000	5096680	something like general intelligence. I mean, you could count them on two hands, right? Across
5097480	5101800	the United States, across Europe, across China. And over time, there'll be even more than that,
5101800	5106440	but that's I think where we are now. As we get closer, I mean, you were talking about how much
5106440	5112760	can happen here in 10 years. If we're getting to a point where somebody is getting near something
5112760	5117960	like a general intelligence system, is that too powerful technology to be in private hands? Should
5118040	5123560	this be something that whichever corporate entity gets their first controls or do we need something
5123560	5129880	else to govern it? My personal view is that this is such a big thing in this fullness of time.
5129880	5135560	I think it's sort of bigger than any one corporation or even one nation. I think it needs
5135560	5140520	sort of international cooperation. I've often talked in the past about a kind of CERN-like
5140520	5146680	effort for AGI. And I quite like to see something like that as we get closer, maybe in many years
5146680	5152920	from now to an AGI system where really careful research is done on the safety side of things,
5153480	5158440	understanding what these systems can do, and maybe testing them in controlled conditions
5158440	5165880	like simulations or games first, like sandboxes, very robust sandboxes with lots of cyber security
5165880	5170840	protection around them. I think that will be a good way forward as we get closer towards human
5170840	5175480	level AI systems. I think it's a good place to end. So always our final question then.
5175480	5177720	What are three books you would recommend to the audience?
5178920	5184040	Well, I've chosen three books that are quite meaningful to me. So I would say, first of all,
5184040	5190520	Fabric of Reality by David Deutsch. I think that poses all the big questions in physics that I would
5190520	5198600	love one day to tackle with our AI tools. The second book I would say is Permutation City by Greg Egan.
5198600	5204600	I think it's an amazing story, actually, wild story of how interesting and strange I think the
5204600	5211560	world can get in the context of AI and simulations and hyper-realistic simulations. And then finally,
5211560	5216840	I would recommend Consider Fleebers by Ian Banks, which is part of the culture series of novels,
5216840	5221320	very formative for me. And I read that while I was writing Theme Park. And I still think it's
5221320	5227160	the best depiction of a post-AGI future, an optimistic post-AGI future, where we're traveling
5227160	5230840	the stars and humanity sort of reached its full flourishing.
5230920	5233080	Ladies and gentlemen, thank you very much.
5233080	5233880	Thanks very much.
5244840	5247560	This episode of The Israel Clown Show was produced by Roger Karma,
5247560	5251880	fact-checking by Michelle Harris. Our senior engineer is a great Jeff Gelb. The show's
5251880	5256760	production team also includes Emma Flaugau, Annie Galvant, and Kristen Lin. Our music is by Isaac
5256760	5261240	Jones. Audience strategy this week by Christina St. Maluski and Shannon Busta.
5261240	5264840	The executive producer of New York Times' opinion audio is Annie Rose Strasser,
5264840	5266920	and special thanks to Sonia Herrero.
