1
00:00:00,000 --> 00:00:23,000
From New York Times Opinion, this is the Ezra Klein Show.

2
00:00:23,000 --> 00:00:29,000
So I think you can date this era in artificial intelligence back to the launch of chat GPT.

3
00:00:29,000 --> 00:00:35,000
And what is weird if you talk to artificial intelligence people about that is they'll tell you chat GPT.

4
00:00:35,000 --> 00:00:38,000
It was just a wrapper, an interface system.

5
00:00:38,000 --> 00:00:42,000
The underlying system GPT-3 had been around for a while.

6
00:00:42,000 --> 00:00:46,000
I mean, I'd had access to GPT-3 for quite a while before chat GPT came around.

7
00:00:46,000 --> 00:00:53,000
What chat GPT did was it allowed you to talk to GPT-3 like you were a human and it was a human.

8
00:00:53,000 --> 00:00:56,000
So it made AI more human.

9
00:00:56,000 --> 00:01:03,000
It made it more able to communicate back and forth with us by doing a better job mimicking us and understanding us,

10
00:01:03,000 --> 00:01:04,000
which is amazing.

11
00:01:04,000 --> 00:01:11,000
I don't mean to take anything away from it, but it created this huge land rush for AIs that functionally mimic human beings.

12
00:01:11,000 --> 00:01:16,000
AIs that relate as if they are human beings and try to fool us into thinking that they're human.

13
00:01:16,000 --> 00:01:20,000
But I've always been more interested in more inhuman AI systems.

14
00:01:20,000 --> 00:01:26,000
When you ask somebody who's working on artificial intelligence, including people who believe it could do terrible harm to the world,

15
00:01:26,000 --> 00:01:29,000
why are you doing it? What's the point of this?

16
00:01:29,000 --> 00:01:36,000
They don't say, oh, you know, we should risk these terrible consequences because it's fun to chat with chat GPT.

17
00:01:36,000 --> 00:01:42,000
They say, oh, AI, it's going to solve all these terrible scientific problems we have, clean energy and drug discovery.

18
00:01:42,000 --> 00:01:48,000
And, you know, it's going to create an era of innovation like nothing humanity's ever experienced.

19
00:01:48,000 --> 00:01:52,000
There aren't that many examples, though, of AI doing that yet.

20
00:01:52,000 --> 00:01:55,000
But there is one, which you may have heard me mention before.

21
00:01:55,000 --> 00:02:01,000
And that's AlphaFold, the system built by DeepMind that solved the protein folding problem.

22
00:02:01,000 --> 00:02:05,000
And the protein folding problem is that there are hundreds of millions of proteins.

23
00:02:05,000 --> 00:02:08,000
The way they function has to do with their 3D structure.

24
00:02:08,000 --> 00:02:12,000
But even though it's fairly straightforward to figure out their amino acid sequence,

25
00:02:12,000 --> 00:02:16,000
it's very hard to predict how they will be structured based on that.

26
00:02:16,000 --> 00:02:17,000
We were never able to do it.

27
00:02:17,000 --> 00:02:22,000
We were doing it one by one, studying each one for years to try to figure out and basically map it.

28
00:02:22,000 --> 00:02:27,000
And then they build the system, AlphaFold, which solves a problem,

29
00:02:27,000 --> 00:02:32,000
is able to predict the structure of hundreds of millions of proteins, a huge scientific advance.

30
00:02:32,000 --> 00:02:34,000
So how did they build that?

31
00:02:34,000 --> 00:02:37,000
And what could other systems like that look like?

32
00:02:37,000 --> 00:02:44,000
What is this other path for AI, this more scientific path where you're tuning these systems to solve scientific problems,

33
00:02:44,000 --> 00:02:48,000
not to communicate with us, but to do what we truly cannot do?

34
00:02:48,000 --> 00:02:51,000
Demis Asabis is the founder of DeepMind.

35
00:02:51,000 --> 00:02:56,000
DeepMind is owned by Google and recently Asabis was put in charge of all Google AI.

36
00:02:56,000 --> 00:03:00,000
So now it's called Google DeepMind and he runs all of it.

37
00:03:00,000 --> 00:03:02,000
That makes him one of the most important people in the world,

38
00:03:02,000 --> 00:03:05,000
charting the future of artificial intelligence.

39
00:03:05,000 --> 00:03:10,000
So I asked him to come on the show to talk me through the development of AlphaFold,

40
00:03:10,000 --> 00:03:15,000
how it was built, what came before it, what could come after it,

41
00:03:15,000 --> 00:03:21,000
and what that says about the different ways and the different pathways forward for artificial intelligence.

42
00:03:21,000 --> 00:03:24,000
As always, my email is reclineshow at nytimes.com.

43
00:03:29,000 --> 00:03:31,000
Demis Asabis, welcome to the show.

44
00:03:31,000 --> 00:03:32,000
Thanks for having me.

45
00:03:32,000 --> 00:03:35,000
So tell me about the founding of DeepMind.

46
00:03:35,000 --> 00:03:36,000
You were pretty young at that point.

47
00:03:36,000 --> 00:03:38,000
How did you decide to start it?

48
00:03:38,000 --> 00:03:41,000
What was the vision for it?

49
00:03:41,000 --> 00:03:45,000
Well, actually, my background in AI starts from way before DeepMind.

50
00:03:45,000 --> 00:03:50,000
So I started actually in the games industry, writing computer games and some best selling games,

51
00:03:50,000 --> 00:03:56,000
actually games like Theme Park and Black and White and other games that I worked on in my teenage years.

52
00:03:56,000 --> 00:03:59,000
And they all had AI as a core component of the game.

53
00:03:59,000 --> 00:04:00,000
So let's take Theme Park.

54
00:04:00,000 --> 00:04:03,000
It was a simulation game, came out in 1994.

55
00:04:03,000 --> 00:04:10,000
And you basically created a theme park and lots of little people came in, played on the rides and bought stuff from your stalls.

56
00:04:10,000 --> 00:04:12,000
So there's a whole kind of simulation underlying it.

57
00:04:12,000 --> 00:04:15,000
And AI was the core part of the gameplay.

58
00:04:15,000 --> 00:04:20,000
So I've been working on AI for nearly 30 years now in various guises.

59
00:04:20,000 --> 00:04:22,000
So if you look at my career, I've done lots of different things,

60
00:04:22,000 --> 00:04:27,000
but they were all doing something like an effort like DeepMind working on AGI.

61
00:04:27,000 --> 00:04:33,000
And so all the things I did, the neuroscience, the computer science undergrad, PhD in neuroscience,

62
00:04:33,000 --> 00:04:39,000
was all for gathering information and inspiration for eventually what would become DeepMind.

63
00:04:39,000 --> 00:04:41,000
So I played Theme Park back in the day.

64
00:04:41,000 --> 00:04:47,000
And if you ask me now and you say, hey, Theme Park, that game you played in the 90s, was that AI?

65
00:04:47,000 --> 00:04:49,000
I would say no.

66
00:04:49,000 --> 00:04:55,000
There's a classic line to paraphrase that AI is anything the computer can't do yet.

67
00:04:55,000 --> 00:04:58,000
But now you look back, you think, oh, no, that's just a little video game.

68
00:04:58,000 --> 00:05:02,000
So when you say that was AI, what are you saying by that?

69
00:05:02,000 --> 00:05:04,000
And to you, what is artificial intelligence?

70
00:05:04,000 --> 00:05:12,000
And what's just machine learning or statistics or some less impressive function?

71
00:05:12,000 --> 00:05:19,000
Yeah, so back in the 90s and with games like Theme Park at the time, that was pretty cutting edge.

72
00:05:19,000 --> 00:05:25,000
We were using relatively simple techniques, cellular automata and relatively narrow AI systems,

73
00:05:25,000 --> 00:05:29,000
sort of logic systems really, which was in vogue back in the 80s and 90s.

74
00:05:29,000 --> 00:05:35,000
But it was AI in terms of making a machine do something smart and actually adapt to,

75
00:05:35,000 --> 00:05:39,000
automatically adapt to the way the gamer in that case was playing the game.

76
00:05:39,000 --> 00:05:44,000
So the cool thing about Theme Park was that and why it was so successful and sold millions of copies around the world

77
00:05:44,000 --> 00:05:48,000
was that everybody who played it got a unique experience.

78
00:05:48,000 --> 00:05:51,000
Because the game adapted to how you were playing.

79
00:05:51,000 --> 00:05:55,000
It's very primitive by today's standards of learning systems.

80
00:05:55,000 --> 00:05:57,000
But back then it was pretty groundbreaking.

81
00:05:57,000 --> 00:06:05,000
It was one of the first games along with SimCity to do these kinds of complicated simulations under the hood powered by AI.

82
00:06:05,000 --> 00:06:09,000
As to your question about what is AI, I think AI is the science of making machines smart.

83
00:06:09,000 --> 00:06:15,000
And then a sub branch of AI is machine learning, which is the kinds of AI systems that learn for themselves

84
00:06:15,000 --> 00:06:18,000
and learn directly from data and experience.

85
00:06:18,000 --> 00:06:23,000
And that's really what, of course, has powered the renaissance of AI in the last 10, 15 years.

86
00:06:23,000 --> 00:06:25,000
And that's the sort of AI that we work on today.

87
00:06:25,000 --> 00:06:27,000
Can you just spend another second on that?

88
00:06:27,000 --> 00:06:31,000
You mentioned a minute ago that what was happening in Theme Park was logic based systems,

89
00:06:31,000 --> 00:06:33,000
which I think is at least one of the other branches.

90
00:06:33,000 --> 00:06:39,000
Can you describe the difference between some of the more logic based or rules based or knowledge encoded systems

91
00:06:39,000 --> 00:06:44,000
and deep learning and some of the other things that are more dominant now?

92
00:06:44,000 --> 00:06:45,000
Sure.

93
00:06:45,000 --> 00:06:50,000
So if you think of AI as being this overarching field of making machines smart,

94
00:06:50,000 --> 00:06:53,000
then they're broadly speaking two approaches to doing that.

95
00:06:53,000 --> 00:06:59,000
One is the classical approach and the sort of approach that was done for the first few decades of AI research

96
00:06:59,000 --> 00:07:02,000
since the 1950s with logic systems.

97
00:07:02,000 --> 00:07:06,000
So this is the idea of the programmers or the creators of the system.

98
00:07:06,000 --> 00:07:12,000
They effectively solve the problem, be that playing chess or controlling little characters in a game.

99
00:07:12,000 --> 00:07:15,000
And then they would program up these routines and heuristics,

100
00:07:15,000 --> 00:07:21,000
and then effectively the system would then deal with new inputs and execute those heuristics.

101
00:07:21,000 --> 00:07:23,000
And you can be pretty powerful systems.

102
00:07:23,000 --> 00:07:25,000
They're sometimes called expert systems.

103
00:07:25,000 --> 00:07:28,000
And the most famous of that is probably Deep Blue IBM's chess program.

104
00:07:28,000 --> 00:07:32,000
They beat Gary Kasparov, the world chess champion at the time in the 90s very famously.

105
00:07:32,000 --> 00:07:35,000
And that was probably the pinnacle of expert systems.

106
00:07:35,000 --> 00:07:39,000
But the problem with them is that they're very brittle and they can't deal with the unexpected, of course,

107
00:07:39,000 --> 00:07:44,000
because they can only do what the programmers have already figured out and the heuristics they've already been given.

108
00:07:44,000 --> 00:07:46,000
They can't learn anything new.

109
00:07:46,000 --> 00:07:50,000
So machine learning is the other approach to solving AI.

110
00:07:50,000 --> 00:07:53,000
And it's turned out to be a lot more powerful and a lot more scalable,

111
00:07:53,000 --> 00:07:56,000
which is what we bet on as well as when we started DeepMind.

112
00:07:56,000 --> 00:08:00,000
And that's the idea of machines, the systems learning for themselves,

113
00:08:00,000 --> 00:08:05,000
learning the structure, learning of heuristics and rules that they should do for themselves,

114
00:08:05,000 --> 00:08:07,000
directly from data or directly from experience.

115
00:08:07,000 --> 00:08:11,000
The big contrast for our first sort of big famous program was AlphaGo,

116
00:08:11,000 --> 00:08:15,000
which learned how to play the complex game of go for itself

117
00:08:15,000 --> 00:08:21,000
and actually came up with better strategies and heuristics than we could have ever thought of as the human designers.

118
00:08:21,000 --> 00:08:25,000
We're going to get to AlphaGo, but I want to hold for a minute in the founding of DeepMind.

119
00:08:25,000 --> 00:08:27,000
So you've been a game designer.

120
00:08:27,000 --> 00:08:31,000
You become a neuroscientist and you do actually some important peer-reviewed research

121
00:08:31,000 --> 00:08:34,000
and highly cited research in that field.

122
00:08:34,000 --> 00:08:38,000
That's a big jump. You got to work pretty hard to become a neuroscientist is my understanding.

123
00:08:38,000 --> 00:08:40,000
So you're an academia, you're doing this research.

124
00:08:40,000 --> 00:08:42,000
It's going well as best I can tell.

125
00:08:42,000 --> 00:08:45,000
And you start deciding you're going to found this AI company.

126
00:08:45,000 --> 00:08:48,000
So take me in that moment of decision making.

127
00:08:48,000 --> 00:08:54,000
Yeah. So again, if you have in mind that I was planning from, I guess I was around 15, 16,

128
00:08:54,000 --> 00:08:59,000
when I was doing this programming on these computer games and theme park and sort of 17 years old,

129
00:08:59,000 --> 00:09:04,000
that's already when I decided my life was going to be and my career was going to be about AI and making AI happen.

130
00:09:04,000 --> 00:09:08,000
So all the other things I chose were in service of that, including the PhD.

131
00:09:08,000 --> 00:09:10,000
So I did my undergrad in computer science.

132
00:09:10,000 --> 00:09:15,000
I got trained in coding, engineering and mathematical side of computer science,

133
00:09:15,000 --> 00:09:20,000
which I love the theoretical side, Turing machines, all of these types of things, computation theory.

134
00:09:20,000 --> 00:09:25,000
And then I decided for my PhD after I ran my own games company for a while,

135
00:09:25,000 --> 00:09:28,000
I went back to academia to do a PhD in neuroscience.

136
00:09:28,000 --> 00:09:32,000
And I chose cognitive neuroscience because first of all, I've always been fascinated by the brain.

137
00:09:32,000 --> 00:09:38,000
The brain is the only existence proof we have in the universe that general intelligence is possible.

138
00:09:38,000 --> 00:09:41,000
So it seems worthy of studying that specific data point.

139
00:09:41,000 --> 00:09:44,000
It's probably not the only way that intelligence could come about,

140
00:09:44,000 --> 00:09:47,000
but it's certainly the only way that we're aware of and that we can study.

141
00:09:47,000 --> 00:09:49,000
And of course, it's fascinating subjects in itself.

142
00:09:49,000 --> 00:09:53,000
But the reason I did the PhD is I wanted to learn about the brain,

143
00:09:53,000 --> 00:09:56,000
maybe get inspiration for some algorithmic ideas, architectural ideas.

144
00:09:56,000 --> 00:10:01,000
And indeed, that's what did happen in things like memory replay, reinforcement learning and things like this,

145
00:10:01,000 --> 00:10:03,000
that we then used in our AI systems.

146
00:10:03,000 --> 00:10:07,000
And also learn how to do cutting edge research too,

147
00:10:07,000 --> 00:10:11,000
and actually learn how to use the scientific method properly and things like control studies and so on.

148
00:10:11,000 --> 00:10:15,000
Really, you learn all of those practical skills by doing a PhD.

149
00:10:15,000 --> 00:10:18,000
You said something about founding DeepMind that I've always found striking,

150
00:10:18,000 --> 00:10:22,000
which is, quote, I want to understand the big questions, the really big ones,

151
00:10:22,000 --> 00:10:26,000
that you normally go into philosophy or physics if you're interested in.

152
00:10:26,000 --> 00:10:31,000
I thought building AI would be the fastest route to answer some of those questions.

153
00:10:31,000 --> 00:10:34,000
And so a lot of people might want to know about the nature of the universe.

154
00:10:34,000 --> 00:10:36,000
The idea is I'll go work on it directly.

155
00:10:36,000 --> 00:10:40,000
I'll get a physics PhD, a mathematics PhD, a PhD in chemistry or neuroscience,

156
00:10:40,000 --> 00:10:42,000
and I'll go solve those problems.

157
00:10:43,000 --> 00:10:51,000
And your theory of the case was I will get this training and build something else to solve those problems.

158
00:10:51,000 --> 00:10:54,000
Why do you think that intermediary intelligence is necessary?

159
00:10:54,000 --> 00:10:56,000
Why not just do it yourself?

160
00:10:56,000 --> 00:10:57,000
It's actually a great question.

161
00:10:57,000 --> 00:11:00,000
I mean, I've always been fascinated by the biggest questions.

162
00:11:00,000 --> 00:11:02,000
I'm not quite sure why that is,

163
00:11:02,000 --> 00:11:07,000
but I've always been interested in the nature of the universe, nature of reality, consciousness,

164
00:11:07,000 --> 00:11:09,000
the meaning of life, all of these big questions.

165
00:11:09,000 --> 00:11:11,000
That's what I wanted to spend my life working on.

166
00:11:11,000 --> 00:11:13,000
And indeed physics was my favorite subject at school.

167
00:11:13,000 --> 00:11:14,000
I love physics.

168
00:11:14,000 --> 00:11:15,000
I still love physics.

169
00:11:15,000 --> 00:11:21,000
I still sort of try to keep tabs on interesting areas of physics like quantum mechanics and so on.

170
00:11:21,000 --> 00:11:26,000
But what happened is a lot of my scientific heroes in my early years were physicists,

171
00:11:26,000 --> 00:11:29,000
so Richard Feynman and Stephen Weinberg, these kinds of people.

172
00:11:29,000 --> 00:11:30,000
But I actually read this book.

173
00:11:30,000 --> 00:11:35,000
It must have been in high school called Dreams of a Final Theory by Stephen Weinberg.

174
00:11:35,000 --> 00:11:39,000
And it's his book, and you know, Nobel Prize winner, amazing physicist of his work,

175
00:11:39,000 --> 00:11:43,000
on trying to come up with a unified theory of physics, you know, to unify everything together.

176
00:11:43,000 --> 00:11:46,000
And I remember reading this book and I was very inspiring book,

177
00:11:46,000 --> 00:11:51,000
but I remember concluding, wow, they hadn't actually made that much progress.

178
00:11:51,000 --> 00:11:54,000
And these incredible people, you know, that I had really admired, you know,

179
00:11:54,000 --> 00:11:57,000
you can think of all the physicists since post-World War II, right?

180
00:11:57,000 --> 00:12:00,000
These incredible people like Feynman and Weinberg and so on.

181
00:12:00,000 --> 00:12:04,000
And I just remember thinking, gosh, I wasn't that convinced they'd got that far.

182
00:12:04,000 --> 00:12:09,000
And then I was thinking, even in the best-case scenario that you might be able to follow their footsteps,

183
00:12:09,000 --> 00:12:12,000
which is a big if, given how brilliant they were,

184
00:12:12,000 --> 00:12:16,000
you still might not be able to make much progress on it in the whole lifetime of, you know,

185
00:12:16,000 --> 00:12:18,000
an amazing career like they had.

186
00:12:18,000 --> 00:12:20,000
And so then I started thinking, well, perhaps the problem is,

187
00:12:20,000 --> 00:12:25,000
is that we need a little bit of help and a little bit of extra intellectual horsepower.

188
00:12:25,000 --> 00:12:27,000
And where can we get that from?

189
00:12:27,000 --> 00:12:31,000
Well, of course, I was also simultaneously programming and doing games

190
00:12:31,000 --> 00:12:33,000
and falling in love with sort of computing.

191
00:12:33,000 --> 00:12:35,000
And that was my real passion.

192
00:12:35,000 --> 00:12:39,000
And I just realized that working on AI, I could satisfy both things at once.

193
00:12:39,000 --> 00:12:44,000
So first of all, it would open up a door to insights into what intelligence is

194
00:12:44,000 --> 00:12:48,000
and how the brain works and, you know, as a comparator and so on,

195
00:12:48,000 --> 00:12:50,000
when those are some of the biggest questions already.

196
00:12:50,000 --> 00:12:55,000
But it could also help with science and physics and help experts in those areas

197
00:12:55,000 --> 00:12:57,000
crack the problems and the questions they care about.

198
00:12:57,000 --> 00:13:01,000
So it seemed to be like the perfect sort of meta solution in a way.

199
00:13:01,000 --> 00:13:06,000
And when I made this realization sometime in high school, that's when I decided, you know,

200
00:13:06,000 --> 00:13:10,000
I was going to work on AI and not go directly, say, for physics

201
00:13:10,000 --> 00:13:13,000
and try and build the ultimate scientific tool.

202
00:13:13,000 --> 00:13:16,000
So let's talk about how some of this experience comes together for you

203
00:13:16,000 --> 00:13:19,000
because one of the early contributions of DeepMind

204
00:13:19,000 --> 00:13:21,000
is you're trying to build this new kind of intelligence

205
00:13:21,000 --> 00:13:28,000
and the way you are trying to build it, train it, test it is on games.

206
00:13:28,000 --> 00:13:31,000
And now it feels like a weird thing to hear,

207
00:13:31,000 --> 00:13:34,000
but it takes you a very long time to build a system

208
00:13:34,000 --> 00:13:37,000
that can score even a single point in Pong.

209
00:13:37,000 --> 00:13:41,000
So tell me first about the decision to begin training AIs on video games

210
00:13:41,000 --> 00:13:43,000
because that doesn't seem totally intuitive.

211
00:13:43,000 --> 00:13:46,000
You want to answer the fundamental questions of physics

212
00:13:46,000 --> 00:13:48,000
and you're building something to play Pong?

213
00:13:48,000 --> 00:13:50,000
It seems ridiculous. Why?

214
00:13:50,000 --> 00:13:54,000
Yes. That was one of the early decisions I think that served us very well

215
00:13:54,000 --> 00:13:58,000
when we started DeepMind was a lot of people who were working on AI at that time

216
00:13:58,000 --> 00:14:01,000
were mostly working on things like robotics

217
00:14:01,000 --> 00:14:03,000
and sort of embodied intelligence

218
00:14:03,000 --> 00:14:06,000
and that's an important branch of AI, of course,

219
00:14:06,000 --> 00:14:09,000
and places like MIT where I was doing my postdoc.

220
00:14:09,000 --> 00:14:12,000
That was a real bastion of that type of work.

221
00:14:12,000 --> 00:14:14,000
But what I realized was that the researchers

222
00:14:14,000 --> 00:14:17,000
ended up spending most of their time fiddling around with the hardware,

223
00:14:17,000 --> 00:14:20,000
you know, in the server motors and they'd always break

224
00:14:20,000 --> 00:14:23,000
and the robots are expensive and they're complicated and they're slow.

225
00:14:23,000 --> 00:14:27,000
And I sort of realized that it would be better to work in simulation

226
00:14:27,000 --> 00:14:31,000
and we could, you know, run millions of experiments in the cloud all at once

227
00:14:31,000 --> 00:14:35,000
and get much faster learning rates relatively cheaply and quickly

228
00:14:35,000 --> 00:14:37,000
and simply if we were to do that.

229
00:14:37,000 --> 00:14:39,000
The other advantage of games, of course,

230
00:14:39,000 --> 00:14:42,000
is that they've been built to be challenging to humans

231
00:14:42,000 --> 00:14:45,000
and you can use top human players as a great benchmark.

232
00:14:45,000 --> 00:14:47,000
The other thing is they have clear objectives, right?

233
00:14:47,000 --> 00:14:50,000
You to win the game or to maximize the score

234
00:14:50,000 --> 00:14:53,000
and those kinds of objectives are very useful

235
00:14:53,000 --> 00:14:55,000
if you want to train reinforcement learning systems

236
00:14:55,000 --> 00:14:59,000
which we specialize in that are reward-seeking and goal-directed,

237
00:14:59,000 --> 00:15:01,000
you know, so they need an objective to solve.

238
00:15:01,000 --> 00:15:04,000
So games are fantastic for all of those reasons.

239
00:15:04,000 --> 00:15:06,000
And the other cool thing about games is, of course,

240
00:15:06,000 --> 00:15:08,000
you can go up the ladder of complexity of games

241
00:15:08,000 --> 00:15:10,000
by just going through the different eras of games.

242
00:15:10,000 --> 00:15:13,000
We started with the 1970s with the simplest Atari games

243
00:15:13,000 --> 00:15:15,000
like you mentioned, like Pong,

244
00:15:15,000 --> 00:15:18,000
and then eventually we went all the way up over, you know,

245
00:15:18,000 --> 00:15:21,000
almost a decade of work to the most complex modern computer games

246
00:15:21,000 --> 00:15:24,000
like StarCraft, and so you can keep on increasing

247
00:15:24,000 --> 00:15:27,000
the difficulty of the test for your AI systems

248
00:15:27,000 --> 00:15:30,000
as your AI systems are getting more sophisticated.

249
00:15:30,000 --> 00:15:33,000
So, you know, it turned out to be a really efficient way

250
00:15:33,000 --> 00:15:37,000
to proving ground, really, to test out these algorithmic ideas.

251
00:15:37,000 --> 00:15:40,000
So I don't want to skip over too quickly what's happening here.

252
00:15:40,000 --> 00:15:42,000
I think when I say the sentence,

253
00:15:42,000 --> 00:15:44,000
oh, you built an AI that can play the game Pong,

254
00:15:44,000 --> 00:15:46,000
that sounds simple.

255
00:15:46,000 --> 00:15:48,000
But it takes you a long time to do that.

256
00:15:48,000 --> 00:15:50,000
It takes you a long time to score any points in Pong,

257
00:15:50,000 --> 00:15:53,000
and then you begin to see exponential dominance of the Pong game,

258
00:15:53,000 --> 00:15:57,000
which is an interesting dynamic here that I want to talk about too.

259
00:15:57,000 --> 00:16:00,000
But tell me what you're actually doing there.

260
00:16:00,000 --> 00:16:03,000
Sure. So the key thing here is,

261
00:16:03,000 --> 00:16:06,000
and this is the difference between what I was doing in the 90s

262
00:16:06,000 --> 00:16:08,000
with my early games career,

263
00:16:08,000 --> 00:16:10,000
where we were directly programming these expert systems.

264
00:16:10,000 --> 00:16:13,000
Of course, you could do that easily for something like Pong.

265
00:16:13,000 --> 00:16:15,000
But what the key breakthrough that we had

266
00:16:15,000 --> 00:16:18,000
that sort of underpinned this whole new field, really,

267
00:16:18,000 --> 00:16:20,000
which we call deep reinforcement learning,

268
00:16:20,000 --> 00:16:23,000
combining neural networks with the reward-seeking algorithms,

269
00:16:23,000 --> 00:16:27,000
is that we played these games directly from the pixels.

270
00:16:27,000 --> 00:16:30,000
So all we gave is the input to our systems.

271
00:16:30,000 --> 00:16:34,000
The first system we built was called DQN to play these Atari games.

272
00:16:34,000 --> 00:16:36,000
What did DQN stand for?

273
00:16:36,000 --> 00:16:38,000
DQ network. So QQ...

274
00:16:38,000 --> 00:16:40,000
I was hoping that was going to be more fun.

275
00:16:40,000 --> 00:16:42,000
No, no, it was a very technical name.

276
00:16:42,000 --> 00:16:45,000
We had one at the beginning before we got better at naming things.

277
00:16:45,000 --> 00:16:47,000
It just refers to the technique that we used.

278
00:16:47,000 --> 00:16:50,000
And so the big innovation and the big breakthrough

279
00:16:50,000 --> 00:16:52,000
was to use just the raw inputs,

280
00:16:52,000 --> 00:16:54,000
in this case the pixels on the screen,

281
00:16:54,000 --> 00:16:56,000
30,000 pixels roughly on the screen,

282
00:16:56,000 --> 00:17:00,000
and not tell the system anything about the game.

283
00:17:00,000 --> 00:17:03,000
Not what it was controlling, not what would get score,

284
00:17:03,000 --> 00:17:05,000
not how it loses points or loses a life.

285
00:17:05,000 --> 00:17:08,000
Any of those things, it had to figure out for itself

286
00:17:08,000 --> 00:17:10,000
from first principles by playing the game,

287
00:17:10,000 --> 00:17:12,000
just like a human would learn.

288
00:17:12,000 --> 00:17:14,000
That was the key thing.

289
00:17:14,000 --> 00:17:16,000
So it learned for itself, and the second thing that was key

290
00:17:16,000 --> 00:17:18,000
was building these general systems.

291
00:17:18,000 --> 00:17:20,000
And that's what the general is in AGI,

292
00:17:20,000 --> 00:17:22,000
and I'm sure we'll come back to that,

293
00:17:22,000 --> 00:17:26,000
is that one single system that can play any of the Atari games,

294
00:17:26,000 --> 00:17:28,000
and the same system out of the box

295
00:17:28,000 --> 00:17:31,000
can play all of them to sort of superhuman level,

296
00:17:31,000 --> 00:17:33,000
like world record scores.

297
00:17:33,000 --> 00:17:36,000
And so those two elements, the generality and the learning,

298
00:17:36,000 --> 00:17:38,000
are the key differences.

299
00:17:38,000 --> 00:17:40,000
So I want to spend just a moment here

300
00:17:40,000 --> 00:17:42,000
on this expert system versus deep learning.

301
00:17:42,000 --> 00:17:45,000
So if you've been building an expert system, a logic system,

302
00:17:45,000 --> 00:17:49,000
you're trying to tell the system how Pong works.

303
00:17:49,000 --> 00:17:51,000
It's like, okay, there are these two paddles,

304
00:17:51,000 --> 00:17:54,000
and there's a ball, and what you want to do is get points,

305
00:17:54,000 --> 00:17:57,000
and you're basically trying to break the game of Pong down

306
00:17:57,000 --> 00:18:00,000
into rules, encode the rules into the system,

307
00:18:00,000 --> 00:18:03,000
and you figure out if you've found all the rules

308
00:18:03,000 --> 00:18:05,000
by how well the system does.

309
00:18:05,000 --> 00:18:08,000
Versus deep learning, where are there any rules there?

310
00:18:08,000 --> 00:18:10,000
Are you just telling the system,

311
00:18:10,000 --> 00:18:12,000
if you get a point that's good,

312
00:18:12,000 --> 00:18:14,000
experiment until you do,

313
00:18:14,000 --> 00:18:18,000
and it gets basically the digital equivalent of a doggy treat

314
00:18:18,000 --> 00:18:20,000
every time it does well,

315
00:18:20,000 --> 00:18:22,000
and then tries to do more of that well.

316
00:18:22,000 --> 00:18:23,000
That's right.

317
00:18:23,000 --> 00:18:24,000
So that's exactly the difference.

318
00:18:24,000 --> 00:18:27,000
So expert systems, you as the human programmers

319
00:18:27,000 --> 00:18:31,000
and designers are trying to break down this complex problem,

320
00:18:31,000 --> 00:18:34,000
be that playing chess or playing an Atari game,

321
00:18:34,000 --> 00:18:36,000
into the set of heuristics and rules.

322
00:18:36,000 --> 00:18:39,000
So sometimes there are rules, but they could be probabilistic,

323
00:18:39,000 --> 00:18:41,000
so there could just be heuristics,

324
00:18:41,000 --> 00:18:44,000
and that's what the system then uses to try and make decisions.

325
00:18:44,000 --> 00:18:47,000
So it's effectively using what it's been given.

326
00:18:47,000 --> 00:18:50,000
But then with machine learning, using techniques like deep learning,

327
00:18:50,000 --> 00:18:52,000
type of machine learning or reinforcement learning,

328
00:18:52,000 --> 00:18:54,000
which is the doggy treat thing that you mentioned.

329
00:18:54,000 --> 00:18:56,000
So you get a point, so it gets a reward,

330
00:18:56,000 --> 00:18:59,000
and then it's more likely to do those actions again.

331
00:18:59,000 --> 00:19:02,000
Those things learn for themselves.

332
00:19:02,000 --> 00:19:04,000
So you just give it the high-level objective.

333
00:19:04,000 --> 00:19:07,000
You say win a game, get a certain number of points,

334
00:19:07,000 --> 00:19:10,000
and then it has to figure it out effectively

335
00:19:10,000 --> 00:19:12,000
what those heuristics are for itself.

336
00:19:12,000 --> 00:19:14,000
And so we're going to talk more about this,

337
00:19:14,000 --> 00:19:18,000
but it's in this divergence where, as I understand it,

338
00:19:18,000 --> 00:19:20,000
the question of alignment problems really begins

339
00:19:20,000 --> 00:19:23,000
to creep into the industry.

340
00:19:23,000 --> 00:19:25,000
Because if I'm encoding all these rules,

341
00:19:25,000 --> 00:19:27,000
if I'm encoding my understanding into a computer,

342
00:19:27,000 --> 00:19:29,000
then I might miss things,

343
00:19:29,000 --> 00:19:31,000
but the computer is basically running off

344
00:19:31,000 --> 00:19:33,000
of what I've told it to do.

345
00:19:33,000 --> 00:19:35,000
Whereas if I've told it to get points,

346
00:19:35,000 --> 00:19:37,000
and it doesn't even know what it's doing

347
00:19:37,000 --> 00:19:38,000
except for getting points.

348
00:19:38,000 --> 00:19:40,000
I mean, as you say, it's learning from pixels.

349
00:19:40,000 --> 00:19:42,000
It doesn't have an understanding of the game Pong.

350
00:19:42,000 --> 00:19:44,000
And ultimately, it's winning points,

351
00:19:44,000 --> 00:19:46,000
but it still may not know it's playing Pong, right?

352
00:19:46,000 --> 00:19:48,000
I've never told it it's playing Pong.

353
00:19:48,000 --> 00:19:53,000
It's not working with that kind of generalized sense of the situation.

354
00:19:53,000 --> 00:19:54,000
It might get points,

355
00:19:54,000 --> 00:19:56,000
but if it can figure out another way to get points,

356
00:19:56,000 --> 00:19:57,000
it'll do that too.

357
00:19:57,000 --> 00:20:01,000
Let's talk a bit about what it means in terms of

358
00:20:01,000 --> 00:20:05,000
what the system is doing versus what we often think it's doing

359
00:20:05,000 --> 00:20:08,000
that comes from when it is doing the learning itself.

360
00:20:08,000 --> 00:20:11,000
You know, this is a very interesting question of

361
00:20:11,000 --> 00:20:13,000
when you do the learning itself,

362
00:20:13,000 --> 00:20:15,000
you can guide that learning by giving it

363
00:20:15,000 --> 00:20:17,000
certain types of data to learn from.

364
00:20:17,000 --> 00:20:19,000
You can set the high level objectives.

365
00:20:19,000 --> 00:20:21,000
You could even set sub-objectives as well

366
00:20:21,000 --> 00:20:23,000
to kind of guide it on the way.

367
00:20:23,000 --> 00:20:24,000
How do you win a game?

368
00:20:24,000 --> 00:20:27,000
Well, you've got to get a certain number of points

369
00:20:27,000 --> 00:20:28,000
or something like that.

370
00:20:28,000 --> 00:20:31,000
So you can sort of give it some sub-objectives

371
00:20:31,000 --> 00:20:33,000
or it can discover that for itself.

372
00:20:33,000 --> 00:20:36,000
But really, you're sort of more coaxing the system

373
00:20:36,000 --> 00:20:38,000
rather than with the logic systems

374
00:20:38,000 --> 00:20:40,000
where you're directly programming that in.

375
00:20:40,000 --> 00:20:43,000
So you have a little bit less direct control over that.

376
00:20:43,000 --> 00:20:45,000
And that, of course, is then linked to

377
00:20:45,000 --> 00:20:46,000
what you mentioned, the alignment problem,

378
00:20:46,000 --> 00:20:48,000
which is sometimes you may care

379
00:20:48,000 --> 00:20:50,000
not just about the overall outcome,

380
00:20:50,000 --> 00:20:52,000
you may care about how it got there, right?

381
00:20:52,000 --> 00:20:55,000
And if you do, then it matters the way that it solves it.

382
00:20:55,000 --> 00:20:58,000
And so then you're kind of leaving the solution

383
00:20:58,000 --> 00:21:00,000
or the type of solution or the approach

384
00:21:00,000 --> 00:21:02,000
up to the system itself.

385
00:21:02,000 --> 00:21:04,000
And what you're caring about is the objective at the end.

386
00:21:04,000 --> 00:21:06,000
So if you care about the approach too,

387
00:21:06,000 --> 00:21:09,000
then you have to add extra constraints into the system

388
00:21:09,000 --> 00:21:12,000
or give it some feedback about the approach too,

389
00:21:12,000 --> 00:21:14,000
which is this sort of reinforcement learning

390
00:21:14,000 --> 00:21:16,000
with human feedback that was now in vogue,

391
00:21:16,000 --> 00:21:20,000
where you can get that feedback directly from human ratings

392
00:21:20,000 --> 00:21:22,000
or you can do it another way.

393
00:21:22,000 --> 00:21:24,000
You can give it sort of further sub-objectives

394
00:21:24,000 --> 00:21:28,000
that help it guide it as to what type of solution you want.

395
00:21:28,000 --> 00:21:30,000
Well, we've talked a bit here about deep learning,

396
00:21:30,000 --> 00:21:32,000
but can you describe reinforcement learning?

397
00:21:32,000 --> 00:21:34,000
What is it? How does it differ from deep learning?

398
00:21:34,000 --> 00:21:36,000
How do they work together?

399
00:21:36,000 --> 00:21:38,000
So both deep learning and reinforcement learning

400
00:21:38,000 --> 00:21:40,000
are types of machine learning,

401
00:21:40,000 --> 00:21:41,000
and they're very complementary.

402
00:21:41,000 --> 00:21:42,000
And we specialize in both

403
00:21:42,000 --> 00:21:44,000
and have done from the start of DeepMind.

404
00:21:44,000 --> 00:21:47,000
So deep learning is on your hierarchical neural networks,

405
00:21:47,000 --> 00:21:50,000
really complex stacks of neural networks,

406
00:21:50,000 --> 00:21:53,000
very loosely modeled on brain neural networks.

407
00:21:53,000 --> 00:21:55,000
And the objective of those neural networks

408
00:21:55,000 --> 00:21:58,000
is to learn the statistics of the environment

409
00:21:58,000 --> 00:22:00,000
that they find themselves in

410
00:22:00,000 --> 00:22:02,000
or the data stream that they're given.

411
00:22:02,000 --> 00:22:03,000
So you can think of the neural network,

412
00:22:03,000 --> 00:22:06,000
the deep learning as building a model of the situation.

413
00:22:06,000 --> 00:22:08,000
And then the reinforcement learning part

414
00:22:08,000 --> 00:22:12,000
is the bit which does the planning and the reward learning.

415
00:22:12,000 --> 00:22:16,000
So effectively, it's kind of like a reward-seeking system

416
00:22:16,000 --> 00:22:19,000
that is trying to solve an objective that you give it.

417
00:22:19,000 --> 00:22:23,000
So for example, we'll be in a game trying to maximize the score.

418
00:22:23,000 --> 00:22:26,000
So for every point it gets, it's sort of like a little reward.

419
00:22:26,000 --> 00:22:29,000
Animals and including humans learn with reinforcement learning.

420
00:22:29,000 --> 00:22:31,000
It's one of the types of learnings we do.

421
00:22:31,000 --> 00:22:33,000
And you see that really well with, as you mentioned earlier,

422
00:22:33,000 --> 00:22:35,000
with dogs and you give them a treat

423
00:22:35,000 --> 00:22:37,000
if they do something that you like or they're well-behaved.

424
00:22:37,000 --> 00:22:40,000
And then they're more likely to do that again in future.

425
00:22:40,000 --> 00:22:43,000
And that's exactly very similar in a digital form

426
00:22:43,000 --> 00:22:46,000
with these reinforcement learning systems that we build.

427
00:22:46,000 --> 00:22:48,000
Effectively, they get treats, a reward,

428
00:22:48,000 --> 00:22:51,000
when they achieve a certain sub-objective.

429
00:22:51,000 --> 00:22:53,000
And so the cool thing is, is that you can combine

430
00:22:53,000 --> 00:22:55,000
these two types of systems together,

431
00:22:55,000 --> 00:22:57,000
and that's called deep reinforcement learning,

432
00:22:57,000 --> 00:23:00,000
where you have the model, and then you also have this

433
00:23:00,000 --> 00:23:03,000
goal-seeking or reward-seeking planning system on top

434
00:23:03,000 --> 00:23:07,000
that uses that model to reach its objectives.

435
00:23:07,000 --> 00:23:09,000
So I want to give ahead a bit now to the system

436
00:23:09,000 --> 00:23:12,000
that most people at least used to know DeepMind for.

437
00:23:12,000 --> 00:23:14,000
And that's AlphaGo.

438
00:23:14,000 --> 00:23:16,000
So tell me what AlphaGo is,

439
00:23:16,000 --> 00:23:19,000
how it differs from the Pong system we're talking about here,

440
00:23:19,000 --> 00:23:23,000
why Go was an important benchmark or milestone

441
00:23:23,000 --> 00:23:25,000
to try to topple?

442
00:23:25,000 --> 00:23:28,000
Yeah, so in effect, AlphaGo was the extension

443
00:23:28,000 --> 00:23:31,000
of the work we'd done in Atari,

444
00:23:31,000 --> 00:23:33,000
but sort of the pinnacle, really,

445
00:23:33,000 --> 00:23:35,000
of what you could achieve in games AI.

446
00:23:35,000 --> 00:23:38,000
So Deep Blue, as I said earlier, beat Gary Kaspar

447
00:23:38,000 --> 00:23:40,000
for chess in the 90s.

448
00:23:40,000 --> 00:23:43,000
He won big first pinnacle in games AI,

449
00:23:43,000 --> 00:23:45,000
but the next sort of Mount Everest, if you like,

450
00:23:45,000 --> 00:23:47,000
was beating the world champion at Go.

451
00:23:47,000 --> 00:23:49,000
And you couldn't use expert systems,

452
00:23:49,000 --> 00:23:51,000
you had to use these learning systems,

453
00:23:51,000 --> 00:23:54,000
because we as even as human, the best human Go players,

454
00:23:54,000 --> 00:23:56,000
they don't understand the game well enough

455
00:23:56,000 --> 00:23:59,000
to break it down into these heuristics and sub-problems.

456
00:23:59,000 --> 00:24:01,000
It's also partly to do with the nature of Go

457
00:24:01,000 --> 00:24:04,000
as a very aesthetic game, it's a very intuitive game

458
00:24:04,000 --> 00:24:06,000
and with a lot of patterns.

459
00:24:06,000 --> 00:24:08,000
So it's quite different from chess,

460
00:24:08,000 --> 00:24:10,000
even top Go players will tell you

461
00:24:10,000 --> 00:24:12,000
why did they play a certain move,

462
00:24:12,000 --> 00:24:14,000
they'll say it just felt right.

463
00:24:14,000 --> 00:24:16,000
So they're kind of using their intuition,

464
00:24:16,000 --> 00:24:18,000
which is not something that we often associate

465
00:24:18,000 --> 00:24:20,000
with computer programs, right?

466
00:24:20,000 --> 00:24:23,000
Being able to do something that mimics intuition.

467
00:24:23,000 --> 00:24:26,000
And so with AlphaGo, what we did is we built a neural network

468
00:24:26,000 --> 00:24:28,000
that modeled the game of Go,

469
00:24:28,000 --> 00:24:31,000
figured out what were good moves in certain positions,

470
00:24:31,000 --> 00:24:33,000
who was likely to win from a certain position,

471
00:24:33,000 --> 00:24:35,000
the probability of either side winning,

472
00:24:35,000 --> 00:24:37,000
and that's what the neural network was predicting.

473
00:24:37,000 --> 00:24:40,000
And then on top of that, we overlaid this reinforcement learning system

474
00:24:40,000 --> 00:24:44,000
that would make plans, do Monte Carlo tree search,

475
00:24:44,000 --> 00:24:46,000
using the model to guide its search

476
00:24:46,000 --> 00:24:49,000
so that it didn't have to search millions and millions of moves

477
00:24:49,000 --> 00:24:51,000
and it would be intractable because it goes too complicated

478
00:24:51,000 --> 00:24:54,000
to search everything, you have to narrow down your search.

479
00:24:54,000 --> 00:24:57,000
And so it used the model to search the most fruitful paths

480
00:24:57,000 --> 00:24:59,000
and then try and find the best move

481
00:24:59,000 --> 00:25:02,000
that would most likely get it to a winning position.

482
00:25:02,000 --> 00:25:05,000
And so it was a kind of culmination

483
00:25:05,000 --> 00:25:07,000
of five, six years worth of working,

484
00:25:07,000 --> 00:25:09,000
our work in deep reinforcement learning.

485
00:25:09,000 --> 00:25:12,000
What I always found striking about that story

486
00:25:12,000 --> 00:25:15,000
is so you beat one of the Go World champions,

487
00:25:15,000 --> 00:25:17,000
it's this big moment,

488
00:25:17,000 --> 00:25:20,000
and then shortly thereafter you beat yourself,

489
00:25:20,000 --> 00:25:22,000
which is to say you have AlphaGo,

490
00:25:22,000 --> 00:25:25,000
which is the system that beats Elisa Dole,

491
00:25:25,000 --> 00:25:27,000
and then you create AlphaZero,

492
00:25:27,000 --> 00:25:30,000
a system that just stomps AlphaGo.

493
00:25:30,000 --> 00:25:33,000
So what was the difference between AlphaGo and AlphaZero

494
00:25:33,000 --> 00:25:36,000
and what was to be learned there?

495
00:25:36,000 --> 00:25:39,000
So AlphaGo was a pretty general system

496
00:25:39,000 --> 00:25:42,000
in that it learned for itself the motifs

497
00:25:42,000 --> 00:25:44,000
and the strategies around Go,

498
00:25:44,000 --> 00:25:46,000
and in fact it created new strategies

499
00:25:46,000 --> 00:25:48,000
very famously in that in the World Championship match

500
00:25:48,000 --> 00:25:50,000
that had never been seen before,

501
00:25:50,000 --> 00:25:53,000
even though we've played Go for thousands of years now

502
00:25:53,000 --> 00:25:55,000
to a couple of thousand years.

503
00:25:55,000 --> 00:25:57,000
So that was pretty incredible to see.

504
00:25:57,000 --> 00:26:00,000
The first version of AlphaGo actually was bootstrapped

505
00:26:00,000 --> 00:26:02,000
by looking at all human games

506
00:26:02,000 --> 00:26:04,000
that had been played on the Internet,

507
00:26:04,000 --> 00:26:06,000
and there's a lot of Internet Go servers

508
00:26:06,000 --> 00:26:09,000
in a very popular career in Japan and China.

509
00:26:09,000 --> 00:26:11,000
So it's using human games as training data?

510
00:26:11,000 --> 00:26:13,000
It used human data as training data.

511
00:26:13,000 --> 00:26:15,000
It also had specific things,

512
00:26:15,000 --> 00:26:18,000
knowledge about Go encoded in it

513
00:26:18,000 --> 00:26:20,000
to do with the symmetry of the board

514
00:26:20,000 --> 00:26:22,000
and some other things that were specific to Go.

515
00:26:22,000 --> 00:26:24,000
So it was a pretty general system,

516
00:26:24,000 --> 00:26:26,000
but it was specialized around Go data,

517
00:26:26,000 --> 00:26:28,000
the human data that it learned from,

518
00:26:28,000 --> 00:26:30,000
and there were some specific things about Go.

519
00:26:30,000 --> 00:26:32,000
Now, once we beat the World Champion, Lisa Doll,

520
00:26:32,000 --> 00:26:34,000
we then, this is quite common for us,

521
00:26:34,000 --> 00:26:36,000
is once we do that we always try

522
00:26:36,000 --> 00:26:38,000
and look back at our systems, in this case AlphaGo,

523
00:26:38,000 --> 00:26:40,000
and we try and remove anything

524
00:26:40,000 --> 00:26:42,000
that was specific to that task

525
00:26:42,000 --> 00:26:44,000
to make it more and more general.

526
00:26:44,000 --> 00:26:46,000
And so that's what AlphaZero was.

527
00:26:46,000 --> 00:26:48,000
AlphaZero was the next version,

528
00:26:48,000 --> 00:26:51,000
and that is able to play any two-player game,

529
00:26:51,000 --> 00:26:53,000
so it doesn't matter whether it's Go or Chess

530
00:26:53,000 --> 00:26:56,000
or Backgammon, any game you can put it in there,

531
00:26:56,000 --> 00:26:58,000
or Japanese chess to be called Shogi,

532
00:26:58,000 --> 00:27:00,000
any two-player game.

533
00:27:00,000 --> 00:27:02,000
And it doesn't need any human data either,

534
00:27:02,000 --> 00:27:05,000
because what it does, it starts off completely random.

535
00:27:05,000 --> 00:27:08,000
So imagine a blank slate neural network

536
00:27:08,000 --> 00:27:11,000
doesn't really know anything about the game or strategies,

537
00:27:11,000 --> 00:27:13,000
and what it does is it plays itself

538
00:27:13,000 --> 00:27:15,000
millions and millions of times,

539
00:27:15,000 --> 00:27:17,000
different versions of itself,

540
00:27:17,000 --> 00:27:20,000
and it learns from its own data and its own experience,

541
00:27:20,000 --> 00:27:23,000
and then eventually it becomes actually stronger

542
00:27:23,000 --> 00:27:26,000
than these individual programs like AlphaGo

543
00:27:26,000 --> 00:27:28,000
that were trained on human data,

544
00:27:28,000 --> 00:27:30,000
and it doesn't require any human data.

545
00:27:30,000 --> 00:27:32,000
It starts literally from random

546
00:27:32,000 --> 00:27:35,000
and explores the space of Go or Chess for itself.

547
00:27:35,000 --> 00:27:37,000
And of course it means it also comes up

548
00:27:37,000 --> 00:27:39,000
with incredible new strategies,

549
00:27:39,000 --> 00:27:43,000
not constrained by what humans have played in the past,

550
00:27:43,000 --> 00:27:45,000
because it doesn't have any knowledge of that.

551
00:27:45,000 --> 00:27:47,000
One of the things I think about,

552
00:27:47,000 --> 00:27:49,000
and that we will talk about later in this conversation,

553
00:27:49,000 --> 00:27:52,000
is the question of how smart something

554
00:27:52,000 --> 00:27:54,000
that is working with our data

555
00:27:54,000 --> 00:27:56,000
and working with the world as we understand it

556
00:27:56,000 --> 00:27:59,000
can really become, if that tops out somewhere.

557
00:27:59,000 --> 00:28:01,000
And one version of saying,

558
00:28:01,000 --> 00:28:03,000
not that smart might be to say,

559
00:28:03,000 --> 00:28:05,000
well, it's kind of constrained by what we know.

560
00:28:05,000 --> 00:28:07,000
It's got to work with what we know,

561
00:28:07,000 --> 00:28:09,000
and if we haven't done that much on something,

562
00:28:09,000 --> 00:28:11,000
well, it can't do that much on something in most cases.

563
00:28:11,000 --> 00:28:14,000
But this is a case where, with some very basic rules,

564
00:28:14,000 --> 00:28:16,000
it actually turned out that it was being held back

565
00:28:16,000 --> 00:28:18,000
by what we knew.

566
00:28:18,000 --> 00:28:21,000
That because human beings are prone to fattishness,

567
00:28:21,000 --> 00:28:24,000
because we follow in the footsteps of those who came before us,

568
00:28:24,000 --> 00:28:26,000
because we're taught by others,

569
00:28:26,000 --> 00:28:28,000
and they tell us, no, that would be a crazy move.

570
00:28:28,000 --> 00:28:31,000
You know, everybody who's done it before did it this way.

571
00:28:31,000 --> 00:28:33,000
I mean, that does help us get better, right?

572
00:28:33,000 --> 00:28:37,000
Cultural learning and evolution is the core of our species advancement.

573
00:28:37,000 --> 00:28:41,000
But it also turns out that that means huge swaths

574
00:28:41,000 --> 00:28:44,000
of useful strategy, information, ideas

575
00:28:44,000 --> 00:28:48,000
have been cut off the board because we just don't do that.

576
00:28:48,000 --> 00:28:52,000
And so in terms of reasons to think these systems

577
00:28:52,000 --> 00:28:56,000
could really be remarkable from our perspective,

578
00:28:56,000 --> 00:28:59,000
the fact that it was being encumbered

579
00:28:59,000 --> 00:29:01,000
by everything we knew about Go,

580
00:29:01,000 --> 00:29:05,000
as opposed to launched forward by everything we knew about Go,

581
00:29:05,000 --> 00:29:09,000
to maybe put less sentiment on the word here,

582
00:29:09,000 --> 00:29:11,000
just strikes me as profound.

583
00:29:11,000 --> 00:29:13,000
Yeah, look, it's an interesting take.

584
00:29:13,000 --> 00:29:17,000
I mean, of course, this is what happens with our cultural civilization,

585
00:29:17,000 --> 00:29:20,000
is that we do get into local maximas, one could say,

586
00:29:20,000 --> 00:29:23,000
in something relatively prescribed like a game.

587
00:29:23,000 --> 00:29:26,000
In fact, I talked to the Go experts after AlphaGo

588
00:29:26,000 --> 00:29:29,000
made these new strategies, most famously Move 37

589
00:29:29,000 --> 00:29:31,000
on Game 2 of the World Championship match,

590
00:29:31,000 --> 00:29:33,000
with this astounding new move,

591
00:29:33,000 --> 00:29:36,000
and I asked all the Go experts afterwards about that move.

592
00:29:36,000 --> 00:29:39,000
And they said, we just told that that's a bad move to make

593
00:29:39,000 --> 00:29:41,000
in that early in the game,

594
00:29:41,000 --> 00:29:43,000
but they can't really explain why.

595
00:29:43,000 --> 00:29:46,000
They just said their teachers would just basically shout at them.

596
00:29:46,000 --> 00:29:49,000
It's interesting that that is a cultural norm then

597
00:29:49,000 --> 00:29:52,000
that actually limits our creativity and exploration.

598
00:29:52,000 --> 00:29:55,000
So what I'm hoping is these systems will expand our own minds,

599
00:29:55,000 --> 00:29:58,000
and I think this is actually what's happening Go and in chess,

600
00:29:58,000 --> 00:30:00,000
again, in very prescribed areas,

601
00:30:00,000 --> 00:30:03,000
but where people have started being more creative themselves.

602
00:30:03,000 --> 00:30:04,000
Oh, actually, we don't.

603
00:30:04,000 --> 00:30:06,000
Maybe we should question some of those cultural norms

604
00:30:06,000 --> 00:30:08,000
and perhaps we would get further.

605
00:30:08,000 --> 00:30:11,000
I believe that's what has happened in Go and other things.

606
00:30:11,000 --> 00:30:15,000
Now, the cool thing is coming back to using AI systems for science.

607
00:30:15,000 --> 00:30:18,000
If you think about that, then science is the pursuit of new knowledge

608
00:30:18,000 --> 00:30:20,000
and understanding.

609
00:30:20,000 --> 00:30:23,000
And so you can now see, I think, of course, in games,

610
00:30:23,000 --> 00:30:25,000
it's just games, and they're fun,

611
00:30:25,000 --> 00:30:28,000
and I love games to death and my huge passion of mine,

612
00:30:28,000 --> 00:30:30,000
but in the end, it's just a game.

613
00:30:30,000 --> 00:30:33,000
But for science, you're actually discovering a medicine,

614
00:30:33,000 --> 00:30:35,000
you're discovering important new knowledge.

615
00:30:35,000 --> 00:30:37,000
There's no reason to assume that isn't going on

616
00:30:37,000 --> 00:30:40,000
in another cultural activity, in this case, science.

617
00:30:40,000 --> 00:30:43,000
And I think these tools could in themselves

618
00:30:43,000 --> 00:30:46,000
help us discover new area regions of knowledge,

619
00:30:46,000 --> 00:30:51,000
but also inspire the human experts to explore more as well in tandem.

620
00:31:14,000 --> 00:31:17,000
Let's shift into science,

621
00:31:17,000 --> 00:31:22,000
because this is where DeepMind begins creating something

622
00:31:22,000 --> 00:31:26,000
that isn't just winning games, but is actually creating an advance.

623
00:31:26,000 --> 00:31:28,000
And I've said before on this show many times

624
00:31:28,000 --> 00:31:31,000
that of all the AI systems that have been released,

625
00:31:31,000 --> 00:31:34,000
the one I've always been most impressed by and interested in is AlphaFold,

626
00:31:34,000 --> 00:31:37,000
which is a DeepMind system.

627
00:31:37,000 --> 00:31:40,000
So tell me what AlphaFold is,

628
00:31:40,000 --> 00:31:43,000
what the problem is, how you come to decide that that's something

629
00:31:43,000 --> 00:31:45,000
that your systems can take on.

630
00:31:45,000 --> 00:31:48,000
I mean, you're doing games, and then you move to this.

631
00:31:48,000 --> 00:31:49,000
What is AlphaFold?

632
00:31:49,000 --> 00:31:52,000
So we were doing games as the testing ground,

633
00:31:52,000 --> 00:31:55,000
but we always had in mind, and I always had in mind

634
00:31:55,000 --> 00:31:58,000
the very thing I was thinking about as a teenager,

635
00:31:58,000 --> 00:32:00,000
of using AI as a tool for science.

636
00:32:00,000 --> 00:32:02,000
And once we'd mastered a lot of games,

637
00:32:02,000 --> 00:32:05,000
the idea was that these systems would be powerful enough

638
00:32:05,000 --> 00:32:07,000
and sophisticated enough we could turn them onto

639
00:32:07,000 --> 00:32:10,000
very important real-world problems and real-world challenges,

640
00:32:10,000 --> 00:32:12,000
especially in the sciences.

641
00:32:12,000 --> 00:32:15,000
So AlphaFold, we pretty much started the day after we got back

642
00:32:15,000 --> 00:32:18,000
from Korea in 2016 and the Lisa Dolmatch,

643
00:32:18,000 --> 00:32:21,000
and that was our next big grand challenge.

644
00:32:21,000 --> 00:32:24,000
And AlphaFold is our program to try and solve the problem

645
00:32:24,000 --> 00:32:26,000
of protein folding, as it's known.

646
00:32:26,000 --> 00:32:29,000
Proteins are the workhorses of biology.

647
00:32:29,000 --> 00:32:32,000
Basically, every biological function in the body

648
00:32:32,000 --> 00:32:34,000
is mediated by proteins.

649
00:32:34,000 --> 00:32:37,000
Proteins are described by their amino acid sequence,

650
00:32:37,000 --> 00:32:40,000
so you can think of it loosely as the genetic sequence

651
00:32:40,000 --> 00:32:44,000
for a protein, and that's a kind of one-dimensional string

652
00:32:44,000 --> 00:32:46,000
of letters you can think of.

653
00:32:46,000 --> 00:32:49,000
But in the body, they scrunch up into a 3D shape

654
00:32:49,000 --> 00:32:53,000
very, very quickly, and it's the 3D shape of the protein

655
00:32:53,000 --> 00:32:56,000
that governs its function, what it does in the body.

656
00:32:56,000 --> 00:32:58,000
And so the protein folding problem, in essence,

657
00:32:58,000 --> 00:33:01,000
is can you predict the 3D shape of the protein

658
00:33:01,000 --> 00:33:05,000
directly from the amino acid sequence?

659
00:33:05,000 --> 00:33:08,000
The reason it's so important is that a lot of disease

660
00:33:08,000 --> 00:33:11,000
is caused by proteins misfolding or folding wrong,

661
00:33:11,000 --> 00:33:14,000
and also if you want to design drugs to combat diseases,

662
00:33:14,000 --> 00:33:17,000
you need to know what the surface of the protein,

663
00:33:17,000 --> 00:33:19,000
therefore the shape of the protein is,

664
00:33:19,000 --> 00:33:21,000
so you know which parts of the protein to target

665
00:33:21,000 --> 00:33:23,000
with your drug compound.

666
00:33:23,000 --> 00:33:27,000
So it's hugely important for many, many biological research questions.

667
00:33:27,000 --> 00:33:30,000
To just give an example of this, people may have heard over time

668
00:33:30,000 --> 00:33:33,000
that coronavirus is a spike protein,

669
00:33:33,000 --> 00:33:36,000
and that's not just an aesthetic point.

670
00:33:36,000 --> 00:33:41,000
The fact that it has this somewhat spiked folding structure

671
00:33:41,000 --> 00:33:43,000
is crucial to the way it actually works.

672
00:33:43,000 --> 00:33:45,000
Do you want to just maybe use the coronavirus protein

673
00:33:45,000 --> 00:33:47,000
as an example of what you're saying?

674
00:33:47,000 --> 00:33:49,000
Yeah, so that's a great example,

675
00:33:49,000 --> 00:33:52,000
and we worked on that as well with the alpha-fold system.

676
00:33:52,000 --> 00:33:54,000
So yes, the spike protein is a thing,

677
00:33:54,000 --> 00:33:57,000
in a sense, that sticks out of the virus.

678
00:33:57,000 --> 00:34:00,000
So that's what you want to latch on to

679
00:34:00,000 --> 00:34:03,000
with a vaccine or a drug to kind of block its function,

680
00:34:03,000 --> 00:34:06,000
so it doesn't attach to the body or the body's cells

681
00:34:06,000 --> 00:34:08,000
in a certain sort of way.

682
00:34:08,000 --> 00:34:11,000
So it's the protein structures that do all the mechanics of that.

683
00:34:11,000 --> 00:34:14,000
So if you understand what the protein structure looks like,

684
00:34:14,000 --> 00:34:16,000
that spike looks like, the shape of it,

685
00:34:16,000 --> 00:34:19,000
you can design something that fits like a glove around it,

686
00:34:19,000 --> 00:34:21,000
right, to block its action.

687
00:34:21,000 --> 00:34:23,000
So that's a great example of, you know,

688
00:34:23,000 --> 00:34:25,000
the criticality of protein structure.

689
00:34:25,000 --> 00:34:32,000
And what made you think that protein folding is like games?

690
00:34:32,000 --> 00:34:34,000
What is the analogy you're drawing here?

691
00:34:34,000 --> 00:34:36,000
When you say, I come back from doing Go,

692
00:34:36,000 --> 00:34:39,000
and I decided to work on protein folding,

693
00:34:39,000 --> 00:34:41,000
what are you seeing here?

694
00:34:41,000 --> 00:34:43,000
Because I would not naturally see a connection

695
00:34:43,000 --> 00:34:45,000
between those two questions.

696
00:34:45,000 --> 00:34:47,000
No, it seems quite far apart,

697
00:34:47,000 --> 00:34:49,000
but actually it depends on if you step back

698
00:34:49,000 --> 00:34:51,000
and look at it sort of meta-level,

699
00:34:51,000 --> 00:34:53,000
they have a lot of things in common.

700
00:34:53,000 --> 00:34:56,000
By the way, protein folding is one of a number of scientific problems,

701
00:34:56,000 --> 00:34:58,000
big sort of grand challenges,

702
00:34:58,000 --> 00:35:00,000
that I came across in my career,

703
00:35:00,000 --> 00:35:03,000
actually protein folding I came across in the 90s in my undergrad,

704
00:35:03,000 --> 00:35:05,000
because I had a lot of biologist friends

705
00:35:05,000 --> 00:35:07,000
who were obsessed about this at Cambridge

706
00:35:07,000 --> 00:35:10,000
and actually went on to do their whole careers on protein structures.

707
00:35:10,000 --> 00:35:12,000
And they explained to me the problem

708
00:35:12,000 --> 00:35:14,000
and I thought it was fascinating,

709
00:35:14,000 --> 00:35:16,000
and I also thought it was a perfect problem

710
00:35:16,000 --> 00:35:18,000
for AI one day to help with.

711
00:35:18,000 --> 00:35:20,000
So I kind of filed it away,

712
00:35:20,000 --> 00:35:23,000
and I always later bought it out of the filing system in a sense

713
00:35:23,000 --> 00:35:26,000
and decided that that was the first big grand challenge

714
00:35:26,000 --> 00:35:29,000
we would apply our learning systems to.

715
00:35:29,000 --> 00:35:31,000
The reason I think it's similar,

716
00:35:31,000 --> 00:35:33,000
that sort of later on, actually while I was doing my postdoc,

717
00:35:33,000 --> 00:35:35,000
the second time I came across protein folding was,

718
00:35:35,000 --> 00:35:37,000
you know, in the late 2000s,

719
00:35:37,000 --> 00:35:39,000
where there was this game called Fold It,

720
00:35:39,000 --> 00:35:41,000
it's called a citizen science game,

721
00:35:41,000 --> 00:35:43,000
you may have come across it,

722
00:35:43,000 --> 00:35:45,000
so basically a lab had created a game, a puzzle game,

723
00:35:45,000 --> 00:35:48,000
where it involved people folding proteins

724
00:35:48,000 --> 00:35:50,000
in a three-dimensional interface.

725
00:35:50,000 --> 00:35:52,000
I don't think it was a very fun game,

726
00:35:52,000 --> 00:35:55,000
but they made it into this sort of quite user-friendly interface,

727
00:35:55,000 --> 00:35:58,000
and a few tens of thousands of amateur games players

728
00:35:58,000 --> 00:36:00,000
got quite obsessed with it.

729
00:36:00,000 --> 00:36:02,000
It got released, I think, in 2008, 2009,

730
00:36:02,000 --> 00:36:04,000
and I remember looking into this and thinking,

731
00:36:04,000 --> 00:36:06,000
wow, this is pretty fascinating

732
00:36:06,000 --> 00:36:09,000
if you can get people to do science by playing a game.

733
00:36:09,000 --> 00:36:11,000
That seems like a great idea,

734
00:36:11,000 --> 00:36:14,000
so my game's designer part of me was fascinated by it.

735
00:36:14,000 --> 00:36:16,000
And so what happened when I looked into it

736
00:36:16,000 --> 00:36:18,000
was some of these gamers, who, by the way,

737
00:36:18,000 --> 00:36:20,000
a lot of them knew nothing about biology, right,

738
00:36:20,000 --> 00:36:22,000
they were just gamers, they'd figured out,

739
00:36:22,000 --> 00:36:24,000
presumably, with their pattern matching of their brain,

740
00:36:24,000 --> 00:36:27,000
their intuition, that certain counter-intuitive folds

741
00:36:27,000 --> 00:36:29,000
of this string of amino acid sequences,

742
00:36:29,000 --> 00:36:31,000
you know, the backbone of the protein,

743
00:36:31,000 --> 00:36:34,000
led it to the right kind of 3D structure.

744
00:36:34,000 --> 00:36:36,000
And they're counter-intuitive in that

745
00:36:36,000 --> 00:36:38,000
if you just do the fold that gets you

746
00:36:38,000 --> 00:36:40,000
locally to the lowest energy state,

747
00:36:40,000 --> 00:36:43,000
which is a kind of greedy search strategy,

748
00:36:43,000 --> 00:36:45,000
you end up with the wrong protein fold,

749
00:36:45,000 --> 00:36:46,000
you have to do that.

750
00:36:46,000 --> 00:36:48,000
So sometimes you have to do local moves,

751
00:36:48,000 --> 00:36:50,000
local bends of the protein

752
00:36:50,000 --> 00:36:52,000
that actually make the energy landscape worse,

753
00:36:52,000 --> 00:36:55,000
effectively the efficiency of the protein structure worse.

754
00:36:55,000 --> 00:36:57,000
And then eventually you resolve that.

755
00:36:57,000 --> 00:37:00,000
And I remember thinking, combining that

756
00:37:00,000 --> 00:37:02,000
with what we then did with AlphaGo,

757
00:37:02,000 --> 00:37:04,000
where in AlphaGo, what had we done?

758
00:37:04,000 --> 00:37:06,000
Well, what we'd managed with AlphaGo and achieved with AlphaGo

759
00:37:06,000 --> 00:37:09,000
is we'd managed to mimic the intuition

760
00:37:09,000 --> 00:37:11,000
of these incredible Go masters.

761
00:37:11,000 --> 00:37:13,000
So I was thinking, wow, if that was the case

762
00:37:13,000 --> 00:37:15,000
for professionals, you spend their whole life on it.

763
00:37:15,000 --> 00:37:17,000
And then these amateur gamers,

764
00:37:17,000 --> 00:37:19,000
who didn't know anything about biology,

765
00:37:19,000 --> 00:37:21,000
were able to, in a couple of cases,

766
00:37:21,000 --> 00:37:23,000
fold a couple of proteins correctly.

767
00:37:23,000 --> 00:37:25,000
Then why wouldn't we be able to mimic

768
00:37:25,000 --> 00:37:26,000
whatever was going on in that?

769
00:37:26,000 --> 00:37:28,000
Those amateur gamers' intuition.

770
00:37:28,000 --> 00:37:30,000
So that gave me some hope, additional hope,

771
00:37:30,000 --> 00:37:32,000
that this would be possible somehow.

772
00:37:32,000 --> 00:37:34,000
And of course, this idea of a game,

773
00:37:34,000 --> 00:37:36,000
protein folding being a puzzle game,

774
00:37:36,000 --> 00:37:38,000
was pretty interesting as an analogy as well.

775
00:37:38,000 --> 00:37:40,000
So for several reasons,

776
00:37:40,000 --> 00:37:42,000
and also the fact it was a grand challenge

777
00:37:42,000 --> 00:37:45,000
and had so many downstream implications

778
00:37:45,000 --> 00:37:47,000
and impact if we were to solve it,

779
00:37:47,000 --> 00:37:49,000
all those factors sort of came together

780
00:37:49,000 --> 00:37:51,000
for me to choose that as the next project.

781
00:37:51,000 --> 00:37:54,000
That brings up, I think, another important part of all this.

782
00:37:54,000 --> 00:37:57,000
So when you think about how are you rewarding the system,

783
00:37:57,000 --> 00:38:00,000
how are you reinforcing the system in Go,

784
00:38:00,000 --> 00:38:03,000
well, Go has rules and you know how you score points

785
00:38:03,000 --> 00:38:05,000
and you know how you win a game.

786
00:38:05,000 --> 00:38:08,000
But when you're predicting the structure

787
00:38:08,000 --> 00:38:13,000
of a heretofore-unpredicted protein,

788
00:38:13,000 --> 00:38:15,000
how do you know if you're right?

789
00:38:15,000 --> 00:38:17,000
How did the gamers know if they were right?

790
00:38:17,000 --> 00:38:19,000
How does the system know if it is right?

791
00:38:19,000 --> 00:38:21,000
What are you reinforcing against?

792
00:38:21,000 --> 00:38:23,000
That was one of the hardest things

793
00:38:23,000 --> 00:38:25,000
and often that's one of the hardest things

794
00:38:25,000 --> 00:38:27,000
with learning systems and machine learning systems

795
00:38:27,000 --> 00:38:29,000
is actually formulating the right objectives.

796
00:38:29,000 --> 00:38:32,000
You can think of it as asking the system the right question.

797
00:38:32,000 --> 00:38:34,000
And how do you formulate what you want

798
00:38:34,000 --> 00:38:38,000
in terms of a simple to optimize objective function?

799
00:38:38,000 --> 00:38:40,000
And you're absolutely right, in the real world

800
00:38:40,000 --> 00:38:42,000
you don't have simple things like scores

801
00:38:42,000 --> 00:38:44,000
or winning conditions, right?

802
00:38:44,000 --> 00:38:45,000
That's obviously in games.

803
00:38:45,000 --> 00:38:47,000
But with proteins and biology,

804
00:38:47,000 --> 00:38:49,000
a lot of the cases there are good proxies for it,

805
00:38:49,000 --> 00:38:52,000
like minimizing the energy in the system.

806
00:38:52,000 --> 00:38:55,000
Most natural systems try to be energy efficient,

807
00:38:55,000 --> 00:38:59,000
so you can sort of follow a gradient of the energy gradient

808
00:38:59,000 --> 00:39:02,000
or the free energy in the system and try to minimize that.

809
00:39:02,000 --> 00:39:03,000
So that's one thing.

810
00:39:03,000 --> 00:39:05,000
The other thing is protein folding.

811
00:39:05,000 --> 00:39:07,000
There is a whole history, 50-year history,

812
00:39:07,000 --> 00:39:11,000
or more actually, of painstaking experimental work.

813
00:39:11,000 --> 00:39:14,000
The rule of thumb is it takes one whole PhD,

814
00:39:14,000 --> 00:39:18,000
the whole PhD time, like one PhD student and their entire PhD,

815
00:39:18,000 --> 00:39:21,000
for five years to crystallize one protein

816
00:39:21,000 --> 00:39:25,000
and then using x-ray crystallography or electron microscopes,

817
00:39:25,000 --> 00:39:27,000
complicated pieces of very expensive,

818
00:39:27,000 --> 00:39:28,000
complicated pieces of equipment

819
00:39:28,000 --> 00:39:32,000
to basically image these incredibly small complex structures.

820
00:39:32,000 --> 00:39:35,000
It's unbelievably painstaking difficult work.

821
00:39:35,000 --> 00:39:37,000
And so over 50 years of human endeavor

822
00:39:37,000 --> 00:39:39,000
from all the labs around the world,

823
00:39:39,000 --> 00:39:41,000
structural biologists managed to find the structure

824
00:39:41,000 --> 00:39:45,000
around 100,000 to 150,000 proteins.

825
00:39:45,000 --> 00:39:48,000
And they're all deposited in this database called the PDB,

826
00:39:48,000 --> 00:39:51,000
and that's what we can use as a training corpus,

827
00:39:51,000 --> 00:39:53,000
but also we can test our predictions.

828
00:39:53,000 --> 00:39:56,000
So you can also do mutagenesis on these systems,

829
00:39:56,000 --> 00:39:58,000
so you can do some genetic experiments

830
00:39:58,000 --> 00:40:01,000
where you change one of the sequences,

831
00:40:01,000 --> 00:40:03,000
one of the residues, one of the amino acids,

832
00:40:03,000 --> 00:40:07,000
and then if it's on the surface from the predictor 3D structure,

833
00:40:07,000 --> 00:40:10,000
it should change the behavior of the protein.

834
00:40:10,000 --> 00:40:13,000
So you can sort of check if your 3D structure prediction

835
00:40:13,000 --> 00:40:16,000
saying that this residue is on the surface of the protein,

836
00:40:16,000 --> 00:40:20,000
you can sort of flip that out with a genetic mutation study

837
00:40:20,000 --> 00:40:23,000
and then see if that's affected the functioning of the protein.

838
00:40:23,000 --> 00:40:25,000
So there are sort of various ways after the fact

839
00:40:25,000 --> 00:40:28,000
to sort of check whether that's right.

840
00:40:29,000 --> 00:40:31,000
Okay, so alpha-fold then.

841
00:40:31,000 --> 00:40:33,000
The training data you're using there

842
00:40:33,000 --> 00:40:37,000
is the 100 to 150,000 proteins that have been figured out.

843
00:40:37,000 --> 00:40:39,000
What you have, as I understand it then,

844
00:40:39,000 --> 00:40:42,000
is their amino acid structures.

845
00:40:42,000 --> 00:40:47,000
You have the final protein 3D structure.

846
00:40:47,000 --> 00:40:51,000
And in the same way that you're setting your original system

847
00:40:51,000 --> 00:40:54,000
on PONG, pixel by pixel,

848
00:40:54,000 --> 00:40:57,000
try to get to an outcome where you're winning points,

849
00:40:57,000 --> 00:41:02,000
you are basically setting alpha-fold loose on this data

850
00:41:02,000 --> 00:41:08,000
and saying, try to figure out how to use the amino acid structure

851
00:41:08,000 --> 00:41:12,000
to predict the protein 3D structure,

852
00:41:12,000 --> 00:41:14,000
and when you do it correctly, you get points.

853
00:41:14,000 --> 00:41:16,000
Is that right?

854
00:41:16,000 --> 00:41:18,000
Yeah, that's basically how the system works.

855
00:41:18,000 --> 00:41:21,000
So you effectively have this amino acid sequence

856
00:41:21,000 --> 00:41:23,000
and you're telling it to predict the 3D structure

857
00:41:23,000 --> 00:41:26,000
and then you compare it against the real structure.

858
00:41:26,000 --> 00:41:28,000
There's various different ways you can compare it,

859
00:41:28,000 --> 00:41:30,000
but basically think about comparing

860
00:41:30,000 --> 00:41:33,000
where all the atoms end up being in 3D coordinate space

861
00:41:33,000 --> 00:41:36,000
and you sort of measure how far it's measured in angstroms,

862
00:41:36,000 --> 00:41:38,000
which is a tiny measure, right?

863
00:41:38,000 --> 00:41:40,000
Basically the width of an atom.

864
00:41:40,000 --> 00:41:44,000
How far away are you from the real 3D position of that atom?

865
00:41:44,000 --> 00:41:46,000
And for it to be useful for biologists,

866
00:41:46,000 --> 00:41:48,000
you've got to get the accuracy of that.

867
00:41:48,000 --> 00:41:50,000
All the atoms in the protein,

868
00:41:50,000 --> 00:41:52,000
and there are many, many thousands,

869
00:41:52,000 --> 00:41:55,000
within one atom width of the correct position.

870
00:41:55,000 --> 00:41:57,000
That's how accurate you have to get it for it

871
00:41:57,000 --> 00:42:00,000
to be useful for downstream biology purposes

872
00:42:00,000 --> 00:42:02,000
like drug discovery or disease understanding.

873
00:42:02,000 --> 00:42:04,000
So effectively the system gets a score

874
00:42:04,000 --> 00:42:06,000
from the average error it's making

875
00:42:06,000 --> 00:42:08,000
across all the atoms in the structure,

876
00:42:08,000 --> 00:42:11,000
and you're trying to get that to less than one angstrom,

877
00:42:11,000 --> 00:42:13,000
less than the width of an atom on average.

878
00:42:13,000 --> 00:42:16,000
So there's 100 to 150,000 of these,

879
00:42:16,000 --> 00:42:20,000
and then there's 100 million, 200 million proteins

880
00:42:20,000 --> 00:42:22,000
that we know of?

881
00:42:22,000 --> 00:42:23,000
That's right, yeah.

882
00:42:23,000 --> 00:42:26,000
So that's not a lot of training data, actually.

883
00:42:26,000 --> 00:42:30,000
You all then do something that I understand to be pretty dangerous

884
00:42:30,000 --> 00:42:33,000
and usually quite frowned upon,

885
00:42:33,000 --> 00:42:36,000
which is the system begins training itself

886
00:42:36,000 --> 00:42:38,000
on the predictions it is making.

887
00:42:38,000 --> 00:42:40,000
It is generating its own data then,

888
00:42:40,000 --> 00:42:43,000
and training itself on that data.

889
00:42:43,000 --> 00:42:45,000
And there's just a new paper that came out

890
00:42:45,000 --> 00:42:47,000
called The Curse of Recursion

891
00:42:47,000 --> 00:42:49,000
about how when AI systems begin training themselves

892
00:42:49,000 --> 00:42:51,000
on AI-generated data,

893
00:42:51,000 --> 00:42:52,000
the models often collapse.

894
00:42:52,000 --> 00:42:54,000
You're basically inbreeding the AI.

895
00:42:54,000 --> 00:42:57,000
So how do you do that

896
00:42:57,000 --> 00:43:00,000
in a way that does not inbreed your AI?

897
00:43:00,000 --> 00:43:02,000
Yeah, you're absolutely right.

898
00:43:02,000 --> 00:43:04,000
You have to be extremely careful

899
00:43:04,000 --> 00:43:06,000
when you start introducing its own, you know,

900
00:43:06,000 --> 00:43:08,000
an AI system's own predictions

901
00:43:08,000 --> 00:43:10,000
back into its training data.

902
00:43:10,000 --> 00:43:13,000
The reason we had to do it, and this is a very interesting,

903
00:43:13,000 --> 00:43:15,000
I think this is the best measure

904
00:43:15,000 --> 00:43:17,000
of how difficult this problem was.

905
00:43:17,000 --> 00:43:18,000
So as you point out,

906
00:43:18,000 --> 00:43:20,000
150,000 data points is tiny

907
00:43:20,000 --> 00:43:22,000
for machine learning systems.

908
00:43:22,000 --> 00:43:24,000
Usually you need millions and millions of data points, right?

909
00:43:24,000 --> 00:43:27,000
Like, for example, with AlphaGo, AlphaZero,

910
00:43:27,000 --> 00:43:29,000
we need a 10 million game,

911
00:43:29,000 --> 00:43:31,000
something like that, that it played itself.

912
00:43:31,000 --> 00:43:33,000
And of course, a game is far simpler

913
00:43:33,000 --> 00:43:35,000
than something like a, you know, protein structure in nature.

914
00:43:35,000 --> 00:43:38,000
So 150,000 is very, very minimal.

915
00:43:38,000 --> 00:43:40,000
I think most people assumed

916
00:43:40,000 --> 00:43:42,000
that there was not enough data,

917
00:43:42,000 --> 00:43:44,000
nowhere near enough data.

918
00:43:44,000 --> 00:43:46,000
And it turned out we had to throw the kitchen sink hat

919
00:43:46,000 --> 00:43:48,000
for Fold to make it work, everything we knew.

920
00:43:48,000 --> 00:43:50,000
So it's by far the most complicated system

921
00:43:50,000 --> 00:43:51,000
we ever worked on,

922
00:43:51,000 --> 00:43:53,000
and it's still the most complicated system we've worked on,

923
00:43:53,000 --> 00:43:56,000
and it took, you know, five years of work

924
00:43:56,000 --> 00:43:58,000
and many difficult wrong turns.

925
00:43:58,000 --> 00:44:01,000
And one of the things we had to do was augment the real data,

926
00:44:01,000 --> 00:44:03,000
which we didn't really have enough of,

927
00:44:03,000 --> 00:44:06,000
use it to build a first version of AlphaFold,

928
00:44:06,000 --> 00:44:08,000
and then that was just about good enough.

929
00:44:08,000 --> 00:44:11,000
I think we got it to do about a million predictions

930
00:44:11,000 --> 00:44:13,000
of new proteins.

931
00:44:13,000 --> 00:44:15,000
And when we got it to assess itself,

932
00:44:15,000 --> 00:44:18,000
how confident it was on those predictions,

933
00:44:18,000 --> 00:44:20,000
and then we sort of triaged it

934
00:44:20,000 --> 00:44:22,000
and cut the top sort of 30, 35%

935
00:44:22,000 --> 00:44:24,000
so around 300,000 predictions

936
00:44:24,000 --> 00:44:26,000
and put them back in the training set,

937
00:44:26,000 --> 00:44:29,000
along with the real data, the 150,000 real data.

938
00:44:29,000 --> 00:44:32,000
So then we had about half a million structures,

939
00:44:32,000 --> 00:44:34,000
obviously including its own predicted ones,

940
00:44:34,000 --> 00:44:36,000
to train the final system.

941
00:44:36,000 --> 00:44:38,000
And that system then was good enough

942
00:44:38,000 --> 00:44:40,000
to reach this atomic accuracy threshold.

943
00:44:40,000 --> 00:44:43,000
What that means is we only just about,

944
00:44:43,000 --> 00:44:45,000
as a scientific community, had enough data

945
00:44:45,000 --> 00:44:49,000
to bootstrap it to do this self-distillation process.

946
00:44:49,000 --> 00:44:51,000
And we think the reason that was okay

947
00:44:51,000 --> 00:44:53,000
was that we were very careful,

948
00:44:53,000 --> 00:44:55,000
first of all, we had enough real data,

949
00:44:55,000 --> 00:44:58,000
so there was a mixture of real and generated data,

950
00:44:58,000 --> 00:45:00,000
and that was enough to keep it on track.

951
00:45:00,000 --> 00:45:03,000
And also, there were lots of very good tests,

952
00:45:03,000 --> 00:45:06,000
independent tests of how good these predictions were

953
00:45:06,000 --> 00:45:08,000
of the overall system.

954
00:45:08,000 --> 00:45:10,000
Because of course, over time,

955
00:45:10,000 --> 00:45:12,000
experimenters were depositing new structures

956
00:45:12,000 --> 00:45:15,000
onto the database that were past

957
00:45:15,000 --> 00:45:18,000
the cut-off training date of when we trained the system.

958
00:45:18,000 --> 00:45:20,000
And so we could compare how accurate the system

959
00:45:20,000 --> 00:45:23,000
was against those new experimental data.

960
00:45:23,000 --> 00:45:24,000
So one thing you're saying here

961
00:45:24,000 --> 00:45:27,000
is the system is actually spitting out two things.

962
00:45:27,000 --> 00:45:30,000
It's spitting out a protein structure prediction

963
00:45:30,000 --> 00:45:33,000
and an uncertainty score.

964
00:45:33,000 --> 00:45:35,000
And that's a really actually cool thing

965
00:45:35,000 --> 00:45:37,000
about the AlphaFold system too,

966
00:45:37,000 --> 00:45:39,000
is that in many machine learning systems,

967
00:45:39,000 --> 00:45:41,000
we would love the system

968
00:45:41,000 --> 00:45:44,000
to not only produce the output, the prediction,

969
00:45:44,000 --> 00:45:48,000
but also measure of its uncertainty about that prediction,

970
00:45:48,000 --> 00:45:50,000
a confidence measure about it.

971
00:45:50,000 --> 00:45:53,000
And that's actually very important in all modern AI systems,

972
00:45:53,000 --> 00:45:55,000
that it would be nice if they, well, it would be very good

973
00:45:55,000 --> 00:45:56,000
if they all did that.

974
00:45:56,000 --> 00:45:58,000
But actually, very few systems do currently,

975
00:45:58,000 --> 00:46:01,000
and it's not known how to do that in general, right?

976
00:46:01,000 --> 00:46:03,000
It's an active area of research.

977
00:46:03,000 --> 00:46:05,000
But we managed to do that with AlphaFold,

978
00:46:05,000 --> 00:46:07,000
and the reason we put so much effort into that

979
00:46:07,000 --> 00:46:10,000
is that ultimately, I wanted this to be useful

980
00:46:10,000 --> 00:46:13,000
to biologists and scientists and medical researchers,

981
00:46:13,000 --> 00:46:17,000
amazing experts, domain experts in their area,

982
00:46:17,000 --> 00:46:19,000
but most of them would not know anything

983
00:46:19,000 --> 00:46:22,000
about machine learning or care, actually, frankly, right?

984
00:46:22,000 --> 00:46:24,000
They're just interested in the structure of the protein

985
00:46:24,000 --> 00:46:26,000
so they can go and cure a disease.

986
00:46:26,000 --> 00:46:30,000
And so it was really important for this particular task

987
00:46:30,000 --> 00:46:32,000
that if these outputs were going to be useful

988
00:46:32,000 --> 00:46:34,000
to anyone, researchers down the line,

989
00:46:34,000 --> 00:46:36,000
it would have to output its confidence level

990
00:46:36,000 --> 00:46:39,000
on the basis of each amino acid.

991
00:46:39,000 --> 00:46:41,000
So it color-coded it in really simple ways

992
00:46:41,000 --> 00:46:44,000
so that anybody, non-expert of machine learning

993
00:46:44,000 --> 00:46:47,000
can understand which parts of the prediction

994
00:46:47,000 --> 00:46:49,000
could they trust as an experimentalist

995
00:46:49,000 --> 00:46:51,000
and which other parts should they basically

996
00:46:51,000 --> 00:46:53,000
probably continue to do experiments on

997
00:46:53,000 --> 00:46:55,000
if they wanted to know the real structure,

998
00:46:55,000 --> 00:46:57,000
or at least tread with caution.

999
00:46:57,000 --> 00:47:01,000
And we wanted the system to really clearly output that

1000
00:47:01,000 --> 00:47:04,000
and those confidence levels to be really accurate.

1001
00:47:04,000 --> 00:47:06,000
So in effect, we needed to do that anyway

1002
00:47:06,000 --> 00:47:08,000
for this tool to be useful downstream

1003
00:47:08,000 --> 00:47:10,000
to biologists and medical researchers,

1004
00:47:10,000 --> 00:47:13,000
but we ended up using that same confidence level

1005
00:47:13,000 --> 00:47:16,000
to allow us to triage our own generated data

1006
00:47:16,000 --> 00:47:19,000
and put back the more confident, better ones

1007
00:47:19,000 --> 00:47:21,000
into the training data.

1008
00:47:21,000 --> 00:47:23,000
So I want to unite this with something

1009
00:47:23,000 --> 00:47:25,000
that people who've been following AI

1010
00:47:25,000 --> 00:47:27,000
are probably more familiar with,

1011
00:47:27,000 --> 00:47:29,000
which is the hallucination problem.

1012
00:47:29,000 --> 00:47:33,000
So when I use chat GPT or at least different versions of it,

1013
00:47:33,000 --> 00:47:37,000
it is common that you can ask it a kind of question,

1014
00:47:37,000 --> 00:47:39,000
like tell me all about this one chemist

1015
00:47:39,000 --> 00:47:41,000
from so-and-so who did such-and-such,

1016
00:47:41,000 --> 00:47:43,000
and you can make it up and it can make it up,

1017
00:47:43,000 --> 00:47:45,000
or maybe you ask it a real question

1018
00:47:45,000 --> 00:47:47,000
and it just makes up a citation for you.

1019
00:47:47,000 --> 00:47:50,000
This has been a big problem with the large language models,

1020
00:47:50,000 --> 00:47:52,000
still a big problem with the large language models.

1021
00:47:52,000 --> 00:47:55,000
And the theory is that the reason it's a problem

1022
00:47:55,000 --> 00:47:58,000
is that they're just making predictions, right?

1023
00:47:58,000 --> 00:48:00,000
They don't know what they're doing

1024
00:48:00,000 --> 00:48:02,000
in the same way that your system didn't know it was playing pong.

1025
00:48:02,000 --> 00:48:04,000
They just know that on the internet,

1026
00:48:04,000 --> 00:48:06,000
on the training data they've been given,

1027
00:48:06,000 --> 00:48:09,000
this is the word that would be most likely

1028
00:48:09,000 --> 00:48:11,000
to come next in a sentence.

1029
00:48:11,000 --> 00:48:15,000
So how come AlphaFold doesn't have

1030
00:48:15,000 --> 00:48:18,000
this kind of hallucination problem?

1031
00:48:18,000 --> 00:48:21,000
Yeah, so the current chatbots today

1032
00:48:21,000 --> 00:48:24,000
have this problem, and partly it's because

1033
00:48:24,000 --> 00:48:27,000
if they were better able to understand the confidence

1034
00:48:27,000 --> 00:48:31,000
and the likelihood that what they're putting out putting

1035
00:48:31,000 --> 00:48:33,000
is correct, they could at some point say,

1036
00:48:33,000 --> 00:48:36,000
I don't know, that would be better than making something up,

1037
00:48:36,000 --> 00:48:39,000
or they could sort of caveat it by it might be this,

1038
00:48:39,000 --> 00:48:41,000
but perhaps you should double check.

1039
00:48:41,000 --> 00:48:43,000
And in fact, if they were able to do that,

1040
00:48:43,000 --> 00:48:45,000
they could cross-check their references.

1041
00:48:45,000 --> 00:48:47,000
That's what they should be doing,

1042
00:48:47,000 --> 00:48:49,000
using tools perhaps even like search to sort of go,

1043
00:48:49,000 --> 00:48:52,000
oh, actually, that paper doesn't really exist.

1044
00:48:52,000 --> 00:48:54,000
Just go and look it up on PubMed.

1045
00:48:54,000 --> 00:48:57,000
It's not there, even though it's very plausible sounding.

1046
00:48:57,000 --> 00:48:59,000
And so in fact, the human users,

1047
00:48:59,000 --> 00:49:02,000
I'm sure you've had experiences, you have to go and look it up.

1048
00:49:02,000 --> 00:49:04,000
Just a very funny idea, right, to make up a paper,

1049
00:49:04,000 --> 00:49:06,000
then go search to see if the paper you've made up

1050
00:49:06,000 --> 00:49:09,000
is actually there, and like, oh, it's actually not.

1051
00:49:09,000 --> 00:49:11,000
It shows me. Yeah, exactly.

1052
00:49:11,000 --> 00:49:13,000
But it would be better if it did that internally

1053
00:49:13,000 --> 00:49:15,000
before we output that to the user.

1054
00:49:15,000 --> 00:49:17,000
So you never saw its hallucination.

1055
00:49:17,000 --> 00:49:20,000
And in a way, that's what is missing from the current systems

1056
00:49:20,000 --> 00:49:22,000
is this actually what AlphaGo does,

1057
00:49:22,000 --> 00:49:24,000
and at the systems we build,

1058
00:49:24,000 --> 00:49:26,000
where there's a little bit of thinking time or search

1059
00:49:26,000 --> 00:49:28,000
or planning that's going on

1060
00:49:28,000 --> 00:49:30,000
before they output their prediction.

1061
00:49:30,000 --> 00:49:34,000
Right now, they're kind of almost like idiot savants, right?

1062
00:49:34,000 --> 00:49:38,000
They just output the immediate thing that just first comes to mind.

1063
00:49:38,000 --> 00:49:40,000
And it may or may not be plausible,

1064
00:49:40,000 --> 00:49:42,000
and it may or may not be correct.

1065
00:49:42,000 --> 00:49:44,000
And we need a bit of sort of, I would say,

1066
00:49:44,000 --> 00:49:47,000
deliberation and planning and reasoning

1067
00:49:47,000 --> 00:49:49,000
to kind of almost sanity check

1068
00:49:49,000 --> 00:49:51,000
what is that the prediction is telling you,

1069
00:49:51,000 --> 00:49:54,000
not just output the first thing that comes to mind.

1070
00:49:54,000 --> 00:49:56,000
And the kind of deep reinforcement learning systems

1071
00:49:56,000 --> 00:49:58,000
we're known for do do that, right?

1072
00:49:58,000 --> 00:50:00,000
Effectively, the reinforcement learning part, the planning part,

1073
00:50:00,000 --> 00:50:02,000
calibrates what the model is telling it.

1074
00:50:02,000 --> 00:50:04,000
It's not just the first most likely move

1075
00:50:04,000 --> 00:50:06,000
that you play in every positioning go.

1076
00:50:06,000 --> 00:50:08,000
It's the best move that you want.

1077
00:50:08,000 --> 00:50:12,000
Is this the difference between building a system

1078
00:50:12,000 --> 00:50:15,000
that is ultimately working with a structured data set

1079
00:50:15,000 --> 00:50:17,000
where, at least for some amount of training data,

1080
00:50:17,000 --> 00:50:19,000
it knows if it has the right answer

1081
00:50:19,000 --> 00:50:22,000
and something that is using a truly unstructured,

1082
00:50:22,000 --> 00:50:24,000
and I don't mean that in the technical way,

1083
00:50:24,000 --> 00:50:26,000
but in the sort of colloquial way,

1084
00:50:26,000 --> 00:50:30,000
the unstructured data set of life where people just talk

1085
00:50:30,000 --> 00:50:34,000
and conversations don't have a right answer or wrong answer.

1086
00:50:34,000 --> 00:50:37,000
If you're inhaling the entire corpus of Reddit,

1087
00:50:37,000 --> 00:50:40,000
Reddit is not about did you get the right answer,

1088
00:50:40,000 --> 00:50:42,000
people just talking.

1089
00:50:42,000 --> 00:50:44,000
And so is what's going on here that because AlphaFold

1090
00:50:44,000 --> 00:50:47,000
has this 100,000 proteins in there

1091
00:50:47,000 --> 00:50:49,000
that it knows if they're right or not,

1092
00:50:49,000 --> 00:50:52,000
and so it knows what it looks like to get something right,

1093
00:50:52,000 --> 00:50:54,000
that you can then build a system where the point is

1094
00:50:54,000 --> 00:50:56,000
to get something right, but when you're building

1095
00:50:56,000 --> 00:50:59,000
these much more generalized language-oriented systems,

1096
00:50:59,000 --> 00:51:01,000
that that isn't the structure of language.

1097
00:51:01,000 --> 00:51:04,000
Language doesn't, in some internal way,

1098
00:51:04,000 --> 00:51:06,000
have like an input and an output

1099
00:51:06,000 --> 00:51:08,000
where you can see that some outputs were correct

1100
00:51:08,000 --> 00:51:10,000
and some outputs weren't.

1101
00:51:10,000 --> 00:51:12,000
Yeah, I think that's the right intuition.

1102
00:51:12,000 --> 00:51:14,000
I mean, I think language and the way, obviously,

1103
00:51:14,000 --> 00:51:16,000
on the internet encapsulates a huge slice

1104
00:51:16,000 --> 00:51:19,000
of human civilization knowledge.

1105
00:51:19,000 --> 00:51:21,000
It's far more complex than a game

1106
00:51:21,000 --> 00:51:24,000
and perhaps even proteins in a kind of general sense.

1107
00:51:24,000 --> 00:51:27,000
I think the difference is actually that in games,

1108
00:51:27,000 --> 00:51:30,000
especially but also even with protein structures,

1109
00:51:30,000 --> 00:51:33,000
you can automate the correction process.

1110
00:51:33,000 --> 00:51:35,000
Like if you don't win the game, then obviously the things

1111
00:51:35,000 --> 00:51:38,000
you were planning or the moves that you tried to make

1112
00:51:38,000 --> 00:51:41,000
or the reasoning you did wasn't very good to a certain extent.

1113
00:51:41,000 --> 00:51:44,000
So you can immediately update on that in an automated way.

1114
00:51:44,000 --> 00:51:46,000
The same with protein structures.

1115
00:51:46,000 --> 00:51:48,000
If the final structures that you're predicting

1116
00:51:48,000 --> 00:51:51,000
have very large errors compared to the known structures,

1117
00:51:51,000 --> 00:51:53,000
you've obviously done something wrong.

1118
00:51:53,000 --> 00:51:55,000
There's no subjective decision there, neither.

1119
00:51:55,000 --> 00:51:57,000
You can just automate that.

1120
00:51:57,000 --> 00:51:59,000
Of course, with language and knowledge, human knowledge,

1121
00:51:59,000 --> 00:52:01,000
it's much more nuanced than that.

1122
00:52:01,000 --> 00:52:03,000
But as we talked earlier, if you hallucinate

1123
00:52:03,000 --> 00:52:06,000
a new reference to a paper that doesn't exist,

1124
00:52:06,000 --> 00:52:08,000
that's pretty black and white, right?

1125
00:52:08,000 --> 00:52:10,000
Like, you know that that's wrong.

1126
00:52:10,000 --> 00:52:12,000
So there are a lot of things, I think,

1127
00:52:12,000 --> 00:52:14,000
that could be done far better than we're doing today

1128
00:52:14,000 --> 00:52:16,000
and we're working really hard on increasing

1129
00:52:16,000 --> 00:52:18,000
the magnitude, the factuality, and the reliability

1130
00:52:18,000 --> 00:52:20,000
of these systems.

1131
00:52:20,000 --> 00:52:23,000
And I don't see any reason why that cannot be improved.

1132
00:52:23,000 --> 00:52:26,000
But also, there are some things which are a bit more subjective,

1133
00:52:26,000 --> 00:52:28,000
and then you need human feedback on it,

1134
00:52:28,000 --> 00:52:30,000
which is why everyone's using reinforcement learning

1135
00:52:30,000 --> 00:52:33,000
with human feedback to sort of train these systems

1136
00:52:33,000 --> 00:52:35,000
better to behave in ways we would like.

1137
00:52:35,000 --> 00:52:37,000
But of course, if you're relying on human feedback,

1138
00:52:37,000 --> 00:52:39,000
that itself is quite a nosy process

1139
00:52:39,000 --> 00:52:42,000
and very time-consuming and takes a large effort.

1140
00:52:42,000 --> 00:52:45,000
And so it's not as quick or as automated

1141
00:52:45,000 --> 00:52:47,000
as when you have some objective measure

1142
00:52:47,000 --> 00:52:49,000
that you can just optimize against.

1143
00:52:49,000 --> 00:52:52,000
So we talked about the exponential curves

1144
00:52:52,000 --> 00:52:54,000
that you all have seen again and again

1145
00:52:54,000 --> 00:52:56,000
in the gameplay systems.

1146
00:52:56,000 --> 00:52:58,000
And you also mentioned that the rule of thumb

1147
00:52:58,000 --> 00:53:01,000
in proteins is it takes one PhD researcher,

1148
00:53:01,000 --> 00:53:04,000
their whole PhD, to figure out the structure of one protein.

1149
00:53:04,000 --> 00:53:08,000
So tell me the timeline then of building off a fold

1150
00:53:08,000 --> 00:53:11,000
and then of beginning to find the proteins.

1151
00:53:11,000 --> 00:53:14,000
My understanding is that there's this kind of slow takeoff

1152
00:53:14,000 --> 00:53:16,000
and then a real takeoff. So what happens here?

1153
00:53:16,000 --> 00:53:18,000
We worked on Alpha Fold.

1154
00:53:18,000 --> 00:53:20,000
A couple of versions of Alpha Fold, actually.

1155
00:53:20,000 --> 00:53:22,000
We had to go to the drawing board at one point

1156
00:53:22,000 --> 00:53:24,000
when we hit an asymptote over around a four-year,

1157
00:53:24,000 --> 00:53:27,000
four-and-a-half-year period from about 2016 to 2020.

1158
00:53:27,000 --> 00:53:32,000
And then we entered it into this competition called CASP,

1159
00:53:32,000 --> 00:53:34,000
which is you can think of it as like the Olympics

1160
00:53:34,000 --> 00:53:35,000
for protein folding.

1161
00:53:35,000 --> 00:53:38,000
So every two years, all of the people working on this

1162
00:53:38,000 --> 00:53:41,000
from all the labs around the world enter this competition.

1163
00:53:41,000 --> 00:53:43,000
It's an amazing competition because what they do

1164
00:53:43,000 --> 00:53:46,000
is they over the last sort of few months, experimentalists

1165
00:53:46,000 --> 00:53:48,000
give them their protein structures.

1166
00:53:48,000 --> 00:53:50,000
They've just found literally hot off the press then,

1167
00:53:50,000 --> 00:53:52,000
but not published yet.

1168
00:53:52,000 --> 00:53:54,000
So they're unknown to anyone other than the experimental lab

1169
00:53:54,000 --> 00:53:56,000
that produced it.

1170
00:53:56,000 --> 00:53:58,000
And they give it to the competition organizers.

1171
00:53:58,000 --> 00:54:00,000
The competition organizers give it to the competing

1172
00:54:00,000 --> 00:54:01,000
computational teams.

1173
00:54:01,000 --> 00:54:04,000
We have to submit within a week our predictions.

1174
00:54:04,000 --> 00:54:06,000
And then later on at the end of the summer,

1175
00:54:06,000 --> 00:54:07,000
this happens all over the summer.

1176
00:54:07,000 --> 00:54:10,000
There's like 100 proteins you get in the competition.

1177
00:54:10,000 --> 00:54:12,000
And then they reveal the true structures.

1178
00:54:12,000 --> 00:54:13,000
You know, they get published.

1179
00:54:13,000 --> 00:54:16,000
And then you compare, obviously you have this double blind

1180
00:54:16,000 --> 00:54:19,000
scoring system where nobody knew who the competing teams were

1181
00:54:19,000 --> 00:54:21,000
and nobody knew what the real structures were

1182
00:54:21,000 --> 00:54:22,000
until the end of the competition.

1183
00:54:22,000 --> 00:54:25,000
So it's a beautifully designed competition.

1184
00:54:25,000 --> 00:54:27,000
And the organizers have been running it for 30 years,

1185
00:54:27,000 --> 00:54:29,000
incredible dedication to do that.

1186
00:54:29,000 --> 00:54:31,000
And that was another reason we picked this problem to work on

1187
00:54:31,000 --> 00:54:33,000
because it had this amazing competition.

1188
00:54:33,000 --> 00:54:35,000
So there was actually a game you could win.

1189
00:54:35,000 --> 00:54:37,000
There was a game we could win and a leaderboard

1190
00:54:37,000 --> 00:54:39,000
we could optimize against.

1191
00:54:39,000 --> 00:54:41,000
And then when those revealed got revealed at the end of 2020,

1192
00:54:42,000 --> 00:54:44,000
then it was announced in a big conference

1193
00:54:44,000 --> 00:54:48,000
and the organizers sort of proclaimed that the structure

1194
00:54:48,000 --> 00:54:50,000
prediction problem or the protein folding problem

1195
00:54:50,000 --> 00:54:54,000
had been solved because we got to within atomic accuracy

1196
00:54:54,000 --> 00:54:57,000
on these predictions of these 100 new proteins.

1197
00:54:57,000 --> 00:55:00,000
And so that was the moment where we knew we had a system

1198
00:55:00,000 --> 00:55:03,000
that was going to be really useful for experimentalists

1199
00:55:03,000 --> 00:55:05,000
and drug design and so on.

1200
00:55:05,000 --> 00:55:08,000
And then the next question was how do we gift this to the world

1201
00:55:08,000 --> 00:55:11,000
and in a way that all these world researchers and biologists

1202
00:55:11,000 --> 00:55:14,000
and medical researchers could make the fastest use of

1203
00:55:14,000 --> 00:55:17,000
to have the biggest impact downstream.

1204
00:55:17,000 --> 00:55:20,000
And what we realized is not only was the system really accurate

1205
00:55:20,000 --> 00:55:23,000
alpha fold, it was also extremely fast.

1206
00:55:23,000 --> 00:55:26,000
So we could fold an average length protein

1207
00:55:26,000 --> 00:55:28,000
in a matter of seconds.

1208
00:55:28,000 --> 00:55:30,000
And then when we did the calculation, it was like,

1209
00:55:30,000 --> 00:55:33,000
well, there are roughly 200 million protein sequences,

1210
00:55:33,000 --> 00:55:36,000
genetic sequences known to science.

1211
00:55:36,000 --> 00:55:39,000
We could probably fold all of them in a year.

1212
00:55:39,000 --> 00:55:41,000
So that's what we set out to do.

1213
00:55:41,000 --> 00:55:43,000
We started off with the most important organisms,

1214
00:55:43,000 --> 00:55:45,000
obviously the human proteome.

1215
00:55:45,000 --> 00:55:47,000
So it's equivalent to the human genome, but in protein space.

1216
00:55:47,000 --> 00:55:50,000
And then we went to all the important research organisms,

1217
00:55:50,000 --> 00:55:53,000
you know, the mouse, the fly, the zebrafish and so on.

1218
00:55:53,000 --> 00:55:56,000
And then some important crops like wheat and rice and so on.

1219
00:55:56,000 --> 00:55:58,000
They're very important, obviously to humanity.

1220
00:55:58,000 --> 00:56:00,000
And so then we put all of those 20 out first

1221
00:56:00,000 --> 00:56:03,000
and then eventually over 2021, we did all of them.

1222
00:56:03,000 --> 00:56:06,000
And then we put that out as a database, free access database

1223
00:56:06,000 --> 00:56:09,000
to the world, the research community in collaboration

1224
00:56:09,000 --> 00:56:13,000
with the European Bioinformatics Institute based in Cambridge.

1225
00:56:13,000 --> 00:56:16,000
Before we move on to some other work you all are doing,

1226
00:56:16,000 --> 00:56:19,000
one thing that as I understand it, alpha fold has spun out into now

1227
00:56:19,000 --> 00:56:23,000
is a group under Alphabet, the Google parent company

1228
00:56:23,000 --> 00:56:25,000
called Isomorphic.

1229
00:56:25,000 --> 00:56:29,000
Tell me a bit about Isomorphic and both the sort of scientific

1230
00:56:29,000 --> 00:56:32,000
theory there, but also it's a very different theory

1231
00:56:32,000 --> 00:56:34,000
of how AI could make money.

1232
00:56:34,000 --> 00:56:37,000
Then, you know, we're going to add a chat bot into a search engine.

1233
00:56:37,000 --> 00:56:40,000
So what's the business theory there?

1234
00:56:40,000 --> 00:56:44,000
Yeah, so alpha fold is a grand challenge in biology

1235
00:56:44,000 --> 00:56:46,000
of understanding the structure of proteins.

1236
00:56:46,000 --> 00:56:49,000
The reason I thought that was so important was

1237
00:56:49,000 --> 00:56:52,000
because I think it can hugely accelerate,

1238
00:56:52,000 --> 00:56:56,000
be part of accelerating drug discovery and therefore curing diseases.

1239
00:56:56,000 --> 00:56:58,000
But it's only one part of the puzzle.

1240
00:56:58,000 --> 00:57:00,000
So, you know, knowing the structure of proteins,

1241
00:57:00,000 --> 00:57:04,000
only one small bit of the whole drug discovery process.

1242
00:57:04,000 --> 00:57:07,000
So there are many other really important things from identifying

1243
00:57:07,000 --> 00:57:09,000
which proteins you should target, so, you know,

1244
00:57:09,000 --> 00:57:12,000
maybe through genetic analysis and genomics,

1245
00:57:12,000 --> 00:57:16,000
all the way to, like, can we design a small molecule,

1246
00:57:16,000 --> 00:57:19,000
a drug compound, a chemical compound that will correctly

1247
00:57:19,000 --> 00:57:22,000
bind to that protein and the bit of the protein you want

1248
00:57:22,000 --> 00:57:25,000
blocked or bind to and not anything else in your body

1249
00:57:25,000 --> 00:57:27,000
because that's side effects, right?

1250
00:57:27,000 --> 00:57:29,000
Effectively, that's what makes things toxic is

1251
00:57:29,000 --> 00:57:31,000
they don't just bind to the thing you want,

1252
00:57:31,000 --> 00:57:33,000
but they bind to all sorts of other proteins

1253
00:57:33,000 --> 00:57:34,000
that you didn't want them to.

1254
00:57:34,000 --> 00:57:37,000
So, in my view, AI is the perfect tool

1255
00:57:37,000 --> 00:57:40,000
to accelerate the time scales of doing that

1256
00:57:40,000 --> 00:57:43,000
because that's done right now in Big Pharma

1257
00:57:43,000 --> 00:57:47,000
in a painstakingly experimental way that takes many, many years.

1258
00:57:47,000 --> 00:57:50,000
You know, I think the average time of going from a target

1259
00:57:50,000 --> 00:57:53,000
to a compound you can start testing in clinical trials

1260
00:57:53,000 --> 00:57:57,000
is, like, five, six years and many, many hundreds of millions of dollars

1261
00:57:57,000 --> 00:58:00,000
per drug, which is incredibly slow and incredibly expensive,

1262
00:58:00,000 --> 00:58:03,000
and I think that could be brought down by an order of magnitude

1263
00:58:03,000 --> 00:58:06,000
using AI and computational techniques

1264
00:58:06,000 --> 00:58:09,000
to do the exploration part, do that in silico,

1265
00:58:09,000 --> 00:58:11,000
using AI and computational techniques,

1266
00:58:11,000 --> 00:58:14,000
and then only at the last step saving experimental work,

1267
00:58:14,000 --> 00:58:17,000
of course, very important experimental work and wet lab work

1268
00:58:17,000 --> 00:58:19,000
for the validation step.

1269
00:58:19,000 --> 00:58:22,000
So, instead of doing all the search, which is the expensive slow part,

1270
00:58:22,000 --> 00:58:24,000
you just do it for validating the compounds

1271
00:58:24,000 --> 00:58:26,000
that your AI system has come up with.

1272
00:58:26,000 --> 00:58:30,000
And so isomorphic is our spin-out, sort of sister company to deep mind,

1273
00:58:30,000 --> 00:58:35,000
that is tasked with building more alpha folds, breakthroughs,

1274
00:58:35,000 --> 00:58:39,000
but in adjacent spaces, so more going into chemistry,

1275
00:58:39,000 --> 00:58:43,000
so designing small molecules, predicting the different properties

1276
00:58:43,000 --> 00:58:45,000
of those small molecules called admi properties

1277
00:58:45,000 --> 00:58:48,000
and making sure, like, we minimize things like toxicity

1278
00:58:48,000 --> 00:58:50,000
and side effects and so on

1279
00:58:50,000 --> 00:58:54,000
and maximize its potential at binding to the protein that we want.

1280
00:58:56,000 --> 00:58:59,000
â

1281
00:59:21,000 --> 00:59:25,000
So there are games, or things that are structured a bit like games-

1282
00:59:25,000 --> 00:59:29,520
that I'm very excited about the possibility of AI winning or getting unbelievably good

1283
00:59:29,520 --> 00:59:34,120
at. So drug discovery being one of them. And then there are ones where I'm a little more

1284
00:59:34,120 --> 00:59:37,800
nervous. I've heard you say, for instance, that from a certain perspective, the stock

1285
00:59:37,800 --> 00:59:43,440
market very much has the structure of a game. And if I am a very rich hedge fund and a lot

1286
00:59:43,440 --> 00:59:47,360
of them do algorithmic trading, I mean, if theme park, the game was AI, then definitely

1287
00:59:47,360 --> 00:59:51,720
what a lot of these hedge funds are doing is AI, they've got a lot of money. If you were

1288
00:59:51,760 --> 00:59:57,320
thinking about a system to win the stock market, what does that look like? I mean, there's a

1289
00:59:57,320 --> 00:59:59,520
lot of training data out there. Like, what do you do?

1290
01:00:00,600 --> 01:00:04,680
Yeah, I think almost certainly some of the top hedge funds must be using, I would have

1291
01:00:04,680 --> 01:00:08,360
thought, some of these techniques that we and others have invented to trade on the stock

1292
01:00:08,360 --> 01:00:12,680
market. It has some of the same properties, as you say, I mean, finance friends of mine

1293
01:00:12,680 --> 01:00:16,680
talk about it as being the biggest game in some ways, right? That's sometimes how it's

1294
01:00:16,680 --> 01:00:21,560
talked about for better or for worse. And I'm sure a lot of these techniques would work.

1295
01:00:21,760 --> 01:00:27,480
Now, the interesting thing is whether you just treat the stock market, say, as a series of

1296
01:00:27,480 --> 01:00:31,600
numbers. And that's one theory that you just treat it as a big sequence of numbers, you

1297
01:00:31,600 --> 01:00:35,280
know, time series of numbers. And you're just trying to predict the next numbers in the

1298
01:00:35,280 --> 01:00:38,840
sequence. You know, you could imagine that's analogous to predicting the next word, you

1299
01:00:38,840 --> 01:00:42,880
know, with the chatbots. So that's possible. My view is it's probably a bit more complex

1300
01:00:42,880 --> 01:00:46,520
than that, because those numbers actually describe real things, you know, profits and

1301
01:00:46,520 --> 01:00:51,160
losses, real companies, and then real people running those companies and having ideas and

1302
01:00:51,320 --> 01:00:56,480
so on. And there's also macroeconomic forces like geopolitical forces and interest rates

1303
01:00:56,480 --> 01:01:02,040
set by governments and so on. And so my thinking is that probably to understand the full

1304
01:01:02,040 --> 01:01:07,680
context required to predict the next number in that sequence would require you to understand

1305
01:01:07,720 --> 01:01:12,480
a lot more about the world than just the stock prices. So you'd somehow have to

1306
01:01:12,520 --> 01:01:17,200
encapsulate all of that knowledge in a way that the machine could ingest and understand.

1307
01:01:18,240 --> 01:01:22,760
Well, you need to do that to fully win the game. But to come up with local strategies, it

1308
01:01:22,760 --> 01:01:26,840
could be very profitable and or very destructive. I mean, one, we already know that all kinds

1309
01:01:26,840 --> 01:01:30,560
of firms have done that with high speed algorithmic trading. Yes. And two, you could just

1310
01:01:30,560 --> 01:01:35,120
imagine all kinds of, again, if you're willing to run through a very large search space and

1311
01:01:35,120 --> 01:01:38,960
try strategies other people don't try, I mean, you know, you could short out this company

1312
01:01:38,960 --> 01:01:42,600
destroying this competitor such as this other one, you could predict would go up immediately

1313
01:01:42,600 --> 01:01:49,280
if that happened. And you can imagine very weird strategies being deployed by a system

1314
01:01:49,560 --> 01:01:55,240
that has the power to move money around and, you know, a lot of data in it and is just

1315
01:01:55,240 --> 01:01:57,280
getting reinforcement learning for making money.

1316
01:01:57,840 --> 01:02:01,600
Yeah, you could imagine that. I think that as I understand it, that world, I mean, there's

1317
01:02:01,600 --> 01:02:05,720
lots of very, very smart people working in that world with algorithms, not necessarily

1318
01:02:05,880 --> 01:02:09,760
the latest machine learning ones, or the latest statistical algorithms. And they're very

1319
01:02:09,760 --> 01:02:15,040
sophisticated because obviously they're very incented to do that, given that it's literally

1320
01:02:15,040 --> 01:02:21,200
money at stake. So I imagine the efficiency is pretty high already in hedge funds and

1321
01:02:21,240 --> 01:02:26,000
high frequency trading and other things you mentioned, where, you know, there's already

1322
01:02:26,160 --> 01:02:32,320
ways to slightly game the system, perhaps, that are already quite profitable. And it's not

1323
01:02:32,320 --> 01:02:37,720
clear to me that sort of more general learning system would be better than that. It may be

1324
01:02:37,720 --> 01:02:42,240
different from that, but it may already be easier. There may be easier ways, which hedge

1325
01:02:42,240 --> 01:02:45,040
funds are probably already doing, I assume, to make that money.

1326
01:02:45,520 --> 01:02:51,560
Well, this actually brings up, I think, a pretty big question for me, which is one of the

1327
01:02:51,560 --> 01:02:56,320
reasons I wanted to have this conversation with you about Alpha Fold is I think people are now

1328
01:02:56,320 --> 01:03:00,760
used to thinking about these very generalized large language models, inhale a tremendous amount

1329
01:03:00,760 --> 01:03:06,920
of data, come up with these patterns, and then, you know, they're able to answer a lot of kinds

1330
01:03:06,920 --> 01:03:11,800
of questions at a certain level, but not a very high level of rigor. And then there's this other

1331
01:03:11,800 --> 01:03:16,760
approach, which is building much more bespoke systems. I mean, Alpha Fold can do something

1332
01:03:16,760 --> 01:03:21,320
amazing in the protein space, and it cannot help me write a college essay, as best as I

1333
01:03:21,320 --> 01:03:28,600
understand it. And I don't want to put too much of a binary choice here, because I think I know

1334
01:03:28,600 --> 01:03:33,640
that there is overlap, but I do think there have been sort of two theories of how AI is going to

1335
01:03:33,640 --> 01:03:38,680
roll out over time. And one is we're going to have lots of different specialized systems that

1336
01:03:38,680 --> 01:03:43,240
are tuned to do different things, a system to do legal contracts, and a system to do proteins,

1337
01:03:43,240 --> 01:03:47,560
and a system to check out radiology results, and a system to help kids with their homework.

1338
01:03:48,200 --> 01:03:55,080
And another is that we are eventually going to pump enough data into GPT-12 or whatever it might be,

1339
01:03:55,880 --> 01:04:01,240
such it it attains a kind of general intelligence, that it becomes a system that can do everything,

1340
01:04:01,880 --> 01:04:06,040
that the general system eventually will emerge, and that system will be able to do all these

1341
01:04:06,040 --> 01:04:09,720
things. And so what you should really be working on is that, can you talk a bit about, because I

1342
01:04:09,720 --> 01:04:13,320
know you're interested in building a general intelligence system, can you tell me a bit about

1343
01:04:13,320 --> 01:04:18,200
what you understand to be the path to that now? Do we want a lot of little systems or not little,

1344
01:04:18,200 --> 01:04:24,280
but specialized, or is the theory here that no, this is right, you want it all in one system

1345
01:04:24,280 --> 01:04:27,560
that is going to be able to span across functionally every domain?

1346
01:04:28,520 --> 01:04:33,720
Yeah, that's a fascinating question. And actually, DeepMind was founded in our still

1347
01:04:33,720 --> 01:04:38,760
our mission is to create that big general system. That's of course, the way the brain works, right?

1348
01:04:38,760 --> 01:04:43,720
We have one system and it can do, we can do many things with our minds, including science and

1349
01:04:43,720 --> 01:04:49,160
playing chess and all with the same brain, right? So that's the ultimate goal. Now, interestingly,

1350
01:04:49,160 --> 01:04:55,320
on the way to that goal, I always believed that we don't have to wait to get into general

1351
01:04:55,320 --> 01:05:01,560
intelligence or AGI before we can get incredible use out of these systems by using the same

1352
01:05:01,560 --> 01:05:07,080
sorts of techniques, but maybe specializing them around a particular domain. And AlphaFold

1353
01:05:07,080 --> 01:05:11,880
is a great example of that, perhaps the best example so far in AI of that. And AlphaGo,

1354
01:05:11,880 --> 01:05:16,520
obviously, all our game systems were like that too. And so what I think is going to happen in the

1355
01:05:16,520 --> 01:05:21,560
next era of systems, and we're working on our own systems called Gemini, is that I think there's

1356
01:05:21,560 --> 01:05:25,640
going to be a kind of combination of the two things. So we'll have this increasingly more

1357
01:05:25,640 --> 01:05:30,600
powerful general system that you basically interact with through language, but has other

1358
01:05:30,600 --> 01:05:35,320
capabilities, general capabilities like math and coding, and perhaps some reasoning and

1359
01:05:35,320 --> 01:05:39,880
planning eventually in the next generations of these systems. One of the things these systems

1360
01:05:39,960 --> 01:05:46,920
can do is use tools. So tool use is a big part of the research area now of these language models

1361
01:05:46,920 --> 01:05:52,600
or chatbots. In order to achieve what they want, they need to do, they can actually call a tool

1362
01:05:52,600 --> 01:05:57,880
and make use of a tool. And those tools can be of different types. They could be existing

1363
01:05:57,880 --> 01:06:03,480
pieces of software, special case software, like a calculator or maybe like Adobe Photoshop or

1364
01:06:03,480 --> 01:06:08,040
something like that. So big pieces of software that they can learn how to use using reinforcement

1365
01:06:08,040 --> 01:06:14,360
learning and learn how to use the interface and interact with. But they can also be other AI systems,

1366
01:06:14,360 --> 01:06:20,520
other learned systems. I'll give you an example. So if you challenge one of these chatbots to a

1367
01:06:20,520 --> 01:06:25,000
game, you want to play a game of chess or a game of Go against it, they're actually all pretty bad

1368
01:06:25,000 --> 01:06:30,520
at it currently, which is one of the tests I give these chatbots is, can they play a good game and

1369
01:06:30,520 --> 01:06:34,440
hold the board state in mind? And they can't really at the moment, they're not very good.

1370
01:06:35,160 --> 01:06:38,920
But actually, maybe there's something to say, well, these general systems

1371
01:06:38,920 --> 01:06:45,080
shouldn't learn how to play chess or Go or fold proteins, there should be specialized AI systems

1372
01:06:45,080 --> 01:06:50,600
that learn how to do those things, AlphaGo, AlphaZero, AlphaFold. And actually, the general

1373
01:06:50,600 --> 01:06:58,120
system can call those specialized AIs as tools. So I don't think it makes much sense for the language

1374
01:06:58,120 --> 01:07:03,560
model to know how to fold proteins. That would seem like an over-specialization in its data

1375
01:07:03,560 --> 01:07:07,640
corpus relative to language and all the other things that general things that it needs to learn.

1376
01:07:07,640 --> 01:07:12,920
I think it will be more efficient for it to call this other AI system and make use of

1377
01:07:12,920 --> 01:07:17,320
something like AlphaFold if it needed to fold proteins. But it's interesting because at some

1378
01:07:17,320 --> 01:07:23,160
point, more of those capabilities will be forwarded back into the general system over time. But I

1379
01:07:23,160 --> 01:07:27,400
think at least the next era, we'll see a general system making use of these specialized systems.

1380
01:07:28,280 --> 01:07:35,240
So when you think about the road from what we have now to these generally intelligent systems,

1381
01:07:36,440 --> 01:07:41,560
do you think it's simply more training data and more compute, right? Like more processors,

1382
01:07:41,560 --> 01:07:47,960
more stuff we feed into the training set? Or do you think that there are other innovations,

1383
01:07:47,960 --> 01:07:51,560
other technologies that we're going to need to figure out first? What's between here and there?

1384
01:07:52,280 --> 01:07:58,840
I'm in the camp that both needed. I think that large multimodal models of the site we have now

1385
01:07:58,840 --> 01:08:04,840
have a lot more improvement to go. So I think more data, more compute, and better

1386
01:08:04,840 --> 01:08:11,560
techniques will result in a lot of gains and more interesting performance. But I do think there are

1387
01:08:11,560 --> 01:08:16,920
probably one or two innovations, a handful of innovations missing from the current systems

1388
01:08:17,000 --> 01:08:20,920
that will deal with things that we talked about like factuality, robustness,

1389
01:08:20,920 --> 01:08:26,040
in the realm of planning and reasoning and memory that the current systems don't have.

1390
01:08:26,040 --> 01:08:30,360
And that's why they fall short still of a lot of interesting things we would like them to do.

1391
01:08:30,360 --> 01:08:35,640
So I think some new innovations are going to be needed there, as well as pushing the existing

1392
01:08:35,640 --> 01:08:42,520
techniques much further. So it's clear to me in terms of building an AGI system or general AI system,

1393
01:08:42,520 --> 01:08:47,960
these large multimodal models are going to be a core component. So they're definitely necessary,

1394
01:08:47,960 --> 01:08:52,840
but I'm not sure they'll be sufficient in of themselves. You've talked about in interviews

1395
01:08:52,840 --> 01:08:59,720
I've heard you give before that you don't want to see this pursuit develop into a move fast and

1396
01:08:59,720 --> 01:09:04,440
break things kind of race. At the same time, you're part of Google. They just aligned actually all

1397
01:09:04,440 --> 01:09:09,000
of Google AI under you. There used to be two groups, DeepMind and Google Brain. Now it's all

1398
01:09:09,080 --> 01:09:15,000
under your empire. Open AI is aligned with Microsoft. Meta is doing a lot more AI. They're

1399
01:09:15,000 --> 01:09:19,960
working under Yanlacun, who's like one of the founders in all this. China obviously has a number

1400
01:09:19,960 --> 01:09:23,720
of systems that seem to be getting better fairly quickly compared to even what we were seeing six

1401
01:09:23,720 --> 01:09:30,440
months ago. It does feel like a race dynamic has developed. And I'm curious how you think about that.

1402
01:09:31,480 --> 01:09:35,880
I don't think it's ideal. That's for sure. I think it's just the way that technology is panned

1403
01:09:35,880 --> 01:09:40,760
out. It's become more of an engineering kind of technology, or at least the phase we're in now

1404
01:09:40,760 --> 01:09:45,720
versus scientific research and innovation, which was perhaps done over the last decade.

1405
01:09:45,720 --> 01:09:50,040
And Google and DeepMind were responsible for a lot of those breakthroughs that we've discussed,

1406
01:09:50,040 --> 01:09:54,840
you know, reinforcement learning, obviously, transformers, which Google research invented

1407
01:09:54,840 --> 01:10:01,640
that underpin all of the modern systems, very critical breakthrough. And so give Google credit,

1408
01:10:01,640 --> 01:10:07,000
they just released publicly. Yes, exactly. So they released publicly, published it available,

1409
01:10:07,000 --> 01:10:13,160
and everyone uses that now, including Open AI. And so that underpins the kind of technologies

1410
01:10:13,160 --> 01:10:19,640
and systems that we see today. And I would prefer it if we took a scientific approach to this as

1411
01:10:19,640 --> 01:10:25,320
a field and as a community and an industry where we were optimistic about what's coming down the

1412
01:10:25,320 --> 01:10:29,960
line. Obviously, I worked on AI my whole career because I think it's going to be the most beneficial

1413
01:10:29,960 --> 01:10:36,520
technology for humanity ever, cure all diseases and help us with energy and also sustainability,

1414
01:10:36,520 --> 01:10:41,400
all sorts of things. I think that AI can be an incredibly useful tool for, but it has risks.

1415
01:10:41,400 --> 01:10:45,480
It's a dual use technology. And like any new transformative technology, and I think AI will

1416
01:10:45,480 --> 01:10:50,920
be one of the most transformative in human history, it can be used for bad too. And so we have to

1417
01:10:50,920 --> 01:10:55,880
think all of that through. And I would like us to have not move fast and break things like you say,

1418
01:10:55,880 --> 01:11:01,480
and actually be more thoughtful and try and have foresight rather than hindsight about these things.

1419
01:11:01,480 --> 01:11:05,160
We're not going to get everything right with a fast moving cutting edge technology like AI

1420
01:11:05,160 --> 01:11:10,520
first time, but we should try and minimize and think very carefully about the risks at each stage

1421
01:11:10,520 --> 01:11:16,280
and try and mitigate those as far as possible while making sure we're bold and brave with the

1422
01:11:16,280 --> 01:11:21,640
benefits. And so we have this mantra of being bold and responsible. And I think there's a creative

1423
01:11:21,720 --> 01:11:26,120
tension there, but it's sort of intentional between those two things. When you look back at the

1424
01:11:26,120 --> 01:11:31,480
technology, perhaps one of the last big technologies of the last decade or two has been social media,

1425
01:11:31,480 --> 01:11:36,760
I think that embodies this view of like move fast and break things. And I feel like that has,

1426
01:11:36,760 --> 01:11:41,880
of course, had huge benefits and huge growth for certain companies. And it's been very beneficial

1427
01:11:41,880 --> 01:11:46,920
in many ways, but it also had some unintended consequences that we only as a society started

1428
01:11:46,920 --> 01:11:52,760
realizing many, many years later once it had reached huge scale. I would like us to avoid that,

1429
01:11:52,760 --> 01:11:57,560
if possible, with AI to the extent that that's possible. There is also commercial realities

1430
01:11:57,560 --> 01:12:03,080
and geopolitical issues. And we are in this sort of race dynamic. And what I hope is that

1431
01:12:03,080 --> 01:12:08,040
there will be a sort of cooperation actually at the international scale on the safety and

1432
01:12:08,040 --> 01:12:13,640
technical risks as these systems become more and more powerful. I want to talk about one benefit

1433
01:12:13,720 --> 01:12:17,080
that you mentioned in passing there. And then I want to talk through some of the risks more

1434
01:12:17,080 --> 01:12:22,600
specifically. You mentioned help us work on energy. And we've talked a lot here about protein

1435
01:12:22,600 --> 01:12:27,320
folding. We've talked about the applicability to drug discovery. I think the idea that AI could

1436
01:12:27,320 --> 01:12:34,200
help us with clean energy is something people often hear said, but don't get a lot of details on.

1437
01:12:34,200 --> 01:12:39,000
But one of the systems you're building or projects you're working on is around nuclear fusion

1438
01:12:39,000 --> 01:12:43,400
and stabilizing nuclear fusion. So I don't want to spend a ton of time here, but just

1439
01:12:43,400 --> 01:12:48,520
put some meat on the bones of that idea. Can you just talk about what you're doing here and why AI

1440
01:12:48,520 --> 01:12:54,200
might be well suited to it? Yeah, I think AI can actually help with climate and sustainability

1441
01:12:54,200 --> 01:12:59,480
in a number of ways, at least three different ways I think of. One is optimizing our existing

1442
01:12:59,480 --> 01:13:03,720
infrastructure. So we get more out of the same infrastructure. We have a really good example

1443
01:13:03,720 --> 01:13:09,720
of that. Actually, we used a similar system to AlphaGo to control the cooling systems in a data

1444
01:13:09,720 --> 01:13:15,000
centers, in these massive data centers that run all of our compute. They use a huge amount of energy

1445
01:13:15,000 --> 01:13:19,720
and we actually managed to save 30% of the energy the cooling systems used by more efficiently

1446
01:13:19,720 --> 01:13:25,480
controlling all the parameters. Secondly, we can monitor the environment better automatically,

1447
01:13:25,480 --> 01:13:30,360
you know, deforestation and other things, forest fires, all of these types of things using AI.

1448
01:13:30,360 --> 01:13:34,760
So that's helpful for NGOs and governmental organizations to keep track of things. And

1449
01:13:34,760 --> 01:13:39,960
then finally, we can use AI to accelerate breakthrough, new breakthrough technologies. And

1450
01:13:39,960 --> 01:13:44,600
our fusion work is a good example of that, controlling the plasma incredibly hot, hotter than

1451
01:13:44,600 --> 01:13:49,400
the surface of the sun. So it can't touch the sides of the magnets and so on in these big machines

1452
01:13:49,400 --> 01:13:54,600
called Tokamaks that control this plasma, super hot plasma that is generating the electricity.

1453
01:13:54,600 --> 01:13:59,560
And we use AI, our sort of reinforcement learning systems to predict effectively what the shape of

1454
01:13:59,560 --> 01:14:04,520
the plasma is going to be. So in milliseconds, we can change the magnetic field by changing the

1455
01:14:04,520 --> 01:14:10,760
current going in the magnets to keep hold of the plasma in place so it doesn't go out of control.

1456
01:14:10,760 --> 01:14:16,040
So that's a huge problem in fusion and one of the big issues with getting fusion working. But

1457
01:14:16,040 --> 01:14:19,640
there are also other ways I could imagine I could help in things like material design,

1458
01:14:19,640 --> 01:14:24,920
designing better batteries, better solar panel technologies, superconductors and so on, which

1459
01:14:24,920 --> 01:14:29,880
I think AI will be able to help with down the line. So I want to hold that there. And then I want

1460
01:14:29,880 --> 01:14:37,160
to talk about a risk here, which is one of the things I see happening is ever since GPT-3 was

1461
01:14:37,160 --> 01:14:42,840
hooked up to chat GPT, and people could begin interfacing with it in natural language, there's

1462
01:14:42,840 --> 01:14:50,600
been a huge rush towards chat systems, towards chat bots. And this is, I know, an oversimplification,

1463
01:14:50,680 --> 01:14:57,080
but I do think there's an idea here about, are we making systems that are designed to do what

1464
01:14:57,080 --> 01:15:01,800
humans can do, but a little bit better? Are we making systems that what we have built here is

1465
01:15:01,800 --> 01:15:07,400
something meant to seem human to humans? Or are we making systems that are actually very inhuman,

1466
01:15:07,400 --> 01:15:11,720
that are doing what humans cannot do because they can think in a way humans cannot think,

1467
01:15:11,720 --> 01:15:15,800
or more to the point calculate in a way humans cannot calculate. And AlphaFold,

1468
01:15:15,800 --> 01:15:19,400
the nuclear fusion system you're talking about, those strike me as more in that area.

1469
01:15:20,040 --> 01:15:23,640
And I don't want to say there's necessarily a sharp choice between the two because we've talked

1470
01:15:23,640 --> 01:15:29,720
about the possibility of general intelligence systems too. But there is where investment goes.

1471
01:15:29,720 --> 01:15:35,080
There is where the corporate priorities are. There is where the best engineers are working.

1472
01:15:35,880 --> 01:15:41,960
And now you have these very big companies that are basically in a battle for search and enterprise

1473
01:15:41,960 --> 01:15:46,760
software funding, right? They want to get subscriptions to Microsoft Office 365 up,

1474
01:15:46,760 --> 01:15:51,400
and Google doesn't want Bing to take its market share. And one thing that I worry about a bit

1475
01:15:51,400 --> 01:15:56,120
is that I see a lot more possible benefit for humanity from these more scientific inhuman

1476
01:15:56,120 --> 01:16:02,200
systems, but that the hype and the investment and the energy is going towards these more human,

1477
01:16:02,200 --> 01:16:08,120
more kind of familiar systems that I worry are not going to be as beneficial. And so one risk

1478
01:16:08,120 --> 01:16:14,120
I see is simply that the business models are not going to be well hooked to public benefit.

1479
01:16:14,120 --> 01:16:18,120
And you said a minute ago, we sort of were leaving the scientific research period of this

1480
01:16:18,120 --> 01:16:22,200
and entering into the competitive period of this. I think of something that kind of kept

1481
01:16:22,200 --> 01:16:26,360
deep mind a little bit apart from it. Always you're in London and you guys always seemed a

1482
01:16:26,360 --> 01:16:30,280
little bit more like you're on the scientific path and now you have to be on top of all Google.

1483
01:16:30,280 --> 01:16:34,040
How do you think about this tension? Yeah, it's a very interesting question. I think about this

1484
01:16:34,040 --> 01:16:38,920
all the time. And you're right, there is that tension. And I think all of, let's say the venture

1485
01:16:38,920 --> 01:16:43,720
capitalist world and so on has almost sort of lost their minds over chatbots, right?

1486
01:16:43,720 --> 01:16:48,120
And all the money's going into there. I think even in our new guys as Google deep mind,

1487
01:16:48,120 --> 01:16:54,120
we're going to keep pushing really hard on both frontiers. Advancing science and medicine is

1488
01:16:54,120 --> 01:16:59,320
always going to be at the heart of what we do and our overall mission for the benefit of humanity.

1489
01:16:59,320 --> 01:17:05,880
But we are also going to push hard on next generation products with incredible new experiences for

1490
01:17:05,880 --> 01:17:10,280
billions of users that help them in their everyday lives. I'm kind of equally excited about potential

1491
01:17:10,360 --> 01:17:16,520
of both types of things. And so that involves us continuing to invest and work on scientific

1492
01:17:16,520 --> 01:17:21,400
problems like AlphaFold or isomorphic labs is doing drug discovery, fusion, except for quantum

1493
01:17:21,400 --> 01:17:28,280
chemistry, mathematics, many, many of our nature and science papers, as well as doubling down on

1494
01:17:28,280 --> 01:17:33,560
these new types of chatbot interfaces and so on. I don't see them as sort of human and inhuman. It's

1495
01:17:33,560 --> 01:17:39,160
more like the AlphaFold things are scientific tools for experts to use and enhance their work

1496
01:17:39,160 --> 01:17:43,560
so they can accelerate their very important research work. And then the other hand, at the

1497
01:17:43,560 --> 01:17:47,880
moment, I think chatbots are more of a fun entertainment thing. I mean, of course, you

1498
01:17:47,880 --> 01:17:52,120
can do your homework on them and you can do amusing things and it's quite helpful. But I think there's

1499
01:17:52,120 --> 01:17:56,680
so much more to come in that space. And I think where I see them joining together is what we

1500
01:17:56,680 --> 01:18:00,440
discussed earlier about these general systems, perhaps that you interact with in language.

1501
01:18:00,440 --> 01:18:05,240
There's nothing wrong with that because language is the mode that we all can use rather than coding

1502
01:18:05,240 --> 01:18:11,080
or mathematics. Language is the simplest thing for everybody to use to interface with these systems,

1503
01:18:11,080 --> 01:18:17,320
but they could call a bunch of specialized systems and specialized tools and make use of them. And

1504
01:18:17,320 --> 01:18:21,880
so I actually think there's quite an interesting combination to come by pushing the frontiers

1505
01:18:21,880 --> 01:18:25,560
of both of those things. And that's what we're planning to do going forward.

1506
01:18:26,280 --> 01:18:33,560
You recently signed a letter. It was alongside Sam Altman, who leads OpenAI and Dario Amade,

1507
01:18:33,560 --> 01:18:37,720
who is a top OpenAI person, now leads Anthropic. And letter simply says,

1508
01:18:38,360 --> 01:18:43,160
mitigating the risk of extinction from AI should be a global priority alongside other

1509
01:18:43,160 --> 01:18:49,320
societal scale risks, such as pandemics and nuclear war. Why do you believe there is any

1510
01:18:49,320 --> 01:18:56,200
risk of extinction from AI at all? Well, that letter was a kind of compromise thing. I think

1511
01:18:56,200 --> 01:19:01,320
it's not 30 words long. So of course, all the nuances are missing from a letter such as that.

1512
01:19:01,400 --> 01:19:05,960
And at some point soon, we'll put out a fuller statement about our position.

1513
01:19:05,960 --> 01:19:10,040
The only thing I was agreeing with there is that this technology has such potential

1514
01:19:10,040 --> 01:19:16,280
for enormous, enormous good, but it's a dual-use technology. So if bad actors get hold of it,

1515
01:19:16,280 --> 01:19:20,120
it could be used for bad things. There are near-term harms we have to be careful of,

1516
01:19:20,120 --> 01:19:24,520
like, deep fakes. And we need to address with things like watermarking, and we're working on

1517
01:19:24,520 --> 01:19:28,120
that. And I think a lot of that will come out later this year. And then there are technical

1518
01:19:28,120 --> 01:19:33,160
risks. This alignment problem that we discussed earlier on how to make sure these systems do

1519
01:19:33,160 --> 01:19:37,560
what we want. And we set them the right objectives and the right values, and they can be contained

1520
01:19:37,560 --> 01:19:42,840
and controlled as they get more powerful. So there's a whole series of at least three buckets of

1521
01:19:42,840 --> 01:19:48,360
worry. And I think they're all equally important, actually, but they require different solutions.

1522
01:19:48,360 --> 01:19:54,280
And one of them is this longer term people think of as maybe a science fiction scenario of inherent

1523
01:19:54,360 --> 01:19:58,760
technical risk from these systems, where if we don't build them in the right way,

1524
01:19:58,760 --> 01:20:02,680
in the limit when they're decades time, when they're very, very powerful,

1525
01:20:02,680 --> 01:20:06,200
and they're capable of planning and all the things I discussed earlier today, but to the

1526
01:20:06,200 --> 01:20:11,480
nth degree, we have to be careful with those sorts of systems. I don't think it's likely,

1527
01:20:11,480 --> 01:20:15,720
I wouldn't even put a probability on it, but there's uncertainty over it. And it's certainly

1528
01:20:15,720 --> 01:20:20,920
non-zero. I think the possibility that that could go wrong if we're not thoughtful and careful and

1529
01:20:20,920 --> 01:20:26,440
use exceptional care with these technologies. So I think that's the part I was trying to indicate

1530
01:20:26,440 --> 01:20:31,960
by signing that was that it's important to have that debate now. You don't want to have that debate

1531
01:20:31,960 --> 01:20:38,040
on the eve of some kind of technology like that arriving, right? Ten years sounds like a long time,

1532
01:20:38,040 --> 01:20:42,440
but it's not that long, given the amount of research that would be required and is required,

1533
01:20:42,440 --> 01:20:48,040
and I think needs to be done to understand the systems better so that we can mitigate any risks

1534
01:20:48,040 --> 01:20:53,400
that may come about. Well, I think right now when people hear about extinction risk from AI,

1535
01:20:53,400 --> 01:20:58,760
one scenario that they've now been told to think about and more people do is the AI itself getting

1536
01:20:58,760 --> 01:21:03,080
out of control or turning the whole world into paperclips or whatever it might be. But I want

1537
01:21:03,080 --> 01:21:08,520
to talk about another here, which is more along the lines of our conversation. So you build Alpha

1538
01:21:08,520 --> 01:21:13,000
Fold now through isomorphic, you're building a whole suite of tools to search through the

1539
01:21:13,000 --> 01:21:20,600
molecular space, the protein space to better understand how to predict the functions of and

1540
01:21:20,600 --> 01:21:30,280
then eventually create bespoke proteins, molecules, etc. And I think one slightly less sci-fi version

1541
01:21:30,280 --> 01:21:36,120
of extinction or at least mass harm that you can imagine here is through synthetic biology,

1542
01:21:36,120 --> 01:21:42,760
through as it becomes very cheap to figure out how to create an unbelievably lethal virus and print

1543
01:21:42,840 --> 01:21:48,600
an unbelievably lethal virus that in the future, it's actually not that hard for some terrorist

1544
01:21:48,600 --> 01:21:55,480
organization to use tools like this to make something far beyond if you think back in America,

1545
01:21:55,480 --> 01:22:00,360
you know, however many years now, when somebody was mailing anthrax around, if it had been very

1546
01:22:00,360 --> 01:22:06,200
easy for that person to create super smallpox, then you get into something really, really horrifying.

1547
01:22:06,760 --> 01:22:10,200
And how do you think about that suite of risks? Because you're doing more work in that space

1548
01:22:10,200 --> 01:22:13,880
really than anyone else, I think. And that's one of the ones that seems actually much near

1549
01:22:13,880 --> 01:22:19,000
at hand to me. Yeah, we think a lot about that. And we talk with a lot of experts in government

1550
01:22:19,000 --> 01:22:24,360
and academia about this. And actually, before we released Alpha Fold, we spent several months

1551
01:22:24,360 --> 01:22:31,320
talking to over 30 experts in biosecurity, biorethics, also Nobel Prize winning biologists

1552
01:22:31,320 --> 01:22:36,120
and chemists about what we were going to release with the database and what they thought of it.

1553
01:22:36,200 --> 01:22:40,120
And all of them unanimously actually came back with in that case, the benefits far outweighed

1554
01:22:40,120 --> 01:22:44,120
the risks. And I think we're seeing all the benefits of that today with millions of biologists

1555
01:22:44,120 --> 01:22:48,680
around the world using it. But look, going forward, as you get more into chemistry space,

1556
01:22:48,680 --> 01:22:53,400
one has to think about these things. But of course, we want to cure many terrible diseases too,

1557
01:22:53,400 --> 01:22:58,520
right? So we have to weigh up that enormous benefit there to society with these inherent

1558
01:22:58,520 --> 01:23:04,040
risks that you're talking about. And I think one of these issues is access to these technologies

1559
01:23:04,040 --> 01:23:08,760
by bad actors, not scientists and people, you know, medical practitioners who are trying to

1560
01:23:08,760 --> 01:23:13,080
do good with it. But as you say, terrorists, other things like that. And I think that's where

1561
01:23:13,080 --> 01:23:18,520
actually becomes a question of things like open sourcing, or do you publish these results? Or

1562
01:23:18,520 --> 01:23:23,320
do you sort of, how secure is your cybersecurity? So you can't be hacked? All of these questions

1563
01:23:23,320 --> 01:23:28,120
come into play. And I think that's where we're going to have to think a lot more carefully

1564
01:23:28,120 --> 01:23:33,160
in the next few years, as these systems become more sophisticated about who should get access

1565
01:23:33,160 --> 01:23:38,360
to those things? How should that be monitored? Can bad actors be shut down if they're using APIs

1566
01:23:38,360 --> 01:23:42,760
very quickly before they do any harm? Maybe we can use AI there actually to detect what are they

1567
01:23:42,760 --> 01:23:47,080
trying to design with these systems as well. This can also happen with chatbots too. What are they

1568
01:23:47,080 --> 01:23:52,440
asking the chatbots, right? So I think there's a role for AI to play there actually as well on the

1569
01:23:52,440 --> 01:23:57,800
monitoring side. And so the other question though to ask is, and when I've discussed this with experts

1570
01:23:57,800 --> 01:24:03,480
in biosecurity is, there are known toxins today, like you mentioned anthrax, you can probably find

1571
01:24:03,480 --> 01:24:08,520
the recipe for that somewhere on the internet or, and people could do that, but you still need a wet

1572
01:24:08,520 --> 01:24:14,760
lab, and you still need some scientific capability. And so those are areas which are also usually beyond

1573
01:24:14,760 --> 01:24:20,040
naive bad actor individual, their capabilities, right? It's not just the recipe, how do you actually

1574
01:24:20,040 --> 01:24:24,360
make it and then distribute it, right? It's actually pretty difficult. And I would argue that's

1575
01:24:24,360 --> 01:24:29,160
already available today if you want it. It's not that there's no bad toxins that are known.

1576
01:24:29,160 --> 01:24:33,480
There are some that are quite simple. It's just not that easy to make them to the uninitiated,

1577
01:24:33,480 --> 01:24:37,320
right? You need a lab and you need access to it and labs can be monitored and so on.

1578
01:24:37,320 --> 01:24:41,400
There are still a lot of barriers. It's not just a question of understanding the design,

1579
01:24:41,400 --> 01:24:45,560
but we do need to think about that as well and figure out how we control that information.

1580
01:24:46,200 --> 01:24:53,000
Right now, the systems and the kind of labs that could create the systems that could become

1581
01:24:53,000 --> 01:24:56,680
something like general intelligence. I mean, you could count them on two hands, right? Across

1582
01:24:57,480 --> 01:25:01,800
the United States, across Europe, across China. And over time, there'll be even more than that,

1583
01:25:01,800 --> 01:25:06,440
but that's I think where we are now. As we get closer, I mean, you were talking about how much

1584
01:25:06,440 --> 01:25:12,760
can happen here in 10 years. If we're getting to a point where somebody is getting near something

1585
01:25:12,760 --> 01:25:17,960
like a general intelligence system, is that too powerful technology to be in private hands? Should

1586
01:25:18,040 --> 01:25:23,560
this be something that whichever corporate entity gets their first controls or do we need something

1587
01:25:23,560 --> 01:25:29,880
else to govern it? My personal view is that this is such a big thing in this fullness of time.

1588
01:25:29,880 --> 01:25:35,560
I think it's sort of bigger than any one corporation or even one nation. I think it needs

1589
01:25:35,560 --> 01:25:40,520
sort of international cooperation. I've often talked in the past about a kind of CERN-like

1590
01:25:40,520 --> 01:25:46,680
effort for AGI. And I quite like to see something like that as we get closer, maybe in many years

1591
01:25:46,680 --> 01:25:52,920
from now to an AGI system where really careful research is done on the safety side of things,

1592
01:25:53,480 --> 01:25:58,440
understanding what these systems can do, and maybe testing them in controlled conditions

1593
01:25:58,440 --> 01:26:05,880
like simulations or games first, like sandboxes, very robust sandboxes with lots of cyber security

1594
01:26:05,880 --> 01:26:10,840
protection around them. I think that will be a good way forward as we get closer towards human

1595
01:26:10,840 --> 01:26:15,480
level AI systems. I think it's a good place to end. So always our final question then.

1596
01:26:15,480 --> 01:26:17,720
What are three books you would recommend to the audience?

1597
01:26:18,920 --> 01:26:24,040
Well, I've chosen three books that are quite meaningful to me. So I would say, first of all,

1598
01:26:24,040 --> 01:26:30,520
Fabric of Reality by David Deutsch. I think that poses all the big questions in physics that I would

1599
01:26:30,520 --> 01:26:38,600
love one day to tackle with our AI tools. The second book I would say is Permutation City by Greg Egan.

1600
01:26:38,600 --> 01:26:44,600
I think it's an amazing story, actually, wild story of how interesting and strange I think the

1601
01:26:44,600 --> 01:26:51,560
world can get in the context of AI and simulations and hyper-realistic simulations. And then finally,

1602
01:26:51,560 --> 01:26:56,840
I would recommend Consider Fleebers by Ian Banks, which is part of the culture series of novels,

1603
01:26:56,840 --> 01:27:01,320
very formative for me. And I read that while I was writing Theme Park. And I still think it's

1604
01:27:01,320 --> 01:27:07,160
the best depiction of a post-AGI future, an optimistic post-AGI future, where we're traveling

1605
01:27:07,160 --> 01:27:10,840
the stars and humanity sort of reached its full flourishing.

1606
01:27:10,920 --> 01:27:13,080
Ladies and gentlemen, thank you very much.

1607
01:27:13,080 --> 01:27:13,880
Thanks very much.

1608
01:27:24,840 --> 01:27:27,560
This episode of The Israel Clown Show was produced by Roger Karma,

1609
01:27:27,560 --> 01:27:31,880
fact-checking by Michelle Harris. Our senior engineer is a great Jeff Gelb. The show's

1610
01:27:31,880 --> 01:27:36,760
production team also includes Emma Flaugau, Annie Galvant, and Kristen Lin. Our music is by Isaac

1611
01:27:36,760 --> 01:27:41,240
Jones. Audience strategy this week by Christina St. Maluski and Shannon Busta.

1612
01:27:41,240 --> 01:27:44,840
The executive producer of New York Times' opinion audio is Annie Rose Strasser,

1613
01:27:44,840 --> 01:27:46,920
and special thanks to Sonia Herrero.

