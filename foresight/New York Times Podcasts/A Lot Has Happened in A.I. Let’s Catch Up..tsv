start	end	text
0	23200	From New York Times Opinion, this is the Ezra Klein Show.
23200	28080	Before we get into the episode today, we are getting ready to do our end of the year Ask
28080	29080	Me Anything.
29080	33080	If you have questions you want to hear me answer on the show, I suspect a lot of them
33080	36720	are going to be about Israel Palestine and AI, but they don't have to be about Israel
36720	38600	Palestine and AI.
38600	43000	Send them to Ezra Klein Show at nytimes.com with AMA in the headline.
43000	54120	Again, to Ezra Klein Show at nytimes.com with AMA in the headline.
54120	59960	If you follow business or tech or artificial intelligence news at all, in recent weeks
59960	66360	you certainly were following Sam Altman being unexpectedly fired as CEO of OpenAI and then
66360	71680	a huge staff revolt at OpenAI where more than 95% of the company said it would resign if
71680	76160	he was not reinstated and then he was reinstated.
76160	80080	And so this whole thing seemed to have happened for nothing.
80080	85120	I spent a lot of time reporting on this and I talked to people on the Altman side of things,
85120	88960	I talked to people on the board side of things, and the thing I am now convinced of, truly
88960	93440	convinced of is that there was less to it than met the eye.
93440	100080	People saw, I saw, Altman fired by this nonprofit board meant to ensure that AI is built to
100080	102080	serve humanity.
102080	108800	And I assumed, and I think many assumed, there was some disagreement here over what OpenAI
108800	115440	was doing, over how much safety was building into the systems, over the pace of commercialization,
115440	120880	over the contracts it was signing, over what it was going to be building next year, over
120880	121880	something.
121880	126120	And that I think I can say conclusively and has been corroborated by other reporting,
126120	128200	that was not what this was about.
128200	132560	The OpenAI board did not trust and did not feel it could control Sam Altman, and that
132560	134360	is why they fired Altman.
134360	137400	It's not that they felt they couldn't trust him on one thing, that they were trying to
137400	141200	control him on X, but he was beating them on X.
141200	143840	It's that a lot of little things added up.
143840	148040	They felt their job was to control the company, that they did not feel they could control
148040	151560	him, and so to do their job, they had to get rid of him.
151560	154720	They did not have, obviously, the support inside the company to do that.
154720	157560	They were not ultimately willing to let OpenAI completely collapse.
157560	161240	And so they largely, although I think in their view, not totally back down.
161240	163240	One of the members is still on the board.
163240	167880	Altman and the president of OpenAI, Greg Brockman, are off the board.
167880	171720	Some new board members are coming in who they think are going to be stronger and more willing
171720	172720	to stand up to them.
172720	176440	There's an investigation that is going to be done of Altman's behavior that will be
176440	181280	at least released to the board, so they'll, I guess, know what to think of him.
181280	182280	It's a very strange story.
182280	186680	I wouldn't be surprised if there's things yet to come out, but I am pretty convinced
186680	191600	that this was truly a struggle for control, not a struggle about X.
191600	196240	But it has been a year since ChatGPT was released, so weird way to mark the year, but it has
196240	197240	been a year.
197240	202880	A year since OpenAI kicked off the whole modern era in artificial intelligence.
202880	207520	A year since a lot of people's estimations of what humanity's future looked like began
207520	211320	to shift and cloud and darken and shimmer.
211320	215240	And so I wanted to have the conversation that many of us thought was a conversation happening
215240	220120	here about what AI was becoming, how it was being used, how it was being commercialized,
220440	224160	the path we're on is going to benefit humanity.
224160	228080	And so I asked my friends over at Hard Fork, another great New York Times podcast, to come
228080	229080	on the show.
229080	231000	Kevin Roos is my colleague at The Times.
231000	233240	He writes a tech column called The Shift.
233240	238120	Casey Newton is the editor of Platformer, an absolutely must read newsletter about the
238120	241800	intersection of technology and democracy.
241800	245440	And they have been following this in and out, but they've been closely following AI for
245440	246440	the past year.
246440	248840	So I wanted to have this broader conversation with them.
248840	251760	As always, my email is reclinedshowatnytimes.com.
258240	259960	Kevin Roos, Casey Newton.
259960	260960	Welcome to the show, my friends.
260960	261960	Hey, Ezra.
261960	263960	Thanks for having us.
263960	264960	All right.
264960	268320	So we're talking on Monday, November 27th.
268320	273840	JetGPT, which kicked off this era in AI, was released on November 30th, 2022.
273840	278600	So the big anniversary party was at Sam Altman got temporarily fired and the company almost
278600	282920	collapsed and was rebuilt over at Microsoft, which I don't think is how people expected
282920	284160	to mark the anniversary.
284160	291720	But it has been now a year, roughly, in this sort of whole new AI world that we're in.
291720	295080	And so I want to talk about what's changed in that year.
295080	299800	And the place I want to begin is with the capabilities of the AI systems we're seeing,
299800	304200	not the ones we're hearing about, but that we know are actually being used by someone
304200	307200	in semi-real world conditions.
307200	311800	What can AI systems do today that they couldn't do a year ago, Kevin?
311800	318160	Well, the first most obvious capabilities improvement is that these models have become
318160	319800	what's called multimodal.
319800	326680	So a year ago, we had ChatGPT, which could take in text input and output other texts
326680	328800	as the response to your prompt.
328800	335520	But now we have models that can take in text and output images, take in text and output
335520	340760	video, take in voice data, and output other voice data.
340760	345960	So these models are now working with many more types of inputs and outputs than they
345960	347760	were a year ago.
347760	353360	And that's sort of the most obvious difference if you just woke up from a year-long nap and
353360	356820	took a look at the AI capabilities on the market, that's the thing that you would probably
356820	357820	notice first.
357820	359320	I want to pull something out about that.
359320	364480	Is that it almost sounds like they're developing what you might call senses.
364480	367920	And I recognize that there's a real danger of anthropomorphizing AI systems.
367920	369480	I'm not trying to do that.
369480	373440	But one thing about having different senses is that we get some information that helps
373440	376360	us learn about the world from our eyes, other information that helps us learn about the
376360	378600	world from our ears, et cetera.
378600	382420	One of the constraints on the models is how much training data they can have.
382420	387400	As they become multimodal, it would seem that would radically expand the amount of training
387400	388400	data.
388400	395600	Not just all of the text on the internet, but all of the audio on YouTube, or all podcast
395600	398640	audio on Spotify or something, or Apple Podcasts.
398640	403120	That's a lot of data to learn about the world from, that in theory will make the models
403120	404440	smarter and more and more capable.
404440	406920	Does it have that kind of recursive quality?
406920	407920	Absolutely.
407920	412800	I mean, part of the backdrop for these capabilities improvements is this race for high-quality
412800	413800	data.
413800	420120	AI labs are obsessed with finding new undiscovered, high-quality data sources that they can use
420120	421200	to train their models.
421200	425640	And so if you run out of text because you've scraped the entire internet, then you've got
425640	431160	to go to podcasts or YouTube videos or some other source of data to keep improving your
431160	432160	models.
432160	435880	For what it's worth, though, I don't think the availability of more training data is
435880	437600	what is interesting about the past year.
437600	441560	I think what was interesting about ChatGBT was that it gave average people a way to interact
441560	443080	with AI for the first time.
443080	447800	It was just a box that you could type in and ask it anything and often get something pretty
447800	449520	good in response.
449520	453640	And even a year into folks using this now, I don't think we fully discovered everything
453640	455360	that it can be used for.
455360	459000	And I think more people are experiencing vertigo every day as they think about what this could
459000	460800	mean for their own jobs and careers.
460800	464200	So to me, the important thing was actually just the box that you type in and get questions
464200	465200	from.
465200	466200	Yeah, I agree with that.
466200	471520	I think if you had just paused there and there was no new development in AI, I think it would
471520	478280	still probably take the next five or 10 years for society to adjust to the new capabilities
478280	479960	in our midst.
479960	483960	So you've made this point in other places, Casey, that a lot of the advances to come
483960	489320	are going to be in user interfaces and in how we interact with these systems.
489320	491520	In a way, that was a big advance of ChatGBT.
491520	495840	The system behind it had been around for a while, but the ability to speak to it, I guess,
495840	501300	write to it in natural language, it created this huge cultural moment around AI.
501300	505180	But what can these AI products actually do that they couldn't do a year ago?
505180	510260	Not just how we interface with them, but their underlying capacity or power.
510260	516260	I mean, as of the developer update that OpenAI had a few weeks back, the world knowledge
516260	520040	of the system has been updated to April of this year.
520040	526060	And so you're able to get something closer to real-time knowledge of world events.
526060	532300	It has now integrated with Microsoft Bing, and so you can get truly real-time information
532300	536300	in a way that was impossible when ChatGBT launched.
536300	539940	And these might sound like relatively minor things, Azura, but you start chaining them
539940	545540	together, you start building the right interfaces, and you actually start to see beyond the internet
545540	546540	as we know it today.
546540	551340	You see a world-worthy web where Google is not our starting point for doing everything
551340	552340	online.
552340	555700	It is just a little box on your computer that you type in and you get the answer without
555700	556900	ever visiting a web page.
556900	561060	So that's all going to take many years to unfold, but the beginnings of it are easy
561060	562060	to see now.
562060	567180	One other capability that didn't exist a year ago, at least in any public products, is the
567180	569980	ability to bring your own data into these models.
569980	574420	So Claude was the first language model that I used that had the ability to, say, upload
574420	575420	a PDF.
575420	581000	So you could say, here's a research paper, it's 100 pages long, help me summarize and
581000	582000	analyze this.
582000	583000	And it could do that.
583020	587920	Now ChatGPT can do the same thing, and I know a bunch of other systems are moving in that
587920	588920	direction too.
588920	593080	There are also companies that have tried to spin up their own language models that are
593080	595120	trained on their own internal data.
595120	602480	So if you are Coca-Cola or BCG or some other business and you want an internal ChatGPT that
602480	607080	you can use for your own employees to ask, say, questions about your HR documents, that
607080	608960	is a thing that companies have been building.
608960	612920	So that's not the sexiest, most consumer-facing application, but that is something that there's
612920	614960	enormous demand for out there.
614960	618840	So one thing it seems to me to be getting better at from what I can tell from others
618840	619840	is coding.
619840	625520	I have to ask people whether they're using AI bots very often, and if so, for what.
625520	629280	And basically nobody says yes unless they are coder.
629280	632520	Everybody says, oh yeah, I played around with it, I thought it was really cool, I sometimes
632520	638600	use Dolly or Mid Journey to make pictures for my kids or for my email newsletter.
638600	642280	But it is the coders who say, I'm using it all the time, it has become completely essential
642280	643280	to me.
643280	647200	I'm curious to hear a bit about that capability increase.
647200	652160	I think where it has sort of become part of the daily habit of programmers is through
652160	658760	tools like GitHub Copilot, which is a basically ChatGPT for coders that finishes whatever line
658760	663360	of code you're working on or helps you debug some code that's broken.
663360	665640	And there have been some studies and tests.
665640	671040	I think there was one test that GitHub itself ran where they gave two groups of coders the
671040	672040	same task.
672040	676560	And one group was allowed to use GitHub Copilot and one group wasn't.
676560	681600	And the group with GitHub Copilot finished the task 55% faster than the group without
681600	682600	it.
682600	685080	Now that is like a radical productivity increase.
685080	689960	And if you tell a programmer, here's a tool that can make you 55% faster, they're going
689960	692400	to want to use that every day.
692400	699040	So when I see function chat bots in the wild, what I see is different versions of what people
699040	704120	used to somewhat derisively call like the fancy autocomplete, right?
704120	707360	Help you finish a line of code, help you finish this email.
707360	711640	You ask a question that you might ask a search engine, like why do I have spots all over
711640	712640	my elbow?
712640	715520	And it gives you an answer that hopefully is right, but maybe is not right.
715520	719240	I do think some of the search implications are interesting, but at the same time, it
719240	722360	is not the case that Bing has made great strides on Google.
722360	725480	People have not moved to asking the kind of Bing chat bot.
725480	730080	Next question is as opposed to asking Google, everybody feels like they need AI in their
730080	731080	thing now, right?
731080	734400	There's a, I don't think you can raise money in Silicon Valley at the moment if you don't
734400	739360	have a generative AI play built into your product or built into your business strategy.
739360	741680	But that was true for a minute for crypto too.
741680	745200	And I'm not one of the people who makes a crypto AI analogy.
745200	749600	I think crypto is largely vaporware and AI is largely real.
749600	752680	But Silicon Valley is faddish and people don't know how to use things.
752680	756200	And so everybody tries to put things in all at once.
756200	758040	What product has actually gotten way better?
758040	760120	I'll just use one example.
760120	763480	There's an app you might be familiar with called Notion.
763480	766440	It's productivity sort of collaborative software.
766440	767440	I write a newsletter.
767440	770640	I save every link that I put in my newsletter into Notion.
770640	774320	And now that there is AI inside Notion, Notion can do a couple of things.
774320	777480	One, it can just look at every link I save and just write a two sentence summary for
777480	780720	me, which is just sort of nice to see at a glance what that story is about.
780760	784760	And most recently, it added a feature where you can just do Q&A with a database and say
784760	790760	like, hey, what are some of the big stories about Meta over the past few weeks?
790760	795120	And it'll just start pulling those up, essentially querying the database that I have built.
795120	800280	And so while we're very early in this, you're beginning to see a world where AI is taking
800280	805360	data that you have stored somewhere and it's turning it into your personal research assistant.
805360	806800	So is it great right now?
806800	808960	No, I would give it like a C.
809200	811200	For one point now, I think it's not bad.
811200	815320	And I'll share another example that is not from my own use, but I was talking a few
815320	819920	weeks ago with a doctor, who's a friend of a friend, and doctors, you get tons of messages
819920	820920	from patients.
820920	822280	You know, what's this rash?
822280	824120	Can you renew this prescription?
824120	825760	Do I need to come in for a blood test?
825760	826760	Like that kind of stuff.
826760	832200	And doctors and nurses spend a ton of time just opening up their message portal, replying
832200	833200	to all these messages.
833200	836240	It's a huge part of being a doctor and it's a part that they don't like.
836240	840720	And so this doctor was telling me that they have this software now that essentially uses
840720	841720	a language model.
841720	846720	I assume it's open AIs or someone very similar to that, that goes in and pre fills the responses
846720	848760	to patient queries.
848760	853080	And the doctor still has to look it over, make sure everything's right and press send.
853080	858200	But just that act of pre-populating the field, this person was saying that saves them a ton
858200	861640	of time, like on the order of several hours a day.
861640	868400	But if you have that and you sort of extrapolate to what if every doctor in America was saving
868400	871800	themselves an hour or two a day of responding to patient messages?
871800	874760	I mean, that's a radical productivity enhancement.
874760	878120	And so you can say that that's just fancy autocomplete and I guess on some level it
878120	883240	is, but just having fancy autocomplete in these paperwork heavy professions could be
883240	884240	very important.
884240	891080	Well, let me push out in two directions because one direction is that I am not super thrilled
891320	896800	about the idea that my doctor theoretically here is glancing over things and clicking
896800	902280	submit as opposed to reading my message themselves and having to do the act of writing, which
902280	905440	helps you think about things and thinking about what I actually emailed them and like
905440	907200	what kind of answer they need to give me.
907200	912680	I mean, I know personally the difference in thought between scanning things and editing
912680	914640	and thinking through things.
914640	919680	So that's like my diminishing response, but the flip of it is the thing I'm not hearing
919720	924480	anybody say here and the thing I keep waiting for and being interested in is the things
924480	926560	that I might be able to do better than my doctor.
926560	930720	I was reading Jack Clark's import AI newsletter today, which I super recommend to people who
930720	935600	want to follow advancements in the field and he was talking about a, I mean, it was a system
935600	939160	being tested, not a system that is in deployment, but it was better at picking up pancreatic
939160	943480	cancer from certain kinds of information than doctors are.
943480	948200	And I keep waiting to hear something like this going out into the field, right?
948200	951120	Something that doesn't just save people a bit of time around the edges.
951120	952800	I agree that's a productivity improvement.
952800	955280	It's fine. You can build a business around that.
955280	959440	But the promise of AI when Sam Altman sat with you all a few weeks ago or however long
959440	962520	it was and said, we're moving to the best world ever.
962520	965200	He didn't mean that our paperwork is going to get a little bit easier to complete.
965200	967600	Like he meant we'd have cures for new diseases.
967600	971160	He meant that we would have new kinds of energy possibilities.
971160	976200	I'm interested in the programs and the models that can create things that don't exist.
977200	980160	Well, to get there, you need systems that can reason.
980160	984000	And right now the systems that we have just aren't very good at reasoning.
984000	988760	I think that over the past year, we have seen them move a little away from the way that
988760	992480	I was thinking of them a year ago, which was a sort of fancy autocomplete, right?
992480	995360	It's sort of making it, making a prediction about what the next word will be.
995360	1000120	This is true that they do it that way, but it is able to create a kind of facsimile of
1000120	1002040	thought that can be interesting in some ways.
1002040	1005840	But you just can't get to where you're going, Ezra, with like a facsimile of thought.
1005840	1008880	You need something that has improved reasoning capabilities.
1008880	1011880	So maybe that comes with the next generation frontier models.
1011880	1014360	But until then, I think you'll be disappointed.
1014360	1016000	But do you need a different kind of model?
1016000	1018200	This is something that lingers in the back of my head.
1018200	1022560	So I did an interview on the show with Demis Isabis, who's the co-founder of DeepMind.
1022560	1026040	Now we're going to see integrated DeepMind Google AI program.
1026040	1031000	And DeepMind had built this system while back called AlphaFold, which treated how
1031000	1034760	proteins are constructed in 3D space, which is to say, in reality,
1034760	1037720	we live in 3D space, it treated it as a game.
1037720	1041640	And it fed itself a bunch of information and it became very good at predicting the
1041640	1045320	structure of proteins and that solved this really big scientific problem.
1045320	1049240	And they then created a subsidiary of Alphabet called Isomorphic Labs to try
1049240	1053280	to build drug discovery on similar foundations.
1053280	1057680	But my understanding is that Google during this period became terrified of Microsoft
1057680	1060400	and open AI beating it up in search and office.
1060440	1064280	And so they pulled a lot of resources, not least Hasabis himself,
1064280	1067680	into this integrated structure to try to win the chatbot wars,
1067680	1070680	which is now what their system barred us trying to do.
1070680	1073840	And so when you said, Casey, that we need things that can reason, I mean, maybe,
1073840	1077560	but also you could say we need things that are tailored to solve problems we care
1077560	1078760	about more.
1078760	1080880	And I think this is one of the things that worries me a bit,
1080880	1088680	that we've backed ourselves into business models that are not that important for humanity.
1088760	1090320	Is there some chance of that?
1090320	1096160	I mean, are we going too hard after language-based general intelligent AI
1096160	1100720	that, by the way, integrates very nicely into a suite of enterprise software
1100720	1104080	as opposed to building things that actually create scientific breakthroughs,
1104080	1108880	but don't have the same kind of high-scalability profit structure behind them?
1110800	1114920	I would stick up for the people who are working on the sort of what you could call
1114920	1117760	like the non-language problems in AI right now.
1117760	1119480	This stuff is going on.
1119480	1123360	It maybe doesn't get as much attention from people like three of us as it should.
1123360	1128680	But if you talk to folks in fields like pharmaceuticals and biotech,
1128680	1132800	there are new AI biotech companies spinning up every day,
1132800	1138120	getting funding to go after drug discovery or some more narrow application.
1138120	1141600	Like we talked to a researcher the other day, formerly of Google,
1141600	1144200	who is teaching AI to smell, right?
1144200	1148560	Taking the same techniques that go into these transformer-based neural networks
1148560	1153640	like chat GPT and applying them to the molecular structures of different chemicals
1153640	1157000	and using that to be able to predict what these things will smell like.
1157000	1159240	And you might say, well, what's the big deal with that?
1159240	1163640	And the answer is that some diseases have smells associated with them
1163640	1169480	that we can't pick up on because our noses aren't as sensitive as, say, dogs or other animals.
1169480	1172880	But if you could train an AI to be able to recognize scent molecules
1172880	1176080	and predict odors from just chemical structures,
1176080	1178040	that could actually be useful in all kinds of ways.
1178040	1180240	So I think this kind of thing is happening.
1180240	1183880	It's just not sort of dominating the coverage the way that chat GPT is.
1203000	1211400	Let me ask you, Kevin, about, I think, an interesting, maybe promising,
1211400	1215960	maybe scary avenue for AI that you possibly personally foreclosed,
1215960	1224880	which is at some point during the year, Microsoft gave you access to a open AI-powered chatbot
1224880	1227560	that had this dual personality of Sydney.
1227560	1230240	And Sydney tried to convince you you didn't love your wife
1230240	1232000	and that you wanted to run away with Sydney.
1232000	1235760	And my understanding is immediately after that happened,
1235760	1238400	everybody with enough money to have a real business model in AI
1238400	1240800	lobotomized the personalities of their AI.
1240800	1242600	It's like, that was the end of Sydney.
1242600	1246640	But there are a lot of startups out there trying to do AI friends, AI therapists,
1246640	1251640	AI sex bots, AI, you know, boyfriends and girlfriends and non-binary partners.
1251640	1256320	Just every kind of AI companion you can imagine.
1256360	1261480	I've always thought this is a pretty obvious way this will affect society.
1261480	1265880	And the Sydney thing convinced me that the technology for it already exists.
1265880	1269680	So where is that?
1269680	1271880	And how are those companies doing?
1271880	1279360	Yeah, I mean, I'm sorry, A, if I did foreclose the possibility of AI personalities.
1279360	1283720	I think what's happening is it's just a little too controversial
1283720	1287560	and sort of fraught force any of the big companies to wait into.
1287560	1293240	Like Microsoft doesn't want its AI assistants and co-pilots to have strong personalities.
1293240	1294760	Like that much is clear.
1294760	1299160	And I don't think their enterprise customers want them to have strong personalities,
1299160	1302560	especially those personalities are adversarial or confrontational
1302560	1304360	or creepy or unpredictable in some way.
1304360	1308280	They want like, they want clippy, but like with real brain power.
1308280	1312640	But there are companies that are going after this more social AI market.
1312680	1315000	One of them is this company Character AI,
1315000	1320720	which was started by one of the original people at Google who made the transformer breakthrough.
1320720	1324000	And that company is growing pretty rapidly.
1324000	1327160	They've got a lot of users, especially young users.
1327160	1329840	And they are doing essentially AI personas.
1329840	1332160	You can make your own AI persona and chat with it
1332160	1335400	or you can pick from ones that others have created.
1335400	1337840	Meta is also going a little bit in this direction.
1337840	1341160	They have these sort of persona driven AI chatbots.
1341160	1345320	And all of these companies have put sort of guardrails around
1345320	1351160	like no one really wants to do the erotic, what they call erotic sort of role play
1351160	1356560	in part because they don't want to run afoul of things like the Apple app store terms of service.
1356560	1360200	But I expect that that will also be a big market for young people.
1360200	1364320	And anecdotally, I mean, I have just heard from a lot of young people
1364320	1369480	who already say like my friends have AI chatbot friends that they talk to all the time.
1369480	1373520	And it does seem to be making inroads into high schools.
1373520	1376600	And that's just an area that I'll be fascinated to track.
1376600	1378320	I mean, this is going to be huge.
1378320	1379320	A couple of thoughts coming to mind.
1379320	1382880	One, I talked to somebody who works at one of the leading AI companies
1382880	1386120	and they told me that 99% of people whose accounts they remove,
1386120	1389040	they remove for trying to get it to write tech space erotica.
1389040	1392280	OK, so that I think speaks to the market demand for this sort of thing.
1392280	1394920	I've also talked to people who have used the models of this
1394920	1397560	that are not constrained by any sort of safety guidelines.
1397560	1400040	And I've been told these things are actually incredible at writing erotica.
1400040	1402600	So what I'm telling you is there is a $10 billion.
1402600	1404360	You've got really a lot of reporting on this case.
1404360	1406600	You say maybe a personal interest.
1406600	1408880	Look, I write about content moderation.
1408880	1412000	And like porn is the content moderation frontier.
1412000	1414440	And it's just very interesting to me that it's so clear
1414440	1417360	that there are billions of dollars to be made here and no company will touch it.
1417360	1419680	And I asked one person involved, I said, why don't you just let people do this?
1419680	1422760	And they basically said, look, if you do this, you become a porn company overnight.
1422760	1424400	It like overwhelms the usage.
1424400	1426120	Like this is what people wind up using your thing for.
1426160	1428280	And you're you're working in a different company then.
1428280	1429120	So I sort of get it.
1429120	1432560	But, you know, even setting aside the explicitly erotic stuff, you know,
1432560	1436000	as you well know and have talked and written about just like the lowliness epidemic
1436000	1439600	that we have in this country, there's a lot of isolated people in this world.
1439720	1442680	And I think there is a very real possibility that a lot of those people
1442680	1447720	will find comfort and joy and delight with talking to these AI based companions.
1447880	1451320	I also think that when that happens, there will be a culture war over it.
1451440	1454920	And we will see lengthy segments on Fox News about how the Silicon Valley
1454920	1457600	technologists created a generation of shut-ins who wants to do nothing
1457600	1459280	but talk to their fake friends on their phones.
1459280	1461760	So I do think this is like the cultural war yet to come.
1461920	1465120	And the question is just sort of when do the enabling technologies get good enough?
1465120	1468640	And when do companies decide that they're willing to deal with the blowback?
1468760	1470840	I also think this is going to be a generational thing.
1470840	1474000	I mean, I'm very interested in this and have been for for a bit, in part
1474000	1478040	because I suspect if I had to make a prediction here, my five year old
1478040	1482560	is going to grow up with AI friends and my sort of pat line is that today
1482600	1485600	we worry that 12 year olds don't see their friends enough in person.
1486000	1489760	And tomorrow we'll worry that not enough of our 12 year old's friends or persons
1490320	1492160	because it's going to become normal.
1492280	1495080	And my sense is that the systems are really good.
1495080	1498400	If you unleashed them, you are already good enough to
1498400	1500600	functionally master this particular application.
1500600	1502720	And the big players simply haven't unleashed them.
1502720	1507280	I've heard from people at the big companies here who are like, oh, yeah,
1507280	1510360	if we wanted to do this, we could dominate it.
1510800	1513960	But that does bring me to a question, which is META kind of does want to do this.
1513960	1516760	META, which owns Facebook, which is a social media company,
1517280	1521760	they seem to want to do it in terms of these lame, seeming celebrity avatars.
1521760	1523360	Like you can talk to AI Snoop Dogg.
1523360	1524160	So bad.
1524160	1528840	But that is interesting to me because their AI division is run by Yanlacun,
1528920	1531920	who is one of the most important AI researchers in the field.
1532320	1536960	And they seem to have very different cultural dynamics in their AI shop
1537400	1540680	than Google DeepMind or OpenAI.
1541600	1545720	Tell me a bit about META's strategy here and what makes them culturally different.
1546680	1551200	Well, Casey, you cover META and have for a long time and may have some insight here.
1551200	1555520	My sense is that they are sort of up against a couple problems,
1555520	1562080	one of which is they have arrived to AI late and to generative AI specifically.
1562080	1566200	You know, Facebook was for many years considered one of the top two labs
1566200	1569640	along with Google when it came to recruiting AI talent,
1569640	1573560	to putting out cutting edge research, to presenting papers at the big AI conferences.
1573560	1575160	They were one of the big dogs.
1575160	1577600	And then they sort of had this funny thing happen
1577600	1583280	where they released a model called Galactica just right before ChatGPT was released last year.
1583640	1588480	And it was supposed to be this sort of like LLM for science and for research papers.
1588840	1591280	And it was out for, I think, three days.
1591560	1595000	And people started noticing that it was making up fake citations.
1595000	1595720	It was hallucinating.
1595720	1599320	It was doing what all the AI models do, but it was from META.
1599320	1600320	And so it felt different.
1600320	1605200	It had sort of this tarnish on it because people already worried about fake news on Facebook.
1605520	1610040	And so it got pulled down and then ChatGPT just shortly thereafter
1610200	1612320	launched and became this global sensation.
1612320	1617000	So they're sort of grappling for what to do with this technology that they've built now.
1617200	1623800	There's not a real obvious business case for shoving AI chatbots into products
1623800	1629200	like Facebook and Instagram, and they don't sell enterprise software like Microsoft does.
1629400	1632440	So they can't really shove it into paid subscription products.
1632440	1634640	So my sense from talking with folks over there
1634640	1638000	is that they're just kind of not sure what to do with this technology that they've built.
1638240	1641040	And so they're just flinging it open to the masses.
1641480	1642920	What do you think?
1642920	1643600	That tracks with me.
1643600	1645720	I sort of basically don't get it either.
1645720	1649400	Like basically what you've just said has been explained to me.
1649600	1655400	They are investing a ton with no obvious return on investment in the near term future.
1655680	1659400	I will say that these celebrity AI chatbots they've made are quite bad.
1659400	1660720	Like it's truly baffling.
1660720	1664560	And the thing is they've taken celebrities, but the celebrities are not playing themselves in the AI.
1664560	1670600	They've given all of the celebrities silly names and you can just sort of like follow their Instagram
1670600	1675360	and like send them messages and say like, hey, like character that Snoop Dogg is portraying.
1675400	1676520	Like, what do you think about it?
1676520	1677680	So it's all very silly.
1677680	1680560	And I expect it'll die a rapid death sometime in the next year.
1680560	1682120	And then we'll see if they have a better idea.
1682120	1685600	What I will say is like, if you're somebody who wakes up from AI nightmares
1685760	1689920	some mornings, as a lot of folks in San Francisco do, go listen to Jan LeCun talk about it.
1690000	1692560	No one has ever been more relaxed about AI than Jan LeCun.
1692560	1695960	You know, it's just sort of like an army of superhuman assistants
1695960	1697280	are about to live inside your computer.
1697280	1701000	They're going to do anything you want to do and there's no risk of them harming you ever.
1701160	1703520	So if you're, you know, you're feeling anxious, go listen to Jan.
1704040	1705560	Do you think he's right?
1705560	1707440	Because it also has led to policy difference.
1707440	1712960	Meta has been much more open source in their approach, which open AI
1712960	1717040	and Google seem to think is irresponsible.
1717080	1720440	But there's something happening there that I think is also built around a different view of safety.
1720440	1721560	Like, what is their view of safety?
1721560	1724600	Why does Jan LeCun, who is like an important figure in this whole world,
1724880	1728520	why is he so much more chill than, you know, name your other founder?
1729480	1732360	I mean, part of it is I just think these these are deeply held convictions
1732360	1735320	from someone who is an expert on this space and who has been a pioneer
1735320	1738080	and who understands the technology certainly far better than I do.
1738240	1741000	And he can just sort of not see from here to kill a robot.
1741000	1745760	So I I respect his viewpoint in that respect, given his credentials in the space.
1746080	1750080	I think on the question of is open source AI safer?
1750320	1753280	This is still an open question, not to pun.
1753520	1756240	The argument for it being safer is well, if it's open source,
1756240	1760520	that means that average people can go in and look at the code and identify flaws
1760720	1763720	and kind of see how the machine works and they can point those out in public
1763720	1765160	and then they can be fixed in public.
1765160	1768920	Whereas if you have something like open AI, which is building very powerful systems
1768920	1771960	behind closed doors, we don't have the same kind of access.
1771960	1774520	And so you might not need to rely on a government regulator
1774520	1776640	to see how safe their systems were.
1776640	1778560	So that is the argument in favor of open source.
1778560	1782680	Of course, the flip side of that is like, well, if you take a very powerful open source model
1782880	1786520	and you put it out on the open web, even if it's true that anyone can poke holes
1786520	1789920	and identify flaws, it's also true that a bad actor could take that model
1789920	1791920	and then use it to do something really, really bad.
1791920	1795240	So that hasn't happened yet, but it certainly seems like it's
1795240	1797520	an obvious possibility at some time in the near future.
1798120	1800600	Let me use that as a bridge to safety more generally.
1800600	1804040	So we've talked a bit about where these systems have gone over the past year,
1804040	1805440	where they seem to be going.
1805440	1810280	But there's been a lot of concern that they are unsafe and fundamentally
1810280	1814160	that they become misaligned or that we don't understand them or what they're doing.
1815000	1817600	What kind of breakthroughs have there been with all this investment
1817600	1819840	and all this attention on safety, Kevin?
1820960	1826760	So a lot of work has gone into what is called fine tuning of these models.
1826960	1831720	So basically, if you're making a large language model like GPT-4,
1831920	1833880	you have several phases of that.
1833880	1838160	Phase one is what's called pre-training, which is sort of just the basic process.
1838160	1840800	You take all of this data, you shove it into this neural network,
1840800	1844080	and it learns to make predictions about the next word in a sequence.
1844600	1846720	Then from there, you do what's called fine tuning.
1847000	1851000	And that is basically where you are trying to turn the model into something
1851000	1854160	that's actually useful or tailored, turned it into a chatbot,
1854160	1858960	turned it into a tool for doctors, turned it into something for social AIs.
1858960	1862960	That's the process that includes things like reinforcement learning from human feedback,
1862960	1866080	which is how a lot of the leading models are fine tuned.
1866400	1868840	And that work has continued to progress.
1869160	1874400	The models they say today are sort of safer and less likely to generate harmful outputs
1874680	1876720	than previous generations of models.
1877000	1879480	There's also this field of interpretability,
1879480	1882720	which is where I've been doing a lot of reporting over the past few months,
1883080	1886040	which is this sort of tiny subfield of AI
1886040	1890840	that is trying to figure out what the guts of a language model look like
1890840	1894040	and what is actually happening inside one of these models
1894320	1898080	when you ask it a question or give it some prompt and it produces an output.
1898320	1902880	And this is a huge deal, not only because I think people want to know how these things work,
1902880	1906080	they're not satisfied by just saying these are like mystical black boxes,
1906520	1909480	but also because if you understand what's going on inside a model,
1909480	1913280	then you can understand if, for example, the model starts lying to you
1913280	1917720	or starts becoming deceptive, which is a thing that AI safety researchers worry about.
1917720	1921360	So that process of interpretability research, I think, is really important.
1921360	1925720	There have been a few sort of minor breakthroughs in that field over the past year,
1925880	1929720	but it is still slow going and it's still a very hard problem to crack.
1930360	1933360	And I think it's worth just pausing to underscore what Kevin said,
1933360	1937240	which is the people building these systems do not know how they work.
1937240	1940720	They know at a high level, but there is a lot within that,
1940720	1943360	where if you show them an individual output from the AI,
1943360	1946760	they will not be able to tell you exactly why it said what it said.
1946800	1950120	Also, if you run the same query multiple times, you'll get slightly different answers.
1950120	1952840	Why is that? Again, the researchers can't tell you.
1952960	1956240	So as we have these endless debates over AI safety,
1956360	1959960	one reason why I do tend to lean on the side of the folks who are scared
1959960	1962120	is this exact point at the end of the day.
1962120	1964040	We still don't know how the systems work.
1964040	1965840	Tell me if this tracks for you.
1965840	1969960	I think compared to a year ago when I talked to the AI safety people,
1969960	1974120	the people who worry about AIs that become misaligned
1974120	1979920	and do terrible civilizational level damage, AIs that could be really badly misused.
1980520	1983960	They seem to think it has been actually a pretty good year, most of them.
1984080	1986760	They think they've been able to keep big models like GPT-4,
1986760	1990920	which of course are much less powerful than what they one day expect to invent.
1990920	1993000	But they think they've been pretty good at keeping them aligned.
1993560	1997600	They have made some progress on interpretability, which wasn't totally clear.
1997600	2001640	You know, a year ago, many people said that was potentially not a problem we could solve.
2002240	2004120	You know, at least we're making some breakthroughs there.
2004800	2006600	They're not relaxed.
2006600	2010560	The people who worry about this and they, you know,
2010560	2013560	will often say like we would need a long time to fully understand
2013560	2016280	even the things we have now and we may not have that long.
2016760	2022640	But nevertheless, I get the sense that the safety people seem a little more confident
2022640	2026000	that the work, the technical work they've been doing is paying off.
2026280	2029920	Then, you know, at least with the impression I got from the reporting prior.
2030600	2031200	I think that's right.
2031200	2035800	I mean, Sam Altman in particular, this has been his strategy is like,
2035800	2039440	we are we are going to release this stuff that is in our our labs.
2039760	2043240	And we're going to kind of wait and see how society reacts to it.
2043240	2045640	And then we'll sort of give it some time to let society address.
2045880	2047760	And then we will release the next thing.
2047760	2052200	That's what he thinks is the best way to slowly integrate AI into our lives.
2052480	2057240	And if you'd asked me maybe 11 months ago, like a month into using chat GPT,
2057600	2060400	what are the odds of something really, really bad happening
2060400	2062600	because of the availability of chat GPT?
2062920	2065680	I would have put them much higher than they turned out to be.
2065680	2067560	Right. And when you talk to folks at Open AI,
2067560	2071720	like they will tell you that that company really has taken AI safety
2071720	2075120	really seriously, you can see this yourself when you use the product.
2075120	2076360	Ask it a question about sex.
2076360	2077880	It basically calls the police.
2077880	2081360	So there is a lot to be said for how these systems have been built so far.
2082000	2086280	And I would say the other thing that I've heard from AI safety researchers
2086280	2090520	is that they are feeling relief, not just that the world has not ended,
2090760	2094120	but that more people are now worried about AI.
2094520	2097800	It was a very lonely thing for many years
2098000	2101840	to be someone who worried about AI safety because
2102440	2105400	there was no apparent reason to be worried about AI safety, right?
2105400	2109600	That the chatbots that were outward, it was like Siri and Alexa and they were terrible.
2109800	2113520	And no one could imagine that these things could become dangerous or harmful
2113520	2116240	because the technology itself was just not that advanced.
2116600	2118720	Now you have congressional hearings.
2118840	2122120	You have regulations coming from multiple countries.
2122320	2126000	You have people like Jeff Hinton and Yashua Bengios,
2126000	2128760	two of the sort of so-called godfathers of deep learning,
2129040	2131680	proclaiming that they are worried about where this technology is headed.
2131680	2135520	So I think for the people who have been working on this stuff for a long time,
2135760	2137800	there is some just palpable relief at like,
2137800	2140520	oh, I don't have to carry this all on my shoulders anymore.
2140520	2143920	The world is now aware of these systems and what risks they could pose.
2144400	2149360	One irony of it is that my read from talking to people is that AI safety
2149360	2153040	is going better as a technical matter than was expected.
2153720	2159320	And I think worse as a matter of governance and inter-corporate
2159840	2163640	competition and regulatory arbitrage than they had hoped.
2164120	2167880	There's a fear, as I understand it, that we could make the technical breakthroughs
2167880	2173680	needed, but that the kind of coordination necessary to go slow enough to make them.
2174360	2176000	Like that's where a lot of the fear is.
2176000	2178560	I think they feel like that's actually going worse, not better.
2179280	2184560	So one of the big narratives coming out of Sam Altman's firing was that it must
2184560	2186640	have had something to do with AI safety.
2186960	2191280	And, you know, based on my reporting and reporting shared by many others,
2191520	2195760	this was not an AI safety issue, but it is very much the story
2195760	2199240	that is being discussed about the whole affair.
2199520	2204320	And the folks who are on the board who are associated with these AI safety ideas,
2204560	2207200	they've taken a huge hit to their public reputation because of the way
2207200	2208760	they handle the firing and sort of all of that.
2209120	2213880	And so I think a really bad outcome of this firing is that the AI safety
2213880	2218800	community loses its credibility, even though AI safety, as far as we can tell,
2218920	2221320	really didn't have a lot to do with what happened to Sam Altman.
2221800	2225560	Yeah, I first agree that clearly, AI safety was not behind
2225560	2227320	whatever disagreements Altman and the board had.
2227320	2231640	I heard that from both sides of this and I didn't believe it and I didn't believe
2231640	2233000	it and I finally was convinced of it.
2233000	2234960	I was like, you guys had to have had some disagreement here.
2234960	2236000	It seems so fundamental.
2236520	2239240	But this is sort of what I mean, the governance is going worse.
2240120	2244040	All the open AI people thought it was very important and Sam Altman himself
2244040	2247440	talked about its importance all the time, that they had this non-profit board
2247440	2250840	connected to this non-financial mission, right?
2250840	2254320	The values of building AI that served humanity that could fire Sam
2254320	2257560	Altman at any time or even shut down the company fundamentally
2257960	2260880	if they thought it was going awry in some way or another.
2261480	2265440	And the moment that board tried to do that, now I think they did not try
2265480	2266760	to do that on very strong grounds.
2266760	2269800	But the moment they tried to do that, it turned out they couldn't.
2270280	2275000	That the company could fundamentally reconstitute itself at Microsoft
2275440	2279080	or that the board itself couldn't withstand the pressure coming back.
2279560	2284120	I think the argument from the board's side, the now mostly defunct board,
2284880	2289040	is that this didn't go as badly for them as the press is reporting,
2289040	2293680	that they brought in some other board members who are not cronies of Sam
2293680	2295000	Altman and Greg Brockman.
2295600	2298480	Sam Altman and Greg Brockman are not on the board now.
2298480	2300200	There's going to be investigation into Altman.
2300600	2304720	So maybe they have a stronger board that is better able to stand up to Altman.
2304760	2306600	That is one argument I have heard.
2307000	2311480	On the other hand, those stronger board members do not hold the views on AI
2311480	2316160	safety that the board members who left like Helen Toner of Georgetown
2316160	2318760	and Tasha Macaulay from Rand held.
2319280	2322000	I mean, these are people who are going to be very interested in whether or not
2322000	2323160	open is making money.
2323440	2326640	I'm not saying they don't care about other things too, but these are people
2326640	2327600	who know how to run companies.
2327600	2332040	They serve on corporate boards in a normal way where like the output of the
2332040	2334800	corporate board is supposed to be shareholder value and that's going to
2334800	2337800	influence them even if they understand themselves to have a different mission here.
2338240	2340080	Am I getting that story wrong to you?
2340360	2341480	No, I think that's right.
2341480	2346200	And it speaks to one of the most interesting and sort of strangest things
2346200	2351560	about this whole industry is that the people who started these companies
2352040	2353400	were weird.
2353400	2356920	And I say that with no sort of like normative judgment, but they made
2356920	2358400	very weird decisions.
2358600	2361120	Like they thought AI was exciting and amazing.
2361120	2365400	They wanted to build AGI, but they were also terrified of it to the point
2365400	2367960	that they developed these elaborate safeguards.
2368120	2372720	I mean, not in open AI's case, they put this nonprofit board in charge
2372720	2376640	of the for-profit subsidiary and gave essentially the nonprofit board the
2376640	2380320	power to push a button and shut down the whole thing if they wanted to.
2380600	2384240	At Anthropic, one of these other AI companies, they are structured as a
2384240	2387800	public benefit corporation and they have kind of this their own version
2387800	2391840	of a nonprofit board that is capable of essentially pushing the big red
2391840	2394800	shut it all down button if things get too crazy.
2395200	2399360	This is not how Silicon Valley typically structures itself.
2399360	2403320	Like Mark Zuckerberg was not in his Harvard dorm room building Facebook
2403520	2406920	thinking like if this thing becomes the most powerful communication
2406920	2411120	platform in the history of technology, like I will need to put in place these
2411120	2414160	checks and balances to keep myself from becoming too powerful.
2414600	2417200	But that was the kind of thing that the people who started open AI
2417200	2418560	in Anthropic were thinking about.
2418560	2423080	And so I think what we're seeing is that that kind of structure is sort
2423080	2427920	of bowing to the requirements of shareholder capitalism, which says
2427920	2431280	that, you know, if you do need all this money to run these companies to train
2431280	2435080	these models, you are going to have to make some concessions to the sort
2435080	2438080	of powers of the shareholder and of the money.
2438080	2441320	And so I think that one of the big pieces of fallout from this open
2441320	2445480	AI drama is just that open AI is going to be structured and run much more
2445480	2449600	like a traditional tech company than this kind of holdover from this nonprofit board.
2450600	2451920	And that is just a sad story.
2452080	2455400	I truly wish that it had not worked out that way.
2455760	2458360	I think one of the reasons why these companies were built in this way was
2458360	2460320	because it just helped them attract better talent.
2460680	2464760	I think that so many people working in AI are idealistic and civic
2464760	2467040	minded and do not want to create harmful things.
2467040	2470200	And they're also really optimistic about the power that good technology has.
2470520	2473440	And so when those people say that as powerful and good as these things could
2473440	2476160	be, it could also be really dangerous, I take them really seriously.
2476400	2477600	And I want them to be empowered.
2477600	2479240	I want them to be on company boards.
2479560	2483880	And those folks have just lost so much ground over the past couple of weeks.
2483880	2487520	And it is a truly tragic development, I think, in the development of this industry.
2494760	2515640	One thing you could just say with that is, yeah, it was always going to be up to governments
2515640	2520640	here, not up to strange nonprofit corporate, semi-corporate structures.
2521520	2526640	And so we actually have seen a huge amount of government activity in recent weeks.
2527080	2528840	And so I want to start here in the US.
2529120	2534080	Biden announced a big package of a big executive order.
2534080	2535120	You could call them regulations.
2535120	2536800	I sort of call them pre-regulations.
2537120	2541720	But Casey, how would you describe in some what they did?
2541720	2546520	Like, what is a Biden administration's approach that it is signaling to regulating AI?
2547320	2555160	The big headline was, if you are going to train a new model, so a sort of successor to a GPT-4,
2555480	2560520	and it uses a certain amount of energy, and the energy there is just sort of a proxy for how
2560520	2564840	powerful and capable this model might be, you have to tell the federal government that you have
2564840	2570840	done this, and you have to inform them what safety testing you did on this model before
2570920	2572600	releasing it to the public.
2572920	2578200	So that is the one kind of break that they attempted to put on the development of this
2578200	2581240	industry. It does not say you can't train these models.
2581240	2584120	It doesn't specify what safety tests you have to do.
2584120	2588920	It just says, if you're going to go down this road, you have to be in touch with us.
2588920	2593640	And that will, I think, slightly decelerate the development of these models.
2593640	2599880	I think critics would say it also pushes us a little bit away from a more open source version
2599880	2604760	of AI, that open source development is sort of chaotic by its nature.
2604760	2608840	And if you want to do some sort of giant open source project that would compete
2608840	2612600	with the GPTs of the world, that would just sort of be harder to do.
2612600	2614680	But to me, those are sort of the big takeaways.
2615320	2620360	One of the things that struck me looking at the order was, go back a year, go back two years.
2621080	2625800	I think the thing that people have said is that the government doesn't understand this at all.
2626600	2629720	It can barely be conversant in technology.
2629720	2633240	People remember Senator Orrin Hatch asking Mark Zuckerberg, well,
2633240	2635480	if you're not making people pay, then how do you make money?
2636680	2642520	When I read the order and looked at it, this actually struck me as pretty seriously engaged.
2643160	2647720	Like, for instance, there's a big debate in the AI world about whether or not you're going to
2647720	2653240	regulate based on the complexity and power of the model or the use of the model.
2653960	2658040	You have a fear about what happens if you're using the model for medical decisions.
2658040	2660200	But if you're just using it as your personal assistant, who cares?
2660760	2664360	Whereas the AI safety people have the view that, no, the personal assistant model might
2664360	2667720	actually be the really dangerous one because that's one that knows how to act in the real world.
2668360	2672040	The Biden administration takes a view of the AI safety people.
2672040	2675480	If you have a model over a certain level of computing complexity,
2675480	2679080	they want this higher level of scrutiny, higher level of disclosure on it.
2679080	2682520	They want everything that comes from an AI to be watermarked in some way,
2682520	2685080	so you can see that it is AI generated.
2685960	2689720	This struck me as a Biden administration actually clearly having taken this seriously
2689720	2694040	and having convened some set of group of stakeholders and experts that knew what they
2694040	2697800	were doing. I mean, I don't necessarily agree with literally every decision and a lot of it
2697800	2701560	is just asking for reports. But when you think about it as a framework for regulation,
2702280	2706360	it didn't read to me as a framework coming from people who had not thought about this for 10 minutes.
2707320	2711400	Absolutely. I was quite impressed. You know, I had a chance to meet with Ben Buchanan at the
2711400	2715480	White House who worked on this, talked to him about this stuff, and it is clear that they have been
2715480	2719400	reading everything. They've been talking to as many people as they can, and they did arrive
2719400	2725400	in a really nuanced place. And I think when you look at the reaction from the AI developers in
2725400	2729640	general, it was mostly like neutral to lightly positive, right? There was not a lot of blowback,
2729640	2734200	but at the same time, folks in civic society, I think, were also excited that the government did
2734200	2739480	have a point of view here and had done its own work. Yeah, it struck me as a very deft set of...
2740440	2744760	I think I would agree that they're more like pre-regulations than regulations. And to me,
2744760	2748600	it sounded like what the Biden White House was trying to do was throw a few bones to everyone.
2748600	2753000	What was like, we're going to throw a few bones to the AI safety community who worries about
2753000	2757560	foundation models becoming too powerful. We're going to throw some bones to the AI
2757560	2762280	harms community that is worried about things like bias and inaccuracy. And we're going to throw
2762280	2769080	some bones to the people who worry about foreign use of AI. So I saw it as a very sort of deliberate
2769080	2773720	attempt to give every sort of camp in this debate a little to feel happy about.
2775240	2783560	One of the things it raised for me as a question, though, was, did it point to a world where you
2783560	2787720	think that regulators are going to be empowered to actually act? This was the thing I was thinking
2787720	2793800	about after the board collapse. You imagine a world sometime in the future where you have open AI,
2794280	2801160	with GPT-6 or META or whomever, right? And they are releasing something that the regulator is
2801720	2807160	looking at the safety data, looking at what's there. They're just itchy about it. It's not
2807160	2811720	obviously going to do a ton of harm, but they're not convinced it's safe. They've seen some things
2811720	2820120	that worry them. Are they really going to have the power to say, no, we don't think your safety
2820120	2824440	testing was good enough. When this is a powerful company, when they won't be able to release a
2824440	2828360	lot of the proprietary data, right? The thing where the board could not really explain why
2828360	2834440	they were firing Sam Altman struck me as almost going to be the situation of virtually every regulator
2834440	2838200	trying to think about the future harms of a model. If you're regulating in time to stop a thing from
2838200	2841880	doing harm, it's going to be a judgment call. And if it's a judgment call, it's going to be a very
2841880	2846760	hard one to make. And so if we ever got to the point where somebody needed to flip the switch
2846760	2853800	and say, no, does anybody actually have the credibility to do it? Or is what we've seen that
2854360	2860200	in fact, like these very lauded successful companies run by smart people who have huge
2860200	2864600	Twitter followings or threads followings, whatever they end up being on, that they actually have
2864600	2868920	so much public power that they'll always be able to make the case for themselves. And like the
2868920	2872920	political economy of this is actually that we better just hope the AI companies get it right
2873560	2877480	because nobody's really going to have the capability stand in front of them.
2878280	2882680	When you talk to folks who are really worried about AI safety, they think that there is a high
2882680	2887560	possibility that at some point in let's say the next five years, AI triggers some sort of event
2887560	2892040	that kills multiple thousands of people. What that event could be, we could speculate, but
2892040	2896120	assume that that is true. I think that changes the political debate a lot, right? Like that's just
2896120	2900760	all of a sudden you start to see jets get scrambled. Hopefully that never happens, but I think that
2901240	2904280	the inciting moment. And this is the thing that just frustrates me as somebody who writes about
2904280	2907800	tech policy is we just live in a country that doesn't pass laws. There are endless hearings,
2907800	2911320	endless debates, and then it gets time to regulate something. And it's like, well, yeah,
2911320	2915800	they can regulate AI, but it's going to be based on this one regulation that was passed to deal with
2915800	2920440	like the oat farming crisis of 1906. And we're just going to hope that it applies. It's like,
2920440	2923320	we should pass new laws in this country. I don't know that there's a law that needs to be passed
2923320	2928600	today to ensure that all of this goes well, but certainly Congress is going to need to do something
2928600	2933160	at some point as the stuff evolves. I mean, one thing I was thinking about as this whole situation
2933160	2939080	at OpenAI was playing out was actually the financial crisis in 2008 and the scenes that were
2939080	2944760	captured in books and movies where you have the heads of all the investment banks and they're
2944760	2951000	scrambling to avoid going under and they're meeting in these boardrooms with people like
2951000	2956600	Ben Bernanke, the chair of the Federal Reserve, and the government actually had a critical role
2956600	2961800	there in patching together the financial system because they were sort of interested,
2961800	2967400	not in which banks survived and which failed, but in making sure that there was a banking system
2967400	2972520	when the markets opened the next Monday. And so I think we just need a new regulatory framework
2972520	2978360	that does have some kind of the sort of cliched word would be stakeholder, but someone who is
2978360	2982920	in there as a representative of the government who's saying, what is the resolution to this
2983000	2986680	conflict that makes sense for most Americans or most people around the world?
2987640	2992840	When you looked at who the government gave power to in this document, when you think about who
2992840	2998040	might play a role like that, when you need to call the government on AI, the way I read it is it
2998040	3003480	spread power out across a lot of different agencies. And there were places where I invested
3003480	3008760	more rather than less, but one thing that different people have called for that I didn't see it do,
3008760	3012760	in part because you would actually need to pass a law to do this, was actually create
3012760	3019480	the AI department, something that is funded and structured and built to do this exact thing,
3019480	3023240	to be the central clearinghouse inside the government, to be led by somebody who would
3023240	3029560	be the most credible on these issues. And would maybe have then the size and strength to do this
3029560	3034200	kind of research, right? The thing that is in my head here, because I find your analogy really
3034200	3040200	compelling, Kevin, is a federal reserve. The federal reserve is a big institution. And it has
3040760	3044600	significant power of its own in terms of setting interest rates. It also does a huge amount of
3044600	3048280	research. Like when you think about where would a public option for AI come from,
3048280	3051640	you would need something like that that has the money to be doing its own research and
3051640	3056600	hiring the really excellent people, in that case, economists, in this case, AI researchers.
3057160	3061400	And there was nothing like that here. It was sort of an assertion that we more or less have
3061400	3065640	the structure we need. We more or less have the laws we need. We can apply all those things
3065640	3071320	creatively. But it did not say like, this is such a big deal that we need a new institution
3071960	3078680	to be our point person on it. Yeah, I mean, I think that's correct. I think there are some
3078680	3086440	reasons for that. But I think you do want a government that has its own technological capacity
3086440	3092440	when it comes to AI, previous waves of innovation, certainly nuclear power during the Manhattan
3092440	3097480	project, but also things like the internet came out of DARPA. These are areas where the government
3097480	3103800	did have significant technical expertise and was building its own technology in sort of competition
3103800	3109160	with the private sector. There is no public sector equivalent of chat GPT. The government has not
3109160	3114680	built anything even remotely close to that. And I think it's worth asking why that is and what would
3114680	3120040	need to happen for the government to have its own capacity, not just to evaluate and regulate
3120040	3126040	these systems, but to actually build some of their own. I think it is genuinely strange on some level
3126840	3134040	that given how important this is, there is not a bill gathering steam. Look, the private sector
3134040	3142120	thinks it is worth pumping 50 or $100 billion into these companies so they can help you make
3142120	3149000	better enterprise software. It seems weird to imagine that there are not public problems that
3149000	3156200	have an economic value that is equal to that or significantly larger. And we may just not want
3156200	3161800	to pay that money fine. But we do that for infrastructure. We just passed a gigantic
3161800	3167960	infrastructure bill. And if we thought of AI like infrastructure, we actually also spend a lot of
3167960	3172440	money on broadband now. It seems to me you want to think about it that way. And I think it is a
3172440	3179480	kind of fecklessness and cowardice on the part of like the political culture that it no longer
3179480	3183640	thinks itself capable of doing things like that. Like at the very least, and I've said this I think
3183640	3188680	on your show probably, I think they should have prize systems where they say a bunch of things
3188680	3192280	they want to see solved. And if you can build an AI system that will solve them, they'll give you a
3192280	3198440	billion dollars. But the one thing is the government does not like to do things that spend money for
3198440	3204120	an uncertain return. And building a giant AI system is spending a lot of money for an uncertain return.
3204920	3208200	And so the only part of the government that is probably doing something like it is a defense
3208200	3212200	department in areas that we don't know. And that does not make me feel better. That makes me feel
3212200	3217400	worse. That's my take on that. Yeah, I mean, I think there's also a piece of this that has to do
3217400	3224440	with labor and talent. You know, there are probably on the order of several thousand people in the
3224440	3233080	world who can oversee the building, training, fine tuning deployment of large language models.
3233080	3239560	It is a very specific skill set. And the people who have it can make gobs of money in the private
3239560	3246040	sector working wherever they want to the numbers that you hear coming out of places like open AI
3246040	3252360	for what engineers are being paid there. I mean, it's like NFL level football compensation packages
3252360	3258120	for some of their people. And the government simply can't or won't pay that much money to
3258120	3262040	someone to do equivalent work for the public sector. Now, I'm not saying they should be paying
3262040	3265960	engineers millions of dollars of taxpayer money to build these things, but that's that's what you
3265960	3271640	would need to do if you wanted to compete in an open market for the top AI talent. I am saying
3271640	3276120	they should. I am saying this is like the factlessness and cowardice point. This is stupid.
3276120	3280040	You think there should be AI engineers working for the federal government making five million
3280040	3284680	dollars a year? Like maybe not five million dollars a year. But it would this is a this thing
3284680	3289880	that we don't think civil servants should make as much as people in the private sector.
3290600	3294200	Because I don't know somebody at a congressional hearing is going to stand and be like that person's
3294200	3301880	making a lot of money. That is a way we rob the public of value. If Google's not wrong,
3301880	3308440	Microsoft is not wrong that you can create things that are of social value through AI.
3308440	3312840	And if you believe that, then leaving it to them, I mean, they intend to make a profit.
3314040	3318360	Why shouldn't the public get great gains from this? It won't necessarily be through profit.
3318920	3323000	But, you know, if we could cure different diseases or, you know, make big advances on energy,
3323000	3327880	I just this way of thinking is actually to me the really significant problem. I'm not sure you
3327880	3331320	would need to pay people as much as you're saying because I actually do think a lot of,
3331320	3335400	I mean, we both know the culture of the AI people and at least up until a year or so ago.
3336280	3340920	It was weird. And a lot of them would do weird things and are not living very lush lives.
3341480	3345720	You know, they're in group houses with each other, taking psychedelics and working on AI
3345720	3351480	on the weekdays. But I think you can get people in to do important work and you should.
3351480	3355080	Now, look, you don't have the votes to do stuff like this. I think that's the real answer.
3355800	3361880	But in other countries, they will and do. Like when Saudi Arabia decides that it needs an AI
3362680	3367400	to be geostrategically competitive, it will take the money it makes from selling oil to the world.
3368040	3371720	And in the same way that it's currently using that money to hire sports stars,
3371720	3376520	it will hire AI engineers for a bazillion dollars and it will get some of them. And then it will
3376520	3382360	have a decent AI system one day. I don't know why we're waiting on other people to do that. We're
3382360	3387000	rich. It's stupid. I agree with you, Ezra. And I'm sorry that Kevin is so resistant to your ideas
3387000	3390360	because I think paying public servants well would do a lot of good for this country.
3390920	3394840	Look, I think public service should be paid well. I'm just saying when Jim Jordan
3394840	3401960	gets up and grills the former deep mind engineer about why the Labor Department is paying them
3401960	3407000	2.6 million dollars to fine tune language models, I'm not sure what the answer is going to be.
3407640	3411080	No, I agree with you. I think we're all saying in a way the same thing. It's like,
3411080	3415320	this is a problem. Government by dumb things Jim Jordan says is not going to be a great government
3415320	3421240	that takes advantage of opportunities from the public. Good. And that sucks. It would be better
3421240	3427640	if we were doing this differently and if we thought about it differently. Let me ask about China
3428520	3432920	because China is where on the one hand, at least on paper, the regulations look much tougher.
3433720	3437960	So one version is maybe the regulating AI much more strictly than we are. Another view that
3437960	3442920	I've heard is that in fact, that's true for companies, but the Chinese government is making
3442920	3447000	sure that it's building very, very strong. Do you know to the extent you all have looked at it,
3447000	3451960	how do you understand the Chinese regulatory approach and how it differs from our own?
3452920	3458520	I mean, I've looked at it mostly from the standpoint of what are the consumer facing systems
3458520	3465000	look like. It has only been I think a couple of months since China approved the first consumer
3465000	3472600	usable chat GPT equivalent. As you might imagine, they have very strict requirements as far as like
3472600	3477720	what the chatbot can say about Tiananmen Square. So they wind up being more limited maybe than
3477720	3484200	what you can use in the United States. As far as what is happening behind closed doors and for
3484200	3489960	their defense systems and that sort of thing, I'm in the dark. So four or five years ago when I
3489960	3495320	started reporting a book about AI, the conventional wisdom among AI researchers was that China was
3495320	3501160	ahead and they were going to make all of the big breakthroughs and beat the U.S. technology
3501160	3506360	companies when it came to AI. So it's been very surprising to me that in the past year since chat
3506360	3512760	GPT has come out, we have not seen anything even sort of remotely close to that level of performance
3512760	3517880	coming out of a Chinese company. Now, I do think they are working on this stuff, but it's been
3517880	3523560	surprising to me that China has been mostly absent from the frontier AI conversation over the past
3523560	3530200	year. And do you think those things are related? Do you think that the Chinese government's
3530920	3537880	risk aversion and the underperformance of at least the products and systems we've seen
3538440	3542680	in China? I mean, there might be things we don't know about. Do you think those things are connected?
3543240	3548680	Absolutely. I think you do need a risk appetite to be able to build and govern these systems because
3548760	3554760	they are unpredictable. We don't know exactly how they work. And what we saw, for example,
3554760	3562280	with Microsoft was that they put out this Bing Sydney chatbot and it got a lot of attention and
3562280	3567880	blowback and people reported all these crazy experiences. And in China, if something like that
3567880	3572280	had happened, they might have shut the company down or they might have been deemed such an
3572280	3576600	embarrassment that they would have radically scaled back the model. And instead, what Microsoft
3576600	3580920	did was just say, we're going to make some changes to try to prevent that kind of thing from
3580920	3584520	happening, but we're keeping this model out there. We're going to let the public use it and
3584520	3588440	they'll probably discover other crazy things and that's just part of the learning process.
3588440	3593400	That's something that I've been convinced of over the past year, talking with AI executives and
3593400	3599400	people at these companies, is that you really do need some contact with the public before you start
3599400	3604040	learning everything that these models are capable of and all the ways that they might misbehave.
3605000	3609960	What is the European Union trying to do? They've had draft regulations that were
3609960	3614360	seemed very expansive. What has been the difference in how they're trying to regulate this versus
3614360	3618280	how we are and what in your understanding is the status of their effort?
3619160	3625560	Europe was quite ahead with developing its AI Act, but it was written in a pre-chat GPT world.
3625560	3631080	It was written in a pre-generative AI world. And so over the past year, they've been trying to
3631960	3640040	retrofit it so that it reflects our new reality and is caught up in debate in the meantime.
3640040	3645240	But my understanding is the AI Act is not particularly restrictive on what these companies
3645240	3648360	can do. So to my understanding, there's nothing in the AI Act that is going to
3648360	3653160	prevent these next generation technologies from being built. It's more about companies being
3653160	3658840	transparent. Let me add a little bit of flavor to that because I was in Europe just recently
3658840	3664280	talking with some lawmakers. And one of the things that people will say about the AI Act
3664280	3671080	is that it has this risk-based framework where different AI products are evaluated and regulated
3671080	3676680	based on these classifications of this is a low-risk system or this is a high-risk system
3676680	3682040	or this is a medium-risk system. And so different rules apply based on which of those buckets
3682040	3687240	a new tool falls into. And so right now, what a lot of regulators and politicians and companies
3687320	3691720	and lobbyists in Europe are arguing about is what level of risk should something like a
3691720	3699160	foundation model, a GPT-4, a Bard, a Claude, are those low-risk systems because they're just chat
3699160	3705160	bots or are they high-risk systems because you can build so many other things once you have
3705160	3711960	that basic technology? And so that's what my understanding is of the current battle in Europe
3711960	3716440	is over whether foundation models, frontier models, whatever you want to call them,
3716440	3721000	whether those should be assigned to one risk bucket or another.
3721640	3726440	I think that's a good survey of the waterfront. And so I guess I'll end on this question, which is
3726440	3729400	all right, we're talking here at the one-year anniversary roughly of chat GPT.
3730280	3734200	If you were to guess, if we were having another conversation a year from now on the
3734200	3739160	two-year anniversary, what do you think would have changed? What are one or two things each
3739160	3743160	of you think is likely to happen over the next year that did not happen this year?
3743960	3751560	I think all communication-based work will start to have an element of AI in it. All email, all
3751560	3756680	presentations, office work essentially. AI will be built into all the applications
3756680	3760120	that we use for that stuff. And so it'll just be sort of part of the background,
3760120	3763400	just like autocomplete is today when you're typing something on your phone.
3764760	3771080	I would say that AI is going to continue to hollow out the media industry. I think you're
3771080	3777080	going to see more publishers turning to these really bad services that just automate the generation
3777080	3782920	of copy. You'll see more sort of content farms springing up on the web. It'll reduce publisher
3782920	3788440	revenue and we'll just see more digital media businesses either get sold or sort of quietly
3788440	3793400	go out of business. And that's going to go hand in hand with the decline of the web in general.
3793400	3797560	A year from now, more and more people are going to be using chat GPT and other tools
3797560	3802520	as their kind of front door to internet knowledge. And that's just going to sap a lot of life out
3802520	3807720	of the web as we know it. So we don't need one more technological breakthrough for any of that
3807720	3813080	to happen. That's just a case of consumer preferences taking a while to change. And I
3813080	3816920	think it's well underway. So do you think then that next year we're going to see something that
3816920	3821880	has been long predicted, which is significant AI related job losses? Is that sort of the argument
3821880	3826680	you're making here? I think that to some degree, it already happened this year in digital media.
3826680	3832280	And yes, I do think it will start to pick up. Just keep in mind, 12 months is not a lot of
3832280	3838440	time for every single industry to ask itself, could I get away with five or 10 or 15% fewer
3838440	3842520	employees? And as the end of this year comes around, I have to believe that in lots and lots
3842520	3846440	of industries, people are going to be asking that question. Yeah, I agree. I don't know whether
3846440	3851080	that there will be sort of one year where all the jobs that are going to vanish, vanish. I think
3851080	3857720	it's more likely to be a slow trickle over time. And it's less likely to be mass layoffs than just
3857720	3863320	new entrants that can do the same work as incumbents with many fewer people. The software
3863320	3869400	development firm that only needs five coders because they have all their coders are using AI
3869400	3874920	and they have software that is sort of building itself competing with companies that have 10,000
3874920	3880120	engineers and doing so much more capably. So I don't think it's going to necessarily look like
3880120	3885080	all the layoffs hit on one day or in one quarter or even in one year, but I do think we're already
3885080	3889880	seeing a displacement of jobs through AI. Those are kind of dark predictions. I mean,
3889880	3894040	we'll have a little bit better sort of integration of AI into office tools and also we'll begin to
3894040	3900200	see really the productivity improvements, create job losses. Is there anything that you think is
3900200	3905480	coming down the pike technologically that would be really deeply to the good things that are
3905480	3909400	not too far from fruition that you think will make life a lot better for people?
3910280	3917160	I mean, I love the idea of universal translators. It's already pretty good using AI to speak in
3917160	3921480	one language and get output in another, but I do think that's going to enable a lot of cross-cultural
3921480	3925240	communications and there are a lot of products remaining to be built that will essentially
3925240	3931240	just drop the latency so that you can talk and hear in real time and have it be quite good.
3931240	3932360	So that's something that makes me happy.
3933000	3937240	And I'm hopeful that we will use AI, not we as in me and Casey.
3937240	3937800	But we might.
3939000	3944680	This would be sort of a career change for us, but we as in society, I have some hope that we will
3944680	3953000	use AI to cure one of the sort of top deadliest diseases, cancer, heart disease, Alzheimer's,
3953000	3957640	things like that that really affect massive numbers of people. I don't have any inside
3957720	3962200	reporting that we are on the cusp of a breakthrough, but I know that a lot of energy and research and
3962200	3967800	funding is going into using AI to discover new drugs and therapies for some of the leading
3967800	3972600	killer diseases and conditions in the world. And so when I want to feel more optimistic,
3972600	3976840	I just think about the possibility that all of that bears fruit sometime in the next few years,
3976840	3977800	and that's pretty exciting.
3978520	3981800	All right. And then also final question, what are a few books you'd each recommend to the
3981800	3985720	audience released recommend the audience ask chat GPT to summarize for them?
3986280	3988760	Kevin, you want to go first?
3988760	3994120	Sure. I actually have two books in a YouTube video. The two books, one of them is called
3994120	4001160	electrifying America by David E. Nye. It is a 30 year old history book about the process by which
4001160	4006600	America got electricity. And it has been very interesting to read. I read it first a few years
4006600	4011320	ago and have been rereading it just to sort of sketch out what would it look like if AI really
4011320	4016280	is the new electricity? What happened the last time society was transformed by technology like
4016280	4021640	this? The other book I'll recommend is your face belongs to us by my colleague, our colleague at
4021640	4027080	the Times, Cashmere Hill, which is about the facial recognition AI company, Clearview AI,
4027080	4032920	and is one of the most compelling tech books I've read in a few years. And then the YouTube
4032920	4038120	video I'll recommend was just posted a few days ago. It's called Intro to Large Language Models.
4038200	4045640	It's made by Andre Karpathy, who is an AI researcher actually at Open AI. And it's his one hour
4045640	4049720	introduction to what is a large language model and how does it work? And I've just found it
4049720	4055320	very helpful for my own understanding. Casey? Well, as we're with permission, and given that
4055320	4059320	Kevin has just given your listeners two great books and a YouTube video to read, I would actually
4059320	4063400	like to recommend three newsletters if I could. And the reason is because the books that were
4063400	4067960	published this year did not help me really understand the future of the AI industry. And to
4067960	4071720	understand what's happening in real time, I really am leaning on newsletters more than I'm
4071720	4075880	leaning on books. So is that okay? Yeah, go for it. All right. So the first one,
4075880	4080120	cruelly, you already mentioned earlier in this podcast, it's import AI from Jack Clark. Jack
4080120	4085480	co-founded Amthropic, one of the big AI developers. And it is fascinating to know which papers he's
4085480	4089080	reading every week that are helping him understand this world. And I think that they're arguably
4089080	4093400	having an effect on how Amthropic is being created because he is sitting in all of those rooms. So
4093400	4098360	that is just an incredible weekly read. I would also recommend AI Snake Oil from the Princeton
4098360	4104200	Professor, Arvind Narayanan and a PhD student at Princeton, Syash Kapoor. They're very skeptical
4104200	4108280	of AI hype and doomsday scenarios, but they also take the technology really seriously and have
4108280	4112040	a lot of smart thoughts about policy and regulation. And then the final one is Pragmatic
4112040	4117320	Engineer by this guy, Gurgely Arose. He's this former Uber engineering manager. And he writes
4117320	4121560	about a lot of companies, but he writes about them as workplaces. And I love when he writes about
4121560	4125800	open AI as a workplace. He interviews people there about culture and management and process. And he
4125800	4129240	just constantly reminds you, they're just human beings showing up to the office every day and
4129240	4133080	building this stuff. And it's just a really unique viewpoint on that world. So read those
4133080	4136280	three newsletters. You'll have a little better sense of what's coming for us in the future.
4136280	4140600	What Casey didn't say is that he actually hasn't read a book in 10 years. So it was a bit of a
4140600	4144680	trick question. You know what I will say? I did read Your Face Belongs to Us by Cashion,
4144680	4148440	incredible book. Definitely read that one. Sure you did. There you go. Casey Newton,
4148440	4152200	Kevin Ruse at your podcast, which requires very little reading. It's hard for it.
4152920	4156120	Thank you all for being on the show. It's the first illiterate podcast, actually,
4156120	4164360	put out by the New York Times. Thank you for having us. Thanks, Ezra. Thanks, guys.
4164360	4179800	This episode of the Ezra Klein Show is produced by Roland Hoof, fact-checking by Michelle Harris
4179800	4184200	with Kate St. Clair and Mary Marge Locker. Our senior engineer is Jeff Geld. Our senior editor
4184200	4188520	is Claire Gordon. The show's production team also includes Emma Vagabou and Kristen Lin,
4188520	4193480	original music by Isaac Jones, audience strategy by Christina Samaluski, and Shannon Busta.
4193480	4197480	The executive producer of New York Times' opinion audio is Anero Strasser. And special
4197480	4207480	thanks to Sonia Herrero.
