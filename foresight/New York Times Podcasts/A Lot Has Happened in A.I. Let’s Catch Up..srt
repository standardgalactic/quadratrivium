1
00:00:00,000 --> 00:00:23,200
From New York Times Opinion, this is the Ezra Klein Show.

2
00:00:23,200 --> 00:00:28,080
Before we get into the episode today, we are getting ready to do our end of the year Ask

3
00:00:28,080 --> 00:00:29,080
Me Anything.

4
00:00:29,080 --> 00:00:33,080
If you have questions you want to hear me answer on the show, I suspect a lot of them

5
00:00:33,080 --> 00:00:36,720
are going to be about Israel Palestine and AI, but they don't have to be about Israel

6
00:00:36,720 --> 00:00:38,600
Palestine and AI.

7
00:00:38,600 --> 00:00:43,000
Send them to Ezra Klein Show at nytimes.com with AMA in the headline.

8
00:00:43,000 --> 00:00:54,120
Again, to Ezra Klein Show at nytimes.com with AMA in the headline.

9
00:00:54,120 --> 00:00:59,960
If you follow business or tech or artificial intelligence news at all, in recent weeks

10
00:00:59,960 --> 00:01:06,360
you certainly were following Sam Altman being unexpectedly fired as CEO of OpenAI and then

11
00:01:06,360 --> 00:01:11,680
a huge staff revolt at OpenAI where more than 95% of the company said it would resign if

12
00:01:11,680 --> 00:01:16,160
he was not reinstated and then he was reinstated.

13
00:01:16,160 --> 00:01:20,080
And so this whole thing seemed to have happened for nothing.

14
00:01:20,080 --> 00:01:25,120
I spent a lot of time reporting on this and I talked to people on the Altman side of things,

15
00:01:25,120 --> 00:01:28,960
I talked to people on the board side of things, and the thing I am now convinced of, truly

16
00:01:28,960 --> 00:01:33,440
convinced of is that there was less to it than met the eye.

17
00:01:33,440 --> 00:01:40,080
People saw, I saw, Altman fired by this nonprofit board meant to ensure that AI is built to

18
00:01:40,080 --> 00:01:42,080
serve humanity.

19
00:01:42,080 --> 00:01:48,800
And I assumed, and I think many assumed, there was some disagreement here over what OpenAI

20
00:01:48,800 --> 00:01:55,440
was doing, over how much safety was building into the systems, over the pace of commercialization,

21
00:01:55,440 --> 00:02:00,880
over the contracts it was signing, over what it was going to be building next year, over

22
00:02:00,880 --> 00:02:01,880
something.

23
00:02:01,880 --> 00:02:06,120
And that I think I can say conclusively and has been corroborated by other reporting,

24
00:02:06,120 --> 00:02:08,200
that was not what this was about.

25
00:02:08,200 --> 00:02:12,560
The OpenAI board did not trust and did not feel it could control Sam Altman, and that

26
00:02:12,560 --> 00:02:14,360
is why they fired Altman.

27
00:02:14,360 --> 00:02:17,400
It's not that they felt they couldn't trust him on one thing, that they were trying to

28
00:02:17,400 --> 00:02:21,200
control him on X, but he was beating them on X.

29
00:02:21,200 --> 00:02:23,840
It's that a lot of little things added up.

30
00:02:23,840 --> 00:02:28,040
They felt their job was to control the company, that they did not feel they could control

31
00:02:28,040 --> 00:02:31,560
him, and so to do their job, they had to get rid of him.

32
00:02:31,560 --> 00:02:34,720
They did not have, obviously, the support inside the company to do that.

33
00:02:34,720 --> 00:02:37,560
They were not ultimately willing to let OpenAI completely collapse.

34
00:02:37,560 --> 00:02:41,240
And so they largely, although I think in their view, not totally back down.

35
00:02:41,240 --> 00:02:43,240
One of the members is still on the board.

36
00:02:43,240 --> 00:02:47,880
Altman and the president of OpenAI, Greg Brockman, are off the board.

37
00:02:47,880 --> 00:02:51,720
Some new board members are coming in who they think are going to be stronger and more willing

38
00:02:51,720 --> 00:02:52,720
to stand up to them.

39
00:02:52,720 --> 00:02:56,440
There's an investigation that is going to be done of Altman's behavior that will be

40
00:02:56,440 --> 00:03:01,280
at least released to the board, so they'll, I guess, know what to think of him.

41
00:03:01,280 --> 00:03:02,280
It's a very strange story.

42
00:03:02,280 --> 00:03:06,680
I wouldn't be surprised if there's things yet to come out, but I am pretty convinced

43
00:03:06,680 --> 00:03:11,600
that this was truly a struggle for control, not a struggle about X.

44
00:03:11,600 --> 00:03:16,240
But it has been a year since ChatGPT was released, so weird way to mark the year, but it has

45
00:03:16,240 --> 00:03:17,240
been a year.

46
00:03:17,240 --> 00:03:22,880
A year since OpenAI kicked off the whole modern era in artificial intelligence.

47
00:03:22,880 --> 00:03:27,520
A year since a lot of people's estimations of what humanity's future looked like began

48
00:03:27,520 --> 00:03:31,320
to shift and cloud and darken and shimmer.

49
00:03:31,320 --> 00:03:35,240
And so I wanted to have the conversation that many of us thought was a conversation happening

50
00:03:35,240 --> 00:03:40,120
here about what AI was becoming, how it was being used, how it was being commercialized,

51
00:03:40,440 --> 00:03:44,160
the path we're on is going to benefit humanity.

52
00:03:44,160 --> 00:03:48,080
And so I asked my friends over at Hard Fork, another great New York Times podcast, to come

53
00:03:48,080 --> 00:03:49,080
on the show.

54
00:03:49,080 --> 00:03:51,000
Kevin Roos is my colleague at The Times.

55
00:03:51,000 --> 00:03:53,240
He writes a tech column called The Shift.

56
00:03:53,240 --> 00:03:58,120
Casey Newton is the editor of Platformer, an absolutely must read newsletter about the

57
00:03:58,120 --> 00:04:01,800
intersection of technology and democracy.

58
00:04:01,800 --> 00:04:05,440
And they have been following this in and out, but they've been closely following AI for

59
00:04:05,440 --> 00:04:06,440
the past year.

60
00:04:06,440 --> 00:04:08,840
So I wanted to have this broader conversation with them.

61
00:04:08,840 --> 00:04:11,760
As always, my email is reclinedshowatnytimes.com.

62
00:04:18,240 --> 00:04:19,960
Kevin Roos, Casey Newton.

63
00:04:19,960 --> 00:04:20,960
Welcome to the show, my friends.

64
00:04:20,960 --> 00:04:21,960
Hey, Ezra.

65
00:04:21,960 --> 00:04:23,960
Thanks for having us.

66
00:04:23,960 --> 00:04:24,960
All right.

67
00:04:24,960 --> 00:04:28,320
So we're talking on Monday, November 27th.

68
00:04:28,320 --> 00:04:33,840
JetGPT, which kicked off this era in AI, was released on November 30th, 2022.

69
00:04:33,840 --> 00:04:38,600
So the big anniversary party was at Sam Altman got temporarily fired and the company almost

70
00:04:38,600 --> 00:04:42,920
collapsed and was rebuilt over at Microsoft, which I don't think is how people expected

71
00:04:42,920 --> 00:04:44,160
to mark the anniversary.

72
00:04:44,160 --> 00:04:51,720
But it has been now a year, roughly, in this sort of whole new AI world that we're in.

73
00:04:51,720 --> 00:04:55,080
And so I want to talk about what's changed in that year.

74
00:04:55,080 --> 00:04:59,800
And the place I want to begin is with the capabilities of the AI systems we're seeing,

75
00:04:59,800 --> 00:05:04,200
not the ones we're hearing about, but that we know are actually being used by someone

76
00:05:04,200 --> 00:05:07,200
in semi-real world conditions.

77
00:05:07,200 --> 00:05:11,800
What can AI systems do today that they couldn't do a year ago, Kevin?

78
00:05:11,800 --> 00:05:18,160
Well, the first most obvious capabilities improvement is that these models have become

79
00:05:18,160 --> 00:05:19,800
what's called multimodal.

80
00:05:19,800 --> 00:05:26,680
So a year ago, we had ChatGPT, which could take in text input and output other texts

81
00:05:26,680 --> 00:05:28,800
as the response to your prompt.

82
00:05:28,800 --> 00:05:35,520
But now we have models that can take in text and output images, take in text and output

83
00:05:35,520 --> 00:05:40,760
video, take in voice data, and output other voice data.

84
00:05:40,760 --> 00:05:45,960
So these models are now working with many more types of inputs and outputs than they

85
00:05:45,960 --> 00:05:47,760
were a year ago.

86
00:05:47,760 --> 00:05:53,360
And that's sort of the most obvious difference if you just woke up from a year-long nap and

87
00:05:53,360 --> 00:05:56,820
took a look at the AI capabilities on the market, that's the thing that you would probably

88
00:05:56,820 --> 00:05:57,820
notice first.

89
00:05:57,820 --> 00:05:59,320
I want to pull something out about that.

90
00:05:59,320 --> 00:06:04,480
Is that it almost sounds like they're developing what you might call senses.

91
00:06:04,480 --> 00:06:07,920
And I recognize that there's a real danger of anthropomorphizing AI systems.

92
00:06:07,920 --> 00:06:09,480
I'm not trying to do that.

93
00:06:09,480 --> 00:06:13,440
But one thing about having different senses is that we get some information that helps

94
00:06:13,440 --> 00:06:16,360
us learn about the world from our eyes, other information that helps us learn about the

95
00:06:16,360 --> 00:06:18,600
world from our ears, et cetera.

96
00:06:18,600 --> 00:06:22,420
One of the constraints on the models is how much training data they can have.

97
00:06:22,420 --> 00:06:27,400
As they become multimodal, it would seem that would radically expand the amount of training

98
00:06:27,400 --> 00:06:28,400
data.

99
00:06:28,400 --> 00:06:35,600
Not just all of the text on the internet, but all of the audio on YouTube, or all podcast

100
00:06:35,600 --> 00:06:38,640
audio on Spotify or something, or Apple Podcasts.

101
00:06:38,640 --> 00:06:43,120
That's a lot of data to learn about the world from, that in theory will make the models

102
00:06:43,120 --> 00:06:44,440
smarter and more and more capable.

103
00:06:44,440 --> 00:06:46,920
Does it have that kind of recursive quality?

104
00:06:46,920 --> 00:06:47,920
Absolutely.

105
00:06:47,920 --> 00:06:52,800
I mean, part of the backdrop for these capabilities improvements is this race for high-quality

106
00:06:52,800 --> 00:06:53,800
data.

107
00:06:53,800 --> 00:07:00,120
AI labs are obsessed with finding new undiscovered, high-quality data sources that they can use

108
00:07:00,120 --> 00:07:01,200
to train their models.

109
00:07:01,200 --> 00:07:05,640
And so if you run out of text because you've scraped the entire internet, then you've got

110
00:07:05,640 --> 00:07:11,160
to go to podcasts or YouTube videos or some other source of data to keep improving your

111
00:07:11,160 --> 00:07:12,160
models.

112
00:07:12,160 --> 00:07:15,880
For what it's worth, though, I don't think the availability of more training data is

113
00:07:15,880 --> 00:07:17,600
what is interesting about the past year.

114
00:07:17,600 --> 00:07:21,560
I think what was interesting about ChatGBT was that it gave average people a way to interact

115
00:07:21,560 --> 00:07:23,080
with AI for the first time.

116
00:07:23,080 --> 00:07:27,800
It was just a box that you could type in and ask it anything and often get something pretty

117
00:07:27,800 --> 00:07:29,520
good in response.

118
00:07:29,520 --> 00:07:33,640
And even a year into folks using this now, I don't think we fully discovered everything

119
00:07:33,640 --> 00:07:35,360
that it can be used for.

120
00:07:35,360 --> 00:07:39,000
And I think more people are experiencing vertigo every day as they think about what this could

121
00:07:39,000 --> 00:07:40,800
mean for their own jobs and careers.

122
00:07:40,800 --> 00:07:44,200
So to me, the important thing was actually just the box that you type in and get questions

123
00:07:44,200 --> 00:07:45,200
from.

124
00:07:45,200 --> 00:07:46,200
Yeah, I agree with that.

125
00:07:46,200 --> 00:07:51,520
I think if you had just paused there and there was no new development in AI, I think it would

126
00:07:51,520 --> 00:07:58,280
still probably take the next five or 10 years for society to adjust to the new capabilities

127
00:07:58,280 --> 00:07:59,960
in our midst.

128
00:07:59,960 --> 00:08:03,960
So you've made this point in other places, Casey, that a lot of the advances to come

129
00:08:03,960 --> 00:08:09,320
are going to be in user interfaces and in how we interact with these systems.

130
00:08:09,320 --> 00:08:11,520
In a way, that was a big advance of ChatGBT.

131
00:08:11,520 --> 00:08:15,840
The system behind it had been around for a while, but the ability to speak to it, I guess,

132
00:08:15,840 --> 00:08:21,300
write to it in natural language, it created this huge cultural moment around AI.

133
00:08:21,300 --> 00:08:25,180
But what can these AI products actually do that they couldn't do a year ago?

134
00:08:25,180 --> 00:08:30,260
Not just how we interface with them, but their underlying capacity or power.

135
00:08:30,260 --> 00:08:36,260
I mean, as of the developer update that OpenAI had a few weeks back, the world knowledge

136
00:08:36,260 --> 00:08:40,040
of the system has been updated to April of this year.

137
00:08:40,040 --> 00:08:46,060
And so you're able to get something closer to real-time knowledge of world events.

138
00:08:46,060 --> 00:08:52,300
It has now integrated with Microsoft Bing, and so you can get truly real-time information

139
00:08:52,300 --> 00:08:56,300
in a way that was impossible when ChatGBT launched.

140
00:08:56,300 --> 00:08:59,940
And these might sound like relatively minor things, Azura, but you start chaining them

141
00:08:59,940 --> 00:09:05,540
together, you start building the right interfaces, and you actually start to see beyond the internet

142
00:09:05,540 --> 00:09:06,540
as we know it today.

143
00:09:06,540 --> 00:09:11,340
You see a world-worthy web where Google is not our starting point for doing everything

144
00:09:11,340 --> 00:09:12,340
online.

145
00:09:12,340 --> 00:09:15,700
It is just a little box on your computer that you type in and you get the answer without

146
00:09:15,700 --> 00:09:16,900
ever visiting a web page.

147
00:09:16,900 --> 00:09:21,060
So that's all going to take many years to unfold, but the beginnings of it are easy

148
00:09:21,060 --> 00:09:22,060
to see now.

149
00:09:22,060 --> 00:09:27,180
One other capability that didn't exist a year ago, at least in any public products, is the

150
00:09:27,180 --> 00:09:29,980
ability to bring your own data into these models.

151
00:09:29,980 --> 00:09:34,420
So Claude was the first language model that I used that had the ability to, say, upload

152
00:09:34,420 --> 00:09:35,420
a PDF.

153
00:09:35,420 --> 00:09:41,000
So you could say, here's a research paper, it's 100 pages long, help me summarize and

154
00:09:41,000 --> 00:09:42,000
analyze this.

155
00:09:42,000 --> 00:09:43,000
And it could do that.

156
00:09:43,020 --> 00:09:47,920
Now ChatGPT can do the same thing, and I know a bunch of other systems are moving in that

157
00:09:47,920 --> 00:09:48,920
direction too.

158
00:09:48,920 --> 00:09:53,080
There are also companies that have tried to spin up their own language models that are

159
00:09:53,080 --> 00:09:55,120
trained on their own internal data.

160
00:09:55,120 --> 00:10:02,480
So if you are Coca-Cola or BCG or some other business and you want an internal ChatGPT that

161
00:10:02,480 --> 00:10:07,080
you can use for your own employees to ask, say, questions about your HR documents, that

162
00:10:07,080 --> 00:10:08,960
is a thing that companies have been building.

163
00:10:08,960 --> 00:10:12,920
So that's not the sexiest, most consumer-facing application, but that is something that there's

164
00:10:12,920 --> 00:10:14,960
enormous demand for out there.

165
00:10:14,960 --> 00:10:18,840
So one thing it seems to me to be getting better at from what I can tell from others

166
00:10:18,840 --> 00:10:19,840
is coding.

167
00:10:19,840 --> 00:10:25,520
I have to ask people whether they're using AI bots very often, and if so, for what.

168
00:10:25,520 --> 00:10:29,280
And basically nobody says yes unless they are coder.

169
00:10:29,280 --> 00:10:32,520
Everybody says, oh yeah, I played around with it, I thought it was really cool, I sometimes

170
00:10:32,520 --> 00:10:38,600
use Dolly or Mid Journey to make pictures for my kids or for my email newsletter.

171
00:10:38,600 --> 00:10:42,280
But it is the coders who say, I'm using it all the time, it has become completely essential

172
00:10:42,280 --> 00:10:43,280
to me.

173
00:10:43,280 --> 00:10:47,200
I'm curious to hear a bit about that capability increase.

174
00:10:47,200 --> 00:10:52,160
I think where it has sort of become part of the daily habit of programmers is through

175
00:10:52,160 --> 00:10:58,760
tools like GitHub Copilot, which is a basically ChatGPT for coders that finishes whatever line

176
00:10:58,760 --> 00:11:03,360
of code you're working on or helps you debug some code that's broken.

177
00:11:03,360 --> 00:11:05,640
And there have been some studies and tests.

178
00:11:05,640 --> 00:11:11,040
I think there was one test that GitHub itself ran where they gave two groups of coders the

179
00:11:11,040 --> 00:11:12,040
same task.

180
00:11:12,040 --> 00:11:16,560
And one group was allowed to use GitHub Copilot and one group wasn't.

181
00:11:16,560 --> 00:11:21,600
And the group with GitHub Copilot finished the task 55% faster than the group without

182
00:11:21,600 --> 00:11:22,600
it.

183
00:11:22,600 --> 00:11:25,080
Now that is like a radical productivity increase.

184
00:11:25,080 --> 00:11:29,960
And if you tell a programmer, here's a tool that can make you 55% faster, they're going

185
00:11:29,960 --> 00:11:32,400
to want to use that every day.

186
00:11:32,400 --> 00:11:39,040
So when I see function chat bots in the wild, what I see is different versions of what people

187
00:11:39,040 --> 00:11:44,120
used to somewhat derisively call like the fancy autocomplete, right?

188
00:11:44,120 --> 00:11:47,360
Help you finish a line of code, help you finish this email.

189
00:11:47,360 --> 00:11:51,640
You ask a question that you might ask a search engine, like why do I have spots all over

190
00:11:51,640 --> 00:11:52,640
my elbow?

191
00:11:52,640 --> 00:11:55,520
And it gives you an answer that hopefully is right, but maybe is not right.

192
00:11:55,520 --> 00:11:59,240
I do think some of the search implications are interesting, but at the same time, it

193
00:11:59,240 --> 00:12:02,360
is not the case that Bing has made great strides on Google.

194
00:12:02,360 --> 00:12:05,480
People have not moved to asking the kind of Bing chat bot.

195
00:12:05,480 --> 00:12:10,080
Next question is as opposed to asking Google, everybody feels like they need AI in their

196
00:12:10,080 --> 00:12:11,080
thing now, right?

197
00:12:11,080 --> 00:12:14,400
There's a, I don't think you can raise money in Silicon Valley at the moment if you don't

198
00:12:14,400 --> 00:12:19,360
have a generative AI play built into your product or built into your business strategy.

199
00:12:19,360 --> 00:12:21,680
But that was true for a minute for crypto too.

200
00:12:21,680 --> 00:12:25,200
And I'm not one of the people who makes a crypto AI analogy.

201
00:12:25,200 --> 00:12:29,600
I think crypto is largely vaporware and AI is largely real.

202
00:12:29,600 --> 00:12:32,680
But Silicon Valley is faddish and people don't know how to use things.

203
00:12:32,680 --> 00:12:36,200
And so everybody tries to put things in all at once.

204
00:12:36,200 --> 00:12:38,040
What product has actually gotten way better?

205
00:12:38,040 --> 00:12:40,120
I'll just use one example.

206
00:12:40,120 --> 00:12:43,480
There's an app you might be familiar with called Notion.

207
00:12:43,480 --> 00:12:46,440
It's productivity sort of collaborative software.

208
00:12:46,440 --> 00:12:47,440
I write a newsletter.

209
00:12:47,440 --> 00:12:50,640
I save every link that I put in my newsletter into Notion.

210
00:12:50,640 --> 00:12:54,320
And now that there is AI inside Notion, Notion can do a couple of things.

211
00:12:54,320 --> 00:12:57,480
One, it can just look at every link I save and just write a two sentence summary for

212
00:12:57,480 --> 00:13:00,720
me, which is just sort of nice to see at a glance what that story is about.

213
00:13:00,760 --> 00:13:04,760
And most recently, it added a feature where you can just do Q&A with a database and say

214
00:13:04,760 --> 00:13:10,760
like, hey, what are some of the big stories about Meta over the past few weeks?

215
00:13:10,760 --> 00:13:15,120
And it'll just start pulling those up, essentially querying the database that I have built.

216
00:13:15,120 --> 00:13:20,280
And so while we're very early in this, you're beginning to see a world where AI is taking

217
00:13:20,280 --> 00:13:25,360
data that you have stored somewhere and it's turning it into your personal research assistant.

218
00:13:25,360 --> 00:13:26,800
So is it great right now?

219
00:13:26,800 --> 00:13:28,960
No, I would give it like a C.

220
00:13:29,200 --> 00:13:31,200
For one point now, I think it's not bad.

221
00:13:31,200 --> 00:13:35,320
And I'll share another example that is not from my own use, but I was talking a few

222
00:13:35,320 --> 00:13:39,920
weeks ago with a doctor, who's a friend of a friend, and doctors, you get tons of messages

223
00:13:39,920 --> 00:13:40,920
from patients.

224
00:13:40,920 --> 00:13:42,280
You know, what's this rash?

225
00:13:42,280 --> 00:13:44,120
Can you renew this prescription?

226
00:13:44,120 --> 00:13:45,760
Do I need to come in for a blood test?

227
00:13:45,760 --> 00:13:46,760
Like that kind of stuff.

228
00:13:46,760 --> 00:13:52,200
And doctors and nurses spend a ton of time just opening up their message portal, replying

229
00:13:52,200 --> 00:13:53,200
to all these messages.

230
00:13:53,200 --> 00:13:56,240
It's a huge part of being a doctor and it's a part that they don't like.

231
00:13:56,240 --> 00:14:00,720
And so this doctor was telling me that they have this software now that essentially uses

232
00:14:00,720 --> 00:14:01,720
a language model.

233
00:14:01,720 --> 00:14:06,720
I assume it's open AIs or someone very similar to that, that goes in and pre fills the responses

234
00:14:06,720 --> 00:14:08,760
to patient queries.

235
00:14:08,760 --> 00:14:13,080
And the doctor still has to look it over, make sure everything's right and press send.

236
00:14:13,080 --> 00:14:18,200
But just that act of pre-populating the field, this person was saying that saves them a ton

237
00:14:18,200 --> 00:14:21,640
of time, like on the order of several hours a day.

238
00:14:21,640 --> 00:14:28,400
But if you have that and you sort of extrapolate to what if every doctor in America was saving

239
00:14:28,400 --> 00:14:31,800
themselves an hour or two a day of responding to patient messages?

240
00:14:31,800 --> 00:14:34,760
I mean, that's a radical productivity enhancement.

241
00:14:34,760 --> 00:14:38,120
And so you can say that that's just fancy autocomplete and I guess on some level it

242
00:14:38,120 --> 00:14:43,240
is, but just having fancy autocomplete in these paperwork heavy professions could be

243
00:14:43,240 --> 00:14:44,240
very important.

244
00:14:44,240 --> 00:14:51,080
Well, let me push out in two directions because one direction is that I am not super thrilled

245
00:14:51,320 --> 00:14:56,800
about the idea that my doctor theoretically here is glancing over things and clicking

246
00:14:56,800 --> 00:15:02,280
submit as opposed to reading my message themselves and having to do the act of writing, which

247
00:15:02,280 --> 00:15:05,440
helps you think about things and thinking about what I actually emailed them and like

248
00:15:05,440 --> 00:15:07,200
what kind of answer they need to give me.

249
00:15:07,200 --> 00:15:12,680
I mean, I know personally the difference in thought between scanning things and editing

250
00:15:12,680 --> 00:15:14,640
and thinking through things.

251
00:15:14,640 --> 00:15:19,680
So that's like my diminishing response, but the flip of it is the thing I'm not hearing

252
00:15:19,720 --> 00:15:24,480
anybody say here and the thing I keep waiting for and being interested in is the things

253
00:15:24,480 --> 00:15:26,560
that I might be able to do better than my doctor.

254
00:15:26,560 --> 00:15:30,720
I was reading Jack Clark's import AI newsletter today, which I super recommend to people who

255
00:15:30,720 --> 00:15:35,600
want to follow advancements in the field and he was talking about a, I mean, it was a system

256
00:15:35,600 --> 00:15:39,160
being tested, not a system that is in deployment, but it was better at picking up pancreatic

257
00:15:39,160 --> 00:15:43,480
cancer from certain kinds of information than doctors are.

258
00:15:43,480 --> 00:15:48,200
And I keep waiting to hear something like this going out into the field, right?

259
00:15:48,200 --> 00:15:51,120
Something that doesn't just save people a bit of time around the edges.

260
00:15:51,120 --> 00:15:52,800
I agree that's a productivity improvement.

261
00:15:52,800 --> 00:15:55,280
It's fine. You can build a business around that.

262
00:15:55,280 --> 00:15:59,440
But the promise of AI when Sam Altman sat with you all a few weeks ago or however long

263
00:15:59,440 --> 00:16:02,520
it was and said, we're moving to the best world ever.

264
00:16:02,520 --> 00:16:05,200
He didn't mean that our paperwork is going to get a little bit easier to complete.

265
00:16:05,200 --> 00:16:07,600
Like he meant we'd have cures for new diseases.

266
00:16:07,600 --> 00:16:11,160
He meant that we would have new kinds of energy possibilities.

267
00:16:11,160 --> 00:16:16,200
I'm interested in the programs and the models that can create things that don't exist.

268
00:16:17,200 --> 00:16:20,160
Well, to get there, you need systems that can reason.

269
00:16:20,160 --> 00:16:24,000
And right now the systems that we have just aren't very good at reasoning.

270
00:16:24,000 --> 00:16:28,760
I think that over the past year, we have seen them move a little away from the way that

271
00:16:28,760 --> 00:16:32,480
I was thinking of them a year ago, which was a sort of fancy autocomplete, right?

272
00:16:32,480 --> 00:16:35,360
It's sort of making it, making a prediction about what the next word will be.

273
00:16:35,360 --> 00:16:40,120
This is true that they do it that way, but it is able to create a kind of facsimile of

274
00:16:40,120 --> 00:16:42,040
thought that can be interesting in some ways.

275
00:16:42,040 --> 00:16:45,840
But you just can't get to where you're going, Ezra, with like a facsimile of thought.

276
00:16:45,840 --> 00:16:48,880
You need something that has improved reasoning capabilities.

277
00:16:48,880 --> 00:16:51,880
So maybe that comes with the next generation frontier models.

278
00:16:51,880 --> 00:16:54,360
But until then, I think you'll be disappointed.

279
00:16:54,360 --> 00:16:56,000
But do you need a different kind of model?

280
00:16:56,000 --> 00:16:58,200
This is something that lingers in the back of my head.

281
00:16:58,200 --> 00:17:02,560
So I did an interview on the show with Demis Isabis, who's the co-founder of DeepMind.

282
00:17:02,560 --> 00:17:06,040
Now we're going to see integrated DeepMind Google AI program.

283
00:17:06,040 --> 00:17:11,000
And DeepMind had built this system while back called AlphaFold, which treated how

284
00:17:11,000 --> 00:17:14,760
proteins are constructed in 3D space, which is to say, in reality,

285
00:17:14,760 --> 00:17:17,720
we live in 3D space, it treated it as a game.

286
00:17:17,720 --> 00:17:21,640
And it fed itself a bunch of information and it became very good at predicting the

287
00:17:21,640 --> 00:17:25,320
structure of proteins and that solved this really big scientific problem.

288
00:17:25,320 --> 00:17:29,240
And they then created a subsidiary of Alphabet called Isomorphic Labs to try

289
00:17:29,240 --> 00:17:33,280
to build drug discovery on similar foundations.

290
00:17:33,280 --> 00:17:37,680
But my understanding is that Google during this period became terrified of Microsoft

291
00:17:37,680 --> 00:17:40,400
and open AI beating it up in search and office.

292
00:17:40,440 --> 00:17:44,280
And so they pulled a lot of resources, not least Hasabis himself,

293
00:17:44,280 --> 00:17:47,680
into this integrated structure to try to win the chatbot wars,

294
00:17:47,680 --> 00:17:50,680
which is now what their system barred us trying to do.

295
00:17:50,680 --> 00:17:53,840
And so when you said, Casey, that we need things that can reason, I mean, maybe,

296
00:17:53,840 --> 00:17:57,560
but also you could say we need things that are tailored to solve problems we care

297
00:17:57,560 --> 00:17:58,760
about more.

298
00:17:58,760 --> 00:18:00,880
And I think this is one of the things that worries me a bit,

299
00:18:00,880 --> 00:18:08,680
that we've backed ourselves into business models that are not that important for humanity.

300
00:18:08,760 --> 00:18:10,320
Is there some chance of that?

301
00:18:10,320 --> 00:18:16,160
I mean, are we going too hard after language-based general intelligent AI

302
00:18:16,160 --> 00:18:20,720
that, by the way, integrates very nicely into a suite of enterprise software

303
00:18:20,720 --> 00:18:24,080
as opposed to building things that actually create scientific breakthroughs,

304
00:18:24,080 --> 00:18:28,880
but don't have the same kind of high-scalability profit structure behind them?

305
00:18:30,800 --> 00:18:34,920
I would stick up for the people who are working on the sort of what you could call

306
00:18:34,920 --> 00:18:37,760
like the non-language problems in AI right now.

307
00:18:37,760 --> 00:18:39,480
This stuff is going on.

308
00:18:39,480 --> 00:18:43,360
It maybe doesn't get as much attention from people like three of us as it should.

309
00:18:43,360 --> 00:18:48,680
But if you talk to folks in fields like pharmaceuticals and biotech,

310
00:18:48,680 --> 00:18:52,800
there are new AI biotech companies spinning up every day,

311
00:18:52,800 --> 00:18:58,120
getting funding to go after drug discovery or some more narrow application.

312
00:18:58,120 --> 00:19:01,600
Like we talked to a researcher the other day, formerly of Google,

313
00:19:01,600 --> 00:19:04,200
who is teaching AI to smell, right?

314
00:19:04,200 --> 00:19:08,560
Taking the same techniques that go into these transformer-based neural networks

315
00:19:08,560 --> 00:19:13,640
like chat GPT and applying them to the molecular structures of different chemicals

316
00:19:13,640 --> 00:19:17,000
and using that to be able to predict what these things will smell like.

317
00:19:17,000 --> 00:19:19,240
And you might say, well, what's the big deal with that?

318
00:19:19,240 --> 00:19:23,640
And the answer is that some diseases have smells associated with them

319
00:19:23,640 --> 00:19:29,480
that we can't pick up on because our noses aren't as sensitive as, say, dogs or other animals.

320
00:19:29,480 --> 00:19:32,880
But if you could train an AI to be able to recognize scent molecules

321
00:19:32,880 --> 00:19:36,080
and predict odors from just chemical structures,

322
00:19:36,080 --> 00:19:38,040
that could actually be useful in all kinds of ways.

323
00:19:38,040 --> 00:19:40,240
So I think this kind of thing is happening.

324
00:19:40,240 --> 00:19:43,880
It's just not sort of dominating the coverage the way that chat GPT is.

325
00:20:03,000 --> 00:20:11,400
Let me ask you, Kevin, about, I think, an interesting, maybe promising,

326
00:20:11,400 --> 00:20:15,960
maybe scary avenue for AI that you possibly personally foreclosed,

327
00:20:15,960 --> 00:20:24,880
which is at some point during the year, Microsoft gave you access to a open AI-powered chatbot

328
00:20:24,880 --> 00:20:27,560
that had this dual personality of Sydney.

329
00:20:27,560 --> 00:20:30,240
And Sydney tried to convince you you didn't love your wife

330
00:20:30,240 --> 00:20:32,000
and that you wanted to run away with Sydney.

331
00:20:32,000 --> 00:20:35,760
And my understanding is immediately after that happened,

332
00:20:35,760 --> 00:20:38,400
everybody with enough money to have a real business model in AI

333
00:20:38,400 --> 00:20:40,800
lobotomized the personalities of their AI.

334
00:20:40,800 --> 00:20:42,600
It's like, that was the end of Sydney.

335
00:20:42,600 --> 00:20:46,640
But there are a lot of startups out there trying to do AI friends, AI therapists,

336
00:20:46,640 --> 00:20:51,640
AI sex bots, AI, you know, boyfriends and girlfriends and non-binary partners.

337
00:20:51,640 --> 00:20:56,320
Just every kind of AI companion you can imagine.

338
00:20:56,360 --> 00:21:01,480
I've always thought this is a pretty obvious way this will affect society.

339
00:21:01,480 --> 00:21:05,880
And the Sydney thing convinced me that the technology for it already exists.

340
00:21:05,880 --> 00:21:09,680
So where is that?

341
00:21:09,680 --> 00:21:11,880
And how are those companies doing?

342
00:21:11,880 --> 00:21:19,360
Yeah, I mean, I'm sorry, A, if I did foreclose the possibility of AI personalities.

343
00:21:19,360 --> 00:21:23,720
I think what's happening is it's just a little too controversial

344
00:21:23,720 --> 00:21:27,560
and sort of fraught force any of the big companies to wait into.

345
00:21:27,560 --> 00:21:33,240
Like Microsoft doesn't want its AI assistants and co-pilots to have strong personalities.

346
00:21:33,240 --> 00:21:34,760
Like that much is clear.

347
00:21:34,760 --> 00:21:39,160
And I don't think their enterprise customers want them to have strong personalities,

348
00:21:39,160 --> 00:21:42,560
especially those personalities are adversarial or confrontational

349
00:21:42,560 --> 00:21:44,360
or creepy or unpredictable in some way.

350
00:21:44,360 --> 00:21:48,280
They want like, they want clippy, but like with real brain power.

351
00:21:48,280 --> 00:21:52,640
But there are companies that are going after this more social AI market.

352
00:21:52,680 --> 00:21:55,000
One of them is this company Character AI,

353
00:21:55,000 --> 00:22:00,720
which was started by one of the original people at Google who made the transformer breakthrough.

354
00:22:00,720 --> 00:22:04,000
And that company is growing pretty rapidly.

355
00:22:04,000 --> 00:22:07,160
They've got a lot of users, especially young users.

356
00:22:07,160 --> 00:22:09,840
And they are doing essentially AI personas.

357
00:22:09,840 --> 00:22:12,160
You can make your own AI persona and chat with it

358
00:22:12,160 --> 00:22:15,400
or you can pick from ones that others have created.

359
00:22:15,400 --> 00:22:17,840
Meta is also going a little bit in this direction.

360
00:22:17,840 --> 00:22:21,160
They have these sort of persona driven AI chatbots.

361
00:22:21,160 --> 00:22:25,320
And all of these companies have put sort of guardrails around

362
00:22:25,320 --> 00:22:31,160
like no one really wants to do the erotic, what they call erotic sort of role play

363
00:22:31,160 --> 00:22:36,560
in part because they don't want to run afoul of things like the Apple app store terms of service.

364
00:22:36,560 --> 00:22:40,200
But I expect that that will also be a big market for young people.

365
00:22:40,200 --> 00:22:44,320
And anecdotally, I mean, I have just heard from a lot of young people

366
00:22:44,320 --> 00:22:49,480
who already say like my friends have AI chatbot friends that they talk to all the time.

367
00:22:49,480 --> 00:22:53,520
And it does seem to be making inroads into high schools.

368
00:22:53,520 --> 00:22:56,600
And that's just an area that I'll be fascinated to track.

369
00:22:56,600 --> 00:22:58,320
I mean, this is going to be huge.

370
00:22:58,320 --> 00:22:59,320
A couple of thoughts coming to mind.

371
00:22:59,320 --> 00:23:02,880
One, I talked to somebody who works at one of the leading AI companies

372
00:23:02,880 --> 00:23:06,120
and they told me that 99% of people whose accounts they remove,

373
00:23:06,120 --> 00:23:09,040
they remove for trying to get it to write tech space erotica.

374
00:23:09,040 --> 00:23:12,280
OK, so that I think speaks to the market demand for this sort of thing.

375
00:23:12,280 --> 00:23:14,920
I've also talked to people who have used the models of this

376
00:23:14,920 --> 00:23:17,560
that are not constrained by any sort of safety guidelines.

377
00:23:17,560 --> 00:23:20,040
And I've been told these things are actually incredible at writing erotica.

378
00:23:20,040 --> 00:23:22,600
So what I'm telling you is there is a $10 billion.

379
00:23:22,600 --> 00:23:24,360
You've got really a lot of reporting on this case.

380
00:23:24,360 --> 00:23:26,600
You say maybe a personal interest.

381
00:23:26,600 --> 00:23:28,880
Look, I write about content moderation.

382
00:23:28,880 --> 00:23:32,000
And like porn is the content moderation frontier.

383
00:23:32,000 --> 00:23:34,440
And it's just very interesting to me that it's so clear

384
00:23:34,440 --> 00:23:37,360
that there are billions of dollars to be made here and no company will touch it.

385
00:23:37,360 --> 00:23:39,680
And I asked one person involved, I said, why don't you just let people do this?

386
00:23:39,680 --> 00:23:42,760
And they basically said, look, if you do this, you become a porn company overnight.

387
00:23:42,760 --> 00:23:44,400
It like overwhelms the usage.

388
00:23:44,400 --> 00:23:46,120
Like this is what people wind up using your thing for.

389
00:23:46,160 --> 00:23:48,280
And you're you're working in a different company then.

390
00:23:48,280 --> 00:23:49,120
So I sort of get it.

391
00:23:49,120 --> 00:23:52,560
But, you know, even setting aside the explicitly erotic stuff, you know,

392
00:23:52,560 --> 00:23:56,000
as you well know and have talked and written about just like the lowliness epidemic

393
00:23:56,000 --> 00:23:59,600
that we have in this country, there's a lot of isolated people in this world.

394
00:23:59,720 --> 00:24:02,680
And I think there is a very real possibility that a lot of those people

395
00:24:02,680 --> 00:24:07,720
will find comfort and joy and delight with talking to these AI based companions.

396
00:24:07,880 --> 00:24:11,320
I also think that when that happens, there will be a culture war over it.

397
00:24:11,440 --> 00:24:14,920
And we will see lengthy segments on Fox News about how the Silicon Valley

398
00:24:14,920 --> 00:24:17,600
technologists created a generation of shut-ins who wants to do nothing

399
00:24:17,600 --> 00:24:19,280
but talk to their fake friends on their phones.

400
00:24:19,280 --> 00:24:21,760
So I do think this is like the cultural war yet to come.

401
00:24:21,920 --> 00:24:25,120
And the question is just sort of when do the enabling technologies get good enough?

402
00:24:25,120 --> 00:24:28,640
And when do companies decide that they're willing to deal with the blowback?

403
00:24:28,760 --> 00:24:30,840
I also think this is going to be a generational thing.

404
00:24:30,840 --> 00:24:34,000
I mean, I'm very interested in this and have been for for a bit, in part

405
00:24:34,000 --> 00:24:38,040
because I suspect if I had to make a prediction here, my five year old

406
00:24:38,040 --> 00:24:42,560
is going to grow up with AI friends and my sort of pat line is that today

407
00:24:42,600 --> 00:24:45,600
we worry that 12 year olds don't see their friends enough in person.

408
00:24:46,000 --> 00:24:49,760
And tomorrow we'll worry that not enough of our 12 year old's friends or persons

409
00:24:50,320 --> 00:24:52,160
because it's going to become normal.

410
00:24:52,280 --> 00:24:55,080
And my sense is that the systems are really good.

411
00:24:55,080 --> 00:24:58,400
If you unleashed them, you are already good enough to

412
00:24:58,400 --> 00:25:00,600
functionally master this particular application.

413
00:25:00,600 --> 00:25:02,720
And the big players simply haven't unleashed them.

414
00:25:02,720 --> 00:25:07,280
I've heard from people at the big companies here who are like, oh, yeah,

415
00:25:07,280 --> 00:25:10,360
if we wanted to do this, we could dominate it.

416
00:25:10,800 --> 00:25:13,960
But that does bring me to a question, which is META kind of does want to do this.

417
00:25:13,960 --> 00:25:16,760
META, which owns Facebook, which is a social media company,

418
00:25:17,280 --> 00:25:21,760
they seem to want to do it in terms of these lame, seeming celebrity avatars.

419
00:25:21,760 --> 00:25:23,360
Like you can talk to AI Snoop Dogg.

420
00:25:23,360 --> 00:25:24,160
So bad.

421
00:25:24,160 --> 00:25:28,840
But that is interesting to me because their AI division is run by Yanlacun,

422
00:25:28,920 --> 00:25:31,920
who is one of the most important AI researchers in the field.

423
00:25:32,320 --> 00:25:36,960
And they seem to have very different cultural dynamics in their AI shop

424
00:25:37,400 --> 00:25:40,680
than Google DeepMind or OpenAI.

425
00:25:41,600 --> 00:25:45,720
Tell me a bit about META's strategy here and what makes them culturally different.

426
00:25:46,680 --> 00:25:51,200
Well, Casey, you cover META and have for a long time and may have some insight here.

427
00:25:51,200 --> 00:25:55,520
My sense is that they are sort of up against a couple problems,

428
00:25:55,520 --> 00:26:02,080
one of which is they have arrived to AI late and to generative AI specifically.

429
00:26:02,080 --> 00:26:06,200
You know, Facebook was for many years considered one of the top two labs

430
00:26:06,200 --> 00:26:09,640
along with Google when it came to recruiting AI talent,

431
00:26:09,640 --> 00:26:13,560
to putting out cutting edge research, to presenting papers at the big AI conferences.

432
00:26:13,560 --> 00:26:15,160
They were one of the big dogs.

433
00:26:15,160 --> 00:26:17,600
And then they sort of had this funny thing happen

434
00:26:17,600 --> 00:26:23,280
where they released a model called Galactica just right before ChatGPT was released last year.

435
00:26:23,640 --> 00:26:28,480
And it was supposed to be this sort of like LLM for science and for research papers.

436
00:26:28,840 --> 00:26:31,280
And it was out for, I think, three days.

437
00:26:31,560 --> 00:26:35,000
And people started noticing that it was making up fake citations.

438
00:26:35,000 --> 00:26:35,720
It was hallucinating.

439
00:26:35,720 --> 00:26:39,320
It was doing what all the AI models do, but it was from META.

440
00:26:39,320 --> 00:26:40,320
And so it felt different.

441
00:26:40,320 --> 00:26:45,200
It had sort of this tarnish on it because people already worried about fake news on Facebook.

442
00:26:45,520 --> 00:26:50,040
And so it got pulled down and then ChatGPT just shortly thereafter

443
00:26:50,200 --> 00:26:52,320
launched and became this global sensation.

444
00:26:52,320 --> 00:26:57,000
So they're sort of grappling for what to do with this technology that they've built now.

445
00:26:57,200 --> 00:27:03,800
There's not a real obvious business case for shoving AI chatbots into products

446
00:27:03,800 --> 00:27:09,200
like Facebook and Instagram, and they don't sell enterprise software like Microsoft does.

447
00:27:09,400 --> 00:27:12,440
So they can't really shove it into paid subscription products.

448
00:27:12,440 --> 00:27:14,640
So my sense from talking with folks over there

449
00:27:14,640 --> 00:27:18,000
is that they're just kind of not sure what to do with this technology that they've built.

450
00:27:18,240 --> 00:27:21,040
And so they're just flinging it open to the masses.

451
00:27:21,480 --> 00:27:22,920
What do you think?

452
00:27:22,920 --> 00:27:23,600
That tracks with me.

453
00:27:23,600 --> 00:27:25,720
I sort of basically don't get it either.

454
00:27:25,720 --> 00:27:29,400
Like basically what you've just said has been explained to me.

455
00:27:29,600 --> 00:27:35,400
They are investing a ton with no obvious return on investment in the near term future.

456
00:27:35,680 --> 00:27:39,400
I will say that these celebrity AI chatbots they've made are quite bad.

457
00:27:39,400 --> 00:27:40,720
Like it's truly baffling.

458
00:27:40,720 --> 00:27:44,560
And the thing is they've taken celebrities, but the celebrities are not playing themselves in the AI.

459
00:27:44,560 --> 00:27:50,600
They've given all of the celebrities silly names and you can just sort of like follow their Instagram

460
00:27:50,600 --> 00:27:55,360
and like send them messages and say like, hey, like character that Snoop Dogg is portraying.

461
00:27:55,400 --> 00:27:56,520
Like, what do you think about it?

462
00:27:56,520 --> 00:27:57,680
So it's all very silly.

463
00:27:57,680 --> 00:28:00,560
And I expect it'll die a rapid death sometime in the next year.

464
00:28:00,560 --> 00:28:02,120
And then we'll see if they have a better idea.

465
00:28:02,120 --> 00:28:05,600
What I will say is like, if you're somebody who wakes up from AI nightmares

466
00:28:05,760 --> 00:28:09,920
some mornings, as a lot of folks in San Francisco do, go listen to Jan LeCun talk about it.

467
00:28:10,000 --> 00:28:12,560
No one has ever been more relaxed about AI than Jan LeCun.

468
00:28:12,560 --> 00:28:15,960
You know, it's just sort of like an army of superhuman assistants

469
00:28:15,960 --> 00:28:17,280
are about to live inside your computer.

470
00:28:17,280 --> 00:28:21,000
They're going to do anything you want to do and there's no risk of them harming you ever.

471
00:28:21,160 --> 00:28:23,520
So if you're, you know, you're feeling anxious, go listen to Jan.

472
00:28:24,040 --> 00:28:25,560
Do you think he's right?

473
00:28:25,560 --> 00:28:27,440
Because it also has led to policy difference.

474
00:28:27,440 --> 00:28:32,960
Meta has been much more open source in their approach, which open AI

475
00:28:32,960 --> 00:28:37,040
and Google seem to think is irresponsible.

476
00:28:37,080 --> 00:28:40,440
But there's something happening there that I think is also built around a different view of safety.

477
00:28:40,440 --> 00:28:41,560
Like, what is their view of safety?

478
00:28:41,560 --> 00:28:44,600
Why does Jan LeCun, who is like an important figure in this whole world,

479
00:28:44,880 --> 00:28:48,520
why is he so much more chill than, you know, name your other founder?

480
00:28:49,480 --> 00:28:52,360
I mean, part of it is I just think these these are deeply held convictions

481
00:28:52,360 --> 00:28:55,320
from someone who is an expert on this space and who has been a pioneer

482
00:28:55,320 --> 00:28:58,080
and who understands the technology certainly far better than I do.

483
00:28:58,240 --> 00:29:01,000
And he can just sort of not see from here to kill a robot.

484
00:29:01,000 --> 00:29:05,760
So I I respect his viewpoint in that respect, given his credentials in the space.

485
00:29:06,080 --> 00:29:10,080
I think on the question of is open source AI safer?

486
00:29:10,320 --> 00:29:13,280
This is still an open question, not to pun.

487
00:29:13,520 --> 00:29:16,240
The argument for it being safer is well, if it's open source,

488
00:29:16,240 --> 00:29:20,520
that means that average people can go in and look at the code and identify flaws

489
00:29:20,720 --> 00:29:23,720
and kind of see how the machine works and they can point those out in public

490
00:29:23,720 --> 00:29:25,160
and then they can be fixed in public.

491
00:29:25,160 --> 00:29:28,920
Whereas if you have something like open AI, which is building very powerful systems

492
00:29:28,920 --> 00:29:31,960
behind closed doors, we don't have the same kind of access.

493
00:29:31,960 --> 00:29:34,520
And so you might not need to rely on a government regulator

494
00:29:34,520 --> 00:29:36,640
to see how safe their systems were.

495
00:29:36,640 --> 00:29:38,560
So that is the argument in favor of open source.

496
00:29:38,560 --> 00:29:42,680
Of course, the flip side of that is like, well, if you take a very powerful open source model

497
00:29:42,880 --> 00:29:46,520
and you put it out on the open web, even if it's true that anyone can poke holes

498
00:29:46,520 --> 00:29:49,920
and identify flaws, it's also true that a bad actor could take that model

499
00:29:49,920 --> 00:29:51,920
and then use it to do something really, really bad.

500
00:29:51,920 --> 00:29:55,240
So that hasn't happened yet, but it certainly seems like it's

501
00:29:55,240 --> 00:29:57,520
an obvious possibility at some time in the near future.

502
00:29:58,120 --> 00:30:00,600
Let me use that as a bridge to safety more generally.

503
00:30:00,600 --> 00:30:04,040
So we've talked a bit about where these systems have gone over the past year,

504
00:30:04,040 --> 00:30:05,440
where they seem to be going.

505
00:30:05,440 --> 00:30:10,280
But there's been a lot of concern that they are unsafe and fundamentally

506
00:30:10,280 --> 00:30:14,160
that they become misaligned or that we don't understand them or what they're doing.

507
00:30:15,000 --> 00:30:17,600
What kind of breakthroughs have there been with all this investment

508
00:30:17,600 --> 00:30:19,840
and all this attention on safety, Kevin?

509
00:30:20,960 --> 00:30:26,760
So a lot of work has gone into what is called fine tuning of these models.

510
00:30:26,960 --> 00:30:31,720
So basically, if you're making a large language model like GPT-4,

511
00:30:31,920 --> 00:30:33,880
you have several phases of that.

512
00:30:33,880 --> 00:30:38,160
Phase one is what's called pre-training, which is sort of just the basic process.

513
00:30:38,160 --> 00:30:40,800
You take all of this data, you shove it into this neural network,

514
00:30:40,800 --> 00:30:44,080
and it learns to make predictions about the next word in a sequence.

515
00:30:44,600 --> 00:30:46,720
Then from there, you do what's called fine tuning.

516
00:30:47,000 --> 00:30:51,000
And that is basically where you are trying to turn the model into something

517
00:30:51,000 --> 00:30:54,160
that's actually useful or tailored, turned it into a chatbot,

518
00:30:54,160 --> 00:30:58,960
turned it into a tool for doctors, turned it into something for social AIs.

519
00:30:58,960 --> 00:31:02,960
That's the process that includes things like reinforcement learning from human feedback,

520
00:31:02,960 --> 00:31:06,080
which is how a lot of the leading models are fine tuned.

521
00:31:06,400 --> 00:31:08,840
And that work has continued to progress.

522
00:31:09,160 --> 00:31:14,400
The models they say today are sort of safer and less likely to generate harmful outputs

523
00:31:14,680 --> 00:31:16,720
than previous generations of models.

524
00:31:17,000 --> 00:31:19,480
There's also this field of interpretability,

525
00:31:19,480 --> 00:31:22,720
which is where I've been doing a lot of reporting over the past few months,

526
00:31:23,080 --> 00:31:26,040
which is this sort of tiny subfield of AI

527
00:31:26,040 --> 00:31:30,840
that is trying to figure out what the guts of a language model look like

528
00:31:30,840 --> 00:31:34,040
and what is actually happening inside one of these models

529
00:31:34,320 --> 00:31:38,080
when you ask it a question or give it some prompt and it produces an output.

530
00:31:38,320 --> 00:31:42,880
And this is a huge deal, not only because I think people want to know how these things work,

531
00:31:42,880 --> 00:31:46,080
they're not satisfied by just saying these are like mystical black boxes,

532
00:31:46,520 --> 00:31:49,480
but also because if you understand what's going on inside a model,

533
00:31:49,480 --> 00:31:53,280
then you can understand if, for example, the model starts lying to you

534
00:31:53,280 --> 00:31:57,720
or starts becoming deceptive, which is a thing that AI safety researchers worry about.

535
00:31:57,720 --> 00:32:01,360
So that process of interpretability research, I think, is really important.

536
00:32:01,360 --> 00:32:05,720
There have been a few sort of minor breakthroughs in that field over the past year,

537
00:32:05,880 --> 00:32:09,720
but it is still slow going and it's still a very hard problem to crack.

538
00:32:10,360 --> 00:32:13,360
And I think it's worth just pausing to underscore what Kevin said,

539
00:32:13,360 --> 00:32:17,240
which is the people building these systems do not know how they work.

540
00:32:17,240 --> 00:32:20,720
They know at a high level, but there is a lot within that,

541
00:32:20,720 --> 00:32:23,360
where if you show them an individual output from the AI,

542
00:32:23,360 --> 00:32:26,760
they will not be able to tell you exactly why it said what it said.

543
00:32:26,800 --> 00:32:30,120
Also, if you run the same query multiple times, you'll get slightly different answers.

544
00:32:30,120 --> 00:32:32,840
Why is that? Again, the researchers can't tell you.

545
00:32:32,960 --> 00:32:36,240
So as we have these endless debates over AI safety,

546
00:32:36,360 --> 00:32:39,960
one reason why I do tend to lean on the side of the folks who are scared

547
00:32:39,960 --> 00:32:42,120
is this exact point at the end of the day.

548
00:32:42,120 --> 00:32:44,040
We still don't know how the systems work.

549
00:32:44,040 --> 00:32:45,840
Tell me if this tracks for you.

550
00:32:45,840 --> 00:32:49,960
I think compared to a year ago when I talked to the AI safety people,

551
00:32:49,960 --> 00:32:54,120
the people who worry about AIs that become misaligned

552
00:32:54,120 --> 00:32:59,920
and do terrible civilizational level damage, AIs that could be really badly misused.

553
00:33:00,520 --> 00:33:03,960
They seem to think it has been actually a pretty good year, most of them.

554
00:33:04,080 --> 00:33:06,760
They think they've been able to keep big models like GPT-4,

555
00:33:06,760 --> 00:33:10,920
which of course are much less powerful than what they one day expect to invent.

556
00:33:10,920 --> 00:33:13,000
But they think they've been pretty good at keeping them aligned.

557
00:33:13,560 --> 00:33:17,600
They have made some progress on interpretability, which wasn't totally clear.

558
00:33:17,600 --> 00:33:21,640
You know, a year ago, many people said that was potentially not a problem we could solve.

559
00:33:22,240 --> 00:33:24,120
You know, at least we're making some breakthroughs there.

560
00:33:24,800 --> 00:33:26,600
They're not relaxed.

561
00:33:26,600 --> 00:33:30,560
The people who worry about this and they, you know,

562
00:33:30,560 --> 00:33:33,560
will often say like we would need a long time to fully understand

563
00:33:33,560 --> 00:33:36,280
even the things we have now and we may not have that long.

564
00:33:36,760 --> 00:33:42,640
But nevertheless, I get the sense that the safety people seem a little more confident

565
00:33:42,640 --> 00:33:46,000
that the work, the technical work they've been doing is paying off.

566
00:33:46,280 --> 00:33:49,920
Then, you know, at least with the impression I got from the reporting prior.

567
00:33:50,600 --> 00:33:51,200
I think that's right.

568
00:33:51,200 --> 00:33:55,800
I mean, Sam Altman in particular, this has been his strategy is like,

569
00:33:55,800 --> 00:33:59,440
we are we are going to release this stuff that is in our our labs.

570
00:33:59,760 --> 00:34:03,240
And we're going to kind of wait and see how society reacts to it.

571
00:34:03,240 --> 00:34:05,640
And then we'll sort of give it some time to let society address.

572
00:34:05,880 --> 00:34:07,760
And then we will release the next thing.

573
00:34:07,760 --> 00:34:12,200
That's what he thinks is the best way to slowly integrate AI into our lives.

574
00:34:12,480 --> 00:34:17,240
And if you'd asked me maybe 11 months ago, like a month into using chat GPT,

575
00:34:17,600 --> 00:34:20,400
what are the odds of something really, really bad happening

576
00:34:20,400 --> 00:34:22,600
because of the availability of chat GPT?

577
00:34:22,920 --> 00:34:25,680
I would have put them much higher than they turned out to be.

578
00:34:25,680 --> 00:34:27,560
Right. And when you talk to folks at Open AI,

579
00:34:27,560 --> 00:34:31,720
like they will tell you that that company really has taken AI safety

580
00:34:31,720 --> 00:34:35,120
really seriously, you can see this yourself when you use the product.

581
00:34:35,120 --> 00:34:36,360
Ask it a question about sex.

582
00:34:36,360 --> 00:34:37,880
It basically calls the police.

583
00:34:37,880 --> 00:34:41,360
So there is a lot to be said for how these systems have been built so far.

584
00:34:42,000 --> 00:34:46,280
And I would say the other thing that I've heard from AI safety researchers

585
00:34:46,280 --> 00:34:50,520
is that they are feeling relief, not just that the world has not ended,

586
00:34:50,760 --> 00:34:54,120
but that more people are now worried about AI.

587
00:34:54,520 --> 00:34:57,800
It was a very lonely thing for many years

588
00:34:58,000 --> 00:35:01,840
to be someone who worried about AI safety because

589
00:35:02,440 --> 00:35:05,400
there was no apparent reason to be worried about AI safety, right?

590
00:35:05,400 --> 00:35:09,600
That the chatbots that were outward, it was like Siri and Alexa and they were terrible.

591
00:35:09,800 --> 00:35:13,520
And no one could imagine that these things could become dangerous or harmful

592
00:35:13,520 --> 00:35:16,240
because the technology itself was just not that advanced.

593
00:35:16,600 --> 00:35:18,720
Now you have congressional hearings.

594
00:35:18,840 --> 00:35:22,120
You have regulations coming from multiple countries.

595
00:35:22,320 --> 00:35:26,000
You have people like Jeff Hinton and Yashua Bengios,

596
00:35:26,000 --> 00:35:28,760
two of the sort of so-called godfathers of deep learning,

597
00:35:29,040 --> 00:35:31,680
proclaiming that they are worried about where this technology is headed.

598
00:35:31,680 --> 00:35:35,520
So I think for the people who have been working on this stuff for a long time,

599
00:35:35,760 --> 00:35:37,800
there is some just palpable relief at like,

600
00:35:37,800 --> 00:35:40,520
oh, I don't have to carry this all on my shoulders anymore.

601
00:35:40,520 --> 00:35:43,920
The world is now aware of these systems and what risks they could pose.

602
00:35:44,400 --> 00:35:49,360
One irony of it is that my read from talking to people is that AI safety

603
00:35:49,360 --> 00:35:53,040
is going better as a technical matter than was expected.

604
00:35:53,720 --> 00:35:59,320
And I think worse as a matter of governance and inter-corporate

605
00:35:59,840 --> 00:36:03,640
competition and regulatory arbitrage than they had hoped.

606
00:36:04,120 --> 00:36:07,880
There's a fear, as I understand it, that we could make the technical breakthroughs

607
00:36:07,880 --> 00:36:13,680
needed, but that the kind of coordination necessary to go slow enough to make them.

608
00:36:14,360 --> 00:36:16,000
Like that's where a lot of the fear is.

609
00:36:16,000 --> 00:36:18,560
I think they feel like that's actually going worse, not better.

610
00:36:19,280 --> 00:36:24,560
So one of the big narratives coming out of Sam Altman's firing was that it must

611
00:36:24,560 --> 00:36:26,640
have had something to do with AI safety.

612
00:36:26,960 --> 00:36:31,280
And, you know, based on my reporting and reporting shared by many others,

613
00:36:31,520 --> 00:36:35,760
this was not an AI safety issue, but it is very much the story

614
00:36:35,760 --> 00:36:39,240
that is being discussed about the whole affair.

615
00:36:39,520 --> 00:36:44,320
And the folks who are on the board who are associated with these AI safety ideas,

616
00:36:44,560 --> 00:36:47,200
they've taken a huge hit to their public reputation because of the way

617
00:36:47,200 --> 00:36:48,760
they handle the firing and sort of all of that.

618
00:36:49,120 --> 00:36:53,880
And so I think a really bad outcome of this firing is that the AI safety

619
00:36:53,880 --> 00:36:58,800
community loses its credibility, even though AI safety, as far as we can tell,

620
00:36:58,920 --> 00:37:01,320
really didn't have a lot to do with what happened to Sam Altman.

621
00:37:01,800 --> 00:37:05,560
Yeah, I first agree that clearly, AI safety was not behind

622
00:37:05,560 --> 00:37:07,320
whatever disagreements Altman and the board had.

623
00:37:07,320 --> 00:37:11,640
I heard that from both sides of this and I didn't believe it and I didn't believe

624
00:37:11,640 --> 00:37:13,000
it and I finally was convinced of it.

625
00:37:13,000 --> 00:37:14,960
I was like, you guys had to have had some disagreement here.

626
00:37:14,960 --> 00:37:16,000
It seems so fundamental.

627
00:37:16,520 --> 00:37:19,240
But this is sort of what I mean, the governance is going worse.

628
00:37:20,120 --> 00:37:24,040
All the open AI people thought it was very important and Sam Altman himself

629
00:37:24,040 --> 00:37:27,440
talked about its importance all the time, that they had this non-profit board

630
00:37:27,440 --> 00:37:30,840
connected to this non-financial mission, right?

631
00:37:30,840 --> 00:37:34,320
The values of building AI that served humanity that could fire Sam

632
00:37:34,320 --> 00:37:37,560
Altman at any time or even shut down the company fundamentally

633
00:37:37,960 --> 00:37:40,880
if they thought it was going awry in some way or another.

634
00:37:41,480 --> 00:37:45,440
And the moment that board tried to do that, now I think they did not try

635
00:37:45,480 --> 00:37:46,760
to do that on very strong grounds.

636
00:37:46,760 --> 00:37:49,800
But the moment they tried to do that, it turned out they couldn't.

637
00:37:50,280 --> 00:37:55,000
That the company could fundamentally reconstitute itself at Microsoft

638
00:37:55,440 --> 00:37:59,080
or that the board itself couldn't withstand the pressure coming back.

639
00:37:59,560 --> 00:38:04,120
I think the argument from the board's side, the now mostly defunct board,

640
00:38:04,880 --> 00:38:09,040
is that this didn't go as badly for them as the press is reporting,

641
00:38:09,040 --> 00:38:13,680
that they brought in some other board members who are not cronies of Sam

642
00:38:13,680 --> 00:38:15,000
Altman and Greg Brockman.

643
00:38:15,600 --> 00:38:18,480
Sam Altman and Greg Brockman are not on the board now.

644
00:38:18,480 --> 00:38:20,200
There's going to be investigation into Altman.

645
00:38:20,600 --> 00:38:24,720
So maybe they have a stronger board that is better able to stand up to Altman.

646
00:38:24,760 --> 00:38:26,600
That is one argument I have heard.

647
00:38:27,000 --> 00:38:31,480
On the other hand, those stronger board members do not hold the views on AI

648
00:38:31,480 --> 00:38:36,160
safety that the board members who left like Helen Toner of Georgetown

649
00:38:36,160 --> 00:38:38,760
and Tasha Macaulay from Rand held.

650
00:38:39,280 --> 00:38:42,000
I mean, these are people who are going to be very interested in whether or not

651
00:38:42,000 --> 00:38:43,160
open is making money.

652
00:38:43,440 --> 00:38:46,640
I'm not saying they don't care about other things too, but these are people

653
00:38:46,640 --> 00:38:47,600
who know how to run companies.

654
00:38:47,600 --> 00:38:52,040
They serve on corporate boards in a normal way where like the output of the

655
00:38:52,040 --> 00:38:54,800
corporate board is supposed to be shareholder value and that's going to

656
00:38:54,800 --> 00:38:57,800
influence them even if they understand themselves to have a different mission here.

657
00:38:58,240 --> 00:39:00,080
Am I getting that story wrong to you?

658
00:39:00,360 --> 00:39:01,480
No, I think that's right.

659
00:39:01,480 --> 00:39:06,200
And it speaks to one of the most interesting and sort of strangest things

660
00:39:06,200 --> 00:39:11,560
about this whole industry is that the people who started these companies

661
00:39:12,040 --> 00:39:13,400
were weird.

662
00:39:13,400 --> 00:39:16,920
And I say that with no sort of like normative judgment, but they made

663
00:39:16,920 --> 00:39:18,400
very weird decisions.

664
00:39:18,600 --> 00:39:21,120
Like they thought AI was exciting and amazing.

665
00:39:21,120 --> 00:39:25,400
They wanted to build AGI, but they were also terrified of it to the point

666
00:39:25,400 --> 00:39:27,960
that they developed these elaborate safeguards.

667
00:39:28,120 --> 00:39:32,720
I mean, not in open AI's case, they put this nonprofit board in charge

668
00:39:32,720 --> 00:39:36,640
of the for-profit subsidiary and gave essentially the nonprofit board the

669
00:39:36,640 --> 00:39:40,320
power to push a button and shut down the whole thing if they wanted to.

670
00:39:40,600 --> 00:39:44,240
At Anthropic, one of these other AI companies, they are structured as a

671
00:39:44,240 --> 00:39:47,800
public benefit corporation and they have kind of this their own version

672
00:39:47,800 --> 00:39:51,840
of a nonprofit board that is capable of essentially pushing the big red

673
00:39:51,840 --> 00:39:54,800
shut it all down button if things get too crazy.

674
00:39:55,200 --> 00:39:59,360
This is not how Silicon Valley typically structures itself.

675
00:39:59,360 --> 00:40:03,320
Like Mark Zuckerberg was not in his Harvard dorm room building Facebook

676
00:40:03,520 --> 00:40:06,920
thinking like if this thing becomes the most powerful communication

677
00:40:06,920 --> 00:40:11,120
platform in the history of technology, like I will need to put in place these

678
00:40:11,120 --> 00:40:14,160
checks and balances to keep myself from becoming too powerful.

679
00:40:14,600 --> 00:40:17,200
But that was the kind of thing that the people who started open AI

680
00:40:17,200 --> 00:40:18,560
in Anthropic were thinking about.

681
00:40:18,560 --> 00:40:23,080
And so I think what we're seeing is that that kind of structure is sort

682
00:40:23,080 --> 00:40:27,920
of bowing to the requirements of shareholder capitalism, which says

683
00:40:27,920 --> 00:40:31,280
that, you know, if you do need all this money to run these companies to train

684
00:40:31,280 --> 00:40:35,080
these models, you are going to have to make some concessions to the sort

685
00:40:35,080 --> 00:40:38,080
of powers of the shareholder and of the money.

686
00:40:38,080 --> 00:40:41,320
And so I think that one of the big pieces of fallout from this open

687
00:40:41,320 --> 00:40:45,480
AI drama is just that open AI is going to be structured and run much more

688
00:40:45,480 --> 00:40:49,600
like a traditional tech company than this kind of holdover from this nonprofit board.

689
00:40:50,600 --> 00:40:51,920
And that is just a sad story.

690
00:40:52,080 --> 00:40:55,400
I truly wish that it had not worked out that way.

691
00:40:55,760 --> 00:40:58,360
I think one of the reasons why these companies were built in this way was

692
00:40:58,360 --> 00:41:00,320
because it just helped them attract better talent.

693
00:41:00,680 --> 00:41:04,760
I think that so many people working in AI are idealistic and civic

694
00:41:04,760 --> 00:41:07,040
minded and do not want to create harmful things.

695
00:41:07,040 --> 00:41:10,200
And they're also really optimistic about the power that good technology has.

696
00:41:10,520 --> 00:41:13,440
And so when those people say that as powerful and good as these things could

697
00:41:13,440 --> 00:41:16,160
be, it could also be really dangerous, I take them really seriously.

698
00:41:16,400 --> 00:41:17,600
And I want them to be empowered.

699
00:41:17,600 --> 00:41:19,240
I want them to be on company boards.

700
00:41:19,560 --> 00:41:23,880
And those folks have just lost so much ground over the past couple of weeks.

701
00:41:23,880 --> 00:41:27,520
And it is a truly tragic development, I think, in the development of this industry.

702
00:41:34,760 --> 00:41:55,640
One thing you could just say with that is, yeah, it was always going to be up to governments

703
00:41:55,640 --> 00:42:00,640
here, not up to strange nonprofit corporate, semi-corporate structures.

704
00:42:01,520 --> 00:42:06,640
And so we actually have seen a huge amount of government activity in recent weeks.

705
00:42:07,080 --> 00:42:08,840
And so I want to start here in the US.

706
00:42:09,120 --> 00:42:14,080
Biden announced a big package of a big executive order.

707
00:42:14,080 --> 00:42:15,120
You could call them regulations.

708
00:42:15,120 --> 00:42:16,800
I sort of call them pre-regulations.

709
00:42:17,120 --> 00:42:21,720
But Casey, how would you describe in some what they did?

710
00:42:21,720 --> 00:42:26,520
Like, what is a Biden administration's approach that it is signaling to regulating AI?

711
00:42:27,320 --> 00:42:35,160
The big headline was, if you are going to train a new model, so a sort of successor to a GPT-4,

712
00:42:35,480 --> 00:42:40,520
and it uses a certain amount of energy, and the energy there is just sort of a proxy for how

713
00:42:40,520 --> 00:42:44,840
powerful and capable this model might be, you have to tell the federal government that you have

714
00:42:44,840 --> 00:42:50,840
done this, and you have to inform them what safety testing you did on this model before

715
00:42:50,920 --> 00:42:52,600
releasing it to the public.

716
00:42:52,920 --> 00:42:58,200
So that is the one kind of break that they attempted to put on the development of this

717
00:42:58,200 --> 00:43:01,240
industry. It does not say you can't train these models.

718
00:43:01,240 --> 00:43:04,120
It doesn't specify what safety tests you have to do.

719
00:43:04,120 --> 00:43:08,920
It just says, if you're going to go down this road, you have to be in touch with us.

720
00:43:08,920 --> 00:43:13,640
And that will, I think, slightly decelerate the development of these models.

721
00:43:13,640 --> 00:43:19,880
I think critics would say it also pushes us a little bit away from a more open source version

722
00:43:19,880 --> 00:43:24,760
of AI, that open source development is sort of chaotic by its nature.

723
00:43:24,760 --> 00:43:28,840
And if you want to do some sort of giant open source project that would compete

724
00:43:28,840 --> 00:43:32,600
with the GPTs of the world, that would just sort of be harder to do.

725
00:43:32,600 --> 00:43:34,680
But to me, those are sort of the big takeaways.

726
00:43:35,320 --> 00:43:40,360
One of the things that struck me looking at the order was, go back a year, go back two years.

727
00:43:41,080 --> 00:43:45,800
I think the thing that people have said is that the government doesn't understand this at all.

728
00:43:46,600 --> 00:43:49,720
It can barely be conversant in technology.

729
00:43:49,720 --> 00:43:53,240
People remember Senator Orrin Hatch asking Mark Zuckerberg, well,

730
00:43:53,240 --> 00:43:55,480
if you're not making people pay, then how do you make money?

731
00:43:56,680 --> 00:44:02,520
When I read the order and looked at it, this actually struck me as pretty seriously engaged.

732
00:44:03,160 --> 00:44:07,720
Like, for instance, there's a big debate in the AI world about whether or not you're going to

733
00:44:07,720 --> 00:44:13,240
regulate based on the complexity and power of the model or the use of the model.

734
00:44:13,960 --> 00:44:18,040
You have a fear about what happens if you're using the model for medical decisions.

735
00:44:18,040 --> 00:44:20,200
But if you're just using it as your personal assistant, who cares?

736
00:44:20,760 --> 00:44:24,360
Whereas the AI safety people have the view that, no, the personal assistant model might

737
00:44:24,360 --> 00:44:27,720
actually be the really dangerous one because that's one that knows how to act in the real world.

738
00:44:28,360 --> 00:44:32,040
The Biden administration takes a view of the AI safety people.

739
00:44:32,040 --> 00:44:35,480
If you have a model over a certain level of computing complexity,

740
00:44:35,480 --> 00:44:39,080
they want this higher level of scrutiny, higher level of disclosure on it.

741
00:44:39,080 --> 00:44:42,520
They want everything that comes from an AI to be watermarked in some way,

742
00:44:42,520 --> 00:44:45,080
so you can see that it is AI generated.

743
00:44:45,960 --> 00:44:49,720
This struck me as a Biden administration actually clearly having taken this seriously

744
00:44:49,720 --> 00:44:54,040
and having convened some set of group of stakeholders and experts that knew what they

745
00:44:54,040 --> 00:44:57,800
were doing. I mean, I don't necessarily agree with literally every decision and a lot of it

746
00:44:57,800 --> 00:45:01,560
is just asking for reports. But when you think about it as a framework for regulation,

747
00:45:02,280 --> 00:45:06,360
it didn't read to me as a framework coming from people who had not thought about this for 10 minutes.

748
00:45:07,320 --> 00:45:11,400
Absolutely. I was quite impressed. You know, I had a chance to meet with Ben Buchanan at the

749
00:45:11,400 --> 00:45:15,480
White House who worked on this, talked to him about this stuff, and it is clear that they have been

750
00:45:15,480 --> 00:45:19,400
reading everything. They've been talking to as many people as they can, and they did arrive

751
00:45:19,400 --> 00:45:25,400
in a really nuanced place. And I think when you look at the reaction from the AI developers in

752
00:45:25,400 --> 00:45:29,640
general, it was mostly like neutral to lightly positive, right? There was not a lot of blowback,

753
00:45:29,640 --> 00:45:34,200
but at the same time, folks in civic society, I think, were also excited that the government did

754
00:45:34,200 --> 00:45:39,480
have a point of view here and had done its own work. Yeah, it struck me as a very deft set of...

755
00:45:40,440 --> 00:45:44,760
I think I would agree that they're more like pre-regulations than regulations. And to me,

756
00:45:44,760 --> 00:45:48,600
it sounded like what the Biden White House was trying to do was throw a few bones to everyone.

757
00:45:48,600 --> 00:45:53,000
What was like, we're going to throw a few bones to the AI safety community who worries about

758
00:45:53,000 --> 00:45:57,560
foundation models becoming too powerful. We're going to throw some bones to the AI

759
00:45:57,560 --> 00:46:02,280
harms community that is worried about things like bias and inaccuracy. And we're going to throw

760
00:46:02,280 --> 00:46:09,080
some bones to the people who worry about foreign use of AI. So I saw it as a very sort of deliberate

761
00:46:09,080 --> 00:46:13,720
attempt to give every sort of camp in this debate a little to feel happy about.

762
00:46:15,240 --> 00:46:23,560
One of the things it raised for me as a question, though, was, did it point to a world where you

763
00:46:23,560 --> 00:46:27,720
think that regulators are going to be empowered to actually act? This was the thing I was thinking

764
00:46:27,720 --> 00:46:33,800
about after the board collapse. You imagine a world sometime in the future where you have open AI,

765
00:46:34,280 --> 00:46:41,160
with GPT-6 or META or whomever, right? And they are releasing something that the regulator is

766
00:46:41,720 --> 00:46:47,160
looking at the safety data, looking at what's there. They're just itchy about it. It's not

767
00:46:47,160 --> 00:46:51,720
obviously going to do a ton of harm, but they're not convinced it's safe. They've seen some things

768
00:46:51,720 --> 00:47:00,120
that worry them. Are they really going to have the power to say, no, we don't think your safety

769
00:47:00,120 --> 00:47:04,440
testing was good enough. When this is a powerful company, when they won't be able to release a

770
00:47:04,440 --> 00:47:08,360
lot of the proprietary data, right? The thing where the board could not really explain why

771
00:47:08,360 --> 00:47:14,440
they were firing Sam Altman struck me as almost going to be the situation of virtually every regulator

772
00:47:14,440 --> 00:47:18,200
trying to think about the future harms of a model. If you're regulating in time to stop a thing from

773
00:47:18,200 --> 00:47:21,880
doing harm, it's going to be a judgment call. And if it's a judgment call, it's going to be a very

774
00:47:21,880 --> 00:47:26,760
hard one to make. And so if we ever got to the point where somebody needed to flip the switch

775
00:47:26,760 --> 00:47:33,800
and say, no, does anybody actually have the credibility to do it? Or is what we've seen that

776
00:47:34,360 --> 00:47:40,200
in fact, like these very lauded successful companies run by smart people who have huge

777
00:47:40,200 --> 00:47:44,600
Twitter followings or threads followings, whatever they end up being on, that they actually have

778
00:47:44,600 --> 00:47:48,920
so much public power that they'll always be able to make the case for themselves. And like the

779
00:47:48,920 --> 00:47:52,920
political economy of this is actually that we better just hope the AI companies get it right

780
00:47:53,560 --> 00:47:57,480
because nobody's really going to have the capability stand in front of them.

781
00:47:58,280 --> 00:48:02,680
When you talk to folks who are really worried about AI safety, they think that there is a high

782
00:48:02,680 --> 00:48:07,560
possibility that at some point in let's say the next five years, AI triggers some sort of event

783
00:48:07,560 --> 00:48:12,040
that kills multiple thousands of people. What that event could be, we could speculate, but

784
00:48:12,040 --> 00:48:16,120
assume that that is true. I think that changes the political debate a lot, right? Like that's just

785
00:48:16,120 --> 00:48:20,760
all of a sudden you start to see jets get scrambled. Hopefully that never happens, but I think that

786
00:48:21,240 --> 00:48:24,280
the inciting moment. And this is the thing that just frustrates me as somebody who writes about

787
00:48:24,280 --> 00:48:27,800
tech policy is we just live in a country that doesn't pass laws. There are endless hearings,

788
00:48:27,800 --> 00:48:31,320
endless debates, and then it gets time to regulate something. And it's like, well, yeah,

789
00:48:31,320 --> 00:48:35,800
they can regulate AI, but it's going to be based on this one regulation that was passed to deal with

790
00:48:35,800 --> 00:48:40,440
like the oat farming crisis of 1906. And we're just going to hope that it applies. It's like,

791
00:48:40,440 --> 00:48:43,320
we should pass new laws in this country. I don't know that there's a law that needs to be passed

792
00:48:43,320 --> 00:48:48,600
today to ensure that all of this goes well, but certainly Congress is going to need to do something

793
00:48:48,600 --> 00:48:53,160
at some point as the stuff evolves. I mean, one thing I was thinking about as this whole situation

794
00:48:53,160 --> 00:48:59,080
at OpenAI was playing out was actually the financial crisis in 2008 and the scenes that were

795
00:48:59,080 --> 00:49:04,760
captured in books and movies where you have the heads of all the investment banks and they're

796
00:49:04,760 --> 00:49:11,000
scrambling to avoid going under and they're meeting in these boardrooms with people like

797
00:49:11,000 --> 00:49:16,600
Ben Bernanke, the chair of the Federal Reserve, and the government actually had a critical role

798
00:49:16,600 --> 00:49:21,800
there in patching together the financial system because they were sort of interested,

799
00:49:21,800 --> 00:49:27,400
not in which banks survived and which failed, but in making sure that there was a banking system

800
00:49:27,400 --> 00:49:32,520
when the markets opened the next Monday. And so I think we just need a new regulatory framework

801
00:49:32,520 --> 00:49:38,360
that does have some kind of the sort of cliched word would be stakeholder, but someone who is

802
00:49:38,360 --> 00:49:42,920
in there as a representative of the government who's saying, what is the resolution to this

803
00:49:43,000 --> 00:49:46,680
conflict that makes sense for most Americans or most people around the world?

804
00:49:47,640 --> 00:49:52,840
When you looked at who the government gave power to in this document, when you think about who

805
00:49:52,840 --> 00:49:58,040
might play a role like that, when you need to call the government on AI, the way I read it is it

806
00:49:58,040 --> 00:50:03,480
spread power out across a lot of different agencies. And there were places where I invested

807
00:50:03,480 --> 00:50:08,760
more rather than less, but one thing that different people have called for that I didn't see it do,

808
00:50:08,760 --> 00:50:12,760
in part because you would actually need to pass a law to do this, was actually create

809
00:50:12,760 --> 00:50:19,480
the AI department, something that is funded and structured and built to do this exact thing,

810
00:50:19,480 --> 00:50:23,240
to be the central clearinghouse inside the government, to be led by somebody who would

811
00:50:23,240 --> 00:50:29,560
be the most credible on these issues. And would maybe have then the size and strength to do this

812
00:50:29,560 --> 00:50:34,200
kind of research, right? The thing that is in my head here, because I find your analogy really

813
00:50:34,200 --> 00:50:40,200
compelling, Kevin, is a federal reserve. The federal reserve is a big institution. And it has

814
00:50:40,760 --> 00:50:44,600
significant power of its own in terms of setting interest rates. It also does a huge amount of

815
00:50:44,600 --> 00:50:48,280
research. Like when you think about where would a public option for AI come from,

816
00:50:48,280 --> 00:50:51,640
you would need something like that that has the money to be doing its own research and

817
00:50:51,640 --> 00:50:56,600
hiring the really excellent people, in that case, economists, in this case, AI researchers.

818
00:50:57,160 --> 00:51:01,400
And there was nothing like that here. It was sort of an assertion that we more or less have

819
00:51:01,400 --> 00:51:05,640
the structure we need. We more or less have the laws we need. We can apply all those things

820
00:51:05,640 --> 00:51:11,320
creatively. But it did not say like, this is such a big deal that we need a new institution

821
00:51:11,960 --> 00:51:18,680
to be our point person on it. Yeah, I mean, I think that's correct. I think there are some

822
00:51:18,680 --> 00:51:26,440
reasons for that. But I think you do want a government that has its own technological capacity

823
00:51:26,440 --> 00:51:32,440
when it comes to AI, previous waves of innovation, certainly nuclear power during the Manhattan

824
00:51:32,440 --> 00:51:37,480
project, but also things like the internet came out of DARPA. These are areas where the government

825
00:51:37,480 --> 00:51:43,800
did have significant technical expertise and was building its own technology in sort of competition

826
00:51:43,800 --> 00:51:49,160
with the private sector. There is no public sector equivalent of chat GPT. The government has not

827
00:51:49,160 --> 00:51:54,680
built anything even remotely close to that. And I think it's worth asking why that is and what would

828
00:51:54,680 --> 00:52:00,040
need to happen for the government to have its own capacity, not just to evaluate and regulate

829
00:52:00,040 --> 00:52:06,040
these systems, but to actually build some of their own. I think it is genuinely strange on some level

830
00:52:06,840 --> 00:52:14,040
that given how important this is, there is not a bill gathering steam. Look, the private sector

831
00:52:14,040 --> 00:52:22,120
thinks it is worth pumping 50 or $100 billion into these companies so they can help you make

832
00:52:22,120 --> 00:52:29,000
better enterprise software. It seems weird to imagine that there are not public problems that

833
00:52:29,000 --> 00:52:36,200
have an economic value that is equal to that or significantly larger. And we may just not want

834
00:52:36,200 --> 00:52:41,800
to pay that money fine. But we do that for infrastructure. We just passed a gigantic

835
00:52:41,800 --> 00:52:47,960
infrastructure bill. And if we thought of AI like infrastructure, we actually also spend a lot of

836
00:52:47,960 --> 00:52:52,440
money on broadband now. It seems to me you want to think about it that way. And I think it is a

837
00:52:52,440 --> 00:52:59,480
kind of fecklessness and cowardice on the part of like the political culture that it no longer

838
00:52:59,480 --> 00:53:03,640
thinks itself capable of doing things like that. Like at the very least, and I've said this I think

839
00:53:03,640 --> 00:53:08,680
on your show probably, I think they should have prize systems where they say a bunch of things

840
00:53:08,680 --> 00:53:12,280
they want to see solved. And if you can build an AI system that will solve them, they'll give you a

841
00:53:12,280 --> 00:53:18,440
billion dollars. But the one thing is the government does not like to do things that spend money for

842
00:53:18,440 --> 00:53:24,120
an uncertain return. And building a giant AI system is spending a lot of money for an uncertain return.

843
00:53:24,920 --> 00:53:28,200
And so the only part of the government that is probably doing something like it is a defense

844
00:53:28,200 --> 00:53:32,200
department in areas that we don't know. And that does not make me feel better. That makes me feel

845
00:53:32,200 --> 00:53:37,400
worse. That's my take on that. Yeah, I mean, I think there's also a piece of this that has to do

846
00:53:37,400 --> 00:53:44,440
with labor and talent. You know, there are probably on the order of several thousand people in the

847
00:53:44,440 --> 00:53:53,080
world who can oversee the building, training, fine tuning deployment of large language models.

848
00:53:53,080 --> 00:53:59,560
It is a very specific skill set. And the people who have it can make gobs of money in the private

849
00:53:59,560 --> 00:54:06,040
sector working wherever they want to the numbers that you hear coming out of places like open AI

850
00:54:06,040 --> 00:54:12,360
for what engineers are being paid there. I mean, it's like NFL level football compensation packages

851
00:54:12,360 --> 00:54:18,120
for some of their people. And the government simply can't or won't pay that much money to

852
00:54:18,120 --> 00:54:22,040
someone to do equivalent work for the public sector. Now, I'm not saying they should be paying

853
00:54:22,040 --> 00:54:25,960
engineers millions of dollars of taxpayer money to build these things, but that's that's what you

854
00:54:25,960 --> 00:54:31,640
would need to do if you wanted to compete in an open market for the top AI talent. I am saying

855
00:54:31,640 --> 00:54:36,120
they should. I am saying this is like the factlessness and cowardice point. This is stupid.

856
00:54:36,120 --> 00:54:40,040
You think there should be AI engineers working for the federal government making five million

857
00:54:40,040 --> 00:54:44,680
dollars a year? Like maybe not five million dollars a year. But it would this is a this thing

858
00:54:44,680 --> 00:54:49,880
that we don't think civil servants should make as much as people in the private sector.

859
00:54:50,600 --> 00:54:54,200
Because I don't know somebody at a congressional hearing is going to stand and be like that person's

860
00:54:54,200 --> 00:55:01,880
making a lot of money. That is a way we rob the public of value. If Google's not wrong,

861
00:55:01,880 --> 00:55:08,440
Microsoft is not wrong that you can create things that are of social value through AI.

862
00:55:08,440 --> 00:55:12,840
And if you believe that, then leaving it to them, I mean, they intend to make a profit.

863
00:55:14,040 --> 00:55:18,360
Why shouldn't the public get great gains from this? It won't necessarily be through profit.

864
00:55:18,920 --> 00:55:23,000
But, you know, if we could cure different diseases or, you know, make big advances on energy,

865
00:55:23,000 --> 00:55:27,880
I just this way of thinking is actually to me the really significant problem. I'm not sure you

866
00:55:27,880 --> 00:55:31,320
would need to pay people as much as you're saying because I actually do think a lot of,

867
00:55:31,320 --> 00:55:35,400
I mean, we both know the culture of the AI people and at least up until a year or so ago.

868
00:55:36,280 --> 00:55:40,920
It was weird. And a lot of them would do weird things and are not living very lush lives.

869
00:55:41,480 --> 00:55:45,720
You know, they're in group houses with each other, taking psychedelics and working on AI

870
00:55:45,720 --> 00:55:51,480
on the weekdays. But I think you can get people in to do important work and you should.

871
00:55:51,480 --> 00:55:55,080
Now, look, you don't have the votes to do stuff like this. I think that's the real answer.

872
00:55:55,800 --> 00:56:01,880
But in other countries, they will and do. Like when Saudi Arabia decides that it needs an AI

873
00:56:02,680 --> 00:56:07,400
to be geostrategically competitive, it will take the money it makes from selling oil to the world.

874
00:56:08,040 --> 00:56:11,720
And in the same way that it's currently using that money to hire sports stars,

875
00:56:11,720 --> 00:56:16,520
it will hire AI engineers for a bazillion dollars and it will get some of them. And then it will

876
00:56:16,520 --> 00:56:22,360
have a decent AI system one day. I don't know why we're waiting on other people to do that. We're

877
00:56:22,360 --> 00:56:27,000
rich. It's stupid. I agree with you, Ezra. And I'm sorry that Kevin is so resistant to your ideas

878
00:56:27,000 --> 00:56:30,360
because I think paying public servants well would do a lot of good for this country.

879
00:56:30,920 --> 00:56:34,840
Look, I think public service should be paid well. I'm just saying when Jim Jordan

880
00:56:34,840 --> 00:56:41,960
gets up and grills the former deep mind engineer about why the Labor Department is paying them

881
00:56:41,960 --> 00:56:47,000
2.6 million dollars to fine tune language models, I'm not sure what the answer is going to be.

882
00:56:47,640 --> 00:56:51,080
No, I agree with you. I think we're all saying in a way the same thing. It's like,

883
00:56:51,080 --> 00:56:55,320
this is a problem. Government by dumb things Jim Jordan says is not going to be a great government

884
00:56:55,320 --> 00:57:01,240
that takes advantage of opportunities from the public. Good. And that sucks. It would be better

885
00:57:01,240 --> 00:57:07,640
if we were doing this differently and if we thought about it differently. Let me ask about China

886
00:57:08,520 --> 00:57:12,920
because China is where on the one hand, at least on paper, the regulations look much tougher.

887
00:57:13,720 --> 00:57:17,960
So one version is maybe the regulating AI much more strictly than we are. Another view that

888
00:57:17,960 --> 00:57:22,920
I've heard is that in fact, that's true for companies, but the Chinese government is making

889
00:57:22,920 --> 00:57:27,000
sure that it's building very, very strong. Do you know to the extent you all have looked at it,

890
00:57:27,000 --> 00:57:31,960
how do you understand the Chinese regulatory approach and how it differs from our own?

891
00:57:32,920 --> 00:57:38,520
I mean, I've looked at it mostly from the standpoint of what are the consumer facing systems

892
00:57:38,520 --> 00:57:45,000
look like. It has only been I think a couple of months since China approved the first consumer

893
00:57:45,000 --> 00:57:52,600
usable chat GPT equivalent. As you might imagine, they have very strict requirements as far as like

894
00:57:52,600 --> 00:57:57,720
what the chatbot can say about Tiananmen Square. So they wind up being more limited maybe than

895
00:57:57,720 --> 00:58:04,200
what you can use in the United States. As far as what is happening behind closed doors and for

896
00:58:04,200 --> 00:58:09,960
their defense systems and that sort of thing, I'm in the dark. So four or five years ago when I

897
00:58:09,960 --> 00:58:15,320
started reporting a book about AI, the conventional wisdom among AI researchers was that China was

898
00:58:15,320 --> 00:58:21,160
ahead and they were going to make all of the big breakthroughs and beat the U.S. technology

899
00:58:21,160 --> 00:58:26,360
companies when it came to AI. So it's been very surprising to me that in the past year since chat

900
00:58:26,360 --> 00:58:32,760
GPT has come out, we have not seen anything even sort of remotely close to that level of performance

901
00:58:32,760 --> 00:58:37,880
coming out of a Chinese company. Now, I do think they are working on this stuff, but it's been

902
00:58:37,880 --> 00:58:43,560
surprising to me that China has been mostly absent from the frontier AI conversation over the past

903
00:58:43,560 --> 00:58:50,200
year. And do you think those things are related? Do you think that the Chinese government's

904
00:58:50,920 --> 00:58:57,880
risk aversion and the underperformance of at least the products and systems we've seen

905
00:58:58,440 --> 00:59:02,680
in China? I mean, there might be things we don't know about. Do you think those things are connected?

906
00:59:03,240 --> 00:59:08,680
Absolutely. I think you do need a risk appetite to be able to build and govern these systems because

907
00:59:08,760 --> 00:59:14,760
they are unpredictable. We don't know exactly how they work. And what we saw, for example,

908
00:59:14,760 --> 00:59:22,280
with Microsoft was that they put out this Bing Sydney chatbot and it got a lot of attention and

909
00:59:22,280 --> 00:59:27,880
blowback and people reported all these crazy experiences. And in China, if something like that

910
00:59:27,880 --> 00:59:32,280
had happened, they might have shut the company down or they might have been deemed such an

911
00:59:32,280 --> 00:59:36,600
embarrassment that they would have radically scaled back the model. And instead, what Microsoft

912
00:59:36,600 --> 00:59:40,920
did was just say, we're going to make some changes to try to prevent that kind of thing from

913
00:59:40,920 --> 00:59:44,520
happening, but we're keeping this model out there. We're going to let the public use it and

914
00:59:44,520 --> 00:59:48,440
they'll probably discover other crazy things and that's just part of the learning process.

915
00:59:48,440 --> 00:59:53,400
That's something that I've been convinced of over the past year, talking with AI executives and

916
00:59:53,400 --> 00:59:59,400
people at these companies, is that you really do need some contact with the public before you start

917
00:59:59,400 --> 01:00:04,040
learning everything that these models are capable of and all the ways that they might misbehave.

918
01:00:05,000 --> 01:00:09,960
What is the European Union trying to do? They've had draft regulations that were

919
01:00:09,960 --> 01:00:14,360
seemed very expansive. What has been the difference in how they're trying to regulate this versus

920
01:00:14,360 --> 01:00:18,280
how we are and what in your understanding is the status of their effort?

921
01:00:19,160 --> 01:00:25,560
Europe was quite ahead with developing its AI Act, but it was written in a pre-chat GPT world.

922
01:00:25,560 --> 01:00:31,080
It was written in a pre-generative AI world. And so over the past year, they've been trying to

923
01:00:31,960 --> 01:00:40,040
retrofit it so that it reflects our new reality and is caught up in debate in the meantime.

924
01:00:40,040 --> 01:00:45,240
But my understanding is the AI Act is not particularly restrictive on what these companies

925
01:00:45,240 --> 01:00:48,360
can do. So to my understanding, there's nothing in the AI Act that is going to

926
01:00:48,360 --> 01:00:53,160
prevent these next generation technologies from being built. It's more about companies being

927
01:00:53,160 --> 01:00:58,840
transparent. Let me add a little bit of flavor to that because I was in Europe just recently

928
01:00:58,840 --> 01:01:04,280
talking with some lawmakers. And one of the things that people will say about the AI Act

929
01:01:04,280 --> 01:01:11,080
is that it has this risk-based framework where different AI products are evaluated and regulated

930
01:01:11,080 --> 01:01:16,680
based on these classifications of this is a low-risk system or this is a high-risk system

931
01:01:16,680 --> 01:01:22,040
or this is a medium-risk system. And so different rules apply based on which of those buckets

932
01:01:22,040 --> 01:01:27,240
a new tool falls into. And so right now, what a lot of regulators and politicians and companies

933
01:01:27,320 --> 01:01:31,720
and lobbyists in Europe are arguing about is what level of risk should something like a

934
01:01:31,720 --> 01:01:39,160
foundation model, a GPT-4, a Bard, a Claude, are those low-risk systems because they're just chat

935
01:01:39,160 --> 01:01:45,160
bots or are they high-risk systems because you can build so many other things once you have

936
01:01:45,160 --> 01:01:51,960
that basic technology? And so that's what my understanding is of the current battle in Europe

937
01:01:51,960 --> 01:01:56,440
is over whether foundation models, frontier models, whatever you want to call them,

938
01:01:56,440 --> 01:02:01,000
whether those should be assigned to one risk bucket or another.

939
01:02:01,640 --> 01:02:06,440
I think that's a good survey of the waterfront. And so I guess I'll end on this question, which is

940
01:02:06,440 --> 01:02:09,400
all right, we're talking here at the one-year anniversary roughly of chat GPT.

941
01:02:10,280 --> 01:02:14,200
If you were to guess, if we were having another conversation a year from now on the

942
01:02:14,200 --> 01:02:19,160
two-year anniversary, what do you think would have changed? What are one or two things each

943
01:02:19,160 --> 01:02:23,160
of you think is likely to happen over the next year that did not happen this year?

944
01:02:23,960 --> 01:02:31,560
I think all communication-based work will start to have an element of AI in it. All email, all

945
01:02:31,560 --> 01:02:36,680
presentations, office work essentially. AI will be built into all the applications

946
01:02:36,680 --> 01:02:40,120
that we use for that stuff. And so it'll just be sort of part of the background,

947
01:02:40,120 --> 01:02:43,400
just like autocomplete is today when you're typing something on your phone.

948
01:02:44,760 --> 01:02:51,080
I would say that AI is going to continue to hollow out the media industry. I think you're

949
01:02:51,080 --> 01:02:57,080
going to see more publishers turning to these really bad services that just automate the generation

950
01:02:57,080 --> 01:03:02,920
of copy. You'll see more sort of content farms springing up on the web. It'll reduce publisher

951
01:03:02,920 --> 01:03:08,440
revenue and we'll just see more digital media businesses either get sold or sort of quietly

952
01:03:08,440 --> 01:03:13,400
go out of business. And that's going to go hand in hand with the decline of the web in general.

953
01:03:13,400 --> 01:03:17,560
A year from now, more and more people are going to be using chat GPT and other tools

954
01:03:17,560 --> 01:03:22,520
as their kind of front door to internet knowledge. And that's just going to sap a lot of life out

955
01:03:22,520 --> 01:03:27,720
of the web as we know it. So we don't need one more technological breakthrough for any of that

956
01:03:27,720 --> 01:03:33,080
to happen. That's just a case of consumer preferences taking a while to change. And I

957
01:03:33,080 --> 01:03:36,920
think it's well underway. So do you think then that next year we're going to see something that

958
01:03:36,920 --> 01:03:41,880
has been long predicted, which is significant AI related job losses? Is that sort of the argument

959
01:03:41,880 --> 01:03:46,680
you're making here? I think that to some degree, it already happened this year in digital media.

960
01:03:46,680 --> 01:03:52,280
And yes, I do think it will start to pick up. Just keep in mind, 12 months is not a lot of

961
01:03:52,280 --> 01:03:58,440
time for every single industry to ask itself, could I get away with five or 10 or 15% fewer

962
01:03:58,440 --> 01:04:02,520
employees? And as the end of this year comes around, I have to believe that in lots and lots

963
01:04:02,520 --> 01:04:06,440
of industries, people are going to be asking that question. Yeah, I agree. I don't know whether

964
01:04:06,440 --> 01:04:11,080
that there will be sort of one year where all the jobs that are going to vanish, vanish. I think

965
01:04:11,080 --> 01:04:17,720
it's more likely to be a slow trickle over time. And it's less likely to be mass layoffs than just

966
01:04:17,720 --> 01:04:23,320
new entrants that can do the same work as incumbents with many fewer people. The software

967
01:04:23,320 --> 01:04:29,400
development firm that only needs five coders because they have all their coders are using AI

968
01:04:29,400 --> 01:04:34,920
and they have software that is sort of building itself competing with companies that have 10,000

969
01:04:34,920 --> 01:04:40,120
engineers and doing so much more capably. So I don't think it's going to necessarily look like

970
01:04:40,120 --> 01:04:45,080
all the layoffs hit on one day or in one quarter or even in one year, but I do think we're already

971
01:04:45,080 --> 01:04:49,880
seeing a displacement of jobs through AI. Those are kind of dark predictions. I mean,

972
01:04:49,880 --> 01:04:54,040
we'll have a little bit better sort of integration of AI into office tools and also we'll begin to

973
01:04:54,040 --> 01:05:00,200
see really the productivity improvements, create job losses. Is there anything that you think is

974
01:05:00,200 --> 01:05:05,480
coming down the pike technologically that would be really deeply to the good things that are

975
01:05:05,480 --> 01:05:09,400
not too far from fruition that you think will make life a lot better for people?

976
01:05:10,280 --> 01:05:17,160
I mean, I love the idea of universal translators. It's already pretty good using AI to speak in

977
01:05:17,160 --> 01:05:21,480
one language and get output in another, but I do think that's going to enable a lot of cross-cultural

978
01:05:21,480 --> 01:05:25,240
communications and there are a lot of products remaining to be built that will essentially

979
01:05:25,240 --> 01:05:31,240
just drop the latency so that you can talk and hear in real time and have it be quite good.

980
01:05:31,240 --> 01:05:32,360
So that's something that makes me happy.

981
01:05:33,000 --> 01:05:37,240
And I'm hopeful that we will use AI, not we as in me and Casey.

982
01:05:37,240 --> 01:05:37,800
But we might.

983
01:05:39,000 --> 01:05:44,680
This would be sort of a career change for us, but we as in society, I have some hope that we will

984
01:05:44,680 --> 01:05:53,000
use AI to cure one of the sort of top deadliest diseases, cancer, heart disease, Alzheimer's,

985
01:05:53,000 --> 01:05:57,640
things like that that really affect massive numbers of people. I don't have any inside

986
01:05:57,720 --> 01:06:02,200
reporting that we are on the cusp of a breakthrough, but I know that a lot of energy and research and

987
01:06:02,200 --> 01:06:07,800
funding is going into using AI to discover new drugs and therapies for some of the leading

988
01:06:07,800 --> 01:06:12,600
killer diseases and conditions in the world. And so when I want to feel more optimistic,

989
01:06:12,600 --> 01:06:16,840
I just think about the possibility that all of that bears fruit sometime in the next few years,

990
01:06:16,840 --> 01:06:17,800
and that's pretty exciting.

991
01:06:18,520 --> 01:06:21,800
All right. And then also final question, what are a few books you'd each recommend to the

992
01:06:21,800 --> 01:06:25,720
audience released recommend the audience ask chat GPT to summarize for them?

993
01:06:26,280 --> 01:06:28,760
Kevin, you want to go first?

994
01:06:28,760 --> 01:06:34,120
Sure. I actually have two books in a YouTube video. The two books, one of them is called

995
01:06:34,120 --> 01:06:41,160
electrifying America by David E. Nye. It is a 30 year old history book about the process by which

996
01:06:41,160 --> 01:06:46,600
America got electricity. And it has been very interesting to read. I read it first a few years

997
01:06:46,600 --> 01:06:51,320
ago and have been rereading it just to sort of sketch out what would it look like if AI really

998
01:06:51,320 --> 01:06:56,280
is the new electricity? What happened the last time society was transformed by technology like

999
01:06:56,280 --> 01:07:01,640
this? The other book I'll recommend is your face belongs to us by my colleague, our colleague at

1000
01:07:01,640 --> 01:07:07,080
the Times, Cashmere Hill, which is about the facial recognition AI company, Clearview AI,

1001
01:07:07,080 --> 01:07:12,920
and is one of the most compelling tech books I've read in a few years. And then the YouTube

1002
01:07:12,920 --> 01:07:18,120
video I'll recommend was just posted a few days ago. It's called Intro to Large Language Models.

1003
01:07:18,200 --> 01:07:25,640
It's made by Andre Karpathy, who is an AI researcher actually at Open AI. And it's his one hour

1004
01:07:25,640 --> 01:07:29,720
introduction to what is a large language model and how does it work? And I've just found it

1005
01:07:29,720 --> 01:07:35,320
very helpful for my own understanding. Casey? Well, as we're with permission, and given that

1006
01:07:35,320 --> 01:07:39,320
Kevin has just given your listeners two great books and a YouTube video to read, I would actually

1007
01:07:39,320 --> 01:07:43,400
like to recommend three newsletters if I could. And the reason is because the books that were

1008
01:07:43,400 --> 01:07:47,960
published this year did not help me really understand the future of the AI industry. And to

1009
01:07:47,960 --> 01:07:51,720
understand what's happening in real time, I really am leaning on newsletters more than I'm

1010
01:07:51,720 --> 01:07:55,880
leaning on books. So is that okay? Yeah, go for it. All right. So the first one,

1011
01:07:55,880 --> 01:08:00,120
cruelly, you already mentioned earlier in this podcast, it's import AI from Jack Clark. Jack

1012
01:08:00,120 --> 01:08:05,480
co-founded Amthropic, one of the big AI developers. And it is fascinating to know which papers he's

1013
01:08:05,480 --> 01:08:09,080
reading every week that are helping him understand this world. And I think that they're arguably

1014
01:08:09,080 --> 01:08:13,400
having an effect on how Amthropic is being created because he is sitting in all of those rooms. So

1015
01:08:13,400 --> 01:08:18,360
that is just an incredible weekly read. I would also recommend AI Snake Oil from the Princeton

1016
01:08:18,360 --> 01:08:24,200
Professor, Arvind Narayanan and a PhD student at Princeton, Syash Kapoor. They're very skeptical

1017
01:08:24,200 --> 01:08:28,280
of AI hype and doomsday scenarios, but they also take the technology really seriously and have

1018
01:08:28,280 --> 01:08:32,040
a lot of smart thoughts about policy and regulation. And then the final one is Pragmatic

1019
01:08:32,040 --> 01:08:37,320
Engineer by this guy, Gurgely Arose. He's this former Uber engineering manager. And he writes

1020
01:08:37,320 --> 01:08:41,560
about a lot of companies, but he writes about them as workplaces. And I love when he writes about

1021
01:08:41,560 --> 01:08:45,800
open AI as a workplace. He interviews people there about culture and management and process. And he

1022
01:08:45,800 --> 01:08:49,240
just constantly reminds you, they're just human beings showing up to the office every day and

1023
01:08:49,240 --> 01:08:53,080
building this stuff. And it's just a really unique viewpoint on that world. So read those

1024
01:08:53,080 --> 01:08:56,280
three newsletters. You'll have a little better sense of what's coming for us in the future.

1025
01:08:56,280 --> 01:09:00,600
What Casey didn't say is that he actually hasn't read a book in 10 years. So it was a bit of a

1026
01:09:00,600 --> 01:09:04,680
trick question. You know what I will say? I did read Your Face Belongs to Us by Cashion,

1027
01:09:04,680 --> 01:09:08,440
incredible book. Definitely read that one. Sure you did. There you go. Casey Newton,

1028
01:09:08,440 --> 01:09:12,200
Kevin Ruse at your podcast, which requires very little reading. It's hard for it.

1029
01:09:12,920 --> 01:09:16,120
Thank you all for being on the show. It's the first illiterate podcast, actually,

1030
01:09:16,120 --> 01:09:24,360
put out by the New York Times. Thank you for having us. Thanks, Ezra. Thanks, guys.

1031
01:09:24,360 --> 01:09:39,800
This episode of the Ezra Klein Show is produced by Roland Hoof, fact-checking by Michelle Harris

1032
01:09:39,800 --> 01:09:44,200
with Kate St. Clair and Mary Marge Locker. Our senior engineer is Jeff Geld. Our senior editor

1033
01:09:44,200 --> 01:09:48,520
is Claire Gordon. The show's production team also includes Emma Vagabou and Kristen Lin,

1034
01:09:48,520 --> 01:09:53,480
original music by Isaac Jones, audience strategy by Christina Samaluski, and Shannon Busta.

1035
01:09:53,480 --> 01:09:57,480
The executive producer of New York Times' opinion audio is Anero Strasser. And special

1036
01:09:57,480 --> 01:10:07,480
thanks to Sonia Herrero.

