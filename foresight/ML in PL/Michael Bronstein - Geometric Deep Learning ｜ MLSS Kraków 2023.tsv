start	end	text
0	2400	Thank you very much.
2400	3840	So great pleasure to be here.
3840	9840	It's actually my second time in Krakow, and I think it's a very beautiful choice for
9840	12320	having a machine learning summer school.
12320	17320	So in the next three hours, I would like to talk about geometric deep learning.
17320	25920	And if you wonder this intriguing image, what does it have to do with machine learning?
25920	29960	So you see it fits more, a kind of an alchemist.
29960	38680	So we wanted to use this image in reference to this famous quote from Ali Rahimi who was
38680	44040	receiving the Best Paper Award or the Proof of Time Award at New Europe in 2017.
44040	48040	And he was speaking this way maybe a little bit critically about deep learning, at least
48040	55520	at that time, that we say things like machine learning is the new electricity, and he's
55520	60880	alternative metaphor was machine learning has become alchemy, so in the sense that some
60880	65560	kind of science that maybe produces something that we don't really understand what it does.
65560	71840	And really what we would like to do here is to try to understand from certain perspective
71840	76440	how these methods work and why they work, and maybe more importantly, when they fail
76440	82440	and you see kind of maybe general blueprint for developing potentially future machine
82440	84040	learning systems.
84040	90800	So the concept that will be important to these lectures is the concept of symmetry.
90800	97800	And symmetry according to Vile, who I'm quoting here is, depending on how wide or narrow you
97800	103600	define its meaning, is an idea by which men through the ages has tried to comprehend and
103600	105800	create order beauty and perfection.
105800	111280	So it sounds a bit poetic, but I think it is true, so that's from his book that is titled
111280	114840	Symmetry, that he published in Princeton.
114840	119760	And symmetry is a Greek word, so it goes back to the ancient Greeks, and as you probably
119760	125000	know, ancient Greeks like Plato considered symmetry to be really the cornerstone of the
125000	126000	universe.
126000	132040	So according to Plato, what is nowadays called platonic solids, symmetric polyhedra were the
132040	137320	basic building blocks of all the stuff in the universe, so probably tiny little things
137320	143040	that build up the matter, and if you think of it in modern terminology, that's not very
143040	144800	far from truth.
144800	154440	So Plato believed that geometry is really the key piece of mathematics, and even according
154440	159400	to a legend, there was an inscription on the entrance to his academy saying that nobody
159400	164880	skilled in geometry, that is not skilled in geometry, should be allowed to enter.
164880	169520	And this idea of matter being built of small symmetric polyhedra, actually not far from
169520	174840	truth, if you consider how crystals are organized, and the first to study this from a formal
174840	181960	perspective was actually Kepler, who is maybe more famous for his discovery of the motion
181960	187360	of planets, but he was also the one who laid the foundations of modern crystallography,
187360	191440	basically considering how spheres can be packed into different configurations, and if you also
191440	197480	think that this is old and boring and outdated stuff, so last year's Fields Medal was given
197480	202680	exactly for solving these kind of problems maybe in higher dimensions.
202680	209240	So geometry itself also, at least in formal way, goes back to ancient Greeks, and what
209240	215240	we still often study at school as the geometry dates back to Euclid, his famous elements,
215240	221520	so a system of axioms from which his geometry was derived, and as you know there are five
221520	226880	axioms or five postulates of Euclidean geometry, that the last one was somehow standing out
226880	233640	and people for many centuries or even thousands of years tried to do something with it, and
233640	241200	for example in the 18th century Giovanni Saccheri, who was a priest, almost arrived to the construction
241200	248120	of a non-Euclidean geometry, but he considered it such a heretical idea that he thought that
248120	253400	it is repugnant to the nature of straight lines, so he abandoned these ideas and never
253400	259040	took it to the full extent, but in the 19th century what happened is that finally came
259040	265560	the realization that dethroned Euclid and broke his monopoly of geometry and the works
265560	271560	that probably gauss himself did, but never published, and then famously Lobochevsky,
271560	278240	Boje and Riemann, who created the first examples of what we now call non-Euclidean geometries,
278240	283760	and in the 19th century this is how the geometry started looking like, so a zoo of different
283760	289840	types of geometry without clear relation or understanding what actually defines the geometry,
289840	295280	so there was obviously a need to put order in this mess, and the new idea, new approach
295280	303840	came from Felix Klein in 1872, so he was only 23 years old, he was lucky to get an appointment
303840	307440	as a professor at the University of Erlangen in Bavaria, so this is something that for
307440	315000	example Euler failed to have in Switzerland, he had to go to Russia to become a faculty,
315000	320800	probably not very dissimilar to situations that some of us are facing in these days,
320800	327640	so Klein was asked, as it was customary in Germany and still customary I think, to deliver
327640	331760	a research prospectus, basically to explain what he is going to do until his retirement
331760	336000	in Erlangen, which actually never happened because he moved three years after to different
336000	342720	places eventually to GÃ¶ttingen, and in this prospectus that entered the history of mathematics
342720	348080	as the Erlangen program, he proposed a kind of algebraization of geometry, so studying
348080	352720	geometry from the perspective of group theory, so essentially considering a geometry as a
352720	358360	space plus some class of transformations formalized using group theory, and studying properties
358360	364480	that remain unchanged or invariant under these transformations, so you take an object and
364480	371880	you apply to it rigid motions and you preserve a lot of things like areas, parallel lines,
371880	376400	distances, so this is how you create Euclidean geometry, but you can consider other groups
376400	381920	like a fine or projective group, and in fact he considered projective group to be the broadest
381920	387560	construction, he in fact showed together with Biltrami that the first non-Euclidean geometry
387560	394160	is hyperbolic, geometry is with negative curvature, could be constructed with a projective model,
394160	401360	and these ideas had really big impact on geometry, on mathematics more broadly, I would say culturally,
401360	407840	like category theory is an extension of these ideas to more abstract objects, but probably
407840	411800	most importantly it had an impact on physics where came the realization in the beginning
411800	417600	of the 19th century, probably starting with Neutra with her famous theorem that the laws
417600	423880	of physics themselves can be derived from considerations of invariance or symmetry, and for example
423880	429240	what Neutra showed is that principles like conservation of energy that previously were
429240	433360	considered to be empirical could be derived mathematically from certain symmetries, so
433360	439800	symmetry of time in this case, and these ideas in a more generalist form led to what nowadays
439800	447080	is known as the standard model, so basically all the world can be modeled and can be derived
447080	452960	from first principles of symmetry, so what I think physicists among you would call external
452960	457480	symmetry or internal symmetry, the symmetry of the spacetime, so what is called the Poincare
457560	462160	group that gives rise to Minkowski geometry of special relativity or internal symmetries
462160	467680	of quantum fields, that's what gives rise to different forces or different interactions,
467680	473040	and I think nobody put it better than Philip Anderson Nobel laureate in physics that without
473040	480880	or with only slightly overstating, you can say that physics is the study of symmetry,
480880	486840	so what does it all have to do with deep learning and neural networks and machine learning
486840	494680	in general, so let's do maybe a brief detour into the history of machine learning or artificial
494680	500720	intelligence, so the term artificial intelligence comes from these people, the Dartmouth conference
500720	508360	that happened in 1956, organized by McCarthy and others, and you can see them, some very
508360	512520	prominent figures sitting here, so this is for example, this is Claude Shannon and this
513400	519240	is Marvin Minsky who would become all very important scientists, and historically apparently
519240	524280	the term artificial intelligence was introduced to kind of distance themselves and not to be
524280	530440	in the shadow of the expert of that time who was Norbert Wiener who introduced the term cybernetics,
531000	535880	which I think is still used, I think here in Poland it's probably still used as a kind of
535880	540840	overarching term for everything that it has to do with computer science and artificial intelligence,
541480	545800	so around that time there were many works that tried to understand how the brain works,
545800	550760	so I think at that time it was already understood that somehow our intelligence is concentrated
550760	557720	in the brain, so models for neural networks, probably the most famous one is by Frank Rosenblatt,
557720	565400	the so-called perceptron, but there are models before that, and he was able to show that was in
565400	571960	the 50s, so these models had to be implemented in analog hardware, he was able to show that he
571960	577800	can solve some simple pattern recognition problems with these neural networks, and it was extremely
577800	582680	remarkable at that time, so there were articles in the popular press like the New Yorker saying
582680	588360	that this is the first serious rival to the human brain ever devised, so I think you can only smile,
588360	593480	I can hear you laughing, and it's remarkable machines capable of what amounts to thought,
593480	598600	so that's according to New Yorker was the perceptron, and there was a little bit of
598600	605560	hype, as you probably know, this MIT computer vision summer project, so they thought that they
605560	610520	would be able to model a large part of the visual system over one summer, of course, we're still
610520	617160	working on these problems 50 years after, and there is no end to it, but also at MIT these two
617160	622040	guys, so one of them appeared already in the picture, Marvin Minsky in the same work, they
622040	627400	published a highly famous or infamous, if you want, book called The Perceptrons, where they
628280	635640	introduced mathematical analysis of these networks proposed by Rosenblatt, and they showed, for
635640	640520	example, one example that is taken from this book, that a very simple logical function like
640520	646760	exclusive or could not actually be implemented as a perceptron, so these patterns are not
647400	653800	linearly separable, so that was a very harsh criticism for the models, and some people say
654360	659560	in retrospective that this was what triggered the AI winter, so some people even say that there might
659560	666440	have been some personal animosity between Rosenblatt and Minsky, they went to the same high school,
666440	674200	and Rosenblatt also died in a boating accident, so as far as to suggest that it was a suicide,
674920	684200	well, the story is probably much more mundane, so the funding that was cut by government agencies
684200	689240	like DARPA was more related to them being more pragmatic and budget restricted, and that had an
689240	694200	impact on the field that coincided with the publication of the book, but if you look at the
696040	701160	substance of the problem, what Minsky and Pebert called in their book Perceptron was actually
701160	707000	not exactly an architecture that was devised by Rosenblatt, so they actually clearly stated, so
707000	712120	they refer to this architecture as simple perceptrons, and this is what we now typically
712120	717480	understand by this term, so it's, as you know, it's just a linear combination of the input
717480	722680	coordinates with learnable weights that go through a nonlinear activation, typically sigmoid or
724360	729080	in simple cases sine function, but what is also very important, they were probably the first
729080	734520	ones, at least to my knowledge, to use geometric approaches to machine learning problems, so
734520	740280	they, for example, formulated this group invariance theorem that tell in which or to what kind of
740280	745400	transformations in the input patterns the neural network will be invariant, and another interesting
745400	749720	thing actually the subtitle of the book is an introduction to computational geometry, so that
749720	754680	was the sixties, so this term didn't exist, they actually introduced it, and one of the reviews
754680	760600	of the book that was critical was asking whether this is just some kind of new mathematical fad
760600	766200	that will go away a few years after, and you probably know computational geometry is now
766200	772200	very well established field, so it has remained there for a long time, but if you go into the
772200	777480	substance of the discussion is actually the question is what kind of classes of functions
777480	781800	these neural networks could represent, right, so what is what is called the expressive power,
782360	787560	and there were results at that time, so coming from mathematicians in particular
788680	794920	Komogorov and Arnold working in the Soviet Union that claim that you can take multi-dimensional
794920	799640	functions and decompose them in this way, and basically any function could be decomposed in
799640	805640	this way, so these kind of results are known as universal approximation, they go probably as far
805720	811320	as the thirteenth problem formulated by David Hilbert, and the modern statements of these
811880	817480	theorems are usually attributed to Seybenko and Hornig, these are late eighties, early nineties
817480	823880	that are specific to deep neural networks, so what these results say is that if you have not a
823880	828680	single layer of perception, but two layers like this, then you can approximate any continuous
828680	834760	function to any desired accuracy, so the results it's a class of results, it's not a single result,
835240	840520	but roughly the way that we can understand it is that with just a configuration like this,
840520	845480	with just two neurons like this, you can approximate, you can represent a step function,
846120	849880	once you can represent step functions, you can decompose a continuous function into
849880	854600	tiny little steps, so the proof is slightly more involved, but that's that's roughly the idea.
855880	861400	Now it's sort of constructive proof, and there are different versions for limited widths or
861400	866520	limited depths, it's sort of constructive proof, so it doesn't tell you how, so it's an existence
866520	871560	result, so it tells you that you can find a neural network with potentially a very large
871560	876840	number of neurons that approximate functions, and if you look at machine learning problems,
876840	881320	so in a very maybe simple and naive setting, like classifying images of cats and dogs,
882760	887400	basically you can think of it as some cynicism say that deep learning is a curve fitting problem,
887480	892440	so it's multi-dimensional curve fitting, so there is some kind of black box where you put
894360	899160	some something that acts as a universal approximator, so some sufficiently rich
899800	905880	architecture, and you try to represent the function that distinguishes between cats and dogs
905880	911160	in this way. Of course this is not a well-defined problem, if I give you a finite sample of,
911160	916920	let's say these are cats and dogs, I can pass infinitely many functions through these points,
916920	921960	so I can interpolate these points in infinitely many ways, so we need somehow to restrict our
921960	927320	class of functions, and that's typically what you do by imposing some sort of regularity,
927320	932760	and mathematicians have very well understood concepts of regularity like Lipschitz continuity,
932760	939320	right, so in simple case you can think of it as a function with bounded derivatives, right,
939320	944520	and the problem is what happens when you increase the dimensionality of your input when it doesn't
944520	948920	look like a one-dimensional curve, but it's an n-dimensional curve, and here the results,
948920	955880	unfortunately, are not favorable because you can show that as you grow the number of dimensions,
955880	962520	the number of samples that you need to take in order to approximate a function to accuracy epsilon
962520	967160	grows exponentially with a number of dimensions, so this is again, it's not a single result,
967160	973560	it's a class of phenomena that are called the curse of dimensionality, so it's a statistical or
973560	979560	geometric phenomena that explains how functions behave in high dimensions, and the term itself
979560	986840	actually goes back to Bellman who spoke about the curse of dimensionality in physical problems.
987720	993320	Now, in these problems, simply if you think of images, right, of even something like 30 by 30
993320	998760	pixels, which is probably the smallest image you can imagine, digits from the MNIST dataset,
998760	1003160	the number of dimensions will be approximately thousands, so if you count the number of samples
1003240	1007640	that you need to take, if you took this kind of straightforward approach, it will probably be
1007640	1011880	more than the number of, not only the cats or dogs on earth, but probably close to the number of
1011880	1017560	particles in the universe, so there are not sufficiently many animals around just to do it,
1018120	1024920	and this kind of problem of curse of dimensionality, right, or you can also call it combinatorial
1024920	1030280	explosion, was brought up in another report that is associated with the AI winter, which is the
1030280	1036360	Lighthill report in the 70s. It was commissioned by the analogy of the DARPA, or basically the
1036360	1043480	British funding agencies, and that was the point of time where they decided to stop funding these
1043480	1050760	crazy ideas in looking at neural networks. So what happened, of course, was this AI winter,
1050760	1056120	but at the same time people continued working on these architectures, and some interesting ideas
1056120	1060760	came from the field of neuroscience, from the study of the organization and the function of
1060760	1067240	the visual cortex, the famous experiments in the 50s and the 60s done by Hubel and Wiesel,
1067240	1072120	a duo from Harvard that won the Nobel Prize in medicine for understanding the organization
1072120	1078520	of the visual cortex, where what they found is that the cells in the cortex were organized
1078520	1085080	with some local shared weights, and this was reproduced by Fukushima in his famous neocognitron
1085080	1092520	paper in 1980. So the idea here was a neural network that does, it has two types of neurons,
1092520	1100120	so neurons that he called simple neurons and neurons that he called complex neurons, so one
1100120	1105240	in modern terminology that would correspond to local filters and pooling operations.
1106120	1111880	And he worked on OCR type problems, so character recognition, and the problem,
1111880	1118600	of course, if you treat this type of problems with the standard perceptrons, if I give you a
1118600	1124040	digit of digit three, and I move it just by one pixel, you see that the input into the neural
1124040	1129880	network can change dramatically, and in fact he complained that perceptrons were not by design
1129880	1135080	invariance to these translations. So his architecture actually is remarkably modern
1135080	1140840	by modern standards, so it was seven layer networks, so I think we can call it deep by
1140840	1145720	modern standards. It had local connectivity, what neuroscientists called receptive fields.
1146920	1152840	The filters were non-linear, and he wrote in neuroscience terminologies, so he talked about
1152840	1158920	inhibition and activation. He had average pooling, so these are the complex layers.
1160200	1167160	He used reloactivation, so already in the late 60s that was common, but the training was not done
1167160	1171640	using backpropagation, so it was a kind of unsupervised type of clustering approach.
1171640	1177000	And backpropagation, again, it existed already in maybe some other forms, but this is how neural
1177000	1181720	networks were trained. So Rosenblatt had a special rule for a single layer perceptron,
1181720	1186200	then there were some other methods that were developed in the 60s, and then backprop became
1186200	1190920	really popular in the 80s, starting from the paper of Rumelhardt.
1191160	1198440	So Lecante was just a fresh graduate at that time, and he was working on, actually,
1198440	1205080	the application of backpropagation neural networks, was interested in Neocognitron,
1205080	1210520	and he also happened to work at AT&T, which they were developing the first digital signal
1210520	1215720	processors at that time. So basically, there was a good application to implement on a DSP,
1215720	1222920	and basically, he stripped down the kind of neuroscience terminology of Fukushima,
1222920	1227400	replaced nonlinear filters by linear filters that could be implemented as convolutions.
1227400	1232680	Actually, the first paper never mentioned the term convolution, and the name came after in,
1232680	1241400	I think, in 89, or even later, and he was able to show in real time complex pattern recognition
1241400	1245800	tasks, such as recognition of handwritten digit, which was a difficult task at that time.
1245800	1249880	It was actually deployed in commercial applications. They were working with banks
1250920	1255800	in the post office, and that's where the MNIST dataset comes from. But the computer vision
1255800	1264520	community took a different path, and the late 90s and the early 2000s, probably until the first
1264520	1271000	decade of this century, the approaches that you would typically use for image recognition
1271000	1278360	were to detect some local features, then compute local feature descriptors, and then create some
1278360	1283000	representation that would be passed into some very simple classifier, like a support vector
1283000	1287080	machine, which also were considered to be more favorable by mathematicians, because
1287080	1292600	you can prove, for example, global optimality results about them. So there are many papers
1292600	1297800	written, so SIFT, the scale environment feature transform, one of the most cited papers in computer
1297800	1306200	science ever, was extremely well engineered detector and feature descriptor for these tasks.
1307240	1313000	And what happened in 2012, as you probably all know, that all these carefully designed handcrafted
1313000	1320920	features were beaten down by a very large margin by a convolutional neural network. So it was this
1320920	1327080	lack of confidence of the computational capabilities of hardware, the GPUs, as well as availability
1327080	1332520	of large data, the image net benchmark, which contained millions of annotated images, that
1332520	1338120	finally allowed these architectures to shine. And since then, all the best results in this
1338120	1344760	benchmark were by deep learning. So if you look at the AlexNet architecture that won this benchmark
1344760	1350200	in 2012, it is more or less the same as what was done by Lekana. It's just slightly deeper. There are
1350200	1356920	some different architectural choices. It has way more parameters. It was trained on GPUs, which by
1356920	1362840	the way was not novel. GPUs were used for general purpose computing at least a decade before and
1362840	1369160	for training neural networks probably at least seven years before. So in a sense, it was a very
1369160	1375800	careful engineering and application of existing ideas on a very large dataset and a very important
1375800	1383400	problem that convinced everyone. Yeah, there is a question. Sorry to go back a couple of slides,
1383400	1391560	but you were mentioning Fukushima's implementation and no sort of back propagation learning. So
1391560	1400760	like was he giving some kind of neuroscience, how do you say, a proof for this? Like some kind of
1400760	1407400	happy learning or how did the... I don't remember what kind of rule he used. I think it was inspired
1407400	1414120	by some hypothetical ideas of how the brain learns. Yeah, but it was not back propagation.
1414120	1418440	He showed many other things. So he showed, for example, geometric stability, stability to noise,
1418440	1425320	but again, it was in the early 80s. So the training examples were very rudimentary. So
1425320	1433640	black and white letters. Thank you. Right. So basically, all the rest is obviously history,
1433640	1440040	right? So the people who started the deep learning thing got the Turing Award and now
1440040	1446040	very famous. And basically, this is technology that has really transformed the field both
1446760	1452680	the academical subjects and the industry. So just to give you, we'll be talking about
1452680	1456200	graph neural networks and you've probably heard from Miguel as well in the previous lectures
1456760	1460920	about graph neural networks. So just to give you a little bit of history of this one,
1461560	1467800	so they're actually very much related and rooted in chemistry. And chemistry is probably one of
1467800	1474280	the fields of science, which is data intensive. It produces a huge amount of data, experimental data.
1474280	1479320	And since early times, chemists tried to organize these data first, publishing these
1479320	1484520	humongous books containing information about molecules or chemical reactions. And then
1485320	1492600	it appeared the first digitized archive, the chemical abstract service. And also with the
1492600	1497560	appearance of the first computers came the need and the idea to search for molecules, right? So
1497560	1502760	if, for example, you're a pharmaceutical or a chemical company and you want to patent a new
1502760	1508440	molecule, how do you know that it has not already been described, right? So you need to search fast
1508440	1514520	for molecular structures in some data set. And these were the first ideas of chemical ciphers
1514520	1520040	that describe a molecule as a string and then try to match it in some data set. So that was the kind
1520040	1528040	of problems that these guys, or he was a Romanian chemist called Vladus, that was one of the pioneers
1528040	1534360	of a field that later became known as chemoinformatics, he was trying to look at molecules as graphs.
1535000	1539000	And as you probably know, the term graph itself, in the sense of graph theory,
1539000	1544280	also is associated with chemistry. This is how Sylvester called the first attempts to design
1544280	1549640	structural molecules, structural formula of molecules, basically trying to understand how
1549640	1555320	atoms are related to each other with their chemical bonds, not only just the number of atoms
1555320	1560040	and the types of atoms. And as you probably know, this was one of the first structural formula of
1560040	1567320	benzene that was derived by Kecoli. And the legend says that he dreamt of a snake biting
1568200	1574840	his own tail, so he came up with this kind of aromatic ring that is described here. So
1576120	1580120	these kind of problems inspired these duo of mathematicians, we'll talk about them later,
1580760	1586920	Weisfried and Lehmann in the 60s to devise an algorithm that would test whether two graphs
1587000	1592840	are structurally similar, what is called the graph isomorphism test. And these ideas maybe were
1593800	1599480	noticed, so there are many related works in the machine learning community and also in
1599480	1605880	a chemical community, trying to devise new types of neural networks that could take as input,
1605880	1612840	not vectors, not images, but graph structured data. And the early works by Alessandro Sperduti
1612840	1618280	in the 90s, for some reason most of the works were by Italians, and probably the most cited ones
1618280	1624840	are by Marco Gore, Scarcelli and others. And interestingly, about 10 years ago, the graph
1624840	1631480	neural networks returned triumphantly to chemistry, so I think worth crediting David Duvenau and Justin
1631480	1638360	Gilmer for who also introduced the terminology of message passing neural networks that try to
1638360	1643480	predict properties of molecules, model these graphs and learning on these graphs. And then,
1643480	1647480	of course, the ideas of geometric learning, as we'll see maybe with some extra stuff.
1648840	1656680	Also, the structural biologists have had their own image net moment with alpha fold, first in
1656680	1664600	2018 and then in 2020, basically predicting the structure of protein folds. So, and this field
1664600	1669000	is very rapidly developing. So, I think these are very exciting and cool problems that you can
1669000	1674920	address with geometric techniques. So, let's just try to summarize basically what this historical
1674920	1680680	excursion gives us, a kind of blueprint for different architectures. So, if you look at convolutional
1680680	1685240	neural networks and graph neural networks, right, they work with very different data, convolutional
1685240	1690840	neural networks work on images, graph neural networks work on graphs, right, let's say molecules,
1690920	1696120	but there are some common patterns, right. So, in both cases, we have some underlying domain.
1696120	1700840	So, in the first case, it's agreed. In the second case, it's a graph. We also, in both cases, have
1700840	1708120	some kind of geometric operation, so a symmetry, right, that is a nature in the context of the
1708120	1712440	problems that we are considering. So, in images, it's translation, right. So, I want to move,
1712440	1716600	for example, an object in the image and I don't care where the object is located if I want to
1716600	1722520	classify it. In case of molecules, it's a permutation symmetry, so no matter how I order
1722520	1726200	the atoms in a molecule, it's still the same molecule, right. So, I want somehow to be
1726200	1734280	insensitive to this ordering. And we can also define natural operations that respect this
1734280	1738280	symmetry, right. So, in case of convolutional neural networks, it's actually the convolutional
1738280	1744920	operation. Basically, I can move a patch around an image and apply the same weights or the same
1744920	1750680	local rule that would extract the features. And as we'll see in graph neural networks,
1750680	1754760	this is some kind of local rule that we call message passing, right, or some versions of it.
1756520	1762760	So, these are ideas that, as you see, these are two examples or architectures that share the
1762760	1768280	common principles and that's the idea of geometric deep learning. So, I can probably take the craze
1768280	1777400	of inventing the term. So, that happened when I was writing one of my ERC grants back in 2015,
1777400	1782440	probably. And of course, everybody was doing deep learning. So, you need to distinguish yourself
1782440	1788600	from everyone. So, I wrote that we are not doing deep learning as everybody else, we are doing
1788600	1796200	geometric deep learning. So, that was the idea. Then we popularized it in this paper in IEEE
1796200	1803960	signal processing magazine and more recently in a book that I'm writing with my collaborators.
1803960	1809880	So, by analogy to the Erlangian program, we basically, we can think of a kind of common
1809880	1814440	denominator for machine learning architectures. So, we would like to take this zoo of different
1814440	1818680	architectures that were historically designed for different types of data with different
1819400	1825400	kind of problem in mind and look at them from the same perspective. And this perspective is
1825400	1832440	through the lens of group theory and properties like invariance, equivariance and symmetry.
1832440	1838040	And if we've seen before this problem of machine learning in high dimension, where you have,
1838040	1843240	for example, your images of cats and dogs as points in a high-dimensional space, we no longer
1843240	1850840	consider these inputs as just a high-dimensional vector that you need to put through some generic
1850840	1857080	class of functions and then you suffer from the curse of dimensionality. But now we know that
1857080	1863320	there is some domain, geometric structure that underlies these inputs. And in images, for example,
1863320	1868920	this is a two-dimensional grid. So, our data lives on some typically low-dimensional domain.
1868920	1876440	And this domain comes equipped with some group. So, in this case, it's the group of translations.
1877240	1882760	And so, we have a domain, we have a group, we have signals that live on this domain and the
1882760	1889240	group that acts on the points of the domain, we see how it can act on the signals themselves
1889240	1894440	through what is called the group representation. And in case of images, again, the group representation
1894440	1900360	will be just the shift operator. We'll see it in more details. And then finally, we have functions
1900360	1905560	that act on these inputs that somehow need to respect this symmetry. And this will be through
1905560	1910840	what we will call invariance and equivariance. So, basically, we want the function either to be
1911800	1918600	insensitive to how I transform the input by acting on it with the group or should change in the same
1918600	1924920	way. And I should say that the choice of the group and the domain are two separate things and they're
1924920	1929160	not only dependent on the data, they're also dependent on the task. So, I might have a problem
1929160	1933720	like this. So, I have, for example, images of traffic signs. And if I'm designing a self-driving
1933720	1939640	car, it's very unlikely that I will see rotated traffic signs, right? They will probably be aligned
1939640	1945480	and move just horizontally and maybe vertically. So, here, for example, the group of transformations
1945480	1950920	that is reasonable to assume will be just two-dimensional translation, maybe even one-dimensional
1950920	1955800	translation. But if, for example, my car can also tilt, right, so imagine that it's a plane and not
1955800	1960040	a car, then rotations are also perfectly valid, right? So, then the group of transformation
1960040	1964120	will be different. So, it's still the same domain, the same data, but the task might be different
1964120	1970040	and, therefore, the assumptions about this, the priorities problem might be different.
1970040	1974200	And if you think of another application, if I have, for example, a pathological sample,
1975000	1980280	so, essentially, a glass of some stained tissue that I put under a microscope, so there I can also
1980280	1985400	have reflections, right? Because I don't have canonical orientation for this glass. So, it can
1985400	1992920	be an even bigger group in this case and, again, task dependent. So, as I mentioned in case of
1992920	2000200	images, the representation that we'll be working with is the shift operator. In case of crafts,
2000200	2005480	actually, the symmetry, as we've seen it, is the group of permutations, what is sometimes called,
2005480	2012360	confusingly, the symmetric group. So, it's the different ways that I can rearrange
2012360	2018200	and different objects. And its representation is a permutation matrix. And as we'll see,
2019080	2024280	the way that it's implemented, so, functions that are equivalent with respect to this symmetry
2025160	2030680	are message passing. And we can also have another type of architectures that are also confusingly
2030680	2035560	called equivalent neural networks or equivalent transformers, where, in addition to the symmetry
2035560	2040840	group of the domain, we also have symmetry group of the data. And this is typical for geometric
2040840	2045000	graphs like molecules, where the nodes also have geometric coordinates. So, they live in three
2045000	2050360	dimensional space. And in addition to reordering the atoms in the molecule, we can also rotate or
2050360	2055480	translate the molecule in three dimensional space, right? So, you want to be equivalent with respect
2055480	2060280	to both transformations. So, it's a kind of analogy of the external and internal symmetries
2060280	2065800	you have in physics. And, basically, geometric architecture is just sequences of
2066600	2071560	equivalent or invariant layers. You can also interleave them with pooling. So, I will not talk
2071560	2076520	about it too much. But pooling is implementation of another principle that is important in physics,
2076520	2082280	which is called scale separation. And this is what makes physics work. So, if you consider, for
2082280	2088200	example, let's say, this room, right? And how we are surrounded by probably a quadrillion of
2088200	2094680	different molecules that move very fast and collide with each other. But that's not how we can model
2094680	2101880	the behavior of gas in some space, right? It's computation intractable to trace all the molecules.
2101880	2107880	So, fortunately, there are just a few parameters that explain statistically how these gas behaves,
2107880	2113480	right? Like temperature and pressure. And this is the main principle of statistical mechanics.
2113480	2117400	But if we want, for example, to model how Earth, with its very complicated atmosphere,
2117400	2122440	moves around the sun, again, we can completely disregard it and consider it as a point. Because
2122440	2128360	at that scale, all these details are completely irrelevant. So, the scales, of course, interact
2128360	2134040	with each other. So, this is maybe a wishful thinking. And in neural networks, you can also
2134040	2142120	mathematically show that in some architectures, pooling operations are necessary for them to
2142120	2148200	operate correctly. So, these ideas can be applied to different types of objects, to geometric domains.
2148200	2152600	So, traditionally to grids, but then also to graphs, maybe to general
2153640	2159000	homogeneous spaces, and then also maybe to more exotic things like manifolds, meshes and
2159000	2163400	geometric graphs. And if you look at some of the standard architectures that are very commonly
2163400	2169880	used in deep learning, whether it's CNNs or maybe LSTMs or deep sets or transformers or GNNs or
2170600	2175480	intrinsic mesh convolutional networks, they can all be derived from the same blueprint. So,
2175480	2181400	there is some kind of domain and associated symmetry and associated environments that you
2181400	2187560	can bake into your architecture and you get, basically, particular instances of this blueprint,
2187560	2194520	some of the most common and famous architectures. Any questions so far before we start talking
2194520	2201560	in detail about graphs? So, if I have time, I will try to cover all these different domains, but I
2201560	2208360	would probably spend most time on graphs and also some physics-inspired perspective on these
2208360	2214280	architectures. Yes, I will have questions. So, if you kind the term geometric deep learning,
2214280	2219400	is there an alternative, like non-geometric deep learning or like traditional deep learning,
2219400	2226120	or what does it mean? Well, so, it's a very broad term. So, I think we use it maybe in a very
2227080	2233720	in a very broad sense. So, it's using geometric ideas or geometric techniques to
2234440	2241080	interpret and build deep learning architectures and vice versa, applying deep learning to
2241080	2246040	geometric objects. Now, whether, for example, the model of group environments and equity
2246040	2250600	environments is really the kind of ultimate truth, of course not. So, it's a mathematical
2250600	2254920	abstraction and in most cases, the transformations, for example, you have an image is, they're
2254920	2260200	actually not well described by groups, but at least it's a good starting point. In many cases,
2260200	2267560	for example, in molecules, this is kind of physical realities or an inductive bias that you
2267560	2275960	rather want to incorporate in your architecture. Okay, I have one more question. Could you please
2275960	2281560	take back to the previous slide? Yeah. Because you mentioned like the existing
2281640	2289720	neural networks architectures and the group of symmetries. And do you think there is any
2290760	2297160	other group of symmetries that we in real world applications should think about? And
2297960	2307800	because of that, create some new architectures that is, I didn't know, for example, was suited for
2308520	2313400	them? Yeah. So, it's a good question. So, there are some other architectures, for example, mostly
2313400	2317880	coming from physics. So, of course, in physics, you have interesting groups. So, I think there are
2317880	2322360	some Lorentz environments, for example, neural network architectures. You can also combine some
2322360	2326280	groups. I will show some examples that you can have, for example, products of permutations or
2326280	2332280	some special group products of permutations in subcraft neural networks. So, it's a general
2332280	2337320	blueprint. So, if you can follow the blueprint and design architecture that implements this
2337400	2344120	invariance, that is relevant for your problem, then you have a way of doing it. Okay. Cool. Thanks.
2344120	2348040	There are also some works that try to discover the symmetry group from the data and from the
2348040	2355400	problem. So, that's also an interesting direction. Could you shortly address the symmetries which
2355400	2360680	the transformers go for, because I still have difficulty to think in this symmetry, like,
2360680	2365400	perspective? So, I will talk. Well, it will take me probably about 20 minutes, but we'll get to
2365480	2369320	transformers. Yeah. So, transformers are special types of craft neural networks. You can think of
2369320	2377080	them really quickly. So, on the slide, there's the LSTM. So, it's written that time warping is
2378040	2383880	you could have architectures that are invariant to time warping, but if you look at speech recognition.
2383880	2390440	So, maybe first of all, we humans also don't have that, right? So, if something is super
2390440	2396040	slow down, we're probably not going to notice what it is. So, my question is, is there any
2396040	2401160	other invariance or equivalence for time series data that you can think of? So, well, here what
2401160	2407880	I mean by time warping, you can actually show that gating is a necessary mechanism to be able
2407880	2415000	to accommodate time warping. So, basically, gating emerges as basically as the architecture for
2415000	2420680	this kind of, for this kind of invariance. Okay. Got it. Thanks. Yeah, I think there is a question there.
2426760	2436600	So, okay. I mean, so you said that you will talk about transformers more in, like, in the next few
2436600	2444680	slides, but I wanted to ask about one of their emergent properties, like in relation, for example,
2444680	2454120	to CNNs, namely that CNNs have, like, kind of encoded in them by definition translation invariance,
2454120	2460200	whereas transformers don't. And I think it has been discovered that transformers actually kind
2460200	2466760	of learned the translation invariance from the, thanks to the amount of data that they, that they
2466760	2476680	consume. But, like, they weren't defined to do that. It's kind of, this translation invariance
2476680	2484360	just emerged from the data. So, I was wondering whether, in your opinion, it is better to, like,
2484360	2494360	use more generic architectures, which learn those inductive biases from the data, or whether we
2494360	2499560	should encode the inductive biases in their architecture, or, and, you know, face the risk that
2500280	2507160	a specific inductive bias may be also a limitation. Yeah. Well, I think the answer is already contained
2507160	2512280	at least one of the answers in your question, right? So, the amount of data is obviously a limitation.
2512280	2518840	So, it might be easy to collect data, like images, right, or maybe text. It might be much more difficult
2518840	2524200	to collect data that comes from biological experiments, right? So, if the data is limited
2524200	2529160	and the data is expensive, then probably you want to work hard to incorporate as many inductive
2529160	2535720	biases as you can. In some other applications, actually, the inductive biases, or some symmetries
2535720	2539160	or some invariances come from the problem that you're trying to model. I think it's
2539160	2544040	true for many applications in what is called the AI for science, right? So, if you have some, I don't
2544040	2547800	know, physical system, you know that certain properties will be conserved. So, it makes no
2547800	2555400	sense just to use a generic black box that will be producing unrealistic outputs, right,
2555400	2563400	that are physically incorrect. But there is no, so it might be that in problems where you don't
2563400	2569640	know a priori what symmetry or invariance you have, it might be a good idea to use transformers if
2570360	2572920	the computational complexity and the scale of the data allows it.
2572920	2581800	Hello, I have a question about augmentation. It feels like, at least in the image case,
2581800	2587880	augmentation is basically exploiting this group symmetry, right? You augment by
2587880	2595480	doing some translation. But if you use some geometric-based architecture, maybe then you
2595480	2600920	don't need to do augmentation anymore. So, what's your opinion of augmentation? Is it like an artifact
2601720	2607320	or something? Well, augmentation is a technique of, basically, when you have limited amount of
2607320	2613560	data, you can generate synthetic data that looks like the kind of data that you want to see in
2613560	2618360	your application. This was actually one of the important features of the AlexNet, for example,
2618360	2625000	so they use certain type of data augmentation for images to make it more robust. So, again,
2625000	2631000	you can incorporate this kind of inductive biases into the architecture. Sometimes,
2631000	2635720	it might be difficult. So, another aspect that is often overlooked is actually the hardware,
2636520	2642920	right? So, it's very easy and it's probably more a coincidence. At least originally,
2643880	2650920	the convolutional neural networks map very nicely to the type of hardware that is used in
2651000	2658760	the GPU, the single instruction multiple data type of architecture. So, it was not by design
2658760	2663800	because the GPUs were designed for different types of problems. With other architectures,
2663800	2667000	it might not be the case, right? So, with graphing a lot of programmable, this is not the case.
2667560	2672760	So, there might be some better, so to say, architectures, maybe from mathematical standpoint,
2672760	2680440	but they're just not as convenient to implement. Therefore, maybe you will prefer to use
2680440	2686520	something that is less correct, but you can do data augmentation. The hardware allows you to
2686520	2696040	implement this architecture better. What do you think is a follow-up question about
2696040	2700680	augmentation? What do you think about augmentation? Not as a simple tool for increasing the size of
2700680	2706280	the data set, but as something as in contrastive learning, as enforcing the symmetry. Can it be
2706280	2712040	kind of equivalent to the symmetry you defined? Well, it was used in this formula. I mean,
2713800	2719800	can the load of enforcing the symmetry be shifted towards augmentation to the model?
2720360	2724680	Yeah, so basically, you're sampling points from the group orbit, right? So, you can think of it
2724680	2731480	this way as well. Whether it's enforced in a hardware or in a software, that's probably not
2731480	2737080	that important. So, data augmentation is a very valid technique, if you know exactly how to do it.
2739720	2746680	Okay, so let's move on to graphs. And, well, graphs, as you probably know, their idea itself
2746680	2753240	is pretty old. So, it's usually attributed to Euler who was thinking of these kind of problems,
2753240	2758600	right? How you can connect land masses without actually accounting for the particular geometry,
2758600	2765400	but only how, what is nearby, right? So, the famous problem about the bridges of KÃ¶nigsberg,
2765400	2771000	and this is what he called the geometry of Cetus, or basically the geometry of place,
2771000	2777320	what in modern terminology we call topology. So, the term was actually introduced by Pankareho,
2777320	2784200	by analogy to Euler's terminology called analysis Cetus, and that was also where his famous conjecture
2784200	2789640	appeared. So, we'll talk about Pankareho conjecture actually later. I hope to get there as well.
2790760	2796040	So, graphs, obviously, I don't need to convince you that graphs are interesting and important. So,
2796040	2800280	more or less anything from very small scales or very large scales can be modeled as a graph.
2800280	2804600	So, any system of relations or interactions, whether it's a molecule, or you model how
2804600	2809560	different atoms interact with each other through chemical bonds, to, for example,
2809640	2817800	interactoms, right, in biological science, how different entities in our body or in a cell
2817800	2821880	interact with each other, different, for example, chemical reactions, or even social networks,
2821880	2827400	right, describing relations, friendship, interactions between different people.
2828120	2832520	So, this model is a graph, and again, graphs can be of different types. So,
2832520	2836920	let's consider a simple model here. So, we'll consider an undirected graph. So,
2836920	2845720	it means that we have a collection of nodes, and we have unordered pairs of nodes as edges,
2845720	2853400	right? So, just pairs, basically, the order doesn't matter, and the nodes are described
2853400	2857400	by d-dimensional vectors. So, these are features that are attached to the nodes. And, of course,
2857400	2861880	it can be more complicated. So, you can have graphs that are directed, you can have both
2861960	2869240	continuous and categorical features, both in the nodes and the edges. But, just, that would be
2869240	2874840	already interesting enough to look at this kind of object. So, one thing that characterizes graphs,
2874840	2879080	right, basically, this is, again, a topological construction. So, it's an abstract object that
2879080	2884680	lives on its own. The moment we need to represent it on a computer, we describe it, for example,
2884680	2888440	as a matrix, right? So, we can describe the structure of the graph as the adjacency matrix,
2888440	2893800	right, of size n by n, and it's a number of nodes. So, we have one, if there is an agent
2894680	2898840	between a pair of nodes and zero, if there is no edge, right? And if the graph is undirected,
2898840	2905400	then this matrix is symmetric. And the features, we can describe them as a matrix of size n by d,
2905400	2911080	right? d is the dimensionality of the node features. So, one key thing here, which is already
2911080	2915240	written on this slide, is that we don't really have a canonical way of ordering the nodes of the
2915240	2920680	graph. So, when I make this description, I automatically assume some ordering of the nodes,
2920680	2925720	but this ordering can be like this or can be like this. So, anything that will take
2926760	2933080	this description of the graph as an input must account for this built-in ambiguity, right? So,
2933080	2941320	I somehow need to be able to produce outputs that disregard, in a correct way, all these possible
2941320	2946920	permutations, right? And the two types of problems that we can consider in relation to graphs,
2947560	2950840	but actually more problems, but let's say these are the prototypical problems.
2950840	2953880	One is graph-level problems. So, I give you an input graph and I try to
2954600	2959160	output a number that describes this graph, right, or maybe a vector. Like, for example,
2959160	2963480	I'm predicting the chemical properties of this molecule, like water solubility, so the input is
2964280	2969160	a graph describing a molecule, the output will be some number, right? Another class of problems,
2969240	2974120	I give you a graph and I want to do node-level decisions, right? For example,
2974120	2979880	I want in a social network to classify which of the users is behaving badly, right? Maybe a
2979880	2985320	spammer. So, in the first case, I give you a graph, right, and the output, no matter how I
2986280	2990520	permute the inputs, should be the same, right? So, we're talking about a permutation invariant
2990520	2995240	function. So, mathematically, it can be described like this, so, right? So, the function here now
2995240	3000280	is a function of x, the node features, but also the structure of the graph. So, they're both
3000280	3007240	from the input, and here I act on these two inputs with the permutation matrix, which is
3007240	3011880	the representation of the permutation group. You see that actually the representation is different
3011880	3017320	for different types of objects. So, the features, you can think of them as vectors, right? So,
3017320	3023480	I permute only the order of the rows. The adjacency matrix, it's two-dimensional tensor,
3023480	3029160	so, I act both on rows and columns, right? I need to permute both rows and columns. Here,
3029160	3035800	this is clear. And together, this form permutation invariant function. In the second case, if I want
3035800	3041240	node-level predictions, the output has the same structure as the input, right? So, if I change
3041240	3047000	the order of the inputs, the order of the outputs is expected to change in the same way, right?
3047000	3051240	And here, I'm interested in permutation equivariant functions. So, equivariance means changing in the
3051240	3056520	same way. So, the output now will be, well, some kind of vector, right? Or a matrix, you know,
3056520	3062440	by f capital. And if I permute the input, the output will be permuted in the same way, okay?
3063560	3067880	Now, what are graph neural networks? Essentially, these are parametric graph functions. So, I provide
3067880	3073640	you a graph as an input, right? Or these matrices x and a, and I output something, right? And
3073640	3078840	something here, as we'll see in a few minutes, is parameterized by some vector of parameters.
3079800	3084680	And sometimes, there is no distinction made between graph neural networks or message passing
3084680	3090440	neural networks. So, we want to make this, that is distinction, but sometimes they're used synonymously,
3090440	3094600	right? So, most of the graph neural networks that are used in practice are of the message
3094600	3100120	passing type. We'll see exactly what it means in a second. And graph neural networks, again,
3100120	3105400	you can consider them as a special instance of these geometric deep learning blueprints. So,
3105480	3111560	we have usually a sequence of permutation-equivariant layers that produce node-wise predictions. And
3111560	3115400	permutation-equivariant, in the sense that if I change the order here, it will change in the
3115400	3121320	same way here. And then, if I have graph-level tasks, I will have permutation-invariant pooling,
3121320	3125480	right? That aggregates all the information from the, from these node features and produces a
3125480	3131560	single output for the graph. And the typical way that, that they work is by neighborhood aggregation.
3131560	3137320	So, you can pick up a node in the graph, right? Let's call it i. And now, I look at the neighborhood
3137320	3143320	of the node. By neighborhood, I mean all the nodes that are connected to i by an edge, right? So,
3143320	3149080	this is how the neighborhood of i looks like. And I can look at the feature vectors associated with
3149080	3154200	these, with these nodes. And you can see that even though the neighbors are unique, right? The
3154200	3158600	feature vectors are not unique. So, this is encoded by color. So, these two nodes have exactly the
3158600	3163720	same feature vector, right? Just for this example. So, together, they form what is called a multi-set,
3163720	3169560	right? So, it's a set where the same object can appear more than once. And we also have the
3169560	3174840	feature vector of the node itself. So, I want a function to aggregate them locally, right? Let's
3174840	3179640	call this function phi. And again, the, the characteristic property of this function is that
3180520	3185240	I don't have a canonical ordering of my neighborhood. So, the feature vectors can appear like this
3186120	3193080	to, to this phi. So, it must be by design permutation invariant, right? It cannot assume any order
3193080	3197960	in which the neighbors come. We can make it more complicated and we can incorporate additional
3197960	3202680	information. But again, as a basic structure of a graph, you don't have this order, right?
3204280	3208840	Now, you can repeat this process everywhere at every point, at every node of the graph. And this is
3208840	3213880	actually very highly parallelizable, at least in principle. And once you do it, you get an output,
3213960	3219720	right? That for every node of the graph, you output some vector. And this is also a function
3219720	3224840	of the graph. And you can easily see that if my choice of this local aggregation is invariant,
3225560	3230200	then the output is equivariant, right? So, if I change the order of the axis here,
3230840	3235960	then the output will change in the same way. And most of the graph neural network architectures
3235960	3243160	differ in the choice of these phi, in how I aggregate locally the features. And while they're
3243160	3247640	probably zillions of different architectures, most of them fall within the following three
3247640	3252040	categories. So, the first one is what is called convolutional graph neural networks. And you
3252040	3259720	can simply think of convolutional neural networks as just summing up the features of the neighbor
3259720	3267800	nodes. So, this is what I write here. So, the update for the representation for the feature at node i
3268760	3277400	is the sum of some transformed neighbor nodes, right? Psi will be some learnable function. Xj
3277400	3284360	is the the the feature of the neighbor node. Here it is. And ai, j are coefficients that depend
3284360	3289000	only on the structure of the graph. In the simple case, just the elements of the adjacency matrix,
3289000	3293720	right? And here, sigma is just some non-linear activation, typically very low, right? So,
3293800	3297800	that's how a convolutional type graph neural network looks like. Yeah.
3306040	3312280	Is this convolution here equivalent to 1D convolution, something like that?
3312280	3317560	So, we'll see it, we'll see it in the, well, not in a second, but we'll see it later. So,
3317800	3325880	basically, you can obtain convolution when the graph is agreed. And we can actually see that
3325880	3331800	that convolution on the grid is a special type. Basically, it's a unique linear equivalent function.
3332760	3338440	So, basically, yeah. So, that's why the term convolution is appropriate. So, it's an extension
3338440	3346200	of conversion to graphs, right? And you can write it in this form, right? So, if I write it as
3346200	3352120	matrix multiplication, so x is my feature matrix of size n by d, I multiply it from the left by
3353000	3356600	some matrix a, right? The diffusion matrix. It can be the adjacency of the graph. It can be
3356600	3361160	something else. But, basically, it propagates information between adjacent nodes. And on the
3361160	3368120	right, I have, in the case of a linear transformation of the nodes, it's a learnable shared matrix
3368120	3372760	that acts on every node in the same way, right? On the features of every node in the same way.
3373480	3378760	We'll see that it's related to diffusion equations on graphs. And this is probably the simplest
3378760	3383480	version of a GNN. It's highly scalable. You can basically just large matrix multiplication.
3384040	3389080	It has been used in industrial use cases. I think the first to use these kind of architectures was
3389080	3395080	Pinterest with Pinsage. We also used it at Twitter. And then there are some statements about these
3395080	3400440	architectures in the kind of graph neural network folklore saying that it works only on homophilic
3400440	3404680	graphs. So, by homophily, I mean this assumption that my neighbors are similar to me. Typically,
3404680	3410600	the assumption is that the labels in the neighborhood are somehow similarly distributed to the label
3410600	3414760	of the node itself. And here's an example of a homophilic graph versus a heterophilic graph.
3415400	3420920	So, and the usual motivation that is given that these matrix a typically will look like a low
3420920	3425880	pass filter, right? So, you're somehow averaging your neighbors. And if the neighbors are of the
3425960	3429560	same type, then it will work. And if the neighbors are very different types, then
3430520	3436360	it makes more harm than help. And this is actually not true. So, I hope to convince you that
3437000	3442200	the story is much more nuanced. There is also this channel mixing matrix. And there is an
3442200	3447320	interesting and subtle interplay between these two matrices. And we'll be able to see how it
3447320	3450920	works when we consider graph neural networks as differential equations.
3451560	3456920	So, slightly more interesting architecture is an attentional GNN. So, here, again, we can think of
3456920	3463720	it, at least in some settings, as a linear combination of the features of the neighbors,
3463720	3468200	maybe transformed by some learnable function. But now the coefficients depend not only on the
3468200	3471880	structure of the graph, but also on the features themselves, typically through an attention
3471880	3477160	mechanism. So, the most famous representative of these architectures, they got the graph
3478120	3483800	attention network. And in the most general case, right? So, we can write got like this. So, that's
3484680	3489560	if we have a linear combination, some, so we have linear combination with the matrix. But now
3489560	3495880	the matrix is actually a matrix valued function of x, right? So, it itself depends nonlinearly on x.
3497480	3502040	And the most general case is a message passing architectures where we have a b-variate function
3502040	3510520	that depends on the feature of the node and the neighbor. And the other cases are specific
3510520	3519720	cases of this architecture. And it was shown that message passing GNNs with specially chosen
3519720	3524120	aggregation function, in particular, it has to be injective in certain restricted settings,
3524120	3530040	again, allow me not to go into the details, are equivalent to graph isomorphism testing
3530040	3534280	algorithm that was derived by vice versa and lemon that I mentioned in the beginning.
3535080	3541960	And let's talk about this. So, basically, from theoretical standpoint, what actually
3541960	3545880	graph neural networks do, right? Or message passing type graph neural networks do.
3545880	3551800	So, first of all, what is graph isomorphism problem? It's telling when two graphs are the same,
3551800	3557800	right? So, I'm writing here equal, but of course, it's not equal. So, it's equivalent in some sense.
3557800	3563480	And what do I mean by this equivalence? What I want to say is that there exists an edge preserving
3563480	3568040	bijection between the nodes of these graphs, right? So, in other words, I can find a correspondence
3568040	3574120	between nodes in G and G prime, such that if there is an edge between a pair of nodes in G,
3574120	3580280	then there is a corresponding edge in G prime, right? So, is this bijection unique?
3581240	3588520	What do you think? No? When is it not unique?
3591800	3595160	Exactly. When the graph has symmetries, right? So, this is an example. Actually,
3595160	3599000	this graph is a good example, right? So, you can reflect the nodes on the left and the right,
3599000	3603800	and then we can have another bijection, right? Like this. So, this bijection is not unique.
3603800	3609960	But for determining if two graphs are isomorphic, it's sufficient to say that
3609960	3614840	there exists such a bijection, right? So, that's what tells us that the two graphs are equivalent,
3614840	3619720	right? So, basically, they are the same up to the ordering of the nodes, right? So,
3619720	3624280	if I look at their adjacency matrix, they will be the same up to applying some permutation,
3624280	3628920	right? So, I can basically, I can relate the two adjacencies by permutation.
3630280	3635160	And related to the question of universal approximation, right? Which is fundamental for
3635160	3641240	traditional neural networks like perceptrons, we can show that a class of functions is universal
3641240	3646200	approximating permutation in variant functions on graphs with, importantly, here, the limitation
3646200	3652520	finite node features, if and only if it can discriminate graph isomorphisms, right? So,
3652520	3657160	basically, universal approximation is equivalent to graph isomorphism testing, right? So, basically,
3657160	3664120	the two things go hand in hand. And if you ask what kind of graphs can we represent with
3664120	3668360	message passing neural networks, right? So, here is, let's say, the space of all graphs.
3669080	3673480	And these would be graphs that are structurally equivalent, right? That is isomorphic. So,
3673480	3678360	by construction, we know that graph neural network cannot distinguish between these graphs,
3678360	3682760	right? They're exactly the same up to the ordering of the nodes. So, just by construction,
3682760	3689720	it will produce the same output for any isomorphic graphs. But the question of the opposite
3689720	3695320	direction is more interesting. And this is not necessarily guaranteed, right? So, I might have
3695320	3700120	different graphs that are not isomorphic, like the reds and the blues, that by chance will have
3700120	3705400	the same representation. So, the graph neural network will output the same output for these
3705400	3710360	different graphs. So, in other words, if we have the space of all permutation in variant functions,
3710360	3715320	right, and we know that these are all graph isomorphism discriminating functions, this is where
3716040	3719880	we'll see a subclass of functions that can be computed by message passing.
3722840	3729320	Okay. And so, the question of graph isomorphism, as I mentioned already, it came from applications
3729320	3734520	in organic chemistry, where people try to compare structures and try to determine whether
3735240	3740520	two molecules are the same, right? So, in the case of isomorphism is a special setting, right?
3740520	3746760	We can also think of distances between graphs. And vice versa in 68 came up with an algorithm
3746760	3751880	that they believe to be a polynomial time method for determining whether two graphs are isomorphic.
3752840	3758520	So, I should say that at that time in the 60s, even the notion of complexity was not totally
3759880	3768600	spelled out. And also, the understanding of what's the complexity of graph isomorphism testing
3769160	3773320	as a computer science problem was not understood. Actually, it's not understood even now. So,
3773320	3778840	we know that it's not NP-hard and we also don't know polynomial time algorithms for it. So,
3778840	3784600	it's a special complexity class that is called GI class. But anyway, so, it was actually disproved
3784600	3792040	by a counter example. So, it was an example, it was shown that the class of graphs that cannot be
3793240	3798280	tested by the device for an algorithm. We'll see such examples in a second. But the way that
3798840	3804040	it goes, it's essentially a color refinement procedure. So, it considers a graph without any
3804040	3809480	features, considers only its structure. And, initially, the graph has every node labeled in
3809480	3815080	the same way. By label, I just mean a natural number that is attached to a node, right? And what
3815080	3821640	the algorithm does, it takes a node and looks at its neighborhood, right? And you can see that
3821640	3825400	originally in this graph, we have two types of neighborhoods. So, we have a blue node with
3825480	3830840	two blue neighbors like this. Sorry, that's blue node with three blue neighbors and blue node with
3831400	3835960	two blue neighbors, right? So, these are two neighborhoods that we see in this graph. So,
3835960	3841880	if I now apply an injective function that they call phi, right? Think of it as hashing. I will
3841880	3847160	have two distinct outputs, right? So, I will have nodes of the yellow type, let's call it, and of
3847160	3851240	green type, right? So, I will be able to distinguish between these different neighborhoods. So, now,
3851240	3855640	I have a graph with refined labels. I can apply the same procedure again, and now we have three
3855640	3860200	types of neighborhoods, right? We have green with one green and one yellow neighbor. We have
3860840	3864840	green with two yellow neighbors, and we have yellow with two green and one yellow neighbor,
3864840	3869720	right? And these become, again, distinct colors. So, this will be, let's call it violet, gray,
3869720	3875240	and orange. But if I repeat this procedure again, the colors will stop changing at which
3875240	3880600	point the algorithm stops and produces a histogram of colors, right? So, that's a graph level
3880600	3886520	descriptor. You can think of it this way. And what they show in the paper, well, the paper is
3886520	3891320	actually complicated to read, but that's, let's say, a reduction of it. It actually describes
3891320	3898680	a different type of algorithm, what is called 2WL, that does edge color refinement, but it
3898680	3904040	doesn't matter. It's equivalent to what I'm showing here. So, if I give you another graph,
3904040	3909720	and the distribution of colors is different, then I can guarantee that they are not isomorphic. But
3909720	3916520	if the distribution of colors is the same, like in this case, we actually don't know. So, it's
3916520	3921560	unnecessary, but insufficient condition, and in fact, you can find examples of non-isomorphic
3921560	3926440	graphs that would be deemed equivalent by WL test, right? Or in this case, WL test cannot
3926440	3931640	determine whether they're not isomorphic. And you can also see why the reason for it, right?
3931640	3940360	Basically, what it does, it refines the colors of the nodes, so every node looks at its neighborhood.
3940360	3944440	And this is how the neighborhoods of nodes look like, right? So, this node has two neighbors,
3944440	3948520	then this node has, again, two neighbors, and so on and so forth, right? So, if you look at the
3948520	3953560	structure of these neighbors, they will be exactly the same in both cases, right? And actually,
3953560	3958200	very simple examples of graphs, for example, regular graphs where the degree of every node
3958200	3966360	is the same cannot be tested by this simple procedure of Weisfeld and Lehmann. You can also not count
3970520	3975960	connected patterns of more than three nodes, like triangles or cycles. And this is, I think,
3975960	3980120	astonishingly disappointing given that the algorithm came from applications in chemistry,
3980120	3986040	so in chemistry, these would be two different molecules, right? And this has a six ring and
3986040	3992680	this has a five ring, right? So, or five cycle using graph theory terminology. So, we cannot
3992680	3998200	distinguish these molecules by device for a Lehmann test. They would appear potentially the same,
3998200	4006680	right? So, we wouldn't know. So, basically, the functions that can be computed by WL are strictly
4006680	4011640	smaller than all permutation invariant functions, right? And we know examples of functions that
4011640	4015560	cannot be computed by WL. For example, we cannot count the number of rings, right? So,
4015560	4019560	if I want to implement a function that counts the number of rings in a graph, I cannot do it by
4019560	4026360	means of WL test or by means of message passing. Now, the relation between WL and message passing
4026360	4031720	is not random, right? You can see this, even the structure of the algorithm is exactly the same,
4031720	4037720	right? So, this is what WL test does, right? So, it updates the color of every node by looking at
4037800	4044040	the structure of the node and the multi-set of neighbors, right? Here, x denotes the colors.
4044040	4048760	And this is what MPNN does, right? So, here, the squared denotes some general
4048760	4053000	permutation invariant aggregator. It can be sum, it can be max, it can be mean, it can be anything,
4053000	4058040	right? Importantly, it's permutation invariant. So, we can see that it's a special case. So,
4058040	4064360	MPNN expressive power is upper bounded by device for a Lehmann test. And the question is when
4064360	4070520	MPNN is as expressive as WL test, right? So, basically, we're interested in this case, right,
4070520	4075560	when the two circles coincide. And if you look at different types of aggregators, right? So,
4075560	4081880	imagine that this is your input graph. So, I have this gray node that has two types of
4081880	4086600	neighbors. We have green neighbors and we have blue neighbors, right? So, if I consider the input
4086600	4091560	as a multi-set, right? I completely disregard the structure of the graph itself, right? So,
4091640	4100280	what the node sees is just a soup of the neighbor features. So, if I use a maximum aggregator,
4100280	4104520	I cannot distinguish between these and these, right? Because for the maximum, it doesn't matter
4104520	4110040	how many times each of these features appears, right? If I use a mean, then I cannot distinguish
4110040	4116520	between these and these, right? I can multiply the neighbors by some constant factor. The sum,
4116520	4122680	though, allows to distinguish between all of them, right? So, you can think that maximum gives
4122680	4129400	a kind of skeleton of the set and the mean gives you the distribution, but the sum is strictly more
4129400	4138040	expressive, right? And here's an example of structures that max or max and mean would fail
4138040	4143400	to distinguish. And indeed, sum appears the most expressive one. And if you assume that,
4143960	4149800	so the theorem about the equivalence between WL and message passing states that if you assume
4149800	4156040	that the node features come from a countable universe, then if you have an MPNN with an
4156040	4161800	injective aggregator, call it square, an update function phi and graph-wise readout function
4162600	4167720	is as powerful as the vice-versa element set, right? And the assumption here is of discrete
4167720	4174120	countable features, which is not always the case in practice. And then the reason architecture
4174120	4178600	that actually implements that is equivalent theoretically to the vice-versa element, which is
4178600	4185240	the graph-wise morphism network or GIN. So, basically, it uses a sum aggregation. So,
4185240	4190280	the epsilon here is just theoretical thing. So, there exists infinitely many constants epsilon
4190280	4197640	that you can use here. So, we know that, basically, there exists at least a choice, right? Within the
4198520	4203400	all possible message-passing neural networks that makes it as expressive as WL. But, of course,
4203400	4208920	we are interested in more expressive architectures, right? Can you do better than WL? And here, again,
4208920	4213560	there is an entire universe of different architectures. So, some of them actually go beyond
4214200	4218440	message-passing, at least in the traditional sense. And, roughly, you can distinguish between
4218440	4223560	four different categories of approaches. So, it's either a higher-order WL test. It's not
4223560	4227960	a single test. It's a hierarchy of tests. The use of positional instruction and coding,
4227960	4233880	so that's how transformers work. Subgraph GNNs and then topological message-passing, right?
4234600	4239080	So, let's talk briefly about all of them and then when do we need to do the break?
4240600	4241000	Sorry?
4243880	4250600	So, let's maybe 20 minutes and then we do the break. So, the first class of higher-order WL
4250600	4255080	tests, as I mentioned, so WL test is just one algorithm that was initially developed
4255880	4263800	and then extended by Babi and collaborators, actually independently also by other people.
4264600	4268520	So, one of them was Eric Lander, who is mostly known as computational biologist,
4268520	4274440	but he started as a mathematician. And, basically, this is an increasingly
4274440	4278680	more expressive hierarchy of tests. Instead of doing node refinement, they look at tuples
4279240	4284520	of nodes. So, it's obviously computationally more expensive and there is always, you can find a
4284520	4289880	family of graphs that these algorithms cannot distinguish. So, like strongly regular graphs for
4290600	4298760	2WL or 3WL tests and what is called CFI graphs for general KWL. I should also say that this
4298760	4304120	terminology of KWL is confusing because they're what is called folklore WL tests versus the
4304120	4309960	classical WL tests that are slightly different. So, but overall the hierarchy, right, up to
4309960	4317800	notation is the same. So, we know that message-passing GNNs are equivalent to the standard WL. You can
4317800	4326280	also design just replicating in the neural network architecture the KWL tests higher-order KGNN.
4327000	4332280	So, this is what Hageim Aron did in his works. And then you can also have some other algorithms
4332280	4336440	we'll talk about in a second that sits somewhere between. They don't exactly follow the
4337400	4342840	hierarchy of the WL tests. So, the second approach is positional encoding. And, again,
4342840	4347960	I remind you of this example of two graphs that cannot be distinguished by the WL tests, but
4347960	4354840	imagine that I could now attach some features to the nodes of the graph, right? And this could be
4354840	4361400	even something as simple as random features. You see that now, because I have some extra information,
4361400	4367000	I can distinguish between these cases, right? So, if I look at the leaves, for example, of
4369160	4375080	this tree, right, I can ask, for example, whether the root appears among the leaves or not, right?
4375080	4380680	And here it does appear and here it doesn't, right? So, they're clearly different, right? I would be
4380680	4385720	able to distinguish between them, right? So, the covering of the nodes removes at least to some
4385720	4390920	extent the ambiguity. Now, of course, if I use random features, then the question is how can I
4390920	4397160	reproduce them on a different graph? So, these type of approaches are equivalent only in expectation.
4397160	4401560	But there are other methods that can do better. And these are structural encoding.
4402440	4408600	So, the idea of structural encoding, you have some substructures, right? So, we have a bank of
4408600	4414440	substructures that call it H. And you can count the substructures, the occurrence of substructure
4414440	4417800	for every node or for every H, right? And there are two ways that you can
4417800	4422200	consider subgraphs, whether what is called a subgraph or any induced subgraph. It doesn't
4422200	4427080	really matter, right? So, the two ways that are slightly distinct. And for example, in these two
4427080	4433240	graphs, if this is my bank of substructures, let's say cycles of size 6 and 5. So, in this molecule,
4434600	4442120	at every edge or at every node, I will count once the six cycle substructure and these nodes,
4442120	4447800	I will count twice, right? Because this node participates in both structure on the left and
4447800	4454680	right. But the five cycle substructure doesn't appear here versus it appears here, right?
4455320	4459640	So, with this encoding, now I have some, these additional features that I can attach to every
4459640	4463880	node or to every edge of the graph. And I can use them in standard message passing. And this would
4463880	4471160	allow me to discriminate between these graphs. And the complexity of this method is basically,
4471240	4476200	it's all hidden in pre-computation, so the counting of substructures. So, in the worst case, it's
4476200	4481000	order of n to the power k, k is the size of the substructure, so it could be large. But in practice,
4481000	4487320	for structures, more friendly structures like triangles, there exist more efficient algorithms.
4487320	4493160	So, in practice, it can be way better. The algorithm itself, and especially the training part,
4493160	4497640	which is typically more expensive, is standard MPNN. So, it has linear complexity in the number of
4497640	4503560	edges, or roughly order of n if the graph is sparse. And the theoretical result is that these
4503560	4508280	kind of architecture that we call GSN, graph substructure network, is strictly more expressive
4508280	4514120	than WL on certain assumptions on these substructures, right? So, it should not be a
4514120	4523160	star graph or it should be a structure of size, bigger than three. And basically,
4523160	4528200	we can formulate it is that GSN is not less expressive than 3WL. You can also do it for
4528200	4534680	different KWLs. Basically, you do it by counter examples. So, you can design a substructure
4534680	4541640	that the standard WL tests cannot count, whether it will be clicks of certain type or other things.
4542520	4547080	Right? And the proof by example is something like this. So, this is a stronger regular graph. It
4547080	4552920	cannot be distinguished by 3WL, but this graph contains four clicks and this doesn't.
4553400	4559400	So, if I count four clicks, I would be able to distinguish between these graphs. So, in a sense,
4559400	4566280	it's a kind of cheating. So, I'm not following really the KWL hierarchy. So, this is an example of
4566280	4571880	different structures I can count, triangles and clicks. But then, basically, the expressive
4571880	4577640	power looks like this. So, this is, for example, a graph substructure network with four-click count.
4577640	4583320	So, it might actually, there might be examples of graphs that are distinguishable by 3WL, but
4583320	4589880	not by this method. We have at least an example of a family that 3WL cannot detect. So, it's outside
4589880	4596840	of the hierarchy. And why this is important, especially in applications related to chemistry,
4597480	4602360	because often we know these substructures are priori, right? Organic molecules, for example,
4602360	4607000	cycles are a very prominent feature. These are what is called aromatic rings, right? Like in the
4607000	4612680	molecule of caffeine, we have two, right? So, we have this ring of cycle of size six and cycle of
4612680	4618280	size five. And we see that if we incorporate this information as a kind of problem-specific
4618280	4624200	inductive bias into the problem, we are able to much better predict the properties of molecules.
4624200	4629640	In this case, it's, I think, the water solubility on the, on a toy data set of molecules that is
4629640	4636200	called zinc. And by incorporating cycles, we significantly reduce the error.
4638280	4644040	Third class of approaches, what is called subgraph GNNs. And here, the idea is also very simple.
4646360	4650120	So, if you look at these two graphs, again, this is probably one of the simplest examples of
4650120	4654760	non-lasomorphic graphs that cannot be tested by WL, right? If you do the color refinement,
4655560	4660280	this is what WL produces, so the histograms are the same, right? That's exactly the case
4660280	4666680	where you cannot say anything about the graphs. But imagine that I can perturb the graph, for
4666680	4673240	example, by removing this edge, right? So, if I did it, the colors will be very different, right?
4673240	4678360	So, these will be the distributions of the, of the colors, and they are clearly distinct. So, in
4678360	4683960	this case, the perturbation allows to distinguish between structures that are otherwise indistinguishable.
4683960	4688760	So, the question is, of course, do I know which edge to remove, right? So, here maybe I was lucky,
4688760	4693640	and the answer is usually I don't. So, let's remove all possible edges, right? So, let's just
4693640	4699080	make this perturbation when I remove one edge at a time, right? So, and there are seven possibilities.
4699080	4705240	I can also do node deletions in the same way, right? And now, instead of a graph, I have a collection
4705960	4710440	of subgraphs that are extracted by some policy. So, in this case, it's a very simple policy,
4710440	4716120	one node removal, right? And actually, it results in graph theory that say that if I give you this
4716120	4721640	collection, so, in terminology of graph theory, this is called a reconstruction. So, if they have
4721640	4727800	the same multi-set of node remove subgraphs, right? So, this is what we denote H tilde G, right?
4728520	4733000	So, graphs where, basically, if we look at these kind of multi-sets, they will be the same, of
4733000	4738840	course, up to, up to reorting. So, the statement in graph theory that is called reconstruction
4738840	4745400	conjecture claims that under some technical assumptions, if H is a reconstruction of G,
4745400	4751960	then H is equivalent to G, isomorphic to G, right? So, why it is called a conjecture? Because
4751960	4755880	it is proved only for small graphs, and it's an open question in general. And there are
4755880	4761160	generalizations for subgraphs where you remove multiple nodes. So, this is, again, not a single
4761160	4768120	result. So, it's a class of results. It was introduced by Paul Kelly in his PhD thesis
4768120	4776600	that was done under the supervision of Stanislaw Ulam, who was a mathematician, a Polish mathematician,
4776600	4782760	but he's probably more famous for initiating the Manhattan Project and developing thermonuclear
4782760	4789080	weapons. So, but he also was, he's also famous for many interesting results in mathematics,
4789080	4794840	and this is one of them, the reconstruction conjectures. So, we don't know whether this is
4794840	4800120	true. So, it might be true, but that's why it is a conjecture. It would be cool if it were true,
4800120	4806440	because, of course, in this case, I could test graphosomorphism by just doing something with
4806440	4810840	this collection. So, what exactly can we do with this collection, regardless whether the
4810840	4814920	conjecture is true, right? So, it will just give us stronger theoretical property of these
4814920	4820200	architectures. So, what we can do is we can consider our graph is a collection of subgraphs
4820200	4824840	that are extracted from the given graph, right? And what is important to understand is that there
4824840	4828920	is a correspondence between them, right? Because we created these subgraphs, right? So, it's built
4828920	4834680	on the same nodes. We just might remove some edges, right? Or some nodes. So, we have here two types of
4835320	4840600	symmetries. So, we have the permutation of the nodes in the graph itself, right? And we also have a
4840600	4845560	permutation of the subgraphs in this multi-set, right? Because we don't have a canonical order of
4845560	4851400	them. So, together, basically, the structure, the symmetry structure of this new object of
4851400	4856680	this collection of subgraphs is a product of two groups, right? If we don't know the correspondence,
4856680	4861240	there will be a special type of product that allow me to skip the details. And basically,
4861240	4865320	what we can do, we can design an architecture that does message passing on each of these subgraphs
4865320	4870040	separately, but then fuses the information across graphs using these known correspondence.
4870040	4876680	And this is probably more powerful than WL, a version of this architecture that we call
4876680	4882520	subgraph union network. We can actually show that it's upper bounded by 3WL, and we hope that
4882520	4888120	it will be equivalent to 3WL. But a few weeks after we published our paper, it was shown by a
4888120	4893960	counter example that it is strictly less powerful than 3WL. So, we don't know exactly. It's surely
4893960	4900760	more powerful than 2WL, but upper bounded by 3WL, and strictly less powerful. So, we have even a
4900760	4908840	blog post about these different architectures, and there are multiple methods that are related.
4908840	4915080	So, one of them is, for example, you can do dropout on your neighbors. That was done by a
4915080	4921240	worker from the group of Roger Battenhofer at ETH, and they actually showed that this has
4921240	4926840	similar effect. So, it increases the expressive power, not only gives some kind of robustness to
4926840	4934360	the architecture. So, the last more expressive type of message passing in your network, so I
4934360	4939160	would like to mention is what we can generally call topological message passing. And if you think of
4939960	4945720	what is a graph, essentially, it's a set where you glue pairs of nodes together, right? So, every
4946600	4950680	element in a set writer, every node in the graph is a zero-dimensional topological object,
4950760	4957240	right? So, you can define this one-dimensional object, the edges that you glue to the nodes,
4957240	4960600	right? And you get the graph, but you don't need to stop here. You can also
4961480	4967720	define cells of higher dimension, right? That you forgot about, glue to cycles in your graph,
4967720	4974360	right? And we get what is called a cellar or CWL complex, right? And basically, now, instead of
4974360	4979000	traditional message passing in graph neural networks, where we exchange information between
4979000	4985880	nodes along the edges, we can also go up and down in this hierarchy. So, we can do message passing
4985880	4991400	within the same dimension, right, of the cellar complex, but we can also go across dimensions.
4992360	4998120	And this hierarchical message passing is strictly more powerful than the vice-versa-lemon,
4998120	5003640	and it's obviously very convenient for molecules, because in molecules, these structures have
5004440	5010040	some chemical meaning, and this probably is closer to how chemist thinks of molecule, because,
5011480	5016200	of course, the graph captures all the information, but it doesn't make certain structures explicit.
5016840	5020280	And in graph neural networks, for example, well, first of all, you cannot even detect by message
5020280	5024840	passing the presence of these structures. And if you want to transfer information from this node
5024840	5028440	to this node, you will need to do a few steps of message passing. Here, we can do it at once.
5029320	5035560	So, it also gives computational advantages. And again, if you want some more details about
5036360	5039320	how these different methods are related to each other, so there are many more
5040440	5045800	expressive architectures, so there is a tutorial that was given at the log conference,
5046600	5053640	and recorded on YouTube by my PhD student, Publizio Frasca, with Beatrizio Bevilacqua
5053640	5059000	and Gagai Maron. So, it's actually a very nice tutorial, and they go into much more details about
5059640	5065240	all these and other different methods for expressive graph neural networks. Any questions so far?
5076360	5082360	I have a question about summation aggregation being the maximally expressive aggregation.
5083960	5089720	Also, probably it is related. Maybe if you can also comment on the, what was it, discrete,
5089720	5096760	countable restriction on the features. Because if I imagine that our features are integers,
5097960	5103480	there are different combinations of integers that sum up to the same number. So, summing them
5103480	5111240	actually does lose the information. So, countable doesn't necessarily mean integers,
5111560	5117400	but they should not be continuous. So, why this is, without going into too much details,
5117400	5123720	basically they use the same proof technique that was used in deep nets to prove the
5123720	5128600	universality there. So, this assumption is important. If you remove this assumption that
5130280	5137880	this proof doesn't work. So, basically they apply locally kind of the result of deep net
5138520	5143160	that works on sets. All right. Thanks.
5146520	5153640	Hello. Very interesting. Thank you. I'm wondering about chemistry, whether you can encode in the
5153640	5159720	features of your graphs, also geometric information. Especially in chemistry, aromaticity is very
5159720	5165240	important. Whether it's possible to encode it directly or you need some additional layers of
5165240	5170120	information. So, some information you can probably compute. And of course, if you can
5170120	5173320	pre-comput it, if you know that these are meaningful features, then I think it makes
5173320	5178040	sense to encode them. So, the geometric information, I'm not sure that you mean
5179720	5183960	information that comes from the positions of the atoms, right? Yeah. So, I will talk about it
5183960	5187720	in a second. So, you can, basically when you deal with geometric information, you also need to do
5187720	5193320	it in a proper way. So, you need to do it in a way that is equivariant to possible transformations.
5193320	5200840	But I think the short answer is yes. In case of these, like, increasing hierarchy of the
5200840	5209240	KWL tests, is it a case that for a given KWL that we know there exists some message passing
5209240	5214440	graph neural network that exists that can do it, but we just can't construct them? Or can we say
5214440	5220040	that if we could make such a message passing GNN for such KWL, it would be, you know, intractably
5220040	5224440	huge and we would need to approximate it or something like that? So, first of all, it is
5224440	5231720	intractably huge. So, it complexes N to the power K. So, I think what is limited in practice is
5231720	5240040	3WL. So, this is what Maron describes in his paper. I don't know, depending whether you can call it
5240040	5244200	message passing or not, it depends on what you consider message passing, right? So, because
5244280	5251080	here you have more than pairs of nodes, I would argue that strictly speaking, it's not message
5251080	5255240	passing, but for example, Petr Wieliszko, which would say that it's message passing on a different
5255240	5261480	graph, right? So, it depends on the perspective. Maybe one more follow-up question for this KWL
5261480	5267480	stuff. If you have a particular application that you are interested about, are there some cases
5267480	5272040	where you can say, for this class of problems that we're working on, we can, it's enough to be
5272040	5276840	able to distinguish up such a level because the higher you go for K, obviously, like these edge cases
5276840	5282120	would get really nasty, which you might not see in your application. Well, so, it's a good question,
5282120	5288600	right? So, for example, planar graphs have WL dimension of 3. So, basically, all planar
5288600	5296600	graphs can be distinguished by 3WL test. And you can argue that molecules, right? Most of the
5296600	5301160	molecules, you can draw two-dimensional structures, maybe some of them you don't, but they're
5301160	5308600	probably a tiny fraction. So, do you need something more powerful than that? The expressive power
5308600	5313640	itself is probably not the end of the story, right? Because nothing tells you about generalization.
5315320	5323640	So, yeah, I don't think that this on its own is really the crucial consideration. It's good,
5323640	5329320	of course, to have an architecture that is, that allows to distinguish between broader class of
5329320	5333160	graphs, but if it comes at the expense of computational complexity, for example, maybe
5333160	5339080	it's a bad idea. So, you probably want some kind of good trade-off between these. Thank you.
5340520	5347720	I had a follow-up question. So, could you please define what message-passing is in this case?
5348760	5353480	Can we give a short definition? Right. So, message-passing is what I call message-passing is
5354200	5359880	these kind of architectures. Let's see where I had it. Yeah. So, basically, architecture of this
5359880	5365080	kind. So, this is the most general type of message-passing. So, we have the update of node i
5366680	5371800	from neighbor j is done in this way. So, I have a function that depends on both i and j,
5372520	5376360	that the function is parametric. So, that's learnable and then aggregate. You can actually
5376360	5379800	show that summation is what you need, right? You don't need anything else.
5383640	5390520	So, well, this is, so this is message-passing. There are higher order architectures, right,
5390520	5398360	like KGNN, right, equivalent to KWO. So, this is equivalent, in the best case, equivalent to WO.
5399160	5402520	It is not equivalent. So, the more expressive WO tests, KWO tests
5403400	5407320	are more expressive than these architecture, but then it doesn't work on pairs of nodes. It
5407320	5413160	considers bigger sets of nodes, right? So, whether you, to call it message-passing or not, you can
5413240	5418280	argue that, so some people argue that you can also think of it as message-passing, right,
5418280	5429080	just with a different graph. In my opinion, it's more a semantic question.
5429640	5448360	So, I don't know. We never really looked at it. Yeah, I don't have an answer.
5452040	5455880	So, it seems like the topological message-passing
5455880	5460600	networks and the graph substructure networks both work on the same phenomenon of identifying
5460600	5466440	and counting substructures. So, is the expressivity of the topological message-passing lower bounded
5466440	5470600	by the expressivity of the graph substructure network? No, it's slightly different, right,
5470600	5475560	because substructure networks, well, first of all, we know they're upper bound, so 3WO. Topological
5475560	5482440	message-passing depends on what kind of substructure. So, there, the kind of side information
5482440	5488440	that you assume is what kind of substructures become cells in this, in this cellular complex.
5488440	5491800	You can actually go beyond two-dimensional cells. You can go to high-dimensional cells.
5492600	5498520	We never tried to go beyond two-dimensional cells. So, depending on the choice of these
5498520	5503400	substructures, what becomes a cell, you might have different levels of expressivity. So,
5503400	5506040	they're distinct methods. I don't think that they're really comparable.
5506440	5514680	So, something that always confused me about the topological ones is where you, like, you glue
5514680	5520520	stuff to structures in the graphs to, like, make them obvious. But can you, like, glue to every
5520520	5525080	structure or does the structure need to be, like, closed? Because in previous works, it was always
5525080	5530440	either cycles or cliques, which are kind of circularly closed.
5531160	5536840	What, I think, basically, the key question is whether it defines a valid cellular complex. I
5536840	5542760	think it should be closed. Yeah, my top, yeah, I also think it should be closed, but I have no idea
5542760	5546920	about topology. Yeah, from what I remember, it should be closed. But, yeah, so it could be a
5546920	5553720	clique, for example, whether it could be a path. I don't think so. So, actually, GSNs can, like,
5554840	5560040	count structures that you couldn't, like, glue into, or, like, form into a cellular complex.
5560040	5564360	So, GSNs can count more general structures, in my opinion. Yeah, so something that doesn't necessarily
5564360	5572520	form a cellular complex. Okay, yes, thank you. Inge, I have a question. If we allow more and more
5572520	5577720	notes, the probability of collision in this representation, do we have any results that
5577720	5584520	it became less or, like, insignificant or, like, in more general, do this approach extend a bit
5584520	5592520	more into randomized or, like, stochastic versions so that maybe the guarantee is very low right now
5592520	5598840	from a sort of, well, secondary run. So, the expressiveness, like, theoretical is pretty low,
5598840	5603880	but then, if we allow a little randomness, then, actually, like, no randomness.
5603880	5607320	Sorry, you're talking about random graph models, right? Something like stochastic walk models.
5608120	5614200	Yeah. You can probably analyze what happens to this kind of graphs given
5616520	5622600	certain type of message passing architecture. I have seen papers of this kind, nothing specific
5622600	5629800	that comes to my mind. Yeah, you can probably, I don't know, compute some probability under the
5629800	5635880	assumption of certain distribution of input random graphs of distinguishing within them or not.
5636840	5637320	Yeah.
5642440	5648280	So, a question regarding the size of the graph in practice. We are talking about, like, in terms of
5648280	5655720	maybe not count and each count, like, what to say in practice for this KWL?
5657080	5661880	So, KWL doesn't scale well, right? So, if it's n to the power k, so also computational
5661960	5667240	complexity versus space complexity, I don't think that you can, in practice, go beyond
5667240	5673320	3WL equivalent. Now, there are sparse versions of these architectures, so you can do slightly
5673320	5678840	better, but I think they are mostly useful for proving theorems. So, basically, because you establish
5679480	5685080	a link to the device for element hierarchy, then you can say that basically, if your network is
5685080	5690680	equivalent to one of these methods, then you know how expressive it is. Thank you.
5692600	5698600	Yeah. So, there was a paper sometime ago claiming that most of the graphs in the most common benchmarks
5698600	5707320	can actually be distinguished by 1WL. Now, I don't know if you read about it. And my question is,
5707320	5713560	now we've seen that even in those cases, more expressive GNNs still get better results. But
5713560	5719160	then what's the reason? So, again, generalization could be one possibility, right, because you
5719160	5724200	train on one set of graphs and then you test on another set of graphs. Then,
5724840	5728440	double your tests, in general, they're designed for fish less graphs, right? So,
5728440	5732360	they only consider the structure. So, how you treat fish is also important.
5734760	5746920	Nice sense. I think we can stop here and we have, like,
5746920	5749640	have an hour for a coffee break and then we resume afterwards.
5749640	5754040	So, I will put the slide with the caffeine molecule that everybody likes. And then,
5764120	5771640	yeah, basically, we stopped at this overview of different more expressive architectures. So,
5771640	5779560	let's now talk about, guys, let's start. So, basically, the situation that we have with
5781560	5787160	the expressive power of graph neural networks, right? So, on the one side, we have the WL hierarchy,
5787160	5794200	right? So, increasingly, more powerful, more expressive graph isomorphism tests, right? And
5794200	5800600	we can find analogies between certain GNN architectures to these tests. On the other hand,
5801480	5807240	the assumption that we are given a graph and we do message passing on this graph might be
5807240	5813080	restrictive in the sense that some graphs are not friendly for message passing. And in this case,
5813080	5817800	you typically, what you would like to do is to change the graph so that message passing works
5817800	5823560	better. This is a very broad category of methods that are called graph rewiring. So, what happens
5823560	5829160	is that you have a gap between theory and practice, right? So, the theory, in order to make the link
5829160	5834600	to the WL tests, you need to use exactly the same input graph. The practice tells you that
5834600	5843080	sometimes you don't want to do it. So, as always, there is this gap. So, if you think maybe take
5843080	5850600	a step back and look at the different types of graph neural network architectures, so the traditional
5850600	5857640	approach in graph neural networks is you're given the input graph and it's both part of the input
5857640	5862120	and part of the computation, right? Because you use the input graph to send information on it,
5862120	5868680	right? So, it's both input and computational object. Now, as we've seen, you can do many
5868680	5873240	different things, right? So, you can enrich the graph with some positional or structural features.
5873960	5880040	You can lift it into a high-dimensional topological space like Simplisher or Cellar complex, right?
5880040	5885640	And do message passing on this object. You can again enrich the graph by considering a collection
5885640	5892200	of subgraphs, right? And do maybe some other more exotic type of aggregation that respects the
5892200	5900600	product symmetry group. You can also enrich your representation by considering also the
5900600	5905720	symmetry of the data, right? So, these are equivalent GNNs, if you have time, I can talk about it.
5905720	5910040	And then maybe in some special cases, your graph will have special structure like a grid, right?
5910040	5915640	And in this case, you can maybe do more specific choices. For example, you can abandon the local
5915640	5922200	permutation invariance. So, you will get back to convolutions. So, basically, the common denominator
5922200	5925880	of these approaches is that you have more structure, right? Sometimes you can assume,
5926520	5933080	sometimes you can invent, right? But that's the idea. On the other hand, you can say that you
5933080	5937960	don't like the graph that is given to you, right? And you choose to completely ignore it, right?
5937960	5944200	So, you just assume empty edge sets. So, you're back to the bare bone, right? So, the object is
5944200	5950040	just a collection of nodes, which is a set. And these are the architectures that are called deep
5950040	5954120	sets or point nets. So, that's the simplest case of graphing electrics, right? Where you don't
5954120	5958680	have a graph, you just have the nodes. The other extreme of this is when you allow interaction
5958680	5963080	between every pair of nodes, right? Again, you don't trust your graph for some reason. So,
5963080	5966280	every pair of nodes can interact. So, it's a complete or a fully connected graph. And this
5966280	5971240	is how transformers work, right? So, in this case, you will use, for example, the attentional
5971240	5977080	flavor of the graph in electrical architectures. And you will learn the right graph for your task,
5977080	5981880	right? In a sense, through the attention mechanism, for example. There is another class of methods,
5981880	5987560	and this is what we call graph rewiring, where you say, okay, I don't like the graph that is given.
5987560	5992840	I would like to change it a little bit, maybe. And this way, the graph will become, in some sense,
5992840	5998760	better for message passing. Okay, we'll talk about it in more details. So, let's talk about
5998760	6004600	graph rewiring and specifically about transformers. So, in transformers, you assume that the graph is
6004600	6009720	complete, right? So, every node is connected to every node. And if you try to apply a convolutional
6009720	6017160	style GNN architecture, right, that depends on the coefficients of the, representing the structure
6017160	6023560	of the graph, then, basically, here, the sum goes over all the nodes, and basically, this argument
6023560	6027160	is equal for every node. So, it's not informative, right? You're not adding any information.
6028120	6034600	So, you need to use at least an attentional architecture. And in this case, this is already,
6034600	6039560	this already looks like a transformer. And you can think of attention weights as a kind of
6039560	6046200	learned graph adjacency, right? That depends on this task that you're trying to solve. And this is
6046280	6051960	a special case of GNNs. Now, of course, in natural language processing, many tasks that you want to
6051960	6056760	solve do not require permutation invariance. You actually want them to be not permutation
6056760	6061800	invariance. For example, you want to depend on the order of words in the sentence. So, this is
6061800	6068280	typically achieved by adding positional encoding. So, in the simplest case, you just equip every
6068280	6072680	node with some additional coordinates that tell you where you are located in the domain, right?
6072680	6078920	Which in case of transformers, it's where you're located in a sequence. And typically,
6078920	6083320	this is how positional encoding looks like, right? So, these are just some synosoids.
6084360	6087720	So, for graphs, you can do other things, right? So, the analogy of synosoids would be the,
6088360	6094840	yeah, question? Sir, are you aware of some studies about positional encoding, especially
6094840	6099560	about relative positional encoding, which is getting some popularity in transformers architecture? So,
6100440	6106920	like, the studies which consider all invariances in those positional encoding,
6107640	6112360	like, we not always want to have absolute positional encoding. Sometimes we want to be,
6113400	6117480	sometimes some shifts or rotations are not relevant. So, do you know some literature on that,
6117480	6122360	perhaps? Yeah, so, relative positional encoding, yeah, good question. So, the analogy, let's say,
6122360	6127240	of this standard global positional encoding would be the plus and eigenvectors. So, the analogy of
6127240	6133400	local positional encoding would be something that, for example, the DGN architecture implemented,
6133400	6139480	Gabriele Corso and others directed graph networks. So, what they do, they take, for example, the
6139480	6145880	same plus and eigenvectors and compute their gradients on the edges and then transform these
6145880	6149800	features in some way. So, this gives you a kind of local direction. So, that would be probably
6150760	6158440	a good analogy of local positional encoding. So, with the plus and eigenvectors as well,
6158440	6163240	you have some ambiguities. So, there has been actually a recent paper from Derek Lim and others
6163240	6168280	from MIT where they are able to solve these ambiguities. You can use random walk kernels,
6169320	6173080	you can use substructure counting, as we've seen, right? So, there are many ways of doing it.
6173960	6179720	But basically, between these two extremes, right, whether ignoring the graph or learning the
6179720	6184520	graph, of course, it comes at the expense of complexity, which is a huge problem in large
6184520	6190600	language models where the size of the domain, right, the length of the text can be very large.
6190600	6195480	So, probably one of the key computational questions would be how to compute these
6195480	6201640	attention more efficiently. So, here somewhere in between comes the graph rewiring approaches.
6202520	6209320	And with graph rewiring, it means that your computational graph is not equal to the input
6209320	6215880	graph. And it's a little bit controversial topic because the graph being part of the input, somehow
6217400	6223080	you don't want to change the input, right? So, but the fact that many architectures do it, right?
6223080	6227880	So, if you have, for example, a very large graph like a social network where you have a lot of
6227880	6232680	neighbors in some nodes, you cannot aggregate information from millions of nodes. So, you need
6233320	6236520	to sample the neighbors, right? Which means that you are using a different graph
6237080	6242040	from the input one. So, neighborhood sampling is a form of graph rewiring, right? You can also use
6242040	6246360	graph neural networks where you bring information from not immediately your one-hop neighbors,
6246360	6252360	but also from multiple hops away, right? So, this is also some form of graph rewiring.
6252360	6257000	Transformers are also an extreme form of graph rewiring, right, where you allow access to all
6257000	6261640	the nodes in the graph. Some kind of pre-processing, right, where you pre-wire the graph for example
6262360	6264920	by some form of diffusion. Yeah, a question?
6268520	6272920	So, how do you do the neighbor sampling? Is it just like a stochastic?
6272920	6276520	Or do you... Yeah, usually you sample with repetition. So, this is what Sage did.
6277800	6282920	Do you think it's optimal or is there a way to optimize it in the best way possible?
6283560	6288920	I don't remember if they looked into it. Probably, of course, it might matter how you sample, but
6289320	6295240	I don't have on top of my mind any significant result that would tell how to do it better.
6298920	6304680	Could you elaborate more on this diffusion processes on the graphs you mentioned?
6304680	6311160	Yeah, I will get to it. So, I will go through the diffusion equations and then I will talk about
6311160	6316920	this as well. So, I'm specifically referring to Degel, the work of Stefan Gundem and his students.
6319720	6325320	So, basically, bottom line is graph is not really any sacrosanct object and in practice,
6325320	6331000	many architectures even without admitting it do some form of graph rewiring, right? So,
6331000	6335240	you can also rewire the graph throughout the neural network, so it doesn't need to stay the same
6336600	6340440	across all layers. And this has been shown efficiently, for example, in the context of
6340440	6344360	our squashing where you can do maybe a few steps of standard message passing and then
6344920	6348040	do message passing on a complete graph, right? So, like, what transformers do?
6348760	6355800	And so, the argumentation usually is that the first steps capture somehow the structure
6355800	6360680	of the graph, similar to Weisberg and Lemon, and then, basically, you accumulate this information
6360680	6367000	as features. And there is an extreme example of this. I think it's called GRIT. So, this is
6367000	6373800	from the group of field torrid Oxford and what they do is they use just something similar to
6373880	6378040	heat kernel to encode the local structure of the graph and then just use MLP.
6378920	6383880	So, there are other extremes like this, right? So, basically, you have just some kind of local
6383880	6388840	feature of the graph that you can maybe make learnable, but then the graph itself is not used,
6388840	6393800	so there is no message passing. And I think it's an open question of how much you want
6395320	6401160	to use to capture the structure in the form of some features versus in the form of the
6401240	6406040	computational architecture. So, the graph can be changed throughout the layers and, well,
6406040	6412600	one of the first architectures to do it was what we did with Justin Solomon and his students,
6413320	6418920	and that was considering problems in computer graphics. So, we have a point cloud, let's say,
6418920	6425000	of this airplane. So, every point has three-dimensional Euclidean coordinates,
6425000	6429400	and here the task is segmentation. So, you want to label each of the points on the airplane,
6429400	6435720	whether they belong to the body, to the engines, to the wings, and so on. And here we create a
6435720	6443320	graph to basically to represent the local structure of the data. And what is shown here by the colors
6443320	6449640	is a distance in the feature space, right, in the latent, in the latent feature space of
6449640	6455640	respective layers in this network to the rest of the points from the red point, right? So, initially,
6455640	6461800	this is Euclidean, as you can see that, that's basically the input space, three-dimensional
6461800	6467400	R3, but then as you go deeper, you see that it becomes more semantic. So, here, for example,
6467400	6473560	points on the same, on the engine become closer, or on the wing become closer. So, basically,
6473560	6479800	the space itself is used for the construction of the graph, and we call it dynamic graph neural
6479800	6488680	networks, or maybe not very, very, very lucky name. I think many architectures are called
6488680	6495000	like this. And we use it also in different incarnations or similar ideas under the name
6495000	6500600	of differentiable graph module, where basically you have applications where you don't know the
6500600	6505080	graph a priori. So, that's, for example, the case with medical imaging, where you have maybe
6505080	6510040	nodes representing different patients, or maybe different regions in the brain, and you have
6510040	6515000	basically two branches of graph neural network architecture, one that is computing the filters
6515000	6519880	on the graph, and another one computing the graph itself. And, of course, there is question of
6519880	6525880	computational complexity, but we're able to show that it is better to learn the graph for the,
6525880	6530520	for the task rather than to construct it ad hoc in some kind of handcrafted way.
6531480	6536520	Now, talking about message passing in general, so these are hecticly added slides based on some
6536520	6544600	discussions that we had in the coffee break. So, if you think of message passing and different
6544600	6551000	versions of it like transformers, basically, the difference is in two questions, is what's
6551000	6556120	information to send, what's information to pass, and where to pass, whether you follow the graph,
6556760	6562920	whether you pass information to your neighbors, or you pass information to distant nodes, whether
6562920	6568360	it's in K-Hope or to all the neighbors in the graph, and the question of what is how you exactly
6568360	6574200	transform your information. So, it's also interesting to add to this another question,
6574200	6579720	is when to send information? And if you look at it, basically, it's a multi-step process. So,
6579720	6585000	this is how a classical message passing neural network works. So, these are three layers of an
6585000	6589720	NPNN or three iterations of, let's say, something similar to vice-fair and lemon, and I'm sending
6589720	6595480	information from these nodes to these nodes, right? So, it takes three steps. So, first here,
6595480	6600440	then these nodes propagate information to their neighbors, and then this green node receives
6600440	6605720	information from both neighbors, right? So, in a transformer, all the information is available
6605720	6611720	at once. So, at the same moment of time, I'm sending information from all the nodes to the
6611720	6616680	green nodes, right? So, it has access to all the information across three layers. But you can also,
6616680	6623640	so, basically, here, the difference is where to send information, right? That's the structure of
6623640	6628840	the computational graph. But we don't consider here also when to send this information. And you
6628840	6637400	can imagine an architecture where, for example, you delay the information. So, here, the first node
6637480	6641880	in first iteration sends information to its neighbor. At the second layer, it sends to
6641880	6646600	two hop neighbors, right? So, this information becomes gradually available. Not like in a transformer
6646600	6652280	where you flood all the nodes with information from all other nodes at once. You make it
6652280	6656680	progressive, right? So, basically, these are kind of shortcuts that you have in the graph,
6657480	6661560	but they are made progressively as you go deeper into the architecture. You can also
6661560	6665880	make, so, you can think of this as a kind of skip connections, but you can also make skip connections
6665960	6670920	sparse in time. So, you can delay this information. And I'm saying that here, the information
6670920	6678600	from the first node comes to the next node and is delayed in time, right? So, it would arrive to
6678600	6683320	it anyway, but it would be entangled with the information in other nodes. So, I'm allowing
6683320	6690360	direct access, but I'm delaying it. And this potentially has interesting implications also
6690360	6695640	on the hardware aspect of graph neural networks, because if your graph is very large, you typically
6695640	6701960	partition it into different parts of memory, right? And the cost of message passing is not
6701960	6706440	the same, right? So, messages within the same memory cost much less than messages across
6706440	6710280	different memories. So, you can imagine a graph neural network or you are doing fast messages
6710280	6716440	within each part of memory hierarchy and slow messages across, right? So, you don't want to
6716440	6721800	wait for all the messages to arrive. You might want to do fast messages while waiting for slow
6721800	6727240	messages, right? So, this architecture that we call drill or end drill, this version with delays
6727960	6732120	potentially allows for it. So, I think it would be interesting to test it on actually some
6733800	6740360	practical hardware like graph core, for example. So, let me move to the next topic and this is
6740360	6748440	physics-inspired graph neural network. So, I promised PDEs, so I need to hold this promise. And
6749240	6754840	let's again take a step back and look at different objects that we've seen so far, right? And also,
6754840	6760280	you can argue objects that are studied in this broader field of geometric deep learning. So,
6760280	6765080	let's say grids, meshes and graphs, right? So, you can think of them as more or less the same thing,
6765080	6770360	just with more structure, right? So, meshes in addition to have also triangles. So, these are
6770360	6775240	simplicial complexes. So, graphs with some extra constraints or extra structure. Grids are also
6775240	6781320	special type of graphs where we have certain organization. So, if you look at the grid and
6781320	6786040	you look at how you aggregate information from your neighbors, there is no ambiguity, right? In a
6786040	6791000	grid, unlike a general graph, I can order my neighbors in a canonical way, right? I have a top
6791000	6796600	neighbor, I have a left neighbor, bottom and right, like shown here. So, it is totally unambiguous,
6796600	6801880	right? So, this ordering is fixed. On a mesh, the situation is slightly different. So, I can pick up
6801880	6807720	my first neighbor and then I can order all the rest of the nodes, all the rest of the neighbors
6807720	6813560	in, for example, clockwise orientation. And this is possible because mesh is a discrete manifold.
6813560	6819320	So, it's locally Euclidean. I have this meaningful ordering, right? But, of course, the choice of
6819320	6824280	the first neighbor is ambiguous. So, I can rotate everything, right? So, the ambiguity here is up
6824280	6829800	to rotation. In a graph, as we've seen, any permutation works, right? So, everything is defined
6830360	6835400	up to a permutation. So, in a sense, graphs have the least structure out of all these objects,
6835400	6841960	right? So, second observation is that if I look at grids and meshes, I can think of them as
6841960	6848280	discretizations of some continuous spaces. So, a grid is discretization of a plane or a mesh is
6848280	6854280	discretization of a two-dimensional surface or a manifold. We don't have immediately this analogy
6854280	6858920	for graphs and even though there is an entire field that is called network geometry that tries to
6858920	6865400	think of graphs as some discretization of continuous space, it would be nice to have
6865400	6872280	some continuous models for GNNs, right? And that's the idea of what we call physics-inspired GNNs.
6872280	6878760	So, you can consider some class of graph neural networks as a dynamic system. So, you have
6878760	6883000	particles that live in the dimensional feature space. And let's assume that the dimensionality
6883000	6888360	always is kept the same, right? It doesn't need to be in general graph neural networks. So, every node
6888360	6893480	is some point, some particle that moves along a trajectory that is shown by this red line here.
6893480	6898040	So, a GNN is essentially a dynamic system, right? That is governed by the system of differential
6898040	6908600	equations. F here is some coupling functions, right? That makes dependency between different
6908600	6913880	particles. And it depends both on the particles and the graph. And it's also parameterized by some
6913960	6919080	trajectory of parameters that are, you know, by data. So, T is a continuous time parameter.
6919080	6923720	I can discretize it and this corresponds to layers of a graph neural network. And the graph,
6923720	6927800	right, you can think of it either as a coupling function, right? So, this F here, how different
6927800	6934360	rows of this matrix interact with each other or discretization of some continuous space,
6934360	6943400	as we'll see in the next few examples. So, basically as the evolution equation,
6943480	6947560	right, that governs the behavior of these particles, I can put here more or less anything,
6947560	6952760	right? So, there's plenty of different differential equations that describe different systems. But
6953720	6959000	probably the first thing that comes to your mind when you think of propagation or diffusion of
6959000	6965160	some stuff is the diffusion equation, right? And this, in fact, was one of the earliest
6965160	6971160	mathematically analyzed physical phenomena, right? The diffusion of heat, analyzed by
6971160	6976760	non-Elst and Newton himself in an anonymous paper written in Leighton in the transactions
6976760	6981400	of the royal site. So, it's interesting actually that the journal had a mixture of papers written
6981400	6989160	in English and Leighton. And it was anonymous. Well, Newton devised an experimental setup where he
6989160	6997960	heated pieces of different objects, metal, I think, mostly, and then he looked with his
6997960	7003800	self-made thermometer here, measured how much heat is lost over a period of time and devised some
7004680	7009400	law that is formulated in modern terminology like this. What is nowadays called the Newton
7009400	7014360	law of cooling, which says that the temperature that the hot body loses in a given time is proportional
7014920	7018760	to the temperature difference between the object and the environment. He actually didn't use the
7018760	7023800	term temperature. That's modern terminology. He used the term heat, which has a different meaning
7024360	7031240	or color in Leighton. And somehow, everybody guessed that it was his paper, even though he
7031240	7037240	didn't sign it. Now, it took some time until this process was fully understood. So, Fourier
7037240	7044840	devised the local version differential equation that governs heat diffusion and then thick
7045720	7052040	in the late 19th century defined what we nowadays understand as diffusion equations.
7052040	7056600	So, if you want to apply this idea to a graph, we can consider a diffusion process on a graph.
7057240	7062040	And that's how it looks like. So, here x is some quantity that is being diffused. Think of it as
7062040	7068200	temperature if it's color. So, a node has certain temperature at time t. So, t is a continuous
7068200	7073000	variable. And what is written here is the self-temperature of the node. This is the temperature
7073000	7078280	of the environment. So, it's the average of the one hop neighborhood of the node. And this is the
7078280	7085000	rate of temperature change. So, there might be some proportion coefficient here. So, if I rearrange
7085000	7090600	the terms on this expression, I can write it like this. So, I'm slightly massaging the formula.
7090600	7095960	What is written here is called the gradient. So, it's just the difference between end points
7096680	7102760	of an inch. So, I take the feature here and the feature here and subtract one from another. So,
7102840	7108120	that's the graph analogy of the standard gradient operator from classical calculus.
7108840	7112920	And what is written here is, again, the discrete analogy of the divergence operator,
7112920	7120040	again, from basic course in calculus. So, in terms of operators, you can think of
7120840	7126040	the features as that they live in the nodes of the graph as a scalar field. Then the gradient
7126040	7130920	makes a scalar field into a vector field, right, that lives on the edges of the graph. And then
7130920	7136600	the divergence does the opposite. So, it collects the information from the edges that live, that
7136600	7140520	emanate from a node and basically sums them up maybe with some different weights, right? So,
7140520	7145800	that's what the divergence, right? So, gradient makes a scalar field into vector fields. Divergence
7145800	7151480	makes vector fields into scalar fields. Acting together, they take a scalar field into a
7151480	7156040	scalar field. The divergence of the gradient or minus divergence of the gradient in our notation
7156040	7159800	is what is called the Laplacian operator, right? So, what is written here is the
7159800	7164760	graph Laplacian operator and by definition, you see what it does. So, it compares you to your
7164760	7169240	neighborhood, how different you are from the average of your neighbors. And this is really a
7169240	7174040	very important operator. It comes everywhere in mathematical physics and from quantum mechanics
7174040	7181400	to wave equations to diffusion equations, and particularly important here. So, what else is
7181400	7185880	the heat equation? So, this is very simple, right? So, this is what we call homogeneous
7185880	7190360	isotropic diffusion equation. Basically, heat propagates everywhere in the same way, right?
7192440	7198440	What else heat equation is? It's an example of prototypical gradient flow. And gradient flow
7198440	7203400	is a type of evolution equation, differential equation that looks like this. So, the step at
7203400	7208600	every point of time is in the direction of the minus gradient of some energy, that you know here by
7208600	7213320	E. And the energy that corresponds to the diffusion equation is what we call the Dirichlet energy.
7214200	7218920	So, the Dirichlet energy, you can write it as a quadratic form with respect to the Laplacian
7218920	7223080	operator. And it measures the smoothness of the node features, right? How different you are from
7223080	7228680	your neighbors. So, the smallest value it can achieve is zero. In this case, all the features are the
7228680	7233640	same, right? And you can show that Dirichlet energy decreases along the flow. So, if you run the
7233640	7239960	diffusion to infinity, all will become constant, right? So, the heat will basically propagate
7240040	7245160	on the domain and everything will have the same temperature. So, this is what we typically call
7245160	7249160	over smoothing in graph neural networks, right? And I should say that over smoothing is not
7249160	7253080	necessarily a bad phenomenon. So, usually it's described as a kind of catastrophe that happens
7253080	7257640	in GNNs and prevents us from making deeper architectures. But it can actually be a very
7257640	7262760	benign thing, right? So, you can imagine without any learning. So, if I give you a graph and I
7262760	7268600	have labels of a few nodes, and if I assume that the structure of the graph is homophilic in the
7268600	7273800	sense that my neighbors are expected to have labels similar to me, then I can just diffuse this
7273800	7277880	information so it will be, you can think of it as a heat diffusion equation with boundary conditions.
7278920	7285160	And by just doing this, it is very likely that the result will be very good, right? If the graph
7285160	7290120	is not homophilic, of course, you need to do something else, maybe work harder, but on its own,
7290120	7294040	the over smoothing is not necessarily bad. Yeah, there is a question.
7299480	7306520	Actually, I have like two questions on this topic. So, first of all, can I see this as the,
7307800	7316440	in some sense, the message passing algorithm there, but via the ordinary differential equation between
7316440	7325560	the edges? Right. So, well, whether they call it message passing or not, I think it's a good
7325560	7329960	question. So, every iteration when you discretize this differential equation, every step will
7329960	7334520	correspond to message passing. Now, in some cases, you can actually have a closed form expression
7334520	7339240	for the solution at any time t, right? This is called the heat kernel. So, it will look like
7339240	7345880	the exponential of the Laplace matrix. So, I can compute the value of the temperature at every
7345880	7353560	point instantaneously without doing these microsteps of message passing. So, whether they call,
7353560	7357080	still call it message passing or not. So, this is where semantically I disagree with better,
7357080	7361800	for example. He still suggests to call it message passing. I think it's not. So, I don't know how
7361800	7366920	to call it better, maybe some kind of spatial coupling, right? Because you have, you have direct
7366920	7374360	information to potentially infinite, all the nodes in the graph, right? So, it is something that
7375480	7380680	doesn't require propagating information explicitly, right? You might be, when you solve the
7380680	7386040	diffusion equation, especially with its own linear, you might not have a choice. But in some cases,
7386040	7392680	you do. You have closed form expressions, right? Okay, thank you. And the second question, because
7392680	7400040	I suppose that here is like the mostly ordinary differential equations. Is there any research
7400040	7408280	to introduce stochasticity there to be, let's say stochastic differential equations, something
7408280	7416760	like that, and when it could help in the, in the, in this field? Yeah. So, it's interesting. I think
7416760	7422120	there have been some works. And there are some works. So, it talks for example, a big
7422120	7428120	expert in this domain is Terry Lyons. So, I think they are working on, on these topics.
7429640	7432440	So, we are not considering stochastic differential equations. We are considering
7432840	7439000	whether to call them ODs or PDs. So, these are coupled ODs, right? When you discretize,
7440040	7444280	a partial differential equation becomes a system of coupled ordinary differential equations. I think
7444280	7451320	they, again here, the terminology is more semantic difference, whether you have a continuous spatial
7451320	7457080	coordinate that you want to discretize. So, that will be the next part of it. Okay. Thank you.
7457640	7463880	Right. Okay. So, that was basically, that was the gradient flow, right? And again,
7463880	7468440	an example of, of the heat diffusion equation as, as a prototype of the gradient flow. So,
7469000	7475320	if you look again at the, at this view on, on graph neural networks as some kind of
7475960	7481560	evolution equations governing some, some physical system. So, the traditional approach
7481560	7485720	to graph neural networks is to take this differential equation, discretize it, and then
7485720	7489160	parameterize the evolution equation, right? The discrete evolution equation. So,
7489720	7493800	every step of an iterative solver here will correspond to a layer, and then you parameterize
7493800	7499560	every layer, right? So, that's how you can view graph neural networks. So, instead, what we can do,
7499560	7506040	we can start with, with an energy, parameterize an energy, and then derive the evolution equation
7506040	7511080	as a gradient flow. So, apparently, there is no difference, right? But the big difference would
7511160	7517240	be that we will, it will allow us to make certain architectural choices. So, it will
7517240	7522600	restrict our space of all the possible architectures that we can do here, right? Like, for example,
7522600	7527640	the form of message passing. That will have better interpretability. And of course, I know that
7527640	7532200	interpretability is maybe a bad word in machine learning. So, what I mean here is that we will
7532200	7539160	be able to guarantee that certain things happen or not happen. Okay. And I will be more specific
7539160	7544760	in a second. So, let's consider the following parametric energy. So, we call it generalized
7544760	7550920	Dirichlet energy. It has these two terms. So, you can think of it as an energy that a system of
7550920	7556120	particles has, and it's parameterized by two matrices of size d by d, right? d, remind you,
7556120	7559720	that's the dimensionality of the features. So, that's the space where the particles move.
7560600	7564920	And we have two terms here. So, the external energy term, it acts on all the particles. So,
7564920	7569800	think of some force that moves them in some directions. And we have internal energy. So,
7569800	7575240	these are the interactions between particles along the edges of the graph, right? Think of maybe
7575240	7582440	colonic interactions, right? Or maybe springs that attached to some pairs of particles. And we
7582440	7587160	have two types of interactions. So, we have attractive interactions, and they happen along the
7588040	7594120	eigenvectors of the matrix W that corresponds to positive eigenvalues and repulsive interactions
7594200	7602200	that happen along negative eigenvectors. And you can see an example here. So, here the space is
7602200	7607240	two-dimensional just for visualization purposes. And the graph here, you see that it's heterophilic.
7607240	7612600	So, the colors of the nodes represent the labels. And the positions represent the features, right?
7612600	7617560	The feature coordinates x and y. So, the graph is actually perfectly heterophilic, right? The
7617560	7623800	blue nodes have only red neighbors, and the red nodes have only blue neighbors. And yet,
7623880	7628040	we can find directions. So, the horizontal direction is repulsive direction, where the
7628040	7633400	particles are separated. And the vertical direction is attractive direction, where the particles
7634360	7641240	cluster together. We can perfectly separate these types of nodes, right? So, if my task is
7641240	7647080	node classification on this graph, by this very simple process, I can solve this problem, right?
7648120	7652760	So, we will see in a second that this corresponds to a convolutional architecture. By convolutional,
7652760	7657320	I mean that my diffusion operator depends only on the structure of the graph, but not on the features,
7657320	7662120	like in more complicated, for example, attentional networks. So, I can write it as AX something,
7662120	7668360	right? So, this is what was our distinction. So, in convolutional architectures, we had something
7668360	7675400	like this. In attentional architectures, we had something like this, right? So, that would be
7675400	7681080	the GCN type of architectures, and this will be the GUT type of architectures. So, the fact that
7681080	7685080	this matrix is constant, it depends only on the structure of the graph, is what I call convolutional
7685080	7691720	architectures, right? So, basically, these goals against this folklore that I mentioned in the
7691720	7696760	beginning, that convolutional architectures are not good for heterophilic graphs. So, you can see
7696760	7703160	that they can work very well, right? So, we need to dive deeper and understand what happens here.
7703880	7707080	So, if we write the gradient flow of, yeah, question.
7707400	7711720	Is there a link between this gradient flow and less and less popular recently,
7714840	7719480	probabilistic graphical method, especially if you showed the previous slide, it was very
7719480	7724360	similar to restricted Boltzmann, right, when we had this bipartite graph?
7724360	7728760	Yeah, you can probably interpret it from, right? So, gradient flows are very basic objects. So,
7728760	7732520	it's essentially steepest descent is also a gradient flow, right? So, it's a continuous
7732520	7739080	version or a variational version of it. Yeah, so, probably there are links to many different things.
7740680	7744600	So, basically, you're minimizing some energy here, right? So, and this energy,
7744600	7749800	so, it's not the learning part, right? So, it's the inference. So, applying a neural network
7750840	7755720	minimizes some energy, right? You design the network, so, it minimizes some energy, right?
7755720	7760520	And the energy is parametric. So, what kind of energy to minimize is what you determine
7760520	7766280	based on the task. So, that's what is done by back propagation. So, basically, the gradient flow,
7766280	7770840	you just differentiate this energy with respect to its parameters, right? By data I denote here,
7770840	7775480	these two matrices omega and w. And one thing that you notice immediately that they all appear
7775480	7780680	in symmetrized form, right? So, they appear in quadratic terms. So, they appear as omega plus
7780680	7784920	omega transpose. So, we can just assume that they are symmetric to start with, right? So,
7784920	7789320	that's already first restriction that comes from this assumption of the gradient flow.
7789960	7793160	The second thing is, again, this is by the design of the energy that
7793880	7799640	the parameters are time-independent, right? So, they don't depend on t, right? Which will become
7799640	7804840	the layer number. And if we discretize it, this is how we discretize. So, we replace the temporal
7804840	7813720	derivative with forward difference. So, this is what is also called the explicit Euler scheme.
7814680	7821880	So, we have some step-sized tau. And this gives residual convolutional type GCN, again,
7822600	7827000	the convolutional type GNN. Why I call it convolutional? Because this matrix A, right,
7827000	7831480	that's where the diffusion happens, where the message passing happens, right? When I send
7831480	7839400	information across adjacent nodes, it's not dependent on x. It's fixed, right? So, you can call
7839480	7845480	it a kind of a convolution. The weights are symmetric and the weights are shared across
7845480	7851560	different layers, okay? So, the way that you can use it, you can optionally do some nonlinear
7851560	7856280	encoding typically to reduce the dimensionality. So, we have some fixed low dimensional dimension
7856280	7862680	of this space. You apply a linear gradient flow. So, all the propagation of information on the
7862680	7868280	graph is linear. So, there are no nonlinear activations, right? And then you do some decoder.
7868280	7871800	That's for node classification, right? So, that's how the architecture looks like.
7872520	7876920	Now, if you compare it to the classical convolutional architectures like GCN of
7876920	7882040	Kieff and Welling, so there are several differences. So, the GCN is non-residual,
7882760	7888360	the weights are non-symmetric, and the weights are different per layer, right? And also,
7888360	7893640	they use nonlinear activation for every layer. We don't. So, in a sense, it's a kind of antithesis
7893720	7899880	to a typical graph neural network or a typical deep learning architecture. So, typically,
7899880	7904040	you say that you need many layers, each layer parameterized separately and nonlinear are they
7904040	7908600	activated. We don't have anything like this here, right? So, all the, again, the diffusion part is
7908600	7915400	linear. We have a nonlinear decoder and potentially a nonlinear encoder. So, what we gain from the
7915400	7921080	it being gradient flow is interpretability in the following sense that we can show that
7921080	7926040	in certain situations we can induce both low and high frequency dominated dynamics.
7926040	7931880	I will define it in a second. And what it means is that we can work both with homophilic and
7931880	7936760	heterophilic graphs. Now, because the weights are shared across layers, actually the number of layers
7936760	7942040	becomes a completely irrelevant notion here. So, what matters is really the diffusion time.
7942760	7946440	The number of layers is just how finely I discretize my differential equation.
7947400	7949880	And the number of parameters is independent of it, right?
7950920	7954600	Unlike, again, the classical architecture where the more layers you have, the more parameters
7954600	7960520	you have. Yeah, question? Question. Why don't you stay in the latent space? Why do you have
7960520	7966440	needed a decoder? Why do I need a decoder? Well, the decoder can, for example, you might need it to
7966440	7974600	produce a label for an old. It's not the decoder in the original space. It could be in any,
7975240	7978440	it could be also decoding the original space, right? So, you might, for example,
7978440	7983320	want to work with low dimensional space and then the number of your classes might be different,
7983320	7989480	right? But the important part is that it's nonlinear. So, all the nonlinearity goes there.
7990920	7995080	Okay. And, well, you can also use an encoder. I will show why encoder can be problematic.
7995080	7997960	We actually, we have experimental evidence that it's not needed.
7999320	8003720	So, first of all, let's talk about homophilic and heterophilic graphs. So, again, homophilic
8003720	8008600	look like this, heterophilic look like this. And the data set here is what we call synthetic
8008600	8015000	core. So, it's probably you're all familiar with it. So, it's this citation network where we can
8015000	8019960	actually change the structure of the graph in a way that it becomes either more homophilic or more
8019960	8026440	heterophilic, right? And here are the two extreme choices of an architecture. So, the task here
8026440	8032200	is no classification. I can either ignore the structure of the graph altogether and do just
8032280	8037480	no wise predictions with a multilayer perceptron. And, of course, it's not great, right? So, it
8037480	8043960	achieves about 67% accuracy. But no matter how homophilic or heterophilic data set is,
8044600	8050360	the result is the same because it simply ignores the graph, right? The other extreme is a GCN.
8050360	8054600	So, when the graph is homophilic, it works extremely well, right? Almost 100% because
8054600	8060040	when my neighbors contain similar information, I will be basically averaging them and I will
8060040	8064920	be doing some form of denoising. But when the graph is heterophilic, then it degrades very
8064920	8068920	badly because, in this case, the neighbor information is more detrimental than helpful.
8069640	8075320	So, the gradient flow framework, it benefits from basically its kind of mixture of both worlds. So,
8075320	8081960	it works as well as GCN in the homophilic case and as well as node-wise predictions in the
8081960	8087560	heterophilic case and the graceful transitions between the two. Now, another important thing,
8087560	8092440	right? And that's where the encoder and decoder question comes into the play. So, if you write
8092440	8097960	the output of the neural network after L layers, this is how it looks like. So, it's basically
8097960	8101560	it's a polynomial in these matrices. So, the matrices are dependent, of course. So, there are
8101560	8108440	some powers here. The parameters are shared. But if we ignore the encoder, right, and we can ignore
8108440	8115480	it, at least in the test that we did, basically what the diffusion part can be precomputed,
8115480	8120440	right? You see that it acts as some kind of powers of the diffusion matrix on the features. So,
8120440	8126280	I can pre-diffuse the features once as a pre-computation step and then it all boils down to node-wise
8127720	8133960	multiplications by these matrices. So, the computationally difficult part, both computation
8133960	8138760	in terms of the number of multiplication operations as well as the memory access, which,
8139560	8145160	unless the graph is very structured, you have random access to your neighbor nodes,
8145800	8151320	this is really the heaviest part of graph neural networks. So, this now can be totally trivialized.
8152440	8157400	So, we've done the earlier versions of this architecture with just one convolutional layer.
8157400	8163000	We call it sine. So, already several years ago, we tested this on the graphs of hundreds of millions
8163000	8167960	of nodes. So, with this architecture, actually, because now we also get rid of the non-linearity
8168760	8171800	you can apply it to a very large graph, basically, you can design
8171800	8175480	multi-layer architectures that work well both in homophilic and heterophilic settings.
8176920	8183080	Now, another thing, and this is what I mentioned regarding the nature of the dynamics that is
8183080	8191880	induced by this gradient flow, is if we look at our data, right, and our data, I remind you, it's
8192840	8198760	the matrix x, right, which is of size n by d and is the number of nodes, d is the number of dimensions.
8199960	8204440	So, we can do an analogy of a two-dimensional Fourier transform, right. I remind you that
8204440	8209240	for a graph that we assumed, an undirected graph, the graph of plus n has orthogonal
8209240	8214200	identity composition. So, it's eigenvectors form an orthogonal basis, right, the basis for the rows
8214200	8220360	of the matrix, and the matrix w, which we assumed by virtue of our process being a gradient flow,
8220360	8224760	right, it was symmetric, we can have also an orthogonal identity composition, let's call
8224760	8230360	it eigenvectors psi and eigenvalues mu, so it forms the orthogonal basis for the columns of
8230360	8237160	these matrix, right, for the dimension d of these matrix, and now in these two-dimensional Fourier
8237160	8243880	basis, right, we can take tensor products of these basis functions of phi and psi, we can write the
8243880	8251320	output of the neural network like this, right, and what you see here is that we have some filter
8251320	8256600	that acts on our signal, right, so this is signal, so these are tensor products, right, so that's
8256600	8263640	the analogy of two-dimensional Fourier transform, sinusoids of sine mx by sine ny, something like
8263640	8271080	this, right, so that's our analogy of this, so this is a filter, and it works both with the
8271080	8277880	frequencies, the eigenvalues of the graph Laplacian and the matrix w, right, lambda and mu, and you
8277880	8283720	see that the low frequencies of the graphs are magnified by the positive eigenvalues of w,
8283720	8288360	and conversely the high frequencies of the graph are magnified by the negative eigenvalues of w,
8288360	8294520	and you can show that if we choose the matrix w in such a way that it has sufficiently negative
8294520	8299880	eigenvalues, then this gradient flow dynamics is high frequency dominant in the sense that the
8299880	8304520	Dirichlet energy or more correctly the normalized Dirichlet energy doesn't converge to zero,
8304520	8308680	so it means that we don't have over smoothing, right, in the case of over smoothing this thing
8308680	8315000	would be going to zero, but here it doesn't, right, so it means that we have some process that
8315000	8322600	doesn't diffuse everything to a constant, it does something more interesting, right, and the
8322600	8328360	condition for it is w having sufficiently large negative eigenvalues, so the analogy of this would
8328360	8334200	be, so if you think of diffusion processes blurring, what we have here is sharpening,
8334200	8340120	and we have both processes at the same time, so the attractive interactions do blurring,
8340120	8344440	the repulsive interactions do sharpening, so we have some directions in the fissure space where
8345240	8352680	these processes happen at the same time, okay, questions? Yep, I think there are many questions,
8352680	8359080	so it was very unclear. So in these situations where you don't
8360360	8364840	dissipate to a constant value, are you obtaining some sort of chaotic behavior,
8364840	8372200	or are you obtaining other periodic oscillations of? So this is a sympathetic analysis, we don't
8372200	8376040	know, I don't think that I have an answer to your question, it might be that it's,
8376360	8385880	so whether we have monotonicity, that's the question, I don't think so, but it might be the case.
8394920	8400680	So if eigenvalues of w are sufficiently negative, then we can just kind of beat over squashing,
8401480	8410120	over smoothing, and the question is, is it possible, is it hard to do so because there are
8410120	8415160	two dynamics, is it hard to get those dynamics right, does it require a lot of hyperparameter
8415160	8421240	tuning or something like that? So it's a good question, so you can force, you can structure the
8421240	8427160	matrix or parameterize the matrix w in such a way that it has two parts, positive eigenvalues and
8427160	8432440	negative eigenvalues, and that's what we do, so basically we help the architecture to have both,
8432440	8437880	and then learning becomes easy. To learn it completely from scratch might be difficult,
8437880	8443960	and we see that GCNs which are in principle the same architectures without this restriction
8443960	8455640	often fail to do it. My question went the same direction, but after the previous answer I got
8456600	8463560	more confused. So those, the matrices which, what's it, the delta and the w, they are learned,
8463560	8469480	right, they are not hyperparameters. W, the elements of the matrix w are learned, right,
8469480	8474280	but the matrix w has constraints, right, so it's, for example, it's symmetric, right, so it has half
8474280	8480520	of the elements of a general matrix w, and then basically by virtue of this theorem what it suggests
8480680	8485880	is that we need to further restrict the structure of w, for example, to make it have negative
8485880	8493160	eigenvalues. So typically what we do, we decompose it into a positive symmetric part and a negative
8493160	8500040	symmetric part, and basically this way we guarantee that it has both positive and negative eigenvalues.
8501560	8508680	Okay, so what is the principle component that makes graph work on heterophilic graphs better
8508680	8515800	than GCNs, and can you somehow update GCNs in a way that they will also work on heterophilic graphs?
8515800	8519560	Yeah, so residual connection is a must. We actually show that without residual connection this
8520200	8527000	doesn't happen. Well, the nonlinear activation, I think it's more a complication for the analysis.
8528920	8535800	We don't know how to analyze this, basically with this nonlinearity you cannot regard it as a
8535800	8544440	diffusion equation. So, yeah, basically residual connection and then nonlinear eigenvalues.
8544440	8550280	If you, and remove the nonlinearity, whether, if you don't constrain w to have non-negative,
8550280	8555640	or if you don't help the architecture to learn w with negative eigenvalues, you might never be
8555640	8561480	able to learn it. So we've seen this happening as well. Okay, and what is the role of symmetric
8561480	8566520	matrices, symmetric matrices w? So, symmetric comes from the assumption of gradient flow.
8568520	8573000	So, anything that looks like a gradient flow for this kind of energy must be symmetric.
8573960	8578680	I see, and what does it mean in terms of operations on your graph, in terms of message passing?
8579320	8584360	So, this is not related to message passing because w is channel mixing matrix attacks on the fissures.
8584440	8597560	I was, I was wondering, what is the relationship? So, for example, you have this gradient flow
8597560	8602440	and say you want to learn some energy with respect to which you flow. What's the relationship between
8602440	8608120	learning the energy and learning a metric to which with you are computing the gradient? Because if
8608120	8613080	you change the metric, then the way in which you flow is also different, right? Is other equivalent
8613080	8619000	in the learning a metric equivalent to learning an energy? Yeah, well, you can think of it
8619000	8624760	indeed in this way, right? So, what is the interpretation, physical interpretation of
8626200	8632360	of this matrix w? So, if you look at the way that Dirichlet energy looks, right, it looks like,
8632360	8638520	right? So, it's something like this. So, do we use, yeah, so let's say continuous version of Dirichlet
8638520	8643640	energy. So, it will be the norm of the gradient of x at some coordinate u, let's call it. So,
8643640	8648200	u would be the index of the null, right, the continuous version, right? And this is, this is
8648200	8655000	the Dirichlet energy and let's write it on some domain omega, right? So, that would be our continuous
8655000	8662920	version of Dirichlet energy. Now, what is written here when omega in general is a manifold,
8662920	8669480	a remaining manifold. So, what is written here is the remaining metric of this kind, right?
8671640	8677800	I hope you can see it. So, basically it's inner product defined at the position u, right? And
8677800	8681480	the way that you can write it, so you can write it using remaining metric tensor, which is exactly
8681480	8689480	the w, right? So, this is something that scales the coordinates of x, doesn't need to be fixed,
8689480	8695320	by the way. This w in general can be position dependent, right, on the remaining manifold.
8695320	8701640	So, a more general construction would allow w to depend on the position, maybe not explicitly,
8701640	8707400	because that would be a huge number of parameters that scales with n, maybe it will be done some,
8707400	8712200	through some form of attention. So, w will, or maybe a positional encoding, right? So,
8712200	8717560	w will be a function of positional encoding of the nodes of the graph, right?
8720120	8725160	So, there are interesting analogies and potential directions of extending this, using
8726920	8732520	basically this is a harmonic energy of an embedding of some manifold, right? Thanks.
8737800	8741800	Yeah, sorry, this will be probably a little bit like far-fetched question,
8741800	8747960	but like in general, like those equations seem to be, seem to be like kind of similar to what you
8747960	8755400	often get when you analyze like the signal propagation or the type of initializations
8755400	8761880	in the neural networks, or like the dynamical isometry property. So, for example, the requirement for
8761880	8768760	residual connections also appears in there. And like, are you aware like whether there's like
8768760	8775480	any connections between, between, between this work and, yeah.
8775480	8781320	Potentially. So, the closest analogy is neural ODE's, right? Well, these are neural, we like to
8781320	8788200	call them neural PD's, but they're coupled ODE's, right? So, neural ODE's, each row of these magics
8788200	8794040	will be separate, independent. Here, we also have the extra complexities coupling, right?
8794520	8801720	So, like, would you say like this, there's something universal in all those?
8803720	8805480	I'm not sure what do I mean by universal.
8810520	8814680	Yeah, I'm not sure either, but like, it's basically like,
8814920	8825320	like, for example, like this residual rule appears very often in different types of.
8825320	8830920	So, residual rules rule here comes just from discretization. So, that's how you, you discretized
8830920	8835080	the temporal derivative. You could discretize it differently. So, you can use a backward scheme,
8835080	8839560	right? And then it would be implicit. So, you will need to solve a linear system to,
8839560	8843240	to get your next iteration. This actually has been done with diffusion equations. So,
8844200	8848120	there are advantages to it because these kind of discretizations are what's called
8848120	8854040	unconditional stable. But in terms of, well, universality may be not the right term, but
8854040	8859160	basically what you have here in practice is a kind of controlled differential equation,
8859160	8864280	right? So, the control is through the privateers W. So, they're time independent, but in principle,
8864280	8869400	you can think of time dependent trajectory. So, you have a controlled PDE that you discretize,
8869400	8873880	and then expressive power becomes a question of, can I reach a certain state of the system,
8873880	8877160	for example, in finite time by choosing the right trajectory? Or how far can I be
8877720	8883480	from, from that state, right? So, universal approximation means that in finite time I can
8883480	8889560	reach, I can be epsilon close to any state that they want. Generalization, for example, can be
8889560	8894440	probably formulated as some kind of perturbation, right? So, I change my initial conditions. I want
8894440	8900200	to see what happens to the system. And there are actually, there are some results, theoretical
8900200	8904840	results that show that architectural choices, like for example, having symmetric matrices might be
8904840	8911640	crucial for these properties. So, I think there is a lot more to explore there. Okay, thanks.
8914600	8919080	Okay. So, more questions? Yeah. So, let's, let's move on with this stuff. So,
8920040	8925480	obviously, right, so here, the conclusion was that we have no over smoothing, but we can also
8925480	8931800	consider more interesting equations. So, so far we consider the very simple isotropic homogeneous
8931800	8936600	diffusion equation. We can also consider nonlinear versions of the diffusion equation. And this one,
8936600	8942760	in particular, comes from the domain of image processing, where imagine that you start with an
8943640	8950760	image like this, right? So, the portrait of Sir Isaac Newton that is noisy. So, if you run a diffusion
8950760	8955240	equation on an image, it actually has a closed form solution. So, it's convolution with the Gaussian
8955240	8960200	kernel, where the variance of the Gaussian is proportional to the time of the diffusion, right?
8960200	8965320	And in the limit, you will have just everything flat, right? So, you average all the pixels in
8965320	8971400	the image. You see that you don't want to have results like this, because it might average out
8971400	8976200	the noise, but it also destroys the discontinuities in the image that, for visual perception, are very
8976200	8985640	important. So, the idea of, that was originally by Peron and Malik in 1990 is to have a nonlinear
8985640	8991560	diffusion equation that is controlled by the gradient of the image, right? So, basically,
8992120	8996840	if you're in a smooth region in the image, like here, so you have standard Gaussian kernel,
8996840	9001880	but the moment you reach a discontinuity, you slow down the diffusion. So, the effect it has,
9001880	9007560	you don't average pixels of different intensity. So, here, I'm not averaging dark and white, right?
9008760	9013880	So, the kernel will look like this. It will look one-sided. And it had a lot of different versions,
9013880	9019640	bilateral filters, non-local means filters and so on, but the idea is always the same. So,
9019640	9026520	here, basically, the diffusion speed, right, is inversely proportional to the edge indicators,
9026520	9030760	to the norm of the gradient. And the result that it produces is like this. So, this nonlinear
9030760	9037000	diffusion equation knows where to stop locally, and therefore, it averages within smooth regions,
9037000	9041560	and it doesn't average across regions. Now, we can do the same thing on a graph, obviously. So,
9041560	9046600	the analogy would be this, right? So, here, we have a gradient, right? This is the divergence,
9046600	9051720	and that's some parametric function that looks suspiciously like a tension, and that's the
9051800	9056840	diffusivity, right? So, it's the local strength of the diffusion. And, in fact, if we discretize it
9056840	9064200	again with explicit forward Euler scheme, then a particular version of this equation corresponds
9064200	9075240	to the attentional architecture. So, this is a gut. But this was so far, this was a continuous
9075240	9080680	time, right? And we wanted continuous space. So, the original motivation, right, when we compared
9080680	9086680	graphs to other objects was somehow to have a continuous analogy of the graph neural networks,
9086680	9092200	and if, again, we take a step back and look at how diffusion equations work in the plane,
9093320	9098120	when I discretize the plane as a grid, I don't really have a canonical graph, right, in the sense
9098120	9103320	that there are many ways I can discretize my differential approaches, right? So, this is how
9103320	9108200	I can discretize the Laplace-Anon grid. So, I can use neighbors like this, or I can rotate everything
9108200	9114920	by 45 degrees, I can use distant neighbors, I can use convex combination of all these operations,
9114920	9122680	right, because this is a linear operator. So, bottom line on a grid, I don't have a canonical
9122680	9126760	graph, right? I can actually use a discretization, maybe that is different at different points in
9126760	9131720	the grid. Of course, there will be some numerical implications, but the discretization that we
9131720	9137320	choose, right, and as a result, how the nodes are connected and which nodes propagate information to
9137320	9144600	which nodes is, to a large extent, a numerical convenience, right? What makes sense, for example,
9144600	9149720	from the organization of the memory or the number of nodes and whatever. So, we would like somehow
9149720	9155000	to extend this mindset to general graphs. And for these purposes, instead of considering these
9155960	9160680	nonlinear diffusion equations, like Perron and Balig, by the way, they called it an isotropic
9160680	9165320	diffusion, which obviously, if you're familiar with PDEs, it's not an isotropic, it's not
9165320	9169560	homogeneous, right, because we have a scalar diffusivity function and isotropic diffusion,
9169560	9176520	we also have direction, so that would be a matrix or a tensor. So, instead of considering this
9177080	9181880	nonlinear diffusion equation, we can consider a non-Euclidean diffusion equation. And the model
9181880	9186520	here is the following, that was actually done by my PhD advisor, Ron Kimmel, also in the 90s,
9187480	9193720	about 25 years ago, maybe even more. So, again, thinking of an image, you can think of it as
9194280	9199160	an embedded two-dimensional manifold, right? And the embedding is in this joint space,
9199160	9203880	where we have a combination of positional coordinates, the x, y coordinates of the pixels,
9203880	9208440	and the fissure coordinates, in this case, for example, R, G and B channels. So, a color image
9208440	9214280	is a two-dimensional surface in R5, right, using this model. Now, by virtue of this embedding,
9214280	9218360	we can define a metric, so we can use the standard pullback mechanism, so in the case of
9218360	9223880	two-dimensional manifold, it's a two-by-two matrix, given like this, right? And we can
9223880	9228520	define a Laplacian with respect to this metric, it's a non-Euclidean analogy of the Laplacian,
9228520	9233560	called the Laplace-Beltrami operator. And we can write a diffusion equation with respect to this
9233560	9239080	operator, it's called the Beltrami flow, and we can actually show that it's a gradient flow of
9240120	9244280	a generalization of the Dirichlet energy that is called the polycop functional. It's used in
9244360	9247720	high-energy physics in bosonic strings, don't ask me what it is, but
9249720	9257160	that's something is described by this energy. So, by analogy, we can do something like this
9257160	9262920	on a graph, so now every node in the graph, in addition to having the fissure coordinates,
9262920	9267640	also has some positional coordinates, right, so positional encoding. Ideally, this positional
9267640	9273080	encoding should somehow represent the structure of the graph, right, in the sense that nearby points
9273080	9279640	in this u-component of the space should be more likely connected by an edge, right? And the Beltrami
9279640	9285880	flow, basically, it evolves, so we have again here a parametric diffusivity, it evolves both
9285880	9291160	components, right, that I collectively denote by z, and the evolution of x-component is the
9291160	9295720	standard fissure diffusion. You can think of the evolution of u as some form of soft graph
9295800	9303240	rewiring, because what I can do, if two nodes become closer in this u-coordinate, I can decide to
9303240	9309000	create an edge between them if there is no edge, or if the drift apart, I can decide to cut the
9309000	9314280	edge, so overall, I will facilitate the propagation of information, and I know that it sounds cumbersome,
9314280	9317960	but this is how it will look like, so again, this is the core graph, so it has,
9319560	9324280	basically, there are three things happening here, so the positions of the circles, right, so circles
9324280	9330120	are nodes, their positions represent some two-dimensional positional coordinates, the colors
9330120	9336120	represent three-dimensional projection of the fissures, and you see that they're both components
9336120	9340360	are evolving, and the graph is also changed on the fly, right, so when the clusters drift apart,
9340360	9346120	then we cut the edges between them, so right, so it's fissure diffusion, positional coordinates
9346120	9351560	are changing, and the graph is rewired, and you can see that the task here is node classification,
9351560	9356040	so there are clearly seven classes of nodes that we can clearly distinguish here.
9356920	9360440	Now, if you think of it from the standpoint of signal processing,
9361080	9364840	we have a very disturbing picture here, right, so we have a filter that happens on the domain,
9365400	9371640	yeah, a question? Maybe before you move to the next one, I was just wondering because I think
9371640	9376280	what started to appear in this intellectual is that you apply some ideas from differential
9376280	9381720	geometry to graphs, and maybe not yet directly, but... Not yet, right, so... Okay, so we are
9381720	9385480	like talking about metrics and this sort of, so I wonder, maybe you'll be talking about it next,
9385480	9392760	so sorry if I'm pushing it forward, but what do you think are the limits which come from the
9392760	9399640	fact that graphs are basically discrete structures, like how free are we to apply ideas? Yeah, give me
9399640	9403480	a few minutes and I will get there, yeah, so you can find the analogies, right, not everything has
9403480	9408840	exactly a correspondence, right, so, but these analogies, I hope to show that they can be quite
9408840	9414920	useful, right, so, but again, if you look at these pictures, so we have some kind of disturbing
9414920	9419640	picture here, so we have a filter on the domain, and the domain is changing under our feet, right,
9419640	9425240	so imagine that you're applying some filter, right, which is what it is, right, the diffusion
9425240	9431160	equation, you can think of it as a form of filter, low pass filter, and the domain is moving, so I'm
9431160	9437480	doing a filter and then nodes are somehow moving away from me, but this is a very common picture
9437480	9441800	in differential geometry actually, and it's very common to take a manifold and evolve it under
9441800	9445560	some evolution equation, and typically what, when you evolve a manifold, you're interested in what
9445560	9450040	happens to the metric, right, so here's an example of an evolution equation that is called the Ricci
9450040	9457800	flow, so you take the first order derivative, the temporal derivative of the metric tensor
9457800	9463560	of the manifold, right, denoted here by g, and you make it equal to the Ricci curvature tensor,
9463560	9469800	right, so basically the metric evolves proportionally to the to the local curvature, so it looks very
9469800	9474360	much like the diffusion equation, so here we have temporal derivative, here we have some second order
9474360	9479640	differential quantity that looks kind of like our Laplacian, right, so structurally it's similar
9479640	9484520	to the diffusion equation, of course what it does is a very different thing, and if you start with
9484520	9490360	this manifold, which has positive curvature on, so this kind of dumbbells on the spheres,
9490360	9494920	it has positive curvature, right, and the neck between them, it has negative curvature, so if
9494920	9501080	you run this diffusion, if you run the Ricci flow backwards in time, what will happen is that this
9501080	9505320	dumbbell become more like an ellipsoid than more like a sphere, and then will collapse into a point,
9505320	9513000	right, and it was introduced by Richard Hamilton in the 80s with the purpose of proving a famous
9513560	9520920	conjecture in topology that claims that you can characterize spheres by your ability to take
9520920	9525960	a closed curve and collapse it into a point, right, so this is how we characterize two-dimensional
9525960	9531960	sphere, I can take any closed curve on a sphere and I can evolve it and collapse into a point,
9531960	9535960	right, I cannot do it on a torus, so if I have a torus and I have a curve like this,
9536680	9541560	then no matter what I do it cannot be collapsed to a point, so the conjecture was that you can,
9542440	9547560	you can characterize higher-dimensional spheres in this way, and you obviously heard about it,
9547560	9553400	the PoincarÃ© conjecture, and it was shown by Perlman, actually a slightly more general result,
9554760	9562520	using the mechanism of Ricci flows, right, and that was a breakthrough of the century,
9562520	9567480	it stood open for more than 100 years. Now, what does it have to do with our graphs
9567560	9572760	and graph neural networks, so I remind you that we had this phenomenon, right, that message
9572760	9578120	passing might not be, might not work well on some graphs, right, so there might be some,
9578120	9582680	some phenomena that some graphs might be unfriendly for, for message passing, and in particular
9582680	9588440	it depends both on the structure of the graph and the task, and if my task requires to propagate
9588440	9593320	information from distant nodes, and the structure of the graph is such that the receptive field
9593320	9597800	of the graph neural network grows exponentially fast, right, so the number of the neighbors of
9597800	9603320	the neighbors of the neighbors becomes very large very quickly, this happens in trees,
9603320	9608040	this happens in what is called small world graphs like social networks, then we have a problem,
9608040	9611640	we have a lot of information that we need to squeeze into a single feature vector,
9612680	9620280	and this is a phenomenon that we call overscorching, so let's, let's define mathematically what we
9620280	9624600	mean by overscorching, so let's say that we have a message passing architecture of this form,
9624600	9631080	right, so we have the node itself at layer k, and we have the neighbors, we combine them with some
9631080	9636120	learnable weights, let's call them w1 and w2, let's say that the depth of this architecture is l,
9637240	9643240	the width, right, so the internal dimension is p, the long linearities are well behaved,
9643240	9648040	right, so the elliptics continues, and we also have some bound on the, on the weights,
9648120	9654040	so this is what characterizes our architecture, so what is overscorching, it's some form of insensitivity,
9654040	9661960	right, so if I look at the output of the neural network at node i and I examine how it depends
9661960	9668520	on the input at some distant node j, I can describe the sensitivity of the output to the input
9669640	9674840	through this Jacobian, right, so through the partial derivative, and if the partial derivative
9674840	9681800	is small it means that the information propagates badly from input to output, right, so basically
9681800	9688520	I will not perceive the change in the input in the output of that node, and what we show in the
9688520	9694120	paper is that we can bound the Jacobian by constants that depend on the model, right, for
9694120	9698920	example the number of flares, the regularity of the activation functions, the bound on the weights,
9698920	9703480	and also something that depends on the graph topology, right, and we show in particular,
9703560	9707160	for example, that width does help, of course, at the usual expense of
9710280	9716920	worst generalization overfitting, depth doesn't help, for example, and the topology has the
9716920	9722120	really the largest effect, and intuitively we expect that in some kind of benign graphs,
9722120	9726520	like grids, for example, we will not have overscorching, and in pathological examples,
9726520	9732040	like trees, that would be probably the worst case, right, so you see that the topology of
9732040	9738920	the graph comes here through the power of the adjacency matrix, but we don't see exactly how,
9738920	9745960	right, so it's hard to say, right, so what does it mean a matrix to the power l, so we need something
9745960	9751320	more nuanced, we need some kind of geometric analysis that will allow us to tell apart structures
9751320	9754440	like this and structures like this, right, something that locally looks like a grid or
9754440	9759000	something that looks like a tree, that's exactly what curvature is designed for, right, so I remind
9759080	9764520	you that in differential geometry, what curvature tells you is that if you take nearby points and
9764520	9770680	shoot geodesics in parallel at the same speed, you can either converge, remain parallel, or diverge,
9770680	9775640	right, and we call these spherical, euclidean, and hyperbolic geometry, right, so locally it looks
9775640	9781720	like a sphere, like a plane, or like a hyperboloid, or high-dimensional cases as well, so on a graph,
9781720	9786440	the analogy could look like this, so there are several definitions of reach-type curvature on
9786440	9792840	graphs, so this is a combinatorial definition that we use here, so you can take nodes that are
9792840	9798120	connected by an edge, let's call them p and q, and look at edges that emanate from these nodes,
9798120	9804680	so if they tend to form triangles, it means that we look at something like a click, if they form
9804680	9809960	rectangles, they will look at something like a grid, and if they drift apart and don't form anything,
9809960	9815080	then we look at locally at something that looks like a tree, right, and basically we can count
9815160	9820040	different types of rectangles and triangles, allow me to skip the details, basically for every
9820040	9823960	edge in the graph we can have this combinatorial quantity that we call the balanced formant
9823960	9830040	curvature, that counts, basically it looks at a two-hop neighborhood of an edge, and it counts
9830040	9835320	certain types of rectangles and triangles that surround this edge, bottom line, each reproduces
9835320	9840200	the continuous behavior, so clicks are positively curved, grids have zero curvature, and trees are
9840200	9846200	negatively curved, right, so that's I think to your previous question how what is the
9847080	9851000	parallel between differential geometry and graphs, so this is an analogy of a curvature,
9851000	9855320	so it's not a discretization of a curvature, it's a discrete curvature that behaves in a
9855320	9861000	similar way, and now the relation between the over-squashing and the curvature, what we show
9861000	9867160	is that if we have strongly negatively curved edges in the in the graph, then we can write down
9867240	9873080	this bound on the Jacobian, and it means that the over-squashing is caused by
9874200	9876600	the presence of strongly negatively curved edges, yeah.
9879880	9889880	Yeah, so it's the number of triangles that surround an edge, and this is the number of
9889880	9895960	rectangles, this is the degree, yeah, doesn't really matter, there are several definitions,
9895960	9900600	so why we call it balanced form and curvature, because there is a classical notion of form
9900600	9906840	and curvature that looks a little bit like this, we just touch it a little bit so it behaves like
9906840	9913480	what is shown here. This really relates to the rigid curvature tensor and the formula from
9913480	9918280	matrix, I mean I don't see it now, but maybe. It's a graph, so you don't have exactly the same thing.
9918280	9924200	Yeah, obviously, but some I don't know components, can we do some analogy or not, this is something
9924200	9931560	completely new. So it shows how, so you can think of curvature as how locally the volume changes,
9931560	9936200	so in a sense it shows how the volume changes, so there are two classical definitions of
9936200	9939960	curvature on graphs, one through optimal transport that is called the Olivia curvature,
9939960	9943320	and this is combinatorial version that is called the the form and curvature.
9946680	9951480	Right, so basically the conclusion, well I should make it explicit that it's strongly
9951480	9956120	negatively curved edges that cause overscorching, right, so basically due to this bound, actually
9956120	9961480	the presence of negative or slightly negative curvature might be benign, this is what is shown
9962120	9967960	in the the expander's paper by Petr Wieliczkiewicz and his and his causes, so these are somehow the
9967960	9974280	optimal graphs, the best for message passing, but expander's needs to be slightly negatively curved.
9974280	9981480	So once we know it, we can actually interfere basically, we can surgically remove the
9982120	9987080	negatively curved edges and replace them potentially with edges with higher, with more
9987080	9993560	positive curvature, and this way we retouch the graph a little bit and we show that it improves
9993560	9997080	the performance of graph neural networks both in homophilic and heterophilic settings.
9997640	10003560	So there was a question about diffusion-based rewiring before, and I promised to tell exactly
10003560	10012280	what I mean by this, so this is the paper that is called DIGL by Stefan GÃ¼nemann and his students
10012280	10016200	from the Technical University of Munich, and DIGL stands for Diffusion Improves Graph Learning.
10016760	10022920	So the idea there is that you rewire the graph by basically by computing page rank and embeddings
10022920	10028520	for a personalized page rank embedding for every node, and you connect then in this new embedding
10028520	10035640	space the nodes that are closest. So what it does essentially, it introduces connections within the
10035640	10041320	same connected component in the graph right, or within the same clique or cluster in the graph,
10041320	10047800	and it has a hard time to connect across different communities in the graph.
10048680	10052840	So when the graph is homophilic, this is a very good thing to do, right, so you're connecting to
10053720	10058200	to similar nodes, but if the graph is heterophilic, it can do more harm than help,
10058200	10063160	and in fact experiments show that this is the case, they also write it in the paper,
10063160	10066200	the curvature-based approach, first of all it changes the graph minimally,
10067320	10070920	here the change can be dramatic, right, so that's the number of edges that are changed,
10071800	10076600	but it also helps in the heterophilic cases because it's not restricted by this property,
10076600	10081240	you actually typically bridge different communities by the new edges that are created.
10083160	10087480	So still talking about diffusion, am I actually out of time?
10089960	10097400	Okay, yeah, so still talking about about diffusion equations, here are some more exotic
10098600	10106600	exotic stuff, right, and this is our maybe creative way to illustrate to illustrate sheaves
10107560	10114840	or bundles, so there has recently been probably a better picture, so that's really sheaves,
10114840	10120280	right, in the literal sense, so what are sheaves? So they actually have very interesting history,
10120280	10126520	and well I like these kind of historical factoids, so the theory of sheaves in algebraic topology
10126520	10132040	was introduced by Jean Lyret, so he was a French mathematician, he was also an officer in the
10132040	10138920	French army, and when the Nazis invaded France he was captured and put with his comrades into
10139480	10145320	concentration camp, and basically he was asked to work on mathematics, and his expertise was
10146600	10151160	mechanics, so he was very afraid that he would be forced to work on something that would be useful
10151160	10157560	for the Nazis, and basically he will be committing treason, helping the war effort, so when he was
10157560	10165080	offered the possibility to teach something in this camp, he chose a very innocuous topic,
10165080	10171240	algebraic topology, which could be useful, and then after, well of course they were released
10171240	10178040	after the war ended, he published it in a course that was taught in captivity, and one of the
10178040	10185240	papers that came out of this course introduced the theory of sheaves, so sheaves are objects that are
10185240	10192120	taught in algebraic topology, so if we apply them to our setting to graph, and this is slightly
10192120	10197400	different construction that is called cellar sheaves, so if you think of graphs by analogy to
10197400	10202440	manifold, so a manifold is a topological space, what I mean by topological space roughly is that
10202440	10209640	you have a notion of neighborhood, I can tell who my neighbors are, but I don't have the notion of
10209720	10217320	distances or angles, so if I want to talk about distances or angles, I need some extra machinery,
10217320	10221320	and on many folds this is typically achieved by what is called an affine connection,
10222520	10228200	or parallel transport, so it's a mechanism that tells me how to move vectors between
10228200	10234280	tangent spaces at nearby points, I can also define a remaining metric if I want to equip
10234280	10238360	a manifold with geometry, and then there is a special type of connection that is called
10238360	10243480	the Levy-Civita connection that is compatible with the metric, so you can think of the same thing
10243480	10249320	on a graph, so a graph is a purely topological object, I have a notion of who my neighbors are,
10249320	10255000	but I don't have any geometry, so in order to introduce geometry I can equip every node and
10255000	10261320	every edge with a vector space, and I can define by analogy to parallel transport, I can define
10261320	10266520	linear maps, so these are called restriction maps that go between these spaces, so slightly
10266520	10272040	different from many folds, I go from the space associated with nodes to a space associated
10272040	10276920	with an edge, and then if I want to transport information from a node to a node, I need to
10276920	10284920	combine these two maps, one with transpose, so basically it's a kind of, so I invent
10286520	10292440	geometry on a graph, so I lift it into a more complicated object, and on this object I can
10292440	10297560	now study, for example, what happens if I choose these restriction maps to be of certain class,
10297560	10302280	so these are matrices of certain dimension, and I can choose them, for example, to be
10302280	10305960	symmetric, or I want them to be orthogonal, or I want them to be something else, right,
10305960	10312520	I can also choose the dimension of these, of these, what is called stocks, right, so these spaces,
10313880	10319400	and I can define differential operators on this structure, right, so the difference between
10319400	10325560	the standard, for example, gradient and the shift gradient would be the same way as we have a
10325560	10331080	manifold, I cannot add or subtract two points on the manifold, so when I need to subtract two
10331080	10335800	vectors, I first need to apply parallel transport, so I need to bring the vector from its vector
10335800	10340600	space to my vector space, to my tangent space, and by doing this typically I would apply some
10340600	10345160	form of rotation, if it's a manifold with the remaining metric, and only then I can subtract
10345160	10349640	them, right, so here the same, if before the gradient looked like just difference between
10349640	10355080	n points of an edge, here we'll have also some linear transformation that sits in between,
10355080	10360040	right, long story short, I can, basically I'm interested in a Laplacian, right, so I have a
10360040	10366920	shift version of the Laplacian, so it's a block matrix where every block transforms the vectors
10366920	10373640	with these kind of, with these kind of matrices, right, the combination, and now I can apply this
10373720	10379960	operator on my data and run a diffusion equation, and I can run it to infinity with some additional
10379960	10385080	conditions, and I can ask questions like how many classes can I separate if I choose this
10385080	10390760	shift in a certain way, yeah. Are you doing graph rewiring here? No, we are not doing any
10390760	10394520	graph rewiring, so the graph structure is encoded in the structure of the Laplacian,
10396280	10403080	right, so basically it's a kind of question about expressive power of this architecture,
10403080	10409560	so I can ask how many classes I can separate, right, so expressive power, slightly different,
10409560	10413880	different version from the, from the Weisferre and Lehmann, because in Weisferre and Lehmann we
10413880	10419720	asked about the, how many, the types of graphs that we can, if we can distinguish, here we're
10419720	10428760	looking at node level problems, yeah. Can you please make some like use cases for this kind of
10429720	10436040	of graph, and the ones that were shown before in which the nodes were actually separated with
10436040	10445080	no connections, one to another? So, the graph here is given, so it's, I think some, I don't remember
10445080	10454120	what data set it is, yeah, sorry, what is again the question? So, like, what kind of, what are
10454120	10461800	you trying to model and why is this configuration? Oh, so, yeah, the colors represent the classes,
10461800	10467400	the ground rules classes, and the positions represent the features. And what's the difference
10467400	10473640	between this type and the one in which you do rewiring and each class series separated,
10473640	10477720	one to another? So, here the features are represented by coordinates, so the closest
10477720	10482680	analogy of this illustration is to the one that I showed with the gradient flow, right,
10482680	10487880	so the coordinates here represent the features, not the positional coordinates, right, and the
10487880	10494040	colors represent the classes, okay. So, there is no rewiring happening here, you can also potentially
10494040	10499880	use rewiring. And the results, well, I don't want to go through all the results, but basically what
10499880	10505480	we show is that by using different types of sheaves, we can guarantee that we can separate
10505480	10511800	different types of, different number of node classes, depending whether the graph is homophilic
10511800	10516600	or heterophilic. For example, we show that you must have non-symmetric relations if you want to
10516600	10525400	deal with heterophilic graphs, yeah. So, in previous method, WL, it's something like, I think the
10525400	10532200	classification was on top of number of edge, if I get it correctly, like if you both go to the
10532200	10539560	higher classes, then the number of edges also increases, but here in sheaves, it's, it's,
10539560	10546200	is it about the dimension that we have, like, can we have a first dimension sheaves or something
10546200	10552760	like this? So, Weisfer and Lehmann is different, so the hierarchy there is, basically, they are
10552760	10558360	different algorithms, right. So, whether they do color refinement for different structures for,
10558840	10566440	for a node, for a pair of nodes, for triplets of nodes, and so on. So, here we have, the choice is
10566440	10570760	what kind of matrices we allow, what class of matrices we allow, so it's typically a group,
10570760	10576120	right, so we say, for example, the, the most general case is, is GL, right, so any invertible
10576120	10581160	matrix, then we can restrict it to be, for example, symmetric matrix, or then we can restrict it to
10581160	10586360	be orthogonal matrix, right, and based on these choices, plus the dimension of the sheave, we get
10586360	10594760	different results. So, it has to take dimension and also the matrix. Exactly. Thank you. So,
10594760	10598520	this is a more theoretical question, right, because it's a good question how we actually,
10598520	10602520	how we learn the sheave from the data, but assuming that we knew the sheave, right,
10602520	10607320	but we allow the sheave to be only from of a certain type, what is in the best case,
10608200	10613080	how many node classes we could separate, under different assumptions also about
10613080	10618360	the structure of the graph, whether it's homophilic or heterophilic. But, basically, the, the, the,
10618360	10624920	the bottom line of this story is that diffusion, when you have, when you have these extra degrees
10624920	10629880	of freedom, looks more interesting than, than, than the standard diffusion on a graph. So,
10629880	10635400	the standard diffusion on a graph corresponds to symmetric restrictions with one dimensional sheave.
10635400	10645240	Yep.
10665400	10668600	well so it's slightly more complicated right so the analogy of the
10668600	10672040	connection would be the composition of two maps right so what we call a
10672040	10675120	transport map so each of these f's is called the restriction map so it goes
10675120	10679140	from node to edge space they actually can have different dimensions so they
10679140	10684720	don't need to be the same the composition right so f f transpose is a map
10684720	10689240	from the space of one node to the space of another node so that's the analogy of
10689240	10692640	parallel transport right so I'm when I move a vector from one node to another
10693600	10698040	geometrically transform it somehow rotated for example or scale it depends
10698040	10707640	on the class of matrix that I use here so in exactly so but then of course in
10707640	10712120	practice you need somehow to parametrize it right so you cannot of course in
10712120	10717080	principle you can say that that let me learn individual f's for every for every
10717080	10721160	age of the of the graph but it's not practically feasible so in practice f is
10721160	10725600	a matrix valued function that depends on the node features so it's a little
10725600	10730440	bit similar to attention but the tension is scholar here it's matrix so it's a
10730440	10740280	geometric operation okay any questions
10743120	10750380	right so basically to summarize this this part so what do we gain from this
10750420	10757220	physics inspired perspective on graph neural networks so first of all I think
10757220	10761020	it's different viewpoint on old problems like over smoothing bottlenecks it
10761020	10765380	allows to on the one hand to interpret existing architectures like guts for
10765380	10769940	example from a different perspective it allows to potentially design your
10769940	10774380	architectures right for example using if you think of a generous discretization
10774380	10778980	of differential equations then of course you can ask what kind of solver can I
10778980	10783180	use can I use some some more interesting things with adaptive step size or maybe
10783180	10788380	I don't know multi grid solvers and so on it allows to make principle
10788380	10793420	architectural choices right like example with gradient flows so basically from
10793420	10798660	the gradient flow we get restriction on symmetric weights we get residual
10798660	10802140	connection we can also have some theoretical guarantees right again like
10802140	10804780	we've seen with the gradient flow but maybe also of other types like
10805100	10809940	convergence stability and so on and so forth but probably more interesting are
10809940	10814500	links to other fields that are less explored in graph neural network literature
10814500	10818580	like in particular differential geometry or algebraic topology and of course
10818580	10822540	diffusion is just one example of evolution equations you can consider more
10822540	10825540	interesting things so this is one example right so probably have seen these
10825540	10831300	kind of things coupled oscillators so the metronomes that are put on a table and
10831340	10837660	because the vibrations transfer from one to another initially they might be
10837660	10841260	oscillating out of phase and then they become synchronized so think of
10841260	10844820	something like this but on a graph so the coupling occurs on a graph in a
10844820	10848300	learnable way and depending on the tasks that we want to do we have we want
10848300	10852980	somehow to to interact between these different oscillators so it's also a
10852980	10857460	differential equation but it also has a second order kinetic term so unlike a
10857540	10862820	diffusion equation it has also a surgery component and here we show for
10862820	10867500	example that we can probably avoid over smoothing by using this type of
10867500	10879580	equations how much time do I have 15 minutes okay so what I would like to do
10879580	10885900	if in 15 minutes let's talk about grids so I definitely ran out of time because
10885940	10889420	out of all the geometric objects at the end well we spent all the time on
10889420	10895740	crafts I think grids also deserve a little bit and probably well everybody
10895740	10899660	is familiar with grids right so let's look at them maybe again from the
10899660	10904300	perspective of geometric deep learning and hopefully some new intuition or at
10904300	10909740	least for some of you have not seen it before so and this also relates to the
10909740	10915740	previous question of why we call graph convolutional networks convolutional so
10915780	10922220	first of all a grid is a graph right so it's a particular type of a graph for
10922220	10926060	simplicity I would like to assume that the grid has periodic boundary conditions
10926060	10931700	so basically it's what is called the ring graph and the idea of geometric deep
10931700	10936980	learning right this group based framework we had some domain and we have a
10936980	10941780	group that acted on the domain we have a signal that was defined on the domain so
10941980	10947500	this is a general type of this mechanism that is often called lifting so now I
10947500	10950860	have a linear operator so this can be anything right so this can be a non
10950860	10954660	linear complicated thing here I have a linear operator so the group
10954660	10959500	representation attacks on functions defined on the domain okay and in the
10959500	10963100	case of a grid this is just the shift operator so this is what they show in
10963100	10971140	one dimension so just cyclically moves the elements of the vector
10972100	10975780	now another thing that you see in a grid is that it has a fixed neighborhood
10975780	10979300	structure right in this example every node is connected to exactly two
10979300	10983020	neighbors and they are also ordered right so I always have the one before and
10983020	10987140	the one after in a two-dimensional grid I might have some partial order right so
10987140	10994060	I have something on top and something on the bottom so in the past on the
10994060	10997260	general graph we had this kind of aggregation function right so we have the
10997580	11003380	feature of the node itself and then we had a multi-set that was unordered of the
11003380	11008180	nearby features and because we didn't have any order in this multi-set we the
11008180	11011380	only thing we could do is to apply a symmetric function apply a permutation
11011380	11015340	invariant function now we have a different situation now the nodes are
11015340	11022820	ordered right so we have a fixed order of x i minus one x i and x i plus one so
11022860	11031060	this function can be more general right so it doesn't need to be doesn't need to
11031060	11041100	be to be symmetric and if this function is linear then we get the convolution
11041100	11045820	right and if I write it as a matrix vector product then it looks like this
11045820	11052460	so it's a special matrix which has fixed elements along the diagonal right so
11052500	11056940	this is the the the local weight-sharing that have been convolutional neural
11056940	11060940	networks so this is a special type of matrices they're called circumvent
11060940	11065900	matrices right or convolutions so it's synonym you take a vector of
11065900	11070940	parameters right let's call it data and you create these matrix by cyclically
11070940	11074660	shifting by one position these vector of parameters and depending it as columns
11074660	11078780	that's how you get you get these matrix again I'm assuming periodic boundary
11078780	11082180	conditions so technically speaking it's not a convergence it's a circle
11082220	11089660	convolution or a cyclic convolution but just to make things simpler okay now one
11089660	11094060	thing that you first thing that you learn in in algebra one-on-one is that
11094060	11099580	matrix multiplication is not commutative right a b is not equal to b a but with
11099580	11103900	these matrices with convolutions there are with circumvent matrices that's not
11103900	11109260	the case it's actually a special type of matrices that do commute right and in
11109260	11115060	particular they commute with one of them which is the shift operator right so a
11115060	11119900	shift is also a circumvent matrix right or also a convolution right so looks
11119900	11126780	like this so what does it mean that that a convolution commutes with a shift so
11126780	11130140	this is what we call shift-equivariance right so in other words I can first
11130140	11133180	apply convolution and then shift or I can first apply shift and then
11133180	11138700	convolution the result will be the same right so convolution is shift-equivariant
11138700	11144220	you can show the other way around right so you can show that if you have shift
11144220	11149140	equivariant linear operations so I take a matrix and I tell you that it's
11149140	11154420	shift-equivariant you can show that it must be a convolution right so basically
11154420	11159540	convolution emerges from considerations of translational symmetry right so the
11159540	11164260	only linear operation that that is shift-equivariant is convolution so
11164260	11170940	convolution is the only thing that satisfies this property and we've seen
11170940	11175260	again this geometric deep learning blueprint so allow me to show it again
11175260	11179980	so we have a grid we have a translation group its representation is the shift
11179980	11185060	operator so the convolution is a function that is
11185060	11191860	equivariant with respect to this to this group now we also know that there
11191860	11195340	is an intimate relation between the Fourier transform and the convolution
11195340	11199620	right and let's actually try to understand what the Fourier transform is
11199620	11204340	where it comes from so we know from algebra again that commuting matrices
11204340	11208140	are jointly diagonalizable it means that they have the same eigenvectors or more
11208140	11212540	correct eigenspaces but here we assume that the multiplicity of eigenvalues is
11212540	11217180	trivial so they actually have the same eigenvectors and the only different
11217180	11220780	different eigenvalues right so all commutative matrices satisfy this property
11220780	11226740	so if I have a set of matrices that commute pair wisely then then this is
11226740	11231020	the case right and this is the case for for convolutions or for circuit matrices
11231020	11238460	so we can pick up one of these matrices right and compute its eigenvectors
11238460	11243500	right and we know that all of them will have the same and it's convenient to
11243500	11250300	look at the shift operator right at the matrix S and if we compute the eigen
11250300	11253500	vectors of the shift operator you can do it by hand it's actually not difficult
11253500	11258220	you see that they look like these complex exponentials so this is exactly the
11258220	11266860	Fourier transform or more correctly the discrete Fourier transform so the
11266860	11270980	question of course that remains is what the eigenvalues are right so we know
11270980	11274020	that the eigenvectors of all conversions are the discrete Fourier
11274020	11279660	transform so these complex sinusoids but the eigenvalues also you can show it
11279660	11283900	are the Fourier transform of the vector theta that forms each of these matrices
11283900	11288740	and this gives us this dual relationship between the Fourier transform and the
11288740	11293060	convolution so if I have a signal x I can do convolution in the spatial domain
11293060	11297780	by multiplying by a circuit matrix or I can do it in the in the Fourier domain
11297780	11302020	I can compute the Fourier transform and there the Fourier transform diagonalizes
11302020	11306980	the convolution so it becomes an element wise product right so basically the
11306980	11311180	product of two Fourier transforms is the Fourier transform of the convolution
11311180	11317260	right and typically in signal processing the filters are already designed in the
11317260	11322180	Fourier domain this is bread and butter of signal processing so the the the
11322180	11326660	advantage of using the Fourier transform because this operation usually on
11326660	11331820	grids can be done efficiently so instead of n squared operations as you would
11331820	11336260	typically require here because the Fourier transform the the the matrix has
11336260	11341260	a very redundant structure you can you can avoid these explicit multiplications
11341260	11345060	you can reuse some of the multiplications and do it in n log n operations so
11345060	11348740	there are classes of algorithms that are called fast Fourier transforms and this
11348740	11353460	is from the approximately the sixes when this was derived with the most famous
11353460	11358220	algorithm is by Kuli and Tuki this is how signal processing has been done and
11358220	11363500	you have it everywhere from your stereo to your iPhone from your computer so this
11363500	11367260	is how it's done you cannot do it on graphs because on graphs the analogy of
11367260	11372100	the Fourier transform would be the eigenvectors of either the adjacency
11372100	11375980	matrix or the Laplacian matrix so if they are symmetric they have orthogonal
11375980	11380100	eigen decomposition but these matrices do not have these redundant structures so
11380100	11385300	the Fourier transform has n squared complexity dense matrix multiplication
11385300	11391540	and actually some of the early crafting of electrical architectures came from
11391540	11396380	this domain of signal processing on graphs where that that used the eigenvectors
11396380	11400980	of the Laplacian or the adjacency matrix as an analogy of the Fourier transform
11400980	11409140	so the difference in the case of the in the Euclidean case on the grid there is
11409140	11412220	no difference between the two right so the Laplacian is also obviously a
11412220	11418140	circuant operator circuant matrix and so is the shift right or the adjacency
11418140	11422820	matrix of of the ring graph which happens to be the shift operator they all
11422820	11425820	commute so they have the same eigenvectors on the general graph they are
11425820	11432140	different so therefore these methods slightly slightly differ so the way to
11432140	11437460	think of why you you want to look at the adjacency at the adjacency matrix is
11437460	11442980	this right so this is how you can think of your convolution so these basically
11442980	11449100	it's multiple diagonal matrix now we can write it as a sum weighted by these
11449100	11453460	coefficients of the powers of the adjacency matrix right so the adjacency
11453460	11458220	matrix of the ring graph will look like this so this red diagonal right so
11458220	11462540	that's the shift operator so if you take a square you will get this if you get
11462540	11467380	cube you will get this right so you combine all of them you will get you
11467420	11473500	will get this general convolution so first architectures that try to do to do
11473500	11477660	learning on graphs looked exactly at this taking powers of either the Laplacian
11477660	11482620	or or the adjacency matrix basically polynomial with learnable coefficients
11482620	11487940	now if you also look at terms of the degrees of freedom so a fully connected
11487940	11493060	layer will look like this right so it has no it has no symmetry so here the
11493540	11499500	symmetry is trivial so it has n square degrees of freedom in the case of
11499500	11506020	convolution so the the the symmetry here is translation we have order of n
11506020	11509460	degrees of freedom right so we reduce dramatically the number of parameters in
11509460	11513900	the neural network we reuse the same coefficients everywhere in the case of a
11513900	11517860	graph because we have permutation in variance we don't have the order of the
11517860	11521500	neighbor so we must use the same coefficient so we can only distinguish
11521540	11526260	between ourselves and our neighborhood right so that's well here it's I'm
11526260	11530340	assuming a complete graph so this will look like something like deep sets for
11530340	11536140	example so but the number of parameters is order of one so it's independent on
11536140	11543100	the on the size of the domain what else can I say can I tell you well I know
11543100	11549140	that I'm out of time so do you want to hear about molecules or you heard about
11549180	11562540	molecules okay so let's talk about molecules I promise that I will try to do
11562540	11567620	it try to do it fast and probably heard in Miguel's lecture as well so it will
11567620	11573460	probably be a little bit repetitive so graphs are a very convenient model for
11573460	11576860	molecules right basically a molecule looks like this so you can represent it
11576900	11581460	as a graph and maybe that's not how chemists think of molecules but at least
11581460	11585740	in some applications graph neural networks have been successful in predicting
11585740	11589500	certain properties of molecules that are required for virtual drug screening
11589500	11596060	right where the space of potentially synthesizable drug like molecules is
11596060	11599940	huge something like 10 to the power 60 the number of molecules that they can
11599940	11603660	actually test in the lab is significantly smaller so you need to
11603660	11608100	reach this by some kind of computational methods and crafting networks have been
11608100	11613340	shown again in predicting some properties to be significantly faster
11613340	11620940	while similar complexity to similar accuracy to to classical methods so one
11620940	11626940	thing that that that is important to say in regard regarding molecules so
11626940	11631460	molecules are not just any graph where the symmetry that we have is the symmetry
11631460	11635540	of the domain right the permutation of the nodes or the reordering of the atoms
11635540	11639820	right so the domain symmetry tells you that no matter how you order the atoms
11639820	11644300	in the molecule I still want to be able to say that it's the same molecule but
11644300	11647740	it also has geometric coordinates right so in addition to the let's say atom
11647740	11651740	types that we have here I also have the XYZ coordinates for every atom right so
11651740	11656900	it's a graph that lives in a continuous Euclidean space so here what I want to
11656900	11661700	say that if I rotate the molecule for example or translated I want to be able
11661700	11666140	to say that the properties remain the same so in this case typically you look
11666140	11670820	at the special Euclidean group so rotations and translations without
11670820	11676220	reflections reflections can actually change the properties of molecules or
11676220	11682100	you can use some other groups as well and there have been already several
11682100	11686860	interesting success stories so one of them was a group of Jim Collins at MIT so
11686860	11690540	they used graph neural networks in virtual screening pipelines where they
11690540	11696700	tried to determine which compounds could be used as new antibiotics against
11696700	11702660	antibiotic resistant bacteria and they famously found that that a candidate
11702660	11707140	drug that was tested against diabetes called Halicin was actually effective
11707140	11714740	across a broad range of of antibiotic resistance bacteria but in things that
11714900	11719100	we are doing we are mostly interested in proteins and this is well I think this
11719100	11723900	is in general proteins are important targets for for drugs because they are
11723900	11728900	involved practically in anything that that happens in our body from defense
11728900	11734100	against pathogens right antibodies are special types of proteins to delivering
11734100	11738380	oxygen to ourselves hemoglobin is also a special type of protein so basically
11738380	11742780	they're everywhere and encoded in our DNA so we really we don't know any life
11742820	11747980	form that is not based on proteins at least for the time being and it was
11747980	11753460	conjectured in the 70s by Anfins and Nobel laureate in chemistry that you can
11753460	11759740	determine the structure of the protein from its sequence so proteins are long
11759740	11763860	chains of amino acids connected to each other and then under the influence of
11763860	11769580	electrostatic forces they fold into these complicated structures but we are
11769580	11775020	interested in the opposite problem so maybe a little bit incorrectly we can
11775020	11781180	call it some kind of inverse problem so I would like to to design a protein that
11781180	11785500	will fall in fold into a certain structure of course it's not that simple
11785500	11789460	because it is tempting to think that we have a sequence that then falls into a
11789460	11792500	structure and the structure and doubts the protein with certain function for
11792500	11799380	example what kind of molecules it binds and initially computer scientists look
11799380	11804420	at proteins as sequences because well it's just strings so we can look for
11804420	11807940	certain patterns you can try to align different sequences together right like
11807940	11812740	multiple multiple sequence alignment then came the problem of structure
11812740	11816460	prediction and that's where alpha fold excelled recently but then the problem
11816460	11821060	of function is distinct and you can find examples of for example proteins with
11821060	11826580	different sequences but similar structure you can find proteins with very
11826580	11830580	similar sequences but very different structure or you can also find proteins
11830580	11833540	with different sequences and different structures but similar function so they
11833540	11839460	happen to to bind the same the same molecule so the good analogy here is
11839460	11844580	this lock and key metaphor that was introduced by I like quotes from
11844580	11848660	Nobel laureates so that was from Emil Fischer also Nobel Nobel laureate in
11848980	11852900	in chemistry so he was talking about enzymes but I think it's more general
11852900	11857700	applies to to proteins broadly so same way as you have a unique key that fits
11857700	11863060	into a lock you might have a unique molecule or at least that's that's the
11863060	11867620	wishful thinking is that a unique molecule that will fit into some pocket
11867620	11871940	that exists on the the surface of this folded protein structure and this is how
11871940	11876900	drugs are typically designed right so you have a protein that is your target so
11877300	11882500	that's how its surface looks like and here is some small molecule that sticks
11882500	11888900	into this hole and binds this this molecule and that's how the drug works so this is
11888900	11893540	actually a molecule not exactly of caffeine but of compound from the same
11893540	11897620	class and that's how it's by how it binds the adenosine receptor in the brain
11898660	11902420	many other interesting targets though they don't have these kind of pocket like
11902420	11907940	structures and there are interesting systems of protein of proteins interacting
11907940	11912260	with each other like this one the program death complex where you have two proteins
11912260	11919060	called pd1 and pdl1 and they are involved in cancer immunotherapy basically these
11919060	11922980	proteins tell our immune system not to kill healthy cells and some cancers
11923540	11928900	have these proteins so they manage to evade the normal functioning of the immune system and
11928900	11935060	the idea is to block one of these proteins either pd1 or pdl1 and this way basically
11935060	11940260	the malignant cells are destroyed by by by the immune system so you need to design
11941060	11947460	some binder that will that will bind to one of these proteins and they happen to have
11947460	11951060	these kind of flat interfaces so they're considered to be hard or impossible to
11951060	11955700	drug by small molecules but you can drug them by proteins so that's the idea of biological drugs
11956660	11962020	where the drug itself is is a protein molecule typically an antibody for variety of reasons
11963380	11967460	so you can use geometrically planning well and unfortunately I didn't have time to talk about
11967460	11972420	it but basically instead of considering graphs we can consider surfaces so we model proteins as
11973220	11979940	many folds as as basically the external surface that that that appears to
11980900	11985860	to the other molecule that that tries to bind it and this way you abstract all the internal
11987060	11991780	intricacies of the fold so let me try to show you an example so this is a plastic model of a protein
11992340	11999220	you see so this is protein is the the one that that the person holds is supposed to bind to
11999220	12005700	these transparent ones so you see that these complicated helixes and and other things inside
12005700	12010980	so that's the protein fold but what appears from the outside is this transparent surface so
12010980	12016180	this guy doesn't care what happens inside so it cares only about the the the structure of course
12016180	12020580	the problem is more complicated because the the conformation of the protein the its geometric
12020580	12025060	structure might change as a result of of the interaction but at least it's in some cases
12025060	12033220	it's a good approximation so long story short we we can do special type of neural networks that
12033220	12038340	operate on these surfaces so they take into account both geometric and and chemical properties of
12038340	12045540	of the molecular surface and they can they try to find complementary structures that are expected
12045540	12050740	to interact so think of kind of pieces of three-dimensional puzzle but it's not only
12050740	12054980	geometric complementarity it's also chemical complementarity so they need to have the right
12054980	12060500	charges so they don't repel each other and this is a method that we call the massive so
12061460	12070660	we were lucky to appear on the cover of nature methods in 2020 and this year we also had a paper
12070660	12076180	in nature that contained experimental results so we also hope to appear on the cover but they chose
12076180	12081140	a different one but because we paid for the cover here you need to you need to see it i think it was
12081700	12089140	a cool image so we used this method to design new binders for different targets and basically it's
12089140	12094740	a fragment based design so we use this neural network architecture to identify potentially
12094740	12102020	complementary targets that then we use to build the binder and here the experimental results show
12102020	12109300	different structures so here's a binder for the pdl one oncological target and we also have the
12109300	12117380	crystal structures and here's an example of another binder for the SARS-CoV-2 spike protein so that's
12117380	12125540	the coronavirus that caused the COVID-19 pandemic that has been terrorizing us for more than three
12125540	12134180	years now and basically this structure binds the region of the of the spike protein that interacts
12134180	12139460	with the ACE2 receptor of the host so that's where how the virus enters into into our body
12140340	12146260	and here we also tested it so we have the structure from cryoEM we also tested it on
12146260	12154100	different variants of the virus so the alpha beta and omicron that probably everybody was
12154100	12160980	following in the newspapers so you see that that it binds many of these maybe some others less
12161620	12167780	and here's also a comparison to a clinically approved drug so that was antibodies that were
12167780	12174180	developed by AstraZeneca so basically what is shown here is how much inhibition you have
12174180	12180340	versus concentration so the smaller concentration the better of course so we are not as good as
12180340	12186100	the AstraZeneca drug but so it's something that was designed totally computationally and this is
12186100	12193940	actually pseudovirus neutralization so it is probably much closer to real validation than
12193940	12200980	at least anything that myself as a computer scientist could think of well I think I will
12200980	12207540	probably stop here but if you think of diffusion models right so generative models everybody is
12208660	12212660	now talking about right like like the Dali2 and now of course you have way better results
12213300	12219860	so you could imagine something like this for for molecular design so we have some condition on
12219860	12224500	on let's say diffusion model that we use here like the geometric structure of the of the target
12224500	12230100	pocket and you'll try to build a molecule that satisfies these these constraints so we don't
12230100	12235220	really have a text prompt but you have maybe some some other way of conditioning the model so
12236020	12239940	so this is one example maybe not very interesting so another example is what we call diffusion
12239940	12245620	linker where we have small molecular fragments what is called pharmacophores that you know how
12245620	12251620	they bind the target but you also need to connect them into bigger molecule and we try to basically
12251620	12257700	to start with these little fragments and to diffuse the the the linking structure that that connects
12257700	12263460	them we're not very lucky in publishing this paper in europe so we'll probably send it to some
12263460	12270820	chemical journal uh well I think I will stop here sorry for running out of time thank you very much
12271220	12273940	uh
12284100	12287380	yeah we are over time but if you have still a couple of questions
12289460	12291620	if not you can ask individual maybe
12293540	12297940	okay but thank you again for for the amazing talk well thank you
12300820	12303940	thank you
