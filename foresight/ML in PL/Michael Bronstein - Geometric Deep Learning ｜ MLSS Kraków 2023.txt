Thank you very much.
So great pleasure to be here.
It's actually my second time in Krakow, and I think it's a very beautiful choice for
having a machine learning summer school.
So in the next three hours, I would like to talk about geometric deep learning.
And if you wonder this intriguing image, what does it have to do with machine learning?
So you see it fits more, a kind of an alchemist.
So we wanted to use this image in reference to this famous quote from Ali Rahimi who was
receiving the Best Paper Award or the Proof of Time Award at New Europe in 2017.
And he was speaking this way maybe a little bit critically about deep learning, at least
at that time, that we say things like machine learning is the new electricity, and he's
alternative metaphor was machine learning has become alchemy, so in the sense that some
kind of science that maybe produces something that we don't really understand what it does.
And really what we would like to do here is to try to understand from certain perspective
how these methods work and why they work, and maybe more importantly, when they fail
and you see kind of maybe general blueprint for developing potentially future machine
learning systems.
So the concept that will be important to these lectures is the concept of symmetry.
And symmetry according to Vile, who I'm quoting here is, depending on how wide or narrow you
define its meaning, is an idea by which men through the ages has tried to comprehend and
create order beauty and perfection.
So it sounds a bit poetic, but I think it is true, so that's from his book that is titled
Symmetry, that he published in Princeton.
And symmetry is a Greek word, so it goes back to the ancient Greeks, and as you probably
know, ancient Greeks like Plato considered symmetry to be really the cornerstone of the
universe.
So according to Plato, what is nowadays called platonic solids, symmetric polyhedra were the
basic building blocks of all the stuff in the universe, so probably tiny little things
that build up the matter, and if you think of it in modern terminology, that's not very
far from truth.
So Plato believed that geometry is really the key piece of mathematics, and even according
to a legend, there was an inscription on the entrance to his academy saying that nobody
skilled in geometry, that is not skilled in geometry, should be allowed to enter.
And this idea of matter being built of small symmetric polyhedra, actually not far from
truth, if you consider how crystals are organized, and the first to study this from a formal
perspective was actually Kepler, who is maybe more famous for his discovery of the motion
of planets, but he was also the one who laid the foundations of modern crystallography,
basically considering how spheres can be packed into different configurations, and if you also
think that this is old and boring and outdated stuff, so last year's Fields Medal was given
exactly for solving these kind of problems maybe in higher dimensions.
So geometry itself also, at least in formal way, goes back to ancient Greeks, and what
we still often study at school as the geometry dates back to Euclid, his famous elements,
so a system of axioms from which his geometry was derived, and as you know there are five
axioms or five postulates of Euclidean geometry, that the last one was somehow standing out
and people for many centuries or even thousands of years tried to do something with it, and
for example in the 18th century Giovanni Saccheri, who was a priest, almost arrived to the construction
of a non-Euclidean geometry, but he considered it such a heretical idea that he thought that
it is repugnant to the nature of straight lines, so he abandoned these ideas and never
took it to the full extent, but in the 19th century what happened is that finally came
the realization that dethroned Euclid and broke his monopoly of geometry and the works
that probably gauss himself did, but never published, and then famously Lobochevsky,
Boje and Riemann, who created the first examples of what we now call non-Euclidean geometries,
and in the 19th century this is how the geometry started looking like, so a zoo of different
types of geometry without clear relation or understanding what actually defines the geometry,
so there was obviously a need to put order in this mess, and the new idea, new approach
came from Felix Klein in 1872, so he was only 23 years old, he was lucky to get an appointment
as a professor at the University of Erlangen in Bavaria, so this is something that for
example Euler failed to have in Switzerland, he had to go to Russia to become a faculty,
probably not very dissimilar to situations that some of us are facing in these days,
so Klein was asked, as it was customary in Germany and still customary I think, to deliver
a research prospectus, basically to explain what he is going to do until his retirement
in Erlangen, which actually never happened because he moved three years after to different
places eventually to GÃ¶ttingen, and in this prospectus that entered the history of mathematics
as the Erlangen program, he proposed a kind of algebraization of geometry, so studying
geometry from the perspective of group theory, so essentially considering a geometry as a
space plus some class of transformations formalized using group theory, and studying properties
that remain unchanged or invariant under these transformations, so you take an object and
you apply to it rigid motions and you preserve a lot of things like areas, parallel lines,
distances, so this is how you create Euclidean geometry, but you can consider other groups
like a fine or projective group, and in fact he considered projective group to be the broadest
construction, he in fact showed together with Biltrami that the first non-Euclidean geometry
is hyperbolic, geometry is with negative curvature, could be constructed with a projective model,
and these ideas had really big impact on geometry, on mathematics more broadly, I would say culturally,
like category theory is an extension of these ideas to more abstract objects, but probably
most importantly it had an impact on physics where came the realization in the beginning
of the 19th century, probably starting with Neutra with her famous theorem that the laws
of physics themselves can be derived from considerations of invariance or symmetry, and for example
what Neutra showed is that principles like conservation of energy that previously were
considered to be empirical could be derived mathematically from certain symmetries, so
symmetry of time in this case, and these ideas in a more generalist form led to what nowadays
is known as the standard model, so basically all the world can be modeled and can be derived
from first principles of symmetry, so what I think physicists among you would call external
symmetry or internal symmetry, the symmetry of the spacetime, so what is called the Poincare
group that gives rise to Minkowski geometry of special relativity or internal symmetries
of quantum fields, that's what gives rise to different forces or different interactions,
and I think nobody put it better than Philip Anderson Nobel laureate in physics that without
or with only slightly overstating, you can say that physics is the study of symmetry,
so what does it all have to do with deep learning and neural networks and machine learning
in general, so let's do maybe a brief detour into the history of machine learning or artificial
intelligence, so the term artificial intelligence comes from these people, the Dartmouth conference
that happened in 1956, organized by McCarthy and others, and you can see them, some very
prominent figures sitting here, so this is for example, this is Claude Shannon and this
is Marvin Minsky who would become all very important scientists, and historically apparently
the term artificial intelligence was introduced to kind of distance themselves and not to be
in the shadow of the expert of that time who was Norbert Wiener who introduced the term cybernetics,
which I think is still used, I think here in Poland it's probably still used as a kind of
overarching term for everything that it has to do with computer science and artificial intelligence,
so around that time there were many works that tried to understand how the brain works,
so I think at that time it was already understood that somehow our intelligence is concentrated
in the brain, so models for neural networks, probably the most famous one is by Frank Rosenblatt,
the so-called perceptron, but there are models before that, and he was able to show that was in
the 50s, so these models had to be implemented in analog hardware, he was able to show that he
can solve some simple pattern recognition problems with these neural networks, and it was extremely
remarkable at that time, so there were articles in the popular press like the New Yorker saying
that this is the first serious rival to the human brain ever devised, so I think you can only smile,
I can hear you laughing, and it's remarkable machines capable of what amounts to thought,
so that's according to New Yorker was the perceptron, and there was a little bit of
hype, as you probably know, this MIT computer vision summer project, so they thought that they
would be able to model a large part of the visual system over one summer, of course, we're still
working on these problems 50 years after, and there is no end to it, but also at MIT these two
guys, so one of them appeared already in the picture, Marvin Minsky in the same work, they
published a highly famous or infamous, if you want, book called The Perceptrons, where they
introduced mathematical analysis of these networks proposed by Rosenblatt, and they showed, for
example, one example that is taken from this book, that a very simple logical function like
exclusive or could not actually be implemented as a perceptron, so these patterns are not
linearly separable, so that was a very harsh criticism for the models, and some people say
in retrospective that this was what triggered the AI winter, so some people even say that there might
have been some personal animosity between Rosenblatt and Minsky, they went to the same high school,
and Rosenblatt also died in a boating accident, so as far as to suggest that it was a suicide,
well, the story is probably much more mundane, so the funding that was cut by government agencies
like DARPA was more related to them being more pragmatic and budget restricted, and that had an
impact on the field that coincided with the publication of the book, but if you look at the
substance of the problem, what Minsky and Pebert called in their book Perceptron was actually
not exactly an architecture that was devised by Rosenblatt, so they actually clearly stated, so
they refer to this architecture as simple perceptrons, and this is what we now typically
understand by this term, so it's, as you know, it's just a linear combination of the input
coordinates with learnable weights that go through a nonlinear activation, typically sigmoid or
in simple cases sine function, but what is also very important, they were probably the first
ones, at least to my knowledge, to use geometric approaches to machine learning problems, so
they, for example, formulated this group invariance theorem that tell in which or to what kind of
transformations in the input patterns the neural network will be invariant, and another interesting
thing actually the subtitle of the book is an introduction to computational geometry, so that
was the sixties, so this term didn't exist, they actually introduced it, and one of the reviews
of the book that was critical was asking whether this is just some kind of new mathematical fad
that will go away a few years after, and you probably know computational geometry is now
very well established field, so it has remained there for a long time, but if you go into the
substance of the discussion is actually the question is what kind of classes of functions
these neural networks could represent, right, so what is what is called the expressive power,
and there were results at that time, so coming from mathematicians in particular
Komogorov and Arnold working in the Soviet Union that claim that you can take multi-dimensional
functions and decompose them in this way, and basically any function could be decomposed in
this way, so these kind of results are known as universal approximation, they go probably as far
as the thirteenth problem formulated by David Hilbert, and the modern statements of these
theorems are usually attributed to Seybenko and Hornig, these are late eighties, early nineties
that are specific to deep neural networks, so what these results say is that if you have not a
single layer of perception, but two layers like this, then you can approximate any continuous
function to any desired accuracy, so the results it's a class of results, it's not a single result,
but roughly the way that we can understand it is that with just a configuration like this,
with just two neurons like this, you can approximate, you can represent a step function,
once you can represent step functions, you can decompose a continuous function into
tiny little steps, so the proof is slightly more involved, but that's that's roughly the idea.
Now it's sort of constructive proof, and there are different versions for limited widths or
limited depths, it's sort of constructive proof, so it doesn't tell you how, so it's an existence
result, so it tells you that you can find a neural network with potentially a very large
number of neurons that approximate functions, and if you look at machine learning problems,
so in a very maybe simple and naive setting, like classifying images of cats and dogs,
basically you can think of it as some cynicism say that deep learning is a curve fitting problem,
so it's multi-dimensional curve fitting, so there is some kind of black box where you put
some something that acts as a universal approximator, so some sufficiently rich
architecture, and you try to represent the function that distinguishes between cats and dogs
in this way. Of course this is not a well-defined problem, if I give you a finite sample of,
let's say these are cats and dogs, I can pass infinitely many functions through these points,
so I can interpolate these points in infinitely many ways, so we need somehow to restrict our
class of functions, and that's typically what you do by imposing some sort of regularity,
and mathematicians have very well understood concepts of regularity like Lipschitz continuity,
right, so in simple case you can think of it as a function with bounded derivatives, right,
and the problem is what happens when you increase the dimensionality of your input when it doesn't
look like a one-dimensional curve, but it's an n-dimensional curve, and here the results,
unfortunately, are not favorable because you can show that as you grow the number of dimensions,
the number of samples that you need to take in order to approximate a function to accuracy epsilon
grows exponentially with a number of dimensions, so this is again, it's not a single result,
it's a class of phenomena that are called the curse of dimensionality, so it's a statistical or
geometric phenomena that explains how functions behave in high dimensions, and the term itself
actually goes back to Bellman who spoke about the curse of dimensionality in physical problems.
Now, in these problems, simply if you think of images, right, of even something like 30 by 30
pixels, which is probably the smallest image you can imagine, digits from the MNIST dataset,
the number of dimensions will be approximately thousands, so if you count the number of samples
that you need to take, if you took this kind of straightforward approach, it will probably be
more than the number of, not only the cats or dogs on earth, but probably close to the number of
particles in the universe, so there are not sufficiently many animals around just to do it,
and this kind of problem of curse of dimensionality, right, or you can also call it combinatorial
explosion, was brought up in another report that is associated with the AI winter, which is the
Lighthill report in the 70s. It was commissioned by the analogy of the DARPA, or basically the
British funding agencies, and that was the point of time where they decided to stop funding these
crazy ideas in looking at neural networks. So what happened, of course, was this AI winter,
but at the same time people continued working on these architectures, and some interesting ideas
came from the field of neuroscience, from the study of the organization and the function of
the visual cortex, the famous experiments in the 50s and the 60s done by Hubel and Wiesel,
a duo from Harvard that won the Nobel Prize in medicine for understanding the organization
of the visual cortex, where what they found is that the cells in the cortex were organized
with some local shared weights, and this was reproduced by Fukushima in his famous neocognitron
paper in 1980. So the idea here was a neural network that does, it has two types of neurons,
so neurons that he called simple neurons and neurons that he called complex neurons, so one
in modern terminology that would correspond to local filters and pooling operations.
And he worked on OCR type problems, so character recognition, and the problem,
of course, if you treat this type of problems with the standard perceptrons, if I give you a
digit of digit three, and I move it just by one pixel, you see that the input into the neural
network can change dramatically, and in fact he complained that perceptrons were not by design
invariance to these translations. So his architecture actually is remarkably modern
by modern standards, so it was seven layer networks, so I think we can call it deep by
modern standards. It had local connectivity, what neuroscientists called receptive fields.
The filters were non-linear, and he wrote in neuroscience terminologies, so he talked about
inhibition and activation. He had average pooling, so these are the complex layers.
He used reloactivation, so already in the late 60s that was common, but the training was not done
using backpropagation, so it was a kind of unsupervised type of clustering approach.
And backpropagation, again, it existed already in maybe some other forms, but this is how neural
networks were trained. So Rosenblatt had a special rule for a single layer perceptron,
then there were some other methods that were developed in the 60s, and then backprop became
really popular in the 80s, starting from the paper of Rumelhardt.
So Lecante was just a fresh graduate at that time, and he was working on, actually,
the application of backpropagation neural networks, was interested in Neocognitron,
and he also happened to work at AT&T, which they were developing the first digital signal
processors at that time. So basically, there was a good application to implement on a DSP,
and basically, he stripped down the kind of neuroscience terminology of Fukushima,
replaced nonlinear filters by linear filters that could be implemented as convolutions.
Actually, the first paper never mentioned the term convolution, and the name came after in,
I think, in 89, or even later, and he was able to show in real time complex pattern recognition
tasks, such as recognition of handwritten digit, which was a difficult task at that time.
It was actually deployed in commercial applications. They were working with banks
in the post office, and that's where the MNIST dataset comes from. But the computer vision
community took a different path, and the late 90s and the early 2000s, probably until the first
decade of this century, the approaches that you would typically use for image recognition
were to detect some local features, then compute local feature descriptors, and then create some
representation that would be passed into some very simple classifier, like a support vector
machine, which also were considered to be more favorable by mathematicians, because
you can prove, for example, global optimality results about them. So there are many papers
written, so SIFT, the scale environment feature transform, one of the most cited papers in computer
science ever, was extremely well engineered detector and feature descriptor for these tasks.
And what happened in 2012, as you probably all know, that all these carefully designed handcrafted
features were beaten down by a very large margin by a convolutional neural network. So it was this
lack of confidence of the computational capabilities of hardware, the GPUs, as well as availability
of large data, the image net benchmark, which contained millions of annotated images, that
finally allowed these architectures to shine. And since then, all the best results in this
benchmark were by deep learning. So if you look at the AlexNet architecture that won this benchmark
in 2012, it is more or less the same as what was done by Lekana. It's just slightly deeper. There are
some different architectural choices. It has way more parameters. It was trained on GPUs, which by
the way was not novel. GPUs were used for general purpose computing at least a decade before and
for training neural networks probably at least seven years before. So in a sense, it was a very
careful engineering and application of existing ideas on a very large dataset and a very important
problem that convinced everyone. Yeah, there is a question. Sorry to go back a couple of slides,
but you were mentioning Fukushima's implementation and no sort of back propagation learning. So
like was he giving some kind of neuroscience, how do you say, a proof for this? Like some kind of
happy learning or how did the... I don't remember what kind of rule he used. I think it was inspired
by some hypothetical ideas of how the brain learns. Yeah, but it was not back propagation.
He showed many other things. So he showed, for example, geometric stability, stability to noise,
but again, it was in the early 80s. So the training examples were very rudimentary. So
black and white letters. Thank you. Right. So basically, all the rest is obviously history,
right? So the people who started the deep learning thing got the Turing Award and now
very famous. And basically, this is technology that has really transformed the field both
the academical subjects and the industry. So just to give you, we'll be talking about
graph neural networks and you've probably heard from Miguel as well in the previous lectures
about graph neural networks. So just to give you a little bit of history of this one,
so they're actually very much related and rooted in chemistry. And chemistry is probably one of
the fields of science, which is data intensive. It produces a huge amount of data, experimental data.
And since early times, chemists tried to organize these data first, publishing these
humongous books containing information about molecules or chemical reactions. And then
it appeared the first digitized archive, the chemical abstract service. And also with the
appearance of the first computers came the need and the idea to search for molecules, right? So
if, for example, you're a pharmaceutical or a chemical company and you want to patent a new
molecule, how do you know that it has not already been described, right? So you need to search fast
for molecular structures in some data set. And these were the first ideas of chemical ciphers
that describe a molecule as a string and then try to match it in some data set. So that was the kind
of problems that these guys, or he was a Romanian chemist called Vladus, that was one of the pioneers
of a field that later became known as chemoinformatics, he was trying to look at molecules as graphs.
And as you probably know, the term graph itself, in the sense of graph theory,
also is associated with chemistry. This is how Sylvester called the first attempts to design
structural molecules, structural formula of molecules, basically trying to understand how
atoms are related to each other with their chemical bonds, not only just the number of atoms
and the types of atoms. And as you probably know, this was one of the first structural formula of
benzene that was derived by Kecoli. And the legend says that he dreamt of a snake biting
his own tail, so he came up with this kind of aromatic ring that is described here. So
these kind of problems inspired these duo of mathematicians, we'll talk about them later,
Weisfried and Lehmann in the 60s to devise an algorithm that would test whether two graphs
are structurally similar, what is called the graph isomorphism test. And these ideas maybe were
noticed, so there are many related works in the machine learning community and also in
a chemical community, trying to devise new types of neural networks that could take as input,
not vectors, not images, but graph structured data. And the early works by Alessandro Sperduti
in the 90s, for some reason most of the works were by Italians, and probably the most cited ones
are by Marco Gore, Scarcelli and others. And interestingly, about 10 years ago, the graph
neural networks returned triumphantly to chemistry, so I think worth crediting David Duvenau and Justin
Gilmer for who also introduced the terminology of message passing neural networks that try to
predict properties of molecules, model these graphs and learning on these graphs. And then,
of course, the ideas of geometric learning, as we'll see maybe with some extra stuff.
Also, the structural biologists have had their own image net moment with alpha fold, first in
2018 and then in 2020, basically predicting the structure of protein folds. So, and this field
is very rapidly developing. So, I think these are very exciting and cool problems that you can
address with geometric techniques. So, let's just try to summarize basically what this historical
excursion gives us, a kind of blueprint for different architectures. So, if you look at convolutional
neural networks and graph neural networks, right, they work with very different data, convolutional
neural networks work on images, graph neural networks work on graphs, right, let's say molecules,
but there are some common patterns, right. So, in both cases, we have some underlying domain.
So, in the first case, it's agreed. In the second case, it's a graph. We also, in both cases, have
some kind of geometric operation, so a symmetry, right, that is a nature in the context of the
problems that we are considering. So, in images, it's translation, right. So, I want to move,
for example, an object in the image and I don't care where the object is located if I want to
classify it. In case of molecules, it's a permutation symmetry, so no matter how I order
the atoms in a molecule, it's still the same molecule, right. So, I want somehow to be
insensitive to this ordering. And we can also define natural operations that respect this
symmetry, right. So, in case of convolutional neural networks, it's actually the convolutional
operation. Basically, I can move a patch around an image and apply the same weights or the same
local rule that would extract the features. And as we'll see in graph neural networks,
this is some kind of local rule that we call message passing, right, or some versions of it.
So, these are ideas that, as you see, these are two examples or architectures that share the
common principles and that's the idea of geometric deep learning. So, I can probably take the craze
of inventing the term. So, that happened when I was writing one of my ERC grants back in 2015,
probably. And of course, everybody was doing deep learning. So, you need to distinguish yourself
from everyone. So, I wrote that we are not doing deep learning as everybody else, we are doing
geometric deep learning. So, that was the idea. Then we popularized it in this paper in IEEE
signal processing magazine and more recently in a book that I'm writing with my collaborators.
So, by analogy to the Erlangian program, we basically, we can think of a kind of common
denominator for machine learning architectures. So, we would like to take this zoo of different
architectures that were historically designed for different types of data with different
kind of problem in mind and look at them from the same perspective. And this perspective is
through the lens of group theory and properties like invariance, equivariance and symmetry.
And if we've seen before this problem of machine learning in high dimension, where you have,
for example, your images of cats and dogs as points in a high-dimensional space, we no longer
consider these inputs as just a high-dimensional vector that you need to put through some generic
class of functions and then you suffer from the curse of dimensionality. But now we know that
there is some domain, geometric structure that underlies these inputs. And in images, for example,
this is a two-dimensional grid. So, our data lives on some typically low-dimensional domain.
And this domain comes equipped with some group. So, in this case, it's the group of translations.
And so, we have a domain, we have a group, we have signals that live on this domain and the
group that acts on the points of the domain, we see how it can act on the signals themselves
through what is called the group representation. And in case of images, again, the group representation
will be just the shift operator. We'll see it in more details. And then finally, we have functions
that act on these inputs that somehow need to respect this symmetry. And this will be through
what we will call invariance and equivariance. So, basically, we want the function either to be
insensitive to how I transform the input by acting on it with the group or should change in the same
way. And I should say that the choice of the group and the domain are two separate things and they're
not only dependent on the data, they're also dependent on the task. So, I might have a problem
like this. So, I have, for example, images of traffic signs. And if I'm designing a self-driving
car, it's very unlikely that I will see rotated traffic signs, right? They will probably be aligned
and move just horizontally and maybe vertically. So, here, for example, the group of transformations
that is reasonable to assume will be just two-dimensional translation, maybe even one-dimensional
translation. But if, for example, my car can also tilt, right, so imagine that it's a plane and not
a car, then rotations are also perfectly valid, right? So, then the group of transformation
will be different. So, it's still the same domain, the same data, but the task might be different
and, therefore, the assumptions about this, the priorities problem might be different.
And if you think of another application, if I have, for example, a pathological sample,
so, essentially, a glass of some stained tissue that I put under a microscope, so there I can also
have reflections, right? Because I don't have canonical orientation for this glass. So, it can
be an even bigger group in this case and, again, task dependent. So, as I mentioned in case of
images, the representation that we'll be working with is the shift operator. In case of crafts,
actually, the symmetry, as we've seen it, is the group of permutations, what is sometimes called,
confusingly, the symmetric group. So, it's the different ways that I can rearrange
and different objects. And its representation is a permutation matrix. And as we'll see,
the way that it's implemented, so, functions that are equivalent with respect to this symmetry
are message passing. And we can also have another type of architectures that are also confusingly
called equivalent neural networks or equivalent transformers, where, in addition to the symmetry
group of the domain, we also have symmetry group of the data. And this is typical for geometric
graphs like molecules, where the nodes also have geometric coordinates. So, they live in three
dimensional space. And in addition to reordering the atoms in the molecule, we can also rotate or
translate the molecule in three dimensional space, right? So, you want to be equivalent with respect
to both transformations. So, it's a kind of analogy of the external and internal symmetries
you have in physics. And, basically, geometric architecture is just sequences of
equivalent or invariant layers. You can also interleave them with pooling. So, I will not talk
about it too much. But pooling is implementation of another principle that is important in physics,
which is called scale separation. And this is what makes physics work. So, if you consider, for
example, let's say, this room, right? And how we are surrounded by probably a quadrillion of
different molecules that move very fast and collide with each other. But that's not how we can model
the behavior of gas in some space, right? It's computation intractable to trace all the molecules.
So, fortunately, there are just a few parameters that explain statistically how these gas behaves,
right? Like temperature and pressure. And this is the main principle of statistical mechanics.
But if we want, for example, to model how Earth, with its very complicated atmosphere,
moves around the sun, again, we can completely disregard it and consider it as a point. Because
at that scale, all these details are completely irrelevant. So, the scales, of course, interact
with each other. So, this is maybe a wishful thinking. And in neural networks, you can also
mathematically show that in some architectures, pooling operations are necessary for them to
operate correctly. So, these ideas can be applied to different types of objects, to geometric domains.
So, traditionally to grids, but then also to graphs, maybe to general
homogeneous spaces, and then also maybe to more exotic things like manifolds, meshes and
geometric graphs. And if you look at some of the standard architectures that are very commonly
used in deep learning, whether it's CNNs or maybe LSTMs or deep sets or transformers or GNNs or
intrinsic mesh convolutional networks, they can all be derived from the same blueprint. So,
there is some kind of domain and associated symmetry and associated environments that you
can bake into your architecture and you get, basically, particular instances of this blueprint,
some of the most common and famous architectures. Any questions so far before we start talking
in detail about graphs? So, if I have time, I will try to cover all these different domains, but I
would probably spend most time on graphs and also some physics-inspired perspective on these
architectures. Yes, I will have questions. So, if you kind the term geometric deep learning,
is there an alternative, like non-geometric deep learning or like traditional deep learning,
or what does it mean? Well, so, it's a very broad term. So, I think we use it maybe in a very
in a very broad sense. So, it's using geometric ideas or geometric techniques to
interpret and build deep learning architectures and vice versa, applying deep learning to
geometric objects. Now, whether, for example, the model of group environments and equity
environments is really the kind of ultimate truth, of course not. So, it's a mathematical
abstraction and in most cases, the transformations, for example, you have an image is, they're
actually not well described by groups, but at least it's a good starting point. In many cases,
for example, in molecules, this is kind of physical realities or an inductive bias that you
rather want to incorporate in your architecture. Okay, I have one more question. Could you please
take back to the previous slide? Yeah. Because you mentioned like the existing
neural networks architectures and the group of symmetries. And do you think there is any
other group of symmetries that we in real world applications should think about? And
because of that, create some new architectures that is, I didn't know, for example, was suited for
them? Yeah. So, it's a good question. So, there are some other architectures, for example, mostly
coming from physics. So, of course, in physics, you have interesting groups. So, I think there are
some Lorentz environments, for example, neural network architectures. You can also combine some
groups. I will show some examples that you can have, for example, products of permutations or
some special group products of permutations in subcraft neural networks. So, it's a general
blueprint. So, if you can follow the blueprint and design architecture that implements this
invariance, that is relevant for your problem, then you have a way of doing it. Okay. Cool. Thanks.
There are also some works that try to discover the symmetry group from the data and from the
problem. So, that's also an interesting direction. Could you shortly address the symmetries which
the transformers go for, because I still have difficulty to think in this symmetry, like,
perspective? So, I will talk. Well, it will take me probably about 20 minutes, but we'll get to
transformers. Yeah. So, transformers are special types of craft neural networks. You can think of
them really quickly. So, on the slide, there's the LSTM. So, it's written that time warping is
you could have architectures that are invariant to time warping, but if you look at speech recognition.
So, maybe first of all, we humans also don't have that, right? So, if something is super
slow down, we're probably not going to notice what it is. So, my question is, is there any
other invariance or equivalence for time series data that you can think of? So, well, here what
I mean by time warping, you can actually show that gating is a necessary mechanism to be able
to accommodate time warping. So, basically, gating emerges as basically as the architecture for
this kind of, for this kind of invariance. Okay. Got it. Thanks. Yeah, I think there is a question there.
So, okay. I mean, so you said that you will talk about transformers more in, like, in the next few
slides, but I wanted to ask about one of their emergent properties, like in relation, for example,
to CNNs, namely that CNNs have, like, kind of encoded in them by definition translation invariance,
whereas transformers don't. And I think it has been discovered that transformers actually kind
of learned the translation invariance from the, thanks to the amount of data that they, that they
consume. But, like, they weren't defined to do that. It's kind of, this translation invariance
just emerged from the data. So, I was wondering whether, in your opinion, it is better to, like,
use more generic architectures, which learn those inductive biases from the data, or whether we
should encode the inductive biases in their architecture, or, and, you know, face the risk that
a specific inductive bias may be also a limitation. Yeah. Well, I think the answer is already contained
at least one of the answers in your question, right? So, the amount of data is obviously a limitation.
So, it might be easy to collect data, like images, right, or maybe text. It might be much more difficult
to collect data that comes from biological experiments, right? So, if the data is limited
and the data is expensive, then probably you want to work hard to incorporate as many inductive
biases as you can. In some other applications, actually, the inductive biases, or some symmetries
or some invariances come from the problem that you're trying to model. I think it's
true for many applications in what is called the AI for science, right? So, if you have some, I don't
know, physical system, you know that certain properties will be conserved. So, it makes no
sense just to use a generic black box that will be producing unrealistic outputs, right,
that are physically incorrect. But there is no, so it might be that in problems where you don't
know a priori what symmetry or invariance you have, it might be a good idea to use transformers if
the computational complexity and the scale of the data allows it.
Hello, I have a question about augmentation. It feels like, at least in the image case,
augmentation is basically exploiting this group symmetry, right? You augment by
doing some translation. But if you use some geometric-based architecture, maybe then you
don't need to do augmentation anymore. So, what's your opinion of augmentation? Is it like an artifact
or something? Well, augmentation is a technique of, basically, when you have limited amount of
data, you can generate synthetic data that looks like the kind of data that you want to see in
your application. This was actually one of the important features of the AlexNet, for example,
so they use certain type of data augmentation for images to make it more robust. So, again,
you can incorporate this kind of inductive biases into the architecture. Sometimes,
it might be difficult. So, another aspect that is often overlooked is actually the hardware,
right? So, it's very easy and it's probably more a coincidence. At least originally,
the convolutional neural networks map very nicely to the type of hardware that is used in
the GPU, the single instruction multiple data type of architecture. So, it was not by design
because the GPUs were designed for different types of problems. With other architectures,
it might not be the case, right? So, with graphing a lot of programmable, this is not the case.
So, there might be some better, so to say, architectures, maybe from mathematical standpoint,
but they're just not as convenient to implement. Therefore, maybe you will prefer to use
something that is less correct, but you can do data augmentation. The hardware allows you to
implement this architecture better. What do you think is a follow-up question about
augmentation? What do you think about augmentation? Not as a simple tool for increasing the size of
the data set, but as something as in contrastive learning, as enforcing the symmetry. Can it be
kind of equivalent to the symmetry you defined? Well, it was used in this formula. I mean,
can the load of enforcing the symmetry be shifted towards augmentation to the model?
Yeah, so basically, you're sampling points from the group orbit, right? So, you can think of it
this way as well. Whether it's enforced in a hardware or in a software, that's probably not
that important. So, data augmentation is a very valid technique, if you know exactly how to do it.
Okay, so let's move on to graphs. And, well, graphs, as you probably know, their idea itself
is pretty old. So, it's usually attributed to Euler who was thinking of these kind of problems,
right? How you can connect land masses without actually accounting for the particular geometry,
but only how, what is nearby, right? So, the famous problem about the bridges of KÃ¶nigsberg,
and this is what he called the geometry of Cetus, or basically the geometry of place,
what in modern terminology we call topology. So, the term was actually introduced by Pankareho,
by analogy to Euler's terminology called analysis Cetus, and that was also where his famous conjecture
appeared. So, we'll talk about Pankareho conjecture actually later. I hope to get there as well.
So, graphs, obviously, I don't need to convince you that graphs are interesting and important. So,
more or less anything from very small scales or very large scales can be modeled as a graph.
So, any system of relations or interactions, whether it's a molecule, or you model how
different atoms interact with each other through chemical bonds, to, for example,
interactoms, right, in biological science, how different entities in our body or in a cell
interact with each other, different, for example, chemical reactions, or even social networks,
right, describing relations, friendship, interactions between different people.
So, this model is a graph, and again, graphs can be of different types. So,
let's consider a simple model here. So, we'll consider an undirected graph. So,
it means that we have a collection of nodes, and we have unordered pairs of nodes as edges,
right? So, just pairs, basically, the order doesn't matter, and the nodes are described
by d-dimensional vectors. So, these are features that are attached to the nodes. And, of course,
it can be more complicated. So, you can have graphs that are directed, you can have both
continuous and categorical features, both in the nodes and the edges. But, just, that would be
already interesting enough to look at this kind of object. So, one thing that characterizes graphs,
right, basically, this is, again, a topological construction. So, it's an abstract object that
lives on its own. The moment we need to represent it on a computer, we describe it, for example,
as a matrix, right? So, we can describe the structure of the graph as the adjacency matrix,
right, of size n by n, and it's a number of nodes. So, we have one, if there is an agent
between a pair of nodes and zero, if there is no edge, right? And if the graph is undirected,
then this matrix is symmetric. And the features, we can describe them as a matrix of size n by d,
right? d is the dimensionality of the node features. So, one key thing here, which is already
written on this slide, is that we don't really have a canonical way of ordering the nodes of the
graph. So, when I make this description, I automatically assume some ordering of the nodes,
but this ordering can be like this or can be like this. So, anything that will take
this description of the graph as an input must account for this built-in ambiguity, right? So,
I somehow need to be able to produce outputs that disregard, in a correct way, all these possible
permutations, right? And the two types of problems that we can consider in relation to graphs,
but actually more problems, but let's say these are the prototypical problems.
One is graph-level problems. So, I give you an input graph and I try to
output a number that describes this graph, right, or maybe a vector. Like, for example,
I'm predicting the chemical properties of this molecule, like water solubility, so the input is
a graph describing a molecule, the output will be some number, right? Another class of problems,
I give you a graph and I want to do node-level decisions, right? For example,
I want in a social network to classify which of the users is behaving badly, right? Maybe a
spammer. So, in the first case, I give you a graph, right, and the output, no matter how I
permute the inputs, should be the same, right? So, we're talking about a permutation invariant
function. So, mathematically, it can be described like this, so, right? So, the function here now
is a function of x, the node features, but also the structure of the graph. So, they're both
from the input, and here I act on these two inputs with the permutation matrix, which is
the representation of the permutation group. You see that actually the representation is different
for different types of objects. So, the features, you can think of them as vectors, right? So,
I permute only the order of the rows. The adjacency matrix, it's two-dimensional tensor,
so, I act both on rows and columns, right? I need to permute both rows and columns. Here,
this is clear. And together, this form permutation invariant function. In the second case, if I want
node-level predictions, the output has the same structure as the input, right? So, if I change
the order of the inputs, the order of the outputs is expected to change in the same way, right?
And here, I'm interested in permutation equivariant functions. So, equivariance means changing in the
same way. So, the output now will be, well, some kind of vector, right? Or a matrix, you know,
by f capital. And if I permute the input, the output will be permuted in the same way, okay?
Now, what are graph neural networks? Essentially, these are parametric graph functions. So, I provide
you a graph as an input, right? Or these matrices x and a, and I output something, right? And
something here, as we'll see in a few minutes, is parameterized by some vector of parameters.
And sometimes, there is no distinction made between graph neural networks or message passing
neural networks. So, we want to make this, that is distinction, but sometimes they're used synonymously,
right? So, most of the graph neural networks that are used in practice are of the message
passing type. We'll see exactly what it means in a second. And graph neural networks, again,
you can consider them as a special instance of these geometric deep learning blueprints. So,
we have usually a sequence of permutation-equivariant layers that produce node-wise predictions. And
permutation-equivariant, in the sense that if I change the order here, it will change in the
same way here. And then, if I have graph-level tasks, I will have permutation-invariant pooling,
right? That aggregates all the information from the, from these node features and produces a
single output for the graph. And the typical way that, that they work is by neighborhood aggregation.
So, you can pick up a node in the graph, right? Let's call it i. And now, I look at the neighborhood
of the node. By neighborhood, I mean all the nodes that are connected to i by an edge, right? So,
this is how the neighborhood of i looks like. And I can look at the feature vectors associated with
these, with these nodes. And you can see that even though the neighbors are unique, right? The
feature vectors are not unique. So, this is encoded by color. So, these two nodes have exactly the
same feature vector, right? Just for this example. So, together, they form what is called a multi-set,
right? So, it's a set where the same object can appear more than once. And we also have the
feature vector of the node itself. So, I want a function to aggregate them locally, right? Let's
call this function phi. And again, the, the characteristic property of this function is that
I don't have a canonical ordering of my neighborhood. So, the feature vectors can appear like this
to, to this phi. So, it must be by design permutation invariant, right? It cannot assume any order
in which the neighbors come. We can make it more complicated and we can incorporate additional
information. But again, as a basic structure of a graph, you don't have this order, right?
Now, you can repeat this process everywhere at every point, at every node of the graph. And this is
actually very highly parallelizable, at least in principle. And once you do it, you get an output,
right? That for every node of the graph, you output some vector. And this is also a function
of the graph. And you can easily see that if my choice of this local aggregation is invariant,
then the output is equivariant, right? So, if I change the order of the axis here,
then the output will change in the same way. And most of the graph neural network architectures
differ in the choice of these phi, in how I aggregate locally the features. And while they're
probably zillions of different architectures, most of them fall within the following three
categories. So, the first one is what is called convolutional graph neural networks. And you
can simply think of convolutional neural networks as just summing up the features of the neighbor
nodes. So, this is what I write here. So, the update for the representation for the feature at node i
is the sum of some transformed neighbor nodes, right? Psi will be some learnable function. Xj
is the the the feature of the neighbor node. Here it is. And ai, j are coefficients that depend
only on the structure of the graph. In the simple case, just the elements of the adjacency matrix,
right? And here, sigma is just some non-linear activation, typically very low, right? So,
that's how a convolutional type graph neural network looks like. Yeah.
Is this convolution here equivalent to 1D convolution, something like that?
So, we'll see it, we'll see it in the, well, not in a second, but we'll see it later. So,
basically, you can obtain convolution when the graph is agreed. And we can actually see that
that convolution on the grid is a special type. Basically, it's a unique linear equivalent function.
So, basically, yeah. So, that's why the term convolution is appropriate. So, it's an extension
of conversion to graphs, right? And you can write it in this form, right? So, if I write it as
matrix multiplication, so x is my feature matrix of size n by d, I multiply it from the left by
some matrix a, right? The diffusion matrix. It can be the adjacency of the graph. It can be
something else. But, basically, it propagates information between adjacent nodes. And on the
right, I have, in the case of a linear transformation of the nodes, it's a learnable shared matrix
that acts on every node in the same way, right? On the features of every node in the same way.
We'll see that it's related to diffusion equations on graphs. And this is probably the simplest
version of a GNN. It's highly scalable. You can basically just large matrix multiplication.
It has been used in industrial use cases. I think the first to use these kind of architectures was
Pinterest with Pinsage. We also used it at Twitter. And then there are some statements about these
architectures in the kind of graph neural network folklore saying that it works only on homophilic
graphs. So, by homophily, I mean this assumption that my neighbors are similar to me. Typically,
the assumption is that the labels in the neighborhood are somehow similarly distributed to the label
of the node itself. And here's an example of a homophilic graph versus a heterophilic graph.
So, and the usual motivation that is given that these matrix a typically will look like a low
pass filter, right? So, you're somehow averaging your neighbors. And if the neighbors are of the
same type, then it will work. And if the neighbors are very different types, then
it makes more harm than help. And this is actually not true. So, I hope to convince you that
the story is much more nuanced. There is also this channel mixing matrix. And there is an
interesting and subtle interplay between these two matrices. And we'll be able to see how it
works when we consider graph neural networks as differential equations.
So, slightly more interesting architecture is an attentional GNN. So, here, again, we can think of
it, at least in some settings, as a linear combination of the features of the neighbors,
maybe transformed by some learnable function. But now the coefficients depend not only on the
structure of the graph, but also on the features themselves, typically through an attention
mechanism. So, the most famous representative of these architectures, they got the graph
attention network. And in the most general case, right? So, we can write got like this. So, that's
if we have a linear combination, some, so we have linear combination with the matrix. But now
the matrix is actually a matrix valued function of x, right? So, it itself depends nonlinearly on x.
And the most general case is a message passing architectures where we have a b-variate function
that depends on the feature of the node and the neighbor. And the other cases are specific
cases of this architecture. And it was shown that message passing GNNs with specially chosen
aggregation function, in particular, it has to be injective in certain restricted settings,
again, allow me not to go into the details, are equivalent to graph isomorphism testing
algorithm that was derived by vice versa and lemon that I mentioned in the beginning.
And let's talk about this. So, basically, from theoretical standpoint, what actually
graph neural networks do, right? Or message passing type graph neural networks do.
So, first of all, what is graph isomorphism problem? It's telling when two graphs are the same,
right? So, I'm writing here equal, but of course, it's not equal. So, it's equivalent in some sense.
And what do I mean by this equivalence? What I want to say is that there exists an edge preserving
bijection between the nodes of these graphs, right? So, in other words, I can find a correspondence
between nodes in G and G prime, such that if there is an edge between a pair of nodes in G,
then there is a corresponding edge in G prime, right? So, is this bijection unique?
What do you think? No? When is it not unique?
Exactly. When the graph has symmetries, right? So, this is an example. Actually,
this graph is a good example, right? So, you can reflect the nodes on the left and the right,
and then we can have another bijection, right? Like this. So, this bijection is not unique.
But for determining if two graphs are isomorphic, it's sufficient to say that
there exists such a bijection, right? So, that's what tells us that the two graphs are equivalent,
right? So, basically, they are the same up to the ordering of the nodes, right? So,
if I look at their adjacency matrix, they will be the same up to applying some permutation,
right? So, I can basically, I can relate the two adjacencies by permutation.
And related to the question of universal approximation, right? Which is fundamental for
traditional neural networks like perceptrons, we can show that a class of functions is universal
approximating permutation in variant functions on graphs with, importantly, here, the limitation
finite node features, if and only if it can discriminate graph isomorphisms, right? So,
basically, universal approximation is equivalent to graph isomorphism testing, right? So, basically,
the two things go hand in hand. And if you ask what kind of graphs can we represent with
message passing neural networks, right? So, here is, let's say, the space of all graphs.
And these would be graphs that are structurally equivalent, right? That is isomorphic. So,
by construction, we know that graph neural network cannot distinguish between these graphs,
right? They're exactly the same up to the ordering of the nodes. So, just by construction,
it will produce the same output for any isomorphic graphs. But the question of the opposite
direction is more interesting. And this is not necessarily guaranteed, right? So, I might have
different graphs that are not isomorphic, like the reds and the blues, that by chance will have
the same representation. So, the graph neural network will output the same output for these
different graphs. So, in other words, if we have the space of all permutation in variant functions,
right, and we know that these are all graph isomorphism discriminating functions, this is where
we'll see a subclass of functions that can be computed by message passing.
Okay. And so, the question of graph isomorphism, as I mentioned already, it came from applications
in organic chemistry, where people try to compare structures and try to determine whether
two molecules are the same, right? So, in the case of isomorphism is a special setting, right?
We can also think of distances between graphs. And vice versa in 68 came up with an algorithm
that they believe to be a polynomial time method for determining whether two graphs are isomorphic.
So, I should say that at that time in the 60s, even the notion of complexity was not totally
spelled out. And also, the understanding of what's the complexity of graph isomorphism testing
as a computer science problem was not understood. Actually, it's not understood even now. So,
we know that it's not NP-hard and we also don't know polynomial time algorithms for it. So,
it's a special complexity class that is called GI class. But anyway, so, it was actually disproved
by a counter example. So, it was an example, it was shown that the class of graphs that cannot be
tested by the device for an algorithm. We'll see such examples in a second. But the way that
it goes, it's essentially a color refinement procedure. So, it considers a graph without any
features, considers only its structure. And, initially, the graph has every node labeled in
the same way. By label, I just mean a natural number that is attached to a node, right? And what
the algorithm does, it takes a node and looks at its neighborhood, right? And you can see that
originally in this graph, we have two types of neighborhoods. So, we have a blue node with
two blue neighbors like this. Sorry, that's blue node with three blue neighbors and blue node with
two blue neighbors, right? So, these are two neighborhoods that we see in this graph. So,
if I now apply an injective function that they call phi, right? Think of it as hashing. I will
have two distinct outputs, right? So, I will have nodes of the yellow type, let's call it, and of
green type, right? So, I will be able to distinguish between these different neighborhoods. So, now,
I have a graph with refined labels. I can apply the same procedure again, and now we have three
types of neighborhoods, right? We have green with one green and one yellow neighbor. We have
green with two yellow neighbors, and we have yellow with two green and one yellow neighbor,
right? And these become, again, distinct colors. So, this will be, let's call it violet, gray,
and orange. But if I repeat this procedure again, the colors will stop changing at which
point the algorithm stops and produces a histogram of colors, right? So, that's a graph level
descriptor. You can think of it this way. And what they show in the paper, well, the paper is
actually complicated to read, but that's, let's say, a reduction of it. It actually describes
a different type of algorithm, what is called 2WL, that does edge color refinement, but it
doesn't matter. It's equivalent to what I'm showing here. So, if I give you another graph,
and the distribution of colors is different, then I can guarantee that they are not isomorphic. But
if the distribution of colors is the same, like in this case, we actually don't know. So, it's
unnecessary, but insufficient condition, and in fact, you can find examples of non-isomorphic
graphs that would be deemed equivalent by WL test, right? Or in this case, WL test cannot
determine whether they're not isomorphic. And you can also see why the reason for it, right?
Basically, what it does, it refines the colors of the nodes, so every node looks at its neighborhood.
And this is how the neighborhoods of nodes look like, right? So, this node has two neighbors,
then this node has, again, two neighbors, and so on and so forth, right? So, if you look at the
structure of these neighbors, they will be exactly the same in both cases, right? And actually,
very simple examples of graphs, for example, regular graphs where the degree of every node
is the same cannot be tested by this simple procedure of Weisfeld and Lehmann. You can also not count
connected patterns of more than three nodes, like triangles or cycles. And this is, I think,
astonishingly disappointing given that the algorithm came from applications in chemistry,
so in chemistry, these would be two different molecules, right? And this has a six ring and
this has a five ring, right? So, or five cycle using graph theory terminology. So, we cannot
distinguish these molecules by device for a Lehmann test. They would appear potentially the same,
right? So, we wouldn't know. So, basically, the functions that can be computed by WL are strictly
smaller than all permutation invariant functions, right? And we know examples of functions that
cannot be computed by WL. For example, we cannot count the number of rings, right? So,
if I want to implement a function that counts the number of rings in a graph, I cannot do it by
means of WL test or by means of message passing. Now, the relation between WL and message passing
is not random, right? You can see this, even the structure of the algorithm is exactly the same,
right? So, this is what WL test does, right? So, it updates the color of every node by looking at
the structure of the node and the multi-set of neighbors, right? Here, x denotes the colors.
And this is what MPNN does, right? So, here, the squared denotes some general
permutation invariant aggregator. It can be sum, it can be max, it can be mean, it can be anything,
right? Importantly, it's permutation invariant. So, we can see that it's a special case. So,
MPNN expressive power is upper bounded by device for a Lehmann test. And the question is when
MPNN is as expressive as WL test, right? So, basically, we're interested in this case, right,
when the two circles coincide. And if you look at different types of aggregators, right? So,
imagine that this is your input graph. So, I have this gray node that has two types of
neighbors. We have green neighbors and we have blue neighbors, right? So, if I consider the input
as a multi-set, right? I completely disregard the structure of the graph itself, right? So,
what the node sees is just a soup of the neighbor features. So, if I use a maximum aggregator,
I cannot distinguish between these and these, right? Because for the maximum, it doesn't matter
how many times each of these features appears, right? If I use a mean, then I cannot distinguish
between these and these, right? I can multiply the neighbors by some constant factor. The sum,
though, allows to distinguish between all of them, right? So, you can think that maximum gives
a kind of skeleton of the set and the mean gives you the distribution, but the sum is strictly more
expressive, right? And here's an example of structures that max or max and mean would fail
to distinguish. And indeed, sum appears the most expressive one. And if you assume that,
so the theorem about the equivalence between WL and message passing states that if you assume
that the node features come from a countable universe, then if you have an MPNN with an
injective aggregator, call it square, an update function phi and graph-wise readout function
is as powerful as the vice-versa element set, right? And the assumption here is of discrete
countable features, which is not always the case in practice. And then the reason architecture
that actually implements that is equivalent theoretically to the vice-versa element, which is
the graph-wise morphism network or GIN. So, basically, it uses a sum aggregation. So,
the epsilon here is just theoretical thing. So, there exists infinitely many constants epsilon
that you can use here. So, we know that, basically, there exists at least a choice, right? Within the
all possible message-passing neural networks that makes it as expressive as WL. But, of course,
we are interested in more expressive architectures, right? Can you do better than WL? And here, again,
there is an entire universe of different architectures. So, some of them actually go beyond
message-passing, at least in the traditional sense. And, roughly, you can distinguish between
four different categories of approaches. So, it's either a higher-order WL test. It's not
a single test. It's a hierarchy of tests. The use of positional instruction and coding,
so that's how transformers work. Subgraph GNNs and then topological message-passing, right?
So, let's talk briefly about all of them and then when do we need to do the break?
Sorry?
So, let's maybe 20 minutes and then we do the break. So, the first class of higher-order WL
tests, as I mentioned, so WL test is just one algorithm that was initially developed
and then extended by Babi and collaborators, actually independently also by other people.
So, one of them was Eric Lander, who is mostly known as computational biologist,
but he started as a mathematician. And, basically, this is an increasingly
more expressive hierarchy of tests. Instead of doing node refinement, they look at tuples
of nodes. So, it's obviously computationally more expensive and there is always, you can find a
family of graphs that these algorithms cannot distinguish. So, like strongly regular graphs for
2WL or 3WL tests and what is called CFI graphs for general KWL. I should also say that this
terminology of KWL is confusing because they're what is called folklore WL tests versus the
classical WL tests that are slightly different. So, but overall the hierarchy, right, up to
notation is the same. So, we know that message-passing GNNs are equivalent to the standard WL. You can
also design just replicating in the neural network architecture the KWL tests higher-order KGNN.
So, this is what Hageim Aron did in his works. And then you can also have some other algorithms
we'll talk about in a second that sits somewhere between. They don't exactly follow the
hierarchy of the WL tests. So, the second approach is positional encoding. And, again,
I remind you of this example of two graphs that cannot be distinguished by the WL tests, but
imagine that I could now attach some features to the nodes of the graph, right? And this could be
even something as simple as random features. You see that now, because I have some extra information,
I can distinguish between these cases, right? So, if I look at the leaves, for example, of
this tree, right, I can ask, for example, whether the root appears among the leaves or not, right?
And here it does appear and here it doesn't, right? So, they're clearly different, right? I would be
able to distinguish between them, right? So, the covering of the nodes removes at least to some
extent the ambiguity. Now, of course, if I use random features, then the question is how can I
reproduce them on a different graph? So, these type of approaches are equivalent only in expectation.
But there are other methods that can do better. And these are structural encoding.
So, the idea of structural encoding, you have some substructures, right? So, we have a bank of
substructures that call it H. And you can count the substructures, the occurrence of substructure
for every node or for every H, right? And there are two ways that you can
consider subgraphs, whether what is called a subgraph or any induced subgraph. It doesn't
really matter, right? So, the two ways that are slightly distinct. And for example, in these two
graphs, if this is my bank of substructures, let's say cycles of size 6 and 5. So, in this molecule,
at every edge or at every node, I will count once the six cycle substructure and these nodes,
I will count twice, right? Because this node participates in both structure on the left and
right. But the five cycle substructure doesn't appear here versus it appears here, right?
So, with this encoding, now I have some, these additional features that I can attach to every
node or to every edge of the graph. And I can use them in standard message passing. And this would
allow me to discriminate between these graphs. And the complexity of this method is basically,
it's all hidden in pre-computation, so the counting of substructures. So, in the worst case, it's
order of n to the power k, k is the size of the substructure, so it could be large. But in practice,
for structures, more friendly structures like triangles, there exist more efficient algorithms.
So, in practice, it can be way better. The algorithm itself, and especially the training part,
which is typically more expensive, is standard MPNN. So, it has linear complexity in the number of
edges, or roughly order of n if the graph is sparse. And the theoretical result is that these
kind of architecture that we call GSN, graph substructure network, is strictly more expressive
than WL on certain assumptions on these substructures, right? So, it should not be a
star graph or it should be a structure of size, bigger than three. And basically,
we can formulate it is that GSN is not less expressive than 3WL. You can also do it for
different KWLs. Basically, you do it by counter examples. So, you can design a substructure
that the standard WL tests cannot count, whether it will be clicks of certain type or other things.
Right? And the proof by example is something like this. So, this is a stronger regular graph. It
cannot be distinguished by 3WL, but this graph contains four clicks and this doesn't.
So, if I count four clicks, I would be able to distinguish between these graphs. So, in a sense,
it's a kind of cheating. So, I'm not following really the KWL hierarchy. So, this is an example of
different structures I can count, triangles and clicks. But then, basically, the expressive
power looks like this. So, this is, for example, a graph substructure network with four-click count.
So, it might actually, there might be examples of graphs that are distinguishable by 3WL, but
not by this method. We have at least an example of a family that 3WL cannot detect. So, it's outside
of the hierarchy. And why this is important, especially in applications related to chemistry,
because often we know these substructures are priori, right? Organic molecules, for example,
cycles are a very prominent feature. These are what is called aromatic rings, right? Like in the
molecule of caffeine, we have two, right? So, we have this ring of cycle of size six and cycle of
size five. And we see that if we incorporate this information as a kind of problem-specific
inductive bias into the problem, we are able to much better predict the properties of molecules.
In this case, it's, I think, the water solubility on the, on a toy data set of molecules that is
called zinc. And by incorporating cycles, we significantly reduce the error.
Third class of approaches, what is called subgraph GNNs. And here, the idea is also very simple.
So, if you look at these two graphs, again, this is probably one of the simplest examples of
non-lasomorphic graphs that cannot be tested by WL, right? If you do the color refinement,
this is what WL produces, so the histograms are the same, right? That's exactly the case
where you cannot say anything about the graphs. But imagine that I can perturb the graph, for
example, by removing this edge, right? So, if I did it, the colors will be very different, right?
So, these will be the distributions of the, of the colors, and they are clearly distinct. So, in
this case, the perturbation allows to distinguish between structures that are otherwise indistinguishable.
So, the question is, of course, do I know which edge to remove, right? So, here maybe I was lucky,
and the answer is usually I don't. So, let's remove all possible edges, right? So, let's just
make this perturbation when I remove one edge at a time, right? So, and there are seven possibilities.
I can also do node deletions in the same way, right? And now, instead of a graph, I have a collection
of subgraphs that are extracted by some policy. So, in this case, it's a very simple policy,
one node removal, right? And actually, it results in graph theory that say that if I give you this
collection, so, in terminology of graph theory, this is called a reconstruction. So, if they have
the same multi-set of node remove subgraphs, right? So, this is what we denote H tilde G, right?
So, graphs where, basically, if we look at these kind of multi-sets, they will be the same, of
course, up to, up to reorting. So, the statement in graph theory that is called reconstruction
conjecture claims that under some technical assumptions, if H is a reconstruction of G,
then H is equivalent to G, isomorphic to G, right? So, why it is called a conjecture? Because
it is proved only for small graphs, and it's an open question in general. And there are
generalizations for subgraphs where you remove multiple nodes. So, this is, again, not a single
result. So, it's a class of results. It was introduced by Paul Kelly in his PhD thesis
that was done under the supervision of Stanislaw Ulam, who was a mathematician, a Polish mathematician,
but he's probably more famous for initiating the Manhattan Project and developing thermonuclear
weapons. So, but he also was, he's also famous for many interesting results in mathematics,
and this is one of them, the reconstruction conjectures. So, we don't know whether this is
true. So, it might be true, but that's why it is a conjecture. It would be cool if it were true,
because, of course, in this case, I could test graphosomorphism by just doing something with
this collection. So, what exactly can we do with this collection, regardless whether the
conjecture is true, right? So, it will just give us stronger theoretical property of these
architectures. So, what we can do is we can consider our graph is a collection of subgraphs
that are extracted from the given graph, right? And what is important to understand is that there
is a correspondence between them, right? Because we created these subgraphs, right? So, it's built
on the same nodes. We just might remove some edges, right? Or some nodes. So, we have here two types of
symmetries. So, we have the permutation of the nodes in the graph itself, right? And we also have a
permutation of the subgraphs in this multi-set, right? Because we don't have a canonical order of
them. So, together, basically, the structure, the symmetry structure of this new object of
this collection of subgraphs is a product of two groups, right? If we don't know the correspondence,
there will be a special type of product that allow me to skip the details. And basically,
what we can do, we can design an architecture that does message passing on each of these subgraphs
separately, but then fuses the information across graphs using these known correspondence.
And this is probably more powerful than WL, a version of this architecture that we call
subgraph union network. We can actually show that it's upper bounded by 3WL, and we hope that
it will be equivalent to 3WL. But a few weeks after we published our paper, it was shown by a
counter example that it is strictly less powerful than 3WL. So, we don't know exactly. It's surely
more powerful than 2WL, but upper bounded by 3WL, and strictly less powerful. So, we have even a
blog post about these different architectures, and there are multiple methods that are related.
So, one of them is, for example, you can do dropout on your neighbors. That was done by a
worker from the group of Roger Battenhofer at ETH, and they actually showed that this has
similar effect. So, it increases the expressive power, not only gives some kind of robustness to
the architecture. So, the last more expressive type of message passing in your network, so I
would like to mention is what we can generally call topological message passing. And if you think of
what is a graph, essentially, it's a set where you glue pairs of nodes together, right? So, every
element in a set writer, every node in the graph is a zero-dimensional topological object,
right? So, you can define this one-dimensional object, the edges that you glue to the nodes,
right? And you get the graph, but you don't need to stop here. You can also
define cells of higher dimension, right? That you forgot about, glue to cycles in your graph,
right? And we get what is called a cellar or CWL complex, right? And basically, now, instead of
traditional message passing in graph neural networks, where we exchange information between
nodes along the edges, we can also go up and down in this hierarchy. So, we can do message passing
within the same dimension, right, of the cellar complex, but we can also go across dimensions.
And this hierarchical message passing is strictly more powerful than the vice-versa-lemon,
and it's obviously very convenient for molecules, because in molecules, these structures have
some chemical meaning, and this probably is closer to how chemist thinks of molecule, because,
of course, the graph captures all the information, but it doesn't make certain structures explicit.
And in graph neural networks, for example, well, first of all, you cannot even detect by message
passing the presence of these structures. And if you want to transfer information from this node
to this node, you will need to do a few steps of message passing. Here, we can do it at once.
So, it also gives computational advantages. And again, if you want some more details about
how these different methods are related to each other, so there are many more
expressive architectures, so there is a tutorial that was given at the log conference,
and recorded on YouTube by my PhD student, Publizio Frasca, with Beatrizio Bevilacqua
and Gagai Maron. So, it's actually a very nice tutorial, and they go into much more details about
all these and other different methods for expressive graph neural networks. Any questions so far?
I have a question about summation aggregation being the maximally expressive aggregation.
Also, probably it is related. Maybe if you can also comment on the, what was it, discrete,
countable restriction on the features. Because if I imagine that our features are integers,
there are different combinations of integers that sum up to the same number. So, summing them
actually does lose the information. So, countable doesn't necessarily mean integers,
but they should not be continuous. So, why this is, without going into too much details,
basically they use the same proof technique that was used in deep nets to prove the
universality there. So, this assumption is important. If you remove this assumption that
this proof doesn't work. So, basically they apply locally kind of the result of deep net
that works on sets. All right. Thanks.
Hello. Very interesting. Thank you. I'm wondering about chemistry, whether you can encode in the
features of your graphs, also geometric information. Especially in chemistry, aromaticity is very
important. Whether it's possible to encode it directly or you need some additional layers of
information. So, some information you can probably compute. And of course, if you can
pre-comput it, if you know that these are meaningful features, then I think it makes
sense to encode them. So, the geometric information, I'm not sure that you mean
information that comes from the positions of the atoms, right? Yeah. So, I will talk about it
in a second. So, you can, basically when you deal with geometric information, you also need to do
it in a proper way. So, you need to do it in a way that is equivariant to possible transformations.
But I think the short answer is yes. In case of these, like, increasing hierarchy of the
KWL tests, is it a case that for a given KWL that we know there exists some message passing
graph neural network that exists that can do it, but we just can't construct them? Or can we say
that if we could make such a message passing GNN for such KWL, it would be, you know, intractably
huge and we would need to approximate it or something like that? So, first of all, it is
intractably huge. So, it complexes N to the power K. So, I think what is limited in practice is
3WL. So, this is what Maron describes in his paper. I don't know, depending whether you can call it
message passing or not, it depends on what you consider message passing, right? So, because
here you have more than pairs of nodes, I would argue that strictly speaking, it's not message
passing, but for example, Petr Wieliszko, which would say that it's message passing on a different
graph, right? So, it depends on the perspective. Maybe one more follow-up question for this KWL
stuff. If you have a particular application that you are interested about, are there some cases
where you can say, for this class of problems that we're working on, we can, it's enough to be
able to distinguish up such a level because the higher you go for K, obviously, like these edge cases
would get really nasty, which you might not see in your application. Well, so, it's a good question,
right? So, for example, planar graphs have WL dimension of 3. So, basically, all planar
graphs can be distinguished by 3WL test. And you can argue that molecules, right? Most of the
molecules, you can draw two-dimensional structures, maybe some of them you don't, but they're
probably a tiny fraction. So, do you need something more powerful than that? The expressive power
itself is probably not the end of the story, right? Because nothing tells you about generalization.
So, yeah, I don't think that this on its own is really the crucial consideration. It's good,
of course, to have an architecture that is, that allows to distinguish between broader class of
graphs, but if it comes at the expense of computational complexity, for example, maybe
it's a bad idea. So, you probably want some kind of good trade-off between these. Thank you.
I had a follow-up question. So, could you please define what message-passing is in this case?
Can we give a short definition? Right. So, message-passing is what I call message-passing is
these kind of architectures. Let's see where I had it. Yeah. So, basically, architecture of this
kind. So, this is the most general type of message-passing. So, we have the update of node i
from neighbor j is done in this way. So, I have a function that depends on both i and j,
that the function is parametric. So, that's learnable and then aggregate. You can actually
show that summation is what you need, right? You don't need anything else.
So, well, this is, so this is message-passing. There are higher order architectures, right,
like KGNN, right, equivalent to KWO. So, this is equivalent, in the best case, equivalent to WO.
It is not equivalent. So, the more expressive WO tests, KWO tests
are more expressive than these architecture, but then it doesn't work on pairs of nodes. It
considers bigger sets of nodes, right? So, whether you, to call it message-passing or not, you can
argue that, so some people argue that you can also think of it as message-passing, right,
just with a different graph. In my opinion, it's more a semantic question.
So, I don't know. We never really looked at it. Yeah, I don't have an answer.
So, it seems like the topological message-passing
networks and the graph substructure networks both work on the same phenomenon of identifying
and counting substructures. So, is the expressivity of the topological message-passing lower bounded
by the expressivity of the graph substructure network? No, it's slightly different, right,
because substructure networks, well, first of all, we know they're upper bound, so 3WO. Topological
message-passing depends on what kind of substructure. So, there, the kind of side information
that you assume is what kind of substructures become cells in this, in this cellular complex.
You can actually go beyond two-dimensional cells. You can go to high-dimensional cells.
We never tried to go beyond two-dimensional cells. So, depending on the choice of these
substructures, what becomes a cell, you might have different levels of expressivity. So,
they're distinct methods. I don't think that they're really comparable.
So, something that always confused me about the topological ones is where you, like, you glue
stuff to structures in the graphs to, like, make them obvious. But can you, like, glue to every
structure or does the structure need to be, like, closed? Because in previous works, it was always
either cycles or cliques, which are kind of circularly closed.
What, I think, basically, the key question is whether it defines a valid cellular complex. I
think it should be closed. Yeah, my top, yeah, I also think it should be closed, but I have no idea
about topology. Yeah, from what I remember, it should be closed. But, yeah, so it could be a
clique, for example, whether it could be a path. I don't think so. So, actually, GSNs can, like,
count structures that you couldn't, like, glue into, or, like, form into a cellular complex.
So, GSNs can count more general structures, in my opinion. Yeah, so something that doesn't necessarily
form a cellular complex. Okay, yes, thank you. Inge, I have a question. If we allow more and more
notes, the probability of collision in this representation, do we have any results that
it became less or, like, insignificant or, like, in more general, do this approach extend a bit
more into randomized or, like, stochastic versions so that maybe the guarantee is very low right now
from a sort of, well, secondary run. So, the expressiveness, like, theoretical is pretty low,
but then, if we allow a little randomness, then, actually, like, no randomness.
Sorry, you're talking about random graph models, right? Something like stochastic walk models.
Yeah. You can probably analyze what happens to this kind of graphs given
certain type of message passing architecture. I have seen papers of this kind, nothing specific
that comes to my mind. Yeah, you can probably, I don't know, compute some probability under the
assumption of certain distribution of input random graphs of distinguishing within them or not.
Yeah.
So, a question regarding the size of the graph in practice. We are talking about, like, in terms of
maybe not count and each count, like, what to say in practice for this KWL?
So, KWL doesn't scale well, right? So, if it's n to the power k, so also computational
complexity versus space complexity, I don't think that you can, in practice, go beyond
3WL equivalent. Now, there are sparse versions of these architectures, so you can do slightly
better, but I think they are mostly useful for proving theorems. So, basically, because you establish
a link to the device for element hierarchy, then you can say that basically, if your network is
equivalent to one of these methods, then you know how expressive it is. Thank you.
Yeah. So, there was a paper sometime ago claiming that most of the graphs in the most common benchmarks
can actually be distinguished by 1WL. Now, I don't know if you read about it. And my question is,
now we've seen that even in those cases, more expressive GNNs still get better results. But
then what's the reason? So, again, generalization could be one possibility, right, because you
train on one set of graphs and then you test on another set of graphs. Then,
double your tests, in general, they're designed for fish less graphs, right? So,
they only consider the structure. So, how you treat fish is also important.
Nice sense. I think we can stop here and we have, like,
have an hour for a coffee break and then we resume afterwards.
So, I will put the slide with the caffeine molecule that everybody likes. And then,
yeah, basically, we stopped at this overview of different more expressive architectures. So,
let's now talk about, guys, let's start. So, basically, the situation that we have with
the expressive power of graph neural networks, right? So, on the one side, we have the WL hierarchy,
right? So, increasingly, more powerful, more expressive graph isomorphism tests, right? And
we can find analogies between certain GNN architectures to these tests. On the other hand,
the assumption that we are given a graph and we do message passing on this graph might be
restrictive in the sense that some graphs are not friendly for message passing. And in this case,
you typically, what you would like to do is to change the graph so that message passing works
better. This is a very broad category of methods that are called graph rewiring. So, what happens
is that you have a gap between theory and practice, right? So, the theory, in order to make the link
to the WL tests, you need to use exactly the same input graph. The practice tells you that
sometimes you don't want to do it. So, as always, there is this gap. So, if you think maybe take
a step back and look at the different types of graph neural network architectures, so the traditional
approach in graph neural networks is you're given the input graph and it's both part of the input
and part of the computation, right? Because you use the input graph to send information on it,
right? So, it's both input and computational object. Now, as we've seen, you can do many
different things, right? So, you can enrich the graph with some positional or structural features.
You can lift it into a high-dimensional topological space like Simplisher or Cellar complex, right?
And do message passing on this object. You can again enrich the graph by considering a collection
of subgraphs, right? And do maybe some other more exotic type of aggregation that respects the
product symmetry group. You can also enrich your representation by considering also the
symmetry of the data, right? So, these are equivalent GNNs, if you have time, I can talk about it.
And then maybe in some special cases, your graph will have special structure like a grid, right?
And in this case, you can maybe do more specific choices. For example, you can abandon the local
permutation invariance. So, you will get back to convolutions. So, basically, the common denominator
of these approaches is that you have more structure, right? Sometimes you can assume,
sometimes you can invent, right? But that's the idea. On the other hand, you can say that you
don't like the graph that is given to you, right? And you choose to completely ignore it, right?
So, you just assume empty edge sets. So, you're back to the bare bone, right? So, the object is
just a collection of nodes, which is a set. And these are the architectures that are called deep
sets or point nets. So, that's the simplest case of graphing electrics, right? Where you don't
have a graph, you just have the nodes. The other extreme of this is when you allow interaction
between every pair of nodes, right? Again, you don't trust your graph for some reason. So,
every pair of nodes can interact. So, it's a complete or a fully connected graph. And this
is how transformers work, right? So, in this case, you will use, for example, the attentional
flavor of the graph in electrical architectures. And you will learn the right graph for your task,
right? In a sense, through the attention mechanism, for example. There is another class of methods,
and this is what we call graph rewiring, where you say, okay, I don't like the graph that is given.
I would like to change it a little bit, maybe. And this way, the graph will become, in some sense,
better for message passing. Okay, we'll talk about it in more details. So, let's talk about
graph rewiring and specifically about transformers. So, in transformers, you assume that the graph is
complete, right? So, every node is connected to every node. And if you try to apply a convolutional
style GNN architecture, right, that depends on the coefficients of the, representing the structure
of the graph, then, basically, here, the sum goes over all the nodes, and basically, this argument
is equal for every node. So, it's not informative, right? You're not adding any information.
So, you need to use at least an attentional architecture. And in this case, this is already,
this already looks like a transformer. And you can think of attention weights as a kind of
learned graph adjacency, right? That depends on this task that you're trying to solve. And this is
a special case of GNNs. Now, of course, in natural language processing, many tasks that you want to
solve do not require permutation invariance. You actually want them to be not permutation
invariance. For example, you want to depend on the order of words in the sentence. So, this is
typically achieved by adding positional encoding. So, in the simplest case, you just equip every
node with some additional coordinates that tell you where you are located in the domain, right?
Which in case of transformers, it's where you're located in a sequence. And typically,
this is how positional encoding looks like, right? So, these are just some synosoids.
So, for graphs, you can do other things, right? So, the analogy of synosoids would be the,
yeah, question? Sir, are you aware of some studies about positional encoding, especially
about relative positional encoding, which is getting some popularity in transformers architecture? So,
like, the studies which consider all invariances in those positional encoding,
like, we not always want to have absolute positional encoding. Sometimes we want to be,
sometimes some shifts or rotations are not relevant. So, do you know some literature on that,
perhaps? Yeah, so, relative positional encoding, yeah, good question. So, the analogy, let's say,
of this standard global positional encoding would be the plus and eigenvectors. So, the analogy of
local positional encoding would be something that, for example, the DGN architecture implemented,
Gabriele Corso and others directed graph networks. So, what they do, they take, for example, the
same plus and eigenvectors and compute their gradients on the edges and then transform these
features in some way. So, this gives you a kind of local direction. So, that would be probably
a good analogy of local positional encoding. So, with the plus and eigenvectors as well,
you have some ambiguities. So, there has been actually a recent paper from Derek Lim and others
from MIT where they are able to solve these ambiguities. You can use random walk kernels,
you can use substructure counting, as we've seen, right? So, there are many ways of doing it.
But basically, between these two extremes, right, whether ignoring the graph or learning the
graph, of course, it comes at the expense of complexity, which is a huge problem in large
language models where the size of the domain, right, the length of the text can be very large.
So, probably one of the key computational questions would be how to compute these
attention more efficiently. So, here somewhere in between comes the graph rewiring approaches.
And with graph rewiring, it means that your computational graph is not equal to the input
graph. And it's a little bit controversial topic because the graph being part of the input, somehow
you don't want to change the input, right? So, but the fact that many architectures do it, right?
So, if you have, for example, a very large graph like a social network where you have a lot of
neighbors in some nodes, you cannot aggregate information from millions of nodes. So, you need
to sample the neighbors, right? Which means that you are using a different graph
from the input one. So, neighborhood sampling is a form of graph rewiring, right? You can also use
graph neural networks where you bring information from not immediately your one-hop neighbors,
but also from multiple hops away, right? So, this is also some form of graph rewiring.
Transformers are also an extreme form of graph rewiring, right, where you allow access to all
the nodes in the graph. Some kind of pre-processing, right, where you pre-wire the graph for example
by some form of diffusion. Yeah, a question?
So, how do you do the neighbor sampling? Is it just like a stochastic?
Or do you... Yeah, usually you sample with repetition. So, this is what Sage did.
Do you think it's optimal or is there a way to optimize it in the best way possible?
I don't remember if they looked into it. Probably, of course, it might matter how you sample, but
I don't have on top of my mind any significant result that would tell how to do it better.
Could you elaborate more on this diffusion processes on the graphs you mentioned?
Yeah, I will get to it. So, I will go through the diffusion equations and then I will talk about
this as well. So, I'm specifically referring to Degel, the work of Stefan Gundem and his students.
So, basically, bottom line is graph is not really any sacrosanct object and in practice,
many architectures even without admitting it do some form of graph rewiring, right? So,
you can also rewire the graph throughout the neural network, so it doesn't need to stay the same
across all layers. And this has been shown efficiently, for example, in the context of
our squashing where you can do maybe a few steps of standard message passing and then
do message passing on a complete graph, right? So, like, what transformers do?
And so, the argumentation usually is that the first steps capture somehow the structure
of the graph, similar to Weisberg and Lemon, and then, basically, you accumulate this information
as features. And there is an extreme example of this. I think it's called GRIT. So, this is
from the group of field torrid Oxford and what they do is they use just something similar to
heat kernel to encode the local structure of the graph and then just use MLP.
So, there are other extremes like this, right? So, basically, you have just some kind of local
feature of the graph that you can maybe make learnable, but then the graph itself is not used,
so there is no message passing. And I think it's an open question of how much you want
to use to capture the structure in the form of some features versus in the form of the
computational architecture. So, the graph can be changed throughout the layers and, well,
one of the first architectures to do it was what we did with Justin Solomon and his students,
and that was considering problems in computer graphics. So, we have a point cloud, let's say,
of this airplane. So, every point has three-dimensional Euclidean coordinates,
and here the task is segmentation. So, you want to label each of the points on the airplane,
whether they belong to the body, to the engines, to the wings, and so on. And here we create a
graph to basically to represent the local structure of the data. And what is shown here by the colors
is a distance in the feature space, right, in the latent, in the latent feature space of
respective layers in this network to the rest of the points from the red point, right? So, initially,
this is Euclidean, as you can see that, that's basically the input space, three-dimensional
R3, but then as you go deeper, you see that it becomes more semantic. So, here, for example,
points on the same, on the engine become closer, or on the wing become closer. So, basically,
the space itself is used for the construction of the graph, and we call it dynamic graph neural
networks, or maybe not very, very, very lucky name. I think many architectures are called
like this. And we use it also in different incarnations or similar ideas under the name
of differentiable graph module, where basically you have applications where you don't know the
graph a priori. So, that's, for example, the case with medical imaging, where you have maybe
nodes representing different patients, or maybe different regions in the brain, and you have
basically two branches of graph neural network architecture, one that is computing the filters
on the graph, and another one computing the graph itself. And, of course, there is question of
computational complexity, but we're able to show that it is better to learn the graph for the,
for the task rather than to construct it ad hoc in some kind of handcrafted way.
Now, talking about message passing in general, so these are hecticly added slides based on some
discussions that we had in the coffee break. So, if you think of message passing and different
versions of it like transformers, basically, the difference is in two questions, is what's
information to send, what's information to pass, and where to pass, whether you follow the graph,
whether you pass information to your neighbors, or you pass information to distant nodes, whether
it's in K-Hope or to all the neighbors in the graph, and the question of what is how you exactly
transform your information. So, it's also interesting to add to this another question,
is when to send information? And if you look at it, basically, it's a multi-step process. So,
this is how a classical message passing neural network works. So, these are three layers of an
NPNN or three iterations of, let's say, something similar to vice-fair and lemon, and I'm sending
information from these nodes to these nodes, right? So, it takes three steps. So, first here,
then these nodes propagate information to their neighbors, and then this green node receives
information from both neighbors, right? So, in a transformer, all the information is available
at once. So, at the same moment of time, I'm sending information from all the nodes to the
green nodes, right? So, it has access to all the information across three layers. But you can also,
so, basically, here, the difference is where to send information, right? That's the structure of
the computational graph. But we don't consider here also when to send this information. And you
can imagine an architecture where, for example, you delay the information. So, here, the first node
in first iteration sends information to its neighbor. At the second layer, it sends to
two hop neighbors, right? So, this information becomes gradually available. Not like in a transformer
where you flood all the nodes with information from all other nodes at once. You make it
progressive, right? So, basically, these are kind of shortcuts that you have in the graph,
but they are made progressively as you go deeper into the architecture. You can also
make, so, you can think of this as a kind of skip connections, but you can also make skip connections
sparse in time. So, you can delay this information. And I'm saying that here, the information
from the first node comes to the next node and is delayed in time, right? So, it would arrive to
it anyway, but it would be entangled with the information in other nodes. So, I'm allowing
direct access, but I'm delaying it. And this potentially has interesting implications also
on the hardware aspect of graph neural networks, because if your graph is very large, you typically
partition it into different parts of memory, right? And the cost of message passing is not
the same, right? So, messages within the same memory cost much less than messages across
different memories. So, you can imagine a graph neural network or you are doing fast messages
within each part of memory hierarchy and slow messages across, right? So, you don't want to
wait for all the messages to arrive. You might want to do fast messages while waiting for slow
messages, right? So, this architecture that we call drill or end drill, this version with delays
potentially allows for it. So, I think it would be interesting to test it on actually some
practical hardware like graph core, for example. So, let me move to the next topic and this is
physics-inspired graph neural network. So, I promised PDEs, so I need to hold this promise. And
let's again take a step back and look at different objects that we've seen so far, right? And also,
you can argue objects that are studied in this broader field of geometric deep learning. So,
let's say grids, meshes and graphs, right? So, you can think of them as more or less the same thing,
just with more structure, right? So, meshes in addition to have also triangles. So, these are
simplicial complexes. So, graphs with some extra constraints or extra structure. Grids are also
special type of graphs where we have certain organization. So, if you look at the grid and
you look at how you aggregate information from your neighbors, there is no ambiguity, right? In a
grid, unlike a general graph, I can order my neighbors in a canonical way, right? I have a top
neighbor, I have a left neighbor, bottom and right, like shown here. So, it is totally unambiguous,
right? So, this ordering is fixed. On a mesh, the situation is slightly different. So, I can pick up
my first neighbor and then I can order all the rest of the nodes, all the rest of the neighbors
in, for example, clockwise orientation. And this is possible because mesh is a discrete manifold.
So, it's locally Euclidean. I have this meaningful ordering, right? But, of course, the choice of
the first neighbor is ambiguous. So, I can rotate everything, right? So, the ambiguity here is up
to rotation. In a graph, as we've seen, any permutation works, right? So, everything is defined
up to a permutation. So, in a sense, graphs have the least structure out of all these objects,
right? So, second observation is that if I look at grids and meshes, I can think of them as
discretizations of some continuous spaces. So, a grid is discretization of a plane or a mesh is
discretization of a two-dimensional surface or a manifold. We don't have immediately this analogy
for graphs and even though there is an entire field that is called network geometry that tries to
think of graphs as some discretization of continuous space, it would be nice to have
some continuous models for GNNs, right? And that's the idea of what we call physics-inspired GNNs.
So, you can consider some class of graph neural networks as a dynamic system. So, you have
particles that live in the dimensional feature space. And let's assume that the dimensionality
always is kept the same, right? It doesn't need to be in general graph neural networks. So, every node
is some point, some particle that moves along a trajectory that is shown by this red line here.
So, a GNN is essentially a dynamic system, right? That is governed by the system of differential
equations. F here is some coupling functions, right? That makes dependency between different
particles. And it depends both on the particles and the graph. And it's also parameterized by some
trajectory of parameters that are, you know, by data. So, T is a continuous time parameter.
I can discretize it and this corresponds to layers of a graph neural network. And the graph,
right, you can think of it either as a coupling function, right? So, this F here, how different
rows of this matrix interact with each other or discretization of some continuous space,
as we'll see in the next few examples. So, basically as the evolution equation,
right, that governs the behavior of these particles, I can put here more or less anything,
right? So, there's plenty of different differential equations that describe different systems. But
probably the first thing that comes to your mind when you think of propagation or diffusion of
some stuff is the diffusion equation, right? And this, in fact, was one of the earliest
mathematically analyzed physical phenomena, right? The diffusion of heat, analyzed by
non-Elst and Newton himself in an anonymous paper written in Leighton in the transactions
of the royal site. So, it's interesting actually that the journal had a mixture of papers written
in English and Leighton. And it was anonymous. Well, Newton devised an experimental setup where he
heated pieces of different objects, metal, I think, mostly, and then he looked with his
self-made thermometer here, measured how much heat is lost over a period of time and devised some
law that is formulated in modern terminology like this. What is nowadays called the Newton
law of cooling, which says that the temperature that the hot body loses in a given time is proportional
to the temperature difference between the object and the environment. He actually didn't use the
term temperature. That's modern terminology. He used the term heat, which has a different meaning
or color in Leighton. And somehow, everybody guessed that it was his paper, even though he
didn't sign it. Now, it took some time until this process was fully understood. So, Fourier
devised the local version differential equation that governs heat diffusion and then thick
in the late 19th century defined what we nowadays understand as diffusion equations.
So, if you want to apply this idea to a graph, we can consider a diffusion process on a graph.
And that's how it looks like. So, here x is some quantity that is being diffused. Think of it as
temperature if it's color. So, a node has certain temperature at time t. So, t is a continuous
variable. And what is written here is the self-temperature of the node. This is the temperature
of the environment. So, it's the average of the one hop neighborhood of the node. And this is the
rate of temperature change. So, there might be some proportion coefficient here. So, if I rearrange
the terms on this expression, I can write it like this. So, I'm slightly massaging the formula.
What is written here is called the gradient. So, it's just the difference between end points
of an inch. So, I take the feature here and the feature here and subtract one from another. So,
that's the graph analogy of the standard gradient operator from classical calculus.
And what is written here is, again, the discrete analogy of the divergence operator,
again, from basic course in calculus. So, in terms of operators, you can think of
the features as that they live in the nodes of the graph as a scalar field. Then the gradient
makes a scalar field into a vector field, right, that lives on the edges of the graph. And then
the divergence does the opposite. So, it collects the information from the edges that live, that
emanate from a node and basically sums them up maybe with some different weights, right? So,
that's what the divergence, right? So, gradient makes a scalar field into vector fields. Divergence
makes vector fields into scalar fields. Acting together, they take a scalar field into a
scalar field. The divergence of the gradient or minus divergence of the gradient in our notation
is what is called the Laplacian operator, right? So, what is written here is the
graph Laplacian operator and by definition, you see what it does. So, it compares you to your
neighborhood, how different you are from the average of your neighbors. And this is really a
very important operator. It comes everywhere in mathematical physics and from quantum mechanics
to wave equations to diffusion equations, and particularly important here. So, what else is
the heat equation? So, this is very simple, right? So, this is what we call homogeneous
isotropic diffusion equation. Basically, heat propagates everywhere in the same way, right?
What else heat equation is? It's an example of prototypical gradient flow. And gradient flow
is a type of evolution equation, differential equation that looks like this. So, the step at
every point of time is in the direction of the minus gradient of some energy, that you know here by
E. And the energy that corresponds to the diffusion equation is what we call the Dirichlet energy.
So, the Dirichlet energy, you can write it as a quadratic form with respect to the Laplacian
operator. And it measures the smoothness of the node features, right? How different you are from
your neighbors. So, the smallest value it can achieve is zero. In this case, all the features are the
same, right? And you can show that Dirichlet energy decreases along the flow. So, if you run the
diffusion to infinity, all will become constant, right? So, the heat will basically propagate
on the domain and everything will have the same temperature. So, this is what we typically call
over smoothing in graph neural networks, right? And I should say that over smoothing is not
necessarily a bad phenomenon. So, usually it's described as a kind of catastrophe that happens
in GNNs and prevents us from making deeper architectures. But it can actually be a very
benign thing, right? So, you can imagine without any learning. So, if I give you a graph and I
have labels of a few nodes, and if I assume that the structure of the graph is homophilic in the
sense that my neighbors are expected to have labels similar to me, then I can just diffuse this
information so it will be, you can think of it as a heat diffusion equation with boundary conditions.
And by just doing this, it is very likely that the result will be very good, right? If the graph
is not homophilic, of course, you need to do something else, maybe work harder, but on its own,
the over smoothing is not necessarily bad. Yeah, there is a question.
Actually, I have like two questions on this topic. So, first of all, can I see this as the,
in some sense, the message passing algorithm there, but via the ordinary differential equation between
the edges? Right. So, well, whether they call it message passing or not, I think it's a good
question. So, every iteration when you discretize this differential equation, every step will
correspond to message passing. Now, in some cases, you can actually have a closed form expression
for the solution at any time t, right? This is called the heat kernel. So, it will look like
the exponential of the Laplace matrix. So, I can compute the value of the temperature at every
point instantaneously without doing these microsteps of message passing. So, whether they call,
still call it message passing or not. So, this is where semantically I disagree with better,
for example. He still suggests to call it message passing. I think it's not. So, I don't know how
to call it better, maybe some kind of spatial coupling, right? Because you have, you have direct
information to potentially infinite, all the nodes in the graph, right? So, it is something that
doesn't require propagating information explicitly, right? You might be, when you solve the
diffusion equation, especially with its own linear, you might not have a choice. But in some cases,
you do. You have closed form expressions, right? Okay, thank you. And the second question, because
I suppose that here is like the mostly ordinary differential equations. Is there any research
to introduce stochasticity there to be, let's say stochastic differential equations, something
like that, and when it could help in the, in the, in this field? Yeah. So, it's interesting. I think
there have been some works. And there are some works. So, it talks for example, a big
expert in this domain is Terry Lyons. So, I think they are working on, on these topics.
So, we are not considering stochastic differential equations. We are considering
whether to call them ODs or PDs. So, these are coupled ODs, right? When you discretize,
a partial differential equation becomes a system of coupled ordinary differential equations. I think
they, again here, the terminology is more semantic difference, whether you have a continuous spatial
coordinate that you want to discretize. So, that will be the next part of it. Okay. Thank you.
Right. Okay. So, that was basically, that was the gradient flow, right? And again,
an example of, of the heat diffusion equation as, as a prototype of the gradient flow. So,
if you look again at the, at this view on, on graph neural networks as some kind of
evolution equations governing some, some physical system. So, the traditional approach
to graph neural networks is to take this differential equation, discretize it, and then
parameterize the evolution equation, right? The discrete evolution equation. So,
every step of an iterative solver here will correspond to a layer, and then you parameterize
every layer, right? So, that's how you can view graph neural networks. So, instead, what we can do,
we can start with, with an energy, parameterize an energy, and then derive the evolution equation
as a gradient flow. So, apparently, there is no difference, right? But the big difference would
be that we will, it will allow us to make certain architectural choices. So, it will
restrict our space of all the possible architectures that we can do here, right? Like, for example,
the form of message passing. That will have better interpretability. And of course, I know that
interpretability is maybe a bad word in machine learning. So, what I mean here is that we will
be able to guarantee that certain things happen or not happen. Okay. And I will be more specific
in a second. So, let's consider the following parametric energy. So, we call it generalized
Dirichlet energy. It has these two terms. So, you can think of it as an energy that a system of
particles has, and it's parameterized by two matrices of size d by d, right? d, remind you,
that's the dimensionality of the features. So, that's the space where the particles move.
And we have two terms here. So, the external energy term, it acts on all the particles. So,
think of some force that moves them in some directions. And we have internal energy. So,
these are the interactions between particles along the edges of the graph, right? Think of maybe
colonic interactions, right? Or maybe springs that attached to some pairs of particles. And we
have two types of interactions. So, we have attractive interactions, and they happen along the
eigenvectors of the matrix W that corresponds to positive eigenvalues and repulsive interactions
that happen along negative eigenvectors. And you can see an example here. So, here the space is
two-dimensional just for visualization purposes. And the graph here, you see that it's heterophilic.
So, the colors of the nodes represent the labels. And the positions represent the features, right?
The feature coordinates x and y. So, the graph is actually perfectly heterophilic, right? The
blue nodes have only red neighbors, and the red nodes have only blue neighbors. And yet,
we can find directions. So, the horizontal direction is repulsive direction, where the
particles are separated. And the vertical direction is attractive direction, where the particles
cluster together. We can perfectly separate these types of nodes, right? So, if my task is
node classification on this graph, by this very simple process, I can solve this problem, right?
So, we will see in a second that this corresponds to a convolutional architecture. By convolutional,
I mean that my diffusion operator depends only on the structure of the graph, but not on the features,
like in more complicated, for example, attentional networks. So, I can write it as AX something,
right? So, this is what was our distinction. So, in convolutional architectures, we had something
like this. In attentional architectures, we had something like this, right? So, that would be
the GCN type of architectures, and this will be the GUT type of architectures. So, the fact that
this matrix is constant, it depends only on the structure of the graph, is what I call convolutional
architectures, right? So, basically, these goals against this folklore that I mentioned in the
beginning, that convolutional architectures are not good for heterophilic graphs. So, you can see
that they can work very well, right? So, we need to dive deeper and understand what happens here.
So, if we write the gradient flow of, yeah, question.
Is there a link between this gradient flow and less and less popular recently,
probabilistic graphical method, especially if you showed the previous slide, it was very
similar to restricted Boltzmann, right, when we had this bipartite graph?
Yeah, you can probably interpret it from, right? So, gradient flows are very basic objects. So,
it's essentially steepest descent is also a gradient flow, right? So, it's a continuous
version or a variational version of it. Yeah, so, probably there are links to many different things.
So, basically, you're minimizing some energy here, right? So, and this energy,
so, it's not the learning part, right? So, it's the inference. So, applying a neural network
minimizes some energy, right? You design the network, so, it minimizes some energy, right?
And the energy is parametric. So, what kind of energy to minimize is what you determine
based on the task. So, that's what is done by back propagation. So, basically, the gradient flow,
you just differentiate this energy with respect to its parameters, right? By data I denote here,
these two matrices omega and w. And one thing that you notice immediately that they all appear
in symmetrized form, right? So, they appear in quadratic terms. So, they appear as omega plus
omega transpose. So, we can just assume that they are symmetric to start with, right? So,
that's already first restriction that comes from this assumption of the gradient flow.
The second thing is, again, this is by the design of the energy that
the parameters are time-independent, right? So, they don't depend on t, right? Which will become
the layer number. And if we discretize it, this is how we discretize. So, we replace the temporal
derivative with forward difference. So, this is what is also called the explicit Euler scheme.
So, we have some step-sized tau. And this gives residual convolutional type GCN, again,
the convolutional type GNN. Why I call it convolutional? Because this matrix A, right,
that's where the diffusion happens, where the message passing happens, right? When I send
information across adjacent nodes, it's not dependent on x. It's fixed, right? So, you can call
it a kind of a convolution. The weights are symmetric and the weights are shared across
different layers, okay? So, the way that you can use it, you can optionally do some nonlinear
encoding typically to reduce the dimensionality. So, we have some fixed low dimensional dimension
of this space. You apply a linear gradient flow. So, all the propagation of information on the
graph is linear. So, there are no nonlinear activations, right? And then you do some decoder.
That's for node classification, right? So, that's how the architecture looks like.
Now, if you compare it to the classical convolutional architectures like GCN of
Kieff and Welling, so there are several differences. So, the GCN is non-residual,
the weights are non-symmetric, and the weights are different per layer, right? And also,
they use nonlinear activation for every layer. We don't. So, in a sense, it's a kind of antithesis
to a typical graph neural network or a typical deep learning architecture. So, typically,
you say that you need many layers, each layer parameterized separately and nonlinear are they
activated. We don't have anything like this here, right? So, all the, again, the diffusion part is
linear. We have a nonlinear decoder and potentially a nonlinear encoder. So, what we gain from the
it being gradient flow is interpretability in the following sense that we can show that
in certain situations we can induce both low and high frequency dominated dynamics.
I will define it in a second. And what it means is that we can work both with homophilic and
heterophilic graphs. Now, because the weights are shared across layers, actually the number of layers
becomes a completely irrelevant notion here. So, what matters is really the diffusion time.
The number of layers is just how finely I discretize my differential equation.
And the number of parameters is independent of it, right?
Unlike, again, the classical architecture where the more layers you have, the more parameters
you have. Yeah, question? Question. Why don't you stay in the latent space? Why do you have
needed a decoder? Why do I need a decoder? Well, the decoder can, for example, you might need it to
produce a label for an old. It's not the decoder in the original space. It could be in any,
it could be also decoding the original space, right? So, you might, for example,
want to work with low dimensional space and then the number of your classes might be different,
right? But the important part is that it's nonlinear. So, all the nonlinearity goes there.
Okay. And, well, you can also use an encoder. I will show why encoder can be problematic.
We actually, we have experimental evidence that it's not needed.
So, first of all, let's talk about homophilic and heterophilic graphs. So, again, homophilic
look like this, heterophilic look like this. And the data set here is what we call synthetic
core. So, it's probably you're all familiar with it. So, it's this citation network where we can
actually change the structure of the graph in a way that it becomes either more homophilic or more
heterophilic, right? And here are the two extreme choices of an architecture. So, the task here
is no classification. I can either ignore the structure of the graph altogether and do just
no wise predictions with a multilayer perceptron. And, of course, it's not great, right? So, it
achieves about 67% accuracy. But no matter how homophilic or heterophilic data set is,
the result is the same because it simply ignores the graph, right? The other extreme is a GCN.
So, when the graph is homophilic, it works extremely well, right? Almost 100% because
when my neighbors contain similar information, I will be basically averaging them and I will
be doing some form of denoising. But when the graph is heterophilic, then it degrades very
badly because, in this case, the neighbor information is more detrimental than helpful.
So, the gradient flow framework, it benefits from basically its kind of mixture of both worlds. So,
it works as well as GCN in the homophilic case and as well as node-wise predictions in the
heterophilic case and the graceful transitions between the two. Now, another important thing,
right? And that's where the encoder and decoder question comes into the play. So, if you write
the output of the neural network after L layers, this is how it looks like. So, it's basically
it's a polynomial in these matrices. So, the matrices are dependent, of course. So, there are
some powers here. The parameters are shared. But if we ignore the encoder, right, and we can ignore
it, at least in the test that we did, basically what the diffusion part can be precomputed,
right? You see that it acts as some kind of powers of the diffusion matrix on the features. So,
I can pre-diffuse the features once as a pre-computation step and then it all boils down to node-wise
multiplications by these matrices. So, the computationally difficult part, both computation
in terms of the number of multiplication operations as well as the memory access, which,
unless the graph is very structured, you have random access to your neighbor nodes,
this is really the heaviest part of graph neural networks. So, this now can be totally trivialized.
So, we've done the earlier versions of this architecture with just one convolutional layer.
We call it sine. So, already several years ago, we tested this on the graphs of hundreds of millions
of nodes. So, with this architecture, actually, because now we also get rid of the non-linearity
you can apply it to a very large graph, basically, you can design
multi-layer architectures that work well both in homophilic and heterophilic settings.
Now, another thing, and this is what I mentioned regarding the nature of the dynamics that is
induced by this gradient flow, is if we look at our data, right, and our data, I remind you, it's
the matrix x, right, which is of size n by d and is the number of nodes, d is the number of dimensions.
So, we can do an analogy of a two-dimensional Fourier transform, right. I remind you that
for a graph that we assumed, an undirected graph, the graph of plus n has orthogonal
identity composition. So, it's eigenvectors form an orthogonal basis, right, the basis for the rows
of the matrix, and the matrix w, which we assumed by virtue of our process being a gradient flow,
right, it was symmetric, we can have also an orthogonal identity composition, let's call
it eigenvectors psi and eigenvalues mu, so it forms the orthogonal basis for the columns of
these matrix, right, for the dimension d of these matrix, and now in these two-dimensional Fourier
basis, right, we can take tensor products of these basis functions of phi and psi, we can write the
output of the neural network like this, right, and what you see here is that we have some filter
that acts on our signal, right, so this is signal, so these are tensor products, right, so that's
the analogy of two-dimensional Fourier transform, sinusoids of sine mx by sine ny, something like
this, right, so that's our analogy of this, so this is a filter, and it works both with the
frequencies, the eigenvalues of the graph Laplacian and the matrix w, right, lambda and mu, and you
see that the low frequencies of the graphs are magnified by the positive eigenvalues of w,
and conversely the high frequencies of the graph are magnified by the negative eigenvalues of w,
and you can show that if we choose the matrix w in such a way that it has sufficiently negative
eigenvalues, then this gradient flow dynamics is high frequency dominant in the sense that the
Dirichlet energy or more correctly the normalized Dirichlet energy doesn't converge to zero,
so it means that we don't have over smoothing, right, in the case of over smoothing this thing
would be going to zero, but here it doesn't, right, so it means that we have some process that
doesn't diffuse everything to a constant, it does something more interesting, right, and the
condition for it is w having sufficiently large negative eigenvalues, so the analogy of this would
be, so if you think of diffusion processes blurring, what we have here is sharpening,
and we have both processes at the same time, so the attractive interactions do blurring,
the repulsive interactions do sharpening, so we have some directions in the fissure space where
these processes happen at the same time, okay, questions? Yep, I think there are many questions,
so it was very unclear. So in these situations where you don't
dissipate to a constant value, are you obtaining some sort of chaotic behavior,
or are you obtaining other periodic oscillations of? So this is a sympathetic analysis, we don't
know, I don't think that I have an answer to your question, it might be that it's,
so whether we have monotonicity, that's the question, I don't think so, but it might be the case.
So if eigenvalues of w are sufficiently negative, then we can just kind of beat over squashing,
over smoothing, and the question is, is it possible, is it hard to do so because there are
two dynamics, is it hard to get those dynamics right, does it require a lot of hyperparameter
tuning or something like that? So it's a good question, so you can force, you can structure the
matrix or parameterize the matrix w in such a way that it has two parts, positive eigenvalues and
negative eigenvalues, and that's what we do, so basically we help the architecture to have both,
and then learning becomes easy. To learn it completely from scratch might be difficult,
and we see that GCNs which are in principle the same architectures without this restriction
often fail to do it. My question went the same direction, but after the previous answer I got
more confused. So those, the matrices which, what's it, the delta and the w, they are learned,
right, they are not hyperparameters. W, the elements of the matrix w are learned, right,
but the matrix w has constraints, right, so it's, for example, it's symmetric, right, so it has half
of the elements of a general matrix w, and then basically by virtue of this theorem what it suggests
is that we need to further restrict the structure of w, for example, to make it have negative
eigenvalues. So typically what we do, we decompose it into a positive symmetric part and a negative
symmetric part, and basically this way we guarantee that it has both positive and negative eigenvalues.
Okay, so what is the principle component that makes graph work on heterophilic graphs better
than GCNs, and can you somehow update GCNs in a way that they will also work on heterophilic graphs?
Yeah, so residual connection is a must. We actually show that without residual connection this
doesn't happen. Well, the nonlinear activation, I think it's more a complication for the analysis.
We don't know how to analyze this, basically with this nonlinearity you cannot regard it as a
diffusion equation. So, yeah, basically residual connection and then nonlinear eigenvalues.
If you, and remove the nonlinearity, whether, if you don't constrain w to have non-negative,
or if you don't help the architecture to learn w with negative eigenvalues, you might never be
able to learn it. So we've seen this happening as well. Okay, and what is the role of symmetric
matrices, symmetric matrices w? So, symmetric comes from the assumption of gradient flow.
So, anything that looks like a gradient flow for this kind of energy must be symmetric.
I see, and what does it mean in terms of operations on your graph, in terms of message passing?
So, this is not related to message passing because w is channel mixing matrix attacks on the fissures.
I was, I was wondering, what is the relationship? So, for example, you have this gradient flow
and say you want to learn some energy with respect to which you flow. What's the relationship between
learning the energy and learning a metric to which with you are computing the gradient? Because if
you change the metric, then the way in which you flow is also different, right? Is other equivalent
in the learning a metric equivalent to learning an energy? Yeah, well, you can think of it
indeed in this way, right? So, what is the interpretation, physical interpretation of
of this matrix w? So, if you look at the way that Dirichlet energy looks, right, it looks like,
right? So, it's something like this. So, do we use, yeah, so let's say continuous version of Dirichlet
energy. So, it will be the norm of the gradient of x at some coordinate u, let's call it. So,
u would be the index of the null, right, the continuous version, right? And this is, this is
the Dirichlet energy and let's write it on some domain omega, right? So, that would be our continuous
version of Dirichlet energy. Now, what is written here when omega in general is a manifold,
a remaining manifold. So, what is written here is the remaining metric of this kind, right?
I hope you can see it. So, basically it's inner product defined at the position u, right? And
the way that you can write it, so you can write it using remaining metric tensor, which is exactly
the w, right? So, this is something that scales the coordinates of x, doesn't need to be fixed,
by the way. This w in general can be position dependent, right, on the remaining manifold.
So, a more general construction would allow w to depend on the position, maybe not explicitly,
because that would be a huge number of parameters that scales with n, maybe it will be done some,
through some form of attention. So, w will, or maybe a positional encoding, right? So,
w will be a function of positional encoding of the nodes of the graph, right?
So, there are interesting analogies and potential directions of extending this, using
basically this is a harmonic energy of an embedding of some manifold, right? Thanks.
Yeah, sorry, this will be probably a little bit like far-fetched question,
but like in general, like those equations seem to be, seem to be like kind of similar to what you
often get when you analyze like the signal propagation or the type of initializations
in the neural networks, or like the dynamical isometry property. So, for example, the requirement for
residual connections also appears in there. And like, are you aware like whether there's like
any connections between, between, between this work and, yeah.
Potentially. So, the closest analogy is neural ODE's, right? Well, these are neural, we like to
call them neural PD's, but they're coupled ODE's, right? So, neural ODE's, each row of these magics
will be separate, independent. Here, we also have the extra complexities coupling, right?
So, like, would you say like this, there's something universal in all those?
I'm not sure what do I mean by universal.
Yeah, I'm not sure either, but like, it's basically like,
like, for example, like this residual rule appears very often in different types of.
So, residual rules rule here comes just from discretization. So, that's how you, you discretized
the temporal derivative. You could discretize it differently. So, you can use a backward scheme,
right? And then it would be implicit. So, you will need to solve a linear system to,
to get your next iteration. This actually has been done with diffusion equations. So,
there are advantages to it because these kind of discretizations are what's called
unconditional stable. But in terms of, well, universality may be not the right term, but
basically what you have here in practice is a kind of controlled differential equation,
right? So, the control is through the privateers W. So, they're time independent, but in principle,
you can think of time dependent trajectory. So, you have a controlled PDE that you discretize,
and then expressive power becomes a question of, can I reach a certain state of the system,
for example, in finite time by choosing the right trajectory? Or how far can I be
from, from that state, right? So, universal approximation means that in finite time I can
reach, I can be epsilon close to any state that they want. Generalization, for example, can be
probably formulated as some kind of perturbation, right? So, I change my initial conditions. I want
to see what happens to the system. And there are actually, there are some results, theoretical
results that show that architectural choices, like for example, having symmetric matrices might be
crucial for these properties. So, I think there is a lot more to explore there. Okay, thanks.
Okay. So, more questions? Yeah. So, let's, let's move on with this stuff. So,
obviously, right, so here, the conclusion was that we have no over smoothing, but we can also
consider more interesting equations. So, so far we consider the very simple isotropic homogeneous
diffusion equation. We can also consider nonlinear versions of the diffusion equation. And this one,
in particular, comes from the domain of image processing, where imagine that you start with an
image like this, right? So, the portrait of Sir Isaac Newton that is noisy. So, if you run a diffusion
equation on an image, it actually has a closed form solution. So, it's convolution with the Gaussian
kernel, where the variance of the Gaussian is proportional to the time of the diffusion, right?
And in the limit, you will have just everything flat, right? So, you average all the pixels in
the image. You see that you don't want to have results like this, because it might average out
the noise, but it also destroys the discontinuities in the image that, for visual perception, are very
important. So, the idea of, that was originally by Peron and Malik in 1990 is to have a nonlinear
diffusion equation that is controlled by the gradient of the image, right? So, basically,
if you're in a smooth region in the image, like here, so you have standard Gaussian kernel,
but the moment you reach a discontinuity, you slow down the diffusion. So, the effect it has,
you don't average pixels of different intensity. So, here, I'm not averaging dark and white, right?
So, the kernel will look like this. It will look one-sided. And it had a lot of different versions,
bilateral filters, non-local means filters and so on, but the idea is always the same. So,
here, basically, the diffusion speed, right, is inversely proportional to the edge indicators,
to the norm of the gradient. And the result that it produces is like this. So, this nonlinear
diffusion equation knows where to stop locally, and therefore, it averages within smooth regions,
and it doesn't average across regions. Now, we can do the same thing on a graph, obviously. So,
the analogy would be this, right? So, here, we have a gradient, right? This is the divergence,
and that's some parametric function that looks suspiciously like a tension, and that's the
diffusivity, right? So, it's the local strength of the diffusion. And, in fact, if we discretize it
again with explicit forward Euler scheme, then a particular version of this equation corresponds
to the attentional architecture. So, this is a gut. But this was so far, this was a continuous
time, right? And we wanted continuous space. So, the original motivation, right, when we compared
graphs to other objects was somehow to have a continuous analogy of the graph neural networks,
and if, again, we take a step back and look at how diffusion equations work in the plane,
when I discretize the plane as a grid, I don't really have a canonical graph, right, in the sense
that there are many ways I can discretize my differential approaches, right? So, this is how
I can discretize the Laplace-Anon grid. So, I can use neighbors like this, or I can rotate everything
by 45 degrees, I can use distant neighbors, I can use convex combination of all these operations,
right, because this is a linear operator. So, bottom line on a grid, I don't have a canonical
graph, right? I can actually use a discretization, maybe that is different at different points in
the grid. Of course, there will be some numerical implications, but the discretization that we
choose, right, and as a result, how the nodes are connected and which nodes propagate information to
which nodes is, to a large extent, a numerical convenience, right? What makes sense, for example,
from the organization of the memory or the number of nodes and whatever. So, we would like somehow
to extend this mindset to general graphs. And for these purposes, instead of considering these
nonlinear diffusion equations, like Perron and Balig, by the way, they called it an isotropic
diffusion, which obviously, if you're familiar with PDEs, it's not an isotropic, it's not
homogeneous, right, because we have a scalar diffusivity function and isotropic diffusion,
we also have direction, so that would be a matrix or a tensor. So, instead of considering this
nonlinear diffusion equation, we can consider a non-Euclidean diffusion equation. And the model
here is the following, that was actually done by my PhD advisor, Ron Kimmel, also in the 90s,
about 25 years ago, maybe even more. So, again, thinking of an image, you can think of it as
an embedded two-dimensional manifold, right? And the embedding is in this joint space,
where we have a combination of positional coordinates, the x, y coordinates of the pixels,
and the fissure coordinates, in this case, for example, R, G and B channels. So, a color image
is a two-dimensional surface in R5, right, using this model. Now, by virtue of this embedding,
we can define a metric, so we can use the standard pullback mechanism, so in the case of
two-dimensional manifold, it's a two-by-two matrix, given like this, right? And we can
define a Laplacian with respect to this metric, it's a non-Euclidean analogy of the Laplacian,
called the Laplace-Beltrami operator. And we can write a diffusion equation with respect to this
operator, it's called the Beltrami flow, and we can actually show that it's a gradient flow of
a generalization of the Dirichlet energy that is called the polycop functional. It's used in
high-energy physics in bosonic strings, don't ask me what it is, but
that's something is described by this energy. So, by analogy, we can do something like this
on a graph, so now every node in the graph, in addition to having the fissure coordinates,
also has some positional coordinates, right, so positional encoding. Ideally, this positional
encoding should somehow represent the structure of the graph, right, in the sense that nearby points
in this u-component of the space should be more likely connected by an edge, right? And the Beltrami
flow, basically, it evolves, so we have again here a parametric diffusivity, it evolves both
components, right, that I collectively denote by z, and the evolution of x-component is the
standard fissure diffusion. You can think of the evolution of u as some form of soft graph
rewiring, because what I can do, if two nodes become closer in this u-coordinate, I can decide to
create an edge between them if there is no edge, or if the drift apart, I can decide to cut the
edge, so overall, I will facilitate the propagation of information, and I know that it sounds cumbersome,
but this is how it will look like, so again, this is the core graph, so it has,
basically, there are three things happening here, so the positions of the circles, right, so circles
are nodes, their positions represent some two-dimensional positional coordinates, the colors
represent three-dimensional projection of the fissures, and you see that they're both components
are evolving, and the graph is also changed on the fly, right, so when the clusters drift apart,
then we cut the edges between them, so right, so it's fissure diffusion, positional coordinates
are changing, and the graph is rewired, and you can see that the task here is node classification,
so there are clearly seven classes of nodes that we can clearly distinguish here.
Now, if you think of it from the standpoint of signal processing,
we have a very disturbing picture here, right, so we have a filter that happens on the domain,
yeah, a question? Maybe before you move to the next one, I was just wondering because I think
what started to appear in this intellectual is that you apply some ideas from differential
geometry to graphs, and maybe not yet directly, but... Not yet, right, so... Okay, so we are
like talking about metrics and this sort of, so I wonder, maybe you'll be talking about it next,
so sorry if I'm pushing it forward, but what do you think are the limits which come from the
fact that graphs are basically discrete structures, like how free are we to apply ideas? Yeah, give me
a few minutes and I will get there, yeah, so you can find the analogies, right, not everything has
exactly a correspondence, right, so, but these analogies, I hope to show that they can be quite
useful, right, so, but again, if you look at these pictures, so we have some kind of disturbing
picture here, so we have a filter on the domain, and the domain is changing under our feet, right,
so imagine that you're applying some filter, right, which is what it is, right, the diffusion
equation, you can think of it as a form of filter, low pass filter, and the domain is moving, so I'm
doing a filter and then nodes are somehow moving away from me, but this is a very common picture
in differential geometry actually, and it's very common to take a manifold and evolve it under
some evolution equation, and typically what, when you evolve a manifold, you're interested in what
happens to the metric, right, so here's an example of an evolution equation that is called the Ricci
flow, so you take the first order derivative, the temporal derivative of the metric tensor
of the manifold, right, denoted here by g, and you make it equal to the Ricci curvature tensor,
right, so basically the metric evolves proportionally to the to the local curvature, so it looks very
much like the diffusion equation, so here we have temporal derivative, here we have some second order
differential quantity that looks kind of like our Laplacian, right, so structurally it's similar
to the diffusion equation, of course what it does is a very different thing, and if you start with
this manifold, which has positive curvature on, so this kind of dumbbells on the spheres,
it has positive curvature, right, and the neck between them, it has negative curvature, so if
you run this diffusion, if you run the Ricci flow backwards in time, what will happen is that this
dumbbell become more like an ellipsoid than more like a sphere, and then will collapse into a point,
right, and it was introduced by Richard Hamilton in the 80s with the purpose of proving a famous
conjecture in topology that claims that you can characterize spheres by your ability to take
a closed curve and collapse it into a point, right, so this is how we characterize two-dimensional
sphere, I can take any closed curve on a sphere and I can evolve it and collapse into a point,
right, I cannot do it on a torus, so if I have a torus and I have a curve like this,
then no matter what I do it cannot be collapsed to a point, so the conjecture was that you can,
you can characterize higher-dimensional spheres in this way, and you obviously heard about it,
the PoincarÃ© conjecture, and it was shown by Perlman, actually a slightly more general result,
using the mechanism of Ricci flows, right, and that was a breakthrough of the century,
it stood open for more than 100 years. Now, what does it have to do with our graphs
and graph neural networks, so I remind you that we had this phenomenon, right, that message
passing might not be, might not work well on some graphs, right, so there might be some,
some phenomena that some graphs might be unfriendly for, for message passing, and in particular
it depends both on the structure of the graph and the task, and if my task requires to propagate
information from distant nodes, and the structure of the graph is such that the receptive field
of the graph neural network grows exponentially fast, right, so the number of the neighbors of
the neighbors of the neighbors becomes very large very quickly, this happens in trees,
this happens in what is called small world graphs like social networks, then we have a problem,
we have a lot of information that we need to squeeze into a single feature vector,
and this is a phenomenon that we call overscorching, so let's, let's define mathematically what we
mean by overscorching, so let's say that we have a message passing architecture of this form,
right, so we have the node itself at layer k, and we have the neighbors, we combine them with some
learnable weights, let's call them w1 and w2, let's say that the depth of this architecture is l,
the width, right, so the internal dimension is p, the long linearities are well behaved,
right, so the elliptics continues, and we also have some bound on the, on the weights,
so this is what characterizes our architecture, so what is overscorching, it's some form of insensitivity,
right, so if I look at the output of the neural network at node i and I examine how it depends
on the input at some distant node j, I can describe the sensitivity of the output to the input
through this Jacobian, right, so through the partial derivative, and if the partial derivative
is small it means that the information propagates badly from input to output, right, so basically
I will not perceive the change in the input in the output of that node, and what we show in the
paper is that we can bound the Jacobian by constants that depend on the model, right, for
example the number of flares, the regularity of the activation functions, the bound on the weights,
and also something that depends on the graph topology, right, and we show in particular,
for example, that width does help, of course, at the usual expense of
worst generalization overfitting, depth doesn't help, for example, and the topology has the
really the largest effect, and intuitively we expect that in some kind of benign graphs,
like grids, for example, we will not have overscorching, and in pathological examples,
like trees, that would be probably the worst case, right, so you see that the topology of
the graph comes here through the power of the adjacency matrix, but we don't see exactly how,
right, so it's hard to say, right, so what does it mean a matrix to the power l, so we need something
more nuanced, we need some kind of geometric analysis that will allow us to tell apart structures
like this and structures like this, right, something that locally looks like a grid or
something that looks like a tree, that's exactly what curvature is designed for, right, so I remind
you that in differential geometry, what curvature tells you is that if you take nearby points and
shoot geodesics in parallel at the same speed, you can either converge, remain parallel, or diverge,
right, and we call these spherical, euclidean, and hyperbolic geometry, right, so locally it looks
like a sphere, like a plane, or like a hyperboloid, or high-dimensional cases as well, so on a graph,
the analogy could look like this, so there are several definitions of reach-type curvature on
graphs, so this is a combinatorial definition that we use here, so you can take nodes that are
connected by an edge, let's call them p and q, and look at edges that emanate from these nodes,
so if they tend to form triangles, it means that we look at something like a click, if they form
rectangles, they will look at something like a grid, and if they drift apart and don't form anything,
then we look at locally at something that looks like a tree, right, and basically we can count
different types of rectangles and triangles, allow me to skip the details, basically for every
edge in the graph we can have this combinatorial quantity that we call the balanced formant
curvature, that counts, basically it looks at a two-hop neighborhood of an edge, and it counts
certain types of rectangles and triangles that surround this edge, bottom line, each reproduces
the continuous behavior, so clicks are positively curved, grids have zero curvature, and trees are
negatively curved, right, so that's I think to your previous question how what is the
parallel between differential geometry and graphs, so this is an analogy of a curvature,
so it's not a discretization of a curvature, it's a discrete curvature that behaves in a
similar way, and now the relation between the over-squashing and the curvature, what we show
is that if we have strongly negatively curved edges in the in the graph, then we can write down
this bound on the Jacobian, and it means that the over-squashing is caused by
the presence of strongly negatively curved edges, yeah.
Yeah, so it's the number of triangles that surround an edge, and this is the number of
rectangles, this is the degree, yeah, doesn't really matter, there are several definitions,
so why we call it balanced form and curvature, because there is a classical notion of form
and curvature that looks a little bit like this, we just touch it a little bit so it behaves like
what is shown here. This really relates to the rigid curvature tensor and the formula from
matrix, I mean I don't see it now, but maybe. It's a graph, so you don't have exactly the same thing.
Yeah, obviously, but some I don't know components, can we do some analogy or not, this is something
completely new. So it shows how, so you can think of curvature as how locally the volume changes,
so in a sense it shows how the volume changes, so there are two classical definitions of
curvature on graphs, one through optimal transport that is called the Olivia curvature,
and this is combinatorial version that is called the the form and curvature.
Right, so basically the conclusion, well I should make it explicit that it's strongly
negatively curved edges that cause overscorching, right, so basically due to this bound, actually
the presence of negative or slightly negative curvature might be benign, this is what is shown
in the the expander's paper by Petr Wieliczkiewicz and his and his causes, so these are somehow the
optimal graphs, the best for message passing, but expander's needs to be slightly negatively curved.
So once we know it, we can actually interfere basically, we can surgically remove the
negatively curved edges and replace them potentially with edges with higher, with more
positive curvature, and this way we retouch the graph a little bit and we show that it improves
the performance of graph neural networks both in homophilic and heterophilic settings.
So there was a question about diffusion-based rewiring before, and I promised to tell exactly
what I mean by this, so this is the paper that is called DIGL by Stefan GÃ¼nemann and his students
from the Technical University of Munich, and DIGL stands for Diffusion Improves Graph Learning.
So the idea there is that you rewire the graph by basically by computing page rank and embeddings
for a personalized page rank embedding for every node, and you connect then in this new embedding
space the nodes that are closest. So what it does essentially, it introduces connections within the
same connected component in the graph right, or within the same clique or cluster in the graph,
and it has a hard time to connect across different communities in the graph.
So when the graph is homophilic, this is a very good thing to do, right, so you're connecting to
to similar nodes, but if the graph is heterophilic, it can do more harm than help,
and in fact experiments show that this is the case, they also write it in the paper,
the curvature-based approach, first of all it changes the graph minimally,
here the change can be dramatic, right, so that's the number of edges that are changed,
but it also helps in the heterophilic cases because it's not restricted by this property,
you actually typically bridge different communities by the new edges that are created.
So still talking about diffusion, am I actually out of time?
Okay, yeah, so still talking about about diffusion equations, here are some more exotic
exotic stuff, right, and this is our maybe creative way to illustrate to illustrate sheaves
or bundles, so there has recently been probably a better picture, so that's really sheaves,
right, in the literal sense, so what are sheaves? So they actually have very interesting history,
and well I like these kind of historical factoids, so the theory of sheaves in algebraic topology
was introduced by Jean Lyret, so he was a French mathematician, he was also an officer in the
French army, and when the Nazis invaded France he was captured and put with his comrades into
concentration camp, and basically he was asked to work on mathematics, and his expertise was
mechanics, so he was very afraid that he would be forced to work on something that would be useful
for the Nazis, and basically he will be committing treason, helping the war effort, so when he was
offered the possibility to teach something in this camp, he chose a very innocuous topic,
algebraic topology, which could be useful, and then after, well of course they were released
after the war ended, he published it in a course that was taught in captivity, and one of the
papers that came out of this course introduced the theory of sheaves, so sheaves are objects that are
taught in algebraic topology, so if we apply them to our setting to graph, and this is slightly
different construction that is called cellar sheaves, so if you think of graphs by analogy to
manifold, so a manifold is a topological space, what I mean by topological space roughly is that
you have a notion of neighborhood, I can tell who my neighbors are, but I don't have the notion of
distances or angles, so if I want to talk about distances or angles, I need some extra machinery,
and on many folds this is typically achieved by what is called an affine connection,
or parallel transport, so it's a mechanism that tells me how to move vectors between
tangent spaces at nearby points, I can also define a remaining metric if I want to equip
a manifold with geometry, and then there is a special type of connection that is called
the Levy-Civita connection that is compatible with the metric, so you can think of the same thing
on a graph, so a graph is a purely topological object, I have a notion of who my neighbors are,
but I don't have any geometry, so in order to introduce geometry I can equip every node and
every edge with a vector space, and I can define by analogy to parallel transport, I can define
linear maps, so these are called restriction maps that go between these spaces, so slightly
different from many folds, I go from the space associated with nodes to a space associated
with an edge, and then if I want to transport information from a node to a node, I need to
combine these two maps, one with transpose, so basically it's a kind of, so I invent
geometry on a graph, so I lift it into a more complicated object, and on this object I can
now study, for example, what happens if I choose these restriction maps to be of certain class,
so these are matrices of certain dimension, and I can choose them, for example, to be
symmetric, or I want them to be orthogonal, or I want them to be something else, right,
I can also choose the dimension of these, of these, what is called stocks, right, so these spaces,
and I can define differential operators on this structure, right, so the difference between
the standard, for example, gradient and the shift gradient would be the same way as we have a
manifold, I cannot add or subtract two points on the manifold, so when I need to subtract two
vectors, I first need to apply parallel transport, so I need to bring the vector from its vector
space to my vector space, to my tangent space, and by doing this typically I would apply some
form of rotation, if it's a manifold with the remaining metric, and only then I can subtract
them, right, so here the same, if before the gradient looked like just difference between
n points of an edge, here we'll have also some linear transformation that sits in between,
right, long story short, I can, basically I'm interested in a Laplacian, right, so I have a
shift version of the Laplacian, so it's a block matrix where every block transforms the vectors
with these kind of, with these kind of matrices, right, the combination, and now I can apply this
operator on my data and run a diffusion equation, and I can run it to infinity with some additional
conditions, and I can ask questions like how many classes can I separate if I choose this
shift in a certain way, yeah. Are you doing graph rewiring here? No, we are not doing any
graph rewiring, so the graph structure is encoded in the structure of the Laplacian,
right, so basically it's a kind of question about expressive power of this architecture,
so I can ask how many classes I can separate, right, so expressive power, slightly different,
different version from the, from the Weisferre and Lehmann, because in Weisferre and Lehmann we
asked about the, how many, the types of graphs that we can, if we can distinguish, here we're
looking at node level problems, yeah. Can you please make some like use cases for this kind of
of graph, and the ones that were shown before in which the nodes were actually separated with
no connections, one to another? So, the graph here is given, so it's, I think some, I don't remember
what data set it is, yeah, sorry, what is again the question? So, like, what kind of, what are
you trying to model and why is this configuration? Oh, so, yeah, the colors represent the classes,
the ground rules classes, and the positions represent the features. And what's the difference
between this type and the one in which you do rewiring and each class series separated,
one to another? So, here the features are represented by coordinates, so the closest
analogy of this illustration is to the one that I showed with the gradient flow, right,
so the coordinates here represent the features, not the positional coordinates, right, and the
colors represent the classes, okay. So, there is no rewiring happening here, you can also potentially
use rewiring. And the results, well, I don't want to go through all the results, but basically what
we show is that by using different types of sheaves, we can guarantee that we can separate
different types of, different number of node classes, depending whether the graph is homophilic
or heterophilic. For example, we show that you must have non-symmetric relations if you want to
deal with heterophilic graphs, yeah. So, in previous method, WL, it's something like, I think the
classification was on top of number of edge, if I get it correctly, like if you both go to the
higher classes, then the number of edges also increases, but here in sheaves, it's, it's,
is it about the dimension that we have, like, can we have a first dimension sheaves or something
like this? So, Weisfer and Lehmann is different, so the hierarchy there is, basically, they are
different algorithms, right. So, whether they do color refinement for different structures for,
for a node, for a pair of nodes, for triplets of nodes, and so on. So, here we have, the choice is
what kind of matrices we allow, what class of matrices we allow, so it's typically a group,
right, so we say, for example, the, the most general case is, is GL, right, so any invertible
matrix, then we can restrict it to be, for example, symmetric matrix, or then we can restrict it to
be orthogonal matrix, right, and based on these choices, plus the dimension of the sheave, we get
different results. So, it has to take dimension and also the matrix. Exactly. Thank you. So,
this is a more theoretical question, right, because it's a good question how we actually,
how we learn the sheave from the data, but assuming that we knew the sheave, right,
but we allow the sheave to be only from of a certain type, what is in the best case,
how many node classes we could separate, under different assumptions also about
the structure of the graph, whether it's homophilic or heterophilic. But, basically, the, the, the,
the bottom line of this story is that diffusion, when you have, when you have these extra degrees
of freedom, looks more interesting than, than, than the standard diffusion on a graph. So,
the standard diffusion on a graph corresponds to symmetric restrictions with one dimensional sheave.
Yep.
well so it's slightly more complicated right so the analogy of the
connection would be the composition of two maps right so what we call a
transport map so each of these f's is called the restriction map so it goes
from node to edge space they actually can have different dimensions so they
don't need to be the same the composition right so f f transpose is a map
from the space of one node to the space of another node so that's the analogy of
parallel transport right so I'm when I move a vector from one node to another
geometrically transform it somehow rotated for example or scale it depends
on the class of matrix that I use here so in exactly so but then of course in
practice you need somehow to parametrize it right so you cannot of course in
principle you can say that that let me learn individual f's for every for every
age of the of the graph but it's not practically feasible so in practice f is
a matrix valued function that depends on the node features so it's a little
bit similar to attention but the tension is scholar here it's matrix so it's a
geometric operation okay any questions
right so basically to summarize this this part so what do we gain from this
physics inspired perspective on graph neural networks so first of all I think
it's different viewpoint on old problems like over smoothing bottlenecks it
allows to on the one hand to interpret existing architectures like guts for
example from a different perspective it allows to potentially design your
architectures right for example using if you think of a generous discretization
of differential equations then of course you can ask what kind of solver can I
use can I use some some more interesting things with adaptive step size or maybe
I don't know multi grid solvers and so on it allows to make principle
architectural choices right like example with gradient flows so basically from
the gradient flow we get restriction on symmetric weights we get residual
connection we can also have some theoretical guarantees right again like
we've seen with the gradient flow but maybe also of other types like
convergence stability and so on and so forth but probably more interesting are
links to other fields that are less explored in graph neural network literature
like in particular differential geometry or algebraic topology and of course
diffusion is just one example of evolution equations you can consider more
interesting things so this is one example right so probably have seen these
kind of things coupled oscillators so the metronomes that are put on a table and
because the vibrations transfer from one to another initially they might be
oscillating out of phase and then they become synchronized so think of
something like this but on a graph so the coupling occurs on a graph in a
learnable way and depending on the tasks that we want to do we have we want
somehow to to interact between these different oscillators so it's also a
differential equation but it also has a second order kinetic term so unlike a
diffusion equation it has also a surgery component and here we show for
example that we can probably avoid over smoothing by using this type of
equations how much time do I have 15 minutes okay so what I would like to do
if in 15 minutes let's talk about grids so I definitely ran out of time because
out of all the geometric objects at the end well we spent all the time on
crafts I think grids also deserve a little bit and probably well everybody
is familiar with grids right so let's look at them maybe again from the
perspective of geometric deep learning and hopefully some new intuition or at
least for some of you have not seen it before so and this also relates to the
previous question of why we call graph convolutional networks convolutional so
first of all a grid is a graph right so it's a particular type of a graph for
simplicity I would like to assume that the grid has periodic boundary conditions
so basically it's what is called the ring graph and the idea of geometric deep
learning right this group based framework we had some domain and we have a
group that acted on the domain we have a signal that was defined on the domain so
this is a general type of this mechanism that is often called lifting so now I
have a linear operator so this can be anything right so this can be a non
linear complicated thing here I have a linear operator so the group
representation attacks on functions defined on the domain okay and in the
case of a grid this is just the shift operator so this is what they show in
one dimension so just cyclically moves the elements of the vector
now another thing that you see in a grid is that it has a fixed neighborhood
structure right in this example every node is connected to exactly two
neighbors and they are also ordered right so I always have the one before and
the one after in a two-dimensional grid I might have some partial order right so
I have something on top and something on the bottom so in the past on the
general graph we had this kind of aggregation function right so we have the
feature of the node itself and then we had a multi-set that was unordered of the
nearby features and because we didn't have any order in this multi-set we the
only thing we could do is to apply a symmetric function apply a permutation
invariant function now we have a different situation now the nodes are
ordered right so we have a fixed order of x i minus one x i and x i plus one so
this function can be more general right so it doesn't need to be doesn't need to
be to be symmetric and if this function is linear then we get the convolution
right and if I write it as a matrix vector product then it looks like this
so it's a special matrix which has fixed elements along the diagonal right so
this is the the the local weight-sharing that have been convolutional neural
networks so this is a special type of matrices they're called circumvent
matrices right or convolutions so it's synonym you take a vector of
parameters right let's call it data and you create these matrix by cyclically
shifting by one position these vector of parameters and depending it as columns
that's how you get you get these matrix again I'm assuming periodic boundary
conditions so technically speaking it's not a convergence it's a circle
convolution or a cyclic convolution but just to make things simpler okay now one
thing that you first thing that you learn in in algebra one-on-one is that
matrix multiplication is not commutative right a b is not equal to b a but with
these matrices with convolutions there are with circumvent matrices that's not
the case it's actually a special type of matrices that do commute right and in
particular they commute with one of them which is the shift operator right so a
shift is also a circumvent matrix right or also a convolution right so looks
like this so what does it mean that that a convolution commutes with a shift so
this is what we call shift-equivariance right so in other words I can first
apply convolution and then shift or I can first apply shift and then
convolution the result will be the same right so convolution is shift-equivariant
you can show the other way around right so you can show that if you have shift
equivariant linear operations so I take a matrix and I tell you that it's
shift-equivariant you can show that it must be a convolution right so basically
convolution emerges from considerations of translational symmetry right so the
only linear operation that that is shift-equivariant is convolution so
convolution is the only thing that satisfies this property and we've seen
again this geometric deep learning blueprint so allow me to show it again
so we have a grid we have a translation group its representation is the shift
operator so the convolution is a function that is
equivariant with respect to this to this group now we also know that there
is an intimate relation between the Fourier transform and the convolution
right and let's actually try to understand what the Fourier transform is
where it comes from so we know from algebra again that commuting matrices
are jointly diagonalizable it means that they have the same eigenvectors or more
correct eigenspaces but here we assume that the multiplicity of eigenvalues is
trivial so they actually have the same eigenvectors and the only different
different eigenvalues right so all commutative matrices satisfy this property
so if I have a set of matrices that commute pair wisely then then this is
the case right and this is the case for for convolutions or for circuit matrices
so we can pick up one of these matrices right and compute its eigenvectors
right and we know that all of them will have the same and it's convenient to
look at the shift operator right at the matrix S and if we compute the eigen
vectors of the shift operator you can do it by hand it's actually not difficult
you see that they look like these complex exponentials so this is exactly the
Fourier transform or more correctly the discrete Fourier transform so the
question of course that remains is what the eigenvalues are right so we know
that the eigenvectors of all conversions are the discrete Fourier
transform so these complex sinusoids but the eigenvalues also you can show it
are the Fourier transform of the vector theta that forms each of these matrices
and this gives us this dual relationship between the Fourier transform and the
convolution so if I have a signal x I can do convolution in the spatial domain
by multiplying by a circuit matrix or I can do it in the in the Fourier domain
I can compute the Fourier transform and there the Fourier transform diagonalizes
the convolution so it becomes an element wise product right so basically the
product of two Fourier transforms is the Fourier transform of the convolution
right and typically in signal processing the filters are already designed in the
Fourier domain this is bread and butter of signal processing so the the the
advantage of using the Fourier transform because this operation usually on
grids can be done efficiently so instead of n squared operations as you would
typically require here because the Fourier transform the the the matrix has
a very redundant structure you can you can avoid these explicit multiplications
you can reuse some of the multiplications and do it in n log n operations so
there are classes of algorithms that are called fast Fourier transforms and this
is from the approximately the sixes when this was derived with the most famous
algorithm is by Kuli and Tuki this is how signal processing has been done and
you have it everywhere from your stereo to your iPhone from your computer so this
is how it's done you cannot do it on graphs because on graphs the analogy of
the Fourier transform would be the eigenvectors of either the adjacency
matrix or the Laplacian matrix so if they are symmetric they have orthogonal
eigen decomposition but these matrices do not have these redundant structures so
the Fourier transform has n squared complexity dense matrix multiplication
and actually some of the early crafting of electrical architectures came from
this domain of signal processing on graphs where that that used the eigenvectors
of the Laplacian or the adjacency matrix as an analogy of the Fourier transform
so the difference in the case of the in the Euclidean case on the grid there is
no difference between the two right so the Laplacian is also obviously a
circuant operator circuant matrix and so is the shift right or the adjacency
matrix of of the ring graph which happens to be the shift operator they all
commute so they have the same eigenvectors on the general graph they are
different so therefore these methods slightly slightly differ so the way to
think of why you you want to look at the adjacency at the adjacency matrix is
this right so this is how you can think of your convolution so these basically
it's multiple diagonal matrix now we can write it as a sum weighted by these
coefficients of the powers of the adjacency matrix right so the adjacency
matrix of the ring graph will look like this so this red diagonal right so
that's the shift operator so if you take a square you will get this if you get
cube you will get this right so you combine all of them you will get you
will get this general convolution so first architectures that try to do to do
learning on graphs looked exactly at this taking powers of either the Laplacian
or or the adjacency matrix basically polynomial with learnable coefficients
now if you also look at terms of the degrees of freedom so a fully connected
layer will look like this right so it has no it has no symmetry so here the
symmetry is trivial so it has n square degrees of freedom in the case of
convolution so the the the symmetry here is translation we have order of n
degrees of freedom right so we reduce dramatically the number of parameters in
the neural network we reuse the same coefficients everywhere in the case of a
graph because we have permutation in variance we don't have the order of the
neighbor so we must use the same coefficient so we can only distinguish
between ourselves and our neighborhood right so that's well here it's I'm
assuming a complete graph so this will look like something like deep sets for
example so but the number of parameters is order of one so it's independent on
the on the size of the domain what else can I say can I tell you well I know
that I'm out of time so do you want to hear about molecules or you heard about
molecules okay so let's talk about molecules I promise that I will try to do
it try to do it fast and probably heard in Miguel's lecture as well so it will
probably be a little bit repetitive so graphs are a very convenient model for
molecules right basically a molecule looks like this so you can represent it
as a graph and maybe that's not how chemists think of molecules but at least
in some applications graph neural networks have been successful in predicting
certain properties of molecules that are required for virtual drug screening
right where the space of potentially synthesizable drug like molecules is
huge something like 10 to the power 60 the number of molecules that they can
actually test in the lab is significantly smaller so you need to
reach this by some kind of computational methods and crafting networks have been
shown again in predicting some properties to be significantly faster
while similar complexity to similar accuracy to to classical methods so one
thing that that that is important to say in regard regarding molecules so
molecules are not just any graph where the symmetry that we have is the symmetry
of the domain right the permutation of the nodes or the reordering of the atoms
right so the domain symmetry tells you that no matter how you order the atoms
in the molecule I still want to be able to say that it's the same molecule but
it also has geometric coordinates right so in addition to the let's say atom
types that we have here I also have the XYZ coordinates for every atom right so
it's a graph that lives in a continuous Euclidean space so here what I want to
say that if I rotate the molecule for example or translated I want to be able
to say that the properties remain the same so in this case typically you look
at the special Euclidean group so rotations and translations without
reflections reflections can actually change the properties of molecules or
you can use some other groups as well and there have been already several
interesting success stories so one of them was a group of Jim Collins at MIT so
they used graph neural networks in virtual screening pipelines where they
tried to determine which compounds could be used as new antibiotics against
antibiotic resistant bacteria and they famously found that that a candidate
drug that was tested against diabetes called Halicin was actually effective
across a broad range of of antibiotic resistance bacteria but in things that
we are doing we are mostly interested in proteins and this is well I think this
is in general proteins are important targets for for drugs because they are
involved practically in anything that that happens in our body from defense
against pathogens right antibodies are special types of proteins to delivering
oxygen to ourselves hemoglobin is also a special type of protein so basically
they're everywhere and encoded in our DNA so we really we don't know any life
form that is not based on proteins at least for the time being and it was
conjectured in the 70s by Anfins and Nobel laureate in chemistry that you can
determine the structure of the protein from its sequence so proteins are long
chains of amino acids connected to each other and then under the influence of
electrostatic forces they fold into these complicated structures but we are
interested in the opposite problem so maybe a little bit incorrectly we can
call it some kind of inverse problem so I would like to to design a protein that
will fall in fold into a certain structure of course it's not that simple
because it is tempting to think that we have a sequence that then falls into a
structure and the structure and doubts the protein with certain function for
example what kind of molecules it binds and initially computer scientists look
at proteins as sequences because well it's just strings so we can look for
certain patterns you can try to align different sequences together right like
multiple multiple sequence alignment then came the problem of structure
prediction and that's where alpha fold excelled recently but then the problem
of function is distinct and you can find examples of for example proteins with
different sequences but similar structure you can find proteins with very
similar sequences but very different structure or you can also find proteins
with different sequences and different structures but similar function so they
happen to to bind the same the same molecule so the good analogy here is
this lock and key metaphor that was introduced by I like quotes from
Nobel laureates so that was from Emil Fischer also Nobel Nobel laureate in
in chemistry so he was talking about enzymes but I think it's more general
applies to to proteins broadly so same way as you have a unique key that fits
into a lock you might have a unique molecule or at least that's that's the
wishful thinking is that a unique molecule that will fit into some pocket
that exists on the the surface of this folded protein structure and this is how
drugs are typically designed right so you have a protein that is your target so
that's how its surface looks like and here is some small molecule that sticks
into this hole and binds this this molecule and that's how the drug works so this is
actually a molecule not exactly of caffeine but of compound from the same
class and that's how it's by how it binds the adenosine receptor in the brain
many other interesting targets though they don't have these kind of pocket like
structures and there are interesting systems of protein of proteins interacting
with each other like this one the program death complex where you have two proteins
called pd1 and pdl1 and they are involved in cancer immunotherapy basically these
proteins tell our immune system not to kill healthy cells and some cancers
have these proteins so they manage to evade the normal functioning of the immune system and
the idea is to block one of these proteins either pd1 or pdl1 and this way basically
the malignant cells are destroyed by by by the immune system so you need to design
some binder that will that will bind to one of these proteins and they happen to have
these kind of flat interfaces so they're considered to be hard or impossible to
drug by small molecules but you can drug them by proteins so that's the idea of biological drugs
where the drug itself is is a protein molecule typically an antibody for variety of reasons
so you can use geometrically planning well and unfortunately I didn't have time to talk about
it but basically instead of considering graphs we can consider surfaces so we model proteins as
many folds as as basically the external surface that that that appears to
to the other molecule that that tries to bind it and this way you abstract all the internal
intricacies of the fold so let me try to show you an example so this is a plastic model of a protein
you see so this is protein is the the one that that the person holds is supposed to bind to
these transparent ones so you see that these complicated helixes and and other things inside
so that's the protein fold but what appears from the outside is this transparent surface so
this guy doesn't care what happens inside so it cares only about the the the structure of course
the problem is more complicated because the the conformation of the protein the its geometric
structure might change as a result of of the interaction but at least it's in some cases
it's a good approximation so long story short we we can do special type of neural networks that
operate on these surfaces so they take into account both geometric and and chemical properties of
of the molecular surface and they can they try to find complementary structures that are expected
to interact so think of kind of pieces of three-dimensional puzzle but it's not only
geometric complementarity it's also chemical complementarity so they need to have the right
charges so they don't repel each other and this is a method that we call the massive so
we were lucky to appear on the cover of nature methods in 2020 and this year we also had a paper
in nature that contained experimental results so we also hope to appear on the cover but they chose
a different one but because we paid for the cover here you need to you need to see it i think it was
a cool image so we used this method to design new binders for different targets and basically it's
a fragment based design so we use this neural network architecture to identify potentially
complementary targets that then we use to build the binder and here the experimental results show
different structures so here's a binder for the pdl one oncological target and we also have the
crystal structures and here's an example of another binder for the SARS-CoV-2 spike protein so that's
the coronavirus that caused the COVID-19 pandemic that has been terrorizing us for more than three
years now and basically this structure binds the region of the of the spike protein that interacts
with the ACE2 receptor of the host so that's where how the virus enters into into our body
and here we also tested it so we have the structure from cryoEM we also tested it on
different variants of the virus so the alpha beta and omicron that probably everybody was
following in the newspapers so you see that that it binds many of these maybe some others less
and here's also a comparison to a clinically approved drug so that was antibodies that were
developed by AstraZeneca so basically what is shown here is how much inhibition you have
versus concentration so the smaller concentration the better of course so we are not as good as
the AstraZeneca drug but so it's something that was designed totally computationally and this is
actually pseudovirus neutralization so it is probably much closer to real validation than
at least anything that myself as a computer scientist could think of well I think I will
probably stop here but if you think of diffusion models right so generative models everybody is
now talking about right like like the Dali2 and now of course you have way better results
so you could imagine something like this for for molecular design so we have some condition on
on let's say diffusion model that we use here like the geometric structure of the of the target
pocket and you'll try to build a molecule that satisfies these these constraints so we don't
really have a text prompt but you have maybe some some other way of conditioning the model so
so this is one example maybe not very interesting so another example is what we call diffusion
linker where we have small molecular fragments what is called pharmacophores that you know how
they bind the target but you also need to connect them into bigger molecule and we try to basically
to start with these little fragments and to diffuse the the the linking structure that that connects
them we're not very lucky in publishing this paper in europe so we'll probably send it to some
chemical journal uh well I think I will stop here sorry for running out of time thank you very much
uh
yeah we are over time but if you have still a couple of questions
if not you can ask individual maybe
okay but thank you again for for the amazing talk well thank you
thank you
