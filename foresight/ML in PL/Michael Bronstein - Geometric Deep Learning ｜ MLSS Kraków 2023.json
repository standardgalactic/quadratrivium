{"text": " Thank you very much. So great pleasure to be here. It's actually my second time in Krakow, and I think it's a very beautiful choice for having a machine learning summer school. So in the next three hours, I would like to talk about geometric deep learning. And if you wonder this intriguing image, what does it have to do with machine learning? So you see it fits more, a kind of an alchemist. So we wanted to use this image in reference to this famous quote from Ali Rahimi who was receiving the Best Paper Award or the Proof of Time Award at New Europe in 2017. And he was speaking this way maybe a little bit critically about deep learning, at least at that time, that we say things like machine learning is the new electricity, and he's alternative metaphor was machine learning has become alchemy, so in the sense that some kind of science that maybe produces something that we don't really understand what it does. And really what we would like to do here is to try to understand from certain perspective how these methods work and why they work, and maybe more importantly, when they fail and you see kind of maybe general blueprint for developing potentially future machine learning systems. So the concept that will be important to these lectures is the concept of symmetry. And symmetry according to Vile, who I'm quoting here is, depending on how wide or narrow you define its meaning, is an idea by which men through the ages has tried to comprehend and create order beauty and perfection. So it sounds a bit poetic, but I think it is true, so that's from his book that is titled Symmetry, that he published in Princeton. And symmetry is a Greek word, so it goes back to the ancient Greeks, and as you probably know, ancient Greeks like Plato considered symmetry to be really the cornerstone of the universe. So according to Plato, what is nowadays called platonic solids, symmetric polyhedra were the basic building blocks of all the stuff in the universe, so probably tiny little things that build up the matter, and if you think of it in modern terminology, that's not very far from truth. So Plato believed that geometry is really the key piece of mathematics, and even according to a legend, there was an inscription on the entrance to his academy saying that nobody skilled in geometry, that is not skilled in geometry, should be allowed to enter. And this idea of matter being built of small symmetric polyhedra, actually not far from truth, if you consider how crystals are organized, and the first to study this from a formal perspective was actually Kepler, who is maybe more famous for his discovery of the motion of planets, but he was also the one who laid the foundations of modern crystallography, basically considering how spheres can be packed into different configurations, and if you also think that this is old and boring and outdated stuff, so last year's Fields Medal was given exactly for solving these kind of problems maybe in higher dimensions. So geometry itself also, at least in formal way, goes back to ancient Greeks, and what we still often study at school as the geometry dates back to Euclid, his famous elements, so a system of axioms from which his geometry was derived, and as you know there are five axioms or five postulates of Euclidean geometry, that the last one was somehow standing out and people for many centuries or even thousands of years tried to do something with it, and for example in the 18th century Giovanni Saccheri, who was a priest, almost arrived to the construction of a non-Euclidean geometry, but he considered it such a heretical idea that he thought that it is repugnant to the nature of straight lines, so he abandoned these ideas and never took it to the full extent, but in the 19th century what happened is that finally came the realization that dethroned Euclid and broke his monopoly of geometry and the works that probably gauss himself did, but never published, and then famously Lobochevsky, Boje and Riemann, who created the first examples of what we now call non-Euclidean geometries, and in the 19th century this is how the geometry started looking like, so a zoo of different types of geometry without clear relation or understanding what actually defines the geometry, so there was obviously a need to put order in this mess, and the new idea, new approach came from Felix Klein in 1872, so he was only 23 years old, he was lucky to get an appointment as a professor at the University of Erlangen in Bavaria, so this is something that for example Euler failed to have in Switzerland, he had to go to Russia to become a faculty, probably not very dissimilar to situations that some of us are facing in these days, so Klein was asked, as it was customary in Germany and still customary I think, to deliver a research prospectus, basically to explain what he is going to do until his retirement in Erlangen, which actually never happened because he moved three years after to different places eventually to G\u00f6ttingen, and in this prospectus that entered the history of mathematics as the Erlangen program, he proposed a kind of algebraization of geometry, so studying geometry from the perspective of group theory, so essentially considering a geometry as a space plus some class of transformations formalized using group theory, and studying properties that remain unchanged or invariant under these transformations, so you take an object and you apply to it rigid motions and you preserve a lot of things like areas, parallel lines, distances, so this is how you create Euclidean geometry, but you can consider other groups like a fine or projective group, and in fact he considered projective group to be the broadest construction, he in fact showed together with Biltrami that the first non-Euclidean geometry is hyperbolic, geometry is with negative curvature, could be constructed with a projective model, and these ideas had really big impact on geometry, on mathematics more broadly, I would say culturally, like category theory is an extension of these ideas to more abstract objects, but probably most importantly it had an impact on physics where came the realization in the beginning of the 19th century, probably starting with Neutra with her famous theorem that the laws of physics themselves can be derived from considerations of invariance or symmetry, and for example what Neutra showed is that principles like conservation of energy that previously were considered to be empirical could be derived mathematically from certain symmetries, so symmetry of time in this case, and these ideas in a more generalist form led to what nowadays is known as the standard model, so basically all the world can be modeled and can be derived from first principles of symmetry, so what I think physicists among you would call external symmetry or internal symmetry, the symmetry of the spacetime, so what is called the Poincare group that gives rise to Minkowski geometry of special relativity or internal symmetries of quantum fields, that's what gives rise to different forces or different interactions, and I think nobody put it better than Philip Anderson Nobel laureate in physics that without or with only slightly overstating, you can say that physics is the study of symmetry, so what does it all have to do with deep learning and neural networks and machine learning in general, so let's do maybe a brief detour into the history of machine learning or artificial intelligence, so the term artificial intelligence comes from these people, the Dartmouth conference that happened in 1956, organized by McCarthy and others, and you can see them, some very prominent figures sitting here, so this is for example, this is Claude Shannon and this is Marvin Minsky who would become all very important scientists, and historically apparently the term artificial intelligence was introduced to kind of distance themselves and not to be in the shadow of the expert of that time who was Norbert Wiener who introduced the term cybernetics, which I think is still used, I think here in Poland it's probably still used as a kind of overarching term for everything that it has to do with computer science and artificial intelligence, so around that time there were many works that tried to understand how the brain works, so I think at that time it was already understood that somehow our intelligence is concentrated in the brain, so models for neural networks, probably the most famous one is by Frank Rosenblatt, the so-called perceptron, but there are models before that, and he was able to show that was in the 50s, so these models had to be implemented in analog hardware, he was able to show that he can solve some simple pattern recognition problems with these neural networks, and it was extremely remarkable at that time, so there were articles in the popular press like the New Yorker saying that this is the first serious rival to the human brain ever devised, so I think you can only smile, I can hear you laughing, and it's remarkable machines capable of what amounts to thought, so that's according to New Yorker was the perceptron, and there was a little bit of hype, as you probably know, this MIT computer vision summer project, so they thought that they would be able to model a large part of the visual system over one summer, of course, we're still working on these problems 50 years after, and there is no end to it, but also at MIT these two guys, so one of them appeared already in the picture, Marvin Minsky in the same work, they published a highly famous or infamous, if you want, book called The Perceptrons, where they introduced mathematical analysis of these networks proposed by Rosenblatt, and they showed, for example, one example that is taken from this book, that a very simple logical function like exclusive or could not actually be implemented as a perceptron, so these patterns are not linearly separable, so that was a very harsh criticism for the models, and some people say in retrospective that this was what triggered the AI winter, so some people even say that there might have been some personal animosity between Rosenblatt and Minsky, they went to the same high school, and Rosenblatt also died in a boating accident, so as far as to suggest that it was a suicide, well, the story is probably much more mundane, so the funding that was cut by government agencies like DARPA was more related to them being more pragmatic and budget restricted, and that had an impact on the field that coincided with the publication of the book, but if you look at the substance of the problem, what Minsky and Pebert called in their book Perceptron was actually not exactly an architecture that was devised by Rosenblatt, so they actually clearly stated, so they refer to this architecture as simple perceptrons, and this is what we now typically understand by this term, so it's, as you know, it's just a linear combination of the input coordinates with learnable weights that go through a nonlinear activation, typically sigmoid or in simple cases sine function, but what is also very important, they were probably the first ones, at least to my knowledge, to use geometric approaches to machine learning problems, so they, for example, formulated this group invariance theorem that tell in which or to what kind of transformations in the input patterns the neural network will be invariant, and another interesting thing actually the subtitle of the book is an introduction to computational geometry, so that was the sixties, so this term didn't exist, they actually introduced it, and one of the reviews of the book that was critical was asking whether this is just some kind of new mathematical fad that will go away a few years after, and you probably know computational geometry is now very well established field, so it has remained there for a long time, but if you go into the substance of the discussion is actually the question is what kind of classes of functions these neural networks could represent, right, so what is what is called the expressive power, and there were results at that time, so coming from mathematicians in particular Komogorov and Arnold working in the Soviet Union that claim that you can take multi-dimensional functions and decompose them in this way, and basically any function could be decomposed in this way, so these kind of results are known as universal approximation, they go probably as far as the thirteenth problem formulated by David Hilbert, and the modern statements of these theorems are usually attributed to Seybenko and Hornig, these are late eighties, early nineties that are specific to deep neural networks, so what these results say is that if you have not a single layer of perception, but two layers like this, then you can approximate any continuous function to any desired accuracy, so the results it's a class of results, it's not a single result, but roughly the way that we can understand it is that with just a configuration like this, with just two neurons like this, you can approximate, you can represent a step function, once you can represent step functions, you can decompose a continuous function into tiny little steps, so the proof is slightly more involved, but that's that's roughly the idea. Now it's sort of constructive proof, and there are different versions for limited widths or limited depths, it's sort of constructive proof, so it doesn't tell you how, so it's an existence result, so it tells you that you can find a neural network with potentially a very large number of neurons that approximate functions, and if you look at machine learning problems, so in a very maybe simple and naive setting, like classifying images of cats and dogs, basically you can think of it as some cynicism say that deep learning is a curve fitting problem, so it's multi-dimensional curve fitting, so there is some kind of black box where you put some something that acts as a universal approximator, so some sufficiently rich architecture, and you try to represent the function that distinguishes between cats and dogs in this way. Of course this is not a well-defined problem, if I give you a finite sample of, let's say these are cats and dogs, I can pass infinitely many functions through these points, so I can interpolate these points in infinitely many ways, so we need somehow to restrict our class of functions, and that's typically what you do by imposing some sort of regularity, and mathematicians have very well understood concepts of regularity like Lipschitz continuity, right, so in simple case you can think of it as a function with bounded derivatives, right, and the problem is what happens when you increase the dimensionality of your input when it doesn't look like a one-dimensional curve, but it's an n-dimensional curve, and here the results, unfortunately, are not favorable because you can show that as you grow the number of dimensions, the number of samples that you need to take in order to approximate a function to accuracy epsilon grows exponentially with a number of dimensions, so this is again, it's not a single result, it's a class of phenomena that are called the curse of dimensionality, so it's a statistical or geometric phenomena that explains how functions behave in high dimensions, and the term itself actually goes back to Bellman who spoke about the curse of dimensionality in physical problems. Now, in these problems, simply if you think of images, right, of even something like 30 by 30 pixels, which is probably the smallest image you can imagine, digits from the MNIST dataset, the number of dimensions will be approximately thousands, so if you count the number of samples that you need to take, if you took this kind of straightforward approach, it will probably be more than the number of, not only the cats or dogs on earth, but probably close to the number of particles in the universe, so there are not sufficiently many animals around just to do it, and this kind of problem of curse of dimensionality, right, or you can also call it combinatorial explosion, was brought up in another report that is associated with the AI winter, which is the Lighthill report in the 70s. It was commissioned by the analogy of the DARPA, or basically the British funding agencies, and that was the point of time where they decided to stop funding these crazy ideas in looking at neural networks. So what happened, of course, was this AI winter, but at the same time people continued working on these architectures, and some interesting ideas came from the field of neuroscience, from the study of the organization and the function of the visual cortex, the famous experiments in the 50s and the 60s done by Hubel and Wiesel, a duo from Harvard that won the Nobel Prize in medicine for understanding the organization of the visual cortex, where what they found is that the cells in the cortex were organized with some local shared weights, and this was reproduced by Fukushima in his famous neocognitron paper in 1980. So the idea here was a neural network that does, it has two types of neurons, so neurons that he called simple neurons and neurons that he called complex neurons, so one in modern terminology that would correspond to local filters and pooling operations. And he worked on OCR type problems, so character recognition, and the problem, of course, if you treat this type of problems with the standard perceptrons, if I give you a digit of digit three, and I move it just by one pixel, you see that the input into the neural network can change dramatically, and in fact he complained that perceptrons were not by design invariance to these translations. So his architecture actually is remarkably modern by modern standards, so it was seven layer networks, so I think we can call it deep by modern standards. It had local connectivity, what neuroscientists called receptive fields. The filters were non-linear, and he wrote in neuroscience terminologies, so he talked about inhibition and activation. He had average pooling, so these are the complex layers. He used reloactivation, so already in the late 60s that was common, but the training was not done using backpropagation, so it was a kind of unsupervised type of clustering approach. And backpropagation, again, it existed already in maybe some other forms, but this is how neural networks were trained. So Rosenblatt had a special rule for a single layer perceptron, then there were some other methods that were developed in the 60s, and then backprop became really popular in the 80s, starting from the paper of Rumelhardt. So Lecante was just a fresh graduate at that time, and he was working on, actually, the application of backpropagation neural networks, was interested in Neocognitron, and he also happened to work at AT&T, which they were developing the first digital signal processors at that time. So basically, there was a good application to implement on a DSP, and basically, he stripped down the kind of neuroscience terminology of Fukushima, replaced nonlinear filters by linear filters that could be implemented as convolutions. Actually, the first paper never mentioned the term convolution, and the name came after in, I think, in 89, or even later, and he was able to show in real time complex pattern recognition tasks, such as recognition of handwritten digit, which was a difficult task at that time. It was actually deployed in commercial applications. They were working with banks in the post office, and that's where the MNIST dataset comes from. But the computer vision community took a different path, and the late 90s and the early 2000s, probably until the first decade of this century, the approaches that you would typically use for image recognition were to detect some local features, then compute local feature descriptors, and then create some representation that would be passed into some very simple classifier, like a support vector machine, which also were considered to be more favorable by mathematicians, because you can prove, for example, global optimality results about them. So there are many papers written, so SIFT, the scale environment feature transform, one of the most cited papers in computer science ever, was extremely well engineered detector and feature descriptor for these tasks. And what happened in 2012, as you probably all know, that all these carefully designed handcrafted features were beaten down by a very large margin by a convolutional neural network. So it was this lack of confidence of the computational capabilities of hardware, the GPUs, as well as availability of large data, the image net benchmark, which contained millions of annotated images, that finally allowed these architectures to shine. And since then, all the best results in this benchmark were by deep learning. So if you look at the AlexNet architecture that won this benchmark in 2012, it is more or less the same as what was done by Lekana. It's just slightly deeper. There are some different architectural choices. It has way more parameters. It was trained on GPUs, which by the way was not novel. GPUs were used for general purpose computing at least a decade before and for training neural networks probably at least seven years before. So in a sense, it was a very careful engineering and application of existing ideas on a very large dataset and a very important problem that convinced everyone. Yeah, there is a question. Sorry to go back a couple of slides, but you were mentioning Fukushima's implementation and no sort of back propagation learning. So like was he giving some kind of neuroscience, how do you say, a proof for this? Like some kind of happy learning or how did the... I don't remember what kind of rule he used. I think it was inspired by some hypothetical ideas of how the brain learns. Yeah, but it was not back propagation. He showed many other things. So he showed, for example, geometric stability, stability to noise, but again, it was in the early 80s. So the training examples were very rudimentary. So black and white letters. Thank you. Right. So basically, all the rest is obviously history, right? So the people who started the deep learning thing got the Turing Award and now very famous. And basically, this is technology that has really transformed the field both the academical subjects and the industry. So just to give you, we'll be talking about graph neural networks and you've probably heard from Miguel as well in the previous lectures about graph neural networks. So just to give you a little bit of history of this one, so they're actually very much related and rooted in chemistry. And chemistry is probably one of the fields of science, which is data intensive. It produces a huge amount of data, experimental data. And since early times, chemists tried to organize these data first, publishing these humongous books containing information about molecules or chemical reactions. And then it appeared the first digitized archive, the chemical abstract service. And also with the appearance of the first computers came the need and the idea to search for molecules, right? So if, for example, you're a pharmaceutical or a chemical company and you want to patent a new molecule, how do you know that it has not already been described, right? So you need to search fast for molecular structures in some data set. And these were the first ideas of chemical ciphers that describe a molecule as a string and then try to match it in some data set. So that was the kind of problems that these guys, or he was a Romanian chemist called Vladus, that was one of the pioneers of a field that later became known as chemoinformatics, he was trying to look at molecules as graphs. And as you probably know, the term graph itself, in the sense of graph theory, also is associated with chemistry. This is how Sylvester called the first attempts to design structural molecules, structural formula of molecules, basically trying to understand how atoms are related to each other with their chemical bonds, not only just the number of atoms and the types of atoms. And as you probably know, this was one of the first structural formula of benzene that was derived by Kecoli. And the legend says that he dreamt of a snake biting his own tail, so he came up with this kind of aromatic ring that is described here. So these kind of problems inspired these duo of mathematicians, we'll talk about them later, Weisfried and Lehmann in the 60s to devise an algorithm that would test whether two graphs are structurally similar, what is called the graph isomorphism test. And these ideas maybe were noticed, so there are many related works in the machine learning community and also in a chemical community, trying to devise new types of neural networks that could take as input, not vectors, not images, but graph structured data. And the early works by Alessandro Sperduti in the 90s, for some reason most of the works were by Italians, and probably the most cited ones are by Marco Gore, Scarcelli and others. And interestingly, about 10 years ago, the graph neural networks returned triumphantly to chemistry, so I think worth crediting David Duvenau and Justin Gilmer for who also introduced the terminology of message passing neural networks that try to predict properties of molecules, model these graphs and learning on these graphs. And then, of course, the ideas of geometric learning, as we'll see maybe with some extra stuff. Also, the structural biologists have had their own image net moment with alpha fold, first in 2018 and then in 2020, basically predicting the structure of protein folds. So, and this field is very rapidly developing. So, I think these are very exciting and cool problems that you can address with geometric techniques. So, let's just try to summarize basically what this historical excursion gives us, a kind of blueprint for different architectures. So, if you look at convolutional neural networks and graph neural networks, right, they work with very different data, convolutional neural networks work on images, graph neural networks work on graphs, right, let's say molecules, but there are some common patterns, right. So, in both cases, we have some underlying domain. So, in the first case, it's agreed. In the second case, it's a graph. We also, in both cases, have some kind of geometric operation, so a symmetry, right, that is a nature in the context of the problems that we are considering. So, in images, it's translation, right. So, I want to move, for example, an object in the image and I don't care where the object is located if I want to classify it. In case of molecules, it's a permutation symmetry, so no matter how I order the atoms in a molecule, it's still the same molecule, right. So, I want somehow to be insensitive to this ordering. And we can also define natural operations that respect this symmetry, right. So, in case of convolutional neural networks, it's actually the convolutional operation. Basically, I can move a patch around an image and apply the same weights or the same local rule that would extract the features. And as we'll see in graph neural networks, this is some kind of local rule that we call message passing, right, or some versions of it. So, these are ideas that, as you see, these are two examples or architectures that share the common principles and that's the idea of geometric deep learning. So, I can probably take the craze of inventing the term. So, that happened when I was writing one of my ERC grants back in 2015, probably. And of course, everybody was doing deep learning. So, you need to distinguish yourself from everyone. So, I wrote that we are not doing deep learning as everybody else, we are doing geometric deep learning. So, that was the idea. Then we popularized it in this paper in IEEE signal processing magazine and more recently in a book that I'm writing with my collaborators. So, by analogy to the Erlangian program, we basically, we can think of a kind of common denominator for machine learning architectures. So, we would like to take this zoo of different architectures that were historically designed for different types of data with different kind of problem in mind and look at them from the same perspective. And this perspective is through the lens of group theory and properties like invariance, equivariance and symmetry. And if we've seen before this problem of machine learning in high dimension, where you have, for example, your images of cats and dogs as points in a high-dimensional space, we no longer consider these inputs as just a high-dimensional vector that you need to put through some generic class of functions and then you suffer from the curse of dimensionality. But now we know that there is some domain, geometric structure that underlies these inputs. And in images, for example, this is a two-dimensional grid. So, our data lives on some typically low-dimensional domain. And this domain comes equipped with some group. So, in this case, it's the group of translations. And so, we have a domain, we have a group, we have signals that live on this domain and the group that acts on the points of the domain, we see how it can act on the signals themselves through what is called the group representation. And in case of images, again, the group representation will be just the shift operator. We'll see it in more details. And then finally, we have functions that act on these inputs that somehow need to respect this symmetry. And this will be through what we will call invariance and equivariance. So, basically, we want the function either to be insensitive to how I transform the input by acting on it with the group or should change in the same way. And I should say that the choice of the group and the domain are two separate things and they're not only dependent on the data, they're also dependent on the task. So, I might have a problem like this. So, I have, for example, images of traffic signs. And if I'm designing a self-driving car, it's very unlikely that I will see rotated traffic signs, right? They will probably be aligned and move just horizontally and maybe vertically. So, here, for example, the group of transformations that is reasonable to assume will be just two-dimensional translation, maybe even one-dimensional translation. But if, for example, my car can also tilt, right, so imagine that it's a plane and not a car, then rotations are also perfectly valid, right? So, then the group of transformation will be different. So, it's still the same domain, the same data, but the task might be different and, therefore, the assumptions about this, the priorities problem might be different. And if you think of another application, if I have, for example, a pathological sample, so, essentially, a glass of some stained tissue that I put under a microscope, so there I can also have reflections, right? Because I don't have canonical orientation for this glass. So, it can be an even bigger group in this case and, again, task dependent. So, as I mentioned in case of images, the representation that we'll be working with is the shift operator. In case of crafts, actually, the symmetry, as we've seen it, is the group of permutations, what is sometimes called, confusingly, the symmetric group. So, it's the different ways that I can rearrange and different objects. And its representation is a permutation matrix. And as we'll see, the way that it's implemented, so, functions that are equivalent with respect to this symmetry are message passing. And we can also have another type of architectures that are also confusingly called equivalent neural networks or equivalent transformers, where, in addition to the symmetry group of the domain, we also have symmetry group of the data. And this is typical for geometric graphs like molecules, where the nodes also have geometric coordinates. So, they live in three dimensional space. And in addition to reordering the atoms in the molecule, we can also rotate or translate the molecule in three dimensional space, right? So, you want to be equivalent with respect to both transformations. So, it's a kind of analogy of the external and internal symmetries you have in physics. And, basically, geometric architecture is just sequences of equivalent or invariant layers. You can also interleave them with pooling. So, I will not talk about it too much. But pooling is implementation of another principle that is important in physics, which is called scale separation. And this is what makes physics work. So, if you consider, for example, let's say, this room, right? And how we are surrounded by probably a quadrillion of different molecules that move very fast and collide with each other. But that's not how we can model the behavior of gas in some space, right? It's computation intractable to trace all the molecules. So, fortunately, there are just a few parameters that explain statistically how these gas behaves, right? Like temperature and pressure. And this is the main principle of statistical mechanics. But if we want, for example, to model how Earth, with its very complicated atmosphere, moves around the sun, again, we can completely disregard it and consider it as a point. Because at that scale, all these details are completely irrelevant. So, the scales, of course, interact with each other. So, this is maybe a wishful thinking. And in neural networks, you can also mathematically show that in some architectures, pooling operations are necessary for them to operate correctly. So, these ideas can be applied to different types of objects, to geometric domains. So, traditionally to grids, but then also to graphs, maybe to general homogeneous spaces, and then also maybe to more exotic things like manifolds, meshes and geometric graphs. And if you look at some of the standard architectures that are very commonly used in deep learning, whether it's CNNs or maybe LSTMs or deep sets or transformers or GNNs or intrinsic mesh convolutional networks, they can all be derived from the same blueprint. So, there is some kind of domain and associated symmetry and associated environments that you can bake into your architecture and you get, basically, particular instances of this blueprint, some of the most common and famous architectures. Any questions so far before we start talking in detail about graphs? So, if I have time, I will try to cover all these different domains, but I would probably spend most time on graphs and also some physics-inspired perspective on these architectures. Yes, I will have questions. So, if you kind the term geometric deep learning, is there an alternative, like non-geometric deep learning or like traditional deep learning, or what does it mean? Well, so, it's a very broad term. So, I think we use it maybe in a very in a very broad sense. So, it's using geometric ideas or geometric techniques to interpret and build deep learning architectures and vice versa, applying deep learning to geometric objects. Now, whether, for example, the model of group environments and equity environments is really the kind of ultimate truth, of course not. So, it's a mathematical abstraction and in most cases, the transformations, for example, you have an image is, they're actually not well described by groups, but at least it's a good starting point. In many cases, for example, in molecules, this is kind of physical realities or an inductive bias that you rather want to incorporate in your architecture. Okay, I have one more question. Could you please take back to the previous slide? Yeah. Because you mentioned like the existing neural networks architectures and the group of symmetries. And do you think there is any other group of symmetries that we in real world applications should think about? And because of that, create some new architectures that is, I didn't know, for example, was suited for them? Yeah. So, it's a good question. So, there are some other architectures, for example, mostly coming from physics. So, of course, in physics, you have interesting groups. So, I think there are some Lorentz environments, for example, neural network architectures. You can also combine some groups. I will show some examples that you can have, for example, products of permutations or some special group products of permutations in subcraft neural networks. So, it's a general blueprint. So, if you can follow the blueprint and design architecture that implements this invariance, that is relevant for your problem, then you have a way of doing it. Okay. Cool. Thanks. There are also some works that try to discover the symmetry group from the data and from the problem. So, that's also an interesting direction. Could you shortly address the symmetries which the transformers go for, because I still have difficulty to think in this symmetry, like, perspective? So, I will talk. Well, it will take me probably about 20 minutes, but we'll get to transformers. Yeah. So, transformers are special types of craft neural networks. You can think of them really quickly. So, on the slide, there's the LSTM. So, it's written that time warping is you could have architectures that are invariant to time warping, but if you look at speech recognition. So, maybe first of all, we humans also don't have that, right? So, if something is super slow down, we're probably not going to notice what it is. So, my question is, is there any other invariance or equivalence for time series data that you can think of? So, well, here what I mean by time warping, you can actually show that gating is a necessary mechanism to be able to accommodate time warping. So, basically, gating emerges as basically as the architecture for this kind of, for this kind of invariance. Okay. Got it. Thanks. Yeah, I think there is a question there. So, okay. I mean, so you said that you will talk about transformers more in, like, in the next few slides, but I wanted to ask about one of their emergent properties, like in relation, for example, to CNNs, namely that CNNs have, like, kind of encoded in them by definition translation invariance, whereas transformers don't. And I think it has been discovered that transformers actually kind of learned the translation invariance from the, thanks to the amount of data that they, that they consume. But, like, they weren't defined to do that. It's kind of, this translation invariance just emerged from the data. So, I was wondering whether, in your opinion, it is better to, like, use more generic architectures, which learn those inductive biases from the data, or whether we should encode the inductive biases in their architecture, or, and, you know, face the risk that a specific inductive bias may be also a limitation. Yeah. Well, I think the answer is already contained at least one of the answers in your question, right? So, the amount of data is obviously a limitation. So, it might be easy to collect data, like images, right, or maybe text. It might be much more difficult to collect data that comes from biological experiments, right? So, if the data is limited and the data is expensive, then probably you want to work hard to incorporate as many inductive biases as you can. In some other applications, actually, the inductive biases, or some symmetries or some invariances come from the problem that you're trying to model. I think it's true for many applications in what is called the AI for science, right? So, if you have some, I don't know, physical system, you know that certain properties will be conserved. So, it makes no sense just to use a generic black box that will be producing unrealistic outputs, right, that are physically incorrect. But there is no, so it might be that in problems where you don't know a priori what symmetry or invariance you have, it might be a good idea to use transformers if the computational complexity and the scale of the data allows it. Hello, I have a question about augmentation. It feels like, at least in the image case, augmentation is basically exploiting this group symmetry, right? You augment by doing some translation. But if you use some geometric-based architecture, maybe then you don't need to do augmentation anymore. So, what's your opinion of augmentation? Is it like an artifact or something? Well, augmentation is a technique of, basically, when you have limited amount of data, you can generate synthetic data that looks like the kind of data that you want to see in your application. This was actually one of the important features of the AlexNet, for example, so they use certain type of data augmentation for images to make it more robust. So, again, you can incorporate this kind of inductive biases into the architecture. Sometimes, it might be difficult. So, another aspect that is often overlooked is actually the hardware, right? So, it's very easy and it's probably more a coincidence. At least originally, the convolutional neural networks map very nicely to the type of hardware that is used in the GPU, the single instruction multiple data type of architecture. So, it was not by design because the GPUs were designed for different types of problems. With other architectures, it might not be the case, right? So, with graphing a lot of programmable, this is not the case. So, there might be some better, so to say, architectures, maybe from mathematical standpoint, but they're just not as convenient to implement. Therefore, maybe you will prefer to use something that is less correct, but you can do data augmentation. The hardware allows you to implement this architecture better. What do you think is a follow-up question about augmentation? What do you think about augmentation? Not as a simple tool for increasing the size of the data set, but as something as in contrastive learning, as enforcing the symmetry. Can it be kind of equivalent to the symmetry you defined? Well, it was used in this formula. I mean, can the load of enforcing the symmetry be shifted towards augmentation to the model? Yeah, so basically, you're sampling points from the group orbit, right? So, you can think of it this way as well. Whether it's enforced in a hardware or in a software, that's probably not that important. So, data augmentation is a very valid technique, if you know exactly how to do it. Okay, so let's move on to graphs. And, well, graphs, as you probably know, their idea itself is pretty old. So, it's usually attributed to Euler who was thinking of these kind of problems, right? How you can connect land masses without actually accounting for the particular geometry, but only how, what is nearby, right? So, the famous problem about the bridges of K\u00f6nigsberg, and this is what he called the geometry of Cetus, or basically the geometry of place, what in modern terminology we call topology. So, the term was actually introduced by Pankareho, by analogy to Euler's terminology called analysis Cetus, and that was also where his famous conjecture appeared. So, we'll talk about Pankareho conjecture actually later. I hope to get there as well. So, graphs, obviously, I don't need to convince you that graphs are interesting and important. So, more or less anything from very small scales or very large scales can be modeled as a graph. So, any system of relations or interactions, whether it's a molecule, or you model how different atoms interact with each other through chemical bonds, to, for example, interactoms, right, in biological science, how different entities in our body or in a cell interact with each other, different, for example, chemical reactions, or even social networks, right, describing relations, friendship, interactions between different people. So, this model is a graph, and again, graphs can be of different types. So, let's consider a simple model here. So, we'll consider an undirected graph. So, it means that we have a collection of nodes, and we have unordered pairs of nodes as edges, right? So, just pairs, basically, the order doesn't matter, and the nodes are described by d-dimensional vectors. So, these are features that are attached to the nodes. And, of course, it can be more complicated. So, you can have graphs that are directed, you can have both continuous and categorical features, both in the nodes and the edges. But, just, that would be already interesting enough to look at this kind of object. So, one thing that characterizes graphs, right, basically, this is, again, a topological construction. So, it's an abstract object that lives on its own. The moment we need to represent it on a computer, we describe it, for example, as a matrix, right? So, we can describe the structure of the graph as the adjacency matrix, right, of size n by n, and it's a number of nodes. So, we have one, if there is an agent between a pair of nodes and zero, if there is no edge, right? And if the graph is undirected, then this matrix is symmetric. And the features, we can describe them as a matrix of size n by d, right? d is the dimensionality of the node features. So, one key thing here, which is already written on this slide, is that we don't really have a canonical way of ordering the nodes of the graph. So, when I make this description, I automatically assume some ordering of the nodes, but this ordering can be like this or can be like this. So, anything that will take this description of the graph as an input must account for this built-in ambiguity, right? So, I somehow need to be able to produce outputs that disregard, in a correct way, all these possible permutations, right? And the two types of problems that we can consider in relation to graphs, but actually more problems, but let's say these are the prototypical problems. One is graph-level problems. So, I give you an input graph and I try to output a number that describes this graph, right, or maybe a vector. Like, for example, I'm predicting the chemical properties of this molecule, like water solubility, so the input is a graph describing a molecule, the output will be some number, right? Another class of problems, I give you a graph and I want to do node-level decisions, right? For example, I want in a social network to classify which of the users is behaving badly, right? Maybe a spammer. So, in the first case, I give you a graph, right, and the output, no matter how I permute the inputs, should be the same, right? So, we're talking about a permutation invariant function. So, mathematically, it can be described like this, so, right? So, the function here now is a function of x, the node features, but also the structure of the graph. So, they're both from the input, and here I act on these two inputs with the permutation matrix, which is the representation of the permutation group. You see that actually the representation is different for different types of objects. So, the features, you can think of them as vectors, right? So, I permute only the order of the rows. The adjacency matrix, it's two-dimensional tensor, so, I act both on rows and columns, right? I need to permute both rows and columns. Here, this is clear. And together, this form permutation invariant function. In the second case, if I want node-level predictions, the output has the same structure as the input, right? So, if I change the order of the inputs, the order of the outputs is expected to change in the same way, right? And here, I'm interested in permutation equivariant functions. So, equivariance means changing in the same way. So, the output now will be, well, some kind of vector, right? Or a matrix, you know, by f capital. And if I permute the input, the output will be permuted in the same way, okay? Now, what are graph neural networks? Essentially, these are parametric graph functions. So, I provide you a graph as an input, right? Or these matrices x and a, and I output something, right? And something here, as we'll see in a few minutes, is parameterized by some vector of parameters. And sometimes, there is no distinction made between graph neural networks or message passing neural networks. So, we want to make this, that is distinction, but sometimes they're used synonymously, right? So, most of the graph neural networks that are used in practice are of the message passing type. We'll see exactly what it means in a second. And graph neural networks, again, you can consider them as a special instance of these geometric deep learning blueprints. So, we have usually a sequence of permutation-equivariant layers that produce node-wise predictions. And permutation-equivariant, in the sense that if I change the order here, it will change in the same way here. And then, if I have graph-level tasks, I will have permutation-invariant pooling, right? That aggregates all the information from the, from these node features and produces a single output for the graph. And the typical way that, that they work is by neighborhood aggregation. So, you can pick up a node in the graph, right? Let's call it i. And now, I look at the neighborhood of the node. By neighborhood, I mean all the nodes that are connected to i by an edge, right? So, this is how the neighborhood of i looks like. And I can look at the feature vectors associated with these, with these nodes. And you can see that even though the neighbors are unique, right? The feature vectors are not unique. So, this is encoded by color. So, these two nodes have exactly the same feature vector, right? Just for this example. So, together, they form what is called a multi-set, right? So, it's a set where the same object can appear more than once. And we also have the feature vector of the node itself. So, I want a function to aggregate them locally, right? Let's call this function phi. And again, the, the characteristic property of this function is that I don't have a canonical ordering of my neighborhood. So, the feature vectors can appear like this to, to this phi. So, it must be by design permutation invariant, right? It cannot assume any order in which the neighbors come. We can make it more complicated and we can incorporate additional information. But again, as a basic structure of a graph, you don't have this order, right? Now, you can repeat this process everywhere at every point, at every node of the graph. And this is actually very highly parallelizable, at least in principle. And once you do it, you get an output, right? That for every node of the graph, you output some vector. And this is also a function of the graph. And you can easily see that if my choice of this local aggregation is invariant, then the output is equivariant, right? So, if I change the order of the axis here, then the output will change in the same way. And most of the graph neural network architectures differ in the choice of these phi, in how I aggregate locally the features. And while they're probably zillions of different architectures, most of them fall within the following three categories. So, the first one is what is called convolutional graph neural networks. And you can simply think of convolutional neural networks as just summing up the features of the neighbor nodes. So, this is what I write here. So, the update for the representation for the feature at node i is the sum of some transformed neighbor nodes, right? Psi will be some learnable function. Xj is the the the feature of the neighbor node. Here it is. And ai, j are coefficients that depend only on the structure of the graph. In the simple case, just the elements of the adjacency matrix, right? And here, sigma is just some non-linear activation, typically very low, right? So, that's how a convolutional type graph neural network looks like. Yeah. Is this convolution here equivalent to 1D convolution, something like that? So, we'll see it, we'll see it in the, well, not in a second, but we'll see it later. So, basically, you can obtain convolution when the graph is agreed. And we can actually see that that convolution on the grid is a special type. Basically, it's a unique linear equivalent function. So, basically, yeah. So, that's why the term convolution is appropriate. So, it's an extension of conversion to graphs, right? And you can write it in this form, right? So, if I write it as matrix multiplication, so x is my feature matrix of size n by d, I multiply it from the left by some matrix a, right? The diffusion matrix. It can be the adjacency of the graph. It can be something else. But, basically, it propagates information between adjacent nodes. And on the right, I have, in the case of a linear transformation of the nodes, it's a learnable shared matrix that acts on every node in the same way, right? On the features of every node in the same way. We'll see that it's related to diffusion equations on graphs. And this is probably the simplest version of a GNN. It's highly scalable. You can basically just large matrix multiplication. It has been used in industrial use cases. I think the first to use these kind of architectures was Pinterest with Pinsage. We also used it at Twitter. And then there are some statements about these architectures in the kind of graph neural network folklore saying that it works only on homophilic graphs. So, by homophily, I mean this assumption that my neighbors are similar to me. Typically, the assumption is that the labels in the neighborhood are somehow similarly distributed to the label of the node itself. And here's an example of a homophilic graph versus a heterophilic graph. So, and the usual motivation that is given that these matrix a typically will look like a low pass filter, right? So, you're somehow averaging your neighbors. And if the neighbors are of the same type, then it will work. And if the neighbors are very different types, then it makes more harm than help. And this is actually not true. So, I hope to convince you that the story is much more nuanced. There is also this channel mixing matrix. And there is an interesting and subtle interplay between these two matrices. And we'll be able to see how it works when we consider graph neural networks as differential equations. So, slightly more interesting architecture is an attentional GNN. So, here, again, we can think of it, at least in some settings, as a linear combination of the features of the neighbors, maybe transformed by some learnable function. But now the coefficients depend not only on the structure of the graph, but also on the features themselves, typically through an attention mechanism. So, the most famous representative of these architectures, they got the graph attention network. And in the most general case, right? So, we can write got like this. So, that's if we have a linear combination, some, so we have linear combination with the matrix. But now the matrix is actually a matrix valued function of x, right? So, it itself depends nonlinearly on x. And the most general case is a message passing architectures where we have a b-variate function that depends on the feature of the node and the neighbor. And the other cases are specific cases of this architecture. And it was shown that message passing GNNs with specially chosen aggregation function, in particular, it has to be injective in certain restricted settings, again, allow me not to go into the details, are equivalent to graph isomorphism testing algorithm that was derived by vice versa and lemon that I mentioned in the beginning. And let's talk about this. So, basically, from theoretical standpoint, what actually graph neural networks do, right? Or message passing type graph neural networks do. So, first of all, what is graph isomorphism problem? It's telling when two graphs are the same, right? So, I'm writing here equal, but of course, it's not equal. So, it's equivalent in some sense. And what do I mean by this equivalence? What I want to say is that there exists an edge preserving bijection between the nodes of these graphs, right? So, in other words, I can find a correspondence between nodes in G and G prime, such that if there is an edge between a pair of nodes in G, then there is a corresponding edge in G prime, right? So, is this bijection unique? What do you think? No? When is it not unique? Exactly. When the graph has symmetries, right? So, this is an example. Actually, this graph is a good example, right? So, you can reflect the nodes on the left and the right, and then we can have another bijection, right? Like this. So, this bijection is not unique. But for determining if two graphs are isomorphic, it's sufficient to say that there exists such a bijection, right? So, that's what tells us that the two graphs are equivalent, right? So, basically, they are the same up to the ordering of the nodes, right? So, if I look at their adjacency matrix, they will be the same up to applying some permutation, right? So, I can basically, I can relate the two adjacencies by permutation. And related to the question of universal approximation, right? Which is fundamental for traditional neural networks like perceptrons, we can show that a class of functions is universal approximating permutation in variant functions on graphs with, importantly, here, the limitation finite node features, if and only if it can discriminate graph isomorphisms, right? So, basically, universal approximation is equivalent to graph isomorphism testing, right? So, basically, the two things go hand in hand. And if you ask what kind of graphs can we represent with message passing neural networks, right? So, here is, let's say, the space of all graphs. And these would be graphs that are structurally equivalent, right? That is isomorphic. So, by construction, we know that graph neural network cannot distinguish between these graphs, right? They're exactly the same up to the ordering of the nodes. So, just by construction, it will produce the same output for any isomorphic graphs. But the question of the opposite direction is more interesting. And this is not necessarily guaranteed, right? So, I might have different graphs that are not isomorphic, like the reds and the blues, that by chance will have the same representation. So, the graph neural network will output the same output for these different graphs. So, in other words, if we have the space of all permutation in variant functions, right, and we know that these are all graph isomorphism discriminating functions, this is where we'll see a subclass of functions that can be computed by message passing. Okay. And so, the question of graph isomorphism, as I mentioned already, it came from applications in organic chemistry, where people try to compare structures and try to determine whether two molecules are the same, right? So, in the case of isomorphism is a special setting, right? We can also think of distances between graphs. And vice versa in 68 came up with an algorithm that they believe to be a polynomial time method for determining whether two graphs are isomorphic. So, I should say that at that time in the 60s, even the notion of complexity was not totally spelled out. And also, the understanding of what's the complexity of graph isomorphism testing as a computer science problem was not understood. Actually, it's not understood even now. So, we know that it's not NP-hard and we also don't know polynomial time algorithms for it. So, it's a special complexity class that is called GI class. But anyway, so, it was actually disproved by a counter example. So, it was an example, it was shown that the class of graphs that cannot be tested by the device for an algorithm. We'll see such examples in a second. But the way that it goes, it's essentially a color refinement procedure. So, it considers a graph without any features, considers only its structure. And, initially, the graph has every node labeled in the same way. By label, I just mean a natural number that is attached to a node, right? And what the algorithm does, it takes a node and looks at its neighborhood, right? And you can see that originally in this graph, we have two types of neighborhoods. So, we have a blue node with two blue neighbors like this. Sorry, that's blue node with three blue neighbors and blue node with two blue neighbors, right? So, these are two neighborhoods that we see in this graph. So, if I now apply an injective function that they call phi, right? Think of it as hashing. I will have two distinct outputs, right? So, I will have nodes of the yellow type, let's call it, and of green type, right? So, I will be able to distinguish between these different neighborhoods. So, now, I have a graph with refined labels. I can apply the same procedure again, and now we have three types of neighborhoods, right? We have green with one green and one yellow neighbor. We have green with two yellow neighbors, and we have yellow with two green and one yellow neighbor, right? And these become, again, distinct colors. So, this will be, let's call it violet, gray, and orange. But if I repeat this procedure again, the colors will stop changing at which point the algorithm stops and produces a histogram of colors, right? So, that's a graph level descriptor. You can think of it this way. And what they show in the paper, well, the paper is actually complicated to read, but that's, let's say, a reduction of it. It actually describes a different type of algorithm, what is called 2WL, that does edge color refinement, but it doesn't matter. It's equivalent to what I'm showing here. So, if I give you another graph, and the distribution of colors is different, then I can guarantee that they are not isomorphic. But if the distribution of colors is the same, like in this case, we actually don't know. So, it's unnecessary, but insufficient condition, and in fact, you can find examples of non-isomorphic graphs that would be deemed equivalent by WL test, right? Or in this case, WL test cannot determine whether they're not isomorphic. And you can also see why the reason for it, right? Basically, what it does, it refines the colors of the nodes, so every node looks at its neighborhood. And this is how the neighborhoods of nodes look like, right? So, this node has two neighbors, then this node has, again, two neighbors, and so on and so forth, right? So, if you look at the structure of these neighbors, they will be exactly the same in both cases, right? And actually, very simple examples of graphs, for example, regular graphs where the degree of every node is the same cannot be tested by this simple procedure of Weisfeld and Lehmann. You can also not count connected patterns of more than three nodes, like triangles or cycles. And this is, I think, astonishingly disappointing given that the algorithm came from applications in chemistry, so in chemistry, these would be two different molecules, right? And this has a six ring and this has a five ring, right? So, or five cycle using graph theory terminology. So, we cannot distinguish these molecules by device for a Lehmann test. They would appear potentially the same, right? So, we wouldn't know. So, basically, the functions that can be computed by WL are strictly smaller than all permutation invariant functions, right? And we know examples of functions that cannot be computed by WL. For example, we cannot count the number of rings, right? So, if I want to implement a function that counts the number of rings in a graph, I cannot do it by means of WL test or by means of message passing. Now, the relation between WL and message passing is not random, right? You can see this, even the structure of the algorithm is exactly the same, right? So, this is what WL test does, right? So, it updates the color of every node by looking at the structure of the node and the multi-set of neighbors, right? Here, x denotes the colors. And this is what MPNN does, right? So, here, the squared denotes some general permutation invariant aggregator. It can be sum, it can be max, it can be mean, it can be anything, right? Importantly, it's permutation invariant. So, we can see that it's a special case. So, MPNN expressive power is upper bounded by device for a Lehmann test. And the question is when MPNN is as expressive as WL test, right? So, basically, we're interested in this case, right, when the two circles coincide. And if you look at different types of aggregators, right? So, imagine that this is your input graph. So, I have this gray node that has two types of neighbors. We have green neighbors and we have blue neighbors, right? So, if I consider the input as a multi-set, right? I completely disregard the structure of the graph itself, right? So, what the node sees is just a soup of the neighbor features. So, if I use a maximum aggregator, I cannot distinguish between these and these, right? Because for the maximum, it doesn't matter how many times each of these features appears, right? If I use a mean, then I cannot distinguish between these and these, right? I can multiply the neighbors by some constant factor. The sum, though, allows to distinguish between all of them, right? So, you can think that maximum gives a kind of skeleton of the set and the mean gives you the distribution, but the sum is strictly more expressive, right? And here's an example of structures that max or max and mean would fail to distinguish. And indeed, sum appears the most expressive one. And if you assume that, so the theorem about the equivalence between WL and message passing states that if you assume that the node features come from a countable universe, then if you have an MPNN with an injective aggregator, call it square, an update function phi and graph-wise readout function is as powerful as the vice-versa element set, right? And the assumption here is of discrete countable features, which is not always the case in practice. And then the reason architecture that actually implements that is equivalent theoretically to the vice-versa element, which is the graph-wise morphism network or GIN. So, basically, it uses a sum aggregation. So, the epsilon here is just theoretical thing. So, there exists infinitely many constants epsilon that you can use here. So, we know that, basically, there exists at least a choice, right? Within the all possible message-passing neural networks that makes it as expressive as WL. But, of course, we are interested in more expressive architectures, right? Can you do better than WL? And here, again, there is an entire universe of different architectures. So, some of them actually go beyond message-passing, at least in the traditional sense. And, roughly, you can distinguish between four different categories of approaches. So, it's either a higher-order WL test. It's not a single test. It's a hierarchy of tests. The use of positional instruction and coding, so that's how transformers work. Subgraph GNNs and then topological message-passing, right? So, let's talk briefly about all of them and then when do we need to do the break? Sorry? So, let's maybe 20 minutes and then we do the break. So, the first class of higher-order WL tests, as I mentioned, so WL test is just one algorithm that was initially developed and then extended by Babi and collaborators, actually independently also by other people. So, one of them was Eric Lander, who is mostly known as computational biologist, but he started as a mathematician. And, basically, this is an increasingly more expressive hierarchy of tests. Instead of doing node refinement, they look at tuples of nodes. So, it's obviously computationally more expensive and there is always, you can find a family of graphs that these algorithms cannot distinguish. So, like strongly regular graphs for 2WL or 3WL tests and what is called CFI graphs for general KWL. I should also say that this terminology of KWL is confusing because they're what is called folklore WL tests versus the classical WL tests that are slightly different. So, but overall the hierarchy, right, up to notation is the same. So, we know that message-passing GNNs are equivalent to the standard WL. You can also design just replicating in the neural network architecture the KWL tests higher-order KGNN. So, this is what Hageim Aron did in his works. And then you can also have some other algorithms we'll talk about in a second that sits somewhere between. They don't exactly follow the hierarchy of the WL tests. So, the second approach is positional encoding. And, again, I remind you of this example of two graphs that cannot be distinguished by the WL tests, but imagine that I could now attach some features to the nodes of the graph, right? And this could be even something as simple as random features. You see that now, because I have some extra information, I can distinguish between these cases, right? So, if I look at the leaves, for example, of this tree, right, I can ask, for example, whether the root appears among the leaves or not, right? And here it does appear and here it doesn't, right? So, they're clearly different, right? I would be able to distinguish between them, right? So, the covering of the nodes removes at least to some extent the ambiguity. Now, of course, if I use random features, then the question is how can I reproduce them on a different graph? So, these type of approaches are equivalent only in expectation. But there are other methods that can do better. And these are structural encoding. So, the idea of structural encoding, you have some substructures, right? So, we have a bank of substructures that call it H. And you can count the substructures, the occurrence of substructure for every node or for every H, right? And there are two ways that you can consider subgraphs, whether what is called a subgraph or any induced subgraph. It doesn't really matter, right? So, the two ways that are slightly distinct. And for example, in these two graphs, if this is my bank of substructures, let's say cycles of size 6 and 5. So, in this molecule, at every edge or at every node, I will count once the six cycle substructure and these nodes, I will count twice, right? Because this node participates in both structure on the left and right. But the five cycle substructure doesn't appear here versus it appears here, right? So, with this encoding, now I have some, these additional features that I can attach to every node or to every edge of the graph. And I can use them in standard message passing. And this would allow me to discriminate between these graphs. And the complexity of this method is basically, it's all hidden in pre-computation, so the counting of substructures. So, in the worst case, it's order of n to the power k, k is the size of the substructure, so it could be large. But in practice, for structures, more friendly structures like triangles, there exist more efficient algorithms. So, in practice, it can be way better. The algorithm itself, and especially the training part, which is typically more expensive, is standard MPNN. So, it has linear complexity in the number of edges, or roughly order of n if the graph is sparse. And the theoretical result is that these kind of architecture that we call GSN, graph substructure network, is strictly more expressive than WL on certain assumptions on these substructures, right? So, it should not be a star graph or it should be a structure of size, bigger than three. And basically, we can formulate it is that GSN is not less expressive than 3WL. You can also do it for different KWLs. Basically, you do it by counter examples. So, you can design a substructure that the standard WL tests cannot count, whether it will be clicks of certain type or other things. Right? And the proof by example is something like this. So, this is a stronger regular graph. It cannot be distinguished by 3WL, but this graph contains four clicks and this doesn't. So, if I count four clicks, I would be able to distinguish between these graphs. So, in a sense, it's a kind of cheating. So, I'm not following really the KWL hierarchy. So, this is an example of different structures I can count, triangles and clicks. But then, basically, the expressive power looks like this. So, this is, for example, a graph substructure network with four-click count. So, it might actually, there might be examples of graphs that are distinguishable by 3WL, but not by this method. We have at least an example of a family that 3WL cannot detect. So, it's outside of the hierarchy. And why this is important, especially in applications related to chemistry, because often we know these substructures are priori, right? Organic molecules, for example, cycles are a very prominent feature. These are what is called aromatic rings, right? Like in the molecule of caffeine, we have two, right? So, we have this ring of cycle of size six and cycle of size five. And we see that if we incorporate this information as a kind of problem-specific inductive bias into the problem, we are able to much better predict the properties of molecules. In this case, it's, I think, the water solubility on the, on a toy data set of molecules that is called zinc. And by incorporating cycles, we significantly reduce the error. Third class of approaches, what is called subgraph GNNs. And here, the idea is also very simple. So, if you look at these two graphs, again, this is probably one of the simplest examples of non-lasomorphic graphs that cannot be tested by WL, right? If you do the color refinement, this is what WL produces, so the histograms are the same, right? That's exactly the case where you cannot say anything about the graphs. But imagine that I can perturb the graph, for example, by removing this edge, right? So, if I did it, the colors will be very different, right? So, these will be the distributions of the, of the colors, and they are clearly distinct. So, in this case, the perturbation allows to distinguish between structures that are otherwise indistinguishable. So, the question is, of course, do I know which edge to remove, right? So, here maybe I was lucky, and the answer is usually I don't. So, let's remove all possible edges, right? So, let's just make this perturbation when I remove one edge at a time, right? So, and there are seven possibilities. I can also do node deletions in the same way, right? And now, instead of a graph, I have a collection of subgraphs that are extracted by some policy. So, in this case, it's a very simple policy, one node removal, right? And actually, it results in graph theory that say that if I give you this collection, so, in terminology of graph theory, this is called a reconstruction. So, if they have the same multi-set of node remove subgraphs, right? So, this is what we denote H tilde G, right? So, graphs where, basically, if we look at these kind of multi-sets, they will be the same, of course, up to, up to reorting. So, the statement in graph theory that is called reconstruction conjecture claims that under some technical assumptions, if H is a reconstruction of G, then H is equivalent to G, isomorphic to G, right? So, why it is called a conjecture? Because it is proved only for small graphs, and it's an open question in general. And there are generalizations for subgraphs where you remove multiple nodes. So, this is, again, not a single result. So, it's a class of results. It was introduced by Paul Kelly in his PhD thesis that was done under the supervision of Stanislaw Ulam, who was a mathematician, a Polish mathematician, but he's probably more famous for initiating the Manhattan Project and developing thermonuclear weapons. So, but he also was, he's also famous for many interesting results in mathematics, and this is one of them, the reconstruction conjectures. So, we don't know whether this is true. So, it might be true, but that's why it is a conjecture. It would be cool if it were true, because, of course, in this case, I could test graphosomorphism by just doing something with this collection. So, what exactly can we do with this collection, regardless whether the conjecture is true, right? So, it will just give us stronger theoretical property of these architectures. So, what we can do is we can consider our graph is a collection of subgraphs that are extracted from the given graph, right? And what is important to understand is that there is a correspondence between them, right? Because we created these subgraphs, right? So, it's built on the same nodes. We just might remove some edges, right? Or some nodes. So, we have here two types of symmetries. So, we have the permutation of the nodes in the graph itself, right? And we also have a permutation of the subgraphs in this multi-set, right? Because we don't have a canonical order of them. So, together, basically, the structure, the symmetry structure of this new object of this collection of subgraphs is a product of two groups, right? If we don't know the correspondence, there will be a special type of product that allow me to skip the details. And basically, what we can do, we can design an architecture that does message passing on each of these subgraphs separately, but then fuses the information across graphs using these known correspondence. And this is probably more powerful than WL, a version of this architecture that we call subgraph union network. We can actually show that it's upper bounded by 3WL, and we hope that it will be equivalent to 3WL. But a few weeks after we published our paper, it was shown by a counter example that it is strictly less powerful than 3WL. So, we don't know exactly. It's surely more powerful than 2WL, but upper bounded by 3WL, and strictly less powerful. So, we have even a blog post about these different architectures, and there are multiple methods that are related. So, one of them is, for example, you can do dropout on your neighbors. That was done by a worker from the group of Roger Battenhofer at ETH, and they actually showed that this has similar effect. So, it increases the expressive power, not only gives some kind of robustness to the architecture. So, the last more expressive type of message passing in your network, so I would like to mention is what we can generally call topological message passing. And if you think of what is a graph, essentially, it's a set where you glue pairs of nodes together, right? So, every element in a set writer, every node in the graph is a zero-dimensional topological object, right? So, you can define this one-dimensional object, the edges that you glue to the nodes, right? And you get the graph, but you don't need to stop here. You can also define cells of higher dimension, right? That you forgot about, glue to cycles in your graph, right? And we get what is called a cellar or CWL complex, right? And basically, now, instead of traditional message passing in graph neural networks, where we exchange information between nodes along the edges, we can also go up and down in this hierarchy. So, we can do message passing within the same dimension, right, of the cellar complex, but we can also go across dimensions. And this hierarchical message passing is strictly more powerful than the vice-versa-lemon, and it's obviously very convenient for molecules, because in molecules, these structures have some chemical meaning, and this probably is closer to how chemist thinks of molecule, because, of course, the graph captures all the information, but it doesn't make certain structures explicit. And in graph neural networks, for example, well, first of all, you cannot even detect by message passing the presence of these structures. And if you want to transfer information from this node to this node, you will need to do a few steps of message passing. Here, we can do it at once. So, it also gives computational advantages. And again, if you want some more details about how these different methods are related to each other, so there are many more expressive architectures, so there is a tutorial that was given at the log conference, and recorded on YouTube by my PhD student, Publizio Frasca, with Beatrizio Bevilacqua and Gagai Maron. So, it's actually a very nice tutorial, and they go into much more details about all these and other different methods for expressive graph neural networks. Any questions so far? I have a question about summation aggregation being the maximally expressive aggregation. Also, probably it is related. Maybe if you can also comment on the, what was it, discrete, countable restriction on the features. Because if I imagine that our features are integers, there are different combinations of integers that sum up to the same number. So, summing them actually does lose the information. So, countable doesn't necessarily mean integers, but they should not be continuous. So, why this is, without going into too much details, basically they use the same proof technique that was used in deep nets to prove the universality there. So, this assumption is important. If you remove this assumption that this proof doesn't work. So, basically they apply locally kind of the result of deep net that works on sets. All right. Thanks. Hello. Very interesting. Thank you. I'm wondering about chemistry, whether you can encode in the features of your graphs, also geometric information. Especially in chemistry, aromaticity is very important. Whether it's possible to encode it directly or you need some additional layers of information. So, some information you can probably compute. And of course, if you can pre-comput it, if you know that these are meaningful features, then I think it makes sense to encode them. So, the geometric information, I'm not sure that you mean information that comes from the positions of the atoms, right? Yeah. So, I will talk about it in a second. So, you can, basically when you deal with geometric information, you also need to do it in a proper way. So, you need to do it in a way that is equivariant to possible transformations. But I think the short answer is yes. In case of these, like, increasing hierarchy of the KWL tests, is it a case that for a given KWL that we know there exists some message passing graph neural network that exists that can do it, but we just can't construct them? Or can we say that if we could make such a message passing GNN for such KWL, it would be, you know, intractably huge and we would need to approximate it or something like that? So, first of all, it is intractably huge. So, it complexes N to the power K. So, I think what is limited in practice is 3WL. So, this is what Maron describes in his paper. I don't know, depending whether you can call it message passing or not, it depends on what you consider message passing, right? So, because here you have more than pairs of nodes, I would argue that strictly speaking, it's not message passing, but for example, Petr Wieliszko, which would say that it's message passing on a different graph, right? So, it depends on the perspective. Maybe one more follow-up question for this KWL stuff. If you have a particular application that you are interested about, are there some cases where you can say, for this class of problems that we're working on, we can, it's enough to be able to distinguish up such a level because the higher you go for K, obviously, like these edge cases would get really nasty, which you might not see in your application. Well, so, it's a good question, right? So, for example, planar graphs have WL dimension of 3. So, basically, all planar graphs can be distinguished by 3WL test. And you can argue that molecules, right? Most of the molecules, you can draw two-dimensional structures, maybe some of them you don't, but they're probably a tiny fraction. So, do you need something more powerful than that? The expressive power itself is probably not the end of the story, right? Because nothing tells you about generalization. So, yeah, I don't think that this on its own is really the crucial consideration. It's good, of course, to have an architecture that is, that allows to distinguish between broader class of graphs, but if it comes at the expense of computational complexity, for example, maybe it's a bad idea. So, you probably want some kind of good trade-off between these. Thank you. I had a follow-up question. So, could you please define what message-passing is in this case? Can we give a short definition? Right. So, message-passing is what I call message-passing is these kind of architectures. Let's see where I had it. Yeah. So, basically, architecture of this kind. So, this is the most general type of message-passing. So, we have the update of node i from neighbor j is done in this way. So, I have a function that depends on both i and j, that the function is parametric. So, that's learnable and then aggregate. You can actually show that summation is what you need, right? You don't need anything else. So, well, this is, so this is message-passing. There are higher order architectures, right, like KGNN, right, equivalent to KWO. So, this is equivalent, in the best case, equivalent to WO. It is not equivalent. So, the more expressive WO tests, KWO tests are more expressive than these architecture, but then it doesn't work on pairs of nodes. It considers bigger sets of nodes, right? So, whether you, to call it message-passing or not, you can argue that, so some people argue that you can also think of it as message-passing, right, just with a different graph. In my opinion, it's more a semantic question. So, I don't know. We never really looked at it. Yeah, I don't have an answer. So, it seems like the topological message-passing networks and the graph substructure networks both work on the same phenomenon of identifying and counting substructures. So, is the expressivity of the topological message-passing lower bounded by the expressivity of the graph substructure network? No, it's slightly different, right, because substructure networks, well, first of all, we know they're upper bound, so 3WO. Topological message-passing depends on what kind of substructure. So, there, the kind of side information that you assume is what kind of substructures become cells in this, in this cellular complex. You can actually go beyond two-dimensional cells. You can go to high-dimensional cells. We never tried to go beyond two-dimensional cells. So, depending on the choice of these substructures, what becomes a cell, you might have different levels of expressivity. So, they're distinct methods. I don't think that they're really comparable. So, something that always confused me about the topological ones is where you, like, you glue stuff to structures in the graphs to, like, make them obvious. But can you, like, glue to every structure or does the structure need to be, like, closed? Because in previous works, it was always either cycles or cliques, which are kind of circularly closed. What, I think, basically, the key question is whether it defines a valid cellular complex. I think it should be closed. Yeah, my top, yeah, I also think it should be closed, but I have no idea about topology. Yeah, from what I remember, it should be closed. But, yeah, so it could be a clique, for example, whether it could be a path. I don't think so. So, actually, GSNs can, like, count structures that you couldn't, like, glue into, or, like, form into a cellular complex. So, GSNs can count more general structures, in my opinion. Yeah, so something that doesn't necessarily form a cellular complex. Okay, yes, thank you. Inge, I have a question. If we allow more and more notes, the probability of collision in this representation, do we have any results that it became less or, like, insignificant or, like, in more general, do this approach extend a bit more into randomized or, like, stochastic versions so that maybe the guarantee is very low right now from a sort of, well, secondary run. So, the expressiveness, like, theoretical is pretty low, but then, if we allow a little randomness, then, actually, like, no randomness. Sorry, you're talking about random graph models, right? Something like stochastic walk models. Yeah. You can probably analyze what happens to this kind of graphs given certain type of message passing architecture. I have seen papers of this kind, nothing specific that comes to my mind. Yeah, you can probably, I don't know, compute some probability under the assumption of certain distribution of input random graphs of distinguishing within them or not. Yeah. So, a question regarding the size of the graph in practice. We are talking about, like, in terms of maybe not count and each count, like, what to say in practice for this KWL? So, KWL doesn't scale well, right? So, if it's n to the power k, so also computational complexity versus space complexity, I don't think that you can, in practice, go beyond 3WL equivalent. Now, there are sparse versions of these architectures, so you can do slightly better, but I think they are mostly useful for proving theorems. So, basically, because you establish a link to the device for element hierarchy, then you can say that basically, if your network is equivalent to one of these methods, then you know how expressive it is. Thank you. Yeah. So, there was a paper sometime ago claiming that most of the graphs in the most common benchmarks can actually be distinguished by 1WL. Now, I don't know if you read about it. And my question is, now we've seen that even in those cases, more expressive GNNs still get better results. But then what's the reason? So, again, generalization could be one possibility, right, because you train on one set of graphs and then you test on another set of graphs. Then, double your tests, in general, they're designed for fish less graphs, right? So, they only consider the structure. So, how you treat fish is also important. Nice sense. I think we can stop here and we have, like, have an hour for a coffee break and then we resume afterwards. So, I will put the slide with the caffeine molecule that everybody likes. And then, yeah, basically, we stopped at this overview of different more expressive architectures. So, let's now talk about, guys, let's start. So, basically, the situation that we have with the expressive power of graph neural networks, right? So, on the one side, we have the WL hierarchy, right? So, increasingly, more powerful, more expressive graph isomorphism tests, right? And we can find analogies between certain GNN architectures to these tests. On the other hand, the assumption that we are given a graph and we do message passing on this graph might be restrictive in the sense that some graphs are not friendly for message passing. And in this case, you typically, what you would like to do is to change the graph so that message passing works better. This is a very broad category of methods that are called graph rewiring. So, what happens is that you have a gap between theory and practice, right? So, the theory, in order to make the link to the WL tests, you need to use exactly the same input graph. The practice tells you that sometimes you don't want to do it. So, as always, there is this gap. So, if you think maybe take a step back and look at the different types of graph neural network architectures, so the traditional approach in graph neural networks is you're given the input graph and it's both part of the input and part of the computation, right? Because you use the input graph to send information on it, right? So, it's both input and computational object. Now, as we've seen, you can do many different things, right? So, you can enrich the graph with some positional or structural features. You can lift it into a high-dimensional topological space like Simplisher or Cellar complex, right? And do message passing on this object. You can again enrich the graph by considering a collection of subgraphs, right? And do maybe some other more exotic type of aggregation that respects the product symmetry group. You can also enrich your representation by considering also the symmetry of the data, right? So, these are equivalent GNNs, if you have time, I can talk about it. And then maybe in some special cases, your graph will have special structure like a grid, right? And in this case, you can maybe do more specific choices. For example, you can abandon the local permutation invariance. So, you will get back to convolutions. So, basically, the common denominator of these approaches is that you have more structure, right? Sometimes you can assume, sometimes you can invent, right? But that's the idea. On the other hand, you can say that you don't like the graph that is given to you, right? And you choose to completely ignore it, right? So, you just assume empty edge sets. So, you're back to the bare bone, right? So, the object is just a collection of nodes, which is a set. And these are the architectures that are called deep sets or point nets. So, that's the simplest case of graphing electrics, right? Where you don't have a graph, you just have the nodes. The other extreme of this is when you allow interaction between every pair of nodes, right? Again, you don't trust your graph for some reason. So, every pair of nodes can interact. So, it's a complete or a fully connected graph. And this is how transformers work, right? So, in this case, you will use, for example, the attentional flavor of the graph in electrical architectures. And you will learn the right graph for your task, right? In a sense, through the attention mechanism, for example. There is another class of methods, and this is what we call graph rewiring, where you say, okay, I don't like the graph that is given. I would like to change it a little bit, maybe. And this way, the graph will become, in some sense, better for message passing. Okay, we'll talk about it in more details. So, let's talk about graph rewiring and specifically about transformers. So, in transformers, you assume that the graph is complete, right? So, every node is connected to every node. And if you try to apply a convolutional style GNN architecture, right, that depends on the coefficients of the, representing the structure of the graph, then, basically, here, the sum goes over all the nodes, and basically, this argument is equal for every node. So, it's not informative, right? You're not adding any information. So, you need to use at least an attentional architecture. And in this case, this is already, this already looks like a transformer. And you can think of attention weights as a kind of learned graph adjacency, right? That depends on this task that you're trying to solve. And this is a special case of GNNs. Now, of course, in natural language processing, many tasks that you want to solve do not require permutation invariance. You actually want them to be not permutation invariance. For example, you want to depend on the order of words in the sentence. So, this is typically achieved by adding positional encoding. So, in the simplest case, you just equip every node with some additional coordinates that tell you where you are located in the domain, right? Which in case of transformers, it's where you're located in a sequence. And typically, this is how positional encoding looks like, right? So, these are just some synosoids. So, for graphs, you can do other things, right? So, the analogy of synosoids would be the, yeah, question? Sir, are you aware of some studies about positional encoding, especially about relative positional encoding, which is getting some popularity in transformers architecture? So, like, the studies which consider all invariances in those positional encoding, like, we not always want to have absolute positional encoding. Sometimes we want to be, sometimes some shifts or rotations are not relevant. So, do you know some literature on that, perhaps? Yeah, so, relative positional encoding, yeah, good question. So, the analogy, let's say, of this standard global positional encoding would be the plus and eigenvectors. So, the analogy of local positional encoding would be something that, for example, the DGN architecture implemented, Gabriele Corso and others directed graph networks. So, what they do, they take, for example, the same plus and eigenvectors and compute their gradients on the edges and then transform these features in some way. So, this gives you a kind of local direction. So, that would be probably a good analogy of local positional encoding. So, with the plus and eigenvectors as well, you have some ambiguities. So, there has been actually a recent paper from Derek Lim and others from MIT where they are able to solve these ambiguities. You can use random walk kernels, you can use substructure counting, as we've seen, right? So, there are many ways of doing it. But basically, between these two extremes, right, whether ignoring the graph or learning the graph, of course, it comes at the expense of complexity, which is a huge problem in large language models where the size of the domain, right, the length of the text can be very large. So, probably one of the key computational questions would be how to compute these attention more efficiently. So, here somewhere in between comes the graph rewiring approaches. And with graph rewiring, it means that your computational graph is not equal to the input graph. And it's a little bit controversial topic because the graph being part of the input, somehow you don't want to change the input, right? So, but the fact that many architectures do it, right? So, if you have, for example, a very large graph like a social network where you have a lot of neighbors in some nodes, you cannot aggregate information from millions of nodes. So, you need to sample the neighbors, right? Which means that you are using a different graph from the input one. So, neighborhood sampling is a form of graph rewiring, right? You can also use graph neural networks where you bring information from not immediately your one-hop neighbors, but also from multiple hops away, right? So, this is also some form of graph rewiring. Transformers are also an extreme form of graph rewiring, right, where you allow access to all the nodes in the graph. Some kind of pre-processing, right, where you pre-wire the graph for example by some form of diffusion. Yeah, a question? So, how do you do the neighbor sampling? Is it just like a stochastic? Or do you... Yeah, usually you sample with repetition. So, this is what Sage did. Do you think it's optimal or is there a way to optimize it in the best way possible? I don't remember if they looked into it. Probably, of course, it might matter how you sample, but I don't have on top of my mind any significant result that would tell how to do it better. Could you elaborate more on this diffusion processes on the graphs you mentioned? Yeah, I will get to it. So, I will go through the diffusion equations and then I will talk about this as well. So, I'm specifically referring to Degel, the work of Stefan Gundem and his students. So, basically, bottom line is graph is not really any sacrosanct object and in practice, many architectures even without admitting it do some form of graph rewiring, right? So, you can also rewire the graph throughout the neural network, so it doesn't need to stay the same across all layers. And this has been shown efficiently, for example, in the context of our squashing where you can do maybe a few steps of standard message passing and then do message passing on a complete graph, right? So, like, what transformers do? And so, the argumentation usually is that the first steps capture somehow the structure of the graph, similar to Weisberg and Lemon, and then, basically, you accumulate this information as features. And there is an extreme example of this. I think it's called GRIT. So, this is from the group of field torrid Oxford and what they do is they use just something similar to heat kernel to encode the local structure of the graph and then just use MLP. So, there are other extremes like this, right? So, basically, you have just some kind of local feature of the graph that you can maybe make learnable, but then the graph itself is not used, so there is no message passing. And I think it's an open question of how much you want to use to capture the structure in the form of some features versus in the form of the computational architecture. So, the graph can be changed throughout the layers and, well, one of the first architectures to do it was what we did with Justin Solomon and his students, and that was considering problems in computer graphics. So, we have a point cloud, let's say, of this airplane. So, every point has three-dimensional Euclidean coordinates, and here the task is segmentation. So, you want to label each of the points on the airplane, whether they belong to the body, to the engines, to the wings, and so on. And here we create a graph to basically to represent the local structure of the data. And what is shown here by the colors is a distance in the feature space, right, in the latent, in the latent feature space of respective layers in this network to the rest of the points from the red point, right? So, initially, this is Euclidean, as you can see that, that's basically the input space, three-dimensional R3, but then as you go deeper, you see that it becomes more semantic. So, here, for example, points on the same, on the engine become closer, or on the wing become closer. So, basically, the space itself is used for the construction of the graph, and we call it dynamic graph neural networks, or maybe not very, very, very lucky name. I think many architectures are called like this. And we use it also in different incarnations or similar ideas under the name of differentiable graph module, where basically you have applications where you don't know the graph a priori. So, that's, for example, the case with medical imaging, where you have maybe nodes representing different patients, or maybe different regions in the brain, and you have basically two branches of graph neural network architecture, one that is computing the filters on the graph, and another one computing the graph itself. And, of course, there is question of computational complexity, but we're able to show that it is better to learn the graph for the, for the task rather than to construct it ad hoc in some kind of handcrafted way. Now, talking about message passing in general, so these are hecticly added slides based on some discussions that we had in the coffee break. So, if you think of message passing and different versions of it like transformers, basically, the difference is in two questions, is what's information to send, what's information to pass, and where to pass, whether you follow the graph, whether you pass information to your neighbors, or you pass information to distant nodes, whether it's in K-Hope or to all the neighbors in the graph, and the question of what is how you exactly transform your information. So, it's also interesting to add to this another question, is when to send information? And if you look at it, basically, it's a multi-step process. So, this is how a classical message passing neural network works. So, these are three layers of an NPNN or three iterations of, let's say, something similar to vice-fair and lemon, and I'm sending information from these nodes to these nodes, right? So, it takes three steps. So, first here, then these nodes propagate information to their neighbors, and then this green node receives information from both neighbors, right? So, in a transformer, all the information is available at once. So, at the same moment of time, I'm sending information from all the nodes to the green nodes, right? So, it has access to all the information across three layers. But you can also, so, basically, here, the difference is where to send information, right? That's the structure of the computational graph. But we don't consider here also when to send this information. And you can imagine an architecture where, for example, you delay the information. So, here, the first node in first iteration sends information to its neighbor. At the second layer, it sends to two hop neighbors, right? So, this information becomes gradually available. Not like in a transformer where you flood all the nodes with information from all other nodes at once. You make it progressive, right? So, basically, these are kind of shortcuts that you have in the graph, but they are made progressively as you go deeper into the architecture. You can also make, so, you can think of this as a kind of skip connections, but you can also make skip connections sparse in time. So, you can delay this information. And I'm saying that here, the information from the first node comes to the next node and is delayed in time, right? So, it would arrive to it anyway, but it would be entangled with the information in other nodes. So, I'm allowing direct access, but I'm delaying it. And this potentially has interesting implications also on the hardware aspect of graph neural networks, because if your graph is very large, you typically partition it into different parts of memory, right? And the cost of message passing is not the same, right? So, messages within the same memory cost much less than messages across different memories. So, you can imagine a graph neural network or you are doing fast messages within each part of memory hierarchy and slow messages across, right? So, you don't want to wait for all the messages to arrive. You might want to do fast messages while waiting for slow messages, right? So, this architecture that we call drill or end drill, this version with delays potentially allows for it. So, I think it would be interesting to test it on actually some practical hardware like graph core, for example. So, let me move to the next topic and this is physics-inspired graph neural network. So, I promised PDEs, so I need to hold this promise. And let's again take a step back and look at different objects that we've seen so far, right? And also, you can argue objects that are studied in this broader field of geometric deep learning. So, let's say grids, meshes and graphs, right? So, you can think of them as more or less the same thing, just with more structure, right? So, meshes in addition to have also triangles. So, these are simplicial complexes. So, graphs with some extra constraints or extra structure. Grids are also special type of graphs where we have certain organization. So, if you look at the grid and you look at how you aggregate information from your neighbors, there is no ambiguity, right? In a grid, unlike a general graph, I can order my neighbors in a canonical way, right? I have a top neighbor, I have a left neighbor, bottom and right, like shown here. So, it is totally unambiguous, right? So, this ordering is fixed. On a mesh, the situation is slightly different. So, I can pick up my first neighbor and then I can order all the rest of the nodes, all the rest of the neighbors in, for example, clockwise orientation. And this is possible because mesh is a discrete manifold. So, it's locally Euclidean. I have this meaningful ordering, right? But, of course, the choice of the first neighbor is ambiguous. So, I can rotate everything, right? So, the ambiguity here is up to rotation. In a graph, as we've seen, any permutation works, right? So, everything is defined up to a permutation. So, in a sense, graphs have the least structure out of all these objects, right? So, second observation is that if I look at grids and meshes, I can think of them as discretizations of some continuous spaces. So, a grid is discretization of a plane or a mesh is discretization of a two-dimensional surface or a manifold. We don't have immediately this analogy for graphs and even though there is an entire field that is called network geometry that tries to think of graphs as some discretization of continuous space, it would be nice to have some continuous models for GNNs, right? And that's the idea of what we call physics-inspired GNNs. So, you can consider some class of graph neural networks as a dynamic system. So, you have particles that live in the dimensional feature space. And let's assume that the dimensionality always is kept the same, right? It doesn't need to be in general graph neural networks. So, every node is some point, some particle that moves along a trajectory that is shown by this red line here. So, a GNN is essentially a dynamic system, right? That is governed by the system of differential equations. F here is some coupling functions, right? That makes dependency between different particles. And it depends both on the particles and the graph. And it's also parameterized by some trajectory of parameters that are, you know, by data. So, T is a continuous time parameter. I can discretize it and this corresponds to layers of a graph neural network. And the graph, right, you can think of it either as a coupling function, right? So, this F here, how different rows of this matrix interact with each other or discretization of some continuous space, as we'll see in the next few examples. So, basically as the evolution equation, right, that governs the behavior of these particles, I can put here more or less anything, right? So, there's plenty of different differential equations that describe different systems. But probably the first thing that comes to your mind when you think of propagation or diffusion of some stuff is the diffusion equation, right? And this, in fact, was one of the earliest mathematically analyzed physical phenomena, right? The diffusion of heat, analyzed by non-Elst and Newton himself in an anonymous paper written in Leighton in the transactions of the royal site. So, it's interesting actually that the journal had a mixture of papers written in English and Leighton. And it was anonymous. Well, Newton devised an experimental setup where he heated pieces of different objects, metal, I think, mostly, and then he looked with his self-made thermometer here, measured how much heat is lost over a period of time and devised some law that is formulated in modern terminology like this. What is nowadays called the Newton law of cooling, which says that the temperature that the hot body loses in a given time is proportional to the temperature difference between the object and the environment. He actually didn't use the term temperature. That's modern terminology. He used the term heat, which has a different meaning or color in Leighton. And somehow, everybody guessed that it was his paper, even though he didn't sign it. Now, it took some time until this process was fully understood. So, Fourier devised the local version differential equation that governs heat diffusion and then thick in the late 19th century defined what we nowadays understand as diffusion equations. So, if you want to apply this idea to a graph, we can consider a diffusion process on a graph. And that's how it looks like. So, here x is some quantity that is being diffused. Think of it as temperature if it's color. So, a node has certain temperature at time t. So, t is a continuous variable. And what is written here is the self-temperature of the node. This is the temperature of the environment. So, it's the average of the one hop neighborhood of the node. And this is the rate of temperature change. So, there might be some proportion coefficient here. So, if I rearrange the terms on this expression, I can write it like this. So, I'm slightly massaging the formula. What is written here is called the gradient. So, it's just the difference between end points of an inch. So, I take the feature here and the feature here and subtract one from another. So, that's the graph analogy of the standard gradient operator from classical calculus. And what is written here is, again, the discrete analogy of the divergence operator, again, from basic course in calculus. So, in terms of operators, you can think of the features as that they live in the nodes of the graph as a scalar field. Then the gradient makes a scalar field into a vector field, right, that lives on the edges of the graph. And then the divergence does the opposite. So, it collects the information from the edges that live, that emanate from a node and basically sums them up maybe with some different weights, right? So, that's what the divergence, right? So, gradient makes a scalar field into vector fields. Divergence makes vector fields into scalar fields. Acting together, they take a scalar field into a scalar field. The divergence of the gradient or minus divergence of the gradient in our notation is what is called the Laplacian operator, right? So, what is written here is the graph Laplacian operator and by definition, you see what it does. So, it compares you to your neighborhood, how different you are from the average of your neighbors. And this is really a very important operator. It comes everywhere in mathematical physics and from quantum mechanics to wave equations to diffusion equations, and particularly important here. So, what else is the heat equation? So, this is very simple, right? So, this is what we call homogeneous isotropic diffusion equation. Basically, heat propagates everywhere in the same way, right? What else heat equation is? It's an example of prototypical gradient flow. And gradient flow is a type of evolution equation, differential equation that looks like this. So, the step at every point of time is in the direction of the minus gradient of some energy, that you know here by E. And the energy that corresponds to the diffusion equation is what we call the Dirichlet energy. So, the Dirichlet energy, you can write it as a quadratic form with respect to the Laplacian operator. And it measures the smoothness of the node features, right? How different you are from your neighbors. So, the smallest value it can achieve is zero. In this case, all the features are the same, right? And you can show that Dirichlet energy decreases along the flow. So, if you run the diffusion to infinity, all will become constant, right? So, the heat will basically propagate on the domain and everything will have the same temperature. So, this is what we typically call over smoothing in graph neural networks, right? And I should say that over smoothing is not necessarily a bad phenomenon. So, usually it's described as a kind of catastrophe that happens in GNNs and prevents us from making deeper architectures. But it can actually be a very benign thing, right? So, you can imagine without any learning. So, if I give you a graph and I have labels of a few nodes, and if I assume that the structure of the graph is homophilic in the sense that my neighbors are expected to have labels similar to me, then I can just diffuse this information so it will be, you can think of it as a heat diffusion equation with boundary conditions. And by just doing this, it is very likely that the result will be very good, right? If the graph is not homophilic, of course, you need to do something else, maybe work harder, but on its own, the over smoothing is not necessarily bad. Yeah, there is a question. Actually, I have like two questions on this topic. So, first of all, can I see this as the, in some sense, the message passing algorithm there, but via the ordinary differential equation between the edges? Right. So, well, whether they call it message passing or not, I think it's a good question. So, every iteration when you discretize this differential equation, every step will correspond to message passing. Now, in some cases, you can actually have a closed form expression for the solution at any time t, right? This is called the heat kernel. So, it will look like the exponential of the Laplace matrix. So, I can compute the value of the temperature at every point instantaneously without doing these microsteps of message passing. So, whether they call, still call it message passing or not. So, this is where semantically I disagree with better, for example. He still suggests to call it message passing. I think it's not. So, I don't know how to call it better, maybe some kind of spatial coupling, right? Because you have, you have direct information to potentially infinite, all the nodes in the graph, right? So, it is something that doesn't require propagating information explicitly, right? You might be, when you solve the diffusion equation, especially with its own linear, you might not have a choice. But in some cases, you do. You have closed form expressions, right? Okay, thank you. And the second question, because I suppose that here is like the mostly ordinary differential equations. Is there any research to introduce stochasticity there to be, let's say stochastic differential equations, something like that, and when it could help in the, in the, in this field? Yeah. So, it's interesting. I think there have been some works. And there are some works. So, it talks for example, a big expert in this domain is Terry Lyons. So, I think they are working on, on these topics. So, we are not considering stochastic differential equations. We are considering whether to call them ODs or PDs. So, these are coupled ODs, right? When you discretize, a partial differential equation becomes a system of coupled ordinary differential equations. I think they, again here, the terminology is more semantic difference, whether you have a continuous spatial coordinate that you want to discretize. So, that will be the next part of it. Okay. Thank you. Right. Okay. So, that was basically, that was the gradient flow, right? And again, an example of, of the heat diffusion equation as, as a prototype of the gradient flow. So, if you look again at the, at this view on, on graph neural networks as some kind of evolution equations governing some, some physical system. So, the traditional approach to graph neural networks is to take this differential equation, discretize it, and then parameterize the evolution equation, right? The discrete evolution equation. So, every step of an iterative solver here will correspond to a layer, and then you parameterize every layer, right? So, that's how you can view graph neural networks. So, instead, what we can do, we can start with, with an energy, parameterize an energy, and then derive the evolution equation as a gradient flow. So, apparently, there is no difference, right? But the big difference would be that we will, it will allow us to make certain architectural choices. So, it will restrict our space of all the possible architectures that we can do here, right? Like, for example, the form of message passing. That will have better interpretability. And of course, I know that interpretability is maybe a bad word in machine learning. So, what I mean here is that we will be able to guarantee that certain things happen or not happen. Okay. And I will be more specific in a second. So, let's consider the following parametric energy. So, we call it generalized Dirichlet energy. It has these two terms. So, you can think of it as an energy that a system of particles has, and it's parameterized by two matrices of size d by d, right? d, remind you, that's the dimensionality of the features. So, that's the space where the particles move. And we have two terms here. So, the external energy term, it acts on all the particles. So, think of some force that moves them in some directions. And we have internal energy. So, these are the interactions between particles along the edges of the graph, right? Think of maybe colonic interactions, right? Or maybe springs that attached to some pairs of particles. And we have two types of interactions. So, we have attractive interactions, and they happen along the eigenvectors of the matrix W that corresponds to positive eigenvalues and repulsive interactions that happen along negative eigenvectors. And you can see an example here. So, here the space is two-dimensional just for visualization purposes. And the graph here, you see that it's heterophilic. So, the colors of the nodes represent the labels. And the positions represent the features, right? The feature coordinates x and y. So, the graph is actually perfectly heterophilic, right? The blue nodes have only red neighbors, and the red nodes have only blue neighbors. And yet, we can find directions. So, the horizontal direction is repulsive direction, where the particles are separated. And the vertical direction is attractive direction, where the particles cluster together. We can perfectly separate these types of nodes, right? So, if my task is node classification on this graph, by this very simple process, I can solve this problem, right? So, we will see in a second that this corresponds to a convolutional architecture. By convolutional, I mean that my diffusion operator depends only on the structure of the graph, but not on the features, like in more complicated, for example, attentional networks. So, I can write it as AX something, right? So, this is what was our distinction. So, in convolutional architectures, we had something like this. In attentional architectures, we had something like this, right? So, that would be the GCN type of architectures, and this will be the GUT type of architectures. So, the fact that this matrix is constant, it depends only on the structure of the graph, is what I call convolutional architectures, right? So, basically, these goals against this folklore that I mentioned in the beginning, that convolutional architectures are not good for heterophilic graphs. So, you can see that they can work very well, right? So, we need to dive deeper and understand what happens here. So, if we write the gradient flow of, yeah, question. Is there a link between this gradient flow and less and less popular recently, probabilistic graphical method, especially if you showed the previous slide, it was very similar to restricted Boltzmann, right, when we had this bipartite graph? Yeah, you can probably interpret it from, right? So, gradient flows are very basic objects. So, it's essentially steepest descent is also a gradient flow, right? So, it's a continuous version or a variational version of it. Yeah, so, probably there are links to many different things. So, basically, you're minimizing some energy here, right? So, and this energy, so, it's not the learning part, right? So, it's the inference. So, applying a neural network minimizes some energy, right? You design the network, so, it minimizes some energy, right? And the energy is parametric. So, what kind of energy to minimize is what you determine based on the task. So, that's what is done by back propagation. So, basically, the gradient flow, you just differentiate this energy with respect to its parameters, right? By data I denote here, these two matrices omega and w. And one thing that you notice immediately that they all appear in symmetrized form, right? So, they appear in quadratic terms. So, they appear as omega plus omega transpose. So, we can just assume that they are symmetric to start with, right? So, that's already first restriction that comes from this assumption of the gradient flow. The second thing is, again, this is by the design of the energy that the parameters are time-independent, right? So, they don't depend on t, right? Which will become the layer number. And if we discretize it, this is how we discretize. So, we replace the temporal derivative with forward difference. So, this is what is also called the explicit Euler scheme. So, we have some step-sized tau. And this gives residual convolutional type GCN, again, the convolutional type GNN. Why I call it convolutional? Because this matrix A, right, that's where the diffusion happens, where the message passing happens, right? When I send information across adjacent nodes, it's not dependent on x. It's fixed, right? So, you can call it a kind of a convolution. The weights are symmetric and the weights are shared across different layers, okay? So, the way that you can use it, you can optionally do some nonlinear encoding typically to reduce the dimensionality. So, we have some fixed low dimensional dimension of this space. You apply a linear gradient flow. So, all the propagation of information on the graph is linear. So, there are no nonlinear activations, right? And then you do some decoder. That's for node classification, right? So, that's how the architecture looks like. Now, if you compare it to the classical convolutional architectures like GCN of Kieff and Welling, so there are several differences. So, the GCN is non-residual, the weights are non-symmetric, and the weights are different per layer, right? And also, they use nonlinear activation for every layer. We don't. So, in a sense, it's a kind of antithesis to a typical graph neural network or a typical deep learning architecture. So, typically, you say that you need many layers, each layer parameterized separately and nonlinear are they activated. We don't have anything like this here, right? So, all the, again, the diffusion part is linear. We have a nonlinear decoder and potentially a nonlinear encoder. So, what we gain from the it being gradient flow is interpretability in the following sense that we can show that in certain situations we can induce both low and high frequency dominated dynamics. I will define it in a second. And what it means is that we can work both with homophilic and heterophilic graphs. Now, because the weights are shared across layers, actually the number of layers becomes a completely irrelevant notion here. So, what matters is really the diffusion time. The number of layers is just how finely I discretize my differential equation. And the number of parameters is independent of it, right? Unlike, again, the classical architecture where the more layers you have, the more parameters you have. Yeah, question? Question. Why don't you stay in the latent space? Why do you have needed a decoder? Why do I need a decoder? Well, the decoder can, for example, you might need it to produce a label for an old. It's not the decoder in the original space. It could be in any, it could be also decoding the original space, right? So, you might, for example, want to work with low dimensional space and then the number of your classes might be different, right? But the important part is that it's nonlinear. So, all the nonlinearity goes there. Okay. And, well, you can also use an encoder. I will show why encoder can be problematic. We actually, we have experimental evidence that it's not needed. So, first of all, let's talk about homophilic and heterophilic graphs. So, again, homophilic look like this, heterophilic look like this. And the data set here is what we call synthetic core. So, it's probably you're all familiar with it. So, it's this citation network where we can actually change the structure of the graph in a way that it becomes either more homophilic or more heterophilic, right? And here are the two extreme choices of an architecture. So, the task here is no classification. I can either ignore the structure of the graph altogether and do just no wise predictions with a multilayer perceptron. And, of course, it's not great, right? So, it achieves about 67% accuracy. But no matter how homophilic or heterophilic data set is, the result is the same because it simply ignores the graph, right? The other extreme is a GCN. So, when the graph is homophilic, it works extremely well, right? Almost 100% because when my neighbors contain similar information, I will be basically averaging them and I will be doing some form of denoising. But when the graph is heterophilic, then it degrades very badly because, in this case, the neighbor information is more detrimental than helpful. So, the gradient flow framework, it benefits from basically its kind of mixture of both worlds. So, it works as well as GCN in the homophilic case and as well as node-wise predictions in the heterophilic case and the graceful transitions between the two. Now, another important thing, right? And that's where the encoder and decoder question comes into the play. So, if you write the output of the neural network after L layers, this is how it looks like. So, it's basically it's a polynomial in these matrices. So, the matrices are dependent, of course. So, there are some powers here. The parameters are shared. But if we ignore the encoder, right, and we can ignore it, at least in the test that we did, basically what the diffusion part can be precomputed, right? You see that it acts as some kind of powers of the diffusion matrix on the features. So, I can pre-diffuse the features once as a pre-computation step and then it all boils down to node-wise multiplications by these matrices. So, the computationally difficult part, both computation in terms of the number of multiplication operations as well as the memory access, which, unless the graph is very structured, you have random access to your neighbor nodes, this is really the heaviest part of graph neural networks. So, this now can be totally trivialized. So, we've done the earlier versions of this architecture with just one convolutional layer. We call it sine. So, already several years ago, we tested this on the graphs of hundreds of millions of nodes. So, with this architecture, actually, because now we also get rid of the non-linearity you can apply it to a very large graph, basically, you can design multi-layer architectures that work well both in homophilic and heterophilic settings. Now, another thing, and this is what I mentioned regarding the nature of the dynamics that is induced by this gradient flow, is if we look at our data, right, and our data, I remind you, it's the matrix x, right, which is of size n by d and is the number of nodes, d is the number of dimensions. So, we can do an analogy of a two-dimensional Fourier transform, right. I remind you that for a graph that we assumed, an undirected graph, the graph of plus n has orthogonal identity composition. So, it's eigenvectors form an orthogonal basis, right, the basis for the rows of the matrix, and the matrix w, which we assumed by virtue of our process being a gradient flow, right, it was symmetric, we can have also an orthogonal identity composition, let's call it eigenvectors psi and eigenvalues mu, so it forms the orthogonal basis for the columns of these matrix, right, for the dimension d of these matrix, and now in these two-dimensional Fourier basis, right, we can take tensor products of these basis functions of phi and psi, we can write the output of the neural network like this, right, and what you see here is that we have some filter that acts on our signal, right, so this is signal, so these are tensor products, right, so that's the analogy of two-dimensional Fourier transform, sinusoids of sine mx by sine ny, something like this, right, so that's our analogy of this, so this is a filter, and it works both with the frequencies, the eigenvalues of the graph Laplacian and the matrix w, right, lambda and mu, and you see that the low frequencies of the graphs are magnified by the positive eigenvalues of w, and conversely the high frequencies of the graph are magnified by the negative eigenvalues of w, and you can show that if we choose the matrix w in such a way that it has sufficiently negative eigenvalues, then this gradient flow dynamics is high frequency dominant in the sense that the Dirichlet energy or more correctly the normalized Dirichlet energy doesn't converge to zero, so it means that we don't have over smoothing, right, in the case of over smoothing this thing would be going to zero, but here it doesn't, right, so it means that we have some process that doesn't diffuse everything to a constant, it does something more interesting, right, and the condition for it is w having sufficiently large negative eigenvalues, so the analogy of this would be, so if you think of diffusion processes blurring, what we have here is sharpening, and we have both processes at the same time, so the attractive interactions do blurring, the repulsive interactions do sharpening, so we have some directions in the fissure space where these processes happen at the same time, okay, questions? Yep, I think there are many questions, so it was very unclear. So in these situations where you don't dissipate to a constant value, are you obtaining some sort of chaotic behavior, or are you obtaining other periodic oscillations of? So this is a sympathetic analysis, we don't know, I don't think that I have an answer to your question, it might be that it's, so whether we have monotonicity, that's the question, I don't think so, but it might be the case. So if eigenvalues of w are sufficiently negative, then we can just kind of beat over squashing, over smoothing, and the question is, is it possible, is it hard to do so because there are two dynamics, is it hard to get those dynamics right, does it require a lot of hyperparameter tuning or something like that? So it's a good question, so you can force, you can structure the matrix or parameterize the matrix w in such a way that it has two parts, positive eigenvalues and negative eigenvalues, and that's what we do, so basically we help the architecture to have both, and then learning becomes easy. To learn it completely from scratch might be difficult, and we see that GCNs which are in principle the same architectures without this restriction often fail to do it. My question went the same direction, but after the previous answer I got more confused. So those, the matrices which, what's it, the delta and the w, they are learned, right, they are not hyperparameters. W, the elements of the matrix w are learned, right, but the matrix w has constraints, right, so it's, for example, it's symmetric, right, so it has half of the elements of a general matrix w, and then basically by virtue of this theorem what it suggests is that we need to further restrict the structure of w, for example, to make it have negative eigenvalues. So typically what we do, we decompose it into a positive symmetric part and a negative symmetric part, and basically this way we guarantee that it has both positive and negative eigenvalues. Okay, so what is the principle component that makes graph work on heterophilic graphs better than GCNs, and can you somehow update GCNs in a way that they will also work on heterophilic graphs? Yeah, so residual connection is a must. We actually show that without residual connection this doesn't happen. Well, the nonlinear activation, I think it's more a complication for the analysis. We don't know how to analyze this, basically with this nonlinearity you cannot regard it as a diffusion equation. So, yeah, basically residual connection and then nonlinear eigenvalues. If you, and remove the nonlinearity, whether, if you don't constrain w to have non-negative, or if you don't help the architecture to learn w with negative eigenvalues, you might never be able to learn it. So we've seen this happening as well. Okay, and what is the role of symmetric matrices, symmetric matrices w? So, symmetric comes from the assumption of gradient flow. So, anything that looks like a gradient flow for this kind of energy must be symmetric. I see, and what does it mean in terms of operations on your graph, in terms of message passing? So, this is not related to message passing because w is channel mixing matrix attacks on the fissures. I was, I was wondering, what is the relationship? So, for example, you have this gradient flow and say you want to learn some energy with respect to which you flow. What's the relationship between learning the energy and learning a metric to which with you are computing the gradient? Because if you change the metric, then the way in which you flow is also different, right? Is other equivalent in the learning a metric equivalent to learning an energy? Yeah, well, you can think of it indeed in this way, right? So, what is the interpretation, physical interpretation of of this matrix w? So, if you look at the way that Dirichlet energy looks, right, it looks like, right? So, it's something like this. So, do we use, yeah, so let's say continuous version of Dirichlet energy. So, it will be the norm of the gradient of x at some coordinate u, let's call it. So, u would be the index of the null, right, the continuous version, right? And this is, this is the Dirichlet energy and let's write it on some domain omega, right? So, that would be our continuous version of Dirichlet energy. Now, what is written here when omega in general is a manifold, a remaining manifold. So, what is written here is the remaining metric of this kind, right? I hope you can see it. So, basically it's inner product defined at the position u, right? And the way that you can write it, so you can write it using remaining metric tensor, which is exactly the w, right? So, this is something that scales the coordinates of x, doesn't need to be fixed, by the way. This w in general can be position dependent, right, on the remaining manifold. So, a more general construction would allow w to depend on the position, maybe not explicitly, because that would be a huge number of parameters that scales with n, maybe it will be done some, through some form of attention. So, w will, or maybe a positional encoding, right? So, w will be a function of positional encoding of the nodes of the graph, right? So, there are interesting analogies and potential directions of extending this, using basically this is a harmonic energy of an embedding of some manifold, right? Thanks. Yeah, sorry, this will be probably a little bit like far-fetched question, but like in general, like those equations seem to be, seem to be like kind of similar to what you often get when you analyze like the signal propagation or the type of initializations in the neural networks, or like the dynamical isometry property. So, for example, the requirement for residual connections also appears in there. And like, are you aware like whether there's like any connections between, between, between this work and, yeah. Potentially. So, the closest analogy is neural ODE's, right? Well, these are neural, we like to call them neural PD's, but they're coupled ODE's, right? So, neural ODE's, each row of these magics will be separate, independent. Here, we also have the extra complexities coupling, right? So, like, would you say like this, there's something universal in all those? I'm not sure what do I mean by universal. Yeah, I'm not sure either, but like, it's basically like, like, for example, like this residual rule appears very often in different types of. So, residual rules rule here comes just from discretization. So, that's how you, you discretized the temporal derivative. You could discretize it differently. So, you can use a backward scheme, right? And then it would be implicit. So, you will need to solve a linear system to, to get your next iteration. This actually has been done with diffusion equations. So, there are advantages to it because these kind of discretizations are what's called unconditional stable. But in terms of, well, universality may be not the right term, but basically what you have here in practice is a kind of controlled differential equation, right? So, the control is through the privateers W. So, they're time independent, but in principle, you can think of time dependent trajectory. So, you have a controlled PDE that you discretize, and then expressive power becomes a question of, can I reach a certain state of the system, for example, in finite time by choosing the right trajectory? Or how far can I be from, from that state, right? So, universal approximation means that in finite time I can reach, I can be epsilon close to any state that they want. Generalization, for example, can be probably formulated as some kind of perturbation, right? So, I change my initial conditions. I want to see what happens to the system. And there are actually, there are some results, theoretical results that show that architectural choices, like for example, having symmetric matrices might be crucial for these properties. So, I think there is a lot more to explore there. Okay, thanks. Okay. So, more questions? Yeah. So, let's, let's move on with this stuff. So, obviously, right, so here, the conclusion was that we have no over smoothing, but we can also consider more interesting equations. So, so far we consider the very simple isotropic homogeneous diffusion equation. We can also consider nonlinear versions of the diffusion equation. And this one, in particular, comes from the domain of image processing, where imagine that you start with an image like this, right? So, the portrait of Sir Isaac Newton that is noisy. So, if you run a diffusion equation on an image, it actually has a closed form solution. So, it's convolution with the Gaussian kernel, where the variance of the Gaussian is proportional to the time of the diffusion, right? And in the limit, you will have just everything flat, right? So, you average all the pixels in the image. You see that you don't want to have results like this, because it might average out the noise, but it also destroys the discontinuities in the image that, for visual perception, are very important. So, the idea of, that was originally by Peron and Malik in 1990 is to have a nonlinear diffusion equation that is controlled by the gradient of the image, right? So, basically, if you're in a smooth region in the image, like here, so you have standard Gaussian kernel, but the moment you reach a discontinuity, you slow down the diffusion. So, the effect it has, you don't average pixels of different intensity. So, here, I'm not averaging dark and white, right? So, the kernel will look like this. It will look one-sided. And it had a lot of different versions, bilateral filters, non-local means filters and so on, but the idea is always the same. So, here, basically, the diffusion speed, right, is inversely proportional to the edge indicators, to the norm of the gradient. And the result that it produces is like this. So, this nonlinear diffusion equation knows where to stop locally, and therefore, it averages within smooth regions, and it doesn't average across regions. Now, we can do the same thing on a graph, obviously. So, the analogy would be this, right? So, here, we have a gradient, right? This is the divergence, and that's some parametric function that looks suspiciously like a tension, and that's the diffusivity, right? So, it's the local strength of the diffusion. And, in fact, if we discretize it again with explicit forward Euler scheme, then a particular version of this equation corresponds to the attentional architecture. So, this is a gut. But this was so far, this was a continuous time, right? And we wanted continuous space. So, the original motivation, right, when we compared graphs to other objects was somehow to have a continuous analogy of the graph neural networks, and if, again, we take a step back and look at how diffusion equations work in the plane, when I discretize the plane as a grid, I don't really have a canonical graph, right, in the sense that there are many ways I can discretize my differential approaches, right? So, this is how I can discretize the Laplace-Anon grid. So, I can use neighbors like this, or I can rotate everything by 45 degrees, I can use distant neighbors, I can use convex combination of all these operations, right, because this is a linear operator. So, bottom line on a grid, I don't have a canonical graph, right? I can actually use a discretization, maybe that is different at different points in the grid. Of course, there will be some numerical implications, but the discretization that we choose, right, and as a result, how the nodes are connected and which nodes propagate information to which nodes is, to a large extent, a numerical convenience, right? What makes sense, for example, from the organization of the memory or the number of nodes and whatever. So, we would like somehow to extend this mindset to general graphs. And for these purposes, instead of considering these nonlinear diffusion equations, like Perron and Balig, by the way, they called it an isotropic diffusion, which obviously, if you're familiar with PDEs, it's not an isotropic, it's not homogeneous, right, because we have a scalar diffusivity function and isotropic diffusion, we also have direction, so that would be a matrix or a tensor. So, instead of considering this nonlinear diffusion equation, we can consider a non-Euclidean diffusion equation. And the model here is the following, that was actually done by my PhD advisor, Ron Kimmel, also in the 90s, about 25 years ago, maybe even more. So, again, thinking of an image, you can think of it as an embedded two-dimensional manifold, right? And the embedding is in this joint space, where we have a combination of positional coordinates, the x, y coordinates of the pixels, and the fissure coordinates, in this case, for example, R, G and B channels. So, a color image is a two-dimensional surface in R5, right, using this model. Now, by virtue of this embedding, we can define a metric, so we can use the standard pullback mechanism, so in the case of two-dimensional manifold, it's a two-by-two matrix, given like this, right? And we can define a Laplacian with respect to this metric, it's a non-Euclidean analogy of the Laplacian, called the Laplace-Beltrami operator. And we can write a diffusion equation with respect to this operator, it's called the Beltrami flow, and we can actually show that it's a gradient flow of a generalization of the Dirichlet energy that is called the polycop functional. It's used in high-energy physics in bosonic strings, don't ask me what it is, but that's something is described by this energy. So, by analogy, we can do something like this on a graph, so now every node in the graph, in addition to having the fissure coordinates, also has some positional coordinates, right, so positional encoding. Ideally, this positional encoding should somehow represent the structure of the graph, right, in the sense that nearby points in this u-component of the space should be more likely connected by an edge, right? And the Beltrami flow, basically, it evolves, so we have again here a parametric diffusivity, it evolves both components, right, that I collectively denote by z, and the evolution of x-component is the standard fissure diffusion. You can think of the evolution of u as some form of soft graph rewiring, because what I can do, if two nodes become closer in this u-coordinate, I can decide to create an edge between them if there is no edge, or if the drift apart, I can decide to cut the edge, so overall, I will facilitate the propagation of information, and I know that it sounds cumbersome, but this is how it will look like, so again, this is the core graph, so it has, basically, there are three things happening here, so the positions of the circles, right, so circles are nodes, their positions represent some two-dimensional positional coordinates, the colors represent three-dimensional projection of the fissures, and you see that they're both components are evolving, and the graph is also changed on the fly, right, so when the clusters drift apart, then we cut the edges between them, so right, so it's fissure diffusion, positional coordinates are changing, and the graph is rewired, and you can see that the task here is node classification, so there are clearly seven classes of nodes that we can clearly distinguish here. Now, if you think of it from the standpoint of signal processing, we have a very disturbing picture here, right, so we have a filter that happens on the domain, yeah, a question? Maybe before you move to the next one, I was just wondering because I think what started to appear in this intellectual is that you apply some ideas from differential geometry to graphs, and maybe not yet directly, but... Not yet, right, so... Okay, so we are like talking about metrics and this sort of, so I wonder, maybe you'll be talking about it next, so sorry if I'm pushing it forward, but what do you think are the limits which come from the fact that graphs are basically discrete structures, like how free are we to apply ideas? Yeah, give me a few minutes and I will get there, yeah, so you can find the analogies, right, not everything has exactly a correspondence, right, so, but these analogies, I hope to show that they can be quite useful, right, so, but again, if you look at these pictures, so we have some kind of disturbing picture here, so we have a filter on the domain, and the domain is changing under our feet, right, so imagine that you're applying some filter, right, which is what it is, right, the diffusion equation, you can think of it as a form of filter, low pass filter, and the domain is moving, so I'm doing a filter and then nodes are somehow moving away from me, but this is a very common picture in differential geometry actually, and it's very common to take a manifold and evolve it under some evolution equation, and typically what, when you evolve a manifold, you're interested in what happens to the metric, right, so here's an example of an evolution equation that is called the Ricci flow, so you take the first order derivative, the temporal derivative of the metric tensor of the manifold, right, denoted here by g, and you make it equal to the Ricci curvature tensor, right, so basically the metric evolves proportionally to the to the local curvature, so it looks very much like the diffusion equation, so here we have temporal derivative, here we have some second order differential quantity that looks kind of like our Laplacian, right, so structurally it's similar to the diffusion equation, of course what it does is a very different thing, and if you start with this manifold, which has positive curvature on, so this kind of dumbbells on the spheres, it has positive curvature, right, and the neck between them, it has negative curvature, so if you run this diffusion, if you run the Ricci flow backwards in time, what will happen is that this dumbbell become more like an ellipsoid than more like a sphere, and then will collapse into a point, right, and it was introduced by Richard Hamilton in the 80s with the purpose of proving a famous conjecture in topology that claims that you can characterize spheres by your ability to take a closed curve and collapse it into a point, right, so this is how we characterize two-dimensional sphere, I can take any closed curve on a sphere and I can evolve it and collapse into a point, right, I cannot do it on a torus, so if I have a torus and I have a curve like this, then no matter what I do it cannot be collapsed to a point, so the conjecture was that you can, you can characterize higher-dimensional spheres in this way, and you obviously heard about it, the Poincar\u00e9 conjecture, and it was shown by Perlman, actually a slightly more general result, using the mechanism of Ricci flows, right, and that was a breakthrough of the century, it stood open for more than 100 years. Now, what does it have to do with our graphs and graph neural networks, so I remind you that we had this phenomenon, right, that message passing might not be, might not work well on some graphs, right, so there might be some, some phenomena that some graphs might be unfriendly for, for message passing, and in particular it depends both on the structure of the graph and the task, and if my task requires to propagate information from distant nodes, and the structure of the graph is such that the receptive field of the graph neural network grows exponentially fast, right, so the number of the neighbors of the neighbors of the neighbors becomes very large very quickly, this happens in trees, this happens in what is called small world graphs like social networks, then we have a problem, we have a lot of information that we need to squeeze into a single feature vector, and this is a phenomenon that we call overscorching, so let's, let's define mathematically what we mean by overscorching, so let's say that we have a message passing architecture of this form, right, so we have the node itself at layer k, and we have the neighbors, we combine them with some learnable weights, let's call them w1 and w2, let's say that the depth of this architecture is l, the width, right, so the internal dimension is p, the long linearities are well behaved, right, so the elliptics continues, and we also have some bound on the, on the weights, so this is what characterizes our architecture, so what is overscorching, it's some form of insensitivity, right, so if I look at the output of the neural network at node i and I examine how it depends on the input at some distant node j, I can describe the sensitivity of the output to the input through this Jacobian, right, so through the partial derivative, and if the partial derivative is small it means that the information propagates badly from input to output, right, so basically I will not perceive the change in the input in the output of that node, and what we show in the paper is that we can bound the Jacobian by constants that depend on the model, right, for example the number of flares, the regularity of the activation functions, the bound on the weights, and also something that depends on the graph topology, right, and we show in particular, for example, that width does help, of course, at the usual expense of worst generalization overfitting, depth doesn't help, for example, and the topology has the really the largest effect, and intuitively we expect that in some kind of benign graphs, like grids, for example, we will not have overscorching, and in pathological examples, like trees, that would be probably the worst case, right, so you see that the topology of the graph comes here through the power of the adjacency matrix, but we don't see exactly how, right, so it's hard to say, right, so what does it mean a matrix to the power l, so we need something more nuanced, we need some kind of geometric analysis that will allow us to tell apart structures like this and structures like this, right, something that locally looks like a grid or something that looks like a tree, that's exactly what curvature is designed for, right, so I remind you that in differential geometry, what curvature tells you is that if you take nearby points and shoot geodesics in parallel at the same speed, you can either converge, remain parallel, or diverge, right, and we call these spherical, euclidean, and hyperbolic geometry, right, so locally it looks like a sphere, like a plane, or like a hyperboloid, or high-dimensional cases as well, so on a graph, the analogy could look like this, so there are several definitions of reach-type curvature on graphs, so this is a combinatorial definition that we use here, so you can take nodes that are connected by an edge, let's call them p and q, and look at edges that emanate from these nodes, so if they tend to form triangles, it means that we look at something like a click, if they form rectangles, they will look at something like a grid, and if they drift apart and don't form anything, then we look at locally at something that looks like a tree, right, and basically we can count different types of rectangles and triangles, allow me to skip the details, basically for every edge in the graph we can have this combinatorial quantity that we call the balanced formant curvature, that counts, basically it looks at a two-hop neighborhood of an edge, and it counts certain types of rectangles and triangles that surround this edge, bottom line, each reproduces the continuous behavior, so clicks are positively curved, grids have zero curvature, and trees are negatively curved, right, so that's I think to your previous question how what is the parallel between differential geometry and graphs, so this is an analogy of a curvature, so it's not a discretization of a curvature, it's a discrete curvature that behaves in a similar way, and now the relation between the over-squashing and the curvature, what we show is that if we have strongly negatively curved edges in the in the graph, then we can write down this bound on the Jacobian, and it means that the over-squashing is caused by the presence of strongly negatively curved edges, yeah. Yeah, so it's the number of triangles that surround an edge, and this is the number of rectangles, this is the degree, yeah, doesn't really matter, there are several definitions, so why we call it balanced form and curvature, because there is a classical notion of form and curvature that looks a little bit like this, we just touch it a little bit so it behaves like what is shown here. This really relates to the rigid curvature tensor and the formula from matrix, I mean I don't see it now, but maybe. It's a graph, so you don't have exactly the same thing. Yeah, obviously, but some I don't know components, can we do some analogy or not, this is something completely new. So it shows how, so you can think of curvature as how locally the volume changes, so in a sense it shows how the volume changes, so there are two classical definitions of curvature on graphs, one through optimal transport that is called the Olivia curvature, and this is combinatorial version that is called the the form and curvature. Right, so basically the conclusion, well I should make it explicit that it's strongly negatively curved edges that cause overscorching, right, so basically due to this bound, actually the presence of negative or slightly negative curvature might be benign, this is what is shown in the the expander's paper by Petr Wieliczkiewicz and his and his causes, so these are somehow the optimal graphs, the best for message passing, but expander's needs to be slightly negatively curved. So once we know it, we can actually interfere basically, we can surgically remove the negatively curved edges and replace them potentially with edges with higher, with more positive curvature, and this way we retouch the graph a little bit and we show that it improves the performance of graph neural networks both in homophilic and heterophilic settings. So there was a question about diffusion-based rewiring before, and I promised to tell exactly what I mean by this, so this is the paper that is called DIGL by Stefan G\u00fcnemann and his students from the Technical University of Munich, and DIGL stands for Diffusion Improves Graph Learning. So the idea there is that you rewire the graph by basically by computing page rank and embeddings for a personalized page rank embedding for every node, and you connect then in this new embedding space the nodes that are closest. So what it does essentially, it introduces connections within the same connected component in the graph right, or within the same clique or cluster in the graph, and it has a hard time to connect across different communities in the graph. So when the graph is homophilic, this is a very good thing to do, right, so you're connecting to to similar nodes, but if the graph is heterophilic, it can do more harm than help, and in fact experiments show that this is the case, they also write it in the paper, the curvature-based approach, first of all it changes the graph minimally, here the change can be dramatic, right, so that's the number of edges that are changed, but it also helps in the heterophilic cases because it's not restricted by this property, you actually typically bridge different communities by the new edges that are created. So still talking about diffusion, am I actually out of time? Okay, yeah, so still talking about about diffusion equations, here are some more exotic exotic stuff, right, and this is our maybe creative way to illustrate to illustrate sheaves or bundles, so there has recently been probably a better picture, so that's really sheaves, right, in the literal sense, so what are sheaves? So they actually have very interesting history, and well I like these kind of historical factoids, so the theory of sheaves in algebraic topology was introduced by Jean Lyret, so he was a French mathematician, he was also an officer in the French army, and when the Nazis invaded France he was captured and put with his comrades into concentration camp, and basically he was asked to work on mathematics, and his expertise was mechanics, so he was very afraid that he would be forced to work on something that would be useful for the Nazis, and basically he will be committing treason, helping the war effort, so when he was offered the possibility to teach something in this camp, he chose a very innocuous topic, algebraic topology, which could be useful, and then after, well of course they were released after the war ended, he published it in a course that was taught in captivity, and one of the papers that came out of this course introduced the theory of sheaves, so sheaves are objects that are taught in algebraic topology, so if we apply them to our setting to graph, and this is slightly different construction that is called cellar sheaves, so if you think of graphs by analogy to manifold, so a manifold is a topological space, what I mean by topological space roughly is that you have a notion of neighborhood, I can tell who my neighbors are, but I don't have the notion of distances or angles, so if I want to talk about distances or angles, I need some extra machinery, and on many folds this is typically achieved by what is called an affine connection, or parallel transport, so it's a mechanism that tells me how to move vectors between tangent spaces at nearby points, I can also define a remaining metric if I want to equip a manifold with geometry, and then there is a special type of connection that is called the Levy-Civita connection that is compatible with the metric, so you can think of the same thing on a graph, so a graph is a purely topological object, I have a notion of who my neighbors are, but I don't have any geometry, so in order to introduce geometry I can equip every node and every edge with a vector space, and I can define by analogy to parallel transport, I can define linear maps, so these are called restriction maps that go between these spaces, so slightly different from many folds, I go from the space associated with nodes to a space associated with an edge, and then if I want to transport information from a node to a node, I need to combine these two maps, one with transpose, so basically it's a kind of, so I invent geometry on a graph, so I lift it into a more complicated object, and on this object I can now study, for example, what happens if I choose these restriction maps to be of certain class, so these are matrices of certain dimension, and I can choose them, for example, to be symmetric, or I want them to be orthogonal, or I want them to be something else, right, I can also choose the dimension of these, of these, what is called stocks, right, so these spaces, and I can define differential operators on this structure, right, so the difference between the standard, for example, gradient and the shift gradient would be the same way as we have a manifold, I cannot add or subtract two points on the manifold, so when I need to subtract two vectors, I first need to apply parallel transport, so I need to bring the vector from its vector space to my vector space, to my tangent space, and by doing this typically I would apply some form of rotation, if it's a manifold with the remaining metric, and only then I can subtract them, right, so here the same, if before the gradient looked like just difference between n points of an edge, here we'll have also some linear transformation that sits in between, right, long story short, I can, basically I'm interested in a Laplacian, right, so I have a shift version of the Laplacian, so it's a block matrix where every block transforms the vectors with these kind of, with these kind of matrices, right, the combination, and now I can apply this operator on my data and run a diffusion equation, and I can run it to infinity with some additional conditions, and I can ask questions like how many classes can I separate if I choose this shift in a certain way, yeah. Are you doing graph rewiring here? No, we are not doing any graph rewiring, so the graph structure is encoded in the structure of the Laplacian, right, so basically it's a kind of question about expressive power of this architecture, so I can ask how many classes I can separate, right, so expressive power, slightly different, different version from the, from the Weisferre and Lehmann, because in Weisferre and Lehmann we asked about the, how many, the types of graphs that we can, if we can distinguish, here we're looking at node level problems, yeah. Can you please make some like use cases for this kind of of graph, and the ones that were shown before in which the nodes were actually separated with no connections, one to another? So, the graph here is given, so it's, I think some, I don't remember what data set it is, yeah, sorry, what is again the question? So, like, what kind of, what are you trying to model and why is this configuration? Oh, so, yeah, the colors represent the classes, the ground rules classes, and the positions represent the features. And what's the difference between this type and the one in which you do rewiring and each class series separated, one to another? So, here the features are represented by coordinates, so the closest analogy of this illustration is to the one that I showed with the gradient flow, right, so the coordinates here represent the features, not the positional coordinates, right, and the colors represent the classes, okay. So, there is no rewiring happening here, you can also potentially use rewiring. And the results, well, I don't want to go through all the results, but basically what we show is that by using different types of sheaves, we can guarantee that we can separate different types of, different number of node classes, depending whether the graph is homophilic or heterophilic. For example, we show that you must have non-symmetric relations if you want to deal with heterophilic graphs, yeah. So, in previous method, WL, it's something like, I think the classification was on top of number of edge, if I get it correctly, like if you both go to the higher classes, then the number of edges also increases, but here in sheaves, it's, it's, is it about the dimension that we have, like, can we have a first dimension sheaves or something like this? So, Weisfer and Lehmann is different, so the hierarchy there is, basically, they are different algorithms, right. So, whether they do color refinement for different structures for, for a node, for a pair of nodes, for triplets of nodes, and so on. So, here we have, the choice is what kind of matrices we allow, what class of matrices we allow, so it's typically a group, right, so we say, for example, the, the most general case is, is GL, right, so any invertible matrix, then we can restrict it to be, for example, symmetric matrix, or then we can restrict it to be orthogonal matrix, right, and based on these choices, plus the dimension of the sheave, we get different results. So, it has to take dimension and also the matrix. Exactly. Thank you. So, this is a more theoretical question, right, because it's a good question how we actually, how we learn the sheave from the data, but assuming that we knew the sheave, right, but we allow the sheave to be only from of a certain type, what is in the best case, how many node classes we could separate, under different assumptions also about the structure of the graph, whether it's homophilic or heterophilic. But, basically, the, the, the, the bottom line of this story is that diffusion, when you have, when you have these extra degrees of freedom, looks more interesting than, than, than the standard diffusion on a graph. So, the standard diffusion on a graph corresponds to symmetric restrictions with one dimensional sheave. Yep. well so it's slightly more complicated right so the analogy of the connection would be the composition of two maps right so what we call a transport map so each of these f's is called the restriction map so it goes from node to edge space they actually can have different dimensions so they don't need to be the same the composition right so f f transpose is a map from the space of one node to the space of another node so that's the analogy of parallel transport right so I'm when I move a vector from one node to another geometrically transform it somehow rotated for example or scale it depends on the class of matrix that I use here so in exactly so but then of course in practice you need somehow to parametrize it right so you cannot of course in principle you can say that that let me learn individual f's for every for every age of the of the graph but it's not practically feasible so in practice f is a matrix valued function that depends on the node features so it's a little bit similar to attention but the tension is scholar here it's matrix so it's a geometric operation okay any questions right so basically to summarize this this part so what do we gain from this physics inspired perspective on graph neural networks so first of all I think it's different viewpoint on old problems like over smoothing bottlenecks it allows to on the one hand to interpret existing architectures like guts for example from a different perspective it allows to potentially design your architectures right for example using if you think of a generous discretization of differential equations then of course you can ask what kind of solver can I use can I use some some more interesting things with adaptive step size or maybe I don't know multi grid solvers and so on it allows to make principle architectural choices right like example with gradient flows so basically from the gradient flow we get restriction on symmetric weights we get residual connection we can also have some theoretical guarantees right again like we've seen with the gradient flow but maybe also of other types like convergence stability and so on and so forth but probably more interesting are links to other fields that are less explored in graph neural network literature like in particular differential geometry or algebraic topology and of course diffusion is just one example of evolution equations you can consider more interesting things so this is one example right so probably have seen these kind of things coupled oscillators so the metronomes that are put on a table and because the vibrations transfer from one to another initially they might be oscillating out of phase and then they become synchronized so think of something like this but on a graph so the coupling occurs on a graph in a learnable way and depending on the tasks that we want to do we have we want somehow to to interact between these different oscillators so it's also a differential equation but it also has a second order kinetic term so unlike a diffusion equation it has also a surgery component and here we show for example that we can probably avoid over smoothing by using this type of equations how much time do I have 15 minutes okay so what I would like to do if in 15 minutes let's talk about grids so I definitely ran out of time because out of all the geometric objects at the end well we spent all the time on crafts I think grids also deserve a little bit and probably well everybody is familiar with grids right so let's look at them maybe again from the perspective of geometric deep learning and hopefully some new intuition or at least for some of you have not seen it before so and this also relates to the previous question of why we call graph convolutional networks convolutional so first of all a grid is a graph right so it's a particular type of a graph for simplicity I would like to assume that the grid has periodic boundary conditions so basically it's what is called the ring graph and the idea of geometric deep learning right this group based framework we had some domain and we have a group that acted on the domain we have a signal that was defined on the domain so this is a general type of this mechanism that is often called lifting so now I have a linear operator so this can be anything right so this can be a non linear complicated thing here I have a linear operator so the group representation attacks on functions defined on the domain okay and in the case of a grid this is just the shift operator so this is what they show in one dimension so just cyclically moves the elements of the vector now another thing that you see in a grid is that it has a fixed neighborhood structure right in this example every node is connected to exactly two neighbors and they are also ordered right so I always have the one before and the one after in a two-dimensional grid I might have some partial order right so I have something on top and something on the bottom so in the past on the general graph we had this kind of aggregation function right so we have the feature of the node itself and then we had a multi-set that was unordered of the nearby features and because we didn't have any order in this multi-set we the only thing we could do is to apply a symmetric function apply a permutation invariant function now we have a different situation now the nodes are ordered right so we have a fixed order of x i minus one x i and x i plus one so this function can be more general right so it doesn't need to be doesn't need to be to be symmetric and if this function is linear then we get the convolution right and if I write it as a matrix vector product then it looks like this so it's a special matrix which has fixed elements along the diagonal right so this is the the the local weight-sharing that have been convolutional neural networks so this is a special type of matrices they're called circumvent matrices right or convolutions so it's synonym you take a vector of parameters right let's call it data and you create these matrix by cyclically shifting by one position these vector of parameters and depending it as columns that's how you get you get these matrix again I'm assuming periodic boundary conditions so technically speaking it's not a convergence it's a circle convolution or a cyclic convolution but just to make things simpler okay now one thing that you first thing that you learn in in algebra one-on-one is that matrix multiplication is not commutative right a b is not equal to b a but with these matrices with convolutions there are with circumvent matrices that's not the case it's actually a special type of matrices that do commute right and in particular they commute with one of them which is the shift operator right so a shift is also a circumvent matrix right or also a convolution right so looks like this so what does it mean that that a convolution commutes with a shift so this is what we call shift-equivariance right so in other words I can first apply convolution and then shift or I can first apply shift and then convolution the result will be the same right so convolution is shift-equivariant you can show the other way around right so you can show that if you have shift equivariant linear operations so I take a matrix and I tell you that it's shift-equivariant you can show that it must be a convolution right so basically convolution emerges from considerations of translational symmetry right so the only linear operation that that is shift-equivariant is convolution so convolution is the only thing that satisfies this property and we've seen again this geometric deep learning blueprint so allow me to show it again so we have a grid we have a translation group its representation is the shift operator so the convolution is a function that is equivariant with respect to this to this group now we also know that there is an intimate relation between the Fourier transform and the convolution right and let's actually try to understand what the Fourier transform is where it comes from so we know from algebra again that commuting matrices are jointly diagonalizable it means that they have the same eigenvectors or more correct eigenspaces but here we assume that the multiplicity of eigenvalues is trivial so they actually have the same eigenvectors and the only different different eigenvalues right so all commutative matrices satisfy this property so if I have a set of matrices that commute pair wisely then then this is the case right and this is the case for for convolutions or for circuit matrices so we can pick up one of these matrices right and compute its eigenvectors right and we know that all of them will have the same and it's convenient to look at the shift operator right at the matrix S and if we compute the eigen vectors of the shift operator you can do it by hand it's actually not difficult you see that they look like these complex exponentials so this is exactly the Fourier transform or more correctly the discrete Fourier transform so the question of course that remains is what the eigenvalues are right so we know that the eigenvectors of all conversions are the discrete Fourier transform so these complex sinusoids but the eigenvalues also you can show it are the Fourier transform of the vector theta that forms each of these matrices and this gives us this dual relationship between the Fourier transform and the convolution so if I have a signal x I can do convolution in the spatial domain by multiplying by a circuit matrix or I can do it in the in the Fourier domain I can compute the Fourier transform and there the Fourier transform diagonalizes the convolution so it becomes an element wise product right so basically the product of two Fourier transforms is the Fourier transform of the convolution right and typically in signal processing the filters are already designed in the Fourier domain this is bread and butter of signal processing so the the the advantage of using the Fourier transform because this operation usually on grids can be done efficiently so instead of n squared operations as you would typically require here because the Fourier transform the the the matrix has a very redundant structure you can you can avoid these explicit multiplications you can reuse some of the multiplications and do it in n log n operations so there are classes of algorithms that are called fast Fourier transforms and this is from the approximately the sixes when this was derived with the most famous algorithm is by Kuli and Tuki this is how signal processing has been done and you have it everywhere from your stereo to your iPhone from your computer so this is how it's done you cannot do it on graphs because on graphs the analogy of the Fourier transform would be the eigenvectors of either the adjacency matrix or the Laplacian matrix so if they are symmetric they have orthogonal eigen decomposition but these matrices do not have these redundant structures so the Fourier transform has n squared complexity dense matrix multiplication and actually some of the early crafting of electrical architectures came from this domain of signal processing on graphs where that that used the eigenvectors of the Laplacian or the adjacency matrix as an analogy of the Fourier transform so the difference in the case of the in the Euclidean case on the grid there is no difference between the two right so the Laplacian is also obviously a circuant operator circuant matrix and so is the shift right or the adjacency matrix of of the ring graph which happens to be the shift operator they all commute so they have the same eigenvectors on the general graph they are different so therefore these methods slightly slightly differ so the way to think of why you you want to look at the adjacency at the adjacency matrix is this right so this is how you can think of your convolution so these basically it's multiple diagonal matrix now we can write it as a sum weighted by these coefficients of the powers of the adjacency matrix right so the adjacency matrix of the ring graph will look like this so this red diagonal right so that's the shift operator so if you take a square you will get this if you get cube you will get this right so you combine all of them you will get you will get this general convolution so first architectures that try to do to do learning on graphs looked exactly at this taking powers of either the Laplacian or or the adjacency matrix basically polynomial with learnable coefficients now if you also look at terms of the degrees of freedom so a fully connected layer will look like this right so it has no it has no symmetry so here the symmetry is trivial so it has n square degrees of freedom in the case of convolution so the the the symmetry here is translation we have order of n degrees of freedom right so we reduce dramatically the number of parameters in the neural network we reuse the same coefficients everywhere in the case of a graph because we have permutation in variance we don't have the order of the neighbor so we must use the same coefficient so we can only distinguish between ourselves and our neighborhood right so that's well here it's I'm assuming a complete graph so this will look like something like deep sets for example so but the number of parameters is order of one so it's independent on the on the size of the domain what else can I say can I tell you well I know that I'm out of time so do you want to hear about molecules or you heard about molecules okay so let's talk about molecules I promise that I will try to do it try to do it fast and probably heard in Miguel's lecture as well so it will probably be a little bit repetitive so graphs are a very convenient model for molecules right basically a molecule looks like this so you can represent it as a graph and maybe that's not how chemists think of molecules but at least in some applications graph neural networks have been successful in predicting certain properties of molecules that are required for virtual drug screening right where the space of potentially synthesizable drug like molecules is huge something like 10 to the power 60 the number of molecules that they can actually test in the lab is significantly smaller so you need to reach this by some kind of computational methods and crafting networks have been shown again in predicting some properties to be significantly faster while similar complexity to similar accuracy to to classical methods so one thing that that that is important to say in regard regarding molecules so molecules are not just any graph where the symmetry that we have is the symmetry of the domain right the permutation of the nodes or the reordering of the atoms right so the domain symmetry tells you that no matter how you order the atoms in the molecule I still want to be able to say that it's the same molecule but it also has geometric coordinates right so in addition to the let's say atom types that we have here I also have the XYZ coordinates for every atom right so it's a graph that lives in a continuous Euclidean space so here what I want to say that if I rotate the molecule for example or translated I want to be able to say that the properties remain the same so in this case typically you look at the special Euclidean group so rotations and translations without reflections reflections can actually change the properties of molecules or you can use some other groups as well and there have been already several interesting success stories so one of them was a group of Jim Collins at MIT so they used graph neural networks in virtual screening pipelines where they tried to determine which compounds could be used as new antibiotics against antibiotic resistant bacteria and they famously found that that a candidate drug that was tested against diabetes called Halicin was actually effective across a broad range of of antibiotic resistance bacteria but in things that we are doing we are mostly interested in proteins and this is well I think this is in general proteins are important targets for for drugs because they are involved practically in anything that that happens in our body from defense against pathogens right antibodies are special types of proteins to delivering oxygen to ourselves hemoglobin is also a special type of protein so basically they're everywhere and encoded in our DNA so we really we don't know any life form that is not based on proteins at least for the time being and it was conjectured in the 70s by Anfins and Nobel laureate in chemistry that you can determine the structure of the protein from its sequence so proteins are long chains of amino acids connected to each other and then under the influence of electrostatic forces they fold into these complicated structures but we are interested in the opposite problem so maybe a little bit incorrectly we can call it some kind of inverse problem so I would like to to design a protein that will fall in fold into a certain structure of course it's not that simple because it is tempting to think that we have a sequence that then falls into a structure and the structure and doubts the protein with certain function for example what kind of molecules it binds and initially computer scientists look at proteins as sequences because well it's just strings so we can look for certain patterns you can try to align different sequences together right like multiple multiple sequence alignment then came the problem of structure prediction and that's where alpha fold excelled recently but then the problem of function is distinct and you can find examples of for example proteins with different sequences but similar structure you can find proteins with very similar sequences but very different structure or you can also find proteins with different sequences and different structures but similar function so they happen to to bind the same the same molecule so the good analogy here is this lock and key metaphor that was introduced by I like quotes from Nobel laureates so that was from Emil Fischer also Nobel Nobel laureate in in chemistry so he was talking about enzymes but I think it's more general applies to to proteins broadly so same way as you have a unique key that fits into a lock you might have a unique molecule or at least that's that's the wishful thinking is that a unique molecule that will fit into some pocket that exists on the the surface of this folded protein structure and this is how drugs are typically designed right so you have a protein that is your target so that's how its surface looks like and here is some small molecule that sticks into this hole and binds this this molecule and that's how the drug works so this is actually a molecule not exactly of caffeine but of compound from the same class and that's how it's by how it binds the adenosine receptor in the brain many other interesting targets though they don't have these kind of pocket like structures and there are interesting systems of protein of proteins interacting with each other like this one the program death complex where you have two proteins called pd1 and pdl1 and they are involved in cancer immunotherapy basically these proteins tell our immune system not to kill healthy cells and some cancers have these proteins so they manage to evade the normal functioning of the immune system and the idea is to block one of these proteins either pd1 or pdl1 and this way basically the malignant cells are destroyed by by by the immune system so you need to design some binder that will that will bind to one of these proteins and they happen to have these kind of flat interfaces so they're considered to be hard or impossible to drug by small molecules but you can drug them by proteins so that's the idea of biological drugs where the drug itself is is a protein molecule typically an antibody for variety of reasons so you can use geometrically planning well and unfortunately I didn't have time to talk about it but basically instead of considering graphs we can consider surfaces so we model proteins as many folds as as basically the external surface that that that appears to to the other molecule that that tries to bind it and this way you abstract all the internal intricacies of the fold so let me try to show you an example so this is a plastic model of a protein you see so this is protein is the the one that that the person holds is supposed to bind to these transparent ones so you see that these complicated helixes and and other things inside so that's the protein fold but what appears from the outside is this transparent surface so this guy doesn't care what happens inside so it cares only about the the the structure of course the problem is more complicated because the the conformation of the protein the its geometric structure might change as a result of of the interaction but at least it's in some cases it's a good approximation so long story short we we can do special type of neural networks that operate on these surfaces so they take into account both geometric and and chemical properties of of the molecular surface and they can they try to find complementary structures that are expected to interact so think of kind of pieces of three-dimensional puzzle but it's not only geometric complementarity it's also chemical complementarity so they need to have the right charges so they don't repel each other and this is a method that we call the massive so we were lucky to appear on the cover of nature methods in 2020 and this year we also had a paper in nature that contained experimental results so we also hope to appear on the cover but they chose a different one but because we paid for the cover here you need to you need to see it i think it was a cool image so we used this method to design new binders for different targets and basically it's a fragment based design so we use this neural network architecture to identify potentially complementary targets that then we use to build the binder and here the experimental results show different structures so here's a binder for the pdl one oncological target and we also have the crystal structures and here's an example of another binder for the SARS-CoV-2 spike protein so that's the coronavirus that caused the COVID-19 pandemic that has been terrorizing us for more than three years now and basically this structure binds the region of the of the spike protein that interacts with the ACE2 receptor of the host so that's where how the virus enters into into our body and here we also tested it so we have the structure from cryoEM we also tested it on different variants of the virus so the alpha beta and omicron that probably everybody was following in the newspapers so you see that that it binds many of these maybe some others less and here's also a comparison to a clinically approved drug so that was antibodies that were developed by AstraZeneca so basically what is shown here is how much inhibition you have versus concentration so the smaller concentration the better of course so we are not as good as the AstraZeneca drug but so it's something that was designed totally computationally and this is actually pseudovirus neutralization so it is probably much closer to real validation than at least anything that myself as a computer scientist could think of well I think I will probably stop here but if you think of diffusion models right so generative models everybody is now talking about right like like the Dali2 and now of course you have way better results so you could imagine something like this for for molecular design so we have some condition on on let's say diffusion model that we use here like the geometric structure of the of the target pocket and you'll try to build a molecule that satisfies these these constraints so we don't really have a text prompt but you have maybe some some other way of conditioning the model so so this is one example maybe not very interesting so another example is what we call diffusion linker where we have small molecular fragments what is called pharmacophores that you know how they bind the target but you also need to connect them into bigger molecule and we try to basically to start with these little fragments and to diffuse the the the linking structure that that connects them we're not very lucky in publishing this paper in europe so we'll probably send it to some chemical journal uh well I think I will stop here sorry for running out of time thank you very much uh yeah we are over time but if you have still a couple of questions if not you can ask individual maybe okay but thank you again for for the amazing talk well thank you thank you", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 2.4, "text": " Thank you very much.", "tokens": [50364, 1044, 291, 588, 709, 13, 50484], "temperature": 0.0, "avg_logprob": -0.22123388226112622, "compression_ratio": 1.5357142857142858, "no_speech_prob": 0.08427701890468597}, {"id": 1, "seek": 0, "start": 2.4, "end": 3.84, "text": " So great pleasure to be here.", "tokens": [50484, 407, 869, 6834, 281, 312, 510, 13, 50556], "temperature": 0.0, "avg_logprob": -0.22123388226112622, "compression_ratio": 1.5357142857142858, "no_speech_prob": 0.08427701890468597}, {"id": 2, "seek": 0, "start": 3.84, "end": 9.84, "text": " It's actually my second time in Krakow, and I think it's a very beautiful choice for", "tokens": [50556, 467, 311, 767, 452, 1150, 565, 294, 591, 11272, 305, 11, 293, 286, 519, 309, 311, 257, 588, 2238, 3922, 337, 50856], "temperature": 0.0, "avg_logprob": -0.22123388226112622, "compression_ratio": 1.5357142857142858, "no_speech_prob": 0.08427701890468597}, {"id": 3, "seek": 0, "start": 9.84, "end": 12.32, "text": " having a machine learning summer school.", "tokens": [50856, 1419, 257, 3479, 2539, 4266, 1395, 13, 50980], "temperature": 0.0, "avg_logprob": -0.22123388226112622, "compression_ratio": 1.5357142857142858, "no_speech_prob": 0.08427701890468597}, {"id": 4, "seek": 0, "start": 12.32, "end": 17.32, "text": " So in the next three hours, I would like to talk about geometric deep learning.", "tokens": [50980, 407, 294, 264, 958, 1045, 2496, 11, 286, 576, 411, 281, 751, 466, 33246, 2452, 2539, 13, 51230], "temperature": 0.0, "avg_logprob": -0.22123388226112622, "compression_ratio": 1.5357142857142858, "no_speech_prob": 0.08427701890468597}, {"id": 5, "seek": 0, "start": 17.32, "end": 25.92, "text": " And if you wonder this intriguing image, what does it have to do with machine learning?", "tokens": [51230, 400, 498, 291, 2441, 341, 32503, 3256, 11, 437, 775, 309, 362, 281, 360, 365, 3479, 2539, 30, 51660], "temperature": 0.0, "avg_logprob": -0.22123388226112622, "compression_ratio": 1.5357142857142858, "no_speech_prob": 0.08427701890468597}, {"id": 6, "seek": 2592, "start": 25.92, "end": 29.96, "text": " So you see it fits more, a kind of an alchemist.", "tokens": [50364, 407, 291, 536, 309, 9001, 544, 11, 257, 733, 295, 364, 419, 17345, 468, 13, 50566], "temperature": 0.0, "avg_logprob": -0.2519522476196289, "compression_ratio": 1.5250965250965252, "no_speech_prob": 0.04287661612033844}, {"id": 7, "seek": 2592, "start": 29.96, "end": 38.68, "text": " So we wanted to use this image in reference to this famous quote from Ali Rahimi who was", "tokens": [50566, 407, 321, 1415, 281, 764, 341, 3256, 294, 6408, 281, 341, 4618, 6513, 490, 12020, 17844, 10121, 567, 390, 51002], "temperature": 0.0, "avg_logprob": -0.2519522476196289, "compression_ratio": 1.5250965250965252, "no_speech_prob": 0.04287661612033844}, {"id": 8, "seek": 2592, "start": 38.68, "end": 44.040000000000006, "text": " receiving the Best Paper Award or the Proof of Time Award at New Europe in 2017.", "tokens": [51002, 10040, 264, 9752, 24990, 13894, 420, 264, 1705, 2670, 295, 6161, 13894, 412, 1873, 3315, 294, 6591, 13, 51270], "temperature": 0.0, "avg_logprob": -0.2519522476196289, "compression_ratio": 1.5250965250965252, "no_speech_prob": 0.04287661612033844}, {"id": 9, "seek": 2592, "start": 44.040000000000006, "end": 48.040000000000006, "text": " And he was speaking this way maybe a little bit critically about deep learning, at least", "tokens": [51270, 400, 415, 390, 4124, 341, 636, 1310, 257, 707, 857, 22797, 466, 2452, 2539, 11, 412, 1935, 51470], "temperature": 0.0, "avg_logprob": -0.2519522476196289, "compression_ratio": 1.5250965250965252, "no_speech_prob": 0.04287661612033844}, {"id": 10, "seek": 2592, "start": 48.040000000000006, "end": 55.52, "text": " at that time, that we say things like machine learning is the new electricity, and he's", "tokens": [51470, 412, 300, 565, 11, 300, 321, 584, 721, 411, 3479, 2539, 307, 264, 777, 10356, 11, 293, 415, 311, 51844], "temperature": 0.0, "avg_logprob": -0.2519522476196289, "compression_ratio": 1.5250965250965252, "no_speech_prob": 0.04287661612033844}, {"id": 11, "seek": 5552, "start": 55.52, "end": 60.88, "text": " alternative metaphor was machine learning has become alchemy, so in the sense that some", "tokens": [50364, 8535, 19157, 390, 3479, 2539, 575, 1813, 419, 339, 3633, 11, 370, 294, 264, 2020, 300, 512, 50632], "temperature": 0.0, "avg_logprob": -0.16262939998081752, "compression_ratio": 1.7615384615384615, "no_speech_prob": 0.010516402311623096}, {"id": 12, "seek": 5552, "start": 60.88, "end": 65.56, "text": " kind of science that maybe produces something that we don't really understand what it does.", "tokens": [50632, 733, 295, 3497, 300, 1310, 14725, 746, 300, 321, 500, 380, 534, 1223, 437, 309, 775, 13, 50866], "temperature": 0.0, "avg_logprob": -0.16262939998081752, "compression_ratio": 1.7615384615384615, "no_speech_prob": 0.010516402311623096}, {"id": 13, "seek": 5552, "start": 65.56, "end": 71.84, "text": " And really what we would like to do here is to try to understand from certain perspective", "tokens": [50866, 400, 534, 437, 321, 576, 411, 281, 360, 510, 307, 281, 853, 281, 1223, 490, 1629, 4585, 51180], "temperature": 0.0, "avg_logprob": -0.16262939998081752, "compression_ratio": 1.7615384615384615, "no_speech_prob": 0.010516402311623096}, {"id": 14, "seek": 5552, "start": 71.84, "end": 76.44, "text": " how these methods work and why they work, and maybe more importantly, when they fail", "tokens": [51180, 577, 613, 7150, 589, 293, 983, 436, 589, 11, 293, 1310, 544, 8906, 11, 562, 436, 3061, 51410], "temperature": 0.0, "avg_logprob": -0.16262939998081752, "compression_ratio": 1.7615384615384615, "no_speech_prob": 0.010516402311623096}, {"id": 15, "seek": 5552, "start": 76.44, "end": 82.44, "text": " and you see kind of maybe general blueprint for developing potentially future machine", "tokens": [51410, 293, 291, 536, 733, 295, 1310, 2674, 35868, 337, 6416, 7263, 2027, 3479, 51710], "temperature": 0.0, "avg_logprob": -0.16262939998081752, "compression_ratio": 1.7615384615384615, "no_speech_prob": 0.010516402311623096}, {"id": 16, "seek": 5552, "start": 82.44, "end": 84.04, "text": " learning systems.", "tokens": [51710, 2539, 3652, 13, 51790], "temperature": 0.0, "avg_logprob": -0.16262939998081752, "compression_ratio": 1.7615384615384615, "no_speech_prob": 0.010516402311623096}, {"id": 17, "seek": 8404, "start": 84.04, "end": 90.80000000000001, "text": " So the concept that will be important to these lectures is the concept of symmetry.", "tokens": [50364, 407, 264, 3410, 300, 486, 312, 1021, 281, 613, 16564, 307, 264, 3410, 295, 25440, 13, 50702], "temperature": 0.0, "avg_logprob": -0.1871207356452942, "compression_ratio": 1.589430894308943, "no_speech_prob": 0.04464275762438774}, {"id": 18, "seek": 8404, "start": 90.80000000000001, "end": 97.80000000000001, "text": " And symmetry according to Vile, who I'm quoting here is, depending on how wide or narrow you", "tokens": [50702, 400, 25440, 4650, 281, 691, 794, 11, 567, 286, 478, 41552, 510, 307, 11, 5413, 322, 577, 4874, 420, 9432, 291, 51052], "temperature": 0.0, "avg_logprob": -0.1871207356452942, "compression_ratio": 1.589430894308943, "no_speech_prob": 0.04464275762438774}, {"id": 19, "seek": 8404, "start": 97.80000000000001, "end": 103.60000000000001, "text": " define its meaning, is an idea by which men through the ages has tried to comprehend and", "tokens": [51052, 6964, 1080, 3620, 11, 307, 364, 1558, 538, 597, 1706, 807, 264, 12357, 575, 3031, 281, 38183, 293, 51342], "temperature": 0.0, "avg_logprob": -0.1871207356452942, "compression_ratio": 1.589430894308943, "no_speech_prob": 0.04464275762438774}, {"id": 20, "seek": 8404, "start": 103.60000000000001, "end": 105.80000000000001, "text": " create order beauty and perfection.", "tokens": [51342, 1884, 1668, 6643, 293, 19708, 13, 51452], "temperature": 0.0, "avg_logprob": -0.1871207356452942, "compression_ratio": 1.589430894308943, "no_speech_prob": 0.04464275762438774}, {"id": 21, "seek": 8404, "start": 105.80000000000001, "end": 111.28, "text": " So it sounds a bit poetic, but I think it is true, so that's from his book that is titled", "tokens": [51452, 407, 309, 3263, 257, 857, 41080, 11, 457, 286, 519, 309, 307, 2074, 11, 370, 300, 311, 490, 702, 1446, 300, 307, 19841, 51726], "temperature": 0.0, "avg_logprob": -0.1871207356452942, "compression_ratio": 1.589430894308943, "no_speech_prob": 0.04464275762438774}, {"id": 22, "seek": 11128, "start": 111.28, "end": 114.84, "text": " Symmetry, that he published in Princeton.", "tokens": [50364, 3902, 2174, 9889, 11, 300, 415, 6572, 294, 36592, 13, 50542], "temperature": 0.0, "avg_logprob": -0.19766685643146947, "compression_ratio": 1.6859504132231404, "no_speech_prob": 0.008665172383189201}, {"id": 23, "seek": 11128, "start": 114.84, "end": 119.76, "text": " And symmetry is a Greek word, so it goes back to the ancient Greeks, and as you probably", "tokens": [50542, 400, 25440, 307, 257, 10281, 1349, 11, 370, 309, 1709, 646, 281, 264, 7832, 31029, 11, 293, 382, 291, 1391, 50788], "temperature": 0.0, "avg_logprob": -0.19766685643146947, "compression_ratio": 1.6859504132231404, "no_speech_prob": 0.008665172383189201}, {"id": 24, "seek": 11128, "start": 119.76, "end": 125.0, "text": " know, ancient Greeks like Plato considered symmetry to be really the cornerstone of the", "tokens": [50788, 458, 11, 7832, 31029, 411, 43027, 4888, 25440, 281, 312, 534, 264, 4538, 11243, 295, 264, 51050], "temperature": 0.0, "avg_logprob": -0.19766685643146947, "compression_ratio": 1.6859504132231404, "no_speech_prob": 0.008665172383189201}, {"id": 25, "seek": 11128, "start": 125.0, "end": 126.0, "text": " universe.", "tokens": [51050, 6445, 13, 51100], "temperature": 0.0, "avg_logprob": -0.19766685643146947, "compression_ratio": 1.6859504132231404, "no_speech_prob": 0.008665172383189201}, {"id": 26, "seek": 11128, "start": 126.0, "end": 132.04, "text": " So according to Plato, what is nowadays called platonic solids, symmetric polyhedra were the", "tokens": [51100, 407, 4650, 281, 43027, 11, 437, 307, 13434, 1219, 3403, 11630, 38536, 11, 32330, 6754, 27096, 424, 645, 264, 51402], "temperature": 0.0, "avg_logprob": -0.19766685643146947, "compression_ratio": 1.6859504132231404, "no_speech_prob": 0.008665172383189201}, {"id": 27, "seek": 11128, "start": 132.04, "end": 137.32, "text": " basic building blocks of all the stuff in the universe, so probably tiny little things", "tokens": [51402, 3875, 2390, 8474, 295, 439, 264, 1507, 294, 264, 6445, 11, 370, 1391, 5870, 707, 721, 51666], "temperature": 0.0, "avg_logprob": -0.19766685643146947, "compression_ratio": 1.6859504132231404, "no_speech_prob": 0.008665172383189201}, {"id": 28, "seek": 13732, "start": 137.32, "end": 143.04, "text": " that build up the matter, and if you think of it in modern terminology, that's not very", "tokens": [50364, 300, 1322, 493, 264, 1871, 11, 293, 498, 291, 519, 295, 309, 294, 4363, 27575, 11, 300, 311, 406, 588, 50650], "temperature": 0.0, "avg_logprob": -0.14415962751521622, "compression_ratio": 1.6177777777777778, "no_speech_prob": 0.023197488859295845}, {"id": 29, "seek": 13732, "start": 143.04, "end": 144.79999999999998, "text": " far from truth.", "tokens": [50650, 1400, 490, 3494, 13, 50738], "temperature": 0.0, "avg_logprob": -0.14415962751521622, "compression_ratio": 1.6177777777777778, "no_speech_prob": 0.023197488859295845}, {"id": 30, "seek": 13732, "start": 144.79999999999998, "end": 154.44, "text": " So Plato believed that geometry is really the key piece of mathematics, and even according", "tokens": [50738, 407, 43027, 7847, 300, 18426, 307, 534, 264, 2141, 2522, 295, 18666, 11, 293, 754, 4650, 51220], "temperature": 0.0, "avg_logprob": -0.14415962751521622, "compression_ratio": 1.6177777777777778, "no_speech_prob": 0.023197488859295845}, {"id": 31, "seek": 13732, "start": 154.44, "end": 159.4, "text": " to a legend, there was an inscription on the entrance to his academy saying that nobody", "tokens": [51220, 281, 257, 9451, 11, 456, 390, 364, 49882, 322, 264, 12014, 281, 702, 25525, 1566, 300, 5079, 51468], "temperature": 0.0, "avg_logprob": -0.14415962751521622, "compression_ratio": 1.6177777777777778, "no_speech_prob": 0.023197488859295845}, {"id": 32, "seek": 13732, "start": 159.4, "end": 164.88, "text": " skilled in geometry, that is not skilled in geometry, should be allowed to enter.", "tokens": [51468, 19690, 294, 18426, 11, 300, 307, 406, 19690, 294, 18426, 11, 820, 312, 4350, 281, 3242, 13, 51742], "temperature": 0.0, "avg_logprob": -0.14415962751521622, "compression_ratio": 1.6177777777777778, "no_speech_prob": 0.023197488859295845}, {"id": 33, "seek": 16488, "start": 164.88, "end": 169.51999999999998, "text": " And this idea of matter being built of small symmetric polyhedra, actually not far from", "tokens": [50364, 400, 341, 1558, 295, 1871, 885, 3094, 295, 1359, 32330, 6754, 27096, 424, 11, 767, 406, 1400, 490, 50596], "temperature": 0.0, "avg_logprob": -0.13546273202607126, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.04092061519622803}, {"id": 34, "seek": 16488, "start": 169.51999999999998, "end": 174.84, "text": " truth, if you consider how crystals are organized, and the first to study this from a formal", "tokens": [50596, 3494, 11, 498, 291, 1949, 577, 23772, 366, 9983, 11, 293, 264, 700, 281, 2979, 341, 490, 257, 9860, 50862], "temperature": 0.0, "avg_logprob": -0.13546273202607126, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.04092061519622803}, {"id": 35, "seek": 16488, "start": 174.84, "end": 181.96, "text": " perspective was actually Kepler, who is maybe more famous for his discovery of the motion", "tokens": [50862, 4585, 390, 767, 3189, 22732, 11, 567, 307, 1310, 544, 4618, 337, 702, 12114, 295, 264, 5394, 51218], "temperature": 0.0, "avg_logprob": -0.13546273202607126, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.04092061519622803}, {"id": 36, "seek": 16488, "start": 181.96, "end": 187.35999999999999, "text": " of planets, but he was also the one who laid the foundations of modern crystallography,", "tokens": [51218, 295, 15126, 11, 457, 415, 390, 611, 264, 472, 567, 9897, 264, 22467, 295, 4363, 31924, 5820, 11, 51488], "temperature": 0.0, "avg_logprob": -0.13546273202607126, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.04092061519622803}, {"id": 37, "seek": 16488, "start": 187.35999999999999, "end": 191.44, "text": " basically considering how spheres can be packed into different configurations, and if you also", "tokens": [51488, 1936, 8079, 577, 41225, 393, 312, 13265, 666, 819, 31493, 11, 293, 498, 291, 611, 51692], "temperature": 0.0, "avg_logprob": -0.13546273202607126, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.04092061519622803}, {"id": 38, "seek": 19144, "start": 191.44, "end": 197.48, "text": " think that this is old and boring and outdated stuff, so last year's Fields Medal was given", "tokens": [50364, 519, 300, 341, 307, 1331, 293, 9989, 293, 36313, 1507, 11, 370, 1036, 1064, 311, 48190, 42437, 390, 2212, 50666], "temperature": 0.0, "avg_logprob": -0.17848064281322337, "compression_ratio": 1.527027027027027, "no_speech_prob": 0.017965849488973618}, {"id": 39, "seek": 19144, "start": 197.48, "end": 202.68, "text": " exactly for solving these kind of problems maybe in higher dimensions.", "tokens": [50666, 2293, 337, 12606, 613, 733, 295, 2740, 1310, 294, 2946, 12819, 13, 50926], "temperature": 0.0, "avg_logprob": -0.17848064281322337, "compression_ratio": 1.527027027027027, "no_speech_prob": 0.017965849488973618}, {"id": 40, "seek": 19144, "start": 202.68, "end": 209.24, "text": " So geometry itself also, at least in formal way, goes back to ancient Greeks, and what", "tokens": [50926, 407, 18426, 2564, 611, 11, 412, 1935, 294, 9860, 636, 11, 1709, 646, 281, 7832, 31029, 11, 293, 437, 51254], "temperature": 0.0, "avg_logprob": -0.17848064281322337, "compression_ratio": 1.527027027027027, "no_speech_prob": 0.017965849488973618}, {"id": 41, "seek": 19144, "start": 209.24, "end": 215.24, "text": " we still often study at school as the geometry dates back to Euclid, his famous elements,", "tokens": [51254, 321, 920, 2049, 2979, 412, 1395, 382, 264, 18426, 11691, 646, 281, 462, 1311, 75, 327, 11, 702, 4618, 4959, 11, 51554], "temperature": 0.0, "avg_logprob": -0.17848064281322337, "compression_ratio": 1.527027027027027, "no_speech_prob": 0.017965849488973618}, {"id": 42, "seek": 21524, "start": 215.24, "end": 221.52, "text": " so a system of axioms from which his geometry was derived, and as you know there are five", "tokens": [50364, 370, 257, 1185, 295, 6360, 72, 4785, 490, 597, 702, 18426, 390, 18949, 11, 293, 382, 291, 458, 456, 366, 1732, 50678], "temperature": 0.0, "avg_logprob": -0.1634786788453447, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.01167308446019888}, {"id": 43, "seek": 21524, "start": 221.52, "end": 226.88, "text": " axioms or five postulates of Euclidean geometry, that the last one was somehow standing out", "tokens": [50678, 6360, 72, 4785, 420, 1732, 2183, 26192, 295, 462, 1311, 31264, 282, 18426, 11, 300, 264, 1036, 472, 390, 6063, 4877, 484, 50946], "temperature": 0.0, "avg_logprob": -0.1634786788453447, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.01167308446019888}, {"id": 44, "seek": 21524, "start": 226.88, "end": 233.64000000000001, "text": " and people for many centuries or even thousands of years tried to do something with it, and", "tokens": [50946, 293, 561, 337, 867, 13926, 420, 754, 5383, 295, 924, 3031, 281, 360, 746, 365, 309, 11, 293, 51284], "temperature": 0.0, "avg_logprob": -0.1634786788453447, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.01167308446019888}, {"id": 45, "seek": 21524, "start": 233.64000000000001, "end": 241.20000000000002, "text": " for example in the 18th century Giovanni Saccheri, who was a priest, almost arrived to the construction", "tokens": [51284, 337, 1365, 294, 264, 2443, 392, 4901, 47089, 35832, 19356, 6759, 72, 11, 567, 390, 257, 15703, 11, 1920, 6678, 281, 264, 6435, 51662], "temperature": 0.0, "avg_logprob": -0.1634786788453447, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.01167308446019888}, {"id": 46, "seek": 24120, "start": 241.2, "end": 248.11999999999998, "text": " of a non-Euclidean geometry, but he considered it such a heretical idea that he thought that", "tokens": [50364, 295, 257, 2107, 12, 36, 1311, 31264, 282, 18426, 11, 457, 415, 4888, 309, 1270, 257, 720, 27800, 1558, 300, 415, 1194, 300, 50710], "temperature": 0.0, "avg_logprob": -0.1355859120686849, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.035834863781929016}, {"id": 47, "seek": 24120, "start": 248.11999999999998, "end": 253.39999999999998, "text": " it is repugnant to the nature of straight lines, so he abandoned these ideas and never", "tokens": [50710, 309, 307, 1085, 697, 25928, 281, 264, 3687, 295, 2997, 3876, 11, 370, 415, 13732, 613, 3487, 293, 1128, 50974], "temperature": 0.0, "avg_logprob": -0.1355859120686849, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.035834863781929016}, {"id": 48, "seek": 24120, "start": 253.39999999999998, "end": 259.03999999999996, "text": " took it to the full extent, but in the 19th century what happened is that finally came", "tokens": [50974, 1890, 309, 281, 264, 1577, 8396, 11, 457, 294, 264, 1294, 392, 4901, 437, 2011, 307, 300, 2721, 1361, 51256], "temperature": 0.0, "avg_logprob": -0.1355859120686849, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.035834863781929016}, {"id": 49, "seek": 24120, "start": 259.03999999999996, "end": 265.56, "text": " the realization that dethroned Euclid and broke his monopoly of geometry and the works", "tokens": [51256, 264, 25138, 300, 1141, 1703, 19009, 462, 1311, 75, 327, 293, 6902, 702, 37061, 295, 18426, 293, 264, 1985, 51582], "temperature": 0.0, "avg_logprob": -0.1355859120686849, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.035834863781929016}, {"id": 50, "seek": 26556, "start": 265.56, "end": 271.56, "text": " that probably gauss himself did, but never published, and then famously Lobochevsky,", "tokens": [50364, 300, 1391, 5959, 2023, 3647, 630, 11, 457, 1128, 6572, 11, 293, 550, 34360, 30719, 78, 1876, 85, 25810, 11, 50664], "temperature": 0.0, "avg_logprob": -0.227481586592538, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.019697880372405052}, {"id": 51, "seek": 26556, "start": 271.56, "end": 278.24, "text": " Boje and Riemann, who created the first examples of what we now call non-Euclidean geometries,", "tokens": [50664, 3286, 2884, 293, 497, 4907, 969, 11, 567, 2942, 264, 700, 5110, 295, 437, 321, 586, 818, 2107, 12, 36, 1311, 31264, 282, 12956, 2244, 11, 50998], "temperature": 0.0, "avg_logprob": -0.227481586592538, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.019697880372405052}, {"id": 52, "seek": 26556, "start": 278.24, "end": 283.76, "text": " and in the 19th century this is how the geometry started looking like, so a zoo of different", "tokens": [50998, 293, 294, 264, 1294, 392, 4901, 341, 307, 577, 264, 18426, 1409, 1237, 411, 11, 370, 257, 25347, 295, 819, 51274], "temperature": 0.0, "avg_logprob": -0.227481586592538, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.019697880372405052}, {"id": 53, "seek": 26556, "start": 283.76, "end": 289.84000000000003, "text": " types of geometry without clear relation or understanding what actually defines the geometry,", "tokens": [51274, 3467, 295, 18426, 1553, 1850, 9721, 420, 3701, 437, 767, 23122, 264, 18426, 11, 51578], "temperature": 0.0, "avg_logprob": -0.227481586592538, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.019697880372405052}, {"id": 54, "seek": 26556, "start": 289.84000000000003, "end": 295.28, "text": " so there was obviously a need to put order in this mess, and the new idea, new approach", "tokens": [51578, 370, 456, 390, 2745, 257, 643, 281, 829, 1668, 294, 341, 2082, 11, 293, 264, 777, 1558, 11, 777, 3109, 51850], "temperature": 0.0, "avg_logprob": -0.227481586592538, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.019697880372405052}, {"id": 55, "seek": 29528, "start": 295.28, "end": 303.84, "text": " came from Felix Klein in 1872, so he was only 23 years old, he was lucky to get an appointment", "tokens": [50364, 1361, 490, 30169, 33327, 294, 2443, 28890, 11, 370, 415, 390, 787, 6673, 924, 1331, 11, 415, 390, 6356, 281, 483, 364, 13653, 50792], "temperature": 0.0, "avg_logprob": -0.20063201240871265, "compression_ratio": 1.4915966386554622, "no_speech_prob": 0.04478425532579422}, {"id": 56, "seek": 29528, "start": 303.84, "end": 307.44, "text": " as a professor at the University of Erlangen in Bavaria, so this is something that for", "tokens": [50792, 382, 257, 8304, 412, 264, 3535, 295, 3300, 75, 10784, 294, 363, 706, 9831, 11, 370, 341, 307, 746, 300, 337, 50972], "temperature": 0.0, "avg_logprob": -0.20063201240871265, "compression_ratio": 1.4915966386554622, "no_speech_prob": 0.04478425532579422}, {"id": 57, "seek": 29528, "start": 307.44, "end": 315.0, "text": " example Euler failed to have in Switzerland, he had to go to Russia to become a faculty,", "tokens": [50972, 1365, 462, 26318, 7612, 281, 362, 294, 23312, 11, 415, 632, 281, 352, 281, 6797, 281, 1813, 257, 6389, 11, 51350], "temperature": 0.0, "avg_logprob": -0.20063201240871265, "compression_ratio": 1.4915966386554622, "no_speech_prob": 0.04478425532579422}, {"id": 58, "seek": 29528, "start": 315.0, "end": 320.79999999999995, "text": " probably not very dissimilar to situations that some of us are facing in these days,", "tokens": [51350, 1391, 406, 588, 7802, 332, 2202, 281, 6851, 300, 512, 295, 505, 366, 7170, 294, 613, 1708, 11, 51640], "temperature": 0.0, "avg_logprob": -0.20063201240871265, "compression_ratio": 1.4915966386554622, "no_speech_prob": 0.04478425532579422}, {"id": 59, "seek": 32080, "start": 320.8, "end": 327.64, "text": " so Klein was asked, as it was customary in Germany and still customary I think, to deliver", "tokens": [50364, 370, 33327, 390, 2351, 11, 382, 309, 390, 2375, 822, 294, 7244, 293, 920, 2375, 822, 286, 519, 11, 281, 4239, 50706], "temperature": 0.0, "avg_logprob": -0.16075274149576824, "compression_ratio": 1.6436363636363636, "no_speech_prob": 0.022083576768636703}, {"id": 60, "seek": 32080, "start": 327.64, "end": 331.76, "text": " a research prospectus, basically to explain what he is going to do until his retirement", "tokens": [50706, 257, 2132, 15005, 301, 11, 1936, 281, 2903, 437, 415, 307, 516, 281, 360, 1826, 702, 15189, 50912], "temperature": 0.0, "avg_logprob": -0.16075274149576824, "compression_ratio": 1.6436363636363636, "no_speech_prob": 0.022083576768636703}, {"id": 61, "seek": 32080, "start": 331.76, "end": 336.0, "text": " in Erlangen, which actually never happened because he moved three years after to different", "tokens": [50912, 294, 3300, 75, 10784, 11, 597, 767, 1128, 2011, 570, 415, 4259, 1045, 924, 934, 281, 819, 51124], "temperature": 0.0, "avg_logprob": -0.16075274149576824, "compression_ratio": 1.6436363636363636, "no_speech_prob": 0.022083576768636703}, {"id": 62, "seek": 32080, "start": 336.0, "end": 342.72, "text": " places eventually to G\u00f6ttingen, and in this prospectus that entered the history of mathematics", "tokens": [51124, 3190, 4728, 281, 460, 12082, 783, 268, 11, 293, 294, 341, 15005, 301, 300, 9065, 264, 2503, 295, 18666, 51460], "temperature": 0.0, "avg_logprob": -0.16075274149576824, "compression_ratio": 1.6436363636363636, "no_speech_prob": 0.022083576768636703}, {"id": 63, "seek": 32080, "start": 342.72, "end": 348.08000000000004, "text": " as the Erlangen program, he proposed a kind of algebraization of geometry, so studying", "tokens": [51460, 382, 264, 3300, 75, 10784, 1461, 11, 415, 10348, 257, 733, 295, 21989, 2144, 295, 18426, 11, 370, 7601, 51728], "temperature": 0.0, "avg_logprob": -0.16075274149576824, "compression_ratio": 1.6436363636363636, "no_speech_prob": 0.022083576768636703}, {"id": 64, "seek": 34808, "start": 348.08, "end": 352.71999999999997, "text": " geometry from the perspective of group theory, so essentially considering a geometry as a", "tokens": [50364, 18426, 490, 264, 4585, 295, 1594, 5261, 11, 370, 4476, 8079, 257, 18426, 382, 257, 50596], "temperature": 0.0, "avg_logprob": -0.14338897429790692, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.011504233814775944}, {"id": 65, "seek": 34808, "start": 352.71999999999997, "end": 358.35999999999996, "text": " space plus some class of transformations formalized using group theory, and studying properties", "tokens": [50596, 1901, 1804, 512, 1508, 295, 34852, 9860, 1602, 1228, 1594, 5261, 11, 293, 7601, 7221, 50878], "temperature": 0.0, "avg_logprob": -0.14338897429790692, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.011504233814775944}, {"id": 66, "seek": 34808, "start": 358.35999999999996, "end": 364.47999999999996, "text": " that remain unchanged or invariant under these transformations, so you take an object and", "tokens": [50878, 300, 6222, 44553, 420, 33270, 394, 833, 613, 34852, 11, 370, 291, 747, 364, 2657, 293, 51184], "temperature": 0.0, "avg_logprob": -0.14338897429790692, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.011504233814775944}, {"id": 67, "seek": 34808, "start": 364.47999999999996, "end": 371.88, "text": " you apply to it rigid motions and you preserve a lot of things like areas, parallel lines,", "tokens": [51184, 291, 3079, 281, 309, 22195, 27500, 293, 291, 15665, 257, 688, 295, 721, 411, 3179, 11, 8952, 3876, 11, 51554], "temperature": 0.0, "avg_logprob": -0.14338897429790692, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.011504233814775944}, {"id": 68, "seek": 34808, "start": 371.88, "end": 376.4, "text": " distances, so this is how you create Euclidean geometry, but you can consider other groups", "tokens": [51554, 22182, 11, 370, 341, 307, 577, 291, 1884, 462, 1311, 31264, 282, 18426, 11, 457, 291, 393, 1949, 661, 3935, 51780], "temperature": 0.0, "avg_logprob": -0.14338897429790692, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.011504233814775944}, {"id": 69, "seek": 37640, "start": 376.4, "end": 381.91999999999996, "text": " like a fine or projective group, and in fact he considered projective group to be the broadest", "tokens": [50364, 411, 257, 2489, 420, 1716, 488, 1594, 11, 293, 294, 1186, 415, 4888, 1716, 488, 1594, 281, 312, 264, 4152, 377, 50640], "temperature": 0.0, "avg_logprob": -0.22963568984821278, "compression_ratio": 1.728888888888889, "no_speech_prob": 0.004695439245551825}, {"id": 70, "seek": 37640, "start": 381.91999999999996, "end": 387.56, "text": " construction, he in fact showed together with Biltrami that the first non-Euclidean geometry", "tokens": [50640, 6435, 11, 415, 294, 1186, 4712, 1214, 365, 363, 2352, 2356, 72, 300, 264, 700, 2107, 12, 36, 1311, 31264, 282, 18426, 50922], "temperature": 0.0, "avg_logprob": -0.22963568984821278, "compression_ratio": 1.728888888888889, "no_speech_prob": 0.004695439245551825}, {"id": 71, "seek": 37640, "start": 387.56, "end": 394.15999999999997, "text": " is hyperbolic, geometry is with negative curvature, could be constructed with a projective model,", "tokens": [50922, 307, 9848, 65, 7940, 11, 18426, 307, 365, 3671, 37638, 11, 727, 312, 17083, 365, 257, 1716, 488, 2316, 11, 51252], "temperature": 0.0, "avg_logprob": -0.22963568984821278, "compression_ratio": 1.728888888888889, "no_speech_prob": 0.004695439245551825}, {"id": 72, "seek": 37640, "start": 394.15999999999997, "end": 401.35999999999996, "text": " and these ideas had really big impact on geometry, on mathematics more broadly, I would say culturally,", "tokens": [51252, 293, 613, 3487, 632, 534, 955, 2712, 322, 18426, 11, 322, 18666, 544, 19511, 11, 286, 576, 584, 28879, 11, 51612], "temperature": 0.0, "avg_logprob": -0.22963568984821278, "compression_ratio": 1.728888888888889, "no_speech_prob": 0.004695439245551825}, {"id": 73, "seek": 40136, "start": 401.36, "end": 407.84000000000003, "text": " like category theory is an extension of these ideas to more abstract objects, but probably", "tokens": [50364, 411, 7719, 5261, 307, 364, 10320, 295, 613, 3487, 281, 544, 12649, 6565, 11, 457, 1391, 50688], "temperature": 0.0, "avg_logprob": -0.16618453695418986, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.024394702166318893}, {"id": 74, "seek": 40136, "start": 407.84000000000003, "end": 411.8, "text": " most importantly it had an impact on physics where came the realization in the beginning", "tokens": [50688, 881, 8906, 309, 632, 364, 2712, 322, 10649, 689, 1361, 264, 25138, 294, 264, 2863, 50886], "temperature": 0.0, "avg_logprob": -0.16618453695418986, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.024394702166318893}, {"id": 75, "seek": 40136, "start": 411.8, "end": 417.6, "text": " of the 19th century, probably starting with Neutra with her famous theorem that the laws", "tokens": [50886, 295, 264, 1294, 392, 4901, 11, 1391, 2891, 365, 1734, 325, 424, 365, 720, 4618, 20904, 300, 264, 6064, 51176], "temperature": 0.0, "avg_logprob": -0.16618453695418986, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.024394702166318893}, {"id": 76, "seek": 40136, "start": 417.6, "end": 423.88, "text": " of physics themselves can be derived from considerations of invariance or symmetry, and for example", "tokens": [51176, 295, 10649, 2969, 393, 312, 18949, 490, 24070, 295, 33270, 719, 420, 25440, 11, 293, 337, 1365, 51490], "temperature": 0.0, "avg_logprob": -0.16618453695418986, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.024394702166318893}, {"id": 77, "seek": 40136, "start": 423.88, "end": 429.24, "text": " what Neutra showed is that principles like conservation of energy that previously were", "tokens": [51490, 437, 1734, 325, 424, 4712, 307, 300, 9156, 411, 16185, 295, 2281, 300, 8046, 645, 51758], "temperature": 0.0, "avg_logprob": -0.16618453695418986, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.024394702166318893}, {"id": 78, "seek": 42924, "start": 429.24, "end": 433.36, "text": " considered to be empirical could be derived mathematically from certain symmetries, so", "tokens": [50364, 4888, 281, 312, 31886, 727, 312, 18949, 44003, 490, 1629, 14232, 302, 2244, 11, 370, 50570], "temperature": 0.0, "avg_logprob": -0.1723487704407935, "compression_ratio": 1.832, "no_speech_prob": 0.012654203921556473}, {"id": 79, "seek": 42924, "start": 433.36, "end": 439.8, "text": " symmetry of time in this case, and these ideas in a more generalist form led to what nowadays", "tokens": [50570, 25440, 295, 565, 294, 341, 1389, 11, 293, 613, 3487, 294, 257, 544, 2674, 468, 1254, 4684, 281, 437, 13434, 50892], "temperature": 0.0, "avg_logprob": -0.1723487704407935, "compression_ratio": 1.832, "no_speech_prob": 0.012654203921556473}, {"id": 80, "seek": 42924, "start": 439.8, "end": 447.08, "text": " is known as the standard model, so basically all the world can be modeled and can be derived", "tokens": [50892, 307, 2570, 382, 264, 3832, 2316, 11, 370, 1936, 439, 264, 1002, 393, 312, 37140, 293, 393, 312, 18949, 51256], "temperature": 0.0, "avg_logprob": -0.1723487704407935, "compression_ratio": 1.832, "no_speech_prob": 0.012654203921556473}, {"id": 81, "seek": 42924, "start": 447.08, "end": 452.96000000000004, "text": " from first principles of symmetry, so what I think physicists among you would call external", "tokens": [51256, 490, 700, 9156, 295, 25440, 11, 370, 437, 286, 519, 48716, 3654, 291, 576, 818, 8320, 51550], "temperature": 0.0, "avg_logprob": -0.1723487704407935, "compression_ratio": 1.832, "no_speech_prob": 0.012654203921556473}, {"id": 82, "seek": 42924, "start": 452.96000000000004, "end": 457.48, "text": " symmetry or internal symmetry, the symmetry of the spacetime, so what is called the Poincare", "tokens": [51550, 25440, 420, 6920, 25440, 11, 264, 25440, 295, 264, 39404, 9764, 11, 370, 437, 307, 1219, 264, 6165, 259, 5685, 51776], "temperature": 0.0, "avg_logprob": -0.1723487704407935, "compression_ratio": 1.832, "no_speech_prob": 0.012654203921556473}, {"id": 83, "seek": 45748, "start": 457.56, "end": 462.16, "text": " group that gives rise to Minkowski geometry of special relativity or internal symmetries", "tokens": [50368, 1594, 300, 2709, 6272, 281, 376, 475, 21866, 18426, 295, 2121, 45675, 420, 6920, 14232, 302, 2244, 50598], "temperature": 0.0, "avg_logprob": -0.16554240590518282, "compression_ratio": 1.6996197718631179, "no_speech_prob": 0.00506854010745883}, {"id": 84, "seek": 45748, "start": 462.16, "end": 467.68, "text": " of quantum fields, that's what gives rise to different forces or different interactions,", "tokens": [50598, 295, 13018, 7909, 11, 300, 311, 437, 2709, 6272, 281, 819, 5874, 420, 819, 13280, 11, 50874], "temperature": 0.0, "avg_logprob": -0.16554240590518282, "compression_ratio": 1.6996197718631179, "no_speech_prob": 0.00506854010745883}, {"id": 85, "seek": 45748, "start": 467.68, "end": 473.04, "text": " and I think nobody put it better than Philip Anderson Nobel laureate in physics that without", "tokens": [50874, 293, 286, 519, 5079, 829, 309, 1101, 813, 21144, 18768, 24611, 49469, 473, 294, 10649, 300, 1553, 51142], "temperature": 0.0, "avg_logprob": -0.16554240590518282, "compression_ratio": 1.6996197718631179, "no_speech_prob": 0.00506854010745883}, {"id": 86, "seek": 45748, "start": 473.04, "end": 480.88, "text": " or with only slightly overstating, you can say that physics is the study of symmetry,", "tokens": [51142, 420, 365, 787, 4748, 48834, 990, 11, 291, 393, 584, 300, 10649, 307, 264, 2979, 295, 25440, 11, 51534], "temperature": 0.0, "avg_logprob": -0.16554240590518282, "compression_ratio": 1.6996197718631179, "no_speech_prob": 0.00506854010745883}, {"id": 87, "seek": 45748, "start": 480.88, "end": 486.84000000000003, "text": " so what does it all have to do with deep learning and neural networks and machine learning", "tokens": [51534, 370, 437, 775, 309, 439, 362, 281, 360, 365, 2452, 2539, 293, 18161, 9590, 293, 3479, 2539, 51832], "temperature": 0.0, "avg_logprob": -0.16554240590518282, "compression_ratio": 1.6996197718631179, "no_speech_prob": 0.00506854010745883}, {"id": 88, "seek": 48684, "start": 486.84, "end": 494.67999999999995, "text": " in general, so let's do maybe a brief detour into the history of machine learning or artificial", "tokens": [50364, 294, 2674, 11, 370, 718, 311, 360, 1310, 257, 5353, 1141, 396, 666, 264, 2503, 295, 3479, 2539, 420, 11677, 50756], "temperature": 0.0, "avg_logprob": -0.17358942729670826, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.0071844435296952724}, {"id": 89, "seek": 48684, "start": 494.67999999999995, "end": 500.71999999999997, "text": " intelligence, so the term artificial intelligence comes from these people, the Dartmouth conference", "tokens": [50756, 7599, 11, 370, 264, 1433, 11677, 7599, 1487, 490, 613, 561, 11, 264, 47883, 7586, 51058], "temperature": 0.0, "avg_logprob": -0.17358942729670826, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.0071844435296952724}, {"id": 90, "seek": 48684, "start": 500.71999999999997, "end": 508.35999999999996, "text": " that happened in 1956, organized by McCarthy and others, and you can see them, some very", "tokens": [51058, 300, 2011, 294, 46379, 11, 9983, 538, 44085, 293, 2357, 11, 293, 291, 393, 536, 552, 11, 512, 588, 51440], "temperature": 0.0, "avg_logprob": -0.17358942729670826, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.0071844435296952724}, {"id": 91, "seek": 48684, "start": 508.35999999999996, "end": 512.52, "text": " prominent figures sitting here, so this is for example, this is Claude Shannon and this", "tokens": [51440, 17034, 9624, 3798, 510, 11, 370, 341, 307, 337, 1365, 11, 341, 307, 12947, 2303, 28974, 293, 341, 51648], "temperature": 0.0, "avg_logprob": -0.17358942729670826, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.0071844435296952724}, {"id": 92, "seek": 51252, "start": 513.4, "end": 519.24, "text": " is Marvin Minsky who would become all very important scientists, and historically apparently", "tokens": [50408, 307, 48722, 376, 44153, 567, 576, 1813, 439, 588, 1021, 7708, 11, 293, 16180, 7970, 50700], "temperature": 0.0, "avg_logprob": -0.1697516765409303, "compression_ratio": 1.7601476014760147, "no_speech_prob": 0.013166019693017006}, {"id": 93, "seek": 51252, "start": 519.24, "end": 524.28, "text": " the term artificial intelligence was introduced to kind of distance themselves and not to be", "tokens": [50700, 264, 1433, 11677, 7599, 390, 7268, 281, 733, 295, 4560, 2969, 293, 406, 281, 312, 50952], "temperature": 0.0, "avg_logprob": -0.1697516765409303, "compression_ratio": 1.7601476014760147, "no_speech_prob": 0.013166019693017006}, {"id": 94, "seek": 51252, "start": 524.28, "end": 530.4399999999999, "text": " in the shadow of the expert of that time who was Norbert Wiener who introduced the term cybernetics,", "tokens": [50952, 294, 264, 8576, 295, 264, 5844, 295, 300, 565, 567, 390, 6966, 4290, 343, 1053, 260, 567, 7268, 264, 1433, 13411, 7129, 1167, 11, 51260], "temperature": 0.0, "avg_logprob": -0.1697516765409303, "compression_ratio": 1.7601476014760147, "no_speech_prob": 0.013166019693017006}, {"id": 95, "seek": 51252, "start": 531.0, "end": 535.88, "text": " which I think is still used, I think here in Poland it's probably still used as a kind of", "tokens": [51288, 597, 286, 519, 307, 920, 1143, 11, 286, 519, 510, 294, 15950, 309, 311, 1391, 920, 1143, 382, 257, 733, 295, 51532], "temperature": 0.0, "avg_logprob": -0.1697516765409303, "compression_ratio": 1.7601476014760147, "no_speech_prob": 0.013166019693017006}, {"id": 96, "seek": 51252, "start": 535.88, "end": 540.84, "text": " overarching term for everything that it has to do with computer science and artificial intelligence,", "tokens": [51532, 45501, 1433, 337, 1203, 300, 309, 575, 281, 360, 365, 3820, 3497, 293, 11677, 7599, 11, 51780], "temperature": 0.0, "avg_logprob": -0.1697516765409303, "compression_ratio": 1.7601476014760147, "no_speech_prob": 0.013166019693017006}, {"id": 97, "seek": 54084, "start": 541.48, "end": 545.8000000000001, "text": " so around that time there were many works that tried to understand how the brain works,", "tokens": [50396, 370, 926, 300, 565, 456, 645, 867, 1985, 300, 3031, 281, 1223, 577, 264, 3567, 1985, 11, 50612], "temperature": 0.0, "avg_logprob": -0.08293924548409203, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.0019952701404690742}, {"id": 98, "seek": 54084, "start": 545.8000000000001, "end": 550.76, "text": " so I think at that time it was already understood that somehow our intelligence is concentrated", "tokens": [50612, 370, 286, 519, 412, 300, 565, 309, 390, 1217, 7320, 300, 6063, 527, 7599, 307, 21321, 50860], "temperature": 0.0, "avg_logprob": -0.08293924548409203, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.0019952701404690742}, {"id": 99, "seek": 54084, "start": 550.76, "end": 557.72, "text": " in the brain, so models for neural networks, probably the most famous one is by Frank Rosenblatt,", "tokens": [50860, 294, 264, 3567, 11, 370, 5245, 337, 18161, 9590, 11, 1391, 264, 881, 4618, 472, 307, 538, 6823, 33630, 5199, 1591, 11, 51208], "temperature": 0.0, "avg_logprob": -0.08293924548409203, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.0019952701404690742}, {"id": 100, "seek": 54084, "start": 557.72, "end": 565.4000000000001, "text": " the so-called perceptron, but there are models before that, and he was able to show that was in", "tokens": [51208, 264, 370, 12, 11880, 43276, 2044, 11, 457, 456, 366, 5245, 949, 300, 11, 293, 415, 390, 1075, 281, 855, 300, 390, 294, 51592], "temperature": 0.0, "avg_logprob": -0.08293924548409203, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.0019952701404690742}, {"id": 101, "seek": 56540, "start": 565.4, "end": 571.9599999999999, "text": " the 50s, so these models had to be implemented in analog hardware, he was able to show that he", "tokens": [50364, 264, 2625, 82, 11, 370, 613, 5245, 632, 281, 312, 12270, 294, 16660, 8837, 11, 415, 390, 1075, 281, 855, 300, 415, 50692], "temperature": 0.0, "avg_logprob": -0.08840008648959073, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.018463710322976112}, {"id": 102, "seek": 56540, "start": 571.9599999999999, "end": 577.8, "text": " can solve some simple pattern recognition problems with these neural networks, and it was extremely", "tokens": [50692, 393, 5039, 512, 2199, 5102, 11150, 2740, 365, 613, 18161, 9590, 11, 293, 309, 390, 4664, 50984], "temperature": 0.0, "avg_logprob": -0.08840008648959073, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.018463710322976112}, {"id": 103, "seek": 56540, "start": 577.8, "end": 582.68, "text": " remarkable at that time, so there were articles in the popular press like the New Yorker saying", "tokens": [50984, 12802, 412, 300, 565, 11, 370, 456, 645, 11290, 294, 264, 3743, 1886, 411, 264, 1873, 3609, 260, 1566, 51228], "temperature": 0.0, "avg_logprob": -0.08840008648959073, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.018463710322976112}, {"id": 104, "seek": 56540, "start": 582.68, "end": 588.36, "text": " that this is the first serious rival to the human brain ever devised, so I think you can only smile,", "tokens": [51228, 300, 341, 307, 264, 700, 3156, 16286, 281, 264, 1952, 3567, 1562, 1905, 2640, 11, 370, 286, 519, 291, 393, 787, 7563, 11, 51512], "temperature": 0.0, "avg_logprob": -0.08840008648959073, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.018463710322976112}, {"id": 105, "seek": 56540, "start": 588.36, "end": 593.48, "text": " I can hear you laughing, and it's remarkable machines capable of what amounts to thought,", "tokens": [51512, 286, 393, 1568, 291, 5059, 11, 293, 309, 311, 12802, 8379, 8189, 295, 437, 11663, 281, 1194, 11, 51768], "temperature": 0.0, "avg_logprob": -0.08840008648959073, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.018463710322976112}, {"id": 106, "seek": 59348, "start": 593.48, "end": 598.6, "text": " so that's according to New Yorker was the perceptron, and there was a little bit of", "tokens": [50364, 370, 300, 311, 4650, 281, 1873, 3609, 260, 390, 264, 43276, 2044, 11, 293, 456, 390, 257, 707, 857, 295, 50620], "temperature": 0.0, "avg_logprob": -0.15618669575658337, "compression_ratio": 1.6289752650176679, "no_speech_prob": 0.0018644207157194614}, {"id": 107, "seek": 59348, "start": 598.6, "end": 605.5600000000001, "text": " hype, as you probably know, this MIT computer vision summer project, so they thought that they", "tokens": [50620, 24144, 11, 382, 291, 1391, 458, 11, 341, 13100, 3820, 5201, 4266, 1716, 11, 370, 436, 1194, 300, 436, 50968], "temperature": 0.0, "avg_logprob": -0.15618669575658337, "compression_ratio": 1.6289752650176679, "no_speech_prob": 0.0018644207157194614}, {"id": 108, "seek": 59348, "start": 605.5600000000001, "end": 610.52, "text": " would be able to model a large part of the visual system over one summer, of course, we're still", "tokens": [50968, 576, 312, 1075, 281, 2316, 257, 2416, 644, 295, 264, 5056, 1185, 670, 472, 4266, 11, 295, 1164, 11, 321, 434, 920, 51216], "temperature": 0.0, "avg_logprob": -0.15618669575658337, "compression_ratio": 1.6289752650176679, "no_speech_prob": 0.0018644207157194614}, {"id": 109, "seek": 59348, "start": 610.52, "end": 617.16, "text": " working on these problems 50 years after, and there is no end to it, but also at MIT these two", "tokens": [51216, 1364, 322, 613, 2740, 2625, 924, 934, 11, 293, 456, 307, 572, 917, 281, 309, 11, 457, 611, 412, 13100, 613, 732, 51548], "temperature": 0.0, "avg_logprob": -0.15618669575658337, "compression_ratio": 1.6289752650176679, "no_speech_prob": 0.0018644207157194614}, {"id": 110, "seek": 59348, "start": 617.16, "end": 622.04, "text": " guys, so one of them appeared already in the picture, Marvin Minsky in the same work, they", "tokens": [51548, 1074, 11, 370, 472, 295, 552, 8516, 1217, 294, 264, 3036, 11, 48722, 376, 44153, 294, 264, 912, 589, 11, 436, 51792], "temperature": 0.0, "avg_logprob": -0.15618669575658337, "compression_ratio": 1.6289752650176679, "no_speech_prob": 0.0018644207157194614}, {"id": 111, "seek": 62204, "start": 622.04, "end": 627.4, "text": " published a highly famous or infamous, if you want, book called The Perceptrons, where they", "tokens": [50364, 6572, 257, 5405, 4618, 420, 30769, 11, 498, 291, 528, 11, 1446, 1219, 440, 3026, 1336, 13270, 11, 689, 436, 50632], "temperature": 0.0, "avg_logprob": -0.12641945117857398, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.003647957928478718}, {"id": 112, "seek": 62204, "start": 628.28, "end": 635.64, "text": " introduced mathematical analysis of these networks proposed by Rosenblatt, and they showed, for", "tokens": [50676, 7268, 18894, 5215, 295, 613, 9590, 10348, 538, 33630, 5199, 1591, 11, 293, 436, 4712, 11, 337, 51044], "temperature": 0.0, "avg_logprob": -0.12641945117857398, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.003647957928478718}, {"id": 113, "seek": 62204, "start": 635.64, "end": 640.52, "text": " example, one example that is taken from this book, that a very simple logical function like", "tokens": [51044, 1365, 11, 472, 1365, 300, 307, 2726, 490, 341, 1446, 11, 300, 257, 588, 2199, 14978, 2445, 411, 51288], "temperature": 0.0, "avg_logprob": -0.12641945117857398, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.003647957928478718}, {"id": 114, "seek": 62204, "start": 640.52, "end": 646.76, "text": " exclusive or could not actually be implemented as a perceptron, so these patterns are not", "tokens": [51288, 13005, 420, 727, 406, 767, 312, 12270, 382, 257, 43276, 2044, 11, 370, 613, 8294, 366, 406, 51600], "temperature": 0.0, "avg_logprob": -0.12641945117857398, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.003647957928478718}, {"id": 115, "seek": 64676, "start": 647.4, "end": 653.8, "text": " linearly separable, so that was a very harsh criticism for the models, and some people say", "tokens": [50396, 43586, 3128, 712, 11, 370, 300, 390, 257, 588, 14897, 15835, 337, 264, 5245, 11, 293, 512, 561, 584, 50716], "temperature": 0.0, "avg_logprob": -0.09869267313103927, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.014668624848127365}, {"id": 116, "seek": 64676, "start": 654.36, "end": 659.56, "text": " in retrospective that this was what triggered the AI winter, so some people even say that there might", "tokens": [50744, 294, 34997, 488, 300, 341, 390, 437, 21710, 264, 7318, 6355, 11, 370, 512, 561, 754, 584, 300, 456, 1062, 51004], "temperature": 0.0, "avg_logprob": -0.09869267313103927, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.014668624848127365}, {"id": 117, "seek": 64676, "start": 659.56, "end": 666.4399999999999, "text": " have been some personal animosity between Rosenblatt and Minsky, they went to the same high school,", "tokens": [51004, 362, 668, 512, 2973, 2383, 20373, 1296, 33630, 5199, 1591, 293, 376, 44153, 11, 436, 1437, 281, 264, 912, 1090, 1395, 11, 51348], "temperature": 0.0, "avg_logprob": -0.09869267313103927, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.014668624848127365}, {"id": 118, "seek": 64676, "start": 666.4399999999999, "end": 674.2, "text": " and Rosenblatt also died in a boating accident, so as far as to suggest that it was a suicide,", "tokens": [51348, 293, 33630, 5199, 1591, 611, 4539, 294, 257, 748, 990, 6398, 11, 370, 382, 1400, 382, 281, 3402, 300, 309, 390, 257, 12308, 11, 51736], "temperature": 0.0, "avg_logprob": -0.09869267313103927, "compression_ratio": 1.6973684210526316, "no_speech_prob": 0.014668624848127365}, {"id": 119, "seek": 67420, "start": 674.9200000000001, "end": 684.2, "text": " well, the story is probably much more mundane, so the funding that was cut by government agencies", "tokens": [50400, 731, 11, 264, 1657, 307, 1391, 709, 544, 43497, 11, 370, 264, 6137, 300, 390, 1723, 538, 2463, 9504, 50864], "temperature": 0.0, "avg_logprob": -0.1352569731799039, "compression_ratio": 1.6266094420600858, "no_speech_prob": 0.0049507892690598965}, {"id": 120, "seek": 67420, "start": 684.2, "end": 689.24, "text": " like DARPA was more related to them being more pragmatic and budget restricted, and that had an", "tokens": [50864, 411, 49274, 10297, 390, 544, 4077, 281, 552, 885, 544, 46904, 293, 4706, 20608, 11, 293, 300, 632, 364, 51116], "temperature": 0.0, "avg_logprob": -0.1352569731799039, "compression_ratio": 1.6266094420600858, "no_speech_prob": 0.0049507892690598965}, {"id": 121, "seek": 67420, "start": 689.24, "end": 694.2, "text": " impact on the field that coincided with the publication of the book, but if you look at the", "tokens": [51116, 2712, 322, 264, 2519, 300, 13001, 2112, 365, 264, 19953, 295, 264, 1446, 11, 457, 498, 291, 574, 412, 264, 51364], "temperature": 0.0, "avg_logprob": -0.1352569731799039, "compression_ratio": 1.6266094420600858, "no_speech_prob": 0.0049507892690598965}, {"id": 122, "seek": 67420, "start": 696.0400000000001, "end": 701.1600000000001, "text": " substance of the problem, what Minsky and Pebert called in their book Perceptron was actually", "tokens": [51456, 12961, 295, 264, 1154, 11, 437, 376, 44153, 293, 2396, 4290, 1219, 294, 641, 1446, 3026, 1336, 2044, 390, 767, 51712], "temperature": 0.0, "avg_logprob": -0.1352569731799039, "compression_ratio": 1.6266094420600858, "no_speech_prob": 0.0049507892690598965}, {"id": 123, "seek": 70116, "start": 701.16, "end": 707.0, "text": " not exactly an architecture that was devised by Rosenblatt, so they actually clearly stated, so", "tokens": [50364, 406, 2293, 364, 9482, 300, 390, 1905, 2640, 538, 33630, 5199, 1591, 11, 370, 436, 767, 4448, 11323, 11, 370, 50656], "temperature": 0.0, "avg_logprob": -0.11346702932197357, "compression_ratio": 1.7378277153558053, "no_speech_prob": 0.004037876147776842}, {"id": 124, "seek": 70116, "start": 707.0, "end": 712.12, "text": " they refer to this architecture as simple perceptrons, and this is what we now typically", "tokens": [50656, 436, 2864, 281, 341, 9482, 382, 2199, 43276, 13270, 11, 293, 341, 307, 437, 321, 586, 5850, 50912], "temperature": 0.0, "avg_logprob": -0.11346702932197357, "compression_ratio": 1.7378277153558053, "no_speech_prob": 0.004037876147776842}, {"id": 125, "seek": 70116, "start": 712.12, "end": 717.48, "text": " understand by this term, so it's, as you know, it's just a linear combination of the input", "tokens": [50912, 1223, 538, 341, 1433, 11, 370, 309, 311, 11, 382, 291, 458, 11, 309, 311, 445, 257, 8213, 6562, 295, 264, 4846, 51180], "temperature": 0.0, "avg_logprob": -0.11346702932197357, "compression_ratio": 1.7378277153558053, "no_speech_prob": 0.004037876147776842}, {"id": 126, "seek": 70116, "start": 717.48, "end": 722.68, "text": " coordinates with learnable weights that go through a nonlinear activation, typically sigmoid or", "tokens": [51180, 21056, 365, 1466, 712, 17443, 300, 352, 807, 257, 2107, 28263, 24433, 11, 5850, 4556, 3280, 327, 420, 51440], "temperature": 0.0, "avg_logprob": -0.11346702932197357, "compression_ratio": 1.7378277153558053, "no_speech_prob": 0.004037876147776842}, {"id": 127, "seek": 70116, "start": 724.36, "end": 729.0799999999999, "text": " in simple cases sine function, but what is also very important, they were probably the first", "tokens": [51524, 294, 2199, 3331, 18609, 2445, 11, 457, 437, 307, 611, 588, 1021, 11, 436, 645, 1391, 264, 700, 51760], "temperature": 0.0, "avg_logprob": -0.11346702932197357, "compression_ratio": 1.7378277153558053, "no_speech_prob": 0.004037876147776842}, {"id": 128, "seek": 72908, "start": 729.08, "end": 734.5200000000001, "text": " ones, at least to my knowledge, to use geometric approaches to machine learning problems, so", "tokens": [50364, 2306, 11, 412, 1935, 281, 452, 3601, 11, 281, 764, 33246, 11587, 281, 3479, 2539, 2740, 11, 370, 50636], "temperature": 0.0, "avg_logprob": -0.10367645990280878, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.0020831450819969177}, {"id": 129, "seek": 72908, "start": 734.5200000000001, "end": 740.2800000000001, "text": " they, for example, formulated this group invariance theorem that tell in which or to what kind of", "tokens": [50636, 436, 11, 337, 1365, 11, 48936, 341, 1594, 33270, 719, 20904, 300, 980, 294, 597, 420, 281, 437, 733, 295, 50924], "temperature": 0.0, "avg_logprob": -0.10367645990280878, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.0020831450819969177}, {"id": 130, "seek": 72908, "start": 740.2800000000001, "end": 745.4000000000001, "text": " transformations in the input patterns the neural network will be invariant, and another interesting", "tokens": [50924, 34852, 294, 264, 4846, 8294, 264, 18161, 3209, 486, 312, 33270, 394, 11, 293, 1071, 1880, 51180], "temperature": 0.0, "avg_logprob": -0.10367645990280878, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.0020831450819969177}, {"id": 131, "seek": 72908, "start": 745.4000000000001, "end": 749.72, "text": " thing actually the subtitle of the book is an introduction to computational geometry, so that", "tokens": [51180, 551, 767, 264, 30706, 306, 295, 264, 1446, 307, 364, 9339, 281, 28270, 18426, 11, 370, 300, 51396], "temperature": 0.0, "avg_logprob": -0.10367645990280878, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.0020831450819969177}, {"id": 132, "seek": 72908, "start": 749.72, "end": 754.6800000000001, "text": " was the sixties, so this term didn't exist, they actually introduced it, and one of the reviews", "tokens": [51396, 390, 264, 13074, 530, 11, 370, 341, 1433, 994, 380, 2514, 11, 436, 767, 7268, 309, 11, 293, 472, 295, 264, 10229, 51644], "temperature": 0.0, "avg_logprob": -0.10367645990280878, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.0020831450819969177}, {"id": 133, "seek": 75468, "start": 754.68, "end": 760.5999999999999, "text": " of the book that was critical was asking whether this is just some kind of new mathematical fad", "tokens": [50364, 295, 264, 1446, 300, 390, 4924, 390, 3365, 1968, 341, 307, 445, 512, 733, 295, 777, 18894, 283, 345, 50660], "temperature": 0.0, "avg_logprob": -0.09383515750660616, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.0019059140468016267}, {"id": 134, "seek": 75468, "start": 760.5999999999999, "end": 766.1999999999999, "text": " that will go away a few years after, and you probably know computational geometry is now", "tokens": [50660, 300, 486, 352, 1314, 257, 1326, 924, 934, 11, 293, 291, 1391, 458, 28270, 18426, 307, 586, 50940], "temperature": 0.0, "avg_logprob": -0.09383515750660616, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.0019059140468016267}, {"id": 135, "seek": 75468, "start": 766.1999999999999, "end": 772.1999999999999, "text": " very well established field, so it has remained there for a long time, but if you go into the", "tokens": [50940, 588, 731, 7545, 2519, 11, 370, 309, 575, 12780, 456, 337, 257, 938, 565, 11, 457, 498, 291, 352, 666, 264, 51240], "temperature": 0.0, "avg_logprob": -0.09383515750660616, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.0019059140468016267}, {"id": 136, "seek": 75468, "start": 772.1999999999999, "end": 777.4799999999999, "text": " substance of the discussion is actually the question is what kind of classes of functions", "tokens": [51240, 12961, 295, 264, 5017, 307, 767, 264, 1168, 307, 437, 733, 295, 5359, 295, 6828, 51504], "temperature": 0.0, "avg_logprob": -0.09383515750660616, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.0019059140468016267}, {"id": 137, "seek": 75468, "start": 777.4799999999999, "end": 781.8, "text": " these neural networks could represent, right, so what is what is called the expressive power,", "tokens": [51504, 613, 18161, 9590, 727, 2906, 11, 558, 11, 370, 437, 307, 437, 307, 1219, 264, 40189, 1347, 11, 51720], "temperature": 0.0, "avg_logprob": -0.09383515750660616, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.0019059140468016267}, {"id": 138, "seek": 78180, "start": 782.3599999999999, "end": 787.56, "text": " and there were results at that time, so coming from mathematicians in particular", "tokens": [50392, 293, 456, 645, 3542, 412, 300, 565, 11, 370, 1348, 490, 32811, 2567, 294, 1729, 50652], "temperature": 0.0, "avg_logprob": -0.09926437750095274, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.0020419771317392588}, {"id": 139, "seek": 78180, "start": 788.68, "end": 794.92, "text": " Komogorov and Arnold working in the Soviet Union that claim that you can take multi-dimensional", "tokens": [50708, 14286, 664, 284, 5179, 293, 30406, 1364, 294, 264, 11348, 8133, 300, 3932, 300, 291, 393, 747, 4825, 12, 18759, 51020], "temperature": 0.0, "avg_logprob": -0.09926437750095274, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.0020419771317392588}, {"id": 140, "seek": 78180, "start": 794.92, "end": 799.64, "text": " functions and decompose them in this way, and basically any function could be decomposed in", "tokens": [51020, 6828, 293, 22867, 541, 552, 294, 341, 636, 11, 293, 1936, 604, 2445, 727, 312, 22867, 1744, 294, 51256], "temperature": 0.0, "avg_logprob": -0.09926437750095274, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.0020419771317392588}, {"id": 141, "seek": 78180, "start": 799.64, "end": 805.64, "text": " this way, so these kind of results are known as universal approximation, they go probably as far", "tokens": [51256, 341, 636, 11, 370, 613, 733, 295, 3542, 366, 2570, 382, 11455, 28023, 11, 436, 352, 1391, 382, 1400, 51556], "temperature": 0.0, "avg_logprob": -0.09926437750095274, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.0020419771317392588}, {"id": 142, "seek": 80564, "start": 805.72, "end": 811.3199999999999, "text": " as the thirteenth problem formulated by David Hilbert, and the modern statements of these", "tokens": [50368, 382, 264, 258, 347, 46897, 1154, 48936, 538, 4389, 19914, 4290, 11, 293, 264, 4363, 12363, 295, 613, 50648], "temperature": 0.0, "avg_logprob": -0.13112003191382485, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.010867689736187458}, {"id": 143, "seek": 80564, "start": 811.88, "end": 817.48, "text": " theorems are usually attributed to Seybenko and Hornig, these are late eighties, early nineties", "tokens": [50676, 10299, 2592, 366, 2673, 30976, 281, 1100, 88, 1799, 4093, 293, 31792, 328, 11, 613, 366, 3469, 3180, 530, 11, 2440, 9616, 43469, 50956], "temperature": 0.0, "avg_logprob": -0.13112003191382485, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.010867689736187458}, {"id": 144, "seek": 80564, "start": 817.48, "end": 823.88, "text": " that are specific to deep neural networks, so what these results say is that if you have not a", "tokens": [50956, 300, 366, 2685, 281, 2452, 18161, 9590, 11, 370, 437, 613, 3542, 584, 307, 300, 498, 291, 362, 406, 257, 51276], "temperature": 0.0, "avg_logprob": -0.13112003191382485, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.010867689736187458}, {"id": 145, "seek": 80564, "start": 823.88, "end": 828.68, "text": " single layer of perception, but two layers like this, then you can approximate any continuous", "tokens": [51276, 2167, 4583, 295, 12860, 11, 457, 732, 7914, 411, 341, 11, 550, 291, 393, 30874, 604, 10957, 51516], "temperature": 0.0, "avg_logprob": -0.13112003191382485, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.010867689736187458}, {"id": 146, "seek": 80564, "start": 828.68, "end": 834.76, "text": " function to any desired accuracy, so the results it's a class of results, it's not a single result,", "tokens": [51516, 2445, 281, 604, 14721, 14170, 11, 370, 264, 3542, 309, 311, 257, 1508, 295, 3542, 11, 309, 311, 406, 257, 2167, 1874, 11, 51820], "temperature": 0.0, "avg_logprob": -0.13112003191382485, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.010867689736187458}, {"id": 147, "seek": 83476, "start": 835.24, "end": 840.52, "text": " but roughly the way that we can understand it is that with just a configuration like this,", "tokens": [50388, 457, 9810, 264, 636, 300, 321, 393, 1223, 309, 307, 300, 365, 445, 257, 11694, 411, 341, 11, 50652], "temperature": 0.0, "avg_logprob": -0.1118146484973384, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.0016350969672203064}, {"id": 148, "seek": 83476, "start": 840.52, "end": 845.48, "text": " with just two neurons like this, you can approximate, you can represent a step function,", "tokens": [50652, 365, 445, 732, 22027, 411, 341, 11, 291, 393, 30874, 11, 291, 393, 2906, 257, 1823, 2445, 11, 50900], "temperature": 0.0, "avg_logprob": -0.1118146484973384, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.0016350969672203064}, {"id": 149, "seek": 83476, "start": 846.12, "end": 849.88, "text": " once you can represent step functions, you can decompose a continuous function into", "tokens": [50932, 1564, 291, 393, 2906, 1823, 6828, 11, 291, 393, 22867, 541, 257, 10957, 2445, 666, 51120], "temperature": 0.0, "avg_logprob": -0.1118146484973384, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.0016350969672203064}, {"id": 150, "seek": 83476, "start": 849.88, "end": 854.6, "text": " tiny little steps, so the proof is slightly more involved, but that's that's roughly the idea.", "tokens": [51120, 5870, 707, 4439, 11, 370, 264, 8177, 307, 4748, 544, 3288, 11, 457, 300, 311, 300, 311, 9810, 264, 1558, 13, 51356], "temperature": 0.0, "avg_logprob": -0.1118146484973384, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.0016350969672203064}, {"id": 151, "seek": 83476, "start": 855.88, "end": 861.4, "text": " Now it's sort of constructive proof, and there are different versions for limited widths or", "tokens": [51420, 823, 309, 311, 1333, 295, 30223, 8177, 11, 293, 456, 366, 819, 9606, 337, 5567, 11402, 82, 420, 51696], "temperature": 0.0, "avg_logprob": -0.1118146484973384, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.0016350969672203064}, {"id": 152, "seek": 86140, "start": 861.4, "end": 866.52, "text": " limited depths, it's sort of constructive proof, so it doesn't tell you how, so it's an existence", "tokens": [50364, 5567, 28439, 11, 309, 311, 1333, 295, 30223, 8177, 11, 370, 309, 1177, 380, 980, 291, 577, 11, 370, 309, 311, 364, 9123, 50620], "temperature": 0.0, "avg_logprob": -0.11950997953061704, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0011027669534087181}, {"id": 153, "seek": 86140, "start": 866.52, "end": 871.56, "text": " result, so it tells you that you can find a neural network with potentially a very large", "tokens": [50620, 1874, 11, 370, 309, 5112, 291, 300, 291, 393, 915, 257, 18161, 3209, 365, 7263, 257, 588, 2416, 50872], "temperature": 0.0, "avg_logprob": -0.11950997953061704, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0011027669534087181}, {"id": 154, "seek": 86140, "start": 871.56, "end": 876.84, "text": " number of neurons that approximate functions, and if you look at machine learning problems,", "tokens": [50872, 1230, 295, 22027, 300, 30874, 6828, 11, 293, 498, 291, 574, 412, 3479, 2539, 2740, 11, 51136], "temperature": 0.0, "avg_logprob": -0.11950997953061704, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0011027669534087181}, {"id": 155, "seek": 86140, "start": 876.84, "end": 881.3199999999999, "text": " so in a very maybe simple and naive setting, like classifying images of cats and dogs,", "tokens": [51136, 370, 294, 257, 588, 1310, 2199, 293, 29052, 3287, 11, 411, 1508, 5489, 5267, 295, 11111, 293, 7197, 11, 51360], "temperature": 0.0, "avg_logprob": -0.11950997953061704, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0011027669534087181}, {"id": 156, "seek": 86140, "start": 882.76, "end": 887.4, "text": " basically you can think of it as some cynicism say that deep learning is a curve fitting problem,", "tokens": [51432, 1936, 291, 393, 519, 295, 309, 382, 512, 28365, 26356, 584, 300, 2452, 2539, 307, 257, 7605, 15669, 1154, 11, 51664], "temperature": 0.0, "avg_logprob": -0.11950997953061704, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0011027669534087181}, {"id": 157, "seek": 88740, "start": 887.48, "end": 892.4399999999999, "text": " so it's multi-dimensional curve fitting, so there is some kind of black box where you put", "tokens": [50368, 370, 309, 311, 4825, 12, 18759, 7605, 15669, 11, 370, 456, 307, 512, 733, 295, 2211, 2424, 689, 291, 829, 50616], "temperature": 0.0, "avg_logprob": -0.12315035320463635, "compression_ratio": 1.6386861313868613, "no_speech_prob": 0.0010329188080504537}, {"id": 158, "seek": 88740, "start": 894.36, "end": 899.16, "text": " some something that acts as a universal approximator, so some sufficiently rich", "tokens": [50712, 512, 746, 300, 10672, 382, 257, 11455, 8542, 1639, 11, 370, 512, 31868, 4593, 50952], "temperature": 0.0, "avg_logprob": -0.12315035320463635, "compression_ratio": 1.6386861313868613, "no_speech_prob": 0.0010329188080504537}, {"id": 159, "seek": 88740, "start": 899.8, "end": 905.88, "text": " architecture, and you try to represent the function that distinguishes between cats and dogs", "tokens": [50984, 9482, 11, 293, 291, 853, 281, 2906, 264, 2445, 300, 11365, 16423, 1296, 11111, 293, 7197, 51288], "temperature": 0.0, "avg_logprob": -0.12315035320463635, "compression_ratio": 1.6386861313868613, "no_speech_prob": 0.0010329188080504537}, {"id": 160, "seek": 88740, "start": 905.88, "end": 911.16, "text": " in this way. Of course this is not a well-defined problem, if I give you a finite sample of,", "tokens": [51288, 294, 341, 636, 13, 2720, 1164, 341, 307, 406, 257, 731, 12, 37716, 1154, 11, 498, 286, 976, 291, 257, 19362, 6889, 295, 11, 51552], "temperature": 0.0, "avg_logprob": -0.12315035320463635, "compression_ratio": 1.6386861313868613, "no_speech_prob": 0.0010329188080504537}, {"id": 161, "seek": 88740, "start": 911.16, "end": 916.92, "text": " let's say these are cats and dogs, I can pass infinitely many functions through these points,", "tokens": [51552, 718, 311, 584, 613, 366, 11111, 293, 7197, 11, 286, 393, 1320, 36227, 867, 6828, 807, 613, 2793, 11, 51840], "temperature": 0.0, "avg_logprob": -0.12315035320463635, "compression_ratio": 1.6386861313868613, "no_speech_prob": 0.0010329188080504537}, {"id": 162, "seek": 91692, "start": 916.92, "end": 921.9599999999999, "text": " so I can interpolate these points in infinitely many ways, so we need somehow to restrict our", "tokens": [50364, 370, 286, 393, 44902, 473, 613, 2793, 294, 36227, 867, 2098, 11, 370, 321, 643, 6063, 281, 7694, 527, 50616], "temperature": 0.0, "avg_logprob": -0.08477360734315677, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.0004001764755230397}, {"id": 163, "seek": 91692, "start": 921.9599999999999, "end": 927.3199999999999, "text": " class of functions, and that's typically what you do by imposing some sort of regularity,", "tokens": [50616, 1508, 295, 6828, 11, 293, 300, 311, 5850, 437, 291, 360, 538, 40288, 512, 1333, 295, 3890, 507, 11, 50884], "temperature": 0.0, "avg_logprob": -0.08477360734315677, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.0004001764755230397}, {"id": 164, "seek": 91692, "start": 927.3199999999999, "end": 932.76, "text": " and mathematicians have very well understood concepts of regularity like Lipschitz continuity,", "tokens": [50884, 293, 32811, 2567, 362, 588, 731, 7320, 10392, 295, 3890, 507, 411, 441, 2600, 339, 6862, 23807, 11, 51156], "temperature": 0.0, "avg_logprob": -0.08477360734315677, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.0004001764755230397}, {"id": 165, "seek": 91692, "start": 932.76, "end": 939.3199999999999, "text": " right, so in simple case you can think of it as a function with bounded derivatives, right,", "tokens": [51156, 558, 11, 370, 294, 2199, 1389, 291, 393, 519, 295, 309, 382, 257, 2445, 365, 37498, 33733, 11, 558, 11, 51484], "temperature": 0.0, "avg_logprob": -0.08477360734315677, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.0004001764755230397}, {"id": 166, "seek": 91692, "start": 939.3199999999999, "end": 944.52, "text": " and the problem is what happens when you increase the dimensionality of your input when it doesn't", "tokens": [51484, 293, 264, 1154, 307, 437, 2314, 562, 291, 3488, 264, 10139, 1860, 295, 428, 4846, 562, 309, 1177, 380, 51744], "temperature": 0.0, "avg_logprob": -0.08477360734315677, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.0004001764755230397}, {"id": 167, "seek": 94452, "start": 944.52, "end": 948.92, "text": " look like a one-dimensional curve, but it's an n-dimensional curve, and here the results,", "tokens": [50364, 574, 411, 257, 472, 12, 18759, 7605, 11, 457, 309, 311, 364, 297, 12, 18759, 7605, 11, 293, 510, 264, 3542, 11, 50584], "temperature": 0.0, "avg_logprob": -0.10886479283238316, "compression_ratio": 1.8661417322834646, "no_speech_prob": 0.0011948718456551433}, {"id": 168, "seek": 94452, "start": 948.92, "end": 955.88, "text": " unfortunately, are not favorable because you can show that as you grow the number of dimensions,", "tokens": [50584, 7015, 11, 366, 406, 29557, 570, 291, 393, 855, 300, 382, 291, 1852, 264, 1230, 295, 12819, 11, 50932], "temperature": 0.0, "avg_logprob": -0.10886479283238316, "compression_ratio": 1.8661417322834646, "no_speech_prob": 0.0011948718456551433}, {"id": 169, "seek": 94452, "start": 955.88, "end": 962.52, "text": " the number of samples that you need to take in order to approximate a function to accuracy epsilon", "tokens": [50932, 264, 1230, 295, 10938, 300, 291, 643, 281, 747, 294, 1668, 281, 30874, 257, 2445, 281, 14170, 17889, 51264], "temperature": 0.0, "avg_logprob": -0.10886479283238316, "compression_ratio": 1.8661417322834646, "no_speech_prob": 0.0011948718456551433}, {"id": 170, "seek": 94452, "start": 962.52, "end": 967.16, "text": " grows exponentially with a number of dimensions, so this is again, it's not a single result,", "tokens": [51264, 13156, 37330, 365, 257, 1230, 295, 12819, 11, 370, 341, 307, 797, 11, 309, 311, 406, 257, 2167, 1874, 11, 51496], "temperature": 0.0, "avg_logprob": -0.10886479283238316, "compression_ratio": 1.8661417322834646, "no_speech_prob": 0.0011948718456551433}, {"id": 171, "seek": 94452, "start": 967.16, "end": 973.56, "text": " it's a class of phenomena that are called the curse of dimensionality, so it's a statistical or", "tokens": [51496, 309, 311, 257, 1508, 295, 22004, 300, 366, 1219, 264, 17139, 295, 10139, 1860, 11, 370, 309, 311, 257, 22820, 420, 51816], "temperature": 0.0, "avg_logprob": -0.10886479283238316, "compression_ratio": 1.8661417322834646, "no_speech_prob": 0.0011948718456551433}, {"id": 172, "seek": 97356, "start": 973.56, "end": 979.56, "text": " geometric phenomena that explains how functions behave in high dimensions, and the term itself", "tokens": [50364, 33246, 22004, 300, 13948, 577, 6828, 15158, 294, 1090, 12819, 11, 293, 264, 1433, 2564, 50664], "temperature": 0.0, "avg_logprob": -0.13857814898857704, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.00102860142942518}, {"id": 173, "seek": 97356, "start": 979.56, "end": 986.8399999999999, "text": " actually goes back to Bellman who spoke about the curse of dimensionality in physical problems.", "tokens": [50664, 767, 1709, 646, 281, 11485, 1601, 567, 7179, 466, 264, 17139, 295, 10139, 1860, 294, 4001, 2740, 13, 51028], "temperature": 0.0, "avg_logprob": -0.13857814898857704, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.00102860142942518}, {"id": 174, "seek": 97356, "start": 987.7199999999999, "end": 993.3199999999999, "text": " Now, in these problems, simply if you think of images, right, of even something like 30 by 30", "tokens": [51072, 823, 11, 294, 613, 2740, 11, 2935, 498, 291, 519, 295, 5267, 11, 558, 11, 295, 754, 746, 411, 2217, 538, 2217, 51352], "temperature": 0.0, "avg_logprob": -0.13857814898857704, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.00102860142942518}, {"id": 175, "seek": 97356, "start": 993.3199999999999, "end": 998.76, "text": " pixels, which is probably the smallest image you can imagine, digits from the MNIST dataset,", "tokens": [51352, 18668, 11, 597, 307, 1391, 264, 16998, 3256, 291, 393, 3811, 11, 27011, 490, 264, 376, 45, 19756, 28872, 11, 51624], "temperature": 0.0, "avg_logprob": -0.13857814898857704, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.00102860142942518}, {"id": 176, "seek": 97356, "start": 998.76, "end": 1003.16, "text": " the number of dimensions will be approximately thousands, so if you count the number of samples", "tokens": [51624, 264, 1230, 295, 12819, 486, 312, 10447, 5383, 11, 370, 498, 291, 1207, 264, 1230, 295, 10938, 51844], "temperature": 0.0, "avg_logprob": -0.13857814898857704, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.00102860142942518}, {"id": 177, "seek": 100316, "start": 1003.24, "end": 1007.64, "text": " that you need to take, if you took this kind of straightforward approach, it will probably be", "tokens": [50368, 300, 291, 643, 281, 747, 11, 498, 291, 1890, 341, 733, 295, 15325, 3109, 11, 309, 486, 1391, 312, 50588], "temperature": 0.0, "avg_logprob": -0.11169858489717756, "compression_ratio": 1.7060931899641576, "no_speech_prob": 0.0025234827771782875}, {"id": 178, "seek": 100316, "start": 1007.64, "end": 1011.88, "text": " more than the number of, not only the cats or dogs on earth, but probably close to the number of", "tokens": [50588, 544, 813, 264, 1230, 295, 11, 406, 787, 264, 11111, 420, 7197, 322, 4120, 11, 457, 1391, 1998, 281, 264, 1230, 295, 50800], "temperature": 0.0, "avg_logprob": -0.11169858489717756, "compression_ratio": 1.7060931899641576, "no_speech_prob": 0.0025234827771782875}, {"id": 179, "seek": 100316, "start": 1011.88, "end": 1017.56, "text": " particles in the universe, so there are not sufficiently many animals around just to do it,", "tokens": [50800, 10007, 294, 264, 6445, 11, 370, 456, 366, 406, 31868, 867, 4882, 926, 445, 281, 360, 309, 11, 51084], "temperature": 0.0, "avg_logprob": -0.11169858489717756, "compression_ratio": 1.7060931899641576, "no_speech_prob": 0.0025234827771782875}, {"id": 180, "seek": 100316, "start": 1018.12, "end": 1024.92, "text": " and this kind of problem of curse of dimensionality, right, or you can also call it combinatorial", "tokens": [51112, 293, 341, 733, 295, 1154, 295, 17139, 295, 10139, 1860, 11, 558, 11, 420, 291, 393, 611, 818, 309, 2512, 31927, 831, 51452], "temperature": 0.0, "avg_logprob": -0.11169858489717756, "compression_ratio": 1.7060931899641576, "no_speech_prob": 0.0025234827771782875}, {"id": 181, "seek": 100316, "start": 1024.92, "end": 1030.28, "text": " explosion, was brought up in another report that is associated with the AI winter, which is the", "tokens": [51452, 15673, 11, 390, 3038, 493, 294, 1071, 2275, 300, 307, 6615, 365, 264, 7318, 6355, 11, 597, 307, 264, 51720], "temperature": 0.0, "avg_logprob": -0.11169858489717756, "compression_ratio": 1.7060931899641576, "no_speech_prob": 0.0025234827771782875}, {"id": 182, "seek": 103028, "start": 1030.28, "end": 1036.36, "text": " Lighthill report in the 70s. It was commissioned by the analogy of the DARPA, or basically the", "tokens": [50364, 8279, 17000, 2275, 294, 264, 5285, 82, 13, 467, 390, 32372, 538, 264, 21663, 295, 264, 49274, 10297, 11, 420, 1936, 264, 50668], "temperature": 0.0, "avg_logprob": -0.13184944240526222, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.0017038545338436961}, {"id": 183, "seek": 103028, "start": 1036.36, "end": 1043.48, "text": " British funding agencies, and that was the point of time where they decided to stop funding these", "tokens": [50668, 6221, 6137, 9504, 11, 293, 300, 390, 264, 935, 295, 565, 689, 436, 3047, 281, 1590, 6137, 613, 51024], "temperature": 0.0, "avg_logprob": -0.13184944240526222, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.0017038545338436961}, {"id": 184, "seek": 103028, "start": 1043.48, "end": 1050.76, "text": " crazy ideas in looking at neural networks. So what happened, of course, was this AI winter,", "tokens": [51024, 3219, 3487, 294, 1237, 412, 18161, 9590, 13, 407, 437, 2011, 11, 295, 1164, 11, 390, 341, 7318, 6355, 11, 51388], "temperature": 0.0, "avg_logprob": -0.13184944240526222, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.0017038545338436961}, {"id": 185, "seek": 103028, "start": 1050.76, "end": 1056.12, "text": " but at the same time people continued working on these architectures, and some interesting ideas", "tokens": [51388, 457, 412, 264, 912, 565, 561, 7014, 1364, 322, 613, 6331, 1303, 11, 293, 512, 1880, 3487, 51656], "temperature": 0.0, "avg_logprob": -0.13184944240526222, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.0017038545338436961}, {"id": 186, "seek": 105612, "start": 1056.12, "end": 1060.76, "text": " came from the field of neuroscience, from the study of the organization and the function of", "tokens": [50364, 1361, 490, 264, 2519, 295, 42762, 11, 490, 264, 2979, 295, 264, 4475, 293, 264, 2445, 295, 50596], "temperature": 0.0, "avg_logprob": -0.07438476880391438, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.02148452214896679}, {"id": 187, "seek": 105612, "start": 1060.76, "end": 1067.2399999999998, "text": " the visual cortex, the famous experiments in the 50s and the 60s done by Hubel and Wiesel,", "tokens": [50596, 264, 5056, 33312, 11, 264, 4618, 12050, 294, 264, 2625, 82, 293, 264, 4060, 82, 1096, 538, 18986, 338, 293, 343, 530, 338, 11, 50920], "temperature": 0.0, "avg_logprob": -0.07438476880391438, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.02148452214896679}, {"id": 188, "seek": 105612, "start": 1067.2399999999998, "end": 1072.12, "text": " a duo from Harvard that won the Nobel Prize in medicine for understanding the organization", "tokens": [50920, 257, 28127, 490, 13378, 300, 1582, 264, 24611, 22604, 294, 7195, 337, 3701, 264, 4475, 51164], "temperature": 0.0, "avg_logprob": -0.07438476880391438, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.02148452214896679}, {"id": 189, "seek": 105612, "start": 1072.12, "end": 1078.52, "text": " of the visual cortex, where what they found is that the cells in the cortex were organized", "tokens": [51164, 295, 264, 5056, 33312, 11, 689, 437, 436, 1352, 307, 300, 264, 5438, 294, 264, 33312, 645, 9983, 51484], "temperature": 0.0, "avg_logprob": -0.07438476880391438, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.02148452214896679}, {"id": 190, "seek": 105612, "start": 1078.52, "end": 1085.08, "text": " with some local shared weights, and this was reproduced by Fukushima in his famous neocognitron", "tokens": [51484, 365, 512, 2654, 5507, 17443, 11, 293, 341, 390, 11408, 1232, 538, 33043, 49754, 294, 702, 4618, 408, 905, 2912, 270, 2044, 51812], "temperature": 0.0, "avg_logprob": -0.07438476880391438, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.02148452214896679}, {"id": 191, "seek": 108508, "start": 1085.08, "end": 1092.52, "text": " paper in 1980. So the idea here was a neural network that does, it has two types of neurons,", "tokens": [50364, 3035, 294, 13626, 13, 407, 264, 1558, 510, 390, 257, 18161, 3209, 300, 775, 11, 309, 575, 732, 3467, 295, 22027, 11, 50736], "temperature": 0.0, "avg_logprob": -0.15956027507781984, "compression_ratio": 1.6650717703349283, "no_speech_prob": 0.004055015277117491}, {"id": 192, "seek": 108508, "start": 1092.52, "end": 1100.12, "text": " so neurons that he called simple neurons and neurons that he called complex neurons, so one", "tokens": [50736, 370, 22027, 300, 415, 1219, 2199, 22027, 293, 22027, 300, 415, 1219, 3997, 22027, 11, 370, 472, 51116], "temperature": 0.0, "avg_logprob": -0.15956027507781984, "compression_ratio": 1.6650717703349283, "no_speech_prob": 0.004055015277117491}, {"id": 193, "seek": 108508, "start": 1100.12, "end": 1105.24, "text": " in modern terminology that would correspond to local filters and pooling operations.", "tokens": [51116, 294, 4363, 27575, 300, 576, 6805, 281, 2654, 15995, 293, 7005, 278, 7705, 13, 51372], "temperature": 0.0, "avg_logprob": -0.15956027507781984, "compression_ratio": 1.6650717703349283, "no_speech_prob": 0.004055015277117491}, {"id": 194, "seek": 108508, "start": 1106.12, "end": 1111.8799999999999, "text": " And he worked on OCR type problems, so character recognition, and the problem,", "tokens": [51416, 400, 415, 2732, 322, 422, 18547, 2010, 2740, 11, 370, 2517, 11150, 11, 293, 264, 1154, 11, 51704], "temperature": 0.0, "avg_logprob": -0.15956027507781984, "compression_ratio": 1.6650717703349283, "no_speech_prob": 0.004055015277117491}, {"id": 195, "seek": 111188, "start": 1111.88, "end": 1118.6000000000001, "text": " of course, if you treat this type of problems with the standard perceptrons, if I give you a", "tokens": [50364, 295, 1164, 11, 498, 291, 2387, 341, 2010, 295, 2740, 365, 264, 3832, 43276, 13270, 11, 498, 286, 976, 291, 257, 50700], "temperature": 0.0, "avg_logprob": -0.11575835999988374, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.0017429615836590528}, {"id": 196, "seek": 111188, "start": 1118.6000000000001, "end": 1124.0400000000002, "text": " digit of digit three, and I move it just by one pixel, you see that the input into the neural", "tokens": [50700, 14293, 295, 14293, 1045, 11, 293, 286, 1286, 309, 445, 538, 472, 19261, 11, 291, 536, 300, 264, 4846, 666, 264, 18161, 50972], "temperature": 0.0, "avg_logprob": -0.11575835999988374, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.0017429615836590528}, {"id": 197, "seek": 111188, "start": 1124.0400000000002, "end": 1129.88, "text": " network can change dramatically, and in fact he complained that perceptrons were not by design", "tokens": [50972, 3209, 393, 1319, 17548, 11, 293, 294, 1186, 415, 33951, 300, 43276, 13270, 645, 406, 538, 1715, 51264], "temperature": 0.0, "avg_logprob": -0.11575835999988374, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.0017429615836590528}, {"id": 198, "seek": 111188, "start": 1129.88, "end": 1135.0800000000002, "text": " invariance to these translations. So his architecture actually is remarkably modern", "tokens": [51264, 33270, 719, 281, 613, 37578, 13, 407, 702, 9482, 767, 307, 37381, 4363, 51524], "temperature": 0.0, "avg_logprob": -0.11575835999988374, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.0017429615836590528}, {"id": 199, "seek": 111188, "start": 1135.0800000000002, "end": 1140.8400000000001, "text": " by modern standards, so it was seven layer networks, so I think we can call it deep by", "tokens": [51524, 538, 4363, 7787, 11, 370, 309, 390, 3407, 4583, 9590, 11, 370, 286, 519, 321, 393, 818, 309, 2452, 538, 51812], "temperature": 0.0, "avg_logprob": -0.11575835999988374, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.0017429615836590528}, {"id": 200, "seek": 114084, "start": 1140.84, "end": 1145.72, "text": " modern standards. It had local connectivity, what neuroscientists called receptive fields.", "tokens": [50364, 4363, 7787, 13, 467, 632, 2654, 21095, 11, 437, 28813, 5412, 1751, 1219, 45838, 7909, 13, 50608], "temperature": 0.0, "avg_logprob": -0.13991172834374438, "compression_ratio": 1.6177777777777778, "no_speech_prob": 0.001637557870708406}, {"id": 201, "seek": 114084, "start": 1146.9199999999998, "end": 1152.84, "text": " The filters were non-linear, and he wrote in neuroscience terminologies, so he talked about", "tokens": [50668, 440, 15995, 645, 2107, 12, 28263, 11, 293, 415, 4114, 294, 42762, 10761, 6204, 11, 370, 415, 2825, 466, 50964], "temperature": 0.0, "avg_logprob": -0.13991172834374438, "compression_ratio": 1.6177777777777778, "no_speech_prob": 0.001637557870708406}, {"id": 202, "seek": 114084, "start": 1152.84, "end": 1158.9199999999998, "text": " inhibition and activation. He had average pooling, so these are the complex layers.", "tokens": [50964, 20406, 849, 293, 24433, 13, 634, 632, 4274, 7005, 278, 11, 370, 613, 366, 264, 3997, 7914, 13, 51268], "temperature": 0.0, "avg_logprob": -0.13991172834374438, "compression_ratio": 1.6177777777777778, "no_speech_prob": 0.001637557870708406}, {"id": 203, "seek": 114084, "start": 1160.1999999999998, "end": 1167.1599999999999, "text": " He used reloactivation, so already in the late 60s that was common, but the training was not done", "tokens": [51332, 634, 1143, 319, 752, 23397, 399, 11, 370, 1217, 294, 264, 3469, 4060, 82, 300, 390, 2689, 11, 457, 264, 3097, 390, 406, 1096, 51680], "temperature": 0.0, "avg_logprob": -0.13991172834374438, "compression_ratio": 1.6177777777777778, "no_speech_prob": 0.001637557870708406}, {"id": 204, "seek": 116716, "start": 1167.16, "end": 1171.64, "text": " using backpropagation, so it was a kind of unsupervised type of clustering approach.", "tokens": [50364, 1228, 646, 79, 1513, 559, 399, 11, 370, 309, 390, 257, 733, 295, 2693, 12879, 24420, 2010, 295, 596, 48673, 3109, 13, 50588], "temperature": 0.0, "avg_logprob": -0.16469180689448804, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.00222593080252409}, {"id": 205, "seek": 116716, "start": 1171.64, "end": 1177.0, "text": " And backpropagation, again, it existed already in maybe some other forms, but this is how neural", "tokens": [50588, 400, 646, 79, 1513, 559, 399, 11, 797, 11, 309, 13135, 1217, 294, 1310, 512, 661, 6422, 11, 457, 341, 307, 577, 18161, 50856], "temperature": 0.0, "avg_logprob": -0.16469180689448804, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.00222593080252409}, {"id": 206, "seek": 116716, "start": 1177.0, "end": 1181.72, "text": " networks were trained. So Rosenblatt had a special rule for a single layer perceptron,", "tokens": [50856, 9590, 645, 8895, 13, 407, 33630, 5199, 1591, 632, 257, 2121, 4978, 337, 257, 2167, 4583, 43276, 2044, 11, 51092], "temperature": 0.0, "avg_logprob": -0.16469180689448804, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.00222593080252409}, {"id": 207, "seek": 116716, "start": 1181.72, "end": 1186.2, "text": " then there were some other methods that were developed in the 60s, and then backprop became", "tokens": [51092, 550, 456, 645, 512, 661, 7150, 300, 645, 4743, 294, 264, 4060, 82, 11, 293, 550, 646, 79, 1513, 3062, 51316], "temperature": 0.0, "avg_logprob": -0.16469180689448804, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.00222593080252409}, {"id": 208, "seek": 116716, "start": 1186.2, "end": 1190.92, "text": " really popular in the 80s, starting from the paper of Rumelhardt.", "tokens": [51316, 534, 3743, 294, 264, 4688, 82, 11, 2891, 490, 264, 3035, 295, 31963, 338, 21491, 83, 13, 51552], "temperature": 0.0, "avg_logprob": -0.16469180689448804, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.00222593080252409}, {"id": 209, "seek": 119092, "start": 1191.16, "end": 1198.44, "text": " So Lecante was just a fresh graduate at that time, and he was working on, actually,", "tokens": [50376, 407, 1456, 66, 2879, 390, 445, 257, 4451, 8080, 412, 300, 565, 11, 293, 415, 390, 1364, 322, 11, 767, 11, 50740], "temperature": 0.0, "avg_logprob": -0.19935001797146268, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0013897415483370423}, {"id": 210, "seek": 119092, "start": 1198.44, "end": 1205.0800000000002, "text": " the application of backpropagation neural networks, was interested in Neocognitron,", "tokens": [50740, 264, 3861, 295, 646, 79, 1513, 559, 399, 18161, 9590, 11, 390, 3102, 294, 1734, 905, 2912, 270, 2044, 11, 51072], "temperature": 0.0, "avg_logprob": -0.19935001797146268, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0013897415483370423}, {"id": 211, "seek": 119092, "start": 1205.0800000000002, "end": 1210.52, "text": " and he also happened to work at AT&T, which they were developing the first digital signal", "tokens": [51072, 293, 415, 611, 2011, 281, 589, 412, 8872, 5, 51, 11, 597, 436, 645, 6416, 264, 700, 4562, 6358, 51344], "temperature": 0.0, "avg_logprob": -0.19935001797146268, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0013897415483370423}, {"id": 212, "seek": 119092, "start": 1210.52, "end": 1215.72, "text": " processors at that time. So basically, there was a good application to implement on a DSP,", "tokens": [51344, 27751, 412, 300, 565, 13, 407, 1936, 11, 456, 390, 257, 665, 3861, 281, 4445, 322, 257, 15816, 47, 11, 51604], "temperature": 0.0, "avg_logprob": -0.19935001797146268, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0013897415483370423}, {"id": 213, "seek": 121572, "start": 1215.72, "end": 1222.92, "text": " and basically, he stripped down the kind of neuroscience terminology of Fukushima,", "tokens": [50364, 293, 1936, 11, 415, 33221, 760, 264, 733, 295, 42762, 27575, 295, 33043, 49754, 11, 50724], "temperature": 0.0, "avg_logprob": -0.19101982116699218, "compression_ratio": 1.634703196347032, "no_speech_prob": 0.0038476758636534214}, {"id": 214, "seek": 121572, "start": 1222.92, "end": 1227.4, "text": " replaced nonlinear filters by linear filters that could be implemented as convolutions.", "tokens": [50724, 10772, 2107, 28263, 15995, 538, 8213, 15995, 300, 727, 312, 12270, 382, 3754, 15892, 13, 50948], "temperature": 0.0, "avg_logprob": -0.19101982116699218, "compression_ratio": 1.634703196347032, "no_speech_prob": 0.0038476758636534214}, {"id": 215, "seek": 121572, "start": 1227.4, "end": 1232.68, "text": " Actually, the first paper never mentioned the term convolution, and the name came after in,", "tokens": [50948, 5135, 11, 264, 700, 3035, 1128, 2835, 264, 1433, 45216, 11, 293, 264, 1315, 1361, 934, 294, 11, 51212], "temperature": 0.0, "avg_logprob": -0.19101982116699218, "compression_ratio": 1.634703196347032, "no_speech_prob": 0.0038476758636534214}, {"id": 216, "seek": 121572, "start": 1232.68, "end": 1241.4, "text": " I think, in 89, or even later, and he was able to show in real time complex pattern recognition", "tokens": [51212, 286, 519, 11, 294, 31877, 11, 420, 754, 1780, 11, 293, 415, 390, 1075, 281, 855, 294, 957, 565, 3997, 5102, 11150, 51648], "temperature": 0.0, "avg_logprob": -0.19101982116699218, "compression_ratio": 1.634703196347032, "no_speech_prob": 0.0038476758636534214}, {"id": 217, "seek": 124140, "start": 1241.4, "end": 1245.8000000000002, "text": " tasks, such as recognition of handwritten digit, which was a difficult task at that time.", "tokens": [50364, 9608, 11, 1270, 382, 11150, 295, 1011, 26859, 14293, 11, 597, 390, 257, 2252, 5633, 412, 300, 565, 13, 50584], "temperature": 0.0, "avg_logprob": -0.13971425047015198, "compression_ratio": 1.5830388692579505, "no_speech_prob": 0.008398499339818954}, {"id": 218, "seek": 124140, "start": 1245.8000000000002, "end": 1249.88, "text": " It was actually deployed in commercial applications. They were working with banks", "tokens": [50584, 467, 390, 767, 17826, 294, 6841, 5821, 13, 814, 645, 1364, 365, 10237, 50788], "temperature": 0.0, "avg_logprob": -0.13971425047015198, "compression_ratio": 1.5830388692579505, "no_speech_prob": 0.008398499339818954}, {"id": 219, "seek": 124140, "start": 1250.92, "end": 1255.8000000000002, "text": " in the post office, and that's where the MNIST dataset comes from. But the computer vision", "tokens": [50840, 294, 264, 2183, 3398, 11, 293, 300, 311, 689, 264, 376, 45, 19756, 28872, 1487, 490, 13, 583, 264, 3820, 5201, 51084], "temperature": 0.0, "avg_logprob": -0.13971425047015198, "compression_ratio": 1.5830388692579505, "no_speech_prob": 0.008398499339818954}, {"id": 220, "seek": 124140, "start": 1255.8000000000002, "end": 1264.52, "text": " community took a different path, and the late 90s and the early 2000s, probably until the first", "tokens": [51084, 1768, 1890, 257, 819, 3100, 11, 293, 264, 3469, 4289, 82, 293, 264, 2440, 8132, 82, 11, 1391, 1826, 264, 700, 51520], "temperature": 0.0, "avg_logprob": -0.13971425047015198, "compression_ratio": 1.5830388692579505, "no_speech_prob": 0.008398499339818954}, {"id": 221, "seek": 124140, "start": 1264.52, "end": 1271.0, "text": " decade of this century, the approaches that you would typically use for image recognition", "tokens": [51520, 10378, 295, 341, 4901, 11, 264, 11587, 300, 291, 576, 5850, 764, 337, 3256, 11150, 51844], "temperature": 0.0, "avg_logprob": -0.13971425047015198, "compression_ratio": 1.5830388692579505, "no_speech_prob": 0.008398499339818954}, {"id": 222, "seek": 127100, "start": 1271.0, "end": 1278.36, "text": " were to detect some local features, then compute local feature descriptors, and then create some", "tokens": [50364, 645, 281, 5531, 512, 2654, 4122, 11, 550, 14722, 2654, 4111, 31280, 830, 11, 293, 550, 1884, 512, 50732], "temperature": 0.0, "avg_logprob": -0.12524728491754816, "compression_ratio": 1.6476868327402134, "no_speech_prob": 0.001044986303895712}, {"id": 223, "seek": 127100, "start": 1278.36, "end": 1283.0, "text": " representation that would be passed into some very simple classifier, like a support vector", "tokens": [50732, 10290, 300, 576, 312, 4678, 666, 512, 588, 2199, 1508, 9902, 11, 411, 257, 1406, 8062, 50964], "temperature": 0.0, "avg_logprob": -0.12524728491754816, "compression_ratio": 1.6476868327402134, "no_speech_prob": 0.001044986303895712}, {"id": 224, "seek": 127100, "start": 1283.0, "end": 1287.08, "text": " machine, which also were considered to be more favorable by mathematicians, because", "tokens": [50964, 3479, 11, 597, 611, 645, 4888, 281, 312, 544, 29557, 538, 32811, 2567, 11, 570, 51168], "temperature": 0.0, "avg_logprob": -0.12524728491754816, "compression_ratio": 1.6476868327402134, "no_speech_prob": 0.001044986303895712}, {"id": 225, "seek": 127100, "start": 1287.08, "end": 1292.6, "text": " you can prove, for example, global optimality results about them. So there are many papers", "tokens": [51168, 291, 393, 7081, 11, 337, 1365, 11, 4338, 5028, 1860, 3542, 466, 552, 13, 407, 456, 366, 867, 10577, 51444], "temperature": 0.0, "avg_logprob": -0.12524728491754816, "compression_ratio": 1.6476868327402134, "no_speech_prob": 0.001044986303895712}, {"id": 226, "seek": 127100, "start": 1292.6, "end": 1297.8, "text": " written, so SIFT, the scale environment feature transform, one of the most cited papers in computer", "tokens": [51444, 3720, 11, 370, 318, 12775, 51, 11, 264, 4373, 2823, 4111, 4088, 11, 472, 295, 264, 881, 30134, 10577, 294, 3820, 51704], "temperature": 0.0, "avg_logprob": -0.12524728491754816, "compression_ratio": 1.6476868327402134, "no_speech_prob": 0.001044986303895712}, {"id": 227, "seek": 129780, "start": 1297.8, "end": 1306.2, "text": " science ever, was extremely well engineered detector and feature descriptor for these tasks.", "tokens": [50364, 3497, 1562, 11, 390, 4664, 731, 38648, 25712, 293, 4111, 31280, 284, 337, 613, 9608, 13, 50784], "temperature": 0.0, "avg_logprob": -0.12707074483235678, "compression_ratio": 1.56, "no_speech_prob": 0.0017403451493009925}, {"id": 228, "seek": 129780, "start": 1307.24, "end": 1313.0, "text": " And what happened in 2012, as you probably all know, that all these carefully designed handcrafted", "tokens": [50836, 400, 437, 2011, 294, 9125, 11, 382, 291, 1391, 439, 458, 11, 300, 439, 613, 7500, 4761, 1011, 5611, 292, 51124], "temperature": 0.0, "avg_logprob": -0.12707074483235678, "compression_ratio": 1.56, "no_speech_prob": 0.0017403451493009925}, {"id": 229, "seek": 129780, "start": 1313.0, "end": 1320.9199999999998, "text": " features were beaten down by a very large margin by a convolutional neural network. So it was this", "tokens": [51124, 4122, 645, 17909, 760, 538, 257, 588, 2416, 10270, 538, 257, 45216, 304, 18161, 3209, 13, 407, 309, 390, 341, 51520], "temperature": 0.0, "avg_logprob": -0.12707074483235678, "compression_ratio": 1.56, "no_speech_prob": 0.0017403451493009925}, {"id": 230, "seek": 129780, "start": 1320.9199999999998, "end": 1327.08, "text": " lack of confidence of the computational capabilities of hardware, the GPUs, as well as availability", "tokens": [51520, 5011, 295, 6687, 295, 264, 28270, 10862, 295, 8837, 11, 264, 18407, 82, 11, 382, 731, 382, 17945, 51828], "temperature": 0.0, "avg_logprob": -0.12707074483235678, "compression_ratio": 1.56, "no_speech_prob": 0.0017403451493009925}, {"id": 231, "seek": 132708, "start": 1327.08, "end": 1332.52, "text": " of large data, the image net benchmark, which contained millions of annotated images, that", "tokens": [50364, 295, 2416, 1412, 11, 264, 3256, 2533, 18927, 11, 597, 16212, 6803, 295, 25339, 770, 5267, 11, 300, 50636], "temperature": 0.0, "avg_logprob": -0.1193141686288934, "compression_ratio": 1.6678200692041523, "no_speech_prob": 0.0007509776041842997}, {"id": 232, "seek": 132708, "start": 1332.52, "end": 1338.12, "text": " finally allowed these architectures to shine. And since then, all the best results in this", "tokens": [50636, 2721, 4350, 613, 6331, 1303, 281, 12207, 13, 400, 1670, 550, 11, 439, 264, 1151, 3542, 294, 341, 50916], "temperature": 0.0, "avg_logprob": -0.1193141686288934, "compression_ratio": 1.6678200692041523, "no_speech_prob": 0.0007509776041842997}, {"id": 233, "seek": 132708, "start": 1338.12, "end": 1344.76, "text": " benchmark were by deep learning. So if you look at the AlexNet architecture that won this benchmark", "tokens": [50916, 18927, 645, 538, 2452, 2539, 13, 407, 498, 291, 574, 412, 264, 5202, 31890, 9482, 300, 1582, 341, 18927, 51248], "temperature": 0.0, "avg_logprob": -0.1193141686288934, "compression_ratio": 1.6678200692041523, "no_speech_prob": 0.0007509776041842997}, {"id": 234, "seek": 132708, "start": 1344.76, "end": 1350.1999999999998, "text": " in 2012, it is more or less the same as what was done by Lekana. It's just slightly deeper. There are", "tokens": [51248, 294, 9125, 11, 309, 307, 544, 420, 1570, 264, 912, 382, 437, 390, 1096, 538, 441, 916, 2095, 13, 467, 311, 445, 4748, 7731, 13, 821, 366, 51520], "temperature": 0.0, "avg_logprob": -0.1193141686288934, "compression_ratio": 1.6678200692041523, "no_speech_prob": 0.0007509776041842997}, {"id": 235, "seek": 132708, "start": 1350.1999999999998, "end": 1356.9199999999998, "text": " some different architectural choices. It has way more parameters. It was trained on GPUs, which by", "tokens": [51520, 512, 819, 26621, 7994, 13, 467, 575, 636, 544, 9834, 13, 467, 390, 8895, 322, 18407, 82, 11, 597, 538, 51856], "temperature": 0.0, "avg_logprob": -0.1193141686288934, "compression_ratio": 1.6678200692041523, "no_speech_prob": 0.0007509776041842997}, {"id": 236, "seek": 135692, "start": 1356.92, "end": 1362.8400000000001, "text": " the way was not novel. GPUs were used for general purpose computing at least a decade before and", "tokens": [50364, 264, 636, 390, 406, 7613, 13, 18407, 82, 645, 1143, 337, 2674, 4334, 15866, 412, 1935, 257, 10378, 949, 293, 50660], "temperature": 0.0, "avg_logprob": -0.10743304504745307, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.004048649687319994}, {"id": 237, "seek": 135692, "start": 1362.8400000000001, "end": 1369.16, "text": " for training neural networks probably at least seven years before. So in a sense, it was a very", "tokens": [50660, 337, 3097, 18161, 9590, 1391, 412, 1935, 3407, 924, 949, 13, 407, 294, 257, 2020, 11, 309, 390, 257, 588, 50976], "temperature": 0.0, "avg_logprob": -0.10743304504745307, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.004048649687319994}, {"id": 238, "seek": 135692, "start": 1369.16, "end": 1375.8000000000002, "text": " careful engineering and application of existing ideas on a very large dataset and a very important", "tokens": [50976, 5026, 7043, 293, 3861, 295, 6741, 3487, 322, 257, 588, 2416, 28872, 293, 257, 588, 1021, 51308], "temperature": 0.0, "avg_logprob": -0.10743304504745307, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.004048649687319994}, {"id": 239, "seek": 135692, "start": 1375.8000000000002, "end": 1383.4, "text": " problem that convinced everyone. Yeah, there is a question. Sorry to go back a couple of slides,", "tokens": [51308, 1154, 300, 12561, 1518, 13, 865, 11, 456, 307, 257, 1168, 13, 4919, 281, 352, 646, 257, 1916, 295, 9788, 11, 51688], "temperature": 0.0, "avg_logprob": -0.10743304504745307, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.004048649687319994}, {"id": 240, "seek": 138340, "start": 1383.4, "end": 1391.5600000000002, "text": " but you were mentioning Fukushima's implementation and no sort of back propagation learning. So", "tokens": [50364, 457, 291, 645, 18315, 33043, 49754, 311, 11420, 293, 572, 1333, 295, 646, 38377, 2539, 13, 407, 50772], "temperature": 0.0, "avg_logprob": -0.201755510249608, "compression_ratio": 1.53125, "no_speech_prob": 0.006775506306439638}, {"id": 241, "seek": 138340, "start": 1391.5600000000002, "end": 1400.76, "text": " like was he giving some kind of neuroscience, how do you say, a proof for this? Like some kind of", "tokens": [50772, 411, 390, 415, 2902, 512, 733, 295, 42762, 11, 577, 360, 291, 584, 11, 257, 8177, 337, 341, 30, 1743, 512, 733, 295, 51232], "temperature": 0.0, "avg_logprob": -0.201755510249608, "compression_ratio": 1.53125, "no_speech_prob": 0.006775506306439638}, {"id": 242, "seek": 138340, "start": 1400.76, "end": 1407.4, "text": " happy learning or how did the... I don't remember what kind of rule he used. I think it was inspired", "tokens": [51232, 2055, 2539, 420, 577, 630, 264, 485, 286, 500, 380, 1604, 437, 733, 295, 4978, 415, 1143, 13, 286, 519, 309, 390, 7547, 51564], "temperature": 0.0, "avg_logprob": -0.201755510249608, "compression_ratio": 1.53125, "no_speech_prob": 0.006775506306439638}, {"id": 243, "seek": 140740, "start": 1407.4, "end": 1414.1200000000001, "text": " by some hypothetical ideas of how the brain learns. Yeah, but it was not back propagation.", "tokens": [50364, 538, 512, 33053, 3487, 295, 577, 264, 3567, 27152, 13, 865, 11, 457, 309, 390, 406, 646, 38377, 13, 50700], "temperature": 0.0, "avg_logprob": -0.14161776448344135, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.008972018957138062}, {"id": 244, "seek": 140740, "start": 1414.1200000000001, "end": 1418.44, "text": " He showed many other things. So he showed, for example, geometric stability, stability to noise,", "tokens": [50700, 634, 4712, 867, 661, 721, 13, 407, 415, 4712, 11, 337, 1365, 11, 33246, 11826, 11, 11826, 281, 5658, 11, 50916], "temperature": 0.0, "avg_logprob": -0.14161776448344135, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.008972018957138062}, {"id": 245, "seek": 140740, "start": 1418.44, "end": 1425.3200000000002, "text": " but again, it was in the early 80s. So the training examples were very rudimentary. So", "tokens": [50916, 457, 797, 11, 309, 390, 294, 264, 2440, 4688, 82, 13, 407, 264, 3097, 5110, 645, 588, 32109, 2328, 822, 13, 407, 51260], "temperature": 0.0, "avg_logprob": -0.14161776448344135, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.008972018957138062}, {"id": 246, "seek": 140740, "start": 1425.3200000000002, "end": 1433.64, "text": " black and white letters. Thank you. Right. So basically, all the rest is obviously history,", "tokens": [51260, 2211, 293, 2418, 7825, 13, 1044, 291, 13, 1779, 13, 407, 1936, 11, 439, 264, 1472, 307, 2745, 2503, 11, 51676], "temperature": 0.0, "avg_logprob": -0.14161776448344135, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.008972018957138062}, {"id": 247, "seek": 143364, "start": 1433.64, "end": 1440.0400000000002, "text": " right? So the people who started the deep learning thing got the Turing Award and now", "tokens": [50364, 558, 30, 407, 264, 561, 567, 1409, 264, 2452, 2539, 551, 658, 264, 314, 1345, 13894, 293, 586, 50684], "temperature": 0.0, "avg_logprob": -0.17096317291259766, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.0041409642435610294}, {"id": 248, "seek": 143364, "start": 1440.0400000000002, "end": 1446.0400000000002, "text": " very famous. And basically, this is technology that has really transformed the field both", "tokens": [50684, 588, 4618, 13, 400, 1936, 11, 341, 307, 2899, 300, 575, 534, 16894, 264, 2519, 1293, 50984], "temperature": 0.0, "avg_logprob": -0.17096317291259766, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.0041409642435610294}, {"id": 249, "seek": 143364, "start": 1446.76, "end": 1452.68, "text": " the academical subjects and the industry. So just to give you, we'll be talking about", "tokens": [51020, 264, 19267, 804, 13066, 293, 264, 3518, 13, 407, 445, 281, 976, 291, 11, 321, 603, 312, 1417, 466, 51316], "temperature": 0.0, "avg_logprob": -0.17096317291259766, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.0041409642435610294}, {"id": 250, "seek": 143364, "start": 1452.68, "end": 1456.2, "text": " graph neural networks and you've probably heard from Miguel as well in the previous lectures", "tokens": [51316, 4295, 18161, 9590, 293, 291, 600, 1391, 2198, 490, 29150, 382, 731, 294, 264, 3894, 16564, 51492], "temperature": 0.0, "avg_logprob": -0.17096317291259766, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.0041409642435610294}, {"id": 251, "seek": 143364, "start": 1456.76, "end": 1460.92, "text": " about graph neural networks. So just to give you a little bit of history of this one,", "tokens": [51520, 466, 4295, 18161, 9590, 13, 407, 445, 281, 976, 291, 257, 707, 857, 295, 2503, 295, 341, 472, 11, 51728], "temperature": 0.0, "avg_logprob": -0.17096317291259766, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.0041409642435610294}, {"id": 252, "seek": 146092, "start": 1461.5600000000002, "end": 1467.8000000000002, "text": " so they're actually very much related and rooted in chemistry. And chemistry is probably one of", "tokens": [50396, 370, 436, 434, 767, 588, 709, 4077, 293, 25277, 294, 12558, 13, 400, 12558, 307, 1391, 472, 295, 50708], "temperature": 0.0, "avg_logprob": -0.1357707856576654, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.0026568102184683084}, {"id": 253, "seek": 146092, "start": 1467.8000000000002, "end": 1474.28, "text": " the fields of science, which is data intensive. It produces a huge amount of data, experimental data.", "tokens": [50708, 264, 7909, 295, 3497, 11, 597, 307, 1412, 18957, 13, 467, 14725, 257, 2603, 2372, 295, 1412, 11, 17069, 1412, 13, 51032], "temperature": 0.0, "avg_logprob": -0.1357707856576654, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.0026568102184683084}, {"id": 254, "seek": 146092, "start": 1474.28, "end": 1479.3200000000002, "text": " And since early times, chemists tried to organize these data first, publishing these", "tokens": [51032, 400, 1670, 2440, 1413, 11, 4771, 1751, 3031, 281, 13859, 613, 1412, 700, 11, 17832, 613, 51284], "temperature": 0.0, "avg_logprob": -0.1357707856576654, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.0026568102184683084}, {"id": 255, "seek": 146092, "start": 1479.3200000000002, "end": 1484.52, "text": " humongous books containing information about molecules or chemical reactions. And then", "tokens": [51284, 1484, 556, 563, 3642, 19273, 1589, 466, 13093, 420, 7313, 12215, 13, 400, 550, 51544], "temperature": 0.0, "avg_logprob": -0.1357707856576654, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.0026568102184683084}, {"id": 256, "seek": 148452, "start": 1485.32, "end": 1492.6, "text": " it appeared the first digitized archive, the chemical abstract service. And also with the", "tokens": [50404, 309, 8516, 264, 700, 14293, 1602, 23507, 11, 264, 7313, 12649, 2643, 13, 400, 611, 365, 264, 50768], "temperature": 0.0, "avg_logprob": -0.12303391369906339, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.004610887262970209}, {"id": 257, "seek": 148452, "start": 1492.6, "end": 1497.56, "text": " appearance of the first computers came the need and the idea to search for molecules, right? So", "tokens": [50768, 8967, 295, 264, 700, 10807, 1361, 264, 643, 293, 264, 1558, 281, 3164, 337, 13093, 11, 558, 30, 407, 51016], "temperature": 0.0, "avg_logprob": -0.12303391369906339, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.004610887262970209}, {"id": 258, "seek": 148452, "start": 1497.56, "end": 1502.76, "text": " if, for example, you're a pharmaceutical or a chemical company and you want to patent a new", "tokens": [51016, 498, 11, 337, 1365, 11, 291, 434, 257, 27130, 420, 257, 7313, 2237, 293, 291, 528, 281, 20495, 257, 777, 51276], "temperature": 0.0, "avg_logprob": -0.12303391369906339, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.004610887262970209}, {"id": 259, "seek": 148452, "start": 1502.76, "end": 1508.44, "text": " molecule, how do you know that it has not already been described, right? So you need to search fast", "tokens": [51276, 15582, 11, 577, 360, 291, 458, 300, 309, 575, 406, 1217, 668, 7619, 11, 558, 30, 407, 291, 643, 281, 3164, 2370, 51560], "temperature": 0.0, "avg_logprob": -0.12303391369906339, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.004610887262970209}, {"id": 260, "seek": 150844, "start": 1508.44, "end": 1514.52, "text": " for molecular structures in some data set. And these were the first ideas of chemical ciphers", "tokens": [50364, 337, 19046, 9227, 294, 512, 1412, 992, 13, 400, 613, 645, 264, 700, 3487, 295, 7313, 269, 24595, 433, 50668], "temperature": 0.0, "avg_logprob": -0.11855176030373087, "compression_ratio": 1.7155172413793103, "no_speech_prob": 0.007958113215863705}, {"id": 261, "seek": 150844, "start": 1514.52, "end": 1520.04, "text": " that describe a molecule as a string and then try to match it in some data set. So that was the kind", "tokens": [50668, 300, 6786, 257, 15582, 382, 257, 6798, 293, 550, 853, 281, 2995, 309, 294, 512, 1412, 992, 13, 407, 300, 390, 264, 733, 50944], "temperature": 0.0, "avg_logprob": -0.11855176030373087, "compression_ratio": 1.7155172413793103, "no_speech_prob": 0.007958113215863705}, {"id": 262, "seek": 150844, "start": 1520.04, "end": 1528.04, "text": " of problems that these guys, or he was a Romanian chemist called Vladus, that was one of the pioneers", "tokens": [50944, 295, 2740, 300, 613, 1074, 11, 420, 415, 390, 257, 49963, 4771, 468, 1219, 21958, 301, 11, 300, 390, 472, 295, 264, 47381, 51344], "temperature": 0.0, "avg_logprob": -0.11855176030373087, "compression_ratio": 1.7155172413793103, "no_speech_prob": 0.007958113215863705}, {"id": 263, "seek": 150844, "start": 1528.04, "end": 1534.3600000000001, "text": " of a field that later became known as chemoinformatics, he was trying to look at molecules as graphs.", "tokens": [51344, 295, 257, 2519, 300, 1780, 3062, 2570, 382, 4771, 78, 37811, 30292, 11, 415, 390, 1382, 281, 574, 412, 13093, 382, 24877, 13, 51660], "temperature": 0.0, "avg_logprob": -0.11855176030373087, "compression_ratio": 1.7155172413793103, "no_speech_prob": 0.007958113215863705}, {"id": 264, "seek": 153436, "start": 1535.0, "end": 1539.0, "text": " And as you probably know, the term graph itself, in the sense of graph theory,", "tokens": [50396, 400, 382, 291, 1391, 458, 11, 264, 1433, 4295, 2564, 11, 294, 264, 2020, 295, 4295, 5261, 11, 50596], "temperature": 0.0, "avg_logprob": -0.1051391315460205, "compression_ratio": 1.808, "no_speech_prob": 0.011403562501072884}, {"id": 265, "seek": 153436, "start": 1539.0, "end": 1544.28, "text": " also is associated with chemistry. This is how Sylvester called the first attempts to design", "tokens": [50596, 611, 307, 6615, 365, 12558, 13, 639, 307, 577, 3902, 14574, 3011, 1219, 264, 700, 15257, 281, 1715, 50860], "temperature": 0.0, "avg_logprob": -0.1051391315460205, "compression_ratio": 1.808, "no_speech_prob": 0.011403562501072884}, {"id": 266, "seek": 153436, "start": 1544.28, "end": 1549.6399999999999, "text": " structural molecules, structural formula of molecules, basically trying to understand how", "tokens": [50860, 15067, 13093, 11, 15067, 8513, 295, 13093, 11, 1936, 1382, 281, 1223, 577, 51128], "temperature": 0.0, "avg_logprob": -0.1051391315460205, "compression_ratio": 1.808, "no_speech_prob": 0.011403562501072884}, {"id": 267, "seek": 153436, "start": 1549.6399999999999, "end": 1555.32, "text": " atoms are related to each other with their chemical bonds, not only just the number of atoms", "tokens": [51128, 16871, 366, 4077, 281, 1184, 661, 365, 641, 7313, 14713, 11, 406, 787, 445, 264, 1230, 295, 16871, 51412], "temperature": 0.0, "avg_logprob": -0.1051391315460205, "compression_ratio": 1.808, "no_speech_prob": 0.011403562501072884}, {"id": 268, "seek": 153436, "start": 1555.32, "end": 1560.04, "text": " and the types of atoms. And as you probably know, this was one of the first structural formula of", "tokens": [51412, 293, 264, 3467, 295, 16871, 13, 400, 382, 291, 1391, 458, 11, 341, 390, 472, 295, 264, 700, 15067, 8513, 295, 51648], "temperature": 0.0, "avg_logprob": -0.1051391315460205, "compression_ratio": 1.808, "no_speech_prob": 0.011403562501072884}, {"id": 269, "seek": 156004, "start": 1560.04, "end": 1567.32, "text": " benzene that was derived by Kecoli. And the legend says that he dreamt of a snake biting", "tokens": [50364, 44335, 1450, 300, 390, 18949, 538, 3189, 8768, 72, 13, 400, 264, 9451, 1619, 300, 415, 3055, 83, 295, 257, 12650, 32912, 50728], "temperature": 0.0, "avg_logprob": -0.20617963956749957, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.005117250140756369}, {"id": 270, "seek": 156004, "start": 1568.2, "end": 1574.84, "text": " his own tail, so he came up with this kind of aromatic ring that is described here. So", "tokens": [50772, 702, 1065, 6838, 11, 370, 415, 1361, 493, 365, 341, 733, 295, 45831, 4875, 300, 307, 7619, 510, 13, 407, 51104], "temperature": 0.0, "avg_logprob": -0.20617963956749957, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.005117250140756369}, {"id": 271, "seek": 156004, "start": 1576.12, "end": 1580.12, "text": " these kind of problems inspired these duo of mathematicians, we'll talk about them later,", "tokens": [51168, 613, 733, 295, 2740, 7547, 613, 28127, 295, 32811, 2567, 11, 321, 603, 751, 466, 552, 1780, 11, 51368], "temperature": 0.0, "avg_logprob": -0.20617963956749957, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.005117250140756369}, {"id": 272, "seek": 156004, "start": 1580.76, "end": 1586.92, "text": " Weisfried and Lehmann in the 60s to devise an algorithm that would test whether two graphs", "tokens": [51400, 492, 271, 22773, 293, 1456, 8587, 969, 294, 264, 4060, 82, 281, 1905, 908, 364, 9284, 300, 576, 1500, 1968, 732, 24877, 51708], "temperature": 0.0, "avg_logprob": -0.20617963956749957, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.005117250140756369}, {"id": 273, "seek": 158692, "start": 1587.0, "end": 1592.8400000000001, "text": " are structurally similar, what is called the graph isomorphism test. And these ideas maybe were", "tokens": [50368, 366, 6594, 6512, 2531, 11, 437, 307, 1219, 264, 4295, 307, 32702, 1434, 1500, 13, 400, 613, 3487, 1310, 645, 50660], "temperature": 0.0, "avg_logprob": -0.14979488199407404, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.002829909324645996}, {"id": 274, "seek": 158692, "start": 1593.8000000000002, "end": 1599.48, "text": " noticed, so there are many related works in the machine learning community and also in", "tokens": [50708, 5694, 11, 370, 456, 366, 867, 4077, 1985, 294, 264, 3479, 2539, 1768, 293, 611, 294, 50992], "temperature": 0.0, "avg_logprob": -0.14979488199407404, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.002829909324645996}, {"id": 275, "seek": 158692, "start": 1599.48, "end": 1605.88, "text": " a chemical community, trying to devise new types of neural networks that could take as input,", "tokens": [50992, 257, 7313, 1768, 11, 1382, 281, 1905, 908, 777, 3467, 295, 18161, 9590, 300, 727, 747, 382, 4846, 11, 51312], "temperature": 0.0, "avg_logprob": -0.14979488199407404, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.002829909324645996}, {"id": 276, "seek": 158692, "start": 1605.88, "end": 1612.8400000000001, "text": " not vectors, not images, but graph structured data. And the early works by Alessandro Sperduti", "tokens": [51312, 406, 18875, 11, 406, 5267, 11, 457, 4295, 18519, 1412, 13, 400, 264, 2440, 1985, 538, 967, 442, 29173, 318, 610, 67, 29161, 51660], "temperature": 0.0, "avg_logprob": -0.14979488199407404, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.002829909324645996}, {"id": 277, "seek": 161284, "start": 1612.84, "end": 1618.28, "text": " in the 90s, for some reason most of the works were by Italians, and probably the most cited ones", "tokens": [50364, 294, 264, 4289, 82, 11, 337, 512, 1778, 881, 295, 264, 1985, 645, 538, 43620, 11, 293, 1391, 264, 881, 30134, 2306, 50636], "temperature": 0.0, "avg_logprob": -0.2292976803249783, "compression_ratio": 1.5546558704453441, "no_speech_prob": 0.004994732327759266}, {"id": 278, "seek": 161284, "start": 1618.28, "end": 1624.84, "text": " are by Marco Gore, Scarcelli and others. And interestingly, about 10 years ago, the graph", "tokens": [50636, 366, 538, 26535, 45450, 11, 2747, 289, 4164, 72, 293, 2357, 13, 400, 25873, 11, 466, 1266, 924, 2057, 11, 264, 4295, 50964], "temperature": 0.0, "avg_logprob": -0.2292976803249783, "compression_ratio": 1.5546558704453441, "no_speech_prob": 0.004994732327759266}, {"id": 279, "seek": 161284, "start": 1624.84, "end": 1631.48, "text": " neural networks returned triumphantly to chemistry, so I think worth crediting David Duvenau and Justin", "tokens": [50964, 18161, 9590, 8752, 29156, 3627, 281, 12558, 11, 370, 286, 519, 3163, 3864, 1748, 4389, 5153, 553, 1459, 293, 11320, 51296], "temperature": 0.0, "avg_logprob": -0.2292976803249783, "compression_ratio": 1.5546558704453441, "no_speech_prob": 0.004994732327759266}, {"id": 280, "seek": 161284, "start": 1631.48, "end": 1638.36, "text": " Gilmer for who also introduced the terminology of message passing neural networks that try to", "tokens": [51296, 17654, 936, 337, 567, 611, 7268, 264, 27575, 295, 3636, 8437, 18161, 9590, 300, 853, 281, 51640], "temperature": 0.0, "avg_logprob": -0.2292976803249783, "compression_ratio": 1.5546558704453441, "no_speech_prob": 0.004994732327759266}, {"id": 281, "seek": 163836, "start": 1638.36, "end": 1643.4799999999998, "text": " predict properties of molecules, model these graphs and learning on these graphs. And then,", "tokens": [50364, 6069, 7221, 295, 13093, 11, 2316, 613, 24877, 293, 2539, 322, 613, 24877, 13, 400, 550, 11, 50620], "temperature": 0.0, "avg_logprob": -0.1927408944992792, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.0010451722191646695}, {"id": 282, "seek": 163836, "start": 1643.4799999999998, "end": 1647.4799999999998, "text": " of course, the ideas of geometric learning, as we'll see maybe with some extra stuff.", "tokens": [50620, 295, 1164, 11, 264, 3487, 295, 33246, 2539, 11, 382, 321, 603, 536, 1310, 365, 512, 2857, 1507, 13, 50820], "temperature": 0.0, "avg_logprob": -0.1927408944992792, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.0010451722191646695}, {"id": 283, "seek": 163836, "start": 1648.84, "end": 1656.6799999999998, "text": " Also, the structural biologists have had their own image net moment with alpha fold, first in", "tokens": [50888, 2743, 11, 264, 15067, 3228, 12256, 362, 632, 641, 1065, 3256, 2533, 1623, 365, 8961, 4860, 11, 700, 294, 51280], "temperature": 0.0, "avg_logprob": -0.1927408944992792, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.0010451722191646695}, {"id": 284, "seek": 163836, "start": 1656.6799999999998, "end": 1664.6, "text": " 2018 and then in 2020, basically predicting the structure of protein folds. So, and this field", "tokens": [51280, 6096, 293, 550, 294, 4808, 11, 1936, 32884, 264, 3877, 295, 7944, 31341, 13, 407, 11, 293, 341, 2519, 51676], "temperature": 0.0, "avg_logprob": -0.1927408944992792, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.0010451722191646695}, {"id": 285, "seek": 166460, "start": 1664.6, "end": 1669.0, "text": " is very rapidly developing. So, I think these are very exciting and cool problems that you can", "tokens": [50364, 307, 588, 12910, 6416, 13, 407, 11, 286, 519, 613, 366, 588, 4670, 293, 1627, 2740, 300, 291, 393, 50584], "temperature": 0.0, "avg_logprob": -0.11983915206489212, "compression_ratio": 1.835820895522388, "no_speech_prob": 0.0021598651073873043}, {"id": 286, "seek": 166460, "start": 1669.0, "end": 1674.9199999999998, "text": " address with geometric techniques. So, let's just try to summarize basically what this historical", "tokens": [50584, 2985, 365, 33246, 7512, 13, 407, 11, 718, 311, 445, 853, 281, 20858, 1936, 437, 341, 8584, 50880], "temperature": 0.0, "avg_logprob": -0.11983915206489212, "compression_ratio": 1.835820895522388, "no_speech_prob": 0.0021598651073873043}, {"id": 287, "seek": 166460, "start": 1674.9199999999998, "end": 1680.6799999999998, "text": " excursion gives us, a kind of blueprint for different architectures. So, if you look at convolutional", "tokens": [50880, 1624, 2156, 313, 2709, 505, 11, 257, 733, 295, 35868, 337, 819, 6331, 1303, 13, 407, 11, 498, 291, 574, 412, 45216, 304, 51168], "temperature": 0.0, "avg_logprob": -0.11983915206489212, "compression_ratio": 1.835820895522388, "no_speech_prob": 0.0021598651073873043}, {"id": 288, "seek": 166460, "start": 1680.6799999999998, "end": 1685.24, "text": " neural networks and graph neural networks, right, they work with very different data, convolutional", "tokens": [51168, 18161, 9590, 293, 4295, 18161, 9590, 11, 558, 11, 436, 589, 365, 588, 819, 1412, 11, 45216, 304, 51396], "temperature": 0.0, "avg_logprob": -0.11983915206489212, "compression_ratio": 1.835820895522388, "no_speech_prob": 0.0021598651073873043}, {"id": 289, "seek": 166460, "start": 1685.24, "end": 1690.84, "text": " neural networks work on images, graph neural networks work on graphs, right, let's say molecules,", "tokens": [51396, 18161, 9590, 589, 322, 5267, 11, 4295, 18161, 9590, 589, 322, 24877, 11, 558, 11, 718, 311, 584, 13093, 11, 51676], "temperature": 0.0, "avg_logprob": -0.11983915206489212, "compression_ratio": 1.835820895522388, "no_speech_prob": 0.0021598651073873043}, {"id": 290, "seek": 169084, "start": 1690.9199999999998, "end": 1696.12, "text": " but there are some common patterns, right. So, in both cases, we have some underlying domain.", "tokens": [50368, 457, 456, 366, 512, 2689, 8294, 11, 558, 13, 407, 11, 294, 1293, 3331, 11, 321, 362, 512, 14217, 9274, 13, 50628], "temperature": 0.0, "avg_logprob": -0.10734039129212845, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0007020061602815986}, {"id": 291, "seek": 169084, "start": 1696.12, "end": 1700.84, "text": " So, in the first case, it's agreed. In the second case, it's a graph. We also, in both cases, have", "tokens": [50628, 407, 11, 294, 264, 700, 1389, 11, 309, 311, 9166, 13, 682, 264, 1150, 1389, 11, 309, 311, 257, 4295, 13, 492, 611, 11, 294, 1293, 3331, 11, 362, 50864], "temperature": 0.0, "avg_logprob": -0.10734039129212845, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0007020061602815986}, {"id": 292, "seek": 169084, "start": 1700.84, "end": 1708.12, "text": " some kind of geometric operation, so a symmetry, right, that is a nature in the context of the", "tokens": [50864, 512, 733, 295, 33246, 6916, 11, 370, 257, 25440, 11, 558, 11, 300, 307, 257, 3687, 294, 264, 4319, 295, 264, 51228], "temperature": 0.0, "avg_logprob": -0.10734039129212845, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0007020061602815986}, {"id": 293, "seek": 169084, "start": 1708.12, "end": 1712.4399999999998, "text": " problems that we are considering. So, in images, it's translation, right. So, I want to move,", "tokens": [51228, 2740, 300, 321, 366, 8079, 13, 407, 11, 294, 5267, 11, 309, 311, 12853, 11, 558, 13, 407, 11, 286, 528, 281, 1286, 11, 51444], "temperature": 0.0, "avg_logprob": -0.10734039129212845, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0007020061602815986}, {"id": 294, "seek": 169084, "start": 1712.4399999999998, "end": 1716.6, "text": " for example, an object in the image and I don't care where the object is located if I want to", "tokens": [51444, 337, 1365, 11, 364, 2657, 294, 264, 3256, 293, 286, 500, 380, 1127, 689, 264, 2657, 307, 6870, 498, 286, 528, 281, 51652], "temperature": 0.0, "avg_logprob": -0.10734039129212845, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0007020061602815986}, {"id": 295, "seek": 171660, "start": 1716.6, "end": 1722.52, "text": " classify it. In case of molecules, it's a permutation symmetry, so no matter how I order", "tokens": [50364, 33872, 309, 13, 682, 1389, 295, 13093, 11, 309, 311, 257, 4784, 11380, 25440, 11, 370, 572, 1871, 577, 286, 1668, 50660], "temperature": 0.0, "avg_logprob": -0.10574905292407887, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0012562900083139539}, {"id": 296, "seek": 171660, "start": 1722.52, "end": 1726.1999999999998, "text": " the atoms in a molecule, it's still the same molecule, right. So, I want somehow to be", "tokens": [50660, 264, 16871, 294, 257, 15582, 11, 309, 311, 920, 264, 912, 15582, 11, 558, 13, 407, 11, 286, 528, 6063, 281, 312, 50844], "temperature": 0.0, "avg_logprob": -0.10574905292407887, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0012562900083139539}, {"id": 297, "seek": 171660, "start": 1726.1999999999998, "end": 1734.28, "text": " insensitive to this ordering. And we can also define natural operations that respect this", "tokens": [50844, 1028, 34465, 281, 341, 21739, 13, 400, 321, 393, 611, 6964, 3303, 7705, 300, 3104, 341, 51248], "temperature": 0.0, "avg_logprob": -0.10574905292407887, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0012562900083139539}, {"id": 298, "seek": 171660, "start": 1734.28, "end": 1738.28, "text": " symmetry, right. So, in case of convolutional neural networks, it's actually the convolutional", "tokens": [51248, 25440, 11, 558, 13, 407, 11, 294, 1389, 295, 45216, 304, 18161, 9590, 11, 309, 311, 767, 264, 45216, 304, 51448], "temperature": 0.0, "avg_logprob": -0.10574905292407887, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0012562900083139539}, {"id": 299, "seek": 171660, "start": 1738.28, "end": 1744.9199999999998, "text": " operation. Basically, I can move a patch around an image and apply the same weights or the same", "tokens": [51448, 6916, 13, 8537, 11, 286, 393, 1286, 257, 9972, 926, 364, 3256, 293, 3079, 264, 912, 17443, 420, 264, 912, 51780], "temperature": 0.0, "avg_logprob": -0.10574905292407887, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0012562900083139539}, {"id": 300, "seek": 174492, "start": 1744.92, "end": 1750.68, "text": " local rule that would extract the features. And as we'll see in graph neural networks,", "tokens": [50364, 2654, 4978, 300, 576, 8947, 264, 4122, 13, 400, 382, 321, 603, 536, 294, 4295, 18161, 9590, 11, 50652], "temperature": 0.0, "avg_logprob": -0.10956033769544664, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.0011011941824108362}, {"id": 301, "seek": 174492, "start": 1750.68, "end": 1754.76, "text": " this is some kind of local rule that we call message passing, right, or some versions of it.", "tokens": [50652, 341, 307, 512, 733, 295, 2654, 4978, 300, 321, 818, 3636, 8437, 11, 558, 11, 420, 512, 9606, 295, 309, 13, 50856], "temperature": 0.0, "avg_logprob": -0.10956033769544664, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.0011011941824108362}, {"id": 302, "seek": 174492, "start": 1756.52, "end": 1762.76, "text": " So, these are ideas that, as you see, these are two examples or architectures that share the", "tokens": [50944, 407, 11, 613, 366, 3487, 300, 11, 382, 291, 536, 11, 613, 366, 732, 5110, 420, 6331, 1303, 300, 2073, 264, 51256], "temperature": 0.0, "avg_logprob": -0.10956033769544664, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.0011011941824108362}, {"id": 303, "seek": 174492, "start": 1762.76, "end": 1768.28, "text": " common principles and that's the idea of geometric deep learning. So, I can probably take the craze", "tokens": [51256, 2689, 9156, 293, 300, 311, 264, 1558, 295, 33246, 2452, 2539, 13, 407, 11, 286, 393, 1391, 747, 264, 2094, 1381, 51532], "temperature": 0.0, "avg_logprob": -0.10956033769544664, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.0011011941824108362}, {"id": 304, "seek": 176828, "start": 1768.28, "end": 1777.3999999999999, "text": " of inventing the term. So, that happened when I was writing one of my ERC grants back in 2015,", "tokens": [50364, 295, 7962, 278, 264, 1433, 13, 407, 11, 300, 2011, 562, 286, 390, 3579, 472, 295, 452, 14929, 34, 16101, 646, 294, 7546, 11, 50820], "temperature": 0.0, "avg_logprob": -0.1611001417808926, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.021072683855891228}, {"id": 305, "seek": 176828, "start": 1777.3999999999999, "end": 1782.44, "text": " probably. And of course, everybody was doing deep learning. So, you need to distinguish yourself", "tokens": [50820, 1391, 13, 400, 295, 1164, 11, 2201, 390, 884, 2452, 2539, 13, 407, 11, 291, 643, 281, 20206, 1803, 51072], "temperature": 0.0, "avg_logprob": -0.1611001417808926, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.021072683855891228}, {"id": 306, "seek": 176828, "start": 1782.44, "end": 1788.6, "text": " from everyone. So, I wrote that we are not doing deep learning as everybody else, we are doing", "tokens": [51072, 490, 1518, 13, 407, 11, 286, 4114, 300, 321, 366, 406, 884, 2452, 2539, 382, 2201, 1646, 11, 321, 366, 884, 51380], "temperature": 0.0, "avg_logprob": -0.1611001417808926, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.021072683855891228}, {"id": 307, "seek": 176828, "start": 1788.6, "end": 1796.2, "text": " geometric deep learning. So, that was the idea. Then we popularized it in this paper in IEEE", "tokens": [51380, 33246, 2452, 2539, 13, 407, 11, 300, 390, 264, 1558, 13, 1396, 321, 3743, 1602, 309, 294, 341, 3035, 294, 286, 7258, 36, 51760], "temperature": 0.0, "avg_logprob": -0.1611001417808926, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.021072683855891228}, {"id": 308, "seek": 179620, "start": 1796.2, "end": 1803.96, "text": " signal processing magazine and more recently in a book that I'm writing with my collaborators.", "tokens": [50364, 6358, 9007, 11332, 293, 544, 3938, 294, 257, 1446, 300, 286, 478, 3579, 365, 452, 39789, 13, 50752], "temperature": 0.0, "avg_logprob": -0.10948843955993652, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.00326169072650373}, {"id": 309, "seek": 179620, "start": 1803.96, "end": 1809.88, "text": " So, by analogy to the Erlangian program, we basically, we can think of a kind of common", "tokens": [50752, 407, 11, 538, 21663, 281, 264, 3300, 25241, 952, 1461, 11, 321, 1936, 11, 321, 393, 519, 295, 257, 733, 295, 2689, 51048], "temperature": 0.0, "avg_logprob": -0.10948843955993652, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.00326169072650373}, {"id": 310, "seek": 179620, "start": 1809.88, "end": 1814.44, "text": " denominator for machine learning architectures. So, we would like to take this zoo of different", "tokens": [51048, 20687, 337, 3479, 2539, 6331, 1303, 13, 407, 11, 321, 576, 411, 281, 747, 341, 25347, 295, 819, 51276], "temperature": 0.0, "avg_logprob": -0.10948843955993652, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.00326169072650373}, {"id": 311, "seek": 179620, "start": 1814.44, "end": 1818.68, "text": " architectures that were historically designed for different types of data with different", "tokens": [51276, 6331, 1303, 300, 645, 16180, 4761, 337, 819, 3467, 295, 1412, 365, 819, 51488], "temperature": 0.0, "avg_logprob": -0.10948843955993652, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.00326169072650373}, {"id": 312, "seek": 179620, "start": 1819.4, "end": 1825.4, "text": " kind of problem in mind and look at them from the same perspective. And this perspective is", "tokens": [51524, 733, 295, 1154, 294, 1575, 293, 574, 412, 552, 490, 264, 912, 4585, 13, 400, 341, 4585, 307, 51824], "temperature": 0.0, "avg_logprob": -0.10948843955993652, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.00326169072650373}, {"id": 313, "seek": 182540, "start": 1825.4, "end": 1832.44, "text": " through the lens of group theory and properties like invariance, equivariance and symmetry.", "tokens": [50364, 807, 264, 6765, 295, 1594, 5261, 293, 7221, 411, 33270, 719, 11, 48726, 3504, 719, 293, 25440, 13, 50716], "temperature": 0.0, "avg_logprob": -0.1477766147879667, "compression_ratio": 1.663716814159292, "no_speech_prob": 0.003966490738093853}, {"id": 314, "seek": 182540, "start": 1832.44, "end": 1838.0400000000002, "text": " And if we've seen before this problem of machine learning in high dimension, where you have,", "tokens": [50716, 400, 498, 321, 600, 1612, 949, 341, 1154, 295, 3479, 2539, 294, 1090, 10139, 11, 689, 291, 362, 11, 50996], "temperature": 0.0, "avg_logprob": -0.1477766147879667, "compression_ratio": 1.663716814159292, "no_speech_prob": 0.003966490738093853}, {"id": 315, "seek": 182540, "start": 1838.0400000000002, "end": 1843.24, "text": " for example, your images of cats and dogs as points in a high-dimensional space, we no longer", "tokens": [50996, 337, 1365, 11, 428, 5267, 295, 11111, 293, 7197, 382, 2793, 294, 257, 1090, 12, 18759, 1901, 11, 321, 572, 2854, 51256], "temperature": 0.0, "avg_logprob": -0.1477766147879667, "compression_ratio": 1.663716814159292, "no_speech_prob": 0.003966490738093853}, {"id": 316, "seek": 182540, "start": 1843.24, "end": 1850.8400000000001, "text": " consider these inputs as just a high-dimensional vector that you need to put through some generic", "tokens": [51256, 1949, 613, 15743, 382, 445, 257, 1090, 12, 18759, 8062, 300, 291, 643, 281, 829, 807, 512, 19577, 51636], "temperature": 0.0, "avg_logprob": -0.1477766147879667, "compression_ratio": 1.663716814159292, "no_speech_prob": 0.003966490738093853}, {"id": 317, "seek": 185084, "start": 1850.84, "end": 1857.08, "text": " class of functions and then you suffer from the curse of dimensionality. But now we know that", "tokens": [50364, 1508, 295, 6828, 293, 550, 291, 9753, 490, 264, 17139, 295, 10139, 1860, 13, 583, 586, 321, 458, 300, 50676], "temperature": 0.0, "avg_logprob": -0.14157985604327658, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.004823186434805393}, {"id": 318, "seek": 185084, "start": 1857.08, "end": 1863.32, "text": " there is some domain, geometric structure that underlies these inputs. And in images, for example,", "tokens": [50676, 456, 307, 512, 9274, 11, 33246, 3877, 300, 833, 24119, 613, 15743, 13, 400, 294, 5267, 11, 337, 1365, 11, 50988], "temperature": 0.0, "avg_logprob": -0.14157985604327658, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.004823186434805393}, {"id": 319, "seek": 185084, "start": 1863.32, "end": 1868.9199999999998, "text": " this is a two-dimensional grid. So, our data lives on some typically low-dimensional domain.", "tokens": [50988, 341, 307, 257, 732, 12, 18759, 10748, 13, 407, 11, 527, 1412, 2909, 322, 512, 5850, 2295, 12, 18759, 9274, 13, 51268], "temperature": 0.0, "avg_logprob": -0.14157985604327658, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.004823186434805393}, {"id": 320, "seek": 185084, "start": 1868.9199999999998, "end": 1876.4399999999998, "text": " And this domain comes equipped with some group. So, in this case, it's the group of translations.", "tokens": [51268, 400, 341, 9274, 1487, 15218, 365, 512, 1594, 13, 407, 11, 294, 341, 1389, 11, 309, 311, 264, 1594, 295, 37578, 13, 51644], "temperature": 0.0, "avg_logprob": -0.14157985604327658, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.004823186434805393}, {"id": 321, "seek": 187644, "start": 1877.24, "end": 1882.76, "text": " And so, we have a domain, we have a group, we have signals that live on this domain and the", "tokens": [50404, 400, 370, 11, 321, 362, 257, 9274, 11, 321, 362, 257, 1594, 11, 321, 362, 12354, 300, 1621, 322, 341, 9274, 293, 264, 50680], "temperature": 0.0, "avg_logprob": -0.10312288685848839, "compression_ratio": 1.9794238683127572, "no_speech_prob": 0.004863219801336527}, {"id": 322, "seek": 187644, "start": 1882.76, "end": 1889.24, "text": " group that acts on the points of the domain, we see how it can act on the signals themselves", "tokens": [50680, 1594, 300, 10672, 322, 264, 2793, 295, 264, 9274, 11, 321, 536, 577, 309, 393, 605, 322, 264, 12354, 2969, 51004], "temperature": 0.0, "avg_logprob": -0.10312288685848839, "compression_ratio": 1.9794238683127572, "no_speech_prob": 0.004863219801336527}, {"id": 323, "seek": 187644, "start": 1889.24, "end": 1894.44, "text": " through what is called the group representation. And in case of images, again, the group representation", "tokens": [51004, 807, 437, 307, 1219, 264, 1594, 10290, 13, 400, 294, 1389, 295, 5267, 11, 797, 11, 264, 1594, 10290, 51264], "temperature": 0.0, "avg_logprob": -0.10312288685848839, "compression_ratio": 1.9794238683127572, "no_speech_prob": 0.004863219801336527}, {"id": 324, "seek": 187644, "start": 1894.44, "end": 1900.3600000000001, "text": " will be just the shift operator. We'll see it in more details. And then finally, we have functions", "tokens": [51264, 486, 312, 445, 264, 5513, 12973, 13, 492, 603, 536, 309, 294, 544, 4365, 13, 400, 550, 2721, 11, 321, 362, 6828, 51560], "temperature": 0.0, "avg_logprob": -0.10312288685848839, "compression_ratio": 1.9794238683127572, "no_speech_prob": 0.004863219801336527}, {"id": 325, "seek": 187644, "start": 1900.3600000000001, "end": 1905.56, "text": " that act on these inputs that somehow need to respect this symmetry. And this will be through", "tokens": [51560, 300, 605, 322, 613, 15743, 300, 6063, 643, 281, 3104, 341, 25440, 13, 400, 341, 486, 312, 807, 51820], "temperature": 0.0, "avg_logprob": -0.10312288685848839, "compression_ratio": 1.9794238683127572, "no_speech_prob": 0.004863219801336527}, {"id": 326, "seek": 190556, "start": 1905.56, "end": 1910.84, "text": " what we will call invariance and equivariance. So, basically, we want the function either to be", "tokens": [50364, 437, 321, 486, 818, 33270, 719, 293, 48726, 3504, 719, 13, 407, 11, 1936, 11, 321, 528, 264, 2445, 2139, 281, 312, 50628], "temperature": 0.0, "avg_logprob": -0.09933527823417418, "compression_ratio": 1.75, "no_speech_prob": 0.0011309835826978087}, {"id": 327, "seek": 190556, "start": 1911.8, "end": 1918.6, "text": " insensitive to how I transform the input by acting on it with the group or should change in the same", "tokens": [50676, 1028, 34465, 281, 577, 286, 4088, 264, 4846, 538, 6577, 322, 309, 365, 264, 1594, 420, 820, 1319, 294, 264, 912, 51016], "temperature": 0.0, "avg_logprob": -0.09933527823417418, "compression_ratio": 1.75, "no_speech_prob": 0.0011309835826978087}, {"id": 328, "seek": 190556, "start": 1918.6, "end": 1924.9199999999998, "text": " way. And I should say that the choice of the group and the domain are two separate things and they're", "tokens": [51016, 636, 13, 400, 286, 820, 584, 300, 264, 3922, 295, 264, 1594, 293, 264, 9274, 366, 732, 4994, 721, 293, 436, 434, 51332], "temperature": 0.0, "avg_logprob": -0.09933527823417418, "compression_ratio": 1.75, "no_speech_prob": 0.0011309835826978087}, {"id": 329, "seek": 190556, "start": 1924.9199999999998, "end": 1929.1599999999999, "text": " not only dependent on the data, they're also dependent on the task. So, I might have a problem", "tokens": [51332, 406, 787, 12334, 322, 264, 1412, 11, 436, 434, 611, 12334, 322, 264, 5633, 13, 407, 11, 286, 1062, 362, 257, 1154, 51544], "temperature": 0.0, "avg_logprob": -0.09933527823417418, "compression_ratio": 1.75, "no_speech_prob": 0.0011309835826978087}, {"id": 330, "seek": 190556, "start": 1929.1599999999999, "end": 1933.72, "text": " like this. So, I have, for example, images of traffic signs. And if I'm designing a self-driving", "tokens": [51544, 411, 341, 13, 407, 11, 286, 362, 11, 337, 1365, 11, 5267, 295, 6419, 7880, 13, 400, 498, 286, 478, 14685, 257, 2698, 12, 47094, 51772], "temperature": 0.0, "avg_logprob": -0.09933527823417418, "compression_ratio": 1.75, "no_speech_prob": 0.0011309835826978087}, {"id": 331, "seek": 193372, "start": 1933.72, "end": 1939.64, "text": " car, it's very unlikely that I will see rotated traffic signs, right? They will probably be aligned", "tokens": [50364, 1032, 11, 309, 311, 588, 17518, 300, 286, 486, 536, 42146, 6419, 7880, 11, 558, 30, 814, 486, 1391, 312, 17962, 50660], "temperature": 0.0, "avg_logprob": -0.09055564278050472, "compression_ratio": 1.8081180811808117, "no_speech_prob": 0.0012137433513998985}, {"id": 332, "seek": 193372, "start": 1939.64, "end": 1945.48, "text": " and move just horizontally and maybe vertically. So, here, for example, the group of transformations", "tokens": [50660, 293, 1286, 445, 33796, 293, 1310, 28450, 13, 407, 11, 510, 11, 337, 1365, 11, 264, 1594, 295, 34852, 50952], "temperature": 0.0, "avg_logprob": -0.09055564278050472, "compression_ratio": 1.8081180811808117, "no_speech_prob": 0.0012137433513998985}, {"id": 333, "seek": 193372, "start": 1945.48, "end": 1950.92, "text": " that is reasonable to assume will be just two-dimensional translation, maybe even one-dimensional", "tokens": [50952, 300, 307, 10585, 281, 6552, 486, 312, 445, 732, 12, 18759, 12853, 11, 1310, 754, 472, 12, 18759, 51224], "temperature": 0.0, "avg_logprob": -0.09055564278050472, "compression_ratio": 1.8081180811808117, "no_speech_prob": 0.0012137433513998985}, {"id": 334, "seek": 193372, "start": 1950.92, "end": 1955.8, "text": " translation. But if, for example, my car can also tilt, right, so imagine that it's a plane and not", "tokens": [51224, 12853, 13, 583, 498, 11, 337, 1365, 11, 452, 1032, 393, 611, 18446, 11, 558, 11, 370, 3811, 300, 309, 311, 257, 5720, 293, 406, 51468], "temperature": 0.0, "avg_logprob": -0.09055564278050472, "compression_ratio": 1.8081180811808117, "no_speech_prob": 0.0012137433513998985}, {"id": 335, "seek": 193372, "start": 1955.8, "end": 1960.04, "text": " a car, then rotations are also perfectly valid, right? So, then the group of transformation", "tokens": [51468, 257, 1032, 11, 550, 44796, 366, 611, 6239, 7363, 11, 558, 30, 407, 11, 550, 264, 1594, 295, 9887, 51680], "temperature": 0.0, "avg_logprob": -0.09055564278050472, "compression_ratio": 1.8081180811808117, "no_speech_prob": 0.0012137433513998985}, {"id": 336, "seek": 196004, "start": 1960.04, "end": 1964.12, "text": " will be different. So, it's still the same domain, the same data, but the task might be different", "tokens": [50364, 486, 312, 819, 13, 407, 11, 309, 311, 920, 264, 912, 9274, 11, 264, 912, 1412, 11, 457, 264, 5633, 1062, 312, 819, 50568], "temperature": 0.0, "avg_logprob": -0.1279581373771735, "compression_ratio": 1.758490566037736, "no_speech_prob": 0.0044106123968958855}, {"id": 337, "seek": 196004, "start": 1964.12, "end": 1970.04, "text": " and, therefore, the assumptions about this, the priorities problem might be different.", "tokens": [50568, 293, 11, 4412, 11, 264, 17695, 466, 341, 11, 264, 15503, 1154, 1062, 312, 819, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1279581373771735, "compression_ratio": 1.758490566037736, "no_speech_prob": 0.0044106123968958855}, {"id": 338, "seek": 196004, "start": 1970.04, "end": 1974.2, "text": " And if you think of another application, if I have, for example, a pathological sample,", "tokens": [50864, 400, 498, 291, 519, 295, 1071, 3861, 11, 498, 286, 362, 11, 337, 1365, 11, 257, 3100, 4383, 6889, 11, 51072], "temperature": 0.0, "avg_logprob": -0.1279581373771735, "compression_ratio": 1.758490566037736, "no_speech_prob": 0.0044106123968958855}, {"id": 339, "seek": 196004, "start": 1975.0, "end": 1980.28, "text": " so, essentially, a glass of some stained tissue that I put under a microscope, so there I can also", "tokens": [51112, 370, 11, 4476, 11, 257, 4276, 295, 512, 39924, 12404, 300, 286, 829, 833, 257, 29753, 11, 370, 456, 286, 393, 611, 51376], "temperature": 0.0, "avg_logprob": -0.1279581373771735, "compression_ratio": 1.758490566037736, "no_speech_prob": 0.0044106123968958855}, {"id": 340, "seek": 196004, "start": 1980.28, "end": 1985.3999999999999, "text": " have reflections, right? Because I don't have canonical orientation for this glass. So, it can", "tokens": [51376, 362, 30679, 11, 558, 30, 1436, 286, 500, 380, 362, 46491, 14764, 337, 341, 4276, 13, 407, 11, 309, 393, 51632], "temperature": 0.0, "avg_logprob": -0.1279581373771735, "compression_ratio": 1.758490566037736, "no_speech_prob": 0.0044106123968958855}, {"id": 341, "seek": 198540, "start": 1985.4, "end": 1992.92, "text": " be an even bigger group in this case and, again, task dependent. So, as I mentioned in case of", "tokens": [50364, 312, 364, 754, 3801, 1594, 294, 341, 1389, 293, 11, 797, 11, 5633, 12334, 13, 407, 11, 382, 286, 2835, 294, 1389, 295, 50740], "temperature": 0.0, "avg_logprob": -0.090462232890882, "compression_ratio": 1.65625, "no_speech_prob": 0.010134189389646053}, {"id": 342, "seek": 198540, "start": 1992.92, "end": 2000.2, "text": " images, the representation that we'll be working with is the shift operator. In case of crafts,", "tokens": [50740, 5267, 11, 264, 10290, 300, 321, 603, 312, 1364, 365, 307, 264, 5513, 12973, 13, 682, 1389, 295, 27831, 11, 51104], "temperature": 0.0, "avg_logprob": -0.090462232890882, "compression_ratio": 1.65625, "no_speech_prob": 0.010134189389646053}, {"id": 343, "seek": 198540, "start": 2000.2, "end": 2005.48, "text": " actually, the symmetry, as we've seen it, is the group of permutations, what is sometimes called,", "tokens": [51104, 767, 11, 264, 25440, 11, 382, 321, 600, 1612, 309, 11, 307, 264, 1594, 295, 4784, 325, 763, 11, 437, 307, 2171, 1219, 11, 51368], "temperature": 0.0, "avg_logprob": -0.090462232890882, "compression_ratio": 1.65625, "no_speech_prob": 0.010134189389646053}, {"id": 344, "seek": 198540, "start": 2005.48, "end": 2012.3600000000001, "text": " confusingly, the symmetric group. So, it's the different ways that I can rearrange", "tokens": [51368, 13181, 356, 11, 264, 32330, 1594, 13, 407, 11, 309, 311, 264, 819, 2098, 300, 286, 393, 39568, 51712], "temperature": 0.0, "avg_logprob": -0.090462232890882, "compression_ratio": 1.65625, "no_speech_prob": 0.010134189389646053}, {"id": 345, "seek": 201236, "start": 2012.36, "end": 2018.1999999999998, "text": " and different objects. And its representation is a permutation matrix. And as we'll see,", "tokens": [50364, 293, 819, 6565, 13, 400, 1080, 10290, 307, 257, 4784, 11380, 8141, 13, 400, 382, 321, 603, 536, 11, 50656], "temperature": 0.0, "avg_logprob": -0.15806759320772612, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.0005638225702568889}, {"id": 346, "seek": 201236, "start": 2019.08, "end": 2024.28, "text": " the way that it's implemented, so, functions that are equivalent with respect to this symmetry", "tokens": [50700, 264, 636, 300, 309, 311, 12270, 11, 370, 11, 6828, 300, 366, 10344, 365, 3104, 281, 341, 25440, 50960], "temperature": 0.0, "avg_logprob": -0.15806759320772612, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.0005638225702568889}, {"id": 347, "seek": 201236, "start": 2025.1599999999999, "end": 2030.6799999999998, "text": " are message passing. And we can also have another type of architectures that are also confusingly", "tokens": [51004, 366, 3636, 8437, 13, 400, 321, 393, 611, 362, 1071, 2010, 295, 6331, 1303, 300, 366, 611, 13181, 356, 51280], "temperature": 0.0, "avg_logprob": -0.15806759320772612, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.0005638225702568889}, {"id": 348, "seek": 201236, "start": 2030.6799999999998, "end": 2035.56, "text": " called equivalent neural networks or equivalent transformers, where, in addition to the symmetry", "tokens": [51280, 1219, 10344, 18161, 9590, 420, 10344, 4088, 433, 11, 689, 11, 294, 4500, 281, 264, 25440, 51524], "temperature": 0.0, "avg_logprob": -0.15806759320772612, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.0005638225702568889}, {"id": 349, "seek": 201236, "start": 2035.56, "end": 2040.84, "text": " group of the domain, we also have symmetry group of the data. And this is typical for geometric", "tokens": [51524, 1594, 295, 264, 9274, 11, 321, 611, 362, 25440, 1594, 295, 264, 1412, 13, 400, 341, 307, 7476, 337, 33246, 51788], "temperature": 0.0, "avg_logprob": -0.15806759320772612, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.0005638225702568889}, {"id": 350, "seek": 204084, "start": 2040.84, "end": 2045.0, "text": " graphs like molecules, where the nodes also have geometric coordinates. So, they live in three", "tokens": [50364, 24877, 411, 13093, 11, 689, 264, 13891, 611, 362, 33246, 21056, 13, 407, 11, 436, 1621, 294, 1045, 50572], "temperature": 0.0, "avg_logprob": -0.10152310870942616, "compression_ratio": 1.758490566037736, "no_speech_prob": 0.0012580696493387222}, {"id": 351, "seek": 204084, "start": 2045.0, "end": 2050.36, "text": " dimensional space. And in addition to reordering the atoms in the molecule, we can also rotate or", "tokens": [50572, 18795, 1901, 13, 400, 294, 4500, 281, 319, 765, 1794, 264, 16871, 294, 264, 15582, 11, 321, 393, 611, 13121, 420, 50840], "temperature": 0.0, "avg_logprob": -0.10152310870942616, "compression_ratio": 1.758490566037736, "no_speech_prob": 0.0012580696493387222}, {"id": 352, "seek": 204084, "start": 2050.36, "end": 2055.48, "text": " translate the molecule in three dimensional space, right? So, you want to be equivalent with respect", "tokens": [50840, 13799, 264, 15582, 294, 1045, 18795, 1901, 11, 558, 30, 407, 11, 291, 528, 281, 312, 10344, 365, 3104, 51096], "temperature": 0.0, "avg_logprob": -0.10152310870942616, "compression_ratio": 1.758490566037736, "no_speech_prob": 0.0012580696493387222}, {"id": 353, "seek": 204084, "start": 2055.48, "end": 2060.2799999999997, "text": " to both transformations. So, it's a kind of analogy of the external and internal symmetries", "tokens": [51096, 281, 1293, 34852, 13, 407, 11, 309, 311, 257, 733, 295, 21663, 295, 264, 8320, 293, 6920, 14232, 302, 2244, 51336], "temperature": 0.0, "avg_logprob": -0.10152310870942616, "compression_ratio": 1.758490566037736, "no_speech_prob": 0.0012580696493387222}, {"id": 354, "seek": 204084, "start": 2060.2799999999997, "end": 2065.7999999999997, "text": " you have in physics. And, basically, geometric architecture is just sequences of", "tokens": [51336, 291, 362, 294, 10649, 13, 400, 11, 1936, 11, 33246, 9482, 307, 445, 22978, 295, 51612], "temperature": 0.0, "avg_logprob": -0.10152310870942616, "compression_ratio": 1.758490566037736, "no_speech_prob": 0.0012580696493387222}, {"id": 355, "seek": 206580, "start": 2066.6000000000004, "end": 2071.5600000000004, "text": " equivalent or invariant layers. You can also interleave them with pooling. So, I will not talk", "tokens": [50404, 10344, 420, 33270, 394, 7914, 13, 509, 393, 611, 728, 306, 946, 552, 365, 7005, 278, 13, 407, 11, 286, 486, 406, 751, 50652], "temperature": 0.0, "avg_logprob": -0.08753069376541396, "compression_ratio": 1.6187290969899666, "no_speech_prob": 0.002713354304432869}, {"id": 356, "seek": 206580, "start": 2071.5600000000004, "end": 2076.52, "text": " about it too much. But pooling is implementation of another principle that is important in physics,", "tokens": [50652, 466, 309, 886, 709, 13, 583, 7005, 278, 307, 11420, 295, 1071, 8665, 300, 307, 1021, 294, 10649, 11, 50900], "temperature": 0.0, "avg_logprob": -0.08753069376541396, "compression_ratio": 1.6187290969899666, "no_speech_prob": 0.002713354304432869}, {"id": 357, "seek": 206580, "start": 2076.52, "end": 2082.28, "text": " which is called scale separation. And this is what makes physics work. So, if you consider, for", "tokens": [50900, 597, 307, 1219, 4373, 14634, 13, 400, 341, 307, 437, 1669, 10649, 589, 13, 407, 11, 498, 291, 1949, 11, 337, 51188], "temperature": 0.0, "avg_logprob": -0.08753069376541396, "compression_ratio": 1.6187290969899666, "no_speech_prob": 0.002713354304432869}, {"id": 358, "seek": 206580, "start": 2082.28, "end": 2088.2000000000003, "text": " example, let's say, this room, right? And how we are surrounded by probably a quadrillion of", "tokens": [51188, 1365, 11, 718, 311, 584, 11, 341, 1808, 11, 558, 30, 400, 577, 321, 366, 13221, 538, 1391, 257, 10787, 81, 11836, 295, 51484], "temperature": 0.0, "avg_logprob": -0.08753069376541396, "compression_ratio": 1.6187290969899666, "no_speech_prob": 0.002713354304432869}, {"id": 359, "seek": 206580, "start": 2088.2000000000003, "end": 2094.6800000000003, "text": " different molecules that move very fast and collide with each other. But that's not how we can model", "tokens": [51484, 819, 13093, 300, 1286, 588, 2370, 293, 49093, 365, 1184, 661, 13, 583, 300, 311, 406, 577, 321, 393, 2316, 51808], "temperature": 0.0, "avg_logprob": -0.08753069376541396, "compression_ratio": 1.6187290969899666, "no_speech_prob": 0.002713354304432869}, {"id": 360, "seek": 209468, "start": 2094.68, "end": 2101.8799999999997, "text": " the behavior of gas in some space, right? It's computation intractable to trace all the molecules.", "tokens": [50364, 264, 5223, 295, 4211, 294, 512, 1901, 11, 558, 30, 467, 311, 24903, 560, 1897, 712, 281, 13508, 439, 264, 13093, 13, 50724], "temperature": 0.0, "avg_logprob": -0.11154097097891348, "compression_ratio": 1.6156462585034013, "no_speech_prob": 0.0018870471976697445}, {"id": 361, "seek": 209468, "start": 2101.8799999999997, "end": 2107.8799999999997, "text": " So, fortunately, there are just a few parameters that explain statistically how these gas behaves,", "tokens": [50724, 407, 11, 25511, 11, 456, 366, 445, 257, 1326, 9834, 300, 2903, 36478, 577, 613, 4211, 36896, 11, 51024], "temperature": 0.0, "avg_logprob": -0.11154097097891348, "compression_ratio": 1.6156462585034013, "no_speech_prob": 0.0018870471976697445}, {"id": 362, "seek": 209468, "start": 2107.8799999999997, "end": 2113.48, "text": " right? Like temperature and pressure. And this is the main principle of statistical mechanics.", "tokens": [51024, 558, 30, 1743, 4292, 293, 3321, 13, 400, 341, 307, 264, 2135, 8665, 295, 22820, 12939, 13, 51304], "temperature": 0.0, "avg_logprob": -0.11154097097891348, "compression_ratio": 1.6156462585034013, "no_speech_prob": 0.0018870471976697445}, {"id": 363, "seek": 209468, "start": 2113.48, "end": 2117.3999999999996, "text": " But if we want, for example, to model how Earth, with its very complicated atmosphere,", "tokens": [51304, 583, 498, 321, 528, 11, 337, 1365, 11, 281, 2316, 577, 4755, 11, 365, 1080, 588, 6179, 8018, 11, 51500], "temperature": 0.0, "avg_logprob": -0.11154097097891348, "compression_ratio": 1.6156462585034013, "no_speech_prob": 0.0018870471976697445}, {"id": 364, "seek": 209468, "start": 2117.3999999999996, "end": 2122.44, "text": " moves around the sun, again, we can completely disregard it and consider it as a point. Because", "tokens": [51500, 6067, 926, 264, 3295, 11, 797, 11, 321, 393, 2584, 44493, 309, 293, 1949, 309, 382, 257, 935, 13, 1436, 51752], "temperature": 0.0, "avg_logprob": -0.11154097097891348, "compression_ratio": 1.6156462585034013, "no_speech_prob": 0.0018870471976697445}, {"id": 365, "seek": 212244, "start": 2122.44, "end": 2128.36, "text": " at that scale, all these details are completely irrelevant. So, the scales, of course, interact", "tokens": [50364, 412, 300, 4373, 11, 439, 613, 4365, 366, 2584, 28682, 13, 407, 11, 264, 17408, 11, 295, 1164, 11, 4648, 50660], "temperature": 0.0, "avg_logprob": -0.09734375910325484, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.0033951837103813887}, {"id": 366, "seek": 212244, "start": 2128.36, "end": 2134.04, "text": " with each other. So, this is maybe a wishful thinking. And in neural networks, you can also", "tokens": [50660, 365, 1184, 661, 13, 407, 11, 341, 307, 1310, 257, 3172, 906, 1953, 13, 400, 294, 18161, 9590, 11, 291, 393, 611, 50944], "temperature": 0.0, "avg_logprob": -0.09734375910325484, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.0033951837103813887}, {"id": 367, "seek": 212244, "start": 2134.04, "end": 2142.12, "text": " mathematically show that in some architectures, pooling operations are necessary for them to", "tokens": [50944, 44003, 855, 300, 294, 512, 6331, 1303, 11, 7005, 278, 7705, 366, 4818, 337, 552, 281, 51348], "temperature": 0.0, "avg_logprob": -0.09734375910325484, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.0033951837103813887}, {"id": 368, "seek": 212244, "start": 2142.12, "end": 2148.2000000000003, "text": " operate correctly. So, these ideas can be applied to different types of objects, to geometric domains.", "tokens": [51348, 9651, 8944, 13, 407, 11, 613, 3487, 393, 312, 6456, 281, 819, 3467, 295, 6565, 11, 281, 33246, 25514, 13, 51652], "temperature": 0.0, "avg_logprob": -0.09734375910325484, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.0033951837103813887}, {"id": 369, "seek": 214820, "start": 2148.2, "end": 2152.6, "text": " So, traditionally to grids, but then also to graphs, maybe to general", "tokens": [50364, 407, 11, 19067, 281, 677, 3742, 11, 457, 550, 611, 281, 24877, 11, 1310, 281, 2674, 50584], "temperature": 0.0, "avg_logprob": -0.12643333750033597, "compression_ratio": 1.6273062730627306, "no_speech_prob": 0.002734648296609521}, {"id": 370, "seek": 214820, "start": 2153.64, "end": 2159.0, "text": " homogeneous spaces, and then also maybe to more exotic things like manifolds, meshes and", "tokens": [50636, 42632, 7673, 11, 293, 550, 611, 1310, 281, 544, 27063, 721, 411, 8173, 31518, 11, 3813, 8076, 293, 50904], "temperature": 0.0, "avg_logprob": -0.12643333750033597, "compression_ratio": 1.6273062730627306, "no_speech_prob": 0.002734648296609521}, {"id": 371, "seek": 214820, "start": 2159.0, "end": 2163.3999999999996, "text": " geometric graphs. And if you look at some of the standard architectures that are very commonly", "tokens": [50904, 33246, 24877, 13, 400, 498, 291, 574, 412, 512, 295, 264, 3832, 6331, 1303, 300, 366, 588, 12719, 51124], "temperature": 0.0, "avg_logprob": -0.12643333750033597, "compression_ratio": 1.6273062730627306, "no_speech_prob": 0.002734648296609521}, {"id": 372, "seek": 214820, "start": 2163.3999999999996, "end": 2169.8799999999997, "text": " used in deep learning, whether it's CNNs or maybe LSTMs or deep sets or transformers or GNNs or", "tokens": [51124, 1143, 294, 2452, 2539, 11, 1968, 309, 311, 24859, 82, 420, 1310, 441, 6840, 26386, 420, 2452, 6352, 420, 4088, 433, 420, 460, 45, 45, 82, 420, 51448], "temperature": 0.0, "avg_logprob": -0.12643333750033597, "compression_ratio": 1.6273062730627306, "no_speech_prob": 0.002734648296609521}, {"id": 373, "seek": 214820, "start": 2170.6, "end": 2175.48, "text": " intrinsic mesh convolutional networks, they can all be derived from the same blueprint. So,", "tokens": [51484, 35698, 17407, 45216, 304, 9590, 11, 436, 393, 439, 312, 18949, 490, 264, 912, 35868, 13, 407, 11, 51728], "temperature": 0.0, "avg_logprob": -0.12643333750033597, "compression_ratio": 1.6273062730627306, "no_speech_prob": 0.002734648296609521}, {"id": 374, "seek": 217548, "start": 2175.48, "end": 2181.4, "text": " there is some kind of domain and associated symmetry and associated environments that you", "tokens": [50364, 456, 307, 512, 733, 295, 9274, 293, 6615, 25440, 293, 6615, 12388, 300, 291, 50660], "temperature": 0.0, "avg_logprob": -0.11717453922133848, "compression_ratio": 1.6127659574468085, "no_speech_prob": 0.0027207117527723312}, {"id": 375, "seek": 217548, "start": 2181.4, "end": 2187.56, "text": " can bake into your architecture and you get, basically, particular instances of this blueprint,", "tokens": [50660, 393, 16562, 666, 428, 9482, 293, 291, 483, 11, 1936, 11, 1729, 14519, 295, 341, 35868, 11, 50968], "temperature": 0.0, "avg_logprob": -0.11717453922133848, "compression_ratio": 1.6127659574468085, "no_speech_prob": 0.0027207117527723312}, {"id": 376, "seek": 217548, "start": 2187.56, "end": 2194.52, "text": " some of the most common and famous architectures. Any questions so far before we start talking", "tokens": [50968, 512, 295, 264, 881, 2689, 293, 4618, 6331, 1303, 13, 2639, 1651, 370, 1400, 949, 321, 722, 1417, 51316], "temperature": 0.0, "avg_logprob": -0.11717453922133848, "compression_ratio": 1.6127659574468085, "no_speech_prob": 0.0027207117527723312}, {"id": 377, "seek": 217548, "start": 2194.52, "end": 2201.56, "text": " in detail about graphs? So, if I have time, I will try to cover all these different domains, but I", "tokens": [51316, 294, 2607, 466, 24877, 30, 407, 11, 498, 286, 362, 565, 11, 286, 486, 853, 281, 2060, 439, 613, 819, 25514, 11, 457, 286, 51668], "temperature": 0.0, "avg_logprob": -0.11717453922133848, "compression_ratio": 1.6127659574468085, "no_speech_prob": 0.0027207117527723312}, {"id": 378, "seek": 220156, "start": 2201.56, "end": 2208.36, "text": " would probably spend most time on graphs and also some physics-inspired perspective on these", "tokens": [50364, 576, 1391, 3496, 881, 565, 322, 24877, 293, 611, 512, 10649, 12, 31637, 1824, 4585, 322, 613, 50704], "temperature": 0.0, "avg_logprob": -0.17035916511048663, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.006555383559316397}, {"id": 379, "seek": 220156, "start": 2208.36, "end": 2214.2799999999997, "text": " architectures. Yes, I will have questions. So, if you kind the term geometric deep learning,", "tokens": [50704, 6331, 1303, 13, 1079, 11, 286, 486, 362, 1651, 13, 407, 11, 498, 291, 733, 264, 1433, 33246, 2452, 2539, 11, 51000], "temperature": 0.0, "avg_logprob": -0.17035916511048663, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.006555383559316397}, {"id": 380, "seek": 220156, "start": 2214.2799999999997, "end": 2219.4, "text": " is there an alternative, like non-geometric deep learning or like traditional deep learning,", "tokens": [51000, 307, 456, 364, 8535, 11, 411, 2107, 12, 432, 29470, 2452, 2539, 420, 411, 5164, 2452, 2539, 11, 51256], "temperature": 0.0, "avg_logprob": -0.17035916511048663, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.006555383559316397}, {"id": 381, "seek": 220156, "start": 2219.4, "end": 2226.12, "text": " or what does it mean? Well, so, it's a very broad term. So, I think we use it maybe in a very", "tokens": [51256, 420, 437, 775, 309, 914, 30, 1042, 11, 370, 11, 309, 311, 257, 588, 4152, 1433, 13, 407, 11, 286, 519, 321, 764, 309, 1310, 294, 257, 588, 51592], "temperature": 0.0, "avg_logprob": -0.17035916511048663, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.006555383559316397}, {"id": 382, "seek": 222612, "start": 2227.08, "end": 2233.72, "text": " in a very broad sense. So, it's using geometric ideas or geometric techniques to", "tokens": [50412, 294, 257, 588, 4152, 2020, 13, 407, 11, 309, 311, 1228, 33246, 3487, 420, 33246, 7512, 281, 50744], "temperature": 0.0, "avg_logprob": -0.180947208404541, "compression_ratio": 1.7480314960629921, "no_speech_prob": 0.0028376539703458548}, {"id": 383, "seek": 222612, "start": 2234.44, "end": 2241.08, "text": " interpret and build deep learning architectures and vice versa, applying deep learning to", "tokens": [50780, 7302, 293, 1322, 2452, 2539, 6331, 1303, 293, 11964, 25650, 11, 9275, 2452, 2539, 281, 51112], "temperature": 0.0, "avg_logprob": -0.180947208404541, "compression_ratio": 1.7480314960629921, "no_speech_prob": 0.0028376539703458548}, {"id": 384, "seek": 222612, "start": 2241.08, "end": 2246.04, "text": " geometric objects. Now, whether, for example, the model of group environments and equity", "tokens": [51112, 33246, 6565, 13, 823, 11, 1968, 11, 337, 1365, 11, 264, 2316, 295, 1594, 12388, 293, 10769, 51360], "temperature": 0.0, "avg_logprob": -0.180947208404541, "compression_ratio": 1.7480314960629921, "no_speech_prob": 0.0028376539703458548}, {"id": 385, "seek": 222612, "start": 2246.04, "end": 2250.6, "text": " environments is really the kind of ultimate truth, of course not. So, it's a mathematical", "tokens": [51360, 12388, 307, 534, 264, 733, 295, 9705, 3494, 11, 295, 1164, 406, 13, 407, 11, 309, 311, 257, 18894, 51588], "temperature": 0.0, "avg_logprob": -0.180947208404541, "compression_ratio": 1.7480314960629921, "no_speech_prob": 0.0028376539703458548}, {"id": 386, "seek": 222612, "start": 2250.6, "end": 2254.92, "text": " abstraction and in most cases, the transformations, for example, you have an image is, they're", "tokens": [51588, 37765, 293, 294, 881, 3331, 11, 264, 34852, 11, 337, 1365, 11, 291, 362, 364, 3256, 307, 11, 436, 434, 51804], "temperature": 0.0, "avg_logprob": -0.180947208404541, "compression_ratio": 1.7480314960629921, "no_speech_prob": 0.0028376539703458548}, {"id": 387, "seek": 225492, "start": 2254.92, "end": 2260.2000000000003, "text": " actually not well described by groups, but at least it's a good starting point. In many cases,", "tokens": [50364, 767, 406, 731, 7619, 538, 3935, 11, 457, 412, 1935, 309, 311, 257, 665, 2891, 935, 13, 682, 867, 3331, 11, 50628], "temperature": 0.0, "avg_logprob": -0.11813922155471075, "compression_ratio": 1.4938271604938271, "no_speech_prob": 0.003975600935518742}, {"id": 388, "seek": 225492, "start": 2260.2000000000003, "end": 2267.56, "text": " for example, in molecules, this is kind of physical realities or an inductive bias that you", "tokens": [50628, 337, 1365, 11, 294, 13093, 11, 341, 307, 733, 295, 4001, 27785, 420, 364, 31612, 488, 12577, 300, 291, 50996], "temperature": 0.0, "avg_logprob": -0.11813922155471075, "compression_ratio": 1.4938271604938271, "no_speech_prob": 0.003975600935518742}, {"id": 389, "seek": 225492, "start": 2267.56, "end": 2275.96, "text": " rather want to incorporate in your architecture. Okay, I have one more question. Could you please", "tokens": [50996, 2831, 528, 281, 16091, 294, 428, 9482, 13, 1033, 11, 286, 362, 472, 544, 1168, 13, 7497, 291, 1767, 51416], "temperature": 0.0, "avg_logprob": -0.11813922155471075, "compression_ratio": 1.4938271604938271, "no_speech_prob": 0.003975600935518742}, {"id": 390, "seek": 225492, "start": 2275.96, "end": 2281.56, "text": " take back to the previous slide? Yeah. Because you mentioned like the existing", "tokens": [51416, 747, 646, 281, 264, 3894, 4137, 30, 865, 13, 1436, 291, 2835, 411, 264, 6741, 51696], "temperature": 0.0, "avg_logprob": -0.11813922155471075, "compression_ratio": 1.4938271604938271, "no_speech_prob": 0.003975600935518742}, {"id": 391, "seek": 228156, "start": 2281.64, "end": 2289.72, "text": " neural networks architectures and the group of symmetries. And do you think there is any", "tokens": [50368, 18161, 9590, 6331, 1303, 293, 264, 1594, 295, 14232, 302, 2244, 13, 400, 360, 291, 519, 456, 307, 604, 50772], "temperature": 0.0, "avg_logprob": -0.21101592903706565, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.047727279365062714}, {"id": 392, "seek": 228156, "start": 2290.7599999999998, "end": 2297.16, "text": " other group of symmetries that we in real world applications should think about? And", "tokens": [50824, 661, 1594, 295, 14232, 302, 2244, 300, 321, 294, 957, 1002, 5821, 820, 519, 466, 30, 400, 51144], "temperature": 0.0, "avg_logprob": -0.21101592903706565, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.047727279365062714}, {"id": 393, "seek": 228156, "start": 2297.96, "end": 2307.7999999999997, "text": " because of that, create some new architectures that is, I didn't know, for example, was suited for", "tokens": [51184, 570, 295, 300, 11, 1884, 512, 777, 6331, 1303, 300, 307, 11, 286, 994, 380, 458, 11, 337, 1365, 11, 390, 24736, 337, 51676], "temperature": 0.0, "avg_logprob": -0.21101592903706565, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.047727279365062714}, {"id": 394, "seek": 230780, "start": 2308.52, "end": 2313.4, "text": " them? Yeah. So, it's a good question. So, there are some other architectures, for example, mostly", "tokens": [50400, 552, 30, 865, 13, 407, 11, 309, 311, 257, 665, 1168, 13, 407, 11, 456, 366, 512, 661, 6331, 1303, 11, 337, 1365, 11, 5240, 50644], "temperature": 0.0, "avg_logprob": -0.16366924313332537, "compression_ratio": 1.9860627177700347, "no_speech_prob": 0.00336705194786191}, {"id": 395, "seek": 230780, "start": 2313.4, "end": 2317.88, "text": " coming from physics. So, of course, in physics, you have interesting groups. So, I think there are", "tokens": [50644, 1348, 490, 10649, 13, 407, 11, 295, 1164, 11, 294, 10649, 11, 291, 362, 1880, 3935, 13, 407, 11, 286, 519, 456, 366, 50868], "temperature": 0.0, "avg_logprob": -0.16366924313332537, "compression_ratio": 1.9860627177700347, "no_speech_prob": 0.00336705194786191}, {"id": 396, "seek": 230780, "start": 2317.88, "end": 2322.36, "text": " some Lorentz environments, for example, neural network architectures. You can also combine some", "tokens": [50868, 512, 36994, 580, 89, 12388, 11, 337, 1365, 11, 18161, 3209, 6331, 1303, 13, 509, 393, 611, 10432, 512, 51092], "temperature": 0.0, "avg_logprob": -0.16366924313332537, "compression_ratio": 1.9860627177700347, "no_speech_prob": 0.00336705194786191}, {"id": 397, "seek": 230780, "start": 2322.36, "end": 2326.28, "text": " groups. I will show some examples that you can have, for example, products of permutations or", "tokens": [51092, 3935, 13, 286, 486, 855, 512, 5110, 300, 291, 393, 362, 11, 337, 1365, 11, 3383, 295, 4784, 325, 763, 420, 51288], "temperature": 0.0, "avg_logprob": -0.16366924313332537, "compression_ratio": 1.9860627177700347, "no_speech_prob": 0.00336705194786191}, {"id": 398, "seek": 230780, "start": 2326.28, "end": 2332.28, "text": " some special group products of permutations in subcraft neural networks. So, it's a general", "tokens": [51288, 512, 2121, 1594, 3383, 295, 4784, 325, 763, 294, 1422, 5611, 18161, 9590, 13, 407, 11, 309, 311, 257, 2674, 51588], "temperature": 0.0, "avg_logprob": -0.16366924313332537, "compression_ratio": 1.9860627177700347, "no_speech_prob": 0.00336705194786191}, {"id": 399, "seek": 230780, "start": 2332.28, "end": 2337.32, "text": " blueprint. So, if you can follow the blueprint and design architecture that implements this", "tokens": [51588, 35868, 13, 407, 11, 498, 291, 393, 1524, 264, 35868, 293, 1715, 9482, 300, 704, 17988, 341, 51840], "temperature": 0.0, "avg_logprob": -0.16366924313332537, "compression_ratio": 1.9860627177700347, "no_speech_prob": 0.00336705194786191}, {"id": 400, "seek": 233732, "start": 2337.4, "end": 2344.1200000000003, "text": " invariance, that is relevant for your problem, then you have a way of doing it. Okay. Cool. Thanks.", "tokens": [50368, 33270, 719, 11, 300, 307, 7340, 337, 428, 1154, 11, 550, 291, 362, 257, 636, 295, 884, 309, 13, 1033, 13, 8561, 13, 2561, 13, 50704], "temperature": 0.0, "avg_logprob": -0.1272099381786282, "compression_ratio": 1.6413793103448275, "no_speech_prob": 0.0036618486046791077}, {"id": 401, "seek": 233732, "start": 2344.1200000000003, "end": 2348.04, "text": " There are also some works that try to discover the symmetry group from the data and from the", "tokens": [50704, 821, 366, 611, 512, 1985, 300, 853, 281, 4411, 264, 25440, 1594, 490, 264, 1412, 293, 490, 264, 50900], "temperature": 0.0, "avg_logprob": -0.1272099381786282, "compression_ratio": 1.6413793103448275, "no_speech_prob": 0.0036618486046791077}, {"id": 402, "seek": 233732, "start": 2348.04, "end": 2355.4, "text": " problem. So, that's also an interesting direction. Could you shortly address the symmetries which", "tokens": [50900, 1154, 13, 407, 11, 300, 311, 611, 364, 1880, 3513, 13, 7497, 291, 13392, 2985, 264, 14232, 302, 2244, 597, 51268], "temperature": 0.0, "avg_logprob": -0.1272099381786282, "compression_ratio": 1.6413793103448275, "no_speech_prob": 0.0036618486046791077}, {"id": 403, "seek": 233732, "start": 2355.4, "end": 2360.6800000000003, "text": " the transformers go for, because I still have difficulty to think in this symmetry, like,", "tokens": [51268, 264, 4088, 433, 352, 337, 11, 570, 286, 920, 362, 10360, 281, 519, 294, 341, 25440, 11, 411, 11, 51532], "temperature": 0.0, "avg_logprob": -0.1272099381786282, "compression_ratio": 1.6413793103448275, "no_speech_prob": 0.0036618486046791077}, {"id": 404, "seek": 233732, "start": 2360.6800000000003, "end": 2365.4, "text": " perspective? So, I will talk. Well, it will take me probably about 20 minutes, but we'll get to", "tokens": [51532, 4585, 30, 407, 11, 286, 486, 751, 13, 1042, 11, 309, 486, 747, 385, 1391, 466, 945, 2077, 11, 457, 321, 603, 483, 281, 51768], "temperature": 0.0, "avg_logprob": -0.1272099381786282, "compression_ratio": 1.6413793103448275, "no_speech_prob": 0.0036618486046791077}, {"id": 405, "seek": 236540, "start": 2365.48, "end": 2369.32, "text": " transformers. Yeah. So, transformers are special types of craft neural networks. You can think of", "tokens": [50368, 4088, 433, 13, 865, 13, 407, 11, 4088, 433, 366, 2121, 3467, 295, 8448, 18161, 9590, 13, 509, 393, 519, 295, 50560], "temperature": 0.0, "avg_logprob": -0.18202518722386035, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.0011606374755501747}, {"id": 406, "seek": 236540, "start": 2369.32, "end": 2377.08, "text": " them really quickly. So, on the slide, there's the LSTM. So, it's written that time warping is", "tokens": [50560, 552, 534, 2661, 13, 407, 11, 322, 264, 4137, 11, 456, 311, 264, 441, 6840, 44, 13, 407, 11, 309, 311, 3720, 300, 565, 1516, 3381, 307, 50948], "temperature": 0.0, "avg_logprob": -0.18202518722386035, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.0011606374755501747}, {"id": 407, "seek": 236540, "start": 2378.04, "end": 2383.88, "text": " you could have architectures that are invariant to time warping, but if you look at speech recognition.", "tokens": [50996, 291, 727, 362, 6331, 1303, 300, 366, 33270, 394, 281, 565, 1516, 3381, 11, 457, 498, 291, 574, 412, 6218, 11150, 13, 51288], "temperature": 0.0, "avg_logprob": -0.18202518722386035, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.0011606374755501747}, {"id": 408, "seek": 236540, "start": 2383.88, "end": 2390.44, "text": " So, maybe first of all, we humans also don't have that, right? So, if something is super", "tokens": [51288, 407, 11, 1310, 700, 295, 439, 11, 321, 6255, 611, 500, 380, 362, 300, 11, 558, 30, 407, 11, 498, 746, 307, 1687, 51616], "temperature": 0.0, "avg_logprob": -0.18202518722386035, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.0011606374755501747}, {"id": 409, "seek": 239044, "start": 2390.44, "end": 2396.04, "text": " slow down, we're probably not going to notice what it is. So, my question is, is there any", "tokens": [50364, 2964, 760, 11, 321, 434, 1391, 406, 516, 281, 3449, 437, 309, 307, 13, 407, 11, 452, 1168, 307, 11, 307, 456, 604, 50644], "temperature": 0.0, "avg_logprob": -0.11199172170538652, "compression_ratio": 1.6347826086956523, "no_speech_prob": 0.0023343886714428663}, {"id": 410, "seek": 239044, "start": 2396.04, "end": 2401.16, "text": " other invariance or equivalence for time series data that you can think of? So, well, here what", "tokens": [50644, 661, 33270, 719, 420, 9052, 655, 337, 565, 2638, 1412, 300, 291, 393, 519, 295, 30, 407, 11, 731, 11, 510, 437, 50900], "temperature": 0.0, "avg_logprob": -0.11199172170538652, "compression_ratio": 1.6347826086956523, "no_speech_prob": 0.0023343886714428663}, {"id": 411, "seek": 239044, "start": 2401.16, "end": 2407.88, "text": " I mean by time warping, you can actually show that gating is a necessary mechanism to be able", "tokens": [50900, 286, 914, 538, 565, 1516, 3381, 11, 291, 393, 767, 855, 300, 290, 990, 307, 257, 4818, 7513, 281, 312, 1075, 51236], "temperature": 0.0, "avg_logprob": -0.11199172170538652, "compression_ratio": 1.6347826086956523, "no_speech_prob": 0.0023343886714428663}, {"id": 412, "seek": 239044, "start": 2407.88, "end": 2415.0, "text": " to accommodate time warping. So, basically, gating emerges as basically as the architecture for", "tokens": [51236, 281, 21410, 565, 1516, 3381, 13, 407, 11, 1936, 11, 290, 990, 38965, 382, 1936, 382, 264, 9482, 337, 51592], "temperature": 0.0, "avg_logprob": -0.11199172170538652, "compression_ratio": 1.6347826086956523, "no_speech_prob": 0.0023343886714428663}, {"id": 413, "seek": 241500, "start": 2415.0, "end": 2420.68, "text": " this kind of, for this kind of invariance. Okay. Got it. Thanks. Yeah, I think there is a question there.", "tokens": [50364, 341, 733, 295, 11, 337, 341, 733, 295, 33270, 719, 13, 1033, 13, 5803, 309, 13, 2561, 13, 865, 11, 286, 519, 456, 307, 257, 1168, 456, 13, 50648], "temperature": 0.0, "avg_logprob": -0.20127226057506742, "compression_ratio": 1.5226130653266332, "no_speech_prob": 0.004530519247055054}, {"id": 414, "seek": 241500, "start": 2426.76, "end": 2436.6, "text": " So, okay. I mean, so you said that you will talk about transformers more in, like, in the next few", "tokens": [50952, 407, 11, 1392, 13, 286, 914, 11, 370, 291, 848, 300, 291, 486, 751, 466, 4088, 433, 544, 294, 11, 411, 11, 294, 264, 958, 1326, 51444], "temperature": 0.0, "avg_logprob": -0.20127226057506742, "compression_ratio": 1.5226130653266332, "no_speech_prob": 0.004530519247055054}, {"id": 415, "seek": 241500, "start": 2436.6, "end": 2444.68, "text": " slides, but I wanted to ask about one of their emergent properties, like in relation, for example,", "tokens": [51444, 9788, 11, 457, 286, 1415, 281, 1029, 466, 472, 295, 641, 4345, 6930, 7221, 11, 411, 294, 9721, 11, 337, 1365, 11, 51848], "temperature": 0.0, "avg_logprob": -0.20127226057506742, "compression_ratio": 1.5226130653266332, "no_speech_prob": 0.004530519247055054}, {"id": 416, "seek": 244468, "start": 2444.68, "end": 2454.12, "text": " to CNNs, namely that CNNs have, like, kind of encoded in them by definition translation invariance,", "tokens": [50364, 281, 24859, 82, 11, 20926, 300, 24859, 82, 362, 11, 411, 11, 733, 295, 2058, 12340, 294, 552, 538, 7123, 12853, 33270, 719, 11, 50836], "temperature": 0.0, "avg_logprob": -0.13381216866629464, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.013535832054913044}, {"id": 417, "seek": 244468, "start": 2454.12, "end": 2460.2, "text": " whereas transformers don't. And I think it has been discovered that transformers actually kind", "tokens": [50836, 9735, 4088, 433, 500, 380, 13, 400, 286, 519, 309, 575, 668, 6941, 300, 4088, 433, 767, 733, 51140], "temperature": 0.0, "avg_logprob": -0.13381216866629464, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.013535832054913044}, {"id": 418, "seek": 244468, "start": 2460.2, "end": 2466.7599999999998, "text": " of learned the translation invariance from the, thanks to the amount of data that they, that they", "tokens": [51140, 295, 3264, 264, 12853, 33270, 719, 490, 264, 11, 3231, 281, 264, 2372, 295, 1412, 300, 436, 11, 300, 436, 51468], "temperature": 0.0, "avg_logprob": -0.13381216866629464, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.013535832054913044}, {"id": 419, "seek": 246676, "start": 2466.76, "end": 2476.6800000000003, "text": " consume. But, like, they weren't defined to do that. It's kind of, this translation invariance", "tokens": [50364, 14732, 13, 583, 11, 411, 11, 436, 4999, 380, 7642, 281, 360, 300, 13, 467, 311, 733, 295, 11, 341, 12853, 33270, 719, 50860], "temperature": 0.0, "avg_logprob": -0.10109370463603251, "compression_ratio": 1.5026178010471205, "no_speech_prob": 0.022099312394857407}, {"id": 420, "seek": 246676, "start": 2476.6800000000003, "end": 2484.36, "text": " just emerged from the data. So, I was wondering whether, in your opinion, it is better to, like,", "tokens": [50860, 445, 20178, 490, 264, 1412, 13, 407, 11, 286, 390, 6359, 1968, 11, 294, 428, 4800, 11, 309, 307, 1101, 281, 11, 411, 11, 51244], "temperature": 0.0, "avg_logprob": -0.10109370463603251, "compression_ratio": 1.5026178010471205, "no_speech_prob": 0.022099312394857407}, {"id": 421, "seek": 246676, "start": 2484.36, "end": 2494.36, "text": " use more generic architectures, which learn those inductive biases from the data, or whether we", "tokens": [51244, 764, 544, 19577, 6331, 1303, 11, 597, 1466, 729, 31612, 488, 32152, 490, 264, 1412, 11, 420, 1968, 321, 51744], "temperature": 0.0, "avg_logprob": -0.10109370463603251, "compression_ratio": 1.5026178010471205, "no_speech_prob": 0.022099312394857407}, {"id": 422, "seek": 249436, "start": 2494.36, "end": 2499.56, "text": " should encode the inductive biases in their architecture, or, and, you know, face the risk that", "tokens": [50364, 820, 2058, 1429, 264, 31612, 488, 32152, 294, 641, 9482, 11, 420, 11, 293, 11, 291, 458, 11, 1851, 264, 3148, 300, 50624], "temperature": 0.0, "avg_logprob": -0.11347730042504482, "compression_ratio": 1.8007246376811594, "no_speech_prob": 0.00476990919560194}, {"id": 423, "seek": 249436, "start": 2500.28, "end": 2507.1600000000003, "text": " a specific inductive bias may be also a limitation. Yeah. Well, I think the answer is already contained", "tokens": [50660, 257, 2685, 31612, 488, 12577, 815, 312, 611, 257, 27432, 13, 865, 13, 1042, 11, 286, 519, 264, 1867, 307, 1217, 16212, 51004], "temperature": 0.0, "avg_logprob": -0.11347730042504482, "compression_ratio": 1.8007246376811594, "no_speech_prob": 0.00476990919560194}, {"id": 424, "seek": 249436, "start": 2507.1600000000003, "end": 2512.28, "text": " at least one of the answers in your question, right? So, the amount of data is obviously a limitation.", "tokens": [51004, 412, 1935, 472, 295, 264, 6338, 294, 428, 1168, 11, 558, 30, 407, 11, 264, 2372, 295, 1412, 307, 2745, 257, 27432, 13, 51260], "temperature": 0.0, "avg_logprob": -0.11347730042504482, "compression_ratio": 1.8007246376811594, "no_speech_prob": 0.00476990919560194}, {"id": 425, "seek": 249436, "start": 2512.28, "end": 2518.84, "text": " So, it might be easy to collect data, like images, right, or maybe text. It might be much more difficult", "tokens": [51260, 407, 11, 309, 1062, 312, 1858, 281, 2500, 1412, 11, 411, 5267, 11, 558, 11, 420, 1310, 2487, 13, 467, 1062, 312, 709, 544, 2252, 51588], "temperature": 0.0, "avg_logprob": -0.11347730042504482, "compression_ratio": 1.8007246376811594, "no_speech_prob": 0.00476990919560194}, {"id": 426, "seek": 249436, "start": 2518.84, "end": 2524.2000000000003, "text": " to collect data that comes from biological experiments, right? So, if the data is limited", "tokens": [51588, 281, 2500, 1412, 300, 1487, 490, 13910, 12050, 11, 558, 30, 407, 11, 498, 264, 1412, 307, 5567, 51856], "temperature": 0.0, "avg_logprob": -0.11347730042504482, "compression_ratio": 1.8007246376811594, "no_speech_prob": 0.00476990919560194}, {"id": 427, "seek": 252420, "start": 2524.2, "end": 2529.16, "text": " and the data is expensive, then probably you want to work hard to incorporate as many inductive", "tokens": [50364, 293, 264, 1412, 307, 5124, 11, 550, 1391, 291, 528, 281, 589, 1152, 281, 16091, 382, 867, 31612, 488, 50612], "temperature": 0.0, "avg_logprob": -0.11536267272427551, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.001367081655189395}, {"id": 428, "seek": 252420, "start": 2529.16, "end": 2535.72, "text": " biases as you can. In some other applications, actually, the inductive biases, or some symmetries", "tokens": [50612, 32152, 382, 291, 393, 13, 682, 512, 661, 5821, 11, 767, 11, 264, 31612, 488, 32152, 11, 420, 512, 14232, 302, 2244, 50940], "temperature": 0.0, "avg_logprob": -0.11536267272427551, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.001367081655189395}, {"id": 429, "seek": 252420, "start": 2535.72, "end": 2539.16, "text": " or some invariances come from the problem that you're trying to model. I think it's", "tokens": [50940, 420, 512, 33270, 2676, 808, 490, 264, 1154, 300, 291, 434, 1382, 281, 2316, 13, 286, 519, 309, 311, 51112], "temperature": 0.0, "avg_logprob": -0.11536267272427551, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.001367081655189395}, {"id": 430, "seek": 252420, "start": 2539.16, "end": 2544.04, "text": " true for many applications in what is called the AI for science, right? So, if you have some, I don't", "tokens": [51112, 2074, 337, 867, 5821, 294, 437, 307, 1219, 264, 7318, 337, 3497, 11, 558, 30, 407, 11, 498, 291, 362, 512, 11, 286, 500, 380, 51356], "temperature": 0.0, "avg_logprob": -0.11536267272427551, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.001367081655189395}, {"id": 431, "seek": 252420, "start": 2544.04, "end": 2547.7999999999997, "text": " know, physical system, you know that certain properties will be conserved. So, it makes no", "tokens": [51356, 458, 11, 4001, 1185, 11, 291, 458, 300, 1629, 7221, 486, 312, 1014, 6913, 13, 407, 11, 309, 1669, 572, 51544], "temperature": 0.0, "avg_logprob": -0.11536267272427551, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.001367081655189395}, {"id": 432, "seek": 254780, "start": 2547.8, "end": 2555.4, "text": " sense just to use a generic black box that will be producing unrealistic outputs, right,", "tokens": [50364, 2020, 445, 281, 764, 257, 19577, 2211, 2424, 300, 486, 312, 10501, 42867, 23930, 11, 558, 11, 50744], "temperature": 0.0, "avg_logprob": -0.10733887086431664, "compression_ratio": 1.6009174311926606, "no_speech_prob": 0.004982902202755213}, {"id": 433, "seek": 254780, "start": 2555.4, "end": 2563.4, "text": " that are physically incorrect. But there is no, so it might be that in problems where you don't", "tokens": [50744, 300, 366, 9762, 18424, 13, 583, 456, 307, 572, 11, 370, 309, 1062, 312, 300, 294, 2740, 689, 291, 500, 380, 51144], "temperature": 0.0, "avg_logprob": -0.10733887086431664, "compression_ratio": 1.6009174311926606, "no_speech_prob": 0.004982902202755213}, {"id": 434, "seek": 254780, "start": 2563.4, "end": 2569.6400000000003, "text": " know a priori what symmetry or invariance you have, it might be a good idea to use transformers if", "tokens": [51144, 458, 257, 4059, 72, 437, 25440, 420, 33270, 719, 291, 362, 11, 309, 1062, 312, 257, 665, 1558, 281, 764, 4088, 433, 498, 51456], "temperature": 0.0, "avg_logprob": -0.10733887086431664, "compression_ratio": 1.6009174311926606, "no_speech_prob": 0.004982902202755213}, {"id": 435, "seek": 254780, "start": 2570.36, "end": 2572.92, "text": " the computational complexity and the scale of the data allows it.", "tokens": [51492, 264, 28270, 14024, 293, 264, 4373, 295, 264, 1412, 4045, 309, 13, 51620], "temperature": 0.0, "avg_logprob": -0.10733887086431664, "compression_ratio": 1.6009174311926606, "no_speech_prob": 0.004982902202755213}, {"id": 436, "seek": 257292, "start": 2572.92, "end": 2581.8, "text": " Hello, I have a question about augmentation. It feels like, at least in the image case,", "tokens": [50364, 2425, 11, 286, 362, 257, 1168, 466, 14501, 19631, 13, 467, 3417, 411, 11, 412, 1935, 294, 264, 3256, 1389, 11, 50808], "temperature": 0.0, "avg_logprob": -0.17917251586914062, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.0057150572538375854}, {"id": 437, "seek": 257292, "start": 2581.8, "end": 2587.88, "text": " augmentation is basically exploiting this group symmetry, right? You augment by", "tokens": [50808, 14501, 19631, 307, 1936, 12382, 1748, 341, 1594, 25440, 11, 558, 30, 509, 29919, 538, 51112], "temperature": 0.0, "avg_logprob": -0.17917251586914062, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.0057150572538375854}, {"id": 438, "seek": 257292, "start": 2587.88, "end": 2595.48, "text": " doing some translation. But if you use some geometric-based architecture, maybe then you", "tokens": [51112, 884, 512, 12853, 13, 583, 498, 291, 764, 512, 33246, 12, 6032, 9482, 11, 1310, 550, 291, 51492], "temperature": 0.0, "avg_logprob": -0.17917251586914062, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.0057150572538375854}, {"id": 439, "seek": 257292, "start": 2595.48, "end": 2600.92, "text": " don't need to do augmentation anymore. So, what's your opinion of augmentation? Is it like an artifact", "tokens": [51492, 500, 380, 643, 281, 360, 14501, 19631, 3602, 13, 407, 11, 437, 311, 428, 4800, 295, 14501, 19631, 30, 1119, 309, 411, 364, 34806, 51764], "temperature": 0.0, "avg_logprob": -0.17917251586914062, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.0057150572538375854}, {"id": 440, "seek": 260092, "start": 2601.7200000000003, "end": 2607.32, "text": " or something? Well, augmentation is a technique of, basically, when you have limited amount of", "tokens": [50404, 420, 746, 30, 1042, 11, 14501, 19631, 307, 257, 6532, 295, 11, 1936, 11, 562, 291, 362, 5567, 2372, 295, 50684], "temperature": 0.0, "avg_logprob": -0.14120543797810872, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.0008940695552155375}, {"id": 441, "seek": 260092, "start": 2607.32, "end": 2613.56, "text": " data, you can generate synthetic data that looks like the kind of data that you want to see in", "tokens": [50684, 1412, 11, 291, 393, 8460, 23420, 1412, 300, 1542, 411, 264, 733, 295, 1412, 300, 291, 528, 281, 536, 294, 50996], "temperature": 0.0, "avg_logprob": -0.14120543797810872, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.0008940695552155375}, {"id": 442, "seek": 260092, "start": 2613.56, "end": 2618.36, "text": " your application. This was actually one of the important features of the AlexNet, for example,", "tokens": [50996, 428, 3861, 13, 639, 390, 767, 472, 295, 264, 1021, 4122, 295, 264, 5202, 31890, 11, 337, 1365, 11, 51236], "temperature": 0.0, "avg_logprob": -0.14120543797810872, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.0008940695552155375}, {"id": 443, "seek": 260092, "start": 2618.36, "end": 2625.0, "text": " so they use certain type of data augmentation for images to make it more robust. So, again,", "tokens": [51236, 370, 436, 764, 1629, 2010, 295, 1412, 14501, 19631, 337, 5267, 281, 652, 309, 544, 13956, 13, 407, 11, 797, 11, 51568], "temperature": 0.0, "avg_logprob": -0.14120543797810872, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.0008940695552155375}, {"id": 444, "seek": 262500, "start": 2625.0, "end": 2631.0, "text": " you can incorporate this kind of inductive biases into the architecture. Sometimes,", "tokens": [50364, 291, 393, 16091, 341, 733, 295, 31612, 488, 32152, 666, 264, 9482, 13, 4803, 11, 50664], "temperature": 0.0, "avg_logprob": -0.1343650698661804, "compression_ratio": 1.5954545454545455, "no_speech_prob": 0.00542656984180212}, {"id": 445, "seek": 262500, "start": 2631.0, "end": 2635.72, "text": " it might be difficult. So, another aspect that is often overlooked is actually the hardware,", "tokens": [50664, 309, 1062, 312, 2252, 13, 407, 11, 1071, 4171, 300, 307, 2049, 32269, 307, 767, 264, 8837, 11, 50900], "temperature": 0.0, "avg_logprob": -0.1343650698661804, "compression_ratio": 1.5954545454545455, "no_speech_prob": 0.00542656984180212}, {"id": 446, "seek": 262500, "start": 2636.52, "end": 2642.92, "text": " right? So, it's very easy and it's probably more a coincidence. At least originally,", "tokens": [50940, 558, 30, 407, 11, 309, 311, 588, 1858, 293, 309, 311, 1391, 544, 257, 22137, 13, 1711, 1935, 7993, 11, 51260], "temperature": 0.0, "avg_logprob": -0.1343650698661804, "compression_ratio": 1.5954545454545455, "no_speech_prob": 0.00542656984180212}, {"id": 447, "seek": 262500, "start": 2643.88, "end": 2650.92, "text": " the convolutional neural networks map very nicely to the type of hardware that is used in", "tokens": [51308, 264, 45216, 304, 18161, 9590, 4471, 588, 9594, 281, 264, 2010, 295, 8837, 300, 307, 1143, 294, 51660], "temperature": 0.0, "avg_logprob": -0.1343650698661804, "compression_ratio": 1.5954545454545455, "no_speech_prob": 0.00542656984180212}, {"id": 448, "seek": 265092, "start": 2651.0, "end": 2658.76, "text": " the GPU, the single instruction multiple data type of architecture. So, it was not by design", "tokens": [50368, 264, 18407, 11, 264, 2167, 10951, 3866, 1412, 2010, 295, 9482, 13, 407, 11, 309, 390, 406, 538, 1715, 50756], "temperature": 0.0, "avg_logprob": -0.18095359127078436, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.002515048487111926}, {"id": 449, "seek": 265092, "start": 2658.76, "end": 2663.8, "text": " because the GPUs were designed for different types of problems. With other architectures,", "tokens": [50756, 570, 264, 18407, 82, 645, 4761, 337, 819, 3467, 295, 2740, 13, 2022, 661, 6331, 1303, 11, 51008], "temperature": 0.0, "avg_logprob": -0.18095359127078436, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.002515048487111926}, {"id": 450, "seek": 265092, "start": 2663.8, "end": 2667.0, "text": " it might not be the case, right? So, with graphing a lot of programmable, this is not the case.", "tokens": [51008, 309, 1062, 406, 312, 264, 1389, 11, 558, 30, 407, 11, 365, 1295, 79, 571, 257, 688, 295, 37648, 712, 11, 341, 307, 406, 264, 1389, 13, 51168], "temperature": 0.0, "avg_logprob": -0.18095359127078436, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.002515048487111926}, {"id": 451, "seek": 265092, "start": 2667.56, "end": 2672.76, "text": " So, there might be some better, so to say, architectures, maybe from mathematical standpoint,", "tokens": [51196, 407, 11, 456, 1062, 312, 512, 1101, 11, 370, 281, 584, 11, 6331, 1303, 11, 1310, 490, 18894, 15827, 11, 51456], "temperature": 0.0, "avg_logprob": -0.18095359127078436, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.002515048487111926}, {"id": 452, "seek": 265092, "start": 2672.76, "end": 2680.44, "text": " but they're just not as convenient to implement. Therefore, maybe you will prefer to use", "tokens": [51456, 457, 436, 434, 445, 406, 382, 10851, 281, 4445, 13, 7504, 11, 1310, 291, 486, 4382, 281, 764, 51840], "temperature": 0.0, "avg_logprob": -0.18095359127078436, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.002515048487111926}, {"id": 453, "seek": 268044, "start": 2680.44, "end": 2686.52, "text": " something that is less correct, but you can do data augmentation. The hardware allows you to", "tokens": [50364, 746, 300, 307, 1570, 3006, 11, 457, 291, 393, 360, 1412, 14501, 19631, 13, 440, 8837, 4045, 291, 281, 50668], "temperature": 0.0, "avg_logprob": -0.16042483936656604, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.002663026563823223}, {"id": 454, "seek": 268044, "start": 2686.52, "end": 2696.04, "text": " implement this architecture better. What do you think is a follow-up question about", "tokens": [50668, 4445, 341, 9482, 1101, 13, 708, 360, 291, 519, 307, 257, 1524, 12, 1010, 1168, 466, 51144], "temperature": 0.0, "avg_logprob": -0.16042483936656604, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.002663026563823223}, {"id": 455, "seek": 268044, "start": 2696.04, "end": 2700.68, "text": " augmentation? What do you think about augmentation? Not as a simple tool for increasing the size of", "tokens": [51144, 14501, 19631, 30, 708, 360, 291, 519, 466, 14501, 19631, 30, 1726, 382, 257, 2199, 2290, 337, 5662, 264, 2744, 295, 51376], "temperature": 0.0, "avg_logprob": -0.16042483936656604, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.002663026563823223}, {"id": 456, "seek": 268044, "start": 2700.68, "end": 2706.28, "text": " the data set, but as something as in contrastive learning, as enforcing the symmetry. Can it be", "tokens": [51376, 264, 1412, 992, 11, 457, 382, 746, 382, 294, 8712, 488, 2539, 11, 382, 25495, 2175, 264, 25440, 13, 1664, 309, 312, 51656], "temperature": 0.0, "avg_logprob": -0.16042483936656604, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.002663026563823223}, {"id": 457, "seek": 270628, "start": 2706.28, "end": 2712.0400000000004, "text": " kind of equivalent to the symmetry you defined? Well, it was used in this formula. I mean,", "tokens": [50364, 733, 295, 10344, 281, 264, 25440, 291, 7642, 30, 1042, 11, 309, 390, 1143, 294, 341, 8513, 13, 286, 914, 11, 50652], "temperature": 0.0, "avg_logprob": -0.17220887930496878, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.008826675824820995}, {"id": 458, "seek": 270628, "start": 2713.8, "end": 2719.8, "text": " can the load of enforcing the symmetry be shifted towards augmentation to the model?", "tokens": [50740, 393, 264, 3677, 295, 25495, 2175, 264, 25440, 312, 18892, 3030, 14501, 19631, 281, 264, 2316, 30, 51040], "temperature": 0.0, "avg_logprob": -0.17220887930496878, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.008826675824820995}, {"id": 459, "seek": 270628, "start": 2720.36, "end": 2724.6800000000003, "text": " Yeah, so basically, you're sampling points from the group orbit, right? So, you can think of it", "tokens": [51068, 865, 11, 370, 1936, 11, 291, 434, 21179, 2793, 490, 264, 1594, 13991, 11, 558, 30, 407, 11, 291, 393, 519, 295, 309, 51284], "temperature": 0.0, "avg_logprob": -0.17220887930496878, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.008826675824820995}, {"id": 460, "seek": 270628, "start": 2724.6800000000003, "end": 2731.48, "text": " this way as well. Whether it's enforced in a hardware or in a software, that's probably not", "tokens": [51284, 341, 636, 382, 731, 13, 8503, 309, 311, 40953, 294, 257, 8837, 420, 294, 257, 4722, 11, 300, 311, 1391, 406, 51624], "temperature": 0.0, "avg_logprob": -0.17220887930496878, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.008826675824820995}, {"id": 461, "seek": 273148, "start": 2731.48, "end": 2737.08, "text": " that important. So, data augmentation is a very valid technique, if you know exactly how to do it.", "tokens": [50364, 300, 1021, 13, 407, 11, 1412, 14501, 19631, 307, 257, 588, 7363, 6532, 11, 498, 291, 458, 2293, 577, 281, 360, 309, 13, 50644], "temperature": 0.0, "avg_logprob": -0.13999435023257606, "compression_ratio": 1.5258964143426295, "no_speech_prob": 0.00181143032386899}, {"id": 462, "seek": 273148, "start": 2739.72, "end": 2746.68, "text": " Okay, so let's move on to graphs. And, well, graphs, as you probably know, their idea itself", "tokens": [50776, 1033, 11, 370, 718, 311, 1286, 322, 281, 24877, 13, 400, 11, 731, 11, 24877, 11, 382, 291, 1391, 458, 11, 641, 1558, 2564, 51124], "temperature": 0.0, "avg_logprob": -0.13999435023257606, "compression_ratio": 1.5258964143426295, "no_speech_prob": 0.00181143032386899}, {"id": 463, "seek": 273148, "start": 2746.68, "end": 2753.2400000000002, "text": " is pretty old. So, it's usually attributed to Euler who was thinking of these kind of problems,", "tokens": [51124, 307, 1238, 1331, 13, 407, 11, 309, 311, 2673, 30976, 281, 462, 26318, 567, 390, 1953, 295, 613, 733, 295, 2740, 11, 51452], "temperature": 0.0, "avg_logprob": -0.13999435023257606, "compression_ratio": 1.5258964143426295, "no_speech_prob": 0.00181143032386899}, {"id": 464, "seek": 273148, "start": 2753.2400000000002, "end": 2758.6, "text": " right? How you can connect land masses without actually accounting for the particular geometry,", "tokens": [51452, 558, 30, 1012, 291, 393, 1745, 2117, 23935, 1553, 767, 19163, 337, 264, 1729, 18426, 11, 51720], "temperature": 0.0, "avg_logprob": -0.13999435023257606, "compression_ratio": 1.5258964143426295, "no_speech_prob": 0.00181143032386899}, {"id": 465, "seek": 275860, "start": 2758.6, "end": 2765.4, "text": " but only how, what is nearby, right? So, the famous problem about the bridges of K\u00f6nigsberg,", "tokens": [50364, 457, 787, 577, 11, 437, 307, 11184, 11, 558, 30, 407, 11, 264, 4618, 1154, 466, 264, 21114, 295, 29077, 17156, 6873, 11, 50704], "temperature": 0.0, "avg_logprob": -0.21376059452692667, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.015325325541198254}, {"id": 466, "seek": 275860, "start": 2765.4, "end": 2771.0, "text": " and this is what he called the geometry of Cetus, or basically the geometry of place,", "tokens": [50704, 293, 341, 307, 437, 415, 1219, 264, 18426, 295, 383, 40506, 11, 420, 1936, 264, 18426, 295, 1081, 11, 50984], "temperature": 0.0, "avg_logprob": -0.21376059452692667, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.015325325541198254}, {"id": 467, "seek": 275860, "start": 2771.0, "end": 2777.3199999999997, "text": " what in modern terminology we call topology. So, the term was actually introduced by Pankareho,", "tokens": [50984, 437, 294, 4363, 27575, 321, 818, 1192, 1793, 13, 407, 11, 264, 1433, 390, 767, 7268, 538, 430, 657, 543, 1289, 11, 51300], "temperature": 0.0, "avg_logprob": -0.21376059452692667, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.015325325541198254}, {"id": 468, "seek": 275860, "start": 2777.3199999999997, "end": 2784.2, "text": " by analogy to Euler's terminology called analysis Cetus, and that was also where his famous conjecture", "tokens": [51300, 538, 21663, 281, 462, 26318, 311, 27575, 1219, 5215, 383, 40506, 11, 293, 300, 390, 611, 689, 702, 4618, 416, 1020, 540, 51644], "temperature": 0.0, "avg_logprob": -0.21376059452692667, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.015325325541198254}, {"id": 469, "seek": 278420, "start": 2784.2, "end": 2789.64, "text": " appeared. So, we'll talk about Pankareho conjecture actually later. I hope to get there as well.", "tokens": [50364, 8516, 13, 407, 11, 321, 603, 751, 466, 430, 657, 543, 1289, 416, 1020, 540, 767, 1780, 13, 286, 1454, 281, 483, 456, 382, 731, 13, 50636], "temperature": 0.0, "avg_logprob": -0.11704795128476303, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.0014660026645287871}, {"id": 470, "seek": 278420, "start": 2790.7599999999998, "end": 2796.04, "text": " So, graphs, obviously, I don't need to convince you that graphs are interesting and important. So,", "tokens": [50692, 407, 11, 24877, 11, 2745, 11, 286, 500, 380, 643, 281, 13447, 291, 300, 24877, 366, 1880, 293, 1021, 13, 407, 11, 50956], "temperature": 0.0, "avg_logprob": -0.11704795128476303, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.0014660026645287871}, {"id": 471, "seek": 278420, "start": 2796.04, "end": 2800.2799999999997, "text": " more or less anything from very small scales or very large scales can be modeled as a graph.", "tokens": [50956, 544, 420, 1570, 1340, 490, 588, 1359, 17408, 420, 588, 2416, 17408, 393, 312, 37140, 382, 257, 4295, 13, 51168], "temperature": 0.0, "avg_logprob": -0.11704795128476303, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.0014660026645287871}, {"id": 472, "seek": 278420, "start": 2800.2799999999997, "end": 2804.6, "text": " So, any system of relations or interactions, whether it's a molecule, or you model how", "tokens": [51168, 407, 11, 604, 1185, 295, 2299, 420, 13280, 11, 1968, 309, 311, 257, 15582, 11, 420, 291, 2316, 577, 51384], "temperature": 0.0, "avg_logprob": -0.11704795128476303, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.0014660026645287871}, {"id": 473, "seek": 278420, "start": 2804.6, "end": 2809.56, "text": " different atoms interact with each other through chemical bonds, to, for example,", "tokens": [51384, 819, 16871, 4648, 365, 1184, 661, 807, 7313, 14713, 11, 281, 11, 337, 1365, 11, 51632], "temperature": 0.0, "avg_logprob": -0.11704795128476303, "compression_ratio": 1.6498194945848375, "no_speech_prob": 0.0014660026645287871}, {"id": 474, "seek": 280956, "start": 2809.64, "end": 2817.7999999999997, "text": " interactoms, right, in biological science, how different entities in our body or in a cell", "tokens": [50368, 4648, 4785, 11, 558, 11, 294, 13910, 3497, 11, 577, 819, 16667, 294, 527, 1772, 420, 294, 257, 2815, 50776], "temperature": 0.0, "avg_logprob": -0.14140980851416493, "compression_ratio": 1.76890756302521, "no_speech_prob": 0.0015239277854561806}, {"id": 475, "seek": 280956, "start": 2817.7999999999997, "end": 2821.88, "text": " interact with each other, different, for example, chemical reactions, or even social networks,", "tokens": [50776, 4648, 365, 1184, 661, 11, 819, 11, 337, 1365, 11, 7313, 12215, 11, 420, 754, 2093, 9590, 11, 50980], "temperature": 0.0, "avg_logprob": -0.14140980851416493, "compression_ratio": 1.76890756302521, "no_speech_prob": 0.0015239277854561806}, {"id": 476, "seek": 280956, "start": 2821.88, "end": 2827.4, "text": " right, describing relations, friendship, interactions between different people.", "tokens": [50980, 558, 11, 16141, 2299, 11, 13216, 11, 13280, 1296, 819, 561, 13, 51256], "temperature": 0.0, "avg_logprob": -0.14140980851416493, "compression_ratio": 1.76890756302521, "no_speech_prob": 0.0015239277854561806}, {"id": 477, "seek": 280956, "start": 2828.12, "end": 2832.52, "text": " So, this model is a graph, and again, graphs can be of different types. So,", "tokens": [51292, 407, 11, 341, 2316, 307, 257, 4295, 11, 293, 797, 11, 24877, 393, 312, 295, 819, 3467, 13, 407, 11, 51512], "temperature": 0.0, "avg_logprob": -0.14140980851416493, "compression_ratio": 1.76890756302521, "no_speech_prob": 0.0015239277854561806}, {"id": 478, "seek": 280956, "start": 2832.52, "end": 2836.92, "text": " let's consider a simple model here. So, we'll consider an undirected graph. So,", "tokens": [51512, 718, 311, 1949, 257, 2199, 2316, 510, 13, 407, 11, 321, 603, 1949, 364, 674, 11890, 292, 4295, 13, 407, 11, 51732], "temperature": 0.0, "avg_logprob": -0.14140980851416493, "compression_ratio": 1.76890756302521, "no_speech_prob": 0.0015239277854561806}, {"id": 479, "seek": 283692, "start": 2836.92, "end": 2845.7200000000003, "text": " it means that we have a collection of nodes, and we have unordered pairs of nodes as edges,", "tokens": [50364, 309, 1355, 300, 321, 362, 257, 5765, 295, 13891, 11, 293, 321, 362, 517, 765, 4073, 15494, 295, 13891, 382, 8819, 11, 50804], "temperature": 0.0, "avg_logprob": -0.09306607497365851, "compression_ratio": 1.6898148148148149, "no_speech_prob": 0.003472341690212488}, {"id": 480, "seek": 283692, "start": 2845.7200000000003, "end": 2853.4, "text": " right? So, just pairs, basically, the order doesn't matter, and the nodes are described", "tokens": [50804, 558, 30, 407, 11, 445, 15494, 11, 1936, 11, 264, 1668, 1177, 380, 1871, 11, 293, 264, 13891, 366, 7619, 51188], "temperature": 0.0, "avg_logprob": -0.09306607497365851, "compression_ratio": 1.6898148148148149, "no_speech_prob": 0.003472341690212488}, {"id": 481, "seek": 283692, "start": 2853.4, "end": 2857.4, "text": " by d-dimensional vectors. So, these are features that are attached to the nodes. And, of course,", "tokens": [51188, 538, 274, 12, 18759, 18875, 13, 407, 11, 613, 366, 4122, 300, 366, 8570, 281, 264, 13891, 13, 400, 11, 295, 1164, 11, 51388], "temperature": 0.0, "avg_logprob": -0.09306607497365851, "compression_ratio": 1.6898148148148149, "no_speech_prob": 0.003472341690212488}, {"id": 482, "seek": 283692, "start": 2857.4, "end": 2861.88, "text": " it can be more complicated. So, you can have graphs that are directed, you can have both", "tokens": [51388, 309, 393, 312, 544, 6179, 13, 407, 11, 291, 393, 362, 24877, 300, 366, 12898, 11, 291, 393, 362, 1293, 51612], "temperature": 0.0, "avg_logprob": -0.09306607497365851, "compression_ratio": 1.6898148148148149, "no_speech_prob": 0.003472341690212488}, {"id": 483, "seek": 286188, "start": 2861.96, "end": 2869.2400000000002, "text": " continuous and categorical features, both in the nodes and the edges. But, just, that would be", "tokens": [50368, 10957, 293, 19250, 804, 4122, 11, 1293, 294, 264, 13891, 293, 264, 8819, 13, 583, 11, 445, 11, 300, 576, 312, 50732], "temperature": 0.0, "avg_logprob": -0.07915221055348715, "compression_ratio": 1.6655052264808363, "no_speech_prob": 0.0017950076144188643}, {"id": 484, "seek": 286188, "start": 2869.2400000000002, "end": 2874.84, "text": " already interesting enough to look at this kind of object. So, one thing that characterizes graphs,", "tokens": [50732, 1217, 1880, 1547, 281, 574, 412, 341, 733, 295, 2657, 13, 407, 11, 472, 551, 300, 2517, 5660, 24877, 11, 51012], "temperature": 0.0, "avg_logprob": -0.07915221055348715, "compression_ratio": 1.6655052264808363, "no_speech_prob": 0.0017950076144188643}, {"id": 485, "seek": 286188, "start": 2874.84, "end": 2879.08, "text": " right, basically, this is, again, a topological construction. So, it's an abstract object that", "tokens": [51012, 558, 11, 1936, 11, 341, 307, 11, 797, 11, 257, 1192, 4383, 6435, 13, 407, 11, 309, 311, 364, 12649, 2657, 300, 51224], "temperature": 0.0, "avg_logprob": -0.07915221055348715, "compression_ratio": 1.6655052264808363, "no_speech_prob": 0.0017950076144188643}, {"id": 486, "seek": 286188, "start": 2879.08, "end": 2884.6800000000003, "text": " lives on its own. The moment we need to represent it on a computer, we describe it, for example,", "tokens": [51224, 2909, 322, 1080, 1065, 13, 440, 1623, 321, 643, 281, 2906, 309, 322, 257, 3820, 11, 321, 6786, 309, 11, 337, 1365, 11, 51504], "temperature": 0.0, "avg_logprob": -0.07915221055348715, "compression_ratio": 1.6655052264808363, "no_speech_prob": 0.0017950076144188643}, {"id": 487, "seek": 286188, "start": 2884.6800000000003, "end": 2888.44, "text": " as a matrix, right? So, we can describe the structure of the graph as the adjacency matrix,", "tokens": [51504, 382, 257, 8141, 11, 558, 30, 407, 11, 321, 393, 6786, 264, 3877, 295, 264, 4295, 382, 264, 22940, 3020, 8141, 11, 51692], "temperature": 0.0, "avg_logprob": -0.07915221055348715, "compression_ratio": 1.6655052264808363, "no_speech_prob": 0.0017950076144188643}, {"id": 488, "seek": 288844, "start": 2888.44, "end": 2893.8, "text": " right, of size n by n, and it's a number of nodes. So, we have one, if there is an agent", "tokens": [50364, 558, 11, 295, 2744, 297, 538, 297, 11, 293, 309, 311, 257, 1230, 295, 13891, 13, 407, 11, 321, 362, 472, 11, 498, 456, 307, 364, 9461, 50632], "temperature": 0.0, "avg_logprob": -0.13715726412259616, "compression_ratio": 1.8045977011494252, "no_speech_prob": 0.0018734527984634042}, {"id": 489, "seek": 288844, "start": 2894.68, "end": 2898.84, "text": " between a pair of nodes and zero, if there is no edge, right? And if the graph is undirected,", "tokens": [50676, 1296, 257, 6119, 295, 13891, 293, 4018, 11, 498, 456, 307, 572, 4691, 11, 558, 30, 400, 498, 264, 4295, 307, 674, 11890, 292, 11, 50884], "temperature": 0.0, "avg_logprob": -0.13715726412259616, "compression_ratio": 1.8045977011494252, "no_speech_prob": 0.0018734527984634042}, {"id": 490, "seek": 288844, "start": 2898.84, "end": 2905.4, "text": " then this matrix is symmetric. And the features, we can describe them as a matrix of size n by d,", "tokens": [50884, 550, 341, 8141, 307, 32330, 13, 400, 264, 4122, 11, 321, 393, 6786, 552, 382, 257, 8141, 295, 2744, 297, 538, 274, 11, 51212], "temperature": 0.0, "avg_logprob": -0.13715726412259616, "compression_ratio": 1.8045977011494252, "no_speech_prob": 0.0018734527984634042}, {"id": 491, "seek": 288844, "start": 2905.4, "end": 2911.08, "text": " right? d is the dimensionality of the node features. So, one key thing here, which is already", "tokens": [51212, 558, 30, 274, 307, 264, 10139, 1860, 295, 264, 9984, 4122, 13, 407, 11, 472, 2141, 551, 510, 11, 597, 307, 1217, 51496], "temperature": 0.0, "avg_logprob": -0.13715726412259616, "compression_ratio": 1.8045977011494252, "no_speech_prob": 0.0018734527984634042}, {"id": 492, "seek": 288844, "start": 2911.08, "end": 2915.2400000000002, "text": " written on this slide, is that we don't really have a canonical way of ordering the nodes of the", "tokens": [51496, 3720, 322, 341, 4137, 11, 307, 300, 321, 500, 380, 534, 362, 257, 46491, 636, 295, 21739, 264, 13891, 295, 264, 51704], "temperature": 0.0, "avg_logprob": -0.13715726412259616, "compression_ratio": 1.8045977011494252, "no_speech_prob": 0.0018734527984634042}, {"id": 493, "seek": 291524, "start": 2915.24, "end": 2920.68, "text": " graph. So, when I make this description, I automatically assume some ordering of the nodes,", "tokens": [50364, 4295, 13, 407, 11, 562, 286, 652, 341, 3855, 11, 286, 6772, 6552, 512, 21739, 295, 264, 13891, 11, 50636], "temperature": 0.0, "avg_logprob": -0.07522255115294724, "compression_ratio": 1.695852534562212, "no_speech_prob": 0.004138553515076637}, {"id": 494, "seek": 291524, "start": 2920.68, "end": 2925.72, "text": " but this ordering can be like this or can be like this. So, anything that will take", "tokens": [50636, 457, 341, 21739, 393, 312, 411, 341, 420, 393, 312, 411, 341, 13, 407, 11, 1340, 300, 486, 747, 50888], "temperature": 0.0, "avg_logprob": -0.07522255115294724, "compression_ratio": 1.695852534562212, "no_speech_prob": 0.004138553515076637}, {"id": 495, "seek": 291524, "start": 2926.7599999999998, "end": 2933.08, "text": " this description of the graph as an input must account for this built-in ambiguity, right? So,", "tokens": [50940, 341, 3855, 295, 264, 4295, 382, 364, 4846, 1633, 2696, 337, 341, 3094, 12, 259, 46519, 11, 558, 30, 407, 11, 51256], "temperature": 0.0, "avg_logprob": -0.07522255115294724, "compression_ratio": 1.695852534562212, "no_speech_prob": 0.004138553515076637}, {"id": 496, "seek": 291524, "start": 2933.08, "end": 2941.3199999999997, "text": " I somehow need to be able to produce outputs that disregard, in a correct way, all these possible", "tokens": [51256, 286, 6063, 643, 281, 312, 1075, 281, 5258, 23930, 300, 44493, 11, 294, 257, 3006, 636, 11, 439, 613, 1944, 51668], "temperature": 0.0, "avg_logprob": -0.07522255115294724, "compression_ratio": 1.695852534562212, "no_speech_prob": 0.004138553515076637}, {"id": 497, "seek": 294132, "start": 2941.32, "end": 2946.92, "text": " permutations, right? And the two types of problems that we can consider in relation to graphs,", "tokens": [50364, 4784, 325, 763, 11, 558, 30, 400, 264, 732, 3467, 295, 2740, 300, 321, 393, 1949, 294, 9721, 281, 24877, 11, 50644], "temperature": 0.0, "avg_logprob": -0.11992705418513372, "compression_ratio": 1.8013698630136987, "no_speech_prob": 0.007359738927334547}, {"id": 498, "seek": 294132, "start": 2947.56, "end": 2950.84, "text": " but actually more problems, but let's say these are the prototypical problems.", "tokens": [50676, 457, 767, 544, 2740, 11, 457, 718, 311, 584, 613, 366, 264, 46219, 34061, 2740, 13, 50840], "temperature": 0.0, "avg_logprob": -0.11992705418513372, "compression_ratio": 1.8013698630136987, "no_speech_prob": 0.007359738927334547}, {"id": 499, "seek": 294132, "start": 2950.84, "end": 2953.88, "text": " One is graph-level problems. So, I give you an input graph and I try to", "tokens": [50840, 1485, 307, 4295, 12, 12418, 2740, 13, 407, 11, 286, 976, 291, 364, 4846, 4295, 293, 286, 853, 281, 50992], "temperature": 0.0, "avg_logprob": -0.11992705418513372, "compression_ratio": 1.8013698630136987, "no_speech_prob": 0.007359738927334547}, {"id": 500, "seek": 294132, "start": 2954.6000000000004, "end": 2959.1600000000003, "text": " output a number that describes this graph, right, or maybe a vector. Like, for example,", "tokens": [51028, 5598, 257, 1230, 300, 15626, 341, 4295, 11, 558, 11, 420, 1310, 257, 8062, 13, 1743, 11, 337, 1365, 11, 51256], "temperature": 0.0, "avg_logprob": -0.11992705418513372, "compression_ratio": 1.8013698630136987, "no_speech_prob": 0.007359738927334547}, {"id": 501, "seek": 294132, "start": 2959.1600000000003, "end": 2963.48, "text": " I'm predicting the chemical properties of this molecule, like water solubility, so the input is", "tokens": [51256, 286, 478, 32884, 264, 7313, 7221, 295, 341, 15582, 11, 411, 1281, 1404, 836, 1140, 11, 370, 264, 4846, 307, 51472], "temperature": 0.0, "avg_logprob": -0.11992705418513372, "compression_ratio": 1.8013698630136987, "no_speech_prob": 0.007359738927334547}, {"id": 502, "seek": 294132, "start": 2964.28, "end": 2969.1600000000003, "text": " a graph describing a molecule, the output will be some number, right? Another class of problems,", "tokens": [51512, 257, 4295, 16141, 257, 15582, 11, 264, 5598, 486, 312, 512, 1230, 11, 558, 30, 3996, 1508, 295, 2740, 11, 51756], "temperature": 0.0, "avg_logprob": -0.11992705418513372, "compression_ratio": 1.8013698630136987, "no_speech_prob": 0.007359738927334547}, {"id": 503, "seek": 296916, "start": 2969.24, "end": 2974.12, "text": " I give you a graph and I want to do node-level decisions, right? For example,", "tokens": [50368, 286, 976, 291, 257, 4295, 293, 286, 528, 281, 360, 9984, 12, 12418, 5327, 11, 558, 30, 1171, 1365, 11, 50612], "temperature": 0.0, "avg_logprob": -0.09229175506099578, "compression_ratio": 1.6840148698884758, "no_speech_prob": 0.0022354312241077423}, {"id": 504, "seek": 296916, "start": 2974.12, "end": 2979.8799999999997, "text": " I want in a social network to classify which of the users is behaving badly, right? Maybe a", "tokens": [50612, 286, 528, 294, 257, 2093, 3209, 281, 33872, 597, 295, 264, 5022, 307, 35263, 13425, 11, 558, 30, 2704, 257, 50900], "temperature": 0.0, "avg_logprob": -0.09229175506099578, "compression_ratio": 1.6840148698884758, "no_speech_prob": 0.0022354312241077423}, {"id": 505, "seek": 296916, "start": 2979.8799999999997, "end": 2985.3199999999997, "text": " spammer. So, in the first case, I give you a graph, right, and the output, no matter how I", "tokens": [50900, 24028, 936, 13, 407, 11, 294, 264, 700, 1389, 11, 286, 976, 291, 257, 4295, 11, 558, 11, 293, 264, 5598, 11, 572, 1871, 577, 286, 51172], "temperature": 0.0, "avg_logprob": -0.09229175506099578, "compression_ratio": 1.6840148698884758, "no_speech_prob": 0.0022354312241077423}, {"id": 506, "seek": 296916, "start": 2986.2799999999997, "end": 2990.52, "text": " permute the inputs, should be the same, right? So, we're talking about a permutation invariant", "tokens": [51220, 4784, 1169, 264, 15743, 11, 820, 312, 264, 912, 11, 558, 30, 407, 11, 321, 434, 1417, 466, 257, 4784, 11380, 33270, 394, 51432], "temperature": 0.0, "avg_logprob": -0.09229175506099578, "compression_ratio": 1.6840148698884758, "no_speech_prob": 0.0022354312241077423}, {"id": 507, "seek": 296916, "start": 2990.52, "end": 2995.24, "text": " function. So, mathematically, it can be described like this, so, right? So, the function here now", "tokens": [51432, 2445, 13, 407, 11, 44003, 11, 309, 393, 312, 7619, 411, 341, 11, 370, 11, 558, 30, 407, 11, 264, 2445, 510, 586, 51668], "temperature": 0.0, "avg_logprob": -0.09229175506099578, "compression_ratio": 1.6840148698884758, "no_speech_prob": 0.0022354312241077423}, {"id": 508, "seek": 299524, "start": 2995.24, "end": 3000.2799999999997, "text": " is a function of x, the node features, but also the structure of the graph. So, they're both", "tokens": [50364, 307, 257, 2445, 295, 2031, 11, 264, 9984, 4122, 11, 457, 611, 264, 3877, 295, 264, 4295, 13, 407, 11, 436, 434, 1293, 50616], "temperature": 0.0, "avg_logprob": -0.11970474802214524, "compression_ratio": 1.7915057915057915, "no_speech_prob": 0.007838355377316475}, {"id": 509, "seek": 299524, "start": 3000.2799999999997, "end": 3007.24, "text": " from the input, and here I act on these two inputs with the permutation matrix, which is", "tokens": [50616, 490, 264, 4846, 11, 293, 510, 286, 605, 322, 613, 732, 15743, 365, 264, 4784, 11380, 8141, 11, 597, 307, 50964], "temperature": 0.0, "avg_logprob": -0.11970474802214524, "compression_ratio": 1.7915057915057915, "no_speech_prob": 0.007838355377316475}, {"id": 510, "seek": 299524, "start": 3007.24, "end": 3011.8799999999997, "text": " the representation of the permutation group. You see that actually the representation is different", "tokens": [50964, 264, 10290, 295, 264, 4784, 11380, 1594, 13, 509, 536, 300, 767, 264, 10290, 307, 819, 51196], "temperature": 0.0, "avg_logprob": -0.11970474802214524, "compression_ratio": 1.7915057915057915, "no_speech_prob": 0.007838355377316475}, {"id": 511, "seek": 299524, "start": 3011.8799999999997, "end": 3017.3199999999997, "text": " for different types of objects. So, the features, you can think of them as vectors, right? So,", "tokens": [51196, 337, 819, 3467, 295, 6565, 13, 407, 11, 264, 4122, 11, 291, 393, 519, 295, 552, 382, 18875, 11, 558, 30, 407, 11, 51468], "temperature": 0.0, "avg_logprob": -0.11970474802214524, "compression_ratio": 1.7915057915057915, "no_speech_prob": 0.007838355377316475}, {"id": 512, "seek": 299524, "start": 3017.3199999999997, "end": 3023.4799999999996, "text": " I permute only the order of the rows. The adjacency matrix, it's two-dimensional tensor,", "tokens": [51468, 286, 4784, 1169, 787, 264, 1668, 295, 264, 13241, 13, 440, 22940, 3020, 8141, 11, 309, 311, 732, 12, 18759, 40863, 11, 51776], "temperature": 0.0, "avg_logprob": -0.11970474802214524, "compression_ratio": 1.7915057915057915, "no_speech_prob": 0.007838355377316475}, {"id": 513, "seek": 302348, "start": 3023.48, "end": 3029.16, "text": " so, I act both on rows and columns, right? I need to permute both rows and columns. Here,", "tokens": [50364, 370, 11, 286, 605, 1293, 322, 13241, 293, 13766, 11, 558, 30, 286, 643, 281, 4784, 1169, 1293, 13241, 293, 13766, 13, 1692, 11, 50648], "temperature": 0.0, "avg_logprob": -0.14246162535652282, "compression_ratio": 1.9397590361445782, "no_speech_prob": 0.0006715437630191445}, {"id": 514, "seek": 302348, "start": 3029.16, "end": 3035.8, "text": " this is clear. And together, this form permutation invariant function. In the second case, if I want", "tokens": [50648, 341, 307, 1850, 13, 400, 1214, 11, 341, 1254, 4784, 11380, 33270, 394, 2445, 13, 682, 264, 1150, 1389, 11, 498, 286, 528, 50980], "temperature": 0.0, "avg_logprob": -0.14246162535652282, "compression_ratio": 1.9397590361445782, "no_speech_prob": 0.0006715437630191445}, {"id": 515, "seek": 302348, "start": 3035.8, "end": 3041.2400000000002, "text": " node-level predictions, the output has the same structure as the input, right? So, if I change", "tokens": [50980, 9984, 12, 12418, 21264, 11, 264, 5598, 575, 264, 912, 3877, 382, 264, 4846, 11, 558, 30, 407, 11, 498, 286, 1319, 51252], "temperature": 0.0, "avg_logprob": -0.14246162535652282, "compression_ratio": 1.9397590361445782, "no_speech_prob": 0.0006715437630191445}, {"id": 516, "seek": 302348, "start": 3041.2400000000002, "end": 3047.0, "text": " the order of the inputs, the order of the outputs is expected to change in the same way, right?", "tokens": [51252, 264, 1668, 295, 264, 15743, 11, 264, 1668, 295, 264, 23930, 307, 5176, 281, 1319, 294, 264, 912, 636, 11, 558, 30, 51540], "temperature": 0.0, "avg_logprob": -0.14246162535652282, "compression_ratio": 1.9397590361445782, "no_speech_prob": 0.0006715437630191445}, {"id": 517, "seek": 302348, "start": 3047.0, "end": 3051.2400000000002, "text": " And here, I'm interested in permutation equivariant functions. So, equivariance means changing in the", "tokens": [51540, 400, 510, 11, 286, 478, 3102, 294, 4784, 11380, 48726, 3504, 394, 6828, 13, 407, 11, 48726, 3504, 719, 1355, 4473, 294, 264, 51752], "temperature": 0.0, "avg_logprob": -0.14246162535652282, "compression_ratio": 1.9397590361445782, "no_speech_prob": 0.0006715437630191445}, {"id": 518, "seek": 305124, "start": 3051.24, "end": 3056.52, "text": " same way. So, the output now will be, well, some kind of vector, right? Or a matrix, you know,", "tokens": [50364, 912, 636, 13, 407, 11, 264, 5598, 586, 486, 312, 11, 731, 11, 512, 733, 295, 8062, 11, 558, 30, 1610, 257, 8141, 11, 291, 458, 11, 50628], "temperature": 0.0, "avg_logprob": -0.12973780998816856, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.0003619343042373657}, {"id": 519, "seek": 305124, "start": 3056.52, "end": 3062.4399999999996, "text": " by f capital. And if I permute the input, the output will be permuted in the same way, okay?", "tokens": [50628, 538, 283, 4238, 13, 400, 498, 286, 4784, 1169, 264, 4846, 11, 264, 5598, 486, 312, 4784, 4866, 294, 264, 912, 636, 11, 1392, 30, 50924], "temperature": 0.0, "avg_logprob": -0.12973780998816856, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.0003619343042373657}, {"id": 520, "seek": 305124, "start": 3063.56, "end": 3067.8799999999997, "text": " Now, what are graph neural networks? Essentially, these are parametric graph functions. So, I provide", "tokens": [50980, 823, 11, 437, 366, 4295, 18161, 9590, 30, 23596, 11, 613, 366, 6220, 17475, 4295, 6828, 13, 407, 11, 286, 2893, 51196], "temperature": 0.0, "avg_logprob": -0.12973780998816856, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.0003619343042373657}, {"id": 521, "seek": 305124, "start": 3067.8799999999997, "end": 3073.64, "text": " you a graph as an input, right? Or these matrices x and a, and I output something, right? And", "tokens": [51196, 291, 257, 4295, 382, 364, 4846, 11, 558, 30, 1610, 613, 32284, 2031, 293, 257, 11, 293, 286, 5598, 746, 11, 558, 30, 400, 51484], "temperature": 0.0, "avg_logprob": -0.12973780998816856, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.0003619343042373657}, {"id": 522, "seek": 305124, "start": 3073.64, "end": 3078.8399999999997, "text": " something here, as we'll see in a few minutes, is parameterized by some vector of parameters.", "tokens": [51484, 746, 510, 11, 382, 321, 603, 536, 294, 257, 1326, 2077, 11, 307, 13075, 1602, 538, 512, 8062, 295, 9834, 13, 51744], "temperature": 0.0, "avg_logprob": -0.12973780998816856, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.0003619343042373657}, {"id": 523, "seek": 307884, "start": 3079.8, "end": 3084.6800000000003, "text": " And sometimes, there is no distinction made between graph neural networks or message passing", "tokens": [50412, 400, 2171, 11, 456, 307, 572, 16844, 1027, 1296, 4295, 18161, 9590, 420, 3636, 8437, 50656], "temperature": 0.0, "avg_logprob": -0.09755210009488192, "compression_ratio": 1.84765625, "no_speech_prob": 0.0005748371477238834}, {"id": 524, "seek": 307884, "start": 3084.6800000000003, "end": 3090.44, "text": " neural networks. So, we want to make this, that is distinction, but sometimes they're used synonymously,", "tokens": [50656, 18161, 9590, 13, 407, 11, 321, 528, 281, 652, 341, 11, 300, 307, 16844, 11, 457, 2171, 436, 434, 1143, 5451, 12732, 5098, 11, 50944], "temperature": 0.0, "avg_logprob": -0.09755210009488192, "compression_ratio": 1.84765625, "no_speech_prob": 0.0005748371477238834}, {"id": 525, "seek": 307884, "start": 3090.44, "end": 3094.6000000000004, "text": " right? So, most of the graph neural networks that are used in practice are of the message", "tokens": [50944, 558, 30, 407, 11, 881, 295, 264, 4295, 18161, 9590, 300, 366, 1143, 294, 3124, 366, 295, 264, 3636, 51152], "temperature": 0.0, "avg_logprob": -0.09755210009488192, "compression_ratio": 1.84765625, "no_speech_prob": 0.0005748371477238834}, {"id": 526, "seek": 307884, "start": 3094.6000000000004, "end": 3100.1200000000003, "text": " passing type. We'll see exactly what it means in a second. And graph neural networks, again,", "tokens": [51152, 8437, 2010, 13, 492, 603, 536, 2293, 437, 309, 1355, 294, 257, 1150, 13, 400, 4295, 18161, 9590, 11, 797, 11, 51428], "temperature": 0.0, "avg_logprob": -0.09755210009488192, "compression_ratio": 1.84765625, "no_speech_prob": 0.0005748371477238834}, {"id": 527, "seek": 307884, "start": 3100.1200000000003, "end": 3105.4, "text": " you can consider them as a special instance of these geometric deep learning blueprints. So,", "tokens": [51428, 291, 393, 1949, 552, 382, 257, 2121, 5197, 295, 613, 33246, 2452, 2539, 888, 23547, 47523, 13, 407, 11, 51692], "temperature": 0.0, "avg_logprob": -0.09755210009488192, "compression_ratio": 1.84765625, "no_speech_prob": 0.0005748371477238834}, {"id": 528, "seek": 310540, "start": 3105.48, "end": 3111.56, "text": " we have usually a sequence of permutation-equivariant layers that produce node-wise predictions. And", "tokens": [50368, 321, 362, 2673, 257, 8310, 295, 4784, 11380, 12, 12816, 592, 3504, 394, 7914, 300, 5258, 9984, 12, 3711, 21264, 13, 400, 50672], "temperature": 0.0, "avg_logprob": -0.10161728243674001, "compression_ratio": 1.8233082706766917, "no_speech_prob": 0.0006609722622670233}, {"id": 529, "seek": 310540, "start": 3111.56, "end": 3115.4, "text": " permutation-equivariant, in the sense that if I change the order here, it will change in the", "tokens": [50672, 4784, 11380, 12, 12816, 592, 3504, 394, 11, 294, 264, 2020, 300, 498, 286, 1319, 264, 1668, 510, 11, 309, 486, 1319, 294, 264, 50864], "temperature": 0.0, "avg_logprob": -0.10161728243674001, "compression_ratio": 1.8233082706766917, "no_speech_prob": 0.0006609722622670233}, {"id": 530, "seek": 310540, "start": 3115.4, "end": 3121.32, "text": " same way here. And then, if I have graph-level tasks, I will have permutation-invariant pooling,", "tokens": [50864, 912, 636, 510, 13, 400, 550, 11, 498, 286, 362, 4295, 12, 12418, 9608, 11, 286, 486, 362, 4784, 11380, 12, 259, 34033, 394, 7005, 278, 11, 51160], "temperature": 0.0, "avg_logprob": -0.10161728243674001, "compression_ratio": 1.8233082706766917, "no_speech_prob": 0.0006609722622670233}, {"id": 531, "seek": 310540, "start": 3121.32, "end": 3125.48, "text": " right? That aggregates all the information from the, from these node features and produces a", "tokens": [51160, 558, 30, 663, 16743, 1024, 439, 264, 1589, 490, 264, 11, 490, 613, 9984, 4122, 293, 14725, 257, 51368], "temperature": 0.0, "avg_logprob": -0.10161728243674001, "compression_ratio": 1.8233082706766917, "no_speech_prob": 0.0006609722622670233}, {"id": 532, "seek": 310540, "start": 3125.48, "end": 3131.56, "text": " single output for the graph. And the typical way that, that they work is by neighborhood aggregation.", "tokens": [51368, 2167, 5598, 337, 264, 4295, 13, 400, 264, 7476, 636, 300, 11, 300, 436, 589, 307, 538, 7630, 16743, 399, 13, 51672], "temperature": 0.0, "avg_logprob": -0.10161728243674001, "compression_ratio": 1.8233082706766917, "no_speech_prob": 0.0006609722622670233}, {"id": 533, "seek": 313156, "start": 3131.56, "end": 3137.32, "text": " So, you can pick up a node in the graph, right? Let's call it i. And now, I look at the neighborhood", "tokens": [50364, 407, 11, 291, 393, 1888, 493, 257, 9984, 294, 264, 4295, 11, 558, 30, 961, 311, 818, 309, 741, 13, 400, 586, 11, 286, 574, 412, 264, 7630, 50652], "temperature": 0.0, "avg_logprob": -0.07426757960356484, "compression_ratio": 1.937007874015748, "no_speech_prob": 0.0013834988931193948}, {"id": 534, "seek": 313156, "start": 3137.32, "end": 3143.32, "text": " of the node. By neighborhood, I mean all the nodes that are connected to i by an edge, right? So,", "tokens": [50652, 295, 264, 9984, 13, 3146, 7630, 11, 286, 914, 439, 264, 13891, 300, 366, 4582, 281, 741, 538, 364, 4691, 11, 558, 30, 407, 11, 50952], "temperature": 0.0, "avg_logprob": -0.07426757960356484, "compression_ratio": 1.937007874015748, "no_speech_prob": 0.0013834988931193948}, {"id": 535, "seek": 313156, "start": 3143.32, "end": 3149.08, "text": " this is how the neighborhood of i looks like. And I can look at the feature vectors associated with", "tokens": [50952, 341, 307, 577, 264, 7630, 295, 741, 1542, 411, 13, 400, 286, 393, 574, 412, 264, 4111, 18875, 6615, 365, 51240], "temperature": 0.0, "avg_logprob": -0.07426757960356484, "compression_ratio": 1.937007874015748, "no_speech_prob": 0.0013834988931193948}, {"id": 536, "seek": 313156, "start": 3149.08, "end": 3154.2, "text": " these, with these nodes. And you can see that even though the neighbors are unique, right? The", "tokens": [51240, 613, 11, 365, 613, 13891, 13, 400, 291, 393, 536, 300, 754, 1673, 264, 12512, 366, 3845, 11, 558, 30, 440, 51496], "temperature": 0.0, "avg_logprob": -0.07426757960356484, "compression_ratio": 1.937007874015748, "no_speech_prob": 0.0013834988931193948}, {"id": 537, "seek": 313156, "start": 3154.2, "end": 3158.6, "text": " feature vectors are not unique. So, this is encoded by color. So, these two nodes have exactly the", "tokens": [51496, 4111, 18875, 366, 406, 3845, 13, 407, 11, 341, 307, 2058, 12340, 538, 2017, 13, 407, 11, 613, 732, 13891, 362, 2293, 264, 51716], "temperature": 0.0, "avg_logprob": -0.07426757960356484, "compression_ratio": 1.937007874015748, "no_speech_prob": 0.0013834988931193948}, {"id": 538, "seek": 315860, "start": 3158.6, "end": 3163.72, "text": " same feature vector, right? Just for this example. So, together, they form what is called a multi-set,", "tokens": [50364, 912, 4111, 8062, 11, 558, 30, 1449, 337, 341, 1365, 13, 407, 11, 1214, 11, 436, 1254, 437, 307, 1219, 257, 4825, 12, 3854, 11, 50620], "temperature": 0.0, "avg_logprob": -0.08955948352813721, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.002815506188198924}, {"id": 539, "seek": 315860, "start": 3163.72, "end": 3169.56, "text": " right? So, it's a set where the same object can appear more than once. And we also have the", "tokens": [50620, 558, 30, 407, 11, 309, 311, 257, 992, 689, 264, 912, 2657, 393, 4204, 544, 813, 1564, 13, 400, 321, 611, 362, 264, 50912], "temperature": 0.0, "avg_logprob": -0.08955948352813721, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.002815506188198924}, {"id": 540, "seek": 315860, "start": 3169.56, "end": 3174.8399999999997, "text": " feature vector of the node itself. So, I want a function to aggregate them locally, right? Let's", "tokens": [50912, 4111, 8062, 295, 264, 9984, 2564, 13, 407, 11, 286, 528, 257, 2445, 281, 26118, 552, 16143, 11, 558, 30, 961, 311, 51176], "temperature": 0.0, "avg_logprob": -0.08955948352813721, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.002815506188198924}, {"id": 541, "seek": 315860, "start": 3174.8399999999997, "end": 3179.64, "text": " call this function phi. And again, the, the characteristic property of this function is that", "tokens": [51176, 818, 341, 2445, 13107, 13, 400, 797, 11, 264, 11, 264, 16282, 4707, 295, 341, 2445, 307, 300, 51416], "temperature": 0.0, "avg_logprob": -0.08955948352813721, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.002815506188198924}, {"id": 542, "seek": 315860, "start": 3180.52, "end": 3185.24, "text": " I don't have a canonical ordering of my neighborhood. So, the feature vectors can appear like this", "tokens": [51460, 286, 500, 380, 362, 257, 46491, 21739, 295, 452, 7630, 13, 407, 11, 264, 4111, 18875, 393, 4204, 411, 341, 51696], "temperature": 0.0, "avg_logprob": -0.08955948352813721, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.002815506188198924}, {"id": 543, "seek": 318524, "start": 3186.12, "end": 3193.08, "text": " to, to this phi. So, it must be by design permutation invariant, right? It cannot assume any order", "tokens": [50408, 281, 11, 281, 341, 13107, 13, 407, 11, 309, 1633, 312, 538, 1715, 4784, 11380, 33270, 394, 11, 558, 30, 467, 2644, 6552, 604, 1668, 50756], "temperature": 0.0, "avg_logprob": -0.1038121223449707, "compression_ratio": 1.654109589041096, "no_speech_prob": 0.003138524480164051}, {"id": 544, "seek": 318524, "start": 3193.08, "end": 3197.9599999999996, "text": " in which the neighbors come. We can make it more complicated and we can incorporate additional", "tokens": [50756, 294, 597, 264, 12512, 808, 13, 492, 393, 652, 309, 544, 6179, 293, 321, 393, 16091, 4497, 51000], "temperature": 0.0, "avg_logprob": -0.1038121223449707, "compression_ratio": 1.654109589041096, "no_speech_prob": 0.003138524480164051}, {"id": 545, "seek": 318524, "start": 3197.9599999999996, "end": 3202.68, "text": " information. But again, as a basic structure of a graph, you don't have this order, right?", "tokens": [51000, 1589, 13, 583, 797, 11, 382, 257, 3875, 3877, 295, 257, 4295, 11, 291, 500, 380, 362, 341, 1668, 11, 558, 30, 51236], "temperature": 0.0, "avg_logprob": -0.1038121223449707, "compression_ratio": 1.654109589041096, "no_speech_prob": 0.003138524480164051}, {"id": 546, "seek": 318524, "start": 3204.2799999999997, "end": 3208.8399999999997, "text": " Now, you can repeat this process everywhere at every point, at every node of the graph. And this is", "tokens": [51316, 823, 11, 291, 393, 7149, 341, 1399, 5315, 412, 633, 935, 11, 412, 633, 9984, 295, 264, 4295, 13, 400, 341, 307, 51544], "temperature": 0.0, "avg_logprob": -0.1038121223449707, "compression_ratio": 1.654109589041096, "no_speech_prob": 0.003138524480164051}, {"id": 547, "seek": 318524, "start": 3208.8399999999997, "end": 3213.8799999999997, "text": " actually very highly parallelizable, at least in principle. And once you do it, you get an output,", "tokens": [51544, 767, 588, 5405, 8952, 22395, 11, 412, 1935, 294, 8665, 13, 400, 1564, 291, 360, 309, 11, 291, 483, 364, 5598, 11, 51796], "temperature": 0.0, "avg_logprob": -0.1038121223449707, "compression_ratio": 1.654109589041096, "no_speech_prob": 0.003138524480164051}, {"id": 548, "seek": 321388, "start": 3213.96, "end": 3219.7200000000003, "text": " right? That for every node of the graph, you output some vector. And this is also a function", "tokens": [50368, 558, 30, 663, 337, 633, 9984, 295, 264, 4295, 11, 291, 5598, 512, 8062, 13, 400, 341, 307, 611, 257, 2445, 50656], "temperature": 0.0, "avg_logprob": -0.08689455328316524, "compression_ratio": 1.8473895582329318, "no_speech_prob": 0.0004843801725655794}, {"id": 549, "seek": 321388, "start": 3219.7200000000003, "end": 3224.84, "text": " of the graph. And you can easily see that if my choice of this local aggregation is invariant,", "tokens": [50656, 295, 264, 4295, 13, 400, 291, 393, 3612, 536, 300, 498, 452, 3922, 295, 341, 2654, 16743, 399, 307, 33270, 394, 11, 50912], "temperature": 0.0, "avg_logprob": -0.08689455328316524, "compression_ratio": 1.8473895582329318, "no_speech_prob": 0.0004843801725655794}, {"id": 550, "seek": 321388, "start": 3225.56, "end": 3230.2000000000003, "text": " then the output is equivariant, right? So, if I change the order of the axis here,", "tokens": [50948, 550, 264, 5598, 307, 48726, 3504, 394, 11, 558, 30, 407, 11, 498, 286, 1319, 264, 1668, 295, 264, 10298, 510, 11, 51180], "temperature": 0.0, "avg_logprob": -0.08689455328316524, "compression_ratio": 1.8473895582329318, "no_speech_prob": 0.0004843801725655794}, {"id": 551, "seek": 321388, "start": 3230.84, "end": 3235.96, "text": " then the output will change in the same way. And most of the graph neural network architectures", "tokens": [51212, 550, 264, 5598, 486, 1319, 294, 264, 912, 636, 13, 400, 881, 295, 264, 4295, 18161, 3209, 6331, 1303, 51468], "temperature": 0.0, "avg_logprob": -0.08689455328316524, "compression_ratio": 1.8473895582329318, "no_speech_prob": 0.0004843801725655794}, {"id": 552, "seek": 321388, "start": 3235.96, "end": 3243.1600000000003, "text": " differ in the choice of these phi, in how I aggregate locally the features. And while they're", "tokens": [51468, 743, 294, 264, 3922, 295, 613, 13107, 11, 294, 577, 286, 26118, 16143, 264, 4122, 13, 400, 1339, 436, 434, 51828], "temperature": 0.0, "avg_logprob": -0.08689455328316524, "compression_ratio": 1.8473895582329318, "no_speech_prob": 0.0004843801725655794}, {"id": 553, "seek": 324316, "start": 3243.16, "end": 3247.64, "text": " probably zillions of different architectures, most of them fall within the following three", "tokens": [50364, 1391, 710, 46279, 295, 819, 6331, 1303, 11, 881, 295, 552, 2100, 1951, 264, 3480, 1045, 50588], "temperature": 0.0, "avg_logprob": -0.07175472412986317, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.0017830629367381334}, {"id": 554, "seek": 324316, "start": 3247.64, "end": 3252.04, "text": " categories. So, the first one is what is called convolutional graph neural networks. And you", "tokens": [50588, 10479, 13, 407, 11, 264, 700, 472, 307, 437, 307, 1219, 45216, 304, 4295, 18161, 9590, 13, 400, 291, 50808], "temperature": 0.0, "avg_logprob": -0.07175472412986317, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.0017830629367381334}, {"id": 555, "seek": 324316, "start": 3252.04, "end": 3259.72, "text": " can simply think of convolutional neural networks as just summing up the features of the neighbor", "tokens": [50808, 393, 2935, 519, 295, 45216, 304, 18161, 9590, 382, 445, 2408, 2810, 493, 264, 4122, 295, 264, 5987, 51192], "temperature": 0.0, "avg_logprob": -0.07175472412986317, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.0017830629367381334}, {"id": 556, "seek": 324316, "start": 3259.72, "end": 3267.7999999999997, "text": " nodes. So, this is what I write here. So, the update for the representation for the feature at node i", "tokens": [51192, 13891, 13, 407, 11, 341, 307, 437, 286, 2464, 510, 13, 407, 11, 264, 5623, 337, 264, 10290, 337, 264, 4111, 412, 9984, 741, 51596], "temperature": 0.0, "avg_logprob": -0.07175472412986317, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.0017830629367381334}, {"id": 557, "seek": 326780, "start": 3268.76, "end": 3277.4, "text": " is the sum of some transformed neighbor nodes, right? Psi will be some learnable function. Xj", "tokens": [50412, 307, 264, 2408, 295, 512, 16894, 5987, 13891, 11, 558, 30, 430, 7691, 486, 312, 512, 1466, 712, 2445, 13, 1783, 73, 50844], "temperature": 0.0, "avg_logprob": -0.16679321989721183, "compression_ratio": 1.6293103448275863, "no_speech_prob": 0.002612330950796604}, {"id": 558, "seek": 326780, "start": 3277.4, "end": 3284.36, "text": " is the the the feature of the neighbor node. Here it is. And ai, j are coefficients that depend", "tokens": [50844, 307, 264, 264, 264, 4111, 295, 264, 5987, 9984, 13, 1692, 309, 307, 13, 400, 9783, 11, 361, 366, 31994, 300, 5672, 51192], "temperature": 0.0, "avg_logprob": -0.16679321989721183, "compression_ratio": 1.6293103448275863, "no_speech_prob": 0.002612330950796604}, {"id": 559, "seek": 326780, "start": 3284.36, "end": 3289.0, "text": " only on the structure of the graph. In the simple case, just the elements of the adjacency matrix,", "tokens": [51192, 787, 322, 264, 3877, 295, 264, 4295, 13, 682, 264, 2199, 1389, 11, 445, 264, 4959, 295, 264, 22940, 3020, 8141, 11, 51424], "temperature": 0.0, "avg_logprob": -0.16679321989721183, "compression_ratio": 1.6293103448275863, "no_speech_prob": 0.002612330950796604}, {"id": 560, "seek": 326780, "start": 3289.0, "end": 3293.7200000000003, "text": " right? And here, sigma is just some non-linear activation, typically very low, right? So,", "tokens": [51424, 558, 30, 400, 510, 11, 12771, 307, 445, 512, 2107, 12, 28263, 24433, 11, 5850, 588, 2295, 11, 558, 30, 407, 11, 51660], "temperature": 0.0, "avg_logprob": -0.16679321989721183, "compression_ratio": 1.6293103448275863, "no_speech_prob": 0.002612330950796604}, {"id": 561, "seek": 329372, "start": 3293.7999999999997, "end": 3297.7999999999997, "text": " that's how a convolutional type graph neural network looks like. Yeah.", "tokens": [50368, 300, 311, 577, 257, 45216, 304, 2010, 4295, 18161, 3209, 1542, 411, 13, 865, 13, 50568], "temperature": 0.0, "avg_logprob": -0.20081932864972016, "compression_ratio": 1.5733333333333333, "no_speech_prob": 0.0027333691250532866}, {"id": 562, "seek": 329372, "start": 3306.04, "end": 3312.2799999999997, "text": " Is this convolution here equivalent to 1D convolution, something like that?", "tokens": [50980, 1119, 341, 45216, 510, 10344, 281, 502, 35, 45216, 11, 746, 411, 300, 30, 51292], "temperature": 0.0, "avg_logprob": -0.20081932864972016, "compression_ratio": 1.5733333333333333, "no_speech_prob": 0.0027333691250532866}, {"id": 563, "seek": 329372, "start": 3312.2799999999997, "end": 3317.56, "text": " So, we'll see it, we'll see it in the, well, not in a second, but we'll see it later. So,", "tokens": [51292, 407, 11, 321, 603, 536, 309, 11, 321, 603, 536, 309, 294, 264, 11, 731, 11, 406, 294, 257, 1150, 11, 457, 321, 603, 536, 309, 1780, 13, 407, 11, 51556], "temperature": 0.0, "avg_logprob": -0.20081932864972016, "compression_ratio": 1.5733333333333333, "no_speech_prob": 0.0027333691250532866}, {"id": 564, "seek": 331756, "start": 3317.7999999999997, "end": 3325.88, "text": " basically, you can obtain convolution when the graph is agreed. And we can actually see that", "tokens": [50376, 1936, 11, 291, 393, 12701, 45216, 562, 264, 4295, 307, 9166, 13, 400, 321, 393, 767, 536, 300, 50780], "temperature": 0.0, "avg_logprob": -0.13050487637519836, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.0007467729155905545}, {"id": 565, "seek": 331756, "start": 3325.88, "end": 3331.7999999999997, "text": " that convolution on the grid is a special type. Basically, it's a unique linear equivalent function.", "tokens": [50780, 300, 45216, 322, 264, 10748, 307, 257, 2121, 2010, 13, 8537, 11, 309, 311, 257, 3845, 8213, 10344, 2445, 13, 51076], "temperature": 0.0, "avg_logprob": -0.13050487637519836, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.0007467729155905545}, {"id": 566, "seek": 331756, "start": 3332.7599999999998, "end": 3338.44, "text": " So, basically, yeah. So, that's why the term convolution is appropriate. So, it's an extension", "tokens": [51124, 407, 11, 1936, 11, 1338, 13, 407, 11, 300, 311, 983, 264, 1433, 45216, 307, 6854, 13, 407, 11, 309, 311, 364, 10320, 51408], "temperature": 0.0, "avg_logprob": -0.13050487637519836, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.0007467729155905545}, {"id": 567, "seek": 331756, "start": 3338.44, "end": 3346.2, "text": " of conversion to graphs, right? And you can write it in this form, right? So, if I write it as", "tokens": [51408, 295, 14298, 281, 24877, 11, 558, 30, 400, 291, 393, 2464, 309, 294, 341, 1254, 11, 558, 30, 407, 11, 498, 286, 2464, 309, 382, 51796], "temperature": 0.0, "avg_logprob": -0.13050487637519836, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.0007467729155905545}, {"id": 568, "seek": 334620, "start": 3346.2, "end": 3352.12, "text": " matrix multiplication, so x is my feature matrix of size n by d, I multiply it from the left by", "tokens": [50364, 8141, 27290, 11, 370, 2031, 307, 452, 4111, 8141, 295, 2744, 297, 538, 274, 11, 286, 12972, 309, 490, 264, 1411, 538, 50660], "temperature": 0.0, "avg_logprob": -0.11190888113226773, "compression_ratio": 1.8160919540229885, "no_speech_prob": 0.003613770008087158}, {"id": 569, "seek": 334620, "start": 3353.0, "end": 3356.6, "text": " some matrix a, right? The diffusion matrix. It can be the adjacency of the graph. It can be", "tokens": [50704, 512, 8141, 257, 11, 558, 30, 440, 25242, 8141, 13, 467, 393, 312, 264, 22940, 3020, 295, 264, 4295, 13, 467, 393, 312, 50884], "temperature": 0.0, "avg_logprob": -0.11190888113226773, "compression_ratio": 1.8160919540229885, "no_speech_prob": 0.003613770008087158}, {"id": 570, "seek": 334620, "start": 3356.6, "end": 3361.16, "text": " something else. But, basically, it propagates information between adjacent nodes. And on the", "tokens": [50884, 746, 1646, 13, 583, 11, 1936, 11, 309, 12425, 1024, 1589, 1296, 24441, 13891, 13, 400, 322, 264, 51112], "temperature": 0.0, "avg_logprob": -0.11190888113226773, "compression_ratio": 1.8160919540229885, "no_speech_prob": 0.003613770008087158}, {"id": 571, "seek": 334620, "start": 3361.16, "end": 3368.12, "text": " right, I have, in the case of a linear transformation of the nodes, it's a learnable shared matrix", "tokens": [51112, 558, 11, 286, 362, 11, 294, 264, 1389, 295, 257, 8213, 9887, 295, 264, 13891, 11, 309, 311, 257, 1466, 712, 5507, 8141, 51460], "temperature": 0.0, "avg_logprob": -0.11190888113226773, "compression_ratio": 1.8160919540229885, "no_speech_prob": 0.003613770008087158}, {"id": 572, "seek": 334620, "start": 3368.12, "end": 3372.7599999999998, "text": " that acts on every node in the same way, right? On the features of every node in the same way.", "tokens": [51460, 300, 10672, 322, 633, 9984, 294, 264, 912, 636, 11, 558, 30, 1282, 264, 4122, 295, 633, 9984, 294, 264, 912, 636, 13, 51692], "temperature": 0.0, "avg_logprob": -0.11190888113226773, "compression_ratio": 1.8160919540229885, "no_speech_prob": 0.003613770008087158}, {"id": 573, "seek": 337276, "start": 3373.48, "end": 3378.76, "text": " We'll see that it's related to diffusion equations on graphs. And this is probably the simplest", "tokens": [50400, 492, 603, 536, 300, 309, 311, 4077, 281, 25242, 11787, 322, 24877, 13, 400, 341, 307, 1391, 264, 22811, 50664], "temperature": 0.0, "avg_logprob": -0.13734780039106095, "compression_ratio": 1.651877133105802, "no_speech_prob": 0.0004745837359223515}, {"id": 574, "seek": 337276, "start": 3378.76, "end": 3383.48, "text": " version of a GNN. It's highly scalable. You can basically just large matrix multiplication.", "tokens": [50664, 3037, 295, 257, 46411, 45, 13, 467, 311, 5405, 38481, 13, 509, 393, 1936, 445, 2416, 8141, 27290, 13, 50900], "temperature": 0.0, "avg_logprob": -0.13734780039106095, "compression_ratio": 1.651877133105802, "no_speech_prob": 0.0004745837359223515}, {"id": 575, "seek": 337276, "start": 3384.0400000000004, "end": 3389.0800000000004, "text": " It has been used in industrial use cases. I think the first to use these kind of architectures was", "tokens": [50928, 467, 575, 668, 1143, 294, 9987, 764, 3331, 13, 286, 519, 264, 700, 281, 764, 613, 733, 295, 6331, 1303, 390, 51180], "temperature": 0.0, "avg_logprob": -0.13734780039106095, "compression_ratio": 1.651877133105802, "no_speech_prob": 0.0004745837359223515}, {"id": 576, "seek": 337276, "start": 3389.0800000000004, "end": 3395.0800000000004, "text": " Pinterest with Pinsage. We also used it at Twitter. And then there are some statements about these", "tokens": [51180, 37986, 365, 430, 1292, 609, 13, 492, 611, 1143, 309, 412, 5794, 13, 400, 550, 456, 366, 512, 12363, 466, 613, 51480], "temperature": 0.0, "avg_logprob": -0.13734780039106095, "compression_ratio": 1.651877133105802, "no_speech_prob": 0.0004745837359223515}, {"id": 577, "seek": 337276, "start": 3395.0800000000004, "end": 3400.44, "text": " architectures in the kind of graph neural network folklore saying that it works only on homophilic", "tokens": [51480, 6331, 1303, 294, 264, 733, 295, 4295, 18161, 3209, 49195, 1566, 300, 309, 1985, 787, 322, 3655, 5317, 388, 299, 51748], "temperature": 0.0, "avg_logprob": -0.13734780039106095, "compression_ratio": 1.651877133105802, "no_speech_prob": 0.0004745837359223515}, {"id": 578, "seek": 340044, "start": 3400.44, "end": 3404.68, "text": " graphs. So, by homophily, I mean this assumption that my neighbors are similar to me. Typically,", "tokens": [50364, 24877, 13, 407, 11, 538, 3655, 5317, 953, 11, 286, 914, 341, 15302, 300, 452, 12512, 366, 2531, 281, 385, 13, 23129, 11, 50576], "temperature": 0.0, "avg_logprob": -0.12116022767691777, "compression_ratio": 1.8358778625954197, "no_speech_prob": 0.0009467490017414093}, {"id": 579, "seek": 340044, "start": 3404.68, "end": 3410.6, "text": " the assumption is that the labels in the neighborhood are somehow similarly distributed to the label", "tokens": [50576, 264, 15302, 307, 300, 264, 16949, 294, 264, 7630, 366, 6063, 14138, 12631, 281, 264, 7645, 50872], "temperature": 0.0, "avg_logprob": -0.12116022767691777, "compression_ratio": 1.8358778625954197, "no_speech_prob": 0.0009467490017414093}, {"id": 580, "seek": 340044, "start": 3410.6, "end": 3414.76, "text": " of the node itself. And here's an example of a homophilic graph versus a heterophilic graph.", "tokens": [50872, 295, 264, 9984, 2564, 13, 400, 510, 311, 364, 1365, 295, 257, 3655, 5317, 388, 299, 4295, 5717, 257, 20789, 5317, 388, 299, 4295, 13, 51080], "temperature": 0.0, "avg_logprob": -0.12116022767691777, "compression_ratio": 1.8358778625954197, "no_speech_prob": 0.0009467490017414093}, {"id": 581, "seek": 340044, "start": 3415.4, "end": 3420.92, "text": " So, and the usual motivation that is given that these matrix a typically will look like a low", "tokens": [51112, 407, 11, 293, 264, 7713, 12335, 300, 307, 2212, 300, 613, 8141, 257, 5850, 486, 574, 411, 257, 2295, 51388], "temperature": 0.0, "avg_logprob": -0.12116022767691777, "compression_ratio": 1.8358778625954197, "no_speech_prob": 0.0009467490017414093}, {"id": 582, "seek": 340044, "start": 3420.92, "end": 3425.88, "text": " pass filter, right? So, you're somehow averaging your neighbors. And if the neighbors are of the", "tokens": [51388, 1320, 6608, 11, 558, 30, 407, 11, 291, 434, 6063, 47308, 428, 12512, 13, 400, 498, 264, 12512, 366, 295, 264, 51636], "temperature": 0.0, "avg_logprob": -0.12116022767691777, "compression_ratio": 1.8358778625954197, "no_speech_prob": 0.0009467490017414093}, {"id": 583, "seek": 342588, "start": 3425.96, "end": 3429.56, "text": " same type, then it will work. And if the neighbors are very different types, then", "tokens": [50368, 912, 2010, 11, 550, 309, 486, 589, 13, 400, 498, 264, 12512, 366, 588, 819, 3467, 11, 550, 50548], "temperature": 0.0, "avg_logprob": -0.09952434690872042, "compression_ratio": 1.65, "no_speech_prob": 0.002061461564153433}, {"id": 584, "seek": 342588, "start": 3430.52, "end": 3436.36, "text": " it makes more harm than help. And this is actually not true. So, I hope to convince you that", "tokens": [50596, 309, 1669, 544, 6491, 813, 854, 13, 400, 341, 307, 767, 406, 2074, 13, 407, 11, 286, 1454, 281, 13447, 291, 300, 50888], "temperature": 0.0, "avg_logprob": -0.09952434690872042, "compression_ratio": 1.65, "no_speech_prob": 0.002061461564153433}, {"id": 585, "seek": 342588, "start": 3437.0, "end": 3442.2000000000003, "text": " the story is much more nuanced. There is also this channel mixing matrix. And there is an", "tokens": [50920, 264, 1657, 307, 709, 544, 45115, 13, 821, 307, 611, 341, 2269, 11983, 8141, 13, 400, 456, 307, 364, 51180], "temperature": 0.0, "avg_logprob": -0.09952434690872042, "compression_ratio": 1.65, "no_speech_prob": 0.002061461564153433}, {"id": 586, "seek": 342588, "start": 3442.2000000000003, "end": 3447.32, "text": " interesting and subtle interplay between these two matrices. And we'll be able to see how it", "tokens": [51180, 1880, 293, 13743, 728, 2858, 1296, 613, 732, 32284, 13, 400, 321, 603, 312, 1075, 281, 536, 577, 309, 51436], "temperature": 0.0, "avg_logprob": -0.09952434690872042, "compression_ratio": 1.65, "no_speech_prob": 0.002061461564153433}, {"id": 587, "seek": 342588, "start": 3447.32, "end": 3450.92, "text": " works when we consider graph neural networks as differential equations.", "tokens": [51436, 1985, 562, 321, 1949, 4295, 18161, 9590, 382, 15756, 11787, 13, 51616], "temperature": 0.0, "avg_logprob": -0.09952434690872042, "compression_ratio": 1.65, "no_speech_prob": 0.002061461564153433}, {"id": 588, "seek": 345092, "start": 3451.56, "end": 3456.92, "text": " So, slightly more interesting architecture is an attentional GNN. So, here, again, we can think of", "tokens": [50396, 407, 11, 4748, 544, 1880, 9482, 307, 364, 3202, 304, 46411, 45, 13, 407, 11, 510, 11, 797, 11, 321, 393, 519, 295, 50664], "temperature": 0.0, "avg_logprob": -0.21659025779137245, "compression_ratio": 1.7238805970149254, "no_speech_prob": 0.0007843453786335886}, {"id": 589, "seek": 345092, "start": 3456.92, "end": 3463.7200000000003, "text": " it, at least in some settings, as a linear combination of the features of the neighbors,", "tokens": [50664, 309, 11, 412, 1935, 294, 512, 6257, 11, 382, 257, 8213, 6562, 295, 264, 4122, 295, 264, 12512, 11, 51004], "temperature": 0.0, "avg_logprob": -0.21659025779137245, "compression_ratio": 1.7238805970149254, "no_speech_prob": 0.0007843453786335886}, {"id": 590, "seek": 345092, "start": 3463.7200000000003, "end": 3468.2000000000003, "text": " maybe transformed by some learnable function. But now the coefficients depend not only on the", "tokens": [51004, 1310, 16894, 538, 512, 1466, 712, 2445, 13, 583, 586, 264, 31994, 5672, 406, 787, 322, 264, 51228], "temperature": 0.0, "avg_logprob": -0.21659025779137245, "compression_ratio": 1.7238805970149254, "no_speech_prob": 0.0007843453786335886}, {"id": 591, "seek": 345092, "start": 3468.2000000000003, "end": 3471.88, "text": " structure of the graph, but also on the features themselves, typically through an attention", "tokens": [51228, 3877, 295, 264, 4295, 11, 457, 611, 322, 264, 4122, 2969, 11, 5850, 807, 364, 3202, 51412], "temperature": 0.0, "avg_logprob": -0.21659025779137245, "compression_ratio": 1.7238805970149254, "no_speech_prob": 0.0007843453786335886}, {"id": 592, "seek": 345092, "start": 3471.88, "end": 3477.16, "text": " mechanism. So, the most famous representative of these architectures, they got the graph", "tokens": [51412, 7513, 13, 407, 11, 264, 881, 4618, 12424, 295, 613, 6331, 1303, 11, 436, 658, 264, 4295, 51676], "temperature": 0.0, "avg_logprob": -0.21659025779137245, "compression_ratio": 1.7238805970149254, "no_speech_prob": 0.0007843453786335886}, {"id": 593, "seek": 347716, "start": 3478.12, "end": 3483.7999999999997, "text": " attention network. And in the most general case, right? So, we can write got like this. So, that's", "tokens": [50412, 3202, 3209, 13, 400, 294, 264, 881, 2674, 1389, 11, 558, 30, 407, 11, 321, 393, 2464, 658, 411, 341, 13, 407, 11, 300, 311, 50696], "temperature": 0.0, "avg_logprob": -0.2264210864751026, "compression_ratio": 1.768181818181818, "no_speech_prob": 0.0008570703212171793}, {"id": 594, "seek": 347716, "start": 3484.68, "end": 3489.56, "text": " if we have a linear combination, some, so we have linear combination with the matrix. But now", "tokens": [50740, 498, 321, 362, 257, 8213, 6562, 11, 512, 11, 370, 321, 362, 8213, 6562, 365, 264, 8141, 13, 583, 586, 50984], "temperature": 0.0, "avg_logprob": -0.2264210864751026, "compression_ratio": 1.768181818181818, "no_speech_prob": 0.0008570703212171793}, {"id": 595, "seek": 347716, "start": 3489.56, "end": 3495.8799999999997, "text": " the matrix is actually a matrix valued function of x, right? So, it itself depends nonlinearly on x.", "tokens": [50984, 264, 8141, 307, 767, 257, 8141, 22608, 2445, 295, 2031, 11, 558, 30, 407, 11, 309, 2564, 5946, 2107, 28263, 356, 322, 2031, 13, 51300], "temperature": 0.0, "avg_logprob": -0.2264210864751026, "compression_ratio": 1.768181818181818, "no_speech_prob": 0.0008570703212171793}, {"id": 596, "seek": 347716, "start": 3497.48, "end": 3502.04, "text": " And the most general case is a message passing architectures where we have a b-variate function", "tokens": [51380, 400, 264, 881, 2674, 1389, 307, 257, 3636, 8437, 6331, 1303, 689, 321, 362, 257, 272, 12, 34033, 473, 2445, 51608], "temperature": 0.0, "avg_logprob": -0.2264210864751026, "compression_ratio": 1.768181818181818, "no_speech_prob": 0.0008570703212171793}, {"id": 597, "seek": 350204, "start": 3502.04, "end": 3510.52, "text": " that depends on the feature of the node and the neighbor. And the other cases are specific", "tokens": [50364, 300, 5946, 322, 264, 4111, 295, 264, 9984, 293, 264, 5987, 13, 400, 264, 661, 3331, 366, 2685, 50788], "temperature": 0.0, "avg_logprob": -0.11688505885112717, "compression_ratio": 1.6133333333333333, "no_speech_prob": 0.0030207319650799036}, {"id": 598, "seek": 350204, "start": 3510.52, "end": 3519.72, "text": " cases of this architecture. And it was shown that message passing GNNs with specially chosen", "tokens": [50788, 3331, 295, 341, 9482, 13, 400, 309, 390, 4898, 300, 3636, 8437, 46411, 45, 82, 365, 22549, 8614, 51248], "temperature": 0.0, "avg_logprob": -0.11688505885112717, "compression_ratio": 1.6133333333333333, "no_speech_prob": 0.0030207319650799036}, {"id": 599, "seek": 350204, "start": 3519.72, "end": 3524.12, "text": " aggregation function, in particular, it has to be injective in certain restricted settings,", "tokens": [51248, 16743, 399, 2445, 11, 294, 1729, 11, 309, 575, 281, 312, 10711, 488, 294, 1629, 20608, 6257, 11, 51468], "temperature": 0.0, "avg_logprob": -0.11688505885112717, "compression_ratio": 1.6133333333333333, "no_speech_prob": 0.0030207319650799036}, {"id": 600, "seek": 350204, "start": 3524.12, "end": 3530.04, "text": " again, allow me not to go into the details, are equivalent to graph isomorphism testing", "tokens": [51468, 797, 11, 2089, 385, 406, 281, 352, 666, 264, 4365, 11, 366, 10344, 281, 4295, 307, 32702, 1434, 4997, 51764], "temperature": 0.0, "avg_logprob": -0.11688505885112717, "compression_ratio": 1.6133333333333333, "no_speech_prob": 0.0030207319650799036}, {"id": 601, "seek": 353004, "start": 3530.04, "end": 3534.2799999999997, "text": " algorithm that was derived by vice versa and lemon that I mentioned in the beginning.", "tokens": [50364, 9284, 300, 390, 18949, 538, 11964, 25650, 293, 11356, 300, 286, 2835, 294, 264, 2863, 13, 50576], "temperature": 0.0, "avg_logprob": -0.1308124002102202, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.0034295443911105394}, {"id": 602, "seek": 353004, "start": 3535.08, "end": 3541.96, "text": " And let's talk about this. So, basically, from theoretical standpoint, what actually", "tokens": [50616, 400, 718, 311, 751, 466, 341, 13, 407, 11, 1936, 11, 490, 20864, 15827, 11, 437, 767, 50960], "temperature": 0.0, "avg_logprob": -0.1308124002102202, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.0034295443911105394}, {"id": 603, "seek": 353004, "start": 3541.96, "end": 3545.88, "text": " graph neural networks do, right? Or message passing type graph neural networks do.", "tokens": [50960, 4295, 18161, 9590, 360, 11, 558, 30, 1610, 3636, 8437, 2010, 4295, 18161, 9590, 360, 13, 51156], "temperature": 0.0, "avg_logprob": -0.1308124002102202, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.0034295443911105394}, {"id": 604, "seek": 353004, "start": 3545.88, "end": 3551.8, "text": " So, first of all, what is graph isomorphism problem? It's telling when two graphs are the same,", "tokens": [51156, 407, 11, 700, 295, 439, 11, 437, 307, 4295, 307, 32702, 1434, 1154, 30, 467, 311, 3585, 562, 732, 24877, 366, 264, 912, 11, 51452], "temperature": 0.0, "avg_logprob": -0.1308124002102202, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.0034295443911105394}, {"id": 605, "seek": 353004, "start": 3551.8, "end": 3557.8, "text": " right? So, I'm writing here equal, but of course, it's not equal. So, it's equivalent in some sense.", "tokens": [51452, 558, 30, 407, 11, 286, 478, 3579, 510, 2681, 11, 457, 295, 1164, 11, 309, 311, 406, 2681, 13, 407, 11, 309, 311, 10344, 294, 512, 2020, 13, 51752], "temperature": 0.0, "avg_logprob": -0.1308124002102202, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.0034295443911105394}, {"id": 606, "seek": 355780, "start": 3557.8, "end": 3563.48, "text": " And what do I mean by this equivalence? What I want to say is that there exists an edge preserving", "tokens": [50364, 400, 437, 360, 286, 914, 538, 341, 9052, 655, 30, 708, 286, 528, 281, 584, 307, 300, 456, 8198, 364, 4691, 33173, 50648], "temperature": 0.0, "avg_logprob": -0.09564350590561375, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.003586634760722518}, {"id": 607, "seek": 355780, "start": 3563.48, "end": 3568.04, "text": " bijection between the nodes of these graphs, right? So, in other words, I can find a correspondence", "tokens": [50648, 3228, 1020, 313, 1296, 264, 13891, 295, 613, 24877, 11, 558, 30, 407, 11, 294, 661, 2283, 11, 286, 393, 915, 257, 38135, 50876], "temperature": 0.0, "avg_logprob": -0.09564350590561375, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.003586634760722518}, {"id": 608, "seek": 355780, "start": 3568.04, "end": 3574.1200000000003, "text": " between nodes in G and G prime, such that if there is an edge between a pair of nodes in G,", "tokens": [50876, 1296, 13891, 294, 460, 293, 460, 5835, 11, 1270, 300, 498, 456, 307, 364, 4691, 1296, 257, 6119, 295, 13891, 294, 460, 11, 51180], "temperature": 0.0, "avg_logprob": -0.09564350590561375, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.003586634760722518}, {"id": 609, "seek": 355780, "start": 3574.1200000000003, "end": 3580.28, "text": " then there is a corresponding edge in G prime, right? So, is this bijection unique?", "tokens": [51180, 550, 456, 307, 257, 11760, 4691, 294, 460, 5835, 11, 558, 30, 407, 11, 307, 341, 3228, 1020, 313, 3845, 30, 51488], "temperature": 0.0, "avg_logprob": -0.09564350590561375, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.003586634760722518}, {"id": 610, "seek": 358028, "start": 3581.2400000000002, "end": 3588.52, "text": " What do you think? No? When is it not unique?", "tokens": [50412, 708, 360, 291, 519, 30, 883, 30, 1133, 307, 309, 406, 3845, 30, 50776], "temperature": 0.0, "avg_logprob": -0.11195322844359251, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0038024468813091516}, {"id": 611, "seek": 358028, "start": 3591.8, "end": 3595.1600000000003, "text": " Exactly. When the graph has symmetries, right? So, this is an example. Actually,", "tokens": [50940, 7587, 13, 1133, 264, 4295, 575, 14232, 302, 2244, 11, 558, 30, 407, 11, 341, 307, 364, 1365, 13, 5135, 11, 51108], "temperature": 0.0, "avg_logprob": -0.11195322844359251, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0038024468813091516}, {"id": 612, "seek": 358028, "start": 3595.1600000000003, "end": 3599.0, "text": " this graph is a good example, right? So, you can reflect the nodes on the left and the right,", "tokens": [51108, 341, 4295, 307, 257, 665, 1365, 11, 558, 30, 407, 11, 291, 393, 5031, 264, 13891, 322, 264, 1411, 293, 264, 558, 11, 51300], "temperature": 0.0, "avg_logprob": -0.11195322844359251, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0038024468813091516}, {"id": 613, "seek": 358028, "start": 3599.0, "end": 3603.8, "text": " and then we can have another bijection, right? Like this. So, this bijection is not unique.", "tokens": [51300, 293, 550, 321, 393, 362, 1071, 3228, 1020, 313, 11, 558, 30, 1743, 341, 13, 407, 11, 341, 3228, 1020, 313, 307, 406, 3845, 13, 51540], "temperature": 0.0, "avg_logprob": -0.11195322844359251, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0038024468813091516}, {"id": 614, "seek": 358028, "start": 3603.8, "end": 3609.96, "text": " But for determining if two graphs are isomorphic, it's sufficient to say that", "tokens": [51540, 583, 337, 23751, 498, 732, 24877, 366, 307, 32702, 299, 11, 309, 311, 11563, 281, 584, 300, 51848], "temperature": 0.0, "avg_logprob": -0.11195322844359251, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0038024468813091516}, {"id": 615, "seek": 360996, "start": 3609.96, "end": 3614.84, "text": " there exists such a bijection, right? So, that's what tells us that the two graphs are equivalent,", "tokens": [50364, 456, 8198, 1270, 257, 3228, 1020, 313, 11, 558, 30, 407, 11, 300, 311, 437, 5112, 505, 300, 264, 732, 24877, 366, 10344, 11, 50608], "temperature": 0.0, "avg_logprob": -0.10944060941713046, "compression_ratio": 1.8601694915254237, "no_speech_prob": 0.0007758059073239565}, {"id": 616, "seek": 360996, "start": 3614.84, "end": 3619.7200000000003, "text": " right? So, basically, they are the same up to the ordering of the nodes, right? So,", "tokens": [50608, 558, 30, 407, 11, 1936, 11, 436, 366, 264, 912, 493, 281, 264, 21739, 295, 264, 13891, 11, 558, 30, 407, 11, 50852], "temperature": 0.0, "avg_logprob": -0.10944060941713046, "compression_ratio": 1.8601694915254237, "no_speech_prob": 0.0007758059073239565}, {"id": 617, "seek": 360996, "start": 3619.7200000000003, "end": 3624.28, "text": " if I look at their adjacency matrix, they will be the same up to applying some permutation,", "tokens": [50852, 498, 286, 574, 412, 641, 22940, 3020, 8141, 11, 436, 486, 312, 264, 912, 493, 281, 9275, 512, 4784, 11380, 11, 51080], "temperature": 0.0, "avg_logprob": -0.10944060941713046, "compression_ratio": 1.8601694915254237, "no_speech_prob": 0.0007758059073239565}, {"id": 618, "seek": 360996, "start": 3624.28, "end": 3628.92, "text": " right? So, I can basically, I can relate the two adjacencies by permutation.", "tokens": [51080, 558, 30, 407, 11, 286, 393, 1936, 11, 286, 393, 10961, 264, 732, 22940, 6464, 538, 4784, 11380, 13, 51312], "temperature": 0.0, "avg_logprob": -0.10944060941713046, "compression_ratio": 1.8601694915254237, "no_speech_prob": 0.0007758059073239565}, {"id": 619, "seek": 360996, "start": 3630.28, "end": 3635.16, "text": " And related to the question of universal approximation, right? Which is fundamental for", "tokens": [51380, 400, 4077, 281, 264, 1168, 295, 11455, 28023, 11, 558, 30, 3013, 307, 8088, 337, 51624], "temperature": 0.0, "avg_logprob": -0.10944060941713046, "compression_ratio": 1.8601694915254237, "no_speech_prob": 0.0007758059073239565}, {"id": 620, "seek": 363516, "start": 3635.16, "end": 3641.24, "text": " traditional neural networks like perceptrons, we can show that a class of functions is universal", "tokens": [50364, 5164, 18161, 9590, 411, 43276, 13270, 11, 321, 393, 855, 300, 257, 1508, 295, 6828, 307, 11455, 50668], "temperature": 0.0, "avg_logprob": -0.12311774662562779, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.00182493613101542}, {"id": 621, "seek": 363516, "start": 3641.24, "end": 3646.2, "text": " approximating permutation in variant functions on graphs with, importantly, here, the limitation", "tokens": [50668, 8542, 990, 4784, 11380, 294, 17501, 6828, 322, 24877, 365, 11, 8906, 11, 510, 11, 264, 27432, 50916], "temperature": 0.0, "avg_logprob": -0.12311774662562779, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.00182493613101542}, {"id": 622, "seek": 363516, "start": 3646.2, "end": 3652.52, "text": " finite node features, if and only if it can discriminate graph isomorphisms, right? So,", "tokens": [50916, 19362, 9984, 4122, 11, 498, 293, 787, 498, 309, 393, 47833, 4295, 307, 32702, 13539, 11, 558, 30, 407, 11, 51232], "temperature": 0.0, "avg_logprob": -0.12311774662562779, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.00182493613101542}, {"id": 623, "seek": 363516, "start": 3652.52, "end": 3657.16, "text": " basically, universal approximation is equivalent to graph isomorphism testing, right? So, basically,", "tokens": [51232, 1936, 11, 11455, 28023, 307, 10344, 281, 4295, 307, 32702, 1434, 4997, 11, 558, 30, 407, 11, 1936, 11, 51464], "temperature": 0.0, "avg_logprob": -0.12311774662562779, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.00182493613101542}, {"id": 624, "seek": 363516, "start": 3657.16, "end": 3664.12, "text": " the two things go hand in hand. And if you ask what kind of graphs can we represent with", "tokens": [51464, 264, 732, 721, 352, 1011, 294, 1011, 13, 400, 498, 291, 1029, 437, 733, 295, 24877, 393, 321, 2906, 365, 51812], "temperature": 0.0, "avg_logprob": -0.12311774662562779, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.00182493613101542}, {"id": 625, "seek": 366412, "start": 3664.12, "end": 3668.3599999999997, "text": " message passing neural networks, right? So, here is, let's say, the space of all graphs.", "tokens": [50364, 3636, 8437, 18161, 9590, 11, 558, 30, 407, 11, 510, 307, 11, 718, 311, 584, 11, 264, 1901, 295, 439, 24877, 13, 50576], "temperature": 0.0, "avg_logprob": -0.08810916380448774, "compression_ratio": 1.7665369649805447, "no_speech_prob": 0.0029902912210673094}, {"id": 626, "seek": 366412, "start": 3669.08, "end": 3673.48, "text": " And these would be graphs that are structurally equivalent, right? That is isomorphic. So,", "tokens": [50612, 400, 613, 576, 312, 24877, 300, 366, 6594, 6512, 10344, 11, 558, 30, 663, 307, 307, 32702, 299, 13, 407, 11, 50832], "temperature": 0.0, "avg_logprob": -0.08810916380448774, "compression_ratio": 1.7665369649805447, "no_speech_prob": 0.0029902912210673094}, {"id": 627, "seek": 366412, "start": 3673.48, "end": 3678.3599999999997, "text": " by construction, we know that graph neural network cannot distinguish between these graphs,", "tokens": [50832, 538, 6435, 11, 321, 458, 300, 4295, 18161, 3209, 2644, 20206, 1296, 613, 24877, 11, 51076], "temperature": 0.0, "avg_logprob": -0.08810916380448774, "compression_ratio": 1.7665369649805447, "no_speech_prob": 0.0029902912210673094}, {"id": 628, "seek": 366412, "start": 3678.3599999999997, "end": 3682.7599999999998, "text": " right? They're exactly the same up to the ordering of the nodes. So, just by construction,", "tokens": [51076, 558, 30, 814, 434, 2293, 264, 912, 493, 281, 264, 21739, 295, 264, 13891, 13, 407, 11, 445, 538, 6435, 11, 51296], "temperature": 0.0, "avg_logprob": -0.08810916380448774, "compression_ratio": 1.7665369649805447, "no_speech_prob": 0.0029902912210673094}, {"id": 629, "seek": 366412, "start": 3682.7599999999998, "end": 3689.72, "text": " it will produce the same output for any isomorphic graphs. But the question of the opposite", "tokens": [51296, 309, 486, 5258, 264, 912, 5598, 337, 604, 307, 32702, 299, 24877, 13, 583, 264, 1168, 295, 264, 6182, 51644], "temperature": 0.0, "avg_logprob": -0.08810916380448774, "compression_ratio": 1.7665369649805447, "no_speech_prob": 0.0029902912210673094}, {"id": 630, "seek": 368972, "start": 3689.72, "end": 3695.3199999999997, "text": " direction is more interesting. And this is not necessarily guaranteed, right? So, I might have", "tokens": [50364, 3513, 307, 544, 1880, 13, 400, 341, 307, 406, 4725, 18031, 11, 558, 30, 407, 11, 286, 1062, 362, 50644], "temperature": 0.0, "avg_logprob": -0.09883499145507812, "compression_ratio": 1.7969924812030076, "no_speech_prob": 0.003778995480388403}, {"id": 631, "seek": 368972, "start": 3695.3199999999997, "end": 3700.12, "text": " different graphs that are not isomorphic, like the reds and the blues, that by chance will have", "tokens": [50644, 819, 24877, 300, 366, 406, 307, 32702, 299, 11, 411, 264, 2182, 82, 293, 264, 24244, 11, 300, 538, 2931, 486, 362, 50884], "temperature": 0.0, "avg_logprob": -0.09883499145507812, "compression_ratio": 1.7969924812030076, "no_speech_prob": 0.003778995480388403}, {"id": 632, "seek": 368972, "start": 3700.12, "end": 3705.3999999999996, "text": " the same representation. So, the graph neural network will output the same output for these", "tokens": [50884, 264, 912, 10290, 13, 407, 11, 264, 4295, 18161, 3209, 486, 5598, 264, 912, 5598, 337, 613, 51148], "temperature": 0.0, "avg_logprob": -0.09883499145507812, "compression_ratio": 1.7969924812030076, "no_speech_prob": 0.003778995480388403}, {"id": 633, "seek": 368972, "start": 3705.3999999999996, "end": 3710.3599999999997, "text": " different graphs. So, in other words, if we have the space of all permutation in variant functions,", "tokens": [51148, 819, 24877, 13, 407, 11, 294, 661, 2283, 11, 498, 321, 362, 264, 1901, 295, 439, 4784, 11380, 294, 17501, 6828, 11, 51396], "temperature": 0.0, "avg_logprob": -0.09883499145507812, "compression_ratio": 1.7969924812030076, "no_speech_prob": 0.003778995480388403}, {"id": 634, "seek": 368972, "start": 3710.3599999999997, "end": 3715.3199999999997, "text": " right, and we know that these are all graph isomorphism discriminating functions, this is where", "tokens": [51396, 558, 11, 293, 321, 458, 300, 613, 366, 439, 4295, 307, 32702, 1434, 20828, 990, 6828, 11, 341, 307, 689, 51644], "temperature": 0.0, "avg_logprob": -0.09883499145507812, "compression_ratio": 1.7969924812030076, "no_speech_prob": 0.003778995480388403}, {"id": 635, "seek": 371532, "start": 3716.04, "end": 3719.88, "text": " we'll see a subclass of functions that can be computed by message passing.", "tokens": [50400, 321, 603, 536, 257, 1422, 11665, 295, 6828, 300, 393, 312, 40610, 538, 3636, 8437, 13, 50592], "temperature": 0.0, "avg_logprob": -0.10518530045432606, "compression_ratio": 1.5565217391304347, "no_speech_prob": 0.0009798739338293672}, {"id": 636, "seek": 371532, "start": 3722.84, "end": 3729.32, "text": " Okay. And so, the question of graph isomorphism, as I mentioned already, it came from applications", "tokens": [50740, 1033, 13, 400, 370, 11, 264, 1168, 295, 4295, 307, 32702, 1434, 11, 382, 286, 2835, 1217, 11, 309, 1361, 490, 5821, 51064], "temperature": 0.0, "avg_logprob": -0.10518530045432606, "compression_ratio": 1.5565217391304347, "no_speech_prob": 0.0009798739338293672}, {"id": 637, "seek": 371532, "start": 3729.32, "end": 3734.52, "text": " in organic chemistry, where people try to compare structures and try to determine whether", "tokens": [51064, 294, 10220, 12558, 11, 689, 561, 853, 281, 6794, 9227, 293, 853, 281, 6997, 1968, 51324], "temperature": 0.0, "avg_logprob": -0.10518530045432606, "compression_ratio": 1.5565217391304347, "no_speech_prob": 0.0009798739338293672}, {"id": 638, "seek": 371532, "start": 3735.2400000000002, "end": 3740.52, "text": " two molecules are the same, right? So, in the case of isomorphism is a special setting, right?", "tokens": [51360, 732, 13093, 366, 264, 912, 11, 558, 30, 407, 11, 294, 264, 1389, 295, 307, 32702, 1434, 307, 257, 2121, 3287, 11, 558, 30, 51624], "temperature": 0.0, "avg_logprob": -0.10518530045432606, "compression_ratio": 1.5565217391304347, "no_speech_prob": 0.0009798739338293672}, {"id": 639, "seek": 374052, "start": 3740.52, "end": 3746.7599999999998, "text": " We can also think of distances between graphs. And vice versa in 68 came up with an algorithm", "tokens": [50364, 492, 393, 611, 519, 295, 22182, 1296, 24877, 13, 400, 11964, 25650, 294, 23317, 1361, 493, 365, 364, 9284, 50676], "temperature": 0.0, "avg_logprob": -0.1356697403982784, "compression_ratio": 1.5875, "no_speech_prob": 0.004992531146854162}, {"id": 640, "seek": 374052, "start": 3746.7599999999998, "end": 3751.88, "text": " that they believe to be a polynomial time method for determining whether two graphs are isomorphic.", "tokens": [50676, 300, 436, 1697, 281, 312, 257, 26110, 565, 3170, 337, 23751, 1968, 732, 24877, 366, 307, 32702, 299, 13, 50932], "temperature": 0.0, "avg_logprob": -0.1356697403982784, "compression_ratio": 1.5875, "no_speech_prob": 0.004992531146854162}, {"id": 641, "seek": 374052, "start": 3752.84, "end": 3758.52, "text": " So, I should say that at that time in the 60s, even the notion of complexity was not totally", "tokens": [50980, 407, 11, 286, 820, 584, 300, 412, 300, 565, 294, 264, 4060, 82, 11, 754, 264, 10710, 295, 14024, 390, 406, 3879, 51264], "temperature": 0.0, "avg_logprob": -0.1356697403982784, "compression_ratio": 1.5875, "no_speech_prob": 0.004992531146854162}, {"id": 642, "seek": 374052, "start": 3759.88, "end": 3768.6, "text": " spelled out. And also, the understanding of what's the complexity of graph isomorphism testing", "tokens": [51332, 34388, 484, 13, 400, 611, 11, 264, 3701, 295, 437, 311, 264, 14024, 295, 4295, 307, 32702, 1434, 4997, 51768], "temperature": 0.0, "avg_logprob": -0.1356697403982784, "compression_ratio": 1.5875, "no_speech_prob": 0.004992531146854162}, {"id": 643, "seek": 376860, "start": 3769.16, "end": 3773.3199999999997, "text": " as a computer science problem was not understood. Actually, it's not understood even now. So,", "tokens": [50392, 382, 257, 3820, 3497, 1154, 390, 406, 7320, 13, 5135, 11, 309, 311, 406, 7320, 754, 586, 13, 407, 11, 50600], "temperature": 0.0, "avg_logprob": -0.15282920149506116, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.003946259152144194}, {"id": 644, "seek": 376860, "start": 3773.3199999999997, "end": 3778.8399999999997, "text": " we know that it's not NP-hard and we also don't know polynomial time algorithms for it. So,", "tokens": [50600, 321, 458, 300, 309, 311, 406, 38611, 12, 21491, 293, 321, 611, 500, 380, 458, 26110, 565, 14642, 337, 309, 13, 407, 11, 50876], "temperature": 0.0, "avg_logprob": -0.15282920149506116, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.003946259152144194}, {"id": 645, "seek": 376860, "start": 3778.8399999999997, "end": 3784.6, "text": " it's a special complexity class that is called GI class. But anyway, so, it was actually disproved", "tokens": [50876, 309, 311, 257, 2121, 14024, 1508, 300, 307, 1219, 26634, 1508, 13, 583, 4033, 11, 370, 11, 309, 390, 767, 717, 4318, 937, 51164], "temperature": 0.0, "avg_logprob": -0.15282920149506116, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.003946259152144194}, {"id": 646, "seek": 376860, "start": 3784.6, "end": 3792.04, "text": " by a counter example. So, it was an example, it was shown that the class of graphs that cannot be", "tokens": [51164, 538, 257, 5682, 1365, 13, 407, 11, 309, 390, 364, 1365, 11, 309, 390, 4898, 300, 264, 1508, 295, 24877, 300, 2644, 312, 51536], "temperature": 0.0, "avg_logprob": -0.15282920149506116, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.003946259152144194}, {"id": 647, "seek": 376860, "start": 3793.24, "end": 3798.2799999999997, "text": " tested by the device for an algorithm. We'll see such examples in a second. But the way that", "tokens": [51596, 8246, 538, 264, 4302, 337, 364, 9284, 13, 492, 603, 536, 1270, 5110, 294, 257, 1150, 13, 583, 264, 636, 300, 51848], "temperature": 0.0, "avg_logprob": -0.15282920149506116, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.003946259152144194}, {"id": 648, "seek": 379828, "start": 3798.84, "end": 3804.0400000000004, "text": " it goes, it's essentially a color refinement procedure. So, it considers a graph without any", "tokens": [50392, 309, 1709, 11, 309, 311, 4476, 257, 2017, 1895, 30229, 10747, 13, 407, 11, 309, 33095, 257, 4295, 1553, 604, 50652], "temperature": 0.0, "avg_logprob": -0.11349975751793903, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.0026212530210614204}, {"id": 649, "seek": 379828, "start": 3804.0400000000004, "end": 3809.48, "text": " features, considers only its structure. And, initially, the graph has every node labeled in", "tokens": [50652, 4122, 11, 33095, 787, 1080, 3877, 13, 400, 11, 9105, 11, 264, 4295, 575, 633, 9984, 21335, 294, 50924], "temperature": 0.0, "avg_logprob": -0.11349975751793903, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.0026212530210614204}, {"id": 650, "seek": 379828, "start": 3809.48, "end": 3815.0800000000004, "text": " the same way. By label, I just mean a natural number that is attached to a node, right? And what", "tokens": [50924, 264, 912, 636, 13, 3146, 7645, 11, 286, 445, 914, 257, 3303, 1230, 300, 307, 8570, 281, 257, 9984, 11, 558, 30, 400, 437, 51204], "temperature": 0.0, "avg_logprob": -0.11349975751793903, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.0026212530210614204}, {"id": 651, "seek": 379828, "start": 3815.0800000000004, "end": 3821.6400000000003, "text": " the algorithm does, it takes a node and looks at its neighborhood, right? And you can see that", "tokens": [51204, 264, 9284, 775, 11, 309, 2516, 257, 9984, 293, 1542, 412, 1080, 7630, 11, 558, 30, 400, 291, 393, 536, 300, 51532], "temperature": 0.0, "avg_logprob": -0.11349975751793903, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.0026212530210614204}, {"id": 652, "seek": 379828, "start": 3821.6400000000003, "end": 3825.4, "text": " originally in this graph, we have two types of neighborhoods. So, we have a blue node with", "tokens": [51532, 7993, 294, 341, 4295, 11, 321, 362, 732, 3467, 295, 20052, 13, 407, 11, 321, 362, 257, 3344, 9984, 365, 51720], "temperature": 0.0, "avg_logprob": -0.11349975751793903, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.0026212530210614204}, {"id": 653, "seek": 382540, "start": 3825.48, "end": 3830.84, "text": " two blue neighbors like this. Sorry, that's blue node with three blue neighbors and blue node with", "tokens": [50368, 732, 3344, 12512, 411, 341, 13, 4919, 11, 300, 311, 3344, 9984, 365, 1045, 3344, 12512, 293, 3344, 9984, 365, 50636], "temperature": 0.0, "avg_logprob": -0.08894201247922835, "compression_ratio": 1.9593495934959348, "no_speech_prob": 0.005487440153956413}, {"id": 654, "seek": 382540, "start": 3831.4, "end": 3835.96, "text": " two blue neighbors, right? So, these are two neighborhoods that we see in this graph. So,", "tokens": [50664, 732, 3344, 12512, 11, 558, 30, 407, 11, 613, 366, 732, 20052, 300, 321, 536, 294, 341, 4295, 13, 407, 11, 50892], "temperature": 0.0, "avg_logprob": -0.08894201247922835, "compression_ratio": 1.9593495934959348, "no_speech_prob": 0.005487440153956413}, {"id": 655, "seek": 382540, "start": 3835.96, "end": 3841.88, "text": " if I now apply an injective function that they call phi, right? Think of it as hashing. I will", "tokens": [50892, 498, 286, 586, 3079, 364, 10711, 488, 2445, 300, 436, 818, 13107, 11, 558, 30, 6557, 295, 309, 382, 575, 571, 13, 286, 486, 51188], "temperature": 0.0, "avg_logprob": -0.08894201247922835, "compression_ratio": 1.9593495934959348, "no_speech_prob": 0.005487440153956413}, {"id": 656, "seek": 382540, "start": 3841.88, "end": 3847.1600000000003, "text": " have two distinct outputs, right? So, I will have nodes of the yellow type, let's call it, and of", "tokens": [51188, 362, 732, 10644, 23930, 11, 558, 30, 407, 11, 286, 486, 362, 13891, 295, 264, 5566, 2010, 11, 718, 311, 818, 309, 11, 293, 295, 51452], "temperature": 0.0, "avg_logprob": -0.08894201247922835, "compression_ratio": 1.9593495934959348, "no_speech_prob": 0.005487440153956413}, {"id": 657, "seek": 382540, "start": 3847.1600000000003, "end": 3851.2400000000002, "text": " green type, right? So, I will be able to distinguish between these different neighborhoods. So, now,", "tokens": [51452, 3092, 2010, 11, 558, 30, 407, 11, 286, 486, 312, 1075, 281, 20206, 1296, 613, 819, 20052, 13, 407, 11, 586, 11, 51656], "temperature": 0.0, "avg_logprob": -0.08894201247922835, "compression_ratio": 1.9593495934959348, "no_speech_prob": 0.005487440153956413}, {"id": 658, "seek": 385124, "start": 3851.24, "end": 3855.64, "text": " I have a graph with refined labels. I can apply the same procedure again, and now we have three", "tokens": [50364, 286, 362, 257, 4295, 365, 26201, 16949, 13, 286, 393, 3079, 264, 912, 10747, 797, 11, 293, 586, 321, 362, 1045, 50584], "temperature": 0.0, "avg_logprob": -0.08536624908447266, "compression_ratio": 1.971731448763251, "no_speech_prob": 0.0029465979896485806}, {"id": 659, "seek": 385124, "start": 3855.64, "end": 3860.2, "text": " types of neighborhoods, right? We have green with one green and one yellow neighbor. We have", "tokens": [50584, 3467, 295, 20052, 11, 558, 30, 492, 362, 3092, 365, 472, 3092, 293, 472, 5566, 5987, 13, 492, 362, 50812], "temperature": 0.0, "avg_logprob": -0.08536624908447266, "compression_ratio": 1.971731448763251, "no_speech_prob": 0.0029465979896485806}, {"id": 660, "seek": 385124, "start": 3860.8399999999997, "end": 3864.8399999999997, "text": " green with two yellow neighbors, and we have yellow with two green and one yellow neighbor,", "tokens": [50844, 3092, 365, 732, 5566, 12512, 11, 293, 321, 362, 5566, 365, 732, 3092, 293, 472, 5566, 5987, 11, 51044], "temperature": 0.0, "avg_logprob": -0.08536624908447266, "compression_ratio": 1.971731448763251, "no_speech_prob": 0.0029465979896485806}, {"id": 661, "seek": 385124, "start": 3864.8399999999997, "end": 3869.72, "text": " right? And these become, again, distinct colors. So, this will be, let's call it violet, gray,", "tokens": [51044, 558, 30, 400, 613, 1813, 11, 797, 11, 10644, 4577, 13, 407, 11, 341, 486, 312, 11, 718, 311, 818, 309, 46480, 11, 10855, 11, 51288], "temperature": 0.0, "avg_logprob": -0.08536624908447266, "compression_ratio": 1.971731448763251, "no_speech_prob": 0.0029465979896485806}, {"id": 662, "seek": 385124, "start": 3869.72, "end": 3875.24, "text": " and orange. But if I repeat this procedure again, the colors will stop changing at which", "tokens": [51288, 293, 7671, 13, 583, 498, 286, 7149, 341, 10747, 797, 11, 264, 4577, 486, 1590, 4473, 412, 597, 51564], "temperature": 0.0, "avg_logprob": -0.08536624908447266, "compression_ratio": 1.971731448763251, "no_speech_prob": 0.0029465979896485806}, {"id": 663, "seek": 385124, "start": 3875.24, "end": 3880.6, "text": " point the algorithm stops and produces a histogram of colors, right? So, that's a graph level", "tokens": [51564, 935, 264, 9284, 10094, 293, 14725, 257, 49816, 295, 4577, 11, 558, 30, 407, 11, 300, 311, 257, 4295, 1496, 51832], "temperature": 0.0, "avg_logprob": -0.08536624908447266, "compression_ratio": 1.971731448763251, "no_speech_prob": 0.0029465979896485806}, {"id": 664, "seek": 388060, "start": 3880.6, "end": 3886.52, "text": " descriptor. You can think of it this way. And what they show in the paper, well, the paper is", "tokens": [50364, 31280, 284, 13, 509, 393, 519, 295, 309, 341, 636, 13, 400, 437, 436, 855, 294, 264, 3035, 11, 731, 11, 264, 3035, 307, 50660], "temperature": 0.0, "avg_logprob": -0.10271353062575425, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.0029346197843551636}, {"id": 665, "seek": 388060, "start": 3886.52, "end": 3891.3199999999997, "text": " actually complicated to read, but that's, let's say, a reduction of it. It actually describes", "tokens": [50660, 767, 6179, 281, 1401, 11, 457, 300, 311, 11, 718, 311, 584, 11, 257, 11004, 295, 309, 13, 467, 767, 15626, 50900], "temperature": 0.0, "avg_logprob": -0.10271353062575425, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.0029346197843551636}, {"id": 666, "seek": 388060, "start": 3891.3199999999997, "end": 3898.68, "text": " a different type of algorithm, what is called 2WL, that does edge color refinement, but it", "tokens": [50900, 257, 819, 2010, 295, 9284, 11, 437, 307, 1219, 568, 54, 43, 11, 300, 775, 4691, 2017, 1895, 30229, 11, 457, 309, 51268], "temperature": 0.0, "avg_logprob": -0.10271353062575425, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.0029346197843551636}, {"id": 667, "seek": 388060, "start": 3898.68, "end": 3904.04, "text": " doesn't matter. It's equivalent to what I'm showing here. So, if I give you another graph,", "tokens": [51268, 1177, 380, 1871, 13, 467, 311, 10344, 281, 437, 286, 478, 4099, 510, 13, 407, 11, 498, 286, 976, 291, 1071, 4295, 11, 51536], "temperature": 0.0, "avg_logprob": -0.10271353062575425, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.0029346197843551636}, {"id": 668, "seek": 388060, "start": 3904.04, "end": 3909.72, "text": " and the distribution of colors is different, then I can guarantee that they are not isomorphic. But", "tokens": [51536, 293, 264, 7316, 295, 4577, 307, 819, 11, 550, 286, 393, 10815, 300, 436, 366, 406, 307, 32702, 299, 13, 583, 51820], "temperature": 0.0, "avg_logprob": -0.10271353062575425, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.0029346197843551636}, {"id": 669, "seek": 390972, "start": 3909.72, "end": 3916.52, "text": " if the distribution of colors is the same, like in this case, we actually don't know. So, it's", "tokens": [50364, 498, 264, 7316, 295, 4577, 307, 264, 912, 11, 411, 294, 341, 1389, 11, 321, 767, 500, 380, 458, 13, 407, 11, 309, 311, 50704], "temperature": 0.0, "avg_logprob": -0.0956019674028669, "compression_ratio": 1.5720338983050848, "no_speech_prob": 0.004073250573128462}, {"id": 670, "seek": 390972, "start": 3916.52, "end": 3921.56, "text": " unnecessary, but insufficient condition, and in fact, you can find examples of non-isomorphic", "tokens": [50704, 19350, 11, 457, 41709, 4188, 11, 293, 294, 1186, 11, 291, 393, 915, 5110, 295, 2107, 12, 271, 32702, 299, 50956], "temperature": 0.0, "avg_logprob": -0.0956019674028669, "compression_ratio": 1.5720338983050848, "no_speech_prob": 0.004073250573128462}, {"id": 671, "seek": 390972, "start": 3921.56, "end": 3926.4399999999996, "text": " graphs that would be deemed equivalent by WL test, right? Or in this case, WL test cannot", "tokens": [50956, 24877, 300, 576, 312, 27637, 10344, 538, 343, 43, 1500, 11, 558, 30, 1610, 294, 341, 1389, 11, 343, 43, 1500, 2644, 51200], "temperature": 0.0, "avg_logprob": -0.0956019674028669, "compression_ratio": 1.5720338983050848, "no_speech_prob": 0.004073250573128462}, {"id": 672, "seek": 390972, "start": 3926.4399999999996, "end": 3931.64, "text": " determine whether they're not isomorphic. And you can also see why the reason for it, right?", "tokens": [51200, 6997, 1968, 436, 434, 406, 307, 32702, 299, 13, 400, 291, 393, 611, 536, 983, 264, 1778, 337, 309, 11, 558, 30, 51460], "temperature": 0.0, "avg_logprob": -0.0956019674028669, "compression_ratio": 1.5720338983050848, "no_speech_prob": 0.004073250573128462}, {"id": 673, "seek": 393164, "start": 3931.64, "end": 3940.3599999999997, "text": " Basically, what it does, it refines the colors of the nodes, so every node looks at its neighborhood.", "tokens": [50364, 8537, 11, 437, 309, 775, 11, 309, 1895, 1652, 264, 4577, 295, 264, 13891, 11, 370, 633, 9984, 1542, 412, 1080, 7630, 13, 50800], "temperature": 0.0, "avg_logprob": -0.09164529688218061, "compression_ratio": 1.943089430894309, "no_speech_prob": 0.016973402351140976}, {"id": 674, "seek": 393164, "start": 3940.3599999999997, "end": 3944.44, "text": " And this is how the neighborhoods of nodes look like, right? So, this node has two neighbors,", "tokens": [50800, 400, 341, 307, 577, 264, 20052, 295, 13891, 574, 411, 11, 558, 30, 407, 11, 341, 9984, 575, 732, 12512, 11, 51004], "temperature": 0.0, "avg_logprob": -0.09164529688218061, "compression_ratio": 1.943089430894309, "no_speech_prob": 0.016973402351140976}, {"id": 675, "seek": 393164, "start": 3944.44, "end": 3948.52, "text": " then this node has, again, two neighbors, and so on and so forth, right? So, if you look at the", "tokens": [51004, 550, 341, 9984, 575, 11, 797, 11, 732, 12512, 11, 293, 370, 322, 293, 370, 5220, 11, 558, 30, 407, 11, 498, 291, 574, 412, 264, 51208], "temperature": 0.0, "avg_logprob": -0.09164529688218061, "compression_ratio": 1.943089430894309, "no_speech_prob": 0.016973402351140976}, {"id": 676, "seek": 393164, "start": 3948.52, "end": 3953.56, "text": " structure of these neighbors, they will be exactly the same in both cases, right? And actually,", "tokens": [51208, 3877, 295, 613, 12512, 11, 436, 486, 312, 2293, 264, 912, 294, 1293, 3331, 11, 558, 30, 400, 767, 11, 51460], "temperature": 0.0, "avg_logprob": -0.09164529688218061, "compression_ratio": 1.943089430894309, "no_speech_prob": 0.016973402351140976}, {"id": 677, "seek": 393164, "start": 3953.56, "end": 3958.2, "text": " very simple examples of graphs, for example, regular graphs where the degree of every node", "tokens": [51460, 588, 2199, 5110, 295, 24877, 11, 337, 1365, 11, 3890, 24877, 689, 264, 4314, 295, 633, 9984, 51692], "temperature": 0.0, "avg_logprob": -0.09164529688218061, "compression_ratio": 1.943089430894309, "no_speech_prob": 0.016973402351140976}, {"id": 678, "seek": 395820, "start": 3958.2, "end": 3966.3599999999997, "text": " is the same cannot be tested by this simple procedure of Weisfeld and Lehmann. You can also not count", "tokens": [50364, 307, 264, 912, 2644, 312, 8246, 538, 341, 2199, 10747, 295, 492, 271, 25115, 293, 1456, 8587, 969, 13, 509, 393, 611, 406, 1207, 50772], "temperature": 0.0, "avg_logprob": -0.14370485283862586, "compression_ratio": 1.5601659751037344, "no_speech_prob": 0.002213777042925358}, {"id": 679, "seek": 395820, "start": 3970.52, "end": 3975.96, "text": " connected patterns of more than three nodes, like triangles or cycles. And this is, I think,", "tokens": [50980, 4582, 8294, 295, 544, 813, 1045, 13891, 11, 411, 29896, 420, 17796, 13, 400, 341, 307, 11, 286, 519, 11, 51252], "temperature": 0.0, "avg_logprob": -0.14370485283862586, "compression_ratio": 1.5601659751037344, "no_speech_prob": 0.002213777042925358}, {"id": 680, "seek": 395820, "start": 3975.96, "end": 3980.12, "text": " astonishingly disappointing given that the algorithm came from applications in chemistry,", "tokens": [51252, 35264, 356, 25054, 2212, 300, 264, 9284, 1361, 490, 5821, 294, 12558, 11, 51460], "temperature": 0.0, "avg_logprob": -0.14370485283862586, "compression_ratio": 1.5601659751037344, "no_speech_prob": 0.002213777042925358}, {"id": 681, "seek": 395820, "start": 3980.12, "end": 3986.04, "text": " so in chemistry, these would be two different molecules, right? And this has a six ring and", "tokens": [51460, 370, 294, 12558, 11, 613, 576, 312, 732, 819, 13093, 11, 558, 30, 400, 341, 575, 257, 2309, 4875, 293, 51756], "temperature": 0.0, "avg_logprob": -0.14370485283862586, "compression_ratio": 1.5601659751037344, "no_speech_prob": 0.002213777042925358}, {"id": 682, "seek": 398604, "start": 3986.04, "end": 3992.68, "text": " this has a five ring, right? So, or five cycle using graph theory terminology. So, we cannot", "tokens": [50364, 341, 575, 257, 1732, 4875, 11, 558, 30, 407, 11, 420, 1732, 6586, 1228, 4295, 5261, 27575, 13, 407, 11, 321, 2644, 50696], "temperature": 0.0, "avg_logprob": -0.11623592786891486, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0007725927862338722}, {"id": 683, "seek": 398604, "start": 3992.68, "end": 3998.2, "text": " distinguish these molecules by device for a Lehmann test. They would appear potentially the same,", "tokens": [50696, 20206, 613, 13093, 538, 4302, 337, 257, 1456, 8587, 969, 1500, 13, 814, 576, 4204, 7263, 264, 912, 11, 50972], "temperature": 0.0, "avg_logprob": -0.11623592786891486, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0007725927862338722}, {"id": 684, "seek": 398604, "start": 3998.2, "end": 4006.68, "text": " right? So, we wouldn't know. So, basically, the functions that can be computed by WL are strictly", "tokens": [50972, 558, 30, 407, 11, 321, 2759, 380, 458, 13, 407, 11, 1936, 11, 264, 6828, 300, 393, 312, 40610, 538, 343, 43, 366, 20792, 51396], "temperature": 0.0, "avg_logprob": -0.11623592786891486, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0007725927862338722}, {"id": 685, "seek": 398604, "start": 4006.68, "end": 4011.64, "text": " smaller than all permutation invariant functions, right? And we know examples of functions that", "tokens": [51396, 4356, 813, 439, 4784, 11380, 33270, 394, 6828, 11, 558, 30, 400, 321, 458, 5110, 295, 6828, 300, 51644], "temperature": 0.0, "avg_logprob": -0.11623592786891486, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0007725927862338722}, {"id": 686, "seek": 401164, "start": 4011.64, "end": 4015.56, "text": " cannot be computed by WL. For example, we cannot count the number of rings, right? So,", "tokens": [50364, 2644, 312, 40610, 538, 343, 43, 13, 1171, 1365, 11, 321, 2644, 1207, 264, 1230, 295, 11136, 11, 558, 30, 407, 11, 50560], "temperature": 0.0, "avg_logprob": -0.07576985586257208, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.0019215102074667811}, {"id": 687, "seek": 401164, "start": 4015.56, "end": 4019.56, "text": " if I want to implement a function that counts the number of rings in a graph, I cannot do it by", "tokens": [50560, 498, 286, 528, 281, 4445, 257, 2445, 300, 14893, 264, 1230, 295, 11136, 294, 257, 4295, 11, 286, 2644, 360, 309, 538, 50760], "temperature": 0.0, "avg_logprob": -0.07576985586257208, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.0019215102074667811}, {"id": 688, "seek": 401164, "start": 4019.56, "end": 4026.3599999999997, "text": " means of WL test or by means of message passing. Now, the relation between WL and message passing", "tokens": [50760, 1355, 295, 343, 43, 1500, 420, 538, 1355, 295, 3636, 8437, 13, 823, 11, 264, 9721, 1296, 343, 43, 293, 3636, 8437, 51100], "temperature": 0.0, "avg_logprob": -0.07576985586257208, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.0019215102074667811}, {"id": 689, "seek": 401164, "start": 4026.3599999999997, "end": 4031.72, "text": " is not random, right? You can see this, even the structure of the algorithm is exactly the same,", "tokens": [51100, 307, 406, 4974, 11, 558, 30, 509, 393, 536, 341, 11, 754, 264, 3877, 295, 264, 9284, 307, 2293, 264, 912, 11, 51368], "temperature": 0.0, "avg_logprob": -0.07576985586257208, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.0019215102074667811}, {"id": 690, "seek": 401164, "start": 4031.72, "end": 4037.72, "text": " right? So, this is what WL test does, right? So, it updates the color of every node by looking at", "tokens": [51368, 558, 30, 407, 11, 341, 307, 437, 343, 43, 1500, 775, 11, 558, 30, 407, 11, 309, 9205, 264, 2017, 295, 633, 9984, 538, 1237, 412, 51668], "temperature": 0.0, "avg_logprob": -0.07576985586257208, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.0019215102074667811}, {"id": 691, "seek": 403772, "start": 4037.7999999999997, "end": 4044.04, "text": " the structure of the node and the multi-set of neighbors, right? Here, x denotes the colors.", "tokens": [50368, 264, 3877, 295, 264, 9984, 293, 264, 4825, 12, 3854, 295, 12512, 11, 558, 30, 1692, 11, 2031, 1441, 17251, 264, 4577, 13, 50680], "temperature": 0.0, "avg_logprob": -0.10701718949179613, "compression_ratio": 1.7245283018867925, "no_speech_prob": 0.00264080916531384}, {"id": 692, "seek": 403772, "start": 4044.04, "end": 4048.7599999999998, "text": " And this is what MPNN does, right? So, here, the squared denotes some general", "tokens": [50680, 400, 341, 307, 437, 14146, 45, 45, 775, 11, 558, 30, 407, 11, 510, 11, 264, 8889, 1441, 17251, 512, 2674, 50916], "temperature": 0.0, "avg_logprob": -0.10701718949179613, "compression_ratio": 1.7245283018867925, "no_speech_prob": 0.00264080916531384}, {"id": 693, "seek": 403772, "start": 4048.7599999999998, "end": 4053.0, "text": " permutation invariant aggregator. It can be sum, it can be max, it can be mean, it can be anything,", "tokens": [50916, 4784, 11380, 33270, 394, 16743, 1639, 13, 467, 393, 312, 2408, 11, 309, 393, 312, 11469, 11, 309, 393, 312, 914, 11, 309, 393, 312, 1340, 11, 51128], "temperature": 0.0, "avg_logprob": -0.10701718949179613, "compression_ratio": 1.7245283018867925, "no_speech_prob": 0.00264080916531384}, {"id": 694, "seek": 403772, "start": 4053.0, "end": 4058.04, "text": " right? Importantly, it's permutation invariant. So, we can see that it's a special case. So,", "tokens": [51128, 558, 30, 26391, 3627, 11, 309, 311, 4784, 11380, 33270, 394, 13, 407, 11, 321, 393, 536, 300, 309, 311, 257, 2121, 1389, 13, 407, 11, 51380], "temperature": 0.0, "avg_logprob": -0.10701718949179613, "compression_ratio": 1.7245283018867925, "no_speech_prob": 0.00264080916531384}, {"id": 695, "seek": 403772, "start": 4058.04, "end": 4064.3599999999997, "text": " MPNN expressive power is upper bounded by device for a Lehmann test. And the question is when", "tokens": [51380, 14146, 45, 45, 40189, 1347, 307, 6597, 37498, 538, 4302, 337, 257, 1456, 8587, 969, 1500, 13, 400, 264, 1168, 307, 562, 51696], "temperature": 0.0, "avg_logprob": -0.10701718949179613, "compression_ratio": 1.7245283018867925, "no_speech_prob": 0.00264080916531384}, {"id": 696, "seek": 406436, "start": 4064.36, "end": 4070.52, "text": " MPNN is as expressive as WL test, right? So, basically, we're interested in this case, right,", "tokens": [50364, 14146, 45, 45, 307, 382, 40189, 382, 343, 43, 1500, 11, 558, 30, 407, 11, 1936, 11, 321, 434, 3102, 294, 341, 1389, 11, 558, 11, 50672], "temperature": 0.0, "avg_logprob": -0.0825315878643253, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0019657756201922894}, {"id": 697, "seek": 406436, "start": 4070.52, "end": 4075.56, "text": " when the two circles coincide. And if you look at different types of aggregators, right? So,", "tokens": [50672, 562, 264, 732, 13040, 13001, 482, 13, 400, 498, 291, 574, 412, 819, 3467, 295, 16743, 3391, 11, 558, 30, 407, 11, 50924], "temperature": 0.0, "avg_logprob": -0.0825315878643253, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0019657756201922894}, {"id": 698, "seek": 406436, "start": 4075.56, "end": 4081.88, "text": " imagine that this is your input graph. So, I have this gray node that has two types of", "tokens": [50924, 3811, 300, 341, 307, 428, 4846, 4295, 13, 407, 11, 286, 362, 341, 10855, 9984, 300, 575, 732, 3467, 295, 51240], "temperature": 0.0, "avg_logprob": -0.0825315878643253, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0019657756201922894}, {"id": 699, "seek": 406436, "start": 4081.88, "end": 4086.6, "text": " neighbors. We have green neighbors and we have blue neighbors, right? So, if I consider the input", "tokens": [51240, 12512, 13, 492, 362, 3092, 12512, 293, 321, 362, 3344, 12512, 11, 558, 30, 407, 11, 498, 286, 1949, 264, 4846, 51476], "temperature": 0.0, "avg_logprob": -0.0825315878643253, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0019657756201922894}, {"id": 700, "seek": 406436, "start": 4086.6, "end": 4091.56, "text": " as a multi-set, right? I completely disregard the structure of the graph itself, right? So,", "tokens": [51476, 382, 257, 4825, 12, 3854, 11, 558, 30, 286, 2584, 44493, 264, 3877, 295, 264, 4295, 2564, 11, 558, 30, 407, 11, 51724], "temperature": 0.0, "avg_logprob": -0.0825315878643253, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0019657756201922894}, {"id": 701, "seek": 409156, "start": 4091.64, "end": 4100.28, "text": " what the node sees is just a soup of the neighbor features. So, if I use a maximum aggregator,", "tokens": [50368, 437, 264, 9984, 8194, 307, 445, 257, 7884, 295, 264, 5987, 4122, 13, 407, 11, 498, 286, 764, 257, 6674, 16743, 1639, 11, 50800], "temperature": 0.0, "avg_logprob": -0.08526474942443191, "compression_ratio": 1.8277511961722488, "no_speech_prob": 0.0040887827053666115}, {"id": 702, "seek": 409156, "start": 4100.28, "end": 4104.5199999999995, "text": " I cannot distinguish between these and these, right? Because for the maximum, it doesn't matter", "tokens": [50800, 286, 2644, 20206, 1296, 613, 293, 613, 11, 558, 30, 1436, 337, 264, 6674, 11, 309, 1177, 380, 1871, 51012], "temperature": 0.0, "avg_logprob": -0.08526474942443191, "compression_ratio": 1.8277511961722488, "no_speech_prob": 0.0040887827053666115}, {"id": 703, "seek": 409156, "start": 4104.5199999999995, "end": 4110.04, "text": " how many times each of these features appears, right? If I use a mean, then I cannot distinguish", "tokens": [51012, 577, 867, 1413, 1184, 295, 613, 4122, 7038, 11, 558, 30, 759, 286, 764, 257, 914, 11, 550, 286, 2644, 20206, 51288], "temperature": 0.0, "avg_logprob": -0.08526474942443191, "compression_ratio": 1.8277511961722488, "no_speech_prob": 0.0040887827053666115}, {"id": 704, "seek": 409156, "start": 4110.04, "end": 4116.5199999999995, "text": " between these and these, right? I can multiply the neighbors by some constant factor. The sum,", "tokens": [51288, 1296, 613, 293, 613, 11, 558, 30, 286, 393, 12972, 264, 12512, 538, 512, 5754, 5952, 13, 440, 2408, 11, 51612], "temperature": 0.0, "avg_logprob": -0.08526474942443191, "compression_ratio": 1.8277511961722488, "no_speech_prob": 0.0040887827053666115}, {"id": 705, "seek": 411652, "start": 4116.52, "end": 4122.68, "text": " though, allows to distinguish between all of them, right? So, you can think that maximum gives", "tokens": [50364, 1673, 11, 4045, 281, 20206, 1296, 439, 295, 552, 11, 558, 30, 407, 11, 291, 393, 519, 300, 6674, 2709, 50672], "temperature": 0.0, "avg_logprob": -0.096918395396029, "compression_ratio": 1.7, "no_speech_prob": 0.003668704070150852}, {"id": 706, "seek": 411652, "start": 4122.68, "end": 4129.400000000001, "text": " a kind of skeleton of the set and the mean gives you the distribution, but the sum is strictly more", "tokens": [50672, 257, 733, 295, 25204, 295, 264, 992, 293, 264, 914, 2709, 291, 264, 7316, 11, 457, 264, 2408, 307, 20792, 544, 51008], "temperature": 0.0, "avg_logprob": -0.096918395396029, "compression_ratio": 1.7, "no_speech_prob": 0.003668704070150852}, {"id": 707, "seek": 411652, "start": 4129.400000000001, "end": 4138.040000000001, "text": " expressive, right? And here's an example of structures that max or max and mean would fail", "tokens": [51008, 40189, 11, 558, 30, 400, 510, 311, 364, 1365, 295, 9227, 300, 11469, 420, 11469, 293, 914, 576, 3061, 51440], "temperature": 0.0, "avg_logprob": -0.096918395396029, "compression_ratio": 1.7, "no_speech_prob": 0.003668704070150852}, {"id": 708, "seek": 411652, "start": 4138.040000000001, "end": 4143.400000000001, "text": " to distinguish. And indeed, sum appears the most expressive one. And if you assume that,", "tokens": [51440, 281, 20206, 13, 400, 6451, 11, 2408, 7038, 264, 881, 40189, 472, 13, 400, 498, 291, 6552, 300, 11, 51708], "temperature": 0.0, "avg_logprob": -0.096918395396029, "compression_ratio": 1.7, "no_speech_prob": 0.003668704070150852}, {"id": 709, "seek": 414340, "start": 4143.96, "end": 4149.799999999999, "text": " so the theorem about the equivalence between WL and message passing states that if you assume", "tokens": [50392, 370, 264, 20904, 466, 264, 9052, 655, 1296, 343, 43, 293, 3636, 8437, 4368, 300, 498, 291, 6552, 50684], "temperature": 0.0, "avg_logprob": -0.16795563167995878, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0052610826678574085}, {"id": 710, "seek": 414340, "start": 4149.799999999999, "end": 4156.04, "text": " that the node features come from a countable universe, then if you have an MPNN with an", "tokens": [50684, 300, 264, 9984, 4122, 808, 490, 257, 1207, 712, 6445, 11, 550, 498, 291, 362, 364, 14146, 45, 45, 365, 364, 50996], "temperature": 0.0, "avg_logprob": -0.16795563167995878, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0052610826678574085}, {"id": 711, "seek": 414340, "start": 4156.04, "end": 4161.799999999999, "text": " injective aggregator, call it square, an update function phi and graph-wise readout function", "tokens": [50996, 10711, 488, 16743, 1639, 11, 818, 309, 3732, 11, 364, 5623, 2445, 13107, 293, 4295, 12, 3711, 1401, 346, 2445, 51284], "temperature": 0.0, "avg_logprob": -0.16795563167995878, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0052610826678574085}, {"id": 712, "seek": 414340, "start": 4162.599999999999, "end": 4167.719999999999, "text": " is as powerful as the vice-versa element set, right? And the assumption here is of discrete", "tokens": [51324, 307, 382, 4005, 382, 264, 11964, 12, 840, 64, 4478, 992, 11, 558, 30, 400, 264, 15302, 510, 307, 295, 27706, 51580], "temperature": 0.0, "avg_logprob": -0.16795563167995878, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0052610826678574085}, {"id": 713, "seek": 416772, "start": 4167.72, "end": 4174.12, "text": " countable features, which is not always the case in practice. And then the reason architecture", "tokens": [50364, 1207, 712, 4122, 11, 597, 307, 406, 1009, 264, 1389, 294, 3124, 13, 400, 550, 264, 1778, 9482, 50684], "temperature": 0.0, "avg_logprob": -0.14628089634718094, "compression_ratio": 1.7316176470588236, "no_speech_prob": 0.0023342708591371775}, {"id": 714, "seek": 416772, "start": 4174.12, "end": 4178.6, "text": " that actually implements that is equivalent theoretically to the vice-versa element, which is", "tokens": [50684, 300, 767, 704, 17988, 300, 307, 10344, 29400, 281, 264, 11964, 12, 840, 64, 4478, 11, 597, 307, 50908], "temperature": 0.0, "avg_logprob": -0.14628089634718094, "compression_ratio": 1.7316176470588236, "no_speech_prob": 0.0023342708591371775}, {"id": 715, "seek": 416772, "start": 4178.6, "end": 4185.240000000001, "text": " the graph-wise morphism network or GIN. So, basically, it uses a sum aggregation. So,", "tokens": [50908, 264, 4295, 12, 3711, 25778, 1434, 3209, 420, 460, 1464, 13, 407, 11, 1936, 11, 309, 4960, 257, 2408, 16743, 399, 13, 407, 11, 51240], "temperature": 0.0, "avg_logprob": -0.14628089634718094, "compression_ratio": 1.7316176470588236, "no_speech_prob": 0.0023342708591371775}, {"id": 716, "seek": 416772, "start": 4185.240000000001, "end": 4190.280000000001, "text": " the epsilon here is just theoretical thing. So, there exists infinitely many constants epsilon", "tokens": [51240, 264, 17889, 510, 307, 445, 20864, 551, 13, 407, 11, 456, 8198, 36227, 867, 35870, 17889, 51492], "temperature": 0.0, "avg_logprob": -0.14628089634718094, "compression_ratio": 1.7316176470588236, "no_speech_prob": 0.0023342708591371775}, {"id": 717, "seek": 416772, "start": 4190.280000000001, "end": 4197.64, "text": " that you can use here. So, we know that, basically, there exists at least a choice, right? Within the", "tokens": [51492, 300, 291, 393, 764, 510, 13, 407, 11, 321, 458, 300, 11, 1936, 11, 456, 8198, 412, 1935, 257, 3922, 11, 558, 30, 15996, 264, 51860], "temperature": 0.0, "avg_logprob": -0.14628089634718094, "compression_ratio": 1.7316176470588236, "no_speech_prob": 0.0023342708591371775}, {"id": 718, "seek": 419764, "start": 4198.52, "end": 4203.400000000001, "text": " all possible message-passing neural networks that makes it as expressive as WL. But, of course,", "tokens": [50408, 439, 1944, 3636, 12, 9216, 278, 18161, 9590, 300, 1669, 309, 382, 40189, 382, 343, 43, 13, 583, 11, 295, 1164, 11, 50652], "temperature": 0.0, "avg_logprob": -0.09605671187578621, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.0011053677881136537}, {"id": 719, "seek": 419764, "start": 4203.400000000001, "end": 4208.92, "text": " we are interested in more expressive architectures, right? Can you do better than WL? And here, again,", "tokens": [50652, 321, 366, 3102, 294, 544, 40189, 6331, 1303, 11, 558, 30, 1664, 291, 360, 1101, 813, 343, 43, 30, 400, 510, 11, 797, 11, 50928], "temperature": 0.0, "avg_logprob": -0.09605671187578621, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.0011053677881136537}, {"id": 720, "seek": 419764, "start": 4208.92, "end": 4213.56, "text": " there is an entire universe of different architectures. So, some of them actually go beyond", "tokens": [50928, 456, 307, 364, 2302, 6445, 295, 819, 6331, 1303, 13, 407, 11, 512, 295, 552, 767, 352, 4399, 51160], "temperature": 0.0, "avg_logprob": -0.09605671187578621, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.0011053677881136537}, {"id": 721, "seek": 419764, "start": 4214.200000000001, "end": 4218.4400000000005, "text": " message-passing, at least in the traditional sense. And, roughly, you can distinguish between", "tokens": [51192, 3636, 12, 9216, 278, 11, 412, 1935, 294, 264, 5164, 2020, 13, 400, 11, 9810, 11, 291, 393, 20206, 1296, 51404], "temperature": 0.0, "avg_logprob": -0.09605671187578621, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.0011053677881136537}, {"id": 722, "seek": 419764, "start": 4218.4400000000005, "end": 4223.56, "text": " four different categories of approaches. So, it's either a higher-order WL test. It's not", "tokens": [51404, 1451, 819, 10479, 295, 11587, 13, 407, 11, 309, 311, 2139, 257, 2946, 12, 4687, 343, 43, 1500, 13, 467, 311, 406, 51660], "temperature": 0.0, "avg_logprob": -0.09605671187578621, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.0011053677881136537}, {"id": 723, "seek": 422356, "start": 4223.56, "end": 4227.96, "text": " a single test. It's a hierarchy of tests. The use of positional instruction and coding,", "tokens": [50364, 257, 2167, 1500, 13, 467, 311, 257, 22333, 295, 6921, 13, 440, 764, 295, 2535, 304, 10951, 293, 17720, 11, 50584], "temperature": 0.0, "avg_logprob": -0.15285967872256323, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.002821930916979909}, {"id": 724, "seek": 422356, "start": 4227.96, "end": 4233.88, "text": " so that's how transformers work. Subgraph GNNs and then topological message-passing, right?", "tokens": [50584, 370, 300, 311, 577, 4088, 433, 589, 13, 8511, 34091, 460, 45, 45, 82, 293, 550, 1192, 4383, 3636, 12, 9216, 278, 11, 558, 30, 50880], "temperature": 0.0, "avg_logprob": -0.15285967872256323, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.002821930916979909}, {"id": 725, "seek": 422356, "start": 4234.6, "end": 4239.080000000001, "text": " So, let's talk briefly about all of them and then when do we need to do the break?", "tokens": [50916, 407, 11, 718, 311, 751, 10515, 466, 439, 295, 552, 293, 550, 562, 360, 321, 643, 281, 360, 264, 1821, 30, 51140], "temperature": 0.0, "avg_logprob": -0.15285967872256323, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.002821930916979909}, {"id": 726, "seek": 422356, "start": 4240.6, "end": 4241.0, "text": " Sorry?", "tokens": [51216, 4919, 30, 51236], "temperature": 0.0, "avg_logprob": -0.15285967872256323, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.002821930916979909}, {"id": 727, "seek": 422356, "start": 4243.88, "end": 4250.6, "text": " So, let's maybe 20 minutes and then we do the break. So, the first class of higher-order WL", "tokens": [51380, 407, 11, 718, 311, 1310, 945, 2077, 293, 550, 321, 360, 264, 1821, 13, 407, 11, 264, 700, 1508, 295, 2946, 12, 4687, 343, 43, 51716], "temperature": 0.0, "avg_logprob": -0.15285967872256323, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.002821930916979909}, {"id": 728, "seek": 425060, "start": 4250.6, "end": 4255.08, "text": " tests, as I mentioned, so WL test is just one algorithm that was initially developed", "tokens": [50364, 6921, 11, 382, 286, 2835, 11, 370, 343, 43, 1500, 307, 445, 472, 9284, 300, 390, 9105, 4743, 50588], "temperature": 0.0, "avg_logprob": -0.12806863200907803, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.00394004862755537}, {"id": 729, "seek": 425060, "start": 4255.88, "end": 4263.8, "text": " and then extended by Babi and collaborators, actually independently also by other people.", "tokens": [50628, 293, 550, 10913, 538, 15820, 72, 293, 39789, 11, 767, 21761, 611, 538, 661, 561, 13, 51024], "temperature": 0.0, "avg_logprob": -0.12806863200907803, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.00394004862755537}, {"id": 730, "seek": 425060, "start": 4264.6, "end": 4268.52, "text": " So, one of them was Eric Lander, who is mostly known as computational biologist,", "tokens": [51064, 407, 11, 472, 295, 552, 390, 9336, 441, 4483, 11, 567, 307, 5240, 2570, 382, 28270, 3228, 9201, 11, 51260], "temperature": 0.0, "avg_logprob": -0.12806863200907803, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.00394004862755537}, {"id": 731, "seek": 425060, "start": 4268.52, "end": 4274.4400000000005, "text": " but he started as a mathematician. And, basically, this is an increasingly", "tokens": [51260, 457, 415, 1409, 382, 257, 48281, 13, 400, 11, 1936, 11, 341, 307, 364, 12980, 51556], "temperature": 0.0, "avg_logprob": -0.12806863200907803, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.00394004862755537}, {"id": 732, "seek": 425060, "start": 4274.4400000000005, "end": 4278.68, "text": " more expressive hierarchy of tests. Instead of doing node refinement, they look at tuples", "tokens": [51556, 544, 40189, 22333, 295, 6921, 13, 7156, 295, 884, 9984, 1895, 30229, 11, 436, 574, 412, 2604, 2622, 51768], "temperature": 0.0, "avg_logprob": -0.12806863200907803, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.00394004862755537}, {"id": 733, "seek": 427868, "start": 4279.240000000001, "end": 4284.52, "text": " of nodes. So, it's obviously computationally more expensive and there is always, you can find a", "tokens": [50392, 295, 13891, 13, 407, 11, 309, 311, 2745, 24903, 379, 544, 5124, 293, 456, 307, 1009, 11, 291, 393, 915, 257, 50656], "temperature": 0.0, "avg_logprob": -0.16339918362197056, "compression_ratio": 1.5625, "no_speech_prob": 0.0026917376089841127}, {"id": 734, "seek": 427868, "start": 4284.52, "end": 4289.88, "text": " family of graphs that these algorithms cannot distinguish. So, like strongly regular graphs for", "tokens": [50656, 1605, 295, 24877, 300, 613, 14642, 2644, 20206, 13, 407, 11, 411, 10613, 3890, 24877, 337, 50924], "temperature": 0.0, "avg_logprob": -0.16339918362197056, "compression_ratio": 1.5625, "no_speech_prob": 0.0026917376089841127}, {"id": 735, "seek": 427868, "start": 4290.6, "end": 4298.76, "text": " 2WL or 3WL tests and what is called CFI graphs for general KWL. I should also say that this", "tokens": [50960, 568, 54, 43, 420, 805, 54, 43, 6921, 293, 437, 307, 1219, 383, 38568, 24877, 337, 2674, 591, 54, 43, 13, 286, 820, 611, 584, 300, 341, 51368], "temperature": 0.0, "avg_logprob": -0.16339918362197056, "compression_ratio": 1.5625, "no_speech_prob": 0.0026917376089841127}, {"id": 736, "seek": 427868, "start": 4298.76, "end": 4304.12, "text": " terminology of KWL is confusing because they're what is called folklore WL tests versus the", "tokens": [51368, 27575, 295, 591, 54, 43, 307, 13181, 570, 436, 434, 437, 307, 1219, 49195, 343, 43, 6921, 5717, 264, 51636], "temperature": 0.0, "avg_logprob": -0.16339918362197056, "compression_ratio": 1.5625, "no_speech_prob": 0.0026917376089841127}, {"id": 737, "seek": 430412, "start": 4304.12, "end": 4309.96, "text": " classical WL tests that are slightly different. So, but overall the hierarchy, right, up to", "tokens": [50364, 13735, 343, 43, 6921, 300, 366, 4748, 819, 13, 407, 11, 457, 4787, 264, 22333, 11, 558, 11, 493, 281, 50656], "temperature": 0.0, "avg_logprob": -0.19428225663992074, "compression_ratio": 1.5542168674698795, "no_speech_prob": 0.0037207803688943386}, {"id": 738, "seek": 430412, "start": 4309.96, "end": 4317.8, "text": " notation is the same. So, we know that message-passing GNNs are equivalent to the standard WL. You can", "tokens": [50656, 24657, 307, 264, 912, 13, 407, 11, 321, 458, 300, 3636, 12, 9216, 278, 46411, 45, 82, 366, 10344, 281, 264, 3832, 343, 43, 13, 509, 393, 51048], "temperature": 0.0, "avg_logprob": -0.19428225663992074, "compression_ratio": 1.5542168674698795, "no_speech_prob": 0.0037207803688943386}, {"id": 739, "seek": 430412, "start": 4317.8, "end": 4326.28, "text": " also design just replicating in the neural network architecture the KWL tests higher-order KGNN.", "tokens": [51048, 611, 1715, 445, 3248, 30541, 294, 264, 18161, 3209, 9482, 264, 591, 54, 43, 6921, 2946, 12, 4687, 591, 38, 45, 45, 13, 51472], "temperature": 0.0, "avg_logprob": -0.19428225663992074, "compression_ratio": 1.5542168674698795, "no_speech_prob": 0.0037207803688943386}, {"id": 740, "seek": 430412, "start": 4327.0, "end": 4332.28, "text": " So, this is what Hageim Aron did in his works. And then you can also have some other algorithms", "tokens": [51508, 407, 11, 341, 307, 437, 389, 609, 332, 1587, 266, 630, 294, 702, 1985, 13, 400, 550, 291, 393, 611, 362, 512, 661, 14642, 51772], "temperature": 0.0, "avg_logprob": -0.19428225663992074, "compression_ratio": 1.5542168674698795, "no_speech_prob": 0.0037207803688943386}, {"id": 741, "seek": 433228, "start": 4332.28, "end": 4336.44, "text": " we'll talk about in a second that sits somewhere between. They don't exactly follow the", "tokens": [50364, 321, 603, 751, 466, 294, 257, 1150, 300, 12696, 4079, 1296, 13, 814, 500, 380, 2293, 1524, 264, 50572], "temperature": 0.0, "avg_logprob": -0.10047386373792376, "compression_ratio": 1.6501766784452296, "no_speech_prob": 0.003421561326831579}, {"id": 742, "seek": 433228, "start": 4337.4, "end": 4342.84, "text": " hierarchy of the WL tests. So, the second approach is positional encoding. And, again,", "tokens": [50620, 22333, 295, 264, 343, 43, 6921, 13, 407, 11, 264, 1150, 3109, 307, 2535, 304, 43430, 13, 400, 11, 797, 11, 50892], "temperature": 0.0, "avg_logprob": -0.10047386373792376, "compression_ratio": 1.6501766784452296, "no_speech_prob": 0.003421561326831579}, {"id": 743, "seek": 433228, "start": 4342.84, "end": 4347.96, "text": " I remind you of this example of two graphs that cannot be distinguished by the WL tests, but", "tokens": [50892, 286, 4160, 291, 295, 341, 1365, 295, 732, 24877, 300, 2644, 312, 21702, 538, 264, 343, 43, 6921, 11, 457, 51148], "temperature": 0.0, "avg_logprob": -0.10047386373792376, "compression_ratio": 1.6501766784452296, "no_speech_prob": 0.003421561326831579}, {"id": 744, "seek": 433228, "start": 4347.96, "end": 4354.84, "text": " imagine that I could now attach some features to the nodes of the graph, right? And this could be", "tokens": [51148, 3811, 300, 286, 727, 586, 5085, 512, 4122, 281, 264, 13891, 295, 264, 4295, 11, 558, 30, 400, 341, 727, 312, 51492], "temperature": 0.0, "avg_logprob": -0.10047386373792376, "compression_ratio": 1.6501766784452296, "no_speech_prob": 0.003421561326831579}, {"id": 745, "seek": 433228, "start": 4354.84, "end": 4361.4, "text": " even something as simple as random features. You see that now, because I have some extra information,", "tokens": [51492, 754, 746, 382, 2199, 382, 4974, 4122, 13, 509, 536, 300, 586, 11, 570, 286, 362, 512, 2857, 1589, 11, 51820], "temperature": 0.0, "avg_logprob": -0.10047386373792376, "compression_ratio": 1.6501766784452296, "no_speech_prob": 0.003421561326831579}, {"id": 746, "seek": 436140, "start": 4361.4, "end": 4367.0, "text": " I can distinguish between these cases, right? So, if I look at the leaves, for example, of", "tokens": [50364, 286, 393, 20206, 1296, 613, 3331, 11, 558, 30, 407, 11, 498, 286, 574, 412, 264, 5510, 11, 337, 1365, 11, 295, 50644], "temperature": 0.0, "avg_logprob": -0.09920818328857423, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0030772113241255283}, {"id": 747, "seek": 436140, "start": 4369.16, "end": 4375.08, "text": " this tree, right, I can ask, for example, whether the root appears among the leaves or not, right?", "tokens": [50752, 341, 4230, 11, 558, 11, 286, 393, 1029, 11, 337, 1365, 11, 1968, 264, 5593, 7038, 3654, 264, 5510, 420, 406, 11, 558, 30, 51048], "temperature": 0.0, "avg_logprob": -0.09920818328857423, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0030772113241255283}, {"id": 748, "seek": 436140, "start": 4375.08, "end": 4380.679999999999, "text": " And here it does appear and here it doesn't, right? So, they're clearly different, right? I would be", "tokens": [51048, 400, 510, 309, 775, 4204, 293, 510, 309, 1177, 380, 11, 558, 30, 407, 11, 436, 434, 4448, 819, 11, 558, 30, 286, 576, 312, 51328], "temperature": 0.0, "avg_logprob": -0.09920818328857423, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0030772113241255283}, {"id": 749, "seek": 436140, "start": 4380.679999999999, "end": 4385.719999999999, "text": " able to distinguish between them, right? So, the covering of the nodes removes at least to some", "tokens": [51328, 1075, 281, 20206, 1296, 552, 11, 558, 30, 407, 11, 264, 10322, 295, 264, 13891, 30445, 412, 1935, 281, 512, 51580], "temperature": 0.0, "avg_logprob": -0.09920818328857423, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0030772113241255283}, {"id": 750, "seek": 436140, "start": 4385.719999999999, "end": 4390.92, "text": " extent the ambiguity. Now, of course, if I use random features, then the question is how can I", "tokens": [51580, 8396, 264, 46519, 13, 823, 11, 295, 1164, 11, 498, 286, 764, 4974, 4122, 11, 550, 264, 1168, 307, 577, 393, 286, 51840], "temperature": 0.0, "avg_logprob": -0.09920818328857423, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0030772113241255283}, {"id": 751, "seek": 439092, "start": 4390.92, "end": 4397.16, "text": " reproduce them on a different graph? So, these type of approaches are equivalent only in expectation.", "tokens": [50364, 29501, 552, 322, 257, 819, 4295, 30, 407, 11, 613, 2010, 295, 11587, 366, 10344, 787, 294, 14334, 13, 50676], "temperature": 0.0, "avg_logprob": -0.13834555943806967, "compression_ratio": 1.8949579831932772, "no_speech_prob": 0.002634837059304118}, {"id": 752, "seek": 439092, "start": 4397.16, "end": 4401.56, "text": " But there are other methods that can do better. And these are structural encoding.", "tokens": [50676, 583, 456, 366, 661, 7150, 300, 393, 360, 1101, 13, 400, 613, 366, 15067, 43430, 13, 50896], "temperature": 0.0, "avg_logprob": -0.13834555943806967, "compression_ratio": 1.8949579831932772, "no_speech_prob": 0.002634837059304118}, {"id": 753, "seek": 439092, "start": 4402.4400000000005, "end": 4408.6, "text": " So, the idea of structural encoding, you have some substructures, right? So, we have a bank of", "tokens": [50940, 407, 11, 264, 1558, 295, 15067, 43430, 11, 291, 362, 512, 4594, 44513, 11, 558, 30, 407, 11, 321, 362, 257, 3765, 295, 51248], "temperature": 0.0, "avg_logprob": -0.13834555943806967, "compression_ratio": 1.8949579831932772, "no_speech_prob": 0.002634837059304118}, {"id": 754, "seek": 439092, "start": 4408.6, "end": 4414.4400000000005, "text": " substructures that call it H. And you can count the substructures, the occurrence of substructure", "tokens": [51248, 4594, 44513, 300, 818, 309, 389, 13, 400, 291, 393, 1207, 264, 4594, 44513, 11, 264, 36122, 295, 4594, 2885, 51540], "temperature": 0.0, "avg_logprob": -0.13834555943806967, "compression_ratio": 1.8949579831932772, "no_speech_prob": 0.002634837059304118}, {"id": 755, "seek": 439092, "start": 4414.4400000000005, "end": 4417.8, "text": " for every node or for every H, right? And there are two ways that you can", "tokens": [51540, 337, 633, 9984, 420, 337, 633, 389, 11, 558, 30, 400, 456, 366, 732, 2098, 300, 291, 393, 51708], "temperature": 0.0, "avg_logprob": -0.13834555943806967, "compression_ratio": 1.8949579831932772, "no_speech_prob": 0.002634837059304118}, {"id": 756, "seek": 441780, "start": 4417.8, "end": 4422.2, "text": " consider subgraphs, whether what is called a subgraph or any induced subgraph. It doesn't", "tokens": [50364, 1949, 1422, 34091, 82, 11, 1968, 437, 307, 1219, 257, 1422, 34091, 420, 604, 33991, 1422, 34091, 13, 467, 1177, 380, 50584], "temperature": 0.0, "avg_logprob": -0.16524853092609065, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.006060176994651556}, {"id": 757, "seek": 441780, "start": 4422.2, "end": 4427.08, "text": " really matter, right? So, the two ways that are slightly distinct. And for example, in these two", "tokens": [50584, 534, 1871, 11, 558, 30, 407, 11, 264, 732, 2098, 300, 366, 4748, 10644, 13, 400, 337, 1365, 11, 294, 613, 732, 50828], "temperature": 0.0, "avg_logprob": -0.16524853092609065, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.006060176994651556}, {"id": 758, "seek": 441780, "start": 4427.08, "end": 4433.24, "text": " graphs, if this is my bank of substructures, let's say cycles of size 6 and 5. So, in this molecule,", "tokens": [50828, 24877, 11, 498, 341, 307, 452, 3765, 295, 4594, 44513, 11, 718, 311, 584, 17796, 295, 2744, 1386, 293, 1025, 13, 407, 11, 294, 341, 15582, 11, 51136], "temperature": 0.0, "avg_logprob": -0.16524853092609065, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.006060176994651556}, {"id": 759, "seek": 441780, "start": 4434.6, "end": 4442.12, "text": " at every edge or at every node, I will count once the six cycle substructure and these nodes,", "tokens": [51204, 412, 633, 4691, 420, 412, 633, 9984, 11, 286, 486, 1207, 1564, 264, 2309, 6586, 4594, 2885, 293, 613, 13891, 11, 51580], "temperature": 0.0, "avg_logprob": -0.16524853092609065, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.006060176994651556}, {"id": 760, "seek": 444212, "start": 4442.12, "end": 4447.8, "text": " I will count twice, right? Because this node participates in both structure on the left and", "tokens": [50364, 286, 486, 1207, 6091, 11, 558, 30, 1436, 341, 9984, 3421, 1024, 294, 1293, 3877, 322, 264, 1411, 293, 50648], "temperature": 0.0, "avg_logprob": -0.11355845425107064, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.005399747751653194}, {"id": 761, "seek": 444212, "start": 4447.8, "end": 4454.68, "text": " right. But the five cycle substructure doesn't appear here versus it appears here, right?", "tokens": [50648, 558, 13, 583, 264, 1732, 6586, 4594, 2885, 1177, 380, 4204, 510, 5717, 309, 7038, 510, 11, 558, 30, 50992], "temperature": 0.0, "avg_logprob": -0.11355845425107064, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.005399747751653194}, {"id": 762, "seek": 444212, "start": 4455.32, "end": 4459.64, "text": " So, with this encoding, now I have some, these additional features that I can attach to every", "tokens": [51024, 407, 11, 365, 341, 43430, 11, 586, 286, 362, 512, 11, 613, 4497, 4122, 300, 286, 393, 5085, 281, 633, 51240], "temperature": 0.0, "avg_logprob": -0.11355845425107064, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.005399747751653194}, {"id": 763, "seek": 444212, "start": 4459.64, "end": 4463.88, "text": " node or to every edge of the graph. And I can use them in standard message passing. And this would", "tokens": [51240, 9984, 420, 281, 633, 4691, 295, 264, 4295, 13, 400, 286, 393, 764, 552, 294, 3832, 3636, 8437, 13, 400, 341, 576, 51452], "temperature": 0.0, "avg_logprob": -0.11355845425107064, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.005399747751653194}, {"id": 764, "seek": 444212, "start": 4463.88, "end": 4471.16, "text": " allow me to discriminate between these graphs. And the complexity of this method is basically,", "tokens": [51452, 2089, 385, 281, 47833, 1296, 613, 24877, 13, 400, 264, 14024, 295, 341, 3170, 307, 1936, 11, 51816], "temperature": 0.0, "avg_logprob": -0.11355845425107064, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.005399747751653194}, {"id": 765, "seek": 447116, "start": 4471.24, "end": 4476.2, "text": " it's all hidden in pre-computation, so the counting of substructures. So, in the worst case, it's", "tokens": [50368, 309, 311, 439, 7633, 294, 659, 12, 1112, 2582, 399, 11, 370, 264, 13251, 295, 4594, 44513, 13, 407, 11, 294, 264, 5855, 1389, 11, 309, 311, 50616], "temperature": 0.0, "avg_logprob": -0.15188832436838456, "compression_ratio": 1.7304964539007093, "no_speech_prob": 0.0009334077476523817}, {"id": 766, "seek": 447116, "start": 4476.2, "end": 4481.0, "text": " order of n to the power k, k is the size of the substructure, so it could be large. But in practice,", "tokens": [50616, 1668, 295, 297, 281, 264, 1347, 350, 11, 350, 307, 264, 2744, 295, 264, 4594, 2885, 11, 370, 309, 727, 312, 2416, 13, 583, 294, 3124, 11, 50856], "temperature": 0.0, "avg_logprob": -0.15188832436838456, "compression_ratio": 1.7304964539007093, "no_speech_prob": 0.0009334077476523817}, {"id": 767, "seek": 447116, "start": 4481.0, "end": 4487.32, "text": " for structures, more friendly structures like triangles, there exist more efficient algorithms.", "tokens": [50856, 337, 9227, 11, 544, 9208, 9227, 411, 29896, 11, 456, 2514, 544, 7148, 14642, 13, 51172], "temperature": 0.0, "avg_logprob": -0.15188832436838456, "compression_ratio": 1.7304964539007093, "no_speech_prob": 0.0009334077476523817}, {"id": 768, "seek": 447116, "start": 4487.32, "end": 4493.16, "text": " So, in practice, it can be way better. The algorithm itself, and especially the training part,", "tokens": [51172, 407, 11, 294, 3124, 11, 309, 393, 312, 636, 1101, 13, 440, 9284, 2564, 11, 293, 2318, 264, 3097, 644, 11, 51464], "temperature": 0.0, "avg_logprob": -0.15188832436838456, "compression_ratio": 1.7304964539007093, "no_speech_prob": 0.0009334077476523817}, {"id": 769, "seek": 447116, "start": 4493.16, "end": 4497.639999999999, "text": " which is typically more expensive, is standard MPNN. So, it has linear complexity in the number of", "tokens": [51464, 597, 307, 5850, 544, 5124, 11, 307, 3832, 14146, 45, 45, 13, 407, 11, 309, 575, 8213, 14024, 294, 264, 1230, 295, 51688], "temperature": 0.0, "avg_logprob": -0.15188832436838456, "compression_ratio": 1.7304964539007093, "no_speech_prob": 0.0009334077476523817}, {"id": 770, "seek": 449764, "start": 4497.64, "end": 4503.56, "text": " edges, or roughly order of n if the graph is sparse. And the theoretical result is that these", "tokens": [50364, 8819, 11, 420, 9810, 1668, 295, 297, 498, 264, 4295, 307, 637, 11668, 13, 400, 264, 20864, 1874, 307, 300, 613, 50660], "temperature": 0.0, "avg_logprob": -0.15918602340522853, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.001457856153137982}, {"id": 771, "seek": 449764, "start": 4503.56, "end": 4508.280000000001, "text": " kind of architecture that we call GSN, graph substructure network, is strictly more expressive", "tokens": [50660, 733, 295, 9482, 300, 321, 818, 460, 32481, 11, 4295, 4594, 2885, 3209, 11, 307, 20792, 544, 40189, 50896], "temperature": 0.0, "avg_logprob": -0.15918602340522853, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.001457856153137982}, {"id": 772, "seek": 449764, "start": 4508.280000000001, "end": 4514.12, "text": " than WL on certain assumptions on these substructures, right? So, it should not be a", "tokens": [50896, 813, 343, 43, 322, 1629, 17695, 322, 613, 4594, 44513, 11, 558, 30, 407, 11, 309, 820, 406, 312, 257, 51188], "temperature": 0.0, "avg_logprob": -0.15918602340522853, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.001457856153137982}, {"id": 773, "seek": 449764, "start": 4514.12, "end": 4523.160000000001, "text": " star graph or it should be a structure of size, bigger than three. And basically,", "tokens": [51188, 3543, 4295, 420, 309, 820, 312, 257, 3877, 295, 2744, 11, 3801, 813, 1045, 13, 400, 1936, 11, 51640], "temperature": 0.0, "avg_logprob": -0.15918602340522853, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.001457856153137982}, {"id": 774, "seek": 452316, "start": 4523.16, "end": 4528.2, "text": " we can formulate it is that GSN is not less expressive than 3WL. You can also do it for", "tokens": [50364, 321, 393, 47881, 309, 307, 300, 460, 32481, 307, 406, 1570, 40189, 813, 805, 54, 43, 13, 509, 393, 611, 360, 309, 337, 50616], "temperature": 0.0, "avg_logprob": -0.14354380439309514, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.008710513822734356}, {"id": 775, "seek": 452316, "start": 4528.2, "end": 4534.68, "text": " different KWLs. Basically, you do it by counter examples. So, you can design a substructure", "tokens": [50616, 819, 591, 54, 43, 82, 13, 8537, 11, 291, 360, 309, 538, 5682, 5110, 13, 407, 11, 291, 393, 1715, 257, 4594, 2885, 50940], "temperature": 0.0, "avg_logprob": -0.14354380439309514, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.008710513822734356}, {"id": 776, "seek": 452316, "start": 4534.68, "end": 4541.639999999999, "text": " that the standard WL tests cannot count, whether it will be clicks of certain type or other things.", "tokens": [50940, 300, 264, 3832, 343, 43, 6921, 2644, 1207, 11, 1968, 309, 486, 312, 18521, 295, 1629, 2010, 420, 661, 721, 13, 51288], "temperature": 0.0, "avg_logprob": -0.14354380439309514, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.008710513822734356}, {"id": 777, "seek": 452316, "start": 4542.5199999999995, "end": 4547.08, "text": " Right? And the proof by example is something like this. So, this is a stronger regular graph. It", "tokens": [51332, 1779, 30, 400, 264, 8177, 538, 1365, 307, 746, 411, 341, 13, 407, 11, 341, 307, 257, 7249, 3890, 4295, 13, 467, 51560], "temperature": 0.0, "avg_logprob": -0.14354380439309514, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.008710513822734356}, {"id": 778, "seek": 452316, "start": 4547.08, "end": 4552.92, "text": " cannot be distinguished by 3WL, but this graph contains four clicks and this doesn't.", "tokens": [51560, 2644, 312, 21702, 538, 805, 54, 43, 11, 457, 341, 4295, 8306, 1451, 18521, 293, 341, 1177, 380, 13, 51852], "temperature": 0.0, "avg_logprob": -0.14354380439309514, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.008710513822734356}, {"id": 779, "seek": 455316, "start": 4553.4, "end": 4559.4, "text": " So, if I count four clicks, I would be able to distinguish between these graphs. So, in a sense,", "tokens": [50376, 407, 11, 498, 286, 1207, 1451, 18521, 11, 286, 576, 312, 1075, 281, 20206, 1296, 613, 24877, 13, 407, 11, 294, 257, 2020, 11, 50676], "temperature": 0.0, "avg_logprob": -0.1327519510306564, "compression_ratio": 1.583673469387755, "no_speech_prob": 0.0014637024141848087}, {"id": 780, "seek": 455316, "start": 4559.4, "end": 4566.28, "text": " it's a kind of cheating. So, I'm not following really the KWL hierarchy. So, this is an example of", "tokens": [50676, 309, 311, 257, 733, 295, 18309, 13, 407, 11, 286, 478, 406, 3480, 534, 264, 591, 54, 43, 22333, 13, 407, 11, 341, 307, 364, 1365, 295, 51020], "temperature": 0.0, "avg_logprob": -0.1327519510306564, "compression_ratio": 1.583673469387755, "no_speech_prob": 0.0014637024141848087}, {"id": 781, "seek": 455316, "start": 4566.28, "end": 4571.88, "text": " different structures I can count, triangles and clicks. But then, basically, the expressive", "tokens": [51020, 819, 9227, 286, 393, 1207, 11, 29896, 293, 18521, 13, 583, 550, 11, 1936, 11, 264, 40189, 51300], "temperature": 0.0, "avg_logprob": -0.1327519510306564, "compression_ratio": 1.583673469387755, "no_speech_prob": 0.0014637024141848087}, {"id": 782, "seek": 455316, "start": 4571.88, "end": 4577.639999999999, "text": " power looks like this. So, this is, for example, a graph substructure network with four-click count.", "tokens": [51300, 1347, 1542, 411, 341, 13, 407, 11, 341, 307, 11, 337, 1365, 11, 257, 4295, 4594, 2885, 3209, 365, 1451, 12, 18548, 1207, 13, 51588], "temperature": 0.0, "avg_logprob": -0.1327519510306564, "compression_ratio": 1.583673469387755, "no_speech_prob": 0.0014637024141848087}, {"id": 783, "seek": 457764, "start": 4577.64, "end": 4583.320000000001, "text": " So, it might actually, there might be examples of graphs that are distinguishable by 3WL, but", "tokens": [50364, 407, 11, 309, 1062, 767, 11, 456, 1062, 312, 5110, 295, 24877, 300, 366, 20206, 712, 538, 805, 54, 43, 11, 457, 50648], "temperature": 0.0, "avg_logprob": -0.13570007095988998, "compression_ratio": 1.6203389830508474, "no_speech_prob": 0.0010624921415001154}, {"id": 784, "seek": 457764, "start": 4583.320000000001, "end": 4589.88, "text": " not by this method. We have at least an example of a family that 3WL cannot detect. So, it's outside", "tokens": [50648, 406, 538, 341, 3170, 13, 492, 362, 412, 1935, 364, 1365, 295, 257, 1605, 300, 805, 54, 43, 2644, 5531, 13, 407, 11, 309, 311, 2380, 50976], "temperature": 0.0, "avg_logprob": -0.13570007095988998, "compression_ratio": 1.6203389830508474, "no_speech_prob": 0.0010624921415001154}, {"id": 785, "seek": 457764, "start": 4589.88, "end": 4596.84, "text": " of the hierarchy. And why this is important, especially in applications related to chemistry,", "tokens": [50976, 295, 264, 22333, 13, 400, 983, 341, 307, 1021, 11, 2318, 294, 5821, 4077, 281, 12558, 11, 51324], "temperature": 0.0, "avg_logprob": -0.13570007095988998, "compression_ratio": 1.6203389830508474, "no_speech_prob": 0.0010624921415001154}, {"id": 786, "seek": 457764, "start": 4597.4800000000005, "end": 4602.360000000001, "text": " because often we know these substructures are priori, right? Organic molecules, for example,", "tokens": [51356, 570, 2049, 321, 458, 613, 4594, 44513, 366, 4059, 72, 11, 558, 30, 12538, 299, 13093, 11, 337, 1365, 11, 51600], "temperature": 0.0, "avg_logprob": -0.13570007095988998, "compression_ratio": 1.6203389830508474, "no_speech_prob": 0.0010624921415001154}, {"id": 787, "seek": 457764, "start": 4602.360000000001, "end": 4607.0, "text": " cycles are a very prominent feature. These are what is called aromatic rings, right? Like in the", "tokens": [51600, 17796, 366, 257, 588, 17034, 4111, 13, 1981, 366, 437, 307, 1219, 45831, 11136, 11, 558, 30, 1743, 294, 264, 51832], "temperature": 0.0, "avg_logprob": -0.13570007095988998, "compression_ratio": 1.6203389830508474, "no_speech_prob": 0.0010624921415001154}, {"id": 788, "seek": 460700, "start": 4607.0, "end": 4612.68, "text": " molecule of caffeine, we have two, right? So, we have this ring of cycle of size six and cycle of", "tokens": [50364, 15582, 295, 31261, 11, 321, 362, 732, 11, 558, 30, 407, 11, 321, 362, 341, 4875, 295, 6586, 295, 2744, 2309, 293, 6586, 295, 50648], "temperature": 0.0, "avg_logprob": -0.11116355356543955, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.0014857308706268668}, {"id": 789, "seek": 460700, "start": 4612.68, "end": 4618.28, "text": " size five. And we see that if we incorporate this information as a kind of problem-specific", "tokens": [50648, 2744, 1732, 13, 400, 321, 536, 300, 498, 321, 16091, 341, 1589, 382, 257, 733, 295, 1154, 12, 29258, 50928], "temperature": 0.0, "avg_logprob": -0.11116355356543955, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.0014857308706268668}, {"id": 790, "seek": 460700, "start": 4618.28, "end": 4624.2, "text": " inductive bias into the problem, we are able to much better predict the properties of molecules.", "tokens": [50928, 31612, 488, 12577, 666, 264, 1154, 11, 321, 366, 1075, 281, 709, 1101, 6069, 264, 7221, 295, 13093, 13, 51224], "temperature": 0.0, "avg_logprob": -0.11116355356543955, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.0014857308706268668}, {"id": 791, "seek": 460700, "start": 4624.2, "end": 4629.64, "text": " In this case, it's, I think, the water solubility on the, on a toy data set of molecules that is", "tokens": [51224, 682, 341, 1389, 11, 309, 311, 11, 286, 519, 11, 264, 1281, 1404, 836, 1140, 322, 264, 11, 322, 257, 12058, 1412, 992, 295, 13093, 300, 307, 51496], "temperature": 0.0, "avg_logprob": -0.11116355356543955, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.0014857308706268668}, {"id": 792, "seek": 462964, "start": 4629.64, "end": 4636.200000000001, "text": " called zinc. And by incorporating cycles, we significantly reduce the error.", "tokens": [50364, 1219, 29062, 13, 400, 538, 33613, 17796, 11, 321, 10591, 5407, 264, 6713, 13, 50692], "temperature": 0.0, "avg_logprob": -0.110812012867261, "compression_ratio": 1.5127118644067796, "no_speech_prob": 0.003113195300102234}, {"id": 793, "seek": 462964, "start": 4638.280000000001, "end": 4644.04, "text": " Third class of approaches, what is called subgraph GNNs. And here, the idea is also very simple.", "tokens": [50796, 12548, 1508, 295, 11587, 11, 437, 307, 1219, 1422, 34091, 46411, 45, 82, 13, 400, 510, 11, 264, 1558, 307, 611, 588, 2199, 13, 51084], "temperature": 0.0, "avg_logprob": -0.110812012867261, "compression_ratio": 1.5127118644067796, "no_speech_prob": 0.003113195300102234}, {"id": 794, "seek": 462964, "start": 4646.360000000001, "end": 4650.12, "text": " So, if you look at these two graphs, again, this is probably one of the simplest examples of", "tokens": [51200, 407, 11, 498, 291, 574, 412, 613, 732, 24877, 11, 797, 11, 341, 307, 1391, 472, 295, 264, 22811, 5110, 295, 51388], "temperature": 0.0, "avg_logprob": -0.110812012867261, "compression_ratio": 1.5127118644067796, "no_speech_prob": 0.003113195300102234}, {"id": 795, "seek": 462964, "start": 4650.12, "end": 4654.76, "text": " non-lasomorphic graphs that cannot be tested by WL, right? If you do the color refinement,", "tokens": [51388, 2107, 12, 7743, 32702, 299, 24877, 300, 2644, 312, 8246, 538, 343, 43, 11, 558, 30, 759, 291, 360, 264, 2017, 1895, 30229, 11, 51620], "temperature": 0.0, "avg_logprob": -0.110812012867261, "compression_ratio": 1.5127118644067796, "no_speech_prob": 0.003113195300102234}, {"id": 796, "seek": 465476, "start": 4655.56, "end": 4660.280000000001, "text": " this is what WL produces, so the histograms are the same, right? That's exactly the case", "tokens": [50404, 341, 307, 437, 343, 43, 14725, 11, 370, 264, 49816, 82, 366, 264, 912, 11, 558, 30, 663, 311, 2293, 264, 1389, 50640], "temperature": 0.0, "avg_logprob": -0.08680088263897856, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.0026441961526870728}, {"id": 797, "seek": 465476, "start": 4660.280000000001, "end": 4666.68, "text": " where you cannot say anything about the graphs. But imagine that I can perturb the graph, for", "tokens": [50640, 689, 291, 2644, 584, 1340, 466, 264, 24877, 13, 583, 3811, 300, 286, 393, 40468, 264, 4295, 11, 337, 50960], "temperature": 0.0, "avg_logprob": -0.08680088263897856, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.0026441961526870728}, {"id": 798, "seek": 465476, "start": 4666.68, "end": 4673.24, "text": " example, by removing this edge, right? So, if I did it, the colors will be very different, right?", "tokens": [50960, 1365, 11, 538, 12720, 341, 4691, 11, 558, 30, 407, 11, 498, 286, 630, 309, 11, 264, 4577, 486, 312, 588, 819, 11, 558, 30, 51288], "temperature": 0.0, "avg_logprob": -0.08680088263897856, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.0026441961526870728}, {"id": 799, "seek": 465476, "start": 4673.24, "end": 4678.360000000001, "text": " So, these will be the distributions of the, of the colors, and they are clearly distinct. So, in", "tokens": [51288, 407, 11, 613, 486, 312, 264, 37870, 295, 264, 11, 295, 264, 4577, 11, 293, 436, 366, 4448, 10644, 13, 407, 11, 294, 51544], "temperature": 0.0, "avg_logprob": -0.08680088263897856, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.0026441961526870728}, {"id": 800, "seek": 465476, "start": 4678.360000000001, "end": 4683.96, "text": " this case, the perturbation allows to distinguish between structures that are otherwise indistinguishable.", "tokens": [51544, 341, 1389, 11, 264, 40468, 399, 4045, 281, 20206, 1296, 9227, 300, 366, 5911, 1016, 468, 7050, 742, 712, 13, 51824], "temperature": 0.0, "avg_logprob": -0.08680088263897856, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.0026441961526870728}, {"id": 801, "seek": 468396, "start": 4683.96, "end": 4688.76, "text": " So, the question is, of course, do I know which edge to remove, right? So, here maybe I was lucky,", "tokens": [50364, 407, 11, 264, 1168, 307, 11, 295, 1164, 11, 360, 286, 458, 597, 4691, 281, 4159, 11, 558, 30, 407, 11, 510, 1310, 286, 390, 6356, 11, 50604], "temperature": 0.0, "avg_logprob": -0.07090021216351053, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.0016702309949323535}, {"id": 802, "seek": 468396, "start": 4688.76, "end": 4693.64, "text": " and the answer is usually I don't. So, let's remove all possible edges, right? So, let's just", "tokens": [50604, 293, 264, 1867, 307, 2673, 286, 500, 380, 13, 407, 11, 718, 311, 4159, 439, 1944, 8819, 11, 558, 30, 407, 11, 718, 311, 445, 50848], "temperature": 0.0, "avg_logprob": -0.07090021216351053, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.0016702309949323535}, {"id": 803, "seek": 468396, "start": 4693.64, "end": 4699.08, "text": " make this perturbation when I remove one edge at a time, right? So, and there are seven possibilities.", "tokens": [50848, 652, 341, 40468, 399, 562, 286, 4159, 472, 4691, 412, 257, 565, 11, 558, 30, 407, 11, 293, 456, 366, 3407, 12178, 13, 51120], "temperature": 0.0, "avg_logprob": -0.07090021216351053, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.0016702309949323535}, {"id": 804, "seek": 468396, "start": 4699.08, "end": 4705.24, "text": " I can also do node deletions in the same way, right? And now, instead of a graph, I have a collection", "tokens": [51120, 286, 393, 611, 360, 9984, 1103, 302, 626, 294, 264, 912, 636, 11, 558, 30, 400, 586, 11, 2602, 295, 257, 4295, 11, 286, 362, 257, 5765, 51428], "temperature": 0.0, "avg_logprob": -0.07090021216351053, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.0016702309949323535}, {"id": 805, "seek": 468396, "start": 4705.96, "end": 4710.44, "text": " of subgraphs that are extracted by some policy. So, in this case, it's a very simple policy,", "tokens": [51464, 295, 1422, 34091, 82, 300, 366, 34086, 538, 512, 3897, 13, 407, 11, 294, 341, 1389, 11, 309, 311, 257, 588, 2199, 3897, 11, 51688], "temperature": 0.0, "avg_logprob": -0.07090021216351053, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.0016702309949323535}, {"id": 806, "seek": 471044, "start": 4710.44, "end": 4716.12, "text": " one node removal, right? And actually, it results in graph theory that say that if I give you this", "tokens": [50364, 472, 9984, 17933, 11, 558, 30, 400, 767, 11, 309, 3542, 294, 4295, 5261, 300, 584, 300, 498, 286, 976, 291, 341, 50648], "temperature": 0.0, "avg_logprob": -0.14837564321664665, "compression_ratio": 1.8941176470588235, "no_speech_prob": 0.0026036936324089766}, {"id": 807, "seek": 471044, "start": 4716.12, "end": 4721.639999999999, "text": " collection, so, in terminology of graph theory, this is called a reconstruction. So, if they have", "tokens": [50648, 5765, 11, 370, 11, 294, 27575, 295, 4295, 5261, 11, 341, 307, 1219, 257, 31565, 13, 407, 11, 498, 436, 362, 50924], "temperature": 0.0, "avg_logprob": -0.14837564321664665, "compression_ratio": 1.8941176470588235, "no_speech_prob": 0.0026036936324089766}, {"id": 808, "seek": 471044, "start": 4721.639999999999, "end": 4727.799999999999, "text": " the same multi-set of node remove subgraphs, right? So, this is what we denote H tilde G, right?", "tokens": [50924, 264, 912, 4825, 12, 3854, 295, 9984, 4159, 1422, 34091, 82, 11, 558, 30, 407, 11, 341, 307, 437, 321, 45708, 389, 45046, 460, 11, 558, 30, 51232], "temperature": 0.0, "avg_logprob": -0.14837564321664665, "compression_ratio": 1.8941176470588235, "no_speech_prob": 0.0026036936324089766}, {"id": 809, "seek": 471044, "start": 4728.5199999999995, "end": 4733.0, "text": " So, graphs where, basically, if we look at these kind of multi-sets, they will be the same, of", "tokens": [51268, 407, 11, 24877, 689, 11, 1936, 11, 498, 321, 574, 412, 613, 733, 295, 4825, 12, 82, 1385, 11, 436, 486, 312, 264, 912, 11, 295, 51492], "temperature": 0.0, "avg_logprob": -0.14837564321664665, "compression_ratio": 1.8941176470588235, "no_speech_prob": 0.0026036936324089766}, {"id": 810, "seek": 471044, "start": 4733.0, "end": 4738.839999999999, "text": " course, up to, up to reorting. So, the statement in graph theory that is called reconstruction", "tokens": [51492, 1164, 11, 493, 281, 11, 493, 281, 319, 477, 278, 13, 407, 11, 264, 5629, 294, 4295, 5261, 300, 307, 1219, 31565, 51784], "temperature": 0.0, "avg_logprob": -0.14837564321664665, "compression_ratio": 1.8941176470588235, "no_speech_prob": 0.0026036936324089766}, {"id": 811, "seek": 473884, "start": 4738.84, "end": 4745.400000000001, "text": " conjecture claims that under some technical assumptions, if H is a reconstruction of G,", "tokens": [50364, 416, 1020, 540, 9441, 300, 833, 512, 6191, 17695, 11, 498, 389, 307, 257, 31565, 295, 460, 11, 50692], "temperature": 0.0, "avg_logprob": -0.09065670172373454, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.001632258528843522}, {"id": 812, "seek": 473884, "start": 4745.400000000001, "end": 4751.96, "text": " then H is equivalent to G, isomorphic to G, right? So, why it is called a conjecture? Because", "tokens": [50692, 550, 389, 307, 10344, 281, 460, 11, 307, 32702, 299, 281, 460, 11, 558, 30, 407, 11, 983, 309, 307, 1219, 257, 416, 1020, 540, 30, 1436, 51020], "temperature": 0.0, "avg_logprob": -0.09065670172373454, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.001632258528843522}, {"id": 813, "seek": 473884, "start": 4751.96, "end": 4755.88, "text": " it is proved only for small graphs, and it's an open question in general. And there are", "tokens": [51020, 309, 307, 14617, 787, 337, 1359, 24877, 11, 293, 309, 311, 364, 1269, 1168, 294, 2674, 13, 400, 456, 366, 51216], "temperature": 0.0, "avg_logprob": -0.09065670172373454, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.001632258528843522}, {"id": 814, "seek": 473884, "start": 4755.88, "end": 4761.16, "text": " generalizations for subgraphs where you remove multiple nodes. So, this is, again, not a single", "tokens": [51216, 2674, 14455, 337, 1422, 34091, 82, 689, 291, 4159, 3866, 13891, 13, 407, 11, 341, 307, 11, 797, 11, 406, 257, 2167, 51480], "temperature": 0.0, "avg_logprob": -0.09065670172373454, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.001632258528843522}, {"id": 815, "seek": 473884, "start": 4761.16, "end": 4768.12, "text": " result. So, it's a class of results. It was introduced by Paul Kelly in his PhD thesis", "tokens": [51480, 1874, 13, 407, 11, 309, 311, 257, 1508, 295, 3542, 13, 467, 390, 7268, 538, 4552, 12345, 294, 702, 14476, 22288, 51828], "temperature": 0.0, "avg_logprob": -0.09065670172373454, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.001632258528843522}, {"id": 816, "seek": 476812, "start": 4768.12, "end": 4776.599999999999, "text": " that was done under the supervision of Stanislaw Ulam, who was a mathematician, a Polish mathematician,", "tokens": [50364, 300, 390, 1096, 833, 264, 32675, 295, 10061, 271, 5901, 624, 4326, 11, 567, 390, 257, 48281, 11, 257, 18504, 48281, 11, 50788], "temperature": 0.0, "avg_logprob": -0.10063160906781207, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.0027931819204241037}, {"id": 817, "seek": 476812, "start": 4776.599999999999, "end": 4782.76, "text": " but he's probably more famous for initiating the Manhattan Project and developing thermonuclear", "tokens": [50788, 457, 415, 311, 1391, 544, 4618, 337, 6265, 990, 264, 23633, 9849, 293, 6416, 8810, 266, 30335, 51096], "temperature": 0.0, "avg_logprob": -0.10063160906781207, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.0027931819204241037}, {"id": 818, "seek": 476812, "start": 4782.76, "end": 4789.08, "text": " weapons. So, but he also was, he's also famous for many interesting results in mathematics,", "tokens": [51096, 7278, 13, 407, 11, 457, 415, 611, 390, 11, 415, 311, 611, 4618, 337, 867, 1880, 3542, 294, 18666, 11, 51412], "temperature": 0.0, "avg_logprob": -0.10063160906781207, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.0027931819204241037}, {"id": 819, "seek": 476812, "start": 4789.08, "end": 4794.84, "text": " and this is one of them, the reconstruction conjectures. So, we don't know whether this is", "tokens": [51412, 293, 341, 307, 472, 295, 552, 11, 264, 31565, 416, 1020, 1303, 13, 407, 11, 321, 500, 380, 458, 1968, 341, 307, 51700], "temperature": 0.0, "avg_logprob": -0.10063160906781207, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.0027931819204241037}, {"id": 820, "seek": 479484, "start": 4794.84, "end": 4800.12, "text": " true. So, it might be true, but that's why it is a conjecture. It would be cool if it were true,", "tokens": [50364, 2074, 13, 407, 11, 309, 1062, 312, 2074, 11, 457, 300, 311, 983, 309, 307, 257, 416, 1020, 540, 13, 467, 576, 312, 1627, 498, 309, 645, 2074, 11, 50628], "temperature": 0.0, "avg_logprob": -0.09270344883942407, "compression_ratio": 1.807843137254902, "no_speech_prob": 0.003704753937199712}, {"id": 821, "seek": 479484, "start": 4800.12, "end": 4806.4400000000005, "text": " because, of course, in this case, I could test graphosomorphism by just doing something with", "tokens": [50628, 570, 11, 295, 1164, 11, 294, 341, 1389, 11, 286, 727, 1500, 4295, 329, 32702, 1434, 538, 445, 884, 746, 365, 50944], "temperature": 0.0, "avg_logprob": -0.09270344883942407, "compression_ratio": 1.807843137254902, "no_speech_prob": 0.003704753937199712}, {"id": 822, "seek": 479484, "start": 4806.4400000000005, "end": 4810.84, "text": " this collection. So, what exactly can we do with this collection, regardless whether the", "tokens": [50944, 341, 5765, 13, 407, 11, 437, 2293, 393, 321, 360, 365, 341, 5765, 11, 10060, 1968, 264, 51164], "temperature": 0.0, "avg_logprob": -0.09270344883942407, "compression_ratio": 1.807843137254902, "no_speech_prob": 0.003704753937199712}, {"id": 823, "seek": 479484, "start": 4810.84, "end": 4814.92, "text": " conjecture is true, right? So, it will just give us stronger theoretical property of these", "tokens": [51164, 416, 1020, 540, 307, 2074, 11, 558, 30, 407, 11, 309, 486, 445, 976, 505, 7249, 20864, 4707, 295, 613, 51368], "temperature": 0.0, "avg_logprob": -0.09270344883942407, "compression_ratio": 1.807843137254902, "no_speech_prob": 0.003704753937199712}, {"id": 824, "seek": 479484, "start": 4814.92, "end": 4820.2, "text": " architectures. So, what we can do is we can consider our graph is a collection of subgraphs", "tokens": [51368, 6331, 1303, 13, 407, 11, 437, 321, 393, 360, 307, 321, 393, 1949, 527, 4295, 307, 257, 5765, 295, 1422, 34091, 82, 51632], "temperature": 0.0, "avg_logprob": -0.09270344883942407, "compression_ratio": 1.807843137254902, "no_speech_prob": 0.003704753937199712}, {"id": 825, "seek": 482020, "start": 4820.2, "end": 4824.84, "text": " that are extracted from the given graph, right? And what is important to understand is that there", "tokens": [50364, 300, 366, 34086, 490, 264, 2212, 4295, 11, 558, 30, 400, 437, 307, 1021, 281, 1223, 307, 300, 456, 50596], "temperature": 0.0, "avg_logprob": -0.07975174096914438, "compression_ratio": 1.8863636363636365, "no_speech_prob": 0.007636087480932474}, {"id": 826, "seek": 482020, "start": 4824.84, "end": 4828.92, "text": " is a correspondence between them, right? Because we created these subgraphs, right? So, it's built", "tokens": [50596, 307, 257, 38135, 1296, 552, 11, 558, 30, 1436, 321, 2942, 613, 1422, 34091, 82, 11, 558, 30, 407, 11, 309, 311, 3094, 50800], "temperature": 0.0, "avg_logprob": -0.07975174096914438, "compression_ratio": 1.8863636363636365, "no_speech_prob": 0.007636087480932474}, {"id": 827, "seek": 482020, "start": 4828.92, "end": 4834.679999999999, "text": " on the same nodes. We just might remove some edges, right? Or some nodes. So, we have here two types of", "tokens": [50800, 322, 264, 912, 13891, 13, 492, 445, 1062, 4159, 512, 8819, 11, 558, 30, 1610, 512, 13891, 13, 407, 11, 321, 362, 510, 732, 3467, 295, 51088], "temperature": 0.0, "avg_logprob": -0.07975174096914438, "compression_ratio": 1.8863636363636365, "no_speech_prob": 0.007636087480932474}, {"id": 828, "seek": 482020, "start": 4835.32, "end": 4840.599999999999, "text": " symmetries. So, we have the permutation of the nodes in the graph itself, right? And we also have a", "tokens": [51120, 14232, 302, 2244, 13, 407, 11, 321, 362, 264, 4784, 11380, 295, 264, 13891, 294, 264, 4295, 2564, 11, 558, 30, 400, 321, 611, 362, 257, 51384], "temperature": 0.0, "avg_logprob": -0.07975174096914438, "compression_ratio": 1.8863636363636365, "no_speech_prob": 0.007636087480932474}, {"id": 829, "seek": 482020, "start": 4840.599999999999, "end": 4845.5599999999995, "text": " permutation of the subgraphs in this multi-set, right? Because we don't have a canonical order of", "tokens": [51384, 4784, 11380, 295, 264, 1422, 34091, 82, 294, 341, 4825, 12, 3854, 11, 558, 30, 1436, 321, 500, 380, 362, 257, 46491, 1668, 295, 51632], "temperature": 0.0, "avg_logprob": -0.07975174096914438, "compression_ratio": 1.8863636363636365, "no_speech_prob": 0.007636087480932474}, {"id": 830, "seek": 484556, "start": 4845.56, "end": 4851.400000000001, "text": " them. So, together, basically, the structure, the symmetry structure of this new object of", "tokens": [50364, 552, 13, 407, 11, 1214, 11, 1936, 11, 264, 3877, 11, 264, 25440, 3877, 295, 341, 777, 2657, 295, 50656], "temperature": 0.0, "avg_logprob": -0.08610889014847782, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0013116176705807447}, {"id": 831, "seek": 484556, "start": 4851.400000000001, "end": 4856.68, "text": " this collection of subgraphs is a product of two groups, right? If we don't know the correspondence,", "tokens": [50656, 341, 5765, 295, 1422, 34091, 82, 307, 257, 1674, 295, 732, 3935, 11, 558, 30, 759, 321, 500, 380, 458, 264, 38135, 11, 50920], "temperature": 0.0, "avg_logprob": -0.08610889014847782, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0013116176705807447}, {"id": 832, "seek": 484556, "start": 4856.68, "end": 4861.240000000001, "text": " there will be a special type of product that allow me to skip the details. And basically,", "tokens": [50920, 456, 486, 312, 257, 2121, 2010, 295, 1674, 300, 2089, 385, 281, 10023, 264, 4365, 13, 400, 1936, 11, 51148], "temperature": 0.0, "avg_logprob": -0.08610889014847782, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0013116176705807447}, {"id": 833, "seek": 484556, "start": 4861.240000000001, "end": 4865.320000000001, "text": " what we can do, we can design an architecture that does message passing on each of these subgraphs", "tokens": [51148, 437, 321, 393, 360, 11, 321, 393, 1715, 364, 9482, 300, 775, 3636, 8437, 322, 1184, 295, 613, 1422, 34091, 82, 51352], "temperature": 0.0, "avg_logprob": -0.08610889014847782, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0013116176705807447}, {"id": 834, "seek": 484556, "start": 4865.320000000001, "end": 4870.04, "text": " separately, but then fuses the information across graphs using these known correspondence.", "tokens": [51352, 14759, 11, 457, 550, 283, 8355, 264, 1589, 2108, 24877, 1228, 613, 2570, 38135, 13, 51588], "temperature": 0.0, "avg_logprob": -0.08610889014847782, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0013116176705807447}, {"id": 835, "seek": 487004, "start": 4870.04, "end": 4876.68, "text": " And this is probably more powerful than WL, a version of this architecture that we call", "tokens": [50364, 400, 341, 307, 1391, 544, 4005, 813, 343, 43, 11, 257, 3037, 295, 341, 9482, 300, 321, 818, 50696], "temperature": 0.0, "avg_logprob": -0.11658926684447009, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.0027314028702676296}, {"id": 836, "seek": 487004, "start": 4876.68, "end": 4882.5199999999995, "text": " subgraph union network. We can actually show that it's upper bounded by 3WL, and we hope that", "tokens": [50696, 1422, 34091, 11671, 3209, 13, 492, 393, 767, 855, 300, 309, 311, 6597, 37498, 538, 805, 54, 43, 11, 293, 321, 1454, 300, 50988], "temperature": 0.0, "avg_logprob": -0.11658926684447009, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.0027314028702676296}, {"id": 837, "seek": 487004, "start": 4882.5199999999995, "end": 4888.12, "text": " it will be equivalent to 3WL. But a few weeks after we published our paper, it was shown by a", "tokens": [50988, 309, 486, 312, 10344, 281, 805, 54, 43, 13, 583, 257, 1326, 3259, 934, 321, 6572, 527, 3035, 11, 309, 390, 4898, 538, 257, 51268], "temperature": 0.0, "avg_logprob": -0.11658926684447009, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.0027314028702676296}, {"id": 838, "seek": 487004, "start": 4888.12, "end": 4893.96, "text": " counter example that it is strictly less powerful than 3WL. So, we don't know exactly. It's surely", "tokens": [51268, 5682, 1365, 300, 309, 307, 20792, 1570, 4005, 813, 805, 54, 43, 13, 407, 11, 321, 500, 380, 458, 2293, 13, 467, 311, 11468, 51560], "temperature": 0.0, "avg_logprob": -0.11658926684447009, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.0027314028702676296}, {"id": 839, "seek": 489396, "start": 4893.96, "end": 4900.76, "text": " more powerful than 2WL, but upper bounded by 3WL, and strictly less powerful. So, we have even a", "tokens": [50364, 544, 4005, 813, 568, 54, 43, 11, 457, 6597, 37498, 538, 805, 54, 43, 11, 293, 20792, 1570, 4005, 13, 407, 11, 321, 362, 754, 257, 50704], "temperature": 0.0, "avg_logprob": -0.11894744756270428, "compression_ratio": 1.5308641975308641, "no_speech_prob": 0.007486728951334953}, {"id": 840, "seek": 489396, "start": 4900.76, "end": 4908.84, "text": " blog post about these different architectures, and there are multiple methods that are related.", "tokens": [50704, 6968, 2183, 466, 613, 819, 6331, 1303, 11, 293, 456, 366, 3866, 7150, 300, 366, 4077, 13, 51108], "temperature": 0.0, "avg_logprob": -0.11894744756270428, "compression_ratio": 1.5308641975308641, "no_speech_prob": 0.007486728951334953}, {"id": 841, "seek": 489396, "start": 4908.84, "end": 4915.08, "text": " So, one of them is, for example, you can do dropout on your neighbors. That was done by a", "tokens": [51108, 407, 11, 472, 295, 552, 307, 11, 337, 1365, 11, 291, 393, 360, 3270, 346, 322, 428, 12512, 13, 663, 390, 1096, 538, 257, 51420], "temperature": 0.0, "avg_logprob": -0.11894744756270428, "compression_ratio": 1.5308641975308641, "no_speech_prob": 0.007486728951334953}, {"id": 842, "seek": 489396, "start": 4915.08, "end": 4921.24, "text": " worker from the group of Roger Battenhofer at ETH, and they actually showed that this has", "tokens": [51420, 11346, 490, 264, 1594, 295, 17666, 10066, 1147, 1289, 612, 412, 462, 9620, 11, 293, 436, 767, 4712, 300, 341, 575, 51728], "temperature": 0.0, "avg_logprob": -0.11894744756270428, "compression_ratio": 1.5308641975308641, "no_speech_prob": 0.007486728951334953}, {"id": 843, "seek": 492124, "start": 4921.24, "end": 4926.84, "text": " similar effect. So, it increases the expressive power, not only gives some kind of robustness to", "tokens": [50364, 2531, 1802, 13, 407, 11, 309, 8637, 264, 40189, 1347, 11, 406, 787, 2709, 512, 733, 295, 13956, 1287, 281, 50644], "temperature": 0.0, "avg_logprob": -0.14892712468686312, "compression_ratio": 1.7355072463768115, "no_speech_prob": 0.004822994116693735}, {"id": 844, "seek": 492124, "start": 4926.84, "end": 4934.36, "text": " the architecture. So, the last more expressive type of message passing in your network, so I", "tokens": [50644, 264, 9482, 13, 407, 11, 264, 1036, 544, 40189, 2010, 295, 3636, 8437, 294, 428, 3209, 11, 370, 286, 51020], "temperature": 0.0, "avg_logprob": -0.14892712468686312, "compression_ratio": 1.7355072463768115, "no_speech_prob": 0.004822994116693735}, {"id": 845, "seek": 492124, "start": 4934.36, "end": 4939.16, "text": " would like to mention is what we can generally call topological message passing. And if you think of", "tokens": [51020, 576, 411, 281, 2152, 307, 437, 321, 393, 5101, 818, 1192, 4383, 3636, 8437, 13, 400, 498, 291, 519, 295, 51260], "temperature": 0.0, "avg_logprob": -0.14892712468686312, "compression_ratio": 1.7355072463768115, "no_speech_prob": 0.004822994116693735}, {"id": 846, "seek": 492124, "start": 4939.96, "end": 4945.719999999999, "text": " what is a graph, essentially, it's a set where you glue pairs of nodes together, right? So, every", "tokens": [51300, 437, 307, 257, 4295, 11, 4476, 11, 309, 311, 257, 992, 689, 291, 8998, 15494, 295, 13891, 1214, 11, 558, 30, 407, 11, 633, 51588], "temperature": 0.0, "avg_logprob": -0.14892712468686312, "compression_ratio": 1.7355072463768115, "no_speech_prob": 0.004822994116693735}, {"id": 847, "seek": 492124, "start": 4946.599999999999, "end": 4950.679999999999, "text": " element in a set writer, every node in the graph is a zero-dimensional topological object,", "tokens": [51632, 4478, 294, 257, 992, 9936, 11, 633, 9984, 294, 264, 4295, 307, 257, 4018, 12, 18759, 1192, 4383, 2657, 11, 51836], "temperature": 0.0, "avg_logprob": -0.14892712468686312, "compression_ratio": 1.7355072463768115, "no_speech_prob": 0.004822994116693735}, {"id": 848, "seek": 495068, "start": 4950.76, "end": 4957.240000000001, "text": " right? So, you can define this one-dimensional object, the edges that you glue to the nodes,", "tokens": [50368, 558, 30, 407, 11, 291, 393, 6964, 341, 472, 12, 18759, 2657, 11, 264, 8819, 300, 291, 8998, 281, 264, 13891, 11, 50692], "temperature": 0.0, "avg_logprob": -0.17698919245627073, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0006345574511215091}, {"id": 849, "seek": 495068, "start": 4957.240000000001, "end": 4960.6, "text": " right? And you get the graph, but you don't need to stop here. You can also", "tokens": [50692, 558, 30, 400, 291, 483, 264, 4295, 11, 457, 291, 500, 380, 643, 281, 1590, 510, 13, 509, 393, 611, 50860], "temperature": 0.0, "avg_logprob": -0.17698919245627073, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0006345574511215091}, {"id": 850, "seek": 495068, "start": 4961.4800000000005, "end": 4967.72, "text": " define cells of higher dimension, right? That you forgot about, glue to cycles in your graph,", "tokens": [50904, 6964, 5438, 295, 2946, 10139, 11, 558, 30, 663, 291, 5298, 466, 11, 8998, 281, 17796, 294, 428, 4295, 11, 51216], "temperature": 0.0, "avg_logprob": -0.17698919245627073, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0006345574511215091}, {"id": 851, "seek": 495068, "start": 4967.72, "end": 4974.360000000001, "text": " right? And we get what is called a cellar or CWL complex, right? And basically, now, instead of", "tokens": [51216, 558, 30, 400, 321, 483, 437, 307, 1219, 257, 2815, 289, 420, 383, 54, 43, 3997, 11, 558, 30, 400, 1936, 11, 586, 11, 2602, 295, 51548], "temperature": 0.0, "avg_logprob": -0.17698919245627073, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0006345574511215091}, {"id": 852, "seek": 495068, "start": 4974.360000000001, "end": 4979.0, "text": " traditional message passing in graph neural networks, where we exchange information between", "tokens": [51548, 5164, 3636, 8437, 294, 4295, 18161, 9590, 11, 689, 321, 7742, 1589, 1296, 51780], "temperature": 0.0, "avg_logprob": -0.17698919245627073, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0006345574511215091}, {"id": 853, "seek": 497900, "start": 4979.0, "end": 4985.88, "text": " nodes along the edges, we can also go up and down in this hierarchy. So, we can do message passing", "tokens": [50364, 13891, 2051, 264, 8819, 11, 321, 393, 611, 352, 493, 293, 760, 294, 341, 22333, 13, 407, 11, 321, 393, 360, 3636, 8437, 50708], "temperature": 0.0, "avg_logprob": -0.12562227249145508, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.001368847442790866}, {"id": 854, "seek": 497900, "start": 4985.88, "end": 4991.4, "text": " within the same dimension, right, of the cellar complex, but we can also go across dimensions.", "tokens": [50708, 1951, 264, 912, 10139, 11, 558, 11, 295, 264, 2815, 289, 3997, 11, 457, 321, 393, 611, 352, 2108, 12819, 13, 50984], "temperature": 0.0, "avg_logprob": -0.12562227249145508, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.001368847442790866}, {"id": 855, "seek": 497900, "start": 4992.36, "end": 4998.12, "text": " And this hierarchical message passing is strictly more powerful than the vice-versa-lemon,", "tokens": [51032, 400, 341, 35250, 804, 3636, 8437, 307, 20792, 544, 4005, 813, 264, 11964, 12, 840, 64, 12, 306, 3317, 11, 51320], "temperature": 0.0, "avg_logprob": -0.12562227249145508, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.001368847442790866}, {"id": 856, "seek": 497900, "start": 4998.12, "end": 5003.64, "text": " and it's obviously very convenient for molecules, because in molecules, these structures have", "tokens": [51320, 293, 309, 311, 2745, 588, 10851, 337, 13093, 11, 570, 294, 13093, 11, 613, 9227, 362, 51596], "temperature": 0.0, "avg_logprob": -0.12562227249145508, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.001368847442790866}, {"id": 857, "seek": 500364, "start": 5004.4400000000005, "end": 5010.04, "text": " some chemical meaning, and this probably is closer to how chemist thinks of molecule, because,", "tokens": [50404, 512, 7313, 3620, 11, 293, 341, 1391, 307, 4966, 281, 577, 4771, 468, 7309, 295, 15582, 11, 570, 11, 50684], "temperature": 0.0, "avg_logprob": -0.08988189697265625, "compression_ratio": 1.7463768115942029, "no_speech_prob": 0.00503497151657939}, {"id": 858, "seek": 500364, "start": 5011.4800000000005, "end": 5016.200000000001, "text": " of course, the graph captures all the information, but it doesn't make certain structures explicit.", "tokens": [50756, 295, 1164, 11, 264, 4295, 27986, 439, 264, 1589, 11, 457, 309, 1177, 380, 652, 1629, 9227, 13691, 13, 50992], "temperature": 0.0, "avg_logprob": -0.08988189697265625, "compression_ratio": 1.7463768115942029, "no_speech_prob": 0.00503497151657939}, {"id": 859, "seek": 500364, "start": 5016.84, "end": 5020.280000000001, "text": " And in graph neural networks, for example, well, first of all, you cannot even detect by message", "tokens": [51024, 400, 294, 4295, 18161, 9590, 11, 337, 1365, 11, 731, 11, 700, 295, 439, 11, 291, 2644, 754, 5531, 538, 3636, 51196], "temperature": 0.0, "avg_logprob": -0.08988189697265625, "compression_ratio": 1.7463768115942029, "no_speech_prob": 0.00503497151657939}, {"id": 860, "seek": 500364, "start": 5020.280000000001, "end": 5024.84, "text": " passing the presence of these structures. And if you want to transfer information from this node", "tokens": [51196, 8437, 264, 6814, 295, 613, 9227, 13, 400, 498, 291, 528, 281, 5003, 1589, 490, 341, 9984, 51424], "temperature": 0.0, "avg_logprob": -0.08988189697265625, "compression_ratio": 1.7463768115942029, "no_speech_prob": 0.00503497151657939}, {"id": 861, "seek": 500364, "start": 5024.84, "end": 5028.4400000000005, "text": " to this node, you will need to do a few steps of message passing. Here, we can do it at once.", "tokens": [51424, 281, 341, 9984, 11, 291, 486, 643, 281, 360, 257, 1326, 4439, 295, 3636, 8437, 13, 1692, 11, 321, 393, 360, 309, 412, 1564, 13, 51604], "temperature": 0.0, "avg_logprob": -0.08988189697265625, "compression_ratio": 1.7463768115942029, "no_speech_prob": 0.00503497151657939}, {"id": 862, "seek": 502844, "start": 5029.32, "end": 5035.5599999999995, "text": " So, it also gives computational advantages. And again, if you want some more details about", "tokens": [50408, 407, 11, 309, 611, 2709, 28270, 14906, 13, 400, 797, 11, 498, 291, 528, 512, 544, 4365, 466, 50720], "temperature": 0.0, "avg_logprob": -0.19844737333409926, "compression_ratio": 1.4956140350877194, "no_speech_prob": 0.013732942752540112}, {"id": 863, "seek": 502844, "start": 5036.36, "end": 5039.32, "text": " how these different methods are related to each other, so there are many more", "tokens": [50760, 577, 613, 819, 7150, 366, 4077, 281, 1184, 661, 11, 370, 456, 366, 867, 544, 50908], "temperature": 0.0, "avg_logprob": -0.19844737333409926, "compression_ratio": 1.4956140350877194, "no_speech_prob": 0.013732942752540112}, {"id": 864, "seek": 502844, "start": 5040.44, "end": 5045.799999999999, "text": " expressive architectures, so there is a tutorial that was given at the log conference,", "tokens": [50964, 40189, 6331, 1303, 11, 370, 456, 307, 257, 7073, 300, 390, 2212, 412, 264, 3565, 7586, 11, 51232], "temperature": 0.0, "avg_logprob": -0.19844737333409926, "compression_ratio": 1.4956140350877194, "no_speech_prob": 0.013732942752540112}, {"id": 865, "seek": 502844, "start": 5046.599999999999, "end": 5053.639999999999, "text": " and recorded on YouTube by my PhD student, Publizio Frasca, with Beatrizio Bevilacqua", "tokens": [51272, 293, 8287, 322, 3088, 538, 452, 14476, 3107, 11, 21808, 75, 590, 1004, 1526, 296, 496, 11, 365, 16031, 24959, 1004, 879, 20202, 326, 34787, 51624], "temperature": 0.0, "avg_logprob": -0.19844737333409926, "compression_ratio": 1.4956140350877194, "no_speech_prob": 0.013732942752540112}, {"id": 866, "seek": 505364, "start": 5053.64, "end": 5059.0, "text": " and Gagai Maron. So, it's actually a very nice tutorial, and they go into much more details about", "tokens": [50364, 293, 460, 559, 1301, 2039, 266, 13, 407, 11, 309, 311, 767, 257, 588, 1481, 7073, 11, 293, 436, 352, 666, 709, 544, 4365, 466, 50632], "temperature": 0.0, "avg_logprob": -0.1633717316847581, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.0018488293280825019}, {"id": 867, "seek": 505364, "start": 5059.64, "end": 5065.240000000001, "text": " all these and other different methods for expressive graph neural networks. Any questions so far?", "tokens": [50664, 439, 613, 293, 661, 819, 7150, 337, 40189, 4295, 18161, 9590, 13, 2639, 1651, 370, 1400, 30, 50944], "temperature": 0.0, "avg_logprob": -0.1633717316847581, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.0018488293280825019}, {"id": 868, "seek": 505364, "start": 5076.360000000001, "end": 5082.360000000001, "text": " I have a question about summation aggregation being the maximally expressive aggregation.", "tokens": [51500, 286, 362, 257, 1168, 466, 28811, 16743, 399, 885, 264, 5138, 379, 40189, 16743, 399, 13, 51800], "temperature": 0.0, "avg_logprob": -0.1633717316847581, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.0018488293280825019}, {"id": 869, "seek": 508364, "start": 5083.96, "end": 5089.72, "text": " Also, probably it is related. Maybe if you can also comment on the, what was it, discrete,", "tokens": [50380, 2743, 11, 1391, 309, 307, 4077, 13, 2704, 498, 291, 393, 611, 2871, 322, 264, 11, 437, 390, 309, 11, 27706, 11, 50668], "temperature": 0.0, "avg_logprob": -0.1328268845876058, "compression_ratio": 1.6409090909090909, "no_speech_prob": 0.002592244651168585}, {"id": 870, "seek": 508364, "start": 5089.72, "end": 5096.76, "text": " countable restriction on the features. Because if I imagine that our features are integers,", "tokens": [50668, 1207, 712, 29529, 322, 264, 4122, 13, 1436, 498, 286, 3811, 300, 527, 4122, 366, 41674, 11, 51020], "temperature": 0.0, "avg_logprob": -0.1328268845876058, "compression_ratio": 1.6409090909090909, "no_speech_prob": 0.002592244651168585}, {"id": 871, "seek": 508364, "start": 5097.96, "end": 5103.4800000000005, "text": " there are different combinations of integers that sum up to the same number. So, summing them", "tokens": [51080, 456, 366, 819, 21267, 295, 41674, 300, 2408, 493, 281, 264, 912, 1230, 13, 407, 11, 2408, 2810, 552, 51356], "temperature": 0.0, "avg_logprob": -0.1328268845876058, "compression_ratio": 1.6409090909090909, "no_speech_prob": 0.002592244651168585}, {"id": 872, "seek": 508364, "start": 5103.4800000000005, "end": 5111.240000000001, "text": " actually does lose the information. So, countable doesn't necessarily mean integers,", "tokens": [51356, 767, 775, 3624, 264, 1589, 13, 407, 11, 1207, 712, 1177, 380, 4725, 914, 41674, 11, 51744], "temperature": 0.0, "avg_logprob": -0.1328268845876058, "compression_ratio": 1.6409090909090909, "no_speech_prob": 0.002592244651168585}, {"id": 873, "seek": 511124, "start": 5111.5599999999995, "end": 5117.4, "text": " but they should not be continuous. So, why this is, without going into too much details,", "tokens": [50380, 457, 436, 820, 406, 312, 10957, 13, 407, 11, 983, 341, 307, 11, 1553, 516, 666, 886, 709, 4365, 11, 50672], "temperature": 0.0, "avg_logprob": -0.12980499500181616, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.009502002038061619}, {"id": 874, "seek": 511124, "start": 5117.4, "end": 5123.719999999999, "text": " basically they use the same proof technique that was used in deep nets to prove the", "tokens": [50672, 1936, 436, 764, 264, 912, 8177, 6532, 300, 390, 1143, 294, 2452, 36170, 281, 7081, 264, 50988], "temperature": 0.0, "avg_logprob": -0.12980499500181616, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.009502002038061619}, {"id": 875, "seek": 511124, "start": 5123.719999999999, "end": 5128.599999999999, "text": " universality there. So, this assumption is important. If you remove this assumption that", "tokens": [50988, 5950, 1860, 456, 13, 407, 11, 341, 15302, 307, 1021, 13, 759, 291, 4159, 341, 15302, 300, 51232], "temperature": 0.0, "avg_logprob": -0.12980499500181616, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.009502002038061619}, {"id": 876, "seek": 511124, "start": 5130.28, "end": 5137.88, "text": " this proof doesn't work. So, basically they apply locally kind of the result of deep net", "tokens": [51316, 341, 8177, 1177, 380, 589, 13, 407, 11, 1936, 436, 3079, 16143, 733, 295, 264, 1874, 295, 2452, 2533, 51696], "temperature": 0.0, "avg_logprob": -0.12980499500181616, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.009502002038061619}, {"id": 877, "seek": 513788, "start": 5138.52, "end": 5143.16, "text": " that works on sets. All right. Thanks.", "tokens": [50396, 300, 1985, 322, 6352, 13, 1057, 558, 13, 2561, 13, 50628], "temperature": 0.0, "avg_logprob": -0.1585659102389687, "compression_ratio": 1.5377358490566038, "no_speech_prob": 0.0016346722841262817}, {"id": 878, "seek": 513788, "start": 5146.52, "end": 5153.64, "text": " Hello. Very interesting. Thank you. I'm wondering about chemistry, whether you can encode in the", "tokens": [50796, 2425, 13, 4372, 1880, 13, 1044, 291, 13, 286, 478, 6359, 466, 12558, 11, 1968, 291, 393, 2058, 1429, 294, 264, 51152], "temperature": 0.0, "avg_logprob": -0.1585659102389687, "compression_ratio": 1.5377358490566038, "no_speech_prob": 0.0016346722841262817}, {"id": 879, "seek": 513788, "start": 5153.64, "end": 5159.72, "text": " features of your graphs, also geometric information. Especially in chemistry, aromaticity is very", "tokens": [51152, 4122, 295, 428, 24877, 11, 611, 33246, 1589, 13, 8545, 294, 12558, 11, 45831, 507, 307, 588, 51456], "temperature": 0.0, "avg_logprob": -0.1585659102389687, "compression_ratio": 1.5377358490566038, "no_speech_prob": 0.0016346722841262817}, {"id": 880, "seek": 513788, "start": 5159.72, "end": 5165.24, "text": " important. Whether it's possible to encode it directly or you need some additional layers of", "tokens": [51456, 1021, 13, 8503, 309, 311, 1944, 281, 2058, 1429, 309, 3838, 420, 291, 643, 512, 4497, 7914, 295, 51732], "temperature": 0.0, "avg_logprob": -0.1585659102389687, "compression_ratio": 1.5377358490566038, "no_speech_prob": 0.0016346722841262817}, {"id": 881, "seek": 516524, "start": 5165.24, "end": 5170.12, "text": " information. So, some information you can probably compute. And of course, if you can", "tokens": [50364, 1589, 13, 407, 11, 512, 1589, 291, 393, 1391, 14722, 13, 400, 295, 1164, 11, 498, 291, 393, 50608], "temperature": 0.0, "avg_logprob": -0.10678259760355778, "compression_ratio": 1.9017543859649122, "no_speech_prob": 0.0030345122795552015}, {"id": 882, "seek": 516524, "start": 5170.12, "end": 5173.32, "text": " pre-comput it, if you know that these are meaningful features, then I think it makes", "tokens": [50608, 659, 12, 1112, 2582, 309, 11, 498, 291, 458, 300, 613, 366, 10995, 4122, 11, 550, 286, 519, 309, 1669, 50768], "temperature": 0.0, "avg_logprob": -0.10678259760355778, "compression_ratio": 1.9017543859649122, "no_speech_prob": 0.0030345122795552015}, {"id": 883, "seek": 516524, "start": 5173.32, "end": 5178.04, "text": " sense to encode them. So, the geometric information, I'm not sure that you mean", "tokens": [50768, 2020, 281, 2058, 1429, 552, 13, 407, 11, 264, 33246, 1589, 11, 286, 478, 406, 988, 300, 291, 914, 51004], "temperature": 0.0, "avg_logprob": -0.10678259760355778, "compression_ratio": 1.9017543859649122, "no_speech_prob": 0.0030345122795552015}, {"id": 884, "seek": 516524, "start": 5179.719999999999, "end": 5183.96, "text": " information that comes from the positions of the atoms, right? Yeah. So, I will talk about it", "tokens": [51088, 1589, 300, 1487, 490, 264, 8432, 295, 264, 16871, 11, 558, 30, 865, 13, 407, 11, 286, 486, 751, 466, 309, 51300], "temperature": 0.0, "avg_logprob": -0.10678259760355778, "compression_ratio": 1.9017543859649122, "no_speech_prob": 0.0030345122795552015}, {"id": 885, "seek": 516524, "start": 5183.96, "end": 5187.719999999999, "text": " in a second. So, you can, basically when you deal with geometric information, you also need to do", "tokens": [51300, 294, 257, 1150, 13, 407, 11, 291, 393, 11, 1936, 562, 291, 2028, 365, 33246, 1589, 11, 291, 611, 643, 281, 360, 51488], "temperature": 0.0, "avg_logprob": -0.10678259760355778, "compression_ratio": 1.9017543859649122, "no_speech_prob": 0.0030345122795552015}, {"id": 886, "seek": 516524, "start": 5187.719999999999, "end": 5193.32, "text": " it in a proper way. So, you need to do it in a way that is equivariant to possible transformations.", "tokens": [51488, 309, 294, 257, 2296, 636, 13, 407, 11, 291, 643, 281, 360, 309, 294, 257, 636, 300, 307, 48726, 3504, 394, 281, 1944, 34852, 13, 51768], "temperature": 0.0, "avg_logprob": -0.10678259760355778, "compression_ratio": 1.9017543859649122, "no_speech_prob": 0.0030345122795552015}, {"id": 887, "seek": 519332, "start": 5193.32, "end": 5200.84, "text": " But I think the short answer is yes. In case of these, like, increasing hierarchy of the", "tokens": [50364, 583, 286, 519, 264, 2099, 1867, 307, 2086, 13, 682, 1389, 295, 613, 11, 411, 11, 5662, 22333, 295, 264, 50740], "temperature": 0.0, "avg_logprob": -0.1301618501978013, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.0014487969456240535}, {"id": 888, "seek": 519332, "start": 5200.84, "end": 5209.24, "text": " KWL tests, is it a case that for a given KWL that we know there exists some message passing", "tokens": [50740, 591, 54, 43, 6921, 11, 307, 309, 257, 1389, 300, 337, 257, 2212, 591, 54, 43, 300, 321, 458, 456, 8198, 512, 3636, 8437, 51160], "temperature": 0.0, "avg_logprob": -0.1301618501978013, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.0014487969456240535}, {"id": 889, "seek": 519332, "start": 5209.24, "end": 5214.44, "text": " graph neural network that exists that can do it, but we just can't construct them? Or can we say", "tokens": [51160, 4295, 18161, 3209, 300, 8198, 300, 393, 360, 309, 11, 457, 321, 445, 393, 380, 7690, 552, 30, 1610, 393, 321, 584, 51420], "temperature": 0.0, "avg_logprob": -0.1301618501978013, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.0014487969456240535}, {"id": 890, "seek": 519332, "start": 5214.44, "end": 5220.04, "text": " that if we could make such a message passing GNN for such KWL, it would be, you know, intractably", "tokens": [51420, 300, 498, 321, 727, 652, 1270, 257, 3636, 8437, 46411, 45, 337, 1270, 591, 54, 43, 11, 309, 576, 312, 11, 291, 458, 11, 560, 1897, 1188, 51700], "temperature": 0.0, "avg_logprob": -0.1301618501978013, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.0014487969456240535}, {"id": 891, "seek": 522004, "start": 5220.04, "end": 5224.44, "text": " huge and we would need to approximate it or something like that? So, first of all, it is", "tokens": [50364, 2603, 293, 321, 576, 643, 281, 30874, 309, 420, 746, 411, 300, 30, 407, 11, 700, 295, 439, 11, 309, 307, 50584], "temperature": 0.0, "avg_logprob": -0.1267537883683747, "compression_ratio": 1.5798319327731092, "no_speech_prob": 0.0027300668880343437}, {"id": 892, "seek": 522004, "start": 5224.44, "end": 5231.72, "text": " intractably huge. So, it complexes N to the power K. So, I think what is limited in practice is", "tokens": [50584, 560, 1897, 1188, 2603, 13, 407, 11, 309, 43676, 426, 281, 264, 1347, 591, 13, 407, 11, 286, 519, 437, 307, 5567, 294, 3124, 307, 50948], "temperature": 0.0, "avg_logprob": -0.1267537883683747, "compression_ratio": 1.5798319327731092, "no_speech_prob": 0.0027300668880343437}, {"id": 893, "seek": 522004, "start": 5231.72, "end": 5240.04, "text": " 3WL. So, this is what Maron describes in his paper. I don't know, depending whether you can call it", "tokens": [50948, 805, 54, 43, 13, 407, 11, 341, 307, 437, 2039, 266, 15626, 294, 702, 3035, 13, 286, 500, 380, 458, 11, 5413, 1968, 291, 393, 818, 309, 51364], "temperature": 0.0, "avg_logprob": -0.1267537883683747, "compression_ratio": 1.5798319327731092, "no_speech_prob": 0.0027300668880343437}, {"id": 894, "seek": 522004, "start": 5240.04, "end": 5244.2, "text": " message passing or not, it depends on what you consider message passing, right? So, because", "tokens": [51364, 3636, 8437, 420, 406, 11, 309, 5946, 322, 437, 291, 1949, 3636, 8437, 11, 558, 30, 407, 11, 570, 51572], "temperature": 0.0, "avg_logprob": -0.1267537883683747, "compression_ratio": 1.5798319327731092, "no_speech_prob": 0.0027300668880343437}, {"id": 895, "seek": 524420, "start": 5244.28, "end": 5251.08, "text": " here you have more than pairs of nodes, I would argue that strictly speaking, it's not message", "tokens": [50368, 510, 291, 362, 544, 813, 15494, 295, 13891, 11, 286, 576, 9695, 300, 20792, 4124, 11, 309, 311, 406, 3636, 50708], "temperature": 0.0, "avg_logprob": -0.15472898326936316, "compression_ratio": 1.6382252559726962, "no_speech_prob": 0.0058198352344334126}, {"id": 896, "seek": 524420, "start": 5251.08, "end": 5255.24, "text": " passing, but for example, Petr Wieliszko, which would say that it's message passing on a different", "tokens": [50708, 8437, 11, 457, 337, 1365, 11, 10472, 81, 343, 1187, 23848, 4093, 11, 597, 576, 584, 300, 309, 311, 3636, 8437, 322, 257, 819, 50916], "temperature": 0.0, "avg_logprob": -0.15472898326936316, "compression_ratio": 1.6382252559726962, "no_speech_prob": 0.0058198352344334126}, {"id": 897, "seek": 524420, "start": 5255.24, "end": 5261.48, "text": " graph, right? So, it depends on the perspective. Maybe one more follow-up question for this KWL", "tokens": [50916, 4295, 11, 558, 30, 407, 11, 309, 5946, 322, 264, 4585, 13, 2704, 472, 544, 1524, 12, 1010, 1168, 337, 341, 591, 54, 43, 51228], "temperature": 0.0, "avg_logprob": -0.15472898326936316, "compression_ratio": 1.6382252559726962, "no_speech_prob": 0.0058198352344334126}, {"id": 898, "seek": 524420, "start": 5261.48, "end": 5267.48, "text": " stuff. If you have a particular application that you are interested about, are there some cases", "tokens": [51228, 1507, 13, 759, 291, 362, 257, 1729, 3861, 300, 291, 366, 3102, 466, 11, 366, 456, 512, 3331, 51528], "temperature": 0.0, "avg_logprob": -0.15472898326936316, "compression_ratio": 1.6382252559726962, "no_speech_prob": 0.0058198352344334126}, {"id": 899, "seek": 524420, "start": 5267.48, "end": 5272.04, "text": " where you can say, for this class of problems that we're working on, we can, it's enough to be", "tokens": [51528, 689, 291, 393, 584, 11, 337, 341, 1508, 295, 2740, 300, 321, 434, 1364, 322, 11, 321, 393, 11, 309, 311, 1547, 281, 312, 51756], "temperature": 0.0, "avg_logprob": -0.15472898326936316, "compression_ratio": 1.6382252559726962, "no_speech_prob": 0.0058198352344334126}, {"id": 900, "seek": 527204, "start": 5272.04, "end": 5276.84, "text": " able to distinguish up such a level because the higher you go for K, obviously, like these edge cases", "tokens": [50364, 1075, 281, 20206, 493, 1270, 257, 1496, 570, 264, 2946, 291, 352, 337, 591, 11, 2745, 11, 411, 613, 4691, 3331, 50604], "temperature": 0.0, "avg_logprob": -0.14011841435586253, "compression_ratio": 1.6655052264808363, "no_speech_prob": 0.0024038103874772787}, {"id": 901, "seek": 527204, "start": 5276.84, "end": 5282.12, "text": " would get really nasty, which you might not see in your application. Well, so, it's a good question,", "tokens": [50604, 576, 483, 534, 17923, 11, 597, 291, 1062, 406, 536, 294, 428, 3861, 13, 1042, 11, 370, 11, 309, 311, 257, 665, 1168, 11, 50868], "temperature": 0.0, "avg_logprob": -0.14011841435586253, "compression_ratio": 1.6655052264808363, "no_speech_prob": 0.0024038103874772787}, {"id": 902, "seek": 527204, "start": 5282.12, "end": 5288.6, "text": " right? So, for example, planar graphs have WL dimension of 3. So, basically, all planar", "tokens": [50868, 558, 30, 407, 11, 337, 1365, 11, 1393, 289, 24877, 362, 343, 43, 10139, 295, 805, 13, 407, 11, 1936, 11, 439, 1393, 289, 51192], "temperature": 0.0, "avg_logprob": -0.14011841435586253, "compression_ratio": 1.6655052264808363, "no_speech_prob": 0.0024038103874772787}, {"id": 903, "seek": 527204, "start": 5288.6, "end": 5296.6, "text": " graphs can be distinguished by 3WL test. And you can argue that molecules, right? Most of the", "tokens": [51192, 24877, 393, 312, 21702, 538, 805, 54, 43, 1500, 13, 400, 291, 393, 9695, 300, 13093, 11, 558, 30, 4534, 295, 264, 51592], "temperature": 0.0, "avg_logprob": -0.14011841435586253, "compression_ratio": 1.6655052264808363, "no_speech_prob": 0.0024038103874772787}, {"id": 904, "seek": 527204, "start": 5296.6, "end": 5301.16, "text": " molecules, you can draw two-dimensional structures, maybe some of them you don't, but they're", "tokens": [51592, 13093, 11, 291, 393, 2642, 732, 12, 18759, 9227, 11, 1310, 512, 295, 552, 291, 500, 380, 11, 457, 436, 434, 51820], "temperature": 0.0, "avg_logprob": -0.14011841435586253, "compression_ratio": 1.6655052264808363, "no_speech_prob": 0.0024038103874772787}, {"id": 905, "seek": 530116, "start": 5301.16, "end": 5308.599999999999, "text": " probably a tiny fraction. So, do you need something more powerful than that? The expressive power", "tokens": [50364, 1391, 257, 5870, 14135, 13, 407, 11, 360, 291, 643, 746, 544, 4005, 813, 300, 30, 440, 40189, 1347, 50736], "temperature": 0.0, "avg_logprob": -0.10683670043945312, "compression_ratio": 1.5755102040816327, "no_speech_prob": 0.0013362631434574723}, {"id": 906, "seek": 530116, "start": 5308.599999999999, "end": 5313.639999999999, "text": " itself is probably not the end of the story, right? Because nothing tells you about generalization.", "tokens": [50736, 2564, 307, 1391, 406, 264, 917, 295, 264, 1657, 11, 558, 30, 1436, 1825, 5112, 291, 466, 2674, 2144, 13, 50988], "temperature": 0.0, "avg_logprob": -0.10683670043945312, "compression_ratio": 1.5755102040816327, "no_speech_prob": 0.0013362631434574723}, {"id": 907, "seek": 530116, "start": 5315.32, "end": 5323.639999999999, "text": " So, yeah, I don't think that this on its own is really the crucial consideration. It's good,", "tokens": [51072, 407, 11, 1338, 11, 286, 500, 380, 519, 300, 341, 322, 1080, 1065, 307, 534, 264, 11462, 12381, 13, 467, 311, 665, 11, 51488], "temperature": 0.0, "avg_logprob": -0.10683670043945312, "compression_ratio": 1.5755102040816327, "no_speech_prob": 0.0013362631434574723}, {"id": 908, "seek": 530116, "start": 5323.639999999999, "end": 5329.32, "text": " of course, to have an architecture that is, that allows to distinguish between broader class of", "tokens": [51488, 295, 1164, 11, 281, 362, 364, 9482, 300, 307, 11, 300, 4045, 281, 20206, 1296, 13227, 1508, 295, 51772], "temperature": 0.0, "avg_logprob": -0.10683670043945312, "compression_ratio": 1.5755102040816327, "no_speech_prob": 0.0013362631434574723}, {"id": 909, "seek": 532932, "start": 5329.32, "end": 5333.16, "text": " graphs, but if it comes at the expense of computational complexity, for example, maybe", "tokens": [50364, 24877, 11, 457, 498, 309, 1487, 412, 264, 18406, 295, 28270, 14024, 11, 337, 1365, 11, 1310, 50556], "temperature": 0.0, "avg_logprob": -0.13761593597103852, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.0016684754518792033}, {"id": 910, "seek": 532932, "start": 5333.16, "end": 5339.08, "text": " it's a bad idea. So, you probably want some kind of good trade-off between these. Thank you.", "tokens": [50556, 309, 311, 257, 1578, 1558, 13, 407, 11, 291, 1391, 528, 512, 733, 295, 665, 4923, 12, 4506, 1296, 613, 13, 1044, 291, 13, 50852], "temperature": 0.0, "avg_logprob": -0.13761593597103852, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.0016684754518792033}, {"id": 911, "seek": 532932, "start": 5340.5199999999995, "end": 5347.719999999999, "text": " I had a follow-up question. So, could you please define what message-passing is in this case?", "tokens": [50924, 286, 632, 257, 1524, 12, 1010, 1168, 13, 407, 11, 727, 291, 1767, 6964, 437, 3636, 12, 9216, 278, 307, 294, 341, 1389, 30, 51284], "temperature": 0.0, "avg_logprob": -0.13761593597103852, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.0016684754518792033}, {"id": 912, "seek": 532932, "start": 5348.759999999999, "end": 5353.48, "text": " Can we give a short definition? Right. So, message-passing is what I call message-passing is", "tokens": [51336, 1664, 321, 976, 257, 2099, 7123, 30, 1779, 13, 407, 11, 3636, 12, 9216, 278, 307, 437, 286, 818, 3636, 12, 9216, 278, 307, 51572], "temperature": 0.0, "avg_logprob": -0.13761593597103852, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.0016684754518792033}, {"id": 913, "seek": 535348, "start": 5354.2, "end": 5359.879999999999, "text": " these kind of architectures. Let's see where I had it. Yeah. So, basically, architecture of this", "tokens": [50400, 613, 733, 295, 6331, 1303, 13, 961, 311, 536, 689, 286, 632, 309, 13, 865, 13, 407, 11, 1936, 11, 9482, 295, 341, 50684], "temperature": 0.0, "avg_logprob": -0.14657862046185663, "compression_ratio": 1.688212927756654, "no_speech_prob": 0.0018800662364810705}, {"id": 914, "seek": 535348, "start": 5359.879999999999, "end": 5365.08, "text": " kind. So, this is the most general type of message-passing. So, we have the update of node i", "tokens": [50684, 733, 13, 407, 11, 341, 307, 264, 881, 2674, 2010, 295, 3636, 12, 9216, 278, 13, 407, 11, 321, 362, 264, 5623, 295, 9984, 741, 50944], "temperature": 0.0, "avg_logprob": -0.14657862046185663, "compression_ratio": 1.688212927756654, "no_speech_prob": 0.0018800662364810705}, {"id": 915, "seek": 535348, "start": 5366.679999999999, "end": 5371.799999999999, "text": " from neighbor j is done in this way. So, I have a function that depends on both i and j,", "tokens": [51024, 490, 5987, 361, 307, 1096, 294, 341, 636, 13, 407, 11, 286, 362, 257, 2445, 300, 5946, 322, 1293, 741, 293, 361, 11, 51280], "temperature": 0.0, "avg_logprob": -0.14657862046185663, "compression_ratio": 1.688212927756654, "no_speech_prob": 0.0018800662364810705}, {"id": 916, "seek": 535348, "start": 5372.5199999999995, "end": 5376.36, "text": " that the function is parametric. So, that's learnable and then aggregate. You can actually", "tokens": [51316, 300, 264, 2445, 307, 6220, 17475, 13, 407, 11, 300, 311, 1466, 712, 293, 550, 26118, 13, 509, 393, 767, 51508], "temperature": 0.0, "avg_logprob": -0.14657862046185663, "compression_ratio": 1.688212927756654, "no_speech_prob": 0.0018800662364810705}, {"id": 917, "seek": 535348, "start": 5376.36, "end": 5379.799999999999, "text": " show that summation is what you need, right? You don't need anything else.", "tokens": [51508, 855, 300, 28811, 307, 437, 291, 643, 11, 558, 30, 509, 500, 380, 643, 1340, 1646, 13, 51680], "temperature": 0.0, "avg_logprob": -0.14657862046185663, "compression_ratio": 1.688212927756654, "no_speech_prob": 0.0018800662364810705}, {"id": 918, "seek": 538348, "start": 5383.639999999999, "end": 5390.5199999999995, "text": " So, well, this is, so this is message-passing. There are higher order architectures, right,", "tokens": [50372, 407, 11, 731, 11, 341, 307, 11, 370, 341, 307, 3636, 12, 9216, 278, 13, 821, 366, 2946, 1668, 6331, 1303, 11, 558, 11, 50716], "temperature": 0.0, "avg_logprob": -0.18286404013633728, "compression_ratio": 1.831275720164609, "no_speech_prob": 0.0010194829665124416}, {"id": 919, "seek": 538348, "start": 5390.5199999999995, "end": 5398.36, "text": " like KGNN, right, equivalent to KWO. So, this is equivalent, in the best case, equivalent to WO.", "tokens": [50716, 411, 591, 38, 45, 45, 11, 558, 11, 10344, 281, 591, 54, 46, 13, 407, 11, 341, 307, 10344, 11, 294, 264, 1151, 1389, 11, 10344, 281, 343, 46, 13, 51108], "temperature": 0.0, "avg_logprob": -0.18286404013633728, "compression_ratio": 1.831275720164609, "no_speech_prob": 0.0010194829665124416}, {"id": 920, "seek": 538348, "start": 5399.16, "end": 5402.5199999999995, "text": " It is not equivalent. So, the more expressive WO tests, KWO tests", "tokens": [51148, 467, 307, 406, 10344, 13, 407, 11, 264, 544, 40189, 343, 46, 6921, 11, 591, 54, 46, 6921, 51316], "temperature": 0.0, "avg_logprob": -0.18286404013633728, "compression_ratio": 1.831275720164609, "no_speech_prob": 0.0010194829665124416}, {"id": 921, "seek": 538348, "start": 5403.4, "end": 5407.32, "text": " are more expressive than these architecture, but then it doesn't work on pairs of nodes. It", "tokens": [51360, 366, 544, 40189, 813, 613, 9482, 11, 457, 550, 309, 1177, 380, 589, 322, 15494, 295, 13891, 13, 467, 51556], "temperature": 0.0, "avg_logprob": -0.18286404013633728, "compression_ratio": 1.831275720164609, "no_speech_prob": 0.0010194829665124416}, {"id": 922, "seek": 538348, "start": 5407.32, "end": 5413.16, "text": " considers bigger sets of nodes, right? So, whether you, to call it message-passing or not, you can", "tokens": [51556, 33095, 3801, 6352, 295, 13891, 11, 558, 30, 407, 11, 1968, 291, 11, 281, 818, 309, 3636, 12, 9216, 278, 420, 406, 11, 291, 393, 51848], "temperature": 0.0, "avg_logprob": -0.18286404013633728, "compression_ratio": 1.831275720164609, "no_speech_prob": 0.0010194829665124416}, {"id": 923, "seek": 541316, "start": 5413.24, "end": 5418.28, "text": " argue that, so some people argue that you can also think of it as message-passing, right,", "tokens": [50368, 9695, 300, 11, 370, 512, 561, 9695, 300, 291, 393, 611, 519, 295, 309, 382, 3636, 12, 9216, 278, 11, 558, 11, 50620], "temperature": 0.0, "avg_logprob": -0.1647324350145128, "compression_ratio": 1.3225806451612903, "no_speech_prob": 0.006924497429281473}, {"id": 924, "seek": 541316, "start": 5418.28, "end": 5429.08, "text": " just with a different graph. In my opinion, it's more a semantic question.", "tokens": [50620, 445, 365, 257, 819, 4295, 13, 682, 452, 4800, 11, 309, 311, 544, 257, 47982, 1168, 13, 51160], "temperature": 0.0, "avg_logprob": -0.1647324350145128, "compression_ratio": 1.3225806451612903, "no_speech_prob": 0.006924497429281473}, {"id": 925, "seek": 542908, "start": 5429.64, "end": 5448.36, "text": " So, I don't know. We never really looked at it. Yeah, I don't have an answer.", "tokens": [50392, 407, 11, 286, 500, 380, 458, 13, 492, 1128, 534, 2956, 412, 309, 13, 865, 11, 286, 500, 380, 362, 364, 1867, 13, 51328], "temperature": 0.0, "avg_logprob": -0.18331443972703887, "compression_ratio": 1.1981132075471699, "no_speech_prob": 0.006957361940294504}, {"id": 926, "seek": 542908, "start": 5452.04, "end": 5455.88, "text": " So, it seems like the topological message-passing", "tokens": [51512, 407, 11, 309, 2544, 411, 264, 1192, 4383, 3636, 12, 9216, 278, 51704], "temperature": 0.0, "avg_logprob": -0.18331443972703887, "compression_ratio": 1.1981132075471699, "no_speech_prob": 0.006957361940294504}, {"id": 927, "seek": 545588, "start": 5455.88, "end": 5460.6, "text": " networks and the graph substructure networks both work on the same phenomenon of identifying", "tokens": [50364, 9590, 293, 264, 4295, 4594, 2885, 9590, 1293, 589, 322, 264, 912, 14029, 295, 16696, 50600], "temperature": 0.0, "avg_logprob": -0.1296520149498655, "compression_ratio": 1.912, "no_speech_prob": 0.004576945677399635}, {"id": 928, "seek": 545588, "start": 5460.6, "end": 5466.4400000000005, "text": " and counting substructures. So, is the expressivity of the topological message-passing lower bounded", "tokens": [50600, 293, 13251, 4594, 44513, 13, 407, 11, 307, 264, 5109, 4253, 295, 264, 1192, 4383, 3636, 12, 9216, 278, 3126, 37498, 50892], "temperature": 0.0, "avg_logprob": -0.1296520149498655, "compression_ratio": 1.912, "no_speech_prob": 0.004576945677399635}, {"id": 929, "seek": 545588, "start": 5466.4400000000005, "end": 5470.6, "text": " by the expressivity of the graph substructure network? No, it's slightly different, right,", "tokens": [50892, 538, 264, 5109, 4253, 295, 264, 4295, 4594, 2885, 3209, 30, 883, 11, 309, 311, 4748, 819, 11, 558, 11, 51100], "temperature": 0.0, "avg_logprob": -0.1296520149498655, "compression_ratio": 1.912, "no_speech_prob": 0.004576945677399635}, {"id": 930, "seek": 545588, "start": 5470.6, "end": 5475.56, "text": " because substructure networks, well, first of all, we know they're upper bound, so 3WO. Topological", "tokens": [51100, 570, 4594, 2885, 9590, 11, 731, 11, 700, 295, 439, 11, 321, 458, 436, 434, 6597, 5472, 11, 370, 805, 54, 46, 13, 8840, 4383, 51348], "temperature": 0.0, "avg_logprob": -0.1296520149498655, "compression_ratio": 1.912, "no_speech_prob": 0.004576945677399635}, {"id": 931, "seek": 545588, "start": 5475.56, "end": 5482.4400000000005, "text": " message-passing depends on what kind of substructure. So, there, the kind of side information", "tokens": [51348, 3636, 12, 9216, 278, 5946, 322, 437, 733, 295, 4594, 2885, 13, 407, 11, 456, 11, 264, 733, 295, 1252, 1589, 51692], "temperature": 0.0, "avg_logprob": -0.1296520149498655, "compression_ratio": 1.912, "no_speech_prob": 0.004576945677399635}, {"id": 932, "seek": 548244, "start": 5482.44, "end": 5488.44, "text": " that you assume is what kind of substructures become cells in this, in this cellular complex.", "tokens": [50364, 300, 291, 6552, 307, 437, 733, 295, 4594, 44513, 1813, 5438, 294, 341, 11, 294, 341, 29267, 3997, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11002023011735342, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0126796904951334}, {"id": 933, "seek": 548244, "start": 5488.44, "end": 5491.799999999999, "text": " You can actually go beyond two-dimensional cells. You can go to high-dimensional cells.", "tokens": [50664, 509, 393, 767, 352, 4399, 732, 12, 18759, 5438, 13, 509, 393, 352, 281, 1090, 12, 18759, 5438, 13, 50832], "temperature": 0.0, "avg_logprob": -0.11002023011735342, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0126796904951334}, {"id": 934, "seek": 548244, "start": 5492.599999999999, "end": 5498.5199999999995, "text": " We never tried to go beyond two-dimensional cells. So, depending on the choice of these", "tokens": [50872, 492, 1128, 3031, 281, 352, 4399, 732, 12, 18759, 5438, 13, 407, 11, 5413, 322, 264, 3922, 295, 613, 51168], "temperature": 0.0, "avg_logprob": -0.11002023011735342, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0126796904951334}, {"id": 935, "seek": 548244, "start": 5498.5199999999995, "end": 5503.4, "text": " substructures, what becomes a cell, you might have different levels of expressivity. So,", "tokens": [51168, 4594, 44513, 11, 437, 3643, 257, 2815, 11, 291, 1062, 362, 819, 4358, 295, 5109, 4253, 13, 407, 11, 51412], "temperature": 0.0, "avg_logprob": -0.11002023011735342, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0126796904951334}, {"id": 936, "seek": 548244, "start": 5503.4, "end": 5506.04, "text": " they're distinct methods. I don't think that they're really comparable.", "tokens": [51412, 436, 434, 10644, 7150, 13, 286, 500, 380, 519, 300, 436, 434, 534, 25323, 13, 51544], "temperature": 0.0, "avg_logprob": -0.11002023011735342, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0126796904951334}, {"id": 937, "seek": 550604, "start": 5506.44, "end": 5514.68, "text": " So, something that always confused me about the topological ones is where you, like, you glue", "tokens": [50384, 407, 11, 746, 300, 1009, 9019, 385, 466, 264, 1192, 4383, 2306, 307, 689, 291, 11, 411, 11, 291, 8998, 50796], "temperature": 0.0, "avg_logprob": -0.17193108255212958, "compression_ratio": 1.6401869158878504, "no_speech_prob": 0.0014161724830046296}, {"id": 938, "seek": 550604, "start": 5514.68, "end": 5520.5199999999995, "text": " stuff to structures in the graphs to, like, make them obvious. But can you, like, glue to every", "tokens": [50796, 1507, 281, 9227, 294, 264, 24877, 281, 11, 411, 11, 652, 552, 6322, 13, 583, 393, 291, 11, 411, 11, 8998, 281, 633, 51088], "temperature": 0.0, "avg_logprob": -0.17193108255212958, "compression_ratio": 1.6401869158878504, "no_speech_prob": 0.0014161724830046296}, {"id": 939, "seek": 550604, "start": 5520.5199999999995, "end": 5525.08, "text": " structure or does the structure need to be, like, closed? Because in previous works, it was always", "tokens": [51088, 3877, 420, 775, 264, 3877, 643, 281, 312, 11, 411, 11, 5395, 30, 1436, 294, 3894, 1985, 11, 309, 390, 1009, 51316], "temperature": 0.0, "avg_logprob": -0.17193108255212958, "compression_ratio": 1.6401869158878504, "no_speech_prob": 0.0014161724830046296}, {"id": 940, "seek": 550604, "start": 5525.08, "end": 5530.44, "text": " either cycles or cliques, which are kind of circularly closed.", "tokens": [51316, 2139, 17796, 420, 596, 4911, 11, 597, 366, 733, 295, 16476, 356, 5395, 13, 51584], "temperature": 0.0, "avg_logprob": -0.17193108255212958, "compression_ratio": 1.6401869158878504, "no_speech_prob": 0.0014161724830046296}, {"id": 941, "seek": 553044, "start": 5531.16, "end": 5536.839999999999, "text": " What, I think, basically, the key question is whether it defines a valid cellular complex. I", "tokens": [50400, 708, 11, 286, 519, 11, 1936, 11, 264, 2141, 1168, 307, 1968, 309, 23122, 257, 7363, 29267, 3997, 13, 286, 50684], "temperature": 0.0, "avg_logprob": -0.16197405288468547, "compression_ratio": 1.8482490272373542, "no_speech_prob": 0.004018774721771479}, {"id": 942, "seek": 553044, "start": 5536.839999999999, "end": 5542.759999999999, "text": " think it should be closed. Yeah, my top, yeah, I also think it should be closed, but I have no idea", "tokens": [50684, 519, 309, 820, 312, 5395, 13, 865, 11, 452, 1192, 11, 1338, 11, 286, 611, 519, 309, 820, 312, 5395, 11, 457, 286, 362, 572, 1558, 50980], "temperature": 0.0, "avg_logprob": -0.16197405288468547, "compression_ratio": 1.8482490272373542, "no_speech_prob": 0.004018774721771479}, {"id": 943, "seek": 553044, "start": 5542.759999999999, "end": 5546.919999999999, "text": " about topology. Yeah, from what I remember, it should be closed. But, yeah, so it could be a", "tokens": [50980, 466, 1192, 1793, 13, 865, 11, 490, 437, 286, 1604, 11, 309, 820, 312, 5395, 13, 583, 11, 1338, 11, 370, 309, 727, 312, 257, 51188], "temperature": 0.0, "avg_logprob": -0.16197405288468547, "compression_ratio": 1.8482490272373542, "no_speech_prob": 0.004018774721771479}, {"id": 944, "seek": 553044, "start": 5546.919999999999, "end": 5553.719999999999, "text": " clique, for example, whether it could be a path. I don't think so. So, actually, GSNs can, like,", "tokens": [51188, 44467, 11, 337, 1365, 11, 1968, 309, 727, 312, 257, 3100, 13, 286, 500, 380, 519, 370, 13, 407, 11, 767, 11, 32047, 45, 82, 393, 11, 411, 11, 51528], "temperature": 0.0, "avg_logprob": -0.16197405288468547, "compression_ratio": 1.8482490272373542, "no_speech_prob": 0.004018774721771479}, {"id": 945, "seek": 553044, "start": 5554.839999999999, "end": 5560.04, "text": " count structures that you couldn't, like, glue into, or, like, form into a cellular complex.", "tokens": [51584, 1207, 9227, 300, 291, 2809, 380, 11, 411, 11, 8998, 666, 11, 420, 11, 411, 11, 1254, 666, 257, 29267, 3997, 13, 51844], "temperature": 0.0, "avg_logprob": -0.16197405288468547, "compression_ratio": 1.8482490272373542, "no_speech_prob": 0.004018774721771479}, {"id": 946, "seek": 556004, "start": 5560.04, "end": 5564.36, "text": " So, GSNs can count more general structures, in my opinion. Yeah, so something that doesn't necessarily", "tokens": [50364, 407, 11, 32047, 45, 82, 393, 1207, 544, 2674, 9227, 11, 294, 452, 4800, 13, 865, 11, 370, 746, 300, 1177, 380, 4725, 50580], "temperature": 0.0, "avg_logprob": -0.15237455269725053, "compression_ratio": 1.5802469135802468, "no_speech_prob": 0.00396306999027729}, {"id": 947, "seek": 556004, "start": 5564.36, "end": 5572.5199999999995, "text": " form a cellular complex. Okay, yes, thank you. Inge, I have a question. If we allow more and more", "tokens": [50580, 1254, 257, 29267, 3997, 13, 1033, 11, 2086, 11, 1309, 291, 13, 682, 432, 11, 286, 362, 257, 1168, 13, 759, 321, 2089, 544, 293, 544, 50988], "temperature": 0.0, "avg_logprob": -0.15237455269725053, "compression_ratio": 1.5802469135802468, "no_speech_prob": 0.00396306999027729}, {"id": 948, "seek": 556004, "start": 5572.5199999999995, "end": 5577.72, "text": " notes, the probability of collision in this representation, do we have any results that", "tokens": [50988, 5570, 11, 264, 8482, 295, 24644, 294, 341, 10290, 11, 360, 321, 362, 604, 3542, 300, 51248], "temperature": 0.0, "avg_logprob": -0.15237455269725053, "compression_ratio": 1.5802469135802468, "no_speech_prob": 0.00396306999027729}, {"id": 949, "seek": 556004, "start": 5577.72, "end": 5584.5199999999995, "text": " it became less or, like, insignificant or, like, in more general, do this approach extend a bit", "tokens": [51248, 309, 3062, 1570, 420, 11, 411, 11, 43685, 420, 11, 411, 11, 294, 544, 2674, 11, 360, 341, 3109, 10101, 257, 857, 51588], "temperature": 0.0, "avg_logprob": -0.15237455269725053, "compression_ratio": 1.5802469135802468, "no_speech_prob": 0.00396306999027729}, {"id": 950, "seek": 558452, "start": 5584.52, "end": 5592.52, "text": " more into randomized or, like, stochastic versions so that maybe the guarantee is very low right now", "tokens": [50364, 544, 666, 38513, 420, 11, 411, 11, 342, 8997, 2750, 9606, 370, 300, 1310, 264, 10815, 307, 588, 2295, 558, 586, 50764], "temperature": 0.0, "avg_logprob": -0.24873767652009662, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.005985932890325785}, {"id": 951, "seek": 558452, "start": 5592.52, "end": 5598.84, "text": " from a sort of, well, secondary run. So, the expressiveness, like, theoretical is pretty low,", "tokens": [50764, 490, 257, 1333, 295, 11, 731, 11, 11396, 1190, 13, 407, 11, 264, 5109, 8477, 11, 411, 11, 20864, 307, 1238, 2295, 11, 51080], "temperature": 0.0, "avg_logprob": -0.24873767652009662, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.005985932890325785}, {"id": 952, "seek": 558452, "start": 5598.84, "end": 5603.88, "text": " but then, if we allow a little randomness, then, actually, like, no randomness.", "tokens": [51080, 457, 550, 11, 498, 321, 2089, 257, 707, 4974, 1287, 11, 550, 11, 767, 11, 411, 11, 572, 4974, 1287, 13, 51332], "temperature": 0.0, "avg_logprob": -0.24873767652009662, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.005985932890325785}, {"id": 953, "seek": 558452, "start": 5603.88, "end": 5607.320000000001, "text": " Sorry, you're talking about random graph models, right? Something like stochastic walk models.", "tokens": [51332, 4919, 11, 291, 434, 1417, 466, 4974, 4295, 5245, 11, 558, 30, 6595, 411, 342, 8997, 2750, 1792, 5245, 13, 51504], "temperature": 0.0, "avg_logprob": -0.24873767652009662, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.005985932890325785}, {"id": 954, "seek": 560732, "start": 5608.12, "end": 5614.2, "text": " Yeah. You can probably analyze what happens to this kind of graphs given", "tokens": [50404, 865, 13, 509, 393, 1391, 12477, 437, 2314, 281, 341, 733, 295, 24877, 2212, 50708], "temperature": 0.0, "avg_logprob": -0.17244277422941184, "compression_ratio": 1.6289592760180995, "no_speech_prob": 0.004913036245852709}, {"id": 955, "seek": 560732, "start": 5616.5199999999995, "end": 5622.599999999999, "text": " certain type of message passing architecture. I have seen papers of this kind, nothing specific", "tokens": [50824, 1629, 2010, 295, 3636, 8437, 9482, 13, 286, 362, 1612, 10577, 295, 341, 733, 11, 1825, 2685, 51128], "temperature": 0.0, "avg_logprob": -0.17244277422941184, "compression_ratio": 1.6289592760180995, "no_speech_prob": 0.004913036245852709}, {"id": 956, "seek": 560732, "start": 5622.599999999999, "end": 5629.799999999999, "text": " that comes to my mind. Yeah, you can probably, I don't know, compute some probability under the", "tokens": [51128, 300, 1487, 281, 452, 1575, 13, 865, 11, 291, 393, 1391, 11, 286, 500, 380, 458, 11, 14722, 512, 8482, 833, 264, 51488], "temperature": 0.0, "avg_logprob": -0.17244277422941184, "compression_ratio": 1.6289592760180995, "no_speech_prob": 0.004913036245852709}, {"id": 957, "seek": 560732, "start": 5629.799999999999, "end": 5635.88, "text": " assumption of certain distribution of input random graphs of distinguishing within them or not.", "tokens": [51488, 15302, 295, 1629, 7316, 295, 4846, 4974, 24877, 295, 11365, 3807, 1951, 552, 420, 406, 13, 51792], "temperature": 0.0, "avg_logprob": -0.17244277422941184, "compression_ratio": 1.6289592760180995, "no_speech_prob": 0.004913036245852709}, {"id": 958, "seek": 563588, "start": 5636.84, "end": 5637.32, "text": " Yeah.", "tokens": [50412, 865, 13, 50436], "temperature": 0.0, "avg_logprob": -0.2143064663733965, "compression_ratio": 1.4486486486486487, "no_speech_prob": 0.0010932163568213582}, {"id": 959, "seek": 563588, "start": 5642.4400000000005, "end": 5648.28, "text": " So, a question regarding the size of the graph in practice. We are talking about, like, in terms of", "tokens": [50692, 407, 11, 257, 1168, 8595, 264, 2744, 295, 264, 4295, 294, 3124, 13, 492, 366, 1417, 466, 11, 411, 11, 294, 2115, 295, 50984], "temperature": 0.0, "avg_logprob": -0.2143064663733965, "compression_ratio": 1.4486486486486487, "no_speech_prob": 0.0010932163568213582}, {"id": 960, "seek": 563588, "start": 5648.28, "end": 5655.72, "text": " maybe not count and each count, like, what to say in practice for this KWL?", "tokens": [50984, 1310, 406, 1207, 293, 1184, 1207, 11, 411, 11, 437, 281, 584, 294, 3124, 337, 341, 591, 54, 43, 30, 51356], "temperature": 0.0, "avg_logprob": -0.2143064663733965, "compression_ratio": 1.4486486486486487, "no_speech_prob": 0.0010932163568213582}, {"id": 961, "seek": 563588, "start": 5657.08, "end": 5661.88, "text": " So, KWL doesn't scale well, right? So, if it's n to the power k, so also computational", "tokens": [51424, 407, 11, 591, 54, 43, 1177, 380, 4373, 731, 11, 558, 30, 407, 11, 498, 309, 311, 297, 281, 264, 1347, 350, 11, 370, 611, 28270, 51664], "temperature": 0.0, "avg_logprob": -0.2143064663733965, "compression_ratio": 1.4486486486486487, "no_speech_prob": 0.0010932163568213582}, {"id": 962, "seek": 566188, "start": 5661.96, "end": 5667.24, "text": " complexity versus space complexity, I don't think that you can, in practice, go beyond", "tokens": [50368, 14024, 5717, 1901, 14024, 11, 286, 500, 380, 519, 300, 291, 393, 11, 294, 3124, 11, 352, 4399, 50632], "temperature": 0.0, "avg_logprob": -0.16300315856933595, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.004997407551854849}, {"id": 963, "seek": 566188, "start": 5667.24, "end": 5673.32, "text": " 3WL equivalent. Now, there are sparse versions of these architectures, so you can do slightly", "tokens": [50632, 805, 54, 43, 10344, 13, 823, 11, 456, 366, 637, 11668, 9606, 295, 613, 6331, 1303, 11, 370, 291, 393, 360, 4748, 50936], "temperature": 0.0, "avg_logprob": -0.16300315856933595, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.004997407551854849}, {"id": 964, "seek": 566188, "start": 5673.32, "end": 5678.84, "text": " better, but I think they are mostly useful for proving theorems. So, basically, because you establish", "tokens": [50936, 1101, 11, 457, 286, 519, 436, 366, 5240, 4420, 337, 27221, 10299, 2592, 13, 407, 11, 1936, 11, 570, 291, 8327, 51212], "temperature": 0.0, "avg_logprob": -0.16300315856933595, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.004997407551854849}, {"id": 965, "seek": 566188, "start": 5679.4800000000005, "end": 5685.08, "text": " a link to the device for element hierarchy, then you can say that basically, if your network is", "tokens": [51244, 257, 2113, 281, 264, 4302, 337, 4478, 22333, 11, 550, 291, 393, 584, 300, 1936, 11, 498, 428, 3209, 307, 51524], "temperature": 0.0, "avg_logprob": -0.16300315856933595, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.004997407551854849}, {"id": 966, "seek": 566188, "start": 5685.08, "end": 5690.68, "text": " equivalent to one of these methods, then you know how expressive it is. Thank you.", "tokens": [51524, 10344, 281, 472, 295, 613, 7150, 11, 550, 291, 458, 577, 40189, 309, 307, 13, 1044, 291, 13, 51804], "temperature": 0.0, "avg_logprob": -0.16300315856933595, "compression_ratio": 1.6948529411764706, "no_speech_prob": 0.004997407551854849}, {"id": 967, "seek": 569188, "start": 5692.6, "end": 5698.6, "text": " Yeah. So, there was a paper sometime ago claiming that most of the graphs in the most common benchmarks", "tokens": [50400, 865, 13, 407, 11, 456, 390, 257, 3035, 15053, 2057, 19232, 300, 881, 295, 264, 24877, 294, 264, 881, 2689, 43751, 50700], "temperature": 0.0, "avg_logprob": -0.1401796918926817, "compression_ratio": 1.4641509433962263, "no_speech_prob": 0.0031288329046219587}, {"id": 968, "seek": 569188, "start": 5698.6, "end": 5707.32, "text": " can actually be distinguished by 1WL. Now, I don't know if you read about it. And my question is,", "tokens": [50700, 393, 767, 312, 21702, 538, 502, 54, 43, 13, 823, 11, 286, 500, 380, 458, 498, 291, 1401, 466, 309, 13, 400, 452, 1168, 307, 11, 51136], "temperature": 0.0, "avg_logprob": -0.1401796918926817, "compression_ratio": 1.4641509433962263, "no_speech_prob": 0.0031288329046219587}, {"id": 969, "seek": 569188, "start": 5707.32, "end": 5713.56, "text": " now we've seen that even in those cases, more expressive GNNs still get better results. But", "tokens": [51136, 586, 321, 600, 1612, 300, 754, 294, 729, 3331, 11, 544, 40189, 46411, 45, 82, 920, 483, 1101, 3542, 13, 583, 51448], "temperature": 0.0, "avg_logprob": -0.1401796918926817, "compression_ratio": 1.4641509433962263, "no_speech_prob": 0.0031288329046219587}, {"id": 970, "seek": 569188, "start": 5713.56, "end": 5719.16, "text": " then what's the reason? So, again, generalization could be one possibility, right, because you", "tokens": [51448, 550, 437, 311, 264, 1778, 30, 407, 11, 797, 11, 2674, 2144, 727, 312, 472, 7959, 11, 558, 11, 570, 291, 51728], "temperature": 0.0, "avg_logprob": -0.1401796918926817, "compression_ratio": 1.4641509433962263, "no_speech_prob": 0.0031288329046219587}, {"id": 971, "seek": 571916, "start": 5719.16, "end": 5724.2, "text": " train on one set of graphs and then you test on another set of graphs. Then,", "tokens": [50364, 3847, 322, 472, 992, 295, 24877, 293, 550, 291, 1500, 322, 1071, 992, 295, 24877, 13, 1396, 11, 50616], "temperature": 0.0, "avg_logprob": -0.2622641294430464, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.004143335856497288}, {"id": 972, "seek": 571916, "start": 5724.84, "end": 5728.44, "text": " double your tests, in general, they're designed for fish less graphs, right? So,", "tokens": [50648, 3834, 428, 6921, 11, 294, 2674, 11, 436, 434, 4761, 337, 3506, 1570, 24877, 11, 558, 30, 407, 11, 50828], "temperature": 0.0, "avg_logprob": -0.2622641294430464, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.004143335856497288}, {"id": 973, "seek": 571916, "start": 5728.44, "end": 5732.36, "text": " they only consider the structure. So, how you treat fish is also important.", "tokens": [50828, 436, 787, 1949, 264, 3877, 13, 407, 11, 577, 291, 2387, 3506, 307, 611, 1021, 13, 51024], "temperature": 0.0, "avg_logprob": -0.2622641294430464, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.004143335856497288}, {"id": 974, "seek": 571916, "start": 5734.76, "end": 5746.92, "text": " Nice sense. I think we can stop here and we have, like,", "tokens": [51144, 5490, 2020, 13, 286, 519, 321, 393, 1590, 510, 293, 321, 362, 11, 411, 11, 51752], "temperature": 0.0, "avg_logprob": -0.2622641294430464, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.004143335856497288}, {"id": 975, "seek": 574692, "start": 5746.92, "end": 5749.64, "text": " have an hour for a coffee break and then we resume afterwards.", "tokens": [50364, 362, 364, 1773, 337, 257, 4982, 1821, 293, 550, 321, 15358, 10543, 13, 50500], "temperature": 0.0, "avg_logprob": -0.25328385202508225, "compression_ratio": 1.4226190476190477, "no_speech_prob": 0.014172665774822235}, {"id": 976, "seek": 574692, "start": 5749.64, "end": 5754.04, "text": " So, I will put the slide with the caffeine molecule that everybody likes. And then,", "tokens": [50500, 407, 11, 286, 486, 829, 264, 4137, 365, 264, 31261, 15582, 300, 2201, 5902, 13, 400, 550, 11, 50720], "temperature": 0.0, "avg_logprob": -0.25328385202508225, "compression_ratio": 1.4226190476190477, "no_speech_prob": 0.014172665774822235}, {"id": 977, "seek": 574692, "start": 5764.12, "end": 5771.64, "text": " yeah, basically, we stopped at this overview of different more expressive architectures. So,", "tokens": [51224, 1338, 11, 1936, 11, 321, 5936, 412, 341, 12492, 295, 819, 544, 40189, 6331, 1303, 13, 407, 11, 51600], "temperature": 0.0, "avg_logprob": -0.25328385202508225, "compression_ratio": 1.4226190476190477, "no_speech_prob": 0.014172665774822235}, {"id": 978, "seek": 577164, "start": 5771.64, "end": 5779.56, "text": " let's now talk about, guys, let's start. So, basically, the situation that we have with", "tokens": [50364, 718, 311, 586, 751, 466, 11, 1074, 11, 718, 311, 722, 13, 407, 11, 1936, 11, 264, 2590, 300, 321, 362, 365, 50760], "temperature": 0.0, "avg_logprob": -0.14266011118888855, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.0028150847647339106}, {"id": 979, "seek": 577164, "start": 5781.56, "end": 5787.160000000001, "text": " the expressive power of graph neural networks, right? So, on the one side, we have the WL hierarchy,", "tokens": [50860, 264, 40189, 1347, 295, 4295, 18161, 9590, 11, 558, 30, 407, 11, 322, 264, 472, 1252, 11, 321, 362, 264, 343, 43, 22333, 11, 51140], "temperature": 0.0, "avg_logprob": -0.14266011118888855, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.0028150847647339106}, {"id": 980, "seek": 577164, "start": 5787.160000000001, "end": 5794.200000000001, "text": " right? So, increasingly, more powerful, more expressive graph isomorphism tests, right? And", "tokens": [51140, 558, 30, 407, 11, 12980, 11, 544, 4005, 11, 544, 40189, 4295, 307, 32702, 1434, 6921, 11, 558, 30, 400, 51492], "temperature": 0.0, "avg_logprob": -0.14266011118888855, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.0028150847647339106}, {"id": 981, "seek": 577164, "start": 5794.200000000001, "end": 5800.6, "text": " we can find analogies between certain GNN architectures to these tests. On the other hand,", "tokens": [51492, 321, 393, 915, 16660, 530, 1296, 1629, 46411, 45, 6331, 1303, 281, 613, 6921, 13, 1282, 264, 661, 1011, 11, 51812], "temperature": 0.0, "avg_logprob": -0.14266011118888855, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.0028150847647339106}, {"id": 982, "seek": 580060, "start": 5801.4800000000005, "end": 5807.240000000001, "text": " the assumption that we are given a graph and we do message passing on this graph might be", "tokens": [50408, 264, 15302, 300, 321, 366, 2212, 257, 4295, 293, 321, 360, 3636, 8437, 322, 341, 4295, 1062, 312, 50696], "temperature": 0.0, "avg_logprob": -0.08727689374957168, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0009581596241332591}, {"id": 983, "seek": 580060, "start": 5807.240000000001, "end": 5813.08, "text": " restrictive in the sense that some graphs are not friendly for message passing. And in this case,", "tokens": [50696, 43220, 294, 264, 2020, 300, 512, 24877, 366, 406, 9208, 337, 3636, 8437, 13, 400, 294, 341, 1389, 11, 50988], "temperature": 0.0, "avg_logprob": -0.08727689374957168, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0009581596241332591}, {"id": 984, "seek": 580060, "start": 5813.08, "end": 5817.8, "text": " you typically, what you would like to do is to change the graph so that message passing works", "tokens": [50988, 291, 5850, 11, 437, 291, 576, 411, 281, 360, 307, 281, 1319, 264, 4295, 370, 300, 3636, 8437, 1985, 51224], "temperature": 0.0, "avg_logprob": -0.08727689374957168, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0009581596241332591}, {"id": 985, "seek": 580060, "start": 5817.8, "end": 5823.56, "text": " better. This is a very broad category of methods that are called graph rewiring. So, what happens", "tokens": [51224, 1101, 13, 639, 307, 257, 588, 4152, 7719, 295, 7150, 300, 366, 1219, 4295, 319, 86, 5057, 13, 407, 11, 437, 2314, 51512], "temperature": 0.0, "avg_logprob": -0.08727689374957168, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0009581596241332591}, {"id": 986, "seek": 580060, "start": 5823.56, "end": 5829.160000000001, "text": " is that you have a gap between theory and practice, right? So, the theory, in order to make the link", "tokens": [51512, 307, 300, 291, 362, 257, 7417, 1296, 5261, 293, 3124, 11, 558, 30, 407, 11, 264, 5261, 11, 294, 1668, 281, 652, 264, 2113, 51792], "temperature": 0.0, "avg_logprob": -0.08727689374957168, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0009581596241332591}, {"id": 987, "seek": 582916, "start": 5829.16, "end": 5834.599999999999, "text": " to the WL tests, you need to use exactly the same input graph. The practice tells you that", "tokens": [50364, 281, 264, 343, 43, 6921, 11, 291, 643, 281, 764, 2293, 264, 912, 4846, 4295, 13, 440, 3124, 5112, 291, 300, 50636], "temperature": 0.0, "avg_logprob": -0.09687026341756184, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.003961621783673763}, {"id": 988, "seek": 582916, "start": 5834.599999999999, "end": 5843.08, "text": " sometimes you don't want to do it. So, as always, there is this gap. So, if you think maybe take", "tokens": [50636, 2171, 291, 500, 380, 528, 281, 360, 309, 13, 407, 11, 382, 1009, 11, 456, 307, 341, 7417, 13, 407, 11, 498, 291, 519, 1310, 747, 51060], "temperature": 0.0, "avg_logprob": -0.09687026341756184, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.003961621783673763}, {"id": 989, "seek": 582916, "start": 5843.08, "end": 5850.599999999999, "text": " a step back and look at the different types of graph neural network architectures, so the traditional", "tokens": [51060, 257, 1823, 646, 293, 574, 412, 264, 819, 3467, 295, 4295, 18161, 3209, 6331, 1303, 11, 370, 264, 5164, 51436], "temperature": 0.0, "avg_logprob": -0.09687026341756184, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.003961621783673763}, {"id": 990, "seek": 582916, "start": 5850.599999999999, "end": 5857.639999999999, "text": " approach in graph neural networks is you're given the input graph and it's both part of the input", "tokens": [51436, 3109, 294, 4295, 18161, 9590, 307, 291, 434, 2212, 264, 4846, 4295, 293, 309, 311, 1293, 644, 295, 264, 4846, 51788], "temperature": 0.0, "avg_logprob": -0.09687026341756184, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.003961621783673763}, {"id": 991, "seek": 585764, "start": 5857.64, "end": 5862.12, "text": " and part of the computation, right? Because you use the input graph to send information on it,", "tokens": [50364, 293, 644, 295, 264, 24903, 11, 558, 30, 1436, 291, 764, 264, 4846, 4295, 281, 2845, 1589, 322, 309, 11, 50588], "temperature": 0.0, "avg_logprob": -0.13317030873791924, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.0014635126572102308}, {"id": 992, "seek": 585764, "start": 5862.12, "end": 5868.68, "text": " right? So, it's both input and computational object. Now, as we've seen, you can do many", "tokens": [50588, 558, 30, 407, 11, 309, 311, 1293, 4846, 293, 28270, 2657, 13, 823, 11, 382, 321, 600, 1612, 11, 291, 393, 360, 867, 50916], "temperature": 0.0, "avg_logprob": -0.13317030873791924, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.0014635126572102308}, {"id": 993, "seek": 585764, "start": 5868.68, "end": 5873.240000000001, "text": " different things, right? So, you can enrich the graph with some positional or structural features.", "tokens": [50916, 819, 721, 11, 558, 30, 407, 11, 291, 393, 18849, 264, 4295, 365, 512, 2535, 304, 420, 15067, 4122, 13, 51144], "temperature": 0.0, "avg_logprob": -0.13317030873791924, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.0014635126572102308}, {"id": 994, "seek": 585764, "start": 5873.96, "end": 5880.04, "text": " You can lift it into a high-dimensional topological space like Simplisher or Cellar complex, right?", "tokens": [51180, 509, 393, 5533, 309, 666, 257, 1090, 12, 18759, 1192, 4383, 1901, 411, 3998, 564, 9807, 420, 28859, 289, 3997, 11, 558, 30, 51484], "temperature": 0.0, "avg_logprob": -0.13317030873791924, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.0014635126572102308}, {"id": 995, "seek": 585764, "start": 5880.04, "end": 5885.64, "text": " And do message passing on this object. You can again enrich the graph by considering a collection", "tokens": [51484, 400, 360, 3636, 8437, 322, 341, 2657, 13, 509, 393, 797, 18849, 264, 4295, 538, 8079, 257, 5765, 51764], "temperature": 0.0, "avg_logprob": -0.13317030873791924, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.0014635126572102308}, {"id": 996, "seek": 588564, "start": 5885.64, "end": 5892.200000000001, "text": " of subgraphs, right? And do maybe some other more exotic type of aggregation that respects the", "tokens": [50364, 295, 1422, 34091, 82, 11, 558, 30, 400, 360, 1310, 512, 661, 544, 27063, 2010, 295, 16743, 399, 300, 24126, 264, 50692], "temperature": 0.0, "avg_logprob": -0.14312130471934442, "compression_ratio": 1.6016949152542372, "no_speech_prob": 0.00204310636036098}, {"id": 997, "seek": 588564, "start": 5892.200000000001, "end": 5900.6, "text": " product symmetry group. You can also enrich your representation by considering also the", "tokens": [50692, 1674, 25440, 1594, 13, 509, 393, 611, 18849, 428, 10290, 538, 8079, 611, 264, 51112], "temperature": 0.0, "avg_logprob": -0.14312130471934442, "compression_ratio": 1.6016949152542372, "no_speech_prob": 0.00204310636036098}, {"id": 998, "seek": 588564, "start": 5900.6, "end": 5905.72, "text": " symmetry of the data, right? So, these are equivalent GNNs, if you have time, I can talk about it.", "tokens": [51112, 25440, 295, 264, 1412, 11, 558, 30, 407, 11, 613, 366, 10344, 46411, 45, 82, 11, 498, 291, 362, 565, 11, 286, 393, 751, 466, 309, 13, 51368], "temperature": 0.0, "avg_logprob": -0.14312130471934442, "compression_ratio": 1.6016949152542372, "no_speech_prob": 0.00204310636036098}, {"id": 999, "seek": 588564, "start": 5905.72, "end": 5910.04, "text": " And then maybe in some special cases, your graph will have special structure like a grid, right?", "tokens": [51368, 400, 550, 1310, 294, 512, 2121, 3331, 11, 428, 4295, 486, 362, 2121, 3877, 411, 257, 10748, 11, 558, 30, 51584], "temperature": 0.0, "avg_logprob": -0.14312130471934442, "compression_ratio": 1.6016949152542372, "no_speech_prob": 0.00204310636036098}, {"id": 1000, "seek": 591004, "start": 5910.04, "end": 5915.64, "text": " And in this case, you can maybe do more specific choices. For example, you can abandon the local", "tokens": [50364, 400, 294, 341, 1389, 11, 291, 393, 1310, 360, 544, 2685, 7994, 13, 1171, 1365, 11, 291, 393, 9072, 264, 2654, 50644], "temperature": 0.0, "avg_logprob": -0.0949200129104873, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.007772312965244055}, {"id": 1001, "seek": 591004, "start": 5915.64, "end": 5922.2, "text": " permutation invariance. So, you will get back to convolutions. So, basically, the common denominator", "tokens": [50644, 4784, 11380, 33270, 719, 13, 407, 11, 291, 486, 483, 646, 281, 3754, 15892, 13, 407, 11, 1936, 11, 264, 2689, 20687, 50972], "temperature": 0.0, "avg_logprob": -0.0949200129104873, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.007772312965244055}, {"id": 1002, "seek": 591004, "start": 5922.2, "end": 5925.88, "text": " of these approaches is that you have more structure, right? Sometimes you can assume,", "tokens": [50972, 295, 613, 11587, 307, 300, 291, 362, 544, 3877, 11, 558, 30, 4803, 291, 393, 6552, 11, 51156], "temperature": 0.0, "avg_logprob": -0.0949200129104873, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.007772312965244055}, {"id": 1003, "seek": 591004, "start": 5926.5199999999995, "end": 5933.08, "text": " sometimes you can invent, right? But that's the idea. On the other hand, you can say that you", "tokens": [51188, 2171, 291, 393, 7962, 11, 558, 30, 583, 300, 311, 264, 1558, 13, 1282, 264, 661, 1011, 11, 291, 393, 584, 300, 291, 51516], "temperature": 0.0, "avg_logprob": -0.0949200129104873, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.007772312965244055}, {"id": 1004, "seek": 591004, "start": 5933.08, "end": 5937.96, "text": " don't like the graph that is given to you, right? And you choose to completely ignore it, right?", "tokens": [51516, 500, 380, 411, 264, 4295, 300, 307, 2212, 281, 291, 11, 558, 30, 400, 291, 2826, 281, 2584, 11200, 309, 11, 558, 30, 51760], "temperature": 0.0, "avg_logprob": -0.0949200129104873, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.007772312965244055}, {"id": 1005, "seek": 593796, "start": 5937.96, "end": 5944.2, "text": " So, you just assume empty edge sets. So, you're back to the bare bone, right? So, the object is", "tokens": [50364, 407, 11, 291, 445, 6552, 6707, 4691, 6352, 13, 407, 11, 291, 434, 646, 281, 264, 6949, 9026, 11, 558, 30, 407, 11, 264, 2657, 307, 50676], "temperature": 0.0, "avg_logprob": -0.1216136625149106, "compression_ratio": 1.8371335504885993, "no_speech_prob": 0.0012816662201657891}, {"id": 1006, "seek": 593796, "start": 5944.2, "end": 5950.04, "text": " just a collection of nodes, which is a set. And these are the architectures that are called deep", "tokens": [50676, 445, 257, 5765, 295, 13891, 11, 597, 307, 257, 992, 13, 400, 613, 366, 264, 6331, 1303, 300, 366, 1219, 2452, 50968], "temperature": 0.0, "avg_logprob": -0.1216136625149106, "compression_ratio": 1.8371335504885993, "no_speech_prob": 0.0012816662201657891}, {"id": 1007, "seek": 593796, "start": 5950.04, "end": 5954.12, "text": " sets or point nets. So, that's the simplest case of graphing electrics, right? Where you don't", "tokens": [50968, 6352, 420, 935, 36170, 13, 407, 11, 300, 311, 264, 22811, 1389, 295, 1295, 79, 571, 7072, 1167, 11, 558, 30, 2305, 291, 500, 380, 51172], "temperature": 0.0, "avg_logprob": -0.1216136625149106, "compression_ratio": 1.8371335504885993, "no_speech_prob": 0.0012816662201657891}, {"id": 1008, "seek": 593796, "start": 5954.12, "end": 5958.68, "text": " have a graph, you just have the nodes. The other extreme of this is when you allow interaction", "tokens": [51172, 362, 257, 4295, 11, 291, 445, 362, 264, 13891, 13, 440, 661, 8084, 295, 341, 307, 562, 291, 2089, 9285, 51400], "temperature": 0.0, "avg_logprob": -0.1216136625149106, "compression_ratio": 1.8371335504885993, "no_speech_prob": 0.0012816662201657891}, {"id": 1009, "seek": 593796, "start": 5958.68, "end": 5963.08, "text": " between every pair of nodes, right? Again, you don't trust your graph for some reason. So,", "tokens": [51400, 1296, 633, 6119, 295, 13891, 11, 558, 30, 3764, 11, 291, 500, 380, 3361, 428, 4295, 337, 512, 1778, 13, 407, 11, 51620], "temperature": 0.0, "avg_logprob": -0.1216136625149106, "compression_ratio": 1.8371335504885993, "no_speech_prob": 0.0012816662201657891}, {"id": 1010, "seek": 593796, "start": 5963.08, "end": 5966.28, "text": " every pair of nodes can interact. So, it's a complete or a fully connected graph. And this", "tokens": [51620, 633, 6119, 295, 13891, 393, 4648, 13, 407, 11, 309, 311, 257, 3566, 420, 257, 4498, 4582, 4295, 13, 400, 341, 51780], "temperature": 0.0, "avg_logprob": -0.1216136625149106, "compression_ratio": 1.8371335504885993, "no_speech_prob": 0.0012816662201657891}, {"id": 1011, "seek": 596628, "start": 5966.28, "end": 5971.24, "text": " is how transformers work, right? So, in this case, you will use, for example, the attentional", "tokens": [50364, 307, 577, 4088, 433, 589, 11, 558, 30, 407, 11, 294, 341, 1389, 11, 291, 486, 764, 11, 337, 1365, 11, 264, 3202, 304, 50612], "temperature": 0.0, "avg_logprob": -0.11254709820414699, "compression_ratio": 1.7661870503597121, "no_speech_prob": 0.0008110659546218812}, {"id": 1012, "seek": 596628, "start": 5971.24, "end": 5977.08, "text": " flavor of the graph in electrical architectures. And you will learn the right graph for your task,", "tokens": [50612, 6813, 295, 264, 4295, 294, 12147, 6331, 1303, 13, 400, 291, 486, 1466, 264, 558, 4295, 337, 428, 5633, 11, 50904], "temperature": 0.0, "avg_logprob": -0.11254709820414699, "compression_ratio": 1.7661870503597121, "no_speech_prob": 0.0008110659546218812}, {"id": 1013, "seek": 596628, "start": 5977.08, "end": 5981.88, "text": " right? In a sense, through the attention mechanism, for example. There is another class of methods,", "tokens": [50904, 558, 30, 682, 257, 2020, 11, 807, 264, 3202, 7513, 11, 337, 1365, 13, 821, 307, 1071, 1508, 295, 7150, 11, 51144], "temperature": 0.0, "avg_logprob": -0.11254709820414699, "compression_ratio": 1.7661870503597121, "no_speech_prob": 0.0008110659546218812}, {"id": 1014, "seek": 596628, "start": 5981.88, "end": 5987.5599999999995, "text": " and this is what we call graph rewiring, where you say, okay, I don't like the graph that is given.", "tokens": [51144, 293, 341, 307, 437, 321, 818, 4295, 319, 86, 5057, 11, 689, 291, 584, 11, 1392, 11, 286, 500, 380, 411, 264, 4295, 300, 307, 2212, 13, 51428], "temperature": 0.0, "avg_logprob": -0.11254709820414699, "compression_ratio": 1.7661870503597121, "no_speech_prob": 0.0008110659546218812}, {"id": 1015, "seek": 596628, "start": 5987.5599999999995, "end": 5992.84, "text": " I would like to change it a little bit, maybe. And this way, the graph will become, in some sense,", "tokens": [51428, 286, 576, 411, 281, 1319, 309, 257, 707, 857, 11, 1310, 13, 400, 341, 636, 11, 264, 4295, 486, 1813, 11, 294, 512, 2020, 11, 51692], "temperature": 0.0, "avg_logprob": -0.11254709820414699, "compression_ratio": 1.7661870503597121, "no_speech_prob": 0.0008110659546218812}, {"id": 1016, "seek": 599284, "start": 5992.84, "end": 5998.76, "text": " better for message passing. Okay, we'll talk about it in more details. So, let's talk about", "tokens": [50364, 1101, 337, 3636, 8437, 13, 1033, 11, 321, 603, 751, 466, 309, 294, 544, 4365, 13, 407, 11, 718, 311, 751, 466, 50660], "temperature": 0.0, "avg_logprob": -0.12646656036376952, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.001005117199383676}, {"id": 1017, "seek": 599284, "start": 5998.76, "end": 6004.6, "text": " graph rewiring and specifically about transformers. So, in transformers, you assume that the graph is", "tokens": [50660, 4295, 319, 86, 5057, 293, 4682, 466, 4088, 433, 13, 407, 11, 294, 4088, 433, 11, 291, 6552, 300, 264, 4295, 307, 50952], "temperature": 0.0, "avg_logprob": -0.12646656036376952, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.001005117199383676}, {"id": 1018, "seek": 599284, "start": 6004.6, "end": 6009.72, "text": " complete, right? So, every node is connected to every node. And if you try to apply a convolutional", "tokens": [50952, 3566, 11, 558, 30, 407, 11, 633, 9984, 307, 4582, 281, 633, 9984, 13, 400, 498, 291, 853, 281, 3079, 257, 45216, 304, 51208], "temperature": 0.0, "avg_logprob": -0.12646656036376952, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.001005117199383676}, {"id": 1019, "seek": 599284, "start": 6009.72, "end": 6017.16, "text": " style GNN architecture, right, that depends on the coefficients of the, representing the structure", "tokens": [51208, 3758, 46411, 45, 9482, 11, 558, 11, 300, 5946, 322, 264, 31994, 295, 264, 11, 13460, 264, 3877, 51580], "temperature": 0.0, "avg_logprob": -0.12646656036376952, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.001005117199383676}, {"id": 1020, "seek": 601716, "start": 6017.16, "end": 6023.5599999999995, "text": " of the graph, then, basically, here, the sum goes over all the nodes, and basically, this argument", "tokens": [50364, 295, 264, 4295, 11, 550, 11, 1936, 11, 510, 11, 264, 2408, 1709, 670, 439, 264, 13891, 11, 293, 1936, 11, 341, 6770, 50684], "temperature": 0.0, "avg_logprob": -0.12128946160068031, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.003187018446624279}, {"id": 1021, "seek": 601716, "start": 6023.5599999999995, "end": 6027.16, "text": " is equal for every node. So, it's not informative, right? You're not adding any information.", "tokens": [50684, 307, 2681, 337, 633, 9984, 13, 407, 11, 309, 311, 406, 27759, 11, 558, 30, 509, 434, 406, 5127, 604, 1589, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12128946160068031, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.003187018446624279}, {"id": 1022, "seek": 601716, "start": 6028.12, "end": 6034.599999999999, "text": " So, you need to use at least an attentional architecture. And in this case, this is already,", "tokens": [50912, 407, 11, 291, 643, 281, 764, 412, 1935, 364, 3202, 304, 9482, 13, 400, 294, 341, 1389, 11, 341, 307, 1217, 11, 51236], "temperature": 0.0, "avg_logprob": -0.12128946160068031, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.003187018446624279}, {"id": 1023, "seek": 601716, "start": 6034.599999999999, "end": 6039.5599999999995, "text": " this already looks like a transformer. And you can think of attention weights as a kind of", "tokens": [51236, 341, 1217, 1542, 411, 257, 31782, 13, 400, 291, 393, 519, 295, 3202, 17443, 382, 257, 733, 295, 51484], "temperature": 0.0, "avg_logprob": -0.12128946160068031, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.003187018446624279}, {"id": 1024, "seek": 601716, "start": 6039.5599999999995, "end": 6046.2, "text": " learned graph adjacency, right? That depends on this task that you're trying to solve. And this is", "tokens": [51484, 3264, 4295, 22940, 3020, 11, 558, 30, 663, 5946, 322, 341, 5633, 300, 291, 434, 1382, 281, 5039, 13, 400, 341, 307, 51816], "temperature": 0.0, "avg_logprob": -0.12128946160068031, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.003187018446624279}, {"id": 1025, "seek": 604620, "start": 6046.28, "end": 6051.96, "text": " a special case of GNNs. Now, of course, in natural language processing, many tasks that you want to", "tokens": [50368, 257, 2121, 1389, 295, 46411, 45, 82, 13, 823, 11, 295, 1164, 11, 294, 3303, 2856, 9007, 11, 867, 9608, 300, 291, 528, 281, 50652], "temperature": 0.0, "avg_logprob": -0.07120064685219213, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.002337629208341241}, {"id": 1026, "seek": 604620, "start": 6051.96, "end": 6056.76, "text": " solve do not require permutation invariance. You actually want them to be not permutation", "tokens": [50652, 5039, 360, 406, 3651, 4784, 11380, 33270, 719, 13, 509, 767, 528, 552, 281, 312, 406, 4784, 11380, 50892], "temperature": 0.0, "avg_logprob": -0.07120064685219213, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.002337629208341241}, {"id": 1027, "seek": 604620, "start": 6056.76, "end": 6061.8, "text": " invariance. For example, you want to depend on the order of words in the sentence. So, this is", "tokens": [50892, 33270, 719, 13, 1171, 1365, 11, 291, 528, 281, 5672, 322, 264, 1668, 295, 2283, 294, 264, 8174, 13, 407, 11, 341, 307, 51144], "temperature": 0.0, "avg_logprob": -0.07120064685219213, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.002337629208341241}, {"id": 1028, "seek": 604620, "start": 6061.8, "end": 6068.28, "text": " typically achieved by adding positional encoding. So, in the simplest case, you just equip every", "tokens": [51144, 5850, 11042, 538, 5127, 2535, 304, 43430, 13, 407, 11, 294, 264, 22811, 1389, 11, 291, 445, 5037, 633, 51468], "temperature": 0.0, "avg_logprob": -0.07120064685219213, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.002337629208341241}, {"id": 1029, "seek": 604620, "start": 6068.28, "end": 6072.679999999999, "text": " node with some additional coordinates that tell you where you are located in the domain, right?", "tokens": [51468, 9984, 365, 512, 4497, 21056, 300, 980, 291, 689, 291, 366, 6870, 294, 264, 9274, 11, 558, 30, 51688], "temperature": 0.0, "avg_logprob": -0.07120064685219213, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.002337629208341241}, {"id": 1030, "seek": 607268, "start": 6072.68, "end": 6078.92, "text": " Which in case of transformers, it's where you're located in a sequence. And typically,", "tokens": [50364, 3013, 294, 1389, 295, 4088, 433, 11, 309, 311, 689, 291, 434, 6870, 294, 257, 8310, 13, 400, 5850, 11, 50676], "temperature": 0.0, "avg_logprob": -0.13064744179708915, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.004283723887056112}, {"id": 1031, "seek": 607268, "start": 6078.92, "end": 6083.320000000001, "text": " this is how positional encoding looks like, right? So, these are just some synosoids.", "tokens": [50676, 341, 307, 577, 2535, 304, 43430, 1542, 411, 11, 558, 30, 407, 11, 613, 366, 445, 512, 5451, 9869, 3742, 13, 50896], "temperature": 0.0, "avg_logprob": -0.13064744179708915, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.004283723887056112}, {"id": 1032, "seek": 607268, "start": 6084.360000000001, "end": 6087.72, "text": " So, for graphs, you can do other things, right? So, the analogy of synosoids would be the,", "tokens": [50948, 407, 11, 337, 24877, 11, 291, 393, 360, 661, 721, 11, 558, 30, 407, 11, 264, 21663, 295, 5451, 9869, 3742, 576, 312, 264, 11, 51116], "temperature": 0.0, "avg_logprob": -0.13064744179708915, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.004283723887056112}, {"id": 1033, "seek": 607268, "start": 6088.360000000001, "end": 6094.84, "text": " yeah, question? Sir, are you aware of some studies about positional encoding, especially", "tokens": [51148, 1338, 11, 1168, 30, 6144, 11, 366, 291, 3650, 295, 512, 5313, 466, 2535, 304, 43430, 11, 2318, 51472], "temperature": 0.0, "avg_logprob": -0.13064744179708915, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.004283723887056112}, {"id": 1034, "seek": 607268, "start": 6094.84, "end": 6099.56, "text": " about relative positional encoding, which is getting some popularity in transformers architecture? So,", "tokens": [51472, 466, 4972, 2535, 304, 43430, 11, 597, 307, 1242, 512, 19301, 294, 4088, 433, 9482, 30, 407, 11, 51708], "temperature": 0.0, "avg_logprob": -0.13064744179708915, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.004283723887056112}, {"id": 1035, "seek": 609956, "start": 6100.4400000000005, "end": 6106.92, "text": " like, the studies which consider all invariances in those positional encoding,", "tokens": [50408, 411, 11, 264, 5313, 597, 1949, 439, 33270, 2676, 294, 729, 2535, 304, 43430, 11, 50732], "temperature": 0.0, "avg_logprob": -0.17213032696698163, "compression_ratio": 1.8502024291497976, "no_speech_prob": 0.004023488610982895}, {"id": 1036, "seek": 609956, "start": 6107.64, "end": 6112.360000000001, "text": " like, we not always want to have absolute positional encoding. Sometimes we want to be,", "tokens": [50768, 411, 11, 321, 406, 1009, 528, 281, 362, 8236, 2535, 304, 43430, 13, 4803, 321, 528, 281, 312, 11, 51004], "temperature": 0.0, "avg_logprob": -0.17213032696698163, "compression_ratio": 1.8502024291497976, "no_speech_prob": 0.004023488610982895}, {"id": 1037, "seek": 609956, "start": 6113.400000000001, "end": 6117.4800000000005, "text": " sometimes some shifts or rotations are not relevant. So, do you know some literature on that,", "tokens": [51056, 2171, 512, 19201, 420, 44796, 366, 406, 7340, 13, 407, 11, 360, 291, 458, 512, 10394, 322, 300, 11, 51260], "temperature": 0.0, "avg_logprob": -0.17213032696698163, "compression_ratio": 1.8502024291497976, "no_speech_prob": 0.004023488610982895}, {"id": 1038, "seek": 609956, "start": 6117.4800000000005, "end": 6122.360000000001, "text": " perhaps? Yeah, so, relative positional encoding, yeah, good question. So, the analogy, let's say,", "tokens": [51260, 4317, 30, 865, 11, 370, 11, 4972, 2535, 304, 43430, 11, 1338, 11, 665, 1168, 13, 407, 11, 264, 21663, 11, 718, 311, 584, 11, 51504], "temperature": 0.0, "avg_logprob": -0.17213032696698163, "compression_ratio": 1.8502024291497976, "no_speech_prob": 0.004023488610982895}, {"id": 1039, "seek": 609956, "start": 6122.360000000001, "end": 6127.240000000001, "text": " of this standard global positional encoding would be the plus and eigenvectors. So, the analogy of", "tokens": [51504, 295, 341, 3832, 4338, 2535, 304, 43430, 576, 312, 264, 1804, 293, 10446, 303, 5547, 13, 407, 11, 264, 21663, 295, 51748], "temperature": 0.0, "avg_logprob": -0.17213032696698163, "compression_ratio": 1.8502024291497976, "no_speech_prob": 0.004023488610982895}, {"id": 1040, "seek": 612724, "start": 6127.24, "end": 6133.4, "text": " local positional encoding would be something that, for example, the DGN architecture implemented,", "tokens": [50364, 2654, 2535, 304, 43430, 576, 312, 746, 300, 11, 337, 1365, 11, 264, 413, 38, 45, 9482, 12270, 11, 50672], "temperature": 0.0, "avg_logprob": -0.09958930637525476, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.007465538568794727}, {"id": 1041, "seek": 612724, "start": 6133.4, "end": 6139.48, "text": " Gabriele Corso and others directed graph networks. So, what they do, they take, for example, the", "tokens": [50672, 50053, 306, 3925, 539, 293, 2357, 12898, 4295, 9590, 13, 407, 11, 437, 436, 360, 11, 436, 747, 11, 337, 1365, 11, 264, 50976], "temperature": 0.0, "avg_logprob": -0.09958930637525476, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.007465538568794727}, {"id": 1042, "seek": 612724, "start": 6139.48, "end": 6145.88, "text": " same plus and eigenvectors and compute their gradients on the edges and then transform these", "tokens": [50976, 912, 1804, 293, 10446, 303, 5547, 293, 14722, 641, 2771, 2448, 322, 264, 8819, 293, 550, 4088, 613, 51296], "temperature": 0.0, "avg_logprob": -0.09958930637525476, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.007465538568794727}, {"id": 1043, "seek": 612724, "start": 6145.88, "end": 6149.8, "text": " features in some way. So, this gives you a kind of local direction. So, that would be probably", "tokens": [51296, 4122, 294, 512, 636, 13, 407, 11, 341, 2709, 291, 257, 733, 295, 2654, 3513, 13, 407, 11, 300, 576, 312, 1391, 51492], "temperature": 0.0, "avg_logprob": -0.09958930637525476, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.007465538568794727}, {"id": 1044, "seek": 614980, "start": 6150.76, "end": 6158.4400000000005, "text": " a good analogy of local positional encoding. So, with the plus and eigenvectors as well,", "tokens": [50412, 257, 665, 21663, 295, 2654, 2535, 304, 43430, 13, 407, 11, 365, 264, 1804, 293, 10446, 303, 5547, 382, 731, 11, 50796], "temperature": 0.0, "avg_logprob": -0.11144755179421943, "compression_ratio": 1.6405693950177935, "no_speech_prob": 0.007029000669717789}, {"id": 1045, "seek": 614980, "start": 6158.4400000000005, "end": 6163.24, "text": " you have some ambiguities. So, there has been actually a recent paper from Derek Lim and others", "tokens": [50796, 291, 362, 512, 40390, 1088, 13, 407, 11, 456, 575, 668, 767, 257, 5162, 3035, 490, 22887, 16406, 293, 2357, 51036], "temperature": 0.0, "avg_logprob": -0.11144755179421943, "compression_ratio": 1.6405693950177935, "no_speech_prob": 0.007029000669717789}, {"id": 1046, "seek": 614980, "start": 6163.24, "end": 6168.28, "text": " from MIT where they are able to solve these ambiguities. You can use random walk kernels,", "tokens": [51036, 490, 13100, 689, 436, 366, 1075, 281, 5039, 613, 40390, 1088, 13, 509, 393, 764, 4974, 1792, 23434, 1625, 11, 51288], "temperature": 0.0, "avg_logprob": -0.11144755179421943, "compression_ratio": 1.6405693950177935, "no_speech_prob": 0.007029000669717789}, {"id": 1047, "seek": 614980, "start": 6169.320000000001, "end": 6173.08, "text": " you can use substructure counting, as we've seen, right? So, there are many ways of doing it.", "tokens": [51340, 291, 393, 764, 4594, 2885, 13251, 11, 382, 321, 600, 1612, 11, 558, 30, 407, 11, 456, 366, 867, 2098, 295, 884, 309, 13, 51528], "temperature": 0.0, "avg_logprob": -0.11144755179421943, "compression_ratio": 1.6405693950177935, "no_speech_prob": 0.007029000669717789}, {"id": 1048, "seek": 614980, "start": 6173.96, "end": 6179.72, "text": " But basically, between these two extremes, right, whether ignoring the graph or learning the", "tokens": [51572, 583, 1936, 11, 1296, 613, 732, 41119, 11, 558, 11, 1968, 26258, 264, 4295, 420, 2539, 264, 51860], "temperature": 0.0, "avg_logprob": -0.11144755179421943, "compression_ratio": 1.6405693950177935, "no_speech_prob": 0.007029000669717789}, {"id": 1049, "seek": 617972, "start": 6179.72, "end": 6184.52, "text": " graph, of course, it comes at the expense of complexity, which is a huge problem in large", "tokens": [50364, 4295, 11, 295, 1164, 11, 309, 1487, 412, 264, 18406, 295, 14024, 11, 597, 307, 257, 2603, 1154, 294, 2416, 50604], "temperature": 0.0, "avg_logprob": -0.1009014402117048, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.0017324320506304502}, {"id": 1050, "seek": 617972, "start": 6184.52, "end": 6190.6, "text": " language models where the size of the domain, right, the length of the text can be very large.", "tokens": [50604, 2856, 5245, 689, 264, 2744, 295, 264, 9274, 11, 558, 11, 264, 4641, 295, 264, 2487, 393, 312, 588, 2416, 13, 50908], "temperature": 0.0, "avg_logprob": -0.1009014402117048, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.0017324320506304502}, {"id": 1051, "seek": 617972, "start": 6190.6, "end": 6195.4800000000005, "text": " So, probably one of the key computational questions would be how to compute these", "tokens": [50908, 407, 11, 1391, 472, 295, 264, 2141, 28270, 1651, 576, 312, 577, 281, 14722, 613, 51152], "temperature": 0.0, "avg_logprob": -0.1009014402117048, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.0017324320506304502}, {"id": 1052, "seek": 617972, "start": 6195.4800000000005, "end": 6201.64, "text": " attention more efficiently. So, here somewhere in between comes the graph rewiring approaches.", "tokens": [51152, 3202, 544, 19621, 13, 407, 11, 510, 4079, 294, 1296, 1487, 264, 4295, 319, 86, 5057, 11587, 13, 51460], "temperature": 0.0, "avg_logprob": -0.1009014402117048, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.0017324320506304502}, {"id": 1053, "seek": 617972, "start": 6202.52, "end": 6209.320000000001, "text": " And with graph rewiring, it means that your computational graph is not equal to the input", "tokens": [51504, 400, 365, 4295, 319, 86, 5057, 11, 309, 1355, 300, 428, 28270, 4295, 307, 406, 2681, 281, 264, 4846, 51844], "temperature": 0.0, "avg_logprob": -0.1009014402117048, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.0017324320506304502}, {"id": 1054, "seek": 620932, "start": 6209.32, "end": 6215.88, "text": " graph. And it's a little bit controversial topic because the graph being part of the input, somehow", "tokens": [50364, 4295, 13, 400, 309, 311, 257, 707, 857, 17323, 4829, 570, 264, 4295, 885, 644, 295, 264, 4846, 11, 6063, 50692], "temperature": 0.0, "avg_logprob": -0.08945439991198088, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0011496803490445018}, {"id": 1055, "seek": 620932, "start": 6217.4, "end": 6223.08, "text": " you don't want to change the input, right? So, but the fact that many architectures do it, right?", "tokens": [50768, 291, 500, 380, 528, 281, 1319, 264, 4846, 11, 558, 30, 407, 11, 457, 264, 1186, 300, 867, 6331, 1303, 360, 309, 11, 558, 30, 51052], "temperature": 0.0, "avg_logprob": -0.08945439991198088, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0011496803490445018}, {"id": 1056, "seek": 620932, "start": 6223.08, "end": 6227.88, "text": " So, if you have, for example, a very large graph like a social network where you have a lot of", "tokens": [51052, 407, 11, 498, 291, 362, 11, 337, 1365, 11, 257, 588, 2416, 4295, 411, 257, 2093, 3209, 689, 291, 362, 257, 688, 295, 51292], "temperature": 0.0, "avg_logprob": -0.08945439991198088, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0011496803490445018}, {"id": 1057, "seek": 620932, "start": 6227.88, "end": 6232.679999999999, "text": " neighbors in some nodes, you cannot aggregate information from millions of nodes. So, you need", "tokens": [51292, 12512, 294, 512, 13891, 11, 291, 2644, 26118, 1589, 490, 6803, 295, 13891, 13, 407, 11, 291, 643, 51532], "temperature": 0.0, "avg_logprob": -0.08945439991198088, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0011496803490445018}, {"id": 1058, "seek": 620932, "start": 6233.32, "end": 6236.5199999999995, "text": " to sample the neighbors, right? Which means that you are using a different graph", "tokens": [51564, 281, 6889, 264, 12512, 11, 558, 30, 3013, 1355, 300, 291, 366, 1228, 257, 819, 4295, 51724], "temperature": 0.0, "avg_logprob": -0.08945439991198088, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0011496803490445018}, {"id": 1059, "seek": 623652, "start": 6237.080000000001, "end": 6242.040000000001, "text": " from the input one. So, neighborhood sampling is a form of graph rewiring, right? You can also use", "tokens": [50392, 490, 264, 4846, 472, 13, 407, 11, 7630, 21179, 307, 257, 1254, 295, 4295, 319, 86, 5057, 11, 558, 30, 509, 393, 611, 764, 50640], "temperature": 0.0, "avg_logprob": -0.10892406304677328, "compression_ratio": 1.9153225806451613, "no_speech_prob": 0.001563924248330295}, {"id": 1060, "seek": 623652, "start": 6242.040000000001, "end": 6246.360000000001, "text": " graph neural networks where you bring information from not immediately your one-hop neighbors,", "tokens": [50640, 4295, 18161, 9590, 689, 291, 1565, 1589, 490, 406, 4258, 428, 472, 12, 9050, 12512, 11, 50856], "temperature": 0.0, "avg_logprob": -0.10892406304677328, "compression_ratio": 1.9153225806451613, "no_speech_prob": 0.001563924248330295}, {"id": 1061, "seek": 623652, "start": 6246.360000000001, "end": 6252.360000000001, "text": " but also from multiple hops away, right? So, this is also some form of graph rewiring.", "tokens": [50856, 457, 611, 490, 3866, 47579, 1314, 11, 558, 30, 407, 11, 341, 307, 611, 512, 1254, 295, 4295, 319, 86, 5057, 13, 51156], "temperature": 0.0, "avg_logprob": -0.10892406304677328, "compression_ratio": 1.9153225806451613, "no_speech_prob": 0.001563924248330295}, {"id": 1062, "seek": 623652, "start": 6252.360000000001, "end": 6257.0, "text": " Transformers are also an extreme form of graph rewiring, right, where you allow access to all", "tokens": [51156, 27938, 433, 366, 611, 364, 8084, 1254, 295, 4295, 319, 86, 5057, 11, 558, 11, 689, 291, 2089, 2105, 281, 439, 51388], "temperature": 0.0, "avg_logprob": -0.10892406304677328, "compression_ratio": 1.9153225806451613, "no_speech_prob": 0.001563924248330295}, {"id": 1063, "seek": 623652, "start": 6257.0, "end": 6261.64, "text": " the nodes in the graph. Some kind of pre-processing, right, where you pre-wire the graph for example", "tokens": [51388, 264, 13891, 294, 264, 4295, 13, 2188, 733, 295, 659, 12, 41075, 278, 11, 558, 11, 689, 291, 659, 12, 42689, 264, 4295, 337, 1365, 51620], "temperature": 0.0, "avg_logprob": -0.10892406304677328, "compression_ratio": 1.9153225806451613, "no_speech_prob": 0.001563924248330295}, {"id": 1064, "seek": 626164, "start": 6262.360000000001, "end": 6264.92, "text": " by some form of diffusion. Yeah, a question?", "tokens": [50400, 538, 512, 1254, 295, 25242, 13, 865, 11, 257, 1168, 30, 50528], "temperature": 0.0, "avg_logprob": -0.210505131312779, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.0033456534147262573}, {"id": 1065, "seek": 626164, "start": 6268.52, "end": 6272.92, "text": " So, how do you do the neighbor sampling? Is it just like a stochastic?", "tokens": [50708, 407, 11, 577, 360, 291, 360, 264, 5987, 21179, 30, 1119, 309, 445, 411, 257, 342, 8997, 2750, 30, 50928], "temperature": 0.0, "avg_logprob": -0.210505131312779, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.0033456534147262573}, {"id": 1066, "seek": 626164, "start": 6272.92, "end": 6276.52, "text": " Or do you... Yeah, usually you sample with repetition. So, this is what Sage did.", "tokens": [50928, 1610, 360, 291, 485, 865, 11, 2673, 291, 6889, 365, 30432, 13, 407, 11, 341, 307, 437, 33812, 630, 13, 51108], "temperature": 0.0, "avg_logprob": -0.210505131312779, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.0033456534147262573}, {"id": 1067, "seek": 626164, "start": 6277.8, "end": 6282.92, "text": " Do you think it's optimal or is there a way to optimize it in the best way possible?", "tokens": [51172, 1144, 291, 519, 309, 311, 16252, 420, 307, 456, 257, 636, 281, 19719, 309, 294, 264, 1151, 636, 1944, 30, 51428], "temperature": 0.0, "avg_logprob": -0.210505131312779, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.0033456534147262573}, {"id": 1068, "seek": 626164, "start": 6283.56, "end": 6288.92, "text": " I don't remember if they looked into it. Probably, of course, it might matter how you sample, but", "tokens": [51460, 286, 500, 380, 1604, 498, 436, 2956, 666, 309, 13, 9210, 11, 295, 1164, 11, 309, 1062, 1871, 577, 291, 6889, 11, 457, 51728], "temperature": 0.0, "avg_logprob": -0.210505131312779, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.0033456534147262573}, {"id": 1069, "seek": 628892, "start": 6289.32, "end": 6295.24, "text": " I don't have on top of my mind any significant result that would tell how to do it better.", "tokens": [50384, 286, 500, 380, 362, 322, 1192, 295, 452, 1575, 604, 4776, 1874, 300, 576, 980, 577, 281, 360, 309, 1101, 13, 50680], "temperature": 0.0, "avg_logprob": -0.18873417895773184, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.00573451304808259}, {"id": 1070, "seek": 628892, "start": 6298.92, "end": 6304.68, "text": " Could you elaborate more on this diffusion processes on the graphs you mentioned?", "tokens": [50864, 7497, 291, 20945, 544, 322, 341, 25242, 7555, 322, 264, 24877, 291, 2835, 30, 51152], "temperature": 0.0, "avg_logprob": -0.18873417895773184, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.00573451304808259}, {"id": 1071, "seek": 628892, "start": 6304.68, "end": 6311.16, "text": " Yeah, I will get to it. So, I will go through the diffusion equations and then I will talk about", "tokens": [51152, 865, 11, 286, 486, 483, 281, 309, 13, 407, 11, 286, 486, 352, 807, 264, 25242, 11787, 293, 550, 286, 486, 751, 466, 51476], "temperature": 0.0, "avg_logprob": -0.18873417895773184, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.00573451304808259}, {"id": 1072, "seek": 628892, "start": 6311.16, "end": 6316.92, "text": " this as well. So, I'm specifically referring to Degel, the work of Stefan Gundem and his students.", "tokens": [51476, 341, 382, 731, 13, 407, 11, 286, 478, 4682, 13761, 281, 413, 1146, 338, 11, 264, 589, 295, 32158, 38299, 443, 293, 702, 1731, 13, 51764], "temperature": 0.0, "avg_logprob": -0.18873417895773184, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.00573451304808259}, {"id": 1073, "seek": 631892, "start": 6319.72, "end": 6325.32, "text": " So, basically, bottom line is graph is not really any sacrosanct object and in practice,", "tokens": [50404, 407, 11, 1936, 11, 2767, 1622, 307, 4295, 307, 406, 534, 604, 4899, 2635, 282, 349, 2657, 293, 294, 3124, 11, 50684], "temperature": 0.0, "avg_logprob": -0.16432527664604538, "compression_ratio": 1.6159420289855073, "no_speech_prob": 0.0002988552732858807}, {"id": 1074, "seek": 631892, "start": 6325.32, "end": 6331.0, "text": " many architectures even without admitting it do some form of graph rewiring, right? So,", "tokens": [50684, 867, 6331, 1303, 754, 1553, 44056, 309, 360, 512, 1254, 295, 4295, 319, 86, 5057, 11, 558, 30, 407, 11, 50968], "temperature": 0.0, "avg_logprob": -0.16432527664604538, "compression_ratio": 1.6159420289855073, "no_speech_prob": 0.0002988552732858807}, {"id": 1075, "seek": 631892, "start": 6331.0, "end": 6335.24, "text": " you can also rewire the graph throughout the neural network, so it doesn't need to stay the same", "tokens": [50968, 291, 393, 611, 319, 42689, 264, 4295, 3710, 264, 18161, 3209, 11, 370, 309, 1177, 380, 643, 281, 1754, 264, 912, 51180], "temperature": 0.0, "avg_logprob": -0.16432527664604538, "compression_ratio": 1.6159420289855073, "no_speech_prob": 0.0002988552732858807}, {"id": 1076, "seek": 631892, "start": 6336.6, "end": 6340.4400000000005, "text": " across all layers. And this has been shown efficiently, for example, in the context of", "tokens": [51248, 2108, 439, 7914, 13, 400, 341, 575, 668, 4898, 19621, 11, 337, 1365, 11, 294, 264, 4319, 295, 51440], "temperature": 0.0, "avg_logprob": -0.16432527664604538, "compression_ratio": 1.6159420289855073, "no_speech_prob": 0.0002988552732858807}, {"id": 1077, "seek": 631892, "start": 6340.4400000000005, "end": 6344.36, "text": " our squashing where you can do maybe a few steps of standard message passing and then", "tokens": [51440, 527, 2339, 11077, 689, 291, 393, 360, 1310, 257, 1326, 4439, 295, 3832, 3636, 8437, 293, 550, 51636], "temperature": 0.0, "avg_logprob": -0.16432527664604538, "compression_ratio": 1.6159420289855073, "no_speech_prob": 0.0002988552732858807}, {"id": 1078, "seek": 634436, "start": 6344.92, "end": 6348.04, "text": " do message passing on a complete graph, right? So, like, what transformers do?", "tokens": [50392, 360, 3636, 8437, 322, 257, 3566, 4295, 11, 558, 30, 407, 11, 411, 11, 437, 4088, 433, 360, 30, 50548], "temperature": 0.0, "avg_logprob": -0.24629070522548915, "compression_ratio": 1.6035714285714286, "no_speech_prob": 0.006150001659989357}, {"id": 1079, "seek": 634436, "start": 6348.759999999999, "end": 6355.799999999999, "text": " And so, the argumentation usually is that the first steps capture somehow the structure", "tokens": [50584, 400, 370, 11, 264, 6770, 399, 2673, 307, 300, 264, 700, 4439, 7983, 6063, 264, 3877, 50936], "temperature": 0.0, "avg_logprob": -0.24629070522548915, "compression_ratio": 1.6035714285714286, "no_speech_prob": 0.006150001659989357}, {"id": 1080, "seek": 634436, "start": 6355.799999999999, "end": 6360.679999999999, "text": " of the graph, similar to Weisberg and Lemon, and then, basically, you accumulate this information", "tokens": [50936, 295, 264, 4295, 11, 2531, 281, 492, 271, 6873, 293, 35404, 11, 293, 550, 11, 1936, 11, 291, 33384, 341, 1589, 51180], "temperature": 0.0, "avg_logprob": -0.24629070522548915, "compression_ratio": 1.6035714285714286, "no_speech_prob": 0.006150001659989357}, {"id": 1081, "seek": 634436, "start": 6360.679999999999, "end": 6367.0, "text": " as features. And there is an extreme example of this. I think it's called GRIT. So, this is", "tokens": [51180, 382, 4122, 13, 400, 456, 307, 364, 8084, 1365, 295, 341, 13, 286, 519, 309, 311, 1219, 10903, 3927, 13, 407, 11, 341, 307, 51496], "temperature": 0.0, "avg_logprob": -0.24629070522548915, "compression_ratio": 1.6035714285714286, "no_speech_prob": 0.006150001659989357}, {"id": 1082, "seek": 634436, "start": 6367.0, "end": 6373.799999999999, "text": " from the group of field torrid Oxford and what they do is they use just something similar to", "tokens": [51496, 490, 264, 1594, 295, 2519, 3930, 8558, 24786, 293, 437, 436, 360, 307, 436, 764, 445, 746, 2531, 281, 51836], "temperature": 0.0, "avg_logprob": -0.24629070522548915, "compression_ratio": 1.6035714285714286, "no_speech_prob": 0.006150001659989357}, {"id": 1083, "seek": 637380, "start": 6373.88, "end": 6378.04, "text": " heat kernel to encode the local structure of the graph and then just use MLP.", "tokens": [50368, 3738, 28256, 281, 2058, 1429, 264, 2654, 3877, 295, 264, 4295, 293, 550, 445, 764, 21601, 47, 13, 50576], "temperature": 0.0, "avg_logprob": -0.1051608420707084, "compression_ratio": 1.715953307392996, "no_speech_prob": 0.0015712585300207138}, {"id": 1084, "seek": 637380, "start": 6378.92, "end": 6383.88, "text": " So, there are other extremes like this, right? So, basically, you have just some kind of local", "tokens": [50620, 407, 11, 456, 366, 661, 41119, 411, 341, 11, 558, 30, 407, 11, 1936, 11, 291, 362, 445, 512, 733, 295, 2654, 50868], "temperature": 0.0, "avg_logprob": -0.1051608420707084, "compression_ratio": 1.715953307392996, "no_speech_prob": 0.0015712585300207138}, {"id": 1085, "seek": 637380, "start": 6383.88, "end": 6388.84, "text": " feature of the graph that you can maybe make learnable, but then the graph itself is not used,", "tokens": [50868, 4111, 295, 264, 4295, 300, 291, 393, 1310, 652, 1466, 712, 11, 457, 550, 264, 4295, 2564, 307, 406, 1143, 11, 51116], "temperature": 0.0, "avg_logprob": -0.1051608420707084, "compression_ratio": 1.715953307392996, "no_speech_prob": 0.0015712585300207138}, {"id": 1086, "seek": 637380, "start": 6388.84, "end": 6393.8, "text": " so there is no message passing. And I think it's an open question of how much you want", "tokens": [51116, 370, 456, 307, 572, 3636, 8437, 13, 400, 286, 519, 309, 311, 364, 1269, 1168, 295, 577, 709, 291, 528, 51364], "temperature": 0.0, "avg_logprob": -0.1051608420707084, "compression_ratio": 1.715953307392996, "no_speech_prob": 0.0015712585300207138}, {"id": 1087, "seek": 637380, "start": 6395.320000000001, "end": 6401.16, "text": " to use to capture the structure in the form of some features versus in the form of the", "tokens": [51440, 281, 764, 281, 7983, 264, 3877, 294, 264, 1254, 295, 512, 4122, 5717, 294, 264, 1254, 295, 264, 51732], "temperature": 0.0, "avg_logprob": -0.1051608420707084, "compression_ratio": 1.715953307392996, "no_speech_prob": 0.0015712585300207138}, {"id": 1088, "seek": 640116, "start": 6401.24, "end": 6406.04, "text": " computational architecture. So, the graph can be changed throughout the layers and, well,", "tokens": [50368, 28270, 9482, 13, 407, 11, 264, 4295, 393, 312, 3105, 3710, 264, 7914, 293, 11, 731, 11, 50608], "temperature": 0.0, "avg_logprob": -0.09096161235462535, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.0029419618658721447}, {"id": 1089, "seek": 640116, "start": 6406.04, "end": 6412.599999999999, "text": " one of the first architectures to do it was what we did with Justin Solomon and his students,", "tokens": [50608, 472, 295, 264, 700, 6331, 1303, 281, 360, 309, 390, 437, 321, 630, 365, 11320, 32209, 293, 702, 1731, 11, 50936], "temperature": 0.0, "avg_logprob": -0.09096161235462535, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.0029419618658721447}, {"id": 1090, "seek": 640116, "start": 6413.32, "end": 6418.92, "text": " and that was considering problems in computer graphics. So, we have a point cloud, let's say,", "tokens": [50972, 293, 300, 390, 8079, 2740, 294, 3820, 11837, 13, 407, 11, 321, 362, 257, 935, 4588, 11, 718, 311, 584, 11, 51252], "temperature": 0.0, "avg_logprob": -0.09096161235462535, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.0029419618658721447}, {"id": 1091, "seek": 640116, "start": 6418.92, "end": 6425.0, "text": " of this airplane. So, every point has three-dimensional Euclidean coordinates,", "tokens": [51252, 295, 341, 17130, 13, 407, 11, 633, 935, 575, 1045, 12, 18759, 462, 1311, 31264, 282, 21056, 11, 51556], "temperature": 0.0, "avg_logprob": -0.09096161235462535, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.0029419618658721447}, {"id": 1092, "seek": 640116, "start": 6425.0, "end": 6429.4, "text": " and here the task is segmentation. So, you want to label each of the points on the airplane,", "tokens": [51556, 293, 510, 264, 5633, 307, 9469, 399, 13, 407, 11, 291, 528, 281, 7645, 1184, 295, 264, 2793, 322, 264, 17130, 11, 51776], "temperature": 0.0, "avg_logprob": -0.09096161235462535, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.0029419618658721447}, {"id": 1093, "seek": 642940, "start": 6429.4, "end": 6435.719999999999, "text": " whether they belong to the body, to the engines, to the wings, and so on. And here we create a", "tokens": [50364, 1968, 436, 5784, 281, 264, 1772, 11, 281, 264, 12982, 11, 281, 264, 11405, 11, 293, 370, 322, 13, 400, 510, 321, 1884, 257, 50680], "temperature": 0.0, "avg_logprob": -0.12178229292233785, "compression_ratio": 1.8169014084507042, "no_speech_prob": 0.004195319022983313}, {"id": 1094, "seek": 642940, "start": 6435.719999999999, "end": 6443.32, "text": " graph to basically to represent the local structure of the data. And what is shown here by the colors", "tokens": [50680, 4295, 281, 1936, 281, 2906, 264, 2654, 3877, 295, 264, 1412, 13, 400, 437, 307, 4898, 510, 538, 264, 4577, 51060], "temperature": 0.0, "avg_logprob": -0.12178229292233785, "compression_ratio": 1.8169014084507042, "no_speech_prob": 0.004195319022983313}, {"id": 1095, "seek": 642940, "start": 6443.32, "end": 6449.639999999999, "text": " is a distance in the feature space, right, in the latent, in the latent feature space of", "tokens": [51060, 307, 257, 4560, 294, 264, 4111, 1901, 11, 558, 11, 294, 264, 48994, 11, 294, 264, 48994, 4111, 1901, 295, 51376], "temperature": 0.0, "avg_logprob": -0.12178229292233785, "compression_ratio": 1.8169014084507042, "no_speech_prob": 0.004195319022983313}, {"id": 1096, "seek": 642940, "start": 6449.639999999999, "end": 6455.639999999999, "text": " respective layers in this network to the rest of the points from the red point, right? So, initially,", "tokens": [51376, 23649, 7914, 294, 341, 3209, 281, 264, 1472, 295, 264, 2793, 490, 264, 2182, 935, 11, 558, 30, 407, 11, 9105, 11, 51676], "temperature": 0.0, "avg_logprob": -0.12178229292233785, "compression_ratio": 1.8169014084507042, "no_speech_prob": 0.004195319022983313}, {"id": 1097, "seek": 645564, "start": 6455.64, "end": 6461.8, "text": " this is Euclidean, as you can see that, that's basically the input space, three-dimensional", "tokens": [50364, 341, 307, 462, 1311, 31264, 282, 11, 382, 291, 393, 536, 300, 11, 300, 311, 1936, 264, 4846, 1901, 11, 1045, 12, 18759, 50672], "temperature": 0.0, "avg_logprob": -0.14139896932274404, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.003972968086600304}, {"id": 1098, "seek": 645564, "start": 6461.8, "end": 6467.400000000001, "text": " R3, but then as you go deeper, you see that it becomes more semantic. So, here, for example,", "tokens": [50672, 497, 18, 11, 457, 550, 382, 291, 352, 7731, 11, 291, 536, 300, 309, 3643, 544, 47982, 13, 407, 11, 510, 11, 337, 1365, 11, 50952], "temperature": 0.0, "avg_logprob": -0.14139896932274404, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.003972968086600304}, {"id": 1099, "seek": 645564, "start": 6467.400000000001, "end": 6473.56, "text": " points on the same, on the engine become closer, or on the wing become closer. So, basically,", "tokens": [50952, 2793, 322, 264, 912, 11, 322, 264, 2848, 1813, 4966, 11, 420, 322, 264, 11162, 1813, 4966, 13, 407, 11, 1936, 11, 51260], "temperature": 0.0, "avg_logprob": -0.14139896932274404, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.003972968086600304}, {"id": 1100, "seek": 645564, "start": 6473.56, "end": 6479.8, "text": " the space itself is used for the construction of the graph, and we call it dynamic graph neural", "tokens": [51260, 264, 1901, 2564, 307, 1143, 337, 264, 6435, 295, 264, 4295, 11, 293, 321, 818, 309, 8546, 4295, 18161, 51572], "temperature": 0.0, "avg_logprob": -0.14139896932274404, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.003972968086600304}, {"id": 1101, "seek": 647980, "start": 6479.8, "end": 6488.68, "text": " networks, or maybe not very, very, very lucky name. I think many architectures are called", "tokens": [50364, 9590, 11, 420, 1310, 406, 588, 11, 588, 11, 588, 6356, 1315, 13, 286, 519, 867, 6331, 1303, 366, 1219, 50808], "temperature": 0.0, "avg_logprob": -0.13320127400484952, "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.0041806697845458984}, {"id": 1102, "seek": 647980, "start": 6488.68, "end": 6495.0, "text": " like this. And we use it also in different incarnations or similar ideas under the name", "tokens": [50808, 411, 341, 13, 400, 321, 764, 309, 611, 294, 819, 30938, 763, 420, 2531, 3487, 833, 264, 1315, 51124], "temperature": 0.0, "avg_logprob": -0.13320127400484952, "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.0041806697845458984}, {"id": 1103, "seek": 647980, "start": 6495.0, "end": 6500.6, "text": " of differentiable graph module, where basically you have applications where you don't know the", "tokens": [51124, 295, 819, 9364, 4295, 10088, 11, 689, 1936, 291, 362, 5821, 689, 291, 500, 380, 458, 264, 51404], "temperature": 0.0, "avg_logprob": -0.13320127400484952, "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.0041806697845458984}, {"id": 1104, "seek": 647980, "start": 6500.6, "end": 6505.08, "text": " graph a priori. So, that's, for example, the case with medical imaging, where you have maybe", "tokens": [51404, 4295, 257, 4059, 72, 13, 407, 11, 300, 311, 11, 337, 1365, 11, 264, 1389, 365, 4625, 25036, 11, 689, 291, 362, 1310, 51628], "temperature": 0.0, "avg_logprob": -0.13320127400484952, "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.0041806697845458984}, {"id": 1105, "seek": 650508, "start": 6505.08, "end": 6510.04, "text": " nodes representing different patients, or maybe different regions in the brain, and you have", "tokens": [50364, 13891, 13460, 819, 4209, 11, 420, 1310, 819, 10682, 294, 264, 3567, 11, 293, 291, 362, 50612], "temperature": 0.0, "avg_logprob": -0.10003507484510107, "compression_ratio": 1.748091603053435, "no_speech_prob": 0.0046950955875217915}, {"id": 1106, "seek": 650508, "start": 6510.04, "end": 6515.0, "text": " basically two branches of graph neural network architecture, one that is computing the filters", "tokens": [50612, 1936, 732, 14770, 295, 4295, 18161, 3209, 9482, 11, 472, 300, 307, 15866, 264, 15995, 50860], "temperature": 0.0, "avg_logprob": -0.10003507484510107, "compression_ratio": 1.748091603053435, "no_speech_prob": 0.0046950955875217915}, {"id": 1107, "seek": 650508, "start": 6515.0, "end": 6519.88, "text": " on the graph, and another one computing the graph itself. And, of course, there is question of", "tokens": [50860, 322, 264, 4295, 11, 293, 1071, 472, 15866, 264, 4295, 2564, 13, 400, 11, 295, 1164, 11, 456, 307, 1168, 295, 51104], "temperature": 0.0, "avg_logprob": -0.10003507484510107, "compression_ratio": 1.748091603053435, "no_speech_prob": 0.0046950955875217915}, {"id": 1108, "seek": 650508, "start": 6519.88, "end": 6525.88, "text": " computational complexity, but we're able to show that it is better to learn the graph for the,", "tokens": [51104, 28270, 14024, 11, 457, 321, 434, 1075, 281, 855, 300, 309, 307, 1101, 281, 1466, 264, 4295, 337, 264, 11, 51404], "temperature": 0.0, "avg_logprob": -0.10003507484510107, "compression_ratio": 1.748091603053435, "no_speech_prob": 0.0046950955875217915}, {"id": 1109, "seek": 650508, "start": 6525.88, "end": 6530.5199999999995, "text": " for the task rather than to construct it ad hoc in some kind of handcrafted way.", "tokens": [51404, 337, 264, 5633, 2831, 813, 281, 7690, 309, 614, 16708, 294, 512, 733, 295, 1011, 5611, 292, 636, 13, 51636], "temperature": 0.0, "avg_logprob": -0.10003507484510107, "compression_ratio": 1.748091603053435, "no_speech_prob": 0.0046950955875217915}, {"id": 1110, "seek": 653052, "start": 6531.4800000000005, "end": 6536.52, "text": " Now, talking about message passing in general, so these are hecticly added slides based on some", "tokens": [50412, 823, 11, 1417, 466, 3636, 8437, 294, 2674, 11, 370, 613, 366, 415, 15518, 356, 3869, 9788, 2361, 322, 512, 50664], "temperature": 0.0, "avg_logprob": -0.23944081081433244, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.005084484815597534}, {"id": 1111, "seek": 653052, "start": 6536.52, "end": 6544.6, "text": " discussions that we had in the coffee break. So, if you think of message passing and different", "tokens": [50664, 11088, 300, 321, 632, 294, 264, 4982, 1821, 13, 407, 11, 498, 291, 519, 295, 3636, 8437, 293, 819, 51068], "temperature": 0.0, "avg_logprob": -0.23944081081433244, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.005084484815597534}, {"id": 1112, "seek": 653052, "start": 6544.6, "end": 6551.0, "text": " versions of it like transformers, basically, the difference is in two questions, is what's", "tokens": [51068, 9606, 295, 309, 411, 4088, 433, 11, 1936, 11, 264, 2649, 307, 294, 732, 1651, 11, 307, 437, 311, 51388], "temperature": 0.0, "avg_logprob": -0.23944081081433244, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.005084484815597534}, {"id": 1113, "seek": 653052, "start": 6551.0, "end": 6556.120000000001, "text": " information to send, what's information to pass, and where to pass, whether you follow the graph,", "tokens": [51388, 1589, 281, 2845, 11, 437, 311, 1589, 281, 1320, 11, 293, 689, 281, 1320, 11, 1968, 291, 1524, 264, 4295, 11, 51644], "temperature": 0.0, "avg_logprob": -0.23944081081433244, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.005084484815597534}, {"id": 1114, "seek": 655612, "start": 6556.76, "end": 6562.92, "text": " whether you pass information to your neighbors, or you pass information to distant nodes, whether", "tokens": [50396, 1968, 291, 1320, 1589, 281, 428, 12512, 11, 420, 291, 1320, 1589, 281, 17275, 13891, 11, 1968, 50704], "temperature": 0.0, "avg_logprob": -0.15994483546206825, "compression_ratio": 1.8359375, "no_speech_prob": 0.0040435208939015865}, {"id": 1115, "seek": 655612, "start": 6562.92, "end": 6568.36, "text": " it's in K-Hope or to all the neighbors in the graph, and the question of what is how you exactly", "tokens": [50704, 309, 311, 294, 591, 12, 46959, 420, 281, 439, 264, 12512, 294, 264, 4295, 11, 293, 264, 1168, 295, 437, 307, 577, 291, 2293, 50976], "temperature": 0.0, "avg_logprob": -0.15994483546206825, "compression_ratio": 1.8359375, "no_speech_prob": 0.0040435208939015865}, {"id": 1116, "seek": 655612, "start": 6568.36, "end": 6574.2, "text": " transform your information. So, it's also interesting to add to this another question,", "tokens": [50976, 4088, 428, 1589, 13, 407, 11, 309, 311, 611, 1880, 281, 909, 281, 341, 1071, 1168, 11, 51268], "temperature": 0.0, "avg_logprob": -0.15994483546206825, "compression_ratio": 1.8359375, "no_speech_prob": 0.0040435208939015865}, {"id": 1117, "seek": 655612, "start": 6574.2, "end": 6579.72, "text": " is when to send information? And if you look at it, basically, it's a multi-step process. So,", "tokens": [51268, 307, 562, 281, 2845, 1589, 30, 400, 498, 291, 574, 412, 309, 11, 1936, 11, 309, 311, 257, 4825, 12, 16792, 1399, 13, 407, 11, 51544], "temperature": 0.0, "avg_logprob": -0.15994483546206825, "compression_ratio": 1.8359375, "no_speech_prob": 0.0040435208939015865}, {"id": 1118, "seek": 655612, "start": 6579.72, "end": 6585.0, "text": " this is how a classical message passing neural network works. So, these are three layers of an", "tokens": [51544, 341, 307, 577, 257, 13735, 3636, 8437, 18161, 3209, 1985, 13, 407, 11, 613, 366, 1045, 7914, 295, 364, 51808], "temperature": 0.0, "avg_logprob": -0.15994483546206825, "compression_ratio": 1.8359375, "no_speech_prob": 0.0040435208939015865}, {"id": 1119, "seek": 658500, "start": 6585.0, "end": 6589.72, "text": " NPNN or three iterations of, let's say, something similar to vice-fair and lemon, and I'm sending", "tokens": [50364, 426, 15466, 45, 420, 1045, 36540, 295, 11, 718, 311, 584, 11, 746, 2531, 281, 11964, 12, 69, 1246, 293, 11356, 11, 293, 286, 478, 7750, 50600], "temperature": 0.0, "avg_logprob": -0.13495971845543903, "compression_ratio": 1.9665271966527196, "no_speech_prob": 0.002487855963408947}, {"id": 1120, "seek": 658500, "start": 6589.72, "end": 6595.48, "text": " information from these nodes to these nodes, right? So, it takes three steps. So, first here,", "tokens": [50600, 1589, 490, 613, 13891, 281, 613, 13891, 11, 558, 30, 407, 11, 309, 2516, 1045, 4439, 13, 407, 11, 700, 510, 11, 50888], "temperature": 0.0, "avg_logprob": -0.13495971845543903, "compression_ratio": 1.9665271966527196, "no_speech_prob": 0.002487855963408947}, {"id": 1121, "seek": 658500, "start": 6595.48, "end": 6600.44, "text": " then these nodes propagate information to their neighbors, and then this green node receives", "tokens": [50888, 550, 613, 13891, 48256, 1589, 281, 641, 12512, 11, 293, 550, 341, 3092, 9984, 20717, 51136], "temperature": 0.0, "avg_logprob": -0.13495971845543903, "compression_ratio": 1.9665271966527196, "no_speech_prob": 0.002487855963408947}, {"id": 1122, "seek": 658500, "start": 6600.44, "end": 6605.72, "text": " information from both neighbors, right? So, in a transformer, all the information is available", "tokens": [51136, 1589, 490, 1293, 12512, 11, 558, 30, 407, 11, 294, 257, 31782, 11, 439, 264, 1589, 307, 2435, 51400], "temperature": 0.0, "avg_logprob": -0.13495971845543903, "compression_ratio": 1.9665271966527196, "no_speech_prob": 0.002487855963408947}, {"id": 1123, "seek": 658500, "start": 6605.72, "end": 6611.72, "text": " at once. So, at the same moment of time, I'm sending information from all the nodes to the", "tokens": [51400, 412, 1564, 13, 407, 11, 412, 264, 912, 1623, 295, 565, 11, 286, 478, 7750, 1589, 490, 439, 264, 13891, 281, 264, 51700], "temperature": 0.0, "avg_logprob": -0.13495971845543903, "compression_ratio": 1.9665271966527196, "no_speech_prob": 0.002487855963408947}, {"id": 1124, "seek": 661172, "start": 6611.72, "end": 6616.68, "text": " green nodes, right? So, it has access to all the information across three layers. But you can also,", "tokens": [50364, 3092, 13891, 11, 558, 30, 407, 11, 309, 575, 2105, 281, 439, 264, 1589, 2108, 1045, 7914, 13, 583, 291, 393, 611, 11, 50612], "temperature": 0.0, "avg_logprob": -0.10377302575618663, "compression_ratio": 1.75, "no_speech_prob": 0.002879189793020487}, {"id": 1125, "seek": 661172, "start": 6616.68, "end": 6623.64, "text": " so, basically, here, the difference is where to send information, right? That's the structure of", "tokens": [50612, 370, 11, 1936, 11, 510, 11, 264, 2649, 307, 689, 281, 2845, 1589, 11, 558, 30, 663, 311, 264, 3877, 295, 50960], "temperature": 0.0, "avg_logprob": -0.10377302575618663, "compression_ratio": 1.75, "no_speech_prob": 0.002879189793020487}, {"id": 1126, "seek": 661172, "start": 6623.64, "end": 6628.84, "text": " the computational graph. But we don't consider here also when to send this information. And you", "tokens": [50960, 264, 28270, 4295, 13, 583, 321, 500, 380, 1949, 510, 611, 562, 281, 2845, 341, 1589, 13, 400, 291, 51220], "temperature": 0.0, "avg_logprob": -0.10377302575618663, "compression_ratio": 1.75, "no_speech_prob": 0.002879189793020487}, {"id": 1127, "seek": 661172, "start": 6628.84, "end": 6637.400000000001, "text": " can imagine an architecture where, for example, you delay the information. So, here, the first node", "tokens": [51220, 393, 3811, 364, 9482, 689, 11, 337, 1365, 11, 291, 8577, 264, 1589, 13, 407, 11, 510, 11, 264, 700, 9984, 51648], "temperature": 0.0, "avg_logprob": -0.10377302575618663, "compression_ratio": 1.75, "no_speech_prob": 0.002879189793020487}, {"id": 1128, "seek": 663740, "start": 6637.48, "end": 6641.879999999999, "text": " in first iteration sends information to its neighbor. At the second layer, it sends to", "tokens": [50368, 294, 700, 24784, 14790, 1589, 281, 1080, 5987, 13, 1711, 264, 1150, 4583, 11, 309, 14790, 281, 50588], "temperature": 0.0, "avg_logprob": -0.11479795260692206, "compression_ratio": 1.88135593220339, "no_speech_prob": 0.0027621882036328316}, {"id": 1129, "seek": 663740, "start": 6641.879999999999, "end": 6646.599999999999, "text": " two hop neighbors, right? So, this information becomes gradually available. Not like in a transformer", "tokens": [50588, 732, 3818, 12512, 11, 558, 30, 407, 11, 341, 1589, 3643, 13145, 2435, 13, 1726, 411, 294, 257, 31782, 50824], "temperature": 0.0, "avg_logprob": -0.11479795260692206, "compression_ratio": 1.88135593220339, "no_speech_prob": 0.0027621882036328316}, {"id": 1130, "seek": 663740, "start": 6646.599999999999, "end": 6652.28, "text": " where you flood all the nodes with information from all other nodes at once. You make it", "tokens": [50824, 689, 291, 10481, 439, 264, 13891, 365, 1589, 490, 439, 661, 13891, 412, 1564, 13, 509, 652, 309, 51108], "temperature": 0.0, "avg_logprob": -0.11479795260692206, "compression_ratio": 1.88135593220339, "no_speech_prob": 0.0027621882036328316}, {"id": 1131, "seek": 663740, "start": 6652.28, "end": 6656.679999999999, "text": " progressive, right? So, basically, these are kind of shortcuts that you have in the graph,", "tokens": [51108, 16131, 11, 558, 30, 407, 11, 1936, 11, 613, 366, 733, 295, 34620, 300, 291, 362, 294, 264, 4295, 11, 51328], "temperature": 0.0, "avg_logprob": -0.11479795260692206, "compression_ratio": 1.88135593220339, "no_speech_prob": 0.0027621882036328316}, {"id": 1132, "seek": 663740, "start": 6657.48, "end": 6661.5599999999995, "text": " but they are made progressively as you go deeper into the architecture. You can also", "tokens": [51368, 457, 436, 366, 1027, 46667, 382, 291, 352, 7731, 666, 264, 9482, 13, 509, 393, 611, 51572], "temperature": 0.0, "avg_logprob": -0.11479795260692206, "compression_ratio": 1.88135593220339, "no_speech_prob": 0.0027621882036328316}, {"id": 1133, "seek": 663740, "start": 6661.5599999999995, "end": 6665.879999999999, "text": " make, so, you can think of this as a kind of skip connections, but you can also make skip connections", "tokens": [51572, 652, 11, 370, 11, 291, 393, 519, 295, 341, 382, 257, 733, 295, 10023, 9271, 11, 457, 291, 393, 611, 652, 10023, 9271, 51788], "temperature": 0.0, "avg_logprob": -0.11479795260692206, "compression_ratio": 1.88135593220339, "no_speech_prob": 0.0027621882036328316}, {"id": 1134, "seek": 666588, "start": 6665.96, "end": 6670.92, "text": " sparse in time. So, you can delay this information. And I'm saying that here, the information", "tokens": [50368, 637, 11668, 294, 565, 13, 407, 11, 291, 393, 8577, 341, 1589, 13, 400, 286, 478, 1566, 300, 510, 11, 264, 1589, 50616], "temperature": 0.0, "avg_logprob": -0.13798337233693977, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.0010220459662377834}, {"id": 1135, "seek": 666588, "start": 6670.92, "end": 6678.6, "text": " from the first node comes to the next node and is delayed in time, right? So, it would arrive to", "tokens": [50616, 490, 264, 700, 9984, 1487, 281, 264, 958, 9984, 293, 307, 20268, 294, 565, 11, 558, 30, 407, 11, 309, 576, 8881, 281, 51000], "temperature": 0.0, "avg_logprob": -0.13798337233693977, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.0010220459662377834}, {"id": 1136, "seek": 666588, "start": 6678.6, "end": 6683.32, "text": " it anyway, but it would be entangled with the information in other nodes. So, I'm allowing", "tokens": [51000, 309, 4033, 11, 457, 309, 576, 312, 948, 39101, 365, 264, 1589, 294, 661, 13891, 13, 407, 11, 286, 478, 8293, 51236], "temperature": 0.0, "avg_logprob": -0.13798337233693977, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.0010220459662377834}, {"id": 1137, "seek": 666588, "start": 6683.32, "end": 6690.36, "text": " direct access, but I'm delaying it. And this potentially has interesting implications also", "tokens": [51236, 2047, 2105, 11, 457, 286, 478, 8577, 278, 309, 13, 400, 341, 7263, 575, 1880, 16602, 611, 51588], "temperature": 0.0, "avg_logprob": -0.13798337233693977, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.0010220459662377834}, {"id": 1138, "seek": 666588, "start": 6690.36, "end": 6695.64, "text": " on the hardware aspect of graph neural networks, because if your graph is very large, you typically", "tokens": [51588, 322, 264, 8837, 4171, 295, 4295, 18161, 9590, 11, 570, 498, 428, 4295, 307, 588, 2416, 11, 291, 5850, 51852], "temperature": 0.0, "avg_logprob": -0.13798337233693977, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.0010220459662377834}, {"id": 1139, "seek": 669564, "start": 6695.64, "end": 6701.96, "text": " partition it into different parts of memory, right? And the cost of message passing is not", "tokens": [50364, 24808, 309, 666, 819, 3166, 295, 4675, 11, 558, 30, 400, 264, 2063, 295, 3636, 8437, 307, 406, 50680], "temperature": 0.0, "avg_logprob": -0.064302735101609, "compression_ratio": 1.9327731092436975, "no_speech_prob": 0.0012868869816884398}, {"id": 1140, "seek": 669564, "start": 6701.96, "end": 6706.4400000000005, "text": " the same, right? So, messages within the same memory cost much less than messages across", "tokens": [50680, 264, 912, 11, 558, 30, 407, 11, 7897, 1951, 264, 912, 4675, 2063, 709, 1570, 813, 7897, 2108, 50904], "temperature": 0.0, "avg_logprob": -0.064302735101609, "compression_ratio": 1.9327731092436975, "no_speech_prob": 0.0012868869816884398}, {"id": 1141, "seek": 669564, "start": 6706.4400000000005, "end": 6710.280000000001, "text": " different memories. So, you can imagine a graph neural network or you are doing fast messages", "tokens": [50904, 819, 8495, 13, 407, 11, 291, 393, 3811, 257, 4295, 18161, 3209, 420, 291, 366, 884, 2370, 7897, 51096], "temperature": 0.0, "avg_logprob": -0.064302735101609, "compression_ratio": 1.9327731092436975, "no_speech_prob": 0.0012868869816884398}, {"id": 1142, "seek": 669564, "start": 6710.280000000001, "end": 6716.4400000000005, "text": " within each part of memory hierarchy and slow messages across, right? So, you don't want to", "tokens": [51096, 1951, 1184, 644, 295, 4675, 22333, 293, 2964, 7897, 2108, 11, 558, 30, 407, 11, 291, 500, 380, 528, 281, 51404], "temperature": 0.0, "avg_logprob": -0.064302735101609, "compression_ratio": 1.9327731092436975, "no_speech_prob": 0.0012868869816884398}, {"id": 1143, "seek": 669564, "start": 6716.4400000000005, "end": 6721.8, "text": " wait for all the messages to arrive. You might want to do fast messages while waiting for slow", "tokens": [51404, 1699, 337, 439, 264, 7897, 281, 8881, 13, 509, 1062, 528, 281, 360, 2370, 7897, 1339, 3806, 337, 2964, 51672], "temperature": 0.0, "avg_logprob": -0.064302735101609, "compression_ratio": 1.9327731092436975, "no_speech_prob": 0.0012868869816884398}, {"id": 1144, "seek": 672180, "start": 6721.8, "end": 6727.24, "text": " messages, right? So, this architecture that we call drill or end drill, this version with delays", "tokens": [50364, 7897, 11, 558, 30, 407, 11, 341, 9482, 300, 321, 818, 11392, 420, 917, 11392, 11, 341, 3037, 365, 28610, 50636], "temperature": 0.0, "avg_logprob": -0.16566214662917117, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.003784731961786747}, {"id": 1145, "seek": 672180, "start": 6727.96, "end": 6732.12, "text": " potentially allows for it. So, I think it would be interesting to test it on actually some", "tokens": [50672, 7263, 4045, 337, 309, 13, 407, 11, 286, 519, 309, 576, 312, 1880, 281, 1500, 309, 322, 767, 512, 50880], "temperature": 0.0, "avg_logprob": -0.16566214662917117, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.003784731961786747}, {"id": 1146, "seek": 672180, "start": 6733.8, "end": 6740.360000000001, "text": " practical hardware like graph core, for example. So, let me move to the next topic and this is", "tokens": [50964, 8496, 8837, 411, 4295, 4965, 11, 337, 1365, 13, 407, 11, 718, 385, 1286, 281, 264, 958, 4829, 293, 341, 307, 51292], "temperature": 0.0, "avg_logprob": -0.16566214662917117, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.003784731961786747}, {"id": 1147, "seek": 672180, "start": 6740.360000000001, "end": 6748.4400000000005, "text": " physics-inspired graph neural network. So, I promised PDEs, so I need to hold this promise. And", "tokens": [51292, 10649, 12, 31637, 1824, 4295, 18161, 3209, 13, 407, 11, 286, 10768, 10464, 20442, 11, 370, 286, 643, 281, 1797, 341, 6228, 13, 400, 51696], "temperature": 0.0, "avg_logprob": -0.16566214662917117, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.003784731961786747}, {"id": 1148, "seek": 674844, "start": 6749.24, "end": 6754.839999999999, "text": " let's again take a step back and look at different objects that we've seen so far, right? And also,", "tokens": [50404, 718, 311, 797, 747, 257, 1823, 646, 293, 574, 412, 819, 6565, 300, 321, 600, 1612, 370, 1400, 11, 558, 30, 400, 611, 11, 50684], "temperature": 0.0, "avg_logprob": -0.10175970292860462, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.0010857409797608852}, {"id": 1149, "seek": 674844, "start": 6754.839999999999, "end": 6760.28, "text": " you can argue objects that are studied in this broader field of geometric deep learning. So,", "tokens": [50684, 291, 393, 9695, 6565, 300, 366, 9454, 294, 341, 13227, 2519, 295, 33246, 2452, 2539, 13, 407, 11, 50956], "temperature": 0.0, "avg_logprob": -0.10175970292860462, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.0010857409797608852}, {"id": 1150, "seek": 674844, "start": 6760.28, "end": 6765.08, "text": " let's say grids, meshes and graphs, right? So, you can think of them as more or less the same thing,", "tokens": [50956, 718, 311, 584, 677, 3742, 11, 3813, 8076, 293, 24877, 11, 558, 30, 407, 11, 291, 393, 519, 295, 552, 382, 544, 420, 1570, 264, 912, 551, 11, 51196], "temperature": 0.0, "avg_logprob": -0.10175970292860462, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.0010857409797608852}, {"id": 1151, "seek": 674844, "start": 6765.08, "end": 6770.36, "text": " just with more structure, right? So, meshes in addition to have also triangles. So, these are", "tokens": [51196, 445, 365, 544, 3877, 11, 558, 30, 407, 11, 3813, 8076, 294, 4500, 281, 362, 611, 29896, 13, 407, 11, 613, 366, 51460], "temperature": 0.0, "avg_logprob": -0.10175970292860462, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.0010857409797608852}, {"id": 1152, "seek": 674844, "start": 6770.36, "end": 6775.24, "text": " simplicial complexes. So, graphs with some extra constraints or extra structure. Grids are also", "tokens": [51460, 1034, 4770, 831, 43676, 13, 407, 11, 24877, 365, 512, 2857, 18491, 420, 2857, 3877, 13, 2606, 3742, 366, 611, 51704], "temperature": 0.0, "avg_logprob": -0.10175970292860462, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.0010857409797608852}, {"id": 1153, "seek": 677524, "start": 6775.24, "end": 6781.32, "text": " special type of graphs where we have certain organization. So, if you look at the grid and", "tokens": [50364, 2121, 2010, 295, 24877, 689, 321, 362, 1629, 4475, 13, 407, 11, 498, 291, 574, 412, 264, 10748, 293, 50668], "temperature": 0.0, "avg_logprob": -0.08336265625492219, "compression_ratio": 1.7859778597785978, "no_speech_prob": 0.003070598468184471}, {"id": 1154, "seek": 677524, "start": 6781.32, "end": 6786.04, "text": " you look at how you aggregate information from your neighbors, there is no ambiguity, right? In a", "tokens": [50668, 291, 574, 412, 577, 291, 26118, 1589, 490, 428, 12512, 11, 456, 307, 572, 46519, 11, 558, 30, 682, 257, 50904], "temperature": 0.0, "avg_logprob": -0.08336265625492219, "compression_ratio": 1.7859778597785978, "no_speech_prob": 0.003070598468184471}, {"id": 1155, "seek": 677524, "start": 6786.04, "end": 6791.0, "text": " grid, unlike a general graph, I can order my neighbors in a canonical way, right? I have a top", "tokens": [50904, 10748, 11, 8343, 257, 2674, 4295, 11, 286, 393, 1668, 452, 12512, 294, 257, 46491, 636, 11, 558, 30, 286, 362, 257, 1192, 51152], "temperature": 0.0, "avg_logprob": -0.08336265625492219, "compression_ratio": 1.7859778597785978, "no_speech_prob": 0.003070598468184471}, {"id": 1156, "seek": 677524, "start": 6791.0, "end": 6796.599999999999, "text": " neighbor, I have a left neighbor, bottom and right, like shown here. So, it is totally unambiguous,", "tokens": [51152, 5987, 11, 286, 362, 257, 1411, 5987, 11, 2767, 293, 558, 11, 411, 4898, 510, 13, 407, 11, 309, 307, 3879, 517, 2173, 30525, 11, 51432], "temperature": 0.0, "avg_logprob": -0.08336265625492219, "compression_ratio": 1.7859778597785978, "no_speech_prob": 0.003070598468184471}, {"id": 1157, "seek": 677524, "start": 6796.599999999999, "end": 6801.88, "text": " right? So, this ordering is fixed. On a mesh, the situation is slightly different. So, I can pick up", "tokens": [51432, 558, 30, 407, 11, 341, 21739, 307, 6806, 13, 1282, 257, 17407, 11, 264, 2590, 307, 4748, 819, 13, 407, 11, 286, 393, 1888, 493, 51696], "temperature": 0.0, "avg_logprob": -0.08336265625492219, "compression_ratio": 1.7859778597785978, "no_speech_prob": 0.003070598468184471}, {"id": 1158, "seek": 680188, "start": 6801.88, "end": 6807.72, "text": " my first neighbor and then I can order all the rest of the nodes, all the rest of the neighbors", "tokens": [50364, 452, 700, 5987, 293, 550, 286, 393, 1668, 439, 264, 1472, 295, 264, 13891, 11, 439, 264, 1472, 295, 264, 12512, 50656], "temperature": 0.0, "avg_logprob": -0.10559300453432145, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.0029916560743004084}, {"id": 1159, "seek": 680188, "start": 6807.72, "end": 6813.56, "text": " in, for example, clockwise orientation. And this is possible because mesh is a discrete manifold.", "tokens": [50656, 294, 11, 337, 1365, 11, 35790, 14764, 13, 400, 341, 307, 1944, 570, 17407, 307, 257, 27706, 47138, 13, 50948], "temperature": 0.0, "avg_logprob": -0.10559300453432145, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.0029916560743004084}, {"id": 1160, "seek": 680188, "start": 6813.56, "end": 6819.32, "text": " So, it's locally Euclidean. I have this meaningful ordering, right? But, of course, the choice of", "tokens": [50948, 407, 11, 309, 311, 16143, 462, 1311, 31264, 282, 13, 286, 362, 341, 10995, 21739, 11, 558, 30, 583, 11, 295, 1164, 11, 264, 3922, 295, 51236], "temperature": 0.0, "avg_logprob": -0.10559300453432145, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.0029916560743004084}, {"id": 1161, "seek": 680188, "start": 6819.32, "end": 6824.28, "text": " the first neighbor is ambiguous. So, I can rotate everything, right? So, the ambiguity here is up", "tokens": [51236, 264, 700, 5987, 307, 39465, 13, 407, 11, 286, 393, 13121, 1203, 11, 558, 30, 407, 11, 264, 46519, 510, 307, 493, 51484], "temperature": 0.0, "avg_logprob": -0.10559300453432145, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.0029916560743004084}, {"id": 1162, "seek": 680188, "start": 6824.28, "end": 6829.8, "text": " to rotation. In a graph, as we've seen, any permutation works, right? So, everything is defined", "tokens": [51484, 281, 12447, 13, 682, 257, 4295, 11, 382, 321, 600, 1612, 11, 604, 4784, 11380, 1985, 11, 558, 30, 407, 11, 1203, 307, 7642, 51760], "temperature": 0.0, "avg_logprob": -0.10559300453432145, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.0029916560743004084}, {"id": 1163, "seek": 682980, "start": 6830.360000000001, "end": 6835.400000000001, "text": " up to a permutation. So, in a sense, graphs have the least structure out of all these objects,", "tokens": [50392, 493, 281, 257, 4784, 11380, 13, 407, 11, 294, 257, 2020, 11, 24877, 362, 264, 1935, 3877, 484, 295, 439, 613, 6565, 11, 50644], "temperature": 0.0, "avg_logprob": -0.08806725852509849, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.0010199676034972072}, {"id": 1164, "seek": 682980, "start": 6835.400000000001, "end": 6841.96, "text": " right? So, second observation is that if I look at grids and meshes, I can think of them as", "tokens": [50644, 558, 30, 407, 11, 1150, 14816, 307, 300, 498, 286, 574, 412, 677, 3742, 293, 3813, 8076, 11, 286, 393, 519, 295, 552, 382, 50972], "temperature": 0.0, "avg_logprob": -0.08806725852509849, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.0010199676034972072}, {"id": 1165, "seek": 682980, "start": 6841.96, "end": 6848.28, "text": " discretizations of some continuous spaces. So, a grid is discretization of a plane or a mesh is", "tokens": [50972, 25656, 14455, 295, 512, 10957, 7673, 13, 407, 11, 257, 10748, 307, 25656, 2144, 295, 257, 5720, 420, 257, 17407, 307, 51288], "temperature": 0.0, "avg_logprob": -0.08806725852509849, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.0010199676034972072}, {"id": 1166, "seek": 682980, "start": 6848.28, "end": 6854.28, "text": " discretization of a two-dimensional surface or a manifold. We don't have immediately this analogy", "tokens": [51288, 25656, 2144, 295, 257, 732, 12, 18759, 3753, 420, 257, 47138, 13, 492, 500, 380, 362, 4258, 341, 21663, 51588], "temperature": 0.0, "avg_logprob": -0.08806725852509849, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.0010199676034972072}, {"id": 1167, "seek": 682980, "start": 6854.28, "end": 6858.92, "text": " for graphs and even though there is an entire field that is called network geometry that tries to", "tokens": [51588, 337, 24877, 293, 754, 1673, 456, 307, 364, 2302, 2519, 300, 307, 1219, 3209, 18426, 300, 9898, 281, 51820], "temperature": 0.0, "avg_logprob": -0.08806725852509849, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.0010199676034972072}, {"id": 1168, "seek": 685892, "start": 6858.92, "end": 6865.4, "text": " think of graphs as some discretization of continuous space, it would be nice to have", "tokens": [50364, 519, 295, 24877, 382, 512, 25656, 2144, 295, 10957, 1901, 11, 309, 576, 312, 1481, 281, 362, 50688], "temperature": 0.0, "avg_logprob": -0.08526712401300414, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.0007083232048898935}, {"id": 1169, "seek": 685892, "start": 6865.4, "end": 6872.28, "text": " some continuous models for GNNs, right? And that's the idea of what we call physics-inspired GNNs.", "tokens": [50688, 512, 10957, 5245, 337, 46411, 45, 82, 11, 558, 30, 400, 300, 311, 264, 1558, 295, 437, 321, 818, 10649, 12, 31637, 1824, 46411, 45, 82, 13, 51032], "temperature": 0.0, "avg_logprob": -0.08526712401300414, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.0007083232048898935}, {"id": 1170, "seek": 685892, "start": 6872.28, "end": 6878.76, "text": " So, you can consider some class of graph neural networks as a dynamic system. So, you have", "tokens": [51032, 407, 11, 291, 393, 1949, 512, 1508, 295, 4295, 18161, 9590, 382, 257, 8546, 1185, 13, 407, 11, 291, 362, 51356], "temperature": 0.0, "avg_logprob": -0.08526712401300414, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.0007083232048898935}, {"id": 1171, "seek": 685892, "start": 6878.76, "end": 6883.0, "text": " particles that live in the dimensional feature space. And let's assume that the dimensionality", "tokens": [51356, 10007, 300, 1621, 294, 264, 18795, 4111, 1901, 13, 400, 718, 311, 6552, 300, 264, 10139, 1860, 51568], "temperature": 0.0, "avg_logprob": -0.08526712401300414, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.0007083232048898935}, {"id": 1172, "seek": 685892, "start": 6883.0, "end": 6888.36, "text": " always is kept the same, right? It doesn't need to be in general graph neural networks. So, every node", "tokens": [51568, 1009, 307, 4305, 264, 912, 11, 558, 30, 467, 1177, 380, 643, 281, 312, 294, 2674, 4295, 18161, 9590, 13, 407, 11, 633, 9984, 51836], "temperature": 0.0, "avg_logprob": -0.08526712401300414, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.0007083232048898935}, {"id": 1173, "seek": 688836, "start": 6888.36, "end": 6893.48, "text": " is some point, some particle that moves along a trajectory that is shown by this red line here.", "tokens": [50364, 307, 512, 935, 11, 512, 12359, 300, 6067, 2051, 257, 21512, 300, 307, 4898, 538, 341, 2182, 1622, 510, 13, 50620], "temperature": 0.0, "avg_logprob": -0.09836649894714355, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.0015650666318833828}, {"id": 1174, "seek": 688836, "start": 6893.48, "end": 6898.04, "text": " So, a GNN is essentially a dynamic system, right? That is governed by the system of differential", "tokens": [50620, 407, 11, 257, 46411, 45, 307, 4476, 257, 8546, 1185, 11, 558, 30, 663, 307, 35529, 538, 264, 1185, 295, 15756, 50848], "temperature": 0.0, "avg_logprob": -0.09836649894714355, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.0015650666318833828}, {"id": 1175, "seek": 688836, "start": 6898.04, "end": 6908.599999999999, "text": " equations. F here is some coupling functions, right? That makes dependency between different", "tokens": [50848, 11787, 13, 479, 510, 307, 512, 37447, 6828, 11, 558, 30, 663, 1669, 33621, 1296, 819, 51376], "temperature": 0.0, "avg_logprob": -0.09836649894714355, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.0015650666318833828}, {"id": 1176, "seek": 688836, "start": 6908.599999999999, "end": 6913.88, "text": " particles. And it depends both on the particles and the graph. And it's also parameterized by some", "tokens": [51376, 10007, 13, 400, 309, 5946, 1293, 322, 264, 10007, 293, 264, 4295, 13, 400, 309, 311, 611, 13075, 1602, 538, 512, 51640], "temperature": 0.0, "avg_logprob": -0.09836649894714355, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.0015650666318833828}, {"id": 1177, "seek": 691388, "start": 6913.96, "end": 6919.08, "text": " trajectory of parameters that are, you know, by data. So, T is a continuous time parameter.", "tokens": [50368, 21512, 295, 9834, 300, 366, 11, 291, 458, 11, 538, 1412, 13, 407, 11, 314, 307, 257, 10957, 565, 13075, 13, 50624], "temperature": 0.0, "avg_logprob": -0.1426414663141424, "compression_ratio": 1.6568265682656826, "no_speech_prob": 0.0040937322191894054}, {"id": 1178, "seek": 691388, "start": 6919.08, "end": 6923.72, "text": " I can discretize it and this corresponds to layers of a graph neural network. And the graph,", "tokens": [50624, 286, 393, 25656, 1125, 309, 293, 341, 23249, 281, 7914, 295, 257, 4295, 18161, 3209, 13, 400, 264, 4295, 11, 50856], "temperature": 0.0, "avg_logprob": -0.1426414663141424, "compression_ratio": 1.6568265682656826, "no_speech_prob": 0.0040937322191894054}, {"id": 1179, "seek": 691388, "start": 6923.72, "end": 6927.8, "text": " right, you can think of it either as a coupling function, right? So, this F here, how different", "tokens": [50856, 558, 11, 291, 393, 519, 295, 309, 2139, 382, 257, 37447, 2445, 11, 558, 30, 407, 11, 341, 479, 510, 11, 577, 819, 51060], "temperature": 0.0, "avg_logprob": -0.1426414663141424, "compression_ratio": 1.6568265682656826, "no_speech_prob": 0.0040937322191894054}, {"id": 1180, "seek": 691388, "start": 6927.8, "end": 6934.36, "text": " rows of this matrix interact with each other or discretization of some continuous space,", "tokens": [51060, 13241, 295, 341, 8141, 4648, 365, 1184, 661, 420, 25656, 2144, 295, 512, 10957, 1901, 11, 51388], "temperature": 0.0, "avg_logprob": -0.1426414663141424, "compression_ratio": 1.6568265682656826, "no_speech_prob": 0.0040937322191894054}, {"id": 1181, "seek": 691388, "start": 6934.36, "end": 6943.400000000001, "text": " as we'll see in the next few examples. So, basically as the evolution equation,", "tokens": [51388, 382, 321, 603, 536, 294, 264, 958, 1326, 5110, 13, 407, 11, 1936, 382, 264, 9303, 5367, 11, 51840], "temperature": 0.0, "avg_logprob": -0.1426414663141424, "compression_ratio": 1.6568265682656826, "no_speech_prob": 0.0040937322191894054}, {"id": 1182, "seek": 694340, "start": 6943.48, "end": 6947.5599999999995, "text": " right, that governs the behavior of these particles, I can put here more or less anything,", "tokens": [50368, 558, 11, 300, 1980, 82, 264, 5223, 295, 613, 10007, 11, 286, 393, 829, 510, 544, 420, 1570, 1340, 11, 50572], "temperature": 0.0, "avg_logprob": -0.08918953895568847, "compression_ratio": 1.748091603053435, "no_speech_prob": 0.0025607654824852943}, {"id": 1183, "seek": 694340, "start": 6947.5599999999995, "end": 6952.759999999999, "text": " right? So, there's plenty of different differential equations that describe different systems. But", "tokens": [50572, 558, 30, 407, 11, 456, 311, 7140, 295, 819, 15756, 11787, 300, 6786, 819, 3652, 13, 583, 50832], "temperature": 0.0, "avg_logprob": -0.08918953895568847, "compression_ratio": 1.748091603053435, "no_speech_prob": 0.0025607654824852943}, {"id": 1184, "seek": 694340, "start": 6953.719999999999, "end": 6959.0, "text": " probably the first thing that comes to your mind when you think of propagation or diffusion of", "tokens": [50880, 1391, 264, 700, 551, 300, 1487, 281, 428, 1575, 562, 291, 519, 295, 38377, 420, 25242, 295, 51144], "temperature": 0.0, "avg_logprob": -0.08918953895568847, "compression_ratio": 1.748091603053435, "no_speech_prob": 0.0025607654824852943}, {"id": 1185, "seek": 694340, "start": 6959.0, "end": 6965.16, "text": " some stuff is the diffusion equation, right? And this, in fact, was one of the earliest", "tokens": [51144, 512, 1507, 307, 264, 25242, 5367, 11, 558, 30, 400, 341, 11, 294, 1186, 11, 390, 472, 295, 264, 20573, 51452], "temperature": 0.0, "avg_logprob": -0.08918953895568847, "compression_ratio": 1.748091603053435, "no_speech_prob": 0.0025607654824852943}, {"id": 1186, "seek": 694340, "start": 6965.16, "end": 6971.16, "text": " mathematically analyzed physical phenomena, right? The diffusion of heat, analyzed by", "tokens": [51452, 44003, 28181, 4001, 22004, 11, 558, 30, 440, 25242, 295, 3738, 11, 28181, 538, 51752], "temperature": 0.0, "avg_logprob": -0.08918953895568847, "compression_ratio": 1.748091603053435, "no_speech_prob": 0.0025607654824852943}, {"id": 1187, "seek": 697116, "start": 6971.16, "end": 6976.76, "text": " non-Elst and Newton himself in an anonymous paper written in Leighton in the transactions", "tokens": [50364, 2107, 12, 17356, 372, 293, 19541, 3647, 294, 364, 24932, 3035, 3720, 294, 1456, 397, 266, 294, 264, 16856, 50644], "temperature": 0.0, "avg_logprob": -0.14354711108737522, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.0052644070237874985}, {"id": 1188, "seek": 697116, "start": 6976.76, "end": 6981.4, "text": " of the royal site. So, it's interesting actually that the journal had a mixture of papers written", "tokens": [50644, 295, 264, 13351, 3621, 13, 407, 11, 309, 311, 1880, 767, 300, 264, 6708, 632, 257, 9925, 295, 10577, 3720, 50876], "temperature": 0.0, "avg_logprob": -0.14354711108737522, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.0052644070237874985}, {"id": 1189, "seek": 697116, "start": 6981.4, "end": 6989.16, "text": " in English and Leighton. And it was anonymous. Well, Newton devised an experimental setup where he", "tokens": [50876, 294, 3669, 293, 1456, 397, 266, 13, 400, 309, 390, 24932, 13, 1042, 11, 19541, 1905, 2640, 364, 17069, 8657, 689, 415, 51264], "temperature": 0.0, "avg_logprob": -0.14354711108737522, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.0052644070237874985}, {"id": 1190, "seek": 697116, "start": 6989.16, "end": 6997.96, "text": " heated pieces of different objects, metal, I think, mostly, and then he looked with his", "tokens": [51264, 18806, 3755, 295, 819, 6565, 11, 5760, 11, 286, 519, 11, 5240, 11, 293, 550, 415, 2956, 365, 702, 51704], "temperature": 0.0, "avg_logprob": -0.14354711108737522, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.0052644070237874985}, {"id": 1191, "seek": 699796, "start": 6997.96, "end": 7003.8, "text": " self-made thermometer here, measured how much heat is lost over a period of time and devised some", "tokens": [50364, 2698, 12, 10341, 42539, 510, 11, 12690, 577, 709, 3738, 307, 2731, 670, 257, 2896, 295, 565, 293, 1905, 2640, 512, 50656], "temperature": 0.0, "avg_logprob": -0.10203088124593099, "compression_ratio": 1.770909090909091, "no_speech_prob": 0.010617605410516262}, {"id": 1192, "seek": 699796, "start": 7004.68, "end": 7009.4, "text": " law that is formulated in modern terminology like this. What is nowadays called the Newton", "tokens": [50700, 2101, 300, 307, 48936, 294, 4363, 27575, 411, 341, 13, 708, 307, 13434, 1219, 264, 19541, 50936], "temperature": 0.0, "avg_logprob": -0.10203088124593099, "compression_ratio": 1.770909090909091, "no_speech_prob": 0.010617605410516262}, {"id": 1193, "seek": 699796, "start": 7009.4, "end": 7014.36, "text": " law of cooling, which says that the temperature that the hot body loses in a given time is proportional", "tokens": [50936, 2101, 295, 14785, 11, 597, 1619, 300, 264, 4292, 300, 264, 2368, 1772, 18293, 294, 257, 2212, 565, 307, 24969, 51184], "temperature": 0.0, "avg_logprob": -0.10203088124593099, "compression_ratio": 1.770909090909091, "no_speech_prob": 0.010617605410516262}, {"id": 1194, "seek": 699796, "start": 7014.92, "end": 7018.76, "text": " to the temperature difference between the object and the environment. He actually didn't use the", "tokens": [51212, 281, 264, 4292, 2649, 1296, 264, 2657, 293, 264, 2823, 13, 634, 767, 994, 380, 764, 264, 51404], "temperature": 0.0, "avg_logprob": -0.10203088124593099, "compression_ratio": 1.770909090909091, "no_speech_prob": 0.010617605410516262}, {"id": 1195, "seek": 699796, "start": 7018.76, "end": 7023.8, "text": " term temperature. That's modern terminology. He used the term heat, which has a different meaning", "tokens": [51404, 1433, 4292, 13, 663, 311, 4363, 27575, 13, 634, 1143, 264, 1433, 3738, 11, 597, 575, 257, 819, 3620, 51656], "temperature": 0.0, "avg_logprob": -0.10203088124593099, "compression_ratio": 1.770909090909091, "no_speech_prob": 0.010617605410516262}, {"id": 1196, "seek": 702380, "start": 7024.360000000001, "end": 7031.24, "text": " or color in Leighton. And somehow, everybody guessed that it was his paper, even though he", "tokens": [50392, 420, 2017, 294, 1456, 397, 266, 13, 400, 6063, 11, 2201, 21852, 300, 309, 390, 702, 3035, 11, 754, 1673, 415, 50736], "temperature": 0.0, "avg_logprob": -0.09790232123398199, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.0070501151494681835}, {"id": 1197, "seek": 702380, "start": 7031.24, "end": 7037.24, "text": " didn't sign it. Now, it took some time until this process was fully understood. So, Fourier", "tokens": [50736, 994, 380, 1465, 309, 13, 823, 11, 309, 1890, 512, 565, 1826, 341, 1399, 390, 4498, 7320, 13, 407, 11, 36810, 51036], "temperature": 0.0, "avg_logprob": -0.09790232123398199, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.0070501151494681835}, {"id": 1198, "seek": 702380, "start": 7037.24, "end": 7044.84, "text": " devised the local version differential equation that governs heat diffusion and then thick", "tokens": [51036, 1905, 2640, 264, 2654, 3037, 15756, 5367, 300, 1980, 82, 3738, 25242, 293, 550, 5060, 51416], "temperature": 0.0, "avg_logprob": -0.09790232123398199, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.0070501151494681835}, {"id": 1199, "seek": 702380, "start": 7045.72, "end": 7052.04, "text": " in the late 19th century defined what we nowadays understand as diffusion equations.", "tokens": [51460, 294, 264, 3469, 1294, 392, 4901, 7642, 437, 321, 13434, 1223, 382, 25242, 11787, 13, 51776], "temperature": 0.0, "avg_logprob": -0.09790232123398199, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.0070501151494681835}, {"id": 1200, "seek": 705204, "start": 7052.04, "end": 7056.6, "text": " So, if you want to apply this idea to a graph, we can consider a diffusion process on a graph.", "tokens": [50364, 407, 11, 498, 291, 528, 281, 3079, 341, 1558, 281, 257, 4295, 11, 321, 393, 1949, 257, 25242, 1399, 322, 257, 4295, 13, 50592], "temperature": 0.0, "avg_logprob": -0.11410129815340042, "compression_ratio": 1.8532818532818534, "no_speech_prob": 0.0018925484037026763}, {"id": 1201, "seek": 705204, "start": 7057.24, "end": 7062.04, "text": " And that's how it looks like. So, here x is some quantity that is being diffused. Think of it as", "tokens": [50624, 400, 300, 311, 577, 309, 1542, 411, 13, 407, 11, 510, 2031, 307, 512, 11275, 300, 307, 885, 7593, 4717, 13, 6557, 295, 309, 382, 50864], "temperature": 0.0, "avg_logprob": -0.11410129815340042, "compression_ratio": 1.8532818532818534, "no_speech_prob": 0.0018925484037026763}, {"id": 1202, "seek": 705204, "start": 7062.04, "end": 7068.2, "text": " temperature if it's color. So, a node has certain temperature at time t. So, t is a continuous", "tokens": [50864, 4292, 498, 309, 311, 2017, 13, 407, 11, 257, 9984, 575, 1629, 4292, 412, 565, 256, 13, 407, 11, 256, 307, 257, 10957, 51172], "temperature": 0.0, "avg_logprob": -0.11410129815340042, "compression_ratio": 1.8532818532818534, "no_speech_prob": 0.0018925484037026763}, {"id": 1203, "seek": 705204, "start": 7068.2, "end": 7073.0, "text": " variable. And what is written here is the self-temperature of the node. This is the temperature", "tokens": [51172, 7006, 13, 400, 437, 307, 3720, 510, 307, 264, 2698, 12, 18275, 610, 1503, 295, 264, 9984, 13, 639, 307, 264, 4292, 51412], "temperature": 0.0, "avg_logprob": -0.11410129815340042, "compression_ratio": 1.8532818532818534, "no_speech_prob": 0.0018925484037026763}, {"id": 1204, "seek": 705204, "start": 7073.0, "end": 7078.28, "text": " of the environment. So, it's the average of the one hop neighborhood of the node. And this is the", "tokens": [51412, 295, 264, 2823, 13, 407, 11, 309, 311, 264, 4274, 295, 264, 472, 3818, 7630, 295, 264, 9984, 13, 400, 341, 307, 264, 51676], "temperature": 0.0, "avg_logprob": -0.11410129815340042, "compression_ratio": 1.8532818532818534, "no_speech_prob": 0.0018925484037026763}, {"id": 1205, "seek": 707828, "start": 7078.28, "end": 7085.0, "text": " rate of temperature change. So, there might be some proportion coefficient here. So, if I rearrange", "tokens": [50364, 3314, 295, 4292, 1319, 13, 407, 11, 456, 1062, 312, 512, 16068, 17619, 510, 13, 407, 11, 498, 286, 39568, 50700], "temperature": 0.0, "avg_logprob": -0.11564465572959498, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.0026114031206816435}, {"id": 1206, "seek": 707828, "start": 7085.0, "end": 7090.599999999999, "text": " the terms on this expression, I can write it like this. So, I'm slightly massaging the formula.", "tokens": [50700, 264, 2115, 322, 341, 6114, 11, 286, 393, 2464, 309, 411, 341, 13, 407, 11, 286, 478, 4748, 2758, 3568, 264, 8513, 13, 50980], "temperature": 0.0, "avg_logprob": -0.11564465572959498, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.0026114031206816435}, {"id": 1207, "seek": 707828, "start": 7090.599999999999, "end": 7095.96, "text": " What is written here is called the gradient. So, it's just the difference between end points", "tokens": [50980, 708, 307, 3720, 510, 307, 1219, 264, 16235, 13, 407, 11, 309, 311, 445, 264, 2649, 1296, 917, 2793, 51248], "temperature": 0.0, "avg_logprob": -0.11564465572959498, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.0026114031206816435}, {"id": 1208, "seek": 707828, "start": 7096.679999999999, "end": 7102.759999999999, "text": " of an inch. So, I take the feature here and the feature here and subtract one from another. So,", "tokens": [51284, 295, 364, 7227, 13, 407, 11, 286, 747, 264, 4111, 510, 293, 264, 4111, 510, 293, 16390, 472, 490, 1071, 13, 407, 11, 51588], "temperature": 0.0, "avg_logprob": -0.11564465572959498, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.0026114031206816435}, {"id": 1209, "seek": 710276, "start": 7102.84, "end": 7108.12, "text": " that's the graph analogy of the standard gradient operator from classical calculus.", "tokens": [50368, 300, 311, 264, 4295, 21663, 295, 264, 3832, 16235, 12973, 490, 13735, 33400, 13, 50632], "temperature": 0.0, "avg_logprob": -0.12397474509019119, "compression_ratio": 1.8803418803418803, "no_speech_prob": 0.002329332986846566}, {"id": 1210, "seek": 710276, "start": 7108.84, "end": 7112.92, "text": " And what is written here is, again, the discrete analogy of the divergence operator,", "tokens": [50668, 400, 437, 307, 3720, 510, 307, 11, 797, 11, 264, 27706, 21663, 295, 264, 47387, 12973, 11, 50872], "temperature": 0.0, "avg_logprob": -0.12397474509019119, "compression_ratio": 1.8803418803418803, "no_speech_prob": 0.002329332986846566}, {"id": 1211, "seek": 710276, "start": 7112.92, "end": 7120.04, "text": " again, from basic course in calculus. So, in terms of operators, you can think of", "tokens": [50872, 797, 11, 490, 3875, 1164, 294, 33400, 13, 407, 11, 294, 2115, 295, 19077, 11, 291, 393, 519, 295, 51228], "temperature": 0.0, "avg_logprob": -0.12397474509019119, "compression_ratio": 1.8803418803418803, "no_speech_prob": 0.002329332986846566}, {"id": 1212, "seek": 710276, "start": 7120.84, "end": 7126.04, "text": " the features as that they live in the nodes of the graph as a scalar field. Then the gradient", "tokens": [51268, 264, 4122, 382, 300, 436, 1621, 294, 264, 13891, 295, 264, 4295, 382, 257, 39684, 2519, 13, 1396, 264, 16235, 51528], "temperature": 0.0, "avg_logprob": -0.12397474509019119, "compression_ratio": 1.8803418803418803, "no_speech_prob": 0.002329332986846566}, {"id": 1213, "seek": 710276, "start": 7126.04, "end": 7130.92, "text": " makes a scalar field into a vector field, right, that lives on the edges of the graph. And then", "tokens": [51528, 1669, 257, 39684, 2519, 666, 257, 8062, 2519, 11, 558, 11, 300, 2909, 322, 264, 8819, 295, 264, 4295, 13, 400, 550, 51772], "temperature": 0.0, "avg_logprob": -0.12397474509019119, "compression_ratio": 1.8803418803418803, "no_speech_prob": 0.002329332986846566}, {"id": 1214, "seek": 713092, "start": 7130.92, "end": 7136.6, "text": " the divergence does the opposite. So, it collects the information from the edges that live, that", "tokens": [50364, 264, 47387, 775, 264, 6182, 13, 407, 11, 309, 39897, 264, 1589, 490, 264, 8819, 300, 1621, 11, 300, 50648], "temperature": 0.0, "avg_logprob": -0.13353795271653396, "compression_ratio": 2.0516605166051662, "no_speech_prob": 0.0006131176487542689}, {"id": 1215, "seek": 713092, "start": 7136.6, "end": 7140.52, "text": " emanate from a node and basically sums them up maybe with some different weights, right? So,", "tokens": [50648, 28211, 473, 490, 257, 9984, 293, 1936, 34499, 552, 493, 1310, 365, 512, 819, 17443, 11, 558, 30, 407, 11, 50844], "temperature": 0.0, "avg_logprob": -0.13353795271653396, "compression_ratio": 2.0516605166051662, "no_speech_prob": 0.0006131176487542689}, {"id": 1216, "seek": 713092, "start": 7140.52, "end": 7145.8, "text": " that's what the divergence, right? So, gradient makes a scalar field into vector fields. Divergence", "tokens": [50844, 300, 311, 437, 264, 47387, 11, 558, 30, 407, 11, 16235, 1669, 257, 39684, 2519, 666, 8062, 7909, 13, 413, 1837, 15260, 51108], "temperature": 0.0, "avg_logprob": -0.13353795271653396, "compression_ratio": 2.0516605166051662, "no_speech_prob": 0.0006131176487542689}, {"id": 1217, "seek": 713092, "start": 7145.8, "end": 7151.4800000000005, "text": " makes vector fields into scalar fields. Acting together, they take a scalar field into a", "tokens": [51108, 1669, 8062, 7909, 666, 39684, 7909, 13, 42413, 1214, 11, 436, 747, 257, 39684, 2519, 666, 257, 51392], "temperature": 0.0, "avg_logprob": -0.13353795271653396, "compression_ratio": 2.0516605166051662, "no_speech_prob": 0.0006131176487542689}, {"id": 1218, "seek": 713092, "start": 7151.4800000000005, "end": 7156.04, "text": " scalar field. The divergence of the gradient or minus divergence of the gradient in our notation", "tokens": [51392, 39684, 2519, 13, 440, 47387, 295, 264, 16235, 420, 3175, 47387, 295, 264, 16235, 294, 527, 24657, 51620], "temperature": 0.0, "avg_logprob": -0.13353795271653396, "compression_ratio": 2.0516605166051662, "no_speech_prob": 0.0006131176487542689}, {"id": 1219, "seek": 713092, "start": 7156.04, "end": 7159.8, "text": " is what is called the Laplacian operator, right? So, what is written here is the", "tokens": [51620, 307, 437, 307, 1219, 264, 2369, 564, 326, 952, 12973, 11, 558, 30, 407, 11, 437, 307, 3720, 510, 307, 264, 51808], "temperature": 0.0, "avg_logprob": -0.13353795271653396, "compression_ratio": 2.0516605166051662, "no_speech_prob": 0.0006131176487542689}, {"id": 1220, "seek": 715980, "start": 7159.8, "end": 7164.76, "text": " graph Laplacian operator and by definition, you see what it does. So, it compares you to your", "tokens": [50364, 4295, 2369, 564, 326, 952, 12973, 293, 538, 7123, 11, 291, 536, 437, 309, 775, 13, 407, 11, 309, 38334, 291, 281, 428, 50612], "temperature": 0.0, "avg_logprob": -0.12406449999128069, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.000734318804461509}, {"id": 1221, "seek": 715980, "start": 7164.76, "end": 7169.24, "text": " neighborhood, how different you are from the average of your neighbors. And this is really a", "tokens": [50612, 7630, 11, 577, 819, 291, 366, 490, 264, 4274, 295, 428, 12512, 13, 400, 341, 307, 534, 257, 50836], "temperature": 0.0, "avg_logprob": -0.12406449999128069, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.000734318804461509}, {"id": 1222, "seek": 715980, "start": 7169.24, "end": 7174.04, "text": " very important operator. It comes everywhere in mathematical physics and from quantum mechanics", "tokens": [50836, 588, 1021, 12973, 13, 467, 1487, 5315, 294, 18894, 10649, 293, 490, 13018, 12939, 51076], "temperature": 0.0, "avg_logprob": -0.12406449999128069, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.000734318804461509}, {"id": 1223, "seek": 715980, "start": 7174.04, "end": 7181.400000000001, "text": " to wave equations to diffusion equations, and particularly important here. So, what else is", "tokens": [51076, 281, 5772, 11787, 281, 25242, 11787, 11, 293, 4098, 1021, 510, 13, 407, 11, 437, 1646, 307, 51444], "temperature": 0.0, "avg_logprob": -0.12406449999128069, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.000734318804461509}, {"id": 1224, "seek": 715980, "start": 7181.400000000001, "end": 7185.88, "text": " the heat equation? So, this is very simple, right? So, this is what we call homogeneous", "tokens": [51444, 264, 3738, 5367, 30, 407, 11, 341, 307, 588, 2199, 11, 558, 30, 407, 11, 341, 307, 437, 321, 818, 42632, 51668], "temperature": 0.0, "avg_logprob": -0.12406449999128069, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.000734318804461509}, {"id": 1225, "seek": 718588, "start": 7185.88, "end": 7190.36, "text": " isotropic diffusion equation. Basically, heat propagates everywhere in the same way, right?", "tokens": [50364, 38018, 39173, 25242, 5367, 13, 8537, 11, 3738, 12425, 1024, 5315, 294, 264, 912, 636, 11, 558, 30, 50588], "temperature": 0.0, "avg_logprob": -0.11325276114723899, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.0027602086775004864}, {"id": 1226, "seek": 718588, "start": 7192.4400000000005, "end": 7198.4400000000005, "text": " What else heat equation is? It's an example of prototypical gradient flow. And gradient flow", "tokens": [50692, 708, 1646, 3738, 5367, 307, 30, 467, 311, 364, 1365, 295, 46219, 34061, 16235, 3095, 13, 400, 16235, 3095, 50992], "temperature": 0.0, "avg_logprob": -0.11325276114723899, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.0027602086775004864}, {"id": 1227, "seek": 718588, "start": 7198.4400000000005, "end": 7203.400000000001, "text": " is a type of evolution equation, differential equation that looks like this. So, the step at", "tokens": [50992, 307, 257, 2010, 295, 9303, 5367, 11, 15756, 5367, 300, 1542, 411, 341, 13, 407, 11, 264, 1823, 412, 51240], "temperature": 0.0, "avg_logprob": -0.11325276114723899, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.0027602086775004864}, {"id": 1228, "seek": 718588, "start": 7203.400000000001, "end": 7208.6, "text": " every point of time is in the direction of the minus gradient of some energy, that you know here by", "tokens": [51240, 633, 935, 295, 565, 307, 294, 264, 3513, 295, 264, 3175, 16235, 295, 512, 2281, 11, 300, 291, 458, 510, 538, 51500], "temperature": 0.0, "avg_logprob": -0.11325276114723899, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.0027602086775004864}, {"id": 1229, "seek": 718588, "start": 7208.6, "end": 7213.32, "text": " E. And the energy that corresponds to the diffusion equation is what we call the Dirichlet energy.", "tokens": [51500, 462, 13, 400, 264, 2281, 300, 23249, 281, 264, 25242, 5367, 307, 437, 321, 818, 264, 34422, 480, 2631, 2281, 13, 51736], "temperature": 0.0, "avg_logprob": -0.11325276114723899, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.0027602086775004864}, {"id": 1230, "seek": 721332, "start": 7214.2, "end": 7218.92, "text": " So, the Dirichlet energy, you can write it as a quadratic form with respect to the Laplacian", "tokens": [50408, 407, 11, 264, 34422, 480, 2631, 2281, 11, 291, 393, 2464, 309, 382, 257, 37262, 1254, 365, 3104, 281, 264, 2369, 564, 326, 952, 50644], "temperature": 0.0, "avg_logprob": -0.08166680454222625, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0016407393850386143}, {"id": 1231, "seek": 721332, "start": 7218.92, "end": 7223.08, "text": " operator. And it measures the smoothness of the node features, right? How different you are from", "tokens": [50644, 12973, 13, 400, 309, 8000, 264, 5508, 1287, 295, 264, 9984, 4122, 11, 558, 30, 1012, 819, 291, 366, 490, 50852], "temperature": 0.0, "avg_logprob": -0.08166680454222625, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0016407393850386143}, {"id": 1232, "seek": 721332, "start": 7223.08, "end": 7228.679999999999, "text": " your neighbors. So, the smallest value it can achieve is zero. In this case, all the features are the", "tokens": [50852, 428, 12512, 13, 407, 11, 264, 16998, 2158, 309, 393, 4584, 307, 4018, 13, 682, 341, 1389, 11, 439, 264, 4122, 366, 264, 51132], "temperature": 0.0, "avg_logprob": -0.08166680454222625, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0016407393850386143}, {"id": 1233, "seek": 721332, "start": 7228.679999999999, "end": 7233.639999999999, "text": " same, right? And you can show that Dirichlet energy decreases along the flow. So, if you run the", "tokens": [51132, 912, 11, 558, 30, 400, 291, 393, 855, 300, 34422, 480, 2631, 2281, 24108, 2051, 264, 3095, 13, 407, 11, 498, 291, 1190, 264, 51380], "temperature": 0.0, "avg_logprob": -0.08166680454222625, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0016407393850386143}, {"id": 1234, "seek": 721332, "start": 7233.639999999999, "end": 7239.96, "text": " diffusion to infinity, all will become constant, right? So, the heat will basically propagate", "tokens": [51380, 25242, 281, 13202, 11, 439, 486, 1813, 5754, 11, 558, 30, 407, 11, 264, 3738, 486, 1936, 48256, 51696], "temperature": 0.0, "avg_logprob": -0.08166680454222625, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0016407393850386143}, {"id": 1235, "seek": 723996, "start": 7240.04, "end": 7245.16, "text": " on the domain and everything will have the same temperature. So, this is what we typically call", "tokens": [50368, 322, 264, 9274, 293, 1203, 486, 362, 264, 912, 4292, 13, 407, 11, 341, 307, 437, 321, 5850, 818, 50624], "temperature": 0.0, "avg_logprob": -0.06914504145232725, "compression_ratio": 1.6978851963746224, "no_speech_prob": 0.00643988698720932}, {"id": 1236, "seek": 723996, "start": 7245.16, "end": 7249.16, "text": " over smoothing in graph neural networks, right? And I should say that over smoothing is not", "tokens": [50624, 670, 899, 6259, 571, 294, 4295, 18161, 9590, 11, 558, 30, 400, 286, 820, 584, 300, 670, 899, 6259, 571, 307, 406, 50824], "temperature": 0.0, "avg_logprob": -0.06914504145232725, "compression_ratio": 1.6978851963746224, "no_speech_prob": 0.00643988698720932}, {"id": 1237, "seek": 723996, "start": 7249.16, "end": 7253.08, "text": " necessarily a bad phenomenon. So, usually it's described as a kind of catastrophe that happens", "tokens": [50824, 4725, 257, 1578, 14029, 13, 407, 11, 2673, 309, 311, 7619, 382, 257, 733, 295, 36043, 300, 2314, 51020], "temperature": 0.0, "avg_logprob": -0.06914504145232725, "compression_ratio": 1.6978851963746224, "no_speech_prob": 0.00643988698720932}, {"id": 1238, "seek": 723996, "start": 7253.08, "end": 7257.64, "text": " in GNNs and prevents us from making deeper architectures. But it can actually be a very", "tokens": [51020, 294, 46411, 45, 82, 293, 22367, 505, 490, 1455, 7731, 6331, 1303, 13, 583, 309, 393, 767, 312, 257, 588, 51248], "temperature": 0.0, "avg_logprob": -0.06914504145232725, "compression_ratio": 1.6978851963746224, "no_speech_prob": 0.00643988698720932}, {"id": 1239, "seek": 723996, "start": 7257.64, "end": 7262.76, "text": " benign thing, right? So, you can imagine without any learning. So, if I give you a graph and I", "tokens": [51248, 3271, 788, 551, 11, 558, 30, 407, 11, 291, 393, 3811, 1553, 604, 2539, 13, 407, 11, 498, 286, 976, 291, 257, 4295, 293, 286, 51504], "temperature": 0.0, "avg_logprob": -0.06914504145232725, "compression_ratio": 1.6978851963746224, "no_speech_prob": 0.00643988698720932}, {"id": 1240, "seek": 723996, "start": 7262.76, "end": 7268.6, "text": " have labels of a few nodes, and if I assume that the structure of the graph is homophilic in the", "tokens": [51504, 362, 16949, 295, 257, 1326, 13891, 11, 293, 498, 286, 6552, 300, 264, 3877, 295, 264, 4295, 307, 3655, 5317, 388, 299, 294, 264, 51796], "temperature": 0.0, "avg_logprob": -0.06914504145232725, "compression_ratio": 1.6978851963746224, "no_speech_prob": 0.00643988698720932}, {"id": 1241, "seek": 726860, "start": 7268.6, "end": 7273.8, "text": " sense that my neighbors are expected to have labels similar to me, then I can just diffuse this", "tokens": [50364, 2020, 300, 452, 12512, 366, 5176, 281, 362, 16949, 2531, 281, 385, 11, 550, 286, 393, 445, 42165, 341, 50624], "temperature": 0.0, "avg_logprob": -0.10096197292722504, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.002399862976744771}, {"id": 1242, "seek": 726860, "start": 7273.8, "end": 7277.88, "text": " information so it will be, you can think of it as a heat diffusion equation with boundary conditions.", "tokens": [50624, 1589, 370, 309, 486, 312, 11, 291, 393, 519, 295, 309, 382, 257, 3738, 25242, 5367, 365, 12866, 4487, 13, 50828], "temperature": 0.0, "avg_logprob": -0.10096197292722504, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.002399862976744771}, {"id": 1243, "seek": 726860, "start": 7278.92, "end": 7285.160000000001, "text": " And by just doing this, it is very likely that the result will be very good, right? If the graph", "tokens": [50880, 400, 538, 445, 884, 341, 11, 309, 307, 588, 3700, 300, 264, 1874, 486, 312, 588, 665, 11, 558, 30, 759, 264, 4295, 51192], "temperature": 0.0, "avg_logprob": -0.10096197292722504, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.002399862976744771}, {"id": 1244, "seek": 726860, "start": 7285.160000000001, "end": 7290.120000000001, "text": " is not homophilic, of course, you need to do something else, maybe work harder, but on its own,", "tokens": [51192, 307, 406, 3655, 5317, 388, 299, 11, 295, 1164, 11, 291, 643, 281, 360, 746, 1646, 11, 1310, 589, 6081, 11, 457, 322, 1080, 1065, 11, 51440], "temperature": 0.0, "avg_logprob": -0.10096197292722504, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.002399862976744771}, {"id": 1245, "seek": 726860, "start": 7290.120000000001, "end": 7294.04, "text": " the over smoothing is not necessarily bad. Yeah, there is a question.", "tokens": [51440, 264, 670, 899, 6259, 571, 307, 406, 4725, 1578, 13, 865, 11, 456, 307, 257, 1168, 13, 51636], "temperature": 0.0, "avg_logprob": -0.10096197292722504, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.002399862976744771}, {"id": 1246, "seek": 729860, "start": 7299.4800000000005, "end": 7306.52, "text": " Actually, I have like two questions on this topic. So, first of all, can I see this as the,", "tokens": [50408, 5135, 11, 286, 362, 411, 732, 1651, 322, 341, 4829, 13, 407, 11, 700, 295, 439, 11, 393, 286, 536, 341, 382, 264, 11, 50760], "temperature": 0.0, "avg_logprob": -0.12296027353365127, "compression_ratio": 1.5105263157894737, "no_speech_prob": 0.004744945093989372}, {"id": 1247, "seek": 729860, "start": 7307.8, "end": 7316.4400000000005, "text": " in some sense, the message passing algorithm there, but via the ordinary differential equation between", "tokens": [50824, 294, 512, 2020, 11, 264, 3636, 8437, 9284, 456, 11, 457, 5766, 264, 10547, 15756, 5367, 1296, 51256], "temperature": 0.0, "avg_logprob": -0.12296027353365127, "compression_ratio": 1.5105263157894737, "no_speech_prob": 0.004744945093989372}, {"id": 1248, "seek": 729860, "start": 7316.4400000000005, "end": 7325.56, "text": " the edges? Right. So, well, whether they call it message passing or not, I think it's a good", "tokens": [51256, 264, 8819, 30, 1779, 13, 407, 11, 731, 11, 1968, 436, 818, 309, 3636, 8437, 420, 406, 11, 286, 519, 309, 311, 257, 665, 51712], "temperature": 0.0, "avg_logprob": -0.12296027353365127, "compression_ratio": 1.5105263157894737, "no_speech_prob": 0.004744945093989372}, {"id": 1249, "seek": 732556, "start": 7325.56, "end": 7329.96, "text": " question. So, every iteration when you discretize this differential equation, every step will", "tokens": [50364, 1168, 13, 407, 11, 633, 24784, 562, 291, 25656, 1125, 341, 15756, 5367, 11, 633, 1823, 486, 50584], "temperature": 0.0, "avg_logprob": -0.09649998924948952, "compression_ratio": 1.6843971631205674, "no_speech_prob": 0.0057859867811203}, {"id": 1250, "seek": 732556, "start": 7329.96, "end": 7334.52, "text": " correspond to message passing. Now, in some cases, you can actually have a closed form expression", "tokens": [50584, 6805, 281, 3636, 8437, 13, 823, 11, 294, 512, 3331, 11, 291, 393, 767, 362, 257, 5395, 1254, 6114, 50812], "temperature": 0.0, "avg_logprob": -0.09649998924948952, "compression_ratio": 1.6843971631205674, "no_speech_prob": 0.0057859867811203}, {"id": 1251, "seek": 732556, "start": 7334.52, "end": 7339.240000000001, "text": " for the solution at any time t, right? This is called the heat kernel. So, it will look like", "tokens": [50812, 337, 264, 3827, 412, 604, 565, 256, 11, 558, 30, 639, 307, 1219, 264, 3738, 28256, 13, 407, 11, 309, 486, 574, 411, 51048], "temperature": 0.0, "avg_logprob": -0.09649998924948952, "compression_ratio": 1.6843971631205674, "no_speech_prob": 0.0057859867811203}, {"id": 1252, "seek": 732556, "start": 7339.240000000001, "end": 7345.88, "text": " the exponential of the Laplace matrix. So, I can compute the value of the temperature at every", "tokens": [51048, 264, 21510, 295, 264, 2369, 6742, 8141, 13, 407, 11, 286, 393, 14722, 264, 2158, 295, 264, 4292, 412, 633, 51380], "temperature": 0.0, "avg_logprob": -0.09649998924948952, "compression_ratio": 1.6843971631205674, "no_speech_prob": 0.0057859867811203}, {"id": 1253, "seek": 732556, "start": 7345.88, "end": 7353.56, "text": " point instantaneously without doing these microsteps of message passing. So, whether they call,", "tokens": [51380, 935, 9836, 13131, 1553, 884, 613, 3123, 27494, 10653, 295, 3636, 8437, 13, 407, 11, 1968, 436, 818, 11, 51764], "temperature": 0.0, "avg_logprob": -0.09649998924948952, "compression_ratio": 1.6843971631205674, "no_speech_prob": 0.0057859867811203}, {"id": 1254, "seek": 735356, "start": 7353.56, "end": 7357.080000000001, "text": " still call it message passing or not. So, this is where semantically I disagree with better,", "tokens": [50364, 920, 818, 309, 3636, 8437, 420, 406, 13, 407, 11, 341, 307, 689, 4361, 49505, 286, 14091, 365, 1101, 11, 50540], "temperature": 0.0, "avg_logprob": -0.1300831252130969, "compression_ratio": 1.7372262773722629, "no_speech_prob": 0.002862080931663513}, {"id": 1255, "seek": 735356, "start": 7357.080000000001, "end": 7361.8, "text": " for example. He still suggests to call it message passing. I think it's not. So, I don't know how", "tokens": [50540, 337, 1365, 13, 634, 920, 13409, 281, 818, 309, 3636, 8437, 13, 286, 519, 309, 311, 406, 13, 407, 11, 286, 500, 380, 458, 577, 50776], "temperature": 0.0, "avg_logprob": -0.1300831252130969, "compression_ratio": 1.7372262773722629, "no_speech_prob": 0.002862080931663513}, {"id": 1256, "seek": 735356, "start": 7361.8, "end": 7366.92, "text": " to call it better, maybe some kind of spatial coupling, right? Because you have, you have direct", "tokens": [50776, 281, 818, 309, 1101, 11, 1310, 512, 733, 295, 23598, 37447, 11, 558, 30, 1436, 291, 362, 11, 291, 362, 2047, 51032], "temperature": 0.0, "avg_logprob": -0.1300831252130969, "compression_ratio": 1.7372262773722629, "no_speech_prob": 0.002862080931663513}, {"id": 1257, "seek": 735356, "start": 7366.92, "end": 7374.360000000001, "text": " information to potentially infinite, all the nodes in the graph, right? So, it is something that", "tokens": [51032, 1589, 281, 7263, 13785, 11, 439, 264, 13891, 294, 264, 4295, 11, 558, 30, 407, 11, 309, 307, 746, 300, 51404], "temperature": 0.0, "avg_logprob": -0.1300831252130969, "compression_ratio": 1.7372262773722629, "no_speech_prob": 0.002862080931663513}, {"id": 1258, "seek": 735356, "start": 7375.4800000000005, "end": 7380.68, "text": " doesn't require propagating information explicitly, right? You might be, when you solve the", "tokens": [51460, 1177, 380, 3651, 12425, 990, 1589, 20803, 11, 558, 30, 509, 1062, 312, 11, 562, 291, 5039, 264, 51720], "temperature": 0.0, "avg_logprob": -0.1300831252130969, "compression_ratio": 1.7372262773722629, "no_speech_prob": 0.002862080931663513}, {"id": 1259, "seek": 738068, "start": 7380.68, "end": 7386.04, "text": " diffusion equation, especially with its own linear, you might not have a choice. But in some cases,", "tokens": [50364, 25242, 5367, 11, 2318, 365, 1080, 1065, 8213, 11, 291, 1062, 406, 362, 257, 3922, 13, 583, 294, 512, 3331, 11, 50632], "temperature": 0.0, "avg_logprob": -0.14444023303771286, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.0026668747887015343}, {"id": 1260, "seek": 738068, "start": 7386.04, "end": 7392.68, "text": " you do. You have closed form expressions, right? Okay, thank you. And the second question, because", "tokens": [50632, 291, 360, 13, 509, 362, 5395, 1254, 15277, 11, 558, 30, 1033, 11, 1309, 291, 13, 400, 264, 1150, 1168, 11, 570, 50964], "temperature": 0.0, "avg_logprob": -0.14444023303771286, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.0026668747887015343}, {"id": 1261, "seek": 738068, "start": 7392.68, "end": 7400.04, "text": " I suppose that here is like the mostly ordinary differential equations. Is there any research", "tokens": [50964, 286, 7297, 300, 510, 307, 411, 264, 5240, 10547, 15756, 11787, 13, 1119, 456, 604, 2132, 51332], "temperature": 0.0, "avg_logprob": -0.14444023303771286, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.0026668747887015343}, {"id": 1262, "seek": 738068, "start": 7400.04, "end": 7408.280000000001, "text": " to introduce stochasticity there to be, let's say stochastic differential equations, something", "tokens": [51332, 281, 5366, 342, 8997, 2750, 507, 456, 281, 312, 11, 718, 311, 584, 342, 8997, 2750, 15756, 11787, 11, 746, 51744], "temperature": 0.0, "avg_logprob": -0.14444023303771286, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.0026668747887015343}, {"id": 1263, "seek": 740828, "start": 7408.28, "end": 7416.759999999999, "text": " like that, and when it could help in the, in the, in this field? Yeah. So, it's interesting. I think", "tokens": [50364, 411, 300, 11, 293, 562, 309, 727, 854, 294, 264, 11, 294, 264, 11, 294, 341, 2519, 30, 865, 13, 407, 11, 309, 311, 1880, 13, 286, 519, 50788], "temperature": 0.0, "avg_logprob": -0.18366185824076334, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0029034451581537724}, {"id": 1264, "seek": 740828, "start": 7416.759999999999, "end": 7422.12, "text": " there have been some works. And there are some works. So, it talks for example, a big", "tokens": [50788, 456, 362, 668, 512, 1985, 13, 400, 456, 366, 512, 1985, 13, 407, 11, 309, 6686, 337, 1365, 11, 257, 955, 51056], "temperature": 0.0, "avg_logprob": -0.18366185824076334, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0029034451581537724}, {"id": 1265, "seek": 740828, "start": 7422.12, "end": 7428.12, "text": " expert in this domain is Terry Lyons. So, I think they are working on, on these topics.", "tokens": [51056, 5844, 294, 341, 9274, 307, 21983, 12687, 892, 13, 407, 11, 286, 519, 436, 366, 1364, 322, 11, 322, 613, 8378, 13, 51356], "temperature": 0.0, "avg_logprob": -0.18366185824076334, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0029034451581537724}, {"id": 1266, "seek": 740828, "start": 7429.639999999999, "end": 7432.44, "text": " So, we are not considering stochastic differential equations. We are considering", "tokens": [51432, 407, 11, 321, 366, 406, 8079, 342, 8997, 2750, 15756, 11787, 13, 492, 366, 8079, 51572], "temperature": 0.0, "avg_logprob": -0.18366185824076334, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0029034451581537724}, {"id": 1267, "seek": 743244, "start": 7432.839999999999, "end": 7439.0, "text": " whether to call them ODs or PDs. So, these are coupled ODs, right? When you discretize,", "tokens": [50384, 1968, 281, 818, 552, 48447, 82, 420, 10464, 82, 13, 407, 11, 613, 366, 29482, 48447, 82, 11, 558, 30, 1133, 291, 25656, 1125, 11, 50692], "temperature": 0.0, "avg_logprob": -0.17652196469514267, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.007290067616850138}, {"id": 1268, "seek": 743244, "start": 7440.04, "end": 7444.28, "text": " a partial differential equation becomes a system of coupled ordinary differential equations. I think", "tokens": [50744, 257, 14641, 15756, 5367, 3643, 257, 1185, 295, 29482, 10547, 15756, 11787, 13, 286, 519, 50956], "temperature": 0.0, "avg_logprob": -0.17652196469514267, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.007290067616850138}, {"id": 1269, "seek": 743244, "start": 7444.28, "end": 7451.32, "text": " they, again here, the terminology is more semantic difference, whether you have a continuous spatial", "tokens": [50956, 436, 11, 797, 510, 11, 264, 27575, 307, 544, 47982, 2649, 11, 1968, 291, 362, 257, 10957, 23598, 51308], "temperature": 0.0, "avg_logprob": -0.17652196469514267, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.007290067616850138}, {"id": 1270, "seek": 743244, "start": 7451.32, "end": 7457.08, "text": " coordinate that you want to discretize. So, that will be the next part of it. Okay. Thank you.", "tokens": [51308, 15670, 300, 291, 528, 281, 25656, 1125, 13, 407, 11, 300, 486, 312, 264, 958, 644, 295, 309, 13, 1033, 13, 1044, 291, 13, 51596], "temperature": 0.0, "avg_logprob": -0.17652196469514267, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.007290067616850138}, {"id": 1271, "seek": 745708, "start": 7457.64, "end": 7463.88, "text": " Right. Okay. So, that was basically, that was the gradient flow, right? And again,", "tokens": [50392, 1779, 13, 1033, 13, 407, 11, 300, 390, 1936, 11, 300, 390, 264, 16235, 3095, 11, 558, 30, 400, 797, 11, 50704], "temperature": 0.0, "avg_logprob": -0.1540990738641648, "compression_ratio": 1.7704918032786885, "no_speech_prob": 0.004097995348274708}, {"id": 1272, "seek": 745708, "start": 7463.88, "end": 7468.44, "text": " an example of, of the heat diffusion equation as, as a prototype of the gradient flow. So,", "tokens": [50704, 364, 1365, 295, 11, 295, 264, 3738, 25242, 5367, 382, 11, 382, 257, 19475, 295, 264, 16235, 3095, 13, 407, 11, 50932], "temperature": 0.0, "avg_logprob": -0.1540990738641648, "compression_ratio": 1.7704918032786885, "no_speech_prob": 0.004097995348274708}, {"id": 1273, "seek": 745708, "start": 7469.0, "end": 7475.32, "text": " if you look again at the, at this view on, on graph neural networks as some kind of", "tokens": [50960, 498, 291, 574, 797, 412, 264, 11, 412, 341, 1910, 322, 11, 322, 4295, 18161, 9590, 382, 512, 733, 295, 51276], "temperature": 0.0, "avg_logprob": -0.1540990738641648, "compression_ratio": 1.7704918032786885, "no_speech_prob": 0.004097995348274708}, {"id": 1274, "seek": 745708, "start": 7475.96, "end": 7481.5599999999995, "text": " evolution equations governing some, some physical system. So, the traditional approach", "tokens": [51308, 9303, 11787, 30054, 512, 11, 512, 4001, 1185, 13, 407, 11, 264, 5164, 3109, 51588], "temperature": 0.0, "avg_logprob": -0.1540990738641648, "compression_ratio": 1.7704918032786885, "no_speech_prob": 0.004097995348274708}, {"id": 1275, "seek": 745708, "start": 7481.5599999999995, "end": 7485.72, "text": " to graph neural networks is to take this differential equation, discretize it, and then", "tokens": [51588, 281, 4295, 18161, 9590, 307, 281, 747, 341, 15756, 5367, 11, 25656, 1125, 309, 11, 293, 550, 51796], "temperature": 0.0, "avg_logprob": -0.1540990738641648, "compression_ratio": 1.7704918032786885, "no_speech_prob": 0.004097995348274708}, {"id": 1276, "seek": 748572, "start": 7485.72, "end": 7489.16, "text": " parameterize the evolution equation, right? The discrete evolution equation. So,", "tokens": [50364, 13075, 1125, 264, 9303, 5367, 11, 558, 30, 440, 27706, 9303, 5367, 13, 407, 11, 50536], "temperature": 0.0, "avg_logprob": -0.10238609815898694, "compression_ratio": 1.8755020080321285, "no_speech_prob": 0.0008230380481109023}, {"id": 1277, "seek": 748572, "start": 7489.72, "end": 7493.8, "text": " every step of an iterative solver here will correspond to a layer, and then you parameterize", "tokens": [50564, 633, 1823, 295, 364, 17138, 1166, 1404, 331, 510, 486, 6805, 281, 257, 4583, 11, 293, 550, 291, 13075, 1125, 50768], "temperature": 0.0, "avg_logprob": -0.10238609815898694, "compression_ratio": 1.8755020080321285, "no_speech_prob": 0.0008230380481109023}, {"id": 1278, "seek": 748572, "start": 7493.8, "end": 7499.56, "text": " every layer, right? So, that's how you can view graph neural networks. So, instead, what we can do,", "tokens": [50768, 633, 4583, 11, 558, 30, 407, 11, 300, 311, 577, 291, 393, 1910, 4295, 18161, 9590, 13, 407, 11, 2602, 11, 437, 321, 393, 360, 11, 51056], "temperature": 0.0, "avg_logprob": -0.10238609815898694, "compression_ratio": 1.8755020080321285, "no_speech_prob": 0.0008230380481109023}, {"id": 1279, "seek": 748572, "start": 7499.56, "end": 7506.04, "text": " we can start with, with an energy, parameterize an energy, and then derive the evolution equation", "tokens": [51056, 321, 393, 722, 365, 11, 365, 364, 2281, 11, 13075, 1125, 364, 2281, 11, 293, 550, 28446, 264, 9303, 5367, 51380], "temperature": 0.0, "avg_logprob": -0.10238609815898694, "compression_ratio": 1.8755020080321285, "no_speech_prob": 0.0008230380481109023}, {"id": 1280, "seek": 748572, "start": 7506.04, "end": 7511.08, "text": " as a gradient flow. So, apparently, there is no difference, right? But the big difference would", "tokens": [51380, 382, 257, 16235, 3095, 13, 407, 11, 7970, 11, 456, 307, 572, 2649, 11, 558, 30, 583, 264, 955, 2649, 576, 51632], "temperature": 0.0, "avg_logprob": -0.10238609815898694, "compression_ratio": 1.8755020080321285, "no_speech_prob": 0.0008230380481109023}, {"id": 1281, "seek": 751108, "start": 7511.16, "end": 7517.24, "text": " be that we will, it will allow us to make certain architectural choices. So, it will", "tokens": [50368, 312, 300, 321, 486, 11, 309, 486, 2089, 505, 281, 652, 1629, 26621, 7994, 13, 407, 11, 309, 486, 50672], "temperature": 0.0, "avg_logprob": -0.08011275007013689, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.005394714884459972}, {"id": 1282, "seek": 751108, "start": 7517.24, "end": 7522.6, "text": " restrict our space of all the possible architectures that we can do here, right? Like, for example,", "tokens": [50672, 7694, 527, 1901, 295, 439, 264, 1944, 6331, 1303, 300, 321, 393, 360, 510, 11, 558, 30, 1743, 11, 337, 1365, 11, 50940], "temperature": 0.0, "avg_logprob": -0.08011275007013689, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.005394714884459972}, {"id": 1283, "seek": 751108, "start": 7522.6, "end": 7527.64, "text": " the form of message passing. That will have better interpretability. And of course, I know that", "tokens": [50940, 264, 1254, 295, 3636, 8437, 13, 663, 486, 362, 1101, 7302, 2310, 13, 400, 295, 1164, 11, 286, 458, 300, 51192], "temperature": 0.0, "avg_logprob": -0.08011275007013689, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.005394714884459972}, {"id": 1284, "seek": 751108, "start": 7527.64, "end": 7532.2, "text": " interpretability is maybe a bad word in machine learning. So, what I mean here is that we will", "tokens": [51192, 7302, 2310, 307, 1310, 257, 1578, 1349, 294, 3479, 2539, 13, 407, 11, 437, 286, 914, 510, 307, 300, 321, 486, 51420], "temperature": 0.0, "avg_logprob": -0.08011275007013689, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.005394714884459972}, {"id": 1285, "seek": 751108, "start": 7532.2, "end": 7539.16, "text": " be able to guarantee that certain things happen or not happen. Okay. And I will be more specific", "tokens": [51420, 312, 1075, 281, 10815, 300, 1629, 721, 1051, 420, 406, 1051, 13, 1033, 13, 400, 286, 486, 312, 544, 2685, 51768], "temperature": 0.0, "avg_logprob": -0.08011275007013689, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.005394714884459972}, {"id": 1286, "seek": 753916, "start": 7539.16, "end": 7544.76, "text": " in a second. So, let's consider the following parametric energy. So, we call it generalized", "tokens": [50364, 294, 257, 1150, 13, 407, 11, 718, 311, 1949, 264, 3480, 6220, 17475, 2281, 13, 407, 11, 321, 818, 309, 44498, 50644], "temperature": 0.0, "avg_logprob": -0.08035039520263672, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.00485995365306735}, {"id": 1287, "seek": 753916, "start": 7544.76, "end": 7550.92, "text": " Dirichlet energy. It has these two terms. So, you can think of it as an energy that a system of", "tokens": [50644, 34422, 480, 2631, 2281, 13, 467, 575, 613, 732, 2115, 13, 407, 11, 291, 393, 519, 295, 309, 382, 364, 2281, 300, 257, 1185, 295, 50952], "temperature": 0.0, "avg_logprob": -0.08035039520263672, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.00485995365306735}, {"id": 1288, "seek": 753916, "start": 7550.92, "end": 7556.12, "text": " particles has, and it's parameterized by two matrices of size d by d, right? d, remind you,", "tokens": [50952, 10007, 575, 11, 293, 309, 311, 13075, 1602, 538, 732, 32284, 295, 2744, 274, 538, 274, 11, 558, 30, 274, 11, 4160, 291, 11, 51212], "temperature": 0.0, "avg_logprob": -0.08035039520263672, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.00485995365306735}, {"id": 1289, "seek": 753916, "start": 7556.12, "end": 7559.72, "text": " that's the dimensionality of the features. So, that's the space where the particles move.", "tokens": [51212, 300, 311, 264, 10139, 1860, 295, 264, 4122, 13, 407, 11, 300, 311, 264, 1901, 689, 264, 10007, 1286, 13, 51392], "temperature": 0.0, "avg_logprob": -0.08035039520263672, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.00485995365306735}, {"id": 1290, "seek": 753916, "start": 7560.599999999999, "end": 7564.92, "text": " And we have two terms here. So, the external energy term, it acts on all the particles. So,", "tokens": [51436, 400, 321, 362, 732, 2115, 510, 13, 407, 11, 264, 8320, 2281, 1433, 11, 309, 10672, 322, 439, 264, 10007, 13, 407, 11, 51652], "temperature": 0.0, "avg_logprob": -0.08035039520263672, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.00485995365306735}, {"id": 1291, "seek": 756492, "start": 7564.92, "end": 7569.8, "text": " think of some force that moves them in some directions. And we have internal energy. So,", "tokens": [50364, 519, 295, 512, 3464, 300, 6067, 552, 294, 512, 11095, 13, 400, 321, 362, 6920, 2281, 13, 407, 11, 50608], "temperature": 0.0, "avg_logprob": -0.10682210644471993, "compression_ratio": 1.926530612244898, "no_speech_prob": 0.0014614901738241315}, {"id": 1292, "seek": 756492, "start": 7569.8, "end": 7575.24, "text": " these are the interactions between particles along the edges of the graph, right? Think of maybe", "tokens": [50608, 613, 366, 264, 13280, 1296, 10007, 2051, 264, 8819, 295, 264, 4295, 11, 558, 30, 6557, 295, 1310, 50880], "temperature": 0.0, "avg_logprob": -0.10682210644471993, "compression_ratio": 1.926530612244898, "no_speech_prob": 0.0014614901738241315}, {"id": 1293, "seek": 756492, "start": 7575.24, "end": 7582.4400000000005, "text": " colonic interactions, right? Or maybe springs that attached to some pairs of particles. And we", "tokens": [50880, 8255, 299, 13280, 11, 558, 30, 1610, 1310, 24647, 300, 8570, 281, 512, 15494, 295, 10007, 13, 400, 321, 51240], "temperature": 0.0, "avg_logprob": -0.10682210644471993, "compression_ratio": 1.926530612244898, "no_speech_prob": 0.0014614901738241315}, {"id": 1294, "seek": 756492, "start": 7582.4400000000005, "end": 7587.16, "text": " have two types of interactions. So, we have attractive interactions, and they happen along the", "tokens": [51240, 362, 732, 3467, 295, 13280, 13, 407, 11, 321, 362, 12609, 13280, 11, 293, 436, 1051, 2051, 264, 51476], "temperature": 0.0, "avg_logprob": -0.10682210644471993, "compression_ratio": 1.926530612244898, "no_speech_prob": 0.0014614901738241315}, {"id": 1295, "seek": 756492, "start": 7588.04, "end": 7594.12, "text": " eigenvectors of the matrix W that corresponds to positive eigenvalues and repulsive interactions", "tokens": [51520, 10446, 303, 5547, 295, 264, 8141, 343, 300, 23249, 281, 3353, 10446, 46033, 293, 1085, 32657, 13280, 51824], "temperature": 0.0, "avg_logprob": -0.10682210644471993, "compression_ratio": 1.926530612244898, "no_speech_prob": 0.0014614901738241315}, {"id": 1296, "seek": 759412, "start": 7594.2, "end": 7602.2, "text": " that happen along negative eigenvectors. And you can see an example here. So, here the space is", "tokens": [50368, 300, 1051, 2051, 3671, 10446, 303, 5547, 13, 400, 291, 393, 536, 364, 1365, 510, 13, 407, 11, 510, 264, 1901, 307, 50768], "temperature": 0.0, "avg_logprob": -0.09871750362848832, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.0012894731480628252}, {"id": 1297, "seek": 759412, "start": 7602.2, "end": 7607.24, "text": " two-dimensional just for visualization purposes. And the graph here, you see that it's heterophilic.", "tokens": [50768, 732, 12, 18759, 445, 337, 25801, 9932, 13, 400, 264, 4295, 510, 11, 291, 536, 300, 309, 311, 20789, 5317, 388, 299, 13, 51020], "temperature": 0.0, "avg_logprob": -0.09871750362848832, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.0012894731480628252}, {"id": 1298, "seek": 759412, "start": 7607.24, "end": 7612.599999999999, "text": " So, the colors of the nodes represent the labels. And the positions represent the features, right?", "tokens": [51020, 407, 11, 264, 4577, 295, 264, 13891, 2906, 264, 16949, 13, 400, 264, 8432, 2906, 264, 4122, 11, 558, 30, 51288], "temperature": 0.0, "avg_logprob": -0.09871750362848832, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.0012894731480628252}, {"id": 1299, "seek": 759412, "start": 7612.599999999999, "end": 7617.5599999999995, "text": " The feature coordinates x and y. So, the graph is actually perfectly heterophilic, right? The", "tokens": [51288, 440, 4111, 21056, 2031, 293, 288, 13, 407, 11, 264, 4295, 307, 767, 6239, 20789, 5317, 388, 299, 11, 558, 30, 440, 51536], "temperature": 0.0, "avg_logprob": -0.09871750362848832, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.0012894731480628252}, {"id": 1300, "seek": 759412, "start": 7617.5599999999995, "end": 7623.8, "text": " blue nodes have only red neighbors, and the red nodes have only blue neighbors. And yet,", "tokens": [51536, 3344, 13891, 362, 787, 2182, 12512, 11, 293, 264, 2182, 13891, 362, 787, 3344, 12512, 13, 400, 1939, 11, 51848], "temperature": 0.0, "avg_logprob": -0.09871750362848832, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.0012894731480628252}, {"id": 1301, "seek": 762380, "start": 7623.88, "end": 7628.04, "text": " we can find directions. So, the horizontal direction is repulsive direction, where the", "tokens": [50368, 321, 393, 915, 11095, 13, 407, 11, 264, 12750, 3513, 307, 1085, 32657, 3513, 11, 689, 264, 50576], "temperature": 0.0, "avg_logprob": -0.0880973634265718, "compression_ratio": 1.858267716535433, "no_speech_prob": 0.001383346039801836}, {"id": 1302, "seek": 762380, "start": 7628.04, "end": 7633.400000000001, "text": " particles are separated. And the vertical direction is attractive direction, where the particles", "tokens": [50576, 10007, 366, 12005, 13, 400, 264, 9429, 3513, 307, 12609, 3513, 11, 689, 264, 10007, 50844], "temperature": 0.0, "avg_logprob": -0.0880973634265718, "compression_ratio": 1.858267716535433, "no_speech_prob": 0.001383346039801836}, {"id": 1303, "seek": 762380, "start": 7634.360000000001, "end": 7641.24, "text": " cluster together. We can perfectly separate these types of nodes, right? So, if my task is", "tokens": [50892, 13630, 1214, 13, 492, 393, 6239, 4994, 613, 3467, 295, 13891, 11, 558, 30, 407, 11, 498, 452, 5633, 307, 51236], "temperature": 0.0, "avg_logprob": -0.0880973634265718, "compression_ratio": 1.858267716535433, "no_speech_prob": 0.001383346039801836}, {"id": 1304, "seek": 762380, "start": 7641.24, "end": 7647.08, "text": " node classification on this graph, by this very simple process, I can solve this problem, right?", "tokens": [51236, 9984, 21538, 322, 341, 4295, 11, 538, 341, 588, 2199, 1399, 11, 286, 393, 5039, 341, 1154, 11, 558, 30, 51528], "temperature": 0.0, "avg_logprob": -0.0880973634265718, "compression_ratio": 1.858267716535433, "no_speech_prob": 0.001383346039801836}, {"id": 1305, "seek": 762380, "start": 7648.12, "end": 7652.76, "text": " So, we will see in a second that this corresponds to a convolutional architecture. By convolutional,", "tokens": [51580, 407, 11, 321, 486, 536, 294, 257, 1150, 300, 341, 23249, 281, 257, 45216, 304, 9482, 13, 3146, 45216, 304, 11, 51812], "temperature": 0.0, "avg_logprob": -0.0880973634265718, "compression_ratio": 1.858267716535433, "no_speech_prob": 0.001383346039801836}, {"id": 1306, "seek": 765276, "start": 7652.76, "end": 7657.320000000001, "text": " I mean that my diffusion operator depends only on the structure of the graph, but not on the features,", "tokens": [50364, 286, 914, 300, 452, 25242, 12973, 5946, 787, 322, 264, 3877, 295, 264, 4295, 11, 457, 406, 322, 264, 4122, 11, 50592], "temperature": 0.0, "avg_logprob": -0.11324744415283203, "compression_ratio": 1.8988326848249026, "no_speech_prob": 0.00416153110563755}, {"id": 1307, "seek": 765276, "start": 7657.320000000001, "end": 7662.12, "text": " like in more complicated, for example, attentional networks. So, I can write it as AX something,", "tokens": [50592, 411, 294, 544, 6179, 11, 337, 1365, 11, 3202, 304, 9590, 13, 407, 11, 286, 393, 2464, 309, 382, 316, 55, 746, 11, 50832], "temperature": 0.0, "avg_logprob": -0.11324744415283203, "compression_ratio": 1.8988326848249026, "no_speech_prob": 0.00416153110563755}, {"id": 1308, "seek": 765276, "start": 7662.12, "end": 7668.360000000001, "text": " right? So, this is what was our distinction. So, in convolutional architectures, we had something", "tokens": [50832, 558, 30, 407, 11, 341, 307, 437, 390, 527, 16844, 13, 407, 11, 294, 45216, 304, 6331, 1303, 11, 321, 632, 746, 51144], "temperature": 0.0, "avg_logprob": -0.11324744415283203, "compression_ratio": 1.8988326848249026, "no_speech_prob": 0.00416153110563755}, {"id": 1309, "seek": 765276, "start": 7668.360000000001, "end": 7675.400000000001, "text": " like this. In attentional architectures, we had something like this, right? So, that would be", "tokens": [51144, 411, 341, 13, 682, 3202, 304, 6331, 1303, 11, 321, 632, 746, 411, 341, 11, 558, 30, 407, 11, 300, 576, 312, 51496], "temperature": 0.0, "avg_logprob": -0.11324744415283203, "compression_ratio": 1.8988326848249026, "no_speech_prob": 0.00416153110563755}, {"id": 1310, "seek": 765276, "start": 7675.400000000001, "end": 7681.08, "text": " the GCN type of architectures, and this will be the GUT type of architectures. So, the fact that", "tokens": [51496, 264, 29435, 45, 2010, 295, 6331, 1303, 11, 293, 341, 486, 312, 264, 460, 8709, 2010, 295, 6331, 1303, 13, 407, 11, 264, 1186, 300, 51780], "temperature": 0.0, "avg_logprob": -0.11324744415283203, "compression_ratio": 1.8988326848249026, "no_speech_prob": 0.00416153110563755}, {"id": 1311, "seek": 768108, "start": 7681.08, "end": 7685.08, "text": " this matrix is constant, it depends only on the structure of the graph, is what I call convolutional", "tokens": [50364, 341, 8141, 307, 5754, 11, 309, 5946, 787, 322, 264, 3877, 295, 264, 4295, 11, 307, 437, 286, 818, 45216, 304, 50564], "temperature": 0.0, "avg_logprob": -0.133729041706432, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.004170913714915514}, {"id": 1312, "seek": 768108, "start": 7685.08, "end": 7691.72, "text": " architectures, right? So, basically, these goals against this folklore that I mentioned in the", "tokens": [50564, 6331, 1303, 11, 558, 30, 407, 11, 1936, 11, 613, 5493, 1970, 341, 49195, 300, 286, 2835, 294, 264, 50896], "temperature": 0.0, "avg_logprob": -0.133729041706432, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.004170913714915514}, {"id": 1313, "seek": 768108, "start": 7691.72, "end": 7696.76, "text": " beginning, that convolutional architectures are not good for heterophilic graphs. So, you can see", "tokens": [50896, 2863, 11, 300, 45216, 304, 6331, 1303, 366, 406, 665, 337, 20789, 5317, 388, 299, 24877, 13, 407, 11, 291, 393, 536, 51148], "temperature": 0.0, "avg_logprob": -0.133729041706432, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.004170913714915514}, {"id": 1314, "seek": 768108, "start": 7696.76, "end": 7703.16, "text": " that they can work very well, right? So, we need to dive deeper and understand what happens here.", "tokens": [51148, 300, 436, 393, 589, 588, 731, 11, 558, 30, 407, 11, 321, 643, 281, 9192, 7731, 293, 1223, 437, 2314, 510, 13, 51468], "temperature": 0.0, "avg_logprob": -0.133729041706432, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.004170913714915514}, {"id": 1315, "seek": 768108, "start": 7703.88, "end": 7707.08, "text": " So, if we write the gradient flow of, yeah, question.", "tokens": [51504, 407, 11, 498, 321, 2464, 264, 16235, 3095, 295, 11, 1338, 11, 1168, 13, 51664], "temperature": 0.0, "avg_logprob": -0.133729041706432, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.004170913714915514}, {"id": 1316, "seek": 770708, "start": 7707.4, "end": 7711.72, "text": " Is there a link between this gradient flow and less and less popular recently,", "tokens": [50380, 1119, 456, 257, 2113, 1296, 341, 16235, 3095, 293, 1570, 293, 1570, 3743, 3938, 11, 50596], "temperature": 0.0, "avg_logprob": -0.18708227194991767, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.018951544538140297}, {"id": 1317, "seek": 770708, "start": 7714.84, "end": 7719.48, "text": " probabilistic graphical method, especially if you showed the previous slide, it was very", "tokens": [50752, 31959, 3142, 35942, 3170, 11, 2318, 498, 291, 4712, 264, 3894, 4137, 11, 309, 390, 588, 50984], "temperature": 0.0, "avg_logprob": -0.18708227194991767, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.018951544538140297}, {"id": 1318, "seek": 770708, "start": 7719.48, "end": 7724.36, "text": " similar to restricted Boltzmann, right, when we had this bipartite graph?", "tokens": [50984, 2531, 281, 20608, 37884, 89, 14912, 11, 558, 11, 562, 321, 632, 341, 28741, 642, 4295, 30, 51228], "temperature": 0.0, "avg_logprob": -0.18708227194991767, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.018951544538140297}, {"id": 1319, "seek": 770708, "start": 7724.36, "end": 7728.76, "text": " Yeah, you can probably interpret it from, right? So, gradient flows are very basic objects. So,", "tokens": [51228, 865, 11, 291, 393, 1391, 7302, 309, 490, 11, 558, 30, 407, 11, 16235, 12867, 366, 588, 3875, 6565, 13, 407, 11, 51448], "temperature": 0.0, "avg_logprob": -0.18708227194991767, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.018951544538140297}, {"id": 1320, "seek": 770708, "start": 7728.76, "end": 7732.5199999999995, "text": " it's essentially steepest descent is also a gradient flow, right? So, it's a continuous", "tokens": [51448, 309, 311, 4476, 16841, 377, 23475, 307, 611, 257, 16235, 3095, 11, 558, 30, 407, 11, 309, 311, 257, 10957, 51636], "temperature": 0.0, "avg_logprob": -0.18708227194991767, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.018951544538140297}, {"id": 1321, "seek": 773252, "start": 7732.52, "end": 7739.080000000001, "text": " version or a variational version of it. Yeah, so, probably there are links to many different things.", "tokens": [50364, 3037, 420, 257, 3034, 1478, 3037, 295, 309, 13, 865, 11, 370, 11, 1391, 456, 366, 6123, 281, 867, 819, 721, 13, 50692], "temperature": 0.0, "avg_logprob": -0.12381438077506372, "compression_ratio": 1.887029288702929, "no_speech_prob": 0.0017227502539753914}, {"id": 1322, "seek": 773252, "start": 7740.68, "end": 7744.6, "text": " So, basically, you're minimizing some energy here, right? So, and this energy,", "tokens": [50772, 407, 11, 1936, 11, 291, 434, 46608, 512, 2281, 510, 11, 558, 30, 407, 11, 293, 341, 2281, 11, 50968], "temperature": 0.0, "avg_logprob": -0.12381438077506372, "compression_ratio": 1.887029288702929, "no_speech_prob": 0.0017227502539753914}, {"id": 1323, "seek": 773252, "start": 7744.6, "end": 7749.8, "text": " so, it's not the learning part, right? So, it's the inference. So, applying a neural network", "tokens": [50968, 370, 11, 309, 311, 406, 264, 2539, 644, 11, 558, 30, 407, 11, 309, 311, 264, 38253, 13, 407, 11, 9275, 257, 18161, 3209, 51228], "temperature": 0.0, "avg_logprob": -0.12381438077506372, "compression_ratio": 1.887029288702929, "no_speech_prob": 0.0017227502539753914}, {"id": 1324, "seek": 773252, "start": 7750.84, "end": 7755.72, "text": " minimizes some energy, right? You design the network, so, it minimizes some energy, right?", "tokens": [51280, 4464, 5660, 512, 2281, 11, 558, 30, 509, 1715, 264, 3209, 11, 370, 11, 309, 4464, 5660, 512, 2281, 11, 558, 30, 51524], "temperature": 0.0, "avg_logprob": -0.12381438077506372, "compression_ratio": 1.887029288702929, "no_speech_prob": 0.0017227502539753914}, {"id": 1325, "seek": 773252, "start": 7755.72, "end": 7760.52, "text": " And the energy is parametric. So, what kind of energy to minimize is what you determine", "tokens": [51524, 400, 264, 2281, 307, 6220, 17475, 13, 407, 11, 437, 733, 295, 2281, 281, 17522, 307, 437, 291, 6997, 51764], "temperature": 0.0, "avg_logprob": -0.12381438077506372, "compression_ratio": 1.887029288702929, "no_speech_prob": 0.0017227502539753914}, {"id": 1326, "seek": 776052, "start": 7760.52, "end": 7766.280000000001, "text": " based on the task. So, that's what is done by back propagation. So, basically, the gradient flow,", "tokens": [50364, 2361, 322, 264, 5633, 13, 407, 11, 300, 311, 437, 307, 1096, 538, 646, 38377, 13, 407, 11, 1936, 11, 264, 16235, 3095, 11, 50652], "temperature": 0.0, "avg_logprob": -0.11570255138255932, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.0007802732870914042}, {"id": 1327, "seek": 776052, "start": 7766.280000000001, "end": 7770.84, "text": " you just differentiate this energy with respect to its parameters, right? By data I denote here,", "tokens": [50652, 291, 445, 23203, 341, 2281, 365, 3104, 281, 1080, 9834, 11, 558, 30, 3146, 1412, 286, 45708, 510, 11, 50880], "temperature": 0.0, "avg_logprob": -0.11570255138255932, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.0007802732870914042}, {"id": 1328, "seek": 776052, "start": 7770.84, "end": 7775.4800000000005, "text": " these two matrices omega and w. And one thing that you notice immediately that they all appear", "tokens": [50880, 613, 732, 32284, 10498, 293, 261, 13, 400, 472, 551, 300, 291, 3449, 4258, 300, 436, 439, 4204, 51112], "temperature": 0.0, "avg_logprob": -0.11570255138255932, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.0007802732870914042}, {"id": 1329, "seek": 776052, "start": 7775.4800000000005, "end": 7780.68, "text": " in symmetrized form, right? So, they appear in quadratic terms. So, they appear as omega plus", "tokens": [51112, 294, 14232, 302, 470, 11312, 1254, 11, 558, 30, 407, 11, 436, 4204, 294, 37262, 2115, 13, 407, 11, 436, 4204, 382, 10498, 1804, 51372], "temperature": 0.0, "avg_logprob": -0.11570255138255932, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.0007802732870914042}, {"id": 1330, "seek": 776052, "start": 7780.68, "end": 7784.92, "text": " omega transpose. So, we can just assume that they are symmetric to start with, right? So,", "tokens": [51372, 10498, 25167, 13, 407, 11, 321, 393, 445, 6552, 300, 436, 366, 32330, 281, 722, 365, 11, 558, 30, 407, 11, 51584], "temperature": 0.0, "avg_logprob": -0.11570255138255932, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.0007802732870914042}, {"id": 1331, "seek": 776052, "start": 7784.92, "end": 7789.320000000001, "text": " that's already first restriction that comes from this assumption of the gradient flow.", "tokens": [51584, 300, 311, 1217, 700, 29529, 300, 1487, 490, 341, 15302, 295, 264, 16235, 3095, 13, 51804], "temperature": 0.0, "avg_logprob": -0.11570255138255932, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.0007802732870914042}, {"id": 1332, "seek": 778932, "start": 7789.96, "end": 7793.16, "text": " The second thing is, again, this is by the design of the energy that", "tokens": [50396, 440, 1150, 551, 307, 11, 797, 11, 341, 307, 538, 264, 1715, 295, 264, 2281, 300, 50556], "temperature": 0.0, "avg_logprob": -0.07915062599993766, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.002422108082100749}, {"id": 1333, "seek": 778932, "start": 7793.88, "end": 7799.639999999999, "text": " the parameters are time-independent, right? So, they don't depend on t, right? Which will become", "tokens": [50592, 264, 9834, 366, 565, 12, 471, 4217, 317, 11, 558, 30, 407, 11, 436, 500, 380, 5672, 322, 256, 11, 558, 30, 3013, 486, 1813, 50880], "temperature": 0.0, "avg_logprob": -0.07915062599993766, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.002422108082100749}, {"id": 1334, "seek": 778932, "start": 7799.639999999999, "end": 7804.84, "text": " the layer number. And if we discretize it, this is how we discretize. So, we replace the temporal", "tokens": [50880, 264, 4583, 1230, 13, 400, 498, 321, 25656, 1125, 309, 11, 341, 307, 577, 321, 25656, 1125, 13, 407, 11, 321, 7406, 264, 30881, 51140], "temperature": 0.0, "avg_logprob": -0.07915062599993766, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.002422108082100749}, {"id": 1335, "seek": 778932, "start": 7804.84, "end": 7813.719999999999, "text": " derivative with forward difference. So, this is what is also called the explicit Euler scheme.", "tokens": [51140, 13760, 365, 2128, 2649, 13, 407, 11, 341, 307, 437, 307, 611, 1219, 264, 13691, 462, 26318, 12232, 13, 51584], "temperature": 0.0, "avg_logprob": -0.07915062599993766, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.002422108082100749}, {"id": 1336, "seek": 781372, "start": 7814.68, "end": 7821.88, "text": " So, we have some step-sized tau. And this gives residual convolutional type GCN, again,", "tokens": [50412, 407, 11, 321, 362, 512, 1823, 12, 20614, 17842, 13, 400, 341, 2709, 27980, 45216, 304, 2010, 29435, 45, 11, 797, 11, 50772], "temperature": 0.0, "avg_logprob": -0.16441350287579476, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.0039595249108970165}, {"id": 1337, "seek": 781372, "start": 7822.6, "end": 7827.0, "text": " the convolutional type GNN. Why I call it convolutional? Because this matrix A, right,", "tokens": [50808, 264, 45216, 304, 2010, 46411, 45, 13, 1545, 286, 818, 309, 45216, 304, 30, 1436, 341, 8141, 316, 11, 558, 11, 51028], "temperature": 0.0, "avg_logprob": -0.16441350287579476, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.0039595249108970165}, {"id": 1338, "seek": 781372, "start": 7827.0, "end": 7831.4800000000005, "text": " that's where the diffusion happens, where the message passing happens, right? When I send", "tokens": [51028, 300, 311, 689, 264, 25242, 2314, 11, 689, 264, 3636, 8437, 2314, 11, 558, 30, 1133, 286, 2845, 51252], "temperature": 0.0, "avg_logprob": -0.16441350287579476, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.0039595249108970165}, {"id": 1339, "seek": 781372, "start": 7831.4800000000005, "end": 7839.400000000001, "text": " information across adjacent nodes, it's not dependent on x. It's fixed, right? So, you can call", "tokens": [51252, 1589, 2108, 24441, 13891, 11, 309, 311, 406, 12334, 322, 2031, 13, 467, 311, 6806, 11, 558, 30, 407, 11, 291, 393, 818, 51648], "temperature": 0.0, "avg_logprob": -0.16441350287579476, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.0039595249108970165}, {"id": 1340, "seek": 783940, "start": 7839.48, "end": 7845.48, "text": " it a kind of a convolution. The weights are symmetric and the weights are shared across", "tokens": [50368, 309, 257, 733, 295, 257, 45216, 13, 440, 17443, 366, 32330, 293, 264, 17443, 366, 5507, 2108, 50668], "temperature": 0.0, "avg_logprob": -0.10870411965699323, "compression_ratio": 1.786259541984733, "no_speech_prob": 0.003467344678938389}, {"id": 1341, "seek": 783940, "start": 7845.48, "end": 7851.5599999999995, "text": " different layers, okay? So, the way that you can use it, you can optionally do some nonlinear", "tokens": [50668, 819, 7914, 11, 1392, 30, 407, 11, 264, 636, 300, 291, 393, 764, 309, 11, 291, 393, 3614, 379, 360, 512, 2107, 28263, 50972], "temperature": 0.0, "avg_logprob": -0.10870411965699323, "compression_ratio": 1.786259541984733, "no_speech_prob": 0.003467344678938389}, {"id": 1342, "seek": 783940, "start": 7851.5599999999995, "end": 7856.28, "text": " encoding typically to reduce the dimensionality. So, we have some fixed low dimensional dimension", "tokens": [50972, 43430, 5850, 281, 5407, 264, 10139, 1860, 13, 407, 11, 321, 362, 512, 6806, 2295, 18795, 10139, 51208], "temperature": 0.0, "avg_logprob": -0.10870411965699323, "compression_ratio": 1.786259541984733, "no_speech_prob": 0.003467344678938389}, {"id": 1343, "seek": 783940, "start": 7856.28, "end": 7862.679999999999, "text": " of this space. You apply a linear gradient flow. So, all the propagation of information on the", "tokens": [51208, 295, 341, 1901, 13, 509, 3079, 257, 8213, 16235, 3095, 13, 407, 11, 439, 264, 38377, 295, 1589, 322, 264, 51528], "temperature": 0.0, "avg_logprob": -0.10870411965699323, "compression_ratio": 1.786259541984733, "no_speech_prob": 0.003467344678938389}, {"id": 1344, "seek": 783940, "start": 7862.679999999999, "end": 7868.28, "text": " graph is linear. So, there are no nonlinear activations, right? And then you do some decoder.", "tokens": [51528, 4295, 307, 8213, 13, 407, 11, 456, 366, 572, 2107, 28263, 2430, 763, 11, 558, 30, 400, 550, 291, 360, 512, 979, 19866, 13, 51808], "temperature": 0.0, "avg_logprob": -0.10870411965699323, "compression_ratio": 1.786259541984733, "no_speech_prob": 0.003467344678938389}, {"id": 1345, "seek": 786828, "start": 7868.28, "end": 7871.8, "text": " That's for node classification, right? So, that's how the architecture looks like.", "tokens": [50364, 663, 311, 337, 9984, 21538, 11, 558, 30, 407, 11, 300, 311, 577, 264, 9482, 1542, 411, 13, 50540], "temperature": 0.0, "avg_logprob": -0.11774794523381005, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.0008216123096644878}, {"id": 1346, "seek": 786828, "start": 7872.5199999999995, "end": 7876.92, "text": " Now, if you compare it to the classical convolutional architectures like GCN of", "tokens": [50576, 823, 11, 498, 291, 6794, 309, 281, 264, 13735, 45216, 304, 6331, 1303, 411, 29435, 45, 295, 50796], "temperature": 0.0, "avg_logprob": -0.11774794523381005, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.0008216123096644878}, {"id": 1347, "seek": 786828, "start": 7876.92, "end": 7882.04, "text": " Kieff and Welling, so there are several differences. So, the GCN is non-residual,", "tokens": [50796, 591, 414, 602, 293, 1042, 278, 11, 370, 456, 366, 2940, 7300, 13, 407, 11, 264, 29435, 45, 307, 2107, 12, 495, 327, 901, 11, 51052], "temperature": 0.0, "avg_logprob": -0.11774794523381005, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.0008216123096644878}, {"id": 1348, "seek": 786828, "start": 7882.759999999999, "end": 7888.36, "text": " the weights are non-symmetric, and the weights are different per layer, right? And also,", "tokens": [51088, 264, 17443, 366, 2107, 12, 3187, 2174, 17475, 11, 293, 264, 17443, 366, 819, 680, 4583, 11, 558, 30, 400, 611, 11, 51368], "temperature": 0.0, "avg_logprob": -0.11774794523381005, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.0008216123096644878}, {"id": 1349, "seek": 786828, "start": 7888.36, "end": 7893.639999999999, "text": " they use nonlinear activation for every layer. We don't. So, in a sense, it's a kind of antithesis", "tokens": [51368, 436, 764, 2107, 28263, 24433, 337, 633, 4583, 13, 492, 500, 380, 13, 407, 11, 294, 257, 2020, 11, 309, 311, 257, 733, 295, 2511, 355, 9374, 51632], "temperature": 0.0, "avg_logprob": -0.11774794523381005, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.0008216123096644878}, {"id": 1350, "seek": 789364, "start": 7893.72, "end": 7899.88, "text": " to a typical graph neural network or a typical deep learning architecture. So, typically,", "tokens": [50368, 281, 257, 7476, 4295, 18161, 3209, 420, 257, 7476, 2452, 2539, 9482, 13, 407, 11, 5850, 11, 50676], "temperature": 0.0, "avg_logprob": -0.1579658233367645, "compression_ratio": 1.737037037037037, "no_speech_prob": 0.0018418707186356187}, {"id": 1351, "seek": 789364, "start": 7899.88, "end": 7904.04, "text": " you say that you need many layers, each layer parameterized separately and nonlinear are they", "tokens": [50676, 291, 584, 300, 291, 643, 867, 7914, 11, 1184, 4583, 13075, 1602, 14759, 293, 2107, 28263, 366, 436, 50884], "temperature": 0.0, "avg_logprob": -0.1579658233367645, "compression_ratio": 1.737037037037037, "no_speech_prob": 0.0018418707186356187}, {"id": 1352, "seek": 789364, "start": 7904.04, "end": 7908.6, "text": " activated. We don't have anything like this here, right? So, all the, again, the diffusion part is", "tokens": [50884, 18157, 13, 492, 500, 380, 362, 1340, 411, 341, 510, 11, 558, 30, 407, 11, 439, 264, 11, 797, 11, 264, 25242, 644, 307, 51112], "temperature": 0.0, "avg_logprob": -0.1579658233367645, "compression_ratio": 1.737037037037037, "no_speech_prob": 0.0018418707186356187}, {"id": 1353, "seek": 789364, "start": 7908.6, "end": 7915.400000000001, "text": " linear. We have a nonlinear decoder and potentially a nonlinear encoder. So, what we gain from the", "tokens": [51112, 8213, 13, 492, 362, 257, 2107, 28263, 979, 19866, 293, 7263, 257, 2107, 28263, 2058, 19866, 13, 407, 11, 437, 321, 6052, 490, 264, 51452], "temperature": 0.0, "avg_logprob": -0.1579658233367645, "compression_ratio": 1.737037037037037, "no_speech_prob": 0.0018418707186356187}, {"id": 1354, "seek": 789364, "start": 7915.400000000001, "end": 7921.08, "text": " it being gradient flow is interpretability in the following sense that we can show that", "tokens": [51452, 309, 885, 16235, 3095, 307, 7302, 2310, 294, 264, 3480, 2020, 300, 321, 393, 855, 300, 51736], "temperature": 0.0, "avg_logprob": -0.1579658233367645, "compression_ratio": 1.737037037037037, "no_speech_prob": 0.0018418707186356187}, {"id": 1355, "seek": 792108, "start": 7921.08, "end": 7926.04, "text": " in certain situations we can induce both low and high frequency dominated dynamics.", "tokens": [50364, 294, 1629, 6851, 321, 393, 41263, 1293, 2295, 293, 1090, 7893, 23755, 15679, 13, 50612], "temperature": 0.0, "avg_logprob": -0.09013778723559333, "compression_ratio": 1.6691449814126393, "no_speech_prob": 0.00194785394705832}, {"id": 1356, "seek": 792108, "start": 7926.04, "end": 7931.88, "text": " I will define it in a second. And what it means is that we can work both with homophilic and", "tokens": [50612, 286, 486, 6964, 309, 294, 257, 1150, 13, 400, 437, 309, 1355, 307, 300, 321, 393, 589, 1293, 365, 3655, 5317, 388, 299, 293, 50904], "temperature": 0.0, "avg_logprob": -0.09013778723559333, "compression_ratio": 1.6691449814126393, "no_speech_prob": 0.00194785394705832}, {"id": 1357, "seek": 792108, "start": 7931.88, "end": 7936.76, "text": " heterophilic graphs. Now, because the weights are shared across layers, actually the number of layers", "tokens": [50904, 20789, 5317, 388, 299, 24877, 13, 823, 11, 570, 264, 17443, 366, 5507, 2108, 7914, 11, 767, 264, 1230, 295, 7914, 51148], "temperature": 0.0, "avg_logprob": -0.09013778723559333, "compression_ratio": 1.6691449814126393, "no_speech_prob": 0.00194785394705832}, {"id": 1358, "seek": 792108, "start": 7936.76, "end": 7942.04, "text": " becomes a completely irrelevant notion here. So, what matters is really the diffusion time.", "tokens": [51148, 3643, 257, 2584, 28682, 10710, 510, 13, 407, 11, 437, 7001, 307, 534, 264, 25242, 565, 13, 51412], "temperature": 0.0, "avg_logprob": -0.09013778723559333, "compression_ratio": 1.6691449814126393, "no_speech_prob": 0.00194785394705832}, {"id": 1359, "seek": 792108, "start": 7942.76, "end": 7946.44, "text": " The number of layers is just how finely I discretize my differential equation.", "tokens": [51448, 440, 1230, 295, 7914, 307, 445, 577, 31529, 286, 25656, 1125, 452, 15756, 5367, 13, 51632], "temperature": 0.0, "avg_logprob": -0.09013778723559333, "compression_ratio": 1.6691449814126393, "no_speech_prob": 0.00194785394705832}, {"id": 1360, "seek": 794644, "start": 7947.4, "end": 7949.879999999999, "text": " And the number of parameters is independent of it, right?", "tokens": [50412, 400, 264, 1230, 295, 9834, 307, 6695, 295, 309, 11, 558, 30, 50536], "temperature": 0.0, "avg_logprob": -0.14745378494262695, "compression_ratio": 1.7469879518072289, "no_speech_prob": 0.004108595661818981}, {"id": 1361, "seek": 794644, "start": 7950.919999999999, "end": 7954.599999999999, "text": " Unlike, again, the classical architecture where the more layers you have, the more parameters", "tokens": [50588, 17657, 11, 797, 11, 264, 13735, 9482, 689, 264, 544, 7914, 291, 362, 11, 264, 544, 9834, 50772], "temperature": 0.0, "avg_logprob": -0.14745378494262695, "compression_ratio": 1.7469879518072289, "no_speech_prob": 0.004108595661818981}, {"id": 1362, "seek": 794644, "start": 7954.599999999999, "end": 7960.5199999999995, "text": " you have. Yeah, question? Question. Why don't you stay in the latent space? Why do you have", "tokens": [50772, 291, 362, 13, 865, 11, 1168, 30, 14464, 13, 1545, 500, 380, 291, 1754, 294, 264, 48994, 1901, 30, 1545, 360, 291, 362, 51068], "temperature": 0.0, "avg_logprob": -0.14745378494262695, "compression_ratio": 1.7469879518072289, "no_speech_prob": 0.004108595661818981}, {"id": 1363, "seek": 794644, "start": 7960.5199999999995, "end": 7966.44, "text": " needed a decoder? Why do I need a decoder? Well, the decoder can, for example, you might need it to", "tokens": [51068, 2978, 257, 979, 19866, 30, 1545, 360, 286, 643, 257, 979, 19866, 30, 1042, 11, 264, 979, 19866, 393, 11, 337, 1365, 11, 291, 1062, 643, 309, 281, 51364], "temperature": 0.0, "avg_logprob": -0.14745378494262695, "compression_ratio": 1.7469879518072289, "no_speech_prob": 0.004108595661818981}, {"id": 1364, "seek": 794644, "start": 7966.44, "end": 7974.599999999999, "text": " produce a label for an old. It's not the decoder in the original space. It could be in any,", "tokens": [51364, 5258, 257, 7645, 337, 364, 1331, 13, 467, 311, 406, 264, 979, 19866, 294, 264, 3380, 1901, 13, 467, 727, 312, 294, 604, 11, 51772], "temperature": 0.0, "avg_logprob": -0.14745378494262695, "compression_ratio": 1.7469879518072289, "no_speech_prob": 0.004108595661818981}, {"id": 1365, "seek": 797460, "start": 7975.240000000001, "end": 7978.4400000000005, "text": " it could be also decoding the original space, right? So, you might, for example,", "tokens": [50396, 309, 727, 312, 611, 979, 8616, 264, 3380, 1901, 11, 558, 30, 407, 11, 291, 1062, 11, 337, 1365, 11, 50556], "temperature": 0.0, "avg_logprob": -0.08204087467058331, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.0034012864343822002}, {"id": 1366, "seek": 797460, "start": 7978.4400000000005, "end": 7983.320000000001, "text": " want to work with low dimensional space and then the number of your classes might be different,", "tokens": [50556, 528, 281, 589, 365, 2295, 18795, 1901, 293, 550, 264, 1230, 295, 428, 5359, 1062, 312, 819, 11, 50800], "temperature": 0.0, "avg_logprob": -0.08204087467058331, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.0034012864343822002}, {"id": 1367, "seek": 797460, "start": 7983.320000000001, "end": 7989.4800000000005, "text": " right? But the important part is that it's nonlinear. So, all the nonlinearity goes there.", "tokens": [50800, 558, 30, 583, 264, 1021, 644, 307, 300, 309, 311, 2107, 28263, 13, 407, 11, 439, 264, 2107, 1889, 17409, 1709, 456, 13, 51108], "temperature": 0.0, "avg_logprob": -0.08204087467058331, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.0034012864343822002}, {"id": 1368, "seek": 797460, "start": 7990.92, "end": 7995.08, "text": " Okay. And, well, you can also use an encoder. I will show why encoder can be problematic.", "tokens": [51180, 1033, 13, 400, 11, 731, 11, 291, 393, 611, 764, 364, 2058, 19866, 13, 286, 486, 855, 983, 2058, 19866, 393, 312, 19011, 13, 51388], "temperature": 0.0, "avg_logprob": -0.08204087467058331, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.0034012864343822002}, {"id": 1369, "seek": 797460, "start": 7995.08, "end": 7997.96, "text": " We actually, we have experimental evidence that it's not needed.", "tokens": [51388, 492, 767, 11, 321, 362, 17069, 4467, 300, 309, 311, 406, 2978, 13, 51532], "temperature": 0.0, "avg_logprob": -0.08204087467058331, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.0034012864343822002}, {"id": 1370, "seek": 797460, "start": 7999.320000000001, "end": 8003.72, "text": " So, first of all, let's talk about homophilic and heterophilic graphs. So, again, homophilic", "tokens": [51600, 407, 11, 700, 295, 439, 11, 718, 311, 751, 466, 3655, 5317, 388, 299, 293, 20789, 5317, 388, 299, 24877, 13, 407, 11, 797, 11, 3655, 5317, 388, 299, 51820], "temperature": 0.0, "avg_logprob": -0.08204087467058331, "compression_ratio": 1.7166666666666666, "no_speech_prob": 0.0034012864343822002}, {"id": 1371, "seek": 800372, "start": 8003.72, "end": 8008.6, "text": " look like this, heterophilic look like this. And the data set here is what we call synthetic", "tokens": [50364, 574, 411, 341, 11, 20789, 5317, 388, 299, 574, 411, 341, 13, 400, 264, 1412, 992, 510, 307, 437, 321, 818, 23420, 50608], "temperature": 0.0, "avg_logprob": -0.10627770624240908, "compression_ratio": 1.8237547892720307, "no_speech_prob": 0.005468249786645174}, {"id": 1372, "seek": 800372, "start": 8008.6, "end": 8015.0, "text": " core. So, it's probably you're all familiar with it. So, it's this citation network where we can", "tokens": [50608, 4965, 13, 407, 11, 309, 311, 1391, 291, 434, 439, 4963, 365, 309, 13, 407, 11, 309, 311, 341, 45590, 3209, 689, 321, 393, 50928], "temperature": 0.0, "avg_logprob": -0.10627770624240908, "compression_ratio": 1.8237547892720307, "no_speech_prob": 0.005468249786645174}, {"id": 1373, "seek": 800372, "start": 8015.0, "end": 8019.96, "text": " actually change the structure of the graph in a way that it becomes either more homophilic or more", "tokens": [50928, 767, 1319, 264, 3877, 295, 264, 4295, 294, 257, 636, 300, 309, 3643, 2139, 544, 3655, 5317, 388, 299, 420, 544, 51176], "temperature": 0.0, "avg_logprob": -0.10627770624240908, "compression_ratio": 1.8237547892720307, "no_speech_prob": 0.005468249786645174}, {"id": 1374, "seek": 800372, "start": 8019.96, "end": 8026.4400000000005, "text": " heterophilic, right? And here are the two extreme choices of an architecture. So, the task here", "tokens": [51176, 20789, 5317, 388, 299, 11, 558, 30, 400, 510, 366, 264, 732, 8084, 7994, 295, 364, 9482, 13, 407, 11, 264, 5633, 510, 51500], "temperature": 0.0, "avg_logprob": -0.10627770624240908, "compression_ratio": 1.8237547892720307, "no_speech_prob": 0.005468249786645174}, {"id": 1375, "seek": 800372, "start": 8026.4400000000005, "end": 8032.2, "text": " is no classification. I can either ignore the structure of the graph altogether and do just", "tokens": [51500, 307, 572, 21538, 13, 286, 393, 2139, 11200, 264, 3877, 295, 264, 4295, 19051, 293, 360, 445, 51788], "temperature": 0.0, "avg_logprob": -0.10627770624240908, "compression_ratio": 1.8237547892720307, "no_speech_prob": 0.005468249786645174}, {"id": 1376, "seek": 803220, "start": 8032.28, "end": 8037.48, "text": " no wise predictions with a multilayer perceptron. And, of course, it's not great, right? So, it", "tokens": [50368, 572, 10829, 21264, 365, 257, 2120, 388, 11167, 43276, 2044, 13, 400, 11, 295, 1164, 11, 309, 311, 406, 869, 11, 558, 30, 407, 11, 309, 50628], "temperature": 0.0, "avg_logprob": -0.12759259076622442, "compression_ratio": 1.6113074204946995, "no_speech_prob": 0.0021079136058688164}, {"id": 1377, "seek": 803220, "start": 8037.48, "end": 8043.96, "text": " achieves about 67% accuracy. But no matter how homophilic or heterophilic data set is,", "tokens": [50628, 3538, 977, 466, 23879, 4, 14170, 13, 583, 572, 1871, 577, 3655, 5317, 388, 299, 420, 20789, 5317, 388, 299, 1412, 992, 307, 11, 50952], "temperature": 0.0, "avg_logprob": -0.12759259076622442, "compression_ratio": 1.6113074204946995, "no_speech_prob": 0.0021079136058688164}, {"id": 1378, "seek": 803220, "start": 8044.599999999999, "end": 8050.36, "text": " the result is the same because it simply ignores the graph, right? The other extreme is a GCN.", "tokens": [50984, 264, 1874, 307, 264, 912, 570, 309, 2935, 5335, 2706, 264, 4295, 11, 558, 30, 440, 661, 8084, 307, 257, 29435, 45, 13, 51272], "temperature": 0.0, "avg_logprob": -0.12759259076622442, "compression_ratio": 1.6113074204946995, "no_speech_prob": 0.0021079136058688164}, {"id": 1379, "seek": 803220, "start": 8050.36, "end": 8054.599999999999, "text": " So, when the graph is homophilic, it works extremely well, right? Almost 100% because", "tokens": [51272, 407, 11, 562, 264, 4295, 307, 3655, 5317, 388, 299, 11, 309, 1985, 4664, 731, 11, 558, 30, 12627, 2319, 4, 570, 51484], "temperature": 0.0, "avg_logprob": -0.12759259076622442, "compression_ratio": 1.6113074204946995, "no_speech_prob": 0.0021079136058688164}, {"id": 1380, "seek": 803220, "start": 8054.599999999999, "end": 8060.04, "text": " when my neighbors contain similar information, I will be basically averaging them and I will", "tokens": [51484, 562, 452, 12512, 5304, 2531, 1589, 11, 286, 486, 312, 1936, 47308, 552, 293, 286, 486, 51756], "temperature": 0.0, "avg_logprob": -0.12759259076622442, "compression_ratio": 1.6113074204946995, "no_speech_prob": 0.0021079136058688164}, {"id": 1381, "seek": 806004, "start": 8060.04, "end": 8064.92, "text": " be doing some form of denoising. But when the graph is heterophilic, then it degrades very", "tokens": [50364, 312, 884, 512, 1254, 295, 1441, 78, 3436, 13, 583, 562, 264, 4295, 307, 20789, 5317, 388, 299, 11, 550, 309, 368, 22626, 588, 50608], "temperature": 0.0, "avg_logprob": -0.09054752089019515, "compression_ratio": 1.695970695970696, "no_speech_prob": 0.0015480543952435255}, {"id": 1382, "seek": 806004, "start": 8064.92, "end": 8068.92, "text": " badly because, in this case, the neighbor information is more detrimental than helpful.", "tokens": [50608, 13425, 570, 11, 294, 341, 1389, 11, 264, 5987, 1589, 307, 544, 45694, 813, 4961, 13, 50808], "temperature": 0.0, "avg_logprob": -0.09054752089019515, "compression_ratio": 1.695970695970696, "no_speech_prob": 0.0015480543952435255}, {"id": 1383, "seek": 806004, "start": 8069.64, "end": 8075.32, "text": " So, the gradient flow framework, it benefits from basically its kind of mixture of both worlds. So,", "tokens": [50844, 407, 11, 264, 16235, 3095, 8388, 11, 309, 5311, 490, 1936, 1080, 733, 295, 9925, 295, 1293, 13401, 13, 407, 11, 51128], "temperature": 0.0, "avg_logprob": -0.09054752089019515, "compression_ratio": 1.695970695970696, "no_speech_prob": 0.0015480543952435255}, {"id": 1384, "seek": 806004, "start": 8075.32, "end": 8081.96, "text": " it works as well as GCN in the homophilic case and as well as node-wise predictions in the", "tokens": [51128, 309, 1985, 382, 731, 382, 29435, 45, 294, 264, 3655, 5317, 388, 299, 1389, 293, 382, 731, 382, 9984, 12, 3711, 21264, 294, 264, 51460], "temperature": 0.0, "avg_logprob": -0.09054752089019515, "compression_ratio": 1.695970695970696, "no_speech_prob": 0.0015480543952435255}, {"id": 1385, "seek": 806004, "start": 8081.96, "end": 8087.56, "text": " heterophilic case and the graceful transitions between the two. Now, another important thing,", "tokens": [51460, 20789, 5317, 388, 299, 1389, 293, 264, 10042, 906, 23767, 1296, 264, 732, 13, 823, 11, 1071, 1021, 551, 11, 51740], "temperature": 0.0, "avg_logprob": -0.09054752089019515, "compression_ratio": 1.695970695970696, "no_speech_prob": 0.0015480543952435255}, {"id": 1386, "seek": 808756, "start": 8087.56, "end": 8092.4400000000005, "text": " right? And that's where the encoder and decoder question comes into the play. So, if you write", "tokens": [50364, 558, 30, 400, 300, 311, 689, 264, 2058, 19866, 293, 979, 19866, 1168, 1487, 666, 264, 862, 13, 407, 11, 498, 291, 2464, 50608], "temperature": 0.0, "avg_logprob": -0.13222335815429687, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.008041899651288986}, {"id": 1387, "seek": 808756, "start": 8092.4400000000005, "end": 8097.96, "text": " the output of the neural network after L layers, this is how it looks like. So, it's basically", "tokens": [50608, 264, 5598, 295, 264, 18161, 3209, 934, 441, 7914, 11, 341, 307, 577, 309, 1542, 411, 13, 407, 11, 309, 311, 1936, 50884], "temperature": 0.0, "avg_logprob": -0.13222335815429687, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.008041899651288986}, {"id": 1388, "seek": 808756, "start": 8097.96, "end": 8101.56, "text": " it's a polynomial in these matrices. So, the matrices are dependent, of course. So, there are", "tokens": [50884, 309, 311, 257, 26110, 294, 613, 32284, 13, 407, 11, 264, 32284, 366, 12334, 11, 295, 1164, 13, 407, 11, 456, 366, 51064], "temperature": 0.0, "avg_logprob": -0.13222335815429687, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.008041899651288986}, {"id": 1389, "seek": 808756, "start": 8101.56, "end": 8108.4400000000005, "text": " some powers here. The parameters are shared. But if we ignore the encoder, right, and we can ignore", "tokens": [51064, 512, 8674, 510, 13, 440, 9834, 366, 5507, 13, 583, 498, 321, 11200, 264, 2058, 19866, 11, 558, 11, 293, 321, 393, 11200, 51408], "temperature": 0.0, "avg_logprob": -0.13222335815429687, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.008041899651288986}, {"id": 1390, "seek": 808756, "start": 8108.4400000000005, "end": 8115.4800000000005, "text": " it, at least in the test that we did, basically what the diffusion part can be precomputed,", "tokens": [51408, 309, 11, 412, 1935, 294, 264, 1500, 300, 321, 630, 11, 1936, 437, 264, 25242, 644, 393, 312, 659, 1112, 2582, 292, 11, 51760], "temperature": 0.0, "avg_logprob": -0.13222335815429687, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.008041899651288986}, {"id": 1391, "seek": 811548, "start": 8115.48, "end": 8120.44, "text": " right? You see that it acts as some kind of powers of the diffusion matrix on the features. So,", "tokens": [50364, 558, 30, 509, 536, 300, 309, 10672, 382, 512, 733, 295, 8674, 295, 264, 25242, 8141, 322, 264, 4122, 13, 407, 11, 50612], "temperature": 0.0, "avg_logprob": -0.1268713858819777, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.002754865912720561}, {"id": 1392, "seek": 811548, "start": 8120.44, "end": 8126.28, "text": " I can pre-diffuse the features once as a pre-computation step and then it all boils down to node-wise", "tokens": [50612, 286, 393, 659, 12, 67, 3661, 438, 264, 4122, 1564, 382, 257, 659, 12, 1112, 2582, 399, 1823, 293, 550, 309, 439, 35049, 760, 281, 9984, 12, 3711, 50904], "temperature": 0.0, "avg_logprob": -0.1268713858819777, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.002754865912720561}, {"id": 1393, "seek": 811548, "start": 8127.719999999999, "end": 8133.959999999999, "text": " multiplications by these matrices. So, the computationally difficult part, both computation", "tokens": [50976, 17596, 763, 538, 613, 32284, 13, 407, 11, 264, 24903, 379, 2252, 644, 11, 1293, 24903, 51288], "temperature": 0.0, "avg_logprob": -0.1268713858819777, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.002754865912720561}, {"id": 1394, "seek": 811548, "start": 8133.959999999999, "end": 8138.759999999999, "text": " in terms of the number of multiplication operations as well as the memory access, which,", "tokens": [51288, 294, 2115, 295, 264, 1230, 295, 27290, 7705, 382, 731, 382, 264, 4675, 2105, 11, 597, 11, 51528], "temperature": 0.0, "avg_logprob": -0.1268713858819777, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.002754865912720561}, {"id": 1395, "seek": 813876, "start": 8139.56, "end": 8145.16, "text": " unless the graph is very structured, you have random access to your neighbor nodes,", "tokens": [50404, 5969, 264, 4295, 307, 588, 18519, 11, 291, 362, 4974, 2105, 281, 428, 5987, 13891, 11, 50684], "temperature": 0.0, "avg_logprob": -0.11345815658569336, "compression_ratio": 1.7518518518518518, "no_speech_prob": 0.005101170390844345}, {"id": 1396, "seek": 813876, "start": 8145.8, "end": 8151.320000000001, "text": " this is really the heaviest part of graph neural networks. So, this now can be totally trivialized.", "tokens": [50716, 341, 307, 534, 264, 3577, 6495, 644, 295, 4295, 18161, 9590, 13, 407, 11, 341, 586, 393, 312, 3879, 26703, 1602, 13, 50992], "temperature": 0.0, "avg_logprob": -0.11345815658569336, "compression_ratio": 1.7518518518518518, "no_speech_prob": 0.005101170390844345}, {"id": 1397, "seek": 813876, "start": 8152.4400000000005, "end": 8157.400000000001, "text": " So, we've done the earlier versions of this architecture with just one convolutional layer.", "tokens": [51048, 407, 11, 321, 600, 1096, 264, 3071, 9606, 295, 341, 9482, 365, 445, 472, 45216, 304, 4583, 13, 51296], "temperature": 0.0, "avg_logprob": -0.11345815658569336, "compression_ratio": 1.7518518518518518, "no_speech_prob": 0.005101170390844345}, {"id": 1398, "seek": 813876, "start": 8157.400000000001, "end": 8163.0, "text": " We call it sine. So, already several years ago, we tested this on the graphs of hundreds of millions", "tokens": [51296, 492, 818, 309, 18609, 13, 407, 11, 1217, 2940, 924, 2057, 11, 321, 8246, 341, 322, 264, 24877, 295, 6779, 295, 6803, 51576], "temperature": 0.0, "avg_logprob": -0.11345815658569336, "compression_ratio": 1.7518518518518518, "no_speech_prob": 0.005101170390844345}, {"id": 1399, "seek": 813876, "start": 8163.0, "end": 8167.96, "text": " of nodes. So, with this architecture, actually, because now we also get rid of the non-linearity", "tokens": [51576, 295, 13891, 13, 407, 11, 365, 341, 9482, 11, 767, 11, 570, 586, 321, 611, 483, 3973, 295, 264, 2107, 12, 1889, 17409, 51824], "temperature": 0.0, "avg_logprob": -0.11345815658569336, "compression_ratio": 1.7518518518518518, "no_speech_prob": 0.005101170390844345}, {"id": 1400, "seek": 816876, "start": 8168.76, "end": 8171.8, "text": " you can apply it to a very large graph, basically, you can design", "tokens": [50364, 291, 393, 3079, 309, 281, 257, 588, 2416, 4295, 11, 1936, 11, 291, 393, 1715, 50516], "temperature": 0.0, "avg_logprob": -0.1578555210776951, "compression_ratio": 1.5707762557077625, "no_speech_prob": 0.0004669967165682465}, {"id": 1401, "seek": 816876, "start": 8171.8, "end": 8175.4800000000005, "text": " multi-layer architectures that work well both in homophilic and heterophilic settings.", "tokens": [50516, 4825, 12, 8376, 260, 6331, 1303, 300, 589, 731, 1293, 294, 3655, 5317, 388, 299, 293, 20789, 5317, 388, 299, 6257, 13, 50700], "temperature": 0.0, "avg_logprob": -0.1578555210776951, "compression_ratio": 1.5707762557077625, "no_speech_prob": 0.0004669967165682465}, {"id": 1402, "seek": 816876, "start": 8176.92, "end": 8183.08, "text": " Now, another thing, and this is what I mentioned regarding the nature of the dynamics that is", "tokens": [50772, 823, 11, 1071, 551, 11, 293, 341, 307, 437, 286, 2835, 8595, 264, 3687, 295, 264, 15679, 300, 307, 51080], "temperature": 0.0, "avg_logprob": -0.1578555210776951, "compression_ratio": 1.5707762557077625, "no_speech_prob": 0.0004669967165682465}, {"id": 1403, "seek": 816876, "start": 8183.08, "end": 8191.88, "text": " induced by this gradient flow, is if we look at our data, right, and our data, I remind you, it's", "tokens": [51080, 33991, 538, 341, 16235, 3095, 11, 307, 498, 321, 574, 412, 527, 1412, 11, 558, 11, 293, 527, 1412, 11, 286, 4160, 291, 11, 309, 311, 51520], "temperature": 0.0, "avg_logprob": -0.1578555210776951, "compression_ratio": 1.5707762557077625, "no_speech_prob": 0.0004669967165682465}, {"id": 1404, "seek": 819188, "start": 8192.84, "end": 8198.76, "text": " the matrix x, right, which is of size n by d and is the number of nodes, d is the number of dimensions.", "tokens": [50412, 264, 8141, 2031, 11, 558, 11, 597, 307, 295, 2744, 297, 538, 274, 293, 307, 264, 1230, 295, 13891, 11, 274, 307, 264, 1230, 295, 12819, 13, 50708], "temperature": 0.0, "avg_logprob": -0.16923822895173105, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.004031159915030003}, {"id": 1405, "seek": 819188, "start": 8199.960000000001, "end": 8204.44, "text": " So, we can do an analogy of a two-dimensional Fourier transform, right. I remind you that", "tokens": [50768, 407, 11, 321, 393, 360, 364, 21663, 295, 257, 732, 12, 18759, 36810, 4088, 11, 558, 13, 286, 4160, 291, 300, 50992], "temperature": 0.0, "avg_logprob": -0.16923822895173105, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.004031159915030003}, {"id": 1406, "seek": 819188, "start": 8204.44, "end": 8209.24, "text": " for a graph that we assumed, an undirected graph, the graph of plus n has orthogonal", "tokens": [50992, 337, 257, 4295, 300, 321, 15895, 11, 364, 674, 11890, 292, 4295, 11, 264, 4295, 295, 1804, 297, 575, 41488, 51232], "temperature": 0.0, "avg_logprob": -0.16923822895173105, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.004031159915030003}, {"id": 1407, "seek": 819188, "start": 8209.24, "end": 8214.2, "text": " identity composition. So, it's eigenvectors form an orthogonal basis, right, the basis for the rows", "tokens": [51232, 6575, 12686, 13, 407, 11, 309, 311, 10446, 303, 5547, 1254, 364, 41488, 5143, 11, 558, 11, 264, 5143, 337, 264, 13241, 51480], "temperature": 0.0, "avg_logprob": -0.16923822895173105, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.004031159915030003}, {"id": 1408, "seek": 819188, "start": 8214.2, "end": 8220.36, "text": " of the matrix, and the matrix w, which we assumed by virtue of our process being a gradient flow,", "tokens": [51480, 295, 264, 8141, 11, 293, 264, 8141, 261, 11, 597, 321, 15895, 538, 20816, 295, 527, 1399, 885, 257, 16235, 3095, 11, 51788], "temperature": 0.0, "avg_logprob": -0.16923822895173105, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.004031159915030003}, {"id": 1409, "seek": 822036, "start": 8220.36, "end": 8224.76, "text": " right, it was symmetric, we can have also an orthogonal identity composition, let's call", "tokens": [50364, 558, 11, 309, 390, 32330, 11, 321, 393, 362, 611, 364, 41488, 6575, 12686, 11, 718, 311, 818, 50584], "temperature": 0.0, "avg_logprob": -0.1534003163432027, "compression_ratio": 1.8398058252427185, "no_speech_prob": 0.0013371877139434218}, {"id": 1410, "seek": 822036, "start": 8224.76, "end": 8230.36, "text": " it eigenvectors psi and eigenvalues mu, so it forms the orthogonal basis for the columns of", "tokens": [50584, 309, 10446, 303, 5547, 20304, 293, 10446, 46033, 2992, 11, 370, 309, 6422, 264, 41488, 5143, 337, 264, 13766, 295, 50864], "temperature": 0.0, "avg_logprob": -0.1534003163432027, "compression_ratio": 1.8398058252427185, "no_speech_prob": 0.0013371877139434218}, {"id": 1411, "seek": 822036, "start": 8230.36, "end": 8237.16, "text": " these matrix, right, for the dimension d of these matrix, and now in these two-dimensional Fourier", "tokens": [50864, 613, 8141, 11, 558, 11, 337, 264, 10139, 274, 295, 613, 8141, 11, 293, 586, 294, 613, 732, 12, 18759, 36810, 51204], "temperature": 0.0, "avg_logprob": -0.1534003163432027, "compression_ratio": 1.8398058252427185, "no_speech_prob": 0.0013371877139434218}, {"id": 1412, "seek": 822036, "start": 8237.16, "end": 8243.880000000001, "text": " basis, right, we can take tensor products of these basis functions of phi and psi, we can write the", "tokens": [51204, 5143, 11, 558, 11, 321, 393, 747, 40863, 3383, 295, 613, 5143, 6828, 295, 13107, 293, 20304, 11, 321, 393, 2464, 264, 51540], "temperature": 0.0, "avg_logprob": -0.1534003163432027, "compression_ratio": 1.8398058252427185, "no_speech_prob": 0.0013371877139434218}, {"id": 1413, "seek": 824388, "start": 8243.88, "end": 8251.32, "text": " output of the neural network like this, right, and what you see here is that we have some filter", "tokens": [50364, 5598, 295, 264, 18161, 3209, 411, 341, 11, 558, 11, 293, 437, 291, 536, 510, 307, 300, 321, 362, 512, 6608, 50736], "temperature": 0.0, "avg_logprob": -0.101285717274883, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.01416202262043953}, {"id": 1414, "seek": 824388, "start": 8251.32, "end": 8256.599999999999, "text": " that acts on our signal, right, so this is signal, so these are tensor products, right, so that's", "tokens": [50736, 300, 10672, 322, 527, 6358, 11, 558, 11, 370, 341, 307, 6358, 11, 370, 613, 366, 40863, 3383, 11, 558, 11, 370, 300, 311, 51000], "temperature": 0.0, "avg_logprob": -0.101285717274883, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.01416202262043953}, {"id": 1415, "seek": 824388, "start": 8256.599999999999, "end": 8263.64, "text": " the analogy of two-dimensional Fourier transform, sinusoids of sine mx by sine ny, something like", "tokens": [51000, 264, 21663, 295, 732, 12, 18759, 36810, 4088, 11, 3343, 24431, 3742, 295, 18609, 275, 87, 538, 18609, 18052, 11, 746, 411, 51352], "temperature": 0.0, "avg_logprob": -0.101285717274883, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.01416202262043953}, {"id": 1416, "seek": 824388, "start": 8263.64, "end": 8271.08, "text": " this, right, so that's our analogy of this, so this is a filter, and it works both with the", "tokens": [51352, 341, 11, 558, 11, 370, 300, 311, 527, 21663, 295, 341, 11, 370, 341, 307, 257, 6608, 11, 293, 309, 1985, 1293, 365, 264, 51724], "temperature": 0.0, "avg_logprob": -0.101285717274883, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.01416202262043953}, {"id": 1417, "seek": 827108, "start": 8271.08, "end": 8277.88, "text": " frequencies, the eigenvalues of the graph Laplacian and the matrix w, right, lambda and mu, and you", "tokens": [50364, 20250, 11, 264, 10446, 46033, 295, 264, 4295, 2369, 564, 326, 952, 293, 264, 8141, 261, 11, 558, 11, 13607, 293, 2992, 11, 293, 291, 50704], "temperature": 0.0, "avg_logprob": -0.11416900263423413, "compression_ratio": 2.172727272727273, "no_speech_prob": 0.004220583476126194}, {"id": 1418, "seek": 827108, "start": 8277.88, "end": 8283.72, "text": " see that the low frequencies of the graphs are magnified by the positive eigenvalues of w,", "tokens": [50704, 536, 300, 264, 2295, 20250, 295, 264, 24877, 366, 4944, 2587, 538, 264, 3353, 10446, 46033, 295, 261, 11, 50996], "temperature": 0.0, "avg_logprob": -0.11416900263423413, "compression_ratio": 2.172727272727273, "no_speech_prob": 0.004220583476126194}, {"id": 1419, "seek": 827108, "start": 8283.72, "end": 8288.36, "text": " and conversely the high frequencies of the graph are magnified by the negative eigenvalues of w,", "tokens": [50996, 293, 2615, 736, 264, 1090, 20250, 295, 264, 4295, 366, 4944, 2587, 538, 264, 3671, 10446, 46033, 295, 261, 11, 51228], "temperature": 0.0, "avg_logprob": -0.11416900263423413, "compression_ratio": 2.172727272727273, "no_speech_prob": 0.004220583476126194}, {"id": 1420, "seek": 827108, "start": 8288.36, "end": 8294.52, "text": " and you can show that if we choose the matrix w in such a way that it has sufficiently negative", "tokens": [51228, 293, 291, 393, 855, 300, 498, 321, 2826, 264, 8141, 261, 294, 1270, 257, 636, 300, 309, 575, 31868, 3671, 51536], "temperature": 0.0, "avg_logprob": -0.11416900263423413, "compression_ratio": 2.172727272727273, "no_speech_prob": 0.004220583476126194}, {"id": 1421, "seek": 827108, "start": 8294.52, "end": 8299.88, "text": " eigenvalues, then this gradient flow dynamics is high frequency dominant in the sense that the", "tokens": [51536, 10446, 46033, 11, 550, 341, 16235, 3095, 15679, 307, 1090, 7893, 15657, 294, 264, 2020, 300, 264, 51804], "temperature": 0.0, "avg_logprob": -0.11416900263423413, "compression_ratio": 2.172727272727273, "no_speech_prob": 0.004220583476126194}, {"id": 1422, "seek": 829988, "start": 8299.88, "end": 8304.519999999999, "text": " Dirichlet energy or more correctly the normalized Dirichlet energy doesn't converge to zero,", "tokens": [50364, 34422, 480, 2631, 2281, 420, 544, 8944, 264, 48704, 34422, 480, 2631, 2281, 1177, 380, 41881, 281, 4018, 11, 50596], "temperature": 0.0, "avg_logprob": -0.08019662939983865, "compression_ratio": 1.896, "no_speech_prob": 0.0013491420540958643}, {"id": 1423, "seek": 829988, "start": 8304.519999999999, "end": 8308.679999999998, "text": " so it means that we don't have over smoothing, right, in the case of over smoothing this thing", "tokens": [50596, 370, 309, 1355, 300, 321, 500, 380, 362, 670, 899, 6259, 571, 11, 558, 11, 294, 264, 1389, 295, 670, 899, 6259, 571, 341, 551, 50804], "temperature": 0.0, "avg_logprob": -0.08019662939983865, "compression_ratio": 1.896, "no_speech_prob": 0.0013491420540958643}, {"id": 1424, "seek": 829988, "start": 8308.679999999998, "end": 8315.0, "text": " would be going to zero, but here it doesn't, right, so it means that we have some process that", "tokens": [50804, 576, 312, 516, 281, 4018, 11, 457, 510, 309, 1177, 380, 11, 558, 11, 370, 309, 1355, 300, 321, 362, 512, 1399, 300, 51120], "temperature": 0.0, "avg_logprob": -0.08019662939983865, "compression_ratio": 1.896, "no_speech_prob": 0.0013491420540958643}, {"id": 1425, "seek": 829988, "start": 8315.0, "end": 8322.599999999999, "text": " doesn't diffuse everything to a constant, it does something more interesting, right, and the", "tokens": [51120, 1177, 380, 42165, 1203, 281, 257, 5754, 11, 309, 775, 746, 544, 1880, 11, 558, 11, 293, 264, 51500], "temperature": 0.0, "avg_logprob": -0.08019662939983865, "compression_ratio": 1.896, "no_speech_prob": 0.0013491420540958643}, {"id": 1426, "seek": 829988, "start": 8322.599999999999, "end": 8328.359999999999, "text": " condition for it is w having sufficiently large negative eigenvalues, so the analogy of this would", "tokens": [51500, 4188, 337, 309, 307, 261, 1419, 31868, 2416, 3671, 10446, 46033, 11, 370, 264, 21663, 295, 341, 576, 51788], "temperature": 0.0, "avg_logprob": -0.08019662939983865, "compression_ratio": 1.896, "no_speech_prob": 0.0013491420540958643}, {"id": 1427, "seek": 832836, "start": 8328.36, "end": 8334.2, "text": " be, so if you think of diffusion processes blurring, what we have here is sharpening,", "tokens": [50364, 312, 11, 370, 498, 291, 519, 295, 25242, 7555, 14257, 2937, 11, 437, 321, 362, 510, 307, 8199, 4559, 11, 50656], "temperature": 0.0, "avg_logprob": -0.0887499177054073, "compression_ratio": 1.882051282051282, "no_speech_prob": 0.002131045563146472}, {"id": 1428, "seek": 832836, "start": 8334.2, "end": 8340.12, "text": " and we have both processes at the same time, so the attractive interactions do blurring,", "tokens": [50656, 293, 321, 362, 1293, 7555, 412, 264, 912, 565, 11, 370, 264, 12609, 13280, 360, 14257, 2937, 11, 50952], "temperature": 0.0, "avg_logprob": -0.0887499177054073, "compression_ratio": 1.882051282051282, "no_speech_prob": 0.002131045563146472}, {"id": 1429, "seek": 832836, "start": 8340.12, "end": 8344.44, "text": " the repulsive interactions do sharpening, so we have some directions in the fissure space where", "tokens": [50952, 264, 1085, 32657, 13280, 360, 8199, 4559, 11, 370, 321, 362, 512, 11095, 294, 264, 283, 891, 540, 1901, 689, 51168], "temperature": 0.0, "avg_logprob": -0.0887499177054073, "compression_ratio": 1.882051282051282, "no_speech_prob": 0.002131045563146472}, {"id": 1430, "seek": 832836, "start": 8345.24, "end": 8352.68, "text": " these processes happen at the same time, okay, questions? Yep, I think there are many questions,", "tokens": [51208, 613, 7555, 1051, 412, 264, 912, 565, 11, 1392, 11, 1651, 30, 7010, 11, 286, 519, 456, 366, 867, 1651, 11, 51580], "temperature": 0.0, "avg_logprob": -0.0887499177054073, "compression_ratio": 1.882051282051282, "no_speech_prob": 0.002131045563146472}, {"id": 1431, "seek": 835268, "start": 8352.68, "end": 8359.08, "text": " so it was very unclear. So in these situations where you don't", "tokens": [50364, 370, 309, 390, 588, 25636, 13, 407, 294, 613, 6851, 689, 291, 500, 380, 50684], "temperature": 0.0, "avg_logprob": -0.17752034489701435, "compression_ratio": 1.5940594059405941, "no_speech_prob": 0.005139876157045364}, {"id": 1432, "seek": 835268, "start": 8360.36, "end": 8364.84, "text": " dissipate to a constant value, are you obtaining some sort of chaotic behavior,", "tokens": [50748, 29544, 473, 281, 257, 5754, 2158, 11, 366, 291, 36749, 512, 1333, 295, 27013, 5223, 11, 50972], "temperature": 0.0, "avg_logprob": -0.17752034489701435, "compression_ratio": 1.5940594059405941, "no_speech_prob": 0.005139876157045364}, {"id": 1433, "seek": 835268, "start": 8364.84, "end": 8372.2, "text": " or are you obtaining other periodic oscillations of? So this is a sympathetic analysis, we don't", "tokens": [50972, 420, 366, 291, 36749, 661, 27790, 18225, 763, 295, 30, 407, 341, 307, 257, 36032, 5215, 11, 321, 500, 380, 51340], "temperature": 0.0, "avg_logprob": -0.17752034489701435, "compression_ratio": 1.5940594059405941, "no_speech_prob": 0.005139876157045364}, {"id": 1434, "seek": 835268, "start": 8372.2, "end": 8376.04, "text": " know, I don't think that I have an answer to your question, it might be that it's,", "tokens": [51340, 458, 11, 286, 500, 380, 519, 300, 286, 362, 364, 1867, 281, 428, 1168, 11, 309, 1062, 312, 300, 309, 311, 11, 51532], "temperature": 0.0, "avg_logprob": -0.17752034489701435, "compression_ratio": 1.5940594059405941, "no_speech_prob": 0.005139876157045364}, {"id": 1435, "seek": 837604, "start": 8376.36, "end": 8385.880000000001, "text": " so whether we have monotonicity, that's the question, I don't think so, but it might be the case.", "tokens": [50380, 370, 1968, 321, 362, 1108, 310, 11630, 507, 11, 300, 311, 264, 1168, 11, 286, 500, 380, 519, 370, 11, 457, 309, 1062, 312, 264, 1389, 13, 50856], "temperature": 0.0, "avg_logprob": -0.19956461588541666, "compression_ratio": 1.3402777777777777, "no_speech_prob": 0.0033323371317237616}, {"id": 1436, "seek": 837604, "start": 8394.92, "end": 8400.68, "text": " So if eigenvalues of w are sufficiently negative, then we can just kind of beat over squashing,", "tokens": [51308, 407, 498, 10446, 46033, 295, 261, 366, 31868, 3671, 11, 550, 321, 393, 445, 733, 295, 4224, 670, 2339, 11077, 11, 51596], "temperature": 0.0, "avg_logprob": -0.19956461588541666, "compression_ratio": 1.3402777777777777, "no_speech_prob": 0.0033323371317237616}, {"id": 1437, "seek": 840068, "start": 8401.48, "end": 8410.12, "text": " over smoothing, and the question is, is it possible, is it hard to do so because there are", "tokens": [50404, 670, 899, 6259, 571, 11, 293, 264, 1168, 307, 11, 307, 309, 1944, 11, 307, 309, 1152, 281, 360, 370, 570, 456, 366, 50836], "temperature": 0.0, "avg_logprob": -0.14490077933486628, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.013276015408337116}, {"id": 1438, "seek": 840068, "start": 8410.12, "end": 8415.16, "text": " two dynamics, is it hard to get those dynamics right, does it require a lot of hyperparameter", "tokens": [50836, 732, 15679, 11, 307, 309, 1152, 281, 483, 729, 15679, 558, 11, 775, 309, 3651, 257, 688, 295, 9848, 2181, 335, 2398, 51088], "temperature": 0.0, "avg_logprob": -0.14490077933486628, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.013276015408337116}, {"id": 1439, "seek": 840068, "start": 8415.16, "end": 8421.24, "text": " tuning or something like that? So it's a good question, so you can force, you can structure the", "tokens": [51088, 15164, 420, 746, 411, 300, 30, 407, 309, 311, 257, 665, 1168, 11, 370, 291, 393, 3464, 11, 291, 393, 3877, 264, 51392], "temperature": 0.0, "avg_logprob": -0.14490077933486628, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.013276015408337116}, {"id": 1440, "seek": 840068, "start": 8421.24, "end": 8427.16, "text": " matrix or parameterize the matrix w in such a way that it has two parts, positive eigenvalues and", "tokens": [51392, 8141, 420, 13075, 1125, 264, 8141, 261, 294, 1270, 257, 636, 300, 309, 575, 732, 3166, 11, 3353, 10446, 46033, 293, 51688], "temperature": 0.0, "avg_logprob": -0.14490077933486628, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.013276015408337116}, {"id": 1441, "seek": 842716, "start": 8427.16, "end": 8432.44, "text": " negative eigenvalues, and that's what we do, so basically we help the architecture to have both,", "tokens": [50364, 3671, 10446, 46033, 11, 293, 300, 311, 437, 321, 360, 11, 370, 1936, 321, 854, 264, 9482, 281, 362, 1293, 11, 50628], "temperature": 0.0, "avg_logprob": -0.09993495941162109, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.005348970647901297}, {"id": 1442, "seek": 842716, "start": 8432.44, "end": 8437.88, "text": " and then learning becomes easy. To learn it completely from scratch might be difficult,", "tokens": [50628, 293, 550, 2539, 3643, 1858, 13, 1407, 1466, 309, 2584, 490, 8459, 1062, 312, 2252, 11, 50900], "temperature": 0.0, "avg_logprob": -0.09993495941162109, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.005348970647901297}, {"id": 1443, "seek": 842716, "start": 8437.88, "end": 8443.96, "text": " and we see that GCNs which are in principle the same architectures without this restriction", "tokens": [50900, 293, 321, 536, 300, 29435, 45, 82, 597, 366, 294, 8665, 264, 912, 6331, 1303, 1553, 341, 29529, 51204], "temperature": 0.0, "avg_logprob": -0.09993495941162109, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.005348970647901297}, {"id": 1444, "seek": 842716, "start": 8443.96, "end": 8455.64, "text": " often fail to do it. My question went the same direction, but after the previous answer I got", "tokens": [51204, 2049, 3061, 281, 360, 309, 13, 1222, 1168, 1437, 264, 912, 3513, 11, 457, 934, 264, 3894, 1867, 286, 658, 51788], "temperature": 0.0, "avg_logprob": -0.09993495941162109, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.005348970647901297}, {"id": 1445, "seek": 845564, "start": 8456.599999999999, "end": 8463.56, "text": " more confused. So those, the matrices which, what's it, the delta and the w, they are learned,", "tokens": [50412, 544, 9019, 13, 407, 729, 11, 264, 32284, 597, 11, 437, 311, 309, 11, 264, 8289, 293, 264, 261, 11, 436, 366, 3264, 11, 50760], "temperature": 0.0, "avg_logprob": -0.19281794723955173, "compression_ratio": 1.8780487804878048, "no_speech_prob": 0.004548569675534964}, {"id": 1446, "seek": 845564, "start": 8463.56, "end": 8469.48, "text": " right, they are not hyperparameters. W, the elements of the matrix w are learned, right,", "tokens": [50760, 558, 11, 436, 366, 406, 9848, 2181, 335, 6202, 13, 343, 11, 264, 4959, 295, 264, 8141, 261, 366, 3264, 11, 558, 11, 51056], "temperature": 0.0, "avg_logprob": -0.19281794723955173, "compression_ratio": 1.8780487804878048, "no_speech_prob": 0.004548569675534964}, {"id": 1447, "seek": 845564, "start": 8469.48, "end": 8474.279999999999, "text": " but the matrix w has constraints, right, so it's, for example, it's symmetric, right, so it has half", "tokens": [51056, 457, 264, 8141, 261, 575, 18491, 11, 558, 11, 370, 309, 311, 11, 337, 1365, 11, 309, 311, 32330, 11, 558, 11, 370, 309, 575, 1922, 51296], "temperature": 0.0, "avg_logprob": -0.19281794723955173, "compression_ratio": 1.8780487804878048, "no_speech_prob": 0.004548569675534964}, {"id": 1448, "seek": 845564, "start": 8474.279999999999, "end": 8480.519999999999, "text": " of the elements of a general matrix w, and then basically by virtue of this theorem what it suggests", "tokens": [51296, 295, 264, 4959, 295, 257, 2674, 8141, 261, 11, 293, 550, 1936, 538, 20816, 295, 341, 20904, 437, 309, 13409, 51608], "temperature": 0.0, "avg_logprob": -0.19281794723955173, "compression_ratio": 1.8780487804878048, "no_speech_prob": 0.004548569675534964}, {"id": 1449, "seek": 848052, "start": 8480.68, "end": 8485.880000000001, "text": " is that we need to further restrict the structure of w, for example, to make it have negative", "tokens": [50372, 307, 300, 321, 643, 281, 3052, 7694, 264, 3877, 295, 261, 11, 337, 1365, 11, 281, 652, 309, 362, 3671, 50632], "temperature": 0.0, "avg_logprob": -0.1017091301050079, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.006230502855032682}, {"id": 1450, "seek": 848052, "start": 8485.880000000001, "end": 8493.16, "text": " eigenvalues. So typically what we do, we decompose it into a positive symmetric part and a negative", "tokens": [50632, 10446, 46033, 13, 407, 5850, 437, 321, 360, 11, 321, 22867, 541, 309, 666, 257, 3353, 32330, 644, 293, 257, 3671, 50996], "temperature": 0.0, "avg_logprob": -0.1017091301050079, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.006230502855032682}, {"id": 1451, "seek": 848052, "start": 8493.16, "end": 8500.04, "text": " symmetric part, and basically this way we guarantee that it has both positive and negative eigenvalues.", "tokens": [50996, 32330, 644, 11, 293, 1936, 341, 636, 321, 10815, 300, 309, 575, 1293, 3353, 293, 3671, 10446, 46033, 13, 51340], "temperature": 0.0, "avg_logprob": -0.1017091301050079, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.006230502855032682}, {"id": 1452, "seek": 848052, "start": 8501.560000000001, "end": 8508.68, "text": " Okay, so what is the principle component that makes graph work on heterophilic graphs better", "tokens": [51416, 1033, 11, 370, 437, 307, 264, 8665, 6542, 300, 1669, 4295, 589, 322, 20789, 5317, 388, 299, 24877, 1101, 51772], "temperature": 0.0, "avg_logprob": -0.1017091301050079, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.006230502855032682}, {"id": 1453, "seek": 850868, "start": 8508.68, "end": 8515.800000000001, "text": " than GCNs, and can you somehow update GCNs in a way that they will also work on heterophilic graphs?", "tokens": [50364, 813, 29435, 45, 82, 11, 293, 393, 291, 6063, 5623, 29435, 45, 82, 294, 257, 636, 300, 436, 486, 611, 589, 322, 20789, 5317, 388, 299, 24877, 30, 50720], "temperature": 0.0, "avg_logprob": -0.11282986342304885, "compression_ratio": 1.6234309623430963, "no_speech_prob": 0.008560547605156898}, {"id": 1454, "seek": 850868, "start": 8515.800000000001, "end": 8519.56, "text": " Yeah, so residual connection is a must. We actually show that without residual connection this", "tokens": [50720, 865, 11, 370, 27980, 4984, 307, 257, 1633, 13, 492, 767, 855, 300, 1553, 27980, 4984, 341, 50908], "temperature": 0.0, "avg_logprob": -0.11282986342304885, "compression_ratio": 1.6234309623430963, "no_speech_prob": 0.008560547605156898}, {"id": 1455, "seek": 850868, "start": 8520.2, "end": 8527.0, "text": " doesn't happen. Well, the nonlinear activation, I think it's more a complication for the analysis.", "tokens": [50940, 1177, 380, 1051, 13, 1042, 11, 264, 2107, 28263, 24433, 11, 286, 519, 309, 311, 544, 257, 1209, 8758, 337, 264, 5215, 13, 51280], "temperature": 0.0, "avg_logprob": -0.11282986342304885, "compression_ratio": 1.6234309623430963, "no_speech_prob": 0.008560547605156898}, {"id": 1456, "seek": 850868, "start": 8528.92, "end": 8535.800000000001, "text": " We don't know how to analyze this, basically with this nonlinearity you cannot regard it as a", "tokens": [51376, 492, 500, 380, 458, 577, 281, 12477, 341, 11, 1936, 365, 341, 2107, 1889, 17409, 291, 2644, 3843, 309, 382, 257, 51720], "temperature": 0.0, "avg_logprob": -0.11282986342304885, "compression_ratio": 1.6234309623430963, "no_speech_prob": 0.008560547605156898}, {"id": 1457, "seek": 853580, "start": 8535.8, "end": 8544.439999999999, "text": " diffusion equation. So, yeah, basically residual connection and then nonlinear eigenvalues.", "tokens": [50364, 25242, 5367, 13, 407, 11, 1338, 11, 1936, 27980, 4984, 293, 550, 2107, 28263, 10446, 46033, 13, 50796], "temperature": 0.0, "avg_logprob": -0.15303385257720947, "compression_ratio": 1.644736842105263, "no_speech_prob": 0.002756986068561673}, {"id": 1458, "seek": 853580, "start": 8544.439999999999, "end": 8550.279999999999, "text": " If you, and remove the nonlinearity, whether, if you don't constrain w to have non-negative,", "tokens": [50796, 759, 291, 11, 293, 4159, 264, 2107, 1889, 17409, 11, 1968, 11, 498, 291, 500, 380, 1817, 7146, 261, 281, 362, 2107, 12, 28561, 1166, 11, 51088], "temperature": 0.0, "avg_logprob": -0.15303385257720947, "compression_ratio": 1.644736842105263, "no_speech_prob": 0.002756986068561673}, {"id": 1459, "seek": 853580, "start": 8550.279999999999, "end": 8555.64, "text": " or if you don't help the architecture to learn w with negative eigenvalues, you might never be", "tokens": [51088, 420, 498, 291, 500, 380, 854, 264, 9482, 281, 1466, 261, 365, 3671, 10446, 46033, 11, 291, 1062, 1128, 312, 51356], "temperature": 0.0, "avg_logprob": -0.15303385257720947, "compression_ratio": 1.644736842105263, "no_speech_prob": 0.002756986068561673}, {"id": 1460, "seek": 853580, "start": 8555.64, "end": 8561.48, "text": " able to learn it. So we've seen this happening as well. Okay, and what is the role of symmetric", "tokens": [51356, 1075, 281, 1466, 309, 13, 407, 321, 600, 1612, 341, 2737, 382, 731, 13, 1033, 11, 293, 437, 307, 264, 3090, 295, 32330, 51648], "temperature": 0.0, "avg_logprob": -0.15303385257720947, "compression_ratio": 1.644736842105263, "no_speech_prob": 0.002756986068561673}, {"id": 1461, "seek": 856148, "start": 8561.48, "end": 8566.52, "text": " matrices, symmetric matrices w? So, symmetric comes from the assumption of gradient flow.", "tokens": [50364, 32284, 11, 32330, 32284, 261, 30, 407, 11, 32330, 1487, 490, 264, 15302, 295, 16235, 3095, 13, 50616], "temperature": 0.0, "avg_logprob": -0.17356625031889156, "compression_ratio": 1.7327188940092166, "no_speech_prob": 0.0042051211930811405}, {"id": 1462, "seek": 856148, "start": 8568.52, "end": 8573.0, "text": " So, anything that looks like a gradient flow for this kind of energy must be symmetric.", "tokens": [50716, 407, 11, 1340, 300, 1542, 411, 257, 16235, 3095, 337, 341, 733, 295, 2281, 1633, 312, 32330, 13, 50940], "temperature": 0.0, "avg_logprob": -0.17356625031889156, "compression_ratio": 1.7327188940092166, "no_speech_prob": 0.0042051211930811405}, {"id": 1463, "seek": 856148, "start": 8573.96, "end": 8578.68, "text": " I see, and what does it mean in terms of operations on your graph, in terms of message passing?", "tokens": [50988, 286, 536, 11, 293, 437, 775, 309, 914, 294, 2115, 295, 7705, 322, 428, 4295, 11, 294, 2115, 295, 3636, 8437, 30, 51224], "temperature": 0.0, "avg_logprob": -0.17356625031889156, "compression_ratio": 1.7327188940092166, "no_speech_prob": 0.0042051211930811405}, {"id": 1464, "seek": 856148, "start": 8579.32, "end": 8584.359999999999, "text": " So, this is not related to message passing because w is channel mixing matrix attacks on the fissures.", "tokens": [51256, 407, 11, 341, 307, 406, 4077, 281, 3636, 8437, 570, 261, 307, 2269, 11983, 8141, 8122, 322, 264, 283, 891, 1303, 13, 51508], "temperature": 0.0, "avg_logprob": -0.17356625031889156, "compression_ratio": 1.7327188940092166, "no_speech_prob": 0.0042051211930811405}, {"id": 1465, "seek": 858436, "start": 8584.44, "end": 8597.560000000001, "text": " I was, I was wondering, what is the relationship? So, for example, you have this gradient flow", "tokens": [50368, 286, 390, 11, 286, 390, 6359, 11, 437, 307, 264, 2480, 30, 407, 11, 337, 1365, 11, 291, 362, 341, 16235, 3095, 51024], "temperature": 0.0, "avg_logprob": -0.1506110600062779, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.0024864161387085915}, {"id": 1466, "seek": 858436, "start": 8597.560000000001, "end": 8602.44, "text": " and say you want to learn some energy with respect to which you flow. What's the relationship between", "tokens": [51024, 293, 584, 291, 528, 281, 1466, 512, 2281, 365, 3104, 281, 597, 291, 3095, 13, 708, 311, 264, 2480, 1296, 51268], "temperature": 0.0, "avg_logprob": -0.1506110600062779, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.0024864161387085915}, {"id": 1467, "seek": 858436, "start": 8602.44, "end": 8608.12, "text": " learning the energy and learning a metric to which with you are computing the gradient? Because if", "tokens": [51268, 2539, 264, 2281, 293, 2539, 257, 20678, 281, 597, 365, 291, 366, 15866, 264, 16235, 30, 1436, 498, 51552], "temperature": 0.0, "avg_logprob": -0.1506110600062779, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.0024864161387085915}, {"id": 1468, "seek": 858436, "start": 8608.12, "end": 8613.08, "text": " you change the metric, then the way in which you flow is also different, right? Is other equivalent", "tokens": [51552, 291, 1319, 264, 20678, 11, 550, 264, 636, 294, 597, 291, 3095, 307, 611, 819, 11, 558, 30, 1119, 661, 10344, 51800], "temperature": 0.0, "avg_logprob": -0.1506110600062779, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.0024864161387085915}, {"id": 1469, "seek": 861308, "start": 8613.08, "end": 8619.0, "text": " in the learning a metric equivalent to learning an energy? Yeah, well, you can think of it", "tokens": [50364, 294, 264, 2539, 257, 20678, 10344, 281, 2539, 364, 2281, 30, 865, 11, 731, 11, 291, 393, 519, 295, 309, 50660], "temperature": 0.0, "avg_logprob": -0.11906470874748608, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.0025622667744755745}, {"id": 1470, "seek": 861308, "start": 8619.0, "end": 8624.76, "text": " indeed in this way, right? So, what is the interpretation, physical interpretation of", "tokens": [50660, 6451, 294, 341, 636, 11, 558, 30, 407, 11, 437, 307, 264, 14174, 11, 4001, 14174, 295, 50948], "temperature": 0.0, "avg_logprob": -0.11906470874748608, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.0025622667744755745}, {"id": 1471, "seek": 861308, "start": 8626.2, "end": 8632.36, "text": " of this matrix w? So, if you look at the way that Dirichlet energy looks, right, it looks like,", "tokens": [51020, 295, 341, 8141, 261, 30, 407, 11, 498, 291, 574, 412, 264, 636, 300, 34422, 480, 2631, 2281, 1542, 11, 558, 11, 309, 1542, 411, 11, 51328], "temperature": 0.0, "avg_logprob": -0.11906470874748608, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.0025622667744755745}, {"id": 1472, "seek": 861308, "start": 8632.36, "end": 8638.52, "text": " right? So, it's something like this. So, do we use, yeah, so let's say continuous version of Dirichlet", "tokens": [51328, 558, 30, 407, 11, 309, 311, 746, 411, 341, 13, 407, 11, 360, 321, 764, 11, 1338, 11, 370, 718, 311, 584, 10957, 3037, 295, 34422, 480, 2631, 51636], "temperature": 0.0, "avg_logprob": -0.11906470874748608, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.0025622667744755745}, {"id": 1473, "seek": 863852, "start": 8638.52, "end": 8643.640000000001, "text": " energy. So, it will be the norm of the gradient of x at some coordinate u, let's call it. So,", "tokens": [50364, 2281, 13, 407, 11, 309, 486, 312, 264, 2026, 295, 264, 16235, 295, 2031, 412, 512, 15670, 344, 11, 718, 311, 818, 309, 13, 407, 11, 50620], "temperature": 0.0, "avg_logprob": -0.12550055980682373, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.021864453330636024}, {"id": 1474, "seek": 863852, "start": 8643.640000000001, "end": 8648.2, "text": " u would be the index of the null, right, the continuous version, right? And this is, this is", "tokens": [50620, 344, 576, 312, 264, 8186, 295, 264, 18184, 11, 558, 11, 264, 10957, 3037, 11, 558, 30, 400, 341, 307, 11, 341, 307, 50848], "temperature": 0.0, "avg_logprob": -0.12550055980682373, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.021864453330636024}, {"id": 1475, "seek": 863852, "start": 8648.2, "end": 8655.0, "text": " the Dirichlet energy and let's write it on some domain omega, right? So, that would be our continuous", "tokens": [50848, 264, 34422, 480, 2631, 2281, 293, 718, 311, 2464, 309, 322, 512, 9274, 10498, 11, 558, 30, 407, 11, 300, 576, 312, 527, 10957, 51188], "temperature": 0.0, "avg_logprob": -0.12550055980682373, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.021864453330636024}, {"id": 1476, "seek": 863852, "start": 8655.0, "end": 8662.92, "text": " version of Dirichlet energy. Now, what is written here when omega in general is a manifold,", "tokens": [51188, 3037, 295, 34422, 480, 2631, 2281, 13, 823, 11, 437, 307, 3720, 510, 562, 10498, 294, 2674, 307, 257, 47138, 11, 51584], "temperature": 0.0, "avg_logprob": -0.12550055980682373, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.021864453330636024}, {"id": 1477, "seek": 866292, "start": 8662.92, "end": 8669.48, "text": " a remaining manifold. So, what is written here is the remaining metric of this kind, right?", "tokens": [50364, 257, 8877, 47138, 13, 407, 11, 437, 307, 3720, 510, 307, 264, 8877, 20678, 295, 341, 733, 11, 558, 30, 50692], "temperature": 0.0, "avg_logprob": -0.11027630981133908, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.004866644274443388}, {"id": 1478, "seek": 866292, "start": 8671.64, "end": 8677.8, "text": " I hope you can see it. So, basically it's inner product defined at the position u, right? And", "tokens": [50800, 286, 1454, 291, 393, 536, 309, 13, 407, 11, 1936, 309, 311, 7284, 1674, 7642, 412, 264, 2535, 344, 11, 558, 30, 400, 51108], "temperature": 0.0, "avg_logprob": -0.11027630981133908, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.004866644274443388}, {"id": 1479, "seek": 866292, "start": 8677.8, "end": 8681.48, "text": " the way that you can write it, so you can write it using remaining metric tensor, which is exactly", "tokens": [51108, 264, 636, 300, 291, 393, 2464, 309, 11, 370, 291, 393, 2464, 309, 1228, 8877, 20678, 40863, 11, 597, 307, 2293, 51292], "temperature": 0.0, "avg_logprob": -0.11027630981133908, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.004866644274443388}, {"id": 1480, "seek": 866292, "start": 8681.48, "end": 8689.48, "text": " the w, right? So, this is something that scales the coordinates of x, doesn't need to be fixed,", "tokens": [51292, 264, 261, 11, 558, 30, 407, 11, 341, 307, 746, 300, 17408, 264, 21056, 295, 2031, 11, 1177, 380, 643, 281, 312, 6806, 11, 51692], "temperature": 0.0, "avg_logprob": -0.11027630981133908, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.004866644274443388}, {"id": 1481, "seek": 868948, "start": 8689.48, "end": 8695.32, "text": " by the way. This w in general can be position dependent, right, on the remaining manifold.", "tokens": [50364, 538, 264, 636, 13, 639, 261, 294, 2674, 393, 312, 2535, 12334, 11, 558, 11, 322, 264, 8877, 47138, 13, 50656], "temperature": 0.0, "avg_logprob": -0.11418282448708474, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.009605958126485348}, {"id": 1482, "seek": 868948, "start": 8695.32, "end": 8701.64, "text": " So, a more general construction would allow w to depend on the position, maybe not explicitly,", "tokens": [50656, 407, 11, 257, 544, 2674, 6435, 576, 2089, 261, 281, 5672, 322, 264, 2535, 11, 1310, 406, 20803, 11, 50972], "temperature": 0.0, "avg_logprob": -0.11418282448708474, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.009605958126485348}, {"id": 1483, "seek": 868948, "start": 8701.64, "end": 8707.4, "text": " because that would be a huge number of parameters that scales with n, maybe it will be done some,", "tokens": [50972, 570, 300, 576, 312, 257, 2603, 1230, 295, 9834, 300, 17408, 365, 297, 11, 1310, 309, 486, 312, 1096, 512, 11, 51260], "temperature": 0.0, "avg_logprob": -0.11418282448708474, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.009605958126485348}, {"id": 1484, "seek": 868948, "start": 8707.4, "end": 8712.199999999999, "text": " through some form of attention. So, w will, or maybe a positional encoding, right? So,", "tokens": [51260, 807, 512, 1254, 295, 3202, 13, 407, 11, 261, 486, 11, 420, 1310, 257, 2535, 304, 43430, 11, 558, 30, 407, 11, 51500], "temperature": 0.0, "avg_logprob": -0.11418282448708474, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.009605958126485348}, {"id": 1485, "seek": 868948, "start": 8712.199999999999, "end": 8717.56, "text": " w will be a function of positional encoding of the nodes of the graph, right?", "tokens": [51500, 261, 486, 312, 257, 2445, 295, 2535, 304, 43430, 295, 264, 13891, 295, 264, 4295, 11, 558, 30, 51768], "temperature": 0.0, "avg_logprob": -0.11418282448708474, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.009605958126485348}, {"id": 1486, "seek": 871948, "start": 8720.119999999999, "end": 8725.16, "text": " So, there are interesting analogies and potential directions of extending this, using", "tokens": [50396, 407, 11, 456, 366, 1880, 16660, 530, 293, 3995, 11095, 295, 24360, 341, 11, 1228, 50648], "temperature": 0.0, "avg_logprob": -0.19739135106404623, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.000847730552777648}, {"id": 1487, "seek": 871948, "start": 8726.92, "end": 8732.52, "text": " basically this is a harmonic energy of an embedding of some manifold, right? Thanks.", "tokens": [50736, 1936, 341, 307, 257, 32270, 2281, 295, 364, 12240, 3584, 295, 512, 47138, 11, 558, 30, 2561, 13, 51016], "temperature": 0.0, "avg_logprob": -0.19739135106404623, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.000847730552777648}, {"id": 1488, "seek": 871948, "start": 8737.8, "end": 8741.8, "text": " Yeah, sorry, this will be probably a little bit like far-fetched question,", "tokens": [51280, 865, 11, 2597, 11, 341, 486, 312, 1391, 257, 707, 857, 411, 1400, 12, 69, 7858, 292, 1168, 11, 51480], "temperature": 0.0, "avg_logprob": -0.19739135106404623, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.000847730552777648}, {"id": 1489, "seek": 871948, "start": 8741.8, "end": 8747.96, "text": " but like in general, like those equations seem to be, seem to be like kind of similar to what you", "tokens": [51480, 457, 411, 294, 2674, 11, 411, 729, 11787, 1643, 281, 312, 11, 1643, 281, 312, 411, 733, 295, 2531, 281, 437, 291, 51788], "temperature": 0.0, "avg_logprob": -0.19739135106404623, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.000847730552777648}, {"id": 1490, "seek": 874796, "start": 8747.96, "end": 8755.4, "text": " often get when you analyze like the signal propagation or the type of initializations", "tokens": [50364, 2049, 483, 562, 291, 12477, 411, 264, 6358, 38377, 420, 264, 2010, 295, 5883, 14455, 50736], "temperature": 0.0, "avg_logprob": -0.24895552983359684, "compression_ratio": 1.5786516853932584, "no_speech_prob": 0.014352994970977306}, {"id": 1491, "seek": 874796, "start": 8755.4, "end": 8761.88, "text": " in the neural networks, or like the dynamical isometry property. So, for example, the requirement for", "tokens": [50736, 294, 264, 18161, 9590, 11, 420, 411, 264, 5999, 804, 307, 34730, 4707, 13, 407, 11, 337, 1365, 11, 264, 11695, 337, 51060], "temperature": 0.0, "avg_logprob": -0.24895552983359684, "compression_ratio": 1.5786516853932584, "no_speech_prob": 0.014352994970977306}, {"id": 1492, "seek": 874796, "start": 8761.88, "end": 8768.759999999998, "text": " residual connections also appears in there. And like, are you aware like whether there's like", "tokens": [51060, 27980, 9271, 611, 7038, 294, 456, 13, 400, 411, 11, 366, 291, 3650, 411, 1968, 456, 311, 411, 51404], "temperature": 0.0, "avg_logprob": -0.24895552983359684, "compression_ratio": 1.5786516853932584, "no_speech_prob": 0.014352994970977306}, {"id": 1493, "seek": 876876, "start": 8768.76, "end": 8775.48, "text": " any connections between, between, between this work and, yeah.", "tokens": [50364, 604, 9271, 1296, 11, 1296, 11, 1296, 341, 589, 293, 11, 1338, 13, 50700], "temperature": 0.0, "avg_logprob": -0.2284052196301912, "compression_ratio": 1.6261682242990654, "no_speech_prob": 0.005178887862712145}, {"id": 1494, "seek": 876876, "start": 8775.48, "end": 8781.32, "text": " Potentially. So, the closest analogy is neural ODE's, right? Well, these are neural, we like to", "tokens": [50700, 9145, 3137, 13, 407, 11, 264, 13699, 21663, 307, 18161, 422, 22296, 311, 11, 558, 30, 1042, 11, 613, 366, 18161, 11, 321, 411, 281, 50992], "temperature": 0.0, "avg_logprob": -0.2284052196301912, "compression_ratio": 1.6261682242990654, "no_speech_prob": 0.005178887862712145}, {"id": 1495, "seek": 876876, "start": 8781.32, "end": 8788.2, "text": " call them neural PD's, but they're coupled ODE's, right? So, neural ODE's, each row of these magics", "tokens": [50992, 818, 552, 18161, 10464, 311, 11, 457, 436, 434, 29482, 422, 22296, 311, 11, 558, 30, 407, 11, 18161, 422, 22296, 311, 11, 1184, 5386, 295, 613, 2258, 1167, 51336], "temperature": 0.0, "avg_logprob": -0.2284052196301912, "compression_ratio": 1.6261682242990654, "no_speech_prob": 0.005178887862712145}, {"id": 1496, "seek": 876876, "start": 8788.2, "end": 8794.04, "text": " will be separate, independent. Here, we also have the extra complexities coupling, right?", "tokens": [51336, 486, 312, 4994, 11, 6695, 13, 1692, 11, 321, 611, 362, 264, 2857, 48705, 37447, 11, 558, 30, 51628], "temperature": 0.0, "avg_logprob": -0.2284052196301912, "compression_ratio": 1.6261682242990654, "no_speech_prob": 0.005178887862712145}, {"id": 1497, "seek": 879404, "start": 8794.52, "end": 8801.720000000001, "text": " So, like, would you say like this, there's something universal in all those?", "tokens": [50388, 407, 11, 411, 11, 576, 291, 584, 411, 341, 11, 456, 311, 746, 11455, 294, 439, 729, 30, 50748], "temperature": 0.0, "avg_logprob": -0.3302655849816664, "compression_ratio": 1.3858267716535433, "no_speech_prob": 0.0045096962712705135}, {"id": 1498, "seek": 879404, "start": 8803.720000000001, "end": 8805.480000000001, "text": " I'm not sure what do I mean by universal.", "tokens": [50848, 286, 478, 406, 988, 437, 360, 286, 914, 538, 11455, 13, 50936], "temperature": 0.0, "avg_logprob": -0.3302655849816664, "compression_ratio": 1.3858267716535433, "no_speech_prob": 0.0045096962712705135}, {"id": 1499, "seek": 879404, "start": 8810.52, "end": 8814.68, "text": " Yeah, I'm not sure either, but like, it's basically like,", "tokens": [51188, 865, 11, 286, 478, 406, 988, 2139, 11, 457, 411, 11, 309, 311, 1936, 411, 11, 51396], "temperature": 0.0, "avg_logprob": -0.3302655849816664, "compression_ratio": 1.3858267716535433, "no_speech_prob": 0.0045096962712705135}, {"id": 1500, "seek": 881468, "start": 8814.92, "end": 8825.32, "text": " like, for example, like this residual rule appears very often in different types of.", "tokens": [50376, 411, 11, 337, 1365, 11, 411, 341, 27980, 4978, 7038, 588, 2049, 294, 819, 3467, 295, 13, 50896], "temperature": 0.0, "avg_logprob": -0.16939803657181765, "compression_ratio": 1.6753731343283582, "no_speech_prob": 0.005708550568670034}, {"id": 1501, "seek": 881468, "start": 8825.32, "end": 8830.92, "text": " So, residual rules rule here comes just from discretization. So, that's how you, you discretized", "tokens": [50896, 407, 11, 27980, 4474, 4978, 510, 1487, 445, 490, 25656, 2144, 13, 407, 11, 300, 311, 577, 291, 11, 291, 25656, 1602, 51176], "temperature": 0.0, "avg_logprob": -0.16939803657181765, "compression_ratio": 1.6753731343283582, "no_speech_prob": 0.005708550568670034}, {"id": 1502, "seek": 881468, "start": 8830.92, "end": 8835.08, "text": " the temporal derivative. You could discretize it differently. So, you can use a backward scheme,", "tokens": [51176, 264, 30881, 13760, 13, 509, 727, 25656, 1125, 309, 7614, 13, 407, 11, 291, 393, 764, 257, 23897, 12232, 11, 51384], "temperature": 0.0, "avg_logprob": -0.16939803657181765, "compression_ratio": 1.6753731343283582, "no_speech_prob": 0.005708550568670034}, {"id": 1503, "seek": 881468, "start": 8835.08, "end": 8839.56, "text": " right? And then it would be implicit. So, you will need to solve a linear system to,", "tokens": [51384, 558, 30, 400, 550, 309, 576, 312, 26947, 13, 407, 11, 291, 486, 643, 281, 5039, 257, 8213, 1185, 281, 11, 51608], "temperature": 0.0, "avg_logprob": -0.16939803657181765, "compression_ratio": 1.6753731343283582, "no_speech_prob": 0.005708550568670034}, {"id": 1504, "seek": 881468, "start": 8839.56, "end": 8843.24, "text": " to get your next iteration. This actually has been done with diffusion equations. So,", "tokens": [51608, 281, 483, 428, 958, 24784, 13, 639, 767, 575, 668, 1096, 365, 25242, 11787, 13, 407, 11, 51792], "temperature": 0.0, "avg_logprob": -0.16939803657181765, "compression_ratio": 1.6753731343283582, "no_speech_prob": 0.005708550568670034}, {"id": 1505, "seek": 884324, "start": 8844.199999999999, "end": 8848.119999999999, "text": " there are advantages to it because these kind of discretizations are what's called", "tokens": [50412, 456, 366, 14906, 281, 309, 570, 613, 733, 295, 25656, 14455, 366, 437, 311, 1219, 50608], "temperature": 0.0, "avg_logprob": -0.15924441257369853, "compression_ratio": 1.7132075471698114, "no_speech_prob": 0.0013822547625750303}, {"id": 1506, "seek": 884324, "start": 8848.119999999999, "end": 8854.039999999999, "text": " unconditional stable. But in terms of, well, universality may be not the right term, but", "tokens": [50608, 47916, 8351, 13, 583, 294, 2115, 295, 11, 731, 11, 5950, 1860, 815, 312, 406, 264, 558, 1433, 11, 457, 50904], "temperature": 0.0, "avg_logprob": -0.15924441257369853, "compression_ratio": 1.7132075471698114, "no_speech_prob": 0.0013822547625750303}, {"id": 1507, "seek": 884324, "start": 8854.039999999999, "end": 8859.16, "text": " basically what you have here in practice is a kind of controlled differential equation,", "tokens": [50904, 1936, 437, 291, 362, 510, 294, 3124, 307, 257, 733, 295, 10164, 15756, 5367, 11, 51160], "temperature": 0.0, "avg_logprob": -0.15924441257369853, "compression_ratio": 1.7132075471698114, "no_speech_prob": 0.0013822547625750303}, {"id": 1508, "seek": 884324, "start": 8859.16, "end": 8864.28, "text": " right? So, the control is through the privateers W. So, they're time independent, but in principle,", "tokens": [51160, 558, 30, 407, 11, 264, 1969, 307, 807, 264, 4551, 433, 343, 13, 407, 11, 436, 434, 565, 6695, 11, 457, 294, 8665, 11, 51416], "temperature": 0.0, "avg_logprob": -0.15924441257369853, "compression_ratio": 1.7132075471698114, "no_speech_prob": 0.0013822547625750303}, {"id": 1509, "seek": 884324, "start": 8864.28, "end": 8869.4, "text": " you can think of time dependent trajectory. So, you have a controlled PDE that you discretize,", "tokens": [51416, 291, 393, 519, 295, 565, 12334, 21512, 13, 407, 11, 291, 362, 257, 10164, 10464, 36, 300, 291, 25656, 1125, 11, 51672], "temperature": 0.0, "avg_logprob": -0.15924441257369853, "compression_ratio": 1.7132075471698114, "no_speech_prob": 0.0013822547625750303}, {"id": 1510, "seek": 886940, "start": 8869.4, "end": 8873.88, "text": " and then expressive power becomes a question of, can I reach a certain state of the system,", "tokens": [50364, 293, 550, 40189, 1347, 3643, 257, 1168, 295, 11, 393, 286, 2524, 257, 1629, 1785, 295, 264, 1185, 11, 50588], "temperature": 0.0, "avg_logprob": -0.09893947463851792, "compression_ratio": 1.7218045112781954, "no_speech_prob": 0.0015814866637811065}, {"id": 1511, "seek": 886940, "start": 8873.88, "end": 8877.16, "text": " for example, in finite time by choosing the right trajectory? Or how far can I be", "tokens": [50588, 337, 1365, 11, 294, 19362, 565, 538, 10875, 264, 558, 21512, 30, 1610, 577, 1400, 393, 286, 312, 50752], "temperature": 0.0, "avg_logprob": -0.09893947463851792, "compression_ratio": 1.7218045112781954, "no_speech_prob": 0.0015814866637811065}, {"id": 1512, "seek": 886940, "start": 8877.72, "end": 8883.48, "text": " from, from that state, right? So, universal approximation means that in finite time I can", "tokens": [50780, 490, 11, 490, 300, 1785, 11, 558, 30, 407, 11, 11455, 28023, 1355, 300, 294, 19362, 565, 286, 393, 51068], "temperature": 0.0, "avg_logprob": -0.09893947463851792, "compression_ratio": 1.7218045112781954, "no_speech_prob": 0.0015814866637811065}, {"id": 1513, "seek": 886940, "start": 8883.48, "end": 8889.56, "text": " reach, I can be epsilon close to any state that they want. Generalization, for example, can be", "tokens": [51068, 2524, 11, 286, 393, 312, 17889, 1998, 281, 604, 1785, 300, 436, 528, 13, 6996, 2144, 11, 337, 1365, 11, 393, 312, 51372], "temperature": 0.0, "avg_logprob": -0.09893947463851792, "compression_ratio": 1.7218045112781954, "no_speech_prob": 0.0015814866637811065}, {"id": 1514, "seek": 886940, "start": 8889.56, "end": 8894.44, "text": " probably formulated as some kind of perturbation, right? So, I change my initial conditions. I want", "tokens": [51372, 1391, 48936, 382, 512, 733, 295, 40468, 399, 11, 558, 30, 407, 11, 286, 1319, 452, 5883, 4487, 13, 286, 528, 51616], "temperature": 0.0, "avg_logprob": -0.09893947463851792, "compression_ratio": 1.7218045112781954, "no_speech_prob": 0.0015814866637811065}, {"id": 1515, "seek": 889444, "start": 8894.44, "end": 8900.2, "text": " to see what happens to the system. And there are actually, there are some results, theoretical", "tokens": [50364, 281, 536, 437, 2314, 281, 264, 1185, 13, 400, 456, 366, 767, 11, 456, 366, 512, 3542, 11, 20864, 50652], "temperature": 0.0, "avg_logprob": -0.12406387957897815, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.004588058218359947}, {"id": 1516, "seek": 889444, "start": 8900.2, "end": 8904.84, "text": " results that show that architectural choices, like for example, having symmetric matrices might be", "tokens": [50652, 3542, 300, 855, 300, 26621, 7994, 11, 411, 337, 1365, 11, 1419, 32330, 32284, 1062, 312, 50884], "temperature": 0.0, "avg_logprob": -0.12406387957897815, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.004588058218359947}, {"id": 1517, "seek": 889444, "start": 8904.84, "end": 8911.640000000001, "text": " crucial for these properties. So, I think there is a lot more to explore there. Okay, thanks.", "tokens": [50884, 11462, 337, 613, 7221, 13, 407, 11, 286, 519, 456, 307, 257, 688, 544, 281, 6839, 456, 13, 1033, 11, 3231, 13, 51224], "temperature": 0.0, "avg_logprob": -0.12406387957897815, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.004588058218359947}, {"id": 1518, "seek": 889444, "start": 8914.6, "end": 8919.08, "text": " Okay. So, more questions? Yeah. So, let's, let's move on with this stuff. So,", "tokens": [51372, 1033, 13, 407, 11, 544, 1651, 30, 865, 13, 407, 11, 718, 311, 11, 718, 311, 1286, 322, 365, 341, 1507, 13, 407, 11, 51596], "temperature": 0.0, "avg_logprob": -0.12406387957897815, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.004588058218359947}, {"id": 1519, "seek": 891908, "start": 8920.039999999999, "end": 8925.48, "text": " obviously, right, so here, the conclusion was that we have no over smoothing, but we can also", "tokens": [50412, 2745, 11, 558, 11, 370, 510, 11, 264, 10063, 390, 300, 321, 362, 572, 670, 899, 6259, 571, 11, 457, 321, 393, 611, 50684], "temperature": 0.0, "avg_logprob": -0.20718798144110318, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.0038584270514547825}, {"id": 1520, "seek": 891908, "start": 8925.48, "end": 8931.8, "text": " consider more interesting equations. So, so far we consider the very simple isotropic homogeneous", "tokens": [50684, 1949, 544, 1880, 11787, 13, 407, 11, 370, 1400, 321, 1949, 264, 588, 2199, 38018, 39173, 42632, 51000], "temperature": 0.0, "avg_logprob": -0.20718798144110318, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.0038584270514547825}, {"id": 1521, "seek": 891908, "start": 8931.8, "end": 8936.6, "text": " diffusion equation. We can also consider nonlinear versions of the diffusion equation. And this one,", "tokens": [51000, 25242, 5367, 13, 492, 393, 611, 1949, 2107, 28263, 9606, 295, 264, 25242, 5367, 13, 400, 341, 472, 11, 51240], "temperature": 0.0, "avg_logprob": -0.20718798144110318, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.0038584270514547825}, {"id": 1522, "seek": 891908, "start": 8936.6, "end": 8942.76, "text": " in particular, comes from the domain of image processing, where imagine that you start with an", "tokens": [51240, 294, 1729, 11, 1487, 490, 264, 9274, 295, 3256, 9007, 11, 689, 3811, 300, 291, 722, 365, 364, 51548], "temperature": 0.0, "avg_logprob": -0.20718798144110318, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.0038584270514547825}, {"id": 1523, "seek": 894276, "start": 8943.64, "end": 8950.76, "text": " image like this, right? So, the portrait of Sir Isaac Newton that is noisy. So, if you run a diffusion", "tokens": [50408, 3256, 411, 341, 11, 558, 30, 407, 11, 264, 17126, 295, 6144, 22505, 19541, 300, 307, 24518, 13, 407, 11, 498, 291, 1190, 257, 25242, 50764], "temperature": 0.0, "avg_logprob": -0.10676654974619547, "compression_ratio": 1.7589928057553956, "no_speech_prob": 0.0027575595304369926}, {"id": 1524, "seek": 894276, "start": 8950.76, "end": 8955.24, "text": " equation on an image, it actually has a closed form solution. So, it's convolution with the Gaussian", "tokens": [50764, 5367, 322, 364, 3256, 11, 309, 767, 575, 257, 5395, 1254, 3827, 13, 407, 11, 309, 311, 45216, 365, 264, 39148, 50988], "temperature": 0.0, "avg_logprob": -0.10676654974619547, "compression_ratio": 1.7589928057553956, "no_speech_prob": 0.0027575595304369926}, {"id": 1525, "seek": 894276, "start": 8955.24, "end": 8960.2, "text": " kernel, where the variance of the Gaussian is proportional to the time of the diffusion, right?", "tokens": [50988, 28256, 11, 689, 264, 21977, 295, 264, 39148, 307, 24969, 281, 264, 565, 295, 264, 25242, 11, 558, 30, 51236], "temperature": 0.0, "avg_logprob": -0.10676654974619547, "compression_ratio": 1.7589928057553956, "no_speech_prob": 0.0027575595304369926}, {"id": 1526, "seek": 894276, "start": 8960.2, "end": 8965.32, "text": " And in the limit, you will have just everything flat, right? So, you average all the pixels in", "tokens": [51236, 400, 294, 264, 4948, 11, 291, 486, 362, 445, 1203, 4962, 11, 558, 30, 407, 11, 291, 4274, 439, 264, 18668, 294, 51492], "temperature": 0.0, "avg_logprob": -0.10676654974619547, "compression_ratio": 1.7589928057553956, "no_speech_prob": 0.0027575595304369926}, {"id": 1527, "seek": 894276, "start": 8965.32, "end": 8971.4, "text": " the image. You see that you don't want to have results like this, because it might average out", "tokens": [51492, 264, 3256, 13, 509, 536, 300, 291, 500, 380, 528, 281, 362, 3542, 411, 341, 11, 570, 309, 1062, 4274, 484, 51796], "temperature": 0.0, "avg_logprob": -0.10676654974619547, "compression_ratio": 1.7589928057553956, "no_speech_prob": 0.0027575595304369926}, {"id": 1528, "seek": 897140, "start": 8971.4, "end": 8976.199999999999, "text": " the noise, but it also destroys the discontinuities in the image that, for visual perception, are very", "tokens": [50364, 264, 5658, 11, 457, 309, 611, 36714, 264, 31420, 84, 1088, 294, 264, 3256, 300, 11, 337, 5056, 12860, 11, 366, 588, 50604], "temperature": 0.0, "avg_logprob": -0.12195289134979248, "compression_ratio": 1.5655737704918034, "no_speech_prob": 0.0009065034682862461}, {"id": 1529, "seek": 897140, "start": 8976.199999999999, "end": 8985.64, "text": " important. So, the idea of, that was originally by Peron and Malik in 1990 is to have a nonlinear", "tokens": [50604, 1021, 13, 407, 11, 264, 1558, 295, 11, 300, 390, 7993, 538, 3026, 266, 293, 5746, 1035, 294, 13384, 307, 281, 362, 257, 2107, 28263, 51076], "temperature": 0.0, "avg_logprob": -0.12195289134979248, "compression_ratio": 1.5655737704918034, "no_speech_prob": 0.0009065034682862461}, {"id": 1530, "seek": 897140, "start": 8985.64, "end": 8991.56, "text": " diffusion equation that is controlled by the gradient of the image, right? So, basically,", "tokens": [51076, 25242, 5367, 300, 307, 10164, 538, 264, 16235, 295, 264, 3256, 11, 558, 30, 407, 11, 1936, 11, 51372], "temperature": 0.0, "avg_logprob": -0.12195289134979248, "compression_ratio": 1.5655737704918034, "no_speech_prob": 0.0009065034682862461}, {"id": 1531, "seek": 897140, "start": 8992.119999999999, "end": 8996.84, "text": " if you're in a smooth region in the image, like here, so you have standard Gaussian kernel,", "tokens": [51400, 498, 291, 434, 294, 257, 5508, 4458, 294, 264, 3256, 11, 411, 510, 11, 370, 291, 362, 3832, 39148, 28256, 11, 51636], "temperature": 0.0, "avg_logprob": -0.12195289134979248, "compression_ratio": 1.5655737704918034, "no_speech_prob": 0.0009065034682862461}, {"id": 1532, "seek": 899684, "start": 8996.84, "end": 9001.880000000001, "text": " but the moment you reach a discontinuity, you slow down the diffusion. So, the effect it has,", "tokens": [50364, 457, 264, 1623, 291, 2524, 257, 31420, 21757, 11, 291, 2964, 760, 264, 25242, 13, 407, 11, 264, 1802, 309, 575, 11, 50616], "temperature": 0.0, "avg_logprob": -0.1101923373437697, "compression_ratio": 1.7740740740740741, "no_speech_prob": 0.003630490507930517}, {"id": 1533, "seek": 899684, "start": 9001.880000000001, "end": 9007.56, "text": " you don't average pixels of different intensity. So, here, I'm not averaging dark and white, right?", "tokens": [50616, 291, 500, 380, 4274, 18668, 295, 819, 13749, 13, 407, 11, 510, 11, 286, 478, 406, 47308, 2877, 293, 2418, 11, 558, 30, 50900], "temperature": 0.0, "avg_logprob": -0.1101923373437697, "compression_ratio": 1.7740740740740741, "no_speech_prob": 0.003630490507930517}, {"id": 1534, "seek": 899684, "start": 9008.76, "end": 9013.880000000001, "text": " So, the kernel will look like this. It will look one-sided. And it had a lot of different versions,", "tokens": [50960, 407, 11, 264, 28256, 486, 574, 411, 341, 13, 467, 486, 574, 472, 12, 30941, 13, 400, 309, 632, 257, 688, 295, 819, 9606, 11, 51216], "temperature": 0.0, "avg_logprob": -0.1101923373437697, "compression_ratio": 1.7740740740740741, "no_speech_prob": 0.003630490507930517}, {"id": 1535, "seek": 899684, "start": 9013.880000000001, "end": 9019.64, "text": " bilateral filters, non-local means filters and so on, but the idea is always the same. So,", "tokens": [51216, 38772, 15995, 11, 2107, 12, 5842, 304, 1355, 15995, 293, 370, 322, 11, 457, 264, 1558, 307, 1009, 264, 912, 13, 407, 11, 51504], "temperature": 0.0, "avg_logprob": -0.1101923373437697, "compression_ratio": 1.7740740740740741, "no_speech_prob": 0.003630490507930517}, {"id": 1536, "seek": 899684, "start": 9019.64, "end": 9026.52, "text": " here, basically, the diffusion speed, right, is inversely proportional to the edge indicators,", "tokens": [51504, 510, 11, 1936, 11, 264, 25242, 3073, 11, 558, 11, 307, 21378, 736, 24969, 281, 264, 4691, 22176, 11, 51848], "temperature": 0.0, "avg_logprob": -0.1101923373437697, "compression_ratio": 1.7740740740740741, "no_speech_prob": 0.003630490507930517}, {"id": 1537, "seek": 902652, "start": 9026.52, "end": 9030.76, "text": " to the norm of the gradient. And the result that it produces is like this. So, this nonlinear", "tokens": [50364, 281, 264, 2026, 295, 264, 16235, 13, 400, 264, 1874, 300, 309, 14725, 307, 411, 341, 13, 407, 11, 341, 2107, 28263, 50576], "temperature": 0.0, "avg_logprob": -0.08777246636859441, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.0009286275017075241}, {"id": 1538, "seek": 902652, "start": 9030.76, "end": 9037.0, "text": " diffusion equation knows where to stop locally, and therefore, it averages within smooth regions,", "tokens": [50576, 25242, 5367, 3255, 689, 281, 1590, 16143, 11, 293, 4412, 11, 309, 42257, 1951, 5508, 10682, 11, 50888], "temperature": 0.0, "avg_logprob": -0.08777246636859441, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.0009286275017075241}, {"id": 1539, "seek": 902652, "start": 9037.0, "end": 9041.560000000001, "text": " and it doesn't average across regions. Now, we can do the same thing on a graph, obviously. So,", "tokens": [50888, 293, 309, 1177, 380, 4274, 2108, 10682, 13, 823, 11, 321, 393, 360, 264, 912, 551, 322, 257, 4295, 11, 2745, 13, 407, 11, 51116], "temperature": 0.0, "avg_logprob": -0.08777246636859441, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.0009286275017075241}, {"id": 1540, "seek": 902652, "start": 9041.560000000001, "end": 9046.6, "text": " the analogy would be this, right? So, here, we have a gradient, right? This is the divergence,", "tokens": [51116, 264, 21663, 576, 312, 341, 11, 558, 30, 407, 11, 510, 11, 321, 362, 257, 16235, 11, 558, 30, 639, 307, 264, 47387, 11, 51368], "temperature": 0.0, "avg_logprob": -0.08777246636859441, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.0009286275017075241}, {"id": 1541, "seek": 902652, "start": 9046.6, "end": 9051.720000000001, "text": " and that's some parametric function that looks suspiciously like a tension, and that's the", "tokens": [51368, 293, 300, 311, 512, 6220, 17475, 2445, 300, 1542, 17931, 356, 411, 257, 8980, 11, 293, 300, 311, 264, 51624], "temperature": 0.0, "avg_logprob": -0.08777246636859441, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.0009286275017075241}, {"id": 1542, "seek": 905172, "start": 9051.8, "end": 9056.84, "text": " diffusivity, right? So, it's the local strength of the diffusion. And, in fact, if we discretize it", "tokens": [50368, 7593, 301, 4253, 11, 558, 30, 407, 11, 309, 311, 264, 2654, 3800, 295, 264, 25242, 13, 400, 11, 294, 1186, 11, 498, 321, 25656, 1125, 309, 50620], "temperature": 0.0, "avg_logprob": -0.09869802612619302, "compression_ratio": 1.641350210970464, "no_speech_prob": 0.0021439366973936558}, {"id": 1543, "seek": 905172, "start": 9056.84, "end": 9064.199999999999, "text": " again with explicit forward Euler scheme, then a particular version of this equation corresponds", "tokens": [50620, 797, 365, 13691, 2128, 462, 26318, 12232, 11, 550, 257, 1729, 3037, 295, 341, 5367, 23249, 50988], "temperature": 0.0, "avg_logprob": -0.09869802612619302, "compression_ratio": 1.641350210970464, "no_speech_prob": 0.0021439366973936558}, {"id": 1544, "seek": 905172, "start": 9064.199999999999, "end": 9075.24, "text": " to the attentional architecture. So, this is a gut. But this was so far, this was a continuous", "tokens": [50988, 281, 264, 3202, 304, 9482, 13, 407, 11, 341, 307, 257, 5228, 13, 583, 341, 390, 370, 1400, 11, 341, 390, 257, 10957, 51540], "temperature": 0.0, "avg_logprob": -0.09869802612619302, "compression_ratio": 1.641350210970464, "no_speech_prob": 0.0021439366973936558}, {"id": 1545, "seek": 905172, "start": 9075.24, "end": 9080.679999999998, "text": " time, right? And we wanted continuous space. So, the original motivation, right, when we compared", "tokens": [51540, 565, 11, 558, 30, 400, 321, 1415, 10957, 1901, 13, 407, 11, 264, 3380, 12335, 11, 558, 11, 562, 321, 5347, 51812], "temperature": 0.0, "avg_logprob": -0.09869802612619302, "compression_ratio": 1.641350210970464, "no_speech_prob": 0.0021439366973936558}, {"id": 1546, "seek": 908068, "start": 9080.68, "end": 9086.68, "text": " graphs to other objects was somehow to have a continuous analogy of the graph neural networks,", "tokens": [50364, 24877, 281, 661, 6565, 390, 6063, 281, 362, 257, 10957, 21663, 295, 264, 4295, 18161, 9590, 11, 50664], "temperature": 0.0, "avg_logprob": -0.13286187648773193, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.002757576061412692}, {"id": 1547, "seek": 908068, "start": 9086.68, "end": 9092.2, "text": " and if, again, we take a step back and look at how diffusion equations work in the plane,", "tokens": [50664, 293, 498, 11, 797, 11, 321, 747, 257, 1823, 646, 293, 574, 412, 577, 25242, 11787, 589, 294, 264, 5720, 11, 50940], "temperature": 0.0, "avg_logprob": -0.13286187648773193, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.002757576061412692}, {"id": 1548, "seek": 908068, "start": 9093.32, "end": 9098.12, "text": " when I discretize the plane as a grid, I don't really have a canonical graph, right, in the sense", "tokens": [50996, 562, 286, 25656, 1125, 264, 5720, 382, 257, 10748, 11, 286, 500, 380, 534, 362, 257, 46491, 4295, 11, 558, 11, 294, 264, 2020, 51236], "temperature": 0.0, "avg_logprob": -0.13286187648773193, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.002757576061412692}, {"id": 1549, "seek": 908068, "start": 9098.12, "end": 9103.32, "text": " that there are many ways I can discretize my differential approaches, right? So, this is how", "tokens": [51236, 300, 456, 366, 867, 2098, 286, 393, 25656, 1125, 452, 15756, 11587, 11, 558, 30, 407, 11, 341, 307, 577, 51496], "temperature": 0.0, "avg_logprob": -0.13286187648773193, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.002757576061412692}, {"id": 1550, "seek": 908068, "start": 9103.32, "end": 9108.2, "text": " I can discretize the Laplace-Anon grid. So, I can use neighbors like this, or I can rotate everything", "tokens": [51496, 286, 393, 25656, 1125, 264, 2369, 6742, 12, 7828, 266, 10748, 13, 407, 11, 286, 393, 764, 12512, 411, 341, 11, 420, 286, 393, 13121, 1203, 51740], "temperature": 0.0, "avg_logprob": -0.13286187648773193, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.002757576061412692}, {"id": 1551, "seek": 910820, "start": 9108.2, "end": 9114.92, "text": " by 45 degrees, I can use distant neighbors, I can use convex combination of all these operations,", "tokens": [50364, 538, 6905, 5310, 11, 286, 393, 764, 17275, 12512, 11, 286, 393, 764, 42432, 6562, 295, 439, 613, 7705, 11, 50700], "temperature": 0.0, "avg_logprob": -0.08043495814005534, "compression_ratio": 1.7509025270758123, "no_speech_prob": 0.0024053717497736216}, {"id": 1552, "seek": 910820, "start": 9114.92, "end": 9122.68, "text": " right, because this is a linear operator. So, bottom line on a grid, I don't have a canonical", "tokens": [50700, 558, 11, 570, 341, 307, 257, 8213, 12973, 13, 407, 11, 2767, 1622, 322, 257, 10748, 11, 286, 500, 380, 362, 257, 46491, 51088], "temperature": 0.0, "avg_logprob": -0.08043495814005534, "compression_ratio": 1.7509025270758123, "no_speech_prob": 0.0024053717497736216}, {"id": 1553, "seek": 910820, "start": 9122.68, "end": 9126.76, "text": " graph, right? I can actually use a discretization, maybe that is different at different points in", "tokens": [51088, 4295, 11, 558, 30, 286, 393, 767, 764, 257, 25656, 2144, 11, 1310, 300, 307, 819, 412, 819, 2793, 294, 51292], "temperature": 0.0, "avg_logprob": -0.08043495814005534, "compression_ratio": 1.7509025270758123, "no_speech_prob": 0.0024053717497736216}, {"id": 1554, "seek": 910820, "start": 9126.76, "end": 9131.720000000001, "text": " the grid. Of course, there will be some numerical implications, but the discretization that we", "tokens": [51292, 264, 10748, 13, 2720, 1164, 11, 456, 486, 312, 512, 29054, 16602, 11, 457, 264, 25656, 2144, 300, 321, 51540], "temperature": 0.0, "avg_logprob": -0.08043495814005534, "compression_ratio": 1.7509025270758123, "no_speech_prob": 0.0024053717497736216}, {"id": 1555, "seek": 910820, "start": 9131.720000000001, "end": 9137.320000000002, "text": " choose, right, and as a result, how the nodes are connected and which nodes propagate information to", "tokens": [51540, 2826, 11, 558, 11, 293, 382, 257, 1874, 11, 577, 264, 13891, 366, 4582, 293, 597, 13891, 48256, 1589, 281, 51820], "temperature": 0.0, "avg_logprob": -0.08043495814005534, "compression_ratio": 1.7509025270758123, "no_speech_prob": 0.0024053717497736216}, {"id": 1556, "seek": 913732, "start": 9137.32, "end": 9144.6, "text": " which nodes is, to a large extent, a numerical convenience, right? What makes sense, for example,", "tokens": [50364, 597, 13891, 307, 11, 281, 257, 2416, 8396, 11, 257, 29054, 19283, 11, 558, 30, 708, 1669, 2020, 11, 337, 1365, 11, 50728], "temperature": 0.0, "avg_logprob": -0.14438829750850282, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.0015621952479705215}, {"id": 1557, "seek": 913732, "start": 9144.6, "end": 9149.72, "text": " from the organization of the memory or the number of nodes and whatever. So, we would like somehow", "tokens": [50728, 490, 264, 4475, 295, 264, 4675, 420, 264, 1230, 295, 13891, 293, 2035, 13, 407, 11, 321, 576, 411, 6063, 50984], "temperature": 0.0, "avg_logprob": -0.14438829750850282, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.0015621952479705215}, {"id": 1558, "seek": 913732, "start": 9149.72, "end": 9155.0, "text": " to extend this mindset to general graphs. And for these purposes, instead of considering these", "tokens": [50984, 281, 10101, 341, 12543, 281, 2674, 24877, 13, 400, 337, 613, 9932, 11, 2602, 295, 8079, 613, 51248], "temperature": 0.0, "avg_logprob": -0.14438829750850282, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.0015621952479705215}, {"id": 1559, "seek": 913732, "start": 9155.96, "end": 9160.68, "text": " nonlinear diffusion equations, like Perron and Balig, by the way, they called it an isotropic", "tokens": [51296, 2107, 28263, 25242, 11787, 11, 411, 3026, 2044, 293, 13140, 328, 11, 538, 264, 636, 11, 436, 1219, 309, 364, 38018, 39173, 51532], "temperature": 0.0, "avg_logprob": -0.14438829750850282, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.0015621952479705215}, {"id": 1560, "seek": 913732, "start": 9160.68, "end": 9165.32, "text": " diffusion, which obviously, if you're familiar with PDEs, it's not an isotropic, it's not", "tokens": [51532, 25242, 11, 597, 2745, 11, 498, 291, 434, 4963, 365, 10464, 20442, 11, 309, 311, 406, 364, 38018, 39173, 11, 309, 311, 406, 51764], "temperature": 0.0, "avg_logprob": -0.14438829750850282, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.0015621952479705215}, {"id": 1561, "seek": 916532, "start": 9165.32, "end": 9169.56, "text": " homogeneous, right, because we have a scalar diffusivity function and isotropic diffusion,", "tokens": [50364, 42632, 11, 558, 11, 570, 321, 362, 257, 39684, 7593, 301, 4253, 2445, 293, 38018, 39173, 25242, 11, 50576], "temperature": 0.0, "avg_logprob": -0.13294300238291423, "compression_ratio": 1.647887323943662, "no_speech_prob": 0.0019546374678611755}, {"id": 1562, "seek": 916532, "start": 9169.56, "end": 9176.52, "text": " we also have direction, so that would be a matrix or a tensor. So, instead of considering this", "tokens": [50576, 321, 611, 362, 3513, 11, 370, 300, 576, 312, 257, 8141, 420, 257, 40863, 13, 407, 11, 2602, 295, 8079, 341, 50924], "temperature": 0.0, "avg_logprob": -0.13294300238291423, "compression_ratio": 1.647887323943662, "no_speech_prob": 0.0019546374678611755}, {"id": 1563, "seek": 916532, "start": 9177.08, "end": 9181.88, "text": " nonlinear diffusion equation, we can consider a non-Euclidean diffusion equation. And the model", "tokens": [50952, 2107, 28263, 25242, 5367, 11, 321, 393, 1949, 257, 2107, 12, 36, 1311, 31264, 282, 25242, 5367, 13, 400, 264, 2316, 51192], "temperature": 0.0, "avg_logprob": -0.13294300238291423, "compression_ratio": 1.647887323943662, "no_speech_prob": 0.0019546374678611755}, {"id": 1564, "seek": 916532, "start": 9181.88, "end": 9186.52, "text": " here is the following, that was actually done by my PhD advisor, Ron Kimmel, also in the 90s,", "tokens": [51192, 510, 307, 264, 3480, 11, 300, 390, 767, 1096, 538, 452, 14476, 19161, 11, 9949, 5652, 10909, 11, 611, 294, 264, 4289, 82, 11, 51424], "temperature": 0.0, "avg_logprob": -0.13294300238291423, "compression_ratio": 1.647887323943662, "no_speech_prob": 0.0019546374678611755}, {"id": 1565, "seek": 916532, "start": 9187.48, "end": 9193.72, "text": " about 25 years ago, maybe even more. So, again, thinking of an image, you can think of it as", "tokens": [51472, 466, 3552, 924, 2057, 11, 1310, 754, 544, 13, 407, 11, 797, 11, 1953, 295, 364, 3256, 11, 291, 393, 519, 295, 309, 382, 51784], "temperature": 0.0, "avg_logprob": -0.13294300238291423, "compression_ratio": 1.647887323943662, "no_speech_prob": 0.0019546374678611755}, {"id": 1566, "seek": 919372, "start": 9194.279999999999, "end": 9199.16, "text": " an embedded two-dimensional manifold, right? And the embedding is in this joint space,", "tokens": [50392, 364, 16741, 732, 12, 18759, 47138, 11, 558, 30, 400, 264, 12240, 3584, 307, 294, 341, 7225, 1901, 11, 50636], "temperature": 0.0, "avg_logprob": -0.11693384608284371, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.0017984614241868258}, {"id": 1567, "seek": 919372, "start": 9199.16, "end": 9203.88, "text": " where we have a combination of positional coordinates, the x, y coordinates of the pixels,", "tokens": [50636, 689, 321, 362, 257, 6562, 295, 2535, 304, 21056, 11, 264, 2031, 11, 288, 21056, 295, 264, 18668, 11, 50872], "temperature": 0.0, "avg_logprob": -0.11693384608284371, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.0017984614241868258}, {"id": 1568, "seek": 919372, "start": 9203.88, "end": 9208.439999999999, "text": " and the fissure coordinates, in this case, for example, R, G and B channels. So, a color image", "tokens": [50872, 293, 264, 283, 891, 540, 21056, 11, 294, 341, 1389, 11, 337, 1365, 11, 497, 11, 460, 293, 363, 9235, 13, 407, 11, 257, 2017, 3256, 51100], "temperature": 0.0, "avg_logprob": -0.11693384608284371, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.0017984614241868258}, {"id": 1569, "seek": 919372, "start": 9208.439999999999, "end": 9214.279999999999, "text": " is a two-dimensional surface in R5, right, using this model. Now, by virtue of this embedding,", "tokens": [51100, 307, 257, 732, 12, 18759, 3753, 294, 497, 20, 11, 558, 11, 1228, 341, 2316, 13, 823, 11, 538, 20816, 295, 341, 12240, 3584, 11, 51392], "temperature": 0.0, "avg_logprob": -0.11693384608284371, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.0017984614241868258}, {"id": 1570, "seek": 919372, "start": 9214.279999999999, "end": 9218.359999999999, "text": " we can define a metric, so we can use the standard pullback mechanism, so in the case of", "tokens": [51392, 321, 393, 6964, 257, 20678, 11, 370, 321, 393, 764, 264, 3832, 2235, 3207, 7513, 11, 370, 294, 264, 1389, 295, 51596], "temperature": 0.0, "avg_logprob": -0.11693384608284371, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.0017984614241868258}, {"id": 1571, "seek": 921836, "start": 9218.36, "end": 9223.880000000001, "text": " two-dimensional manifold, it's a two-by-two matrix, given like this, right? And we can", "tokens": [50364, 732, 12, 18759, 47138, 11, 309, 311, 257, 732, 12, 2322, 12, 20534, 8141, 11, 2212, 411, 341, 11, 558, 30, 400, 321, 393, 50640], "temperature": 0.0, "avg_logprob": -0.16315438365208285, "compression_ratio": 1.8203125, "no_speech_prob": 0.00332854688167572}, {"id": 1572, "seek": 921836, "start": 9223.880000000001, "end": 9228.52, "text": " define a Laplacian with respect to this metric, it's a non-Euclidean analogy of the Laplacian,", "tokens": [50640, 6964, 257, 2369, 564, 326, 952, 365, 3104, 281, 341, 20678, 11, 309, 311, 257, 2107, 12, 36, 1311, 31264, 282, 21663, 295, 264, 2369, 564, 326, 952, 11, 50872], "temperature": 0.0, "avg_logprob": -0.16315438365208285, "compression_ratio": 1.8203125, "no_speech_prob": 0.00332854688167572}, {"id": 1573, "seek": 921836, "start": 9228.52, "end": 9233.560000000001, "text": " called the Laplace-Beltrami operator. And we can write a diffusion equation with respect to this", "tokens": [50872, 1219, 264, 2369, 6742, 12, 33, 2018, 2356, 72, 12973, 13, 400, 321, 393, 2464, 257, 25242, 5367, 365, 3104, 281, 341, 51124], "temperature": 0.0, "avg_logprob": -0.16315438365208285, "compression_ratio": 1.8203125, "no_speech_prob": 0.00332854688167572}, {"id": 1574, "seek": 921836, "start": 9233.560000000001, "end": 9239.08, "text": " operator, it's called the Beltrami flow, and we can actually show that it's a gradient flow of", "tokens": [51124, 12973, 11, 309, 311, 1219, 264, 38869, 2356, 72, 3095, 11, 293, 321, 393, 767, 855, 300, 309, 311, 257, 16235, 3095, 295, 51400], "temperature": 0.0, "avg_logprob": -0.16315438365208285, "compression_ratio": 1.8203125, "no_speech_prob": 0.00332854688167572}, {"id": 1575, "seek": 921836, "start": 9240.12, "end": 9244.28, "text": " a generalization of the Dirichlet energy that is called the polycop functional. It's used in", "tokens": [51452, 257, 2674, 2144, 295, 264, 34422, 480, 2631, 2281, 300, 307, 1219, 264, 6754, 13084, 11745, 13, 467, 311, 1143, 294, 51660], "temperature": 0.0, "avg_logprob": -0.16315438365208285, "compression_ratio": 1.8203125, "no_speech_prob": 0.00332854688167572}, {"id": 1576, "seek": 924428, "start": 9244.36, "end": 9247.720000000001, "text": " high-energy physics in bosonic strings, don't ask me what it is, but", "tokens": [50368, 1090, 12, 49016, 10649, 294, 30641, 11630, 13985, 11, 500, 380, 1029, 385, 437, 309, 307, 11, 457, 50536], "temperature": 0.0, "avg_logprob": -0.12710393678157703, "compression_ratio": 1.7911646586345382, "no_speech_prob": 0.004895749036222696}, {"id": 1577, "seek": 924428, "start": 9249.720000000001, "end": 9257.16, "text": " that's something is described by this energy. So, by analogy, we can do something like this", "tokens": [50636, 300, 311, 746, 307, 7619, 538, 341, 2281, 13, 407, 11, 538, 21663, 11, 321, 393, 360, 746, 411, 341, 51008], "temperature": 0.0, "avg_logprob": -0.12710393678157703, "compression_ratio": 1.7911646586345382, "no_speech_prob": 0.004895749036222696}, {"id": 1578, "seek": 924428, "start": 9257.16, "end": 9262.92, "text": " on a graph, so now every node in the graph, in addition to having the fissure coordinates,", "tokens": [51008, 322, 257, 4295, 11, 370, 586, 633, 9984, 294, 264, 4295, 11, 294, 4500, 281, 1419, 264, 283, 891, 540, 21056, 11, 51296], "temperature": 0.0, "avg_logprob": -0.12710393678157703, "compression_ratio": 1.7911646586345382, "no_speech_prob": 0.004895749036222696}, {"id": 1579, "seek": 924428, "start": 9262.92, "end": 9267.640000000001, "text": " also has some positional coordinates, right, so positional encoding. Ideally, this positional", "tokens": [51296, 611, 575, 512, 2535, 304, 21056, 11, 558, 11, 370, 2535, 304, 43430, 13, 40817, 11, 341, 2535, 304, 51532], "temperature": 0.0, "avg_logprob": -0.12710393678157703, "compression_ratio": 1.7911646586345382, "no_speech_prob": 0.004895749036222696}, {"id": 1580, "seek": 924428, "start": 9267.640000000001, "end": 9273.08, "text": " encoding should somehow represent the structure of the graph, right, in the sense that nearby points", "tokens": [51532, 43430, 820, 6063, 2906, 264, 3877, 295, 264, 4295, 11, 558, 11, 294, 264, 2020, 300, 11184, 2793, 51804], "temperature": 0.0, "avg_logprob": -0.12710393678157703, "compression_ratio": 1.7911646586345382, "no_speech_prob": 0.004895749036222696}, {"id": 1581, "seek": 927308, "start": 9273.08, "end": 9279.64, "text": " in this u-component of the space should be more likely connected by an edge, right? And the Beltrami", "tokens": [50364, 294, 341, 344, 12, 21541, 30365, 295, 264, 1901, 820, 312, 544, 3700, 4582, 538, 364, 4691, 11, 558, 30, 400, 264, 38869, 2356, 72, 50692], "temperature": 0.0, "avg_logprob": -0.13921893372827646, "compression_ratio": 1.6347826086956523, "no_speech_prob": 0.001730657066218555}, {"id": 1582, "seek": 927308, "start": 9279.64, "end": 9285.88, "text": " flow, basically, it evolves, so we have again here a parametric diffusivity, it evolves both", "tokens": [50692, 3095, 11, 1936, 11, 309, 43737, 11, 370, 321, 362, 797, 510, 257, 6220, 17475, 7593, 301, 4253, 11, 309, 43737, 1293, 51004], "temperature": 0.0, "avg_logprob": -0.13921893372827646, "compression_ratio": 1.6347826086956523, "no_speech_prob": 0.001730657066218555}, {"id": 1583, "seek": 927308, "start": 9285.88, "end": 9291.16, "text": " components, right, that I collectively denote by z, and the evolution of x-component is the", "tokens": [51004, 6677, 11, 558, 11, 300, 286, 24341, 45708, 538, 710, 11, 293, 264, 9303, 295, 2031, 12, 21541, 30365, 307, 264, 51268], "temperature": 0.0, "avg_logprob": -0.13921893372827646, "compression_ratio": 1.6347826086956523, "no_speech_prob": 0.001730657066218555}, {"id": 1584, "seek": 927308, "start": 9291.16, "end": 9295.72, "text": " standard fissure diffusion. You can think of the evolution of u as some form of soft graph", "tokens": [51268, 3832, 283, 891, 540, 25242, 13, 509, 393, 519, 295, 264, 9303, 295, 344, 382, 512, 1254, 295, 2787, 4295, 51496], "temperature": 0.0, "avg_logprob": -0.13921893372827646, "compression_ratio": 1.6347826086956523, "no_speech_prob": 0.001730657066218555}, {"id": 1585, "seek": 929572, "start": 9295.8, "end": 9303.24, "text": " rewiring, because what I can do, if two nodes become closer in this u-coordinate, I can decide to", "tokens": [50368, 319, 86, 5057, 11, 570, 437, 286, 393, 360, 11, 498, 732, 13891, 1813, 4966, 294, 341, 344, 12, 1291, 37326, 11, 286, 393, 4536, 281, 50740], "temperature": 0.0, "avg_logprob": -0.14738322061205666, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.02517903409898281}, {"id": 1586, "seek": 929572, "start": 9303.24, "end": 9309.0, "text": " create an edge between them if there is no edge, or if the drift apart, I can decide to cut the", "tokens": [50740, 1884, 364, 4691, 1296, 552, 498, 456, 307, 572, 4691, 11, 420, 498, 264, 19699, 4936, 11, 286, 393, 4536, 281, 1723, 264, 51028], "temperature": 0.0, "avg_logprob": -0.14738322061205666, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.02517903409898281}, {"id": 1587, "seek": 929572, "start": 9309.0, "end": 9314.279999999999, "text": " edge, so overall, I will facilitate the propagation of information, and I know that it sounds cumbersome,", "tokens": [51028, 4691, 11, 370, 4787, 11, 286, 486, 20207, 264, 38377, 295, 1589, 11, 293, 286, 458, 300, 309, 3263, 12713, 1616, 423, 11, 51292], "temperature": 0.0, "avg_logprob": -0.14738322061205666, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.02517903409898281}, {"id": 1588, "seek": 929572, "start": 9314.279999999999, "end": 9317.96, "text": " but this is how it will look like, so again, this is the core graph, so it has,", "tokens": [51292, 457, 341, 307, 577, 309, 486, 574, 411, 11, 370, 797, 11, 341, 307, 264, 4965, 4295, 11, 370, 309, 575, 11, 51476], "temperature": 0.0, "avg_logprob": -0.14738322061205666, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.02517903409898281}, {"id": 1589, "seek": 929572, "start": 9319.56, "end": 9324.279999999999, "text": " basically, there are three things happening here, so the positions of the circles, right, so circles", "tokens": [51556, 1936, 11, 456, 366, 1045, 721, 2737, 510, 11, 370, 264, 8432, 295, 264, 13040, 11, 558, 11, 370, 13040, 51792], "temperature": 0.0, "avg_logprob": -0.14738322061205666, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.02517903409898281}, {"id": 1590, "seek": 932428, "start": 9324.28, "end": 9330.12, "text": " are nodes, their positions represent some two-dimensional positional coordinates, the colors", "tokens": [50364, 366, 13891, 11, 641, 8432, 2906, 512, 732, 12, 18759, 2535, 304, 21056, 11, 264, 4577, 50656], "temperature": 0.0, "avg_logprob": -0.08991662242956329, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.003281214041635394}, {"id": 1591, "seek": 932428, "start": 9330.12, "end": 9336.12, "text": " represent three-dimensional projection of the fissures, and you see that they're both components", "tokens": [50656, 2906, 1045, 12, 18759, 22743, 295, 264, 283, 891, 1303, 11, 293, 291, 536, 300, 436, 434, 1293, 6677, 50956], "temperature": 0.0, "avg_logprob": -0.08991662242956329, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.003281214041635394}, {"id": 1592, "seek": 932428, "start": 9336.12, "end": 9340.36, "text": " are evolving, and the graph is also changed on the fly, right, so when the clusters drift apart,", "tokens": [50956, 366, 21085, 11, 293, 264, 4295, 307, 611, 3105, 322, 264, 3603, 11, 558, 11, 370, 562, 264, 23313, 19699, 4936, 11, 51168], "temperature": 0.0, "avg_logprob": -0.08991662242956329, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.003281214041635394}, {"id": 1593, "seek": 932428, "start": 9340.36, "end": 9346.12, "text": " then we cut the edges between them, so right, so it's fissure diffusion, positional coordinates", "tokens": [51168, 550, 321, 1723, 264, 8819, 1296, 552, 11, 370, 558, 11, 370, 309, 311, 283, 891, 540, 25242, 11, 2535, 304, 21056, 51456], "temperature": 0.0, "avg_logprob": -0.08991662242956329, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.003281214041635394}, {"id": 1594, "seek": 932428, "start": 9346.12, "end": 9351.560000000001, "text": " are changing, and the graph is rewired, and you can see that the task here is node classification,", "tokens": [51456, 366, 4473, 11, 293, 264, 4295, 307, 319, 86, 1824, 11, 293, 291, 393, 536, 300, 264, 5633, 510, 307, 9984, 21538, 11, 51728], "temperature": 0.0, "avg_logprob": -0.08991662242956329, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.003281214041635394}, {"id": 1595, "seek": 935156, "start": 9351.56, "end": 9356.039999999999, "text": " so there are clearly seven classes of nodes that we can clearly distinguish here.", "tokens": [50364, 370, 456, 366, 4448, 3407, 5359, 295, 13891, 300, 321, 393, 4448, 20206, 510, 13, 50588], "temperature": 0.0, "avg_logprob": -0.11331105718807298, "compression_ratio": 1.6297709923664123, "no_speech_prob": 0.0019198942463845015}, {"id": 1596, "seek": 935156, "start": 9356.92, "end": 9360.439999999999, "text": " Now, if you think of it from the standpoint of signal processing,", "tokens": [50632, 823, 11, 498, 291, 519, 295, 309, 490, 264, 15827, 295, 6358, 9007, 11, 50808], "temperature": 0.0, "avg_logprob": -0.11331105718807298, "compression_ratio": 1.6297709923664123, "no_speech_prob": 0.0019198942463845015}, {"id": 1597, "seek": 935156, "start": 9361.08, "end": 9364.84, "text": " we have a very disturbing picture here, right, so we have a filter that happens on the domain,", "tokens": [50840, 321, 362, 257, 588, 21903, 3036, 510, 11, 558, 11, 370, 321, 362, 257, 6608, 300, 2314, 322, 264, 9274, 11, 51028], "temperature": 0.0, "avg_logprob": -0.11331105718807298, "compression_ratio": 1.6297709923664123, "no_speech_prob": 0.0019198942463845015}, {"id": 1598, "seek": 935156, "start": 9365.4, "end": 9371.64, "text": " yeah, a question? Maybe before you move to the next one, I was just wondering because I think", "tokens": [51056, 1338, 11, 257, 1168, 30, 2704, 949, 291, 1286, 281, 264, 958, 472, 11, 286, 390, 445, 6359, 570, 286, 519, 51368], "temperature": 0.0, "avg_logprob": -0.11331105718807298, "compression_ratio": 1.6297709923664123, "no_speech_prob": 0.0019198942463845015}, {"id": 1599, "seek": 935156, "start": 9371.64, "end": 9376.279999999999, "text": " what started to appear in this intellectual is that you apply some ideas from differential", "tokens": [51368, 437, 1409, 281, 4204, 294, 341, 12576, 307, 300, 291, 3079, 512, 3487, 490, 15756, 51600], "temperature": 0.0, "avg_logprob": -0.11331105718807298, "compression_ratio": 1.6297709923664123, "no_speech_prob": 0.0019198942463845015}, {"id": 1600, "seek": 937628, "start": 9376.28, "end": 9381.720000000001, "text": " geometry to graphs, and maybe not yet directly, but... Not yet, right, so... Okay, so we are", "tokens": [50364, 18426, 281, 24877, 11, 293, 1310, 406, 1939, 3838, 11, 457, 485, 1726, 1939, 11, 558, 11, 370, 485, 1033, 11, 370, 321, 366, 50636], "temperature": 0.0, "avg_logprob": -0.14786135765814012, "compression_ratio": 1.6747404844290656, "no_speech_prob": 0.03188594803214073}, {"id": 1601, "seek": 937628, "start": 9381.720000000001, "end": 9385.480000000001, "text": " like talking about metrics and this sort of, so I wonder, maybe you'll be talking about it next,", "tokens": [50636, 411, 1417, 466, 16367, 293, 341, 1333, 295, 11, 370, 286, 2441, 11, 1310, 291, 603, 312, 1417, 466, 309, 958, 11, 50824], "temperature": 0.0, "avg_logprob": -0.14786135765814012, "compression_ratio": 1.6747404844290656, "no_speech_prob": 0.03188594803214073}, {"id": 1602, "seek": 937628, "start": 9385.480000000001, "end": 9392.76, "text": " so sorry if I'm pushing it forward, but what do you think are the limits which come from the", "tokens": [50824, 370, 2597, 498, 286, 478, 7380, 309, 2128, 11, 457, 437, 360, 291, 519, 366, 264, 10406, 597, 808, 490, 264, 51188], "temperature": 0.0, "avg_logprob": -0.14786135765814012, "compression_ratio": 1.6747404844290656, "no_speech_prob": 0.03188594803214073}, {"id": 1603, "seek": 937628, "start": 9392.76, "end": 9399.640000000001, "text": " fact that graphs are basically discrete structures, like how free are we to apply ideas? Yeah, give me", "tokens": [51188, 1186, 300, 24877, 366, 1936, 27706, 9227, 11, 411, 577, 1737, 366, 321, 281, 3079, 3487, 30, 865, 11, 976, 385, 51532], "temperature": 0.0, "avg_logprob": -0.14786135765814012, "compression_ratio": 1.6747404844290656, "no_speech_prob": 0.03188594803214073}, {"id": 1604, "seek": 937628, "start": 9399.640000000001, "end": 9403.480000000001, "text": " a few minutes and I will get there, yeah, so you can find the analogies, right, not everything has", "tokens": [51532, 257, 1326, 2077, 293, 286, 486, 483, 456, 11, 1338, 11, 370, 291, 393, 915, 264, 16660, 530, 11, 558, 11, 406, 1203, 575, 51724], "temperature": 0.0, "avg_logprob": -0.14786135765814012, "compression_ratio": 1.6747404844290656, "no_speech_prob": 0.03188594803214073}, {"id": 1605, "seek": 940348, "start": 9403.48, "end": 9408.84, "text": " exactly a correspondence, right, so, but these analogies, I hope to show that they can be quite", "tokens": [50364, 2293, 257, 38135, 11, 558, 11, 370, 11, 457, 613, 16660, 530, 11, 286, 1454, 281, 855, 300, 436, 393, 312, 1596, 50632], "temperature": 0.0, "avg_logprob": -0.10863251573457493, "compression_ratio": 1.89453125, "no_speech_prob": 0.004138392396271229}, {"id": 1606, "seek": 940348, "start": 9408.84, "end": 9414.92, "text": " useful, right, so, but again, if you look at these pictures, so we have some kind of disturbing", "tokens": [50632, 4420, 11, 558, 11, 370, 11, 457, 797, 11, 498, 291, 574, 412, 613, 5242, 11, 370, 321, 362, 512, 733, 295, 21903, 50936], "temperature": 0.0, "avg_logprob": -0.10863251573457493, "compression_ratio": 1.89453125, "no_speech_prob": 0.004138392396271229}, {"id": 1607, "seek": 940348, "start": 9414.92, "end": 9419.64, "text": " picture here, so we have a filter on the domain, and the domain is changing under our feet, right,", "tokens": [50936, 3036, 510, 11, 370, 321, 362, 257, 6608, 322, 264, 9274, 11, 293, 264, 9274, 307, 4473, 833, 527, 3521, 11, 558, 11, 51172], "temperature": 0.0, "avg_logprob": -0.10863251573457493, "compression_ratio": 1.89453125, "no_speech_prob": 0.004138392396271229}, {"id": 1608, "seek": 940348, "start": 9419.64, "end": 9425.24, "text": " so imagine that you're applying some filter, right, which is what it is, right, the diffusion", "tokens": [51172, 370, 3811, 300, 291, 434, 9275, 512, 6608, 11, 558, 11, 597, 307, 437, 309, 307, 11, 558, 11, 264, 25242, 51452], "temperature": 0.0, "avg_logprob": -0.10863251573457493, "compression_ratio": 1.89453125, "no_speech_prob": 0.004138392396271229}, {"id": 1609, "seek": 940348, "start": 9425.24, "end": 9431.16, "text": " equation, you can think of it as a form of filter, low pass filter, and the domain is moving, so I'm", "tokens": [51452, 5367, 11, 291, 393, 519, 295, 309, 382, 257, 1254, 295, 6608, 11, 2295, 1320, 6608, 11, 293, 264, 9274, 307, 2684, 11, 370, 286, 478, 51748], "temperature": 0.0, "avg_logprob": -0.10863251573457493, "compression_ratio": 1.89453125, "no_speech_prob": 0.004138392396271229}, {"id": 1610, "seek": 943116, "start": 9431.16, "end": 9437.48, "text": " doing a filter and then nodes are somehow moving away from me, but this is a very common picture", "tokens": [50364, 884, 257, 6608, 293, 550, 13891, 366, 6063, 2684, 1314, 490, 385, 11, 457, 341, 307, 257, 588, 2689, 3036, 50680], "temperature": 0.0, "avg_logprob": -0.13407860861884224, "compression_ratio": 1.8120300751879699, "no_speech_prob": 0.001980424392968416}, {"id": 1611, "seek": 943116, "start": 9437.48, "end": 9441.8, "text": " in differential geometry actually, and it's very common to take a manifold and evolve it under", "tokens": [50680, 294, 15756, 18426, 767, 11, 293, 309, 311, 588, 2689, 281, 747, 257, 47138, 293, 16693, 309, 833, 50896], "temperature": 0.0, "avg_logprob": -0.13407860861884224, "compression_ratio": 1.8120300751879699, "no_speech_prob": 0.001980424392968416}, {"id": 1612, "seek": 943116, "start": 9441.8, "end": 9445.56, "text": " some evolution equation, and typically what, when you evolve a manifold, you're interested in what", "tokens": [50896, 512, 9303, 5367, 11, 293, 5850, 437, 11, 562, 291, 16693, 257, 47138, 11, 291, 434, 3102, 294, 437, 51084], "temperature": 0.0, "avg_logprob": -0.13407860861884224, "compression_ratio": 1.8120300751879699, "no_speech_prob": 0.001980424392968416}, {"id": 1613, "seek": 943116, "start": 9445.56, "end": 9450.039999999999, "text": " happens to the metric, right, so here's an example of an evolution equation that is called the Ricci", "tokens": [51084, 2314, 281, 264, 20678, 11, 558, 11, 370, 510, 311, 364, 1365, 295, 364, 9303, 5367, 300, 307, 1219, 264, 21215, 537, 51308], "temperature": 0.0, "avg_logprob": -0.13407860861884224, "compression_ratio": 1.8120300751879699, "no_speech_prob": 0.001980424392968416}, {"id": 1614, "seek": 943116, "start": 9450.039999999999, "end": 9457.8, "text": " flow, so you take the first order derivative, the temporal derivative of the metric tensor", "tokens": [51308, 3095, 11, 370, 291, 747, 264, 700, 1668, 13760, 11, 264, 30881, 13760, 295, 264, 20678, 40863, 51696], "temperature": 0.0, "avg_logprob": -0.13407860861884224, "compression_ratio": 1.8120300751879699, "no_speech_prob": 0.001980424392968416}, {"id": 1615, "seek": 945780, "start": 9457.8, "end": 9463.56, "text": " of the manifold, right, denoted here by g, and you make it equal to the Ricci curvature tensor,", "tokens": [50364, 295, 264, 47138, 11, 558, 11, 1441, 23325, 510, 538, 290, 11, 293, 291, 652, 309, 2681, 281, 264, 21215, 537, 37638, 40863, 11, 50652], "temperature": 0.0, "avg_logprob": -0.07901001791668753, "compression_ratio": 1.8609022556390977, "no_speech_prob": 0.001881997101008892}, {"id": 1616, "seek": 945780, "start": 9463.56, "end": 9469.8, "text": " right, so basically the metric evolves proportionally to the to the local curvature, so it looks very", "tokens": [50652, 558, 11, 370, 1936, 264, 20678, 43737, 16068, 379, 281, 264, 281, 264, 2654, 37638, 11, 370, 309, 1542, 588, 50964], "temperature": 0.0, "avg_logprob": -0.07901001791668753, "compression_ratio": 1.8609022556390977, "no_speech_prob": 0.001881997101008892}, {"id": 1617, "seek": 945780, "start": 9469.8, "end": 9474.359999999999, "text": " much like the diffusion equation, so here we have temporal derivative, here we have some second order", "tokens": [50964, 709, 411, 264, 25242, 5367, 11, 370, 510, 321, 362, 30881, 13760, 11, 510, 321, 362, 512, 1150, 1668, 51192], "temperature": 0.0, "avg_logprob": -0.07901001791668753, "compression_ratio": 1.8609022556390977, "no_speech_prob": 0.001881997101008892}, {"id": 1618, "seek": 945780, "start": 9474.359999999999, "end": 9479.64, "text": " differential quantity that looks kind of like our Laplacian, right, so structurally it's similar", "tokens": [51192, 15756, 11275, 300, 1542, 733, 295, 411, 527, 2369, 564, 326, 952, 11, 558, 11, 370, 6594, 6512, 309, 311, 2531, 51456], "temperature": 0.0, "avg_logprob": -0.07901001791668753, "compression_ratio": 1.8609022556390977, "no_speech_prob": 0.001881997101008892}, {"id": 1619, "seek": 945780, "start": 9479.64, "end": 9484.519999999999, "text": " to the diffusion equation, of course what it does is a very different thing, and if you start with", "tokens": [51456, 281, 264, 25242, 5367, 11, 295, 1164, 437, 309, 775, 307, 257, 588, 819, 551, 11, 293, 498, 291, 722, 365, 51700], "temperature": 0.0, "avg_logprob": -0.07901001791668753, "compression_ratio": 1.8609022556390977, "no_speech_prob": 0.001881997101008892}, {"id": 1620, "seek": 948452, "start": 9484.52, "end": 9490.36, "text": " this manifold, which has positive curvature on, so this kind of dumbbells on the spheres,", "tokens": [50364, 341, 47138, 11, 597, 575, 3353, 37638, 322, 11, 370, 341, 733, 295, 39316, 82, 322, 264, 41225, 11, 50656], "temperature": 0.0, "avg_logprob": -0.11206537279589422, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.0027195881120860577}, {"id": 1621, "seek": 948452, "start": 9490.36, "end": 9494.92, "text": " it has positive curvature, right, and the neck between them, it has negative curvature, so if", "tokens": [50656, 309, 575, 3353, 37638, 11, 558, 11, 293, 264, 6189, 1296, 552, 11, 309, 575, 3671, 37638, 11, 370, 498, 50884], "temperature": 0.0, "avg_logprob": -0.11206537279589422, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.0027195881120860577}, {"id": 1622, "seek": 948452, "start": 9494.92, "end": 9501.08, "text": " you run this diffusion, if you run the Ricci flow backwards in time, what will happen is that this", "tokens": [50884, 291, 1190, 341, 25242, 11, 498, 291, 1190, 264, 21215, 537, 3095, 12204, 294, 565, 11, 437, 486, 1051, 307, 300, 341, 51192], "temperature": 0.0, "avg_logprob": -0.11206537279589422, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.0027195881120860577}, {"id": 1623, "seek": 948452, "start": 9501.08, "end": 9505.32, "text": " dumbbell become more like an ellipsoid than more like a sphere, and then will collapse into a point,", "tokens": [51192, 39316, 1813, 544, 411, 364, 8284, 647, 539, 327, 813, 544, 411, 257, 16687, 11, 293, 550, 486, 15584, 666, 257, 935, 11, 51404], "temperature": 0.0, "avg_logprob": -0.11206537279589422, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.0027195881120860577}, {"id": 1624, "seek": 948452, "start": 9505.32, "end": 9513.0, "text": " right, and it was introduced by Richard Hamilton in the 80s with the purpose of proving a famous", "tokens": [51404, 558, 11, 293, 309, 390, 7268, 538, 9809, 18484, 294, 264, 4688, 82, 365, 264, 4334, 295, 27221, 257, 4618, 51788], "temperature": 0.0, "avg_logprob": -0.11206537279589422, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.0027195881120860577}, {"id": 1625, "seek": 951300, "start": 9513.56, "end": 9520.92, "text": " conjecture in topology that claims that you can characterize spheres by your ability to take", "tokens": [50392, 416, 1020, 540, 294, 1192, 1793, 300, 9441, 300, 291, 393, 38463, 41225, 538, 428, 3485, 281, 747, 50760], "temperature": 0.0, "avg_logprob": -0.08064380118517371, "compression_ratio": 2.0129310344827585, "no_speech_prob": 0.008457143791019917}, {"id": 1626, "seek": 951300, "start": 9520.92, "end": 9525.96, "text": " a closed curve and collapse it into a point, right, so this is how we characterize two-dimensional", "tokens": [50760, 257, 5395, 7605, 293, 15584, 309, 666, 257, 935, 11, 558, 11, 370, 341, 307, 577, 321, 38463, 732, 12, 18759, 51012], "temperature": 0.0, "avg_logprob": -0.08064380118517371, "compression_ratio": 2.0129310344827585, "no_speech_prob": 0.008457143791019917}, {"id": 1627, "seek": 951300, "start": 9525.96, "end": 9531.96, "text": " sphere, I can take any closed curve on a sphere and I can evolve it and collapse into a point,", "tokens": [51012, 16687, 11, 286, 393, 747, 604, 5395, 7605, 322, 257, 16687, 293, 286, 393, 16693, 309, 293, 15584, 666, 257, 935, 11, 51312], "temperature": 0.0, "avg_logprob": -0.08064380118517371, "compression_ratio": 2.0129310344827585, "no_speech_prob": 0.008457143791019917}, {"id": 1628, "seek": 951300, "start": 9531.96, "end": 9535.96, "text": " right, I cannot do it on a torus, so if I have a torus and I have a curve like this,", "tokens": [51312, 558, 11, 286, 2644, 360, 309, 322, 257, 3930, 301, 11, 370, 498, 286, 362, 257, 3930, 301, 293, 286, 362, 257, 7605, 411, 341, 11, 51512], "temperature": 0.0, "avg_logprob": -0.08064380118517371, "compression_ratio": 2.0129310344827585, "no_speech_prob": 0.008457143791019917}, {"id": 1629, "seek": 951300, "start": 9536.68, "end": 9541.56, "text": " then no matter what I do it cannot be collapsed to a point, so the conjecture was that you can,", "tokens": [51548, 550, 572, 1871, 437, 286, 360, 309, 2644, 312, 24578, 281, 257, 935, 11, 370, 264, 416, 1020, 540, 390, 300, 291, 393, 11, 51792], "temperature": 0.0, "avg_logprob": -0.08064380118517371, "compression_ratio": 2.0129310344827585, "no_speech_prob": 0.008457143791019917}, {"id": 1630, "seek": 954156, "start": 9542.439999999999, "end": 9547.56, "text": " you can characterize higher-dimensional spheres in this way, and you obviously heard about it,", "tokens": [50408, 291, 393, 38463, 2946, 12, 18759, 41225, 294, 341, 636, 11, 293, 291, 2745, 2198, 466, 309, 11, 50664], "temperature": 0.0, "avg_logprob": -0.12949220426790006, "compression_ratio": 1.4979253112033195, "no_speech_prob": 0.004855557810515165}, {"id": 1631, "seek": 954156, "start": 9547.56, "end": 9553.4, "text": " the Poincar\u00e9 conjecture, and it was shown by Perlman, actually a slightly more general result,", "tokens": [50664, 264, 6165, 4647, 36832, 416, 1020, 540, 11, 293, 309, 390, 4898, 538, 3026, 75, 1601, 11, 767, 257, 4748, 544, 2674, 1874, 11, 50956], "temperature": 0.0, "avg_logprob": -0.12949220426790006, "compression_ratio": 1.4979253112033195, "no_speech_prob": 0.004855557810515165}, {"id": 1632, "seek": 954156, "start": 9554.76, "end": 9562.519999999999, "text": " using the mechanism of Ricci flows, right, and that was a breakthrough of the century,", "tokens": [51024, 1228, 264, 7513, 295, 21215, 537, 12867, 11, 558, 11, 293, 300, 390, 257, 22397, 295, 264, 4901, 11, 51412], "temperature": 0.0, "avg_logprob": -0.12949220426790006, "compression_ratio": 1.4979253112033195, "no_speech_prob": 0.004855557810515165}, {"id": 1633, "seek": 954156, "start": 9562.519999999999, "end": 9567.48, "text": " it stood open for more than 100 years. Now, what does it have to do with our graphs", "tokens": [51412, 309, 9371, 1269, 337, 544, 813, 2319, 924, 13, 823, 11, 437, 775, 309, 362, 281, 360, 365, 527, 24877, 51660], "temperature": 0.0, "avg_logprob": -0.12949220426790006, "compression_ratio": 1.4979253112033195, "no_speech_prob": 0.004855557810515165}, {"id": 1634, "seek": 956748, "start": 9567.56, "end": 9572.76, "text": " and graph neural networks, so I remind you that we had this phenomenon, right, that message", "tokens": [50368, 293, 4295, 18161, 9590, 11, 370, 286, 4160, 291, 300, 321, 632, 341, 14029, 11, 558, 11, 300, 3636, 50628], "temperature": 0.0, "avg_logprob": -0.10814501621105053, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.0055824690498411655}, {"id": 1635, "seek": 956748, "start": 9572.76, "end": 9578.119999999999, "text": " passing might not be, might not work well on some graphs, right, so there might be some,", "tokens": [50628, 8437, 1062, 406, 312, 11, 1062, 406, 589, 731, 322, 512, 24877, 11, 558, 11, 370, 456, 1062, 312, 512, 11, 50896], "temperature": 0.0, "avg_logprob": -0.10814501621105053, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.0055824690498411655}, {"id": 1636, "seek": 956748, "start": 9578.119999999999, "end": 9582.68, "text": " some phenomena that some graphs might be unfriendly for, for message passing, and in particular", "tokens": [50896, 512, 22004, 300, 512, 24877, 1062, 312, 3971, 4896, 356, 337, 11, 337, 3636, 8437, 11, 293, 294, 1729, 51124], "temperature": 0.0, "avg_logprob": -0.10814501621105053, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.0055824690498411655}, {"id": 1637, "seek": 956748, "start": 9582.68, "end": 9588.439999999999, "text": " it depends both on the structure of the graph and the task, and if my task requires to propagate", "tokens": [51124, 309, 5946, 1293, 322, 264, 3877, 295, 264, 4295, 293, 264, 5633, 11, 293, 498, 452, 5633, 7029, 281, 48256, 51412], "temperature": 0.0, "avg_logprob": -0.10814501621105053, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.0055824690498411655}, {"id": 1638, "seek": 956748, "start": 9588.439999999999, "end": 9593.32, "text": " information from distant nodes, and the structure of the graph is such that the receptive field", "tokens": [51412, 1589, 490, 17275, 13891, 11, 293, 264, 3877, 295, 264, 4295, 307, 1270, 300, 264, 45838, 2519, 51656], "temperature": 0.0, "avg_logprob": -0.10814501621105053, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.0055824690498411655}, {"id": 1639, "seek": 959332, "start": 9593.32, "end": 9597.8, "text": " of the graph neural network grows exponentially fast, right, so the number of the neighbors of", "tokens": [50364, 295, 264, 4295, 18161, 3209, 13156, 37330, 2370, 11, 558, 11, 370, 264, 1230, 295, 264, 12512, 295, 50588], "temperature": 0.0, "avg_logprob": -0.09929119291759672, "compression_ratio": 1.8582995951417005, "no_speech_prob": 0.002185442252084613}, {"id": 1640, "seek": 959332, "start": 9597.8, "end": 9603.32, "text": " the neighbors of the neighbors becomes very large very quickly, this happens in trees,", "tokens": [50588, 264, 12512, 295, 264, 12512, 3643, 588, 2416, 588, 2661, 11, 341, 2314, 294, 5852, 11, 50864], "temperature": 0.0, "avg_logprob": -0.09929119291759672, "compression_ratio": 1.8582995951417005, "no_speech_prob": 0.002185442252084613}, {"id": 1641, "seek": 959332, "start": 9603.32, "end": 9608.039999999999, "text": " this happens in what is called small world graphs like social networks, then we have a problem,", "tokens": [50864, 341, 2314, 294, 437, 307, 1219, 1359, 1002, 24877, 411, 2093, 9590, 11, 550, 321, 362, 257, 1154, 11, 51100], "temperature": 0.0, "avg_logprob": -0.09929119291759672, "compression_ratio": 1.8582995951417005, "no_speech_prob": 0.002185442252084613}, {"id": 1642, "seek": 959332, "start": 9608.039999999999, "end": 9611.64, "text": " we have a lot of information that we need to squeeze into a single feature vector,", "tokens": [51100, 321, 362, 257, 688, 295, 1589, 300, 321, 643, 281, 13578, 666, 257, 2167, 4111, 8062, 11, 51280], "temperature": 0.0, "avg_logprob": -0.09929119291759672, "compression_ratio": 1.8582995951417005, "no_speech_prob": 0.002185442252084613}, {"id": 1643, "seek": 959332, "start": 9612.68, "end": 9620.279999999999, "text": " and this is a phenomenon that we call overscorching, so let's, let's define mathematically what we", "tokens": [51332, 293, 341, 307, 257, 14029, 300, 321, 818, 670, 4417, 284, 17354, 11, 370, 718, 311, 11, 718, 311, 6964, 44003, 437, 321, 51712], "temperature": 0.0, "avg_logprob": -0.09929119291759672, "compression_ratio": 1.8582995951417005, "no_speech_prob": 0.002185442252084613}, {"id": 1644, "seek": 962028, "start": 9620.28, "end": 9624.6, "text": " mean by overscorching, so let's say that we have a message passing architecture of this form,", "tokens": [50364, 914, 538, 670, 4417, 284, 17354, 11, 370, 718, 311, 584, 300, 321, 362, 257, 3636, 8437, 9482, 295, 341, 1254, 11, 50580], "temperature": 0.0, "avg_logprob": -0.09484053802490235, "compression_ratio": 1.9256198347107438, "no_speech_prob": 0.0029137504752725363}, {"id": 1645, "seek": 962028, "start": 9624.6, "end": 9631.08, "text": " right, so we have the node itself at layer k, and we have the neighbors, we combine them with some", "tokens": [50580, 558, 11, 370, 321, 362, 264, 9984, 2564, 412, 4583, 350, 11, 293, 321, 362, 264, 12512, 11, 321, 10432, 552, 365, 512, 50904], "temperature": 0.0, "avg_logprob": -0.09484053802490235, "compression_ratio": 1.9256198347107438, "no_speech_prob": 0.0029137504752725363}, {"id": 1646, "seek": 962028, "start": 9631.08, "end": 9636.12, "text": " learnable weights, let's call them w1 and w2, let's say that the depth of this architecture is l,", "tokens": [50904, 1466, 712, 17443, 11, 718, 311, 818, 552, 261, 16, 293, 261, 17, 11, 718, 311, 584, 300, 264, 7161, 295, 341, 9482, 307, 287, 11, 51156], "temperature": 0.0, "avg_logprob": -0.09484053802490235, "compression_ratio": 1.9256198347107438, "no_speech_prob": 0.0029137504752725363}, {"id": 1647, "seek": 962028, "start": 9637.24, "end": 9643.24, "text": " the width, right, so the internal dimension is p, the long linearities are well behaved,", "tokens": [51212, 264, 11402, 11, 558, 11, 370, 264, 6920, 10139, 307, 280, 11, 264, 938, 8213, 1088, 366, 731, 48249, 11, 51512], "temperature": 0.0, "avg_logprob": -0.09484053802490235, "compression_ratio": 1.9256198347107438, "no_speech_prob": 0.0029137504752725363}, {"id": 1648, "seek": 962028, "start": 9643.24, "end": 9648.04, "text": " right, so the elliptics continues, and we also have some bound on the, on the weights,", "tokens": [51512, 558, 11, 370, 264, 8284, 22439, 1167, 6515, 11, 293, 321, 611, 362, 512, 5472, 322, 264, 11, 322, 264, 17443, 11, 51752], "temperature": 0.0, "avg_logprob": -0.09484053802490235, "compression_ratio": 1.9256198347107438, "no_speech_prob": 0.0029137504752725363}, {"id": 1649, "seek": 964804, "start": 9648.12, "end": 9654.04, "text": " so this is what characterizes our architecture, so what is overscorching, it's some form of insensitivity,", "tokens": [50368, 370, 341, 307, 437, 2517, 5660, 527, 9482, 11, 370, 437, 307, 670, 4417, 284, 17354, 11, 309, 311, 512, 1254, 295, 1028, 694, 270, 4253, 11, 50664], "temperature": 0.0, "avg_logprob": -0.06790650259588182, "compression_ratio": 1.7935779816513762, "no_speech_prob": 0.002103684935718775}, {"id": 1650, "seek": 964804, "start": 9654.04, "end": 9661.960000000001, "text": " right, so if I look at the output of the neural network at node i and I examine how it depends", "tokens": [50664, 558, 11, 370, 498, 286, 574, 412, 264, 5598, 295, 264, 18161, 3209, 412, 9984, 741, 293, 286, 17496, 577, 309, 5946, 51060], "temperature": 0.0, "avg_logprob": -0.06790650259588182, "compression_ratio": 1.7935779816513762, "no_speech_prob": 0.002103684935718775}, {"id": 1651, "seek": 964804, "start": 9661.960000000001, "end": 9668.52, "text": " on the input at some distant node j, I can describe the sensitivity of the output to the input", "tokens": [51060, 322, 264, 4846, 412, 512, 17275, 9984, 361, 11, 286, 393, 6786, 264, 19392, 295, 264, 5598, 281, 264, 4846, 51388], "temperature": 0.0, "avg_logprob": -0.06790650259588182, "compression_ratio": 1.7935779816513762, "no_speech_prob": 0.002103684935718775}, {"id": 1652, "seek": 964804, "start": 9669.640000000001, "end": 9674.84, "text": " through this Jacobian, right, so through the partial derivative, and if the partial derivative", "tokens": [51444, 807, 341, 14117, 952, 11, 558, 11, 370, 807, 264, 14641, 13760, 11, 293, 498, 264, 14641, 13760, 51704], "temperature": 0.0, "avg_logprob": -0.06790650259588182, "compression_ratio": 1.7935779816513762, "no_speech_prob": 0.002103684935718775}, {"id": 1653, "seek": 967484, "start": 9674.84, "end": 9681.8, "text": " is small it means that the information propagates badly from input to output, right, so basically", "tokens": [50364, 307, 1359, 309, 1355, 300, 264, 1589, 12425, 1024, 13425, 490, 4846, 281, 5598, 11, 558, 11, 370, 1936, 50712], "temperature": 0.0, "avg_logprob": -0.0794022315371353, "compression_ratio": 1.858267716535433, "no_speech_prob": 0.001813556533306837}, {"id": 1654, "seek": 967484, "start": 9681.8, "end": 9688.52, "text": " I will not perceive the change in the input in the output of that node, and what we show in the", "tokens": [50712, 286, 486, 406, 20281, 264, 1319, 294, 264, 4846, 294, 264, 5598, 295, 300, 9984, 11, 293, 437, 321, 855, 294, 264, 51048], "temperature": 0.0, "avg_logprob": -0.0794022315371353, "compression_ratio": 1.858267716535433, "no_speech_prob": 0.001813556533306837}, {"id": 1655, "seek": 967484, "start": 9688.52, "end": 9694.12, "text": " paper is that we can bound the Jacobian by constants that depend on the model, right, for", "tokens": [51048, 3035, 307, 300, 321, 393, 5472, 264, 14117, 952, 538, 35870, 300, 5672, 322, 264, 2316, 11, 558, 11, 337, 51328], "temperature": 0.0, "avg_logprob": -0.0794022315371353, "compression_ratio": 1.858267716535433, "no_speech_prob": 0.001813556533306837}, {"id": 1656, "seek": 967484, "start": 9694.12, "end": 9698.92, "text": " example the number of flares, the regularity of the activation functions, the bound on the weights,", "tokens": [51328, 1365, 264, 1230, 295, 283, 19415, 11, 264, 3890, 507, 295, 264, 24433, 6828, 11, 264, 5472, 322, 264, 17443, 11, 51568], "temperature": 0.0, "avg_logprob": -0.0794022315371353, "compression_ratio": 1.858267716535433, "no_speech_prob": 0.001813556533306837}, {"id": 1657, "seek": 967484, "start": 9698.92, "end": 9703.48, "text": " and also something that depends on the graph topology, right, and we show in particular,", "tokens": [51568, 293, 611, 746, 300, 5946, 322, 264, 4295, 1192, 1793, 11, 558, 11, 293, 321, 855, 294, 1729, 11, 51796], "temperature": 0.0, "avg_logprob": -0.0794022315371353, "compression_ratio": 1.858267716535433, "no_speech_prob": 0.001813556533306837}, {"id": 1658, "seek": 970348, "start": 9703.56, "end": 9707.16, "text": " for example, that width does help, of course, at the usual expense of", "tokens": [50368, 337, 1365, 11, 300, 11402, 775, 854, 11, 295, 1164, 11, 412, 264, 7713, 18406, 295, 50548], "temperature": 0.0, "avg_logprob": -0.13569372350519354, "compression_ratio": 1.8170212765957447, "no_speech_prob": 0.0038655344396829605}, {"id": 1659, "seek": 970348, "start": 9710.279999999999, "end": 9716.92, "text": " worst generalization overfitting, depth doesn't help, for example, and the topology has the", "tokens": [50704, 5855, 2674, 2144, 670, 69, 2414, 11, 7161, 1177, 380, 854, 11, 337, 1365, 11, 293, 264, 1192, 1793, 575, 264, 51036], "temperature": 0.0, "avg_logprob": -0.13569372350519354, "compression_ratio": 1.8170212765957447, "no_speech_prob": 0.0038655344396829605}, {"id": 1660, "seek": 970348, "start": 9716.92, "end": 9722.119999999999, "text": " really the largest effect, and intuitively we expect that in some kind of benign graphs,", "tokens": [51036, 534, 264, 6443, 1802, 11, 293, 46506, 321, 2066, 300, 294, 512, 733, 295, 3271, 788, 24877, 11, 51296], "temperature": 0.0, "avg_logprob": -0.13569372350519354, "compression_ratio": 1.8170212765957447, "no_speech_prob": 0.0038655344396829605}, {"id": 1661, "seek": 970348, "start": 9722.119999999999, "end": 9726.52, "text": " like grids, for example, we will not have overscorching, and in pathological examples,", "tokens": [51296, 411, 677, 3742, 11, 337, 1365, 11, 321, 486, 406, 362, 670, 4417, 284, 17354, 11, 293, 294, 3100, 4383, 5110, 11, 51516], "temperature": 0.0, "avg_logprob": -0.13569372350519354, "compression_ratio": 1.8170212765957447, "no_speech_prob": 0.0038655344396829605}, {"id": 1662, "seek": 970348, "start": 9726.52, "end": 9732.039999999999, "text": " like trees, that would be probably the worst case, right, so you see that the topology of", "tokens": [51516, 411, 5852, 11, 300, 576, 312, 1391, 264, 5855, 1389, 11, 558, 11, 370, 291, 536, 300, 264, 1192, 1793, 295, 51792], "temperature": 0.0, "avg_logprob": -0.13569372350519354, "compression_ratio": 1.8170212765957447, "no_speech_prob": 0.0038655344396829605}, {"id": 1663, "seek": 973204, "start": 9732.04, "end": 9738.92, "text": " the graph comes here through the power of the adjacency matrix, but we don't see exactly how,", "tokens": [50364, 264, 4295, 1487, 510, 807, 264, 1347, 295, 264, 22940, 3020, 8141, 11, 457, 321, 500, 380, 536, 2293, 577, 11, 50708], "temperature": 0.0, "avg_logprob": -0.08519331340132089, "compression_ratio": 1.927710843373494, "no_speech_prob": 0.0016841966426}, {"id": 1664, "seek": 973204, "start": 9738.92, "end": 9745.960000000001, "text": " right, so it's hard to say, right, so what does it mean a matrix to the power l, so we need something", "tokens": [50708, 558, 11, 370, 309, 311, 1152, 281, 584, 11, 558, 11, 370, 437, 775, 309, 914, 257, 8141, 281, 264, 1347, 287, 11, 370, 321, 643, 746, 51060], "temperature": 0.0, "avg_logprob": -0.08519331340132089, "compression_ratio": 1.927710843373494, "no_speech_prob": 0.0016841966426}, {"id": 1665, "seek": 973204, "start": 9745.960000000001, "end": 9751.320000000002, "text": " more nuanced, we need some kind of geometric analysis that will allow us to tell apart structures", "tokens": [51060, 544, 45115, 11, 321, 643, 512, 733, 295, 33246, 5215, 300, 486, 2089, 505, 281, 980, 4936, 9227, 51328], "temperature": 0.0, "avg_logprob": -0.08519331340132089, "compression_ratio": 1.927710843373494, "no_speech_prob": 0.0016841966426}, {"id": 1666, "seek": 973204, "start": 9751.320000000002, "end": 9754.44, "text": " like this and structures like this, right, something that locally looks like a grid or", "tokens": [51328, 411, 341, 293, 9227, 411, 341, 11, 558, 11, 746, 300, 16143, 1542, 411, 257, 10748, 420, 51484], "temperature": 0.0, "avg_logprob": -0.08519331340132089, "compression_ratio": 1.927710843373494, "no_speech_prob": 0.0016841966426}, {"id": 1667, "seek": 973204, "start": 9754.44, "end": 9759.0, "text": " something that looks like a tree, that's exactly what curvature is designed for, right, so I remind", "tokens": [51484, 746, 300, 1542, 411, 257, 4230, 11, 300, 311, 2293, 437, 37638, 307, 4761, 337, 11, 558, 11, 370, 286, 4160, 51712], "temperature": 0.0, "avg_logprob": -0.08519331340132089, "compression_ratio": 1.927710843373494, "no_speech_prob": 0.0016841966426}, {"id": 1668, "seek": 975900, "start": 9759.08, "end": 9764.52, "text": " you that in differential geometry, what curvature tells you is that if you take nearby points and", "tokens": [50368, 291, 300, 294, 15756, 18426, 11, 437, 37638, 5112, 291, 307, 300, 498, 291, 747, 11184, 2793, 293, 50640], "temperature": 0.0, "avg_logprob": -0.11442139434814454, "compression_ratio": 1.8191881918819188, "no_speech_prob": 0.006506652105599642}, {"id": 1669, "seek": 975900, "start": 9764.52, "end": 9770.68, "text": " shoot geodesics in parallel at the same speed, you can either converge, remain parallel, or diverge,", "tokens": [50640, 3076, 1519, 4789, 1167, 294, 8952, 412, 264, 912, 3073, 11, 291, 393, 2139, 41881, 11, 6222, 8952, 11, 420, 18558, 432, 11, 50948], "temperature": 0.0, "avg_logprob": -0.11442139434814454, "compression_ratio": 1.8191881918819188, "no_speech_prob": 0.006506652105599642}, {"id": 1670, "seek": 975900, "start": 9770.68, "end": 9775.64, "text": " right, and we call these spherical, euclidean, and hyperbolic geometry, right, so locally it looks", "tokens": [50948, 558, 11, 293, 321, 818, 613, 37300, 11, 308, 1311, 31264, 282, 11, 293, 9848, 65, 7940, 18426, 11, 558, 11, 370, 16143, 309, 1542, 51196], "temperature": 0.0, "avg_logprob": -0.11442139434814454, "compression_ratio": 1.8191881918819188, "no_speech_prob": 0.006506652105599642}, {"id": 1671, "seek": 975900, "start": 9775.64, "end": 9781.72, "text": " like a sphere, like a plane, or like a hyperboloid, or high-dimensional cases as well, so on a graph,", "tokens": [51196, 411, 257, 16687, 11, 411, 257, 5720, 11, 420, 411, 257, 9848, 65, 7902, 327, 11, 420, 1090, 12, 18759, 3331, 382, 731, 11, 370, 322, 257, 4295, 11, 51500], "temperature": 0.0, "avg_logprob": -0.11442139434814454, "compression_ratio": 1.8191881918819188, "no_speech_prob": 0.006506652105599642}, {"id": 1672, "seek": 975900, "start": 9781.72, "end": 9786.44, "text": " the analogy could look like this, so there are several definitions of reach-type curvature on", "tokens": [51500, 264, 21663, 727, 574, 411, 341, 11, 370, 456, 366, 2940, 21988, 295, 2524, 12, 20467, 37638, 322, 51736], "temperature": 0.0, "avg_logprob": -0.11442139434814454, "compression_ratio": 1.8191881918819188, "no_speech_prob": 0.006506652105599642}, {"id": 1673, "seek": 978644, "start": 9786.44, "end": 9792.84, "text": " graphs, so this is a combinatorial definition that we use here, so you can take nodes that are", "tokens": [50364, 24877, 11, 370, 341, 307, 257, 2512, 31927, 831, 7123, 300, 321, 764, 510, 11, 370, 291, 393, 747, 13891, 300, 366, 50684], "temperature": 0.0, "avg_logprob": -0.06682230786579411, "compression_ratio": 1.9516129032258065, "no_speech_prob": 0.0014665144262835383}, {"id": 1674, "seek": 978644, "start": 9792.84, "end": 9798.12, "text": " connected by an edge, let's call them p and q, and look at edges that emanate from these nodes,", "tokens": [50684, 4582, 538, 364, 4691, 11, 718, 311, 818, 552, 280, 293, 9505, 11, 293, 574, 412, 8819, 300, 28211, 473, 490, 613, 13891, 11, 50948], "temperature": 0.0, "avg_logprob": -0.06682230786579411, "compression_ratio": 1.9516129032258065, "no_speech_prob": 0.0014665144262835383}, {"id": 1675, "seek": 978644, "start": 9798.12, "end": 9804.68, "text": " so if they tend to form triangles, it means that we look at something like a click, if they form", "tokens": [50948, 370, 498, 436, 3928, 281, 1254, 29896, 11, 309, 1355, 300, 321, 574, 412, 746, 411, 257, 2052, 11, 498, 436, 1254, 51276], "temperature": 0.0, "avg_logprob": -0.06682230786579411, "compression_ratio": 1.9516129032258065, "no_speech_prob": 0.0014665144262835383}, {"id": 1676, "seek": 978644, "start": 9804.68, "end": 9809.960000000001, "text": " rectangles, they will look at something like a grid, and if they drift apart and don't form anything,", "tokens": [51276, 24077, 904, 11, 436, 486, 574, 412, 746, 411, 257, 10748, 11, 293, 498, 436, 19699, 4936, 293, 500, 380, 1254, 1340, 11, 51540], "temperature": 0.0, "avg_logprob": -0.06682230786579411, "compression_ratio": 1.9516129032258065, "no_speech_prob": 0.0014665144262835383}, {"id": 1677, "seek": 978644, "start": 9809.960000000001, "end": 9815.08, "text": " then we look at locally at something that looks like a tree, right, and basically we can count", "tokens": [51540, 550, 321, 574, 412, 16143, 412, 746, 300, 1542, 411, 257, 4230, 11, 558, 11, 293, 1936, 321, 393, 1207, 51796], "temperature": 0.0, "avg_logprob": -0.06682230786579411, "compression_ratio": 1.9516129032258065, "no_speech_prob": 0.0014665144262835383}, {"id": 1678, "seek": 981508, "start": 9815.16, "end": 9820.039999999999, "text": " different types of rectangles and triangles, allow me to skip the details, basically for every", "tokens": [50368, 819, 3467, 295, 24077, 904, 293, 29896, 11, 2089, 385, 281, 10023, 264, 4365, 11, 1936, 337, 633, 50612], "temperature": 0.0, "avg_logprob": -0.0969424114049038, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.002106190426275134}, {"id": 1679, "seek": 981508, "start": 9820.039999999999, "end": 9823.96, "text": " edge in the graph we can have this combinatorial quantity that we call the balanced formant", "tokens": [50612, 4691, 294, 264, 4295, 321, 393, 362, 341, 2512, 31927, 831, 11275, 300, 321, 818, 264, 13902, 1254, 394, 50808], "temperature": 0.0, "avg_logprob": -0.0969424114049038, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.002106190426275134}, {"id": 1680, "seek": 981508, "start": 9823.96, "end": 9830.039999999999, "text": " curvature, that counts, basically it looks at a two-hop neighborhood of an edge, and it counts", "tokens": [50808, 37638, 11, 300, 14893, 11, 1936, 309, 1542, 412, 257, 732, 12, 9050, 7630, 295, 364, 4691, 11, 293, 309, 14893, 51112], "temperature": 0.0, "avg_logprob": -0.0969424114049038, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.002106190426275134}, {"id": 1681, "seek": 981508, "start": 9830.039999999999, "end": 9835.32, "text": " certain types of rectangles and triangles that surround this edge, bottom line, each reproduces", "tokens": [51112, 1629, 3467, 295, 24077, 904, 293, 29896, 300, 6262, 341, 4691, 11, 2767, 1622, 11, 1184, 11408, 887, 51376], "temperature": 0.0, "avg_logprob": -0.0969424114049038, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.002106190426275134}, {"id": 1682, "seek": 981508, "start": 9835.32, "end": 9840.2, "text": " the continuous behavior, so clicks are positively curved, grids have zero curvature, and trees are", "tokens": [51376, 264, 10957, 5223, 11, 370, 18521, 366, 25795, 24991, 11, 677, 3742, 362, 4018, 37638, 11, 293, 5852, 366, 51620], "temperature": 0.0, "avg_logprob": -0.0969424114049038, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.002106190426275134}, {"id": 1683, "seek": 984020, "start": 9840.2, "end": 9846.2, "text": " negatively curved, right, so that's I think to your previous question how what is the", "tokens": [50364, 29519, 24991, 11, 558, 11, 370, 300, 311, 286, 519, 281, 428, 3894, 1168, 577, 437, 307, 264, 50664], "temperature": 0.0, "avg_logprob": -0.11072881266755878, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.0043624225072562695}, {"id": 1684, "seek": 984020, "start": 9847.08, "end": 9851.0, "text": " parallel between differential geometry and graphs, so this is an analogy of a curvature,", "tokens": [50708, 8952, 1296, 15756, 18426, 293, 24877, 11, 370, 341, 307, 364, 21663, 295, 257, 37638, 11, 50904], "temperature": 0.0, "avg_logprob": -0.11072881266755878, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.0043624225072562695}, {"id": 1685, "seek": 984020, "start": 9851.0, "end": 9855.320000000002, "text": " so it's not a discretization of a curvature, it's a discrete curvature that behaves in a", "tokens": [50904, 370, 309, 311, 406, 257, 25656, 2144, 295, 257, 37638, 11, 309, 311, 257, 27706, 37638, 300, 36896, 294, 257, 51120], "temperature": 0.0, "avg_logprob": -0.11072881266755878, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.0043624225072562695}, {"id": 1686, "seek": 984020, "start": 9855.320000000002, "end": 9861.0, "text": " similar way, and now the relation between the over-squashing and the curvature, what we show", "tokens": [51120, 2531, 636, 11, 293, 586, 264, 9721, 1296, 264, 670, 12, 33292, 11077, 293, 264, 37638, 11, 437, 321, 855, 51404], "temperature": 0.0, "avg_logprob": -0.11072881266755878, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.0043624225072562695}, {"id": 1687, "seek": 984020, "start": 9861.0, "end": 9867.16, "text": " is that if we have strongly negatively curved edges in the in the graph, then we can write down", "tokens": [51404, 307, 300, 498, 321, 362, 10613, 29519, 24991, 8819, 294, 264, 294, 264, 4295, 11, 550, 321, 393, 2464, 760, 51712], "temperature": 0.0, "avg_logprob": -0.11072881266755878, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.0043624225072562695}, {"id": 1688, "seek": 986716, "start": 9867.24, "end": 9873.08, "text": " this bound on the Jacobian, and it means that the over-squashing is caused by", "tokens": [50368, 341, 5472, 322, 264, 14117, 952, 11, 293, 309, 1355, 300, 264, 670, 12, 33292, 11077, 307, 7008, 538, 50660], "temperature": 0.0, "avg_logprob": -0.12011445926714547, "compression_ratio": 1.625, "no_speech_prob": 0.0028534824959933758}, {"id": 1689, "seek": 986716, "start": 9874.2, "end": 9876.6, "text": " the presence of strongly negatively curved edges, yeah.", "tokens": [50716, 264, 6814, 295, 10613, 29519, 24991, 8819, 11, 1338, 13, 50836], "temperature": 0.0, "avg_logprob": -0.12011445926714547, "compression_ratio": 1.625, "no_speech_prob": 0.0028534824959933758}, {"id": 1690, "seek": 986716, "start": 9879.88, "end": 9889.88, "text": " Yeah, so it's the number of triangles that surround an edge, and this is the number of", "tokens": [51000, 865, 11, 370, 309, 311, 264, 1230, 295, 29896, 300, 6262, 364, 4691, 11, 293, 341, 307, 264, 1230, 295, 51500], "temperature": 0.0, "avg_logprob": -0.12011445926714547, "compression_ratio": 1.625, "no_speech_prob": 0.0028534824959933758}, {"id": 1691, "seek": 986716, "start": 9889.88, "end": 9895.96, "text": " rectangles, this is the degree, yeah, doesn't really matter, there are several definitions,", "tokens": [51500, 24077, 904, 11, 341, 307, 264, 4314, 11, 1338, 11, 1177, 380, 534, 1871, 11, 456, 366, 2940, 21988, 11, 51804], "temperature": 0.0, "avg_logprob": -0.12011445926714547, "compression_ratio": 1.625, "no_speech_prob": 0.0028534824959933758}, {"id": 1692, "seek": 989596, "start": 9895.96, "end": 9900.599999999999, "text": " so why we call it balanced form and curvature, because there is a classical notion of form", "tokens": [50364, 370, 983, 321, 818, 309, 13902, 1254, 293, 37638, 11, 570, 456, 307, 257, 13735, 10710, 295, 1254, 50596], "temperature": 0.0, "avg_logprob": -0.1460744063059489, "compression_ratio": 1.7427536231884058, "no_speech_prob": 0.005473046097904444}, {"id": 1693, "seek": 989596, "start": 9900.599999999999, "end": 9906.839999999998, "text": " and curvature that looks a little bit like this, we just touch it a little bit so it behaves like", "tokens": [50596, 293, 37638, 300, 1542, 257, 707, 857, 411, 341, 11, 321, 445, 2557, 309, 257, 707, 857, 370, 309, 36896, 411, 50908], "temperature": 0.0, "avg_logprob": -0.1460744063059489, "compression_ratio": 1.7427536231884058, "no_speech_prob": 0.005473046097904444}, {"id": 1694, "seek": 989596, "start": 9906.839999999998, "end": 9913.48, "text": " what is shown here. This really relates to the rigid curvature tensor and the formula from", "tokens": [50908, 437, 307, 4898, 510, 13, 639, 534, 16155, 281, 264, 22195, 37638, 40863, 293, 264, 8513, 490, 51240], "temperature": 0.0, "avg_logprob": -0.1460744063059489, "compression_ratio": 1.7427536231884058, "no_speech_prob": 0.005473046097904444}, {"id": 1695, "seek": 989596, "start": 9913.48, "end": 9918.279999999999, "text": " matrix, I mean I don't see it now, but maybe. It's a graph, so you don't have exactly the same thing.", "tokens": [51240, 8141, 11, 286, 914, 286, 500, 380, 536, 309, 586, 11, 457, 1310, 13, 467, 311, 257, 4295, 11, 370, 291, 500, 380, 362, 2293, 264, 912, 551, 13, 51480], "temperature": 0.0, "avg_logprob": -0.1460744063059489, "compression_ratio": 1.7427536231884058, "no_speech_prob": 0.005473046097904444}, {"id": 1696, "seek": 989596, "start": 9918.279999999999, "end": 9924.199999999999, "text": " Yeah, obviously, but some I don't know components, can we do some analogy or not, this is something", "tokens": [51480, 865, 11, 2745, 11, 457, 512, 286, 500, 380, 458, 6677, 11, 393, 321, 360, 512, 21663, 420, 406, 11, 341, 307, 746, 51776], "temperature": 0.0, "avg_logprob": -0.1460744063059489, "compression_ratio": 1.7427536231884058, "no_speech_prob": 0.005473046097904444}, {"id": 1697, "seek": 992420, "start": 9924.2, "end": 9931.560000000001, "text": " completely new. So it shows how, so you can think of curvature as how locally the volume changes,", "tokens": [50364, 2584, 777, 13, 407, 309, 3110, 577, 11, 370, 291, 393, 519, 295, 37638, 382, 577, 16143, 264, 5523, 2962, 11, 50732], "temperature": 0.0, "avg_logprob": -0.1200134681932854, "compression_ratio": 1.8132780082987552, "no_speech_prob": 0.002152884379029274}, {"id": 1698, "seek": 992420, "start": 9931.560000000001, "end": 9936.2, "text": " so in a sense it shows how the volume changes, so there are two classical definitions of", "tokens": [50732, 370, 294, 257, 2020, 309, 3110, 577, 264, 5523, 2962, 11, 370, 456, 366, 732, 13735, 21988, 295, 50964], "temperature": 0.0, "avg_logprob": -0.1200134681932854, "compression_ratio": 1.8132780082987552, "no_speech_prob": 0.002152884379029274}, {"id": 1699, "seek": 992420, "start": 9936.2, "end": 9939.960000000001, "text": " curvature on graphs, one through optimal transport that is called the Olivia curvature,", "tokens": [50964, 37638, 322, 24877, 11, 472, 807, 16252, 5495, 300, 307, 1219, 264, 26023, 37638, 11, 51152], "temperature": 0.0, "avg_logprob": -0.1200134681932854, "compression_ratio": 1.8132780082987552, "no_speech_prob": 0.002152884379029274}, {"id": 1700, "seek": 992420, "start": 9939.960000000001, "end": 9943.320000000002, "text": " and this is combinatorial version that is called the the form and curvature.", "tokens": [51152, 293, 341, 307, 2512, 31927, 831, 3037, 300, 307, 1219, 264, 264, 1254, 293, 37638, 13, 51320], "temperature": 0.0, "avg_logprob": -0.1200134681932854, "compression_ratio": 1.8132780082987552, "no_speech_prob": 0.002152884379029274}, {"id": 1701, "seek": 992420, "start": 9946.68, "end": 9951.480000000001, "text": " Right, so basically the conclusion, well I should make it explicit that it's strongly", "tokens": [51488, 1779, 11, 370, 1936, 264, 10063, 11, 731, 286, 820, 652, 309, 13691, 300, 309, 311, 10613, 51728], "temperature": 0.0, "avg_logprob": -0.1200134681932854, "compression_ratio": 1.8132780082987552, "no_speech_prob": 0.002152884379029274}, {"id": 1702, "seek": 995148, "start": 9951.48, "end": 9956.119999999999, "text": " negatively curved edges that cause overscorching, right, so basically due to this bound, actually", "tokens": [50364, 29519, 24991, 8819, 300, 3082, 670, 4417, 284, 17354, 11, 558, 11, 370, 1936, 3462, 281, 341, 5472, 11, 767, 50596], "temperature": 0.0, "avg_logprob": -0.19339969032689144, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.0055487509816884995}, {"id": 1703, "seek": 995148, "start": 9956.119999999999, "end": 9961.48, "text": " the presence of negative or slightly negative curvature might be benign, this is what is shown", "tokens": [50596, 264, 6814, 295, 3671, 420, 4748, 3671, 37638, 1062, 312, 3271, 788, 11, 341, 307, 437, 307, 4898, 50864], "temperature": 0.0, "avg_logprob": -0.19339969032689144, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.0055487509816884995}, {"id": 1704, "seek": 995148, "start": 9962.119999999999, "end": 9967.96, "text": " in the the expander's paper by Petr Wieliczkiewicz and his and his causes, so these are somehow the", "tokens": [50896, 294, 264, 264, 1278, 4483, 311, 3035, 538, 10472, 81, 343, 1187, 17946, 74, 1093, 17946, 293, 702, 293, 702, 7700, 11, 370, 613, 366, 6063, 264, 51188], "temperature": 0.0, "avg_logprob": -0.19339969032689144, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.0055487509816884995}, {"id": 1705, "seek": 995148, "start": 9967.96, "end": 9974.279999999999, "text": " optimal graphs, the best for message passing, but expander's needs to be slightly negatively curved.", "tokens": [51188, 16252, 24877, 11, 264, 1151, 337, 3636, 8437, 11, 457, 1278, 4483, 311, 2203, 281, 312, 4748, 29519, 24991, 13, 51504], "temperature": 0.0, "avg_logprob": -0.19339969032689144, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.0055487509816884995}, {"id": 1706, "seek": 997428, "start": 9974.28, "end": 9981.480000000001, "text": " So once we know it, we can actually interfere basically, we can surgically remove the", "tokens": [50364, 407, 1564, 321, 458, 309, 11, 321, 393, 767, 23946, 1936, 11, 321, 393, 19560, 984, 4159, 264, 50724], "temperature": 0.0, "avg_logprob": -0.1840362734007604, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.004283999092876911}, {"id": 1707, "seek": 997428, "start": 9982.12, "end": 9987.08, "text": " negatively curved edges and replace them potentially with edges with higher, with more", "tokens": [50756, 29519, 24991, 8819, 293, 7406, 552, 7263, 365, 8819, 365, 2946, 11, 365, 544, 51004], "temperature": 0.0, "avg_logprob": -0.1840362734007604, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.004283999092876911}, {"id": 1708, "seek": 997428, "start": 9987.08, "end": 9993.560000000001, "text": " positive curvature, and this way we retouch the graph a little bit and we show that it improves", "tokens": [51004, 3353, 37638, 11, 293, 341, 636, 321, 1533, 2220, 264, 4295, 257, 707, 857, 293, 321, 855, 300, 309, 24771, 51328], "temperature": 0.0, "avg_logprob": -0.1840362734007604, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.004283999092876911}, {"id": 1709, "seek": 997428, "start": 9993.560000000001, "end": 9997.08, "text": " the performance of graph neural networks both in homophilic and heterophilic settings.", "tokens": [51328, 264, 3389, 295, 4295, 18161, 9590, 1293, 294, 3655, 5317, 388, 299, 293, 20789, 5317, 388, 299, 6257, 13, 51504], "temperature": 0.0, "avg_logprob": -0.1840362734007604, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.004283999092876911}, {"id": 1710, "seek": 997428, "start": 9997.640000000001, "end": 10003.560000000001, "text": " So there was a question about diffusion-based rewiring before, and I promised to tell exactly", "tokens": [51532, 407, 456, 390, 257, 1168, 466, 25242, 12, 6032, 319, 86, 5057, 949, 11, 293, 286, 10768, 281, 980, 2293, 51828], "temperature": 0.0, "avg_logprob": -0.1840362734007604, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.004283999092876911}, {"id": 1711, "seek": 1000356, "start": 10003.56, "end": 10012.279999999999, "text": " what I mean by this, so this is the paper that is called DIGL by Stefan G\u00fcnemann and his students", "tokens": [50364, 437, 286, 914, 538, 341, 11, 370, 341, 307, 264, 3035, 300, 307, 1219, 413, 10489, 43, 538, 32158, 50225, 443, 969, 293, 702, 1731, 50800], "temperature": 0.0, "avg_logprob": -0.1325272530624547, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.003533752402290702}, {"id": 1712, "seek": 1000356, "start": 10012.279999999999, "end": 10016.199999999999, "text": " from the Technical University of Munich, and DIGL stands for Diffusion Improves Graph Learning.", "tokens": [50800, 490, 264, 35512, 3535, 295, 40601, 11, 293, 413, 10489, 43, 7382, 337, 413, 3661, 5704, 8270, 340, 977, 21884, 15205, 13, 50996], "temperature": 0.0, "avg_logprob": -0.1325272530624547, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.003533752402290702}, {"id": 1713, "seek": 1000356, "start": 10016.76, "end": 10022.92, "text": " So the idea there is that you rewire the graph by basically by computing page rank and embeddings", "tokens": [51024, 407, 264, 1558, 456, 307, 300, 291, 319, 42689, 264, 4295, 538, 1936, 538, 15866, 3028, 6181, 293, 12240, 29432, 51332], "temperature": 0.0, "avg_logprob": -0.1325272530624547, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.003533752402290702}, {"id": 1714, "seek": 1000356, "start": 10022.92, "end": 10028.519999999999, "text": " for a personalized page rank embedding for every node, and you connect then in this new embedding", "tokens": [51332, 337, 257, 28415, 3028, 6181, 12240, 3584, 337, 633, 9984, 11, 293, 291, 1745, 550, 294, 341, 777, 12240, 3584, 51612], "temperature": 0.0, "avg_logprob": -0.1325272530624547, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.003533752402290702}, {"id": 1715, "seek": 1002852, "start": 10028.52, "end": 10035.640000000001, "text": " space the nodes that are closest. So what it does essentially, it introduces connections within the", "tokens": [50364, 1901, 264, 13891, 300, 366, 13699, 13, 407, 437, 309, 775, 4476, 11, 309, 31472, 9271, 1951, 264, 50720], "temperature": 0.0, "avg_logprob": -0.13875929875807327, "compression_ratio": 1.8267326732673268, "no_speech_prob": 0.020496420562267303}, {"id": 1716, "seek": 1002852, "start": 10035.640000000001, "end": 10041.32, "text": " same connected component in the graph right, or within the same clique or cluster in the graph,", "tokens": [50720, 912, 4582, 6542, 294, 264, 4295, 558, 11, 420, 1951, 264, 912, 44467, 420, 13630, 294, 264, 4295, 11, 51004], "temperature": 0.0, "avg_logprob": -0.13875929875807327, "compression_ratio": 1.8267326732673268, "no_speech_prob": 0.020496420562267303}, {"id": 1717, "seek": 1002852, "start": 10041.32, "end": 10047.800000000001, "text": " and it has a hard time to connect across different communities in the graph.", "tokens": [51004, 293, 309, 575, 257, 1152, 565, 281, 1745, 2108, 819, 4456, 294, 264, 4295, 13, 51328], "temperature": 0.0, "avg_logprob": -0.13875929875807327, "compression_ratio": 1.8267326732673268, "no_speech_prob": 0.020496420562267303}, {"id": 1718, "seek": 1002852, "start": 10048.68, "end": 10052.84, "text": " So when the graph is homophilic, this is a very good thing to do, right, so you're connecting to", "tokens": [51372, 407, 562, 264, 4295, 307, 3655, 5317, 388, 299, 11, 341, 307, 257, 588, 665, 551, 281, 360, 11, 558, 11, 370, 291, 434, 11015, 281, 51580], "temperature": 0.0, "avg_logprob": -0.13875929875807327, "compression_ratio": 1.8267326732673268, "no_speech_prob": 0.020496420562267303}, {"id": 1719, "seek": 1005284, "start": 10053.72, "end": 10058.2, "text": " to similar nodes, but if the graph is heterophilic, it can do more harm than help,", "tokens": [50408, 281, 2531, 13891, 11, 457, 498, 264, 4295, 307, 20789, 5317, 388, 299, 11, 309, 393, 360, 544, 6491, 813, 854, 11, 50632], "temperature": 0.0, "avg_logprob": -0.10774225573385915, "compression_ratio": 1.797872340425532, "no_speech_prob": 0.004776585381478071}, {"id": 1720, "seek": 1005284, "start": 10058.2, "end": 10063.16, "text": " and in fact experiments show that this is the case, they also write it in the paper,", "tokens": [50632, 293, 294, 1186, 12050, 855, 300, 341, 307, 264, 1389, 11, 436, 611, 2464, 309, 294, 264, 3035, 11, 50880], "temperature": 0.0, "avg_logprob": -0.10774225573385915, "compression_ratio": 1.797872340425532, "no_speech_prob": 0.004776585381478071}, {"id": 1721, "seek": 1005284, "start": 10063.16, "end": 10066.2, "text": " the curvature-based approach, first of all it changes the graph minimally,", "tokens": [50880, 264, 37638, 12, 6032, 3109, 11, 700, 295, 439, 309, 2962, 264, 4295, 4464, 379, 11, 51032], "temperature": 0.0, "avg_logprob": -0.10774225573385915, "compression_ratio": 1.797872340425532, "no_speech_prob": 0.004776585381478071}, {"id": 1722, "seek": 1005284, "start": 10067.32, "end": 10070.92, "text": " here the change can be dramatic, right, so that's the number of edges that are changed,", "tokens": [51088, 510, 264, 1319, 393, 312, 12023, 11, 558, 11, 370, 300, 311, 264, 1230, 295, 8819, 300, 366, 3105, 11, 51268], "temperature": 0.0, "avg_logprob": -0.10774225573385915, "compression_ratio": 1.797872340425532, "no_speech_prob": 0.004776585381478071}, {"id": 1723, "seek": 1005284, "start": 10071.8, "end": 10076.6, "text": " but it also helps in the heterophilic cases because it's not restricted by this property,", "tokens": [51312, 457, 309, 611, 3665, 294, 264, 20789, 5317, 388, 299, 3331, 570, 309, 311, 406, 20608, 538, 341, 4707, 11, 51552], "temperature": 0.0, "avg_logprob": -0.10774225573385915, "compression_ratio": 1.797872340425532, "no_speech_prob": 0.004776585381478071}, {"id": 1724, "seek": 1005284, "start": 10076.6, "end": 10081.24, "text": " you actually typically bridge different communities by the new edges that are created.", "tokens": [51552, 291, 767, 5850, 7283, 819, 4456, 538, 264, 777, 8819, 300, 366, 2942, 13, 51784], "temperature": 0.0, "avg_logprob": -0.10774225573385915, "compression_ratio": 1.797872340425532, "no_speech_prob": 0.004776585381478071}, {"id": 1725, "seek": 1008284, "start": 10083.16, "end": 10087.48, "text": " So still talking about diffusion, am I actually out of time?", "tokens": [50380, 407, 920, 1417, 466, 25242, 11, 669, 286, 767, 484, 295, 565, 30, 50596], "temperature": 0.0, "avg_logprob": -0.22095779010227748, "compression_ratio": 1.6, "no_speech_prob": 0.001674886210821569}, {"id": 1726, "seek": 1008284, "start": 10089.960000000001, "end": 10097.4, "text": " Okay, yeah, so still talking about about diffusion equations, here are some more exotic", "tokens": [50720, 1033, 11, 1338, 11, 370, 920, 1417, 466, 466, 25242, 11787, 11, 510, 366, 512, 544, 27063, 51092], "temperature": 0.0, "avg_logprob": -0.22095779010227748, "compression_ratio": 1.6, "no_speech_prob": 0.001674886210821569}, {"id": 1727, "seek": 1008284, "start": 10098.6, "end": 10106.6, "text": " exotic stuff, right, and this is our maybe creative way to illustrate to illustrate sheaves", "tokens": [51152, 27063, 1507, 11, 558, 11, 293, 341, 307, 527, 1310, 5880, 636, 281, 23221, 281, 23221, 750, 5423, 51552], "temperature": 0.0, "avg_logprob": -0.22095779010227748, "compression_ratio": 1.6, "no_speech_prob": 0.001674886210821569}, {"id": 1728, "seek": 1010660, "start": 10107.56, "end": 10114.84, "text": " or bundles, so there has recently been probably a better picture, so that's really sheaves,", "tokens": [50412, 420, 13882, 904, 11, 370, 456, 575, 3938, 668, 1391, 257, 1101, 3036, 11, 370, 300, 311, 534, 750, 5423, 11, 50776], "temperature": 0.0, "avg_logprob": -0.16234342675460012, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.019446734338998795}, {"id": 1729, "seek": 1010660, "start": 10114.84, "end": 10120.28, "text": " right, in the literal sense, so what are sheaves? So they actually have very interesting history,", "tokens": [50776, 558, 11, 294, 264, 20411, 2020, 11, 370, 437, 366, 750, 5423, 30, 407, 436, 767, 362, 588, 1880, 2503, 11, 51048], "temperature": 0.0, "avg_logprob": -0.16234342675460012, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.019446734338998795}, {"id": 1730, "seek": 1010660, "start": 10120.28, "end": 10126.52, "text": " and well I like these kind of historical factoids, so the theory of sheaves in algebraic topology", "tokens": [51048, 293, 731, 286, 411, 613, 733, 295, 8584, 42225, 3742, 11, 370, 264, 5261, 295, 750, 5423, 294, 21989, 299, 1192, 1793, 51360], "temperature": 0.0, "avg_logprob": -0.16234342675460012, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.019446734338998795}, {"id": 1731, "seek": 1010660, "start": 10126.52, "end": 10132.04, "text": " was introduced by Jean Lyret, so he was a French mathematician, he was also an officer in the", "tokens": [51360, 390, 7268, 538, 13854, 12687, 1505, 11, 370, 415, 390, 257, 5522, 48281, 11, 415, 390, 611, 364, 8456, 294, 264, 51636], "temperature": 0.0, "avg_logprob": -0.16234342675460012, "compression_ratio": 1.6422413793103448, "no_speech_prob": 0.019446734338998795}, {"id": 1732, "seek": 1013204, "start": 10132.04, "end": 10138.92, "text": " French army, and when the Nazis invaded France he was captured and put with his comrades into", "tokens": [50364, 5522, 7267, 11, 293, 562, 264, 29812, 35882, 6190, 415, 390, 11828, 293, 829, 365, 702, 42249, 666, 50708], "temperature": 0.0, "avg_logprob": -0.10822951656648483, "compression_ratio": 1.794392523364486, "no_speech_prob": 0.019890977069735527}, {"id": 1733, "seek": 1013204, "start": 10139.480000000001, "end": 10145.320000000002, "text": " concentration camp, and basically he was asked to work on mathematics, and his expertise was", "tokens": [50736, 9856, 2255, 11, 293, 1936, 415, 390, 2351, 281, 589, 322, 18666, 11, 293, 702, 11769, 390, 51028], "temperature": 0.0, "avg_logprob": -0.10822951656648483, "compression_ratio": 1.794392523364486, "no_speech_prob": 0.019890977069735527}, {"id": 1734, "seek": 1013204, "start": 10146.6, "end": 10151.160000000002, "text": " mechanics, so he was very afraid that he would be forced to work on something that would be useful", "tokens": [51092, 12939, 11, 370, 415, 390, 588, 4638, 300, 415, 576, 312, 7579, 281, 589, 322, 746, 300, 576, 312, 4420, 51320], "temperature": 0.0, "avg_logprob": -0.10822951656648483, "compression_ratio": 1.794392523364486, "no_speech_prob": 0.019890977069735527}, {"id": 1735, "seek": 1013204, "start": 10151.160000000002, "end": 10157.560000000001, "text": " for the Nazis, and basically he will be committing treason, helping the war effort, so when he was", "tokens": [51320, 337, 264, 29812, 11, 293, 1936, 415, 486, 312, 26659, 2192, 1258, 11, 4315, 264, 1516, 4630, 11, 370, 562, 415, 390, 51640], "temperature": 0.0, "avg_logprob": -0.10822951656648483, "compression_ratio": 1.794392523364486, "no_speech_prob": 0.019890977069735527}, {"id": 1736, "seek": 1015756, "start": 10157.56, "end": 10165.08, "text": " offered the possibility to teach something in this camp, he chose a very innocuous topic,", "tokens": [50364, 8059, 264, 7959, 281, 2924, 746, 294, 341, 2255, 11, 415, 5111, 257, 588, 10843, 12549, 4829, 11, 50740], "temperature": 0.0, "avg_logprob": -0.077961089875963, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.019285129383206367}, {"id": 1737, "seek": 1015756, "start": 10165.08, "end": 10171.24, "text": " algebraic topology, which could be useful, and then after, well of course they were released", "tokens": [50740, 21989, 299, 1192, 1793, 11, 597, 727, 312, 4420, 11, 293, 550, 934, 11, 731, 295, 1164, 436, 645, 4736, 51048], "temperature": 0.0, "avg_logprob": -0.077961089875963, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.019285129383206367}, {"id": 1738, "seek": 1015756, "start": 10171.24, "end": 10178.039999999999, "text": " after the war ended, he published it in a course that was taught in captivity, and one of the", "tokens": [51048, 934, 264, 1516, 4590, 11, 415, 6572, 309, 294, 257, 1164, 300, 390, 5928, 294, 48607, 11, 293, 472, 295, 264, 51388], "temperature": 0.0, "avg_logprob": -0.077961089875963, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.019285129383206367}, {"id": 1739, "seek": 1015756, "start": 10178.039999999999, "end": 10185.24, "text": " papers that came out of this course introduced the theory of sheaves, so sheaves are objects that are", "tokens": [51388, 10577, 300, 1361, 484, 295, 341, 1164, 7268, 264, 5261, 295, 750, 5423, 11, 370, 750, 5423, 366, 6565, 300, 366, 51748], "temperature": 0.0, "avg_logprob": -0.077961089875963, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.019285129383206367}, {"id": 1740, "seek": 1018524, "start": 10185.24, "end": 10192.119999999999, "text": " taught in algebraic topology, so if we apply them to our setting to graph, and this is slightly", "tokens": [50364, 5928, 294, 21989, 299, 1192, 1793, 11, 370, 498, 321, 3079, 552, 281, 527, 3287, 281, 4295, 11, 293, 341, 307, 4748, 50708], "temperature": 0.0, "avg_logprob": -0.12836448267886513, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.00855119526386261}, {"id": 1741, "seek": 1018524, "start": 10192.119999999999, "end": 10197.4, "text": " different construction that is called cellar sheaves, so if you think of graphs by analogy to", "tokens": [50708, 819, 6435, 300, 307, 1219, 2815, 289, 750, 5423, 11, 370, 498, 291, 519, 295, 24877, 538, 21663, 281, 50972], "temperature": 0.0, "avg_logprob": -0.12836448267886513, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.00855119526386261}, {"id": 1742, "seek": 1018524, "start": 10197.4, "end": 10202.44, "text": " manifold, so a manifold is a topological space, what I mean by topological space roughly is that", "tokens": [50972, 47138, 11, 370, 257, 47138, 307, 257, 1192, 4383, 1901, 11, 437, 286, 914, 538, 1192, 4383, 1901, 9810, 307, 300, 51224], "temperature": 0.0, "avg_logprob": -0.12836448267886513, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.00855119526386261}, {"id": 1743, "seek": 1018524, "start": 10202.44, "end": 10209.64, "text": " you have a notion of neighborhood, I can tell who my neighbors are, but I don't have the notion of", "tokens": [51224, 291, 362, 257, 10710, 295, 7630, 11, 286, 393, 980, 567, 452, 12512, 366, 11, 457, 286, 500, 380, 362, 264, 10710, 295, 51584], "temperature": 0.0, "avg_logprob": -0.12836448267886513, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.00855119526386261}, {"id": 1744, "seek": 1020964, "start": 10209.72, "end": 10217.32, "text": " distances or angles, so if I want to talk about distances or angles, I need some extra machinery,", "tokens": [50368, 22182, 420, 14708, 11, 370, 498, 286, 528, 281, 751, 466, 22182, 420, 14708, 11, 286, 643, 512, 2857, 27302, 11, 50748], "temperature": 0.0, "avg_logprob": -0.13224078159706265, "compression_ratio": 1.7411764705882353, "no_speech_prob": 0.01454977598041296}, {"id": 1745, "seek": 1020964, "start": 10217.32, "end": 10221.32, "text": " and on many folds this is typically achieved by what is called an affine connection,", "tokens": [50748, 293, 322, 867, 31341, 341, 307, 5850, 11042, 538, 437, 307, 1219, 364, 2096, 533, 4984, 11, 50948], "temperature": 0.0, "avg_logprob": -0.13224078159706265, "compression_ratio": 1.7411764705882353, "no_speech_prob": 0.01454977598041296}, {"id": 1746, "seek": 1020964, "start": 10222.519999999999, "end": 10228.199999999999, "text": " or parallel transport, so it's a mechanism that tells me how to move vectors between", "tokens": [51008, 420, 8952, 5495, 11, 370, 309, 311, 257, 7513, 300, 5112, 385, 577, 281, 1286, 18875, 1296, 51292], "temperature": 0.0, "avg_logprob": -0.13224078159706265, "compression_ratio": 1.7411764705882353, "no_speech_prob": 0.01454977598041296}, {"id": 1747, "seek": 1020964, "start": 10228.199999999999, "end": 10234.279999999999, "text": " tangent spaces at nearby points, I can also define a remaining metric if I want to equip", "tokens": [51292, 27747, 7673, 412, 11184, 2793, 11, 286, 393, 611, 6964, 257, 8877, 20678, 498, 286, 528, 281, 5037, 51596], "temperature": 0.0, "avg_logprob": -0.13224078159706265, "compression_ratio": 1.7411764705882353, "no_speech_prob": 0.01454977598041296}, {"id": 1748, "seek": 1020964, "start": 10234.279999999999, "end": 10238.359999999999, "text": " a manifold with geometry, and then there is a special type of connection that is called", "tokens": [51596, 257, 47138, 365, 18426, 11, 293, 550, 456, 307, 257, 2121, 2010, 295, 4984, 300, 307, 1219, 51800], "temperature": 0.0, "avg_logprob": -0.13224078159706265, "compression_ratio": 1.7411764705882353, "no_speech_prob": 0.01454977598041296}, {"id": 1749, "seek": 1023836, "start": 10238.36, "end": 10243.480000000001, "text": " the Levy-Civita connection that is compatible with the metric, so you can think of the same thing", "tokens": [50364, 264, 1456, 11869, 12, 34, 592, 2786, 4984, 300, 307, 18218, 365, 264, 20678, 11, 370, 291, 393, 519, 295, 264, 912, 551, 50620], "temperature": 0.0, "avg_logprob": -0.09422283337034028, "compression_ratio": 1.7262773722627738, "no_speech_prob": 0.0020031973253935575}, {"id": 1750, "seek": 1023836, "start": 10243.480000000001, "end": 10249.32, "text": " on a graph, so a graph is a purely topological object, I have a notion of who my neighbors are,", "tokens": [50620, 322, 257, 4295, 11, 370, 257, 4295, 307, 257, 17491, 1192, 4383, 2657, 11, 286, 362, 257, 10710, 295, 567, 452, 12512, 366, 11, 50912], "temperature": 0.0, "avg_logprob": -0.09422283337034028, "compression_ratio": 1.7262773722627738, "no_speech_prob": 0.0020031973253935575}, {"id": 1751, "seek": 1023836, "start": 10249.32, "end": 10255.0, "text": " but I don't have any geometry, so in order to introduce geometry I can equip every node and", "tokens": [50912, 457, 286, 500, 380, 362, 604, 18426, 11, 370, 294, 1668, 281, 5366, 18426, 286, 393, 5037, 633, 9984, 293, 51196], "temperature": 0.0, "avg_logprob": -0.09422283337034028, "compression_ratio": 1.7262773722627738, "no_speech_prob": 0.0020031973253935575}, {"id": 1752, "seek": 1023836, "start": 10255.0, "end": 10261.32, "text": " every edge with a vector space, and I can define by analogy to parallel transport, I can define", "tokens": [51196, 633, 4691, 365, 257, 8062, 1901, 11, 293, 286, 393, 6964, 538, 21663, 281, 8952, 5495, 11, 286, 393, 6964, 51512], "temperature": 0.0, "avg_logprob": -0.09422283337034028, "compression_ratio": 1.7262773722627738, "no_speech_prob": 0.0020031973253935575}, {"id": 1753, "seek": 1023836, "start": 10261.32, "end": 10266.52, "text": " linear maps, so these are called restriction maps that go between these spaces, so slightly", "tokens": [51512, 8213, 11317, 11, 370, 613, 366, 1219, 29529, 11317, 300, 352, 1296, 613, 7673, 11, 370, 4748, 51772], "temperature": 0.0, "avg_logprob": -0.09422283337034028, "compression_ratio": 1.7262773722627738, "no_speech_prob": 0.0020031973253935575}, {"id": 1754, "seek": 1026652, "start": 10266.52, "end": 10272.04, "text": " different from many folds, I go from the space associated with nodes to a space associated", "tokens": [50364, 819, 490, 867, 31341, 11, 286, 352, 490, 264, 1901, 6615, 365, 13891, 281, 257, 1901, 6615, 50640], "temperature": 0.0, "avg_logprob": -0.11141660478379992, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.0028799562714993954}, {"id": 1755, "seek": 1026652, "start": 10272.04, "end": 10276.92, "text": " with an edge, and then if I want to transport information from a node to a node, I need to", "tokens": [50640, 365, 364, 4691, 11, 293, 550, 498, 286, 528, 281, 5495, 1589, 490, 257, 9984, 281, 257, 9984, 11, 286, 643, 281, 50884], "temperature": 0.0, "avg_logprob": -0.11141660478379992, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.0028799562714993954}, {"id": 1756, "seek": 1026652, "start": 10276.92, "end": 10284.92, "text": " combine these two maps, one with transpose, so basically it's a kind of, so I invent", "tokens": [50884, 10432, 613, 732, 11317, 11, 472, 365, 25167, 11, 370, 1936, 309, 311, 257, 733, 295, 11, 370, 286, 7962, 51284], "temperature": 0.0, "avg_logprob": -0.11141660478379992, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.0028799562714993954}, {"id": 1757, "seek": 1026652, "start": 10286.52, "end": 10292.44, "text": " geometry on a graph, so I lift it into a more complicated object, and on this object I can", "tokens": [51364, 18426, 322, 257, 4295, 11, 370, 286, 5533, 309, 666, 257, 544, 6179, 2657, 11, 293, 322, 341, 2657, 286, 393, 51660], "temperature": 0.0, "avg_logprob": -0.11141660478379992, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.0028799562714993954}, {"id": 1758, "seek": 1029244, "start": 10292.44, "end": 10297.560000000001, "text": " now study, for example, what happens if I choose these restriction maps to be of certain class,", "tokens": [50364, 586, 2979, 11, 337, 1365, 11, 437, 2314, 498, 286, 2826, 613, 29529, 11317, 281, 312, 295, 1629, 1508, 11, 50620], "temperature": 0.0, "avg_logprob": -0.09958954544754715, "compression_ratio": 2.0353982300884956, "no_speech_prob": 0.006165969651192427}, {"id": 1759, "seek": 1029244, "start": 10297.560000000001, "end": 10302.28, "text": " so these are matrices of certain dimension, and I can choose them, for example, to be", "tokens": [50620, 370, 613, 366, 32284, 295, 1629, 10139, 11, 293, 286, 393, 2826, 552, 11, 337, 1365, 11, 281, 312, 50856], "temperature": 0.0, "avg_logprob": -0.09958954544754715, "compression_ratio": 2.0353982300884956, "no_speech_prob": 0.006165969651192427}, {"id": 1760, "seek": 1029244, "start": 10302.28, "end": 10305.960000000001, "text": " symmetric, or I want them to be orthogonal, or I want them to be something else, right,", "tokens": [50856, 32330, 11, 420, 286, 528, 552, 281, 312, 41488, 11, 420, 286, 528, 552, 281, 312, 746, 1646, 11, 558, 11, 51040], "temperature": 0.0, "avg_logprob": -0.09958954544754715, "compression_ratio": 2.0353982300884956, "no_speech_prob": 0.006165969651192427}, {"id": 1761, "seek": 1029244, "start": 10305.960000000001, "end": 10312.52, "text": " I can also choose the dimension of these, of these, what is called stocks, right, so these spaces,", "tokens": [51040, 286, 393, 611, 2826, 264, 10139, 295, 613, 11, 295, 613, 11, 437, 307, 1219, 12966, 11, 558, 11, 370, 613, 7673, 11, 51368], "temperature": 0.0, "avg_logprob": -0.09958954544754715, "compression_ratio": 2.0353982300884956, "no_speech_prob": 0.006165969651192427}, {"id": 1762, "seek": 1029244, "start": 10313.880000000001, "end": 10319.4, "text": " and I can define differential operators on this structure, right, so the difference between", "tokens": [51436, 293, 286, 393, 6964, 15756, 19077, 322, 341, 3877, 11, 558, 11, 370, 264, 2649, 1296, 51712], "temperature": 0.0, "avg_logprob": -0.09958954544754715, "compression_ratio": 2.0353982300884956, "no_speech_prob": 0.006165969651192427}, {"id": 1763, "seek": 1031940, "start": 10319.4, "end": 10325.56, "text": " the standard, for example, gradient and the shift gradient would be the same way as we have a", "tokens": [50364, 264, 3832, 11, 337, 1365, 11, 16235, 293, 264, 5513, 16235, 576, 312, 264, 912, 636, 382, 321, 362, 257, 50672], "temperature": 0.0, "avg_logprob": -0.12125492095947266, "compression_ratio": 1.891566265060241, "no_speech_prob": 0.0015766454162076116}, {"id": 1764, "seek": 1031940, "start": 10325.56, "end": 10331.08, "text": " manifold, I cannot add or subtract two points on the manifold, so when I need to subtract two", "tokens": [50672, 47138, 11, 286, 2644, 909, 420, 16390, 732, 2793, 322, 264, 47138, 11, 370, 562, 286, 643, 281, 16390, 732, 50948], "temperature": 0.0, "avg_logprob": -0.12125492095947266, "compression_ratio": 1.891566265060241, "no_speech_prob": 0.0015766454162076116}, {"id": 1765, "seek": 1031940, "start": 10331.08, "end": 10335.8, "text": " vectors, I first need to apply parallel transport, so I need to bring the vector from its vector", "tokens": [50948, 18875, 11, 286, 700, 643, 281, 3079, 8952, 5495, 11, 370, 286, 643, 281, 1565, 264, 8062, 490, 1080, 8062, 51184], "temperature": 0.0, "avg_logprob": -0.12125492095947266, "compression_ratio": 1.891566265060241, "no_speech_prob": 0.0015766454162076116}, {"id": 1766, "seek": 1031940, "start": 10335.8, "end": 10340.6, "text": " space to my vector space, to my tangent space, and by doing this typically I would apply some", "tokens": [51184, 1901, 281, 452, 8062, 1901, 11, 281, 452, 27747, 1901, 11, 293, 538, 884, 341, 5850, 286, 576, 3079, 512, 51424], "temperature": 0.0, "avg_logprob": -0.12125492095947266, "compression_ratio": 1.891566265060241, "no_speech_prob": 0.0015766454162076116}, {"id": 1767, "seek": 1031940, "start": 10340.6, "end": 10345.16, "text": " form of rotation, if it's a manifold with the remaining metric, and only then I can subtract", "tokens": [51424, 1254, 295, 12447, 11, 498, 309, 311, 257, 47138, 365, 264, 8877, 20678, 11, 293, 787, 550, 286, 393, 16390, 51652], "temperature": 0.0, "avg_logprob": -0.12125492095947266, "compression_ratio": 1.891566265060241, "no_speech_prob": 0.0015766454162076116}, {"id": 1768, "seek": 1034516, "start": 10345.16, "end": 10349.64, "text": " them, right, so here the same, if before the gradient looked like just difference between", "tokens": [50364, 552, 11, 558, 11, 370, 510, 264, 912, 11, 498, 949, 264, 16235, 2956, 411, 445, 2649, 1296, 50588], "temperature": 0.0, "avg_logprob": -0.12643925618317167, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.002731603803113103}, {"id": 1769, "seek": 1034516, "start": 10349.64, "end": 10355.08, "text": " n points of an edge, here we'll have also some linear transformation that sits in between,", "tokens": [50588, 297, 2793, 295, 364, 4691, 11, 510, 321, 603, 362, 611, 512, 8213, 9887, 300, 12696, 294, 1296, 11, 50860], "temperature": 0.0, "avg_logprob": -0.12643925618317167, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.002731603803113103}, {"id": 1770, "seek": 1034516, "start": 10355.08, "end": 10360.039999999999, "text": " right, long story short, I can, basically I'm interested in a Laplacian, right, so I have a", "tokens": [50860, 558, 11, 938, 1657, 2099, 11, 286, 393, 11, 1936, 286, 478, 3102, 294, 257, 2369, 564, 326, 952, 11, 558, 11, 370, 286, 362, 257, 51108], "temperature": 0.0, "avg_logprob": -0.12643925618317167, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.002731603803113103}, {"id": 1771, "seek": 1034516, "start": 10360.039999999999, "end": 10366.92, "text": " shift version of the Laplacian, so it's a block matrix where every block transforms the vectors", "tokens": [51108, 5513, 3037, 295, 264, 2369, 564, 326, 952, 11, 370, 309, 311, 257, 3461, 8141, 689, 633, 3461, 35592, 264, 18875, 51452], "temperature": 0.0, "avg_logprob": -0.12643925618317167, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.002731603803113103}, {"id": 1772, "seek": 1034516, "start": 10366.92, "end": 10373.64, "text": " with these kind of, with these kind of matrices, right, the combination, and now I can apply this", "tokens": [51452, 365, 613, 733, 295, 11, 365, 613, 733, 295, 32284, 11, 558, 11, 264, 6562, 11, 293, 586, 286, 393, 3079, 341, 51788], "temperature": 0.0, "avg_logprob": -0.12643925618317167, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.002731603803113103}, {"id": 1773, "seek": 1037364, "start": 10373.72, "end": 10379.96, "text": " operator on my data and run a diffusion equation, and I can run it to infinity with some additional", "tokens": [50368, 12973, 322, 452, 1412, 293, 1190, 257, 25242, 5367, 11, 293, 286, 393, 1190, 309, 281, 13202, 365, 512, 4497, 50680], "temperature": 0.0, "avg_logprob": -0.12584201846502524, "compression_ratio": 1.696629213483146, "no_speech_prob": 0.0008306928793899715}, {"id": 1774, "seek": 1037364, "start": 10379.96, "end": 10385.08, "text": " conditions, and I can ask questions like how many classes can I separate if I choose this", "tokens": [50680, 4487, 11, 293, 286, 393, 1029, 1651, 411, 577, 867, 5359, 393, 286, 4994, 498, 286, 2826, 341, 50936], "temperature": 0.0, "avg_logprob": -0.12584201846502524, "compression_ratio": 1.696629213483146, "no_speech_prob": 0.0008306928793899715}, {"id": 1775, "seek": 1037364, "start": 10385.08, "end": 10390.76, "text": " shift in a certain way, yeah. Are you doing graph rewiring here? No, we are not doing any", "tokens": [50936, 5513, 294, 257, 1629, 636, 11, 1338, 13, 2014, 291, 884, 4295, 319, 86, 5057, 510, 30, 883, 11, 321, 366, 406, 884, 604, 51220], "temperature": 0.0, "avg_logprob": -0.12584201846502524, "compression_ratio": 1.696629213483146, "no_speech_prob": 0.0008306928793899715}, {"id": 1776, "seek": 1037364, "start": 10390.76, "end": 10394.519999999999, "text": " graph rewiring, so the graph structure is encoded in the structure of the Laplacian,", "tokens": [51220, 4295, 319, 86, 5057, 11, 370, 264, 4295, 3877, 307, 2058, 12340, 294, 264, 3877, 295, 264, 2369, 564, 326, 952, 11, 51408], "temperature": 0.0, "avg_logprob": -0.12584201846502524, "compression_ratio": 1.696629213483146, "no_speech_prob": 0.0008306928793899715}, {"id": 1777, "seek": 1037364, "start": 10396.279999999999, "end": 10403.08, "text": " right, so basically it's a kind of question about expressive power of this architecture,", "tokens": [51496, 558, 11, 370, 1936, 309, 311, 257, 733, 295, 1168, 466, 40189, 1347, 295, 341, 9482, 11, 51836], "temperature": 0.0, "avg_logprob": -0.12584201846502524, "compression_ratio": 1.696629213483146, "no_speech_prob": 0.0008306928793899715}, {"id": 1778, "seek": 1040308, "start": 10403.08, "end": 10409.56, "text": " so I can ask how many classes I can separate, right, so expressive power, slightly different,", "tokens": [50364, 370, 286, 393, 1029, 577, 867, 5359, 286, 393, 4994, 11, 558, 11, 370, 40189, 1347, 11, 4748, 819, 11, 50688], "temperature": 0.0, "avg_logprob": -0.19465461730957032, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.0013041204074397683}, {"id": 1779, "seek": 1040308, "start": 10409.56, "end": 10413.88, "text": " different version from the, from the Weisferre and Lehmann, because in Weisferre and Lehmann we", "tokens": [50688, 819, 3037, 490, 264, 11, 490, 264, 492, 271, 612, 265, 293, 1456, 8587, 969, 11, 570, 294, 492, 271, 612, 265, 293, 1456, 8587, 969, 321, 50904], "temperature": 0.0, "avg_logprob": -0.19465461730957032, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.0013041204074397683}, {"id": 1780, "seek": 1040308, "start": 10413.88, "end": 10419.72, "text": " asked about the, how many, the types of graphs that we can, if we can distinguish, here we're", "tokens": [50904, 2351, 466, 264, 11, 577, 867, 11, 264, 3467, 295, 24877, 300, 321, 393, 11, 498, 321, 393, 20206, 11, 510, 321, 434, 51196], "temperature": 0.0, "avg_logprob": -0.19465461730957032, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.0013041204074397683}, {"id": 1781, "seek": 1040308, "start": 10419.72, "end": 10428.76, "text": " looking at node level problems, yeah. Can you please make some like use cases for this kind of", "tokens": [51196, 1237, 412, 9984, 1496, 2740, 11, 1338, 13, 1664, 291, 1767, 652, 512, 411, 764, 3331, 337, 341, 733, 295, 51648], "temperature": 0.0, "avg_logprob": -0.19465461730957032, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.0013041204074397683}, {"id": 1782, "seek": 1042876, "start": 10429.72, "end": 10436.04, "text": " of graph, and the ones that were shown before in which the nodes were actually separated with", "tokens": [50412, 295, 4295, 11, 293, 264, 2306, 300, 645, 4898, 949, 294, 597, 264, 13891, 645, 767, 12005, 365, 50728], "temperature": 0.0, "avg_logprob": -0.17076405392417424, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.004766487050801516}, {"id": 1783, "seek": 1042876, "start": 10436.04, "end": 10445.08, "text": " no connections, one to another? So, the graph here is given, so it's, I think some, I don't remember", "tokens": [50728, 572, 9271, 11, 472, 281, 1071, 30, 407, 11, 264, 4295, 510, 307, 2212, 11, 370, 309, 311, 11, 286, 519, 512, 11, 286, 500, 380, 1604, 51180], "temperature": 0.0, "avg_logprob": -0.17076405392417424, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.004766487050801516}, {"id": 1784, "seek": 1042876, "start": 10445.08, "end": 10454.12, "text": " what data set it is, yeah, sorry, what is again the question? So, like, what kind of, what are", "tokens": [51180, 437, 1412, 992, 309, 307, 11, 1338, 11, 2597, 11, 437, 307, 797, 264, 1168, 30, 407, 11, 411, 11, 437, 733, 295, 11, 437, 366, 51632], "temperature": 0.0, "avg_logprob": -0.17076405392417424, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.004766487050801516}, {"id": 1785, "seek": 1045412, "start": 10454.12, "end": 10461.800000000001, "text": " you trying to model and why is this configuration? Oh, so, yeah, the colors represent the classes,", "tokens": [50364, 291, 1382, 281, 2316, 293, 983, 307, 341, 11694, 30, 876, 11, 370, 11, 1338, 11, 264, 4577, 2906, 264, 5359, 11, 50748], "temperature": 0.0, "avg_logprob": -0.11654257093157087, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.004368412774056196}, {"id": 1786, "seek": 1045412, "start": 10461.800000000001, "end": 10467.400000000001, "text": " the ground rules classes, and the positions represent the features. And what's the difference", "tokens": [50748, 264, 2727, 4474, 5359, 11, 293, 264, 8432, 2906, 264, 4122, 13, 400, 437, 311, 264, 2649, 51028], "temperature": 0.0, "avg_logprob": -0.11654257093157087, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.004368412774056196}, {"id": 1787, "seek": 1045412, "start": 10467.400000000001, "end": 10473.640000000001, "text": " between this type and the one in which you do rewiring and each class series separated,", "tokens": [51028, 1296, 341, 2010, 293, 264, 472, 294, 597, 291, 360, 319, 86, 5057, 293, 1184, 1508, 2638, 12005, 11, 51340], "temperature": 0.0, "avg_logprob": -0.11654257093157087, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.004368412774056196}, {"id": 1788, "seek": 1045412, "start": 10473.640000000001, "end": 10477.720000000001, "text": " one to another? So, here the features are represented by coordinates, so the closest", "tokens": [51340, 472, 281, 1071, 30, 407, 11, 510, 264, 4122, 366, 10379, 538, 21056, 11, 370, 264, 13699, 51544], "temperature": 0.0, "avg_logprob": -0.11654257093157087, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.004368412774056196}, {"id": 1789, "seek": 1045412, "start": 10477.720000000001, "end": 10482.68, "text": " analogy of this illustration is to the one that I showed with the gradient flow, right,", "tokens": [51544, 21663, 295, 341, 22645, 307, 281, 264, 472, 300, 286, 4712, 365, 264, 16235, 3095, 11, 558, 11, 51792], "temperature": 0.0, "avg_logprob": -0.11654257093157087, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.004368412774056196}, {"id": 1790, "seek": 1048268, "start": 10482.68, "end": 10487.880000000001, "text": " so the coordinates here represent the features, not the positional coordinates, right, and the", "tokens": [50364, 370, 264, 21056, 510, 2906, 264, 4122, 11, 406, 264, 2535, 304, 21056, 11, 558, 11, 293, 264, 50624], "temperature": 0.0, "avg_logprob": -0.09959421572477921, "compression_ratio": 1.8576923076923078, "no_speech_prob": 0.0008807321428321302}, {"id": 1791, "seek": 1048268, "start": 10487.880000000001, "end": 10494.04, "text": " colors represent the classes, okay. So, there is no rewiring happening here, you can also potentially", "tokens": [50624, 4577, 2906, 264, 5359, 11, 1392, 13, 407, 11, 456, 307, 572, 319, 86, 5057, 2737, 510, 11, 291, 393, 611, 7263, 50932], "temperature": 0.0, "avg_logprob": -0.09959421572477921, "compression_ratio": 1.8576923076923078, "no_speech_prob": 0.0008807321428321302}, {"id": 1792, "seek": 1048268, "start": 10494.04, "end": 10499.880000000001, "text": " use rewiring. And the results, well, I don't want to go through all the results, but basically what", "tokens": [50932, 764, 319, 86, 5057, 13, 400, 264, 3542, 11, 731, 11, 286, 500, 380, 528, 281, 352, 807, 439, 264, 3542, 11, 457, 1936, 437, 51224], "temperature": 0.0, "avg_logprob": -0.09959421572477921, "compression_ratio": 1.8576923076923078, "no_speech_prob": 0.0008807321428321302}, {"id": 1793, "seek": 1048268, "start": 10499.880000000001, "end": 10505.48, "text": " we show is that by using different types of sheaves, we can guarantee that we can separate", "tokens": [51224, 321, 855, 307, 300, 538, 1228, 819, 3467, 295, 750, 5423, 11, 321, 393, 10815, 300, 321, 393, 4994, 51504], "temperature": 0.0, "avg_logprob": -0.09959421572477921, "compression_ratio": 1.8576923076923078, "no_speech_prob": 0.0008807321428321302}, {"id": 1794, "seek": 1048268, "start": 10505.48, "end": 10511.800000000001, "text": " different types of, different number of node classes, depending whether the graph is homophilic", "tokens": [51504, 819, 3467, 295, 11, 819, 1230, 295, 9984, 5359, 11, 5413, 1968, 264, 4295, 307, 3655, 5317, 388, 299, 51820], "temperature": 0.0, "avg_logprob": -0.09959421572477921, "compression_ratio": 1.8576923076923078, "no_speech_prob": 0.0008807321428321302}, {"id": 1795, "seek": 1051180, "start": 10511.8, "end": 10516.599999999999, "text": " or heterophilic. For example, we show that you must have non-symmetric relations if you want to", "tokens": [50364, 420, 20789, 5317, 388, 299, 13, 1171, 1365, 11, 321, 855, 300, 291, 1633, 362, 2107, 12, 3187, 2174, 17475, 2299, 498, 291, 528, 281, 50604], "temperature": 0.0, "avg_logprob": -0.1231681164180007, "compression_ratio": 1.6016949152542372, "no_speech_prob": 0.0014824711252003908}, {"id": 1796, "seek": 1051180, "start": 10516.599999999999, "end": 10525.4, "text": " deal with heterophilic graphs, yeah. So, in previous method, WL, it's something like, I think the", "tokens": [50604, 2028, 365, 20789, 5317, 388, 299, 24877, 11, 1338, 13, 407, 11, 294, 3894, 3170, 11, 343, 43, 11, 309, 311, 746, 411, 11, 286, 519, 264, 51044], "temperature": 0.0, "avg_logprob": -0.1231681164180007, "compression_ratio": 1.6016949152542372, "no_speech_prob": 0.0014824711252003908}, {"id": 1797, "seek": 1051180, "start": 10525.4, "end": 10532.199999999999, "text": " classification was on top of number of edge, if I get it correctly, like if you both go to the", "tokens": [51044, 21538, 390, 322, 1192, 295, 1230, 295, 4691, 11, 498, 286, 483, 309, 8944, 11, 411, 498, 291, 1293, 352, 281, 264, 51384], "temperature": 0.0, "avg_logprob": -0.1231681164180007, "compression_ratio": 1.6016949152542372, "no_speech_prob": 0.0014824711252003908}, {"id": 1798, "seek": 1051180, "start": 10532.199999999999, "end": 10539.56, "text": " higher classes, then the number of edges also increases, but here in sheaves, it's, it's,", "tokens": [51384, 2946, 5359, 11, 550, 264, 1230, 295, 8819, 611, 8637, 11, 457, 510, 294, 750, 5423, 11, 309, 311, 11, 309, 311, 11, 51752], "temperature": 0.0, "avg_logprob": -0.1231681164180007, "compression_ratio": 1.6016949152542372, "no_speech_prob": 0.0014824711252003908}, {"id": 1799, "seek": 1053956, "start": 10539.56, "end": 10546.199999999999, "text": " is it about the dimension that we have, like, can we have a first dimension sheaves or something", "tokens": [50364, 307, 309, 466, 264, 10139, 300, 321, 362, 11, 411, 11, 393, 321, 362, 257, 700, 10139, 750, 5423, 420, 746, 50696], "temperature": 0.0, "avg_logprob": -0.19122153346978346, "compression_ratio": 1.7432432432432432, "no_speech_prob": 0.0016464540967717767}, {"id": 1800, "seek": 1053956, "start": 10546.199999999999, "end": 10552.76, "text": " like this? So, Weisfer and Lehmann is different, so the hierarchy there is, basically, they are", "tokens": [50696, 411, 341, 30, 407, 11, 492, 271, 612, 293, 1456, 8587, 969, 307, 819, 11, 370, 264, 22333, 456, 307, 11, 1936, 11, 436, 366, 51024], "temperature": 0.0, "avg_logprob": -0.19122153346978346, "compression_ratio": 1.7432432432432432, "no_speech_prob": 0.0016464540967717767}, {"id": 1801, "seek": 1053956, "start": 10552.76, "end": 10558.359999999999, "text": " different algorithms, right. So, whether they do color refinement for different structures for,", "tokens": [51024, 819, 14642, 11, 558, 13, 407, 11, 1968, 436, 360, 2017, 1895, 30229, 337, 819, 9227, 337, 11, 51304], "temperature": 0.0, "avg_logprob": -0.19122153346978346, "compression_ratio": 1.7432432432432432, "no_speech_prob": 0.0016464540967717767}, {"id": 1802, "seek": 1053956, "start": 10558.84, "end": 10566.439999999999, "text": " for a node, for a pair of nodes, for triplets of nodes, and so on. So, here we have, the choice is", "tokens": [51328, 337, 257, 9984, 11, 337, 257, 6119, 295, 13891, 11, 337, 1376, 31023, 295, 13891, 11, 293, 370, 322, 13, 407, 11, 510, 321, 362, 11, 264, 3922, 307, 51708], "temperature": 0.0, "avg_logprob": -0.19122153346978346, "compression_ratio": 1.7432432432432432, "no_speech_prob": 0.0016464540967717767}, {"id": 1803, "seek": 1056644, "start": 10566.44, "end": 10570.76, "text": " what kind of matrices we allow, what class of matrices we allow, so it's typically a group,", "tokens": [50364, 437, 733, 295, 32284, 321, 2089, 11, 437, 1508, 295, 32284, 321, 2089, 11, 370, 309, 311, 5850, 257, 1594, 11, 50580], "temperature": 0.0, "avg_logprob": -0.12193224922059075, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.0031875246204435825}, {"id": 1804, "seek": 1056644, "start": 10570.76, "end": 10576.12, "text": " right, so we say, for example, the, the most general case is, is GL, right, so any invertible", "tokens": [50580, 558, 11, 370, 321, 584, 11, 337, 1365, 11, 264, 11, 264, 881, 2674, 1389, 307, 11, 307, 16225, 11, 558, 11, 370, 604, 33966, 964, 50848], "temperature": 0.0, "avg_logprob": -0.12193224922059075, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.0031875246204435825}, {"id": 1805, "seek": 1056644, "start": 10576.12, "end": 10581.16, "text": " matrix, then we can restrict it to be, for example, symmetric matrix, or then we can restrict it to", "tokens": [50848, 8141, 11, 550, 321, 393, 7694, 309, 281, 312, 11, 337, 1365, 11, 32330, 8141, 11, 420, 550, 321, 393, 7694, 309, 281, 51100], "temperature": 0.0, "avg_logprob": -0.12193224922059075, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.0031875246204435825}, {"id": 1806, "seek": 1056644, "start": 10581.16, "end": 10586.36, "text": " be orthogonal matrix, right, and based on these choices, plus the dimension of the sheave, we get", "tokens": [51100, 312, 41488, 8141, 11, 558, 11, 293, 2361, 322, 613, 7994, 11, 1804, 264, 10139, 295, 264, 750, 946, 11, 321, 483, 51360], "temperature": 0.0, "avg_logprob": -0.12193224922059075, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.0031875246204435825}, {"id": 1807, "seek": 1056644, "start": 10586.36, "end": 10594.76, "text": " different results. So, it has to take dimension and also the matrix. Exactly. Thank you. So,", "tokens": [51360, 819, 3542, 13, 407, 11, 309, 575, 281, 747, 10139, 293, 611, 264, 8141, 13, 7587, 13, 1044, 291, 13, 407, 11, 51780], "temperature": 0.0, "avg_logprob": -0.12193224922059075, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.0031875246204435825}, {"id": 1808, "seek": 1059476, "start": 10594.76, "end": 10598.52, "text": " this is a more theoretical question, right, because it's a good question how we actually,", "tokens": [50364, 341, 307, 257, 544, 20864, 1168, 11, 558, 11, 570, 309, 311, 257, 665, 1168, 577, 321, 767, 11, 50552], "temperature": 0.0, "avg_logprob": -0.12110958428218446, "compression_ratio": 1.7732793522267207, "no_speech_prob": 0.0033052314538508654}, {"id": 1809, "seek": 1059476, "start": 10598.52, "end": 10602.52, "text": " how we learn the sheave from the data, but assuming that we knew the sheave, right,", "tokens": [50552, 577, 321, 1466, 264, 750, 946, 490, 264, 1412, 11, 457, 11926, 300, 321, 2586, 264, 750, 946, 11, 558, 11, 50752], "temperature": 0.0, "avg_logprob": -0.12110958428218446, "compression_ratio": 1.7732793522267207, "no_speech_prob": 0.0033052314538508654}, {"id": 1810, "seek": 1059476, "start": 10602.52, "end": 10607.32, "text": " but we allow the sheave to be only from of a certain type, what is in the best case,", "tokens": [50752, 457, 321, 2089, 264, 750, 946, 281, 312, 787, 490, 295, 257, 1629, 2010, 11, 437, 307, 294, 264, 1151, 1389, 11, 50992], "temperature": 0.0, "avg_logprob": -0.12110958428218446, "compression_ratio": 1.7732793522267207, "no_speech_prob": 0.0033052314538508654}, {"id": 1811, "seek": 1059476, "start": 10608.2, "end": 10613.08, "text": " how many node classes we could separate, under different assumptions also about", "tokens": [51036, 577, 867, 9984, 5359, 321, 727, 4994, 11, 833, 819, 17695, 611, 466, 51280], "temperature": 0.0, "avg_logprob": -0.12110958428218446, "compression_ratio": 1.7732793522267207, "no_speech_prob": 0.0033052314538508654}, {"id": 1812, "seek": 1059476, "start": 10613.08, "end": 10618.36, "text": " the structure of the graph, whether it's homophilic or heterophilic. But, basically, the, the, the,", "tokens": [51280, 264, 3877, 295, 264, 4295, 11, 1968, 309, 311, 3655, 5317, 388, 299, 420, 20789, 5317, 388, 299, 13, 583, 11, 1936, 11, 264, 11, 264, 11, 264, 11, 51544], "temperature": 0.0, "avg_logprob": -0.12110958428218446, "compression_ratio": 1.7732793522267207, "no_speech_prob": 0.0033052314538508654}, {"id": 1813, "seek": 1061836, "start": 10618.36, "end": 10624.92, "text": " the bottom line of this story is that diffusion, when you have, when you have these extra degrees", "tokens": [50364, 264, 2767, 1622, 295, 341, 1657, 307, 300, 25242, 11, 562, 291, 362, 11, 562, 291, 362, 613, 2857, 5310, 50692], "temperature": 0.0, "avg_logprob": -0.16897717118263245, "compression_ratio": 1.7409638554216869, "no_speech_prob": 0.009498439729213715}, {"id": 1814, "seek": 1061836, "start": 10624.92, "end": 10629.880000000001, "text": " of freedom, looks more interesting than, than, than the standard diffusion on a graph. So,", "tokens": [50692, 295, 5645, 11, 1542, 544, 1880, 813, 11, 813, 11, 813, 264, 3832, 25242, 322, 257, 4295, 13, 407, 11, 50940], "temperature": 0.0, "avg_logprob": -0.16897717118263245, "compression_ratio": 1.7409638554216869, "no_speech_prob": 0.009498439729213715}, {"id": 1815, "seek": 1061836, "start": 10629.880000000001, "end": 10635.400000000001, "text": " the standard diffusion on a graph corresponds to symmetric restrictions with one dimensional sheave.", "tokens": [50940, 264, 3832, 25242, 322, 257, 4295, 23249, 281, 32330, 14191, 365, 472, 18795, 750, 946, 13, 51216], "temperature": 0.0, "avg_logprob": -0.16897717118263245, "compression_ratio": 1.7409638554216869, "no_speech_prob": 0.009498439729213715}, {"id": 1816, "seek": 1063540, "start": 10635.4, "end": 10645.24, "text": " Yep.", "tokens": [50380, 7010, 13, 50856], "temperature": 1.0, "avg_logprob": -1.3964945793151855, "compression_ratio": 0.3333333333333333, "no_speech_prob": 0.34844088554382324}, {"id": 1817, "seek": 1066540, "start": 10665.4, "end": 10668.6, "text": " well so it's slightly more complicated right so the analogy of the", "tokens": [50364, 731, 370, 309, 311, 4748, 544, 6179, 558, 370, 264, 21663, 295, 264, 50524], "temperature": 0.0, "avg_logprob": -0.15996701930596577, "compression_ratio": 2.003831417624521, "no_speech_prob": 0.03257317468523979}, {"id": 1818, "seek": 1066540, "start": 10668.6, "end": 10672.039999999999, "text": " connection would be the composition of two maps right so what we call a", "tokens": [50524, 4984, 576, 312, 264, 12686, 295, 732, 11317, 558, 370, 437, 321, 818, 257, 50696], "temperature": 0.0, "avg_logprob": -0.15996701930596577, "compression_ratio": 2.003831417624521, "no_speech_prob": 0.03257317468523979}, {"id": 1819, "seek": 1066540, "start": 10672.039999999999, "end": 10675.119999999999, "text": " transport map so each of these f's is called the restriction map so it goes", "tokens": [50696, 5495, 4471, 370, 1184, 295, 613, 283, 311, 307, 1219, 264, 29529, 4471, 370, 309, 1709, 50850], "temperature": 0.0, "avg_logprob": -0.15996701930596577, "compression_ratio": 2.003831417624521, "no_speech_prob": 0.03257317468523979}, {"id": 1820, "seek": 1066540, "start": 10675.119999999999, "end": 10679.14, "text": " from node to edge space they actually can have different dimensions so they", "tokens": [50850, 490, 9984, 281, 4691, 1901, 436, 767, 393, 362, 819, 12819, 370, 436, 51051], "temperature": 0.0, "avg_logprob": -0.15996701930596577, "compression_ratio": 2.003831417624521, "no_speech_prob": 0.03257317468523979}, {"id": 1821, "seek": 1066540, "start": 10679.14, "end": 10684.72, "text": " don't need to be the same the composition right so f f transpose is a map", "tokens": [51051, 500, 380, 643, 281, 312, 264, 912, 264, 12686, 558, 370, 283, 283, 25167, 307, 257, 4471, 51330], "temperature": 0.0, "avg_logprob": -0.15996701930596577, "compression_ratio": 2.003831417624521, "no_speech_prob": 0.03257317468523979}, {"id": 1822, "seek": 1066540, "start": 10684.72, "end": 10689.24, "text": " from the space of one node to the space of another node so that's the analogy of", "tokens": [51330, 490, 264, 1901, 295, 472, 9984, 281, 264, 1901, 295, 1071, 9984, 370, 300, 311, 264, 21663, 295, 51556], "temperature": 0.0, "avg_logprob": -0.15996701930596577, "compression_ratio": 2.003831417624521, "no_speech_prob": 0.03257317468523979}, {"id": 1823, "seek": 1066540, "start": 10689.24, "end": 10692.64, "text": " parallel transport right so I'm when I move a vector from one node to another", "tokens": [51556, 8952, 5495, 558, 370, 286, 478, 562, 286, 1286, 257, 8062, 490, 472, 9984, 281, 1071, 51726], "temperature": 0.0, "avg_logprob": -0.15996701930596577, "compression_ratio": 2.003831417624521, "no_speech_prob": 0.03257317468523979}, {"id": 1824, "seek": 1069264, "start": 10693.599999999999, "end": 10698.039999999999, "text": " geometrically transform it somehow rotated for example or scale it depends", "tokens": [50412, 12956, 81, 984, 4088, 309, 6063, 42146, 337, 1365, 420, 4373, 309, 5946, 50634], "temperature": 0.0, "avg_logprob": -0.1607865979594569, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0010767901549115777}, {"id": 1825, "seek": 1069264, "start": 10698.039999999999, "end": 10707.64, "text": " on the class of matrix that I use here so in exactly so but then of course in", "tokens": [50634, 322, 264, 1508, 295, 8141, 300, 286, 764, 510, 370, 294, 2293, 370, 457, 550, 295, 1164, 294, 51114], "temperature": 0.0, "avg_logprob": -0.1607865979594569, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0010767901549115777}, {"id": 1826, "seek": 1069264, "start": 10707.64, "end": 10712.119999999999, "text": " practice you need somehow to parametrize it right so you cannot of course in", "tokens": [51114, 3124, 291, 643, 6063, 281, 6220, 302, 470, 1381, 309, 558, 370, 291, 2644, 295, 1164, 294, 51338], "temperature": 0.0, "avg_logprob": -0.1607865979594569, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0010767901549115777}, {"id": 1827, "seek": 1069264, "start": 10712.119999999999, "end": 10717.08, "text": " principle you can say that that let me learn individual f's for every for every", "tokens": [51338, 8665, 291, 393, 584, 300, 300, 718, 385, 1466, 2609, 283, 311, 337, 633, 337, 633, 51586], "temperature": 0.0, "avg_logprob": -0.1607865979594569, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0010767901549115777}, {"id": 1828, "seek": 1069264, "start": 10717.08, "end": 10721.16, "text": " age of the of the graph but it's not practically feasible so in practice f is", "tokens": [51586, 3205, 295, 264, 295, 264, 4295, 457, 309, 311, 406, 15667, 26648, 370, 294, 3124, 283, 307, 51790], "temperature": 0.0, "avg_logprob": -0.1607865979594569, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0010767901549115777}, {"id": 1829, "seek": 1072116, "start": 10721.16, "end": 10725.6, "text": " a matrix valued function that depends on the node features so it's a little", "tokens": [50364, 257, 8141, 22608, 2445, 300, 5946, 322, 264, 9984, 4122, 370, 309, 311, 257, 707, 50586], "temperature": 0.0, "avg_logprob": -0.2365594679309476, "compression_ratio": 1.6204819277108433, "no_speech_prob": 0.0007310353685170412}, {"id": 1830, "seek": 1072116, "start": 10725.6, "end": 10730.44, "text": " bit similar to attention but the tension is scholar here it's matrix so it's a", "tokens": [50586, 857, 2531, 281, 3202, 457, 264, 8980, 307, 17912, 510, 309, 311, 8141, 370, 309, 311, 257, 50828], "temperature": 0.0, "avg_logprob": -0.2365594679309476, "compression_ratio": 1.6204819277108433, "no_speech_prob": 0.0007310353685170412}, {"id": 1831, "seek": 1072116, "start": 10730.44, "end": 10740.28, "text": " geometric operation okay any questions", "tokens": [50828, 33246, 6916, 1392, 604, 1651, 51320], "temperature": 0.0, "avg_logprob": -0.2365594679309476, "compression_ratio": 1.6204819277108433, "no_speech_prob": 0.0007310353685170412}, {"id": 1832, "seek": 1072116, "start": 10743.119999999999, "end": 10750.38, "text": " right so basically to summarize this this part so what do we gain from this", "tokens": [51462, 558, 370, 1936, 281, 20858, 341, 341, 644, 370, 437, 360, 321, 6052, 490, 341, 51825], "temperature": 0.0, "avg_logprob": -0.2365594679309476, "compression_ratio": 1.6204819277108433, "no_speech_prob": 0.0007310353685170412}, {"id": 1833, "seek": 1075038, "start": 10750.42, "end": 10757.22, "text": " physics inspired perspective on graph neural networks so first of all I think", "tokens": [50366, 10649, 7547, 4585, 322, 4295, 18161, 9590, 370, 700, 295, 439, 286, 519, 50706], "temperature": 0.0, "avg_logprob": -0.17854224529462992, "compression_ratio": 1.811764705882353, "no_speech_prob": 0.004121636506170034}, {"id": 1834, "seek": 1075038, "start": 10757.22, "end": 10761.019999999999, "text": " it's different viewpoint on old problems like over smoothing bottlenecks it", "tokens": [50706, 309, 311, 819, 35248, 322, 1331, 2740, 411, 670, 899, 6259, 571, 44641, 2761, 309, 50896], "temperature": 0.0, "avg_logprob": -0.17854224529462992, "compression_ratio": 1.811764705882353, "no_speech_prob": 0.004121636506170034}, {"id": 1835, "seek": 1075038, "start": 10761.019999999999, "end": 10765.38, "text": " allows to on the one hand to interpret existing architectures like guts for", "tokens": [50896, 4045, 281, 322, 264, 472, 1011, 281, 7302, 6741, 6331, 1303, 411, 28560, 337, 51114], "temperature": 0.0, "avg_logprob": -0.17854224529462992, "compression_ratio": 1.811764705882353, "no_speech_prob": 0.004121636506170034}, {"id": 1836, "seek": 1075038, "start": 10765.38, "end": 10769.939999999999, "text": " example from a different perspective it allows to potentially design your", "tokens": [51114, 1365, 490, 257, 819, 4585, 309, 4045, 281, 7263, 1715, 428, 51342], "temperature": 0.0, "avg_logprob": -0.17854224529462992, "compression_ratio": 1.811764705882353, "no_speech_prob": 0.004121636506170034}, {"id": 1837, "seek": 1075038, "start": 10769.939999999999, "end": 10774.38, "text": " architectures right for example using if you think of a generous discretization", "tokens": [51342, 6331, 1303, 558, 337, 1365, 1228, 498, 291, 519, 295, 257, 14537, 25656, 2144, 51564], "temperature": 0.0, "avg_logprob": -0.17854224529462992, "compression_ratio": 1.811764705882353, "no_speech_prob": 0.004121636506170034}, {"id": 1838, "seek": 1075038, "start": 10774.38, "end": 10778.98, "text": " of differential equations then of course you can ask what kind of solver can I", "tokens": [51564, 295, 15756, 11787, 550, 295, 1164, 291, 393, 1029, 437, 733, 295, 1404, 331, 393, 286, 51794], "temperature": 0.0, "avg_logprob": -0.17854224529462992, "compression_ratio": 1.811764705882353, "no_speech_prob": 0.004121636506170034}, {"id": 1839, "seek": 1077898, "start": 10778.98, "end": 10783.18, "text": " use can I use some some more interesting things with adaptive step size or maybe", "tokens": [50364, 764, 393, 286, 764, 512, 512, 544, 1880, 721, 365, 27912, 1823, 2744, 420, 1310, 50574], "temperature": 0.0, "avg_logprob": -0.22765001686670447, "compression_ratio": 1.78714859437751, "no_speech_prob": 0.0030614205170422792}, {"id": 1840, "seek": 1077898, "start": 10783.18, "end": 10788.38, "text": " I don't know multi grid solvers and so on it allows to make principle", "tokens": [50574, 286, 500, 380, 458, 4825, 10748, 1404, 840, 293, 370, 322, 309, 4045, 281, 652, 8665, 50834], "temperature": 0.0, "avg_logprob": -0.22765001686670447, "compression_ratio": 1.78714859437751, "no_speech_prob": 0.0030614205170422792}, {"id": 1841, "seek": 1077898, "start": 10788.38, "end": 10793.42, "text": " architectural choices right like example with gradient flows so basically from", "tokens": [50834, 26621, 7994, 558, 411, 1365, 365, 16235, 12867, 370, 1936, 490, 51086], "temperature": 0.0, "avg_logprob": -0.22765001686670447, "compression_ratio": 1.78714859437751, "no_speech_prob": 0.0030614205170422792}, {"id": 1842, "seek": 1077898, "start": 10793.42, "end": 10798.66, "text": " the gradient flow we get restriction on symmetric weights we get residual", "tokens": [51086, 264, 16235, 3095, 321, 483, 29529, 322, 32330, 17443, 321, 483, 27980, 51348], "temperature": 0.0, "avg_logprob": -0.22765001686670447, "compression_ratio": 1.78714859437751, "no_speech_prob": 0.0030614205170422792}, {"id": 1843, "seek": 1077898, "start": 10798.66, "end": 10802.14, "text": " connection we can also have some theoretical guarantees right again like", "tokens": [51348, 4984, 321, 393, 611, 362, 512, 20864, 32567, 558, 797, 411, 51522], "temperature": 0.0, "avg_logprob": -0.22765001686670447, "compression_ratio": 1.78714859437751, "no_speech_prob": 0.0030614205170422792}, {"id": 1844, "seek": 1077898, "start": 10802.14, "end": 10804.779999999999, "text": " we've seen with the gradient flow but maybe also of other types like", "tokens": [51522, 321, 600, 1612, 365, 264, 16235, 3095, 457, 1310, 611, 295, 661, 3467, 411, 51654], "temperature": 0.0, "avg_logprob": -0.22765001686670447, "compression_ratio": 1.78714859437751, "no_speech_prob": 0.0030614205170422792}, {"id": 1845, "seek": 1080478, "start": 10805.1, "end": 10809.94, "text": " convergence stability and so on and so forth but probably more interesting are", "tokens": [50380, 32181, 11826, 293, 370, 322, 293, 370, 5220, 457, 1391, 544, 1880, 366, 50622], "temperature": 0.0, "avg_logprob": -0.19495358069737753, "compression_ratio": 1.789272030651341, "no_speech_prob": 0.0047125015407800674}, {"id": 1846, "seek": 1080478, "start": 10809.94, "end": 10814.5, "text": " links to other fields that are less explored in graph neural network literature", "tokens": [50622, 6123, 281, 661, 7909, 300, 366, 1570, 24016, 294, 4295, 18161, 3209, 10394, 50850], "temperature": 0.0, "avg_logprob": -0.19495358069737753, "compression_ratio": 1.789272030651341, "no_speech_prob": 0.0047125015407800674}, {"id": 1847, "seek": 1080478, "start": 10814.5, "end": 10818.58, "text": " like in particular differential geometry or algebraic topology and of course", "tokens": [50850, 411, 294, 1729, 15756, 18426, 420, 21989, 299, 1192, 1793, 293, 295, 1164, 51054], "temperature": 0.0, "avg_logprob": -0.19495358069737753, "compression_ratio": 1.789272030651341, "no_speech_prob": 0.0047125015407800674}, {"id": 1848, "seek": 1080478, "start": 10818.58, "end": 10822.54, "text": " diffusion is just one example of evolution equations you can consider more", "tokens": [51054, 25242, 307, 445, 472, 1365, 295, 9303, 11787, 291, 393, 1949, 544, 51252], "temperature": 0.0, "avg_logprob": -0.19495358069737753, "compression_ratio": 1.789272030651341, "no_speech_prob": 0.0047125015407800674}, {"id": 1849, "seek": 1080478, "start": 10822.54, "end": 10825.54, "text": " interesting things so this is one example right so probably have seen these", "tokens": [51252, 1880, 721, 370, 341, 307, 472, 1365, 558, 370, 1391, 362, 1612, 613, 51402], "temperature": 0.0, "avg_logprob": -0.19495358069737753, "compression_ratio": 1.789272030651341, "no_speech_prob": 0.0047125015407800674}, {"id": 1850, "seek": 1080478, "start": 10825.54, "end": 10831.300000000001, "text": " kind of things coupled oscillators so the metronomes that are put on a table and", "tokens": [51402, 733, 295, 721, 29482, 18225, 3391, 370, 264, 1131, 2044, 18168, 300, 366, 829, 322, 257, 3199, 293, 51690], "temperature": 0.0, "avg_logprob": -0.19495358069737753, "compression_ratio": 1.789272030651341, "no_speech_prob": 0.0047125015407800674}, {"id": 1851, "seek": 1083130, "start": 10831.34, "end": 10837.66, "text": " because the vibrations transfer from one to another initially they might be", "tokens": [50366, 570, 264, 32339, 5003, 490, 472, 281, 1071, 9105, 436, 1062, 312, 50682], "temperature": 0.0, "avg_logprob": -0.1221805987971844, "compression_ratio": 1.7707509881422925, "no_speech_prob": 0.0023840051144361496}, {"id": 1852, "seek": 1083130, "start": 10837.66, "end": 10841.259999999998, "text": " oscillating out of phase and then they become synchronized so think of", "tokens": [50682, 18225, 990, 484, 295, 5574, 293, 550, 436, 1813, 19331, 1602, 370, 519, 295, 50862], "temperature": 0.0, "avg_logprob": -0.1221805987971844, "compression_ratio": 1.7707509881422925, "no_speech_prob": 0.0023840051144361496}, {"id": 1853, "seek": 1083130, "start": 10841.259999999998, "end": 10844.82, "text": " something like this but on a graph so the coupling occurs on a graph in a", "tokens": [50862, 746, 411, 341, 457, 322, 257, 4295, 370, 264, 37447, 11843, 322, 257, 4295, 294, 257, 51040], "temperature": 0.0, "avg_logprob": -0.1221805987971844, "compression_ratio": 1.7707509881422925, "no_speech_prob": 0.0023840051144361496}, {"id": 1854, "seek": 1083130, "start": 10844.82, "end": 10848.3, "text": " learnable way and depending on the tasks that we want to do we have we want", "tokens": [51040, 1466, 712, 636, 293, 5413, 322, 264, 9608, 300, 321, 528, 281, 360, 321, 362, 321, 528, 51214], "temperature": 0.0, "avg_logprob": -0.1221805987971844, "compression_ratio": 1.7707509881422925, "no_speech_prob": 0.0023840051144361496}, {"id": 1855, "seek": 1083130, "start": 10848.3, "end": 10852.98, "text": " somehow to to interact between these different oscillators so it's also a", "tokens": [51214, 6063, 281, 281, 4648, 1296, 613, 819, 18225, 3391, 370, 309, 311, 611, 257, 51448], "temperature": 0.0, "avg_logprob": -0.1221805987971844, "compression_ratio": 1.7707509881422925, "no_speech_prob": 0.0023840051144361496}, {"id": 1856, "seek": 1083130, "start": 10852.98, "end": 10857.46, "text": " differential equation but it also has a second order kinetic term so unlike a", "tokens": [51448, 15756, 5367, 457, 309, 611, 575, 257, 1150, 1668, 27135, 1433, 370, 8343, 257, 51672], "temperature": 0.0, "avg_logprob": -0.1221805987971844, "compression_ratio": 1.7707509881422925, "no_speech_prob": 0.0023840051144361496}, {"id": 1857, "seek": 1085746, "start": 10857.539999999999, "end": 10862.82, "text": " diffusion equation it has also a surgery component and here we show for", "tokens": [50368, 25242, 5367, 309, 575, 611, 257, 7930, 6542, 293, 510, 321, 855, 337, 50632], "temperature": 0.0, "avg_logprob": -0.18516638507581737, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.00403712410479784}, {"id": 1858, "seek": 1085746, "start": 10862.82, "end": 10867.5, "text": " example that we can probably avoid over smoothing by using this type of", "tokens": [50632, 1365, 300, 321, 393, 1391, 5042, 670, 899, 6259, 571, 538, 1228, 341, 2010, 295, 50866], "temperature": 0.0, "avg_logprob": -0.18516638507581737, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.00403712410479784}, {"id": 1859, "seek": 1085746, "start": 10867.5, "end": 10879.58, "text": " equations how much time do I have 15 minutes okay so what I would like to do", "tokens": [50866, 11787, 577, 709, 565, 360, 286, 362, 2119, 2077, 1392, 370, 437, 286, 576, 411, 281, 360, 51470], "temperature": 0.0, "avg_logprob": -0.18516638507581737, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.00403712410479784}, {"id": 1860, "seek": 1085746, "start": 10879.58, "end": 10885.9, "text": " if in 15 minutes let's talk about grids so I definitely ran out of time because", "tokens": [51470, 498, 294, 2119, 2077, 718, 311, 751, 466, 677, 3742, 370, 286, 2138, 5872, 484, 295, 565, 570, 51786], "temperature": 0.0, "avg_logprob": -0.18516638507581737, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.00403712410479784}, {"id": 1861, "seek": 1088590, "start": 10885.94, "end": 10889.42, "text": " out of all the geometric objects at the end well we spent all the time on", "tokens": [50366, 484, 295, 439, 264, 33246, 6565, 412, 264, 917, 731, 321, 4418, 439, 264, 565, 322, 50540], "temperature": 0.0, "avg_logprob": -0.13756414488250135, "compression_ratio": 1.7234848484848484, "no_speech_prob": 0.007013281341642141}, {"id": 1862, "seek": 1088590, "start": 10889.42, "end": 10895.74, "text": " crafts I think grids also deserve a little bit and probably well everybody", "tokens": [50540, 27831, 286, 519, 677, 3742, 611, 9948, 257, 707, 857, 293, 1391, 731, 2201, 50856], "temperature": 0.0, "avg_logprob": -0.13756414488250135, "compression_ratio": 1.7234848484848484, "no_speech_prob": 0.007013281341642141}, {"id": 1863, "seek": 1088590, "start": 10895.74, "end": 10899.66, "text": " is familiar with grids right so let's look at them maybe again from the", "tokens": [50856, 307, 4963, 365, 677, 3742, 558, 370, 718, 311, 574, 412, 552, 1310, 797, 490, 264, 51052], "temperature": 0.0, "avg_logprob": -0.13756414488250135, "compression_ratio": 1.7234848484848484, "no_speech_prob": 0.007013281341642141}, {"id": 1864, "seek": 1088590, "start": 10899.66, "end": 10904.3, "text": " perspective of geometric deep learning and hopefully some new intuition or at", "tokens": [51052, 4585, 295, 33246, 2452, 2539, 293, 4696, 512, 777, 24002, 420, 412, 51284], "temperature": 0.0, "avg_logprob": -0.13756414488250135, "compression_ratio": 1.7234848484848484, "no_speech_prob": 0.007013281341642141}, {"id": 1865, "seek": 1088590, "start": 10904.3, "end": 10909.74, "text": " least for some of you have not seen it before so and this also relates to the", "tokens": [51284, 1935, 337, 512, 295, 291, 362, 406, 1612, 309, 949, 370, 293, 341, 611, 16155, 281, 264, 51556], "temperature": 0.0, "avg_logprob": -0.13756414488250135, "compression_ratio": 1.7234848484848484, "no_speech_prob": 0.007013281341642141}, {"id": 1866, "seek": 1088590, "start": 10909.74, "end": 10915.74, "text": " previous question of why we call graph convolutional networks convolutional so", "tokens": [51556, 3894, 1168, 295, 983, 321, 818, 4295, 45216, 304, 9590, 45216, 304, 370, 51856], "temperature": 0.0, "avg_logprob": -0.13756414488250135, "compression_ratio": 1.7234848484848484, "no_speech_prob": 0.007013281341642141}, {"id": 1867, "seek": 1091574, "start": 10915.78, "end": 10922.22, "text": " first of all a grid is a graph right so it's a particular type of a graph for", "tokens": [50366, 700, 295, 439, 257, 10748, 307, 257, 4295, 558, 370, 309, 311, 257, 1729, 2010, 295, 257, 4295, 337, 50688], "temperature": 0.0, "avg_logprob": -0.14623045659327244, "compression_ratio": 1.7747747747747749, "no_speech_prob": 0.0014085219008848071}, {"id": 1868, "seek": 1091574, "start": 10922.22, "end": 10926.06, "text": " simplicity I would like to assume that the grid has periodic boundary conditions", "tokens": [50688, 25632, 286, 576, 411, 281, 6552, 300, 264, 10748, 575, 27790, 12866, 4487, 50880], "temperature": 0.0, "avg_logprob": -0.14623045659327244, "compression_ratio": 1.7747747747747749, "no_speech_prob": 0.0014085219008848071}, {"id": 1869, "seek": 1091574, "start": 10926.06, "end": 10931.699999999999, "text": " so basically it's what is called the ring graph and the idea of geometric deep", "tokens": [50880, 370, 1936, 309, 311, 437, 307, 1219, 264, 4875, 4295, 293, 264, 1558, 295, 33246, 2452, 51162], "temperature": 0.0, "avg_logprob": -0.14623045659327244, "compression_ratio": 1.7747747747747749, "no_speech_prob": 0.0014085219008848071}, {"id": 1870, "seek": 1091574, "start": 10931.699999999999, "end": 10936.98, "text": " learning right this group based framework we had some domain and we have a", "tokens": [51162, 2539, 558, 341, 1594, 2361, 8388, 321, 632, 512, 9274, 293, 321, 362, 257, 51426], "temperature": 0.0, "avg_logprob": -0.14623045659327244, "compression_ratio": 1.7747747747747749, "no_speech_prob": 0.0014085219008848071}, {"id": 1871, "seek": 1091574, "start": 10936.98, "end": 10941.78, "text": " group that acted on the domain we have a signal that was defined on the domain so", "tokens": [51426, 1594, 300, 20359, 322, 264, 9274, 321, 362, 257, 6358, 300, 390, 7642, 322, 264, 9274, 370, 51666], "temperature": 0.0, "avg_logprob": -0.14623045659327244, "compression_ratio": 1.7747747747747749, "no_speech_prob": 0.0014085219008848071}, {"id": 1872, "seek": 1094178, "start": 10941.980000000001, "end": 10947.5, "text": " this is a general type of this mechanism that is often called lifting so now I", "tokens": [50374, 341, 307, 257, 2674, 2010, 295, 341, 7513, 300, 307, 2049, 1219, 15798, 370, 586, 286, 50650], "temperature": 0.0, "avg_logprob": -0.153461668226454, "compression_ratio": 1.8874458874458875, "no_speech_prob": 0.001758431433700025}, {"id": 1873, "seek": 1094178, "start": 10947.5, "end": 10950.86, "text": " have a linear operator so this can be anything right so this can be a non", "tokens": [50650, 362, 257, 8213, 12973, 370, 341, 393, 312, 1340, 558, 370, 341, 393, 312, 257, 2107, 50818], "temperature": 0.0, "avg_logprob": -0.153461668226454, "compression_ratio": 1.8874458874458875, "no_speech_prob": 0.001758431433700025}, {"id": 1874, "seek": 1094178, "start": 10950.86, "end": 10954.66, "text": " linear complicated thing here I have a linear operator so the group", "tokens": [50818, 8213, 6179, 551, 510, 286, 362, 257, 8213, 12973, 370, 264, 1594, 51008], "temperature": 0.0, "avg_logprob": -0.153461668226454, "compression_ratio": 1.8874458874458875, "no_speech_prob": 0.001758431433700025}, {"id": 1875, "seek": 1094178, "start": 10954.66, "end": 10959.5, "text": " representation attacks on functions defined on the domain okay and in the", "tokens": [51008, 10290, 8122, 322, 6828, 7642, 322, 264, 9274, 1392, 293, 294, 264, 51250], "temperature": 0.0, "avg_logprob": -0.153461668226454, "compression_ratio": 1.8874458874458875, "no_speech_prob": 0.001758431433700025}, {"id": 1876, "seek": 1094178, "start": 10959.5, "end": 10963.1, "text": " case of a grid this is just the shift operator so this is what they show in", "tokens": [51250, 1389, 295, 257, 10748, 341, 307, 445, 264, 5513, 12973, 370, 341, 307, 437, 436, 855, 294, 51430], "temperature": 0.0, "avg_logprob": -0.153461668226454, "compression_ratio": 1.8874458874458875, "no_speech_prob": 0.001758431433700025}, {"id": 1877, "seek": 1094178, "start": 10963.1, "end": 10971.140000000001, "text": " one dimension so just cyclically moves the elements of the vector", "tokens": [51430, 472, 10139, 370, 445, 19474, 984, 6067, 264, 4959, 295, 264, 8062, 51832], "temperature": 0.0, "avg_logprob": -0.153461668226454, "compression_ratio": 1.8874458874458875, "no_speech_prob": 0.001758431433700025}, {"id": 1878, "seek": 1097114, "start": 10972.099999999999, "end": 10975.779999999999, "text": " now another thing that you see in a grid is that it has a fixed neighborhood", "tokens": [50412, 586, 1071, 551, 300, 291, 536, 294, 257, 10748, 307, 300, 309, 575, 257, 6806, 7630, 50596], "temperature": 0.0, "avg_logprob": -0.15187792096819197, "compression_ratio": 1.8612244897959183, "no_speech_prob": 0.00036576579441316426}, {"id": 1879, "seek": 1097114, "start": 10975.779999999999, "end": 10979.3, "text": " structure right in this example every node is connected to exactly two", "tokens": [50596, 3877, 558, 294, 341, 1365, 633, 9984, 307, 4582, 281, 2293, 732, 50772], "temperature": 0.0, "avg_logprob": -0.15187792096819197, "compression_ratio": 1.8612244897959183, "no_speech_prob": 0.00036576579441316426}, {"id": 1880, "seek": 1097114, "start": 10979.3, "end": 10983.019999999999, "text": " neighbors and they are also ordered right so I always have the one before and", "tokens": [50772, 12512, 293, 436, 366, 611, 8866, 558, 370, 286, 1009, 362, 264, 472, 949, 293, 50958], "temperature": 0.0, "avg_logprob": -0.15187792096819197, "compression_ratio": 1.8612244897959183, "no_speech_prob": 0.00036576579441316426}, {"id": 1881, "seek": 1097114, "start": 10983.019999999999, "end": 10987.14, "text": " the one after in a two-dimensional grid I might have some partial order right so", "tokens": [50958, 264, 472, 934, 294, 257, 732, 12, 18759, 10748, 286, 1062, 362, 512, 14641, 1668, 558, 370, 51164], "temperature": 0.0, "avg_logprob": -0.15187792096819197, "compression_ratio": 1.8612244897959183, "no_speech_prob": 0.00036576579441316426}, {"id": 1882, "seek": 1097114, "start": 10987.14, "end": 10994.06, "text": " I have something on top and something on the bottom so in the past on the", "tokens": [51164, 286, 362, 746, 322, 1192, 293, 746, 322, 264, 2767, 370, 294, 264, 1791, 322, 264, 51510], "temperature": 0.0, "avg_logprob": -0.15187792096819197, "compression_ratio": 1.8612244897959183, "no_speech_prob": 0.00036576579441316426}, {"id": 1883, "seek": 1097114, "start": 10994.06, "end": 10997.26, "text": " general graph we had this kind of aggregation function right so we have the", "tokens": [51510, 2674, 4295, 321, 632, 341, 733, 295, 16743, 399, 2445, 558, 370, 321, 362, 264, 51670], "temperature": 0.0, "avg_logprob": -0.15187792096819197, "compression_ratio": 1.8612244897959183, "no_speech_prob": 0.00036576579441316426}, {"id": 1884, "seek": 1099726, "start": 10997.58, "end": 11003.380000000001, "text": " feature of the node itself and then we had a multi-set that was unordered of the", "tokens": [50380, 4111, 295, 264, 9984, 2564, 293, 550, 321, 632, 257, 4825, 12, 3854, 300, 390, 517, 765, 4073, 295, 264, 50670], "temperature": 0.0, "avg_logprob": -0.17370122792769452, "compression_ratio": 1.8246445497630333, "no_speech_prob": 0.0037976193707436323}, {"id": 1885, "seek": 1099726, "start": 11003.380000000001, "end": 11008.18, "text": " nearby features and because we didn't have any order in this multi-set we the", "tokens": [50670, 11184, 4122, 293, 570, 321, 994, 380, 362, 604, 1668, 294, 341, 4825, 12, 3854, 321, 264, 50910], "temperature": 0.0, "avg_logprob": -0.17370122792769452, "compression_ratio": 1.8246445497630333, "no_speech_prob": 0.0037976193707436323}, {"id": 1886, "seek": 1099726, "start": 11008.18, "end": 11011.380000000001, "text": " only thing we could do is to apply a symmetric function apply a permutation", "tokens": [50910, 787, 551, 321, 727, 360, 307, 281, 3079, 257, 32330, 2445, 3079, 257, 4784, 11380, 51070], "temperature": 0.0, "avg_logprob": -0.17370122792769452, "compression_ratio": 1.8246445497630333, "no_speech_prob": 0.0037976193707436323}, {"id": 1887, "seek": 1099726, "start": 11011.380000000001, "end": 11015.34, "text": " invariant function now we have a different situation now the nodes are", "tokens": [51070, 33270, 394, 2445, 586, 321, 362, 257, 819, 2590, 586, 264, 13891, 366, 51268], "temperature": 0.0, "avg_logprob": -0.17370122792769452, "compression_ratio": 1.8246445497630333, "no_speech_prob": 0.0037976193707436323}, {"id": 1888, "seek": 1099726, "start": 11015.34, "end": 11022.82, "text": " ordered right so we have a fixed order of x i minus one x i and x i plus one so", "tokens": [51268, 8866, 558, 370, 321, 362, 257, 6806, 1668, 295, 2031, 741, 3175, 472, 2031, 741, 293, 2031, 741, 1804, 472, 370, 51642], "temperature": 0.0, "avg_logprob": -0.17370122792769452, "compression_ratio": 1.8246445497630333, "no_speech_prob": 0.0037976193707436323}, {"id": 1889, "seek": 1102282, "start": 11022.86, "end": 11031.06, "text": " this function can be more general right so it doesn't need to be doesn't need to", "tokens": [50366, 341, 2445, 393, 312, 544, 2674, 558, 370, 309, 1177, 380, 643, 281, 312, 1177, 380, 643, 281, 50776], "temperature": 0.0, "avg_logprob": -0.11507437680218671, "compression_ratio": 1.718232044198895, "no_speech_prob": 0.001101521891541779}, {"id": 1890, "seek": 1102282, "start": 11031.06, "end": 11041.1, "text": " be to be symmetric and if this function is linear then we get the convolution", "tokens": [50776, 312, 281, 312, 32330, 293, 498, 341, 2445, 307, 8213, 550, 321, 483, 264, 45216, 51278], "temperature": 0.0, "avg_logprob": -0.11507437680218671, "compression_ratio": 1.718232044198895, "no_speech_prob": 0.001101521891541779}, {"id": 1891, "seek": 1102282, "start": 11041.1, "end": 11045.82, "text": " right and if I write it as a matrix vector product then it looks like this", "tokens": [51278, 558, 293, 498, 286, 2464, 309, 382, 257, 8141, 8062, 1674, 550, 309, 1542, 411, 341, 51514], "temperature": 0.0, "avg_logprob": -0.11507437680218671, "compression_ratio": 1.718232044198895, "no_speech_prob": 0.001101521891541779}, {"id": 1892, "seek": 1102282, "start": 11045.82, "end": 11052.46, "text": " so it's a special matrix which has fixed elements along the diagonal right so", "tokens": [51514, 370, 309, 311, 257, 2121, 8141, 597, 575, 6806, 4959, 2051, 264, 21539, 558, 370, 51846], "temperature": 0.0, "avg_logprob": -0.11507437680218671, "compression_ratio": 1.718232044198895, "no_speech_prob": 0.001101521891541779}, {"id": 1893, "seek": 1105246, "start": 11052.5, "end": 11056.939999999999, "text": " this is the the the local weight-sharing that have been convolutional neural", "tokens": [50366, 341, 307, 264, 264, 264, 2654, 3364, 12, 2716, 1921, 300, 362, 668, 45216, 304, 18161, 50588], "temperature": 0.0, "avg_logprob": -0.24203062465048245, "compression_ratio": 1.8848920863309353, "no_speech_prob": 0.002286154543980956}, {"id": 1894, "seek": 1105246, "start": 11056.939999999999, "end": 11060.939999999999, "text": " networks so this is a special type of matrices they're called circumvent", "tokens": [50588, 9590, 370, 341, 307, 257, 2121, 2010, 295, 32284, 436, 434, 1219, 7125, 2475, 50788], "temperature": 0.0, "avg_logprob": -0.24203062465048245, "compression_ratio": 1.8848920863309353, "no_speech_prob": 0.002286154543980956}, {"id": 1895, "seek": 1105246, "start": 11060.939999999999, "end": 11065.9, "text": " matrices right or convolutions so it's synonym you take a vector of", "tokens": [50788, 32284, 558, 420, 3754, 15892, 370, 309, 311, 5451, 12732, 291, 747, 257, 8062, 295, 51036], "temperature": 0.0, "avg_logprob": -0.24203062465048245, "compression_ratio": 1.8848920863309353, "no_speech_prob": 0.002286154543980956}, {"id": 1896, "seek": 1105246, "start": 11065.9, "end": 11070.939999999999, "text": " parameters right let's call it data and you create these matrix by cyclically", "tokens": [51036, 9834, 558, 718, 311, 818, 309, 1412, 293, 291, 1884, 613, 8141, 538, 19474, 984, 51288], "temperature": 0.0, "avg_logprob": -0.24203062465048245, "compression_ratio": 1.8848920863309353, "no_speech_prob": 0.002286154543980956}, {"id": 1897, "seek": 1105246, "start": 11070.939999999999, "end": 11074.66, "text": " shifting by one position these vector of parameters and depending it as columns", "tokens": [51288, 17573, 538, 472, 2535, 613, 8062, 295, 9834, 293, 5413, 309, 382, 13766, 51474], "temperature": 0.0, "avg_logprob": -0.24203062465048245, "compression_ratio": 1.8848920863309353, "no_speech_prob": 0.002286154543980956}, {"id": 1898, "seek": 1105246, "start": 11074.66, "end": 11078.779999999999, "text": " that's how you get you get these matrix again I'm assuming periodic boundary", "tokens": [51474, 300, 311, 577, 291, 483, 291, 483, 613, 8141, 797, 286, 478, 11926, 27790, 12866, 51680], "temperature": 0.0, "avg_logprob": -0.24203062465048245, "compression_ratio": 1.8848920863309353, "no_speech_prob": 0.002286154543980956}, {"id": 1899, "seek": 1105246, "start": 11078.779999999999, "end": 11082.179999999998, "text": " conditions so technically speaking it's not a convergence it's a circle", "tokens": [51680, 4487, 370, 12120, 4124, 309, 311, 406, 257, 32181, 309, 311, 257, 6329, 51850], "temperature": 0.0, "avg_logprob": -0.24203062465048245, "compression_ratio": 1.8848920863309353, "no_speech_prob": 0.002286154543980956}, {"id": 1900, "seek": 1108218, "start": 11082.220000000001, "end": 11089.66, "text": " convolution or a cyclic convolution but just to make things simpler okay now one", "tokens": [50366, 45216, 420, 257, 38154, 1050, 45216, 457, 445, 281, 652, 721, 18587, 1392, 586, 472, 50738], "temperature": 0.0, "avg_logprob": -0.18650052880728116, "compression_ratio": 1.8537735849056605, "no_speech_prob": 0.0016459872713312507}, {"id": 1901, "seek": 1108218, "start": 11089.66, "end": 11094.06, "text": " thing that you first thing that you learn in in algebra one-on-one is that", "tokens": [50738, 551, 300, 291, 700, 551, 300, 291, 1466, 294, 294, 21989, 472, 12, 266, 12, 546, 307, 300, 50958], "temperature": 0.0, "avg_logprob": -0.18650052880728116, "compression_ratio": 1.8537735849056605, "no_speech_prob": 0.0016459872713312507}, {"id": 1902, "seek": 1108218, "start": 11094.06, "end": 11099.58, "text": " matrix multiplication is not commutative right a b is not equal to b a but with", "tokens": [50958, 8141, 27290, 307, 406, 800, 325, 1166, 558, 257, 272, 307, 406, 2681, 281, 272, 257, 457, 365, 51234], "temperature": 0.0, "avg_logprob": -0.18650052880728116, "compression_ratio": 1.8537735849056605, "no_speech_prob": 0.0016459872713312507}, {"id": 1903, "seek": 1108218, "start": 11099.58, "end": 11103.9, "text": " these matrices with convolutions there are with circumvent matrices that's not", "tokens": [51234, 613, 32284, 365, 3754, 15892, 456, 366, 365, 7125, 2475, 32284, 300, 311, 406, 51450], "temperature": 0.0, "avg_logprob": -0.18650052880728116, "compression_ratio": 1.8537735849056605, "no_speech_prob": 0.0016459872713312507}, {"id": 1904, "seek": 1108218, "start": 11103.9, "end": 11109.26, "text": " the case it's actually a special type of matrices that do commute right and in", "tokens": [51450, 264, 1389, 309, 311, 767, 257, 2121, 2010, 295, 32284, 300, 360, 36750, 558, 293, 294, 51718], "temperature": 0.0, "avg_logprob": -0.18650052880728116, "compression_ratio": 1.8537735849056605, "no_speech_prob": 0.0016459872713312507}, {"id": 1905, "seek": 1110926, "start": 11109.26, "end": 11115.06, "text": " particular they commute with one of them which is the shift operator right so a", "tokens": [50364, 1729, 436, 36750, 365, 472, 295, 552, 597, 307, 264, 5513, 12973, 558, 370, 257, 50654], "temperature": 0.0, "avg_logprob": -0.16699140722101385, "compression_ratio": 2.057777777777778, "no_speech_prob": 0.0019049772527068853}, {"id": 1906, "seek": 1110926, "start": 11115.06, "end": 11119.9, "text": " shift is also a circumvent matrix right or also a convolution right so looks", "tokens": [50654, 5513, 307, 611, 257, 7125, 2475, 8141, 558, 420, 611, 257, 45216, 558, 370, 1542, 50896], "temperature": 0.0, "avg_logprob": -0.16699140722101385, "compression_ratio": 2.057777777777778, "no_speech_prob": 0.0019049772527068853}, {"id": 1907, "seek": 1110926, "start": 11119.9, "end": 11126.78, "text": " like this so what does it mean that that a convolution commutes with a shift so", "tokens": [50896, 411, 341, 370, 437, 775, 309, 914, 300, 300, 257, 45216, 800, 1819, 365, 257, 5513, 370, 51240], "temperature": 0.0, "avg_logprob": -0.16699140722101385, "compression_ratio": 2.057777777777778, "no_speech_prob": 0.0019049772527068853}, {"id": 1908, "seek": 1110926, "start": 11126.78, "end": 11130.14, "text": " this is what we call shift-equivariance right so in other words I can first", "tokens": [51240, 341, 307, 437, 321, 818, 5513, 12, 12816, 592, 3504, 719, 558, 370, 294, 661, 2283, 286, 393, 700, 51408], "temperature": 0.0, "avg_logprob": -0.16699140722101385, "compression_ratio": 2.057777777777778, "no_speech_prob": 0.0019049772527068853}, {"id": 1909, "seek": 1110926, "start": 11130.14, "end": 11133.18, "text": " apply convolution and then shift or I can first apply shift and then", "tokens": [51408, 3079, 45216, 293, 550, 5513, 420, 286, 393, 700, 3079, 5513, 293, 550, 51560], "temperature": 0.0, "avg_logprob": -0.16699140722101385, "compression_ratio": 2.057777777777778, "no_speech_prob": 0.0019049772527068853}, {"id": 1910, "seek": 1110926, "start": 11133.18, "end": 11138.7, "text": " convolution the result will be the same right so convolution is shift-equivariant", "tokens": [51560, 45216, 264, 1874, 486, 312, 264, 912, 558, 370, 45216, 307, 5513, 12, 12816, 592, 3504, 394, 51836], "temperature": 0.0, "avg_logprob": -0.16699140722101385, "compression_ratio": 2.057777777777778, "no_speech_prob": 0.0019049772527068853}, {"id": 1911, "seek": 1113870, "start": 11138.7, "end": 11144.220000000001, "text": " you can show the other way around right so you can show that if you have shift", "tokens": [50364, 291, 393, 855, 264, 661, 636, 926, 558, 370, 291, 393, 855, 300, 498, 291, 362, 5513, 50640], "temperature": 0.0, "avg_logprob": -0.17513634608342096, "compression_ratio": 2.0427807486631018, "no_speech_prob": 0.002167274011299014}, {"id": 1912, "seek": 1113870, "start": 11144.220000000001, "end": 11149.140000000001, "text": " equivariant linear operations so I take a matrix and I tell you that it's", "tokens": [50640, 1267, 592, 3504, 394, 8213, 7705, 370, 286, 747, 257, 8141, 293, 286, 980, 291, 300, 309, 311, 50886], "temperature": 0.0, "avg_logprob": -0.17513634608342096, "compression_ratio": 2.0427807486631018, "no_speech_prob": 0.002167274011299014}, {"id": 1913, "seek": 1113870, "start": 11149.140000000001, "end": 11154.42, "text": " shift-equivariant you can show that it must be a convolution right so basically", "tokens": [50886, 5513, 12, 12816, 592, 3504, 394, 291, 393, 855, 300, 309, 1633, 312, 257, 45216, 558, 370, 1936, 51150], "temperature": 0.0, "avg_logprob": -0.17513634608342096, "compression_ratio": 2.0427807486631018, "no_speech_prob": 0.002167274011299014}, {"id": 1914, "seek": 1113870, "start": 11154.42, "end": 11159.54, "text": " convolution emerges from considerations of translational symmetry right so the", "tokens": [51150, 45216, 38965, 490, 24070, 295, 5105, 1478, 25440, 558, 370, 264, 51406], "temperature": 0.0, "avg_logprob": -0.17513634608342096, "compression_ratio": 2.0427807486631018, "no_speech_prob": 0.002167274011299014}, {"id": 1915, "seek": 1113870, "start": 11159.54, "end": 11164.26, "text": " only linear operation that that is shift-equivariant is convolution so", "tokens": [51406, 787, 8213, 6916, 300, 300, 307, 5513, 12, 12816, 592, 3504, 394, 307, 45216, 370, 51642], "temperature": 0.0, "avg_logprob": -0.17513634608342096, "compression_ratio": 2.0427807486631018, "no_speech_prob": 0.002167274011299014}, {"id": 1916, "seek": 1116426, "start": 11164.26, "end": 11170.94, "text": " convolution is the only thing that satisfies this property and we've seen", "tokens": [50364, 45216, 307, 264, 787, 551, 300, 44271, 341, 4707, 293, 321, 600, 1612, 50698], "temperature": 0.0, "avg_logprob": -0.21129161496705648, "compression_ratio": 1.7676767676767677, "no_speech_prob": 0.002947605913504958}, {"id": 1917, "seek": 1116426, "start": 11170.94, "end": 11175.26, "text": " again this geometric deep learning blueprint so allow me to show it again", "tokens": [50698, 797, 341, 33246, 2452, 2539, 35868, 370, 2089, 385, 281, 855, 309, 797, 50914], "temperature": 0.0, "avg_logprob": -0.21129161496705648, "compression_ratio": 1.7676767676767677, "no_speech_prob": 0.002947605913504958}, {"id": 1918, "seek": 1116426, "start": 11175.26, "end": 11179.98, "text": " so we have a grid we have a translation group its representation is the shift", "tokens": [50914, 370, 321, 362, 257, 10748, 321, 362, 257, 12853, 1594, 1080, 10290, 307, 264, 5513, 51150], "temperature": 0.0, "avg_logprob": -0.21129161496705648, "compression_ratio": 1.7676767676767677, "no_speech_prob": 0.002947605913504958}, {"id": 1919, "seek": 1116426, "start": 11179.98, "end": 11185.06, "text": " operator so the convolution is a function that is", "tokens": [51150, 12973, 370, 264, 45216, 307, 257, 2445, 300, 307, 51404], "temperature": 0.0, "avg_logprob": -0.21129161496705648, "compression_ratio": 1.7676767676767677, "no_speech_prob": 0.002947605913504958}, {"id": 1920, "seek": 1116426, "start": 11185.06, "end": 11191.86, "text": " equivariant with respect to this to this group now we also know that there", "tokens": [51404, 1267, 592, 3504, 394, 365, 3104, 281, 341, 281, 341, 1594, 586, 321, 611, 458, 300, 456, 51744], "temperature": 0.0, "avg_logprob": -0.21129161496705648, "compression_ratio": 1.7676767676767677, "no_speech_prob": 0.002947605913504958}, {"id": 1921, "seek": 1119186, "start": 11191.86, "end": 11195.34, "text": " is an intimate relation between the Fourier transform and the convolution", "tokens": [50364, 307, 364, 20215, 9721, 1296, 264, 36810, 4088, 293, 264, 45216, 50538], "temperature": 0.0, "avg_logprob": -0.15405354457618917, "compression_ratio": 1.9381818181818182, "no_speech_prob": 0.0046114325523376465}, {"id": 1922, "seek": 1119186, "start": 11195.34, "end": 11199.62, "text": " right and let's actually try to understand what the Fourier transform is", "tokens": [50538, 558, 293, 718, 311, 767, 853, 281, 1223, 437, 264, 36810, 4088, 307, 50752], "temperature": 0.0, "avg_logprob": -0.15405354457618917, "compression_ratio": 1.9381818181818182, "no_speech_prob": 0.0046114325523376465}, {"id": 1923, "seek": 1119186, "start": 11199.62, "end": 11204.34, "text": " where it comes from so we know from algebra again that commuting matrices", "tokens": [50752, 689, 309, 1487, 490, 370, 321, 458, 490, 21989, 797, 300, 800, 10861, 32284, 50988], "temperature": 0.0, "avg_logprob": -0.15405354457618917, "compression_ratio": 1.9381818181818182, "no_speech_prob": 0.0046114325523376465}, {"id": 1924, "seek": 1119186, "start": 11204.34, "end": 11208.140000000001, "text": " are jointly diagonalizable it means that they have the same eigenvectors or more", "tokens": [50988, 366, 46557, 21539, 22395, 309, 1355, 300, 436, 362, 264, 912, 10446, 303, 5547, 420, 544, 51178], "temperature": 0.0, "avg_logprob": -0.15405354457618917, "compression_ratio": 1.9381818181818182, "no_speech_prob": 0.0046114325523376465}, {"id": 1925, "seek": 1119186, "start": 11208.140000000001, "end": 11212.54, "text": " correct eigenspaces but here we assume that the multiplicity of eigenvalues is", "tokens": [51178, 3006, 10446, 4952, 2116, 457, 510, 321, 6552, 300, 264, 17596, 507, 295, 10446, 46033, 307, 51398], "temperature": 0.0, "avg_logprob": -0.15405354457618917, "compression_ratio": 1.9381818181818182, "no_speech_prob": 0.0046114325523376465}, {"id": 1926, "seek": 1119186, "start": 11212.54, "end": 11217.18, "text": " trivial so they actually have the same eigenvectors and the only different", "tokens": [51398, 26703, 370, 436, 767, 362, 264, 912, 10446, 303, 5547, 293, 264, 787, 819, 51630], "temperature": 0.0, "avg_logprob": -0.15405354457618917, "compression_ratio": 1.9381818181818182, "no_speech_prob": 0.0046114325523376465}, {"id": 1927, "seek": 1119186, "start": 11217.18, "end": 11220.78, "text": " different eigenvalues right so all commutative matrices satisfy this property", "tokens": [51630, 819, 10446, 46033, 558, 370, 439, 800, 325, 1166, 32284, 19319, 341, 4707, 51810], "temperature": 0.0, "avg_logprob": -0.15405354457618917, "compression_ratio": 1.9381818181818182, "no_speech_prob": 0.0046114325523376465}, {"id": 1928, "seek": 1122078, "start": 11220.78, "end": 11226.74, "text": " so if I have a set of matrices that commute pair wisely then then this is", "tokens": [50364, 370, 498, 286, 362, 257, 992, 295, 32284, 300, 36750, 6119, 37632, 550, 550, 341, 307, 50662], "temperature": 0.0, "avg_logprob": -0.18266491223407047, "compression_ratio": 1.8774509803921569, "no_speech_prob": 0.002175255911424756}, {"id": 1929, "seek": 1122078, "start": 11226.74, "end": 11231.02, "text": " the case right and this is the case for for convolutions or for circuit matrices", "tokens": [50662, 264, 1389, 558, 293, 341, 307, 264, 1389, 337, 337, 3754, 15892, 420, 337, 9048, 32284, 50876], "temperature": 0.0, "avg_logprob": -0.18266491223407047, "compression_ratio": 1.8774509803921569, "no_speech_prob": 0.002175255911424756}, {"id": 1930, "seek": 1122078, "start": 11231.02, "end": 11238.460000000001, "text": " so we can pick up one of these matrices right and compute its eigenvectors", "tokens": [50876, 370, 321, 393, 1888, 493, 472, 295, 613, 32284, 558, 293, 14722, 1080, 10446, 303, 5547, 51248], "temperature": 0.0, "avg_logprob": -0.18266491223407047, "compression_ratio": 1.8774509803921569, "no_speech_prob": 0.002175255911424756}, {"id": 1931, "seek": 1122078, "start": 11238.460000000001, "end": 11243.5, "text": " right and we know that all of them will have the same and it's convenient to", "tokens": [51248, 558, 293, 321, 458, 300, 439, 295, 552, 486, 362, 264, 912, 293, 309, 311, 10851, 281, 51500], "temperature": 0.0, "avg_logprob": -0.18266491223407047, "compression_ratio": 1.8774509803921569, "no_speech_prob": 0.002175255911424756}, {"id": 1932, "seek": 1122078, "start": 11243.5, "end": 11250.300000000001, "text": " look at the shift operator right at the matrix S and if we compute the eigen", "tokens": [51500, 574, 412, 264, 5513, 12973, 558, 412, 264, 8141, 318, 293, 498, 321, 14722, 264, 10446, 51840], "temperature": 0.0, "avg_logprob": -0.18266491223407047, "compression_ratio": 1.8774509803921569, "no_speech_prob": 0.002175255911424756}, {"id": 1933, "seek": 1125030, "start": 11250.3, "end": 11253.5, "text": " vectors of the shift operator you can do it by hand it's actually not difficult", "tokens": [50364, 18875, 295, 264, 5513, 12973, 291, 393, 360, 309, 538, 1011, 309, 311, 767, 406, 2252, 50524], "temperature": 0.0, "avg_logprob": -0.14260102281666764, "compression_ratio": 1.9567099567099566, "no_speech_prob": 0.005311828106641769}, {"id": 1934, "seek": 1125030, "start": 11253.5, "end": 11258.22, "text": " you see that they look like these complex exponentials so this is exactly the", "tokens": [50524, 291, 536, 300, 436, 574, 411, 613, 3997, 21510, 82, 370, 341, 307, 2293, 264, 50760], "temperature": 0.0, "avg_logprob": -0.14260102281666764, "compression_ratio": 1.9567099567099566, "no_speech_prob": 0.005311828106641769}, {"id": 1935, "seek": 1125030, "start": 11258.22, "end": 11266.859999999999, "text": " Fourier transform or more correctly the discrete Fourier transform so the", "tokens": [50760, 36810, 4088, 420, 544, 8944, 264, 27706, 36810, 4088, 370, 264, 51192], "temperature": 0.0, "avg_logprob": -0.14260102281666764, "compression_ratio": 1.9567099567099566, "no_speech_prob": 0.005311828106641769}, {"id": 1936, "seek": 1125030, "start": 11266.859999999999, "end": 11270.98, "text": " question of course that remains is what the eigenvalues are right so we know", "tokens": [51192, 1168, 295, 1164, 300, 7023, 307, 437, 264, 10446, 46033, 366, 558, 370, 321, 458, 51398], "temperature": 0.0, "avg_logprob": -0.14260102281666764, "compression_ratio": 1.9567099567099566, "no_speech_prob": 0.005311828106641769}, {"id": 1937, "seek": 1125030, "start": 11270.98, "end": 11274.019999999999, "text": " that the eigenvectors of all conversions are the discrete Fourier", "tokens": [51398, 300, 264, 10446, 303, 5547, 295, 439, 42256, 366, 264, 27706, 36810, 51550], "temperature": 0.0, "avg_logprob": -0.14260102281666764, "compression_ratio": 1.9567099567099566, "no_speech_prob": 0.005311828106641769}, {"id": 1938, "seek": 1125030, "start": 11274.019999999999, "end": 11279.66, "text": " transform so these complex sinusoids but the eigenvalues also you can show it", "tokens": [51550, 4088, 370, 613, 3997, 3343, 24431, 3742, 457, 264, 10446, 46033, 611, 291, 393, 855, 309, 51832], "temperature": 0.0, "avg_logprob": -0.14260102281666764, "compression_ratio": 1.9567099567099566, "no_speech_prob": 0.005311828106641769}, {"id": 1939, "seek": 1127966, "start": 11279.66, "end": 11283.9, "text": " are the Fourier transform of the vector theta that forms each of these matrices", "tokens": [50364, 366, 264, 36810, 4088, 295, 264, 8062, 9725, 300, 6422, 1184, 295, 613, 32284, 50576], "temperature": 0.0, "avg_logprob": -0.18389965057373048, "compression_ratio": 2.0789473684210527, "no_speech_prob": 0.0038881944492459297}, {"id": 1940, "seek": 1127966, "start": 11283.9, "end": 11288.74, "text": " and this gives us this dual relationship between the Fourier transform and the", "tokens": [50576, 293, 341, 2709, 505, 341, 11848, 2480, 1296, 264, 36810, 4088, 293, 264, 50818], "temperature": 0.0, "avg_logprob": -0.18389965057373048, "compression_ratio": 2.0789473684210527, "no_speech_prob": 0.0038881944492459297}, {"id": 1941, "seek": 1127966, "start": 11288.74, "end": 11293.06, "text": " convolution so if I have a signal x I can do convolution in the spatial domain", "tokens": [50818, 45216, 370, 498, 286, 362, 257, 6358, 2031, 286, 393, 360, 45216, 294, 264, 23598, 9274, 51034], "temperature": 0.0, "avg_logprob": -0.18389965057373048, "compression_ratio": 2.0789473684210527, "no_speech_prob": 0.0038881944492459297}, {"id": 1942, "seek": 1127966, "start": 11293.06, "end": 11297.78, "text": " by multiplying by a circuit matrix or I can do it in the in the Fourier domain", "tokens": [51034, 538, 30955, 538, 257, 9048, 8141, 420, 286, 393, 360, 309, 294, 264, 294, 264, 36810, 9274, 51270], "temperature": 0.0, "avg_logprob": -0.18389965057373048, "compression_ratio": 2.0789473684210527, "no_speech_prob": 0.0038881944492459297}, {"id": 1943, "seek": 1127966, "start": 11297.78, "end": 11302.02, "text": " I can compute the Fourier transform and there the Fourier transform diagonalizes", "tokens": [51270, 286, 393, 14722, 264, 36810, 4088, 293, 456, 264, 36810, 4088, 21539, 5660, 51482], "temperature": 0.0, "avg_logprob": -0.18389965057373048, "compression_ratio": 2.0789473684210527, "no_speech_prob": 0.0038881944492459297}, {"id": 1944, "seek": 1127966, "start": 11302.02, "end": 11306.98, "text": " the convolution so it becomes an element wise product right so basically the", "tokens": [51482, 264, 45216, 370, 309, 3643, 364, 4478, 10829, 1674, 558, 370, 1936, 264, 51730], "temperature": 0.0, "avg_logprob": -0.18389965057373048, "compression_ratio": 2.0789473684210527, "no_speech_prob": 0.0038881944492459297}, {"id": 1945, "seek": 1130698, "start": 11306.98, "end": 11311.18, "text": " product of two Fourier transforms is the Fourier transform of the convolution", "tokens": [50364, 1674, 295, 732, 36810, 35592, 307, 264, 36810, 4088, 295, 264, 45216, 50574], "temperature": 0.0, "avg_logprob": -0.1825914068536444, "compression_ratio": 2.039647577092511, "no_speech_prob": 0.00254768505692482}, {"id": 1946, "seek": 1130698, "start": 11311.18, "end": 11317.26, "text": " right and typically in signal processing the filters are already designed in the", "tokens": [50574, 558, 293, 5850, 294, 6358, 9007, 264, 15995, 366, 1217, 4761, 294, 264, 50878], "temperature": 0.0, "avg_logprob": -0.1825914068536444, "compression_ratio": 2.039647577092511, "no_speech_prob": 0.00254768505692482}, {"id": 1947, "seek": 1130698, "start": 11317.26, "end": 11322.18, "text": " Fourier domain this is bread and butter of signal processing so the the the", "tokens": [50878, 36810, 9274, 341, 307, 5961, 293, 5517, 295, 6358, 9007, 370, 264, 264, 264, 51124], "temperature": 0.0, "avg_logprob": -0.1825914068536444, "compression_ratio": 2.039647577092511, "no_speech_prob": 0.00254768505692482}, {"id": 1948, "seek": 1130698, "start": 11322.18, "end": 11326.66, "text": " advantage of using the Fourier transform because this operation usually on", "tokens": [51124, 5002, 295, 1228, 264, 36810, 4088, 570, 341, 6916, 2673, 322, 51348], "temperature": 0.0, "avg_logprob": -0.1825914068536444, "compression_ratio": 2.039647577092511, "no_speech_prob": 0.00254768505692482}, {"id": 1949, "seek": 1130698, "start": 11326.66, "end": 11331.82, "text": " grids can be done efficiently so instead of n squared operations as you would", "tokens": [51348, 677, 3742, 393, 312, 1096, 19621, 370, 2602, 295, 297, 8889, 7705, 382, 291, 576, 51606], "temperature": 0.0, "avg_logprob": -0.1825914068536444, "compression_ratio": 2.039647577092511, "no_speech_prob": 0.00254768505692482}, {"id": 1950, "seek": 1130698, "start": 11331.82, "end": 11336.26, "text": " typically require here because the Fourier transform the the the matrix has", "tokens": [51606, 5850, 3651, 510, 570, 264, 36810, 4088, 264, 264, 264, 8141, 575, 51828], "temperature": 0.0, "avg_logprob": -0.1825914068536444, "compression_ratio": 2.039647577092511, "no_speech_prob": 0.00254768505692482}, {"id": 1951, "seek": 1133626, "start": 11336.26, "end": 11341.26, "text": " a very redundant structure you can you can avoid these explicit multiplications", "tokens": [50364, 257, 588, 40997, 3877, 291, 393, 291, 393, 5042, 613, 13691, 17596, 763, 50614], "temperature": 0.0, "avg_logprob": -0.18396885578448957, "compression_ratio": 1.8098859315589353, "no_speech_prob": 0.00388406403362751}, {"id": 1952, "seek": 1133626, "start": 11341.26, "end": 11345.06, "text": " you can reuse some of the multiplications and do it in n log n operations so", "tokens": [50614, 291, 393, 26225, 512, 295, 264, 17596, 763, 293, 360, 309, 294, 297, 3565, 297, 7705, 370, 50804], "temperature": 0.0, "avg_logprob": -0.18396885578448957, "compression_ratio": 1.8098859315589353, "no_speech_prob": 0.00388406403362751}, {"id": 1953, "seek": 1133626, "start": 11345.06, "end": 11348.74, "text": " there are classes of algorithms that are called fast Fourier transforms and this", "tokens": [50804, 456, 366, 5359, 295, 14642, 300, 366, 1219, 2370, 36810, 35592, 293, 341, 50988], "temperature": 0.0, "avg_logprob": -0.18396885578448957, "compression_ratio": 1.8098859315589353, "no_speech_prob": 0.00388406403362751}, {"id": 1954, "seek": 1133626, "start": 11348.74, "end": 11353.460000000001, "text": " is from the approximately the sixes when this was derived with the most famous", "tokens": [50988, 307, 490, 264, 10447, 264, 2309, 279, 562, 341, 390, 18949, 365, 264, 881, 4618, 51224], "temperature": 0.0, "avg_logprob": -0.18396885578448957, "compression_ratio": 1.8098859315589353, "no_speech_prob": 0.00388406403362751}, {"id": 1955, "seek": 1133626, "start": 11353.460000000001, "end": 11358.22, "text": " algorithm is by Kuli and Tuki this is how signal processing has been done and", "tokens": [51224, 9284, 307, 538, 591, 25484, 293, 314, 11788, 341, 307, 577, 6358, 9007, 575, 668, 1096, 293, 51462], "temperature": 0.0, "avg_logprob": -0.18396885578448957, "compression_ratio": 1.8098859315589353, "no_speech_prob": 0.00388406403362751}, {"id": 1956, "seek": 1133626, "start": 11358.22, "end": 11363.5, "text": " you have it everywhere from your stereo to your iPhone from your computer so this", "tokens": [51462, 291, 362, 309, 5315, 490, 428, 29029, 281, 428, 7252, 490, 428, 3820, 370, 341, 51726], "temperature": 0.0, "avg_logprob": -0.18396885578448957, "compression_ratio": 1.8098859315589353, "no_speech_prob": 0.00388406403362751}, {"id": 1957, "seek": 1136350, "start": 11363.5, "end": 11367.26, "text": " is how it's done you cannot do it on graphs because on graphs the analogy of", "tokens": [50364, 307, 577, 309, 311, 1096, 291, 2644, 360, 309, 322, 24877, 570, 322, 24877, 264, 21663, 295, 50552], "temperature": 0.0, "avg_logprob": -0.23224087556203207, "compression_ratio": 1.8, "no_speech_prob": 0.00758969085291028}, {"id": 1958, "seek": 1136350, "start": 11367.26, "end": 11372.1, "text": " the Fourier transform would be the eigenvectors of either the adjacency", "tokens": [50552, 264, 36810, 4088, 576, 312, 264, 10446, 303, 5547, 295, 2139, 264, 22940, 3020, 50794], "temperature": 0.0, "avg_logprob": -0.23224087556203207, "compression_ratio": 1.8, "no_speech_prob": 0.00758969085291028}, {"id": 1959, "seek": 1136350, "start": 11372.1, "end": 11375.98, "text": " matrix or the Laplacian matrix so if they are symmetric they have orthogonal", "tokens": [50794, 8141, 420, 264, 2369, 564, 326, 952, 8141, 370, 498, 436, 366, 32330, 436, 362, 41488, 50988], "temperature": 0.0, "avg_logprob": -0.23224087556203207, "compression_ratio": 1.8, "no_speech_prob": 0.00758969085291028}, {"id": 1960, "seek": 1136350, "start": 11375.98, "end": 11380.1, "text": " eigen decomposition but these matrices do not have these redundant structures so", "tokens": [50988, 10446, 48356, 457, 613, 32284, 360, 406, 362, 613, 40997, 9227, 370, 51194], "temperature": 0.0, "avg_logprob": -0.23224087556203207, "compression_ratio": 1.8, "no_speech_prob": 0.00758969085291028}, {"id": 1961, "seek": 1136350, "start": 11380.1, "end": 11385.3, "text": " the Fourier transform has n squared complexity dense matrix multiplication", "tokens": [51194, 264, 36810, 4088, 575, 297, 8889, 14024, 18011, 8141, 27290, 51454], "temperature": 0.0, "avg_logprob": -0.23224087556203207, "compression_ratio": 1.8, "no_speech_prob": 0.00758969085291028}, {"id": 1962, "seek": 1136350, "start": 11385.3, "end": 11391.54, "text": " and actually some of the early crafting of electrical architectures came from", "tokens": [51454, 293, 767, 512, 295, 264, 2440, 29048, 295, 12147, 6331, 1303, 1361, 490, 51766], "temperature": 0.0, "avg_logprob": -0.23224087556203207, "compression_ratio": 1.8, "no_speech_prob": 0.00758969085291028}, {"id": 1963, "seek": 1139154, "start": 11391.54, "end": 11396.380000000001, "text": " this domain of signal processing on graphs where that that used the eigenvectors", "tokens": [50364, 341, 9274, 295, 6358, 9007, 322, 24877, 689, 300, 300, 1143, 264, 10446, 303, 5547, 50606], "temperature": 0.0, "avg_logprob": -0.22327635023328993, "compression_ratio": 1.8309859154929577, "no_speech_prob": 0.0021423082798719406}, {"id": 1964, "seek": 1139154, "start": 11396.380000000001, "end": 11400.980000000001, "text": " of the Laplacian or the adjacency matrix as an analogy of the Fourier transform", "tokens": [50606, 295, 264, 2369, 564, 326, 952, 420, 264, 22940, 3020, 8141, 382, 364, 21663, 295, 264, 36810, 4088, 50836], "temperature": 0.0, "avg_logprob": -0.22327635023328993, "compression_ratio": 1.8309859154929577, "no_speech_prob": 0.0021423082798719406}, {"id": 1965, "seek": 1139154, "start": 11400.980000000001, "end": 11409.140000000001, "text": " so the difference in the case of the in the Euclidean case on the grid there is", "tokens": [50836, 370, 264, 2649, 294, 264, 1389, 295, 264, 294, 264, 462, 1311, 31264, 282, 1389, 322, 264, 10748, 456, 307, 51244], "temperature": 0.0, "avg_logprob": -0.22327635023328993, "compression_ratio": 1.8309859154929577, "no_speech_prob": 0.0021423082798719406}, {"id": 1966, "seek": 1139154, "start": 11409.140000000001, "end": 11412.220000000001, "text": " no difference between the two right so the Laplacian is also obviously a", "tokens": [51244, 572, 2649, 1296, 264, 732, 558, 370, 264, 2369, 564, 326, 952, 307, 611, 2745, 257, 51398], "temperature": 0.0, "avg_logprob": -0.22327635023328993, "compression_ratio": 1.8309859154929577, "no_speech_prob": 0.0021423082798719406}, {"id": 1967, "seek": 1139154, "start": 11412.220000000001, "end": 11418.140000000001, "text": " circuant operator circuant matrix and so is the shift right or the adjacency", "tokens": [51398, 3510, 84, 394, 12973, 3510, 84, 394, 8141, 293, 370, 307, 264, 5513, 558, 420, 264, 22940, 3020, 51694], "temperature": 0.0, "avg_logprob": -0.22327635023328993, "compression_ratio": 1.8309859154929577, "no_speech_prob": 0.0021423082798719406}, {"id": 1968, "seek": 1141814, "start": 11418.14, "end": 11422.82, "text": " matrix of of the ring graph which happens to be the shift operator they all", "tokens": [50364, 8141, 295, 295, 264, 4875, 4295, 597, 2314, 281, 312, 264, 5513, 12973, 436, 439, 50598], "temperature": 0.0, "avg_logprob": -0.17631108733429307, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.005683485418558121}, {"id": 1969, "seek": 1141814, "start": 11422.82, "end": 11425.82, "text": " commute so they have the same eigenvectors on the general graph they are", "tokens": [50598, 36750, 370, 436, 362, 264, 912, 10446, 303, 5547, 322, 264, 2674, 4295, 436, 366, 50748], "temperature": 0.0, "avg_logprob": -0.17631108733429307, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.005683485418558121}, {"id": 1970, "seek": 1141814, "start": 11425.82, "end": 11432.14, "text": " different so therefore these methods slightly slightly differ so the way to", "tokens": [50748, 819, 370, 4412, 613, 7150, 4748, 4748, 743, 370, 264, 636, 281, 51064], "temperature": 0.0, "avg_logprob": -0.17631108733429307, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.005683485418558121}, {"id": 1971, "seek": 1141814, "start": 11432.14, "end": 11437.46, "text": " think of why you you want to look at the adjacency at the adjacency matrix is", "tokens": [51064, 519, 295, 983, 291, 291, 528, 281, 574, 412, 264, 22940, 3020, 412, 264, 22940, 3020, 8141, 307, 51330], "temperature": 0.0, "avg_logprob": -0.17631108733429307, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.005683485418558121}, {"id": 1972, "seek": 1141814, "start": 11437.46, "end": 11442.98, "text": " this right so this is how you can think of your convolution so these basically", "tokens": [51330, 341, 558, 370, 341, 307, 577, 291, 393, 519, 295, 428, 45216, 370, 613, 1936, 51606], "temperature": 0.0, "avg_logprob": -0.17631108733429307, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.005683485418558121}, {"id": 1973, "seek": 1144298, "start": 11442.98, "end": 11449.1, "text": " it's multiple diagonal matrix now we can write it as a sum weighted by these", "tokens": [50364, 309, 311, 3866, 21539, 8141, 586, 321, 393, 2464, 309, 382, 257, 2408, 32807, 538, 613, 50670], "temperature": 0.0, "avg_logprob": -0.10780001210642386, "compression_ratio": 1.9738219895287958, "no_speech_prob": 0.00535620329901576}, {"id": 1974, "seek": 1144298, "start": 11449.1, "end": 11453.46, "text": " coefficients of the powers of the adjacency matrix right so the adjacency", "tokens": [50670, 31994, 295, 264, 8674, 295, 264, 22940, 3020, 8141, 558, 370, 264, 22940, 3020, 50888], "temperature": 0.0, "avg_logprob": -0.10780001210642386, "compression_ratio": 1.9738219895287958, "no_speech_prob": 0.00535620329901576}, {"id": 1975, "seek": 1144298, "start": 11453.46, "end": 11458.22, "text": " matrix of the ring graph will look like this so this red diagonal right so", "tokens": [50888, 8141, 295, 264, 4875, 4295, 486, 574, 411, 341, 370, 341, 2182, 21539, 558, 370, 51126], "temperature": 0.0, "avg_logprob": -0.10780001210642386, "compression_ratio": 1.9738219895287958, "no_speech_prob": 0.00535620329901576}, {"id": 1976, "seek": 1144298, "start": 11458.22, "end": 11462.539999999999, "text": " that's the shift operator so if you take a square you will get this if you get", "tokens": [51126, 300, 311, 264, 5513, 12973, 370, 498, 291, 747, 257, 3732, 291, 486, 483, 341, 498, 291, 483, 51342], "temperature": 0.0, "avg_logprob": -0.10780001210642386, "compression_ratio": 1.9738219895287958, "no_speech_prob": 0.00535620329901576}, {"id": 1977, "seek": 1144298, "start": 11462.539999999999, "end": 11467.38, "text": " cube you will get this right so you combine all of them you will get you", "tokens": [51342, 13728, 291, 486, 483, 341, 558, 370, 291, 10432, 439, 295, 552, 291, 486, 483, 291, 51584], "temperature": 0.0, "avg_logprob": -0.10780001210642386, "compression_ratio": 1.9738219895287958, "no_speech_prob": 0.00535620329901576}, {"id": 1978, "seek": 1146738, "start": 11467.42, "end": 11473.5, "text": " will get this general convolution so first architectures that try to do to do", "tokens": [50366, 486, 483, 341, 2674, 45216, 370, 700, 6331, 1303, 300, 853, 281, 360, 281, 360, 50670], "temperature": 0.0, "avg_logprob": -0.15430722453377463, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.0026517855003476143}, {"id": 1979, "seek": 1146738, "start": 11473.5, "end": 11477.66, "text": " learning on graphs looked exactly at this taking powers of either the Laplacian", "tokens": [50670, 2539, 322, 24877, 2956, 2293, 412, 341, 1940, 8674, 295, 2139, 264, 2369, 564, 326, 952, 50878], "temperature": 0.0, "avg_logprob": -0.15430722453377463, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.0026517855003476143}, {"id": 1980, "seek": 1146738, "start": 11477.66, "end": 11482.619999999999, "text": " or or the adjacency matrix basically polynomial with learnable coefficients", "tokens": [50878, 420, 420, 264, 22940, 3020, 8141, 1936, 26110, 365, 1466, 712, 31994, 51126], "temperature": 0.0, "avg_logprob": -0.15430722453377463, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.0026517855003476143}, {"id": 1981, "seek": 1146738, "start": 11482.619999999999, "end": 11487.939999999999, "text": " now if you also look at terms of the degrees of freedom so a fully connected", "tokens": [51126, 586, 498, 291, 611, 574, 412, 2115, 295, 264, 5310, 295, 5645, 370, 257, 4498, 4582, 51392], "temperature": 0.0, "avg_logprob": -0.15430722453377463, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.0026517855003476143}, {"id": 1982, "seek": 1146738, "start": 11487.939999999999, "end": 11493.06, "text": " layer will look like this right so it has no it has no symmetry so here the", "tokens": [51392, 4583, 486, 574, 411, 341, 558, 370, 309, 575, 572, 309, 575, 572, 25440, 370, 510, 264, 51648], "temperature": 0.0, "avg_logprob": -0.15430722453377463, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.0026517855003476143}, {"id": 1983, "seek": 1149306, "start": 11493.539999999999, "end": 11499.5, "text": " symmetry is trivial so it has n square degrees of freedom in the case of", "tokens": [50388, 25440, 307, 26703, 370, 309, 575, 297, 3732, 5310, 295, 5645, 294, 264, 1389, 295, 50686], "temperature": 0.0, "avg_logprob": -0.1747669451164477, "compression_ratio": 1.9781659388646289, "no_speech_prob": 0.0018996096914634109}, {"id": 1984, "seek": 1149306, "start": 11499.5, "end": 11506.019999999999, "text": " convolution so the the the symmetry here is translation we have order of n", "tokens": [50686, 45216, 370, 264, 264, 264, 25440, 510, 307, 12853, 321, 362, 1668, 295, 297, 51012], "temperature": 0.0, "avg_logprob": -0.1747669451164477, "compression_ratio": 1.9781659388646289, "no_speech_prob": 0.0018996096914634109}, {"id": 1985, "seek": 1149306, "start": 11506.019999999999, "end": 11509.46, "text": " degrees of freedom right so we reduce dramatically the number of parameters in", "tokens": [51012, 5310, 295, 5645, 558, 370, 321, 5407, 17548, 264, 1230, 295, 9834, 294, 51184], "temperature": 0.0, "avg_logprob": -0.1747669451164477, "compression_ratio": 1.9781659388646289, "no_speech_prob": 0.0018996096914634109}, {"id": 1986, "seek": 1149306, "start": 11509.46, "end": 11513.9, "text": " the neural network we reuse the same coefficients everywhere in the case of a", "tokens": [51184, 264, 18161, 3209, 321, 26225, 264, 912, 31994, 5315, 294, 264, 1389, 295, 257, 51406], "temperature": 0.0, "avg_logprob": -0.1747669451164477, "compression_ratio": 1.9781659388646289, "no_speech_prob": 0.0018996096914634109}, {"id": 1987, "seek": 1149306, "start": 11513.9, "end": 11517.859999999999, "text": " graph because we have permutation in variance we don't have the order of the", "tokens": [51406, 4295, 570, 321, 362, 4784, 11380, 294, 21977, 321, 500, 380, 362, 264, 1668, 295, 264, 51604], "temperature": 0.0, "avg_logprob": -0.1747669451164477, "compression_ratio": 1.9781659388646289, "no_speech_prob": 0.0018996096914634109}, {"id": 1988, "seek": 1149306, "start": 11517.859999999999, "end": 11521.5, "text": " neighbor so we must use the same coefficient so we can only distinguish", "tokens": [51604, 5987, 370, 321, 1633, 764, 264, 912, 17619, 370, 321, 393, 787, 20206, 51786], "temperature": 0.0, "avg_logprob": -0.1747669451164477, "compression_ratio": 1.9781659388646289, "no_speech_prob": 0.0018996096914634109}, {"id": 1989, "seek": 1152150, "start": 11521.54, "end": 11526.26, "text": " between ourselves and our neighborhood right so that's well here it's I'm", "tokens": [50366, 1296, 4175, 293, 527, 7630, 558, 370, 300, 311, 731, 510, 309, 311, 286, 478, 50602], "temperature": 0.0, "avg_logprob": -0.1504773180535499, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.002528894692659378}, {"id": 1990, "seek": 1152150, "start": 11526.26, "end": 11530.34, "text": " assuming a complete graph so this will look like something like deep sets for", "tokens": [50602, 11926, 257, 3566, 4295, 370, 341, 486, 574, 411, 746, 411, 2452, 6352, 337, 50806], "temperature": 0.0, "avg_logprob": -0.1504773180535499, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.002528894692659378}, {"id": 1991, "seek": 1152150, "start": 11530.34, "end": 11536.14, "text": " example so but the number of parameters is order of one so it's independent on", "tokens": [50806, 1365, 370, 457, 264, 1230, 295, 9834, 307, 1668, 295, 472, 370, 309, 311, 6695, 322, 51096], "temperature": 0.0, "avg_logprob": -0.1504773180535499, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.002528894692659378}, {"id": 1992, "seek": 1152150, "start": 11536.14, "end": 11543.1, "text": " the on the size of the domain what else can I say can I tell you well I know", "tokens": [51096, 264, 322, 264, 2744, 295, 264, 9274, 437, 1646, 393, 286, 584, 393, 286, 980, 291, 731, 286, 458, 51444], "temperature": 0.0, "avg_logprob": -0.1504773180535499, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.002528894692659378}, {"id": 1993, "seek": 1152150, "start": 11543.1, "end": 11549.14, "text": " that I'm out of time so do you want to hear about molecules or you heard about", "tokens": [51444, 300, 286, 478, 484, 295, 565, 370, 360, 291, 528, 281, 1568, 466, 13093, 420, 291, 2198, 466, 51746], "temperature": 0.0, "avg_logprob": -0.1504773180535499, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.002528894692659378}, {"id": 1994, "seek": 1154914, "start": 11549.18, "end": 11562.539999999999, "text": " molecules okay so let's talk about molecules I promise that I will try to do", "tokens": [50366, 13093, 1392, 370, 718, 311, 751, 466, 13093, 286, 6228, 300, 286, 486, 853, 281, 360, 51034], "temperature": 0.0, "avg_logprob": -0.16386427006251375, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.0015298444777727127}, {"id": 1995, "seek": 1154914, "start": 11562.539999999999, "end": 11567.619999999999, "text": " it try to do it fast and probably heard in Miguel's lecture as well so it will", "tokens": [51034, 309, 853, 281, 360, 309, 2370, 293, 1391, 2198, 294, 29150, 311, 7991, 382, 731, 370, 309, 486, 51288], "temperature": 0.0, "avg_logprob": -0.16386427006251375, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.0015298444777727127}, {"id": 1996, "seek": 1154914, "start": 11567.619999999999, "end": 11573.46, "text": " probably be a little bit repetitive so graphs are a very convenient model for", "tokens": [51288, 1391, 312, 257, 707, 857, 29404, 370, 24877, 366, 257, 588, 10851, 2316, 337, 51580], "temperature": 0.0, "avg_logprob": -0.16386427006251375, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.0015298444777727127}, {"id": 1997, "seek": 1154914, "start": 11573.46, "end": 11576.859999999999, "text": " molecules right basically a molecule looks like this so you can represent it", "tokens": [51580, 13093, 558, 1936, 257, 15582, 1542, 411, 341, 370, 291, 393, 2906, 309, 51750], "temperature": 0.0, "avg_logprob": -0.16386427006251375, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.0015298444777727127}, {"id": 1998, "seek": 1157686, "start": 11576.900000000001, "end": 11581.460000000001, "text": " as a graph and maybe that's not how chemists think of molecules but at least", "tokens": [50366, 382, 257, 4295, 293, 1310, 300, 311, 406, 577, 4771, 1751, 519, 295, 13093, 457, 412, 1935, 50594], "temperature": 0.0, "avg_logprob": -0.1581342842267907, "compression_ratio": 1.725868725868726, "no_speech_prob": 0.002337887417525053}, {"id": 1999, "seek": 1157686, "start": 11581.460000000001, "end": 11585.74, "text": " in some applications graph neural networks have been successful in predicting", "tokens": [50594, 294, 512, 5821, 4295, 18161, 9590, 362, 668, 4406, 294, 32884, 50808], "temperature": 0.0, "avg_logprob": -0.1581342842267907, "compression_ratio": 1.725868725868726, "no_speech_prob": 0.002337887417525053}, {"id": 2000, "seek": 1157686, "start": 11585.74, "end": 11589.5, "text": " certain properties of molecules that are required for virtual drug screening", "tokens": [50808, 1629, 7221, 295, 13093, 300, 366, 4739, 337, 6374, 4110, 17732, 50996], "temperature": 0.0, "avg_logprob": -0.1581342842267907, "compression_ratio": 1.725868725868726, "no_speech_prob": 0.002337887417525053}, {"id": 2001, "seek": 1157686, "start": 11589.5, "end": 11596.060000000001, "text": " right where the space of potentially synthesizable drug like molecules is", "tokens": [50996, 558, 689, 264, 1901, 295, 7263, 26617, 22395, 4110, 411, 13093, 307, 51324], "temperature": 0.0, "avg_logprob": -0.1581342842267907, "compression_ratio": 1.725868725868726, "no_speech_prob": 0.002337887417525053}, {"id": 2002, "seek": 1157686, "start": 11596.060000000001, "end": 11599.94, "text": " huge something like 10 to the power 60 the number of molecules that they can", "tokens": [51324, 2603, 746, 411, 1266, 281, 264, 1347, 4060, 264, 1230, 295, 13093, 300, 436, 393, 51518], "temperature": 0.0, "avg_logprob": -0.1581342842267907, "compression_ratio": 1.725868725868726, "no_speech_prob": 0.002337887417525053}, {"id": 2003, "seek": 1157686, "start": 11599.94, "end": 11603.66, "text": " actually test in the lab is significantly smaller so you need to", "tokens": [51518, 767, 1500, 294, 264, 2715, 307, 10591, 4356, 370, 291, 643, 281, 51704], "temperature": 0.0, "avg_logprob": -0.1581342842267907, "compression_ratio": 1.725868725868726, "no_speech_prob": 0.002337887417525053}, {"id": 2004, "seek": 1160366, "start": 11603.66, "end": 11608.1, "text": " reach this by some kind of computational methods and crafting networks have been", "tokens": [50364, 2524, 341, 538, 512, 733, 295, 28270, 7150, 293, 29048, 9590, 362, 668, 50586], "temperature": 0.0, "avg_logprob": -0.20208089192708334, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.007763859815895557}, {"id": 2005, "seek": 1160366, "start": 11608.1, "end": 11613.34, "text": " shown again in predicting some properties to be significantly faster", "tokens": [50586, 4898, 797, 294, 32884, 512, 7221, 281, 312, 10591, 4663, 50848], "temperature": 0.0, "avg_logprob": -0.20208089192708334, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.007763859815895557}, {"id": 2006, "seek": 1160366, "start": 11613.34, "end": 11620.94, "text": " while similar complexity to similar accuracy to to classical methods so one", "tokens": [50848, 1339, 2531, 14024, 281, 2531, 14170, 281, 281, 13735, 7150, 370, 472, 51228], "temperature": 0.0, "avg_logprob": -0.20208089192708334, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.007763859815895557}, {"id": 2007, "seek": 1160366, "start": 11620.94, "end": 11626.94, "text": " thing that that that is important to say in regard regarding molecules so", "tokens": [51228, 551, 300, 300, 300, 307, 1021, 281, 584, 294, 3843, 8595, 13093, 370, 51528], "temperature": 0.0, "avg_logprob": -0.20208089192708334, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.007763859815895557}, {"id": 2008, "seek": 1160366, "start": 11626.94, "end": 11631.46, "text": " molecules are not just any graph where the symmetry that we have is the symmetry", "tokens": [51528, 13093, 366, 406, 445, 604, 4295, 689, 264, 25440, 300, 321, 362, 307, 264, 25440, 51754], "temperature": 0.0, "avg_logprob": -0.20208089192708334, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.007763859815895557}, {"id": 2009, "seek": 1163146, "start": 11631.46, "end": 11635.539999999999, "text": " of the domain right the permutation of the nodes or the reordering of the atoms", "tokens": [50364, 295, 264, 9274, 558, 264, 4784, 11380, 295, 264, 13891, 420, 264, 319, 765, 1794, 295, 264, 16871, 50568], "temperature": 0.0, "avg_logprob": -0.09030708899864784, "compression_ratio": 1.895582329317269, "no_speech_prob": 0.011874650605022907}, {"id": 2010, "seek": 1163146, "start": 11635.539999999999, "end": 11639.82, "text": " right so the domain symmetry tells you that no matter how you order the atoms", "tokens": [50568, 558, 370, 264, 9274, 25440, 5112, 291, 300, 572, 1871, 577, 291, 1668, 264, 16871, 50782], "temperature": 0.0, "avg_logprob": -0.09030708899864784, "compression_ratio": 1.895582329317269, "no_speech_prob": 0.011874650605022907}, {"id": 2011, "seek": 1163146, "start": 11639.82, "end": 11644.3, "text": " in the molecule I still want to be able to say that it's the same molecule but", "tokens": [50782, 294, 264, 15582, 286, 920, 528, 281, 312, 1075, 281, 584, 300, 309, 311, 264, 912, 15582, 457, 51006], "temperature": 0.0, "avg_logprob": -0.09030708899864784, "compression_ratio": 1.895582329317269, "no_speech_prob": 0.011874650605022907}, {"id": 2012, "seek": 1163146, "start": 11644.3, "end": 11647.74, "text": " it also has geometric coordinates right so in addition to the let's say atom", "tokens": [51006, 309, 611, 575, 33246, 21056, 558, 370, 294, 4500, 281, 264, 718, 311, 584, 12018, 51178], "temperature": 0.0, "avg_logprob": -0.09030708899864784, "compression_ratio": 1.895582329317269, "no_speech_prob": 0.011874650605022907}, {"id": 2013, "seek": 1163146, "start": 11647.74, "end": 11651.74, "text": " types that we have here I also have the XYZ coordinates for every atom right so", "tokens": [51178, 3467, 300, 321, 362, 510, 286, 611, 362, 264, 48826, 57, 21056, 337, 633, 12018, 558, 370, 51378], "temperature": 0.0, "avg_logprob": -0.09030708899864784, "compression_ratio": 1.895582329317269, "no_speech_prob": 0.011874650605022907}, {"id": 2014, "seek": 1163146, "start": 11651.74, "end": 11656.9, "text": " it's a graph that lives in a continuous Euclidean space so here what I want to", "tokens": [51378, 309, 311, 257, 4295, 300, 2909, 294, 257, 10957, 462, 1311, 31264, 282, 1901, 370, 510, 437, 286, 528, 281, 51636], "temperature": 0.0, "avg_logprob": -0.09030708899864784, "compression_ratio": 1.895582329317269, "no_speech_prob": 0.011874650605022907}, {"id": 2015, "seek": 1165690, "start": 11656.9, "end": 11661.699999999999, "text": " say that if I rotate the molecule for example or translated I want to be able", "tokens": [50364, 584, 300, 498, 286, 13121, 264, 15582, 337, 1365, 420, 16805, 286, 528, 281, 312, 1075, 50604], "temperature": 0.0, "avg_logprob": -0.14879696709769114, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.006683382671326399}, {"id": 2016, "seek": 1165690, "start": 11661.699999999999, "end": 11666.14, "text": " to say that the properties remain the same so in this case typically you look", "tokens": [50604, 281, 584, 300, 264, 7221, 6222, 264, 912, 370, 294, 341, 1389, 5850, 291, 574, 50826], "temperature": 0.0, "avg_logprob": -0.14879696709769114, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.006683382671326399}, {"id": 2017, "seek": 1165690, "start": 11666.14, "end": 11670.82, "text": " at the special Euclidean group so rotations and translations without", "tokens": [50826, 412, 264, 2121, 462, 1311, 31264, 282, 1594, 370, 44796, 293, 37578, 1553, 51060], "temperature": 0.0, "avg_logprob": -0.14879696709769114, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.006683382671326399}, {"id": 2018, "seek": 1165690, "start": 11670.82, "end": 11676.22, "text": " reflections reflections can actually change the properties of molecules or", "tokens": [51060, 30679, 30679, 393, 767, 1319, 264, 7221, 295, 13093, 420, 51330], "temperature": 0.0, "avg_logprob": -0.14879696709769114, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.006683382671326399}, {"id": 2019, "seek": 1165690, "start": 11676.22, "end": 11682.1, "text": " you can use some other groups as well and there have been already several", "tokens": [51330, 291, 393, 764, 512, 661, 3935, 382, 731, 293, 456, 362, 668, 1217, 2940, 51624], "temperature": 0.0, "avg_logprob": -0.14879696709769114, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.006683382671326399}, {"id": 2020, "seek": 1165690, "start": 11682.1, "end": 11686.859999999999, "text": " interesting success stories so one of them was a group of Jim Collins at MIT so", "tokens": [51624, 1880, 2245, 3676, 370, 472, 295, 552, 390, 257, 1594, 295, 6637, 27973, 412, 13100, 370, 51862], "temperature": 0.0, "avg_logprob": -0.14879696709769114, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.006683382671326399}, {"id": 2021, "seek": 1168686, "start": 11686.86, "end": 11690.54, "text": " they used graph neural networks in virtual screening pipelines where they", "tokens": [50364, 436, 1143, 4295, 18161, 9590, 294, 6374, 17732, 40168, 689, 436, 50548], "temperature": 0.0, "avg_logprob": -0.18356099393632677, "compression_ratio": 1.75, "no_speech_prob": 0.004461200442165136}, {"id": 2022, "seek": 1168686, "start": 11690.54, "end": 11696.7, "text": " tried to determine which compounds could be used as new antibiotics against", "tokens": [50548, 3031, 281, 6997, 597, 21810, 727, 312, 1143, 382, 777, 26922, 1970, 50856], "temperature": 0.0, "avg_logprob": -0.18356099393632677, "compression_ratio": 1.75, "no_speech_prob": 0.004461200442165136}, {"id": 2023, "seek": 1168686, "start": 11696.7, "end": 11702.66, "text": " antibiotic resistant bacteria and they famously found that that a candidate", "tokens": [50856, 37828, 20383, 11763, 293, 436, 34360, 1352, 300, 300, 257, 11532, 51154], "temperature": 0.0, "avg_logprob": -0.18356099393632677, "compression_ratio": 1.75, "no_speech_prob": 0.004461200442165136}, {"id": 2024, "seek": 1168686, "start": 11702.66, "end": 11707.140000000001, "text": " drug that was tested against diabetes called Halicin was actually effective", "tokens": [51154, 4110, 300, 390, 8246, 1970, 13881, 1219, 13896, 299, 259, 390, 767, 4942, 51378], "temperature": 0.0, "avg_logprob": -0.18356099393632677, "compression_ratio": 1.75, "no_speech_prob": 0.004461200442165136}, {"id": 2025, "seek": 1168686, "start": 11707.140000000001, "end": 11714.74, "text": " across a broad range of of antibiotic resistance bacteria but in things that", "tokens": [51378, 2108, 257, 4152, 3613, 295, 295, 37828, 7335, 11763, 457, 294, 721, 300, 51758], "temperature": 0.0, "avg_logprob": -0.18356099393632677, "compression_ratio": 1.75, "no_speech_prob": 0.004461200442165136}, {"id": 2026, "seek": 1171474, "start": 11714.9, "end": 11719.1, "text": " we are doing we are mostly interested in proteins and this is well I think this", "tokens": [50372, 321, 366, 884, 321, 366, 5240, 3102, 294, 15577, 293, 341, 307, 731, 286, 519, 341, 50582], "temperature": 0.0, "avg_logprob": -0.14620743375835996, "compression_ratio": 1.7453183520599251, "no_speech_prob": 0.00685223750770092}, {"id": 2027, "seek": 1171474, "start": 11719.1, "end": 11723.9, "text": " is in general proteins are important targets for for drugs because they are", "tokens": [50582, 307, 294, 2674, 15577, 366, 1021, 12911, 337, 337, 7766, 570, 436, 366, 50822], "temperature": 0.0, "avg_logprob": -0.14620743375835996, "compression_ratio": 1.7453183520599251, "no_speech_prob": 0.00685223750770092}, {"id": 2028, "seek": 1171474, "start": 11723.9, "end": 11728.9, "text": " involved practically in anything that that happens in our body from defense", "tokens": [50822, 3288, 15667, 294, 1340, 300, 300, 2314, 294, 527, 1772, 490, 7654, 51072], "temperature": 0.0, "avg_logprob": -0.14620743375835996, "compression_ratio": 1.7453183520599251, "no_speech_prob": 0.00685223750770092}, {"id": 2029, "seek": 1171474, "start": 11728.9, "end": 11734.1, "text": " against pathogens right antibodies are special types of proteins to delivering", "tokens": [51072, 1970, 44760, 558, 28356, 366, 2121, 3467, 295, 15577, 281, 14666, 51332], "temperature": 0.0, "avg_logprob": -0.14620743375835996, "compression_ratio": 1.7453183520599251, "no_speech_prob": 0.00685223750770092}, {"id": 2030, "seek": 1171474, "start": 11734.1, "end": 11738.38, "text": " oxygen to ourselves hemoglobin is also a special type of protein so basically", "tokens": [51332, 9169, 281, 4175, 8636, 45841, 13496, 307, 611, 257, 2121, 2010, 295, 7944, 370, 1936, 51546], "temperature": 0.0, "avg_logprob": -0.14620743375835996, "compression_ratio": 1.7453183520599251, "no_speech_prob": 0.00685223750770092}, {"id": 2031, "seek": 1171474, "start": 11738.38, "end": 11742.78, "text": " they're everywhere and encoded in our DNA so we really we don't know any life", "tokens": [51546, 436, 434, 5315, 293, 2058, 12340, 294, 527, 8272, 370, 321, 534, 321, 500, 380, 458, 604, 993, 51766], "temperature": 0.0, "avg_logprob": -0.14620743375835996, "compression_ratio": 1.7453183520599251, "no_speech_prob": 0.00685223750770092}, {"id": 2032, "seek": 1174278, "start": 11742.820000000002, "end": 11747.980000000001, "text": " form that is not based on proteins at least for the time being and it was", "tokens": [50366, 1254, 300, 307, 406, 2361, 322, 15577, 412, 1935, 337, 264, 565, 885, 293, 309, 390, 50624], "temperature": 0.0, "avg_logprob": -0.15618360042572021, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.005589893553406}, {"id": 2033, "seek": 1174278, "start": 11747.980000000001, "end": 11753.460000000001, "text": " conjectured in the 70s by Anfins and Nobel laureate in chemistry that you can", "tokens": [50624, 416, 1020, 3831, 294, 264, 5285, 82, 538, 1107, 69, 1292, 293, 24611, 49469, 473, 294, 12558, 300, 291, 393, 50898], "temperature": 0.0, "avg_logprob": -0.15618360042572021, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.005589893553406}, {"id": 2034, "seek": 1174278, "start": 11753.460000000001, "end": 11759.74, "text": " determine the structure of the protein from its sequence so proteins are long", "tokens": [50898, 6997, 264, 3877, 295, 264, 7944, 490, 1080, 8310, 370, 15577, 366, 938, 51212], "temperature": 0.0, "avg_logprob": -0.15618360042572021, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.005589893553406}, {"id": 2035, "seek": 1174278, "start": 11759.74, "end": 11763.86, "text": " chains of amino acids connected to each other and then under the influence of", "tokens": [51212, 12626, 295, 24674, 21667, 4582, 281, 1184, 661, 293, 550, 833, 264, 6503, 295, 51418], "temperature": 0.0, "avg_logprob": -0.15618360042572021, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.005589893553406}, {"id": 2036, "seek": 1174278, "start": 11763.86, "end": 11769.58, "text": " electrostatic forces they fold into these complicated structures but we are", "tokens": [51418, 7072, 555, 2399, 5874, 436, 4860, 666, 613, 6179, 9227, 457, 321, 366, 51704], "temperature": 0.0, "avg_logprob": -0.15618360042572021, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.005589893553406}, {"id": 2037, "seek": 1176958, "start": 11769.58, "end": 11775.02, "text": " interested in the opposite problem so maybe a little bit incorrectly we can", "tokens": [50364, 3102, 294, 264, 6182, 1154, 370, 1310, 257, 707, 857, 42892, 321, 393, 50636], "temperature": 0.0, "avg_logprob": -0.1439298854154699, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.00920163094997406}, {"id": 2038, "seek": 1176958, "start": 11775.02, "end": 11781.18, "text": " call it some kind of inverse problem so I would like to to design a protein that", "tokens": [50636, 818, 309, 512, 733, 295, 17340, 1154, 370, 286, 576, 411, 281, 281, 1715, 257, 7944, 300, 50944], "temperature": 0.0, "avg_logprob": -0.1439298854154699, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.00920163094997406}, {"id": 2039, "seek": 1176958, "start": 11781.18, "end": 11785.5, "text": " will fall in fold into a certain structure of course it's not that simple", "tokens": [50944, 486, 2100, 294, 4860, 666, 257, 1629, 3877, 295, 1164, 309, 311, 406, 300, 2199, 51160], "temperature": 0.0, "avg_logprob": -0.1439298854154699, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.00920163094997406}, {"id": 2040, "seek": 1176958, "start": 11785.5, "end": 11789.46, "text": " because it is tempting to think that we have a sequence that then falls into a", "tokens": [51160, 570, 309, 307, 37900, 281, 519, 300, 321, 362, 257, 8310, 300, 550, 8804, 666, 257, 51358], "temperature": 0.0, "avg_logprob": -0.1439298854154699, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.00920163094997406}, {"id": 2041, "seek": 1176958, "start": 11789.46, "end": 11792.5, "text": " structure and the structure and doubts the protein with certain function for", "tokens": [51358, 3877, 293, 264, 3877, 293, 22618, 264, 7944, 365, 1629, 2445, 337, 51510], "temperature": 0.0, "avg_logprob": -0.1439298854154699, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.00920163094997406}, {"id": 2042, "seek": 1179250, "start": 11792.5, "end": 11799.38, "text": " example what kind of molecules it binds and initially computer scientists look", "tokens": [50364, 1365, 437, 733, 295, 13093, 309, 41515, 293, 9105, 3820, 7708, 574, 50708], "temperature": 0.0, "avg_logprob": -0.13733787327022343, "compression_ratio": 1.796875, "no_speech_prob": 0.02141169272363186}, {"id": 2043, "seek": 1179250, "start": 11799.38, "end": 11804.42, "text": " at proteins as sequences because well it's just strings so we can look for", "tokens": [50708, 412, 15577, 382, 22978, 570, 731, 309, 311, 445, 13985, 370, 321, 393, 574, 337, 50960], "temperature": 0.0, "avg_logprob": -0.13733787327022343, "compression_ratio": 1.796875, "no_speech_prob": 0.02141169272363186}, {"id": 2044, "seek": 1179250, "start": 11804.42, "end": 11807.94, "text": " certain patterns you can try to align different sequences together right like", "tokens": [50960, 1629, 8294, 291, 393, 853, 281, 7975, 819, 22978, 1214, 558, 411, 51136], "temperature": 0.0, "avg_logprob": -0.13733787327022343, "compression_ratio": 1.796875, "no_speech_prob": 0.02141169272363186}, {"id": 2045, "seek": 1179250, "start": 11807.94, "end": 11812.74, "text": " multiple multiple sequence alignment then came the problem of structure", "tokens": [51136, 3866, 3866, 8310, 18515, 550, 1361, 264, 1154, 295, 3877, 51376], "temperature": 0.0, "avg_logprob": -0.13733787327022343, "compression_ratio": 1.796875, "no_speech_prob": 0.02141169272363186}, {"id": 2046, "seek": 1179250, "start": 11812.74, "end": 11816.46, "text": " prediction and that's where alpha fold excelled recently but then the problem", "tokens": [51376, 17630, 293, 300, 311, 689, 8961, 4860, 45817, 292, 3938, 457, 550, 264, 1154, 51562], "temperature": 0.0, "avg_logprob": -0.13733787327022343, "compression_ratio": 1.796875, "no_speech_prob": 0.02141169272363186}, {"id": 2047, "seek": 1179250, "start": 11816.46, "end": 11821.06, "text": " of function is distinct and you can find examples of for example proteins with", "tokens": [51562, 295, 2445, 307, 10644, 293, 291, 393, 915, 5110, 295, 337, 1365, 15577, 365, 51792], "temperature": 0.0, "avg_logprob": -0.13733787327022343, "compression_ratio": 1.796875, "no_speech_prob": 0.02141169272363186}, {"id": 2048, "seek": 1182106, "start": 11821.06, "end": 11826.58, "text": " different sequences but similar structure you can find proteins with very", "tokens": [50364, 819, 22978, 457, 2531, 3877, 291, 393, 915, 15577, 365, 588, 50640], "temperature": 0.0, "avg_logprob": -0.2041181066761846, "compression_ratio": 2.018099547511312, "no_speech_prob": 0.007592258509248495}, {"id": 2049, "seek": 1182106, "start": 11826.58, "end": 11830.58, "text": " similar sequences but very different structure or you can also find proteins", "tokens": [50640, 2531, 22978, 457, 588, 819, 3877, 420, 291, 393, 611, 915, 15577, 50840], "temperature": 0.0, "avg_logprob": -0.2041181066761846, "compression_ratio": 2.018099547511312, "no_speech_prob": 0.007592258509248495}, {"id": 2050, "seek": 1182106, "start": 11830.58, "end": 11833.539999999999, "text": " with different sequences and different structures but similar function so they", "tokens": [50840, 365, 819, 22978, 293, 819, 9227, 457, 2531, 2445, 370, 436, 50988], "temperature": 0.0, "avg_logprob": -0.2041181066761846, "compression_ratio": 2.018099547511312, "no_speech_prob": 0.007592258509248495}, {"id": 2051, "seek": 1182106, "start": 11833.539999999999, "end": 11839.46, "text": " happen to to bind the same the same molecule so the good analogy here is", "tokens": [50988, 1051, 281, 281, 14786, 264, 912, 264, 912, 15582, 370, 264, 665, 21663, 510, 307, 51284], "temperature": 0.0, "avg_logprob": -0.2041181066761846, "compression_ratio": 2.018099547511312, "no_speech_prob": 0.007592258509248495}, {"id": 2052, "seek": 1182106, "start": 11839.46, "end": 11844.58, "text": " this lock and key metaphor that was introduced by I like quotes from", "tokens": [51284, 341, 4017, 293, 2141, 19157, 300, 390, 7268, 538, 286, 411, 19963, 490, 51540], "temperature": 0.0, "avg_logprob": -0.2041181066761846, "compression_ratio": 2.018099547511312, "no_speech_prob": 0.007592258509248495}, {"id": 2053, "seek": 1182106, "start": 11844.58, "end": 11848.66, "text": " Nobel laureates so that was from Emil Fischer also Nobel Nobel laureate in", "tokens": [51540, 24611, 49469, 1024, 370, 300, 390, 490, 36983, 479, 19674, 611, 24611, 24611, 49469, 473, 294, 51744], "temperature": 0.0, "avg_logprob": -0.2041181066761846, "compression_ratio": 2.018099547511312, "no_speech_prob": 0.007592258509248495}, {"id": 2054, "seek": 1184866, "start": 11848.98, "end": 11852.9, "text": " in chemistry so he was talking about enzymes but I think it's more general", "tokens": [50380, 294, 12558, 370, 415, 390, 1417, 466, 29299, 457, 286, 519, 309, 311, 544, 2674, 50576], "temperature": 0.0, "avg_logprob": -0.14031931139388173, "compression_ratio": 1.80078125, "no_speech_prob": 0.003741106018424034}, {"id": 2055, "seek": 1184866, "start": 11852.9, "end": 11857.7, "text": " applies to to proteins broadly so same way as you have a unique key that fits", "tokens": [50576, 13165, 281, 281, 15577, 19511, 370, 912, 636, 382, 291, 362, 257, 3845, 2141, 300, 9001, 50816], "temperature": 0.0, "avg_logprob": -0.14031931139388173, "compression_ratio": 1.80078125, "no_speech_prob": 0.003741106018424034}, {"id": 2056, "seek": 1184866, "start": 11857.7, "end": 11863.06, "text": " into a lock you might have a unique molecule or at least that's that's the", "tokens": [50816, 666, 257, 4017, 291, 1062, 362, 257, 3845, 15582, 420, 412, 1935, 300, 311, 300, 311, 264, 51084], "temperature": 0.0, "avg_logprob": -0.14031931139388173, "compression_ratio": 1.80078125, "no_speech_prob": 0.003741106018424034}, {"id": 2057, "seek": 1184866, "start": 11863.06, "end": 11867.619999999999, "text": " wishful thinking is that a unique molecule that will fit into some pocket", "tokens": [51084, 3172, 906, 1953, 307, 300, 257, 3845, 15582, 300, 486, 3318, 666, 512, 8963, 51312], "temperature": 0.0, "avg_logprob": -0.14031931139388173, "compression_ratio": 1.80078125, "no_speech_prob": 0.003741106018424034}, {"id": 2058, "seek": 1184866, "start": 11867.619999999999, "end": 11871.94, "text": " that exists on the the surface of this folded protein structure and this is how", "tokens": [51312, 300, 8198, 322, 264, 264, 3753, 295, 341, 23940, 7944, 3877, 293, 341, 307, 577, 51528], "temperature": 0.0, "avg_logprob": -0.14031931139388173, "compression_ratio": 1.80078125, "no_speech_prob": 0.003741106018424034}, {"id": 2059, "seek": 1184866, "start": 11871.94, "end": 11876.9, "text": " drugs are typically designed right so you have a protein that is your target so", "tokens": [51528, 7766, 366, 5850, 4761, 558, 370, 291, 362, 257, 7944, 300, 307, 428, 3779, 370, 51776], "temperature": 0.0, "avg_logprob": -0.14031931139388173, "compression_ratio": 1.80078125, "no_speech_prob": 0.003741106018424034}, {"id": 2060, "seek": 1187690, "start": 11877.3, "end": 11882.5, "text": " that's how its surface looks like and here is some small molecule that sticks", "tokens": [50384, 300, 311, 577, 1080, 3753, 1542, 411, 293, 510, 307, 512, 1359, 15582, 300, 12518, 50644], "temperature": 0.0, "avg_logprob": -0.11563504397214114, "compression_ratio": 1.7990867579908676, "no_speech_prob": 0.0028517853934317827}, {"id": 2061, "seek": 1187690, "start": 11882.5, "end": 11888.9, "text": " into this hole and binds this this molecule and that's how the drug works so this is", "tokens": [50644, 666, 341, 5458, 293, 41515, 341, 341, 15582, 293, 300, 311, 577, 264, 4110, 1985, 370, 341, 307, 50964], "temperature": 0.0, "avg_logprob": -0.11563504397214114, "compression_ratio": 1.7990867579908676, "no_speech_prob": 0.0028517853934317827}, {"id": 2062, "seek": 1187690, "start": 11888.9, "end": 11893.539999999999, "text": " actually a molecule not exactly of caffeine but of compound from the same", "tokens": [50964, 767, 257, 15582, 406, 2293, 295, 31261, 457, 295, 14154, 490, 264, 912, 51196], "temperature": 0.0, "avg_logprob": -0.11563504397214114, "compression_ratio": 1.7990867579908676, "no_speech_prob": 0.0028517853934317827}, {"id": 2063, "seek": 1187690, "start": 11893.539999999999, "end": 11897.619999999999, "text": " class and that's how it's by how it binds the adenosine receptor in the brain", "tokens": [51196, 1508, 293, 300, 311, 577, 309, 311, 538, 577, 309, 41515, 264, 614, 20161, 533, 32264, 294, 264, 3567, 51400], "temperature": 0.0, "avg_logprob": -0.11563504397214114, "compression_ratio": 1.7990867579908676, "no_speech_prob": 0.0028517853934317827}, {"id": 2064, "seek": 1187690, "start": 11898.66, "end": 11902.42, "text": " many other interesting targets though they don't have these kind of pocket like", "tokens": [51452, 867, 661, 1880, 12911, 1673, 436, 500, 380, 362, 613, 733, 295, 8963, 411, 51640], "temperature": 0.0, "avg_logprob": -0.11563504397214114, "compression_ratio": 1.7990867579908676, "no_speech_prob": 0.0028517853934317827}, {"id": 2065, "seek": 1190242, "start": 11902.42, "end": 11907.94, "text": " structures and there are interesting systems of protein of proteins interacting", "tokens": [50364, 9227, 293, 456, 366, 1880, 3652, 295, 7944, 295, 15577, 18017, 50640], "temperature": 0.0, "avg_logprob": -0.11064366088516411, "compression_ratio": 1.8812785388127853, "no_speech_prob": 0.0042596906423568726}, {"id": 2066, "seek": 1190242, "start": 11907.94, "end": 11912.26, "text": " with each other like this one the program death complex where you have two proteins", "tokens": [50640, 365, 1184, 661, 411, 341, 472, 264, 1461, 2966, 3997, 689, 291, 362, 732, 15577, 50856], "temperature": 0.0, "avg_logprob": -0.11064366088516411, "compression_ratio": 1.8812785388127853, "no_speech_prob": 0.0042596906423568726}, {"id": 2067, "seek": 1190242, "start": 11912.26, "end": 11919.06, "text": " called pd1 and pdl1 and they are involved in cancer immunotherapy basically these", "tokens": [50856, 1219, 280, 67, 16, 293, 280, 67, 75, 16, 293, 436, 366, 3288, 294, 5592, 13154, 23208, 1936, 613, 51196], "temperature": 0.0, "avg_logprob": -0.11064366088516411, "compression_ratio": 1.8812785388127853, "no_speech_prob": 0.0042596906423568726}, {"id": 2068, "seek": 1190242, "start": 11919.06, "end": 11922.98, "text": " proteins tell our immune system not to kill healthy cells and some cancers", "tokens": [51196, 15577, 980, 527, 11992, 1185, 406, 281, 1961, 4627, 5438, 293, 512, 31063, 51392], "temperature": 0.0, "avg_logprob": -0.11064366088516411, "compression_ratio": 1.8812785388127853, "no_speech_prob": 0.0042596906423568726}, {"id": 2069, "seek": 1190242, "start": 11923.54, "end": 11928.9, "text": " have these proteins so they manage to evade the normal functioning of the immune system and", "tokens": [51420, 362, 613, 15577, 370, 436, 3067, 281, 1073, 762, 264, 2710, 18483, 295, 264, 11992, 1185, 293, 51688], "temperature": 0.0, "avg_logprob": -0.11064366088516411, "compression_ratio": 1.8812785388127853, "no_speech_prob": 0.0042596906423568726}, {"id": 2070, "seek": 1192890, "start": 11928.9, "end": 11935.06, "text": " the idea is to block one of these proteins either pd1 or pdl1 and this way basically", "tokens": [50364, 264, 1558, 307, 281, 3461, 472, 295, 613, 15577, 2139, 280, 67, 16, 420, 280, 67, 75, 16, 293, 341, 636, 1936, 50672], "temperature": 0.0, "avg_logprob": -0.10656120262893976, "compression_ratio": 1.7768595041322315, "no_speech_prob": 0.0031567940022796392}, {"id": 2071, "seek": 1192890, "start": 11935.06, "end": 11940.26, "text": " the malignant cells are destroyed by by by the immune system so you need to design", "tokens": [50672, 264, 2806, 36818, 5438, 366, 8937, 538, 538, 538, 264, 11992, 1185, 370, 291, 643, 281, 1715, 50932], "temperature": 0.0, "avg_logprob": -0.10656120262893976, "compression_ratio": 1.7768595041322315, "no_speech_prob": 0.0031567940022796392}, {"id": 2072, "seek": 1192890, "start": 11941.06, "end": 11947.46, "text": " some binder that will that will bind to one of these proteins and they happen to have", "tokens": [50972, 512, 45630, 300, 486, 300, 486, 14786, 281, 472, 295, 613, 15577, 293, 436, 1051, 281, 362, 51292], "temperature": 0.0, "avg_logprob": -0.10656120262893976, "compression_ratio": 1.7768595041322315, "no_speech_prob": 0.0031567940022796392}, {"id": 2073, "seek": 1192890, "start": 11947.46, "end": 11951.06, "text": " these kind of flat interfaces so they're considered to be hard or impossible to", "tokens": [51292, 613, 733, 295, 4962, 28416, 370, 436, 434, 4888, 281, 312, 1152, 420, 6243, 281, 51472], "temperature": 0.0, "avg_logprob": -0.10656120262893976, "compression_ratio": 1.7768595041322315, "no_speech_prob": 0.0031567940022796392}, {"id": 2074, "seek": 1192890, "start": 11951.06, "end": 11955.699999999999, "text": " drug by small molecules but you can drug them by proteins so that's the idea of biological drugs", "tokens": [51472, 4110, 538, 1359, 13093, 457, 291, 393, 4110, 552, 538, 15577, 370, 300, 311, 264, 1558, 295, 13910, 7766, 51704], "temperature": 0.0, "avg_logprob": -0.10656120262893976, "compression_ratio": 1.7768595041322315, "no_speech_prob": 0.0031567940022796392}, {"id": 2075, "seek": 1195570, "start": 11956.66, "end": 11962.02, "text": " where the drug itself is is a protein molecule typically an antibody for variety of reasons", "tokens": [50412, 689, 264, 4110, 2564, 307, 307, 257, 7944, 15582, 5850, 364, 34507, 337, 5673, 295, 4112, 50680], "temperature": 0.0, "avg_logprob": -0.17605298274272196, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0017352205468341708}, {"id": 2076, "seek": 1195570, "start": 11963.380000000001, "end": 11967.460000000001, "text": " so you can use geometrically planning well and unfortunately I didn't have time to talk about", "tokens": [50748, 370, 291, 393, 764, 12956, 81, 984, 5038, 731, 293, 7015, 286, 994, 380, 362, 565, 281, 751, 466, 50952], "temperature": 0.0, "avg_logprob": -0.17605298274272196, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0017352205468341708}, {"id": 2077, "seek": 1195570, "start": 11967.460000000001, "end": 11972.42, "text": " it but basically instead of considering graphs we can consider surfaces so we model proteins as", "tokens": [50952, 309, 457, 1936, 2602, 295, 8079, 24877, 321, 393, 1949, 16130, 370, 321, 2316, 15577, 382, 51200], "temperature": 0.0, "avg_logprob": -0.17605298274272196, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0017352205468341708}, {"id": 2078, "seek": 1195570, "start": 11973.220000000001, "end": 11979.94, "text": " many folds as as basically the external surface that that that appears to", "tokens": [51240, 867, 31341, 382, 382, 1936, 264, 8320, 3753, 300, 300, 300, 7038, 281, 51576], "temperature": 0.0, "avg_logprob": -0.17605298274272196, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0017352205468341708}, {"id": 2079, "seek": 1197994, "start": 11980.9, "end": 11985.86, "text": " to the other molecule that that tries to bind it and this way you abstract all the internal", "tokens": [50412, 281, 264, 661, 15582, 300, 300, 9898, 281, 14786, 309, 293, 341, 636, 291, 12649, 439, 264, 6920, 50660], "temperature": 0.0, "avg_logprob": -0.10771880752738865, "compression_ratio": 1.8756218905472637, "no_speech_prob": 0.0032107760198414326}, {"id": 2080, "seek": 1197994, "start": 11987.060000000001, "end": 11991.78, "text": " intricacies of the fold so let me try to show you an example so this is a plastic model of a protein", "tokens": [50720, 30242, 20330, 295, 264, 4860, 370, 718, 385, 853, 281, 855, 291, 364, 1365, 370, 341, 307, 257, 5900, 2316, 295, 257, 7944, 50956], "temperature": 0.0, "avg_logprob": -0.10771880752738865, "compression_ratio": 1.8756218905472637, "no_speech_prob": 0.0032107760198414326}, {"id": 2081, "seek": 1197994, "start": 11992.34, "end": 11999.220000000001, "text": " you see so this is protein is the the one that that the person holds is supposed to bind to", "tokens": [50984, 291, 536, 370, 341, 307, 7944, 307, 264, 264, 472, 300, 300, 264, 954, 9190, 307, 3442, 281, 14786, 281, 51328], "temperature": 0.0, "avg_logprob": -0.10771880752738865, "compression_ratio": 1.8756218905472637, "no_speech_prob": 0.0032107760198414326}, {"id": 2082, "seek": 1197994, "start": 11999.220000000001, "end": 12005.7, "text": " these transparent ones so you see that these complicated helixes and and other things inside", "tokens": [51328, 613, 12737, 2306, 370, 291, 536, 300, 613, 6179, 801, 36005, 293, 293, 661, 721, 1854, 51652], "temperature": 0.0, "avg_logprob": -0.10771880752738865, "compression_ratio": 1.8756218905472637, "no_speech_prob": 0.0032107760198414326}, {"id": 2083, "seek": 1200570, "start": 12005.7, "end": 12010.980000000001, "text": " so that's the protein fold but what appears from the outside is this transparent surface so", "tokens": [50364, 370, 300, 311, 264, 7944, 4860, 457, 437, 7038, 490, 264, 2380, 307, 341, 12737, 3753, 370, 50628], "temperature": 0.0, "avg_logprob": -0.057281541352224824, "compression_ratio": 1.7961538461538462, "no_speech_prob": 0.0029514413326978683}, {"id": 2084, "seek": 1200570, "start": 12010.980000000001, "end": 12016.18, "text": " this guy doesn't care what happens inside so it cares only about the the the structure of course", "tokens": [50628, 341, 2146, 1177, 380, 1127, 437, 2314, 1854, 370, 309, 12310, 787, 466, 264, 264, 264, 3877, 295, 1164, 50888], "temperature": 0.0, "avg_logprob": -0.057281541352224824, "compression_ratio": 1.7961538461538462, "no_speech_prob": 0.0029514413326978683}, {"id": 2085, "seek": 1200570, "start": 12016.18, "end": 12020.58, "text": " the problem is more complicated because the the conformation of the protein the its geometric", "tokens": [50888, 264, 1154, 307, 544, 6179, 570, 264, 264, 416, 8663, 295, 264, 7944, 264, 1080, 33246, 51108], "temperature": 0.0, "avg_logprob": -0.057281541352224824, "compression_ratio": 1.7961538461538462, "no_speech_prob": 0.0029514413326978683}, {"id": 2086, "seek": 1200570, "start": 12020.58, "end": 12025.060000000001, "text": " structure might change as a result of of the interaction but at least it's in some cases", "tokens": [51108, 3877, 1062, 1319, 382, 257, 1874, 295, 295, 264, 9285, 457, 412, 1935, 309, 311, 294, 512, 3331, 51332], "temperature": 0.0, "avg_logprob": -0.057281541352224824, "compression_ratio": 1.7961538461538462, "no_speech_prob": 0.0029514413326978683}, {"id": 2087, "seek": 1200570, "start": 12025.060000000001, "end": 12033.220000000001, "text": " it's a good approximation so long story short we we can do special type of neural networks that", "tokens": [51332, 309, 311, 257, 665, 28023, 370, 938, 1657, 2099, 321, 321, 393, 360, 2121, 2010, 295, 18161, 9590, 300, 51740], "temperature": 0.0, "avg_logprob": -0.057281541352224824, "compression_ratio": 1.7961538461538462, "no_speech_prob": 0.0029514413326978683}, {"id": 2088, "seek": 1203322, "start": 12033.22, "end": 12038.34, "text": " operate on these surfaces so they take into account both geometric and and chemical properties of", "tokens": [50364, 9651, 322, 613, 16130, 370, 436, 747, 666, 2696, 1293, 33246, 293, 293, 7313, 7221, 295, 50620], "temperature": 0.0, "avg_logprob": -0.09061941808583784, "compression_ratio": 1.8473895582329318, "no_speech_prob": 0.004367848392575979}, {"id": 2089, "seek": 1203322, "start": 12038.34, "end": 12045.539999999999, "text": " of the molecular surface and they can they try to find complementary structures that are expected", "tokens": [50620, 295, 264, 19046, 3753, 293, 436, 393, 436, 853, 281, 915, 40705, 9227, 300, 366, 5176, 50980], "temperature": 0.0, "avg_logprob": -0.09061941808583784, "compression_ratio": 1.8473895582329318, "no_speech_prob": 0.004367848392575979}, {"id": 2090, "seek": 1203322, "start": 12045.539999999999, "end": 12050.74, "text": " to interact so think of kind of pieces of three-dimensional puzzle but it's not only", "tokens": [50980, 281, 4648, 370, 519, 295, 733, 295, 3755, 295, 1045, 12, 18759, 12805, 457, 309, 311, 406, 787, 51240], "temperature": 0.0, "avg_logprob": -0.09061941808583784, "compression_ratio": 1.8473895582329318, "no_speech_prob": 0.004367848392575979}, {"id": 2091, "seek": 1203322, "start": 12050.74, "end": 12054.98, "text": " geometric complementarity it's also chemical complementarity so they need to have the right", "tokens": [51240, 33246, 17103, 17409, 309, 311, 611, 7313, 17103, 17409, 370, 436, 643, 281, 362, 264, 558, 51452], "temperature": 0.0, "avg_logprob": -0.09061941808583784, "compression_ratio": 1.8473895582329318, "no_speech_prob": 0.004367848392575979}, {"id": 2092, "seek": 1203322, "start": 12054.98, "end": 12060.5, "text": " charges so they don't repel each other and this is a method that we call the massive so", "tokens": [51452, 12235, 370, 436, 500, 380, 1085, 338, 1184, 661, 293, 341, 307, 257, 3170, 300, 321, 818, 264, 5994, 370, 51728], "temperature": 0.0, "avg_logprob": -0.09061941808583784, "compression_ratio": 1.8473895582329318, "no_speech_prob": 0.004367848392575979}, {"id": 2093, "seek": 1206050, "start": 12061.46, "end": 12070.66, "text": " we were lucky to appear on the cover of nature methods in 2020 and this year we also had a paper", "tokens": [50412, 321, 645, 6356, 281, 4204, 322, 264, 2060, 295, 3687, 7150, 294, 4808, 293, 341, 1064, 321, 611, 632, 257, 3035, 50872], "temperature": 0.0, "avg_logprob": -0.0886585401452106, "compression_ratio": 1.8082191780821917, "no_speech_prob": 0.006651827599853277}, {"id": 2094, "seek": 1206050, "start": 12070.66, "end": 12076.18, "text": " in nature that contained experimental results so we also hope to appear on the cover but they chose", "tokens": [50872, 294, 3687, 300, 16212, 17069, 3542, 370, 321, 611, 1454, 281, 4204, 322, 264, 2060, 457, 436, 5111, 51148], "temperature": 0.0, "avg_logprob": -0.0886585401452106, "compression_ratio": 1.8082191780821917, "no_speech_prob": 0.006651827599853277}, {"id": 2095, "seek": 1206050, "start": 12076.18, "end": 12081.14, "text": " a different one but because we paid for the cover here you need to you need to see it i think it was", "tokens": [51148, 257, 819, 472, 457, 570, 321, 4835, 337, 264, 2060, 510, 291, 643, 281, 291, 643, 281, 536, 309, 741, 519, 309, 390, 51396], "temperature": 0.0, "avg_logprob": -0.0886585401452106, "compression_ratio": 1.8082191780821917, "no_speech_prob": 0.006651827599853277}, {"id": 2096, "seek": 1206050, "start": 12081.7, "end": 12089.14, "text": " a cool image so we used this method to design new binders for different targets and basically it's", "tokens": [51424, 257, 1627, 3256, 370, 321, 1143, 341, 3170, 281, 1715, 777, 14786, 433, 337, 819, 12911, 293, 1936, 309, 311, 51796], "temperature": 0.0, "avg_logprob": -0.0886585401452106, "compression_ratio": 1.8082191780821917, "no_speech_prob": 0.006651827599853277}, {"id": 2097, "seek": 1208914, "start": 12089.14, "end": 12094.74, "text": " a fragment based design so we use this neural network architecture to identify potentially", "tokens": [50364, 257, 26424, 2361, 1715, 370, 321, 764, 341, 18161, 3209, 9482, 281, 5876, 7263, 50644], "temperature": 0.0, "avg_logprob": -0.09802351679120745, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.0039929961785674095}, {"id": 2098, "seek": 1208914, "start": 12094.74, "end": 12102.019999999999, "text": " complementary targets that then we use to build the binder and here the experimental results show", "tokens": [50644, 40705, 12911, 300, 550, 321, 764, 281, 1322, 264, 45630, 293, 510, 264, 17069, 3542, 855, 51008], "temperature": 0.0, "avg_logprob": -0.09802351679120745, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.0039929961785674095}, {"id": 2099, "seek": 1208914, "start": 12102.019999999999, "end": 12109.3, "text": " different structures so here's a binder for the pdl one oncological target and we also have the", "tokens": [51008, 819, 9227, 370, 510, 311, 257, 45630, 337, 264, 280, 67, 75, 472, 40592, 4383, 3779, 293, 321, 611, 362, 264, 51372], "temperature": 0.0, "avg_logprob": -0.09802351679120745, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.0039929961785674095}, {"id": 2100, "seek": 1208914, "start": 12109.3, "end": 12117.38, "text": " crystal structures and here's an example of another binder for the SARS-CoV-2 spike protein so that's", "tokens": [51372, 13662, 9227, 293, 510, 311, 364, 1365, 295, 1071, 45630, 337, 264, 34233, 12, 21141, 53, 12, 17, 21053, 7944, 370, 300, 311, 51776], "temperature": 0.0, "avg_logprob": -0.09802351679120745, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.0039929961785674095}, {"id": 2101, "seek": 1211738, "start": 12117.38, "end": 12125.539999999999, "text": " the coronavirus that caused the COVID-19 pandemic that has been terrorizing us for more than three", "tokens": [50364, 264, 13043, 300, 7008, 264, 4566, 12, 3405, 5388, 300, 575, 668, 8127, 3319, 505, 337, 544, 813, 1045, 50772], "temperature": 0.0, "avg_logprob": -0.14176802302515784, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.004781537689268589}, {"id": 2102, "seek": 1211738, "start": 12125.539999999999, "end": 12134.179999999998, "text": " years now and basically this structure binds the region of the of the spike protein that interacts", "tokens": [50772, 924, 586, 293, 1936, 341, 3877, 41515, 264, 4458, 295, 264, 295, 264, 21053, 7944, 300, 43582, 51204], "temperature": 0.0, "avg_logprob": -0.14176802302515784, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.004781537689268589}, {"id": 2103, "seek": 1211738, "start": 12134.179999999998, "end": 12139.46, "text": " with the ACE2 receptor of the host so that's where how the virus enters into into our body", "tokens": [51204, 365, 264, 44606, 17, 32264, 295, 264, 3975, 370, 300, 311, 689, 577, 264, 5752, 18780, 666, 666, 527, 1772, 51468], "temperature": 0.0, "avg_logprob": -0.14176802302515784, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.004781537689268589}, {"id": 2104, "seek": 1211738, "start": 12140.339999999998, "end": 12146.259999999998, "text": " and here we also tested it so we have the structure from cryoEM we also tested it on", "tokens": [51512, 293, 510, 321, 611, 8246, 309, 370, 321, 362, 264, 3877, 490, 3305, 78, 6683, 321, 611, 8246, 309, 322, 51808], "temperature": 0.0, "avg_logprob": -0.14176802302515784, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.004781537689268589}, {"id": 2105, "seek": 1214626, "start": 12146.26, "end": 12154.1, "text": " different variants of the virus so the alpha beta and omicron that probably everybody was", "tokens": [50364, 819, 21669, 295, 264, 5752, 370, 264, 8961, 9861, 293, 3406, 299, 2044, 300, 1391, 2201, 390, 50756], "temperature": 0.0, "avg_logprob": -0.10986912250518799, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.004889389034360647}, {"id": 2106, "seek": 1214626, "start": 12154.1, "end": 12160.98, "text": " following in the newspapers so you see that that it binds many of these maybe some others less", "tokens": [50756, 3480, 294, 264, 20781, 370, 291, 536, 300, 300, 309, 41515, 867, 295, 613, 1310, 512, 2357, 1570, 51100], "temperature": 0.0, "avg_logprob": -0.10986912250518799, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.004889389034360647}, {"id": 2107, "seek": 1214626, "start": 12161.62, "end": 12167.78, "text": " and here's also a comparison to a clinically approved drug so that was antibodies that were", "tokens": [51132, 293, 510, 311, 611, 257, 9660, 281, 257, 48392, 10826, 4110, 370, 300, 390, 28356, 300, 645, 51440], "temperature": 0.0, "avg_logprob": -0.10986912250518799, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.004889389034360647}, {"id": 2108, "seek": 1214626, "start": 12167.78, "end": 12174.18, "text": " developed by AstraZeneca so basically what is shown here is how much inhibition you have", "tokens": [51440, 4743, 538, 45242, 57, 44852, 370, 1936, 437, 307, 4898, 510, 307, 577, 709, 20406, 849, 291, 362, 51760], "temperature": 0.0, "avg_logprob": -0.10986912250518799, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.004889389034360647}, {"id": 2109, "seek": 1217418, "start": 12174.18, "end": 12180.34, "text": " versus concentration so the smaller concentration the better of course so we are not as good as", "tokens": [50364, 5717, 9856, 370, 264, 4356, 9856, 264, 1101, 295, 1164, 370, 321, 366, 406, 382, 665, 382, 50672], "temperature": 0.0, "avg_logprob": -0.10133908987045288, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.00420153234153986}, {"id": 2110, "seek": 1217418, "start": 12180.34, "end": 12186.1, "text": " the AstraZeneca drug but so it's something that was designed totally computationally and this is", "tokens": [50672, 264, 45242, 57, 44852, 4110, 457, 370, 309, 311, 746, 300, 390, 4761, 3879, 24903, 379, 293, 341, 307, 50960], "temperature": 0.0, "avg_logprob": -0.10133908987045288, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.00420153234153986}, {"id": 2111, "seek": 1217418, "start": 12186.1, "end": 12193.94, "text": " actually pseudovirus neutralization so it is probably much closer to real validation than", "tokens": [50960, 767, 25505, 532, 5179, 9619, 10598, 2144, 370, 309, 307, 1391, 709, 4966, 281, 957, 24071, 813, 51352], "temperature": 0.0, "avg_logprob": -0.10133908987045288, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.00420153234153986}, {"id": 2112, "seek": 1217418, "start": 12193.94, "end": 12200.98, "text": " at least anything that myself as a computer scientist could think of well I think I will", "tokens": [51352, 412, 1935, 1340, 300, 2059, 382, 257, 3820, 12662, 727, 519, 295, 731, 286, 519, 286, 486, 51704], "temperature": 0.0, "avg_logprob": -0.10133908987045288, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.00420153234153986}, {"id": 2113, "seek": 1220098, "start": 12200.98, "end": 12207.539999999999, "text": " probably stop here but if you think of diffusion models right so generative models everybody is", "tokens": [50364, 1391, 1590, 510, 457, 498, 291, 519, 295, 25242, 5245, 558, 370, 1337, 1166, 5245, 2201, 307, 50692], "temperature": 0.0, "avg_logprob": -0.12224418976727654, "compression_ratio": 1.7832699619771863, "no_speech_prob": 0.003083591116592288}, {"id": 2114, "seek": 1220098, "start": 12208.66, "end": 12212.66, "text": " now talking about right like like the Dali2 and now of course you have way better results", "tokens": [50748, 586, 1417, 466, 558, 411, 411, 264, 413, 5103, 17, 293, 586, 295, 1164, 291, 362, 636, 1101, 3542, 50948], "temperature": 0.0, "avg_logprob": -0.12224418976727654, "compression_ratio": 1.7832699619771863, "no_speech_prob": 0.003083591116592288}, {"id": 2115, "seek": 1220098, "start": 12213.3, "end": 12219.859999999999, "text": " so you could imagine something like this for for molecular design so we have some condition on", "tokens": [50980, 370, 291, 727, 3811, 746, 411, 341, 337, 337, 19046, 1715, 370, 321, 362, 512, 4188, 322, 51308], "temperature": 0.0, "avg_logprob": -0.12224418976727654, "compression_ratio": 1.7832699619771863, "no_speech_prob": 0.003083591116592288}, {"id": 2116, "seek": 1220098, "start": 12219.859999999999, "end": 12224.5, "text": " on let's say diffusion model that we use here like the geometric structure of the of the target", "tokens": [51308, 322, 718, 311, 584, 25242, 2316, 300, 321, 764, 510, 411, 264, 33246, 3877, 295, 264, 295, 264, 3779, 51540], "temperature": 0.0, "avg_logprob": -0.12224418976727654, "compression_ratio": 1.7832699619771863, "no_speech_prob": 0.003083591116592288}, {"id": 2117, "seek": 1220098, "start": 12224.5, "end": 12230.1, "text": " pocket and you'll try to build a molecule that satisfies these these constraints so we don't", "tokens": [51540, 8963, 293, 291, 603, 853, 281, 1322, 257, 15582, 300, 44271, 613, 613, 18491, 370, 321, 500, 380, 51820], "temperature": 0.0, "avg_logprob": -0.12224418976727654, "compression_ratio": 1.7832699619771863, "no_speech_prob": 0.003083591116592288}, {"id": 2118, "seek": 1223010, "start": 12230.1, "end": 12235.220000000001, "text": " really have a text prompt but you have maybe some some other way of conditioning the model so", "tokens": [50364, 534, 362, 257, 2487, 12391, 457, 291, 362, 1310, 512, 512, 661, 636, 295, 21901, 264, 2316, 370, 50620], "temperature": 0.0, "avg_logprob": -0.06662833336556312, "compression_ratio": 1.8687258687258688, "no_speech_prob": 0.0024541153106838465}, {"id": 2119, "seek": 1223010, "start": 12236.02, "end": 12239.94, "text": " so this is one example maybe not very interesting so another example is what we call diffusion", "tokens": [50660, 370, 341, 307, 472, 1365, 1310, 406, 588, 1880, 370, 1071, 1365, 307, 437, 321, 818, 25242, 50856], "temperature": 0.0, "avg_logprob": -0.06662833336556312, "compression_ratio": 1.8687258687258688, "no_speech_prob": 0.0024541153106838465}, {"id": 2120, "seek": 1223010, "start": 12239.94, "end": 12245.62, "text": " linker where we have small molecular fragments what is called pharmacophores that you know how", "tokens": [50856, 2113, 260, 689, 321, 362, 1359, 19046, 29197, 437, 307, 1219, 31818, 5317, 2706, 300, 291, 458, 577, 51140], "temperature": 0.0, "avg_logprob": -0.06662833336556312, "compression_ratio": 1.8687258687258688, "no_speech_prob": 0.0024541153106838465}, {"id": 2121, "seek": 1223010, "start": 12245.62, "end": 12251.62, "text": " they bind the target but you also need to connect them into bigger molecule and we try to basically", "tokens": [51140, 436, 14786, 264, 3779, 457, 291, 611, 643, 281, 1745, 552, 666, 3801, 15582, 293, 321, 853, 281, 1936, 51440], "temperature": 0.0, "avg_logprob": -0.06662833336556312, "compression_ratio": 1.8687258687258688, "no_speech_prob": 0.0024541153106838465}, {"id": 2122, "seek": 1223010, "start": 12251.62, "end": 12257.7, "text": " to start with these little fragments and to diffuse the the the linking structure that that connects", "tokens": [51440, 281, 722, 365, 613, 707, 29197, 293, 281, 42165, 264, 264, 264, 25775, 3877, 300, 300, 16967, 51744], "temperature": 0.0, "avg_logprob": -0.06662833336556312, "compression_ratio": 1.8687258687258688, "no_speech_prob": 0.0024541153106838465}, {"id": 2123, "seek": 1225770, "start": 12257.7, "end": 12263.460000000001, "text": " them we're not very lucky in publishing this paper in europe so we'll probably send it to some", "tokens": [50364, 552, 321, 434, 406, 588, 6356, 294, 17832, 341, 3035, 294, 27207, 370, 321, 603, 1391, 2845, 309, 281, 512, 50652], "temperature": 0.0, "avg_logprob": -0.19478636202604874, "compression_ratio": 1.416058394160584, "no_speech_prob": 0.0041092513129115105}, {"id": 2124, "seek": 1225770, "start": 12263.460000000001, "end": 12270.820000000002, "text": " chemical journal uh well I think I will stop here sorry for running out of time thank you very much", "tokens": [50652, 7313, 6708, 2232, 731, 286, 519, 286, 486, 1590, 510, 2597, 337, 2614, 484, 295, 565, 1309, 291, 588, 709, 51020], "temperature": 0.0, "avg_logprob": -0.19478636202604874, "compression_ratio": 1.416058394160584, "no_speech_prob": 0.0041092513129115105}, {"id": 2125, "seek": 1227082, "start": 12271.22, "end": 12273.94, "text": " uh", "tokens": [50384, 2232, 50520], "temperature": 0.0, "avg_logprob": -0.31887429410761053, "compression_ratio": 1.4083333333333334, "no_speech_prob": 0.009659226052463055}, {"id": 2126, "seek": 1227082, "start": 12284.1, "end": 12287.38, "text": " yeah we are over time but if you have still a couple of questions", "tokens": [51028, 1338, 321, 366, 670, 565, 457, 498, 291, 362, 920, 257, 1916, 295, 1651, 51192], "temperature": 0.0, "avg_logprob": -0.31887429410761053, "compression_ratio": 1.4083333333333334, "no_speech_prob": 0.009659226052463055}, {"id": 2127, "seek": 1227082, "start": 12289.46, "end": 12291.619999999999, "text": " if not you can ask individual maybe", "tokens": [51296, 498, 406, 291, 393, 1029, 2609, 1310, 51404], "temperature": 0.0, "avg_logprob": -0.31887429410761053, "compression_ratio": 1.4083333333333334, "no_speech_prob": 0.009659226052463055}, {"id": 2128, "seek": 1227082, "start": 12293.539999999999, "end": 12297.94, "text": " okay but thank you again for for the amazing talk well thank you", "tokens": [51500, 1392, 457, 1309, 291, 797, 337, 337, 264, 2243, 751, 731, 1309, 291, 51720], "temperature": 0.0, "avg_logprob": -0.31887429410761053, "compression_ratio": 1.4083333333333334, "no_speech_prob": 0.009659226052463055}, {"id": 2129, "seek": 1230082, "start": 12300.82, "end": 12303.94, "text": " thank you", "tokens": [50376, 1309, 291, 50520], "temperature": 1.0, "avg_logprob": -1.6982006072998046, "compression_ratio": 0.5294117647058824, "no_speech_prob": 0.16875368356704712}], "language": "en"}