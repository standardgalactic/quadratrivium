WEBVTT

00:00.000 --> 00:02.400
Thank you very much.

00:02.400 --> 00:03.840
So great pleasure to be here.

00:03.840 --> 00:09.840
It's actually my second time in Krakow, and I think it's a very beautiful choice for

00:09.840 --> 00:12.320
having a machine learning summer school.

00:12.320 --> 00:17.320
So in the next three hours, I would like to talk about geometric deep learning.

00:17.320 --> 00:25.920
And if you wonder this intriguing image, what does it have to do with machine learning?

00:25.920 --> 00:29.960
So you see it fits more, a kind of an alchemist.

00:29.960 --> 00:38.680
So we wanted to use this image in reference to this famous quote from Ali Rahimi who was

00:38.680 --> 00:44.040
receiving the Best Paper Award or the Proof of Time Award at New Europe in 2017.

00:44.040 --> 00:48.040
And he was speaking this way maybe a little bit critically about deep learning, at least

00:48.040 --> 00:55.520
at that time, that we say things like machine learning is the new electricity, and he's

00:55.520 --> 01:00.880
alternative metaphor was machine learning has become alchemy, so in the sense that some

01:00.880 --> 01:05.560
kind of science that maybe produces something that we don't really understand what it does.

01:05.560 --> 01:11.840
And really what we would like to do here is to try to understand from certain perspective

01:11.840 --> 01:16.440
how these methods work and why they work, and maybe more importantly, when they fail

01:16.440 --> 01:22.440
and you see kind of maybe general blueprint for developing potentially future machine

01:22.440 --> 01:24.040
learning systems.

01:24.040 --> 01:30.800
So the concept that will be important to these lectures is the concept of symmetry.

01:30.800 --> 01:37.800
And symmetry according to Vile, who I'm quoting here is, depending on how wide or narrow you

01:37.800 --> 01:43.600
define its meaning, is an idea by which men through the ages has tried to comprehend and

01:43.600 --> 01:45.800
create order beauty and perfection.

01:45.800 --> 01:51.280
So it sounds a bit poetic, but I think it is true, so that's from his book that is titled

01:51.280 --> 01:54.840
Symmetry, that he published in Princeton.

01:54.840 --> 01:59.760
And symmetry is a Greek word, so it goes back to the ancient Greeks, and as you probably

01:59.760 --> 02:05.000
know, ancient Greeks like Plato considered symmetry to be really the cornerstone of the

02:05.000 --> 02:06.000
universe.

02:06.000 --> 02:12.040
So according to Plato, what is nowadays called platonic solids, symmetric polyhedra were the

02:12.040 --> 02:17.320
basic building blocks of all the stuff in the universe, so probably tiny little things

02:17.320 --> 02:23.040
that build up the matter, and if you think of it in modern terminology, that's not very

02:23.040 --> 02:24.800
far from truth.

02:24.800 --> 02:34.440
So Plato believed that geometry is really the key piece of mathematics, and even according

02:34.440 --> 02:39.400
to a legend, there was an inscription on the entrance to his academy saying that nobody

02:39.400 --> 02:44.880
skilled in geometry, that is not skilled in geometry, should be allowed to enter.

02:44.880 --> 02:49.520
And this idea of matter being built of small symmetric polyhedra, actually not far from

02:49.520 --> 02:54.840
truth, if you consider how crystals are organized, and the first to study this from a formal

02:54.840 --> 03:01.960
perspective was actually Kepler, who is maybe more famous for his discovery of the motion

03:01.960 --> 03:07.360
of planets, but he was also the one who laid the foundations of modern crystallography,

03:07.360 --> 03:11.440
basically considering how spheres can be packed into different configurations, and if you also

03:11.440 --> 03:17.480
think that this is old and boring and outdated stuff, so last year's Fields Medal was given

03:17.480 --> 03:22.680
exactly for solving these kind of problems maybe in higher dimensions.

03:22.680 --> 03:29.240
So geometry itself also, at least in formal way, goes back to ancient Greeks, and what

03:29.240 --> 03:35.240
we still often study at school as the geometry dates back to Euclid, his famous elements,

03:35.240 --> 03:41.520
so a system of axioms from which his geometry was derived, and as you know there are five

03:41.520 --> 03:46.880
axioms or five postulates of Euclidean geometry, that the last one was somehow standing out

03:46.880 --> 03:53.640
and people for many centuries or even thousands of years tried to do something with it, and

03:53.640 --> 04:01.200
for example in the 18th century Giovanni Saccheri, who was a priest, almost arrived to the construction

04:01.200 --> 04:08.120
of a non-Euclidean geometry, but he considered it such a heretical idea that he thought that

04:08.120 --> 04:13.400
it is repugnant to the nature of straight lines, so he abandoned these ideas and never

04:13.400 --> 04:19.040
took it to the full extent, but in the 19th century what happened is that finally came

04:19.040 --> 04:25.560
the realization that dethroned Euclid and broke his monopoly of geometry and the works

04:25.560 --> 04:31.560
that probably gauss himself did, but never published, and then famously Lobochevsky,

04:31.560 --> 04:38.240
Boje and Riemann, who created the first examples of what we now call non-Euclidean geometries,

04:38.240 --> 04:43.760
and in the 19th century this is how the geometry started looking like, so a zoo of different

04:43.760 --> 04:49.840
types of geometry without clear relation or understanding what actually defines the geometry,

04:49.840 --> 04:55.280
so there was obviously a need to put order in this mess, and the new idea, new approach

04:55.280 --> 05:03.840
came from Felix Klein in 1872, so he was only 23 years old, he was lucky to get an appointment

05:03.840 --> 05:07.440
as a professor at the University of Erlangen in Bavaria, so this is something that for

05:07.440 --> 05:15.000
example Euler failed to have in Switzerland, he had to go to Russia to become a faculty,

05:15.000 --> 05:20.800
probably not very dissimilar to situations that some of us are facing in these days,

05:20.800 --> 05:27.640
so Klein was asked, as it was customary in Germany and still customary I think, to deliver

05:27.640 --> 05:31.760
a research prospectus, basically to explain what he is going to do until his retirement

05:31.760 --> 05:36.000
in Erlangen, which actually never happened because he moved three years after to different

05:36.000 --> 05:42.720
places eventually to GÃ¶ttingen, and in this prospectus that entered the history of mathematics

05:42.720 --> 05:48.080
as the Erlangen program, he proposed a kind of algebraization of geometry, so studying

05:48.080 --> 05:52.720
geometry from the perspective of group theory, so essentially considering a geometry as a

05:52.720 --> 05:58.360
space plus some class of transformations formalized using group theory, and studying properties

05:58.360 --> 06:04.480
that remain unchanged or invariant under these transformations, so you take an object and

06:04.480 --> 06:11.880
you apply to it rigid motions and you preserve a lot of things like areas, parallel lines,

06:11.880 --> 06:16.400
distances, so this is how you create Euclidean geometry, but you can consider other groups

06:16.400 --> 06:21.920
like a fine or projective group, and in fact he considered projective group to be the broadest

06:21.920 --> 06:27.560
construction, he in fact showed together with Biltrami that the first non-Euclidean geometry

06:27.560 --> 06:34.160
is hyperbolic, geometry is with negative curvature, could be constructed with a projective model,

06:34.160 --> 06:41.360
and these ideas had really big impact on geometry, on mathematics more broadly, I would say culturally,

06:41.360 --> 06:47.840
like category theory is an extension of these ideas to more abstract objects, but probably

06:47.840 --> 06:51.800
most importantly it had an impact on physics where came the realization in the beginning

06:51.800 --> 06:57.600
of the 19th century, probably starting with Neutra with her famous theorem that the laws

06:57.600 --> 07:03.880
of physics themselves can be derived from considerations of invariance or symmetry, and for example

07:03.880 --> 07:09.240
what Neutra showed is that principles like conservation of energy that previously were

07:09.240 --> 07:13.360
considered to be empirical could be derived mathematically from certain symmetries, so

07:13.360 --> 07:19.800
symmetry of time in this case, and these ideas in a more generalist form led to what nowadays

07:19.800 --> 07:27.080
is known as the standard model, so basically all the world can be modeled and can be derived

07:27.080 --> 07:32.960
from first principles of symmetry, so what I think physicists among you would call external

07:32.960 --> 07:37.480
symmetry or internal symmetry, the symmetry of the spacetime, so what is called the Poincare

07:37.560 --> 07:42.160
group that gives rise to Minkowski geometry of special relativity or internal symmetries

07:42.160 --> 07:47.680
of quantum fields, that's what gives rise to different forces or different interactions,

07:47.680 --> 07:53.040
and I think nobody put it better than Philip Anderson Nobel laureate in physics that without

07:53.040 --> 08:00.880
or with only slightly overstating, you can say that physics is the study of symmetry,

08:00.880 --> 08:06.840
so what does it all have to do with deep learning and neural networks and machine learning

08:06.840 --> 08:14.680
in general, so let's do maybe a brief detour into the history of machine learning or artificial

08:14.680 --> 08:20.720
intelligence, so the term artificial intelligence comes from these people, the Dartmouth conference

08:20.720 --> 08:28.360
that happened in 1956, organized by McCarthy and others, and you can see them, some very

08:28.360 --> 08:32.520
prominent figures sitting here, so this is for example, this is Claude Shannon and this

08:33.400 --> 08:39.240
is Marvin Minsky who would become all very important scientists, and historically apparently

08:39.240 --> 08:44.280
the term artificial intelligence was introduced to kind of distance themselves and not to be

08:44.280 --> 08:50.440
in the shadow of the expert of that time who was Norbert Wiener who introduced the term cybernetics,

08:51.000 --> 08:55.880
which I think is still used, I think here in Poland it's probably still used as a kind of

08:55.880 --> 09:00.840
overarching term for everything that it has to do with computer science and artificial intelligence,

09:01.480 --> 09:05.800
so around that time there were many works that tried to understand how the brain works,

09:05.800 --> 09:10.760
so I think at that time it was already understood that somehow our intelligence is concentrated

09:10.760 --> 09:17.720
in the brain, so models for neural networks, probably the most famous one is by Frank Rosenblatt,

09:17.720 --> 09:25.400
the so-called perceptron, but there are models before that, and he was able to show that was in

09:25.400 --> 09:31.960
the 50s, so these models had to be implemented in analog hardware, he was able to show that he

09:31.960 --> 09:37.800
can solve some simple pattern recognition problems with these neural networks, and it was extremely

09:37.800 --> 09:42.680
remarkable at that time, so there were articles in the popular press like the New Yorker saying

09:42.680 --> 09:48.360
that this is the first serious rival to the human brain ever devised, so I think you can only smile,

09:48.360 --> 09:53.480
I can hear you laughing, and it's remarkable machines capable of what amounts to thought,

09:53.480 --> 09:58.600
so that's according to New Yorker was the perceptron, and there was a little bit of

09:58.600 --> 10:05.560
hype, as you probably know, this MIT computer vision summer project, so they thought that they

10:05.560 --> 10:10.520
would be able to model a large part of the visual system over one summer, of course, we're still

10:10.520 --> 10:17.160
working on these problems 50 years after, and there is no end to it, but also at MIT these two

10:17.160 --> 10:22.040
guys, so one of them appeared already in the picture, Marvin Minsky in the same work, they

10:22.040 --> 10:27.400
published a highly famous or infamous, if you want, book called The Perceptrons, where they

10:28.280 --> 10:35.640
introduced mathematical analysis of these networks proposed by Rosenblatt, and they showed, for

10:35.640 --> 10:40.520
example, one example that is taken from this book, that a very simple logical function like

10:40.520 --> 10:46.760
exclusive or could not actually be implemented as a perceptron, so these patterns are not

10:47.400 --> 10:53.800
linearly separable, so that was a very harsh criticism for the models, and some people say

10:54.360 --> 10:59.560
in retrospective that this was what triggered the AI winter, so some people even say that there might

10:59.560 --> 11:06.440
have been some personal animosity between Rosenblatt and Minsky, they went to the same high school,

11:06.440 --> 11:14.200
and Rosenblatt also died in a boating accident, so as far as to suggest that it was a suicide,

11:14.920 --> 11:24.200
well, the story is probably much more mundane, so the funding that was cut by government agencies

11:24.200 --> 11:29.240
like DARPA was more related to them being more pragmatic and budget restricted, and that had an

11:29.240 --> 11:34.200
impact on the field that coincided with the publication of the book, but if you look at the

11:36.040 --> 11:41.160
substance of the problem, what Minsky and Pebert called in their book Perceptron was actually

11:41.160 --> 11:47.000
not exactly an architecture that was devised by Rosenblatt, so they actually clearly stated, so

11:47.000 --> 11:52.120
they refer to this architecture as simple perceptrons, and this is what we now typically

11:52.120 --> 11:57.480
understand by this term, so it's, as you know, it's just a linear combination of the input

11:57.480 --> 12:02.680
coordinates with learnable weights that go through a nonlinear activation, typically sigmoid or

12:04.360 --> 12:09.080
in simple cases sine function, but what is also very important, they were probably the first

12:09.080 --> 12:14.520
ones, at least to my knowledge, to use geometric approaches to machine learning problems, so

12:14.520 --> 12:20.280
they, for example, formulated this group invariance theorem that tell in which or to what kind of

12:20.280 --> 12:25.400
transformations in the input patterns the neural network will be invariant, and another interesting

12:25.400 --> 12:29.720
thing actually the subtitle of the book is an introduction to computational geometry, so that

12:29.720 --> 12:34.680
was the sixties, so this term didn't exist, they actually introduced it, and one of the reviews

12:34.680 --> 12:40.600
of the book that was critical was asking whether this is just some kind of new mathematical fad

12:40.600 --> 12:46.200
that will go away a few years after, and you probably know computational geometry is now

12:46.200 --> 12:52.200
very well established field, so it has remained there for a long time, but if you go into the

12:52.200 --> 12:57.480
substance of the discussion is actually the question is what kind of classes of functions

12:57.480 --> 13:01.800
these neural networks could represent, right, so what is what is called the expressive power,

13:02.360 --> 13:07.560
and there were results at that time, so coming from mathematicians in particular

13:08.680 --> 13:14.920
Komogorov and Arnold working in the Soviet Union that claim that you can take multi-dimensional

13:14.920 --> 13:19.640
functions and decompose them in this way, and basically any function could be decomposed in

13:19.640 --> 13:25.640
this way, so these kind of results are known as universal approximation, they go probably as far

13:25.720 --> 13:31.320
as the thirteenth problem formulated by David Hilbert, and the modern statements of these

13:31.880 --> 13:37.480
theorems are usually attributed to Seybenko and Hornig, these are late eighties, early nineties

13:37.480 --> 13:43.880
that are specific to deep neural networks, so what these results say is that if you have not a

13:43.880 --> 13:48.680
single layer of perception, but two layers like this, then you can approximate any continuous

13:48.680 --> 13:54.760
function to any desired accuracy, so the results it's a class of results, it's not a single result,

13:55.240 --> 14:00.520
but roughly the way that we can understand it is that with just a configuration like this,

14:00.520 --> 14:05.480
with just two neurons like this, you can approximate, you can represent a step function,

14:06.120 --> 14:09.880
once you can represent step functions, you can decompose a continuous function into

14:09.880 --> 14:14.600
tiny little steps, so the proof is slightly more involved, but that's that's roughly the idea.

14:15.880 --> 14:21.400
Now it's sort of constructive proof, and there are different versions for limited widths or

14:21.400 --> 14:26.520
limited depths, it's sort of constructive proof, so it doesn't tell you how, so it's an existence

14:26.520 --> 14:31.560
result, so it tells you that you can find a neural network with potentially a very large

14:31.560 --> 14:36.840
number of neurons that approximate functions, and if you look at machine learning problems,

14:36.840 --> 14:41.320
so in a very maybe simple and naive setting, like classifying images of cats and dogs,

14:42.760 --> 14:47.400
basically you can think of it as some cynicism say that deep learning is a curve fitting problem,

14:47.480 --> 14:52.440
so it's multi-dimensional curve fitting, so there is some kind of black box where you put

14:54.360 --> 14:59.160
some something that acts as a universal approximator, so some sufficiently rich

14:59.800 --> 15:05.880
architecture, and you try to represent the function that distinguishes between cats and dogs

15:05.880 --> 15:11.160
in this way. Of course this is not a well-defined problem, if I give you a finite sample of,

15:11.160 --> 15:16.920
let's say these are cats and dogs, I can pass infinitely many functions through these points,

15:16.920 --> 15:21.960
so I can interpolate these points in infinitely many ways, so we need somehow to restrict our

15:21.960 --> 15:27.320
class of functions, and that's typically what you do by imposing some sort of regularity,

15:27.320 --> 15:32.760
and mathematicians have very well understood concepts of regularity like Lipschitz continuity,

15:32.760 --> 15:39.320
right, so in simple case you can think of it as a function with bounded derivatives, right,

15:39.320 --> 15:44.520
and the problem is what happens when you increase the dimensionality of your input when it doesn't

15:44.520 --> 15:48.920
look like a one-dimensional curve, but it's an n-dimensional curve, and here the results,

15:48.920 --> 15:55.880
unfortunately, are not favorable because you can show that as you grow the number of dimensions,

15:55.880 --> 16:02.520
the number of samples that you need to take in order to approximate a function to accuracy epsilon

16:02.520 --> 16:07.160
grows exponentially with a number of dimensions, so this is again, it's not a single result,

16:07.160 --> 16:13.560
it's a class of phenomena that are called the curse of dimensionality, so it's a statistical or

16:13.560 --> 16:19.560
geometric phenomena that explains how functions behave in high dimensions, and the term itself

16:19.560 --> 16:26.840
actually goes back to Bellman who spoke about the curse of dimensionality in physical problems.

16:27.720 --> 16:33.320
Now, in these problems, simply if you think of images, right, of even something like 30 by 30

16:33.320 --> 16:38.760
pixels, which is probably the smallest image you can imagine, digits from the MNIST dataset,

16:38.760 --> 16:43.160
the number of dimensions will be approximately thousands, so if you count the number of samples

16:43.240 --> 16:47.640
that you need to take, if you took this kind of straightforward approach, it will probably be

16:47.640 --> 16:51.880
more than the number of, not only the cats or dogs on earth, but probably close to the number of

16:51.880 --> 16:57.560
particles in the universe, so there are not sufficiently many animals around just to do it,

16:58.120 --> 17:04.920
and this kind of problem of curse of dimensionality, right, or you can also call it combinatorial

17:04.920 --> 17:10.280
explosion, was brought up in another report that is associated with the AI winter, which is the

17:10.280 --> 17:16.360
Lighthill report in the 70s. It was commissioned by the analogy of the DARPA, or basically the

17:16.360 --> 17:23.480
British funding agencies, and that was the point of time where they decided to stop funding these

17:23.480 --> 17:30.760
crazy ideas in looking at neural networks. So what happened, of course, was this AI winter,

17:30.760 --> 17:36.120
but at the same time people continued working on these architectures, and some interesting ideas

17:36.120 --> 17:40.760
came from the field of neuroscience, from the study of the organization and the function of

17:40.760 --> 17:47.240
the visual cortex, the famous experiments in the 50s and the 60s done by Hubel and Wiesel,

17:47.240 --> 17:52.120
a duo from Harvard that won the Nobel Prize in medicine for understanding the organization

17:52.120 --> 17:58.520
of the visual cortex, where what they found is that the cells in the cortex were organized

17:58.520 --> 18:05.080
with some local shared weights, and this was reproduced by Fukushima in his famous neocognitron

18:05.080 --> 18:12.520
paper in 1980. So the idea here was a neural network that does, it has two types of neurons,

18:12.520 --> 18:20.120
so neurons that he called simple neurons and neurons that he called complex neurons, so one

18:20.120 --> 18:25.240
in modern terminology that would correspond to local filters and pooling operations.

18:26.120 --> 18:31.880
And he worked on OCR type problems, so character recognition, and the problem,

18:31.880 --> 18:38.600
of course, if you treat this type of problems with the standard perceptrons, if I give you a

18:38.600 --> 18:44.040
digit of digit three, and I move it just by one pixel, you see that the input into the neural

18:44.040 --> 18:49.880
network can change dramatically, and in fact he complained that perceptrons were not by design

18:49.880 --> 18:55.080
invariance to these translations. So his architecture actually is remarkably modern

18:55.080 --> 19:00.840
by modern standards, so it was seven layer networks, so I think we can call it deep by

19:00.840 --> 19:05.720
modern standards. It had local connectivity, what neuroscientists called receptive fields.

19:06.920 --> 19:12.840
The filters were non-linear, and he wrote in neuroscience terminologies, so he talked about

19:12.840 --> 19:18.920
inhibition and activation. He had average pooling, so these are the complex layers.

19:20.200 --> 19:27.160
He used reloactivation, so already in the late 60s that was common, but the training was not done

19:27.160 --> 19:31.640
using backpropagation, so it was a kind of unsupervised type of clustering approach.

19:31.640 --> 19:37.000
And backpropagation, again, it existed already in maybe some other forms, but this is how neural

19:37.000 --> 19:41.720
networks were trained. So Rosenblatt had a special rule for a single layer perceptron,

19:41.720 --> 19:46.200
then there were some other methods that were developed in the 60s, and then backprop became

19:46.200 --> 19:50.920
really popular in the 80s, starting from the paper of Rumelhardt.

19:51.160 --> 19:58.440
So Lecante was just a fresh graduate at that time, and he was working on, actually,

19:58.440 --> 20:05.080
the application of backpropagation neural networks, was interested in Neocognitron,

20:05.080 --> 20:10.520
and he also happened to work at AT&T, which they were developing the first digital signal

20:10.520 --> 20:15.720
processors at that time. So basically, there was a good application to implement on a DSP,

20:15.720 --> 20:22.920
and basically, he stripped down the kind of neuroscience terminology of Fukushima,

20:22.920 --> 20:27.400
replaced nonlinear filters by linear filters that could be implemented as convolutions.

20:27.400 --> 20:32.680
Actually, the first paper never mentioned the term convolution, and the name came after in,

20:32.680 --> 20:41.400
I think, in 89, or even later, and he was able to show in real time complex pattern recognition

20:41.400 --> 20:45.800
tasks, such as recognition of handwritten digit, which was a difficult task at that time.

20:45.800 --> 20:49.880
It was actually deployed in commercial applications. They were working with banks

20:50.920 --> 20:55.800
in the post office, and that's where the MNIST dataset comes from. But the computer vision

20:55.800 --> 21:04.520
community took a different path, and the late 90s and the early 2000s, probably until the first

21:04.520 --> 21:11.000
decade of this century, the approaches that you would typically use for image recognition

21:11.000 --> 21:18.360
were to detect some local features, then compute local feature descriptors, and then create some

21:18.360 --> 21:23.000
representation that would be passed into some very simple classifier, like a support vector

21:23.000 --> 21:27.080
machine, which also were considered to be more favorable by mathematicians, because

21:27.080 --> 21:32.600
you can prove, for example, global optimality results about them. So there are many papers

21:32.600 --> 21:37.800
written, so SIFT, the scale environment feature transform, one of the most cited papers in computer

21:37.800 --> 21:46.200
science ever, was extremely well engineered detector and feature descriptor for these tasks.

21:47.240 --> 21:53.000
And what happened in 2012, as you probably all know, that all these carefully designed handcrafted

21:53.000 --> 22:00.920
features were beaten down by a very large margin by a convolutional neural network. So it was this

22:00.920 --> 22:07.080
lack of confidence of the computational capabilities of hardware, the GPUs, as well as availability

22:07.080 --> 22:12.520
of large data, the image net benchmark, which contained millions of annotated images, that

22:12.520 --> 22:18.120
finally allowed these architectures to shine. And since then, all the best results in this

22:18.120 --> 22:24.760
benchmark were by deep learning. So if you look at the AlexNet architecture that won this benchmark

22:24.760 --> 22:30.200
in 2012, it is more or less the same as what was done by Lekana. It's just slightly deeper. There are

22:30.200 --> 22:36.920
some different architectural choices. It has way more parameters. It was trained on GPUs, which by

22:36.920 --> 22:42.840
the way was not novel. GPUs were used for general purpose computing at least a decade before and

22:42.840 --> 22:49.160
for training neural networks probably at least seven years before. So in a sense, it was a very

22:49.160 --> 22:55.800
careful engineering and application of existing ideas on a very large dataset and a very important

22:55.800 --> 23:03.400
problem that convinced everyone. Yeah, there is a question. Sorry to go back a couple of slides,

23:03.400 --> 23:11.560
but you were mentioning Fukushima's implementation and no sort of back propagation learning. So

23:11.560 --> 23:20.760
like was he giving some kind of neuroscience, how do you say, a proof for this? Like some kind of

23:20.760 --> 23:27.400
happy learning or how did the... I don't remember what kind of rule he used. I think it was inspired

23:27.400 --> 23:34.120
by some hypothetical ideas of how the brain learns. Yeah, but it was not back propagation.

23:34.120 --> 23:38.440
He showed many other things. So he showed, for example, geometric stability, stability to noise,

23:38.440 --> 23:45.320
but again, it was in the early 80s. So the training examples were very rudimentary. So

23:45.320 --> 23:53.640
black and white letters. Thank you. Right. So basically, all the rest is obviously history,

23:53.640 --> 24:00.040
right? So the people who started the deep learning thing got the Turing Award and now

24:00.040 --> 24:06.040
very famous. And basically, this is technology that has really transformed the field both

24:06.760 --> 24:12.680
the academical subjects and the industry. So just to give you, we'll be talking about

24:12.680 --> 24:16.200
graph neural networks and you've probably heard from Miguel as well in the previous lectures

24:16.760 --> 24:20.920
about graph neural networks. So just to give you a little bit of history of this one,

24:21.560 --> 24:27.800
so they're actually very much related and rooted in chemistry. And chemistry is probably one of

24:27.800 --> 24:34.280
the fields of science, which is data intensive. It produces a huge amount of data, experimental data.

24:34.280 --> 24:39.320
And since early times, chemists tried to organize these data first, publishing these

24:39.320 --> 24:44.520
humongous books containing information about molecules or chemical reactions. And then

24:45.320 --> 24:52.600
it appeared the first digitized archive, the chemical abstract service. And also with the

24:52.600 --> 24:57.560
appearance of the first computers came the need and the idea to search for molecules, right? So

24:57.560 --> 25:02.760
if, for example, you're a pharmaceutical or a chemical company and you want to patent a new

25:02.760 --> 25:08.440
molecule, how do you know that it has not already been described, right? So you need to search fast

25:08.440 --> 25:14.520
for molecular structures in some data set. And these were the first ideas of chemical ciphers

25:14.520 --> 25:20.040
that describe a molecule as a string and then try to match it in some data set. So that was the kind

25:20.040 --> 25:28.040
of problems that these guys, or he was a Romanian chemist called Vladus, that was one of the pioneers

25:28.040 --> 25:34.360
of a field that later became known as chemoinformatics, he was trying to look at molecules as graphs.

25:35.000 --> 25:39.000
And as you probably know, the term graph itself, in the sense of graph theory,

25:39.000 --> 25:44.280
also is associated with chemistry. This is how Sylvester called the first attempts to design

25:44.280 --> 25:49.640
structural molecules, structural formula of molecules, basically trying to understand how

25:49.640 --> 25:55.320
atoms are related to each other with their chemical bonds, not only just the number of atoms

25:55.320 --> 26:00.040
and the types of atoms. And as you probably know, this was one of the first structural formula of

26:00.040 --> 26:07.320
benzene that was derived by Kecoli. And the legend says that he dreamt of a snake biting

26:08.200 --> 26:14.840
his own tail, so he came up with this kind of aromatic ring that is described here. So

26:16.120 --> 26:20.120
these kind of problems inspired these duo of mathematicians, we'll talk about them later,

26:20.760 --> 26:26.920
Weisfried and Lehmann in the 60s to devise an algorithm that would test whether two graphs

26:27.000 --> 26:32.840
are structurally similar, what is called the graph isomorphism test. And these ideas maybe were

26:33.800 --> 26:39.480
noticed, so there are many related works in the machine learning community and also in

26:39.480 --> 26:45.880
a chemical community, trying to devise new types of neural networks that could take as input,

26:45.880 --> 26:52.840
not vectors, not images, but graph structured data. And the early works by Alessandro Sperduti

26:52.840 --> 26:58.280
in the 90s, for some reason most of the works were by Italians, and probably the most cited ones

26:58.280 --> 27:04.840
are by Marco Gore, Scarcelli and others. And interestingly, about 10 years ago, the graph

27:04.840 --> 27:11.480
neural networks returned triumphantly to chemistry, so I think worth crediting David Duvenau and Justin

27:11.480 --> 27:18.360
Gilmer for who also introduced the terminology of message passing neural networks that try to

27:18.360 --> 27:23.480
predict properties of molecules, model these graphs and learning on these graphs. And then,

27:23.480 --> 27:27.480
of course, the ideas of geometric learning, as we'll see maybe with some extra stuff.

27:28.840 --> 27:36.680
Also, the structural biologists have had their own image net moment with alpha fold, first in

27:36.680 --> 27:44.600
2018 and then in 2020, basically predicting the structure of protein folds. So, and this field

27:44.600 --> 27:49.000
is very rapidly developing. So, I think these are very exciting and cool problems that you can

27:49.000 --> 27:54.920
address with geometric techniques. So, let's just try to summarize basically what this historical

27:54.920 --> 28:00.680
excursion gives us, a kind of blueprint for different architectures. So, if you look at convolutional

28:00.680 --> 28:05.240
neural networks and graph neural networks, right, they work with very different data, convolutional

28:05.240 --> 28:10.840
neural networks work on images, graph neural networks work on graphs, right, let's say molecules,

28:10.920 --> 28:16.120
but there are some common patterns, right. So, in both cases, we have some underlying domain.

28:16.120 --> 28:20.840
So, in the first case, it's agreed. In the second case, it's a graph. We also, in both cases, have

28:20.840 --> 28:28.120
some kind of geometric operation, so a symmetry, right, that is a nature in the context of the

28:28.120 --> 28:32.440
problems that we are considering. So, in images, it's translation, right. So, I want to move,

28:32.440 --> 28:36.600
for example, an object in the image and I don't care where the object is located if I want to

28:36.600 --> 28:42.520
classify it. In case of molecules, it's a permutation symmetry, so no matter how I order

28:42.520 --> 28:46.200
the atoms in a molecule, it's still the same molecule, right. So, I want somehow to be

28:46.200 --> 28:54.280
insensitive to this ordering. And we can also define natural operations that respect this

28:54.280 --> 28:58.280
symmetry, right. So, in case of convolutional neural networks, it's actually the convolutional

28:58.280 --> 29:04.920
operation. Basically, I can move a patch around an image and apply the same weights or the same

29:04.920 --> 29:10.680
local rule that would extract the features. And as we'll see in graph neural networks,

29:10.680 --> 29:14.760
this is some kind of local rule that we call message passing, right, or some versions of it.

29:16.520 --> 29:22.760
So, these are ideas that, as you see, these are two examples or architectures that share the

29:22.760 --> 29:28.280
common principles and that's the idea of geometric deep learning. So, I can probably take the craze

29:28.280 --> 29:37.400
of inventing the term. So, that happened when I was writing one of my ERC grants back in 2015,

29:37.400 --> 29:42.440
probably. And of course, everybody was doing deep learning. So, you need to distinguish yourself

29:42.440 --> 29:48.600
from everyone. So, I wrote that we are not doing deep learning as everybody else, we are doing

29:48.600 --> 29:56.200
geometric deep learning. So, that was the idea. Then we popularized it in this paper in IEEE

29:56.200 --> 30:03.960
signal processing magazine and more recently in a book that I'm writing with my collaborators.

30:03.960 --> 30:09.880
So, by analogy to the Erlangian program, we basically, we can think of a kind of common

30:09.880 --> 30:14.440
denominator for machine learning architectures. So, we would like to take this zoo of different

30:14.440 --> 30:18.680
architectures that were historically designed for different types of data with different

30:19.400 --> 30:25.400
kind of problem in mind and look at them from the same perspective. And this perspective is

30:25.400 --> 30:32.440
through the lens of group theory and properties like invariance, equivariance and symmetry.

30:32.440 --> 30:38.040
And if we've seen before this problem of machine learning in high dimension, where you have,

30:38.040 --> 30:43.240
for example, your images of cats and dogs as points in a high-dimensional space, we no longer

30:43.240 --> 30:50.840
consider these inputs as just a high-dimensional vector that you need to put through some generic

30:50.840 --> 30:57.080
class of functions and then you suffer from the curse of dimensionality. But now we know that

30:57.080 --> 31:03.320
there is some domain, geometric structure that underlies these inputs. And in images, for example,

31:03.320 --> 31:08.920
this is a two-dimensional grid. So, our data lives on some typically low-dimensional domain.

31:08.920 --> 31:16.440
And this domain comes equipped with some group. So, in this case, it's the group of translations.

31:17.240 --> 31:22.760
And so, we have a domain, we have a group, we have signals that live on this domain and the

31:22.760 --> 31:29.240
group that acts on the points of the domain, we see how it can act on the signals themselves

31:29.240 --> 31:34.440
through what is called the group representation. And in case of images, again, the group representation

31:34.440 --> 31:40.360
will be just the shift operator. We'll see it in more details. And then finally, we have functions

31:40.360 --> 31:45.560
that act on these inputs that somehow need to respect this symmetry. And this will be through

31:45.560 --> 31:50.840
what we will call invariance and equivariance. So, basically, we want the function either to be

31:51.800 --> 31:58.600
insensitive to how I transform the input by acting on it with the group or should change in the same

31:58.600 --> 32:04.920
way. And I should say that the choice of the group and the domain are two separate things and they're

32:04.920 --> 32:09.160
not only dependent on the data, they're also dependent on the task. So, I might have a problem

32:09.160 --> 32:13.720
like this. So, I have, for example, images of traffic signs. And if I'm designing a self-driving

32:13.720 --> 32:19.640
car, it's very unlikely that I will see rotated traffic signs, right? They will probably be aligned

32:19.640 --> 32:25.480
and move just horizontally and maybe vertically. So, here, for example, the group of transformations

32:25.480 --> 32:30.920
that is reasonable to assume will be just two-dimensional translation, maybe even one-dimensional

32:30.920 --> 32:35.800
translation. But if, for example, my car can also tilt, right, so imagine that it's a plane and not

32:35.800 --> 32:40.040
a car, then rotations are also perfectly valid, right? So, then the group of transformation

32:40.040 --> 32:44.120
will be different. So, it's still the same domain, the same data, but the task might be different

32:44.120 --> 32:50.040
and, therefore, the assumptions about this, the priorities problem might be different.

32:50.040 --> 32:54.200
And if you think of another application, if I have, for example, a pathological sample,

32:55.000 --> 33:00.280
so, essentially, a glass of some stained tissue that I put under a microscope, so there I can also

33:00.280 --> 33:05.400
have reflections, right? Because I don't have canonical orientation for this glass. So, it can

33:05.400 --> 33:12.920
be an even bigger group in this case and, again, task dependent. So, as I mentioned in case of

33:12.920 --> 33:20.200
images, the representation that we'll be working with is the shift operator. In case of crafts,

33:20.200 --> 33:25.480
actually, the symmetry, as we've seen it, is the group of permutations, what is sometimes called,

33:25.480 --> 33:32.360
confusingly, the symmetric group. So, it's the different ways that I can rearrange

33:32.360 --> 33:38.200
and different objects. And its representation is a permutation matrix. And as we'll see,

33:39.080 --> 33:44.280
the way that it's implemented, so, functions that are equivalent with respect to this symmetry

33:45.160 --> 33:50.680
are message passing. And we can also have another type of architectures that are also confusingly

33:50.680 --> 33:55.560
called equivalent neural networks or equivalent transformers, where, in addition to the symmetry

33:55.560 --> 34:00.840
group of the domain, we also have symmetry group of the data. And this is typical for geometric

34:00.840 --> 34:05.000
graphs like molecules, where the nodes also have geometric coordinates. So, they live in three

34:05.000 --> 34:10.360
dimensional space. And in addition to reordering the atoms in the molecule, we can also rotate or

34:10.360 --> 34:15.480
translate the molecule in three dimensional space, right? So, you want to be equivalent with respect

34:15.480 --> 34:20.280
to both transformations. So, it's a kind of analogy of the external and internal symmetries

34:20.280 --> 34:25.800
you have in physics. And, basically, geometric architecture is just sequences of

34:26.600 --> 34:31.560
equivalent or invariant layers. You can also interleave them with pooling. So, I will not talk

34:31.560 --> 34:36.520
about it too much. But pooling is implementation of another principle that is important in physics,

34:36.520 --> 34:42.280
which is called scale separation. And this is what makes physics work. So, if you consider, for

34:42.280 --> 34:48.200
example, let's say, this room, right? And how we are surrounded by probably a quadrillion of

34:48.200 --> 34:54.680
different molecules that move very fast and collide with each other. But that's not how we can model

34:54.680 --> 35:01.880
the behavior of gas in some space, right? It's computation intractable to trace all the molecules.

35:01.880 --> 35:07.880
So, fortunately, there are just a few parameters that explain statistically how these gas behaves,

35:07.880 --> 35:13.480
right? Like temperature and pressure. And this is the main principle of statistical mechanics.

35:13.480 --> 35:17.400
But if we want, for example, to model how Earth, with its very complicated atmosphere,

35:17.400 --> 35:22.440
moves around the sun, again, we can completely disregard it and consider it as a point. Because

35:22.440 --> 35:28.360
at that scale, all these details are completely irrelevant. So, the scales, of course, interact

35:28.360 --> 35:34.040
with each other. So, this is maybe a wishful thinking. And in neural networks, you can also

35:34.040 --> 35:42.120
mathematically show that in some architectures, pooling operations are necessary for them to

35:42.120 --> 35:48.200
operate correctly. So, these ideas can be applied to different types of objects, to geometric domains.

35:48.200 --> 35:52.600
So, traditionally to grids, but then also to graphs, maybe to general

35:53.640 --> 35:59.000
homogeneous spaces, and then also maybe to more exotic things like manifolds, meshes and

35:59.000 --> 36:03.400
geometric graphs. And if you look at some of the standard architectures that are very commonly

36:03.400 --> 36:09.880
used in deep learning, whether it's CNNs or maybe LSTMs or deep sets or transformers or GNNs or

36:10.600 --> 36:15.480
intrinsic mesh convolutional networks, they can all be derived from the same blueprint. So,

36:15.480 --> 36:21.400
there is some kind of domain and associated symmetry and associated environments that you

36:21.400 --> 36:27.560
can bake into your architecture and you get, basically, particular instances of this blueprint,

36:27.560 --> 36:34.520
some of the most common and famous architectures. Any questions so far before we start talking

36:34.520 --> 36:41.560
in detail about graphs? So, if I have time, I will try to cover all these different domains, but I

36:41.560 --> 36:48.360
would probably spend most time on graphs and also some physics-inspired perspective on these

36:48.360 --> 36:54.280
architectures. Yes, I will have questions. So, if you kind the term geometric deep learning,

36:54.280 --> 36:59.400
is there an alternative, like non-geometric deep learning or like traditional deep learning,

36:59.400 --> 37:06.120
or what does it mean? Well, so, it's a very broad term. So, I think we use it maybe in a very

37:07.080 --> 37:13.720
in a very broad sense. So, it's using geometric ideas or geometric techniques to

37:14.440 --> 37:21.080
interpret and build deep learning architectures and vice versa, applying deep learning to

37:21.080 --> 37:26.040
geometric objects. Now, whether, for example, the model of group environments and equity

37:26.040 --> 37:30.600
environments is really the kind of ultimate truth, of course not. So, it's a mathematical

37:30.600 --> 37:34.920
abstraction and in most cases, the transformations, for example, you have an image is, they're

37:34.920 --> 37:40.200
actually not well described by groups, but at least it's a good starting point. In many cases,

37:40.200 --> 37:47.560
for example, in molecules, this is kind of physical realities or an inductive bias that you

37:47.560 --> 37:55.960
rather want to incorporate in your architecture. Okay, I have one more question. Could you please

37:55.960 --> 38:01.560
take back to the previous slide? Yeah. Because you mentioned like the existing

38:01.640 --> 38:09.720
neural networks architectures and the group of symmetries. And do you think there is any

38:10.760 --> 38:17.160
other group of symmetries that we in real world applications should think about? And

38:17.960 --> 38:27.800
because of that, create some new architectures that is, I didn't know, for example, was suited for

38:28.520 --> 38:33.400
them? Yeah. So, it's a good question. So, there are some other architectures, for example, mostly

38:33.400 --> 38:37.880
coming from physics. So, of course, in physics, you have interesting groups. So, I think there are

38:37.880 --> 38:42.360
some Lorentz environments, for example, neural network architectures. You can also combine some

38:42.360 --> 38:46.280
groups. I will show some examples that you can have, for example, products of permutations or

38:46.280 --> 38:52.280
some special group products of permutations in subcraft neural networks. So, it's a general

38:52.280 --> 38:57.320
blueprint. So, if you can follow the blueprint and design architecture that implements this

38:57.400 --> 39:04.120
invariance, that is relevant for your problem, then you have a way of doing it. Okay. Cool. Thanks.

39:04.120 --> 39:08.040
There are also some works that try to discover the symmetry group from the data and from the

39:08.040 --> 39:15.400
problem. So, that's also an interesting direction. Could you shortly address the symmetries which

39:15.400 --> 39:20.680
the transformers go for, because I still have difficulty to think in this symmetry, like,

39:20.680 --> 39:25.400
perspective? So, I will talk. Well, it will take me probably about 20 minutes, but we'll get to

39:25.480 --> 39:29.320
transformers. Yeah. So, transformers are special types of craft neural networks. You can think of

39:29.320 --> 39:37.080
them really quickly. So, on the slide, there's the LSTM. So, it's written that time warping is

39:38.040 --> 39:43.880
you could have architectures that are invariant to time warping, but if you look at speech recognition.

39:43.880 --> 39:50.440
So, maybe first of all, we humans also don't have that, right? So, if something is super

39:50.440 --> 39:56.040
slow down, we're probably not going to notice what it is. So, my question is, is there any

39:56.040 --> 40:01.160
other invariance or equivalence for time series data that you can think of? So, well, here what

40:01.160 --> 40:07.880
I mean by time warping, you can actually show that gating is a necessary mechanism to be able

40:07.880 --> 40:15.000
to accommodate time warping. So, basically, gating emerges as basically as the architecture for

40:15.000 --> 40:20.680
this kind of, for this kind of invariance. Okay. Got it. Thanks. Yeah, I think there is a question there.

40:26.760 --> 40:36.600
So, okay. I mean, so you said that you will talk about transformers more in, like, in the next few

40:36.600 --> 40:44.680
slides, but I wanted to ask about one of their emergent properties, like in relation, for example,

40:44.680 --> 40:54.120
to CNNs, namely that CNNs have, like, kind of encoded in them by definition translation invariance,

40:54.120 --> 41:00.200
whereas transformers don't. And I think it has been discovered that transformers actually kind

41:00.200 --> 41:06.760
of learned the translation invariance from the, thanks to the amount of data that they, that they

41:06.760 --> 41:16.680
consume. But, like, they weren't defined to do that. It's kind of, this translation invariance

41:16.680 --> 41:24.360
just emerged from the data. So, I was wondering whether, in your opinion, it is better to, like,

41:24.360 --> 41:34.360
use more generic architectures, which learn those inductive biases from the data, or whether we

41:34.360 --> 41:39.560
should encode the inductive biases in their architecture, or, and, you know, face the risk that

41:40.280 --> 41:47.160
a specific inductive bias may be also a limitation. Yeah. Well, I think the answer is already contained

41:47.160 --> 41:52.280
at least one of the answers in your question, right? So, the amount of data is obviously a limitation.

41:52.280 --> 41:58.840
So, it might be easy to collect data, like images, right, or maybe text. It might be much more difficult

41:58.840 --> 42:04.200
to collect data that comes from biological experiments, right? So, if the data is limited

42:04.200 --> 42:09.160
and the data is expensive, then probably you want to work hard to incorporate as many inductive

42:09.160 --> 42:15.720
biases as you can. In some other applications, actually, the inductive biases, or some symmetries

42:15.720 --> 42:19.160
or some invariances come from the problem that you're trying to model. I think it's

42:19.160 --> 42:24.040
true for many applications in what is called the AI for science, right? So, if you have some, I don't

42:24.040 --> 42:27.800
know, physical system, you know that certain properties will be conserved. So, it makes no

42:27.800 --> 42:35.400
sense just to use a generic black box that will be producing unrealistic outputs, right,

42:35.400 --> 42:43.400
that are physically incorrect. But there is no, so it might be that in problems where you don't

42:43.400 --> 42:49.640
know a priori what symmetry or invariance you have, it might be a good idea to use transformers if

42:50.360 --> 42:52.920
the computational complexity and the scale of the data allows it.

42:52.920 --> 43:01.800
Hello, I have a question about augmentation. It feels like, at least in the image case,

43:01.800 --> 43:07.880
augmentation is basically exploiting this group symmetry, right? You augment by

43:07.880 --> 43:15.480
doing some translation. But if you use some geometric-based architecture, maybe then you

43:15.480 --> 43:20.920
don't need to do augmentation anymore. So, what's your opinion of augmentation? Is it like an artifact

43:21.720 --> 43:27.320
or something? Well, augmentation is a technique of, basically, when you have limited amount of

43:27.320 --> 43:33.560
data, you can generate synthetic data that looks like the kind of data that you want to see in

43:33.560 --> 43:38.360
your application. This was actually one of the important features of the AlexNet, for example,

43:38.360 --> 43:45.000
so they use certain type of data augmentation for images to make it more robust. So, again,

43:45.000 --> 43:51.000
you can incorporate this kind of inductive biases into the architecture. Sometimes,

43:51.000 --> 43:55.720
it might be difficult. So, another aspect that is often overlooked is actually the hardware,

43:56.520 --> 44:02.920
right? So, it's very easy and it's probably more a coincidence. At least originally,

44:03.880 --> 44:10.920
the convolutional neural networks map very nicely to the type of hardware that is used in

44:11.000 --> 44:18.760
the GPU, the single instruction multiple data type of architecture. So, it was not by design

44:18.760 --> 44:23.800
because the GPUs were designed for different types of problems. With other architectures,

44:23.800 --> 44:27.000
it might not be the case, right? So, with graphing a lot of programmable, this is not the case.

44:27.560 --> 44:32.760
So, there might be some better, so to say, architectures, maybe from mathematical standpoint,

44:32.760 --> 44:40.440
but they're just not as convenient to implement. Therefore, maybe you will prefer to use

44:40.440 --> 44:46.520
something that is less correct, but you can do data augmentation. The hardware allows you to

44:46.520 --> 44:56.040
implement this architecture better. What do you think is a follow-up question about

44:56.040 --> 45:00.680
augmentation? What do you think about augmentation? Not as a simple tool for increasing the size of

45:00.680 --> 45:06.280
the data set, but as something as in contrastive learning, as enforcing the symmetry. Can it be

45:06.280 --> 45:12.040
kind of equivalent to the symmetry you defined? Well, it was used in this formula. I mean,

45:13.800 --> 45:19.800
can the load of enforcing the symmetry be shifted towards augmentation to the model?

45:20.360 --> 45:24.680
Yeah, so basically, you're sampling points from the group orbit, right? So, you can think of it

45:24.680 --> 45:31.480
this way as well. Whether it's enforced in a hardware or in a software, that's probably not

45:31.480 --> 45:37.080
that important. So, data augmentation is a very valid technique, if you know exactly how to do it.

45:39.720 --> 45:46.680
Okay, so let's move on to graphs. And, well, graphs, as you probably know, their idea itself

45:46.680 --> 45:53.240
is pretty old. So, it's usually attributed to Euler who was thinking of these kind of problems,

45:53.240 --> 45:58.600
right? How you can connect land masses without actually accounting for the particular geometry,

45:58.600 --> 46:05.400
but only how, what is nearby, right? So, the famous problem about the bridges of KÃ¶nigsberg,

46:05.400 --> 46:11.000
and this is what he called the geometry of Cetus, or basically the geometry of place,

46:11.000 --> 46:17.320
what in modern terminology we call topology. So, the term was actually introduced by Pankareho,

46:17.320 --> 46:24.200
by analogy to Euler's terminology called analysis Cetus, and that was also where his famous conjecture

46:24.200 --> 46:29.640
appeared. So, we'll talk about Pankareho conjecture actually later. I hope to get there as well.

46:30.760 --> 46:36.040
So, graphs, obviously, I don't need to convince you that graphs are interesting and important. So,

46:36.040 --> 46:40.280
more or less anything from very small scales or very large scales can be modeled as a graph.

46:40.280 --> 46:44.600
So, any system of relations or interactions, whether it's a molecule, or you model how

46:44.600 --> 46:49.560
different atoms interact with each other through chemical bonds, to, for example,

46:49.640 --> 46:57.800
interactoms, right, in biological science, how different entities in our body or in a cell

46:57.800 --> 47:01.880
interact with each other, different, for example, chemical reactions, or even social networks,

47:01.880 --> 47:07.400
right, describing relations, friendship, interactions between different people.

47:08.120 --> 47:12.520
So, this model is a graph, and again, graphs can be of different types. So,

47:12.520 --> 47:16.920
let's consider a simple model here. So, we'll consider an undirected graph. So,

47:16.920 --> 47:25.720
it means that we have a collection of nodes, and we have unordered pairs of nodes as edges,

47:25.720 --> 47:33.400
right? So, just pairs, basically, the order doesn't matter, and the nodes are described

47:33.400 --> 47:37.400
by d-dimensional vectors. So, these are features that are attached to the nodes. And, of course,

47:37.400 --> 47:41.880
it can be more complicated. So, you can have graphs that are directed, you can have both

47:41.960 --> 47:49.240
continuous and categorical features, both in the nodes and the edges. But, just, that would be

47:49.240 --> 47:54.840
already interesting enough to look at this kind of object. So, one thing that characterizes graphs,

47:54.840 --> 47:59.080
right, basically, this is, again, a topological construction. So, it's an abstract object that

47:59.080 --> 48:04.680
lives on its own. The moment we need to represent it on a computer, we describe it, for example,

48:04.680 --> 48:08.440
as a matrix, right? So, we can describe the structure of the graph as the adjacency matrix,

48:08.440 --> 48:13.800
right, of size n by n, and it's a number of nodes. So, we have one, if there is an agent

48:14.680 --> 48:18.840
between a pair of nodes and zero, if there is no edge, right? And if the graph is undirected,

48:18.840 --> 48:25.400
then this matrix is symmetric. And the features, we can describe them as a matrix of size n by d,

48:25.400 --> 48:31.080
right? d is the dimensionality of the node features. So, one key thing here, which is already

48:31.080 --> 48:35.240
written on this slide, is that we don't really have a canonical way of ordering the nodes of the

48:35.240 --> 48:40.680
graph. So, when I make this description, I automatically assume some ordering of the nodes,

48:40.680 --> 48:45.720
but this ordering can be like this or can be like this. So, anything that will take

48:46.760 --> 48:53.080
this description of the graph as an input must account for this built-in ambiguity, right? So,

48:53.080 --> 49:01.320
I somehow need to be able to produce outputs that disregard, in a correct way, all these possible

49:01.320 --> 49:06.920
permutations, right? And the two types of problems that we can consider in relation to graphs,

49:07.560 --> 49:10.840
but actually more problems, but let's say these are the prototypical problems.

49:10.840 --> 49:13.880
One is graph-level problems. So, I give you an input graph and I try to

49:14.600 --> 49:19.160
output a number that describes this graph, right, or maybe a vector. Like, for example,

49:19.160 --> 49:23.480
I'm predicting the chemical properties of this molecule, like water solubility, so the input is

49:24.280 --> 49:29.160
a graph describing a molecule, the output will be some number, right? Another class of problems,

49:29.240 --> 49:34.120
I give you a graph and I want to do node-level decisions, right? For example,

49:34.120 --> 49:39.880
I want in a social network to classify which of the users is behaving badly, right? Maybe a

49:39.880 --> 49:45.320
spammer. So, in the first case, I give you a graph, right, and the output, no matter how I

49:46.280 --> 49:50.520
permute the inputs, should be the same, right? So, we're talking about a permutation invariant

49:50.520 --> 49:55.240
function. So, mathematically, it can be described like this, so, right? So, the function here now

49:55.240 --> 50:00.280
is a function of x, the node features, but also the structure of the graph. So, they're both

50:00.280 --> 50:07.240
from the input, and here I act on these two inputs with the permutation matrix, which is

50:07.240 --> 50:11.880
the representation of the permutation group. You see that actually the representation is different

50:11.880 --> 50:17.320
for different types of objects. So, the features, you can think of them as vectors, right? So,

50:17.320 --> 50:23.480
I permute only the order of the rows. The adjacency matrix, it's two-dimensional tensor,

50:23.480 --> 50:29.160
so, I act both on rows and columns, right? I need to permute both rows and columns. Here,

50:29.160 --> 50:35.800
this is clear. And together, this form permutation invariant function. In the second case, if I want

50:35.800 --> 50:41.240
node-level predictions, the output has the same structure as the input, right? So, if I change

50:41.240 --> 50:47.000
the order of the inputs, the order of the outputs is expected to change in the same way, right?

50:47.000 --> 50:51.240
And here, I'm interested in permutation equivariant functions. So, equivariance means changing in the

50:51.240 --> 50:56.520
same way. So, the output now will be, well, some kind of vector, right? Or a matrix, you know,

50:56.520 --> 51:02.440
by f capital. And if I permute the input, the output will be permuted in the same way, okay?

51:03.560 --> 51:07.880
Now, what are graph neural networks? Essentially, these are parametric graph functions. So, I provide

51:07.880 --> 51:13.640
you a graph as an input, right? Or these matrices x and a, and I output something, right? And

51:13.640 --> 51:18.840
something here, as we'll see in a few minutes, is parameterized by some vector of parameters.

51:19.800 --> 51:24.680
And sometimes, there is no distinction made between graph neural networks or message passing

51:24.680 --> 51:30.440
neural networks. So, we want to make this, that is distinction, but sometimes they're used synonymously,

51:30.440 --> 51:34.600
right? So, most of the graph neural networks that are used in practice are of the message

51:34.600 --> 51:40.120
passing type. We'll see exactly what it means in a second. And graph neural networks, again,

51:40.120 --> 51:45.400
you can consider them as a special instance of these geometric deep learning blueprints. So,

51:45.480 --> 51:51.560
we have usually a sequence of permutation-equivariant layers that produce node-wise predictions. And

51:51.560 --> 51:55.400
permutation-equivariant, in the sense that if I change the order here, it will change in the

51:55.400 --> 52:01.320
same way here. And then, if I have graph-level tasks, I will have permutation-invariant pooling,

52:01.320 --> 52:05.480
right? That aggregates all the information from the, from these node features and produces a

52:05.480 --> 52:11.560
single output for the graph. And the typical way that, that they work is by neighborhood aggregation.

52:11.560 --> 52:17.320
So, you can pick up a node in the graph, right? Let's call it i. And now, I look at the neighborhood

52:17.320 --> 52:23.320
of the node. By neighborhood, I mean all the nodes that are connected to i by an edge, right? So,

52:23.320 --> 52:29.080
this is how the neighborhood of i looks like. And I can look at the feature vectors associated with

52:29.080 --> 52:34.200
these, with these nodes. And you can see that even though the neighbors are unique, right? The

52:34.200 --> 52:38.600
feature vectors are not unique. So, this is encoded by color. So, these two nodes have exactly the

52:38.600 --> 52:43.720
same feature vector, right? Just for this example. So, together, they form what is called a multi-set,

52:43.720 --> 52:49.560
right? So, it's a set where the same object can appear more than once. And we also have the

52:49.560 --> 52:54.840
feature vector of the node itself. So, I want a function to aggregate them locally, right? Let's

52:54.840 --> 52:59.640
call this function phi. And again, the, the characteristic property of this function is that

53:00.520 --> 53:05.240
I don't have a canonical ordering of my neighborhood. So, the feature vectors can appear like this

53:06.120 --> 53:13.080
to, to this phi. So, it must be by design permutation invariant, right? It cannot assume any order

53:13.080 --> 53:17.960
in which the neighbors come. We can make it more complicated and we can incorporate additional

53:17.960 --> 53:22.680
information. But again, as a basic structure of a graph, you don't have this order, right?

53:24.280 --> 53:28.840
Now, you can repeat this process everywhere at every point, at every node of the graph. And this is

53:28.840 --> 53:33.880
actually very highly parallelizable, at least in principle. And once you do it, you get an output,

53:33.960 --> 53:39.720
right? That for every node of the graph, you output some vector. And this is also a function

53:39.720 --> 53:44.840
of the graph. And you can easily see that if my choice of this local aggregation is invariant,

53:45.560 --> 53:50.200
then the output is equivariant, right? So, if I change the order of the axis here,

53:50.840 --> 53:55.960
then the output will change in the same way. And most of the graph neural network architectures

53:55.960 --> 54:03.160
differ in the choice of these phi, in how I aggregate locally the features. And while they're

54:03.160 --> 54:07.640
probably zillions of different architectures, most of them fall within the following three

54:07.640 --> 54:12.040
categories. So, the first one is what is called convolutional graph neural networks. And you

54:12.040 --> 54:19.720
can simply think of convolutional neural networks as just summing up the features of the neighbor

54:19.720 --> 54:27.800
nodes. So, this is what I write here. So, the update for the representation for the feature at node i

54:28.760 --> 54:37.400
is the sum of some transformed neighbor nodes, right? Psi will be some learnable function. Xj

54:37.400 --> 54:44.360
is the the the feature of the neighbor node. Here it is. And ai, j are coefficients that depend

54:44.360 --> 54:49.000
only on the structure of the graph. In the simple case, just the elements of the adjacency matrix,

54:49.000 --> 54:53.720
right? And here, sigma is just some non-linear activation, typically very low, right? So,

54:53.800 --> 54:57.800
that's how a convolutional type graph neural network looks like. Yeah.

55:06.040 --> 55:12.280
Is this convolution here equivalent to 1D convolution, something like that?

55:12.280 --> 55:17.560
So, we'll see it, we'll see it in the, well, not in a second, but we'll see it later. So,

55:17.800 --> 55:25.880
basically, you can obtain convolution when the graph is agreed. And we can actually see that

55:25.880 --> 55:31.800
that convolution on the grid is a special type. Basically, it's a unique linear equivalent function.

55:32.760 --> 55:38.440
So, basically, yeah. So, that's why the term convolution is appropriate. So, it's an extension

55:38.440 --> 55:46.200
of conversion to graphs, right? And you can write it in this form, right? So, if I write it as

55:46.200 --> 55:52.120
matrix multiplication, so x is my feature matrix of size n by d, I multiply it from the left by

55:53.000 --> 55:56.600
some matrix a, right? The diffusion matrix. It can be the adjacency of the graph. It can be

55:56.600 --> 56:01.160
something else. But, basically, it propagates information between adjacent nodes. And on the

56:01.160 --> 56:08.120
right, I have, in the case of a linear transformation of the nodes, it's a learnable shared matrix

56:08.120 --> 56:12.760
that acts on every node in the same way, right? On the features of every node in the same way.

56:13.480 --> 56:18.760
We'll see that it's related to diffusion equations on graphs. And this is probably the simplest

56:18.760 --> 56:23.480
version of a GNN. It's highly scalable. You can basically just large matrix multiplication.

56:24.040 --> 56:29.080
It has been used in industrial use cases. I think the first to use these kind of architectures was

56:29.080 --> 56:35.080
Pinterest with Pinsage. We also used it at Twitter. And then there are some statements about these

56:35.080 --> 56:40.440
architectures in the kind of graph neural network folklore saying that it works only on homophilic

56:40.440 --> 56:44.680
graphs. So, by homophily, I mean this assumption that my neighbors are similar to me. Typically,

56:44.680 --> 56:50.600
the assumption is that the labels in the neighborhood are somehow similarly distributed to the label

56:50.600 --> 56:54.760
of the node itself. And here's an example of a homophilic graph versus a heterophilic graph.

56:55.400 --> 57:00.920
So, and the usual motivation that is given that these matrix a typically will look like a low

57:00.920 --> 57:05.880
pass filter, right? So, you're somehow averaging your neighbors. And if the neighbors are of the

57:05.960 --> 57:09.560
same type, then it will work. And if the neighbors are very different types, then

57:10.520 --> 57:16.360
it makes more harm than help. And this is actually not true. So, I hope to convince you that

57:17.000 --> 57:22.200
the story is much more nuanced. There is also this channel mixing matrix. And there is an

57:22.200 --> 57:27.320
interesting and subtle interplay between these two matrices. And we'll be able to see how it

57:27.320 --> 57:30.920
works when we consider graph neural networks as differential equations.

57:31.560 --> 57:36.920
So, slightly more interesting architecture is an attentional GNN. So, here, again, we can think of

57:36.920 --> 57:43.720
it, at least in some settings, as a linear combination of the features of the neighbors,

57:43.720 --> 57:48.200
maybe transformed by some learnable function. But now the coefficients depend not only on the

57:48.200 --> 57:51.880
structure of the graph, but also on the features themselves, typically through an attention

57:51.880 --> 57:57.160
mechanism. So, the most famous representative of these architectures, they got the graph

57:58.120 --> 58:03.800
attention network. And in the most general case, right? So, we can write got like this. So, that's

58:04.680 --> 58:09.560
if we have a linear combination, some, so we have linear combination with the matrix. But now

58:09.560 --> 58:15.880
the matrix is actually a matrix valued function of x, right? So, it itself depends nonlinearly on x.

58:17.480 --> 58:22.040
And the most general case is a message passing architectures where we have a b-variate function

58:22.040 --> 58:30.520
that depends on the feature of the node and the neighbor. And the other cases are specific

58:30.520 --> 58:39.720
cases of this architecture. And it was shown that message passing GNNs with specially chosen

58:39.720 --> 58:44.120
aggregation function, in particular, it has to be injective in certain restricted settings,

58:44.120 --> 58:50.040
again, allow me not to go into the details, are equivalent to graph isomorphism testing

58:50.040 --> 58:54.280
algorithm that was derived by vice versa and lemon that I mentioned in the beginning.

58:55.080 --> 59:01.960
And let's talk about this. So, basically, from theoretical standpoint, what actually

59:01.960 --> 59:05.880
graph neural networks do, right? Or message passing type graph neural networks do.

59:05.880 --> 59:11.800
So, first of all, what is graph isomorphism problem? It's telling when two graphs are the same,

59:11.800 --> 59:17.800
right? So, I'm writing here equal, but of course, it's not equal. So, it's equivalent in some sense.

59:17.800 --> 59:23.480
And what do I mean by this equivalence? What I want to say is that there exists an edge preserving

59:23.480 --> 59:28.040
bijection between the nodes of these graphs, right? So, in other words, I can find a correspondence

59:28.040 --> 59:34.120
between nodes in G and G prime, such that if there is an edge between a pair of nodes in G,

59:34.120 --> 59:40.280
then there is a corresponding edge in G prime, right? So, is this bijection unique?

59:41.240 --> 59:48.520
What do you think? No? When is it not unique?

59:51.800 --> 59:55.160
Exactly. When the graph has symmetries, right? So, this is an example. Actually,

59:55.160 --> 59:59.000
this graph is a good example, right? So, you can reflect the nodes on the left and the right,

59:59.000 --> 01:00:03.800
and then we can have another bijection, right? Like this. So, this bijection is not unique.

01:00:03.800 --> 01:00:09.960
But for determining if two graphs are isomorphic, it's sufficient to say that

01:00:09.960 --> 01:00:14.840
there exists such a bijection, right? So, that's what tells us that the two graphs are equivalent,

01:00:14.840 --> 01:00:19.720
right? So, basically, they are the same up to the ordering of the nodes, right? So,

01:00:19.720 --> 01:00:24.280
if I look at their adjacency matrix, they will be the same up to applying some permutation,

01:00:24.280 --> 01:00:28.920
right? So, I can basically, I can relate the two adjacencies by permutation.

01:00:30.280 --> 01:00:35.160
And related to the question of universal approximation, right? Which is fundamental for

01:00:35.160 --> 01:00:41.240
traditional neural networks like perceptrons, we can show that a class of functions is universal

01:00:41.240 --> 01:00:46.200
approximating permutation in variant functions on graphs with, importantly, here, the limitation

01:00:46.200 --> 01:00:52.520
finite node features, if and only if it can discriminate graph isomorphisms, right? So,

01:00:52.520 --> 01:00:57.160
basically, universal approximation is equivalent to graph isomorphism testing, right? So, basically,

01:00:57.160 --> 01:01:04.120
the two things go hand in hand. And if you ask what kind of graphs can we represent with

01:01:04.120 --> 01:01:08.360
message passing neural networks, right? So, here is, let's say, the space of all graphs.

01:01:09.080 --> 01:01:13.480
And these would be graphs that are structurally equivalent, right? That is isomorphic. So,

01:01:13.480 --> 01:01:18.360
by construction, we know that graph neural network cannot distinguish between these graphs,

01:01:18.360 --> 01:01:22.760
right? They're exactly the same up to the ordering of the nodes. So, just by construction,

01:01:22.760 --> 01:01:29.720
it will produce the same output for any isomorphic graphs. But the question of the opposite

01:01:29.720 --> 01:01:35.320
direction is more interesting. And this is not necessarily guaranteed, right? So, I might have

01:01:35.320 --> 01:01:40.120
different graphs that are not isomorphic, like the reds and the blues, that by chance will have

01:01:40.120 --> 01:01:45.400
the same representation. So, the graph neural network will output the same output for these

01:01:45.400 --> 01:01:50.360
different graphs. So, in other words, if we have the space of all permutation in variant functions,

01:01:50.360 --> 01:01:55.320
right, and we know that these are all graph isomorphism discriminating functions, this is where

01:01:56.040 --> 01:01:59.880
we'll see a subclass of functions that can be computed by message passing.

01:02:02.840 --> 01:02:09.320
Okay. And so, the question of graph isomorphism, as I mentioned already, it came from applications

01:02:09.320 --> 01:02:14.520
in organic chemistry, where people try to compare structures and try to determine whether

01:02:15.240 --> 01:02:20.520
two molecules are the same, right? So, in the case of isomorphism is a special setting, right?

01:02:20.520 --> 01:02:26.760
We can also think of distances between graphs. And vice versa in 68 came up with an algorithm

01:02:26.760 --> 01:02:31.880
that they believe to be a polynomial time method for determining whether two graphs are isomorphic.

01:02:32.840 --> 01:02:38.520
So, I should say that at that time in the 60s, even the notion of complexity was not totally

01:02:39.880 --> 01:02:48.600
spelled out. And also, the understanding of what's the complexity of graph isomorphism testing

01:02:49.160 --> 01:02:53.320
as a computer science problem was not understood. Actually, it's not understood even now. So,

01:02:53.320 --> 01:02:58.840
we know that it's not NP-hard and we also don't know polynomial time algorithms for it. So,

01:02:58.840 --> 01:03:04.600
it's a special complexity class that is called GI class. But anyway, so, it was actually disproved

01:03:04.600 --> 01:03:12.040
by a counter example. So, it was an example, it was shown that the class of graphs that cannot be

01:03:13.240 --> 01:03:18.280
tested by the device for an algorithm. We'll see such examples in a second. But the way that

01:03:18.840 --> 01:03:24.040
it goes, it's essentially a color refinement procedure. So, it considers a graph without any

01:03:24.040 --> 01:03:29.480
features, considers only its structure. And, initially, the graph has every node labeled in

01:03:29.480 --> 01:03:35.080
the same way. By label, I just mean a natural number that is attached to a node, right? And what

01:03:35.080 --> 01:03:41.640
the algorithm does, it takes a node and looks at its neighborhood, right? And you can see that

01:03:41.640 --> 01:03:45.400
originally in this graph, we have two types of neighborhoods. So, we have a blue node with

01:03:45.480 --> 01:03:50.840
two blue neighbors like this. Sorry, that's blue node with three blue neighbors and blue node with

01:03:51.400 --> 01:03:55.960
two blue neighbors, right? So, these are two neighborhoods that we see in this graph. So,

01:03:55.960 --> 01:04:01.880
if I now apply an injective function that they call phi, right? Think of it as hashing. I will

01:04:01.880 --> 01:04:07.160
have two distinct outputs, right? So, I will have nodes of the yellow type, let's call it, and of

01:04:07.160 --> 01:04:11.240
green type, right? So, I will be able to distinguish between these different neighborhoods. So, now,

01:04:11.240 --> 01:04:15.640
I have a graph with refined labels. I can apply the same procedure again, and now we have three

01:04:15.640 --> 01:04:20.200
types of neighborhoods, right? We have green with one green and one yellow neighbor. We have

01:04:20.840 --> 01:04:24.840
green with two yellow neighbors, and we have yellow with two green and one yellow neighbor,

01:04:24.840 --> 01:04:29.720
right? And these become, again, distinct colors. So, this will be, let's call it violet, gray,

01:04:29.720 --> 01:04:35.240
and orange. But if I repeat this procedure again, the colors will stop changing at which

01:04:35.240 --> 01:04:40.600
point the algorithm stops and produces a histogram of colors, right? So, that's a graph level

01:04:40.600 --> 01:04:46.520
descriptor. You can think of it this way. And what they show in the paper, well, the paper is

01:04:46.520 --> 01:04:51.320
actually complicated to read, but that's, let's say, a reduction of it. It actually describes

01:04:51.320 --> 01:04:58.680
a different type of algorithm, what is called 2WL, that does edge color refinement, but it

01:04:58.680 --> 01:05:04.040
doesn't matter. It's equivalent to what I'm showing here. So, if I give you another graph,

01:05:04.040 --> 01:05:09.720
and the distribution of colors is different, then I can guarantee that they are not isomorphic. But

01:05:09.720 --> 01:05:16.520
if the distribution of colors is the same, like in this case, we actually don't know. So, it's

01:05:16.520 --> 01:05:21.560
unnecessary, but insufficient condition, and in fact, you can find examples of non-isomorphic

01:05:21.560 --> 01:05:26.440
graphs that would be deemed equivalent by WL test, right? Or in this case, WL test cannot

01:05:26.440 --> 01:05:31.640
determine whether they're not isomorphic. And you can also see why the reason for it, right?

01:05:31.640 --> 01:05:40.360
Basically, what it does, it refines the colors of the nodes, so every node looks at its neighborhood.

01:05:40.360 --> 01:05:44.440
And this is how the neighborhoods of nodes look like, right? So, this node has two neighbors,

01:05:44.440 --> 01:05:48.520
then this node has, again, two neighbors, and so on and so forth, right? So, if you look at the

01:05:48.520 --> 01:05:53.560
structure of these neighbors, they will be exactly the same in both cases, right? And actually,

01:05:53.560 --> 01:05:58.200
very simple examples of graphs, for example, regular graphs where the degree of every node

01:05:58.200 --> 01:06:06.360
is the same cannot be tested by this simple procedure of Weisfeld and Lehmann. You can also not count

01:06:10.520 --> 01:06:15.960
connected patterns of more than three nodes, like triangles or cycles. And this is, I think,

01:06:15.960 --> 01:06:20.120
astonishingly disappointing given that the algorithm came from applications in chemistry,

01:06:20.120 --> 01:06:26.040
so in chemistry, these would be two different molecules, right? And this has a six ring and

01:06:26.040 --> 01:06:32.680
this has a five ring, right? So, or five cycle using graph theory terminology. So, we cannot

01:06:32.680 --> 01:06:38.200
distinguish these molecules by device for a Lehmann test. They would appear potentially the same,

01:06:38.200 --> 01:06:46.680
right? So, we wouldn't know. So, basically, the functions that can be computed by WL are strictly

01:06:46.680 --> 01:06:51.640
smaller than all permutation invariant functions, right? And we know examples of functions that

01:06:51.640 --> 01:06:55.560
cannot be computed by WL. For example, we cannot count the number of rings, right? So,

01:06:55.560 --> 01:06:59.560
if I want to implement a function that counts the number of rings in a graph, I cannot do it by

01:06:59.560 --> 01:07:06.360
means of WL test or by means of message passing. Now, the relation between WL and message passing

01:07:06.360 --> 01:07:11.720
is not random, right? You can see this, even the structure of the algorithm is exactly the same,

01:07:11.720 --> 01:07:17.720
right? So, this is what WL test does, right? So, it updates the color of every node by looking at

01:07:17.800 --> 01:07:24.040
the structure of the node and the multi-set of neighbors, right? Here, x denotes the colors.

01:07:24.040 --> 01:07:28.760
And this is what MPNN does, right? So, here, the squared denotes some general

01:07:28.760 --> 01:07:33.000
permutation invariant aggregator. It can be sum, it can be max, it can be mean, it can be anything,

01:07:33.000 --> 01:07:38.040
right? Importantly, it's permutation invariant. So, we can see that it's a special case. So,

01:07:38.040 --> 01:07:44.360
MPNN expressive power is upper bounded by device for a Lehmann test. And the question is when

01:07:44.360 --> 01:07:50.520
MPNN is as expressive as WL test, right? So, basically, we're interested in this case, right,

01:07:50.520 --> 01:07:55.560
when the two circles coincide. And if you look at different types of aggregators, right? So,

01:07:55.560 --> 01:08:01.880
imagine that this is your input graph. So, I have this gray node that has two types of

01:08:01.880 --> 01:08:06.600
neighbors. We have green neighbors and we have blue neighbors, right? So, if I consider the input

01:08:06.600 --> 01:08:11.560
as a multi-set, right? I completely disregard the structure of the graph itself, right? So,

01:08:11.640 --> 01:08:20.280
what the node sees is just a soup of the neighbor features. So, if I use a maximum aggregator,

01:08:20.280 --> 01:08:24.520
I cannot distinguish between these and these, right? Because for the maximum, it doesn't matter

01:08:24.520 --> 01:08:30.040
how many times each of these features appears, right? If I use a mean, then I cannot distinguish

01:08:30.040 --> 01:08:36.520
between these and these, right? I can multiply the neighbors by some constant factor. The sum,

01:08:36.520 --> 01:08:42.680
though, allows to distinguish between all of them, right? So, you can think that maximum gives

01:08:42.680 --> 01:08:49.400
a kind of skeleton of the set and the mean gives you the distribution, but the sum is strictly more

01:08:49.400 --> 01:08:58.040
expressive, right? And here's an example of structures that max or max and mean would fail

01:08:58.040 --> 01:09:03.400
to distinguish. And indeed, sum appears the most expressive one. And if you assume that,

01:09:03.960 --> 01:09:09.800
so the theorem about the equivalence between WL and message passing states that if you assume

01:09:09.800 --> 01:09:16.040
that the node features come from a countable universe, then if you have an MPNN with an

01:09:16.040 --> 01:09:21.800
injective aggregator, call it square, an update function phi and graph-wise readout function

01:09:22.600 --> 01:09:27.720
is as powerful as the vice-versa element set, right? And the assumption here is of discrete

01:09:27.720 --> 01:09:34.120
countable features, which is not always the case in practice. And then the reason architecture

01:09:34.120 --> 01:09:38.600
that actually implements that is equivalent theoretically to the vice-versa element, which is

01:09:38.600 --> 01:09:45.240
the graph-wise morphism network or GIN. So, basically, it uses a sum aggregation. So,

01:09:45.240 --> 01:09:50.280
the epsilon here is just theoretical thing. So, there exists infinitely many constants epsilon

01:09:50.280 --> 01:09:57.640
that you can use here. So, we know that, basically, there exists at least a choice, right? Within the

01:09:58.520 --> 01:10:03.400
all possible message-passing neural networks that makes it as expressive as WL. But, of course,

01:10:03.400 --> 01:10:08.920
we are interested in more expressive architectures, right? Can you do better than WL? And here, again,

01:10:08.920 --> 01:10:13.560
there is an entire universe of different architectures. So, some of them actually go beyond

01:10:14.200 --> 01:10:18.440
message-passing, at least in the traditional sense. And, roughly, you can distinguish between

01:10:18.440 --> 01:10:23.560
four different categories of approaches. So, it's either a higher-order WL test. It's not

01:10:23.560 --> 01:10:27.960
a single test. It's a hierarchy of tests. The use of positional instruction and coding,

01:10:27.960 --> 01:10:33.880
so that's how transformers work. Subgraph GNNs and then topological message-passing, right?

01:10:34.600 --> 01:10:39.080
So, let's talk briefly about all of them and then when do we need to do the break?

01:10:40.600 --> 01:10:41.000
Sorry?

01:10:43.880 --> 01:10:50.600
So, let's maybe 20 minutes and then we do the break. So, the first class of higher-order WL

01:10:50.600 --> 01:10:55.080
tests, as I mentioned, so WL test is just one algorithm that was initially developed

01:10:55.880 --> 01:11:03.800
and then extended by Babi and collaborators, actually independently also by other people.

01:11:04.600 --> 01:11:08.520
So, one of them was Eric Lander, who is mostly known as computational biologist,

01:11:08.520 --> 01:11:14.440
but he started as a mathematician. And, basically, this is an increasingly

01:11:14.440 --> 01:11:18.680
more expressive hierarchy of tests. Instead of doing node refinement, they look at tuples

01:11:19.240 --> 01:11:24.520
of nodes. So, it's obviously computationally more expensive and there is always, you can find a

01:11:24.520 --> 01:11:29.880
family of graphs that these algorithms cannot distinguish. So, like strongly regular graphs for

01:11:30.600 --> 01:11:38.760
2WL or 3WL tests and what is called CFI graphs for general KWL. I should also say that this

01:11:38.760 --> 01:11:44.120
terminology of KWL is confusing because they're what is called folklore WL tests versus the

01:11:44.120 --> 01:11:49.960
classical WL tests that are slightly different. So, but overall the hierarchy, right, up to

01:11:49.960 --> 01:11:57.800
notation is the same. So, we know that message-passing GNNs are equivalent to the standard WL. You can

01:11:57.800 --> 01:12:06.280
also design just replicating in the neural network architecture the KWL tests higher-order KGNN.

01:12:07.000 --> 01:12:12.280
So, this is what Hageim Aron did in his works. And then you can also have some other algorithms

01:12:12.280 --> 01:12:16.440
we'll talk about in a second that sits somewhere between. They don't exactly follow the

01:12:17.400 --> 01:12:22.840
hierarchy of the WL tests. So, the second approach is positional encoding. And, again,

01:12:22.840 --> 01:12:27.960
I remind you of this example of two graphs that cannot be distinguished by the WL tests, but

01:12:27.960 --> 01:12:34.840
imagine that I could now attach some features to the nodes of the graph, right? And this could be

01:12:34.840 --> 01:12:41.400
even something as simple as random features. You see that now, because I have some extra information,

01:12:41.400 --> 01:12:47.000
I can distinguish between these cases, right? So, if I look at the leaves, for example, of

01:12:49.160 --> 01:12:55.080
this tree, right, I can ask, for example, whether the root appears among the leaves or not, right?

01:12:55.080 --> 01:13:00.680
And here it does appear and here it doesn't, right? So, they're clearly different, right? I would be

01:13:00.680 --> 01:13:05.720
able to distinguish between them, right? So, the covering of the nodes removes at least to some

01:13:05.720 --> 01:13:10.920
extent the ambiguity. Now, of course, if I use random features, then the question is how can I

01:13:10.920 --> 01:13:17.160
reproduce them on a different graph? So, these type of approaches are equivalent only in expectation.

01:13:17.160 --> 01:13:21.560
But there are other methods that can do better. And these are structural encoding.

01:13:22.440 --> 01:13:28.600
So, the idea of structural encoding, you have some substructures, right? So, we have a bank of

01:13:28.600 --> 01:13:34.440
substructures that call it H. And you can count the substructures, the occurrence of substructure

01:13:34.440 --> 01:13:37.800
for every node or for every H, right? And there are two ways that you can

01:13:37.800 --> 01:13:42.200
consider subgraphs, whether what is called a subgraph or any induced subgraph. It doesn't

01:13:42.200 --> 01:13:47.080
really matter, right? So, the two ways that are slightly distinct. And for example, in these two

01:13:47.080 --> 01:13:53.240
graphs, if this is my bank of substructures, let's say cycles of size 6 and 5. So, in this molecule,

01:13:54.600 --> 01:14:02.120
at every edge or at every node, I will count once the six cycle substructure and these nodes,

01:14:02.120 --> 01:14:07.800
I will count twice, right? Because this node participates in both structure on the left and

01:14:07.800 --> 01:14:14.680
right. But the five cycle substructure doesn't appear here versus it appears here, right?

01:14:15.320 --> 01:14:19.640
So, with this encoding, now I have some, these additional features that I can attach to every

01:14:19.640 --> 01:14:23.880
node or to every edge of the graph. And I can use them in standard message passing. And this would

01:14:23.880 --> 01:14:31.160
allow me to discriminate between these graphs. And the complexity of this method is basically,

01:14:31.240 --> 01:14:36.200
it's all hidden in pre-computation, so the counting of substructures. So, in the worst case, it's

01:14:36.200 --> 01:14:41.000
order of n to the power k, k is the size of the substructure, so it could be large. But in practice,

01:14:41.000 --> 01:14:47.320
for structures, more friendly structures like triangles, there exist more efficient algorithms.

01:14:47.320 --> 01:14:53.160
So, in practice, it can be way better. The algorithm itself, and especially the training part,

01:14:53.160 --> 01:14:57.640
which is typically more expensive, is standard MPNN. So, it has linear complexity in the number of

01:14:57.640 --> 01:15:03.560
edges, or roughly order of n if the graph is sparse. And the theoretical result is that these

01:15:03.560 --> 01:15:08.280
kind of architecture that we call GSN, graph substructure network, is strictly more expressive

01:15:08.280 --> 01:15:14.120
than WL on certain assumptions on these substructures, right? So, it should not be a

01:15:14.120 --> 01:15:23.160
star graph or it should be a structure of size, bigger than three. And basically,

01:15:23.160 --> 01:15:28.200
we can formulate it is that GSN is not less expressive than 3WL. You can also do it for

01:15:28.200 --> 01:15:34.680
different KWLs. Basically, you do it by counter examples. So, you can design a substructure

01:15:34.680 --> 01:15:41.640
that the standard WL tests cannot count, whether it will be clicks of certain type or other things.

01:15:42.520 --> 01:15:47.080
Right? And the proof by example is something like this. So, this is a stronger regular graph. It

01:15:47.080 --> 01:15:52.920
cannot be distinguished by 3WL, but this graph contains four clicks and this doesn't.

01:15:53.400 --> 01:15:59.400
So, if I count four clicks, I would be able to distinguish between these graphs. So, in a sense,

01:15:59.400 --> 01:16:06.280
it's a kind of cheating. So, I'm not following really the KWL hierarchy. So, this is an example of

01:16:06.280 --> 01:16:11.880
different structures I can count, triangles and clicks. But then, basically, the expressive

01:16:11.880 --> 01:16:17.640
power looks like this. So, this is, for example, a graph substructure network with four-click count.

01:16:17.640 --> 01:16:23.320
So, it might actually, there might be examples of graphs that are distinguishable by 3WL, but

01:16:23.320 --> 01:16:29.880
not by this method. We have at least an example of a family that 3WL cannot detect. So, it's outside

01:16:29.880 --> 01:16:36.840
of the hierarchy. And why this is important, especially in applications related to chemistry,

01:16:37.480 --> 01:16:42.360
because often we know these substructures are priori, right? Organic molecules, for example,

01:16:42.360 --> 01:16:47.000
cycles are a very prominent feature. These are what is called aromatic rings, right? Like in the

01:16:47.000 --> 01:16:52.680
molecule of caffeine, we have two, right? So, we have this ring of cycle of size six and cycle of

01:16:52.680 --> 01:16:58.280
size five. And we see that if we incorporate this information as a kind of problem-specific

01:16:58.280 --> 01:17:04.200
inductive bias into the problem, we are able to much better predict the properties of molecules.

01:17:04.200 --> 01:17:09.640
In this case, it's, I think, the water solubility on the, on a toy data set of molecules that is

01:17:09.640 --> 01:17:16.200
called zinc. And by incorporating cycles, we significantly reduce the error.

01:17:18.280 --> 01:17:24.040
Third class of approaches, what is called subgraph GNNs. And here, the idea is also very simple.

01:17:26.360 --> 01:17:30.120
So, if you look at these two graphs, again, this is probably one of the simplest examples of

01:17:30.120 --> 01:17:34.760
non-lasomorphic graphs that cannot be tested by WL, right? If you do the color refinement,

01:17:35.560 --> 01:17:40.280
this is what WL produces, so the histograms are the same, right? That's exactly the case

01:17:40.280 --> 01:17:46.680
where you cannot say anything about the graphs. But imagine that I can perturb the graph, for

01:17:46.680 --> 01:17:53.240
example, by removing this edge, right? So, if I did it, the colors will be very different, right?

01:17:53.240 --> 01:17:58.360
So, these will be the distributions of the, of the colors, and they are clearly distinct. So, in

01:17:58.360 --> 01:18:03.960
this case, the perturbation allows to distinguish between structures that are otherwise indistinguishable.

01:18:03.960 --> 01:18:08.760
So, the question is, of course, do I know which edge to remove, right? So, here maybe I was lucky,

01:18:08.760 --> 01:18:13.640
and the answer is usually I don't. So, let's remove all possible edges, right? So, let's just

01:18:13.640 --> 01:18:19.080
make this perturbation when I remove one edge at a time, right? So, and there are seven possibilities.

01:18:19.080 --> 01:18:25.240
I can also do node deletions in the same way, right? And now, instead of a graph, I have a collection

01:18:25.960 --> 01:18:30.440
of subgraphs that are extracted by some policy. So, in this case, it's a very simple policy,

01:18:30.440 --> 01:18:36.120
one node removal, right? And actually, it results in graph theory that say that if I give you this

01:18:36.120 --> 01:18:41.640
collection, so, in terminology of graph theory, this is called a reconstruction. So, if they have

01:18:41.640 --> 01:18:47.800
the same multi-set of node remove subgraphs, right? So, this is what we denote H tilde G, right?

01:18:48.520 --> 01:18:53.000
So, graphs where, basically, if we look at these kind of multi-sets, they will be the same, of

01:18:53.000 --> 01:18:58.840
course, up to, up to reorting. So, the statement in graph theory that is called reconstruction

01:18:58.840 --> 01:19:05.400
conjecture claims that under some technical assumptions, if H is a reconstruction of G,

01:19:05.400 --> 01:19:11.960
then H is equivalent to G, isomorphic to G, right? So, why it is called a conjecture? Because

01:19:11.960 --> 01:19:15.880
it is proved only for small graphs, and it's an open question in general. And there are

01:19:15.880 --> 01:19:21.160
generalizations for subgraphs where you remove multiple nodes. So, this is, again, not a single

01:19:21.160 --> 01:19:28.120
result. So, it's a class of results. It was introduced by Paul Kelly in his PhD thesis

01:19:28.120 --> 01:19:36.600
that was done under the supervision of Stanislaw Ulam, who was a mathematician, a Polish mathematician,

01:19:36.600 --> 01:19:42.760
but he's probably more famous for initiating the Manhattan Project and developing thermonuclear

01:19:42.760 --> 01:19:49.080
weapons. So, but he also was, he's also famous for many interesting results in mathematics,

01:19:49.080 --> 01:19:54.840
and this is one of them, the reconstruction conjectures. So, we don't know whether this is

01:19:54.840 --> 01:20:00.120
true. So, it might be true, but that's why it is a conjecture. It would be cool if it were true,

01:20:00.120 --> 01:20:06.440
because, of course, in this case, I could test graphosomorphism by just doing something with

01:20:06.440 --> 01:20:10.840
this collection. So, what exactly can we do with this collection, regardless whether the

01:20:10.840 --> 01:20:14.920
conjecture is true, right? So, it will just give us stronger theoretical property of these

01:20:14.920 --> 01:20:20.200
architectures. So, what we can do is we can consider our graph is a collection of subgraphs

01:20:20.200 --> 01:20:24.840
that are extracted from the given graph, right? And what is important to understand is that there

01:20:24.840 --> 01:20:28.920
is a correspondence between them, right? Because we created these subgraphs, right? So, it's built

01:20:28.920 --> 01:20:34.680
on the same nodes. We just might remove some edges, right? Or some nodes. So, we have here two types of

01:20:35.320 --> 01:20:40.600
symmetries. So, we have the permutation of the nodes in the graph itself, right? And we also have a

01:20:40.600 --> 01:20:45.560
permutation of the subgraphs in this multi-set, right? Because we don't have a canonical order of

01:20:45.560 --> 01:20:51.400
them. So, together, basically, the structure, the symmetry structure of this new object of

01:20:51.400 --> 01:20:56.680
this collection of subgraphs is a product of two groups, right? If we don't know the correspondence,

01:20:56.680 --> 01:21:01.240
there will be a special type of product that allow me to skip the details. And basically,

01:21:01.240 --> 01:21:05.320
what we can do, we can design an architecture that does message passing on each of these subgraphs

01:21:05.320 --> 01:21:10.040
separately, but then fuses the information across graphs using these known correspondence.

01:21:10.040 --> 01:21:16.680
And this is probably more powerful than WL, a version of this architecture that we call

01:21:16.680 --> 01:21:22.520
subgraph union network. We can actually show that it's upper bounded by 3WL, and we hope that

01:21:22.520 --> 01:21:28.120
it will be equivalent to 3WL. But a few weeks after we published our paper, it was shown by a

01:21:28.120 --> 01:21:33.960
counter example that it is strictly less powerful than 3WL. So, we don't know exactly. It's surely

01:21:33.960 --> 01:21:40.760
more powerful than 2WL, but upper bounded by 3WL, and strictly less powerful. So, we have even a

01:21:40.760 --> 01:21:48.840
blog post about these different architectures, and there are multiple methods that are related.

01:21:48.840 --> 01:21:55.080
So, one of them is, for example, you can do dropout on your neighbors. That was done by a

01:21:55.080 --> 01:22:01.240
worker from the group of Roger Battenhofer at ETH, and they actually showed that this has

01:22:01.240 --> 01:22:06.840
similar effect. So, it increases the expressive power, not only gives some kind of robustness to

01:22:06.840 --> 01:22:14.360
the architecture. So, the last more expressive type of message passing in your network, so I

01:22:14.360 --> 01:22:19.160
would like to mention is what we can generally call topological message passing. And if you think of

01:22:19.960 --> 01:22:25.720
what is a graph, essentially, it's a set where you glue pairs of nodes together, right? So, every

01:22:26.600 --> 01:22:30.680
element in a set writer, every node in the graph is a zero-dimensional topological object,

01:22:30.760 --> 01:22:37.240
right? So, you can define this one-dimensional object, the edges that you glue to the nodes,

01:22:37.240 --> 01:22:40.600
right? And you get the graph, but you don't need to stop here. You can also

01:22:41.480 --> 01:22:47.720
define cells of higher dimension, right? That you forgot about, glue to cycles in your graph,

01:22:47.720 --> 01:22:54.360
right? And we get what is called a cellar or CWL complex, right? And basically, now, instead of

01:22:54.360 --> 01:22:59.000
traditional message passing in graph neural networks, where we exchange information between

01:22:59.000 --> 01:23:05.880
nodes along the edges, we can also go up and down in this hierarchy. So, we can do message passing

01:23:05.880 --> 01:23:11.400
within the same dimension, right, of the cellar complex, but we can also go across dimensions.

01:23:12.360 --> 01:23:18.120
And this hierarchical message passing is strictly more powerful than the vice-versa-lemon,

01:23:18.120 --> 01:23:23.640
and it's obviously very convenient for molecules, because in molecules, these structures have

01:23:24.440 --> 01:23:30.040
some chemical meaning, and this probably is closer to how chemist thinks of molecule, because,

01:23:31.480 --> 01:23:36.200
of course, the graph captures all the information, but it doesn't make certain structures explicit.

01:23:36.840 --> 01:23:40.280
And in graph neural networks, for example, well, first of all, you cannot even detect by message

01:23:40.280 --> 01:23:44.840
passing the presence of these structures. And if you want to transfer information from this node

01:23:44.840 --> 01:23:48.440
to this node, you will need to do a few steps of message passing. Here, we can do it at once.

01:23:49.320 --> 01:23:55.560
So, it also gives computational advantages. And again, if you want some more details about

01:23:56.360 --> 01:23:59.320
how these different methods are related to each other, so there are many more

01:24:00.440 --> 01:24:05.800
expressive architectures, so there is a tutorial that was given at the log conference,

01:24:06.600 --> 01:24:13.640
and recorded on YouTube by my PhD student, Publizio Frasca, with Beatrizio Bevilacqua

01:24:13.640 --> 01:24:19.000
and Gagai Maron. So, it's actually a very nice tutorial, and they go into much more details about

01:24:19.640 --> 01:24:25.240
all these and other different methods for expressive graph neural networks. Any questions so far?

01:24:36.360 --> 01:24:42.360
I have a question about summation aggregation being the maximally expressive aggregation.

01:24:43.960 --> 01:24:49.720
Also, probably it is related. Maybe if you can also comment on the, what was it, discrete,

01:24:49.720 --> 01:24:56.760
countable restriction on the features. Because if I imagine that our features are integers,

01:24:57.960 --> 01:25:03.480
there are different combinations of integers that sum up to the same number. So, summing them

01:25:03.480 --> 01:25:11.240
actually does lose the information. So, countable doesn't necessarily mean integers,

01:25:11.560 --> 01:25:17.400
but they should not be continuous. So, why this is, without going into too much details,

01:25:17.400 --> 01:25:23.720
basically they use the same proof technique that was used in deep nets to prove the

01:25:23.720 --> 01:25:28.600
universality there. So, this assumption is important. If you remove this assumption that

01:25:30.280 --> 01:25:37.880
this proof doesn't work. So, basically they apply locally kind of the result of deep net

01:25:38.520 --> 01:25:43.160
that works on sets. All right. Thanks.

01:25:46.520 --> 01:25:53.640
Hello. Very interesting. Thank you. I'm wondering about chemistry, whether you can encode in the

01:25:53.640 --> 01:25:59.720
features of your graphs, also geometric information. Especially in chemistry, aromaticity is very

01:25:59.720 --> 01:26:05.240
important. Whether it's possible to encode it directly or you need some additional layers of

01:26:05.240 --> 01:26:10.120
information. So, some information you can probably compute. And of course, if you can

01:26:10.120 --> 01:26:13.320
pre-comput it, if you know that these are meaningful features, then I think it makes

01:26:13.320 --> 01:26:18.040
sense to encode them. So, the geometric information, I'm not sure that you mean

01:26:19.720 --> 01:26:23.960
information that comes from the positions of the atoms, right? Yeah. So, I will talk about it

01:26:23.960 --> 01:26:27.720
in a second. So, you can, basically when you deal with geometric information, you also need to do

01:26:27.720 --> 01:26:33.320
it in a proper way. So, you need to do it in a way that is equivariant to possible transformations.

01:26:33.320 --> 01:26:40.840
But I think the short answer is yes. In case of these, like, increasing hierarchy of the

01:26:40.840 --> 01:26:49.240
KWL tests, is it a case that for a given KWL that we know there exists some message passing

01:26:49.240 --> 01:26:54.440
graph neural network that exists that can do it, but we just can't construct them? Or can we say

01:26:54.440 --> 01:27:00.040
that if we could make such a message passing GNN for such KWL, it would be, you know, intractably

01:27:00.040 --> 01:27:04.440
huge and we would need to approximate it or something like that? So, first of all, it is

01:27:04.440 --> 01:27:11.720
intractably huge. So, it complexes N to the power K. So, I think what is limited in practice is

01:27:11.720 --> 01:27:20.040
3WL. So, this is what Maron describes in his paper. I don't know, depending whether you can call it

01:27:20.040 --> 01:27:24.200
message passing or not, it depends on what you consider message passing, right? So, because

01:27:24.280 --> 01:27:31.080
here you have more than pairs of nodes, I would argue that strictly speaking, it's not message

01:27:31.080 --> 01:27:35.240
passing, but for example, Petr Wieliszko, which would say that it's message passing on a different

01:27:35.240 --> 01:27:41.480
graph, right? So, it depends on the perspective. Maybe one more follow-up question for this KWL

01:27:41.480 --> 01:27:47.480
stuff. If you have a particular application that you are interested about, are there some cases

01:27:47.480 --> 01:27:52.040
where you can say, for this class of problems that we're working on, we can, it's enough to be

01:27:52.040 --> 01:27:56.840
able to distinguish up such a level because the higher you go for K, obviously, like these edge cases

01:27:56.840 --> 01:28:02.120
would get really nasty, which you might not see in your application. Well, so, it's a good question,

01:28:02.120 --> 01:28:08.600
right? So, for example, planar graphs have WL dimension of 3. So, basically, all planar

01:28:08.600 --> 01:28:16.600
graphs can be distinguished by 3WL test. And you can argue that molecules, right? Most of the

01:28:16.600 --> 01:28:21.160
molecules, you can draw two-dimensional structures, maybe some of them you don't, but they're

01:28:21.160 --> 01:28:28.600
probably a tiny fraction. So, do you need something more powerful than that? The expressive power

01:28:28.600 --> 01:28:33.640
itself is probably not the end of the story, right? Because nothing tells you about generalization.

01:28:35.320 --> 01:28:43.640
So, yeah, I don't think that this on its own is really the crucial consideration. It's good,

01:28:43.640 --> 01:28:49.320
of course, to have an architecture that is, that allows to distinguish between broader class of

01:28:49.320 --> 01:28:53.160
graphs, but if it comes at the expense of computational complexity, for example, maybe

01:28:53.160 --> 01:28:59.080
it's a bad idea. So, you probably want some kind of good trade-off between these. Thank you.

01:29:00.520 --> 01:29:07.720
I had a follow-up question. So, could you please define what message-passing is in this case?

01:29:08.760 --> 01:29:13.480
Can we give a short definition? Right. So, message-passing is what I call message-passing is

01:29:14.200 --> 01:29:19.880
these kind of architectures. Let's see where I had it. Yeah. So, basically, architecture of this

01:29:19.880 --> 01:29:25.080
kind. So, this is the most general type of message-passing. So, we have the update of node i

01:29:26.680 --> 01:29:31.800
from neighbor j is done in this way. So, I have a function that depends on both i and j,

01:29:32.520 --> 01:29:36.360
that the function is parametric. So, that's learnable and then aggregate. You can actually

01:29:36.360 --> 01:29:39.800
show that summation is what you need, right? You don't need anything else.

01:29:43.640 --> 01:29:50.520
So, well, this is, so this is message-passing. There are higher order architectures, right,

01:29:50.520 --> 01:29:58.360
like KGNN, right, equivalent to KWO. So, this is equivalent, in the best case, equivalent to WO.

01:29:59.160 --> 01:30:02.520
It is not equivalent. So, the more expressive WO tests, KWO tests

01:30:03.400 --> 01:30:07.320
are more expressive than these architecture, but then it doesn't work on pairs of nodes. It

01:30:07.320 --> 01:30:13.160
considers bigger sets of nodes, right? So, whether you, to call it message-passing or not, you can

01:30:13.240 --> 01:30:18.280
argue that, so some people argue that you can also think of it as message-passing, right,

01:30:18.280 --> 01:30:29.080
just with a different graph. In my opinion, it's more a semantic question.

01:30:29.640 --> 01:30:48.360
So, I don't know. We never really looked at it. Yeah, I don't have an answer.

01:30:52.040 --> 01:30:55.880
So, it seems like the topological message-passing

01:30:55.880 --> 01:31:00.600
networks and the graph substructure networks both work on the same phenomenon of identifying

01:31:00.600 --> 01:31:06.440
and counting substructures. So, is the expressivity of the topological message-passing lower bounded

01:31:06.440 --> 01:31:10.600
by the expressivity of the graph substructure network? No, it's slightly different, right,

01:31:10.600 --> 01:31:15.560
because substructure networks, well, first of all, we know they're upper bound, so 3WO. Topological

01:31:15.560 --> 01:31:22.440
message-passing depends on what kind of substructure. So, there, the kind of side information

01:31:22.440 --> 01:31:28.440
that you assume is what kind of substructures become cells in this, in this cellular complex.

01:31:28.440 --> 01:31:31.800
You can actually go beyond two-dimensional cells. You can go to high-dimensional cells.

01:31:32.600 --> 01:31:38.520
We never tried to go beyond two-dimensional cells. So, depending on the choice of these

01:31:38.520 --> 01:31:43.400
substructures, what becomes a cell, you might have different levels of expressivity. So,

01:31:43.400 --> 01:31:46.040
they're distinct methods. I don't think that they're really comparable.

01:31:46.440 --> 01:31:54.680
So, something that always confused me about the topological ones is where you, like, you glue

01:31:54.680 --> 01:32:00.520
stuff to structures in the graphs to, like, make them obvious. But can you, like, glue to every

01:32:00.520 --> 01:32:05.080
structure or does the structure need to be, like, closed? Because in previous works, it was always

01:32:05.080 --> 01:32:10.440
either cycles or cliques, which are kind of circularly closed.

01:32:11.160 --> 01:32:16.840
What, I think, basically, the key question is whether it defines a valid cellular complex. I

01:32:16.840 --> 01:32:22.760
think it should be closed. Yeah, my top, yeah, I also think it should be closed, but I have no idea

01:32:22.760 --> 01:32:26.920
about topology. Yeah, from what I remember, it should be closed. But, yeah, so it could be a

01:32:26.920 --> 01:32:33.720
clique, for example, whether it could be a path. I don't think so. So, actually, GSNs can, like,

01:32:34.840 --> 01:32:40.040
count structures that you couldn't, like, glue into, or, like, form into a cellular complex.

01:32:40.040 --> 01:32:44.360
So, GSNs can count more general structures, in my opinion. Yeah, so something that doesn't necessarily

01:32:44.360 --> 01:32:52.520
form a cellular complex. Okay, yes, thank you. Inge, I have a question. If we allow more and more

01:32:52.520 --> 01:32:57.720
notes, the probability of collision in this representation, do we have any results that

01:32:57.720 --> 01:33:04.520
it became less or, like, insignificant or, like, in more general, do this approach extend a bit

01:33:04.520 --> 01:33:12.520
more into randomized or, like, stochastic versions so that maybe the guarantee is very low right now

01:33:12.520 --> 01:33:18.840
from a sort of, well, secondary run. So, the expressiveness, like, theoretical is pretty low,

01:33:18.840 --> 01:33:23.880
but then, if we allow a little randomness, then, actually, like, no randomness.

01:33:23.880 --> 01:33:27.320
Sorry, you're talking about random graph models, right? Something like stochastic walk models.

01:33:28.120 --> 01:33:34.200
Yeah. You can probably analyze what happens to this kind of graphs given

01:33:36.520 --> 01:33:42.600
certain type of message passing architecture. I have seen papers of this kind, nothing specific

01:33:42.600 --> 01:33:49.800
that comes to my mind. Yeah, you can probably, I don't know, compute some probability under the

01:33:49.800 --> 01:33:55.880
assumption of certain distribution of input random graphs of distinguishing within them or not.

01:33:56.840 --> 01:33:57.320
Yeah.

01:34:02.440 --> 01:34:08.280
So, a question regarding the size of the graph in practice. We are talking about, like, in terms of

01:34:08.280 --> 01:34:15.720
maybe not count and each count, like, what to say in practice for this KWL?

01:34:17.080 --> 01:34:21.880
So, KWL doesn't scale well, right? So, if it's n to the power k, so also computational

01:34:21.960 --> 01:34:27.240
complexity versus space complexity, I don't think that you can, in practice, go beyond

01:34:27.240 --> 01:34:33.320
3WL equivalent. Now, there are sparse versions of these architectures, so you can do slightly

01:34:33.320 --> 01:34:38.840
better, but I think they are mostly useful for proving theorems. So, basically, because you establish

01:34:39.480 --> 01:34:45.080
a link to the device for element hierarchy, then you can say that basically, if your network is

01:34:45.080 --> 01:34:50.680
equivalent to one of these methods, then you know how expressive it is. Thank you.

01:34:52.600 --> 01:34:58.600
Yeah. So, there was a paper sometime ago claiming that most of the graphs in the most common benchmarks

01:34:58.600 --> 01:35:07.320
can actually be distinguished by 1WL. Now, I don't know if you read about it. And my question is,

01:35:07.320 --> 01:35:13.560
now we've seen that even in those cases, more expressive GNNs still get better results. But

01:35:13.560 --> 01:35:19.160
then what's the reason? So, again, generalization could be one possibility, right, because you

01:35:19.160 --> 01:35:24.200
train on one set of graphs and then you test on another set of graphs. Then,

01:35:24.840 --> 01:35:28.440
double your tests, in general, they're designed for fish less graphs, right? So,

01:35:28.440 --> 01:35:32.360
they only consider the structure. So, how you treat fish is also important.

01:35:34.760 --> 01:35:46.920
Nice sense. I think we can stop here and we have, like,

01:35:46.920 --> 01:35:49.640
have an hour for a coffee break and then we resume afterwards.

01:35:49.640 --> 01:35:54.040
So, I will put the slide with the caffeine molecule that everybody likes. And then,

01:36:04.120 --> 01:36:11.640
yeah, basically, we stopped at this overview of different more expressive architectures. So,

01:36:11.640 --> 01:36:19.560
let's now talk about, guys, let's start. So, basically, the situation that we have with

01:36:21.560 --> 01:36:27.160
the expressive power of graph neural networks, right? So, on the one side, we have the WL hierarchy,

01:36:27.160 --> 01:36:34.200
right? So, increasingly, more powerful, more expressive graph isomorphism tests, right? And

01:36:34.200 --> 01:36:40.600
we can find analogies between certain GNN architectures to these tests. On the other hand,

01:36:41.480 --> 01:36:47.240
the assumption that we are given a graph and we do message passing on this graph might be

01:36:47.240 --> 01:36:53.080
restrictive in the sense that some graphs are not friendly for message passing. And in this case,

01:36:53.080 --> 01:36:57.800
you typically, what you would like to do is to change the graph so that message passing works

01:36:57.800 --> 01:37:03.560
better. This is a very broad category of methods that are called graph rewiring. So, what happens

01:37:03.560 --> 01:37:09.160
is that you have a gap between theory and practice, right? So, the theory, in order to make the link

01:37:09.160 --> 01:37:14.600
to the WL tests, you need to use exactly the same input graph. The practice tells you that

01:37:14.600 --> 01:37:23.080
sometimes you don't want to do it. So, as always, there is this gap. So, if you think maybe take

01:37:23.080 --> 01:37:30.600
a step back and look at the different types of graph neural network architectures, so the traditional

01:37:30.600 --> 01:37:37.640
approach in graph neural networks is you're given the input graph and it's both part of the input

01:37:37.640 --> 01:37:42.120
and part of the computation, right? Because you use the input graph to send information on it,

01:37:42.120 --> 01:37:48.680
right? So, it's both input and computational object. Now, as we've seen, you can do many

01:37:48.680 --> 01:37:53.240
different things, right? So, you can enrich the graph with some positional or structural features.

01:37:53.960 --> 01:38:00.040
You can lift it into a high-dimensional topological space like Simplisher or Cellar complex, right?

01:38:00.040 --> 01:38:05.640
And do message passing on this object. You can again enrich the graph by considering a collection

01:38:05.640 --> 01:38:12.200
of subgraphs, right? And do maybe some other more exotic type of aggregation that respects the

01:38:12.200 --> 01:38:20.600
product symmetry group. You can also enrich your representation by considering also the

01:38:20.600 --> 01:38:25.720
symmetry of the data, right? So, these are equivalent GNNs, if you have time, I can talk about it.

01:38:25.720 --> 01:38:30.040
And then maybe in some special cases, your graph will have special structure like a grid, right?

01:38:30.040 --> 01:38:35.640
And in this case, you can maybe do more specific choices. For example, you can abandon the local

01:38:35.640 --> 01:38:42.200
permutation invariance. So, you will get back to convolutions. So, basically, the common denominator

01:38:42.200 --> 01:38:45.880
of these approaches is that you have more structure, right? Sometimes you can assume,

01:38:46.520 --> 01:38:53.080
sometimes you can invent, right? But that's the idea. On the other hand, you can say that you

01:38:53.080 --> 01:38:57.960
don't like the graph that is given to you, right? And you choose to completely ignore it, right?

01:38:57.960 --> 01:39:04.200
So, you just assume empty edge sets. So, you're back to the bare bone, right? So, the object is

01:39:04.200 --> 01:39:10.040
just a collection of nodes, which is a set. And these are the architectures that are called deep

01:39:10.040 --> 01:39:14.120
sets or point nets. So, that's the simplest case of graphing electrics, right? Where you don't

01:39:14.120 --> 01:39:18.680
have a graph, you just have the nodes. The other extreme of this is when you allow interaction

01:39:18.680 --> 01:39:23.080
between every pair of nodes, right? Again, you don't trust your graph for some reason. So,

01:39:23.080 --> 01:39:26.280
every pair of nodes can interact. So, it's a complete or a fully connected graph. And this

01:39:26.280 --> 01:39:31.240
is how transformers work, right? So, in this case, you will use, for example, the attentional

01:39:31.240 --> 01:39:37.080
flavor of the graph in electrical architectures. And you will learn the right graph for your task,

01:39:37.080 --> 01:39:41.880
right? In a sense, through the attention mechanism, for example. There is another class of methods,

01:39:41.880 --> 01:39:47.560
and this is what we call graph rewiring, where you say, okay, I don't like the graph that is given.

01:39:47.560 --> 01:39:52.840
I would like to change it a little bit, maybe. And this way, the graph will become, in some sense,

01:39:52.840 --> 01:39:58.760
better for message passing. Okay, we'll talk about it in more details. So, let's talk about

01:39:58.760 --> 01:40:04.600
graph rewiring and specifically about transformers. So, in transformers, you assume that the graph is

01:40:04.600 --> 01:40:09.720
complete, right? So, every node is connected to every node. And if you try to apply a convolutional

01:40:09.720 --> 01:40:17.160
style GNN architecture, right, that depends on the coefficients of the, representing the structure

01:40:17.160 --> 01:40:23.560
of the graph, then, basically, here, the sum goes over all the nodes, and basically, this argument

01:40:23.560 --> 01:40:27.160
is equal for every node. So, it's not informative, right? You're not adding any information.

01:40:28.120 --> 01:40:34.600
So, you need to use at least an attentional architecture. And in this case, this is already,

01:40:34.600 --> 01:40:39.560
this already looks like a transformer. And you can think of attention weights as a kind of

01:40:39.560 --> 01:40:46.200
learned graph adjacency, right? That depends on this task that you're trying to solve. And this is

01:40:46.280 --> 01:40:51.960
a special case of GNNs. Now, of course, in natural language processing, many tasks that you want to

01:40:51.960 --> 01:40:56.760
solve do not require permutation invariance. You actually want them to be not permutation

01:40:56.760 --> 01:41:01.800
invariance. For example, you want to depend on the order of words in the sentence. So, this is

01:41:01.800 --> 01:41:08.280
typically achieved by adding positional encoding. So, in the simplest case, you just equip every

01:41:08.280 --> 01:41:12.680
node with some additional coordinates that tell you where you are located in the domain, right?

01:41:12.680 --> 01:41:18.920
Which in case of transformers, it's where you're located in a sequence. And typically,

01:41:18.920 --> 01:41:23.320
this is how positional encoding looks like, right? So, these are just some synosoids.

01:41:24.360 --> 01:41:27.720
So, for graphs, you can do other things, right? So, the analogy of synosoids would be the,

01:41:28.360 --> 01:41:34.840
yeah, question? Sir, are you aware of some studies about positional encoding, especially

01:41:34.840 --> 01:41:39.560
about relative positional encoding, which is getting some popularity in transformers architecture? So,

01:41:40.440 --> 01:41:46.920
like, the studies which consider all invariances in those positional encoding,

01:41:47.640 --> 01:41:52.360
like, we not always want to have absolute positional encoding. Sometimes we want to be,

01:41:53.400 --> 01:41:57.480
sometimes some shifts or rotations are not relevant. So, do you know some literature on that,

01:41:57.480 --> 01:42:02.360
perhaps? Yeah, so, relative positional encoding, yeah, good question. So, the analogy, let's say,

01:42:02.360 --> 01:42:07.240
of this standard global positional encoding would be the plus and eigenvectors. So, the analogy of

01:42:07.240 --> 01:42:13.400
local positional encoding would be something that, for example, the DGN architecture implemented,

01:42:13.400 --> 01:42:19.480
Gabriele Corso and others directed graph networks. So, what they do, they take, for example, the

01:42:19.480 --> 01:42:25.880
same plus and eigenvectors and compute their gradients on the edges and then transform these

01:42:25.880 --> 01:42:29.800
features in some way. So, this gives you a kind of local direction. So, that would be probably

01:42:30.760 --> 01:42:38.440
a good analogy of local positional encoding. So, with the plus and eigenvectors as well,

01:42:38.440 --> 01:42:43.240
you have some ambiguities. So, there has been actually a recent paper from Derek Lim and others

01:42:43.240 --> 01:42:48.280
from MIT where they are able to solve these ambiguities. You can use random walk kernels,

01:42:49.320 --> 01:42:53.080
you can use substructure counting, as we've seen, right? So, there are many ways of doing it.

01:42:53.960 --> 01:42:59.720
But basically, between these two extremes, right, whether ignoring the graph or learning the

01:42:59.720 --> 01:43:04.520
graph, of course, it comes at the expense of complexity, which is a huge problem in large

01:43:04.520 --> 01:43:10.600
language models where the size of the domain, right, the length of the text can be very large.

01:43:10.600 --> 01:43:15.480
So, probably one of the key computational questions would be how to compute these

01:43:15.480 --> 01:43:21.640
attention more efficiently. So, here somewhere in between comes the graph rewiring approaches.

01:43:22.520 --> 01:43:29.320
And with graph rewiring, it means that your computational graph is not equal to the input

01:43:29.320 --> 01:43:35.880
graph. And it's a little bit controversial topic because the graph being part of the input, somehow

01:43:37.400 --> 01:43:43.080
you don't want to change the input, right? So, but the fact that many architectures do it, right?

01:43:43.080 --> 01:43:47.880
So, if you have, for example, a very large graph like a social network where you have a lot of

01:43:47.880 --> 01:43:52.680
neighbors in some nodes, you cannot aggregate information from millions of nodes. So, you need

01:43:53.320 --> 01:43:56.520
to sample the neighbors, right? Which means that you are using a different graph

01:43:57.080 --> 01:44:02.040
from the input one. So, neighborhood sampling is a form of graph rewiring, right? You can also use

01:44:02.040 --> 01:44:06.360
graph neural networks where you bring information from not immediately your one-hop neighbors,

01:44:06.360 --> 01:44:12.360
but also from multiple hops away, right? So, this is also some form of graph rewiring.

01:44:12.360 --> 01:44:17.000
Transformers are also an extreme form of graph rewiring, right, where you allow access to all

01:44:17.000 --> 01:44:21.640
the nodes in the graph. Some kind of pre-processing, right, where you pre-wire the graph for example

01:44:22.360 --> 01:44:24.920
by some form of diffusion. Yeah, a question?

01:44:28.520 --> 01:44:32.920
So, how do you do the neighbor sampling? Is it just like a stochastic?

01:44:32.920 --> 01:44:36.520
Or do you... Yeah, usually you sample with repetition. So, this is what Sage did.

01:44:37.800 --> 01:44:42.920
Do you think it's optimal or is there a way to optimize it in the best way possible?

01:44:43.560 --> 01:44:48.920
I don't remember if they looked into it. Probably, of course, it might matter how you sample, but

01:44:49.320 --> 01:44:55.240
I don't have on top of my mind any significant result that would tell how to do it better.

01:44:58.920 --> 01:45:04.680
Could you elaborate more on this diffusion processes on the graphs you mentioned?

01:45:04.680 --> 01:45:11.160
Yeah, I will get to it. So, I will go through the diffusion equations and then I will talk about

01:45:11.160 --> 01:45:16.920
this as well. So, I'm specifically referring to Degel, the work of Stefan Gundem and his students.

01:45:19.720 --> 01:45:25.320
So, basically, bottom line is graph is not really any sacrosanct object and in practice,

01:45:25.320 --> 01:45:31.000
many architectures even without admitting it do some form of graph rewiring, right? So,

01:45:31.000 --> 01:45:35.240
you can also rewire the graph throughout the neural network, so it doesn't need to stay the same

01:45:36.600 --> 01:45:40.440
across all layers. And this has been shown efficiently, for example, in the context of

01:45:40.440 --> 01:45:44.360
our squashing where you can do maybe a few steps of standard message passing and then

01:45:44.920 --> 01:45:48.040
do message passing on a complete graph, right? So, like, what transformers do?

01:45:48.760 --> 01:45:55.800
And so, the argumentation usually is that the first steps capture somehow the structure

01:45:55.800 --> 01:46:00.680
of the graph, similar to Weisberg and Lemon, and then, basically, you accumulate this information

01:46:00.680 --> 01:46:07.000
as features. And there is an extreme example of this. I think it's called GRIT. So, this is

01:46:07.000 --> 01:46:13.800
from the group of field torrid Oxford and what they do is they use just something similar to

01:46:13.880 --> 01:46:18.040
heat kernel to encode the local structure of the graph and then just use MLP.

01:46:18.920 --> 01:46:23.880
So, there are other extremes like this, right? So, basically, you have just some kind of local

01:46:23.880 --> 01:46:28.840
feature of the graph that you can maybe make learnable, but then the graph itself is not used,

01:46:28.840 --> 01:46:33.800
so there is no message passing. And I think it's an open question of how much you want

01:46:35.320 --> 01:46:41.160
to use to capture the structure in the form of some features versus in the form of the

01:46:41.240 --> 01:46:46.040
computational architecture. So, the graph can be changed throughout the layers and, well,

01:46:46.040 --> 01:46:52.600
one of the first architectures to do it was what we did with Justin Solomon and his students,

01:46:53.320 --> 01:46:58.920
and that was considering problems in computer graphics. So, we have a point cloud, let's say,

01:46:58.920 --> 01:47:05.000
of this airplane. So, every point has three-dimensional Euclidean coordinates,

01:47:05.000 --> 01:47:09.400
and here the task is segmentation. So, you want to label each of the points on the airplane,

01:47:09.400 --> 01:47:15.720
whether they belong to the body, to the engines, to the wings, and so on. And here we create a

01:47:15.720 --> 01:47:23.320
graph to basically to represent the local structure of the data. And what is shown here by the colors

01:47:23.320 --> 01:47:29.640
is a distance in the feature space, right, in the latent, in the latent feature space of

01:47:29.640 --> 01:47:35.640
respective layers in this network to the rest of the points from the red point, right? So, initially,

01:47:35.640 --> 01:47:41.800
this is Euclidean, as you can see that, that's basically the input space, three-dimensional

01:47:41.800 --> 01:47:47.400
R3, but then as you go deeper, you see that it becomes more semantic. So, here, for example,

01:47:47.400 --> 01:47:53.560
points on the same, on the engine become closer, or on the wing become closer. So, basically,

01:47:53.560 --> 01:47:59.800
the space itself is used for the construction of the graph, and we call it dynamic graph neural

01:47:59.800 --> 01:48:08.680
networks, or maybe not very, very, very lucky name. I think many architectures are called

01:48:08.680 --> 01:48:15.000
like this. And we use it also in different incarnations or similar ideas under the name

01:48:15.000 --> 01:48:20.600
of differentiable graph module, where basically you have applications where you don't know the

01:48:20.600 --> 01:48:25.080
graph a priori. So, that's, for example, the case with medical imaging, where you have maybe

01:48:25.080 --> 01:48:30.040
nodes representing different patients, or maybe different regions in the brain, and you have

01:48:30.040 --> 01:48:35.000
basically two branches of graph neural network architecture, one that is computing the filters

01:48:35.000 --> 01:48:39.880
on the graph, and another one computing the graph itself. And, of course, there is question of

01:48:39.880 --> 01:48:45.880
computational complexity, but we're able to show that it is better to learn the graph for the,

01:48:45.880 --> 01:48:50.520
for the task rather than to construct it ad hoc in some kind of handcrafted way.

01:48:51.480 --> 01:48:56.520
Now, talking about message passing in general, so these are hecticly added slides based on some

01:48:56.520 --> 01:49:04.600
discussions that we had in the coffee break. So, if you think of message passing and different

01:49:04.600 --> 01:49:11.000
versions of it like transformers, basically, the difference is in two questions, is what's

01:49:11.000 --> 01:49:16.120
information to send, what's information to pass, and where to pass, whether you follow the graph,

01:49:16.760 --> 01:49:22.920
whether you pass information to your neighbors, or you pass information to distant nodes, whether

01:49:22.920 --> 01:49:28.360
it's in K-Hope or to all the neighbors in the graph, and the question of what is how you exactly

01:49:28.360 --> 01:49:34.200
transform your information. So, it's also interesting to add to this another question,

01:49:34.200 --> 01:49:39.720
is when to send information? And if you look at it, basically, it's a multi-step process. So,

01:49:39.720 --> 01:49:45.000
this is how a classical message passing neural network works. So, these are three layers of an

01:49:45.000 --> 01:49:49.720
NPNN or three iterations of, let's say, something similar to vice-fair and lemon, and I'm sending

01:49:49.720 --> 01:49:55.480
information from these nodes to these nodes, right? So, it takes three steps. So, first here,

01:49:55.480 --> 01:50:00.440
then these nodes propagate information to their neighbors, and then this green node receives

01:50:00.440 --> 01:50:05.720
information from both neighbors, right? So, in a transformer, all the information is available

01:50:05.720 --> 01:50:11.720
at once. So, at the same moment of time, I'm sending information from all the nodes to the

01:50:11.720 --> 01:50:16.680
green nodes, right? So, it has access to all the information across three layers. But you can also,

01:50:16.680 --> 01:50:23.640
so, basically, here, the difference is where to send information, right? That's the structure of

01:50:23.640 --> 01:50:28.840
the computational graph. But we don't consider here also when to send this information. And you

01:50:28.840 --> 01:50:37.400
can imagine an architecture where, for example, you delay the information. So, here, the first node

01:50:37.480 --> 01:50:41.880
in first iteration sends information to its neighbor. At the second layer, it sends to

01:50:41.880 --> 01:50:46.600
two hop neighbors, right? So, this information becomes gradually available. Not like in a transformer

01:50:46.600 --> 01:50:52.280
where you flood all the nodes with information from all other nodes at once. You make it

01:50:52.280 --> 01:50:56.680
progressive, right? So, basically, these are kind of shortcuts that you have in the graph,

01:50:57.480 --> 01:51:01.560
but they are made progressively as you go deeper into the architecture. You can also

01:51:01.560 --> 01:51:05.880
make, so, you can think of this as a kind of skip connections, but you can also make skip connections

01:51:05.960 --> 01:51:10.920
sparse in time. So, you can delay this information. And I'm saying that here, the information

01:51:10.920 --> 01:51:18.600
from the first node comes to the next node and is delayed in time, right? So, it would arrive to

01:51:18.600 --> 01:51:23.320
it anyway, but it would be entangled with the information in other nodes. So, I'm allowing

01:51:23.320 --> 01:51:30.360
direct access, but I'm delaying it. And this potentially has interesting implications also

01:51:30.360 --> 01:51:35.640
on the hardware aspect of graph neural networks, because if your graph is very large, you typically

01:51:35.640 --> 01:51:41.960
partition it into different parts of memory, right? And the cost of message passing is not

01:51:41.960 --> 01:51:46.440
the same, right? So, messages within the same memory cost much less than messages across

01:51:46.440 --> 01:51:50.280
different memories. So, you can imagine a graph neural network or you are doing fast messages

01:51:50.280 --> 01:51:56.440
within each part of memory hierarchy and slow messages across, right? So, you don't want to

01:51:56.440 --> 01:52:01.800
wait for all the messages to arrive. You might want to do fast messages while waiting for slow

01:52:01.800 --> 01:52:07.240
messages, right? So, this architecture that we call drill or end drill, this version with delays

01:52:07.960 --> 01:52:12.120
potentially allows for it. So, I think it would be interesting to test it on actually some

01:52:13.800 --> 01:52:20.360
practical hardware like graph core, for example. So, let me move to the next topic and this is

01:52:20.360 --> 01:52:28.440
physics-inspired graph neural network. So, I promised PDEs, so I need to hold this promise. And

01:52:29.240 --> 01:52:34.840
let's again take a step back and look at different objects that we've seen so far, right? And also,

01:52:34.840 --> 01:52:40.280
you can argue objects that are studied in this broader field of geometric deep learning. So,

01:52:40.280 --> 01:52:45.080
let's say grids, meshes and graphs, right? So, you can think of them as more or less the same thing,

01:52:45.080 --> 01:52:50.360
just with more structure, right? So, meshes in addition to have also triangles. So, these are

01:52:50.360 --> 01:52:55.240
simplicial complexes. So, graphs with some extra constraints or extra structure. Grids are also

01:52:55.240 --> 01:53:01.320
special type of graphs where we have certain organization. So, if you look at the grid and

01:53:01.320 --> 01:53:06.040
you look at how you aggregate information from your neighbors, there is no ambiguity, right? In a

01:53:06.040 --> 01:53:11.000
grid, unlike a general graph, I can order my neighbors in a canonical way, right? I have a top

01:53:11.000 --> 01:53:16.600
neighbor, I have a left neighbor, bottom and right, like shown here. So, it is totally unambiguous,

01:53:16.600 --> 01:53:21.880
right? So, this ordering is fixed. On a mesh, the situation is slightly different. So, I can pick up

01:53:21.880 --> 01:53:27.720
my first neighbor and then I can order all the rest of the nodes, all the rest of the neighbors

01:53:27.720 --> 01:53:33.560
in, for example, clockwise orientation. And this is possible because mesh is a discrete manifold.

01:53:33.560 --> 01:53:39.320
So, it's locally Euclidean. I have this meaningful ordering, right? But, of course, the choice of

01:53:39.320 --> 01:53:44.280
the first neighbor is ambiguous. So, I can rotate everything, right? So, the ambiguity here is up

01:53:44.280 --> 01:53:49.800
to rotation. In a graph, as we've seen, any permutation works, right? So, everything is defined

01:53:50.360 --> 01:53:55.400
up to a permutation. So, in a sense, graphs have the least structure out of all these objects,

01:53:55.400 --> 01:54:01.960
right? So, second observation is that if I look at grids and meshes, I can think of them as

01:54:01.960 --> 01:54:08.280
discretizations of some continuous spaces. So, a grid is discretization of a plane or a mesh is

01:54:08.280 --> 01:54:14.280
discretization of a two-dimensional surface or a manifold. We don't have immediately this analogy

01:54:14.280 --> 01:54:18.920
for graphs and even though there is an entire field that is called network geometry that tries to

01:54:18.920 --> 01:54:25.400
think of graphs as some discretization of continuous space, it would be nice to have

01:54:25.400 --> 01:54:32.280
some continuous models for GNNs, right? And that's the idea of what we call physics-inspired GNNs.

01:54:32.280 --> 01:54:38.760
So, you can consider some class of graph neural networks as a dynamic system. So, you have

01:54:38.760 --> 01:54:43.000
particles that live in the dimensional feature space. And let's assume that the dimensionality

01:54:43.000 --> 01:54:48.360
always is kept the same, right? It doesn't need to be in general graph neural networks. So, every node

01:54:48.360 --> 01:54:53.480
is some point, some particle that moves along a trajectory that is shown by this red line here.

01:54:53.480 --> 01:54:58.040
So, a GNN is essentially a dynamic system, right? That is governed by the system of differential

01:54:58.040 --> 01:55:08.600
equations. F here is some coupling functions, right? That makes dependency between different

01:55:08.600 --> 01:55:13.880
particles. And it depends both on the particles and the graph. And it's also parameterized by some

01:55:13.960 --> 01:55:19.080
trajectory of parameters that are, you know, by data. So, T is a continuous time parameter.

01:55:19.080 --> 01:55:23.720
I can discretize it and this corresponds to layers of a graph neural network. And the graph,

01:55:23.720 --> 01:55:27.800
right, you can think of it either as a coupling function, right? So, this F here, how different

01:55:27.800 --> 01:55:34.360
rows of this matrix interact with each other or discretization of some continuous space,

01:55:34.360 --> 01:55:43.400
as we'll see in the next few examples. So, basically as the evolution equation,

01:55:43.480 --> 01:55:47.560
right, that governs the behavior of these particles, I can put here more or less anything,

01:55:47.560 --> 01:55:52.760
right? So, there's plenty of different differential equations that describe different systems. But

01:55:53.720 --> 01:55:59.000
probably the first thing that comes to your mind when you think of propagation or diffusion of

01:55:59.000 --> 01:56:05.160
some stuff is the diffusion equation, right? And this, in fact, was one of the earliest

01:56:05.160 --> 01:56:11.160
mathematically analyzed physical phenomena, right? The diffusion of heat, analyzed by

01:56:11.160 --> 01:56:16.760
non-Elst and Newton himself in an anonymous paper written in Leighton in the transactions

01:56:16.760 --> 01:56:21.400
of the royal site. So, it's interesting actually that the journal had a mixture of papers written

01:56:21.400 --> 01:56:29.160
in English and Leighton. And it was anonymous. Well, Newton devised an experimental setup where he

01:56:29.160 --> 01:56:37.960
heated pieces of different objects, metal, I think, mostly, and then he looked with his

01:56:37.960 --> 01:56:43.800
self-made thermometer here, measured how much heat is lost over a period of time and devised some

01:56:44.680 --> 01:56:49.400
law that is formulated in modern terminology like this. What is nowadays called the Newton

01:56:49.400 --> 01:56:54.360
law of cooling, which says that the temperature that the hot body loses in a given time is proportional

01:56:54.920 --> 01:56:58.760
to the temperature difference between the object and the environment. He actually didn't use the

01:56:58.760 --> 01:57:03.800
term temperature. That's modern terminology. He used the term heat, which has a different meaning

01:57:04.360 --> 01:57:11.240
or color in Leighton. And somehow, everybody guessed that it was his paper, even though he

01:57:11.240 --> 01:57:17.240
didn't sign it. Now, it took some time until this process was fully understood. So, Fourier

01:57:17.240 --> 01:57:24.840
devised the local version differential equation that governs heat diffusion and then thick

01:57:25.720 --> 01:57:32.040
in the late 19th century defined what we nowadays understand as diffusion equations.

01:57:32.040 --> 01:57:36.600
So, if you want to apply this idea to a graph, we can consider a diffusion process on a graph.

01:57:37.240 --> 01:57:42.040
And that's how it looks like. So, here x is some quantity that is being diffused. Think of it as

01:57:42.040 --> 01:57:48.200
temperature if it's color. So, a node has certain temperature at time t. So, t is a continuous

01:57:48.200 --> 01:57:53.000
variable. And what is written here is the self-temperature of the node. This is the temperature

01:57:53.000 --> 01:57:58.280
of the environment. So, it's the average of the one hop neighborhood of the node. And this is the

01:57:58.280 --> 01:58:05.000
rate of temperature change. So, there might be some proportion coefficient here. So, if I rearrange

01:58:05.000 --> 01:58:10.600
the terms on this expression, I can write it like this. So, I'm slightly massaging the formula.

01:58:10.600 --> 01:58:15.960
What is written here is called the gradient. So, it's just the difference between end points

01:58:16.680 --> 01:58:22.760
of an inch. So, I take the feature here and the feature here and subtract one from another. So,

01:58:22.840 --> 01:58:28.120
that's the graph analogy of the standard gradient operator from classical calculus.

01:58:28.840 --> 01:58:32.920
And what is written here is, again, the discrete analogy of the divergence operator,

01:58:32.920 --> 01:58:40.040
again, from basic course in calculus. So, in terms of operators, you can think of

01:58:40.840 --> 01:58:46.040
the features as that they live in the nodes of the graph as a scalar field. Then the gradient

01:58:46.040 --> 01:58:50.920
makes a scalar field into a vector field, right, that lives on the edges of the graph. And then

01:58:50.920 --> 01:58:56.600
the divergence does the opposite. So, it collects the information from the edges that live, that

01:58:56.600 --> 01:59:00.520
emanate from a node and basically sums them up maybe with some different weights, right? So,

01:59:00.520 --> 01:59:05.800
that's what the divergence, right? So, gradient makes a scalar field into vector fields. Divergence

01:59:05.800 --> 01:59:11.480
makes vector fields into scalar fields. Acting together, they take a scalar field into a

01:59:11.480 --> 01:59:16.040
scalar field. The divergence of the gradient or minus divergence of the gradient in our notation

01:59:16.040 --> 01:59:19.800
is what is called the Laplacian operator, right? So, what is written here is the

01:59:19.800 --> 01:59:24.760
graph Laplacian operator and by definition, you see what it does. So, it compares you to your

01:59:24.760 --> 01:59:29.240
neighborhood, how different you are from the average of your neighbors. And this is really a

01:59:29.240 --> 01:59:34.040
very important operator. It comes everywhere in mathematical physics and from quantum mechanics

01:59:34.040 --> 01:59:41.400
to wave equations to diffusion equations, and particularly important here. So, what else is

01:59:41.400 --> 01:59:45.880
the heat equation? So, this is very simple, right? So, this is what we call homogeneous

01:59:45.880 --> 01:59:50.360
isotropic diffusion equation. Basically, heat propagates everywhere in the same way, right?

01:59:52.440 --> 01:59:58.440
What else heat equation is? It's an example of prototypical gradient flow. And gradient flow

01:59:58.440 --> 02:00:03.400
is a type of evolution equation, differential equation that looks like this. So, the step at

02:00:03.400 --> 02:00:08.600
every point of time is in the direction of the minus gradient of some energy, that you know here by

02:00:08.600 --> 02:00:13.320
E. And the energy that corresponds to the diffusion equation is what we call the Dirichlet energy.

02:00:14.200 --> 02:00:18.920
So, the Dirichlet energy, you can write it as a quadratic form with respect to the Laplacian

02:00:18.920 --> 02:00:23.080
operator. And it measures the smoothness of the node features, right? How different you are from

02:00:23.080 --> 02:00:28.680
your neighbors. So, the smallest value it can achieve is zero. In this case, all the features are the

02:00:28.680 --> 02:00:33.640
same, right? And you can show that Dirichlet energy decreases along the flow. So, if you run the

02:00:33.640 --> 02:00:39.960
diffusion to infinity, all will become constant, right? So, the heat will basically propagate

02:00:40.040 --> 02:00:45.160
on the domain and everything will have the same temperature. So, this is what we typically call

02:00:45.160 --> 02:00:49.160
over smoothing in graph neural networks, right? And I should say that over smoothing is not

02:00:49.160 --> 02:00:53.080
necessarily a bad phenomenon. So, usually it's described as a kind of catastrophe that happens

02:00:53.080 --> 02:00:57.640
in GNNs and prevents us from making deeper architectures. But it can actually be a very

02:00:57.640 --> 02:01:02.760
benign thing, right? So, you can imagine without any learning. So, if I give you a graph and I

02:01:02.760 --> 02:01:08.600
have labels of a few nodes, and if I assume that the structure of the graph is homophilic in the

02:01:08.600 --> 02:01:13.800
sense that my neighbors are expected to have labels similar to me, then I can just diffuse this

02:01:13.800 --> 02:01:17.880
information so it will be, you can think of it as a heat diffusion equation with boundary conditions.

02:01:18.920 --> 02:01:25.160
And by just doing this, it is very likely that the result will be very good, right? If the graph

02:01:25.160 --> 02:01:30.120
is not homophilic, of course, you need to do something else, maybe work harder, but on its own,

02:01:30.120 --> 02:01:34.040
the over smoothing is not necessarily bad. Yeah, there is a question.

02:01:39.480 --> 02:01:46.520
Actually, I have like two questions on this topic. So, first of all, can I see this as the,

02:01:47.800 --> 02:01:56.440
in some sense, the message passing algorithm there, but via the ordinary differential equation between

02:01:56.440 --> 02:02:05.560
the edges? Right. So, well, whether they call it message passing or not, I think it's a good

02:02:05.560 --> 02:02:09.960
question. So, every iteration when you discretize this differential equation, every step will

02:02:09.960 --> 02:02:14.520
correspond to message passing. Now, in some cases, you can actually have a closed form expression

02:02:14.520 --> 02:02:19.240
for the solution at any time t, right? This is called the heat kernel. So, it will look like

02:02:19.240 --> 02:02:25.880
the exponential of the Laplace matrix. So, I can compute the value of the temperature at every

02:02:25.880 --> 02:02:33.560
point instantaneously without doing these microsteps of message passing. So, whether they call,

02:02:33.560 --> 02:02:37.080
still call it message passing or not. So, this is where semantically I disagree with better,

02:02:37.080 --> 02:02:41.800
for example. He still suggests to call it message passing. I think it's not. So, I don't know how

02:02:41.800 --> 02:02:46.920
to call it better, maybe some kind of spatial coupling, right? Because you have, you have direct

02:02:46.920 --> 02:02:54.360
information to potentially infinite, all the nodes in the graph, right? So, it is something that

02:02:55.480 --> 02:03:00.680
doesn't require propagating information explicitly, right? You might be, when you solve the

02:03:00.680 --> 02:03:06.040
diffusion equation, especially with its own linear, you might not have a choice. But in some cases,

02:03:06.040 --> 02:03:12.680
you do. You have closed form expressions, right? Okay, thank you. And the second question, because

02:03:12.680 --> 02:03:20.040
I suppose that here is like the mostly ordinary differential equations. Is there any research

02:03:20.040 --> 02:03:28.280
to introduce stochasticity there to be, let's say stochastic differential equations, something

02:03:28.280 --> 02:03:36.760
like that, and when it could help in the, in the, in this field? Yeah. So, it's interesting. I think

02:03:36.760 --> 02:03:42.120
there have been some works. And there are some works. So, it talks for example, a big

02:03:42.120 --> 02:03:48.120
expert in this domain is Terry Lyons. So, I think they are working on, on these topics.

02:03:49.640 --> 02:03:52.440
So, we are not considering stochastic differential equations. We are considering

02:03:52.840 --> 02:03:59.000
whether to call them ODs or PDs. So, these are coupled ODs, right? When you discretize,

02:04:00.040 --> 02:04:04.280
a partial differential equation becomes a system of coupled ordinary differential equations. I think

02:04:04.280 --> 02:04:11.320
they, again here, the terminology is more semantic difference, whether you have a continuous spatial

02:04:11.320 --> 02:04:17.080
coordinate that you want to discretize. So, that will be the next part of it. Okay. Thank you.

02:04:17.640 --> 02:04:23.880
Right. Okay. So, that was basically, that was the gradient flow, right? And again,

02:04:23.880 --> 02:04:28.440
an example of, of the heat diffusion equation as, as a prototype of the gradient flow. So,

02:04:29.000 --> 02:04:35.320
if you look again at the, at this view on, on graph neural networks as some kind of

02:04:35.960 --> 02:04:41.560
evolution equations governing some, some physical system. So, the traditional approach

02:04:41.560 --> 02:04:45.720
to graph neural networks is to take this differential equation, discretize it, and then

02:04:45.720 --> 02:04:49.160
parameterize the evolution equation, right? The discrete evolution equation. So,

02:04:49.720 --> 02:04:53.800
every step of an iterative solver here will correspond to a layer, and then you parameterize

02:04:53.800 --> 02:04:59.560
every layer, right? So, that's how you can view graph neural networks. So, instead, what we can do,

02:04:59.560 --> 02:05:06.040
we can start with, with an energy, parameterize an energy, and then derive the evolution equation

02:05:06.040 --> 02:05:11.080
as a gradient flow. So, apparently, there is no difference, right? But the big difference would

02:05:11.160 --> 02:05:17.240
be that we will, it will allow us to make certain architectural choices. So, it will

02:05:17.240 --> 02:05:22.600
restrict our space of all the possible architectures that we can do here, right? Like, for example,

02:05:22.600 --> 02:05:27.640
the form of message passing. That will have better interpretability. And of course, I know that

02:05:27.640 --> 02:05:32.200
interpretability is maybe a bad word in machine learning. So, what I mean here is that we will

02:05:32.200 --> 02:05:39.160
be able to guarantee that certain things happen or not happen. Okay. And I will be more specific

02:05:39.160 --> 02:05:44.760
in a second. So, let's consider the following parametric energy. So, we call it generalized

02:05:44.760 --> 02:05:50.920
Dirichlet energy. It has these two terms. So, you can think of it as an energy that a system of

02:05:50.920 --> 02:05:56.120
particles has, and it's parameterized by two matrices of size d by d, right? d, remind you,

02:05:56.120 --> 02:05:59.720
that's the dimensionality of the features. So, that's the space where the particles move.

02:06:00.600 --> 02:06:04.920
And we have two terms here. So, the external energy term, it acts on all the particles. So,

02:06:04.920 --> 02:06:09.800
think of some force that moves them in some directions. And we have internal energy. So,

02:06:09.800 --> 02:06:15.240
these are the interactions between particles along the edges of the graph, right? Think of maybe

02:06:15.240 --> 02:06:22.440
colonic interactions, right? Or maybe springs that attached to some pairs of particles. And we

02:06:22.440 --> 02:06:27.160
have two types of interactions. So, we have attractive interactions, and they happen along the

02:06:28.040 --> 02:06:34.120
eigenvectors of the matrix W that corresponds to positive eigenvalues and repulsive interactions

02:06:34.200 --> 02:06:42.200
that happen along negative eigenvectors. And you can see an example here. So, here the space is

02:06:42.200 --> 02:06:47.240
two-dimensional just for visualization purposes. And the graph here, you see that it's heterophilic.

02:06:47.240 --> 02:06:52.600
So, the colors of the nodes represent the labels. And the positions represent the features, right?

02:06:52.600 --> 02:06:57.560
The feature coordinates x and y. So, the graph is actually perfectly heterophilic, right? The

02:06:57.560 --> 02:07:03.800
blue nodes have only red neighbors, and the red nodes have only blue neighbors. And yet,

02:07:03.880 --> 02:07:08.040
we can find directions. So, the horizontal direction is repulsive direction, where the

02:07:08.040 --> 02:07:13.400
particles are separated. And the vertical direction is attractive direction, where the particles

02:07:14.360 --> 02:07:21.240
cluster together. We can perfectly separate these types of nodes, right? So, if my task is

02:07:21.240 --> 02:07:27.080
node classification on this graph, by this very simple process, I can solve this problem, right?

02:07:28.120 --> 02:07:32.760
So, we will see in a second that this corresponds to a convolutional architecture. By convolutional,

02:07:32.760 --> 02:07:37.320
I mean that my diffusion operator depends only on the structure of the graph, but not on the features,

02:07:37.320 --> 02:07:42.120
like in more complicated, for example, attentional networks. So, I can write it as AX something,

02:07:42.120 --> 02:07:48.360
right? So, this is what was our distinction. So, in convolutional architectures, we had something

02:07:48.360 --> 02:07:55.400
like this. In attentional architectures, we had something like this, right? So, that would be

02:07:55.400 --> 02:08:01.080
the GCN type of architectures, and this will be the GUT type of architectures. So, the fact that

02:08:01.080 --> 02:08:05.080
this matrix is constant, it depends only on the structure of the graph, is what I call convolutional

02:08:05.080 --> 02:08:11.720
architectures, right? So, basically, these goals against this folklore that I mentioned in the

02:08:11.720 --> 02:08:16.760
beginning, that convolutional architectures are not good for heterophilic graphs. So, you can see

02:08:16.760 --> 02:08:23.160
that they can work very well, right? So, we need to dive deeper and understand what happens here.

02:08:23.880 --> 02:08:27.080
So, if we write the gradient flow of, yeah, question.

02:08:27.400 --> 02:08:31.720
Is there a link between this gradient flow and less and less popular recently,

02:08:34.840 --> 02:08:39.480
probabilistic graphical method, especially if you showed the previous slide, it was very

02:08:39.480 --> 02:08:44.360
similar to restricted Boltzmann, right, when we had this bipartite graph?

02:08:44.360 --> 02:08:48.760
Yeah, you can probably interpret it from, right? So, gradient flows are very basic objects. So,

02:08:48.760 --> 02:08:52.520
it's essentially steepest descent is also a gradient flow, right? So, it's a continuous

02:08:52.520 --> 02:08:59.080
version or a variational version of it. Yeah, so, probably there are links to many different things.

02:09:00.680 --> 02:09:04.600
So, basically, you're minimizing some energy here, right? So, and this energy,

02:09:04.600 --> 02:09:09.800
so, it's not the learning part, right? So, it's the inference. So, applying a neural network

02:09:10.840 --> 02:09:15.720
minimizes some energy, right? You design the network, so, it minimizes some energy, right?

02:09:15.720 --> 02:09:20.520
And the energy is parametric. So, what kind of energy to minimize is what you determine

02:09:20.520 --> 02:09:26.280
based on the task. So, that's what is done by back propagation. So, basically, the gradient flow,

02:09:26.280 --> 02:09:30.840
you just differentiate this energy with respect to its parameters, right? By data I denote here,

02:09:30.840 --> 02:09:35.480
these two matrices omega and w. And one thing that you notice immediately that they all appear

02:09:35.480 --> 02:09:40.680
in symmetrized form, right? So, they appear in quadratic terms. So, they appear as omega plus

02:09:40.680 --> 02:09:44.920
omega transpose. So, we can just assume that they are symmetric to start with, right? So,

02:09:44.920 --> 02:09:49.320
that's already first restriction that comes from this assumption of the gradient flow.

02:09:49.960 --> 02:09:53.160
The second thing is, again, this is by the design of the energy that

02:09:53.880 --> 02:09:59.640
the parameters are time-independent, right? So, they don't depend on t, right? Which will become

02:09:59.640 --> 02:10:04.840
the layer number. And if we discretize it, this is how we discretize. So, we replace the temporal

02:10:04.840 --> 02:10:13.720
derivative with forward difference. So, this is what is also called the explicit Euler scheme.

02:10:14.680 --> 02:10:21.880
So, we have some step-sized tau. And this gives residual convolutional type GCN, again,

02:10:22.600 --> 02:10:27.000
the convolutional type GNN. Why I call it convolutional? Because this matrix A, right,

02:10:27.000 --> 02:10:31.480
that's where the diffusion happens, where the message passing happens, right? When I send

02:10:31.480 --> 02:10:39.400
information across adjacent nodes, it's not dependent on x. It's fixed, right? So, you can call

02:10:39.480 --> 02:10:45.480
it a kind of a convolution. The weights are symmetric and the weights are shared across

02:10:45.480 --> 02:10:51.560
different layers, okay? So, the way that you can use it, you can optionally do some nonlinear

02:10:51.560 --> 02:10:56.280
encoding typically to reduce the dimensionality. So, we have some fixed low dimensional dimension

02:10:56.280 --> 02:11:02.680
of this space. You apply a linear gradient flow. So, all the propagation of information on the

02:11:02.680 --> 02:11:08.280
graph is linear. So, there are no nonlinear activations, right? And then you do some decoder.

02:11:08.280 --> 02:11:11.800
That's for node classification, right? So, that's how the architecture looks like.

02:11:12.520 --> 02:11:16.920
Now, if you compare it to the classical convolutional architectures like GCN of

02:11:16.920 --> 02:11:22.040
Kieff and Welling, so there are several differences. So, the GCN is non-residual,

02:11:22.760 --> 02:11:28.360
the weights are non-symmetric, and the weights are different per layer, right? And also,

02:11:28.360 --> 02:11:33.640
they use nonlinear activation for every layer. We don't. So, in a sense, it's a kind of antithesis

02:11:33.720 --> 02:11:39.880
to a typical graph neural network or a typical deep learning architecture. So, typically,

02:11:39.880 --> 02:11:44.040
you say that you need many layers, each layer parameterized separately and nonlinear are they

02:11:44.040 --> 02:11:48.600
activated. We don't have anything like this here, right? So, all the, again, the diffusion part is

02:11:48.600 --> 02:11:55.400
linear. We have a nonlinear decoder and potentially a nonlinear encoder. So, what we gain from the

02:11:55.400 --> 02:12:01.080
it being gradient flow is interpretability in the following sense that we can show that

02:12:01.080 --> 02:12:06.040
in certain situations we can induce both low and high frequency dominated dynamics.

02:12:06.040 --> 02:12:11.880
I will define it in a second. And what it means is that we can work both with homophilic and

02:12:11.880 --> 02:12:16.760
heterophilic graphs. Now, because the weights are shared across layers, actually the number of layers

02:12:16.760 --> 02:12:22.040
becomes a completely irrelevant notion here. So, what matters is really the diffusion time.

02:12:22.760 --> 02:12:26.440
The number of layers is just how finely I discretize my differential equation.

02:12:27.400 --> 02:12:29.880
And the number of parameters is independent of it, right?

02:12:30.920 --> 02:12:34.600
Unlike, again, the classical architecture where the more layers you have, the more parameters

02:12:34.600 --> 02:12:40.520
you have. Yeah, question? Question. Why don't you stay in the latent space? Why do you have

02:12:40.520 --> 02:12:46.440
needed a decoder? Why do I need a decoder? Well, the decoder can, for example, you might need it to

02:12:46.440 --> 02:12:54.600
produce a label for an old. It's not the decoder in the original space. It could be in any,

02:12:55.240 --> 02:12:58.440
it could be also decoding the original space, right? So, you might, for example,

02:12:58.440 --> 02:13:03.320
want to work with low dimensional space and then the number of your classes might be different,

02:13:03.320 --> 02:13:09.480
right? But the important part is that it's nonlinear. So, all the nonlinearity goes there.

02:13:10.920 --> 02:13:15.080
Okay. And, well, you can also use an encoder. I will show why encoder can be problematic.

02:13:15.080 --> 02:13:17.960
We actually, we have experimental evidence that it's not needed.

02:13:19.320 --> 02:13:23.720
So, first of all, let's talk about homophilic and heterophilic graphs. So, again, homophilic

02:13:23.720 --> 02:13:28.600
look like this, heterophilic look like this. And the data set here is what we call synthetic

02:13:28.600 --> 02:13:35.000
core. So, it's probably you're all familiar with it. So, it's this citation network where we can

02:13:35.000 --> 02:13:39.960
actually change the structure of the graph in a way that it becomes either more homophilic or more

02:13:39.960 --> 02:13:46.440
heterophilic, right? And here are the two extreme choices of an architecture. So, the task here

02:13:46.440 --> 02:13:52.200
is no classification. I can either ignore the structure of the graph altogether and do just

02:13:52.280 --> 02:13:57.480
no wise predictions with a multilayer perceptron. And, of course, it's not great, right? So, it

02:13:57.480 --> 02:14:03.960
achieves about 67% accuracy. But no matter how homophilic or heterophilic data set is,

02:14:04.600 --> 02:14:10.360
the result is the same because it simply ignores the graph, right? The other extreme is a GCN.

02:14:10.360 --> 02:14:14.600
So, when the graph is homophilic, it works extremely well, right? Almost 100% because

02:14:14.600 --> 02:14:20.040
when my neighbors contain similar information, I will be basically averaging them and I will

02:14:20.040 --> 02:14:24.920
be doing some form of denoising. But when the graph is heterophilic, then it degrades very

02:14:24.920 --> 02:14:28.920
badly because, in this case, the neighbor information is more detrimental than helpful.

02:14:29.640 --> 02:14:35.320
So, the gradient flow framework, it benefits from basically its kind of mixture of both worlds. So,

02:14:35.320 --> 02:14:41.960
it works as well as GCN in the homophilic case and as well as node-wise predictions in the

02:14:41.960 --> 02:14:47.560
heterophilic case and the graceful transitions between the two. Now, another important thing,

02:14:47.560 --> 02:14:52.440
right? And that's where the encoder and decoder question comes into the play. So, if you write

02:14:52.440 --> 02:14:57.960
the output of the neural network after L layers, this is how it looks like. So, it's basically

02:14:57.960 --> 02:15:01.560
it's a polynomial in these matrices. So, the matrices are dependent, of course. So, there are

02:15:01.560 --> 02:15:08.440
some powers here. The parameters are shared. But if we ignore the encoder, right, and we can ignore

02:15:08.440 --> 02:15:15.480
it, at least in the test that we did, basically what the diffusion part can be precomputed,

02:15:15.480 --> 02:15:20.440
right? You see that it acts as some kind of powers of the diffusion matrix on the features. So,

02:15:20.440 --> 02:15:26.280
I can pre-diffuse the features once as a pre-computation step and then it all boils down to node-wise

02:15:27.720 --> 02:15:33.960
multiplications by these matrices. So, the computationally difficult part, both computation

02:15:33.960 --> 02:15:38.760
in terms of the number of multiplication operations as well as the memory access, which,

02:15:39.560 --> 02:15:45.160
unless the graph is very structured, you have random access to your neighbor nodes,

02:15:45.800 --> 02:15:51.320
this is really the heaviest part of graph neural networks. So, this now can be totally trivialized.

02:15:52.440 --> 02:15:57.400
So, we've done the earlier versions of this architecture with just one convolutional layer.

02:15:57.400 --> 02:16:03.000
We call it sine. So, already several years ago, we tested this on the graphs of hundreds of millions

02:16:03.000 --> 02:16:07.960
of nodes. So, with this architecture, actually, because now we also get rid of the non-linearity

02:16:08.760 --> 02:16:11.800
you can apply it to a very large graph, basically, you can design

02:16:11.800 --> 02:16:15.480
multi-layer architectures that work well both in homophilic and heterophilic settings.

02:16:16.920 --> 02:16:23.080
Now, another thing, and this is what I mentioned regarding the nature of the dynamics that is

02:16:23.080 --> 02:16:31.880
induced by this gradient flow, is if we look at our data, right, and our data, I remind you, it's

02:16:32.840 --> 02:16:38.760
the matrix x, right, which is of size n by d and is the number of nodes, d is the number of dimensions.

02:16:39.960 --> 02:16:44.440
So, we can do an analogy of a two-dimensional Fourier transform, right. I remind you that

02:16:44.440 --> 02:16:49.240
for a graph that we assumed, an undirected graph, the graph of plus n has orthogonal

02:16:49.240 --> 02:16:54.200
identity composition. So, it's eigenvectors form an orthogonal basis, right, the basis for the rows

02:16:54.200 --> 02:17:00.360
of the matrix, and the matrix w, which we assumed by virtue of our process being a gradient flow,

02:17:00.360 --> 02:17:04.760
right, it was symmetric, we can have also an orthogonal identity composition, let's call

02:17:04.760 --> 02:17:10.360
it eigenvectors psi and eigenvalues mu, so it forms the orthogonal basis for the columns of

02:17:10.360 --> 02:17:17.160
these matrix, right, for the dimension d of these matrix, and now in these two-dimensional Fourier

02:17:17.160 --> 02:17:23.880
basis, right, we can take tensor products of these basis functions of phi and psi, we can write the

02:17:23.880 --> 02:17:31.320
output of the neural network like this, right, and what you see here is that we have some filter

02:17:31.320 --> 02:17:36.600
that acts on our signal, right, so this is signal, so these are tensor products, right, so that's

02:17:36.600 --> 02:17:43.640
the analogy of two-dimensional Fourier transform, sinusoids of sine mx by sine ny, something like

02:17:43.640 --> 02:17:51.080
this, right, so that's our analogy of this, so this is a filter, and it works both with the

02:17:51.080 --> 02:17:57.880
frequencies, the eigenvalues of the graph Laplacian and the matrix w, right, lambda and mu, and you

02:17:57.880 --> 02:18:03.720
see that the low frequencies of the graphs are magnified by the positive eigenvalues of w,

02:18:03.720 --> 02:18:08.360
and conversely the high frequencies of the graph are magnified by the negative eigenvalues of w,

02:18:08.360 --> 02:18:14.520
and you can show that if we choose the matrix w in such a way that it has sufficiently negative

02:18:14.520 --> 02:18:19.880
eigenvalues, then this gradient flow dynamics is high frequency dominant in the sense that the

02:18:19.880 --> 02:18:24.520
Dirichlet energy or more correctly the normalized Dirichlet energy doesn't converge to zero,

02:18:24.520 --> 02:18:28.680
so it means that we don't have over smoothing, right, in the case of over smoothing this thing

02:18:28.680 --> 02:18:35.000
would be going to zero, but here it doesn't, right, so it means that we have some process that

02:18:35.000 --> 02:18:42.600
doesn't diffuse everything to a constant, it does something more interesting, right, and the

02:18:42.600 --> 02:18:48.360
condition for it is w having sufficiently large negative eigenvalues, so the analogy of this would

02:18:48.360 --> 02:18:54.200
be, so if you think of diffusion processes blurring, what we have here is sharpening,

02:18:54.200 --> 02:19:00.120
and we have both processes at the same time, so the attractive interactions do blurring,

02:19:00.120 --> 02:19:04.440
the repulsive interactions do sharpening, so we have some directions in the fissure space where

02:19:05.240 --> 02:19:12.680
these processes happen at the same time, okay, questions? Yep, I think there are many questions,

02:19:12.680 --> 02:19:19.080
so it was very unclear. So in these situations where you don't

02:19:20.360 --> 02:19:24.840
dissipate to a constant value, are you obtaining some sort of chaotic behavior,

02:19:24.840 --> 02:19:32.200
or are you obtaining other periodic oscillations of? So this is a sympathetic analysis, we don't

02:19:32.200 --> 02:19:36.040
know, I don't think that I have an answer to your question, it might be that it's,

02:19:36.360 --> 02:19:45.880
so whether we have monotonicity, that's the question, I don't think so, but it might be the case.

02:19:54.920 --> 02:20:00.680
So if eigenvalues of w are sufficiently negative, then we can just kind of beat over squashing,

02:20:01.480 --> 02:20:10.120
over smoothing, and the question is, is it possible, is it hard to do so because there are

02:20:10.120 --> 02:20:15.160
two dynamics, is it hard to get those dynamics right, does it require a lot of hyperparameter

02:20:15.160 --> 02:20:21.240
tuning or something like that? So it's a good question, so you can force, you can structure the

02:20:21.240 --> 02:20:27.160
matrix or parameterize the matrix w in such a way that it has two parts, positive eigenvalues and

02:20:27.160 --> 02:20:32.440
negative eigenvalues, and that's what we do, so basically we help the architecture to have both,

02:20:32.440 --> 02:20:37.880
and then learning becomes easy. To learn it completely from scratch might be difficult,

02:20:37.880 --> 02:20:43.960
and we see that GCNs which are in principle the same architectures without this restriction

02:20:43.960 --> 02:20:55.640
often fail to do it. My question went the same direction, but after the previous answer I got

02:20:56.600 --> 02:21:03.560
more confused. So those, the matrices which, what's it, the delta and the w, they are learned,

02:21:03.560 --> 02:21:09.480
right, they are not hyperparameters. W, the elements of the matrix w are learned, right,

02:21:09.480 --> 02:21:14.280
but the matrix w has constraints, right, so it's, for example, it's symmetric, right, so it has half

02:21:14.280 --> 02:21:20.520
of the elements of a general matrix w, and then basically by virtue of this theorem what it suggests

02:21:20.680 --> 02:21:25.880
is that we need to further restrict the structure of w, for example, to make it have negative

02:21:25.880 --> 02:21:33.160
eigenvalues. So typically what we do, we decompose it into a positive symmetric part and a negative

02:21:33.160 --> 02:21:40.040
symmetric part, and basically this way we guarantee that it has both positive and negative eigenvalues.

02:21:41.560 --> 02:21:48.680
Okay, so what is the principle component that makes graph work on heterophilic graphs better

02:21:48.680 --> 02:21:55.800
than GCNs, and can you somehow update GCNs in a way that they will also work on heterophilic graphs?

02:21:55.800 --> 02:21:59.560
Yeah, so residual connection is a must. We actually show that without residual connection this

02:22:00.200 --> 02:22:07.000
doesn't happen. Well, the nonlinear activation, I think it's more a complication for the analysis.

02:22:08.920 --> 02:22:15.800
We don't know how to analyze this, basically with this nonlinearity you cannot regard it as a

02:22:15.800 --> 02:22:24.440
diffusion equation. So, yeah, basically residual connection and then nonlinear eigenvalues.

02:22:24.440 --> 02:22:30.280
If you, and remove the nonlinearity, whether, if you don't constrain w to have non-negative,

02:22:30.280 --> 02:22:35.640
or if you don't help the architecture to learn w with negative eigenvalues, you might never be

02:22:35.640 --> 02:22:41.480
able to learn it. So we've seen this happening as well. Okay, and what is the role of symmetric

02:22:41.480 --> 02:22:46.520
matrices, symmetric matrices w? So, symmetric comes from the assumption of gradient flow.

02:22:48.520 --> 02:22:53.000
So, anything that looks like a gradient flow for this kind of energy must be symmetric.

02:22:53.960 --> 02:22:58.680
I see, and what does it mean in terms of operations on your graph, in terms of message passing?

02:22:59.320 --> 02:23:04.360
So, this is not related to message passing because w is channel mixing matrix attacks on the fissures.

02:23:04.440 --> 02:23:17.560
I was, I was wondering, what is the relationship? So, for example, you have this gradient flow

02:23:17.560 --> 02:23:22.440
and say you want to learn some energy with respect to which you flow. What's the relationship between

02:23:22.440 --> 02:23:28.120
learning the energy and learning a metric to which with you are computing the gradient? Because if

02:23:28.120 --> 02:23:33.080
you change the metric, then the way in which you flow is also different, right? Is other equivalent

02:23:33.080 --> 02:23:39.000
in the learning a metric equivalent to learning an energy? Yeah, well, you can think of it

02:23:39.000 --> 02:23:44.760
indeed in this way, right? So, what is the interpretation, physical interpretation of

02:23:46.200 --> 02:23:52.360
of this matrix w? So, if you look at the way that Dirichlet energy looks, right, it looks like,

02:23:52.360 --> 02:23:58.520
right? So, it's something like this. So, do we use, yeah, so let's say continuous version of Dirichlet

02:23:58.520 --> 02:24:03.640
energy. So, it will be the norm of the gradient of x at some coordinate u, let's call it. So,

02:24:03.640 --> 02:24:08.200
u would be the index of the null, right, the continuous version, right? And this is, this is

02:24:08.200 --> 02:24:15.000
the Dirichlet energy and let's write it on some domain omega, right? So, that would be our continuous

02:24:15.000 --> 02:24:22.920
version of Dirichlet energy. Now, what is written here when omega in general is a manifold,

02:24:22.920 --> 02:24:29.480
a remaining manifold. So, what is written here is the remaining metric of this kind, right?

02:24:31.640 --> 02:24:37.800
I hope you can see it. So, basically it's inner product defined at the position u, right? And

02:24:37.800 --> 02:24:41.480
the way that you can write it, so you can write it using remaining metric tensor, which is exactly

02:24:41.480 --> 02:24:49.480
the w, right? So, this is something that scales the coordinates of x, doesn't need to be fixed,

02:24:49.480 --> 02:24:55.320
by the way. This w in general can be position dependent, right, on the remaining manifold.

02:24:55.320 --> 02:25:01.640
So, a more general construction would allow w to depend on the position, maybe not explicitly,

02:25:01.640 --> 02:25:07.400
because that would be a huge number of parameters that scales with n, maybe it will be done some,

02:25:07.400 --> 02:25:12.200
through some form of attention. So, w will, or maybe a positional encoding, right? So,

02:25:12.200 --> 02:25:17.560
w will be a function of positional encoding of the nodes of the graph, right?

02:25:20.120 --> 02:25:25.160
So, there are interesting analogies and potential directions of extending this, using

02:25:26.920 --> 02:25:32.520
basically this is a harmonic energy of an embedding of some manifold, right? Thanks.

02:25:37.800 --> 02:25:41.800
Yeah, sorry, this will be probably a little bit like far-fetched question,

02:25:41.800 --> 02:25:47.960
but like in general, like those equations seem to be, seem to be like kind of similar to what you

02:25:47.960 --> 02:25:55.400
often get when you analyze like the signal propagation or the type of initializations

02:25:55.400 --> 02:26:01.880
in the neural networks, or like the dynamical isometry property. So, for example, the requirement for

02:26:01.880 --> 02:26:08.760
residual connections also appears in there. And like, are you aware like whether there's like

02:26:08.760 --> 02:26:15.480
any connections between, between, between this work and, yeah.

02:26:15.480 --> 02:26:21.320
Potentially. So, the closest analogy is neural ODE's, right? Well, these are neural, we like to

02:26:21.320 --> 02:26:28.200
call them neural PD's, but they're coupled ODE's, right? So, neural ODE's, each row of these magics

02:26:28.200 --> 02:26:34.040
will be separate, independent. Here, we also have the extra complexities coupling, right?

02:26:34.520 --> 02:26:41.720
So, like, would you say like this, there's something universal in all those?

02:26:43.720 --> 02:26:45.480
I'm not sure what do I mean by universal.

02:26:50.520 --> 02:26:54.680
Yeah, I'm not sure either, but like, it's basically like,

02:26:54.920 --> 02:27:05.320
like, for example, like this residual rule appears very often in different types of.

02:27:05.320 --> 02:27:10.920
So, residual rules rule here comes just from discretization. So, that's how you, you discretized

02:27:10.920 --> 02:27:15.080
the temporal derivative. You could discretize it differently. So, you can use a backward scheme,

02:27:15.080 --> 02:27:19.560
right? And then it would be implicit. So, you will need to solve a linear system to,

02:27:19.560 --> 02:27:23.240
to get your next iteration. This actually has been done with diffusion equations. So,

02:27:24.200 --> 02:27:28.120
there are advantages to it because these kind of discretizations are what's called

02:27:28.120 --> 02:27:34.040
unconditional stable. But in terms of, well, universality may be not the right term, but

02:27:34.040 --> 02:27:39.160
basically what you have here in practice is a kind of controlled differential equation,

02:27:39.160 --> 02:27:44.280
right? So, the control is through the privateers W. So, they're time independent, but in principle,

02:27:44.280 --> 02:27:49.400
you can think of time dependent trajectory. So, you have a controlled PDE that you discretize,

02:27:49.400 --> 02:27:53.880
and then expressive power becomes a question of, can I reach a certain state of the system,

02:27:53.880 --> 02:27:57.160
for example, in finite time by choosing the right trajectory? Or how far can I be

02:27:57.720 --> 02:28:03.480
from, from that state, right? So, universal approximation means that in finite time I can

02:28:03.480 --> 02:28:09.560
reach, I can be epsilon close to any state that they want. Generalization, for example, can be

02:28:09.560 --> 02:28:14.440
probably formulated as some kind of perturbation, right? So, I change my initial conditions. I want

02:28:14.440 --> 02:28:20.200
to see what happens to the system. And there are actually, there are some results, theoretical

02:28:20.200 --> 02:28:24.840
results that show that architectural choices, like for example, having symmetric matrices might be

02:28:24.840 --> 02:28:31.640
crucial for these properties. So, I think there is a lot more to explore there. Okay, thanks.

02:28:34.600 --> 02:28:39.080
Okay. So, more questions? Yeah. So, let's, let's move on with this stuff. So,

02:28:40.040 --> 02:28:45.480
obviously, right, so here, the conclusion was that we have no over smoothing, but we can also

02:28:45.480 --> 02:28:51.800
consider more interesting equations. So, so far we consider the very simple isotropic homogeneous

02:28:51.800 --> 02:28:56.600
diffusion equation. We can also consider nonlinear versions of the diffusion equation. And this one,

02:28:56.600 --> 02:29:02.760
in particular, comes from the domain of image processing, where imagine that you start with an

02:29:03.640 --> 02:29:10.760
image like this, right? So, the portrait of Sir Isaac Newton that is noisy. So, if you run a diffusion

02:29:10.760 --> 02:29:15.240
equation on an image, it actually has a closed form solution. So, it's convolution with the Gaussian

02:29:15.240 --> 02:29:20.200
kernel, where the variance of the Gaussian is proportional to the time of the diffusion, right?

02:29:20.200 --> 02:29:25.320
And in the limit, you will have just everything flat, right? So, you average all the pixels in

02:29:25.320 --> 02:29:31.400
the image. You see that you don't want to have results like this, because it might average out

02:29:31.400 --> 02:29:36.200
the noise, but it also destroys the discontinuities in the image that, for visual perception, are very

02:29:36.200 --> 02:29:45.640
important. So, the idea of, that was originally by Peron and Malik in 1990 is to have a nonlinear

02:29:45.640 --> 02:29:51.560
diffusion equation that is controlled by the gradient of the image, right? So, basically,

02:29:52.120 --> 02:29:56.840
if you're in a smooth region in the image, like here, so you have standard Gaussian kernel,

02:29:56.840 --> 02:30:01.880
but the moment you reach a discontinuity, you slow down the diffusion. So, the effect it has,

02:30:01.880 --> 02:30:07.560
you don't average pixels of different intensity. So, here, I'm not averaging dark and white, right?

02:30:08.760 --> 02:30:13.880
So, the kernel will look like this. It will look one-sided. And it had a lot of different versions,

02:30:13.880 --> 02:30:19.640
bilateral filters, non-local means filters and so on, but the idea is always the same. So,

02:30:19.640 --> 02:30:26.520
here, basically, the diffusion speed, right, is inversely proportional to the edge indicators,

02:30:26.520 --> 02:30:30.760
to the norm of the gradient. And the result that it produces is like this. So, this nonlinear

02:30:30.760 --> 02:30:37.000
diffusion equation knows where to stop locally, and therefore, it averages within smooth regions,

02:30:37.000 --> 02:30:41.560
and it doesn't average across regions. Now, we can do the same thing on a graph, obviously. So,

02:30:41.560 --> 02:30:46.600
the analogy would be this, right? So, here, we have a gradient, right? This is the divergence,

02:30:46.600 --> 02:30:51.720
and that's some parametric function that looks suspiciously like a tension, and that's the

02:30:51.800 --> 02:30:56.840
diffusivity, right? So, it's the local strength of the diffusion. And, in fact, if we discretize it

02:30:56.840 --> 02:31:04.200
again with explicit forward Euler scheme, then a particular version of this equation corresponds

02:31:04.200 --> 02:31:15.240
to the attentional architecture. So, this is a gut. But this was so far, this was a continuous

02:31:15.240 --> 02:31:20.680
time, right? And we wanted continuous space. So, the original motivation, right, when we compared

02:31:20.680 --> 02:31:26.680
graphs to other objects was somehow to have a continuous analogy of the graph neural networks,

02:31:26.680 --> 02:31:32.200
and if, again, we take a step back and look at how diffusion equations work in the plane,

02:31:33.320 --> 02:31:38.120
when I discretize the plane as a grid, I don't really have a canonical graph, right, in the sense

02:31:38.120 --> 02:31:43.320
that there are many ways I can discretize my differential approaches, right? So, this is how

02:31:43.320 --> 02:31:48.200
I can discretize the Laplace-Anon grid. So, I can use neighbors like this, or I can rotate everything

02:31:48.200 --> 02:31:54.920
by 45 degrees, I can use distant neighbors, I can use convex combination of all these operations,

02:31:54.920 --> 02:32:02.680
right, because this is a linear operator. So, bottom line on a grid, I don't have a canonical

02:32:02.680 --> 02:32:06.760
graph, right? I can actually use a discretization, maybe that is different at different points in

02:32:06.760 --> 02:32:11.720
the grid. Of course, there will be some numerical implications, but the discretization that we

02:32:11.720 --> 02:32:17.320
choose, right, and as a result, how the nodes are connected and which nodes propagate information to

02:32:17.320 --> 02:32:24.600
which nodes is, to a large extent, a numerical convenience, right? What makes sense, for example,

02:32:24.600 --> 02:32:29.720
from the organization of the memory or the number of nodes and whatever. So, we would like somehow

02:32:29.720 --> 02:32:35.000
to extend this mindset to general graphs. And for these purposes, instead of considering these

02:32:35.960 --> 02:32:40.680
nonlinear diffusion equations, like Perron and Balig, by the way, they called it an isotropic

02:32:40.680 --> 02:32:45.320
diffusion, which obviously, if you're familiar with PDEs, it's not an isotropic, it's not

02:32:45.320 --> 02:32:49.560
homogeneous, right, because we have a scalar diffusivity function and isotropic diffusion,

02:32:49.560 --> 02:32:56.520
we also have direction, so that would be a matrix or a tensor. So, instead of considering this

02:32:57.080 --> 02:33:01.880
nonlinear diffusion equation, we can consider a non-Euclidean diffusion equation. And the model

02:33:01.880 --> 02:33:06.520
here is the following, that was actually done by my PhD advisor, Ron Kimmel, also in the 90s,

02:33:07.480 --> 02:33:13.720
about 25 years ago, maybe even more. So, again, thinking of an image, you can think of it as

02:33:14.280 --> 02:33:19.160
an embedded two-dimensional manifold, right? And the embedding is in this joint space,

02:33:19.160 --> 02:33:23.880
where we have a combination of positional coordinates, the x, y coordinates of the pixels,

02:33:23.880 --> 02:33:28.440
and the fissure coordinates, in this case, for example, R, G and B channels. So, a color image

02:33:28.440 --> 02:33:34.280
is a two-dimensional surface in R5, right, using this model. Now, by virtue of this embedding,

02:33:34.280 --> 02:33:38.360
we can define a metric, so we can use the standard pullback mechanism, so in the case of

02:33:38.360 --> 02:33:43.880
two-dimensional manifold, it's a two-by-two matrix, given like this, right? And we can

02:33:43.880 --> 02:33:48.520
define a Laplacian with respect to this metric, it's a non-Euclidean analogy of the Laplacian,

02:33:48.520 --> 02:33:53.560
called the Laplace-Beltrami operator. And we can write a diffusion equation with respect to this

02:33:53.560 --> 02:33:59.080
operator, it's called the Beltrami flow, and we can actually show that it's a gradient flow of

02:34:00.120 --> 02:34:04.280
a generalization of the Dirichlet energy that is called the polycop functional. It's used in

02:34:04.360 --> 02:34:07.720
high-energy physics in bosonic strings, don't ask me what it is, but

02:34:09.720 --> 02:34:17.160
that's something is described by this energy. So, by analogy, we can do something like this

02:34:17.160 --> 02:34:22.920
on a graph, so now every node in the graph, in addition to having the fissure coordinates,

02:34:22.920 --> 02:34:27.640
also has some positional coordinates, right, so positional encoding. Ideally, this positional

02:34:27.640 --> 02:34:33.080
encoding should somehow represent the structure of the graph, right, in the sense that nearby points

02:34:33.080 --> 02:34:39.640
in this u-component of the space should be more likely connected by an edge, right? And the Beltrami

02:34:39.640 --> 02:34:45.880
flow, basically, it evolves, so we have again here a parametric diffusivity, it evolves both

02:34:45.880 --> 02:34:51.160
components, right, that I collectively denote by z, and the evolution of x-component is the

02:34:51.160 --> 02:34:55.720
standard fissure diffusion. You can think of the evolution of u as some form of soft graph

02:34:55.800 --> 02:35:03.240
rewiring, because what I can do, if two nodes become closer in this u-coordinate, I can decide to

02:35:03.240 --> 02:35:09.000
create an edge between them if there is no edge, or if the drift apart, I can decide to cut the

02:35:09.000 --> 02:35:14.280
edge, so overall, I will facilitate the propagation of information, and I know that it sounds cumbersome,

02:35:14.280 --> 02:35:17.960
but this is how it will look like, so again, this is the core graph, so it has,

02:35:19.560 --> 02:35:24.280
basically, there are three things happening here, so the positions of the circles, right, so circles

02:35:24.280 --> 02:35:30.120
are nodes, their positions represent some two-dimensional positional coordinates, the colors

02:35:30.120 --> 02:35:36.120
represent three-dimensional projection of the fissures, and you see that they're both components

02:35:36.120 --> 02:35:40.360
are evolving, and the graph is also changed on the fly, right, so when the clusters drift apart,

02:35:40.360 --> 02:35:46.120
then we cut the edges between them, so right, so it's fissure diffusion, positional coordinates

02:35:46.120 --> 02:35:51.560
are changing, and the graph is rewired, and you can see that the task here is node classification,

02:35:51.560 --> 02:35:56.040
so there are clearly seven classes of nodes that we can clearly distinguish here.

02:35:56.920 --> 02:36:00.440
Now, if you think of it from the standpoint of signal processing,

02:36:01.080 --> 02:36:04.840
we have a very disturbing picture here, right, so we have a filter that happens on the domain,

02:36:05.400 --> 02:36:11.640
yeah, a question? Maybe before you move to the next one, I was just wondering because I think

02:36:11.640 --> 02:36:16.280
what started to appear in this intellectual is that you apply some ideas from differential

02:36:16.280 --> 02:36:21.720
geometry to graphs, and maybe not yet directly, but... Not yet, right, so... Okay, so we are

02:36:21.720 --> 02:36:25.480
like talking about metrics and this sort of, so I wonder, maybe you'll be talking about it next,

02:36:25.480 --> 02:36:32.760
so sorry if I'm pushing it forward, but what do you think are the limits which come from the

02:36:32.760 --> 02:36:39.640
fact that graphs are basically discrete structures, like how free are we to apply ideas? Yeah, give me

02:36:39.640 --> 02:36:43.480
a few minutes and I will get there, yeah, so you can find the analogies, right, not everything has

02:36:43.480 --> 02:36:48.840
exactly a correspondence, right, so, but these analogies, I hope to show that they can be quite

02:36:48.840 --> 02:36:54.920
useful, right, so, but again, if you look at these pictures, so we have some kind of disturbing

02:36:54.920 --> 02:36:59.640
picture here, so we have a filter on the domain, and the domain is changing under our feet, right,

02:36:59.640 --> 02:37:05.240
so imagine that you're applying some filter, right, which is what it is, right, the diffusion

02:37:05.240 --> 02:37:11.160
equation, you can think of it as a form of filter, low pass filter, and the domain is moving, so I'm

02:37:11.160 --> 02:37:17.480
doing a filter and then nodes are somehow moving away from me, but this is a very common picture

02:37:17.480 --> 02:37:21.800
in differential geometry actually, and it's very common to take a manifold and evolve it under

02:37:21.800 --> 02:37:25.560
some evolution equation, and typically what, when you evolve a manifold, you're interested in what

02:37:25.560 --> 02:37:30.040
happens to the metric, right, so here's an example of an evolution equation that is called the Ricci

02:37:30.040 --> 02:37:37.800
flow, so you take the first order derivative, the temporal derivative of the metric tensor

02:37:37.800 --> 02:37:43.560
of the manifold, right, denoted here by g, and you make it equal to the Ricci curvature tensor,

02:37:43.560 --> 02:37:49.800
right, so basically the metric evolves proportionally to the to the local curvature, so it looks very

02:37:49.800 --> 02:37:54.360
much like the diffusion equation, so here we have temporal derivative, here we have some second order

02:37:54.360 --> 02:37:59.640
differential quantity that looks kind of like our Laplacian, right, so structurally it's similar

02:37:59.640 --> 02:38:04.520
to the diffusion equation, of course what it does is a very different thing, and if you start with

02:38:04.520 --> 02:38:10.360
this manifold, which has positive curvature on, so this kind of dumbbells on the spheres,

02:38:10.360 --> 02:38:14.920
it has positive curvature, right, and the neck between them, it has negative curvature, so if

02:38:14.920 --> 02:38:21.080
you run this diffusion, if you run the Ricci flow backwards in time, what will happen is that this

02:38:21.080 --> 02:38:25.320
dumbbell become more like an ellipsoid than more like a sphere, and then will collapse into a point,

02:38:25.320 --> 02:38:33.000
right, and it was introduced by Richard Hamilton in the 80s with the purpose of proving a famous

02:38:33.560 --> 02:38:40.920
conjecture in topology that claims that you can characterize spheres by your ability to take

02:38:40.920 --> 02:38:45.960
a closed curve and collapse it into a point, right, so this is how we characterize two-dimensional

02:38:45.960 --> 02:38:51.960
sphere, I can take any closed curve on a sphere and I can evolve it and collapse into a point,

02:38:51.960 --> 02:38:55.960
right, I cannot do it on a torus, so if I have a torus and I have a curve like this,

02:38:56.680 --> 02:39:01.560
then no matter what I do it cannot be collapsed to a point, so the conjecture was that you can,

02:39:02.440 --> 02:39:07.560
you can characterize higher-dimensional spheres in this way, and you obviously heard about it,

02:39:07.560 --> 02:39:13.400
the PoincarÃ© conjecture, and it was shown by Perlman, actually a slightly more general result,

02:39:14.760 --> 02:39:22.520
using the mechanism of Ricci flows, right, and that was a breakthrough of the century,

02:39:22.520 --> 02:39:27.480
it stood open for more than 100 years. Now, what does it have to do with our graphs

02:39:27.560 --> 02:39:32.760
and graph neural networks, so I remind you that we had this phenomenon, right, that message

02:39:32.760 --> 02:39:38.120
passing might not be, might not work well on some graphs, right, so there might be some,

02:39:38.120 --> 02:39:42.680
some phenomena that some graphs might be unfriendly for, for message passing, and in particular

02:39:42.680 --> 02:39:48.440
it depends both on the structure of the graph and the task, and if my task requires to propagate

02:39:48.440 --> 02:39:53.320
information from distant nodes, and the structure of the graph is such that the receptive field

02:39:53.320 --> 02:39:57.800
of the graph neural network grows exponentially fast, right, so the number of the neighbors of

02:39:57.800 --> 02:40:03.320
the neighbors of the neighbors becomes very large very quickly, this happens in trees,

02:40:03.320 --> 02:40:08.040
this happens in what is called small world graphs like social networks, then we have a problem,

02:40:08.040 --> 02:40:11.640
we have a lot of information that we need to squeeze into a single feature vector,

02:40:12.680 --> 02:40:20.280
and this is a phenomenon that we call overscorching, so let's, let's define mathematically what we

02:40:20.280 --> 02:40:24.600
mean by overscorching, so let's say that we have a message passing architecture of this form,

02:40:24.600 --> 02:40:31.080
right, so we have the node itself at layer k, and we have the neighbors, we combine them with some

02:40:31.080 --> 02:40:36.120
learnable weights, let's call them w1 and w2, let's say that the depth of this architecture is l,

02:40:37.240 --> 02:40:43.240
the width, right, so the internal dimension is p, the long linearities are well behaved,

02:40:43.240 --> 02:40:48.040
right, so the elliptics continues, and we also have some bound on the, on the weights,

02:40:48.120 --> 02:40:54.040
so this is what characterizes our architecture, so what is overscorching, it's some form of insensitivity,

02:40:54.040 --> 02:41:01.960
right, so if I look at the output of the neural network at node i and I examine how it depends

02:41:01.960 --> 02:41:08.520
on the input at some distant node j, I can describe the sensitivity of the output to the input

02:41:09.640 --> 02:41:14.840
through this Jacobian, right, so through the partial derivative, and if the partial derivative

02:41:14.840 --> 02:41:21.800
is small it means that the information propagates badly from input to output, right, so basically

02:41:21.800 --> 02:41:28.520
I will not perceive the change in the input in the output of that node, and what we show in the

02:41:28.520 --> 02:41:34.120
paper is that we can bound the Jacobian by constants that depend on the model, right, for

02:41:34.120 --> 02:41:38.920
example the number of flares, the regularity of the activation functions, the bound on the weights,

02:41:38.920 --> 02:41:43.480
and also something that depends on the graph topology, right, and we show in particular,

02:41:43.560 --> 02:41:47.160
for example, that width does help, of course, at the usual expense of

02:41:50.280 --> 02:41:56.920
worst generalization overfitting, depth doesn't help, for example, and the topology has the

02:41:56.920 --> 02:42:02.120
really the largest effect, and intuitively we expect that in some kind of benign graphs,

02:42:02.120 --> 02:42:06.520
like grids, for example, we will not have overscorching, and in pathological examples,

02:42:06.520 --> 02:42:12.040
like trees, that would be probably the worst case, right, so you see that the topology of

02:42:12.040 --> 02:42:18.920
the graph comes here through the power of the adjacency matrix, but we don't see exactly how,

02:42:18.920 --> 02:42:25.960
right, so it's hard to say, right, so what does it mean a matrix to the power l, so we need something

02:42:25.960 --> 02:42:31.320
more nuanced, we need some kind of geometric analysis that will allow us to tell apart structures

02:42:31.320 --> 02:42:34.440
like this and structures like this, right, something that locally looks like a grid or

02:42:34.440 --> 02:42:39.000
something that looks like a tree, that's exactly what curvature is designed for, right, so I remind

02:42:39.080 --> 02:42:44.520
you that in differential geometry, what curvature tells you is that if you take nearby points and

02:42:44.520 --> 02:42:50.680
shoot geodesics in parallel at the same speed, you can either converge, remain parallel, or diverge,

02:42:50.680 --> 02:42:55.640
right, and we call these spherical, euclidean, and hyperbolic geometry, right, so locally it looks

02:42:55.640 --> 02:43:01.720
like a sphere, like a plane, or like a hyperboloid, or high-dimensional cases as well, so on a graph,

02:43:01.720 --> 02:43:06.440
the analogy could look like this, so there are several definitions of reach-type curvature on

02:43:06.440 --> 02:43:12.840
graphs, so this is a combinatorial definition that we use here, so you can take nodes that are

02:43:12.840 --> 02:43:18.120
connected by an edge, let's call them p and q, and look at edges that emanate from these nodes,

02:43:18.120 --> 02:43:24.680
so if they tend to form triangles, it means that we look at something like a click, if they form

02:43:24.680 --> 02:43:29.960
rectangles, they will look at something like a grid, and if they drift apart and don't form anything,

02:43:29.960 --> 02:43:35.080
then we look at locally at something that looks like a tree, right, and basically we can count

02:43:35.160 --> 02:43:40.040
different types of rectangles and triangles, allow me to skip the details, basically for every

02:43:40.040 --> 02:43:43.960
edge in the graph we can have this combinatorial quantity that we call the balanced formant

02:43:43.960 --> 02:43:50.040
curvature, that counts, basically it looks at a two-hop neighborhood of an edge, and it counts

02:43:50.040 --> 02:43:55.320
certain types of rectangles and triangles that surround this edge, bottom line, each reproduces

02:43:55.320 --> 02:44:00.200
the continuous behavior, so clicks are positively curved, grids have zero curvature, and trees are

02:44:00.200 --> 02:44:06.200
negatively curved, right, so that's I think to your previous question how what is the

02:44:07.080 --> 02:44:11.000
parallel between differential geometry and graphs, so this is an analogy of a curvature,

02:44:11.000 --> 02:44:15.320
so it's not a discretization of a curvature, it's a discrete curvature that behaves in a

02:44:15.320 --> 02:44:21.000
similar way, and now the relation between the over-squashing and the curvature, what we show

02:44:21.000 --> 02:44:27.160
is that if we have strongly negatively curved edges in the in the graph, then we can write down

02:44:27.240 --> 02:44:33.080
this bound on the Jacobian, and it means that the over-squashing is caused by

02:44:34.200 --> 02:44:36.600
the presence of strongly negatively curved edges, yeah.

02:44:39.880 --> 02:44:49.880
Yeah, so it's the number of triangles that surround an edge, and this is the number of

02:44:49.880 --> 02:44:55.960
rectangles, this is the degree, yeah, doesn't really matter, there are several definitions,

02:44:55.960 --> 02:45:00.600
so why we call it balanced form and curvature, because there is a classical notion of form

02:45:00.600 --> 02:45:06.840
and curvature that looks a little bit like this, we just touch it a little bit so it behaves like

02:45:06.840 --> 02:45:13.480
what is shown here. This really relates to the rigid curvature tensor and the formula from

02:45:13.480 --> 02:45:18.280
matrix, I mean I don't see it now, but maybe. It's a graph, so you don't have exactly the same thing.

02:45:18.280 --> 02:45:24.200
Yeah, obviously, but some I don't know components, can we do some analogy or not, this is something

02:45:24.200 --> 02:45:31.560
completely new. So it shows how, so you can think of curvature as how locally the volume changes,

02:45:31.560 --> 02:45:36.200
so in a sense it shows how the volume changes, so there are two classical definitions of

02:45:36.200 --> 02:45:39.960
curvature on graphs, one through optimal transport that is called the Olivia curvature,

02:45:39.960 --> 02:45:43.320
and this is combinatorial version that is called the the form and curvature.

02:45:46.680 --> 02:45:51.480
Right, so basically the conclusion, well I should make it explicit that it's strongly

02:45:51.480 --> 02:45:56.120
negatively curved edges that cause overscorching, right, so basically due to this bound, actually

02:45:56.120 --> 02:46:01.480
the presence of negative or slightly negative curvature might be benign, this is what is shown

02:46:02.120 --> 02:46:07.960
in the the expander's paper by Petr Wieliczkiewicz and his and his causes, so these are somehow the

02:46:07.960 --> 02:46:14.280
optimal graphs, the best for message passing, but expander's needs to be slightly negatively curved.

02:46:14.280 --> 02:46:21.480
So once we know it, we can actually interfere basically, we can surgically remove the

02:46:22.120 --> 02:46:27.080
negatively curved edges and replace them potentially with edges with higher, with more

02:46:27.080 --> 02:46:33.560
positive curvature, and this way we retouch the graph a little bit and we show that it improves

02:46:33.560 --> 02:46:37.080
the performance of graph neural networks both in homophilic and heterophilic settings.

02:46:37.640 --> 02:46:43.560
So there was a question about diffusion-based rewiring before, and I promised to tell exactly

02:46:43.560 --> 02:46:52.280
what I mean by this, so this is the paper that is called DIGL by Stefan GÃ¼nemann and his students

02:46:52.280 --> 02:46:56.200
from the Technical University of Munich, and DIGL stands for Diffusion Improves Graph Learning.

02:46:56.760 --> 02:47:02.920
So the idea there is that you rewire the graph by basically by computing page rank and embeddings

02:47:02.920 --> 02:47:08.520
for a personalized page rank embedding for every node, and you connect then in this new embedding

02:47:08.520 --> 02:47:15.640
space the nodes that are closest. So what it does essentially, it introduces connections within the

02:47:15.640 --> 02:47:21.320
same connected component in the graph right, or within the same clique or cluster in the graph,

02:47:21.320 --> 02:47:27.800
and it has a hard time to connect across different communities in the graph.

02:47:28.680 --> 02:47:32.840
So when the graph is homophilic, this is a very good thing to do, right, so you're connecting to

02:47:33.720 --> 02:47:38.200
to similar nodes, but if the graph is heterophilic, it can do more harm than help,

02:47:38.200 --> 02:47:43.160
and in fact experiments show that this is the case, they also write it in the paper,

02:47:43.160 --> 02:47:46.200
the curvature-based approach, first of all it changes the graph minimally,

02:47:47.320 --> 02:47:50.920
here the change can be dramatic, right, so that's the number of edges that are changed,

02:47:51.800 --> 02:47:56.600
but it also helps in the heterophilic cases because it's not restricted by this property,

02:47:56.600 --> 02:48:01.240
you actually typically bridge different communities by the new edges that are created.

02:48:03.160 --> 02:48:07.480
So still talking about diffusion, am I actually out of time?

02:48:09.960 --> 02:48:17.400
Okay, yeah, so still talking about about diffusion equations, here are some more exotic

02:48:18.600 --> 02:48:26.600
exotic stuff, right, and this is our maybe creative way to illustrate to illustrate sheaves

02:48:27.560 --> 02:48:34.840
or bundles, so there has recently been probably a better picture, so that's really sheaves,

02:48:34.840 --> 02:48:40.280
right, in the literal sense, so what are sheaves? So they actually have very interesting history,

02:48:40.280 --> 02:48:46.520
and well I like these kind of historical factoids, so the theory of sheaves in algebraic topology

02:48:46.520 --> 02:48:52.040
was introduced by Jean Lyret, so he was a French mathematician, he was also an officer in the

02:48:52.040 --> 02:48:58.920
French army, and when the Nazis invaded France he was captured and put with his comrades into

02:48:59.480 --> 02:49:05.320
concentration camp, and basically he was asked to work on mathematics, and his expertise was

02:49:06.600 --> 02:49:11.160
mechanics, so he was very afraid that he would be forced to work on something that would be useful

02:49:11.160 --> 02:49:17.560
for the Nazis, and basically he will be committing treason, helping the war effort, so when he was

02:49:17.560 --> 02:49:25.080
offered the possibility to teach something in this camp, he chose a very innocuous topic,

02:49:25.080 --> 02:49:31.240
algebraic topology, which could be useful, and then after, well of course they were released

02:49:31.240 --> 02:49:38.040
after the war ended, he published it in a course that was taught in captivity, and one of the

02:49:38.040 --> 02:49:45.240
papers that came out of this course introduced the theory of sheaves, so sheaves are objects that are

02:49:45.240 --> 02:49:52.120
taught in algebraic topology, so if we apply them to our setting to graph, and this is slightly

02:49:52.120 --> 02:49:57.400
different construction that is called cellar sheaves, so if you think of graphs by analogy to

02:49:57.400 --> 02:50:02.440
manifold, so a manifold is a topological space, what I mean by topological space roughly is that

02:50:02.440 --> 02:50:09.640
you have a notion of neighborhood, I can tell who my neighbors are, but I don't have the notion of

02:50:09.720 --> 02:50:17.320
distances or angles, so if I want to talk about distances or angles, I need some extra machinery,

02:50:17.320 --> 02:50:21.320
and on many folds this is typically achieved by what is called an affine connection,

02:50:22.520 --> 02:50:28.200
or parallel transport, so it's a mechanism that tells me how to move vectors between

02:50:28.200 --> 02:50:34.280
tangent spaces at nearby points, I can also define a remaining metric if I want to equip

02:50:34.280 --> 02:50:38.360
a manifold with geometry, and then there is a special type of connection that is called

02:50:38.360 --> 02:50:43.480
the Levy-Civita connection that is compatible with the metric, so you can think of the same thing

02:50:43.480 --> 02:50:49.320
on a graph, so a graph is a purely topological object, I have a notion of who my neighbors are,

02:50:49.320 --> 02:50:55.000
but I don't have any geometry, so in order to introduce geometry I can equip every node and

02:50:55.000 --> 02:51:01.320
every edge with a vector space, and I can define by analogy to parallel transport, I can define

02:51:01.320 --> 02:51:06.520
linear maps, so these are called restriction maps that go between these spaces, so slightly

02:51:06.520 --> 02:51:12.040
different from many folds, I go from the space associated with nodes to a space associated

02:51:12.040 --> 02:51:16.920
with an edge, and then if I want to transport information from a node to a node, I need to

02:51:16.920 --> 02:51:24.920
combine these two maps, one with transpose, so basically it's a kind of, so I invent

02:51:26.520 --> 02:51:32.440
geometry on a graph, so I lift it into a more complicated object, and on this object I can

02:51:32.440 --> 02:51:37.560
now study, for example, what happens if I choose these restriction maps to be of certain class,

02:51:37.560 --> 02:51:42.280
so these are matrices of certain dimension, and I can choose them, for example, to be

02:51:42.280 --> 02:51:45.960
symmetric, or I want them to be orthogonal, or I want them to be something else, right,

02:51:45.960 --> 02:51:52.520
I can also choose the dimension of these, of these, what is called stocks, right, so these spaces,

02:51:53.880 --> 02:51:59.400
and I can define differential operators on this structure, right, so the difference between

02:51:59.400 --> 02:52:05.560
the standard, for example, gradient and the shift gradient would be the same way as we have a

02:52:05.560 --> 02:52:11.080
manifold, I cannot add or subtract two points on the manifold, so when I need to subtract two

02:52:11.080 --> 02:52:15.800
vectors, I first need to apply parallel transport, so I need to bring the vector from its vector

02:52:15.800 --> 02:52:20.600
space to my vector space, to my tangent space, and by doing this typically I would apply some

02:52:20.600 --> 02:52:25.160
form of rotation, if it's a manifold with the remaining metric, and only then I can subtract

02:52:25.160 --> 02:52:29.640
them, right, so here the same, if before the gradient looked like just difference between

02:52:29.640 --> 02:52:35.080
n points of an edge, here we'll have also some linear transformation that sits in between,

02:52:35.080 --> 02:52:40.040
right, long story short, I can, basically I'm interested in a Laplacian, right, so I have a

02:52:40.040 --> 02:52:46.920
shift version of the Laplacian, so it's a block matrix where every block transforms the vectors

02:52:46.920 --> 02:52:53.640
with these kind of, with these kind of matrices, right, the combination, and now I can apply this

02:52:53.720 --> 02:52:59.960
operator on my data and run a diffusion equation, and I can run it to infinity with some additional

02:52:59.960 --> 02:53:05.080
conditions, and I can ask questions like how many classes can I separate if I choose this

02:53:05.080 --> 02:53:10.760
shift in a certain way, yeah. Are you doing graph rewiring here? No, we are not doing any

02:53:10.760 --> 02:53:14.520
graph rewiring, so the graph structure is encoded in the structure of the Laplacian,

02:53:16.280 --> 02:53:23.080
right, so basically it's a kind of question about expressive power of this architecture,

02:53:23.080 --> 02:53:29.560
so I can ask how many classes I can separate, right, so expressive power, slightly different,

02:53:29.560 --> 02:53:33.880
different version from the, from the Weisferre and Lehmann, because in Weisferre and Lehmann we

02:53:33.880 --> 02:53:39.720
asked about the, how many, the types of graphs that we can, if we can distinguish, here we're

02:53:39.720 --> 02:53:48.760
looking at node level problems, yeah. Can you please make some like use cases for this kind of

02:53:49.720 --> 02:53:56.040
of graph, and the ones that were shown before in which the nodes were actually separated with

02:53:56.040 --> 02:54:05.080
no connections, one to another? So, the graph here is given, so it's, I think some, I don't remember

02:54:05.080 --> 02:54:14.120
what data set it is, yeah, sorry, what is again the question? So, like, what kind of, what are

02:54:14.120 --> 02:54:21.800
you trying to model and why is this configuration? Oh, so, yeah, the colors represent the classes,

02:54:21.800 --> 02:54:27.400
the ground rules classes, and the positions represent the features. And what's the difference

02:54:27.400 --> 02:54:33.640
between this type and the one in which you do rewiring and each class series separated,

02:54:33.640 --> 02:54:37.720
one to another? So, here the features are represented by coordinates, so the closest

02:54:37.720 --> 02:54:42.680
analogy of this illustration is to the one that I showed with the gradient flow, right,

02:54:42.680 --> 02:54:47.880
so the coordinates here represent the features, not the positional coordinates, right, and the

02:54:47.880 --> 02:54:54.040
colors represent the classes, okay. So, there is no rewiring happening here, you can also potentially

02:54:54.040 --> 02:54:59.880
use rewiring. And the results, well, I don't want to go through all the results, but basically what

02:54:59.880 --> 02:55:05.480
we show is that by using different types of sheaves, we can guarantee that we can separate

02:55:05.480 --> 02:55:11.800
different types of, different number of node classes, depending whether the graph is homophilic

02:55:11.800 --> 02:55:16.600
or heterophilic. For example, we show that you must have non-symmetric relations if you want to

02:55:16.600 --> 02:55:25.400
deal with heterophilic graphs, yeah. So, in previous method, WL, it's something like, I think the

02:55:25.400 --> 02:55:32.200
classification was on top of number of edge, if I get it correctly, like if you both go to the

02:55:32.200 --> 02:55:39.560
higher classes, then the number of edges also increases, but here in sheaves, it's, it's,

02:55:39.560 --> 02:55:46.200
is it about the dimension that we have, like, can we have a first dimension sheaves or something

02:55:46.200 --> 02:55:52.760
like this? So, Weisfer and Lehmann is different, so the hierarchy there is, basically, they are

02:55:52.760 --> 02:55:58.360
different algorithms, right. So, whether they do color refinement for different structures for,

02:55:58.840 --> 02:56:06.440
for a node, for a pair of nodes, for triplets of nodes, and so on. So, here we have, the choice is

02:56:06.440 --> 02:56:10.760
what kind of matrices we allow, what class of matrices we allow, so it's typically a group,

02:56:10.760 --> 02:56:16.120
right, so we say, for example, the, the most general case is, is GL, right, so any invertible

02:56:16.120 --> 02:56:21.160
matrix, then we can restrict it to be, for example, symmetric matrix, or then we can restrict it to

02:56:21.160 --> 02:56:26.360
be orthogonal matrix, right, and based on these choices, plus the dimension of the sheave, we get

02:56:26.360 --> 02:56:34.760
different results. So, it has to take dimension and also the matrix. Exactly. Thank you. So,

02:56:34.760 --> 02:56:38.520
this is a more theoretical question, right, because it's a good question how we actually,

02:56:38.520 --> 02:56:42.520
how we learn the sheave from the data, but assuming that we knew the sheave, right,

02:56:42.520 --> 02:56:47.320
but we allow the sheave to be only from of a certain type, what is in the best case,

02:56:48.200 --> 02:56:53.080
how many node classes we could separate, under different assumptions also about

02:56:53.080 --> 02:56:58.360
the structure of the graph, whether it's homophilic or heterophilic. But, basically, the, the, the,

02:56:58.360 --> 02:57:04.920
the bottom line of this story is that diffusion, when you have, when you have these extra degrees

02:57:04.920 --> 02:57:09.880
of freedom, looks more interesting than, than, than the standard diffusion on a graph. So,

02:57:09.880 --> 02:57:15.400
the standard diffusion on a graph corresponds to symmetric restrictions with one dimensional sheave.

02:57:15.400 --> 02:57:25.240
Yep.

02:57:45.400 --> 02:57:48.600
well so it's slightly more complicated right so the analogy of the

02:57:48.600 --> 02:57:52.040
connection would be the composition of two maps right so what we call a

02:57:52.040 --> 02:57:55.120
transport map so each of these f's is called the restriction map so it goes

02:57:55.120 --> 02:57:59.140
from node to edge space they actually can have different dimensions so they

02:57:59.140 --> 02:58:04.720
don't need to be the same the composition right so f f transpose is a map

02:58:04.720 --> 02:58:09.240
from the space of one node to the space of another node so that's the analogy of

02:58:09.240 --> 02:58:12.640
parallel transport right so I'm when I move a vector from one node to another

02:58:13.600 --> 02:58:18.040
geometrically transform it somehow rotated for example or scale it depends

02:58:18.040 --> 02:58:27.640
on the class of matrix that I use here so in exactly so but then of course in

02:58:27.640 --> 02:58:32.120
practice you need somehow to parametrize it right so you cannot of course in

02:58:32.120 --> 02:58:37.080
principle you can say that that let me learn individual f's for every for every

02:58:37.080 --> 02:58:41.160
age of the of the graph but it's not practically feasible so in practice f is

02:58:41.160 --> 02:58:45.600
a matrix valued function that depends on the node features so it's a little

02:58:45.600 --> 02:58:50.440
bit similar to attention but the tension is scholar here it's matrix so it's a

02:58:50.440 --> 02:59:00.280
geometric operation okay any questions

02:59:03.120 --> 02:59:10.380
right so basically to summarize this this part so what do we gain from this

02:59:10.420 --> 02:59:17.220
physics inspired perspective on graph neural networks so first of all I think

02:59:17.220 --> 02:59:21.020
it's different viewpoint on old problems like over smoothing bottlenecks it

02:59:21.020 --> 02:59:25.380
allows to on the one hand to interpret existing architectures like guts for

02:59:25.380 --> 02:59:29.940
example from a different perspective it allows to potentially design your

02:59:29.940 --> 02:59:34.380
architectures right for example using if you think of a generous discretization

02:59:34.380 --> 02:59:38.980
of differential equations then of course you can ask what kind of solver can I

02:59:38.980 --> 02:59:43.180
use can I use some some more interesting things with adaptive step size or maybe

02:59:43.180 --> 02:59:48.380
I don't know multi grid solvers and so on it allows to make principle

02:59:48.380 --> 02:59:53.420
architectural choices right like example with gradient flows so basically from

02:59:53.420 --> 02:59:58.660
the gradient flow we get restriction on symmetric weights we get residual

02:59:58.660 --> 03:00:02.140
connection we can also have some theoretical guarantees right again like

03:00:02.140 --> 03:00:04.780
we've seen with the gradient flow but maybe also of other types like

03:00:05.100 --> 03:00:09.940
convergence stability and so on and so forth but probably more interesting are

03:00:09.940 --> 03:00:14.500
links to other fields that are less explored in graph neural network literature

03:00:14.500 --> 03:00:18.580
like in particular differential geometry or algebraic topology and of course

03:00:18.580 --> 03:00:22.540
diffusion is just one example of evolution equations you can consider more

03:00:22.540 --> 03:00:25.540
interesting things so this is one example right so probably have seen these

03:00:25.540 --> 03:00:31.300
kind of things coupled oscillators so the metronomes that are put on a table and

03:00:31.340 --> 03:00:37.660
because the vibrations transfer from one to another initially they might be

03:00:37.660 --> 03:00:41.260
oscillating out of phase and then they become synchronized so think of

03:00:41.260 --> 03:00:44.820
something like this but on a graph so the coupling occurs on a graph in a

03:00:44.820 --> 03:00:48.300
learnable way and depending on the tasks that we want to do we have we want

03:00:48.300 --> 03:00:52.980
somehow to to interact between these different oscillators so it's also a

03:00:52.980 --> 03:00:57.460
differential equation but it also has a second order kinetic term so unlike a

03:00:57.540 --> 03:01:02.820
diffusion equation it has also a surgery component and here we show for

03:01:02.820 --> 03:01:07.500
example that we can probably avoid over smoothing by using this type of

03:01:07.500 --> 03:01:19.580
equations how much time do I have 15 minutes okay so what I would like to do

03:01:19.580 --> 03:01:25.900
if in 15 minutes let's talk about grids so I definitely ran out of time because

03:01:25.940 --> 03:01:29.420
out of all the geometric objects at the end well we spent all the time on

03:01:29.420 --> 03:01:35.740
crafts I think grids also deserve a little bit and probably well everybody

03:01:35.740 --> 03:01:39.660
is familiar with grids right so let's look at them maybe again from the

03:01:39.660 --> 03:01:44.300
perspective of geometric deep learning and hopefully some new intuition or at

03:01:44.300 --> 03:01:49.740
least for some of you have not seen it before so and this also relates to the

03:01:49.740 --> 03:01:55.740
previous question of why we call graph convolutional networks convolutional so

03:01:55.780 --> 03:02:02.220
first of all a grid is a graph right so it's a particular type of a graph for

03:02:02.220 --> 03:02:06.060
simplicity I would like to assume that the grid has periodic boundary conditions

03:02:06.060 --> 03:02:11.700
so basically it's what is called the ring graph and the idea of geometric deep

03:02:11.700 --> 03:02:16.980
learning right this group based framework we had some domain and we have a

03:02:16.980 --> 03:02:21.780
group that acted on the domain we have a signal that was defined on the domain so

03:02:21.980 --> 03:02:27.500
this is a general type of this mechanism that is often called lifting so now I

03:02:27.500 --> 03:02:30.860
have a linear operator so this can be anything right so this can be a non

03:02:30.860 --> 03:02:34.660
linear complicated thing here I have a linear operator so the group

03:02:34.660 --> 03:02:39.500
representation attacks on functions defined on the domain okay and in the

03:02:39.500 --> 03:02:43.100
case of a grid this is just the shift operator so this is what they show in

03:02:43.100 --> 03:02:51.140
one dimension so just cyclically moves the elements of the vector

03:02:52.100 --> 03:02:55.780
now another thing that you see in a grid is that it has a fixed neighborhood

03:02:55.780 --> 03:02:59.300
structure right in this example every node is connected to exactly two

03:02:59.300 --> 03:03:03.020
neighbors and they are also ordered right so I always have the one before and

03:03:03.020 --> 03:03:07.140
the one after in a two-dimensional grid I might have some partial order right so

03:03:07.140 --> 03:03:14.060
I have something on top and something on the bottom so in the past on the

03:03:14.060 --> 03:03:17.260
general graph we had this kind of aggregation function right so we have the

03:03:17.580 --> 03:03:23.380
feature of the node itself and then we had a multi-set that was unordered of the

03:03:23.380 --> 03:03:28.180
nearby features and because we didn't have any order in this multi-set we the

03:03:28.180 --> 03:03:31.380
only thing we could do is to apply a symmetric function apply a permutation

03:03:31.380 --> 03:03:35.340
invariant function now we have a different situation now the nodes are

03:03:35.340 --> 03:03:42.820
ordered right so we have a fixed order of x i minus one x i and x i plus one so

03:03:42.860 --> 03:03:51.060
this function can be more general right so it doesn't need to be doesn't need to

03:03:51.060 --> 03:04:01.100
be to be symmetric and if this function is linear then we get the convolution

03:04:01.100 --> 03:04:05.820
right and if I write it as a matrix vector product then it looks like this

03:04:05.820 --> 03:04:12.460
so it's a special matrix which has fixed elements along the diagonal right so

03:04:12.500 --> 03:04:16.940
this is the the the local weight-sharing that have been convolutional neural

03:04:16.940 --> 03:04:20.940
networks so this is a special type of matrices they're called circumvent

03:04:20.940 --> 03:04:25.900
matrices right or convolutions so it's synonym you take a vector of

03:04:25.900 --> 03:04:30.940
parameters right let's call it data and you create these matrix by cyclically

03:04:30.940 --> 03:04:34.660
shifting by one position these vector of parameters and depending it as columns

03:04:34.660 --> 03:04:38.780
that's how you get you get these matrix again I'm assuming periodic boundary

03:04:38.780 --> 03:04:42.180
conditions so technically speaking it's not a convergence it's a circle

03:04:42.220 --> 03:04:49.660
convolution or a cyclic convolution but just to make things simpler okay now one

03:04:49.660 --> 03:04:54.060
thing that you first thing that you learn in in algebra one-on-one is that

03:04:54.060 --> 03:04:59.580
matrix multiplication is not commutative right a b is not equal to b a but with

03:04:59.580 --> 03:05:03.900
these matrices with convolutions there are with circumvent matrices that's not

03:05:03.900 --> 03:05:09.260
the case it's actually a special type of matrices that do commute right and in

03:05:09.260 --> 03:05:15.060
particular they commute with one of them which is the shift operator right so a

03:05:15.060 --> 03:05:19.900
shift is also a circumvent matrix right or also a convolution right so looks

03:05:19.900 --> 03:05:26.780
like this so what does it mean that that a convolution commutes with a shift so

03:05:26.780 --> 03:05:30.140
this is what we call shift-equivariance right so in other words I can first

03:05:30.140 --> 03:05:33.180
apply convolution and then shift or I can first apply shift and then

03:05:33.180 --> 03:05:38.700
convolution the result will be the same right so convolution is shift-equivariant

03:05:38.700 --> 03:05:44.220
you can show the other way around right so you can show that if you have shift

03:05:44.220 --> 03:05:49.140
equivariant linear operations so I take a matrix and I tell you that it's

03:05:49.140 --> 03:05:54.420
shift-equivariant you can show that it must be a convolution right so basically

03:05:54.420 --> 03:05:59.540
convolution emerges from considerations of translational symmetry right so the

03:05:59.540 --> 03:06:04.260
only linear operation that that is shift-equivariant is convolution so

03:06:04.260 --> 03:06:10.940
convolution is the only thing that satisfies this property and we've seen

03:06:10.940 --> 03:06:15.260
again this geometric deep learning blueprint so allow me to show it again

03:06:15.260 --> 03:06:19.980
so we have a grid we have a translation group its representation is the shift

03:06:19.980 --> 03:06:25.060
operator so the convolution is a function that is

03:06:25.060 --> 03:06:31.860
equivariant with respect to this to this group now we also know that there

03:06:31.860 --> 03:06:35.340
is an intimate relation between the Fourier transform and the convolution

03:06:35.340 --> 03:06:39.620
right and let's actually try to understand what the Fourier transform is

03:06:39.620 --> 03:06:44.340
where it comes from so we know from algebra again that commuting matrices

03:06:44.340 --> 03:06:48.140
are jointly diagonalizable it means that they have the same eigenvectors or more

03:06:48.140 --> 03:06:52.540
correct eigenspaces but here we assume that the multiplicity of eigenvalues is

03:06:52.540 --> 03:06:57.180
trivial so they actually have the same eigenvectors and the only different

03:06:57.180 --> 03:07:00.780
different eigenvalues right so all commutative matrices satisfy this property

03:07:00.780 --> 03:07:06.740
so if I have a set of matrices that commute pair wisely then then this is

03:07:06.740 --> 03:07:11.020
the case right and this is the case for for convolutions or for circuit matrices

03:07:11.020 --> 03:07:18.460
so we can pick up one of these matrices right and compute its eigenvectors

03:07:18.460 --> 03:07:23.500
right and we know that all of them will have the same and it's convenient to

03:07:23.500 --> 03:07:30.300
look at the shift operator right at the matrix S and if we compute the eigen

03:07:30.300 --> 03:07:33.500
vectors of the shift operator you can do it by hand it's actually not difficult

03:07:33.500 --> 03:07:38.220
you see that they look like these complex exponentials so this is exactly the

03:07:38.220 --> 03:07:46.860
Fourier transform or more correctly the discrete Fourier transform so the

03:07:46.860 --> 03:07:50.980
question of course that remains is what the eigenvalues are right so we know

03:07:50.980 --> 03:07:54.020
that the eigenvectors of all conversions are the discrete Fourier

03:07:54.020 --> 03:07:59.660
transform so these complex sinusoids but the eigenvalues also you can show it

03:07:59.660 --> 03:08:03.900
are the Fourier transform of the vector theta that forms each of these matrices

03:08:03.900 --> 03:08:08.740
and this gives us this dual relationship between the Fourier transform and the

03:08:08.740 --> 03:08:13.060
convolution so if I have a signal x I can do convolution in the spatial domain

03:08:13.060 --> 03:08:17.780
by multiplying by a circuit matrix or I can do it in the in the Fourier domain

03:08:17.780 --> 03:08:22.020
I can compute the Fourier transform and there the Fourier transform diagonalizes

03:08:22.020 --> 03:08:26.980
the convolution so it becomes an element wise product right so basically the

03:08:26.980 --> 03:08:31.180
product of two Fourier transforms is the Fourier transform of the convolution

03:08:31.180 --> 03:08:37.260
right and typically in signal processing the filters are already designed in the

03:08:37.260 --> 03:08:42.180
Fourier domain this is bread and butter of signal processing so the the the

03:08:42.180 --> 03:08:46.660
advantage of using the Fourier transform because this operation usually on

03:08:46.660 --> 03:08:51.820
grids can be done efficiently so instead of n squared operations as you would

03:08:51.820 --> 03:08:56.260
typically require here because the Fourier transform the the the matrix has

03:08:56.260 --> 03:09:01.260
a very redundant structure you can you can avoid these explicit multiplications

03:09:01.260 --> 03:09:05.060
you can reuse some of the multiplications and do it in n log n operations so

03:09:05.060 --> 03:09:08.740
there are classes of algorithms that are called fast Fourier transforms and this

03:09:08.740 --> 03:09:13.460
is from the approximately the sixes when this was derived with the most famous

03:09:13.460 --> 03:09:18.220
algorithm is by Kuli and Tuki this is how signal processing has been done and

03:09:18.220 --> 03:09:23.500
you have it everywhere from your stereo to your iPhone from your computer so this

03:09:23.500 --> 03:09:27.260
is how it's done you cannot do it on graphs because on graphs the analogy of

03:09:27.260 --> 03:09:32.100
the Fourier transform would be the eigenvectors of either the adjacency

03:09:32.100 --> 03:09:35.980
matrix or the Laplacian matrix so if they are symmetric they have orthogonal

03:09:35.980 --> 03:09:40.100
eigen decomposition but these matrices do not have these redundant structures so

03:09:40.100 --> 03:09:45.300
the Fourier transform has n squared complexity dense matrix multiplication

03:09:45.300 --> 03:09:51.540
and actually some of the early crafting of electrical architectures came from

03:09:51.540 --> 03:09:56.380
this domain of signal processing on graphs where that that used the eigenvectors

03:09:56.380 --> 03:10:00.980
of the Laplacian or the adjacency matrix as an analogy of the Fourier transform

03:10:00.980 --> 03:10:09.140
so the difference in the case of the in the Euclidean case on the grid there is

03:10:09.140 --> 03:10:12.220
no difference between the two right so the Laplacian is also obviously a

03:10:12.220 --> 03:10:18.140
circuant operator circuant matrix and so is the shift right or the adjacency

03:10:18.140 --> 03:10:22.820
matrix of of the ring graph which happens to be the shift operator they all

03:10:22.820 --> 03:10:25.820
commute so they have the same eigenvectors on the general graph they are

03:10:25.820 --> 03:10:32.140
different so therefore these methods slightly slightly differ so the way to

03:10:32.140 --> 03:10:37.460
think of why you you want to look at the adjacency at the adjacency matrix is

03:10:37.460 --> 03:10:42.980
this right so this is how you can think of your convolution so these basically

03:10:42.980 --> 03:10:49.100
it's multiple diagonal matrix now we can write it as a sum weighted by these

03:10:49.100 --> 03:10:53.460
coefficients of the powers of the adjacency matrix right so the adjacency

03:10:53.460 --> 03:10:58.220
matrix of the ring graph will look like this so this red diagonal right so

03:10:58.220 --> 03:11:02.540
that's the shift operator so if you take a square you will get this if you get

03:11:02.540 --> 03:11:07.380
cube you will get this right so you combine all of them you will get you

03:11:07.420 --> 03:11:13.500
will get this general convolution so first architectures that try to do to do

03:11:13.500 --> 03:11:17.660
learning on graphs looked exactly at this taking powers of either the Laplacian

03:11:17.660 --> 03:11:22.620
or or the adjacency matrix basically polynomial with learnable coefficients

03:11:22.620 --> 03:11:27.940
now if you also look at terms of the degrees of freedom so a fully connected

03:11:27.940 --> 03:11:33.060
layer will look like this right so it has no it has no symmetry so here the

03:11:33.540 --> 03:11:39.500
symmetry is trivial so it has n square degrees of freedom in the case of

03:11:39.500 --> 03:11:46.020
convolution so the the the symmetry here is translation we have order of n

03:11:46.020 --> 03:11:49.460
degrees of freedom right so we reduce dramatically the number of parameters in

03:11:49.460 --> 03:11:53.900
the neural network we reuse the same coefficients everywhere in the case of a

03:11:53.900 --> 03:11:57.860
graph because we have permutation in variance we don't have the order of the

03:11:57.860 --> 03:12:01.500
neighbor so we must use the same coefficient so we can only distinguish

03:12:01.540 --> 03:12:06.260
between ourselves and our neighborhood right so that's well here it's I'm

03:12:06.260 --> 03:12:10.340
assuming a complete graph so this will look like something like deep sets for

03:12:10.340 --> 03:12:16.140
example so but the number of parameters is order of one so it's independent on

03:12:16.140 --> 03:12:23.100
the on the size of the domain what else can I say can I tell you well I know

03:12:23.100 --> 03:12:29.140
that I'm out of time so do you want to hear about molecules or you heard about

03:12:29.180 --> 03:12:42.540
molecules okay so let's talk about molecules I promise that I will try to do

03:12:42.540 --> 03:12:47.620
it try to do it fast and probably heard in Miguel's lecture as well so it will

03:12:47.620 --> 03:12:53.460
probably be a little bit repetitive so graphs are a very convenient model for

03:12:53.460 --> 03:12:56.860
molecules right basically a molecule looks like this so you can represent it

03:12:56.900 --> 03:13:01.460
as a graph and maybe that's not how chemists think of molecules but at least

03:13:01.460 --> 03:13:05.740
in some applications graph neural networks have been successful in predicting

03:13:05.740 --> 03:13:09.500
certain properties of molecules that are required for virtual drug screening

03:13:09.500 --> 03:13:16.060
right where the space of potentially synthesizable drug like molecules is

03:13:16.060 --> 03:13:19.940
huge something like 10 to the power 60 the number of molecules that they can

03:13:19.940 --> 03:13:23.660
actually test in the lab is significantly smaller so you need to

03:13:23.660 --> 03:13:28.100
reach this by some kind of computational methods and crafting networks have been

03:13:28.100 --> 03:13:33.340
shown again in predicting some properties to be significantly faster

03:13:33.340 --> 03:13:40.940
while similar complexity to similar accuracy to to classical methods so one

03:13:40.940 --> 03:13:46.940
thing that that that is important to say in regard regarding molecules so

03:13:46.940 --> 03:13:51.460
molecules are not just any graph where the symmetry that we have is the symmetry

03:13:51.460 --> 03:13:55.540
of the domain right the permutation of the nodes or the reordering of the atoms

03:13:55.540 --> 03:13:59.820
right so the domain symmetry tells you that no matter how you order the atoms

03:13:59.820 --> 03:14:04.300
in the molecule I still want to be able to say that it's the same molecule but

03:14:04.300 --> 03:14:07.740
it also has geometric coordinates right so in addition to the let's say atom

03:14:07.740 --> 03:14:11.740
types that we have here I also have the XYZ coordinates for every atom right so

03:14:11.740 --> 03:14:16.900
it's a graph that lives in a continuous Euclidean space so here what I want to

03:14:16.900 --> 03:14:21.700
say that if I rotate the molecule for example or translated I want to be able

03:14:21.700 --> 03:14:26.140
to say that the properties remain the same so in this case typically you look

03:14:26.140 --> 03:14:30.820
at the special Euclidean group so rotations and translations without

03:14:30.820 --> 03:14:36.220
reflections reflections can actually change the properties of molecules or

03:14:36.220 --> 03:14:42.100
you can use some other groups as well and there have been already several

03:14:42.100 --> 03:14:46.860
interesting success stories so one of them was a group of Jim Collins at MIT so

03:14:46.860 --> 03:14:50.540
they used graph neural networks in virtual screening pipelines where they

03:14:50.540 --> 03:14:56.700
tried to determine which compounds could be used as new antibiotics against

03:14:56.700 --> 03:15:02.660
antibiotic resistant bacteria and they famously found that that a candidate

03:15:02.660 --> 03:15:07.140
drug that was tested against diabetes called Halicin was actually effective

03:15:07.140 --> 03:15:14.740
across a broad range of of antibiotic resistance bacteria but in things that

03:15:14.900 --> 03:15:19.100
we are doing we are mostly interested in proteins and this is well I think this

03:15:19.100 --> 03:15:23.900
is in general proteins are important targets for for drugs because they are

03:15:23.900 --> 03:15:28.900
involved practically in anything that that happens in our body from defense

03:15:28.900 --> 03:15:34.100
against pathogens right antibodies are special types of proteins to delivering

03:15:34.100 --> 03:15:38.380
oxygen to ourselves hemoglobin is also a special type of protein so basically

03:15:38.380 --> 03:15:42.780
they're everywhere and encoded in our DNA so we really we don't know any life

03:15:42.820 --> 03:15:47.980
form that is not based on proteins at least for the time being and it was

03:15:47.980 --> 03:15:53.460
conjectured in the 70s by Anfins and Nobel laureate in chemistry that you can

03:15:53.460 --> 03:15:59.740
determine the structure of the protein from its sequence so proteins are long

03:15:59.740 --> 03:16:03.860
chains of amino acids connected to each other and then under the influence of

03:16:03.860 --> 03:16:09.580
electrostatic forces they fold into these complicated structures but we are

03:16:09.580 --> 03:16:15.020
interested in the opposite problem so maybe a little bit incorrectly we can

03:16:15.020 --> 03:16:21.180
call it some kind of inverse problem so I would like to to design a protein that

03:16:21.180 --> 03:16:25.500
will fall in fold into a certain structure of course it's not that simple

03:16:25.500 --> 03:16:29.460
because it is tempting to think that we have a sequence that then falls into a

03:16:29.460 --> 03:16:32.500
structure and the structure and doubts the protein with certain function for

03:16:32.500 --> 03:16:39.380
example what kind of molecules it binds and initially computer scientists look

03:16:39.380 --> 03:16:44.420
at proteins as sequences because well it's just strings so we can look for

03:16:44.420 --> 03:16:47.940
certain patterns you can try to align different sequences together right like

03:16:47.940 --> 03:16:52.740
multiple multiple sequence alignment then came the problem of structure

03:16:52.740 --> 03:16:56.460
prediction and that's where alpha fold excelled recently but then the problem

03:16:56.460 --> 03:17:01.060
of function is distinct and you can find examples of for example proteins with

03:17:01.060 --> 03:17:06.580
different sequences but similar structure you can find proteins with very

03:17:06.580 --> 03:17:10.580
similar sequences but very different structure or you can also find proteins

03:17:10.580 --> 03:17:13.540
with different sequences and different structures but similar function so they

03:17:13.540 --> 03:17:19.460
happen to to bind the same the same molecule so the good analogy here is

03:17:19.460 --> 03:17:24.580
this lock and key metaphor that was introduced by I like quotes from

03:17:24.580 --> 03:17:28.660
Nobel laureates so that was from Emil Fischer also Nobel Nobel laureate in

03:17:28.980 --> 03:17:32.900
in chemistry so he was talking about enzymes but I think it's more general

03:17:32.900 --> 03:17:37.700
applies to to proteins broadly so same way as you have a unique key that fits

03:17:37.700 --> 03:17:43.060
into a lock you might have a unique molecule or at least that's that's the

03:17:43.060 --> 03:17:47.620
wishful thinking is that a unique molecule that will fit into some pocket

03:17:47.620 --> 03:17:51.940
that exists on the the surface of this folded protein structure and this is how

03:17:51.940 --> 03:17:56.900
drugs are typically designed right so you have a protein that is your target so

03:17:57.300 --> 03:18:02.500
that's how its surface looks like and here is some small molecule that sticks

03:18:02.500 --> 03:18:08.900
into this hole and binds this this molecule and that's how the drug works so this is

03:18:08.900 --> 03:18:13.540
actually a molecule not exactly of caffeine but of compound from the same

03:18:13.540 --> 03:18:17.620
class and that's how it's by how it binds the adenosine receptor in the brain

03:18:18.660 --> 03:18:22.420
many other interesting targets though they don't have these kind of pocket like

03:18:22.420 --> 03:18:27.940
structures and there are interesting systems of protein of proteins interacting

03:18:27.940 --> 03:18:32.260
with each other like this one the program death complex where you have two proteins

03:18:32.260 --> 03:18:39.060
called pd1 and pdl1 and they are involved in cancer immunotherapy basically these

03:18:39.060 --> 03:18:42.980
proteins tell our immune system not to kill healthy cells and some cancers

03:18:43.540 --> 03:18:48.900
have these proteins so they manage to evade the normal functioning of the immune system and

03:18:48.900 --> 03:18:55.060
the idea is to block one of these proteins either pd1 or pdl1 and this way basically

03:18:55.060 --> 03:19:00.260
the malignant cells are destroyed by by by the immune system so you need to design

03:19:01.060 --> 03:19:07.460
some binder that will that will bind to one of these proteins and they happen to have

03:19:07.460 --> 03:19:11.060
these kind of flat interfaces so they're considered to be hard or impossible to

03:19:11.060 --> 03:19:15.700
drug by small molecules but you can drug them by proteins so that's the idea of biological drugs

03:19:16.660 --> 03:19:22.020
where the drug itself is is a protein molecule typically an antibody for variety of reasons

03:19:23.380 --> 03:19:27.460
so you can use geometrically planning well and unfortunately I didn't have time to talk about

03:19:27.460 --> 03:19:32.420
it but basically instead of considering graphs we can consider surfaces so we model proteins as

03:19:33.220 --> 03:19:39.940
many folds as as basically the external surface that that that appears to

03:19:40.900 --> 03:19:45.860
to the other molecule that that tries to bind it and this way you abstract all the internal

03:19:47.060 --> 03:19:51.780
intricacies of the fold so let me try to show you an example so this is a plastic model of a protein

03:19:52.340 --> 03:19:59.220
you see so this is protein is the the one that that the person holds is supposed to bind to

03:19:59.220 --> 03:20:05.700
these transparent ones so you see that these complicated helixes and and other things inside

03:20:05.700 --> 03:20:10.980
so that's the protein fold but what appears from the outside is this transparent surface so

03:20:10.980 --> 03:20:16.180
this guy doesn't care what happens inside so it cares only about the the the structure of course

03:20:16.180 --> 03:20:20.580
the problem is more complicated because the the conformation of the protein the its geometric

03:20:20.580 --> 03:20:25.060
structure might change as a result of of the interaction but at least it's in some cases

03:20:25.060 --> 03:20:33.220
it's a good approximation so long story short we we can do special type of neural networks that

03:20:33.220 --> 03:20:38.340
operate on these surfaces so they take into account both geometric and and chemical properties of

03:20:38.340 --> 03:20:45.540
of the molecular surface and they can they try to find complementary structures that are expected

03:20:45.540 --> 03:20:50.740
to interact so think of kind of pieces of three-dimensional puzzle but it's not only

03:20:50.740 --> 03:20:54.980
geometric complementarity it's also chemical complementarity so they need to have the right

03:20:54.980 --> 03:21:00.500
charges so they don't repel each other and this is a method that we call the massive so

03:21:01.460 --> 03:21:10.660
we were lucky to appear on the cover of nature methods in 2020 and this year we also had a paper

03:21:10.660 --> 03:21:16.180
in nature that contained experimental results so we also hope to appear on the cover but they chose

03:21:16.180 --> 03:21:21.140
a different one but because we paid for the cover here you need to you need to see it i think it was

03:21:21.700 --> 03:21:29.140
a cool image so we used this method to design new binders for different targets and basically it's

03:21:29.140 --> 03:21:34.740
a fragment based design so we use this neural network architecture to identify potentially

03:21:34.740 --> 03:21:42.020
complementary targets that then we use to build the binder and here the experimental results show

03:21:42.020 --> 03:21:49.300
different structures so here's a binder for the pdl one oncological target and we also have the

03:21:49.300 --> 03:21:57.380
crystal structures and here's an example of another binder for the SARS-CoV-2 spike protein so that's

03:21:57.380 --> 03:22:05.540
the coronavirus that caused the COVID-19 pandemic that has been terrorizing us for more than three

03:22:05.540 --> 03:22:14.180
years now and basically this structure binds the region of the of the spike protein that interacts

03:22:14.180 --> 03:22:19.460
with the ACE2 receptor of the host so that's where how the virus enters into into our body

03:22:20.340 --> 03:22:26.260
and here we also tested it so we have the structure from cryoEM we also tested it on

03:22:26.260 --> 03:22:34.100
different variants of the virus so the alpha beta and omicron that probably everybody was

03:22:34.100 --> 03:22:40.980
following in the newspapers so you see that that it binds many of these maybe some others less

03:22:41.620 --> 03:22:47.780
and here's also a comparison to a clinically approved drug so that was antibodies that were

03:22:47.780 --> 03:22:54.180
developed by AstraZeneca so basically what is shown here is how much inhibition you have

03:22:54.180 --> 03:23:00.340
versus concentration so the smaller concentration the better of course so we are not as good as

03:23:00.340 --> 03:23:06.100
the AstraZeneca drug but so it's something that was designed totally computationally and this is

03:23:06.100 --> 03:23:13.940
actually pseudovirus neutralization so it is probably much closer to real validation than

03:23:13.940 --> 03:23:20.980
at least anything that myself as a computer scientist could think of well I think I will

03:23:20.980 --> 03:23:27.540
probably stop here but if you think of diffusion models right so generative models everybody is

03:23:28.660 --> 03:23:32.660
now talking about right like like the Dali2 and now of course you have way better results

03:23:33.300 --> 03:23:39.860
so you could imagine something like this for for molecular design so we have some condition on

03:23:39.860 --> 03:23:44.500
on let's say diffusion model that we use here like the geometric structure of the of the target

03:23:44.500 --> 03:23:50.100
pocket and you'll try to build a molecule that satisfies these these constraints so we don't

03:23:50.100 --> 03:23:55.220
really have a text prompt but you have maybe some some other way of conditioning the model so

03:23:56.020 --> 03:23:59.940
so this is one example maybe not very interesting so another example is what we call diffusion

03:23:59.940 --> 03:24:05.620
linker where we have small molecular fragments what is called pharmacophores that you know how

03:24:05.620 --> 03:24:11.620
they bind the target but you also need to connect them into bigger molecule and we try to basically

03:24:11.620 --> 03:24:17.700
to start with these little fragments and to diffuse the the the linking structure that that connects

03:24:17.700 --> 03:24:23.460
them we're not very lucky in publishing this paper in europe so we'll probably send it to some

03:24:23.460 --> 03:24:30.820
chemical journal uh well I think I will stop here sorry for running out of time thank you very much

03:24:31.220 --> 03:24:33.940
uh

03:24:44.100 --> 03:24:47.380
yeah we are over time but if you have still a couple of questions

03:24:49.460 --> 03:24:51.620
if not you can ask individual maybe

03:24:53.540 --> 03:24:57.940
okay but thank you again for for the amazing talk well thank you

03:25:00.820 --> 03:25:03.940
thank you

